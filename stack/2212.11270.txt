Generalized Decoding for Pixel, Image, and Language
Xueyan Zou∗§, Zi-Yi Dou∗♯, Jianwei Yang∗‡♠, Zhe Gan†, Linjie Li†, Chunyuan Li‡, Xiyang Dai†, Harkirat Behl‡
Jianfeng Wang†, Lu Yuan†, Nanyun Peng♯, Lijuan Wang†, Yong Jae Lee¶§, Jianfeng Gao¶‡
§ University of Wisconsin-Madison
♯UCLA
‡ Microsoft Research at Redmond
† Microsoft Cloud & AI
∗Equal Technical Contribution ¶ Equal Advisory Contribution ♠Project Lead
{xueyan,yongjaelee}@cs.wisc.edu
{zdou,violetpeng}@cs.ucla.edu
{jianwyan,jfgao,zhgan,linjli,chunyl,jianfw,luyuan,lijuanw,hbehl,xidai}@microsoft.com
Figure 1. With one suite of parameters, X-Decoder after pretraining supports all types of image segmentation tasks ranging from open-vocabulary in-
stance/semantic/panoptic segmentation to referring segmentation, and vision-language tasks including image-text retrieval, and image captioning (labeled in
green boxes). It further empowers composite tasks like referring captioning using X-Decoder itself and image editing that combines with generative models
such as Stable Diffusion [66] (labeled in yellow boxes).
Abstract
We present X-Decoder, a generalized decoding model
that can predict pixel-level segmentation and language to-
kens seamlessly.
X-Decoder takes as input two types of
queries: (i) generic non-semantic queries and (ii) semantic
queries induced from text inputs, to decode different pixel-
level and token-level outputs in the same semantic space.
With such a novel design, X-Decoder is the ﬁrst work that
provides a uniﬁed way to support all types of image segmen-
tation and a variety of vision-language (VL) tasks. Further,
our design enables seamless interactions across tasks at dif-
ferent granularities and brings mutual beneﬁts by learn-
ing a common and rich pixel-level visual-semantic under-
standing space, without any pseudo-labeling.
After pre-
training on a mixed set of a limited amount of segmenta-
tion data and millions of image-text pairs, X-Decoder ex-
hibits strong transferability to a wide range of downstream
tasks in both zero-shot and ﬁnetuning settings.
Notably,
it achieves (1) state-of-the-art results on open-vocabulary
segmentation and referring segmentation on eight datasets;
(2) better or competitive ﬁnetuned performance to other
generalist and specialist models on segmentation and VL
tasks; and (3) ﬂexibility for efﬁcient ﬁnetuning and novel
task composition (e.g., referring captioning and image edit-
ing shown in Fig. 1). Code, demo, video and visualization
are available at: https://x-decoder-vl.github.io.
1. Introduction
Visual understanding at different levels of granularity
has been a longstanding problem in the vision community.
The tasks span from image-level tasks (e.g., image clas-
siﬁcation [15], image-text retrieval, image captioning [8],
and visual question answering (VQA) [2]), region-level lo-
The work is developed during an internship at Microsoft.
1
arXiv:2212.11270v1  [cs.CV]  21 Dec 2022

calization tasks (e.g., object detection and phrase ground-
ing [63]), to pixel-level grouping tasks (e.g., image in-
stance/semantic/panoptic segmentation [28, 37, 51]). Until
recently, most of these tasks have been separately tackled
with specialized model designs, preventing the synergy of
tasks across different granularities from being exploited. In
light of the versatility of transformers [72], we are now wit-
nessing a growing interest in building general-purpose mod-
els that can learn from and be applied to a diverse set of
vision and vision-language tasks, through multi-task learn-
ing [27, 32], sequential decoding [7, 54, 76, 85], or uniﬁed
learning strategy [84, 91, 94, 95]. While these works have
shown encouraging cross-task generalization capabilities,
most target the uniﬁcation of image-level and region-level
tasks, leaving the important pixel-level understanding un-
derexplored. In [7, 54], the authors attempt to unify seg-
mentation into a decoding of a coordinate sequence or a
color map, which, however, produces suboptimal perfor-
mance and limited support for open-world generalization.
Arguably, understanding images down to the pixel level
is one of the most important yet challenging problems in
that: (1) pixel-level annotations are costly and undoubt-
edly much more scarce compared to other types of anno-
tations; (2) grouping every pixel and recognizing them in
an open-vocabulary manner is less studied; and (3) more
importantly, it is non-trivial to learn from data at two sub-
stantially different granularities while also obtaining mutual
beneﬁts. Some recent efforts have attempted to bridge this
gap from different aspects. In [12], Chen et al. propose
a uniﬁed architecture Mask2Former that tackles all three
types of segmentation tasks but in a closed set. To support
open vocabulary recognition, a number of works study how
to transfer or distill rich semantic knowledge from image-
level vision-language foundation models such as CLIP [64]
and ALIGN [34] to specialist models [17,25,65]. However,
all these initial explorations focus on speciﬁc segmentation
tasks of interest and do not show generalization to tasks at
different granularities. In this work, we take one step fur-
ther to build a generalized decoder called X-Decoder1 to-
wards the uniﬁcation of pixel-level and image-level vision-
language understanding, as shown in Figure 1.
A generalized decoding framework.
We formulate
all tasks including pixel-level image segmentation, image-
level retrieval and vision-language tasks into a generic de-
coding procedure. Speciﬁcally, X-Decoder is built on top
of a vision backbone and a transformer encoder for ex-
tracting multi-scale image features, following the frame-
work of Mask2Former [12]. The key novelty lies in the
decoder design. First, it takes two sets of queries as in-
put: (i) generic non-semantic queries that aim to decode
segmentation masks for universal segmentation, similar
to Mask2Former [12], and (ii) newly introduced textual
1Here, ‘X’ denotes versatile, and also represents ‘piXel’.
queries to make the decoder language-aware for a diverse
set of language-related vision tasks. Second, it predicts two
types of outputs: pixel-level masks and token-level seman-
tics, and their different combinations can seamlessly sup-
port all tasks of interest. Third, we use a single text encoder
to encode the textual corpus involved in all tasks, includ-
ing concepts in segmentation, phrases in referring segmen-
tation, tokens in image captioning and questions in VQA,
etc. As a result, our X-Decoder can naturally facilitate the
synergy across tasks and advocate the learning of a shared
visual-semantic space, while respecting the heterogeneous
nature of different tasks.
An end-to-end learning paradigm. With our general-
ized decoder design, we propose an end-to-end pretraining
method to learn from all granularities of supervision. We
unite three types of data: panoptic segmentation, referring
segmentation, and image-text pairs. Unlike previous works
that use pseudo-labeling techniques to extract ﬁne-grained
supervision from image-text pairs [25, 95], X-Decoder di-
rectly groups and proposes a few meaningful segmentation
candidates, so that it can map the regions easily to the con-
tents described in the captions on the ﬂy. Meanwhile, the re-
ferring segmentation task bridges generic segmentation and
image captioning by sharing the pixel-level decoding with
the former and semantic queries with the latter.
Strong zero-shot and task-speciﬁc transferability to a
wide range of segmentation and VL tasks. Pre-trained
with a limited amount of segmentation data and millions
of image-text pairs, our X-Decoder supports a diversity of
tasks in a zero-shot and open-vocabulary manner.
Con-
cretely, our model can be directly applied for all three types
of segmentation tasks in a wide range of domains, estab-
lishing new state-of-the-art on ten settings of seven datasets.
When transferred to speciﬁc tasks, our model also exhibits
consistent superiority to previous works. Finally, we ob-
serve some intriguing properties in our model that it can
support some novel task compositions and efﬁcient ﬁnetun-
ing, thanks to the ﬂexibility endowed by our model design.
2. From Specialist to Generalist Models
2.1. Pixel-Level Understanding
Pixel-level image understanding, also known as image
segmentation, has been a long-standing problem [23,62].
Generic Segmentation.
There are mainly three well-
deﬁned tasks for pixel-level understanding, including se-
mantic [51], instance [28], and panoptic [37] segmentation.
Semantic segmentation cares about the per-pixel semantic
within an image [6,11,51], whereas instance segmentation
groups pixels of the same semantic meaning into object in-
stances. Models for both tasks have evolved from CNN-
based architectures [51] to transformer-based ones [11], and
from two-stage models [29] to one-stage models [3, 71]
and to the recent query-based approaches [18, 100]. With
2

the capability of per-pixel and instance-level understand-
ing, a natural step was taken to formulate panoptic seg-
mentation [12, 37, 73]. Most recently, Mask2Former [12]
proposed to address all three tasks with a uniﬁed encoder-
decoder architecture. Nevertheless, all these works cope
with a limited number of categories, i.e., models can hardly
recognize concepts absent in the training set. In MSeg [40],
the authors manually merge different datasets and train a
more generalized model on the composite set, which is still
limited to being a closed set.
Open-Vocabulary Segmentation.
Recently, a number
of works opt to transfer or distill the rich visual-semantic
knowledge from foundation models like CLIP [64] and
ALIGN [34] to speciﬁc segmentation tasks. Prominent ex-
amples include LSeg [41], OpenSeg [25], and [33].
In-
stead of using existing models, GroupViT [81] performed
language-image pretraining from scratch with a bottom-up
grouping ViT [19], while DenseCLIP [65] demonstrated the
superiority of foundation models in ﬁnetuning settings com-
pared with supervised models. Recently, MaskCLIP [17]
proposed to tackle open-vocabulary panoptic and semantic
segmentation by leveraging CLIP, and achieved SoTA per-
formance on ADE20K [98] and PASCAL [22,59].
Referring Segmentation by nature is open-vocabulary in
that it does not presume a ﬁxed number of phrases in the
training and inference times. Models are usually designed
speciﬁcally to learn from target datasets using various mul-
timodal fusion strategies [31,49,57,88,92]. Since the emer-
gence of vision transformers, works like LAVT [86] en-
hance the cross-modal interactions from the very beginning,
which led to SoTA on RefCOCO [92], RefCOCO+ [92] and
G-Ref [56,60]. CLIPSeg [55] extended the textual query to
a visual query and showed superior performance not only on
referring segmentation but also on semantic segmentation.
In this work, we propose X-Decoder, which is the ﬁrst
model to tackle generic and referring segmentation tasks all
in one model. Furthermore, the generalized decoder jointly
learns from segmentation data and image-text pairs end-to-
end, and thus can augment the synergy across tasks for rich
pixel-level and image-level understanding.
2.2. Vision-Language Understanding
Vision-language (VL) pretraining has proven to be ef-
fective for various VL tasks [44, 53, 69, 70].
The ﬁeld
has evolved from a transformer fusion model [10, 46, 96]
with pre-extracted object features [1] to end-to-end trans-
formers [21, 36, 43], that directly learn from raw image
pixels. Recently, researchers [68, 78, 79] have found that
image-text data at scale can be helpful for visual repre-
sentation learning (e.g., enabling zero-shot image classiﬁ-
cation [34, 64] and action recognition [91, 94]). VL pre-
trained models can be further extended to region-level tasks,
such as phrase grounding and open-vocabulary object de-
tection [26, 35, 58, 97], and uniﬁed frameworks that aim to
Figure 2. Overall pipeline for our model. It consists of an image encoder,
a text encoder and our own designed X-Decoder.
combine image-text pairs with region-level data have also
been proposed [4, 20, 45, 87, 95]. A comprehensive review
on this topic is provided in [24].
We are witnessing a clear trend from building special-
ist models to generalist ones. Early efforts [27, 32] build
a multi-task learning paradigm to accommodate a diversity
of tasks. However, the interactions among different tasks
in these works are less studied, and the combination usu-
ally leads to performance degradation compared with spe-
cialist models.
Recently, a number of works aim to re-
formulate the tasks into a uniﬁed sequential decoding pro-
cess [7, 38, 54, 76, 85]. In this work, instead of developing
a uniﬁed interface for vision and VL tasks, our X-Decoder
builds a generalized decoding paradigm that can seamlessly
connect the tasks by taking the common (e.g., semantic) but
respecting the natural differences (e.g., spatial mask v.s. se-
quential language), leading to signiﬁcant improvements for
different segmentation and VL tasks across the board.
3. X-Decoder
3.1. Formulation
Our model follows the generic design of encoder-
decoder architecture as shown in Fig. 2. Given an input
image I ∈RH×W ×3, we ﬁrst use an image encoder EncI
to extract features Z. Afterwards, we use the text encoder
EncT to encode a textual query T into Qt = ⟨qt
1, · · · , qt
n⟩
of length n. The visual features, textual queries and the m
non-semantic or latent queries Qh = ⟨qh
1 , · · · , qh
m⟩are fed
to our X-Decoder to predict the outputs:
⟨Op, Os⟩= XDec

⟨Qh, Qt⟩; Z

(1)
where Op and Os are the pixel-level masks and token-level
semantics, respectively. In the above formula, we note three
critical designs to empower the generalization ability of our
X-Decoder to a variety of vision and vision-language tasks.
We deﬁne two types of queries and outputs for X-
Decoder. As discussed earlier, the queries for the decoder
are categorized into latent queries Qh and text queries Qt,
which undertake generic vision and vision-language tasks,
respectively, and their combinations can further support var-
ious language-aware tasks such as referring segmentation,
3

Figure 3. Unifying four different types of tasks with our proposed X-Decoder. From left to right, they are: (a) generic semantic/instance/panoptic
segmentation; (b) referring segmentation; (c) image-text retrieval and (d) image captioning and VQA. The components with white text indicate not applied.
VQA, etc. Likewise, the output is categorized into pixel-
level mask Op and semantic embedding Os. By simply
using different combinations, we can adapt our X-Decoder
to various tasks with the same suite of parameters.
We employ a single text encoder EncT to encode the
textual corpus from all tasks. The common text encoder
is used to encode referring phrases, text descriptions, im-
age captions in the task of referring segmentation, image-
text retrieval and image captioning, respectively. Further-
more, we reformulate the mask classiﬁcation in segmenta-
tion into a mask-text matching problem between Os and
the textual embeddings of prompted textual concepts simi-
lar to [25,84]. Sharing the text encoder for all textual corpus
could maximally exchange knowledge from different tasks
and learn a richer and more coherent semantic space.
We fully decouple the image and text encoder. In many
previous uniﬁed encoder-decoder models [7, 35, 85], the
image and text are fused in the encoder side.
This de-
sign makes it intractable not only for global image-text
contrastive learning [64, 84], but also generative pretrain-
ing [75]. In contrast, by fully decoupling the image and text
encoder and using the outputs all as queries, X-Decoder can
learn from both intra-image supervisions and inter-image
ones, which is essential to learn stronger pixel-level repre-
sentations and support different granularity of tasks.
3.2. Uniﬁcation of Tasks
Based on the above designs, X-Decoder can be used to
seamlessly unify different vision and vision-language tasks,
simply with different combinations of queries as inputs.
Generic Segmentation. For this task, there are no textual
queries as inputs. Hence, Eq. (1) becomes:
⟨Op, Os⟩= XDec(Qh; Z)
(2)
where Op, Os have the same size of Qh. Eq. (2) reduces to
Mask2former [12], but with open-vocabulary capacity since
we use mask-text matching for mask classiﬁcation.
Referring Segmentation. It requires both latent and text
queries as inputs, thus shares the same formula as Eq. (1).
Similar to generic segmentation, we only use the ﬁrst m
decoded outputs corresponding to the latent queries. Com-
pared with Eq. (2), referring segmentation can be regarded
as language-conditioned generic segmentation.
Image-Text Retrieval. The decoupled image and text en-
coder in our X-Decoder makes it straightforward for inter-
image retrieval tasks. Speciﬁcally, we only feed the latent
queries to the decoder and obtain the semantic representa-
tion of an image:
Os = XDec

Qh; Z

(3)
where Os has the same length as Qh, and the last (m-th)
token in Os is then used to compute the similarities between
images and texts.
Image Captioning and VQA. For both tasks, X-Decoder
takes both latent and text queries and decodes the outputs:
Os = XDec

⟨Qh, Qt⟩; Z

(4)
where Os correspondingly has equal size to Qt, and no
masks are predicted. There are two slight differences be-
tween the two tasks. First, the caption prediction follows a
causal masking strategy while VQA does not. Second, we
use all the outputs in Os for captioning, but only the last
one to predict the answer for VQA.
The adaptation of our X-Decoder to each task is further
depicted in Fig. 3. Based on this uniﬁcation, we can pre-
train our X-Decoder jointly with all tasks using a proper
combination of queries and losses, and further ﬁnetune for
individual tasks without any extra heads.2 As discussed ear-
lier, a lineup of works exploited a sequential decoding in-
terface for the uniﬁcation [7, 7, 13, 54, 77, 85]. However,
in this work, we advocate the uniﬁcation by functionality
rather than interface, namely, we maximally share the com-
mon parts of different tasks while keeping the remaining
unchanged for individual tasks.
3.3. Uniﬁed Architecture
We follow Mask2Former [12] to build our decoder archi-
tecture. Given an image I ∈RH×W ×3, we extract hierar-
chical visual features from L layers:
Z = EncI(I) = ⟨zl⟩L
l=1
(5)
where zl ∈RHl×Wl×d and {Hl, Wl} is the size of fea-
ture map at level l and d is the feature dimension. These
hierarchical feature maps are important for pixel-level un-
derstanding at different scales.
2VQA is used for pretraining following common practice.
4

Figure 4. Interaction among latent queries (gree), between latent and text
queries (yellow) for (a) Generic segmentation and image/text retrieval (b)
referring segmentation and (c) image captioning. The square latent query
is designated for image-text retrieval.
One Decoder XDec for All Tasks. Given the visual fea-
tures Z, X-Decoder uses a stack of transformer layers to
reﬁne the queries and render the outputs.
At layer l, it
ﬁrst cross-attends the visual features and then performs self-
attention among latent and text queries:
⟨ˆQh
l−1, ˆQt
l−1⟩= CrossAtt(⟨Qh
l−1, Qt
l−1⟩; Z)
(6)
⟨Qh
l , Qt
l⟩= SelfAtt(⟨ˆQh
l−1, ˆQt
l−1⟩)
(7)
In Eq. (6), we let all queries cross-attend the visual features.
For latent queries, we use a masked cross-attention mech-
anism as in [12], and full attention for the textual queries.
In Eq. (7), we speciﬁcally design the self-attention mech-
anism to prompt the synergy of tasks: (i) we use the last
latent query to extract the global image representation and
the remaining for generic segmentation; (ii) for image cap-
tioning, each textual query can attend itself, its predecessors
and all latent queries; (iii) for referring segmentation, latent
queries will attend all text queries to use it as the language
condition.
Based on these rules, the resulting self-attention in our
X-Decoder is shown in Fig. 4.
The output of our X-Decoder is also categorized into
two types: 1) pixel-wise mask and 2) semantic outputs. X-
Decoder always produces the masks only for the m latent
queries, i.e., Op = {op
1, · · · , op
m} ∈{0, 1}m×H×W for all
the latent queries. As for the semantic outputs, X-Decoder
predicts the outputs for both latent and text queries, i.e.,
Os = {os
1, · · · , os
m+n} ∈R(m+n)×d, to cover both mask
recognition and caption generation.
One Encoder EncT for All Texts. Our text encoder con-
sists of a number of transformer layers. Given the raw text
such as a phrase or caption, we convert it to discrete tokens
using an off-the-shelf tokenizer and then send it to the text
encoder. We apply causal masking to ensure its outputs are
compatible with caption decoding. For segmentation, we
follow [64,84] to convert the class name into a phrase with
a text prompt (e.g., “dog” →“an image of dog”), and en-
code the phrase as above.
3.4. End-to-End Pre-training
We train our X-Decoder in an end-to-end manner with
two types of losses corresponding to the outputs.
Semantic Loss. There are three losses on the semantic out-
puts corresponding to three tasks. For image-text retrieval,
we compute the language-image contrastive loss as [64].
We take the last valid token feature of Qt from the text
encoder to represent a text as ˆqt and take the last entry in
Os derived from X-Decoder as ˆos. As a result, we obtain B
pairs of features ⟨ˆqt
i, ˆos
i⟩B
i=1 for a minibatch of B image-
text pairs.
Afterwards, we compute the dot-product be-
tween these B × B feature pairs to obtain an afﬁnity matrix
Sit ∈RB×B, and compute the bidirectional cross-entropy
loss:
Lit = CE(Sit, yit) + CE(ST
it, yit)
(8)
where yit are the class labels corresponding to diagonal en-
tries in Sit, and ST
it is the transpose of Sit.
For mask classiﬁcation, we encode all C class names in-
cluding “background” into C text queries and take the last
valid token feature from each to represent the concept. Af-
terward, we take the decoder outputs corresponding to the
ﬁrst (m −1) latent queries and compute the dot-product
between these outputs and concept embeddings to obtain
an afﬁnity matrix Scls ∈R(m−1)×C and compute the loss
Lcls = CE(Scls, ycls), with the ground-truth class ycls.
For image captioning, we ﬁrst extract the embeddings
for all tokens in the vocabulary of size V from the text en-
coder. Given the last n semantic outputs from X-Decoder,
we compute the dot-product with all token embeddings to
obtain an afﬁnity matrix Scap ∈Rn×V . Then we compute
the cross-entropy loss Lcap = CE(Scap, ycap), with the
ground-truth next-token id ycap.
Mask Loss. Given the predictions ⟨Op, Os⟩derived from
m latent queries, we use Hungarian matching [5,12] to ﬁnd
the matched entries of ﬁrst (m −1) outputs to ground-truth
annotations. Afterward, we follow [12] to use binary cross-
entropy loss Lbce and dice loss Ldice to compute the loss
for masks. We combine the above four losses to pretrain
our X-Decoder. More details can be found in Appendix.
4. Experiments
4.1. Experimental Setup
Datasets and Settings. We pretrain X-Decoder on three
types of data including panoptic segmentation, image-text
pairs (itp), and referring segmentation. For panoptic and
referring segmentation, we use COCO2017 [48] with seg-
mentation annotations and exclude the validation sets of
Ref-COCOg UMD [92] and COCO Karpathy [89]. In total,
there are 104k images for segmentation pretraining, out of
which 30k images are with referring segmentation annota-
tions. For image-text pairs, we use the standard 4M corpora,
including Conceptual Captions [67], SBU Captions [61],
Visual Genome [39], and COCO Captions [9]. We broadly
evaluate our models on all tasks covered by pretraining,
including generic (Semantic/Instance/Panoptic) segmenta-
tion, referring segmentation, image-text retrieval, and im-
age captioning. In particular, we benchmark on 10 settings
of 7 datasets covering a wide range of domains. Moreover,
5

Method
Type
Generic Segmentation
Referring
Retrieval
Captioning
VQA
ADE
COCO
g-Ref
COCO-Karpathy F30k-Karpathy COCO-Karpathy
VQAv2-test
PQ
mAP mIoU
PQ
mAP
mIoU
cIoU
IR@1
TR@1
IR@1
TR@1
CIDEr
BLEU
dev
std
Mask2Former (T) [12]
Segmentation
39.7
26.4
47.7
53.2
43.3
63.2
-
-
-
-
-
-
-
-
-
Mask2Former (B) [12]
⋆
⋆
53.9
56.4
46.3
67.1
-
-
-
-
-
-
-
-
-
Mask2Former (L) [12]
48.1
34.2
56.1
57.8
48.6
67.4
-
-
-
-
-
-
-
-
-
Pano/SegFormer (B) [47,80]
⋆
⋆
51.0
55.4
⋆
⋆
-
-
-
-
-
-
-
-
-
kMaX-DeepLab (L) [93]
48.7
⋆
54.8
58.1
⋆
⋆
-
-
-
-
-
-
-
-
-
LAVT (B) [86]
-
-
-
-
-
-
61.2
-
-
-
-
-
-
-
-
UNITER (B) [10]
Vision Language
(VL)
-
-
-
-
-
-
-
50.3
64.4
72.5
85.9
-
-
72.7
72.9
UNITER (L) [10]
-
-
-
-
-
-
-
52.9
65.6
75.6
87.3
-
-
73.8
74.0
VinVL (B) [96]
-
-
-
-
-
-
-
58.1
74.6
⋆
⋆
129.3
38.2
76.0
76.1
VinVL (L) [96]
-
-
-
-
-
-
-
58.8
75.4
⋆
⋆
130.8
38.5
76.5
76.6
ALBEF-4M (B) [43]
-
-
-
-
-
-
-
56.8
73.1
82.8
94.3
⋆
⋆
74.5
74.7
METER-Swin (B) [21]
-
-
-
-
-
-
-
54.9
73.0
79.0
92.4
⋆
⋆
76.4
76.4
UViM (L) [38]
⋆
⋆
⋆
45.8 1
⋆
⋆
-
-
-
-
-
-
-
-
-
UniT (T) [32]
General Purpose
-
-
-
-
-
-
-
-
-
-
-
-
-
67.6
⋆
GPV (T) [27]
-
-
-
-
-
-
-
-
-
-
-
102.3 2
⋆
62.5
⋆
UniTAB (B) [85]
-
-
-
-
-
-
-
-
-
-
-
119.8
36.1
70.7
71.0
Pix2Seq v2 (B) [7]
-
⋆
-
-
38.2
-
-
-
-
-
-
⋆
34.9
-
-
Uniﬁed-IO (B) [54]
-
⋆
-
-
⋆
-
-
-
-
-
-
⋆
⋆
61.8
⋆
Uniﬁed-IO (L) [54]
-
⋆
-
-
⋆
-
-
-
-
-
-
⋆
⋆
67.8
⋆
GLIPv2 (T) [95]
-
⋆
-
-
-/42.0
-
⋆
-
-
-
-
122.1
⋆
71.6
71.8
GLIPv2 (B) [95]
-
⋆
-
-
-/45.8
-
⋆
-
-
-
-
128.5
⋆
73.1
73.3
GLIPv2 (H) [95]
-
⋆
-
-
-/48.9
-
⋆
-
-
-
-
131.0
⋆
74.6
74.8
X-Decoder (T)
41.6
27.7
51.0
52.6
41.3/42.3
62.4
59.8 | 61.9
49.3
66.7
74.4
89.1
122.3
37.8
70.6
70.9
X-Decoder (B)
46.8
33.5
54.6
56.2
45.8/45.8
66.0
62.4 | 64.5
54.5
71.2
80.8
93.2
129.0
39.6
74.1
74.2
X-Decoder (L)
49.6
35.8
58.1
56.9
46.7/47.1
67.5
64.6 | 64.6
58.6
76.1
84.4
94.4
132.1
40.2
76.8
77.0
Table 1. Task-speciﬁc transfer of X-Decoder to different segmentation and VL tasks. Note: “⋆” denotes the model has the capability for the task but
does not have number reported. “-” means the model does not have the ability for the speciﬁc task. “L*” is the large model with deformable encoder.
“model name” means the model does not have task speciﬁc ﬁnetune. “1” is the reported pretrained number for UViM, the corresponding X-Decoder (L) has
pretrained PQ 56.7. “2” is the reported coco test2014 value for GPV. “a|b” means “pretrain|ﬁnetune”. “a/b” indicate “val/test”.
we ﬁnetune and report results on VQA for ﬁne-grained vi-
sual reasoning.
Implementation Details. Our visual encoder follows [12]
to use 100 latent queries and 9 decoder layers for segmenta-
tion, and we add one additional latent query for image-level
task. However, we do not adopt a deformable encoder as
it does not generalize well to open-vocabulary settings (see
in Appendix). We adopt Focal-T [83] and DaViT-B/L [16]
as the vision encoder and a transformer text encoder with
causal masking [64, 94] as language encoder. The models
are pretrained on large-scale image-text data [94] (Base or
Large) or UniCL [84] for the tiny model. During pretrain-
ing, we set a minibatch for segmentation to 32 and image-
text pairs to 1024. The image resolution is set to 1024 for
segmentation and 224 for image-text data respectively. We
follow a similar balanced sampling strategy in [84] to en-
sure the segmentation data are always observed for a con-
sistent number of epochs, regardless of the total number of
image-text pairs. Based on this, we pretrain all models for
50 epochs using AdamW [52] as the optimizer. During ﬁne-
tuning, we have task-speciﬁc designs, please refer to details
in Appendix.
4.2. Task-Speciﬁc Transfer
Without any architecture change except adding a head
for VQA, we directly ﬁnetune X-Decoder to demonstrate
its task transfer capability. Table 1 presents the comparisons
with previous specialized and generalized models.
Comparison with segmentation models.
We list
the most recent models for individual tasks, including
Mask2Former [12], Panoptic SegFormer [47], KMaX-
DeepLab [93] for generic segmentation, and LAVT [86] for
referring segmentation. Notably, our 25 epoch ﬁnetuned X-
Decoder (L) establishes a new SoTA on ADE20k dataset
that outperforms the current SoTA KMaX-DeepLab (L) on
ADE Panoptic Segmentation (our model trained with 1024
resolution achieves 51.0 PQ), as well as Instance Segmen-
tation SoTA, Mask2Former-L. On COCO, our model at-
tains comparable performance to Mask2Former and kMaX-
DeepLab. There are three reasons to explain minor infe-
riority.
First, we do not use deformable attention in X-
Decoder, which typically beneﬁts supervised settings but
hurts open-vocabulary performance. Second, we use the
language-image pretrained model as the backbone, which
can understand richer semantics but lags behind the super-
vised model for classiﬁcation tasks [42].
Third, we use
100 latent queries for segmentation, which is half of that in
Mask2Former (L). Finally, we compare with LAVT [86] on
COCO G-ref. It is worth pointing out that with lightweight
ﬁnetuning, our tiny model already outperforms LAVT-Base
(61.9 v.s. 61.2). Further increasing the model size can bring
additional gains by 2.6 and 2.7 points respectively, which
helps to set a new record on this benchmark.
Comparison with VL models. We compare with a set of
VL models on image-text retrieval, image captioning and
VQA in Table 1. X-Decoder achieves competitive perfor-
mance across the board. Speciﬁcally, X-Decoder outper-
forms strong baseline UNITER [10] and rivals VinVL [96]
on COCO retrieval, and even beats all the methods on
Flickr30k [63]. Unlike all these works, the image and text
encoders are fully decoupled in X-Decoder, which leads to
a much faster inference speed. On captioning and VQA,
6

Model
COCO (p/s) ITP Fix EM Pse-
udo
ADE-150
A-857 VOC PC-59 PC-459 SUN
SCAN-20
SCAN-41
Cityscapes
BDD
m cls cap
PQ
mAP mIoU mIoU mIoU mIoU
mIoU
mIoU mIoU
PQ
mIoU
mIoU mAP
PQ
mIoU
PQ
MSeg (B) [40]
 





33.7 32.6
19.1
⋆
73.4
43.4
⋆
29.6
33.4
⋆
⋆
46.9
24.8 51.1
44.9
⋆
GroupViT (S)







-
-
⋆
⋆
52.3
22.4
⋆
⋆
⋆
-
⋆
⋆
-
-
⋆
-
LSeg+ (B) [41]
 





-
-
18.0
3.8
⋆
46.5
7.8
⋆
⋆
-
⋆
⋆
-
-
⋆
-
ZegFormer (B) [30]
 





-
-
⋆
8.1
80.7
⋆
⋆
⋆
⋆
-
⋆
⋆
-
-
⋆
-
OpenSeg (B) [26]







-
-
21.1
6.3
70.3
45.9
9.0
⋆
⋆
-
⋆
⋆
-
-
⋆
-
OpenSeg (B) [26]







-
-
26.4
8.1
70.2
44.8
11.5
⋆
⋆
-
⋆
⋆
-
-
⋆
-
MaskCLIP (L) [17]
 





15.1
6.0
23.7
8.2
⋆
45.9
10.0
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
X-Decoder-Seg (B)
 





15.3
8.3
19.5
2.9
95.7
63.5
13.3
33.0
41.6
32.5
22.4
47.3
22.8 35.2
44.1
14.1
X-Decoder-Seg+ (B)  





16.9
9.5
23.8
4.6
97.8
64.7
12.1
32.2
35.1
33.8
18.5
47.6
25.9 36.9
42.7
16.6
X-Decoder (T)
 





18.8
9.8
25.0
6.4
96.2
62.9
12.3
34.5
37.8
30.7
21.7
47.3
16.0 37.2
42.4
16.4
X-Decoder (B)
 





21.1 11.7
27.2
8.2
97.9
65.1
14.7
39.6
40.3
35.4
24.8
50.8
22.3 39.5
45.1
17.1
X-Decoder (L)
 





21.8 13.1
29.6
9.2
97.7
64.0
16.1
43.0
49.5
39.5
29.7
52.0
24.9 38.1
47.2
17.8
Table 2. One suite of model weights for open-vocabulary image segmentation. Note: “ITP” means image-text pairs. “Fix” indicates whether contains
ﬁxed text/image encoder. “EM” means whether the model has extra modules that are designed for open-vocabulary settings (e.g. Adaptor, class agnostic
proposal, and etc.). “Pseudo” means whether the method uses an extra step to extract pseudo label image-text pairs. “gray” color means a fully supervised
approach. “light purple” color means a semi-supervised learning approach. “FL-in21k” means the backbone is pretained with in21k data using a FocalNet
backbone. For COCO, different methods use different supervisions of mask (m), class label (cls) and caption (cap). “⋆and -” follows Table 1
our models also demonstrate superior performance to their
counterparts. For example, it outperforms VinVL by 1.3
and 1.7 on CIDEr and BLEU, respectively. Note that most
of these works use sophisticatedly designed training objec-
tives, such as masked data modeling, image-text match-
ing and hard-negative mining [20, 43, 74]. In contrast, X-
Decoder is pretrained with image-text contrastive and im-
age captioning, along with the segmentation losses. The
simplicity and effectiveness imply a great potential of using
X-Decoder as a general pretraining paradigm for VL.
Comparison with generalist models.
We further com-
pare with prior arts that explore general-purpose vision
models.
Limited works report the generic segmentation
performance.
Our model outperforms UViM [38] and
Pix2Seq v2 [7] signiﬁcantly on COCO panoptic (56.7 v.s.
45.8) and instance segmentation (46.7 v.s. 38.2), respec-
tively. With the same amount of segmentation data, these
margins strongly justify our model design, i.e., unifying
functionality without any tweaks for individual tasks. When
compared with GLIPv2 [95], our model achieves compara-
ble performance. Note that GLIPv2 uses over 10M pre-
training data, including around 2M with box supervision.
Despite the huge gap in pretraining data, X-Decoder outper-
forms GLIPv2 on both captioning and VQA. Furthermore,
X-Decoder also beats other general-purpose models like
UniT [32], GPV [27], UniTAB [85] and Uniﬁed-IO [54].
Efﬁcient Finetuning. Finally, we study whether our pre-
trained X-Decoder can be ﬁnetuned for segmentation with
a low cost. In Table 3, we show that we can simply ﬁne-
tune the class embedding layer, mask embedding layer or
the whole decoder to reach a decent segmentation perfor-
mance and surpass the fully ﬁnetuned tiny SoTA models
like kMaX-DeepLab [93]. These results imply an efﬁcient
way of using our pretrained X-Decoder models.
4.3. Zero-Shot Transfer
Without any change in model weights, X-Decoder can be
directly applied to various segmentation tasks and datasets
Method
C.E. M.E Q.E Dec. #Param
ADE
Cityscapes
PQ mAP mIoU PQ mAP mIoU
Mask2Former (T) [12]
-
-
-
-
-
39.7 26.4
47.7 63.9 39.1
80.5
Pano/SegFormer (T) [47,80]
-
-
-
-
-
36.4
⋆
46.5
⋆
⋆
⋆
kMaX-DeepLab (T) [93]
-
-
-
-
-
41.5
⋆
45.0 64.3 38.5
79.7
Mask2Former (S) [12]
-
-
-
-
-
⋆
⋆
51.3 64.8 40.7
81.8
Mask2Former (B) [12]
-
-
-
-
-
⋆
⋆
53.9 66.1 42.8
82.7
Mask2Former (L) [12]
-
-
-
-
-
48.1 34.9
56.1 66.6 43.6
82.9




0.26M 44.3 33.2
54.6 65.1 41.4
81.7




1.05M 43.9 33.2
53.9 64.8 41.2
81.2




1.15M 44.0 32.8
54.0 64.6 41.1
81.5
X-Decoder (L)




38.3M 47.0 35.1
56.0 65.6 42.2
81.7
Table 3. Performance with different efﬁcient ﬁnetuning strategies for X-
Decoder large, and comparisons with fully-ﬁnetuned models.
after pretraining.
In Table 2, we evaluate our model in
a zero-shot manner on seven commonly used segmenta-
tion datasets in 10 different settings from diverse domains,
including common indoor (e.g., ADE20K [98] and Pas-
cal [22]), outdoor (e.g., Cityscapes [14]) and self-driving
scenarios (e.g., BDD [90]). We report PQ, mAP and mIoU
for panoptic, instance and semantic segmentation respec-
tively. And we visualize the predicted open-vocabulary seg-
mentation result on each dataset in Fig. 5.
Comparison with baselines.
We build two X-Decoder
variants: (1) X-Decoder-Seg, which is only trained with
COCO panoptic segmentation using a text encoder for class
names; and (2) X-Decoder-Seg+, where we take the heuris-
tic way to extract noun phrases from COCO captions and
use them as extra supervision on top of the matched decoder
outputs. First, X-Decoder-Seg shows clear advantages on
open-vocabulary segmentation over MSeg [40], that manu-
ally conducts label mapping across different datasets. Sec-
ond, the extra supervision from COCO captions improves
model performance on 9 out of 15 metrics, which indicates
the beneﬁt of joint learning with image-level supervision.
Third, when pretraining with the full X-Decoder, the per-
formance is signiﬁcantly boosted. Notably, the mIoU met-
ric is improved by 7.4, 3.4 and 2.6 on SUN, ADE-150 and
PC-459, respectively.
Comparison with state-of-the-art. We further compare
with the most advanced methods for open-vocabulary im-
age segmentation in Table 2. Clearly, our models achieve
7

Figure 5. Visualization of zero-shot semantic segmentation on 10 settings of 7 datasets.
Model
COCO
ADE
COCO-Karparthy
g-Ref
PQ
mAP mIoU PQ
mAP mIoU IR@1 IR@1 CIDEr cIoU
X-Decoder
51.4 40.5 62.8
14.7 9.6
23.4
30.7
48.5
82.0
59.7
* text: [yny]
51.4 39.8 61.7
14.7 9.4
22.2
29.9
46.9
78.6
57.7
* text: [nyy]
51.4 38.6 61.7
15.2 9.4
23.1
30.3
47.5
78.9
59.4
* latent: [yyn] 50.9 39.6 62.0
15.5 9.4
22.8
29.8
47.6
81.1
57.6
Table 4. Ablation of query interaction in X-Decoder. [x,x,x] denotes
whether attend [object latent query, image latent query, text query]
COCO
ADE
COCO-Karparthy
g-Ref
Model
PQ
mAP mIoU PQ
mAP mIoU IR@1 TR@1 CIDEr cIoU
* bs 1024 50.9 39.5 62.4
15.2 10.0 24.6
30.6
48.1
85.0
58.0
* bs 768
51.0 39.5 62.4
15.4 10.0 24.2
29.0
46.8
78.6
58.8
* bs 512
50.7 39.3 62.0
14.9 9.7
24.3
27.4
43.8
76.1
58.6
Table 5. Ablation of VL batch size. We mark the signiﬁcant drop metrics
in green.
the best results across all datasets. Among the base-sized
models, X-Decoder (B) outperforms OpenSeg (B) [25] on
two challenging datasets, ADE-150 and PC-459 for se-
mantic segmentation. Scaling X-Decoder to large size fur-
ther improves mIoU by 2.4 and 1.4 on these two datasets.
Among prior arts, MaskCLIP [17] is the ﬁrst proposed
for open-vocabulary panoptic segmentation by combining
Mask2Former with CLIP models. With COCO caption su-
pervisions, our simple baseline X-Decoder-Seg+ already
performs comparably. The full version of our tiny model X-
Decoder (T) surpasses MaskCLIP across the board except
A-847. We note that these comparisons are not strictly fair
in terms of supervision, settings and models used. How-
ever, these results demonstrate the effectiveness of our X-
Decoder to learn from the different granularity of supervi-
sions end-to-end for open-vocabulary segmentation, which
leads to new SoTA on 10 settings of 7 datasets across three
segmentation tasks.
4.4. Model Inspection
Pretraining Tasks. By default, we exploit four pretraining
tasks including generic and referring segmentation, caption-
ing and retrieval. In Table 6, we keep the generic segmenta-
tion while ablating the importance of the other pretraining
tasks. Accordingly, we have the following observations:
• Image-text retrieval can help open-vocabulary seg-
mentation. On ADE, the mIoU drops from 23.4 to 21.8,
and PQ drops 0.7 without image-text retrieval. Since we
share the same semantic space for both tasks, a good
visual-semantic alignment learned from the retrieval task
can directly beneﬁt the recognition of novel concepts.
• Image captioning helps referring segmentation and
vice versa. We observe a drop of 2.0 pts on COCO g-
Ref without captioning task, and a 3.2 pts drop of CIDEr
from removing referring task. The two tasks share the
same text encoder for text queries. Joint training, there-
fore, improves the understanding of text inputs.
• Image captioning and retrieval can mutually beneﬁt
each other. When removing captioning during pretrain-
ing, the image retrieval R@1 drops by 0.8, and the cap-
tioning CIDEr drops signiﬁcantly by 3.2 pts from remov-
ing retrieval task. Our X-Decoder promotes harmony of
generative and contrastive learning.
The above observations verify that the uniﬁed design of X-
Decoder can prompt the synergy of different tasks.
Query Interactions.
The interaction among tasks is
highly dependent on the interaction between latent and text
queries. We have described how the queries interact with
each other by default in Fig. 4. Here, we investigate how
our model behaves with different interactions. In Table 4,
we show the performance across tasks with ablated versions
and have the following takeaways:
• Image captioning requires both ﬁne-grained and
global image information. Comparing the ﬁrst with the
second and third row in the table, we ﬁnd the CIDEr
score signiﬁcantly drops if we cut off the information ﬂow
from the global latent query or other latent queries to text
queries (82.0 →78.6 and 78.9, respectively).
• Language-condition is important for referring seg-
mentation. In the last row, we turn off the interaction
from text queries to latent queries. This signiﬁcantly hurts
referring segmentation (59.7→57.6). On the one hand,
this indicates that we can convert generic segmentation to
8

COCO
ADE
COCO-Karparthy
g-Ref
Model
PQ
mAP mIoU PQ
mAP mIoU IR@1 TR@1 CIDEr cIoU
X-Decoder
51.4 40.5 62.8
14.7 9.6
23.4
30.7
48.5
82.0
59.7
- Retrieval
51.4 40.4 62.6
14.0 9.2
21.8
n/a
n/a
78.8
59.2
- Captioning 51.1 40.4 63.2
15.0 9.6
23.2
29.9
48.1
n/a
57.7
- Referring
51.1 39.7 62.3
15.2 8.9
22.6
30.0
47.6
78.8
n/a
Table 6. Ablation of pretraining tasks by removing one at a time. We bold
the best entry and underline the worst entry in each column.
Model
COCO
ADE
COCO-Karparthy
g-Ref
PQ
mAP mIoU PQ
mAP mIoU IR@1 TR@1 CIDEr cIoU
Full Datasets 50.9 39.5
62.4 15.2 10.0
24.6
30.6
48.1
85.0
58.0
- coco
50.9 39.9
62.2 15.3
9.8
24.4
27.4
38.2
32.6
59.4
- cc3m
51.2 39.7
62.6 15.5 10.1
24.6
31.0
50.0
81.2
58.3
- vg
51.1 39.8
62.4 14.6
9.7
23.8
36.1
56.1
107.1
58.3
- sbu
51.1 39.8
62.4 15.3
9.5
24.6
30.3
48.3
81.2
58.3
Table 7. Ablation of VL datasets in X-Decoder. A single VL dataset is re-
moved in each row. And we mark the metrics that signiﬁcantly drop/increase
in green/red.
Figure 6. Top: Region Retrieval. Bottom: Referring Captioning (Left), Referring Image Editing (right). Please refer to more results in Appendix.
referring segmentation using post-hoc matching with re-
ferring texts. On the other hand, sending the text phrase
as input to X-Decoder is essential to modulate our model
to speciﬁcally decode the targets.
VL Batch Size & Dataset The default batch size of VL task
is 1024, here we explore the gradual decreasing of VL batch
size. In addition, each VL dataset is removed individually to
investigate the pre-trained performance on different tasks.
• Decreasing VL batch size hurts VL tasks and open-
vocab Segmentation performance.
As shown in Ta-
ble. 5, decreasing the VL task batch size from 1024 to 256
signiﬁcantly hurts the retrieval and captioning tasks’ per-
formance, where ir@1, tr@1, CIDEr decrease by 3.2, 4.3,
and 8.9 points respectively. Further, the open-vocabulary
performance also drops 0.3 points on each metric.
• VG dataset hurts pretraining VL tasks performance
but improves open-vocab segmentation. As shown in
Table 7, removing the visual genome from the pretraining
VL dataset signiﬁcantly improves captioning task with
22.1 points during pretraining, but only 0.2 points af-
ter ﬁnetuning. Moreover, open-vocabulary semantic seg-
mentation drops around 0.8 points.
4.5. Task Composition
X-Decoder has the unique beneﬁt of task interaction,
thanks to the sophisticated architecture design on latent and
text queries as well as the decoder architecture. It enables
joint task inference and iterative task inference with a sin-
gle set of weights.
In Fig. 6, we show our model can
perform region-based retrieval and referring based caption-
ing without any architecture/weight change. For example,
given a set of animal images (row 1, Fig. 6) and text query,
our model ﬁrst retrieves the correct image (ﬂamingo and
giraffe) and then grounds the query with pixel-level pre-
dictions. Further, our model can easily adapted to refer-
ring captioning by ﬁrst localizing a given word and then
modulating the predicted mask in the cross-attention layers.
Lastly, we also integrate X-Deocder with diffusion model
to do referring image editing demonstrated in the latter half
of the second row in Fig. 6.
5. Conclusion
We present X-Decoder, a model that seamlessly supports
pixel-level and image-level vision-language understanding.
With a simple and generalized design, X-Decoder can unite
and support generic segmentation, referring segmentation
and VL tasks effortlessly, achieving strong generalizability
and competitive or even SoTA performance. We hope this
work can shed a light on the design of the next-generation
general-purpose vision system.
Acknowledgements. We appreciated the constructive dis-
cussion with Haotian Zhang.
This work was also sup-
ported in part by NSF CAREER IIS2150012, the Wisconsin
Alumni Research Foundation, and the Institute of Informa-
tion & communications Technology Planning & Evaluation
(IITP) grant funded by the Korea government (MSIT) (No.
2022- 0-00871, Development of AI Autonomy and Knowl-
9

edge Enhancement for AI Agent Collaboration).
References
[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien
Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning
and visual question answering. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pages 6077–6086, 2018.
[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi
Parikh. Vqa: Visual question answering. In ICCV, 2015.
[3] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.
Yolact: Real-time instance segmentation. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 9157–9166, 2019.
[4] Zhaowei Cai, Gukyeong Kwon, Avinash Ravichandran, Er-
han Bas, Zhuowen Tu, Rahul Bhotika, and Stefano Soatto.
X-detr: A versatile architecture for instance-wise vision-
language tasks. arXiv preprint arXiv:2204.05626, 2022.
[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-
las Usunier, Alexander Kirillov, and Sergey Zagoruyko.
End-to-end object detection with transformers. In European
Conference on Computer Vision, pages 213–229. Springer,
2020.
[6] Liang-Chieh Chen, George Papandreou, Florian Schroff,
and Hartwig Adam.
Rethinking atrous convolution
for
semantic
image
segmentation.
arXiv
preprint
arXiv:1706.05587, 2017.
[7] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J
Fleet, and Geoffrey Hinton. A uniﬁed sequence interface
for vision tasks. arXiv preprint arXiv:2206.07669, 2022.
[8] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna
Vedantam, Saurabh Gupta, Piotr Doll´ar, and C. Lawrence
Zitnick. Microsoft COCO captions: Data collection and
evaluation server. arXiv preprint arXiv:1504.00325, 2015.
[9] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna
Vedantam, Saurabh Gupta, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco captions: Data collection and eval-
uation server. arXiv preprint arXiv:1504.00325, 2015.
[10] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,
Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.
UNITER: Universal image-text representation learning. In
ECCV, 2020.
[11] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong
Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for
dense predictions. arXiv preprint arXiv:2205.08534, 2022.
[12] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar.
Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 1290–1299, 2022.
[13] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying
vision-and-language tasks via text generation. In Interna-
tional Conference on Machine Learning, pages 1931–1942.
PMLR, 2021.
[14] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Scharw¨achter, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele.
The cityscapes
dataset. In CVPR Workshop on the Future of Datasets in
Vision, volume 2. sn, 2015.
[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In 2009 IEEE conference on computer vision
and pattern recognition, pages 248–255. Ieee, 2009.
[16] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong
Wang, and Lu Yuan. Davit: Dual attention vision trans-
formers. arXiv preprint arXiv:2204.03645, 2022.
[17] Zheng Ding, Jieke Wang, and Zhuowen Tu.
Open-
vocabulary panoptic segmentation with maskclip.
arXiv
preprint arXiv:2208.08984, 2022.
[18] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang,
and Yichen Wei.
Solq: Segmenting objects by learning
queries. Advances in Neural Information Processing Sys-
tems, 34:21898–21909, 2021.
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold,
Sylvain Gelly, et al.
An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
[20] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan
Zhang, Jianfeng Wang, Linjie Li, Zicheng Liu, Ce Liu,
Yann LeCun, Nanyun Peng, Jianfeng Gao, and Lijuan
Wang. Coarse-to-ﬁne vision-language pre-training with fu-
sion in the backbone. In NeurIPS, 2022.
[21] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuo-
hang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan
Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and Michael
Zeng. An empirical study of training end-to-end vision-
and-language transformers. In CVPR, 2022.
[22] Mark Everingham and John Winn. The pascal visual object
classes challenge 2012 (voc2012) development kit. Pattern
Analysis, Statistical Modelling and Computational Learn-
ing, Tech. Rep, 8(5), 2011.
[23] King-Sun Fu and JK Mui. A survey on image segmentation.
Pattern recognition, 13(1):3–16, 1981.
[24] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng
Liu, and Jianfeng Gao. Vision-language pre-training: Ba-
sics, recent advances, and future trends.
arXiv preprint
arXiv:2210.09263, 2022.
[25] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin.
Open-vocabulary image segmentation.
arXiv preprint
arXiv:2112.12143, 2021.
[26] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. arXiv preprint arXiv:2104.13921,
2021.
[27] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and
Derek Hoiem.
Towards general purpose vision systems:
An end-to-end task-agnostic vision-language architecture.
In CVPR, 2022.
[28] Abdul Mueed Haﬁz and Ghulam Mohiuddin Bhat. A survey
on instance segmentation: state of the art. International
10

journal of multimedia information retrieval, 9(3):171–189,
2020.
[29] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE interna-
tional conference on computer vision, pages 2961–2969,
2017.
[30] Lisa Anne Hendricks, John Mellor, Rosalia Schneider,
Jean-Baptiste Alayrac, and Aida Nematzadeh. Decoupling
the role of data, attention, and losses in multimodal trans-
formers. TACL, 2021.
[31] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Seg-
mentation from natural language expressions. In European
Conference on Computer Vision, pages 108–124. Springer,
2016.
[32] Ronghang Hu and Amanpreet Singh.
Unit: Multimodal
multitask learning with a uniﬁed transformer. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision, pages 1439–1449, 2021.
[33] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan
Elhamifar. Open-vocabulary instance segmentation via ro-
bust cross-modal pseudo-labeling. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7020–7031, 2022.
[34] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li,
and Tom Duerig.
Scaling up visual and vision-language
representation learning with noisy text supervision.
In
ICML, 2021.
[35] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion.
Mdetr-
modulated detection for end-to-end multi-modal under-
standing. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 1780–1790, 2021.
[36] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-
and-language transformer without convolution or region su-
pervision. In ICML, 2021.
[37] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll´ar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9404–9413, 2019.
[38] Alexander Kolesnikov, Andr´e Susano Pinto, Lucas Beyer,
Xiaohua Zhai, Jeremiah Harmsen, and Neil Houlsby.
UViM: A uniﬁed modeling approach for vision with learned
guiding codes. arXiv preprint arXiv:2205.10337, 2022.
[39] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vi-
sion, 123(1):32–73, 2017.
[40] John Lambert, Zhuang Liu, Ozan Sener, James Hays,
and Vladlen Koltun.
Mseg:
A composite dataset for
multi-domain semantic segmentation.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 2879–2888, 2020.
[41] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Ren´e Ranftl. Language-driven semantic seg-
mentation. arXiv preprint arXiv:2201.03546, 2022.
[42] Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan
Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Yong Jae Lee,
Houdong Hu, Zicheng Liu, et al. Elevater: A benchmark
and toolkit for evaluating language-augmented visual mod-
els. arXiv preprint arXiv:2204.08790, 2022.
[43] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak
Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Hoi.
Align before fuse:
Vision and language representation
learning with momentum distillation. In NeurIPS, 2021.
[44] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,
and Kai-Wei Chang. VisualBERT: A simple and performant
baseline for vision and language. arXiv preprint, 2019.
[45] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al.
Grounded
language-image pre-training.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10965–10975, 2022.
[46] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xi-
aowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,
Furu Wei, et al.
Oscar: Object-semantics aligned pre-
training for vision-language tasks. In ECCV, 2020.
[47] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima
Anandkumar, Jose M Alvarez, Ping Luo, and Tong Lu.
Panoptic segformer: Delving deeper into panoptic segmen-
tation with transformers. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 1280–1289, 2022.
[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and
C Lawrence Zitnick. Microsoft coco: Common objects in
context. In ECCV, 2014.
[49] Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,
and Alan Yuille. Recurrent multimodal interaction for re-
ferring image segmentation. In Proceedings of the IEEE
International Conference on Computer Vision, pages 1271–
1280, 2017.
[50] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 10012–10022, 2021.
[51] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. In Pro-
ceedings of the IEEE conference on computer vision and
pattern recognition, pages 3431–3440, 2015.
[52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017.
[53] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViL-
BERT: Pretraining task-agnostic visiolinguistic representa-
tions for vision-and-language tasks. In NeurIPS, 2019.
[54] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh
Mottaghi, and Aniruddha Kembhavi. Uniﬁed-io: A uniﬁed
model for vision, language, and multi-modal tasks. arXiv
preprint arXiv:2206.08916, 2022.
[55] Timo L¨uddecke and Alexander Ecker.
Image segmenta-
tion using text and image prompts. In Proceedings of the
11

IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7086–7096, 2022.
[56] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L Yuille, and Kevin Murphy. Generation
and comprehension of unambiguous object descriptions. In
Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 11–20, 2016.
[57] Edgar Margffoy-Tuay, Juan C P´erez, Emilio Botero, and
Pablo Arbel´aez. Dynamic multimodal instance segmenta-
tion guided by natural language queries. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 630–645, 2018.
[58] Matthias Minderer,
Alexey Gritsenko,
Austin Stone,
Maxim Neumann, Dirk Weissenborn, Alexey Dosovit-
skiy, Aravindh Mahendran, Anurag Arnab, Mostafa De-
hghani, Zhuoran Shen, et al. Simple open-vocabulary ob-
ject detection with vision transformers.
arXiv preprint
arXiv:2205.06230, 2022.
[59] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu
Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and
Alan Yuille. The role of context for object detection and
semantic segmentation in the wild. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 891–898, 2014.
[60] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis.
Modeling context between objects for referring expression
understanding. In European Conference on Computer Vi-
sion, pages 792–807. Springer, 2016.
[61] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
Im2text: Describing images using 1 million captioned pho-
tographs. Advances in neural information processing sys-
tems, 24, 2011.
[62] Nikhil R Pal and Sankar K Pal. A review on image segmen-
tation techniques. Pattern recognition, 26(9):1277–1294,
1993.
[63] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
nik. Flickr30k entities: Collecting region-to-phrase corre-
spondences for richer image-to-sentence models. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 2641–2649, 2015.
[64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021.
[65] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen
Lu.
Denseclip: Language-guided dense prediction with
context-aware prompting. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 18082–18091, 2022.
[66] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684–10695, 2022.
[67] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages
2556–2565, 2018.
[68] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-
laume Couairon, Wojciech Galuba, Marcus Rohrbach, and
Douwe Kiela. Flava: A foundational language and vision
alignment model. In CVPR, 2022.
[69] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu
Wei, and Jifeng Dai.
VL-BERT: Pre-training of generic
visual-linguistic representations. In ICLR, 2019.
[70] Hao Tan and Mohit Bansal.
LXMERT: Learning cross-
modality encoder representations from transformers.
In
EMNLP, 2019.
[71] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional con-
volutions for instance segmentation. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part I 16, pages 282–298.
Springer, 2020.
[72] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin.
Attention is all you need.
In Ad-
vances in neural information processing systems, pages
5998–6008, 2017.
[73] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and
Liang-Chieh Chen.
Max-deeplab: End-to-end panoptic
segmentation with mask transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5463–5474, 2021.
[74] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang,
Xiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang.
Ufo: A uniﬁed transformer for vision-language representa-
tion learning. arXiv preprint arXiv:2111.10023, 2021.
[75] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li,
Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan
Wang. Git: A generative image-to-text transformer for vi-
sion and language. arXiv preprint arXiv:2205.14100, 2022.
[76] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang.
Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. In ICML, 2022.
[77] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Unifying architectures, tasks, and modali-
ties through a simple sequence-to-sequence learning frame-
work. arXiv preprint arXiv:2202.03052, 2022.
[78] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
hammed, Saksham Singhal, Subhojit Som, et al. Image as a
foreign language: Beit pretraining for all vision and vision-
language tasks. arXiv preprint arXiv:2208.10442, 2022.
[79] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia
Tsvetkov, and Yuan Cao. Simvlm: Simple visual language
model pretraining with weak supervision. arXiv preprint
arXiv:2108.10904, 2021.
12

[80] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,
Jose M Alvarez, and Ping Luo. Segformer: Simple and ef-
ﬁcient design for semantic segmentation with transformers.
arXiv preprint arXiv:2105.15203, 2021.
[81] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:
Semantic segmentation emerges from text supervision. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 18134–18144, 2022.
[82] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue,
Yuchen Liang,
Jianchao Yang,
and Thomas Huang.
Youtube-vos:
A large-scale video object segmentation
benchmark. arXiv preprint arXiv:1809.03327, 2018.
[83] Jianwei Yang, Chunyuan Li, and Jianfeng Gao. Focal mod-
ulation networks. arXiv preprint arXiv:2203.11926, 2022.
[84] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao,
Ce Liu, Lu Yuan, and Jianfeng Gao. Uniﬁed contrastive
learning in image-text-label space. In CVPR, 2022.
[85] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,
Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.
Unitab: Unifying text and box outputs for grounded vision-
language modeling. In European Conference on Computer
Vision, pages 521–539. Springer, 2022.
[86] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-
shuang Zhao, and Philip HS Torr. Lavt: Language-aware
vision transformer for referring image segmentation.
In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 18155–18165, 2022.
[87] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang,
Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang
Xu.
Detclip: Dictionary-enriched visual-concept paral-
leled pre-training for open-world detection. arXiv preprint
arXiv:2209.09407, 2022.
[88] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.
Cross-modal self-attention network for referring image seg-
mentation.
In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 10502–
10511, 2019.
[89] Xuwang Yin and Vicente Ordonez. Obj2text: Generating
visually descriptive language from object layouts.
arXiv
preprint arXiv:1707.07102, 2017.
[90] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu,
Mike Liao,
Vashisht Madhavan,
and Trevor Darrell.
Bdd100k:
A diverse driving video database with scal-
able annotation tooling. arXiv preprint arXiv:1805.04687,
2(5):6, 2018.
[91] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung,
Mojtaba Seyedhosseini, and Yonghui Wu.
Coca: Con-
trastive captioners are image-text foundation models. arXiv
preprint arXiv:2205.01917, 2022.
[92] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-
sions. In European Conference on Computer Vision, pages
69–85. Springer, 2016.
[93] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,
Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh
Chen. k-means mask transformer. In European Conference
on Computer Vision, pages 288–307. Springer, 2022.
[94] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng
Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang,
Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Lu-
owei Zhou, and Pengchuan Zhang.
Florence:
A new
foundation model for computer vision.
arXiv preprint
arXiv:2111.11432, 2021.
[95] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun
Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu
Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
Glipv2:
Unifying localization and vision-language understanding.
arXiv preprint arXiv:2206.05836, 2022.
[96] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang,
Lei Zhang,
Lijuan Wang,
Yejin Choi,
and Jianfeng
Gao. VinVL: Revisiting visual representations in vision-
language models. In CVPR, 2021.
[97] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-
yuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,
Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-
based language-image pretraining. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16793–16803, 2022.
[98] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba.
Scene parsing through
ade20k dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 633–641,
2017.
[99] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang, and Jifeng Dai. Deformable detr: Deformable trans-
formers for end-to-end object detection.
arXiv preprint
arXiv:2010.04159, 2020.
[100] Xueyan Zou, Haotian Liu, and Yong Jae Lee. End-to-end
instance edge detection. arXiv preprint arXiv:2204.02898,
2022.
13

A. Experiment Settings
A.1. Pretraining
In the main paper, all the pre-trained models are trained
with 50 epochs of COCO data and roughly 45 epochs of 10
million image-text pairs. The batch size of COCO images
and image text pairs are 32 and 1024 respectively. And 32
GPUs are used for pretraining. The AdamW optimizer is
used in pretraining with the initial learning rate 1e-4. A
step-wise scheduler is used to decay the learning rate by 0.1
on the fraction [0.88889, 0.96296] of training steps.
A.2. Finetuning
Image-Text Retrieval. For both COCO and Flickr30k
image-text retrieval, we ﬁnetune the models for 10 epochs
using AdamW as the optimizer. We set the image resolu-
tion to 384 and the batch size to 2048. The learning rates
are 3e-5 for the X-Decoder part and 3e-6 for the vision and
language backbones.
Image Captioning. Similar to image-text retrieval, we
ﬁnetune the captioning models for 10 epochs using AdamW
as the optimizer. We set the image resolution to 480 and
the batch size to 256. The learning rates are 2e-5 for the
X-Decoder part and 2e-6 for the vision and language back-
bones. We use beam search during caption generation with
the beam size set to 5. We do not use CIDEr optimization
for our captioning models.
VQA. For VQA, we add a new classiﬁcation layer on
the top of the model and ﬁnetune the models for 10 epochs
using AdamW as the optimizer. We set the image resolution
to 640 and the batch size to 256. The learning rates are 1e-
4 for the X-Decoder part, 1e-5 for the vision and language
backbones, and 1e-3 for the VQA classiﬁcation layer.
Generic Segmentation. For generic segmentation, we
ﬁnetune the pretrained checkpoint with 24 epochs with start
learning rate 1e-4. We decay the learning rate by factor 10
at epoch 21 and 23, respectively. The batch size of ADE20k
is 64, and 32 for COCO.
Referring Segmentation. For referring segmentation,
we also ﬁnetune the pretrained checkpoint with 24 epochs.
However, as RefCOCO has been used in pretraining, thus
the initial learning rate is 1e-5. It also decays twice at 21
and 23 epochs. We use a batch size of 64 during training.
Further, in addition to the normal setting that multiple back-
bone and language encoder learning rates with 0.1, here we
also multiply the transformer encoder learning rate by 0.1.
B. Open-Vocab Segmentation Benchmark
We propose an open vocabulary segmentation bench-
mark on 9 datasets with different evaluation metrics. The
goal of this benchmark is to provide a comprehensive and
standard evaluation protocol for open-vocabulary segmen-
tation on different vocabulary sizes and image domains.
Dataset
Scene
Annotation Format # Images # Classes
Sem Inst
Pano
ADE-150
common



2000
150
ADE-847
common



2000
847
Pascal Voc
common



1449
20
Pascal Context-59 common



5105
59
Pascal Context-459 common



5105
459
SUN RGB-D
in-door



5050
37
ScanNet-20
in-door



5436
20
ScanNet-41
in-door



5436
41
Cityscapes
driving



500
19/8/19
BDD
driving



1000
19//40
Table 8. Open-Vocabulary Segmentation Benchmark Statistics.
Table 8 shows the dataset statistics in the benchmark. It
supports all generic segmentation tasks including seman-
tic/instance/panoptic segmentation. It covers a variety of
scopes ranging from 20 to 847 classes.
In addition, the
evaluation scene includes common objects, in-door scenes
as well as autonomous driving scenarios. To enable a bet-
ter understanding of the open-vocabulary ability on the
training/evaluation datasets. We evaluate the coverage of
training datasets captions and evaluation datasets concepts
in Fig. 10-16 (we split the caption into single words and
phrases to ﬁnd mappings in categories). The major results
of the open-vocabulary segmentation are evaluated in the
main paper, Tab. 2.
Method
COCO Karpathy
VQAv2
IR@1
TR@1
CIDEr
BLEU
test-dev
X-Decoder (T)
49.3
66.7
122.3
37.8
70.6
X-Decoder-VL (T)
44.3 ↓5.0
60.3 ↓6.4
113.2 ↓9.1
34.8 ↓3.0
69.4 ↓1.2
Table 9. Compare ﬁnetuning result between X-Decoder and X-Decoder-
VL which merely uses 4M image-text pairs for pretraining.
C. Extra Ablation Studies
C.1. Complementariness between Vision and VL
In our main paper,
we observed that the vision-
language pretraining objectives including image-text con-
trastive learning and image captioning have clear beneﬁts
to image segmentation, particularly in the zero-shot setting.
Here, we further study the role of segmentation objectives in
vision-language understanding. To investigate, we remove
the segmentation data (COCO panoptic segmentation and
referring segmentation) and only pretrain X-Decoder on the
four million image-text pairs, denoted by X-Decoder-VL.
Afterwards, we transfer the model to downstream VL tasks.
As we can see from Table 9, the performance signiﬁcantly
drops across all tasks after removing the segmentation data
for pretraining. We suspect that segmentation data can help
models to learn more ﬁne-grained visual understanding and
consequently beneﬁt vision-language tasks. Along with our
ﬁndings in the main paper, we conclude that pixel-level seg-
mentation and vision-language learning are complemen-
tary to each other for zero-shot and task-speciﬁc transfer.
14

Method
Backbone
Deformable Attn.
Generic Segmentation
Referring
Retrieval
Captioning
COCO
ADE (open)
g-Ref
COCO-Karpathy
COCO-Karpathy
PQ
mAP
mIoU
PQ
mAP
mIoU
cIoU
IR@1
TR@1
CIDEr
BLEU
X-Decoder (T)
Swin

50.2
38.8
61.9
17.3
9.4
23.7
55.3
28.0
43.7
79.9
24.2
X-Decoder (T)
Swin

52.3
42.7
64.5
17.0
9.3
22.1
59.1
28.1
43.1
87.2
26.9
X-Decoder (T)
Focal

51.4
40.5
62.8
18.8
9.8
25.0
59.8
30.7
48.5
79.9
24.2
X-Decoder (T)
Davit

51.0
39.7
62.4
17.3
9.4
23.6
58.4
31.4
48.8
86.8
26.0
X-Decoder (L)
Davit

56.9
46.7
67.7
21.8
13.1
29.6
64.2
44.7
60.3
111.0
32.6
X-Decoder (L)
Davit

57.4
48.0
69.7
19.1
12.6
26.6
65.1
46.2
61.8
108.2
30.1
Table 10. Model architecture inspection among Swin [50], FocalNet [83] and DaViT [16]. “Deformable Attn.” means multi-scale deformable attention [99]
that is used in Mask2Former [12]. All numbers are reported in zero-shot manner without any task-speciﬁc ﬁnetuning, and the row colored in gray corresponds
to the architecture used the main paper.
Model
COCO (p/s) ITP
ADE-150
VOC PC-59 PC-459
SUN
SCAN-20
SCAN-41
Cityscapes
BDD
m cls cap
PQ
mAP mIoU mIoU mIoU
mIoU
mIoU mIoU
PQ
mIoU
mIoU mAP
PQ
mIoU
PQ
X-Decoder-Seg (T)




13.7
6.3
18.0
89.3
59.3
11.5
16.3
8.6
16.3
6.4
46.6
14.9
30.2
36.9
13.0
X-Decoder-Seg+(T)




15.0
7.8
21.3
93.1
61.7
10.4
28.7
30.7
30.8
17.1
48.2
16.7
37.1
40.0
13.5
X-Decoder (T)




16.6
8.3
22.3
94.4
57.6
11.9
33.1
39.7
26.4
21.9
51.0
15.6
35.5
45.0
14.4
X-Decoder (T)




18.8
9.8
25.0
96.2
62.9
12.3
34.5
37.8
30.7
21.7
47.3
16.0
37.2
42.4
16.4
X-Decoder (L-IN21K) 



19.9
11.7
29.6
95.8
54.2
20.5
42.4
44.9
29.5
27.4
47.2
18.3
33.3
44.9
15.2
X-Decoder (L)




21.8
13.1
29.6
97.7
64.0
16.1
43.0
49.5
39.5
29.7
52.0
24.9
38.1
47.2
17.8
Table 11. More open-vocabulary segmentation results. We report the results for our X-Decoder pretrained with COCO segmentation and caption annotations
only in 3rd row. Additionally, we compare the model initialized with two different pre-trained large vision backbones, FocalNet-Large and DaViT-d5 trained
on ImageNet-21K (row 5) and hundreds of millions of image-text pairs (row 6), respectively.
C.2. Model Architecture Inspection
In Table. 10, we report the results using three different
vision backbone architectures, including Swin [50], Focal-
Net [83] and DaViT [16]. All models in the ﬁrst block are
with tiny size and trained on the combination of image-label
and image-text pairs, following the settings in UniCL [84].
In the second block, all the models are initialized with Flo-
rence [94] pre-trained DaVit-d5 model. Through the com-
parisons, we have the following observations: (1) FocalNet
and DaViT achieve better performance than Swin across
all metrics. Particularly, FocalNet achieves the best perfor-
mance on generic and referring segmentation, while DaViT
is better on the zero-shot vision-language evaluations; (2)
After adding the deformable attention, we can see a boost
on supervised segmentation but signiﬁcant (especially large
model) degradation on the open-vocabulary segmentation
on ADE20K dataset. Based on these experimental results,
we make the design choices as mentioned in our main sub-
mission: (1) we remove deformable attention in the favor of
open-vocabulary segmentation; (2) we use FocalNet as the
tiny vision encoder and train it by ourselves using UniCL,
while using DaViT [94] as the base and large vision en-
coder.
C.3. Open-Vocabulary Generic Segmentation Set-
tings Inspection
In Tab. 11, we study the progressive enrichment of data
and training settings as well as the pre-trained model us-
age. X-Decoder-Seg is the baseline of adding a text en-
coder to Mask2Former [12] with a learnable language en-
coder.
X-Decoder-Seg+ takes use of caption nouns for
Hungarian matching to enrich the vocabulary size. In ad-
dition to the main paper, we add row 3 in Tab. 11 to
demonstrate the performance of X-Decoder with only coco
image text pairs.
Comparing 3rd row and 4th row, we
ﬁnd adding extra image-text pairs for pretraining clearl
improve open-vocabulary segmentation performance espe-
cially when the vocabulary size is large (e.g. ADE-150,
CONTEXT-59/459). The way of pretraining vision back-
bone also matters. Comparing the last two rows side by
side, though the backbone model sizes are similar, using
ImageNet-21K for pretraining leads to inferior performance
on most of the datasets except for CONTEXT-459 which
contains most number of categories. These results demon-
strate the beneﬁts of using more image-text pairs for pre-
training the vision backbone or our X-Decoder.
D. Segmentation In the Wild Benchmark
As shown in the main submission, our X-Decoder ex-
hibits a strong generalization ability to segment images in
ten settings of seven datasets from different domains, with-
out any dataset-speciﬁc ﬁnetuning.
Inspired by the ob-
ject detection in the wild setting proposed in GLIP [45],
we resort to more domain-speciﬁc datasets on the web
to further examine the generality of our model. Speciﬁ-
cally, we download 55 instance segmentation datasets from
Roboﬂow 3. Afterward, we clean the datasets by exclud-
ing those containing visually undetectable categories (e.g.
Different species of plant) or categories labeled with other
languages. In the end, we compile 25 datasets that are suit-
able for evaluation into segmentation in the wild (SegInW)
benchmark and report instance segmentation mAP. The
3https://roboflow.com/
15

Dataset
Categories
# Class # Images
URL
Train Val
Phones
[phone]
1
25
11
https://universe.roboflow.com/workspace-c4esq/phone-xccez/dataset/1
Elephants
[elephant]
1
883
99
https://universe.roboflow.com/ds/4YwrXd1bFy?key=Q5GY9ITu14
Hand-Metal
[hand, metal]
2
504
65
https://universe.roboflow.com/nk950357-gmail-com/lab-k8hyn
Watermelon
[watermelon]
1
65
23
https://universe.roboflow.com/gnous-b5xq6/my_project-38aqt/dataset/4
House-Parts
[aluminium door, aluminium window, ...]
22
700
201 https://universe.roboflow.com/testcoco/abc-fqun0/dataset/1
HouseHold-Items [bottle, mouse, perfume, phone]
4
45
3
https://universe.roboflow.com/maths/household-items-sltdd/dataset/2
Strawberry
[R strawberry, people]
2
971
87
https://universe.roboflow.com/strawberry-25w7z/strawberry_coco_1/dataset/1
Fruits
[apple, lemon, orange, pear, strawberry]
5
120
9
https://universe.roboflow.com/ds/PEVo9xHLFl?key=PXeJGF0D5q
Nutterﬂy-Squireel [butterﬂy, squirrel]
2
951
237 https://universe.roboflow.com/handwashhygeine/nature-3tkys
Hand
[Hand-Segmentation, hand]
2
210
60
https://universe.roboflow.com/rmutsb-xxgii/hand-segmentation-gqzuh/dataset/1
Garbage
[bin, garbage, pavement, road]
4
325
142 https://universe.roboflow.com/project-blmh9/d2-bj1a0/dataset/1
Chicken
[chicken]
1
19
1
https://universe.roboflow.com/nena-trikic-ljxt3/chickenstf/dataset/1
Rail
[rail]
1
3067 1069 https://universe.roboflow.com/wzk789wzk-gmail-com/rail_dataset/dataset/4
Airplane-Parts
[Airplane, Body, Cockpit, Engine, Wing]
5
39
7
https://universe.roboflow.com/foxehcorp/foxehcorp_airplane_dataset/dataset/4/download
Brain-Tumor
[tumor]
1
236
28
https://universe.roboflow.com/detection-qskiw/segmnetation/dataset/2
Poles
[poles]
1
11
3
https://universe.roboflow.com/ohsee/pole2/dataset/2
Electric-Shaver
[caorau]
1
288
24
https://universe.roboflow.com/fpt-university-1tkhk/caurau
Bottles
[bottle, can, label]
3
357
16
https://universe.roboflow.com/beerup/bottels2/dataset/1
Toolkits
[Allen-key, block, gasket, ...]
8
48
6
https://universe.roboflow.com/mst/mask-2ihnt/dataset/1
Trash
[Aluminium foil, Cigarette, ...]
12
832
92
https://universe.roboflow.com/sara-najafi/trash_segmentation2/dataset/2
Salmon-Fillet
[Salmon ﬁllet]
1
1991
64
https://universe.roboflow.com/rishik-mishra-rljwe/f1225
Puppies
[puppy]
1
15
3
https://universe.roboflow.com/marcin-bak/puppies-fmoxu/dataset/2
Tablets
[tablets]
1
237
13
https://universe.roboflow.com/detection-qskiw/tablets-instance-segmentation/dataset/1
Cows
[cow]
1
630
60
https://universe.roboflow.com/new-workspace-5abdm/maskrcnn-ofglr/dataset/2
Ginger-Garlic
[garlic, ginger]
2
28
8
https://universe.roboflow.com/george-brown-college-1omrb/
ginger-and-garlic-object-segmentation/dataset/1
Table 12. Meta information of SegInW benchmark. We list the source links, annotated category names and number of categories for each dataset.
(a) Phones
(b) Elephants
(c) Hand-Metal
(d) Watermelon
(e) House-parts
(f) Household-items
(g) Strawberry
(h) Fruits
(i) Butterﬂy-Squirrel
(j) Hand
(k) Garbage
(l) Chicken
(m) Rail
(n) Airplane-parts
(o) Brain-tumor
(p) Poles
(q) Electric-shaver
(r) Bottles
(s) Toolkits
(t) Trash
(u) Salmon
(v) Puppies
(w) Tablets
(x) Cows
(y) Ginger-Garlic
Figure 7. Examplar images and annotations in SegInW benchmark. The benchmark covers a diversity of visual domains and concepts in the daily life.
16

Figure 8. Zero-shot segmentation performance on SeginW with X-Decoder-L model. We report the mAP in descending order.
0 1 2
5
10
Full
# Shots
20
25
30
35
40
mAP
X-Decoder (T)
X-Decoder (B)
X-Decoder (FL)
X-Decoder (L)
X-Decoder-Seg+ (B)
(a) Tune Linear (0.26M)
0 1 2
5
10
Full
# Shots
20
25
30
35
40
mAP
X-Decoder (T)
X-Decoder (B)
X-Decoder (FL)
X-Decoder (L)
X-Decoder-Seg+ (B)
(b) Tune Prompt (1.15M)
0 1 2
5
10
Full
# Shots
20
25
30
35
40
45
mAP
X-Decoder (T)
X-Decoder (B)
X-Decoder (FL)
X-Decoder (L)
X-Decoder-Seg+ (B)
(c) Tune Decoder (39.3M)
0 1 2
5
10
Full
# Shots
25.0
27.5
30.0
32.5
35.0
37.5
40.0
42.5
mAP
#Param: 0.26M
#Param: 1.15M
#Param: 39.3M
(d) X-Decoder-Seg+ (B)
0 1 2
5
10
Full
# Shots
22.5
25.0
27.5
30.0
32.5
35.0
37.5
40.0
42.5
mAP
#Param: 0.26M
#Param: 1.15M
#Param: 39.3M
(e) X-Decoder (T)
0 1 2
5
10
Full
# Shots
25.0
27.5
30.0
32.5
35.0
37.5
40.0
42.5
45.0
mAP
#Param: 0.26M
#Param: 1.15M
#Param: 39.3M
(f) X-Decoder (B)
0 1 2
5
10
Full
# Shots
25
30
35
40
45
mAP
#Param: 0.26M
#Param: 1.15M
#Param: 39.3M
(g) X-Decoder (L-IN21K)
0 1 2
5
10
Full
# Shots
32
34
36
38
40
42
44
46
mAP
#Param: 0.26M
#Param: 1.15M
#Param: 39.3M
(h) X-Decoder (L)
Figure 9. (a-c) Line chart of tuning shot and mAP with different tuning strategies on each backbone architecture. (d-h) Line chart of tuning shot and mAP
with different backbone architecture on different number of tuning parameters (strategies).
dataset meta information is listed in Tab. 12, and examplar
images are shown in Fig. 7.
On the SegInW benchmark, we evaluate zero-shot, few-
shot, and ﬁne-tuned segmentation for ﬁve models (X-
Decoder-Seg+ as baselines, and X-Decoder with differ-
ent visual backbone) on three different tuning scales. In
Fig. 8, we report the zero-shot instance segmentation per-
formance on 25 datasets separately in a descending order.
Accordingly, X-Decoder shows reasonably good general-
ization ability to a wide range of visual and concept do-
mains. Speciﬁcally, it achieves higher mAP on common ob-
jects like fruits and animals but lower ones on ﬁne-grained
datasets like toolkits and rare concepts like rail and brain
tumor. In Fig. 9, we further show the line chars for few-shot
learning and fully-ﬁnetuning, and observe that:
X-Decoder has privilege on small-scale tuning. As shown
in Fig. 9 (a-b), comparing with X-Decoder-Seg+ that only
extract noun phrase to increase vocabulary size, X-Decoder
performs much better with few-shot/ﬁnetune setting. Al-
though X-Decoder (B) and X-Decoder-Seg+ (B) have sim-
ilar zero-shot performance, the gap increases with the num-
ber of images tuned. However, as the number of parame-
ters tuned increased by a large margin Fig. 9 (c), the per-
formance gap between X-Decoder and X-Decoder-Seg+ is
shrunk to a small margin.
Zero-Shot gap could be bridged by tuning. X-Decoder
(L) and X-Decoder (L-IN21K) are initialized with different
pre-trained image backbones. Speciﬁcally, X-Decoder (L)
is initialized by Florence [94] pre-trained Davit-d5, whereas
X-Decoder (L-IN21K) is initialized with FocalNet-L pre-
trained on ImageNet-21k [15]. As shown in Fig. 9 (a-c),
although the gap between X-Decoder-L and X-Decoder-L-
IN21K on the zero-shot setting is relatively large. However,
the gap on 5/10/full ﬁnetuned settings is much smaller and
even cross in some settings.
Tuning class embedding is enough for few-shot settings.
As shown in Fig. 9 (e-h), on the smaller scale backbone in-
cluding (T/B), although tuning the full decoder has a better
result, the gap is not obvious on 0-10 shots. And on larger
scale models including L/L-IN21K, tuning with class em-
bedding has similar/better results on 0-10 shots.
We show more detailed results in Table 13, Table 14 and
Table 15. Similar to Table 3, we report the number of pa-
rameters tuned in each setting.
17

E. Extra Visualization
In this part, we demonstrate the generalization ability to
video datasets and ﬂexibility to support task compositions
for X-Decoder with more qualitative visualizations.
E.1. Zero-Shot Generic Video Segmentation
Open-vocabulary generic segmentation is one of the
main advantages of X-Decoder. We also apply generic seg-
mentation in a zero-shot manner to the YoutubeVOS [82]
dataset. As shown in Fig. 17, our model can be well gen-
eralized to video zero-shot generic segmentation and make
predictions that are consistent across frames. As a result,
our model can be used in video segmentation directly or a
good initialization for further ﬁnetuning.
E.2. Zero-Shot Referring Video Segmentation
Besides the generic segmentaton on video frames, our X-
Decoder can be easily adapted to referring video segmen-
tation as well without any architectural change or ﬁnetun-
ing. In Fig. 18, we visualize some examples of referring
video segmentation on the YoutubeVOS [82] dataset in a
zero-shot manner. We can see that our model can generate
rather accurate outputs given various referring phrases. No-
tably, in addition to the strong segmentation performance
for given concepts, the model can also correctly distinguish
the spatial locations (e.g., left v.s. right in the ﬁrst row),
and object attributes (e.g., a baby gorilla instead of an adult
gorilla in the second row) in these unseen videos.
E.3. Zero-Shot Image Captioning
To test the generalization ability of X-Decoder, we
also ask the model generate image captions on the
YoutubeVOS [82] dataset, which is in a different domain
from the image data. As we can see from the examples in
Fig. 19, the model can correctly predict the object, activity,
and environment in an image. Interestingly, the captions
for the ﬁrst 6 images sampled from 3 different videos show
that our approach can correctly differentiate the movements
from similar scenarios (e.g., a man playing vs. a man stand-
ing in the ﬁrst two samples.).
E.4. Zero-Shot Referring Captioning
In compensating for the visualization of the main paper,
we add more referring captioning samples in Fig 20. The
phrase before “:” is the referring phrase, and the sentence af-
ter “:” is the generated caption. The grounding mask of the
referring phrase is highlighted in pink. Clearly, our model
can simultaneously segments the referred region and gen-
erates a region-speciﬁc caption. Complementary to regular
image captioning systems, such a novel functionality pro-
vides a way of interpreting images in a more ﬁne-grained
manner. Note that our X-Decoder was never trained to gen-
erate such regional captions.
E.5. Zero-Shot Referring Image Editing
Finally, given the high-quality referring segmentation re-
sults with X-Decoder, we can effortlessly combine it with
off-the-shelf Stable-Diffusion image inpainting model [66]
and perform zero-shot referring image editing. As shown
in Fig. 21, the model ﬁrst performs referring segmentation,
then the original image and the segmentation mask are fed
into the inpainting model to generate the inpainted image.
For example, given “change bird to squirrel”, it ﬁrst extracts
the bird segment (blue region) from the input image and
then replace the segmented region with a generated squir-
rel. Likewise in other samples, we can see all the generated
images look natural and follow the inpainting instructions
very well. These impressive plug-and-play results imply a
great potential of combining our X-Decoder and advanced
generative AI models for ﬁne-grained precise image editing.
F. Discussions
Future Directions. The extensive quantitative and qual-
itative results have demonstrated the strong performance
and generalization ability of our X-Decoder for a variety
of vision and vision-language tasks at different granulari-
ties. Upon the current X-Decoder design, we see two di-
rections worth future explorations: (1) Pretrain the whole
model in one stage effectively and efﬁciently. Currently, the
model still requires a separate pretraining for the image and
text encoders. However, since our model supports large-
scale image-text contrastive learning thanks to the decou-
pled design, we can easily unify the CLIP-style pretraining
with the decoder pretraining in an end-to-end manner. (2)
Unify all level of supervisions. Due to high annotation costs,
the pixel-level segmentation annotations by nature are much
less than the region-level box and image-level annotations.
It is worth building a more uniﬁed learning paradigm to
jointly learn from pixel-level, region-level and image-level
supervision to attain a more powerful uniﬁed model.
Social Impact. This work is mainly focused on the de-
sign of a generalized decoder for various vision and vision-
language tasks. We have used a pretrained image and text
encoder and further pretrained the models on a combination
of various datasets and tasks. Since the models are trained
on large-scale webly-crawled image-text pairs, the negative
impact might arise due to the potential offensive or biased
content in the data. To mitigate this issue, we need to have
a careful sanity check on the training data and model pre-
dictions before deploying it in practical scenarios.
18

Model
Shot #Param Avg Airplane-
Parts
Bottles Brain-
Tumor Chicken
Cows Electric-
Shaver Elephants Fruits Garbage Ginger-
Garlic
Hand
Hand-
Metal
House-
Parts
HH.-
Items
Nutterﬂy-
Squireel Phones
Poles Puppies
Rail
Salmon-
Fillet
Strawberry Tablets Toolkits
Trash
Watermelon
X-Decoder (T)
0
0.0M 22.7
10.5
19.0
1.1
12.0
12.0
1.2
65.6
66.5
28.7
7.9
0.6
22.4
5.5
50.6
62.1
29.9
3.6
48.9
0.7
15.0
41.6
15.2
9.5
19.3
16.2
X-Decoder (T)
1
39.3M 22.7 10.5±0.0 19.0±0.0 1.1±0.0 12.0±0.0 12.0±0.0 0.8±0.7
65.6±0.0 66.5±0.0 28.7±0.0
7.9±5.8
0.6±0.0
22.4±0.0 5.5±0.0 50.6±4.6 62.1±0.0 29.9±2.3 3.6±0.0 48.9±4.6 0.7±0.0 15.0±1.1
41.6±0.0
15.2±0.0 9.5±0.0 19.3±0.0
16.2±0.0
X-Decoder (T)
3
39.3M 24.1 10.5±0.0 19.0±0.0 1.1±0.0 27.2±26.3 13.4±0.6 1.2±0.0
65.6±0.0 66.9±0.6 28.7±0.0
9.9±2.5
0.6±0.0
21.2±2.2 6.1±0.1 50.6±4.6 62.1±0.0 36.6±7.5 7.6±8.0 49.8±2.5 0.7±0.0 15.0±1.1
41.0±1.0
14.3±1.5 11.5±0.6 19.8±0.7
20.3±5.7
X-Decoder (T)
5
39.3M 29.1 10.0±0.9 30.2±1.2 6.3±3.9 51.8±6.6 18.2±2.7 2.3±1.5
64.9±0.8 67.2±2.1 33.2±1.2 16.9±3.3 30.3±22.8 23.8±6.4 7.1±1.2 50.6±0.1 66.4±1.1 46.4±5.9 14.0±4.8 49.0±0.4 1.5±0.9 15.7±4.0
42.0±0.7
16.1±3.2 12.7±1.6 21.1±1.0
27.9±6.7
X-Decoder (T)
10
39.3M 34.6 10.0±2.0 34.1±6.3 9.7±2.3 59.6±2.4 19.8±5.8 17.4±23.3 64.8±2.1 68.2±7.1 38.1±1.9 17.6±5.9 81.6±4.0 45.6±7.2 8.4±0.9 51.0±0.4 63.9±3.1 41.3±8.4 3.6±0.0 48.4±2.4 1.7±1.5 29.7±7.0
44.4±3.0
24.1±5.7 14.1±2.1 21.6±1.1
44.5±2.4
X-Decoder (T)
3×All 39.3M 37.5 10.5±0.5 35.1±4.4 6.6±1.2 14.5±4.3 30.9±0.9 7.1±0.5
68.3±0.4 70.3±6.9 36.9±0.8
9.9±3.5 37.7±20.1 46.0±3.7 8.5±0.0 50.6±0.0 79.8±0.3 48.7±0.8 13.1±9.4 50.6±0.2 59.6±0.9 54.2±0.5 83.4±10.5
27.4±0.8 13.2±1.0 27.6±0.9
44.5±0.9
X-Decoder-Seg+ (B)
0
0.0M 26.3
13.2
17.2
0.8
33.0
28.6
4.9
67.9
71.1
28.8
5.2
0.0
0.8
6.8
50.6
53.2
18.8
17.9
68.2
0.7
21.1
86.3
5.8
11.5
12.1
31.7
X-Decoder-Seg+ (B)
1
39.3M 26.3 13.2±0.0 17.2±0.0 0.8±0.0 33.0±0.0 28.6±2.3 4.9±0.0
68.0±0.2 71.1±0.0 28.8±0.0
5.2±0.0
0.0±0.0
0.8±0.0 6.8±0.0 50.7±0.0 53.2±0.0 18.8±0.0 17.9±0.0 68.2±0.0 0.7±0.0 21.0±0.0
86.5±0.3
5.8±5.8 11.5±0.0 12.1±1.1
31.7±2.3
X-Decoder-Seg+ (B)
3
39.3M 26.6 13.2±0.0 17.2±0.2 0.9±0.1 33.0±0.0 29.6±0.6 4.4±0.8
67.3±0.5 74.4±4.9 28.6±0.3
6.0±0.8
0.0±0.0
1.1±0.2 7.0±0.2 50.7±0.0 53.1±0.1 23.3±3.3 17.9±0.0 67.1±1.9 0.7±0.0 21.4±0.4
86.5±0.7
5.8±0.0 10.3±1.0 12.2±0.0
32.9±1.0
X-Decoder-Seg+ (B)
5
39.3M 27.5 13.3±0.1 18.6±1.3 1.4±0.1 34.3±3.5 30.6±1.1 5.2±0.7
67.6±2.2 79.5±1.0 28.8±0.1
4.3±0.8
0.0±0.0
1.4±0.3 7.5±0.1 50.7±0.0 54.9±0.9 26.9±4.1 18.8±1.1 66.9±3.0 0.8±0.1 21.6±0.9
85.5±2.1
7.4±0.1 13.0±2.7 12.4±0.0
35.7±1.8
X-Decoder-Seg+ (B)
10
39.3M 30.0 13.5±0.3 18.0±0.2 2.1±0.8 49.7±1.9 35.9±1.8 8.0±3.1
68.2±2.8 79.9±4.0 28.6±0.3
7.3±2.8
0.0±0.0
6.0±1.6 8.1±0.3 50.5±0.0 55.1±2.4 33.6±1.7 17.9±0.0 69.4±3.9 1.6±0.3 24.8±2.5
87.9±0.6
9.0±1.3 14.3±0.6 13.0±0.4
45.7±2.8
X-Decoder-Seg+ (B) 3×All 39.3M 31.9 13.5±0.2 19.8±0.4 2.2±0.1 31.7±2.3 37.7±0.3 4.1±0.3
73.9±0.2 78.8±1.9 29.0±0.0
5.3±0.2
0.0±0.0
3.3±0.6 8.6±0.1 50.7±0.0 63.9±0.0 24.3±1.1 20.1±0.0 65.4±0.0 46.4±1.5 54.0±1.6
89.6±0.0
8.4±0.8 12.7±0.9 13.7±0.1
39.1±1.0
X-Decoder (B)
0
0.0M 27.7
13.0
45.9
0.3
13.6
36.8
4.2
68.0
76.7
30.2
19.4
20.6
18.5
6.7
51.7
53.1
8.9
5.6
55.4
0.8
18.2
81.6
8.0
13.9
27.3
13.0
X-Decoder (B)
1
39.3M 27.8 13.0±0.0 45.9±0.0 0.3±0.0 13.6±1.1 36.8±0.0 4.2±0.0
68.0±0.0 76.7±0.0 30.2±0.0 19.4±0.0 20.6±0.0 18.5±0.0 6.7±5.8 51.7±0.0 53.1±4.6 10.2±2.1 5.6±0.0 55.4±0.0 0.8±0.0 18.2±0.0
81.6±0.0
8.0±0.0 13.9±0.0 27.3±0.0
13.0±0.0
X-Decoder (B)
3
39.3M 28.2 12.6±0.3 41.6±5.2 0.3±0.0 13.6±1.1 37.2±0.6 4.2±0.0
68.0±0.1 77.4±1.1 30.4±0.2 19.8±0.6 26.7±9.6 22.2±6.4 7.2±0.2 51.9±0.3 53.1±4.6 12.2±2.8 5.5±0.4 55.4±0.0 0.3±0.5 19.4±3.4
82.9±1.5
8.7±0.7 10.9±3.1 28.0±0.6
15.3±2.0
X-Decoder (B)
5
39.3M 33.1 12.6±0.4 41.2±0.9 2.2±2.0 27.5±7.5 37.9±0.3 9.8±4.6
67.6±1.2 78.3±1.7 30.4±0.3 33.9±5.8 75.2±14.1 35.5±16.5 8.5±0.4 53.0±1.8 61.9±7.8 12.3±2.7 4.5±0.5 54.7±0.8 0.9±0.1 20.7±0.4
86.7±0.8
8.8±2.5 14.9±6.8 28.9±1.3
19.4±6.8
X-Decoder (B)
10
39.3M 38.6 10.9±1.0 38.3±3.5 1.8±0.9 46.3±4.2 38.6±1.8 24.5±10.3 67.0±4.4 78.4±3.3 30.8±0.4 37.9±2.1 89.1±2.9 63.5±1.0 8.7±0.5 58.4±2.9 72.8±3.0 19.5±7.4 5.6±0.0 57.1±1.1 2.3±0.7 24.6±2.4
87.8±2.8
13.4±5.0 15.0±3.3 32.6±0.4
39.9±3.1
X-Decoder (B)
3×All 39.3M 38.9 12.5±0.0 42.7±0.2 1.0±0.0 14.6±1.4 36.8±0.3 17.4±3.1 71.7±0.2 79.7±0.6 31.5±0.1 29.1±1.6 53.6±0.5 65.8±0.5 9.2±0.0 54.0±0.9 82.3±0.2 17.1±3.2 5.7±1.2 55.4±0.4 48.9±2.7 48.3±1.4
90.3±0.0
18.8±2.5 15.0±0.0 36.3±0.5
33.0±2.1
X-Decoder (L-IN21K)
0
0.0M 26.7
12.3
43.2
0.5
3.5
12.3
18.8
63.9
79.1
24.3
15.6
0.0
20.3
4.9
50.5
58.8
43.4
13.4
57.3
1.3
12.3
74.4
6.9
14.6
20.1
13.5
X-Decoder (L-IN21K)
1
39.3M 26.8 12.3±0.0 43.2±4.6 0.5±0.0 3.5±0.0 13.9±2.8 18.8±0.0 63.9±4.6 79.1±0.0 25.1±1.4 15.6±1.1
0.0±0.0
21.4±1.9 4.9±0.0 50.5±0.0 58.8±0.0 43.4±0.0 14.0±0.9 57.3±0.0 1.3±0.0 12.3±0.0
74.4±0.0
6.9±0.0 14.6±0.0 20.1±0.0
13.5±0.0
X-Decoder (L-IN21K)
3
39.3M 29.5 14.0±0.4 44.9±3.0 0.9±0.7 3.5±1.3 27.0±1.0 21.6±4.9 63.7±0.8 79.1±0.0 24.3±0.0 13.8±4.2 29.6±51.3 16.9±5.8 6.0±0.4 50.6±0.1 59.6±1.3 45.2±0.7 18.4±1.7 57.7±1.4 0.5±0.9 16.7±7.7
83.1±1.8
5.4±3.0 15.3±0.9 23.3±3.1
14.7±2.0
X-Decoder (L-IN21K)
5
39.3M 36.2 12.1±1.6 50.4±4.3 0.4±0.0 31.7±6.9 32.7±0.7 51.9±18.6 64.2±0.7 75.7±3.9 27.8±0.7 22.4±13.4 60.0±33.8 23.9±10.0 7.1±0.1 51.4±1.0 63.0±3.6 42.7±2.9 15.7±5.6 59.7±1.7 1.8±0.1 21.4±9.0
83.7±2.5
16.3±11.6 16.5±0.4 34.0±4.7
37.1±2.7
X-Decoder (L-IN21K)
10
39.3M 40.5 11.8±0.4 52.0±2.3 0.6±0.2 34.1±6.1 34.3±1.1 48.7±16.6 65.3±1.7 80.0±0.9 30.4±11.3 28.0±10.8 91.5±2.8 47.4±29.2 7.0±0.5 54.2±5.1 73.0±6.9 44.6±1.8 13.4±0.0 55.0±5.2 4.6±1.3 24.4±3.6
85.3±1.1
24.7±18.6 20.2±1.3 37.0±1.5
43.8±3.1
X-Decoder (L-IN21K) 3×All 39.3M 40.7 14.1±1.0 53.2±0.7 0.4±0.1 4.5±1.7 36.6±0.4 62.7±4.2 70.3±0.1 80.0±1.2 31.7±0.7 15.6±6.0 22.2±0.5 61.7±0.8 7.8±0.2 51.8±1.0 85.2±0.0 44.1±0.3 12.6±5.9 60.7±0.0 41.7±1.0 48.3±0.6
90.4±0.0
22.9±1.7 18.7±1.2 42.3±0.6
36.8±2.4
X-Decoder (L)
0
0.0M 32.3
13.1
42.1
2.2
8.6
44.9
7.5
66.0
79.2
33.0
11.6
75.9
42.1
7.0
53.0
68.4
15.6
20.1
59.0
2.3
19.0
67.1
22.5
9.9
22.3
13.8
X-Decoder (L)
1
39.3M 32.0 13.1±0.0 42.1±0.0 2.2±0.0 8.6±0.0 44.9±0.0 7.5±0.0
66.0±0.0 79.2±0.0 33.0±0.0 11.6±1.1 75.9±0.0 42.1±0.0 7.0±0.0 53.0±0.0 68.4±0.0 15.6±1.1 20.1±0.0 59.0±0.0 2.3±0.0 19.0±0.0
67.1±0.0
22.5±0.0 9.9±0.0 15.4±13.4
13.8±0.0
X-Decoder (L)
3
39.3M 32.6 13.1±0.0 42.1±0.0 2.2±0.0 12.3±6.4 45.1±0.2 7.5±0.0
66.0±0.0 78.6±0.5 33.3±0.5 11.6±1.1 75.9±0.0 42.1±0.0 7.0±0.0 53.0±0.0 68.4±0.0 17.0±5.2 21.6±1.7 59.0±0.0 2.4±0.1 19.0±0.0
67.1±0.0
23.3±1.4 9.5±0.7 22.3±0.0
13.8±0.0
X-Decoder (L)
5
39.3M 35.0 14.0±0.3 45.3±3.6 4.1±0.4 24.9±11.0 46.1±0.1 11.2±7.1 65.8±0.8 77.9±1.1 33.6±0.5 13.2±1.6 85.1±4.1 43.5±4.1 7.4±0.0 52.9±0.2 69.2±1.4 16.9±8.2 21.6±1.6 58.5±3.1 2.6±0.2 18.4±0.9
81.2±3.9
25.8±4.8 9.7±0.3 24.9±1.8
19.6±2.6
X-Decoder (L)
10
39.3M 40.3 13.3±0.3 45.2±3.5 3.2±1.7 42.3±3.9 45.8±0.1 29.3±3.5 68.3±2.0 76.0±3.1 37.9±1.9 24.4±1.3 93.7±0.4 57.5±1.2 7.9±0.5 52.1±0.3 78.8±1.3 27.0±1.5 20.1±0.0 56.7±4.9 3.3±0.3 17.5±0.9
85.2±1.4
40.1±7.0 8.4±0.6 31.4±0.7
42.0±8.7
X-Decoder (L)
3×All 39.3M 42.2 13.9±0.5 48.4±0.2 7.9±2.8 8.6±0.0 45.3±0.2 20.5±0.2 72.4±0.0 80.5±1.0 36.7±1.1 14.8±1.4 86.7±1.8 63.8±0.3 7.5±0.2 52.8±0.8 83.3±0.1 20.1±1.2 18.1±6.6 57.4±3.0 45.1±0.9 50.2±1.1
92.0±0.1
40.4±1.0 10.4±0.7 36.3±0.6
40.2±0.7
Table 13. SegInW results with tuning on class embedding for different image shots and backbone architectures. (39.3M parameters tuned in the setting.)
Model
Shot #Param Avg Airplane-
Parts
Bottles Brain-
Tumor Chicken
Cows Electric-
Shaver Elephants Fruits Garbage Ginger-
Garlic
Hand
Hand-
Metal
House-
Parts
HH.-
Items
Nutterﬂy-
Squireel
Phones
Poles Puppies
Rail
Salmon-
Fillet
Strawberry Tablets Toolkits
Trash
Watermelon
X-Decoder (T)
0
0.0M 22.7
10.5
19.0
1.1
12.0
12.0
1.2
65.6
66.5
28.7
7.9
0.6
22.4
5.5
50.6
62.1
29.9
3.6
48.9
0.7
15.0
41.6
15.2
9.5
19.3
16.2
X-Decoder (T)
1
1.15M 22.8 10.5±0.0 19.0±0.0 1.1±0.0 12.0±0.0 12.0±0.0 1.2±0.0
65.6±0.0 66.5±0.0 28.7±0.0 7.9±5.8
0.6±0.0
22.4±0.0 5.5±0.0 50.6±4.6 62.1±0.0 33.9±4.9 3.6±0.0 48.9±4.6 0.7±0.0 15.0±1.1
41.6±0.0
15.2±0.0 9.5±0.0 19.3±0.0
16.2±0.0
X-Decoder (T)
3
1.15M 24.2 10.1±0.1 22.9±6.7 1.6±0.6 19.2±12.4 13.5±0.4 1.2±0.0
66.0±0.4 66.5±0.0 29.9±2.0 7.9±5.8
0.6±0.0
22.4±0.0 5.8±0.4 50.6±4.6 62.1±0.0 38.1±3.9 7.2±8.4 48.9±4.6 0.9±0.3 15.0±1.1
42.4±1.3
16.5±2.4 10.2±0.6 19.9±1.0
24.1±5.0
X-Decoder (T)
5
1.15M 27.9 10.5±0.3 30.1±1.8 4.1±3.3 36.6±3.4 15.6±1.4 2.2±0.7
66.0±0.4 69.7±2.9 31.6±1.2 7.2±0.0 22.3±25.9 31.8±5.9 7.9±0.5 50.6±0.0 65.4±1.4 43.7±5.4 17.9±1.9 51.6±1.0 1.0±0.3 15.7±1.7
39.4±4.7
17.8±4.2 11.8±1.3 21.4±0.2
24.1±7.8
X-Decoder (T)
10
1.15M 34.5 10.1±0.0 38.4±2.6 4.7±1.1 56.3±7.4 20.7±5.6 2.6±1.5
66.8±2.7 64.0±7.7 36.1±2.1 21.8±5.5 84.9±2.6 41.1±14.3 8.3±0.6 53.1±3.2 68.0±1.6 45.4±5.6 3.6±0.0 49.7±0.8 6.7±2.4 24.3±5.3
43.0±1.5
32.1±5.8 14.6±3.1 22.6±1.7
41.8±12.1
X-Decoder (T)
3×All 1.15M 37.9 10.7±0.1 37.1±3.6 7.7±2.9 13.1±2.0 30.5±0.8 7.3±3.5
68.1±0.5 73.7±1.2 38.0±0.8 10.0±2.0 38.1±20.3 41.5±2.9 8.6±0.1 50.6±0.0 80.0±0.2 45.4±2.1 17.9±1.9 50.7±0.9 59.6±2.0 54.6±1.0
89.7±0.1
28.9±2.1 12.9±0.2 27.6±0.4
43.8±0.5
X-Decoder-Seg+ (B)
0
0.0M 26.3
13.2
17.2
0.8
33.0
28.6
4.9
67.9
71.1
28.8
5.2
0.0
0.8
6.8
50.6
53.2
18.8
17.9
68.2
0.7
21.1
86.3
5.8
11.5
12.1
31.7
X-Decoder-Seg+ (B)
1
1.15M 26.4 13.2±0.0 17.2±0.0 0.8±0.0 33.0±0.0 28.6±2.3 4.9±0.0
67.9±0.0 74.1±5.1 28.8±0.0 5.2±0.0
0.0±0.0
0.8±0.0 6.8±0.0 50.6±0.0 53.2±0.0 18.8±0.0 17.9±0.0 68.2±0.0 0.7±0.0 21.1±0.0
86.3±0.0
5.8±5.8 11.5±0.0 12.1±1.1
31.7±2.3
X-Decoder-Seg+ (B)
3
1.15M 26.6 13.3±0.1 17.2±0.0 1.1±0.3 33.0±0.0 28.9±0.5 4.9±0.0
67.2±1.3 74.4±4.9 28.7±0.0 5.2±0.1
0.0±0.0
1.0±0.0 7.0±0.2 50.6±0.0 53.3±0.0 22.1±3.5 18.3±0.3 66.1±1.8 0.7±0.0 21.4±0.8
86.6±0.0
6.2±0.7 11.1±0.5 12.2±0.2
33.5±0.7
X-Decoder-Seg+ (B)
5
1.15M 27.4 13.5±0.1 17.7±1.0 1.6±0.8 31.5±6.9 32.7±3.0 4.6±0.9
68.9±2.9 79.7±1.5 28.7±0.0 6.7±2.8
0.0±0.0
1.2±0.1 7.6±0.3 50.6±0.0 53.7±0.6 25.6±2.6 18.6±1.3 66.1±1.4 0.8±0.1 21.2±0.3
87.2±0.6
6.9±0.9 10.1±0.8 12.6±0.2
35.5±3.2
X-Decoder-Seg+ (B)
10
1.15M 29.9 13.5±0.5 19.4±0.9 2.5±1.1 45.1±10.9 37.5±2.8 7.2±3.9
69.4±2.9 81.4±0.4 28.8±0.3 8.2±2.3
0.0±0.0
2.1±0.3 8.2±0.1 51.9±1.8 54.6±1.7 32.2±7.7 17.9±0.0 71.7±1.5 1.7±0.1 24.5±0.8
87.5±0.2
10.7±1.3 14.9±1.5 12.5±0.1
42.0±3.8
X-Decoder-Seg+ (B) 3×All 1.15M 31.6 13.5±0.1 19.0±0.5 1.7±0.0 33.0±0.0 37.7±0.3 3.8±0.1
73.7±0.2 75.0±3.8 29.0±0.1 5.0±0.4
0.0±0.0
3.6±0.5 8.6±0.2 50.7±0.0 63.7±0.9 24.7±1.2 19.4±1.2 65.3±0.1 45.7±1.0 53.0±1.0
89.6±0.0
8.4±0.7 13.5±1.3 13.4±0.1
38.2±0.5
X-Decoder (B)
0
0.0M 27.7
13.0
45.9
0.3
13.6
36.8
4.2
68.0
76.7
30.2
19.4
20.6
18.5
6.7
51.7
53.1
8.9
5.6
55.4
0.8
18.2
81.6
8.0
13.9
27.3
13.0
X-Decoder (B)
1
1.15M 26.8 13.0±0.0 45.9±0.0 0.3±0.0 9.0±7.8 36.8±0.0 4.1±0.2
68.0±0.0 76.7±0.0 30.2±0.0 19.4±0.0 20.6±0.0 18.5±0.0 6.7±5.8 51.7±0.0 35.4±30.6
8.9±0.0
4.9±0.5 55.4±0.0 0.8±0.0 18.2±0.0
81.6±0.0
8.0±0.0 13.9±0.0 27.3±0.0
13.0±0.0
X-Decoder (B)
3
1.15M 28.6 12.5±0.1 44.3±2.4 0.2±0.0 21.4±7.2 37.7±0.8 5.1±1.5
67.9±0.1 76.6±0.2 30.3±0.0 24.9±4.9 23.4±2.4 18.5±0.0 7.1±0.6 51.7±0.0 53.1±4.6 10.7±5.4 6.7±0.4 55.1±0.3 0.8±0.0 18.6±2.7
84.2±2.3
8.1±1.5 12.6±2.3 27.3±0.0
14.0±1.8
X-Decoder (B)
5
1.15M 33.0 12.4±0.9 43.7±0.9 0.4±0.3 40.0±4.1 38.1±0.7 9.0±3.7
67.4±0.4 79.9±1.2 30.9±0.8 34.8±4.8 34.4±10.0 51.1±13.7 8.4±0.1 54.2±2.1 65.6±2.2 15.1±5.3 4.5±0.4 55.4±0.0 1.1±0.2 24.4±5.6
87.0±2.3
8.5±0.1 12.5±3.5 28.5±0.4
16.6±2.1
X-Decoder (B)
10
1.15M 39.7 10.3±1.8 36.8±4.1 1.4±0.8 49.3±0.9 38.8±1.3 28.4±2.2 69.2±1.6 76.3±0.4 31.4±0.5 37.2±1.1 92.2±0.2 60.1±6.0 8.8±0.7 55.8±3.8 74.2±1.8 28.7±11.4 5.6±0.0 57.0±2.4 3.2±0.4 30.2±2.9
88.0±1.7
19.8±2.3 17.6±6.0 30.9±1.4
40.4±2.5
X-Decoder (B)
3×All 1.15M 38.7 12.6±0.2 41.7±1.7 1.1±0.1 13.6±1.1 36.5±0.1 23.1±7.9 71.5±0.3 77.3±0.3 31.5±0.2 32.4±6.1 47.3±14.1 65.6±0.5 9.2±0.0 53.8±0.3 82.2±0.1 17.1±1.2 5.5±1.4 55.8±0.7 48.4±3.8 47.9±1.2
90.0±0.3
18.3±0.3 12.8±0.7 36.9±1.0
33.2±0.9
X-Decoder (L-IN21K)
0
0.0M 26.7
12.3
43.2
0.5
3.5
12.3
18.8
63.9
79.1
24.3
15.6
0.0
20.3
4.9
50.5
58.8
43.4
13.4
57.3
1.3
12.3
74.4
6.9
14.6
20.1
13.5
X-Decoder (L-IN21K)
1
1.15M 27.0 12.3±0.0 43.2±4.6 0.5±0.0 3.5±0.0 20.1±6.8 18.8±0.0 63.9±4.6 78.6±0.4 24.3±0.0 15.6±1.1
0.0±0.0
20.3±0.0 4.9±0.0 50.5±0.0 58.8±0.0 43.4±0.0 13.6±0.3 57.3±0.0 1.3±0.0 12.3±0.0
76.4±3.4
7.1±0.2 14.6±0.0 20.1±0.0
13.5±0.0
X-Decoder (L-IN21K)
3
1.15M 30.1 12.7±0.8 45.8±4.5 0.6±0.1 15.4±9.0 28.6±1.9 38.0±18.2 64.1±0.2 78.2±1.5 24.3±0.0 15.6±1.1 26.3±23.1 20.9±3.3 5.6±0.4 50.5±0.0 60.8±3.5 42.2±2.3 12.2±3.1 57.3±0.0 1.5±0.3 17.8±9.5
77.8±5.7
7.0±4.4 15.2±1.0 13.4±11.6
19.3±9.9
X-Decoder (L-IN21K)
5
1.15M 34.0 13.8±1.4 48.2±3.1 0.7±0.3 26.3±6.2 28.7±3.9 33.3±25.4 62.8±0.2 79.0±1.9 29.3±2.0 16.4±10.1 60.1±52.0 34.5±22.5 7.2±0.4 52.2±2.5 63.5±1.7 37.0±10.8 7.6±6.2 59.8±0.8 2.3±0.2 21.8±3.5
85.4±1.1
5.7±2.3 16.0±2.1 27.7±10.8
29.8±8.7
X-Decoder (L-IN21K)
10
1.15M 40.3 12.5±1.4 46.5±7.0 0.8±0.5 36.2±3.6 31.9±3.9 44.7±29.6 63.1±1.6 79.8±1.0 33.2±5.5 34.3±3.6 88.9±3.1 62.2±0.8 7.2±1.4 53.2±2.2 76.0±2.1 27.2±21.0 13.4±0.0 58.8±3.7 2.4±0.1 23.3±4.1
88.0±1.4
25.9±12.8 17.6±2.7 34.3±6.9
43.9±1.3
X-Decoder (L-IN21K) 3×All 1.15M 40.7 14.5±0.5 53.3±0.4 0.5±0.1 4.1±1.0 36.8±0.0 64.3±0.2 70.7±0.4 80.7±1.1 32.1±0.1 13.2±5.6 20.4±1.9 61.5±0.7 7.9±0.1 51.6±0.7 84.8±0.0 43.2±1.1 13.5±6.0 60.5±0.3 42.9±2.7 48.8±0.9
90.4±0.3
24.5±3.6 19.2±0.8 41.6±0.3
35.7±1.2
X-Decoder (L)
0
0.0M 32.3
13.1
42.1
2.2
8.6
44.9
7.5
66.0
79.2
33.0
11.6
75.9
42.1
7.0
53.0
68.4
15.6
20.1
59.0
2.3
19.0
67.1
22.5
9.9
22.3
13.8
X-Decoder (L)
1
1.15M 32.3 13.1±0.0 42.1±0.0 2.2±0.0 8.6±0.0 44.9±0.0 7.5±0.0
66.0±0.0 79.2±0.0 33.0±0.0 11.6±1.1 75.9±0.0 42.1±0.0 7.0±0.0 53.0±0.0 68.4±0.0 15.6±1.1 20.1±0.0 59.0±0.0 2.3±0.0 19.0±0.0
67.1±0.0
22.5±0.0 9.9±0.0 22.3±0.0
13.8±0.0
X-Decoder (L)
3
1.15M 32.6 13.3±0.3 41.4±1.2 2.2±0.0 8.6±0.0 45.5±0.4 7.5±0.0
66.4±0.6 79.2±0.0 33.0±0.0 11.6±1.1 75.9±0.0 42.1±0.0 7.1±0.2 53.0±0.0 68.4±0.0 18.0±3.3 20.7±0.4 59.0±0.0 2.3±0.0 19.0±0.0
67.1±0.0
22.0±0.8 9.9±0.0 22.9±0.9
17.1±5.6
X-Decoder (L)
5
1.15M 35.5 13.7±0.5 46.9±4.1 4.0±1.6 33.2±1.0 45.7±0.6 12.1±4.8 65.9±1.6 77.6±0.7 32.9±0.5 20.3±10.3 75.9±0.0 41.4±1.2 6.9±0.5 53.0±0.2 70.3±2.0 20.2±1.5 22.4±1.9 59.1±0.9 3.1±0.4 16.8±2.1
80.1±2.3
27.7±4.3 9.4±0.8 23.9±0.4
23.6±8.2
X-Decoder (L)
10
1.15M 40.5 13.6±0.1 45.4±1.8 4.5±2.4 44.9±0.4 46.0±1.1 35.2±10.8 66.7±3.8 78.6±1.7 39.2±2.6 20.1±5.2 94.3±0.3 59.9±3.4 7.2±0.2 52.0±0.7 77.8±1.3 24.5±2.8 20.1±0.0 56.7±1.3 3.2±0.6 19.7±2.5
86.9±0.4
38.8±4.5 8.8±3.2 30.7±1.2
36.5±4.5
X-Decoder (L)
3×All 1.15M 42.3 13.4±0.0 48.8±0.7 6.3±1.2 8.6±0.0 45.1±0.0 20.5±1.0 72.1±0.1 79.3±0.9 36.9±0.9 12.8±1.1 88.5±2.5 63.1±1.9 7.6±0.0 52.8±0.8 83.6±0.2 22.1±1.1 21.7±1.9 59.2±0.8 43.7±2.7 50.0±1.0
91.7±0.0
40.9±1.4 9.8±0.3 36.6±0.3
40.7±1.7
Table 14. SegInW results with tuning on class & mask embeddings and latent queries for different image shots and backbone architectures. (1.15M
parameters tuned in the setting.)
Model
Shot #Param Avg Airplane-
Parts
Bottles Brain-
Tumor Chicken
Cows
Electric-
Shaver Elephants
Fruits
Garbage Ginger-
Garlic
Hand
Hand-
Metal
House-
Parts
HH.-
Items
Nutterﬂy-
Squireel
Phones
Poles
Puppies
Rail
Salmon-
Fillet
Strawberry Tablets Toolkits Trash Watermelon
X-Decoder (T)
0
0.0M 22.7
10.5
19.0
1.1
12.0
12.0
1.2
65.6
66.5
28.7
7.9
0.6
22.4
5.5
50.6
62.1
29.9
3.6
48.9
0.7
15.0
41.6
15.2
9.5
19.3
16.2
X-Decoder (T)
1
0.26M 22.6 10.5±0.0 19.0±0.0 1.1±0.0 12.0±0.0 12.0±0.0
1.2±0.0
65.6±0.0 66.5±0.0 28.7±0.0 7.9±5.8
0.6±0.0
22.4±0.0 5.5±0.0 50.6±4.6 62.1±0.0 28.5±2.1
3.6±0.0
48.9±4.6
0.7±0.0
15.0±1.1
41.6±0.0
15.2±0.0 9.5±0.0 19.3±0.0
16.2±0.0
X-Decoder (T)
3
0.26M 25.1 10.4±0.1 20.2±2.1 3.4±4.0 42.7±17.8 14.4±2.3
1.2±0.0
66.1±0.8 65.5±3.1 28.8±0.2 11.6±6.3
0.6±0.0
22.4±0.0 7.4±1.1 50.6±0.0 62.1±0.0 33.4±14.3 9.6±6.6
49.3±0.6
0.7±0.0
15.0±1.1
41.6±0.0
15.3±3.2 11.8±2.5 20.3±1.7
20.5±3.7
X-Decoder (T)
5
0.26M 29.7 10.5±0.7 33.7±3.6 7.8±2.6 33.0±16.3 15.2±2.6 14.2±12.7 65.5±1.7 65.5±8.8 34.7±3.1 16.7±1.6 51.0±18.1 30.3±4.6 7.9±0.6 51.1±0.8 63.6±4.1 46.0±5.6 13.8±11.0 49.5±0.8
0.5±0.4
18.4±8.5
34.6±2.8
19.7±3.7 13.2±1.7 18.9±1.4
26.8±6.4
X-Decoder (T)
10
0.26M 36.2 10.9±1.6 35.2±5.5 6.2±2.6 61.8±3.1 19.8±5.7 46.2±6.8 66.0±2.7 63.2±10.3 34.9±3.8 19.5±13.6 92.0±1.0 46.1±15.0 10.6±1.2 56.3±3.3 67.9±2.1 33.3±12.5 3.6±0.0
45.8±3.5
6.3±0.6
22.8±8.7
52.4±21.2 25.7±10.4 16.4±6.3 21.2±0.9
39.9±2.1
X-Decoder (T)
3×All 0.26M 41.9 10.7±0.4 42.8±2.2 8.7±0.7 13.6±2.8 30.5±3.5 27.5±11.0 69.0±0.8 70.8±2.8 38.5±0.0 10.3±1.6 74.0±1.2 61.0±2.0 13.9±0.3 50.7±0.0 81.3±0.8 44.2±3.0 20.1±0.0 50.3±0.6 62.6±2.6 55.0±0.6
90.8±0.1
28.0±1.9 18.4±2.7 27.3±0.9
45.3±1.7
X-Decoder-Seg+ (B)
0
0.0M 26.3
13.2
17.2
0.8
33.0
28.6
4.9
67.9
71.1
28.8
5.2
0.0
0.8
6.8
50.6
53.2
18.8
17.9
68.2
0.7
21.1
86.3
5.8
11.5
12.1
31.7
X-Decoder-Seg+ (B)
1
0.26M 26.3 13.2±0.0 17.2±0.0 0.8±0.0 33.0±0.0 28.6±0.0
4.9±0.0
67.9±0.0 71.1±0.0 28.8±0.0 5.2±0.0
0.0±0.0
1.5±1.2
6.8±5.8 50.6±0.0 53.2±0.0 18.8±0.0 17.9±0.0 67.3±1.6
0.7±0.0
21.1±0.0
86.3±0.0
5.8±5.8 11.5±0.0 12.1±1.1
31.8±0.0
X-Decoder-Seg+ (B)
3
0.26M 28.5 13.2±0.3 17.2±0.0 1.4±0.5 33.0±0.0 31.2±3.4
9.5±6.7
67.9±0.0 74.1±5.2 28.8±0.0 7.0±3.1
1.3±0.7 12.4±16.4 7.3±0.3 52.6±3.4 55.8±2.5 31.3±7.3 17.6±2.5 68.2±0.0
0.7±0.0
21.9±3.6
86.9±0.9
9.7±0.7 11.9±0.6 12.0±0.7
39.0±7.2
X-Decoder-Seg+ (B)
5
0.26M 32.4 13.7±0.0 20.7±0.9 2.3±0.5 27.1±14.4 30.2±0.6 41.7±27.2 67.3±1.7 77.5±0.2 28.0±1.0 17.8±7.7
3.2±1.8 28.3±11.3 7.2±0.4 50.8±0.1 58.5±4.3 39.6±12.5 20.4±0.3 69.6±2.3
1.0±0.4
31.4±5.7
86.6±1.5
10.9±3.5 14.1±1.3 13.0±1.0
48.3±5.4
X-Decoder-Seg+ (B)
10
0.26M 41.7 14.2±1.7 24.9±4.3 5.5±0.1 66.5±2.7 36.5±0.4 68.0±5.8 69.3±2.6 69.8±9.4 28.5±0.5 24.9±6.0 94.2±0.6 46.4±7.1 9.5±0.0 50.5±0.1 70.3±3.8 52.4±2.2 17.9±0.0 59.5±5.2 16.0±12.1 37.6±6.1
87.6±0.5
13.4±0.3 13.1±3.3 11.8±0.7
51.8±3.2
X-Decoder-Seg+ (B) 3×All 0.26M 40.3 13.5±0.2 21.8±1.2 3.3±0.5 33.0±0.0 40.9±0.9 68.1±6.8 73.2±0.0 73.0±5.4 30.3±0.6 11.6±3.2 26.1±14.4 49.2±2.1 11.4±0.0 50.8±0.1 81.6±0.8 35.6±5.0 20.7±0.9 64.7±1.2 59.2±0.7 51.7±0.8
90.5±0.8
12.8±1.2 15.5±0.3 17.5±0.5
50.6±0.4
X-Decoder (B)
0
0.0M 27.7
13.0
45.9
0.3
13.6
36.8
4.2
68.0
76.7
30.2
19.4
20.6
18.5
6.7
51.7
53.1
8.9
5.6
55.4
0.8
18.2
81.6
8.0
13.9
27.3
13.0
X-Decoder (B)
1
0.26M 27.7 13.0±0.0 45.9±0.0 0.3±0.0 13.6±1.1 36.8±0.0
4.2±0.0
68.0±0.0 76.7±0.0 30.2±0.0 19.4±0.0 20.6±0.0 18.5±0.0 6.7±5.8 51.7±0.0 53.0±4.6
9.9±1.6
5.6±0.0
55.4±0.0
0.8±0.0
18.2±0.0
81.6±0.0
8.0±0.0 13.9±0.0 27.3±0.0
13.0±0.0
X-Decoder (B)
3
0.26M 31.9 13.1±0.4 47.4±3.9 0.3±0.1 18.9±9.2 38.1±2.2
9.4±5.4
69.6±1.4 76.8±0.0 30.9±1.1 19.4±0.0 84.9±1.7 32.2±23.7 8.2±1.3 52.8±1.9 53.0±4.6 10.9±2.0
5.3±0.7
55.4±0.0
0.8±0.0
18.5±1.1
81.6±0.0
8.5±1.3 15.5±2.6 27.4±0.2
17.5±1.2
X-Decoder (B)
5
0.26M 35.4 12.6±0.5 48.7±1.8 1.0±0.8 19.0±16.0 37.1±1.2 21.3±21.3 67.9±1.8 79.4±3.6 32.2±1.7 32.2±3.0 82.8±7.8 63.4±5.0 8.6±0.2 53.7±2.7 65.6±10.9 17.8±6.9
5.0±1.4
54.2±2.1
1.2±0.4
21.0±1.3
81.6±10.0
8.6±3.3 18.2±6.6 30.5±0.6
19.2±3.2
X-Decoder (B)
10
0.26M 41.0 13.7±0.8 42.4±2.8 4.0±2.2 50.8±3.8 40.3±0.8 70.9±6.9 68.8±2.9 78.1±0.6 30.8±6.4 40.4±9.1 76.6±23.9 63.0±3.6 10.7±0.6 60.6±0.6 69.8±1.7 21.0±20.7 5.6±0.0
55.4±1.4
4.4±2.1
27.8±5.4
88.3±2.1
22.4±8.9 14.2±2.7 31.1±2.3
31.4±9.1
X-Decoder (B)
3×All 0.26M 44.7 13.0±0.0 43.8±2.8 3.3±0.0 15.4±3.1 36.5±1.2 69.3±9.3 72.2±0.5 79.6±1.1 34.0±0.7 38.9±1.0 89.4±3.3 74.8±0.9 14.1±0.2 57.9±1.9 84.0±0.4 17.8±3.1
5.1±0.4
55.9±0.3 57.5±1.8 48.4±0.4
90.0±0.1
18.4±0.2 21.0±4.0 38.3±0.5
37.0±0.5
X-Decoder (L-IN21K)
0
0.0M 26.7
12.3
43.2
0.5
3.5
12.3
18.8
63.9
79.1
24.3
15.6
0.0
20.3
4.9
50.5
58.8
43.4
13.4
57.3
1.3
12.3
74.4
6.9
14.6
20.1
13.5
X-Decoder (L-IN21K)
1
0.26M 25.9 12.3±0.0 28.8±24.9 0.5±0.0 3.5±0.0
12.3±1.1 18.8±0.0 63.9±4.6 79.1±0.0 24.3±0.0 15.6±1.1
0.0±0.0
20.3±0.0 4.9±0.0 50.5±0.0 58.8±0.0 43.4±0.0 11.2±3.8 57.3±0.0
1.3±0.0
12.3±0.0
74.4±0.0
5.4±2.5 14.6±0.0 20.1±0.0
13.5±0.0
X-Decoder (L-IN21K)
3
0.26M 29.1 12.6±2.1 44.7±2.6 1.1±0.9 8.0±11.0 15.7±3.0 32.7±24.0 63.9±4.6 76.7±4.4 24.5±0.4 15.6±1.1 30.2±52.4 16.9±5.9 6.8±2.0 51.0±0.7 61.1±4.0 43.0±3.8 14.6±5.8 57.5±3.1
1.4±0.1
12.3±0.0
74.0±0.7
4.4±2.7 14.8±0.3 20.8±1.2
21.0±10.7
X-Decoder (L-IN21K)
5
0.26M 33.7 13.9±1.0 46.4±5.9 2.1±2.0 9.5±8.5
31.4±1.3 52.6±12.3 64.1±0.6 78.0±6.2 32.9±1.2 19.2±8.2 71.0±24.3 26.3±20.1 7.5±0.8 54.9±4.7 66.6±1.7 32.6±9.3 10.3±9.2 59.0±1.6
1.3±0.6
15.5±8.3
80.1±5.5
3.0±3.6 14.0±5.0 26.6±9.2
22.3±8.5
X-Decoder (L-IN21K)
10
0.26M 35.1 10.4±1.5 39.1±7.2 4.4±1.1 31.7±8.5 24.7±11.7 55.8±6.0 61.4±6.1 73.9±7.1 28.6±1.8 17.5±10.2 85.4±7.0 40.8±28.3 6.4±2.1 58.4±2.9 54.2±4.3 32.2±19.3 13.4±0.0 40.2±13.5 2.2±2.1 20.8±14.8
81.0±2.0
17.9±15.8 17.6±3.1 26.4±4.4
31.9±9.4
X-Decoder (L-IN21K) 3×All 0.26M 44.5 12.1±0.8 57.0±0.4 1.5±0.5 4.9±2.3
41.4±1.0 74.7±2.9 70.3±0.4 79.1±2.6 36.6±0.7 23.6±3.8 54.6±2.3 70.0±1.6 12.7±0.1 60.1±0.0 86.1±0.3 43.1±2.1
5.2±2.2
59.7±0.9 46.6±0.8 52.0±0.4
91.0±0.0
23.0±5.5 22.7±1.7 43.8±0.5
40.5±1.3
X-Decoder (L)
0
0.0M 32.3
13.1
42.1
2.2
8.6
44.9
7.5
66.0
79.2
33.0
11.6
75.9
42.1
7.0
53.0
68.4
15.6
20.1
59.0
2.3
19.0
67.1
22.5
9.9
22.3
13.8
X-Decoder (L)
1
0.26M 32.3 13.1±0.0 42.1±0.0 2.2±0.0 8.6±0.0
44.9±0.0
7.5±0.0
66.0±0.0 79.2±0.0 33.0±0.0 11.6±1.1 75.9±0.0 42.1±0.0 7.0±0.0 53.0±0.0 68.4±0.0 15.6±1.1 20.1±0.0 59.0±0.0
2.3±0.0
19.0±0.0
67.1±0.0
22.5±2.3 9.9±0.0 22.3±0.0
13.8±0.0
X-Decoder (L)
3
0.26M 33.2 12.9±0.4 45.9±6.4 1.8±0.6 8.6±0.0
44.9±0.0
7.5±0.0
66.0±0.0 79.2±0.0 33.0±0.0 13.2±2.7 75.9±0.0 42.1±0.0 7.2±0.3 53.0±0.0 68.4±0.0 18.1±0.9 22.4±1.9 59.0±0.0
2.3±0.0
19.8±1.3
67.1±0.0
26.0±6.0 9.6±0.4 25.8±6.0
18.3±5.1
X-Decoder (L)
5
0.26M 35.9 12.5±0.5 44.9±2.2 2.4±2.5 28.4±6.8 44.9±1.0 15.7±1.6 67.1±2.0 77.1±0.2 36.3±0.7 9.8±8.7
93.1±0.9 45.6±7.5 7.6±1.0 53.0±0.8 71.3±3.1 19.4±3.5 22.5±1.7 55.8±2.2
2.4±0.5
12.0±2.1
78.4±6.0
30.1±6.3 10.0±1.3 30.1±1.7
25.9±2.8
X-Decoder (L)
10
0.26M 40.3 14.0±1.5 33.7±14.6 4.4±5.9 41.2±3.0 44.6±1.7 73.0±2.2 68.8±5.5 79.4±1.8 39.2±3.4 17.5±4.7 93.9±0.6 53.9±3.1 8.8±3.0 52.5±2.5 77.3±0.9 24.0±7.0 20.1±0.0 55.3±0.7
3.0±1.6
15.0±6.6
72.7±16.1
39.1±7.8 9.0±5.5 32.1±2.4
32.5±9.4
X-Decoder (L)
3×All 0.26M 44.7 13.6±0.3 49.3±0.6 4.2±1.1 8.8±0.4
44.6±0.3 51.1±0.8 72.8±0.6 78.9±1.2 42.0±0.0 13.8±3.1 90.2±0.6 67.8±0.3 11.8±0.1 52.7±0.4 84.3±0.0 18.9±1.2 21.6±1.6 54.3±2.3 56.8±3.5 50.4±0.6
90.7±0.2
42.1±2.7 11.1±1.0 39.7±1.2
44.3±2.9
Table 15. SegInW results with tuning on X-Decoder for different image shots and backbone architectures. (0.26M parameters tuned in the setting.)
19

0
100000
200000
300000
400000
coffee table
arcade machine
plaything
trade name
crt screen
chest of drawers
minibike
hovel
trash can
traffic light
swivel chair
grandstand
conveyer belt
dirt track
bulletin board
sconce
screen door
cradle
kitchen island
pool table
signboard
buffet
armchair
escalator
ottoman
wardrobe
bookcase
stairway
radiator
dishwasher
booth
chandelier
street lamp
countertop
barrel
canopy
stool
awning
skyscraper
cushion
hood
blind
fountain
column
pier
tent
microwave
fireplace
washer
refrigerator
rug
monitor
sofa
sculpture
tub
stairs
tank
blanket
bannister
tray
stove
case
land
palm
path
curtain
clothes
pool
step
towel
ceiling
cabinet
van
shelf
fan
rail
bicycle
screen
basket
pillow
ship
runway
base
pot
counter
vase
lamp
tv
falls
desk
sink
seat
bar
mirror
stage
sea
hill
sidewalk
computer
toilet
poster
bottle
lake
bag
book
flag
sand
animal
bench
tower
truck
box
food
plant
bridge
fence
plane
earth
bed
river
clock
painting
chair
mountain
ball
bus
door
shower
boat
floor
pole
rock
field
plate
road
grass
glass
flower
house
table
oven
light
car
wall
sky
water
window
building
tree
person
cc
coco
sbu
vg
Figure 10. Image captions overlap with ADE20K-150
0
100000
200000
300000
400000
ego vehicle
polegroup
traffic sign
traffic sign frame
traffic device
parking sign
lane divider
traffic light
street light
rail track
traffic cone
guard rail
caravan
dynamic
terrain
billboard
tunnel
garage
banner
trailer
bicycle
vegetation
static
sidewalk
motorcycle
truck
bridge
fence
rider
bus
ground
pole
road
parking
train
car
wall
sky
building
person
cc
coco
sbu
vg
Figure 11. Image captions overlap with BDD-Panoptic
0
100000
200000
300000
400000
traffic sign
traffic light
terrain
bicycle
vegetation
sidewalk
motorcycle
truck
fence
rider
bus
pole
road
train
car
wall
sky
building
person
cc
coco
sbu
vg
Figure 12. Image captions overlap with BDD-Semantic/Cityscapes
0
100000
200000
300000
400000
pottedplant
tvmonitor
diningtable
aeroplane
motorbike
sofa
bicycle
sheep
bottle
cow
chair
horse
bird
bus
boat
cat
dog
train
car
person
cc
coco
sbu
vg
Figure 13. Image captions overlap with Pascal VOC
0
100000
200000
300000
400000
hair drier
door-stuff
floor-wood
mirror-stuff
playingfield
wall-brick
wall-stone
wall-tile
wall-wood
window-blind
potted plant
sports ball
traffic light
parking meter
stop sign
dining table
teddy bear
baseball glove
handbag
wine glass
baseball bat
cardboard
toothbrush
toaster
gravel
scissors
hot dog
railroad
tent
microwave
fire hydrant
refrigerator
rug-merged
net
spoon
backpack
platform
carrot
stairs
broccoli
tennis racket
mouse
remote
cell phone
blanket
banner
knife
donut
suitcase
fork
avement-merged
curtain
banana
snowboard
keyboard
sandwich
towel
ceiling-merged
cabinet-merged
frisbee
couch
shelf
apple
tie
bicycle
pillow
counter
vase
fruit
dirt-merged
surfboard
tv
airplane
sink
cup
sheep
umbrella
laptop
roof
sea
kite
zebra
bowl
motorcycle
toilet
skateboard
cake
paper-merged
bottle
cow
book
sand
bench
pizza
truck
od-other-merged
bridge
elephant
fence-merged
giraffe
bed
river
clock
bear
chair
skis
mountain-merged
horse
bird
bus
orange
boat
oor-other-merged
rock-merged
snow
cat
road
grass-merged
dog
flower
house
train
table-merged
oven
light
car
all-other-merged
ky-other-merged
water-other
window-other
ng-other-merged
tree-merged
person
cc
coco
sbu
vg
Figure 14. Image captions overlap with COCO
0
100000
200000
300000
400000
potted plant
bed clothes
dining table
tv monitor
aeroplane
side walk
motorbike
shelves
cloth
sofa
platform
mouse
curtain
keyboard
ceiling
cabinet
bicycle
cup
sheep
computer
track
bottle
cow
bag
book
bench
truck
food
fence
bed
chair
mountain
horse
bird
bus
door
ground
boat
floor
rock
snow
plate
cat
road
grass
dog
flower
wood
train
light
car
wall
sign
sky
water
window
building
tree
person
cc
coco
sbu
vg
Figure 15. Image captions overlap with Pascal Context-59
0
100000
200000
300000
400000
otherstructure
otherfurniture
otherprop
floor mat
whiteboard
refridgerator
night stand
shower curtain
bookshelf
bathtub
shelves
blinds
sofa
television
curtain
clothes
towel
ceiling
cabinet
pillow
counter
lamp
desk
sink
mirror
toilet
paper
bag
books
box
bed
dresser
chair
picture
door
floor
table
wall
window
person
cc
coco
sbu
vg
Figure 16. Image captions overlap with ScanNet-40
20

Figure 17. Zero-Shot Video Generic Segmentation. (Source: YoutubeVOS videos)
Figure 18. Zero-Shot Referring Video Segmentation. (Source: YoutubeVOS videos)
21

Figure 19. Zero-Shot Image Captioning. (Source: YoutubeVOS videos)
Figure 20. Referring Captioning. (Source: COCO 2017 val images)
Figure 21. Referring Image Inpainting. (Source: web images)
22

