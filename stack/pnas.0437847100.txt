Optimally sparse representation in general
(nonorthogonal) dictionaries via 1 minimization
David L. Donoho†‡ and Michael Elad§
Departments of †Statistics and §Computer Science, Stanford University, Stanford, CA 94305
Contributed by David L. Donoho, December 20, 2002
Given a dictionary D  {d k} of vectors d k, we seek to represent a signal
S as a linear combination S  k (k)d k, with scalar coefﬁcients (k).
In particular, we aim for the sparsest representation possible. In
general, this requires a combinatorial optimization process. Previous
work considered the special case where D is an overcomplete system
consisting of exactly two orthobases and has shown that, under a
condition of mutual incoherence of the two bases, and assuming that
S has a sufﬁciently sparse representation, this representation is
unique and can be found by solving a convex optimization problem:
speciﬁcally, minimizing the 1 norm of the coefﬁcients . In this article,
we obtain parallel results in a more general setting, where the
dictionary D can arise from two or several bases, frames, or even less
structured systems. We sketch three applications: separating linear
features from planar ones in 3D data, noncooperative multiuser
encoding, and identiﬁcation of over-complete independent compo-
nent models.
W
orkers throughout engineering and the applied sciences
frequently want to represent data (signals, images) in the
most parsimonious terms. In signal analysis specifically, they often
consider models proposing that the signal of interest is sparse in
some transform domain, such as the wavelet or Fourier domain (1).
However, there is a growing realization that many signals are
mixtures of diverse phenomena, and no single transform can be
expected to describe them well; instead, we should consider models
making sparse combinations of generating elements from several
different transforms (1–4). Unfortunately, as soon as we start
considering general collections of generating elements, the attempt
to find sparse solutions enters mostly uncharted territory, and one
expects at best to use plausible heuristic methods (5–8) and
certainly to give up hope of rigorous optimality. In this article, we
will develop some rigorous results showing that it can be possible
to find optimally sparse representations by efficient techniques in
certain cases.
Suppose we are given a dictionary D of generating ele-
ments {dk}k1
L
, each one a vector in CN, which we assume normal-
ized: dk
Hdk  1. The dictionary D can be viewed a matrix of size N 
L, with generating elements for columns. We do not suppose any
fixed relationship between N and L. In particular, the dictionary can
be overcomplete and contain linearly dependent subsets, and in
particular need not be a basis. As examples of such dictionaries, we
can mention: wavelet packets and cosine packets dictionaries of
Coifman et al. (3), which contain L  N log(N) elements, repre-
senting transient harmonic phenomena with a variety of durations
and locations; wavelet frames, such as the directional wavelet
frames of Ron and Shen (9), which contain L  CN elements for
various constants C  1; and the combined ridgeletwavelet
systems of Starck, Cande`s, and Donoho (8, 10). Faced with such
variety, we cannot call individual elements in the dictionary basis
elements; we will use the term atom instead; see refs. 2 and 11
for elaboration on the atomsdictionary terminology and other
examples.
Given a signal S  CN, we seek the sparsest coefficient vector 
in CL such that D  S. Formally, we aim to solve the optimization
problem
P0
Minimize 0
subject to
S  D.
[1]
Here the 0 norm 0 is simply the number of nonzeros in .
While this problem is easily solved if there happens to be a
unique solution D  S among general (not necessarily sparse)
vectors , such uniqueness does not hold in any of our applica-
tions; in fact, these mostly involve L  N, where such unique-
ness is, of course, impossible. In general, solution of (P0) requires
enumerating subsets of the dictionary looking for the smallest
subset able to represent the signal; of course, the complexity of
such a subset search grows exponentially with L.
It is now known that in several interesting dictionaries, highly
sparse solutions can be obtained by convex optimization; this has
been found empirically (2, 12) and theoretically (13–15). Consider
replacing the 0 norm in (P0) by the 1 norm, getting the minimi-
zation problem
P1
Minimize 1
subject to
S  D.
[2]
This can be viewed as a kind of convexification of (P0). For example,
over the set of signals that can be generated with coefficient
magnitudes bounded by 1, the 1 norm is the largest convex function
less than the 0 norm; (P1) is in a sense the closest convex
optimization problem to (P0).
Convex optimization problems are extremely well studied (16),
and this study has led to a substantial body of algorithmic knowl-
edge and high-quality software. In fact, (P1) can be cast as a linear
programming problem and solved by modern interior point meth-
ods (2), even for very large N and L.
Given the tractability of (P1) and the general intractibility of (P0),
it is perhaps surprising that for certain dictionaries solutions to (P1),
when sufficiently sparse, are the same as the solutions to (P0).
Indeed, results in refs. 13–15 show that for certain dictionaries, if
there exists a highly sparse solution to (P0) then it is identical to the
solution of (P1); and, if we obtain the solution of (P1) and observe
that if it happens to be sparse beyond a certain specific threshold,
then we know (without checking) that we have also solved (P0).
Previous work studying this phenomenon (13–15) considered
dictionaries D built by concatenating two orthobases  and , thus
giving that L  2N. For example, one could let  be the Fourier
basis and  be the Dirac basis, so that we are trying to represent
a signal as a combination of spikes and sinusoids. In this dictionary,
the latest results in ref. 15 show that if a signal is built from
0.914N spikes and sinusoids, then this is the sparsest possible
representation by spikes and sinusoids, i.e., the solution of (P0), and
it is also necessarily the solution found by (P1).
We are aware of numerous applications where the dictionaries
would not be covered by the results just mentioned. These include:
cases where the dictionary is overcomplete by more than a factor 2
(i.e., L  2N), and where we have a collection of nonorthogonal
elements, for example, a frame or other system. In this article, we
consider general dictionaries D and obtain results paralleling those
in refs. 13–15. We then sketch applications in the more general
dictionaries.
We address two questions for a given dictionary D and signal S:
(i) uniqueness, under which conditions is a highly sparse represen-
tation necessarily the sparsest possible representation; and (ii)
‡To whom correspondence should be addressed. E-mail: donoho@stat.stanford.edu.
www.pnas.orgcgidoi10.1073pnas.0437847100
PNAS 
March 4, 2003 
vol. 100 
no. 5 
2197–2202
ENGINEERING
Downloaded from https://www.pnas.org by 159.2.218.54 on April 15, 2023 from IP address 159.2.218.54.

equivalence, under which conditions is a highly sparse solution to
the (P0) problem also necessarily the solution to the (P1) problem?
Our analysis avoids reliance on the orthogonal subdictionary
structure in refs. 13–15; our results are just as strong as those of ref.
13 while working for a much wider range of dictionaries. We base
our analysis on the concept of the Spark of a matrix, the size of the
smallest linearly dependent subset. We show that uniqueness
follows whenever the signal S is built from fewer than Spark(D)2
atoms. We develop two bounds on Spark that yield previously
known inequalities in the two-orthobasis case but can be used more
generally. The simpler bound involves the quantity M(G), the
largest off-diagonal entry in the Gram matrix G  DHD. The more
general bound involves 1(G), the size of the smallest group of
nondiagonal magnitudes arising in a single row or column of G and
having a sum 1. We have Spark(D)  1  1M. We also show,
in the general dictionary case, that one can determine a threshold
on sparsity whereby every sufficiently sparse representation of S
must be the unique minimal 1 representation; our thresholds
involve M(G) and a quantity 1/2(G) related to 1.
We sketch three applications of our results.
Y Separating Points, Lines, and Planes: Suppose we have a data
vector S representing 3D volumetric data. It is supposed that the
volume contains some number of points, lines, and planes in
superposition. Under what conditions can we correctly identify
the lines and planes and the densities on each? This is a caricature
of a timely problem in extragalactic astronomy (17).
Y Robust Multiuser Private Broadcasting: J different individuals
encode information by superposition in a single signal vector S
that is intended to be received by a single recipient. The encoded
vector must look like random noise to anyone but the intended
recipient (including the transmitters), it must be decodable
perfectly, and the perfect decoding must be robust against
corruption of some limited number of entries in the vector. We
present a scheme that achieves these properties.
Y Blind Identification of Sources: Suppose that a random signal S
is a superposition of independent components taken from an
overcomplete dictionary D. Without assuming sparsity of this
representation, we cannot in general know which atoms from D
are active in S or what there statistics are. However, we show that
it is possible, without any sparsity, to determine the activity of the
sources, exploiting higher-order statistics.
Note: After presenting our work publicly we learned of related
work about to be published by R. Gribonval and M. Nielsen (18).
Their work initially addresses the same question of obtaining the
solution of (P0) by instead solving (P1), with results paralleling ours.
Later, their work focuses on analysis of more special dictionaries
built by concatenating several bases.
The Two-Orthobasis Case
Consider a special type of dictionary: the concatenation of two
orthobases  and , each represented by N  N unitary matrices;
thus D  [, ]. Let i and  j (1  i, j  N) denote the elements
in the two bases. Following ref. 13 we define the mutual incoher-
ence between these two bases by
M,   Sup	
 i,  j, 1  i, j  N.
It is easy to show (13, 15) that 1N  M  1; the lower bound
is attained by a basis pair such as spikes and sinusoids (13) or spikes
and Hadamard–Walsh functions (15). The upper bound is attained
if at least one of the vectors in  is also found in . A condition
for uniqueness of sparse solutions to (P0) can be stated in terms of
M; the following is proved in refs. 14 and 15.
Theorem 1: Uniqueness. A representation S  D is necessarily the
sparsest possible if 0  1M.
In words, having obtained a sparse representation of a signal, for
example by (P1) or by any other means, if the 0 norm of the
representation is sufficiently small (1M), we conclude that this
is also the (P0) solution. In the best case where 1M  N, the
sparsity requirement translates into 0  N.
A condition for equivalence of the solution to (P1) with that for
(P0) can also be stated in terms of M; see refs. 14 and 15.
Theorem 2: Equivalence. If there exists any sparse representation of S
satisfying 0  (2  0.5)M, then this is necessarily the unique
solution of (P1).
Note the slight gap between the criteria in the above two
theorems. Recently, Feuer and Nemirovsky (19) managed to prove
that the bound in the equivalence theorem is sharp, so this gap is
unbridgeable.
To summarize, if we solve the (P1) problem and observe that it
is sufficiently sparse, we know we have obtained the solution to (P0)
as well. Moreover, if the signal S has sparse enough representation
to begin with, (P1) will find it.
These results correspond to the case of dictionaries built from
two orthobases. Both theorems immediately extend in a limited way
to nonorthogonal bases  and  (13). We merely replace M by the
quantity M˜ in the statement of both results:
M˜  Sup	1i,j, 1i,j, 1  i, j  N.
However, as we may easily have M˜  1, this is of limited value.
A Uniqueness Theorem for Arbitrary Dictionaries
We return to the setting of the Introduction; the dictionary D is
a matrix of size N  L, with normalized columns {dk}k1
L
. An
incoming signal vector S is to be represented by using the dictionary
by S  D. Assume that we have two different suitable represen-
tations, i.e.
	 1 
 2  S  D1  D2.
[3]
Thus the difference of the representation vectors,   1  2,
must be in the null space of the representation: D(1  2) 
D  0. Hence some group of columns from D must be linearly
dependent. To discuss the size of this group we introduce
terminology.
Deﬁnition 1, Spark: Given a matrix A we define   Spark(A) as
the smallest possible number such that there exists a subgroup of 
columns from A that are linearly dependent.
Clearly, if there are no zero columns in A, then   2, with
equality only if two columns from A are linearly dependent. Note
that, although spark and rank are in some ways similar, they are
totally different. The rank of a matrix A is defined as the maximal
number of columns from A that are linearly independent, and its
evaluation is a sequential process requiring L steps. Spark, on the
other hand, is the minimal number of columns from A that are
linearly dependent, and its computation requires a combinato-
rial process of complexity 2L steps. The matrix A can be of full
rank and yet have   2. At the other extreme we get that  
Min{L, Rank(A)  1}.
As an interesting example, let us consider a two-ortho basis case
made of spikes and sinusoids. Here D  [, ],   IN the identity
matrix of size N  N, and   FN the discrete Fourier transform
matrix of size N  N. Then, supposing N is a perfect square, using
the Poisson summation formula we can show (13) that there is a
group of 2N linearly-dependent vectors in this D, and so
Spark(D)  2N. As we shall see later, Spark(D)  2N.
We now use the Spark to bound the sparsity of nonunique
representations of S. Let   Spark(D). Every nonuniqueness pair
(1, 2) generates   1  2 in the column null-space; and
D  0 f 1  20  0  .
[4]
2198 
www.pnas.orgcgidoi10.1073pnas.0437847100
Donoho and Elad
Downloaded from https://www.pnas.org by 159.2.218.54 on April 15, 2023 from IP address 159.2.218.54.

On the other hand, the 0 pseudo-norm obeys the triangle inequal-
ity 1  20  10  20. Thus, for the two supposed
representations of S, we must have
 10   20  .
[5]
Formalizing matters:
Theorem 3: Sparsity Bound. If a signal S has two different represen-
tations S  D1  D2, the two representations must have no less than
Spark(D) nonzero entries combined.
From Eq. 5, if there exists a representation satisfying 10  2,
then any other representation 2 must satisfy 20  2, implying
that 1 is the sparsest representation. This gives:
Corollary 1: Uniqueness. A representation S  D is necessarily the
sparsest possible if 0  Spark(D)2.
We note that these results are sharp, essentially by definition.
Lower Bounds on Spark(D). Let G  DHD be the Gram matrix of D.
As every entry in G is an inner product of a pair of columns from
the dictionary, the diagonal entries are all 1, owing to our normal-
ization of D’s columns; the off-diagonal entries have magnitudes
between 0 and 1.
Let S  {k1, . . . , ks} be a subset of indices, and suppose the
corresponding columns of D are linearly independent. Then the
corresponding leading minor GS  (Gki, kj : i, j  1, . . . , s) is positive
definite (20). The converse is also true. This proves
Lemma 1.   Spark(D) if and only if every (  1)  (  1) leading
minor of G is positive definite.
To apply this criterion, we use the notion of diagonal dominance
(20): If a symmetric matrix A  (Aij) satisfies Aii  ji Aij for every
i, then A is positive definite. Applying this criterion to all principal
minors of the Gram matrix G leads immediately to a lower bound
for Spark(G).
Theorem 4: Lower-Bound Using 1. Given the dictionary D and its
Gram matrix G  DHD, let 1(G) be the smallest integer m such that
at least one collection of m off-diagonal magnitudes arising within a
single row or column of G sums at least to 1. Then Spark(D)  1(G).
Indeed, by hypothesis, every 1  1 principal minor has its
off-diagonal entries in any single row or column summing to 1.
Hence every such principal minor is diagonally dominant, and
hence every group of 1 columns from D is linearly independent.
For a second bound on Spark, define M(G)  maxij Gi,j, giving
a uniform bound on off-diagonal elements. Note that the earlier
quantity M(, ) defined in the two-orthobasis case agrees with
the new quantity. Now consider the implications of M  M(G) for
1(G). Since we have to sum at least 1M off-diagonal entries of size
M to reach a cumulative sum equal or exceeding 1, we always have
1(G)  1M(G). This proves
Theorem 5: Lower Bound Using M. If the Gram matrix has all
off-diagonal entries  M,
SparkD  1M.
[6]
Combined with our earlier observations on Spark, we have: a
representation with fewer than (1  1M)2 nonzeros is necessarily
maximally sparse.
Refinements. In general, Theorems 4 and 5 can contain slack, in two
ways. First, there may be additional dictionary structure to be
exploited. For example, return to the special two-orthobasis case of
spikes and sinusoids: D  [, ],   IN and   FN. Here M 
1N, and so the theorem says that Spark(D)  N. In fact, the
exact value is 2N, so there is room to do better. This improvement
follows directly from the uncertainty theorem given in refs. 14 and
15, leading to Spark(D)  2M. The reasoning is general and
applies to any two-orthobasis dictionary, giving
Theorem 6: Improved Lower Bound, Two Orthobasis Case. Given a
two-orthobasis dictionary with mutual coherence value at most M,
Spark(D)  2M.
This bound, together with Corollary 1, gives precisely theorem 1
in refs. 14 and 15. Returning again to the spikessinusoids example,
from M  1N we have Spark(D)  2N. On the other hand,
if N is a perfect square, the spike train example provides a
linearly-dependent subset of 2N atoms, and so in fact   2N.
Second, Theorem 5 can contain slack simply because the metric
information in the Gram matrix is not sufficiently expressive on
linear dependence issues. We may construct examples where
Spark(D)  1M(G). Our bounds measure diagonal dominance
rather than positive definiteness, and this is well known to introduce
a gap in a wide variety of settings (20).
For a specific example, consider a two-basis dictionary made of
spikes and exponentially decaying sinusoids (13). Thus, let   IN
and, for a fixed   (0, 1), construct the nonorthogonal basis N
() 
{k
}, where k
()(i)  i exp{1(2kN)i}. Let S1 and S2 be
subsets of indices in each basis. Now suppose this pair of subsets
generates a linear dependency in the combined representation, so
that, for appropriate N vectors of coefficients  and ,
0  
kS1
kki  
kS2
kk
i,
i.
Then, as k
()(i)  ik
(0)(i) it is equally true that
i0  
kS1
kkki  
kS2
kk
0i,
i.
Hence, there would have been a linear dependency in the two-
orthobasis case [IN, FN], with the same subsets and with coefficients
˜(k)  k(k), @k, and ˜(k)  (k). In short, we see that
SparkIN, N
  SparkIN, FN.
On the other hand, picking  close to zero, we get that the basis
elements in N
() decay so quickly that the decaying sinusoids begin
to resemble spikes; thus M(IN, N
()) 3 1 as  3 0.
This is a special case of a more general phenomenon, invariance
of the Spark under row and column scaling, combined with
noninvariance of the Gram matrix under row scaling. For any
diagonal nonsingular matrices W1 (N  N) and W2 (L  L), we have
SparkW1DW2  SparkD,
while the Gram matrix G  G(D) exhibits no such invariance; even
if we force normalization so that diag(G)  IL, we have only
invariance against the action of W2 (column scaling), but not W1
(row scaling). In particular, while the positive definiteness of any
particular minor of G(W1D) is invariant under such row scaling, the
diagonal dominance is not.
Upper Bounds on Spark. Consider a concrete method for evaluating
Spark(D). We define a sequence of optimization problems, for k 
1, . . . , L:
Rk
Minimize 0
subject to
D  0
k  1.
[7]
Letting k
0, denote the solution of problem (Rk), we get
Spark(D)  Min1  k  Lk
0,0.
[8]
The implied 0 optimization is computationally intractable. Alter-
natively, any sequence of vectors obeying the same constraints
furnishes an upper bound. To obtain such a sequence, we replace
Donoho and Elad
PNAS 
March 4, 2003 
vol. 100 
no. 5 
2199
ENGINEERING
Downloaded from https://www.pnas.org by 159.2.218.54 on April 15, 2023 from IP address 159.2.218.54.

minimization of the l0 norm by the more tractable 1 norm. Define
the sequence of optimization problems for k  1, 2, . . . , L:
Qk
Minimize 1
subject to
D  0
k  1.
[9]
(The same sequence of optimization problems was defined in ref.
13, which noted the formal identity with the definition of analytic
capacities in harmonic analysis, and called them capacity problems.)
The problems can in principle be tackled straightforwardly by using
linear programming solvers. Denote the solution of the (Qk)
problem by k
1,. Then, clearly k
0,0  k
1,0, so
SparkD  Min1  k  Lk
1,0 .
[10]
An Equivalence Theorem for Arbitrary Dictionaries
We now set conditions under which, if a signal S admits a highly
sparse representation in the dictionary D, the 1 optimization
problem (P1) must necessarily obtain that representation.
Let 0 denote the unique sparsest representation of S, so D0 
S. For any other 1 providing a representation (i.e. D1  S) we
therefore have 10  00. To show that 0 solves (P1), we need
to show that 11  01. Consider the reformulation of (P1):
Minimize
1
11  01
subject to
D1  D0  S.
[11]
If the minimum is no less than zero, the 1 norm for any alternate
representation is at least as large as for the sparsest representation.
This in turn means that the (P1) problem admits 0 as a solution. If
the minimum is uniquely attained, then the minimizer of (P1) is 0.
As in refs. 13 and 15, we develop a simple lower bound. Let S
denote the support of 0. Writing the difference in norms in Eq. 11
as a difference in sums, and relabeling the summations, gives

k1
L
1k  
k1
L
0k 
Sc
1k 
S
1k  0k

Sc
1k 
S
1k  0k 
Sc
k 
S
k,
[12]
where we introduced the vector   1  0. Note that the right side
of this display is positive only if

S
k 
1
2 1,
i.e., only if at least 50% of the magnitude in  is concentrated in S.
This motivates a constrained optimization problem:
CS
Maximize
S
k
to
D  0,
1  1.
[13]
This new problem is closely related to Eq. 11 and hence also to (P1).
It has this interpretation: if val(CS)  1
2, then every direction of
movement away from 0 that remains a representation of S (i.e.,
stays in the nullspace) causes a definite increase in the objective
function for Eq. 11. Recording this formally,
Lemma 2: Less than 50% Concentration Implies Equivalence. If
supp(0)  S, and val(CS)  1
2, then 0 is the unique solution of (P1)
from data S  D0.
On the other hand, if val(CS)  1
2, it can happen that 0 is not a
solution to (P1).
Lemma 3. If val(CS)  1
2, there exists a vector 0 supported in S, and
a corresponding signal S  D0, such that 0 is not a unique minimizer
of (P1).
Proof: Given a solution * to problem (CS) with value  1
2, pick
any 0 supported in S with sign pattern arranged so that
sign(*(k)0(k))  0 for k  S. Then consider 1 of the general form
1  0  t*, for small t  0. Equality must hold in Eq. 12 for
sufficiently small t, showing that 11  01 and so 0 is not the
unique solution of (P1) when S  D0.
We now bring the Spark into the picture.
Lemma 4. Let   Spark(D). There is a subset S* with #(S*)  2 
1 and val(CS*)  1
2.
Proof: Let S index a subset of dictionary atoms exhibiting linear
dependence. There is a nonzero vector  supported in S with D 
0. Let S* be the subset of S containing the #(S)2largest-
magnitude entries in . Then #(S*)  #(S)2  1 while

S*
k 
1
2 1.
It follows necessarily that val(CS*)  1
2.
In short, the size of Spark(D) controls uniqueness not only in the
0 problem, but also in the 1 problem. No sparsity bound 00 
s can imply (P0)–(P1) equivalence in general unless s  Spark(D)2.
Equivalence Using M(G). Theorem 7. Suppose that the dictionary D has
a Gram matrix G with off-diagonal entries bounded by M. If S  D0
where 00  (1  1M)2, then 0 is the unique solution of (P1).
We prove this by showing that if D  0, then for any set S of
indices,

S
k 
M#S
M  1 1.
[14]
Then because #(S)  (1  1M)2, it is impossible to achieve 50%
concentration on S, and so Lemma 2 gives our result.
To get Eq. 14, we recall the sequence of capacity problems
(Qk) introduced earlier. By their definition, if  obeys D  0,
k  valQk11.
It follows (see also ref. 13) that for any set S,

kS
k 
kS
valQk11.
Result 14 and hence Theorem 7 follow immediately on substituting
the following bound for the capacities:
Lemma 5. Suppose the Gram matrix has off-diagonal entries bounded
by M. Then
valQk  1M  1,
k  1, . . . , L.
To prove the lemma, we remark that as D  0 then also G 
DHD  0. In particular, (G)k  0, where k is the specific index
mentioned in the statement of the lemma. Now as k  1 by
definition of (Qk), and as Gk,k  1 by normalization, in order that
(G)k  0 we must have 1  Gk,kk  jk Gk,jj . Now
1 
jk
Gk,jj 
jk
Gk,j j  M
jk
j  M1  1.
Hence 1  1M  1, and the lemma follows.
2200 
www.pnas.orgcgidoi10.1073pnas.0437847100
Donoho and Elad
Downloaded from https://www.pnas.org by 159.2.218.54 on April 15, 2023 from IP address 159.2.218.54.

Equivalence Using 1/2(G). Recall the quantity 1 defined in our
analysis of the uniqueness problem in the previous section. We now
define an analogous concept relevant to equivalence.
Deﬁnition 2: For G a symmetric matrix, 1/2(G) denotes the
smallest number m such that some collection of m off-diagonal
magnitudes arising in a single row or column of G sums at
least to 1
2.
We remark that 1/2(G)  1
21(G)  1(G). Using this notion, we
can show:
Theorem 8. Consider a dictionary D with Gram matrix G. If S  D0
where 00  1/2(G), then 0 is the unique solution of (P1).
Indeed, let S be a set of size #(S)  1/2(G). We show that any
vector in the nullspace of D exhibits 50% concentration to S, so

S
k 
1
2 1,
@: D  0.
[15]
The theorem then follows from Lemma 2.
Note again that D  0 implies G  0. In particular, (G  I) 
. Let m  #S and let H  (Hi,k) denote the m  L matrix formed
from the rows of G  I corresponding to indices in S. Then (G 
I)   implies

S
k  H1.
[16]
As is well known, H, viewed as a matrix mapping L
1 into m
1 , has its
norm controlled by its maximum 1 column sum:
Hx1  H1,1x1
 x  RL,
where H1,1

maxk i1
m
Hi,k. By the assumption that
1/2(G)  #S,
max
j 
kS
Gk,j  1	kj 
1
2.
It follows that H(1,1)  1
2. Together with Eq. 16 this yields Eq. 15
and hence also the theorem.
Stylized Applications
We now sketch a few application scenarios in which our results may
be of interest.
Separating Points, Lines, and Planes. A timely problem in extraga-
lactic astronomy is to analyze 3D volumetric data and quantitatively
identify and separate components concentrated on filaments and
sheets from those scattered uniformly through 3D space; see ref. 17
for related references, and ref. 8 for some examples of empirical
success in performing such separations. It seems of interest to have
a theoretical perspective assuring us that, at least in an idealized
setting, such separation is possible in principle.
For sake of brevity, we work with specific algebraic notions of
digital line, digital point, and digital plane. Let p be a prime (e.g.,
127, 257) and consider a vector to be represented by a p  p  p
array S(i1, i2, i3)  S(i). As representers of digital points we consider
the Kronecker sequences Pointk(i)  1{k1i1,k2i2,k3i3} as k  (k1, k2,
k3) ranges over all triples with 0  ki  p. We consider a digital plane
Planep(j, k1, k2) to be the collection of all triples (i1, i2, i3) obeying
ij3  k1ij1  k2ij2 mod p, and a digital line Linep(io, k) to be the
collection of all triples i  i0  k mod p,   0, . . . , p.
By using properties of arithmetic mod p, it is not hard to show the
following:
Y Any digital plane contains p2 points.
Y Any digital line contains p points.
Y Any two digital lines are disjoint, or intersect in a single point.
Y Any two digital planes are parallel, or intersect in a digital line.
Y Any digital line intersects a digital plane not at all, in a single
point, or along the whole extent of the digital line.
Suppose now that we construct a dictionary D consisting of
indicators of points, of digital lines and of digital planes. These
should all be normalized to unit 2 norm. Let G denote the Gram
matrix. We make the observation that
MG  1p.
Indeed, if d1 and d2 are each normalized indicator functions
corresponding to subsets I1, I2  Zp
3, then

d 1, d 2 
I1  I2
I11/2I21/2 .
Using this fact and the above incidence estimates we get that
the inner product between two planes or two lines could not
exceed 1p, while an inner product between a plane and a line
or line and a point cannot exceed 1p. We obtain immediately:
Theorem 9. If the 3D array can be written as a superposition of p
digital planes, lines, and points, this is the unique sparsest represen-
tation, and it is also the unique minimal 1 decomposition.
This shows that there is at least some possibility to separate 3D
data rigorously into unique geometric components. It is in accord
with recent experiments in ref. 8 successfully separating data into
a relatively few such components.
Robust Multiuser Private Broadcasting. Consider now a stylized
problem in private digital communication. Suppose that J individ-
uals are to communicate privately and without coordination over a
broadcast channel with an intended recipient. The jth individual is
to create a signal Sj that is superposed with all the other individuals’
signals, producing S  j1
J
Sj. The intended recipient gets a copy
of S. The signals Sj are encoded in such a way that, to everyone but
the intended recipient, encoders included, S looks like white
Gaussian noise; participating individuals cannot understand each
other’s messages, but nevertheless the recipient is able to separate
S unambiguously into its underlying components Si and decipher
each such apparent noise signal into meaningful data. Finally, it is
desired that all this remain true even if the recipient gets a defective
copy of S that may be badly corrupted in a few entries.
Here is a way to approach this problem. The intended recipient
arranges things in advance so that each of the J individuals has a
private codebook Cj containing K different N vectors whose entries
are generated by sampling independent Gaussian white noises. The
codebook is known to the jth encoder and to the recipient. An
overall dictionary is created by concatenating these together with an
identity matrix, producing
D  IN, C1, . . . , CJ.
The jth individual creates a signal by selecting a random subset of
Ij indices in Cj and generating
S j 
iIj
i
jcj,i .
The positions of the indices i  Ij and coefficients i
j are the
meaningful data to be conveyed. Now each cj,i is the realization of
a Gaussian white noise; hence each Sj resembles such a white noise.
Individuals don’t know each other’s codebooks, so they are unable
to decipher each other’s meaningful data (in a sense that can be
made precise; compare ref. 21).
The recipient can extract all the meaningful data simply by
performing minimum 1 atomic decomposition in dictionary D.
This will provide a list of coefficients associated with spikes and with
Donoho and Elad
PNAS 
March 4, 2003 
vol. 100 
no. 5 
2201
ENGINEERING
Downloaded from https://www.pnas.org by 159.2.218.54 on April 15, 2023 from IP address 159.2.218.54.

the components associated with each codebook. Provided the
participants encode a sparse enough set of coefficients, the scheme
works perfectly to separate errors and the components of the
individual message.
To see this, note that by using properties of extreme values of
Gaussian random vectors (compare ref. 13) we can show that the
normalized dictionary obeys
MD  2
logKJ  N
KJ  N
1  N,
where N is a random variable that tends to zero in probability as
N 3 . Hence we have:
Theorem 10. There is a capacity threshold C(D) such that, if each
participant transmits an Sj using at most #(Ij)  C(D) coefficients and
if there are  C(D) errors in the recipient’s copy of Sˆ, then the
minimum 1 decomposition of Sˆ, precisely recovers the indices Ij and
the coefficients i
j. We have
J  1CD 
1
2M .
Identification of Overcomplete Blind Sources. Suppose that a statis-
tically independent set of random variables X  (Xi) generates an
observed data vector according to
Y  AX
[17]
where Y is the vector of observed data and A represents the coupling
of independent sources to observable sensors. Such models are
often called independent components models (22). We are inter-
ested in the case where A is known, but is overcomplete, in the sense
that L  N (many more sources than sensors, a realistic assump-
tion). We are not supposing that the vector X is highly sparse; hence
there is no hope in general to recover uniquely the components X
generating a single realization Y. We ask instead whether it is
possible to recover statistical properties of X ; in particular higher-
order cumulants.
The order 1 and 2 cumulants (mean and variance) are well
known; the order 3 and 4 cumulants (essentially, skewness and
kurtosis) are also well known; for general definitions see ref. 22 and
other standard references, such as McCullagh (23). In general, the
mth order joint cumulant tensor of a random variable U  CN is an
m-way array CumU
m(i1, . . . , im) with 1  im  N. If Y is generated
in terms of the independent components model Eq. 17 then we have
an additive decomposition:
CumY
mi 
k
CumXk
m ak  ak  . . . aki,
where CumXk
m is the mth order cumulant of the scalar random
variable Xk. Here (ak R ak R . . . ak) denotes an m-way array built
from exterior powers of columns of A:
a k  a k  . . . a ki1, . . . , im  a ki1a ki2. . . a kim.
Define then a dictionary D(m) for m-way arrays with k-th atom the
m-the exterior power of ak: dk  ak  ak  . . . ak. Now let S(m)
denote the m-order cumulant array CumY
m. Then
S m  Dmm,
[18]
where D(m) is the dictionary and (m) is the vector of m-order scalar
cumulants of the independent components random variable X . If we
can uniquely solve this system, then we have a way of learning
cumulants of the (unobservable) X from cumulants of the (observ-
able) Y. Abusing notation so that M(D)  M(DHD) etc., we have
MDm  MDm.
In short, if M(D)  1 then M(D(m)) can be very small for large
enough m. Hence, even if the M value for determining a not-very-
sparse X from Y is unfavorable, the corresponding M value for
determining CumX
m from CumY
m can be very favorable, for m large
enough. Suppose for example that A is an N by L system
with M(A) 1N, but X has (say) N3 active components. This
is not very sparse; on typical realizations X0  N3  N  1M.
In short, unique recovery of X based on sparsity will not hold.
However, the second-order cumulant array (the covariance) CumX
2
still has N3 active components, while M(D(2))  M(D(2))2  1N;
as N3  12M, the cumulant array CumY
2 is sparsely represented
and CumX
2 is recovered uniquely by 1 optimization. Generalizing,
we have
Theorem 11. Consider any coupling matrix A with M  1. For all
sufficiently large m  m*(L, M), the minimum 1 solution of the
cumulant decomposition problem (18) is unique (and correct!).
Partial support was from National Science Foundation Grants DMS 00-
77261, ANI-008584 (ITR), DMS 01-40698 (FRG), and DMS 98-72890
(KDI), the Defense Advanced Research Projects Agency, the Applied
and Computational Mathematics Program, and Air Force Office of
Scientific Research Multidisciplinary University Research Initiative
Grant 95-P49620-96-1-0028.
1. Mallat, S. (1998) A Wavelet Tour of Signal Processing (Academic, London), 2nd
Ed.
2. Chen, S. S., Donoho, D. L. & Saunders, M. A. (2001) SIAM Rev. 43, 129–59.
3. Coifman, R., Meyer, Y. & Wickerhauser, M. V. (1992) in ICIAM 1991,
Proceedings of the Second International Conference on Industrial and Applied
Mathematics (Society for Industrial and Applied Mathematics, Philadelphia),
pp. 41–50.
4. Wickerhauser, M. V. (1994) Adapted Wavelet Analysis from Theory to Software
(Addison–Wesley, Reading, MA).
5. Berg, A. P. & Mikhael, W. B. (1999) in Proceedings of the 1999 IEEE
International Symposium on Circuits and Systems (IEEE, New York), Vol. 4, pp.
106–109.
6. DeBrunner, V. E., Chen, L. X. & Li, H. J. (1997) IEEE Trans. Image Proc. 6,
1316–1322.
7. Huo, X. (1999) Ph.D. dissertation (Stanford University, Stanford, CA).
8. Starck, J. L., Donoho, D. L. & Cande`s, E. J. (2002) Astron. Astrophys., in press.
9. Ron, A. & Shen, X. (1996) Math. Comp. 65, 1513–1530.
10. Starck, J. L., Cande`s, E. J. & Donoho, D. L. (2002) IEEE Trans. Signal Processing
11, 670–684.
11. Mallat, S. & Zhang, Z. (1993) IEEE Trans. Signal Processing 41, 3397–3415.
12. Chen, S. S. (1995) Ph.D. dissertation (Stanford University, Stanford, CA).
13. Donoho, D. L. & Huo, X. (2001) IEEE Trans. Inf. Theory 47, 2845–2862.
14. Elad, M. & Bruckstein, A. M. (2001) in Proceedings of the IEEE International
Conference on Image Processing (ICIP) (IEEE, New York).
15. Elad, M. & Bruckstein, A. M. (2002) IEEE Trans. Inf. Theory 48, 2558–2567.
16. Gill, P. E., Murray, W. & Wright, M. H. (1991) Numerical Linear Algebra and
Optimization (Addison–Wesley, London).
17. Donoho, D. L., Levi, O., Starck, J. L. & Martinez, V. (2002) in Proceedings
of SPIE on Astronomical Telescopes Instrumentations (International Society for
Optical Engineering, Bellingham, WA).
18. Gribonval, R. & Nielsen, M. (2002) IEEE Trans. Inf. Theory, in press.
19. Feuer, A. & Nemirovsky, A. (2002) IEEE Trans. Inf. Theory, in press.
20. Horn, R. A. & Johnson, C. R. (1991) Matrix Analysis (Addison–Wesley,
Redwood City, CA).
21. Sloane, N. J. A. (1983) in Cryptography, Lecture Notes in Computer Science
(Springer, Heidelberg), Vol. 149, pp. 71–128.
22. Hyvarinen, A., Karhunen, J. & Oja, E. (2001) Independent Components Analysis
(Wiley, New York).
23. McCullagh, P. (1987) Tensor Methods in Statistics (Chapman & Hall, London).
2202 
www.pnas.orgcgidoi10.1073pnas.0437847100
Donoho and Elad
Downloaded from https://www.pnas.org by 159.2.218.54 on April 15, 2023 from IP address 159.2.218.54.

