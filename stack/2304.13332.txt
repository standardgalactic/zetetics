arXiv:2304.13332v2  [math.NA]  23 Feb 2024
February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Mathematical Models and Methods in Applied Sciences
© World Scientiﬁc Publishing Company
Entropy-based convergence rates of greedy algorithms
Yuwen Li∗
School of Mathematical Sciences, Zhejiang University,
Hangzhou, Zhejiang 310058, China
liyuwen@zju.edu.cn
Jonathan W. Siegel
Department of Mathematics, Texas A&M University,
College Station, TX 77845, USA
jwsiegel@tamu.edu
Received 29 April 2023
Revised 13 November 2023
Accepted 30 December 2023
We present convergence estimates of two types of greedy algorithms in terms of the
entropy numbers of underlying compact sets. In the ﬁrst part, we measure the error of a
standard greedy reduced basis method for parametric PDEs by the entropy numbers of
the solution manifold in Banach spaces. This contrasts with the classical analysis based
on the Kolmogorov n-widths and enables us to obtain direct comparisons between the
algorithm error and the entropy numbers, where the multiplicative constants are explicit
and simple. The entropy-based convergence estimate is sharp and improves upon the
classical width-based analysis of reduced basis methods for elliptic model problems. In the
second part, we derive a novel and simple convergence analysis of the classical orthogonal
greedy algorithm for nonlinear dictionary approximation using the entropy numbers of
the symmetric convex hull of the dictionary. This also improves upon existing results by
giving a direct comparison between the algorithm error and the entropy numbers.
Keywords: Reduced basis method; orthogonal greedy algorithm; nonlinear approxima-
tion; entropy numbers; Kolmogorov n-width; symmetric convex hull.
AMS Subject Classiﬁcation: 41A25, 41A46, 41A65, 65M12, 65N15
1. Introduction
Greedy algorithms are ubiquitous in advanced scientiﬁc computing and computa-
tional mathematics. Successful greedy-type numerical algorithms include adaptive
ﬁnite element2, 13, 41, 42, 49 and wavelet14, 29 methods for boundary value problems,
certiﬁed model reduction technique for parametric diﬀerential equations,3, 35, 52
and nonlinear dictionary approximation4, 23, 27, 37, 56, 64 in signal processing, machine
∗Corresponding author.
1

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
2
Y. Li & J. W. Siegel
learning and statistics. The aforementioned algorithms use local extreme criteria
and iterations to search a global quasi-optimizer. It is known that many greedy
algorithms have guaranteed error controls, solid convergence analysis, and optimal
computational complexity as advantages. In this paper, we are focused on two appli-
cations of greedy algorithms, one for the Reduced Basis Method (RBM) and one for
approximating functions. We shall show that these two types of greedy algorithms
originally designed for seemingly unrelated problems can be analyzed in a uniﬁed
way.
The RBM is a class of popular numerical methods developed in recent decades for
eﬃciently solving parametric PDEs.15, 35, 46, 52 In a Banach space X, let P(uµ, µ) = 0
be the model problem with the solution uµ ∈X, where µ is a parameter in a compact
set U. A standard RBM is split into two stages. In the oﬄine stage, the RBM
builds a highly accurate ﬁnite-dimensional subspace Xn = span{uµ1, . . . , uµn} ⊂X
to uniformly approximate the compact solution manifold
M = u(U) = {uµ ∈X : µ ∈U}.
The oﬄine construction of Xn is potentially very expensive but is only implemented
once during the oﬄine setup stage of the RBM. Then the online module of the RBM
is able to rapidly produce accurate Galerkin approximations to uµ based on Xn for
many instances of the parameter µ.
There are two main types of computational methods for constructing Xn. One is
called the Proper Orthogonal Decomposition (POD),5 which utilizes a (costly) sin-
gular value decomposition to extract the low-rank structure of a given high-quality
dataset. The other, is an iterative greedy algorithm for building the RBM subspace
Xn, which relies on a posteriori error estimates. Compared with POD, greedy al-
gorithms are computationally more feasible and have certiﬁed error bounds and
convergence rates. The work47 provided a priori convergence rates of greedy RBMs
for single-parameter problems. In general, the convergence of greedy algorithms for
the RBM is given by comparing the numerical error of the algorithm σn(M) against
the Kolmogorov n-width dn(M), which measures the performance of the best pos-
sible n-dimensional subspace for approximating the solution manifold M. A direct
comparison between σn(M) and dn(M) was developed in Ref. 10. Later on, the
works7, 19 showed for the ﬁrst time that
dn(M) = O(n−s) =⇒σn(M) = O(n−s),
(1.1)
where s > 0 and O is Landau’s big O notation. In other words, greedy-type RBMs
are rate-optimal for polynomial decaying dn(M).
As far as we know, convergence results for the greedy RBM in the literature
are all based on comparison with the Kolmogorov n-width. In this paper, we derive
convergence analysis of greedy-type RBMs in terms of εn = εn(co(M)), the entropy
number38 of the symmetric convex hull co(M). Just as the n-width dn = dn(M),
εn is also an asymptotically small quantity for compact sets (see Refs. 11 and 45).
Unlike the rate-optimality result (1.1), our analysis leads to direct and optimal

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
3
comparisons of the type
σn(M) ≤Cntεn(co(M)),
t ∈
1
2, 1

,
(1.2)
where C is an explicit constant and the exponent t depends upon the Banach space
in which the error is measured. Speciﬁcally, t = (1/2) + |1/2 −1/p| for the spaces
Lp. As n →∞(1.2) is sharper than the direct comparisons based on dn(M) given
in Refs. 10 and 19. Moreover, in the context of solving parametric PDEs, it is easier
to calculate the entropy numbers than the Kolmogorov n-width of relevant sets,
which is another advantage of bounding σn(M) in terms of εn, see Subsection 2.2
for more details.
On the other hand, there are a variety of greedy algorithms for constructing
nonlinear approximations (see Ref. 20) of a single target function in the ﬁelds of
machine learning, statistics and signal processing. The historically ﬁrst of these
algorithms is known as projection pursuit regression in statistics,28, 36 the match-
ing pursuit algorithm in signal procession,17, 48 and the Pure Greedy Algorithm
(PGA) in approximation theory.23 Other variants of these greedy algorithms have
been proposed which enjoy signiﬁcantly better convergence guarantees for general
dictionaries, including the Relaxed Greedy Algorithm (RGA)18, 23, 37, 51 and the Or-
thogonal Greedy Algorithm (OGA).4, 23, 50 We refer to the article64 and the book60
for a thorough introduction to greedy algorithms for function approximation.
In recent decades, greedy algorithms have also been used to solve PDEs,
see1, 26, 40 and the recent work55 which uses RGAs and OGAs to solve PDEs with
neural networks. These algorithms adaptively select basis functions from a redun-
dant dictionary D ⊂X and use a sparse linear combination
fn =
n
X
i=1
cigi,
gi ∈D
of dictionary elements to approximate a target function f. Among these greedy
algorithms, the RGA and OGA achieve the worst case optimal convergence rate
O(n−1/2) for target functions in the convex hull of the dictionary D,23, 51 while the
PGA in general may perform worse.43 In Ref. 56, it is shown that for dictionaries
whose convex hull has small entropy numbers, the orthogonal greedy algorithm may
converge faster, speciﬁcally for f ∈co(D) and s > 1/2,
εn(co(D)) = O(n−s) =⇒∥f −fn∥X = O(n−s).
(1.3)
As mentioned before, the OGA converges with rate O(n−1/2) and no better for a
general dictionary D (see Ref. 23). The result (1.3) is an improved convergence rate
for OGAs under additional assumptions, which hold for many popular dictionaries,
e.g., the ReLUk dictionary in shallow neural networks (see Ref. 57).
The second main contribution of our work is a novel direct comparison for the
OGA:
∥f −fn∥X ≤Cfεn(co(D)),
(1.4)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
4
Y. Li & J. W. Siegel
where Cf is an explicit uniform constant depending on the target function f. We
remark that (1.3) is indeed a rate comparison while our estimate (1.4) is direct and
implies (1.3) as special cases. In addition, the validity of (1.3) relies on the polyno-
mial decay of εn(co(D)), while the estimate (1.4) is non-asymptotic and is always
valid for general dictionaries, e.g., when εn(co(D)) = O(e−αns) is exponentially
diminishing.
Throughout the rest of this section, we brieﬂy introduce the Kolmogorov n-
width, the entropy numbers, and the details of greedy algorithms for the RBM and
function approximation.
1.1. Greedy Algorithms for RBMs
Let K be a compact set in a Banach space X equipped with the norm ∥•∥= ∥•∥X.
The Kolmogorov n-width of K is deﬁned as
dn(K) = dn(K)X := inf
Xn sup
f∈K
dist(f, Xn),
where
dist(f, Xn) := inf
g∈Xn ∥f −g∥
is the distance from f to Xn, and the inﬁmum is taken over all n-dimensional
subspaces of X.
On the other hand, Kolmogorov proposed and investigated the (dyadic) entropy
numbers of K (see Ref. 38), which are given by
εn(K) = εn(K)X := inf{ε > 0 : K is covered by 2n balls of radius ε}.
It is known that {dn(K)}n≥1 and {εn(K)}n≥1 are decreasing sequences and
limn→0 dn(K) = limn→0 εn(K) = 0 for any compact set K (see Ref. 45). When
K is compact, the symmetric or absolutely convex hull of K,
co(K) :=
n X
i
cigi :
X
i
|ci| ≤1, gi ∈K for each i
o
,
is also compact. The key quantity used in our convergence analysis is the entropy
number εn(co(K)), while in traditional analyses the Kolmogorov n-widths dn(K)
are typically used.
In the oﬄine stage, the greedy algorithm for RBMs selects a sequence {fn}n≥1
from K in an adaptive way as follows.
Algorithm 1.1 (Greedy Algorithm for RBMs). Set n = 1 and X0 = {0}.
Step 1: Compute
fn = arg max
f∈K dist(f, Xn−1).
Step 2: Set Xn = span{f1, . . . , fn}. Set n = n + 1. Go to Step 1.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
5
In practice, K is often the solution manifold M of a parametric PDE corre-
sponding to a range of varying parameters, and the computational cost of the exact
greedy approach (Algorithm 1.1) might still be exceedingly high. In such cases, a
weak greedy algorithm driven by a posterior error estimation is able to relax the
argmax criterion in Algorithm 1.1 and to realize the next more feasible algorithm.
This modiﬁcation is analogous to the weak greedy algorithms for function approx-
imation,62 which we discuss in more detail in Subsection 1.2.
Algorithm 1.2 (Weak Greedy Algorithm for RBMs). Set n = 1, γ ∈(0, 1]
and X0 = {0}.
Step 1: Select fn ∈K such that
dist(fn, Xn−1) ≥γ max
f∈K dist(f, Xn−1).
Step 2: Set Xn = span{f1, . . . , fn}. Set n = n + 1. Go to Step 1.
We remark that Step 1 of Algorithm 1.2 is not directly implemented but prac-
tically achieved by an a posteriori error estimation.25, 35, 53 The constant γ ∈(0, 1]
actually depends on the quality of a posteriori error estimators. The numerical error
of Algorithm 1.2 is measured by
σn(K) = σn(K)X := sup
f∈K
dist(f, Xn).
By deﬁnition the simple bound
dn(K) ≤σn(K)
(1.5)
is always true. We note that Algorithm 1.2 with γ = 1 reduces to Algorithm 1.1
and the iterates {fn}n≥1 are not uniquely determined by the above greedy selection
processes. When X is a Hilbert space, let PU be the orthogonal projection onto a
subspace U ⊂X. In this case we have
dist(f, Xn) = ∥f −PXnf∥,
σn(K) = max
f∈K ∥f −PXnf∥.
1.2. Greedy Algorithms for Approximating Functions
Let the dictionary D be a collection of elements in a real Hilbert space X equipped
with inner product ⟨·, ·⟩. For learning algorithms based upon wavelets, shallow neu-
ral networks, statistical regression or compressed sensing, it is often essential to
construct nonlinear approximants to a target function f ∈X. To achieve this goal,
Refs. 28 and 48 developed the projection pursuit or matching pursuit algorithm:
gn = arg max
g∈D |⟨g, f −fn−1⟩|,
fn = fn−1 + ⟨f −fn−1, gn⟩gn,

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
6
Y. Li & J. W. Siegel
where f0 = 0 and n = 1, 2, . . . is the iteration counter. Note that if the dictionary
elements are normalized in X, then this algorithm is adding the single term which
minimizes the error in each step and validates the name “greedy algorithm.” The
theoretical analysis of this algorithm is notoriously complex,23, 36, 58 and in any case
it does not achieve an optimal rate of convergence.43 As an alternative, a variety
of diﬀerent greedy algorithms which achieve better convergence behavior have been
introduced, for example the following RGA4, 23, 37 and its variants:51
(αn, βn, gn) = arg
min
α,β∈R,g∈D ∥f −αfn−1 −βgn∥,
fn = αnfn−1 + βngn.
In this work, the algorithm under consideration is the OGA which has been devel-
oped and analyzed in, e.g., Refs. 4,18,23,50,60 and 56.
Algorithm 1.3 (Orthogonal Greedy Algorithm). Set n = 1 and f0 = 0.
Step 1: Compute the optimizer
gn = arg max
g∈D |⟨g, f −fn−1⟩|.
Step 2: Set Xn = span{g1, . . . , gn} and compute
fn = PXnf = Pnf.
Set n = n + 1. Go to Step 1.
In Step 1 of Algorithm 1.3, it is possible that the argmax may not exist or may
be impossible to compute. To overcome this, one could relax the selection criterion
to
|⟨gn, f −fn−1⟩| ≥γ max
g∈D |⟨g, f −fn−1⟩|
with γ ∈(0, 1]. This modiﬁed algorithm is closer to what is typically implemented
in practice and is called the weak OGA.62 Weak versions of other greedy algorithms
have also been introduced and analyzed in e.g., Ref. 60.
In our error estimates, C is a generic and uniform constant that may change
from line to line but is independent of n and underlying compact sets or target
functions. By A ≲B we denote A ≤CB, and A ≂B is equivalent to A ≲B and
B ≲A.
The rest of this paper is organized as follows. In Section 2, we present an
entropy-based convergence analysis of greedy RBMs and corresponding applications
in Hilbert spaces. In Section 3, we derive similar optimal convergence estimates for
RBMs in Banach spaces. Section 4 is devoted to an optimal direct comparison
estimate for the convergence of OGAs in Hilbert spaces. Concluding remarks are
presented in Section 5.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
7
2. Convergence of RBMs in Hilbert spaces
Throughout this section, we assume that X is a Hilbert space unless confusion
arises. The next lemma is the main tool of our entropy-based convergence analysis
for greedy algorithms.
Lemma 2.1. Let Vn =
π
n
2
Γ
 n
2 +1
 be the volume of an ℓ2-unit ball in Rn. Let K be a
compact set in a Hilbert space X. Then for any v1, . . . , vn ∈K, it holds that

n
Y
k=1
vk −Pk−1vk

 1
n ≤(n!Vn)
1
n εn(co(K))X.
where P0 = 0 and Pk is the orthogonal projection onto span{v1, . . . , vk}.
Proof. Consider the symmetric simplex
S = co
 {v1, . . . , vn}

=
( n
X
i=1
civi :
n
X
i=1
|ci| ≤1
)
.
We assume that {vk}n
k=1 are linearly independent otherwise the inequality auto-
matically holds. Let Xn be the n-dimensional subspace spanned by v1, . . . , vn. We
identify Xn with the Euclidean space Rn and S with a n-dimensional skewed simplex
in Rn. It is straightforward to see that
{vk}n
k=1 7→{vk −Pk−1vk}n
k=1
is the Gram-Schmidt orthogonalization of v1, ..., vn. As a result, the volume of S is
given by
|S| = 2n
n!
n
Y
k=1
vk −Pk−1vk
.
(2.1)
The deﬁnition of εn = εn(co(K))X implies that S is covered by 2n balls of radius
εn. Therefore by (2.1) and a volume comparison between S and the union of balls,
we have
2n
n!
n
Y
k=1
vk −Pk−1vk
 ≤2nεn
nVn.
The proof is complete.
2.1. Convergence estimates
Using Lemma 2.1, we immediately obtain a direct comparison result between the
RBM error and the entropy number on convex compact sets.
Theorem 2.1. Let K be a compact set in a Hilbert space X. For the weak greedy
algorithm (Algorithm 1.2) we have
σn(K)X ≤γ−1(n!Vn)
1
n εn(co(K))X.
(2.2)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
8
Y. Li & J. W. Siegel
Proof. For Algorithm 1.2 and n ≥1, we have
γσn−1(K) ≤
fn −PXn−1fn
 ≤σn−1(K).
(2.3)
It then follows from (2.3) and Lemma 2.1 that

n
Y
k=1
σk−1(K)
 1
n ≤γ−1
n
Y
k=1
fk −PXk−1fk

 1
n
≤γ−1(n!Vn)
1
n εn(co(K))X.
(2.4)
Combining (2.4) with the fact
σ0(K) ≥σ1(K) ≥· · · ≥σn−1(K) ≥σn(K)
completes the proof.
Using the well-known Stirling’s formula
lim
n→∞n!
.√
2πn
n
e
n
= 1,
lim
n→∞Vn

1
√nπ
2πe
n
 n
2 = 1,
we obtain a corollary about the convergence rate of greedy-type RBMs.
Corollary 2.1. Let K be a compact set in a Hilbert space X. For Algorithm 1.2
there exists an absolute constant C independent of K and n, such that
σn(K)X ≤C√nεn(co(K))X.
A natural question is whether the convergence rate of σn(K) in Corollary 2.1 is
sharp and whether the factor √n is removable. In the following we give a negative
answer to this question.
Proposition 2.1. There exists a Hilbert space X and a symmetric convex compact
set K ⊂X such that Algorithm 1.1 satisﬁes
σn(K)X ≂√nεn(K)X.
Proof. On a Lipschitz domain Ω⊂Rd, we consider the ReLUk activation function
σ(x) = max(xk, 0) with k ≥1 and the dictionary
Pd
k =

σ(ω · x + b) : ω ∈Sd
1, b ∈[b,¯b]
	
,
where Sd
1 is the d-dimensional unit sphere centered at 0, and b, ¯b are constants
depending on Ω. We set K = co(Pd
k) and X = L2(Ω). It has been shown in Ref. 57
that
dn(co(Pd
k))L2(Ω) = O(n−2k+1
2d ),
(2.5a)
εn(co(Pd
k))L2(Ω) = O(n−1
2 −2k+1
2d ).
(2.5b)
Combining (2.5) and Corollary 2.1 we arrive at
dn(co(Pd
k))L2(Ω) ≤σn(co(Pd
k))L2(Ω) ≲√nεn(co(Pd
k))L2(Ω),

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
9
and obtain the convergence rate
σn(co(Pd
k))L2(Ω) = O(n−2k+1
2d ).
As a result, the error estimate in Corollary 2.1 is not improvable.
In the following we discuss the advantage of our direct comparisons over the
classical ones. The ﬁrst direct comparison result for greedy-type RBMs is derived
in Ref. 10:
σn(K) ≲n2ndn(K),
which is useful if dn(K) = o(n−12−n). Later on, this estimate has been improved
in Refs. 7 and 19. Currently the sharpest direct comparison in the RBM literature
(see Ref. 19) is
σ2n(K) ≲
p
dn(K).
(2.6)
Next we show that our direct comparison result in Theorem 2.1 is indeed sharper
than (2.6) as n →∞. In doing so, we need the well-known Carl’s inequality (see
Refs. 11 and 45).
Lemma 2.2 (Carl’s inequality). For every α > 0, there exists a constant C(α) >
0 such that for any compact set K in a Banach space X,
εn(K) ≤C(α)n−α max
1≤i≤n(iαdi−1(K)).
For a compact set K with dn(K) ≲n−s and s > 0, Lemma 2.2 implies that
εn(K) ≲n−s, i.e., the entropy number decays as fast as the polynomial-decaying
Kolmogorov n-width. Therefore combining the fact (see Ref. 45)
dn(K) = dn(co(K))
(2.7)
with Lemma 2.2 and Corollary 2.1, we have
dn(K) ≲n−s =⇒σn(K) ≲√nεn(co(K)) ≲n
1
2 −s.
(2.8)
In contrast, the estimate (2.6) under the same assumption yields
dn(K) ≲n−s =⇒σn(K) ≲n−s
2 .
Thus our direct comparisons in Theorem 2.1 are asymptotically sharper than the
classical result (2.6) when s > 1.
2.2. Examples
In the rest of this section, we discuss applications of our results to the second order
elliptic equation
−∇· (a∇u(a)) = f
in Ω⊂Rd,
(2.9a)
u(a) = 0
on ∂Ω.
(2.9b)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
10
Y. Li & J. W. Siegel
Here f ∈H−1(Ω) is ﬁxed, the diﬀusion coeﬃcient a ∈L∞(Ω) is the varying input
parameter belonging to the function class A that will be speciﬁed later, and u(a) ∈
H1
0(Ω) is the weak solution corresponding to a. First we present a simple lemma
for calculating the entropy numbers of the solution manifold M = u(A) of (2.9).
Lemma 2.3. Let 0 < α ≤1 and Φ : X →Y be an α-H¨older continuous mapping
between Banach spaces X and Y such that for all x1, x2 ∈X,
∥Φ(x1) −Φ(x2)∥Y ≤L∥x1 −x2∥α
X.
Then for any compact set K ⊂X we have
εn(Φ(K))Y ≤Lεn(K)α
X.
Proof. Let K be covered by 2n balls {Bε(xi)}2n
i=1 of radius ε = εn(K)X, where
Bε(xi) is centered at xi ∈X. Then the H¨older continuity of Φ implies that Φ(K)
can be covered by balls {BLεα(Φ(xi))}2n
i=1, suggesting that εn(Φ(K))Y ≤Lεα.
For parametric PDEs, it is generally much more diﬃcult to calculate the n-width
dn(u(A)) of the solution manifold M = u(A) than computing εn(u(A)). To remedy
this situation, Cohen and DeVore15, 16 developed a technical tool for estimating
dn(u(A)) in terms of dn(A).
Theorem 2.2 (Theorem 1 from Ref. 16). For a pair of complex Banach spaces
X and Y , suppose u is a holomorphic mapping from an open set O ⊂X into Y
and u is uniformly bounded on O:
sup
a∈O
∥u(a)∥Y < ∞.
If K ⊂O is a compact set, then for any α > 1 and β < α −1 we have
sup
n≥1
nαdn(K)X < ∞=⇒sup
n≥1
nβdn(u(K))Y < ∞.
2.2.1. Example 1
Let Cs(Ω) = Ck,α(Ω) with regularity index
s := k + α
be the space of functions on Ωwhose k-th derivatives are H¨older continuous with
exponent α ∈(0, 1]. In the ﬁrst example, we focus on the following function class
of diﬀusion coeﬃcients
A =

a ∈L∞(Ω) : inf
x∈Ωa(x) > M0 > 0, ∥a∥Ck,α(Ω) ≤M1

,
where constants M0, M1 are ﬁxed. In practice, a ∈A has additional aﬃnely
parametrized structures allowing an eﬃcient online implementation in RBMs, see,

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
11
e.g., Refs. 35 and 52. Otherwise, researchers often utilize the empirical interpola-
tion method3 to construct aﬃnely parametrized approximations to a and then apply
RBMs to the approximate model.
Using Theorem 2.2, the holomorphy of the solution operator u : a 7→u(a), and
dn(A)L∞(Ω) = O(n−s
d ) (see Refs. 44 and 16), it has been proved in Ref. 16 that
dn(u(A))H1(Ω) ≲n−s
d +1+t
(2.10)
for any t > 0. Then as a result of (2.10) and the classical convergence estimate of
greed-type RBMs (see Ref. 7), for s > d,
σn(u(A))H1(Ω) ≲n−s
d +1+t.
(2.11)
However, (2.11) fails to indicate any convergence of RBMs for approximating the
solution process a 7→u(a) of (2.9) when s ≤d.
Our convergence results for RBMs shed some light on such cases. For (2.9) there
exists a useful perturbation result (see Theorem 2.1 from Ref. 8)
∥u(a1) −u(a2)∥H1(Ω) ≤CΩ∥∇u(a)∥Lp(Ω)∥a1 −a2∥Lq(Ω),
(2.12)
where p ≥2 and q := 2p/(p −2). In particular, (2.12) with p = 2 implies the
Lipschitz property of
u : L∞(Ω) →H1(Ω),
a 7→u(a).
Combining this fact with Lemma 2.3 and εn(A)L∞(Ω) = O(n−s
d ) (see Ref. 38), we
have
εn(u(A))H1(Ω) ≲εn(A)L∞(Ω) ≲n−s
d .
(2.13)
Next we need a lemma from Ref. 12 about the relations between the entropy num-
bers of K and co(K), see also Refs. 59 and 30 similar results.
Lemma 2.4 (Proposition 6.2 from Ref. 12). Let K be a compact set in a
Banach space X of type p ∈(1, 2], and assume that
εn(K)X ≲n−α
for some α > 1 −1/p. Then there exists a constant c(α, p) such that
εn(co(K))X ≤c(α, p)n−1+ 1
p (log(n + 1))−α+1−1
p .
In particular, the Hilbert space X = H1(Ω) is of type p = 2. As a result of
Lemma 2.4 and (2.13), when d/2 < s ≤d we have
εn(co(u(A)))H1(Ω) ≲n−1
2 (log(n + 1))−s
d + 1
2 .
(2.14)
Therefore using Corollary 2.1 and (2.14) we obtain the convergence of the weak
greedy algorithm for (2.9):
dn(u(A))H1(Ω) ≤σn(u(A))H1(Ω) ≲(log(n + 1))−s
d + 1
2 ,
where the function class A is of lower regularity s ∈(d/2, d].

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
12
Y. Li & J. W. Siegel
2.2.2. Example 2
In the second example, we consider the same boundary value problem (2.9) on Ω=
[0, 1]2 with a diﬀerent set of diﬀusion coeﬃcients, which corresponds to DeVore’s
geometric model in Ref. 21. Let
H =

φ ∈Ck,α[0, 1] : 0 ≤φ(x) ≤1 for x ∈[0, 1]
	
.
The graph of a function φ ∈H partitions Ωinto subregions Ωφ
+ and Ωφ
−, deﬁned by
Ωφ
+ = {(x, y) ∈Ω, y ≥φ(x)}
(2.15)
and Ωφ
−= Ω\Ωφ
+. The diﬀusion coeﬃcient a is a member of
A = {a = aφ ∈L∞(Ω) : a|Ωφ
+ = 2, a|Ωφ
−= 1 for φ ∈H}.
(2.16)
Functions in A have discontinuities along the curve represented by φ ∈H, posing a
real challenge for reducing the model problem (2.9). In fact, there is no convergence
result of the RBM for the geometric model in the classical literature.
Elementary arguments show that for q ≥1 the mapping φ 7→aφ is 1/q-H¨older
continuous from L∞[0, 1] to Lq(Ω). Therefore using Lemma 2.3 we can estimate the
metric entropy of A as
εn(A)Lq(Ω) ≤εn(H)
1
q
L∞[0,1] ≂n−s
q .
(2.17)
Moreover, there exists an exponent P > 2 depending on Ωsuch that ∇u(a) ∈LP(Ω)
(see Refs. 32 and 8). Then the perturbation result (2.12) with p = P, q = Q :=
2P/(P −2) ∈(2, ∞) implies that
u : LQ(Ω) →H1(Ω),
a 7→u(a)
is a Lipschitz mapping. Then using (2.17) and Lemma 2.3 again, we arrive at
εn(u(A))H1(Ω) ≲εn(A)LQ(Ω) ≲n−s
Q .
Combining it with Corollary 2.1 and Lemma 2.4 shows that
σn(u(A))H1(Ω) ≲log(n + 1)−s
Q + 1
2 ,
s > Q/2,
indicating the convergence of the RBM Algorithm 1.2 with K = u(A). In addition,
we obtain the novel decay result of the n-width
dn(u(A))H1(Ω) ≲log(n + 1)−s
Q + 1
2 .
3. Convergence of RBMs in Banach spaces
In this section, we derive direct comparison and convergence estimates for the weak
greedy algorithm (Algorithm 1.2), where X is only assumed to be a general Banach
space. First we introduce the Banach-Mazur distance d(X, Y ) (see Ref. 67) between
two isomorphic Banach spaces X and Y :
d(X, Y ) = inf

∥T ∥∥T −1∥: T is an isomorphism from X to Y
	
.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
13
Let ℓn
2 denote the space Rn under the Euclidean ℓ2-norm. For all n ≥1 we consider
the number
δn(X) := sup
Yn⊂X
d(Yn, ℓn
2) ≤√n,
(3.1)
where the supremum is taken over all n-dimensional Banach subspaces Yn in X,
and the upper bound in (3.1) has been proven in Section III.B.9 of Ref. 66. The
quantity δn(X) measures how far an n-dimensional subspace of X is away from the
Hilbert space ℓn
2. With the help of (3.1), we are able to establish the key lemma for
our analysis in Banach spaces.
Lemma 3.1. Let K be a compact set in a Banach space X. For any v1, . . . , vn ∈K
with Xk = span{v1, . . . , vk} and X0 = {0}, we have

n
Y
k=1
dist(vk, Xk−1)
 1
n ≤δn(X)(n!Vn)
1
n εn(co(K))X.
Proof. Let T be an isomorphism from Xn to ℓn
2 and
∥v∥T := ∥T v∥ℓn
2 for any v ∈Xn.
Then we have the norm equivalence
∥T −1∥−1∥v∥X ≤∥v∥T ≤∥T ∥∥v∥X,
∀v ∈Xn.
(3.2)
In addition, by the deﬁnition of δn(X) we can choose T such that
∥T ∥∥T −1∥= δn(X).
(3.3)
Let Xn,T denote the Hilbert space, which is the same as Xn as a set, and is equipped
with the inner product
(•, •)T := (T •, T •)ℓn
2 .
As a result of (3.2) it holds that
dist(vk, Xk−1) ≤∥T −1∥∥vk −Qk−1vk∥T,
(3.4)
where Qk is the orthogonal projection onto Xk with respect to (•, •)T. Let S =
co

v1, . . . , vn
	
. Then applying our previous estimate in Lemma 2.1 to the Hilbert
space Xn,T and using (3.4), we have
 n
Y
k=1
dist(vk, Xk−1)
! 1
n
≤∥T −1∥
 n
Y
k=1
∥vk −Qk−1vk∥T
! 1
n
≤∥T −1∥(n!Vn)
1
n εn(S)Xn,T .
(3.5)
On the other hand, the lower bound in (3.2) leads to
εn(S)Xn,T ≤∥T ∥εn(S)X ≤∥T ∥εn(co(K))X.
(3.6)
Therefore combining (3.5) with (3.6) and (3.3) completes the proof.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
14
Y. Li & J. W. Siegel
The estimate (3.1) on the Banach-Mazur distance can be improved in special
Banach spaces like Lp(dµ) with 1 < p < ∞. For instance, it has been shown in
Ref. 66 that
δn(X) ≤n| 1
2 −1
p |
for X = Lp(dµ), p ∈[1, ∞].
(3.7)
Following the same proof of Theorem 2.1, we obtain the following error bounds
of Algorithm 1.2 in Banach spaces by Lemma 3.1.
Theorem 3.1. Let K be a compact set in a Banach space X. For the weak greedy
algorithm (Algorithm 1.2) we have
σn(K)X ≤γ−1δn(X)(n!Vn)
1
n εn(co(K))X.
In particular, the asymptotic error estimates hold:
σn(K)X ≲
(
γ−1nεn(co(K))X,
for a general Banach space X,
γ−1n
1
2 +| 1
2 −1
p |εn(co(K))X,
X = Lp(dµ), p ∈[1, ∞].
As a byproduct of (1.5) and Theorem 3.1, we also have a direct comparison
between the Kolmogorov n-width and entropy number in general Banach spaces,
which generalizes Proposition 2 from Ref. 57 for Hilbert spaces.
Corollary 3.1. For a compact set K in a Banach space X we have
dn(K)X ≤δn(X)(n!Vn)
1
n εn(co(K))X,
n ≥1.
In the end of this section, we shall show that the factor n
1
2 +| 1
2 −1
p | in Theorem
3.1 cannot be removed when p ≥2. To that end, we need the following lemma
concerning the metric entropy of the ℓn
1 unit ball in ℓp. We remark that we do not
know whether Theorem 3.1 is tight when 1 ≤p < 2.
Lemma 3.2 (Theorem 1 from Ref. 54). Let B1
m be an ℓ1-unit ball in Rm. For
m/2 < s < m and 1 ≤p ≤∞it holds that
εs(B1
m)ℓp ≂log(m/s)s−1+ 1
p .
Proposition 3.1. There exists a symmetric and convex compact set K in the Ba-
nach space X = ℓp with 2 ≤p ≤∞such that Algorithm 1.1 satisﬁes
σn(K)X ≂n1−1
p εn(K)X.
(3.8)
Proof. Let {xi}∞
i=1 be a sequence of real numbers decreasing to 0. We consider the
compact set
K = co
 ∪∞
i=1 {fi}

⊂X = ℓp
where fi = xiei and ei is the i-th unit vector in ℓp. Clearly the greedy algorithm
1.1 selects f1, f2, . . . , fn, . . . as the iterates and
σn(K) = ∥fn∥ℓp = |xn|.
(3.9)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
15
Given an exponent α > 1, let xj = 2−kα for 2k−1 ≤j ≤2k −1. By construction
xn = O(n−α) and we shall show that
εN(K)ℓp = O(N −1+ 1
p −α) for N = 3 · 2n.
(3.10)
We consider the following subsets of K:
K0 = co
 ∪2n
i=1 {fi}

,
Kk = co
 ∪2n+k−1
i=2n+k−1 {fi}

,
1 ≤k ≤n,
K∞= co
 ∪∞
i=22n {fi}

.
Using the elementary fact εs+t(A + B) ≤εs(A) + εt(B) (see Refs. 45 and 57) we
have
εN(K)ℓp ≤ε2n(K0)ℓp +
n
X
k=1
ε2n−k(Kk)ℓp + ε2n(K∞)ℓp.
(3.11)
Using the formula for the volume of an ℓp ball of radius ε in Rm (see Refs. 24 and
65)
V p
m(ε) = 2mεm Γ(1 + 1
p)m
Γ(1 + m
p ) ,
we estimate εm(K0)ℓp by calculating the volume of K0, with m = 2n:
εm(K0)ℓp ≂
 
2m
m! |x1| · · · |xm|
Γ(1 + m
p )
2mΓ(1 + 1
p)m
! 1
m
≂m−1+ 1
p −α.
(3.12)
For 1 ≤k ≤n, Lemma 3.2 implies that
ε2n−k(Kk)ℓp ≲2−(n+k)αε2n−k
 B1
2n+k

ℓp
≲2−(n+k)α√
k2(−1+ 1
p )(n−k)
≲
√
k2−k(α−1)2−n(α+1−1
p ).
As a result, we obtain the following bound
n
X
k=1
ε2n−k(Kk)ℓp ≲
∞
X
k=1
√
k2−k(α−1)
2−n(α+1−1
p ) ≲2−n(α+1−1
p ).
(3.13)
Since the ℓp distance from 0 to the elements of K∞is less than 2−2nα, one can
simply estimate ε2n(K∞)ℓp by
ε2n(K∞)ℓp ≲2−2nα.
(3.14)
Therefore combining (3.12), (3.13) and (3.14), we conﬁrm (3.10) and thus
εn(K)ℓp ≲n−α−1+ 1
p
for n ≥1.
(3.15)
Finally (3.8) with p ≥2 follows from Theorem 3.1 and (3.9), (3.15).

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
16
Y. Li & J. W. Siegel
4. Convergence analysis of OGAs
Let X be a real Hilbert space and D ⊂X be a ﬁxed dictionary or collection
of functions. To analyze the convergence behavior of the OGA, we consider the
following norm and space:4, 23
∥f∥L1(D) := inf



X
i
|ci| : f =
X
gi∈D
cigi


,
L1(D) :=

f ∈X : ∥f∥L1(D) < ∞
	
.
Here ∥f∥L1(D) is sometimes known as the variation of f with respect to D in the
classical literature (see Ref. 39). For the OGA 1.3 with f ∈L1(D) and ∥D∥:=
supg∈D ∥g∥≤1, the classical error estimate (see Refs. 23 and 4) reads
∥f −fn∥≤∥f∥L1(D)(n + 1)−1
2 .
(4.1)
Recently, the work56 developed a rate-optimal convergence estimate of the OGA
based on the entropy numbers of co(D). For example, convergence of the OGA based
on a suﬃciently regular dictionary like Pd
k would be much faster than O(n−1
2 ).
Theorem 4.1 (Theorem 1 from Ref. 56). Let f ∈L1(D) and t > 0. For
Algorithm 1.3 it holds that
εn(co(D)) ≲n−1
2 −t =⇒∥f −fn∥≲∥f∥L1(D)n−1
2 −t.
4.1. Direct comparison estimate
Although the OGA and the greedy-type RBM are not closely related research areas,
we show that the main lemma 2.1 used in the analysis of greedy RBMs can be used to
derive a new type of clean and transparent direct comparison between ∥f −fn∥and
εn(co(D)). It turns out that our analysis greatly simpliﬁes the argument in Ref. 56
and leads to an improvement of Theorem 4.1. To this end, we need the following
upper bound for recursively related sequences. We note that an equivalent result
has previously appeared in a slightly diﬀerent form in the literature, see Lemma
3.1 in Ref. 62 or Lemma 3.4 in Ref. 23. For the readers convenience we provide the
complete proof here.
Lemma 4.1. Let {an}n≥0 and {b}n≥1 be non-negative sequences satisfying
an ≤an−1(1 −bnan−1) for n ≥1.
(4.2)
and b0 = 1/a0. Then we have
an ≤
1
b0 + b1 + · · · + bn
.
Proof. We prove the lemma by induction. First we note {an}n≥0 is a decreasing
sequence. By deﬁnition we have a0 ≤1/b0 for n = 0. For the time being assume
an−1 ≤
1
b0 + · · · + bn−1
.
(4.3)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
17
If an−1 ≤
1
b0+···+bn , then the monotonicity of {an}n≥1 implies
an ≤an−1 ≤
1
b0 + · · · + bn
.
If an−1 >
1
b0+···+bn , then by (4.2) and the induction assumption (4.3), we obtain
an ≤
1
b0 + · · · + bn−1

1 −
bn
b0 + · · · + bn

=
1
b0 + · · · + bn
.
Therefore the induction is complete and Lemma 4.1 is true.
Theorem 4.2. For f ∈L1(D) and Algorithm 1.3 we have
∥f −fn∥≤(n!Vn)
1
n
√n
∥f∥L1(D)εn(co(D)).
(4.4)
Proof. Let rn = f −fn in Algorithm 1.3. Following the analysis in Ref. 56 (see
also the analysis in Section 3 of Ref. 31), we have
∥rn∥2 ≤
rn−1 −⟨rn−1, gn −Pn−1gn⟩
∥gn −Pn−1gn∥2
(gn −Pn−1gn)

2
= ∥rn−1∥2 −|⟨rn−1, gn −Pn−1gn⟩|2
∥gn −Pn−1gn∥2
.
(4.5)
We remark ∥gn −Pn−1gn∦= 0 otherwise gn ∈Xn−1 = span{g1, . . . , gn−1} and
arg maxg∈D |⟨rn−1, g⟩| = |⟨rn−1, gn⟩| = 0, which implies rn−1 = 0 and termination
of the OGA. On the other hand, using rn−1 ⊥Xn−1 we have
∥rn−1∥2 = ⟨f, rn−1⟩≤∥f∥L1(D)|⟨rn−1, gn⟩|
= ∥f∥L1(D)|⟨rn−1, gn −Pn−1gn⟩|.
(4.6)
Therefore combining (4.5) and (4.6) leads to
∥rn∥2 ≤∥rn−1∥2 −
∥rn−1∥4
∥f∥2
L1(D)∥gn −Pn−1gn∥2 .
(4.7)
With the notation
an = ∥f −fn∥2/∥f∥2
L1(D),
bn = ∥gn −Pn−1gn∥−2,
the inequality (4.7) reduces to the recurrence relation
an ≤an−1(1 −bnan−1).
(4.8)
By the deﬁnition of ∥f∥L1(D) we have a0 ≤1. It then follows from (4.8) and Lemma
4.1 that
an ≤
1
1 + b1 + · · · + bn
.
(4.9)

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
18
Y. Li & J. W. Siegel
Using (4.9), a mean value inequality and the key Lemma 2.1, we obtain
an ≤
1
1 + n(b1 · · · bn)
1
n
=
1
1 + n
  Qn
k=1 ∥gk −Pk−1gk∥
−2
n
≤
1
1 + n(nVn)−2
n εn(co(D))−2
≤(nVn)
2
n
n
εn(co(D))2.
The proof is complete.
We remark that the classical Theorem 4.1 is valid under an asymptotic assump-
tion while Theorem 4.2 is a non-asymptotic and unconditional estimate. In addition,
the constant in the upper bound of (4.4) as n →∞behaves like
lim
n→∞
(n!Vn)
1
n
√n
=
r
2π
e .
Therefore Theorem 4.2 yields
∥f −fn∥≲∥f∥L1(D)εn(co(D)),
which is indeed stronger than Theorem 4.1.
4.2. Best n-term approximation
Consider the set of n-sparse elements
Σn(D) =
n
n
X
i=1
ci˜gi : ci ∈R, ˜gi ∈D, i = 1, . . . , n
o
.
It is natural to compare the error of greedy algorithms with the best approximation
error
En(f, D) =
inf
g∈Σn(D) ∥f −g∥
by n-sparse elements. When D is an orthonormal set in X, the equality ∥f −fn∥=
En(f, D) automatically holds (see Ref. 64). In general, such convergence analysis
using En(f, D) often requires near orthogonality assumption on D. For example, on
a M-coherent dictionary D, the following Lebesgue inequality is true (see Section
2.6 of Ref. 64):
∥f −f⌊n log n⌋∥≤c1En(f, D),
for n ≤c2M −2
3 with c1, c2 being explicit constants. We also refer to Ref. 61 for
Lebesgue-type inequality of generalized OGAs in Banach spaces. On the other hand,

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
19
the entropy-based estimate (4.4) applies to any normalized dictionary in Hilbert
spaces.
In addition, we note that Theorem 4.4 yields the following direct comparison
between En(f, D) and εn(co(D)):
En(f, D) ≤(n!Vn)
1
n
√n
∥f∥L1(D)εn(co(D)),
(4.10a)
En(D) :=
sup
f∈co(D)
En(f, D) ≤(n!Vn)
1
n
√n
εn(co(D)).
(4.10b)
When the number |D| of elements in D is ﬁnite, an indirect comparison between
En(D) and εn(co(D)) in the converse direction of (4.10b) can be found in Section
7.4 of Ref. 61. In particular, combining Theorem 7.4.3 of Ref. 61 with Theorem 4.4,
we obtain the following novel comparison between the convergence rate of En(D)
and the error of OGAs whose target functions belonging to co(D).
Corollary 4.1. Assume there exists a constant s > 0 such that
En(D) ≲n−s,
n ≤|D|,
where the cardinality |D| is ﬁnite. Then for fn generated by the OGA (Algorithm
1.3) with n ≤|D|, we have
sup
f∈co(D)
∥f −fn∥≤C(s)(log(2|D|/n))sn−s,
where C(s) is a constant depending only on s.
We note that the above result is independent of the coherence property of D.
Moreover, Corollary 4.1 is not a Lebesgue-type inequality but is in a similar fashion
to the classical n-width based convergence estimate (1.1) for the RBM. It conﬁrms
that the OGA collectively achieves the best n-term approximation rate up to a
logarithmic factor on ﬁnite dictionaries.
4.3. General target functions
In general, if f is not contained in L1(D), one can use the next corollary to estimate
the error of the OGA.
Corollary 4.2. For all f ∈X and any h ∈L1(D), Algorithm 1.3 satisﬁes
∥f −fn∥2 ≤∥f −h∥2 + 4(n!Vn)
2
n
n
∥h∥2
L1(D)εn(co(D))2.
Proof. We use the same notation as in the proof of Theorem 4.2. We note (4.5)
still holds and
∥rn−1∥2 = ⟨h, rn−1⟩+ ⟨f −h, rn−1⟩
≤∥h∥L1(D)|⟨rn−1, gn −Pn−1gn⟩| + ∥f −h∥∥rn−1∥.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
20
Y. Li & J. W. Siegel
Then combining the above inequality and a mean value inequality yields
1
2
 ∥rn−1∥2 −∥f −h∥2
≤∥h∥L1(D)|⟨rn−1, gn −Pn−1gn⟩|.
(4.11)
Let cn = (∥rn∥2 −∥f −h∥2)/(4∥h∥2
L1(D)). Using (4.5) and (4.11) we obtain
cn ≤cn−1(1 −bncn−1).
The rest of the proof is same as Theorem 4.2.
For t > 0 and g ∈X the K-functional
K(t, g) =
inf
h∈L1(D)
 ∥g −h∥X + t∥h∥L1(D)

measures how well the element of X can be approximated by L1(D) with an ap-
proximant of small L1(D) norm (see Refs. 6, 22 and 9). For the index pair (θ, ∞)
with θ ∈(0, 1), the interpolation norm and space between X and L1(D) are
∥g∥θ = ∥g∥[X,L1(D)]θ,∞:=
sup
0<t<∞
t−θK(t, g),
Xθ =

g ∈X : ∥g∥θ < ∞
	
,
respectively. As in Refs. 4 and 56, we also obtain an explicit and improved error
estimate of the OGA in terms of the interpolation norm ∥f∥θ.
Proposition 4.1. For all f ∈Xθ and θ ∈(0, 1) Algorithm 1.3 satisﬁes
∥f −fn∥≤2θ (n!Vn)
θ
n
n
θ
2
∥f∥θεn(co(D))θ.
Proof. By Corollary 4.2, we obtain for all h ∈L1(D)
∥f −fn∥≤∥f −h∥+ 2(n!Vn)
1
n
√n
∥h∥L1(D)εn(co(D)).
As a result, setting t = 2 (n!Vn)
1
n
√n
εn(co(D)), we have
∥f −fn∥≤K(t, f) ≤tθ∥f∥θ
and complete the proof.
5. Concluding Remarks
We have derived direct and optimal convergence estimates of the greedy-type RBM
and the OGA based on Kolmogorov’s entropy numbers. Constants in our upper
bounds are explicit and simple. A future research direction is to extend our analysis
to POD-greedy algorithms33, 34 for time-dependent parametric PDEs, which is a
combination of greedy algorithms and POD for temporal compression of snapshots.
In addition, the generalization of the OGA in Banach spaces is called the Chebyshev
Greedy Algorithm (CGA).18, 63, 64 It would also be interesting to investigate if our
analysis in Section 4 and Lemma 3.1 can be applied to the CGA.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
21
Acknowledgements
The authors would like to thank the two anonymous referees for constructive com-
ments that improve the quality of the manuscript. Subsection 4.2 is partially due
to the bibliographic suggestion61 of one referee.
The work of Li was partially supported by the Fundamental Research Funds
for the Central Universities 226-2023-00039. Siegel was supported by the National
Science Foundation (DMS-2111387 and CCF-2205004).
References
1. A. Ammar, B. Mokdad, F. Chinesta and R. Keunings, A new family of solvers for
some classes of multidimensional partial diﬀerential equations encountered in kinetic
theory modeling of complex ﬂuids, Journal of non-Newtonian ﬂuid Mechanics 139
(2006) 153–176.
2. I. Babuˇska and W. C. Rheinboldt, Error estimates for adaptive ﬁnite element compu-
tations, SIAM J. Numer. Anal. 15 (1978) 736–754.
3. M. Barrault, Y. Maday, N. C. Nguyen and A. T. Patera, An ‘empirical interpola-
tion’ method: application to eﬃcient reduced-basis discretization of partial diﬀerential
equations, C. R. Math. Acad. Sci. Paris 339 (2004) 667–672.
4. A. R. Barron, A. Cohen, W. Dahmen and R. A. DeVore, Approximation and learning
by greedy algorithms, Ann. Statist. 36 (2008) 64–94.
5. P. Benner, S. Gugercin and K. Willcox, A survey of projection-based model reduction
methods for parametric dynamical systems, SIAM Rev. 57 (2015) 483–531.
6. J. Bergh and J. L¨ofstr¨om, Interpolation spaces. An introduction, Grundlehren der
Mathematischen Wissenschaften, No. 223 (Springer-Verlag, Berlin-New York, 1976).
7. P. Binev, A. Cohen, W. Dahmen, R. DeVore, G. Petrova and P. Wojtaszczyk, Con-
vergence rates for greedy algorithms in reduced basis methods, SIAM J. Math. Anal.
43 (2011) 1457–1472.
8. A. Bonito, R. A. DeVore and R. H. Nochetto, Adaptive ﬁnite element methods for
elliptic problems with discontinuous coeﬃcients, SIAM J. Numer. Anal. 51 (2013)
3106–3134.
9. S. C. Brenner and L. R. Scott, The mathematical theory of ﬁnite element methods,
volume 35 of Texts in Applied Mathematics, 15 (Springer, New York, 2008), 3 edition.
10. A. Buﬀa, Y. Maday, A. T. Patera, C. Prud’homme and G. Turinici, A priori con-
vergence of the greedy algorithm for the parametrized reduced basis method, ESAIM
Math. Model. Numer. Anal. 46 (2012) 595–603.
11. B. Carl, Entropy numbers, s-numbers, and eigenvalue problems, J. Functional Anal-
ysis 41 (1981) 290–306.
12. B. Carl, I. Kyrezi and A. Pajor, Metric entropy of convex hulls in Banach spaces, J.
London Math. Soc. (2) 60 (1999) 871–896.
13. J. M. Cascon, C. Kreuzer, R. H. Nochetto and K. G. Siebert, Quasi-optimal conver-
gence rate for an adaptive ﬁnite element method, SIAM J. Numer. Anal. 46 (2008)
2524–2550.
14. A. Cohen, W. Dahmen and R. DeVore, Adaptive wavelet methods for elliptic operator
equations: convergence rates, Math. Comp. 70 (2001) 27–75.
15. A. Cohen and R. DeVore, Approximation of high-dimensional parametric PDEs, Acta
Numer. 24 (2015) 1–159.
16. A. Cohen and R. DeVore, Kolmogorov widths under holomorphic mappings, IMA J.
Numer. Anal. 36 (2016) 1–12.

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
22
Y. Li & J. W. Siegel
17. G. Davis, S. Mallat and M. Avellaneda, Adaptive greedy approximations, Constr.
Approx. 13 (1997) 57–98.
18. A. V. Dereventsov and V. N. Temlyakov, A uniﬁed way of analyzing some greedy
algorithms, J. Funct. Anal. 277 (2019) 108286, 30.
19. R. DeVore, G. Petrova and P. Wojtaszczyk, Greedy algorithms for reduced bases in
Banach spaces, Constr. Approx. 37 (2013) 455–466.
20. R. A. DeVore, Nonlinear approximation, in Acta numerica, 1998 (Cambridge Univ.
Press, Cambridge, 1998), volume 7 of Acta Numer., pp. 51–150.
21. R. A. DeVore, The theoretical foundation of reduced basis methods, in Model Reduc-
tion and Approximation (SIAM Computational Science & Engineering) (2014).
22. R. A. DeVore and G. G. Lorentz, Constructive approximation, volume 303 of
Grundlehren der mathematischen Wissenschaften [Fundamental Principles of Mathe-
matical Sciences] (Springer-Verlag, Berlin, 1993).
23. R. A. DeVore and V. N. Temlyakov, Some remarks on greedy algorithms, Adv. Com-
put. Math. 5 (1996) 173–187.
24. P. G. L. Dirichlet, Sur une nouvelle m´ethode pour la d´etermination des int´egrales
multiples, Journal de Math´ematiques Pures et Appliqu´ees 4 (1839) 164–168.
25. P. Edel and Y. Maday, Dual natural-norm a posteriori error estimators for reduced
basis approximations to parametrized linear equations, Math. Models Methods Appl.
Sci. .
26. L. E. Figueroa and E. S¨uli, Greedy approximation of high-dimensional Ornstein–
Uhlenbeck operators, Foundations of Computational Mathematics 12 (2012) 573–623.
27. J. H. Friedman, Greedy function approximation: a gradient boosting machine, Ann.
Statist. 29 (2001) 1189–1232.
28. J. H. Friedman and W. Stuetzle, Projection pursuit regression, Journal of the Amer-
ican statistical Association 76 (1981) 817–823.
29. T. Gantumur, H. Harbrecht and R. Stevenson, An optimal adaptive wavelet method
without coarsening of the iterands, Math. Comp. 76 (2007) 615–629.
30. F. Gao, Metric entropy of convex hulls, Israel J. Math. 123 (2001) 359–364.
31. Y. Gao, T. Qian, L.-F. Cao and V. Temlyakov, Aspects of 2D-adaptive Fourier de-
compositions, arXiv preprint arXiv:1710.09277.
32. P. Grisvard, Singularities in boundary value problems, Research in Applied Mathe-
matics, 22 (Springer-Verlag, Berlin, 1992).
33. B. Haasdonk, Convergence rates of the POD-greedy method, ESAIM Math. Model.
Numer. Anal. 47 (2013) 859–873.
34. B. Haasdonk and M. Ohlberger, Reduced basis method for ﬁnite volume approxima-
tions of parametrized linear evolution equations, M2AN Math. Model. Numer. Anal.
42 (2008) 277–302.
35. J. S. Hesthaven, G. Rozza and B. Stamm, Certiﬁed reduced basis methods for
parametrized partial diﬀerential equations, SpringerBriefs in Mathematics (Springer,
Cham; BCAM Basque Center for Applied Mathematics, Bilbao, 2016), bCAM
SpringerBriefs.
36. L. K. Jones, On a conjecture of Huber concerning the convergence of projection pursuit
regression, The Annals of statistics (1987) 880–882.
37. L. K. Jones, A simple lemma on greedy approximation in Hilbert space and conver-
gence rates for projection pursuit regression and neural network training, Ann. Statist.
20 (1992) 608–613.
38. A. N. Kolmogorov and V. M. Tihomirov, ε-entropy and ε-capacity of sets in functional
space, Amer. Math. Soc. Transl. (2) 17 (1961) 277–364.
39. V. Kurkov´a and M. Sanguineti, Bounds on rates of variable-basis and neural-network

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
Entropy-based convergence rates of greedy algorithms
23
approximation, IEEE Trans. Inform. Theory 47 (2001) 2659–2665.
40. C. Le Bris, T. Lelievre and Y. Maday, Results and questions on a nonlinear approxima-
tion approach for solving high-dimensional partial diﬀerential equations, Constructive
Approximation 30 (2009) 621–651.
41. Y. Li, Some convergence and optimality results of adaptive mixed methods in ﬁnite
element exterior calculus, SIAM J. Numer. Anal. 57 (2019) 2019–2042.
42. Y. Li, Quasi-optimal adaptive mixed ﬁnite element methods for controlling natural
norm errors, Math. Comp. 90 (2021) 565–593.
43. E. D. Livshits, Lower bounds for the rate of convergence of greedy algorithms,
Izvestiya: Mathematics 73 (2009) 1197.
44. G. G. Lorentz, Metric entropy, widths, and superpositions of functions, Amer. Math.
Monthly 69 (1962) 469–485.
45. G. G. Lorentz, M. v. Golitschek and Y. Makovoz, Constructive approximation, volume
304 of Grundlehren der mathematischen Wissenschaften [Fundamental Principles of
Mathematical Sciences] (Springer-Verlag, Berlin, 1996), advanced problems.
46. Y. Maday, Reduced basis method for the rapid and reliable solution of partial diﬀer-
ential equations, in International Congress of Mathematicians. Vol. III (Eur. Math.
Soc., Z¨urich, 2006), pp. 1255–1270.
47. Y. Maday, A. T. Patera and G. Turinici, A priori convergence theory for reduced-
basis approximations of single-parameter elliptic partial diﬀerential equations, J. Sci.
Comput. 17 (2002) 437–446.
48. S. G. Mallat and Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE
Transactions on Signal Processing 41 (1993) 3397–3415.
49. P. Morin, K. G. Siebert and A. Veeser, A basic convergence result for conforming
adaptive ﬁnite elements, Math. Models Methods Appl. Sci. 18 (2008) 707–737.
50. Y. Pati, R. Rezaiifar and P. Krishnaprasad, Orthogonal matching pursuit: Recursive
function approximation with applications to wavelet decomposition, in Proceedings of
27th Asilomar conference on signals, systems and computers (1993), pp. 40–44. IEEE.
51. G. Petrova, Rescaled pure greedy algorithm for Hilbert and Banach spaces, Applied
and Computational Harmonic Analysis 41 (2016) 852–866.
52. A. Quarteroni, A. Manzoni and F. Negri, Reduced basis methods for partial diﬀer-
ential equations, volume 92 of Unitext (Springer, Cham, 2016), an introduction, La
Matematica per il 3+2.
53. G. Rozza, D. B. P. Huynh and A. T. Patera, Reduced basis approximation and a
posteriori error estimation for aﬃnely parametrized elliptic coercive partial diﬀerential
equations: application to transport and continuum mechanics, Arch. Comput. Methods
Eng. 15 (2008) 229–275.
54. C. Sch¨utt, Entropy numbers of diagonal operators between symmetric Banach spaces,
J. Approx. Theory 40 (1984) 121–128.
55. J. W. Siegel, Q. Hong, X. Jin, W. Hao and J. Xu, Greedy training algorithms for
neural networks and applications to pdes, J. Comput. Phys. 484 (2023) 112084.
56. J. W. Siegel and J. Xu, Optimal convergence rates for the orthogonal greedy algorithm,
IEEE Trans. Inform. Theory 68 (2022) 3354–3361.
57. J. W. Siegel and J. Xu, Sharp bounds on the approximation rates, metric entropy,
and n-widths of shallow neural networks, Found. Comput. Math. .
58. A. Sil’nichenko, Rate of convergence of greedy algorithms, Mathematical Notes 76
(2004) 582–586.
59. I. Steinwart, Entropy of C(K)-valued operators, J. Approx. Theory 103 (2000) 302–
328.
60. V. Temlyakov, Greedy approximation, volume 20 of Cambridge Monographs on Applied

February 26, 2024
1:26
WSPC/INSTRUCTION FILE
arXiv
24
Y. Li & J. W. Siegel
and Computational Mathematics (Cambridge University Press, Cambridge, 2011).
61. V. Temlyakov, Multivariate approximation, volume 32 of Cambridge Monographs on
Applied and Computational Mathematics (Cambridge University Press, Cambridge,
2018).
62. V. N. Temlyakov, Weak greedy algorithms, Adv. Comput. Math. 12 (2000) 213–227.
63. V. N. Temlyakov, Greedy algorithms in Banach spaces, Adv. Comput. Math. 14 (2001)
277–292.
64. V. N. Temlyakov, Greedy approximation, Acta Numer. 17 (2008) 235–409.
65. X. Wang, Volumes of generalized unit balls, Mathematics Magazine 78 (2005) 390–
395.
66. P. Wojtaszczyk, Banach spaces for analysts, volume 25 of Cambridge Studies in Ad-
vanced Mathematics (Cambridge University Press, Cambridge, 1991).
67. P. Wojtaszczyk, On greedy algorithm approximating Kolmogorov widths in Banach
spaces, J. Math. Anal. Appl. 424 (2015) 685–695.

