Conditional Noise Deep Learning for Parameter Estimation
of Gravitational Wave Events
Han-Shiang Kuo1, ∗and Feng-Li Lin1, 2, †
1Department of Physics, National Taiwan Normal University, Taipei 11677, Taiwan
2Center of Astronomy and Gravitation, National Taiwan Normal University, Taipei 11677, Taiwan
(Dated: August 5, 2021)
We construct a Bayesian inference deep learning machine for parameter estimation of gravitational
wave events of binaries of black hole coalescence.
The structure of our deep Bayseian machine
adopts the conditional variational autoencoder scheme by conditioning both the gravitational wave
strains and the variations of amplitude spectral density of the detector noise. We show that our
deep Bayesian machine is capable of yielding the posteriors compatible with the ones from the
nest sampling method, and of ﬁghting against the noise outliers. We also apply our deep Bayesian
machine to the LIGO/Virgo O3 events, and ﬁnd that conditioning detector noise to ﬁght against
its drifting is relevant for the events with medium signal-to-noise ratios.
I.
INTRODUCTION
Detection of gravitational wave (GW) from the distant
compact binary coalescence has now become quite com-
mon since the ﬁrst operation runs of LIGO started in
2015 [1], and up to now about hundred events have been
found [2, 3]. Due to the extreme weakness of the GW sig-
nal, the extraction of the source parameters from a given
strain data requires heavy computational cost based on
Nested sampling [4–6] and Markov-Chain-Mote-Carlo al-
gorithm [7, 8], and this is very time-consuming. This will
then delay the announcement of the discoveries and the
public sharing of the strain data for more general usages
and the results of parameter estimation (PE). Once the
frequency of the detection increases from few events per
month to few events per day, this time delay issue of the
parameter inference will be more severe. Therefore, the
acceleration of the PE for GW events is a urgent task
in the vision of the improvement of the sensitivity for
the new generation of gravitational wave detectors [9].
The main obstacle for accelerating the PE is the time-
consuming scan of the likelihood function for obtaining
the posteriors in Bayesian inference scheme [10–14]. One
way to bypass this issue is to ﬁnd a way of performing
likelihood free inference. This is indeed what the deep
learning can do by training the Bayesian inference ma-
chine with lots of mocking data so that it can mimic the
likelihood without event-by-event scanning. This deep-
learning-based machine (or deep machine, for short) can
then be implemented to extract the parameters of the
GW events in a very eﬃcient way. Some pioneer works
in this direction have been done in [15, 16] by adopting
the variational autoencoder (VAE) [17, 18] or the nor-
malizing ﬂow [19], and see [20, 21] for the more recent
progress. However, in these works all the training data
shared the same power spectral density (PSD), which
may not be realistic since the detector noise will drift in
∗hance30258@gmail.com
† fengli.lin@gmail.com, corresponding author.
general. This means that the deep machine should be
retrained for the events with diﬀerent PSDs.
In this work, we extend the conditional VAE (CVAE)
scheme developed in [15] to also conditioning the PSD
of the detector noise, so that the resultant deep machine
can deal with the GW events measured at diﬀerent time
intervals, for which the PSD will drift accordingly. When
ﬁnishing this note, we ﬁnd that similar consideration is
also adopted in recent work [21] in the scheme of normal-
izing ﬂow.
The remaining of this paper is organized as following.
In the next section we will sketch the scheme of CVAE
for the inference of the source parameters of the GW
events without or with the conditional PSD. In section
III we describe how we prepare the training data, es-
pecially on how to prepare the variations of PSD, and
the mock strain data. In section IV we describe the de-
tailed structure of our CVAE model such as the layer
structures and the hyperparameters.
In section V we
ﬁrst discuss the training procedure, including the way of
avoid KL collapse and the learning rate decay, and carry
out the self-check of our Bayesian inference machine. We
then show the performance of our machine when apply-
ing to the mock data by comparing to the traditional
PE method by their posteriors. We also consider the en-
durance of our machine to the drift of the PSD when
comparing to the CVAE model but without conditional
PSD. In section VI, we apply our Bayesian inference ma-
chine to LIGO/Virgo O3 events [22, 23], and show their
performance. Finally, we conclude our paper in section
VII.
II.
CVAE FOR BAYESIAN INFERENCE OF GW
EVENTS
The variational autoencoder (VAE) is a unsupervised
machine learning scheme, which can be used to reveal
the distribution functions of the input data. It ﬁrst com-
presses the input data into the hidden layer by its en-
coder part, and then decompresses the hidden layer into
arXiv:2107.10730v2  [gr-qc]  4 Aug 2021

2
the output by its decoder part. For example, if we pre-
pare many mocking strains as the training data, then the
resultant well-trained machine can learn the distribution
of the strains, and the hidden layers will encode the infor-
mation about the distribution of the source parameters.
However, to make the VAE be useful for the inference of
the source parameters, we need to train the machine by
simultaneously providing the strains {y} and the associ-
ated source parameters {x} as the input data but feeding
to diﬀerent encoders. The schematic structure of CAVE
is similar to what is shown in Fig. 1. The loss function
of this machine can be thought as the upper bound on
the negative of the posterior distribution p(x|y), i.e., the
so-called evidence lower bound (ELBO) and denoted by
LELBO,
−log p(x|y) ≤Ez∼Ew1(z|x,y)[−log Dw3(x|y, z)]
+DKL[Ew1(z|x, y)||Ew2(z|y)]
(1)
where Ewi for i = 1, 2 denote the distributions of the
encoders with the associated weights and biases denoted
by wi, and Dw3 the one of the decoder with w3 the as-
sociated weights and biases.
Moreover, the arguments
and the conditional arguments of the encoders and de-
coder denote their outputs and inputs, respectively. The
right-handed-side of the ﬁrst line of (1) is the so-called re-
construction loss measuring the diﬀerence between input
and output, and the second line is the Kullback–Leibler
(KL) loss measuring the diﬀerence between the hidden
layer distributions of the two encoders.
After the training, we can remove the part associated
with the source parameters but keep only the one asso-
ciated with the strains, so that the remaining part (as
shown on the right part of Fig. 1) can be treated as the
Bayesian inference machine to output the posteriors of
the source parameters for a given input strain. Namely,
we expect
p(x|y) ≈Ez∼Ew2(z|y)[Dw3(x|z, y)] .
(2)
Even though Dw3(x|z, y) is a Gaussian distribution, the
average over z ∼Ew2 will lead to non-Gaussian posterior
approximation, as generally expected.
The above scheme was ﬁrst proposed and implemented
in [15], and can be shown to produce compatible poste-
riors in comparison to the conventional PE. However, in
reality, the PSD (or its squared-root, the amplitude spec-
tral density (ASD)) can drift so that PSD/ASD varies
event by event. This drifting eﬀect has not been taken
into account in [15]. In this note we extend the CAVE
scheme of [15] to also include the variations of ASD as the
conditional input data. The new scheme is shown in Fig.
1. This is the same as the one implemented in [15] except
that an ensemble of ASD is also conditioned when train-
ing, and an ASD should be provided as the input along
with the corresponding strain data when generating the
posteriors of a GW event by the resultant Bayesian in-
ference machine, i.e., the right part of the Fig. 1.
FIG. 1. The schematic structure of CAVE for the inference
of source parameters of GW events.
The goal is to gener-
ate the posteriors p(x|y) of source parameters eﬃciently for
a given strain data without knowing the likelihood p(y|x).
(Left) the CVAE machine with two encoders Ew1, Ew2 and
one decoder Dw3. (Right) the Bayesian inference machine,
which is obtained by removing the Ew1 part of CVAE af-
ter the CVAE is well trained, so that we expect p(x|y) ≈
Ez∼Ew2 (z|y)[Dw3(x|z, y)]. Therefore, its outputs are the pos-
teriors of the source parameters. Our scheme shown here is a
generalization of [15] by adding the ASD of the detector noise
as the conditional inputs besides the associated strain data.
III.
PREPARATION OF TRAINING DATA
As discussed, the training data include both the strain
data and the ASD of the detector noise. We prepare the
former by following the similar prescription given in [15].
As a proof of concept study we only consider the bina-
ries of black holes (BBH) without spin, which are labeled
by two intrinsic parameters, i.e., the component masses
m1, m2. Besides, we also have the extrinsic parameters
describing the locations of the binaries. For simplicity,
we ﬁx all the extrinsic parameters except the luminos-
ity distance dL, which dictates the signal-to-noise ratio
(SNR). Moreover, in the usual conventional PE, we need
to optimize the matched ﬁltering overlap by adjusting
the time of coalescence tc and phase at coalescence φ0.
Thus, we also include tc and φ0 as the parameters for
inference. In total, we have ﬁve parameters for inference,
and their ranges for ﬂat priors and the ﬁxed values of
other parameters are given in Table I.
Unlike in [15], we adopt the frequency-domain tem-
plates to generate the strain data, instead of the time-
domain ones as in [15].
We also change the sampling

3
TABLE I.
Ranges of the priors for the BBH GW events
adopted for the training data of the CVAE models used in
this paper and in [15].
Based on the same priors we can
compare the performance of two CVAE models later on.
parameters
symbol
prior
range
units
mass 1
m1
Uniform
[35, 80]
solar masses
mass 2
m2
Uniform
[35, 80]
solar masses
luminosity distance
dL
Uniform Volume [1000, 3000]
Mpc
time of coalescence
tc
Uniform
[0.65, 0.85]
seconds
phase at coalescence
φ0
Uniform
[0, 2π]
radiance
right ascension
α
·
1.375
radiance
declination
δ
·
-1.2
radiance
inclination
η
·
0
radiance
polarization
φ
·
0
radiance
epoch
·
·
1126259642
GPS time
detector
·
·
Hanford
·
rate from 256Hz to 1024Hz to match the frequency range
of the typical waveforms with high frequency. With the
setup of priors given in Table I, we sample 2×106 sets of
parameters to produce the theoretical waveforms by the
IMRPhnomPv2 waveform model [24], which later will be
used to superpose with the sample detector noise to pro-
duce the mocking strain data.
Now, we turn to the preparation of the set of ASD
templates for the generation of mock detector noises. We
start with some initial set of ASD templates, denoted
as Ai[f] with i = 1, 2, · · · , N, which can be obtained
either theoretically or from the real detector data. We
then generate the training set of ASD by summing up
the N initial ASD templates with random weights and
variations, i.e.,
A[f] = α β[f]
N
X
i=1
ϵiAi[f],
(3)
where ϵi’s are random variables of uniform distribution
within [0, 1] but satisfy the constraint PN
i ϵi = 1. The
eﬀect of β[f] = eN(0,1/8)[f] is to introduce the variation
on each frequency bin, and α = eN(0,1/16) to introduce
the variations on the overall scale, noting that N(µ, σ)[X]
denotes a Gaussian random variable X with mean and
variance (µ, σ2), i.e., X ∼N(µ, σ)[X] 1.
In this work, we use both the theoretical ASD and
the ones from LIGO/Virgo O3. For the former, we use
only aLIGOZeroDetHighPower PSD curve as the initial
template. For the latter, we use the set of ASDs near
39 released BBH events of LIGO/Virgo in O3 run as
the initial templates. In Fig. 2 we show some simulated
ASDs and their possible ranges of 2σ variations generated
from either theoretical or O3 ASDs by (3).
Based on
1 These variances are determined by extracting the range of vari-
ations from the ASDs of some sample mock or real-data strains.
this variational setup of ASD we generate 2 × 106 ASD
templates.
FIG. 2. Some ASD templates and their variations generated
by (3). The blue line is the theoretical aLIGOZeroDetHigh-
Power ASD curve, based on which we generate the black line
as the simulated ASD and the gray region for the possible
range of 2σ variation. Similarly, we also use the ASDs nearby
all 39 BBH GW events of LIGO/Virgo O3 as the initial ASD
templates, one of which is the red line, to generate the pink
region for the range of 2σ variation.
FIG. 3. Typical example of ASD from aLIGOZeroDetHigh-
Power ASD curve (blue), the associated mock detector noise
(orange) and a theoretical waveform (green), all in frequency
domain.
To generate a mock strain, we randomly pick up a
theoretical waveform h[f] and ASD A[f] from the above
prepared sets, then we can form a noise n[f] and a strain
d[f] in the frequency domain as following
d[f] = h[f] + n[f],
(4)
n[f] =
1
∆f W[f] ⊙A[f]
(5)
where ∆f is the frequency bin size which we set to 1Hz
in this work, and W[f] is the white noise in the frequency
domain, which is responsible for the unit Gaussian noise.
In Fig. 3, we show a typical example for a mock ASD,
the associated noise and mock strain.

4
With the above procedure, we generate about 2 × 106
mock strains, of which 80% will be used as the training
data set for CVAE, and 20% as validation data set for
the resultant Bayesian inference machine. This amount
of the training data set is huge enough to exhaust almost
all the possible strain data realizations.
Moreover, to justify the viability of our variational
scheme of ASD by (3), we compare the histograms of the
SNR obtained from the 2 × 106 mock strain data with
and without ASD variation. For the variational ASDs,
we consider both the ones obtained from the theoretical
ASD and from the LIGO/Virgo O3. The result is shown
in Fig. 4, from which we can see that they are compati-
ble. This implies that our ASD variations are faithful to
the SNR of a strain, and should be helpful to train the
Bayesian inference machine against the drift of ASD.
FIG. 4. Histogram of SNR for all the training strain data gen-
erated by using the priors in Table I and the ASD examples
shown in Fig. 2. The blue one is using the ﬁxed theoretical
ASD, i.e., aLIGOZeroDetHighPower. The orange one is us-
ing the variational ones corresponding to the gray region of
Fig. 2, and the green one is using the variational ones from
LIGO/Virgo O3 ASDs, which correspond to the pink region
of Fig. 2.
IV.
THE DETAILED STRUCTURE OF CVAE
MODEL
The schematic structure of our CVAE model and the
resultant Bayesian inference machine has been shown in
Fig. 1. Now we would like to expose its detailed struc-
ture.
For simplicity, our CAVE model is composed of
only the dense layers but not other types of layers. How-
ever, it works. We simply stack the dense layers to con-
struct the three neural networks (NNs), i.e., two encoders
and decoder. Moreover, we adopt almost the same layer
structure for all three NNs, see Fig. 5 for the details.
The only diﬀerences among them are the input data and
the dimensions and the realizations of the hidden lay-
ers. Speciﬁcally, we use 8- and 5-dimensional multivari-
ate Gaussian distributions for the hidden layers of Ew1
and Dw3, and adopt a more powerful mixture Gaussian
distribution layer for Ew2, which has eight dimensions
and each dimension is composed of eight components
of Gaussian normal distributions. All the hidden layers
with Gaussian distributions are realized by the standard
reparameterization trick used for variational autoencoder
[17]. In Table II, we summarize these diﬀerences.
FIG. 5. Structure of neural network used in CVAE model of
Fig. 1. Note that we adopt this same NN for all three NNs,
i.e., Ew1, Ew2 and Dw3 of Fig. 1. The ASD is the common
input, and there is additional input denoted by Input/ASD.
The Input/ASD and the dimensions of the hidden layer vary
for diﬀerent NNs, which we summarize in Table II. The output
is a random latent vector z or x ∼Distribution which is also
speciﬁed in Table II. The dash-lined box contains the part
associated with the conditional ASD, which is absent in the
CVAE model of [15].
Note that the latent vectors for the encoders Ew1 and
Ew2 are denoted by z, which will then be input to the
decoder. However, the output of the decoder is again a
random vector, whose components are identiﬁed as the
source parameters , i.e., x = θ. The distribution of x
gives the approximate posterior of the source parameter
θ through the averaging procedure given in (2).
TABLE II. Input and Hidden Layers of the CVAE model
Ew1(z|x, y)a
Ew2(z|y)
Dw3(x|z, y)
Input/ASDb
[θ, d]
d
[z, d]
Hiddenc
Dense[16, linear]
Dense[24, linear]
Dense[10, linear]
Distributiond
Gaussian(8)
MixtureNormal(8, 8)e
Gaussian(5)
a Here x = θ denoting the source parameters, y = (ASD, d) with d
the strain, and z the random latent vector.
b This means the additional input other than ASD.
c This is the hidden layer whose outputs are means and variances.
d This is the distribution used to generate the random latent
vector z, whose means and variances are given by the outputs of
the hidden layers.
e This is the linear combination of 8 Gaussian distributions.

5
The hyperparameters speciﬁed in Fig.5 and Table II
achieve well-training of our CVAE model, despite that
they can be varied. However, we ﬁnd that it is suﬃcient
for well-training if the dimension of the hidden layer is
greater than the dimension of the target variable. Using
more dimensions may need longer time to training but
improve the performance just slightly.
V.
TRAINING THE CVAE MODEL AND THE
PERFORMANCE
With the above structure of CVAE model, we train the
model by the aforementioned training data set with the
batch size of 2048. We then calculate the loss function,
i.e., LELBO and update the model by Adam optimizer [25]
with learning rate 10−4. The reconstruction loss is evalu-
ated by replacing x in Ez∼Ew1(z|x,y)[−log Dw3(x|y, z)] by
the input source parameter θ. The averaging procedure
over z in the above and in the evaluating the KL loss is
done by the Monte-Carlo method.
There are two eﬀective ways to achieve well-training.
The ﬁrst way is addressed to the so-called KL collapse,
which states that KL loss may happen to be extremely
small so that the variational nature of CVAE is lost. To
avoid the KL collapse, we can adopt the annealing proce-
dure by introducing a annealing factor b ∈[0, 1] so that
the ELBO is changed to
L(b)
ELBO = Ez∼Ew1[−log Dw3] + b DKL[Ew1||Ew2]
(6)
In the early training phase, we slowly tune up the anneal-
ing factor to avoid the KL collapse. When b is far smaller
than one, we are mainly training the VAE, i.e., ignoring
the Ew2 which will be optimized again when b is close to
one. Speciﬁcally, we proceed the KL annealing for the
ﬁrst 5 epochs with the following annealing behavior
b(t) = b0 sin(π
2 t/c),
(7)
where t denotes the number of generations (each genera-
tion means ﬁnishing a batch training) and c ≈103 is the
number of generations within an epoch, and the values
of b0 for these 5 epochs are set to [10−2, 1
4, 1
2, 1, 1]. Note
that the annealing rate gradually approaches zero at the
end of each epoch. After the 5 epochs, b will be set to
one for the remaining training period, which is about 103
epochs.
The second eﬀective way to achieve the well-training
more eﬃciently is to reduce the learning rate gradually.
We reduce the learning rate lr at every generation at such
a rate lr(t) = 2
−t
2×105 lr0 in the total training period of 106
generations. With the implementation of the above two
eﬀective ways, we can achieve well-training of our CVAE
model. A typical example for the evolution of the recon-
struction and KL losses at the training and validation
periods is shown Fig. 6, which indicates the KL anneal-
ing at early training phase. Moreover, the perfect overlap
FIG. 6. Training and validation loss for each generation. The
variation at early stage are caused by cyclic KL annealing
[26]. The perfect overlap between training and validation loss
indicates there is no overﬁtting.
between training and validation losses indicates there is
no overﬁtting.
In the following, we will compare our CVAE model,
which we denote as CVAEASD, and the one used in
[15] but with KL annealing and learning rate decay in-
corporated, which we denote as CVAEnc-ASD with “no-
conditioning” short-handed by nc .
Also, the mixture
Gaussian distribution is used for Ew2 in CVAEnc-ASD
rather than the simple diagonal Gaussian distribution in
[15]. Here the overline is to remind that the KL annealing
and learning rate decay are implemented in the training
procedure. This is to contrast to the CVAE model used in
[15], which we denote as CVAEnc-ASD. Both CVAEnc-ASD
and CVAEnc-ASD have the same layer structure as shown
in Fig. 5 but discarding the part inside the dash-lined
box, which is associated with the part of the conditional
ASD 2. It turns out that the implementation of KL an-
nealing and learning rate decay in the training procedure
is important in achieving better accuracy of ﬁnal pos-
teriors as shown below in comparing the P-P plots and
histograms of KL divergences.
The ﬁrst thing is to check the self-consistency of the
resultant Bayesian inference machine, i.e., calculating the
P-P plot, which is the cumulative distribution function
of the p-value of the posteriors, i.e., p-value = p[p(x|y) >
x|null hypothesis]. By construction, the distribution of
the input parameters should equal the posterior so that
p-value should be the uniform of unity. Thus, the P-P
plot should be diagonal to be self-consistent. The result
is shown in Fig. 7 and indicates that our Bayesian in-
ference machine CVAEASD is self-consistent. Compared
2 This is the layer structure used in the version 1 and 2 of [15]. In
the latest version (version 3) of [15] more complicated structure
with convolutional neural networks is adopted. However, the per-
formance of P-P plot [27] and KL divergence [28] of CVAEnc-ASD
shown below is still better than the latest ones in [15].

6
to the P-P plot shown in Fig.
4 of [15] obtained for
CVAEnc-ASD, the one shown here is more convergent.
This is due to the implementation of KL annealing and
learning rate decay.
FIG. 7. P-P plot for our CVAE model. The CDF is calculated
by 103 mock data. For each mock data, we use 2×104 samples
to estimate the p-value of each parameter.
Next, we compute the posterior of a typical mock
GW event by CVAEASD.
To produce this posterior,
we need to sample about 8 × 104 latent vectors from
z ∼Ew2(z|y), and then use (2) to average over z by
Monte-Carlo method to obtain the posterior p(θ|d, ASD)
for the source parameters θ. The results are shown in
Fig. 8, in which we also compare with the results ob-
tained from the transitional PE algorithm dynesty. We
can see that the marginal posteriors from both methods
are compatible.
One essential question about the performance of
our CAVE Bayesian machine CVAEASD is how good
it is when comparing to CVAEnc-ASD.
One way
to characterize such a performance is to compare
their KL divergences with the posterior obtained from
dynesty,
i.e.,
to
compare
DKL(pdynesty||pASD)
and
DKL(pdynesty||pnc-ASD), where pdynesty, pASD and pnc-ASD
denote the posteriors obtained from dynesty, CVAEASD
and CVAEnc-ASD, respectively. Note that smaller KL di-
vergence means the posteriors from both CVAE models
are close to the one from dynesty. Usually, the threshold
for an acceptable “nice” result is for the KL divergence to
be smaller than 0.1. Moreover, compared to the KL di-
vergences shown in Fig. 5 of [15] obtained for CVAEASD,
the results shown here is about one to two orders better.
Again, this is due to the implementation of KL annealing
and learning rate decay in the training procedure.
We
prepare
512
mock
GW
strains
as
the
in-
puts to the three Bayesian machines for comparison.
These mock GW strains are generated according to
the BBH priors in Table I and the ASDs obtained
by equation (3) from the initial aLIGOZeroDetHigh-
Power ASD. We then evaluate the distributions of
FIG. 8.
The marginal posteriors for a typical mock GW
event evaluated from CVAEASD (red) and the traditional PE
method, i.e., dynesty (blue). The contour represents 50% and
90% credible level and the true parameter are shown by the
blue lines. The KL divergences between posteriors of these
two method are (0.017, 0.039, 0.11, 0.006, 0.031) in the follow-
ing order of the parameters: (q, M, φ0, tc, dL).
DKL(pdynesty||pASD) and DKL(pdynesty||pnc-ASD) for all
ﬁve parameters (q, M, φ0, tc, dL) over the above mock
strains. To obtain pdynesty we use Bilby to perform the
dynesty sampling [14] with 5000 live points and dlogz
equal to 0.1. To train CVAEASD, we use the variational
ASDs as mentioned before, and to train CVAEnc-ASD we
use the ﬁxed ASD aLIGOZeroDetHighPower.
The re-
sults are shown in Fig. 9. We see that CVAEASD per-
form better than CVAEnc-ASD, especially for φ0 and dL
at DKL ∼O(1) by about 1/3 of order improvement.
Besides the histograms of KL divergences shown in Fig.
9, we can also compare the dependence on the SNR for
these two KL divergences. The results are shown in Fig.
10. We can see that CVAEASD has slightly better per-
formance than CVAEnc-ASD, especially for the phase φ0
at low SNR. This implies that the conditioning ASD can
help to sort out the GW events with low SNR.
Finally, we would like to compare the capability
against the outliers of the ASD, i.e., the variation of
ASD about 3σ or even higher.
To check this, we
classify the mock strains used in Fig.
9 and Fig.
10 by the variation of their ASD with respect to the
initial ASD template, i.e., by the sample value of
the random variable α = eN(0,1/16) of (3), and plot
the KL divergences DKL[pdynesty||pASD] (solid line) and
DKL[pdynesty||pnc-ASD] (dashed line) for each class.
We show the results for the parameters φ0 and dL in
Fig. 11 for three classes, i.e., the ones with variation in
the following three ranges: [0σ, 1σ] (red), [1σ, 2σ] (green)

7
FIG.
9.
Histograms
of
KL
divergences,
i.e.,
DKL(pdynesty||pASD)
(blue)
and
DKL(pdynesty||pnc-ASD)
(orange) for all ﬁve parameters (q, M, φ0, tc, dL) over 512
mock GW strains of BBH with ASD variations similar to
the one in Fig.
2.
The preparation of these mock strains
is described in the main text. Note that pdynesty, pASD and
pnc-ASD are the posteriors obtained from dynesty, CVAEASD
and CVAEnc-ASD, respectively.
We see that CVAEASD
performs better than CVAEnc-ASD, especially for φ0 and dL
at DKL ∼O(1).
and [3σ, 4σ] (blue). We see that CVAEASD is better in
ﬁghting against noise drift, and in some case against the
outliers. For the parameter φ0, CVAEASD has the better
performance than CVAEnc-ASD disregarding the varia-
tion amplitude of the ASD. However, for the parameter
dL, CVAEASD shows better ability against the outliers,
especially for the events of smaller KL divergence at or-
der of 10−2. This means that by conditioning the ASDs,
CVAEASD will not miss the “nice” events with KL di-
vergence smaller than 10−1 even under the inﬂuence of
outlier variation of the ASD. Otherwise, it is not the case
for CVAEnc-ASD. Our models are all trained by one GPU
device, NVIDIA RTX3090, where the training time is 12
hours for CVAEASD and 6 hours for CVAEnc-ASD. How-
ever, the computational times for evaluating 105 distribu-
tion samples for these two model are all below 1 second.
VI.
APPLICATION TO O3 EVENTS
We now apply our CVAE model CVAEASD to the
LIGO/Virgo O3 data analysis of 39 BBH events, of which
the prior ranges are listed in Table III. The mock strains
for training CVAEASD are prepared in the same way as
described in section III except that we are now using the
priors listed in Table III, and the variational ASDs ob-
tained by (3) but with the nearby ASDs for all 39 BBH
FIG. 10.
Comparison of the dependence on SNR for the
two KL divergences, i.e., DKL[pdynesty||pASD] (blue) and
DKL[pdynesty||pnc-ASD] (orange), which are already evaluated
in Fig. 9. We see that our CVAE model has slightly better
performance, especially for the phase φ0 at low SNR.
events as the initial templates. Similarly, the layer struc-
ture and its training procedure is the same as described
in section IV.
TABLE III.
Priors for the LIGO/Virgo O3 BBH events
adopted for the preparation of the training data for CVAEASD
parameters
symbol
prior
rangea
units
mass 1
m1
Uniform
[20, 65]
solar masses
mass 2
m2
Uniform
[20, 65]
solar masses
luminosity distance
dL
Uniform Volume [1200, 2200]
Mpc
time of coalescence
tc
Uniform
[0.65, 0.85]
seconds
phase at coalescence
φ0
Uniform
[0, 2π]
radiance
right ascension
α
·
1.84
radiance
declination
δ
·
-0.62
radiance
inclination
η
·
0
radiance
polarization
φ
·
0
radiance
epoch
·
·
1242459857
GPS time
detector
·
·
Livingston
·
a The prior ranges listed here are diﬀerent from the ones used by
LIGO/Virgo for their data analysis. This is due to the limitation
of our computing resources in handling the more complicated
deep machine structure when enlarging the prior ranges.
As a proof of concept study we ﬁx the extrinsic pa-
rameters with the values given in Table III. This of
course will aﬀect the accuracy of the estimating poste-
riors for the parameters of interest: (q, M, φ0, tc, dL). In
Fig. VI we show the marginal posteriors of the O3 event
GW190630 185205, of which the SNR is 19.5, the highest
among 39 BBH events. We see that the results obtained
by CVAEASD match very well with the ones from the
dynesty. This can be further characterized by the values
of the KL divergence for the parameters (q, M, φ0, tc, dL),
which are (0.068, 0.034, 0.11, 0.094, 0.094). Almost all the
values of KL divergences are below the threshold value

8
FIG. 11. Comparison of the ability against the outlier vari-
ation of ASD, i.e., around 3σ or higher.
We classify the
mock strains used in Fig.
9 and Fig.
10 by the sample
value of the random variable α = eN (0,1/16) of (3), and
plot the KL divergences DKL[pdynesty||pASD] (solid line) and
DKL[pdynesty||pnc-ASD] (dashed line) of parameters φ0 (up)
and dL (bottom) for three classes:
[0σ, 1σ] (red), [1, 2σ]
(green) and [3, 4σ] (blue). The plots show that CVAEASD is
better than CVAEnc-ASD in ﬁghting against the general noise
drifts, including the outliers, e.g., see dL plot for the events
of smaller KL divergence at order of 10−2.
0.1, this means that our PE results agree well with the
ones by dynesty. To demonstrate our conditional noise
scheme can yield better PE results than the one without,
we need to consider the more O3 BBH events and com-
pare the corresponding KL divergences. This is what we
do next.
As done in Fig. 10 for the theoretical ASD study, we
also consider the dependence of the KL divergences on
the SNR over the 39 BBH LIGO/Virgo O3 events by re-
placing the mock GW events by the 39 BBH LIGO/Virgo
O3 events.
Due to the limitation of the chosen prior
ranges in Table III, we use only the strain data from the
Livingston detector so that the SNRs are lower than the
ones published by LIGO/Virgo. The results are shown
in Fig. 13 with DKL[pdynesty||pASD]’s denoted by orange
dots, and DKL[pdynesty||pnc-ASD]’s by blue dots. We see
that both CVAEASD and CVAEnc-ASD do not perform
well for the low SNR data.
This is expected as the
lower SNR implies weaker signal and more diﬃcult for
tasks of inferences.
This can be further characterized
by the scatter plot of Bayes factor vs SNR as shown in
Fig. 14, from which we see that the discovery thresh-
old is around SNR = 7, below which the Bayes factor
is smaller than one.
This can then explain why over-
all both CVAE models do not perform well for the low
SNR data, e.g., the shaded region in Fig. 14 with SNR
FIG. 12.
Marginal posteriors of GW190630 185205 event
obtained by CVAEASD (red) and the dynesty (blue).
The
SNR of this event is 19.5,
the highest among all O3
BBH events. The KL divergences, i.e., DKL[pdynesty||pASD],
of
this
event
for
the
parameters
(q, M, φ0, tc, dL)
are
(0.068, 0.034, 0.11, 0.094, 0.094). All the KL divergences are
below the threshold value, i.e., 0.1, thus our PE results
agree well with the ones by dynesty.
The values and the
error margins of the parameters shown in this ﬁgure are the
ones obtained by CVAEASD, and the ones obtained by the
dynesty are q = 0.73+0.15
−0.12, M = 37.72+0.86
−0.89, φ0 = 2.21+2.63
−0.97,
tc = 0.6932+3.7e−4
−3.7e−4 and dL = 1606.86+92.07
−88.44.
less than 10.
However, as shown in Fig.
14 we can
see that our CVAEASD performs deﬁnitely better than
the CVAEnc-ASD for the events with SNR in between 10
and 15. On the other hand, for the highest SNR event
GW190630 185205, CVAEASD does not perform overall
well than CVAEnc-ASD.
The above results in fact indicate that conditioning
detector noise is relevant when the signal is comparable
with the noise, such as the events with SNR between 10
and 15. When the SNR is too low, the signal is over-
whelmed by the noise so that there are not many ways
to improve the data quality for PE, and taking care of
the noise drifting is deﬁnitely not the cure. Similarly, if
the SNR is high enough, the noise is not a important is-
sue to yield the “nice” PE result, so is the drifting of the
noise. The drifting of the noise should be relevant only
when the signal and noise are compatible as our results
shown in Fig. 13.
VII.
CONCLUSION
In this work we construct a conditional noise deep
Bayesian machine to perform the parameter estimation

9
FIG.
13.
SNR
dependence
of
the
KL
divergences,
i.e., DKL[pdynesty||pASD] (orange) and DKL[pdynesty||pnc-ASD]
(blue), over 39 BBH LIGO/Virgo O3 events. This is similar to
what we have shown in Fig. 10 except that we have replaced
the mock GW events by the real LIGO/Virgo O3 eventsa.
As shown, both CVAE models do not perform overall well
for the events with SNR below 10 (the shaded region). How-
ever, CVAEASD performs deﬁnitely better than CVAEnc-ASD
for the events of SNR between 10 and 15, but not for the
event of maximal SNR 19.5. This indicates that conditioning
detector noise to ﬁght against the noise drifting is relevant for
the medium strong signal, i.e., when the signal is compatible
with the noise.
a Due to the limitation of our chosen prior ranges, we only use
the strain data from one detector (Livingston) so that the
SNRs are lower than the ones published by LIGO/Virgo [29].
FIG. 14. The log-Bayes factors of null hypothesis of 39 O3
events. They are calculated by the dynesty for our chosen
priors listed in Table III. The SNR is calculated using the
theoretical GW templates, which are chosen from the dynesty
chain with the maximal likelihood.
(PE) of binary black holes’ gravitational wave (GW)
events based on the deep learning scheme of conditional
variational autoencoder (CVAE). This is a simple exten-
sion of the CVAE model proposed in [15] in which only
strains but not the amplitude spectral density (ASD) of
the detector noise are adopted as the conditional inputs
to CVAE. Our motivation is to construct a deep Bayesian
machine which can adapt to the variations or drift of the
detector noise. This kind of machine can save the time
for retraining when performing PE for various GW events
with slight variations of the detector noise.
As a proof of concept study, we choose a very simple
layer structure, i.e., three dense layers, for two encoders
and one decoder of CVAE. Despite of such a humble
deep machine, we show that the PE results for the mock
strains with the variations from a theoretical ASD are
compatible with ones obtained from the traditional PE
method such as the dynesty once the tricks of KL an-
nealing and learning rate decay are implemented in the
training procedure. Besides, we also show that our CVAE
machine is better than the one of [15] in ﬁghting against
the noise variations.
To test our CVAE model for the real events and demon-
strate the relevance for conditioning the detector noise,
we also apply our CVAE Bayesian machine to 39 BBH
LIGO/Virgo O3 GW events. We ﬁnd that conditioning
detector noise to ﬁght against its drifting is most rele-
vant when the signal is medium strong, e.g., when the
signal-to-noise ratio (SNR) is between 10 to 15.
This
implies that the drifting of the noise is relevant for PE
when the signal is compatible with the noise. We hope
this implication and other results in the paper will be
helpful to construct a more universal and eﬃcient deep
learning based inference machine for the PE tasks of the
GW events.
ACKNOWLEDGEMENT
This work is supported by Taiwan’s Ministry of Science
and Technology (MoST) through Grant No. 109-2112-M-
003-007-MY3.
We thank Guo-Chin Liu for generosity
in providing support of the computing facility. We also
thank NCTS for partial ﬁnancial support. Finally, We
thank TGWG members for the helpful discussions.
[1] B. Abbott et al. (LIGO Scientiﬁc, Virgo), Phys. Rev.
Lett. 116, 061102 (2016), arXiv:1602.03837 [gr-qc].
[2] B. Abbott et al. (LIGO Scientiﬁc, Virgo), Phys. Rev. X
9, 031040 (2019), arXiv:1811.12907 [astro-ph.HE].
[3] R. Abbott et al. (LIGO Scientiﬁc, Virgo),
(2020),
arXiv:2010.14527 [gr-qc].
[4] J. Skilling, in AIP Conference Proceedings, Vol. 735
(American Institute of Physics, 2004) pp. 395–405.

10
[5] W. Del Pozzo and J. Veitch, GitHub https://github.
com/johnveitch/cpnest (2015).
[6] J. S. Speagle, Monthly Notices of the Royal Astronomical
Society 493, 3132 (2020).
[7] D. Foreman-Mackey, D. W. Hogg, D. Lang, and J. Good-
man, Publications of the Astronomical Society of the Pa-
ciﬁc 125, 306 (2013).
[8] T. G. F. Li, Extracting Physics from Gravitational
Waves:
Testing the Strong-ﬁeld Dynamics of General
Relativity and Inferring the Large-scale Structure of the
Universe, Ph.D. thesis, Vrije U., Amsterdam (2013).
[9] B. P. Abbott et al. (KAGRA, LIGO Scientiﬁc, Virgo),
Living Rev. Rel. 23, 3 (2020).
[10] B. Allen, W. G. Anderson, P. R. Brady, D. A. Brown,
and J. D. E. Creighton, Phys. Rev. D 85, 122006 (2012).
[11] C. Messick, K. Blackburn, P. Brady, P. Brockill, K. Can-
non, R. Cariou, S. Caudill, S. J. Chamberlin, J. D.
Creighton, R. Everett,
and et al., Physical Review D
95 (2017), 10.1103/physrevd.95.042001.
[12] J. Veitch, V. Raymond, B. Farr, W. Farr, P. Graﬀ, S. Vi-
tale, B. Aylott, K. Blackburn, N. Christensen, M. Cough-
lin, et al., Physical Review D 91, 042003 (2015).
[13] C. M. Biwer, C. D. Capano, S. De, M. Cabero, D. A.
Brown, A. H. Nitz, and V. Raymond, Publications of the
Astronomical Society of the Paciﬁc 131, 024503 (2019).
[14] G. Ashton, M. H¨ubner, P. D. Lasky, C. Talbot, K. Ack-
ley, S. Biscoveanu, Q. Chu, A. Divakarla, P. J. Easter,
B. Goncharov, et al., The Astrophysical Journal Supple-
ment Series 241, 27 (2019).
[15] H. Gabbard, C. Messenger, I. S. Heng, F. Tonolini,
and R. Murray-Smith, arXiv preprint arXiv:1909.06296
(2019).
[16] S. R. Green, C. Simpson, and J. Gair, Physical Review
D 102 (2020), 10.1103/physrevd.102.104057.
[17] D.
P.
Kingma
and
M.
Welling,
arXiv
preprint
arXiv:1906.02691 (2019).
[18] R. Yu, “A tutorial on vaes: From bayes’ rule to lossless
compression,” (2020), arXiv:2006.10273 [cs.LG].
[19] D. P. Kingma, T. Salimans,
and M. Welling, CoRR
abs/1606.04934 (2016), arXiv:1606.04934.
[20] S. R. Green and J. Gair, Machine Learning: Science and
Technology 2, 03LT01 (2021).
[21] M. Dax, S. R. Green, J. Gair, J. H. Macke, A. Buo-
nanno,
and B. Sch¨olkopf, “Real-time gravitational-
wave science with neural posterior estimation,” (2021),
arXiv:2106.12594 [gr-qc].
[22] M. Vallisneri, J. Kanner, R. Williams, A. Weinstein, and
B. Stephens, in Journal of Physics: Conference Series,
Vol. 610 (IOP Publishing, 2015) p. 012021.
[23] “Gracedb gravitational-wave candidate event database
(ligo/virgo
o3
public
alerts).https://gracedb.ligo.
org/superevents/public/O3/,” .
[24] S. Khan, K. Chatziioannou, M. Hannam, and F. Ohme,
Physical Review D 100, 024059 (2019).
[25] D. P. Kingma and J. Ba, arXiv preprint arXiv:1412.6980
(2014).
[26] H. Fu, C. Li, X. Liu, J. Gao, A. Celikyilmaz,
and
L. Carin, arXiv preprint arXiv:1903.10145 (2019).
[27] A. Ghasemi and S. Zahediasl, International journal of
endocrinology and metabolism 10, 486 (2012).
[28] F. P´erez-Cruz, in 2008 IEEE international symposium
on information theory (IEEE, 2008) pp. 1666–1670.
[29] R. Abbott, T. Abbott, S. Abraham, F. Acernese, K. Ack-
ley, A. Adams, C. Adams, R. Adhikari, V. Adya, C. Af-
feldt, et al., Physical Review X 11, 021053 (2021).

