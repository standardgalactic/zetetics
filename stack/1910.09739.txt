1
Composite Neural Network: Theory and
Application to PM2.5 Prediction
Ming-Chuan Yang and Meng Chang Chen
Institute of Information Science, Academia Sinica, Taiwan
{mingchuan,mcc}@iis.sinica.edu.tw
Abstract
This work investigates the framework and statistical performance guarantee of the composite neural network, which is
composed of a collection of pre-trained and non-instantiated neural network models connected as a rooted directed acyclic graph,
for solving complicated applications. A pre-trained neural network model is generally well trained, targeted to approximate a
speciﬁc function. The advantages of adopting a pre-trained model as a component in composing a complicated neural network are
two-fold. One is beneﬁting from the intelligence and diligence of domain experts, and the other is saving effort in data acquisition
as well as computing resources and time for model training. Despite a general belief that a composite neural network may perform
better than any a single component, the overall performance characteristics are not clear. In this work, we propose the framework
of a composite network, and prove that a composite neural network performs better than any of its pre-trained components with
a high probability.
In the study, we explore a complicated application—PM2.5 prediction—to support the correctness of the proposed composite
network theory. In the empirical evaluations of PM2.5 prediction, the constructed composite neural network models perform better
than other machine learning models.
Index Terms
deep learning, pre-trained component, composite neural network, PM2.5 prediction.
I. INTRODUCTION
Deep learning has seen great success in dealing with natural signals such as images and voices as well as artiﬁcial signals
such as natural language, whereas it is still in the early stages of handling complicated social and natural applications shaped
by diverse factors (e.g., stock market prediction [1]) or that result from complicated natural processes (e.g., PM2.5 pollution
level prediction [2]). Common to these complicated applications is their unbounded applicable data sources, which may not
be available all at once, and their processes, which are difﬁcult to learn from limited data. Consequently, their neural network
based solutions often require frequent revisions as more relevant data are available or more data is made available, or the
understanding of the process is enhanced. Although neural networks can approximate arbitrary functions [3], competent neural
networks for complicated applications are unrealistic for the above reasons, which motivates this study to devise an effective,
realistic approach for such applications.
The obvious drawbacks of traditional approaches to suitable neural network models include a lack of ﬂexibility given new
data sources and knowledge, difﬁculty in improving problem modeling and decomposition, and an inability to employ the
proven efforts of others. The main idea of the proposed composite neural network is to compose several neural network
models, especially pre-trained models (i.e., neural network models with instantiated weights), based on the availability of data
and domain knowledge, to solve complicated applications.
An emerging trend in deep learning solution development is to employ well-crafted pre-trained neural networks, especially
for use as a speciﬁc function/component to synthesize a neural network model. Many popular pre-trained neural network models
are ﬁne-tuned with adequate training data and made available to the public either as open-source or commercial products. In
practice, training a large neural network is infeasible due to the limitations of computing resources. Pre-trained components may
alleviate the problem by decomposing the problem into several sub-problems, each of which can be solved by a neural network
component which can be trained separately. The advantages of adopting a pre-trained model in composing a complicated neural
network are two-fold. One is beneﬁting from the intelligence and diligence of domain experts, and the other is saving effort
in data acquisition as well as computing resources and time for model training.
During the training phase of a composite network, the weights of pre-trained models are frozen to maintain their original
quality, and to save training time for less trainable parameters, whereas the weights of their incoming and outgoing edges
are trainable. Note that a user may choose the weights of a pre-trained component trainable for their particular purpose. For
instance, in transfer learning, the weights of the pre-trained network may be used as initial values in the training phase of the
overall neural network. Ensemble learning [4] and transfer learning [5] both apply additional data and neural network models
to improve accuracy. In deep learning, ensemble learning (Fig. 1(a)) employs multiple neural networks together to make
decisions whereas transfer learning (Fig. 1(b)) applies knowledge learned from other neural networks to assist in solving the
This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
arXiv:1910.09739v2  [cs.LG]  19 Jul 2021

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
Fig. 1. Illustrations of (a) ensemble learning, (b) transfer learning, and (c) composite neural network.
Fig. 2. Framework of Composite Neural Network Construction
original problem. Although all the models consider pre-trained components, the ensemble learning basically adopts homogeneity
learners (i.e., learners for the same function), and transfer learning methods are applied in the situation of insufﬁcient training
data. However, the proposed composite neural network is mainly for the incorporation of solutions of sub-problems, regardless
heterogeneous not.
Ensemble learning and transfer learning have their constraints. Ensemble learning ensembles learners that must have the
accuracy > 50% [6]. Transfer learning [5] assumes, for a source-target domain pair, there is an intermediate representation
that can be transferred for domain adaptation. Unlike ensemble learning or transfer learning, the proposed composite network
framework allows a generic condition with a statistical performance guarantee. In addition, in the literature, some negative
effects have been observed, e.g., Zhou et al. [7] pointed out that “many could be better than all” in the typical ensemble settings,
and Chen et al. [8] showed the transfer learning has suffered from the “negative transfer” problem. The papers of Dˇzeroski et
al. [9] and of Gashler et al. [10] also concluded that an ensemble is not always strictly better than its best component because
of the low diversity between members. These facts imply that the claim ”the more components, the better performance” may
be not always true. The gap of theoretical analysis that supports or opposes the claim motivates this study.
PM2.5 (particulate matter with a diameter less than 2.5 µm) has become a great concern due to its proven threat to human
health [11]. PM2.5 is a collection of aerosol material primarily composed of ammonium sulfate, ammonium nitrate, organic
carbonaceous mass, elemental carbon, and crustal mineral material emitted from sources such as vehicles, power plants and
factories, fossil fuel burning, construction, farming activities, sea salt and dust, and remote transportation [2], [12]–[14]. Both
the constituents and sources of PM2.5 vary from one location to the other [12], [13], from one season to the other [2], [14]. For
instance, for the seaside rural areas, dust and sea salt are the major causes, while in industrialized countries, fossil fuel burning
is the major source. Therefore, PM2.5 prediction must be temporally and spatially dependent. The life cycle and dispersion
of PM2.5 depend on factors such as the type of PM2.5, weather conditions, terrain context, and chemical transformations that
furthermore complicate the PM2.5 prediction [2]. As a result, predicting the PM2.5 level in the next few hours for a particular
area is a great challenge.
In this paper, we answer the challenge of solving complicated applications, and propose a framework and construction
algorithms for a composite neural network. Then we use the complicated application – PM2.5 prediction to demonstrate the
efﬁcacy of the composite neural network and its applicability to complicated real-world problems. As illustrated in Fig 2, ﬁrst,
a complicated application is decomposed into subtasks, and then some of them are selected as the candidates of pre-trained
components. Once the pre-trained components are trained separately or obtained elsewhere, and the topology is decided, then
an end-to-end training is performed to construct the ﬁnal composite neural network.
The contributions in this paper are the following. (1) We propose a framework for the composite neural network, and provide
a theoretical analysis of statistical performance guarantee. (2) We provide two heuristic algorithms with alternative composite
neural network design principles for performance comparison. (3) We empirically evaluate the performance of composite neural
network algorithms and several traditional machine learning methods on PM2.5 prediction data sets; the outcomes support the
proposed theory.
The remainder of this paper is organized as follows. We introduce the composite neural network in Section 2, and analyze
its performance bounds in Section 3. Section 4 includes several algorithms for composite neural network construction. Section
5 shows intensive evaluations of various composite neural network constructions and traditional machine learning methods,
and their comparisons. We discuss related work in Section 6 and the issues discovered during this study in Section 7.
2

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
II. CONCEPT OF COMPOSITE NEURAL NETWORK
A typical single-layer neural network can be presented as fσ,W1(x)=w1,1σ
Pd
i=1 w0,ixi + w0,0

+w1,0, where x is the input
vector, W1 is the matrix of weights, and σ is the activation function. In this work we consider differentiable activation functions
σ : R →R such as the the logistic function σ(z) = 1/(1 + e−z) and the hyperbolic tangent σ(z) = (ez −e−z)/(ez + e−z).
If there is no ambiguity on the activation function, the σ function is skipped to simplify notation and the neural network is
denoted as fW(x).
A composite neural network (also termed a composite network) is composed of a set of pre-trained and non-instantiated
neural network models that form a directed acyclic graph. For a pre-trained model, its weight matrix Wj is ﬁxed after its
original training process, denoted as fj to distinguish it from a non-instantiated network. A non-instantiated network is denoted
as fWj; its weights Wj are not determined until the completion of the training process of the whole composite neural network.
Both pre-trained and non-instantiated networks are called components of a composite neural network.
TABLE I
SUMMARY OF NOTATIONS
notation
deﬁnition
W
a matrix of weights in a neural network
σ(z)
activation functions in a neural network
[N]
{1, ..., N}; further, [K]+ ≜{0, 1, ..., K}
{(x(i), y(i))}i∈[N]
a set of N input-label pairs
fσ,W(x)
a neural network deﬁned by σ and W
{fj(x)}j∈[K1]
a set of K1 pre-trained networks; K1 ≥1
{fWj (x)}j∈[K2]
a set of K2 non-instantiated networks
{hj(x)}j∈[K]
{fj(x)}j∈[K1] ∪{fWj (x)}j∈[K2]
Θ
a matrix of weights in a composite network
gΘ(h1, ..., hK)
an r-layer composite network of hjs by σ, Θ:
LΘ(r+1)

σ(r+1)

· · · σ(1)

LΘ(0) (h1, ..., hK)

L(Θ; h1, ..., hK)
PK
j=0 θjhj(x); h0 = 1, linear combination
⃗hj
(hj(x(1)), · · · , hj(x(N))); ⃗h0 ≜⃗1
⃗ej
an unit vector in the standard basis of RK+1
BK+1
{⃗ej}j∈[K]+
For a given set of K components {hj(x)}K
j=1, each component hj, which can be pre-trained or non-instantiated, has
an input vector x and an output vector yj. Let h0 be the constant function 1. Then the linear combination with a bias
Θ = (θ0, θ1, . . . , θK) is deﬁned as L(Θ; h1, ..., hK) = PK
j=0 θjhj(x). When Θ is learned in the training phase, the composite
network is denoted as LΘ(h1, ..., hK). To extend the notation further, a neural network with h hidden layers is denoted as
LΘ(h+1)

σ(h+1)

· · · σ(1)

LΘ(0) (h1, ..., hK)

, illustrated as in Fig. 2(c), where the braced number in the subscript indicates
the layer number. The components can be in any layer and its output can be fed to any components in the upper layers.
Example 1 shows an example composite network.
Example 1. A composite neural network σ(2)(θ1,0 + θ1,1f4(x4) + θ1,2σ(1)(θ0,0 + θ0,1f1(x1)+ θ0,2fW2(x2) + θ0,3f3(x3))),
as depicted in Fig. 2(c), can be denoted as σ(2)
 L(1)
 f4, σ(1)
 L(0)(f1, fW2, f3)

, with Θs removed for simplicity.
We assume that the training algorithm of the composite network is the stochastic gradient descent backpropagation algorithm
and the loss function is the L2-norm of the difference vector. The loss function for a trained composite neural network gΘ is
deﬁned as
EΘ (x; gΘ) = ⟨gΘ (x) −⃗y, gΘ (x) −⃗y⟩
N
,
(1)
where ⟨·, ·⟩is the standard inner product and ⃗y is the ground truth. EΘ (x; gΘ) may be shortened to E (gΘ). Clearly, the total
loss depends on the training data x, the components deﬁned by {hj}K
j=1, the output activation σ, and the weight vector W.
Deﬁne E(x; fj) (shortened to E(fj), if there is no ambiguity) as the loss function of a single component fj. It is expected
that a good composite network design has low L2 loss, in particular lower than all its pre-trained components. Therefore, the
goal is to ﬁnd a feasible Θ such that it meets the “No-Worse” property, i.e., E (gΘ) < minj∈[K] E(fj).
In the following section we will prove that in some reasonable conditions, with high probability, a composite network has
strictly lower training L2 loss than all of its pre-trained components. The expectation of L2 loss of a composite network is
also with high probability lower than the expectation of the loss of all its pre-trained components. Furthermore, we will show
a multi-layer composite network of mixed non-instantiated and pre-trained models that also, with high probability, performs
better than any of its pre-trained models.
III. THEORETICAL ANALYSIS
In this section, we ﬁrst analyze the loss functions of a single-layer composite network, and subsequently extend the analysis
to a complicated composite network to explore the characteristics of the composite network. Due to limited space, only ideas
3

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
and sketches of proof are presented in this section. For the complete proof, please refer to the appendix in the supplementary
material.
A composite network constructed from a given set of pre-trained components {fj}K
j=1 forms an acyclic directed graph, which
can be represented by postorder tree traversal. Without loss of generality, we assume the dimension of the output vector of all
components is 1 in the following proofs. We denote [K]+ the set from 0 to K, ⃗f0 = ⃗1, and ⃗fj = (fj(x(1)), · · · , fj(x(N)))
as the sequence of the status of fj with input data x during the training phase. Similarly, the representation of the ground
truth is ⃗y := (y(1), · · · , y(N)). Let ⃗ej be an unit vector in the standard basis of RK for j ∈[K], e.g., ⃗e1 = (1, 0, · · · , 0) and
BK := {⃗ej}K
j=1. By C1-mapping (function) we mean the mapping is differentiable and its derivative is a continuous function.
The following assumptions are the default conditions in the following proofs.
A1. Linearly independent components assumption:
∀i ∈[K]+, ∄{βj} ⊂R, s.t. ⃗fi = P
j∈[K]\{i} βj ⃗fj.
A2. No perfect component assumption:
minj∈[K]
nP
i∈[N] |fj(x(i)) −y(i)|
o
> 0.
A3. The activation function and its derivative are C1-mappings (i.e., it is differentiable and its differential is continuous) and
the derivative is non-zero at some points in the domain.
A4. The number of components, K, is less than 2
√
N −1, where N is the size of the training data set.
A. Single-Layer Composite Network
The ﬁrst theorem states that if a single-layer composite network satisﬁes the above ﬁve assumptions, it meets the “No-Worse”
property with high probability.
Theorem 1. Consider a single-layer composite network g(x) = L(1)(σ(L(0)(f1, ..., fK)))(x). Then with probability of at least
1 −K+1
√
N there exists Θ = {Θ1, Θ0} s.t. EΘ (x; g) < minj∈[K] E(fj(x)).
We discuss two cases of the activation σ.
• Case 1: σ is a linear function.
• Case 2: σ is not a linear function.
(Case 1) σ is a linear activation such that a single-layer composite network such as L(1)(σ(L(0)(f1, ..., fK))) can be rewritten
as a linear combination with bias, i.e., gθ(x) = P
j∈[K]+ θjfj(x) with a mean squared error of EΘ (x; g) = 1
N
PN
i=1(gΘ(x(i))−
y(i))2. Clearly, the composite network gθ should have a mean squared error equal to or better than any of its components fj,
as gθ can always act as its best component. To obtain the minimizer Θ∗for the error EΘ (x; g), we must compute the partial
differential ∂EΘ/∂θj for all j ∈[K]+. After some calculations [15], we have Eq (2).
Θ∗= [θj]j∈[K]+ =
h
⟨⃗fi, ⃗fj⟩
i−1
i,j∈[K]+ ×
h
⟨⃗fi, ⃗y⟩
i
i∈[K]+
(2)
Since Assumption A1 holds, the inverse matrix
h
⟨⃗fi, ⃗fj⟩
i−1
i,j∈[K]+ exists and can be written down concretely to obtain Θ∗as
in Eq. (2). Lemma 1 summarizes the above arguments.
Lemma 1. Set Θ∗as in Eq. (2); then
E(gΘ∗) ≤min
j∈[K]+{E(fj)}.
(3)
There is a ≤constraint on the loss function E(gΘ∗) in Eq. (3) that is replaced by < and a probability bound. If Θ∗is
not a unit vector, it is obvious that E(gΘ∗) must be less than any E(fj). Therefore, we proceed to estimate the probability of
Θ∗= ⃗ej∗, where j∗∈[K]+.
∀i ∈[K]+, ∂E
∂θi

Θ=⃗ej∗= 2⟨⃗fj∗−⃗y, ⃗fi⟩
(4)
Eq. (4) shows the gradient of the error function with respect to θi conditioned on Θ∗= ⃗ej∗, which is the inner products of
the difference between fj∗(the output of gΘ∗) and the ground truth ⃗y, and the output of each pre-trained component ⃗fi. When
the minimizer Θ∗= ⃗ej∗, all the differentials ∂E
∂θi must equal zero, i.e., ⟨⃗
fj∗−⃗y, ⃗fi⟩= 0, or ⃗
fj∗−⃗y is perpendicular to ⃗fi. The
following Lemma 2 is an implication from the proof of the Johnson-Lindenstrauss Lemma [16], which states that a randomly
sampled unit vector ⃗v (denoted as Pr⃗v∈RN ) is approximately perpendicular to a given vector ⃗u with high probability in a high
dimensional space..
Lemma 2. For a large enough N and given ⃗u ∈RN, there is a constant c > 0, s.t. for η = cos−1(1 −c/
√
N),
Pr
⃗v∈RN
n
|∠⃗u,⃗v −π
2 | ≤η
o
≥1 −
1
√
N
(5)
where ∠⃗u,⃗v is the angle between ⃗u and ⃗v.
4

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
The complement of Eq. (5) is
Pr
⃗v∈RN
n
|∠⃗u,⃗v −π
2 |>η
o
< 1
√
N
(6)
Note that angles ∠⃗y, ⃗f , ∠⃗f−⃗y, ⃗f, and ∠⃗f−⃗y,−⃗y are the three inner angles of the triangle such that ∠⃗y, ⃗f +∠⃗f−⃗y, ⃗f +∠⃗f−⃗y,−⃗y = π.
From Lemma 2, as ∠⃗y, ⃗f is likely a vertical angle (i.e., π/2), ∠⃗f−⃗y, ⃗f must be less likely to be a vertical angle, which implies
Pr{⟨⃗f −⃗y, ⃗f⟩= 0} ≤Pr{|∠⃗f−⃗y, ⃗f −π/2| < η}; thus, ≤Pr{|∠⃗y, ⃗f −π/2| > η}. The following Lemma 3 immediately follows
Lemma 2 and Eq. (6).
Lemma 3. Following Lemma 2, then for given ⃗y ∈RN,
Pr
⃗f∈RN
n
⟨⃗f −⃗y, ⃗f⟩= 0
o
<
1
√
N
.
Lemma 3 shows that the probability of the output of one component is perpendicular to the difference between itself and
the ground truth. For K components and a bias, Lemma 4 gives a worst bound.
Lemma 4. Pr

E(gΘ∗) = minj∈[K]+{E(fj)}
	
< K+1
√
N , i.e., Pr

∃Θ∗: E(gΘ∗) < minj∈[K]+{E(fj)}
	
≥1 −K+1
√
N .
(Case 2) σ is not a linear function. The idea of the proof is to ﬁnd an interval in the domain of σ such that the output of
L(1)(σ(·)) approximates a linear function as close as possible. This means there is a setting such that the non-linear activation
function performs almost as well as the linear one; since the activation L(1)(σ(·)) acts like a linear function, the lemmas of
Case 1 are applicable. The conclusion of this case is stated as Lemma 7, while we introduce important properties in Lemmas 5
and 6 for key steps in the proof.
Since σ satisﬁes Assumption A3, the inverse function theorem of Lemma 5 is applicable.
Lemma 5. (Inverse function theorem [17])
Suppose µ is a C1-mapping of an open set E ⊂Rn to Rn, µ′(z0) in invertible for some z0 ∈E, and y0 = µ(z0). (I.e., µ
satisﬁes Assumption A3.) Then
(1) there exist open sets U and V in Rn such that z0 ∈U, y0 ∈V , µ is one-to-one on U, and µ(U) = V ;
(2) if ν is the inverse of µ, deﬁned in V by ν(µ(x)) = x for x ∈U, then ν ∈C1(V ).
We also need the following lemma as an important tool.
Lemma 6. (Taylor’s theorem with Lagrange remainder [18])
If a function τ(y) has continuous derivatives up to the (l + 1)-th order on a closed interval containing the two points y0 and
y, then
τ(y) = τ(y0) + τ (1)(y0)(y −y0) + · · · + τ (l)(y0)
l!
(y −y0)l + Rl
with the remainder Rl given by the expression for some c ∈[0, 1]:
Rl = τ (l+1)(c(y −y0))
(l + 1)!
(y −y0)l+1.
Let l = 1, τ(y) be obtained such that
τ(y) = τ(y0) + τ (1)(y0)(y −y0) + τ (2)(c(y −y0))
2!
(y −y0)2.
(7)
The second-degree term can be used to bound the approximation error.
Now we are ready to give more details to sketch the proof of Case 2. Denote Θ∗
0 as the minimizer of Case 1, i.e., the
corresponding gΘ∗
0 = L∗
(0)(f1, ..., fK) satisﬁes E(gΘ∗
0) < minj∈[K]+{E(fj)} = E(fj∗) with high probability, and denote
Θϵ = {Θ1,ϵ, Θ0,ϵ} corresponding to
gΘϵ = L(1),ϵ(σ(L(0),ϵ(f1, ..., fK))),
(8)
called the scaled σ function. Lemma 7 below states a clear condition of a linear approximation of a non-linear activation
function.
Lemma 7. For the given gΘ∗
0, {x(i)}i∈[N], and any 0 < ϵ ≤1, there exists Θϵ = {Θ1,ϵ, Θ0,ϵ} such that
∀i ∈[N], |gΘϵ(x(i)) −gΘ∗
0(x(i))| < ϵ.
(9)
Furthermore, for small enough ϵ,
Pr

E(gΘϵ) < min
j∈[K]+{E(fj)}

≥1 −K + 1
√
N
.
(10)
5

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
From the deﬁnition of gΘϵ, ﬁnding a proper L(0),ϵ(·) and L(1),ϵ(·) are the major steps in the proof of Eq. (9). L(0),ϵ(·)
maps the output range of gΘ∗
0(x) to an interval (−γ + z0, γ + z0) ⊂U0 for some γ > 0 satisfying σ′(z0) ̸= 0. The scaling
factors M0 and L(0),ϵ(·) are deﬁned as
M0 = 2
γ max
i∈[N]{|gΘ∗
0(x(i))|}
(11)
L(0),ϵ(x) = M −1
0 gΘ∗
0(x) + z0.
(12)
It is clear that the range of L(0),ϵ(x) falls within U0. L(1),ϵ(y) intends to map the output range of σ back to gΘ∗
0(·), and is
deﬁned as the expansion of τ(·) following Eq. (7) without the error term.
L(1),ϵ(y) = M0 · τ (1)(y0) · y + M0 ·

z0 −τ (1)(y0) · y0

.
(13)
Reversing the scaling and translating, Eq. (13) can be rewritten as
M0

τ(y0) + τ (1)(y0)(σ
 M −1
0 gΘ∗
0(x) + z0

−y0)

−z0,
(14)
which equals gΘ∗
0(x) plus an error bounded by M0M1γ2, where
M1 = 5 sup
z∈U0
(
|τ (2)(σ(z) −σ(z0))| ·
 σ(z) −σ(z0)
z −z0
2)
.
(15)
The precise setting of γ can be obtained from M0M1γ2 < ϵ. Then, with γ and the properties of Lemmas 5 and 6, it can
be veriﬁed that gΘϵ(x(i)) = L(1),ϵ(σ(L(0),ϵ(x(i)))) ﬁts Eq. (9).
Eq. (9) implies (gΘϵ(x(i)) −y(i))2 < (|gΘ∗
0(x(i)) −y(i)| + ϵ)2, which can derive E(gΘϵ) < E(gΘ∗
0) + ∆(ϵ), where ∆(ϵ) is
an increasing function of ϵ when the other parameters are ﬁxed. Hence, if ϵ is small enough, we have ∆(ϵ) ≤
E(fj∗)−E(gΘ∗
0 )
3
.
By further considering E(gΘϵ) < E(gΘ∗
0) + ∆(ϵ), it is easy to see that E(gΘϵ) < E(fj∗). The probability of E(gΘ∗
0) < E(fj∗)
of Eq. (10) can be inferred from Lemma 4 of Case 1. Example 2 below shows how to construct a scaled activation function
that satisﬁes Eq. (9).
Example 2. Here we take a logistic function σ(z) =
1
1+e−z in the context of PM2.5 prediction to construct a scaled logistic
function. Let notations gΘ∗
0(·), z0, U0, V0, and τ(·) be as previously deﬁned. The assumption that the highest PM2.5 measurement
is less than 1000 (i.e., maxi∈[N]{|gΘ∗
0(x(i))|} < 1000) ﬁts the reality for most countries. Observe that σ(1)(0) = 1
4, σ(0) = 1
2,
and hence it is valid to set z0 = 0. Consider (−γ, γ) ⊂[−1, 1] and hence , y0 = σ(0) and y = σ(z) ∈(0.25, 0.75).
The inverse function of σ(z) is τ(y) = ln
y
1−y for y ∈(0, 1), which also can be represented as τ(y) = 4y −2 +
τ (2)(c(y−y0))
2
(y−y0)2 for some c ∈(0, 1) by Lemma 6. From Eq. (11), the scaling factors M0 = 2γ−1maxi∈[N]{|gΘ∗
0(x(i))|} <
2 · 103γ−1, and from Eq. (15), M1 = 5 supz∈U0
n
τ (2)(σ(z) −σ(z0)) [(σ(z) −σ(z0)) /(z −z0)]2o
, which is less than 50 for
z ∈(−γ, γ). From Eq. (14), the scaled logistic function as gΘϵ(x) = M0 ·
 4σ
 M −1
0 gΘ∗
0(x)

−2

.
Now we claim that for any given ϵ ∈(0, 1], gΘ∗
0(·) and {x(i)}i∈[N], we have |gΘϵ(x(i)) −gΘ∗
0(x(i))| < ϵ. Here is a
short veriﬁcation. Observe ∀i ∈[N], M −1
0 gΘ∗
0(x(i)) ∈(−γ, γ). Also, if z ∈(−γ, γ), then | τ (2)(c(y−y0))
2
|(y −y0)2 < M1γ2.
Recall that τ ◦σ(·) is an identity function, y = σ(M −1gΘ∗
0(x)), and |τ(y) −(4y −2)| < M1γ2. That is, |M −1
0 gΘ∗
0(x) −

4σ(M −1
0 gΘ∗
0(x)) −2

| < M1γ2. Multiply by M0 on both sides and replace the bracket term with gΘϵ(x); we have |gΘ∗
0(x)−
gΘϵ(x)| < M0M1γ2 < 105γ. Hence, setting γ = 10−5ϵ veriﬁes this claim.
From Lemma 7, we can conclude that there exists Θϵ such that a non-linear single-layer composite network performs at
least as well as the linear case with arbitrary small error. Thus, the proof of Case 2 is concluded. The proofs of Cases 1 and
2 above complete the proof of Theorem 1.
B. Complicated Composite Network
In the previous section, we investigated the performance of a single-layer composite network comprising several pre-trained
components connected by an activation function. Now we consider expanding the composite network in terms of width
and depth. Formally, for a given pre-trained component fK and a trained composite network gK−1 of K −1 components
(f1, ..., fK−1), we study the following two questions in this section.
Q1: (Adding width) By adding a new pre-trained component fK, we deﬁne gK = L(1)(σ(L(0)(f1, ..., fK−1, fK)). Is there Θ
such that E(gK−1) > EΘ(gK)?
Q2: (Adding depth) By adding a new pre-trained component fK, let gK = L(K)(σ(L(K−1)(gK−1, fK)). Is there Θ such that
E(gK−1) > EΘ(gK)?
Lemma 8 answers Q1, and we require Proposition 1 as the base of induction to prove it.
6

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
Lemma 8. Set gK = L(1)(σ(L(0)((f1, ..., fK−1, fK))). With probability of at least 1 −K+1
√
N , there is Θ s.t. E (gK−1) >
EΘ (gK).
Proposition 1. Consider the case of only two pre-trained models f0 and f1. There exists (α0, α1) ∈R2 s.t.
X
i∈[N]
(f1(x(i)) −y(i))2 >
X
i∈[N]

α0f0(x(i)) + α1f1(x(i)) −y(i)2
with a probability of at least 1 −
2
√
N .
Proposition 1 can be proved by solving the inequality directly for the case of K = 2, and then generalizing the result to
larger K by induction with the help of Lemma 3 to prove Lemma 8. Adding a new component fK to a composite network
gK−1 as in Q2, the depth of resulting gK increments by 1. If ⃗gK−1 and ⃗fK satisfy A1 and A2, consider {gK−1, fK} as a
new set of {f1, f2} in the same layer. Consequently, we can apply the arguments in Case 2 of Theorem 1 to show Lemma 9
in the following, which answers Q2 and says the resulting gK has a minimizer Θ∗such that with high probability the loss
decreases.
Lemma 9. Set gK = L(1)(σ(L(0)((gK−1, fK)). If ⃗gK−1 and ⃗fK satisfy A1 and A2, then with a probability of at least 1−
2
√
N ,
there is Θ s.t. E (gK−1) > EΘ (gK).
The proof of Lemma 9 is similar to the proof of Case 2 in the previous sub-section. Lemmas 8 and 9 imply a greedy
strategy to build a complicated composite network. Recursively applying both lemmas, we can build a complicated composite
network as desired. Theorem 2 gives a formal statement of the constructed complicated composite network with a probability
bound. The proof of Theorem 2 is based on mathematical induction on layers and the worst case probability is over-estimated
by assuming each layer could have up to K components.
Theorem 2. For an H-hidden layer composite network with K pre-trained components, there exists Θ∗s.t.
EΘ∗(g) <
min
j∈[K]+{E(fj)}
with a probability of at least

1 −K+1
√
N
H
.
IV. COMPOSITE NETWORK CONSTRUCTION
The theoretical analysis in the previous section suggests that with high probability, a trained composite network performs
better than any of its pre-trained components. It also encourages users to apply their domain expertise to design and train critical
pre-trained components and incorporate them in their composite network. In this section, we propose heuristic algorithms for
composite network construction. Ensemble learning is a simple case of the composite network that will be evaluated and
compared with the proposed algorithm.
For a given set of components, we deﬁne the component whose output gives an answer to the main problem as a base
component. If the outputs of a component do not directly answer the main problem, we call this an auxiliary component. For
example, in the problem of PM2.5 value prediction, the base components output their PM2.5 predictions, whereas a component
predicting weather conditions such as wind speed and precipitation is categorized as an auxiliary component.
The Deep Binary Composite Network (DBCN) Algorithm depicted in Algorithm 1 is a greedy method, the main idea of
which is to construct a composite network by inserting one component at a time in some particular order. After each insertion,
the depth of the network is increased by 1, as described in Lemma 9. We consider the base components ﬁrst in the insertion
order since a base component answers the main problem and it makes sense to use auxiliary components to enhance the
performance of the base components later. The pre-trained components are considered before the non-instantiated ones, as pre-
trained components are commonly well-crafted and performance-proven. Thus, we insert the components such that pre-trained
components are ahead of non-instantiated components, and for each pre-trained and non-instantiated set, the base components
are ahead of auxiliary components; ﬁnally, the components with lower L2 errors are before those with higher L2 errors.
Algorithm 1 takes pre-trained components {fj}K1
1
and non-instantiated components {fWj}K
K1+1, sorted according to the
criteria in the previous paragraph, as inputs, and outputs a deep binary composite network. Line 1 initializes the variables used
in this algorithm. The ﬁrst-level for block (from Lines 2 to 12) computes the composite network gj of depth j, iteratively.
The second-level for block from Lines 3 to 9 generates possible composite networks with both linear and modiﬁed logistic
activation functions σ(·). In Line 10, we use traditional stochastic gradient descent backpropagation to train every composite
network in Tj. Line 12 ﬁnds the composite network with the lowest L2 error. Lines 13 to 20 prune the obtained {gj} to avoid
over-ﬁtting. Once the L2 loss gain is larger than a speciﬁed pruning threshold ∆, the pruning process stops and the algorithm
outputs the current gj; otherwise, gj−1 is examined as a consequence.
The second algorithm, Balanced Base Composite Network (BBCN), is presented in Algorithm 2. The ﬁrst-level for block
(from Lines 4 to 16) generates a ﬂat composite network from the base components, in which each iteration constructs a level
of the composite network. The for block (Lines 5 to 15) combines a pair of two base components or two subtrees. Line 18
7

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
Algorithm 1: Deep Binary Composite Network
Input: F = {fj}K1
0
∪{fWj }K
K1+1, a set of activation functions A, pruning threshold ∆
Output: gK
1 g1 ←f1; ∀j ≤K, Tj ←∅
2 for j = 2 to K do
3
for σ(·) ∈A do
4
if j ≤K1 then
5
Tj ←Tj ∪{σ(gj−1, fj)}
6
else
7
Tj ←Tj ∪{σ(gj−1, fWj )}
8
end
9
end
10
Train all h ∈Tj
11
gj ←argminh∈Tj {E(h)}
12 end
13 for j = K to 2 do
14
if E(gj) −E(gj−1) ≤∆then
15
gj ←gj−1
16
else
17
output gj
18
break
19
end
20 end
Algorithm 2: Balanced Base Composite Network
Input: F = {fj}K1
0
∪{fWj }K
K1+1, a set of activation functions A, the number of base components K0, pruning threshold ∆
Output: gK
1 ∀j ∈[K0], h0,j ←fj
2 ∀s ≤⌈log2(K0)⌉, t ≤⌈K0/2s⌉, Ts,t ←∅
3 ∀j ≤K, Tj ←∅
4 for s = 1 to ⌈log2(K0)⌉do
5
for t = 1 to ⌈K0/2s⌉do
6
if ⌈K0/2s−1⌉is an odd number & t = ⌈K0/2s⌉then
7
hs,t ←hs−1,2t−1 ;
8
else
9
for σ(·) ∈A do
10
Ts,t ←Ts,t ∪{σ(hs−1,2t−1, hs−1,2t)};
11
end
12
Train all h ∈Ts,t;
13
hs,t ←argminh∈Ts,t{E(h)} ;
14
end
15
end
16 end
17 gK0 ←h⌈log2(K0)⌉,1
18 Run Algorithm 1 on ({gK0} ∪F \ {fj}j∈[K0])
calls Algorithm 1 to complete the execution. In general, Algorithm 1 generates a deep binary composite network, whereas
Algorithm 2 constructs a more balanced composite network, as shown in Fig. 4.
V. PM2.5 PREDICTIONS
In this section, we design ﬁve pre-trained components and a non-instantiated component and apply composite network
construction methods including exhaustive search, ensemble learning [6], and Algorithm 1 (DBCN) and Algorithm 2 (BBCN)
for PM2.5 prediction. Real-world open data was used to numerically compare the performance of different construction methods
and to examine the correctness and efﬁcacy of the proposed theory. In addition, we also compared the methods with traditional
machine learning methods, namely, SVM [19] and random forests [20]. For the hardware and software environment, each of
the three servers used in this evaluation was equipped with two Intel Xeon CPUs, 128GB memory, four NVIDIA 1080 GPUs,
the Linux operating system, and Keras and Tensorﬂow as deep learning platforms.
A. Datasets
The open data were from two sources: the Environmental Protection Administration (EPA) for air quality data [21], and
the Central Weather Bureau (CWB) for weather data [22]. There are 21 features in the EPA dataset including values such as
PM2.5, PM10, SO2, CO, NO, and NOx. The EPA air quality data were collected from eighteen monitoring stations recorded
hourly. The second dataset, the CWB open data, has one record per six hours, collected from 31 monitoring stations with 26
8

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
features, including temperature, dew point, precipitation, and wind speed and direction. In this study, for all evaluations, the
data of years 2014 and 2015 were used as training data and those of 2016 as testing data.
We created a grid of 30 × 38 = 1140 km2 covering the Taipei area, each block of which was 1 × 1 km2. The EPA and
CWB data were loaded into the corresponding blocks so that both datasets were temporally aligned at the hour scale (i.e., one
record per hour). Interpolation was applied to the CWB data to downscale from 6 hours to 1 hour. Note that there were 1140
blocks in the grid, whereas there were only 18 EPA stations and 31 CWB stations; thus more than 1000 blocks were empty,
i.e. without EPA or CWB data. We adopted the KNN method (K = 4, i.e., averaging the values of the four nearest neighbors)
to initialize the values of the empty blocks, as discussed in [23].
B. Pre-trained Component Design
Here we introduce the design rationales of the ﬁve pre-trained components in this evaluation. As PM2.5 dispersion is highly
spatially and temporally dependent, we designed four pre-trained components as base components to model this dependency.
Among these, two were convolutional LSTM neural networks (ConvLSTMs [24]) with the EPA data (denoted as f1) and
CWB data (denoted as f2) as input; the other two were fully connected neural networks (FNNs) with the EPA data (denoted
as f3) and CWB data (denoted as f4) as input. To model the temporal relationship conveniently using the neural network,
the data was fed to the pre-trained components one sequence at a time. We used two pairs of components—f1 and f2, and
f3 and f4—for the same functions to determine whether component redundancy improves performance. The ﬁfth pre-trained
component (denoted as f5) was to model the association between time and the PM2.5 value.
TABLE II
LSTM (LSM) V.S. CONVLSTM (CVL)
Hour
Dataset
EPA
CWB
Models
Training
Testing
Training
Testing
+24h
LsM
8.4158
10.9586
8.2741
11.3947
CvL
7.5873
10.5789
8.5529
11.2074
+48h
LsM
8.7185
11.5229
8.5232
11.8144
CvL
8.6541
11.3904
8.2890
11.7081
+72h
LsM
8.7530
11.7329
8.8905
11.8672
CvL
8.8170
11.5279
9.2177
11.7756
The ﬁrst experiment was designed to examine the effect of the grid structure in capturing the spatial relationship by comparing
the outcomes of LSTM and ConvLSTM. The LSTM model only used the EPA and CWB data without spatial information
about the monitoring stations, whereas the ConvLSTM model used the grid data (i.e., considering the whole 1140 blocks
with KNN (K = 4) initialization). The accuracy of both models measured in RMSE is presented in Table II, which shows
the ConvLSTM performs consistently better for the +24h (next 24 hours), +48h (next 48 hours), and +72h (next 72 hours)
predictions. Hence, we selected ConvLSTM as the model for f1 and f2.
TABLE III
VARIOUS CONFIGURATIONS OF PRE-TRAINED COMPONENTS
Forecast
+24h
+48h
+72h
Model
Train.Params
Training
Testing
Training
Testing
Training
Testing
f1
917492
7.5873
10.5789
8.6541
11.3904
8.8170
11.5279
f1,W r
3632482
9.3054
11.9440
9.1503
11.6550
8.1616
11.7556
f1,Dr
1278692
7.6342
10.9471
8.6297
11.4844
9.0803
11.5993
f2
916908
8.5529
11.2074
8.2890
11.7081
9.2177
11.7756
f2,W r
3631322
7.0685
11.4974
9.2233
12.0710
9.1766
11.9827
f2,Dr
790828
6.5404
11.7970
8.4491
8.4491
9.1500
11.9162
f3,(2)
1038054
11.6064
10.8907
11.9008
11.6977
12.1729
11.9999
f3,(3)
1068538
11.5648
10.9179
11.9726
11.7017
12.0585
11.9414
f4,(2)
582038
11.8238
11.3400
11.6948
11.6147
11.9484
11.8687
f4,(3)
603318
11.8253
11.2748
11.7112
11.6176
12.0199
11.7512
In the second experiment, we trained the four pre-trained components (f1, f2, f3, f4) individually with different conﬁgura-
tions. For instance, we trained the ConvLSTM models (f1 and f2) with a normal conﬁguration, a deeper one (denoted as Dr)
with stack of two LSTMs, and a wider one (denoted as Wr) with a double-width ConvLSTM. Similarly, FNN models f3 and
f4 were trained with two or three hidden layers, (denoted as fi, (2or3)). Their performance was measured by RMSE as shown
in Table III. The best performing conﬁgurations were selected for the pre-trained components in the following experiments.
Note that instead of using execution time as a measurement of time complexity, we indicated the complexity using the
number of trainable (tunable) parameters in our study, as shown in the second column of Table III, as the execution times
varied widely even for the same training conﬁguration due to diverse server execution contexts, randomness incurred from
training commands, and hyperparameter tuning setups.
9

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
Fig. 3. Annual PM2.5 values at different frequencies
The ﬁfth pre-trained component (f5) is the association between time and PM2.5 value, which is highly temporally dependent.
Fig. 3 shows the PM2.5 values resulting from the different frequency ﬁlters [25]. The top ﬁgure shows the original PM2.5
values of the Taitung EPA station in 2014 and the second ﬁgure shows the annual trend, which clearly shows that cold months
are prone to high PM2.5 pollution. The third graph shows the PM2.5 trend from May to July, which does not reveal a consistent
pattern. The fourth ﬁgure shows the trends within a week: we observe lower PM2.5 values during the weekend. The ﬁfth ﬁgure
is the daily trend: PM2.5 values are lower after midnight. Based on these observations, we generated an embedding [26] of
features including the month, day of the week, and the hour of the day, and trained a LSTM model labeled with PM2.5 values
as the pre-trained component f5.
C. Composite Network
TABLE IV
PRE-TRAINED COMPONENTS AND TESTING RMSE
Component
Data
+24h
+48h
+72h
f1: ConvLSTM (2 CNN layers, 1 LSTM)
EPA
10.5789
11.3904
11.5279
f2: ConvLSTM (2 CNN layers, 1 LSTM)
CWB
11.2074
11.7081
11.7756
f3: FNN (2 hidden layers)
EPA
10.6459
11.3291
11.6169
f4: FNN (2 hidden layers)
CWB
11.5112
11.6915
11.8017
f5: LSTM
hr-week-month
11.4738
11.5359
11.4540
EPA 9 features: CO, NO, NO2, NOx, O3, PM10, PM2.5, SO2, THC
CWB 5 features: AMB-TEMP, RH, rainfall, wind direction-speed (represented as a vector)
There are ﬁve pre-trained components from f1 to f5 and one non-instantiated auxiliary component, denoted as fW6, for the
composite network construction. The model of fW6 is a convolutional neural network (CNN) with CWB weather data and
forecasts as input to predict upcoming precipitation. The six components are connected by activation functions, either a linear
function or a scaled logistic function (S(z) = 2000/(1 + e−z/500) −1000). Note that any activation function that meets all
six assumptions in Sec. 3 could be used; for simplicity, we used only the scaled logistic function. The prediction accuracy
in RMSE of all ﬁve pre-trained components is listed in Table IV. Note that in this study we did not set out to design an
optimized composite network for the best PM2.5 prediction. Rather, our main purpose was to implement and evaluate the
proposed composite network theory. Nevertheless, the design of components and composite network follows the advice of
domain experts and exhibits reasonably good performance in PM2.5 prediction.
1) DBCN and BBCN: The step-by-step running of Algorithm 1 (DBCN) and the results are shown in Table V for the
+24h predictions. First, f1 is automatically selected as g1, after which f3 is included, as it has the lowest RMSE among the
remaining components. In the ﬁrst column of the table, L(g1, f3) has a lower RMSE than SL(g1, f3) and is selected as g2,
as marked in the last column (“Front-runner”). (Note that SL is an abbreviation of the scaled logistic function cascading a
linear function.) Next, Algorithm 1 generates the composite network L(g5, fW6) with a testing RMSE of 10.9531 for the +24h
prediction. Table VI shows the +48h and +72h prediction results: the generated models are different from each other and the
model for +24h.
The “Trainable/total” column indicates the number of trainable parameters and total parameters during the training phase. The
trainable parameters are updated during each backpropagation stochastic gradient descent optimization, and the total parameters
are the number of trainable parameters plus the ﬁxed parameters in the pre-trained components. As only the trainable parameters
are updated during training, the composite network framework may greatly alleviate many burdens in training a complicated
composite network.
The processes and results of Algorithm 2 (BBCN) are shown in Table VII for the +24h PM2.5 predictions and in Table VIII
+48h and +72h. Note that Algorithm 2 constructs a composite network by merging the base components in the beginning:
10

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
TABLE V
COMPOSITE NETWORKS USING ALGO 1: DBCN, +24H
RMSE
Parameters
Model
Training
Testing
Trainable/total
Front-runner
g1 ←f1
0/
L(g1, f3)
7.2128
10.2277
666/1956230
g2
SL(g1, f3)
7.3311
10.3454
666/1956230
L(g2, f2)
7.1364
10.2410
666/2873814
SL(g2, f2)
7.3208
10.2409
666/2873814
g3
L(g3, f4)
7.0787
10.3039
666/3456518
g4
SL(g3, f4)
7.1931
10.3501
666/3456518
L(g4, f5)
7.0911
10.3275
666/4411100
SL(g4, f5)
7.0560
10.2119
666/4411100
g5
L(g5, fW6 )
6.9608
10.1131
42046/4453146
SL(g5, fW6 )
6.9705
10.1053
42046/4453146
g6
TABLE VI
COMPOSITE NETWORKS USING ALGO 1: DBCN, +48H, +72H
RMSE
Parameters
Prediction
Model
Training
Testing
Trainable/total
Front-runner
+48h
SL(g4, f2)
8.0678
11.0469
666/4411100
g5
SL(g5, fW6 )
7.8941
10.9531
42046/4453146
g6
+72h
L(g4, f3)
8.2305
11.4274
666/4411100
g5
L(g5, fW6 )
8.2448
11.2541
42046/4453146
g6
the ﬁrst row of Table VII combines f1 and f2, and the second row combines f3 and f4. Generally, both DBCN and BBCN
methods meet the claim of the proposed composite network theory: combining more pre-trained components yields improved
RMSE results. The composite networks constructed using Algorithms 1 and 2 for +24h prediction are contrasted in Fig. 4.
2) Exhaustive Search Construction: In this subsection, an exhaustive search method based on Algorithm 2 is introduced to
construct a high-accuracy PM2.5 prediction composite network for use as a high-mark benchmark for comparison. In contrast
to the previous approaches, in the exhaustive search approach the parameters inside a pre-trained component can be either
ﬁxed or open in order to guarantee the best construction. Hence, instead of the 5 pre-trained and 1 non-instantiated components
used by the previous algorithms, we now have ﬁve additional pre-trained components with open (tunable) parameters (i.e.,
non-instantiated components). The new notation × denotes pre-trained components and ◦denotes non-instantiated components.
For instance, f ◦
1 is component 1 but non-instantiated. Inherently, with exhaustive search the construction takes a substantially
longer time to complete (i.e., with time complexity of O(2K) ), but has the potential for better performance. A complete
exhaustive search example for PM2.5 prediction is conducted to evaluate the performance improvement.
For +24h prediction, the the exhaustive search algorithm employs the same composite network layout as Algorithm 2. The
best composition combining f1 and f2 is g1 = SL(f ◦
1 , f ◦
2 )), as shown in Table IX, which corresponds to combining non-
instantiated f1 and f2 and applying the scaled logistic activation function results in the lowest RMSE. In the next step, f3 and
f4 are combined with the front-runner as g2 = SL(f ◦
3 , f ◦
4 )) as shown in Table X. Step 3 considers all possible combinations
of g1 and g2 to ﬁnd the best g3, as shown in Table XI. Note that we treat g◦
i as having all non-instantiated components; for
g×
i , all components are pre-trained.
Now only f5 and fW6 are not combined. Here we examine different sequences of f5 and fW6. In Steps 4a and 5a, f5 is
considered ﬁrst and then fW6. The results are shown in Table XII. Steps 4b and 5b consider the opposite sequence from the
results listed in Table XIII. The best models of g5a and g5b are illustrated in Fig. 5. The composite networks for +48h and
Fig. 4. Composite networks using Algorithms 1 (left) and 2 (right) for +24h prediction
11

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
TABLE VII
COMPOSITE NETWORKS USING ALGO 2: BBCN, +24H
RMSE
Parameters
Model
Training
Testing
Trainable/total
Front-runner
h0,1 ←f1, h0,2 ←f2
L(h0,1, h0,2)
7.1016
10.4075
666/1835094
h1,1
SL(h0,1, h0,2)
6.5801
10.4581
666/1835094
h0,3 ←f3, h0,4 ←f4
L(h0,3, h0,4)
11.4359
10.7670
666/1620758
h1,2
SL(h0,3, h0,4)
11.5389
10.8508
666/1620758
L(h1,1, h1,2)
7.2375
10.4536
666/3456518
SL(h1,1, h1,2)
7.2523
10.3226
666/3456518
h2,1
h2,2 ←h1,3 ←h0,5 ←f5
L(h2,1, h2,2)
7.1069
10.4712
666/4411100
h3,1
SL(h2,1, h2,2)
7.1202
10.5064
666/4411100
g5 ←h3,1
L(g5, fW6 )
6.9828
10.1938
42046/4453146
g6
SL(g5, fW6 )
6.9964
10.2257
42046/4453146
TABLE VIII
COMPOSITE NETWORKS USING ALGO 2: BBCN, +48H, +72H
RMSE
Parameters
Prediction
Model
Training
Testing
Trainable/total
Front-runner
+48h
SL(h2,1, h2,2)
7.9949
11.0516
666/4411100
h3,1
L(g5, fW6 )
8.5736
11.0182
42046/4453146
g6
+72h
L(h2,1, h2,2)
8.4460
11.5100
666/4411100
h3,1
L(g5, fW6 )
9.1848
11.4153
42046/4453146
g6
+72h predictions using exhaustive search were conducted accordingly and their results are used for performance comparisons
in the next subsection.
3)
Comparisons of All Methods: In this section, we compare the performance of different composite network algorithms,
including DBCN, BBCN, exhaustive search, and ensemble methods, as well as machine learning methods, SVM and random
forest. In addition, we use Relu and logistic activation functions to replace the scaled logistic function in DBCN and BBCN
to show the performance differences. As claimed, the composite network theory guarantees, with high probability, that the
composite network has lower RMSE than any of its components, which is supported by all DBCN, BBCN, exhaustive search,
and ensemble methods.
We summarize the results of all methods in Table XIV for RMSE, and in Table XX for MAE (mean absolute error) and
SMAPE (symmetric mean absolute percentage error). For the SVM and random forest experiments, we used the tools from
scikit-learn [27] with pre-trained components only (i.e., f1 to f5, with α the total parameters inside these ﬁve components.)
Likewise with ensemble learning and with ensemble learning with the scaled logistic function as the activation function (denoted
as SL(ensemble)). The four evaluations yielded close testing RMSE values for all predictions, but the ensemble learning method
performed slightly better, while the random forest method seems overﬁtted, as the training RMSE is low. DBCN performs
slightly better than DBNN, and the exhaustive search has the best outcome. For the activation functions, it is interesting to
discover that the scaled logistic function performs almost better than the regular logistic and Relu functions.
Now that fW6 is included in composite network construction, it can be seen that DBCN, DBNN, Exhaustive Search (a),
and Exhaustive Search (b), as depicted in Fig. 5, show improvements over the composite networks without fW6.The sum of
parameters inside these six components is denoted as β in Table XIV. The second column of the table gives the number
of trainable parameters for each evaluation; this shows that for the composite network the training parameters are moderate.
Table XX shows the MAE measurements of the evaluations in Table XIV. The ordering of the testing MAE results are very
Fig. 5. Composite networks of (a) Tables XII and (b) XIII for +24h prediction
12

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
TABLE IX
COMPOSITE NETWORKS OF f1, f2 USING EXHAUSTIVE SEARCH, +24H
Step
Model
Training
Testing
Front-runner
1
L(f×
1 , f×
2 )
7.1016
10.4075
L(f×
1 , f◦
2 )
6.5417
10.2097
L(f◦
1 , f×
2 )
6.6574
10.3484
L(f◦
1 , f◦
2 )
6.3394
10.0423
SL(f×
1 , f×
2 )
6.5801
10.4581
SL(f×
1 , f◦
2 )
6.6648
9.9048
SL(f◦
1 , f×
2 )
6.5052
10.1654
SL(f◦
1 , f◦
2 )
6.5109
9.7275
g1
TABLE X
COMPOSITE NETWORKS OF f3, f4 USING EXHAUSTIVE SEARCH, +24H
Step
Model
Training
Testing
Front-runner
2
L(f×
3 , f×
4 )
11.4359
10.7670
L(f×
3 , f◦
4 )
10.9690
10.8618
L(f◦
3 , f×
4 )
11.0442
10.8285
L(f◦
3 , f◦
4 )
11.2553
10.7017
SL(f×
3 , f×
4 )
11.5389
10.8508
SL(f×
3 , f◦
4 )
11.2916
10.9000
SL(f◦
3 , f×
4 )
11.1600
10.8505
SL(f◦
3 , f◦
4 )
11.1543
10.6877
g2
similar to that of the RMSE results.
VI. RELATED WORK
In this section, we discuss related work in the literature from the perspective of the composite network framework and
PM2.5 prediction. For the framework, the composite network is related to the methods such as ensemble learning [6], transfer
learning [28] and model reuse [29], [30]. We will also discuss some representative work on air quality prediction.
Ensemble Learning. Typical ensemble learning methods include bagging, boosting, stacking, and linear combination/regression.
Since the bagging groups data by sampling and boosting tunes the probability of data [7], these frameworks are not similar
to composite neural networks. However, there are ﬁne research results that are instructive for accuracy improvement [7], [9],
[10]. In this work, we consider the neural network composition, but not data enrichment.
Among the ensemble methods, stacking is closely related to our framework. The idea of stacked generalization [31], in
Wolpert’s terminology, is to combine two levels of generalizers. The original data are taken by several level-0 generalizers,
after which their outputs are concatenated as an input vector to the level-1 generalizer. According to the empirical study of
Ting and Witten [32], the probability distribution of the outputs from level 0, instead of their values, is critical to accuracy.
Their experimental results also imply that multi-linear regression is the best level-1 generalizer, and a non-negative weight
restriction is necessary for regression but not for classiﬁcation. However, our analysis shows that the activation functions that
satisfy Assumption A3 have a high probability guarantee of reducing the L2 error. In addition, our empirical evaluations show
that the scaled logistic activation usually performs well.
The work of Breiman [33] restricts non-negative combination weights to prevent poor generalization errors and concludes
that it is not necessary to restrict the sum of weights to equal 1. In [34], Hashem shows that linear dependence of components
could be, but is not necessarily always, harmful to ensemble accuracy, whereas our work allows a mix of pre-deﬁned and
non-instantiated components as well as negative weights to provide ﬂexibility in solution design.
Transfer Learning. In the context of one task with a very small amount of training data with another similar task that has
sufﬁcient data, transfer learning can be useful [35]. Typically the two data sets—the source and target domains—have different
distributions. A neural network such as an auto-encoder is trained with source-domain data and the corresponding hidden layer
weights or output labels are used for the target task. Part of transplanted weights can be kept ﬁxed during the consequent steps,
whereas others are trainable for ﬁne-tuning [28]. This is in contrast to the composite neural network, in which the pre-trained
weights are always ﬁxed.
Model Reuse. In recent years, some proposed frameworks emphasize the reuse of ﬁxed models [29], [30], [36]. In this
framework, pre-trained models are usually connected with the main (i.e., target) model, and then the dependency is gradually
weakened by removing or reducing the connections during the training process. In this way, the knowledge of the ﬁxed model
is transferred to the main model; the key point is that model reuse is different from transfer learning as well as the composite
neural network.
Pre-trained models are widely applied in applications of natural language processing to improve the generation ability of
the main model, such as in BERT [37] and ELMo [38]. Multi-view learning [39] is another method to improve generalization
performance. In this approach, a speciﬁc task owns several sets of features corresponding to different views, just like an object
13

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
TABLE XI
COMPOSITE NETWORKS OF g1, g2 USING EXHAUSTIVE SEARCH, +24H
Step
Model
Training
Testing
Front-runner
3
L(g×
1 , g×
2 )
5.8897
9.6059
L(g×
1 , g◦
2 )
5.6321
9.5278
L(g◦
1 , g×
2 )
4.9207
9.8139
L(g◦
1 , g◦
2 )
4.5724
9.5881
SL(g×
1 , g×
2 )
5.8941
9.6162
SL(g×
1 , g◦
2 )
5.3703
9.5250
g3
SL(g◦
1 , g×
2 )
4.7438
9.8185
SL(g◦
1 , g◦
2 )
4.1957
9.6039
TABLE XII
COMPOSITE NETWORKS OF g3 AND f5, THEN fW6, +24H
Step
Model
Training
Testing
Front-runner
4a
L(g×
3 , f×
5 )
5.3349
9.3055
L(g×
3 , f◦
5 )
5.2516
9.3186
L(g◦
3 , f×
5 )
5.5953
9.5570
L(g◦
3 , f◦
5 )
6.6938
9.4190
SL(g×
3 , f×
5 )
5.5415
9.4504
SL(g×
3 , f◦
5 )
5.1646
9.2438
g4a
SL(g◦
3 , f×
5 )
7.2401
9.4492
SL(g◦
3 , f◦
5 )
7.0476
9.4947
5a
L(g×
4a, f◦
W6 )
5.3665
9.2730
L(g◦
4a, f◦
W6 )
4.3968
9.4362
SL(g×
4a, f◦
W6 )
5.5986
9.1971
g5a
SL(g◦
4a, f◦
W6 )
5.5421
9.4882
observed from various perspectives, and separate models are trained accordingly. Then, the trained models for different views
are combined using co-training, co-regularization, or transfer learning methods.
Air Quality Forecasting. There are several air quality prediction systems that combine different components, although these
components are usually not pre-trained. In [40], Zheng et al. propose a model combining two components—an artiﬁcial neural
network as the spatial classiﬁer and a conditional random ﬁeld as the temporal classiﬁer—to infer air quality indices. Zheng et
al. [41] propose a prediction model for +48h forecasting composed of four components: a temporal predictor (linear regression),
a spatial predictor (neural network), a dynamic aggregator of both temporal and spatial predictors, and an inﬂection predictor
capturing sudden changes. According to the data provided by the monitoring stations, Hsieh et al. [42] propose a system to
predict the air quality class even for locations without monitoring stations. Furthermore, for locations with poor prediction, a
location is recommended to install a new monitoring station for best prediction. Their inference model is based on an afﬁnity
graph. In [12], Wei et al. employ transfer learning to address the problem of big cities with a large amount of air quality
data along with small cities that have insufﬁcient data to train a model from scratch. Using pre-trained components shows
strengths in ﬂexibility in design and efﬁciency in training, the work in
[13] presents well-thought component designs, and
feature engineering and encoding that are valuable for forthcoming PM2.5 prediction studies. Yi et al. [2] propose a deep neural
network consisting of a spatial transformation component and a deep distributed fusion network to fuse heterogeneous urban
data to capture the factors affecting air quality. The hybird architecture of CNN and Bi-LSTM trained from scratch by Du et
al. [43] is designed to learn the correlation and interdependence spatial-temporal information. In the reverse of decomposition,
Qi et al. [44] integrate the three tasks, feature analysis, prediction and interpolation, into one deep learning model.
VII. CONCLUSIONS
In this work, we investigate a composite neural network composed of pre-trained components connected by differentiable
activation functions. Through theoretical analysis and empirical evaluations, we show that if assumptions A1 to A4 are satisﬁed,
especially when training data is sufﬁcient, then a composite network has better performance than all of its components with
high probability.
While the proposed theory ensures overall performance improvement, it is still not clear how to decompose a complicated
problem into components and how to construct them into a composite network to yield acceptable performance. Another
problem worth investigating is when the performance improvements diminish even after adding more components. Note that
in real-world applications, the amount of data, the data distribution, and the data quality affect performance considerably.
REFERENCES
[1] F. Feng, X. He, X. Wang, C. Luo, Y. Liu, and T. Chua, “Temporal relational ranking for stock prediction,” ACM Trans. Inf. Syst., vol. 37, no. 2, pp.
27:1–27:30, 2019.
14

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
TABLE XIII
COMPOSITE NETWORKS OF g3 AND fW6, THEN f5, +24H
Step
Model
Training
Testing
Front-runner
4b
L(g×
3 , f◦
W6 )
4.5310
9.4551
L(g◦
3 , f◦
W6 )
4.6822
9.4677
SL(g×
3 , f◦
W6 )
5.5339
9.3423
g4b
SL(g◦
3 , f◦
W6 )
4.7831
9.4991
5b
L(g×
4b, f×
5 )
5.6073
9.3961
L(g◦
4b, f×
5 )
6.6991
9.2591
g5b
L(g×
4b, f◦
5 )
5.3298
9.2721
L(g◦
4b, f◦
5 )
7.1666
9.5607
SL(g×
4b, f×
5 )
5.4710
9.3313
SL(g×
4b, f◦
5 )
5.3607
9.3130
SL(g◦
4b, f×
5 )
6.2875
9.3586
SL(g◦
4b, f◦
5 )
6.5281
9.5541
TABLE XIV
SUMMARY OF ALL METHODS (RMSE)
+24h
+48h
+72h
Method
Trainable
Training
Testing
Training
Testing
Training
Testing
SVM
-
11.6440
10.9117
12.1246
11.5469
12.1670
11.6376
Random forests
-
3.3181
10.9386
3.4304
11.9037
3.4148
12.0917
Ensemble
1638
11.6955
11.0200
12.2609
11.3969
12.6605
11.6119
SL(Ensemble)
1638
11.5855
10.9184
12.2080
11.2815
12.5690
11.5411
DBCNRelu
2664
12.4800
11.4540
13.3464
12.1947
14.0421
12.6546
DBCNSigm
4032
11.7786
10.9803
13.6521
12.4418
13.4414
12.2825
DBCN
2664
7.0560
10.2119
8.0678
11.0469
8.2305
11.4274
BBCNRelu
2664
13.3711
12.4575
14.6168
13.2662
15.8200
14.0754
BBCNSigm
4032
12.5376
11.4600
13.0951
12.2047
13.5416
12.0388
BBCN
2664
7.1069
10.4712
7.9949
11.0935
8.4460
11.5100
Exhaustive-a
2664+α
5.1646
9.2438
5.0981
10.2402
6.7830
10.4265
(Include fW6 ), note that α =4408436, β =4449816
Ensemble
43684
11.5253
10.7338
12.4490
11.1874
12.5822
11.4804
SL(Ensemble)
43684
11.5117
10.8125
12.3939
11.1628
12.7025
11.3376
DBCNRelu
44710
12.9434
11.8209
14.3413
12.8331
14.3562
12.7689
DBCNSigm
46420
11.9444
10.9167
12.1700
10.9474
13.2754
11.8630
DBCN
44710
6.9705
10.1053
7.8941
10.9531
8.2448
11.2541
BBCNRelu
44710
11.4985
10.5742
12.0386
11.0392
12.7188
11.4047
BBCNSigm
46420
12.4675
11.3664
13.1786
11.9285
13.3815
11.8680
BBCN
44710
6.9828
10.1938
8.5736
11.0182
9.1848
11.4153
Exhaustive-a
44710+β
5.5986
9.1971
5.1292
10.2190
7.9572
10.3588
Exhaustive-b
44710+β
6.6991
9.2591
5.6125
10.0632
5.7376
10.2671
[2] X. Yi, Z. Duan, R. Li, J. Zhang, T. Li, and Y. Zheng, “Predicting ﬁne-grained air quality based on deep neural networks,” IEEE Transactions on Big
Data, 2020.
[3] K. Hornik, “Approximation capabilities of multilayer feedforward networks,” Neural networks, vol. 4, no. 2, pp. 251–257, 1991.
[4] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of on-line learning and an application to boosting,” Journal of computer and system
sciences, vol. 55, no. 1, pp. 119–139, 1997.
[5] T. Galanti, L. Wolf, and T. Hazan, “A theoretical framework for deep transfer learning,” Information and Inference: A Journal of the IMA, vol. 5, no. 2,
pp. 159–209, 2016.
[6] Z.-H. Zhou, Ensemble methods: foundations and algorithms.
CRC press, 2012.
[7] Z.-H. Zhou, J. Wu, and W. Tang, “Ensembling neural networks: many could be better than all,” Artiﬁcial intelligence, vol. 137, no. 1-2, pp. 239–263,
2002.
[8] X. Chen, S. Wang, B. Fu, M. Long, and J. Wang, “Catastrophic forgetting meets negative transfer: Batch spectral shrinkage for safe transfer learning,”
32th Advances in Neural Information Processing Systems (NeurIPS), 2019.
[9] S. Dˇzeroski and B. ˇZenko, “Is combining classiﬁers with stacking better than selecting the best one?” Machine learning, vol. 54, no. 3, pp. 255–273,
2004.
[10] M. Gashler, C. Giraud-Carrier, and T. Martinez, “Decision tree ensemble: Small heterogeneous is better than large homogeneous,” in Machine Learning
and Applications, 2008. ICMLA’08. Seventh International Conference on.
IEEE, 2008, pp. 900–905.
[11] M. C. Turner, D. Krewski, W. R. Diver, C. A. Pope III, R. T. Burnett, M. Jerrett, J. D. Marshall, and S. M. Gapstur, “Ambient air pollution and cancer
mortality in the cancer prevention study ii,” Environmental health perspectives, vol. 125, no. 8, p. 087013, 2017.
[12] Y. Wei, Y. Zheng, and Q. Yang, “Transfer knowledge between cities,” in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining.
ACM, 2016, pp. 1905–1914.
[13] X. Yi, J. Zhang, Z. Wang, T. Li, and Y. Zheng, “Deep distributed fusion network for air quality prediction,” in Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining.
ACM, 2018, pp. 965–973.
[14] J. Li, H. Zhang, C.-Y. Chao, C.-H. Chien, C.-Y. Wu, C. H. Luo, L.-J. Chen, and P. Biswas, “Integrating low-cost air quality sensor networks with ﬁxed
and satellite monitoring systems to study ground-level pm2. 5,” Atmospheric Environment, vol. 223, 2020.
[15] R. A. Horn and C. R. Johnson, Matrix analysis, 2nd ed.
Cambridge university press, 2012.
[16] W. B. Johnson and J. Lindenstrauss, “Extensions of lipschitz mappings into a hilbert space,” Contemporary mathematics, vol. 26, no. 189-206, p. 1,
1984.
[17] W. Rudin, Principles of mathematical analysis, 3rd ed.
McGraw-hill New York, 1964.
[18] R. Courant and F. John, Introduction to calculus and analysis I.
Springer Science & Business Media, 2012.
[19] M.
A.
Hearst,
“Support
vector
machines,”
IEEE
Intelligent
Systems,
vol.
13,
no.
4,
pp.
18–28,
Jul.
1998.
[Online].
Available:
http://dx.doi.org/10.1109/5254.708428
[20] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32, Oct. 2001. [Online]. Available: https://doi.org/10.1023/A:1010933404324
[21] Environmental Protection Administration. [Online]. Available: https://opendata.epa.gov.tw/Home
15

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
[22] Center Weather Bureau. [Online]. Available: https://opendata.cwb.gov.tw/index
[23] D. W. Wong, L. Yuan, and S. A. Perlin, “Comparison of spatial interpolation methods for the estimation of air quality data,” Journal of Exposure Science
and Environmental Epidemiology, vol. 14, no. 5, p. 404, 2004.
[24] S. Xingjian, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, “Convolutional lstm network: A machine learning approach for precipitation
nowcasting,” in Advances in neural information processing systems, 2015, pp. 802–810.
[25] A. Akansu and R. Haddad, Multiresolution Signal Decomposition: Transforms, Subbands, and Wavelets, 2nd ed.
Academic Press, 2001.
[26] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin, “A neural probabilistic language model,” J. Mach. Learn. Res., vol. 3, pp. 1137–1155, Mar. 2003.
[27] scikit-learn. [Online]. Available: https://github.com/scikit-learn/scikit-learn
[28] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio, “Why does unsupervised pre-training help deep learning?” Journal of
Machine Learning Research, vol. 11, no. Feb, pp. 625–660, 2010.
[29] Y. Yang, D.-C. Zhan, Y. Fan, Y. Jiang, and Z.-H. Zhou, “Deep learning for ﬁxed model reuse,” in Thirty-First AAAI Conference on Artiﬁcial Intelligence,
2017.
[30] X.-Z. Wu, S. Liu, and Z.-H. Zhou, “Heterogeneous model reuse via optimizing multiparty multiclass margin,” in International Conference on Machine
Learning (ICML), 2019, pp. 6840–6849.
[31] D. H. Wolpert, “Stacked generalization,” Neural networks, vol. 5, no. 2, pp. 241–259, 1992.
[32] K. M. Ting and I. H. Witten, “Issues in stacked generalization,” Journal of artiﬁcial intelligence research, vol. 10, pp. 271–289, 1999.
[33] L. Breiman, “Stacked regressions,” Machine learning, vol. 24, no. 1, pp. 49–64, 1996.
[34] S. Hashem, “Optimal linear combinations of neural networks,” Neural networks, vol. 10, no. 4, pp. 599–614, 1997.
[35] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–1359, 2009.
[36] J. Feng and Z.-H. Zhou, “Autoencoder by forest,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.
[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” NAACL2019,
2018.
[38] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” NAACL2018,
2018.
[39] J. Zhao, X. Xie, X. Xu, and S. Sun, “Multi-view learning overview: Recent progress and new challenges,” Information Fusion, vol. 38, pp. 43–54, 2017.
[40] Y. Zheng, F. Liu, and H.-P. Hsieh, “U-air: When urban air quality inference meets big data,” in Proceedings of the 19th ACM SIGKDD international
conference on Knowledge discovery and data mining.
ACM, 2013, pp. 1436–1444.
[41] Y. Zheng, X. Yi, M. Li, R. Li, Z. Shan, E. Chang, and T. Li, “Forecasting ﬁne-grained air quality based on big data,” in Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
ACM, 2015, pp. 2267–2276.
[42] H.-P. Hsieh, S.-D. Lin, and Y. Zheng, “Inferring air quality for station location recommendation based on urban big data,” in Proceedings of the 21th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
ACM, 2015, pp. 437–446.
[43] S. Du, T. Li, Y. Yang, and S.-J. Horng, “Deep air quality forecasting using hybrid deep learning framework,” IEEE Transactions on Knowledge and
Data Engineering, vol. 33, no. 6, 2021.
[44] Z. Qi, T. Wang, G. Song, W. Hu, X. Li, and Z. Zhang, “Deep air learning: Interpolation, prediction, and feature analysis of ﬁne-grained air quality,”
IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 12, 2018.
16

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
APPENDIX
A1. More details of Proofs
Proof. (of Lemma 1)
Recall that in the case of linear activate function, g(x) = L(f1, ...fK) = P
j∈[K]+ θjfj(x). Also recall that EΘ(x; g) =
PN
i=1 (g(x(i)) −y(i))2. To prove the existence of the minimizer, it is sufﬁcient to ﬁnd the critical point for the deferential of
Eq. (1). That is, to calculate the solution, the set of equations:
∇ΘE (x; g) =


∂E
∂θ0...
∂E
∂θK

=


0
...
0

,
where for each s ∈[K]+, and
∂E
∂θs
=2
N
X
i=1

g(x(i)) −y(i)
· fs(x(i))
=2
N
X
i=1

X
j∈[K]+
θjfj(x(i)
j ) −y(i)

· fs(x(i))
=2

X
j∈[K]+
θj⟨⃗fs, ⃗fj⟩−⟨⃗fs, ⃗y⟩

.
Hence, to solve ∇ΘE (x; g) = ⃗0 is equivalent to solve θts in the equation
h
⟨⃗fs, ⃗ft⟩
i
(K+1)×(K+1) × [θt](K+1)×1 =
h
⟨⃗fs, ⃗y⟩
i
(K+1)×1
where the indexes s, t are in [K]+.
Note that linear independence of {⃗fj}j∈[K]+ makes
h
⟨⃗fs, ⃗ft⟩
i
(K+1)×(K+1) a positive-deﬁnite Gram matrix [15], which
means the inversion
h
⟨⃗fs, ⃗ft⟩
i−1
(K+1)×(K+1) exists. Then the minimizer Θ∗is solved:
[θt](K+1)×1 =
h
⟨⃗fs, ⃗ft⟩
i−1
(K+1)×(K+1) ×
h
⟨⃗fs, ⃗y⟩
i
(K+1)×1
(16)
The above shows the existence of the critical points. It is easy to check that the critical point can only be the minimizer of
the squared error EΘ (x; g). Furthermore, we immediately have E(gΘ∗) ≤minj∈[K]+{E(fj)}.
From the above proof, we can compute the minimizer for the case of the linear activation.
Corollary 1. The closed form of the minimizer is:
Θ∗= [θj](K+1)×1 =
h
⟨⃗fi, ⃗fj⟩
i−1
(K+1)×(K+1) ×
h
⟨⃗fj, ⃗y⟩
i
(K+1)×1 .
Based on Lemma 2, we can prove our next lemma
Proof. (of Lemma 3)
Apply Lemma 2 to the given ⃗y and randomly selected ⃗f, then we have
Pr
⃗f∈RN
n
|∠⃗y, ⃗f −π
2 | ≤η
o
≥1 −
1
√
N
.
Also note that vectors ⃗y, ⃗f and ⃗f −⃗y form a triangle with the three inner angles ∠⃗y, ⃗f , ∠⃗f−⃗y, ⃗f and ∠⃗f−⃗y,−⃗y, which means
∠⃗y, ⃗f + ∠⃗f−⃗y, ⃗f + ∠⃗f−⃗y,−⃗y = π. Hence, for large N,
∠⃗y, ⃗f = π
2 ⇒∠⃗f−⃗y, ⃗f ̸= π
2
⇒Pr
n
∠⃗y, ⃗f = π
2
o
≤Pr
n
∠⃗f−⃗y, ⃗f ̸= π
2
o
⇒Pr
n
∠⃗y, ⃗f ≈π
2
o
≤Pr
n
∠⃗f−⃗y, ⃗f ̸≈π
2
o
⇒1 −
1
√
N
≤Pr
n
∠⃗y, ⃗f ≈π
2
o
≤Pr
n
∠⃗f−⃗y, ⃗f ̸≈π
2
o
17

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
This means there exists a small enough η > 0 s.t.
1 −
1
√
N
≤Pr
n
|∠⃗y, ⃗f −π
2 | ≤η
o
≤Pr
n
|∠⃗f−⃗y, ⃗f −π
2 | ≥η
o
⇒
1
√
N
> Pr
n
|∠⃗f−⃗y, ⃗f −π
2 | < η
o
In short, as ∠⃗f−⃗y, ⃗f is likely π/2, ∠⃗y, ⃗f must be less likely a vertical angle. Hence, 1 −
1
√
N ≤Pr{|∠⃗f−⃗y, ⃗f −π
2 | ≤η} ≤
Pr{|∠⃗y, ⃗f −π
2 | > η}. This completes the proof.
Proof. (of Lemma 4)
Observe that as j∗is ﬁxed and known,
Pr
n
∇ΘE|Θ∗= ⃗
ej∗= ⃗0
o
= Pr
n
⟨⃗fj∗−⃗y, ⃗f0⟩= 0 ∧· · · ∧⟨⃗fj∗−⃗y, ⃗fK⟩= 0
o
≤Pr
n
⟨⃗fj∗−⃗y, ⃗fj∗⟩= 0
o
<
1
√
N
The last inequality is from Lemma 3. However, in general j∗is unknown,
Pr

∃Θ∗: E(gΘ∗) = min
j∈[K]+{E(fj)}

= Pr
n
∃j ∈[K]+s.t.∇ΘE|Θ∗= ⃗ej = ⃗0
o
≤Pr
n
∨K
j=0
n
⟨⃗fj −⃗y, ⃗fj⟩= 0
oo
= (K + 1) Pr
n
⟨⃗f −⃗y, ⃗f⟩= 0
o
< K + 1
√
N
Hence,
Pr

∃Θ∗∈RK+1s.t.E(gΘ∗) < min
j∈[K]+{E(fj)}

> 1 −K + 1
√
N
Proof. (of Lemma 7)
For Eq. (8): We ﬁrst give a procedure of obtaining gΘϵ(x(i)), then verify these settings in the procedure ﬁt the conclusion of
the ﬁrst part: ∀i ∈[N], |gΘϵ(x(i)) −gΘ∗
0(x(i))| < ϵ.
Procedure for Eq. (8):
For the given ϵ and σ(·), we ﬁrst ﬁnd the following items based on the conclusions of Case 1 and Lemmas:
gΘ∗
0(·). (By case 1)
z0 ∈R s.t.
d
dzσ(z) ̸= 0. (By A3)
U contains z0. (By Lemma 5)
V contains y0. (By Lemma 5)
τ : V →U s.t. ∀z ∈U, τ(σ(z)) = z. (By Lemma 5)
18

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
(Denote y0 = σ(z0), so τ(y0) = z0.)
Then compute:
Mg = max

1, max
i∈[N]{2 · |gΘ∗
0(x(i))|}

Mσ = max
(
1, sup
z∈U
{2 ·
σ(z) −σ(z0)
z −z0
2
}
)
Mτ = max

1, sup
z∈U
{|τ (2)(σ(z) −σ(z0))|}

Mγ = ⌈log2(MgMσMτϵ−1)⌉+ 1
γ0 = sup
z∈U
{r = |z −z0| : (z0 −r, z0 + r) ⊂U}
γ = min

γ0, 2−Mγ	
M0 = γ−1Mg
M1 = MσMτ
Deﬁne:
L(0),ϵ(x) = M −1
0 gΘ∗
0(x) + z0
L(1),ϵ(y) = M0 · τ (1)(y0) · y + M0 ·

z0 −τ (1)(y0) · y0

Veriﬁcation:
First observe that L(0),ϵ(x) is a linear combination with a bias, i.e., an afﬁne mapping, since gΘ∗
0(x) itself is an afﬁne mapping.
Similarly, L(1),ϵ(y) is an afﬁne mapping of y.
Next, for all i ∈[N], LΘ0,ϵ(x(i)) = M −1
0 gΘ∗
0(x(i))+z0 ∈(−γ+z0, z0+γ) ⊂U since γ ≤γ0
2 and (−γ0
2 +z0, z0+ γ0
2 ) ⊂U.
Hence, by Lemma 5,
τ

σ

LΘ0,ϵ(x(i))

= LΘ0,ϵ(x(i)).
Now let z ∈(−γ + z0, z0 + γ) and y = σ(z), then by Lemma 6 and Eq. (7),
|τ (y) −

τ(y0) + τ (1)(y0)(y −y0)

|
= τ (2)(c(y −y0))
2!
(y −y0)2
< 2 · sup
z∈U
(
|τ (2)(σ(z) −σ(z0))| ·
σ(z) −σ(z0)
z −z0
2)
· (z −z0)2
≤MτMσγ2 = M1γ2
Replace y with σ(z) and simplify the expression in the absolute value symbol, then we have τ (y) = τ (σ(z)) = z. Furthermore,
τ(y0) + τ (1)(y0)(y −y0) = τ (1)(y0) · y +
 τ(y0) −τ (1)(y0) · y0

. Then replace z with LΘ0,ϵ(x(i)), and τ(y0) with z0,
|M −1
0 gΘ∗
0(x) + z0 −
n
z0 + τ (1)(y0)

σ

LΘ0,ϵ(x(i))

−y0
o
|
< M1γ2
This means that
|gΘ∗
0(x) −LΘ1,ϵ

σ

LΘ0,ϵ(x(i))

| < M0M1γ2
⇒|gΘ∗
0(x) −gΘϵ(x)| < M0M1γ2
Recall that γ ≤2−Mγ <
ϵ
MgMσMτ . Hence,
M0M1γ2 = γ−1MgMσMτγ2 = MgMσMτγ < ϵ
achieve the goal of the ﬁrst part of this Lemma.
For Eq. (9): For the second part, we claim the following settings satisfy E(gΘϵ) ≤
2E(gΘ∗
0 )+E(fj∗)
3
< E(fj∗).
Procedure for Eq. (9):
Compute and then set these:
M2 = max
i∈[N]
n
|gΘ∗
0(x(i)) −y(i)|
o
ϵ = E(fj∗) −E(gΘ∗
0)
4N(2M2 + 1)
19

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
Veriﬁcation:
Observe that
E(gΘ∗
0) < E(fj∗) ⇒
max
i∈[N]
n
(fj∗(x(i)) −y(i))2 −(gΘ∗
0(x(i)) −y(i))2o
> 0
E(gΘ∗
0) + E(fj∗) −E(gΘ∗
0)
3
= 2E(gΘ∗
0) + E(fj∗)
3
< E(fj∗)
Besides,
N · (2M2 + 1) · ϵ < E(fj∗) −E(gΘ∗
0)
3
and
|gΘϵ(x) −gΘ∗
0(x)| < ϵ
⇒|(gΘϵ(x) −y) −(gΘ∗
0(x) −y)| < ϵ
⇒0 ≤|gΘϵ(x) −y| < |gΘ∗
0(x) −y| + ϵ
⇒(gΘϵ(x) −y)2 < (|gΘ∗
0(x) −y| + ϵ)2
Hence, based on above observations we have
E(gΘϵ) =
X
i∈[N]
(gΘϵ(x(i)) −y(i))2
<
X
i∈[N]
{|gΘ∗
0(x(i)) −y(i)| + ϵ}2
=
X
i∈[N]
(gΘ∗
0(x(i)) −y(i))2
+
X
i∈[N]
n
2ϵ · |gΘ∗
0(x(i)) −y(i)| + ϵ2o
=E(gΘ∗
0) + ϵ ·
X
i∈[N]

2|gΘ∗
0(x(i)) −y(i)| + ϵ

≤E(gΘ∗
0) + ϵ · N · (2M2 + 1)
<E(gΘ∗
0) + E(fj∗) −E(gΘ∗
0)
3
=E(fj∗) + 2E(gΘ∗
0)
3
<E(fj∗)
which means that E(gΘϵ) < minj∈[K]+{E(fj)}. The proof is complete.
Proof. (of Proposition 1)
Let
D(α0, α1)
=
X
i∈[N]
(f1(x(i)) −y(i))2 −

α0f0(x(i)) + α1f1(x(i)) −y(i)2
.
First observe that D(0, 1) = 0 and hence if ∇D(0, 1) ̸= (0, 0) then it is easy to know that ∃(α∗
0, α∗
1) s.t. D(α∗
0, α∗
1) > 0.
∇D(α0, α1) = −2 ·
"
⟨α0 ⃗f0 + α1 ⃗f1 −⃗y, ⃗f0⟩
⟨α0 ⃗f0 + α1 ⃗f1 −⃗y, ⃗f1⟩
#
Then, by considering (α0, α1) = (0, 1) we have
∇D(0, 1) = −2 ·
"
⟨⃗f1 −⃗y, ⃗f0⟩
⟨⃗f1 −⃗y, ⃗f1⟩
#
Apply Lemma 3,
Pr
n
∇D|Θ∗= ⃗
ej∗= ⃗0
o
≤Pr{∃j ∈[1]+s.t.⟨⃗fj −⃗y, ⃗fj⟩= 0}
<
2
√
N
20

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
That is,
Pr{∃(α0, α1)s.t.D(α0, α1) > 0}
≥Pr{∇D(0, 1) ̸= ⃗0}
> 1 −
2
√
N
Proof. (of Lemma 8)
We ﬁrst prove this lemma of linear activation, and then similar to previous section apply Lemma 7 to address the non-linear
activation. For the linear activation, it can be proved by induction.
Base case: It is done in Proposition 1.
Inductive step: Suppose as J = k −1 the statement is true. That is, gk−1 = LΘ(f1, ..., fk−1) and with probability at least
1 −
K
√
N , there is Θ s.t. E (gK−2) > EΘ (gK−1). As J = k, let f0 and f1 in Proposition 1 be gk−1 and fk respectively. Then
we have α0gk−1 + α1fk as the composite network. Repeat the argument in the previous proposition, then we can conclude
with probability at least 1−k+1
√
N there is (α0, α1) s.t. E (gK−1) > EΘ (α0gk−1 + α1fk). Note that α0gk−1 +α1fk is a possible
form of gK. So the statement holds. The details are as follows:
D(α0, α1)
=
X
i∈[N]
(gk−1(x(i)) −y(i))2 −

α0gk−1(x(i)) + α1fk(x(i)) −y(i)2.
First observe that D(1, 0) = 0 and hence if ∇D(1, 0) ̸= ⃗0 then it is easy to know that ∃(α∗
0, α∗
1) s.t. D(α∗
0, α∗
1) > 0.
∇D(α0, α1) = −2 ·
"
⟨α0⃗gk−1 + α1 ⃗fk −⃗y,⃗gk−1⟩
⟨α0⃗gk−1 + α1 ⃗fk −⃗y, ⃗fk⟩
#
Then,
∇D(1, 0) = −2 ·
⟨⃗gk−1 −⃗y,⃗gk−1⟩
⟨⃗gk−1 −⃗y, ⃗fk⟩)

Apply Lemma 4 and by Induction hypothesis, we have
Pr
n
∇D|Θ∗= ⃗
ej∗= ⃗0
o
≤Pr{⟨⃗gk−1 −⃗y,⃗gk−1⟩= 0} + Pr{⟨⃗fk −⃗y, ⃗fk⟩= 0}
<
k
√
N
+
1
√
N
= k + 1
√
N
Thus,
Pr{∃(α0, α1)s.t.D(α0, α1) > 0}
≥Pr
n
∇D|Θ∗= ⃗
ej∗̸= ⃗0
o
> 1 −k + 1
√
N
This completes the inductive step.
For the non-linear activation, repeat the argument of Lemma 7 to obtain a proper gΘϵ corresponding to the given ϵ and
the linear mapping gΘ∗
0, and a small enough ϵ can yield a proper Θϵ that ﬁts the conclusion of E(gK−1) > EΘϵ(gK). The
probability of existence is inherently obtained as the same as in Lemma 7.
Proof. (of Lemma 9)
Observe that for the given set of pre-tained components {fj}j∈[K] and by the deﬁnition of gK−1, fK is not a component
of gK−1. Hence, if the activation functions used in the construction of gK−1 are all linear, the assumption A1 implies that
⃗gK−1 is linear independent of ⃗fK. Furthermore, if there is at least one non-linear activation function used in the construction
of gK−1, then as N is large enough, Lemma 2 implies that ⃗gK−1 and ⃗fK are not parallel with a very high probability.
This means the assumption that ⃗gK−1 is linear independent of ⃗fK is reasonable. Furthermore, this implies that the events
E1 : ∃Θs.t.EΘ(gK) < min{E(gk−1), E(fk)}, and E2 : E(gK−1) < · · · < minj∈[K]+{E(fj)}, are independent. Hence,
Pr{E1|E2} = Pr{E1}.
21

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
Proof. (of Theorem 2)
For a set of given K pre-trained components, gk := L(k)(σ(L(k−1)(· · · L(1)(σ(L(0)(f1, · · · , fK))) · · · ))) is one of possible
H-hidden layer composite network. Hence obviously,
Pr

∃Θ∗: E(gΘ∗) < min
j∈[K]+{E(fj)}

≥Pr

E(gH) < E(gH−1) < · · · < E(g1) < min
j∈[K]+{E(fj)}

≥Pr

E(g1) < min
j∈[K]+{E(fj)}

× Pr

E(g2) < E(g1) | E(g1) < min
j∈[K]+{E(fj)}

× · · · ×
Pr

E(gH) < E(gH−1) | E(gH−1) < · · · < min
j∈[K]+{E(fj)}

=

1 −K + 1
√
N
H
The last inequality is based on Lemmas 8 and 9:
min
k∈[H] {Pk} ≥1 −K + 1
√
N
,
where
Pk = Pr

∃Θ : E(gk) < E(gk−1) | E(gk−1) < · · · <
min
j∈[K]+{E(fj)}

= Pr {∃Θ : E(gk) < E(gk−1)} by Lemma 9.
This completes the proof.
22

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
A2. More Details of Experiments
In Table XV shows the details of each component:
TABLE XV
ARCHITECTURES AND HYPERPARAMETERS OF COMPONENTS
Comp.
Descriptions: attention (decoder) layer (Att),
time length l ∈{24, 48, 72}, batch normalization (BN),
convolutional layer (Cvl), max-pooling (MaxP),
ﬂatten layer (Fltn), dense layer (Den), dropout (Drop)
f1
Input layer: (30 × 38) × 9
hidden layers: BN, Cvl-1(30 × 38, 32 ﬁlters), MaxP,
Cvl-2(15 × 19, 16 ﬁlters), MaxP, Fltn,
LSTM(150, time l), Att, Fltn, Den, Drop(0.2), Den
Output layer: Den(RELU), 18
total parameters: 917,510
f2
Input layer: (30 × 38) × 4
hidden layers: same with f1
Output layer: Den(RELU), 18
total parameters: 916,918
f3
Input layer: (30 × 38) × 9
hidden layers: Fltn, BN, Den-1(100), Den-2(100)
Output layer: Den(RELU), 18
total parameters: 1,038,054
f4
Input layer: (30 × 38) × 4
hidden layers: same with f3
Output layer: Den(RELU), 18
total parameters: 582,038
f5
Input layer: one-hot (24 + 7 + 12)
hidden layers: LSTM(150, time l), Att, Fltn, Den,
Drop(0.2), Den
Output layer: Den(Scaled-Logistic), 18
total parameters: 953,916
fW6
Input layer: (30 × 38) × 4
hidden layers: Cvl-1(30 × 38, 16 ﬁlters)
Cvl-2(15 × 19, 16 ﬁlters), MaxP, Fltn,
Output layer: Den(RELU), 18
total parameters: 41,380
TABLE XVI
COMPOSITE NETWORKS BY ALGO 1: DBCN, NEXT 48HR.
RMSE
parameter
Model
Training
Testing
trainable/total
note
g1 ←f1
0/
L(g1, f5)
8.1850
11.0995
666/1872092
g2
SL(g1, f5)
8.1675
11.2399
666/1872092
L(g2, f4)
8.1520
11.2632
666/2454796
SL(g2, f4)
8.1902
11.1647
666/2454796
g3
L(g3, f3)
8.1382
11.1163
666/3493516
g4
SL(g3, f3)
8.1085
11.1442
666/3493516
L(g4, f2)
8.0361
11.0991
666/4411100
SL(g4, f2)
8.0678
11.0469
666/4411100
g5
L(g5, fW6)
7.8941
10.9531
42046/4453146
g6
SL(g5, fW6)
7.9009
10.9754
42046/4453146
23

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
TABLE XVII
COMPOSITE NETWORKS BY ALGO 1: DBCN, NEXT 72HR.
RMSE
parameter
Model
Training
Testing
trainable/total
note
g1 ←f5
L(g1, f1)
8.4308
11.3572
666/1872092
SL(g1, f1)
8.2979
11.3323
666/1872092
g2
L(g2, f2)
8.3579
11.3634
666/2789676
g3
SL(g2, f2)
8.3252
11.4001
666/2789676
L(g3, f4)
8.4116
11.4195
666/3372380
g4
SL(g3, f4)
8.6230
11.4530
666/3372380
L(g4, f3)
8.2305
11.4274
666/4411100
g5
SL(g4, f3)
8.1284
11.4482
666/4411100
L(g5, fW6)
8.2448
11.2541
42046/4453146
g6
SL(g5, fW6)
8.2125
11.3232
42046/4453146
TABLE XVIII
COMPOSITE NETWORKS BY ALGO 2: BBCN, NEXT 48HR.
RMSE
parameter
Model
Training
Testing
trainable/total
note
h0,1 ←f1, h0,2 ←f2
L(h0,1, h0,2)
6.1001
11.1004
666/1835094
SL(h0,1, h0,2)
5.5894
11.0907
666/1835094
h1,1
h0,3 ←f3, h0,4 ←f4
L(h0,3, h0,4)
11.8311
11.5098
666/1620758
h1,2
SL(h0,3, h0,4)
11.6587
11.5436
666/1620758
L(h1,1, h1,2)
8.2277
11.1739
666/3456518
SL(h1,1, h1,2)
7.9990
11.0935
666/3456518
h2,1
h2,2 ←h1,3 ←h0,5 ←f5
L(h2,1, h2,2)
8.0273
11.0808
666/4411100
SL(h2,1, h2,2)
7.9949
11.0516
666/4411100
h3,1
g5 ←h3,1
L(g5, fW6)
8.5736
11.0182
42046/4453146
g6
SL(g5, fW6)
7.7346
11.0208
42046/4453146
TABLE XIX
COMPOSITE NETWORKS BY ALGO 2: BBCN, NEXT 72HR.
RMSE
parameter
Model
Training
Testing
trainable/total
note
h0,1 ←f1, h0,2 ←f2
L(h0,1, h0,2)
7.7873
11.4480
666/1835094
SL(h0,1, h0,2)
7.9830
11.4198
666/1835094
h1,1
h0,3 ←f3, h0,4 ←f4
L(h0,3, h0,4)
12.0284
11.7405
666/1620758
h1,2
SL(h0,3, h0,4)
12.0460
11.8005
666/1620758
L(h1,1, h1,2)
8.3884
11.7030
666/3456518
SL(h1,1, h1,2)
8.5149
11.5703
666/3456518
h2,1
h2,2 ←h1,3 ←h0,5 ←f5
L(h2,1, h2,2)
8.4460
11.5100
666/4411100
h3,1
SL(h2,1, h2,2)
8.4093
11.5526
666/4411100
g5 ←h3,1
L(g5, fW6)
9.1848
11.4153
42046/4453146
g6
SL(g5, fW6)
9.0706
11.4474
42046/4453146
24

This work is accepted by IEEE Transactions on Knowledge and Data Engineering (TKDE).
TABLE XX
SUMMARY OF ALL METHODS (MAE)
+24h
+48h
+72h
Method
Training
Testing
Training
Testing
Training
Testing
SVM
8.0701
7.7026
8.5887
8.3525
8.6831
8.6157
Random forests
2.3956
8.3523
2.5007
9.2156
2.5131
9.4219
Ensemble
8.8470
8.5245
9.1113
8.7430
9.3899
8.8767
SL(Ensemble)
8.7689
8.4413
9.0365
8.6116
9.3121
8.8657
DBCNRelu
9.3082
8.7309
9.8383
9.1429
10.4367
9.6356
DBCNSigm
8.6927
8.2972
10.0452
9.6134
9.9240
9.5408
DBCN
3.7188
7.7868
4.3161
8.4258
4.3261
8.8250
BBCNRelu
9.7099
9.2274
10.6558
10.0856
11.6371
10.5833
BBCNSigm
9.1872
8.7346
9.7944
9.5822
9.6479
8.8532
BBCN
3.7676
7.9866
4.4403
8.5564
4.5324
8.8781
Exhaustive-a
2.8646
6.8319
2.8078
7.6136
3.6612
7.7701
(Include fW6)
Ensemble
8.7257
8.2739
8.9901
8.2476
9.2801
8.7263
SL(Ensemble)
8.7470
8.3988
9.0098
8.3065
9.2281
8.4536
DBCNRelu
9.3969
8.5961
10.2851
9.5191
10.4696
9.4412
DBCNSigm
8.7275
8.1512
8.9281
8.2625
9.6004
8.8777
DBCN
3.6608
7.5614
4.2419
8.2766
4.4143
8.5776
BBCNRelu
8.5198
8.0122
8.9259
8.4752
9.2290
8.5274
BBCNSigm
9.1376
8.6430
9.6253
9.0825
9.6115
8.8030
BBCN
3.6698
7.6156
4.6652
8.4528
4.8884
8.6097
Exhaustive-a
3.1250
6.7757
2.8133
7.5986
4.5569
7.6798
Exhaustive-b
3.8088
6.9032
3.1118
7.5156
3.0740
7.6561
Fig. 6. Case study for Tamsui station
Figure 6 illustrates a typical example of the next-24-hour predictions of various models, including g5 and g6 of DBCN,
ConvLSTM, SVM and random forest, of Tamsui for 60 hours starting from 9:00 pm, October 22, 2016. f1 is a ConvLSTM
model that its prediction is central to the average of the ground truth. DBCN (g6) considers the one extra weather feature, i.e.,
the chance of rain in the future, that it produces a lower PM2.5 prediction than DBCN(g5). In this duration, the traditional
machine learning methods SVM and RF usually overestimated, although they apply the same weather and pollutant features.
25

