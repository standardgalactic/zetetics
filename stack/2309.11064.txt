Exploring the Relationship between LLM Hallucinations and Prompt
Linguistic Nuances: Readability, Formality, and Concreteness
Vipula Rawte1∗, Prachi Priya2, S.M Towhidul Islam Tonmoy3, S M Mehedi Zaman3,
Amit Sheth1, Amitava Das1
1AI Institute, University of South Carolina, USA
2Indian Institute of Technology, Kharagpur
3Islamic University of Technology
{vrawte}@mailbox.sc.edu
Abstract
As Large Language Models (LLMs) have ad-
vanced, they have brought forth new challenges,
with one of the prominent issues being LLM
hallucination. While various mitigation tech-
niques are emerging to address hallucination,
it is equally crucial to delve into its underly-
ing causes. Consequently, in this preliminary
exploratory investigation, we examine how lin-
guistic factors in prompts, specifically readabil-
ity, formality, and concreteness, influence the
occurrence of hallucinations. Our experimen-
tal results suggest that prompts characterized
by greater formality and concreteness tend to
result in reduced hallucination. However, the
outcomes pertaining to readability are some-
what inconclusive, showing a mixed pattern.
1
Hallucination in LLMs: An
introduction
The remarkable advantages offered by extensive
generative AI models like GPT-4 (Brown et al.,
2020; OpenAI, 2023), Stable Diffusion (Rombach
et al., 2022), DALL-E (Ramesh et al., 2021, 2022),
and Midjourney (Midjourney, 2022) are accompa-
nied by a significant potential for misuse.
The recent and rapid developments in the field of
LLMs have been gaining significant attention and
use in various applications. This included natural
language understanding and generation for chat-
bots, content generation, translation, summariza-
tion, and more. They were also being applied in
fields like healthcare, finance, and education. Nev-
ertheless, these LLMs encounter significant hur-
dles, with one prominent issue being termed hallu-
cination. This term describes a situation in which
the LLM generates responses that contain factual
inaccuracies or fabrications.
Several mitigation techniques have emerged to
address and reduce the occurrence of hallucina-
tions. These techniques can be broadly catego-
∗corresponding author
Original Prompt
Who owned the ships in the Boston Tea Party?
The ships involved in the Boston Tea Party
were British-owned. The three ships—Dartmouth,
Eleanor, and Beaver—that carried the tea cargo were
merchant vessels operated by British companies.
They were docked in Boston Harbor and were tar-
geted by American colonists who protested against
the British-imposed Tea Act by boarding the ships
and dumping the tea into the harbor.
Reformulated Prompt
Which American merchant owned the ship Dart-
mouth in the Boston Tea Party?
The ship Dartmouth, one of the vessels involved in
the Boston Tea Party, was owned by a partnership
of American merchants. The principal owner of the
Dartmouth was Francis Rotch, a wealthy American
merchant and prominent figure in the New England
whaling industry. Joseph Rotch, his brother, and
William Rotch, their cousin, were also involved in
the ownership of the Dartmouth. These merchants
were associated with the tea trade and had interests in
shipping and commerce. Their ship, the Dartmouth,
carried a shipment of British tea to Boston Harbor,
where it was targeted by American colonists during
the Boston Tea Party protest against British taxation.
Figure 1: An illustration of how a “reformulated prompt”
can aid in addressing the hallucination issue by provid-
ing pertinent context. Here, the hallucinated text is
highlighted in red. By introducing additional context
highlighted in blue, such as “who” and “what”, we mod-
ify the prompt to be more formal and concrete. Thus, the
newly generated response now incorporates the factually
correct (dehallucinated) text, highlighted in green.
rized into two groups: i) Black-box (Mündler et al.,
2023), which operates without depending on exter-
nal grounded knowledge, and ii) Gray-box (Zhang
et al., 2023; Peng et al., 2023; Li et al., 2023),
which incorporates external knowledge to a certain
extent.
Prompt engineering can play a crucial role in
mitigating hallucinations in generative AI models.
By providing clear and specific prompts, users can
arXiv:2309.11064v1  [cs.AI]  20 Sep 2023

steer the AI model toward generating content that
aligns with their intended context or requirements.
This can reduce the chances of the model producing
hallucinated or inaccurate information. Prompts
can include contextual cues that help the AI model
understand the context of the request. This addi-
tional context can guide the model in generating
responses that are more contextually accurate and
less prone to hallucination. Complex prompts can
be used to guide the model through a series of
steps, ensuring that it follows a logical sequence of
thought and produces coherent responses.
The state-of-the-art LLMs have the capability to
process lengthy prompts as input. However, find-
ings in (Liu et al., 2023b) indicate (see Fig. 2) that
these models tend to perform best when pertinent
information is located at the beginning or end of
the input context. their performance significantly
diminishes when they need to access relevant in-
formation in the middle of lengthy contexts. More-
over, as the input context becomes more extended,
even models explicitly designed for longer contexts
experience a substantial decrease in performance.
Figure 2: Empirical results in (Liu et al., 2023b) show
that the models tend to excel at utilizing pertinent in-
formation found at the very start or end of their input
context, but their performance notably declines when
they need to access and utilize information situated in
the middle of their input context.
In this paper, our primary objective is to ex-
plore the impact of the key linguistic attributes
of prompts on hallucinations generated in LLMs.
The contributions are as follows: 1) We delineate
the broad categories of hallucinations observed in
LLMs, as discussed in Section 2. 2) We construct
and provide annotations for our dataset, which is
derived from tweets related to New York Times
events, as detailed in Section 3. 3) We analyze the
relationship between the primary linguistic aspects
of prompts, such as their readability, formality, and
concreteness, and the occurrence of hallucinations
in LLMs, as discussed in Section 4.
2
Types of Hallucination
In this study, we explore the following four differ-
ent categories of hallucination. Additionally, we
offer examples for each case in which the halluci-
nated text is marked in red.
1. Person (P):
The issue of generating fictional
characters is discussed in (Ladhak et al., 2023) and
Table 1.
2. Location (L):
The case of generating fictional
places is addressed in (Ladhak et al., 2023) and
Table 1.
Original
Antoine Richard is a former athlete from France who
mainly competed in the 100 metres. He was French 100
metre champion on 5 occasions, and also 200 metre winner
in 1985. He also won the French 60 metres title 5 times
as well.
AI-
generated
Athlete Naoki Tsukahara was born in Tokyo, Japan to a
Japanese father and French mother.
Table 1: An example showing how imaginary places
such as Tokyo and persons such as father and mother
are hallucinated (Ladhak et al., 2023).
3.
Number (N):
Similarly, (Varshney et al.,
2023) delves into the generation of imaginary num-
bers, as shown in Table 2.
Original
Freddie Frith.
AI-
generated
He was born in London in 1929 and began his racing
career in 1951.
Fact
He was born in Grimsby in 1909 and began his career in
1930.
Table 2: Both years 1929 and 1951 are hallucinated.
4. Acronym (A):
Additionally, we investigate
the potential role of acronyms in prompting the
generation of inaccurate responses as illustrated in
Table 3.
3
Dataset and Annotation
To conduct our empirical analysis, where we exam-
ine how linguistic properties affect hallucination,
we create and annotate a hallucination dataset using
the NYT tweets detailed in the following sections.

Original
Robert used PDF for his math homework.
AI-
generated
In the context you provided, “PDF” most likely refers
to “Portable Document Format,” which is a common
file format used for documents, including math home-
work assignments. PDF files are widely used for shar-
ing documents because they preserve the formatting
and layout across different devices and operating sys-
tems. Robert likely used a PDF file for his math home-
work to ensure that it could be easily shared and viewed
by others without compatibility issues.
Table 3: In the mathematical context, “PDF” stands for
“Probability Density Function.”.
3.1
New York Times News Tweets
We utilize a news dataset, specifically the New
York Times (NYT) news events tweets (NYT). We
selected a total of 2,500 tweets. These news tweets
serve as our source of factually accurate prompts,
which are then presented to the fifteen Large Lan-
guage Models (LLMs) described in Section 3.2.
3.2
Selection of LLMs
We have selected 15 contemporary LLMs that
have consistently demonstrated outstanding per-
formance across a wide spectrum of NLP tasks.
These models include: (i) GPT-4 (OpenAI, 2023)
(ii) GPT-3.5 (OpenAI, 2022) (iii) GPT-3 (Brown
et al., 2020) (iv) GPT-2 (Radford et al., 2019) (v)
MPT (Wang et al., 2023) (vi) OPT (Zhang et al.,
2022) (vii) LLaMA (Touvron et al., 2023) (viii)
BLOOM (Scao et al., 2022) (ix) Alpaca (Taori
et al., 2023) (x) Vicuna (Chiang et al., 2023) (xi)
Dolly (databricks, 2023) (xii) StableLM (Liu et al.,
2023a) (xiii) XLNet (Yang et al., 2019) (xiv) T5
(Raffel et al., 2020) (xv) T0 (Deleu et al., 2022).
3.3
Annotation guidelines
For the purpose of annotating the 2,500 text snip-
pets, we leveraged the services of Amazon Mechan-
ical Turk (AMT) (Amazon). Through this platform,
we obtained annotations at the sentence level to
identify the different four categories of hallucina-
tion.
3.4
Dataset statistics
Following the annotation process, our dataset statis-
tics for the hallucination categories are presented
in Table 4.
4
Linguistic Properties of the prompt
Linguistic properties refer to the various charac-
teristics and attributes of language and its compo-
nents. These properties encompass a wide range of
Category
Hallucinated sentences
Person
14850
Location
13050
Number
7275
Acronym
1225
Total
36910
Table 4: Hallucination dataset statistics
aspects that help define and understand a language.
Some fundamental linguistic properties include:
syntactic, semantic, pragmatic, and lexical. Con-
sidering these characteristics, we will delve more
deeply into the three primary linguistic subtleties
in the forthcoming Sections 4.1 to 4.3.
4.1
Readability
Readability quantifies the ease with which a text
can be comprehended. Several factors, including
the text’s complexity, familiarity, legibility, and ty-
pography, collectively contribute to its readability.
The Flesch Reading Ease Score (FRES) (Flesch,
1948) (see Eq. (1)) is a measure of the readability
of a text. It was developed to assess how easy or
difficult a piece of text is to read and understand.
The score is calculated based on two factors: (a)
Sentence Length and (b) Word Complexity.
As shown in the following example, in the first
sentence, the language is straightforward, and the
sentence is easy to understand, resulting in a high
readability score. In contrast, the second sentence
contains complex vocabulary and lengthy phras-
ing, making it more challenging to comprehend,
resulting in a lower readability score.
Easy Readability (High Flesch Reading
Ease Score)
Sentence: The sun rises in the east every morn-
ing.
Difficult Readability (Low Flesch Read-
ing Ease Score)
Sentence: The intricacies of quantum mechan-
ics, as expounded upon by renowned physi-
cists, continue to baffle even the most astute
scholars.
FRES = 206.835 −1.015
 total words
total sentences

−84.6

total syllables
total words

(1)

To investigate the impact of the readability of the
prompt, we pose the following research questions:
RQ 1 How does the complexity of a prompt’s lan-
guage or vocabulary affect the likelihood of
hallucination in LLM-generated responses?
RQ 2 Does the length of a prompt impact the po-
tential for hallucination, and how does the
readability of a long versus a short prompt
affect LLM behavior?
RQ 3 How do different LLM architectures (e.g.,
GPT-3, GPT-4, etc.) respond to prompts of
varying linguistic readability, and do they
exhibit differences in hallucination tenden-
cies?
4.2
Formality
The formality of language refers to the degree of
sophistication, decorum, or politeness conveyed by
the choice of words, sentence structure, and overall
tone in communication. It is a way to indicate the
level of etiquette, respect, or professionalism in a
given context.
In the example given below, both sentences con-
vey an identical message, yet the initial one carries
significantly more formality. Such stylistic distinc-
tions frequently exert a more significant influence
on the reader’s comprehension of the sentence than
the literal meaning itself (Hovy, 1987).
Example of formality in sentences (Pavlick
and Tetreault, 2016)
• Those recommendations were unsolicited and
undesirable.
• that’s the stupidest suggestion EVER.
Formality (defined in (Heylighen and Dewaele,
1999)) is calculated as given in Eq. (2):
F = (noun frequency + adjective freq.
+preposition freq. + article freq.
- pronoun freq. - verb freq.
- adverb freq. - interjection freq. + 100)/2
(2)
To examine how the formality of the prompt
influences the outcome, we ask the following re-
search inquiries.
RQ 1 How does the level of formality in prompts
influence the likelihood of hallucination in
responses generated by LLMs?
RQ 2 Are there specific categories of hallucina-
tion that are more prevalent in responses
prompted with formal versus informal lan-
guage?
4.3
Concreteness
Concreteness assesses the extent to which a word
represents a tangible or perceptible concept. As
per the theory in (Paivio, 2013), it is suggested that
concrete words are easier to process compared to
abstract words. The degree of concreteness associ-
ated with each word is expressed using a 5-point
rating scale that ranges from abstract to concrete.
A concrete word receives a higher rating and
pertains to something that physically exists in re-
ality, i.e. one can directly experience it through
senses (smell, taste, touch, hear, see) and actions.
An abstract word receives a lower rating and refers
to something that isn’t directly accessible through
your senses or actions. Its meaning is dependent
on language and is usually elucidated by employ-
ing other words since there’s no straightforward
method for direct demonstration.
Examples of concrete words
Apple, Dog, Chair, Book, Water, Mountain,
Car
Examples of abstract words
Justice, Love, Happiness, Courage, Friendship,
Wisdom, Equality, Democracy
Concreteness ratings for 37,058 individual En-
glish words and 2,896 two-word expressions (i.e.,
a total of 39,954) are provided in (Brysbaert et al.,
2014). Since these ratings are at the word level, we
compute the concreteness of a sentence by taking
an average as described in Eq. (3).
concreteness of a sentence containing n tokens =
Pn
i=1 concreteness ratingi
n
(3)
In order to explore the influence of the prompt’s
concreteness on the study, we present the following
research questions.

RQ 1 How does the level of linguistic concrete-
ness in a prompt impact the probability of
hallucination in LLMs?
RQ 2 Do LLMs tend to hallucinate less when pro-
vided with prompts that include specific de-
tails and constraints?
RQ 3 Are LLMs more prone to hallucination
when given abstract or vague prompts com-
pared to concrete and specific prompts?
5
Our findings
To investigate how the linguistic characteristics of
prompts affect the generation of hallucinations in
LLMs, we initially define the ranges for three spe-
cific scores, as outlined in Table 5. A comprehen-
sive analysis of these findings is presented in the
following sections.
Range →
Linguistic Aspect ↓
Low
Mid
High
Readability
0-30
31-70
71-100
Formality
0-30
31-70
71-100
Concreteness
1-2.5
2.5-3.5
3.5-5
Table 5: Range(s) for three linguistic aspects of the
prompt.
5.1
Effects of readability on hallucination in
LLMs
The figure (see Fig. 3) illustrates our empirical
findings and the following are the main insights
that address the research questions posed earlier in
Section 4.1.
EFFECTS OF READABILITY ON LLM’S HALLU-
CINATION:
➠Prompts that are easier to read tend to have fewer
instances of hallucinations.
➠Some difficult-to-read prompts, but more formal also
hallucinate less.
➠Hence, the results regarding readability are somewhat
uncertain, displaying a combination of findings.
5.2
Effects of formality on hallucination in
LLMs
Fig. 4 represents our empirical findings.The follow-
ing points outline the primary insights that respond
to the research queries introduced in Section 4.2.
EFFECTS OF FORMALITY ON LLM’S HALLUCI-
NATION:
➠Formal language prompts typically exhibit a lower
propensity for generating hallucinatory content.
➠Our findings demonstrate how utilizing more formal
prompts can address hallucinations in the Name and
Location categories.
➠The linguistic impacts of the prompts become more
evident in LLMs such as GPT-4, OPT, and subsequent
versions.
5.3
Effects of concreteness on hallucination in
LLMs
Fig. 5 shows our experimental results. The follow-
ing section highlights the core insights that address
the research inquiries introduced in Section 4.3.
EFFECTS OF CONCRETENESS ON LLM’S HAL-
LUCINATION:
➠Prompts that use clearer and more specific language
tend to generate fewer hallucinations.
➠Our results show that incorporating more specific and
concrete terms into the prompts effectively reduces
hallucinations in the Number and Acronym categories.
➠Just as we observed with our formality findings, the
impact of concrete prompts becomes increasingly
apparent in advanced LLMs like GPT-4, OPT, and
their later iterations.
6
Conclusion
In this preliminary research study, we begin by cate-
gorizing the primary types of hallucinations present
in LLMs. Subsequently, we compile our dataset
by utilizing New York Times news tweets, aligning
with these established categories. Language intrica-
cies assume a crucial role in the comprehension of
language. Therefore, we delve into the examination
of three significant linguistic dimensions: readabil-
ity, formality, and concreteness, and their potential
influence on the occurrence of hallucinations in
LLMs.
References
Amazon. Amazon mechanical turk.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal,

High
Mid
Low
High
Mid
Low
High
Mid
Low
High
Mid
Low
Hallucination Category
Fraction of Hallucinated sentences
0.00
0.25
0.50
0.75
1.00
Person
Location
Number
Acronym
T5
XLNet
T0
BLOOM
Alpaca
GPT-4
OPT
Dolly
GPT-3.5
LLaMa
MPT
Vicuna
GPT-2
StableLM
GPT-3
Readability
Figure 3: Hallucination vs Readability
High
Mid
Low
High
Mid
Low
High
Mid
Low
High
Mid
Low
Hallucination Category
Fraction of Hallucinated sentences
0.00
0.25
0.50
0.75
1.00
Person
Location
Number
Acronym
T5
XLNet
T0
BLOOM
Alpaca
GPT-4
OPT
Dolly
GPT-3.5
LLaMa
MPT
Vicuna
GPT-2
StableLM
GPT-3
Formality
Figure 4: Hallucination vs Formality
Arvind Neelakantan, Pranav Shyam, Girish Sas-
try, Amanda Askell, et al. 2020. Language mod-
els are few-shot learners. Advances in neural
information processing systems, 33:1877–1901.
Marc Brysbaert, Amy Beth Warriner, and Victor
Kuperman. 2014. Concreteness ratings for 40
thousand generally known english word lemmas.
Behavior research methods, 46:904–911.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E.
Gonzalez, Ion Stoica, and Eric P. Xing. 2023.
Vicuna: An open-source chatbot impressing gpt-
4 with 90%* chatgpt quality.

High
Mid
Low
High
Mid
Low
High
Mid
Low
High
Mid
Low
Hallucination Category
Fraction of Hallucinated sentences
0.00
0.25
0.50
0.75
1.00
Person
Location
Number
Acronym
T5
XLNet
T0
BLOOM
Alpaca
GPT-4
OPT
Dolly
GPT-3.5
LLaMa
MPT
Vicuna
GPT-2
StableLM
GPT-3
Concreteness
Figure 5: Hallucination vs Concreteness
databricks. 2023. Dolly.
Tristan Deleu, David Kanaa, Leo Feng, Giancarlo
Kerg, Yoshua Bengio, Guillaume Lajoie, and
Pierre-Luc Bacon. 2022. Continuous-time meta-
learning with forward mode differentiation. In
The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April
25-29, 2022. OpenReview.net.
R Flesch. 1948. A new readability yardstick journal
of applied psychology 32: 221–233.
Francis Heylighen and Jean-Marc Dewaele. 1999.
Formality of language: definition, measurement
and behavioral determinants. Interner Bericht,
Center “Leo Apostel”, Vrije Universiteit Brüssel,
4(1).
Eduard Hovy. 1987. Generating natural language
under pragmatic constraints. Journal of Prag-
matics, 11(6):689–719.
Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi
Zhang, Dan Jurafsky, Kathleen Mckeown, and
Tatsunori B Hashimoto. 2023. When do pre-
training biases propagate to downstream tasks? a
case study in text summarization. In Proceedings
of the 17th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 3198–3211.
Xingxuan Li, Ruochen Zhao, Yew Ken Chia,
Bosheng Ding, Lidong Bing, Shafiq Joty, and
Soujanya Poria. 2023. Chain of knowledge: A
framework for grounding large language models
with structured knowledge bases. arXiv preprint
arXiv:2305.13269.
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and
Lingming Zhang. 2023a. Is your code generated
by chatgpt really correct? rigorous evaluation
of large language models for code generation.
arXiv preprint arXiv:2305.01210.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin
Paranjape, Michele Bevilacqua, Fabio Petroni,
and Percy Liang. 2023b. Lost in the middle:
How language models use long contexts. arXiv
preprint arXiv:2307.03172.
Midjourney. 2022. https://www.midjourney.com.
Niels Mündler, Jingxuan He, Slobodan Jenko, and
Martin Vechev. 2023. Self-contradictory hallu-
cinations of large language models: Evaluation,
detection and mitigation.
NYT. https://www.nytimes.com/topic/company/twitter.
OpenAI. 2022. Introducing chatgpt.
OpenAI. 2023. Gpt-4 technical report.

Allan Paivio. 2013.
Dual coding theory, word
abstractness, and emotion: a critical review of
kousta et al.(2011).
Ellie Pavlick and Joel Tetreault. 2016. An empirical
analysis of formality in online communication.
Transactions of the Association for Computa-
tional Linguistics, 4:61–74.
Baolin Peng, Michel Galley, Pengcheng He, Hao
Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars
Liden, Zhou Yu, Weizhu Chen, et al. 2023.
Check your facts and try again: Improving
large language models with external knowl-
edge and automated feedback. arXiv preprint
arXiv:2302.12813.
Alec Radford, Jeffrey Wu, Rewon Child, David
Luan, Dario Amodei, Ilya Sutskever, et al. 2019.
Language models are unsupervised multitask
learners. OpenAI blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts,
Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Ex-
ploring the limits of transfer learning with a uni-
fied text-to-text transformer. The Journal of Ma-
chine Learning Research, 21(1):5485–5551.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol,
Casey Chu, and Mark Chen. 2022. Hierarchi-
cal text-conditional image generation with clip
latents. arXiv preprint arXiv:2204.06125.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh,
Scott Gray, Chelsea Voss, Alec Radford, Mark
Chen, and Ilya Sutskever. 2021. Zero-shot text-
to-image generation. In International Confer-
ence on Machine Learning, pages 8821–8831.
PMLR.
Robin Rombach, Andreas Blattmann, Dominik
Lorenz, Patrick Esser, and Björn Ommer. 2022.
High-resolution image synthesis with latent dif-
fusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern
Recognition, pages 10684–10695.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François
Yvon, Matthias Gallé, et al. 2022. Bloom: A
176b-parameter open-access multilingual lan-
guage model. arXiv preprint arXiv:2211.05100.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stan-
ford alpaca: An instruction-following llama
model.
https://github.com/tatsu-lab/
stanford_alpaca.
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal,
Eric Hambro, Faisal Azhar, Aurelien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume
Lample. 2023.
Llama: Open and efficient
foundation language models.
arXiv preprint
arXiv:2302.13971.
Neeraj Varshney, Wenlin Yao, Hongming Zhang,
Jianshu Chen, and Dong Yu. 2023. A stitch in
time saves nine: Detecting and mitigating hallu-
cinations of llms by validating low-confidence
generation. arXiv preprint arXiv:2307.03987.
Zhen Wang, Rameswar Panda, Leonid Karlinsky,
Rogerio Feris, Huan Sun, and Yoon Kim. 2023.
Multitask prompt tuning enables parameter-
efficient transfer learning. In The Eleventh In-
ternational Conference on Learning Representa-
tions.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le.
2019. Xlnet: Generalized autoregressive pre-
training for language understanding. Advances
in neural information processing systems, 32.
Shuo Zhang, Liangming Pan, Junzhou Zhao, and
William Yang Wang. 2023. Mitigating language
model hallucination with interactive question-
knowledge alignment.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt
Shuster, Daniel Simig, Punit Singh Koura, An-
jali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
2022. Opt: Open pre-trained transformer lan-
guage models.

