Toward a theory of evolution as multilevel learning
Vitaly Vanchurina,b,1, Yuri I. Wolfa
, Mikhail I. Katsnelsonc
, and Eugene V. Koonina,1
aNational Center for Biotechnology Information, National Library of Medicine, Bethesda, MD 20894; bDuluth Institute for Advanced Study, Duluth, MN 55804;
and cInstitute for Molecules and Materials, Radboud University, Nijmegen 6525AJ, The Netherlands
Contributed by Eugene V. Koonin; received November 2, 2021; accepted January 3, 2022; reviewed by Steven Frank and E€ors Szathmary
We apply the theory of learning to physically renormalizable sys-
tems in an attempt to outline a theory of biological evolution,
including the origin of life, as multilevel learning. We formulate
seven fundamental principles of evolution that appear to be nec-
essary and sufﬁcient to render a universe observable and show
that they entail the major features of biological evolution, includ-
ing replication and natural selection. It is shown that these corner-
stone phenomena of biology emerge from the fundamental
features of learning dynamics such as the existence of a loss func-
tion, which is minimized during learning. We then sketch the the-
ory of evolution using the mathematical framework of neural
networks, which provides for detailed analysis of evolutionary
phenomena. To demonstrate the potential of the proposed theo-
retical framework, we derive a generalized version of the Central
Dogma of molecular biology by analyzing the ﬂow of information
during learning (back propagation) and predicting (forward propa-
gation) the environment by evolving organisms. The more com-
plex evolutionary phenomena, such as major transitions in
evolution (in particular, the origin of life), have to be analyzed in
the thermodynamic limit, which is described in detail in the paper
by Vanchurin et al. [V. Vanchurin, Y. I. Wolf, E. V. Koonin, M. I.
Katsnelson,
Proc.
Natl.
Acad.
Sci.
U.S.A.
119,
10.1073/
pnas.2120042119 (2022)].
theory of learning j loss function j natural selection j major evolutionary
transitions j origin of life
W
hat is life? If this question is asked in the scientiﬁc rather
than in the philosophical context, a satisfactory answer
should assume the form of a theoretical model of the origin and
evolution of complex systems that are identiﬁed with life (1).
NASA has operationally deﬁned life as follows: “Life is a self-
sustaining chemical system capable of Darwinian evolution” (2,
3). Apart from the insistence on chemistry, long-term evolution
that involves (random) mutation, diversiﬁcation, and adaptation
is, indeed, an intrinsic, essential feature of life that is not apparent
in any other natural phenomena. The problem with this deﬁni-
tion, however, is that natural (Darwinian) selection itself appears
to be a complex rather than an elementary phenomenon (4). In
all evolving organisms we are aware of, for natural selection to
kick off and to sustain long-term evolution, an essential condition
is replication of a complex digital information carrier (a DNA or
RNA molecule). The replication ﬁdelity must be sufﬁciently high
to provide for the differential replication of emerging mutants
and survival of the ﬁttest ones (this replication ﬁdelity level is
often referred to as Eigen threshold) (5). In modern organisms,
accurate replication is ensured by elaborate molecular machiner-
ies that include not only replication and repair enzymes but also,
the entire metabolic network of the cell, which supplies energy
and building blocks for replication. Thus, the origin of life is a typ-
ical chicken-and-egg problem (or catch-22); accurate replication is
essential for evolution, but the mechanisms ensuring replication
ﬁdelity are themselves products of complex evolutionary processes
(6, 7).
Because genome replication that underlies natural selection
is itself a product of evolution, origin of life has to be explained
outside of the traditional framework of evolutionary biology.
Modern evolutionary theory, steeped in population genetics,
gives a detailed and arguably, largely satisfactory account of
microevolutionary processes: that is, evolution of allele fre-
quencies in a population of organisms under selection and ran-
dom genetic drift (8, 9). However, this theory has little to say
about the actual history of life, especially the emergence of new
levels of biological complexity, and nothing at all about the ori-
gin of life.
The crucial feature of biological complexity is its hierarchical
organization. Indeed, multilevel hierarchies permeate biology:
from small molecules to macromolecules; from macromolecules
to functional complexes, subcellular compartments, and cells;
from unicellular organisms to communities, consortia, and mul-
ticellularity; from simple multicellular organisms to highly com-
plex forms with differentiated tissues; and from organisms to
communities and eventually, to eusociality and to complex bio-
cenoses involved in biogeochemical processes on the planetary
scale. All these distinct levels jointly constitute the hierarchical
organization of the biosphere. Understanding the origin and
evolution of this hierarchical complexity, arguably, is one of the
principal goals of biology.
In large part, evolution of the multilevel organization of bio-
logical systems appears to be driven by solving optimization
problems, which entails conﬂicts or trade-offs between optimi-
zation criteria at different levels or scales, leading to frustrated
states, in the language of physics (10–12). Two notable cases in
point are parasite–host arms races that permeate biological
evolution and makes major contributions to the diversity and
complexity of life-forms (13–16) and multicellular organization
of complex organisms, where the tendency of individual cells to
Signiﬁcance
Modern evolutionary theory gives a detailed quantitative
description of microevolutionary processes that occur within
evolving populations of organisms, but evolutionary transi-
tions and emergence of multiple levels of complexity remain
poorly understood. Here, we establish the correspondence
among the key features of evolution, learning dynamics,
and renormalizability of physical theories to outline a theory
of evolution that strives to incorporate all evolutionary pro-
cesses within a uniﬁed mathematical framework of the the-
ory of learning. According to this theory, for example,
replication of genetic material and natural selection readily
emerge from the learning dynamics, and in sufﬁciently com-
plex systems, the same learning phenomena occur on multi-
ple levels or on different scales, similar to the case of
renormalizable physical theories.
Author
contributions:
V.V.,
Y.I.W.,
M.I.K.,
and
E.V.K.
designed
research;
V.V.
performed research; and V.V., Y.I.W., and E.V.K. wrote the paper.
Reviewers: S.F., University of California, Irvine; and E.S., Parmenides Foundation.
The authors declare no competing interest.
This open access article is distributed under Creative Commons Attribution License 4.0
(CC BY).
1To whom correspondence may be addressed. Email: vitaly.vanchurin@gmail.com or
koonin@ncbi.nlm.nih.gov.
This article contains supporting information online at http://www.pnas.org/lookup/
suppl/doi:10.1073/pnas.2120037119/-/DCSupplemental.
Published February 4, 2022.
PNAS 2022 Vol. 119 No. 6 e2120037119
https://doi.org/10.1073/pnas.2120037119
j
1 of 12
EVOLUTION
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

reproduce at the highest possible rate is countered by the con-
trol of cell division imposed at the organismal level (17, 18).
Two tightly linked but distinct fundamental concepts that lie
effectively outside the canonical narrative of evolutionary biol-
ogy address evolution of biological complexity: major transi-
tions in evolution (MTEs) (19–21) and multilevel selection
(MLS) (22–27). Each MTE involves the emergence of a new
level of organization, often described as an evolutionary transi-
tion in individuality. A clear-cut example is the evolution of
multicellularity, whereby a new level of selection emerges,
namely selection among ensembles of cells rather than among
individual cells. Multicellular life-forms (even counting only
complex organisms with multiple cell types) evolved on many
independent occasions during the evolution of life (28, 29),
implying that emergence of new levels of complexity is a major
evolutionary trend rather than a rare, chance event.
The MLS remains a controversial concept, presumably
because of the link to the long-debated subject of group selec-
tion (27, 30). However, as a deﬁning component of MTE, MLS
appears to be indispensable. A proposed general mechanism
behind the MTE, formulated by analogy with the physical the-
ory of the origin of patterns (for example, in glass-like systems),
involves competing interactions at different levels and the frus-
trated states, such interactions cause (12). In the physical the-
ory of spin glasses, frustrations result in nonergodicity and
enable formation and persistence of long-term memory: that is,
history (31, 32). By contrast, ergodic systems have no true his-
tory because they reach all possible states during their evolution
(at least in the large time limit), and thus, the only content of
quasihistory of such systems is the transition from less probable
to more probable states for purely combinatorial reasons: that
is, entropy increase (33). As emphasized in Schroedinger’s sem-
inal book (34), even if only in general terms, life is based on
“negentropic” processes, and frustrations at different levels are
necessary for these processes to take off and persist (12).
The origin of cells, which can and probably should be equated
with the origin of life, was the ﬁrst and most momentous transi-
tion at the onset of biological evolution, and as such, it is outside
the purview of evolutionary biology sensu stricto. Arguably, the
theoretical investigation of the origin of life can be feasible only
within the framework of an envelope theory that would incorpo-
rate biological evolution as a special case. It is natural to envisage
such a theory as encompassing all nonergodic processes occurring
in the universe, of which life is a special case, emerging under
conditions that remain to be investigated and deﬁned.
Here, in pursuit of a maximally general theory of evolution,
we adopt the formalism of the theory of machine learning (35).
Importantly, learning here is perceived in the maximally gen-
eral sense as an objective process that occurs in all evolving sys-
tems, including but not limited to biological ones (36). As such,
the analogy between learning and selection appears obvious.
Both types of processes involve trial and error and acceptance
or rejection of the results based on some formal criteria; in
other words, both are optimization processes (22, 37, 38). Here,
we assess how far this analogy extends by establishing the corre-
spondence between key features of biological evolution and
concepts as well as the mathematical formalism of learning the-
ory. We make the case that loss function, which is central to the
learning theory, can be usefully and generally employed as the
equivalent of the ﬁtness function in the context of evolution.
Our original motivation was to explain major features of biolog-
ical evolution from more general principles of physics. How-
ever, after formulating such principles and embedding them
within the mathematical framework of learning, we ﬁnd that
the theory can potentially apply to the entire history of the
evolving universe (36), including physical processes that have
been taking place since the big bang and chemical processes
that directly antedated and set the stage for the origin of life.
The central propositions of the evolution theory outlined here
include both key physical principles (namely, hierarchy of scale,
frequency gaps, and renormalizability) (39, 40) and major fea-
tures of life (such as MLS, persistence of genetic parasites, and
programmed cell death).
We show that learning in a complex environment leads to sepa-
ration of scales, with trainable variables splitting into at least two
classes: faster- and slower-changing ones. Such separation of
scales underlies all processes that involve the formation of com-
plex structure in the universe from the scale of an atom to that of
clusters of galaxies. We argue that, for the emergence of life, at
least three temporal scales, which correspond to environmental,
phenotypic, and genotypic variables, are essential. In evolving
learning systems, the slowest-changing variables are digitized and
acquire the replication capacity, resulting in differential reproduc-
tion depending on the loss (ﬁtness) function value, which is neces-
sary and sufﬁcient for the onset of evolution by natural selection.
Subsequent evolution of life involves emergence of many addi-
tional scales, which correspond to MTE. Hereafter, we use the
term “evolution” to describe temporal changes of living and life-
like and prebiotic systems (organisms), whereas the more general
term “dynamics” refers to temporal processes in other physical
systems.
At least since the publication of Schroedinger’s book, the
possibility has been discussed that, although life certainly obeys
the laws of physics, a different class of laws unique to biology
could exist. Often, this putative physics of life is associated with
emergence (41–43), but the nature of the involved emergent
phenomena, to our knowledge, has not been clariﬁed until very
recently (36). Here, we outline a general approach to modeling
and studying evolution as multilevel learning, supporting the
view that a distinct type of physical theory, namely the theory
of learning (35, 36), is necessary to investigate the evolution of
complex objects in the universe, of which evolution of life is a
speciﬁc, even if highly remarkable form.
1. Fundamental Principles of Evolution
In this section, we attempt to formulate the minimal universal
principles that deﬁne an observable universe, in which evolu-
tion is possible and perhaps, inevitable. Our analysis started
from the major features of biological evolution discussed in the
next section and proceeded toward the general principles.
However, we begin the discussion with the latter for the sake of
transparency and generality.
What are the requirements for a universe to be observable?
The possibility to make meaningful observations implies a
degree of order and complexity in the observed universe emerg-
ing from evolutionary processes, and such evolvability itself
seems to be predicated on several fundamental principles. It
has to be emphasized that “observation” and “learning” here
by no means imply “mind” or “consciousness” but a far more
basic requirement. To learn and survive in an environment, a
system (or observer) must predict, with some minimal but sufﬁ-
cient degree of accuracy, the response of that environment to
various actions and to be able to choose such actions that are
compatible with the observer’s continued existence in that envi-
ronment. In this sense, any life-form is an observer, and so are
even inanimate systems endowed with the ability of feedback
reaction. In this most general sense, observation is a prerequi-
site for evolution. We ﬁrst formulate the basic principles under-
lying observability and evolvability and then, give the pertinent
comments and explanations.
P1. Loss function. In any evolving system, there exists a loss
function of time-dependent variables that is minimized during
evolution.
2 of 12
j
PNAS
Vanchurin et al.
https://doi.org/10.1073/pnas.2120037119
Toward a theory of evolution as multilevel learning
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

P2. Hierarchy of scales. Evolving systems encompass multiple
dynamical variables that change on different temporal scales
(with different characteristic frequencies).
P3. Frequency gaps. Dynamical variables are split among dis-
tinct levels of organization separated by sufﬁciently wide fre-
quency gaps.
P4. Renormalizability. Across the entire range of organization
of evolving systems, a statistical description of faster-changing
(higher-frequency) variables is feasible through the slower-
changing (lower-frequency) variables.
P5. Extension. Evolving systems have the capacity to recruit
additional variables that can be utilized to sustain the system
and the ability to exclude variables that could destabilize the
system.
P6. Replication. In evolving systems, replication and elimina-
tion of the corresponding information-processing units (IPUs)
can take place on every level of organization.
P7. Information ﬂow. In evolving systems, slower-changing lev-
els absorb information from faster-changing levels during
learning and pass information down to the faster levels for pre-
diction of the state of the environment and the system itself.
The ﬁrst principle (P1) is of special importance as the starting
point for a formal description of evolution as a learning process.
The very existence of a loss function implies that the dynamical
system of the universe or simpler, the universe itself is a learning
(evolving) system (36). Effectively, here we assume that stability
or survival of any subsystem of the universe is equivalent to solv-
ing an optimization or learning problem in the mathematical
sense and that there is always something to learn. Crucially, for
solving complex optimization problems dependent on many varia-
bles, the best and in fact, the only efﬁcient method is selection
implemented in various stochastic algorithms (Markov Chain
Monte Carlo, stochastic gradient descent, genetic algorithms, and
more). All evolution can be perceived as an implementation of a
stochastic learning algorithm as well. Put another way, learning is
optimization by trial and error, and so is evolution.
The remaining principles P2 to P7 provide sufﬁcient condi-
tions for observers of our type (that is, complex life-forms) to
evolve within a learning system. In particular, P2, P3, and P4
comprise the necessary conditions for observability of a uni-
verse by any observer, whereas P5, P6, and P7 represent the
deﬁning conditions for the origin of life of our type (hereafter,
we omit the qualiﬁcation for brevity). More precisely, P2 and
P3 provide for the possibility of at least a simple form of
learning of the environment (fast-changing variables) by an
observer (slow-changing variables) and hence, the emergence
of complex organization of the slow-changing variables. P4
corresponds to the physical concept of renormalizability, or
renormalization group (39, 40), whereby the same macro-
scopic equations, albeit with different parameters, govern pro-
cesses at different levels or scales, thus limiting the number of
relevant variables, constraining the complexity, and allowing
for a coarse-grained description. This principle ensures a
renormalizable universe capable of evolution and amenable to
observation. Together, P2 to P4 deﬁne a universe, in which
partial or approximate knowledge of the environment (in
other words, coarse graining) is both attainable and useful for
the survival of evolving systems (observers). In a universe
where P4 does not apply (that is, one with nonrenormalizable
physical laws), what happens at the macroscopic level will crit-
ically depend on the details of the processes at the microlevel.
In a universe where P2 and P3 do not apply, the separation of
the micro- and macrolevels itself would not be apparent. In
such a universe, it would be impossible to survive without ﬁrst
discovering fundamental physical laws, whereas living organ-
isms on our planet have evolved for billions of years before
starting to study quantum physics.
Principles P5, P6, and P7 endow evolving systems with the
access to more advanced algorithms for learning and predicting
the environment, paving the way for the evolution of complex sys-
tems, including eventually, life. These principles jointly underlie
the emergence of the crucial phenomenon of selection (44, 45).
In its simplest form, selection is for stability and persistence of
evolving, learning systems (46). Learning and survival are tightly
linked because survival is predicated on the system’s ability to
extract information from the environment, and this ability
depends on the stability of the system on timescales required for
learning. Roughly, a system cannot survive in a world where the
properties of the environment change faster than the evolving sys-
tem can learn them. According to P5, evolving systems consume
resources (such as food), which themselves could be produced by
other evolving systems, to be utilized as building blocks and
energy sources, which are required for learning. This principle
embodies
Schroedinger’s
vision
that
“organisms
feed
on
negentropy” (34). Under P6, replication of the carriers of slowly
changing variables becomes the basis of long-term persistence and
memory in evolving systems. This principle can be viewed as a
learning algorithm built on P3, whereby the timescales character-
istic of an individual organism and of consecutive generations are
separated. This principle excludes from consideration certain
imaginary forms of life: for example, Stanislav Lem’s famous
Solaris (47). Finally, P7 describes how information ﬂows between
different levels in the multilevel learning, giving rise to a general-
ized Central Dogma of molecular biology, which is discussed in
Generalized Central Dogma of Molecular Biology.
2. Fundamental Evolutionary Phenomena
In this section, we link the fundamental principles of evolution P1
to P7 formulated above to the basic phenomenological features of
life (E1 to E10) and seek equivalencies in the theory of learning.
The list below is organized by ﬁrst formulating a biological fea-
ture, and then, it is organized by 1) tracing the connections to the
fundamental principles and 2) adding more general comments.
E1. IPUs. Discrete IPUs (that is, self- vs. nonself-differentiation
and discrimination) exist at all levels of organization. All bio-
logical systems at all levels of organization, such as genes, cells,
organisms, populations, and so on up to the level of the entire
biosphere, possess some degree of self-coherence that separates
them, ﬁrst and foremost, from the environment at large and
from other similar-level IPUs.
1) The existence of IPUs is predicated on the fundamental
principles P1 to P4. The wide range of temporal scales (P2)
in dynamical systems and gaps between the scales (P3) natu-
rally enable the separation of slower- and faster-changing
components. In particular, renormalizability (P4) applies to
the hierarchy of IPUs. The statistical predictability of the
higher frequencies allows the IPUs to decrease the loss func-
tion of the lower frequencies, despite the much slower reac-
tion times.
2) Separation of (relatively) slow-changing prebiological IPUs
from the (typically) fast-changing environment kicked off the
most primitive form of prebiological selection: selection for sta-
bility and persistence (survivor bias). More stable, slower-
changing IPUs win in the competition and accumulate over
time, increasing the separation along the temporal axis as the
boundary between the IPUs and the environment grows
sharper. Additional key phenomena, such as utilization of avail-
able environmental resources (P5) and the stimulus–response
mode of information exchange (P7), stem from the ﬂow of mat-
ter and information across this boundary and the ensuing sepa-
ration of internal and external physicochemical processes.
Increasing self- vs. nonself-differentiation, combined with repli-
cation of the carriers of slow-changing variables (P6), sets the
EVOLUTION
Vanchurin et al.
Toward a theory of evolution as multilevel learning
PNAS
j
3 of 12
https://doi.org/10.1073/pnas.2120037119
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

stage for competition between evolving entities and for the
onset of the ultimate evolutionary phenomenon, natural selec-
tion (E6).
E2. Frustration. All complex, dynamical systems face multidi-
mensional and multiscale optimization problems, which generi-
cally lead to frustration resulting from conﬂicting objectives at
different scales. This is a key, intrinsic feature of all such sys-
tems and a major force driving the advent of increasing multile-
vel complexity (12). Frustration is an extremely general physical
phenomenon that is by no account limited to biology but
rather, occurs already in much simpler physical systems, such as
spin and structural glasses, the behavior of which is determined
by competing interactions so that a degree of complexity is
attained (31, 32).
1) The multiscale organization of the universe (P2) provides
the physical foundation for the ubiquity of frustrated states
that typically arises whenever there is a conﬂict (trade-off)
between short- and long-range optimization problems. Frus-
trated interactions yield multiwell potential landscapes, in
which no single state is substantially ﬁtter than numerous
other local optima. Multiparameter and multiscale optimiza-
tion of the loss function on such a landscape involves noner-
godic (history-dependent) dynamics, which is characteristic
of complex systems.
2) IPUs face conﬂicting interactions starting from the most
primitive prebiological state (12). Indeed, the separation of
any system from the environment immediately results in the
conﬂict of permeability; a stronger separation enhances the
self- vs. nonself-differentiation and thus, increases the stabil-
ity of the system, but it compromises information and matter
exchange with the environment, limiting the potential for
growth. In biology, virtually all aspects of the organismal
architecture and operation are subject to such frustrations or
trade-offs: the conﬂict between the ﬁdelity and speed of
information transmission at all levels, between specialization
and generalism, between the individual- and population-
level beneﬁts, and more. The ubiquity of frustrations and the
fundamental impossibility of their resolution in a universally
optimal manner are perpetual drivers of evolution and give
rise to evolutionary transitions, attaining otherwise unreach-
able levels of complexity.
There are two distinct types of frustrations, spatial and tempo-
ral. Spatial frustration is similar to the frustration that is com-
monly analyzed in condensed matter systems, such as spin
glasses (31, 32). In this case, the spatially local and nonlocal
interacting terms have opposite signs so that the equilibrium
state is determined by the balance between the terms. In neural
networks, a neuron (like a single spin) might have a local objec-
tive (such as binary classiﬁcation of incoming signals) but is
also a part of a neural network (like a spin network), which has
its own global objective (such as predicting its boundary
conditions). For a particular neuron, optimization of the local
objective can conﬂict with the global objective, causing spatial
frustration. Temporal frustration emerges because in the con-
text of multilevel learning, the same neuron becomes a part of
higher-level IPUs that operate at different temporal scales (fre-
quencies). Then, the optimal state of the neuron with respect
to an IPU operating at a given timescale can differ from the
optimal state of the same neuron with respect to another IPU
operating at a different timescale (36). Similarly to the spatial
frustrations,
temporal
frustrations
cannot
be
completely
resolved, but an optimal balance between different spatial and
temporal scales is achievable and represents a local equilibrium
of the learning system.
E3. Multilevel Hierarchy. The hierarchy of multiple levels of orga-
nization is an intrinsic, essential feature of evolving biological
systems in terms of both the structure of these systems (genes,
genomes, cells, organisms, kin groups, populations, species,
communities, and more) and the substrate the evolutionary
forces act upon.
1) Renormalizability of the universe (P4) implies that there is
no inherently preferred level of organization, for which
everything above and below would behave as a homogenous
ensemble. Even if some levels of organization come into exis-
tence before others (for example, organisms before genes or
unicellular organisms before multicellular ones), the other
levels will necessarily emerge and consolidate subsequently.
2) The hierarchy of the structural organization of biological sys-
tems was apparent to scholars from the earliest days of science.
However, MLS was and remains a controversial subject in evo-
lutionary biology (23, 26, 27). Intuitively and as implied by the
Price equation (48), MLS should emerge in all evolving systems
as long as the higher-level agency of selection possesses a sufﬁ-
cient degree of self- vs. nonself-differentiation. In particular, if
organisms of a given species form populations that are
sufﬁciently distinct genetically and interact competitively,
population-level selection will ensue. Evolution of biolog-
ical systems is driven by conﬂicting interactions (E2) that
tend to lead to ever-increasing complexity (12). This trend
further feeds the propensity of these systems to form new
levels of organization and is associated with evolutionary
transitions that involve the advent of new units of selec-
tion at multiple levels of complexity. Thus, E3 can be con-
sidered a major consequence of E2.
E4. Near Optimality. Stochastic optimization or the use of sto-
chastic optimization algorithms is the only feasible approach to
complex optimization, but it guarantees neither ﬁnding the
globally optimal solution nor retention of the optimal conﬁgu-
ration when and if it is found. Rather, stochastic optimization
tends to rapidly ﬁnd local optima and keeps the system in their
vicinity, sustaining the value of the loss function at a near-
optimal level.
1) According to P1, the dynamics of a learning (that is, self-
optimizing) system is deﬁned by a loss function (35, 36).
When there is a steep gradient in the loss function, a system
undergoing stochastic optimization rapidly descends in the
right direction. However, because of frustrations that inevita-
bly arise from interactions in a complex system, actual local
peaks on the landscape are rarely reached, and the global
peak is effectively unreachable. Learning systems tend to get
stalled near local saddle points where changes along most of
the dimensions either lead “up” or are “ﬂat” in terms of the
loss function, with only a small minority of the available
moves decreasing the loss function (49).
2) The extant biological systems (cells, multicellular organisms,
and higher-level entities, such as populations and communi-
ties) are products of about 4 billion y of the evolution of life,
and as such, they are highly, albeit not completely, opti-
mized. As a consequence, the typical distribution of the
effects of heritable changes in biological evolution comprises
numerous deleterious changes, comparatively rare beneﬁcial
changes and common neutral changes, and those with ﬁtness
effects below the noise level (50). The preponderance of
neutral and slightly deleterious changes provides for evolu-
tion by genetic drift whereby a population moves on the
same level or even slightly downward on the ﬁtness land-
scape, potentially reaching another region of the landscape
where beneﬁcial mutations are available (51, 52).
4 of 12
j
PNAS
Vanchurin et al.
https://doi.org/10.1073/pnas.2120037119
Toward a theory of evolution as multilevel learning
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

E5. Diversity of Near-Optimal Solutions. Solutions on the loss
function landscapes that arise in complex optimization prob-
lems span numerous local peaks of comparable heights.
1) The existence of multiple peaks of comparable heights in the
loss function landscapes is a fundamental physical property of
frustrated systems (E2), whereas the pervasiveness of frustra-
tion itself is a consequence of the multiscale and multilevel
organization of the universe (P2). Frustrated dynamical sys-
tems are nonergodic, which from the biological perspective,
means that, once separated, evolutionary trajectories diverge
rather than converge. Because most of these trajectories trav-
erse parts of the genotype space with comparable ﬁtness val-
ues, competition rarely results in complete dominance of one
lineage over the others but rather, generates rich diversity.
2) In terms of evolutionary biology, ﬁtness landscapes are rug-
ged, with multiple adaptive peaks of comparable ﬁtness (53,
54), and a salient trend during evolution is the spread of life-
forms across multiple peaks as opposed to concentrating on
one or few. Evolution pushes evolving organisms to explore
and occupy all available niches and try all possible strategies.
In the context of machine learning, identical neural networks
can start from the same initial state but for example, under
the stochastic gradient descent algorithm, would generically
evolve toward different local minima. Thus, the diversity of
solutions is a generic property of learning systems. More
technically, the diversiﬁcation is due to the entropy produc-
tion through the dynamics of the neutral trainable variables
(see the next section).
E6. Separation of Phenotype from Genotype. This quintessential
feature of life embodies two distinct (albeit inseparable in known
organisms)
symmetry-breaking
phenomena:
1)
separation
between dedicated digital information storage media (stable,
rarely updatable, tending to distributions with discrete values) and
mostly analog processing devices and 2) asymmetry of the infor-
mation ﬂow within the IPUs whereby the genotype provides
“instructions” for the phenotype, whereas the phenotype largely
loses the ability to update the genotype directly. The separation
between the information storage and processing subsystems is a
prerequisite for efﬁcient evolution that probably emerged early on
the path from prebiotic entities to the emergence of life.
1) The separation between phenotype and genotype extends
the scale separation on the intra-IPU level as follows from
the fundamental principles P1 to P4. Intermediate-frequency
components of an IPU (phenotype) buffer the slowest com-
ponents from direct interaction with the environment (the
highest-frequency variables), further increasing the stability
of the slowest components and making them suitable for
long-term information storage. As the temporal scales sepa-
rate further, the interactions between them change. Asym-
metric information ﬂow (P7) stabilizes the system, enabling
long-term preservation of information (genotype) while
retaining the reactive ﬂexibility of the faster-changing com-
ponents (phenotype).
2) The emergence of the separation between phenotype and
genotype is a crucial event in prebiotic evolution. This separa-
tion is prominent in all known as well as hypothetical life-
forms. Even when the phenotype and genotype roles are
fulﬁlled by chemically identical molecules, as in the RNA
world scenario of primordial evolution (55, 56), their roles as
effectors and information storage devices are sharply distinct.
In biological terms, the split is between replicators (that is, the
digital
information
carriers
[genomes])
and
reproducers
(57–59), the analog devices (cells, organisms) that host the rep-
licators, supply them with building blocks (P5), and themselves
reproduce
(P6)
under
the
replicators’
instruction
(P7).
Although the genotype/phenotype separation is a major staple
of life, it is in itself insufﬁcient to qualify an IPU as a life-form
(computers and record players, in which the separation
between information storage and operational parts is promi-
nent and essential, clearly are not life, even though invented
by advanced organisms). The asymmetry of information ﬂow
between genotype and phenotype (P7) is the most general
form of the phenomenon known as the Central Dogma of
molecular biology: the unidirectional ﬂow of information from
nucleic acids to proteins as originally formulated by Crick (60).
This asymmetry is also prominent in other information-
processing systems, in particular computers. Indeed, von
Neumann architecture computers have inherently distinct
memory and processing units, with the instruction ﬂow from
the former to the latter (61, 62). It appears that any advanced
information-processing
systems
are
endowed
with
this
property.
E7. Replication. Emergence of long-term digital storage devices,
that is genomes consisting of RNA or DNA (E6) provides for
long-term information preservation, facilitates adaptive reactions
to changes in the environment, and promotes the stability of IPUs
to the point where (at least in chemical systems) it is limited by
the energy of the chemical bonds rather than the energy of ther-
mal ﬂuctuations. Obviously, however, as long as this information
is conﬁned to a single IPU, it will disappear with the inevitable
eventual destruction of that IPU. Should this be the case, other
IPUs of similar architecture would need to accumulate a compa-
rable amount of information from scratch to reach the same level
of stability. Thus, copying and sharing information are essential
for long-term (effectively, indeﬁnite) persistence of IPUs.
1) The fundamental principle P6 postulates the existence of
mechanisms for information copying and elimination. If
genomic information can be replicated, even most primitive
sharing mechanisms (such as physical splitting of an IPU
under forces of surface tension) would result (even if not
reliably) in the emergence of distinct IPUs preloaded with
information that was amassed by their progenitor(s). This
process short circuits learning and allows the information to
accumulate at timescales far exceeding the characteristic life-
times of individual IPUs.
2) Information copying and sharing are beneﬁcial only if the
ﬁdelity exceeds a certain threshold, sometimes called Eigen
limit in evolutionary biology (5–7). Nevertheless, in primitive
prebiotic systems, the required ﬁdelity level could have been
quite low (63). For instance, even a biased chemical compo-
sition of a hydrophobic droplet could enhance the stability of
the descendant droplets and thus, endow them with an
advantage in the selection for persistence. However, once
relatively sophisticated mechanisms of information copying
and sharing emerge or more precisely, when replicators
become information storage devices, the overall stability of
the system can increase by orders of magnitude. To wit and
astonishingly, the only biosphere known to us represents an
unbroken chain of genetic information transmission that
spans about 4 billion y, commensurate with the stellar evolu-
tion scale.
E8. Natural Selection. Evolution by natural selection (Darwinian
evolution) arises from the combination of all the principles and
phenomena described above. The necessary and sufﬁcient con-
ditions for Darwinian evolution to operate are 1) the existence
of IPUs that are distinct from the environment and from each
other (E1), 2) the dependence of the stability of an IPU on the
information it contains (that is, the phenotype–genotype feed-
back; E6), and 3) the ability of IPUs to make copies of
EVOLUTION
Vanchurin et al.
Toward a theory of evolution as multilevel learning
PNAS
j
5 of 12
https://doi.org/10.1073/pnas.2120037119
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

embedded information and share it with other IPUs (E7).
When these three conditions are met, the relative frequencies
of the more stable IPUs will increase with time via attrition of
the less stable ones (survival of the ﬁttest) and transfer of infor-
mation among IPUs, both vertically (to progeny) and horizon-
tally. This process engenders the key feature of Darwinian
evolution, differential reproduction of genotypes, based on
the
feedback
from
the
environment
transmitted
through
the phenotype.
1) All seven fundamental principles of life-compatible universes
(P1 to P7) are involved in enabling evolution by natural selec-
tion. The very existence of units, on which selection can oper-
ate, hinges on self- vs. nonself-discrimination of prebiotic IPUs
(E1) and the emergence of shareable information storage (E6
and E7). The crucial step to biology is the emergence of the
link between the loss function (P1), on the one hand, and the
existence of the IPUs (P2, P3, P4, and E1), on the other hand.
Consumption of (limited) external resources (P5) entails com-
petition between IPUs that share the same environment and
turns mere shifts of the relative frequencies into true “survival
of the ﬁttest.” The ability of the IPUs to replicate (P6) and
expand their memory storage (genotype; P7, E6, and E7) pro-
vides them with access to hitherto unavailable degrees of free-
dom, making evolution an open-ended process rather than a
quick, limited search for a local optimum.
2) Evolution by natural selection is the central tenet of evolution-
ary biology and a key part of the NASA deﬁnition of life. An
important note on deﬁnitions is due. We already referred to
selection when discussing prebiotic evolution (E1); however,
the term “natural (Darwinian) selection” is here reserved for
the efﬁcient form of selection that emerges with the replication
of dedicated information storage devices (P6 and E6). Differ-
ential reproduction, whereby the environment provides feed-
back on the ﬁtness of genotypes while acting on phenotypes,
turns into Darwinian survival of the ﬁttest in the presence of
competition. When IPUs depend on environmental resources,
such competition inevitably arises, except in the unrealistic
case of unlimited supply (44). With the onset of Darwinian
evolution, the system can be considered to cross the threshold
from prelife to life (64, 65). The evolutionary process is natu-
rally represented by movement of an evolving IPU in a geno-
type space, where proximity is deﬁned by similarity between
distinct genotypes and transitions correspond to elementary
evolutionary events: that is, mutations in the most general
sense (66). For any given environment, ﬁtness—that is, a mea-
sure of the ability of a genotype to produce viable offspring—-
can be deﬁned for each point in the genotype space, forming a
multidimensional ﬁtness landscape (53, 54). Selection creates a
bias for preferential ﬁxation of mutations that increase ﬁtness,
even if the mutations themselves occur completely randomly.
E9. Parasitism. Parasites and host–parasite coevolution are ubiqui-
tous across biological systems at multiple levels of organization
and are both intrinsic to and indispensable for the evolution
of life.
1) Due to the ﬂexibility of life-compatible systems (P5 and P6)
and the symmetry breaking in the information ﬂow (P7)
combined with the inherent tendency of life to diversify
(E5), parts of the system inevitably settle on a parasitic state:
that is, scavenging information and matter from the host
without making a positive contribution to its ﬁtness.
2) From the biological perspective, parasites evolve to minimize
their direct interface with the environment and conversely,
maximize their interaction with the host; in other words, the
host replaces most of the environment for the parasite. Para-
sites inevitably emerge and persist in biological systems
because of two reasons. 1) The parasitic state is reachable
via an entropy-increasing step and therefore, is highly proba-
ble (16), and 2) highly efﬁcient antiparasite immunity is
costly (67). The cost of immunity reﬂects another universal
trade-off analogous to the trade-off between information
transfer ﬁdelity and energy expenditure; in both cases, an
inﬁnite amount of energy is required to reach a zero error
rate or a parasite-free state. From a complementary stand-
point, parasites inevitably evolve as cheaters in the game of
life that exploit the host as a resource, without expending
energy on resource production. Short-term, parasites reduce
the host ﬁtness by both direct drain on its resources and vari-
ous indirect effects, including the cost of defense. However,
in a longer-term perspective, parasites make up a reservoir
for recruitment of new functions (especially, but far from
exclusively,
for
defense)
by
the
hosts
(14,
15).
The
host–parasite relationship can evolve toward transition to a
mutually beneﬁcial, symbiotic lifestyle that can further pro-
gress
to
mutualism
and
in
some
cases,
complete
integration as exempliﬁed by the origin of essential endosym-
biotic organelles in eukaryotes, mitochondria, and chloro-
plasts (68, 69). Parasites emerge at similar levels of biological
organization (organisms parasitizing other organisms) or
across
levels
(genetic
elements
parasitizing
organismal
genomes or cell clones parasitizing multicellular organisms).
E10. Programmed Death. Programmed (to various degrees) death
is an intrinsic feature of life.
1) Replication and elimination of IPUs (P6) and utilization of
additional degrees of freedom (P5) form the foundation for the
phenomenon of programmed death. At some levels of organi-
zation (for example, intragenomic), the ability to add and elimi-
nate units (such as genes) for the beneﬁt of the higher-level
systems (such as organisms) provides an obvious path of opti-
mization. Elimination of units could be, in principle, completely
random, but selection (E8) generates a sufﬁciently strong feed-
back to facilitate and structure the loss process (for example,
purging low-ﬁtness genes via homologous recombination or
altruistic suicide of infected or otherwise impaired cells). The
same forces operate at least at the cell level and conceivably, at
all levels of organization and selection (P4). In particular, if
population-level or kin-level selection is sufﬁciently strong,
mechanisms for altruistic death of individual organisms appar-
ently can be ﬁxed in evolution (70, 71).
2) Programmed death is a prominent case of minimization of the
higher-level (for example, organismal) loss function at the cost
of increasing the lower-level loss function (such as that of indi-
vidual cells). Although (tightly controlled) programmed cell
death was originally discovered in multicellular organisms and
has been thought to be limited to these complex life-forms,
altruistic cell suicide now appears to be a universal biological
phenomenon (71–73).
To conclude this section, which we titled “fundamental evo-
lutionary phenomena,” deliberately omitting “biological,” it
seems important to note that phenomena E1 to E7 are generic,
applying to all learning systems, including purely physical and
prebiotic ones. However, the onset of natural selection (E8)
marks the origin of life, so that the phenomena E8 to E10
belong in the realm of biology.
3. Optimization and Scale Separation in Evolving Systems
In the previous sections, we formulated the seven fundamen-
tal principles of evolution P1 to P7 and then, argued that the
key evolutionary phenomena E1 to E10 can be interpreted
and analyzed in the context of these principles and appar-
ently, derived from the latter. The next step is to formulate a
6 of 12
j
PNAS
Vanchurin et al.
https://doi.org/10.1073/pnas.2120037119
Toward a theory of evolution as multilevel learning
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

mathematical framework that would be consistent with the
fundamental principles and thus, would allow us to model
evolutionary phenomena analytically or numerically. For
concreteness, the proposed framework is based on a mathe-
matical model of artiﬁcial neural networks (74, 75), but we
ﬁrst outline a general optimization approach in a form suit-
able for modeling biological evolution.
We are interested in the broadest class of optimization prob-
lems, where the loss (or cost) function H x,q
ð
Þ is minimized
with respect to some trainable variables,
q ¼
qðcÞ,qðaÞ, qðnÞ


,
[3.1]
for a given training set of nontrainable variables,
x ¼
xðoÞ,xðeÞ


:
[3.2]
Near a local minimum, the ﬁrst derivatives of the average loss
function with respect to trainable variables q are small, and the
depth of the minimum usually depends on the second derivatives.
In particular, the second derivative can be large for the effectively
constant degrees of freedom, qðcÞ; small for adaptable degrees of
freedom, qðaÞ; or near zero for symmetries or neutral directions
qðnÞ. The separation of the neutral directions qðnÞ into a special
class of variables simply means that some of the trainable varia-
bles can be changed without affecting the learning outcome: that
is, the value of the loss function. Put another way, neutral changes
are always possible. The neutral directions qðnÞ are the fastest
changing among the trainable variables because ﬂuctuations
resulting in their change are, in general, fully stochastic. On the
other end of the spectrum of variables, even minor changes to the
effectively constant variables qðcÞ compromise the entire learning
(evolution) process: that is, result in a substantial increase of the
loss function value; these variables correspond to deep minima of
the loss function. When the basin of attraction of a minimum is
deep and narrow, the system stays in its bottom for a long time,
and then, to describe such a state, it is sufﬁcient to use discrete
information (that is, to indicate that the system stays in a given
minimum) rather than to list all speciﬁc values of the coordinates
in a multidimensional space.
In a generic optimization problem, the dynamics of both
trainable and nontrainable variables involves a broad distribu-
tion of characteristic timescales τ, and switching between scales
is equivalent to switching between different frequencies or in
the context of biological evolution, between different levels of
organization. For any ﬁxed τ, all variables can be partitioned
into three classes depending on how fast they change with
respect to the speciﬁed timescale:
fast-changing
nontrainable
variables
that
characterize
an
organism ðx o
ð ÞÞ and its environment xðeÞ and change on time-
scales ≪τ;
intermediate-changing adaptable variables qðaÞ
or neutral
directions qðnÞ that change on timescales ~τ; and
slow-changing variables, which are the degrees of freedom qðcÞ
that have already been well trained and are effectively constant
(at or near equilibrium), only changing on timescales ≫τ.
As will become evident shortly, the separation of these three
classes of variables and interactions between them are central
to the evolution and selection on all levels of organization,
resulting in pervasive multilevel learning and selection.
Depending on the considered timescale τ (or as a result of
environmental changes), the same dynamical degree of free-
dom can be assigned to different classes of variables: that is,
xðoÞ, xðeÞ, qðcÞ, qðaÞ, or qðnÞ. For example, on the shortest
timescale, which corresponds to the lifetime of an individual
organism (one generation), the adaptable variables are the phe-
notypic traits that quickly respond to environmental changes,
whereas the slowest, near-constant variables are the genomic
sequences (genotype) that change minimally if at all. On longer
timescales, corresponding to thousands or millions of genera-
tions, fast-evolving portions of the genome become adaptable
variables, whereas the conserved core of the genome remains in
the near-constant class (50). Analogously, the neutral directions
correspond either to nonconsequential phenotypic changes or
to neutral genomic mutations, depending on the timescale. It is
well known that the overwhelming majority of the mutations
are either deleterious and therefore, eliminated by purifying
selection or (nearly) neutral and thus, can be either lost or ﬁxed
via drift (76, 77). However, when the environment changes or
under the inﬂuence of other mutations, some of the neutral
mutations can become beneﬁcial [a genetic phenomenon
known as epistasis, which is pervasive in evolution (78, 79)],
and in their entirety, neutral mutations form the essential reser-
voir of variation available for adaptive evolution (80). Even
which variables are classiﬁed as nontrainable ðxÞ depends on
the timescale τ. For example, if a learning system was trained
for a sufﬁciently long time, some of the trainable variables qðaÞ
or
qðnÞ
might
have
already
equilibrated
and
become
nontrainable.
4. The Neural Network Framework
Now that we described an optimization problem that is suitable
for modeling evolution of organisms (or populations of organ-
isms), we can construct a mathematical framework to solve such
optimization problems. For this purpose, we employ a mathemati-
cal theory of artiﬁcial neural networks (74, 75), which is simple
enough to perform calculations while being consistent with all of
the fundamental principles (P1 to P7), and thus, it can be used
for modeling evolutionary phenomena (E1 to E10). We ﬁrst recall
a general framework of the neural network theory.
Consider a learning system represented as a neural network,
with the state vector described by trainable variables q (which
describe a collective notation for weight matrix ^w and bias vec-
tor b) and nontrainable variables x (which describe the current
state vector of individual neurons). In the biological context, x
collectively represent the current state of the organism xðoÞ and
of its environment xðeÞ, and q determines how x changes with
time, in particular, how the organism reacts to environmental
challenges. The nontrainable variables are modeled as changing
in discrete time steps
xi t þ 1
ð
Þ ¼ fi
∑
j
wijxjðtÞ þ bi
 
!
,
[4.1]
where fiðyÞ’s are some nonlinear activation functions (for exam-
ple, hyperbolic tangent or rectiﬁer activation functions). The
trainable variables are modeled as changing according to the
gradient descent (or stochastic gradient descent) algorithm
qi t þ 1
ð
Þ ¼ qi tð Þ  γ ∂Hðx tð Þ,q tð ÞÞ
∂qi
,
[4.2]
where γ is the learning rate parameter and Hðx,qÞ is a suitably
deﬁned loss function (Eqs. 4.3 and 4.4). In other words, q are
“gross” or “main” variables, which determine the rules of
dynamics, and the dynamics of all other variables x is governed
by these rules, per Eq. 4.1. In the biological context, Eq. 4.1
represents fast, often stochastic environmental changes and the
corresponding fast reaction of organisms at the phenotype
level, whereas [4.2] reﬂects slower-learning dynamics of evolu-
tionary adaptation via changes in the intermediate, adaptable
variables: that is, the variable portion of the genome. The main
learning objective is to adjust the trainable variables such that
the average loss function is minimized subject to boundary
EVOLUTION
Vanchurin et al.
Toward a theory of evolution as multilevel learning
PNAS
j
7 of 12
https://doi.org/10.1073/pnas.2120037119
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

conditions (also known as the training dataset), which in our
case, is modeled as a time sequence of the environmental
variables.
For example, on a single-generation timescale, the fast-changing
variables represent the environment xðeÞ and nontrainable variables
associated with organisms xðoÞ, the intermediate-changing variables
represent adaptive qðaÞ and neutral qðnÞ phenotype changes, and
the slow-changing variables qðcÞ
represent the genotype (SI
Appendix, Fig. S1).
The temporal-scale separation in biology is readily appar-
ent in all organisms. Indeed, consequential changes in the
environment xðeÞ often occur on the scale of milliseconds to
seconds, triggering physical changes within organisms xðoÞ
at matching timescales. In response, individual organisms
respond with phenotypic changes both adaptive qðaÞ and neu-
tral qðnÞ on the scale of minutes to hours, exploiting their
genetically encoded phenotypic plasticity. A paradigmatic
example is induction of bacterial operons in response to a
change in the chemical composition of the environment, such
as the switch from glucose to galactose as the primary nutri-
ent (81, 82). In contrast, changes in the genome qðcÞ take
much longer. Mutations typically occur at rates of about 1 to
10 per genome replication cycle (83), which for unicellular
organisms, is the same as a generation comprising from
about an hour to hundreds or even thousands of hours. How-
ever, ﬁxation of mutations, which represents an evolution-
arily stable change at the genome level, typically takes many
generations and thus, always occurs orders of magnitude
slower than phenotype changes. Accordingly, on this time-
scale, any changes in the genome represent the third layer in
the network, the slowly changing variables.
To specify a microscopic loss function that would be
appropriate for describing evolution and thus, give a speciﬁc
form to the fundamental principle P1, we ﬁrst note that
adaptation to the environment is more efﬁcient (that is, the
loss function value is smaller) for a learning system, such as
an organism, that can predict the state of its environment
with a smaller error. Then, the relevant quantity is the
so-called “boundary” loss function deﬁned as the sum of
squared errors,
He x, q
ð
Þ ≡1
2 ∑
i∈E
xðeÞ
i
 fiðx o
ð Þ,qÞ

2
,
[4.3]
where the summation is taken only over the boundary (or
environmental) nontrainable variables. It is helpful to think
of the boundary loss function as the mismatch between the
actual state of the environment and the state that would be
predicted by the
neural network
if the
environmental
dynamics was switched off. In neuroscience, boundary loss is
closely related to the surprise (or prediction error) associ-
ated with predictions of sensations, which depend on an
internal model of the environment (84). In machine learn-
ing, boundary loss functions are most often used in the con-
text of supervised learning (35), and in biological evolution,
the “supervision” comes from the environment, which the
evolving system, such as an organism or a population, is
learning to predict.
Another possibility for a learning system is to search for the
minimum of the “bulk” loss function, which is deﬁned as the
sum of squared errors over all neurons:
H x, q
ð
Þ ¼ 1
2 ∑
i
xi  fiðx o
ð Þ,qÞ

2
:
[4.4]
The bulk loss function assumes extra cost incurred by changing
the states of organismal neurons, x o
ð Þ: that is, rewarding stationary
states. In the limit of a very large number of environmental
neurons,
the
two
loss
functions
are
indistinguishable,
H x,q
ð
Þ ≈He x,q
ð
Þ, but bulk loss is easier to handle mathemati-
cally (the details of boundary and bulk loss functions are
addressed in ref. 35).
More generally, in addition to the “kinetic” term [4.4], the
loss function can include a “potential” term V x,q
ð
Þ:
H x,q
ð
Þ ¼ 1
2∑
i
xi  fiðx o
ð Þ,qÞ

2
þ V x,q
ð
Þ:
[4.5]
The kinetic term in [4.5] reﬂects the ability of organisms x o
ð Þ to
predict the changes in the state of the given environment x e
ð Þ
over time, whereas the potential term reﬂects their compatibil-
ity with a given environment and hence, the capacity to choose
among different environments.
In the context of biological evolution, Malthusian ﬁtness φ is
deﬁned as the expected reproductive success of a given geno-
type: that is, the rate of change of the prevalence of the given
genotype in an evolving population (85). However, in the con-
text of the theory of learning, the loss function must be identi-
ﬁed with additive ﬁtness: that is,
H x,q
ð
Þ ¼ Tlogφ x,q
ð
Þ:
[4.6]
For a microscopic description of learning, the proportionality
constant is unimportant, but as we argue in detail in the accom-
panying paper (86), in the description of the evolutionary pro-
cess from the point of view of thermodynamics, T plays the role
of “evolutionary temperature.”
Given a concrete mathematical model of neural networks, one
might wonder if all fundamental principles of evolution (P1 to P7)
can be derived from this model. Such derivation would comprise
additional evidence supporting the claim that the entire universe
can be adequately described as a neural network (36). Clearly, the
existence of a loss function (P1) follows automatically because
learning of any neural network is always described relative to a
speciﬁed loss function (Eq. 4.4 or 4.5). The other six principles
also seem to naturally emerge from the learning dynamics of neu-
ral networks. In particular, the hierarchy of scales (P2) and fre-
quency gaps (P3) are generic consequences of the learning
dynamics, whereby a system that involves a wide range of varia-
bles changing at different rates is attracted toward a self-
organized critical state of slow-changing trainable variables (87).
Additional gaps between levels of organization are also expected
to appear through phase transitions as becomes apparent in the
thermodynamic description of evolution we develop in the accom-
panying paper (86). Renormalizability (P4) is a direct conse-
quence of the second law of learning (35), according to which
entropy of a system (and consequently, complexity of neural net-
work or rank of its weight matrix) decreases with learning. This
phenomenon was observed in neural network simulations (35)
and is the exact type of dynamics that can make the system renor-
malizable even if it started off as a highly entangled (large rank of
weight matrix), nonrenormalizable neural network. The extension
(P5) and replication (P6) principles simply indicate that additional
variables can lead to either increase or decrease in the value of
the loss function (35). It is also important to note that in neural
networks, an additional computational advantage (“quantum
advantage”) can be achieved if the number of IPUs can vary (88).
Therefore, to achieve such an advantage, a system must learn how
to replicate and eliminate its IPUs (P6). Finally, in Generalized
Central Dogma of Molecular Biology, we illustrate how Fourier
transform (or more generally, wavelet transform) of the environ-
mental degrees of freedom can be used for learning the environ-
ment and how the inverse transform can be used for predicting it.
Thus, to be able to predict the environment (and hence, to be
competitive), any evolving system must learn the mechanism
behind such asymmetric information ﬂow (P7).
8 of 12
j
PNAS
Vanchurin et al.
https://doi.org/10.1073/pnas.2120037119
Toward a theory of evolution as multilevel learning
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

5. Multilevel Learning
In the previous sections, we argued that the learning process
naturally divides all the dynamical variables into three distinct
classes: fast-changing ones, x o
ð Þ and x e
ð Þ; intermediate-changing
ones, q a
ð Þ and q n
ð Þ (q n
ð Þ being faster than q a
ð Þ); and slow-
changing ones, q c
ð Þ (SI Appendix, Fig. S1). Evidently, this sepa-
ration of variables depends on the timescale τ during which the
system is observed, and variables migrate between classes when
τ is increased or decreased (SI Appendix, Fig. S2). The longer
the time, the more variables reach equilibrium and therefore,
can be modeled as nontrainable and fast changing, x e
ð Þ, and the
fewer variables remain slowly varying and can be modeled as
effectively constant q c
ð Þ. In other words, many variables that are
nearly constant at short timescales migrate to the intermediate
class at longer timescales, whereas variables from the interme-
diate class migrate to the fast class.
In biological terms, if we consider learning dynamics on the
timescale of one generation, then q a
ð Þ and q n
ð Þ represent pheno-
type variables, and q c
ð Þ represents genotype variables; however,
on much longer timescales of multiple generations, the learning
dynamics of populations (or communities) of organisms becomes
relevant. On such timescales, the genotype variables acquire
dynamics, with purifying and positive selection getting into play,
whereas the phenotype variables progressively equilibrate. There
is a clear connection between learning dynamics, including that in
biological systems, and renormalizability of physical theories (P4).
Indeed, from the point of view of an efﬁcient learning algorithm,
the parameters controlling learning dynamics, such as effective
learning (or information-processing) rate γ, can vary from one
timescale to another (for example, from individual organisms to
populations or communities of organisms), but the general princi-
ples as well as speciﬁc dependencies captured in the equations
above that govern the learning dynamics on different timescales
remain the same. We refer to this universality of the learning pro-
cess on different timescales and partitioning of the variables into
temporal classes as multilevel learning.
More precisely, multilevel learning is a property of learning sys-
tems, which allows for the basic equations of learning, such as
[4.4], to remain the same on all levels of organization but for the
parameters, which describe the dynamics such as γðτÞ, to depend
on the level or on the timescale τ. For example, if the effective
learning (or information-processing) rate γðτÞ decreases with
timescale τ, then the local processing time, which depends on
γðτÞ, runs differently for different trainable variables: slower for
slow-changing variables (or larger τ) and faster for fast-changing
ones (or smaller τ). For such a system, the concept of global time
(that is, the same time for all variables) becomes irrelevant and
should be replaced with the proper or local time, which is deﬁned
for each scale τ separately:
tτ ∝γðτÞt:
[5.1]
This effect closely resembles time dilation phenomena in physics,
except that in special and general relativity, time dilation is linked
with the possibility of movement between slow and fast clocks (or
variables) (89). To illustrate the role time dilation plays in biology,
consider only two types of variables: slow changing and fast chang-
ing. Then, the slow variables should be able to “outsource” certain
computational tasks to faster variables. Because the local clock for
the fast-changing variables runs faster, the slow-changing variables
can take advantage of the fast-changing ones to accelerate compu-
tation, which would be rewarded by evolution. The ﬂow of infor-
mation between slow-changing and fast-changing variables in the
opposite direction is also beneﬁcial because the fast-changing
variables can use the slow-changing variables to store useful infor-
mation for future retrieval: that is, the slow variables function as
long-term memory. In the next section, we show that such cooper-
ation between slow- and fast-changing variables, which is a
concrete manifestation of principle P7, corresponds to a crucial
biological phenomenon as the Central Dogma of molecular biol-
ogy (60).
6. Generalized Central Dogma of Molecular Biology
In terms of learning theory, the two directions of the asymmetric
information ﬂow (P7) represent learning the state of the environ-
ment and predicting the state of the environment from the results
of learning. For learning, information is passed from faster varia-
bles to slower variables, and for predicting, information ﬂows in
the opposite direction from slower variables to the faster ones. A
more formal analysis of the asymmetric information ﬂows (or a
generalized Central Dogma) can be carried out by forward propa-
gation (from slow variables to fast variables) and back propaga-
tion (from fast variables to slow variables) of information within
the framework of the mathematical model of neural networks
developed in the previous sections (SI Appendix, Fig. S3).
Consider nontrainable environmental variables that change
continuously with time x e
ð ÞðtÞ, while the learning objective of an
organism is to predict x e
ð ÞðtÞ at time t > τ given that it was
observed for time 0 < t < τ. Thus, the organism has to extrapo-
late the function x e
ð ÞðtÞ for t > τ, and to do so, it should be able
to store and retrieve the values of the Fourier coefﬁcients
qk ¼ 1
τ
ðτ
0
dtx e
ð ÞðtÞei2πfkt,
[6.1]
or more generally, wavelet coefﬁcients
qk ¼ 1
τ
ðτ
0
dtWkðτ  tÞx e
ð ÞðtÞei2πfkt,
[6.2]
for suitably deﬁned window functions Wi. Then, a prediction
could be made by extrapolating x e
ð ÞðtÞ using the inverse trans-
formation
x e
ð Þðt þ δÞ ≈2 ∑
kmax
k¼kmin
Reðqkei2πfkδÞ,
[6.3]
for some δ > 0, which is not too large compared with τ. How-
ever, in general, the total number of (Fourier or wavelet) coefﬁ-
cients qk would be countably inﬁnite. Therefore, any ﬁnite-size
organism has to “decide” which frequencies to observe (and
remember) and which ones to ﬁlter out (and “forget”).
Let us assume that the organism “decided” to only observe/
remember discrete frequencies
fmin ≡fkmin, …, fkmax ≡fmax,
[6.4]
and forget everything else. Then, to predict the state of the envi-
ronment [6.3] and as a result, minimize the loss function [4.5], the
organism should be able to store, retrieve, and adjust information
about coefﬁcients qk in some adaptable trainable variables q a
ð Þ.
Given this simple model, we can study the ﬂow of informa-
tion between different nontrainable variables of the organism
x o
ð Þ. To this end, it is convenient to organize the variables as
x ¼
x o
ð Þ,x e
ð Þ


¼
xkmin,…,xkmax,x e
ð Þ


,
[6.5]
where
xkðt þ δÞ ≈2 ∑
k
l¼kmin
Reðqlei2πflδÞ,
[6.6]
and assume that the relevant information about qi’s is stored in
the adaptable trainable variables
q a
ð Þ ¼ qkmin,…,qkmax


:
[6.7]
In the estimate of xkðt þ δÞ in Eq. 6.6, all the higher-frequency
modes are assumed to average to zero as is often the case if we
EVOLUTION
Vanchurin et al.
Toward a theory of evolution as multilevel learning
PNAS
j
9 of 12
https://doi.org/10.1073/pnas.2120037119
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

are only interested in the timescale f 1
k . A better estimate can
be obtained using, once again, the ideas of the renormalization
group ﬂow following the fundamental principle P4. To make
learning (and thus, survival) efﬁcient, truncation of the set of
variables relevant for learning is crucial. The main point is that
the higher-frequency modes can still contribute statistically,
and then, an improved estimate of xkðt þ δÞ would be obtained
by appropriately modifying the values of the coefﬁcients qk.
Either way, in order to make an actual prediction, the organism
should ﬁrst calculate xkðt þ δÞ for small fk and then, pass the
result to the next level to calculate xkþ1ðt þ δÞ for larger fkþ1
and so on. Such computations can be described by a simple
mapping
xkþ1 t þ δ
ð
Þ ¼ xk t þ δ
ð
Þ þ 2Reðqkþ1ei2πfkþ1δ
,
[6.8]
which can be interpreted as passage of data from one layer to
another in a deep, multilayer neural network (SI Appendix, Fig.
S2). Eq. 6.8 implies that, during the predicting phase, relevant
information only ﬂows from variables encoding low frequencies
to variables encoding high frequencies but not in the reverse
direction. In other words, in the process of predicting the envi-
ronment, information propagates from slower variables to
faster variables: that is, from genotype to phenotype or from
nucleic acids to proteins (hence, the Central Dogma). Because
only the fast variables change in this process, the prediction of
the state of the environment is rapid, as it is indeed required to
be for the organism survival. Conversely, in the process of
learning the environment, information is back propagated in
the opposite direction: that is, from faster to slower variables.
However, this back propagation is not a microscopic reversal of
the forward propagation but a distinct, much slower process
(given that changes in slow variables are required) that involves
mutation and selection.
Thus, the meaning of the generalized Central Dogma from
the point of view of the learning theory—and our theory of evo-
lution—is that slow dynamics (that is, evolution on a long time-
scale) should be mostly independent of the fast variables. In
less formal terms, slow variables determine the rules of the
game, and changing these rules depending on the results of
some particular games would be detrimental for the organism.
Optimization within the space of opportunities constrained by
temporally stable rules is advantageous compared with optimi-
zation without such constraints. The trade-off between global
and local optimization is a general, intrinsic property of frus-
trated systems (E2). For the system to function efﬁciently, the
impact of local optimization on the global optimization should
be restricted. The separation of the long-term and short-term
forms of memory through different elemental bases (nucleic
acids vs. proteins) serves this objective.
7. Discussion
In this work, we outline a theory of evolution on the basis of
the theory of learning. The parallel between learning and bio-
logical evolution becomes obvious as soon as the mapping
between the loss function and the ﬁtness function is established
(Eq. 4.6). Indeed, both processes represent movement of an
evolving (learning) system on a ﬁtness (loss function) land-
scape, where adaptive (learning), upward moves are most con-
sequential, although neutral moves are most common, and
downward moves also occur occasionally. However, we go
beyond the obvious analogy and trace a detailed correspon-
dence between the essential features of the evolutionary and
learning processes. Arguably, the most important fundamental
commonality between evolution and learning is the stratiﬁca-
tion of the trainable variables (degrees of freedom) into classes
that differ by the rate of change. At least in complex
environments, all learning is multilevel, and so is all selection
that is relevant for the evolutionary process. The framework of
evolution as learning developed here implies that evolution of
biological complexity would be impossible without MLS perme-
ating
the
entire
history
of
life.
Under
this
perspective,
emergence of new levels of organization, in learning and in
evolution, and in particular, MTE represent genuine phase
transitions as previously suggested (41). Such transitions can be
analyzed consistently only in the thermodynamic limit, which is
addressed in detail in the accompanying paper (86).
The origin of complexity and long-term memory from simple
fundamental physical laws is one of the hardest problems in all
of science. One popular approach is synergetics pioneered by
Haken (91, 92) and the related nonequilibrium thermodynam-
ics founded by Prigogine and Stengers (93) that employ mathe-
matical tools of the theory of dynamical systems, such as theory
of bifurcations and analysis of attractors. However, these con-
cepts appear to be too general and oversimpliﬁed to usefully
analyze biological phenomena, which are far more complex
than “dissipative structures” that are central to nonequilibrium
thermodynamics, such as, for example, autowave chemical
reactions.
An alternative is the approach based on the theory of spin
glasses (43, 94), which employs the mathematical apparatus of
statistical physics and seems to provide a deeper insight into
the origin of complexity. However, the energy landscape of spin
glasses contains too many minima that are too shallow to
account for long-term memory that is central to biology (12,
41). Thus, some generalization of the spin glass concept is likely
to be required for productive application in evolutionary biol-
ogy (95).
A popular and promising approach is self-organized criticality
(SOC), a concept developed by Bak et al. (96, 97). Although rele-
vant in biological contexts (12), SOC, by deﬁnition, implies
self-similarity between different levels of organization, whereas
biologically relevant complexity is rather associated with distinct
emergent phenomena at different spatiotemporal scales (90).
A fundamental shortcoming of all these approaches is that
they do not include, at least not as a major component, evolu-
tionary concepts, such as natural selection. The framework of
learning theory used here allows us to naturally unify the
descriptions of physical and biological phenomena in terms of
optimization by trial and error and loss (ﬁtness) functions.
Indeed, a key point of the present analysis is that most of our
general principles apply to both living and nonliving systems.
The detailed correspondence between the key features of the
processes of learning and biological evolution implies that this is
not a simple analogy but rather, a reﬂection of the deep unity
of evolutionary processes occurring in the universe. Indeed,
separation of the relevant degrees of freedom into multiple tem-
poral classes is ubiquitous in the universe from composite sub-
atomic particles, such as protons, to atoms, molecules, life-forms,
planetary systems, and galaxy clusters. If the entire universe is
conceptualized as a neural network (36), all these systems can be
considered emerging from the learning dynamics. Furthermore,
scale separation and renormalizability appear to be essential con-
ditions for a universe to be observable. According to the evolution
theory outlined here, any observable universe consists of systems
that undergo learning or synonymously, adaptive evolution, and
actually, the universe itself is such a system (36). The famous dic-
tum of Dobzhansky (98), thus, can and arguably should be
rephrased as “[n]othing in the world is comprehensible except in
the light of learning.”
Within the theory of evolution outlined here, the difference
between life and nonliving systems, however important, can be
considered as one in the type and degree of optimization, so
that all evolutionary phenomena can be described within the
same formal framework of the theory of learning. Crucially, any
10 of 12
j
PNAS
Vanchurin et al.
https://doi.org/10.1073/pnas.2120037119
Toward a theory of evolution as multilevel learning
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

complex optimization problem can be addressed only with a
stochastic learning algorithm: hence, the ubiquity of selection.
Origin of life can then be conceptualized within the framework
of multilevel learning as we explicitly show in the accompanying
paper (86). The point when life begins can be naturally associ-
ated with the emergence of a distinct class of slowly changing
variables that are digitized and thus, can be accurately repli-
cated; these digital variables store and supply information for
forward propagation to predict the state of the environment. In
biological terms, this focal point corresponds to the advent of
replicators (genomes) that carry information on the operation
of reproducers within which they reside (99). This is also the
point when natural (Darwinian) selection takes off (64). Our
theory of evolution implies that this pivotal stage was preceded
by evolution of “prelife,” which comprised reproducers that
lacked genomes but nevertheless, were learning systems that
were subject to selection for persistence. Self-reproducing
micelles that harbor autocatalytic protometabolic reaction net-
works appear to be plausible models of such primordial repro-
ducers (100). The ﬁrst replicators (RNA molecules) would
evolve within these reproducers, perhaps, initially, as molecular
parasites (E9) but subsequently, under selection for the ability
to store, express, and share information essential for the entire
system. This key step greatly increased the efﬁciency of evolu-
tion/learning and provided for long-term memory that persisted
throughout the history of life, enabling the onset of natural
selection and the unprecedented diversiﬁcation of life-forms
(E5). It has to be emphasized that, compared with the existing
evolutionary models that explore replicator dynamics, the
learning approach described here is more microscopic in that
the existence of replicators is not initially assumed but rather,
appears as an emergent property of multilevel learning dynam-
ics. For learning to be efﬁcient, the capacity of the system to
add new adaptable variables is essential. In biological terms,
this implies expandability of the genome (that is, the ability to
add new genes), which necessitated the transition from RNA to
DNA as the genome substrate given the apparent intrinsic size
constraints on replicating RNA molecules. Another essential
condition for efﬁcient learning is information sharing, which in
the biological context, corresponds to horizontal gene transfer.
The essentiality of horizontal gene transfer at the earliest stages
of life evolution is perceived as the cause of the universality of
the translation machinery and genetic code in all known life-
forms (101). The conceptual model of the origin of life implied
by our learning-based theoretical framework appears to be fully
compatible with Ganti’s chemoton, a model of protocell emer-
gence and evolution based on autocatalytic reaction networks
(102–104).
The origin of life scenario within the encompassing frame-
work of the present evolution theory, even if formulated in
most general terms, implies that emergence of complexity com-
mensurate with life is a general trend in the evolution of com-
plex systems. At face value, this conclusion might seem to be at
odds with the magnitude of complexiﬁcation involved in the
origin of life [sufﬁce it to consider the complexity of the transla-
tion system (7)] and the uniqueness of this event, at least on
Earth and probably, on a much greater cosmic scale. Neverthe-
less, the origin of life appears to be an expected outcome of
learning subject to the relevant constraints, such as the pres-
ence of the required chemicals in sufﬁcient concentrations.
Such constraints would make life a rare phenomenon but likely
far from unique on the scale of the universe. The universe is
sometimes claimed to be ﬁne-tuned for the existence of life
(105). What we posit here is that the universe is self-tuned for
life emergence.
Evidently, the analysis presented here and in the accompany-
ing paper (86) is only an outline of a theory of evolution as
learning. The details and implications, including directly test-
able ones, remain to be worked out.
Data Availability. There are no data underlying this work.
ACKNOWLEDGMENTS. E.V.K. is grateful to Dr. Puriﬁcacion Lopez-Garcia for
essential discussions and critical reading of the manuscript. V.V. was supported
in part by the Foundational Questions Institute and the Oak Ridge Institute
for Science and Education. Y.I.W. and E.V.K. are supported by the Intramural
Research Program of the NIH.
1. M. Morange, The recent evolution of the question “What is life”? Hist. Philos. Life
Sci. 34, 425–436 (2012).
2. S. A. Benner, Deﬁning life. Astrobiology 10, 1021–1030 (2010).
3. E. N. Trifonov, Vocabulary of deﬁnitions of life suggests a deﬁnition. J. Biomol.
Struct. Dyn. 29, 259–266 (2011).
4. E. V. Koonin, The Logic of Chance: The Nature and Origin of Biological Evolution (FT
Press, Upper Saddle River, NJ, 2011).
5. M. Eigen, Selforganization of matter and the evolution of biological macromole-
cules. Naturwissenschaften 58, 465–523 (1971).
6. D. Penny, An interpretative review of the origin of life research. Biol. Philos. 20,
633–671 (2005).
7. Y. I. Wolf, E. V. Koonin, On the origin of the translation system and the genetic code
in the RNA world by means of natural selection, exaptation, and subfunctionaliza-
tion. Biol. Direct 2, 14 (2007).
8. D. L. Hartl, A. G. Clark, Principles of Population Genetics (Sinauer Associates, Sunder-
land, MA, ed. 4, 2006).
9. M. Lynch, The Origins of Genome Architecture (Sinauer Associates, Sunderland,
MA, 2007).
10. J. P. Bernardes et al., The evolution of convex trade-offs enables the transition
towards multicellularity. Nat. Commun. 12, 4222 (2021).
11. R. E. Michod, The group covariance effect and ﬁtness trade-offs during evolu-
tionary transitions in individuality. Proc. Natl. Acad. Sci. U.S.A. 103, 9113–9117
(2006).
12. Y. I. Wolf, M. I. Katsnelson, E. V. Koonin, Physical foundations of biological complex-
ity. Proc. Natl. Acad. Sci. U.S.A. 115, E8678–E8687 (2018).
13. P. Forterre, D. Prangishvili, The great billion-year war between ribosome- and
capsid-encoding organisms (cells and viruses) as the major source of evolutionary
novelties. Ann. N. Y. Acad. Sci. 1178, 65–77 (2009).
14. E. V. Koonin, Viruses and mobile elements as drivers of evolutionary transitions.
Philos. Trans. R. Soc. Lond. B Biol. Sci. 371, 20150442 (2016).
15. E. V. Koonin, K. S. Makarova, Y. I. Wolf, M. Krupovic, Evolutionary entanglement of
mobile genetic elements and host defence systems: Guns for hire. Nat. Rev. Genet.
21, 119–131 (2020).
16. E. V. Koonin, Y. I. Wolf, M. I. Katsnelson, Inevitability of the emergence and persis-
tence of genetic parasites caused by thermodynamic instability of parasite-free
states. Biol. Direct 12, 31 (2017).
17. P. Nelson, J. Masel, Intercellular competition and the inevitability of multicellular
aging. Proc. Natl. Acad. Sci. U.S.A. 114, 12982–12987 (2017).
18. A. M. Boddy, W. Huang, A. Aktipis, Life history trade-offs in tumors. Curr. Pathobiol.
Rep. 6, 201–207 (2018).
19. E. Szathmary, J. M. Smith, The major evolutionary transitions. Nature 374, 227–232
(1995).
20. J. Maynard Smith, E. Szathmary, The Major Transitions in Evolution (Oxford Univer-
sity Press, Oxford, United Kingdom, 1997).
21. E. Szathmary, Toward major evolutionary transitions theory 2.0. Proc. Natl. Acad.
Sci. U.S.A. 112, 10104–10111 (2015).
22. D. Czegel, I. Zachar, E. Szathmary, Multilevel selection as Bayesian inference,
major transitions in individuality as structure learning. R. Soc. Open Sci. 6,
190202 (2019).
23. S. Okashi, Evolution and the Levels of Selection (Oxford University Press, Oxford,
United Kingdom, 2006).
24. T. D. Brunet, W. F. Doolittle, Multilevel selection theory and the evolutionary func-
tions of transposable elements. Genome Biol. Evol. 7, 2445–2457 (2015).
25. A. Gardner, The genetical theory of multilevel selection. J. Evol. Biol. 28, 305–319 (2015).
26. C. Jeler, Explanatory goals and explanatory means in multilevel selection theory.
Hist. Philos. Life Sci. 42, 36 (2020).
27. M. J. Wade et al., Multilevel and kin selection in a connected world. Nature 463,
E8–E9 (2010).
28. P. Marquez-Zacarıas et al., Evolution of cellular differentiation: From hypotheses to
models. Trends Ecol. Evol. 36, 49–60 (2021).
29. K. J. Niklas, S. A. Newman, The many roads to and from multicellularity. J. Exp. Bot.
71, 3247–3253 (2020).
30. J. Kramer, J. Meunier, Kin and multilevel selection in social evolution: A never-
ending controversy? F1000 Res. 5, 5 (2016).
31. S. F. Edwards, P. W. Anderson, Theory of spin glasses. J. Phys. F Met. Phys. 5, 965–974
(1975).
EVOLUTION
Vanchurin et al.
Toward a theory of evolution as multilevel learning
PNAS
j
11 of 12
https://doi.org/10.1073/pnas.2120037119
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

32. K. H. Fischer, J. A. Hertz, Spin Glasses (Cambridge University Press, Cambridge,
United Kingdom, 1993).
33. T. C. McLeish, Are there ergodic limits to evolution? Ergodic exploration of genome
space and convergence. Interface Focus 5, 20150041 (2015).
34. E. Schroedinger, What is Life? The Physical Aspect of the Living Cell (Trinity College
Press, Dublin, Ireland, 1944).
35. V. Vanchurin, Towards a theory of machine learning. Mach. Learn. Sci. Technol. 2,
035012 (2021).
36. V. Vanchurin, The world as a neural network. Entropy (Basel) 22, E1210 (2020).
37. L. Valiant, Probably Approximately Correct: Nature’s Algorithms for Learning and
Prospering in a Complex World (Basic Books, New York, NY, ed. 1, 2013).
38. R. A. Watson, E. Szathmary, How can evolution learn? Trends Ecol. Evol. 31, 147–157
(2016).
39. G. Benfatto, G. Gallavotti, Renormalization Group (Physics Notes Book 1) (Princeton
University Press, Princeton, NJ, 2020).
40. N. Goldenfeld, Lectures on Phase Transitions and the Renormalization Group (Fron-
tiers in Physics) (Addison-Wesley, New York, NY, 1972).
41. M. I. Katsnelson, Y. I. Wolf, E. V. Koonin, Towards physical principles of biological
evolution. Phys. Scr. 93, 043001 (2018).
42. R. B. Laughlin, D. Pines, The theory of everything. Proc. Natl. Acad. Sci. U.S.A. 97,
28–31 (2000).
43. R. B. Laughlin, D. Pines, J. Schmalian, B. P. Stojkovic, P. Wolynes, The middle way.
Proc. Natl. Acad. Sci. U.S.A. 97, 32–37 (2000).
44. C. Darwin, On the Origin of Species (A. F. Murray, London, United Kingdom, 1859).
45. T. Dobzhansky, Genetics and the Origin of Species (Columbia University Press, New
York, NY, ed. 2, 1951).
46. W. F. Doolittle, S. A. Inkpen, Processes and patterns of interaction as units of selec-
tion: An introduction to ITSNTS thinking. Proc. Natl. Acad. Sci. U.S.A. 115,
4006–4014 (2018).
47. S. Lem, Solaris (Pro Auctore Wojciech Zemek, ed. 2, 2014).
48. D. E. Shelton, R. E. Michod, Group and individual selection during evolutionary tran-
sitions in individuality: Meanings and partitions. Philos. Trans. R. Soc. Lond. B Biol.
Sci. 375, 20190364 (2020).
49. Y. Bakhtin, M. I. Katsnelson, Y. I. Wolf, E. V. Koonin, Evolution in the weak-
mutation limit: Stasis periods punctuated by fast transitions between saddle
points on the ﬁtness landscape. Proc. Natl. Acad. Sci. U.S.A. 118, e2015665118
(2021).
50. E. V. Koonin, Y. I. Wolf, Constraints and plasticity in genome and molecular-
phenome evolution. Nat. Rev. Genet. 11, 487–498 (2010).
51. S. Wright, Adaptation and Selection. Genetics, Paleontology and Evolution (Prince-
ton University Press, Princeton, NJ, 1949).
52. M. Lynch et al., Genetic drift, selection and the evolution of the mutation rate. Nat.
Rev. Genet. 17, 704–714 (2016).
53. S. Gavrilets, Fitness Landscapes and the Origin of Species (Princeton University Press,
Princeton, NJ, 2004).
54. E. Svensson, R. Calsbeek, The Adaptive Landscape in Evolutionary Biology (Oxford
University Press, Oxford, United Kingdom, 2012).
55. G. F. Joyce, The antiquity of RNA-based evolution. Nature 418, 214–221 (2002).
56. G. F. Joyce, J. W. Szostak, Protocells and RNA self-replication. Cold Spring Harb. Per-
spect. Biol. 10, a034801 (2018).
57. E. Szathmary, The evolution of replicators. Philos. Trans. R. Soc. Lond. B Biol. Sci.
355, 1669–1676 (2000).
58. E. Szathmary, J. Maynard Smith, From replicators to reproducers: The ﬁrst major
transitions leading to life. J. Theor. Biol. 187, 555–571 (1997).
59. P. Adamski et al., From self-replication to replicator systems en route to de novo
life. Nat. Rev. Chem. 4, 386–403 (2020).
60. F. Crick, Central dogma of molecular biology. Nature 227, 561–563 (1970).
61. M. Davis, The Universal Computer: The Road from Leibniz to Turing (W. W. Norton
& Co., New York, NY, 2010).
62. J. Von Neumann, First draft of a report on the EDVAC (1945). https://web.archive.
org/web/20130314123032/qss.stanford.edu/∼godfrey/vonNeumann/vnedvac.pdf.
Accessed 27 September 2021.
63. A. Kun, M. Santos, E. Szathmary, Real ribozymes suggest a relaxed error threshold.
Nat. Genet. 37, 1008–1011 (2005).
64. C. Woese, The universal ancestor. Proc. Natl. Acad. Sci. U.S.A. 95, 6854–6859 (1998).
65. C. R. Woese, On the evolution of cells. Proc. Natl. Acad. Sci. U.S.A. 99, 8742–8747
(2002).
66. J. M. Smith, Natural selection and the concept of a protein space. Nature 225,
563–564 (1970).
67. J. Iranzo, J. A. Cuesta, S. Manrubia, M. I. Katsnelson, E. V. Koonin, Disentangling the
effects of selection and loss bias on gene dynamics. Proc. Natl. Acad. Sci. U.S.A. 114,
E5616–E5624 (2017).
68. L. Sagan, On the origin of mitosing cells. J. Theor. Biol. 14, 255–274 (1967).
69. T. M. Embley, W. Martin, Eukaryotic evolution, changes and challenges. Nature 440,
623–630 (2006).
70. J. Iranzo, A. E. Lobkovsky, Y. I. Wolf, E. V. Koonin, Virus-host arms race at the joint ori-
gin of multicellularity and programmed cell death. Cell Cycle 13, 3083–3088 (2014).
71. A. M. Nedelcu, W. W. Driscoll, P. M. Durand, M. D. Herron, A. Rashidi, On the para-
digm of altruistic suicide in the unicellular world. Evolution 65, 3–20 (2011).
72. P. M. Durand, The Evolutionary Origins of Life and Death (University of Chicago
Press, Chicago, IL, 2021).
73. E. V. Koonin, L. Aravind, Origin and evolution of eukaryotic apoptosis: The bacterial
connection. Cell Death Differ. 9, 394–404 (2002).
74. S. S. Haykin, Neural Networks: A Comprehensive Foundation (Prentice Hall, New
York, NY, 1999).
75. A. I. Galushkin, Neural Networks Theory (Springer, New York, NY, 2007).
76. M. Kimura, The Neutral Theory of Molecular Evolution (Cambridge University Press,
Cambridge, United Kingdom, 1983).
77. M. Nei, Selectionism and neutralism in molecular evolution. Mol. Biol. Evol. 22,
2318–2342 (2005).
78. G. Reddy, M. M. Desai, Global epistasis emerges from a generic model of a complex
trait. eLife 10, e64740 (2021).
79. Z. R. Sailer, M. J. Harms, Molecular ensembles make evolution unpredictable. Proc.
Natl. Acad. Sci. U.S.A. 114, 11938–11943 (2017).
80. A. Wagner, Neutralism and selectionism: A network-based reconciliation. Nat. Rev.
Genet. 9, 965–974 (2008).
81. J. H. Miller, The Operon (Cold Spring Harbor Laboratory Press, Cold Spring Harbor,
NY, 1980).
82. C. Mejıa-Almonte et al., Redeﬁning fundamental concepts of transcription initia-
tion in bacteria. Nat. Rev. Genet. 21, 699–714 (2020).
83. M. Lynch, Evolution of the mutation rate. Trends Genet. 26, 345–352 (2010).
84. K. Friston, J. Kilner, L. Harrison, A free energy principle for the brain. J. Physiol. Paris
100, 70–87 (2006).
85. J. F. Crow, M. Kimura, Introduction to Population Genetics Theory (Harper and Row,
New York, NY, 1970).
86. V. Vanchurin, Y. I. Wolf, E. V. Koonin, M. I. Katsnelson, Thermodynamics of evolu-
tion and the origin of life. Proc. Natl. Acad. Sci. U.S.A. 119, 10.1073/pnas.
2120042119 (2022).
87. M. I. Katsnelson, V. Vanchurin, T. Westerhout, Self-organized criticality in neural
networks. arXiv [Preprint] (2021). https://arxiv.org/abs/2107.03402 (Accessed 22 Jan-
uary 2022).
88. M. I. Katsnelson, V. Vanchurin, Quantumness in neural networks. Found. Phys. 51,
94 (2021).
89. S. M. Carroll, Spacetime and Geometry: An Introduction to General Relativity (Addi-
son-Wesley, San Francisco, CA, 2004).
90. A. A. Bagrov, I. A. Iakovlev, A. A. Iliasov, M. I. Katsnelson, V. V. Mazurenko, Multi-
scale structural complexity of natural patterns. Proc. Natl. Acad. Sci. U.S.A. 117,
30241–30251 (2020).
91. H. Haken, Synergetics, an Introduction: Nonequilibrium Phase Transitions and Self-
Organization in Physics, Chemistry, and Biology (Springer, New York, NY, 1983).
92. H. Haken, Advanced Synergetics: Instability Hierarchies of Self-Organizing Systems
and Devices (Springer, New York, NY, 1994).
93. I. R. Prigogine, I. Stengers, Order out of Chaos (Bantam, London, United Kingdom, 1984).
94. M. Mezard, G. Parisi, M. A. Virasoro, Eds., Spin Glass Theory and Beyond (World Sci-
entiﬁc, Singapore, 1987).
95. A. Kolmus, M. I. Katsnelson, A. A. Khajetoorians, H. J. Kappen, Atom-by-atom construc-
tion of attractors in a tunable ﬁnite size spin array. New J. Phys. 22, 023038 (2020).
96. P. Bak, How Nature Works. The Science of Self-Organized Criticality (Springer, New
York, NY, 1996).
97. P. Bak, C. Tang, K. Wiesenfeld, Self-organized criticality: An explanation of the 1/f
noise. Phys. Rev. Lett. 59, 381–384 (1987).
98. T. Dobzhansky, Nothing in biology makes sense except in the light of evolution.
Am. Biol. Teach. 35, 125–129 (1973).
99. S. D. Copley, E. Smith, H. J. Morowitz, The origin of the RNA world: Co-evolution of
genes and metabolism. Bioorg. Chem. 35, 430–443 (2007).
100. A. Kahana, D. Lancet, Self-reproducing catalytic micelles as nanoscopic protocell
precursors. Nat. Rev. Chem. 5, 870–878 (2021).
101. K. Vetsigian, C. Woese, N. Goldenfeld, Collective evolution and the genetic code.
Proc. Natl. Acad. Sci. U.S.A. 103, 10696–10701 (2006).
102. T. Ganti, Chemoton Theory: Theory of Living Systems (Kluwer Academic, Plenum,
New York, NY, 2004).
103. E. Szathmary, Life: In search of the simplest cell. Nature 433, 469–470 (2005).
104. I. Zachar, A. Fedor, E. Szathmary, Two different template replicators coexisting in
the same protocell: Stochastic simulation of an extended chemoton model. PLoS
One 6, e21380 (2011).
105. R. H. Dicke, Dirac’s cosmology and Mach’s principle. Nature 192, 440–441 (1961).
12 of 12
j
PNAS
Vanchurin et al.
https://doi.org/10.1073/pnas.2120037119
Toward a theory of evolution as multilevel learning
Downloaded from https://www.pnas.org by 159.2.199.174 on January 3, 2023 from IP address 159.2.199.174.

