Steganographic Generative Adversarial Networks
Denis Volkhonskiy1, Ivan Nazarov1, Evgeny Burnaev1
1Skolkovo Institute of Science and Technology
Nobel street, 3, Moscow, Moskovskaya oblast’, Russia
e-mail: e.burnaev@skoltech.ru
ABSTRACT
Steganography is collection of methods to hide secret information (“payload”) within non-secret information “con-
tainer”). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recov-
ering it if possible. Presence of hidden payloads is typically detected by a binary classiﬁer. In the present study, we
propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks
(DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography
algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for
this reason, can be used in steganographic applications.
Keywords: generative adversarial networks, steganography, security
1. INTRODUCTION
Recent years have seen signiﬁcant advances in estimation methods and application of deep generative models. There
are two major general frameworks for learning deep generative models: Variational Autoencoders (VAEs), [17], and
Generative Adversarial Networks (GANs), [7]. The recent work of Hu et al. [11] develops a unifying framework, which
establishes strong connections of these approaches to Adversarial Domain Adaptation (ADA), [5].
GANs have achieved impressive results in semi-supervised learning, [16], and image-to-image translation, [13]. In [7]
the success of GANs framework was illustrated on the problem of image generation. A more recent paper [29] proposed
as set of constraints on the architecture of convolutional GANs and showed that thus restricted deep convolutional GANs
(DCGANs) are capable of learning a hierarchy of representations from object parts to scenes, which are sufﬁciently robust
to transfer across domains.
In this study we apply the DCGAN framework to the domain of steganography, i.e. practical approaches to concealing
information (payload) within another piece of information (stego-container). In particular, we train a generator, images pro-
duced by which are less susceptible to steganographic analysis compared to the original images used as stego-containers.
At the same time, we require that the induced distribution of the synthetic images approximate well the distribution of the
real images in the dataset.
Thus we train a generative model for image stego-containers by confronting it with two deep convolutional adversaries:
a discriminator network, which regularizes the output to look like samples from the real dataset, and a steganographic an-
alyzer, which aims at detecting if an image conceals a hidden message. The presence of two regularizers in the generator’s
objective resembles the recently proposed multi-target GAN framework [2].
2. STEGANOGRAPHY
Steganography is a set of algorithms for concealing information in inconspicuous-looking communication and a col-
lection of methods to detect and recover the hidden message from suspicious media (Steganalysis). In steganography the
information to be hidden, the payload, is embedded by an algorithm inside a cover medium, the stego-container. The
key drawback is that steganography offers security through obscurity: an embedded message is sent in the hope that a
third party won’t detect or discover it. This makes pure steganography impractical without cryptography, which deals with
secure communication over an insecure channel: the message is scrambled and authenticated with some keyed algorithm
before being concealed in a cover medium, [1, 21]. In this respect steganography serves as a layer of weak security by
adding a cyphertext detection and extraction step: encrypted data has much higher entropy than the regular data. Besides
information protection and covert communication, steganography is useful for watermarking in digital rights management
and user identiﬁcation.
The simplest and most popular algorithm of unkeyed stego-embedding is called the Least Signiﬁcant Bit (LSB) match-
ing. The main idea is to take the binary representation of a secret message, pad it, and store it in a stego-container by
overwriting the LSB of each byte within. The cover-media used for the LSB embedding must be resilient with respect to
arXiv:1703.05502v2  [cs.MM]  7 Oct 2019

bit-level augmentation. In the case of images the least signiﬁcant bits of each colour channel of each pixel in the given
image are used to hide the payload.
The perturbations introduced by the LSB algorithm do not preserve marginal or joint colour statistics, which despite
being imperceptible to a human observer, simplify detection of the hidden payload with machine learning or statistical
models. A modiﬁcation of this method, which addresses this issue to some extent, is the so-called ±1-embedding: each
bit of the message is hidden by randomly adding or subtracting 1 from a pixel’s colour channel so that the last bit matches
it.
More sophisticated steganographic schemes modify the digital media adaptively. The key idea is to constrain the
embedding to regions of high local entropy, e.g. complex textures or noise. Each pixel is assigned an embedding cost and
the embedding locations are picked in such a way as to minimize the distortion function
D(I, ˆI) = ∑
i,j
ρij(I, ˆIij),
where I is the cover image, ˆI is the stego-embedding, and ρij(I, ˆIij) is the bounded cost of altering a pixel (i, j) in the
cover-image I. The embedding itself is performed by coding methods such as Syndrome-Trellis Codes (STC) [3], which
are essentially binary linear convolutional codes represented by parity-check matrix. The state-of-the-art content-adaptive
stego-embedding algorithms include HUGO [24], which computes the embedding costs based on Subtractive Pixel Adja-
cency Matrix (SPAM) features [23]; WOW [9] and S-UNIWARD [10], which use directional wavelet ﬁlters to weigh and
pick regions with high entropy, but implement different embedding cost functional.
2.1 Steganalysis
The simplest approach to steganalysis is based on special feature extractors, e.g. SPAM [23], SRM [4], combined
with traditional machine learning models, such as support vector classiﬁers, decision trees, classiﬁer ensembles, et c. With
the recent overwhelming success of deep learning, speciﬁcally in the image classiﬁcation and generation domain, newer
approaches based on deep Convolutional Neural Networks (deep CNN) are gaining popularity. For example, in [27] it is
shown that deep CNN with Gaussian activation functions achieve competitive performance with hand-crafted features, and
in [25] it is demonstrated that even shallow CNN are able to outperform the usual ML based stego-analysis techniques in
terms of the detection accuracy.
In this paper we consider steganographic embedding of random bit messages into speciﬁcally crafted images using
the ±1-embedding algorithm. The security of the stego-containers is tested against a class of deep convolutional neural
network stego-analyzers, which try to distinguish images with hidden data from the empty ones.
2.2 Problem Statement
The total scheme of steganography and steganalysis is presented at Fig. 1:
• Usually all images are attacked by a stegoanalyser (Eve);
• Alice (Steganography algorithm) tries to deceive Eve;
Figure 1: Complete scheme of steganography and steganalysis
The disadvantage of the standard steganography approach is inadaptability of the containers and algorithms for Eve.
By this we mean that containers don’t adopt (and even know) for type of Eve. The goal for this work is to create adaptive
containers generator and steganography new method.

2.3 Tasks for the research
We would like to obtain adaptability of the containers to the given Steganalysis in order to deceive it. We set the
following tasks for the current work:
1. Adaptive containers generation.
• Create a model for image containers generation, that can be used with any Steganography algorithm;
• Using of these generated containers should deceive Eve (steganalysis);
• Containers should be adaptive to any type on Eve.
2. New Steganography method:
• Create a model for adaptive generation of images with hidden information inside;
• Test the quality of encryption-decryption on MNIST and CIFAR-10 datasets
The difference between these two tasks is the following.
In the ﬁrst model, we would like to build a generator of empty containers (images). This images could be used with
any Steganography algorithm.
In the second model, we would like to generate not only empty images, but to encode the information to them for
further extraction. In other words, we would like to obtain an analogy of visual markers (such as QR-codes).
3. GENERATIVE ADVERSARIAL NETWORKS
Generative Adversarial Networks (GANs) training, [7], is a powerful framework for estimating generative models in
unsupervised learning setting by way of a two-player minimax game. The generator player attempts to mimic the true data
distribution pdata(x) by learning a transformation function z 7→Gθ(z) of random input values z, drawn from a tractable
distribution pz. The generator receives feedback from the discriminator Dφ, which strives to distinguish synthetic, “fake”
samples x = G(z), z ∼pz, from genuine, “real” samples x ∼pdata.
In the original formulation, [7], the learning process of the generator (G) and the discriminator (D) consists of searching
for a saddle point solution of the following optimization problem
min
θ max
φ
L (θ,φ) = Ex∼pdata [logD(x;φ)]+Ez∼pz [log(1−D(G(z;θ);φ))] ,
(1)
where D(x;φ) is the probability output by the player D that x is a real sample rather then synthetic, and G(z;θ) is the
generated sample. In a typical application the ground truth data distribution is provided implicitly through its ﬁnite sample
approximation on the dataset D = (xi)m
i=1. Furthermore, the expectations in the are approximated by sample averages over
randomly drawn mini-batches of (G(zi;θ))B
i=1 for (zi)B
i=1 ∼pz i.i.d. and (xi)B
i=1 sampled without replacement from the
training set.
Despite many advantages, such as generation with a single forward pass and asymptotically consistent data distribution
estimation, the major disadvantage of GANs is that training them requires ﬁnding a Nash (best response) equilibrium, [6].
Furthermore, since for deep networks G(·;θ) and D(·;φ) the objective is non-convex w.r.t. the parameters φ and θ, the
order of min and max in (1) matters. Therefore [7] propose to solve the problem by iteratively alternating between SG
maximization and minimization steps, but giving optimizational advantage to the discriminator player. The idea is to make
several gradient ascent steps on L (θ,φ) w.r.t. φ, before a single gradient descent step w.r.t. θ.
By giving advantage to the discriminator, the proposed approach attempts to approximate what in fact is the genera-
tor’s true objective: θ 7→maxφ L (θ,φ). However, since in practice may GANs are trained with alternating single-step
updates, [22] propose and justify a simpler joint single-step gradient method: the SGD update moves along the joint di-
rection (∇θL ,−∇φL ) obtained via a single back-propagation step. However, there is no consensus as to what the best
training scheme for solving (1) is, [6].
GAN training is also complicated by the different regimes the networks undergo in the process. For instance, during
early stage of training the discriminator is prone to becoming excessively powerful, which makes the last term of (1)
provide weak feedback to the generator. In [7] authors suggest to use LGen(θ;φ) = −Ez∼pz [logD(G(z;θ);φ)] as the
minimization objective of the generator, and to set −L (θ,φ) as the discriminator’s loss, LDis(φ;θ), to be minimized.
In spite of changing the loss and making the game no longer zero-sum, this heuristic leads to the same saddle point as
demonstrated in [22].
In ﬁg. 2 we depict a sample of synthetic images of a freshly trained DCGAN on the Celebrities dataset [20]. The
images indeed look realistic, albeit with occasional artifacts.

Figure 2: Sample synthetic images generated by DCGAN
4. STEGANOGRAPHIC GENERATIVE ADVERSARIAL NETWORKS
4.1 Model description
Let I = [−1,1]H×W×3 be the space of images with dimensions H ×W and RGB channel saturation values between
−1 and 1. Let pz be a uniform distribution on Z = [−1,1]dZ, and pdata be the distribution of reference images on a subset
of I . Finally, the set of messages M is given by {0,1}dM, dM ≤HW, and by Sm : I 7→I we denote the LSB embedding
of the message m ∈M in the cover image x ∈I .
We introduce Steganographic Generative Adversarial Networks model (SGAN), which is a zero-sum game between
two players: a generator network G : Z 7→I , which tries to mimic pdata, and an adversary consisting of two parts
• a discriminator network D : I 7→[0,1], which distinguishes synthetic images x = G(z), z ∼pz, from real x ∼pdata;
• a steganalyzer network A : I 7→[0,1], which tries to separate cover images Sm(x) with payload m ∈M , from empty
images x, x ∼Q, for some image distribution Q on I .
The value function of the game is ﬁg. 3
L (θ,φ,ψ) = αLDis(φ;θ)+(1−α)LSan(ψ;θ) →min
θ max
φ,ψ ,
(2)
LDis(φ;θ) = Ex∼pdata [logD(x;φ)]+Ez∼pz [log(1−D(G(z;θ);φ))] ,
(3)
LSan(ψ;θ) = Ez∼pz [Em [logA(Sm(G(z;θ));ψ)]]+Ez∼pz [log(1−A(G(z;θ);ψ))] .
(4)
Here the discriminator and the steganalyzer maximize the likelihood LDis and LSan, respectively, while the generator
minimizes the convex combination of likelihoods of D and A in (2). The mixing parameter α ∈(0,1) controls the trade-off
between the importance of realism of generated images and their quality as containers against the steganalysis. Analysis
of preliminary experimental results showed that for α ≤0.5 the generated fails to approximate the distribution of the
reference images.
This model resembles the recently proposed multi-target GANs framework, [2], which pits the generator against mul-
tiple discriminators: a boosted ensemble, a mean aggregated combination of discriminators, or discriminators with adap-
tively adjustable power. The main idea of the paper naturally suggests that additional discriminators can be used for
regularization, adaptation of the generator’s output to other domains, or endowing the synthetic samples with certain
required properties.
The we propose to train the model with the joint simultaneous gradient update scheme proposed in [22]: jointly
backpropagate through the networks and update

• for D with φ ←φ +γD∇φLDis;
• for A with ψ ←ψ +γA∇ψLSan;
• for G with θ ←θ −γG∇θL , where L is as in (2).
The expectations are substituted by the empirical averages over the joint mini-batch of images x ∼pdata, noise z ∼pz, and
messages m ∼{0,1}dM.
It is expected that the SGAN game would, for suitable class of deep CNN and an appropriate training schedule, yield
an equilibrium in which the generator produces realistic images capable of concealing messages embedded by LSB against
a deep convolutional steganalyzer. As is in the case of the original formulation [7], if the players were not conﬁned to the
class of deep convolutional networks, the optimal steganalyzer would have been given by the ratio estimator between the
distribution of the images q, induced by G(z;θ) for z ∼pz, and the distribution qs, implicitly deﬁned as the distribution
of Sm(x) over x ∼q and m ∼{0,1}dM. We expect that the optimal generator would induce a distribution, the variates
from which would have uniformly random least signiﬁcant bit in the colour data of each pixel, since for a random m the
stego-embedding Sm(x) alters x only on the scale of 2−7, and the result is, essentially, a random bit.
4.2 Challenges
One of the challenges in the proposed model is the fact that any stego-embedding algorithm introduces a distortion
to the cover medium, which is generally a non-differentiable perturbation as a function of the data in the medium. For
instance, the LSB matching modiﬁes the colour channel data of the pixels independently, and thus can be represented
as a residual-like transformation of each colour channel value: Sm(x) = x + δm(x), for any message bit m ∈{0,1} and
x ∈[−1,+1], where δm(x) ∈{0,±ε}, ε = 2−7, is the distortion. The δm(x) is a random function given by
δm(x) = ξ pm
 (x+1)ε−1
,
(5)
where pm(z) is 0 when ∃k ∈Z : z−m ∈[2k,2k +1), i.e. the LSB of the integer part of z matches m, and 1 otherwise. The
value ξ is +ε if m = 1 and x ∈[−1,−1+ε), and −ε if m = 0 and x ∈(+1−ε,+1], but otherwise an independent random
variable ±ε, which is the addition / subtraction mask in the LSB algorithm.
The key observation is that for ﬁxed m and ξ this perturbation is fully determined by pm(z), which is constant on
the intervals (k,k + 1) with k ∈{0...255}. While this function is non-differentiable at ﬁnitely many points, where the
0 −1 switching occurs, everywhere else in (0,256) it is constant and has zero derivative w.r.t. z. Thus for ﬁxed m and
ξ the distortion δm(x) has derivative zero almost for every x ∈(−1,1). In light of this heuristic argument, the following
procedure for stego-embedding can be used while training: during the forward pass the embedding x 7→Sm(x) is exact, but
during the back propagation the embedding response is approximate by an identity function Sm(x) ≈x.
The main drawback of this “linear” approximation is that it provides essentially no gradient feedback to the generator
and acts as if nothing was embedded. Thus, although being almost correct it is ill suited for the purpose of learning
stego-secure cover entities by design. Therefore we propose another approach: for a ﬁxed message m ∈{0,1} and noise
ξ ∈{±1} approximate the LSB embedding layer by a differentiable transformation. To this end we substitute the mismatch
indicator in (5) by a sine waveform with nonlinear gain at extremes of its range:
pm(z) ≈sβ(z;m) = σ
 β sin(m−z)π

,
(6)
where β > 0 determines the ﬁdelity of the approximation and σ(a) is the Sigmoid function, a 7→(1+e−a)−1. The
derivative of the Sigmoid function is a 7→σ(a)(1 −σ(a)), and therefore this approximation is not computationally too
demanding and provides accurate gradient feedback at the jump points of pm(z).
Figure 3: SGAN information ﬂow diagram

4.3 Training process
Stochastic mini-batch Gradient descent update rules for components of SGAN are listed below:
• for D the rule is θD ←θD +γD∇GL with
∇GL =
∂
∂θD
n
Ex∼pdata(x) [logD(x,θD)]+Ez∼pnoise(z) [log(1−D(G(z,θG),θD))]
o
;
• for S (it is updated similarly to D): θS ←θS +γS∇SL where
∇SL =
∂
∂θS
Ez∼pnoise(z) [logS(Stego(G(z,θG)),θS)+log(1−S(G(z,θG),θS))] ;
• for the generator G: θG ←θG −γG∇GL with ∇GL given by
∇GL =
∂
∂θG
αEz∼pnoise(z) [log(1−D(G(z,θG),θD))]+
∂
∂θG
(1−α)Ez∼pnoise(z) [log(S(Stego(G(z,θG),θS)))]
+
∂
∂θG
(1−α)Ez∼pnoise(z) [log(1−S(G(z,θG),θS))] .
The main distinction from the GAN model is that we update G in order to maximize not only the error of D, but to
maximize the error of the linear combination of the classiﬁers D and S.
5. STEGANOGRAPHIC ENCRYPTION GENERATIVE ADVERSARIAL NETWORKS
5.1 Model Description
Steganographic Encryption Generative Adversarial Networks (SEGAN) model was constructed for information en-
cryption/ description purposes. It consists of
• Alice — A generator network: produce realistic images, that contains hidden information.
– Input: Secret Key (binary), Secret Message (binary), Class for generation (y), noise
– Output: Image with hidden secret message inside
• Bob — A decryption network: extract hidden message from the image.
– Input: Image, Secret Key, Class of the image (y)
– Output: Secret hidden message
• Discriminator — A discriminator, that tries to detect whether the image is real or generated.
– Input: Image, Class of the image (y)
– Output: 0/1 – generated/real class
The full scheme is presented in Fig. 4. This model can be considered as a autoencoder with the high-dimensional
hidden representation. In such case Discriminator is considered as such kind of regularization.
5.2 Training process
The training process can be represented as a usual GAN training process, but with some modiﬁcation.
Firstly we should deﬁne the loss functions for constructing the updating rules:
• Alice (as realistic image Generator) loss. Standard GANs generator loss.
LA =EyEm∼pmessageEk∼pkeyEz∼pnoise[log(1−C(θC,(θA,m,k,z,y),y)) | y,m,k,z]
(7)
• Alice-Bob encryption loss. Here we compute standard l2 loss between the original message and reconstructed
message.
LAB =EyEm∼pmessageEk∼pkeyEz∼pnoiseE[(m−B(θB,A(θA,m,k,z,y),k,y))2 | y,m,k,z] →min
(8)

Figure 4: SEGAN information ﬂow diagram
Input : dk — The dimension of a binary key
dm — The dimension of a binary message,
dn — The dimension of the input noise
for epoch in 1...n_epoch do
for minibatch from the data set do
Sample minibatch of dn noise samples {z1,...,zdn}
Sample minibatch of dm noise samples {m1,...,mdm}
Sample minibatch of dk noise samples {k1,...,kdk}
Update A according to loss LA
Update C according to loss LC
if epoch > 1 then
Update A, B according to loss LAB
end
end
end
Algorithm 1: Algorithm of training SEGAN
• Discriminator’s loss. This is a usual GANs discriminator loss, witch is calculated as an average cross-entropy.
LC = EyEx∼pdata [log(C(θC,x,y)) | y]−
−EyEm∼pmessageEk∼pkeyEz∼pnoise[log(1−C(θC,A(θA,m,k,z,y),y)) | y,m,k,z] →min
(9)
The total SEGAN train procedure is presented in Algorithm 1.
6. EXPERIMENTS WITH STEGANOGRAPHIC GENERATIVE ADVERSARIAL NETWORKS
6.1 Steganographic Vectors
Before conducting extensive numerical experiments on the security of the LSB embedding in cover images produced
by a SGAN trained generator, we run a simpler experiment as a proof-of-concept. Since the LSB matching embeds each
bit of the message into a pixel independently from the its context, we study numerically the stego-security properties of
SGAN generated 1-d vectors.
6.1.1
Validation Protocol
In the process of training SGAN for T iterations we obtain a sequence of generators (Gt)T
t=1 where Gt(·) = G(·;θt) and
θt are the parameters of the generator after t minibatch SGD updates. We use the following empirical validation protocol
for the generator after the t-th iteration:
1. Draw a sample S t = (xi,yi)M
i=1, i.e. for i = 1,...M

(a) independently draw yi ∼{0,1} and zi ∼pz;
(b) get a message mi ∈M = {0,1}dM;
(c) synthesize a cover entity x∗
i = Gt(zi) and set xi =
(
Smi(x∗
i ),
if yi = 1,
x∗
i ,
if yi = 0.
2. Assess the performance of an independent steganalyzer A∗with K-fold cross validation on S t.
This sampling procedure ensures that the examples in the stego-sample S t are independent and identically distributed.
We also control the diversity of the embedded messages mi through different message generation scenarios:
• Fixed: mi = m0, with m0 picked once from M ;
• Pool (n): random mi from M0 ⊂M , |M0| = n;
• Arbitrary: random mi from M .
Under the “Fixed” and “Pool (n)” variants m0 and M0, respectively, are chosen independently for each sample S t in
the above outline. The motivation behind controlling the diversity stems from the idea that by comparing the optimal
performance of A∗under each scenario it is possible to verify if Sm(G(z)) ≡G(z) in distribution for independent z ∼pz
and m ∼M . In fact, with SGAN it is possible to train a generator, that induces an distribution, which is invariant under
LSB embedding of messages from a speciﬁc, non-uniform distribution of messages M just by sampling from it during
training.
We use this validation protocol for the experiments in sec. 6.1. Howeer, for the experiments with image generation (
sec. 6.2) we extend the protocol, since in this case we have a reference distribution Q on I , which the generator G aims
to replicate.
The extension is illustrated in ﬁg. 5. Firstly, several independent real and synthetic training datasets are generated:
1. Draw a synthetic stego-sample SS as outlined above;
2. Generate similarly a real stego-sample SR by sampling x∗
i from Q (sample without replacement if Q is an empirical
distribution);
Secondly, on each of these datasets we train an independent steganalyzer A∗and aggregate them with weighted majority
voting to obtain the ﬁnal pair of steganalyzers A∗
S and A∗
R, where the former is based on synthetic training datasets and the
latter – on real stego-samples. Finally, we independently generate several real and synthetic datasets as outlined above,
and used them for cross-validation of the steganalyzers: each one is validated on both kinds of test samples to assess if and
how well the learned features transfer across real / synthetic domains.
Figure 5: Extended validation protocol for experiments in sec. 6.1.
6.1.2
Details of the Experiment
The generator G is a 1-d convolutional neural network (CNN) that expands the input z ∼NdI(0,IdI) from RdI into
[−1,+1]dO, where dI = 4 and dO = 16. The generator uses a series of 1-d transposed convolution layers with nonunit
strides and the ReLU nonliearity, a 7→max{a,0}, to upsample the input noise into a intermediate 32 × dO state matrix,
which is ﬁnally passed through a 1-d convolution layer with Tanh nonlinearity, a 7→ea−e−a
ea+e−a , to ensure [−1,+1]dO output.
The steganalyzer A is also a 1-d CNN which takes a sample x ∈[−1,+1]dO and feeds the input through a set of 1-d
convolution layers with Leaky ReLU, a 7→max{αa,a} for α = 1
20 to get the 256×4 matrix of intermediate features. The
matrix is passed through two fully connected layers with the Leaky ReLU and 512 units each. The ﬁnal layer of A outputs
a single logit score, which reﬂects the degree to which the analyzer is “conﬁdent” that the sample x contains a message

hidden with the LSB embedding. This network A, as well as G, has a Batch Normalization (BN) layer, [12], before the
nonlinearity, except for the output layers.
The losses are set-up as in (2) and (4), but without an explicit “discriminator” player. We simulate its feedback by
imposing a total variation with l2 loss on the generator’s output:
LDis(θ) = Ez∼pz
" dO
∑
i=1
Ctv
xi −xi−1
+Cl2
1
2x2
i
#x=G(z;θ)
x0=xdO
.
The relative weight α in the generator’s loss is set to α =
Csan
Csan+Ctv+Cl2 for Csan = 2, Ctv = 10−2, and Cl2 = 10−1.
We train SGAN generator for T = 75 · 103 minibatch iterations of 256 pairs of noise and message vectors. We set
dM = dO and thus embed 1 bit of message in each element of the cover vector, achieving the ﬂat vector analogue of
image’s 1 bit-per-pixel embedding density.
SGAN networks are trained with Adam, [14], with default parameter values and learning rates 2−14 and 2−18 for the
steganalyzer (A) and the generator (G), respectively. We use different learning rates to make the generator adapt slowly
to the steganalyzer’s feedback, while allowing the analyzer to learn to detect hidden data in the generator’s output more
quickly, [8]. The training was carried out with the strategy proposed in [7]: 4 updates of A per 1 update of G. The scale
parameter β of (6) was set to 8.
We run the validation protocol with the sample size M = 216 and the number of folds K set to 7. In this setting we
validate two different independent steganalyzers A∗: a Random Forest classiﬁer ensemble with 101 trees and a 1-d CNN
similar to the network used during SGAN training. The only difference of the CNN A∗is that it yields two values and
passes them through the Softmax nonlinearity, (a,b) 7→
 1
1+eb−a ,
1
1+ea−b

. The convolutional A∗is trained using Adam
SGD updates with learning rate 2−14 and weight decay of 2−3 (l2 parameter regularizer).
To simplify the task of the independent steganalyzer A∗we use hand-crafted features instead of raw input vectors.
These features are derived with the 8-bit reversal procedure and are strongly tailored to the LSB embedding. The main
idea is that through “reversal” of the bit order in each element these features amplify the signal from the stego-embedding,
which is naturally a weak high frequency distortion. Each value v ∈[−1,+1] in the input vector is transformed as follows:
• set u = (v+1)·ε for ε = 255
2 and let ˆu = ⌊u⌋;
• convert ˆu to an unsigned 8-bit value;
• produce 1
ε (rev( ˆu)+(u−ˆu))−1;
where rev(·) : {0,1}8 7→{0,1}8 reverses the bit order: rev
 (b0,b1,··· ,b7)

= (b7,b6,··· ,b0), with the bit at the ﬁrst
position in the sequence being the least signiﬁcant bit (the LSB bit endianness). With these features any change in the least
signiﬁcant bit would affect the whole value on the 27 scale instead of 20, and make more prominent the arithmetic effects
in the LSB matching due to ±1 addition.
6.1.3
Results
The performance of the Random Forest A∗on the output vectors of the generator after T iterations is shown in ta-
ble 1, of the 1-d CNN – in Table 2. Both tables were obtained via the validation protocol and the feature preprocessing
outlined above. It is worth noting that in both runs all metrics drop dramatically when the variety of embedded messages
is increased, which lends evidence to the invariance of the induced distribution under the LSB distortion. Numerical
experiments were also conducted on raw features, but both steganalyzers performed poorly.
We also experimented with the strength of the feedback to G from the steganalyzer A in SGAN training by setting
Ctv = 0 and changing Csan. The trained generator appears to be capable of successfully hiding message bits against the
random forest A∗even at the proportion Cl2 : Csan as low as 99 : 1. This can be attributed to simplicity of the generator’s
goal in this experiment. For Csan = 10−3 and Cl2 = 1 −Csan, however, the generator failed to produce LSB-invariant
output distribution even in the “Arbitrary” message generation scenario. This implies that the feedback provided by (4) is
relevant to the task of generating cover vectors, which make the LSB matching embedding less susceptible to steganalysis.
The ﬁgures 6 and 7 show the performance dynamics of the tested steganalyzers. The generator converges to the
desired output distribution is very quickly: in all SGAN re-runs with moderate to high values of Csan the steganalyzer
A∗fails to discriminate between empty and non-empty vectors after at most 20 of training. Note that in the ﬁgures the
proposed validation protocol was performed on each iteration during the ﬁrst 100 SGAN iterations, and only on every
100-th iteration onward.

Table 1: Performance metrics of the Random Forest A∗on the cover vectors generated after all iterations of training
(µ ±5σ scaled by 100).
metric
ROC-AUC
F1-score
Accuracy
OOB
type
Fixed
100.0±0.0
100.0±0.0
100.0±0.0
100.0
Pool (64)
99.9±0.1
99.9±0.2
99.9±0.2
99.9
Pool (256)
99.7±0.2
98.9±0.6
98.9±0.6
98.8
Pool (1024)
84.7±3.0
77.2±3.2
76.7±2.9
75.2
Pool (4096)
54.8±2.7
53.1±3.8
53.5±3.2
52.4
Arbitrary
49.6±3.0
50.4±2.7
49.8±2.2
49.9
Table 2: Performance metrics of the 1-d CNN A∗on the cover vectors generated after all iterations of training (µ ± 5σ
scaled by 100).
metric
ROC-AUC
F1-score
Accuracy
type
Fixed
100.0±0.0
99.9±0.1
99.9±0.1
Pool (64)
98.9±0.3
95.6±1.3
95.5±1.2
Pool (256)
92.1±1.7
84.7±2.6
84.5±2.3
Pool (1024)
70.0±4.5
64.9±5.5
64.6±3.7
Pool (4096)
56.6±2.5
55.1±4.8
54.8±2.8
Arbitrary
50.0±1.9
49.9±5.9
50.2±2.6
100
101
102
103
104
iteration
0.5
0.6
0.7
0.8
0.9
1.0
metric
ROC-AUC 
± 5
F1-score 
± 5
Figure 6: k-fold CV performance of A∗(Random Forest) on synthetic images produced by the generator at different stages
of training.
The main conclusion from this set of experiments is that the SGAN model (2) is useful in generating cover vectors, the
unconditional distribution of which is invariant under the LSB embedding distortion (5). Also the embedding approxima-
tion (6) is adequate and provides relevant gradient feedback. In the next section we experiment with generating realistic
cover images, which make the simple LSB embedding more steganographically secure.

100
101
102
103
104
iteration
0.5
0.6
0.7
0.8
0.9
1.0
metric
ROC-AUC 
± 5
F1-score 
± 5
Figure 7: k-fold CV performance of A∗(1-d CNN) on synthetic images generated at different stages during the SGAN
trainnig.
6.2 Steganographic Images
6.2.1
Data Description
In our experiments we use the Celebrities dataset [20] that contains 200000 images. All images were cropped to
64×64 pixels.
For steganalysis purposes we consider 10% of data as a test set. We denote the train set by A, the test set by B and
steganography algorithms used for hiding information by Stego(x). After embedding some secret information we get the
train set A +Stego(A) and the test set B +Stego(B), We end up with 380000 images for steganalysis training and 20000
for testing. For training the SGAN model we used all 200000 cropped images. After 8 epochs of training our SGAN
produces images displayed in ﬁg. 8.
For information embedding we use the ±1-embedding algorithm with a payload size equal to 0.4 bits per pixel for
only one channel out of three. As a text for embedding we use randomly selected excerpts from some article from The
New York Times.
6.2.2
Experimental Setup
In this section we describe the SGAN model structure. By C2D-BN-LR we denote the following structural block of a
convolutional neural network: Conv2d →Batch Normalization →Leaky ReLU.
The Steganalyser network S and the Image Discriminator network have similar structure: four C2D-BN-LR layers, then a
fully connected layer (1 neuron) →Sigmoid function is used to compute an output. The Image generator network G is (in
order) a fully-connected layer (8192 neurons), four C2D-BN-LR with Fractional-Strided convolution, then the Hyperbolic
tangent function layer is used to compute normalised output.
The SGAN model is trained to solve (2) using the Adam optimization algorithm [15] with the learning rate 2−4 and
update parameters β1 = 0.5 and β2 = 0.999. For each mini-batch of images we update weights of D and S once, then we
update weights of G twice.
In the following experiments, in addition to the steganalyser S we use an independent steganalyser S∗. We deﬁne a
ﬁlter F(0) that is special for steganalysis applications (see [26], [28], [30]), as follows
F(0) = 1
12





−1
2
−2
2
−1
2
−6
8
−6
2
−2
8
−12
8
−2
2
−6
8
−6
2
−1
2
−2
2
−1




.

Figure 8: Examples of images, generated by SGAN after training for 8 epochs on the Celebrities dataset
Table 3: Accuracy of the steganalyser S∗trained on real images
Type of a test set \ Image generator
SGANs
DCGANs
Real images
0.962
Generated images
0.501
0.522
The structure of the individual steganalyser S∗has the form: 2D convolution with F(0) ﬁlter →Conv2D →Conv2D →
Max Pooling →Conv2D →Conv2D →Max Pooling →Fully connected layer (1024 neurons) →Fully connected layer
(1 neuron) →Sigmoid function for output. This structure provides state-of-the-art steganalysis accuracy, [26], and the
ﬁlter F(0) allows to increase convergence speed of the steganalyser S∗training.
For training of this steganalyser we use the Adam optimization algorithm on the loss (2) with the learning rate equal to
5−6, β1 = 0.9, β2 = 0.999. As a loss function we use a binary cross-entropy.
The setup of experiments can be described as follows:
• We train and use the SGAN and/or DCGAN model to generate images to be used as containers;
• We train the independent steganalyser S∗using either real images (sec. 6.2.3) or generated images (sec. 6.2.4);
• We measure the accuracy of the steganalyser S∗.
6.2.3
Training/Testing on Real Images
In this set of experiments we train the independent steganalyser S∗on real images. Results are provided in tab. 3. From
the results we conclude that even the usual DCGAN generate synthetic container images, that can easily deceive a ste-
ganalyser. Although containers generated by on the SGAN model, which is specially designed to deceive the steganalyser,
improve on the result of DCGAN, the improvement is not signiﬁcant. These results also suggest that S∗may have been
overﬁt to the data distribution of the real images, and failed to generalize to the GAN approximation of the data distribution
in the synthetic image domain.
Another interesting result follows from an experiment with HUGO steganography algorithm and SGAN generated
containers:
• We use the ±1-embedding algorithm for training the SGAN model;

• We test the HUGO steganography analyzer on real images and on images, generated by the SGAN model.
Containers generated by the SGAN model decrease HUGO steganalysis accuracy from 0.624 to 0.499, which demon-
strates that the Steganographic Generative Adversarial Networks can potentially be used as a universal tool for generating
Steganography containers tuned to deceive any speciﬁc steganalysis algorithm.
6.2.4
Training/Testing on Generated Images
In this set of experiments we train the steganalyser S∗on images, generated using the DCGAN model from the prior
noise distribution pnoise(z) for some ﬁxed seed value. In all experiments the size of the train set is ﬁxed at ≈160000. After
training we test the analyser S∗on images generated with the DCGAN model according to the following experimental
setups:
C1. We use the same seed value;
C1. We use the same seed value, as in C1, and we additionally tune the DCGAN model for several epochs.
Table 4: Training/testing on generated images according to experimental conditions C1-C3
Experimental Conditions
Accuracy
C1
0.982
C3
0.499
The experiment results in tab. 4 indicate that additional tuning of the image generator G make this effect even more
pronounced.
We also conduct an experiment with classiﬁcation of generated images without steganographic embeddings. For this
purposes we train a DCGAN conditional model on the MNIST dataset, and train a separate classiﬁer for the MNIST
classiﬁcation task. The trained classiﬁer achieved almost perfect accuracy both on the held-out real MNIST dataset, and
on synthetic images produced by the DCGAN. This provides evidence that it is possible to train an image classiﬁer that
shows acceptable accuracy both on real and synthetic images. However it is the artiﬁcial generation of image containers
that breaks the usual approaches to steganalysis.
7. INFORMATION ENCRYPTION WITH SEGAN
7.1 Data description
For the experiments in the current section we use MNIST [19] and CIFAR-10 [18] data sets. Both datasets can be
considered as a benchmark in Deep Learning and Compute Vision.
• The MNIST dataset is a set of gray handwritten digits from 0 to 9 of size 28×28;
• The CIFAR-10 dataset consists of 60000 32×32 RGB images of totally 10 classes: airplane, automobile, bird, cat,
deer, dog, frog, horse, ship, truck.
7.2 Encryption experiments
We consider SEGAN model as a model for information encryption. It allows to generate images with hidden informa-
tion inside. In the experimental section we should
• check if the generated images are realistic;
• look at the quality of encryption.
The examples of generate images for MNIST dataset are presented in Fig. 9. We observe the full realism in the
generated images.
Figure 9: Sample synthetic images: MNIST

The examples of generate images for CIFAR dataset are presented in Fig. 10. This images are small and looks quit
realistic. This level of realism occurs because of current limitation of generative modeling.
Figure 10: Sample synthetic images: CIFAR-10
The quality of encryption is presented in Table 5. As a measure of quality we considered
EM[#{i : Mi ̸= B(A(Mi))}],
which means the average percentage of reconstructed pixels. As we can see, our new model allows to encrypt-decrypt
messages with the quality of almost 1. It is a little bit harder to encrypt longer messages that short.
Table 5: Quality of encryption/decryption (% of reconstruction bits)
Dataset \Number of bits
16 bit
32 bit
64 bit
MNIST
99.98
99.54
98.65
CIFAR-10
99.96
99.82
99.81
8. CONCLUSIONS
In this work,
1. We open a new ﬁeld for applications of Generative Adversarial Networks, namely, container generation for steganog-
raphy applications;
2. We consider the ±1-embedding algorithm and test novel approaches to more steganalysis-secure information em-
bedding: we demonstrate that both SGAN and DCGAN models are capable of decreasing the detection accuracy of
a steganalysis method almost to that of a random classiﬁer;
3. A model for secure adaptive steganographic containers generation has been presented;
4. A number of ways to adopt in order to deceive steganalysis has been proposed;
5. A new GAN-based steganography model has been proposed and tested on MNIST and CIFAR-10 dataset;
6. As a result an article ([31]) has been written and submitted to NIPS 2016, Workshop on Adversarial Training. This
paper cross-checks and signiﬁcantly extends results of that initial paper.
References
[1] Abbas Cheddad, Joan Condell, Kevin Curran, and Paul Mc Kevitt. Digital image steganography: Survey and analysis
of current methods. Signal Processing, 90(3):727 – 752, 2010.
[2] Ishan Durugkar, Ian Gemp, and Sridhar Mahadevan.
Generative multi-adversarial networks.
arXiv preprint
arXiv:1611.01673, 2016.
[3] Tomáš Filler, Jan Judas, and Jessica Fridrich. Minimizing additive distortion in steganography using syndrome-trellis
codes. IEEE Transactions on Information Forensics and Security, 6(3):920–935, Sept 2011.
[4] Jessica Fridrich and Jan Kodovsk`y. Rich models for steganalysis of digital images. IEEE Transactions on Information
Forensics and Security, 7(3):868–882, 2012.
[5] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. ArXiv e-prints, May 2015.

[6] Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. ArXiv e-prints, December 2017.
[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial nets. pages 2672–2680, 2014.
[8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Günter Klambauer, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a nash equilibrium. ArXiv e-prints, June.
[9] Vojtˇech Holub and Jessica Fridrich. Designing steganographic distortion using directional ﬁlters. In WIFS, 2012.
[10] Vojtˇech Holub, Jessica Fridrich, and Tomáš Denemark. Universal distortion function for steganography in an arbitrary
domain. EURASIP Journal on Information Security, 2014(1):1–13, 2014.
[11] Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric P. Xing. On unifying deep generative models. ArXiv
e-prints, June 2017.
[12] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[13] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adver-
sarial networks. ArXiv e-prints, November 2016.
[14] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[15] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[16] Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, and Max Welling. Semi-supervised learning with deep
generative models. ArXiv e-prints, June 2014.
[17] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. ArXiv e-prints, December 2013.
[18] Alex Krizhevsky and G Hinton. Convolutional deep belief networks on cifar-10. Unpublished manuscript, 40, 2010.
[19] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
[20] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV), 2015.
[21] Chandreyee Maiti, Debanjana Baksi, Ipsita Zamider, Pinky Gorai, and Dakshina Ranjan Kisku. Data Hiding in
Images Using Some Efﬁcient Steganography Techniques, pages 195–203. Springer Berlin Heidelberg, Berlin, Hei-
delberg, 2011.
[22] Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational
divergence minimization. ArXiv e-prints, June 2016.
[23] Tomáš Pevn`y, Patrick Bas, and Jessica Fridrich. Steganalysis by subtractive pixel adjacency matrix. IEEE Transac-
tions on Information Forensics and Security, 5(2):215–224, 2010.
[24] Tomáš Pevný, Tomáš Filler, and Patrick Bas. Using high-dimensional image models to perform highly undetectable
steganography. In Information Hiding, page 2010, Calgary, Canada, June 2010.
[25] Lionel Pibre, Pasquet Jérôme, Dino Ienco, and Marc Chaumont. Deep learning for steganalysis is better than a rich
model with an ensemble classiﬁer, and is natively robust to the cover source-mismatch. ArXiv e-prints, November
2015.
[26] Lionel Pibre, Pasquet Jérôme, Dino Ienco, and Marc Chaumont.
Deep learning for steganalysis is better than
a rich model with an ensemble classiﬁer, and is natively robust to the cover source-mismatch.
arXiv preprint
arXiv:1511.04855, 2015.
[27] Yinlong Qian, Jing Dong, Wei Wang, and Tieniu Tan.
Deep learning for steganalysis via convolutional neural
networks. In SPIE/IS&T Electronic Imaging, pages 94090J–94090J. International Society for Optics and Photonics,
2015.
[28] Yinlong Qian, Jing Dong, Wei Wang, and Tieniu Tan.
Deep learning for steganalysis via convolutional neural
networks. In IS&T/SPIE Electronic Imaging, pages 94090J–94090J. International Society for Optics and Photonics,
2015.
[29] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
[30] Shunquan Tan and Bin Li. Stacked convolutional auto-encoders for steganalysis of digital images. In Asia-Paciﬁc
Signal and Information Processing Association, 2014 Annual Summit and Conference (APSIPA), pages 1–4. IEEE,
2014.
[31] Denis Volkhonskiy, Ivan Nazarov, Boris Borisenko, and Evgeny Burnaev. Steganographic generative adversarial
networks. Workshop on Adversarial Training, Neural Information Processing Systems, 2016.

