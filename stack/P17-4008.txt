Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 43–48
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-4008
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 43–48
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-4008
Hafez: an Interactive Poetry Generation System
Marjan Ghazvininejad∗, Xing Shi∗, Jay Priyadarshi, and Kevin Knight
†Information Sciences Institute & Computer Science Department
University of Southern California
{ghazvini,xingshi,jpriyada,knight}@isi.edu
Abstract
Hafez is an automatic poetry generation
system that integrates a Recurrent Neural
Network (RNN) with a Finite State Accep-
tor (FSA). It generates sonnets given arbi-
trary topics. Furthermore, Hafez enables
users to revise and polish generated poems
by adjusting various style conﬁgurations.
Experiments demonstrate that such “pol-
ish” mechanisms consider the user’s inten-
tion and lead to a better poem. For evalua-
tion, we build a web interface where users
can rate the quality of each poem from 1
to 5 stars. We also speed up the whole
system by a factor of 10, via vocabulary
pruning and GPU computation, so that ad-
equate feedback can be collected at a fast
pace. Based on such feedback, the system
learns to adjust its parameters to improve
poetry quality.
1
Introduction
Automated poetry generation is attracting increas-
ing research effort.
Researchers approach the
problem by using grammatical and semantic tem-
plates (Oliveira, 2009, 2012) or treating the gen-
eration task as a translation/summarization task
(Zhou et al., 2009; He et al., 2012; Yan et al., 2013;
Zhang and Lapata, 2014; Yi et al., 2016; Wang
et al., 2016; Ghazvininejad et al., 2016). How-
ever, such poetry generation systems face these
challenges:
1. Difﬁculty of evaluating poetry quality. Au-
tomatic evaluation methods, like BLEU, can-
not judge the rhythm, meter, creativity or syn-
tactic/semantic coherence, and furthermore,
there is no test data in most cases. Subjective
∗*equal contributions
evaluation requires evaluators to have rela-
tively high literary training, so systems will
receive limited feedback during the develop-
ment phase.1
2. Inability to adjust the generated poem.
When poets compose a poem, they usually
need to revise and polish the draft from differ-
ent aspects (e.g., word choice, sentiment, al-
literation, etc.) for several iterations until sat-
isfaction. This is a crucial step for poetry cre-
ation. However, given a user-supplied topic
or phrase, most existing automated systems
can only generate different poems by using
different random seeds, providing no other
support for the user to polish the generated
poem in a desired direction.
3. Slow generation speed. Generating a poem
may require a heavy search procedure. For
example, the system of Ghazvininejad et al.
(2016) needs 20 seconds for a four-line poem.
Such slow speed is a serious bottleneck for
a smooth user experience, and prevents the
large-scale collection of feedback for system
tuning.
This work is based on our previous poetry gen-
eration system called Hafez (Ghazvininejad et al.,
2016), which generates poems in three steps:
(1) search for related rhyme words given user-
supplied topic, (2) create a ﬁnite-state acceptor
(FSA) that incorporates the rhyme words and con-
trols meter, and (3) use a recurrent neural network
(RNN) to generate the poem string, guided by the
FSA. We address the above-mentioned challenges
with the following approaches:
1The Dartmouth Turing Tests in the Creative Arts
(bit.ly/20WGLF3), in which human experts are employed to
judge the generation quality, is held only once a year.
43

Topic: Presidential elections
To hear the sound of communist aggression!
I never thought about an exit poll,
At a new Republican convention,
On the other side of gun control.
Table 1: One poem generated in a 15-minute hu-
man/computer interactive poetry contest.
1. We build a web interface2 for our poem gen-
eration system, and for each generated poem,
the user can rate its quality from 1-star to 5-
stars. Our logging system collects poems, re-
lated parameters, and user feedback. Such
crowd-sourcing enables us to obtain large
amounts of feedback in a cheap and efﬁcient
way. Once we collect enough feedback, the
system learns to ﬁnd a better set of parame-
ters and updates the system continuously.
2. We add additional weights during decoding
to control the style of generated poem, in-
cluding the extent of words repetition, allit-
eration, word length, cursing, sentiment, and
concreteness.
3. We increase speed by pre-calculation, pre-
loading model parameters, and pruning the
vocabulary. We also parallelize the computa-
tion of FSA expansion, weight merging, and
beam search, and we port them into a GPU.
Overall, we can generate a four-line poem
within 2 seconds, ten times faster than our
previous CPU-based system.
With the web interface’s style control and fast
generation speed, people can generate creative po-
ems within a short time. Table 1 shows one of
the poems generated in a poetry mini-competition
where 7 people are asked to use Hafez to generate
poems within 15 minutes. We also conduct exper-
iments on Amazon Mechanical Turk, which show:
ﬁrst, through style-control interaction, 71% users
can ﬁnd a better poem than the poem generated by
the default conﬁguration. Second, based on users’
evaluation results, the system learns a new conﬁg-
uration which generates better poems.
2Live demo at http://52.24.230.241/poem/
advance/
2
System Description
Figure 1: Overview of Hafez
Figure 1 shows an overview of Hafez. In the
web interface, a user can input topic words or
phrases and adjust the style conﬁguration. This in-
formation is then sent to our backend server, which
is primarily based on our previously-described
work (Ghazvininejad et al., 2016). First, the back-
end will use the topic words/phrases to ﬁnd related
rhyme word pairs by using a word2vec model
and a pre-calculated rhyme-type dictionary. Given
these rhyme word pairs, an FSA that encodes all
valid word sequences is generated, where a valid
word sequence follows certain type of meter and
puts the rhyme word at the end of each line. This
FSA, together with the user-supplied style conﬁg-
uration, is then used to guide the Recurrent Neural
Network (RNN) decoder to generate the rest of the
poem. User can rate the generated poem using a
5-star system. Finally, the tuple (topic, style con-
ﬁguration, generated poem, star-rating) is pushed
to the logging system. Periodically, a module will
analyze the logs, learn a better style conﬁguration
and update it as the new default style conﬁgura-
tion.
2.1
Example in Action
Figure 2 provides an example in action.
The
user has input the topic word “love” and left the
style conﬁguration as default.
After they click
the “Generate” button, a four-line poem is gen-
erated and displayed. The user may not be sat-
isﬁed with current generation, and may decide to
add more positive sentiment and encourage a little
bit of the alliteration. After they move the cor-
responding slider bars and click the “Re-generate
with the same rhyme words” button, a new poem
is returned. This poem has more positive senti-
44

ment (“A lonely part of you and me tonight” vs.
“A lovely dream of you and me tonight”) and more
alliteration (“My merry little love”, “The lucky
lady” and “She sings the sweetest song” ).
2.2
Style Control
During the RNN’s beam search, each beam cell
records the current FSA state s. Its succeeding
state is denoted as ssuc. All the words over all the
succeeding states forms a vocabulary Vsuc. To ex-
pand the beam state b, we need to calculate a score
for each word in Vsuc:
score(w, b) = score(b) + log PRNN(w)
+
X
i
wi ∗fi(w); ∀w ∈Vsuc
(1)
where log PRNN(w) is the log-probability of word
w calculated by RNN. score(b) is the accumulated
score of the already-generated words in beam state
b . fi(w) is ith feature function and wi is the cor-
responding weight.
To control the style, we design the following 8
features:
1. Encourage/discourage words. User can input
words that they would like in the poem, or
words to be banned. f(w) = I(w, Venc/dis)
where I(w, V ) = 1 if w is in the word list
V , otherwise I(w, V ) = 0. wenc = 5 and
wdis = −5.
2. Curse words. We pre-build a curse-word list
Vcurse, and f(w) = I(w, Vcurse).
3. Repetition. To control the extent of repeated
words in the poem.
For each beam, we
record the current generated words Vhistory,
and f(w) = I(w, Vhistory).
4. Alliteration. To control how often adjacent
non-function words start with the same con-
sonant sound.
In the beam cell, we also
record the previous generated word wt−1, and
f(wt) = 1 if wt and wt−1 share the same ﬁrst
consonant sound, otherwise it equals 0.
5. Word length.
To control a preference for
longer words in the generated poem. f(w) =
length(w)2.
6. Topical words. For each user-supplied topic
words, we generate a list of related words
Vtopical. f(w) = I(w, Vtopical).
7. Sentiment. We pre-build a word list together
with its sentiment scores based on Senti-
WordNet (Baccianella et al., 2010).
f(w)
equals to w′s sentiment score.
8. Concrete words. We pre-build a word list to-
gether with a score to reﬂect its concreteness
based on Brysbaert et al. (2014). f(w) equals
to w’s concreteness score.
2.3
Speedup
To ﬁnd the rhyming words related to the topic, we
employ a word2vec model. Given a topic word or
phrase wt ∈V , we ﬁnd related words wr based on
the cosine distance:
wr = argmax
wr∈V ′⊆V
cosine(ewr, ewt)
(2)
where ew is the embedding of word w. Then we
calculate the rhyme type of each related word wr
to ﬁnd rhyme pairs.
To speed up this step, we carefully optimize the
computation with these methods:
1. Pre-load all parameters into RAM. As we
are aiming to accept arbitrary topics, the vo-
cabulary V of word2vec model is very large
(1.8M words and phrases). Pre-loading saves
3-4 seconds.
2. Pre-calculate the rhyme types for all words
w ∈V ′. During runtime, we use this dictio-
nary to lookup the rhyme type.
3. Shrink V’. As every rhyme word/phrase pairs
must be in the target vocabulary VRNN of the
RNN, we further shrink V ′ = V ∩VRNN.
To speedup the RNN decoding step, we use
GPU processing for all forward-propagation com-
putations. For beam search, we port to GPU the
two most time-consuming parts, calculating scores
with Equation 1 and ﬁnding the top words based
the score:
1. We warp all the computation needed in Equa-
tion 1 into a single large GPU kernel launch.
2. With beam size B,
to ﬁnd the top k
words, instead of using a heap sort on
CPU with complexity O(B|Vsuc|logk), we
do a global sort on GPU with complex-
ity O(B|Vsuc|log(B|Vsuc|)) in one kernel
launch.
Even though the complexity in-
creases, the computation time in practice re-
duces quite a bit.
45

(a) Poem generated with default style settings
(b) Poem generated with user adjusted style settings
Figure 2: A poem generated with (a) default style conﬁguration and (b) user-adjusted style conﬁguration.
Finally, our system can generate a 4-line poem
within 2 seconds, which is 10 times faster than the
previous CPU-based version.
2.4
Learn a New Style Conﬁguration
Except the ﬁxed weight for encouragement and
discouragement of words, all other 7 weights form
our style conﬁguration space:
W = {wi|i = 1..7}
(3)
and we denote the default conﬁguration as Wd, in-
tuitively selected.
Users will usually start with Wd to generate
their ﬁrst poem pd, and if they later adjust the
style conﬁguration and click “Re-generate with
same rhyme words” button, the new poem pi will
change the style accordingly, but still with the
same rhyme words as pd. In the logging system, a
unique hash hr is recorded to distinguish different
rhyme word sets in the poem.
After proper processing, our logging system or-
ganizes the data points as a dictionary D = {hr :
[(pd, Wd, rd), (p1, W1, r1), ..., (pn, Wn, rn)]}. rd
represents user’s star rating for the poem with de-
fault setting(pd) while ri ∈{1, ..., n} represents
user’s star rating for p1, ..., pn.
To learn a new style conﬁguration Wnew, we
construct our training data Dtrain
=
{hr
:
[(∆W1, ∆r1), ..., (∆Wn, ∆rn)]}, where ∆Wi =
Wi −Wd and ∆ri = ri −rd. Then, we ﬁt a
quadratic regression between the rating change ∆r
and each weight change ∆wj ∈∆W indepen-
dently:
∆r = a(∆wj)2 + b∆wj + c
(4)
and the new weight wj,new is
wj,new = wj,d + argmax
∆wj
∆r
(5)
2.5
Alexa Skill
To feel beauty, rhythm, and emotion in poetry, it
is better to listen to a spoken version. With this
46

User: Alexa, start Custom Poetry
Alexa: Welcome to Custom Poetry.
Please give me a topic.
User: Mountains
Alexa:
A hundred thousand Morrison formation,
An ancient crown of gold or mountain chains,
Mountains from the land of elevation,
A northern storm across the hills and plains.
Alexa: Feel free to give me another topic.
User: Lakes
Alexa: ...
Alexa: Feel free to give me another topic.
User: Stop
Alexa: Thank you for trying Custom
Poetry. Have a nice day!
Table 2: Spoken onversation between a user and
Alexa.
in mind, we also publish our system as an Ama-
zon Alexa Skill (“Custom Poetry”), so that users
can ask Alexa to compose and recite a poem on
any topic. Table 2 shows a sample conversation
between a user and Alexa.
3
Experiments
We design an Amazon Mechanical Turk task to ex-
plore the effect of style options. In this task Turk-
ers ﬁrst use Hafez to generate a default poem on
an arbitrary topic with the default style conﬁgura-
tion, and rate it. Next, they are asked to adjust the
style conﬁgurations to re-generate at least ﬁve dif-
ferent adjusted poems with the same rhyme words,
and rate them as well. Improving the quality of
adjusted poems over the default poem is not re-
quired for ﬁnishing the task, but it is encouraged.
For each task, Turkers can select the best gener-
ated poem, and if subsequent human judges (do-
main experts) rank that poem as “great”, a bonus
reward will be assigned to that Turker. We gath-
ered data from 62 completed HITs (Human Intel-
ligence Tasks) for this task.
3.1
Human-Computer Collaboration
This experiment tests whether human collabora-
tion can help Hafez generate better poems.
In only 10% of the HITs, the reported best poem
was generated by the default style options, i.e., the
default poem. Additionally, in 71% of the HITs,
users assign a higher star rating to at least one of
-1
0
1
2
3
4
Normalized Topical Weight
-4
-3
-2
-1
0
1
2
3
4
Normalized Score
(a)
-5
-4
-3
-2
-1
0
1
2
3
4
5
Normalized Concreteness Weight
-4
-3
-2
-1
0
1
2
3
4
Normalized Score
(b)
-5
-4
-3
-2
-1
0
1
2
3
4
5
Normalized Sentiment Weight
-4
-3
-2
-1
0
1
2
3
4
Normalized Score
(c)
-5
-4
-3
-2
-1
0
1
2
3
4
5
Normalized Repetition Weight
-4
-3
-2
-1
0
1
2
3
4
Normalized Score
(d)
Figure 3: The distribution of poem star-ratings
against normalized topical, concreteness, senti-
ment and repetition weights. Star ratings are com-
puted as an offset from the version of the poem
generated from default settings. We normalize all
features weights by calculating their offset from
the default values. The solid curve represents a
quadratic regression ﬁt to the data. To avoid over-
lapping points, we plot with a small amount of ran-
dom noise added.
the adjust poems than the default poem. On aver-
age the best poems got +1.4 more stars compared
to the default one.
However, poem creators might have a tendency
to report a higher ranking for poems generated
through the human/machine collaboration process.
To sanity check the results we designed another
task and asked 18 users to compare the default
and the reported best poems. This experiment sec-
47

onded the original rankings in 72% of the cases.
3.2
Automatic tuning for quality
We learn new default conﬁgurations using the data
gathered from Mechanical Turk. As we explained
in section 2.4, we examine the effect of different
feature weights like repetition and sentiment on
star ranking scores. We aim to cancel out the effect
of topic and rhyme words on our scoring function.
We achieve this by plotting the score offset from
the default poem for each topic and set of rhyme
words. Figure 3 shows the distribution of scores
against topical, concreteness, sentiment and repe-
tition weights. In each plot the zero weight rep-
resents the default value. Each plot also shows a
quadratic regression curve ﬁt to its data.
In order to alter the style options toward gener-
ating better default poems, we re-set each weight
to the maximum of each quadratic curve. Hence,
the new weights encourage more topical, less con-
crete, more positive words and less repetition. It is
notable that for sentiment, users prefer both more
positive and more negative words to the initial
neutral setting, but the preference is slightly biased
towards positive words.
We update Hafez’s default settings based on this
analysis. We ask 29 users to compare poems gen-
erated on the same topic and rhyme words using
both old and new style settings. In 59% of the
cases, users prefer the poem generated by the new
setting.
We thus improve the default settings for gen-
erating a poem, though this does not mean that
the poems cannot be further improved by human
collaboration. In most cases, a better poem can
be generated by collaboration with the system
(changing the style options) for the speciﬁc topic
and set of rhyme words.
4
Conclusion
We demonstrate Hafez, an interactive poetry gen-
eration system. It enables users to generate poems
about any topic, and revise generated texts through
multiple style conﬁgurations.
We speed up the
system by vocabulary pruning and GPU computa-
tion. Together with an easily-accessible web in-
terface, we can collect large numbers of human
evaluations in a short timespan, making automatic
system tuning possible.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In LREC.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2014. Concreteness ratings for 40 thousand
generally known english word lemmas.
Behavior
research methods .
Marjan Ghazvininejad, Xing Shi, Yejin Choi, and
Kevin Knight. 2016. Generating topical poetry. In
Proc. EMNLP.
Jing He, Ming Zhou, and Long Jiang. 2012. Generat-
ing Chinese classical poems with statistical machine
translation models. In Proc. AAAI.
Hugo Oliveira. 2009. Automatic generation of poetry:
an overview. In Proc. 1st Seminar of Art, Music,
Creativity and Artiﬁcial Intelligence.
Hugo Oliveira. 2012. PoeTryMe: a versatile platform
for poetry generation.
Computational Creativity,
Concept Invention, and General Intelligence 1.
Qixin Wang, Tianyi Luo, Dong Wang, and Chao Xing.
2016. Chinese song iambics generation with neural
attention-based model. arXiv:1604.06274 .
Rui Yan, Han Jiang, Mirella Lapata, Shou-De Lin,
Xueqiang Lv, and Xiaoming Li. 2013. I, Poet: Au-
tomatic Chinese poetry composition through a gen-
erative summarization framework under constrained
optimization. In Proc. IJCAI.
Xiaoyuan Yi, Ruoyu Li, and Maosong Sun. 2016. Gen-
erating chinese classical poems with RNN encoder-
decoder. arXiv:1604.01537 .
Xingxing Zhang and Mirella Lapata. 2014.
Chinese
poetry generation with recurrent neural networks. In
Proc. EMNLP.
Ming Zhou, Long Jiang, and Jing He. 2009. Generat-
ing Chinese couplets and quatrain using a statistical
approach. In Proc. Paciﬁc Asia Conference on Lan-
guage, Information and Computation.
48

