Ivan Varzinczak (Ed.)
LNCS 13388
Foundations
of Information and
Knowledge Systems
12th International Symposium, FoIKS 2022
Helsinki, Finland, June 20–23, 2022
Proceedings

Lecture Notes in Computer Science
13388
Founding Editors
Gerhard Goos
Karlsruhe Institute of Technology, Karlsruhe, Germany
Juris Hartmanis
Cornell University, Ithaca, NY, USA
Editorial Board Members
Elisa Bertino
Purdue University, West Lafayette, IN, USA
Wen Gao
Peking University, Beijing, China
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Moti Yung
Columbia University, New York, NY, USA

More information about this series at https://link.springer.com/bookseries/558

Ivan Varzinczak (Ed.)
Foundations
of Information and
Knowledge Systems
12th International Symposium, FoIKS 2022
Helsinki, Finland, June 20–23, 2022
Proceedings

Editor
Ivan Varzinczak
Université d’Artois and CNRS
Lens, France
ISSN 0302-9743
ISSN 1611-3349 (electronic)
Lecture Notes in Computer Science
ISBN 978-3-031-11320-8
ISBN 978-3-031-11321-5 (eBook)
https://doi.org/10.1007/978-3-031-11321-5
© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Switzerland AG 2022
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, expressed or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
These proceedings contain the papers that have been selected for presentation at the
12th International Symposium on Foundations of Information and Knowledge Systems,
FoIKS 2022. The symposium was held during June 20–23, 2022, at the University of
Helsinki in Finland.
The FoIKS symposia provide a biennial forum for presenting and discussing
theoretical and applied research on information and knowledge systems. The goal is
to bring together researchers with an interest in this subject, share research experiences,
promote collaboration, and identify new issues and directions for future research.
Previous FoIKSmeetings wereheldinSchloss Salzau(Germany, 2002), Vienna(Austria,
2004), Budapest (Hungary, 2006), Pisa (Italy, 2008), Soﬁa (Bulgaria, 2010), Kiel
(Germany, 2012), Bordeaux (France, 2014), Linz (Austria, 2016), Budapest (Hungary,
2018), and Dortmund (Germany, 2020).
The call for papers solicited original contributions dealing with any foundational
aspect of information and knowledge systems, including submissions that apply ideas,
theories, or methods from speciﬁc disciplines to information and knowledge systems.
Examples of such disciplines are discrete mathematics, logic and algebra, model
theory, databases, information theory, complexity theory, algorithmics and computation,
statistics, and optimization.
Traditionally, the FoIKS symposia are a forum for intensive discussion where
speakers are given sufﬁcient time to present their ideas and results within the larger
context of their research. Furthermore, participants are asked to prepare a ﬁrst response
to another contribution in order to initiate discussion.
FoIKS 2022 received 21 submissions, which were evaluated by the Program
Committee on the basis of their signiﬁcance, novelty, technical soundness, and
appropriateness for the FoIKS audience. Each paper was subjected to three reviews
(two reviews in only one case). In the end, 13 papers were selected for oral presentation
at the symposium and publication in the archival proceedings.
We were delighted to have ﬁve outstanding keynote speakers. The abstracts of their
talks were included in this volume:
– Patricia Bouyer-Decitre (CNRS, France): Memory complexity for winning games on
graphs
– Gabriele Kern-Isberner (Technische Universität Dortmund, Germany): The Relevance
of Formal Logics for Cognitive Logics, and Vice Versa
– Jussi Rintanen (Aalto University, Finland): More automation to software engineering
– Jouko Väänänen (University of Helsinki, Finland): Dependence logic: Some recent
developments
– Zeev Volkovich (ORT Braude College of Engineering, Israel): Text classiﬁcation using
“imposter” projections method

vi
Preface
We want to thank all the people who contributed to making FoIKS 2022 a successful
meeting. In particular, we thank the invited speakers for their inspirational talks, the
authors for providing their high-quality submissions and revising and presenting their
work, and all the attendees for contributing to the discussions at the symposium. We
thank the Program Committee and the external reviewers for their prompt and careful
reviews.
We extend our thanks to the Local Organizing Committee, chaired by Matti Järvisalo
and Juha Kontinen, who have always been proactive and responsive throughout the
whole organization process, and to the FoIKS Steering Committee members for their
trust and support. We gratefully acknowledge the ﬁnancial support of FoIKS 2022 by the
University of Helsinki, the Finnish Academy of Science and Letters, and the Artiﬁcial
Intelligence Journal.
June 2022
Ivan Varzinczak

Organization
Program Chair
Ivan Varzinczak
CRIL, Université d’Artois, and CNRS, France
Program Committee
Yamine Ait Ameur
IRIT/INPT-ENSEEIHT, France
Kim Bauters
University of Bristol, UK
Christoph Beierle
FernUniversität in Hagen, Germany
Leopoldo Bertossi
Universidad Adolfo Ibanez, Chile
Meghyn Bienvenu
CNRS and University of Bordeaux, France
Joachim Biskup
Technische Universität Dortmund, Germany
Elena Botoeva
University of Kent, UK
Arina Britz
CAIR, Stellenbosch University, South Africa
Giovanni Casini
ISTI-CNR, Italy
Fabio Cozman
University of São Paulo, Brazil
Dragan Doder
Utrecht University, The Netherlands
Thomas Eiter
Vienna University of Technology, Austria
Christian Fermüller
Vienna University of Technology, Austria
Flavio Ferrarotti
Software Competence Center Hagenberg, Austria
Laura Giordano
Università del Piemonte Orientale, Italy
Dirk Van Gucht
Indiana University Bloomington, USA
Marc Gyssens
Universiteit Hasselt, Belgium
Andreas Herzig
IRIT, CNRS, and Université de Toulouse, France
Tomi Janhunen
Tampere University, Finland
Matti Järvisalo
University of Helsinki, Finland
Gabriele Kern-Isberner
Technische Universität Dortmund, Germany
Elena Kleiman
Braude College Karmiel, Israel
Sébastien Konieczny
CRIL, CNRS, and Université d’Artois, France
Juha Kontinen
University of Helsinki, Finland
Antti Kuusisto
Tampere University, Finland
Sebastian Link
University of Auckland, New Zealand
Thomas Lukasiewicz
University of Oxford, UK
Pierre Marquis
CRIL, Université d’Artois, and CNRS, France
Thomas Meyer
CAIR, University of Cape Town, South Africa
Nicola Olivetti
LSIS, Aix-Marseille University, France
Iliana Petrova
Inria - Sophia Antipolis, France

viii
Organization
Ramon Pino Perez
CRIL, Université d’Artois, and CNRS, France
Gian Luca Pozzato
Università di Torino, Italy
Sebastian Rudolph
TU Dresden, Germany
Attila Sali
Alfréd Rényi Institute, Hungary
Katsuhiko Sano
Hokkaido University, Japan
Kostyantyn Shchekotykhin
Alpen-Adria University Klagenfurt, Austria
Klaus-Dieter Schewe
Zhejiang University, China
Steven Schockaert
Cardiff University, UK
Guillermo Simari
Universidad del Sur in Bahia Blanca, Argentina
Mantas Simkus
Vienna University of Technology, Austria
Michael Sioutis
Otto-Friedrich-University Bamberg, Germany
Umberto Straccia
ISTI-CNR, Italy
Karim Tabia
CRIL, Université d’Artois, and CNRS, France
Bernhard Thalheim
University of Kiel, Germany
Alex Thomo
University of Victoria, Canada
David Toman
University of Waterloo, Canada
Qing Wang
Australian National University, Australia
Renata Wassermann
University of São Paulo, Brazil
Stefan Woltran
Vienna University of Technology, Austria
Local Organizing Chairs
Matti Järvisalo
University of Helsinki, Finland
Juha Kontinen
University of Helsinki, Finland
Local Organizing Committee
Jeremias Berg
University of Helsinki, Finland
Miika Hannula
University of Helsinki, Finland
Minna Hirvonen
University of Helsinki, Finland
Tuomo Lehtonen
University of Helsinki, Finland
Andreas Niskanen
University of Helsinki, Finland
Max Sandström
University of Helsinki, Finland
Additional Reviewers
Tamara Drucks
Anna Rapberger

Abstracts of Invited Talks

Memory Complexity for Winning Games on Graphs
Patricia Bouyer-Decitre
CNRS, France
Abstract. Two-player games are relevant models for reactive synthesis,
with application to the veriﬁcation of systems. Winning strategies are then
viewed as controllers, which guarantee that the system under control will
satisfy its speciﬁcation. In this context, simpler controllers will be easier
to implement. Simplicity will refer to the memory complexity, that is,
how much memory is needed to win the games. We will in particular
discuss cases where the required memory is ﬁnite.
This talk will be based on recent works made with my colleagues
Mickael Randour and Pierre Vandenhove.

The Relevance of Formal Logics for Cognitive Logics,
and Vice Versa
Gabriele Kern-Isberner
Technische Universität Dortmund, Germany
Abstract. Classical logics like propositional or predicate logic have been
considered as the gold standard for rational human reasoning, and hence
as a solid, desirable norm on which all human knowledge and decision
making should be based, ideally. For instance, Boolean logic was set
up as kind of an arithmetic framework that should help make rational
reasoning computable in an objective way, similar to the arithmetics of
numbers. Computer scientists adopted this view to (literally) implement
objective knowledge and rational deduction, in particular for AI appli-
cations. Psychologists have used classical logics as norms to assess the
rationality of human commonsense reasoning. However, both disciplines
could not ignore the severe limitations of classical logics, e.g., computa-
tional complexity and undecidedness, failures of logic-based AI systems
in practice, and lots of psychological paradoxes. Many of these problems
are caused by the inability of classical logics to deal with uncertainty in
an adequate way. Both disciplines have used probabilities as a way out
of this dilemma, hoping that numbers and the Kolmogoroff axioms can
do the job (somehow). However, psychologists have been observing also
lots of paradoxes here (maybe even more).
So then, are humans hopelessly irrational? Is human reasoning incom-
patible with formal, axiomatic logics? In the end, should computer-based
knowledge and information processing be considered as superior in terms
of objectivity and rationality?
Cognitive logics aim at overcoming the limitations of classical log-
ics and resolving the observed paradoxes by proposing logic-based
approaches that can model human reasoning consistently and coherently
in benchmark examples. The basic idea is to reverse the normative way of
assessing human reasoning in terms of logics resp. probabilities, and to
use typical human reasoning patterns as norms for assessing the cognitive
quality of logics. Cognitive logics explore the broad ﬁeld of logic-based
approaches between the extreme points marked by classical logics and
probability theory with the goal to ﬁnd more suitable logics for AI appli-
cations, on the one hand, and to gain more insights into the rational
structures of human reasoning, on the other.

More Automation to Software Engineering
Jussi Rintanen
Aalto University, Finland
Abstract. My background in AI is in knowledge representation and later
in automated planning and other problems that can be solved with auto-
mated reasoning methods. When challenged about the applications of
my research, I identiﬁed software production as a promising area to look
at, differently from many in automated planning who have considered
robotics or generic problem-solving as the potential future applications
of their work. In this talk, I will describe my work since in 2016 on
producing software fully automatically from high-level speciﬁcations of
its functionalities. I will compare my approach to existing commercial
tools and to works in academic AI research, especially to models used
in automated planning and knowledge representation, and discuss my
experiences in moving research from academia to real world use.

Dependence Logic: Some Recent Developments
Jouko Väänänen
University of Helsinki, Finland
Abstract. In the traditional so-called Tarski’s Truth Deﬁnition the seman-
ticsofﬁrst-orderlogicisdeﬁnedwithrespecttoanassignmentofvaluesto
the free variables. A richer family of semantic concepts can be modelled
if semantics is deﬁned with respect to a set (a “team”) of such assign-
ments. This is called team semantics. Examples of semantic concepts
available in team semantics but not in traditional Tarskian semantics are
the concepts of dependence and independence. Dependence logic is an
extension of ﬁrst-order logic based on team semantics. It has emerged
that teams appear naturally in several areas of sciences and humanities,
which has made it possible to apply dependence logic and its variants
to these areas. In my talk, I will give a quick introduction to the basic
ideas of team semantics and dependence logic as well as an overview of
some new developments, such as an analysis of dependence and indepen-
dence in terms of diversity, a quantitative analysis of team properties in
terms of dimension, and a probabilistic independence logic inspired by
the foundations of quantum mechanics.

Text Classiﬁcation Using “Imposter” Projections Method
Zeev Volkovich
Ort Braude College of Engineering, Israel
Abstract. The paper presents a novel approach to text classiﬁcation. The
method comprehends the considered documents as random samples gen-
erated from distinct probability sources and tries to estimate a difference
between them through random projections. A deep learning mechanism
composed, for example, of a Convolutional Neural Network (CNN) and
a Bi-Directional Long Short-Term Memory Network (Bi-LSTM) deep
networks together with an appropriate word embedding is applied to
estimate the “imposter” projections using imposters data collection. As
a result, each studied document is transformed into a digital signal aim-
ing to exploit signal classiﬁcation methods. Examples of the application
of the suggested method include studies of the Shakespeare Apocrypha,
the New and Old Testament, and the Abu Hamid Al Ghazali creations
particularly.

Contents
On Sampling Representatives of Relational Schemas with a Functional
Dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Maximilian Berens and Joachim Biskup
On the Expressive Power of Message-Passing Neural Networks as Global
Feature Map Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
Floris Geerts, Jasper Steegmans, and Jan Van den Bussche
Assumption-Based Argumentation for Extended Disjunctive Logic
Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
Toshiko Wakaki
A Graph Based Semantics for Logical Functional Diagrams in Power
Plant Controllers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
Aziz Sfar, Dina Irofti, and Madalina Croitoru
Database Repair via Event-Condition-Action Rules in Dynamic Logic . . . . . . . .
75
Guillaume Feuillade, Andreas Herzig, and Christos Rantsoudis
Statistics of RDF Store for Querying Knowledge Graphs . . . . . . . . . . . . . . . . . . . .
93
Iztok Savnik, Kiyoshi Nitta, Riste Skrekovski, and Nikolaus Augsten
Can You Answer While You Wait? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
Luís Cruz-Filipe, Graça Gaspar, and Isabel Nunes
The Implication Problem for Functional Dependencies and Variants
of Marginal Distribution Equivalences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
Minna Hirvonen
Approximate Keys and Functional Dependencies in Incomplete Databases
with Limited Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
Munqath Al-atar and Attila Sali
The Fault-Tolerant Cluster-Sending Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
Jelle Hellings and Mohammad Sadoghi
Optimizing Multiset Relational Algebra Queries Using Weak-Equivalent
Rewrite Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
Jelle Hellings, Yuqing Wu, Dirk Van Gucht, and Marc Gyssens

xviii
Contents
Properties of System W and Its Relationships to Other Inductive Inference
Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
Jonas Haldimann and Christoph Beierle
Towards the Evaluation of Action Reversibility in STRIPS Using Domain
Generators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
Tobias Schwartz, Jan H. Boockmann, and Leon Martin
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237

On Sampling Representatives
of Relational Schemas with a Functional
Dependency
Maximilian Berens and Joachim Biskup(B)
Fakult¨at f¨ur Informatik, Technische Universit¨at Dortmund,
44227 Dortmund, Germany
{maximilian.berens,joachim.biskup}@cs.tu-dortmund.de
Abstract. We further contribute to numerous eﬀorts to provide tools
for generating sample database instances, complement a recent approach
to achieve a uniform probability distribution over all samples of a spec-
iﬁed size, and add new insight to the impact of schema normalization
for a relational schema with one functional dependency and size restric-
tions on the attribute domains. These achievements result from studying
the problem how to probabilistically generate a relation instance that
is a representative of a class of equivalent or similar instances, respec-
tively. An instance is equivalent to another instance if there are bijective
domain mappings under which the former one is mapped on the other
one. An instance is similar to another instance if they share the same
combinatorial counting properties that can be understood as a solution
to a layered system of equalities and lessthan-relationships among (non-
negative) integer variables and some (non-negative) integer constants.
For a normalized schema, the two notions turn out to coincide. Based on
this result, we conceptually design and formally verify a probabilistic gen-
eration procedure that provides a random representative of a randomly
selected class, i.e., each class is represented with the same probability
or, alternatively, with the probability reﬂecting the number of its mem-
bers. We also discuss the performance of a prototype implementation and
further optimizations. For a non-normalized schema, however, the coin-
cidence of the respective notions does not hold. So we only present some
basic features of these notions, including a relationship to set uniﬁcation.
Keywords: Combinatorial analysis · Domain cardinality ·
Duplicate-freeness · Functional dependency · Instance equivalence ·
Instance similarity · Integer partition · Normalized schema ·
Non-normalized schema · Random representative · Relational schema ·
Set uniﬁcation · System of relationships among integer variables ·
Uniform probability distribution
1
Introduction
Since many years, there have been numerous eﬀorts to provide tools for generat-
ing sample database instances under various aspects of software engineering, see
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 1–19, 2022.
https://doi.org/10.1007/978-3-031-11321-5_1

2
M. Berens and J. Biskup
Table 1. Array representations r1[], r2[] and r3[], also indicating colored domain renam-
ings to show the equivalence of r1 with r2.
r1[]
A
B
C
1
1
1
2
2
1
3
2
1
4
2
1
4
2
2
r2[]
A / A
B / B
C
1 / 2
1 / 2
1
2 / 3
1 / 2
1
3 / 1
2 / 1
1
4
1 / 2
1
4
1 / 2
2
r3[]
A
B
C
1
1
1
2
2
1
3
2
2
4
2
1
4
2
2
[2,14] just to mention two of them, or inspect the section on related work in our
previous articles [4,5]. To perform reasonably unbiased experiments, a software
engineer will expect the samples generated to be random in some situation-
speciﬁc sense. As a default, an engineer might want to achieve a uniform prob-
ability distribution over all possible samples. We followed such a wish in our
previous work [4,5] for the special case that relation instances of a relational
schema with just one functional dependency should be generated.
As already discussed but left open in [4], more sophisticated experimental
situations might demand for achieving probabilistic uniformity over some more
speciﬁc range of possibilities. For our case, dealing with both normalized and
non-normalized relational schemas, in this article we investigate how to achieve
uniformity over representatives of suitably deﬁned classes of relation instances.
Intuitively, two relation instances are seen to belong to a same class if they share
some structural properties of interest and might only diﬀer in the actually occur-
ring values taking from the declared domains. We will consider two approaches
to formalize this intuition, called equivalence and similarity, respectively.
The notion of equivalence focuses on the abstraction from concrete values:
formally, a relation instance r1 is deﬁned to be equivalent to a relation instance
r2 if there are bijective domain renamings under which r1 is mapped on r2.
Notably, such mappings preserve all equalities and inequalities among values
occurring under the same attribute.
Example 1. We assume a relational schema with an attribute A from which the
attribute B functionally depends, but not the additional attribute C. Table 1
shows three relation instances ri in array representations ri[], for which the order
of tuples (rows) is irrelevant since relation instances are deﬁned as (unordered,
duplicate-free) sets of tuples. The instances r1 and r2 are equivalent: leaving
the attribute C untouched, the B-domain renaming exchanges the values 1 and
2, and a cyclic (+1 mod 3) A-domain renaming of the values 1, 2 and 3 maps
the instance r2 on r1. The instances r1 and r3 are not equivalent: there is no
way to unify the ﬁrst three identical entries in the C-column of r1 with the
corresponding two identical and one diﬀerent entries in the C-column of r3.
The notion of similarity focuses on the sharing of structural properties, fully
abstracting from concrete and identiﬁable values: still roughly outlined, a rela-
tion instance r1 is deﬁned to be (combinatorially) similar to a relation instance

Sampling Representative Relation Instances
3
r2 if the following basic counting properties are the same: (i) the number of
diﬀerent values occurring under an attribute and (ii) the multiplicities of such
existing values, as well as (iii) the multiplicities of combinations and associations
between such existing values under diﬀerent attributes. Notably, the considered
counting properties of a relation instance can be understood as a solution to
a layered system of equalities and lessthan-relationships among (non-negative)
integer variables and some (non-negative) integer constants.
Example 2. Resuming Example 1, the relation instance r1 satisﬁes the following
combinatorial properties, which are intended to describe the internal structure
of the instance merely in terms of pure existences. Under attribute A, there are
ak A= 4 diﬀerent values; m1 = 3 of them occur only s1 = 1 often; m2 = 1 of them
occurs s2 = 2 often. Under attribute B, there are ak B = 2 diﬀerent values; one
of them occurs m1
1 = 1 often combined with an A-value that appears only once;
the other one appears m2
1 = 2 often combined with an A-value that appears
only once and m2
2 = 2 often together with an A-value that appears twice. Under
attribute C, there are ak C = 2 diﬀerent values; one of them together with that
B-value that appears only once occurs m1,1
1
= 1 often in combination/association
with an A-value with multiplicity 1; moreover, that C-value together the other
B-value occurs m2,1
1
= 2 often in combination/association with an A-value with
multiplicity 1, and m2,1
2
= 1 often in combination/association with an A-value
with multiplicity 2; the other C-value together with the latter B-value occurs
m2,2
2
= 1 often in combination/association with an A-value with multiplicity 2.
The reader might want to explicitly verify that the same combinatorial properties
hold for the relation instance r2, but this fact is an easy consequence of the
equivalence. In contrast, the combinatorial properties of the relation instance r3
diﬀer, since we have m1,1
1
= 1, m2,1
1
= 1, m2,1
2
= 1, m2,2
1
= 1 and m2,2
2
= 1.
The examples deal with a non-normalized schema, since the left-hand side
A of the assumed functional dependency does not form a primary key. For a
normalized schema the situation is essentially less complicated.
Example 3. Let r′
1 and r′
2 be the relation instances that would result from storing
only the ﬁrst four subtuples over the attributes A and B of the arrays r1[] and
r[]2, respectively. For A, we still have ak A = 4, such that there are m1 = 4 many
values with multiplicity s1 = 1; and for B, we still have ak B = 2, one value with
m1
1 = 1 and another one with m2
1 = 3.
For either kind of classes, our task is to exhibit a probabilistic generation pro-
cedure for relation instances of a given relational schema and having a required
size (number of tuples), such that each class is represented “uniformly”. A
straightforward interpretation of “uniformity” just means that each class is rep-
resented with exactly the same probability. An alternative interpretation means
that each class is represented with the probability that is proportional to the con-
tribution of the class to the set of all relation instances, which is the cardinality
of the class divided by the cardinality of the set of all instances. A prerequisite
for the task outlined so far is to characterize classes in more operational terms
such that the required counting and sampling can eﬀectively be computed.

4
M. Berens and J. Biskup
Section 2 speciﬁes the problem more formally. Section 3 deals with normal-
ized schemas, laying the combinatorial foundations for a probabilistic genera-
tion procedure. Section 4 describes our prototype implementation and indicates
possible optimizations. Section 5 discusses partial results and open issues for
non-normalized schemas. Section 6 summarizes and suggests future research.
2
Formal Problem Speciﬁcation
Throughout this work, we are mostly employing a kind of standard terminology,
see, e.g., [1]. We will consider relational schemas over a set of attributes U with
just one functional dependency fd : A →B. This semantic constraint requires
that values occurring under the left-hand side A uniquely determine the values
under the disjoint right-hand side B, not aﬀecting the possibly existing remain-
ing attributes in C = U \(A∪B). If C = ∅, then A forms a primary key and, thus,
such a schema is normalized. Otherwise, if C ̸= ∅, then A ∪C forms a unique
primary key and, thus, such a schema in non-normalized, satisfying the multi-
valued dependency mvd : A ↠B|C. Without loss of generality, we can assume
that the left-hand side, the right-hand side and, if existing, the set of remaining
attributes contain just one attribute, denoted by A, B and C, respectively.
For each attribute, the relational schemas considered will also specify a ﬁnite
domain, i.e., a set of values that might occur under the attribute. Notably, the-
oretical work on relational databases often assumes an inﬁnite global domain
for all attributes. In contrast: to come up with discrete probability spaces, we
assume ﬁnite domains; to emphasize the speciﬁc roles of the attributes (in appli-
cations presumably stemming from some real-world observations) we intuitively
think of assigning a diﬀerent domain as some kind of a “semantic type” to each
of the attributes. This assumption implies that a value occurring under some
attribute cannot occur under any of the other attributes and, thus, our notion
of equivalence will be based on domain-speciﬁc renamings.
Formally summarizing these settings, we consider either a normalized rela-
tional schema ⟨R({A : domA, B : domB}, {fd : A →B}⟩or a non-normalized
relational schema ⟨R({A : domA, B : domB, C : domC}, {fd : A →B}⟩. Each
attribute att has a ﬁnite domain domatt of cardinality katt ≥2. A relation
instance r is a (duplicate-free and unordered) set of tuples over the pertinent set
of attributes, complying with the declared domains and satisfying the functional
dependency. Further requiring a size n that is compatible with the declared
domains, i.e., with 1 ≤n ≤kA or 1 ≤n ≤kA · kC, respectively, let Ins be the
set of all relation instances that contain exactly n many (diﬀerent) tuples1.
In [4,5] we were primarily interested in uniformly generating each of the
relation instances r ∈Ins with the same probability 1/∥Ins∥. Now we aim to
probabilistically draw samples that are seen as a representative of a class of rela-
tion instances. Such classes are induced by some suitably deﬁned reﬂexive, sym-
1 To keep the notations simple, we deliberately refrain from explicitly denoting the
dependence of Ins—and related items—from the size n, as well as from the relational
schema, most relevantly including the attributes and their cardinalities.

Sampling Representative Relation Instances
5
metric and transitive relationship on Ins, namely either an equality/inequality-
preserving equivalence relationship ≡or a combinatorial similarity relation-
ship ≈, denoting such classes by [r]≡∈Ins/≡or [r]≈∈Ins/≈, respectively.
The equivalence relationship ≡⊆Ins ×Ins can uniformly be deﬁned for both
kinds of relational schemas, as indicated below. However, the concrete deﬁnition
of the similarity relationship will depend on the normalization status of the
relational schema and will later be speciﬁed in the pertinent sections.
Deﬁnition 1 (Equivalence by equality/inequality preservation).
1. A domain renaming for an attribute att is a function renatt : domatt −→
domatt that is both injective and surjective.
2. An instance renaming, based on some domain renamings renatt, is a function
ren : Ins −→Ins such that ren(r) :=
{ ν | there exists μ ∈r such that for all att : ν(att) = renatt(μ(att)) }.
3. The relation instances r1 and r2 of Ins are equivalent, r1 ≡r2, iﬀthere exists
an instance renaming ren such that ren(r1) = r2.
The task of sampling representative instances is then to design, formally ver-
ify and implement a probabilistic generation procedure with the following prop-
erties. On input of a relational schema together with a size n, the procedure
returns three outputs: a relation instance r ∈Ins of size n and a description
and the cardinality of its class [r], i.e., r is a representative of [r], deﬁned as
[r]≡∈Ins/≡or [r]≈∈Ins/≈, achieving a probability distribution over the
pertinent quotient Cla deﬁned as Ins/≡or Ins/≈, respectively, such that
– either each class is represented with the same probability 1/∥Cla∥
– or, alternatively, a class [r] is represented with probability ∥[r]∥/∥Ins∥, i.e.,
by its relative contribution to the set Ins of all instances.
Basically, solving this task requires us to determine the cardinalities involved,
i.e., the cardinality ∥Ins∥of the set of all relational instances, the cardinality
∥Cla∥of the quotient and the cardinalities ∥[r]∥of each of its classes. While the
ﬁrst cardinality has already been given in our previous work [4,5], the other ones
are central issues in this article.
3
Equivalence and Similarity for Normalized Schemas
In this section we treat the normalized case of a single relational schema with one
functional dependency in the simpliﬁed form, i.e., we consider a schema ⟨R({A :
domA, B : domB}, {fd : A →B}⟩with ﬁnite domains domatt of cardinality
katt ≥2. In this case, the left-hand side A of the functional dependency alone
forms a key. Hence, a relation instance of size n contains exactly n diﬀerent
key values taken from the domain domA of the attribute A, each of which is
independently combined with some value of the domain domB of the attribute
B, in particular allowing repeated occurrences of a value under the attribute B.
Accordingly, as already reported in [4,5], we can immediately count the possible
relation instances as expressed by the following proposition.

6
M. Berens and J. Biskup
Proposition 1 (Instance count for normalized schemas).
For a nor-
malized relational schema, the number of relation instances of size n with
1 ≤n ≤kA, satisfying the given functional dependency and complying with
the given domains, equals
kA
n

· kn
B .
(1)
While the notion of (equality/inequality-preserving) equivalence has gener-
ically been speciﬁed by Deﬁnition 1, we deﬁne the notion of (combinatorial)
similarity for normalized schemas as a specialization of the more general notion
for non-normalized schemas, to be later discussed in Sect. 5 below.
Deﬁnition 2 (Combinatorial similarity for normalized schemas).
For
a normalized relational schema and an instance size n with 1 ≤n ≤kA:2
1. A relation instance r satisﬁes a specialized A-structure of the (uniquely deter-
mined) kind SA = (s1 = 1, m1 = n) if there are exactly ak A := n many
diﬀerent values that form the active domain actA ⊆domA of the attribute A,
each of them occurring exactly s1 = 1 often, i.e., exactly once (as required by
the key property implied by the functional dependency).
2. A relation instance r satisﬁes a specialized B-structure of the kind SB =
(mj
1)j=1,...,ak B with mj
1 ≥1, 1 ≤ak B ≤min(kB, n) and 
j=1,...,ak B mj
1 = n,
if there are exactly ak B many diﬀerent values that form the active domain
actB ⊆domB of the attribute B, such that the multiplicities of their occur-
rences are given by the numbers mj
1 (in any order).
3. The set Str of all specialized structures is deﬁned as
{ ⟨SA, SB⟩| SA is specialized A-structure and SB is specialized B-structure }.
4. The relation instances r1 and r2 of Ins are similar, r1 ≈r2, iﬀthey satisfy
the same specialized structure str = ⟨SA, SB⟩∈Str.
To prove the coincidence of equivalence and similarity of relation instances
(seen as unordered sets of tuples) for normalized schemas, in the following we will
establish a one-to-one correspondence between the purely arithmetic counting
properties of specialized structures and a dedicated kind of suitably normalized
abstract arrays of value variables, instantiations of which will lead to representa-
tions of relation instances as (ordered) arrays. Figure 1 visualizes our concepts.
More speciﬁcally, for each size n of relation instances, we choose a ﬁxed
sequence of A-value variables α1, . . . , αn and a ﬁxed sequence of B-value vari-
ables β1, . . . , βn. For each specialized structure str = ⟨SA, SB⟩such that SA =
(s1 = 1, m1 = n) and SB = (mj
1)j=1,...,ak B with mj
1 ≥1, 1 ≤ak B ≤min(kB, n)
and 
j=1,...,ak B mj
1 = n, we create an abstract array vr[] with n rows for
tuples/pairs of value variables and two columns for the attributes A and B
and uniquely populate its n · 2 many entries with the chosen value variables as
follows:
2 For the sake of brevity, in the following we combine the deﬁnition of a “structure”
with the deﬁnition of “satisfaction” of a structure by a relation instance; we will
proceed similarly in Sect. 5.

Sampling Representative Relation Instances
7
stores
Instances
Arrays
Abstract Arrays
Specialized  Structures
r
r[]
vr[]
instantiates
populates
SA , SB 
sequentialize: display unordered set         abstract: replace values by
of pairs of an A-value and a B-value         attribute-specific           
as an array of dimension n 2                  domain-value variables
(completely) normalize:
satisfies
               partially normalize: 
from top to bottom, use
-  fixed sequence of different 
   sizes of explicitly formed 
   in ascending order of 
   A-value variables i
-  fixed sequence of different  
   B-value variables
j , 
   B-uniqueness areas 
B-uniqueness areas in ascending size 
sort/permute rows/tuples to form explicit    
(in any order on equal size)  
(in any order)
Fig. 1. Instances, arrays, abstract arrays and specialized structures.
1. vr[i, A] = αi for i = 1, . . . , n.
2. Assuming w.l.o.g. the numbers of SB = (mj
1)j=1,...,ak B being ascendingly
sorted according to numerical values,
vr[i, B] = βj for i = 
0≤j′<j mj′
1 + 1, . . . , 
0≤j′≤j mj′
1 and j = 1, . . . , ak B,
i.e., intuitively, we arrange the multiplicities of the existing B-values from top
to bottom and insert the B-value variables accordingly.
By abuse of language, we denote this normalized abstract array by vr[] =
[αi, βj]i=1,...,n; SB:j=1,...,ak B. Using this notation, we aim to capture the some-
how complex conditions for SB in a short way by just annotating the range
“j = 1, . . . , ak B” by the preﬁx “SB”. In the remainder, we will introduce further
similarly normalized arrays and then employ a similar denotation.
Evidently, we can store a relation instance r as an array r[], also of dimension
n × 2, ﬁlling the n rows with the tuples in any order. Following the same approach
as described above, we can partially normalize such an array by (i) permuting
rows, to form explicit B-uniqueness areas and (ii) sorting these areas according
to their sizes, in any order on equal size. In general, however, the result is not
unique, since there might be B-uniqueness areas of equal size.
An instantiation of an abstract array vr[], by assigning diﬀerent concrete
values to diﬀerent value variables in a domain-complying way, leads to a partially
normalized array r[]. Abstracting a partially normalized array r[], by replacing
values by value variables in a domain-complying way and according to the chosen
ﬁxed sequences from top to bottom, results in a unique abstract array vr[].

8
M. Berens and J. Biskup
Fig. 2. Visualization of the basic features of the proof of Theorem 1, with explicit
B-uniqueness areas marked in green. (Color ﬁgure online)
Theorem 1 (Coincidence of classes for normalized schemas).
For a
normalized schema, the following assertions about relation instances r1 and r2
mutually imply each other:
1. r1 ≡r2, as witnessed by some domain renamings.
2. r1 ≈r2, satisfying the same specialized structure ⟨SA, SB⟩.
3. r1 and r2 can be stored as arrays that are instantiations of the normalized
abstract array vr[] corresponding to the shared specialized structure ⟨SA, SB⟩.
Proof. The basic features of the proof are visualized by Fig. 2.
“1 ⇒2”: For any relation instance, its specialized structure only refers to prop-
erties that are invariant under equality/inequality-preserving domain renamings.
“2 ⇒3”: First, we store the relation instances r1 and r2 as partially normalized
arrays r1[] = [ai, bj]i=1,...,n; SB:j=1,...,ak B and r2[] = [a′
i, b′
j]i=1,...,n; SB:j=1,...,ak B,
respectively, both satisfying the given ⟨SA, SB⟩. Second, we generate the com-
pletely normalized abstract array vr[] = [αi, βj]i=1,...,n; SB:j=1,...,ak B, uniquely

Sampling Representative Relation Instances
9
corresponding to ⟨SA, SB⟩. Finally, the following instantiations witness the claim:
uni1(αi) := ai and uni1(βj) := bj as well as uni2(αi) := a′
i and uni2(βj) := b′
j.
“3 ⇒1”: The claimed equivalence of r1 and r2 is witnessed as follows:
renA(ai) := uni2(uni−1
1 (ai)) = uni2(αi) := a′
i;
renB(bj) := uni2(uni−1
1 (bj)) = uni2(βj) := b′
j.
⊓⊔
Given a speciﬁc size n and the cardinality kB, the preceding theorem imme-
diately implies that the pertinent number of classes is exactly determined by
the number of pertinent integer partitions. Various kinds of such partitions have
extensively been studied in mathematical and algorithmic number theory, see,
e.g., [7,11,13]. So our task of identifying the number of classes is closely related
to combinatorial counting problems for ﬁnite structures that are known to have
(in the required size n) exponentially growing solution spaces.
Corollary 1 (Class count for normalized schemas).
For a normalized
schema, for each size n of relation instances, the number of classes equals the
number of integer partitions of n with up to min(kB, n) parts, denoted by
p≤(n, min(kB, n)).
(2)
Proof. By Theorem 1, each class is uniquely determined by a specialized struc-
ture ⟨SA, SB⟩. There is only one choice for SA, namely (s1 = 1, m1 = n),
and each SB = (mj
1)j=1,...,ak B deﬁnes an integer partition of n of the speciﬁed
kind.
⊓⊔
Now, already knowing the number of instances by Proposition 1 and the
number of classes (for both equivalence and similarity) by Corollary 1 based on
Theorem 1, we are left with investigating the sizes of the individual classes.
Theorem 2 (Member count for normalized schemas).
For a normal-
ized schema, for the size n of relation instances, for the class [r] uniquely
determined by the specialized structure ⟨SA, SB⟩with SB = (mj
1)j=1,...,ak B,
as an integer partition having an (ascendingly ordered) multiplicity representa-
tion with diﬀerent summands mj1
1 , . . . , mj¯k
1 and multiplicities ¯j1, . . . ,¯j¯k such that

l=1,...,¯k mjl
1 · ¯jl = n and 
l=1,...,¯k ¯jl = ak B, the number of members equals
kA
n

· n! ·
 kB
ak B

· ak B!
¯k
l=1
(mjl
1 !)¯jl · ¯jl!
.
(3)
Proof. By Theorem 1, the class [r] is uniquely determined by the specialized
structure ⟨SA, SB⟩, which in turn uniquely determines a normalized abstract
array vr[] = [αi, βj]i=1,...,n; SB:j=1,...,ak B. Moreover, each relation instance in [r]
can be obtained as an instantiation of that vr[].
According to SB and its multiplicity representation, there are ak B many
B-uniqueness sections [range/set of (indices for) rows] (as determined by B-
uniqueness areas, of respective sizes mj
1): for j = 1, . . . , ak B, deﬁne Ij :=

10
M. Berens and J. Biskup
uni[Ij]
...
uni[ j]
uni[ j]
...
for   j 
Jl
m1      many rows
jl
m1      many rows
jl
jl  many B-uniqueness 
sections of size m1   
jl
l
.  .  .
. . .
. . .
k
.  .  .
1
uni[Ij]
Fig. 3. Visualization of instantiated B-multiplicity sections.
{ i | 
j′<j mj′
1 < i ≤
j′≤j mj′
1 }. Moreover, there are ¯k many B-multiplicity
sections [range/set of indices for B-uniqueness sections] (as determined by B-
uniqueness areas of the same size, of respective size ¯jl): for l = 1, . . . , ¯k, deﬁne
Jl := { j | 
l′<l ¯jl′ < j ≤
l′≤l ¯jl′ }.
Based on these deﬁnitions, we investigate the properties of any instantiation
uni of the normalized abstract array vr[]. First, for j = 1, . . . , ak B, let uni[Ij] :=
{ uni(αi) | i ∈Ij } be the set of A-values combined with the B-value uni(βj).
Second, for l = 1, . . . , ¯k, let uni[Jl] := { uni(βj) | j ∈Jl } be the set of B-values
within the B-multiplicity section determined by the B-uniqueness areas of the
same size ¯jl. The preceding settings are partially visualized by Fig. 3.
Finally, the transformation of the completely normalized abstract array into
a relation instance as a set of tuples can be described as follows:
– normalized abstract array: vr[] = [αi, βj]i=1,...,n; SB:j=1,...,ak B;
– instantiation: uni(αi) ∈domA and uni(βj) ∈domB;
– partially normalized array: uni(vr[]) = uni([αi, βj]i=1,...,n; SB:j=1,...,ak B)
:= [uni(αi), uni(βj)]i=1,...,n; SB:j=1,...,ak B;
– relation instance: stores( uni(vr[]) )
= stores( [uni(αi), uni(βj)]i=1,...,n; SB:j=1,...,ak B)
:= { (a, b) | exists i ∈{1, . . . , n}, exists j ∈{1, . . . , ak B} such that
i ∈Ij and (a, b) = (uni(αi), uni(βj)) }.
Accordingly, the function stores induces a set partition Uni/stores on the
set Uni of all pertinent instantiations, the blocks of which are deﬁned by
[uni1]stores := { uni2 | stores( uni1(vr[]) ) = stores( uni2(vr[]) ) }. The following
lemma completely characterizes these blocks.
Lemma 1. For instantiations uni1 and uni2 of the abstract array vr[]:
stores( uni1(vr[]) ) = stores( uni2(vr[]) ) [same relation instance generated]
iﬀfor all 1 ≤l ≤¯k [within each B-multiplicity section]:
1. { uni1[Ij] | j ∈Jl } = { uni2[Ij] | j ∈Jl }
[same sets of A-values within a B-uniqueness area];
2. { uni1(βj) | j ∈Jl } = { uni2(βj) | j ∈Jl }
[same set of B-values];

Sampling Representative Relation Instances
11
3. for all j, j′ ∈Jl:
uni1(βj) = uni2(βj′)
iﬀ
uni1[Ij] = uni2[Ij′]
[same combinations of a B-value with a set of A-values within a B-uniqueness
area].
Proof (Lemma). “⇒”: Consequences of the assumed set equality regarding the
deﬁnitions involved.
“⇐”: Let (a, b) ∈stores(uni1(vr[])). Then, as explained above, there exists
i ∈{1, . . . , n} and there exists j ∈{1, . . . , ak B} such that i ∈Ij and
(a, b) = (uni1(αi), uni1(βj)). Moreover, by the deﬁnition of the B-multiplicity
sections, there exists l ∈{1, . . . , ¯k} such that j ∈Jl. By the assumption, namely
the validity of the assertions 1, 2 and 3, there exist i′ ∈Ij and j′ ∈Jl such that
(uni1(αi), uni1(βj)) = (uni2(αi′), uni2(βj′)). In turn, again as explained above,
this implies that (uni2(αi′), uni2(βj′)) ∈stores(uni2(vr[])).
⊓⊔
To ﬁnalize the proof of the theorem, we justify the numerator and the denomi-
nator of the quotient (3) in turn.
The numerator counts the number ∥Uni∥of pertinent instantiations. In fact,
there are
kA
n

possibilities to select an active domain for the attribute A, the
elements of which can be assigned to the A-value variables occurring in vr[] in n!
many ways and, independently, there are
 kB
ak B

possibilities to select an active
domain for the attribute B, the elements of which can be assigned to the B-value
variables occurring in vr[] in ak B! many ways.
The denominator counts the number ∥[uni]stores∥of pertinent instantiations
in each of the blocks [uni]stores of the set partition Uni/stores. In fact, if we ﬁx
a particular instantiation uni of the abstract array vr[], then the lemma tells us
precisely which other instantiations yield the same relation instance, implying
that this common relation instance remains invariant exactly under the following
mutually independent permutations:
– For the ¯k many B-multiplicity sections numbered by 1 ≤l ≤¯k, for the
included ¯jl many B-uniqueness areas denoted by j ∈Jl, permute the A-
values within uni[Ij], which has size mjl
1 .
– For the ¯k many B-multiplicity sections numbered by 1 ≤l ≤¯k, permute the
included ¯jl many B-uniqueness areas as a whole.
The denominator just counts these permutations.
The quotient is then justiﬁed by the mutual disjointness of the blocks of the
set partition Uni/stores.
⊓⊔
4
Probabilistic Generation for Normalized Schemas
Depending on the variant of uniformity, we may choose diﬀerent probabilis-
tic generation procedures. In the following, we describe our implementations of
both versions and discuss respective optimizations. Further, we brieﬂy assess the
runtime performance of our prototype implementation of the second and more
critical case of uniformity.

12
M. Berens and J. Biskup
Fig. 4. The discrete probability distribution Pr(str) for the parameter conﬁgurations
n = kA = kB = 5 (red squares) and n = kA = kB = 20 (yellow diamonds) and
n = kA = 20, kB = 5 (blue pentagons) and n = kA = kB = 25 (green dots). p(n)
denotes the (unrestricted) partition function. Both axes are log scaled. (Color ﬁgure
online)
4.1
Drawing a Class [r] with Proportional Probability
If our goal is to sample relation instances with uniform probability 1/∥[r]∥from
the respective class [r], where the class was selected with a probability propor-
tional to the cardinality ∥[r]∥/∥Ins∥of the class, a simple generation procedure
can be used. Instead of determining the class ﬁrst, we may generate the relation
instance directly from the set of all instances with uniform probability 1/∥Ins∥.
This can be achieved by ﬁrst drawing a random active domain actA from domA,
with ∥actA∥= n. Many algorithms for the generation of random subsets exist
and may perform diﬀerently, depending on the ratio of ∥actA∥/∥domA∥. To
illustrate runtimes, drawing a subset of 500000 values from a domain with cardi-
nality 1000000 takes about 5.2 ms. See [4] for other examples and a more detailed
discussion on this topic.
After selecting the active domain actA, we form tuples by combining every
value of actA with a B-value, drawn uniformly at random from domB. This
procedure can generate any relation instance r with probability 1/∥Ins∥. Further,
the specialized structure str of the produced instance r, which can be derived
from its value distributions, is selected with probability Pr(str) = ∥[r]∥/∥Ins∥,
since ∥[r]∥many instances share the same structure and all those instances are
equally likely.
Figure 4 shows the discrete probability distribution of the specialized struc-
tures for the following parameter conﬁgurations: n = kA = kB = 5 and
n = kA = 20, kB = 5 and n = kA = kB = 20 and n = kA = kB = 25.
Dashed lines between the probabilities are added for visual clarity. The special-
ized structures str ∈Str are given in lexicographically ascending order along
the x-axis, as they are produced by Gupta’s partition generation algorithm [9].
For example, the respectively smallest partition/structure, n itself, is located to
the far-left side, and the partitions 20 = (19, 1), (1, 1) and 25 = (22, 1), (3, 1)
are shown at location 11 on the x-axis (just right of position 101). The plot

Sampling Representative Relation Instances
13
indicates that a class [r] contains more instances (has a higher probability), if
akB is large and if the B-uniqueness areas are similar in size. For example, the
class with SB = (19, 1), (1, 1) (small akB; non-similar sizes) has cardinality 400,
whereas the class with SB = (7, 2), (6, 1) (larger akB; similar sizes) already con-
tains about 400 · 107 relation instances. In particular, this number—as most of
the other ones depicted in Fig. 4—indicates that in general we are dealing with
pretty large probability spaces.
4.2
Drawing a Class [r] with Uniform Probability
If we are instead interested in generating relation instances from a class [r]
selected with uniform probability 1/∥Cla∥= 1/p≤(n, min(kB, n)), we require
the explicit generation of the representing structure str.
Conceptually, but not regarding the computational complexity, this approach
is comparable to the one used for random relation instances of non-normalized
schemas, presented in our previous work [4,5]. It has been based on the counting
formula (4–5) for relation instances of such schemas recalled and explained in
Sect. 5 below. Roughly outlined, the procedure explicitly generates the desired
probability distribution by computing all relevant structures ﬁrst and then deter-
mining their probabilities from the respective instance counts. Due to the expo-
nentially increasing number of structures, generating the distribution for n > 90
quickly becomes infeasible. See [4] for details.
In our case, however, the desired distribution is uniform (i.e., the instance
counts do not matter), which enables eﬃcient procedures for the random gener-
ation of a specialized structure str ∈Str. Since SA is trivial in the normalized
case, we get structures by drawing a random integer partition as SB. Although a
thorough treatment of the more general ﬁeld of eﬃcient random integer partition
generation is beyond the scope of this article, we will now describe one simple
example and its adoption to our context.
4.3
Eﬃcient Random Generation of a Specialized B-Structure SB
One algorithm is given by Fristedt [8]: Treating the integer n itself as a random
variable, the iterative algorithm generates partitions by independently drawing
the number of occurrences of every possible part. Speciﬁcally, the number of
occurrences of a part mj
1 is drawn from a geometric distribution with parameter
1−exp(−mj
1π/(
√
6n)). If the sum of all drawn parts equals n, an unbiased integer
partition of n was found. If the drawn parts do not sum up to n, the algorithm
is repeated. The procedure is expected to make 2
4√
6n3/4 proposals until a valid
partition is found [3].
If kB < n, however, not all partitions qualify. This constraint can be imple-
mented by restricting the number of parts in a partition, because the number
of parts in SB corresponds to the active domain size ak B = ∥actB∥. In general,
there are p≤(n, min(kB, n)) valid integer partitions (see Corollary 1). Various
approaches generate restricted partitions with at most min(kB, n) parts. One
way is to adopt Fristedt’s algorithm by simply ignoring the multiplicities of the

14
M. Berens and J. Biskup
invalid parts of size larger then akB. Since the conjugate of an integer partition
[12] with largest part akB corresponds to a partition with exactly akB parts, the
adopted algorithm produces the desired partitions uniformly at random.
On a Intel Xeon Gold 6226 CPU with 2.7 GHz, generating relation instances
with n = 1000 and kA = kB = 100000 with our (single-core) Python- and
Numpy-based [10] implementation takes about 366.517 ms (average over 200
runs). Notably, for n = 1000, the step of drawing a valid structure SB alone took
366.390 ms. The runtime for increasing n progresses in a slightly below quadratic
fashion, as indicated by the expected number of proposals, and suggests that it
is generally possible to generate even very large instances. For a faster imple-
mentation, more eﬃcient random partition generation algorithms are necessary.
For example, Arratia and DaSalvo [3] introduce a more involved, probabilistic
divide-and-conquer-based approach, which requires only about 2π
4√
6n1/4 pro-
posals. Further, they describe an implementation that generates unrestricted
partitions up to n = 258, before running out of memory.
5
Results and Issues for Non-normalized Schemas
In this section we discuss partial results and open issues for the non-normalized
case of a single relational schema with one functional dependency in the simpli-
ﬁed form, i.e., we consider a schema ⟨R({A : domA, B : domB, C : domC}, {fd :
A →B}⟩with ﬁnite domains domatt of cardinality katt ≥2. For this case,
we tacitly adapt the previously introduced notations. Moreover, the cardinality
∥Ins∥has already been determined in [4,5], as shown next.
Proposition 2 (Instance count for non-normalized schemas).
For a
non-normalized relational schema, the number of relation instances of size n
with 1 ≤n ≤kA · kC that satisfy the given functional dependency and comply
with the given domains equals
	
⌈n
kC ⌉≤akA≤min(kA,n)
	
1≤k≤n ,
(s1,m1),...,(si,mi),...,(sk,mk):
1≤si<si+1≤kC ;
1≤mi≤n ;
n=k
i=1 si · mi ;
akA=k
i=1 mi
(4)

k

i=1
kA −
1≤j<i mj
mi

·
kakA
B
·
k

i=1
kC
si
mi
.
(5)
In the counting formula (4–5), in the ﬁrst summation of (4) the term akA
ranges over the possible cardinalities of the active domain actA of A, and
in the second summation of (4) the suitably normalized size–multiplicity-list
(si, mi)i=1,...,k (for A-uniqueness areas) ranges over all possibilities to let mi
elements of the active domain of A appear exactly si often. Each accepted pair
of akA and (si, mi)i=1,...,k together (i.e., jointly satisfying the conditions in the
two summations of (4)) constitutes a solution to a restricted integer partition

Sampling Representative Relation Instances
15
problem for the number n, and also vice versa. We shortly refer to such a pair
as an n-partition or an A-structure, denoted as SA = (si, mi)i=1,...,k leaving akA
implicit.
To prepare for a general deﬁnition of (combinatorial) similarity, let us inspect
a relation instance r, the active domains of which are enumerated (in some
ﬁxed order) as (ae)e=1,...,akA, (bj)j=1,...,akB and (cd)d=1,...,akC , respectively. Where
convenient, we also employ alternative indices for A-values and enumerate the
active domain of A as a1,1, . . . a1,m1, . . . , ai,1, . . . ai,mi, . . . , ak,1, . . . ak,mk.
Regarding the attribute A, let mi be the multiplicity of A-uniqueness areas
of size si, containing some diﬀerent values ai,1, . . . , ai,mi, respectively, each of
them occurring si often. Thus3, the inspected relation instance r satisﬁes
the A-structure SA = (si, mi)i=1,...,k (with implicit size akA of actA)
for which, as already stated in (4), the following properties hold:
⌈n
kC
⌉≤akA ≤min(kA, n)
and
1≤k≤n ;
1≤si<si+1≤kC ;
1≤mi≤n ;
n=k
i=1 si · mi ;
akA=k
i=1 mi .
(6)
Regarding the attribute B, already given the A-structure of r, let mj
i be the
number of combinations of partial B-uniqueness areas (deﬁned by A-uniqueness
areas) containing the value bj with an A-uniqueness area of size si (for short,
A-B-combinations). Thus, the relation instance r inspected satisﬁes
the B-structure SB = (mj
i)i=1,...,k; j=1,...,ak B (with implicit size akB of actB),
for which the following properties hold:
1 ≤akB ≤min(kB, ak A)
and
0≤mj
i ≤mi ;
akB
j=1 mj
i =mi ;
1≤k
i=1 mj
i .
(7)
Regarding the attributes B and C jointly, already given the A-structure and
the B-structure of r, let mj,d
i
be the number of occurrences of the subtuple
(bj, cd) where the second component cd occurs in a C-diversity area associated
with diﬀerent A-uniqueness areas (deﬁning partial B-uniqueness areas) of size
si (for short: A-(Bv, Cv)-multiplicities), equivalently the number of combina-
tions/associations of the (B, C)-uniqueness area containing (bj, cd) with diﬀerent
A-uniqueness areas of size si. Thus, the relation instance r inspected satisﬁes
the BC-structure SBC = ( mj,d
i )i=1,...,k; j=1,...,akB; d=1,...,akC
(with implicit size akC of actC), for which the following properties hold:
sk ≤akC ≤min(kC, n)
and
0≤
mj,d
i
≤mj
i
(≤mi redundant!) ;
( 
1≤j≤akB 
mj,d
i
≤mi redundant!) ;

1≤d≤akC 
mj,d
i
=si·mj
i ;
(
1≤j≤akB

1≤d≤akC 
mj,d
i
=si·mi redundant!) ;
( 
1≤i≤k

1≤j≤akB

1≤d≤akC 
mj,d
i
=n redundant!) ;
1≤
1≤i≤k

1≤j≤akB 
mj,d
i
(≤akA redundant!) .
(8)
3 As in Deﬁnition 2, here and in the following we integrate the deﬁnition of a “struc-
ture” with the deﬁnition of “satisfaction”.

16
M. Berens and J. Biskup
Conversely, we can treat (6), (7) and (8) as a layered system ELTS of equal-
ities and less–than-relationships among integer variables and some constants
and additive operations on such terms. Accordingly, the combinatorial structure
⟨SA, SB, SBC⟩of the relation instance r inspected constitutes a speciﬁc solution
to ELTS. Further on, we will consider each solution to ELTS as a structure,
where the sequencing of the lists for B, C and BC regarding the respective
index does not matter4 (all sequences are seen to be equivalent).
Deﬁnition 3 (Combinatorial similarity for non-normalized schemas).
For a non-normalized relational schema and an instance size n with 1 ≤n ≤
kA · kC:
1. The set StrSS of structures as system solutions is deﬁned as
{ ⟨SA, SB, SBC⟩| SX solves stepwise (6), (7), (8), respectively }.
2. A relation instance r ∈Ins satisﬁes ⟨SA, SB, SBC⟩∈StrSS if there are exactly
ak A, ak B and ak C many diﬀerent values that form the active domains actA ⊆
domA of the attribute A, actB ⊆domB of the attribute B and actC ⊆domC
of the attribute C, respectively, such that (6), (7), and (8) stepwise hold for
suitable enumerations of the active domains of r.
3. The relation instances r1 and r2 of Ins are similar, r1 ≈r2, if they satisfy
the same structure as system solution str = ⟨SA, SB, SBC⟩∈StrSS.
The following theorem conﬁrms that each solution to ELTS can indeed be
seen as the description of the structure of some relation instances.
Theorem 3 (Satisﬁability of system solutions for non-normalized sche-
mas). For each structure as system solution str = ⟨SA, SB, SBC⟩∈StrSS there
exists a relation instance r ∈Ins that satisﬁes str.
Proof. (sketch) We construct satisfying relation instances in two steps: (i) for a
given ⟨SA, SB, SBC⟩, we populate an abstract array vr[] with rows i = 1, . . . , n
and columns att = A, B, C by domain-value variables; (ii) we instantiate the
domain-value variables with diﬀerent values of the respective domains, getting
an array r[] that stores a relation instance r of the required kind.
ad (i): The deﬁnition of vr[] is given attribute-wise from A over B to C accord-
ing to the given A-structure, B-structure and BC-structure in turn, as roughly
outlined in the following.
According to SA, the A-column of vr[] is populated from top to bottom by the
A-value variables α1,1, . . . α1,m1, . . . , αi,1, . . . αi,mi, . . . , αk,1, . . . αk,mk, such that
an A-value variable of the form αi,· is placed si often in sequence in the pertinent
A-uniqueness area. According to SB, the B-column of vr[] is populated by the
B-value variables β1, . . . , βakB , such that, for i = 1, . . . , k, the B-value variable
βj is placed in combination with mj
i many A-uniqueness areas of size i, for each
of them si often in sequence, thus forming the pertinent combined partial B-
uniqueness area (for the sake of satisfying the functional dependency). According
4 However, since such a list might contain duplicates, a usual set notation would not
be appropriate; instead we could use multi-sets.

Sampling Representative Relation Instances
17
Table 2. Two arrays r1[] and r2[] storing relation instances r1 and r2 that share the
combinatorial structure as system solution but are not equivalent.
r1[]
A
B
C
1
1
1
1
1
2
2
1
1
2
1
2
3
1
3
3
1
4
r2[]
A
B
C
1
1
1
1
1
4
2
1
1
2
1
2
3
1
2
3
1
3
to SBC, the C-column of vr[] is populated by the C-value variables γ1, . . . , γakC ,
such that in total 1 ≤
1≤i≤k

1≤j≤akB mj,d
i
copies of γd are placed, following a
circularly sequential choice/selection strategy: While repeatedly [in total si often]
circling through the mj
i many determined combinations of an A-uniqueness area
of size si with a partial B-uniqueness area also of size si, as w.l.o.g. numbered
1, . . . , mj
i, in the round h visiting the h-th entries (rows) of the areas.
ad (ii): We just instantiate each of the pairwise diﬀerent A-value variables αi,e
with a diﬀerent value ai,e ∈domA, each of the pairwise diﬀerent B-value vari-
ables βj with a diﬀerent value bj ∈domB, and each of the pairwise diﬀerent
C-value variables γd with a diﬀerent value cd ∈domC. By the construction, the
resulting array r[] stores a relation instance r ∈Ins.
⊓⊔
Proposition 3 (Similarity strictly generalizes equivalence for non-nor-
malized schemas).
For a non-normalized schema, if two relation instances
are equivalent then they are similar. Moreover, there exist relation instances r1
and r2 that are similar but not equivalent.
Proof (sketch). The ﬁrst claim follows from the invariance of the combinato-
rial properties of structures under equality/inequality-preserving renamings. The
second claim is witnessed by the example shown in Table 2. The similarity can
straightforwardly be veriﬁed. The non-equivalence can be formally proved by
some subtle case considerations that—though the purely combinatorial require-
ments of the BC-structure SBC = (2, 2, 1, 1) are satisﬁed by both relation
instances— the respective sets of associations, each formed by an A-value with
two C-values, cannot faithfully be mapped on each other.
⊓⊔
Accordingly, the cardinalities ∥Cla∥and ∥[r]∥for the classes diﬀer for the
two notions of classes, and their determination for equivalence cannot directly
be based on the determination for similarity, as successfully done for normal-
ized schemas. Moreover, even for similarity an explicit determination would be
much more involved, since the corresponding integer partitions are much more
involved: they have to be layered like ELTS, possibly contain 0′s as summands,
and on each layer forming a suitable collection of somehow “compatible” ones.

18
M. Berens and J. Biskup
Considering an imagined relation instance r ∈Ins, we can abstract from an
array representation r[] of the set r by replacing concrete values by attribute-
speciﬁc domain-value variables, resulting in an abstract array vr[]. Conversely, we
can again directly instantiate vr[] by assigning some diﬀerent domain-complying
values to the domain-value variables in vr[], getting an array representation ˜r[]
of a relation instance ˜r that is obviously equivalent with r. Furthermore, deﬁning
vr as the set of rows (tuples of variables) of the abstract array vr[], by the same
assignment we directly get the relation instance ˜r. We extend such consideration
by identifying a relationship to the notion of set uniﬁcation [6].
Theorem 4 (Equivalence, uniﬁcation and matching for non-normalized
schemas). Let r1 and r2 be relation instances with some array representations r1[]
and r2[] and corresponding abstract arrays vr 1[] and vr 2[], deﬁning sets vr 1 and vr 2
of rows (tuples of variables), respectively. Then the following assertions mutually
imply each other:
1. r1 ≡r2, by a (bijective and thus injective) renaming ren with ren(r1) = r2 (here:
set-equality).
2. ren(r1[]) = r2[]per, with an (injective) renaming ren and a permutation per of
the array rows (tuples) (here: array-equality).
3. vren(vr 1[]) = vr 2[]per, with an injective variable renaming vren and a permu-
tation per of the array rows (tuples of variables) (here: array-equality).
4. vr 1 and vr 2 are injectively set-uniﬁable, by an injective set-uniﬁer uni with
uni(vr 1) = vr 2 (here: set-equality).
5. vr 1 and r2 are injectively set-matching, by an injective set-matcher mat with
mat(vr 1) = r2 (here: set-equality).
Proof. (sketch) Basically, the implications can be circularly veriﬁed by suitably
applying the pertinent deﬁnitions.
⊓⊔
6
Conclusions
We aimed at probabilistically generating representatives of classes of relation
instances of a relational schema with one functional dependency and speciﬁed
attribute domains. Such classes are deﬁned by means of either invariance under
bijective functions on the domains or sharing of combinatorial counting proper-
ties related to the existence and multiplicities of values and their combinations
and associations.
For normalized schemas, the two notions of classes coincide, the cardinalities
of the set of all instances, the set of all classes and the single classes, respec-
tively, can eﬀectively be determined, and the designed generation procedure
has formally been veriﬁed. The availability of eﬃcient random integer partition
generation algorithms indicates general feasibility, up to suitable optimizations.
The main optimization issues to be further explored regard options for selecting
active domains and integer partitions uniformly at random.
In contrast, for non-normalized schemas the two notions diﬀer and, as a con-
sequence, the further features (as stated above) remain mostly open. We suggest

Sampling Representative Relation Instances
19
future research to understand the deeper reasons for the observed diﬀerences, to
identify further manageable cases, and to improve the combinatorial analysis.
As already remarked in our previous work [4,5], further research should also
treat more advanced relational schemas including, e.g., any number of suitable
equality-generating and tuple-generating dependencies, possibly spanning over
more than one relation scheme. For such situations, but also for the currently
considered quite simple case, we could reﬁne the investigations by studying mean-
ingful variants of the notion of equivalence.
Acknowledgements. We sincerely thank the anonymous reviewers for their careful
evaluations and constructive remarks.
References
1. Abiteboul, S., Hull, R., Vianu, V.: Foundations of Databases. Addison-Wesley,
Reading (1995)
2. Arasu, A., Kaushik, R., Li, J.: Data generation using declarative constraints. In:
Sellis, T.K., Miller, R.J., Kementsietsidis, A., Velegrakis, Y. (eds.) SIGMOD 2011,
pp. 685–696. ACM (2011)
3. Arratia, R., DeSalvo, S.: Probabilistic divide-and-conquer: a new exact simulation
method, with integer partitions as an example. Comb. Probab. Comput. 25(3),
324–351 (2016)
4. Berens, M., Biskup, J., Preuß, M.: Uniform probabilistic generation of relation
instances satisfying a functional dependency. Inform. Syst. 103, 101848 (2021)
5. Biskup, J., Preuß, M.: Can we probabilistically generate uniformly distributed
relation instances eﬃciently? In: Darmont, J., Novikov, B., Wrembel, R. (eds.)
ADBIS 2020. LNCS, vol. 12245, pp. 75–89. Springer, Cham (2020). https://doi.
org/10.1007/978-3-030-54832-2 8
6. Dovier, A., Pontelli, E., Rossi, G.: Set uniﬁcation. Theory Pract. Log. Program.
6(6), 645–701 (2006)
7. Flajolet, P., Sedgewick, R.: Analytic Combinatorics. Cambridge University Press,
Cambridge (2009)
8. Fristedt, B.: The structure of random partitions of large integers. Trans. Am. Math.
Soc. 337(2), 703–735 (1993)
9. Gupta, U.I., Lee, D.T., Wong, C.K.: Ranking and unranking of B-trees. J. Algo-
rithms 4(1), 51–60 (1983)
10. Harris, C.R., Millman, K.J., van der Walt, S.J., et al.: Array programming with
numpy. Nature 585(7825), 357–362 (2020)
11. Nijenhuis, A., Wilf, H.S.: A method and two algorithms on the theory of partitions.
J. Comb. Theory Ser. A 18(2), 219–222 (1975)
12. Stanley, R.P.: Enumerative Combinatorics, vol. 1, 2nd edn. Cambridge University
Press, Cambridge (2012)
13. Stojmenovic, I., Zoghbi, A.: Fast algorithms for generating integer partitions. Int.
J. Comput. Math. 70(2), 319–332 (1998)
14. Transaction Processing Performance Council, TPC: TCP Benchmarks & Bench-
mark Results. http://www.tpc.org

On the Expressive Power
of Message-Passing Neural Networks
as Global Feature Map Transformers
Floris Geerts1
, Jasper Steegmans2(B)
, and Jan Van den Bussche2
1 University of Antwerp, Antwerp, Belgium
2 Hasselt University, Hasselt, Belgium
jasper.steegmans@uhasselt.be
Abstract. We investigate the power of message-passing neural networks
(MPNNs) in their capacity to transform the numerical features stored in
the nodes of their input graphs. Our focus is on global expressive power,
uniformly over all input graphs, or over graphs of bounded degree with
features from a bounded domain. Accordingly, we introduce the notion of
a global feature map transformer (GFMT). As a yardstick for expressive-
ness, we use a basic language for GFMTs, which we call MPLang. Every
MPNN can be expressed in MPLang, and our results clarify to which
extent the converse inclusion holds. We consider exact versus approxi-
mate expressiveness; the use of arbitrary activation functions; and the
case where only the ReLU activation function is allowed.
Keywords: Closure under concatenation · Semiring provenance
semantics for modal logic · Query languages for numerical data
1
Introduction
An important issue in machine learning is the choice of formalism to represent the
functions to be learned [24,25]. For example, feedforward neural networks with
hidden layers are a popular formalism for representing functions from Rn to Rp.
When considering functions over graphs, graph neural networks (GNNs) have
come to the fore [18]. GNNs come in many variants; in this paper, speciﬁcally, we
will work with the variant known as message-passing neural networks (MPNNs)
[12].
MPNNs compute numerical values on the nodes of an input graph, where,
initially, the nodes already store vectors of numerical values, known as features.
Such an assignment of features to nodes may be referred to as a feature map
on the graph [15]. We can thus view an MPNN as representing a function that
maps a graph, together with a feature map, to a new feature map on that graph.
We refer to such functions as global feature map transformers (GFMTs).
Of course, MPNNs are not intended to be directly speciﬁed by human design-
ers, but rather to be learned automatically from input–output examples. Still,
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 20–34, 2022.
https://doi.org/10.1007/978-3-031-11321-5_2

On the Expressive Power of Message-Passing Neural Networks
21
MPNNs do form a language for GFMTs. Thus the question naturally arises:
what is the expressive power of this language?
We believe GFMTs provide a suitable basis for investigating this question
rigorously. The G for ‘global’ here is borrowed from the terminology of global
function introduced by Gurevich [16,17]. Gurevich was interested in deﬁning
functions in structures (over some ﬁxed vocabulary) uniformly, over all input
structures. Likewise, here we are interested in expressing GFMTs uniformly over
all input graphs. We also consider inﬁnite subclasses of all graphs, notably, the
class of all graphs with a ﬁxed bound on the degree.
As a concrete handle on our question about the expressive power of MPNNs,
in this paper we deﬁne the language MPLang. This language serves as a yardstick
for expressing GFMTs, in analogy to the way Codd’s relational algebra serves as
a yardstick for relational database queries [2]. Expressions in MPLang can deﬁne
features built arbitrarily from the input features using three basic operations also
found in MPNNs:
1. Summing a feature over all neighbors in the graph, which provides the
message-passing aspect;
2. Applying an activation function, which can be an arbitrary continuous func-
tion;
3. Performing arbitrary aﬃne transformations (built using constants, addition,
and scalar multiplication).
The diﬀerence between MPLang-expressions and MPNNs is that the latter must
apply the above three operations in a rigid order, whereas the operations can be
combined arbitrarily in MPLang. In particular, every MPNN is readily express-
ible in MPLang.
Our research question can now be made concrete: is, conversely, every GFMT
expressible in MPLang also expressible by an MPNN? We oﬀer the following
answers.
1. We begin by considering the case of the popular activation function ReLU
[3,13]. In this case, we show that every MPLang expression can indeed be
converted into an MPNN (Theorem 1).
2. When arbitrary activation functions are allowed, we show that Theorem 1
still holds in restriction to any class of graphs of bounded degree, equipped
with features taken from a bounded domain (Theorem 2).
3. Finally, when the MPNN is required to use the ReLU activation function, we
show that every MPLang expression can still be approximated by an MPNN;
for this result we again restrict to graphs of bounded degree, and moreover
to features taken from a compact domain (Theorem 3).
This paper is organized as follows. Section 2 discusses related work. Section 3
deﬁnes GFMTs, MPNNs and MPLang formally. Sections 4, 5 and 6 develop our
Theorems 1, 2 and 3, respectively. We conclude in Sect. 7.
In this paper, proofs of some lemmas and theorems are only sketched.
Detailed proofs will be given in the journal version of this paper. Certain con-
cepts and arguments assume some familiarity with real analysis [23].

22
F. Geerts et al.
2
Related Work
The expressive power of GNNs has received a great deal of attention in recent
years. A very nice introduction, highlighting the connections with ﬁnite model
theory and database theory, has been given by Grohe [15].
One important line of research is focused on characterizing the distinguishing
power (also called separating power) of GNNs, in their many variants. There,
one is interested in the question: given two graphs, when can they be distin-
guished by a GNN? This question is closely related to strong methods for graph
isomorphism checking, and more speciﬁcally, the Weisfeiler-Leman algorithm. A
recent overview has been given by Morris et al. [21].
Another line of research has as goal to extend classical results on the “uni-
versality” of neural networks [22] to graphs [1,4]. (There are close connections
between this line of research and the one just mentioned on distinguishing power
[11].) These results consider graphs with a ﬁxed number n of nodes; functions on
graphs are shown to be approximable by appropriate variants of GNNs, which,
however, may depend on n.
A notable exception is the work by Barcel´o et al. [6,7], which inspired our
present work. Barcel´o et al. were the ﬁrst to consider expressiveness of GNNs
uniformly over all graphs (note, however, the earlier work of Hella et al. [19] on
similar message-passing distributed computation models). Barcel´o et al. focus
on MPNNs, which they ﬁt in a more general framework named AC-GNNs, and
they also consider extensions of MPNNs. They further focus on node classiﬁers,
which, in our terminology, are GFMTs where the input and output features are
boolean values. Using the truncated ReLU activation function, they show that
MPNNs can express every node classiﬁers expressible in graded modal logic (the
converse inclusion holds as well).
In a way, our work can be viewed as generalizing the boolean setting consid-
ered by Barcel´o et al. to the numerical setting. Indeed, the language MPLang
can be viewed as giving a numerical semantics to positive modal logic with-
out conjunction, following the established methodology of semiring provenance
semantics for query languages [9,14], and extending the logic with application
of arbitrary activation functions. By focusing on boolean inputs and outputs,
Barcel´o et al. are able to capture a stronger logic than our positive modal logic,
notably, by expressing negation and counting.
We note that MPLang is a sublanguage of the Tensor Language deﬁned
recently by one of us and Reutter [11]. That language serves to unify several
GNN variants and clarify their separating power and universality (cf. the ﬁrst
two lines of research on GNN expressiveness mentioned above).
Finally, one can also take a matrix computation perspective, and view a graph
on n nodes, together with a d-dimensional feature map, as an n × n adjacency
matrix, together with d column vectors of dimension n. To express GFMTs, one
may then simply use a general matrix query language such as MATLANG [8].
Indeed, results on the distinguishing power of MATLANG fragments [10] have
been applied to analyze the distinguishing power of GNN variants [5]. Of course,

On the Expressive Power of Message-Passing Neural Networks
23
the speciﬁc message-passing nature of computation with MPNNs is largely lost
when performing general computations with the adjacency and feature matrices.
3
Models and Languages
In this section, we recall preliminaries on graphs; introduce the notion of global
feature map transformer (GFMT); formally recall message-passing neural net-
works and deﬁne their semantics in terms of GFMTs; and deﬁne the language
MPLang.
3.1
Graphs and Feature Maps
We deﬁne a graph as a pair G = (V, E), where V is the set of nodes and E ⊆V ×V
is the edge relation. We denote V and E of a particular graph G as V (G) and
E(G) respectively. By default, we assume graphs to be ﬁnite, undirected, and
without loops, so E is symmetric and antireﬂexive. If (v, u) ∈E(G) then we call
u a neighbor of v in G. We denote the set of neighbors of v in G by N(G)(v).
The number of neighbors of a node is called the degree of that node, and the
degree of a graph is the maximum degree of its nodes. We use G to denote the
set of all graphs, and Gp, for a natural number p, to denote the set of all graphs
with degree at most p.
For a natural number d, a d-dimensional feature map on a graph G is a
function χ : V (G) →Rd, mapping the nodes to feature vectors. We use Feat(G, d)
to denote the set of all possible d-dimensional feature maps on G. Similarly, for
a subset X of Rd, we write Feat(G, d, X) for the set of all feature maps from
Feat(G, d) whose image is contained in X.
3.2
Global Feature Map Transformers
Let d and r be natural numbers. We deﬁne a global feature map transformer
(GFMT) of type d →r, to be a function T : G →(Feat(G, d) →Feat(G, r)),
where G ∈G is the input of T. Thus, if G is a graph and χ is a d-dimensional
feature map on G, then T(G)(χ) is an r-dimensional feature map on G. We call
d and r the input and output arity of T, respectively.
Example 1. We give a few simple examples, just to ﬁx the notion, all with output
arity 1. (GFMTs with higher output arities, after all, are just tuples of GFMTs
with output arity 1.)
1. The GFMT T1 of type 2 →1 that assigns to every node the average of its
two feature values. Formally, T1(G)(χ)(v) = (x + y)/2, where χ(v) = (x, y).
2. The GFMT T2 deﬁned like T1, but taking the maximum instead of the aver-
age.
3. The GFMT T3 of type 1 →1 that assigns to every node the maximum of the
features of its neighbors. Formally, T3(G)(χ)(v) = max{χ(u) | u ∈N(G)(v)}.

24
F. Geerts et al.
4. The GFMT T4 of type 1 →1 that assigns to every node v the sum, over
all paths of length two from v, of the feature values of the end nodes of the
paths. Formally,
T4(G)(χ)(v) =

(v,u)∈E(G)

(u,w)∈E(G)
χ(w).
3.3
Operations on GFMTs
If T1, . . . , Tr are GFMTs of type d →1, then the tuple (T1, . . . , Tr) deﬁnes a
GFMT T of type d →r in the obvious manner:
T(G)(χ)(v) := (T1(G)(χ)(v), . . . , Tr(G)(χ)(v))
(1)
Conversely, it is clear that any T of type d →r can be expressed as a tuple
(T1, . . . , Tr) as above, where Ti(G)(χ)(v) equals the i-th component in the tuple
T(G)(χ)(v).
Related to the above tupling operation is concatenation. Let T1 and T2 be
GFMTs of type d →r1 and d →r2, respectively. Their concatenation T1 | T2
is the GFMT T of type d →r1 + r2 deﬁned by T(G)(χ)(v) = T1(G)(χ)(v) |
T2(G)(χ)(v)), where | denotes concatenation of vectors. Concatenation is associa-
tive. Thus, we could write the previously deﬁned (T1, . . . , Tr) also as T1 | · · · | Tr.
We also deﬁne the parallel composition T1 ∥T2 of two GFMTs T1 and T2, of
type d1 →r1 and d2 →r2, respectively. It is the GFMT T of type (d1 + d2) →
(r1 + r2) deﬁned by T(G)(χ)(v) = T1(G)(χ1)(v) | T2(G)(χ2)(v), where χ1 (χ2)
is the feature map that assigns to any node w the projection of χ(w) to its ﬁrst
(last) d1 (d2) components.
In contrast, the sequential composition T1; T2 of two GFMTs T1 and T2, of
type d1 →d2 and d2 →d3 respectively, is the GFMT T of type d1 →d3
that maps every graph G to T2(G) ◦T1(G). In other words, (T1; T2)(G)(χ)(v) =
T2(G)(T1(G)(χ))(v).
Finally, for two GFMTS T1 and T2 of type d →r, we naturally deﬁne their
sum T1 + T2 by (T1 + T2)(G)(χ)(v) := T1(G)(χ)(v) + T2(G)(χ)(v) (addition of
r-dimensional vectors). The diﬀerence T1 −T2 is deﬁned similarly.
Example 2. Recall T1 and T4 from Example 1, and consider the following simple
GFMTs:
– For j = 1, 2, the GFMT Pj of type 2 →1 deﬁned by Pj(G)(χ)(v) = xj, where
χ(v) = (x1, x2).
– The GFMT Thalf of type 1 →1 deﬁned by Thalf(G)(χ)(v) = χ(v)/2.
– The GFMT Tsum of type 1 →1 deﬁned by Tsum(G)(χ)(v) = 
u∈N(G)(v) χ(u).
Then T1 equals (P1 + P2); Thalf, and T4 equals Tsum; Tsum.

On the Expressive Power of Message-Passing Neural Networks
25
3.4
Message-passing Neural Networks
A message-passing neural network (MPNN) consists of layers. Formally, let d
and r be natural numbers. An MPNN layer of type d →r is a 4-tuple L =
(W1, W2, b, σ), where σ : R →R is a continuous function, and W1, W2 and b are
real matrices of dimensions r × d, r × d and r × 1, respectively. We call σ the
activation function of the layer; we also refer to L as a σ-layer.
An MPNN layer L as above deﬁnes a GFMT of type d →r as follows:
L(G)(χ)(v) := σ

W1χ(v) + W2

u∈N(G)(v)
χ(u) + b

.
(2)
In the above formula, feature vectors are used as column vectors, i.e., d × 1
matrices. The matrix multiplications involving W1 and W2 then produce r × 1
matrices, i.e., r-dimensional feature vectors as desired. We see that matrix W1
transforms the feature vector of the current node from a d-dimensional vector
to an r-dimensional vector. Matrix W2 does a similar transformation but for
the sum of the feature vectors of the neighbors. Vector b serves as a bias. The
application of σ is performed component-wise on the resulting vector.
We now deﬁne an MPNN as a ﬁnite, nonempty sequence L1, . . . , Lp of MPNN
layers, such that the input arity of each layer, except the ﬁrst, equals the output
arity of the previous layer. Such an MPNN naturally deﬁnes a GFMT that is
simply the sequential composition L1; . . . ; Lp of its layers. Thus, the input arity
of the ﬁrst layer serves as the input arity, and the output arity of the last layer
serves as the output arity. Next we shall give examples of MPNNs that express
commonly known functions.
Example 3. Recall the “rectiﬁed linear unit” function ReLU : R →R :
z →max(0, z). Observe that max(x, y) = ReLU(y −x) + x, and also that
x = ReLU(x) −ReLU(−x). Hence, T2 from Example 1 can be expressed by
a two-layer MPNN, where the ﬁrst layer L1 transforms input feature vectors
(x, y) to feature vectors (y −x, x, −x) and then applies ReLU, and the second
layer L2 transforms the feature vector (a, b, c) produced by L1 to the ﬁnal result
a + b −c. Formally, L1 = (A, 03×2, 03×1, ReLU), with
A =
⎛
⎝
−1 1
1 0
−1 0
⎞
⎠,
and L2 = ((1, 1, −1), (0, 0, 0), 0, id), with id the identity function.
For another, simple, example, Tsum from Example 2 is expressed by the single
layer (0, 1, 0, id).
Same activation function If, for a particular MPNN, and an activation function
σ, all layers except the last one are σ-layers, and the last layer is either also a σ-
layer, or has the identity function as activation function, we refer to the MPNN
as a σ-MPNN. Thus, the two MPNNs in the above example are ReLU-MPNNs.

26
F. Geerts et al.
3.5
MPLang
We introduce a basic language for expressing GFMTs. The syntax of expressions
e in MPLang is given by the following grammar:
e :: = 1 | Pi | ae | e + e | f(e) | ♦e
where i is a non-zero natural number, a ∈R is a constant, and f : R →R is
continuous.
An expression e is called appropriate for input arity d if all subexpressions of
e of the form Pi satisfy 1 ≤i ≤d. In this case, e deﬁnes a GFMT of type d →1,
as follows:
– if e = 1, then e(G)(χ)(v) := 1
– if e = Pi, then e(G)(χ)(v) := the i-th component of χ(v)
– if e = ae1, then e(G)(χ)(v) := ae1(G)(χ)(v)
– if e = e1 + e2, then e(G)(χ)(v) := e1(G)(χ)(v) + e2(G)(χ)(v)
– if e = f(e1), then e(G)(χ)(v) := f(e1(G)(χ)(v))
– if e = ♦e1, then e(G)(χ)(v) := 
u∈N(G)(v) e1(G)(χ)(u)
Notice how there is no concatenation operator since the output arity of an
expression is always 1. To express higher output arities, we agree that a GFMT
T of type d →r is expressible in MPLang if there exists a tuple (e1, . . . , er) of
expressions that deﬁnes T in the sense of Eq. 1. We further agree:
– The constant a will be used as a shorthand for the expression a1, i.e., the
scalar multiplication of expression 1 by a.
– For any ﬁxed function f, we denote by f-MPLang the language fragment of
MPLang where all function applications apply f.
Example 4. Continuing Example 3, we can also express T2 and Tsum in MPLang,
namely, T2 as ReLU(P2 −P1) + P1, and Tsum as ♦P1.
3.6
Equivalence
Let T1 and T2 be MPNNs, or tuples of MPLang expressions, of the same type
d →r.
– We say that T1 and T2 are equivalent if they express the same GFMT.
– For a class G of graphs and a subset X of Rd, we say that T1 and T2 are
equivalent over G and X if the GFMTs expressed by T1 and T2 are equal on
every graph G in G and every χ ∈Feat(G, d, X) (see Sect. 3.1).
Example 4 illustrates the following general observation:
Proposition 1. For every MPNN T there is an equivalent MPLang-expression
that applies, in function applications, only activation functions used in T.

On the Expressive Power of Message-Passing Neural Networks
27
Proof. (Sketch.) Since we can always substitute subexpressions of the form Pi
by more complex expressions, MPLang is certainly closed under sequential com-
position. It thus suﬃces to verify that single MPNN layers L are expressible
in MPLang. For each output component of L we devise a separate MPLang
expression. Inspecting Eq. 2, we must argue for linear transformation; summa-
tion over neighbors; addition of a constant (component from the bias vector);
and application of an activation function. Linear transformation, and addition of
a constant, are expressible using the addition and scalar multiplication operators
of MPLang. Summation over neighbors is provided by the ♦operator. Applica-
tion of an activation function is provided by function application in MPLang.
4
From MPLang to MPNN Under ReLU
In Proposition 1 we observed that MPLang readily provides all the operators
that are implicitly present in MPNNs. MPLang, however, allows these operators
to be combined arbitrarily in expressions, whereas MPNNs have a more rigid
architecture. Nevertheless, at least under the ReLU activation function, we have
the following strong result:
Theorem 1. Every GFMT expressible in ReLU-MPLang is also expressible as
a ReLU-MPNN.
Crucial to proving results of this kind will be that the MPNN architecture
allows the construction of concatenations of MPNNs. We begin by noting:
Lemma 1. Let σ be an activation function. The class of GFMTs expressible as
a single σ-MPNN layer is closed under concatenation and under parallel compo-
sition.
Proof. (Sketch.) For parallel composition, we construct block-diagonal matrices
from the matrices provided by the two layers. For concatenation, we can simply
stack the matrices vertically.
⊓⊔
For σ = ReLU, we can extend the above Lemma to multi-layer MPNNs:
Lemma 2. ReLU-MPNNs are closed under concatenation.
Proof. Let L and K be two ReLU-MPNNs. Since ReLU is idempotent, every
n-layer ReLU-MPNN is equivalent to an n + 1-layer ReLU-MPNN. Hence we
may assume that L = L1; . . . , Ln and K = K1; . . . ; Kn have the same number
of layers. Now L | K = (L1 | K1); (L2 ∥K2); . . . ; (Ln ∥Kn) if n ≥2; if n = 1,
clearly L | K = L1 | K1. Hence, the claim follows from Lemma 1.
⊓⊔
Note that a ReLU-MPNN layer can only output positive numeric values,
since the result of ReLU is always positive. This explains why we must allow
the identity function (id) in the last layer of a ReLU-MPNN (see the end of
Sect. 3.4). Moreover, we can simulate intermediate id-layers in a ReLU-MPNN,
thanks to the identity x = ReLU(x) −ReLU(−x). Speciﬁcally, we have:

28
F. Geerts et al.
Lemma 3. Let L be an id-layer and let K be a σ-layer. Then there exists a
ReLU-layer L′ and a σ-layer K′ such that L; K is equivalent to L′; K′.
Proof. Let L = (W1, W2, b, id). We put
L′ = (W1, W2, b, ReLU) | (−W1, −W2, −b, ReLU)
which corresponds to a ReLU-layer by Lemma 1. Let K = (A, B, c, σ). Consider
the block matrices A′ = (A|−A) and B′ = (B|−B) (single-row block matri-
ces, with two matrices stacked horizontally, not vertically). Now for K′ we use
(A′, B′, c, σ).
⊓⊔
We are now ready to prove Theorem 1. By Lemma 2, it suﬃces to focus on
MPLang expressions, i.e., GFMTs of output arity one. So, our task is to con-
struct, for every expression e in ReLU-MPLang, an equivalent ReLU-MPNN E.
However, by Lemma 3, we are free to use intermediate id-layers in the construc-
tion of E. We proceed by induction on the structure of e. We skip the base cases
and consider the inductive cases where e is of one of the forms ae1, e1 +e2, f(e1)
(with f = ReLU), or ♦e1. By induction, we have MPNNs E1 and E2 for e1 and
e2.
– If e is of the form ae1, we set E = E1; (a, 0, 0, id).
– If e is of the form e1 + e2, we set E = (E1 | E2); ((1, 1), (0, 0), 0, id). Here,
E1 | E2 corresponds to a ReLU-MPNN by Lemma 2.
– If e is of the form f(e1), we set E = E1; (1, 0, 0, f).
– If e is of the form ♦e1, we set E = E1; (0, 1, 0, id).
5
Arbitrary Activation Functions
Theorem 1 only supports the ReLU function in MPLang expressions. On the
other hand, the equivalent MPNN then only uses ReLU as well. If we allow
arbitrary activation functions in MPNNs, can they then simulate also MPLang
expressions that apply arbitrary functions? We can answer this question aﬃr-
matively, under the assumption that graphs have bounded degree and feature
vectors come from a bounded domain. The proof of our Lemma 4 explains how
we rely on the bounded-domain assumption. Moreover, also the degree has to
be bounded, for otherwise we can still create unbounded values using ♦(Pi).
Theorem 2. Let p and d be natural numbers, let Gp be the class of graphs of
degree at most p, and let X ⊆Rd be bounded. For every GFMT T expressible in
MPLang there exists an MPNN that is equivalent to T over Gp and X.
The above theorem can be proven exactly as Theorem 1, once we can deal
with the concatenation of two MPNN layers with possibly diﬀerent activation
functions. The following result addresses this task:
Lemma 4. Let L and K be MPNN layers of type dL →rL and dK →rk,
respectively. Let XL ⊆RdL and XK ⊆RdK be bounded, and let p be a natural
number. There exist two MPNN layers L′ and K′ such that

On the Expressive Power of Message-Passing Neural Networks
29
−1
1
2
3
−1
1
x
σ1
−2
−1
1
−2
−1
1
x
σ2
−5
−4
−3
−2
−1
1
2
3
4
−2
−1
1
x
σ
Fig. 1. Illustration of the proof of Lemma 4.
1. L′ and K′ use the same activation function;
2. L′ is equivalent to L over Gp and XL;
3. K′ is equivalent to K over Gp and XK.
Proof. Let L = (W1L, W2L, bL, σL) and K = (W1K, W2K, bK, σK). Let w1,i, w2,i
and bi be the i-th row of W1L, W2L and bL respectively. For each i ∈{1, . . . , rL}
and for any k ∈{1, . . . , p} consider the function
λk
i : R(k+1)dL →R : ( ⃗x0, ⃗x1, . . . , ⃗xk) →w1,i · ⃗x0 + w2,i · ⃗x1 + · · · + w2,i · ⃗xk + bi.
Then for any G ∈Gp, any χ ∈Feat(G, d, XL), and v ∈V (G), each component
of L(G)(χ)(v) will belong to the image of some function λk
i on Xk+1
L
, with k
the degree of v. Since Xk+1
L
is bounded and λk
i is continuous, these images are
also bounded and their ﬁnite union over i ∈{1, . . . , r} and k ∈{1, . . . , p} is also
bounded. Let Y1 be this union and let M = max Y1.
For K we can similarly deﬁne the functions κk
i and arrive at a bounded set
YK ⊆R. We then deﬁne m = min Y2.
We will now construct a new activation function σ′. First deﬁne the functions
σ′
L(x) := σL(x + Mi + 1) for x ∈] −∞, −1] and σ′
K(x) := σK(x −mi −1) for
x ∈[1, ∞[. Notice how σ′
L is simply σL shifted to the left so that its highest
possible input value, which is M, aligns with −1. Similarly, σ′
K is simply σK
shifted to the right so that its lowest possible input value, which is m, aligns
with 1. We then deﬁne σ′ to be any continuous function that extends both σ′
L
and σ′
K. An example of this construction can be seen in Fig. 1 with σ1 = tanh,
M = 3, σ2 the identity, and m = −2.

30
F. Geerts et al.
We also construct new bias vectors, obtained by shifting bL and bK left and
right respectively to provide appropriate inputs for σ′. Speciﬁcally, we deﬁne
b′
L := bL −(M + 1)r×1 and b′
K := bK + (m + 1)r×1.
Finally, we can set L′ = (W1L, W2L, b′
L, σ′) and K′ = (W1K, W2K, b′
K, σ′) as
desired.
⊓⊔
Thanks to the above lemma, Lemma 1 remains available to concatenate lay-
ers. The part of Lemma 1 that deals with parallel composition (which is needed
to prove closure under concatenation for multi-layer MPNNs) must be slightly
adapted as follows. It follows immediately from Lemma 4 above and the original
Lemma 1.
Lemma 5. Let L and K be MPNN layers of type dL →rL and dK →rk,
respectively. Let XL ⊆RdL and XK ⊆RdK be bounded, and let p be a natural
number. Let X = {( ⃗xL, ⃗
xK) | ⃗xL ∈XL and ⃗
xK ∈XK} ⊆RdL+dK. There exists
an MPNN layer that is equivalent to L ∥K over Gp and X.
6
Approximation by ReLU-MPNNs
Theorem 2 allows the use of arbitrary activation functions in the MPNN simulat-
ing an MPLang expression; these activation functions may even be diﬀerent from
the ones applied in the expression (see the proof of Lemma 4). What if we insist
on MPNNs using a ﬁxed activation function? In this case we can still recover
our result, if we allow approximation. Moreover, we must slightly strengthen our
assumption of feature vectors coming from a bounded domain, to coming from
a compact domain.1
We will rely on a classical result in the approximation theory of neural net-
works [20,22], to the eﬀect that continuous functions can be approximated arbi-
trarily well by piecewise linear functions, which can be modeled using ReLU.2
In order to recall this result, we recall that the uniform distance between two
continuous functions g and h from R to R on a compact domain Y equals
ρY (g, h) = supx∈Y |g(x) −h(x)|.
Density Property. Let Y be a compact subset of R, let f : R →R be con-
tinuous on Y , and let ϵ > 0 be a real number. There exists a positive integer
n and real coeﬃcients ai, bi, ci, for i = 1, . . . , n, such that ρY (f, f ′) ≤ϵ, where
f ′(x) = n
i=1 ciReLU(aix −bi).
We want to extend the notion of uniform distance to GFMTs expressed in
MPLang. For any MPLang expression e appropriate for input arity d, any class
1 A subset of R or Rd is called compact if it is bounded and closed in the ordinary
topology.
2 The stated Density Property actually holds not just for ReLU, but for any nonpoly-
nomial continuous function.

On the Expressive Power of Message-Passing Neural Networks
31
G of graphs, and any subset X ⊆Rd, the image of e over G and X is deﬁned
as the set
{e(G)(χ)(v) : G ∈G & χ ∈Feat(G, d, X) & v ∈V (G)}.
It is a subset of R. We observe: (proof omitted)
Lemma 6. For any natural number p and compact X, the image of e over Gp
and X is a compact set.
With p and X as in the lemma, and any two MPLang expression e1 and e2
appropriate for input arity d, the set
{|e1(G)(χ)(v) −e2(G)(χ)(v)| : G ∈Gp & χ ∈Feat(G, d, X) & v ∈V (G)}
has a supremum. We deﬁne ρGp,X(e1, e2), the uniform distance between e1 and
e2 over Gp and X, to be that supremum.
The main result of this section can now be stated as follows. Note that
we approximate MPLang expressions by ReLU-MPLang expressions. These can
then be further converted to ReLU-MPNNs by Theorem 1.
Theorem 3. Let p and d be natural numbers, and let X ⊆Rd be compact. Let
e be an MPLang expression appropriate for d, and let ϵ > 0 be a real number.
There exists a ReLU-MPLang expression e′ such that ρGp,X(e, e′) ≤ϵ.
Proof. By induction on the structure of e. We skip the base cases. In the induc-
tive cases where e is of the form ae1, e1 + e2, or f(e1), we consider any G ∈Gp,
any χ ∈Feat(G, d, X), and any v ∈V (G), but abbreviate e(G)(χ)(v) simply
as e.
Let e be of the form ae1. If a = 0 we set e′ = 0. Otherwise, let e′
1 be the
expression obtained by induction applied to e1 and ϵ/a. We then set e′ = ae′
1.
The inequality |e −e′| ≤ϵ is readily veriﬁed.
Let e be of the form e1 + e2. For j = 1, 2, let e′
j be the expression obtained
by induction applied to ej and ϵ/2. We then set e′ = e′
1 + e′
2. The inequality
|e −e′| ≤ϵ now follows from the triangle inequality.
Let e is of the form f(e1). By Lemma 6, the image of e1 is a compact set
Y1 ⊆R. We deﬁne the closed interval Y = [min(Y1) −ϵ/2, max(Y1) + ϵ/2].
By the Density Property, there exists f ′ such that ρY (f, f ′) ≤ϵ/2. Since Y is
compact, f ′ is uniformly continuous on Y . Thus there exists δ > 0 such that
|f ′(x) −f ′(x′)| < ϵ/2 whenever |x −x′| < δ.
We now take e′
1 to be the expression obtained by induction applied to e1 and
min(δ, ϵ/2). We see that the image of e′
1 is contained in Y . Setting e′ = f(e′
1),
we verify that |e −e′| = |f(e1) −f ′(e′
1)| + |f ′(e1) −f ′(e′
1)| ≤ϵ as desired.
Our ﬁnal inductive case is when e is of the form ♦e1. We again consider any
G ∈Gp, any χ ∈Feat(G, d, X), and any v ∈V (G), but this time abbreviate
e(G)(χ)(v) as e(v). Let e′
1 be the expression obtained by induction applied to e1
and ϵ/p. Setting e′ = ♦e′
1, we verify, as desired:

32
F. Geerts et al.
|e(v) −e′(v)| = |

u∈N(G)(v)
e1(u) −

u∈N(G)(v)
e′
1(u)|
≤

u∈N(G)(v)
|e1(u) −e′
1(u)|
≤p(ϵ/p)
= ϵ.
The penultimate step clearly uses that G has degree bound p. (This degree bound
is also used in Lemma 6.)
7
Concluding Remarks
We believe that our approach has the advantage of modularity. For example,
Theorem 1 is stated for ReLU, but holds for any activation function for which
Lemmas 1 and 3 can be shown. We already noted that the Density Property holds
not just for ReLU but for any nonpolynomial continuous activation function. It
follows that for any activation function σ for which Lemmas 1 and 3 can be
shown, every MPLang expression can be approximated by a σ-MPNN.
It would be interesting to see counterexamples that show that Theorems
2 and 3 do not hold without the restriction to bounded-degree graphs, or to
features from a bounded or compact domain. Such counterexamples can probably
be derived from known counterexamples in analysis or approximation theory.
Finally, in this work we have focused on the question whether MPLang can be
simulated by MPNNs. However, it is also interesting to investigate the expressive
power of MPLang by itself. For example, is the GFMT T3 from Example 1
expressible in MPLang?
Acknowledgments. We thank the anonymous reviewers for suggesting improvements
to the text. Jasper Steegmans is supported by the Special Research Fund (BOF) of
Hasselt University.
References
1. Abboud, R., Ceylan, I., Grohe, M., Lukasiewicz, T.: The surprising power of graph
neural networks with random node initialization. In: Zhou, Z.H. (ed.) Proceed-
ings 30th International Joint Conference on Artiﬁcial Intelligence, pp. 2112–2118.
ijcai.org (2021)
2. Abiteboul, S., Hull, R., Vianu, V.: Foundations of Databases. Addison-Wesley,
Boston (1995)
3. Arora, R., Basu, A., Mianjy, P., Mukherjee, A.: Understanding deep neural net-
works with rectiﬁed linear units. In: Proceedings 6th International Conference on
Learning Representations. OpenReview.net (2018)

On the Expressive Power of Message-Passing Neural Networks
33
4. Azizian, W., Lelarge, M.: Expressive power of invariant and equivariant graph
neural networks. In: Proceedings 9th International Conference on Learning Repre-
sentations. OpenReview.net (2021)
5. Balcilar, M., H´eroux, P., et al.: Breaking the limits of message passing graph neural
networks. In: Meila, M., Zhang, T. (eds.) Proceedings 38th International Confer-
ence on Machine Learning. Proceedings of Machine Learning Research, vol. 139,
pp. 599–608 (2021)
6. Barcel´o, P., Kostylev, E., Monet, M., P´erez, J., Reutter, J., Silva, J.: The expressive
power of graph neural networks as a query language. SIGMOD Rec. 49(2), 6–17
(2020)
7. Barcel´o, P., Kostylev, E., Monet, M., P´erez, J., Reutter, J., Silva, J.: The logical
expressiveness of graph neural networks. In: Proceedings 8th International Confer-
ence on Learning Representations. OpenReview.net (2020)
8. Brijder, R., Geerts, F., Van den Bussche, J., Weerwag, T.: On the expressive power
of query languages for matrices. ACM Trans. Database Syst. 44(4), 15:1–15:31
(2019)
9. Dannert, K.M., Gr¨adel, E.: Semiring provenance for guarded logics. In: Madar´asz,
J., Sz´ekely, G. (eds.) Hajnal Andr´eka and Istv´an N´emeti on Unity of Science.
OCL, vol. 19, pp. 53–79. Springer, Cham (2021). https://doi.org/10.1007/978-3-
030-64187-0 3
10. Geerts, F.: On the expressive power of linear algebra on graphs. Theory Comput.
Syst. 65(1), 179–239 (2021)
11. Geerts, F., Reutter, J.: Expressiveness and approximation properties of graph neu-
ral networks. In: ICLR. OpenReview.net (2022). (to appear)
12. Gilmer, J., et al.: Neural message passing for quantum chemistry. In: Precup, D.,
Teh, Y. (eds.) Proceedings 34th International Conference on Machine Learning.
Proceedings of Machine Learning Research, vol. 70, pp. 1263–1272 (2017)
13. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press, Cambridge
(2016)
14. Green, T., Karvounarakis, G., Tannen, V.: Provenance semirings. In: Proceedings
26th ACM Symposium on Principles of Database Systems, pp. 31–40 (2007)
15. Grohe, M.: The logic of graph neural networks. In: Proceedings 36th Annual
ACM/IEEE Symposium on Logic in Computer Science, pp. 1–17. IEEE (2021)
16. Gurevich, Y.: Algebras of feasible functions. In: Proceedings 24th Symposium on
Foundations of Computer Science, pp. 210–214. IEEE Computer Society (1983)
17. Gurevich, Y.: Logic and the challenge of computer science. In: B¨orger, E. (ed.)
Current Trends in Theoretical Computer Science, pp. 1–57. Computer Science
Press (1988)
18. Hamilton, W.: Graph Representation Learning. Synthesis Lectures on Artiﬁcial
Intelligence and Machine Learning, Morgan & Claypool, San Rafael (2020)
19. Hella, L., et al.: Weak models of distributed computing, with connections to modal
logic. Distrib. Comput. 28(1), 31–53 (2013). https://doi.org/10.1007/s00446-013-
0202-3
20. Leshno, M., Lin, V., Pinkus, A., Schocken, S.: Multilayer feedforward networks
with a nonpolynomial activation function can approximate any function. Neural
Netw. 6(6), 861–867 (1993)
21. Morris, C., et al.: Weisfeiler and Leman go machine learning: the story so far.
arXiv:2122.09992 (2021)
22. Pinkus, A.: Approximation theory of the MLP model in neural networks. Acta
Numerica 8, 143–195 (1999)

34
F. Geerts et al.
23. Rudin, W.: Principles of Mathematical Analysis, 3rd edn. McGraw Hill, New York
(1976)
24. Russell, S., Norvig, P.: Artiﬁcial Intelligence: A Modern Approach, 4th edn. Pear-
son, London (2022)
25. Shalev-Shwartz, S., Ben-David, S.: Understanding Machine Learning: From Theory
to Algorithms. Cambridge University Press, Cambridge (2014)

Assumption-Based Argumentation
for Extended Disjunctive Logic
Programming
Toshiko Wakaki(B)
Shibaura Institute of Technology, 307 Fukasaku Minuma-ku,
Saitama-city, Saitama 337–8570, Japan
twakaki@shibaura-it.ac.jp
Abstract. We present the semantic correspondence between generalized
assumption-based argumentation and extended disjunctive logic pro-
gramming. In this paper, we propose an assumption-based framework
(ABF) translated from an extended disjunctive logic program (EDLP),
which incorporates explicit negation into Heyninck and Arieli’s ABF
induced by a disjunctive logic program to resolve problems remained
in their approach. In our proposed ABF, we show how an argument is
constructed from disjunctive rules. Then we show a 1-1 correspondence
between answer sets of an EDLP P and stable argument extensions of
the ABF translated from P with trivialization rules. Hereby thanks to
introducing explicit negation, the semantic relationship between disjunc-
tive default theories and assumption-based frameworks is obtained in our
approach. Finally, after deﬁning rationality postulates and consistency in
our ABF, we show answer sets of a consistent EDLP can be captured by
consistent stable extensions of the translated ABF with no trivialization
rules, that is useful for ABA applications containing explicit negation.
Keywords: assumption-based argumentation · extended disjunctive
logic programs · disjunctive default theories · arguments · argument
extensions
1
Introduction
In our daily life, disjunctive information is often required in reasoning and argu-
mentation to solve problems. In nonmonotonic reasoning, Gelfond et al. [16] pro-
posed disjunctive default logic as a generalization of Reiter’s default logic [19]
to overcome problems of default logic in handling disjunctive information. They
also showed that extended disjunctive logic programs (EDLPs) [15] which allow
disjunctions with the connective “|” in rule heads as well as classical negation
“¬” can be embedded into disjunctive default theories.
In contrast, in the context of formal argumentation, Beirlaen et al. [2] pre-
sented the extended ASPIC+ framework where disjunctive reasoning is inte-
grated in structured argumentation with defeasible rules [17] by incorporating
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 35–54, 2022.
https://doi.org/10.1007/978-3-031-11321-5_3

36
T. Wakaki
reasoning by cases inference scheme [1]. In their framework, an argument is
allowed to have a disjunctive conclusion, while disjunction is expressed by the
classical connective “∨”. However, they did not show the relationship between
their framework and other approaches in nonmonotonic reasoning such as a dis-
junctive default theory. Consider the following example shown by them.
Example 1. (Kyoto protocol). [2] There are two candidates for an upcoming
presidential election. The candidates had a debate in the capital. They were asked
what measures are to be taken in order for the country to reach the Kyoto pro-
tocol objectives for reducing greenhouse gas emissions. The ﬁrst candidate, a
member of the purple party, argued that if she wins the election, she will reach
the objectives by supporting investments in renewable energy. The second can-
didate, a member of the yellow party, argued that if she wins the election, she
will reach the objectives by supporting sustainable farming methods. We have
reasons to believe that, no matter which candidate wins the election, the Kyoto
protocol objectives will be reached. If the purple candidate wins, she will support
investments in renewable energy (p ⇒r), which would in turn result in meeting
the Kyoto objectives (r ⇒k). Similarly, if the yellow candidate wins, she will
support sustainable farming methods (y ⇒f), which would result in meeting the
Kyoto objectives (f ⇒k). Since one of the two candidates is going to win (p∨y),
we can reason by cases to conclude that the Kyoto objectives will be reached (k).
Beirlaen et al. [2] represented the information shown above in terms of the
knowledge base K1=({p ⇒r, r ⇒k, y ⇒f, f ⇒k}, {p ∨y}) which consists of
defeasible rules in ASPIC+ [17] and the classical formula p ∨y as facts. Their
formulation derives the intended result k, however, a problem happens when K1
is translated into a default theory. In fact, ψ ⇒φ encodes the normal default
ψ:φ
φ , then K1 is expressed by the default theory:
D1 = {p : r
r
, r : k
k
, y : f
f
, f : k
k
, p ∨y}.
D1 has a single extension, consisting of the disjunction p ∨y and its logical
consequences, where the four defaults “don’t work”. Thus the result is contrary
to what we would expect. Instead of D1, let us represent the information by the
disjunctive default theory as follows:
D2 = {p : r
r
, r : k
k
, y : f
f
, f : k
k
, p | y},
or the associated EDLP P1 = {r ←p, not ¬r; k ←r, not ¬k; f ←y, not ¬f;
k ←f, not ¬k; p | y ←}. Then D2 has two extensions E1 and E2 s.t. k ∈Ei
(i = 1, 2), while P1 has two answer sets S1 = {p, r, k} and S2 = {y, f, k}, where
Si ⊆Ei(i=1, 2). These results agree with our expectation that the Kyoto protocol
objectives will be reached no matter which candidate wins the election.
□
Recently, Heyninck and Arieli [13] proposed a generalized assumption-based
framework (ABF, for short) induced by a disjunctive logic program (DLP), where
disjunctions with the connective “∨” are allowed to appear in rule heads. Though

Assumption-Based Argumentation for Extended DLP
37
their ABF has a contrariness operator ¯¯ such that not p = p for every atom p,
its language does not contain explicit negation. Then they showed a 1-1 cor-
respondence between the stable models of a DLP and the stable assumption
extensions of the ABF induced by a DLP. However, several issues are remained
to be explored. First, they did not show how to construct an argument from
disjunctive rules in a DLP nor took account of argument extensions of their
ABF. Second, the semantic relationship between their ABF induced by a DLP
and a disjunctive default theory is not considered in [13]. In logic programming,
EDLPs are introduced to extend DLPs for knowledge representation, and in for-
mal argumentation, generally assumption-based frameworks (ABFs) are capable
of expressing explicit negation [12]. To the best of our knowledge, however, there
is no study to show the semantic relationship between ABFs and EDLPs as well
as the relationship between ABFs and disjunctive default theories.
The purpose of this paper is ﬁrst to show how to construct an argument from
disjunctive rules in (E)DLPs, and second to investigate the semantic relationship
between ABFs and EDLPs (resp. disjunctive default theories). So far explicit
negation has been used in various applications of argumentation. In structured
argumentation, for example, as the ways in which arguments can be in conﬂict,
ASPIC+ allows the rebutting attack between two arguments having the mutu-
ally contradictory conclusions w.r.t. explicit negation along with undercutting
and undermining attacks. In contrast, assumption-based argumentation (ABA)
[9] whose language may contain explicit negation allows only attacks against the
support of arguments as deﬁned in terms of a notion of contrary. Since both sys-
tems incorporate explicit negation, conditions under which each system satisﬁes
rationality postulates [5] were proposed to avoid anomalous results [10,17].
As for recent ABA applications containing explicit negation, Schulz and Toni
[21] proposed the approach of justifying answer sets of an extended logic program
(which contains explicit negation) using argumentation. However counterexam-
ples exist in their theorems [21, Theorems 1, 2] due to not taking account of
rationality postulates [5,10] or consistency in ABA as shown in [23].
To resolve the above-mentioned problems, this paper proposes an assumption-
based framework (ABF) translated from an EDLP, which incorporates explicit
negation into Heyninck et al.’s ABF induced by a DLP. Contribution of this study
is as follows: First, we deﬁne an argument in the ABF translated from a given
EDLP which is constructed from rules allowing disjunctions of the EDLP based on
three inference rules provided in our ABF. Second, we show there is a one-to-one
correspondence between answer sets of an EDLP P and stable argument extensions
(resp. stable assumption extensions) of the ABF translated from P with trivializa-
tion rules. Third, hereby thanks to introducing explicit negation, the semantic rela-
tionship between disjunctive default theories and assumption-based frameworks
is obtained on the basis of our ABF. To the best of our knowledge, this paper is
the ﬁrst one to show this relationship. Finally, since our ABF incorporates explicit
negation ¬ along with disjunction |, we deﬁne rationality postulates and consis-
tency in our ABFs to avoid anomalous outcomes as arise in [21] (see [23]). Then
we show answer sets of a consistent EDLP can be captured by consistent stable
extensions of the translated ABF with no trivialization rules. This is useful for
ABA applications containing explicit negation (e.g. [21]).

38
T. Wakaki
The rest of this paper is as follows. Section 2 shows preliminaries. Section 3
presents an ABF translated from an EDLP and shows the deﬁnition of its argu-
ment, the semantic relationship between an EDLP (resp. a disjunctive default
theory) and its translated ABF, and the relationship between a consistent EDLP
and its translated ABF. Sections 4 discusses related work and concludes.
2
Preliminaries
2.1
Assumption-Based Argumentation
An ABA framework [4,9] is a tuple ⟨L, R, A,¯¯⟩, where (L, R) is a deductive
system, consisting of a language L and a set R of inference rules of the form:
b0 ←b1, . . . , bm (bi ∈L for 0 ≤i ≤m), A ⊆L is a (non-empty) set of assumptions,
and ¯¯ is a total mapping from A into L, which we call a contrariness operator.
α is referred to as the contrary of α ∈A. We enforce that ABA frameworks are
ﬂat, namely assumptions in A do not appear in the heads of rules in R. In ABA,
an argument for a claim (or conclusion) c ∈L supported by K ⊆A (K ⊢c in
short) is deﬁned as a (ﬁnite) tree with nodes labelled by sentences in L or by
τ ̸∈L denoting “true”, where the root is labelled by c, and leaves are labelled
either by τ or by assumptions in K. attacks is deﬁned as follows.
• An argument K1 ⊢c1 attacks an argument K2 ⊢c2 iﬀc1 = α for ∃α∈K2.
• For Δ, Δ′ ⊆A, and α ∈A, Δ attacks α iﬀΔ enables the construction of an
argument for conclusion α. Thus Δ attacks Δ′ iﬀΔ attacks some α∈Δ′.
A set of arguments Args is conﬂict-free iﬀ̸∃A, B ∈Args such that A attacks B.
A set of assumptions Δ is conﬂict-free iﬀΔ does not attack itself.
Let AFF = (AR, attacks) be the abstract argumentation (AA) framework [6]
generated from an ABA framework F, where AR is the set of arguments s.t.
a ∈AR iﬀan argument a : K ⊢c is in F, and (a, b) ∈attacks in AFF iﬀa
attacks b in F [8]. Let σ ∈{complete, preferred, grounded, stable, ideal} be the
name of the argumentation semantics. Then the ABA semantics is given by σ
argument extensions as well as by σ assumption extensions under σ semantics.
In case σ=stable, Args⊆AR is a stable argument extension iﬀit is conﬂict-free
and attacks every A ∈AR\Args, while Δ ⊆A is a stable assumption extension
iﬀit is conﬂict-free and attacks every α ∈A \ Δ. There is a 1-1 correspondence
between σ assumption extensions and σ argument extensions. Let claim(Ag) be
the conclusion (or claim) of an argument Ag. Then the conclusion of a set of
arguments E is deﬁned as Concs(E)={c∈L | c=claim(Ag) for Ag∈E}.
Rationality postulates [5] are stated in ABA as follows.
Deﬁnition 1. (Rationality postulates). [5,10] Let ⟨L, R, A,¯¯⟩be a ﬂat ABA
framework, where ¯¯ has the property such that contraries of assumptions are
not assumptions. A set X ⊆L is said to be contradictory iﬀX is contradictory
w.r.t. ¯¯, i.e. there exists an assumption α ∈A such that {α, α} ⊆X; or X is

Assumption-Based Argumentation for Extended DLP
39
contradictory w.r.t. ¬, i.e. there exists s ∈L such that {s, ¬s} ⊆X if L contains
an explicit negation operator ¬. Let CNR : 2L →2L be a consequence operator.
For a set X ⊆L, CNR(X) is the smallest set such that X ⊆CNR(X), and for
each rule r ∈R, if body(r) ⊆CNR(X) then head(r) ∈CNR(X). X is closed iﬀ
X = CNR(X). A set X ⊆L is said to be inconsistent iﬀits closure CNR(X)
is contradictory. X is said to be consistent iﬀit is not inconsistent. A ﬂat ABA
framework F = ⟨L, R, A,¯¯⟩is said to satisfy the consistency-property (resp. the
closure-property) if for each complete extension E of AFF generated from F,
Concs(E) is consistent (resp. Concs(E) is closed).
We say that a set of arguments E is consistent if Concs(E) is consistent [23].
Heyninck and Arieli [12] proposed (generalized) assumption-based frameworks.
Deﬁnition 2. (Assumption-based frameworks). [12,13] Let L be a propo-
sitional language, ψ, φ ∈L be formulas and Γ, Γ ′, Λ ⊆L be sets of formulas.
A (propositional) logic for a language L is a pair L=(L, ⊢), where ⊢is a
(Tarskian) consequence relation for L, that is, a binary relation between sets of
formulas and formulas in L, which is reﬂexive (if ψ ∈Γ then Γ ⊢ψ), monotonic
(if Γ ⊢ψ and Γ ⊆Γ ′, then Γ ′ ⊢ψ), and transitive (if Γ ⊢ψ and Γ ′ ∪{ψ} ⊢φ,
then Γ ∪Γ ′ ⊢φ). The L-based transitive closure of a set Γ of L-formulas is
CnL(Γ)={ψ ∈L | Γ ⊢ψ} [12].
Let Γ, Λ⊆L, Λ ̸= ∅, and ℘(·) is the powerset operator. An assumption-based
framework (or ABF, for short) is a tuple ABF= ⟨L, Γ, Λ, −⟩, where L=(L, ⊢) is
a propositional Tarskian logic, Γ (the strict assumptions) and Λ (the candidate
or defeasible assumptions) are distinct countable sets of L-formulas, and −:Λ →
℘(L) is a contrariness operator, assigning a ﬁnite set of L-formulas to every
defeasible assumption in Λ. In ABF, attacks is deﬁned as follows.
For Δ, Θ ⊆Λ, and ψ ∈Λ, Δ attacks ψ iﬀΓ ∪Δ ⊢φ for some φ ∈−ψ.
Accordingly, Δ attacks Θ if Δ attacks some ψ ∈Θ.
The usual semantics in AA frameworks [6] is adapted to their ABFs as follows.
Deﬁnition 3. (ABF semantics).[4] Let ABF=⟨L, Γ, Λ, −⟩be an assumption-
based framework. A set of assumptions Δ⊆Λ is closed iﬀΔ={α∈Λ | Γ ∪Δ ⊢α}.
ABF is said to be ﬂat iﬀevery set of assumptions Δ⊆Λ is closed.
Let Δ ⊆Λ. Then Δ is conﬂict-free iﬀthere is no Δ′ ⊆Δ that attacks some
ψ∈Δ. Δ is stable iﬀit is closed, conﬂict-free and attacks every ψ∈Λ \ Δ.
2.2
Disjunctive Logic Programs
A disjunctive logic program (DLP) [18] is a ﬁnite set of rules of the form1
p1 ∨. . . ∨pk ←pk+1, . . . , pm, not pm+1, . . . , not pn,
(1)
1 A disjunctive logic program (DLP) deﬁned in this paper is diﬀerent from a normal
disjunctive program (NDP) deﬁned in Sect. 2.3.

40
T. Wakaki
where n ≥m ≥0, k ≥1. Each pi (1 ≤i ≤n) is a ground atom and not
means negation as failure (NAF). An atom preceded by not is called a NAF-
atom. Let HBP be the set of all ground atoms appearing in a DLP P called
the Herbrand base. A set M ⊆HBP satisﬁes a ground rule of the form (1) if
{pk+1, . . . , pm} ⊆M and {pm+1, . . . , pn} ∩M = ∅imply {p1, . . . , pk} ∩M ̸= ∅.
M is a model of P if it satisﬁes every ground rule in P.
The reduct of P w.r.t. M is the DLP P M= {p1∨. . .∨pk ←pk+1,. . . ,pm|there is
(p1∨. . .∨pk ←pk+1,. . .,pm, not pm+1, . . . , not pn)∈P s.t. {pm+1,. . ., pn}∩M =∅}.
Then M is a stable model if it is a ⊆-minimal model of P M [14,18].
In [13], Heyninck and Arieli deﬁned A(P) as HBP , and denoted by L=⟨LDLP, ⊢⟩
the logic based on the language LDLP consisting of disjunctions of atoms (p1∨. . .∨
pn, for n ≥1), NAF-atoms (not p for p ∈A(P)) and rules of the form (1), where
⊢is constructed for LDLP by three inference rules: Modus Ponens (MP), Resolution
(Res) and Reasoning by Cases (RBC)2. The ABF induced by a DLP P is deﬁned
by: ABF(P)=⟨L, P,∼A(P), −⟩, where L=⟨LDLP, ⊢⟩, ∼A(P)={not p| p∈A(P)},
−not p={p} for every p ∈A(P). Then they showed a one-to-one correspondence
between stable models of a DLP P and stable extensions of ABF(P) as follows.
Proposition 1. [13, Proposition 2 and Proposition 3]. Let P be a ﬁnite DLP.
If M is a stable model of P, then M={not p | p ∈(A(P) \ M)} is a stable
extension 3 of ABF(P). Conversely if E is a stable extension of ABF(P), then
E={p ∈A(P)| not p ̸∈E} is a stable model of P.
2.3
Extended Disjunctive Logic Programs
We consider a ﬁnite propositional extended disjunctive logic program (EDLP)
[15] in this paper. An EDLP is a ﬁnite set of rules of the form:
L1| . . . |Lk ←Lk+1, . . . , Lm, notLm+1, . . . , notLn,
(2)
where n ≥m ≥0, k ≥1. Each Li (1 ≤i ≤n) is a literal, that is, either a ground
atom A (i.e. a propositional variable) or ¬A preceded by classical negation ¬.
not means NAF as before and not L is called a NAF-literal. The left (resp. right)
part of ←is the head (resp. the body). The symbol “|” is used to distinguish
disjunction in the head of a rule from disjunction “∨” used in classical logic. An
EDLP is called a normal disjunctive logic program (NDP) if “¬” does not occur
in it, while an EDLP is called an extended logic program (ELP) if it contains
no disjunction (k = 1). An ELP is called a normal logic program (NLP) if “¬”
does not occur in it. Let LitP be the set of all ground literals in the language
of an EDLP P. When an EDLP P is an NDP (or NLP), LitP reduces to the
2 The precise form of these inference rules will be described later in Remark 1.
3 Stable extensions in [13] denote stable assumption extensions.

Assumption-Based Argumentation for Extended DLP
41
Herbrand base HBP . The semantics of an EDLP is given by answer sets [15]
(resp. paraconsistent stable models or p-stable models 4 [20]) deﬁned as follows.
Deﬁnition 4. [15,20]. First, let P be a not-free EDLP (i.e., for each rule m =
n). Then, S ⊆LitP is an answer set of P if S is a minimal set (w.r.t. ⊆) satisfying
the following two conditions:
(i) For each rule L1|. . .|Lk ←Lk+1,. . . ,Lm in P, if {Lk+1, . . . , Lm} ⊆S, then
Li ∈S for some i (1 ≤i ≤k).
(ii) If S contains a pair of literals L and ¬L, then S = LitP .
Second, let P be any EDLP and S ⊆LitP . The reduct P S of P w.r.t. S is
a not-free EDLP P S = {L1| . . . |Lk ←Lk+1,. . . ,Lm | there is a rule of the form
L1| . . . |Lk ←Lk+1,. . .Lm, not Lm+1, . . . not Ln in P s.t. {Lm+1,. . ., Ln}∩S =∅}.
Then S is an answer set of P if S is the answer set of P S.
On the other hand, p-stable models are regarded as answer sets deﬁned with-
out the condition (ii). An answer set is consistent if it is not LitP ; otherwise
it is inconsistent. An EDLP P is consistent if it has a consistent answer set;
otherwise it is inconsistent under answer set semantics. In contrast, a p-stable
model is inconsistent if it contains a pair of complementary literals; otherwise
it is consistent. An EDLP P is consistent if it has a consistent p-stable model;
otherwise it is inconsistent under paraconsistent stable model semantics.
Gelfond et al. [16] showed in the following theorem that a propositional EDLP
P can be translated into a disjunctive default theory embD(P) by replacing every
rule in P of the form (2) with the disjunctive default
Lk+1 ∧. . . ∧Lm : ¬Lm+1, . . . , ¬Ln
L1| . . . |Lk
.
Theorem 1. [16, Theorem 7.2]. Let P be a propositional EDLP. Then S is an
answer set of P iﬀS is the set of all literals from an extension of embD(P).
Let P be an ELP, Ptr
def
= P ∪{L ←p, ¬p | p ∈LitP , L ∈LitP } be the
ELP obtained from P by incorporating the trivialization rules [20], and F(P) =
⟨LP , P, AP ,¯¯⟩be the ABA framework translated from P, where NAFP = {not L
| L ∈LitP }, LP = LitP ∪NAFP , AP =NAFP , not L = L for not L ∈AP . Let
ΔM={not L | L∈LitP\M} 5 for M ⊆LitP . It was shown that answer sets (resp.
paraconsistent stable models) of an ELP P can be captured by stable argument
extensions of the ABA translated from the ELP Ptr (resp. P) as follows.
4 In this paper, the term “p-stable models” is used as not abbreviation of partial
stable model semantics by Przymusinski [18] but as that of paraconsistent stable
model semantics by Sakama and Inoue [20].
5 In [22], ¬.CM is used rather than ΔM to refer to the set {not L | L ∈LitP \M}.
However for notational convenience, ΔM is used instead of ¬.CM to refer to this set
in this paper.

42
T. Wakaki
Theorem 2. [22, Theorem 3]. Let P be an ELP. Then M is a p-stable model
of P iﬀthere is a stable extension E of the ABA framework F(P) such that
M ∪ΔM = Concs(E)
(in other words, M = Concs(E) ∩LitP ).
Theorem 3. [22, Theorem 4]. Let P be an ELP. Then S is an answer set of
P iﬀthere is a stable extension Etr of the ABA framework F(Ptr) such that
S ∪ΔS = Concs(Etr)
(in other words, S = Concs(Etr) ∩LitP ).
For a consistent ELP, the following Theorem holds.
Theorem 4. [23, Theorem 5]. Let P be a consistent ELP. Then S is an answer
set of P iﬀthere is a consistent stable argument extension E of the ABA frame-
work F(P) such that S ∪ΔS = Concs(E).
3
ABA for Extended Disjunctive Logic Programming
3.1
Assumption-Based Frameworks Translated from EDLPs
We propose a generalized assumption-based framework (ABF) translated from
an EDLP, which incorporates explicit negation in Heyninck et al.’s ABF induced
by a DLP to resolve problems addressed in the introduction. An ABF translated
from an EDLP is based on the logic constructed by three inference rules: Modus
Ponens (MP), Resolution (Res) and Reasoning by Cases (RBC):
[MP]
ψ ←φ1, . . . , φn
φ1
φ2 · · · φn
ψ
[Res]
ψ′
1| . . . |ψ′
m|ℓ1| . . . |ℓn|ψ′′
1| . . . |ψ′′
k
not ℓ1 · · · not ℓn
ψ′
1| . . . |ψ′m|ψ′′
1| . . . |ψ′′
k
ℓ1
ℓ2
ℓn
...
...
...
[RBC]
ψ
ψ
· · ·
ψ
ℓ1| . . . |ℓn
ψ
where | is the connective of a disjunction, ℓi is a propositional literal, each
φi ∈{ℓi, not ℓi} is a propositional literal or its NAF-literal, and ψ, ψi are dis-
junctions of propositional literals using |. Since ψ ←is identiﬁed with ψ ←τ,
[MP] implies Reﬂexivity: [Ref] ψ←
ψ . In what follows, ⊢R denotes derivability using
three inference rules: [MP] (including [Ref]), [Res] and [RBC].
Deﬁnition 5. Given an EDLP P, we denote by L=⟨LEDLP, ⊢R⟩the logic based
on the language LEDLP6 which consists of disjunctions of propositional literals
(ℓ1| . . . |ℓn, for n ≥1, ℓi ∈LitP ), NAF-literals (not ℓfor ℓ∈LitP ) and rules
from P of the form (2).
6 LEDLP (resp. LDLP) is deﬁned under a given EDLP (resp. DLP).

Assumption-Based Argumentation for Extended DLP
43
Remark 1: Heyninck and Arieli’s ABF [13] is based on the logic L=⟨LDLP, ⊢⟩,
where ⊢is constructed by three inference rules: [MP], [Res] and [RBC] having the
restricted forms such that | (resp. a literal ℓi) is replaced with ∨(resp. an atom
pi), each φi ∈{pi, not pi} is an atom or a NAF-atom, and ψ, ψi are disjunctions
of atoms (p1 ∨. . . ∨pn, for n ≥1).
Σ ⊢R ϕ holds iﬀϕ is either in Σ or is derived from Σ using three inference
rules above. According to Deﬁnition 2, Σ ⊢R ϕ iﬀϕ ∈CnL(Σ), where CnL(Σ)
is the L-based transitive closure of Σ (namely, the ⊆-smallest set that contains
Σ ⊆LEDLP and is closed under [MP], [Res] and [RBC]). Notice that for any ϕ ∈
CnL(Σ), if ϕ is not of the form ℓ1| . . . |ℓn (where ℓi ∈LitP ), then ϕ ∈Σ.
For a special ABF ⟨L, Γ, Λ, −⟩such that each assumption from Λ has a unique
contrary (i.e. | −α|=1 7 for ∀α ∈Λ), we may denote such an ABF by a tuple
⟨L, Γ, Λ, ¯¯⟩, where ¯¯ is the total mapping from Λ into L and α ∈L is the contrary
of α ∈Λ. In what follows, let NAFP ={not ℓ| ℓ∈LitP } and LP =LitP ∪NAFP for
an EDLP P. We are now ready to deﬁne an ABF translated from an EDLP.
Deﬁnition 6. Let P be an EDLP. The assumption-based framework (ABF)
translated from P is deﬁned by: ABF(P)=⟨L, P, AP ,¯¯⟩, where L=⟨LEDLP, ⊢R⟩,
AP = NAFP ={not ℓ| ℓ∈LitP } and not ℓ=ℓfor every not ℓ∈AP .
In the following, we show the ABF translated from an EDLP P is always ﬂat.
Proposition 2. ABF(P)=⟨L, P, AP ,¯¯⟩translated from an EDLP P is ﬂat.
Proof. Let Δ ⊆AP . Since ⊢R is reﬂexive, it holds that (i) P ∪Δ ⊢R α for
∀α ∈Δ. On the other hand, it holds that (ii) P ∪Δ ̸⊢R β for ∀β ∈AP \Δ, since
assumptions do not occur in the head of a rule from P. Then due to (i),(ii), it
holds that Δ={α ∈AP | P ∪Δ ⊢R α} for ∀Δ ⊆AP . Thus ABF(P) is ﬂat. □
In ABF(P), consistency of a set of literals X ⊆LP is deﬁned using a consequence
operator CNP though in a standard ABA, CNR is used as shown in Deﬁnition 1.
Deﬁnition 7. Let ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF translated from an
EDLP P. For a set X ⊆LP = LitP ∪NAFP , X is said to be contradictory
iﬀX is contradictory w.r.t. ¯¯, i.e. there exists an assumption α ∈AP such that
{α, α} ⊆X; or X is contradictory w.r.t. ¬, i.e. there exists s ∈LP such that
{s, ¬s} ⊆X.
Let CNP : ℘(LP )→℘(LP ) be a consequence operator such that for X ⊆LP ,
CNP (X)
def
= {φ ∈LP | P ∪X ⊢R φ} = CnL(P ∪X) ∩LP .
Then CNP (X) is said to be the closure of X. X is said to be closed w.r.t. CNP iﬀ
X=CNP (X). A set X ⊆LP is said to be inconsistent iﬀthe closure CNP (X) is
contradictory. X is said to be consistent iﬀit is not inconsistent.
7 For a set S, |S| denotes the cardinality of S.

44
T. Wakaki
3.2
Correspondence Between Answer Sets of an EDLP and Stable
Assumption Extensions
Proposition 1 [13] for a DLP is generalized to Proposition 3 and Proposition 4
for an EDLP shown below. To this end, we prepare the following lemma.
Lemma 1. Let P be a propositional NDP and PD be the DLP translated
from P which is obtained by replacing a rule: p1|. . .|pk ←pk+1,. . ., pm, not
pm+1,. . ., not pn from P with the rule p1∨. . .∨pk ←pk+1,. . ., pm, not pm+1,. . .,
not pn, where HBPD = HBP . Then M is an answer set of P iﬀM is a stable
model of PD.
Based on Lemma 1, Proposition 1 for a DLP under the logic L=⟨LDLP, ⊢⟩is
mapped to Corollary 1 for an NDP under the logic L=⟨LEDLP, ⊢R⟩as follows.
Corollary 1. Let P be an NDP and ABF(P)=⟨L,P,AP ,¯¯⟩where L=⟨LEDLP,⊢R⟩.
Then (i) If M is an answer set of P, then Δ={not p | p ∈(HBP \M)} is a stable
assumption extension of ABF(P). (ii) Conversely if Δ is a stable assumption
extension of ABF(P), then M={p ∈HBP | not p ̸∈Δ} is an answer set of P.
We show answer sets (resp. p-stable models) of an EDLP P are captured by
stable assumption extensions of the ABF translated from Ptr (resp. P) as follows.
Proposition 3. Let ABF(P) = ⟨L, P, AP ,¯¯⟩be the ABF translated from an
EDLP P. If M is a p-stable model of P, Δ={not ℓ| ℓ∈(LitP \ M)} is a
stable assumption extension of ABF(P). Conversely if Δ is a stable assumption
extension of ABF(P), M={ℓ∈LitP | not ℓ̸∈Δ} is a p-stable model of P.
Proof. Let P + be the NDP which is obtained from an EDLP P by replacing each
negative literal ¬L in P with a newly introduced atom L
′. For a set S ⊆LitP ,
let S+ be the set obtained by replacing each negative literal ¬L in S with a
newly introduced atom L
′. Then the Herbrand base HBP + of P + is (LitP )+.
(i) Suppose that M is a p-stable model of an EDLP P. Then
M is a p-stable model of an EDLP P iﬀM + is an answer set of an NDP P +
iﬀΔ+={not p | p ∈(HBP +\M +)} is a stable assumption extension of ABF(P +)
iﬀΔ={not ℓ| ℓ∈(LitP \ M)} is a stable assumption extension of ABF(P).
(ii) The converse is also proved in a similar way to (i).
□
Proposition 4. Let P be an EDLP and ABF(Ptr)=⟨L, Ptr, AP ,¯¯⟩be the ABF
translated from the EDLP Ptr=P ∪{L ←p, ¬p | p ∈LitP , L ∈LitP }, where
LPtr=LP . If S is an answer set of P, then Δ={not ℓ| ℓ∈(LitP \S)} is a
stable assumption extension of ABF(Ptr). Conversely if Δ is a stable assumption
extension of ABF(Ptr), then S={ℓ∈LitP | not ℓ̸∈Δ} is an answer set of P.
Proof. Due to [20, Theorem 3.5], S is an answer set of an EDLP P iﬀS is
p-stable model of Ptr. Then based on Proposition 3 and [20, Theorem 3.5], this
proposition is easily proved.
□

Assumption-Based Argumentation for Extended DLP
45
CNP (Δ) gives us the conclusion of an assumption extension Δ. In Proposition 5
below, we show the relationship between answer sets (or p-stable models) of an
EDLP and the conclusions of assumption extensions of the translated ABF.
Lemma 2. Let M be an answer set of an NDP P and let ΔM={not L | L ∈
HBP \M}. Then p ∈M iﬀp ∈CnL(P ∪ΔM) for p ∈HBP .
Proof. Let PD be the DLP translated from an NDP P as shown in Lemma 1.
Based on [13, Corollary 1] and Lemma 1,
p ∈M iﬀp ∈CnL(PD ∪ΔM) for p ∈HBP , where L=⟨LDLP, ⊢⟩,
iﬀp ∈CnL(P ∪ΔM), for p ∈HBP , where L=⟨LEDLP, ⊢R⟩.
□
Lemma 3. Let M be a p-stable model of an EDLP P and ΔM={not ℓ| ℓ∈
(LitP \ M)}. Then ℓ∈M iﬀℓ∈CnL(P ∪ΔM) for ℓ∈LitP .
Proposition 5. Let P be an EDLP, M (resp. S) be a p-stable model (resp. an
answer set) of P, and ΔM={not ℓ| ℓ∈(LitP \ M)} (resp. ΔS={not ℓ| ℓ∈
(LitP \ S)}) be the stable assumption extension of ABF(P) (resp. ABF(Ptr)).
Then it holds that,
CNP (ΔM)=M ∪ΔM,
and
CNPtr(ΔS)=S ∪ΔS.
Proof.
1. Let M be a p-stable model of an EDLP P. Then for ℓ∈LitP , it holds that
ℓ∈M iﬀℓ∈CnL(P ∪ΔM) iﬀℓ∈CNP (ΔM) according to Lemma 3. Besides
not ℓ∈ΔM iﬀnot ℓ∈CNP (ΔM). Hence CNP (ΔM)=M ∪ΔM holds.
2. Let S be an answer set of P. Then S is a p-stable model of Ptr due to [20,
Theorem 3.5]. Besides ΔS={not ℓ| ℓ∈(LitP \ S)} is the stable assumption
extension of ABF(Ptr) due to Proposition 4. Hence by applying the result of
the item 1 to a p-stable model S of Ptr, we obtain CNPtr(ΔS)=S ∪ΔS.
□
3.3
Arguments and Argument Extensions in ABFs Translated
from EDLPs
We deﬁne arguments and attacks in the ABF translated from an EDLP P.
Deﬁnition 8. Let ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF translated from an
EDLP P, where L=⟨LEDLP, ⊢R⟩. Ψ ∈LEDLP is said to be a defeasible consequence
of P and K ⊆AP if P ∪K ⊢R Ψ in which any assumption belonging to K is
used to derive Ψ. K is said to be a support for Ψ w.r.t. P.
P∪K ⊢R Ψ addressed above is represented by a tree structure TΨ(K) as follows.
Deﬁnition 9. Let L=⟨LEDLP, ⊢R⟩and ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF
translated from an EDLP P. Let TΨ(K) denote P ∪K ⊢R Ψ where K is a
support for Ψ w.r.t. P. In other words, TΨ(K) is a tree with the root node
labelled by Ψ ∈LEDLP deﬁned as follows.
1. The cases using no inference rules:
(1) For not ℓ∈AP , there is a one-node tree TΨ(K) whose root node (i.e. the
child) is labelled by Ψ=not ℓand K={not ℓ}.

46
T. Wakaki
(2) For a rule r ∈P, there is a one-node tree TΨ(K) whose root node (i.e.
the child) is labelled by Ψ=r and K = ∅.
2. The cases using inference rules:
(1) i. For a rule ψ←∈P, by [Ref], there is a tree Tψ(K) whose root node N
is labelled by ψ and N has a unique child node, namely a one-node tree
Tr(∅) where r = ψ←. Then K=∅.
ii. For a rule ψ ←φ1, · · · , φn in P, if for each φi (1 ≤i ≤n), there exists
a tree Tφi(Ki) with the root node Ni labelled by φi, then by [MP], there is
a tree Tψ(K) with the root node N labelled by ψ and N has a child N0
labelled by r = ψ ←φ1, · · · , φn which is a one-node tree Tr(∅) as well as
n children Ni (1 ≤i ≤n) where Ni is the root of a tree Tφi(Ki). Then
K=
i Ki.
(2) Let Φ=ψ′
1| . . . |ψ′
m|ℓ1| . . . |ℓn|ψ′′
1| . . . |ψ′′
k and Ψ=ψ′
1| . . . |ψ′
m|ψ′′
1| . . . |ψ′′
k,
where ℓi ∈LitP (1 ≤i ≤n). If there is a tree TΦ(K′) with the root node
N0 labelled by Φ, then by [Res], there is a tree TΨ(K) with the root node
N labelled by Ψ and N has a child N0 as well as n children N1, . . . Nn
each of which is a one-node tree Tφi({φi}) where φi=not ℓi (1 ≤i ≤n).
Then K=K′ ∪n
i=1{not ℓi}.
(3) Let (ℓi . . . ψ) 8 denote the reasoning for the case ℓi and Tℓi(∅) be a one-
node tree whose root is labelled by ℓi.
If the following conditions (1), (2) are satisﬁed: (1) there is a tree TΦ(K′)
whose root node N0 is labelled by Φ=ℓ1|. . .|ℓn; (2) for each ℓi (1 ≤i ≤n),
there exists reasoning for a case ℓi such that (ℓi . . . ψ), namely P ∪{ℓi} ∪
Ki ⊢R ψ for ∃Ki ⊆AP , which is represented by a tree Tψ(Ki) constructed
by newly introducing a tree Tℓi(∅) in this deﬁnition;
then by [RBC], there is a tree Tψ(K) with the root node N labelled
by ψ and N has the child N0 as well as n children N1, . . . Nn where
each Ni (1 ≤i ≤n) is the root of a tree Tψ(Ki) for the case ℓi. Then
K=K′ ∪n
i=1{Ki}.
In ABF(P) translated from an EDLP P, an argument is deﬁned as a special
tree Tφ(K) whose root node is labelled by a literal or a NAF-literal φ ∈LP, and
the attack relation attacks is deﬁned as usual.
Deﬁnition 10. Let ABF(P)=⟨L, P, AP , ¯¯⟩be the ABF translated from an
EDLP P and φ ∈LP = LitP ∪NAFP . Then in ABF(P),
• an argument for a conclusion (or claim) φ supported by K ⊆AP (K ⊢φ, for
short) is a (ﬁnite) tree Tφ(K) whose root node is labelled by φ ∈LP.
• K1 ⊢φ1 attacks K2 ⊢φ2 iﬀφ1=α for ∃α ∈K2.
In ABF(P), the semantics is also given by argument extensions as follows.
8 This is depicted vertically in the inference rule of [RBC].

Assumption-Based Argumentation for Extended DLP
47
Deﬁnition 11. Let ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF translated from an
EDLP P, AR be the set of arguments generated from ABF(P), and Args⊆AR.
Then Args is conﬂict-free iﬀ̸ ∃A, B ∈Args such that A attacks B. Args is a
stable argument extension iﬀit is conﬂict-free and attacks every argument in
AR \ Args.
We show there is a 1-1 correspondence between stable argument extensions and
stable assumption extensions of an ABF translated from an EDLP as follows.
Deﬁnition 12. Let ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF translated from an
EDLP P, and AR be the set of all arguments that can be generated from
ABF(P). Asms2Args : ℘(AP ) →℘(AR) and Args2Asms : ℘(AR) →℘(AP ) are
functions s.t.
Asms2Args(Asms)={K ⊢φ ∈AR | K ⊆Asms},
Args2Asms(Args)={α ∈AP | α ∈K for an argument K ⊢φ ∈Args}.
Theorem 5. Let ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF translated from an EDLP
P, and AR be the set of all arguments generated from ABF(P). If Asms⊆AP
is a stable assumption extension, then Asms2Args(Asms) is a stable argu-
ment extension, and if Args ⊆AR is a stable argument extension, then
Args2Asms(Args) is a stable assumption extension.
Proof.(Sketch) This is proved in a similar way to the proof in [22, Theorem 2].
3.4
Correspondence between Answer Sets of an EDLP and Stable
Argument Extensions
First of all, we show there is a 1-1 correspondence between answer sets of an
NDP P and stable argument extensions of the ABF translated from P as follows.
Theorem 6. Let ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF translated from an NDP
P, AR be the set of all arguments generated from ABF(P) and E ⊆AR. Then
M is an answer set of an NDP P iﬀthere is a stable argument extension E of
ABF(P) such that M ∪ΔM = Concs(E)=CNP (ΔM), where ΔM={not p | p ∈
(HBP \ M)} is a stable assumption extension of ABF(P).
Proof.
Let NAFP ={not p | p ∈HBP } and ΔM ⊆NAFP . Firstly we show that
due to the form of inference rules, P ∪ΔM ⊢R not p iﬀnot p ∈ΔM. In other
words,
not p ∈CnL(P ∪ΔM)
iﬀ
not p ∈ΔM.
(3)
⇒:
Let M be an answer set of an NDP P. Then there exists the stable
assumption extension ΔM of ABF(P) such that ΔM={not p | p ∈(HBP \ M)}
due to Corollary 1(i). Hence due to Theorem 5, there exists the stable argument
extension E = Asms2Args(ΔM) = {K ⊢φ | K ⊆ΔM, φ ∈LP = HBP ∪NAFP }.
Thus for M, ΔM and E=Asms2Args(ΔM), it holds that
Concs(E)= {φ ∈LP | K ⊢φ, K ⊆ΔM, LP = HBP ∪NAFP }
={φ ∈LP | P ∪ΔM ⊢R φ} = {φ ∈LP | φ ∈CnL(P ∪ΔM)} = M ∪ΔM.
(due to Lemma 2 and (3))

48
T. Wakaki
⇐:
Let E be a stable argument extension of ABF(P) translated from an NDP
P. Then due to Theorem 5, there is the stable assumption extension Δ of
ABF(P) s.t. Δ = Args2Asms(E) = {α | α ∈K for K ⊢φ in E} = 
i Ki for
Ki ⊢φi ∈E. Moreover due to Corollary 1(ii), for this stable assumption exten-
sion Δ, there is the answer set M of the NDP P such that M={p ∈HBP | not p ̸∈
Δ)}, while for this answer set M, ΔM={not p ∈NAFP | p ̸∈M} is the sta-
ble assumption extension of ABF(P) due to Corollary 1(i). Thus obviously
ΔM = Δ. Then for a stable argument extension E, Δ=
i Ki s.t. Ki ⊢φi ∈E,
M={p ∈HBP | not p ̸∈Δ} and ΔM = Δ, it holds that
Concs(E) = {φ | K ⊢φ ∈E for φ ∈LP = HBP ∪NAFP }
={φ ∈LP | P ∪Δ ⊢R φ for Δ = 
i Ki where Ki ⊢φi ∈E}
={φ ∈LP | P ∪ΔM ⊢R φ for M = {p ∈HBP | not p ̸∈ΔM = Δ}}
={φ ∈LP | φ ∈CnL(P ∪ΔM)} = M ∪ΔM. (due to Lemma 2 and (3))
□
Based on Theorem 6, we show that there is a 1-1 correspondence between answer
sets (resp. p-stable models) of an EDLP P and stable argument extensions of
ABF(Ptr) (resp. ABF(P)) as follows, though Propositions 4 and 3 show the
similar correspondences for stable assumption extensions of the respective ABFs.
Theorem 7. Let ABF(P)=⟨L, P, AP ,¯¯⟩be the ABF translated from an EDLP
P, AR be the set of all arguments generated from ABF(P) and E ⊆AR. Then
M is a p-stable model of an EDLP P iﬀthere is a stable argument extension E
of ABF(P) such that M ∪ΔM = Concs(E)=CNP (ΔM), where ΔM is a stable
assumption extension of ABF(P).
Proof. (Sketch) Let P + be the NDP which is obtained from an EDLP P by
replacing each negative literal ¬L in P with a newly introduced atom L
′. Then
since Theorem 6 holds for the NDP P +, this theorem for an EDLP P is proved
in a similar way to Proposition 3.
□
Theorem 8. Let P be an EDLP, ABF(Ptr)=⟨L, Ptr, AP ,¯¯⟩be the ABF trans-
lated from the EDLP Ptr=P ∪{L ←p, ¬p | p ∈LitP , L ∈LitP }, AR be the set
of all arguments generated from ABF(Ptr) and Etr ⊆AR, where LitPtr = LitP ,
NAFP ={not ℓ| ℓ∈LitP }, AP =NAFP and not ℓ=ℓfor every not ℓ∈AP . Then
S is an answer set of an EDLP P iﬀthere is a stable argument extension Etr
of ABF(Ptr) such that S ∪ΔS = Concs(Etr)=CNPtr(ΔS), where ΔS is a stable
assumption extension of ABF(Ptr).
Proof. (Sketch) Based on [20, Theorem 3.5] and Theorem 7, this theorem for
an EDLP P is proved.
□
Theorems 7, 8 for an EDLP are the generalization of Theorems 2, 3 for an ELP.
In what follows, we show the relationship to a disjunctive default theory [16].
Theorem 9. Let P be an EDLP. Then S is the set of all literals from an exten-
sion for the disjunctive default theory emb(P) if and only if there is a stable
argument extension Etr of ABF(Ptr) such that S = Concs(Etr) ∩LitP .
Proof. This theorem is easily proved based on Theorem 8 and Theorem 1.
□

Assumption-Based Argumentation for Extended DLP
49
Example 2. (Cont. Ex.1). To solve the Kyoto protocol problem in argumenta-
tion, we construct ABF(P1) from P1. Its arguments and attacks are as follows:
A1 :{not y}⊢p, A2 :{not p}⊢y, A3 :{not y, not¬r}⊢r, A4 :{not p, not¬f}⊢f,
A5 :{not y, not¬r, not¬k}⊢k,
A6 :{not p, not¬f, not¬k}⊢k,
A7 :{not¬r, not¬f, not¬k}⊢k,
A8 :{not p}⊢not p,
A9 :{not y}⊢not y,
A10 :{not r}⊢not r,
A11 :{not f}⊢not f,
A12 :{not k}⊢not k,
A13 :{not ¬p} ⊢not ¬p,
A14 :{not ¬y} ⊢not ¬y,
A15 :{not ¬r} ⊢not ¬r,
A16 :{not ¬f} ⊢not ¬f,
A17 :{not ¬k} ⊢not ¬k;
attacks={(A1, A2), (A1, A4), (A1, A6), (A1, A8), (A2, A1), (A2, A3), (A2, A5),
(A2, A9), (A3, A10), (A4, A11), (A5, A12), (A6, A12), (A7, A12)}.
Figure 1 shows A3 which is constructed based on [Ref], [MP] and [Res]. Each Ai
(1 ≤i ≤6) uses [Res]. Instead A7 uses [RBC]. Arguments in ABF(P1) coincide
with those in ABF((P1)tr). Then ABF((P1)tr) (resp. ABF(P1)) has two stable
argument extensions:
E1 = {A1, A3, A5, A7, A9, A11} ∪{Ai|13≤i≤17},
E2 = {A2, A4, A6, A7, A8, A10} ∪{Ai|13≤i≤17},
where Concs(E1)={p, r, k, not y, not f} ∪U and Concs(E2)={y, f, k, not p,
not r} ∪U for U={not ¬p, not ¬y, not ¬r, not ¬f, not ¬k}. Hence the expected
result is successfully obtained since k ∈Concs(Ei) for ∀Ei (i=1,2).
r
p
r
p, not 
r
not r
p | y
p | y
not y
Fig. 1. A3:{not y, not¬r}⊢r in Ex.2
p
p
q
p
p|q
p 
q, not p  
not p
p|q
Fig. 2. A1:{not p} ⊢¬p in Ex.3
Example 3. Consider P2={¬p|q ←,
q ←¬p, not ¬q,
¬p ←q, not p}, where
LitP2 = {p, q, ¬p, ¬q}. P2 is not a head-cycle-free EDLP (HEDLP)9 [3] but a
general EDLP [11]. P2 has the unique answer set S = {¬p, q}, which is its unique
p-stable model. In contrast, ABF(P2) translated from P2 has the unique stable
assumption extension ΔS = {not p, not ¬q}. Besides ABF(P2) has arguments:
A1 :{not p} ⊢¬p,
A2 :{not ¬q} ⊢q,
A3 :{not q} ⊢¬p,
A4 :{not ¬p} ⊢q,
A5 :{not ¬q, not p} ⊢¬p,
A6 :{not p, not ¬q} ⊢q,
A7 :{not ¬p, not p} ⊢¬p,
A8 :{not q, not ¬q} ⊢q,
A9 :{not ¬p} ⊢not ¬p,
A10 :{not q} ⊢not q,
A11 :{not p} ⊢not p,
A12 :{not ¬q} ⊢not ¬q,
and attacks = {(A1, A4), (A1, A7), (A1, A9), (A2, A3), (A2, A8), (A2, A10), (A3, A4),
(A3,A7), (A3,A9), (A4,A3), (A4,A8), (A4,A10), (A5,A4), (A5,A7), (A5,A9), (A6,A3),
9 This means that the knowledge expressed by the EDLP P2 cannot be expressed by
ELPs due to complexity results shown in [3,11].

50
T. Wakaki
(A6, A8), (A6, A10), (A7, A4), (A7, A7), (A7, A9), (A8, A3), (A8, A8), (A8, A10)}.
Figure 2 shows the tree structure of the argument A1 : {not p} ⊢¬p which
is constructed based on the inference rules: [Ref], [MP] and [RBC].
In this case, Arguments in ABF(P2) coincide with those in ABF((P2)tr). Thus
ABF(P2) (resp. ABF((P2)tr)) has the unique stable argument extension:
E={A1, A2, A5, A6, A11, A12}, where Concs(E)={¬p, q, not p, not ¬q}=S ∪ΔS
for the answer set S and the stable assumption extensionΔS shown above.
Example 4. Consider the EDLP P3 = {¬a|b ←not ¬b,
a ←not c}, It has the
unique answer set S={a, b}, while it has two p-stable models M1={a, b}=S,
M2={a, ¬a}, where M1 is consistent but M2 is inconsistent. Let ABF(P3) be
the ABF translated from P3, which has arguments and attacks as follows:
A1 :{not c} ⊢a,
A2 :{not b, not ¬b} ⊢¬a,
A3 :{not ¬a, not ¬b} ⊢b,
A4 :{not a} ⊢not a,
A5 :{not b} ⊢not b,
A6 :{not c} ⊢not c,
A7 :{not ¬a} ⊢not ¬a,
A8 :{not ¬b} ⊢not ¬b,
A9 :{not ¬c} ⊢not ¬c,
attacks={(A1, A4), (A2, A3), (A2, A7), (A3, A2), (A3, A5)}.
Then ABF(P3) has two stable argument extensions E1, E2 as follows.
E1 = {A1, A3, A6, A7, A8, A9},
E2 = {A1, A2, A5, A6, A8, A9},
where
Concs(E1)= {a, b, not c, not ¬a, not ¬b, not¬c} =M1 ∪ΔM1,
Concs(E2)= {a, ¬a, not b, not c, not ¬b, not¬c} =M2 ∪ΔM2,
for p-stable models M1, M2 of P3 and stable assumption extensions ΔM1, ΔM2
s.t. ΔM1= {not c, not ¬a, not ¬b, not¬c}, ΔM2={not b, not c, not ¬b, not¬c}.
In contrast, ABF((P3)tr) has the arguments Ai (1≤i≤9) along with Aj (10≤
j ≤15), where A10 :{not b, not ¬b, not c}⊢a, A11 :{not b, not ¬b, not c}⊢b,
A12 :{not b, not ¬b, not c}⊢c,
A13 :{not b, not¬b, not c}⊢¬a,
A14 :{not b, not¬b, not c}⊢¬b,
A15 :{not b, not¬b, not c}⊢¬c.
As a result, ABF((P3)tr) has the unique stable argument extensions Etr:
Etr = {A1, A3, A6, A7, A8, A9}=E1,
where Concs(Etr)={a, b, not c, not ¬a, not ¬b, not¬c}=S ∪ΔS for the unique
answer set S of P3 and the unique stable assumption extension ΔS of
ABF((P3)tr).
Remark 2: In Example 2, [Ref] is used to construct each Ai (1 ≤i ≤7). Thus
we need [Ref]. Without [MP], we cannot build Ai (3 ≤i ≤7), which means
we cannot infer k. Thus we need [MP]. In Example 3, [RBC] is used to construct
A1, A2, A5, A6. Hence without [RBC], we cannot construct these arguments, which
means ABF(P2) has no stable extension. Thus we need [RBC]. In Example 4,
without [Res], we cannot construct A3 along with A2. Thus we need [Res].
3.5
Correspondence Between Answer Sets of a Consistent EDLP
and Consistent Stable Argument Extensions
Rationality postulates are deﬁned in ABF(P) translated from an EDLP P like
Deﬁnition 1. In what follows, we show that such ABF(P) always satisﬁes the
closure-property (or direct consistency postulate [5]) under the stable semantics.

Assumption-Based Argumentation for Extended DLP
51
Deﬁnition 13. (Rationality postulates). Given an EDLP P, ABF(P)=
⟨L, P, AP ,¯¯⟩is said to satisfy the consistency-property (resp. the closure-
property) under the σ semantics if for each σ argument extension E of the
AA framework AF F generated from F=ABF(P), Concs(E) is consistent (resp.
Concs(E) is closed w.r.t. CNP ).
Theorem 10. Let F be ABF(P) =⟨L, P, AP ,¯¯⟩translated from an EDLP P
and E be a stable argument extension of AFF generated from F=ABF(P).
1. F satisﬁes the closure-property under the stable semantics.
2. F satisﬁes the consistency-property under the stable semantics
iﬀfor every E, Concs(E) is consistent
iﬀfor every E, Concs(E) is not contradictory w.r.t. explicit negation ¬.
Proof.
1. Let M be a p-stable model of P and E be a stable argument extension of
F =ABF(P) satisfying M ∪ΔM = Concs(E). Then CNP (M ∪ΔM) = {φ ∈
LP |P ∪M ∪ΔM ⊢R φ}=ΔM ∪{ℓ∈LitP |P ∪M ∪ΔM ⊢R ℓ}.
(4)
Due to Lemma 3 and the transitive closure property of CnL, for ℓ∈LitP ,
ℓ∈M iﬀℓ∈CnL(P ∪ΔM) iﬀℓ∈CnL(P ∪ΔM ∪
ℓ∈M{ℓ})=CnL(P ∪ΔM ∪M).
Hence ℓ∈M iﬀP ∪M ∪ΔM ⊢R ℓ
for ℓ∈LitP .
(5)
Then (5) means that M={ℓ∈LitP |P ∪M ∪ΔM ⊢R ℓ}. As a result,
(4) leads to CNP (M ∪ΔM)=M ∪ΔM, namely, CNP (Concs(E))=Concs(E).
Thus F satisﬁes the closure-property under the stable semantics.
2. Based on the result of the item 1, the item 2 is easily proved.
□
Given an EDLP P, the notions of consistent argument extensions and consis-
tency in ABF(P) are deﬁned like [23, Deﬁnitions 6, 7] as follows.
Deﬁnition 14. (Consistent argument extensions). Given an EDLP P, let
E be a σ argument extension of ABF(P)=⟨L, P, AP ,¯¯⟩. Then E is said to be
consistent if Concs(E) is not contradictory w.r.t. ¬; otherwise it is inconsistent.
Deﬁnition 15. (Consistency in ABFs translated from EDLPs). Given
an EDLP P, ABF(P)=⟨L, P, AP ,¯¯⟩is said to be consistent under σ semantics
if ABF(P) has a consistent σ argument extension (or a consistent σ assumption
extension); otherwise it is inconsistent.
We show that there is a 1-1 correspondence between answer sets of a consis-
tent EDLP P and the consistent stable argument extensions of ABF(P) trans-
lated from P, which is a generalization of Theorem 4 for a consistent ELP.
Theorem 11. Let P be a consistent EDLP and ABF(P)=⟨L, P, AP ,¯¯⟩be the
ABF translated from P. Then S is an answer set of P iﬀthere is a consistent sta-
ble argument extension E of ABF(P) such that S ∪ΔS = Concs(E)=CNP (ΔS),
where ΔS is the consistent stable assumption extension of ABF(P).
Proof.(Sketch) We can prove that S is a consistent answer set of an EDLP P
iﬀthere is a consistent p-stable model S of P.
(6)

52
T. Wakaki
Now let P a consistent EDLP. Then based on both Theorem 7 for ABF(P) and
(6), this theorem is proved in a similar way to the proof of [23, Theorem 5]. □
Corollary 2. Let P be a consistent EDLP. The following holds. (i) E is a
consistent stable extension of ABF(P) iﬀE is a stable extension of ABF(Ptr).
(ii) ABF(Ptr) satisﬁes the rationality postulates under the stable semantics.
Example 5. (Innocent unless proved guilty). Consider the EDLP P4 [18],
which states that everyone is pronounced not guilty unless proven otherwise:
P4 = {innocent | guilty ←charged,
¬guilty ←not proven,
charged ←}.
Let i, g, c, p stand for innocent, guilty, charged, proven. Then P4 has the unique
answer set S = {c, i, ¬g}, while it has two p-stable models M1 = {c, i, ¬g} and
M2 ={c, g, ¬g}, where M1 =S is consistent but M2 is inconsistent.
To solve this problem in argumentation, we construct ABF(P4) from P4,
which has arguments:
A1 :{} ⊢c,
A2 :{not g} ⊢i,
A3 :{not i} ⊢g,
A4 :{not p} ⊢¬g,
A5 :{not i}⊢not i,
A6 :{not g}⊢not g,
A7 :{not c}⊢not c,
A8 :{not p}⊢not p,
A9 :{not ¬i}⊢not ¬i,
A10 :{not ¬g}⊢not ¬g,
A11 :{not ¬c}⊢not ¬c,
A12 :{not ¬p}⊢not ¬p,
and attacks = {(A1, A7), (A2, A3), (A2, A5), (A3, A2), (A3, A6), (A4, A10)} where
|attacks|=6. Then ABF(P4) has two stable argument extensions, E1 and E2:
E1 = {A1, A2, A4, A6, A8, A9, A11, A12},
E2 = {A1, A3, A4, A5, A8, A9, A11, A12},
where Concs(E1)= {c, i, ¬g, not g, not p, not ¬i, not ¬c, not ¬p},
Concs(E2)= {c, g, ¬g, not i, not p, not ¬i, not ¬c, not ¬p}.
Thus
E1 is consistent but E2 is inconsistent, where Concs(E1) ∩LitΠ ={c, i, ¬g}=S.
Hence ABF(P4) is consistent under the stable semantics. In contrast,
ABF((P4)tr) has the unique stable argument extension E1 due to six additionally
introduced arguments to Ai(1≤i≤12) and |attacks|=26.
Using ABF(P4), we can decide that the attorney-at-law having the argument
A4 for the claim ¬g wins and the prosecutor having A3 for g loses since A4 ∈E1
and A3 ̸∈E1 for its unique consistent extension E1. Therefore ¬g is decided.
4
Related Work and Conclusion
Beirlaen et al.’s extended ASPIC+ framework [2] as well as Heyninck and Arieli’s
ABF induced by a DLP [13] can handle disjunctive information in argumenta-
tion. However the semantic correspondence to a disjunctive default theory is not
shown in both [2] and [13]. In contrast, thanks to introducing explicit negation,
our approach can capture not only the answer set semantics of an EDLP but
also the semantics of a disjunctive default theory as shown in Theorem 8 and
Theorem 9. Besides Beirlaen et al. [2] allowed arguments to have disjunctive
conclusions, while Heyninck et al. [13] did not deﬁne arguments. In contrast, in
our ABFs, arguments are deﬁned but they are not allowed to have disjunctive

Assumption-Based Argumentation for Extended DLP
53
conclusions (i.e. claims). As one of practical advantages of our approach, even if
disjunctive information exists, we can directly use dialectic proof procedures [7,8]
since the AA framework [6] can be generated from our ABF treating disjunctive
information.
To sum up, as for argument extensions, Theorem 2 and Theorem 3 for an
ELP (resp. Theorem 4 for a consistent ELP) in standard ABA frameworks are
broaden to Theorem 7 and Theorem 8 for an EDLP (resp. Theorem 11 for a
consistent EDLP) in generalized ABA frameworks, i.e. ABFs translated from
EDLPs. Similarly as for assumption extensions, Proposition 1 for ABF induced
by a DLP is generalized to Proposition 3 and Proposition 4 for the respective
ABFs translated from EDLPs.
In (extended) disjunctive logic programming, the existence of disjunction
generally increases the expressive power of logic programs while brings com-
putational penalty [11]. By analogy, argumentation in ABFs translated from
(E)DLPs increases the expressive power of ABF while it would introduce addi-
tional complexity. The analysis of complexity is left for future work.
Acknowledgments. The author would like to thank Chiaki Sakama and the anony-
mous reviewers of the paper for their valuable comments and suggestions.
References
1. Beirlaen, M., Heyninck, J., Straβer, C: Reasoning by cases in structured argumen-
tation. In: Proceedings of the 2017 ACM Symposium on Applied Computing, pp.
989–994, ACM (2017)
2. Beirlaen, M., Heyninck, J., Straβer, C: A critical assessment of Pollock’s work on
logic-based argumentation with suppositions. In: Proceedings of the 17th Interna-
tional Workshop on Non-Monotonic Reasoning (NMR-2018), pp. 63–72 (2018)
3. Ben-Eliyahu, R., Dechter, R.: Propositional semantics for disjunctive logic pro-
grams. Ann. Math. Artif. Intell. 12(1–2), 53–87 (1994)
4. Bondarenko,
A.,
Dung,
P.M.,
Kowalski,
R.A.,
Toni,
F.:
An
abstract,
argumentation-theoretic approach to default reasoning. Artif. Intell. 93, 63–101
(1997)
5. Caminada, M., Amgoud, L.: On the evaluation of argumentation formalisms. Artif.
Intell. 171(5–6), 286–310 (2007)
6. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artif. Intell. 77,
321–357 (1995)
7. Dung, P.M., Kowalski, R.A., Toni, F.: Dialectic proof procedures for assumption-
based, admissible argumentation. Artif. Intell. 170(2), 114–159 (2006)
8. Dung, P.M., Mancarella, P., Toni, F.: Computing ideal sceptical argumentation.
Artif. Intell. 171, 642–674 (2007)
9. Dung, P.M., Kowalski, R.A., Toni, F.: Assumption-based argumentation. In:
Simari, G., Rahwan, I. (eds) Argumentation in Artiﬁcial Intelligence, pp. 199–218,
Springer, Boston (2009). https://doi.org/10.1007/978-0-387-98197-0 10
10. Dung, P.M., Thang, P.M.: Closure and consistency in logic-associated argumenta-
tion. J. Artif. Intell. Res. 49, 79–109 (2014)
11. Eiter, T., Gottlob, G.: Complexity results for disjunctive logic programming and
application to nonmonotonic logics. In: Proceedings of the International Logic Pro-
gramming Symposium (ILPS 1993), pp. 266–278. MIT Press (1993)

54
T. Wakaki
12. Heyninck, J., Arieli, O.: On the semantics of simple contrapositive assumption-
based argumentation frameworks. In: Proceedings of Computational Models of
Argument (COMMA-2018), pp. 9–20, IOS Press (2018)
13. Heyninck, J., Arieli, O.: An argumentative characterization of disjunctive logic
programming. In: Moura Oliveira, P., Novais, P., Reis, L.P. (eds.) EPIA 2019.
LNCS (LNAI), vol. 11805, pp. 526–538. Springer, Cham (2019). https://doi.org/
10.1007/978-3-030-30244-3 44
14. Gelfond, M., Lifschitz, V.: The stable model semantics for logic programming. In:
Proceedings of ICLP/SLP-1988, pp. 1070–1080. MIT Press, Cambridge (1988)
15. Gelfond, M., Lifschitz, V.: Classical negation in logic programs and disjunctive
databases. New Gener. Comput. 9, 365–385 (1991)
16. Gelfond, M., Lifschitz, V., Przymusinska, H., Truszczynski, M.: Disjunctive
Defaults. In: Proceedings of International Conference on Principles of Knowledge
Representation and Reasoning (KR-1991), pp. 230–237 (1991)
17. Prakken, H.: An abstract framework for argumentation with structured arguments.
Argum. Comput. 1(2), 93–124 (2010)
18. Przymusinski, T.C.: Stable semantics for disjunctive programs. New Gener. Com-
put. 9, 401–424 (1991)
19. Reiter, R.: A Logic for default reasoning. Artif Intell. 13, 81–132 (1980)
20. Sakama, C., Inoue, K.: Paraconsistent stable semantics for extended disjunctive
programs. J. Logic Comput. 5(3), 265–285 (1995)
21. Schulz, C., Toni, F.: Justifying answer sets using argumentation. Theory Pract.
Logic Program. 16(1), 59–110 (2016)
22. Wakaki, T.: Assumption-based argumentation equipped with preferences and its
application to decision-making, practical reasoning, and epistemic reasoning. J.
Comput. Intell. 33(4), 706–736 (2017)
23. Wakaki, T.: Consistency in assumption-based argumentation. In: Proceedings of
Computational Models of Argument (COMMA-2020), pp. 371–382, IOS Press
(2020)

A Graph Based Semantics for Logical
Functional Diagrams in Power Plant
Controllers
Aziz Sfar1,2(B)
, Dina Irofti2
, and Madalina Croitoru1
1 GraphIK, INRIA, LIRMM, CNRS and University of Montpellier,
Montpellier, France
medaziz.sfar@gmail.com
2 PRISME Department, EDF R&D, Paris, France
Abstract. In this paper we place ourselves in the setting of formal rep-
resentation of functional speciﬁcations given in logical diagrams (LD) for
veriﬁcation and test purposes. Our contribution consists in deﬁning a for-
mal structure that explicitly encodes the semantics and behavior of a LD.
We put in a complete transformation procedure of the non-formal LD
speciﬁcations into a directed state graph such that properties like oscilla-
tory behavior become formally veriﬁable on LDs. We motivate and illus-
trate our approach with a scenario inspired from a real world power plant
speciﬁcation.
Keywords: System Validation · Functional Speciﬁcations · Logic
Functional Diagram · Graph based Knowledge Representation and
Reasoning
1
Introduction
A power plant is a complex system and its functional behavior is described, for
each of its subsystems, using logical diagrams. The logical diagrams are coded
and uploaded into the controllers. During the power plant life-cycle (around 60
years and even more), the controllers’ code needs to be updated and veriﬁed.
Engineers generate scenarios in order to verify the new code. However, the sce-
narios generation is far from being a simple procedure because of the system’s
complexity. Indeed, the power plant contains a few hundred subsystems, and the
behavior of each subsystem is described in a few hundred pages of logical dia-
grams. Knowing that a logical diagram page contains on average 10 logic blocks,
a quick calculation shows that a power plant behavior can be described by a few
hundred of thousands of logic blocks. Another nontrivial problem for scenario
generation for such systems is caused by the loops existing between the logic
blocks, i.e. the input of some logic blocks depends on their outputs, which can
cause cyclic behaviors. These are indeﬁnite variations of signals in the controller
without a change occurring on its input parameters.
Logical diagram speciﬁcations lack the formal semantics that allow the use
of formal methods for properties veriﬁcation and test scenarios generation.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 55–74, 2022.
https://doi.org/10.1007/978-3-031-11321-5_4

56
A. Sfar et al.
Done through manual procedures, these tasks are tedious. In this paper, we
tackle the problem of lack of semantics of logical diagram speciﬁcations. To
solve this problem, we propose a formal graph model called the Sequential Graph
of State/Transition (SGST) and we deﬁne a transformation method of logical
diagrams into the proposed graph. On the SGST, we show how to formally
verify that the functional behavior described by the logical diagram speciﬁca-
tion is deterministic. In fact, the speciﬁcation model is supposed to provide a
description of the expected behavior of the controller. If the expected behav-
ior itself is non-deterministic, then test generation based on that behavior does
not make sense. This problem can be generated by the presence of loop struc-
tures in the logical diagrams that may prevent the behavior (i.e. the expected
outputs) from converging. The convergence property has to be veriﬁed before
getting to test generation. Verifying this property directly on the logical dia-
gram, which is a mix of logical blocks and connections presented in a non-formal
diagram, is not easy to achieve. This task is possible in theory, as the logical
diagrams can be reduced to combinatorial circuits. In literature, a combinatorial
circuit [7] is a collection of logic gates for which the outputs can be written as
Boolean functions of the inputs. In [11] it is shown that a cyclic circuit can be
combinatorial, and a method based on binary decision diagrams is proposed to
obtain the truth table of the circuit. The problem of where to cut the loops
in the circuits and how to solve this loops has also been addressed in other
studies [12], and applied in particular on the Esterel synchronous programming
language [6]. Another algorithm for analysis cyclic circuits based on minimising
the set of input assignments to cover all the combinatorial circuit has been pro-
posed in [8]. Identifying oscillatory behavior due the combinatorial loops in the
circuit has also been studied (see [2] and references therein). However, all stud-
ies cited here are mainly based on simulation rather than formal veriﬁcation on
models. The focus of these works is entirely dedicated to the veriﬁcation of the
cyclic behavior of the circuits and not to test purposes. Yet, several studies have
already been published for the matter of both formal properties veriﬁcation and
test sequences generation. For instance, in their survey [5] Lee and Yannakakis
address the techniques and challenges of black box tests derived from design
speciﬁcations given in the form of ﬁnite state machines (Mealy machines). In
[13], the author extends the test sequences generation to timed state machines
inspired from the theory of timed automata [1]. These results and many others
(such as formal veriﬁcation of properties [9]) are applicable on state/transition
graphs and can by no means be directly used on logical diagrams. In order to
take advantage of the already established techniques, we focus our study on
transforming logical diagrams into formal state graphs. Prvosot [10], has pro-
posed transformation procedures of Grafcet speciﬁcations into Mealy machines,
allowing the application of the previously mentioned formal methods of veri-
ﬁcation and test generation. However, Grafcets and logical diagrams are com-
pletely diﬀerent representation models. A model transformation of logical dia-
grams into state graphs has been conducted by Electrict´e de France (EDF) [3]
for cyclic behavior veriﬁcation purposes; we note that [3] is not suitable for test
generation and does not take into consideration the behavior of timer blocks.
We inspired our work from both [10] and [3] to develop a formal state graph

A Graph Based Semantics for Logical Functional Diagrams
57
representation of the exhaustive behavior encoded in the logical diagram, the
SGST. The proposed graph allows the veriﬁcation of the cyclic behavior (called
convergence in this paper) and potentially the formal veriﬁcation of other prop-
erties. It also provides the ground to obtain the equivalent Mealy machine on
which the existing formal test generation results can be applied.
This paper is organized as follows. The second section introduces the Logical
Diagram speciﬁcation with an example. A formal deﬁnition of the proposed
SGST is given in Sect. 3. Section 4 details the model transformation procedure
from logical diagrams to SGST graphs. In Sect. 5 we show how the behavior
convergence property could be formally veriﬁed on the SGST. A discussion and
a conclusion are given in the last section.
2
Motivating Example and Preliminary Notions
The main objective of a logical controller is to fulﬁll the set of requirements that
it was built for. After their deﬁnition, the requirements are transformed into a
functional description of the expected behavior called functional speciﬁcation.
A two level veriﬁcation is needed to validate the controllers: ﬁrst, the model is
compared with respect to the speciﬁcation, and second, the physical controller is
tested to verify the conformity with respect to the speciﬁcation (see functional
validation and system validation in [4]). For both aspects, the speciﬁcation model
is the key point and the basis of the procedure, therefore it has to be well
established and comprehended. In this section, we introduce Logical Diagram
speciﬁcations used for power plant controllers, we deﬁne its composing elements
and explain how it describes the functional behavior of the controller.
2.1
Logical Diagrams
Logical diagrams are speciﬁcation models used to describe control functions in
power plants. They contain a number of interconnected logic blocks that deﬁne
how a system should behave under a set of input values.
Figure 1 illustrates a logical diagram extracted from a larger real world con-
troller’s logic speciﬁcation in a power plant. It has ﬁve inputs (denoted by i1
to i5), one output (denoted o1) and logic blocks: either blocks corresponding to
logic gates or status blocks (corresponding to memory and on-delay timer blocks
described below). The gates in Fig. 1 are: two NOT gates followed by two AND
gates and two OR gates. They correspond to the conjunction (·), disjunction
(+), and negation (¯) Boolean operators, respectively (e.g. the output of an OR
gate with two inputs is equal to 0 if and only if both inputs are equal to 0 etc.).
The on-delay timer block gives the value 1 at its output if its input maintains
the value 1 for 2 s; 2 s being the characteristic delay θ of the timer shown in the
T block in Fig. 1. The memory block is a set (E) /reset (H) block: if the E input
is equal to 1, then the output is equal to 1; if the H input is equal to 1, then
the output of the block is 0. If both E and H inputs are equal to 1, the output
is equal to 0 since the memory in this example gives priority to the reset H over

58
A. Sfar et al.
the set E. This priority is indicated in the block symbol by the letter p. A 0 at
both inputs keeps the output of the memory block at the same last given value.
The timer and the memory are blocks whose outputs not only depend on
the values at their inputs, but also on their last memorized status. In this paper
we call them status blocks. Each of them possesses a ﬁnite set of status values
and evolves between them. A status block output value {0, 1} is associated to
each possible status. In the case of the example of Fig. 1, the memory block M2
has two possible status values M 1 and M 0 where the status M 1 gives a logic
value of 1 at the output of the block M2 and M 0 status corresponds to the logic
value 0. The on-delay timer block T1 has 3 statuses denoted TD 0, TI 0 and
TA 1, where the associated block output values are 0, 0 and 1, respectively. We
also note on this example the presence of a loop structure (containing the block
T1, an OR block and the memory block M2).
Fig. 1. Example of a logical diagram speciﬁcation.
More formally, a logical diagram speciﬁcation is composed of I, the set of
inputs of the diagram, O, the set of outputs of the diagram and B, the set of the
logic blocks of the diagram. The logic blocks B connect the outputs O to the
inputs I and deﬁne the function that relates them. B = BS ∪BLG, namely:
– the logic gates BLG: these are the AND, OR and NOT blocks in the diagram.
Each of them is equivalent to a Boolean expression over its entries using the
Boolean operators (+), (.) and (¯) for AND, OR and NOT, respectively.
– the status blocks BS: these are blocks that have a status that evolves between
a set of values. The evolution of a status of a block bs ∈BS depends on the
values at its entries and the last value of its status. A logic value at the output
of the status block is associated to each of these status values.
Deﬁnition 1 (Pstatus set). We denote by Pstatus the set of all the possible
status values of the blocks BS of the logical diagram.
We note that the status values that a block bs ∈BS can take are in a subset of
Pstatus. Some insights are given in the following example.

A Graph Based Semantics for Logical Functional Diagrams
59
Example 1 (Illustration of Pstatus set on the motivating example given in Fig. 1).
For an on-delay timer status block (such as the block denoted T1 in Fig. 1),
P TON
status = {TD 0, TI 0, TA 1}; for a memory status block (such as M2 block
in Fig. 1), P M
status = {M 0, M 1}. The associated block output logic value of a
status value ‘S X’ is indicated in its name by the numeric ‘X’. The Pstatus set
for the example illustrated in Fig. 1 is Pstatus = P TON
status ∪P M
status.
Logic Variables Vars. Logic gates BLG in the diagram can be developed into
Boolean expressions over logic variables Vars by substituting them with their
equivalent Boolean operator. Basically, we end up having outputs O and entries
of BS blocks that are equal to Boolean expressions on Vars.
Deﬁnition 2 (Vars and ExpV ars sets). We deﬁne Vars = I ∪OBS by the set
of logic variables, that includes I, the set of input variables of the logical diagram
and OBS: the set of output variables of status blocks Bs in the diagram.
We denote by ExpV ars the inﬁnite set of all possible Boolean expressions on
logic variables in Vars. For example, (obs
k + ik) ∈ExpV ars. In the reminder of
this paper, we will use the following mathematical notations on sets. Let A be
a set of elements:
Ak =
k



A × A × ... × A is the set of all ordered k-tuples of elements of A. Given
e = (a1, ..., ak) ∈Ak, we denote e(i) the ith element of e, i.e. e(i) = ai. Given
e = (a1, ..., ak) ∈Ak, we denote orde(ak) = k the order k of ak in e.
2.2
Test Generation for Logical Diagrams
Let us explain how these diagrams are supposed to be read and subsequently
implemented in a physical system (i.e. the logic controller1). The diagrams are
evaluated in evaluation cycles repeated periodically. Within each evaluation cycle
the status blocks BS are sequentially evaluated in accordance to a deﬁned order
ω while logic gates are evaluated from left to right.
The logic speciﬁcation diagrams are implemented using a low level program-
ming language into logical controllers. In order to check the conformity of the
code with respect to the diagram, test beds are generated. The tests function in
a black box manner: we check the conformance of the observed output values to
the expected ones for diﬀerent input values.
As one can see, even for a simple diagram like the one given in Fig. 1, ﬁnding
an exhaustive testing strategy is not obvious. A simple solution for scenario test
generation is through simulations of the diagram for each and every possible
combination on the inputs i1...i5. This poses practical diﬃculties for two main
reasons. On one hand, manual exhaustive test generation is a tedious, time-
consuming task that has to be done to hundreds and hundreds of logical diagram
speciﬁcations uploaded on logic controllers. On the other hand, a loop structure
in the logical diagram could cause oscillation problems. This means that logic
1 We refer to the implemented logical diagram speciﬁcation as a logic controller.

60
A. Sfar et al.
values that circulate in a loop could keep changing indeﬁnitely when passing
through the blocks of the loop. This is a non desired phenomenon as it might
prevent the controller’s outputs from converging for a ﬁxed set of input values. To
overcome these diﬃculties, we propose (1) a graph state model called sequential
graph of state/transition (SGST) and (2) a transformation procedure of the
logical diagrams into the SGST. In this new graph, the nodes represent the states
of the logical controller. The edges are labelled with the Boolean conditions over
logic variables Vars. For instance, using the procedure we propose in this paper,
we obtain for the logical diagram shown in Fig. 1 the corresponding sequential
graph of state/transition given by Fig. 2. Throughout this paper, the logical
diagram in Fig. 1 will be our case study.
Fig. 2. The SGST corresponding to the non-formal logical diagram given by Fig. 1,
obtained using our transformation procedure.
3
The Sequential Graph of State/Transition (SGST )
A sequential graph of state transition (SGST) is a combinatorial structure that
explicitly represents all the possible evaluation steps within evaluation cycles of
the logical diagram by the controller.
Formally, the Sequential Graph of State/Transition (SGST) is an
directed graph deﬁned by the tuple (N, E) where N is the set of nodes and
E ⊆N × N is the set of directed edges. Nodes and edges of the graph are both
labeled using the labeling functions lN and lE, respectively.
Deﬁnition 3 (lN function).
For a given set of status blocks BS, the label-
ing function of the nodes of the SGST graph lN is deﬁned as lN:
N →(Pstatus)L, where L = Card(BS). This function assigns, for each sta-
tus block bs ∈BS in the logical diagram, a status value to the node n ∈N in the
SGST.

A Graph Based Semantics for Logical Functional Diagrams
61
Deﬁnition 4 (lE function). For a given set of logic variables Vars, the label-
ing function of the edges of the SGST graph lE : E →ExpV ars, assigns
a logical expression including logic variables from Vars to e ∈E in the SGST.
Fig. 3. SGST graph representation: example of two states nS and nA linked with a
transition e. An edge e of the SGST graph links two states, from the starting node
nS to the arrival node nA. The edge e is labelled with a Boolean expression label
from the ExpV ars set. The starting and arrival nodes are labelled with a set of L
status values, where L is the total number of status blocks in the logical diagram,
i.e. Card(BS) = L. The set ExpV ars contains Boolean expressions on logic variables
in Vars. The set Vars = I ∪OBS contains both the input variables I of the logical
diagram, and the output variables OBS of status blocks BS in the logical diagram.
Deﬁnition 5 (Evallogic function).
We deﬁne Evallogic : Pstatus →{0, 1}
as the logic evaluation function that returns the equivalent logic value of a
status value. In a node n ∈N, the logic value at the output of a status block bs
i is
obs
i = Evallogic(n(i)) where n(i) is the status value of the block bs
i in the node n.
Some notions from Deﬁnitions 2–5 are illustrated in Fig. 3. We remind that a
logic value at the output obs is associated to each status value of bs assigned to
a node n ∈N. Therefore, a node n containing the status values of all blocks BS
encodes the logic values at each of their outputs.
Deﬁnition 6 (nSeq logical sequence).
For a given set of status blocks BS,
we deﬁne nSeq of a node n ∈N as the logical sequence on status blocks output
variables oBS. It is a logical expression associated to the set of status values
in N. This expression uses only the conjunction operator AND ( . ) and the
complement operator (¯) and involves all the status output variables obs ∈OBS
of status blocks bs ∈BS:
nSeq =
L

k=1
(Evallogic(n(k)) · obs
k + Evallogic(n(k)) · obs
k); where L = Card(BS).
Example 2 (Illustration of nSeq on the motivating example given by Fig. 1). In
the SGST example of Fig. 2, the node n1 encapsulates the status values {M2 0,
T1D 0}. These status values correspond to the logic values 0 and 0 at the outputs
of the blocks M2 and T1, respectively. The logical sequence nSeq
1
of the node n1 is
nSeq
1
= (
0



Evallogic(M2 0) ·oM2 +
1



Evallogic(M2 0) ·oM2) · (
0



Evallogic(T1D 0) ·oT1 +
1



Evallogic(T1D 0) ·oT1) = oM2 · oT1.

62
A. Sfar et al.
4
SGST Construction
Given a logical diagram D, we consider ID (the set of input variables of D)
and BD (the set of logic blocks of D). As explained in the previous sections,
BD = BD
S ∪BD
LG, where BD
S is the set of status blocks of D and BD
LG
is the set of logic gates of D. The SGST graph of the diagram D is denoted
SGSTD : (ND, ED) and is constructed as follows.
4.1
Building the SGST Nodes
We will construct one node for each possible combination of status values
between the status blocks. Let us start by deﬁning the set of all the possi-
ble combinations of status values of blocks bs ∈BS
D. Let nT be the num-
ber of on-delay timer blocks and nM the number of memory blocks in the
logical diagram D. We deﬁne the set of all combinations of status values as
Cstatus = (P M
status)nM × (P TON
status)nT . The number of nodes of the SGSTD is
Card(ND) = Card(Cstatus) = 3nT ×2nM ; Card(P TON
status) = 3, Card(P M
status) = 2.
To each of these nodes we attribute a combination of status values using the
lN function: ∀ni ∈ND, lN(ni) = ci where ci ∈Cstatus, i ∈{1..Card(ND)}.
In the example of Fig. 1, P TON
status = {TD 0, TI 0, TA 1}, P M
status = {M 0, M 1},
Cstatus ={(M 0, TD 0), (M 1, TD 0), (M 1, TI 0), (M 1, TA 1), (M 0, TI 0),
(M 0, TA 1)}. Card(Cstatus) = 6, the SGSTD has therefore six nodes n1 to
n6 ∈ND labeled c1...c6 ∈Cstatus, as shown in Fig. 2.
4.2
Building the SGST Edges
Edges that link the nodes in the graph are labelled with a logical expressions
over Vars. If for a set of logic values of Vars, a Boolean expression that labels
an edge starting from a node nS and arriving to a node nA is True, a change of
status values of a BS block in the diagram takes place. The SGSTD of a logical
diagram D is developed to represent the evolution of states of the diagram in a
formal model. The way this evolution works is deﬁned by the evaluation process
of the diagram by the controller. This evaluation is done in periodic cycles:
1. Reading and saving the values of all input variables ik, where k ∈N.
2. Running a sequential evaluation algorithm on status blocks: at this point,
each status block is evaluated, one after another, in accordance to the logic
values at their entries and their last evaluated status value. The logic values
at the entries of status blocks are obtained by the evaluation of logic gates
connected to these entries in a left to right direction. We denote by ω the
order of the evaluation sequence of status blocks.
3. Evaluating outputs. Outputs ok are Boolean expressions of input variables
ID and status blocks output variables OBS.
We build the edges of the SGSTD that link the nodes following the sequential
evaluation of status blocks that we just established. This sequential evaluation

A Graph Based Semantics for Logical Functional Diagrams
63
dictates that only one status block is evaluated at a time. In other words, status
blocks are not evaluated simultaneously. The result of evaluation of a status
block is used in the evaluation of the next status block in the ordered sequence
ω. This is translated in the graph by building edges that only connect nodes that
have the same status values for all status blocks except for one. We call these
nodes neighboring nodes.
Proposition 1. Let n1 and n2 be two nodes of ND and lN(n1) = (μ1, μ2, ..., μL)
and lN(n2) = (λ1, λ2, ..., λL) be their status values. n1 and n2 are two neighboring
nodes and can possibly be linked by an edge in the SGSTD if ∃c ∈{1, ..., L}, with
L = Card(BS
D), satisfying the following two conditions:
– ∀k ∈{1, ..., L}\c, μk = λk; we note that μk = n1(k) is the status value of bs
k
in n1 and λk = n2(k) is the status value of bs
k in n2 where bs
k are status blocks
in BS
D.
– n1(c) = μc ̸= λc = n2(c) where bs
c ∈BS
D is the only status block that changes
value from μc in node n1 to λc in node n2.
Roughly speaking, Proposition 1 tells us that two nodes in the SGST graph can
be neighbours only if all their status values are identical except one. To conclude,
an edge of the graph is equivalent to a change of the status value of a single status
block between two neighboring nodes nS and nA linked by that edge. We refer
to this change of value as an evolution evol and we deﬁne EV OLbs as the set of
all evolution possibilities of bs ∈BS
D between its status values P bs
status.
Deﬁnition 7 (evol tuple). An evolution evol ∈EV OLbs is deﬁned by the
tuple (si, sf, Cevol), with:
– si: the initial status value of the evolution evol; si ∈P bs
status
– sf: the ﬁnal status value of the evolution evol; sf ∈P bs
status
– Cevol: the evolution condition; this is a Boolean expression deducted from the
logical diagram. The evolution from si to sf can only occur if this expression
is True. We note that Cevol ∈ExpV ars.
In order to construct the edges of the SGSTD, ﬁrst the Boolean expressions of
the status block entries have to be calculated (A). Second, the evolution sets
EV OLbs of every status block bs have to be determined using the calculated
expressions (B). Finally, edges of the graph are constructed based on the deter-
mined evolution sets of status blocks (C).
A. Developing the logical expressions at the entries of status blocks:
We remind that the status value of a block bs ∈BS
D is calculated based on its
previous status value and the logic values at the entries of the block. The logic
values at these entries are obtained by evaluating the elements connected to
them. We develop these connections into Boolean expressions. The evaluation
of a Boolean expression associated to an entry of a status block gives the logic
value of that entry. These expressions are developed as follows:
Let xbs
k be an entry of a status block bs
k ∈BS
D. xbs
k could be connected to one
of the following elements:

64
A. Sfar et al.
Algorithm 1. Evolution construction algorithm for status blocks of memory
type
Input: Boolean expressions (E,H)
▷E and H are the two entries of the memory
block
Output: evolution set EV OLbs; Reminder: evol ∈EV OLbs, evol =(si, sf, Cevol)
for all (si, sf) ∈P M
status × P M
status do
if (si, sf) = (M 1, M 0) then
Cevol ←H
else if (si, sf) = (M 0, M 1) then
Cevol ←E · H
end if
evol ←(si, sf, Cevol)
add evol to EV OLM
end for
– An input ij ∈ID: in this case the logic value of this entry is equal to the logic
value of the input variable xbs
k = ij;
– The output of a status block bs
j, with j ̸= k: here, the entry takes the logic
value of the output of the block bs
j denoted by obs
j. Then, xbs
k = obs
j;
– The output of a logic gate bLG: we denote by obLG the output of the logic gate
bLG; then, xbs
k = obLG. The output obLG of the logic gate bLG can be developed
into a Boolean expression that uses the logic operator of the block bLG over
its entries. Entries of bLG that are connected to the output of another logic
gate are further developed into Boolean expressions and so on. This recursive
development continues through all the encountered logic gates and stops at
logic inputs ID and status block outputs OBS.
Example 3. The logical diagram of Fig. 1 has two status blocks T1 and M2 with
outputs denoted oT1 and oM2, respectively. The block T1 has a single entry xT
directly connected to the output oM: xT = oM2. The block M2 has two input
terminals denoted E and H. The entry H is connected to the output of an ‘OR’
gate that we call or1, H = or1. The variable or1 can be developed into the
following expression: or1 = i5 + oT1. The expression of the entry H is therefore
H = i5 + oT Similarly, the input terminal E is connected to an ‘OR’ logic
gate E = or2. We denote by x1 and x2 the input terminals of this ‘OR’ gate.
or2 = x1 + x2. Both x1 and x2 are connected to logic gates. They are therefore
developed into Boolean expression in their turn. Following this process, we obtain
E = i1 · i2 + i3 · i4.
B. Building the evolution sets EV OLbs of every status blocks bs ∈BS
D:
The evolution possibilities of each status block are determined by the nature of
the status block (i.e. memory blocks or timer blocks). Knowing the Boolean
expressions at the entries of a block bs ∈BS
D we deﬁne the algorithms that
construct all the evolution possibilities EV OLbs of the block: Algorithm 1 cor-
responds to the evolution set construction for memory blocks, and Algorithm 2

A Graph Based Semantics for Logical Functional Diagrams
65
Algorithm 2. Evolution construction algorithm for status blocks of timer type
Input: Boolean expressions X
▷X is the entry of the block
Output: evolution set EV OLbs; evol ∈EV OLbs, evol =(si, sf, Cevol)
for all (si, sf) ∈P TON
status × P TON
status do
if (si, sf) = (TD 0, TI 0) then
Cevol ←X
else if (si, sf) = (TI 0, TD 0) then
Cevol ←X
else if (si, sf) = (TI 0, TA 1) then
Cevol ←X · X/θ
else if (si, sf) = (TA 1, TD 0) then
Cevol ←X
end if
evol ←(si, sf, Cevol)
add evol to EV OLM
end for
constructs the evolution set for a timer block. We note that, in the case of timers,
in addition to the logic value at the entry of the block, the status and output
value of timer blocks also depend on time. After receiving a stimulus (i.e. a rins-
ing or falling edge), a timer changes its status automatically after a time period
during which the stimulus action is maintained. In the case of an on-delay timer
with a characteristic delay θ, if its input X is set to 1 for a period Δt > θ, the
timer goes to the activated status TA giving the value 1 at its output instead of
0 in its deactivated status TD. We introduce another logic variable X/θ ∈Vars
such that:

X/θ = 1
if X holds the value 1 for a period t > θ
X/θ = 0
otherwise
Example 4. Applying Algorithms 1 and 2 on the diagram example of Fig. 1, we
obtain the evolution sets of the two status blocks in the diagram. For the timer
T1, we obtain T1 = {evol1, evol2, evol3, evol4}, with:
evol1 = (T1D 0, T1I 0, OM2); evol2 = (T1I 0, T1D 0, OM2);
evol3 = (T1I 0, T1A 1, OM2 · OM2\θ); evol4 = (T1A 1, T1D 0, OM2). For the
memory M2, we obtain EV OLM2 = {evol1, evol2}, with:
evol1 = (M2 0, M2 1, OT1 · e5 · (i1 + i4) · (i1 + i3) · (i4 + i2) · (i2 + i3));
evol2 = (M2 1, M2 0, OT1 + e5).
C. Building the edges ED of the SGSTD: Having built the nodes of the
graph and determined all the evolution sets EV OLbs of status blocks BS
D, we
build the edges that connect these nodes. We remind that the controller evaluates
its status blocks in an ordered sequence ω. In other words, status blocks are
not evaluated simultaneously; they are evaluated one at a time. A node in the
SGSTD encapsulates the status values of all the blocks BS
D. Two nodes in ND
can have one or many diﬀerent status values. The sequential evaluation of the
controller is reproduced in the graph by building edges that only link neighboring
nodes that have the exact same status values of all blocks BS
D except for one

66
A. Sfar et al.
(see Proposition 1). An edge linking two neighboring nodes corresponds to an
evolution evol ∈EV OLbs of a single status block bs. We propose Algorithm 5
for building the edges of the graph SGSTD of a logical diagram D. For each
node nk ∈ND of the SGST, the algorithm generates all the possible outgoing
edges corresponding to all the evolution possibilities of all status blocks BS
D from
the node nk. Algorithms 3 and 4 are used in Algorithm 5 for neighboring nodes
recognition and nodes logical sequences generation.
Algorithm 3. Test whether nS and nA are neighboring nodes (see Proposi-
tion 1)
Input: nS, nA ∈ND
Output1: AreNeighbors ∈{True, False}
Output2: c the index of the status block whose status value is changed from nS to
nA
AreNeighbors ←True
differences ←0
▷number of diﬀerent status values between nS and nA
for all k = 1 to Card(BS
D) do
if nS(k) ̸= nA(k) then
differences ←differences + 1
if differences > 1 then
AreNeighbors ←False
break loop
end if
c ←k
▷c is the index of the block that changes status from nS to nA
end if
end for
5
Reasoning with the SGST
In this section, we show how the convergence of the expected behavior of the
controller described by its logical diagram could be veriﬁed using the equivalent
SGST of that diagram. The SGST is composed of a set of nodes and edges
that reproduce the information encoded in the logical diagram in a formal and
explicit description. A node in the SGST corresponds to a possible state of the
controller, i.e. a possible combination of status values. An edge corresponds to
an evolution of a single status block. That is a change of the status value of a
block bs ∈BS
D. The outgoing edges Enj of a node nj ∈ND in the SGSTD graph
of a diagram D, are all the theoretical evolution possibilities of all the status
blocks from the node nj. In practice, only one of these outgoing edges e ∈Enj,
is traversed depending on the values of the input variables ID of the diagram.
A traversal of an edge ( nS, nA) is the eﬀective transition of the controller’s
state from the node nS to the node nA by running the correspondent status
block evolution of the traversed edge.
We remind that a full evaluation cycle of the logical diagram is held peri-
odically by the controller. In each evaluation cycle of the diagram, status blocks

A Graph Based Semantics for Logical Functional Diagrams
67
Algorithm 4. Build nseq
S
the logical values sequence of outputs OBS
D equivalent
to status values in nS (see Deﬁnition 6)
Input: nS ∈ND
Output: nseq
S
nseq
S
←True
for all k = 1 to Card(BS
D) do
nseq
S
←nseq
S
· (Evallogic(nS(bs
k)) · obs
k + Evallogic(nS(k)) · obs
k)
end for
are evaluated one after another according to an order ω until each and every
block bs ∈BS is evaluated once and only once.
In the SGST, for a set of input values Iv a full evaluation cycle corresponds
to a chain of successive edges traversed one after another in respect to the order
of evaluation ω. In some cases, many successive evaluation sequences ω may have
to be run to ﬁnally converge to a node. However in other cases, even after
multiple evaluation sequences, this convergence may never be reached; Traversal
of edges could be endlessly possible for a set of input values Iv.
The convergence of status values is the property that we are going
to study in the rest of this paper. If we consider the real world case of power
plants, the convergence property has to be veriﬁed on the logical diagrams before
implementing them in the controllers. The non convergence of the evaluation
cycles of the diagram for a set of input values Iv leads to the physical output
signals of the controller alternating continuously between 0 and 1 which is a
non-desired phenomenon. In the SGST graph, this corresponds to a circuit of
nodes being visited over and over again indeﬁnitely. We will deﬁne trails and
circuits in the graph then propose a formal criteria of behavior convergence on
the SGST.
5.1
Traversal of the SGST : Trails
In the SGST, nodes are visited by traversing the edges that link them. A
sequence of visited nodes in the graph is called a trail τ and is deﬁned as follows:
Deﬁnition 8 (Trail τ). For a given SGSTD = (ND, ED), a trail τ ∈(ND)k,
with k ∈N, is an ordered set of nodes (n1, n2,..., nk) where each pair of succes-
sive nodes ni and ni+1, with i ∈{1..k −1}, are neighboring nodes.
A trail is therefore a series of state changes along neighboring nodes in the SGST
graph. From the SGST we can form an inﬁnite number of trails. However, only a
ﬁnite subset of these trails could be eﬀectively traversed in practice. This is due
to the order ω of the evaluation of status blocks. We call trails that are conform
to the order ω determined trails. These trails correspond to the progressive
traversal of viable edges in the SGST for a set of input values Iv.
Deﬁnition 9 (Viability of an edge). Let e = (nS, nA) ∈ED be an edge in
the SGST linking the start node nS to the arrival node nA. The edge e is said

68
A. Sfar et al.
Algorithm 5. Edge construction Algorithm
Input1: EV OLbs ∀bs ∈BS
D
Input2: the set of nodes ND of the SGSTD
Output: the set of edges ED of the SGSTD; e ∈E, e = (nS, nA, label)
for all (nS, nA) ∈ND × ND do
AreNeighbors, c =Algorithm 3(nS, nA)
▷c is the index of the status block
whose status value is changed from nS to nA
if AreNeighbors = True then
nseq
S =Algorithm 4(nS)
for all evol ∈EV OLbsc do
▷bs
c is the block that changes status from nS
▷to nA; EV OLbsc is the evolution set of bs
c
if si(evol) = nS(c) and sf(evol) = nA(c) then ▷Find the evolution of bs
c
▷that corresponds to
▷nS(c) →nA(c)
expression = Cevol(evol)
if expression ∧nSeq
S
̸= False then
▷check if the expression of the evolution is contradictory
▷to the logic values of status blocks outputs OBS
D
▷given by the sequence nSeq
S
of nS
e = (nS, nA)
lE(e) = Cevol(evol)
add e to the set of edges ED
break loop
end if
end if
end for
end if
end for
to be viable for a set of input values Iv if the Boolean expression label(e) is True
for the values Iv. The traversal of the edge e changes the state of the controller
from nS to nA by changing the status value of a single block bs ∈BS
D. We denote
by bs
nS→nA the status block bs whose value was altered by going from nS to nA.
A node nk visited in the middle of a determined trail τ, can have multiple
outgoing edges in the SGST that are viable at the same time for a set of input
values Iv. Only one of the viable edges is traversed in accordance to the edge
traversal determination rule (nk−1, nk)→nk+1: the next viable edge (nk, nk+1)
to be traversed is the one that alters the status value of the block bs
nk→nk+1 of
the lowest order in ω after bs
nk−1→nk the block whose status value changed from
nk−1 to nk.
Proposition 2 (edge traversal determination rule). Let ω ∈(BS)L, be an
order of the evaluation sequence of status blocks, L = Card(BS). Let us suppose
that for a set of input values Iv, the controller is placed in the state of node nk,
coming from the previous node nk−1; between these two nodes, the status value

A Graph Based Semantics for Logical Functional Diagrams
69
of the block bs
nk−1→nk has changed. Let Nnext be the set of all the reachable nodes
from nk by the viable edges e = (nk, nnext), with nnext ∈Nnext.
Then, the next node nk+1 ∈Nnext to be eﬀectively visited in the trail is satisfying:
ordω(bs
nk−1→nk) < ordω(bs
nk→nk+1) < ordω(bs
nk→nnext) ∀nnext ∈Nnext\{nk+1},
where ∀nj ∈ND; ∀(nk, nj) ∈ED then ordω(bs
nk→nj) is the order of evaluation
of the block bs
nk→nj in ω; bs
nk→nj is the status changing block from nk to nj.
Fig. 4. Trail building in an SGST graph. Two possible trails are valid for the same
input values Iv in this example, depending on the order of evaluation of the three status
blocks, M1, M2, M3.
Example 5. Let us consider a logical diagram with three status blocks of memory
type BS = {M1, M2, M3}; the corresponding SGST graph is illustrated in Fig. 4,
and has four nodes and three edges. We ﬁx the set of inputs values (i1, i2, i3) =
(1, 1, 1). We suppose that the last visited node is nk coming from nk−1. We note
that, from the node nk, both edges e2 = (nk, nu) and e3 = (nk, nv), labeled
i2 and i3, respectively, are viable for the input values (1, 1, 1). The following
node of the trail τ = (nk−1, nk) is determined in accordance with Proposition 2.
We ﬁrst consider ω = (M1, M2, M3) the order of evaluation of the three status
blocks. The last traversed edge is e1 = (nk−1, nk) with a change on the status
value of the block M1 of order ordω(M1) = 1 in the evaluation sequence ω.
Edge e2 alters the status value of the block M2 of order ordω(M2) = 2 while
the edge e3 alters the status value of the block M3 of order ordω(M3) = 3.
Since ordω(M1) = 1 < ordω(M2) = 2 < ordω(M3) = 3, the next status block
to be evaluated after M1 is M2, so the next node in the trail τ is nk+1 = nu.
In this case, τ = (nk−1, nk, nu). However, if we consider ω = (M3, M2, M1), i.e.
ordω(M3) = 1, ordω(M2) = 2, ordω(M1) = 3, then the last evaluated block
M1 in the trail (nk−1, nk) is of order 3 which is the last order in the evaluation
sequence ω. For the same input values (i1, i2, i3) = (1, 1, 1), the next block to
be evaluated from nk is this time M3 of the order 1, which corresponds to edge
e3 = (nk, nv). In this case, τ = (nk−1, nk, nv).
We make the assumption that the initial node of a determined trail is a perma-
nent node. A permanent node, unlike a transitional node, is a node in which
the controller’s state can remain permanently for a certain set of input values.
Deﬁnition 10 (permanent node). Let nk ∈ND be a node, and Enk ⊂ED the
set of outgoing edges from the node nk. We say that the node nk is a permanent

70
A. Sfar et al.
node if ∃Iv, a set of input values, satisfying the holding on condition of the node
nk: CHold = 
ei∈Enk label(ei).
Example 6. For the SGST graph given in Fig. 2, node n2 has two outgoing
edges labeled i5 and True. The holding on condition of node n2 is CHold =
i5 · True = False. This condition is False for any set of input values Iv; thus,
n2 is not a permanent node. The node n1 has only one outgoing edge labeled
i5 · (i1 + i4) · (i1 + i3) · (i4 + i2) · (i2 + i3). The holding on condition of node n1 is
CHold = i5 · (i1 + i4) · (i1 + i3) · (i4 + i2) · (i2 + i3) = i5 + i2 · i3 + i2 · i4 + i3 · i1 + i1 · i4,
and can be satisﬁed for certain sets of input values, e.g. (i1, i2, i3, i4, i5) =
(0, 0, 0, 0, 1). Thus, node n1 is a permanent node. From the permanent node n1,
and for an order of evaluation ω = (M2, T1), a possible determined trail that
could be eﬀectively traversed would be τ1 = (n1, n2, n3, n4, n6, n1) for the order
ω = (M2, T1) and the set of input values (i1, i2, i3, i4, i5) = (1, 0, 0, 1, 0).
5.2
Formal Veriﬁcation of the Convergence Property in the SGST
In practice, we say that a signal converges if its periodic evaluation by the logic
controller gives a constant value over a long time range during which the input
signals I remain constant. A non convergent Boolean signal is a signal that
keeps oscillating between 0 and 1 over multiple evaluation cycles of the logic
controller while input values are unchanged. In the SGST, oscillating Boolean
signals correspond to an indeﬁnite visiting of the same subset of nodes over and
over again. This causes an indeﬁnite change of status values, which results in its
turn to an indeﬁnite change of logic values at the output of status blocks.
Deﬁnition 11 (Circuits in the SGST). We deﬁne a circuit in a SGST
graph as a ﬁnite series of nodes (n1, n2, ..., nm) such that the consecutive nodes
nk and nk+1 are neighboring nodes and n1 = nm.
However, a determined trail in the SGST graph could contain a circuit of nodes
without necessarily traversing it indeﬁnitely. Indeed, a trail could correspond to
a one-time traversal of a circuit to leave it as soon as it visits the same node
twice, as shown by Example 7.
Example 7. We consider the SGST graph given by Fig. 5. The status blocks of
the SGST are BS = {M1, M2}. It contains three possible circuits (n1, n2, n1),
(n1, n3, n1) and (n1, n2, n1, n3, n1). We suppose the evaluation order ω = (M2,
M1). We ﬁx a set of input values (i1, i2, i3, i4) = (1, 1, 1, 0). Using Proposition 2,
we obtain the trail τ = (n1, n3, n1, n2), starting from the permanent node n1. We
can observe that τ contains the circuit (n1, n3, n1), but this circuit is quit to the
node n2. However, if we ﬁx the set of input values at (i1, i2, i3, i4) = (1, 1, 0, 0),
and start from node n1, we obtain the trail τ =(n1, n3, n1, n3) that is equivalent
to an indeﬁnite traversal of the circuit (n1, n3, n1).

A Graph Based Semantics for Logical Functional Diagrams
71
Fig. 5. Example of multiple circuits in an SGST graph.
Generally, if for an input Iv the progressive calculation of a the nodes of a trail τ
results in visiting twice the same successive neighboring node couple (nk, nk+1),
then τ corresponds to a circuit of nodes that can be indeﬁnitely visited and the
outputs of the blocks whose status values are changed in that trail are oscillating.
Deﬁnition 12 (Convergence property in a trail). We denote by T ω
SGST
the set of all determined trails in the SGST that can be eﬀectively traversed for
an evaluation order ω. A trail τ = (n1, n2, ..., nm) ∈T ω
SGST is convergent if
ek ̸= ej ∀ek = (nk, nk+1), ej = (nj, nj+1) two tuples of neighboring nodes in τ.
Deﬁnition 13 (Convergent logical diagram). We say that a logical diagram
D is convergent for all the sets of input values if all the determined trails of its
SGSTD are convergent.
We propose a method for searching all the determined trails T ω
SGST for an evalu-
ation order ω. Trails are determined by giving their symbolic Boolean condition
of traversal instead of the sets of input values. This means that a determined
trail τ ∈T ω
SGST is deﬁned by the sequence of its nodes τ = (n1, n2, ..., nm−1, nm)
and its traversal condition Cτ = 
k∈{1..m−1} label(ek = (nk, nk+1)).
Starting from each permanent node in the SGST we calculate all the possible
trails based on the trail determination rule for an order ω (Proposition 2). From
each node we explore all the possible outgoing edges by negating the condition
labels of edges alternating the blocks of the least order. Each label of an explored
edge is added to Cτ. For instance, let us suppose that a trail reaches a node nk
coming from nk−1 and that nk that has two outgoing edges e1 = (nk, nu) and
e2 = (nk, nv). τ = (n1, n2, ..., nk−1, nk), Cτ = 
j∈{1..k−1} label(ek = (nj, nj+1))
(Fig. 6).
We suppose that ordω(bs
nk−1→nk) < ordω(bs
nk→nu) < ordω(bs
nk→nv), for the
order ω. Since the status block altered by e1 is of a lower order than the one
altered by e2, if label(e1) = True then e1 is the next movement in τ, but if
label(e1) = False and label(e2) = True then the next movement in τ is e2.
Thus, two determined trails τ1 and τ2 can branch oﬀfrom the determined trail
τ at nk such that Cτ1 = Cτ · label(e1) and Cτ2 = Cτ · label(e1) · label(e2).
Both new trails continue the course and branch oﬀto more possible trails at
each bifurcation. Path exploration of a trail can stop in one of the following
scenarios:

72
A. Sfar et al.
Fig. 6. Trail traversal condition update.
– If the last encountered node nk is a permanent node. Here, τ is a determined
trail that puts the controller in the state of the node nk starting from the
state of the initial node n1 for all the input values Iv that satisfy Cτ.
– If for the last encountered node nk, the update of the traversal condition
Cτ · label(e = (nk−1, nk)) is False. This means that the trail is not possible
due to a contradiction of the condition labels of the graph edges crossed by
the trail.
– If the last two couple of nodes (nk−1, nk) have already been visited in τ. In
this case τ corresponds to a circuit of nodes that can be eﬀectively traversed
an inﬁnite number of times for the inputs values Iv satisfying condition Cτ.
Example 8. The SGST example of Fig. 2 has only one permanent node n1. Start-
ing from n1, only two determined trails are possible in the case of the evaluation
order ω = (T1, M2):
τ1 = (n1, n1), Cτ1 = i5+i2·i3+i2·i4+i3·i1+i1·i4, τ2 = (n1, n2, n3, n4, n6, n1, n2),
Cτ2 = i5 · (i1 + i4) · (i1 + i3) · (i4 + i2) · (i2 + i3). τ2 does not converge meaning
that for any set of input values Iv that satisﬁes Cτ2 the nodes of τ2 are visited
indeﬁnitely which causes oscillating signals in the controller.
6
Discussion
In this paper, we proposed a formal model, called the SGST graph, representing
the possible states of a controller programmed based on a logical diagram spec-
iﬁcation. We show how to transform the logical diagram into the corresponding
SGST graph, and how to verify the convergence property, i.e. verify that the
controller does not have undesired oscillatory behavior.
Making sure that the behavior described by the logical diagram converges
is crucial for test generation and for the overall veriﬁcation and validation pro-
cedure. However, this is not the sole goal of transforming logical diagrams into
SGST graphs. We developed the SGST to take a step in the application of the
existing formal testing methods on logical diagrams. For the time being, gen-
erating tests derived from logical diagram speciﬁcations of power plants logical
controllers is still handled manually or simulation-based. So, we designed the
SGST to provide an explicit formal and exhaustive representation of the evolu-
tion steps between possible states of the controller described by a logical diagram.
A test scenario is a sequence of these steps which are modeled with edges in the

A Graph Based Semantics for Logical Functional Diagrams
73
SGST. Therefore, the test sequences generation could be transposed into the
application of existing graph traversal techniques such as the Chinese postman
tour [14]. The existing test generation [5,13] and selection techniques are based
on ﬁnite state machines speciﬁcations. We consider the SGST to be an impor-
tant intermediate step to move from non-formal diagrams to state machines. A
coded solution of the developed method has shown that the construction of the
SGST is feasible for small to medium sized logical diagrams and provides very
accurate graph representations. However, the SGST generation for diagrams
with tens of memory and timer blocks is quite expensive in terms of complexity.
This is due to the number of maximal states being in the worst case exponential
to the number of blocks which can quickly lead to an explosion of the graph size.
We are currently working on methods to resolve the complexity problems
such as the partition of the diagram into sub-diagrams and transforming them
in SGSTs then synchronizing them. Further work on complexity as well as
transforming SGST graphs into state machines to apply test generation results
in the literature for controllers described with logical diagrams will be handled
in the future.
References
1. Alur, R., Dill, D.L.: A theory of timed automata. Theor. Comput. Sci. 126(2),
183–235 (1994)
2. Fayyazi, M., Kirsch, L.: Eﬃcient simulation of oscillatory combinational loops. In:
Proceedings of the 47th Design Automation Conference, pp. 777–780 (2010)
3. Jean-fran¸cois Hery, J.C.L.: Stabilit´e de la sp´eciﬁcation logique du contrˆole-
commande - m´ethodologie et mise en œuvre. Technical report, EDF R&D (2019)
4. Power Plants IEC: Instrumentation and control important to safety-general
requirements for systems. IEC 61513. International Electrotechnical Commission
(2011)
5. Lee, D., Yannakakis, M.: Principles and methods of testing ﬁnite state machines-a
survey. Proc. IEEE 84(8), 1090–1123 (1996)
6. Lukoschus, J., Von Hanxleden, R.: Removing cycles in Esterel programs. EURASIP
J. Embed. Syst. 2007, 1–23 (2007)
7. Malik, S.: Analysis of cyclic combinational circuits. IEEE Trans. Comput. Aided
Des. Integr. Circ. Syst. 13(7), 950–956 (1994)
8. Neiroukh, O., Edwards, S., Song, X.: An eﬃcient algorithm for the analysis of cyclic
circuits, vol. 2006, p. 6–pp, April 2006. https://doi.org/10.1109/ISVLSI.2006.18
9. Peled, D., Vardi, M.Y., Yannakakis, M.: Black box checking. In: Wu, J., Chanson,
S.T., Gao, Q. (eds.) Formal Methods for Protocol Engineering and Distributed
Systems. IAICT, vol. 28, pp. 225–240. Springer, Boston (1999). https://doi.org/
10.1007/978-0-387-35578-8 13
10. Provost, J., Roussel, J.M., Faure, J.M.: Translating Grafcet speciﬁcations into
mealy machines for conformance test purposes. Control. Eng. Pract. 19(9), 947–
957 (2011)
11. Riedel, M.D.: Cyclic combinational circuits. California Institute of Technology
(2004)

74
A. Sfar et al.
12. Shiple, T.R., Berry, G., Touati, H.: Constructive analysis of cyclic circuits. In:
Proceedings ED&TC European Design and Test Conference, pp. 328–333 (1996)
13. Springintveld, J., Vaandrager, F., D’Argenio, P.R.: Testing timed automata. Theor.
Comput. Sci. 254(1–2), 225–257 (2001)
14. Thimbleby, H.: The directed Chinese postman problem. Softw. Pract. Exp. 33(11),
1081–1096 (2003)

Database Repair
via Event-Condition-Action Rules
in Dynamic Logic
Guillaume Feuillade, Andreas Herzig(B), and Christos Rantsoudis
IRIT, Univ. Paul Sabatier, CNRS, Toulouse, France
andreas.herzig@irit.fr
Abstract. Event-condition-action (ECA) rules equip a database with
information about preferred ways to restore integrity. They face prob-
lems of non-terminating executions and only procedural semantics had
been given to them up to now. Declarative semantics however exist for
a particular class of ECA rules lacking the event argument, called active
integrity constraints. We generalise one of these semantics to ECA rules
and couch it in a simple dynamic logic with deterministic past.
Keywords: ECA rules · database repair · dynamic logic
1
Introduction
To restore the integrity of a database violating some constraints is an old and
notoriously diﬃcult problem. One of the main diﬃculties is that there are typ-
ically several possible repairs: the integrity constraints alone do not provide
enough clues which the ‘right’ repair is. A natural idea is to add more informa-
tion to the integrity constraints. The most inﬂuential proposal was to move from
classical, static integrity constraints to so-called Event-Condition-Action (ECA)
rules. Such rules indicate how integrity should be restored: when the event occurs
and the condition is satisﬁed then the action is triggered which, intuitively,
makes the violating condition false [4,13,15]. A relational database together
with a set of ECA rules make up an active database [13,24]. Such databases
were studied intensely in the database literature in the last four decades. The
existing semantics are mainly procedural and chain rule applications: the actions
of some ECA rule trigger other ECA rules, and so on. As argued in [7], “their lack
of declarative semantics makes it diﬃcult to understand the behavior of multiple
ECAs acting together and to evaluate rule-processing algorithms in a principled
way”. They are moreover plagued by termination problems.
Up to now only few declarative, logical semantics were given to ECA rules.
Most of them adopted the logic programming paradigm [6,14,19,22,23]; Bertossi
and Pinto considered the Situation Calculus where ECA rules are described in
a ﬁrst-order language plus one second-order axiom and where repairs are per-
formed by means of auxiliary actions [5]. Our aim in this paper is to associate
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 75–92, 2022.
https://doi.org/10.1007/978-3-031-11321-5_5

76
G. Feuillade et al.
a semantics to ECA rules that is based in dynamic logic. A main reason for our
choice is that the latter has built-in constructions allowing us to reason about ter-
minating executions. Another reason is that this allows us to start from existing
declarative semantics for a simpliﬁed version of ECA rules. The latter lack the
event argument and are simply condition-action couples and were investigated
in the database and AI literature since more than 15 years under the denomi-
nation active integrity constraints, abbreviated AIC [7,10–12,20]. AICs “encode
explicitly both an integrity constraint and preferred basic actions to repair it, if it
is violated” [12]. Their semantics notably avoids problems with non-terminating
executions that plague ECA rules. Syntactically, an AIC is a couple of the form
r = ⟨C(r), A(r)⟩
where the condition C(r) is a conjunction of literals L1 ∧· · · ∧Ln (that we
can think of as the negation of a classical, static integrity constraint) and the
action A(r) is a set of assignments of the form either +p or –p. It is supposed
that each of the elements of A(r) makes one of the literals false: +p ∈A(r)
implies ¬p ∈C(r), and –p ∈A(r) implies p ∈C(r). For instance, the AIC
⟨Bachelor ∧Married, –Bachelor⟩says that when Bachelor ∧Married is true
then the action –Bachelor should be executed, that is, Bachelor should be
deleted. Given a set of AICs, a repair of a database D is a set of AIC actions
whose execution produces a database that is consistent with the integrity con-
straints. Their semantics is clear when there is only one AIC: if the set of AICs is
the singleton R = {⟨C(r), A(r)⟩} and D |= C(r) then each assignment α ∈A(r)
is a possible repair of D. When there are several AICs then their actions may
interfere in complex ways, just as ECA rules do: the execution of A(r) makes
C(r) false but may have the side eﬀect that the constraint part C(r′) of another
AIC r′ that was false before the execution becomes satisﬁed, and so on. The con-
sistency restoring actions should therefore be arranged in a way such that overall
consistency is obtained. Moreover, the database should be changed minimally
only. Several semantics achieving this were designed, among which preferred,
founded, and justiﬁed repairs [12] as well as more recent prioritised versions [8].
Implementations of such repairs were also studied, and the account was extended
in order to cope with various applications [16–18].
In this paper we examine whether and how the existing logical semantics of
AICs can be generalised to ECA rules. For that reason (and also because the
term ‘rule’ has a procedural connotation) we will henceforth use the term ECA
constraints instead of ECA rules. Syntactically, we consider ECA constraints of
the form
κ = ⟨E(κ), C(κ), A(κ)⟩
where ⟨C(κ), A(κ)⟩is an AIC and E(κ) is a boolean formula built from assign-
ments of the form either +p or –p. The latter describes the assignments that must
have occurred for the AIC to trigger. Our convention is to call events assign-
ments that happened in the past and led to a violation state, and to call actions
assignments to be performed in the future in order to repair the violation.

Database Repair via Event-Condition-Action Rules in Dynamic Logic
77
Example 1. Let eme,d stand for “e is an employee of department d”. The set of
ECA constraints
Kem =

⟨+eme,d1, eme,d1 ∧eme,d2, {–eme,d2}⟩,
⟨+eme,d2, eme,d1 ∧eme,d2, {–eme,d1}⟩

implements a ‘priority to the input’ policy repairing the functionality constraint
for the employee relation: when the integrity constraint ¬eme,d1 ∨¬eme,d2 is
violated and eme,d1 was made true in the last update action then the latter is
retained and eme,d2 is made false in order to enforce the constraint; symmetri-
cally, eme,d1 is made false when +eme,d2 is part of the last update action.
Observe that if the last update action of our example made neither eme,d1 nor
eme,d2 true, i.e., if ¬+eme,d1 ∧¬+eme,d2 holds, then the database already violated
the integrity constraints before the last update action. In such cases ECA con-
straints are of no help: the violation should have been repaired earlier. Moreover,
if the last update action of our example made eme,d1 and eme,d2 simultaneously
true, i.e., if +eme,d1 ∧+eme,d2 holds, then the ECA constraints of Kem authorise
both –eme,d1 and –eme,d2 as legal repairs.
A logical account of ECA constraints not only requires reasoning about
integrity constraints and actions to repair them when violated, but also rea-
soning about the event bringing about the violation. Semantically, we have to
go beyond the models of AIC-based repairs, which are simply classical valua-
tions (alias databases): in order to account for the last update action we have
to add information about the events that led to the present database state. Our
models are therefore couples made up of a classical valuation D and a set of
assignments H. The intuition is that the assignments in H are among the set
of assignments that led to D; that is, H is part of the last update action that
took place and brought the database into its present state D. For these mod-
els, we show that the AIC deﬁnitions of founded repair and well-founded repair
can be generalised in a natural way to ECA constraints. We then provide a
dynamic logic analysis of ECA constraints. More precisely, we resort to a dialect
of dynamic logic: Dynamic Logic of Propositional Assignments DL-PA [2,3]. In
order to take events into account we have to extend the basic logic by connec-
tives referring to a deterministic past. We are going to show that this can be
done without harm, i.e., without modifying the formal properties of the logic;
in particular, satisﬁability, validity and model checking stay in PSPACE. Our
extension is appropriate to reason about ECA-based repairs: for several deﬁni-
tions of ECA repairs we give DL-PA programs whose executability characterises
the existence of a repair (Theorems 1–4). Furthermore, several other interesting
reasoning problems can be expressed in DL-PA, such as uniqueness of a repair
or equivalence of two diﬀerent sets of constraints.
Just as most of the papers in the AIC literature we rely on grounding and
restrict our presentation to the propositional case. This simpliﬁes the presen-
tation and allows us to abstract away from orthogonal ﬁrst-order aspects. A
full-ﬂedged analysis would require a ﬁrst-order version of DL-PA, which is some-
thing that has not been studied yet.

78
G. Feuillade et al.
The paper is organised as follows. In Sect. 2 we recall AICs and their seman-
tics. We then deﬁne ECA constraints (Sect. 3) and their semantics in terms of
databases with histories (Sect. 4). In Sect. 5 we introduce a version of dynamic
logic with past and in Sect. 6 we capture well-founded ECA repairs in DL-PA.
In Sect. 7 we show that other interesting decision problems can be expressed.
Section 8 concludes. Proofs are contained in the long version.1
2
Background: AICs and Their Semantics
We suppose given a set of propositional variables P with typical elements p, q, . . .
Just as most of the papers in the AIC literature we suppose that P is ﬁnite. A
literal is an element of P or a negation thereof. A database, alias a valuation, is
a set of propositional variables D ⊆P.
An assignment is of the form either +p or –p, for p ∈P. The former sets p to
true and the latter sets p to false. We use α, α′,. . . for assignments. For every α,
the assignment ¯α is the opposite assignment, formally deﬁned by +p = –p and
–p = +p. The set of all assignments is
A = {+p : p ∈P} ∪{–p : p ∈P}.
An update action is some subset of A. For every update action A ⊆A we
deﬁne the set of variables it makes true and the set of variables it makes false:
A+ = {p : +p ∈A},
A−= {p : –p ∈A}.
An update action A is consistent if A+ ∩A−= ∅. It is relevant w.r.t. a database
D if for every p ∈P, if +p ∈A then p /∈D and if –p ∈A then p ∈D. Hence
relevance implies consistency. The update of a database D by an update action
A is a partial function ◦that is deﬁned if and only if A is consistent, and if so
then
D ◦A = (D \ A−) ∪A+.
Proposition 1. Let A = {α1, . . . , αn} be a consistent update action. Then D ◦
A = D ◦{α1} ◦· · · ◦{αn}, for every ordering of the αi.
An active integrity constraint (AIC) is a couple r = ⟨C(r), A(r)⟩where
C(r) is a conjunction of literals and A(r) ⊆A is an update action such that
if +p ∈A(r) then ¬p ∈C(r) and if –p ∈A(r) then p ∈C(r). The condition
C(r) can be thought of as the negation of an integrity constraint. For example,
in r = ⟨Bachelor ∧Married, –Bachelor⟩the ﬁrst element is the negation of the
integrity constraint ¬Bachelor∨¬Married and the second element indicates the
way its violation should be repaired, namely by deleting Bachelor.
1 https://www.irit.fr/∼Andreas.Herzig/P/Foiks22Long.pdf.

Database Repair via Event-Condition-Action Rules in Dynamic Logic
79
In the rest of the section we recall several deﬁnitions of repairs via a given
set of AICs R. The formula
OK(R) =

r∈R
¬C(r)
expresses that none of the AICs in R is applicable: all the static constraints hold.
When OK(R) is false then the database has to be repaired.
The ﬁrst two semantics do not make any use of the active part of AICs. First,
an update action A is a weak repair of a database D via a set of AICs R if it
is relevant w.r.t. D and D ◦A |= OK(R). The latter means that A makes the
static constraints true. Second, A is a minimal repair of D via R if it is a weak
repair that is set inclusion minimal: there is no weak repair A′ of D via R such
that A′ ⊂A.2 The remaining semantics all require that each of the assignments
of an update action is supported by an AIC in some way.
First, an update action A is a founded weak repair of a database D via a set
of AICs R if it is a weak repair of D via R and for every α ∈A there is an r ∈R
such that
– α ∈A(r),
– D ◦(A \ {α}) |= C(r).
Second, A is a founded repair if it is both a founded weak repair and a minimal
repair [10].3
A weak repair A of D via R is well-founded [7] if there is a sequence
⟨α1, . . . , αn⟩such that A = {α1, . . . , αn} and for every αi, 1 ≤i ≤n, there
is an ri ∈R such that
– αi ∈A(ri),
– D ◦{α1, . . . , αi−1} |= C(ri).
A well-founded repair is a well-founded weak repair that is also a minimal repair.4
For example, for D = ∅and R = {⟨¬p ∧¬q, {+q}⟩} any update action
A ⊆{+p
:
p ∈P} containing +p or +q is a weak repair of D via R; the
minimal repairs of D via R are {+p} and {+q}; and the only founded and
well-founded repair of D via R is {+q}. An example where founded repairs and
well-founded repairs behave diﬀerently is D = ∅and R = {r1, r2, r3} with
r1 = ⟨¬p ∧¬q, ∅⟩,
r2 = ⟨¬p ∧q, {+p}⟩,
r3 = ⟨p ∧¬q, {+q}⟩.
2 Most papers in the AIC literature call these simply repairs, but we prefer our denom-
ination because it avoids ambiguities.
3 Note that a founded repair is diﬀerent from a minimal founded weak repair: whereas
the latter is guaranteed to exist in case a founded weak repair does, the former is not;
in other words, it may happen that founded weak repairs exist but founded repairs
do not (because none of the founded weak repairs happens to also be a minimal
repair in the traditional sense).
4 Again, the existence of a well-founded weak repair does not guarantee the existence
of a well-founded repair.

80
G. Feuillade et al.
Then A = {+p, +q} is a founded repair: the third AIC supports +q because
D ◦{+p} |= C(r3); and the second AIC supports +p because D ◦{+q} |= C(r2).
However, there is no well-founded weak repair because the only applicable rule
from D (viz. the ﬁrst one) has an empty active part.
Two other deﬁnitions of repairs are prominent in the literature. Grounded
repairs generalise founded repairs [7]. As each of them is both a founded and
well-founded repair, we skip this generalisation. We also do not present justiﬁed
repairs [12]: their deﬁnition is the most complex one and it is not obvious how
to transfer it to ECA constraints. Moreover, as argued in [7] they do not provide
the intuitive repairs at least in some cases.
3
ECA Constraints
We consider two kinds of boolean formulas. The ﬁrst, static language LP is built
from the variables in P and describes the condition part of ECAs. The second,
dynamic language LA is built from the set of assignments A and describes the
event part of ECAs, i.e., the last update action that took place. The grammars
for the two languages are therefore
LP : ϕ::= p | ¬ϕ | ϕ ∨ϕ,
LA : ϕ::= α | ¬ϕ | ϕ ∨ϕ,
where p ranges over P and α over the set of all assignments A. We call the
elements of A history atoms.
An event-condition-action (ECA) constraint combines an event description
in LA, a condition description in LP, and an update action in 2A: it is a triple
κ = ⟨E(κ), C(κ), A(κ)⟩
where E(κ) ∈LA, C(κ) ∈LP, and A(κ) ⊆A. The event part of an ECA
constraint describes the last update action. It does so only partially: only events
that are relevant for the triggering of the rule are described.5
Sets of ECA constraints are noted K. We take over from AICs the formula
OK(K) expressing that none of the ECA constraints is applicable:
OK(K) =

κ∈K
¬C(κ).
Example 2. [20, Example 4.6] Every manager of a project carried out by a
department must be an employee of that department; if employee e just became
the manager of project p or if the project was just assigned to department d1
then the constraint should be repaired by making e a member of d1. Moreover,
if e has just been removed from d1 then the project should either be removed
from d1, too, or should get a new manager. Together with the ECA version of
5 We observe that in the literature, C(κ) is usually restricted to conjunctions of literals.
Our account however does not require this.

Database Repair via Event-Condition-Action Rules in Dynamic Logic
81
the functionality constraint on the em relation of Example 1 we obtain the set
of ECA constraints
Kmg =

⟨+eme,d1, eme,d1 ∧eme,d2, {–eme,d2}⟩,
⟨+eme,d2, eme,d1 ∧eme,d2, {–eme,d1}⟩,
⟨+mge,p ∨+prp,d1, mge,p ∧prp,d1 ∧¬eme,d1, {+eme,d1}⟩,
⟨–eme,d1, mge,p ∧prp,d1 ∧¬eme,d1, {–mge,p, –prp,d1}⟩

.
Example 3. Suppose some IoT device functions (fun) if and only if the battery is
loaded (bat) and the wiring is in order (wire); this is captured by the equivalence
fun ↔(bat ∧wire), or, equivalently, by the three implications fun →bat,
fun →wire, and (bat ∧wire) →fun. If we assume a ‘priority to the input’
repair strategy then the set of ECA constraints has to be
Kiot =

⟨+fun, fun ∧¬bat, {+bat}⟩,
⟨–bat, fun ∧¬bat, {–fun}⟩,
⟨+fun, fun ∧¬wire, {+wire}⟩,
⟨–wire, fun ∧¬wire, {–fun}⟩,
⟨+bat, bat ∧wire ∧¬fun, {+fun}⟩,
⟨+wire, bat ∧wire ∧¬fun, {+fun}⟩,
⟨–fun, bat ∧wire ∧¬fun, {–bat, –wire}⟩

.
Observe that when the last update action made bat and wire simultaneously
true then Kiot allows to go either way. We can exclude this and force the repair
to fail in that case by reﬁning the last three ECA constraints to
⟨+bat ∧+wire ∧–fun, bat ∧wire ∧¬fun, ∅⟩,
⟨+bat ∧¬–fun, bat ∧wire ∧¬fun, {+fun}⟩,
⟨+wire ∧¬–fun, bat ∧wire ∧¬fun, {+fun}⟩,
⟨+bat ∧¬+wire ∧–fun, bat ∧wire ∧¬fun, {–wire}⟩,
⟨+wire ∧¬+bat ∧–fun, bat ∧wire ∧¬fun, {–bat}⟩.
4
Semantics for ECA Constraints
As ECA constraints refer to the past, their semantics requires more than just
valuations, alias databases: we have to add the immediately preceding update
action that caused the present database state. Based on such models, we exam-
ine several deﬁnitions of AIC-based repairs in view of extending them to ECA
constraints.
4.1
Databases with History
A database with history, or h-database for short, is a couple Δ = ⟨D, H⟩made
up of a valuation D ⊆P and an update action H ⊆A such that H+ ⊆D and

82
G. Feuillade et al.
H−∩D = ∅. The intuition is that H is the most recent update action that
brought the database into the state D. This explains the two above conditions:
they guarantee that if +p is among the last assignments in H then p ∈D and
if –p is among the last assignments in H then p /∈D. Note that thanks to this
constraint H is consistent: H+ and H−are necessarily disjoint.
The update of a history H by an event A is deﬁned as
H ◦A = A ∪{α ∈H : A ∪{α} is consistent}.
For example, {+p, +q} ◦{–p} = {–p, +q}. The history H ◦A is consistent if and
only if A and H are both consistent.
Let M be the class of all couples Δ = ⟨D, H⟩with D ⊆P and H ⊆A such
that H+ ⊆D and H−∩D = ∅. We interpret the formulas of LP in D and those of
LA in H, in the standard way. For example, consider the h-database ⟨D, H⟩with
D = {eme,d1, eme,d2} and H = {+eme,d1}. Then we have D |= eme,d1 ∧¬eme,d3
and H |= +eme,d1 ∧¬+eme,d2.
Now that we can interpret the event part and the condition part of an ECA
constraint, it remains to interpret the action part. This is more delicate and
amounts to designing the semantics of repairs based on ECA constraints. In
the rest of the section we examine several possible deﬁnitions. For a start, we
take over the two most basic deﬁnitions of repairs from AICs: weak repairs and
minimal repairs of an h-database ⟨D, H⟩via a set of ECA constraints K.
– An update action A is a weak repair of ⟨D, H⟩via K if A is relevant w.r.t. D
and D ◦A |= OK(K);
– An update action A is a minimal repair of ⟨D, H⟩via K if A is a weak repair
that is set inclusion minimal: there is no weak repair A′ of ⟨D, H⟩via K such
that A′ ⊂A.
4.2
Founded and Well-Founded ECA Repairs
A straightforward adaption of founded repairs to ECA constraints goes as fol-
lows. Suppose given a candidate repair A ⊆A. For an ECA constraint κ to
support an assignment α ∈A given an h-database ⟨D, H⟩, the constraint E(κ)
about the immediately preceding event should be satisﬁed by H together with
the rest of changes imposed by A, i.e., we should also have H◦(A\{α}) |= E(κ).6
This leads to the following deﬁnition: a weak repair A is a founded weak ECA
repair of ⟨D, H⟩via a set of ECAs K if for every α ∈A there is a κ ∈K such
that
– α ∈A(κ),
– D ◦(A \ {α}) |= C(κ),
– H ◦(A \ {α}) |= E(κ).
6 A naive adaption would only require H |= E(κ). However, the support for α would
be much weaker; see also the example below.

Database Repair via Event-Condition-Action Rules in Dynamic Logic
83
Once again, a founded ECA repair is a founded weak ECA repair that is also a
minimal repair.
Moving on to well-founded repairs, an appropriate deﬁnition of ECA-based
repairs should not only check the condition part of constraints in a sequential
way, but should also do so for their triggering event part. Thus we get the
following deﬁnition: a weak repair A of ⟨D, H⟩via the set of ECA constraints
K is a well-founded weak ECA repair
if there is a sequence of assignments
⟨α1, . . . , αn⟩such that A = {α1, . . . , αn} and such that for every αi, 1 ≤i ≤n,
there is a κi ∈K such that
– αi ∈A(κi),
– D ◦{α1, . . . , αi−1} |= C(κi),7
– H ◦{α1} ◦· · · ◦{αi−1} |= E(κi).
As always, a well-founded ECA repair is deﬁned as a well-founded weak ECA
repair that is also a minimal repair.
Example 4 (Example 2, ctd.). Consider the ECA constraints Kmg of Example 2
and the h-database ⟨D, H⟩with D = {mge,p, prp,d1, eme,d2} and H = {+mge,p},
that is, e just became manager of project p. There is only one intended repair:
A = {+eme,d1, –eme,d2}. Based on the above deﬁnitions it is easy to check that
A is a founded ECA repair (both assignments in A are suﬃciently supported by
D and H) as well as a well-founded ECA repair of ⟨D, H⟩via Kmg.
Remark 1. A well-founded weak ECA repair of ⟨D, H⟩via K is also a weak
repair. It follows that D ◦A |= OK(K), i.e., the repaired database satisﬁes the
static integrity constraints.
In the rest of the paper we undertake a formal analysis of ECA repairs in
a version of dynamic logic. The iteration operator of the latter allows us to get
close to the procedural semantics while discarding inﬁnite runs. We follow the
arguments in [7] against founded and justiﬁed AICs and focus on well-founded
ECA repairs.
5
Dynamic Logic of Propositional Assignments
with Deterministic Past
In the models of Sect. 4 we can actually interpret a richer language that is
made up of boolean formulas built from P ∪A. We further add to that hybrid
language modal operators indexed by programs. The resulting logic DL-PA±
extends Dynamic Logic of Propositional Assignments DL-PA [2,3].
7 This is equivalent to D ◦{α1} ◦· · · ◦{αi−1} |= C(κi) because A is consistent (cf.
Proposition 1).

84
G. Feuillade et al.
5.1
Language of DL-PA±
The hybrid language LDL-PA± of DL-PA with past has two kinds of atomic
formulas: propositional variables of the form p and history atoms of the form
+p and –p. Moreover, it has two kinds of expressions: formulas, noted ϕ, and
programs, noted π. It is deﬁned by the following grammar
ϕ ::= p | +p | –p | ¬ϕ | ϕ ∨ϕ | ⟨π⟩ϕ,
π ::= A | π; π | π ∪π | π∗| ϕ?,
where p ranges over P and A over subsets of A. The formula ⟨π⟩ϕ combines a
formula ϕ and a program π and reads “π can be executed and the resulting
database satisﬁes ϕ”. Programs are built from sets of assignments by means
of the program operators of PDL: π1; π2 is sequential composition, π1 ∪π2 is
nondeterministic composition, π∗is unbounded iteration, and ϕ? is test. Note
that the expression +p is a formula while the expression {+p} is a program.
The program {+p} makes p true, while the formula +p expresses that p has just
been made true. The languages LP and LA of Sect. 3 are both fragments of the
language of DL-PA±.
For a set of propositional variables P = {p1, . . . , pn}, we abbreviate the
atomic program {–p1, . . . , –pn} by –P. The program π+ abbreviates π; π∗. For
n ≥0, we deﬁne n-ary sequential composition ;i=1,...,n πi by induction on n
;i=1,...,0 πi = ⊤?,
;i=1,...,n+1 πi = (;i=1,...,n πi); πn+1.
A particular case of such arbitrary sequences is n-times iteration of π, expressed
as πn = ;i=1,...,n π. We also deﬁne π≤n as (⊤?∪π)n. (It could as well be deﬁned as

0≤i≤n πi.) Finally and as usual in dynamic logic, the formula [π]ϕ abbreviates
¬⟨π⟩¬ϕ.
Given a program π, Pπ is the set of propositional variables occurring in π.
For example, P+p∧⟨–q⟩¬–r? = {p, q, r}.
5.2
Semantics of DL-PA±
The semantics is the same as that of ECA constraints, namely in terms of
databases with history. The interpretation of a formula is a subset of the set
of all h-databases M, and the interpretation of a program is a relation on M.
More precisely, the interpretation of formulas is
Δ |= p
if
Δ = ⟨D, H⟩and p ∈D,
Δ |= +p
if
Δ = ⟨D, H⟩and +p ∈H,
Δ |= –p
if
Δ = ⟨D, H⟩and –p ∈H,
Δ |= ⟨π⟩ϕ
if
Δ||π||Δ′ and Δ′ |= ϕ for some Δ′,

Database Repair via Event-Condition-Action Rules in Dynamic Logic
85
and as expected for the boolean connectives; the interpretation of programs is
Δ||A||Δ′
if
A is consistent, Δ = ⟨D, H⟩, and Δ′ = ⟨D ◦A, H ◦A⟩,
Δ||π1; π2||Δ′
if
Δ||π1||Δ′′ and Δ′′||π2||Δ′ for some Δ′′,
Δ||π1 ∪π2||Δ′
if
Δ||π1||Δ′ or Δ||π2||Δ′,
Δ||π∗||Δ′
if
Δ||πn||Δ′ for some n ≥0,
Δ||ϕ?||Δ′
if
Δ |= ϕ and Δ′ = Δ.
Hence the interpretation of a set of assignments A updates both the database D
and the history H by A.8
We say that a program π is executable at an h-database Δ if Δ |= ⟨π⟩⊤, i.e.,
if there is an h-database Δ′ such that Δ||π||Δ′. Clearly, consistency of A ⊆A is
the same as executability of the program A at every h-database Δ.
The deﬁnitions of validity and satisﬁability in the class of models M are
standard. For example, +p →p and –p →¬p are both valid while the other
direction is not, i.e., p ∧¬+p and ¬p ∧¬–p are both satisﬁable.
Proposition 2. The decision problems of DL-PA± satisﬁability, validity and
model checking are all PSPACE complete.
6
Well-Founded ECA Repairs in DL-PA±
We now show that well-founded weak ECA repairs and well-founded ECA repairs
can be captured in DL-PA±.
6.1
Well-Founded Weak ECA Repairs
Given a set of ECA constraints K, our translation of well-founded weak ECA
repairs uses fresh auxiliary propositional variables D(α) recording that α ∈A
has been executed during the (tentative) repair. For the set of assignments A =
{α1, . . . , αn} ⊆A, let the associated set of auxiliary propositional variables be
D(A) = {D(α) : α ∈A}. Then the program
–D(A) = {–D(α1), . . . , –D(αn), –D(α1), . . . , –D(αn)}
initialises the auxiliary variables D(αi) and D(αi) to false. Note that the proposi-
tional variable D(α) is to be distinguished from the history atom α ∈A express-
ing that α was one of the last assignments that brought the database into the
violation state.
8 An alternative to the update of H would be to erase the history and replace it
by A. Altogether, this would make us go from ⟨D, H⟩to ⟨D ◦A, A⟩. This would be
appropriate in order to model external updates such as the update {+mge,p} that had
occurred in Example 2 and brought the database into an inconsistent state, diﬀering
from the kind of updates occurring during the repair process that our deﬁnition
accounts for.

86
G. Feuillade et al.
The next step is to associate repair programs rep(α) to assignments α. These
programs check whether α is triggered by some ECA constraint κ and if so
performs it. This involves some bookkeeping by means of the auxiliary variables
D(α), for two reasons: ﬁrst, to make sure that none of the assignments α ∈A is
executed twice; second, to make sure that A is consistent w.r.t. α, in the sense
that it does not contain both α and its opposite α.
rep(α) = ¬D(α) ∧¬D(α) ∧

κ∈K : α∈A(κ)
(E(κ) ∧C(κ))?; {α, +D(α)}.
The program ﬁrst performs a test: neither α nor its opposite α has been done
up to now (this is the conjunct ¬D(α) ∧¬D(α)) and there is an ECA constraint
κ ∈K with α in the action part such that the event description E(κ) and the
condition C(κ) are both true (this is the conjunct 
κ : α∈A(κ)(E(κ) ∧C(κ))). If
that big test program succeeds then α is executed and this is stored by making
D(α) true (this is the update {α, +D(α)}).
Theorem 1. Let Δ be an h-database and K a set of ECA constraints. There
exists a well-founded weak ECA repair of Δ via K if and only if the program
repwwf
K
= –D(A);
 	
α∈A
rep(α)

∗
; OK(K)?
is executable at Δ.
The unbounded iteration (
α∈A rep(α))∗in our repair program repwwf
K
can be
replaced by (
α∈A rep(α))≤card(P), i.e., the number of iterations of 
α∈A rep(α)
can be bound by the cardinality of P. This is the case because well-founded ECA
repairs are consistent update actions: each propositional variable can occur at
most once in a well-founded ECA repair. This also holds for the other repair
programs that we are going to deﬁne below.
Observe that ﬁniteness of P is necessary for Theorem 1. This is not the case
for the next result.
Theorem 2. Let Δ be an h-database and K a set of ECA constraints. The set
of assignments A = {α1, . . . , αn} is a well-founded weak ECA repair of Δ via K
if and only if the program
repwwf
K (A) = –D(A);
 	
α∈A
rep(α)

∗
; OK(K)?;

α∈A
D(α)?
is executable at Δ.
The length of both repair programs is polynomial in the size of the set of
ECA constraints and the cardinality of P.

Database Repair via Event-Condition-Action Rules in Dynamic Logic
87
6.2
Well-Founded ECA Repairs
In order to capture well-founded ECA repairs we have to integrate a minimality
check into its DL-PA± account. We take inspiration from the translation of
Winslett’s Possible Models Approach for database updates [25,26] into DL-PA
of [21]. The translation associates to each propositional variable p a fresh copy
p′ whose role is to store the truth value of p. This is done before repair programs
rep(+p) or rep(–p) are executed so that the initial value of p is remembered: once
a candidate repair has been computed we check that it is minimal, in the sense
that no consistent state can be obtained by modifying less variables.
For α being either +p or –p, the programs copy(α) and undo(α) respectively
copy the truth value of p into p′ and, the other way round, copy back the value
of p′ into p. They are deﬁned as follows
copy(α) = (p?; +p′) ∪(¬p?; –p′),
undo(α) = (p′?; +p) ∪(¬p′?; –p).
For example, the formulas p →⟨copy(+p)⟩(p ∧p′) and ¬p →⟨copy(+p)⟩(¬p ∧
¬p′) are both valid, as well as p′
→
⟨undo(+p)⟩(p ∧p′) and ¬p′
→
⟨undo(+p)⟩(¬p ∧¬p′). Then the program
init({α1, . . . , αn}) = {–D(α1), . . . , –D(αn),
–D(α1), . . . , –D(αn)}; copy(α1); · · · ; copy(αn)
initialises the values of both kinds of auxiliary variables: it resets all ‘done’
variables D(αi) and D(αi) to false as before and moreover makes copies of all
variables that are going to be assigned by αi (where the order of the αi does not
matter).
Theorem 3. Let Δ be an h-database and K a set of ECA constraints. There
exists a well-founded ECA repair of Δ via K if and only if the program
repwf
K = init(A);
 	
α∈A
rep(α)

∗
; OK(K)?;
¬
 	
α∈A
D(α)?; undo(α)

+
OK(K)?
is executable at Δ.
Theorem 4. Let Δ be an h-database and K a set of ECA constraints. The set
of assignments A = {α1, . . . , αn} is a well-founded ECA repair of Δ via K if
and only if the program
repwf
K (A) = init(A);
 	
α∈A
rep(α)

∗
; OK(K)?;
 
α∈A
D(α)

?;
¬
 	
α∈A
undo(α)

+
OK(K)?
is executable at Δ.

88
G. Feuillade et al.
The length of both repair programs is polynomial in the size of the set of
ECA constraints and the cardinality of P.
7
Other Decision Problems
In the present section we discuss how several other interesting decision problems
related to well-founded weak ECA repairs and well-founded ECA repairs can be
expressed. Let repK stand for either repwwf
K
or repwf
K , i.e., the program performing
well-founded (weak) ECA repairs according to the set of ECA constraints K.
7.1
Properties of a Set of ECA Constraints
Here are three other decision problems about a given set of ECA constraints:
1. Is there a unique repair of Δ via K?
2. Does every Δ have a unique repair via K?
3. Does every Δ have a repair via K?
Each of them can be expressed in DL-PA±. First, we can verify whether
there is a unique repair of Δ via K by checking whether the program repK is
deterministic. This can be done by model checking, for each of the variables
p ∈P occurring in K, whether Δ |= ⟨repK⟩p →[repK]p. Second, global unicity of
the repairs (independently of a speciﬁc database Δ) can be veriﬁed by checking
for each of the variables p ∈P whether ⟨repK⟩p →[repK]p is DL-PA± valid.
Third, we can verify whether K can repair every database by checking whether
the formula ⟨repK⟩⊤is DL-PA± valid.
7.2
Comparing Two Sets of ECA Constraints
We start by two deﬁnitions that will be useful for our purposes. The program
π1 is included in the program π2 if ||π1|| ⊆||π2||. Two programs π1 and π2 are
equivalent if each is included in the other, that is, if ||π1|| = ||π2||.
These comparisons can be polynomially reduced into validity checking prob-
lems. Our translation makes use of the assignment-recording propositional vari-
ables of Sect. 6.1 and of the copies of propositional variables that we have intro-
duced in Sect. 6.2. Hence we suppose that for every variable p there is a fresh
variable p′ that will store the truth value of p, as well as two fresh variables
D(+p′) and D(–p′). For a given set of propositional variables P ⊆P we make
use of the following three sets of auxiliary variables
P ′ = {p′ : p ∈P},
D(+(P ′)) = {D(+p′) : p ∈P},
D(–(P ′)) = {D(–p′) : p ∈P}.

Database Repair via Event-Condition-Action Rules in Dynamic Logic
89
The auxiliary variables are used in a ‘generate and test’ schema. For a given set
of propositional variables P ⊆P, the program
guess(P) = –(P ′ ∪D(+(P ′)) ∪D(–(P ′))); (
	
q∈P ′
+q)∗
guesses nondeterministically which of the auxiliary variables for P are going to
be modiﬁed: ﬁrst all are set to false and then some subset is made true. The
formula
Guessed(P) =

p∈P

(p ↔p′) ∧(+p ↔D(+p)) ∧(–p ↔D(–p))

checks that the guess was correct.
Now we are ready to express inclusion of programs in DL-PA±: we predict
the outcome of π1 and then check if π2 produces the same set of changes as π1.
Proposition 3. Let π1 and π2 be two DL-PA programs.
1. π1 is included in π2 if and only if
[guess(Pπ1 ∪Pπ2)]

⟨π1⟩Guessed(Pπ1 ∪Pπ2)
→⟨π2⟩Guessed(Pπ1 ∪Pπ2)

is DL-PA± valid.
2. π1 and π2 are equivalent if and only if
[guess(Pπ1 ∪Pπ2)]

⟨π1⟩Guessed(Pπ1 ∪Pπ2)
↔⟨π2⟩Guessed(Pπ1 ∪Pπ2)

is DL-PA± valid.
The reduction is polynomial. The following decision problems can be reformu-
lated in terms of program inclusion and equivalence:
1. Is the ECA constraint κ ∈K redundant?
2. Are two sets of ECA constraints K1 and K2 equivalent?
3. Can all databases that are repaired by K1 also be repaired by K2?
Each can be expressed in DL-PA± by means of program inclusion or equivalence:
the ﬁrst can be decided by checking whether the programs repK and repK\{κ}
are equivalent; the second can be decided by checking whether the programs
repK1 and repK2 are equivalent; the third problem can be decided by checking
the validity of ⟨repK1⟩⊤→⟨repK2⟩⊤.

90
G. Feuillade et al.
7.3
Termination
In our DL-PA± framework, the taming of termination problems is not only due to
the deﬁnition of well-founded ECA repairs itself: in dynamic logics, the Kleene
star is about unbounded but ﬁnite iterations. The modal diamond operator
therefore quantiﬁes over terminating executions only, disregarding any inﬁnite
executions. We can nevertheless reason about inﬁnite computations by drop-
ping the tests ¬D(α) ∧¬D(α) from the deﬁnition of rep(α). Let the resulting
‘unbounded’ repair program be urep(α). Let urepK stand for either the programs
urepwwf
K
or urepwf
K resulting from the replacement of rep(α) by urep(α). Then the
repair of Δ loops if and only if Δ |= [(urepK)∗]⟨urepK⟩⊤.
8
Conclusion
Our dynamic logic semantics for ECA constraints generalises the well-founded
AIC repairs of [7], and several decision problems can be captured in DL-PA±.
Proposition 2 provides a PSPACE upper bound for all these problems. A closer
look at the characterisations of Sect. 6 shows that the complexity is actually
lower. For the results of Sect. 6.1 (Theorem 1 and Theorem 2), as executability
of a program π at Δ is the same as truth of ⟨π⟩⊤at Δ, our characterisation
involves a single existential quantiﬁcation (a modal diamond operator), with
a number of nondeterministic choices that is quadratic in card(P) (precisely,
1 + 2card(P) nondeterministic choices that are iterated card(P) times, cf. what
we have remarked after the theorem, as well as the deﬁnition of π≤n in Sect. 5).
Just as the corresponding QBF fragment, this fragment is in NP. For the results
of Sect. 6.2 (Theorem 3 and Theorem 4), as executability of π; ¬⟨π′⟩ϕ? at Δ is
the same as truth of ⟨π⟩[π′]ϕ at Δ, our characterisation involves an existential
diamond containing the same program as above that is preceded by init(A) and
that is followed by a universal quantiﬁcation (a modal box operator). Just as
the corresponding QBF fragment, this fragment is in Σp
2.
Beyond these decision problems we can express repair algorithms as DL-PA±
programs, given that the standard programming constructions such as if-then-
else and while can all be expressed in dynamic logic. Correctness of such a
program π can be veriﬁed by checking whether π is included in the program
repK. The other way round, one can check whether π is able to output any
well-founded ECA repair by checking whether repK is included in π.
It remains to study further our founded ECA repairs of Sect. 4.2 and their
grounded versions. We also plan to check whether the more expressive existential
AICs of [9] transfer. Finally, we would like to generalise the history component
of h-databases from update actions to event algebra expressions as studied e.g.
in [1,22]; dynamic logic should be beneﬁcial here, too.

Database Repair via Event-Condition-Action Rules in Dynamic Logic
91
References
1. Alferes, J.J., Banti, F., Brogi, A.: An event-condition-action logic programming
language. In: Fisher, M., van der Hoek, W., Konev, B., Lisitsa, A. (eds.) JELIA
2006. LNCS (LNAI), vol. 4160, pp. 29–42. Springer, Heidelberg (2006). https://
doi.org/10.1007/11853886 5
2. Balbiani, P., Herzig, A., Schwarzentruber, F., Troquard, N.: DL-PA and DCL-
PC: model checking and satisﬁability problem are indeed in PSPACE. CoRR
abs/1411.7825 (2014). http://arxiv.org/abs/1411.7825
3. Balbiani, P., Herzig, A., Troquard, N.: Dynamic logic of propositional assignments:
a well-behaved variant of PDL. In: 28th Annual ACM/IEEE Symposium on Logic
in Computer Science, LICS 2013, New Orleans, LA, USA, 25–28 June 2013, pp.
143–152. IEEE Computer Society (2013). https://doi.org/10.1109/LICS.2013.20
4. Bertossi, L.E.: Database Repairing and Consistent Query Answering. Synthesis
Lectures on Data Management. Morgan & Claypool Publishers (2011). https://
doi.org/10.2200/S00379ED1V01Y201108DTM020
5. Bertossi, L.E., Pinto, J.: Specifying active rules for database maintenance. In:
Saake, G., Schwarz, K., T¨urker, C. (eds.) Transactions and Database Dynamics,
Proceedings of the Eight International Workshop on Foundations of Models and
Languages for Data and Objects, Schloß Dagstuhl, Germany, 27–30 September
1999, vol. Preprint Nr. 19, pp. 65–81. Fakult¨at f¨ur Informatik, Otto-von-Guericke-
Universit¨at Magdeburg (1999)
6. Bidoit, N., Maabout, S.: A model theoretic approach to update rule programs. In:
Afrati, F., Kolaitis, P. (eds.) ICDT 1997. LNCS, vol. 1186, pp. 173–187. Springer,
Heidelberg (1997). https://doi.org/10.1007/3-540-62222-5 44
7. Bogaerts, B., Cruz-Filipe, L.: Fixpoint semantics for active integrity constraints.
Artif. Intell. 255, 43–70 (2018)
8. Calautti, M., Caroprese, L., Greco, S., Molinaro, C., Trubitsyna, I., Zumpano, E.:
Consistent query answering with prioritized active integrity constraints. In: Desai,
B.C., Cho, W. (eds.) IDEAS 2020: 24th International Database Engineering and
Applications Symposium, Seoul, Republic of Korea, 12–14 August 2020, pp. 3:1–
3:10. ACM (2020). https://dl.acm.org/doi/10.1145/3410566.3410592
9. Calautti, M., Caroprese, L., Greco, S., Molinaro, C., Trubitsyna, I., Zumpano, E.:
Existential active integrity constraints. Expert Syst. Appl. 168, 114297 (2021)
10. Caroprese, L., Greco, S., Sirangelo, C., Zumpano, E.: Declarative semantics of
production rules for integrity maintenance. In: Etalle, S., Truszczy´nski, M. (eds.)
ICLP 2006. LNCS, vol. 4079, pp. 26–40. Springer, Heidelberg (2006). https://doi.
org/10.1007/11799573 5
11. Caroprese, L., Greco, S., Zumpano, E.: Active integrity constraints for database
consistency maintenance. IEEE Trans. Knowl. Data Eng. 21(7), 1042–1058 (2009).
https://doi.org/10.1109/TKDE.2008.226
12. Caroprese, L., Truszczynski, M.: Active integrity constraints and revision program-
ming. TPLP 11(6), 905–952 (2011)
13. Ceri, S., Fraternali, P., Paraboschi, S., Tanca, L.: Automatic generation of produc-
tion rules for integrity maintenance. ACM Trans. Database Syst. 19(3), 367–422
(1994). http://doi.acm.org/10.1145/185827.185828
14. Chomicki, J., Lobo, J., Naqvi, S.A.: Conﬂict resolution using logic programming.
IEEE Trans. Knowl. Data Eng. 15(1), 244–249 (2003)
15. Chomicki, J., Marcinkowski, J.: Minimal-change integrity maintenance using tuple
deletions. Inf. Comput. 197(1–2), 90–121 (2005). https://doi.org/10.1016/j.ic.
2004.04.007

92
G. Feuillade et al.
16. Cruz-Filipe, L.: Optimizing computation of repairs from active integrity con-
straints. In: Beierle, C., Meghini, C. (eds.) FoIKS 2014. LNCS, vol. 8367, pp.
361–380. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-04939-7 18
17. Cruz-Filipe, L., Gaspar, G., Engr´acia, P., Nunes, I.: Computing repairs from active
integrity constraints. In: Seventh International Symposium on Theoretical Aspects
of Software Engineering, TASE 2013, 1–3 July 2013, Birmingham, UK, pp. 183–
190. IEEE Computer Society (2013). https://doi.org/10.1109/TASE.2013.32
18. Cruz-Filipe, L., Gaspar, G., Nunes, I., Schneider-Kamp, P.: Active integrity con-
straints for general-purpose knowledge bases. Ann. Math. Artif. Intell. 83(3–4),
213–246 (2018)
19. Flesca, S., Greco, S.: Declarative semantics for active rules. Theory Pract. Log. Pro-
gram. 1(1), 43–69 (2001). http://journals.cambridge.org/action/displayAbstract?
aid=71136
20. Flesca, S., Greco, S., Zumpano, E.: Active integrity constraints. In: Moggi, E., War-
ren, D.S. (eds.) Proceedings of the 6th International ACM SIGPLAN Conference on
Principles and Practice of Declarative Programming, 24–26 August 2004, Verona,
Italy, pp. 98–107. ACM (2004). http://doi.acm.org/10.1145/1013963.1013977
21. Herzig, A.: Belief change operations: a short history of nearly everything, told in
dynamic logic of propositional assignments. In: Baral, C., Giacomo, G.D., Eiter,
T. (eds.) Principles of Knowledge Representation and Reasoning: Proceedings of
the Fourteenth International Conference, KR 2014, Vienna, Austria, 20–24 July
2014. AAAI Press (2014). http://www.aaai.org/ocs/index.php/KR/KR14/paper/
view/7960
22. Lausen, G., Lud¨ascher, B., May, W.: On logical foundations of active databases.
In: Chomicki, J., Saake, G. (eds.) Logics for Databases and Information Systems
(the book grow out of the Dagstuhl Seminar 9529: Role of Logics in Information
Systems, 1995), pp. 389–422. Kluwer (1998)
23. Lud¨ascher, B., May, W., Lausen, G.: Nested transactions in a logical language for
active rules. In: Pedreschi, D., Zaniolo, C. (eds.) LID 1996. LNCS, vol. 1154, pp.
197–222. Springer, Heidelberg (1996). https://doi.org/10.1007/BFb0031742
24. Widom, J., Ceri, S.: Active Database Systems: Triggers and Rules for Advanced
Database Processing. Morgan Kaufmann, Burlington (1996)
25. Winslett, M.: Reasoning about action using a possible models approach. In: Shrobe,
H.E., Mitchell, T.M., Smith, R.G. (eds.) Proceedings of the 7th National Confer-
ence on Artiﬁcial Intelligence, St. Paul, MN, USA, 21–26 August 1988, pp. 89–93.
AAAI Press/The MIT Press (1988). http://www.aaai.org/Library/AAAI/1988/
aaai88-016.php
26. Winslett, M.A.: Updating Logical Databases. Cambridge Tracts in Theoretical
Computer Science. Cambridge University Press, Cambridge (1990)

Statistics of RDF Store for Querying Knowledge
Graphs
Iztok Savnik1(B), Kiyoshi Nitta2, Riste Skrekovski3, and Nikolaus Augsten4
1 Faculty of Mathematics, Natural Sciences and Information Technologies,
University of Primorska, Koper, Slovenia
iztok.savnik@upr.si
2 Yahoo Japan Corporation, Tokyo, Japan
knitta@yahoo-corp.jp
3 Faculty of Information Studies, Novo Mesto, Slovenia
4 Department of Computer Sciences, University of Salzburg, Salzburg, Austria
nikolaus.augsten@sbg.ac.at
Abstract. Many RDF stores treat graphs as simple sets of vertices and edges
without a conceptual schema. The statistics in the schema-less RDF stores are
based on the cardinality of the keys representing the constants in triple patterns.
In this paper, we explore the effects of storing knowledge graphs in an RDF store
on the structure of the space of queries and, consequently, on the deﬁnition of the
framework for the computation of the statistics. We propose a formal framework
for an RDF store with a complete conceptual schema. The poset of schema triples
deﬁnes the structure of the types of triple patterns and, therefore, the structure of
the query space. The set of schema triples, together with the ontology of classes
and predicates, form the conceptual schema of a knowledge graph, referred to as
a schema graph. We present an algorithm for computing the statistics of a schema
graph that consists of the schema triples from the stored schema graph and the
schema triples that are more general/speciﬁc than the stored schema triples up to
a user-deﬁned level.
Keywords: RDF stores · graph databases · knowledge graphs · database
statistics · statistics of graph databases
1
Introduction
The statistics of RDF stores is an essential tool used in the processes of query opti-
mization [7,19,20,31]. They are used to estimate the size of a query result and the time
needed to evaluate a given query. The estimations are required to ﬁnd the most efﬁcient
query evaluation plan for a given input query. Furthermore, the statistics of large graphs
are used to solve problems closely related to query evaluation; for example, they can be
employed in algorithms for partitioning of large graphs [27].
Many RDF stores treat graphs as simple sets of vertices and edges without a concep-
tual schema. The statistics in the schema-less RDF stores are based on the cardinality
of the keys representing the constants in triple patterns. While some of the initial RDF
stores are implemented on top of relational database systems [21,34], most of them are
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 93–110, 2022.
https://doi.org/10.1007/978-3-031-11321-5_6

94
I. Savnik et al.
implemented around a subset of seven indexes corresponding to the keys that are sub-
sets of {S, P, O}, i.e., the subject, predicate, and object of the triples. The indexes can
be either a B+ tree [8,20], a custom-designed index [11,13,33,35], or some other index
data structure (e.g., radix trie) [10,12,32]. Statistics at the level of relations (as used in
relational systems) are not useful at the level of RDF graphs; therefore, statistics are
obtained by sampling the indexes [21], or by creating separate aggregates of indexes
[8,20].
There are a few approaches to the computation of statistics of RDF stores that
use some form of the semantic structure to which the statistics are attached. Stocker
et al. use RDF Schema information to precompute the size of all possible joins where
domain/range of some predicate is joined with domain/range component of some other
predicate [31]. Neumann and Moetkotte propose the use of the characteristic sets [19],
i.e., the sets of predicates with a common S/O component, for the computation of statis-
tics of star-shaped queries. Gubichev and Neumann further organize characteristic sets
[7] in a hierarchy to allow precise estimation of joins in star queries.
In this paper, we explore the effects of storing knowledge graphs [15] in an RDF
store on the structure of the space of queries and, consequently, on the deﬁnition of the
framework for the computation of the statistics. The space of SPARQL queries is struc-
tured primarily on the basis of the triple patterns. To capture the set of triples addressed
by a triple pattern, we deﬁne a type of a triple pattern to be a triple referred to as a
schema triple. A schema triple (s, p, o) includes a class s as subject, a predicate p and a
class o as object. For example, the type of the triple (john, livesIn, capodistria) is the
schema triple (person, livesIn, location) where the domain and the range of predicate
livesIn are person and location. The set of schema triples is partially ordered by the
relationship more-speciﬁc reﬂecting sub-class and sub-predicate relationships among
classes and predicates.
The space of the sets of triples that are the targets of triple patterns is partially
ordered in the same way as are the schema triples that are the types of triple patterns.
Namely, the set of triples targeted by a triple pattern depends on the interpretation of
the type of a triple pattern. Since the schema triples (as well as their interpretations)
are partially ordered, we have the framework to which the statistics of the types of
triple patterns are attached. The set of possible types of triple-patters together with the
ontology of classes and predicates comprise the conceptual schema of a knowledge
graph that we call a schema graph.
The selection of the schema triples that form the schema graph is the ﬁrst step
in the computation of the statistics. The stored schema graph is the minimal schema
graph, including only the schema triples that are stored in an RDF store by means of the
deﬁnitions of domains and ranges of predicates. The complete schema graph includes
all legal types (schema triples) of the triples from an RDF store. We propose to use the
schema graph that includes the schema triples from a strip around the stored schema
graph, i.e., the schema triples from the stored schema graph and some adjacent levels
of schema triples. In the second step, we compute the statistics for the selected schema
triples that form the schema graph. For each RDF triple, the set of its types (schema
triples) that intersects with the schema graph is computed. The statistics are updated for
the computed types.

Statistics of RDF Store for Querying Knowledge Graphs
95
The contributions of the work presented in this paper are the following. First, we
propose a formal framework for an RDF store with a complete conceptual schema. The
poset of schema triples deﬁnes the structure of the types of triple patterns and, therefore,
the structure of the query space and the framework for the computation of the statistics.
The proposed formalization uniﬁes the representation of the instance and the schema
levels of RDF stores. Second, we propose the computation of the statistics of the schema
triples from a strip around the stored schema graph. We show that the assignment of
types to joining triple patterns can lead to more speciﬁc types than those speciﬁed in a
stored schema. For this reason, we often need the statistics for a schema triple that are
more speciﬁc/general than some stored schema triple. Finally, we propose two ways
of counting the instances and keys of a given schema triple. When adding individual
triples to the statistics, one schema triple represents the seven types of keys. The bound
type of counting a key respects underlying schema triple (the complete type of the key),
and the unbound type of counting is free from the underlying schema triple.
The paper is organized as follows. Section 2 provides a formalization of a knowl-
edge graph and introduces the denotational semantics of concepts. Section 3 presents
three novel algorithms for the computation of statistics. The ﬁrst algorithm computes
the statistics for the stored schema graph, and the second algorithm computes the statis-
tics of all possible schema triples. The third algorithm focuses on the schema triples
from a strip around the stored schema graph, i.e., in addition to the stored schema
graph, also the schema triples some levels below and above the schema triples in the
stored schema graph. Section 3.4 introduces the concepts of a key and a key type that
are used for counting the instances of a schema triple. Further, the concepts of bound
and unbound counting are discussed. An empirical study of the algorithms for the com-
putation of the statistics is presented in Sect. 4. Two experiments are presented, ﬁrst, on
a simple toy domain and, second, on the core of the Yago2 knowledge graph. Related
work is presented in Sect. 5. Finally, concluding remarks are given in Sect. 6.
2
Conceptual Schema of a Knowledge Graph
The formal deﬁnition of the schema of a knowledge graph is based on the RDF [25] and
RDF-Schema [26] data models. Let I be the set of URI-s, B be the set of blanks and L
be the set of literals. Let us also deﬁne sets S = I ∪B, P = I, and O = I ∪B ∪L.
A RDF triple is a triple (s, p, o) ∈S × P × O. An RDF graph g ⊆S × P × O is
a set of triples [15]. The set of all graphs is denoted as G. We state that an RDF graph
g1 is a sub-graph of g2, denoted as g1 ⊆g2, if all triples of g1 are also triples of g2. We
call the elements of the set, I, identiﬁers to abstract away the details of the RDF data
model. An extended formal deﬁnition of the conceptual schema of a knowledge graph
is presented in [30].
2.1
Identiﬁers
We deﬁne a set of concepts to be used for a more precise characterization of identiﬁers
I. Individual identiﬁers, denoted as set Ii, are identiﬁers that have speciﬁed their classes

96
I. Savnik et al.
using property rdf:type. Class identiﬁers denoted as set Ic, are identiﬁers that are sub-
classes of the top class ⊤of ontology—Yago, for instance, uses the top class owl:Thing.
Further, predicate identiﬁers denoted as set Ip, are individual identiﬁers (i.e., IP ⊂Ii)
that represent RDF predicates. The predicate identiﬁers are, from one perspective, sim-
ilar to the class identiﬁers: they have sub-predicates in the same manner as the classes
have sub-classes. However, predicates do not have instances. They are the instances of
rdf:Property.
We deﬁne the partial ordering relationship more-speciﬁc, formally denoted as ⪯,
over the complete set of identiﬁers I. The relationship ⪯subsumes the relationships
rdf:type, rdfs:subClassOf and rdfs:subPropertyOf. The top element of partial ordering
is the top-class ⊤. The bottom of the partial ordering includes the individual identiﬁers
from Ii. The individual identiﬁers Ii are related to their classes from Ic using the rela-
tionship rdf:type. The class identiﬁers Ic are related by the relationship rdf:subClassOf,
and the predicate identiﬁers IP are related by the relationship rdf:subPropertyOf. Rela-
tionship “⪯” is reﬂexive, transitive, and anti-symmetric, i.e., it deﬁnes a partial order
relationship [30].
Let us now deﬁne the two semantic interpretations of the identiﬁers from I. The
ordinary interpretation ig ⊆Ii maps individual identiﬁers to themselves and the class
identiﬁers c to the instances of c or any of c’s sub-classes. The natural interpretation
i∗
g maps identiﬁers i to the sets of all identiﬁers that are more speciﬁc than i. The
individual identiﬁers are mapped to themselves, while the class identiﬁers c are mapped
to all instances of c and c’s sub-classes, and including all sub-classes of c.
The partial ordering relationship ⪯and the semantic functions g and ∗
g are con-
sistent. The relationship ⪯between two identiﬁers implies the subsumption of the ordi-
nary interpretations as well as natural interpretations.
2.2
Triples, Types and Graphs
The partial ordering relation ⪯is extended to triples. A triple t1 is more speciﬁc or equal
to a triple t2, denoted t1 ⪯t2, if all components of t1 are more speciﬁc or equal than
the related components of t2. Similar to the set of identiﬁers I, where we distinguish
between Ii, and Ic, the set of triples is divided into ground triples and schema triples.
Ground triples include at least one individual identiﬁer, while the schema triples include
the predicate (as a component P) and class identiﬁers solely.
The schema triples represent the types of the ground triples. In order to give a
uniform representation of the “schema” and “instance” levels of a datasets we deﬁne
schema triples as (s, p, o) where s, o ∈Ic and p ∈IP . For example, the triple
(plato,wasBornIn,athens) is a ground triple, and its type, the triple (person,wasBorn-
In,location), is a schema triple. Further, a schema triple (s, p, o) must represent a type
of a nonempty set of ground triples. In terms of the semantic interpretations that are
presented shortly: a schema triple has a nonempty interpretation in a graph g.
A special kind of schema triples are the stored schema triples (s, p, o) that are rep-
resented in a graph g by the triples (p, rdfs:domain, s) and (p, rdfs:range, o). Looking
from this perspective, a schema triple is any triple t = (s, p, o) (with s, o ∈Ic and
p ∈IP ) such that there exist a stored schema triple ts which is related to t using the
relationship more-speciﬁc, i.e., t ⪯ts or ts ⪯t. This deﬁnition ties the schema triples to

Statistics of RDF Store for Querying Knowledge Graphs
97
the partially ordered set of triples in order to represent a legal type in a given knowledge
graph.
We can now deﬁne the interpretation of a schema triple t, similarly to the case of
identiﬁers. The interpretation function g maps a schema triple t to the set of ground
triples that are the instances of a given schema triple t: tg = {(s′, p′, o′)|(s′, p′, o′) ∈
g ∧s′ ∈sg ∧p′ ∈pg ∧o′ ∈og}. The natural interpretation function ∗
g maps
a schema triple t to the set of triples that are more speciﬁc than t, or equal to t: t∗
g =
{(s′, p′, o′)|(s′, p′, o′) ∈g ∧s′ ⪯s ∧p′ ⪯p ∧o′ ⪯o}.
The set of all stored schema triples together with the triples that deﬁne the classiﬁ-
cation hierarchy of the classes and predicates form the stored schema graph (abbr. ssg).
The ssg deﬁnes the structure of ground triples in g; each ground triple is an instance of
at least one schema triple ts ∈ssg. The ssg represents the minimal schema graph. On
the other hand, the complete schema graph includes the ssg and all possible types that
are either more speciﬁc or more general than some t ∈ssg. Therefore, the complete
schema graph is a maximal schema graph of g. Finally, the schema graph that we use
for the computation of the statistics of a knowledge graph contains, besides the ssg, also
the schema triples that form a strip around the stored schema triples. These additional
schema triples allow us to make a better estimation of the sizes of queries.
2.3
Triple Patterns
We assumed that we have a set of variables V . A triple pattern (s, p, o) ∈(S ∪V ) ×
(P ∪V ) × (O ∪V ) is a triple that includes at least one variable. The components of
a triple pattern t = (s, p, o) can be accessed in, similarly to the elements in an array,
as tp[1] = s, tp[2] = p and tp[3] = o. Let a set tpv ⊆{1, 2, 3} be a set of indices of
components that are variables, i.e., ∀j ∈tpv : tp[j] ∈V .
The interpretation of a triple pattern tp in a graph g is the set of triples t ∈g such
that t includes any value in place of variables indexed by the elements of tpv, and, has
the values of other components equal to the corresponding tp components. Formally,
the interpretation of tp is tpg = {t | t ∈g ∧∀j ∈{1, 2, 3} \ tpv : t[j] = tp[j]}.
The type of a triple pattern tp = (s, p, o) is a schema triple ttp = (ts, tp, to) such
that the interpretation of the schema triple subsumes the interpretation of the triple
pattern, i.e., tpg ⊆ttp∗
g.. Note that we only specify the semantic conditions for the
deﬁnition of the type of triple patterns.
3
Computing Statistics
The statistical index is implemented as a dictionary where keys represent schema triples,
and the values represent the statistical information for the given keys. For instance, the
index entry for the schema triple (person,wasBornIn,location) represents the statistical
information about the triples that have the instance of a person as the ﬁrst component,
the property wasBornIn as the second component, and the instance of location as the
third component.
The main procedure for the computation of the statistics of a knowledge graph is
presented as Algorithm 1. The statistic is computed for each triple t from a given knowl-
edge graph. The function STATISTICS-TRIPLE(t) represents one of three functions:

98
I. Savnik et al.
Fig. 1. Triple-store Simple.
STATISTICS-STORED(t), STATISTICS-ALL(t) and STATISTICS-LEVELS(t), which are
presented in detail in the sequel. Each of the functions computes, in the ﬁrst phase, the
set of schema triples, which include the given triple, t, in their natural interpretations
∗. In the second phase, the function STATISTICS-TRIPLE updates the corresponding
statistics for the triple t.
Algorithm 1. Procedure COMPUTE-STATISTICS(ts : knowledge-graph)
1: procedure COMPUTE-STATISTICS(ts : knowlege-graph)
2:
for all t ∈ts do
3:
STATISTICS-TRIPLE(t)
The examples in the following subsections are based on a simple knowledge graph
presented in Fig. 1. The knowledge graph referred to in the rest of the text as Simple
includes 33 triples. Four user-deﬁned classes are included. The class person has sub-
classes scientist and philosopher. The class location represents the concept of a physical
location. The schema part of Simple is colored blue, and the data part is colored black.
The dataset Simple is available from the epsilon data repository [28].
Example 1. The stored schema graph of Simple is represented by the schema triples
(person,wasBornIn, location), (person,inﬂuence,person), (philosopher,hasAge,integer)
and (owl:Thing,subjectStartRelation,owl:Thing), that are deﬁned by using the predi-
cates rdfs:domain and rdfs:range. Further, the stored schema includes the specialization
hierarchy of classes and predicates—the specialization hierarchy is deﬁned with the
triples that include the predicates rdfs:subClassOf, rdfs:subPropertyOf, and rdf:type.
For example, the hierarchy of a class scientist is represented by triples (scientist,rdfs:-
subClassOf,person) and (person,rdfs:subClassOf,owl:Thing).

Statistics of RDF Store for Querying Knowledge Graphs
99
The interpretation of a schema triple t comprises a set of triples composed of
individual identiﬁers that belong to the classes speciﬁed by the schema triple t. The
interpretation of the schema triple (person,wasBornIn,location), for example, are the
ground triples (goedel,wasBornIn,brno), (leibniz,wasBornIn,leipzig) and (plato,was-
BornIn,athens). Therefore, the index key (person,wasBornIn,location) is mapped to the
statistical information describing the presented instances.
⊓⊔
In the following Sects. 3.1–3.3, we present the algorithms for the computation of
the statistics of the schema graphs. The procedure STATISTICS-STORED that computes
the statistics for the stored schema graph is described in Sect. 3.1. Example 2 demon-
strates that it is beneﬁcial to store the statistics not only for the schema triples from the
stored schema graph but also for more speciﬁc schema triples. This allows more precise
estimation of the size of triple patterns that form a query.
Example 2. The triple pattern (?x,wasBornIn,?y) has the type (person,wasBornIn,loca-
tion). Therefore, solely the instances of (person,wasBornIn,location) are addressed
by the triple pattern. Further, a join can be deﬁned by triple patterns (?x,was-
BornIn,?y) and (?x,hasAge,?z). The types of triple patterns are (person,wasBornIn,-
location) and (philosopher,hasAge,integer), respectively. We see that the variable ?x
has the type person as well as philosopher. Since the interpretation of a person sub-
sumes the interpretation of philosopher, we can safely use the schema triple (philoso-
pher,wasBornIn,location) as the type of (?x,wasBornIn,?y).
⊓⊔
The second procedure STATISTICS-ALL, presented in Sect. 3.2, computes statistics
for all legal schema triples. The problem with storing the statistics for all legal schema
triples is the size of such a set. Indeed, knowledge graphs include a large number of
classes and predicates [5]. To be able to control the size of the schema graph, we use
the stored schema graph as the reference point. The stored schema graph is extended
with the schema triples that are up to a given number of levels more general or more
speciﬁc than the schema triples that form the stored schema graph.
The procedure STATISTICS-LEVELS that computes the statistics for the schema
triples included in a strip around the stored schema graph is presented in Sect. 3.3.
Finally, the implementation of the procedure UPDATE-STATISTICS for counting the keys
for a given schema triple is given in Sect. 3.4.
3.1
Statistics of the Stored Schema Graph
The ﬁrst procedure for computing the statistics is based on the schema information that
is a part of the knowledge graph. As we have already stated in the introduction, we
suppose that the knowledge graph would include a complete schema, i.e., the ontology
of classes and predicates, as well as the deﬁnition of domains and ranges of predicates.
Let us now present Algorithm 2. We assumed that t = (s, p, o) was an arbitrary
triple from graph g ∈G. First, the algorithm initializes set gp in the line 2 to include the
element p, and the transitive closure of {p} computed with respect to the relationship
rdfs:subPropertyOf to obtain all more general properties of p. Note that ‘+’ denotes one
or more applications of the predicate rdfs:subPropertyOf.

100
I. Savnik et al.
Algorithm 2. Function STATISTICS-STORED(t : triple-type)
1: function STATISTICS-STORED(t = (s, p, o) : triple-type)
2:
gp ←{p} ∪{cp|(p, rdfs:subPropertyOf+, cp) ∈g}
3:
for all pg ∈gp do
4:
dp ←{ts|(pg, rdfs:domain, ts) ∈g}
5:
rp ←{to|(pg, rdfs:range, to) ∈g}
6:
for all ts ∈dp, to ∈rp do
7:
UPDATE-STATISTICS((ts, pg, to), t)
After the set gp is computed, the domains and the ranges of each particular property
pg ∈gp are retrieved from the graph in lines 4–5. After we have computed all the
sets, including the types of t’s components, the schema triples can be enumerated by
taking property pg and a pair of the domain and range classes of pg. The statistics
are updated using the procedure UPDATE-STATISTICS for each of the generated schema
triples in line 7. A detailed description of the procedure UPDATE-STATISTICS is given in
Sect. 3.4.
3.2
Statistics of all Schema Triples
Let t = (s, p, o) be arbitrary triple from graph g ∈G. The set of schema triples for
a given triple t is obtained by ﬁrst computing all possible classes of the components s
and o, and all existing super-predicates of p. In this way, we obtain the sets of classes
gs and go and, the set of predicates gp. These sets are then used to generate all possible
schema triples, i.e., all possible types of t. The statistics are updated for the generated
schema triples.
Algorithm 3. Function STATISTICS-ALL(t : triple-type)
1: function STATISTICS-ALL(t = (s, p, o) : triple-type)
2:
gs ←{ts|is class(s) ∧ts = s ∨¬is class(s) ∧(s, rdf:type, ts) ∈g}
3:
go ←{to|is class(o) ∧to = o ∨¬is class(o) ∧(o, rdf:type, to) ∈g}
4:
gs ←gs ∪{c′
s|cs ∈gs ∧(cs, rdfs:subClassOf+, c′
s) ∈g}
5:
gp ←{p} ∪{cp|(p, rdfs:subPropertyOf+, cp) ∈g}
6:
go ←go ∪{c′
o|co ∈go ∧(co, rdfs:subClassOf+, c′
o) ∈g}
7:
for all cs ∈gs, cp ∈gp, co ∈go do
8:
UPDATE-STATISTICS((cs, cp, co), t)
The procedure for the computation of statistics for a triple t is presented in
Algorithm 3. The triple t includes the components s, p and o, i.e., t = (s, p, o). The
procedure STATISTICS-ALL computes in lines 1–6 the sets of types gs, gp and go of the
triple elements s, p and o, respectively. The set of types gs is in line 2 initialized by the
set of the types of s, or, by s itself when s is a class. The set gs is then closed by means
of the relationship refs:subClassOf in line 4. Set go is computed in the same way as set

Statistics of RDF Store for Querying Knowledge Graphs
101
gs. The set of predicates gp is computed differently. Set gp obtains the value by closing
set {p} using the relationship rdfs:subPropertyOf in line 5. Indeed, predicates play a
similar role to classes in many knowledge representation systems [24].
The schema triples of t are enumerated in the last part of the procedure STATISTICS-
ALL by using sets gs, gp and go in lines 7-8. For each of the generated schema triples
(cs, cp, co) the interpretation (cs, cp, co)∗
g includes the triple t = (s.p.o). Since gs
and go include all classes of s and o, and, gp includes p and all its super-predicates, all
possible schema triples in which interpretation includes t are enumerated.
3.3
Statistics of a Strip Around the Stored Schema Graph
In the procedure STATISTICS-STORED, we did not compute the statistics of the schema
triples that are either more speciﬁc or more general than the schema triples that are
included in the knowledge graph for a given predicate. For instance, while we do have
the statistics for the schema triple (person,wasBornIn,location) since this schema triple
is part of the knowledge graph, we do not have the statistics for the schema triples
(scientist,wasBornIn,location) and (philosopher,wasBornIn,location).
The procedure STATISTICS-ALL is in a sense the opposite of the procedure
STATISTICS-STORED. Given the parameter triple t, the procedure STATISTICS-ALL
updates statistics for all possible schema triples which interpretation includes the
parameter triple t. The number of all possible schema triples may be much too large
for a knowledge graph with a rich conceptual schema. The conceptual schema of Yago
knowledge graph, for instance, includes a half-million classes.
Algorithm 4. Function STATISTICS-LEVELS(t : triple-type, k : integer)
1: function STATISTICS-LEVELS(t = (s, p, o) : triple-type, l, u : integer)
2:
gp ←{p} ∪{tp|(p, rdfs:subPropertyOf+, tp) ∈g}
3:
gs ←{ts|is class(s) ∧ts = s ∨¬is class(s) ∧(s, rdf:type, ts) ∈g}
4:
go ←{to|is class(o) ∧to = o ∨¬is class(o) ∧(o, rdf:type, to) ∈g}
5:
gs ←gs ∪{c′
s|cs ∈gs ∧(cs, rdfs:subClassOf+, c′
s) ∈g}
6:
go ←go ∪{c′
o|co ∈go ∧(co, rdfs:subClassOf+, c′
o) ∈g}
7:
dp ←{cs|(p, rdfs:domain, cs) ∈g}
8:
rp ←{cs|(p, rdfs:range, cs) ∈g}
9:
ss ←{cs|cs ∈gs ∧∃c′
s ∈dp((cs ⪰c′
s ∧DIST(cs, c′
s) ≤u) ∨
10:
(cs ⪯c′
s ∧DIST(cs, c′
s) ≤l))}
11:
so ←{co|co ∈go ∧∃c′
o ∈rp((co ⪰c′
o ∧DIST(co, c′
o) ≤u) ∨
12:
(co ⪯c′
o ∧DIST(co, c′
o) ≤l))}
13:
for all cs ∈ss, pp ∈gp, co ∈so do
14:
UPDATE-STATISTICS((cs, pp, co), t)
Let us now present the algorithm STATISTICS-LEVELS. The ﬁrst part of the algo-
rithm (lines 1–6) is the same as in the algorithm STATISTICS-ALL. Given a triple
t = (s, p, o), all types (classes) of s and o and super-predicates of p are computed
as the variables gs, go and gp, respectively.

102
I. Savnik et al.
The domain and range classes of the predicate p are computed in lines 7–8 as the
variables dp and rp. The sets of classes ss and so that include the set of selected types
for the S and P components of t are computed in lines 9–12. The set ss includes all
classes from gs such that their distance to at least one class deﬁned as the domain of
p is less than l for more speciﬁc classes, and less than u for more general classes. The
function DIST(c1, c2) computes the number of edges on the path from c1 to c2 including
solely the predicate rdfs:subClassOf, if such a path exists, or, ∞if there is no such path.
Similarly, the set so includes all classes from go such that their distance to at least one
class from rp is less than l for the classes below the stored schema, and less than u for
the classes above the stored schema.
In the last part, the algorithm STATISTICS-LEVELS in lines 13–14 enumerates the
types (schema triples) of the triple t by listing the Cartesian product of the sets ss, gp
and so.
Retrieving Statistics of a Schema Triple. We assume that S is a schema graph obtained
using the algorithm STATISTICS-LEVELS. When we want to retrieve the statistical data
for a schema triple t, we have two possible scenarios. The ﬁrst is that the schema triple
t is an element of S. The statistics are, in this case, directly retrieved from the statistical
index. This is the expected scenario, i.e., we expect that S includes all schema triples that
represent the types of triple patterns. The second scenario covers cases when the schema
triples t are either above or below S. Here, the statistics of t have to be approximated
from the statistics computed for the schema graph S. The presentation of the algorithm
for retrieving the statistics of a given schema triple is given in [30].
3.4
On Counting Keys
Let g ∈G be a knowledge graph stored in a triplestore, and, let t = (s, p, o) be a
triple. All algorithms for the computation of the statistics of g presented in the previous
section enumerate a set of schema triples S such that for each schema triple ts ∈S the
interpretation ts∗
g includes the triple t. For each of the computed schema triples ts ∈S
the procedure UPDATE-STATISTICS() updates the statistics of ts as the consequence of
the insertion of the triple t into the graph g. The triple t = (s, p, o) includes seven keys:
s, p, o, sp, so, po and spo. These keys are the targets to be queried in the triple patterns.
Keys and Key Types. Let us now deﬁne the concepts of the key and the key type. A
key is a triple (sk, pk, ok) composed of sk ∈I ∪{ }, pk ∈P ∪{ } and ok ∈O ∪{ }.
Note that we use the notation presented in Sect. 2. The symbol “ ” denotes a missing
component. A key type is a schema triple that includes the types of the key components
as well as the underlined types of components that are not parts of keys. Formally, a type
of a key (sk, pk, ok) is a schema triple (st, pt, ot), such that sk ∈st∗
g, pk ∈pt∗
g and
ok ∈ot∗
g for all the components sk, pk and ok that are not “ ”. The underlined types of
the missing components are computed from the stored schema graph. The complete type
of the key represents the key type where the underlines are omitted, i.e., the complete
type of the key is a bare schema triple.
Example 3. The
triple
( ,wasBornIn,athens)
is
a
key
with
the
components
p=wasBornIn and o=athens, while the component s does not contain a value.

Statistics of RDF Store for Querying Knowledge Graphs
103
The schema triple (person,wasBornIn,location) is the type of the key ( ,wasBorn-
In,athens). Finally, the complete type of ( ,wasBornIn,athens) is the schema triple
(person,wasBornIn,location).
Computing Statistics for Keys. Let us now present the procedure for updating the
statistics for a given triple t = (s, p, o) and the corresponding schema triple tt =
(ts, tp, to). In order to store the statistics of a triple t of type tt, we split tt into the
seven key types: (ts, tp, to), (ts, tp, to), . . ., (ts, tp, to), (ts, tp, to). Furthermore, the
triple t is split into the seven keys: (s, , ), ( p, ), . . ., ( , p, o), (s, p, o). The statistics
is updated for each of the selected key types.
There is more than one way of counting the instances of a given key type. For the
following discussion we select an example of the key type, say tk = (ts, tp, to), and
consider all the ways we can count the key ( , p, o) for a given key type tk. The conclu-
sions that we draw in the following paragraphs are general in the sense that they hold
for all key types.
1. Firstly, we can either count all keys, including duplicates, or, secondly, we can count
only the different keys. We denote these two options by using the parameters all or
distinct, respectively.
2. Secondly, we can either count triples of the type (ts, tp, to), or, the triples of the type
(⊤, tp, to). In the ﬁrst case we call counting bound and in the second case we call it
unbound.
The above stated choices are speciﬁed as parameters of the generic procedure
UPDATE-KEYTYPE (all|distinct,bound|unbound,tk,t). The ﬁrst parameter speciﬁes if
we count all or distinct triples. The second parameter deﬁnes the domain of counters,
which is either restricted by a given complete type tt of key type tk, or unrestricted, i.e.,
the underlined component is not bound by any type. The third parameter is a key type
tk, and, ﬁnally, the fourth parameter represents the triple t.
Procedure UPDATE-STATISTICS. Let us now present the procedure UPDATE-STATIS-
TICS(tt,t) for updating the entry of the statistical index that corresponds to the key
schema triple tt and a triple t. The procedure is presented in Algorithm 5.
Algorithm 5. Procedure UPDATE-STATISTICS(tt:schema-triple, t : triple)
1: procedure UPDATE-STATISTICS(tt: schema-triple, t : triple)
2:
for all key types tk of tt do
3:
UPDATE-KEYTYPE(all,unbound,tk);
4:
UPDATE-KEYTYPE(all,bound,tk);
5:
UPDATE-KEYTYPE(dist,unbound,tk, t);
6:
UPDATE-KEYTYPE(dist,bound,tk, t);
The parameters of the procedure UPDATE-STATISTICS are the schema triple tt and
the triple t such that tt is a type of t. The FOR statement in line 1 generates all seven key
types of the schema triple tt. The procedure UPDATE-KEYTYPE is applied to each of
the generated key types with the different values of the ﬁrst and the second parameters.

104
I. Savnik et al.
4
Experimental Evaluation
In this section, we present the evaluation of the algorithms for the computation of the
statistics for the two example knowledge graphs. First of all, we present the experimen-
tal environment used to compute statistics. Secondly, the statistics of a simple knowl-
edge graph introduced in Fig. 1 is presented in Sect. 4.2. Finally, the experiments with
the computation of the statistics of the Yago are presented in Sect. 4.3.
4.1
Testbed Description
The algorithms for the computation of the statistics of graphs are implemented in the
open-source system for querying and manipulation of graph databases epsilon [29].
epsilon is a lightweight RDF store based on Berkeley DB [22]. It can execute basic
graph-pattern queries on datasets that include up to 1G (109) triples.
epsilon was primarily used as a tool for browsing ontologies. The operations imple-
mented in epsilon are based on the sets of identiﬁers I, as they are deﬁned in Sect.
2.1. The operations include computing transitive closures based on some relationship
(e.g., the relationship rdfs:subClassOf), level-wise computation of transitive closures,
and computing the least upper bounds of the set elements with respect to the stored
ontology. These operations are used in the procedures for the computation of the triple-
store statistics.
4.2
Knowledge Graph Simple
Table 1(a) describes the properties of the schema graphs computed by the three algo-
rithms for the computation of the statistics. Each line of the table represents an eval-
uation of the particular algorithm on the knowledge graph Simple. The algorithms
denoted by the keywords ST, AL and LV refer to the algorithms STATISTICS-STORED,
STATISTICS-ALL and STATISTICS-LEVELS presented in the Sects. 3.1–3.3, respectively.
The running time of all the algorithms used in the experiments was below 1ms.
The columns #ulevel and #llevel represent the number of levels above and below the
stored schema graph that are relevant solely for the algorithm LV. The columns #bound
and #ubound store the number of schema triples included in the schema graph of the
computed statistics. The former is computed with the bound type of counting, and the
latter with the unbound type of counting.
Algorithm ST computes the statistics solely for the stored schema graph. This algo-
rithm can be compared to the relational approach, where the statistics are computed for
each relation. Algorithm AL computes the statistics for all possible schema triples. The
number of schema triples computed by this algorithm is signiﬁcant, even for this small
instance. In algorithm LV, there are only three levels of the schemata, i.e., the maximal
#ulevel and #dlevel both equal 2. The statistics of the schema triples that are above the
stored schema triples are usually computed since we are interested in having the global
statistics based on the most general schema triples from the schema graph. Note that the
algorithm ST gives the same results as the algorithm LV with the parameters #ulevel =
0 and #dlevel = 0.

Statistics of RDF Store for Querying Knowledge Graphs
105
Table 1. The evaluation of the Algorithms 1–3 on: a) Simple b) Yago-S
algorithm
#ulevel
#dlevel
#bound
#unbound
ST
-
-
63
47
AL
-
-
630
209
LV
0
0
63
47
LV
0
1
336
142
LV
0
2
462
173
LV
1
0
147
72
LV
1
1
476
175
LV
1
2
602
202
LV
2
0
161
76
LV
2
1
490
178
LV
2
2
616
205
algorithm
#ulevel
#dlevel
#bound
time-b
#unbound
time-u
ST
-
-
532
1
405
1
AL
-
-
>1M
>24
>1M
>24
LV
0
0
532
4.9
405
4.9
LV
1
0
3325
6.1
1257
5.4
LV
2
0
8673
8.1
2528
6.2
LV
3
0
12873
9.1
3400
6.8
LV
4
0
15988
10.9
3998
7.1
LV
5
0
18669
11.7
4464
7.5
LV
6
0
20762
12.5
4819
7.8
LV
7
0
21525
13.1
4939
7.9
LV
0
1
116158
5.9
27504
5.4
LV
1
1
148190
7.8
34196
6.3
LV
2
1
183596
10.5
40927
7.4
LV
3
1
207564
12.7
45451
7.8
LV
4
1
225540
14.4
48545
8.3
LV
5
1
239463
15.6
50677
8.7
LV
6
1
250670
17.3
52376
9.1
LV
7
1
255906
19.8
53147
9.2
LV
0
2
969542
7.6
220441
6.6
LV
1
2
>1M
>24
>1M
>24
4.3
Knowledge Graph Yago-S
In this section, we present the evaluation of the algorithms for the computation of the
statistics of the Yago-S knowledge graph—we use the core of the Yago 2.0 knowledge
base [14], including approximately 25M (106) triples. The Yago-S dataset, together
with the working environment for the computation of the statistics, is available from
the epsilon data repository [28].
Yago includes three types of classes: the Wordnet classes, the Yago classes, and the
Wikipedia classes. There are approximately 68000 Wordnet classes used in Yago that
represent the top hierarchy of the Yago taxonomy. The classes introduced within the
Yago dataset are mostly used to link the parts of the datasets. For example, they link
the Wikipedia classes to the Wordnet hierarchy. There are less than 30 newly deﬁned
Yago classes. The Wikipedia classes are deﬁned to represent group entities, individual
entities, or some properties of the entities. There are approximately 500000 Wikipedia
classes in Yago 2.0.
We use solely the Wordnet taxonomy and newly deﬁned Yago classes for the com-
putation of the statistics. We do not use the Wikipedia classes since, in most cases, they
are very speciﬁc. The height of the taxonomy is 18 levels, but there are a small number
of branches that are higher than 10. While there are approximately 571000 classes in
Yago, it includes only a small number of predicates. There are 133 predicates deﬁned in
Yago 2.0 [14], and there are only a few sub-predicates deﬁned; therefore, the structure
of the predicates is almost ﬂat.
Table 1(b) presents the execution of the algorithms
STATISTICS-STORED,
STATISTICS-ALL and STATISTICS-LEVELS on the Yago-S knowledge graph. It includes
the same columns as deﬁned for Table 1(a) (see Sect. 4.2), and two additional columns.
The two additional columns, named time-b and time-u, represent the time in hours used

106
I. Savnik et al.
for the computation of the statistics with either bound or unbound ways of counting,
respectively.
The computation time for updating the statistics for a given triple t depends on
the selected algorithm and the complexity of the triple enrollment into the conceptual
schemata. In general, the more schema triples that include t in their interpretation, the
longer the computation time is. For example, the computation time increases signiﬁ-
cantly in the case that lower levels of the ontology are used to compute the statistics,
simply because there are many schema triples (approx. 450K) on the lower levels of the
ontology. Finally, the computation of the statistics for the triples that describe people
and their activities takes much more time than the computation of the statistics for the
triples that represent links between websites, since the triples describing people have
richer relationships to the schema than the triples describing links among URIs.
Let us now give some comments on the results presented in Table 1(b). The algo-
rithm ST for the computation of statistics of a knowledge graph, including 25 Mega
triples, takes about 1 h to complete in the case of the bound and unbound types of
counting. The computation of the algorithm AL does not complete because it consumes
more than 32 GB of main memory after generating more than 2M schema triples.
The algorithm STATISTICS-LEVELS can regulate the amount of the schema triples
that serve as the framework for the computation of the statistics. The number of
the generated schema triples increases when more levels, either above or below the
stored schemata, are taken into account. We executed the algorithm with the parameter
#dlevel= 0, 1, 2. For each ﬁxed value of #dlevel, the second parameter #ulevel value
varied from 0 to 7. The number of generated schema triples increases by about a fac-
tor of 10 when the value of #dlevel changes from 0 to 1 and from 1 to 2. Note also
that varying #ulevel from 0 to 7 for each particular #dlevel results in an almost linear
increase of the generated schema triples. This is because the number of schema triples
is falling fast as they are closer to the most general schema triples from the knowledge
graph.
5
Related Work
In relational systems, the statistics are used for the estimation of the selectivity of simple
predicates and join queries in a query optimization process [6]. For each relation, the
gathered statistics include the cardinality, the number of pages, and the fraction of the
pages that store tuples. Further, for each index, a relational system stores the number
of distinct keys and the number of pages of the index. The equidistant histograms [3]
were proposed to capture the non-uniform distribution of the attribute values. Piatetsky-
Shapiro and Connell show in [23] that the equidistant histograms fail very often to give
a precise estimation of the selectivity of simple predicates. To improve the precision of
the selectivity estimation, they propose the use of histograms that have equal heights,
instead of equal intervals.
Most of the initially designed RDF stores treated stored triples as the edges of the
schema-less graphs [9,10,13,18,21,33,34]. To efﬁciently process triple patterns, these
RDF stores use either a subset of the six SPO indexes, or a set of tables (a single triple-
table or property tables) stored in a RDBMS. However, the RDF data model [25] was

Statistics of RDF Store for Querying Knowledge Graphs
107
extended with the RDF-Schema [26] that allows for the representation of knowledge
bases [2,17]. Consequently, RDF graphs can separate the conceptual and instance levels
of the representation, i.e., between the TBox and ABox [1]. Moreover, RDF-Schema
can serve as the means for the deﬁnition of taxonomies of classes (or concepts) and
predicates, i.e., the roles of a knowledge representation language [2]. Let us now present
the existent approaches to collect the statistics of triplestores.
Virtuoso [21] is based on relational technology. Since relational statistics can not
capture well the semantics of SPARQL queries, the statistics are computed in real-time
by inspecting one of 6 SPO indexes [4]. The size of conjunctive queries that can include
one comparison operation (≥, >, <, or ≤) is estimated by counting the pointers of index
blocks satisfying the conditions on the complete path from the root to the leaf block of
the index. In the case there are no conditions in a query, then sampling (1% of triples)
is used to estimate the size of the query—we choose pointers of the index blocks at
random. The results of query estimation are always stored in the index so that they are
available for the following requests.
RDF-3X [20] is a centralized triplestore system. RDF-3X uses six indexes for each
ordering of triplestore columns S, P, and O. B+ tree indexes are customized in the fol-
lowing ways. Firstly, triples are stored directly in the leaves of B+ trees. Secondly,
each index uses the lexicographic ordering of triples, which provides the opportunity to
compress triples in leaves by storing only the differences between the triples. RDF-3X
includes additional aggregated indexes where the number of triples is stored for each
particular instance (value) of the preﬁx for each of the six indexes. Aggregate indexes
can be used for the selectivity estimation of arbitrary triple patterns. They are converted
into selectivity histograms that can be stored in the main memory to improve the per-
formance of selectivity estimation. Furthermore, to provide a more precise estimation
of the size of queries in the presence of correlated predicates, frequent paths are deter-
mined, and their cardinality is computed and stored.
TriAD [8] is a distributed triplestore implemented on shared-nothing servers run-
ning centralized RDF-3X [20]. The triplestore is partitioned utilizing a multilevel graph
partitioning algorithm [16] that generates graph summarizations. These are further used
for query optimization as well as during the query execution to enable join-ahead prun-
ing. Six distributed indexes are generated for each of the SPO permutations. Partitions
are stored and indexed on slave servers, while the summary graph is stored and also
indexed at the master server. The statistics of the triplestore are stored locally for the
local partitions and, globally, for the summary graph. In both cases, the cardinality is
stored for each value of S, P, and O, and, for the pairs of values SP, SO and PO. Fur-
thermore, the selectivity of joins between P1 and P2 predicates is also stored in the
distributed index on all the slave servers and for the summary graph on the master
server.
Stocker et al. [31] was the ﬁrst to use the statistics based on some form of semantic
data. The statistics are used for the computation of the selectivity estimations in the
query optimization algorithm that chooses the ordering of joins for basic graph pat-
terns. For the triple patterns, the statistics are gathered for the bound (concrete) S, P,
and O components. The number of triples with bound S component is approximated
with the total number of triples and the number of distinct S values. Next, the number

108
I. Savnik et al.
of triples with a given concrete predicate P is computed. Finally, equidistant histograms
are computed for each particular predicate. The histograms represent the O component
value distribution. Further, to compute the statistics of joins, RDF schema statements
are used to enumerate all pairs of predicates that match in domain/range of the ﬁrst
predicate with the domain/range of the second predicate. The number of triples is com-
puted for each such pair of related predicates and employed as the upper bound of any
basic graph pattern that includes a given pair of predicates.
Neumann and Moetkotte propose the use of the characteristic sets [19] (abbr. CS)
for the computation of statistics of star-shaped queries. For a given graph g and the
subject s, a CS includes the predicates {p|(s, p, o) ∈g}. The statistics are computed for
each CS of a given RDF store. Furthermore, the CS-s of data and the CS-s of queries
are computed. A star-shaped SPARQL query retrieves instances of all CS-s that are
the super-sets of some CS from a query. Besides the statistics of CS-s, the number of
triples with a given predicate is computed for each CS and each predicate. The size of
joins in star-queries can be accurately estimated in this way. Gubichev and Neumann
further extended the work on characteristic sets (abbr. CS) in [7]. CS-s are organized
in a hierarchy. The ﬁrst level includes all CS-s of a given triplestore. The next level
includes the cheapest CS-s that are the subsets of CS-s from the previous level. The
hierarchical characterization of CS-s allows precise estimation of joins in star queries.
6
Conclusions
This paper presents a new method for the computation of the statistics of knowledge
graphs. The statistics are based on the schema graph. We propose a technique to tune
the size of the statistics; the user can choose between small, coarse-grained statistics
and different levels of larger, ﬁner-grained statistics. The smallest schema graph that
can be used as the framework for the computation of the statistics is the stored schema
graph, while the largest schema graph corresponds to all possible schema triples that
can be induced from the knowledge graph. In between, we can set the number of levels
above and below the schema triples from the stored schema graph to be included.
The computation of the statistics of the core of Yago, including around 25M triples,
takes from 1–24 h, depending on the size of the resulted schema graph of the statistics.
The experiments were run on a low-cost server with a 3.3 GHz Intel Core CPU, 16 GB
of RAM, and 7200 RPM 1 TB disk. Therefore, it is feasible to compute statistics even
for large knowledge graphs. Note that knowledge graphs are not frequently updated so
that the statistics can be computed ofﬂine in a batch job.
Finally, the current implementation of algorithms for the computation of the statis-
tics of knowledge graphs is not tuned for performance. While it includes some opti-
mizations, for instance, the transitive closures of speciﬁc sets of classes are cached in
the main memory index, there is a list of additional tuning options at the implementation
level that can speed up the computation of the statistics in practice. For example, the
complete schema graph can be stored in the main memory to speed up the operations
that involve the schema graph solely.
Acknowledgments. The authors acknowledge the ﬁnancial support from the Slovenian Research
Agency (research core funding No. P1-00383).

Statistics of RDF Store for Querying Knowledge Graphs
109
References
1. Baader, F., Calvanese, D., McGuinness, D., Nardi, D., Patel-Schneider, P.: Description Logic
Handbook. Cambridge University Press, Cambridge (2002)
2. Brachman, R.J., Levesque, H.J.: Knowledge Representation and Reasoning. Elsevier, Ams-
terdam (2004)
3. Christodoulakis, S.: Estimating block transfers and join sizes. In: Proceedings of SIGMOD
1983, SIGMOD, New York, NY, USA, pp. 40–54. ACM (1903)
4. Erling, O.: Implementing a SPARQL Compliant RDF Triple Store Using a SQL-ORDBMS.
OpenLink Software (2009)
5. F¨arber, M., Bartscherer, F., Menne, C., Rettinger, A.: Linked data quality of DBpedia, Free-
base, OpenCyc, Wikidata, and YAGO. Seman. Web J. 1, 1–53 (2017)
6. Grifﬁths-Selinger, P., Astrahan, M., Chamberlin, D., Lorie, A., Price, T.: Access path selec-
tion in a relational database management system. In: Proceedings of SIGMOD 1979, SIG-
MOD, New York, NY, USA, pp. 23–34. ACM (1979)
7. Gubichev, A., Neumann, T.: Exploiting the query structure for efﬁcient join ordering in
sparql queries. In: Amer-Yahia, S., Christophides, V., Kementsietsidis, A., Garofalakis,
M.N., Idreos, S., Leroy, V. (eds.) EDBT, pp. 439–450. OpenProceedings.org (2014)
8. Gurajada, S., Seufert, S., Miliaraki, I., Theobald, M.: TriAD: a distributed shared-nothing
RDF engine based on asynchronous message passing. In: Proceedings of the 2014 ACM
SIGMOD International Conference on Management of Data, SIGMOD 2014, New York,
NY, USA, pp. 289–300. ACM (2014)
9. Harris, S., Gibbins, N.: 3store: efﬁcient bulk RDF storage. In: 1st International Workshop on
Practical and Scalable Semantic Systems (PSSS 2003), pp. 1–15, 20 October 2003
10. Harris, S., Lamb, N., Shadbolt, N.: 4store: the design and implementation of a clustered
Rdf store. In: Proceedings of the 5th International Workshop on Scalable Semantic Web
Knowledge Base Systems (2009)
11. Harth, A., Decker, S.: Optimized index structures for querying RDF from the web. In: Third
Latin American Web Congress (LA-WEB’2005), vol. 2005, pp. 71–80, January 2005
12. Harth, A., Hose, K., Schenkel, R.: Database techniques for linked data management. In:
Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data,
SIGMOD 2012, pp. 597–600, New York, NY, USA. ACM (2012)
13. Harth, A., Umbrich, J., Hogan, A., Decker, S.: YARS2: a federated repository for query-
ing graph structured data from the web. In: Aberer, K., et al. (eds.) ASWC/ISWC -2007.
LNCS, vol. 4825, pp. 211–224. Springer, Heidelberg (2007). https://doi.org/10.1007/978-3-
540-76298-0 16
14. Hoffart, J., Suchanek, F.M., Berberich, K., Weikum, G.: YAGO2: a spatially and temporally
enhanced knowledge base from Wikipedia. Artif. Intell. 194, 28–61 (2013). Artiﬁcial Intel-
ligence, Wikipedia and Semi-Structured Resources
15. Hogan, A., et al.: Knowledge graphs. ACM Comput. Surv. 54(4) (2021)
16. Karypis, G., Kumar, V.: A fast and high quality multilevel scheme for partitioning irregular
graphs. SIAM J. Sci. Comput. 20(1), 359–392 (1999)
17. Lenat, D.B.: Cyc: a large-scale investment in knowledge infrastructure. Commun. ACM
38(11), 33–38 (1995)
18. McBride, B.: Jena: a semantic web toolkit. IEEE Internet Comput. 6(6), 55–59 (2002)
19. Neumann, T., Moerkotte, G.: Characteristic sets: accurate cardinality estimation for RDF
queries with multiple joins. In: Proceedings of the 2011 IEEE 27th International Conference
on Data Engineering, ICDE 2011, pp. 984–994, Washington, DC, USA. IEEE Computer
Society (2011)

110
I. Savnik et al.
20. Neumann, T., Weikum, G.: The RDF3X engine for scalable management of RDF data.
VLDB J. 19(1), 91–113 (2010)
21. OpenLink Software Documentation Team. OpenLink Virtuoso Universal Server: Documen-
tation (2009)
22. Oracle Corporation. Oracle Berkeley DB 11g Release 2 (2011)
23. Piatetsky-Shapiro, G., Connell, C.: Accurate estimation of number of tuples satisfying con-
dition. In: Proceedings of SIGMOD 1984, SIGMOD, pp. 256–276, New York, NY, USA.
ACM (1984)
24. Ramachandran, D., Reagan, P., Goolsbey, K.: First-orderized ResearchCyc: expressivity and
efﬁciency in a common-sense ontology. In: AAAI Reports, AAAI (2005)
25. Resource description framework (RDF) (2004). http://www.w3.org/RDF/
26. RDF schema (2004). http://www.w3.org/TR/rdf-schema/
27. Savnik, I., Nitta, K.: Method of Big-graph partitioning using a skeleton graph. In: Miluti-
novic, V., Kotlar, M. (eds.) Exploring the DataFlow Supercomputing Paradigm. CCN, pp.
3–39. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-13803-5 1
28. Savnik, I., Nitta, K.: Datasets: Simple and YAGO-s (2021). http://osebje.famnit.upr.si/
∼savnik/epsilon/datasets/
29. Savnik, I., Nitta, K.: Epsilon: data and knowledge graph database system (2021). http://
osebje.famnit.upr.si/∼savnik/epsilon/epsilon/
30. Savnik, I., Nitta, K., Skrekovski, R., Augsten, N.: Statistics of knowledge graphs based on
the conceptual schema. Technical Report, Cornell University (2021). arXiv:2109.09391
31. Stocker, M., Seaborne, A., Bernstein, A., Kiefer, C., Reynolds, D.: Sparql basic graph pattern
optimization using selectivity estimation. In: WWW 2008, Semantic Web II, WWW 2008,
pp. 595–604, New York, NY, USA. ACM (2008)
32. Webber, J.: A programmatic introduction to Neo4j. In: Proceedings of the 3rd Annual Con-
ference on Systems, Programming, and Applications: Software for Humanity, SPLASH
2012, pp. 217–218, New York, NY, USA. ACM (2012)
33. Weiss, C., Karras, P., Bernstein, A.: Hexastore: sextuple indexing for semantic web data
management. Proc. VLDB Endow. 1(1), 1008–1019 (2008)
34. Wilkinson, K., Sayers, C., Kuno, H., Reynolds, D.: Efﬁcient RDF storage and retrieval in
Jena2 (2003)
35. Zou, L., ¨Ozsu, M.T., Chen, L., Shen, X., Huang, R., Zhao, D.: gStore: a graph-based
SPARQL query engine. VLDB J. 23(4), 565–590 (2014)

Can You Answer While You Wait?
Lu´ıs Cruz-Filipe1(B)
, Gra¸ca Gaspar2
, and Isabel Nunes2
1 Department of Mathematics and Computer Science,
University of Southern Denmark, Odense, Denmark
lcfilipe@gmail.com
2 LASIGE, Department of Informatics, Faculty of Sciences,
University of Lisbon, Lisbon, Portugal
{mdgaspar,minunes}@fc.ul.pt
Abstract. Continuous query answering is a challenging problem faced
by systems that need to reason over data as it arrives. Recently, a logic-
based approach to this problem has been proposed that advocates gener-
ating hypothetical query answers – potential answers that are consistent
with the available data, but still require conﬁrmation by future input.
The current work studies hypothetical query answering in realistic
settings, where data may arrive out of order. This requires revising its
semantics and reanalysing the intuitions that led to the design of the
existing algorithms, in order to develop a novel incremental online algo-
rithm that takes into account that past data may yet arrive. We also
discuss how our methods may be extended to channels with losses.
1
Introduction
Reasoning systems used in today’s world must react in real time to information
that they receive from e.g. sensors, continuously producing results in an online
fashion. Conceptually, one of the ways to model this inﬂux of information is as
a data stream, and the reasoning tasks that these systems have to perform are
usually called continuous queries.
One important line of work [9,17,39,42,46] views continuous queries as logic
programs whose facts arrive through a data stream, and applies logic-based
methods to compute answers to those queries. In practice, answers may take a
long time to compute, and it may be useful to know that some answers are likely
to be produced in the future, based on available information that might lead to
their generation. This motivated the introduction of hypothetical answers [15]:
answers supported by the information provided so far by the input stream, but
that depend on other facts in the future being true.
Depending on the underlying communication system, information may be
delayed, and may arrive unordered. The reasoning system must therefore allow
for the possibility that absent data may still arrive, and if necessary delay
the production of answers [6,18,43]. The approaches mentioned above abstract
from communication delays by assuming that the data from the data stream is
ordered: information with timestamp greater than t is “put on hold” until all
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 111–129, 2022.
https://doi.org/10.1007/978-3-031-11321-5_7

112
L. Cruz-Filipe et al.
data about timestamp t is produced. This assumption simpliﬁes the theoretical
development greatly, but is not easily implementable.
As soon as we remove the assumption that the data stream is ordered, the
typical strategies for continuous query answering start failing, since they rely on
the possibility of processing information grouped by time point. In the present
work, we propose an alternative approach to computing and updating hypothet-
ical answers that treats communication delays explicitly. We deﬁne a procedure
for incrementally computing hypothetical answers in the presence of delays, by
means of ﬂexible strategies that deal with information as it arrives while acknowl-
edging the possibility that older data may arrive later on. The only requirement
is that a limit is known to how delayed the information may be, which we argue
is reasonable in many practical applications. In the conclusions, we discuss how
to remove this requirement.
Structure. Section 2 revisits the syntax of Temporal Datalog [42] and the main
ideas behind the formalism of hypothetical answers [15], and introduces the run-
ning example that we use throughout this article. Section 3 updates the declara-
tive and operational semantics of hypothetical answers to allow for communica-
tion delays. The new online algorithm for maintaining and updating hypothetical
answers, given in Sect. 4, relies on the novel concept of local mgu. This section
also includes proofs of soundness and completeness of the algorithm. Section 5
discusses alternative approaches to communication delays, and Sect. 6 includes
some conclusions and directions for future work.
2
Background
2.1
Continuous Queries in Temporal Datalog
We work in Temporal Datalog, the fragment of negation-free Datalog extended
with the special temporal sort from [13]. The formalism for writing continuous
queries over datastreams follows [42] with slight adaptations.
Syntax of Temporal Datalog. Temporal Datalog is an extension of Datalog [12]
where constants and variables can have two sorts: object or temporal. Terms are
also sorted: an object term is either an object constant or an object variable,
and a time term is either a natural number, a time variable, or an expression of
the form T + k where T is a time variable and k is an integer. Time constants
are also called timestamps.
Predicates take exactly one temporal parameter, which is the last one. (Some
works also allow predicates with no temporal parameter, but this adds no expres-
sive power to the language.) Atoms, rules, facts and programs are deﬁned as
usual. Rules are assumed to be safe: each variable in the head must occur in the
body.
A term, atom, rule, or program is ground if it contains no variables. We write
var(α) for the set of variables occurring in an atom α, and extend this function
homomorphically to rules and sets. A fact is a function-free ground atom.

Can You Answer While You Wait?
113
A predicate symbol is said to be intensional or IDB if it occurs in an atom in
the head of a rule with non-empty body, and extensional or EDB if it is deﬁned
only through facts. This classiﬁcation extends to atoms in the natural way.
Substitutions are functions mapping a ﬁnite set of variables to terms of the
expected sort. Given a rule r and a substitution θ, the corresponding instance
r′ = rθ of r is obtained by simultaneously replacing every variable X in r by
θ(X) and computing any additions of temporal constants.
A temporal query is a pair Q = ⟨P, Π⟩where Π is a program and P is an
IDB atom in the language underlying Π. We do not require P to be ground, and
typically the temporal parameter is uninstantiated.1
A dataset is a family D = {D|τ | τ ∈N}, where D|τ represents the set of
EDB facts delivered by a data stream at time point τ. Note that every fact in
D|τ has timestamp at most τ; facts with timestamp lower than τ correspond
to communication delays. We call D|τ the τ-slice of D, and deﬁne also the τ-
history Dτ = {D|τ ′ | τ ′ ≤τ}. It follows that D|τ = Dτ \ Dτ−1 for every τ,
and that Dτ also contains only facts whose temporal argument is at most τ. By
convention, D−1 = ∅.
Semantics. The semantics of Temporal Datalog is deﬁned in the usual way over
Herbrand models, evaluating time terms to a natural number in the obvious way.
An answer to a query Q = ⟨P, Π⟩over a set of ground facts S is a ground
substitution θ over the set of variables in P such that Π ∪S |= Pθ. In this work,
S is typically a τ-history of some dataset D. We denote the set of all answers to
Q over Dτ as A(Q, D, τ).
We illustrate these concepts with our running example, which is a variant of
Example 1 in [42].
Example 1. The following program ΠE tracks activation of cooling measures
in a set of wind turbines equipped with sensors, recording malfunctions and
shutdowns, based on temperature readings Temp(Device, Level, Time).
Temp(X, high, T) →Flag(X, T)
Flag(X, T) ∧Flag(X, T + 1) →Cool(X, T + 1)
Cool(X, T) ∧Flag(X, T + 1) →Shdn(X, T + 1)
Shdn(X, T) →Malf(X, T −2)
Malfunctions correspond to answers to the query QE = ⟨Malf(X, T), ΠE⟩. If
we assume D0 = {Temp(wt2, high, 0)}, then at time point 0 there is no answer
to QE. If Temp(wt2, high, 1) arrives to D at time point 1, then D1 = D0 ∪
{Temp(wt2, high, 1)}, and there still is no answer to QE. Finally, the arrival of
Temp(wt2, high, 2) to D at time point 2 yields D2 = D1 ∪{Temp(wt2, high, 2)},
allowing us to infer Malf(wt2, 0). Then {X := wt2, T := 0} ∈A(QE, D, 2).
◁
1 The most common exception is if P represents a property that does not depend on
time, where by convention the temporal parameter is instantiated to 0.

114
L. Cruz-Filipe et al.
In this example, all facts were delivered instantly by the data stream. In the
presence of communication delays, the answer {X := wt2, T := 0} might not be
produced at time point 2, but only later.
2.2
SLD-resolution
Our development is based on SLD-resolution. We summarize the key relevant
concepts and results, following [10,35].
A deﬁnite clause is a disjunction of literals containing at most one positive
literal. A deﬁnite clause with only negative literals is called a goal and written
¬ ∧j βj, where the βj are atoms. Deﬁnite clauses with one positive literal are
written in the standard rule notation ∧iαi →α, where the αi and α are atoms.
A most general uniﬁer (mgu) of two atomic formulas P( ⃗X) and P(⃗Y ) is a
substitution θ such that: (i) P( ⃗X)θ = P(⃗Y )θ and (ii) for all σ, if P( ⃗X)σ = P(⃗Y )σ
then σ = θγ for some substitution γ. If C is a rule ∧iαi →α, G is a goal ¬ ∧j βj
with var(G) ∩var(C) = ∅, and θ is an mgu of α and βk, then the resolvent of G
and C using θ is the goal ¬

j<k βj ∧
i αi ∧
j>k βj

θ.
Let P be a program and G be a goal. An SLD-derivation of P ∪{G} is a
sequence G0, G1, . . . of goals, a sequence C1, C2, . . . of α-renamings of program
clauses of P and a sequence θ1, θ2, . . . of substitutions such that G0 = G and
Gi+1 is the resolvent of Gi and Ci+1 using θi+1. An SLD-refutation of P ∪{G}
is a ﬁnite SLD-derivation of P ∪{G} ending in the empty clause (□), and its
computed answer is obtained by restricting the composition of θ1, . . . , θn to the
variables occurring in G.
SLD-resolution is sound and complete. If θ is a computed answer for P ∪{G},
then P |= ¬(∀Gθ). Conversely, if P |= ¬(∀Gθ), then there exist σ and γ such
that θ = σγ and σ is a computed answer for P ∪{G}. The independence of
the computation rule states that the order in which the literals in the goal are
resolved with rules from the program does not aﬀect the existence of an SLD-
refutation.
2.3
Hypothetical Answers
In Example 1, the answer to the query QE could only be determined when τ = 2.
However, we could already infer that this answer might arise from the fact that
D0 = {Temp(wt2, high, 0)}. The later delivery of Temp(wt2, high, 1) supports this
possibility, while its absence from the data stream would have discarded it.
The formalism of hypothetical answers [15] builds on this idea. A hypothetical
answer to a query Q = ⟨P, Π⟩given a dataset D and a time instant τ is a pair
⟨θ, H⟩where θ is a substitution over the variables in P and H is a ﬁnite set of
ground EDB atoms (the hypotheses) such that all atoms in H have a timestamp
larger than τ, Π ∪Dτ ∪H |= Pθ, and H is minimal with respect to set inclusion.
Furthermore, if the minimal subset E of Dτ such that Π ∪E ∪H |= Pθ is
non-empty, then ⟨θ, H⟩is said to be supported by E (the evidence).

Can You Answer While You Wait?
115
Example 2. In Example 1, ⟨[X := wt2, T := 0] , {Temp(wt2, high, 2)}⟩is a hypo-
thetical answer for QE and D at time 1. This answer is supported by the evidence
{Temp(wt2, high, 0), Temp(wt2, high, 1)}.
The pair ⟨[X := wt2, T := 3] , {Temp(wt2, high, t) | t = 3, 4, 5}⟩is also a hypo-
thetical answer for QE and D at time 1, but it is not supported.
◁
Hypothetical answers can be computed dynamically as information is deliv-
ered by the data stream. Given a query Q = ⟨P, Π⟩, an oﬄine pre-processing
step applies SLD-resolution to Π and ←P until only EDB atoms remain. All
leaves and corresponding substitutions are collected in a set PQ of pairs ⟨θ, H⟩.
For each time point τ, a set of schematic hypothetical answers Sτ of the form
⟨θ, E, H⟩is computed online as follows.
1. If ⟨θ, H⟩∈PQ and σ is such that Hσ \ D|τ only contains atoms with times-
tamp higher than τ, then ⟨θσ, Hσ ∩D|τ, Hσ \ D|τ⟩is added to Sτ.
2. If ⟨θ, E, H⟩∈Sτ−1 and σ is such that Hσ \ D|τ only contains atoms whose
timestamp is higher than τ, then the triple ⟨θσ, E ∪(Hσ ∩D|τ), Hσ \ D|τ⟩
is added to Sτ.
The candidate substitutions σ are easily obtained by unifying the elements of
each set H that have minimal temporal argument with the elements of D|τ.
Example 3. In the context of our running example, the pre-processing step yields
the singleton set
PQE = {⟨∅, {Temp(X, high, T), Temp(X, high, T + 1), Temp(X, high, T + 2)}⟩} .
Taking D0, D1 and D2 as in Example 1, the previous algorithm yields
S0 = {⟨[X := wt2, T := 0] , {Temp(wt2, high, 0)}, {Temp(wt2, high, i) | i = 1, 2}⟩}
S1 = {⟨[X := wt2, T := 0] , {Temp(wt2, high, i) | i = 0, 1}, {Temp(wt2, high, 2)}⟩,
⟨[X := wt2, T := 1] , {Temp(wt2, high, 1)}, {Temp(wt2, high, i) | i = 2, 3}⟩}
S2 = {⟨[X := wt2, T := 0] , {Temp(wt2, high, i) | i = 0, 1, 2}, ∅⟩,
{⟨[X := wt2, T := 1] , {Temp(wt2, high, i) | i = 1, 2}, {Temp(wt2, high, 3)}⟩,
⟨[X := wt2, T := 2] , {Temp(wt2, high, 2)}, {Temp(wt2, high, i) | i = 3, 4}⟩}
This algorithm is sound and complete: the supported hypothetical answers
at each time point τ are the ground instantiations of the schematic answers
computed at the same time point.
3
Introducing Communication Delays
So far, facts are assumed to be received instantaneously at the data stream: any
fact with timestamp τ must be in D|τ, if present. However, communications take
time, so a fact with timestamp τ may arrive later.

116
L. Cruz-Filipe et al.
In this section, we extend the formalism of hypothetical answers to deal with
these communication delays. We assume that communications may be delayed
(but not lost) and that there is a known upper bound on the delay. In practice,
there are communication protocols that ensure this property (with high enough
probability).
The bound on the delay may be diﬀerent for diﬀerent predicates, and for
diﬀerent instantiations of the same predicate; the only restriction is that it may
not depend on the timestamp.2 We model this as a function δ mapping each
ground EDB atom in the language of Π to a natural number, with the restric-
tion that: if a, a′ diﬀer only in their temporal argument, then δ(a) = δ(a′). We
extend δ to non-ground atoms by deﬁning δ(P(t1, . . . , tn)) as the maximum of
all δ(P(t′
1, . . . , t′
n)) such that P(t′
1 . . . , t′
n) is a ground instance of P(t1, . . . , tn),
and to predicate symbols by δ(P) = δ(P(X1, . . . , Xn)).
3.1
Declarative Semantics
The ﬁrst ingredient in our extension is the following notion.
Deﬁnition 1. A ground atom P(t1, . . . , tn) is future-possible for τ if τ < tn +
δ(P(t1, . . . , tn)).
In other words, an atom is future-possible for τ if it still may be delivered
by the data stream. Future-possible atoms generalize the notion of “atom with
timestamp greater than τ” – in particular, any such atom is future-possible for τ.
Hypothetical answers can now be generalized to include future-possible
atoms.
Deﬁnition 2. A hypothetical answer to query Q over Dτ is a pair ⟨θ, H⟩, where
θ is a substitution and H is a ﬁnite set of ground EDB atoms (the hypotheses)
such that:
– supp(θ) = var(()P), i.e., θ only changes variables that occur in P;
– H only contains atoms future-possible for τ;
– Π ∪Dτ ∪H |= Pθ;
– H is minimal with respect to set inclusion.
H(Q, D, τ) is the set of hypothetical answers to Q over Dτ.
As before, if the minimal subset E of Dτ such that Π ∪E ∪H |= Pθ is non-
empty, then ⟨θ, H, E⟩is a supported answer to Q over Dτ. We denote the set of
all supported answers to Q over Dτ by E(Q, D, τ).
The key properties of hypothetical and supported answers still hold for this
generalized notion. The proofs are straightforward adaptations of those in [14].
2 This is a reasonable assumption e.g. in the case of information originating from
sensors, where the delay may depend on the distance and infrastructure and therefore
be diﬀerent for each sensor, but typically does not change over time. At the end of
this article, we brieﬂy discuss how to extend our framework to deal with unbounded
communication delays and possible loss of information.

Can You Answer While You Wait?
117
Proposition 1. Let Q = ⟨P, Π⟩be a query, D be a dataset and τ be a time
instant. If ⟨θ, ∅⟩∈H(Q, D, τ), then θ ∈A(Q, D, τ).
Proposition 2. Let Q = ⟨P, Π⟩be a query, D be a dataset and τ be a time
instant. If ⟨θ, H⟩∈H(Q, D, τ), then there exist a time point τ ′ ≥τ and a
dataset D′ such that Dτ = D′
τ and θ ∈A(Q, D′, τ ′).
Proposition 3. Let Q = ⟨P, Π⟩be a query, D be a dataset and τ be a time
instant. If ⟨θ, H⟩∈H(Q, D, τ), then there exists H0 such that ⟨θ, H0⟩∈
H(Q, D, −1) and H = H0 \ Dτ. Furthermore, if H ̸= H0, then ⟨θ, H, H0 \ H⟩∈
E(Q, D, τ).
Proposition 4. Let Q = ⟨P, Π⟩be a query, D be a dataset and τ be a
time instant. If ⟨θ, H⟩∈H(Q, D, τ) and τ ′ < τ, then there exists ⟨θ, H′⟩∈
H(Q, D, τ ′) such that H = H′ \ (Dτ \ Dτ ′).
Example 4. We illustrate these concepts with the program from Example 1. We
assume that δ(Temp) = 1, that D|0 = {Temp(wt4, high, 0)}, and that D|1 = ∅.
Let θ = [X := wt4, T := 0]. Then
⟨θ, {Temp(wt4, high, 1), Temp(wt4, high, 2)}⟩∈H(QE, D, 1) ,
reﬂecting the intuition that Temp(wt4, high, 1) may still arrive in D|2. This
answer is supported by Temp(wt4, high, 0).
◁
3.2
Operational Semantics
The operational semantics based on SLD-resolution, which forms the basis for
the online algorithm in [15], also extends to a scenario with communication
delays: the key change is deﬁnining an operational counterpart to the notion of
future-possible atom.
Deﬁnition 3. An atom P(t1, . . . , tn) is a potentially future atom wrt τ if either
tn contains a temporal variable or tn is ground and τ < tn + δ(P(t1, . . . , tn)).
This concept generalizes the notion of future-possible atom for τ to possibly
non-ground atoms. In particular, any atom whose temporal parameter contains a
variable is potentially future, since it may be instantiated to a future timestamp.
Without loss of generality, we assume that all SLD-derivations have the fol-
lowing property: when unifying a goal ¬∧iαi with a clause ∧jβj →β, the chosen
mgu θ = [X1 := t1, . . . , Xn := tn] is such that all variables occurring in t1, . . . , tn
also occur in the chosen atom αk.
Deﬁnition 4. An SLD-refutation with future premises of Q over Dτ is a ﬁnite
SLD-derivation of Π ∪Dτ ∪{¬P} whose last goal only contains potentially future
EDB atoms wrt τ.
If R is an SLD-refutation with future premises of Q over Dτ with last goal
G = ¬ ∧i αi and θ is the substitution obtained by restricting the composition of
the mgus in R to var(P), then ⟨θ, ∧iαi⟩is a computed answer with premises to
Q over Dτ, denoted ⟨Q, Dτ⟩⊢SLD ⟨θ, ∧iαi⟩.

118
L. Cruz-Filipe et al.
Example 5. Consider the setting of Example 4 and τ = 1. There is an SLD-
derivation of Π ∪D1 ∪{¬Malf(X, T)} ending with
←Temp(wt4, high, 1), Temp(wt4, high, 2) ,
which contains two potentially future EDB atoms with respect to 1. Thus,
⟨QE, D1⟩⊢SLD ⟨θ, Temp(wt4, high, 1) ∧Temp(wt4, high, 2)⟩
with θ = [X := wt4, T := 0].
◁
As before, computed answers with premises are the operational counterpart
to hypothetical answers, with two caveats: a computed answer with premises
need not be ground, as the corresponding SLD-derivation may include some
universally quantiﬁed variables in the last goal; and ∧iαi may contain redundant
conjuncts, in the sense that they might not be needed to establish the goal (see
the examples in [15]).
With the new deﬁnitions, computed answers with premises and hypothetical
answers are related as before.
Proposition 5 (Soundness).
Let Q = ⟨P, Π⟩be a query, D be a dataset
and τ be a time instant. Assume that ⟨Q, Dτ⟩⊢SLD ⟨θ, ∧iαi⟩. Let σ be a ground
substitution such that: (i) supp(σ) = var(∧iαi) ∪(var(P) \ supp(θ)) and (ii) if αi
is P(t1, . . . , tn), then tnσ +δ(P(t1, . . . , tn)) > τ. Then there is a set H ⊆{αiσ}i
such that ⟨(θσ)|var(P ), H⟩∈H(Q, D, τ).
Proposition 6 (Completeness). Let Q = ⟨P, Π⟩be a query, D be a dataset
and τ be a time instant. If ⟨θ, H⟩∈H(Q, D, τ), then there exist substitutions
ρ and σ and a ﬁnite set of atoms {αi}i such that θ = ρσ, H = {αiσ}i and
⟨Q, Dτ⟩⊢SLD ⟨ρ, ∧iαi⟩.
The incremental algorithm in [15] is based on the idea of “organizing” SLD-
derivations adequately to pre-process Π independently of Dτ, so that the com-
putation of (hypothetical) answers can be split into an oﬄine part and a less
expensive online part. The following result asserts that this can be done.
Proposition 7. Let Q = ⟨P, Π⟩be a query and D be a dataset. For any time
constant τ, if ⟨Q, Dτ⟩⊢SLD ⟨θ, ∧iαi⟩, then there exist an SLD-refutation with
future premises of Q over Dτ computing ⟨θ, ∧iαi⟩and a sequence k−1 ≤k0 ≤
. . . ≤kτ such that:
– goals G1, . . . , Gk−1 are obtained by resolving with clauses from Π;
– for 0 ≤i ≤τ, goals Gki−1+1, . . . , Gki are obtained by resolving with clauses
from D|i.
Proof. Immediate consequence of the independence of the computation rule. ⊓⊔

Can You Answer While You Wait?
119
An SLD-refutation with the structure described in Proposition 7, which we
call stratiﬁed, also has the property that all goals after Gk−1 are always resolved
with EDB atoms. Furthermore, each goal Gki contains only potentially future
EDB atoms with respect to i. Let θi be the restriction of the composition of all
substitutions in the SLD-derivation up to step ki to var(P). Then Gki = ¬ ∧j αj
represents all hypothetical answers to Q over Di of the form ⟨(θiσ)|var(P ), ∧jαj⟩
for some ground substitution σ (cf. Proposition 5).
This yields an online procedure to compute supported answers to continuous
queries over data streams. In a pre-processing step, we calculate all computed
answers with premises to Q over D−1, and keep the ones with minimal set of
formulas. (Note that Proposition 6 guarantees that all minimal sets are generated
by this procedure, although some non-minimal sets may also appear.) The online
part of the procedure then resolves each of these sets with the facts delivered by
the data stream, adding the resulting resolvents to a set of schemata of supported
answers (i.e. where variables may still occur). By Proposition 7, if there is at
least one resolution step at this stage, then the hypothetical answers represented
by these schemata all have evidence, so they are indeed supported.
The pre-processing step coincides exactly with that described in [15], which
also includes examples and suﬃcient conditions for termination. The online part,
though, needs to be reworked in order to account for communication delays. This
is the topic of the next section.
4
Incremental Computation of Hypothetical Answers
Pre-processing a query returns a ﬁnite set PQ that represents H(Q, D, −1): for
each computed answer ⟨θ, ∧iαi⟩with premises to Q over D−1 where {αi}i is
minimal, PQ contains an entry ⟨θ, {αi}i⟩. Each tuple ⟨θ, H⟩∈PQ represents the
set of all hypothetical answers ⟨θσ, Hσ⟩as in Proposition 5. We now show how
to compute and update the set E(Q, D, τ) schematically.
Deﬁnition 5. Let Γ and Δ be sets of atoms such that all atoms in Δ are ground.
A substitution σ is a local mgu for Γ and Δ if: for every substitution θ such
that Γθ ∩Δ = Γσ ∩Δ there exists another substitution ρ such that θ = σρ.
Intuitively, a local mgu for Γ and Δ is a substitution that behaves as an mgu of
some subsets of Γ and Δ. Local mgus allow us to postpone instantiating some
atoms, in order to cope with delayed information that needs to be processed
later – see Example 6 below.
Lemma 1. If Γ and Δ are ﬁnite, then the set of local mgus for Γ and Δ is
computable.
Proof. We claim that the local mgus for Γ and Δ are precisely the computed
substitutions at some node of an SLD-tree for ¬  Γ and Δ.
Suppose σ is a local mgu for Γ and Δ, and let Ψ = {a ∈Γ|aσ ∈Δ}. Build
an SLD-derivation by unifying at each stage an element of Ψ with an element of

120
L. Cruz-Filipe et al.
Δ. This is trivially a valid SLD-derivation, and the substitution θ it computes
must be σ: (i) by soundness of SLD-resolution, θ is an mgu of Ψ and a subset of
Δ; but Ψθ ∩Δ = Δ is a set of ground atoms, so θ must coincide with σ on all
variables occurring in Ψ, and be the identity on all other variables; and (ii) by
construction Γθ ∩Δ = Γσ ∩Δ, so θ cannot instantiate fewer variables than σ.
Consider now an arbitrary SLD-derivation for ¬  Γ and Δ with computed
substitution σ, and let Ψ be the set of elements of Γ that were uniﬁed in this
derivation. Then σ is an mgu of Ψ and a subset of Δ, and as before this means
that it maps every variable in Ψ to a ground term and every other variable to
itself. Suppose that θ is such that Γθ ∩Δ = Ψθ ∩Δ. In particular, θ also uniﬁes
Ψ and a subset of Δ, so it must coincide with σ in all variables that occur in Ψ.
Taking ρ as the restriction of θ to the variables outside Ψ shows that σ is a local
mgu for Γ and Δ.
⊓⊔
Example 6. Consider the program Π consisting of the rule
q(X, T) ←p(X, T), r(Y, T) ,
where p is an EDB with δ(p) = 2 and r is an EDB with δ(r) = 0, and the
query ⟨q(X, T), Π⟩. The pre-processing step for this query yields the singleton
set PQ = {⟨∅, {p(X, T), r(Y, T)}⟩}.
Suppose that D|0 = {p(a, 0), r(b, 0)}. There are two SLD-trees for the goal
←p(X, T), r(Y, T) and D|0:
←p(X, T), r(Y, T)
[X:=a,T :=0]

←p(X, T), r(Y, T)
[Y :=b,T :=0]

←r(Y, 0)
[Y :=b]

←p(X, 0)
[X:=a]

□
□
These trees include the computed substitutions σ0 = ∅, σ1 = [X := a, T := 0],
σ2 = [Y := b, T := 0] and σ3 = [X := a, Y := b, T := 0], which are easily seen to
be local mgus.
Although σ3 is an answer to the query, we also need to consider substitution
σ2: since δ(p) = 2, it may be the case that additional answers are produced
(e.g. if p(c, 0) ∈D|2). However, since δ(r) = 0, substitution σ1 can be safely
discarded. Substitution σ0 does not need to be considered: since it does not
unify any element of H with D|0, any potential answers it might produce would
be generated by step 1 of the algorithm in future time points.
◁
Deﬁnition 6. The set Sτ of schematic supported answers for query Q at time
τ is deﬁned as follows.
– S−1 = {⟨θ, ∅, H⟩| ⟨θ, H⟩∈PQ}.
– If ⟨θ, E, H⟩∈Sτ−1 and σ is a local mgu for H and D|τ such that Hσ\D|τ only
contains potentially future atoms wrt τ, then ⟨θσ, E ∪E′, Hσ \ D|τ⟩∈Sτ,
where E′ = Hσ ∩D|τ.

Can You Answer While You Wait?
121
Example 7. We illustrate this mechanism in the setting of Example 4, where
PQ = {⟨∅, {Temp(X, high, T), Temp(X, high, T + 1), Temp(X, high, T + 2)}


	
H
⟩}
and we assume that δ(Temp) = 1. We start by setting S−1 = {⟨∅, ∅, H⟩}.
Since D|0 = {Temp(wt2, high, 0)}, SLD-resolution between H and D|0 yields
the local mgus ∅and [X := wt2, T := 0]. Therefore,
S0 = {⟨∅, ∅, {Temp(X, high, T), Temp(X, high, T + 1), Temp(X, high, T + 2)}⟩,
⟨[X := wt2, T := 0] , {Temp(wt2, high, 0)}, {Temp(wt2, high, i) | i = 1, 2


	
H0
}⟩} .
Next, D|1 = ∅, so trivially D|1 ⊆H0 and therefore the empty substitution
is a local mgu of H0 and D|1. Furthermore, H0 only contains potentially future
atoms wrt 1 because δ(Temp) = 1. The same argument applies to D|1 and H.
So S1 = S0.
We now consider several possibilities for what happens to the schematic sup-
ported answer ⟨[X := wt2, T := 0] , {Temp(wt2, high, 0)}, H0⟩at time instant 2.
Since H0 is ground, the only local mgu of H0 and D|2 will always be ∅.
– If Temp(wt2, high, 1) /∈D|2, then H0 \ D|2 contains Temp(wt2, high, 1), which
is not a potentially future atom wrt 2, and therefore this schematic supported
answer is discarded.
– If Temp(wt2, high, 1) ∈D|2 but Temp(wt2, high, 2) /∈D|2, then H0 \ D|2 =
{Temp(wt2, high, 2)}, which only contains potentially future atoms wrt 2, and
therefore S2 contains the schematic supported answer
⟨[X := wt2, T := 0] , {Temp(wt2, high, i) | i = 0, 1, 2}⟩.
– Finally, if {Temp(wt2, high, 1), Temp(wt2, high, 2)} ⊆D|2, then H2 \ D|2 = ∅,
and the system can output the answer [X := wt2, T := 0] to the original query.
In this case, this answer would be added to S2, and then trivially copied to
all subsequent Sτ.
◁
As in this example, answers are always propagated from Sτ to Sτ+1. In an
actual implementation of this algorithm, we would likely expect these answers
to be output when generated, and discarded afterwards.
Proposition 8 (Soundness). If ⟨θ, E, H⟩∈Sτ, E ̸= ∅, and σ instantiates all
free variables in E ∪H, then ⟨θσ, H′, Eσ⟩∈E(Q, D, τ) for some H′ ⊆Hσ.
Proof. By induction on τ, we show that ⟨Q, Dτ⟩⊢SLD ⟨θ, ∧iαi⟩with H = {αi}i.
For S−1 this is trivially the case, due to the way that PQ is computed.
Assume now that there exists ⟨θ0, E0, H0⟩∈Sτ−1 such that σ0 is a local
mgu of H0 and D|τ, θ = θ0σ0, E = E0 ∪(H0σ0 ∩D|τ) and H = H0σ0 \ D|τ.
If H0σ0 ∩Dτ = ∅, then necessarily σ0 = ∅(no steps of SLD-resolution were
performed), and the thesis holds by induction hypothesis. Otherwise, we can

122
L. Cruz-Filipe et al.
build the required derivation by extending the derivation obtained by induction
hypothesis with uniﬁcation steps between the relevant elements of H0 and D|τ.
By Lemma 1, this derivation computes the substitution θ.
Applying Proposition 5 to this SLD-derivation yields an H′ ⊆Hσ such that
⟨θσ, H′⟩∈H(Q, D, τ). By construction, Eσ ̸= ∅is evidence for this answer.
⊓⊔
It may be the case that Sτ contains some elements that do not correspond to
hypothetical answers because of the minimality requirement. Consider a simple
case of a query Q where
PQ = {⟨∅, {p(a, 0), p(b, 1), p(c, 2)}⟩, ⟨∅, {p(a, 0), p(c, 2), p(d, 3)}⟩} ,
and suppose that D|0 = {p(a, 0)} and D|1 = {p(b, 1)}. Then
S1 = {⟨∅, {p(a, 0), p(b, 1)}, {p(c, 2)}⟩, ⟨∅, {p(a, 0)}, {p(c, 2), p(d, 3)}⟩} ,
and the second element of this set has a non-minimal set of hypotheses. For
simplicity, we do not to include a test for set inclusion in the deﬁnition of Sτ.
Proposition 9 (Completeness).
If ⟨σ, H, E⟩∈E(Q, D, τ), then there exist
a substitution ρ and a triple ⟨θ, E′, H′⟩∈Sτ such that σ = θρ, H = H′ρ and
E = E′ρ.
Proof. By Proposition 6, ⟨Q, Dτ⟩⊢SLD ⟨θ, ∧iαi⟩for some substitution ρ and set
of atoms H′ = {αi}i with H = {αiρ}i and σ = θρ for some θ. By Proposition 7,
there is a stratiﬁed SLD-derivation computing this answer. The sets of atoms
from D|τ uniﬁed in each stratum of this derivation deﬁne the set of elements
from H that need to be uniﬁed to construct the corresponding element of Sτ. ⊓⊔
The use of local mgus gives a worst-case exponential complexity to the com-
putation of Sτ from Sτ−1: since every node of the SLD-tree can give a local
mgu as in the proof of Lemma 1, there can be as many local mgus as subsets of
H that can be uniﬁed with D|τ. In practice, the number of substitutions that
eﬀectively needs to be considered is at most (k + 1)v, where v is the number of
variables in H and k is the maximum number of possible instantiations for a
variable in D|τ. In practice, v only depends on the program Π, and is likely to
be small, while k is 0 if the elements of H are already instantiated.
If there are no communication delays and the program satisﬁes an additional
property, we can regain the polynomial complexity from [15].
Deﬁnition 7. A query Q = ⟨P, Π⟩is connected if each rule in P contains at
most one temporal variable, which occurs in the head if it occurs in the body.
Proposition 10. If the query Q is connected and δ(P) = 0 for every predicate
symbol P, then Sτ can be computed from PQ and Sτ−1 in time polynomial in
the size of PQ, Sτ−1 and D|τ.

Can You Answer While You Wait?
123
Proof. Suppose that δ(P) = 0 for every P.
If ⟨θ, ∅, H⟩∈Sτ−1, then (i) ∅is always a local mgu of H and D|τ, so this
schematic answer can be added to Sτ (in constant time); and (ii) we can compute
in polynomial time the set M ⊆H of elements that need to be in D|τ in order to
yield a non-empty local mgu σ of H and D|τ such that Hσ \ D|τ only contains
potentially future atoms wrt τ – these are the elements whose timestamps are
less or equal to all other elements’ timestamps. (Since there are no delays, they
must all arrive simultaneously at the data stream.) To decide which substitutions
make M a subset of D|τ, we can perform classical SLD-resolution between M and
D|τ. For each such element of Sτ−1, the size of every SLD-derivation that needs
to be constructed is bound by the number of atoms in the initial goal, since D|τ
only contains facts. Furthermore, all uniﬁers can be constructed in time linear
in the size of the formulas involved, since the only function symbol available is
addition of temporal terms. Finally, the total number of SLD-derivations that
needs to be considered is bound by the size of Sτ−1 × D|τ.
If ⟨θ, E, H⟩∈Sτ−1 and E ̸= ∅, then we observe that the temporal argument
is instantiated in all elements of E and H – this follows from connectedness and
the fact that the elements of E are ground by construction. Therefore, we know
exactly which facts in H must unify with D|τ – these are the elements of H
whose timestamp is exactly τ. As above, the elements that must be added to Sτ
can then be computed in polynomial time by SLD-resolution.
⊓⊔
The key step of this proof amounts to showing that the algorithm from [15]
computes all local mgus that can lead to schematic supported answers.
Both conditions stated above are necessary. If Q is not connected, then Sτ
may contain elements ⟨θ, E, H⟩where E ̸= ∅and H contains elements with
uninstantiated timestamps; and if for some predicate δ(P) ̸= 0, then we must
account for the possibility that facts about P arrive with some delay to the data
stream. In either case, this means that we do not know the set of elements that
need to unify with the data stream, and need to consider all possibilities.
Example 8. In the context of our running example, we say that a turbine has a
manufacturing defect if it exhibits two speciﬁc failures during its lifetime: at some
time it overheats, and at some (diﬀerent) time it does not send a temperature
reading. Since this is a manufacturing defect, we set it to hold at timepoint 0,
regardless of when the failures occur. We model this property by the rule
Temp(X, high, T1), Temp(X, n/a, T2) →Defective(X, 0) .
Let Π′
E be the program obtained from ΠE by adding this rule, and consider
now the query Q′ = ⟨Defective(X, T), Π′
E⟩. Performing SLD-resolution between
Π′
E and Defective(X, 0) yields the goal ¬ (Temp(X, high, T1) ∧Temp(X, n/a, T2)),
which only contains potentially future atoms with respect to −1.
Assume that Temp(wt2, high, 0) ∈D0. Then
⟨θ′ = [X := wt2, T := 0] , {Temp(wt2, high, 0)}, {Temp(wt2, n/a, T2)}⟩∈S0 .
We do not know if or when θ′ will become an answer to the original query, but
our algorithm is still able to output relevant information to the user.
◁

124
L. Cruz-Filipe et al.
Under some assumptions, we can also bound the interval in which a schematic
hypothetical answer can be present in S. Assume that the query Q has a temporal
variable T and that Sτ contains a triple ⟨θ, E, H⟩with Tθ ≤τ and H ̸= ∅.
– Suppose that there is a number d such that for every substitution σ and every
τ ≥Tσ + d, σ ∈A(Q, D, τ) iﬀσ ∈A(Q, D, Tσ + d). Then the timestamp of
each element of H is at most τ + d.
– Similarly, suppose that there is a natural number w such that each σ is an
answer to Q over Dτ iﬀσ is an answer to Q over Dτ \Dτ−w. Then all elements
in E must have timestamp at least τ −w.
The values d and w are known in the literature as (query) delay and window
size, respectively, see e.g. [42].
5
Related Work
This work contributes to the ﬁeld of stream reasoning, the task of conjunctively
reasoning over streaming data and background knowledge [44].
Research advances on Complex Event Processors and Data Stream Manage-
ment Systems [16], together with Knowledge Representation and the Semantic
Web, all contributed to the several stream reasoning languages, systems and
mechanisms proposed during the last decade [18].
Computing answers to a query over a data source that is continuously pro-
ducing information, be it at slow or very fast rates, asks for techniques that allow
for some kind of incremental evaluation, in order to avoid reevaluating the query
from scratch each time a new tuple of information arrives. Several eﬀorts have
been made in that direction, capitalising on incremental algorithms based on
seminaive evaluation [1,7,24,25,37], on truth maintenance systems [8], window
oriented [21] among others. The framework we build upon [15] ﬁts naturally in
the ﬁrst class, as it is an incremental variant of SLD-resolution.
Hypothetical query answering over streams is broadly related to works that
study abduction in logic programming [19,26], namely those that view negation
in logic programs as hypotheses and relate it to contradiction avoidance [3,20].
Furthermore, our framework can be characterized as applying an incremental
form of data-driven abductive inference. Such forms of abduction have been
used in other works [36], but with a rather diﬀerent approach and in the plan
interpretation context. To our knowledge, hypothetical or abductive reasoning
has not been previously used in the context of stream reasoning, to answer
continuous queries, although it has been applied to annotation of stream data [4].
A similar problem also arises in the area of incomplete databases, where the
notions of possible and certain answers have been developed [28]. In this con-
text, possible answers are answers to a complete database D′ that the incomplete
database D can represent, while certain answers consist of the tuples that belong
to all complete databases that D can represent (the intersection of all possible
answers). Libkin [34] the way those notions were deﬁned, exploring an alternative
way of looking at incomplete databases that dates back to Reiter [41], and that

Can You Answer While You Wait?
125
views a database as a logical theory. Libkin’s approach was to explore the seman-
tics of incompleteness further in a setting that is independent of a particular data
model, and appealing to orderings to describe the degree of incompleteness of
a data model and relying on those orderings to deﬁne certain answers. Other
authors [22,30,31,40] have also investigated ways to assign conﬁdence levels to
the information output to the user.
Most theoretical approaches to stream-processing systems commonly require
input streams to be ordered. However, some approaches, namely from the area
of databases and event processing, have developed techniques to deal with out-
of-order data. An example of such a technique requires inserting special marks
in the input stream (punctuations) to guide window processing [32]. Punctua-
tions assert a timestamp that is a lower bound for all future incoming values
of a attribute. Another technique starts by the potential generation of out-of-
order results, which are then ordered by using stackbased data structures and
associated purge algorithms [33].
Commercial systems [27] for continuous query answering use other techniques
to deal with communication delays, not always with a solid foundational back-
ground. One such technique is the use of watermarks [2], which specify how long
the system should wait for information to arrive. In practice, watermarks serve
the same purpose as the function δ in our framework. However, they do not
come with guarantees of correctness: in fact, information may arrive after the
time point speciﬁed by the watermark, in which case it is typically discarded.
Memory consumption is a concern in [45], where a sound and complete
stream reasoning algorithm is presented for a fragment of datalogMTL – forward-
propagating datalogMTL – that also disallows propagation of derived informa-
tion towards the past. DatalogMTL is a Datalog extension of the Horn fragment
of MTL [5,29], which was proposed in [11] for ontology-based access to temporal
log data. DatalogMTL rules allow propagation of derived information to both
past and future time points. Concerned with the possibly huge or unbounded
set of input facts that have to be kept in memory, the authors of [45] restrict the
set of operators of DatalogMTL to one that generates only so-called forward-
propagating rules. They present a sound and complete algorithm for stream rea-
soning with forward-propagating datalogMTL that limits memory usage through
the use of a sliding window.
6
Discussion, Conclusions and Future Work
In the current work, we expanded the formalism of hypothetical answers to
continuous queries with the possibility of communication delays, and showed
how we could deﬁne a declarative semantics and a corresponding operational
semantics by suitably adapting and enriching the deﬁnitions from [15].
Our work deals with communication delays without requiring any previous
processing of the input stream, as long as bounds for delays are known. To
the best of our knowledge, this is the ﬁrst work providing this ﬂexibility in the
context of a logical approach to stream query answering. This motivated us to

126
L. Cruz-Filipe et al.
develop a resolution search strategy driven by the arrival of data, instead of
a static one. It would be interesting to try to combine our resolution strategy
with dynamic strategies [23], or techniques to produce compact representations
of search trees [38].
The algorithm we propose needs to store signiﬁcant amounts of information.
This was already a problem in [15], and the addition of delays enhances it (since
invalid answers are in general discarded later). One possible way to overcome this
limitation would be to assign conﬁdence levels to schematic supported answers,
and either discard answers when their conﬁdence level is below a given threshold,
or discard the answers with lowest conﬁdence when there are too many. This
requires having information about (i) the probability that a given premise is
true, and (ii) the probability that premise arrives with a given delay. These
probabilities can be used to infer the likelihood that a given schematic supported
answer will be conﬁrmed. The relevant probabilities could be either evaluated
by experts, or inferred using machine-learning tools.
Our approach extends naturally extends to the treatment of lossy channels.
If we have a sub-probability distribution for communication delays (i.e. where
the sum of all probabilities is strictly smaller than 1), then we are also allowing
for the chance that information is lost in transit. Thus, the same analysis would
also be able to estimate the likelihood that a given substitution is an answer,
even though some of the relevant premises are missing. We plan to extend our
framework along these lines, in order to have enough ingredients to develop a
prototype of our system and conduct a full practical evaluation.
The presentation in [15] also discussed brieﬂy how to add negation to the lan-
guage. Those ideas are incompatible with the way we deal with communication
delays, since they relied heavily on the fact that the exact time at which facts
arrive is known. Adding negation to the current framework is also a direction
for future research.
Acknowledgements. This work was partially supported by FCT through the LASIGE
Research Unit, ref. UIDB/00408/2020 and ref. UIDP/00408/2020.
References
1. Abiteboul, S., Hull, R., Vianu, V.: Foundations of Databases. Addison-Wesley,
Boston (1995)
2. Akidau, T., et al.: Millwheel: fault-tolerant stream processing at internet scale.
Proc. VLDB Endow. 6(11), 1033–1044 (2013)
3. Alferes, J.J., Moniz Pereira, L. (eds.): Reasoning with Logic Programming. LNCS,
vol. 1111. Springer, Heidelberg (1996). https://doi.org/10.1007/3-540-61488-5
4. Alirezaie, M., Loutﬁ, A.: Automated reasoning using abduction for interpretation
of medical signals. J. Biomed. Semant. 5, 35 (2014)
5. Alur, R., Henzinger, T.A.: Real-time logics: complexity and expressiveness. Inf.
Comput. 104(1), 35–77 (1993)
6. Babcock, B., Babu, S., Datar, M., Motwani, R., Widom, J.: Models and issues in
data stream systems. In: Popa, L., Abiteboul, S., Kolaitis, P.G. (eds.) Proceedings
of PODS, pp. 1–16. ACM (2002)

Can You Answer While You Wait?
127
7. Barbieri, D.F., Braga, D., Ceri, S., Della Valle, E., Grossniklaus, M.: Incremental
reasoning on streams and rich background knowledge. In: Aroyo, L., et al. (eds.)
ESWC 2010. LNCS, vol. 6088, pp. 1–15. Springer, Heidelberg (2010). https://doi.
org/10.1007/978-3-642-13486-9 1
8. Beck, H., Dao-Tran, M., Eiter, T.: Answer update for rule-based stream reasoning.
In: Yang, Q., Wooldridge, M.J. (eds.) Proceedings of IJCAI, pp. 2741–2747. AAAI
Press (2015)
9. Beck, H., Dao-Tran, M., Eiter, T., Fink, M.: LARS: a logic-based framework for
analyzing reasoning over streams. In: Bonet, B., Koenig, S. (eds.) 29th AAAI
Conference on Artiﬁcial Intelligence, AAAI 2015, Austin, TX, USA, pp. 1431–
1438. AAAI Press (2015)
10. Ben-Ari, M.: Mathematical Logic for Computer Science, 3rd edn. Springer, London
(2012). https://doi.org/10.1007/978-1-4471-4129-7
11. Brandt, S., Kalayci, E.G., Ryzhikov, V., Xiao, G., Zakharyaschev, M.: Querying
log data with metric temporal logic. J. Artif. Intell. Res. 62, 829–877 (2018)
12. Ceri, S., Gottlob, G., Tanca, L.: What you always wanted to know about datalog
(and never dared to ask). IEEE Trans. Knowl. Data Eng. 1(1), 146–166 (1989)
13. Chomicki, J., Imielinski, T.: Temporal deductive databases and inﬁnite objects. In:
Edmondson-Yurkanan, C., Yannakakis, M. (eds.) Proceedings of PODS, pp. 61–73.
ACM (1988)
14. Cruz-Filipe, L., Gaspar, G., Nunes, I.: Hypothetical answers to continuous queries
over data streams. CoRR abs/1905.09610 (2019)
15. Cruz-Filipe, L., Gaspar, G., Nunes, I.: Hypothetical answers to continuous queries
over data streams. In: Proceedings of AAAI, pp. 2798–2805. AAAI Press (2020)
16. Cugola, G., Margara, A.: Processing ﬂows of information: from data stream to
complex event processing. ACM Comput. Surv. 44(3), 15:1–15:62 (2012)
17. Dao-Tran, M., Eiter, T.: Streaming multi-context systems. In: Sierra, C. (ed.)
Proceedings of IJCAI, pp. 1000–1007. ijcai.org (2017)
18. Dell’Aglio, D., Valle, E.D., van Harmelen, F., Bernstein, A.: Stream reasoning: a
survey and outlook. Data Sci. 1(1–2), 59–83 (2017)
19. Denecker, M., Kakas, A.: Abduction in logic programming. In: Kakas, A.C., Sadri,
F. (eds.) Computational Logic: Logic Programming and Beyond. LNCS (LNAI),
vol. 2407, pp. 402–436. Springer, Heidelberg (2002). https://doi.org/10.1007/3-
540-45628-7 16
20. Dung, P.M.: Negations as hypotheses: an abductive foundation for logic program-
ming. In: Furukawa, K. (ed.) Proceedings of ICLP, pp. 3–17. MIT Press (1991)
21. Ghanem, T.M., Hammad, M.A., Mokbel, M.F., Aref, W.G., Elmagarmid, A.K.:
Incremental evaluation of sliding-window queries over data streams. IEEE Trans.
Knowl. Data Eng. 19(1), 57–72 (2007)
22. Gray, A.J., Nutt, W., Williams, M.H.: Answering queries over incomplete data
stream histories. IJWIS 3(1/2), 41–60 (2007)
23. Guo, H., Gupta, G.: Dynamic reordering of alternatives for deﬁnite logic programs.
Comput. Lang. Syst. Struct. 35(3), 252–265 (2009)
24. Gupta, A., Mumick, I.S., Subrahmanian, V.: Maintaining views incrementally. In:
Buneman, P., Jajodia, S. (eds.) Proceedings of SIGMOD, pp. 157–166. ACM Press
(1993)
25. Hu, P., Motik, B., Horrocks, I.: Optimised maintenance of datalog materialisations.
In: McIlraith, S.A., Weinberger, K.Q. (eds.) 32nd AAAI Conference on Artiﬁcial
Intelligence, AAAI 2018, New Orleans, LA, USA, pp. 1871–1879. AAAI Press
(2018)

128
L. Cruz-Filipe et al.
26. Inoue, K.: Hypothetical reasoning in logic programs. J. Log. Program. 18(3), 191–
227 (1994)
27. Isah, H., Abughofa, T., Mahfuz, S., Ajerla, D., Zulkernine, F.H., Khan, S.: A survey
of distributed data stream processing frameworks. IEEE Access 7, 154300–154316
(2019)
28. Jr., W.L.: On semantic issues connected with incomplete information databases.
ACM Trans. Database Syst. 4(3), 262–296 (1979)
29. Koymans, R.: Specifying real-time properties with metric temporal logic. Real-
Time Syst. 2(4), 255–299 (1990)
30. Lang, W., Nehme, R.V., Robinson, E., Naughton, J.F.: Partial results in database
systems. In: Dyreson, C.E., Li, F., ¨Ozsu, M.T. (eds.) Proceedings of SIGMOD, pp.
1275–1286. ACM (2014)
31. de Leng, D., Heintz, F.: Approximate stream reasoning with metric temporal logic
under uncertainty. In: 33rd AAAI Conference on Artiﬁcial Intelligence, AAAI 2019,
Honolulu, Hawaii, USA, pp. 2760–2767. AAAI Press (2019)
32. Li, J., Tufte, K., Shkapenyuk, V., Papadimos, V., Johnson, T., Maier, D.: Out-of-
order processing: a new architecture for high-performance stream systems. Proc.
VLDB Endow. 1(1), 274–288 (2008)
33. Li, M., Liu, M., Ding, L., Rundensteiner, E.A., Mani, M.: Event stream processing
with out-of-order data arrival. In: Proceedings of ICDCS, p. 67. IEEE Computer
Society (2007)
34. Libkin, L.: Incomplete data: what went wrong, and how to ﬁx it. In: Hull, R.,
Grohe, M. (eds.) Proceedings of PODS, pp. 1–13. ACM (2014)
35. Lloyd, J.W.: Foundations of Logic Programming. Springer, Heidelberg (1984).
https://doi.org/10.1007/978-3-642-96826-6
36. Meadows, B.L., Langley, P., Emery, M.J.: Seeing beyond shadows: incremental
abductive reasoning for plan understanding. In: Plan, Activity, and Intent Recog-
nition. AAAI Workshops, vol. WS-13-13. AAAI (2013)
37. Motik, B., Nenov, Y., Piro, R.E.F., Horrocks, I.: Incremental update of datalog
materialisation: the backward/forward algorithm. In: Bonet, B., Koenig, S. (eds.)
29th AAAI Conference on Artiﬁcial Intelligence, AAAI 2015, Austin, TX, USA,
pp. 1560–1568. AAAI Press (2015)
38. Nishida, N., Vidal, G.: A framework for computing ﬁnite SLD trees. J. Log. Alge-
braic Methods Program. 84(2), 197–217 (2015)
39. ¨Oz¸cep, ¨O.L., M¨oller, R., Neuenstadt, C.: A stream-temporal query language for
ontology based data access. In: Lutz, C., Thielscher, M. (eds.) KI 2014. LNCS
(LNAI), vol. 8736, pp. 183–194. Springer, Cham (2014). https://doi.org/10.1007/
978-3-319-11206-0 18
40. Razniewski, S., Korn, F., Nutt, W., Srivastava, D.: Identifying the extent of com-
pleteness of query answers over partially complete databases. In: Sellis, T.K.,
Davidson, S.B., Ives, Z.G. (eds.) Proceedings of SIGMOD, pp. 561–576. ACM
(2015)
41. Reiter, R.: Towards a logical reconstruction of relational database theory. In:
Brodie, M.L., Mylopoulos, J., Schmidt, J.W. (eds.) On Conceptual Modelling.
Topics in Information Systems, pp. 191–233. Springer, New York (1984). https://
doi.org/10.1007/978-1-4612-5196-5 8
42. Ronca, A., Kaminski, M., Grau, B.C., Motik, B., Horrocks, I.: Stream reasoning in
temporal datalog. In: McIlraith, S.A., Weinberger, K.Q. (eds.) 32nd AAAI Confer-
ence on Artiﬁcial Intelligence, AAAI 2018, New Orleans, LA, USA, pp. 1941–1948.
AAAI Press (2018)

Can You Answer While You Wait?
129
43. Stonebraker, M., C¸etintemel, U., Zdonik, S.B.: The 8 requirements of real-time
stream processing. SIGMOD Rec. 34(4), 42–47 (2005)
44. Valle, E.D., Ceri, S., van Harmelen, F., Fensel, D.: It’s a streaming world! Reasoning
upon rapidly changing information. IEEE Intell. Syst. 24(6), 83–89 (2009)
45. Walega, P.A., Kaminski, M., Grau, B.C.: Reasoning over streaming data in metric
temporal datalog. In: 33rd AAAI Conference on Artiﬁcial Intelligence, AAAI 2019,
Honolulu, Hawaii, USA, pp. 3092–3099. AAAI Press (2019)
46. Zaniolo, C.: Logical foundations of continuous query languages for data streams.
In: Barcel´o, P., Pichler, R. (eds.) Datalog 2.0 2012. LNCS, vol. 7494, pp. 177–189.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-32925-8 18

The Implication Problem for Functional
Dependencies and Variants of Marginal
Distribution Equivalences
Minna Hirvonen(B)
Department of Mathematics and Statistics, University of Helsinki, Helsinki, Finland
minna.hirvonen@helsinki.fi
Abstract. We study functional dependencies together with two diﬀer-
ent probabilistic dependency notions: unary marginal identity and unary
marginal distribution equivalence. A unary marginal identity states that
two variables x and y are identically distributed. A unary marginal dis-
tribution equivalence states that the multiset consisting of the marginal
probabilities of all the values for variable x is the same as the correspond-
ing multiset for y. We present a sound and complete axiomatization and
a polynomial-time algorithm for the implication problem for the class of
these dependencies, and show that this class has Armstrong relations.
Keywords: Armstrong relations · Complete axiomatization ·
Functional dependence · Marginal distribution equivalence ·
Polynomial-time algorithm · Probabilistic team semantics
1
Introduction
Notions of dependence and independence are central to many areas of science.
In database theory, the study of dependencies (or logical integrity constraints)
is a central topic because it has applications to database design and many other
data management tasks. For example, functional dependencies and inclusion
dependencies are commonly used in practice as primary key and foreign key
constraints. In this paper, we study functional dependencies together with unary
marginal identity and unary marginal distribution equivalence. The latter two
are probabilistic dependency notions that compare distributions of two variables.
A unary marginal identity states that two variables x and y are identically
distributed. A unary marginal distribution equivalence states that the multiset
consisting of the marginal probabilities of all the values for variable x is the
same as the corresponding multiset for y. Marginal identity can actually be
viewed as a probabilistic version of inclusion dependency; it is sometimes called
“probabilistic inclusion” [8].
The author was supported by the Finnish Academy of Science and Letters (the Vilho,
Yrj¨o and Kalle V¨ais¨al¨a Foundation) and by grant 345634 of the Academy of Finland.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 130–146, 2022.
https://doi.org/10.1007/978-3-031-11321-5_8

The Implication Problem for Functional Dependencies and Variants
131
We consider the so-called implication problem for the class of these depen-
dencies. The implication problem for a class of dependencies is the problem of
deciding whether for a given ﬁnite set Σ ∪{σ} of dependencies from the class, any
database that satisﬁes every dependency from Σ also satisﬁes σ. If the databases
are required to be ﬁnite, the problem is called the ﬁnite implication problem.
Otherwise, we speak of the unrestricted implication problem. We axiomatize the
ﬁnite implication problem for functional dependence, unary marginal identity,
and unary marginal distribution equivalence over uni-relational databases that
are complemented with a probability distribution over the set of tuples appearing
in the relation.
The implication problem that we axiomatize contains one qualitative class
(functional dependence) and two probabilistic classes (marginal identity and
marginal distribution equivalence) of dependencies, so we have to consider how
these diﬀerent kinds of dependencies interact. Some probabilistic dependencies
have already been studied separately. The implication problem for probabilistic
independence has been axiomatized over 30 years ago in [12]. More recently,
the implication problem for marginal identity (over ﬁnite probability distribu-
tions) was axiomatized in [16]. The study of joint implication problems for diﬀer-
ent probabilistic and relational atoms have potential for practical applications
because various probabilistic dependency notions appear in many situations.
An example of an interesting class of probabilistic dependencies is probabilis-
tic independencies together with marginal identities. In probability theory and
statistics, random variables are often assumed to be independent and identically
distributed (IID), a property which can be expressed with probabilistic indepen-
dencies and marginal identities. Another example comes from the foundations of
quantum mechanics where functional dependence and probabilistic conditional
independence can be used to express certain properties of hidden-variable models
[1,2].
For practical purposes, it is important to consider implication problems also
from a computational point of view: the usability of a class of dependencies, e.g.
in database design, depends on the computational properties of its implication
problem. We show that the implication problem for functional dependencies,
unary marginal identities, and unary marginal distribution equivalences has a
polynomial-time algorithm.
A class of dependencies is said to have Armstrong relations if for any ﬁnite
set Σ of dependencies from the class, there is a relation that satisﬁes a depen-
dency σ in the class if and only if σ is true for every relation that satisﬁes Σ.
An Armstrong relation can be used as a convenient representation of a depen-
dency set. If a class of dependecies has Armstrong relations, then the implication
problem for a ﬁxed set Σ and an arbitrary dependency σ is reduced to checking
whether the Armstrong relation for Σ satisﬁes σ. When Armstrong axiomatized
functional dependence in [3], he also implicitly proved that the class has Arm-
strong relations [9]. Unfortunately, there are sets of functional dependecies for
which the size of a minimal Armstrong relation is exponential in the number
of variables (attributes) of the set [5]. Armstrong relations can still be useful in
practice [18,19].

132
M. Hirvonen
Sometimes integrity constraints, e.g. inclusion dependencies, are considered
on multi-relational databases. In this case one looks for Armstrong databases [10]
instead of single relations. In this terminology, an Armstrong relation is simply
a uni-relational Armstrong database. Not all classes of dependencies enjoy Arm-
strong databases: functional dependence together with unary inclusion depen-
dence (over multi-relational databases where empty relations are allowed) does
not have Armstrong databases [10]. However, standard functional dependencies
(i.e. functional dependencies with a nonempty left-side) and inclusion dependen-
cies do have Armstrong databases. It is known that probabilistic independence
has Armstrong relations [12]. We show that the class of functional dependencies,
unary marginal identities, and unary marginal distribution equivalences enjoys
Armstrong relations.
Instead of working with notions and conventions from database theory, we
have chosen to formulate the axiomatization in team semantics which is a seman-
tical framework for logics. This is because the dependency notions that we con-
sider can be viewed as certain kinds of atomic formulas in logics developed for
expressing probabilistic dependencies (see the logics studied e.g. in [14]), and we
want to keep this connection1 to these logics explicit. A “team” in team seman-
tics is basically a uni-relational database, so the proofs that we present could
easily also be stated in terms of databases.
Team semantics was originally introduced by Hodges [17]. The systematic
development of the framework began with dependence logic, a logic for functional
dependence introduced by V¨a¨an¨anen [24], and the setting has turned out to be
useful for formulating logics for other dependency notions as well. These logics
include, e.g., inclusion logic [11] and (conditional) independence logic [13]. In
team semantics, logical formulas are evaluated over sets of assignments (called
teams) instead of single assignments as, for example, in ﬁrst-order logic. This
allows us to deﬁne atomic formulas that express dependencies. For example, the
dependency atom =(¯x, ¯y) expresses the functional dependency stating that the
values of ¯x determine the values of ¯y. As mentioned above, a team of assignments
can be thought of as a relation (or a table) in a database: each assignment
corresponds to a tuple in the relation (or a row in the table).
Since we want to study functional dependencies together with probabilistic
dependency notions, we turn to probabilistic team semantics which is a gen-
eralization of the relational team semantics. A probabilistic team is a set of
assignments with an additional function that maps each assignment to some
numerical value, a weight. The function is usually a probability distribution.
As probabilistic team semantics is currently deﬁned only for discrete distribu-
tions that have a ﬁnite number of possible values for variables, we consider the
implication problem only for ﬁnite teams.
Although some probabilistic dependencies might seem similar to their quali-
tative variants, their implication problems are diﬀerent: probabilistic dependen-
cies refer to the weights of the rows rather than the rows themselves. Probabilistic
dependencies can be tricky, especially if one considers two or more diﬀerent vari-
1 Our axiomatization is obviously only for the atomic level of these logics.

The Implication Problem for Functional Dependencies and Variants
133
ants together. For example, consider marginal identity together with probabilis-
tic independence. The chase algorithm that was used for proving the complete-
ness of the axiomatization of marginal identity in [16] uses inclusion dependencies
that contain index variables for counting multiplicities of certain tuples. There
does not seem to be a simple way of extending this procedure to also include
probabilistic independencies. This is because adding a new row aﬀects the prob-
ability distribution and often breaks existing probabilistic independencies. On
the other hand, the approach that was used for probabilistic independencies in
[12] cannot easily be generalized to also cover inclusion dependencies either.
Luckily, in our case we can utilize the implication problem for functional
dependencies and unary inclusion dependencies which has been axiomatized in
[7] for both ﬁnite and unrestricted implication. The axiomatization for ﬁnite
implication is proved to be complete by constructing relations with the help of
multigraphs that depict the dependecies between variables. This approach has
also been applied to unary functional dependence, unary inclusion dependence,
and independence [15]. Our approach is similar, but since we are working in the
probabilistic setting, we have to be careful that our construction also works with
the two kinds of variants of unary marginal distribution equivalences.
2
Preliminaries
Let D be a ﬁnite set of variables and A a ﬁnite set of values. We usually denote
variables by x, y, z and values by a, b, c. Tuples of variables and tuples of values
are denoted by ¯x, ¯y, ¯z and ¯a,¯b, ¯c, respectively. The notation |¯x| means the length
of the tuple ¯x, and var(¯x) means the set of variables that appear in the tuple ¯x.
An assignment of values from A for the set D is a function s: D →A. A
team X of A over D is a ﬁnite set of assignments s: D →A. When the variables
of D are ordered in some way, e.g. D = {x1, . . . , xn}, we identify the assignment
s with the tuple s(x1, . . . , xn) = s(x1) . . . s(xn) ∈An, and also explicitly call s
a tuple. A team X over D = {x1, . . . , xn} can then be viewed as a table whose
columns are the variables x1, . . . , xn, and rows are the tuples s ∈X. For any
tuple of variables ¯x from D, we let
X(¯x) := {s(¯x) ∈A|¯x| | s ∈X}.
A probabilistic team X is a function X: X →(0, 1] such that ∑s∈X(s) = 1. For
a tuple of variables ¯x from D and a tuple of values ¯a from A, we let
|X¯x=¯a| := ∑
s(¯x)=¯a
s∈X
X(s).
Let ¯x, ¯y be tuples of variables from D. Then =(¯x, ¯y) is a (functional) dependency
atom. If the tuples ¯x, ¯y are of the same length, then ¯x ≈¯y and ¯x ≈∗¯y are
marginal identity and marginal distribution equivalence atoms, respectively. We
also use the abbreviations FD (functional dependency), MI (marginal identity),

134
M. Hirvonen
and MDE (marginal distribution equivalence) for the atoms. If |¯x| = |¯y| = 1, an
atom is called unary and abbreviated by UFD, UMI, or UMDE.
Before deﬁning the semantics for the atoms, we need to introduce the notion
of a multiset. A multiset is a pair (B, m) where B is a set, and m: B →N is
a multiplicity function. The function m determines for each element b ∈B how
many multiplicities of b the multiset (B, m) contains. We often denote multisets
using double wave brackets, e.g., (B, m) = {{0, 1, 1}} when B = {0, 1} and
m(n) = n + 1 for all n ∈B.
Let σ be either a dependency, a marginal identity, or a marginal distribu-
tion equivalence atom. The notation X |= σ means that a probabilistic team X
satisﬁes σ, which is deﬁned as follows:
(i) X |= =(¯x, ¯y) iﬀfor all s, s′ ∈X, if s(¯x) = s′(¯x), then s(¯y) = s′(¯y).
(ii) X |= ¯x ≈¯y iﬀ|X¯x=¯a| = |X¯y=¯a| for all ¯a ∈A|¯x|.
(iii) X |= ¯x ≈∗¯y iﬀ{{|X¯x=¯a| | ¯a ∈X(¯x)}} = {{|X¯y=¯a| | ¯a ∈X(¯y)}}.
An atom =(¯x, ¯y) is called a functional dependency, because X |= =(¯x, ¯y) iﬀthere
is a function f : X(¯x) →X(¯y) such that f(s(¯x)) = s(¯y) for all s ∈X. An FD
of the form =(λ, x), where λ is the empty tuple, is called a constant atom and
denoted by =(x). Intuitively, the constant atom =(x) states that variable x is
constant in the team. The atom ¯x ≈¯y states that the tuples ¯x and ¯y give rise
to identical distributions. The meaning of the atom ¯x ≈∗¯y is similar but allows
the marginal probabilities to be attached to diﬀerent tuples of values for ¯x and
¯y.
Let Σ ∪{σ} be a set of dependency, marginal identity, and marginal distri-
bution equivalence atoms. We write X |= Σ iﬀX |= σ′ for all σ′ ∈Σ, and we
write Σ |= σ iﬀX |= Σ implies X |= σ for all X.
3
Axiomatization for FDs+UMIs+UMDEs
In this section, we present an axiomatization for dependency, unary marginal
identity, and unary marginal distribution equivalence atoms. The axiomatization
is inﬁnite.
3.1
The Axioms
The axioms for unary marginal identity and unary marginal distribution equiv-
alence are the equivalence axioms of reﬂexivity, symmetry, and transitivity:
UMI1: x ≈x
UMI2: If x ≈y, then y ≈x.
UMI3: If x ≈y and y ≈z, then x ≈z.
UMDE1: x ≈∗x
UMDE2: If x ≈∗y, then y ≈∗x.
UMDE3: If x ≈∗y and y ≈∗z, then x ≈∗z.

The Implication Problem for Functional Dependencies and Variants
135
For functional dependencies, we take the Armstrong axiomatization [3] which
consists of reﬂexivity, transitivity, and augmentation:
FD1: =(¯x, ¯y) when var(¯y) ⊆var(¯x).
FD2: If =(¯x, ¯y) and =(¯y, ¯z), then =(¯x, ¯z).
FD3: If =(¯x, ¯y), then =(¯x¯z, ¯y¯z).
Since marginal identity is a special case of marginal distribution equivalence, we
have the following axiom:
UMI & UMDE: If x ≈y, then x ≈∗y.
For the unary functional dependencies and unary marginal distribution equiv-
alencies, we have the cycle rule:
If =(x0, x1) and x1 ≈∗x2 and . . . and =(xk−1, xk) and xk ≈∗x0,
then =(x1, x0) and . . . and =(xk, xk−1) and x0 ≈∗x1 and . . . and xk−1 ≈∗xk
for all k ∈{1, 3, 5, . . . }. Note that the cycle rule is not a single axiom but
an inﬁnite axiom schema, making our axiomatization inﬁnite. The following
inference rule is as a useful special case of the cycle rule
UMDE & FD: If =(x, y) and =(y, x), then x ≈∗y.
This can be seen by noticing that by the cycle rule
=(x, y) and y ≈∗y and =(y, x) and x ≈∗x
implies x ≈∗y.
From here on, the “inference rules” refers to the axioms deﬁned above. Let
Σ ∪{σ} be a set of dependency, marginal identity, and a marginal distribution
equivalence atoms. We write Σ ⊢σ iﬀσ can be derived from Σ by using the
inference rules. We denote by cl(Σ) the set of atoms obtained by closing Σ under
the inference rules, i.e., for all σ with variables from D, σ ∈cl(Σ) iﬀΣ ⊢σ.
We say that an axiomatization is sound if for any set Σ∪{σ} of dependencies,
Σ ⊢σ implies Σ |= σ. Correspondingly, an axiomatization is complete if for any
set Σ ∪{σ} of dependencies, Σ |= σ implies Σ ⊢σ.
3.2
Soundness of the Axioms
In this section, we show that our axiomatization is sound. We only show that
the cycle rule is sound; since it is straightforward to check the soundness of other
the axioms, we leave out their proofs.
We notice that for all x, y ∈D, X |= =(x, y) implies |X(x)| ≥|X(y)| and
X |= x ≈∗y implies |X(x)| = |X(y)|. Suppose now that the precedent of the
cycle rule holds for X. It then follows that
|X(x0)| ≥|X(x1)| = |X(x2)| ≥· · · = |X(xk−1)| ≥|X(xk)| = |X(x0)|,

136
M. Hirvonen
which implies that |X(xi)| = |X(xj)| for all i, j ∈{0, . . . k}. If i, j are additionally
such that X |= =(xi, xj), then there is a surjective function f : X(xi) →X(xj)
for which f(s(xi)) = s(xj) for all s ∈X. Since X(xi) and X(xj) are both
ﬁnite and have the same number of elements, the function f is also one-to-one.
Therefore the inverse of f is also a function, and we have X |= =(xj, xi), as
wanted. Since f is bijective, there is a one-to-one correspondence between X(xi)
and X(xj). Thus |Xxi=a| = |Xxj=f(a)| for all a ∈X(xi), and we have {{|Xxi=a| |
a ∈X(xi)}} = {{|Xxj=a| | a ∈X(xj)}}, which implies that X |= xi ≈∗xj.
4
Completeness and Armstrong Relations
In this section, we show that our axiomatization is complete. We do this by
showing that for any set Σ of FDs, UMIs, and UMDEs, there is a probabilistic
team X such that X |= σ iﬀσ ∈cl(Σ). Note that proving this implies complete-
ness: if σ ̸∈cl(Σ) (i.e. Σ ̸⊢σ), then X ̸|= σ. Since X |= Σ, we have Σ ̸|= σ. By
doing the proof this way, we obtain Armstrong relations because the constructed
team X is an Armstrong relation2 for Σ.
We ﬁrst deﬁne a multigraph in which diﬀerent-colored edges correspond to
diﬀerent types of dependencies between variables. By using the properties of
the graph, we can construct a suitable team, which can then be made into a
probabilistic team by taking the uniform distribution over the assignments.
Deﬁnition 1. Let Σ be a set of FDs, UMIs, and UMDEs. We deﬁne a multi-
graph G(Σ) as follows:
(i) the set of vertices consists of the variables appearing in Σ,
(ii) for each marginal identity x ≈y ∈Σ, there is an undirected black edge
between x and y,
(iii) for each marginal distribution equivalence x ≈∗y ∈Σ, there is an undirected
blue edge between x and y, and
(iv) for each functional dependency =(x, y) ∈Σ, there is a directed red edge
from x to y.
If there are red directed edges both from x to y and from y to x, they can be
thought of as a single red undirected edge between x and y.
In order to use the above graph construction to show the existence of Arm-
strong relations, we need to consider certain properties of the graph.
Lemma 2. Let Σ be a set of FDs, UMIs and UMDEs that is closed under
the inference rules, i.e. cl(Σ) = Σ. Then the graph G(Σ) has the following
properties:
2 Note that a probabilistic team is actually not a relation but a probability distri-
bution. Therefore, to be exact, instead of Armstrong relations, we should speak of
Armstrong models, which is a more general notion introduced in [9]. In our setting,
the Armstrong models we construct are uniform distributions over a relation, so each
model is determined by a relation, and it suﬃces to speak of Armstrong relations.

The Implication Problem for Functional Dependencies and Variants
137
(i) Every vertex has a black, blue, and red self-loop.
(ii) The black, blue, and red subgraphs of G(Σ) are all transitively closed.
(iii) The black subgraph of G(Σ) is a subgraph of the blue subgraph of G(Σ).
(iv) The subgraphs induced by the strongly connected components of G(Σ) are
undirected. Each such component contains a black, blue, and red undirected
subgraph. In the subgraph of each color, the vertices of the component can
be partitioned into a collection of disjoint cliques. All the vertices of the
component belong to a single blue clique.
(v) If =(¯x, y) ∈Σ and the vertices ¯x have a common ancestor z in the red
subgraph of G(Σ), then there is a red edge from z to y.
Proof. (i) By the reﬂexivity rules UMI1, UMDE1, and FD1, atoms x ≈x,
x ≈∗x, and =(x, x) are in Σ for every vertex x.
(ii) The transitivity rules UMI3, UMDE3, and FD2 ensure the transitivity of
the black, blue, and red subgraphs, respectively.
(iii) By the UMI & UMDE rule, we have that if there is a black edge between
x and y, then there is also a blue edge between x and y.
(iv) In a strongly connected component every black or blue edge is undirected
by the deﬁnition of G(Σ). Consider then a red edge from x to y. Since we
are in a strongly connected component, there is also a path from y to x. By
(iii), we may assume that this path only consist of blue and red edges. By
adding the red edge from x to y to the beginning of the path, we obtain a
cycle from x to x. (Note that in this cycle, some vertices might be visited
more than once.) By using the part (i), we can add blue and red self-loops, if
necessary, to make sure that the cycle is constructed from alternating blue
and red edges as in the cycle rule. From the corresponding k-cycle rule, it
then follows that there is also a red edge from y to x.
The existence of a partition follows from the fact that in each strongly
connected component the black/blue/red edges deﬁne an equivalence rela-
tion for the vertices. (The reﬂexivity and transitivity follow from parts (i)
and (ii), and the fact that each strongly connected component is undirected
implies symmetry.) Lastly, pick any vertices x and y in a strongly connected
component. We claim that there is a blue edge between x and y. Since we
are in a strongly connected component, there is a path from x to y. The
path consists of undirected black, blue and red edges. By (iii), each black
edge of the path can be replaced with a blue one. Similarly, by the rule
UMDE & FD, each undirected red edge can be replaced with a blue one.
Thus, there is a blue path from x to y, and, by transitivity, also a blue edge.
(v) Let ¯x = x0 . . . xn. Suppose that we have =(z, x0 . . . xk) and =(z, xk+1) for
some 0 ≤k ≤n −1. By FD1 and FD3, we have =(z, zz), =(zz, xk+1z), and
=(xk+1z, zxk+1). Then by using FD2 twice, we obtain =(z, zxk+1). Next,
by FD3, we also have =(zxk+1, x0 . . . xkxk+1), and then by using FD2, we
obtain =(z, x0 . . . xk+1). Since we have =(z, xj) for all 0 ≤j ≤n, this shows
that =(z, ¯x). By using FD2 to =(z, ¯x) and =(¯x, y), we obtain =(z, y).
We next give a unique number to each strongly connected component of
G(Σ). The numbers are assigned such that the number of a descendant com-

138
M. Hirvonen
ponent is always greater that the number of its ancestor component. We call
these numbers scc-numbers and denote by scc(x) the scc-number of the strongly
connected component that the vertex x belongs to.
Deﬁnition 3. Let Σ and G(Σ) be as in Lemma 2 and assign an scc-numbering
to G(Σ). Let D be the set of the variables appearing in Σ. We construct a team
X over the variables D as follows:
(i) Add a tuple of all zeroes, i.e., an assignment s such that s(x) = 0 for all
x ∈D.
(ii) Process each strongly connected component in turn, starting with the one
with the smallest scc-number and then proceeding in the ascending order
of the numbers. For each strongly connected component, handle each of its
maximal red cliques in turn.
(a) For each maximal red clique k, add a tuple with zeroes in the columns
corresponding to the variables in k and to the variables that are in any
red clique that is a red descendant of k. Leave all the other positions in
the tuple empty for now.
(b) Choose a variable in k and count the number of zeroes in a column corre-
sponding to the chosen variable. It suﬃces to consider only one variable,
because the construction ensures that all the columns corresponding to
variables in k have the same number of zeroes. Denote this number by
count(k).
After adding one tuple for each maximal red clique, check that the count(k)
is equal for every clique k in the current component and strictly greater than
the count of each clique in the previous component. If it is not, repeat some
of the tuples added to make it so. This can be done, because a red clique
k can be a red descendant of another red clique j only if j is in a strongly
connected component with a strictly smaller scc-number than the one of
the component that k is in, and thus j’s component is already processed.
Note that the counts of the cliques in a component do not change after the
component has been processed.
(iii) The last component is a single red clique consisting of those variables x for
which =(x) ∈Σ, if any. Each variable in this clique functionally depends on
all the other variables in the graph, so we do not leave any empty positions
in its column. Therefore the columns corresponding to these variables con-
tain only zeroes. If there are no variables x for which =(x) ∈Σ, we ﬁnish
processing the last component by adding one tuple with all positions empty.
(iv) After all strongly connected components have been processed, we start ﬁlling
the empty positions. Process again each strongly connected component in
turn, starting with the one with the smallest scc-number. For each strongly
connected component, count the number of maximal black cliques. If there
are n such cliques, number them from 0 to n−1. Then handle each maximal
black clique k, 0 ≤k ≤n −1 in turn.
(a) For each column in clique k, count the number of empty positions. If the
column has d > 0 empty positions, ﬁll them with numbers 1, . . . d−1, d+k
without repetitions. (Note that each column in k has the same number
of empty positions.)

The Implication Problem for Functional Dependencies and Variants
139
(v) If there are variables x for which =(x) ∈Σ, they are all in the last com-
ponent. The corresponding columns contain only zeroes and have no empty
positions. As before, count the number of maximal black cliques. If there are
n such cliques, number them from 0 to n −1. Then handle each maximal
black clique k, 0 ≤k ≤n −1 in turn.
(a) For each column in clique k, change all the zeroes into k’s.
After handling all the components (including the one consisting of constant
columns, if there are any), there are no empty positions anymore, and the
construction is ﬁnished.
The next lemma shows that team we constructed above already has many of the
wanted properties:
Lemma 4. Let Σ, G(Σ), and X be as in Deﬁnition 3. Deﬁne a probabilistic
team X as the uniform distribution over X. Then the following statements hold:
(i) For any nonunary functional dependency σ, if σ ∈Σ, then X |= σ.
(ii) For any unary functional dependency or constancy atom σ, X |= σ if and
only if σ ∈Σ.
(iii) For any unary marginal distribution equivalence atom σ, X |= σ if and only
if σ ∈Σ.
(iv) For any unary marginal identity atom σ, X |= σ if and only if σ ∈Σ.
Proof. (i) Suppose that σ = =(¯x, ¯y) ∈Σ. We may assume that ¯x,¯y do not
contain variables z for which =(z) ∈Σ, as it is easy to show that X |= =(¯x, ¯y)
implies X |= =(¯xz, ¯y) and X |= =(¯x, ¯yz) for any such variable z. For a
contradiction suppose that X ̸|= σ. Then there are tuples s, s′ that violate
σ. By our assumption, the only repeated number in each column of variables
¯x,¯y is 0. Thus s(¯x) = s′(¯x) = ¯0, and either s(¯y) ̸= ¯0 or s′(¯y) ̸= ¯0. Assume
that s(¯y) ̸= ¯0. Then there is y0 ∈var(¯y) such that s(y0) ̸= 0. By our
construction, tuple s corresponds to a red clique. This means that in s,
each variable that is 0, is functionally determined by every variable of the
corresponding red clique. Since Σ is closed under FD rules, there is a variable
z such that s(z) = 0 and =(z, ¯x) ∈Σ. Then also =(z, y0) ∈Σ, and thus, by
the construction, s(y0) = 0, which is a contradiction.
(ii) Suppose that σ = =(x). If σ ∈Σ, then x is determined by all the other
variables in Σ. This means that x is in the last component (the one with
the highest scc-number), and thus the only value appearing in column x is
k, where k is the number assigned to the maximal black clique of x. Hence,
we have X |= σ. If σ ̸∈Σ, then either x is not in the last component or
there are no variables y for which =(y) ∈Σ. In either case, we have at some
point added a tuple in which the position of x is empty, and therefore there
are more then one value appearing in column x, i.e., X ̸|= σ.
Suppose then that σ = =(x, y). We may assume that neither =(x) nor =(y)
are in Σ. If =(y) ∈Σ, then y is constant, and we have σ ∈Σ and X |= σ.
If =(x) ∈Σ and =(y) ̸∈Σ, then σ ̸∈Σ and X ̸|= σ. If σ ∈Σ, we are done
since this is a special case of (i). If σ ̸∈Σ, then the ﬁrst tuple and the tuple
that was added for the red clique of x violate σ.

140
M. Hirvonen
(iii) Since X is obtained by taking the uniform distribution over X, X |= x ≈∗y
holds if and only if |X(x)| = |X(y)|. The latter happens if and only if x
and y are in the same strongly connected component. Since any strongly
connected component is itself the maximal blue clique, this is equivalent
to x and y being in the same maximal blue clique in a strongly connected
component, i.e., x ≈∗y ∈Σ.
(iv) Since X is obtained by taking the uniform distribution over X, X |= x ≈y
holds if and only if X(x) = X(y). The latter happens if and only if x and y
are in the same maximal black clique in a strongly connected component.
This happens if and only if x ≈y ∈Σ.
Example 1. Let Δ = {=(x0, x1), x1 ≈x2, =(x2, x3), x3 ≈∗x0, =(x4), =(x5)}, and
Σ = cl(Δ). Then the strongly connected components of G(Σ) are {x0, x1, x2, x3}
and {x4, x5}, the maximal red cliques are {x0, x1}, {x2, x3}, and {x4, x5}, the
maximal blue cliques are {x0, x1, x2, x3} and {x4, x5} and the maximal black
cliques are {x0}, {x1, x2}, {x3}, {x4}, and {x5}. The probabilistic team X for Σ
(as in Lemma 4) and the graph G(Σ) are depicted in Fig. 1.
The construction of Lemma 4 does not yet give us Armstrong relations.
This is because there might be nonunary functional dependencies σ such that
X |= σ even though σ ̸∈Σ. In the lemma below, we show how to construct a
probabilistic team for which X |= σ implies σ ∈Σ also for nonunary functional
dependencies σ.
Lemma 5. Let Σ and G(Σ) be as in Lemma 2. Then there exists a probabilistic
team X such that X |= σ if and only if σ ∈Σ.
Proof. Let D be the set of the variables appearing in Σ. We again begin by con-
structing a relational team X over D, but this time we modify the construction
from Deﬁnition 3 by adding a new step (0), which will be processed ﬁrst, before
continuing with step (i). From step (i), we will proceed as in Deﬁnition 3. Let
Σ′ be a set of all nonunary functional dependencies =(¯x, ¯y) ̸∈Σ where ¯x, ¯y ∈D.
The step (0) is then deﬁned as follows:
Fig. 1. (a) the probabilistic team X and (b) the graph G(Σ) of Example 1. For the sake
of clarity, we have removed from G(Σ) all self-loops and some edges that are implied
by transitivity.

The Implication Problem for Functional Dependencies and Variants
141
(0) For each C ⊆D with |C| ≥2, check if there is =(¯x, ¯y) ∈Σ′ such that
var(¯x) = C. If yes, add a tuple with zeroes in exactly those positions z that
are functionally determined by ¯x, i.e., those z for which =(¯x, z) ∈Σ. Leave
all the other positions in the tuple empty for now.
As before, we deﬁne the probabilistic team X as the uniform distribution over
X. We ﬁrst show that X violates all atoms from Σ′. Let σ = =(¯x, ¯y) ∈Σ′. We
may assume that ¯x,¯y do not contain variables z for which =(z) ∈Σ because
X ̸|= =(¯x, ¯y) implies X ̸|= =(¯xz, ¯y) and X ̸|= =(¯x, ¯yz) for any such variable z.
Since σ ̸∈Σ, there is y0 ∈var(¯y) such that y0 is not determined by ¯x, i.e.,
=(¯x, y0) ̸∈Σ. Thus, the tuple that was added for the set C = var(¯x) in step (0),
and the tuple added in step (i), agree on variables ¯x (they are all zeroes), but in
the ﬁrst tuple, variable y0 is nonzero and in the other tuple, y0 is zero. Hence,
we have X ̸|= σ.
We still need to show that X |= Σ, and that σ ̸∈Σ implies X ̸|= σ, when σ
is a UMI, UMDE, UFD or a constancy atom. Note that since the construction
proceeds from step (i) as in Deﬁnition 3, it ensures that the counts of the empty
positions are the same for all the columns that correspond to the variables in the
same strongly connected component. Therefore, for any unary marginal identity
or distribution equivalence atom σ, X |= σ if and only if σ ∈Σ as shown in
Lemma 4, parts (iii) & (iv). For a unary functional dependency, or a constancy
atom σ ̸∈Σ, we obtain, by Lemma 4, part (ii), that already the part of the team
X that was constructed as in Deﬁnition 3 contains tuples that violate σ.
If =(z) ∈Σ, then clearly =(¯x, z) ∈Σ for any ¯x, and thus adding the tuples in
step (0) does not end up violating =(z). Suppose then that =(¯z, ¯z′) ∈Σ, where
¯z and ¯z′ can also be single variables. For a contradiction, suppose there are
two tuples that together violate =(¯z, ¯z′). We may again assume that ¯z,¯z′ do not
contain variables u for which =(u) ∈Σ. By our assumption, the only repeated
numbers in columns ¯z are zeroes, so it has to be the case that the variables ¯z
are all zeroes in the two tuples that violate =(¯z, ¯z′). We now show that in any
such tuple, the variables ¯z′ are also all zeroes, contradicting the assumption that
there are two tuples that agree on ¯z but not on ¯z′.
Suppose that the tuple was added on step (0). By the construction, we then
have some variables ¯x, ¯y such that |¯x| ≥2, =(¯x, ¯y) ∈Σ′, and =(¯x, ¯z) ∈Σ. Since
=(¯z, ¯z′) ∈Σ, by transitivity, we have =(¯x, ¯z′) ∈Σ, and thus the variables ¯z′
are also all zeroes in the tuple, as wanted. Suppose then that the tuple was not
added in step (0), i.e., it was added for some maximal red clique in a strongly
connected component. Suppose then that there is z0 ∈var(¯z′) which is not zero
in the tuple. This leads to a contradiction as shown in Lemma 4 part (i). Hence,
the tuple has to have all zeroes for variables ¯z′.
5
Complexity of the Implication Problem
for FDs+UMIs+UMDEs
Let Σ ∪{σ} be a set of functional dependencies, unary marginal identity, and
unary marginal distribution equivalence atoms. The decision problem of checking

142
M. Hirvonen
whether Σ |= σ is called the implication problem for FDs+UMIs+UMDEs. We
show that this implication problem has a polynomial-time algorithm. Denote by
D the set of the variables appearing in Σ∪{σ}, and suppose that Σ is partitioned
into the sets ΣFD, ΣUMI, and ΣUMDE that consist of the FDs, UMIs, and UMDEs
from Σ, respectively.
First, note that if σ is a UMI, it suﬃces to check whether σ ∈cl(ΣUMI)
as the inference rules for FDs and UMDEs do not produce new UMIs. Notice
that ΣUMI can be viewed as an undirected graph G = (D, ≈) such that each
x ≈y ∈ΣUMI means there is an undirected edge between x and y. (We may
assume that each vertex has a self-loop and all the edges are undirected, even
though the corresponding UMIs might not be explicitly listed in ΣUMI.) Suppose
that σ = x ≈y. Then σ ∈cl(ΣUMI) iﬀy is reachable from x in G. This is the
problem of undirected s-t connectivity, which is known to be L-complete [20].
Thus, we can assume that σ is an FD or a UMDE. The idea behind the
following part of the algorithm is similar to the algorithm (introduced in [7]) for
the implication problem for FDs and unary inclusion dependencies.
The implication problem for FDs is known to be linear-time computable by
the Beeri-Bernstein algorithm [4]. Given a set Δ ∪{=(¯x, ¯y)} of FDs, the Beeri-
Bernstein algorithm computes the set fdclosure(¯x, Δ) = {z | Δ |= =(¯x, z)}. Then
Δ |= =(¯x, ¯y) holds iﬀvar(¯y) ⊆fdclosure(¯x, Δ). The Beeri-Bernstein algorithm
keeps two lists: fdlist(¯x) and attrlist(¯x). The set fdlist(¯x) consists of FDs and is
updated in order to keep the algorithm in linear-time (for more details, see [4]).
The set attrlist(¯x) lists the variables that are found to be functionally dependent
on ¯x. The algorithm3 is as follows:
(i) Initialization:
Let fdlist(¯x) = Δ and attrlist(¯x) = var(¯x).
(ii) Repeat the following until no new variables are added to attrlist(¯x):
(a) For all z ∈attrlist(¯x), if there is an FD =(u0 . . . uk, ¯u′) ∈fdlist(¯x)
such that z = ui for some i = 0, . . . , k, replace =(u0 . . . uk, ¯u′) with
=(u0 . . . ui−1ui+1 . . . uk, ¯u′).
(b) For each constant atom =(¯u) ∈fdlist(¯x), add var(¯u) to attrlist(¯x).
(ii) Return attrlist(¯x).
Let x ∈D, and deﬁne desclist(x) = {y ∈D | Σ |= =(x, y)}. We will construct
an algorithm, called Closure(x, Σ), that computes the set desclist(x) by utilizing
the Beeri-Bernstein algorithm. If we view the variables D and the atoms from
cl(Σ) as a multigraph of Deﬁnition 1, the set desclist(x) consists of the variables
that are red descendants of x. Since FDs and UMIs interact only via UMDEs,
there is no need to construct the whole multigraph. We only consider the UMIs
in the initialization step, as they imply UMDEs. The algorithm Closure(x, Σ)
keeps and updates fdlist(y) and attrlist(y) for each y ∈D. When the algorithm
halts, attrlist(x) = desclist(x). The algorithm is as follows:
(i) Initialization:
3 This presentation of the algorithm is based on [7].

The Implication Problem for Functional Dependencies and Variants
143
(a) For each variable y ∈D, attrlist(y) is initialized to {y} and fdlist(y)
to ΣFD. We begin by running the Beeri-Bernstein algorithm for each
y ∈D. The algorithm adds to attrlist(y) all variables z ∈D that are
functionally dependent on y under ΣFD. Note that each fdlist(y) is also
modiﬁed.
(b) We then construct a multigraph G similarly as before, but the black
edges are replaced with blue ones, i.e. the set of vertices is D, and there
is a directed red edge from y to z iﬀz ∈attrlist(y), and there is an
undirected blue edge between y and z iﬀat least one of the following
y ≈z, z ≈y, y ≈∗z, z ≈∗y is in ΣUMI ∪ΣUMDE.
(ii) Repeat the following iteration until halting and then return attrlist(x):
(1) Ignore the colors of the edges, and ﬁnd the strongly connected compo-
nents of G.
(2) For any y and z that are in the same strongly connected component, if
there is a red directed edge from y to z, check whether there is a red
directed edge from z to y and a blue undirected edge between y and z.
If either or both are missing, add the missing edges.
(3) If no new red edges were added in step (2), then halt. Otherwise, add
all the new red edges to fdlist(y).
(4) For each y, continue the Beeri-Bernstein algorithm.
(5) If for all y, no new variables were added to attrlist(y) in step (4), then
halt. Otherwise, for each new z ∈attrlist(y), add to G a directed red
edge from y to z.
Let n = |D|. Since we can add at most n2 new red edges to the graph,
|fdlist(y)| ≤|ΣFD| + O(n2) for each y ∈D. Thus the Beeri-Bernstein algorithm
takes O(|ΣFD| + n2) time for each y ∈D. Because we run the algorithm for all
y ∈D, the total time is O(n(n2 + |ΣFD|)).
In each iteration, before running the Beeri-Bernstein algorithm (step (4)), we
ﬁnd the strongly connected components (step (1)) and check all pairs of vertices
for red edges and add red and blue edges if needed (step (2)). The time required
for ﬁnding the strongly connected components is linear in the number of vertices
and edges [23], so it takes O(|D| + |ΣFD| + n2 + |ΣUMI| + |ΣUMDE|), i.e. O(n2)
time. Going through the pairs of vertices in step (2) also takes O(n2) time. Thus
each iteration takes O(n3 + n|ΣFD|) time in total.
An iteration proceeds to step (4) only if new red edges are added in step (2).
In each iteration, if new red edges are added, they merge together two or more
red cliques in the same strongly connected component. In the beginning we have
at most n such cliques, so the algorithm cannot take more than O(n) iterations.
The total running time of Closure(x, Σ) is then O(n4 + n2|ΣFD|).
If σ = =(x, y), then Σ |= σ iﬀy ∈desclist(x). If σ is a non-unary FD, in
order to check whether Σ |= σ, it suﬃces to check whether ΣFD ∪Δ |= σ, where
Δ = {=(y, ¯z) | y ∈D, var(¯z) = desclist(y)}. This, again, can be done using the
Beeri-Bernstein algorithm. Since the order of the variables var(¯z) in the tuple ¯z
does not matter for each FD =(y, ¯z), we may assume that the set Δ contains only
one FD for each y ∈D. Thus this step only adds O(|ΣFD|+n) extra time. (Note

144
M. Hirvonen
that constructing the set Δ does not aﬀect the time-bound because Closure(x, Σ)
actually computes desclist(y) for all y ∈D.)
If σ = x ≈∗y, we can use the blue (undirected) subgraph of G to check
whether Σ |= σ. After the algorithm Closure(x, Σ) has halted, we only need to
check whether y is reachable from x in the subgraph, which is an L-complete
problem, as noted before.
6
Conclusion
We have presented a sound and complete axiomatization for functional depen-
dence, unary marginal identity, and unary marginal distribution equivalence. The
class of these dependencies was also shown to have Armstrong relations because
in the completeness proof, we constructed an Armstrong relation for any set of
FDs, UMIs, and UMDEs. We also showed that the implication problem for these
dependencies is in polynomial-time.
The following questions remain open:
(i) Can we extend the axiomatization to nonunary marginal identity and
marginal distribution equivalence?
(ii) What is the connection between the implication problem considered in this
paper and the ﬁnite implication problem for functional dependence and
unary inclusion dependence?
(iii) Can we ﬁnd an axiomatization for probabilistic independence and marginal
identity (and marginal distribution equivalence)?
Concerning the ﬁrst question, one should not rush to conjectures based on
the results in the qualitative (non-probabilistic) case. Our results for functional
dependencies and unary variants of marginal identities and marginal distribution
equivalences were obtained by methods that resemble the ones used in the case
of functional dependence and unary inclusion dependence in [7]. The implication
problem for functional and inclusion dependencies (where also nonunary inclu-
sions are allowed) is undecidable for both ﬁnite and unrestricted implication [6],
so one might think that allowing nonunary marginal identities with functional
dependencies makes the corresponding (ﬁnite) implication problem undecidable.
However, it is not clear whether this is the case, as the reduction proof for the
undecidability result does not directly generalize to the probabilistic case.
The second question also relates to how similar the qualitative version (inclu-
sion dependence) is to the probabilistic version (marginal identity). It seems that
the implication problem for functional dependencies and unary marginal identi-
ties (without unary marginal distribution equivalencies) can be simulated with
the implication problem for functional dependencies and unary inclusion depen-
decies, by simulating each marginal identity x ≈y with two inclusion atoms
(x ⊆y and y ⊆x). It would be interesting to see whether the full implica-
tion problem (i.e. the one with unary marginal equivalencies as well) can also
be simulated with the implication problem for functional and unary inclusion
dependence.

The Implication Problem for Functional Dependencies and Variants
145
As for the third question, the implication problems for probabilistic indepen-
dence and marginal identity have already been studied separately: the problem
for probabilistic independence has been axiomatized in [12], and the problem
for marginal identity (over ﬁnite probability distributions) has recently been
axiomatized in [16]. Obtaining an axiomatization for both of these dependencies
together is appealing, given how commonly IID assumption is used in probability
theory and statistics.
In addition to probabilistic independence, we can consider probabilistic con-
ditional independence ¯y ⊥⊥¯x ¯z which states that ¯y and ¯z are conditionally inde-
pendent given ¯x. A probabilistic conditional independency of the form ¯y ⊥⊥¯x ¯y4
is equivalent with the functional dependency =(¯x, ¯y), so functional dependence
can be seen as a special case of probabilistic conditional independence. Prob-
abilistic conditional independence has no ﬁnite complete axiomatization [21],
although sound (but not complete) axiomatizations, the so-called graphoid and
semigraphoid axioms, exist and are often used in practice. Every sound inference
rule for probabilistic conditional independence that has at most two antecedents
can be derived from the semigraphoid axioms [22]. It would be interesting to
ﬁnd axiomatizations for marginal identity and some subclasses of probabilistic
conditional independence. The results of this paper imply that if we consider
a subclass that contains probabilistic conditional independencies of the form
¯y ⊥⊥¯x ¯y, we eﬀectively include functional dependencies and thus we will also
have the cycle rules.
Acknowledgements. I would like to thank the anonymous referees for valuable com-
ments, and Miika Hannula and Juha Kontinen for useful discussions and advice.
References
1. Abramsky, S., Puljuj¨arvi, J., V¨a¨an¨anen, J.: Team semantics and independence
notions in quantum physics (2021)
2. Albert, R., Gr¨adel, E.: Unifying hidden-variable problems from quantum mechanics
by logics of dependence and independence. Ann. Pure Appl. Logic 103088 (2022)
3. Armstrong, W.W.: Dependency structures of data base relationships. In: Proceed-
ings of IFIP World Computer Congress, pp. 580–583 (1974)
4. Beeri, C., Bernstein, P.A.: Computational problems related to the design of normal
form relational schemas. ACM Trans. Database Syst. 4(1), 30–59 (1979)
5. Beeri, C., Dowd, M., Fagin, R., Statman, R.: On the structure of armstrong rela-
tions for functional dependencies. J. ACM 31(1), 30–46 (1984)
6. Chandra, A.K., Vardi, M.Y.: The implication problem for functional and inclusion
dependencies is undecidable. SIAM J. Comput. 14(3), 671–677 (1985)
7. Cosmadakis, S.S., Kanellakis, P.C., Vardi, M.Y.: Polynomial-time implication
problems for unary inclusion dependencies. J. ACM 37(1), 15–46 (1990)
4 Sometimes the tuples of variables are assumed to be disjoint which disallows the
probabilistic conditional independencies of this form. In the context of team seman-
tics, such an assuption is usually not made.

146
M. Hirvonen
8. Durand, A., Hannula, M., Kontinen, J., Meier, A., Virtema, J.: Approximation and
dependence via multiteam semantics. Ann. Math. Artif. Intell. 83(3–4), 297–320
(2018)
9. Fagin, R.: Horn clauses and database dependencies. J. ACM 29, 952–985 (1982)
10. Fagin, R., Vardi, M.Y.: Armstrong databases for functional and inclusion depen-
dencies. Inf. Process. Lett. 16(1), 13–19 (1983)
11. Galliani, P.: Inclusion and exclusion dependencies in team semantics: on some
logics of imperfect information. Ann. Pure Appl. Logic 163(1), 68–84 (2012)
12. Geiger, D., Paz, A., Pearl, J.: Axioms and algorithms for inferences involving prob-
abilistic independence. Inf. Comput. 91(1), 128–141 (1991)
13. Gr¨adel, E., V¨a¨an¨anen, J.: Dependence and independence. Stud. Logica. 101(2),
399–410 (2013)
14. Hannula, M., Hirvonen, ˚A., Kontinen, J., Kulikov, V., Virtema, J.: Facets of dis-
tribution identities in probabilistic team semantics. In: Calimeri, F., Leone, N.,
Manna, M. (eds.) JELIA 2019. LNCS (LNAI), vol. 11468, pp. 304–320. Springer,
Cham (2019). https://doi.org/10.1007/978-3-030-19570-0 20
15. Hannula, M., Link, S.: On the interaction of functional and inclusion dependencies
with independence atoms. In: Pei, J., Manolopoulos, Y., Sadiq, S., Li, J. (eds.)
DASFAA 2018. LNCS, vol. 10828, pp. 353–369. Springer, Cham (2018). https://
doi.org/10.1007/978-3-319-91458-9 21
16. Hannula, M., Virtema, J.: Tractability frontiers in probabilistic team semantics
and existential second-order logic over the reals (2021)
17. Hodges, W.: Compositional Semantics for a Language of Imperfect Information. J.
Interest Group Pure Appl. Logics 5(4), 539–563 (1997)
18. Langeveldt, W.-D., Link, S.: Empirical evidence for the usefulness of armstrong
relations in the acquisition of meaningful functional dependencies. Inf. Syst. 35(3),
352–374 (2010)
19. Mannila, H., R¨aih¨a, K.-J.: Design by example: an application of armstrong rela-
tions. J. Comput. Syst. Sci. 33(2), 126–141 (1986)
20. Reingold, O.: Undirected connectivity in log-space. J. ACM 55(4), 1–24 (2008)
21. Studen´y, M.: Conditional independence relations have no ﬁnite complete charac-
terization, pp. 377–396. Kluwer (1992)
22. Studen´y, M.: Semigraphoids are two-antecedental approximations of stochastic
conditional independence models. In: Uncertainty in Artiﬁcial Intelligence, Pro-
ceedings of the Tenth Conference, pp. 546–552 (1994)
23. Tarjan, R.: Depth ﬁrst search and linear graph algorithms. SIAM J. Comput. 1(2),
146–160 (1972)
24. V¨a¨an¨anen, J.: Dependence Logic. Cambridge University Press, Cambridge (2007)

Approximate Keys and Functional
Dependencies in Incomplete Databases
with Limited Domains
Munqath Al-atar1,2 and Attila Sali1,3(B)
1 Department of Computer Science and Information Theory,
Budapest University of Technology and Economics, Budapest, Hungary
m.attar@cs.bme.hu
2 ITRDC, University of Kufa, Kufa, Iraq
munqith.alattar@uokufa.edu.iq
3 Alfr´ed R´enyi Institute of Mathematics, Budapest, Hungary
sali.attila@renyi.hu
Abstract. A possible world of an incomplete database table is obtained
by imputing values from the attributes (inﬁnite) domain to the place of
NULLs. A table satisﬁes a possible key or possible functional dependency
constraint if there exists a possible world of the table that satisﬁes the
given key or functional dependency constraint. A certain key or func-
tional dependency is satisﬁed by a table if all of its possible worlds sat-
isfy the constraint. Recently, an intermediate concept was introduced. A
strongly possible key or functional dependency is satisﬁed by a table if
there exists a strongly possible world that satisﬁes the key or functional
dependency. A strongly possible world is obtained by imputing values
from the active domain of the attributes, that is from the values appear-
ing in the table. In the present paper, we study approximation measures
of strongly possible keys and FDs. Measure g3 is the ratio of the mini-
mum number of tuples to be removed in order that the remaining table
satisﬁes the constraint. We introduce a new measure g5, the ratio of the
minimum number of tuples to be added to the table so the result satis-
ﬁes the constraint. g5 is meaningful because the addition of tuples may
extend the active domains. We prove that if g5 can be deﬁned for a table
and a constraint, then the g3 value is always an upper bound of the g5
value. However, the two measures are independent of each other in the
sense that for any rational number 0 ≤
p
q < 1 there are tables of an
arbitrarily large number of rows and a constant number of columns that
satisfy g3 −g5 = p
q . A possible world is obtained usually by adding many
new values not occurring in the table before. The measure g5 measures
the smallest possible distortion of the active domains.
Research of the second author was partially supported by the National Research, Devel-
opment and Innovation Oﬃce (NKFIH) grants K–116769 and SNN-135643. This work
was also supported by the BME- Artiﬁcial Intelligence FIKP grant of EMMI (BME
FIKP-MI/SC) and by the Ministry of Innovation and Technology and the National
Research, Development and Innovation Oﬃce within the Artiﬁcial Intelligence National
Laboratory of Hungary.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 147–167, 2022.
https://doi.org/10.1007/978-3-031-11321-5_9

148
M. Al-atar and A. Sali
Keywords: Strongly possible functional dependencies · Strongly
possible keys · incomplete databases · data Imputation · Approximate
functional dependencies · approximate keys
1
Introduction
The information in many industrial and research databases may usually be
incomplete due to many reasons. For example, databases related to instrument
maintenance, medical applications, and surveys [10]. This makes it necessary
to handle the cases when some information missing from a database and are
required by the user. Imputation (ﬁlling in) is one of the common ways to han-
dle the missing values [20].
A new approach for imputing values in place of the missing information
was introduced in [2], to achieve complete data tables, using only informa-
tion already contained in the SQL table attributes (which are called the active
domain of an attribute). Any total table obtained in this way is called a
strongly possible world. We use only the data shown on the table to replace the
missing information because in many cases there is no proper reason to consider
any other attribute values than the ones that already exist in the table. Using
this concept, new key and functional dependency constraints called strongly pos-
sible keys (spKeys) and strongly possible functional dependencies (spFDs) were
deﬁned in [3,5] that are satisﬁed after replacing any missing value (NULL) with a
value that is already shown in the corresponding attribute. In Sect. 2, we provide
the formal deﬁnitions of spKeys and spFDs.
The present paper continues the work started in [5], where an approximation
notion was introduced to calculate how close any given set of attributes can be
considered as a key, even when it does not satisfy the conditions of spKeys. This
is done by calculating the minimum number of tuples that need to be removed
from the table so that the spKey constraint holds.
Tuple removal may be necessary because the active domains do not contain
enough values to be able to replace the NULL values so that the tuples are pairwise
distinct on a candidate key set of attributes K. In the present paper, we introduce
approximation measures of spKeys and spFDs by adding tuples. Adding a tuple
with new unique values will add more values to the attributes’ active domains,
thus some unsatisﬁed constraints may get satisﬁed. For example, Car Model and
DoorNo is designed to form a key in the Cars Types table shown in Table 1 but
the table does not satisfy the spKey sp⟨Car Model, DoorNo⟩. Two tuples would
need to be removed, but adding a new tuple with distinct door number value to
satisfy sp⟨Car Model, DoorNo⟩is better than removing two tuples. In addition
to that, we know that the car model and door number determines the engine
type, then the added tuple can also have a new value in the DoorNo attribute
so that the table satisfy (Car Model, DoorNo) →sp Engine Type rather than
removing other two tuples.

Approximate Keys and FDs
149
Table 1. Cars Types Incomplete Table
Car Model
Door No Engine Type
BMW I3
4 doors
⊥
BMW I3
⊥
electric
Ford explorer
⊥
V8
Ford explorer
⊥
V6
Adding tuples with new values provides more values in the active domains
used to satisfy the spKey. But if the total part of the table does not satisfy
the key, then it is useless to add more values to the active domain. Thus, we
assume throughout this paper that the K-total part of the table satisﬁes the
spKey sp⟨K⟩constraint, and that the X-total part satisﬁes the spFD constraint
X →sp Y (for exact deﬁnitions see Sect. 2). The interaction between spFDs and
spKeys is studied in [1]. We also assume that every attribute has at least one
non-null value (so that the active domain is not the empty set) and we have
at least 2 attributes in the key set K since it was observed in [5] that a single
attribute can only be an spKey if the table does not contain NULL in it.
The main objectives of this paper are:
– Extend the g3 measure deﬁned for spKeys in [5] to spFDs.
– Propose a new approximation measure for spKeys and spFDs called g5, that
adopt adding tuples with new values to the tables that violate the constraints.
– Compare the newly proposed measure g5 with the earlier introduced measure
g3 and show that adding new tuples is more eﬀective than removing violating
ones.
– Nevertheless, g3 and g5 are independent of each other.
It is important to observe the diﬀerence between possible worlds and strongly
possible worlds. The former one was deﬁned and studied by several sets of
authors, for example in [9,18,28]. In possible worlds, any value from the usually
countably inﬁnite domain of the attribute can be imputed in place of NULLs.
This allows an inﬁnite number of worlds to be considered. By taking the newly
introduced active domain values given by the added tuples and minimizing the
number of the tuples added, we sort of determine a minimum world that satisﬁes
the constraints and contains an spWorld allowed by the original table given.
The paper is organized as follows. Section 2 gives the basic deﬁnitions and
notations. Some related work and research results are discussed in Sect. 3. The
approximation measures for spKeys and spFDs are provided in Sects. 4 and 5
respectively. And ﬁnally, the conclusions and the future directions are explained
in Sect. 6.

150
M. Al-atar and A. Sali
2
Basic Deﬁnitions
Let R = {A1, A2, . . . An} be a relation schema. The set of all the possible values
for each attribute Ai ∈R is called the domain of Ai and denoted as Di =
dom(Ai) for i = 1,2,. . . n. Then, for X ⊆R, then DX =

∀Ai∈K
Di.
An instance T = (t1,t2, . . . ts) over R is a list of tuples such that each tuple
is a function t : R →
Ai∈R dom(Ai) and t[Ai] ∈dom(Ai) for all Ai in R. By
taking a list of tuples we use the bag semantics that allows several occurrences
of the same tuple. Usage of the bag semantics is justiﬁed by that SQL allows
multiple occurrences of tuples. Of course, the order of the tuples in an instance
is irrelevant, so mathematically speaking we consider a multiset of tuples as an
instance. For a tuple tr ∈T and X ⊂R, let tr[X] be the restriction of tr to X.
It is assumed that ⊥is an element of each attribute’s domain that denotes
missing information. tr is called V -total for a set V of attributes if ∀A ∈V ,
tr[A] ̸= ⊥. Also, tr is a total tuple if it is R-total. t1 and t2 are weakly similar
on X ⊆R denoted as t1[X] ∼w t2[X] deﬁned by K¨ohler et.al. [17] if
∀A ∈X
(t1[A] = t2[A] or t1[A] = ⊥or t2[A] = ⊥).
Furthermore, t1 and t2 are strongly similar on X ⊆R denoted by t1[X] ∼s
t2[X] if
∀A ∈X
(t1[A] = t2[A] ̸= ⊥).
For the sake of convenience we write t1 ∼w t2 if t1 and t2 are weakly similar
on R and use the same convenience for strong similarity. Let T = (t1, t2, . . . ts)
be a table instance over R. Then, T ′ = (t′
1, t′
2, . . . t′
s) is a possible world of T, if
ti ∼w t′
i for all i = 1, 2, . . . s and T ′ is completely NULL-free. That is, we replace
the occurrences of ⊥with a value from the domain Di diﬀerent from ⊥for all
tuples and all attributes. A active domain of an attribute is the set of all the
distinct values shown under the attribute except the NULL. Note that this was
called the visible domain of the attribute in papers [1–3,5].
Deﬁnition 2.1. The active domain of an attribute Ai (V DT
i ) is the set of all
distinct values except ⊥that are already used by tuples in T:
V DT
i = {t[Ai] : t ∈T} \ {⊥} for Ai ∈R.
To simplify notation, we omit the upper index T if it is clear from the context
what instance is considered.
Then the V D1 in Table 2 is {Mathematics, Datamining}. The term active
domain refers to the data that already exist in a given dataset. For example, if
we have a dataset with no information about the deﬁnitions of the attributes’
domains, then we use the data itself to deﬁne their own structure and domains.
This may provide more realistic results when extracting the relationship between
data so it is more reliable to consider only what information we have in a given
dataset.

Approximate Keys and FDs
151
While a possible world is obtained by using the domain values instead of the
occurrence of NULL, a strongly possible world is obtained by using the active
domain values.
Deﬁnition 2.2. A
possible
world
T ′
of
T
is
called a strongly possible world (spWorld) if t′[Ai] ∈V DT
i
for all t′ ∈T ′ and
Ai ∈R.
The concept of strongly possible world was introduced in [2]. A strongly
possible
worlds
allow
us
to
deﬁne
strongly possible keys (spKeys)
and
strongly possible functional dependencies (spFDs).
Deﬁnition 2.3. A strongly possible functional dependency, in notation X →sp
Y , holds in table T over schema R if there exists a strongly possible world T ′
of T such that T ′ |= X →Y . That is, for any t′
1, t′
2 ∈T ′ t′
1[X] = t′
2[X] implies
t′
1[Y ] = t′
2[Y ]. The set of attributes X is a strongly possible key, if there exists a
strongly possible world T ′ of T such that X is a key in T ′, in notation sp⟨X⟩.
That is, for any t′
1, t′
2 ∈T ′ t′
1[X] = t′
2[X] implies t′
1 = t′
2.
Note that this is not equivalent with spFD X →sp R, since we use the bag seman-
tics. For example, {Course Name, Year} is a strongly possible key of Table 2 as
the strongly possible world in Table 3 shows it.
Table 2. Incomplete Dataset
Course Name Year Lecturer Credits Semester
Mathematics
2019
⊥
5
1
Datamining
2018
Sarah
7
⊥
⊥
2019
Sarah
⊥
2
If T = {t1, t2, . . . , tp} and T ′ = {t′
1, t′
2, . . . , t′
p} is an spWorld of it with
ti ∼w t′
i, then t′
i is called an sp-extension or in short an extension of ti. Let X ⊆R
be a set of attributes and let ti ∼w t′
i such that for each A ∈R: t′
i[A] ∈V D(A),
then t′
i[X] is an strongly possible extension of ti on X (sp-extension)
Table 3. Complete Dataset
Course Name Year Lecturer Credits Semester
Mathematics
2019
Sarah
5
1
Datamining
2018
Sarah
7
2
Datamining
2019
Sarah
7
2

152
M. Al-atar and A. Sali
3
Related Work
Giannella et al. [11] measure the approximate degree of functional dependencies.
They developed the IFD approximation measure and compared it with the other
two measures: g3 (minimum number of tuples need to be removed so that the
dependency holds) and τ (the probability of a correct guess of an FD satisfaction)
introduced in [16] and [12] respectively. They developed analytical bounds on the
measure diﬀerences and compared these measures analysis on ﬁve datasets. The
authors show that when measures are meant to deﬁne the knowledge degree of X
determines Y (prediction or classiﬁcation), then IFD and τ measures are more
appropriate than g3. On the other hand, when measures are meant to deﬁne the
number of “violating” tuples in an FD, then, g3 measure is more appropriate
than IFD and τ. This paper extends the earlier work of [5] that utilized the g3
measure for spKeys by calculating the minimum number of tuples to be removed
from a table so that an spKey holds if it is not. The same paper proposed the
g4 measure that is derived from g3 by emphasizing the eﬀect of each connected
component in the table’s corresponding bipartite graph (where vertices of the
ﬁrst class of the graph represent the table’s tuples and the second class represent
all the possible combinations of the attributes’ active domains). In this paper,
we propose a new measure g5 to approximate FDs by adding new tuples with
unique values rather than deleting tuples as in g3.
Several other researchers worked on approximating FDs in the literature.
King et al. [15] provided an algorithmic method to discover functional and
approximate functional dependencies in relational databases. The method pro-
vided is based upon the mathematical theory of partitions of row identiﬁcation
numbers from the relation, then determining non-trivial minimal dependencies
from the partitions. They showed that the operations that need to be done on
partitions are both simple and fast.
In [26], Varkonyi et al. introduced a structure called Sequential Indexing
Tables (SIT) to detect an FD regarding the last attribute in their sequence. SIT
is a fast approach so it can process large data quickly. The structure they used
does not scale eﬃciently with the number of the attributes and the sizes of their
domains, however. Other methods, such as TANE and FastFD face the same
problem [23]. TANE was introduced by Huhtala [13] to discover functional and
approximate dependencies by taking into consideration partitions and deriving
valid dependencies from these partitions in a breadth-ﬁrst or level-wise manner.
Bra, P. De, and Jan Paredaens gave a new decomposition theory for func-
tional dependencies in [8]. They break up a relation into two subrelations whose
union is the given relation and a functional dependency that holds in one sub-
relation is not in the other.
In [25], Tusor et al. presented the Parallelized Sequential Indexing Tables
method that is memory-eﬃcient for large datasets to ﬁnd exact and approximate
functional dependencies. Their method uses the same principle of Sequential
Indexing Tables in storing data, but their training approach and operation are
diﬀerent.

Approximate Keys and FDs
153
Pyro is an algorithm to discover all approximate FDs in a dataset presented
by Kruse [19]. Pyro veriﬁes samples of agree sets and prunes the search spaces
with the discovered FDs. On the other hand, based on the concept of “agree
sets”, Lopes et al. [22] developed an algorithm to ﬁnd a minimum cover of a
set of FDs for a given table by applying the so-called “Luxenburger basis” to
develop a basis of the set of approximate FDs in the table.
Simovici et al. [24] provide an algorithm to ﬁnd purity dependencies such
that, for a ﬁxed right-hand side (Y ), the algorithm applies a level-wise search on
the left-hand sides (X) so that X →Y has a purity measure below a user-deﬁned
threshold. Other algorithms were proposed in [14,21] to discover all FDs that
hold in a given table by searching through the lattice of subsets of attributes.
In [27], Jef Wijsen summarizes and discusses some theoretical developments
and concepts in Consistent query answering CQA (when a user queries a
database that is inconsistent with respect to a set of constraints). Database
repairing was modeled by an acyclic binary relation ≤db on the set of consistent
database instances, where r1 ≤db r2 means that r1 is at least as close to db as
r2. One possible distance is the number of tuples to be added and/or removed.
In addition to that, Bertossi studied the main concepts of database repairs and
CQA in [6], and emphasis on tracing back the origin, motivation, and early
developments. J. Biskup and L. Wiese present and analyze an algorithm called
preCQE that is able to correctly compute a solution instance, for a given original
database instance, that obeys the formal properties of inference-proofness and
distortion minimality of a set of appropriately formed constraints in [7].
4
SPKey Approximation
In [5], the authors studied strongly possible keys, and the main motivation is
to uniquely identify tuples in incomplete tables, if it is possible, by using the
already shown values only to ﬁll up the occurrences of NULLs. Consider the
relational schema R = and K ⊆R. Furthermore, let T be an instance over
R with NULLs. Let T ′ be the set of total tuples T ′ = {t′ ∈Πb
i=1V DT
i : ∃t ∈
T such that t[K] ∼w t′[K]}, furthermore let G = (T, T ′; E) be the bipartite
graph, called the K-extension graph of T, deﬁned by {t, t′} ∈E ⇐⇒t[K] ∼w
t′[K]. Finding a matching of G that covers all the tuples in T (if exists) provides
the set of tuples in T ′ to replace the incomplete tuples in T with, to verify that K
is an spKey. A polynomial-time algorithm was given in [3] to ﬁnd such matching.
It is a non-trivial application of the well-known matching algorithms, as |T ′| is
usually an exponential function of the size of the input table T.
The Approximate Strongly Possible Key (ASP Key) was deﬁned in [5] as
follows.
Deﬁnition 4.1. Attribute set K is an approximate strongly possible key of ratio
a in table T, in notation asp−
a ⟨K⟩, if there exists a subset S of the tuples T such
that T \S satisﬁes sp ⟨K⟩, and |S|/|T| ≤a. The minimum a such that asp−
a ⟨K⟩
holds is denoted by g3(K).

154
M. Al-atar and A. Sali
The measure g3(K) represents the approximation which is the ratio of the
number of tuples needed to be removed over the total number of tuples so that
sp ⟨K⟩holds. The measure g3(K) has a value between 0 and 1, and it is exactly
0 when sp ⟨K⟩holds in T, which means we don’t need to remove any tuples.
For this, we used the g3 measure introduced in [16], to determine the degree
to which ASP key is approximate. For example, the g3 measure of sp⟨X⟩on
Table 4 is 0.5, as we are required to remove two out of four tuples to satisfy the
key constraint as shown in Table 5.
It was shown in [5] that the g3 approximation measure for strongly possible
keys satisﬁes
g3(K) = |T| −ν(G)
|T|
.
where ν(G) denotes the maximum matching size in the K-extension graph G.
The smaller value of g3(K), the closer K is to being an spKey.
For the bipartite graph G deﬁned above, let C be the collection of all the
connected components in G that satisfy the spKey, i.e. for which there exists a
matching that covers all tuples in the set (∀C∈C ∄X ⊆C ∩T such that |X| >
N(X) by Hall’s Theorem). Let D ⊆G be deﬁned as D = G \ 
∀C∈C C, and let
C ′ be the set of connected components of D. Let VC denote the set of vertices in
a connected component C. The approximation measure of strongly possible keys
may be more appropriate by considering the eﬀect of each connected component
in the bipartite graph on the matching. We consider the eﬀect of the components
of C to get doubled in the approximation measure, as these components represent
that part of the data that do not require tuple removal. So a derived version of the
g3 measure was proposed and named g4 considering these components’ eﬀects,
g4(K) = |T| −(
C∈C (|VC|) + 
C′∈C ′ ν(C′))
|T| + 
C∈C |VC|
.
Furthermore, it was proved that for a set of attributes K in any table, we
have either g3(K) = g4(K) or 1 < g3(K)/g4(K) < 2. Moreover, there exist
tables of an arbitrarily large number of tuples with g3(K)/g4(K) = p
q for any
rational number 1 ≤p
q < 2.
In this paper, we extend our investigation on approximating spKeys by con-
sidering adding new tuples instead of removing them to satisfy an spKey if
possible. Removing a non-total tuple t1 means that there exist another total
and/or non-total tuple(s) that share the same strongly possible extension with
t2. The following proposition shows that we can always remove only non-total
tuples if the total part of the table satisﬁes the key.
Proposition 4.1. Let T be an instance over schema R and let K ⊆R. If the
K-total part of the table T satisﬁes the key sp ⟨K⟩, then there exists a minimum
set of tuples U to be removed that are all non-K-total so that T \ U satisﬁes
sp ⟨K⟩.
Proof. Observe that a minimum set of tuples to be removed is T \X for a subset
X of the set of vertices (tuples) covered by a particular maximum matching of

Approximate Keys and FDs
155
the K-extension graph. Let M be a maximum matching, and assume that t1 is
total and not covered by M. Then, the unique neighbour t′
1 of t1 in T ′ is covered
by an edge (t2, t′
1) of M. Then t2 is non-total since the K-total part satisﬁes
sp ⟨K⟩, so we replace the edge (t2, t′) by the edge (t1, t′) to get matching M1 of
size |M1| = |M|, and M1 covers one more total tuple. Repeat this until all total
tuples are covered.
4.1
Measure g5 for SpKeys
The g3 approximation measure for spKeys was introduced in [5]. In this section,
we introduce a new approximation measure for spKeys. As we consider the active
domain to be the source of the values to replace each null with, adding a new
tuple to the table may increase the number of the values in the active domain of
an attribute. for example, consider Table 4, the active domain of the attribute
X1 is {2} and it changed to {2, 3} after adding a tuple with new values as shown
in Table 6.
Table
4.
Incomplete
Table to measure sp⟨X⟩
X
X1 X2
⊥
1
2
⊥
2
⊥
2
2
Table 5. The table after
removing (asp−
a ⟨X⟩)
X
X1 X2
⊥
1
2
2
Table 6. The table after
adding (asp+
b ⟨X⟩)
X
X1 X2
⊥
1
2
⊥
2
⊥
2
2
3
3
In the following deﬁnition, we deﬁne the g5 measure as the ratio of the min-
imum number of tuples that need to be added over the total number of tuples
to have the spKey satisﬁed.
Deﬁnition 4.2. Attribute set K is an add-approximate strongly possible key of
ratio b in table T, in notation asp+
b ⟨K⟩, if there exists a set of tuples S such
that the table TS satisﬁes sp ⟨K⟩, and |S|/|T| ≤b. The minimum b such that
asp+
b ⟨K⟩holds is denoted by g5(K).
The measure g5(K) represents the approximation which is the ratio of the
number of tuples needed to be added over the total number of tuples so that
sp ⟨K⟩holds. The value of the measure g3(K) ranges between 0 and 1, and it
is exactly 0 when sp ⟨K⟩holds in T, which means we do not have to add any
tuple. For example, the g5 measure of sp⟨X⟩on Table 4 is 0.25, as it is enough
to add one tuple to satisfy the key constraint as shown in Table 6.
Let T be a table and U ⊆T be the set of the tuples that we need to remove so
that the spKey holds in T, i.e., we need to remove |U| tuples, while by adding a
tuple with new values, we may make more than one of the tuples in U satisfy the

156
M. Al-atar and A. Sali
spKey using the new added values for their NULLs. In other words, we may need
to add a fewer number of tuples than the number of tuples we need to remove to
satisfy an spKey in the same given table. For example, Table 4 requires removing
two tuples to satisfy sp ⟨X⟩, while adding one tuple is enough.
On the other hand, one may think about mixed modiﬁcation of both adding
and deleting tuples for Keys approximation, by ﬁnding the minimum number of
tuples needs to be either added or removed. If ﬁrst the additions are performed,
then after that by Proposition 4.1, it is always true that we can remove only
non-total tuples; then, instead of any tuple removal, we may add a new tuple
with distinct values. Therefore, mixed modiﬁcation in that way would not change
the approximation measure, as it is always equivalent to tuples addition only.
However, if the order of removals and additions count, then it is a topic of further
research whether the removals can be substituted by additions.
The values of the two measures, g3 and g5, range between 0 and 1, and they
are both equal to 0 if the spKey holds (we do not have to add or remove any
tuples). Proposition 4.2 proves that the value of g3 measure is always larger than
or equal to the value of g5 measure.
Proposition 4.2. For any K ⊆R with |K| ≥2, we have g3(K) ≥g5(K).
Proof. Indeed, we can always remove non-total tuples for g3 by Proposition 4.1.
Let the tuples to be removed be U = {t1, t2, . . . tu}. Assume that T ∗is an
spWorld of T \ U, which certiﬁes that T \ U |= sp ⟨K⟩For each tuple ti ∈U, we
add tuple t′
i = (zi, zi, . . . , zi) where zi is a value that does not occur in any other
tuple originally of T or added. The purpose of adding t′
i is twofold. First it is
intended to introduce a completely new active domain value for each attribute.
Second, their special structure ensures that they will never agree with any other
tuple in the spWorld constructed below for the extended instance. Let ti” be a
tuple such that exactly one NULL in K of ti is replaced by zi, any other NULLs of
ti are imputed by values from the original active domain of the attributes. It is
not hard to see that tuples in T ∗∪{t′
1, t′
2 . . . , t′
u}∪{t1”, t2” . . . , tu”} are pairwise
distinct on K.
According to Proposition 4.2 we have 0 ≤g3(K) −g5(K) < 1 and the diﬀerence
is a rational number. What is not immediate is that for any rational number
0 ≤p
q < 1 there exist a table T and K ⊆R such that g3(K) −g5(K) = p
q in
table T.
Proposition 4.3. Let 0 ≤p
q < 1 be a rational number. Then there exists a table
T with an arbitrarily large number of rows and K ⊆R such that g3(K)−g5(K) =
p
q in table T.
Proof. We may assume without loss of generality that K = R, since T ′ |= sp ⟨K⟩
if and only if we can make the tuples pairwise distinct on K by imputing values
from the active domains, that is values in R \ K are irrelevant. Let T be the
following q × (p + 2) table (with x = q −p −1).

Approximate Keys and FDs
157
T =
1 1 1 . . . 1
1 1 1 . . . 2
...
1 1 1 . . . x
⎫
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎭
q −p −1
⊥1 . . . 1 1
1 ⊥. . . 1 1
...
1 1 . . . ⊥1
⎫
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎭
p + 1
(1)
Since the active domain of the ﬁrst p + 1 attributes is only {1}, we have to
remove p + 1 rows so g3(K) = p+1
q . On the other hand it is enough to add one
new row (2, 2, . . . , 2, q −p) so g5(K) = 1
q. Since p
q = cp
cq for any positive integer
c, the number of rows in the table could be arbitrarily large.
The tables constructed in the proof of Proposition 4.3 have an arbitrarily
large number of rows, however, the price for this is that the number of columns
is also not bounded. The question arises naturally whether there are tables with
a ﬁxed number of attributes but with an arbitrarily large number of rows that
satisfy g3(K) −g5(K) = p
q for any rational number 0 ≤p
q < 1? The following
theorem answers this problem.
Theorem 4.1. Let 0 ≤p
q < 1 be a rational number. Then there exist tables over
schema {A1, A2} with arbitrarily large number of rows, such that g3({A1, A2})−
g5({A1, A2}) = p
q .
Proof. The proof is divided into three cases according to whether p
q < 1
2, p
q = 1
2
or p
q > 1
2. In each case, the number of rows of the table will be an increasing
function of q and one just has to note that q can be chosen arbitrarily large
without changing the value of the fraction p
q .
Case p
q < 1
2 Let T<.5 be deﬁned as
T<.5 =
q −p −1
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
1
1
2
...
...
1 q −p −1
p + 1
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
⊥
⊥
⊥
⊥
...
...
⊥
⊥
Clearly, g3(K) = p+1
q , as all the tuples with NULLs have to be removed. On
the other hand, if tuple (2, q −p) is added, then the total number of active
domain combinations is 2 · (q −p), out of which q −p is used up in the table,

158
M. Al-atar and A. Sali
so there are q −p possible pairwise distinct tuples to replace the NULLs. Since
p
q < 1
2, we have that q −p ≥p + 1 so all the tuples in the q + 1-rowed table can
be made pairwise distinct. Thus, g3(K) −g5(K) = p+1
q
−1
q.
Case p
q = 1
2 Let T=.5 be deﬁned as
T=.5 =
q −p −2
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
1
1
2
...
...
1 q −p −2
p + 2
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
⊥
⊥
⊥
⊥
...
...
⊥
⊥
Table T=.5 contains all possible combinations of the active domain values, so
we have to remove every tuple containing NULLs, so g3(K) = p+2
q . On the other
hand, if we add just one new tuple (say (2, q −p −1)), then the largest number
of active domain combinations is 2 · (q −p −1) that can be achieved. There are
already q −p −1 pairwise distinct total tuples in the extended table, so only
q −p −1 < p + 2 would be available to replace the NULLs. On the other hand,
adding two new tuples, (2, q −p −1) and (3, q −p) creates a pool of 3 · (q −p)
combinations of active domains, which is more than (q −p −1) + p + 2 that is
needed.
Case p
q > 1
2 Table T is deﬁned similarly to the previous cases, but we need more
careful analysis of the numbers.
T =
b
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1 1
1 2
...
...
1 b
x
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
⊥⊥
⊥⊥
... ...
⊥⊥
(2)
Clearly, g3(K) =
x
x+b. Let us assume that y tuples are needed to be added.
The maximum number of active domain combinations is (y + 1)(y + b) obtained
by adding tuples (2, b + 1), (3, b + 2), . . . , (y + 1, y + b). This is enough to replace
all tuples with NULLs if
(y + 1)(y + b) ≥x + y + b.
(3)
On the other hand, y −1 added tuples are not enough, so
y(y −1 + b) < x + y −1 + b.
(4)

Approximate Keys and FDs
159
Since the total number of active domain combinations must be less than the
tuples in the extended table. We have p
q = g3(K) −g5(K) = x−y
x+b that is for
some positive integer c we must have cp = x −y and cq = x + b if gcd(p, q) = 1.
This can be rewritten as
y = x −cp ;
y + b = c(q −p) ;
b = cq −x ;
x + y + b = y + cq.
(5)
Using (5) we obtain that (3) is equivalent with
y ≥
cp
c(q −p) −1.
(6)
If c is large enough then ⌈
cp
c(q−p)−1⌉= ⌈
p
q−p⌉so if y = ⌈
p
q−p⌉is chosen then
(6) and consequently (3) holds. On the other hand, (4) is equivalent to
y <
cq −1
c(q −p) −2.
(7)
The right hand side of (7) tends to
q
q−p as c tends to inﬁnity. Thus, for large
enough c we have ⌊
cq−1
c(q−p)−2⌋= ⌊
q
q−p⌋. Thus, if
y = ⌈
p
q −p⌉≤⌊
q
q −p⌋
(8)
and
q
q−p is not an integer, then both (3) and (4) are satisﬁed for large enough c.
Observe that
p
q−p +1 =
q
q−p, thus (8) always holds. Also, if
q
q−p is indeed an inte-
ger, then we have strict inequality in (8) that implies (7) and consequently (4).
5
spFD Approximation
In this section, we measure to which extent a table satisﬁes a Strongly Possible
Functional Dependency (spFD) X →sp Y if T ̸|= X →sp Y .
Similarly to Sect. 4, we assume that the X-total part of the table satisﬁes the
FD X →Y , so we can always consider adding tuples. The measures g3 and g5
are deﬁned analogously to the spKey case.
Deﬁnition 5.1. For the attribute sets X and Y , σ : X →sp Y is a remove-
approximate strongly possible functional dependency of ratio a in a table T, in
notation.
T |=≈−
a X →sp Y , if there exists a set of tuples S such that the table
T \ S |= X →sp Y , and |S|/|T| ≤a. Then, g3(σ) is the smallest a such that
T |=≈−
a σ holds.
The measure g3(σ) represents the approximation which is the ratio of the
number of tuples needed to be removed over the total number of tuples so that
T |= X →sp Y holds.

160
M. Al-atar and A. Sali
Deﬁnition 5.2. For the attribute sets X and Y , σ : X →sp Y is an add-
approximate strongly possible functional dependency of ratio b in a table T, in
notation T |=≈+
b X →sp Y , if there exists a set of tuples S such that the table
T ∪S |= X →sp Y , and |S|/|T| ≤b. Then, g5(σ) is the smallest b such that
T |=≈+
b σ holds.
The measure g5(σ) represents the approximation which is the ratio of the number
of tuples needed to be added over the total number of tuples so that T |= X →sp
Y holds. For example, consider Table 7. We are required to remove at least 2
tuples so that X →sp Y holds, as it is easy to check that if we remove only
one tuple, then T ̸|= X →sp Y , but on the other hand, the table obtained by
removing tuples 4 and 5, shown in Table 8 satisﬁes X →sp Y . It is enough to
add only one tuple to satisfy the dependency as the table in Table 9 shows.
Table 7. Incomplete Table to
measure (X →sp Y )⟩
X
Y
X1 X2
⊥
1
1
2
⊥
1
2
⊥
1
2
1
2
2
1
2
2
2
2
Table 8. The table after
removing (−
a X →sp Y )
X
Y
X1 X2
⊥
1
1
2
⊥
1
2
⊥
1
2
2
2
Table 9. The table after
adding (+
b X →sp Y )
X
Y
X1 X2
⊥
1
1
2
⊥
1
2
⊥
1
2
1
2
2
1
2
2
2
2
3
3
3
5.1
The Diﬀerence of G3 and G5 for SpFDs
The same table may get diﬀerent approximation measure values for g3 and g5.
For example, the g3 approximation measure for Table 7 is 0.334 (it requires
removing at least 2 tuples out of 6), while its g5 approximation measure is 0.167
(it requires adding at least one tuple with new values).
The following theorem proves that it is always true that the g3 measure value
of a table is greater than or equal to the g5 for spFDs.
Theorem 5.1. Let T be a table over schema R, σ : X →sp Y for some X, Y ⊆
R. Then g3(σ) ≥g5(σ).
The proof is much more complicated than the one in the case of spKeys, because
we cannot assume that there always exists a minimum set of non-total tuples to
be removed for g3, as the table in Table 10 shows. In this table the third tuple
alone forms a minimum set of tuples to be removed to satisfy the dependency
and it has no NULL.

Approximate Keys and FDs
161
Table 10. X-total tuple needs to be removed
X
Y
X1 X2
1
⊥
1
1
⊥
1
1
1
2
1
1
⊥
1
2
3
From that table, we need to remove the third row to have X →sp Y satisﬁed.
Let us note that adding row (3, 3, 3) gives the same result, so g3(X →sp Y ) =
g5(X →sp Y ) = 1. However, there exist no spWorlds that realize the g3 and g5
measure values and agree on those tuples that are not removed for g3.
Proof. of Theorem 5.1 Without loss of generality, we may assume that X∩Y = ∅,
because T |= X →sp Y ⇐⇒T |= X\Y →sp Y \X. Also, it is enough to consider
attributes in X ∪Y . Let U = {t1, t2, . . . , tp} be a minimum set of tuples to be
removed from T. Let T ′ be the spWorld of T \ U that satisﬁes X →Y . Let us
assume that t1, . . . ta are such that ti[X] is not total for 1 ≤i ≤a. Furthermore,
let ta+1[X] = . . . = tj1[X], tj1+1[X] = . . . = tj2[X], . . ., tjf +1[X] = . . . = tp[X]
be the maximal sets of tuples that have the same total projection on X. We
construct a collection of tuples {s1, . . . sa+f+1}, together with an spWorld T ∗of
T ∪{s1, . . . , sa+f+1} that satisﬁes X →Y as follows.
Case 1. 1 ≤i ≤a. Let zi be a value not occurring in T neither in every tuple sj
constructed so far. Let si[A] = zi for ∀A ∈X and si[B] = ti[B] for B ∈R \ X.
The corresponding sp-extensions s∗
i , t∗
i ∈T ∗are given by setting s∗
i [B] = t∗
i [B] =
β where β ∈V DB arbitrarily ﬁxed if ti[B] = ⊥in case B ∈R \ X, furthermore
t∗
i [A] = zi if A ∈X and ti[A] = ⊥.
Case 2. X-total tuples. For each such set tjg−1+1[X] = . . . = tjg[X] (g ∈
{1, 2, . . . , f + 1}) we construct a tuple sa+g. Let vg
1, vg
2, . . . vg
kg ∈T \ U be the
tuples whose sp-extension vg
j
′ in T ′ satisﬁes vg
j
′[X] = tjg[X] for 1 ≤j ≤kg.
Let vg
1, vg
2, . . . vg
ℓbe those that are also X-total. Since the X-total part of the
table satisﬁes X →sp Y , tjg−1+1, . . . tjg, vg
1, vg
2, . . . vg
ℓcan be sp-extended to be
identical on Y . Let us take those extensions in T ∗.
Let sa+g be deﬁned as sa+g[A] = za+g where za+g is a value not used before
for A ∈X, furthermore sa+g[B] = vg
ℓ+1[B] for B ∈R \ X. The sp-extensions are
given as vg∗
q [A] = za+g if vg∗
q [A] = ⊥and A ∈X, otherwise vg∗
q [A] = vg
q
′[A] for
ℓ+ 1 ≤q ≤kg. Finally, let s∗
a+g[B] = vg
1
′[B] for B ∈R \ X.
For any tuple t ∈T \ U for which no sp-extension has been deﬁned yet, let
us keep its extension in T ′, that is let t∗= t′.

162
M. Al-atar and A. Sali
Claim. T ∗|= X →sp Y . Indeed, let t1, t2 ∈T ∪{s1, . . . , sa+f+1} be two tuples
such that their sp-extensions in T ∗agree on X, that is t1∗[X] = t2∗[X]. If t1∗[X]
contains a new value zj for some 1 ≤j ≤a + f + 1, then by deﬁnition of the
sp-extensions above, we have t1∗[Y ] = t2∗[Y ]. Otherwise, either both t1, t2 are X-
total, so again by deﬁnition of the sp-extensions above, we have t1∗[Y ] = t2∗[Y ],
or at least one of them is not X-total, and then t1∗= t1′ and t2∗= t2′. But in
this latter case using T ′ |= X →sp Y we get t1∗[Y ] = t2∗[Y ].
The values g3 and g5 are similarly independent of each other for spFDs as in the
case of spKeys.
Theorem 5.2. For any rational number 0 ≤p
q < 1 there exists tables with an
arbitrarily large number of rows and bounded number of columns that satisfy
g3(σ) −g5(σ) = p
q for σ: X →sp Y .
Table 11. g3 −g5 = p
q
T =
X
Y
X1 X2
1
1
1
1
2
2
...
...
...
1
b
b
⊥
⊥b + 1
⊥
⊥b + 2
...
...
...
⊥
⊥b + x
Proof. Consider the following table T. We clearly have g3(X →sp Y ) =
x
x+b for
T as all tuples with NULLs must be removed. On the other hand, by adding new
tuples and so extending the active domains, we need to be able to make at least
x + b pairwise distinct combinations of X-values. If y tuples are added, then we
can extend the active domains to the sizes |V D1| = y + 1 and |V D2| = y + b.
Also, if y is the minimum number of tuples to be added, then
g3(X →sp Y ) −g5(X →sp Y ) = x −y
x + b = p
q
(9)
if cp = x −y and cq = x + b for some positive integer c. From here y = x −cp
and y + b = c(q −p) Thus, what we need is
(y + 1)(y + b) = (y + 1)c(q −p) ≥cq
(10)

Approximate Keys and FDs
163
and, to make sure that y −1 added tuples are not enough,
y(y + b −1) = y(c(q −p) −1) ≤cq −1.
(11)
Easy calculation shows that (10) is equivalent with y ≥
p
q−p, so we take
y =

p
q−p

. On the other hand, (11) is equivalent with y ≤
cq−1
c(q−p)−1. Now,
similarly to Case 3 of the proof of Theorem 4.1 observe that
cq−1
c(q−p)−1 →∞as
c →∞, so, if c is large enough, then (11) holds.
5.2
Semantic Comparison of g3 and g5
In this section, we compare the g3 and g5 measures to analyze their applicability
and usability for diﬀerent cases. The goal is to specify when it is semantically
better to consider adding or removing rows for approximation for both spFDs
and spKeys.
Considering the teaching table in Table 12, we have the two strongly
possible constraints Semester TeacherID →sp CourseID and sp⟨Semester
TeacherID⟩. It requires adding one row so that asp+
a ⟨Semester Teacher
ID⟩=+
a
Semester TeacherID →sp CourseID. But on the other hand, it
requires removing 3 out of the 6 rows. Then, it would be more convenient to
add a new row rather than removing half of the table, which makes the remain-
ing rows not useful for analysis for some cases.
Adding new tuples to satisfy some violated strongly possible constraints
ensures that we make the minimum changes. In addition to that, in the case
of deletion, some active domain values may be removed. There are some cases
where it may be more appropriate to remove rather than add tuples, how-
ever. This is to preserve semantics of the data and to avoid using values that
are out of the appropriate domain of the attributes while adding new tuples
with new unseen values. For example, Table 13 represents the grade records
for some students in a course that imply the key (Name, Group) and the
dependency Points Assignment →Result, while both of sp⟨NameGroup⟩and
Points Assignment →sp Result are violated by the table. Then, adding one
tuple with the new values (Dummy, 3, 3, Maybe, Hopeless) is enough to satisfy
the two strongly possible constraints, while they can also be satisﬁed by remov-
ing the last two tuples. However, it is not convenient to use these new values for
the attributes, since they are probably not contained in the intended domains.
Hence, removing two tuples is semantically more acceptable than adding one
tuple.
If g3 is much larger than g5 for a table, it is better to add rows than remove
them. Row removal may leave only a short version of the table which may not
give a useful data analysis, as is the case in Table 11. If g3 and g5 are close to
each other, it is mostly better to add rows, but when the attributes’ domains are
restricted to a short-range, then it may be better to remove rows rather than
adding new rows with “noise” values that are semantically not related to the
meaning of the data, as is the case in Table 13.

164
M. Al-atar and A. Sali
Table 12. Incomplete teaching table
Semester TeacherID CourseID
First
1
1
⊥
1
2
First
2
3
⊥
2
4
First
3
5
⊥
3
6
Table 13. Incomplete course grading table
Name Group Points
Assignment
Result
Bob
1
2
Submitted
Pass
Sara
1
1
Not Submitted
Fail
Alex
1
2
Not Submitted
Fail
John
1
1
Submitted
Pass
⊥
1
1
⊥
Retake
Alex
⊥
2
⊥
Retake
6
Conclusion and Future Directions
Two approximation measures for spKeys and spFDs were investigated. The ﬁrst
one, g3, is the ratio of the minimum number of rows to be removed, and was
introduced for functional dependencies in tables without NULL values in [11] and
for spKeys in [2]. In the present paper, we extended the deﬁnition for spFDs,
as well. A new measure g5 was also introduced here, which measures the ratio
of the minimum number of tuples to be added to satisfy a strongly possible
constraint. This measure is only meaningful for strongly possible constraints
because ordinary functional dependencies or possible functional dependencies
cannot be made valid by adding tuples. However, the new tuples may extend
the active domains of the attributes and hence may make some strongly possible
constraints satisﬁed. Note that any add-approximate spKey or spFD is a possible
key, respectively possible FD. Thus, the g5 measure measures the minimum
number of “extra” attribute values one has to use in a possible world satisfying
the constraint.
We proved that the value of g5 is at most as large as the value of g3 for both
spKeys and spFDs. Otherwise, however, the two measures are independent of
each other, as their diﬀerence can take any non-negative rational value less than
one.
The referees suggested considering tuple removal and addition concurrently,
or tuple modiﬁcation. If ﬁrst the additions are performed, then after that by
Proposition 4.1, it is always true that we can remove only non-total tuples;

Approximate Keys and FDs
165
then, instead of any tuple removal, we may add a new tuple with distinct values.
Therefore, mixed modiﬁcation in that way would not change the approximation
measure, as it is always equivalent to tuples addition only. However, if the order
of removals and additions count, then it is a topic of further research whether
the removals can be substituted by additions. Also, Proposition 4.1 is only valid
for spKeys, so mixed modiﬁcations are interesting research problem for spFDs.
One tuple modiﬁcation can easily be replaced by one removal and one addition.
The question remains open whether one can gain more with tuple modiﬁcations
than the above replacement. A future research direction is to tackle algorithmic
and complexity questions. It was proven in [3] that checking whether for a given
subset K ⊆R and table T, T |= sp⟨K⟩holds can be decided in polynomial
time. However, the questions whether g3(sp⟨K⟩) ≤q and g5(sp⟨K⟩) ≤q are
not known to be polynomial. The problem is that we would have to check all
possible tables T ′ ⊂T with |T ′|/|T| ≥1 −q which could mean exponentially
many tables. On the other hand, it is clear that both problems, g3(sp⟨K⟩) ≤q
and g5(sp⟨K⟩) ≤q are in NP.
The analogous question for spFDs, that is whether T |= X →sp Y for a
table T and subsets X, Y ⊆R, is itself NP-complete [3]. This suggests that the
problem of bounding the approximation measures g3 and g5 for spFDs is also
intractable. However, it is a topic of further study to really prove it.
We studied handling missing values for Multi-valued Dependencies (spMVDs)
in [4]. An interesting future research direction can be measuring approximation
ratio of spMVDs.
Acknowledgement. The authors are indebted to the unknown referees for their care-
ful reading of the paper. The authors are thankful for the many suggestions of improve-
ments and calling their attention to several related works.
References
1. Al-Atar, M., Sali, A.: Strongly possible functional dependencies for SQL. Acta
Cybernetica (2022)
2. Alattar, M., Sali, A.: Keys in relational databases with nulls and bounded domains.
In: Welzer, T., Eder, J., Podgorelec, V., Kamiˇsali´c Latiﬁ´c, A. (eds.) ADBIS 2019.
LNCS, vol. 11695, pp. 33–50. Springer, Cham (2019). https://doi.org/10.1007/978-
3-030-28730-6 3
3. Alattar, M., Sali, A.: Functional dependencies in incomplete databases with limited
domains. In: Herzig, A., Kontinen, J. (eds.) FoIKS 2020. LNCS, vol. 12012, pp.
1–21. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-39951-1 1
4. Alattar, M., Sali, A.: Multivalued dependencies in incomplete databases with lim-
ited domain: properties and rules. In: 16th International Miklos Ivanyi PhD &
DLA Symposium, p. 226 (2020)
5. Alattar, M., Sali, A.: Strongly possible keys for SQL. J. Data Semant. 9(2), 85–99
(2020)
6. Bertossi, L.: Database repairs and consistent query answering: origins and fur-
ther developments. In: Proceedings of the 38th ACM SIGMOD-SIGACT-SIGAI
Symposium on Principles of Database Systems, pp. 48–58 (2019)

166
M. Al-atar and A. Sali
7. Biskup, J., Wiese, L.: A sound and complete model-generation procedure for con-
sistent and conﬁdentiality-preserving databases. Theoret. Comput. Sci. 412(31),
4044–4072 (2011)
8. De Bra, P., Paredaens, J.: Conditional dependencies for horizontal decompositions.
In: Diaz, J. (ed.) ICALP 1983. LNCS, vol. 154, pp. 67–82. Springer, Heidelberg
(1983). https://doi.org/10.1007/BFb0036898
9. De Keijzer, A., Van Keulen, M.: A possible world approach to uncertain relational
data. In: Proceedings of 15th International Workshop on Database and Expert
Systems Applications, pp. 922–926. IEEE (2004)
10. Farhangfar, A., Kurgan, L.A., Pedrycz, W.: A novel framework for imputation of
missing values in databases. IEEE Trans. Syst. Man Cybern. Part A Syst. Hum.
37(5), 692–709 (2007)
11. Giannella, C., Robertson, E.: On approximation measures for functional depen-
dencies. Inf. Syst. 29(6), 483–507 (2004)
12. Goodman, L.A., Kruskal, W.H.: Measures of association for cross classiﬁcations.
In: Goodman, L.A., Kruskal, W.H. (eds.) Measures of Association for Cross Clas-
siﬁcations, pp. 2–34. Springer, New York (1979). https://doi.org/10.1007/978-1-
4612-9995-0 1
13. Huhtala, Y., K¨arkk¨ainen, J., Porkka, P., Toivonen, H.: Tane: an eﬃcient algorithm
for discovering functional and approximate dependencies. Comput. J. 42(2), 100–
111 (1999)
14. Kantola, M., Mannila, H., R¨aih¨a, K.-J., Siirtola, H.: Discovering functional and
inclusion dependencies in relational databases. Int. J. Intell. Syst. 7(7), 591–607
(1992)
15. King, R.S., Legendre, J.J.: Discovery of functional and approximate functional
dependencies in relational databases. J. Appl. Math. Decis. Sci. 7(1), 49–59 (2003)
16. Kivinen, J., Mannila, H.: Approximate inference of functional dependencies from
relations. Theoret. Comput. Sci. 149(1), 129–149 (1995)
17. K¨ohler, H., Leck, U., Link, S., Zhou, X.: Possible and certain keys for SQL. VLDB
J. 25(4), 571–596 (2016)
18. K¨ohler, H., Link, S., Zhou, X.: Possible and certain SQL keys. Proc. VLDB Endow.
8(11), 1118–1129 (2015)
19. Kruse, S., Naumann, F.: Eﬃcient discovery of approximate dependencies. Proc.
VLDB Endow. 11(7), 759–772 (2018)
20. Lipski Jr, W.: On databases with incomplete information. J. ACM (JACM) 28(1),
41–70 (1981)
21. Lopes, S., Petit, J.-M., Lakhal, L.: Eﬃcient discovery of functional dependencies
and armstrong relations. In: Zaniolo, C., Lockemann, P.C., Scholl, M.H., Grust,
T. (eds.) EDBT 2000. LNCS, vol. 1777, pp. 350–364. Springer, Heidelberg (2000).
https://doi.org/10.1007/3-540-46439-5 24
22. Lopes, S., Petit, J.-M., Lakhal, L.: Functional and approximate dependency mining:
database and FCA points of view. J. Exp. Theor. Artif. Intell. 14(2–3), 93–114
(2002)
23. Papenbrock, T., et al.: Functional dependency discovery: an experimental evalua-
tion of seven algorithms. Proc. VLDB Endow. 8(10), 1082–1093 (2015)
24. Simovici, D.A., Cristofor, D., Cristofor, L.: Impurity measures in databases. Acta
Informatica 38(5), 307–324 (2002)
25. Tusor, B., V´arkonyi-K´oczy, A.R.: Memory eﬃcient exact and approximate func-
tional dependency extraction with parsit. In: 2020 IEEE 24th International Con-
ference on Intelligent Engineering Systems (INES), pp. 133–138. IEEE (2020)

Approximate Keys and FDs
167
26. V´arkonyi-K´oczy, A.R., Tusor, B., T´oth, J.T.: A multi-attribute classiﬁcation
method to solve the problem of dimensionality. In: Jablo´nski, R., Szewczyk, R.
(eds.) Recent Global Research and Education: Technological Challenges. AISC,
vol. 519, pp. 403–409. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
46490-9 54
27. Wijsen, J.: Foundations of query answering on inconsistent databases. ACM SIG-
MOD Rec. 48(3), 6–16 (2019)
28. Zim´anyi, E., Pirotte, A.: Imperfect information in relational databases. In: Motro,
A., Smets, P. (eds.) Uncertainty Management in Information Systems, pp. 35–87.
Springer, Boston (1997). https://doi.org/10.1007/978-1-4615-6245-0 3

The Fault-Tolerant Cluster-Sending
Problem
Jelle Hellings1(B) and Mohammad Sadoghi2
1 McMaster University, 1280 Main St. W., Hamilton, ON L8S 4L7, Canada
jhellings@mcmaster.ca
2 Exploratory Systems Lab, Department of Computer Science,
University of California, Davis, USA
Abstract. The emergence of blockchains is fueling the development of
resilient data management systems that can deal with Byzantine failures
due to crashes, bugs, or even malicious behavior. As traditional resilient
systems lack the scalability required for modern data, several recent sys-
tems explored using sharding. Enabling these sharded designs requires
two basic primitives: a primitive to reliably make decisions within a clus-
ter and a primitive to reliably communicate between clusters. Unfortu-
nately, such communication has not yet been formally studied.
In this work, we improve on this situation by formalizing the cluster-
sending problem: the problem of sending a message from one resilient
system to another in a fault-tolerant manner. We also establish lower
bounds on the complexity of cluster-sending under both crashes and
Byzantine failures. Finally, we present worst-case optimal cluster-sending
protocols that meet these lower bounds in practical settings. As such, our
work provides a strong foundation for the future development of sharded
resilient data management systems.
Keywords: Byzantine Failures · Sharding · Message Sending ·
Communication Lower Bounds · Worst-Case Optimal Communication
1
Introduction
The emergence of blockchain technology is fueling interest in the development of
new data management systems that can manage data between fully-independent
parties (federated data management) and can provide services continuously, even
during Byzantine failures (e.g., network failure, hardware failure, software fail-
ure, or malicious attacks) [5,13,14,18,20,22]. Recently, this has led to the devel-
opment of several resilient data management systems based on permissioned
blockchain technology [6–9].
Unfortunately, systems based on traditional fully-replicated consensus-based
permissioned blockchain technology lack the scalability required for modern
data management. Consequently, several recent systems have proposed to com-
bine sharding with consensus-based designs (e.g., AHL [3], ByShard [10], and
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 168–186, 2022.
https://doi.org/10.1007/978-3-031-11321-5_10

The Fault-Tolerant Cluster-Sending Problem
169
r1
r2
r3
r4
Cluster
(All Data)
Requests
(All Data)
e1
e2
e3
e4
Cluster
(European Data)
a1
a2
a3
a4
Cluster
(American Data)
Cluster-Sending
(coordination)
Requests
(European Data)
Requests
(Mixed Data)
Requests
(American Data)
Fig. 1. Left, a traditional fully-replicated resilient system in which all four replicas each
hold all data. Right, a sharded design in which each resilient cluster of four replicas
holds only a part of the data. (Color ﬁgure online)
Chainspace [1]). These systems all follow a familiar sharded design: the data is
split up into individual pieces called shards and each shard is managed by diﬀer-
ent independent blockchain-driven clusters. To illustrate the beneﬁts of sharding,
consider a system with a sharded design in which data is kept in local Byzan-
tine fault-tolerant clusters, e.g., as sketched in Fig. 1 by storing data relevant to
American customers on systems located in the United States, whereas systems
located in Europe contain data relevant to European customers. Compared to the
traditional fully-replicated design of blockchain systems, this sharded design will
improve storage scalability by distributing data storage and improve performance
scalability by enabling concurrent transaction processing, e.g., transactions on
American and European data can be performed independently of each other.
At the core of any sharded data processing system are two crucial primi-
tives [17]. First, individual shards need primitives to independently make deci-
sions, e.g., to execute transactions that only aﬀect data held within that shard.
In the setting where each shard is a fault-tolerant cluster, such per-shard decision
making is formalized by the well-known consensus problem, which can be solved
by practical consensus protocols such as Pbft [2]. Second, shards need primitives
to communicate between each other, e.g., to coordinate the execution of trans-
actions that aﬀect data held by multiple shards. Unfortunately, even though
inter-shard communication is a fundamental basic primitive, it has not yet been
studied in much detail. Indeed, existing sharded blockchain-inspired data pro-
cessing systems typically use expensive ad-hoc techniques to enable coordination
between shards (e.g., Chainspace [1] uses expensive all-to-all broadcasts).
In this work, we improve on this situation by formalizing the problem of
inter-shard communication in permissioned fault-tolerant systems: the cluster-
sending problem. In speciﬁc, we fully formalize the cluster-sending problem in
Sect. 2. Then, in Sect. 3, we prove strict lower bounds on the complexity of the
cluster-sending problem that are linear in terms of the number of messages (when
faulty replicas only crash) and in terms of the number of signatures (when faulty
replicas can be malicious and messages are signed via public-key cryptography).

170
J. Hellings and M. Sadoghi
Protocol
System
Robustness
Messages
(size)
BS-cs
Omit
nC1, nC2 > fC1 + fC2
fC1 + fC2 + 1 (optimal)
O(∥v∥)
BS-rs
Byzantine, RS nC1, nC2 > 2fC1 + fC2
2fC1 + fC2 + 1 (optimal)
O(∥v∥)
BS-cs
Byzantine, CS
nC1, nC2 > fC1 + fC2
fC1 + fC2 + 1 (optimal)
O(∥v∥)
PBS-cs
Omit
nC1 > 3fC1, nC2 > 3fC2 O(max(nC1, nC2)) (optimal) O(∥v∥)
PBS-rs
Byzantine, RS nC1 > 4fC1, nC2 > 4fC2 O(max(nC1, nC2)) (optimal) O(∥v∥)
PBS-cs
Byzantine, CS nC1 > 3fC1, nC2 > 3fC2 O(max(nC1, nC2)) (optimal) O(∥v∥)
Chainspace [1] Byzantine, CS nC1 > 3fC1, nC2 > 3fC2
O(nC1 · nC2)
O(∥v∥)
Fig. 2. Overview of cluster-sending protocols that sends a value v of size ∥v∥from
cluster C1 to cluster C2. Cluster Ci, i ∈{1, 2}, has nCi replicas of which fCi are faulty.
The protocol names (ﬁrst column) indicate the main principle the protocol relies on
(BS for bijective sending, and PBS for partitioned bijective sending), and the speciﬁc
variant the protocol is designed for (variant -cs is designed to use cluster signing,
and variant -rs is designed to use replica signing). The system column describe the
type of Byzantine behavior the protocol must deal with (“Omit” for systems in which
Byzantine replicas can drop messages, and “Byzantine” for systems in which Byzantine
replicas have arbitrary behavior) and the signature scheme present in the system (“RS”
is shorthand for replica signing, and “CS” is shorthand for cluster signing).
Next, in Sects. 4 and 5, we introduce bijective sending and partitioned bijec-
tive sending, powerful techniques to provide worst-case optimal cluster-sending
between clusters of roughly the same size (bijective sending) and of arbitrary
sizes (partitioned bijective sending). Finally, in Sect. 6, we evaluate the behav-
ior of the proposed cluster-sending protocols via an in-depth evaluation. In
this evaluation, we show that our worst-case optimal cluster-sending protocols
have exceptionally low communication costs in comparison with existing ad-hoc
approaches from the literature. A full overview of all environmental conditions
in which we study the cluster-sending problem and the corresponding worst-case
optimal cluster-sending protocols we propose can be found in Fig. 2.
Our cluster-sending problem is closely related to cross-chain coordination in
permissionless blockchains such as Bitcoin [16] and Ethereum [21], e.g., as pro-
vided via atomic swaps [11], atomic commitment [24], and cross-chain deals [12].
Unfortunately, such permissionless solutions are not ﬁt for a permissioned envi-
ronment. Although cluster-sending can be solved using well-known permissioned
techniques such as consensus, interactive consistency, Byzantine broadcasts, and
message broadcasting [2,4], the best-case costs for these primitives are much higher
than the worst-case costs of our cluster-sending protocols, making them unsuitable
for cluster-sending. As such, the cluster-sending problem is an independent prob-
lem and our initial results on this problem provide novel directions for the design
and implementation of high-performance resilient data management systems.
2
Formalizing the Cluster-Sending Problem
A cluster C is a set of replicas. We write f(C) ⊆C to denote the set of faulty
replicas in C and nf(C) = C \ f(C) to denote the set of non-faulty replicas in
C. We write nC = |C|, fC = |f(C)|, and nf C = |nf(C)| to denote the number of

The Fault-Tolerant Cluster-Sending Problem
171
Ping round-trip times (ms)
Bandwidth (Mbit/s)
O
I
M
B
T
S
O
I
M
B
T
S
Oregon (O)
≤1 38
65 136 118
161
7998
669
371
194
188
136
Iowa (I )
≤1 33
98 153
172
10004 752
243
144
120
Montreal (M )
≤1 82 186
202
7977 283
111
102
Belgium (B)
≤1 252
270
9728
79
66
Taiwan (T)
≤1
137
7998 160
Sydney (S)
≤1
7977
Fig. 3. Real-world communication costs in Google Cloud, using clusters of n1 machines
deployed in six diﬀerent regions, in terms of the ping round-trip times (which deter-
mines latency) and bandwidth (which determines throughput). These measurements
are reproduced from Gupta et al. [8]. (Color ﬁgure online)
replicas, faulty replicas, and non-faulty replicas in the cluster, respectively. We
extend the notations f(·), nf(·), n(·), f(·), and nf (·) to arbitrary sets of replicas.
We assume that all replicas in each cluster have a predetermined order (e.g., on
identiﬁer or on public address), which allows us to deterministically select any
number of replicas in a unique order from each cluster. In this work, we consider
faulty replicas that can crash, omit messages, or behave Byzantine. A crashing
replica executes steps correctly up till some point, after which it does not execute
anything. An omitting replica executes steps correctly, but can decide to not send
a message when it should or decide to ignore messages it receives. A Byzantine
replica can behave in arbitrary, possibly coordinated and malicious, manners.
A cluster system S is a ﬁnite set of clusters such that communication between
replicas in a cluster is local and communication between clusters is non-local. We
assume that there is no practical bound on local communication (e.g., within a
single data center), while global communication is limited, costly, and to be
avoided (e.g., between data centers in diﬀerent continents). If C1, C2 ∈S are
distinct clusters, then we assume that C1 ∩C2 = ∅: no replica is part of two dis-
tinct clusters. Our abstract model of a cluster system—in which we distinguish
between unbounded local communication and costly global communication—is
supported by practice. E.g., the ping round-trip time and bandwidth measure-
ments of Fig. 3 imply that message latencies between clusters are at least 33–
270 times higher than within clusters, while the maximum throughput is 10–151
times lower, both implying that communication between clusters is up-to-two
orders of magnitude more costly than communication within clusters.
Deﬁnition 1. Let S be a system and C1, C2 ∈S be two clusters with non-faulty
replicas (nf(C1) ̸= ∅and nf(C2) ̸= ∅). The cluster-sending problem is the problem
of sending a value v from C1 to C2 such that: (1.) all non-faulty replicas in C2
receive the value v; (2.) all non-faulty replicas in C1 conﬁrm that the value v
was received by all non-faulty replicas in C2; and (3.) non-faulty replicas in C2
can only receive a value v if all non-faulty replicas in C1 agree upon sending v.
In the following, we assume asynchronous reliable communication: all mes-
sages sent by non-faulty replicas eventually arrive at their destination. None of
the protocols we propose rely on message delivery timings for their correctness.

172
J. Hellings and M. Sadoghi
We assume that communication is authenticated: on receipt of a message m from
replica r ∈C, one can determine that r did send m if r ∈nf(C) and if r ∈nf(C),
then one can only determine that m was sent by r if r did send m. Hence, faulty
replicas are only able to impersonate each other. We study the cluster-sending
problem for Byzantine systems in two types of environments:
1. A system provides replica signing if every replica r can sign arbitrary mes-
sages m, resulting in a certiﬁcate ⟨m⟩r. These certiﬁcates are non-forgeable
and can be constructed only if r cooperates in constructing them. Based on
only the certiﬁcate ⟨m⟩r, anyone can verify that m was supported by r.
2. A system provides cluster signing if it is equipped with a signature scheme
that can be used to cluster-sign arbitrary messages m, resulting in a certiﬁcate
⟨m⟩C. These certiﬁcates are non-forgeable and can be constructed whenever
all non-faulty replicas in nf(C) cooperate in constructing them. Based on only
the certiﬁcate ⟨m⟩C, anyone can verify that m was originally supported by all
non-faulty replicas in C.
In practice, replica signing can be implemented using digital signatures, which
rely on a public-key cryptography infrastructure [15], and cluster signing can be
implemented using threshold signatures, which are available for some public-
key cryptography infrastructures [19]. Let m be a message, C ∈S a cluster,
and r ∈C a replica. We write ∥v∥to denote the size of any arbitrary value v.
We assume that the size of certiﬁcates ⟨m⟩r, obtained via replica signing, and
certiﬁcates ⟨m⟩C, obtained via cluster signing, are both linearly upper-bounded
by ∥m∥. More speciﬁcally, ∥(m, ⟨m⟩r)∥= O(∥m∥) and ∥(m, ⟨m⟩C)∥= O(∥m∥).
When necessary, we assume that replicas in each cluster C ∈S can reach
agreement on a value using an oﬀ-the-shelf consensus protocol [2,23]. In general,
these protocols require nC > 2fC (crash failures) or nC > 3fC (Byzantine failures),
which we assume to be the case for all sending clusters. Finally, in this paper
we use the notation i sgn j, with i, j ≥0 and sgn the sign function, to denote i
if j > 0 and 0 otherwise.
3
Lower Bounds for Cluster-Sending
In the previous section, we formalized the cluster-sending problem. The cluster-
sending problem can be solved intuitively using message broadcasts (e.g., as
used by Chainspace [1]), a principle technique used in the implementation of
Byzantine primitives such as consensus and interactive consistency to assure
that all non-faulty replicas reach the same conclusions. Unfortunately, broadcast-
based protocols have a high communication cost that is quadratic in the size of
the clusters involved. To determine whether we can do better than broadcasting,
we will study the lower bound on the communication cost for any protocol solving
the cluster-sending problem.
First, we consider systems with only crash failures, in which case we can
lower bound the number of messages exchanged. As systems with omit failures
or Byzantine failures can behave as-if they have only crash failures, these lower

The Fault-Tolerant Cluster-Sending Problem
173
C1:
C2:
r1,1
r1,2
r1,3
r1,4
r1,5
r1,6
r1,7
r1,8
r1,9
r1,10
r1,11
r1,12
r1,13
r1,14
r1,15
r2,1
r2,2
r2,3
r2,4
r2,5
Fig. 4. A run of a protocol that sends messages from C1 and C2. The protocol P sends
13 messages, which is one message short of guaranteeing successful cluster-sending.
Hence, to thwart cluster-sending in this particular run we can crash (highlighted using
a red background) fC1 = 7 and fC2 = 2 replicas in C1 and C2, respectively. (Color ﬁgure
online)
bounds apply to all environments. Any lower bound on the number of messages
exchanged is determined by the maximum number of messages that can get lost
due to crashed replicas that do not send or receive messages. If some replicas
need to send or receive multiple messages, the capabilities of crashed replicas to
lose messages is likewise multiplied, as the following example illustrates.
Example 1. Consider a system S with clusters C1, C2 ∈S such that nC1 = 15,
fC1 = 7, nC2 = 5, and fC2 = 2. We assume that S only has crash failures and
that the cluster C1 wants to send value v to C2. We will argue that any correct
cluster-sending protocol P needs to send at least 14 messages in the worst case,
as we can always assure that up-to-13 messages will get lost by crashing fC1
replicas in C1 and fC2 replicas in C2.
Consider the messages of a protocol P that wants to send only 13 messages
from C1 to C2, e.g., the run in Fig. 4. Notice that 13 > nC2. Hence, the run of
P can only send 13 messages to replicas in C2 if some replicas in C2 will receive
several messages. Neither P nor the replicas in C1 know which replicas in C2 have
crashed. Hence, in the worst case, the fC2 = 2 replicas in C2 that received the
most messages have crashed. As we are sending 13 messages and nC2 = 5, the two
replicas that received the most messages must have received at least 6 messages
in total. Hence, out of the 13 messages sent, at least 6 can be considered lost.
In the run of Fig. 4, this loss would happen if r2,1 and r2,2 crash. Consequently,
at most 13 −6 = 7 messages will arrive at non-faulty replicas. These messages
are sent by at most 7 distinct replicas. As fC1 = 7, all these sending replicas
could have crashed. In the run of Fig. 4, this loss would happen if r1,3, r1,4,
r1,5, r1,8, r1,9, r1,10, and r1,13 crash. Hence, we can thwart any run of P that
intends to send 13 messages by crashing fC1 replicas in C1 and fC2 replicas in
C2. Consequently, none of the messages of the run will be sent and received by
non-faulty replicas, assuring that cluster-sending does not happen.
At least fC1 + 1 replicas in C1 need to send messages to non-faulty replicas in
C2 to assure that at least a single such message is sent by a non-faulty replica
in nf(C1) and, hence, is guaranteed to arrive. We combine this with a thorough
analysis along the lines of Example 1 to arrive at the following lower bounds:

174
J. Hellings and M. Sadoghi
Theorem 1. Let S be a system with crash failures, let C1, C2 ∈S, and let
{i, j} = {1, 2} such that nCi ≥nCj. Let qi = (fCi + 1) div nf Cj, ri = (fCi +
1) mod nf Cj, and σi = qinCj +ri +fCj sgn ri. Any protocol that solves the cluster-
sending problem in which C1 sends a value v to C2 needs to exchange at least σi
messages.1
Proof. The proof uses the same reasoning as Example 1: if a protocol sends at
most σi −1 messages, then we can choose fC1 replicas in C1 and fC2 replicas in
C2 that will crash and thus assure that each of the σi −1 messages is either sent
by a crashed replica in C1 or received by a crashed replica in C2.
We assume i = 1, j = 2, and nC1 ≥nC2. The proof is by contradiction.
Hence, assume that a protocol P can solve the cluster-sending problem using at
most σ1 −1 messages. Consider a run of P that sends messages M. Without loss
of generality, we can assume that |M| = σ1 −1. Let R be the top fC2 receivers of
messages in M, let S = C2 \R, let MR ⊂M be the messages received by replicas
in R, and let N = M \ MR. We notice that nR = fC2 and nS = nf C2.
First, we prove that |MR| ≥q1fC2 + fC2 sgn r1, this by contradiction. Assume
|MR| = q1fC2+fC2 sgn r1−v, v ≥1. Hence, we must have |N| = q1nf C2+r1+v−1.
Based on the value r1, we distinguish two cases. The ﬁrst case is r1 = 0. In
this case, |MR| = q1fC2 −v < q1fC2 and |N| = q1nf C2 + v −1 ≥q1nf C2. As
q1fC2 > |MR|, there must be a replica in R that received at most q1−1 messages.
As |N| ≥q1nf C2, there must be a replica in S that received at least q1 messages.
The other case is r1 > 0. In this case, |MR| = q1fC2 + fC2 −v < (q1 + 1)fC2
and |N| = q1nf C2 + r1 + v −1 > q1nf C2. As (q1 + 1)fC2 > |MR|, there must
be a replica in R that received at most q1 messages. As |N| > q1nf C2, there
must be a replica in S that received at least q1 + 1 messages. In both cases,
we identiﬁed a replica in S that received more messages than a replica in R,
a contradiction. Hence, we must conclude that |MR| ≥q1fC2 + fC2 sgn r1 and,
consequently, |N| ≤q1nf C2 + r1 −1 ≤fC1. As nR = fC2, all replicas in R could
have crashed, in which case only the messages in N are actually received. As
|N| ≤fC1, all messages in N could be sent by replicas that have crashed. Hence,
in the worst case, no message in M is successfully sent by a non-faulty replica
in C1 and received by a non-faulty replica in C2, implying that P fails.
⊓⊔
The above lower bounds guarantee that at least one message can be delivered.
Next, we look at systems with Byzantine failures and replica signing. In this case,
at least 2fC1 + 1 replicas in C1 need to send a replica certiﬁcate to non-faulty
replicas in C2 to assure that at least fC1 +1 such certiﬁcates are sent by non-faulty
replicas and, hence, are guaranteed to arrive. Via a similar analysis to the one
of Theorem 1, we arrive at:
1 Example 1 showed that the impact of faulty replicas is minimal if we minimize the
number of messages each replica exchanges. Let nC1 > nC2. If the number of messages
sent to nC2 is not a multiple of nC2, then minimizing the number of messages received
by each replica in nC2 means that some replicas in nC2 will receive one more message
than others: each replica in nC2 will receive at least q1 messages, while the term
r1+fC2 sgn r1 speciﬁes the number of replicas in nC2 that will receive q1+1 messages.

The Fault-Tolerant Cluster-Sending Problem
175
Theorem 2. Let S be a system with Byzantine failures and replica signing,
let C1, C2 ∈S, and let {i, j} = {1, 2} such that nCi ≥nCj. Let q1 = (2fC1 +
1) div nf C2, r1 = (2fC1 + 1) mod nf C2, and τ1 = q1nC2 + r1 + fC2 sgn r1; and
let q2 = (fC2 + 1) div (nf C1 −fC1), r2 = (fC2 + 1) mod (nf C1 −fC1), and τ2 =
q2nC1 + r2 + 2fC1 sgn r2. Any protocol that solves the cluster-sending problem in
which C1 sends a value v to C2 needs to exchange at least τi messages.2
Proof. For simplicity, we assume that each certiﬁcate is sent to C2 in an individ-
ual message independent of the other certiﬁcates. Hence, each certiﬁcate has a
sender and a signer (both replicas in C1) and a receiver (a replica in C2).
First, we prove the case for nC1 ≥nC2 using contradiction. Assume that a
protocol P can solve the cluster-sending problem using at most τ1−1 certiﬁcates.
Consider a run of P that sends messages C, each message representing a single
certiﬁcate, with |C| = τ1 −1. Following the proof of Theorem 1, one can show
that, in the worst case, at most fC1 messages are sent by non-faulty replicas in C1
and received by non-faulty replicas in C2. Now consider the situation in which the
faulty replicas in C1 mimic the behavior in C by sending certiﬁcates for another
value v′ to the same receivers. For the replicas in C2, the two runs behave the
same, as in both cases at most fC1 certiﬁcates for a value, possibly signed by
distinct replicas, are received. Hence, either both runs successfully send values,
in which case v′ is received by C2 without agreement, or both runs fail to send
values. In both cases, P fails to solve the cluster-sending problem.
Next, we prove the case for nC2 ≥nC1 using contradiction. Assume that a pro-
tocol P can solve the cluster-sending problem using at most τ2 −1 certiﬁcates.
Consider a run of P that sends messages C, each message representing a single cer-
tiﬁcate, with |C| = τ2 −1. Let R be the top 2fC1 signers of certiﬁcates in C, let
CR ⊂C be the certiﬁcates signed by replicas in R, and let D = C \ CR. Via a
contradiction argument similar to the one used in the proof of Theorem 1, one can
show that |CR| ≥2q2fC1 + 2fC1 sgn r and |D| ≤q2(nf C1 −fC1) + r −1 = fC2.
As |D| ≤fC2, all replicas receiving these certiﬁcates could have crashed. Hence,
the only certiﬁcates that are received by C2 are in CR. Partition CR into two sets
of certiﬁcates CR,1 and CR,2 such that both sets contain certiﬁcates signed by at
most fC1 distinct replicas. As the certiﬁcates in CR,1 and CR,2 are signed by fC1
distinct replicas, one of these sets can contain only certiﬁcates signed by Byzan-
tine replicas. Hence, either CR,1 or CR,2 could certify a non-agreed upon value v′,
while only the other set certiﬁes v. Consequently, the replicas in C2 cannot distin-
guish between receiving an agreed-upon value v or a non-agreed-upon-value v′. We
conclude that P fails to solve the cluster-sending problem.
⊓⊔
2 Tolerating Byzantine failures in an environment with replica signatures leads to an
asymmetry between the sending cluster C1, in which 2fC1 + 1 replicas need to send,
and the receiving cluster C2, in which only fC2 + 1 replicas need to receive. This
asymmetry results in two distinct cases based on the relative cluster sizes.

176
J. Hellings and M. Sadoghi
4
Cluster-Sending via Bijective Sending
In the previous section, we established lower bounds for the cluster-sending prob-
lem. Next, we develop bijective sending, a powerful technique that allows the
design of eﬃcient cluster-sending protocols that match these lower bounds.
Protocol for the sending cluster C1:
1: All replicas in nf(C1) agree on v and construct ⟨v⟩C1.
2: Choose replicas S1 ⊆C1 and S2 ⊆C2 with nS2 = nS1 = fC1 + fC2 + 1.
3: Choose a bijection b : S1 →S2.
4: for r1 ∈S1 do
5:
r1 sends (v, ⟨v⟩C1) to b(r1).
Protocol for the receiving cluster C2:
6: event r2 ∈nf(C2) receives (w, ⟨w⟩C1) from r1 ∈C1 do
7:
Broadcast (w, ⟨w⟩C1) to all replicas in C2.
8: event r′
2 ∈nf(C2) receives (w, ⟨w⟩C1) from r2 ∈C2 do
9:
r′
2 considers w received.
Fig. 5. BS-cs, the bijective sending cluster-sending protocol that sends a value v from
C1 to C2. We assume Byzantine failures and a system that provides cluster signing.
First, we present a bijective sending protocol for systems with Byzantine
failures and cluster signing. Let C1 be a cluster in which the non-faulty replicas
have reached agreement on sending a value v to a cluster C2 and have access to a
cluster certiﬁcate ⟨v⟩C1. Let Ci, i ∈{1, 2}, be the cluster with the most replicas.
To assure that at least a single non-faulty replica in C1 sends a message to a non-
faulty replica in C2, we use the lower bound of Theorem 1: we choose σi distinct
replicas S1 ⊆C1 and replicas S2 ⊆C2 and instruct each replica in S1 ⊆C1 to
send v to a distinct replica in C2. By doing so, we guarantee that at least a
single message is sent and received by non-faulty replicas and, hence, guarantee
successful cluster-sending. To be able to choose S1 and S2 with nS1 = nS2 = σi,
we need σi ≤min(nC1, nC2), in which case we have σi = fC1 + fC2 + 1. The
pseudo-code for this bijective sending protocol for systems that provide cluster
signing (BS-cs), can be found in Fig. 5. Next, we illustrate bijective sending:
Example 2. Consider system S = {C1, C2} of Fig. 6 with C1 = {r1,1, . . . , r1,8},
f(C1) = {r1,1, r1,3, r1,4}, C2 = {r2,1, . . . , r2,7}, and f(C2) = {r2,1, r2,3}. We have
fC1 + fC2 + 1 = 6 and we choose S1 = {r1,2, . . . , r1,7}, S2 = {r2,1, . . . , r2,6}, and
b = {r1,i →r2,i−1 | 2 ≤i ≤7}. Replica r1,2 sends a valid message to r2,1. As
r2,1 is faulty, it might ignore this message. Replicas r1,3 and r1,4 are faulty and
might not send a valid message. Additionally, r2,3 is faulty and might ignore
any message it receives. The messages sent from r1,5 to r2,4, from r1,6 to r2,5,
and from r1,7 to r2,6 are all sent by non-faulty replicas to non-faulty replicas.
Hence, these messages all arrive correctly.

The Fault-Tolerant Cluster-Sending Problem
177
Having illustrated the concept of bijective sending, as employed by BS-cs,
we are now ready to prove correctness of BS-cs:
Proposition 1. Let S be a system with Byzantine failures and cluster signing
and let C1, C2 ∈S. If nC1 > 2fC1, nC1 > fC1 + fC2, and nC2 > fC1 + fC2, then
BS-cs satisﬁes Deﬁnition 1 and sends fC1 + fC2 + 1 messages, of size O(∥v∥)
each, between C1 and C2.
C1:
C2:
r1,1
r1,2
r1,3
r1,4
r1,5
r1,6
r1,7
r1,8
r2,1
r2,2
r2,3
r2,4
r2,5
r2,6
r2,7
Fig. 6. Bijective sending from C1 to C2. The faulty replicas are highlighted using a
red background. The edges connect replicas r ∈C1 with b(r) ∈C2. Each solid edge
indicates a message sent and received by non-faulty replicas. Each dashed edge indicates
a message sent or received by a faulty replica. (Color ﬁgure online)
Proof. Choose S1 ⊆C1, S2 ⊆C2, and b : S1 →S2 in accordance with BS-cs
(Fig. 5). We have nS1 = nS2 = fC1 + fC2 + 1. Let T = {b(r) | r ∈nf(S1)}. By
construction, we have nf S1 = nT ≥fC2 + 1. Hence, we have nf T ≥1. Due to
Line 5, each replica in nf(T) will receive the message (v, ⟨v⟩C1) from a distinct
replica in nf(S1) and broadcast (v, ⟨v⟩C1) to all replicas in C2. As nf T ≥1,
each replica r′
2 ∈nf(C2) will receive (v, ⟨v⟩C1) from a replica in C2 and meet the
condition at Line 8, proving receipt and conﬁrmation. Finally, we have agreement,
as ⟨v⟩C1 is non-forgeable.
⊓⊔
To provide cluster-sending in environments with only replica signing, we com-
bine the principle idea of bijective sending with the lower bound on the number
of replica certiﬁcates exchanged, as provided by Theorem 2. Let Ci, i ∈{1, 2},
be the cluster with the most replicas. To assure that at least fC1 + 1 non-faulty
replicas in C1 send replica certiﬁcates to non-faulty replicas in C2, we choose sets
of replicas S1 ⊆C1 and S2 ⊆C2 with nS1 = nS2 = τi. To be able to choose
S1 and S2 with nS1 = nS2 = τi, we need τi ≤min(nC1, nC2), in which case we
have τi = 2fC1 + fC2 + 1. The pseudo-code for this bijective sending protocol for
systems that provide replica signing (BS-rs), can be found in Fig. 7. Next, we
prove the correctness of BS-rs:
Proposition 2. Let S be a system with Byzantine failures and replica signing
and let C1, C2 ∈S. If nC1 > 2fC1 + fC2 and nC2 > 2fC1 + fC2, then BS-rs satisﬁes
Deﬁnition 1 and sends 2fC1 + fC2 + 1 messages, of size O(∥v∥) each, between C1
and C2.

178
J. Hellings and M. Sadoghi
Proof. Choose S1 ⊆C1, S2 ⊆C2, and b : S1 →S2 in accordance with BS-rs
(Fig. 7). We have nS1 = nS2 = 2fC1 + fC2 + 1. Let T = {b(r) | r ∈nf(S1)}. By
construction, we have nf S1 = nT ≥fC1 + fC2 + 1. Hence, we have nf T ≥fC1 + 1.
Due to Line 5, each replica in nf(T) will receive the message (v, ⟨v⟩r1) from a
distinct replica r1 ∈nf(S1) and meet the condition at Line 8, proving receipt
and conﬁrmation.
Next, we prove agreement. Consider a value v′ not agreed upon by C1. Hence,
no non-faulty replicas nf(C1) will sign v′. Due to non-forgeability of replica cer-
tiﬁcates, the only certiﬁcates that can be constructed for v′ are of the form ⟨v′⟩r1,
r1 ∈f(C1). Consequently, each replica in C2 can only receive and broadcast up
to fC1 distinct messages of the form (v′, ⟨v′⟩r′
1), r′
1 ∈C1. We conclude that no
non-faulty replica will meet the conditions for v′ at Line 8.
⊓⊔
Protocol for the sending cluster C1:
1: All replicas in nf(C1) agree on v.
2: Choose replicas S1 ⊆C1 and S2 ⊆C2 with nS2 = nS1 = 2fC1 + fC2 + 1.
3: Choose bijection b : S1 →S2.
4: for r1 ∈S1 do
5:
r1 sends (v, ⟨v⟩r1) to b(r1).
Protocol for the receiving cluster C2:
6: event r2 ∈nf(C2) receives (w, ⟨w⟩r′
1) from r′
1 ∈C1 do
7:
Broadcast (w, ⟨w⟩r′
1) to all replicas in C2.
8: event r′
2 ∈nf(C2) receives fC1 + 1 messages (w, ⟨w⟩r′
1):
(i) each message is sent by a replica in C2;
(ii) each message carries the same value w; and
(iii) each message has a distinct signature ⟨w⟩r′
1, r′
1 ∈C1
do
9:
r′
2 considers w received.
Fig. 7. BS-rs, the bijective sending cluster-sending protocol that sends a value v from
C1 to C2. We assume Byzantine failures and a system that provides replica signing.
5
Cluster-Sending via Partitioning
Unfortunately, the worst-case optimal bijective sending techniques introduced in
the previous section are limited to similar-sized clusters:
Example 3. Consider a system S with Byzantine failures and cluster certiﬁcates.
The cluster C1 ∈S wants to send value v to C2 ∈S with nC1 ≥nC2. To do so,
BS-cs requires σ1 = fC1 + fC2 ≤nC2. Hence, BS-cs requires that fC1 is upper-
bounded by nf C2 ≤nC2, which is independent of the size of cluster C1.
Next, we show how to generalize bijective sending to arbitrary-sized clusters.
We do so by partitioning the larger-sized cluster into a set of smaller clusters,
and then letting suﬃcient of these smaller clusters participate independently in
bijective sending. First, we introduce the relevant partitioning notation.

The Fault-Tolerant Cluster-Sending Problem
179
Deﬁnition 2. Let S be a system, let P be a subset of the replicas in S, let
c > 0 be a constant, let q = nP div c, and let r = nP mod c. A c-partition
partition(P) = {P1, . . . , Pq, P ′} of P is a partition of the set of replicas P into
sets P1, . . . , Pq, P ′ such that nPi = c, 1 ≤i ≤q, and nP ′ = r.
Example 4. Consider system S = {C} of Fig. 8 with C = {r1, . . . , r11} and
f(C) = {r1, . . . , r5}. The set partition(C) = {P1, P2, P ′} with P1 = {r1, . . . , r4},
P2 = {r5, . . . , r8}, and P ′ = {r9, r10, r11} is a 4-partition of C. We have f(P1) =
P1, nf(P1) = ∅, and nP1 = fP1 = 4. Likewise, we have f(P2) = {r5}, nf(P2) =
{r6, r7, r8}, nP2 = 4, and fP2 = 1.
Cluster C:
P1
P2
P ′
r1
r2
r3
r4
r5
r6
r7
r8
r9
r10
r11
Fig. 8. An example of a 4-partition of a cluster C with 11 replicas, of which the ﬁrst
ﬁve are faulty. The three partitions are grouped in blue boxes, the faulty replicas are
highlighted using a red background. (Color ﬁgure online)
Next, we apply partitioning to BS-cs. Let C1 be a cluster in which the non-
faulty replicas have reached agreement on sending a value v to a cluster C2 and
constructed ⟨v⟩C1. First, we consider the case nC1 ≥nC2. In this case, we choose a
set P ⊆C1 of σ1 replicas in C1 to sent v to replicas in C2. To minimize the number
of values v received by faulty replicas in C2, we minimize the number of values v
sent to each replica in C2. Conceptually, we do so by constructing an nC2-partition
of the σ1 replicas in P and instruct each resultant set in the partition to perform
bijective sending. The pseudo-code for the resultant sender-partitioned bijective
sending protocol for systems that provide cluster signing, named SPBS-(σ1,cs),
can be found in Fig. 10. In a similar fashion, we can apply partitioning to BS-rs,
in which case we instruct τ1 replicas in C1 to send v to replicas in C2, which yields
the sender-partitioned bijective sending protocol SPBS-(τ1,rs) for systems that
provide replica signing. Next, we illustrate sender-partitioned bijective sending:
Example 5. We continue from Example 1. Hence, we have C1 = {r1,1, . . . , r1,15}
and C2 = {r2,1, . . . , r2,5} with f(C1) = {r1,3, r1,4, r1,5, r1,8, r1,9, r1,10, r1,13} and
f(C2) = {r2,1, r2,2}. We assume that S provides cluster signing and we apply
sender-partitioned bijective sending. We have nC1 > nC2, q1 = 8 div 3 = 2,
r1 = 8 mod 3 = 2, and σ1 = 2 · 5 + 2 + 2 = 14. We choose the replicas
P = {r1,1, . . . , r1,14} ⊆C1 and the nC2-partition partition(P) = {P1, P2, P ′}
with P1 = {r1,1, r1,2, r1,3, r1,4, r1,5}, P2 = {r1,6, r1,7, r1,8, r1,9, r1,10}, and
P ′ = {r1,11, r1,12, r1,13, r1,14}. Hence, SPBS-(σ1,cs) will perform three rounds
of bijective sending. In the ﬁrst two rounds, SPBS-(σ1,cs) will send to all
replicas in C2. In the last round, SPBS-(σ1,cs) will send to the replicas Q =
{r2,1, r2,2, r2,3, r2,4}. We choose bijections b1 = {r1,1 →r2,1, . . . , r1,5 →r2,5},

180
J. Hellings and M. Sadoghi
b2 = {r1,6 →r2,1, . . . , r1,10 →r2,5}, and b′ = {r1,11 →r2,1, . . . , r1,14 →r2,4}.
In the ﬁrst two rounds, we have fP1+fC2 = fP2+fC2 = 3+2 = 5 = nC2. Due to the
particular choice of bijections b1 and b2, these rounds will fail cluster-sending.
In the last round, we have fP ′ + fQ = 1 + 2 = 3 < nP ′ = nQ. Hence, these two
sets of replicas satisfy the conditions of BS-cs, can successfully apply bijective
sending, and we will have successful cluster-sending (as the non-faulty replica
r1,14 ∈C1 will send v to the non-faulty replica r2,4 ∈C2). We have illustrated
the described working of SPBS-(σ1,cs) in Fig. 9.
C1:
C2:
r1,1
r1,2
r1,3
r1,4
r1,5
r1,6
r1,7
r1,8
r1,9
r1,10
r1,11
r1,12
r1,13
r1,14
r1,15
r2,1
r2,2
r2,3
r2,4
r2,5
r2,1
r2,2
r2,3
r2,4
r2,5
r2,1
r2,2
r2,3
r2,4
b1
b1
b1
b1
b1
b2
b2
b2
b2
b2
b′
b′
b′
b′
P1
P2
P ′
Fig. 9. An example of SPBS-(σ1,cs) with σ1 = 14 and partition(P) = {P1, P2, P ′}.
Notice that only the instance of bijective sending with the replicas in P ′ and bijection
b′ will succeed in cluster-sending. (Color ﬁgure online)
Protocol for the sending cluster C1:
1: The agreement step of BS-ζ for value v.
2: Choose replicas P ⊆C1 with nP = α and choose nC2-partition partition(P) of P.
3: for P ∈partition(P) do
4:
Choose replicas Q ⊆C2 with nQ = nP and choose bijection b : P →Q.
5:
for r1 ∈P do
6:
Send v from r1 to b(r1) via the send step of BS-ζ.
Protocol for the receiving cluster C2:
7: See the protocol for the receiving cluster in BS-ζ.
Fig. 10. SPBS-(α,ζ), ζ ∈{cs, rs}, the sender-partitioned bijective sending cluster-
sending protocol that sends a value v from C1 to C2. We assume the same system
properties as BS-ζ.
Next, we prove the correctness of sender-partitioned bijective sending:
Proposition 3. Let S be a system with Byzantine failures, let C1, C2 ∈S, let
σ1 be as deﬁned in Theorem 1, and let τ1 be as deﬁned in Theorem 2.
1. If S provides cluster signing and σ1 ≤nC1, then SPBS-(σ1,cs) satisﬁes
Deﬁnition 1 and sends σ1 messages, of size O(∥v∥) each, between C1 and C2.
2. If S provides replica signing and τ1 ≤nC1, then SPBS-(τ1,rs) satisﬁes Def-
inition 1 and sends τ1 messages, of size O(∥v∥) each, between C1 and C2.

The Fault-Tolerant Cluster-Sending Problem
181
Proof. Let β = (fC1 + 1) in the case of cluster signing and let β = (2fC1 + 1)
in the case of replica signing. Let q = β div nf C2 and r = β mod nf C2. We have
α = qnC2 + r + fC2 sgn r. Choose P and choose partition(P) = {P1, . . . , Pq, P ′}
in accordance with SPBS-(α,ζ) (Fig. 10). For each P ∈P, choose a Q and b in
accordance with SPBS-(α,ζ), and let z(P) = {r ∈P | b(r) ∈f(Q)}. As each
such b has a distinct domain, the union of them is a surjection f : P →C2. By
construction, we have nP ′ = r + fC2 sgn r, nz(P ′) ≤fC2 sgn r, and, for every i,
1 ≤i ≤q, nPi = nC2 and nz(Pi) = fC2. Let V = P \

P ∈partition(P) z(P)

. We
have
Protocol for the sending cluster C1:
1: The agreement step of BS-ζ for value v.
2: Choose replicas P ⊆C2 with nP = α and choose nC1-partition partition(P) of P.
3: for P ∈partition(P) do
4:
Choose replicas Q ⊆C1 with nQ = nP and choose bijection b : Q →P.
5:
for r1 ∈Q do
6:
Send v from r1 to b(r1) via the send step of BS-ζ.
Protocol for the receiving cluster C2:
7: See the protocol for the receiving cluster in BS-ζ.
Fig. 11. RPBS-(α,ζ), ζ ∈{cs, rs}, the receiver-partitioned bijective sending cluster-
sending protocol that sends a value v from C1 to C2. We assume the same system
properties as BS-ζ.
nV ≥nP −(qfC2 + fC2 sgn r) = (qnC2 + r + fC2 sgn r) −(qfC2 + fC2 sgn r) =
qnf C2 + r = β.
Let T = {f(r) | r ∈nf(V )}. By construction, we have nf T = nT . To
complete the proof, we consider cluster signing and replica signing separately.
First, the case for cluster signing. As nV ≥β = fC1 + 1, we have nf V ≥1.
By construction, the replicas in nf(T) will receive the messages (v, ⟨v⟩C1) from
the replicas r1 ∈nf(V ). Hence, analogous to the proof of Proposition 1, we can
prove receipt, conﬁrmation, and agreement. Finally, the case for replica signing.
As nV ≥β = 2fC1 + 1, we have nf V ≥fC1 + 1. By construction, the replicas in
nf(T) will receive the messages (v, ⟨v⟩r1) from each replica r1 ∈nf(V ). Hence,
analogous to the proof of Proposition 2, we can prove receipt, conﬁrmation, and
agreement.
⊓⊔
Finally, we consider the case nC1 ≤nC2. In this case, we apply partitioning
to BS-cs by choosing a set P of σ2 replicas in C2, constructing an nC1-partition
of P, and instruct C1 to perform bijective sending with each set in the partition.
The pseudo-code for the resultant receiver-partitioned bijective sending protocol
for systems that provide cluster signing, named RPBS-(σ2,cs), can be found in
Fig. 11. In a similar fashion, we can apply partitioning to BS-rs, which yields

182
J. Hellings and M. Sadoghi
the receiver-partitioned bijective sending protocol RPBS-(τ2,rs) for systems that
provide replica signing. Next, we prove the correctness of these instances of
receiver-partitioned bijective sending:
Proposition 4. Let S be a system with Byzantine failures, let C1, C2 ∈S, let
σ2 be as deﬁned in Theorem 1, and let τ2 be as deﬁned in Theorem 2.
1. If S provides cluster signing and σ2 ≤nC2, then RPBS-(σ2,cs) satisﬁes
Deﬁnition 1 and sends σ2 messages, of size O(∥v∥) each, between C1 and C2.
2. If S provides replica signing and τ2 ≤nC2, then RPBS-(τ2,rs) satisﬁes Def-
inition 1 and sends τ2 messages, of size O(∥v∥) each, between C1 and C2.
Proof. Let β = nf C1 and γ = 1 in the case of cluster signing and let β =
(nf C1 −fC1) and γ = 2 in the case of replica signing. Let q = (fC2 + 1) div β
and r = (fC2 + 1) mod β. We have α = qnC1 + r + γfC1 sgn r. Choose P and
choose partition(P) = {P1, . . . , Pq, P ′} in accordance with RPBS-(α,ζ) (Fig. 11).
For each P ∈P, choose a Q and b in accordance with RPBS-(α,ζ), and let
z(P) = {r ∈P | b−1(r) ∈f(Q)}. As each such b−1 has a distinct domain,
the union of them is a surjection f −1 : P →C1. By construction, we have
nP ′ = r + γfC1 sgn r, nz(P ′) ≤fC1 sgn r, and, for every i, 1 ≤i ≤q, nPi = nC1
and nz(Pi) = fC1. Let T = P \
 
P ∈partition(P) z(P)

. We have
nT ≥nP −(qfC1 + fC1 sgn r) = (qnC1 + r + γfC1 sgn r) −(qfC1 + fC1 sgn r)
= qnf C1 + r + (γ −1)fC1 sgn r.
To complete the proof, we consider cluster signing and replica signing
separately.
First, the case for cluster signing. We have β = nf C1 and γ = 1. Hence,
nT ≥qnf C1 +r+(γ−1)fC1 sgn r = qβ+r = fC2 +1. We have nf T ≥nT −fC2 ≥1.
Let V = {f −1(r) | r ∈nf(T)}. By construction, we have nf V = nV and we have
nf V ≥1. Consequently, the replicas in nf(T) will receive the messages (v, ⟨v⟩C1)
from the replicas r1 ∈nf(V ). Analogous to the proof of Proposition 1, we can
prove receipt, conﬁrmation, and agreement.
Finally, the case for replica signing. We have β = nf C1 −fC1 and γ = 2.
Hence, nT ≥qnf C1 + r + (γ −1)fC1 sgn r = q(β + fC1) + r + fC1 sgn r = (qβ + r) +
qfC1 +fC1 sgn r = (fC2 +1)+qfC1 +fC1 sgn r. We have nf T ≥qfC1 +fC1 sgn r +1 =
(q + sgn r)fC1 + 1. As there are (q + sgn r) non-empty sets in partition(P), there
must be a set P ∈P with nP ∩nf T ≥fC1 +1. Let b be the bijection chosen earlier
for P and let V = {b−1(r) | r ∈(P ∩nf T )}. By construction, we have nf V = nV
and we have nf V ≥fC1 + 1. Consequently, the replicas in nf(T) will receive the
messages (v, ⟨v⟩r1) from each replica r1 ∈nf(V ). Hence, analogous to the proof
of Proposition 2, we can prove receipt, conﬁrmation, and agreement.
⊓⊔
The bijective sending cluster-sending protocols, the sender-partitioned bijec-
tive cluster-sending protocols, and the receiver-partitioned bijective cluster-
sending protocols each deal with diﬀerently-sized clusters. By choosing the appli-
cable protocols, we have the following:

The Fault-Tolerant Cluster-Sending Problem
183
Corollary 1. Let S be a system, let C1, C2 ∈S, let σ1 and σ2 be as deﬁned in
Theorem 1, and let τ1 and τ2 be as deﬁned in Theorem 2. Consider the cluster-
sending problem in which C1 sends a value v to C2.
1. If nC > 3fC, C ∈S, and S has crash failures, omit failures, or Byzantine
failures and cluster signing, then BS-cs, SPBS-(σ1,cs), and RPBS-(σ2,cs)
are a solution to the cluster-sending problem with optimal message complex-
ity. These protocols solve the cluster-sending problem using O(max(nC1, nC2))
messages, of size O(∥v∥) each.
2. If nC > 4fC, C ∈S, and S has Byzantine failures and replica signing, then
BS-rs, SPBS-(τ1,rs), and RPBS-(τ2,rs) are a solution to the cluster-sending
problem with optimal replica certiﬁcate usage. These protocols solve the cluster-
sending problem using O(max(nC1, nC2)) messages, of size O(∥v∥) each.
6
Performance Evaluation
In the previous sections, we introduced worst-case optimal cluster-sending proto-
cols. To gain further insight in the performance attainable by these protocols, we
implemented these protocols in a simulated sharded resilient system environment
that allows us to control the faulty replicas in each cluster. In the experiments, we
used equal-sized clusters, which corresponds to the setup used by recent sharded
consensus-based system proposals [1,3,10]. Hence, we used only the bijective
cluster-sending protocols BS-cs and BS-rs. As a baseline of comparison, we also
evaluated the broadcast-based cluster-sending protocol of Chainspace [1] that
can perform cluster-sending using nC1 · nC2 messages. We refer to Fig. 2 for a
theoretical comparison between our cluster-sending protocols and the protocol
utilized by Chainspace. Furthermore, we have implemented MC-cs and MC-rs,
two multicast-based cluster-sending protocols, one using cluster signing and the
other using replica signing), that work similar to the protocol of Chainspace,
but minimize the number of messages to provide cluster-sending.
In the experiment, we measured the number of messages exchanged as a
function of the number of faulty replicas. In speciﬁc, we measured the number
of messages exchanged in 10 000 runs of the cluster-sending protocols under
consideration. In each run we measure the number of messages exchanged when
sending a value v from a cluster C1 to a cluster C2 with nC1 = nC2 = 3fC1 + 1 =
3fC2 +1, and we aggregate this data over 10 000 runs. Furthermore, we measured
in each run the number of messages exchanged between non-faulty replicas, as
these messages are necessary to guarantee cluster-sending. The results of the
experiment can be found in Fig. 12.
As is clear from the results, our worst-case optimal cluster-sending proto-
cols are able to out-perform existing cluster-sending protocols by a wide margin,
which is a direct consequence of the diﬀerence between quadratic message com-
plexity (Chainspace, MC-cs, and MC-rs) and a worst-case optimal linear message
complexity (BS-cs and BS-rs). As can be seen in Fig. 12, right, our protocols
do so by massively cutting back on sending messages between faulty replicas,
while still ensuring that in all cases suﬃcient messages are exchanged between
non-faulty replicas (thereby assuring cluster-sending).

184
J. Hellings and M. Sadoghi
1
5
9
13
17
21
25
29
33
0.0
0.2
0.4
0.6
0.8
1.0
·108
Number of faulty replicas f
Messages Exchanged
Cluster-Sending performance
BS-cs
BS-rs
MC-cs
MC-rs
Chainspace [1]
1
5
9
13
17
21
25
29
33
0.0
0.5
1.0
1.5
2.0
·106
Number of faulty replicas f
(Zoomed)
1
5
9
13
17
21
25
29
33
0.0
0.2
0.4
0.6
0.8
1.0
·102
Number of faulty replicas f
(minimum, over all runs)
Messages exchanged between non-faulty
replicas (Zoomed)
Fig. 12. A comparison of the number of message exchange steps as a function of the
number of faulty replicas in both clusters by our worst-case optimal cluster-sending
protocols BS-cs and BS-rs, and by three protocols based on the literature. For each
protocol, we measured the number of message exchange steps to send 10 000 values
between two equally-sized clusters, each cluster having n = 3f +1 replicas. The dashed
lines in the plot on the right indicate the minimum number of messages that need to be
exchanged between non-faulty replicas for the protocols BS-cs and BS-rs, respectively,
to guarantee cluster-sending (no protocol can do better). (Color ﬁgure online)
7
Conclusion
In this paper, we identiﬁed and formalized the cluster-sending problem, a fun-
damental primitive in the design and implementation of blockchain-inspired
sharded fault-tolerant data processing systems. Not only did we formalize the
cluster-sending problem, we also proved lower bounds on the complexity of this
problem. Furthermore, we developed bijective sending and partitioned bijective
sending, two powerful techniques that can be used in the construction of prac-
tical cluster-sending protocols with optimal complexity that matches the lower
bounds established. We believe that our work provides a strong foundation for
future blockchain-inspired sharded fault-tolerant data processing systems that
can deal with Byzantine failures and the challenges of large-scale data processing.
Our fundamental results open a number of key research avenues to further
high-performance fault-tolerant data processing. First, we are interested in fur-
ther improving our understanding of cluster-sending, e.g., by establishing lower
bounds on cluster-sending in the absence of public-key cryptography and in
the absence of reliable networks. Second, we are interested in improved cluster-
sending protocols that can perform cluster-sending with less-than a linear num-
ber of messages, e.g., by using randomization or by optimizing for cases without
failures. Finally, we are interested in putting cluster-sending protocols to practice
by incorporating them in the design of practical sharded fault-tolerant systems,
thereby moving even closer to general-purpose high-performance fault-tolerant
data processing.

The Fault-Tolerant Cluster-Sending Problem
185
References
1. Al-Bassam, M., Sonnino, A., Bano, S., Hrycyszyn, D., Danezis, G.: Chainspace: a
sharded smart contracts platform (2017). http://arxiv.org/abs/1708.03778
2. Castro, M., Liskov, B.: Practical byzantine fault tolerance and proactive recov-
ery. ACM Trans. Comput. Syst. 20(4), 398–461 (2002). https://doi.org/10.1145/
571637.571640
3. Dang, H., Dinh, T.T.A., Loghin, D., Chang, E.C., Lin, Q., Ooi, B.C.: Towards
scaling blockchain systems via sharding. In: Proceedings of the 2019 International
Conference on Management of Data. pp. 123–140. ACM (2019). https://doi.org/
10.1145/3299869.3319889
4. Dolev, D., Strong, H.R.: Authenticated algorithms for byzantine agreement. SIAM
J. Comput. 12(4), 656–666 (1983). https://doi.org/10.1137/0212045
5. Gordon, W.J., Catalini, C.: Blockchain technology for healthcare: Facilitating the
transition to patient-driven interoperability. Comput. Struct. Biotechnol. J. 16,
224–230 (2018). https://doi.org/10.1016/j.csbj.2018.06.003
6. Gupta, S., Hellings, J., Sadoghi, M.: Fault-Tolerant distributed transactions on
Blockchain. Synthesis Lectures on Data Management, Morgan & Claypool (2021).
https://doi.org/10.1007/978-3-031-01877-0
7. Gupta, S., Hellings, J., Sadoghi, M.: RCC: resilient concurrent consensus for high-
throughput secure transaction processing. In: 2021 IEEE 37th International Con-
ference on Data Engineering (ICDE), pp. 1392–1403. IEEE (2021). https://doi.
org/10.1109/ICDE51399.2021.00124
8. Gupta, S., Rahnama, S., Hellings, J., Sadoghi, M.: ResilientDB: global scale
resilient blockchain fabric. Proc. VLDB Endow. 13(6), 868–883 (2020). https://
doi.org/10.14778/3380750.3380757
9. Gupta, S., Rahnama, S., Hellings, J., Sadoghi, M.: Proof-of-execution: reaching
consensus through fault-tolerant speculation. In: Proceedings of the 24th Inter-
national Conference on Extending Database Technology (EDBT), pp. 301–312.
OpenProceedings.org (2021). https://doi.org/10.5441/002/edbt.2021.27
10. Hellings, J., Sadoghi, M.: ByShard: sharding in a byzantine environment.
Proc. VLDB Endow. 14(11), 2230–2243 (2021). https://doi.org/10.14778/3476249.
3476275
11. Herlihy, M.: Atomic cross-chain swaps. In: Proceedings of the 2018 ACM Sympo-
sium on Principles of Distributed Computing. pp. 245–254. ACM (2018). https://
doi.org/10.1145/3212734.3212736
12. Herlihy, M., Liskov, B., Shrira, L.: Cross-chain deals and adversarial commerce.
Proc. VLDB Endow. 13(2), 100–113 (2019). https://doi.org/10.14778/3364324.
3364326
13. Kamel Boulos, M.N., Wilson, J.T., Clauson, K.A.: Geospatial blockchain: promises,
challenges, and scenarios in health and healthcare. Int. J. Health Geogr. 17(1),
1211–1220 (2018). https://doi.org/10.1186/s12942-018-0144-x
14. Lao, L., Li, Z., Hou, S., Xiao, B., Guo, S., Yang, Y.: A survey of IoT applications in
blockchain systems: architecture, consensus, and traﬃc modeling. ACM Comput.
Surv. 53(1) (2020). https://doi.org/10.1145/3372136
15. Menezes, A.J., Vanstone, S.A., Oorschot, P.C.V.: Handbook of Applied Cryptog-
raphy, 1st edn. CRC Press Inc., Boca Raton (1996)
16. Nakamoto, S.: Bitcoin: a peer-to-peer electronic cash system. https://bitcoin.org/
en/bitcoin-paper

186
J. Hellings and M. Sadoghi
17. ¨Ozsu, M.T., Valduriez, P.: Principles of Distributed Database Systems. (2020).
https://doi.org/10.1007/978-3-030-26253-2
18. Rejeb, A., Keogh, J.G., Zailani, S., Treiblmaier, H., Rejeb, K.: Blockchain tech-
nology in the food industry: a review of potentials, challenges and future research
directions. Logistics 4(4) (2020). https://doi.org/10.3390/logistics4040027
19. Shoup, V.: Practical threshold signatures. In: Preneel, B. (ed.) EUROCRYPT 2000.
LNCS, vol. 1807, pp. 207–220. Springer, Heidelberg (2000). https://doi.org/10.
1007/3-540-45539-6 15
20. Treiblmaier, H., Beck, R. (eds.): Business Transformation Through Blockchain.
Springer, Cham (2019). https://doi.org/10.1007/978-3-319-98911-2
21. Wood, G.: Ethereum: a secure decentralised generalised transaction ledger. https://
gavwood.com/paper.pdf, EIP-150 revision
22. Wu, M., Wang, K., Cai, X., Guo, S., Guo, M., Rong, C.: A comprehensive survey
of blockchain: from theory to IoT applications and beyond. IEEE Internet Things
J. 6(5), 8114–8154 (2019). https://doi.org/10.1109/JIOT.2019.2922538
23. Yin, M., Malkhi, D., Reiter, M.K., Gueta, G.G., Abraham, I.: HotStuﬀ: BFT con-
sensus with linearity and responsiveness. In: Proceedings of the 2019 ACM Sympo-
sium on Principles of Distributed Computing, pp. 347–356. ACM (2019). https://
doi.org/10.1145/3293611.3331591
24. Zakhary, V., Agrawal, D., El Abbadi, A.: Atomic commitment across blockchains.
Proc. VLDB Endow. 13(9), 1319–1331 (2020). https://doi.org/10.14778/3397230.
3397231

Optimizing Multiset Relational Algebra
Queries Using Weak-Equivalent
Rewrite Rules
Jelle Hellings1(B), Yuqing Wu2, Dirk Van Gucht3, and Marc Gyssens4
1 McMaster University, 1280 Main St., W., Hamilton, ON L8S 4L7, Canada
jhellings@mcmaster.ca
2 Pomona College, 185 E 6th St., Claremont, CA 91711, USA
3 Indiana University, 919 E 10th St., Bloomington, IN 47408, USA
4 Hasselt University, Martelarenlaan 42, 3500 Hasselt, Belgium
Abstract. Relational query languages rely heavily on costly join opera-
tions to combine tuples from multiple tables into a single resulting tuple.
In many cases, the cost of query evaluation can be reduced by manually
optimizing (parts of) queries to use cheaper semi-joins instead of joins.
Unfortunately, existing database products can only apply such optimiza-
tions automatically in rather limited cases.
To improve on this situation, we propose a framework for automatic
query optimization via weak-equivalent rewrite rules for a multiset rela-
tional algebra (that serves as a faithful formalization of core SQL). The
weak-equivalent rewrite rules we propose aim at replacing joins by semi-
joins. To further maximize their usability, these rewrite rules do so by
only providing “weak guarantees” on the evaluation results of rewrit-
ten queries. We show that, in the context of certain operators, these
weak-equivalent rewrite rules still provide strong guarantees on the ﬁnal
evaluation results of the rewritten queries.
Keywords: Query Optimization · Relational Algebra · Multiset
Semantics · Semi-Joins
1
Introduction
To combine tables, SQL relies on join operations that are costly to evaluate. To
reduce the high costs of joins, a signiﬁcant portion of query optimization and
query planning is aimed at evaluating joins as eﬃcient as possible. Still, it is
well-known that some complex join-based SQL queries can be further optimized
manually by rewriting these queries into queries that involve semi-joins implic-
itly. As an illustration, we consider the following example involving a graph
represented as a binary relation. The SQL query
SELECT DISTINCT S.nfrom
FROM edges S, edges R, edges T, edges U
WHERE S.nto = R.nfrom AND R.nto = T.nfrom AND T.nto = U.nfrom;
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 187–205, 2022.
https://doi.org/10.1007/978-3-031-11321-5_11

188
J. Hellings et al.
computes the sources of paths of length four. A straightforward way to evaluate
this query is to compute the joins, then project the join result onto S.nfrom,
and, ﬁnally, remove duplicates. The cost of this straightforward evaluation is
very high: in the worst-case, the join result is quartic in size with respect to the
size of the number of edges (the size of the Edges relation). The typical way to
manually optimize this query is to rewrite it as follows:
SELECT DISTINCT nfrom FROM edges
WHERE nto IN (SELECT nfrom FROM edges
WHERE nto IN (SELECT nfrom FROM edges
WHERE nto IN (SELECT nfrom
FROM edges)));
In most relational database systems, the WHERE ... IN ... clauses in the
rewritten query are evaluated using a semi-join algorithm. In doing so, this query
can be evaluated in linear time with respect to the number of edges, which is a
signiﬁcant improvement. E.g., when evaluated on a randomly generated database
with 75 000 rows (each row a single edge) managed by PostgreSQL 14.1, the
original query could not ﬁnish in a reasonable amount of time, whereas the
manually rewritten semi-join style query ﬁnished in 90 ms.
We believe that query optimizers should not require users to manually rewrite
queries to enforce particular evaluation strategies: manual rewriting goes against
the advantages of using high-level declarative languages such as SQL. Instead,
we want query optimizers to be able to recognize situations in which semi-join
rewriting is appropriate, and apply such optimizing rewritings automatically.
Traditional approaches towards query optimization for SQL and the rela-
tional algebra usually employ two basic steps [12]. First, the query involved is
rewritten. The rewrite rules used in these rewrites guarantee strong-equivalence:
the original subquery and the rewritten subquery always evaluate to the same
result. Examples of such rules are the well-known push-down rules for selection
and projection, which can reduce the size of intermediate query results signiﬁ-
cantly. Second, the order of execution of the operations, appropriate algorithms
to perform each operation, and data access methods are chosen to evaluate the
rewritten query.
Unfortunately, requiring strong-equivalence imposes a severe restriction on
the rewrite rules that can be considered. As a consequence, there are signiﬁcant
limitations to query optimization using traditional query rewriting: often, these
rewrite rules only manipulate the order of operations. More lucrative rewritings,
such as replacing expensive join operations by semi-joins, are not considered
because such rewrites cannot guarantee strong-equivalence.
To improve on this situation, we propose the concept of weak-equivalence,
which is a relaxation of strong-equivalence. Weak-equivalent rewrite rules only
guarantee that the original subquery and the rewritten subquery evaluate to
the same result up to duplicate elimination (with respect to the attributes of
interest). The rewrite rules we propose are aimed at eliminating joins in favor
of semi-joins and eliminating the need for deduplication altogether. To illustrate
the beneﬁts of weak-equivalent rewrites, we present two examples.

Optimizing Multiset Relational Algebra Queries
189
As a ﬁrst example, consider a university database containing the relation
Course, with attributes id and name, and the relation Enroll, with, among its
attributes, cid (the course id). Other attributes of Enroll refer to students. Now
consider the task of rewriting the relational algebra query
πC.name(q) with q = ρC(Course) ⋊⋉C.id=E.cid ρE(Enroll)
that computes the set of names of courses in which students are eﬀectively
enrolled. As the end result of this query is a projection on only the name
attribute of Course (which we assume to be a key), any rewrite of the subquery
q can forgo any of the other attributes. E.g., although subquery q yields a com-
pletely diﬀerent result than subquery q′ = ρC(Course) ⋉C.id=E.cid ρE(Enroll),
their projection onto C.name is identical.
As a second example, consider a sales database containing the relation Cus-
tomer, with among its attributes cname, the relation Product, with among its
attributes pname and type, and the relation Bought, with among its attributes
cname and pname. We refer to Fig. 1 for an instance of this sales database. Now
consider the following query:
SELECT DISTINCT C.cname, P.type
FROM customer C, bought B, product P
WHERE C.cname = B.cname AND B.pname = P.pname AND
P.type = ’food’;
which our rules can rewrite into the following query:
SELECT cname, ’food’ AS type FROM customer WHERE cname IN (
SELECT cname FROM bought WHERE pname IN (
SELECT pname FROM product WHERE type = ’food’));
Observe that the rewritten query does not perform any joins, even though
information from several relations is combined and returned. Moreover, the need
for deduplication is eliminated, as the available key on cname guarantees that
no duplicates are possible in the resultant query. In this particular example, the
original query joins table Bought with two tables on their primary keys. Hence,
all intermediate results remain small if query evaluation chooses a proper join
order. Still, even in this case, the rewritten query evaluates 15%–20% faster on a
randomly generated database with 500 customers, 24 077 products, and 100 000
sale records in Bought.
The latter example also illustrates that the applicability of weak-equivalent
rewrite rules may depend on structural properties, such as keys, of the input
relations and of the intermediate query results. Therefore, we also propose tech-
niques to derive these properties from the available schema information.
The automatic use of semi-join algorithms in query evaluation has already
been studied and applied before. In these approaches, however, semi-joins are
typically only employed as a preprocessing step for joins. E.g., in distributed
databases, semi-joins are used as an intermediate step to reduce the size of

190
J. Hellings et al.
Customer
Product
Bought
cname age
pname
type
cname pname price
Alice 19
apple
food
Alice apple
0.35
Bob
20
apple
fruit
Alice apple
0.35
Eve
21
banana
fruit
Bob
apple
0.45
car
non-food
Bob
banana 0.50
Eve
car
10000
Fig. 1. A database instance for a sales database that has customers, a categorization
of products according to their types, and transaction information for each sale.
relations before joining them and, as a consequence, reducing the communica-
tion overhead of distributing (intermediate) relations to other computational
nodes [3]. The semi-join has a similar role in the well-known acyclic multi-join
algorithm of Yannakakis [13]. We take a diﬀerent approach: using semi-joins, we
aim at eliminating join operations altogether instead of merely reducing their
cost.
The usage of weak-equivalent rewrite rules for query optimization is inspired
by the projection-equivalent rewriting techniques for graph query languages on
binary relations proposed by Hellings et al. [6,7]. In the current paper, we not
only adapt these projection-equivalent rewriting techniques to the setting of
relational algebra, but we also extend them to eﬀectively deal with multiset
semantics [2,4,5,9–11]. The latter is essential, because we want to use our weak-
equivalent rewriting rules to optimize SQL queries, and SQL has multiset seman-
tics. Furthermore, we integrate rewrite techniques based on derived structural
knowledge on the schema and knowledge derived from selection conditions, which
are both not applicable in the setting of binary relations.
Finally, we note that there have been previous SQL-inspired studies for rewrit-
ing and optimizing relational algebra with multiset semantics, e.g., [5,11]. These
studies do not cover the main optimizations explored in this work, however.
2
Preliminaries
We consider disjoint inﬁnitely enumerable sets U and N of constants and names,
respectively. A relation schema is a ﬁnite set of names, which are used as
attributes. Let a ⊆N be a relation schema. A tuple over a is a function t : a →U,
a relation over a ⊆N is a set of tuples over a, and a multiset relation over a
is a pair ⟨R; τ⟩, in which R is a relation over a and τ : R →N+ is a function
mapping tuples t ∈R to the number of copies of t in the multiset relation. We
say that ⟨R; τ⟩is a set relation if, for every t ∈R, we have τ(t) = 1. We write
(t : n) ∈⟨R; τ⟩for tuple-count pair (t : n) to indicate t ∈R with τ(t) = n. We
write t ∈⟨R; τ⟩to indicate t ∈R and we write t /∈⟨R; τ⟩to indicate t /∈R.
Let t, t1, and t2 be tuples over the relation schema a and let b ⊆a. The
restriction of t to b, denoted by t|b, is deﬁned by t|b = {a →t(a) | a ∈b}.
Tuples t1 and t2 agree on b, denoted by t1 ≡b t2, if t1|b = t2|b. Let ⟨R; τ⟩be a

Optimizing Multiset Relational Algebra Queries
191
multiset relation over relation schema a and let k ⊆a. We say that k is a key of
⟨R; τ⟩if, for every pair of tuples t1, t2 ∈⟨R; τ⟩with t1 ≡k t2, we have t1 = t2.
A database schema is a 4-tuple D = (N, A, K, S), in which N ⊆N is a set of
relation names, A is a function mapping each relation name to a relation schema,
K is a function mapping each relation name to a set of sets of attributes of the
corresponding relation schema, and S is a function mapping relation names to
booleans. A database instance over schema D = (N, A, K, S) is a function I
mapping each name R ∈N into a multiset relation. This multiset relation I(R)
has schema A(R), keys K(R), and must be a set relation if S(R) = true.
Let t1 and t2 be tuples over relation schemas a1 and a2, respectively, such
that t1 ≡a1∩a2 t2. The concatenation of t1 and t2, denoted by t1 · t2, is deﬁned
by t1 · t2 = t1 ∪t2.1 Notice that t1 · t2 is a tuple over a1 ∪a2. Let a be a relation
schema. A condition on a is either an expression of the form a1 = a2 or of
the form a = u, a, a1, a2 ∈a and u ∈U. A tuple t over a satisﬁes a1 = a2 if
t(a1) = t(a2) and satisﬁes a = u if t(a) = u. If E is a set of conditions over a,
then tuple t satisﬁes E if t satisﬁes each condition in E. By attrs(E), we denote
the set of attributes used in conditions in E.
Example 1. The database schema for the sales database of Fig. 1 consist of
the named multiset relations Customer with schema {cname, age}, Product
with schema {pname, type}, and Bought with schema {cname, pname, price}.
Customer and Product are set relations with key {cname} and trivial key
{pname, type}, respectively. Bought is not a set relation and only has the trivial
key {cname, pname, price}.
3
Multiset Relational Algebra
In this work, we study the multiset relational algebra. To distinguish a tradi-
tional set-based relational algebra operator ⊕from its multiset relational algebra
counterpart introduced below, we annotate the latter as ˙⊕.
Let D = (N, A, K, S) be a database schema and let I be a database instance
over D. If e is a multiset relational algebra expression over D, which we will
formally deﬁne next, then we write S(e; D) to denote the schema of the multiset
relation obtained by evaluating e on an instance over D, and we write [[e]]I
to denote the evaluation of e on instance I. The standard relational algebra
expressions over D are built from the following operators:
– Multiset relation. If R ∈N is a relation name, then R is an expression with
S(R; D) = A(R) and [[R]]I = I(R).
– Selection.2 If e is an expression and E is a set of conditions on S(e; D), then
˙σE(e) is an expression with S( ˙σE(e); D) = S(e; D) and [[ ˙σE(e)]]I = {(t : n) ∈
[[e]]I | (t satisﬁes E)}.
1 Every occurrence of the operators ∪, ∩, and −in this paper is to be interpreted
using standard set semantics.
2 We only consider conjunctions of conditions in the selection operator because more
general boolean combinations of conditions do not provide additional opportunities
for rewriting in our framework.

192
J. Hellings et al.
– Projection. If e is an expression and b ⊆S(e; D), then ˙πb(e) is an expression
with S( ˙πb(e); D) = b and [[ ˙πb(e)]]I = {(t|b :count(t|b, e)) | t ∈[[e]]I}, in which
count(t|b, e) = 
((s:m)∈[[e]]I)∧(s≡bt) m.
– Renaming. If e is an expression, b ⊆N, and f : S(e; D) →b is a bijec-
tion, then ˙ρf(e) is an expression with S( ˙ρf(e); D) = b and [[ ˙ρf(e)]]I =
{(rename(t, f) : m) | (t : m) ∈[[e]]I}, in which rename(t, f) = {f(a) →t(a) |
a ∈S(e; D)}.
– Deduplication. If e is an expression, then ˙δ(e) is an expression with S( ˙δ(e);
D) = A(e) and [[ ˙δ(e)]]I = {(t : 1) | (t : n) ∈[[e]]I}.
– Union, intersection, and diﬀerence.3 If e1 and e2 are expressions such that
a = S(e1; D) = S(e2; D), then, for ⊕∈{∪, ∩, −}, e1 ˙⊕e2 is an expression
with S(e1 ˙⊕e2; D) = a and
[[e1 ˙∪e2]]I = {(t : n1 + n2) | (t : n1) ∈[[e1]]I ∧(t : n2) ∈[[e2]]I} ∪
{(t : n) ∈[[e1]]I | t /∈[[e2]]I} ∪{(t : n) ∈[[e2]]I | t /∈[[e1]]I};
[[e1 ˙∩e2]]I = {(t : min(n1, n2)) | (t : n1) ∈[[e1]]I ∧(t : n2) ∈[[e2]]I};
[[e1 ·−e2]]I = {(t : n) ∈[[e1]]I | t /∈[[e2]]I} ∪
{(t : n1 −n2) | (n1 > n2) ∧(t : n1) ∈[[e1]]I ∧(t : n2) ∈[[e2]]I}.
– θ-join and natural join.4 If e1 and e2 are expressions and E is a set of con-
ditions on a = S(e1; D) ∪S(e2; D), then e1 ·⋊⋉E e2 is an expression with
S(e1 ·⋊⋉E e2; D) = a and [[e1 ·⋊⋉E e2]]I is deﬁned by
{(t1 · t2 : n1 · n2) | (t1 : n1) ∈[[e1]]I ∧(t2 : n2) ∈[[e2]]I ∧
t1 ≡S(e1;D)∩S(e2;D) t2 ∧(t1 · t2 satisﬁes E)}.
If E = ∅, then we simply write e1 ·⋊⋉e2 (the natural join).
Example 2. Consider the database instance I of Example 1, which is visualized
in Fig. 1. The expression e = ˙πage( ˙σtype=non-food(Customer ·⋊⋉Bought ·⋊⋉Product))
returns the ages of people that bought non-food products: as Eve bought a car,
we have [[e]]I = {(age →21:1)}. If we change the condition type = non-food into
type = food, resulting in the expression e′, then [[e′]]I = {(age →19 : 2), (age →
20 : 1)}, which includes Alice’s age twice as she bought two apples. Observe that
we have [[ ˙δ(e′)]]I = {(age →19 : 1), (age →20 : 1)}.
The extended multiset relational algebra. We aim at reducing the complexity
of query evaluation by rewriting joins into semi-joins, which are not part of the
standard relational algebra described above. Extended relational algebra expres-
sions over D are built from the standard relational algebra operators and the
following additional ones:
3 The set operators we deﬁne have the same semantics as the UNION ALL, INTERSECT
ALL, and EXCEPT ALL operators of standard SQL [8]. The semantics of UNION,
INTERSECT, and EXCEPT can be obtained using deduplication.
4 To simplify presentation, the θ-join and the θ-semi-join also perform equi-join on all
attributes common to the multiset relations involved.

Optimizing Multiset Relational Algebra Queries
193
– θ-semi-join and semi-join. If e1 and e2 are expressions and E is a set of
conditions on S(e1; D)∪S(e2; D), then e1 ·⋉Ee2 is an expression with S(e1 ·⋉E
e2; D) = S(e1; D) and [[e1 ·⋉E e2]]I is deﬁned by
{(t1 : n1) ∈[[e1]]I | ∃t2 (t2 ∈[[e2]]I ∧
t1 ≡S(e1;D)∩S(e2;D) t2 ∧(t1 · t2 satisﬁes E))}.
If E = ∅, then we simply write e1 ·⋉e2 (the semi-join).
– Max-union.5 If e1 and e2 are expressions such that a = S(e1; D) = S(e2; D),
then e1 ˙⊔e2 is an expression with S(e1 ˙⊔e2; D) = a and
[[e1 ˙⊔e2]]I = {(t : max(n1, n2)) | (t : n1) ∈[[e1]]I ∧(t : n2) ∈[[e2]]I} ∪
{(t : n) ∈[[e1]]I | t /∈[[e2]]I} ∪{(t : n) ∈[[e2]]I | t /∈[[e1]]I};
– Attribute introduction.6 If e is an expression, b is a set of attributes with
b ∩S(e; D) = ∅, and f = {b := x | b ∈b ∧x ∈(S(e; D) ∪U)} is a set of
assignment-pairs, then ˙ιf(e) is an expression with S(˙ιf(e); D) = S(e; D) ∪b
and [[˙ιf(e)]]I = {(t · {b →value(t, x) | (b := x) ∈f} : m) | (t : m) ∈[[e]]I}, in
which value(t, x) = t(x) if x ∈S(e; D) and value(t, x) = x otherwise.
The extended relational algebra provides all the operators used in our frame-
work and the following example illustrates the use of these operators:
Example 3. Consider the database instance I of Examples 1 and 2. Let e =
˙σtype=food(Customer ·⋊⋉Bought ·⋊⋉product). The query ˙δ( ˙πage(e)) is equivalent to
˙δ( ˙πage(Customer ·⋉(Bought ·⋉type=food Product))). The query ˙δ( ˙πcname,type(e))
is equivalent to ˙ιtype:=food( ˙πcname(Customer ·⋉(Bought ·⋉type=food Product))).
Notice that we were able to eliminate joins altogether in these rewritings. In
the latter rewriting, we were also able to eliminate deduplication, even though
attributes from several relations are involved. This is because cname is a key of
the set relation Customer.
4
Rewriting Queries
Traditional rewrite rules for optimizing queries, such as the selection and projec-
tion push-down rewrite rules, guarantee strong-equivalence: the original subquery
and the rewritten subquery always evaluate to the same result. Requiring this
strong form of equivalence severely limits the optimizations one can perform in
queries that involve projection and/or deduplication steps, as is illustrated in
the following example:
5 The max-union operators is inspired by the max-based multiset relation union [2].
6 Attribute introduction is a restricted form of the operator commonly known as gen-
eralized projection or extended projection [1,5].

194
J. Hellings et al.
Example 4. Consider the database instance I of Examples 1–3. The query
e = ˙δ( ˙πcname(Customer ·⋊⋉Bought)) returns the names of customers that bought
a product. This query is equivalent to e′ = ˙δ( ˙πcname(Customer) ·⋉Bought).
Observe that the subqueries Customer ·⋊⋉Bought and ˙πcname(Customer ·⋊⋉
Bought) of e are not equivalent to any subqueries of e′, however. Hence, this
rewriting cannot be achieved using strong-equivalent rewriting only. Finally, as
{cname} is a key of Customer, e and e′ are also equivalent to e′′ = ˙πcname(
Customer) ·⋉Bought.
To be able to discuss the full range of possible optimizations within the
scope of projection and deduplication operations, we diﬀerentiate between the
following notions of query equivalence:
Deﬁnition 1. Let e1 and e2 be multiset relational algebra expressions over D
with a = S(e1; D) ∩S(e2; D), and let b ⊆a. We say that e1 and e2 are strong-
equivalent, denoted by e1 ˙= e2, if, for every database instance I over D, we
have [[e1]]I = [[e2]]I. We say that e1 and e2 are weak-equivalent, denoted by
e1 ˜= e2, if, for every database instance I over D, we have [[ ˙δ(e1)]]I = [[ ˙δ(e2)]]I.
We say that e1 and e2 are strong-b-equivalent, denoted by e1 ˙=b e2, if, for every
database instance I over D, we have [[ ˙πb(e1)]]I = [[ ˙πb(e2)]]I. Finally, we say that
e1 and e2 are weak-b-equivalent, denoted by e1 ˜=b e2, if, for every database
instance I over D, we have [[ ˙δ( ˙πb(e1))]]I = [[ ˙δ( ˙πb(e2))]]I.
While strong-equivalent expressions always yield the same result, weak-
equivalent expressions yield the same result only up to duplicate elimination.
For query optimization, the latter is often suﬃcient at the level of subqueries:
structural properties, such as the presence of a key in one of the relations involved,
may have as a side eﬀect that any duplicates in subqueries are eliminated in the
end result anyway.
Example 5. Consider the queries of Example 4. We have e ˙= e′ ˙= e′′, e ˙={cname}
Customer ·⋉Bought, Customer ·⋊⋉Bought ˜={cname,age} Customer ·⋉Bought, and
˙πcname(Customer ·⋊⋉Bought) ˜={cname} ˙πcname(Customer) ·⋉Bought.
Examples 4 and 5 not only show the relevance of non-strong-equivalent rewrit-
ing rules, they also show that further optimizations are possible if the expressions
evaluate to set relations or satisfy certain keys. Therefore, to facilitate the dis-
cussion, we extend the deﬁnition of set relations and keys to expressions. Let e
be an expression over D with a = S(e; D). We say that e is a set relation if, for
every database instance I over D, [[e]]I is a set relation and we say that b ⊆a
is a key of e if, for every database instance I over D, b is a key of [[e]]I.
The following simple rules can be derived by straightforwardly applying Def-
inition 1:
Proposition 1. Let e, e1, and e2 be expressions over D with a = S(e; D) =
S(e1; D) ∩S(e2; D) and b ⊆a. Let ˆ= be either ˙= or ˜=. We have:
(i) if e1 ˆ= e2, then e1 ˆ=b e2;

Optimizing Multiset Relational Algebra Queries
195
(ii) if e1 ˆ=a e2, then e1 ˆ= e2;
(iii) if c ⊆b and e1 ˆ=b e2, then e1 ˆ=c e2;
(iv) if e1 ˆ=b e2, then ˙πb(e1) ˆ= ˙πb(e2);
(v) if e1 ˙=b e2, then e1 ˜=b e2;
(vi) if e1 ˜= e2, then ˙δ(e1) ˙= ˙δ(e2);
(vii) if e1 ˜=b e2, e1 and e2 are set relations, and b is a key of e1 and e2, then
e1 ˙=b e2; and
(viii) e ˙=b ˙πb(e) and e ˜= ˙δ(e).
Proposition 1.(iii) allows us to restrict the scope of strong-b-equivalences and
weak-b-equivalences to subsets of b.
In the presence of conditions, as enforced by selections, θ-joins, or θ-
semi-joins, we can also extend the scope of strong-b-equivalences and weak-b-
equivalences. E.g., if u ∈U is a constant and e1 and e2 are expressions over D
with a, a′, b ∈S(e1; D) ∩S(e2; D) and e1 ˜={a} e2, then ˙σa=a′,b=u(e1) ˜={a,a′,b}
˙σa=a′,b=u(e2). Next, we develop the framework for these scope extensions, for
which we ﬁrst introduce the closure of a set of attributes under a set of condi-
tions:
Deﬁnition 2. Let a be a relation schema, let b ⊆a, and let E be a set of
conditions over a. The closure of b under E, denoted by C(b; E), is the smallest
superset of b such that, for every condition (v = w) ∈E or (w = v) ∈E,
v ∈C(b; E) if and only if w ∈C(b; E).
If a ∈a, then we write C(a; E) for C({a}; E).
Notice that, besides attributes, C(b; E) may also contain constants. E.g., if
(b = u) ∈E for b ∈b and u ∈U, then u ∈C(b; E). We denote C(b; E) ∩a, the
set of attributes in C(b; E), by attrs(b; E); and we denote C(b; E) −a, the set
of constants in C(b; E), by consts(b; E).
Intuitively, the values of a tuple for the attributes in attrs(b; E) are uniquely
determined by the values of that tuple for the attributes in b. There may be
other attributes c /∈attrs(b; E) that can only have a single value, however. E.g.,
if attribute c is constraint by a constant condition of the form c = u, u ∈U.
The values of a tuple for such attributes c are therefore trivially determined by
the values of that tuple for the attributes in b. This observation leads to the
following deﬁnition:
Deﬁnition 3. Let a be a relation schema, let b ⊆a, and let E be a set of
conditions over a. The set of attributes determined by b, denoted by det(b; E),
is deﬁned by attrs(b; E) ∪{a ∈a | consts(a; E) ̸= ∅}.
The intuition given above behind the notions in Deﬁnitions 2 and 3 can be
formalized as follows:
Lemma 1. Let t be a tuple over a relation schema a and let E be a set of
conditions over a. If t satisﬁes E, then, for every a ∈a, we have
(i) t(a) = t(b) for every b ∈attrs(a; E);

196
J. Hellings et al.
(ii) t(a) = u for every u ∈consts(a; E);
(iii) |consts(a; E)| ≤1; and
(iv) t ≡det(b;E) t′ if b ⊆a, t′ is a tuple over a, t′ satisﬁes E, and t ≡b t′.
Using Lemma 1, we prove the following rewrite rules for selection:
Theorem 1. Let g and h be expressions over D with g = S(g; D) and h = S(h;
D), let E be a set of conditions over g, let a ⊆(h ∩attrs(E)), let b ⊆(det(a;
E) −h), and let f be a set of assignment-pairs such that, for each b ∈b, there
exists (b := x) ∈f with x ∈(C(b; E) ∩(a ∪U)). We have
(i) if ˙σE(g) ˜=a h, then ˙σE(g) ˜=a∪b ˙ιf(h); and
(ii) if ˙σE(g) ˙=a h, then ˙σE(g) ˙=a∪b ˙ιf(h).
Proof. We ﬁrst prove (i). Assume ˙σE(g) ˜=a h and let I be a database instance
over D. We prove [[ ˙δ( ˙πa∪b( ˙σE(g)))]]I = [[ ˙δ( ˙πa∪b(˙ιf(h)))]]I by showing that there
exist n and n′ such that
(t : n) ∈[[ ˙πa∪b( ˙σE(g))]]I ⇐⇒(t : n′) ∈[[ ˙πa∪b(˙ιf(h))]]I.
Assume (t : n) ∈[[ ˙πa∪b( ˙σE(g))]]I, and let (t1 : p1), . . . , (tj : pj) ∈[[ ˙σE(g)]]I be
all tuple-count pairs such that, for every i, 1 ≤i ≤j, ti ≡a∪b t. By construction,
we have n = p1 +· · ·+pj. By Lemma 1.(iv) and b ⊆det(a; E), no other t′ exists
with t′ /∈{t1, . . . , tj}, (t′:p′) ∈[[ ˙σE(g)]]I and t′ ≡a t as this would imply t′ ≡a∪b t.
By ˙σE(g) ˜=a h, we have (t|a :q) ∈[[ ˙πa(h)]]I. Let (s1 :q1), . . . , (sk :qk) ∈[[h]]I with,
for every i, 1 ≤i ≤k, si ≡a t. By construction, we have q = q1 + · · · + qk. We
prove that, for all 1 ≤i ≤k, t ≡b si. Consider b ∈b with (b := x) ∈f:
1. If x ∈attrs(b; E), then x ∈a and, by Lemma 1.(i), we have t(b) = t(x). By
x ∈a and t ≡a si, we have t(b) = t(x) = si(x). From the semantics of ˙ιb:=x,
it follows that t(b) = t(x) = si(x) = si(b).
2. If x ∈consts(b; E), then, by Lemma 1.(iii), there exists a constant u ∈U
such that consts(b; E) = {u}. By Lemma 1.(ii), we have t(x) = u. From the
semantics of ˙ιb:=u, it follows that t(b) = u = si(b).
We conclude that (t : q) ∈[[ ˙πa∪b(˙ιf(h))]]I and n′ = q. To prove (ii), we
bootstrap the proof of (i). By ˙σE(g) ˙=a h, we have n = q. Hence, we have
n = n′ and (t : n) ∈[[ ˙πa∪b( ˙σE(g))]]I if and only if (t : n) ∈[[ ˙πa∪b(˙ιf(h))]]I.
⊓⊔
In the presence of constant conditions, Theorem 1 can be applied to joins to
rewrite them into semi-joins combined with attribute introduction. An example
of such rewrites is exhibited in Example 3.
Applications of Theorem 1 involve attribute introduction, which makes query
results larger. Hence, it is best to delay this operation by pulling the operator
up. Proposition 2 presents rewrite rules to do so.
Proposition 2. Let e, e1, and e2 be expressions over D. We have
(i) ˙σE(˙ιf(e)) ˙= ˙ιf( ˙σE(e)) if, for all (b := x) ∈f, b /∈attrs(E);

Optimizing Multiset Relational Algebra Queries
197
(ii) ˙πb(˙ιf(e)) ˙= ˙ιf ′( ˙πb′(e)) if f ′ ⊆f and b = b′ ∪{b | (b := x) ∈f ′}.
(iii) ˙ρg(˙ιf(e)) ˙= ˙ιf ′( ˙ρg′(e)) if g′ = {a →g(a) | a ∈S(e; D)} and
f ′ ={g(b) := g(x) | (b := x) ∈f ∧x ∈S(e; D)} ∪
{g(b) := x | (b := x) ∈f ∧x ∈U}.
(iv) ˙δ(˙ιf(e)) ˙= ˙ιf( ˙δ(e));
(v) ˙ιf(e1) ⊕˙ιf(e2) ˙= ˙ιf(e1 ⊕e2) if ⊕∈{ ˙∪, ˙∩, ·−};
(vi) ˙ιf(e1) ·⋊⋉E e2 ˙= ˙ιf ′(e1 ·⋊⋉E′ e2) if f ′ = {(b := x) ∈f | b /∈S(e2; D)} and
E′ = E ∪{b = x | (b := x) ∈f ∧b ∈S(e2; D)};
(vii) ˙ιf(e1) ·⋉E e2 ˙= ˙ιf(e1 ·⋉E′ e2) if
E′ = E ∪{b = x | (b := x) ∈f ∧b ∈S(e2; D)};
(viii) e1 ·⋉E ˙ιf(e2) ˙= e1 ·⋉E′ e2 if
E′ = E ∪{b = x | (b := x) ∈f ∧b ∈S(e1; D)};
(ix) ˙ιf(e1) ˙⊔˙ιf(e2) ˙= ˙ιf(e1 ˙⊔e2); and
(x) ˙ιf(˙ιg(e)) ˙= ˙ιf∪g(e) if, for all (b := x) ∈f, we have x ∈(S(e; D) ∪U).
Attribute introduction and renaming play complementary roles in the context
of projection, as is illustrated by the following example:
Example 6. Consider the expression e with S(e; D) = {a, b} and consider
the query ˙πa,c,d(˙ιc:=a,d:=b(e)). As we do not need b after the projection, we
can also rename b to d instead of introducing d. This alternative approach
results in ˙πa,c,d(˙ιc:=a( ˙ρa	→a,b	→d(e)). In this expression, we can easily push
the attribute introduction through the projection, resulting in the expression
˙ιc:=a( ˙πa,d( ˙ρa	→a,b	→d(e))).
The following rewrite rules can be used to apply the rewriting of Example 6:
Proposition 3. Let e be an expression over D and let ˙ιf be an attribute intro-
duction operator applicable to e. We have
(i) if (b := x) ∈f, x ∈S(e; D), and c = b for all (c := x) ∈f, then
˙ιf(e) ˙=(S(e;D)∪{b})−{x} ˙ιf\{b:=x}( ˙ρ{x	→b}∪{a	→a|a∈(S(e;D)−{x})}(e));
(ii) if (b1 := x), (b2 := x) ∈f, b1 ̸= b2, then ˙ιf(e) ˙= ˙ι{b2:=b1}(˙ιf−{b2:=x}(e)).
The rewrite rules of Proposition 2 that involve selections and attribute intro-
ductions put heavy restrictions on the sets of conditions involved. To alleviate
these restrictions, we can use Proposition 3 and the well-known push-down rules
for selection [2,5,10,12], to push some attribute introductions through selections.
What we have done up to now is examining how selection conditions interact
with the notions of strong-equivalence and weak-equivalence. Next, we will put
these results to use. As explained in the Introduction, our focus is twofold:
1. eliminating joins in favor of semi-joins; and
2. eliminating deduplication.
These foci are covered in Sects. 4.1 and 4.2, respectively. In Sect. 4.3, ﬁnally,
we investigate how the other operators interact with strong-equivalence and
weak-equivalence.

198
J. Hellings et al.
4.1
θ-Joins and θ-Semi-Joins
Above, we explored how selection conditions interact with strong-equivalence
and weak-equivalence. Since θ-joins and θ-semi-joins implicitly use selection con-
ditions, the techniques developed also apply to θ-joins and θ-semi-joins, which
are the focus of this subsection. First, we notice that we can use the following
rules to change the conditions involved in selections, θ-joins, and θ-semi-joins.
Proposition 4. Let E and E′ be sets of conditions over the same set of
attributes a. If we have C(a; E) = C(a; E′) for every a ∈a, then we have
(i) ˙σE(e) ˙= ˙σE′(e);
(ii) e1 ·⋊⋉E e2 ˙= e1 ·⋊⋉E′ e2; and
(iii) e1 ·⋉E e2 ˙= e1 ·⋉E′ e2.
The equivalence at the basis of semi-join-based query rewriting in the set-
based relational algebra is e1 ⋉e2 = πa1(e1 ⋊⋉e2) with a1 the relation schema
of the relation to which e1 evaluates. Notice, however, that the equivalence
e1 ·⋉e2 = ˙πa1(e1 ·⋊⋉e2) does not hold, because the multiset semi-join does not
take into account the number of occurrences of tuples in [[e2]]I:7
Example 7. Consider the database instance I of Example 1, which is visualized
in Fig. 1. We apply the queries e1 = ˙π{cname,age}(Customer ·⋊⋉Bought) and
e2 = Customer ·⋉Bought. We have
[[e1]]I = {(cname →Alice, age →19 : 2), . . . };
[[e2]]I = {(cname →Alice, age →19 : 1), . . . }.
Even though the above rewriting of the projection of a join into the corre-
sponding semi-join is not a strong-equivalent rewriting, we observe that it is still
a weak-equivalent rewriting.
We now formalize rewrite rules involving joins and semi-joins:
Theorem 2. Let g1, g2, h1, and h2 be expressions over D with g1 = S(g1; D),
g2 = S(g2; D), h1 = S(h1; D), h2 = S(h2; D), a1 ⊆(g1 ∩h1), a2 ⊆(g2 ∩h2),
and a1 ∩a2 = g1 ∩g2 = h1 ∩h2, let E be a set of conditions over a1 ∪a2, let
b ⊆(a2 −a1), and let f be a set of assignment-pairs such that, for each b ∈b,
there exists (b := x) ∈f with x ∈(C(b; E) ∩(a1 ∪U)). We have
(i) g1 ·⋊⋉E g2 ˜=a1∪a2 h1 ·⋊⋉E h2 if g1 ˜=a1 h1 and g2 ˜=a2 h2;
(ii) g1 ·⋊⋉E g2 ˙=a1∪a2 h1 ·⋊⋉E h2 if g1 ˙=a1 h1 and g2 ˙=a2 h2;
(iii) g1 ·⋊⋉E g2 ˜=a1 h1 ·⋉E h2 if g1 ˜=a1 h1 and g2 ˜=a2 h2;
(iv) g1 ·⋊⋉E g2 ˙=a1 h1 ·⋉E h2 if g1 ˙=a1 h1, g2 ˜=a2 h2, g2 is a set relation, and
g2 has a key c with c ⊆det(a1 ∩a2; E);
7 We could have deﬁned a multiset semi-join operator that does take into account the
number of occurrences of tuples in [[e2]]I. With such a semi-join operator, we would
no longer be able to sharply reduce the size of intermediate query results, however,
and lose some potential to optimize query evaluation.

Optimizing Multiset Relational Algebra Queries
199
(v) g1 ·⋊⋉E g2 ˜=a1∪b ˙ιf(h1 ·⋉E h2) if g1 ˜=a1 h1 and g2 ˜=a2 h2; and
(vi) g1 ·⋊⋉E g2 ˙=a1∪b ˙ιf(h1 ·⋉E h2) if g1 ˙=a1 h1, g2 ˜=a2 h2, g2 is a set relation,
and g2 has a key c with c ⊆det(a1 ∩a2; E).
Proof (Sketch). We prove (i). Let I be a database instance over D. We prove
[[ ˙δ( ˙πa1∪a2(g1 ·⋊⋉E g2))]]I = [[ ˙δ( ˙πa1∪a2(h1 ·⋊⋉E h2))]]I by showing that there exist n
and n′ such that
(t : n) ∈[[ ˙πa1∪a2(g1 ·⋊⋉E g2)]]I ⇐⇒(t : n′) ∈[[ ˙πa1∪a2(h1 ·⋊⋉E h2)]]I.
Assume (t : n) ∈[[ ˙πa1∪a2(g1 ·⋊⋉E g2)]]I, and let (r1 : p1), . . . , (rk1 : pk1) ∈[[g1]]I
and (s1 : q1), . . . , (sk2 : qk2) ∈[[g2]]I be all tuple-count pairs such that, for every
i1, 1 ≤i1 ≤k1, ri1 ≡a1 t, and, for every i2, 1 ≤i2 ≤k2, si2 ≡a2 t. Let
p = p1 + · · · + pk1 and q = q1 + · · · + qk2. By construction, we have that, for
every i1 and i2, 1 ≤i1 ≤k1 and 1 ≤i2 ≤k2, (ri1 · si2 : pi1 · qi2) ∈[[g1 ·⋊⋉E g2]]I.
Hence, (t|a1 : p) ∈[[ ˙πa1(g1)]]I, (t|a2 : q) ∈[[ ˙πa2(g2)]]I, and n = p · q. By g1 ˜=a1 h1
and g2 ˜=a2 h2, we have (t|a1 : p′) ∈[[ ˙πa1(h1)]]I and (t|a2 : q′) ∈[[ ˙πa2(h2)]]I. Let
(r′
1 : p′
1), . . . , (r′
l1 : p′
l1) ∈[[h1]]I and (s′
1 : q′
1), . . . , (s′
l2 : q′
l2) ∈[[h2]]I be all tuple-
count pairs such that, for every j1, 1 ≤j1 ≤l1, r′
j1 ≡a1 t, and, for every j2,
1 ≤j2 ≤l2, s′
j2 ≡a2 t. By construction, p′ = p′
1 + · · · + p′
l1 and q′ = q′
1 + · · · + q′
l2
and, for every j1 and j2, 1 ≤j1 ≤l1 and 1 ≤j2 ≤l2, r′
j1 · s′
j2 satisﬁes E and
(r′
j1 · s′
j2 : p′
j1 · q′
j2) ∈[[h1 ·⋊⋉E h2]]I. We conclude (t : p′ · q′) ∈[[ ˙πa1∪a2(h1 ·⋊⋉E h2)]]I
and n′ = p′ · q′.
Next, we prove (ii) by bootstrapping the proof of (i). Observe that, by g1 ˙=a1
h1 and g2 ˙=a2 h2, we have that p = p′ and q = q′. Hence, n = n′ and (t : n) ∈
[[ ˙πa1∪a2(g1 ·⋊⋉E g2)]]I if and only if (t : n) ∈[[ ˙πa1∪a2(h1 ·⋊⋉E h2)]]I.
The other statements can be proven in analogous ways.
⊓⊔
The rules of Theorem 2 can be specialized to semi-joins only:
Corollary 1. Let g1, g2, h1, and h2 be expressions which satisfy the conditions
of Theorem 2 and let ˆ= be either ˙= or ˜=. If g1 ˆ=a1 h1 and g2 ˜=a2 h2, then
g1 ·⋉E g2 ˆ=a1 h1 ·⋉E h2.
4.2
Deduplication
The second optimization goal we have set ourselves is to eliminate the need for
removing duplicates. This is possible if we can push down deduplication opera-
tors to a level where subexpressions are guaranteed to evaluate to set relations,
in which case deduplication becomes redundant.
The rewrite rules relevant for pushing down deduplication are the following:
Proposition 5. Let e, e1, e2 be expressions over D. We have
(i) ˙δ( ˙σE(e)) ˙= ˙σE( ˙δ(e));
(ii) ˙δ( ˙πb(e) ˙= ˙πb( ˙δ(e)) if b is a key of e;
(iii) ˙δ( ˙ρf(e)) ˙= ˙ρf( ˙δ(e));
(iv) ˙δ( ˙δ(e)) ˙= ˙δ(e);
(v) ˙δ(e1 ∪e2) ˙= ˙δ(e1) ˙⊔˙δ(e2);
(vi) ˙δ(e1 ˙∩e2) ˙= ˙δ(e1) ˙∩˙δ(e2);
(vii) ˙δ(e1 ·⋊⋉E e2) ˙= ˙δ(e1) ·⋊⋉E ˙δ(e2);
(viii) ˙δ(e1 ·⋉E e2) ˙= ˙δ(e1) ·⋉E e2;
(ix) ˙δ(e1 ˙⊔e2) ˙= ˙δ(e1) ˙⊔˙δ(e2); and
(x) ˙δ(˙ιf(e)) ˙= ˙ιf( ˙δ(e)).

200
J. Hellings et al.
One cannot push down deduplication through diﬀerence, however, as ˙δ(e1 ·−
e2) ˙= ˙δ(e1) ·−˙δ(e2) does not hold in general.
If, in the end, deduplication operates on expressions that evaluate to set
relations, it can be eliminated altogether:
Proposition 6. Let e be an expression over D. If e is a set relation, then ˙δ(e) ˙=
e.
4.3
Other Rewrite Rules
To use the rewrite rules of Proposition 1, Theorem 1, Theorem 2, Proposition 2,
and Proposition 5 in the most general way possible, we also need to know how
the other operators interact with strong-b-equivalence and weak-b-equivalence.
For the unary operators, we have the following:
Proposition 7. Let g and h be expressions over D with a ⊆S(g; D) ∩S(h; D).
Let ˆ= be either ˙= or ˜=. If g ˆ=a h, then we have
(i) ˙σE(g) ˆ=a ˙σE(h) if E is a set of equalities with attrs(E) ⊆a;
(ii) ˙πbg(g) ˆ=a∩b ˙πbh(h) if bg ⊆S(g; D), bh ⊆S(h; D), and b = bg ∩bh;
(iii) ˙ρfg(g) ˆ={fg(a)|a∈a} ˙ρfh(h) if fg : S(g; D) →bg and fh : S(h; D) →bh are
bijections with, for all a ∈a, fg(a) = fh(a);
(iv) ˙δ(g) ˆ=a ˙δ(h); and
(v) ˙ιf(g) ˆ=a∪b ˙ιf(h) if b ⊆(N −(S(g; D) ∪S(h; D))) and f is a set of
assignment-pairs such that, for each b ∈b, there exists (b := x) ∈f with
x ∈(a ∪U).
For the binary operators, we observe that the θ-join and θ-semi-join operators
are completely covered by Theorem 2 and Corollary 1.
For the union and max-union operators, we have the following:
Proposition 8. Let g1, g2, h1, and h2 be expressions over D and let a be a set
of attributes with ag = S(g1; D) = S(g2; D), ah = S(h1; D) = S(h2; D), and
a ⊆(ag ∩ah). Let ˆ= be either ˙= or ˜=. If g1 ˆ=a h1 and g2 ˆ=a h2, then we have
g1 ˙∪g2 ˆ=a h1 ˙∪h2 and g1 ˙⊔g2 ˜=a h1 ˙⊔h2. If, in addition, a = ag = ah, then
we also have g1 ˙⊔g2 ˆ=a h1 ˙⊔h2.
Finally, for the operators intersection and diﬀerence, we propose the following
straightforward rewrite rules:
Proposition 9. Let g1, g2, h1 and h2 be expressions over D with S(g1; D) =
S(g2; D) and S(h1; D) = S(h2; D). Let ˆ= be either ˙= or ˜=. If g1 ˆ= h1 and
g2 ˆ= h2, then we have g1 ˙∩g2 ˆ= h1 ˙∩h2; and if g1 ˙= h1 and g2 ˙= h2, then
g1 ·−g2 ˙= h1 ·−h2.
The above rewrite rules for max-union, intersection, and diﬀerence are very
restrictive. Next, we illustrate that we cannot simply relax these restrictive
rewrite rules to more general strong-b-equivalence or weak-b-equivalence rewrite
rules:

Optimizing Multiset Relational Algebra Queries
201
Example 8. Let a = {m, n}, let U, V , V ′, and W be four relation names, and let
I be the database instance mapping these four relation names to the following
multiset relations over a:
[[U]]I = {({m →u, n →v} : 1), ({m →u, n →w} : 1)};
[[V ]]I = {({m →u, n →v} : 2)};
[[V ′]]I = {({m →u, n →v} : 3)};
[[W]]I = {({m →u, n →w} : 2)}.
We have V ˜= V ′ and, due to [[ ˙πm(U)]]I = [[ ˙πm(V )]]I = [[ ˙πm(W)]]I = {({m →
u} : 2)}, we have U ˙=m V ˙=m W. We also have
[[V ·−V ′]]I = ∅;
[[V ′ ·−V ]]I = {({m →u, n →v} : 1)};
[[V ˙⊔V ]]I = [[V ]]I
[[V ˙⊔W]]I = [[V ]]I ∪[[W]]I;
[[V ˙∩U]]I = {({m →u, n →v} : 1)};
[[V ˙∩W]]I = ∅;
[[V ·−U]]I = {({m →u, n →v} : 1)};
[[V ·−W]]I = {({m →u, n →v} : 2)}.
Hence, V ·−V ′ ̸˜= V ′ ·−V , V ˙⊔V ̸ ˙=m V ˙⊔W, V ˙∩U ̸ ˙=m V ˙∩W, V ˙∩U ̸˜=m
V ˙∩W, and V ·−U ̸ ˙=m V ·−W. Let I′ be the database instance obtained from
I by changing all the counts in [[U]]I to 2. Then,
[[V ·−U]]I′ = ∅;
[[V ·−W]]I′ = {({m →u, n →v} : 2)},
and, hence, we also have V ·−U ̸˜=m V ·−W. We must conclude that we cannot
hope for meaningful rewrite rules for max-union, intersection, and diﬀerence if
the conditions of Propositions 8 and 9 are not satisﬁed.
Besides the rewrite rules we introduced, the usual strong-equivalent (multiset)
relational algebra rewrite rules can, of course, also be used in the setting of non-
strong-equivalent rewriting. This includes rules for pushing down selections and
projections [5,12] and the usual associativity and distributivity rules for union,
intersection, diﬀerence, and joins [2].
5
Deriving Structural Query Information
Some of the rewrite rules of Proposition 1, Theorem 2, and Proposition 6 can
only be applied if it is known that some subexpression is a set relation or has
certain keys. Therefore, we introduce rules to derive this information.
We use the well-known functional dependencies as a framework to reason
about keys. A functional dependency over a is of the form b →c, with b, c ⊆a.
Let b →c be a functional dependency over a. A relation R over a satisﬁes
b →c if, for every pair of tuples r, s ∈R with r ≡b s, we have r ≡c s. A multiset
relation ⟨R; τ⟩over a satisﬁes b →c if R satisﬁes b →c. Let D be a set of
functional dependencies over a. The closure of D, which we denote by +(D), is
the set of all functional dependencies over a that are logically implied by D [1].
By fds(e; D), we denote the functional dependencies we derive from expression

202
J. Hellings et al.
e over D. We deﬁne fds(e; D) inductively. The base cases are relation names
R ∈N, for which we have fds(R; D) = +({k →A(R) | k ∈K(R)}). For the
other operations, we have
fds( ˙σE(e); D) = +({a →b | ∃c b ⊆det(c; E) ∧(a →c) ∈fds(e; D)});
fds( ˙πc(e); D) = {a →b ∩c | a ⊆c ∧(a →b) ∈fds(e; D)};
fds( ˙ρf(e); D) = {f(a) →f(b) | (a →b) ∈fds(e; D)};
fds( ˙δ(e); D) = fds(e; D);
fds(e1 ˙∪e2; D) = +(∅);
fds(e1 ˙∩e2; D) = +((fds(e1; D) ∪fds(e2; D)));
fds(e1 ·−e2; D) = fds(e1; D);
fds(e1 ·⋊⋉e2; D) = +((fds(e1; D) ∪fds(e2; D)));
fds(e1 ·⋊⋉E e2; D) = fds( ˙σE(e1 ·⋊⋉e2); D);
fds(e1 ·⋉E e2; D) = fds( ˙πS(e1;D)(e1 ·⋊⋉E e2); D)
fds(e1 ˙⊔e2; D) = +(∅);
fds(˙ιf(e); D) = +({({x} ∩S(e; D) →{b}) | (b := x) ∈f} ∪fds(e; D)).
We deﬁne keys(e; D) = {a | (a →S(e; D)) ∈fds(e; D)} to be the keys we
derive from expression e over D. Next, we deﬁne the predicate set(e; D) which is
true if we can derive that expression e over D always evaluates to a set relation.
We deﬁne set(e; D) inductively. The base cases are relation names R ∈N, for
which we have set(R; D) = S(R). For the other operations, we have
set( ˙σE(e); D) = set(e; D);
set( ˙πb(e); D) = set(e; D) ∧b ∈keys(e; D);
set( ˙ρf(e); D) = set(e; D);
set( ˙δ(e); D) = true;
set(e1 ˙∪e2; D) = false;
set(e1 ˙∩e2; D) = set(e1; D) ∨set(e2; D);
set(e1 ·−e2; D) = set(e1; D);
set(e1 ·⋊⋉E e2; D) = set(e1; D) ∧set(e2; D);
set(e1 ·⋉E e2; D) = set(e1; D);
set(e1 ˙⊔e2; D) = set(e1; D) ∧set(e2; D);
set(˙ιf(e); D) = set(e; D).
The derivation rules for fds(e; D) and set(e; D) are not complete: it is not
guaranteed that fds(e; D) contains all functional dependencies that must hold
in [[e]]I, for every database instance I over D. Likewise, it is not guaranteed that
set(e; D) = false implies that e is not a set relation.
Example 9. Let u1, u2 ∈U with u1 ̸= u2 and let e be an expression over D.
Consider the expressions e1 = ˙σa=u1,a=u2(e) and e2 = e −e. For every database
instance I over D, we have [[e1]]I = [[e2]]I = ∅. Hence, every functional depen-
dency a →b with a, b ⊆S(e; D) holds on [[e1]]I and on [[e2]]I. In addition,
both [[e1]]I and [[e2]]I are set relations. If fds(e; D) = ∅, then we derive that
fds(e1; D) = +({∅→{a}}) and fds(e2; D) = +(∅). Observe that fds(e1; D) con-
tains all functional dependencies over S(e1; D) if and only if S(e; D) = {a} and
that fds(e2; D) never contains all functional dependencies over S(e1; D). We also

Optimizing Multiset Relational Algebra Queries
203
derive that set(e1; D) = set(e2; D) = false, despite e1 and e2 evaluating to set
relations.
Although the above derivation rules are not complete, they are sound:
Theorem 3. Let e be an expression over D. The derivation rules for fds(e; D)
and set(e; D) are sound: if I is a database instance over D, then
(i) [[e]]I satisﬁes every functional dependency in fds(e; D); and
(ii) if set(e; D) = true, then [[e]]I is a set relation.
We introduce the following notions. Let ⟨R1; τ1⟩and ⟨R2; τ2⟩be multiset
relations over a. We say that ⟨R1; τ1⟩is a weak subset of ⟨R2; τ2⟩, denoted by
⟨R1; τ1⟩˜⊆⟨R2; τ2⟩, if R1 ⊆R2. We say that ⟨R1; τ1⟩is a strong subset of
⟨R2; τ2⟩, denoted by ⟨R1; τ1⟩˙⊆⟨R2; τ2⟩, if (t : n) ∈⟨R1; τ1⟩implies (t : m) ∈
⟨R2; τ2⟩with n ≤m.8 Lemma 2 lists the main properties of these notions needed
to prove Theorem 3.
Lemma 2. Let ⟨R1; τ1⟩and ⟨R2; τ2⟩be multiset relations over a. We have
(i) ⟨R1; τ1⟩˜⊆⟨R2; τ2⟩if ⟨R1; τ1⟩˙⊆⟨R2; τ2⟩;
(ii) ⟨R1; τ1⟩satisﬁes b →c if ⟨R1; τ1⟩˜⊆⟨R2; τ2⟩and ⟨R2; τ2⟩satisﬁes func-
tional dependency b →c; and
(iii) ⟨R1; τ1⟩is a set relation if ⟨R1; τ1⟩˙⊆⟨R2; τ2⟩and ⟨R2; τ2⟩is a set relation.
6
Rewriting the Example Queries
To illustrate the techniques introduced in this paper, we provide a detailed rewrit-
ing of the queries exhibited in Example 3. We start by considering the queries
˙δ( ˙πage(e)) and ˙δ( ˙πcname,type(e)) with e = ˙σtype=food(Customer ·⋊⋉Bought ·⋊⋉
Product). As a ﬁrst step, we use well-known push-down rules for selection [12],
and we get e ˙= Customer ·⋊⋉Bought ·⋊⋉type=food Product.
Consider the query e′
= Bought ·⋊⋉type=food Product, subquery of the
above query. We have {pname} ∈keys(Product; D) and Product is a set
relation. Hence, we can apply Theorem 2.(iv) and Theorem 2.(vi) with
f = {type := food}, and we obtain e′
˙={cname,pname} Bought ·⋉type=food
Product and e′ ˙={cname,pname,type} ˙ιf(Bought ·⋉type=food Product). Let e′′ =
Bought ·⋉type=food Product. For the rewriting of ˙δ( ˙πage(e)), we use Proposi-
tion 1.(iii) and Proposition 1.(v) and infer e′ ˜={cname} e′′. Using Theorem 2.(iii)
and transitivity, we infer e ˜={cname,age} Customer ·⋉e′′, and, using Proposi-
tion 1.(v), we infer e ˜={age} Customer ·⋉e′′. Finally, we can apply Proposi-
tion 1.(iv) and Proposition 1.(vi) to conclude ˙δ( ˙πage(e)) ˙= ˙δ( ˙πage(Customer ·⋉
e′′)). Hence, ˙δ( ˙πage(e)) ˙= ˙δ( ˙πage(Customer ·⋉(Bought ·⋉type=food Product))),
which is the query resulting from optimizing ˙δ( ˙πage(e)) in Example 3.
8 Notice that ⟨R1; τ1⟩˜⊆⟨R2; τ2⟩does not imply that ⟨R1; τ1⟩is fully included in
⟨R2; τ2⟩: there can be tuples t ∈R1 for which τ1(t) > τ2(t).

204
J. Hellings et al.
For the rewriting of ˙δ( ˙πcname,type(e)), we directly use Proposition 1.(iii)
to obtain e′
˙={cname,type}
˙ιf(e′′). We use Theorem 2.(ii) and transitiv-
ity to obtain e
˙={cname,type} Customer ·⋊⋉˙ιf(e′′). Using Proposition 2.(vi)
and commutativity of θ-join, we conclude Customer ·⋊⋉˙ιf(e′′)
˙={cname,type}
˙ιf(Customer ·⋊⋉e′′). Next, consider the query Customer ·⋊⋉e′′, subquery of
the query ˙ιf(Customer ·⋊⋉e′′). Using Theorem 2.(iii), we infer Customer ·⋊⋉
e′′
˜={cname}
Customer ·⋉e′′ and we apply Proposition 7.(v) with f on
both sides to obtain ˙ιf(Customer ·⋊⋉e′′)
˜={cname,type}
˙ιf(Customer ·⋉e′′).
Using transitivity, we get e ˜={cname,type} ˙ιf(Customer ·⋉e′′) and we apply
Proposition 7.(ii) with b = {cname, type} to obtain
˙πb(e)
˜={cname,type}
˙πb(˙ιf(Customer ·⋉e′′)). Next, we apply Proposition 2.(ii) on ˙πb(˙ιf(Customer ·⋉
e′′)) and transitivity to obtain ˙πb(e) ˜={cname,type} ˙ιf( ˙πcname(Customer ·⋉e′′).
Then we apply Proposition 7.(iv) on both sides and we use Propositions 1.(iv)
and 1.(vii) to obtain ˙δ( ˙πb(e)) ˙= ˙δ(˙ιf( ˙πcname(Customer ·⋉e′′))). We observe
that Customer is a set relation. Hence, ˙ιf( ˙πcname(Customer ·⋉e′′)) is also set
relation. Finally, we can now apply Proposition 6 and transitivity to conclude
˙δ( ˙πcname,type(e)) ˙= ˙ιf( ˙πcname(Customer ·⋉e′′)) and, hence, ˙δ( ˙πcname,type(e)) ˙=
˙ιtype:=food( ˙πcname(Customer ·⋉(Bought ·⋉type=foodProduct))), which is the query
resulting from optimizing ˙δ( ˙πcname,type(e)) in Example 3.
7
Conclusion
In this work, we provide a formal framework for optimizing SQL queries using
query rewriting rules aimed at optimizing query evaluation for relational algebra
queries using multiset semantics. The main goals of our rewrite rules are the
automatic elimination of costly join steps, in favor of semi-join steps, and the
automatic elimination of deduplication steps. We believe that our rewrite rules
can be applied to many practical queries on which traditional techniques fall
short. Hence, we believe that our results provide a promising strengthening of
traditional query rewriting and optimization techniques. Based upon the ideas
of our work, there are several clear directions for future work.
We have primarily studied the automatic rewriting of queries using joins
into queries using semi-joins instead. In the setting of SQL, this optimization
is usually obtained by rewriting joins into WHERE ... IN ... clauses. The anti-
semi-join plays a similar role in performing WHERE ... NOT IN ... clauses. As
such, it is only natural to ask whether our framework can be extended to also
automatically rewrite towards anti-semi-join operators. A careful investigation
is needed to fully incorporate anti-semi-joins in our framework, however.
The multiset relational algebra we studied does not cover all features provided
by SQL. Among the missing features are aggregation and recursive queries (via
WITH RECURSIVE), and both are candidates for further study. With respect to
recursive queries, we observe that in the setting of graph query languages, usage
of transitive closure to express reachability can automatically be rewritten to
very fast ﬁxpoint queries [6,7]. Similar optimizations also apply to simple WITH
RECURSIVE queries, but it remains open whether a general technique exists to
optimize such queries.

Optimizing Multiset Relational Algebra Queries
205
Acknowledgement. This material is based upon work supported by the National
Science Foundation under Grant No. #1606557.
References
1. Abiteboul, S., Hull, R., Vianu, V. (eds.): Foundations of Databases, 1st edn.
Addison-Wesley Publishing Company, Boston (1995)
2. Albert, J.: Algebraic properties of bag data types. In: Proceedings of the 17th Inter-
national Conference on Very Large Data Base, pp. 211–219. VLDB 1991, Morgan
Kaufmann Publishers Inc. (1991)
3. Bernstein, P.A., Chiu, D.M.W.: Using semi-joins to solve relational queries. J. ACM
28(1), 25–40 (1981). https://doi.org/10.1145/322234.322238
4. Dayal, U., Goodman, N., Katz, R.H.: An extended relational algebra with control
over duplicate elimination. In: Proceedings of the 1st ACM SIGACT-SIGMOD
Symposium on Principles of Database Systems, pp. 117–123. PODS 1982, ACM
(1982). https://doi.org/10.1145/588111.588132
5. Grefen, P.W.P.J., de By, R.A.: A multi-set extended relational algebra: a formal
approach to a practical issue. In: Proceedings of 1994 IEEE 10th International
Conference on Data Engineering, pp. 80–88. IEEE (1994). https://doi.org/10.1109/
ICDE.1994.283002
6. Hellings, J., Pilachowski, C.L., Van Gucht, D., Gyssens, M., Wu, Y.: From rela-
tion algebra to semi-join algebra: an approach for graph query optimization. In:
Proceedings of The 16th International Symposium on Database Programming Lan-
guages. ACM (2017). https://doi.org/10.1145/3122831.3122833
7. Hellings, J., Pilachowski, C.L., Van Gucht, D., Gyssens, M., Wu, Y.: From relation
algebra to semi-join algebra: an approach to graph query optimization. Comput. J.
64(5), 789–811 (2020). https://doi.org/10.1093/comjnl/bxaa031
8. International Organization for Standardization: ISO/IEC 9075–1: Information tech-
nology - database languages - SQL (2011)
9. Klausner, A., Goodman, N.: Multirelations: semantice and languages. In: Proceed-
ings of the 11th International Conference on Very Large Data Bases, pp. 251–258.
VLDB 1985, VLDB Endowment (1985)
10. Lamperti, G., Melchiori, M., Zanella, M.: On multisets in database systems. In:
Calude, C.S., Paun, G., Rozenberg, G., Salomaa, A. (eds.) WMC 2000. LNCS,
vol. 2235, pp. 147–215. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-
45523-X 9
11. Paulley, G.N.: Exploiting Functional Dependence in Query Optimization. Ph.D.
thesis, University of Waterloo (2000)
12. Ullman, J.D.: Principles of Database and Knowledge-Base Systems: Volume II:
The New Technologies. W.H. Freeman & Co, San Francisco (1990)
13. Yannakakis, M.: Algorithms for acyclic database schemes. In: Proceedings of the
Seventh International Conference on Very Large Data Bases, vol. 7, pp. 82–94.
VLDB 1981, VLDB Endowment (1981)

Properties of System W and Its
Relationships to Other Inductive
Inference Operators
Jonas Haldimann and Christoph Beierle(B)
Faculty of Mathematics and Computer Science,
FernUniversit¨at in Hagen, 58084 Hagen, Germany
{jonas.haldimann,christoph.beierle}@fernuni-hagen.de
Abstract. System W is a recently introduced inference method for con-
ditional belief bases with some notable properties like capturing system
Z and thus rational closure and, in contrast to system Z, fully satisfy-
ing syntax splitting. This paper further investigates properties of system
W. We show how system W behaves with respect to postulates put for-
ward for nonmonotonic reasoning like rational monotony, weak rational
monotony, or semi-monotony. We develop tailored postulates ensuring
syntax splitting for any inference operator based on a strict partial order
on worlds. By showing that system W satisﬁes these axioms, we obtain
an alternative and more general proof that system W satisﬁes syntax
splitting. We explore how syntax splitting aﬀects the strict partial order
underlying system W and exploit this for answering certain queries with-
out having to determine the complete strict partial order. Furthermore,
we investigate the relationships among system W and other inference
methods, showing that, for instance, lexicographic inference extends both
system W and c-inference, and leading to a full map of interrelationships
among various inductive inference operators.
1
Introduction
For answering the question of what should be entailed by a conditional belief base
[21] many approaches have been proposed. Established examples are p-entailment
[19] which coincides with system P [1], system Z [9,24] which coincides with
rational closure [21], or lexicographic inference [22]. Two further model based
approaches are reasoning with a single c-representation [12,13] and c-inference [2]
which is deﬁned with respect to all c-representations of a belief base.
This paper addresses the inference operator system W [17,18] that has
already been shown to have interesting properties like capturing and extend-
ing the inference relations induced by system Z and c-inference or avoiding the
drowning problem [6]. In a recent short paper, it has been pointed out that
system W also satisﬁes syntax splitting [10], a concept originally introduced
by Parikh [23] for postulates for the revision of belief sets and later extended
to other frameworks [15,25] and to inductive inference from conditional belief
bases [14].
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 206–225, 2022.
https://doi.org/10.1007/978-3-031-11321-5_12

Properties of System W
207
The main objective of this paper is to advance the investigation of properties
of system W. We introduce the notion of strict partial order-based (SPO-based)
inductive inference operators of which system W is an instance. We adapt the
syntax splitting postulates from [14] to SPO-based inductive inference operators.
By showing that system W fulﬁls the resulting postulates we extend our work in
[10] and obtain a full formal proof that system W complies to syntax splitting.
Note that the highly desirable property of syntax splitting is indeed a distinguish-
ing feature: Among the inference methods investigated in [14], only reasoning
with c-representations satisﬁes syntax splitting. Our results make system W,
besides c-inference, another inference operator to fully comply with syntax split-
ting; in addition, system W also extends rational closure [21] which is not the
case for c-inference. Furthermore, we demonstrate how splitting properties may
be exploited in certain situations for simplifying query answering with system
W even if the syntax splitting postulates are not applicable.
While it is known that system W extends rational closure, we establish the
properties of system W regarding rationality and monotony related postulates
suggested for nonmonotonic reasoning. Finally, we elaborate the relationship of
system W to other inference relations, leading to a full map of interrelationships
among inductive inference operators.
In summary, the main contributions of this paper are
– adapting splitting postulates to SPO-based inductive inference operators,
– proving that system W fulﬁls these syntax splitting postulates,
– investigating rationality and monotony related postulates for system W,
– showing how syntax splitting can be exploited to simplify query answering for
system W even in cases where syntax splitting postulates are not applicable,
– establishing a map of relations among system W and other inference methods.
In Sect. 2, we recall the background on conditional logic. In Sect. 3 we intro-
duce SPO-based inductive inference operators and corresponding syntax split-
ting postulates. We recall system W and investigate its properties in Sect. 4.
Section 5 shows that system W fulﬁls the syntax splitting postulates. Section 6
deals with the eﬀect of syntax splittings on the induced SPO and how this can
be helpful for answering certain queries. In Sect. 7 we elaborate the relationships
among inductive inference operators, before we conclude in Sect. 8.
2
Reasoning with Conditional Logic
A (propositional) signature is a ﬁnite set Σ of propositional variables. For a
signature Σ, we denote the propositional language over Σ by LΣ. Usually, we
denote elements of signatures with lowercase letters a, b, c, . . . and formulas with
uppercase letters A, B, C, . . .. We may denote a conjunction A ∧B by AB and
a negation ¬A by A for brevity of notation. The set of interpretations over a
signature Σ is denoted as ΩΣ. Interpretations are also called worlds and ΩΣ is
called the universe. An interpretation ω ∈ΩΣ is a model of a formula A ∈LΣ
if A holds in ω. This is denoted as ω |= A. The set of models of a formula (over

208
J. Haldimann and C. Beierle
a signature Σ) is denoted as Mod Σ(A) = {ω ∈ΩΣ | ω |= A} or sometimes
as ΩA. The Σ in Mod Σ(A) can be omitted if the signature is clear from the
context. A formula A entails a formula B if Mod Σ(A) ⊆Mod Σ(B). By slight
abuse of notation we sometimes interpret worlds as the corresponding complete
conjunction of all elements in the signature in either positive or negated form.
Worlds over (sub-)signatures can be merged or marginalized. Let Σ be a
signature with disjoint sub-signatures Σ1, Σ2 such that Σ = Σ1 ∪Σ2. Let ω1 ∈
ΩΣ1 and ω2 ∈ΩΣ2. Then (ω1 · ω2) denotes the world from ΩΣ that assigns the
truth values for variables in Σ1 as ω1 and truth values for variables in Σ2 as ω2.
For ω ∈ΩΣ, the world from ΩΣ1 that assigns the truth values for variables in
Σ1 as ω is denoted as ω|Σ1.
A conditional (B|A) connects two formulas A, B and represents the rule
“If A then usually B”, where A is called the antecedent and B the consequent
of the conditional. The conditional language over a signature Σ is denoted as
(L|L)Σ = {(B|A) | A, B ∈LΣ}. A ﬁnite set of conditionals is called a conditional
belief base. We use a three-valued semantics of conditionals in this paper [8]. For
a world ω a conditional (B|A) is either veriﬁed by ω if ω |= AB, falsiﬁed by ω if
ω |= AB, or not applicable to ω if ω |= A. Popular models for conditional belief
bases are ranking functions (also called ordinal conditional functions, OCF) [27]
and total preorders (TPO) on ΩΣ [7].
3
Syntax Splitting and SPO-Based Inductive Inference
Reasoning with conditionals is often modelled by inference relations. An infer-
ence relation is a binary relation |∼on formulas over an underlying signature
Σ with the intuition that A |∼B means that A (plausibly) entails B. (Non-
monotonic) inference is closely related to conditionals: an inference relation |∼
can also be seen as a set of conditionals {(B|A) | A, B ∈LΣ, A |∼B}. The follow-
ing deﬁnition formalizes the inductive completion of a belief base to an inference
relation.
Deﬁnition 1 (inductive inference operator [14]). An inductive inference
operator is a mapping C : Δ →|∼Δ that maps each belief base to an inference
relation such that direct inference (DI) and trivial vacuity (TV) are fulﬁlled, i.e.,
(DI) if (B|A) ∈Δ then A |∼Δ B and
(TV) if Δ = ∅and A |∼Δ B then A |= B.
Examples for inductive inference operators are p-entailment [19], system Z
[24], model-based reasoning with respect to a c-representation [12,13], or various
forms of c-inference taking sets of c-representations into account [2,3].
The concept of syntax splitting was originally developed by Parikh [23]
describing that a belief set contains independent information over diﬀerent parts
of the signature. He proposed a postulate (P) stating that for a belief set with
a syntax splitting the revision with a formula relevant to only one such part
should only aﬀect the information about that part of the signature. The notion

Properties of System W
209
of syntax splitting was later extended to other representations of beliefs such as
ranking functions [15] and belief bases [14]. Furthermore, in [14] properties for
inductive inference operators, that govern the behaviour for belief bases with
syntax splitting, were formulated. To formulate syntax splitting postulates for
inductive inference operators, we need a notion of syntax splitting for belief
bases.
Deﬁnition 2 (syntax splitting for belief bases (adapted from [14])). Let
Δ be a belief base over a signature Σ. A partitioning {Σ1, . . . , Σn} of Σ is a
syntax splitting for Δ if there is a partitioning {Δ1, . . . , Δn} of Δ such that
Δi ⊆(L|L)Σi for every i = 1, . . . , n.
In this paper, we focus on syntax splittings {Σ1, Σ2} of Δ with two parts
and corresponding partition {Δ1, Δ2}, denoted as Δ = Δ1

Σ1,Σ2
Δ2. Results for
belief bases with syntax splittings in more than two parts can be obtained by
iteratively applying the postulates and constructions presented here.
Example 1 (Δve).
Consider the signature Σ = {m, b, e, t, g} for modelling
aspects about vehicles with the intended meaning (m) being a motorized vehicle,
(b) being a bike, (e) having an electric motor, (t) having two wheels, and (g)
requiring gasoline. The belief base
Δve = {(m|e), (g|m), (g|me), (t|b)}
over Σ states that vehicles with an electric motor are usually motorized vehicles,
motorized vehicles usually require gasoline, motorized vehicles with an electric
motor usually do not require gasoline, and bikes usually have two wheels. This
belief base has a syntax splitting Δve = Δ1

Σ1,Σ2
Δ2 with Σ1 = {m, e, g}, Σ2 =
{b, t} and
Δ1 = {(m|e), (g|m), (g|me)}, Δ2 = {(t|b)}.
For belief bases with syntax splitting, the postulate (Rel) describes that con-
ditionals corresponding to one part of the syntax splitting do not have any inﬂu-
ence on inferences that only use the other part of the syntax splitting, i.e., that
only conditionals from the considered part of the syntax splitting are relevant.
(Rel) [14] An inductive inference operator C : Δ →|∼Δ satisﬁes (Rel) if for
any Δ = Δ1

Σ1,Σ2
Δ2, and for any A, B ∈LΣi for i = 1, 2 we have that
A |∼Δ B
iﬀ
A |∼Δi B.
(1)
The postulate (Ind) describes that inferences should not be aﬀected by beliefs
in formulas over other sub-signatures in the splitting, i.e., inferences using only
atoms from one part of the syntax splitting should be drawn independently of
beliefs about other parts of the splitting.

210
J. Haldimann and C. Beierle
(Ind) [14] An inductive inference operator C : Δ →|∼Δ satisﬁes (Ind) if for
any Δ = Δ1

Σ1,Σ2
Δ2, and for any A, B ∈LΣi, D ∈LΣj for i, j ∈{1, 2}, i ̸= j
such that D is consistent, we have
A |∼Δ B
iﬀ
AD |∼Δ B.
(2)
Syntax splitting is the combination of (Rel) and (Ind):
(SynSplit) [14] An inductive inference operator satisﬁes (SynSplit) if it satis-
ﬁes (Rel) and (Ind).
The eﬀect of the postulates is illustrated by the following example.
Example 2. Consider the belief base Δve = Δ1

Σ1,Σ2
Δ2 from Example 1. We can
deduce b |∼Δvet with any inductive inference operator C : Δ →|∼Δ because of
(DI). The postulate (Rel) requires that b |∼Δ2t should hold because the formulas
b and t contain only variables from Σ2. The postulate (Ind) requires that be |∼Δ t
should hold because e contains only variables from Σ1.
Note that in Example 2 we cannot deduce either be |∼z
Δve t with system Z or
be |∼P
Δve t with system P; the additional information e from an independent part
of the signature prevents the deduction of t. Therefore, both p-entailment and
system Z do not fulﬁl (SynSplit).
In [14], the properties (Rel), (Ind), and (SynSplit) are lifted to total preorders
and ranking functions since both TPOs and OCFs induce inference relations.
But there are also nonmonotonic inference methods induced by a strict partial
order on worlds that cannot be expressed by a TPO or an OCF, e.g., structural
inference [16] or system W [18]. Therefore, in the following we will develop syntax
splitting postulates tailored to inference relations induced by strict partial orders.
Strict partial orders (SPOs) are irreﬂexive, transitive, and antisymmetric
binary relations. Analogously to inductive inference operator for TPOs, a SPO-
based inductive inference operators uses a SPO on worlds as intermediate step.
To deﬁne SPO-based inductive inference operators, we lift SPOs on worlds to
formulas ﬁrst.
Deﬁnition 3. Let ≺be a strict partial order on ΩΣ and A, B ∈LΣ. Then
A ≺B
iﬀ
for every ω′ ∈ΩB there is an ω ∈ΩA such that ω ≺ω′.
(3)
Note that we can use (3) to deﬁne when a strict partial order ≺on worlds
is a model of a conditional (B|A) by setting ≺|= (B|A) iﬀAB ≺AB, i.e., iﬀ
the veriﬁcation AB of (B|A) is strictly smaller in ≺than the falsiﬁcation AB
of (B|A); furthermore ≺|= Δ if ≺|= (B|A) for every (B|A) ∈Δ. Using this
concept, every strict partial order ≺induces an inference relation |∼≺by
A |∼≺B
iﬀ
AB ≺AB.
(4)

Properties of System W
211
Deﬁnition 4 (SPO-based inductive inference operator).
A SPO-based
inductive inference operator is a mapping Cspo : Δ →≺Δ that maps a belief
base to a SPO ≺Δ such that ≺Δ |= Δ and ≺∅= ∅. The induced inference |∼Δ
is obtained from ≺Δ as in (4).
The syntax splitting postulates can now be formulated for the case of SPO-
based inductive inference operators.
(Relspo) A SPO-based inductive inference operator Cspo : Δ →≺Δ satisﬁes
(Relspo) if for Δ = Δ1

Σ1,Σ2
Δ2 and for A, B ∈LΣi for i = 1, 2, we have
A ≺Δ B
iﬀ
A ≺Δi B.
(5)
(Indspo) A SPO-based inductive inference operator Cspo : Δ →≺Δ satisﬁes
(Indspo) if for any Δ = Δ1

Σ1,Σ2
Δ2, and for any A, B ∈LΣi, D ∈LΣj for
i, j ∈{1, 2}, i ̸= j, such that D is consistent, we have
A ≺Δ B
iﬀ
AD ≺Δ BD.
(6)
(SynSplitspo) A SPO-based inductive inference operator Cspo : Δ →≺Δ satis-
ﬁes (SynSplitspo) if it satisﬁes (Relspo) and (Indspo).
The new postulates for SPO-based inductive inference operators cover the
initial postulates (Rel) and (Ind).
Proposition 1. If a SPO-based inductive inference operator satisﬁes (Relspo),
then it satisﬁes (Rel).
Proof. Let Δ = Δ1

Σ1,Σ2
Δ2. Let i ∈{1, 2} and A, B ∈LΣi. As AB, AB ∈LΣi,
we have A |∼Δ B
iﬀ
AB ≺Δ AB
iﬀ
AB ≺Δi AB
iﬀ
A |∼Δi B.
⊓⊔
Proposition 2. If an inductive inference operator for strict partial orders sat-
isﬁes (Indspo), then it satisﬁes (Ind).
Proof. Let Δ = Δ1

Σ1,Σ2
Δ2. Let i, j ∈{1, 2}, i ̸= j and A, B ∈LΣi, D ∈LΣj
such that D is consistent. As AB, AB ∈LΣi, we have A |∼Δ B
iﬀ
AB ≺Δ
AB
iﬀ
ABD ≺Δ ABD
iﬀ
AD |∼Δ B.
⊓⊔
Hence, if a SPO-based inductive inference operator C satisﬁes both (Relspo)
and (Indspo), and thus also (SynSplitspo), then it also satisﬁes (SynSplit).
Having the postulates for SPO-based inference at hand enables easier and
more succinct proofs for showing that an inductive inference operator for SPOs
satisﬁes syntax splitting. In the following, we will exploit this for showing that
system W fully complies with syntax splitting.

212
J. Haldimann and C. Beierle
4
System W and the Preferred Structure on Worlds
System W is an inductive inference operator [18] that takes into account the
inclusion maximal tolerance partition of a belief base Δ, which is also used for
the deﬁnition of system Z [24].
Deﬁnition 5 (inclusion maximal tolerance partition [24]). A conditional
(B|A) is tolerated by Δ = {(Bi|Ai) | i = 1, . . . , n} if there exists a world
ω ∈ΩΣ such that ω veriﬁes (B|A) and ω does not falsify any conditional in Δ,
i.e., ω |= AB and ω |= n
i=1(Ai∨Bi). The inclusion maximal tolerance partition
OP(Δ) = (Δ0, . . . , Δk) of a consistent belief base Δ is the ordered partition of
Δ where each Δi is the inclusion maximal subset of n
j=i Δj that is tolerated by
n
j=i Δj.
It is well-known that OP(Δ) exists iﬀΔ is consistent; moreover, because the
Δi are chosen inclusion-maximal, the tolerance partitioning is unique [24].
In addition to OP(Δ), system W also takes into account the structural infor-
mation which conditionals are falsiﬁed. System W is based on a binary relation
called preferred structure on worlds <w
Δ over ΩΣ induced by every consistent
belief base Δ.
Deﬁnition 6 (ξj, ξ, preferred structure <w
Δ on worlds [18]). Consider a
consistent belief base Δ = {ri = (Bi|Ai) | i ∈{1, . . . , n}} with the tolerance
partition OP(Δ) = (Δ0, . . . , Δk). For j = 0, . . . , k, the functions ξj and ξ are
the functions mapping worlds to the set of falsiﬁed conditionals from the set Δj
in the tolerance partition and from Δ, respectively, given by
ξj(ω) := {ri ∈Δj | ω |= AiBi},
(7)
ξ(ω) := {ri ∈Δ | ω |= AiBi}.
(8)
The preferred structure on worlds is given by the binary relation <w
Δ ⊆Ω × Ω
deﬁned by, for any ω, ω′ ∈Ω,
ω <w
Δ ω′ iﬀthere exists an m ∈{0 , . . . , k} such that
ξi(ω) = ξi(ω′)
∀i ∈{m + 1 , . . . , k} and
(9)
ξm(ω) ⫋ξm(ω′) .
(10)
Thus, ω <w
Δ ω′ if and only if ω falsiﬁes strictly fewer conditionals than ω′ in
the partition with the biggest index m where the conditionals falsiﬁed by ω and
ω′ diﬀer. Note, that <w
Δ is a strict partial order [18, Lemma 3].
Deﬁnition 7 (system W, |∼w
Δ [18]).
Let Δ be a belief base and A, B be
formulas. Then B is a system W inference from A (in the context of Δ), denoted
A |∼w
ΔB, if for every ω′ ∈ΩAB there is an ω ∈ΩAB such that ω <w
Δ ω′.
Thus, employing Deﬁnition 4, since <w
Δ is a strict partial order, system W is
a SPO-based inductive inference operator Cw : Δ →<w
Δ.
System W fulﬁls system P, extends system Z [24] and c-inference [2], and
enjoys further desirable properties for nonmonotonic reasoning like avoiding the
drowning problem [6,18]. We illustrate system W with an example.

Properties of System W
213
Fig. 1. The preferred structure on worlds induced by the belief base Δve from Exam-
ple 3. Edges that can be obtained from transitivity are omitted.
Example 3. Consider the belief base Δve = {(m|e), (g|m), (g|me), (t|b)} over the
signature Σ = {m, b, e, t, g} from Example 1. The inclusion maximal tolerance
partition of Δ is OP(Δve) = (Δ0, Δ1) with Δ0 = {(g|m), (t|b)} and Δ1 =
{(m|e), (g|me)}. The resulting preferred structure <w
Δve is given in Fig. 1. Using
<w
Δve we can verify that for each world ω′ with ω′ |= bet there is a world ω with
ω |= bet such that ω <w
Δve ω′. Therefore, with system W we can infer be |∼w
Δvet,
complying with (SynSplit) (see Example 2).
As system W is a form of nonmonotonic reasoning, it does not fulﬁl monotony.
For instance, in Example 3 we have m |∼w
Δveg but me ̸|∼w
Δveg.
Several weaker notions of monotony have been introduced for nonmonotonic
reasoning. As system W fulﬁls system P, system W fulﬁls cautious monotony:
(CM) If A |∼B and A |∼C then AB |∼C.
Three further monotony related properties are rational monotony (RM),
weak rational monotony (WRM), and semi-monotony (SM) [9,26]:
(RM) If A |∼B and A ̸|∼C then AC |∼B.
(WRM) If ⊤|∼B and ⊤̸|∼A then A |∼B.
(SM) If Δ ⊆Δ′ and A |∼Δ B then A |∼Δ′B.
First, we observe that system W does not satisfy rational monotony, in con-
trast to, e.g., system Z or lexicographic inference [22].
Proposition 3. System W does not fulﬁl rational monotony (RM).
Proof. Let Σ = {a, b, c}. Consider the belief base Δ = {(ac ∨bc|⊤), (a ∨b|⊤),
(a|⊤), (ab|⊤)} and the formulas A = ¬(abc), B = abc ∨abc, and C = ¬(ac). We
have A |∼w
ΔB and A ̸|∼w
ΔC but not AC |∼w
ΔB.
⊓⊔

214
J. Haldimann and C. Beierle
Table 1. Comparison of properties of inductive inference operators.
CM RM WRM SM Extends rational closure
p-entailment
Yes
No
No
Yes
No
system Z
Yes
Yes
Yes
No
Yes
c-inference
Yes
No
Yes
No
No
system W
Yes
No
Yes
No
Yes
lexicographic inference Yes
Yes
Yes
No
Yes
Requiring (RM) to hold for a nonmonotonic inference method is a rather
strict requirement; for instance, p-entailment and c-inference also do not satisfy
(RM). On the other hand, extending rational closure [21] is a desirable property
of nonmonotonic inference [22], and it has been shown that system W extends
rational closure [18, Proposition 13]. While system W does not satisfy (RM) it
does satisfy the weaker notion (WRM).
Proposition 4. System W fulﬁls weak rational monotony (WRM).
Proof. Let Σ be a signature, Δ a consistent belief base and |∼w
Δ the instance
of a system W inference over Σ induced by Δ with the preferred order <w
Δ. Let
A, B ∈LΣ such that ⊤|∼w
ΔB and ⊤̸|∼w
ΔA.
Let Ωnf ⊆ΩΣ be the set of all worlds that falsify none of the conditionals in
Δ. As Δ is consistent, Ωnf is not empty. The worlds in Ωnf are lower than other
worlds with respect to <w
Δ: for every ω′ ∈Ωnf , ω ∈ΩΣ \ Ωnf we have ω′ <w
Δ ω.
For worlds ω, ω′ ∈Ωnf neither ω <w
Δ ω′ nor ω′ <w
Δ ω holds. Because ⊤|∼w
ΔB,
every world in Ωnf models B, i.e. Ωnf ⊆Mod (B). Because ⊤̸|∼w
ΔA, there is at
least one world ωA ∈Ωnf such that ω |= A.
Let ω be a model of AB. Ωnf ⊆Mod (B) (see above) entails that ω /∈Ωnf .
Because ωA ∈Ωnf and ω /∈Ωnf we know that ωA <w
Δ ω. Furthermore, we have
ωA |= AB. Therefore, A |∼w
ΔB. In summary, system W fulﬁls (WRM).
⊓⊔
While p-entailment fulﬁls (SM), system W, like many other inference meth-
ods, does not satisfy (SM).
Proposition 5. System W does not fulﬁl semi-monotony (SM).
Proof. Let Σ = {a, b} and consider the belief bases Δ = {(b|⊤)} and Δ′ =
{(b|⊤), (b|a)}. We have a |∼w
Δb but a ̸|∼w
Δ′b.
⊓⊔
Table 1 summarizes some properties of system W and of other inductive
inference operators for comparison. System W behaves similar to c-inference
with respect to the properties (CM), (RM), (WRM), and (SM), but in contrast
to c-inference, it extends rational closure [21].

Properties of System W
215
5
Syntax Splitting and System W
In this section, we evaluate system W with respect to syntax splitting. As system
W is an SPO-based inductive inference operator mapping a belief base Δ to
the preferred structure <w
Δ we can apply the postulates (Relspo), (Indspo), and
(SynSplitspo). For proving that system W fulﬁls these postulates, we will show
four lemmas on the properties of <w
Δ in the presence of a syntax splitting Δ =
Δ1

Σ1,Σ2
Δ2. While these lemmas are stated in the short paper [10], but without
proofs, we will now present full proofs for these lemmas.
Note that we consider the belief bases Δ1, Δ2 as belief bases over Σ = Σ1∪Σ2
in this section. Thus, in particular <w
Δ1 and <w
Δ2 are relations on ΩΣ and the
inference relations induced by Δ1, Δ2 are calculated with respect to Σ.
The following Lemma 1 shows how a syntax splitting on a belief base carries
over to the corresponding inclusion maximal tolerance partitioning.
Lemma 1. Let Δ = Δ1

Σ1,Σ2
Δ2 be a consistent belief base with syntax splitting.
Let OP(Δ) = (Δ0, . . . , Δk) and let OP(Δi) = (Δ0
i , . . . , Δli
i ) for i = 1, 2.
1. For i = 1, 2 and j = 0, . . . , li it is Δj
i = Δj ∩Δi and thus especially Δj
i ⊆Δj.
2. max{l1, l2} = k
3. If l1 ⩽l2, we have Δj =

Δj
1 ∪Δj
2
for j = 1, . . . , l1
Δj
2
for j = l1 + 1, . . . , k
Proof. Ad 1.:
W.l.o.g. consider i = 1. We will show that Δ0
1 = Δ0 ∩Δ1 for
Δ1 ̸= ∅. This implies Δj
1 = Δj ∩Δ1 for j = 1, . . . , l1 as the tolerance partitioning
can be constructed by recursively selecting the conditionals tolerated by all other
conditionals in the belief base.
Every conditional r ∈Δ1 is tolerated by all conditionals in (L|L)Σ2 that are
not contradictory. Especially, r is tolerated by all conditionals in Δ2. Therefore,
r is tolerated by all conditionals in Δ iﬀit is tolerated by all conditional in Δ1.
If q ∈Δ0
1, then it is in Δ1 and tolerated by every conditional in Δ1. Hence,
q is also in Δ and tolerated by every conditional in Δ (see above). Thus, q ∈
Δ0 ∩Δ1. If p ∈Δ0 ∩Δ1, then it is in Δ1 and it is tolerated by every conditional
in Δ. Therefore, q is also tolerated by Δ1. Thus, q ∈Δ0
1. Together, we have
Δ0
1 = Δ0 ∩Δ1.
Ad 2:
As Δi ⊆Δ for i = 1, 2 we have l1, l2 ⩽k. As Δk is not empty, it
contains at least one conditional r. This r is either in Δ1 or in Δ2. Let i ∈{1, 2}
be such that r ∈Δi. Then there is some m such that r ∈Δm
i . With (1.) we have
Δm
i
⊆Δm. As r can only be in one set of the tolerance partitioning of Δ we
have m = k. Therefore, the tolerance partitioning of Δi has at least k elements,
i.e., li = k.
Ad 3:
First, consider the case that j ⩽l1 and therefore also j ⩽l2. As
Δj
i ⊆Δj for i = 1, 2 we get Δj ⊇Δj
1 ∪Δj
2. Every r ∈Δj is either in Δ1 or in
Δ2. W.l.o.g. assume r ∈Δ1. Then there is some m such that r ∈Δm
1 . With (1.)

216
J. Haldimann and C. Beierle
we have Δm
1 ⊆Δm. As r can only be in one set of the tolerance partitioning of
Δ we have m = j. Therefore, Δj ⊆Δj
1 ∪Δj
2 and thus Δj = Δj
1 ∪Δj
2.
Now consider l1 < j ⩽k. From (1.) we get Δj ⊇Δj
2. Analogous to the ﬁrst
case, we know that every conditional r ∈Δj is either in Δj
1 or Δj
2. Because j > l1
there is no Δj
1 in the tolerance partitioning of Δ1, and thus we have r ∈Δj
2.
Therefore, Δj ⊆Δj
2 and thus Δj = Δj
2.
⊓⊔
If we have ω <w
Δ ω′, then there is some conditional r that falsiﬁes ω′ but not
ω and thus causes the ⫋relation in (9) in Deﬁnition 6. If Δ = Δ1

Σ1,Σ2
Δ2, this
r is either in Δ1 or in Δ2. Lemma 2 states that the relation ω <w
Δ ω′ can also
be obtained using only Δ1 or only Δ2.
Lemma 2. Let Δ = Δ1

Σ1,Σ2
Δ2 and let ω, ω′ ∈Ω. If ω <w
Δ ω′, then ω <w
Δ1 ω′
or ω <w
Δ2 ω′.
Proof. Let OP(Δ)
=
(Δ0, . . . , Δk) and let ξ, ξi be the functions as in
Deﬁnition 6. Let ω, ω′ ∈Ω be worlds with ω <w
Δ ω′. By deﬁnition of <w
Δ
there is an m ∈{0, . . . , k} such that ξi(ω) = ξi(ω′) for every i = m + 1, . . . , k
and ξm(ω) ⫋ξm(ω′). Therefore, there is an r ∈Δ such that r ∈ξm(ω′) and
r /∈ξm(ω). The conditional r is either in Δ1 or Δ2. Assume that r ∈Δx with
x being either 1 or 2. Let OP(Δx) = (Δ0
x, . . . , Δl
x) be the tolerance partition of
Δx. Let ξx, ξi
x for i = 0, . . . , k be the functions mapping worlds to the set of fal-
siﬁed conditionals for Δx. Because Δi
x ⊆Δi (see Lemma 1) and ξi(ω) = ξi(ω′)
for every i = m + 1, . . . , k we have ξi
x(ω) = ξi
x(ω′) for every i = m + 1, . . . , l.
And because r ∈ξm
x (ω′) and r /∈ξm
x (ω) we have ξm
x (ω) ⫋ξm
x (ω′). Therefore,
ω <w
Δx ω′.
Hence, we have ω <w
Δ1 ω′ or ω <w
Δ2 ω′.
⊓⊔
Note, that both ω <w
Δ1 ω′ and ω <w
Δ2 ω′ might be true.
Example 4. Consider the belief base Δve = Δ1

Σ1,Σ2
Δ2 from Examples 1 and
3 together with the worlds ω = mbetg and ω′ = mbetg. We have ω <w
Δve ω′. It
also holds that ω <w
Δ1 ω′ and ω <w
Δ2 ω′.
Lemma 3 considers the reverse direction of Lemma 2 and shows a situation
where we can infer ω <w
Δ ω′ from ω <w
Δ1 ω′ for a belief base with syntax splitting.
Lemma 3. Let Δ = Δ1

Σ1,Σ2
Δ2 and let ω, ω′ ∈Ω. If ω <w
Δ1 ω′ and ω|Σ2 =
ω′|Σ2, then ω <w
Δ ω′.
Proof. Let ω, ω′ ∈Ω with ω <w
Δ1 ω′ and ω|Σ2 = ω′|Σ2. Let OP(Δ) =
(Δ0, . . . , Δk) and let ξ, ξi be the functions as in Deﬁnition 6. Let OP(Δ1) =
(Δ0
1, . . . , Δl
1) be the tolerance partition of Δ1. Let ξx, ξi
x for i = 0, . . . , k be the
functions mapping worlds to the set of falsiﬁed conditionals for Δx. By deﬁ-
nition of <w
Δ1 there is an m ∈{0, . . . , l} such that ξi
1(ω) = ξi
1(ω′) for every

Properties of System W
217
i = m + 1, . . . , l and ξm
1 (ω) ⫋ξm
1 (ω′). With Lemma 1 we have ξj
x(ω∗) = {r ∈
Δj
x | ω∗falsiﬁes r} = {r ∈Δx ∩Δj | ω∗falsiﬁes r} = ξj ∩Δx for every world ω∗,
x = 1, 2 and j = 0, . . . , l. This implies ξi(ω)∩Δ1 = ξi
1(ω) = ξi
1(ω′) = ξi(ω′)∩Δ1
for every i = m + 1, . . . , k and ξm(ω) ∩Δ1 = ξm
1 (ω) ⫋ξm
1 (ω) = ξm(ω′) ∩Δ1.
Because ω|Σ2 = ω′|Σ2, we have ξi(ω)∩Δ2 = ξi(ω′)∩Δ2 for every i = 0, . . . , k.
Hence, we have ξi(ω) = ξi(ω′) for every i = m + 1, . . . , k and ξm(ω) ⫋ξm(ω′)
and therefore ω <w
Δ ω′.
⊓⊔
The next Lemma 4 captures that the variable assignment for variables that
do not occur in the belief set has no inﬂuence on the position of a world in the
resulting preferential structure on worlds.
Lemma 4. Let Δ = Δ1

Σ1,Σ2
Δ2 and ωa, ωb, ω′ ∈ΩΣ with ωa|Σ1 = ωb|Σ1. Then
we have ωa <w
Δ1 ω′ iﬀωb <w
Δ1 ω′.
Proof. Let ωa, ωb, ω′ be as introduced above. As ωa|Σ1 = ωb|Σ1 and Δ1 ⊆
(L|L)Σ1 the worlds ωa and ωb falsify the same conditionals in Δ1. Whether
ωX <w
Δ1 ω′ holds or does not depends entirely on the conditionals in Δ1 falsiﬁed
by ωX and ω′ for X ∈{a, b}. Because ωa and ωb behave alike with respect to
falsiﬁcation of conditionals in Δ1, we have that ωa <w
Δ1 ω′ iﬀωb <w
Δ1 ω′.
⊓⊔
Using these lemmas, we show that system W fulﬁls both (Relspo) and
(Indspo).
Proposition 6. System W fulﬁls (Relspo).
Proof. Let Δ = Δ1

Σ1,Σ2
Δ2 be a belief base with syntax splitting and let A, B ∈
LΣ1 be propositional formulas. W.l.o.g. we need to show that
A <w
Δ B if and only if A <w
Δ1 B.
(11)
Direction ⇒of (11):
Assume that A <w
Δ B. We need to show that A <w
Δ1
B. Let ω′ be any world in ΩB. Now choose ω′
min ∈Ω such that
1. ω′
min ⩽w
Δ ω′,
2. ω′|Σ1 = ω′
min |Σ1, and
3. there is no world ω′
min2 with ω′
min2 < ω′
min that fulﬁls (1.) and (2.).
Such an ω′
min exists because ω′ fulﬁls properties (1.) and (2.), <w
Δ is irreﬂexive
and transitive, and there are only ﬁnitely many worlds in Ω.
Because of (2.) and because ω′ |= B we have that ω′
min |= B. Because
A <w
Δ B, there is a world ω such that ω |= A and ω <w
Δ ω′
min. Lemma 2 yields
that ω <w
Δ1 ω′
min or ω <w
Δ2 ω′
min.
The case ω <w
Δ2 ω′
min is not possible: Assuming ω <w
Δ2 ω′
min, it follows that
ω′
min2 = (ω′
min |Σ1 · ω|Σ2) <w
Δ2 ω′
min with Lemma 4. With Lemma 3 it follows
that ω′
min2 <w
Δ ω′
min. This contradicts (3.). Hence, ω <w
Δ1 ω′
min. Because of (2.)

218
J. Haldimann and C. Beierle
and Lemma 4 it follows that ω <w
Δ1 ω′. As we can ﬁnd an ω such that ω <w
Δ1 ω′
and ω |= A for every ω′ |= B we have that A <w
Δ1 B.
Direction ⇐of (11):
Assume that A <w
Δ1 B. We need to show that A <w
Δ
B. Let ω′ be any world in ΩB. Because A <w
Δ1 B, there is a world ω∗such that
ω∗|= A and ω∗<w
Δ1 ω′. Let ω = (ω∗|Σ1 · ω′|Σ2). Because ω∗|= A we have that
ω |= A. Furthermore, with Lemma 4 it follows that ω <w
Δ1 ω′ and thus ω <w
Δ ω′
with Lemma 3. As we can construct ω such that ω <w
Δ ω′ and ω |= A for every
ω′ |= B we have that A <w
Δ B.
⊓⊔
Proposition 7. System W fulﬁls (Indspo).
Proof. Let Δ = Δ1

Σ1,Σ2
Δ2 be a belief base with syntax splitting. W.l.o.g. let
A, B ∈LΣ1 and C ∈LΣ2 be propositional formulas such that C is consistent.
We need to show that
A <w
Δ B if and only if AC <w
Δ BC.
(12)
Direction ⇒of (12):
Assume that A <w
Δ B. We need to show that AC <w
Δ
BC. Let ω′ be any world in ΩBC. Now choose ω′
min ∈Ω such that
1. ω′
min ⩽w
Δ ω′,
2. ω′|Σ1 = ω′
min |Σ1, and
3. there is no world ω′
min2 < ω′
min that fulﬁls (1.) and (2.).
Such an ω′
min exists because ω′ fulﬁls properties (1.) and (2.), <w
Δ is irreﬂexive
and transitive, and there are only ﬁnitely many worlds in Ω. Because of (2.) and
because ω′ |= BC we have that ω′
min |= B. Because A <w
Δ B, there is a world ω∗
such that ω∗|= A and ω∗<w
Δ ω′
min. Lemma 2 yields that either ω∗<w
Δ1 ω′
min or
ω∗<w
Δ2 ω′
min.
The case ω∗<w
Δ2 ω′
min is not possible: Assuming ω∗<w
Δ2 ω′
min, it follows
that ω′
min2 = (ω′
min |Σ1 · ω∗|Σ2) <w
Δ2 ω′
min with Lemma 4. With Lemma 3 it
follows that ω′
min2 <w
Δ ω′
min. This contradicts (3.). Hence, ω∗<w
Δ1 ω′
min. Let
ω = (ω∗|Σ1 · ω′|Σ2). Because ω∗|= A we have that ω |= A. Because ω′ |= C we
have that ω |= C. Because of (2.) and Lemma 4 it follows that ω <w
Δ1 ω′ and
thus with Lemma 3 it follows that ω <w
Δ ω′. As we can construct an ω for every
ω′ we have that AC |∼w
ΔBC.
Direction ⇐of (12):
Assume that AC <w
Δ BC. We need to show that
A <w
Δ B. Let ω′ be any world in ΩB. Now choose ω′
min ∈Ω such that
1. ω′
min |= C
2. ω′|Σ1 = ω′
min |Σ1, and
3. there is no world ω∗∗′ < ω′
min that fulﬁls (1.) and (2.).
Such an ω′
min exists because C is consistent, <w
Δ is irreﬂexive and transitive,
and there are only ﬁnitely many worlds in Ω. Because of (2.) and because
ω′ |= B we have that ω′
min |= B. Because of (1.) we have that ω′
min |= C.

Properties of System W
219
Because AC <w
Δ BC, there is a world ω∗such that ω∗|= AC and ω∗<w
Δ ω′
min.
Lemma 2 yields that either ω∗<w
Δ1 ω′
min or ω∗<w
Δ2 ω′
min.
The case ω∗<w
Δ2 ω′
min is not possible: Assuming ω∗<w
Δ2 ω′
min, it follows
that ω′
min2 = (ω′
min |Σ1 · ω∗|Σ2) <w
Δ2 ω′
min with Lemma 4. With Lemma 3 it
follows that ω′
min2 <w
Δ ω′
min. This contradicts (3.). Hence, ω∗<w
Δ1 ω′
min. Let
ω = (ω∗|Σ1 · ω′|Σ2). Because ω∗|= A we have that ω |= A. Because of (2.) and
Lemma 4 it follows that ω <w
Δ1 ω′ and thus with Lemma 3 ω <w
Δ ω′.
As we can construct an ω for every ω′ we have that A <w
Δ B.
⊓⊔
From the fulﬁlment of the syntax splitting postulates for SPO-based inductive
inference operators we can conclude that system W also fulﬁls the general syntax
splitting postulates.
Proposition 8. System W fulﬁls (Rel).
Proof. This follows from Proposition 6 and Proposition 1.
⊓⊔
Proposition 9. System W fulﬁls (Ind).
Proof. This follows from Proposition 7 and Proposition 2.
⊓⊔
Combining Propositions 8 and 9 yields that system W fulﬁls (SynSplit).
Proposition 10. System W fulﬁls (SynSplit).
Thus, by employing the SPO-based syntax splitting postulates, we have
established a complete formal proof that system W fulﬁls (SynSplit).
6
Syntax Splitting and the Preferred Structure on Worlds
Let us assume that we have a belief base Δ = Δ1

Σ1,Σ2
Δ2 with syntax splitting.
To answer a query “does A entail B” for A, B ∈(L|L)Σ1 (or A, B ∈(L|L)Σ2)
we can employ the syntax splitting and consider only Δ1 (or Δ2) to answer
that query. But if A, B contain variables from both Σ1 and Σ2, or if we want
to calculate the preferred structure over Δ, (SynSplit) is not applicable. In this
section, we show how a syntax splitting aﬀects the preferred structure on worlds
over the whole belief base.
Deﬁnition 8 (hdl(ω, ω′, Δ)). Let OP(Δ) = (Δ0, . . . , Δk) and let ξ, ξi be as in
Deﬁnition 6. We deﬁne the number of the highest diﬀerentiating layer
hdl(ω, ω′, Δ) = max{i ∈N | ξi(ω) ̸= ξi(ω′)},
i.e., ξi(ω) ̸= ξi(ω′) and ξj(ω) = ξj(ω′) for i = hdl(ω, ω′, Δ) and j > i. If ω, ω′
falsify the same conditionals in Δ, we deﬁne hdl(ω, ω′, Δ) = −1.

220
J. Haldimann and C. Beierle
Example 5. Consider the belief base Δve from Example 1 with OP(Δve) =
(Δ0, Δ1) with Δ0 = {(g|m), (t|b)} for Δ1 = {(m|e), (g|me)} (see Example 3).
For ω = mbetg and ω′ = mbetg we have hdl(ω, ω′, Δve) = 1 because ω falsiﬁes
(g|me) from Δ1 and ω′ does not. The veriﬁcation behaviour of ω, ω′ regarding
Δ0 does not make a diﬀerence.
For ω′′ = mbetg and ω′′′ = mbetg we have hdl(ω′′, ω′′′, Δve) = 0 because
ξ1(ω′′) = ξ1(ω′′′) = {(m|e)} and ξ0(ω′′) ̸= ξ0(ω′′′).
The following proposition states that for a belief based Δ with syntax split-
ting we can calculate the preferred structure on worlds separately on the parts
of the syntax splitting for deciding whether ω <w
Δ ω′ holds for a pair of worlds
ω, ω′. The number hdl(ω, ω′, Δ) indicates how the preferred structures induced
by each of the sub-bases should be combined. Sometimes it is only necessary
to consider one of the parts in the syntax splitting, making answering queries
easier.
Proposition 11. Let Δ = Δ1

Σ1,Σ2
Δ2 and ω, ω′ ∈ΩΣ. Then ω <w
Δ ω′ iﬀeither
– hdl(ω, ω′, Δ1) > hdl(ω, ω′, Δ2) and ω|Σ1 <w
Δ1 ω′|Σ1 or
– hdl(ω, ω′, Δ1) < hdl(ω, ω′, Δ2) and ω|Σ2 <w
Δ2 ω′|Σ2 or
– hdl(ω, ω′, Δ1) = hdl(ω, ω′, Δ2) and ω|Σ1 <w
Δ1 ω′|Σ1 and ω|Σ2 <w
Δ2 ω′|Σ2.
Proof. Let Δ = Δ1

Σ1,Σ2
Δ2 and ω, ω′ ∈ΩΣ. We will prove both directions of
the “iﬀ” seperately.
Direction ⇒:
Assume ω <w
Δ ω′. We can distinguish three cases for the
numbers k = hdl(ω, ω′, Δ1) and l = hdl(ω, ω′, Δ2).
Case 1: hdl(ω, ω′, Δ1) > hdl(ω, ω′, Δ2)
In this case, we have ξk
1(ω) ̸= ξk
1(ω′)
and ξk
2(ω) = ξk
2(ω′). Additionally, we have ξi
1(ω) = ξi
1(ω′) and ξi
2(ω) = ξi
2(ω′) for
i > k. With Lemma 1 it follows that ξi(ω) = ξi(ω′) for i > k and ξk(ω) ̸= ξk(ω′).
As ω <w
Δ ω′ we know that ξk(ω) ⫋ξk(ω′) and therefore ξk
1(ω) ⫋ξk
1(ω′) because
of ξk
2(ω) = ξk
2(ω′) and Lemma 1. As the falsiﬁcation of conditionals in Δ1 over
Σ1 does not depend on the assignments of truth values for Σ1, we have that
ξk
1(ω|Σ1) ⫋ξk
1(ω′|Σ1) and therefore ω|Σ1 <w
Δ1 ω′|Σ1.
Case 2: hdl(ω, ω′, Δ1) < hdl(ω, ω′, Δ2)
Analogous to Case 1 we conclude
ω|Σ2 <w
Δ2 ω′|Σ2.
Case 3: hdl(ω, ω′, Δ1) = hdl(ω, ω′, Δ2)
In this case, we have ξk
1(ω) ̸= ξk
1(ω′)
and ξk
2(ω) ̸= ξk
2(ω′). Additionally, we have ξi
1(ω) = ξi
1(ω′) and ξi
2(ω) = ξi
2(ω′) for
i > k. With Lemma 1 it follows that ξi(ω) = ξi(ω′) for i > k and ξk(ω) ̸= ξk(ω′).
As ω <w
Δ ω′ we know that ξk(ω) ⫋ξk(ω′). Therefore, we have ξk
1(ω) ⫋ξk
1(ω′) and
ξk
2(ω) ⫋ξk
2(ω′). As the falsiﬁcation of conditionals in Δ1 over Σ1 does not depend
on the assignments of truth values for Σ1, we have that ξk
1(ω|Σ1) ⫋ξk
1(ω′|Σ1)
and ξk
2(ω|Σ2) ⫋ξk
2(ω′|Σ2). Hence, ω|Σ1 <w
Δ1 ω′|Σ1 and ω|Σ2 <w
Δ2 ω′|Σ2.
Direction ⇐:
Assume one of the three items in the proposition are true.
Case 1: hdl(ω, ω′, Δ1) > hdl(ω, ω′, Δ2) and ω|Σ1 <w
Δ1 ω′|Σ1
We have
ξk
1(ω) ̸= ξk
1(ω′) and ξk
2(ω) = ξk
2(ω′). Additionally, we have ξi
1(ω) = ξi
1(ω′) and

Properties of System W
221
ξi
2(ω) = ξi
2(ω′) for i > k. Because of ω|Σ1 <w
Δ1 ω′|Σ1, we have ξk
1(ω) ⫋ξk
1(ω′).
With Lemma 1 it follows that ξi(ω) = ξi(ω′) for i > k and ξk(ω) ⫋ξk(ω′).
Therefore ω <w
Δ ω′.
Case 2: hdl(ω, ω′, Δ1) < hdl(ω, ω′, Δ2) and ω|Σ2 <w
Δ2 ω′|Σ2
Analogous to
Case 1 we have ω <w
Δ ω′.
Case 3: hdl(ω, ω′, Δ1) = hdl(ω, ω′, Δ2) and ω1|Σ1 <w
Δ1 ω2|Σ1 and ω|Σ2 <w
Δ2
ω′|Σ2
We have ξi
1(ω) = ξi
1(ω′) and ξi
2(ω) = ξi
2(ω′) for i > k. Because of
ω|Σ1 <w
Δ1 ω′|Σ1, we have ξk
1(ω) ⫋ξk
1(ω′). Because of ω|Σ2 <w
Δ2 ω′|Σ2, we have
ξk
2(ω) ⫋ξk
2(ω′). With Lemma 1 it follows that ξi(ω) = ξi(ω′) for i > k and
ξk(ω) ⫋ξk(ω′). Therefore, ω <w
Δ ω′.
⊓⊔
Proposition 11 can be used to decide whether ω <w
Δ ω′ holds in presence of
a syntax splitting Δ = Δ1

Σ1,Σ2
Δ2 considering Δ1 and Δ2 separately.
Example 6. Consider again the belief base Δve = Δ1

Σ1,Σ2
Δ2 from Example 1
and Example 3. To check whether ω <w
Δve ω′ for ω = mbetg and ω′ = mbetg
holds, we ﬁrst calculate h1 = hdl(ω, ω′, Δ1) = 1 and h2 = hdl(ω, ω′, Δ2) = 0. As
h1 > h2 we only have to check whether ω <w
Δ1 ω′ holds. As this is the case, we
can conclude that ω <w
Δve ω′.
Applications of this approach include the calculation of <w
Δ or parts of this
relation. Deciding whether ω <w
Δ ω′ holds also coincides with deciding whether Δ
entails the base conditional (ω|ω ∨ω′) with system W. More generally, deciding
whether a conditional (B|A) is entailed by a certain belief base requires only
comparing models of the antecedent A in <w
Δ. If A has only few models, it is
not necessary to calculate the complete preferred structure <w
Δ but only a small
part of it to decide if (B|A) holds. Here, Proposition 11 might be of use as well
for simplifying inferences.
7
Relations Among Inductive Inference Operators
Some of the relations of system W to other inductive inference operators have
already been established. In [18] it is shown that system W properly captures and
strictly extends p-entailment, system Z, and c-inference. We will now investigate
and establish the relationship between system W and lexicographic inference.
Lexicographic inference [22] is an inductive inference operator that, similar to
system W, is based on an ordering on worlds which is obtained by considering
the falsiﬁed conditionals in each partition of the inclusion maximal tolerance
partitioning of the belief base. But unlike system W, lexicographic inference only
considers the number of falsiﬁed conditionals in each part of the partition instead
of checking for a subsumption of the sets of falsiﬁed conditionals. The following
deﬁnition of lexicographic inference is adapted from [22]; in it we employ the
notation introduced in Deﬁnition 6 and use min ≺S to denote the minima in the
set S with respect to the ordering ≺.

222
J. Haldimann and C. Beierle
Deﬁnition 9 (<lex
Δ , lexicographic inference). The lexicographic ordering on
vectors in Nn is deﬁned by (v1, . . . , vn) <lex (w1, . . . , wn) iﬀthere is a k ∈
{1, . . . , n} such that vk < wk and vj = wj for j = k + 1, . . . , n.
The binary relation ⩽lex
Δ ⊆Ω × Ω on worlds induced by a belief base Δ with
|OP(Δ)| = n is deﬁned by, for any ω, ω′ ∈Ω,
ω ⩽lex
Δ ω′
if
(|ξ1
Δ(ω)|, . . . , |ξn
Δ(ω)|) ⩽lex (|ξ1
Δ(ω′)|, . . . , |ξn
Δ(ω′)|).
For formulas F, G, A, B, lexicographic inference |∼lex
Δ
is induced by <lex
Δ :
F <lex
Δ G
iﬀ
min <lex
Δ {ω ∈Ω | ω |= F} <lex
Δ min <lex
Δ {ω ∈Ω | ω |= G}
A |∼lex
Δ B
iﬀ
AB <lex
Δ AB
As the deﬁnitions for lexicographic inference and system W are similar, we
want to compare these inference operators in more detail. First, we observe that
the condition for ω <lex
Δ ω′ is stronger than the condition for ω <w
Δ ω′.
Proposition 12. Let Δ be a belief base and ω, ω′ be worlds. Then ω <w
Δ ω′
implies ω <lex
Δ ω′.
Proof. If ω <w
Δ ω′ then there is a k such that ξk(ω) ⫋ξk(ω′) and ξi(ω) = ξi(ω′)
for i > k. This implies |ξk(ω)| < |ξk(ω′)| and |ξi(ω)| = |ξi(ω′)| for i > k. Hence,
ω <lex
Δ ω′.
⊓⊔
Using Proposition 12, we can show that every system W entailment is also
an entailment for lexicographic inference.
Proposition 13. Lexicographic inference captures system W, i.e., for a belief
base Δ and formulas A, B it holds that if A |∼w
ΔB then A |∼lex
Δ B.
Proof. Let A |∼w
ΔB. For every ω′ ∈Mod (AB), by the deﬁnition of system W
there is another world ω ∈Mod (AB) such that ω <w
Δ ω′. This implies that for
every ω′ ∈Mod (AB) there is a world ω ∈Mod (AB) such that ω <lex
Δ ω′. Hence,
AB <lex
Δ AB and therefore A |∼lex
Δ B.
⊓⊔
Moreover, some lexicographic inferences are not licensed by system W.
Proposition 14. There are belief bases Δ such that |∼w
Δ ⫋|∼lex
Δ .
Proof. Consider Δve from Example 1 as well as the worlds ω = mbetg and ω′ =
mbetg. We have that (|ξ0
Δ(ω)|, |ξ1
Δ(ω)|) = (0, 1) and (|ξ0
Δ(ω′)|, |ξ1
Δ(ω′)|) = (1, 1)
(see Example 3 for the ordered partition of Δ). Hence, ω <lex
Δ ω′ and therefore
|∼lex
Δ |= r with r = (ω|ω ∨ω′). But ω ̸<w
Δ ω′, and therefore |∼w
Δ ̸|= r.
⊓⊔
These propositions allow us to establish further relationships among the
inductive inference operators.
Proposition 15. Lexicographic inference captures and strictly extends c-
inference, i.e., every c-inference is also a lexicographic inference but there are
lexicographic inferences that are not c-inferences.

Properties of System W
223
Fig. 2. Overview over relationships between diﬀerent inference operators. An arrow
I1 →I2 indicates that inductive inference operator I1 is captured by I2 and that I1 is
strictly extended by I2 for some belief bases.
Proof. Because system W captures and strictly extends c-inference [18, Propo-
sition 11], this follows from Propositions 13 and 14.
⊓⊔
In a similar way, as system W captures system Z, Propositions 13 and 14
imply that lexicographic inference, which has very recently be shown to satisfy
syntax splitting [11], captures and strictly extends system Z and thus rational
closure. An overview over the relations between the inference relations mentioned
in this paper can be found in Fig. 2.
8
Conclusions and Further Work
In this paper, we investigated the properties of the inductive inference operator
system W. We show that system W fulﬁls (WRM) but not (RM) and or (SM).
We generalize results on syntax splitting and system W, introduce postulates
for SPO-based inductive inference operators, and prove that these postulates
are fulﬁlled by system W. We demonstrate that the inﬂuence of syntax splitting
on the preferred structure on worlds underlying system W may simplify query
answering. Furthermore, by establishing the relationship among system W and
other inductive inference operators we obtain a full map of capturing relations for
established inductive inference operators. In our current work, we are enlarging
this map by comparing system W to further inference approaches for conditional
belief bases. We are currently also extending the reasoning plattform InfOCF-
Web [20] by a web-based implementation of system W, and we work on more
eﬃcient algorithms for reasoning with system W. Furthermore, we investigate
normal forms of conditional belief bases respecting system W inferences [4,5].
Acknowledgement. We thank the anonymous reviewers for their detailed and helpful
comments. This work was supported by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation), grant BE 1700/10-1 awarded to Christoph Beierle as
part of the priority program “Intentional Forgetting in Organizations” (SPP 1921).
References
1. Adams, E.: The logic of conditionals. Inquiry 8(1–4), 166–197 (1965)
2. Beierle, C., Eichhorn, C., Kern-Isberner, G., Kutsch, S.: Properties of skeptical
c-inference for conditional knowledge bases and its realization as a constraint sat-
isfaction problem. Ann. Math. Artif. Intell. 83(3–4), 247–275 (2018)

224
J. Haldimann and C. Beierle
3. Beierle, C., Eichhorn, C., Kern-Isberner, G., Kutsch, S.: Properties and interrela-
tionships of skeptical, weakly skeptical, and credulous inference induced by classes
of minimal models. Artif. Intell. 297, 103489 (2021)
4. Beierle, C., Haldimann, J.: Normal forms of conditional belief bases respect-
ing inductive inference. In: Keshtkar, F., Franklin, M. (eds.) Proceedings of the
Thirty-Fifth International Florida Artiﬁcial Intelligence Research Society Confer-
ence (FLAIRS), Hutchinson Island, Florida, USA, 15–18 May 2022 (2022)
5. Beierle, C., Haldimann, J.: Normal forms of conditional knowledge bases respecting
system p-entailments and signature renamings. Ann. Math. Artif. Intell. 90(2),
149–179 (2022)
6. Benferhat, S., Cayrol, C., Dubois, D., Lang, J., Prade, H.: Inconsistency manage-
ment and prioritized syntax-based entailment. In: Proceedings of IJCAI 1993, vol.
1, pp. 640–647. Morgan Kaufmann Publishers, San Francisco (1993)
7. Darwiche, A., Pearl, J.: On the logic of iterated belief revision. Artif. Intell. 89(1–
2), 1–29 (1997)
8. de Finetti, B.: La pr´evision, ses lois logiques et ses sources subjectives. Ann. Inst.
H. Poincar´e 7(1), 1–68 (1937). Engl. transl. Theory of Probability, J. Wiley &
Sons, 1974
9. Goldszmidt, M., Pearl, J.: Qualitative probabilities for default reasoning, belief
revision, and causal modeling. Artif. Intell. 84(1–2), 57–112 (1996)
10. Haldimann, J., Beierle, C.: Inference with system W satisﬁes syntax splitting. In:
Principles of Knowledge Representation and Reasoning: Proceedings of the 19th
International Conference, KR 2022, Haifa, Israel, 31 July–5 August 2022 (2022, to
appear)
11. Heyninck, J., Kern-Isberner, G., Meyer, T.: Lexicographic entailment, syntax split-
ting and the drowning problem. In: 31st International Joint Conference on Artiﬁcial
Intelligence, IJCAI 2022. ijcai.org (2022, to appear)
12. Kern-Isberner, G.: Conditionals in Nonmonotonic Reasoning and Belief Revi-
sion. LNCS, vol. 2087. Springer, Heidelberg (2001). https://doi.org/10.1007/3-540-
44600-1 3
13. Kern-Isberner, G.: A thorough axiomatization of a principle of conditional preser-
vation in belief revision. Ann. Math. Artif. Intell. 40(1–2), 127–164 (2004)
14. Kern-Isberner, G., Beierle, C., Brewka, G.: Syntax splitting = relevance + indepen-
dence: New postulates for nonmonotonic reasoning from conditional belief bases.
In: Calvanese, D., Erdem, E., Thielscher, M. (eds.) Principles of Knowledge Rep-
resentation and Reasoning: Proceedings of the 17th International Conference, KR
2020, pp. 560–571. IJCAI Organization (2020)
15. Kern-Isberner, G., Brewka, G.: Strong syntax splitting for iterated belief revision.
In: IJCAI-2017, pp. 1131–1137 (2017)
16. Kern-Isberner, G., Eichhorn, C.: Structural inference from conditional knowledge
bases. Stud. Logica. 102(4), 751–769 (2014)
17. Komo, C., Beierle, C.: Nonmonotonic inferences with qualitative conditionals based
on preferred structures on worlds. In: Schmid, U., Kl¨ugl, F., Wolter, D. (eds.) KI
2020. LNCS (LNAI), vol. 12325, pp. 102–115. Springer, Cham (2020). https://doi.
org/10.1007/978-3-030-58285-2 8
18. Komo, C., Beierle, C.: Nonmonotonic reasoning from conditional knowledge bases
with system W. Ann. Math. Artif. Intell. 90(1), 107–144 (2021). https://doi.org/
10.1007/s10472-021-09777-9
19. Kraus, S., Lehmann, D., Magidor, M.: Nonmonotonic reasoning, preferential mod-
els and cumulative logics. Artif. Intell. 44(1–2), 167–207 (1990)

Properties of System W
225
20. Kutsch, S., Beierle, C.: InfOCF-Web: an online tool for nonmonotonic reasoning
with conditionals and ranking functions. In: Zhou, Z. (ed.) Proceedings of the Thir-
tieth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2021, Virtual
Event/Montreal, Canada, 19–27 August 2021, pp. 4996–4999. ijcai.org (2021)
21. Lehmann, D., Magidor, M.: What does a conditional knowledge base entail? Artif.
Intell. 55, 1–60 (1992)
22. Lehmann, D.: Another perspective on default reasoning. Ann. Math. Artif. Intell.
15(1), 61–82 (1995). https://doi.org/10.1007/BF01535841
23. Parikh, R.: Beliefs, belief revision, and splitting languages. Logic Lang. Comput.
2, 266–278 (1999)
24. Pearl, J.: System Z: a natural ordering of defaults with tractable applications to
nonmonotonic reasoning. In: Proceedings of TARK 1990, pp. 121–135. Morgan
Kaufmann (1990)
25. Peppas, P., Williams, M.A., Chopra, S., Foo, N.Y.: Relevance in belief revision.
Artif. Intell. 229((1–2)), 126–138 (2015)
26. Reiter, R.: A logic for default reasoning. Artif. Intell. 13, 81–132 (1980)
27. Spohn, W.: Ordinal conditional functions: a dynamic theory of epistemic states. In:
Harper, W., Skyrms, B. (eds.) Causation in Decision, Belief Change, and Statistics,
II, pp. 105–134. Kluwer Academic Publishers (1988)

Towards the Evaluation of Action
Reversibility in STRIPS Using Domain
Generators
Tobias Schwartz(B)
, Jan H. Boockmann
, and Leon Martin
University of Bamberg, Bamberg, Germany
{tobias.schwartz,jan.boockmann,leon.martin}@uni-bamberg.de
Abstract. Robustness is a major prerequisite for using AI systems in
real world applications. In the context of AI planning, the reversibility
of actions, i.e., the possibility to undo the eﬀects of an action using a
reverse plan, is one promising direction to achieve robust plans. Plans
only made of reversible actions are resilient against goal changes during
plan execution. This paper presents a naive implementation of a non-
deterministic theoretical algorithm for determining action reversibility
in STRIPS planning. However, evaluating action reversibility systems
turns out to be a diﬃcult challenge, as standard planning benchmarks
are hardly applicable. We observed that manually crafted domains and
in particular those obtained from domain generators easily contain bias.
Based on an existing domain generator, we propose two slight varia-
tions that exhibit a completely diﬀerent search tree characteristics. We
use these domain generators to evaluate our implementation in close
comparison to an existing ASP implementation and show that diﬀer-
ent generators indeed favor diﬀerent implementations. Thus, a variety of
domain generators is a necessary foundation for the evaluation of action
reversibility systems.
1
Introduction
A classical planning problem [8] is typically solved upon ﬁnding a sequence of
actions, i.e., a plan, that leads from the predeﬁned initial state to a desired
goal state. In this context, the notion of action reversibility has extensively been
studied recently [2,3,5,6,11] and is used to assess whether the eﬀects of apply-
ing an action can be undone using a reverse plan. Although this property is less
important in a static execution environment, it is crucial in a dynamic execu-
tion environment, e.g., as found in autonomous spacecraft control [14] or cloud
management [13] domains, where the desired goal state might change during the
execution of a precomputed plan. While a plan containing non-reversible actions
might lead to dead ends upon a goal change introduced by the environment, a
plan with only reversible actions is resilient against this change. A similar issue
arises in a closed world setting when using online planning strategies since these
are susceptible to dead ends [4]. Consequently, implementing action reversibility
algorithms in current AI planning tools can be useful.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2022
I. Varzinczak (Ed.): FoIKS 2022, LNCS 13388, pp. 226–236, 2022.
https://doi.org/10.1007/978-3-031-11321-5_13

Towards the Evaluation of Action Reversibility in STRIPS
227
The works in [3,11] introduced a framework for action reversibility that gen-
eralizes diﬀerent reversibility notions studied in the past [5,6]. Besides a com-
plexity analysis, which inherits the PSpace completeness of basic STRIPS plan-
ning [1], the work in [11] proposes a non-deterministic algorithm for computing
their generalized notion of uniform ϕ-reversibility, but leaves an implementation
and evaluation of their algorithm to future work. In a follow-up paper [2], the
authors provide an Answer Set Programming (ASP) based implementation, eval-
uate it using STRIPS domains obtained from a domain generator, and motivate
a comparison with a procedural implementation for future work.
In this paper, we follow their calls to action and evaluate the performance of
our prototypical breadth-ﬁrst search (BFS) and depth-ﬁrst search (DFS) imple-
mentation of their theoretical algorithm, before comparing the results to their
ASP implementation. We show that the domain generator considered so far does
not suﬃce for evaluating other action reversibility algorithms in STRIPS with-
out bias. To elaborate, we propose two additional domain generators leading to
domains with multiple reverse plans and with potential dead ends. Our results
indicate that a rigorous analysis of the bias contained in generated domains is
necessary to properly evaluate implementations of action reversibility systems.
2
Background and Related Work
We follow the idea of [7] and use the same naming conventions as [11] to deﬁne
a STRIPS planning problem as a tuple Π = (F, A, s0, G) consisting of facts F,
actions A, an initial state s0 ⊆F and a goal speciﬁcation G ⊆F. In this context,
the fact set F contains atomic statements about the world, and a state is a subset
of facts s ⊆F. Each action is a tuple a = ⟨pre(a), add(a), del(a)⟩, where pre(a) ⊆
F denotes the preconditions of action a, add(a) ⊆F and del(a) ⊆F denote the
positive add eﬀects and negative delete eﬀects, respectively. We assume that
actions are well-formed, i.e., add(a)∩del(a) = ∅and pre(a)∩add(a) = ∅. Action
a in state s is applicable iﬀpre(a) ⊆s. Applying an action a in state s, given that
a is applicable with respect to s, yields the state a[s] = (s \ del(a)) ∪add(a). An
action sequence π = ⟨a1, . . . , an⟩is applicable in state s0 iﬀthere is a sequence
of states ⟨s1, . . . , sn⟩with 0 ≤i ≤n such that ai is applicable in si−1 and
ai[si−1] = si. Applying an action sequence π in state s0 yields π[s0] = sn. |π|
denotes the length of an action sequence π.
Intuitively, an action a can be considered reversible if there exists a reverse
plan that revokes all changes introduced by a. In the literature, diﬀerent notions
of reversibility have been studied: the work of [5] introduces the notion of undoa-
bility, which deﬁnes an action a to be undoable iﬀthere exists a reverse plan for
every state s reachable from the initial state s0 in a STRIPS planning problem
Π. Their notion of universal undoability lifts this reachability restriction and
holds if there exists a reverse plan for any state s ∈2F. The work of [6] operates
on a restricted notion of reversibility and considers an action a reversible iﬀthere
exists a reverse plan that is independent of the state s in which a was applied.
The works in [3,11] provide a generalized framework that distinguishes sev-
eral notions of reversibility where an action a is

228
T. Schwartz et al.
1. ϕ-reversible iﬀa is S-reversible in the set of models S of the propositional
formulas ϕ over F. An action a is S-reversible iﬀthere exists a reverse plan
π = ⟨a1, . . . , an⟩∈An for every state s ∈S where a is applicable in a[s] such
that π[a[s]] = s;
2. reversible in Π iﬀa is RΠ-reversible with respect to a STRIPS planning
problem Π, where RΠ denotes the set of states reachable from the initial
state s0 of Π;
3. (universally) reversible iﬀa is 2F-reversible; and
4. uniformly ϕ-reversible iﬀa is ϕ-reversible using the same reverse plan π.
As noted in [11], the second notion coincides with the work of [5], while a
uniform restriction of the third notion coincides with the concept studied by [6].
This uniform restriction requires that the computed reverse plan is applicable
disregarding the state in which action a was applied. Note that such a restriction
is of practical interest because reverse plans can thereby be computed upfront.
Algorithm 1: Uniform ϕ-reversibility of an action a (adopted from [11]).
Input
: A set of actions A, an action a ∈A
Output: A formula ϕ, a reverse plan π
1 F + = (pre(a) \ del(a)) ∪add(a)
2 F −= del(a)
3 F 0 = ∅
4 π = ⟨⟩
5 while pre(a) ̸⊆F + ∨F 0 ∩F −̸= ∅do
6
non-deterministically choose a′ ∈A such that pre(a′) ∩F −= ∅
7
if a′ does not exist then return ⊥, ⟨⟩
8
F 0 = F 0 ∪(pre(a′) \ F +)
9
F + = (F + \ del(a′)) ∪add(a′)
10
F −= (F −\ add(a′)) ∪del(a′)
11
π = π · a′
12 ϕ = 
l∈F +∪F 0 l ∧
l∈F −¬l
13 return ϕ, π
For reference, Algorithm 1 depicts the non-deterministic algorithm proposed
by [11] to compute a reverse plan π for an action a and an associated formula
ϕ, such that a is at least uniformly ϕ-reversible. Note that F +/F −represent
sets of facts that are true/false within a state, while F 0 represents the necessary
preconditions. Since uniform reversibility is state independent, the algorithm
only requires a set of actions A and the action a to be reversed as input. Upon
termination, it provides a reverse plan π and a formula ϕ yielding states in which
the reverse plan is applicable as output.

Towards the Evaluation of Action Reversibility in STRIPS
229
Listing 1.1. Single path domain generator (adopted from [2]).
1
( define
(domain single−path−<i >)
2
( :requirements
: s t r i p s )
3
( :predicates
( f0 )
. . .
( f<i >))
4
5
( :action
d e l −a l l
6
:precondition
(and
( f0 )
. . .
( f<i >))
7
: e f f e c t
(and
( not
( f0 ) )
. . .
( not
( f<i >)) ) )
8
9
( :action
add−f0
10
: e f f e c t
( f0 ) )
11
12
. . .
13
14
( :action
add−f<i>
15
:precondition
( f<i−1>)
16
: e f f e c t
( f<i >)) )
3
Domains for Evaluating Action Reversibility
In contrast to traditional planning problems, the complexity of action reversibil-
ity primarily depends on the domain and not on particular instances. Hence,
the characteristics of the domain are key for a proper evaluation. This section
discusses why existing planning benchmarks, such as the IPC domains, and the
domain generator proposed by [2], are not suﬃcient for an in-depth evaluation
of action reversibility systems. We subsequently propose two concrete domain
generators that yield search trees containing multiple paths suitable as reverse
plans and potential dead ends.
3.1
Existing Benchmarks for Planning
The International Planning Competition (IPC) provides a large number of
domains and accompanying instances encoded in the Planning Domain Deﬁ-
nition Language (PDDL) [9] that enable an empirical comparison of planning
systems. Recall that Algorithm 1 computes a uniform notion of reversibility and
thereby only relies on the set of actions A. Contrary to traditional planning where
in particular PDDL instances are of interest, the evaluation of action reversibil-
ity systems solely depends on the PDDL domains. The work in [5] examined the
STRIPS domains from IPC’98 to IPC’14 in their experiments and observed that
most actions in these domains are not reversible. They have further noted that
the reverse plans for the remaining reversible actions have very small reverse
plans, i.e., often of length 1 where the eﬀects of an action a can be completely
undone in a single step by applying a so-called inverse action a [10]. For example
in the well-known Blocksworld domain, the action pickup is the inverse action
of action putdown and vice versa.

230
T. Schwartz et al.
Listing 1.2. Our multiple paths domain generator.
1
( define
(domain multiple−paths−<i >)
2
( :requirements
: s t r i p s )
3
( :predicates
( f0 )
. . .
( f<i >))
4
5
( :action
d e l −a l l
6
:precondition
(and
( f0 )
. . .
( f<i >))
7
: e f f e c t
(and
( not
( f0 ) )
. . .
( not
( f<i >)) ) )
8
9
( :action
add−f0
10
: e f f e c t
( f0 ) )
11
12
. . .
13
14
( :action
add−f<i>
15
:precondition
( f<i−1>)
16
: e f f e c t
(and ( f<i >)
( not
( f0 ) )
. . .
( not
( f<i−1>)) ) ) )
Note that the existence of small reverse plans in STRIPS planning domains is
not necessarily surprising since the complexity of a planning problem primarily
arises from the PDDL instances, i.e., the concrete initial state and goal state.
This is also the case for domains studied in the ﬁeld of real-time planning [4]
where domains typically contain a few actions only and the complexity of the
modeled system is encoded in the states. In contrast, complexity of the action
reversibility problem primarily arises from the action to be reversed in the con-
text of other available actions in the respective domain. Accordingly, the IPC
domains in particular are not well-suited for a sophisticated evaluation of action
reversibility systems, due to the domains containing very small reverse plans.
3.2
Single Path Domain
To overcome the limitations of the IPC domains for evaluating action reversibil-
ity, the authors of [2] use a handcrafted domain generator. This generator, as
shown in Listing 1.1, receives a single integer i as input and generates the PDDL
domain single-path-<i> where action del-all is to be reversed. A reverse plan
must reestablish the precondition, i.e., the predicates f0 to f<i>. These pred-
icates can be regained using the actions (add-f<i>) that each require the add
eﬀect from their preceding action f<i-1> to regain predicate f<i>. Hence, each
action add-f<i> has to be called exactly once in ascending order to reverse
the eﬀect of action del-all. With respect to the PDDL domain, this prop-
erty is primarily encoded by the eﬀect of the add-f<i> action, i.e., :effect
((f<i>)) (see line 16 in Listing 1.1). The length of the reverse plan for action
del-all grows linear with respect to input i, precisely |π| = i.
The search trees produced by this domain comprise exactly a single reverse
plan containing the actions add-f<i> in ascending order. Note that this is not a
problem for the ASP implementation [2], which follows a guess-and-check pat-
tern. Their approach ﬁrst guesses action sequences of ﬁxed length, but disregards
their applicability. Whether a sequence of actions is executable is checked in the
subsequent step. Hence, the performance of their approach, disregarding the

Towards the Evaluation of Action Reversibility in STRIPS
231
impact of the ASP encoding and ASP solver, is expected to primarily depend
on the length of the reverse plan and not on the search tree characteristics.
Thereby, a single path domain is only suitable for the evaluation of
approaches that follow a similar pattern. However, this is not the case for search
algorithms, which consider the applicability of actions during the construction
and traversal of the search tree. For those algorithms, the length of the reverse
plan and the characteristics of the search tree are expected to have an impact on
their performance. Accordingly, the single path domain generator is not suﬃcient
to evaluate arbitrary action reversibility systems.
3.3
Multiple Path Domain
We propose to extend the aforementioned single path domain such that multiple
paths, each representing a valid reverse plan, exist. This is achieved by mak-
ing permutations of the prior reverse plan also feasible. To do so, our multiple
paths domain generator, as shown in Listing 1.2, adjusts the eﬀects of actions
add-f<i>. Instead of only adding predicate f<i>, these actions now delete all
prior predicates f0 to f<i-1>. As result, these have to be regained by subse-
quently executing all previous actions. This change modiﬁes the characteristics
of the search tree and also lifts the length of the reverse plan from linear to
quadratic with respect to the input i, precisely |π| = i
k=1 k.
Obviously, computing a reverse plan of quadratic length with respect to i
requires signiﬁcantly more time than one of linear length. However, since all
applicable paths resemble valid reverse plans, the performance of a DFS approach
should not diﬀer much from the single path domain given that both reverse
plans have the same length. By contrast, the performance of a BFS approach
is expected to be much worse due to the increased size of the search tree. The
guess-and-check pattern employed by the aforementioned ASP implementation
should not be aﬀected signiﬁcantly by the diﬀerence in the search tree, but rather
by the increased length of the reverse plan.
3.4
Dead End Domains
Both domains discussed so far have the property that any applicable path in
the search tree always leads to a valid reverse plan, which strongly favors a
DFS approach. In real world planning problems, executing an arbitrary action
does not necessarily provide progress towards the goal. In hindsight, an action
might have been unnecessary for reaching the desired goal state or even led to
a dead end, i.e., a state where no further actions are applicable. From a search
perspective, this requires the need to backtrack to a previous conﬁguration and
follow a diﬀerent path instead. Consequently, a DFS approach can in this context
be drastically slower, because it follows potentially long paths that eventually
reach a dead end. In contrast, dead end paths longer than the reverse plan should
hardly aﬀect a BFS disregarding the larger search tree.

232
T. Schwartz et al.
Listing 1.3. Our dead ends domain generator.
1
( define
(domain dead−ends−<i >)
2
( :requirements
: s t r i p s )
3
( :predicates
( f0 )
. . .
( f<i >)
( token ) )
4
5
( :action
d e l −a l l
6
:precondition
(and
( f0 )
. . .
( f<i >))
7
: e f f e c t
(and
( not
( f0 ) )
. . .
( not
( f<i >))
( token ) ) )
8
9
( :action
consume
10
:precondition
( token )
11
: e f f e c t
( not
( token ) ) )
12
13
( :action
add−f0
14
:precondition
( token )
15
: e f f e c t
( f0 ) )
16
17
. . .
18
19
( :action
add−f<i>
20
:precondition
( f<i−1>)
( token )
21
: e f f e c t
(and ( f<i >)
( not
( f0 ) )
. . .
( not
( f<i−1>)) ) ) )
PDDL domains containing dead ends can be constructed similarly to the
single and multiple path domain generators. Recall that the starting state
upon computing reverse plans for an action a is the state a[s], i.e., the state
after applying a. This state is characterized by (pre(a) \ del(a)) ∪add(a),
which resembles the initial value of F + in Algorithm 1. Further note that
this starting state is empty for the single and multiple path domain gener-
ator, because pre(del-all) = del(del-all) ∧add(del-all) = ∅holds. While the
derived search trees for these domains do not contain dead ends, this obser-
vation can be generalized even further: If there exists a reverse plan for action
a and pre(a) = del(a) ∧add(a) = ∅then the search tree for computing action
reversibility does not contain a dead end. The proof follows from the necessary
existence of actions that do not contain preconditions and can be used to pro-
duce predicates, e.g., the actions add-f<i>. A reverse plan would not be possible
in the absence of such actions.
Dead end domains can be constructed with the help of a token predicate,
which is required but cannot be produced by other actions, and adding it to the
add eﬀects of the to be reversed action. Our proposed dead ends domain genera-
tor is shown in Listing 1.3 and accordingly adds the token predicate to the add
eﬀect of action del-all and adds it to the precondition of all other actions. The
new action consume irreversibly removes this token predicate upon execution,
thereby resulting in a dead end, because no action is applicable anymore. Such
a “malicious” action can potentially always be executed during the search.
4
Experiments
In what follows, we brieﬂy describe our implementation of Algorithm 1, and
primarily discuss the ﬁndings of our conducted performance evaluation for the

Towards the Evaluation of Action Reversibility in STRIPS
233
0
100
200
300
400
500
0
5
10
15
Single path domain argument i
Runtime in seconds
DFS
BFS
ASP
0
10
20
30
0
200
400
Multiple paths domain argument i
Runtime in seconds
DFS
BFS
ASP
0
10
20
30
0
200
400
Dead ends domain argument i
Runtime in seconds
DFS
BFS
ASP
0
100
200
300
400
500
0
500
1,000
Single path domain argument i
Memory usage in megabytes
DFS
BFS
ASP
0
10
20
30
0
100
200
Multiple paths domain argument i
Memory usage in megabytes
DFS
BFS
ASP
0
10
20
30
0
100
200
Dead ends domain argument i
Memory usage in megabytes
DFS
BFS
ASP
Fig. 1. Runtime (top) and memory usage (bottom) of our procedural BFS and DFS
and the ASP implementation [2] for the single path (left), multiple paths (middle), and
dead ends (right) domains with a timeout of 10 min.
DFS, BFS, and ASP implementation with respect to the aforementioned single
path, multiple path, and dead ends domain generators. Our implementation,
the domain generators, and the results of our experiments are available online
at https://github.com/TobiasSchwartz/strips-reversibility-benchmarks.
4.1
Implementation
Our implementation of Algorithm 1 is written in Python and uses PDDL as input
format. Non-deterministic choice of the next action a′ in line 6, which requires
a suitable selection strategy in a deterministic implementation, is implemented
as follows: We successively construct a search tree where paths correspond to
action sequences and explore it either in a BFS or DFS manner. Note that we do
not employ heuristics for action selection on purpose to exclude the possibility of
overﬁtting with respect to the small number of domains used in the evaluation.
We consider two nodes in our search tree equivalent if they share the same values
for F +, F −, and F 0, but ignore the value of π. As an optimization, we use this
notion to introduce a cycle detection and do not consider a candidate action if
its application yields an equivalent node, which has already or is to be traversed.
4.2
Results and Discussion
We conducted our experiments using a desktop PC with a stock AMD RyzenTM 5
3600 and 16 GB of RAM. Figure 1 depicts the runtime (top) and memory
usage (bottom) for the three implementations, i.e., our DFS and BFS imple-
mentation, and the simple encoding for the ASP implementation of [2]. We use

234
T. Schwartz et al.
domains generated from the aforementioned three domain generators, i.e., single
path (left), multiple path (middle), and dead ends (right). In accordance with [2],
we use the single path domain generator starting from input value i = 10 to
i = 500 with step size 10. For our multiple paths and dead ends domain gener-
ators, we start at input value i = 1 and reduce the step size to 1. We use the
conﬁguration from [2] to evaluate their ASP implementation.
Observe that DFS and BFS perform similarly and outperform ASP for the
single path domain in most cases. As outlined above, only a single action is
applicable in each iteration when computing uniform ϕ-reversibility. Hence, the
behavior of BFS and DFS are identical for this problem domain. Recall that
there exists only a single reverse plan for the single path domain such that
computing uniform ϕ-reversibility is identical to the ASP approach computing
universal uniform reversibility. However, the guess-and-check pattern employed
by the ASP implementation only validates action sequences as a whole, leading
to a weaker runtime performance. By contrast, the multiple paths domain yields
a large number of possible reverse plans. Hence, as expected, the DFS approach
performs best for these domains, because every admissible action sequence even-
tually yields an applicable reverse plan. However, this poses a problem for the
BFS approach due to the large number of potential states, which is also reﬂected
by the increased memory usage. Albeit the naive use of a guess-and-check pat-
tern, the ASP approach outperforms the BFS approach. We assume that this
observation can be ascribed to the fact that our BFS implementation is not opti-
mized with respect to performance and memory usage in contrast to the ASP
solver underlying the ASP approach. Finally, in the dead ends domain we again
ﬁnd a large number of potential states. But, in this domain, not every action
sequence necessarily has a valid reverse plan. Accordingly, both DFS and BFS
require more time and space than the ASP approach.
In summary, our results show that minor changes to the domain generation
and thus the resulting domains can have a major impact on the performance of
diﬀerent action reversibility algorithms. In particular, evaluating search based
algorithms, e.g., the aforementioned DFS and BFS, using existing domain gen-
erators can easily lead to a biased result.
5
Conclusion
In this paper, we implemented the notion of uniform ϕ-reversibility based on
the non-deterministic algorithm proposed by [11]. With this we follow their
call to provide an actual implementation for said theoretical algorithm. Our
implementation considers a breadth-ﬁrst search and depth-ﬁrst search variant.
Following the suggestion of [2], we evaluate our procedural implementation in
close comparison to their Answer Set Programming implementation. Evaluating
action reversibility, however, turns out to be a diﬃcult challenge on its own. In
contrast to traditional planning problems, the complexity of action reversibil-
ity primarily depends on the planning domain and not on particular planning
instances. However, the majority of existing planning benchmark problems, e.g.,

Towards the Evaluation of Action Reversibility in STRIPS
235
from the IPC, are complex at instance level, whereas actions in the domain are
often, if reversible at all, trivially reversible using inverse actions. The domain
generator introduced by [2] generates STRIPS domains of increasing complex-
ity. We show that these contain a strong bias favoring other search strategies
over their ASP approach. We have designed two domain generators that produce
domains containing multiple reverse plans and containing dead ends. All gen-
erators, albeit similar in construction, produce domains that drastically favor
diﬀerent approaches. We thereby highlight the challenge of constructing and
using domain generators with respect to evaluating action reversibility systems.
Regarding future work, we consider the design of further domain generators
worth exploring in order to broaden the variety of generable domains, i.e., diﬀer-
ent problem classes, and thereby improve the validity of obtained performance
results. Understanding the characteristics of domain generators allow to iden-
tify the most promising approach for a given problem instance paving the way
for automated algorithm selection [12]. Finally, a solid benchmark of diﬀerent
domain generators enables to assess whether future algorithm improvements do
result in a better performance in general or only for particular problem classes.
Acknowledgements. We would like to thank the anonymous reviewers for their
insightful feedback. This work has been partially supported by BMBF funding for the
project Dependable Intelligent Software Lab. Financial support is gratefully acknowl-
edged.
References
1. Bylander, T.: The computational complexity of propositional STRIPS planning.
Artif. Intell. 69(1–2), 165–204 (1994)
2. Chrpa, L., Faber, W., Fiser, D., Morak, M.: Determining action reversibility in
STRIPS using answer set programming. In: Workshop Proceedings Co-located
with ICLP 2020. CEUR Workshop Proceedings, vol. 2678. CEUR-WS.org (2020).
http://ceur-ws.org/Vol-2678/paper2.pdf
3. Chrpa, L., Faber, W., Morak, M.: Universal and uniform action reversibility. In:
KR 2021, pp. 651–654 (2021). https://doi.org/10.24963/kr.2021/63
4. Cserna, B., Doyle, W.J., Ramsdell, J.S., Ruml, W.: Avoiding dead ends in real-
time heuristic search. In: AAAI 2018, pp. 1306–1313. AAAI Press (2018). https://
www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17405
5. Daum, J., Torralba, ´A., Hoﬀmann, J., Haslum, P., Weber, I.: Practical undoabil-
ity checking via contingent planning. In: ICAPS 2016, pp. 106–114. AAAI Press
(2016). http://www.aaai.org/ocs/index.php/ICAPS/ICAPS16/paper/view/13091
6. Eiter, T., Erdem, E., Faber, W.: Undoing the eﬀects of action sequences. J. Appl.
Log. 6(3), 380–415 (2008)
7. Ghallab, M., Nau, D.S., Traverso, P.: Automated Planning - Theory and Practice.
Elsevier, San Francisco (2004)
8. Ghallab, M., Nau, D.S., Traverso, P.: Automated Planning and Acting. Cambridge
University Press, Cambridge (2016)
9. Haslum, P., Lipovetzky, N., Magazzeni, D., Muise, C.: An Introduction to the
Planning Domain Deﬁnition Language. Synthesis Lectures on Artiﬁcial Intelligence
and Machine Learning. Morgan & Claypool Publishers (2019)

236
T. Schwartz et al.
10. Koehler, J., Hoﬀmann, J.: On reasonable and forced goal orderings and their use
in an agenda-driven planning algorithm. J. Artif. Intell. Res. 12, 338–386 (2000)
11. Morak, M., Chrpa, L., Faber, W., Fiser, D.: On the reversibility of actions in
planning. In: KR 2020, pp. 652–661 (2020)
12. Rice, J.R.: The algorithm selection problem. Adv. Comput. 15, 65–118 (1976).
https://doi.org/10.1016/S0065-2458(08)60520-3
13. Weber, I., Wada, H., Fekete, A.D., Liu, A., Bass, L.: Automatic undo for cloud
management via AI planning. In: HotDep 2012. USENIX Association (2012)
14. Williams, B.C., Nayak, P.P.: A reactive planner for a model-based executive.
In: IJCAI 1997, pp. 1178–1185. Morgan Kaufmann (1997). http://ijcai.org/
Proceedings/97-2/Papers/056.pdf

Author Index
Al-atar, Munqath
147
Augsten, Nikolaus
93
Beierle, Christoph
206
Berens, Maximilian
1
Biskup, Joachim
1
Boockmann, Jan H.
226
Croitoru, Madalina
55
Cruz-Filipe, Luís
111
Feuillade, Guillaume
75
Gaspar, Graça
111
Geerts, Floris
20
Gyssens, Marc
187
Haldimann, Jonas
206
Hellings, Jelle
168, 187
Herzig, Andreas
75
Hirvonen, Minna
130
Irofti, Dina
55
Martin, Leon
226
Nitta, Kiyoshi
93
Nunes, Isabel
111
Rantsoudis, Christos
75
Sadoghi, Mohammad
168
Sali, Attila
147
Savnik, Iztok
93
Schwartz, Tobias
226
Sfar, Aziz
55
Skrekovski, Riste
93
Steegmans, Jasper
20
Van den Bussche, Jan
20
Van Gucht, Dirk
187
Wakaki, Toshiko
35
Wu, Yuqing
187

