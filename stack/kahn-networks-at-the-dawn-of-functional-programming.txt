5
Kahn networks at the dawn of
functional programming
David B. MacQueen
University of Chicago
5.1 Introduction
The
evolution
of
programming
languages
involves
isolating
and
describing abstractions that allow us to solve problems more elegantly,
eﬃciently, and reliably, and then providing appropriate linguistic
support for these abstractions. Ideally, a new abstraction can be
described precisely with a mathematical semantics, and the semantics
leads to logical techniques for reasoning about programs that use the
abstraction. Gilles Kahn’s early work on stream processing networks is
a beautiful example of this process at work.
Gilles began thinking about parallel graph programs at Stanford, and
he developed his ideas in a series of papers starting in 1971: [44], [45],
and [46]. Gilles’ original motivation was to provide a formal model for
reasoning about aspects of operating systems programming, based on
early data ﬂow models of computation. But the model he developed
turned out to be of much more general interest, both in terms of program
architecture and in terms of semantics. During his Edinburgh visit in
1975–76, Gilles and I collaborated on a prototype implementation of the
model that allowed further development and experimentation, reported
in [47]. By 1976 it was clear that his model, while inspired by early data
ﬂow research, was also closely connected to several other developments,
including coroutines, Landin’s notion of streams, and the then emerging
lazy functional languages.
While staying in Edinburgh, Gilles was also working with Gordon
Plotkin on a general theory of “concrete” domains that could make
a precise distinction between functions and data [48, 43]. This theory
provided a semantic explanation of streams and other incrementally
computed, potentially inﬁnite data structures. This work not only
From Semantics to Computer Science Essays in Honour of Gilles Kahn,
eds Yves
Bertot, G´erard Huet, Jean-Jacques L´evy and Gordon Plotkin. Published by Cambridge
University Press.
c
⃝Cambridge University Press 2009.
95
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

96
D. B. MacQueen
strengthened the theoretical foundations of stream processing networks,
but also contributed to deeper understanding of fundamental concepts
such as sequentiality.
Gilles’ key insight was that the behavior of a program organized as
a graph of communicating sequential processes could be adequately
represented by the sequences of data passing over the communication
channels (the channel histories), and that these histories could be
modeled as members of a complete lattice (or complete partial order)
ordered by the preﬁx relation, following Scott’s pioneering work on
domain theory [72, 73]. Because of their sequential nature, communi-
cating processes expressed continuous functions mapping histories of
their input channels to histories of their output channels. The semantics
of a parallel graph program could then be expressed as the least ﬁxpoint
of a set of recursive equations deﬁning the channel histories in terms of
the continuous functions representing processes. The uniqueness of
the least ﬁxpoint guaranteed determinacy of the program viewed as a
function on histories.
The translation of a parallel graph program into a set of continuous
stream functions and recursive stream equations was originally intended
to provide a semantics for an imperative language of communi-
cating processes. But this equational form of expression could also be
considered as a language in its own right, and if we assume a coroutine-
like, demand-driven execution strategy for this language it could be
considered the earliest description of a pure, lazy functional language
capable of computing with inﬁnite data structures. The naturalness of
this formulation is conﬁrmed by the fact that Gilles’ stream processing
networks are easily expressed in modern lazy languages like Haskell [66].
The elegant and powerful abstraction of functional stream processing
that Gilles developed has become a common idiom of programming in
lazy and strict functional languages and has inspired many developments
in areas such as reactive languages (e.g. LUSTRE [22]), digital signal
processing, and semantics of concurrency.
In this paper, we will start by providing some historical context,
discussing earlier data ﬂow models, coroutines, and Landin’s streams. We
then summarize the development of Kahn networks through the series of
papers Gilles published in the early to mid 1970s [14, 44, 45, 47]. We will
brieﬂy review the related Kahn–Plotkin development of concrete data
types [43]. Next we will consider the roughly contemporary development
of lazy functional languages, and ﬁnally we will discuss a few samples
of the vast quantity of later work that built on or exploited these ideas.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
97
Our goal is to illuminate this particular aspect of Gilles Kahn’s scientiﬁc
contributions by exploring its development in some detail and relating
it to both earlier and contemporary developments.
5.2 Some precursors of Kahn networks
Three lines of conceptual development arising in the 1960s played a part
in the origins of Kahn networks. One was the modeling of concurrent
programs as data ﬂow graphs, the second was Landin’s introduction of
the notion of functional streams, and the third was the investigation of
coroutines. In this section we brieﬂy review these precursors.
5.2.1 Graph models and data ﬂow
In the late 1960s researchers began investigating new graph-based
models of computation which later came to be known as data ﬂow
models.1 The basic idea underlying programs as graphs is that edges
represent channels carrying data values between nodes that represent
operations or computational processes. These models typically involve
concurrency, since it is usually assumed that multiple nodes can be
active simultaneously. A large number of variations on graph models
were proposed over the following years, including Kahn networks. A
1973 survey by Baer [11] covers several of the early graph models.
The earliest data ﬂow language was probably the “block diagram”
language, BLODI, developed at Bell Laboratories around 1960 [50].
BLODI enabled engineers to solve problems in digital acoustic signal
processing
by
connecting
elements
from
a
library
of
predeﬁned
processing programs. A BLODI program speciﬁed a circuit, which was
a graph consisting of blocks, or function nodes, connected by channels
called signals. A block could have multiple input signals but only a
single output signal, which could be shared as input by several other
blocks. Some blocks performed pure arithmetic operations like addition
to compute the next output in terms of the current inputs, while other
blocks had internal state (e.g. delay blocks and accumulator blocks).
Signals, which represent communication channels, carried a single value
at a time, and the operation of the circuit was synchronous: the output
of a block at one clock tick becomes the input for its consumers on
the next clock tick. BLODI was the ﬁrst of a line of block-diagram
languages designed for digital signal processing [52].
1 The term “data ﬂow” may have originated in the title of Duane Adam’s thesis [3].
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

98
D. B. MacQueen
The ﬁrst theoretical model for a form of data ﬂow computation was
described by Karp and Miller in 1966 [49]. In this model, programs are
ﬁxed graphs whose nodes represent atomic operations, and whose edges
transmit data between nodes, but in this case the edges can store a queue
of values in transit. A node is enabled if it has suﬃciently many input
values on each input edge (an example of data-driven control ﬂow), and
an enabled node may execute, consuming a predetermined number of
values from each input and producing a predetermined number of values
on each output. The model is not very ﬂexible or realistic, since the
graph is ﬁxed and the rates of consumption or production of each node
on each connected edge are ﬁxed independent of the values consumed.
An execution of a graph program is modeled as a sequence of sets of
simultaneously ﬁring nodes, and the main theoretical result is that the
behavior of a program is determinate when viewed in terms of the history
of values on edges.
The Karp–Miller model introduced several important ideas: (1)
modeling channels as unbounded queues of values, (2) characterizing
a computation in terms of complete channel histories, and (3) data-
driven execution. Furthermore, it provided an example of a parallel,
nondeterministic operational model that produces deterministic global
behavior.
The next major development in data ﬂow models was the 1967 MIT
thesis of Jorge Rodriguez [71], a student of Jack Dennis. This work was
the beginning of a long line of data ﬂow models intended to explore new
ways to organize computations to achieve high levels of concurrency. The
Rodriguez model was intended to be more realistic than the Karp–Miller
model. The model is quite complex, with two types of edges and seven
types of nodes. The two varieties of edges are data edges that carry a
single unstructured value and have one of four statuses, or control edges,
which have only a status. One type of node performs basic operations
on data, while six other types of node perform various control functions.
The model is basically data-driven, but the edge status values also play
a part in enabling execution of a node. The program graphs are ﬁxed,
as in the Karp–Miller model, and edges can carry at most one value at
a time. The thesis deﬁned an operational semantics for the model and
proved that it was determinate.
This data ﬂow model was followed by a similar one presented by Duane
Adams in his 1968 Stanford PhD thesis [3]. In this model, edges have
only two status values (locked or unlocked), and as in Karp–Miller they
can carry an unbounded queue of values of a speciﬁed type. Execution
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
99
is data-driven, conditioned on input edge status values. Only one value
is consumed from each unlocked input edge at each node execution, but
multiple values (or none) can be output on each output edge. Nodes come
in two ﬂavors: r-nodes that perform unconditional operations on input
values to compute output values, and s-nodes that implement status-
dependent conditional operations, yielding both outputs and modiﬁed
input statuses. An r-node may be a procedure node, whose function is
itself deﬁned in terms of a program graph. Procedure node graphs may
be recursive, and may fail to terminate.
The operational model for Adams’ graph programs is quite complex
compared to Karp and Miller’s and Rodriguez’s models, because
executions of r-nodes, though they must terminate before producing
output, have duration and are allowed to overlap in a pipelined fashion.
Thus Adams’s model supports both spacial concurrency where multiple
nodes are ﬁring at the same time, and overlapped concurrent executions
of a single node. Despite this complication, the operational semantics is
shown to be determinate like the earlier models.
Adams presented several examples of data ﬂow graph programs,
including the factorial function (in iterative and recursive versions),
Gaussian elimination, a merge sort, and a program for simplifying
arithmetic terms.
Meanwhile, Jack Dennis, following up on Rodriguez’s work, published
another model in 1968 [29] that included a notion of graph procedures
similar to that of Adams, except that a more general procedure apply
node takes the name of a procedure to be applied as one of its inputs,
making the model “higher-order” in a sense. Because data values could
be modiﬁed, a read/write capability and signaling system had to be
added to ensure mutual exclusion and determinacy of the model. The
paper also sketches a machine architecture designed to directly execute
the graph model. Like the Rodriguez model, this model has at most one
value per edge and is supply-driven.
In a series of papers during the early to mid-1970s [31, 30, 32],
Dennis continued to develop and reﬁne his determinate, data-driven
data ﬂow models and corresponding computer architectures that could
execute them [34], with the goal of achieving greater concurrency while
maintaining a relatively simple, deterministic semantics. These models
all involved single value edges and a distinction between operation
nodes and control nodes. These ideas led to experimental data ﬂow
computer designs at MIT (Monsoon), the University of Manchester [37],
and Imperial College (the Alice machine [27]).
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

100
D. B. MacQueen
5.2.2 Landin’s streams
In Landin’s paper “A correspondence between ALGOL 60 and Church’s
lambda-notation: Part I” [53], he introduces the concept of a stream as
an open-ended and potentially inﬁnite sequence of values, and uses them
to translate the Algol 60 for statement.2
Landin’s representation of a stream is simply a nullary function that,
when called, produces an element and another stream function. Thus to
“cons” an element x onto a stream s, we form the lambda abstraction
λ().(x, s), for which Landin introduced the special syntax “x :∗s”. This
is a fully lazy cons operator, since it suspends the evaluation of both
the head and tail of the stream. The basic list operators are then easily
deﬁned; for instance:
nulls(S)
=
null(S())
hds(S)
=
fst(S())
Landin’s streams are a precursor to lists in lazy functional languages,
but they diﬀer from lazy lists in the absence of memoization in a data
structure, and stream cons is strict in its ﬁrst argument, so initial
segments of streams must consist of fully deﬁned elements.
Programming with these functional streams is straightforward,
as shown by Burge in [17, 16]. For instance, it is easy to deﬁne
functionals like map, fold, and ﬁlter over streams. Since streams
are functions, it is also possible to deﬁne a recursive stream, as in
“let rec s = λ().(1, s) in s.” Because the values of a stream are not
consolidated into a data structure and are therefore ephemeral, using
Landin’s streams in situations where the streams are shared usually
requires recomputation of their elements. Burge also notes that one can
construct a cascade of stream processing functions that work like the
coroutine pipelines described by Conway [25].
In the early 1970s, Landin’s streams inspired the feature called
dynamic lists in POP-2 [19]. A dynamic list is created by applying a
primitive function fntolist to a generator function, which produces a
new value each time it is called (normally depending on modiﬁed state).
Dynamic lists were typically used to model input sources, but it was
possible to use them for rudimentary stream processing applications.
Landin’s streams were also one inspiration of the notion of process that
Milner began to explore in [64].
2 Burge comments that he learned about the stream concept from Landin as early
as 1962.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
101
5.2.3 Coroutines
The concept of a coroutine is usually attributed to Melvin Conway,
who coined the term in 1958 and originally used coroutines in the
implementation of an assembler. He described the notion in his paper
[25], which is concerned with the design of a compact, single-pass Cobol
compiler.3 In Conway’s scheme, two communicating coroutines interact
by transferring control back and forth at read and write operations
(one is the producer, performing writes, and one is the consumer,
performing reads). Each read/write interaction passes a single value
directly between the coroutines. It is not indicated which coroutine is
the driver, so the interaction could be either data-driven or demand
driven. Such coroutines could be decoupled into separate passes, where
the producer writes all its output values to a tape, and then the
consumer runs in a separate pass and reads the values from the tape.
This multi-pass variant is a form of data-driven execution, with buﬀered
communication. The reason this decoupling is possible is that transfer
of control is performed indirectly and implicitly by the input/output
operations on a connecting channel.
Conway’s version of coroutines allows them to have multiple outputs
and multiple inputs, but he doesn’t admit cycles. He goes on to describe
the architecture of his Cobol compiler as a graph of communicating
coroutines passing values along the links.
In 1968, Doug McIlroy gave a talk on coroutines at the Cambridge
University Computer Laboratory, the contents of which later appeared
as an unpublished note [59]. McIlroy cites Conway and mentions stream
processes as one of the main applications for coroutines. McIlroy
sketches a coroutine facility as an extension to a PL/I-like language,
generalizing the mechanisms associated with PL/I procedures. He uses
the term connection for ports through which coroutines communicate
and by which control is transferred via either a resume operation or a
function call notation using the port name as the function. The passing
of parameters and results via connected ports is symmetrical, meaning
connected ports can pass values (and control) both ways.
Connections are externally accessible attributes of coroutine instances,
and a connect operation can create new linkages between connections.
This makes it possible for a coroutine program to reconﬁgure itself,
3 In a footnote, Conway notes that the coroutine concept was independently invented
by Joel Erdwinn, then at the Computer Sciences Corporation. An article on
coroutines in [67] attributes the independent invention to Erdwinn and Jack
Merner.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

102
D. B. MacQueen
anticipating the process reconﬁgurations of [47] that we will discuss
in Section 4. McIlroy used a coroutine implementation of the Sieve
of Eratosthenes to demonstrate how one can exploit the ability
to dynamically generate new coroutine instances and reconﬁgure
connections. This was the direct inspiration of the sieve example in [47].4
He also mentions an idea suggested by David Park called “functional
assignment”, which corresponds to function closures, and he notes that
some, but not all, coroutine programs can be expressed using function
closures.5
5.3 The origin of Kahn networks
5.3.1 Early data ﬂow research
Three years before the milestone publication of [46], while Gilles was a
student at Stanford, he was already working on the idea of processes
communicating via queue-like channels, and he published his ﬁrst paper,
“An approach to systems correctness,” [44], in the 1971 Symposium
on Operating Systems Principles (SOSP 71). This paper describes a
graph model of computation as a formalism for specifying the functional
behavior of systems programs (e.g. components of an operating system).
It cites Duane Adams’ thesis [3] and was probably also inﬂuenced by
earlier data ﬂow work, but it introduces a much simpler model.
A program is organized as a graph. Vertexes in the graph represent
operations or processes, chosen from a predeﬁned library, and they may
execute independently and in parallel. Edges of the graph represent
communication lines that convey values of various types between the
processes, and processes communicate exclusively via these channels. A
communication line behaves as a FIFO queue of values, with a capacity
that may be either unbounded or bounded.
The vertexes of the graph are of two kinds. Multiplexer vertices take
the values arriving on either of two input lines and send them as they
arrive on a single output line, resulting in a fair but nondeterministic
merge of the input sequences. The second kind of vertex is a computa-
tional node, which can have multiple inputs and multiple outputs. Some
of these nodes perform discrete or atomic operations to transform inputs
4 A functional version of the sieve also appears in [40], attributed there to
P. Quarendon.
5 A few years later, McIlroy’s familiarity with stream processing coroutines and their
pipeline composition inﬂuenced the design of the Unix shell [70] and led to Ken
Thompson’s adding the pipe combinator to an early version of the shell.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
103
into outputs, while others represent a continuing sequential process (e.g.
a number-generating process that emits an inﬁnite increasing sequence of
integers on its output line). Some sorts of computational nodes have local
memory. The internal details of how computational nodes are deﬁned
are not discussed, but a basic set of sample nodes are described and used
in examples.
Some node operations perform actions conditioned on the input values
available (they perform lookahead on their input lines), and they can
selectively input values from a subset of their input lines. An example
would be the ordered merge operation, called M, that waits for integer
values on its two input lines, compares those two values, then consumes
only the larger value and sends it on its output line.
There is no explicit discussion of the execution model for graph
programs, other than the statement that “nodes will represent processes
that may operate independently” implying parallel execution. There is
also a fairness assumption that any node that has inputs on all of its
incoming edges will eventually become “active”, i.e. will execute. This
suggests a data-driven mode of execution similar to previous data ﬂow
models.
Gilles
discusses
the
functional
description
of
graph
programs,
including how to characterize the correctness of a program. A functional
description involves an input condition that restricts the sequences
of values supplied on the input lines (those lines that do not have a
source within the program), and an output condition in the form of a
relation between input sequences and output sequences (the sequences
appearing on output lines, which are lines having no target within the
program). The paper presents no formal semantics for graph programs,
nor does it develop a theory for formally proving programs to be
correct with respect to a functional description, but several example
proofs are sketched using informal reasoning about the sequences
representing communication histories. These example proofs involve a
simple pipeline computation operating on two inputs and producing
two outputs, a binary merge tree, a simple ﬁltering program involving
a cycle, and a couple variations on mutual exclusion mechanisms for
managing resources. This approach to the description and analysis
of program behavior through communication histories goes beyond
anything appearing in the earlier data ﬂow work of Karp and Miller,
Rodriguez, and Adams [49, 71, 3], and anticipates the formal semantic
model and reasoning techniques Gilles described in [45] and [46].
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

104
D. B. MacQueen
As mentioned, the graph model of [44] was probably most directly
inﬂuenced by that of Adams. It is considerably simpler than Adams’
model in that it eliminates the idea of channel status and procedure
nodes. It also diﬀers from Adams’ model in that it is nondeterministic,
because of the multiplex nodes, and it allows for channels of bounded
capacity.
The data ﬂow model of [44] is a transitional step between the data
ﬂow models of Karp and Miller, Rodriguez, and Adams and the more
streamlined, elegant model described in [45] and [46]. In the later papers,
Gilles returns to a deterministic model with sequential processing nodes,
and develops the informal techniques for reasoning about communication
histories into a functional semantics for graph programs.
5.3.2 Process networks
In an early IRIA report entitled “A Preliminary Theory for Parallel
Programs” [45], followed by the 1974 IFIP Congress paper “The
semantics of a simple language for parallel programming” [46], Gilles
presented a new model for parallel programs consisting of communi-
cating sequential processes. As in earlier models, programs consist
of graphs where edges are communication channels and vertices are
computational processes performing input and output on the connecting
channels. Like the earlier data ﬂow models of Karp–Miller and Adams,
but unlike the model in [44] with its multiplexer nodes, this model is
determinate when program behaviors are abstracted in terms of channel
histories. Both [45] and [46] follow [44] in motivating the design of
the model by its potential to clarify aspects of programming operating
systems ([45] cites [75] in support of this objective).
This model is distinguished from earlier data ﬂow models by the fact
that the computations performed at nodes are modeled as continuously
running sequential programs, or processes, performing data processing
interspersed with occasional input and output operations. In most
data ﬂow models, nodes are associated with discrete operations that
are initiated when inputs are available, perform some operation on
the available input values, and terminate after producing some output
values. In Gilles’ model, availability of input does not activate a node,
but nonavailability of input values on a channel may cause an active
process to block (usually temporarily) waiting on an input operation.
Even assuming parallel execution, it is fundamentally a demand-driven
model, with the node processes driving the computation, rather than a
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
105
data-driven model of processing where data ﬂowing through the graph
activates the operations at the nodes.
The key innovation in these two papers, however, is the development
of a denotational semantics for these graph programs that abstracts
away from the details of processing within the nodes and the timing
of input/output interactions. The behavior of programs is represented
abstractly through the histories of the channels, with the processes
implementing continuous functions over these histories. The meaning of
the program is then deﬁned as a ﬁxpoint of a set of stream equations.
Beyond explaining the sequential process language, this formalized
semantics actually provides an alternate purely functional language for
implementing stream processing programs.
5.3.3 Basic assumptions
The key assumption underlying the functional semantics is that the
communicating processes are executing sequential programs. Reference
[45] is not speciﬁc about how these processes are expressed, suggesting as
possibilities Turing machines communicating via one-way tapes, or, more
concretely, assembly language programs. Reference [46] was more speciﬁc
about the nature of the process language, sketching a simple ALGOL-
like imperative language enriched with wait and send operations for
communications via channels (see Figure 5.1). Regardless of the exact
nature of the language used to express processes, it was assumed to be
“sequential” in an intuitive sense, meaning that the primitive operations
and control structures of the language dictated a single, completely
determined sequence of computational steps, with no ambiguity about
the order in which events occur. This intuition about sequentiality of
processes implies certain principles of operation.
(i) Processes communicate only via input/output operations on
channels (i.e. no shared variables or other implicit means of
communication).
(ii) Values sent on communication channels will be received within a
ﬁnite though indeﬁnite amount of time (i.e. values communicated
on a channel will not be lost or delayed forever in transit).
(iii) At most one process will output on a given channel, and at most one
process will input from a given channel (i.e. two processes cannot
share the same end of a channel).
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

106
D. B. MacQueen
(iv) Channels act like FIFO queues, so values will be received in the
same order that they were sent.
(v) The only operations on channels are outputting a value, which
cannot block, and inputting a value, which, if the channel is empty,
will block until a value appears on the channel.
The last condition implies that there is no polling operation to query
whether input is available on a channel without blocking, nor is there
a select operation that can wait for input on any of several channels.
Such operations would introduce behavior that depends on timing of
events, introducing nondeterminacy. The third restriction is based on the
assumption that wait and send are destructive operations on channels.
Not being able to share channels for input is not much of a limitation,
since it is easy to deﬁne a duplicator process that duplicates each input
value on two output channels.
With respect to processes, we have the following assumptions.
• Processes have ﬁnite but unbounded internal memory.
• Processes run independently (except as constrained by communi-
cation), and multiple processes can run simultaneously, in parallel.
Since the output operation is nonblocking and processes can run in
parallel it follows that channels have unbounded capacity. It is also
assumed that channels are typed, i.e. each channel carries only values
from a speciﬁed value domain D. In [45] these values could be either basic
or structured, while in [46] it is assumed that only values of simple types
are communicated, but the exact nature of the values communicated is
not important as long as the primitive operations on those values are
determinate and the values are not mutable.
Two other crucial intuitive properties of this model are: (1) as a
process consumes more input values, it can only produce more output
values, and additional inputs cannot aﬀect earlier outputs, and (2) when
a process produces an output, it can have performed only ﬁnitely many
input operations, and hence any output value can only depend on ﬁnitely
many input values. Viewing processes as functions over channel histories
(ordered by the preﬁx relation), these two properties imply that they
are monotonic and continuous.
5.3.4 An imperative stream processing language
Reference [46] begins with a sketch of a simple language for parallel
programming based on processes communicating via channels. The
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
107
language is anonymous, but we might retroactively name it ISPL,
for “imperative stream processing language”. The details of the base
language are taken for granted, as the language is mainly meant as a
concrete illustration of how sequential processes might be programmed,
and the main focus was the study of the semantics of the parallel
programs. The salient features of ISPL are listed here.
(i) Declarations introducing and naming typed channels, which are
used to transmit values of speciﬁed base types like integer, real,
logical (or boolean).
(ii) Operations
of
wait
(blocking)
and
send
(nonblocking)
for
performing input and output, respectively, on channels.
(iii) Declarations of processes, which are like procedures that when
called, execute autonomous imperative programs that perform
wait and send operations on input and output channels passed as
arguments to the process.
(iv) A par operator for parallel execution of a set of process activations,
linked by globally declared channels passed as parameters to the
process procedures.
The principles discussed above are assumed to be satisﬁed by
programs in this language, in particular that the processes will perform
sequential computations. The body of a process is expressed in an
Algol-like simple imperative language with assignment, conditionals,
and loops. Process declarations are neither nested nor recursive, so a
program will consist of a ﬁxed, ﬁnite number of processes instances
invoked at top level and connected in a ﬁxed geometry determined
by the channel arguments. Ordinary assignable variables can only be
declared locally within process bodies.
To illustrate these concepts, we use the same program that served as
the main example in [47], shown in Figure 5.1. This program gives rise
to a ﬁxed network pictured in Figure 5.2.
In the example of Figures 5.1 and 5.2, the network is closed, in the
sense that all channels originate and terminate within the network. It
is not obvious which channel should be regarded as representing the
“result” of the computation, but we assume that it is possible to observe
all of the channel histories generated during computation. In an open
network with “dangling” channels that either have no source or no sink
in the network, the dangling channel ends serve as top-level I/O ports
that the network uses to communicate with the external environment.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

108
D. B. MacQueen
Begin
Integer channel X, Y, Z, T1, T2;
Process f(integer in U,V; integer out W);
Begin integer I; logical B;
B := true;
Repeat Begin
I := if B then wait(U) else wait(V);
print (I);
send I on W;
B := ¬ B;
End;
End;
Process g(integer in U; integer out V, W);
Begin integer I; logical B;
B := true;
Repeat Begin
I := wait(U);
if B then send I on V else send I on W;
B := ¬ B;
End;
End;
Process h(integer in U; integer out V; Integer INIT);
Begin integer I;
Repeat Begin
I := wait(U);
print (I);
send I on V;
End;
End;
f(Y,Z,X) par G(X,T1,T2) par h(T1,Y,0) par h(T2,Z,1);
End;
Fig. 5.1. Simple parallel program S.
Control ﬂow is not explicitly addressed in the informal description
of ISPL. The assumption is that all the processes activated in a par
statement will run concurrently, so there is no control hierarchy among
the processes (i.e. no main driver process), and all processes will
run unless they are blocked waiting for input on an input channel.
However, if one wanted to have a sequential control model for the
process networks, the natural choice would be a demand-driven model,
with demand initiated by a driver process. In this example, we might
decided to view X as the principle output of the program, making the f
process the obvious choice for the driver.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
109
g
f
h0
h1
Y
Z
X
T1
T2
Fig. 5.2. The network for program S.
5.3.5 Semantics
As discussed above, the point of the imperative stream processing
language is essentially to give concrete expression to the idea of
sequential processes. The sequential nature of the basic process language,
together with the characteristics of the wait and send operations, the
restrictions placed on the way processes and channels are connected,
and the assumptions about the behavior of channels mentioned above
allow us to abstract from the details of what goes on within a process,
and the timing of communication events. The behavior of a process
can be characterized as a computable, and hence continuous, function
from its input channel histories to its output channel histories. Thus
by focusing on complete channel histories, we can translate any process
network into a simple functional program over these histories, which we
will call streams.
The stream of values transmitted over a channel carrying values from
a domain D is a ﬁnite or inﬁnite sequence of elements of D, and this
domain of streams is denoted Dω. Dω is a complete partial order under
the preﬁx ordering, which we can denote as ⊑, with the least element
being Λ, the empty sequence (Reference [45] views it as a complete lattice
by adding a top element).
Three basic operations are deﬁned [45] over Dω:
HD
:
Dω →Dω
TL
:
Dω →Dω
CONS
:
Dω × Dω →Dω
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

110
D. B. MacQueen
Mnemonically, HD stands for head (the ﬁrst element of a sequence), TL
stands for tail, and CONS stands for construct.6 These are roughly the
same as the conventional list operators head, tail, cons, except that HD
returns a singleton sequence instead of an element of D and the ﬁrst
argument of CONS is a sequence rather than an element. They also diﬀer
from the conventional operators in terms of their behavior on Λ, where
we have
HD(Λ) = Λ
TL(Λ) = Λ
CONS(Λ, X) = Λ
The point of these somewhat unconventional deﬁnitions is that they
simplify some equations by avoiding having to move between sequence
and element types and by avoiding some tests for the null sequence.
An important attribute of a sequence in Dω is its length. The length
of a sequence is the obvious function from Dω to ¯N, where ¯N = N ∪
{∞} is N (the natural numbers) ordered numerically and topped with
a maximal element ∞.
Now a network such as the one shown in Figure 5.2 can be viewed as
a program schema (a graph with nodes labeled by function names and
edges labeled by channel names) together with an interpretation of each
node in the form of a tuple of history functions, one for each outgoing
edge, taking the input streams as arguments. Thus for our example, we
have functions
f
:
Nω × Nω →Nω
h
:
Nω × N →Nω
g1
:
Nω →Nω
g2
:
Nω →Nω
where the node labeled g has two associated functions, g1 and g2, each
deﬁning the history of one of the two output channels for that node.
The recursive deﬁnitions of these history functions can be derived from
the bodies of the process procedures by a variation on classical methods
going back to McCarthy [57] for translating simple imperative code into
recursive functions, and using the operators HD, TL, and CONS to express
the eﬀects of waits and sends. The details of this process are taken for
granted in [45] and [46], and the results are illustrated by example. For
6 In [46], Gilles used diﬀerent names for these operators: HD becomes F (short for
ﬁrst), TL becomes R (rest), and CONS becomes A (append).
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
111
instance, the process applied to our example program yields the following
set of function deﬁnitions:
f(U, V )
=
CONS(HD(U), CONS(HD(V ), f(TL(U), TL(V ))))
g1(U)
=
CONS(HD(U), g1(TL(TL(U))))
g2(U)
=
CONS(HD(TL(U)), g2(TL(TL(U))))
h(U, x)
=
CONS(⟨x⟩, U)
This translation also makes manifest the determinacy of the process
behaviors with respect to I/O histories, since these equations do indeed
deﬁne functions. The resulting functions are also continuous, since
they are deﬁned by composition and recursion starting with continuous
primitives.
Finally, the graph schema can be re-expressed as a set of equations
on the sequence names representing streams. In our example, this yields
the following system of equations:
X
=
f(Y, Z)
Y
=
h(T1, 0)
Z
=
h(T2, 1)
T1
=
g1(X)
T2
=
g2(X)
As the basis for interpreting such equations over streams, Gilles uses
a generalization of the construction of ﬁxpoints of equations over
continuous operators originally due to Kleene [51], as generalized by
Scott. The initial approximation to the streams is given by a tuple of
empty sequences, and an ascending chain of successive approximations
is built by iterating the stream functions deﬁned by the equations to get
the next approximation. The limit of this sequence is the least ﬁxpoint
solving the equations. The global determinacy of the parallel program
is then a consequence of the uniqueness of the least ﬁxpoint of such
equations. [45] looks at several examples of networks and their ﬁxed
points, including the one above. As a very simple example, he shows
that for the equation
S
=
CONS(a, S)
the successive approximations of the solution for X are a, aa, . . . an, . . .
and the value of S is the limit of this sequence of approximations, which
is aω.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

112
D. B. MacQueen
Gilles continued by showing that one could prove properties of such
deﬁnitions using recently developed techniques for proving properties
of
recursively
deﬁned
functions,
including
Scott
and
deBakker’s
computation induction ([74], [76]) and structural induction. As an
illustration of such proof techniques, Gilles shows that the history X in
the example above is equal to the sequence 0, 1, 0, 1, . . ..
Another technique ﬁrst used in [45] is to use a simple abstract interpre-
tation mapping sequences to their lengths to prove that a history is
inﬁnite. For instance, for the simple loop above this yields
length(X)
=
1 + length(X)
and the only solution of this in the domain ¯N is ∞, thus showing that
X is an inﬁnite sequence.
5.3.6 Recursive schema – evolving process networks
Initially, Gilles’ functional model deals with process programs with
a ﬁxed set of processes and ﬁxed network geometries. The style of
recursion used in deﬁning process functions like f, g1, g2, and h has a
particular form resulting from the translation of the iterative loops in
the sequential process programs. The deﬁnitions are quasi-tail-recursive,
i.e tail-recursive except for a leading CONS operation representing a
send operation. This limited form of recursion is appropriate for stream
functions representing stable, continuing elements of a network.
But Gilles next goes beyond the limitations of ﬁxed networks by
realizing that once one has functional representations of the network
programs, one can use more general, non-tail-recursive deﬁnitions to
express networks that evolve during computation, with processes and
channels being created and destroyed dynamically. In terms of program
graphs, he introduces recursive graph schemas, where a named program
graph (or graphs) contain nodes labeled by that same name, and
as the graph program is executed, the named node can be unfolded
into another copy of the graph schema. The named schema and the
corresponding named nodes have to have consistent “types”, in the sense
that the numbers and types of their input and output channels have to
agree. The example used in [46] is illustrated in Figure 5.3, where the
types of channels I, I′, and I′′ have to agree, and similarly for O, O′,
and O′′.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
113
g
f
F
X'
I''
O''
g
f
X
I'
I
O
O'
g
f
F
X
I'
I
O
O'
F
Fig. 5.3. A recursive schema.
The functional representation for this particular recursive network is
deﬁned by the (non-tail-recursive!) equations:
O
=
F(I)
F(I)
=
g2(F(f(I, X)))
X
=
g1(F(f(I, X)))
The concept of dynamic networks is just sketched in [45] and [46], and
illustrated using abstract examples involving uninterpreted schemas. No
operational model for executing such programs is suggested, and in
particular there is no discussion of how the imperative process language
(ISPL) might be extended to support dynamic networks. There are also
no realistic applications presented in these papers. However, dynamic
networks became a major topic in the next paper in the series, [47],
which we will consider in the next section.
5.3.7 Schematology
Schematology (roughly, the study of properties of generic programs
involving uninterpreted function symbols) was a fairly active area in
the early 1970s [54], and Gilles was able to apply or generalize some
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

114
D. B. MacQueen
known results to his process networks via their functional interpre-
tation. Between [45] and [46], Gilles collaborated with Bruno Courcelle
and Jean Vuillemin [26], to prove several general results that apply
in particular to the basic graph program schemas representing static
process networks with uninterpreted process nodes. These results show
that the equivalence of such schemas is decidable and there exists
a unique minimal schema equivalent to any given schema. Another
observation is that general equations for ﬁxpoints such as the following
(from [81])
µx.g(f(x, i))
=
g(µx.f(g(x, i)))
can be translated into equivalences between corresponding graph schema
programs.
5.3.8 Observations and summary
Gilles
argues
that
despite
the
limitations
imposed
to
maintain
determinacy, this language is still expressive enough to be useful. And
determinism simpliﬁes reasoning, because we are able to reason about
complex systems without having to deal with the complications of
state and nondeterministic changes in state in a parallel program, or
the details of timing and synchronization of events. The sequentiality
property guarantees that the behavior of processes can be modeled
by continuous functions mapping their input streams to their output
streams, and the communication discipline based on queue-like, non-
shared channels ensures that the behavior of whole programs can be
expressed as a unique ﬁxpoint of equations derived from the connection
topology of the program. Another desirable property of the model
that follows from this semantics is that the concepts are closed under
composition and recursion.
Gilles also notes that in principle, proofs can be mechanically checked
in a theorem proving system like Milner’s prover for LCF, the earliest
version of which had recently been developed by Milner and Weyrauch
at Stanford [63, 65].
Another major contribution of these papers that emerges in retrospect
is that they include what can be viewed as the earliest description
of a lazy functional language. The functional translation of network
programs introduces non-strict functions over the data structure of lazy
lists or streams, and cycles in the network introduce recursive deﬁnitions
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
115
of stream data structures. The model also shows that such functional
programs support parallel execution without losing determinacy.
5.4 Implementation, evolution, experimentation
Now we turn to the part of the history of Kahn networks in which I
personally was involved, and which was reported in [47]. Gilles and I
were both in Edinburgh for the 1975–76 academic year: I had arrived
in May to join Rod Burstall’s research group in the School of Artiﬁcial
Intelligence and Gilles arrived in the late summer for a year’s visit.
I had become interested in coroutines and related “nonstandard”
control structures, and by the winter of 1976 I was experimenting with
implementing coroutines in the POP-10 language7 [19], which was the
main language used in the School of Artiﬁcial Intelligence at Edinburgh
at that time. POP-10 had provided a kind of delimited ﬁrst-class continu-
ation, with operations barrierapply (a control delimiter), appstate
(call/cc), and reinstate (throw).8 I told Gilles about my experiments
and he suggested that we try implementing the language from his 1974
paper. We soon had a prototype running complete with new concrete
syntax using POP-10’s syntax macros.
The resulting language was described in [47], but it was not given a
name, so let us retroactively call it ISPL-POP since it is embedded in
POP-10. The basic expression and statement syntax of ISPL-POP was
inherited from POP-10, so it diﬀered superﬁcially from the Algol-based
syntax of ISPL in [46]. The primitive I/O operations on channels were
also renamed, with wait replaced by GET and send e on C being written
as PUT(e,C). The par operation was replaced by a statement of the form
doco <channel decls>; <process calls> closeco
in which a declaration of fresh local channels is followed by a sequence of
process calls applying process functions to channels. A doco statement
could appear at top level as the argument of a start command, or
embedded as the last statement in the body of a process function. In
the later case, the process instance would use the doco statement to
permanently reconﬁgure itself into the speciﬁed subnetwork, and the
channels passed to process invocations could be either freshly created or
7 POP-10 was a port of POP-2 to the DEC 10 computer recently acquired by
Edinburgh [28].
8 My coroutine experiments revealed some performance and semantic bugs in this
feature that were corrected by Robert Rae later in 1976.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

116
D. B. MacQueen
Process INTEGERS out QO;
Vars N; 1 -> N; repeat INCREMENT N; PUT(N,QO) forever
Endprocess;
Process FILTER PRIME in QI out QO;
Vars N;
repeat GET(QI) -> N;
if (N MOD PRIME) \= 0 then PUT(N,QO) close
forever
Endprocess;
Process SIFT in QI out QO;
Vars PRIME; GET(QI) -> PRIME;
PUT(PRIME,QO);
doco channels Q;
FILTER(PRIME,QI,Q); SIFT(Q,QO)
closeco
Endprocess;
Process OUTPUT in QI;
repeat PRINT(GET(QI)) forever
Endprocess;
Start doco channels Q1 Q2;
INTEGERS(Q1); SIFT(Q1,Q2); OUTPUT(Q2)
closeco;
Fig. 5.4. Sieve of Eratosthenes.
inherited from the parent process. This gives rise to dynamic changes
in the geometry of networks, as anticipated by the recursive network
schemes described in [45] and [46].
As an example, Figure 5.4 reproduces the ISPL-POP version of the
Sieve of Eratosthenes program inspired by McIlroy’s coroutine version
in [59]. Note that the body of the process SIFT ends in a doco statement
that eﬀectively replaces the SIFT process with a network of two new
processes that inherit the current instance’s input and output channels
and communicate between themselves on a fresh internal channel, Q.
This is a concrete example of the recursive schema from [45, 46].
The I/O operations GET and PUT have essentially the same semantics
as the wait and send operations of the earlier ISPL, and there is the
same restriction that a channel can only have one source, i.e. only one
process (at a time) performs output on it. But ISPL-POP treats reading
from a channel as nondestructive, so that two or more processes can
consume input from a channel without interfering with one another. This
is because channels are implemented as dynamically generated linked
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
117
lists,9 and when a channel is bound to an input port variable Q, Q is
just a pointer into the list representing the channel, and GET(Q) returns
the next element of the list designated by Q while setting Q := tail(Q).
5.4.1 Toward a functional notation
As in References [45] and [46], the critical observation is that processes
express functions from their input channel histories to their output
channel histories, or in other words, stream functions. In order to more
directly reﬂect the functional semantics of [45, 46], we wanted to evolve
the ISPL-POP language into a more functional form, principally by
treating the input and output channels of processes as stream values
constituting arguments and results of a process call.
The ﬁrst step toward a more functional notation was, for the common
case of processes with a single output channel, to treat the output
channel not as a parameter but as a result of process invocation.
Following the POP-10 notation for functions with named return values,
we use the alternate declaration notation:
Process SIFT in QI => QO;
This allows us to create a pipeline of process activations (with
anonymous connecting streams) using an applicative expression:
doco SIFT(FILTER(PRIME,QI)) => QO closeco
This also expresses, with the notation “=> Q0” the splicing of the
result stream of the expression SIFT(FILTER(PRIME,QI)) as the stream
continuation of the output stream QO of the parent process.
Process SIFT in QI => QO;
Vars PRIME; GET(QI) -> PRIME;
PUT(PRIME,QO);
doco SIFT(FILTER(PRIME,QI)) => QO closeco
Endprocess;
This doco statement can occur only at the end of the SIFT process
(a tail doco?), as shown above, and the eﬀect is that the parent SIFT
process terminates and continuation of its output stream becomes the
9 This was a natural choice for a POP-10 based implementation, because of its
dynamic lists.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

118
D. B. MacQueen
OUTPUT
SIFT
FILTER p
OUTPUT
SIFT
p
p
Fig. 5.5. The evolving Sieve network.
responsibility of the new SIFT process created by the nested doco. The
result is that we have an evolving network, as illustrated in Figure 5.5.
As another step toward a functional stream language, we use splicing
to deﬁne a cons process for consing a value onto an input stream, corres-
ponding to the semantic CONS primitive in [45] (or the A operator of [46]):
Process CONS in QI => QO;
PUT(A,QO)
doco QI => QO closeco
Endprocess;
Then we can use this stream function to make SIFT even more functional
by replacing the imperative PUT command with a call of CONS in the
continuation expression.
Process SIFT in QI => QO;
Vars PRIME; GET(QI) -> PRIME;
doco CONS(PRIME,SIFT(FILTER(PRIME,QI))) => QO closeco
Endprocess;
One more language construct, a version of Landin’s whererec, is added to
allow doco reconﬁguration statements to create cyclic subnetworks, thus
eliminating the need for explicit channel declarations and the passing
of output channels to processes in order to create cyclic networks. As
a simple example, the following program generates the sequence of all
natural numbers, assuming PLUS(n,Q) adds n to each element of the
stream Q.
Start doco OUTPUTF(X)
where channels X is CONS(0,PLUS(1,X))
closeco
Or for a more complicated example here is our often cited solution of
a problem posed by Dijkstra (to generate the sequence of all positive
integers of the form 2a3b5c where a, b, c ≥0, in order):
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
119
Start doco OUTPUTF(X)
where channels X,Y,Z are
CONS(1,MERGE(TIMES(2,X),Y)),
CONS(3,MERGE(TIMES(3,Y),Z)),
CONS(5,TIMES(5,Z))
closeco
where TIMES is scalar multiplication of an integer stream by an integer,
and MERGE merges two ordered streams into one (suppressing duplicates,
though in this version there will be none).
Having replaced the imperative PUT with CONS, we can also express
input in a functional style by introducing HEAD and TAIL operations
on streams. However, the emerging functional language for stream
processing is still embedded in the imperative language, and at the
top level a program must consist of one or more process calls, linked
by channels either explicitly in the procedural style, or by applicative
expressions with where-deﬁnitions for cycles. Thus streams were still not
quite ﬁrst-class values and the simple functional language for streams is
approximated but not quite achieved.
Another obvious limitation is that our functional form of process
can only have a single output channel (i.e. only return a single output
stream). Thus we have to fall back to the imperative style to express
some natural stream operations like splitting a stream into two streams
by taking alternate elements, which is naturally performed by the process
g in Figure 5.1. In the functional semantics of Kahn73/74, this problem
is handled by splitting the process into multiple processes, (in this case
g1 and g2) each responsible for just one of the output streams, but
typically duplicating work. A more eﬃcient functional programming
solution would be to express the g process as a mutually recursive
deﬁnition of two stream functions, each mapping a stream to a pair of
output streams, but there is still some extra overhead in constructing
and destructing the stream pairs at each level of recursion.
5.4.2 Operational semantics
Now let us consider execution strategies for ISPL-POP programs. The
the initial implementation used a sequential, demand-driven, coroutine
mode of execution. In this mode, when a network is created there is
a designated driver process (by convention the last process created).
Control passes from a consumer of a stream to the producer of the
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

120
D. B. MacQueen
stream when input from the stream is attempted and there are no
elements available (the channel is designated hungry), and control
returns to the consumer when the producer outputs on a hungry
channel. Later we added a fair process scheduler to simulate parallel
execution as envisioned in [46].
The coroutine mode performs the minimum computation, while the
concurrent mode can take advantage of multiple processors but may
perform nonessential computation, including the creation of unnecessary
processes and channels. Both modes are capable of encountering
deadlocks, where a process is waiting for input on a stream whose
producer is also blocked on input that in turn ultimately depends on
the ﬁrst process making progress.
For the concurrent mode of execution, where there is a danger of
runaway computation, we experimented with a throttling method
involving an integer anticipation coeﬃcient A(C) for each channel C. A
single producer process will run until the number of unconsumed items
in its output channel reaches A(C). This is easily implemented for the
imperative process language by having the state of a channel include the
number of unconsumed items in the channel, and having this number
be updated by the GET and PUT operations. This scheme is limited by
the restriction to single-output processes, and it also doesn’t transfer
naturally to the functional interpretation of channels where channels
are just pointers to a position in a pure stream value.
In terms of program proofs, [47] reiterated the techniques described
in [46], providing additional proof sketches for the Dijkstra program and
the Sieve of Eratosthenes. As before, the proofs depended on a variant of
recursion induction [58] and a form of structural induction (co-induction)
for proving that properties hold on an inﬁnite stream by showing that
they hold for a set of ﬁnite initial segments of unbounded lengths, and
that the property is admissible (the analog of continuity for predicates).
We also applied a technique from [45] to prove freedom from deadlock
by showing that an output stream was inﬁnite in length, using a simple
form of abstract interpretation where we map a stream to its length and
interpret a stream function abstractly as a mapping from lengths (of
input streams) to lengths (of output streams).
5.4.3 Stream programming experiments
Once ISPL-POP was implemented, we had motivation and opportunity
to explore a wide range of stream processing applications beyond the
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
121
small examples found in the previous papers. We implemented a variety
of standard algorithms and data structures.
• Sorting algorithms (bubble sort, insertion sort, merge sort, topological
sort).
• Several additional variants on the prime sieve program, including one
that used priority queues.
• Generating the stream of Fibonacci numbers.
• A coroutine version of the equalfringe function using streams [41].
• A generator of the Pascal triangle.
• A generator of lucky numbers.
• A program that searches for solutions of xn
1 + yn
1 = xn
2 + yn
2 .
• A pipeline version of Yates’ method for the discrete Fourier transform.
A more extensive example involved representing formal power series
as streams of coeﬃcients and writing stream programs to implement
a variety of operations on power series, such as arithmetic operations,
diﬀerentiation
and
integration,
logarithms
and
exponentials,
and
trigonometric functions (sine, cosine, etc.). An interesting discovery
about the series operations was that many of the operations could be
expressed in terms of cyclic process networks. For instance, the inverse
function on power series was deﬁned as follows:
Process INVERSE in V => W;
Vars V0; GET(V) -> V0;
if V0 = 0 then COMPLAIN(’Div by zero’) close;
doco T ==> W
where T is CONS(1/V0,(-1/V0) */ T /*/ V) closeco
Endprocess;
where */ is scalar multiplication by a number, and /*/ gives the
product of two power series. Some years later, Doug McIllroy continued
to develop the power series example and implemented it in a variety of
stream-supporting languages [60, 61, 62].
The ﬁnal relatively large-scale example that we developed involved
inﬁnite precision real arithmetic using algorithms developed by Wiedmer
in [84]. Real numbers were represented as streams of variable radix
coeﬃcients. This application proved more challenging and the expression
of the operations less elegant, mainly because the streams had to be
renormalized frequently to prevent the radixes from growing excessively.
Through these experimental applications, we veriﬁed that the ISPL-
POP language was adequate for expressing the stream-based algorithms
that we had in mind, and that in certain applications, like power series,
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

122
D. B. MacQueen
there were interesting ways of exploiting recursive stream deﬁnitions (i.e.
cyclic networks). In the course of debugging and informal performance
analysis of the programs, we discovered a couple of pitfalls. One of these
was that it was quite possible to have bugs that would lead to deadlock,
but when this occurred it was usually fairly easy, for the relatively
small programs we were writing, to analyze the source of the deadlock.
Deadlocks were typically caused by mismatched production rates or lack
of adequate “priming” of channels.
The other pitfall was that some programs had serious space leaks
due to a phenomenon that might be called “demand mismatch.” This
occurred in two related situations: (1) if a stream was being shared
by two (or more) consumers, with one consumer reading the stream at
a much faster rate than the other, then a longer and longer segment
of live stream elements would build up in the interval between the
positions of the two consumers in the stream, and (2) a process with
two outputs might have a fast consumer on one output and a much
slower consumer on the other output (this is really a generalization of the
ﬁrst situation to processes generating pairs of streams). With pseudo-
concurrent execution, there was also the problem of runaway execution
and creation of redundant processes whose outputs would never be used,
or would be used only long after they were generated.
The source code for many of these examples is still available from the
current author, though unfortunately the POP-10 source code for the
implementation of ISPL-POP itself has been lost.
5.4.4 Summary
The main contribution of [47] was to report on an actual implementation,
ISPL-POP, of the imperative stream processing language ISPL sketched
in [46], and to show how that imperative language could evolve by the
addition of a few features (functional processes, CONS, HEAD, and TAIL
operations, and recursive stream deﬁnitions using where) most of the
way toward the functional language for streams given as a semantics for
the imperative language in the earlier papers [45, 46]. The other main
enhancement of ISPL-POP relative to the earlier ISPL was the ability
to implement the dynamically evolving networks that were suggested by
the recursive network schemas of [45, 46].
The implementation was fairly straightforward: it was embedded in
POP-2 which supplied the basic data structures and basic imperative/
functional control constructs. Additional special syntax was provided
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
123
through macros, while processes were implemented using POP-2’s
delimited continuation feature, and streams were represented using
POP-2’s dynamic lists. Performance was not a primary concern, but it
was good enough that all our experimental programs ran acceptably
fast on the hardware of the day (DEC PDP10).
In terms of semantics, [47] described two modes of execution – the
concurrent or parallel mode assumed in the earlier papers, and also a
sequential, demand-driven coroutine mode. A key observation was that
the choice between these execution modes had no eﬀect on the denota-
tional semantics of programs as stream functions. The semantics remains
exactly the same as in the earlier papers, but the language evolved so
that it could come fairly close to directly expressing that semantics.
Meanwhile, more or less contemporarily with the research reported in
[47], lazy functional languages were being described and implemented
that could even more directly capture the functional semantic deﬁnitions
from [45, 46]. These included Ashcroft and Wadge’s Lucid [5, 6],
which in its early form could express a subset of the functional stream
programs, Turner’s SASL [78], which could handle all of them, and
probably also the lazy LISP variants described in [40] and [35]. These
parallel developments in lazy functional languages are explored further
below in Section 5.6.
5.5 Theoretical foundations: concrete domains
Although [46] presented a semi-formal denotational semantics for his
process networks, based on an informal notion of sequentiality of the
processes, Gilles wanted to develop a more rigorous semantic foundation
for stream processing networks and similar modes of incremental
computation on data structures. When he arrived in Edinburgh at the
end of the summer of 1975, he began collaborating with Gordon Plotkin
on this problem.
The result, ﬁrst written up as a manuscript draft dated December
1975 [48], was a theory of concrete domains, which were designed to
model forms of data, in contrast with domains modeling functions
or procedures. Concrete domains were meant not only to encompass
familiar ﬁnite data values like integers and ﬁnite structures built from
them, such as tuples, sums, and lists, but also potentially inﬁnite
structures like streams that could be computed incrementally, as, for
example, in Kahn networks.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

124
D. B. MacQueen
The deﬁnition of concrete domains started with a standard notion
of domain (ω-algebraic, coherent, complete partial orders) and added
additional order-theoretic properties that distinguish “data” domains
from more general domains. They also developed a representation
theory for this class of concrete domains in terms of concrete structures
they called information matrices, and which G´erard Berry later called
concrete data structures. In further unpublished work Gilles and Gordon
Plotkin showed that concrete domains could be used as the basis for
deﬁning a precise notion of sequential function.
The Kahn–Plotkin theory of concrete domains and sequential
functions provided a rigorous basis for specifying the semantics of
Kahn networks with the coroutine mode of execution discussed in [47].
The manuscript of 1975 lead to an INRIA technical report [42], and
then eventual publication in Theoretical Computer Science [43]. That
publication was accompanied by an excellent historical introduction
by Stephen Brookes [15] that explains both the origins of the work in
developing a semantics for Kahn networks, and the substantial further
development (up to 1993) by many researchers, notably Gerard Berry
and Pierre-Louis Curien (sequential algorithms, CDS [12, 13]), Glynn
Winskel (event structures [85]), and Brookes himself [14]. The concrete
domains that arose out of the theoretical investigation of Kahn networks
are highly relevant to several foundational problems, including the
notion of a sequential function, fully abstract models, the semantics of
parallel computation, and the semantics of lazy functional languages.
5.6 Lazy functional languages
Functional programming is a particularly natural context for the stream-
processing paradigm, and particularly lazy functional programming.
We have seen that Gilles’ work was preceded by Landin’s design
ISWIM (the ﬁrst true functional language) in the mid-1960s and his
invention of a functional representation of streams. But when lazy
functional languages began to emerge in the mid-1970s it quickly
became apparent that the functional semantics of Gilles’ graph programs
could be directly expressed in such languages. The development of lazy
functional languages began with the theoretical insights of Wadsworth
and Vuillemin.
Wadsworth and Vuillemin in their PhD theses [82], [80] independently
developed theoretical models for pure functional programming that
included a technique for eliminating wasted computation. Wadsworth
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
125
called this “call-by-need” while Vuillemin called his version the “delay
rule”. These techniques were similar in that they modiﬁed the classic
call-by-name discipline to avoid duplicating computational tasks by
sharing them and memoizing their results. Wadsworth achieved sharing
by maintaining the terms being reduced as directed acyclic graphs.
The lazy functional languages followed LISP in making lists the
central data structure, but they incorporated suspended computation
and memoization into these data structures so that they could also
represent potentially inﬁnite streams. Although Gilles presented the
functional form of stream processing as a semantics for an imperative
process language, and did not describe an operational semantics for
the functional form, this language did anticipate the kinds of programs
one could write with (potentially inﬁnite) lists in later lazy functional
languages. A demand driven, i.e. lazy, mode of execution for these
functional stream programs was a natural operational semantics,
corresponding to the coroutine mode of execution.
Several other investigations of lazy evaluation were going on in parallel
during the mid 1970s. These included Ashcroft and Wadge’s Lucid
language, the 1976 papers by Henderson and Morris, and Friedman and
Wise, and the SASL language of David Turner. At the same time, Milner
and his colleagues10 at Edinburgh were resurrecting and enhancing
Landin’s ISWIM as ML, the metalanguage of the LCF theorem prover.
5.6.1 Lucid
Work on Lucid began at Waterloo University in 1974, and several papers
emerged in 1975 through 1977 [5, 6, 8]. The central idea of Lucid was
to abstract from the semantics of a simple imperative language with
variables and assignment, conditional expressions and loops by focusing
on the history of values assigned to a variable through the iterations of
a loop. These iteration variable histories are sequences of values, which
may be inﬁnite in the case of a nonterminating loop, and even in the
case of a terminating loop they can be extended to an inﬁnite sequence
by repeating the last value.
Lucid treated its variables as representing these complete histories,
instead of the value at any particular instant. To simulate the eﬀect
of imperative loops, Lucid used limited forms of declarative equations
10 Lockwood Morris, Malcolm Newey, Mike Gordon, and Chris Wadsworth
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

126
D. B. MacQueen
on variables, in conjunction with certain operators on histories such as
first, next, and as soon as. Thus, an imperative program like
I := 0;
while true do
I := I + 1
end
would be represented in Lucid by the equations
first I = 0;
next I = I + 1;
Constant integer values are represented by constant histories, and
arithmetic operators like + are extended pointwise to apply to two
histories or a history and a number. Loop termination is modeled by
the “as soon as” operator: if T is a history of a boolean variable, then
J as soon as T yields a constant (history) equal to the element of J
corresponding to the ﬁrst true element of T.
Many simple imperative programs can be translated into this
equational language, and it then becomes possible to reason about the
histories using a set of axioms reﬂecting the semantics of the basic
operators (plus some additional modal operators such as eventually
and hitherto), and standard logical and algebraic reasoning methods.
So Lucid is presented as both a declarative programming language and
a logic for proving properties of programs.
If we view Lucid’s histories as streams, it is obvious that Lucid can be
used as a stream processing language, expressing streams as solutions
of recursive equations. For instance, the equations for I above are equi-
valent to:
I = 0 :: (I + 1);
More complicated programs can be modeled using nested loops that
allow subsidiary equations to be expressed relative to each iteration of
an outer loop. The operations on histories were enriched with a cons
operator, followed by, and a ﬁlter operator whenever [6].
Initially, Lucid lacked the ability to deﬁne functions on histories, so it
was not possible, for instance, to express an ordered merge operation on
two ordered histories. But soon user-deﬁned pointwise operations, called
mappings, and then general recursive deﬁnitions of history functions,
called transformations were added [7] to the language. This made it
possible to deﬁne recursive functions over histories and thus perform
general stream-processing.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
127
The semantics of Lucid’s core equational language subsumed that
of a typical lazy functional language, but it was more general because
histories were treated as a “random access” rather than a sequential
access data structure. This meant that it was possible to have rather
unusual recursive deﬁnitions, where, for instance, a value in a history
could be deﬁned recursively in terms of the future of the history. These
kinds of deﬁnitions, however, did not seem to usefully increase the power
of the language, and Lucid programs would normally fall within the
range of those expressible in conventional functional languages.
Later on [9, 10], the syntax of Lucid evolved to be more like other
functional languages, and a combination of Lucid and Landin’s ISWIM
emerged, called Luswim.
5.6.2 Lazy pure LISP
Morris and Henderson proposed a lazy evaluator for a dialect of pure
LISP [56]. The syntax is slightly simpliﬁed, static binding is assumed
instead of dynamic binding, and a FUNARG expression form is added to
represent suspended evaluations (i.e. a pair consisting of an expression
and an environment). The lazy evaluator is deﬁned by giving an
interpreter that operates on expressions stored in labeled cells in a
memory. Evaluation proceeds by overwriting cells with the value of
their stored expressions, sometimes allocating new cells in the process
(e.g. for function application). A denotational semantics is given for the
lazy language, and a soundness argument is sketched, showing that the
evaluator is consistent with the semantics.
The evaluator deﬁnes a fully lazy LISP variant that suspends
evaluation of all function arguments until they are acted upon by strict
primitive operations. CONS is treated as a nonstrict primitive that
does not evaluate its arguments, so lists can represent inﬁnite streams.
Examples showing the capabilities of lazy evaluation include a recursive
deﬁnition of an inﬁnite list, the prime sieve example, and the same
fringe example from [41] (a coroutine solution for the problem of testing
whether two binary trees have the same list of leaf nodes).
Friedman and Wise [35] took a somewhat more implementation-
oriented approach to deﬁning a lazy variant of LISP. They start by
simply suggesting that the primitive cons operation should separately
suspend evaluation of its two arguments, storing those suspensions in
the car and cdr ﬁelds of the newly allocated cons cell. The primitives
car and cdr are correspondingly modiﬁed to coerce the suspensions of
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

128
D. B. MacQueen
the respective components when they are applied. They then modify
McCarthy’s metacircular interpreter for LISP 1.0 [57] by simply substi-
tuting these new versions of cons, car, and cdr for the original strict
versions, and they observe that the result is a fully lazy LISP interpreter
where every function is lazy by default, except for the strict primitives
like atom and eq.
They note that any stream processing function written for Landin’s
streams will also work with their lazy list data type, but that it is more
general since it supports traversal of lists where some of the list elements
may be undeﬁned, as long as the traversal does not need their values.
The Henderson–Morris and Friedman–Wise versions of lazy LISP are
essentially equivalent in their behavior. The LABEL form traditionally
used for expressing a recursive function is generalized (at least
in Henderson and Morris’s evaluator), to allow recursive stream
expressions.
5.6.3 SASL, KRC, and Miranda
David Turner began working on a simple functional language around
1972–73, and this evolved into SASL (Saint Andrews Static Language)
[78] in 1976, when lazy evaluation was added. SASL can be considered
the beginning of the evolution that lead to Haskell, and SASL programs
would look very familiar to anyone who knows Haskell.
SASL was dynamically typed, with a spare, calculator-like syntax.
Its ﬁxed repertoire of data types included integers, booleans, strings,
and lists. It inherited a number of features from ISWIM, such as simple
structure deﬁnitions (bindings with list patterns), where-clauses, and the
use of the colon for list cons.
SASL had recursive function deﬁnitions with multiple equations or
clauses, and multi-argument functions were typically curried. There were
no lambda expressions or anonymous function expressions – all functions
were deﬁned either at top-level or in a where clause. Lists as well as
functions could be deﬁned recursively.
Since SASL is a lazy functional language with lists and recursive
deﬁnitions of both functions and lists, it is easy to express examples like
the prime sieve and the Hamming problem in SASL.
Turner followed SASL with a second-generation language called
KRC (Kent Recursive Calculator) [79]. KRC was a relatively minor
update from SASL, with almost identical syntax except for the use of
square brackets for lists. The major language innovation introduced by
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
129
KRC was the addition of list comprehensions (ZF expressions).11 KRC
retained the features of SASL supporting stream processing programs,
but list comprehensions could sometimes produce even more concise
expressions of these algorithms, as illustrated by this KRC version of
the prime sieve program (where “%” is the integer mod function):
primes = sieve [2..]
sieve (p:x) = p : sieve n; n <- x; n%p > 0
Miranda [77] followed SASL and KRC in the mid-1980s and added a
statically checked type system with polymorphic types based on the type
systems of ML and Hope. It also retained SASL and KRC’s support
for stream processing. Miranda was a direct ancestor of Haskell [66],
which also inherited these earlier languages’ stream processing features,
as shown by the following Haskell version of the Hamming Problem
program:
merge xs [] = xs
merge [] ys = ys
merge (x:xs) (y:ys) =
if (x == y) then x : merge xs ys
else if (x < y) then x : merge xs (y:ys)
else y : merge (x:xs) ys
times n (x:xs) = (n * x) : times n xs
h = 1 : merge (times 2 h) (merge (times 3 h) (times 5 h))
5.6.4 Streams in strict functional languages
Although lazy functional languages have an obvious advantage in being
able to represent streams directly in terms of their built-in list types,
streams have become a very commonly used structure in strict functional
languages as well. It is, of course, possible to use Landin-style streams
in strict functional languages, but it is more common to use a stream
implementation supporting memoization.
Scheme, for instance, being a lexically scoped dialect of LISP, is
a strict functional language. As explained in Chapter 3 of [1] one
can deﬁne stream operations in terms of more basic delay and force
operations that respectively create a suspension and force evaluation
of it. A semi-lazy stream cons operation (cons-stream) can be deﬁned
as a macro such that the expression (cons-stream e1 e2) expands to
(cons e1 (delay e2)). Then stream head is deﬁned to the same as
car while the stream tail operation is given by:
11 List comprehensions were also added to a later edition of SASL in 1979.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

130
D. B. MacQueen
(define (tail stream) (force (cdr stream)))
The delay operator in turn is deﬁned as a macro that suspends
its argument expression by wrapping it with a lambda abstraction
and passes it to a memoizing functional that produces a function
equivalent to the suspension (assuming no side eﬀects) but evaluating
the suspended expression only the ﬁrst time it is used. Since scheme
supports data-level recursion, it is possible to deﬁne recursive streams:
(define ones (cons-stream 1 ones))
Another strict functional language, Hope, was developed in Edinburgh
in the late 1970s [21], inspired by the functional equational language
used by Burstall and Darlington in their research on program derivation
[20, 18], and incorporating the polymorphic type system of LCF/ML [36]
enhanced by algebraic data types and pattern matching. Although this
was a strict language, a special lazy cons operation was added specif-
ically to support lazy lists or streams, and a whererec declaration form
supported direct recursive deﬁnitions of streams. Thus Hope had built-in
facilities and syntax support for stream programming.
ML is a strict functional language directly inspired by ISWIM. Stream
libraries are widely and routinely used in ML programming, though
without macros, ML provides less syntactic sugar than Scheme and the
suspension lambda abstractions are explicit. A typical deﬁnition of a
semi-lazy stream type in Standard ML would be
datatype ’a susp = EVAL of ’a | UNEVAL of unit -> ’a
datatype ’a stream = Nils | Conss of ’a * ’a stream susp ref
fun cons (x, f) = Conss(x, ref(UNEVAL f))
fun head (Conss(x,_)) = x
fun force (ref(EVAL x)) = x
| force (s as ref(UNEVAL f)) =
let val x = f() in s := EVAL x; x end
fun tail (Conss(_,y) = force y
Use of the cons operator as in the function deﬁnition
fun ints n = cons(n, (fn () => ints(n+1)))
requires an explicit function abstraction to create the suspension. Also,
because ML does not support data-level recursion, it is not possible to
have direct recursive deﬁnitions of streams.
To overcome the problem of explicit suspensions, Phil Wadler, Walid
Taha, and I developed an extension of Standard ML supporting the
deﬁnition of lazy datatypes and lazy functions over such datatypes [55],
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
131
and this design was implemented as an experimental feature of the
SML/NJ compiler [4]. It is also possible in principle to add data-level
recursive deﬁnitions to Standard ML.
The fundamental insight of [46] was that the computation performed
by Kahn networks could be expressed as recursive operations on streams
and recursive deﬁnitions of streams. This insight led us to evolve from an
imperative process language toward a functional language for streams
in [47]. But at the same time, lazy functional languages were emerging
that proved to have the necessary features to express the functional
semantics of Kahn networks, and the creators of these languages saw
Kahn networks as a natural area of application demonstrating the
expressive power of lazy functional programming. Some of the stream
programs we developed have become standard examples in the literature
of functional programming, and stream processing techniques have
become part of the standard tool set in functional programming.
On the other hand, it has to be admitted that gems like the Hamming
Problem and the recursive deﬁnition of the Fibonacci stream do not
seem to be representative of applications of stream processing at a larger
scale. Experience has shown that most realistic applications turn out to
involve linear, or at least acyclic, networks.
5.7 Lasting impact of kahn networks
In the 30 years since Gilles’ invention of Kahn networks and development
of the related theory, the inﬂuence of this work has spread widely
and touched many parts of computer science. In functional languages,
both lazy and strict, the stream processing paradigm has become
a routine aspect of programming. Even in conventional systems
programming streams are a common abstraction, playing an important
role in shell languages, I/O libraries (from Standard ML [68] to Java
[39]), networking [69], signal processing [33], computer graphics and
data management (e.g. streaming query languages). The notion has
even leaked into the quotidian world of consumer technology and
entertainment in the form of “streaming media.”
Some of these manifestations of the stream concept, like ﬁlters and
pipes in the Unix shell, or early data ﬂow models, arose before or
independently of Gilles’ work on streams, but his work provided for
the ﬁrst time a clear, elegant, and principled understanding of these
phenomena, and tools for reasoning about the behavior of stream
processing. The theoretical legacy of Kahn networks is also connected
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

132
D. B. MacQueen
to the development of Kahn–Plotkin concrete domains and the impact
of that work on the development of the concept of sequentiality.
Several variations on Kahn networks and stream processing have been
deﬁned and studied in the succeeding years, including synchronous Kahn
networks [23], N-synchronous Kahn networks [2], abstract synchronous
networks [2], among many others, and Gilles’ basic insight that networks
can be modeled by recursive functions over streams has been elevated
to the status of a general Principle (the Kahn Principle).
Finally, Kahn networks have a strong legacy for language design,
with a number of important languages owing part of their inspiration
to Gilles’ work on streams. These would include the synchronous
data ﬂow language LUSTRE [22, 38], SISAL, and Functional Reactive
Programming [83].
5.8 Conclusions
The development of Kahn networks as a programming technique, a
semantic model, and a basis for tractable reasoning about concurrent
programs is a model of clear development. The guiding principle behind
Gilles’ work in general, and Kahn networks in particular, is that
mathematical abstractions and formal semantics are valuable tools in
the design of computing artifacts.
My aim in this work has been to show how this work is connected
to ideas from several important lines of development, including data
ﬂow, coroutines, Landin’s streams, and the early development of lazy
functional programming.
I also claim that the functional semantics of Kahn networks should
be considered an early functional language supporting either a lazy,
demand-driven or a concurrent operational model. The architectural
ideas, semantics, and reasoning techniques associated with this model
became a part of the core culture of functional programming.
However, the inﬂuence of these ideas was not restricted to functional
programming. It had a continuing inﬂuence on modeling and analyzing
concurrent programs, on speciﬁcation languages, on software architecture,
and on systems programming.
5.9 Acknowledgements
I wish to thank Doug McIlroy for useful background information on the
history of coroutines and the development of the Unix shell, Jack Dennis
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
133
for material on the early development of data ﬂow models, and David
Turner for information on the early history of SASL. I also thank the
editors of this volume for their patience and forbearance during the long
gestation of this paper, and the anonymous reviewers for their comments.
Bibliography
[1]
H. Abelson, G. J. Sussman, and J. Sussman. Structure and Interpretation
of Computer Programs. McGraw-Hill, New York, 1985.
[2]
S. Abramsky. A generalized Kahn principle for abstract asynchronous
networks. In Mathematical Foundations of Programming Semantics, 5th
International Conference, pp. 1–21. Springer-Verlag, 1989.
[3]
D. A. Adams. A Computation Model with Data Flow Sequencing. PhD
thesis, Computer Science Dept, Stanford University, December 1968.
Technical Report CS-117.
[4]
A.
Appel
and
D. B.
MacQueen.
Standard
ml
of
new
jersey.
In
J. Maluszynski and M. Wirsing (eds) Programming Language Implemen-
tation and Logic Programming, Proceedings of the 3rd International
Symposium, volume 528 Lecture Notes in Computer Science, pp. 1–13.
Springer Verlag, 1991.
[5]
A. E. Ashcroft. Program proving without tears. In G. Huet and G. Kahn
(eds) Symposium on Proving and Improving Programs, pp. 99–111. INRIA
Rocquencourt, July 1975.
[6]
A. E. Ashcroft and W. W. Wadge. Lucid – a formal system for writing
and proving programs. SIAM J. Comput., 5:519–526, 1976.
[7]
A. E. Ashcroft and W. W. Wadge. Lucid: Scope Structures and Deﬁned
Functions. Technical Report Rep. CS-76-22, Computer Science Dept.,
University of Waterloo, 1976.
[8]
A. E. Ashcroft and W. W. Wadge. Lucid, a nonprocedural language with
iteration. Commun. ACM, 20(7):519–526, 1977.
[9]
A. E. Ashcroft and W. W. Wadge. Structured Lucid. Technical Report
CS-79-21, Computer Science Department, University of Waterloo, 1979.
[10]
A. E. Ashcroft and W. W. Wadge. Lucid, the Dataﬂow Programming
Language. Number 22 in APIC Studies in Data Processing. Academic Press,
1985.
[11]
J. L. Baer. A survey of some theoretical aspects of multiprocessing. ACM
Comput. Surv., 5(1):31–80, 1973.
[12]
G. Berry and P.-L. Currien. Sequential algorithms on concrete data
structures. Theoret. Comput. Sci., 20:265–322, 1982.
[13]
G. Berry and P.-L. Currien. The kernel of the applicative language cds:
theory and practice. In Proc. French-US Seminar on the Applications of
Algebra to Language Deﬁnition and Compilation, pp. 35–87. Cambridge
University Press, 1985.
[14]
S. Brookes and S. Geva. Continuous functions and parallel algorithms on
concrete data structures. In Proc. 7th International Conf. on Mathematical
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

134
D. B. MacQueen
Foundations of Programming Semantics, volume 598 in Lecture Notes in
Computer Science, 1991.
[15]
S. Brookes. Historical introduction to “concrete domains” by G. Kahn
and G. D. Plotkin. Theoret. Comput. Sci., 121(1-2):179–186, 1993.
[16]
W. H. Burge. Recursive Programming Techniques. Addison Wesley, 1975.
[17]
W. H. Burge. Stream processing functions. IBM J. Res. Develop., pp.
12–25, 1975.
[18]
R. M. Burstall. Design considerations for a functional programming
language. In Infotech State of the Art Conference: The Software Revolution,
Copenhagen, October 1977.
[19]
R. M. Burstall, J. S. Collins, and R. J. Popplestone. Programming in
POP-2. Edinburgh University Press, 1977.
[20]
R. M. Burstall and J. Darlington. A tranformation system for developing
recursive programs. J. ACM, 24(1), 1977.
[21]
R. M. Burstall, D. B. MacQueen, and D. Sannella. Hope: An experi-
mental applicative language. In Conference Record of the 1980 Lisp
Conference, pp. 136–143, August 1980. Stanford.
[22]
P. Caspi, D. Pilaud, N. Halbwachs and J. A. Plaice. LUSTRE: a
declarative language for real-time programming. In POPL ’87: Proceedings
of
the
14th
ACM
SIGACT-SIGPLAN
Symposium
on
Principles
of
Programming Languages, pp. 178–188, New York, NY, USA, 1987. ACM
Press.
[23]
P. Caspi and M. Pouzet. Synchronous Kahn networks. In ICFP ’96:
Proceedings of the ﬁrst ACM SIGPLAN International Conference on
Functional Programming, pp. 226–238, New York, NY, USA, 1996. ACM
Press.
[24]
A. Cohen, M. Duranton, C. Eisenbeis, C. Pagetti, F. Plateau and
M. Pouzet. N-synchronous Kahn networks: a relaxed model of synchrony
for real-time systems. In POPL ’06: Conference Record of the 33rd ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages,
pp. 180–193, New York, NY, USA, 2006. ACM Press.
[25]
M. E. Conway. Design of a separable transition-diagram compiler.
Commun. ACM, 6(7):396–408, 1963.
[26]
B. Courcelle, G. Kahn and J. Vuillemin. Algorithmes d’equivalence et de
reduction a des expressions minimales dans une classe d’equations recursives
simples. In Proceedings of the 2nd Colloquium on Automata, Languages and
Programming, pp. 200–213, London, UK. Springer-Verlag, 1974.
[27]
J. Darlington and M. Reeve. Alice a multi-processor reduction machine
for
the
parallel
evaluation
cf
applicative
languages.
In
FPCA
’81:
Proceedings of the 1981 Conference on Functional Programming Languages
and Computer Architecture, pp. 65–76, New York, NY, USA, 1981. ACM
Press.
[28]
J. Davies. POP-10 User’s Manual. Technical Report CS R25, University
of Western Ontario Computer Science Dept., 1976.
[29]
J. B.
Dennis.
Programming
generality,
parallelism,
and
computer
architecture. In Information Processing 68, pp. 484–492. North Holland,
1969.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
135
[30]
J. B. Dennis. First version of a data ﬂow procedure language. In
Programming Symposium, Proceedings Colloque sur la Programmation,
volume 19 of Lecture Notes in Computer Science, pp. 362–376, Springer-
Verlag, 1974.
[31]
J. B. Dennis, J. B. Fosseen and J. P. Linderman. Data ﬂow schemas. In
G. Goos and J. Hartmanis (eds), International Symposium on Theoretical
Programming, volume 5, Lecture Notes in Computer Science, pp. 187–216,
Springer-Verlag, 1974.
[32]
J. B. Dennis. A language design for structured concurrency. In J. H.
Williams and D. A. Fisher (eds), Proceedings of the DoD Sponsored
Workshop on Design and Implementation of Programming Languages,
volume 54, Lecture Notes in Computer Science, pp. 231–242. Springer-
Verlag, 1977.
[33]
J. B. Dennis. Stream Data Types for Signal Processing. Computation
Structures Group Memo 36, MIT LCS, October 1994.
[34]
J. B. Dennis and D. P. Misunas. A preliminary architecture for a basic
data-ﬂow processor. In ISCA ’75: Proceedings of the 2nd Annual Symposium
on Computer Architecture, pp. 126–132, New York, NY, USA, 1975. ACM
Press.
[35]
D. P. Friedman and D. S. Wise. Cons should not evaluate its arguments.
In
S.
Michaelson
and
R.
Milner
(eds),
Automata,
Languages
and
Programming, pp. 257–284. Edinburgh University Press, 1976.
[36]
M. J. C. Gordon, A. J. R. G. Milner, L. Morris, M. C. Newey and
C. P. Wadsworth. A metalanguage for interactive proof in LCF. In Fifth
ACM Symposium on Principles of Programming Languages, New York,
1978. ACM Press.
[37]
J. R Gurd, C. C Kirkham and I. Watson. The Manchester prototype
dataﬂow computer. Commun. ACM, 28(1):34–52, 1985.
[38]
N. Halbwachs, P. Caspi, P. Raymond and D. Pilaud. The synchronous
dataﬂow programming language LUSTRE. Proceedings of the IEEE,
79(9):1305–1320, Sept. 1991.
[39]
E. Harold. Java I/O, 2nd Edition. O’Reilly Media, 2006.
[40]
P. Henderson and J. H. Morris. A lazy evaluator. In Third ACM
Symposium on Principles of Programming Languages, pp. 123–42, New
York, 1976. ACM Press.
[41]
C. Hewitt, P. Bishop, R. Steiger, I. Greif, B. Smith, T. Matson
and R. Hale. Behavioral semantics of nonrecursive control structures.
In Programming Symposium, Proceedings Colloque sur la Programmation,
volume 19, Lecture Notes in Computer Science, pp. 385–407. Springer-
Verlag, 1974.
[42]
G. Kahn and G. D. Plotkin. Domaines concrets. INRIA Rapport 336,
INRIA, 1978.
[43]
G. Kahn and G. D. Plotkin. Concrete domains. Theoret. Comput. Sci.,
121(1–2):187–277, 1993.
[44]
G. Kahn. An approach to systems correctness. In SOSP ’71: Proceedings
of the Third ACM Symposium on Operating Systems Principles, pp. 86–94,
New York, NY, USA, 1971. ACM Press.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

136
D. B. MacQueen
[45]
G. Kahn. A Preliminary Theory for Parallel Programs. Technical Report
Rapport Laboria no. 6, IRIA Rocquencourt, January 1973.
[46]
G. Kahn. The semantics of a simple language for parallel programming.
In Information Processing 74, Proceedings of the IFIP Congress 74, pp.
471–475. Elsevier North Holland, 1974.
[47]
G. Kahn and D. B. MacQueen. Coroutines and networks of parallel
processes. In B. Gilchrist (ed.), Information Processing 77, pp. 993–998.
North Holland, 1977.
[48]
G. Kahn and G. Plotkin. Concrete Data-types. ﬁrst draft manuscript,
December 1975.
[49]
R. M. Karp and R. E. Miller. Properties of a model for parallel
computations: Determinacy, termination, queueing. SIAM J. Appl. Maths,
14(6):1390–1411, 1966.
[50]
J. Kelly, C. Lochbaum and V. Vyssotsky. A block diagram compiler.
Bell System Tech. J., 40(3):669–676, May 1961.
[51]
S. Kleene. Introduction to Metamathematics. Van Nostrand, 1952.
[52]
G. Kopec. A high-level block-diagram signal processing language. In
IEEE International Conference on Acoustics, Speech, and Signal Processing,
pp. 684–687, 1979.
[53]
P. J. Landin. A correspondence between ALGOL 60 and Church’s
lambda-notation: Part I. Commun. ACM, 8(2):89–101, 1965.
[54]
D. C. Luckham, D. M. R. Park and M. S. Paterson. On formalized
computer programs. J. System Sci., 4(3):220–249, 1970.
[55]
D. B. MacQueen, P. Wadler and W. Taha. How to add laziness to a
strict language without even being odd. In Proceedings of the 1998 ACM
Workshop on ML, pp. 24–30, September 1998. Baltimore, MD.
[56]
J.
McCarthy,
P. W.
Abrahams,
D. J.
Edwards,
T. P.
Hart
and
M. E. Levin. LISP 1.5 Programmer’s Manual. MIT Press, 1962.
[57]
J. McCarthy. Towards a mathematical science of computation. In
Proceedings of the IFIP Congress 1962, pp. 21–28. North-Holland, 1962.
[58]
J. McCarthy. A basis of a mathematical theory of computation. In
P. Braﬀort and D. Hirshberg (eds), Computer Programming and Formal
Systems, pp. 33–70. North-Holland, 1963.
[59]
M. D. McIlroy. Coroutines. unpublished note, May 1968.
[60]
M. D. McIlroy. Squinting at power series. Software Pract. Exper.,
20:661–683, 1990.
[61]
M. D. McIlroy. Power Series as Lazy Streams. Technical Report
BL011276-970313-02TMS, Bell Laboratories, Lucent Technologies, 1997.
[62]
M. D. McIlroy. Power series, power serious. J. Funct. Program., 9(3):325–
337, May 1999.
[63]
R.
Milner.
Implementation
and
applications
of
Scott’s
logic
for
computable functions. In Proceedings of ACM Conference on Proving
Assertions About Programs, pp. 1–6, New York, NY, USA, 1972. ACM.
[64]
R. Milner. Processes: A mathematical model of computing agents. In
Proceedings of the Colloquium in Mathematical Logic, pp. 157–173. North-
Holland, 1973.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

Kahn networks at the dawn of functional programming
137
[65]
R. Milner and R. Weyrauch. Proving compiler correctness in a
mechanized logic. In Machine Intelligence 7. Edinburgh University Press,
1972.
[66]
S. Peyton-Jones (ed.) Haskell98, Languages and Libraries, The Revised
Report. Cambridge University Press, 2003.
[67]
E. D.
Reilly.
Milestones
in
Computer
Science
and
Information
Technology. Greenwood Press, 2003.
[68]
J. Reppy and E. Gansner. The Standard ML Basis Library. Cambridge
University Press, 2006.
[69]
D. M. Ritchie. A stream input-output system. AT&T Bell Laboratories
Tech. J., 63(8):1897–1910, 1984.
[70]
D. M. Ritchie and K. Thompson. The UNIX time-sharing system.
Commun. ACM, 17(7):365–375, 1974.
[71]
J. E. Rodriguez. A Graph Model for Parallel Computations. Technical
Report TR-64, MIT Project MAC, September 1969.
[72]
D. Scott. Outline of a Mathematical Theory of Computation. Technical
Report Technical Monograph PRG-2, Programming Research Group,
Oxford University, November 1970.
[73]
D. Scott. Continuous Lattices. Technical Report Technical Monograph
PRG-7, Programming Research Group, Oxford University, August 1971.
[74]
D. Scott. Data types as lattices. SIAM J. Comput., 5:522–587, 1976.
[75]
D. Seror. D.C.P.L: A Distributed Control Programming Language. PhD
thesis, University of Utah, 1970.
[76]
S. F. Smith. A computational induction principle. unpublished note, July
1991.
[77]
D. A. Turner. An overview of Miranda. SIGPLAN Notices, 21, 1986.
[78]
D. A. Turner. SASL Language Manual. Technical report, St. Andrews
University, Department of Computational Science, December 1976.
[79]
D. A. Turner. The semantic elegance of applicative languages. In
Proceedings of the 1981 Conf. on Functional Programming and Computer
Architecture, 1981.
[80]
J. Vuillemin. Correct and optimal implementations of recursion in a
simple programming language. In STOC ’73: Proceedings of the Fifth
Annual ACM Symposium on Theory of Computing, pp. 224–239, New York,
NY, USA, 1973. ACM Press.
[81]
J. Vuillemin. Correct and optimal implementations of recursion in
a simple programming language. J. Comput. System Sci., 9(3):332–354,
December 1974.
[82]
C. P. Wadsworth. Semantics and Pragmatics of the Lambda-calculus.
PhD thesis, Oxford University, 1971.
[83]
Z. Wan and P. Hudak. Functional Reactive Programming from
ﬁrst principles. In Proceedings of the ACM SIGPLAN’00 Conference on
Programming Language Design and Implementation (PLDI’00), 2000.
[84]
E. Wiedmer. Exaktes rechnen mit reellen zahlen. Technical Report
Bericht no. 20, Eidgen¨ossische Technische Hocchschule, Zurich, July 1976.
[85]
G.
Winskel.
Events
in
Computation.
PhD
thesis,
Edinburgh
University, 1981.
https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

https://doi.org/10.1017/CBO9780511770524.006 Published online by Cambridge University Press

