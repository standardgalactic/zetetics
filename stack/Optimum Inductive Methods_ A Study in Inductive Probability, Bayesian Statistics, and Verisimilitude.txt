OPTIMUM INDUCTIVE METHODS 

SYNTHESE LIBRARY 
STUDIES IN EPISTEMOLOGY, 
LOGIC, METHODOLOGY, AND PHll...OSOPHY OF SCIENCE 
Managing Editor: 
JAAKKO HINTIKKA, Boston University 
Editors: 
DIRK VAN DALEN, University of Utrecht, The Netherlands 
DONALD DAVIDSON, University of California, Berkeley 
THEO A.F. KUIPERS, University ofGroningen, The Netherlands 
PATRICK SUPPES, Stanford University, California 
JAN WOLENSKI, Jagiellonian University, Krakow. Poland 
VOLUME232 

ROBERTO FESTA 
Fellow of the Department of Philosophy of Science, 
University of Groningen, The Netherlands 
OPTIMUM INDUCTIVE METHODS 
A Study in Inductive Probability, 
Bayesian Statistics, and Verisimilitude 
SPRINGER-SCIENCE+BUSINESS MEDIA, B.V. 

Library of Congress Cataloging-in-Publication Data 
Festa, Roberto. 
Optimum inductive methods 
a study in inductive probability, 
Bayesian statistics, and verisimi litude / by Roberto Festa. 
p. 
cm. -- (Synthese 1 ibrary ; v. 232) 
Includes bibl iographical references and indexes. 
ISBN 978-90-481-4318-4 
ISBN 978-94-015-8131-8 (eBook) 
1. Bayesian statistical decision theory. 
2. Probabilities. 
3. Induction (Mathematics) 
4. Truth. 
r. Title. 
II. Series. 
QA279.5.F47 
1993 
519.5'42--dc20 
93-11840 
ISBN 978-90-481-4318-4 
Printed on acid-free paper 
AH Rights Reserved 
© 1993 Springer Science+Business Media Dordrecht 
Originally published by Kluwer Academic Publishers in 1993 
Softcover reprint ofthe hardcover Ist edition 1993 
No part of the material protected by this copyright notice may be reproduced or 
utilized in any form or by any means, electronic or mechanical, 
including photocopying, recording or by any information storage and 
retrieval system, without written permission from the copyright owner. 
DOI 10.1007/978-94-015-8131-8 

To my parents 

TABLE OF CONTENTS 
ACKNOWLEDGMENTS 
x1 
1. INTRODUCfiON 
1 
1. An outline of issues and objectives 
1 
2. The probabilistic and the verisimilitude views: two fallibilistic 
methodological traditions 
3 
3. Bayesian statistics and the theory of inductive probabilities 
4 
4. Optimum prior probabilities: the contextual approach 
6 
5. The layout of the book 
7 
PART I. INDUCTIVE PROBABILITIES, BAYESIAN 
STATISTICS, AND VERISIMILITUDE 
2. THE THEORY OF INDUCfiVE PROBABILITIES: BASIC 
FEATURES AND APPLICATIONS 
13 
1. Inductive methods 
13 
2. Multicategorical inferences 
16 
3. Multinomial contexts 
17 
3. BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES: 
BASIC FEATURES 
20 
1. What is Bayesian statistics? 
20 
2. Probability distributions 
23 
3. Bayesian statistical inferences 
29 
4. Probability distributions for multinomial contexts 
30 
5. Bayesian multinomial inferences 
35 
vii 

viii 
TABLE OF CON1ENTS 
4. BAYESIAN POINT ESTIMATION, VERISIMILITUDE, AND 
IMMODESTY 
38 
1. Bayesian point estimation and verisimilitude 
38 
2. The principle of immodesty 
40 
3. The basic issues of the verisimilitude theory 
45 
PART II. DE FINETTI'S THEOREM, GC-SYSTEMS, AND 
DIRICHLET DISTRIBUTIONS 
5. EXCHANGEABLE INDUCTIVE METHODS, BAYESIAN 
STATISTICS, AND CONVERGENCE TOWARDS THE TRUTH 
51 
1. De Finetti's representation theorem 
51 
2. Convergence of opinion and convergence towards the truth 
53 
6. GC-SYSTEMS AND DIRICHLET DISTRIBUTIONS 
57 
1. GC-systems 
57 
2. Dirichlet distributions 
60 
3. The equivalence between GC-systems and Dirichlet distributions 
65 
4. Extreme GC-systems and extreme Dirichlet distributions 
66 
5. The axiomatization of GC-systems 
68 
6. The axiomatization of Dirichlet distributions 
70 
PART III. VERISIMILITUDE, DISORDER, AND OPTIMUM 
PRIOR PROBABILITIES 
7. THE CHOICE OF PRIOR PROBABILmES: THE SUBJECTIVE, 
APRIORISTIC, AND CONTEXTUAL APPROACHES 
75 
1. The choice of priors in Bayesian statistics: the subjective 
approach 
76 

TABLE OF CONTENTS 
ix 
2. The choice of priors in Bayesian statistics: the aprioristic 
approach 
79 
3. The subjective interpretation of the theory of inductive 
probabilities 
88 
4. The aprioristic interpretation of the theory of inductive 
probabilities 
90 
5. The contextual view of prior probabilities 
95 
6. A contextual justification of Dirichlet distributions 
and GC-systems 
100 
8. THE EPISTEMIC PROBLEM OF OPTIMALITY (EPO): 
A CONTEXTUAL APPROACH 
103 
1. The optimum prior vector 
104 
2. GO-contexts 
105 
3. The CC-solution to EPO 
108 
4. The logical problem of optimality 
109 
5. The V-solution to EPO 
116 
6. The equivalence between the V -solution and the CC-solution 
to EPO 
118 
7. Camap's optimum inductive methods 
121 
9. THE CONTEXTUAL APPROACH TO EPO: COMPARISONS 
WITH OTHER VIEWS 
123 
1. The universalistic view 
123 
2. The hyperempiricist view 
127 
3. The presupposition view 
132 
4. The verisimilitude view 
136 
10. DISORDERED UNIVERSES: DIVERSITY MEASURES IN 
STATISTICS AND THE EMPIRICAL SCIENCES 
139 
1. Gini diversity 
140 
2. Explicating diversity 
141 
3. Diversity measures in the empirical sciences 
147 
11. CONCLUDING REMARKS 
150 

X 
TABLE OF CON1ENTS 
NOTES 
REFERENCES 
INDEX OF NAMES 
INDEX OF SUBJECTS 
List of requirements and acronyms 
154 
177 
185 
188 
192 

ACKNOWLEDGMENTS 
My views on scientific inference have been decisively influenced by the 
works of Karl Popper and Rudolf Camap. In particular, my research in this 
area has been inspired by Popper's view- that verisimilitude, or truthlikeness, 
is the main cognitive goal of science - and by Carnap's theory of inductive 
probabilities (and, more generally, by his Bayesian approach to scientific 
methodology). 
In spite of the well known disputes between these two great masters of 
contemporary epistemology, I believe that a 'reconciliation' between Sir Karl 
Popper and Rudolf Carnap is possible. This book, indeed, is inspired by the 
conviction that Popper's verisimilitude view can be fruitfully embedded 
within the Bayesian approach. 
Unfortunately there are very few epistemologists interested in inductive 
probabilities and verisimilitude. In particular, the set of students who have 
given important contributions to both fields seems to include only two 
members: Professor I. Niiniluoto and Professor T.A.F. Kuipers. Thanks to 
scholarships offered by the Italian Ministry of Foreign Affairs, I had the 
opportunity to work under the supervision of both. 
In 1981-82 I was at the Department of Philosophy at the University of 
Helsinki where I worked on verisimilitude under Professor Niiniluoto's 
supervision (see Festa, 1982, 1983). I have been deeply influenced by 
Professor Niiniluoto's views on scientific inference and, more generally, on 
the methods and tools of philosophy of science. Among other things, I am 
grateful to Professor Niiniluoto for giving a sharp exposition of my 
unpublished proposals on verisimilitude in monadic first-order languages 
(Festa, 1982) in his book Truthlikeness (1987, pp. 319-321). 
Afterwards, in 1983-84, I was at the Department of Philosophy at the 
University of Groningen where I continued my research into verisimilitude 
under Professor Kuipers's supervision (see Festa, 1986, 1987a). 
In 1986-87 I started working, under Professor Kuipers's supervision, on the 
subject of this book (see Festa, 1987b and Festa and Buttasi, 1987). My 
personal debt to Professor Kuipers for the research leading to the completion 
of this book is enormous. His continuous support and encouragement during 
many years have been of the utmost importance to me. Indeed he was much 
xi 

xii 
ACKNOWLEDGMENTS 
more than the supervisor of this research: without him this book would never 
have been written. 
At the beginning of 1990 Professor Kuipers suggested inviting Professor 
W. Molenaar, from the Department of Statistics and Measurement Theory of 
the Faculty of Behavioral and Social Sciences (University of Groningen), to 
join him in the supervision of my work. In spite of the clumsy statistical 
terminology used in the first draft of this book, Professor Molenaar very 
kindly accepted the invitation. His help in improving the form and the content 
of the earlier drafts has been invaluable. If a Bayesian statistician can 
appreciate.the proposals advanced here without rejecting them immediately 
the credit is largely due to him. 
Although I have profited enormously from the time and care that both my 
supervisors devoted to reading and criticizing the earlier drafts, this does not 
mean that they will be satisfied with everything that is in the final version. 
Of course any mistakes in it are mine, not theirs. 
I also wish to thank Professor W. K B. Hofstee, Professor I. Niiniluoto and 
Professor W. Schaafsma for reading the manuscript and suggesting several 
improvements. 
I am most indebted to Professor A. Pasquinelli, Professor G. Sandri and 
Professor M. C. Galavotti of the Department of Philosophy at the University 
of Bologna. From all of them I received stimulus and encouragement while 
studying for my degree in philosophy and later. 
My interest in Bayesian statistics and its relationships with the theory of 
inductive probabilities was strongly stimulated by Professor D. Costantini 
through his works, numerous talks and his comments on my first paper on the 
subject of this book (Festa, 1984). 
I also have an intellectual debt to several friends and colleagues who have -
through publications, correspondence, and personal contacts - influenced my 
work. In particular, I would like to mention Dario Antiseri, Alberto Artosi, 
Roger Cooke, Paolo Garbolino, Pierdaniele Giaretta, Giulio Giorello, Risto 
Hilpinen, Isaac Levi, Marco Mondadori, David Pearce, Marcello Pera, 
Gianguido Piazza, Claudio Pizzi, Yao H. Tan, Raimo Tuomela, Bas van 
Fraassen, Henk Zandvoort. 
I am grateful to Professor L. Nauta of the Department of Philosophy at the 
University of Groningen, for his help and kindness. I also owe my thanks to 
Ms Mariette Elzenga and Mr Nico de Jager for their assistance while I was 
working at the library of the Department of Philosophy at the University of 
Groningen. 

ACKNOWLEDGMENTS 
xiii 
For practical help in preparing the manuscript, I wish to thank especially 
Ms Cheryl Gwyther (for improving the English style) and Mr Massimo Bonra 
(for printing the camera-ready copy and drawing the figures). 
The time I spent in Holland would not have been so enjoyable without the 
friendship of Hans, Simone, Annet, Claar, and Jos Mooij, Inge de Wilde, 
Barend van Heusden, Harrie Jonkman, Ineke Siersema and Jos Griffioen. I am 
particularly grateful to Claar Mooij and Barend van Heusden for the 
enormous practical help they gave me while I was working on the completion 
of this book, not only during my presence in Holland but also, and especially, 
during my absence. 

CHAPTER 1 
INTRODUCTION 
This book deals with a basic problem arising within the Bayesian approach 
to scientific methodology, namely the choice of prior probabilities. 1 The 
problem will be considered with special reference to some inference methods 
used within Bayesian statistics (BS) and the so-called theory of inductive 
probabilities (T/P). 2 In this study an important role will be played by the 
assumption - defended by Sir Karl Popper and the supporters of the current 
verisimilitude theory (VT) - that the cognitive goal of science is the 
achievement of a high degree of truthlikeness or verisimilitude. 
A more detailed outline of the issues and objectives of the book is given 
in Section 1. 
In Section 2 the historical background of the Bayesian approach and the 
verisimilitude theory is briefly illustrated. In Section 3, the methods used in 
TIP and BS for making multinomial inference~ are considered and some 
conceptual relationships between TIP and BS are pointed out. In Section 4 the 
main lines of a new approach to the problem of the choice of prior 
probabilities are illustrated. Lastly, in Section 5 >the structure of the book is 
described and a first explanation of some technical terms is provided. 
1.1 AN OUTLINE OF ISSUES AND OBJECTIVES 
According to the Bayesian approach the hypotheses3 taken into examination 
by a scientist within a given empirical inquiry are appraised in terms of their 
posterior probabilities relative to the available data, where such posterior 
probabilities are obtained as follows: 
(1) 
first the scientist's initial opmwns about the hypotheses are 
formally represented by certain prior probabilities following the 
rules of the calculus of probability; 
(2) 
subsequently the posterior probabilities of such hypotheses relative 
to the result of a given experiment are derived from their prior 
probabilities by applying Bayes's theorem. 
1 

2 
CHAPTER 1 
The above mentioned probabilities, used to represent the scientist's degrees 
of belief in the hypotheses, will be called epistemic probabilities.4 
Furthermore, the term "inductive probabilities" will be often used w.r.t. the 
epistemic probabilities considered in TIP. 
A troublesome point concerning (1) is the choice of the prior probabilities 
used to represent the scientist's initial opinions. The two 'traditional' views 
on this point are the subjective view and the aprioristic view. 
According to the subjective view the axioms of the probability calculus are 
the only 'principles of rationality' ruling the choice of prior probabilities. 
Any choice is admissible as long as a given scientist feels that it corresponds 
with his initial opinions. More precisely, any scientist should select the prior 
probabilities best fitting his initial opinions. 
In conflict with the subjective view, the different types of 'objectivism' 
share the assumption that the choice of prior probabilities is ruled - in 
addition to the probability axioms - by further principles of rationality. In 
particular, according to the aprioristic view a unique admissible choice can 
be specified on the basis of appropriate principles of rationality justified by 
a priori considerations. 
In this book the two traditional views are rejected and a new 'objective 
view' - called the contextual view- is proposed. According to this view the 
choice of prior probabilities is ruled ~y appropriate context-dependent 
principles of rationality which can' be applied only in certain 'cognitive 
contexts'. The notion of cognitive context can be illustrated as follows. 
An empirical inquiry does not consist simply in collecting experimental 
data to (dis)confirm a given hypothesis. Indeed in any empirical inquiry an 
important role is played by different elements such as: (1) the background 
knowledge shared by the scientists, i.e., the corpus of theories and other 
assumptions taken for granted, at least within that inquiry; (2) the cognitive 
goals pursued by scientists qua scientists.5 These elements form the cognitive 
context of the inquiry, i.e., the specific 'cognitive situation' where the inquiry 
is actually made.6 
The notions of background knowledge and cognitive goals will play an 
important role in the development of the contextual view. More specifically, 
concerning cognitive goals, the following verisimilitude thesis (VER) - which 
represents a basic assumption of VT - will be adopted: 
(VER) The main cognitive goal of science is the achievement of a high 
degree of truthlikeness or verisimilitude. 

INTRODUCTION 
3 
1.2 THE PROBABILISTIC AND THE VERISIMILITUDE VIEWS: 
TWO FALLIBILISTIC METHODOLOGICAL TRADITIONS 
The Bayesian view of scientific methodology and the verisimilitude theory 
belong to two different methodological traditions which originated in the 
seventeenth and the eighteenth centuries from the crisis of infallibilism. 
From antiquity to the first half of the eighteenth century infallibilism was 
the prevailing methodological view in philosophy of science. 7 According to 
this view the cognitive goal of science is to discover exactly true theories 
about the world and to achieve complete certainty about the truth of such 
theories.8 
The truth represents the antic element of infallibilism (since the truth of a 
theory concerns its relationships with reality) whilst certainty represents the 
epistemic element (since certainty can be seen as one of the possible 
'epistemic attitudes' towards a theory).9 
From the middle of the eighteenth century it became increasingly clear that 
infallibilism was indefensible and that more modest 'methodological ideals' 
were needed (see Laudan, 1973, pp. 277-278). For instance, the epistemic 
element or the ontic element of infallibilism could be weakened: certainty 
could be replaced by probability and the truth by verisimilitude. Indeed, both 
possibilities were explored and, accordingly, two different fallibilistic views 
of scientific knowledge were proposed. 
In the middle of the seventeenth century the ancient tradition of 
'probabilism', dating back to Carneades in the second century B. C., was 
revived by a number of philosophers and mathematicians including Pascal, 
Huygens and Jacques Bernoulli. Contrary to the infallibilistic tenet that 
certainty is the only epistemic state compatible with genuine science, 10 the 
probabilistic view suggests that in most cases scientists can only attain 
non-extreme degrees of belief in the truth of the considered hypotheses, i.e., 
degrees of belief represented by probabilities different from 0 and 1.11 
In the middle of the eighteenth century a number of scientists and 
philosophers, such as David Hartley, George LeSage and Joseph Priestley, 
proposed a different fallibilistic view - henceforth referred to as the 
verisimilitude view - based on "the doctrine that scientific methods are self-
corrective, that science in its development is inexorably moving closer to the 
truth by a process of successive approximation" (Laudan, 1973, p. 276). 12 
The probabilistic view and the verisimilitude view evolved into two distinct 
methodological traditions often in competition with each other. I am inclined 
to believe that the rivalry between these two fallibilistic traditions is due more 
to historical reasons than to theoretical incompatibility and that a 'mature' 

4 
CHAPTER 1 
fallibilistic methodology can be developed by integrating the two traditions. 13 
For instance, the contextual view of prior probabilities proposed in this book 
can be seen as a 'co-ordinated development' of TIP, BS and VT where the 
first two methodological theories belong to the tradition of probabilistic 
fallibilism, and the third belongs to the tradition of verisimilitude 
fallibilism. 14 
1.3 BAYESIAN STATISTICS AND THE THEORY OF 
INDUCTIVE PROBABILITIES 
Empirical sciences make use of different kinds of inferences leading from a 
set of premises to another statement, usually called conclusion. 15 According 
to a traditional distinction inferences are divided into deductive and inductive. 
While in deductive inferences the information conveyed by the conclusion 
is already included, more or less explicitly, in the premises, in inductive 
inferences the conclusion says something more- or something new- w.r.t. the 
premises. Hence, even though the truth of the premises is taken as guaranteed 
(e.g. when the premises describe some experimental evidence) one cannot be 
completely certain of the truth of the conclusion, since there is inevitably the 
risk that the 'additional information' contained in it is false. 16 
A particular kind of inductive inference is given by multinomial inferences, 
which can be described as follows. 
Suppose that k <!: 2 mutually exclusive and jointly exhaustive categories 
Q1, ... ,Qk are used to classify the outcomes of any trial of an experimental 
process Ex. Then, if there is a physical probability11 q1 that the outcome of 
an arbitrary trial of Ex is Q; (for i = l, ... ,k) and this probability is constant 
across trials (neither influenced by trial number nor by the outcomes of 
previous trials), then Ex may be referred to as a Bernoulli process (if k = 2) 
or a multivariate Bernoulli process (if k > 2). In the case where the scientist's 
background knowledge includes the assumption that the investigated 
'multicategorical' process Ex is a (multivariate) Bernoulli process, the 
scientist's cognitive context will be called multinomial context. Although 
typically a scientist does not know the true value of the parameter vector 
q=(q1, ... ,<1J..) governing Ex, he can formulate different hypotheses about q or 
the results of future trials of Ex. The inductive inferences concerning such 
hypotheses will be called multinomial inferences. 
As seen above, according to the Bayesian view the scientist's degrees of 
belief in the hypotheses under examination can be represented by suitable 
epistemic probabilities. For instance, given the evidence e describing a 

IN1RODUCTION 
5 
sequence of trials of Ex and a hypothesis h concerning q, or future trials of 
Ex, a Bayesian multinomial inference is given by the determination of the 
posterior probability p(h/e) of h relative to e. 
In particular, in BS the starting point of multinomial inferences is to 
provide an adequate representation of the scientist's initial opinions about the 
value of q. For this purpose a so-called cumulative distribution function 
F(q1, ... ,qk_1), defined for all the values (q1, ... ,qic_1) of the parameter vector 
(q1, .... ~_J, is used. (The last component~ follows from the requirement that 
the sum of all k probabilities equals 1: see Chapter 3.4.) From F(q1, ... ,qk_J -
which is frequently called prior distribution - the prior probability p(en) of 
any sequence en of outcomes of n trials of Ex can be derived. Moreover, 
given the evidence e describing one or more earlier trials of Ex, the prior 
distribution F(q1,._.,qk_1) can be updated by Bayes's theorem in order to obtain 
the posterior distribution F(q1, ... ,qic_1/e) which allows one to derive the 
posterior probability p(h!e) of any hypothesis h concerning q or future trials 
of Ex. 
The inductive methods introduced in TIP can be seen as procedures to 
determine the epistemic probabilities p(h!e) for couples (h,e) of statements 
relative to multivariate Bernoulli processes and other kinds of multicategorical 
processes.18 Usually an inductive method I is introduced by defining the 
corresponding prior predictive distribution PI which specifies the prior 
probability PI (en) for any sequence en of outcomes of n trials of the 
considered process Ex. Afterwards, the requested probabilities p(h/e) can be 
derived from PI. 
An inductive method I is called exchangeable iff, for any n and any en, the 
prior probability PI (en) depends only on the number of outcomes in en 
belonging to each of the categories Qw .. ,Qk and not on the order in which 
they occur in en. Somewhat surprisingly, it follows from de Finetti's 
representation theorem that the prior predictive distribution PI corresponding 
to an exchangeable inductive method I can be derived from a unique prior 
distribution F(q1, ... ,qk_J. This signifies that the exchangeable inductive 
methods considered in TIP are 'equivalent' to the prior distributions used in 
BS for making multinomial inferences. In particular, it can be proved by 
using de Finetti's theorem that the well known exchangeable inductive 
methods proposed by Carnap and Stegmiiller (1959) - henceforth referred to 
as GC-systems19 - are equivalent to the so-called prior Dirichlet distributions 
commonly used in BS. 

6 
CHAPTER 1 
1.4 OPTIMUM PRIOR PROBABILITIES: THE CONTEXTUAL 
APPROACH 
In this book the choice of prior probabilities will be considered with 
particular reference to multinomial contexts and, more specifically, to the 
problem of the choice of a GC-system for such contexts. 
A GC-system can be referred to as (y,A) since it is fully characterized by 
a prior vector y=(Yw···Yk) and a parameter A (0 < A< oo) where: (1) each 
component Y; (i = 1, ... ,k) of y denotes the prior probability of the hypothesis 
that the outcome of the first trial of the investigated process Ex is Qi; (2) A 
can be seen as an index of the reluctance of a subject using (y,A) to change 
his prior probabilities in response to the experimental evidence.20 
The problem of the choice of a GC-system for a given multinomial context 
will be called the epistemic problem of optimality (EPO) and the selected 
GC-system will be referred to as the optimum GC-system (y 0,A 0 ). The term 
"optimum" is suggested by the idea - underlying the approach to EPO 
developed in Chapter 8 - that a possible reason for selecting a given inductive 
method, from a certain class of inductive methods, is that there are grounds 
to believe that it is 'the optimum tool' for achieving a high degree of 
verisimilitude. 
EPO can be split in two subproblems: (i) the selection of the optimum prior 
vector y0 , (ii) the selection of the optimum GC-system (y 0,A o) from the set 
of all the GC-systems (y 0,A) with prior vector y0 • The suggested solution to 
EPO - which can be seen as a particular application of the contextual view 
of prior probabilities (see Section 1) - applies to multinomial contexts where: 
(1) 
The background knowledge shared by the scientists engaged in the 
inquiry includes an 'informal estimate' Est[G(q)] of the 
(unknown) Gini diversity G(q) = 1 - ~qi of Ex;21 
(2) 
the cognitive goal pursued by the scientists is to minimize the 
distance between their (y,A.)-based estimate of q and the truth (the 
true value of q). 
Given the above mentioned equivalence between GC-systems and prior 
Dirichlet distributions, solving EPO, i.e., choosing a GC-system for a given 
multinomial context, amounts to choosing the corresponding prior Dirichlet 
distribution. Hence, the suggested solution to EPO can be seen as a 
contribution to an issue which has been extensively investigated in BS. 
However, it should be pointed out that the aims of this book do not include 
the foundation of BS, let alone the foundation of statistics. Indeed 

INTRODUCTION 
7 
multinomial inferences are only one of many kinds of statistical inferences 
and the results obtained for multinomial inferences are not directly 
generalizable to the other cases. My purposes w.r.t. statistical practice are 
neither descriptive nor prescriptive. Far from saying what statisticians do or 
should do, I just suggest what Bayesian statisticians could do in a very 
idealized kind of cognitive context, i.e., multinomial contexts.Z2 
1.5 THE lAYOUT OF THE BOOK 
The prospective audience of this book includes readers interested in: (1) the 
theory of inductive probabilities (TIP); (2) the problem of the choice of prior 
probabilities in Bayesian statistics (BS); and (3) the verisimilitude theory 
(VT). More generally, it includes anyone interested in statistics and the 
philosophy of science. Given that many of these readers may be relatively 
unfamiliar with any of the three above mentioned subjects, the first part of the 
book (Chapters 2-4) provides introductory material on TIP, BS and VT. 
In Chapter 2 some basic features of TIP are considered. In particular, some 
basic requirements satisfied by different types of inductive methods are 
illustrated and a concise classification of the 'intended applications' of such 
methods is given. 
In Chapter 3 some basic features of BS are described and some methods 
used in BS for making multinomial inferences are briefly illustrated. A reader 
familiar with BS can skip the first part of the Chapter (Sections 1-3) and 
examine only the second part (Sections 4-5) where new 'predictive 
interpretations' of some well-known typical values of the probability 
distributions on the parameter vector of a multivariate Bernoulli process are 
suggested and some relatively unknown typical values - such as the expected 
Gini diversity - are considered. 
In Chapter 4 a well known Bayesian point estimator, i.e., the mean 
estimator, is considered and a 'verisimilitude justification' of this estimator 
is suggested. The principal objective of this chapter is to prove that epistemic 
probabilities satisfy the so-called principle of immodesty which plays a very 
important role in the contextual approach to EPO. This chapter also provides 
a brief outline of the basic issues of the verisimilitude theory. 
The main purpose of the second part of the book (Chapters 5-6) is to 
illustrate the basic features of GC-systems and Dirichlet distributions and the 
relationships between the two. 
In Chapter 5 some implications of de Finetti's representation theorem are 
described. In particular, the one-to-one correspondence between the set of 

8 
CHAPTER 1 
exchangeable inductive methods and the set of prior distributions on the 
parameter vector q of a multivariate Bernoulli process is considered. 
Moreover, some problems regarding the convergence of opinion in scientific 
research are examined. For instance, it is shown that exchangeable inductive 
methods normally allow scientists to learn from experience, not only in the 
'epistemic sense' that scientists using different inductive methods are led to 
a considerable convergence of opinion, but also in the 'realist sense' that the 
predictive probabilities derived from such methods tend, in the long run, to 
approach the truth. 
In Chapter 6 the basic features of GC-systems and Dirichlet distributions 
are considered and the relationships between the two are elucidated. More 
specifically, it is shown that there is a one-to-one correspondence between the 
class of GC-systems and the class of the prior Dirichlet distributions on the 
parameter vector q of a multivariate Bernoulli process. Moreover, some 
axiomatizations of GC-systems and Dirichlet distributions are considered. 
The contextual view of prior probabilities and, in particular, a contextual 
approach to the epistemic problem of optimality (EPO) are illustrated in the 
third part of the book (Chapters 7-11). 
In Chapter 7 the two 'traditional' views about the problem of the choice of 
prior probabilities, i.e., the subjective view and the aprioristic view, are 
considered and a relatively new contextual view is presented. In particular, a 
context-dependent justification for using a prior Dirichlet distribution (or, 
equivalently, a GC-system) for a given inquiry is suggested. 
Chapter 8 contains the main technical results of the book. A contextual 
approach to EPO - i.e., the problem of determining the optimum Dirichlet 
distribution (or, equivalently, the optimum GC-system) for investigating a 
given multivariate Bernoulli process - is proposed. In particular, a 
verisimilitude solution, or V-solution, to EPO based on the verisimilitude 
thesis (VER) is given. The V-solution to EPO is obtained using the following 
strategy: (a) having defined the so-called logical problem of optimality, or 
LPO, for the set DIR(y) of all Dirichlet distributions with a given prior vector 
y, a V-solution to LPO is provided (Section 4); (b) using (VER) and the 
V-solution to LPO, a V-solution to EPO is elaborated (Section 5). Among 
other things it is shown (Section 7) that the V -solution to LPO represents a 
generalization of some results concerning the 'logically optimum inductive 
methods' obtained by Carnap (1952). 
In Chapter 9 some features of the contextual approach to EPO are 
elucidated by comparing this approach with others. 
In Chapter 10 Gini diversity, which plays a crucial role in the contextual 
approach to EPO, is considered in some detail. 

INTRODUCTION 
9 
Finally, in Chapter 11, the relationships between the contextual view and 
some recent methodological programmes of research are considered and a 
number of possible developments of the contextual approach to EPO are 
suggested. 
How to use this book. The book uses the standard notation from elementary 
mathematics and assumes no previous knowledge on the part of the reader. 
Sections, formulae, figures and notes are numbered according to their order 
of appearance in each chapter. In the same chapter they are referred to simply 
by their numbers. In a chapter different from that where a formula is 
originally given, the formula number is preceded by its chapter number: e.g., 
formula 23 stated first in Chapter 5 is referred to as 'formula (5.23)'. 

PART I 
INDUCTIVE PROBABILITIES, BAYESIAN 
STATISTICS, AND VERISIMILITUDE 

CHAPTER 2 
THE THEORY OF INDUCTIVE PROBABILITIES: 
BASIC FEATURES AND APPLICATIONS 
In this chapter some basic features and applications of the theory of inductive 
probabilities (TIP) are described. 
In Section 1 the notion of inductive method is introduced and a number of 
fundamental requirements, satisfied by different types of inductive methods, 
are illustrated. In Section 2 a concise classification of the 'intended 
applications' of inductive methods, i.e., multicategorical inferences, is given. 
Lastly, in Section 3 the so-called multinomial contexts, where multi categorical 
inferences are frequently made, are defined. 
2.1 INDUCfiVE METHODS 
A set Q including k mutually exclusive and jointly exhaustive categories, or 
properties, Q1, ... ,Qk can be used to describe (the composition of) a population 
U or samples of individuals randomly drawn from U. More generally Q can 
be used to describe an experimental process Ex or sequences of trials of Ex. 
The statements recording these descriptions will be referred to as 
multicategorical statements. 
Multicategorial statements may refer to different kinds of multicategorial 
processes (see Chapter 11, note 4). The objective of TIP is to define 
appropriate inductive methods for such processes, i.e., appropriate procedures 
to determine the inductive probabilities p(hle) for all couples (h,e) of 
multicategorical statements or, at least, for a wide set of such couples. 
Given a sequence e. of outcomes of n trials of a multicategorical process 
Ex, the number of outcomes in e. belonging to the category Q; , or 
Q;-outcomes, will be referred to as n;(e.). Two sequences e. and e~ are called 
structurally identical (e. =s e~) iff, for any Q;, n;(e.) = n;(e~). 
The symbol "e." will also be used to denote a complete description of the 
sequence e., i.e., a statement specifying to which category each outcome in 
e. belongs. 
13 

14 
CHAPTER 2 
The hypotheses: "the outcome of the next trial of Ex will be Q;", "the 
outcomes of the next two trials will be Q/', and "the outcomes of the next 
two trials will be Q; and Q/' will be referred to as Q; , Q; Q; , and Q; Qj , 
respectively, and their inductive probabilities relative to the evidence e 
describing a sequence of trials of Ex as p(Q; /e), p(Q; Q; /e), and p(Q; Qj /e). 
Lastly, the evidence obtained by adding the observation of a Q; -outcome to 
e will be referred to as eQ;. 
A prior predictive distribution on Ex defines the prior probability p(en) for 
any sequence en of outcomes of n trials of Ex. In particular, given an 
inductive method I, the prior predictive distribution specified by I will be 
referred to as PI· An inductive method I is virtually identical to its prior 
predictive distribution PI since, in general, the inductive probabilities p(h!e) 
can be derived from PI ( cf. Camap, 1952, p. 18). Hence, the expressions 
"prior predictive distribution on Ex" and "inductive method (for Ex)" will be 
used interchangeably. 
In TIP the inductive probabilities p(h!e) determined by any inductive 
method I are required to satisfy the axioms of probability calculus ( cf. 
Carnap, 1971, p. 40): 
(A1) 
Lower Bound. p(h!e) 0!: 0 
(A2) 
Self-confirmation. p(e/e) = 1 
(A3) 
Complement. p(h/e) + p(-.hfe) = 1 
(A4) 
Multiplication. p(h & h'!e) = p(h!e)p(h'/e & h) 
In (A1)-(A4) it is assumed that the second argument of function pis logically 
possible. Notice that (A1)-(A4) are stated in terms of the conditional 
probability function p(h/e). The absolute probability function p(h) is given by 
the definition "p(h) = p(h!z)" where z denotes the tautological proposition. 
It follows from the multiplication rule (A4) that p(en Q;) = p(en )p(Q;Ien ). 
The repeated application of this equality shows that the prior probability p(en) 
of any sequence en can be obtained from the so-called special values 
p(Q; /en) including the initial ones p(Q). For instance, the prior probat.llity of 
e4 = Q 1Q3Q3Q2 is given by p(e4) = p(Ql)p(Q3/Ql)p(QiQIQ3)p(QiQ1Q3Q3). 
This means that the special values allow one to determine a predictive 
distribution on Ex or, equivalently, an inductive method for Ex (cf. Camap, 
1952, pp. 16-18). 
In addition to the axioms of probability calculus, other assumptions - or, 
to use the terminology of analytical philosophy, other requirements of 
adequacy - have been suggested in TIP to characterize different types of 
inductive methods. Some of these will be given below. 

THE THEORY OF INDUCTIVE PROBABiliTIES 
15 
The requirement of exchangeability (Exc) states that identical inductive 
probabilities should be attributed to two structurally identically sequences of 
outcomes: 
An inductive method satisfying (Exc) is called exchangeable. 
Another requirement of adequacy considered in TIP is the requirement of 
regularity (Reg) which demands that any sequence e. should receive a 
positive probability: 
(Reg) 
p(e.) > 0 
Finally, let us mention three requirements which might be considered 
inductive requirements in the strictest sense, since they concern the effect of 
experimental evidence on the inductive probabilities attributed to certain 
hypotheses. 
The first of these 'strictly inductive' requirements is the requirement of 
positive relevance (PR) which demands that, whenever the empirical evidence 
e is enriched by the observation of a further Qi -outcome, the inductive 
probability attributed to the hypothesis that the next trial be Qi should 
zncrease: 
(PR) 
p(QJeQ;) > p(QJe) 
By replacing ">" with "~" in (PR) the weaker requirement of non-negative 
relevance (NNR) is obtained: 
(NNR) p(Qi /eQi) ~ p(Qi !e.) 
The third strictly inductive requirement is the so-called axiom of convergence, 
or Reichenbach axiom (Reich) (cf. Gaifman, 1971, p. 244):2 
(Reich) For every e > 0 there exists N such that, for every e., if n > N 
then I p(QJe.) - nJn I < e, for i = 1, ... ,k. 
Put simply, (Reich) requires that "as the total size of the sample becomes 
larger, the [probability p(Qi !e.)] that, given the sample, the next trial will 
belong to the i-th attribute, should approach the relative frequency [nJn] of 
this attribute within the sample" (Gaifman, 1971, pp. 243-244). 

16 
CHAPTER 2 
The above-mentioned requirements of adequacy are predictive requirements 
in the sense that they concern the inductive probabilities attributed to the 
'predictions' of certain future events. In particular, the above requirements are 
stated, or can be restated, in terms of the special values p(Q; /en) which 
represent the inductive probabilities of certain 'singular predictions'. For 
instance, (Reg) is logically equivalent to the following requirement ( cf. 
Kuipers, 1980, p. 185)~3 
(Reg)* p(Qi /en) > 0 
The specification of a certain class of inductive methods on the basis of a 
corresponding set of predictive requirements relative to special values has 
noticeable intuitive, philosophical and technical implications. 
From an intuitive point of view, this strategy is convenient as it would 
appear much easier to explore our presystematic intuitions concerning the 
special values p(Q; /en) than, for instance, our intuitions regarding the prior 
probability p( en) of certain finite sequences of experimental results. 
From a philosophical point of view the special emphasis on special values 
springs from the idea, going back at least to John Stuart Mill, that inductive 
inferences are essentially inferences from particulars to particulars.4 
Lastly, from a technical point of view the importance of special values is 
mainly related to the fact that they allow one to determine the predictive 
distribution p1 which corresponds to a given inductive method I (see above). 
2.2 MULTICATEGORICAL INFERENCES 
Let the evidence e in p(hle) describe a sample drawn from a given population 
U. Then, depending on the nature of h, one can distinguish different types of 
multicategorical inferences.5 
For instance, one may distinguish between predictive and global inferences. 
In a predictive inference, the hypothesis h describes a sample to be taken 
from U. In particular, in a singular predictive inference the sample described 
by h consists of only one trial. In a global inference, h describes certain 
'global features' of U. 6 A special type of global inference is the universal 
inference, where h is a universal hypothesis, for example, "All the individuals 
in U belong to one of the categories Qil, ... ,Q;8", where Q;1, ••• ,Q;8 is a subset 
ofQ. 

THE THEORY OF INDUCTIVE PROBABILITIES 
17 
Another important type of global inference is the statistical inference where 
h is a 'statistical hypothesis' concerning the value of N; IN and N; is the 
number of individuals in U belonging to the category Q;. 
For instance, one might try to calculate the inductive probabilities p(h!e) 
and p(h'!e) of the statistical hypotheses h = "NtfN = 0.3" and h' = "0.3 s 
N1/N s 0.7" relative to evidence e. In principle, such probabilities can be 
derived from the special values of a given inductive method by repeated 
application of the multiplication rule and other axioms of the probability 
calculus. In practice, however, when U includes a very large number N of 
individuals, such calculations become enormously laborious and the 
difficulties appear to increase when U is infinite (since, in this case, the field 
of the possible values of N/N does not include only (0, liN, 2/N, ... ,1) but is 
identical to the interval [0,1] including a non-denumerable infinity of 
values).7 
2.3 MULTINOMIAL CONTEXTS 
Multicategorical inferences are frequently made in those cognitive contexts 
which might be called multinomial (cognitive) contexts. 
Before introducing multinomial contexts, the so-called multivariate 
Bernoulli processes should be considered: 
(1) 
Let Ex be an experimental process with k > 2 possible outcomes 
Q1,. .. ,Qk for any trial. Then Ex is a multivariate Bernoulli process 
iff: 
(i) 
the objective probability of obtaining Q; (with i = 1, ... ,k) in any 
trial is independent of the outcomes obtained in previous trials, and 
(ii) 
it is stable over the sequence of trials. 
It follows from (1) that the objective probability of Q; in a multivariate 
Bernoulli process Ex is given by a fixed value q; and, consequently, the 
objective probability Pq(en) of a given sequence en of outcomes of n trials is 
given by: 
k 
(2) 
Pq(en) = q':_' rb_' ••• CJ7c' = TI q';; 
i=l 
where n; denotes the number of Q;-outcomes in en. Accordingly, a multivariate 
Bernoulli process is characterized by an 'exchangeable distribution of 

18 
CHAPTER 2 
outcomes', in the sense that the objective probability distribution Pq (en) does 
not depend on the particular order in which Q1, ••• ,Qk occur in en, but on their 
frequencies n 1, ... ,nk only. This implies that identical objective probabilities are 
attributed to two sequences en and e~ which are structurally identical: 
In particular, it follows from (2) that: 
(4) (a) 
(b) 
where Pq(Q;) is the objective probability that the outcome Q; in the next trial 
of Ex and Pq (Q; Q;) is the objective probability of a sequence Q; Q; of two 
Q; -outcomes. 
The set of sequences which are structurally identical to a given sequence 
en will be referred to as en. If n 1, ... ,nk is the k-tuple of the frequencies of 
Q 1, ••• ,Qk in en, then en includes n!/(n1!n2! ··· nk!) sequences. Hence, it 
follows from (2) and from (3) that the objective probability P(en) of en is 
given by: 
k 
(5) 
P(en) = n!/(n1!n2! ··· nk!) II if/ 
i=1 
The probability distribution in (5) is usually called the multinomial 
distribution. 
Multivariate Bernoulli processes can be seen as an extension of the more 
familiar Bernoulli processes which are obtained from (1) by replacing "k > 
2" by "k = 2". 
Although complete certainty that a given real-world experimental process 
Ex is a (multivariate) Bernoulli process can never be guaranteed, in many 
cases it can be assumed, on the basis of the available background knowledge, 
that Ex will behave, at least approximately, as a (multivariate) Bernoulli 
process. 
A typical application of the 'Bernoulli model' is given by the successive 
tosses of an apparently regular coin, where it is typically assumed that the 
objective probability q1 of the appearance of the head is 112 in each toss. The 
Bernoulli model is also applied when the coin is visibly irregular. Although 
in this case the value of the objective probability q1 governing the process 

THE THEORY OF INDUCTIVE PROBABiliTIES 
19 
cannot be specified it can be estimated on the basis of a sequence of trials 
(see Chapter 3.5). 
Consider an urn of constant composition containing N balls of k different 
colors Q 1, ... ,Qk where Ni is the number of balls of color Qi in the urn. Then 
random sampling with replacement from the urn can be considered a 
multivariate Bernoulli process with objective probabilities q1, ... ,qk where qi = 
Ni /N. Where the composition of the urn is unknown, it can only be assumed 
that random sampling with replacement from the urn is a multivariate 
Bernoulli process, although in this case the value of the objective probabilities 
q1, ... ,qk which govern the process cannot be specified. The objective 
probabilities q1, ... ,qk of a multivariate Bernoulli process Ex 'related' to a 
given population U may be directly referred to as the objective probabilities 
of Q 1, ... ,Qk in U. 
The practical importance of the above 'urn model' cannot be over-
estimated since (i) typically the composition of a multicategorical population 
is assumed to be (approximately) constant and (ii) the experimental process 
Ex consisting in the observation of individuals taken from the population is 
(approximately equal to) a random sampling with replacement.8 
Multinomial contexts can be defined as follows: 
(6) 
Given an experimental process Ex with k > 2 possible outcomes 
Q 1, ... ,Qk for any trial, the cognitive context C is a multinomial 
context iff the background knowledge BK available in C includes 
the assumption that Ex is a multivariate Bernoulli process.9 
"Multinomial context" will be used herein in a wide sense to include also 
those cases where Ex is a Bernoulli process (k = 2). 
Although in a multinomial context the true value of the parameter vector 
q=( q1, ... ,qk) governing Ex is typically unknown, a researcher can, on the basis 
of the observations of a sequence of trials of Ex, make certain inductive 
inferences - which can be called multinomial inferences - concerning q or the 
results of future trials of Ex. 

CHAPTER 3 
BAYESIAN STATISTICS AND MULTINOMIAL 
INFERENCES: BASIC FEATURES 
In this chapter some basic features of Bayesian statistics (BS) are described 
and some methods used in BS for making multinomial inferences are 
illustrated. 
Section 1 includes a definition of BS and a taxonomy of Bayesian 
statistical inferences. In Section 2 some introductory notions of probability 
theory concerning univariate and multivariate probability distributions are 
given. In Section 3 the application of Bayes's theorem to global and 
predictive statistical inferences is illustrated. 
Lastly, the probability distributions .on the parameter vector q of a 
multivariate Bernoulli process Ex are considered (Section 4) and some 
methods used in BS for making multinomial inferences are examined (Section 
5). 
A reader familiar with BS can skip the first part of this chapter (Sections 
1-3) and read only the second part (Sections 4-5) where new 'predictive 
interpretations' of some well-known typical values of the probability 
distributions on q are suggested and some relatively unknown typical values -
such as the expected Gini diversity - are considered. 
3.1 WHAT IS BAYESIAN STATISTICS? 
A statistical inference is a particular type of inductive inference (see Chapter 
1.3) where the empirical evidence concerns a sample taken from a given 
population and the inductive conclusion concerns the whole population or 
certain samples to be selected from it. More generally, a statistical inference 
is an inductive inference which concerns certain parameters of a given 
experimental process or sequences of trials of this process. 
A theory of statistical inference provides a systematic corpus of procedures 
which should govern the different types of statistical inferences. 
Although Good (1983b, p. 191), perhaps somewhat amusingly, says that 
there are at least 93312 varieties of Bayesians, all Bayesian statisticians 
20 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
21 
presumably share a nucleus of views which represents, as it were, the 
differentia specifica distinguishing Bayesian statistics (BS) from the so-called 
orthodox statistics. The distinctive features of BS are the following: 
(i) 
While orthodox statistics uses only empirical information 
concerning the sample, BS also takes account of the prior 
information researchers possess before sampling. 
(ii) 
Prior information is formally specified by a distribution of prior 
probabilities on the possible values of certain parameters which 
characterize the population or process under investigation, where 
such probabilities are epistemic. 
(iii) 
Using Bayes's theorem prior probabilities are updated on the basis 
of the empirical sample so as to obtain a distribution of posterior 
probabilities on the parameters. Posterior probabilities provide a 
formal representation of the researcher's beliefs after sampling. 
(iv) 
The conclusions of a statistical inference are (derived from) the 
posterior probabilities. 
Given a set of t mutually exclusive and jointly exhaustive hypotheses h1, ... ,h1 
concerning the composition of a given population, the posterior probabilities 
p(hj /e) are derived from the prior probabilities p(hj ) and the so-called 
likelihoods p(e/hj) by the following frequently used formulation of Bayes's 
theorem: 
(1) 
= 
p( e/hj )p( hj) 
~ p(e!hJp(hJ 
i=l 
The Bayesian strategy for combining prior information and new empirical 
information, so as to obtain the posterior probabilities, is represented in 
Figure 1 ( cf. Zellner, 1971, p. 10). 
Bayesian inferences may be classified using two different criteria. 
According to the first criterium - relating to the 'target' of Bayesian 
inferences -
a distinction should be made between global and predictive 
inferences: 
(1a) 
Global inferences concern certain global features or parameters of 
the population or process under investigation. 

22 
CHAPTER 3 
(lb) 
Predictive inferences concern certain features of samples 
(sequences of trials) still to be drawn from the population (process) 
under investigation. 
According to the second criterium 
- relating to the 'nature' of the 
conclusions of a statistical inference - a distinction should be made between 
probabilistic and accepiational inferences: 
(2a) 
In probabilistic inferences the conclusions are the posterior 
probabilities of the different hypotheses considered. 
(2b) 
In acceptational inferences the conclusions consist of the 
(inductive) acceptance of a hypothesis, while posterior probabilities 
play an important role in accepting that hypothesis. 1 
As far as the rules of inductive acceptance are concerned, one may 
distinguish between pure acceptational inferences, where the acceptance 
depends exclusively on the posterior probabilities of the hypotheses 
considered, and utilistic acceptational inferences, where the utility (loss) 
involved in the acceptance of each of the hypotheses in each of the possible 
Prior 
Prior 
_____. 
probablBty 
~ 
information 
p(h} 
Bayes's 
Posterior 
theorem 
_____. 
probability 
/ 
p(h/e] 
Empirical 
IJkellhood 
information 
_____. 
function 
e 
p(e/h] 
Figure 1 
The Bayesian strategy for combining prior information and new 
empirical information 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
23 
states of nature is also taken into account. In utilistic acceptational inferences 
various kinds of utilities may be employed. For instance, the so-called 
decision-theoretic statistics ( cf. Ferguson, 1970) usually refers - more or less 
explicitly - to pragmatic utilities such as economical gains or losses. On the 
other hand, the so-called theory of cognitive decisions ( cf. Levi, 1967, 1980 
and Niiniluoto, 1987, Ch. 12) employs several cognitive utilities, such as 
truth, information content and verisimilitude. 
On the basis of the methodological importance attributed to the various 
kinds of Bayesian inferences, it is possible to identify different types of 
Bayesian approaches. 
Firstly consider the distinction between the probabilistic approach and the 
acceptational approach. According to the probabilistic approach the only 
information scientists really need are the posterior probabilities of hypotheses, 
while 
the 
acceptational 
approach 
emphasizes 
the 
methodological 
indispensability of acceptational inferences? 
Secondly consider the distinction between the globalistic approach and the 
predictivistic approach. According to the globalistic approach statistical 
inferences concern mainly the 'global features' of the population, while the 
predictivistic approach emphasizes the practical and scientific importance of 
predictive problems.3 More specifically, the predictivistic approach stresses 
that the most important objective of statistical inferences consists in 
calculating the posterior predictive distributions p(y!z), where y and z denote 
possible values of two given experiments y and z.4 
3.2 PROBABILITY DISTRIBUTIONS 
This section introduces the main features of the probability distributions of 
unknown parameters. Note, however, that all the included notions and 
formulae also apply to the probability distributions of random variables where 
a random variable is "a numerical quantity whose value is determined by an 
experiment of chance" (Lindgren, 1976, p. 50). 
Kinds of probability distributions 
The statistical inferences concerning the value of an unknown real-valued 
quantity - or parameter - 9 characterizing a given population or process are 
called univariate inferences. For instance, 9 may be (i) the proportion of 
defective pieces in a batch of industrially manufactured articles, (ii) the 

24 
CHAPTER 3 
chance of an irregularly shaped coin turning up heads, (iii) the mean lifetime 
of a particular component of a mechanical instrument. 
Bayesian univariate inferences are made on the basis of an appropriate 
probabilistic representation of the researcher's uncertainty regarding 9, i.e., of 
a cumulative distribution function, or cdf, F((}). Given a value a of 9, the 
epistemic probability p(9 s a) that 9 is lower than or equal to a is given by 
F(a): 
(2) 
F(a) = p(9 s a) 
In the place of "cumulative distribution function", the term "(probability) 
distribution" is frequently used. 
When the range R(9) of 9, i.e., the set of possible values of 9, is a 
countable or finite sequence of values a1 < a2 < ... , the distribution of 9 is 
discrete. This means that F((}) may be based on a probability function p((}) 
which specifies the epistemic probability p(9 = a;) attributed to any possible 
value a; of 9. 
When R(9) is an interval of values - as in the above example (iii) - the 
appropriate distribution F((}) on 9 is continuous.5 The derivative ofF((}) is 
called the (probability) density function, or pdf, of F( (}) and is denoted by 
f{ (}). 
The statistical inferences concerning the value of a 
parameter vector 
9=(91, ... ,9..) - where 91, .•. ,9k represent certain quantities characterizing a 
certain population or process - are called multivariate inferences. 
Consider, for instance, a bivariate parameter vector 9=(91,92). Analogous 
to the univariate case, the uncertainty about 9 can be represented by a joint 
cdf F((}1,0z). Given a value (a 1,az) of (91,9z), the epistemic probability p(91 s 
a1 & 92 s az) that 9 belongs to the 'bidimensional interval' (91 s a1, 92 s az) 
is denoted by F(a 1,az): 
The distinction between discrete and continuous distributions can also be 
made in the bivariate case.6 In particular, a bivariate distribution F((}1,(}z) is 
said to be of continuous type if it is continuous and has a second-order, 
mixed partial derivative function f{ (}1,0z) = iPF/iJ(}1 ()(}2 from which F( (}1,(}z) can 
be recovered by integration. This derivative will be referred to as the joint 
probability density function - or joint pdf- j{(}1,(}z) on (91,9z). 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
25 
From the joint cdf F(01,(J;) on 0=(01,02) it is possible to determine the 
marginal distributions, or marginal cdf's, F(01) and F(O;) on 01 and 02, and 
the conditional distribution F(OtfO;) expressing the probability distribution on 
01 given the value 02 of 02. In particular, from the joint pdf 1{01,0;) on a 
parameter vector (01,0~, it is possible to determine the marginal pdf's, 1{01) 
and j{O;), and the conditional pdf j{OtfO;). 
A possible feature of a cdf F(01,0;) on (01,0~ is the so-called independence 
which is defined as follows: 
In particular, given a continuous parameter vector (01,0~, definition ( 4) is 
equivalent to: 
Put simply, (5) means that 01 and 02 are independent iff the prior density 
function on 01 cannot be modified by any subsequent information about 02. 
All the above definitions can be adapted to a multivariate cdf F(01, ••• ,(J;) on 
a parameter vector 0=(01, ... ,0J with k > 2. In particular, the notion of 
(mutual) independence of 01, ... ,0k, relative to F(01, ••• ,0;) can be defined as 
follows: 
(6) 
01, ... ,0k are (mutually) independent iff 
F(01, ••• ,0J = F(01)F(O;)··-F(O;). 
Lastly, the independence of two vectors 0=(01, ... ,0J and '=( , 1, ••• ,,.,) is 
defined by the following condition: 
where F(01, ••• ,0k,~ 1 , ••• ,~h) is the joint cdf on the combined vector (O,t) = 
(01, ... ,ok,f1, ... ,+J. 
Typical values of a probability distribution 
A probability distribution can be described using certain 'typical values' such 
as mean, variance and moments of various orders which give a general idea 
of its shape. 

26 
CHAPTER 3 
The mean, or expected value, £(9) of a cdf F(Q) on a parameter 9 is 
defined as follows: 
(8) 
£(9) = f _: (} dF( Q) 
where J:(} dF(Q) is the Stieltjes integral of(} with respect to F(Q). In the case 
of a continuous distribution with pdf j((}), definition (8) is equivalent to: 
(9) 
£(9) = r: (} j((}) d(} 
For certain purposes one might wish to calculate the mean of certain 
parameters 'related' to 9. For instance, given a pdf j((}) on 9, it would be 
possible to calculate the mean E(g) of the parameter g = g(9) - where 
g: R1 -+ R1 is a continuous function. One could derive the pdf f(g) on g from 
j((}) and, subsequently, apply (9). However a simpler procedure can be used; 
indeed it can be proved that the value of E(g) is given by the following 
equality: 
(10) 
E(g) = r: g((}) j((}) d(} 
A similar formula can be applied in the case of a pdf j((}) = j((}1, ••• ,(}w) on 
9=(91, ... ,9w)· It can be proved that the mean E(g) of the parameter g = g(9) 
= g(91, ••. ,9w) - where g: Rw -+ R1 is a continuous function - is given by: 
( 11) 
E(g) = f 
Rw g( Q)f( Q) d(} 
where, following La Valle (1970, p. 170), the convention "d(} = d(}1 • ••• ·d(}w" 
is used. 
Given a positive integer r, the rth moment fl;(9) and the rth central moment 
#rC9) of 9 w.r.t. F(Q) are defined as the expected values of 9'" and [9- £(9)]', 
respectively: 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
27 
(12) (a) ,u;(9) = £(9'") 
(b) ,u,{9) = £(((9 - £(9)]') 
The variance var(9) of 9 w.r.t. F(Q) is defined as the second central moment 
of 9, namely, as the mean of [9 - E(9)f: 
(13) 
var(9) = £([(9 - £(9)]2) 
Given a multivariate cdf F(Q1, .. ,(}k) on 9=(91, ... ,9..), the marginal mean £(91) 
and the marginal variance var(91 ) of 91 are defined as the mean and the 
variance of the marginal cdf F(Q;). The k-tuple £(9)=(£(91), ... ,£(9..)) may be 
called mean vector of F((}1, .. ,{}J. 
The covariance cov(91 ,9J) of 91 and 9J, w.r.t. F(Q1, .. ,{}J, is defined as 
follows: 
As Lindgren (1976, p. 135) points out, cov(91 ,9J) "purports to measure 
'covariation' - to be indicative of the degree to which the variables are 
concordant". A defect of cov(91 ,9J) is that "it is sensitive to the scale of 
measurement adopted" (ibid.) in measuring 91 and 9J . A measure of 
covariation that does not have this defect is the correlation coefficient 
p(91 ,9J ), defined as follows: 
(15) 
The above definitions can also be applied to define the typical values of 
conditional distributions. So, for instance, £(91 e) would denote the mean of 
the conditional cdf F((}fe) where "e" denotes a given empirical evidence. 
Finally, notice the properties of typical values as given below. 
(16)(a) E(a9 + b) = a£(9) + b (where a and bare real numbers) 
(b) E(~ 9J = ~ E(9J 
Given the parameter vector 9=(91, ... ,9w), consider the parameter g = g(9) = 
g(91, ... ,9w) where g: Rw --+ R1 is a continuous function. Then, if 9=(91, ... ,9w) 
is partitioned into 98=(91, ... ,9.J and 9b=(9m+l• ... ,9w), formula (11) can be 
rewritten as follows: 

28 
CHAPTER 3 
(17) 
E[g(Oa,Ob)] = J 
R"'Rw-"'g((}a,(}b)j{(}a,{}b) d(}ad(}b 
Given a pdf j{01, ••• ,0w) on 0, the conditional pdf j{OJOJ can be derived. 
Moreover, given a value (}a of Oa, the expectation of the function g(Oa, ·) of 
Ob w .r.t. the conditional cdf j{ OJO J can be defined. This expectation, which 
will be referred to as the conditional expectation of g(Oa,Ob) with respect to 
Ob given (}a, is defined as follows (cf. La Valle, 1970, p. 171): 
(18) 
E(g(Oa,Ob) I OJ = J 
Rw-m g((}a,(}b)j{OJOa) d(}b 
It can be proved that the value of E[g(08,0b)] in (17) is equal to the expected 
(w.r.t. the marginal pdfj{OJ) value of E(g(Oa,Ob) I OJ (cf. La Valle, ibid.): 
It should be noticed that (19) does not hold only in the case of a pdf 
j{01, ... ,0w) but for any cdf F(01, ... ,0w) (cf. Jeffrey, 1971, p. 197). 
Some important properties of variance are listed below: 
(20) (a) 
var(O) 0!: 0 (where var(O) = 0 iff the whole probability is 
concentrated on a single value of 0) 
(b) 
var(O) = E(O~ - [E(OW 
(c) 
var(aO +b) = a\Tar(O) 
From (20)(a) and (20)(b) it follows that: 
(21) 
£(02) 0!: [£(0)] 2 (where E(O~ = [£(0)]2 iff var(O) = 0) 
Lastly, the following properties of covariance and correlation should be noted: 
(22) (a) 
(b) 
(c) 
cov(01 ,OJ) = cov(OJ ,01) 
cov(01 ,OJ) = E(010J) - £(01 )E(OJ) 
If 01 and OJ are independent, cov(01 ,OJ) = 0. 
It follows from (15) and (22)(c) that: 
(23) 
If 01 and OJ are independent, p(01 ,OJ) = 0. 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
29 
Properties (16)-(23) hold for any cdf and, hence, also for the typical values 
E(Q I e), var(91l e), cov(91 ,QJ I e), etc. of any conditional distribution F(Q/e) on 
the parameter vector Q. 
A sampling distribution for a (multivariate) Bernoulli process 
Consider an experiment en consisting of n trials of a (multivariate) Bernoulli 
process Ex with parameter vector q=( q1, ... ,qJ. Any function defined on the 
possible results en of en is called a statistic (for en). Usually a statistic is a 
'descriptive measure' summarizing certain features of the experimental result 
en. 
An important statistic is the number n1 of Q; -outcomes in en. The 
probability distribution Pq (n;) on n1 relative to a given value q of q - called 
the sampling distribution of n1 - can be derived from the probability 
distribution Pq(en) (see formula (2.2)). The mean and the variance of n1 are 
given by: 
(24) 
Eq(n1) = nq; 
(25) 
varq(n1) = nq;(l- q;) 
3.3 BAYESIAN STATISTICAL INFERENCES 
Following a widely used convention, "(probability distribution) p(Q)" will 
denote both probability functions and density functions, whenever a given 
property holds for both. 
Using Bayes's theorem, a prior probability distributionp(Q) on a parameter 
(vector) g can be updated in response to the result y of a certain experiment 
y:7 
(26) 
p(Q!y) = 
p(y/Q)p(Q) 
p(y) 
where p(y), in the case of a continuous p(Q), is given by: 
(27) 
p(y) = J: p(y/Q)p(Q) dQ 

30 
CHAPTER 3 
This formula for the prior predictive distribution p(y) can be generalized to 
a cdf F(lJ) as follows: 
(28) 
p(y) = J: p(y/lJ) dF(lJ) 
Furthermore, given two experiments y and z, one may calculate the so-called 
posterior predictive distribution p(y/z) where the hypothesis y denotes a 
possible outcome of y and the evidence z a possible outcome of z. Suppose 
that, for each value l} of 0, the experiments y and z are conditionally 
independent in the sense that p(y,z/lJ) = p(y!lJ)p(zllJ). Then p(y!z) is given by:8 
(29) 
[i.y/z) = 
J: p(y/lJ)p(z!lJ)p(lJ) dlJ 
p(z) 
~~"' p(y/lJ)p(z/lJ)p(lJ) dlJ 
= -------------------
3.4 PROBABILITY DISTRIBUTIONS FOR MULTINOMIAL 
CONTEXTS 
Although a researcher X normally does not know the exact objective 
probabilities q1, ... ,qk governing a given (multivariate) Bernoulli process Ex 
( cf. Chapter 2.3), he might be able to specify a distribution of epistemic 
probabilities on the possible values of q=(q1, ... ,qJ. Given the relation qk = 
1 - q1 -
··· - ~.1 among the k components of the parameter vector ( ~ •... ,qJ, 
a k-1-variate cdf F( q1, ••• ,qk_1) on the parameter vector ( q1, ... ,qk_ 1) is sufficient 
to represent the researcher's uncertainty about q. For this reason F(q1, ••• ,qk_1) 
will be called prior (probability) distribution on q. In particular, a univariate 
cdf F(q1) on the parameter q1 is sufficient to represent the researcher's 
uncertainty about the probabilities ( q1,qz) governing a given Bernoulli process. 
Note that in a Bayesian multinomial context researchers should use the 
following likelihood functionp(eiq) = p(en/q1, ••• ,qk-U (cf. formula (2.2)): 
k 
(30) 
= II ifi' 
t=l 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
31 
Hence, it follows from formulae (30) and (2.2) that: 
(31) (a) p(Q)q) = qi 
(b) p(QiQJq) = q7 
where "Q/' and "Qi Q/' denote the hypothesis that the outcome of the next 
trial will be Qi and the hypothesis that the outcomes of the next two trials 
will be Qi. 
The marginal cdf's of F(q1, ... ,qk-U are denoted by F(q;) and, in particular, 
the marginal distribution of the 'residual' k-th component <b.. of (q1, ... ,qJ -
which can be derived from F(q1, ... ,qk_1) - is denoted by F(qt). From the k 
marginal cdf' s F( qi) one can derive the vector E( q) of the k marginal means 
E(q1), ... ,E(qJ which will be called the mean vector of F(q1, ... ,qk_1). 
The moments of F(q1, ... ,qk_1) can be interpreted in terms of the predictive 
probabilities attributed to possible sequences of trials of Ex. For instance, it 
follows from (31)(a) and (28) -where in the latter y and ()are replaced with 
Qi and qi, respectively - that the marginal mean E( qJ is equal to p(Q;): 
(32) 
E('IJ) = J: q; dF(q;) = J: p(QJq) dF(q;) = p(Q;) 
This means that E( q1) is identical to the probability p(Qi) that the next trial 
of Ex is Q;. Similarly, it follows from (31)(b) and (28) that the second 
marginal moment E( qi) is equal to the probability p(Qi Qi) that the next two 
trials of Ex will be Qi: 
Analogously, it can be proved that: 
Moreover, a predictive interpretation of var( q1 ) and cov( q1 ,q_, ) can be 
provided. In fact, given (31 )(b) and (32), the value of var( 'II ) = E( qi) -
[E(q1)]2 (cf. (20)(a)) can be expressed by the following formula: 

32 
CHAPTER 3 
(36) 
Consider the cdf F(01, •• ,0J on a given parameter vector 0=(01, ... ,0J. As no 
standard name exists, the sum VAR(O) of the k marginal variances var(O.) will 
be referred to as the total variance of F(01, •• ,0J: 
(37) 
V AR(O) = ~ var(01) 
In particular, the total variance V AR( q) of a cdf F( qw··•qk_1) can be defined 
as the sum of the k marginal variances var( q1): VAR( q) = ~var( q1). It follows 
from (20)( a) that: 
(38) 
V AR(O) :!!: 0 (where VAR(O) = 0 iff the whole probability is 
concentrated on a single value of 0) 
The following predictive interpretation of V AR( q) can be derived from the 
definition of V AR( q) and (35): 
(39) 
VAR(q) = ~ (p(Q;Q;)-
[p(Q;)]~ 
where ~p(Q; Q;) is the probability that the next two trials of Ex belong to the 
same category. 
The above predictive interpretations hold for any cdf and, hence, also for 
the moments E(q1l e), var(q1l e), cov(q1 •% I e) of any conditional cdf 
F(qw .. ,qkje). 
Notice that the ratio between var(q1l e)(= p(Q;Q;Ie)- [p(Q;Ie)Y: cf. (32)) 
and E(q1l e)(= p(Q;Ie): cf. (35)) is given by: 10 
( 40) 
var( q1l e)/E( q1 I e) = p(Q;feQ;) - p(Q;Ie) 
where the quantity on the right side can be interpreted as an 'index uf the 
relevance of Q; to itself (given e). 
Notice too that the ratio between cov(q1 •% I e) (= p(Q;fe)[p(Q)eQ;) -
p(Qj /e)]: cf. (36)) and E( q1l e) is given by: 
( 41) 
cov( 'It •% I e)IE( q1l e) = p(Qj /eQ;) - p(Qj /e) 
where the quantity on the right side can be interpreted as an 'index of the 
relevance of Q; to Q/ (given e). 11 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
33 
Given a value q of q, consider the following two measures: (1) the 
uniformity measure C(q) = ~q7 where "C" is in homage to Carnap who 
proposed ~q7 as a measure of the degree of order of uniformity of a 
population (for more details see Chapter 8.7, note 10); (2) the Gini diversity 
G(q) = 1 - ~q7 (for more details see Chapter 10). W.r.t. these two measures 
the following typical values of a probability distribution F(q1, ... ,qk_1) can be 
considered: 
(i) 
the expected uniformity E[C(q)] and the expected (Gini) diversity 
E[G(q)] of q; 
(ii) 
the uniformity C[E(q)] = I [E('h)]2 and the (Gini) diversity 
G[E(q)] = 1- I [E(q1W of the mean vector E(q)=(E(q1), ••. ,E(qJ). 
It can be proved that the typical values given above and the total variance 
V AR( q) are connected as follows: 12 
(42) 
VAR(q) = E[C(q)]- C[E(q)] = G[E(q)]- E[G(q)] 
Although at first sight one might think that the expected diversity E[G(q)] is 
identical to the diversity G[E( q)] of the mean vector, it follows from ( 42) and 
(38) that these two quantities are usually different from each other and, more 
specifically, that E[G(q)] does not exceed G[E(q)]: 
(43) 
0 s E[G(q)] s G[E(q)] 
Given a certain mean vector E( q) and the corresponding G[E( q)], if VAR( q) 
increases then E[G(q)] decreases so as to approach its minimum value 0, 
whereas if VAR(q) decreases then E[G(q)] increases so as to approach its 
maximum value G[E(q)]. 
An intuitive illustration of this can be provided by considering two 
univariate pdf's f{q 1) and f'(q 1) on the binomial parameter q=(q1,qz) of a 
given Bernoulli process Ex, where E(q1) = E'(q1) = 1/2 and var(q1) >> 
var'(q1) (see Figure 2). From E(q1) = E'(q1) = 1/2 it follows that G[E(q)] = 
G[E '( q)] = 1/2, where 1/2 is the maximum value of Gini diversity for a 
binomial parameter. Moreover, given the equality var( qz) = var( q1) - which 
always holds for a binomial parameter q=(q1,qz)- it follows from var(q1) >> 
var'(q1) that VAR(q) = 2var(q1) » 2var'(q1) = VAR'(q). Hence, VAR(q) » 
VAR'(q). Given (43) this implies that, notwithstanding the equality G[E(q)] 
= G[E'(q)] = 1/2, the expected diversity E[G(q)] is much lower than the 
expected diversity E'[G(q)]. 

34 
CHAPTER 3 
Indeed, looking at Figure 2 one sees that the expected diversity E[G(q)] is 
much smaller than 1/2, since a substantial proportion of the whole probability 
is concentrated on the two regions near the extremes of the interval [0,1 ], 
which are characterized by a very low degree of diversity. On the contrary, 
E'[G(q)] approaches the maximum value 1/2 (= G[E'(q)]) since a substantial 
proportion of the whole probability is concentrated on the region around the 
centre, which is characterized by a very high degree of diversity. 13 
4 
3 
2 
0 
4 
3 
2 
1/2 
0 
1/2 
Figure 2 
E( qJ = E '( qJ = 1/2 and var( q1) » var' ( q1) 
Hence: G[E(q)] = G[E'(q)] = 1/2 and E[G(q)] « E'[G(q)] 
A predictive interpretation of E[(C(q)] and E[G(q)] can be given thus:14 
(44) (a) E[C(q)] = I p(Q;Q;) 
(b) E[G(q)] = 1 -I p(QiQ;) 
where };p(Qi Q;) (1 - };p(Qi Q;)) represents the probability that the next two 
trials of Ex belong to the same category (different categories). 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
35 
3.5 BAYESIAN MULTINOMIAL INFERENCES 
The Bayesian statistical inferences made in a multinomial context where a 
given multivariate Bernoulli process Ex is investigated can be referred to as 
Bayesian multinomial inferences. 
The prior probability distribution p(qw··•qk_1) on the parameter vector q of 
a multivariate Bernoulli process Ex may be updated on the basis of (the 
complete description ot) a given sequence e. of n trials of Ex ( cf. Chapter 
2.2) using the following formulation of Bayes's theorem (cf. (26)): 
(45) 
p( e. !ql,. .. ,qk-1)p( ql, ... ,qk-1) 
p(en) 
Recalling that the likelihood function p( e.lq1, ... ,qk_1) is given by (30), formula 
(45) can be rewritten as follows: 
k n if'/ p(ql, ... ,qk-1) 
i=l 
(46) 
It follows from (27) that, given a continuous p(q1, ... ,qk_1), the value of p(e.) 
in ( 46) is given by: 
Using (28) formula ( 47) can be generalized as follows to an arbitrary cdf 
F(q): 
(48) 
Consider the case where a researcher, after performing an experiment eo 
consisting of n trials of Ex, intends to perform a similar experiment em by 
performing m further trials of Ex. Then given a prior p(q1, ... ,qk_1) on q the 
following predictive distribution p( em! e.) may be derived from (29) by 
replacing y and z with em and e.: 

36 
CHAPTER 3 
Formula (49) can be generalized to an arbitrary prior cdf F(q1, ••• ,qk_1), by 
replacing ''p(qv···•qk_;)dq" with the integral "dF(q1, ••• ,qk_;)": 
It follows from (50) and (30) that: 
(51) 
p(e,/e,J = 
Notice that the denominator of (51) represents the prior predictive probability 
p(e,J (cf. formula (48)). 
The special value, or singular predictive probability, p(Qje.J is simply a 
particular case of (51): 
J1 !1 ... t rt:' ch.' ... rt;•+1 ••• q: dF(q 
q ) 
0 
0 
J 0 
1 
2 
i 
k 
1•···· k-1 
Given that an inductive method (for Ex) is virtually identical to a prior 
predictive distribution on Ex or, equivalently, to a given class of special 
values (see Chapter 2.1), it follows from (48) and (52) that any cdf 
F(q1, ••• ,qk_1) on the parameter (vector) q of a (multivariate) Bernoulli process 

BAYESIAN STATISTICS AND MULTINOMIAL INFERENCES 
37 
Ex determines a unique inductive method for Ex. For instance, the uniform 
distribution on the parameter q1 - which attributes equal density f{q1) = 1 to 
all the values of q1 
- determines Laplace's rule of succession, i.e., the 
inductive method characterized by the special values: p(Qi len) 
= 
(ni + 1)/(n + 2). 15 
Lastly, it can be proved that: 
(53) 
The inductive method I derived from F(q1, ... ,qk_1) satisfies: 
(a) the requirement of exchangeability (Exc); 
(b) the requirement of non-negative relevance (NNR). 16 

CHAPTER 4 
BAYESIAN POINT ESTIMATION, VERISIMILITUDE, 
AND IMMODESTY 
The principal objective of this chapter is to prove that epistemic probability 
distributions satisfy the so-called property of immodesty which plays a very 
important role in the analysis of EPO given herein ( cf. Chapter 8.6). 
In Section 1 the mean estimator, which takes the mean (vector) of the 
posterior probability distribution as the point estimate of the parameter 
(vector) 9, is considered and a verisimilitude interpretation of this estimator 
is given. 
With reference to this interpretation the notion of immodesty is illustrated 
and the immodesty of epistemic probability distributions is proved (Section 
2). In addition some implications of this property relative to the probability 
distributions on the parameter (vector) q of a (multivariate) Bernoulli process 
are considered. 
Lastly, in Section 3 a brief outline of the basic issues of the verisimilitude 
theory is given. 
4.1 BAYESIAN POINT ESTIMATION AND VERISIMILITUDE 
A point estimator is an inductive procedure used for making point estimates 
of the unknown value of a given quantity, on the basis of certain possible 
experimental data. More precisely, given a parameter (vector) 9 and an 
experiment y, a point estimator T: y -
R(9) is a real-valued function which 
assigns to each possible result y of y a point estimate T(91 y) E R(9) - where 
R(9) is the range of the possible values of 9. 
A commonly used Bayesian point estimator -which will be referred to as 
mean estimator or M-estimator E - takes the mean (vector) £(91 y) of the 
posterior probability distribution p(Oiy) as the point estimate of 9. 
As shown below the M-estimator E may be seen as a particular application 
of the Bayes's principle which recommends the action which minimizes the 
expected loss. A loss function L(a,O) denotes the loss resulting from the 
38 

BAYESIAN POINT ESTIMATION, VERISIMIUTUDE, AND IMMODESTY 
39 
adoption of the point estimate a when Q = (). In particular, given a parameter 
Q, the quadratic loss Lz(a,()) is defined as follows: 1 
(1) 
Lz(a,()) = ((} - a)2 
Similarly, given the estimate a=(a1, ... ,aJ of a parameter vector 0=(01, ... ,0J, 
the quadratic loss Lz(a,(}) is defined thus: 
Some standard results concerning the expected (w.r.t. p((}fy)) quadratic loss 
E(Lz(a,Q) I y) of the estimate a of a parameter (vector) Q are given by the 
following formulae (3)-( 4): 
(3) 
Given a parameter Q, E(Lz(a,Q) I y) = var(Q I y) + (E(Q I y) - af 
Given a parameter vector Q, 
E(Lz(a,Q) I y) = VAR(Q I y) + l: (£(01 I y) - a; )2• 
As an immediate consequence of (3): 
( 4) 
E(Lz(a,Q) I y) is minimized by a = E(Q I y). 
This means that the M-estimator E is the appropriate strategy for minimizing 
the expected quadratic loss. 
In the statistical literature the losses employed in the 'justification' of 
M-estimators and other Bayesian estimators are usually described as pecuniary 
or, more generally, pragmatic. However, in the epistemological verisimilitude 
theory a different view concerning the nature of such losses has recently been 
expounded. According to this view, such losses should be interpreted - at least 
in pure science - as cognitive losses representing the 'distance of an estimate 
a from the truth (}' .2 Accordingly, M-estimators should be considered as the 
appropriate strategies for minimizing the expected quadratic distance (of our 
estimates) from the truth (the true value of Q). The interpretation of quadratic 
losses as 'quadratic distances' will be emphasized by putting Dz(a,(}) in place 
of Lz(a,(}). 3 
It is often argued that "the uncertainty about the quantity of interest ... 
might be measured by the variance" (Winkler, 1972, p. 155). The 
verisimilitude interpretation of quadratic losses (Lz(a,(}) = Dz(a,(})) allows a 
more precise formulation of this claim. Indeed, given this interpretation, it 

40 
CHAPTER 4 
follows from (3) that the variance var(9 I y) of the posterior probability 
distribution p(O!y) on a given parameter 9 is equal to the expected quadratic 
distance of the expected value £(9 I y)) from the true value of 9: var(9 I y) = 
E(DiE(9 I y),9) I y). Analogously, as far as the total variance V AR(9 I y) of the 
posterior probability distribution p(Oiy) on a parameter vector 9 is concerned, 
it follows from (3) that V AR(9 I y) = E(Dz(E(9 I y),9) I y). Obviously these 
equalities also apply to the case where y is a tautological evidence. In 
particular, the following equality concerning the total variance of a given 
prior distribution p( 0) on a parameter vector 9 will be used later in this book: 
(5) 
VAR(9) = E[Dz(£(9),9)] 
4.2 THE PRINCIPLE OF IMMODESTY 
Immodest inductive methods and the related requirement of immodesty are 
informally illustrated by Lewis (1971, p. 54) as follows: 
Inductive methods can be used to estimate the accuracies of inductive methods. Call a method 
immodest if it estimates that it is at least as accurate as any of its rivals. It would be 
unreasonable to adopt any but an immodest method. 
After observing that an inductive method C is used to make estimates of 
certain magnitudes, where such estimates are equated to the expected values 
of those magnitudes calculated on the basis of C, Lewis (ibid., pp. 54-55) 
explains the concept of immodest inductive method as follows: 
You should hope to give your trust to an inductive method C that will give you accurate 
estimates ... 
But you cannot just pick the most accurate method - not unless you already know the 
actual values of the magnitude you wish to estimate in which case you do not need to 
estimate them. The best you can do is pick up the inductive method with the highest 
estimated accuracy, just as you might bet on the horse with the highest estimated speed. 
The trouble is that you need an inductive method to estimate anything, even to estimate 
the accuracy of various inductive methods. And your selection of a method with the highest 
estimated accuracy will come out differently depending on which method you use to make 
the estimate. It is as if Consumer Reports, Consumer Bulletin, etc., each published rankings 
of the consumers' magazines, as they do of other products. You would have to know which 
one to read in order to fmd out which one to read. Let us say that an inductive method C 
recommends an inductive method C' if the C-mean estimate of the accuracy of C' is not 
exceeded by the C-mean estimate of the accuracy of any rival method. An inductive method 
might or might not recommend itself. If it does, let us call it immodest. When asked which 
method has the best estimated accuracy, the immodest method answers: "I have". 

BAYESIAN POINT ESTIMATION, VERISIMIUTUDE, AND IMMODESTY 
41 
Lewis (ibid., p. 56) argues most convincingly that immodesty is a necessary 
requirement for adopting an inductive method: 
.... would non-immodesty give you any good reason not to trust an inductive method? Indeed 
it would. Suppose you did trust some non-immodest method. By defmition, it estimates some 
competing method to be more accurate than itself. So if you really did trust your original 
method, you should take its advice and transfer your trust to one of the competing methods 
it recommends. It is as if Consumer Bulletin were to advise you that Consumer Reports was 
a best buy whereas Consumer Bulletin itself was not acceptable; you could not possibly trust 
Consumer Bulletin completely thereafter.4 
On the other hand, Lewis (1971, p. 56) recognizes that "immodesty is a 
necessary but not sufficient condition of adequacy for inductive methods". 
Accordingly, if a given class of inductive methods includes several immodest 
ones, then they "must compete among themselves on other grounds". 
Regarding Carnap's inductive methods (1952) -which are a particular class 
of exchangeable inductive methods (cf. Chapter 1.3 and 6.1) - Lewis 
mistakenly believes that the requirement of immodesty can restrict the choice 
to a uniquely admissible immodest method.5 In the next subsection it will be 
shown - contrary to Lewis's assumption - that any exchangeable inductive 
method is immodest. Therefore immodesty as such cannot impose any 
genuine restriction on exchangeable inductive methods and, in particular, it 
cannot impose any restriction on Carnap's methods. On the other hand, it will 
be seen that immodesty plays an important - although more indirect - role in 
the present contextual approach to the epistemological problem of optimality 
( cf. Chapter 8.6). 
Immodest probability distributions 
Suppose that a researcher investigating a given parameter (vector) Q intends 
to perform an experiment y. Remember that, for any possible result y of y the 
expected (w .r.t. the posterior probability distribution p( 0/y)) quadratic distance 
E(Dz(a,Q) I y) is minimized by a= E(Q I y) (see (4)). In other words, for any 
possible result y of y, the M-estimator E is the appropriate strategy for 
minimizing the expected quadratic distance from the truth (the true 
value of Q). 
Now consider the moment where the experiment y has still to be performed 
and, therefore, the posterior distribution p(O!y) has not yet been obtained. 
Then the researcher has at his disposal only a given prior probability 
distribution p( 0) on Q, a given likelihood function p(y/0), and the derived joint 
distribution p( O,y) ( = p( O)p(y/0)). Now a prior distribution p( 0) is said to 

42 
CHAPTER 4 
satisfy the principle of immodesty if, for any experiment y and any likelihood 
function p(y/Q), the expected (w.r.t. p(Q,y)) quadratic distance between the 
M-estimates of 9 made by the p(Q)-based M-estimator E and the truth is not 
higher than the quadratic distance between the estimates of 9 made by any 
other estimator and the truth. In order to prove that any prior distribution on 
9 is immodest, a more rigorous definition of immodest prior distribution is 
needed. 
Given the result y of an experiment y, let £(91 y) and T(91 y) be the point 
estimates of 9 made by the M-estimator E and an arbitrary point estimator T. 
Moreover, given the statistics £(91 y) and T(91 y), let E[DiE(91 y),9)] denote 
the expected (w.r.t. p(Q,y)) distance between £(91 y) and 9, and 
E[DiT(91 y),9)] the expected (w.r.t. p(Q,y)) distance between T(91 y) and 9. 
Immodest priors can now be defined as follows: 
(6) 
A prior distribution p((}) on the parameter (vector) 9 is immodest 
iff, for any experiment y, any likelihood function p(y/(}) and any 
estimator T: 
E[DiE(91 y),9)] s E[DiT(91 y),9)] 
In other words p(Q) is immodest iff E[DiT(91 y),9)] is minimized by the 
estimator T = E. 
In particular, it should be pointed out that an immodest prior p(Q) is 
'immodest w.r.t. all other prior distributions' p'(Q) on 9 in the sense that an 
immodest prior p(Q) 'estimates' that the estimates of 9 made by the 
p(Q)-based M-estimator E will be no more distant from the truth than the 
estimates of 9 made by any other estimator T, including any other p '( Q)-based 
M-estimator E'. 6 
Before demonstrating the universality of immodesty, the following lemma 
must be proved: 
(7) 
Given a parameter 9, 
E[DiT(91 y),9)] = E[var(91 y)] + E(D2[E(91 y) - 1'(91 y)]); 
Given a parameter vector 9, 
E[DiT(91 y),9)] = E[VAR(91 y)] + E(D2[E(91 y) - 1'(91 y)]). 
where 
(i) E[DiT(91 y),9)] is the expected (w.r.t. p(Q,y)) 
quadratic 
distance between 1'(91 y) and 9; (ii) E[var(91 y)], E[V AR(91 y)] and 

BAYESIAN POINT ESTIMATION, VERISIMIUTUDE, AND IMMODESTY 
43 
E(D2[E(Q I y) - T(Q I y))) are the expected (w.r.t. p(y)) values of var(Q I y), 
VAR(Q I y) and D 2[E(Q I y)- T(Q I y)).7 
Proof" 
Given a parameter 9, let E(DiT(Q I y),Q) I y) be the expected (w .r.t. 
p(()fy)) value of DiT(Q I y),Q). Then the expected (w.r.t. p(y)) value of 
E(DlT(9 I y),Q) I y) will be referred to as E[E(D2(T(9 I y),Q) I y)). It follows 
from formula (3.19) that E[E(D2[T(Q I y),9] I y)) is identical to E[DiT(Q I y),9)]: 
(a) 
E[DiT(Q I y),9)] = E[E(DiT(Q I y),Q) I y)) 
In addition, it follows from (3), by replacing a with T(Q I y), that the expected 
(w.r.t. p(()/y)) quadratic loss E(LiT(Q I y),Q) I y) is given by: 
(b) 
E(LiT(9 I y),Q) I y) = var(Q I y) + [E(Q I y) - T(Q I y))2 
Equality (b) can be restated as follows: 
(c) 
E(DiT(Q I y),Q) I y) = var(Q I y) + D 2[E(Q I y) - T(Q I y)) 
From (c) and (3.16.a) it follows that the expected (w.r.t. p(y)) value 
E[E(DiT(Q I y),Q) I y)) of E(D2(T(Q I y),Q) I y) is: 
(d) 
E[E(DiT(Q I y),Q) I y)) = E(var(Q I y) + D 2[E(9 I y) - T(Q I y)]) 
= E[var(Q I y)) + E(D2[E(9 I y) - T(Q I Y)]) 
The first part of (7) is an immediate consequence of (a) and (d). The second 
part of (7) can be proved similarly, Q.E.D. 
Using (7) it can be proved that: 
(8) 
Any prior distribution p( ()) on a parameter (vector) 9 is immodest. 
Proof" 
Given a prior distribution p(()) on a parameter 9, consider the 
equality E[DlT(Q I y),Q)) =E[var(Q I y)) +E(D2[E(9 I y)- T(Q I y))) in(7). Since 
E[var(Q I y)) is constant and since E(D2[E(Q I y)- T(Q I y))) is necessarily non-
negative and equal to 0 only in the case where, for any y, T(Q I y) = E(Q I y), 
then the expected distance E[DlT(Q I y),9)) is minimized by the estimator T 
= E. This signifies that p(O) is immodest, given the definition (6) of 
immodesty. 

44 
CHAPTER 4 
An analogous proof can be provided for a parameter vector 0, Q.E.D. 
Definition (6) and theorem (8) refer to an arbitrary prior distribution p(Q) 
where p(Q) may be a probability function or a density function (see the first 
lines of Chapter 3.3). However, it is clear that both definition ( 6) and theorem 
(8) can be generalized to apply to any prior cdf F(Q) on 0. In particular, 
theorem (8) can be extended as follows: 
(9) 
Any prior cdf F(Q) on a parameter (vector) 0 is immodest.8 
For our purposes the following particular consequence of (9) should be 
considered: 
(10) 
Any prior cdf F(q1, ... ,qk_1) on the parameter (vector) q of a 
(multivariate) Bernoulli process is immodest. 
Since any exchangeable inductive method is 'equivalent' to a corresponding 
prior cdf F(q1, ... ,qk_1) (see Chapters 1.3 and 5.1), it follows from (10) that: 
(11) 
Any exchangeable inductive method is immodest. 
The theorems (10)-(11) generalize some results obtained by Spielman (1972) 
and Horwich (1982). 9 
Finally, consider the following theorem which will be of fundamental 
importance in the analysis of EPO (see Chapter 8.6). Given two distributions 
F(q1, ... ,qk_1) and F'(q1, ... ,qk_1) on the parameter (vector) q - and a likelihood 
functionp(enfq) -let E and E' be the correspondingM-estimators. Then, given 
an experiment en, where the results are the possible sequences en of n trials 
of Ex, it follows from (6) and (10) that: 
where E[Dz(E(q I en),q)] and E[Dz(E'(q I en),q)] are the expected (w.r.t. the 
joint probability distribution on ( q,en) derived from F( q1, ... ,qk_1) and p( en /q)) 
values of Dz(E(q I en),q) and D 2(E'(q I en),q), respectively. 

BAYESIAN POINT ESTIMATION, VERISIMIUTUDE, AND IMMODESTY 
45 
4.3 THE BASIC ISSUES OF THE VERISIMILITUDE THEORY 
The verisimilitude interpretation of M-estimators (see Section 1) is a small 
part of a methodological programme of research dealing with two related 
problem areas: (1) the logical problem of verisimilitude and (2) the epistemic 
problem of verisimilitude. The first problem area involves questions such as: 
what does it mean to say that a theory is closer to the truth than another or 
that a given theory has a certain degree of verisimilitude? The second 
problem area involves questions such as: how can one rationally conjecture, 
on some evidence, that one theory is closer to the truth than another? how can 
one conjecturally evaluate the degree of verisimilitude of a given theory? 
The first systematic attempt to define a formal concept of verisimilitude, 
or truthlikeness, for scientific theories was made at the beginning of the 
sixties by Popper. A vivid account of the impact of Popper's comparative 
notion of verisimilitude on the epistemological community is provided by 
Kuipers (1987, p. 1): 
When Karl Popper published in 1963 his definition of closer-to-the truth this was an 
important intellectual event, but not a shocking one. Everybody could react by saying that the 
definition was as it should be, and even that it could have been expected. For plausible the 
definition was indeed: a theory is closer to the truth than another one if the first has more true 
and less false consequences than the second. 
About then years later the 1963 event became shocking with retrospective effect when 
David Miller and Pavel Tichy independently proved that a false theory, that is a theory with 
at least one false consequence, could according to Popper's definition never be closer to the 
truth than another one. 
With this proof they demolished the defmition, for it could not do justice to the 
presupposed nature of most of the culminating-points in the history of science: new theories, 
such as Einstein's theory, though presumably false, are more successful than their 
predecessors, such as Newton's theory, just because they are closer to the truth, that is, closer 
to the unknown true theory about the subject matter. 
Miller and Tichy unchained with their proof in the beginning ... mainly sceptical remarks 
like "only the intuitive idea is important, fortunately", "it shows that one can't solve 
philosophical problems by formal means" and last but not least "it is the punishment for 
speaking freely about the truth". 
On the other hand, after some time the realization that Popper's definition 
was not almost obviously correct but (indeed surprisingly) was mistaken, 
stimulated the attention of an increasing number of epistemologists towards 
verisimilitude.10 
Most of them abandoned the consequence approach proposed by Popper 
and explored alternative approaches to the explication of verisimilitude. 11 As 

46 
CHAPTER 4 
a result of almost twenty years on research there are now several formal 
approaches to the logical problem of verisimilitude. 
The most widely supported approach is the so-called similarity approach 
developed by Tichy, Oddie, Tuomela, Niiniluoto and others. The basic idea 
of this approach is that the verisimilitude (distance from the truth) of a 
hypothesis h depends on the similarities between the states of affairs allowed 
by h and the true state of the world. The similarity approach is primarily 
syntactic since - as Kuipers (1987a, p. 2) remarks - it "centers around the 
syntactic similarities and dissimilarities between the relevant potential answers 
to the [cognitive] problem" under consideration where a cognitive problem is 
characterized by a specific language and 'the truth' is identified with the most 
informative true statement in that language. 
A possible alternative to the similarity approach to verisimilitude is the 
semantic approach developed by Miller (1978) and Kuipers (1982, 1987b). 
They start from the assumption of a previously given set of conceptual 
possibilities or, more precisely, of a given set of conceptually possible 
structures, which may or may not be explicitly based on some given language. 
This means that within the semantic approach "the appropriate category for 
defining truthlikeness is not the category of sentences but of (sets of) models 
of sentences or, more generally, structures and sets of structures" (Kuipers, 
1987a, p. 4). For instance, within Kuipers's formulation of the semantic 
approach, a distinction between the so-called descriptive truth and theoretical 
truth is drawn, where the descriptive truth is given by one structure 
representing the actual world, and the theoretical truth is defined as the set 
of structures representing the set of physically possible worlds (or set of 
empirical possibilities). 12 
Different methodological rules have been suggested for the conjectural 
evaluation of the verisimilitude of a given hypothesis - and the conjectural 
comparison of the verisimilitude of two hypotheses - on the ground of the 
available evidence. 
For instance, according to the Bayesian approach to the epistemic problem 
of verisimilitude, an appropriate procedure consists of estimating the 
verisimilitude (distance from the truth) of a hypothesis on evidence by 
calculating its expected degree of verisimilitude (distance from the truth) 
relative to an epistemic probability measure. 13 Within the Bayesian approach 
other procedures for evaluating the verisimilitude of hypotheses can be 
explored. Suppose, for instance, that the maximum verisimilitude is 1. Then 
the evaluation of a given hypothesis h may be based on the "degree of 
probable verisimilitude" of h which is defined as the probability, on a given 

BAYESIAN POINT ESTIMATION, VERISIMIUTUDE, AND IMMODESTY 
47 
evidence e, that the degree of verisimilitude of h exceeds a certain threshold 
1 - e (see Niiniluoto 1987, p. 278; 1989, pp. 237-239). 14 
A limitation of the Bayesian approach "is that it is almost inevitably 
assuming a quantitative truthlikeness" (Kuipers, 1987b, p. 95). With reference 
to this Kuipers (ibid.) maintains that an adequate rule of choice between two 
rival hypotheses "should also, and primarily, deal with the comparative case, 
without necessarily presupposing an explication of quantitative truthlikeness". 
In several papers Kuipers develops a non-Bayesian approach to the epistemic 
problem of verisimilitude where a number of methodological applications of 
his comparative notions of truthlikeness are explored. 15 

PART II 
DE FINETII'S THEOREM, GC-SYSTEMS, AND 
DIRICHLET DISTRIBUTIONS 

CHAPTER 5 
EXCHANGEABLE INDUCTIVE METHODS, BAYESIAN 
STATISTICS, AND CONVERGENCE 
TOWARDS THE TRUTH 
In this chapter the so-called de Finetti's representation theorem (DFRT) is 
used to elucidate some conceptual relationships between TIP and the analysis 
of multinomial inferences in BS. 
In Section 1 some implications of DFRT are noted. For instance, there is 
a one-to-one correspondence between the set of exchangeable inductive 
methods and the set of prior distributions on the parameter (vector) q of a 
(multivariate) Bernoulli process. 
In Section 2 some problems regarding the convergence of opinion in 
scientific research are considered. In particular, it is shown that exchangeable 
inductive methods which satisfy the requirement of adequacy (Reich) allow 
scientists to learn from experience, not only in the 'epistemic sense' that 
scientists using different inductive methods are led to a considerable 
convergence of opinion, but also in the 'realist sense' that the predictive 
probabilities derived from such methods tend, in the long run, to approach the 
truth. 
5.1 DE FINETTI'S REPRESENTATION THEOREM 
At the end of Chapter 3 it was pointed out that any cdf F(q1, ••• ,qk_1) on the 
parameter (vector) q of a (multivariate) Bernoulli process Ex determines a 
unique inductive method for Ex and that, in addition, (cf. (3.53)): 
(1) 
The inductive method I derived from F(qt> ... ,qk_1) satisfies the 
requirement of exchangeability (Exc). 
It may be asked whether the reverse also holds true, i.e., whether an 
exchangeable inductive method I for Ex 'determines' a (unique) probability 
distribution F(q1, ••• ,qk_1). 
51 

52 
CHAPTER 5 
Rather surprisingly a positive answer to this question is provided by DFRT, 
that is: 
(2) 
For any (predictive distribution p1 specified by an) exchangeable 
inductive method I for Ex, 
(a) 
a cdf F1(q 1, ••• ,qk_1) does exist such that, for any sequence en 
of trials of Ex: 
Put simply, DFRT asserts that any exchangeable inductive method I for Ex 
can be derived from, or 'represented' by, one and only one cdf F1(q1, ••• ,qk_1). 
Note that F1(q1, ••• ,qk_1) is, or at least behaves as, a probability distribution on 
the parameter (vector) q of a (multivariate) Bernoulli process.2 
DFRT can be used to clarify the conceptual relationships between TIP and 
the analysis of multinomial inferences made in BS. Note, for instance, that, 
given (1) and (2) then: 
(3) 
There is a one-to-one correspondence, or 'equivalence', between 
the set of exchangeable inductive methods and the set of the 
probability distributions on the parameter (vector) q of a 
(multivariate) Bernoulli process.3 
More specifically, DFRT can be used to prove the equivalence between a 
given class of exchangeable inductive methods and a corresponding class of 
probability distributions on q. An example of this kind is given by the 
equivalence between GC-systcms and Dirichlet distributions which ~an be 
proved by using DFRT (see Chapter 6.3). 
Lastly, DFRT could be applied to identify the conceptual relations existing 
between: 
(i) 
certain predictive requirements of adequacy concerning special 
values and other predictive probabilities as employed in TIP (see 
Chapter 2.1), and 
(ii) 
certain parametric requirements concerning the form of a 
probability distribution on the parameter (vector) q of a 

EXCHANGEABLE INDUCTIVE METHODS AND BAYESIAN STATISTICS 
53 
(multivariate) Bernoulli process as considered in BS (see Chapter 
6.6). 
For instance, DFRT could be used to prove that a given predictive 
requirement A is equivalent to a parametric requirement B, in the sense that 
an exchangeable inductive method satisfies A iff the corresponding probability 
distribution on q satisfies B. 
Consider, for example, the following 'no gap' parametric requirement 
(NG): 
(NG) 
Given the cdf F(q1) on the parameter q1 of a Bernoulli process, 
p(H) > 0 for any non-degenerate interval H in [0,1 ]. 
Put simply, (NG) states that F(q1) has "no 'flat spots' of positive length" 
(Jeffrey, 1984, p. 82). 
Using DFRT it can be proved that the predictive requirement (Reich) (see 
Chapter 2.1) is equivalent to (NG).4 
5.2 CONVERGENCE OF OPINION AND CONVERGENCE 
TOWARDS THE TRUTH 
DFRT does imply that a researcher X using an exchangeable inductive method 
I behaves as if (i) he believed that the process Ex under investigation were a 
(multivariate) Bernoulli process with parameter (vector) q, and (ii) his beliefs 
about the value of q were expressed by the cdf F1(q1, ••• ,qk_1) representing/. 
However, DFRT does not imply that X should believe that Ex is a 
multivariate Bernoulli process governed by a vector q of objective 
probabilities q 1, ••• ,lJJ... 
Regarding this problem de Finetti sees F1(q1, ... ,qk_1) as sheer 'mathematical 
fiction' relative to the 'fictitious entities' q1, ... ,qk. However this standpoint 
originates more from de Finetti's anti-realistic philosophical view that 
physical probabilities do not exist than from the mathematical content of 
DFRT.5 Indeed, if the realist view that physical, or objective, probabilities do 
exist is accepted, there is no problem in assuming that X may believe that Ex 
is a multivariate Bernoulli process governed by a vector q of objective 
probabilities q 1, ••• ,lJJ...6 
Different 
'philosophical 
interpretations' 
of DFRT have 
different 
implications concerning the nature of 'scientific consensus', as will be seen 
below. 

54 
CHAPTER 5 
In almost all disciplines, the scientific community reveals a remarkable 
capacity to attain a considerable degree of consensus on the subjects under 
examination. Two questions need to be tackled: (1) which are the mechanisms 
governing the formation of this consensus? (2) to what extent is this 
consensus rational? 
Many epistemologists and statisticians have dealt with the problems 
concerning the convergence of opinion among researchers who hold different 
initial beliefs. 
For instance, in BS it has been pointed out that "a sufficiently large body 
of experience" can bring "holders of different opinions ... to similar 
opinions", i.e., to similar posterior distributions (Savage, 1972, pp. 68). 
Regarding the issue of rational consensus among different scientists, de 
Finetti, as early as 1937 (p. 118), poses the question: 
Why are we obliged in the majority of problems to evaluate a probability according to the 
observation of a frequency? 
Having remarked that this "is a question of the relations between the 
observation of past frequencies and the prediction of future frequencies" 
(ibid.), de Finetti advances several formulations (at least partially equivalent 
to each other) of "the requirement of induction": 
[the requirement of induction is] ... the requirement according to which the probability ought 
to be close to the observed frequency. (Ibid., p. 120) 
... a rich enough experience leads us always to consider as probable future frequencies ... 
close to those which have been observed. (Ibid., p. 142) 
... in the prediction of a frequency one is generally guided, or at least influenced, by the 
observation of past frequencies. (Ibid., p. 152) 
... the probability of a subsequent trial, relative to the observation of a certain frequency, 
tends to coincide with the value of the latter. (Ibid., p. 153) 
It is clear that the first and fourth formulations can be seen as informal 
anticipations of (Reich). 
According to de Finetti (ibid., p. 120) the requirement of induction is not 
a universally valid principle: 
It is evident that by posing the problem as we have, it will be impossible for us to 
demonstrate the validity of the requirement of induction ... That this requirement can only be 
justified in particular cases is not due to an insufficiency of the method followed ... Indeed, 
probability being purely subjective, nothing obliges us to choose it close to the frequency; all 

EXCHANGEABLE INDUCTIVE METHODS AND BAYESIAN STATISTICS 
55 
that can be shown is that such an evaluation follows in a coherent manner from our initial 
judgment when the latter satisfies certain perfectly clear and natural conditions. 
The above mentioned "perfectly clear and natural conditions" refer to the 
probability axioms and the condition of exchangeability. In fact, de Finetti 
(ibid., pp. 142-147) argues that (normally) exchangeable probabilities do 
satisfy the requirement of induction, i.e., that exchangeable probabilities 
normally approach the observed relative frequencies. Given the convergence 
of exchangeable predictive probabilities towards the observed relative 
frequencies, the convergence of opinions among researchers with different 
initial beliefs follows directly. In this connection, de Finetti (ibid., p. 147) 
points out that: 
It is true that in many cases - as for example on the hypothesis of exchangeability - these 
subjective factors [i.e., different initial opinions] never have too pronounced an influence, 
provided that the experience be rich enough; the circumstance is very important, for it 
explains how in certain conditions more or less close agreement between the predictions of 
different individuals is produced. 
According to de Finetti the process of learning from experience is no more 
than this convergence of predictive probabilities towards the observed relative 
frequencies. This assumption is strictly related to his anti-realistic view 
according to which physical probabilities do not exist and, consequently, there 
can be nothing to learn about them. 
However if one accepts the realist view that physical, or objective, 
probabilities do exist, then a different account of learning from experience can 
be advanced. In fact, it could be argued that the process of learning does not 
consist only in the specific manner in which predictive probabilities are 
modified by experience; on the contrary, it also includes the tendency of 
predictive probabilities to converge towards the true objective probabilities. 
In other words, learning from experience is related to the fact that the 
predictive probabilities derived from a given inductive method tend to 
approach the truth. This signifies that a scientist using that particular 
inductive method tends to learn something about the real world. 
Consider a (multivariate) Bernoulli process Ex with parameter (vector) q. 
Then a realist interpretation of "learning from experience" should include the 
claim that an inductive method I allows us to learn from experience iff, in 
any possible state of nature, i.e., for any value q=(q1, ... ,qk) of q, the predictive 
probabilities p(Qi /en) specified by I tend to converge towards qi in response 
to an increasingly large body of experience. 

56 
CHAPTER 5 
With reference to this, it should be pointed out that any exchangeable 
inductive method I which satisfies (Reich) makes learning from experience 
possible in the sense specified above. This can be proved as follows: 
(a) 
if I satisfies (Reich), the predictive probabilities p(Q; len) derived 
from I approach the empirical frequencies nJn (cf. Chapter 2.1); 
(b) 
for any value q of q, empirical frequencies n; In conv.erge almost 
surely (i.e., with probability one) to the objective probabilities q;, 
as stated by the strong law of large numbers of the probability 
calculus; 
(c) 
hence, if I satisfies (Reich), then, for any value q of q, the 
predictive probabilities p(Qi len) derived from I converge almost 
surely to the objective probabilities q;. 
This signifies that an exchangeable inductive I method which satisfies (Reich) 
allows learning from experience not only in de Finetti's anti-realist sense but 
also in. the realist sense that the predictive probabilities derived from I tend, 
in the long run, to approach the truth.7 However, adopting (Reich) is of 
negligible help to a scientist who has to select an exchangeable inductive 
method since (Reich) is satisfied by 'almost all' exchangeable inductive 
methods. 
It may be asked whether a scientist's concern with approaching the truth 
in the short run - i.e., on the basis of 'short' sequences of trials of Ex - can 
affect his choice of the inductive method. This question will be dealt with in 
Chapter 8.5 where the assumption that researchers attempt to maximize their 
possibilities of approaching the truth in the short run will be used to develop 
a 'verisimilitude approach' in selecting the optimum inductive method. 

CHAPTER 6 
GC-SYSTEMS AND DIRICHLET DISTRIBUTIONS 
In this chapter a well known class of inductive methods, i.e., GC -systems, and 
a family of probability distributions widely used in BS, i.e., Dirichlet 
distributions, will be considered. 
The main features of GC-systems and Dirichlet distributions are described 
in Sections 1-2. In Section 3 the relationships between the two are elucidated 
- more specifically it is shown that there is a one-to-one correspondence 
between the class of GC-systems and the class of the prior Dirichlet 
distributions on the parameter (vector) q of a (multivariate) Bernoulli process. 
In Section 4 the 'extreme' GC-systems and the corresponding 'extreme' 
Dirichlet distributions are described. 
Lastly, in Sections 5 and 6 some axiomatizations of GC-systems and 
Dirichlet distributions are considered. 
6.1 GC-SYSTEMS 
Generalized Carnapian systems, or GC-systems, are a class of inductive 
methods proposed by Carnap and Stegmiiller (1959).1 Such methods are 
defined as follows: 
(1) 
A GC-system is an inductive method characterized by the special 
values: 
(n;+y;f..) 
(n + /..) 
where Y; > 0, 'i:.y; = 1, and 0 < f.. < oo. 
An interpretation of the parameter f.. is given after formula (4) below (see also 
Chapter 7, note 18). 
The vector y=(y1, ... ,yk) related to a given GC-system will be called the prior 
vector of that system since it follows from (1), by putting n; = n = 0, that 
GC-systems satisfy the following requirement of initial possibility: 
57 

58 
CHAPTER 6 
(IP) 
p(Q;) = Y; > 0 
Since a given GC-system is fully characterized by a couple (y,"A.) it may be 
referred to as (y,"A.). To emphasize that a special value p(Q;Ie,.) belongs to a 
specific GC-system (y,"A.) it may be denoted by Py,>..(Q;Ie,. ). 
The inductive methods introduced by Camap (1952), which will be referred 
to as Carnapian systems, or C-systems, can be defined as follows: 2 
(2) 
A C-system is an inductive method characterized by the special 
values: 
(n; + "A..k) 
(n +A.) 
where 0 < A. < oo. 
A C-system can be seen as a GC-system with prior vector y equal to 
1/k=(1/k, ... ,1/k). In other words, a C-system is a GC-system which satisfies, 
besides (IP), the stronger requirement ofinitial equipossibility (IE): 
(IE) 
p(Qi) = llk 
GC-systems satisfy all the basic requirements of TIP as illustrated in Chapter 
2: 
(3) 
GC-systems satisfy (Exc), (Reg), (PR) and (Reich). 
From (1) it can be seen that the special values p(Qi /e,. ) of a GC-system 
depend on the empirical evidence e,. only via n and n;. More specifically, 
GC-systems satisfy the following requirement of restricted relevance (RR): 
(RR) 
If e,. and e~ are such that ni = n;, then p(Q;Ie,.) = p(Q;fe~ ). 
The requirement (RR) implies that the predictive probabilities p(Q;fe,.) should 
in no way be affected by the empirical frequencies ni of Qi in the sequence 
e,. - where Qi -:/:. Qi - independently of the possible, more or less strong, 
similarity occurring between the properties Qi and Qi. This signifies that 
'analogy by similarity' is outside the scope of GC-systems.3 
The posterior vector (p(Q 1/e,.), ... ,p(QJe,.)) of a GC-system is obtained by 
a simple combination of the prior information contained in the prior vector 
y=(y1, ... ,y:J and the empirical information contained in the empirical vector 
(n 1/n, ... ,nJn). Indeed, the predictive probabilities p(Q;fe,.) of a GC-system are 

GC-SYSTEMS AND DIRICHLET DISTRffiUTIONS 
59 
a weighted average of the corresponding prior probabilities Y; and empirical 
frequencies nJn, where /J(n+A) is the weight of the prior probability Y; and 
n/(n+A) is the weight of the empirical frequency nJn ( cf. Carnap, 1980, p. 94): 
n 
(4) 
---yi + 
n+A 
n+A 
n 
It can be seen that the weight n/(n+A) of the empirical frequency n; In 
increases, as one would naturally expect, with the cardinality n of the 
empirical sample: this weight is 0 when n = 0 and approaches its maximum 
value 1 when n becomes much higher than A. 
GC-systems can be seen as a natural modification of the so-called straight 
rule p0(Q; len ) = n; In which equates the posterior predictive probability 
straight away to the relative frequency n; In of Q; in en ( cf. Carnap, 1980, p. 
85). In fact the special values p(QJen) occurring in (1) may be obtained from 
p 0(Q; /en) by augmenting "the n real individuals with A virtual individuals, 
among which Ay; have the property Q; for each i = 1, ... ,k. Then the value of 
p(Q; len) ... is simply the relative frequency of Q; among the n + A real and 
virtual individuals" (Jeffrey, 1980, pp. 2-3). 
Let p(Q; /Qi) be the predictive probability of Q; relative to the empirical 
evidence given by the observation of a single Qi -outcome. Then, for any 
couple of distinct attribute indices i, j, the relevance quotient 11;; is defined as 
follows: 
(5) 
11ij = 
The relevance quotient 11ii represents the proportion of the prior probability 
p(QJ of Q; which 'survives' after the observation of the first 'negative 
example' Qi. An important property of GC-systems is the so-called 11-equality 
(Carnap, 1980, p. 57): 
(6) 
11ij = 11 = 
(1 + A) 
where 0 < 11 < 1. 

60 
CHAPTER 6 
6.2 DIRICHLET DISTRIBUTIONS 
Beta distributions 
A Beta distribution Beta( a 1,a:z) on a parameter q1, where 0 s ~ s 1, is 
characterized by a pdf Beta(q1;a1,a:z) defined as follows: 
(7) 
r(al + a:J 
r(al) r(a:J 
r(a1+a:z)/[r(aJr(a:z)] is simply a 'normalizing factor' employed to render the 
total probability on q1 equal to 1. (Here r(x) denotes the so-called gamma 
function - or generalized factorial function - which is defined for all positive 
real numbers x and has the value of an ordinary factorial for any integer 
number n: r(n) = (n - 1)!.) 
The mean and the variance of Beta( a 1,a:z) are given by: 
(8) 
E(qJ = 
(9) 
Beta priors are conjugate priors for the parameter~ of a Bernoulli process. 
Essentially this means that if the prior distribution on q1 is a Beta then the 
posterior distribution, relative to en, is also a Beta. More specifically: 
From (8) and (10) it follows that, given the prior Beta( a 1,a:J, the posterior 
expectation E( q1 I en) is given by: 
(11) 
A Beta distribution Beta(a1,a:z) with parameters a 1 = a 2 = a* is called a 
symmetrical Beta distribution since its mean E(qJ is 1/2 (cf. (8)). In 
particular: (i) when a* < 1, aU-shaped beta distribution Beta(a*,a*) with 
very high variance is obtained; (ii) when a* = 1, the beta distribution 
Beta(1,1) identical to the so-called uniform distribution on q1 is obtained;5 

4 
3 
2 
0 
GC-SYSTEMS AND DIRICHLET DISTRffiUTIONS 
Beta(q1; tl*, a*) 
a*= V4 
.... 
Figure 1 
Density functions of symmetrical Beta distributions 
61 
(iii) when a* >> 1, a beta distribution Beta(a*,a*) with small variance is 
obtained (see Figure 1: cf. Winkler, 1972, p. 151). 
A Beta distribution Beta( a 1,a:z) with parameters a 1 f. a 2 is called an 
asymmetrical Beta distribution. Some of the shapes which may be assumed 
by the probability densities of asymmetrical Beta distributions are shown in 
Figure 2 (cf. Winkler, ibid., pp. 151-152) 
The above observations demonstrate that the Beta family is very "rich" in 
the sense that it includes "distributions with different locations, dispersions, 
shapes .... so as to be able to represent a wide variety of states of prior 
information" (Winkler, ibid., p. 148). 
Dirichlet distributions 
The multivariate analogues of Beta distributions for parameter vectors 
q=( q1, ... ,qJ are the so-called Dirichlet distributions Dir( aw .. ,ak). The pdf 
Dir( q1, ••• ,qk_1;a1, ••• ,a0 of a k-1-variate Dirichlet distribution is defined as 

62 
CHAPTER 6 
Beta (q1; a i' a 2) 
20 
15 
a,= 28. 5, a 2= 1. 5 
10 
5 
0 
.75 
q, 
a 1 > a 2 
Figure 2 
Probability densities of asymmetrical Beta distributions 
follows: 
(12) 
r(a) 
q';'-1 ... q~·-1 
The constant r(a)/[f(a1) 
••• f(a.J] is again a normalizing factor rendering 
the total probability equal to 1. 
Notice that the marginal distributions of a Dirichlet distribution are Beta 
distributions: 
(13) 
If j{q1, •.. ,qk_1) = Dir(q1, ... ,qk_1;a1, ... ,ak) then J{q;) = Beta(q;;JJJJ0 
where jJ1 = a; and jJ2 = a - a;. 
The marginal means of Dir( a~> ... ,a.J are given by: 

GC-SYSTEMS AND DIRICHLET DISTRffiUTIONS 
63 
Other typical values of Dirichlet distributions are given by the following 
formulae: 
(15) 
(16) 
(17) 
(18) 
E(qi) 
a;(a -a;) 
= 
a(a + 1) 
var(q1) 
a;(a- a;) 
= a 2(a + 1) 
E(q,qJ) 
a;aj 
= 
a(a + 1) 
aiaj 
cov( ~ ,qJ ) = - _a_z_( a-+-'---1 )-
As a consequence of (16), the value of VAR(q) = ~var(q1 ) (cf. (3.37)) is 
given by: 
(19) 
It follows from the general property (3.32) of the cdf's F(q1, ... ,qk_1) that the 
prior predictive probability p(Q; ) derived from a Dirichlet distribution 
Dir( a 1, ... ,aJ is identical to its marginal mean E( q1 ). Here, E( q1) ( = p(Q; )) 
will be denoted by the shorter Y; and the mean vector E(q)=(E(q1), ... , E(qJ) 
by y=(yl>' .. ,yk). These symbols have already been used to denote the prior 
probabilities of GC-systems (see Section 1). The vector y=(y1, ... ,yJ will hence 
be referred to as the prior vector of Dir( a 1, ... ,ak). 
Given that a;= aE('It) = ay; (cf. (14)), formulae (15), (16), (18) and (19) 
can be rewritten as follows: 
(20) 

64 
(21) 
var('lt) = 
(22) 
cov( 'It ,qJ ) = 
(23) 
VAR(q) = 
CHAPTER 6 
Yi (1 - y;) 
a+1 
YiYj 
a+1 
1 - l: Y7 
a+1 
G(y) 
= a+1 
The value of E[G(q)] = 1 - m(qD is derived from (20): 
a 
(24) 
E[G(q)] = 
G(y) 
a+1 
where G(y) (= 1 - ~y/) is the diversity of the prior vector y. 
Remember that the expected diversity E[G(q)] of any cdf F(q1, ••• ,qk_1) is 
included in the interval [O,G(y)] (see (3.43)). In particular, regarding Dirichlet 
distributions Dir( a 1, ••• ,a~, it follows from (24) and from the inequalities 0 < 
a < oo, that: 
(25) 
E[G(q)] is included in the interval [O,G(y)]. 
More specifically: when a- 0 then E[G(q)] -
0 and when 
a -
oo then E[G(q)] -
G(y). 
Analogous to the Beta distributions, the Dirichlet distributions are conjugate 
distributions for the parameter vector q=( q1, ••• ,qJ of a multivariate Bernoulli 
process. More specifically: 
(26) 
If /(q1, ••• ,qk_1) = Dir(q1, ••• ,qk_1;a1, ••• ,a~, 
thenf(q1, ••• ,qk-tfen) = Dir(q1, ••• ,qk_1;a1+n1, ••• ,...:k+n~. 
From (26), (13) and (11) it follows that the posterior marginal mean 
E( 'It I en ) of Dir( a 1, ••• ,a~ is given by: 
(27) 
a+n 

GC-SYSTEMS AND DIRICHLET DISTRffiUTIONS 
65 
A Dirichlet distribution Dir( a 1, •.• ,av with parameters a 1 = ··· = ak = a*, 
where a = ka*, is called a symmetrical Dirichlet distribution since the 
marginal means E( q1) are equal to 1/k (see (14)). The pdf of a symmetrical 
Dirichlet distribution Dir(a*, ... ,a*) has the following form (cf. Good, 1965, 
p. 25): 
(28) 
f(ka*) 
a 0·l 
a 0·l 
[f(a*}t ql 
... qk 
In particular, when a* = 1, the Dirichlet distribution Dir(1, ... ,1) is identical 
to the so-called uniform distribution on the parameter vector q.7 
6.3 THE EQUIVALENCE BETWEEN GC-SYSTEMS 
AND DIRICHLET DISTRIBUTIONS 
Given that E(<L I en)= p(QJen) (see (3.32)) and a;= ay; (see (14)), it follows 
from (27) that the special values p(QJen) derived from Dir( a 1, ... ,av are given 
by: 
(29) 
a+n 
It follows from (29) and from the definition (1) of GC-systems that: 
(30) 
Given the prior Dir( a 1, ... ,av, the derived set of predictive 
probabilities p(Q; len) is a GC-system (y,a) where y=(y1, ••• ,yv = 
( atfa, ... ,aJa). 
As a direct consequence of (30): 
(31) 
The GC-system (y,J...) can be derived from the corresponding prior 
Dir(y1J..., ... ,y~). 
Given that GC-systems are exchangeable inductive methods, it follows from 
(31) and from the uniqueness clause of DFRT (see (5.2)), that: 

66 
(32) 
CHAPTER 6 
The GC-system (y,A.) can be derived from a unique prior 
distribution on the parameter vector q where this prior is 
Dir(y1A, •.. ,y.J..). 
It follows from (30) and (32) that: 
(33) 
There is a .one-to-one correspondence between the class of 
GC-systems (y,A.) and the class of the prior Dirichlet Dir( a 1, ... ,aJ 
on the parameter vector q of a multivariate Bernoulli process. The 
parameters are related as follows: A.= a= l:a; andY;= aja. 
Two corollaries of (33) are: (i) there is a one-to-one correspondence between 
the class of C-systems (1/k,A.) and the class of the symmetrical Dirichlet 
distributions Dir(A./k, ... ,A./k); (ii) there is a one-to-one correspondence between 
the class of GC-systems (y,A.) with y=(y1,y;) and the class of the Beta 
distributions Beta(y1A.,y2A.) on the parameter q1 of a Bernoulli process.8 
6.4 EXTREME GC-SYSTEMS AND EXTREME DIRICHLET 
DISTRIBUTIONS 
Consider the set of GC-systems (y,A.) with a given prior vector y. Then the 
inductive methods (y,O) and (y,oo) are characterized by the special values 
P1,0(Qjen) and p1,..,(Qjen) which are defined thus ( cf. Carnap, 1980, p. 95): 
(34) 
(35) 
It follows from (34) and (1) that (y,O) is almost identical to the above 
mentioned straight rule Po since, for any n =1:. 0, Pr.0(QJen) = n;ln.9 Since the 
special values p1,0(Q; /en ) are equated to the corresponding empirical 
frequencies and no weight is given to prior probabilities, (y,O) can be seen as 
an extreme inductive method characterized by an extreme propensity to learn 
from experience.10 
As far as (y,oo) is concerned, it follows from (35) and (1) that p1,co(Q;Ie") 
= Y;· Since the special values p1,co(Q;Ien) are equal to the corresponding prior 
probabilities Y; and no weight is given to the empirical frequencies, (y,oo) may 

GC-SYSTEMS AND DIRICHlET DISTRffiUTIONS 
67 
be seen as a non-inductive method since whoever adopts (y,oo) "refuses to 
learn anything from experience" (Camap, ibid., p. 95). 
Although, strictly speaking, (y,O) and (y,oo) are not GC-systems (cf. (1)), 
they may be considered as extreme GC-systems (y,f..), where f.. = 0 
and f..= oo. 
It can be shown that (y,O) and (y,oo) are, in a certain sense, equivalent to 
two types of prior distributions on the parameter vector q which could be 
called extreme Dirichlet distributions. 
For this purpose it should be recalled that an improper distribution is one 
which assigns infinite 'probability' to the whole range of a given parameter 
(vector) but finite probability to any compact (i.e., bounded and closed) subset 
of it. It has been argued that improper distributions may be used as priors in 
Bayesian inferences given that the corresponding posterior distributions 
derived using Bayes's theorem are proper (cf. Novick and Hall, 1965). 
Consider, for instance, the distribution characterized by the density: 
k 
(36) 
n q""/ 
i=1 
It can be proved that this distribution is improper since the integral of II q;1 
on the range of q diverges ( cf. Good, 1965, p. 28). 
Since II q;1 has the algebraic form of the density of a Dirichlet distribution 
(see (12)) - except for the values a; = 0 of the parameters - it may be seen 
as an improper Dirichlet distribution and referred to as Dir(O, ... ,O). It can be 
proved that: 
(37) 
If f{q 1, ••• ,qk_1) = Dir(qw .. ,qk_1;0, ... ,0), 
then f{q 1, •• .,qkjen) = Dir(q1, ••• ,qk_1;n1, ••• ,nk), 
where Dir(q1, ••• ,qk_1;n1, ••• ,nk) is proper, provided that all n; (i = 1, ... ,k) are 
positive. 
From (37), (13) and (14) it follows that the posterior marginal means 
E( 'L I en) ( = p(Q; /en)) of Dir(O, ... ,O) are equal to n; ln. Therefore Dir(O, ... ,O) 
is virtually equivalent to the extreme GC-system (y,O). 
Lastly, consider a 'degenerate' prior F(q1, ••• ,qk_1) attributing the whole 
probability to a single value (y1, ••• ,yk-1) of the parameter vector q. In this case 
the prior and posterior marginal means and, accordingly, the prior and 
posterior predictive probabilities p(Q; /en ) derived from F(q1, ••• ,qk_1) are 
identical to Y;· Hence, this distribution is equivalent to the extreme GC-system 
(y,oo). The prior cdf F(qw .. ,qk_1) may be called a degenerate Dirichlet 

68 
CHAPTER 6 
distribution, since it can be seen as the limit of a class of Dirichlet priors 
Dir( a 1, ... ,aJ whose parameters a; tend to oo, while the ratios Y; = a; /a remain 
unchanged. In fact, when a 1, ... ,ak and, hence, a tend to oo, the marginal 
variances of Dir(a1, ... ,aJ tend to zero (cf. (21)) and the distribution 
concentrates almost entirely on the prior vector (y1, ... ,yk_1). Consequently 
Dir( a 1, ... ,aJ becomes virtually indistinguishable from the particular 
F(q1, ... ,qk_1) we started with. 
6.5 THE AXIOMATIZATION OF GC-SYSTEMS 
Axiomatizing a given class of inductive methods is equal to proving that the 
class includes all and only those methods which satisfy a given set of axioms. 
In particular, C- and GC-systems have been axiomatized by several authors, 
e.g. Carnap and Stegmiiller (1959), Kemeny (1964), Carnap (1980, § 19), and 
Kuipers (1978, Ch. 5). Their results may be summarized as follows: 
(38) 
An inductive method for a family (Q1, ... ,Qk), where k > 2, is a 
GC-system iff it satisfies the requirements of exchangeability 
(Exc), of restricted relevance (RR), and of initial possibility 
(IP)Y 
They have also proved that, when k > 2, an inductive method is a C-system 
iff it satisfies (Exc), (RR) and the requirement of initial equipossibility 
(IE).l2 
Whilst it is very simple to prove that all GC-systems satisfy (Exc), (RR) 
and (IP) (see Section 1), proving that these requirements are satisfied only by 
GC-systems is anything but easy. This proof is essentially based on the 
following lemma: 
(39) 
Given the axioms of probability calculus and (Exc), fork> 2 the 
requirement of restricted relevance (RR) is equivalent to the 
following requirement of linearity: 
(RL) p(Qjen) is a linear function of n;. More precisely, there are 
constants a; > 0 and b such that p(QJen) = a; + bn;. 
Unfortunately, this strategy cannot be applied when k = 2 since in this case 
(RR) and (RL) are not equivalentY For this reason, where k = 2, the 
pre-mentioned authors directly include (RL) among the axioms: 

GC-SYSTEMS AND DIRICHLET DISTRffiUTIONS 
69 
( 40) 
An inductive method for a family (Q1,Q:J is a GC-system iff it 
satisfies (Exc), (RL), and (IP). 
They have also proved that, where k = 2, an inductive method is a C-system 
iff it satisfies (Exc), (RL) and (IE). 
In place of this 'double' axiomatization, "it would be desirable" - Carnap 
(1980, p. 101) remarks - "to have a single assumption applicable to any k C!: 
2, one that would yield the same results as our present two assumptions 
together". 14 
As far as I know, the only available 'unified axiomatization' of GC-systems 
was proposed by Costantini (1979) and employs a "single assumption" called 
the condition of invariance of the relevance quotient, or (CIRQ). 
Given the sequence of trials en , and the sequence en Qi which is obtained 
by adding a Qi-outcome to en, Costantini (ibid., p. 151) defines the relevance 
quotient Rij(en) for Qi and Qj w.r.t. en thus: 
(41) 
p(Q)enQ) 
p(Q)en) 
The relevance quotient Rij (en) represents the proportion of the predictive 
probability p(Qi /en) which 'survives' after the observation of a 'negative 
example' Qj. The relevance quotient can be seen as a generalization of 
Carnap's relevance quotient lJij 
since it follows from (5) that lJij = 
p(Qi !Qj )lp(Qi) = Rij (e0), where e0 denotes an 'empty sequence', or 
tautological evidence. 
Now (CIRQ) is a natural generalization of the 11-equality lJij = 11 (cf. (6)): 
(CIRQ) Rij (en) - where i f. j - depends only on n. 
(CIRQ) demands that Rij(en) should depend neither on the particular sequence 
en nor on the particular couple Qi, Qj (where i f. J) but exclusively on the 
'length' n of e/5 
The axiomatization of GC-systems proposed by Costantini (ibid., pp. 154-
156) may be summarized as follows: 16 
( 42) 
An inductive method for a family (Q1, ... ,QJ, where k C!: 2, is a 
GC-system iff it satisfies (Exc), (CIRQ), and (IP). 

70 
CHAPTER 6 
6.6 THE AXIOMATIZATION OF DIRICHLET DISTRIBUTIONS 
Axiomatizing a given class of probability distributions for a given parameter 
(vector) is equal to proving that the class includes all and only those 
distributions which satisfy a given set of parametric requirements used as 
axioms (cf. Chapter 5.1). 
In particular, certain parametric requirements which use the concept of 
neutrality, as introduced by Connor and Mosimann (1969, pp. 194-199), are 
applied to axiomatize Dirichlet distributions. 
Put simply, given the proportions q1, ... ,qk - which are defined as non-
negative parameters satisfying the constraint 
"~q1 = 1" - the proportion q1 
is said to be neutral w.r.t. q=(q1, ... ,qJ when no information on the value of 
q1 can influence our beliefs concerning "the manner in which the remaining 
proportions q 2, ••• ,qkproportionally divide the remainder of the unit interval" 
(Connor and Mosimann, ibid., p. 196).17 
A precise formulation of the concept of neutrality is given by Fabius 
(1973, p. 583) who defines the notion of (CM)i-neutral vector as follows: 
( 43) 
Given a cdf F(q1, ... ,qk_1), the vector q=( q1, ... ,qJ is (CM);-neutral 
iff the fractions lJ.J /(1 - <L ), where j f. i, are independent of <L· 
Recall that <J.J /(1 - q,) is independent of q,, w.r.t. a cdf F(qw .. ,qk_1) on q, iff 
F(q/(1 - q;),q;) = F(qi/(1- qi))F(q;) (cf. definition (3.4)). 
The following (parametric) requirement of neutrality (CM) on a cdf 
F(q1,. .. ,qk_1) can be stated by using the concept of (CM)i -neutrality: 
(CM) 
Given a cdf F(q1, ... ,qk_1), the vector q=(q1, ... ,qJ is (CM)i -neutral 
for i = 1, ... ,k. 
The intuitive content of (CM) is akin to that of the predictive requirement 
(RR) used to axiomatize GC-systems. In fact, as seen above, (RR) eJ~. ~ludes 
any 'analogy by similarity' since it requires that the predictive probabilities 
p(Qi /en) should in no way be affected by the empirical frequencies ni of Qi 
in en - where Qi f. Qi - independently of the possible, more or less strong, 
similarity occurring between the properties Qi and Qi. Now, in a certain sense, 
(CM) also excludes any 'analogy by similarity' since it requires that our 
beliefs about q1 should in no way be affected by any information concerning 
the value of the fraction q/(1 - <tt) in the remainder (1 - qJ of the unit. 
interval; this independently of the possible, more or less strong, similarity 
occurring between the properties Qi and Qi corresponding to q1 and qJ. 

GC-SYSTEMS AND DIRICHLET DISTRffiUTIONS 
71 
The parametric requirement (CM) forms the basis of Fabius's 
axiomatization (1973) of Dirichlet distributions (ibid.) which can be stated as 
follows: 18 
( 44) 
A cdf F(q1, ••• ,qk_1) on q=( q1, ... ,q.J is a Dirichlet distribution iff it 
satisfies (CM). 
The formulation of Fabius's theorem given in (44) is somewhat imprecise. 
Fabius (ibid., p. 583), indeed, proves that a probability distribution on q 
satisfies (CM) iff "the distribution ... is a Dirichlet distribution or a limit of 
Dirichlet distribution" which "is just a short way of saying that the 
distribution ... is either a Dirichlet distribution, or discrete and concentrated 
in the vertices of the simplex in which [ q] takes its values, or degenerate" 
(ibid., p. 584). 
This axiomatization of Dirichlet distributions does not apply when k = 2, 
where the parametric principle (CM) is trivially true. As far as I know a 
unified axiomatic characterization of Beta and Dirichlet distribution on the 
basis of appropriate parametric requirements has still to be found. 

PART III 
VERISIMILITUDE, DISORDER, AND OPTIMUM 
PRIOR PROBABILITIES 

CHAPTER 7 
THE CHOICE OF PRIOR PROBABILITIES: THE 
SUBJECTIVE, APRIORISTIC, AND CONTEXTUAL 
APPROACHES 
In Bayesian methodology the posterior probabilities which a scientist 
attributes to the hypotheses under investigation depend on his prior 
information (prior beliefs) and the new empirical information obtained from 
a given experiment (cf. Chapter 3.1). Given a sufficiently large body of 
experimental data, different scientists with different prior beliefs are led to 
attribute similar posterior probabilities (cf. Chapter 5.2). Conversely, when the 
available experimental evidence is limited, then the posterior probabilities of 
different scientists will depend to a large extent on their prior beliefs, i.e., on 
their prior probabilities. In such cases, the degree of consensus about posterior 
probabilities arrived at by the scientific community depends almost entirely 
on the corresponding degree of consensus in selecting prior probabilities. 
In this chapter three competing approaches to the problem of the choice of 
priors probabilities are considered. 
In Sections 1-4 the two 'traditional' approaches to this problem, i.e., the 
subjective approach and the aprioristic approach, are considered. In Section 
1 and 2 these approaches are considered w.r.t. the choice of priors in BS. In 
Sections 3 and 4 the subjective and the aprioristic interpretations of TIP are 
examined. 
In Section 5 a 'new' contextual approach to the selection of prior 
probabilities is presented. 
Finally, in Section 6 a context-dependent justification of the decision to use 
a prior Dirichlet distribution or, equivalently, a GC-system, within a given 
inquiry is proposed. 
75 

76 
CHAPTER 7 
7.1 THE CHOICE OF PRIORS IN BAYESIAN STATISTICS: 
THE SUBJECTIVE APPROACH 
According to the subjective approach to Bayesian inference the probability 
axioms are the only principles of rationality governing the selection of prior 
probabilities in Bayesian inferences. Subjective Bayesians recognize that some 
'descriptive principles' capture important features of the beliefs of (almost) 
all researchers engaged in certain types of empirical researches. For instance, 
Savage (1972, pp. 63-67) points out the role of symmetry in a number of 
situations where the probability judgments of different individuals typically 
tend to agree. However, subjective Bayesians reject the idea that epistemic 
probabilities are ruled by other principles of rationality - or normative 
principles - besides the probability axioms. 
A good example of this view is given by de Finetti's interpretation of 
(Exc). De Finetti does not consider (Exc) as a normative principle subject to 
rational justification. Although he claims that his "hypothesis of 
exchangeability" offers a convincing "justification" - or "explanation" - of the 
"principle of induction" (cf. Chapter 5.2), he is very clear about the "purely 
psychological" character of this justification (de Finetti, 1937, p. 152). Indeed, 
de Finetti's (ibid.) objective is to show that: 
... there are rather profound psychological reasons which make the exact or approximate 
agreement that is observed between the opinions of different individuals very natural, but that 
there are no reasons, rationa~ positive, or metaphysical, that can give this fact any meaning 
beyond that of a simple agreement of subjective opinions. 
Since the only principles of rationality admitted by subjective Bayesians are 
the probability axioms, in their view there is no place for the problem of the 
'rational choice' of a prior distribution. As Winkler (1967, p. 777) remarks: 
The personalistic view [i.e., the subjective approach] differs from other approaches by not 
attempting to specify what assessments are 'correct'. All self-consistent, or coherent 
assessments are admissible as long as the individual feels that they correspond with his 
judgments. 
That means that in subjective Bayesianism the problem of the choice of the 
priors is identical to the problem of selecting the prior distribution best fitting 
the prior beliefs of the researcher. 
Several techniques have been developed to "bring to light" the actual 
beliefs of a given person by using a suitable ''verbal" or "behavioral 

THE CHOICE OF PRIOR PROBABILITIES 
77 
interrogation" (Savage, 1972, pp. 27-29). With reference to this, Winkler 
(ibid., p. 778) says that: 
What is needed is ... an interviewing procedure, with which one can "interview" the assessor 
and elicit enough information to be able to write down a prior distribution which accurately 
reflects the assessor's prior knowledge and judgments. It must be stressed that the assessor 
has no built-in prior distribution which is there for the taking. Rather, the assessor has certain 
prior knowledge which is not easy to express quantitatively without careful thought. An 
elicitation technique used by the statistician does not elicit a "true" prior distribution, but in 
a sense helps to draw out an assessment of a prior distribution from the prior knowledge. 
Different techniques may produce different distributions because the method of questioning 
may have some effect on the way the problem is viewed. 
An efficient "interviewing procedure" should assess the prior distribution of 
a given subject with the minimal amount of (self-)interrogation.1 
Suppose that the Beta prior Beta(a1,az) best fitting a researcher X's prior 
beliefs about the parameter q1 of a Bernoulli process Ex has to be assessed. 
For this purpose the values of the mean and the variance of q1 could be 
assessed and, subsequently, the unique Beta with these two values could be 
determined using formulae (6.8) and (6.9).2 
A similar strategy could be used where the Dirichlet prior Dir( a 1, ••• ,aJ best 
fitting X's prior beliefs about the parameter vector q = (q1, ... ,qJ of a 
multivariate Bernoulli process Ex has to be assessed. We could first try to 
specify the prior vector y=(y1, •.• ,yJ by assessing X's expectations £( q1) = y;; 
secondly, we could assess X's expectation E[G(q)] of Gini diversity G(q); 
thirdly, the value of a can be found using the following formula derived from 
(6.24): 
(1) 
a = 
E[G(q)] 
G(y) - E[G(q)] 
where G(y) is the Gini diversity of the prior vector y. 
Lastly, the values of a 1, ... ,ak are derived from a and y=(y1, •.• ,yJ by the 
equalities a;= ay; (cf. 6.14). 
In some cases X might find it too difficult to specify E[G(q)], whereas he 
might find it easier to specify his expected distance E(D2(y,q)] between his 
prior vector y and the true value of q. Therefore, given the equality E(D2(y,q)] 
= VAR(q) = G(y)/(a+1) (see formulae (4.5) and (6.23)), the value of a can 
be found using the following formula: 

78 
(2) 
a = 
CHAPTER 7 
G(y) - E[Dz(y,q)] 
E[Dz(y,q)] 
The above interviewing procedures can be called direct methods since the 
assessment of a prior distribution is based on the direct specification of a 
sufficient number of its typical values (mean, variance, etc.). However, often 
direct methods fail since, as Winkler (1980, p. 95) remarks: 
... it is often quite difficult for experts, even those with a good background in probability and 
statistics, to think about the model parameters and to quantify their prior information in terms 
of a probability distribution for these parameters. 
In such cases Winkler (ibid.) suggests employing 
... an indirect method whereby the expert assesses probability distributions for statistics based 
on samples from the process, with no mention being made of parameters. These distributions, 
which are called predictive distributions, involve observable variables instead of unobservable 
parameters. Hence, they should relate more directly to the expert's knowledge and experience 
and should be easier for the expert to think about than are prior distributions of parameters. 
[Author's italics]. 
For example, consider the prior predictive probability p( tn) of the occurrence 
of tn Q1-outcomes in a sequence of n trials of a given Bernoulli process Ex, 
where 0 s tn s n. It is known that the parameters a 1, a 2 of the Beta prior 
Beta( a 1,a2) can be determined by fixing - for two different couples (n,tn) -
the predictive probabilitiesp(tn) derived from Beta(a1,az) (cf. Winkler, 1972, 
pp. 206-208). If X specifies more than two predictive probabilities p(tn), then 
the result would as a rule be 'overdetermined': in other words, there would 
be no Beta( a 1,az) which corresponds perfectly to X's predictive probabilities. 
However, it would be erratic to discard any part (which part?) of the available 
information about X's prior opinions, especially when X has specified several 
predictive probabilities. The following appears to be a wiser strategy tc adopt: 
(i) 
ascertain whether there are any Beta distributions where the 
derived predictive probabilities display a sufficient 'proximity' to 
the set of X's predictive probabilities, 
(ii) 
if such distributions exist, select the one which provides the best 
fit (cf. Winkler, 1972, pp. 190-191; 1980, pp. 102-103). 
The subjective approach can be seen as a sort of 'anarchic Bayesianism' 
where every prior goes. To reject the subjective approach it must be shown 

THE CHOICE OF PRIOR PROBABIUTIES 
79 
that, at least in certain situations, some degree of rational consensus on prior 
probabilities is attainable by applying appropriate normative principles (to be 
added to the probability axioms). A radical formulation of this claim is given 
in the aprioristic approach to BS. 
7.2 THE CHOICE OF PRIORS IN BAYESIAN STATISTICS: 
THE APRIORISTIC APPROACH 
The case of complete ignorance 
The form of subjective Bayesianism as described above might be called 
radical subjectivism, given its refusal of any principle of rationality other than 
the probability axioms. This view may be seen as one of the two poles of a 
continuum of possible attitudes concerning the choice of priors, where the 
other pole - which may be called radical objectivism - is the view that, for 
any scientific problem, there is always a unique admissible prior. Between 
radical subjectivism and radical objectivism one finds different intermediate 
'degrees of objectivism' depending on the extent to which the accepted 
principles of rationality restrict the range of allowed freedom in the choice of 
prior distributions. 
Although objective Bayesianism is a very old view going back to at least 
Laplace, it has been revitalized over the last fifty years by the works of H. 
Jeffreys and E. T. Jaynes. As Seidenfeld (1979a, p. 416) points out, 
... objectivism (as defended by Jeffreys and Jaynes) ... argues for a uniquely admissible 
probability function ... given a knowledge state K. The probability function is objective 
because it is agent-invariant once the knowledge base K is fixed. Of course, objective 
Bayesianism requires extra inductive postulates (besides the probability axioms] to identify 
the uniquely admissible probability function. 
While Jeffreys and Jaynes have tried to identify the uniquely admissible prior 
distribution for any kind of "knowledge state", the first supporters of 
objective Bayesianism - beginning with Bayes and Laplace - dealt only with 
states of complete ignorance. 
As is known Bayes and Laplace thought that uniform priors were the 
uniquely admissible priors in the case where somebody is completely ignorant 
about the value of a parameter. Laplace justified this claim by his famous 
principle of insufficient reason (PIR) which may be stated as follows 
(Jeffreys, 1961, p. 33): 

80 
CHAPTER 7 
If there is no reason to believe one hypothesis rather than another, the probabilities are equal. 
... to say that the probabilities are equal is a precise way to say that we have no ground for 
choosing between the alternatives. 
A very common criticism of (PIR) is that very often the application of (PIR) 
to different 'partitions' of the same set of alternatives leads to mutually 
inconsistent probability assignments (cf. Seidenfeld, 1979b, pp. 11-12). The 
most striking cases of inconsistency are given by the so-called paradoxes of 
re-parametrization where a uniform prior on a continuous parameter 9 
determines non-uniform priors on certain parameters, such as 92, log 9, 1/9 
derived from the re-parametrization of 9.3 This signifies that a uniform 
distribution on 9 is incompatible, for instance, with a uniform distribution on 
'= 92. 
Now, assuming that a state of complete ignorance is represented by 
uniform priors - as (PIR) requires - we are then led to the conclusion that a 
person completely ignorant of the value of 9 is not completely ignorant of the 
value off = 92 (and vice versa). Therefore one might be completely ignorant 
about the length of a square but quite well informed about its area. This 
'arbitrariness of parametrization' conflicts with the presystematic intuition 
according to which the 'ignorance distributions' representing a complete 
ignorance of a certain parameter should be invariant - in an appropriately 
defined sense of the term - relative to certain 'admissible' transformations of 
the parameter under investigation. 
With the rejection of (PIR) new procedures for the formal representation 
of complete ignorance were designed to escape the paradoxes of re-
parametrization. For instance, an important objective of Jeffreys's research 
program is to define ignorance distributions (or, equivalently: informationless 
distributions, indifference distributions) such that certain 'invariance 
properties' are satisfied.4 According to Jeffreys (1961, p. 119): 
If we have no information relevant to the actual value of a parameter, the probability must 
be chosen so as to express the fact that we have none. It must say nothing about the value 
of the parameter, except the bare fact that it may possibly, by its very nature, be restricted 
to lie within definite limits. 
Jeffreys's proposal is based on the assumption that the selected ignorance 
prior should respect certain invariance properties depending on the 
mathematical form of 9. For instance, according to Jeffreys (1961, p. 117): 
(3) (a) If -oo < 9 < oo then the ignorance prior is the (improper) uniform 
prior. 

THE CHOICE OF PRIOR PROBABIUTIES 
(b) 
If 0 s 9 < oo then the ignorance prior is the (improper) prior 
proportional to 1/(). 
81 
The adoption of these priors is prompted by their invariance over important 
families of re-parametrizations. In particular, the priors in (a) and (b) are 
supported by the following invariance properties, respectively: 
(a)' 
Given a parameter 9, where -oo < 9 < oo, the uniform prior on 9 
leads to priors of the same kind on any parameter obtained through 
a linear transformation of 9. 
(b)' 
Given a parameter 9, where 0 s 9 < oo, the prior proportional to 
1/() leads to priors of the same kind on any parameter 9', 
with t -f. 0. 
It follows from (a)' and (b)' that, within a given group of 'admissible' 
transformations of the parameter 9, the ignorance prior on one of the 
members of the group never leads to a non-ignorance prior on some of the 
other members. Hence, Jeffreys's approach succeeds in avoiding the 
paradoxes of re-parametrization arising in relation to (PIR). 
On the other hand, serious criticisms have also been made against 
Jeffreys's standpoint. For instance, Barnett (1974, p. 179) asks: 
Why should we use a concept of linear invariance when [R(Q) = (-oo,oo)], and power-law 
invariance when [R(Q) = (O,oo)]? 
In general, it seems problematic to justify the choice of the specific group of 
transformations w.r.t. which the invariance of ignorance priors is requested. 
An analogous point is made by Fine (1973, p. 171): 
A source of difficulty with the invariance approach ... is the necessity for the identification 
of the transformation groups that properly characterize our incomplete prior knowledge .... 
On the one hand, a cautious selection of ... [the group of transformations] may not lead to 
a unique prior distribution ... On the other hand, it is often the case that the use of invariance 
so overdetermines the prior distribution that there does not exist an invariant distribution. 
Apart from the mathematical problems relating to the under- and over-
determination of priors, a more substantial question concerns the 'naturalness' 
of the criteria to be used for selecting the group of admissible 
transformations.5 Invariance properties, indeed, are nothing but 'formal 
requirements of consistency' which are imposed on ignorance priors. In 

82 
CHAPTER 7 
themselves they do not supply a formal representation of the intuitive concept 
of complete ignorance. As Huzurbazar (1980, p. 446) remarks: 
... mere logical consistency, though essential, is not enough. It is also desirable that an 
invariance rule should lead to results agreeing with common sense.6 
Although many 'indifference procedures' have been proposed in the statistical 
literature, 7 a satisfying explication of the notion of complete ignorance 
presumably has still to be found ( cf. Barnett, 1974, p. 180). 
Jeffreys (1948) himself, not being completely satisfied with his earliest 
invariance theory, proposed a new theory which is called the theory of 
Invariants. 8 As Seidenfeld (1979a, p. 418) remarks: 
The innovation was to consider not just the quantity of interest, m, but also to consider the 
statistical distribution of the observable random variable which is to provide the information 
about m. That is, lo specify an 'ignorance' distribution about m one must take into account 
the statistical model for the data which are to be the evidence acquired. 
The theory of Invariants is much more powerful than Jeffreys's previous 
invariance theory and, in particular, allows identification of the ignorance 
prior for different kinds of parameters with a finite range of values. For 
instance, in the case of the parameter vector q of a multivariate Bernoulli 
process, the informationless prior derived from the theory of Invariants is the 
Dirichlet distribution Dir(l/2, ... ,1/2) (cf. Good, 1965, pp. 28-29). 
However, it is not clear whether the new theory is more intuitively 
plausible than the old one. For instance, Lindley (1972, p. 71) remarks that: 
A ... serious objection ... lies in the fact that the ignorance prior depends on the likelihood 
function. ... Why should one's knowledge, or ignorance, of a quantity depend on the 
experiment being used to determine it?9 
The case of partial information: the maximum entropy approach 
Although scientists are almost never in a state of complete ignorance, the 
identification of informationless priors for epistemic states of complete 
ignorance is important for other reasons. In fact, even if the actual epistemic 
state of a researcher includes much 'private information' about the parameter 
to be investigated, he may decide to report his experimental findings by using 
an indifference prior representing a supposed state of complete ignorance. 
When used in this function a prior is called reference prior ( cf. Box and Tiao, 
1973, p. 22). 10 

TilE CHOICE OF PRIOR PROBABIUTIES 
83 
Having recognized the methodological importance of the case of complete 
ignorance, one has to recall that typically the researcher's initial cognitive 
state concerning the parameter 9 of a given process Ex is one of partial 
information. More specifically, his background knowledge BKwould include 
some prior information I 'concerning' 9 in Ex. For instance, BK may include 
some theoretical information about certain relations occurring between 9 and 
other parameters characterizing Ex, or empirical information about the value 
of 9 in processes 'sufficiently similar' to Ex. 11 
Objective Bayesians assume that the above mentioned prior information I 
should not be neglected when selecting the prior distribution on 9. In 
particular Jaynes makes the stronger assumption that, starting from I, one can 
determine - by using suitable principles of rationality - a uniquely admissible 
'objective prior' cdf F*(O) on 9 which will be referred to as the partial 
information distribution - or the PI-distribution - on 9, relative to I. 
According to Jaynes (1984, pp. 167-168): 
The direct problem is: Given some verbal statement /, which says something relevant to Q 
without uniquely fixing it, translate I into a prior probability assignment f(Q). 
Jaynes's analysis of this problem may be illustrated by the following example. 
Consider a discrete parameter 9 where the possible values are a1, ••• ,a1• Given 
a discrete probability distribution p=(p1, ••• ,p1) on 9 - where Pi = p(ai) - then 
the entropy H(p) of p is defined as follows: 
( 4) 
H(p) = - 1: Pi log(p;) 
The entropy H(p) introduced by Shannon (1948), is commonly interpreted as 
a measure of the uncertainty - or lack of information - of a subject whose 
probability distribution on 9 is p. This interpretation of H(p) is also accepted 
by Jaynes, who suggests the following principle of maximum entropy (PME) 
for selecting the objective prior distribution p*: 
(PME) (a) Identify the set PI of all probability distributions on 9 
'consistent' with the prior information I; 
(b) then select within PI the maximum entropy distribution p* 
maximizing H(p*). 
Subsequently, p* will be referred to as the MAXENT distribution (cf. 
Seidenfeld, 1986), and "MAXENT' will denote the forms of objective 
Bayesianism using (PME) or other 'entropic principles'. 

84 
CHAPTER 7 
Some explicative remarks about the 'consistency clause' (a) of (PME) are 
necessary. In many applications of MAXENT, the partial information I 
includes a number of 'prior constraints' c1, ... ,cm given by the expected values 
of certain functions II(Q), ... fm(Q) of Q (cf. Jaynes, 1979, pp. 45-47 and 
Rosenkrantz, 1977, pp. 55-56). Prior constraints of this sort - such as mean, 
variance and other moments of Q - will be referred to as moment constraints 
or m-constraints. However, many other kinds of distributional constraints on 
p( 0) can be used: in principle one can employ any constraint leading to some 
restriction on the set of possible prior distributions on Q ( cf. Fine, 1973, p. 
172). One might distinguish between (a) 'propositional constraints', which are 
expressed by a certain proposition about the value of the investigated 
parameter Q as, for instance: "The value of 9 is lower than 2", and (b) 
'probabilistic constraints' consisting in the specification of some features -
in general typical values as moments, fractiles, etc. - of the prior probability 
distribution on Q as, for instance: "There is at least a 90% probability that Q 
is less than 2" and "The mean value of Q is 2" ( cf. Jaynes, ibid., pp. 44-45 
and Seidenfeld, 1986, p. 471). 
An attractive feature of MAXENT is the existence of a general solution to 
the problem of finding the MAXENT distribution in cases of this kind ( cf. 
Jaynes, ibid. pp. 46-47). 
A concept of entropy analogous to ( 4) can be defined also for a pdf f{ 0) on 
a continuous parameter 8: 
(5) 
H[f{O)] = -J f{O) log[j{O)] dO 
Unfortunately, if applied to a continuous parameter 8, (PME) gives rise to 
inconsistencies not that different from the paradoxes of re-parametrization 
generated by (PIR), in the sense that the maximally entropic pdf f*(O) for Q 
is typically inconsistent with the maximally entropic distributions for other 
parameters + obtained by certain 'smooth' transformations of Q ( cf. 
Seidenfeld, 1979a, p. 424; 1986, p. 475). 
The solution suggested within MAXENT employs the notion of cross 
entropy (divergence), also known as Kullback-information (Kullback, 1959). 
In the continuous case the cross entropy /K(/2/ 1) between two pdf's fiO) and 
f1(0) is defined as follows: 
(6) 
/K(/2,/1) = J h(O) log[(fl0)/f1(0)] d(} 

THE CHOICE OF PRIOR PROBABIUTIES 
85 
IJ/2, f1) is commonly interpreted as a measure of the distance - or 
dissimilarity - between [iQ) and ft(Q) since, in one sense, it represents the 
'change of belief' involved in shifting from ft(Q) to fz(Q). 12 
Let fo( (}) be the informationless prior on Q derived from an appropriate 
invariance theory. Then, within MAXENT the PI-distribution[*((}) is selected 
by applying the rule of minimization of the change of belief (MCB) which can 
be stated as follows: 
(MCB) (a) Identify the set f1 of all pdf's j((}) on Q 'consistent' with the 
prior information I; 
(b) select within f1 the pdf f*( (}) which minimizes the cross entropy 
divergence IK(f*,f0). 
According to (MCB) the 'new' cognitive state represented by j'*(Q) should be 
- compatibly with I - 'maximally similar' 
to the 'old' cognitive state 
represented by f0(Q)Y 
A positive feature of (MCB) concerns its application to discrete parameters. 
In fact it can be proved that, if one assumes, as is usually the case, that the 
ignorance distribution p0 for a discrete parameter Q is the uniform distribution, 
then the PI-distribution p* minimizing IK(p*,p0) -within the set p1 of all the 
distributions on Q 'consistent' with I- is identical to the PI-distribution which 
is derived from (PME) ( cf. Rosenkrantz, 1981, Chapter 4.3, p. 3 and 
Seidenfeld 1986, p. 474). Therefore, in the case of discrete parameters, 
(MCB) is logically equivalent to the principle of maximum entropy (PME). 
Since (MCB) can be seen as a generalization of (PME) to continuous 
parameters, then also the PI-distributions selected on the basis of (MCB) will 
be called MAXENT -distributions. 
A critical evaluation of MAXENT 
Some important problems concern the 'nature' and the 'source' of the 
constraints included in I. Regarding the first problem Jaynes (1978, p. 61) 
points out that: 
In our mathematical formalism, a "constraint" is some piece of information that leads to 
modify a probability distribution ... It is perhaps not yet clear just what we mean by 
"constraints" in a physical experiment. Of course, by these we do not mean the gross 
constraining linkages by levers, cables, and gears of a mechanics textbook, but something 
more subtle. In our applications, a "physical constraint" is any physical influence that exerts 
a systematic tendency - however slight ·- on the outcome of an experiment. 

86 
CHAPTER 7 
Jaynes (ibid., pp. 62-72), for instance, considers the data collected by the 
Zurich astronomer R. Wolf who, in the period roughly between 1850-1890, 
tossed an ivory die 20,000 times. After remarking that "any imperfection in 
the die may ... give rise to a 'physical constraint"' (ibid., p. 62), Jaynes (ibid., 
p. 63) then considers two typical imperfections of a die: 
The most obvious imperfection is that different faces have different numbers of spots. This 
affects the center of gravity, because the weight of ivory removed from a spot is obviously 
not (in any die I have seen) compensated by the paint then applied. Now the numbers of 
spots on opposite faces add up to seven. Thus the center of gravity is moved toward the "3" 
face, away from "4", by a small distance E corresponding to the one spot discrepancy. The 
effect of this must be a slight frequency difference which is surely, for very small E, 
proportional to E. 
The (estimated) value of this "slight frequency difference" will be one of the 
distributional constraints to be used when applying (PME) to Wolf's Dice. 
It is difficult to imagine that the probabilistic constraints in I are 'given 
directly' to us as a result of some previous experimental evidence or accepted 
theory. It seems more reasonable to assume that there is some 'prior 
inference' leading from the background knowledge BK to I. A possible 
answer to the question about the source of the constraints included in I is 
suggested by the above example of Wolf's Dice Data. It suggests, indeed, that 
the conjectures about the "physical constraints" - or "physical influences" -
working in the investigated experimental set-up derive from a certain 
background knowledge or, more specifically, from the laws of classical 
mechanics together with the observed physical features of the dice used (and 
of other dice similar to them). 
More generally, it seems to me that, in the applications of MAXENT 
commonly considered in the literature, the constraints in I may be interpreted 
as 'formal representations' of certain guesses - derived from the available 
background knowledge BK - concerning a corresponding set of physical 
constraints. 14 Let us consider, for instance, the probabilistic constraints 
c1, ••• ,cm specifying some moments - or other typical values - of certain 
functions / 1(9), ... ,fm(9) of the parameter 9 of a given process Ex. Here ft(9), ... , 
fm(9) denote objective features of Ex, while c1, ••• ,cm can be seen as the formal 
representations of certain well-grounded conjectures - derived BK - about 
f 1(9), ... ,fm(9). For instance, if we guess - starting from BK- that the value of 
/ 1(9) is a, we may formally represent this guess by the constraint c1: "the 
expected value of / 1(9) is a". 
Now the question concerning the source of the probabilistic constraints 
c1, ••• ,cm included in a given prior information can be put as follows: by which 

THE CHOICE OF PRIOR PROBABIUTIES 
87 
'prior inferences' does BK - which typically consists of experimental and 
theoretical propositions- lead to the 'acceptance' of c1, ••• ,c,? In other words: 
which is the path leading from BK to certain probabilistic constraints on 9? 
It seems to me that the claim that the MAXENT distribution is 'the 
objective prior' involves, at least implicitly, the assumption that the 
distributional constraints in /, far from expressing simple subjective opinions, 
are based on some 'objective' - and intersubjectively shared - background 
knowledge BK. Unfortunately, in the literature on MAXENT one does not 
find any general analysis of the prior inferences leading from BK to the 
distributional constraints in I and, at present, it is not clear whether a 
satisfying general account of these inferences is indeed possible ( cf. final 
section of this chapter). 
Many objections to MAXENT concern the cognitive status and the intuitive 
plausibility of the principles (PME) and (MCB).15 For instance, Miller 
(1980, p. 264) points out that, 
... even if a unique proper prior distribution were always attainable by use of some variant 
or other of Jaynes' rule, it would stiii be pertinent to ask why that prior should be used. 
Uniqueness is not itself any guarantee of objectivity, or rather, the objectivity that it does 
guarantee has no obvious connection with correctness. 
Some reasons for the assumed objectivity - and universal applicability - of the 
MAXENT distributions derived from (PME) and (MCB) should be given. 
Consider, for instance, (MCB). Arguing for the universal applicability of 
(MCB) involves the justification of two claims: (i) that within the set of all 
the distributions satisfying the constraints in /, the MAXENT distribution 
f*( ()) should be selected in any case so as to minimize the distance between 
j'*(Q) and the ignorance distribution fo(Q) and (ii) that the 'right' measure of 
distance between two probability distributions is given in any case by the 
cross entropy divergence (see (6)). 
Even supposing that claim (i) may be justified, 16 serious doubts arise in 
relation to claim (ii). For instance: since in statistical literature there are so 
many measures of dissimilarity (distance) between probability distributions, 
why should we choose just the cross entropy divergence as the 'universally 
appropriate' distance measure? One should recall, in this connection, that the 
definition of plausible measures of distance for epistemic probability functions 
is still an open question and that cross entropy divergence is no more than a 
possible solution to this problem.17 
Further, it should be recalled that, generally speaking, the ( dis)similarity 
between two 'complex entities' is a 'multi-dimensional matter' and, therefore, 
a 'global measure' of (dis)similarity has to reflect the relative importance 

88 
CHAPTER 7 
attributed to several partial similarities' between different features of the 
entities in consideration. Since probability distributions are also somewhat 
complex entities, it is reasonable to assume that there is no 'universally 
appropriate' measure of dissimilarity (distance) between epistemic probability 
distributions; on the contrary, it would appear that different distance measures 
may be used in different cognitive contexts. This is tantamount to rejecting 
claim (ii). 
Analogous remarks apply, in the case of discrete parameters, to the 
principle (PME) which requires maximization of the uncertainty of the prior 
distribution as defined by its Shannon entropy. In fact, although accepting the 
idea that in the selection of prior probabilities uncertainty should be 
maximized, one might wonder whether H(p) is a universally appropriate 
explicatum for the uncertainty of a probability distribution p. The introduction 
of many alternative indices of entropy (uncertainty) - for instance, the 
"entropy of order a" proposed by Renyi (1961), the "weighted entropy" 
proposed by Guiasu and many others (see Guiasu, 1977, Chapters 4-5) -
seems . to suggest that this is not the case. It would appear much more 
plausible to suppose that different measures of uncertainty may be appropriate 
in different cognitive contexts. This conflicts with the strong belief held by 
Jaynes that (PME) is universally applicable to all discrete parameters. 
7.3 THE SUBJECTIVE INTERPRETATION OF THE THEORY 
OF INDUCTIVE PROBABILITIES 
There are two basic assumptions underlying the subjective interpretation of 
TIP: 
(i) 
the requirements of adequacy used in TIP should be descriptive 
principles able to capture important features of the inductive 
probabilities of the researchers engaged in certain types of 
empirical research; 
(ii) 
such principles should leave room for a wide variety of inductive 
methods. 
According to Jeffrey (1973, pp. 302-303) Carnap's approach to TIP can be 
seen as a particular form of subjectivism: 
... perhaps [Carnap] is describable as a special sort of subjectivist. Differences among the 
[C-systems] which represent our various inductive intuitions are matters of individual 

THE CHOICE OF PRIOR PROBABIUTIES 
89 
psychology, while the respects in which all of our [C-systems] agree represent human nature -
general psychology. Perhaps Camap would accept this subjectivist reading, but would think 
it appropriate to dignify with the name 'logical' the common features of all [C-systems] ... 
Inductive logic is a description of the norms we shall find ourselves accepting, once we 
have thought the matter through. It is no part of Camap's program, to base those norms on 
still deeper philosophical considerations. 
This is in keeping with the first of the two above assumptions. Regarding the 
second assumption, Jeffrey (ibid., pp. 302) points out that: 
Camap was prepared to admit the possibility that different people might have somewhat 
different inductive intuitions, e.g. when, ca. 1951, he thought that the right [C-system] might 
be found somewhere in the continuum of inductive methods, he thought that different people 
might discover that they had somewhat different values of /... ... He thought it possible that 
these differences were irreducible, so that ... there might be no such thing as the [C-system] 
which represents our inductive intuitions. Rather there might prove to be a range of inductive 
intuitions among us, which might prove to be represented by different [C-systems]. 
For most of his thirty five years of work on TIP, Carnap accepted the idea 
that TIP leaves researchers 
considerably free to choose their prior 
probabilities. According to Carnap (1980, p. 106): 
The person X wishes to assign rational credence values to unknown propositions on the basis 
of the observations he has made. It is the purpose of inductive logic to help him to do this 
in a rational way; or, more precisely, to give him some general rules, each of which warns 
him against certain unreasonable steps. The rules do not, in general, lead him to specific 
values; they leave some freedom for choice within certain limits. 
Referring to GC-systems, Carnap (1980, pp. 111-112) states that, 
... although certain boundaries for /... can be determined objectively by considerations of 
rationality requirements, within these limits everyone is free to make his choice as he pleases. 
We might call this a (modified) subjectivist point of view. (A similar view, but without 
acceptance of objective limits might be called a purely subjectivist view.) 
Regarding the choice of a A.-value by an individual X, Carnap (ibid. p. 112-
113) points out that: 
... we might regard X's choice of a A.-value ... as determined by, and therefore symptomatic 
of, certain features of X's personality .... X's choice should not be regarded as arbitrary or 
unpredictable in the sense that a person of X's nature could just as well have made another 
choice; X's choice is rather regarded as determined or at least influenced by his personality. 

90 
CHAPTER 7 
Lastly, Camap (ibid., pp. 113-114), referring to the different effects of 
experimental evidence on the posterior probabilities depending on the value 
of A, suggests that, 
... A. measures the resistance or inertia against this influence of experience. . .. from the 
personalist point of view it is regarded as possible that rational people use different A.-values 
... The difference may be attributed to their different inductive inertia. 18 
It is clear that according to the subjective interpretation of TIP, the optimum 
inductive method is the one which supplies the best formal representation of 
the researcher's beliefs. To identify such a method, procedures similar to 
those illustrated in the subjective approach to the choice of priors in BS 
should be used. 
7.4 THE APRIORISTIC INTERPRETATION OF THE THEORY 
OF INDUCTIVE PROBABILITIES 
There are two basic assumptions underlying the aprioristic interpretation of 
TIP: 
(i) 
the requirements of adequacy used in TIP should be a priori 
normative principles, i.e., principles of rationality justified by a 
priori considerations; 
(ii) 
on the basis of such principles, a unique inductive method should 
be specified or, at least, the range of admissible inductive methods 
should be considerably restricted. 
Both assumptions were advocated by Camap who, during his long career as 
an inductive logician, often oscillated between subjectivism and apriorism. 
For instance, Camap (1968, p. 265) stresses the importance of "inductive 
intuition" in justifying the axioms for TIP: 
I think that it is not only legitimate to appeal to inductive reasoning in defending inductive 
reasoning, but that it is indispensable .... If a person were unable to distinguish valid from 
invalid steps in inductive reasoning, even in the simplest cases, then it would be hopeless to 
try to convince him of anything in inductive logic. In order to learn inductive reasoning, we 
must have what I call the ability of inductive intuition. 

THE CHOICE OF PRIOR PROBABIUTIES 
91 
Although Camap is somewhat vague about the relationships between 
inductive axioms and inductive intuitions, it seems to me that he does not 
consider inductive axioms as a mere description of our inductive intuition but 
that he looks at inductive intuition as a source of a priori arguments for the 
justification of inductive axioms. 19 
Regarding the second assumption, (Camap, 1968, p. 314) points out that 
he is inclined 
to base the choice of an inductive method, and in particular the choice of a value of A., ... 
only on a priori considerations. The considerable narrowing of the range of admissible 
A.-values which we have achieved in the course of the last ten years was obtained exclusively 
by a priori arguments. 
More specifically, let us examine how, from Camap's (1971, 1980) aprioristic 
standpoint, the decision to use a given GC-system is justified. This decision 
is seen as a 'four step decision' (see Camap, 1980, p. 106): 
(i) 
the decision to use a GC-system or not, 
(ii) 
the choice of they-values, or prior predictive probabilities, y1, ••• ,yk> 
(iii) 
the determination of the range of admissible A.-values, 
(iv) 
the choice of a specific A.-value. 
Before considering these steps in detail, let us recall that, w.r.t. an arbitrary 
family of properties Q=(Q1, ... ,Qk), Camap (1971, §§ 2 and 4-5) assumes that: 
(a) 
the properties, or "attributes", Qw··,Qk can be represented as 
extended regions of an "attribute space" related to Q, 
(b) 
for any attribute space a suitable metric (i.e., a distance function 
based on similarity relations) can be defined, 
(c) 
starting from this metric, the "width" of each attribute and the 
"distances" between couples of attributes can be determined. 
The basic assumption underlying Camap's aprioristic approach is that the 
assignment of inductive probabilities "to propositions involving attributes of 
a given family is frequently influenced by certain features of these attributes 
and relations among them" (Carnap, 1980, p. 8) such as the above mentioned 
widths and distances. 
More specifically, w.r.t. step (i), Carnap (1980, p. 107) maintains that 
GC-systems should be used (1) whenever Q is an "equal-distance family" 
where the distances between the different couples of properties are 
(approximately) equal to each other, and (2) whenever Q is a "long-distance 

92 
CHAPTER 7 
family", where none of the distances between pairs of properties is below a 
certain threshold value. This claim is defended as follows. 
According to Carnap (1980, p. 42) when Q is an equal-distance (long-
distance) family, any 'analogy by similarity' between different properties in 
Q should be excluded and accordingly the inductive probabilities should 
satisfy the so-called YJ-equality (cf. (6.6)). It can be proved that, when k > 2, 
the YJ-equality is entailed by the requirement of restricted relevance (RR) 
(Carnap, ibid., p. 87, T19-1). Referring to this logical relation between 
YJ-equality and (RR), Carnap (ibid., pp. 104-105) argues that, if YJ-equality is 
assumed, (RR) should be adopted20 and therefore GC-systems should be 
used. (Remember that, when k > 2, GC-systems can be axiomatized using 
(RR), together with (Exc) and (IP): cf. (6.38).) 
Regarding step (ii), Carnap (1980, pp. 33-34 and 106) suggests that the 
y-value Yi of each property Qi should be proportional to the width of the 
corresponding attribute. For instance, the GC-systems "with y-equality", or 
C-systems, should be used where all attributes have equal width (Carnap, pp. 
106-107). 
Concerning the determination of the range of admissible A.-values for 
GC-systems (step (iii)), Carnap (1980, pp. 107-111) argues that, in the 
particular case of C-systems, this range may be considerably restricted given 
the following a priori considerations. 
Consider two complete descriptions e1 and e2 of a sample of n individuals 
(with n > k) where e1 is minimally ordered (i.e., the k numbers n1, ... ,nk differ 
at most by one) and where e2 is maximally ordered (i.e., all the members of 
the sample have the same property). Having proved that in any C-systemp(ez) 
> p(e 1), Carnap (ibid. pp. 108-109) proposes a "uniformity requiremenf' (UR) 
which strengthens the above inequality by replacing e1 and e2 with the 
corresponding sets e1 and e2 (see Chapter 2.3): 
(UR) (a) 
(b) 
p(i!z) 
~ p(i!1) 
p(i!z) 
-:f. p(i!t) 
The intuitive idea underlying (UR) is that a maximally ordered sample has a 
"preferential status" and - consequently - should receive "preferential 
treatment" w.r.t. a minimally ordered sample (Camap, ibid., p. 109). 
Carnap (ibid., pp. 109-110) proves that accepting clause (a) is tantamount 
to applying - for any family (Q1, ... ,Qk) where k ~ 2 - the restriction "A. s k" 
on the range of admissible A.-values, while the stronger restriction "A. < k:' 
should be applied if clause (b) is also accepted. 

THE CHOICE OF PRIOR PROBABIUTIES 
93 
Other a priori considerations lead Camap (ibid., pp. 110-111) to exclude 
"very small" values of "A., so as to reach the provisional conclusion that the 
range of admissible "A.-values is given by 1/2 < J... < k. 
The four step selection of the optimum inductive method is completed, in 
step (iv), by the choice of a specific "A.-value (see Camap 1980, pp. 111-118). 
Concluding a detailed analysis, Carnap (ibid., p. 118) tentatively suggests that 
"all the long-distance families, for any attribute space and for any size of the 
family [i.e., for any value of k], have practically the same "A.-value". In 
combination with (UR)(a), this proposal implies that J... s 2. Restricting, for 
simplicity, the choice of J... to integer values, this is equal to assuming J... = 1 
and J... = 2 as the only admissible values of "A.. Finally, if (UR)(b) is also 
accepted, one is led to selecting J... = 1 as the uniquely admissible value of "A.. 
Indeed, Carnap (ibid., p. 119) expresses his preference for J... = 1 as the 
optimum "A.-value for all the long-distance families.21 
Some remarks on Carnap's aprioristic interpretation of TIP may be useful 
to note. For instance, Carnap's justification of (RR) does take account, to 
some extent, of the cognitive context of a given empirical research, since the 
metrical structure of (the attribute space related to) the family Q used to 
describe the population or process under investigation is one of the features 
of this context. Notwithstanding this, Carnap's justification is highly 
aprioristic since he claims that (RR) should be adopted wherever an equal-
distance - or long-distance - family Q is employed, independently of the 
specific background knowledge BK available within a given cognitive context. 
It seems to me that, against Camap's claim, BK plays an important role in 
justifying the decision to adopt - or reject - the requirement of adequacy 
(RR). Let us suppose, for instance, that, according to the available background 
knowledge BK concerning the population of cars produced over the last year 
by a certain factory, all cars are coloured with one of the colours in the 
family Q = (Yellow, Orange, Black). According to Carnap's view, (RR) 
should never be applied in the case where Q is used since, due to the 
nearness between Yellow and Orange, Q is neither an equal-distance family 
nor a long-distance family. However, it seems to me that Carnap is wrong on 
this point, since there may be cases where the available BK justifies the 
adoption of (RR) also where the population under investigation is described 
using Q. For instance, in the case considered above, I believe that, if the 
available BK concerning the marketing strategies of factories excludes that 
they have the tendency to concentrate their production of cars on 
chromatically similar kinds, then (RR) should be adopted. (For a more 
detailed 'contextual justification' of the decision to adopt inductive methods 
satisfying (RR), see Section 6). 

94 
CHAPTER 7 
Now consider a case where, according to the available BK, certain widely 
adopted marketing strategies suggest the large scale production of some 
sharply differentiated 'basic kinds' of cars and a limited production of some 
'variants' of each basic kind. This implies that, typically, a factory does not 
engage in large scale production of balls of very similar colour. Hence, as far 
as Q = (Yellow, Orange, Black) is concerned, there should be a strong 
'negative correlation' between the proportions of Yellow and Orange balls 
within the set of cars produced last year by a factory. It follows that, in spite 
of the similarity between Yellow and Orange, there should be a 'negative 
(inductive) analogy' between these two properties, in the sense that the 
observation of a yellow ball should be negatively relevant to the hypothesis 
that the next observed ball will be orange.22 This conflicts with Carnap's 
view (1980, pp. 42-49) that a high proximity between two properties in a 
given attribute space should always imply a 'positive (inductive) analogy' 
between them. On the contrary, this example shows that the occurrence of a 
positive (negative) inductive analogy between two properties depends- rather 
than on their proximity in a certain attribute space - on the specific 
information conveyed by BK.23 
One can imagine many other, more or less realistic, situations characterized 
by 'competition' between similar properties, so that the positive inductive 
analogy between two properties is inversely proportional to their proximity 
in a given attribute space. For instance, in a given ecological context, there 
may be strong competition between similar species. Another example 
concerns the competition among political parties where each party tends to 
increase its electorate at the expense of other ideologically similar parties. 
Of course one can also imagine situations where the available BK implies 
that the positive inductive analogy between two properties is directly 
proportional to their proximity in a given attribute space. For instance, BK 
may imply that the 'causal mechanism' through which a given population is 
generated tends to produce positively correlated percentages of similar 
properties. However, there is no reason to believe that this kind of 
background knowledge is available in any cognitive context. Hence, it is 
difficult to see why one should accept Carnap's idea that a high proximity 
between two properties in a given attribute space should always imply a 
positive inductive analogy between them. 
Analogous objections may be raised against Carnap's aprioristic view of the 
choice of y-values (step (ii)): in Chapter 8.1 it is argued that this choice 
should be affected by the available BK. 
Regarding Carnap's postulate (UR), I agree with Hilpinen (1973, p. 325) 
that: 

THE CHOICE OF PRIOR PROBABIUTIES 
95 
The postulate ... expresses a strong a priori belief in a high degree of uniformity in our 
universe of individuals. It may be justified in special cases (for instance, on the basis of 
suitable background information), but it is hardly acceptable as a general principle of 
inductive logic. 
Consider that the expected disorder E[G(q)] for a GC-system (y,J...) is a 
function of I. and the Gini diversity G(y) of the prior vector y- i.e., E[G(q)] 
= [f../(/.. + l))G(y) - where G(y) is the maximum value of E[G(q)] for a 
GC-system with prior vector y (see (6.24), (6.25), and (6.33)). Taking account 
of this equality, Hilpinen's remark can be restated as given below. 
Since (UR) implies that I. < k, it also implies that E[G(q)) = 
[/../(/.. + l))G(y) < [k!(k + l)]G(y), i.e., that E[G(q)] is lower than k!(k + 1) 
times the maximum value G(y). While this is a weak restriction for a family 
Q={Q1, ... ,Qk} with a large k, it is quite strong for small values of k. For 
instance, for k = 2 the restriction imposed by (UR) is given by the inequality 
E[G(q)] < (2/3)G(y) which corresponds to "a strong a priori belief in a high 
degree of uniformity in our universe". However, it is difficult to see why this 
strong a priori belief should be adopted in any empirical research, 
independently of the population - or process - under investigation and the 
family Q=(Q1,Qz) used to describe it.24 
It seems much more plausible to assume - as will be discussed in Chapter 
8.2 - that a researcher's initial opinions about the degree of disorder in a 
population depend more on his 'contextual information' than on a priori 
assumptions. 
7.5 THE CONTEXTUAL VIEW OF PRIOR PROBABILITIES 
Some general considerations 
We saw above (Section 2) that (partially) objective Bayesian approaches are 
characterized by the adoption of at least one principle of rationality other than 
the probability axioms, and that the degree of objectivism of a Bayesian 
approach is related to the 'strength' of the restrictions imposed by such 
principles of rationality on a researcher's freedom in his choice of prior 
probabilities. 
Here I would like to make the distinction between two kinds of principles 
of rationality - or inductive principles - i.e., universal and contextual 
principles. Universal principles -or a priori principles- are claimed to hold 
in any cognitive context and, accordingly, are justified by a priori 
considerations. On the contrary, contextual principles are claimed to hold only 

96 
Radically 
obiective 
approaches 
DEGREE OF 
OBJECTIVISM 
Radically 
subjective 
approaches 
0 
Radically 
contextual 
approaches 
CHAPTER 7 
DEGREE OF 
APRIORISM 
Figure 1 
MAXENT 
Radically 
aprioristic 
approaches 
The 'two-dimensional space' of Bayesian approaches 
in certain appropriate cognitive contexts and, accordingly, they are justified 
w.r.t. the particular kind of background knowledge which is available in such 
contexts. 
Objective Bayesian approaches may be differentiated according to their 
degree of apriorism which depends on the number and the 'strength' of the 
adopted a priori principles. In particular, between the two poles represented 
by 'radically aprioristic approaches' and 'radically contextual approaches' one 
can visualize a wide range of 'partially contextual approaches' including both 
universal and contextual principles. 
By combining the classification of Bayesian approaches according to their 
degree of objectivism with the classification according to their degree of 

THE CHOICE OF PRIOR PROBABIUTIES 
97 
objectivism one obtains the 'bidimensional space' of Bayesian approaches 
which is represented in Figure 1. 
Notice that in Figure 1 the MAXENT approach is represented as a radically 
aprioristic and radically objective one. A reason frequently advocated for 
adopting MAXENT is, indeed, its radical objectivism, i.e., the fact that the a 
priori principles (PME) and (MCB) always allow identification, on the basis 
of the available partial information, of the uniquely admissible 'objective 
prior' in a given context. However, below I will argue that the need for 
objectivity (intersubjectivity) may also be satisfied by adopting a contextual 
approach. More specifically, I will show that (a) contextual principles may 
exist, and (b) that a contextual approach may be (radically) objective. 
Regarding claim (a), it should be noted that a requirement of adequacy for 
a prior distribution may be seen as a particular type of distributional 
constraint. Let us consider, for instance, the parameter vectors (8,t) and the 
probabilistic constraint c - where c = "Q and t are independent" - on the prior 
distribution p(61,~) (cf. Domotor, 1985. p. 75). One sees that c, unlike the 
constraints which are commonly used in the illustrations of MAXENT, does 
not specify any typical value ofp(61,~). On the contrary, cis a sort of 'formal 
constraint' expressing a requirement of adequacy which should be satisfied 
by the mathematical form of p(61,~). To say that the requirement of adequacy 
c is a contextual principle is equal to saying that c may be derived from the 
'objective' - and intersubjectively shared - background knowledge BK 
available in a given cognitive context. It seems to me that assuming this 
possibility is not more complicated, in principle, than assuming the possibility 
of a 'contextual justification' of the m-constraints and the other kinds of 
distributional constraints employed in the usual applications of MAXENT. A 
concrete example of this possibility will be given in Section 6, where I will 
show that the requirement of adequacy- or formal constraint- (CM), used for 
characterizing Dirichlet distributions (<;:f. Chapter 6.6), can be justified w .r.t. 
a certain background knowledge and, hence, can be seen as a contextual 
principle. 
Concerning claim (b) - according to which a contextual approach may be 
(radically) objective - note that the use of (CM) in the axiomatization of the 
Dirichlet family for a parameter vector q=(q1, ... ,qJ provides a concrete 
example of the possibility that a contextual requirement might impose a very 
strong restriction on the possible prior distributions on q. Notice too that, 
given the formal constraint (CM) and other k distributional constraints (for 
instance the mean vector y=(y1, ... ,yk) and the expected Gini diversity E[G(q)]), 
the uniquely admissible prior distribution on q for a given cognitive context 

98 
CHAPTER 7 
can be identified ( cf. (1)). 25 This signifies that the contextual Bayesian 
approach related to the contextual principle (CM) is radically objective. 
Let us now return to the previously mentioned problem (Section 2) of 
providing an account of the 'prior inferences' from the available BK to certain 
m(ean)-constraints in the prior information/. 
The basic idea underlying this account is that, in several cases, the prior 
beliefs of the researchers regarding parameter 9 of the experimental process 
Ex under investigation ·are (or, at least, should be) determined by certain 
information included in BK such as: 
(i) 
all available experimental information on the value of 9 in 
processes 'sufficiently similar' to Ex; 26 
(ii) 
all available theoretical information on the relations between the 
value of 9 and the values of other quantities +1, ... ,fm in processes 
'sufficiently similar' to Ex (together with information available on 
the values of f 1, ... ,fm in Ex). 
Such information may be called external information on 9 in Ex, or 
Eln(9,Ex).27 In certain cognitive contexts the external information Eln(9,Ex) 
in BK may be 'rich' enough to suggest a 'prior point estimate' of 9 in Ex. 
This prior estimate will now be referred to as the external estimate of 9 and 
denoted by Est(9). More generally, in cerfatn cognitive contexts Eln(9,Ex) 
may suggest an external estimate Est[h(9)] of a given function h(9) of 9. 
Although external estimates are made in a somewhat informal and 
spontaneous manner, it will be assumed that they satisfy the following 
minimal requirement of adequacy: 
(Ext) (a) 
(b) 
Est(a91 + b92 + c) = aEst(9) + bEst(92) + c 
If, for any () E R(9), () 0!: 0 then Est(9) 0!: 0.28 
Note that, if a researcher is able to make two of the three estimates 
Est(a91+b92+c), Est(91), Est(92) using appropriate 'direct procedures', he may 
derive the third using (Ext)( a) as an 'indirect procedure'. 
In my view external estimates are the 'intermediate step' in the prior 
inference from BK to the m-constraints in /. More specifically, I would 
assume that the prior inferences from external estimates to m-constraints are 
ruled by the following requirement of cognitive coherence (CC): 
(CC) (a) 
Given the external estimate Est(9), the following m-constraint 
on p(Q) should be included in/: "E(9) = Est(9)". 

THE CHOICE OF PRIOR PROBABIUTIES 
99 
(b) 
Given the external 
estimate Est[h(9)], 
the 
following 
m-constraint on p(lJ) should be included in I: 
"E[h(9)] = Est[h(9)]". 
The intuitive idea underlying (CC) is that our m-constraints on p(9) should 
provide a formal representation of our (informal) prior estimates of 9 - or 
other parameters related to 9 - which are derived from the available 
background knowledge BK. 
The above account of the prior inferences from BK to the m-constraints in 
I is represented in the Figure 2. 
BK 
I 
Eln(8, &) 
l 
l 
(Ext) 
ICC) 
Figure 2 
The prior inferences from the background knowledge BK to 
the m-constraints in I 

100 
CHAPTER 7 
7.6 A CONTEXTUAL JUSTIFICATION OF DIRICHLET 
DISTRIBUTIONS AND GC-SYSTEMS 
In my opinion a context-dependent justification of the decision to use a prior 
Dirichlet distribution or, equivalently, a GC-system, can be given as shown 
below. 
Suppose that the background knowledge BK available in a given cognitive 
context includes the assumption that the experimental process Ex - whose 
trials are classified w.r.t. a given family of properties Q=(Q1, ... ,QJ - is a 
multivariate Bernoulli process with parameter vector q=( 'h, ... ,'IJ<.1). In this 
case those researchers who wish to use the Bayesian approach should select 
a prior cdf F(qw .. ,qJ ori. q on the basis of BK. 
With reference to this, recall that BK may include some external 
information Eln(q,Ex) about q in Ex. In particular, BK may include (i) some 
experimental information on the value of q in processes 'sufficiently similar' 
to Ex (such as the statement "the value of q in the multivariate Bernoulli 
process Ex' is a"), or (ii) some theoretical information on the relations 
between the value of q and the values of other quantities +1, ... ,fm in processes 
'sufficiently similar' to Ex (together with information available on the values 
of t 1, ... ,fm in Ex). 
Although a more precise description of the above external information 
would require specification of the concept of "sufficiently similar processes", 
an informal use of this concept is sufficient for our purposes. For instance, 
the multivariate Bernoulli processes consisting in throwing several dice of the 
same shape and made of the same material might be considered sufficiently 
similar to each other also in the case where the dice are of different size. 
From now on, a multivariate Bernoulli process Ex' will be referred to as an 
"Ex-similar process" in the case where it is sufficiently similar to Ex. 
Below I will describe a situation where the external information Eln( q,Ex) 
justifies the use of the requirement of adequacy (CM) and, consequently, the 
Dirichlet family. 
Let us suppose that the experimental information contained in Eln( q,Ex) 
includes the statements H 1, ... ,H,. which specify the values of q for the 
Ex-similar processes Ex1, ... ,Ex,.. Given a statement H, where t = 1, ... ,m, the 
value of q specified by H, will be referred to as a'=(a~, ... ,a;), where a: denotes 
the objective probability q1 of Q; in Ex,. Then the k - 1 fractions aj/(1 - aD 
(where j ,;. t) specify how the 'residual part' (1 - aD of q in Ex, is divided 
among the remaining properties Qj. 
Given two distinct properties- Q; and Qj- let us consider (1) the quantities 
a~, ... ,a7 denoting the objective probabilities of Q; in Ex1, ••• ,Ex,. specified by 

1HE CHOICE OF PRIOR PROBABIUTIES 
101 
the hypotheses H 1, ... ,H .. , respectively, and (2) the quantities a~/(1 - a}), ... , 
aj/(1 - ai) expressing the ratios between the objective probabilities af, ... ,aj 
of Qj in Ex1, ... ,Ex .. and the 'residual parts' (1 -
a~), ... ,(1 - a7) of q in 
Exi> ... ,Ex ... 
Now, each of the two sets a!, ... ,a7 and a}/(1 - a!), ... ,aj/(1 - ai) will 
typically include values somewhat different from each other. However, 
suppose that the changes in the values of the ratios aj(1 - a~ - where t = 
1, ... ,m - are not appreciably related (in some systematic way) to certain 
corresponding changes in the values of a:. 29 In this case we may say that Qj 
is H 1, ... ,H .. -independent of Q/0 
When m is sufficiently high, the H 1, ... ,H .. -independence of Qj and Q; 
suggests that the 'causal mechanism' producing Ex-similar processes works 
in such a way that, as it were, the 'production' of a high (low) ratio 'II of 
Q;-outcomes neither favours nor hampers the 'production' of a high (low) 
residual ratio qJ /(1 - 'II) of Qj -outcomes. This hypothesis may be called the 
hypothesis of independence of Qj and Q; (for Ex-similar processes). 
Suppose that the independence of Qj and Q; for Ex-similar processes is 
accepted. The acceptance of this hypothesis could be 'formally represented' 
by the following formal constraint cji on the prior cdf F(q1, ... ,qk_1) on 
q=( q1, ... ,qk_1) in Ex: "% /(1 - 'II) (where j =I= z) is independent of <L".31 Notice 
that the conjunction of the k - 1 formal constraints, or requirements of 
adequacy, cji (where j =I= z) is equivalent to the (CM); -neutrality of q (cf. 
Chapter 6.6). 
Now, assume that, given the experimental information H 1, ••• ,H .. included 
in Eln(q,Ex), researchers have established the H 1, ••• ,H.,-independence of Qj 
and Q;, for any couple Q; =I= Qj. If the above arguments are valid, then this 
assumption implies that researchers: 
(a) 
should accept the corresponding hypotheses of independence of Qj 
an Q; for Ex-similar processes; 
(b) 
should adopt the corresponding constraints cji on q; 
(c) 
should assume the (CM);-neutrality of q, fori= 1, ... ,k. 
This is tantamount to assuming that the prior distribution F(q1, ••• ,qk_1) on q 
satisfies the requirement of adequacy (CM) or, equivalently, that F(q1, ••• ,qk_1) 
is a Dirichlet distribution (cf. 6.6). In other words the above contextual 
justification of (CM) can be interpreted as a contextual justification of the 
decision to use a prior Dirichlet. 
Moreover, given the equivalence between Dirichlet distributions and 
GC-systems (cf. 6.3), the above contextual justification of (CM) can be seen 

102 
CHAPTER 7 
as a contextual justification of the decision to adopt, within a given empirical 
research, an inductive method (y,J..) belonging to the class of GC-systems.32 
Having decided to use a GC-system (y,J..), the values of the prior vector y 
and of the parameter J.. must be selected. A 'contextual analysis' of these 
decisions will be provided in the next chapter. 

CHAPTER 8 
THE EPISTEMIC PROBLEM OF OPTIMALITY (EPO): 
A CONTEXTUAL APPROACH 
The problem of determining the optimum Dirichlet distribution - or, 
equivalently, the optimum GC-system- for investigating a given multivariate 
Bernoulli process Ex with parameter vector q=( q1, ••• ,qJ may be referred to 
as the epistemic problem of optimality, or EPO. In this chapter a contextual 
approach to EPO is proposed. 
Given the equivalence between GC-systems and Dirichlet distributions (see 
Chapter 6.3), only Dirichlet distributions will be dealt with. A Dirichlet 
distribution Dir(a1, ... ,a.J on q will be referred to as Dir(y,A), where 
y=(y1, ... ,y.J = (a/f.., ... ,aJA.) is the prior vector of Dir(a1, ... ,a.J and l:a; is 
denoted by A, to recall visually that Dir(a1, ... ,aJ = Dir(y,A) is equivalent to 
the GC-system (y,A). 
Solving EPO for a given empirical research situation consists in 
establishing the optimum prior distribution Dir(y,A), i.e., the optimum values 
ofy and A. 
The selection of the optimum prior vector y 0=(y~ , .... ,y'{) will be considered 
in Section 1. 
Subsequent sections will deal with the problem of determining the optimum 
prior (and the related optimum A-value) within the set DIR(yj of Dirichlet 
distributions Dir(y 0,A) with prior vector yo. Since selection of the optimum 
A-value represents the most difficult part of EPO, this problem will hence be 
referred to as EPO. 
In Section 2, an important type of multinomial cognitive context called 
GD-context is defined. 
Later two 'contextual solutions' to EPO applicable to GO-contexts are 
proposed. 
The first solution, illustrated in Section 3, is called a cognitive coherence 
solution, or CC-solution, since it is based on the requirement of cognitive 
coherence (CC). 
The second solution, described in Sections 4-5, is called a verisimilitude 
solution, or V-solution, since it is based on the verisimilitude thesis (VER) 
which states that the most important cognitive goal of pure science is the 
103 

104 
CHAPTER 8 
achievement of a high degree of verisimilitude (see Chapter 1.1). The 
V-solution is obtained using the following strategy: 
(a) 
Having defined the logical problem of optimality, or LPO, for the 
set DIR(y) of all Dirichlet distributions with a given prior vector 
y, a V -solution to LPO is provided (Section 4). 
(b) 
Using (VER) and the V-solution to LPO, a V-solution to EPO is 
elaborated (Section 5). 
Somewhat surprisingly, in spite of the conceptual differences between the V-
and the CC-solution to EPO, they give identical results. It will be argued that 
this equivalence, far from being a fortunate mathematical coincidence, can be 
conceptually explained (Section 6). 
Lastly, in Section 7 the relationships between the above-mentioned 
V-solution to LPO and some results obtained by Carnap (1952) are 
considered. 
8.1 THE OPTIMUM PRIOR VECTOR 
In several cases the optimum prior vector yo can be derived from the available 
background knowledge BK. For instance, suppose that the external 
information Eln( q,Ex) in BK is given by the experimental information 
H 1, ... ,Hm, where such statements specify the values of q=(q1, ... ,qJ for the 
Ex-similar process Ex1, ... ,Exm (cf. Chapter 7.6). In particular, remember that 
the values of q1 specified by H 1, ... ,Hm are referred to as a}, ... ,a7, respectively. 
If m is not too low the following external estimate Est( <It) of q1 in Ex may 
be made: 
m 
~a: 
1=1 
(1) 
Est(qJ = (a~+ ··· + ai)/m = 
m 
Alternatively the arithmetical average ~aYm in (1) could be replaced by the 
following more sophisticated weighted average: 

THE EPISTEMIC PROBLEM OF OPTIMAUTY 
105 
m 
~w1a: 
1=1 
m 
w 
(W= ~ w1) 
1=1 
(2) 
Est(qJ = 
where the weights w1, ••• ,wm may reflect the varying 'degrees of similarity' 
between Ex and each of the Ex-similar process Ex1, ••• ,Exm (and, possibly, the 
researchers' commitment to the 'accuracy' of each of the statements 
Ht, ... ,HJ.t 
In the cases described above and, more generally, whenever the external 
estimates Est(q1), •.• ,Est(qJ are available, it seems reasonable to equate the 
marginal means of Dir(y,A.) to the corresponding external estimates: E(q1) = 
Est(q1), ••• , E(qJ = Est(qJ.2 Given the equalities E(qJ = Y; (see formula 
(3.32)) this is tantamount to taking Est(q) = (Est(q1), •.. , Est(qJ) as the 
optimum prior vector y 0=(y~ , .... ,y'!J. 
8.2 GD-CONTEXTS 
As stated at the beginning of this chapter, the analysis of EPO made in 
Sections 3-5 will be made w.r.t. to a particular type of multinomial context 
called GD-contexts. Before defining GD-contexts, the notions of G-context 
and D-context must be introduced. 
A G-context is a multinomial context where, in addition to an external 
estimate Est(q) of q in Ex, an external estimate Est[G(q)] of the Gini 
diversity G( q) = 1 - ~qi can be made. 
Consider, for instance, a multinomial context where the external 
information Eln( q,Ex) in BK is given by the statements H1, ••• ,Hm which 
specify the values of q for the Ex-similar process Ex1, ••• ,Exm (cf. Section 8.1). 
Given the value a'=(aJ, ... ,afJ of q specified by H, where t = 1, ... ,m, the 
corresponding Gini diversity 1 - ~(aD 2 will be referred to as G(a1). If m is 
not too low, the following external estimate Est[G(q)] of G(q) in Ex may be 
made: 
m 
~ G(a') 
(3) 
Est[G(q)] = 
1=1 
m 
Alternatively, the following weighted average could be used: 

106 
CHAPTER 8 
m 
(4) 
Est[G(q)] = 
l: w,G(a') 
,..1 
w 
where w1, ... ,wm and Ware interpreted as in (2V 
In other G-contexts the external estimates Est[G(q)] could be made using 
also the theoretical information contained in Eln(q,Ex) (cf. Chapter 7.5). For 
instance, in ecology, an estimate Est[G(q)] of the Gini diversity of a given 
population could be made using appropriate hypotheses (cf. Chapter 10.3.) 
about the relations between G( q) and other characteristics of the population 
(such as stability) or its environment (such as pollution). 
A D-context is a multinomial context where, in addition to Est(q), an 
external estimate Est[Diy 0,q)] of the distance D2(y 0,q) between the prior 
vector y0 = Est(q) and the true value of q in Ex can be made. 
Consider, for instance, the case where the available external information 
Eln( q,Ex) is given by the above-mentioned statements H1, ••• ,Hm concerning the 
Ex-similar process Ex1, ••• ,Exm. Here, if m is not too low, the following 
external estimate Est[D2(y 0,q)] could be made: 
(5) Est[Db 0,q)] = 
m 
Alternatively, the following weighted average could be used: 
w 
where w1, ••• ,wm and Ware interpreted as in (2) and (4).4 
Now a GD-context can be defined as a multinomial context which is both 
a G- and a 0-context. In other words a GO-context is a multinomial context 
where, in addition to Est(q), both the external estimates Est[G(q)] and 
Est[D2(y 0,q)] can be made. Since Est(q), Est[G(q)] and Est[Da{y 0,q)] are all 
that is needed to choose the optimum prior vector yo (see Section 1) and the 
optimum A.-value A. 0 (see Sections 3-5), it follows that the background 
knowledge BK available in a GO-context is sufficient to identify the optimum 
GC-system (y 0,A. j, i.e., to provide a complete solution to EPO. 

THE EPISTEMIC PROBLEM OF OPTIMAUTY 
107 
In Chapter 7.6, it was seen that the empirical information conveyed by the 
statements H 1, ••• ,Hm can be used, if m is sufficiently high, to decide whether 
the inductive method to be employed within a given empirical research should 
or should not be a GC-system. From (1)-(6) it appears that H 1, ••• ,Hm can also 
be used to derive the external estimates Est(q), Est[G(q)], and Est[Dz(y 0,q)] 
which in turn can be used to solve EPO, i.e. to specify the optimum 
GC-system (y 0,A j 
(see Sections 3-5). This signifies that the same 
experimental information H1, ••• ,Hm used to decide whether a GC-system 
should be adopted can be 'reprocessed' to establish which GC-system should 
be adopted. 
It follows from the requirement of adequacy (Ext)(a) (see Chapter 7.6, p. 
98) that yo= Est(q), Est[G(q)] and Est[D2(y 0,q)] are linked by the following 
equality: 
(7) 
Est[Dz(y 0,q)] = G(yj- Est[G(q)] 
Proof of (7): 
Given the equalities Dz(y 0,q) = k(Y?- q1) 2 andy?= Est(q1), 
it follows from (Ext)(a) that Est[Dz(y 0,q)] = Est[k(y?/ + kqi - 2ky?qd = 
Est(kqi) + k(y?)2 - 2ky?Est(q1) 
= Est(kqi) - k(y?)2 = [1- k(y?)2] 
-
[1 -Est(kqi}]. From this sequence of equalities, and taking into account that 
[1-k(y?)2] = G(yj and [1 - Est(kqi}] = Est(1 - kqi} = Est[G(q)], it follows 
that Est[Diy 0,q)] = G(yj - Est[G(q)], Q.E.D. 5 
In GD-contexts the external estimates Est(q), Est[G(q)], and£st[D2(y 0,q)] can 
be made using specific procedures such as those defined in (1)-(6). The 
equality (7) can be seen as a 'constraint', or requirement of adequacy, on such 
procedures. In particular, it can be proved that such a constraint is satisfied 
by the procedures defined in (1), (3), and (5).6 Analogously, it is also 
satisfied by the 'weighted' external estimates suggested in (2), (4), and (6).7 
On the other hand, in those G-contexts where no 'direct procedure' for the 
external estimate of Diy 0,q) is available the equality (7) provides an 'indirect 
procedure' for deriving Est[D2(y 0,q)] (see remarks below (Ext), p. 98). This 
implies that any G-context is also a D-context and, consequently, a 
GD-context. Similarly, any D-context is also a GD-context.8 Hence, the 
distinction between G-contexts, D-contexts and GD-contexts is only a prima 
facie distinction since all these contexts are, indeed, GD-contexts.9 

108 
CHAPTER 8 
8.3 THE CC-SOLUTION TO EPO 
The CC-solution to EPO for GD-contexts is based on the requirement of 
cognitive coherence (CC). 
As it was suggested in the last paragraph of Section 1, whenever the 
external estimates Est(q1), ••• ,Est(qJ are available, the marginal means of 
Dir(y,A) can be equated to the corresponding external estimates: E(q1) = 
Est(q1), ..• , E(qJ = Est(qJ where - given the equalities E(qJ = Yi - this is 
tantamount to taking Est( q) = (Est( q1), ••• , Est( qJ) as the optimum prior vector 
y 0=(y~ , .... ,y'J. In note 1 it was pointed out that this procedure for the 
selection of the optimum prior vector yo can be seen as an application of the 
requirement of cognitive coherence (CC). 
One can show that in a GD-context - given a certain value of yo = Est( q) -
the requirement (CC) can be used also for solving EPO, i.e., for determining 
the optimum prior (and the related optimum A-value) within the set DIR(yj 
of Dirichlet distributions Dir(y 0,A) with prior vector y0 • 
Indeed, given the external estimate Est[G(q)] available in a certain 
GD-context, the requirement of cognitive coherence (CC) imposes the 
following m-constraint Cc on Dir(y 0,A): Cc = "E[G(q)] = Est[G(q)]". Taking 
account of the equality A= E[G(q)]/(G(yj- E[G(q)])- derived from formula 
(7.1) by replacing a andy with A and yo- the constraint Cc implies that A is 
given by: 
(8) 
A = 
Est[G(q)] 
G(yj - Est[G(q)] 
Similarly, given the external estimate Est[Dz(y 0,q)] of Dz(y 0,q), (CC) imposes 
the following m-constraint Cv on Dir(y 0,A): Cv = "E[Dz(y 0,q)] = Est[D2(y 0,q)]". 
Taking account of the equality A= (G(yj- E[Dz(y 0,q)])/E[Dz(y 0,q)]- derived 
from formula (7.2) by replacing a and y with A and yo - the constraint Cv 
implies that A is given by: 
(9) 
A = 
G(yj - Est[Dz(y 0,q)] 
Est[D2(y 0,q)] 
Note that the application of (CC) in GD-contexts cannot lead to the 
acceptance of two incompatible constraints Cc and Cv which 'overdetermine' 
the problem of identifying the value of A. Indeed, it directly follows from 
equality (7) that formulae (8) and (9) should provide the same A-value which 

THE EPISTEMIC PROBLEM OF OPTIMAUTY 
109 
will hence be referred to as the CC-optimum 'A-value Ace· Note that the value 
of Ace can be also expressed by the following more simple formula: 
(10) 
Formula (9) shows that, given a certain value of G(yj, when Est[Db 0,q)] 
increases then the CC-optimum value Ace decreases.10 The intuitive 
plausibility of this behaviour can be explained as follows. 
Firstly, given a certain value of G(yj, consider why, if Est[D2(y 0,q)] is low, 
it is reasonable to select a high value of Ace· Recalling that Est[D2(y 0,q)] can 
be seen as the estimate - based on the available background knowledge BK -
of the distance from the truth, or 'inaccuracy', ofy 0 (cf. note 4), a low value 
of Est[Dz(y 0,q)] amounts to guessing that yo= Est(q) is pretty accurate, i.e., 
very near to the true value of q (the truth). Now it appears perfectly natural 
to represent this guess by selecting - within the set DIR(yj of Dirichlet 
distributions Dir(y 0,A) with prior vector yo- a distribution Dir(y 0,Acc) with a 
high value of Ace· This prior Dirichlet distribution, indeed, attributes a very 
large amount of the whole probability to the region around the optimum prior 
vector yo. 
Secondly, given a certain value of G(yj; consider why, if Est[Dz(y 0,q)] is 
high, it is reasonable to select a low value of Ace· According to the above 
mentioned interpretation of Est[Dz(y 0,q)] a high value of Est[Dz(y 0,q)] 
amounts to guessing that yo = Est( q) is pretty inaccurate, i.e., quite far from 
the true value of q (the truth). Now it appears perfectly natural to represent 
this guess by selecting - within the set DIR(yj - a distribution Dir(y 0,Acc) 
with a low value of Ace· In fact this prior Dirichlet distribution does not 
attribute a very large amount of probability to the region around yo but, on 
the contrary, spreads out the whole probability in a relative uniform manner 
over all the possible values of q. 11 
8.4 THE LOGICAL PROBLEM OF OPTIMALITY 
Some general considerations 
Suppose that, within an empirical research about a given parameter (vector) 
9, a researcher X intends to 
(i) 
perform an experiment y; 

110 
CHAPTER 8 
(ii) 
apply an estimator T for estimating 0 on the basis of the possible 
results y of y (see Chapter 4.1), and 
(iii) 
use a distance function D(T(O I y),O) as a measure of his cognitive 
failure in the case where 0 = 0 and y = y. 12 
Assume that 0 and y are linked by a probability function P Jy) which 
represents the physical (or objective) probability of the occurrence of y when 
y is performed in the case where 0 = 0. Then, given a value 0 of 0, the 
expected (w.r.t. P,jy)) distance E(D(T(O I y),O) I 0) is given by: 
(11) 
E(D(T(O I y),O) I 0) = J 
P,jy) D(T(O I y),O) dy 
The quantity E(D(T(O I y),O) I 0) can be seen as a measure of the prospect of 
(cognitive) failure of the estimator T in the state of nature 0 relative to the 
experiment y. Note that E(D(T(O I y),O) I 0) does not concern the beliefs of X-
or of any other researcher - about 0, since it does not involve any epistemic 
probability but only the physical probability P Jy). Rather, E(D(T(O I y),O) I 0) 
may be seen as the description of a particular aspect of the 'structure of the 
world', i.e., the actual, or objective, prospect of failure of the estimator Tin 
the state of nature 0, relative to the experiment y. In other words, 
E(D(T(O I y),O) I 0) is not an epistemic notion but an ontic notion concerning, 
so to speak, an 'objective feature' of the triple T, 0, y. 
Given a triple (T,O,y), where T is a given class of estimators, T' E T can 
be defined as the optimum estimator relative to (T,O,y) iff it minimizes the 
prospect of failure E(D(T(O I y),O) I 0). Analogous to E(D(T(O I y),O) I 0), the 
notion of optimum estimator T' is a non-epistemic notion referring to an 
'objective feature' of (T,O,y). Such a notion could be seen as a logical notion 
because it is defined exclusively w.r.t. the 'logical relations' among T, 0, and 
y. Accordingly, the problem of identifying the optimum estimator T' for a 
given triple (T,O,y) will be called the logical problem of optimality, or LPO, 
for (T,O,y). 
The intuitive meaning ofLPO may be elucidated from the following 'quasi-
theological' situation. Imagine that: 
(i) Researcher X, in his empirical inquiry on 0, intends to perform an 
experiment y and to use an estimator selected from a given class T. 
(ii) You are a semi-omniscient demigod in the sense that You know the 
true value 0 of 0 and, although not knowing which particular value of 

THE EPISTEMIC PROBLEM OF OPTIMAUTY 
111 
y will be observed by X, You do know the (true) physical probability 
PJ.y). 
(iii) You know the distance function D(T(O I y),Q) used by X as a measure 
of cognitive failure and, consequently, You know the prospect of 
cognitive failure E(D(T(O I y),Q) I 0) of any estimator T E T in the 
actual state of nature 0. 
(iv) Being benevolent towards X- as a demigod should be -You want to 
help him in pursuing his cognitive goals. Suppose that, for some 
reason, You cannot reveal to X the true value of 0 but You can only 
suggest to him to use a given estimator from T. Which one would 
You suggest? It seems clear that You would suggest to X the estimator 
r which minimizes the prospect of failure E(D(T(O I y),O) I Q) in the 
actual state of nature. 
(v) Accordingly, the following 'quasi-theological' interpretation of the 
logical notion of optimum estimator could be proposed: the optimum 
estimator T' relative to (T,O,y), is that which would be suggested by 
a semi-omniscient and benevolent demigod to a researcher who intends 
to use the result of the experiment y for estimating 0. 
A solution to LPO for certain triples (T,O,y) may be theoretically interesting. 
But what of its practical relevance? In other words: may a solution to LPO 
help to solve the 'corresponding' EPO, i.e., the problem of selecting, among 
the estimators in T, the 'epistemically optimum' for the empirical inquiry on 
0 made by X? 
Of course, if X knew the logically optimum estimator for the actual state 
of nature, he would select it as the epistemically optimum estimator for his 
inquiry on 0. However, in the absence of a benevolent and semi-omniscient 
demigod, X cannot identify the logically optimum estimator w.r.t. the actual 
state of nature but can only establish, by solving LPO, the logically optimum 
estimator for any value Q of 0, i.e., for any possible state of nature. 
Although solving LPO does not directly offer a solution to EPO, in certain 
multinomial contexts it is a crucial element in the formulation of an adequate 
solution to EPO (see Section 5). 
Logically optimum Dirichlet distributions 
Consider the prior distribution F(q1, ... ,qk_1) on the parameter vector 
q=( q1, ••. ,qJ of a multivariate Bernoulli process Ex and the experiment e0 
which consists in giving a complete description of a sequence of n trials of 
Ex, where en denotes a possible result of en. Moreover, let E be the 

112 
CHAPTER 8 
F(q1, ... ,qk_1)-based M-estimator which takes, for any en, the mean vector 
E(q I en)=(E(q1/en), ... , E(qJen)) of the posterior distribution F(q1, ... ,qk_tfen) as 
the point estimate of q (see Chapter 4.2). Here it is assumed that 
F(q1, ... ,qk.tfen) is derived from the prior distribution F(q1, ... ,qk_1) and the 
likelihood function p(en lq) = pq (en) where pq (en) = n if/ represents the 
physical (or objective) probability of en (see formula (2.2)). 
Then, given a prior distribution Dir(y,A.) on q, the Dir(y,A.)-based 
M-estimator will be referred to as E~.. and the Dir(y,A.)-based M-estimates of 
q as E~..(q I en)=(E~..(q1/en), ... , E~..(qJen)). 
Now suppose that X, in an attempt to estimate the value of q, intends to 
(i) 
perform experiment eo; 
(ii) 
apply the Dir(y,A.)-based M-estimator E~.. for estimating q on the 
basis of the possible results en of eo, and 
(iii) 
use the quadratic distance DiE~..( q I en ),q) = ~[E~..( q1 len) - q;]2 as 
a measure of his cognitive failure in the case where q = q 
and eo= en. 13 
Given a value q of q, the expected (w.r.t. Pq (en)) quadratic distance 
E(DiE~..(q I e0 ),q) I q) can be seen as a measure of the prospect of (cognitive) 
failure of the M-estimator E~.. in the state of nature q relative to the 
experiment eo. 
Now it follows from (11) and (3.16.b) that E(DiE~..(q I eo ),q) I q) is given 
by: 
k 
(12) 
E(DiE~..(q I e0 ),q) I q) = E[ L (E~..(q1
l eo)- q;)2 1 q] 
i=1 
k 
= L E[(E~..(q,l eo)- qi I q] 
i=l 
k 
= L (L Pq(en) (E~..(q1
l eo)- q;)~ 
i=1 
where the inner sum is the expected (w.r.t Pq(en)) value of (E~..(<L I eo)- qy 
for a given value q of q. Note that the inner summation extends over all 
possible results en of eo. 
The quantities DiE~..(q I eo ),q), (E~..(q,l eo) - qi )2, 
E~..(q1
l eo) - qi and 
(E~..(q1
l eo) occurring in (12) are en-related statistics. In particular, E~..(q1
l eo)-

THE EPISTEMIC PROBLEM OF OPTIMAUTY 
113 
qi and (E,_(q1l en)- qy are called the error and the square error, respectively, 
of the point estimate E,_( q1 I en) relative to the value qi of 'L· 
The sampling distributions of the above statistics can be calculated from 
the probability distribution Pq (en) (see (2.2)). In particular, it follows from 
(3.20.b) and (3.20.c) that the expected value E[(E,_(q1 l en)- q;)2 1 q] is: 
(13) 
E[(E,_('L I en) - q;)2 1 q] 
= var(E,_(q,l en) I q) + [E(E,_(q1l en) - q;) I q]2 
To calculate [E(E,_(q,l en) - qi )) I q]2 remember that E,_(q,l en) is a linear 
function of ni (cf. (6.4)): 
(14) 
In fact it follows from (14), (3.16.a) and (3.24), that the mean E[E,_('L I en) I q] 
of E,_( q1 I en) is given by: 
(15) 
Given (15) and (3.16.a), the following formula for the expected value 
E(E,_( q1 I en) - qi I q) of E,_( q1 I en) - qi can be obtained: 
(16) 
Moreover, it follows from (14), (3.20.c) and (3.25) that the variance 
var(E,_( 'L I en) I q) of£,_( q1 I en) - qi is given by: 
var(E,(q, I e.) I q) = var[ 
1 
A.yi ] 
(17) 
n, + 
n+A. 
n+A. 
1 
nqi(1- q'J 
= 
varq(n1) = 
(n + A. )z 
(n + A.)z 

114 
CHAPTER 8 
Given (13), (16) and (17), the following formula can be obtained: 
(18) 
Finally, it can be deduced from (12) and (18), after a number of routine 
calculations, that: 
(19) 
E(D2(EA( q I en ),q) I q) 
== 
n(1 - L r/i) + J....ZL (Y; - qy 
(n + I,Y 
Recalling that G(q) 
== (1 - l:qD (see Chapter 3.4) and that Dz(y,q) ::: 
l:(y; - qi )2 (see Chapter 4.1) formula (19) can be rewritten thus: 
(20) 
E(D (E ( I e ) ) I ) == 
nG(q) + J.}Dz(y,q) 
2 
A q 
n ,q q 
(n + A)2 
Formula (20) reveals two positive aspects of E(Dz(EA(q I en),q) I q). Firstly, as 
would 
be 
natural 
to 
expect, 
the 
prospect of cognitive 
failure 
E(D2(EA(q I en ),q) I q) of theM-estimator EA in the state of nature q, relative 
to the experiment en, depends on n, /..., the prior vector y=(y1, ... ,yJ and the 
'objective vector' q==(q1, ... ,qJ. Secondly, E(Dz(EA(q I en),q) I q) depends on y 
and q only via the Gini diversity G(q) and the quadratic distance Dz(y,q). 
Consider the set DIR(y) of the Dirichlet distributions Dir(y,A) with prior 
vector y, and the corresponding set EY containing the Dir(y,A)-based 
M-estimators EA. To solve LPO for a given triple (Ey,q,en) the M-estimator 
which minimizes the prospect of failure E(Dz(EA(q I en),q) I q) in Ey must be 
identified. In other words, the solution to LPO for (Ey,q,en) is theM-estimator 
P with the A-value A~ which minimizes [nG(q) + A2Dz(y,q)]l(n + A)2, i.e., the 
value of A for which the A-derivative of this expression is 0. This leads to the 
conclusion that the logically optimum A-value A~ is given by: 
(21) 
A~ == 
G(q) 
Dz(y,q) 
The prior distribution Dir(y,A~) will be called the logically optimum Dirichlet 
within DIR(y). 

1HE EPISTEMIC PROBLEM OF OPTIMALITY 
115 
Formula (21) reveals a positive feature of!.}, i.e., the optimum value f.} 
does not 
depend on 
n even 
though the 
prospect 
of 
failure 
E(Dz(E;..(q I en ),q) I q) of any M-estimator E;.. does depend on n (cf. (20)). 
Therefore the A.-value A. 4 optimum for a given experiment en is optimum tout 
court. 
Formula (21) also shows that A4 is directly proportional to G(q) and 
inversely proportional to Dz(y,q). An 'intuitive interpretation' of this 
behaviour is given below. 
Firstly, given a certain value of G(q), consider why if Dz(y,q) is low it is 
reasonable to expect A4 to be high. Remember that if, and only if, A. is high, 
E;..(q1l en) tends to have a value close toY; (cf. (6.4)). This signifies that an 
M-estimator E;.. with a high A.-value is 'rigid' in the sense that the point 
estimates E;..(q I en) tend to have values close to the prior vector y. It is clear 
that if y and q are near to each other a rigid M-estimator helps to make 
estimates approaching q. In other words, if there is a small distance Dz(y,q) 
between y and q, then only a high A.-value can be a 'good instrument' for 
approaching the truth (the true value q of q). Hence, only a high A.-value can 
be the 'optimum instrument' A4 for approaching the truth. 
Secondly, given a certain value of D 2(y,q), consider why if G(q) is low it 
is reasonable to expect t...• to be low. Remember that if, and only if, A. is low, 
E;..(q1l en) tends to approach nJn (cf. (6.4)). This signifies that anM-estimator 
E;.. with a low A.-value is 'flexible' in the sense that the point estimates 
E;..(q!en) tend to approach the empirical vector v (en)=(ntfn, ... ,nJn). It is clear 
that if there is a high physical probability that the empirical vectors v (en) 
related to the experimental results en of en are near to q, then a flexible 
M-estimator helps to make estimates approaching q. More precisely, if the 
expected (w.r.t. Pq(en)) value E(Dz(v(en),q) I q) of the distance Dz(v(e0 ),q) 
is low, then only a low A.-value can be a 'good instrument' for approaching 
the truth. Now it can be proved that E(Dz(v(e0 ),q) I q) is directly proportional 
to G(q): 14 
(22) 
G(q) 
E(Dz(v(e0 ),q) I q) = --
n 
This implies that, if there is a small Gini diversity G(q), then only a low 
A.-value can be a 'good instrument' for approaching the truth (the true value 
q of q). Hence, only a low A.-value can be the 'optimum instrument' A4 for 
approaching the truth. 

116 
CHAPTER 8 
8.5 THE V-SOLUTION TO EPO 
Suppose that the cognitive context C of researcher X investigating q is a 
GO-context. Solving EPO for C is equal to determining the optimum prior 
distribution within the set DIR(yj of Dirichlet distributions Dir(y 0,A) with 
prior vector yo(= Est(q)). 
A verisimilitude solution, or V -solution, to EPO for C may be based on a 
suitable combination of the above V -solution to LPO and the verisimilitude 
thesis (VER) which takes the achievement of a high degree of verisimilitude 
as the aim of pure science (see Chapter 1.1). Indeed it is a basic assumption 
of the V-solution to EPO that X should use the external estimates Est[G(q)] 
and Est[Dly,q)] available in C in an attempt to approach the truth. 
In particular, the following two step V -solution to EPO will be referred to 
as V1: 
(i) X assumes that G(q) = Est[G(q)] and D2(y 0,q) = Est[Db 0,q)] and, 
accordingly, 
(ii) X takes the distribution in DIR(yj, which would be logically 
optimum if the above assumptions were true, as the epistemically 
optimum prior. 
The epistemically optimum prior suggested by V1 will henceforth be referred 
to as Dir(y 0,A~) where A~ is the Vcoptimum A.-value. The value of A~ can be 
calculated as follows. 
Given (20), the prospect of cognitive failure E(DiE.iq I en ),q) I q) of a 
Dir(y 0,A)-based M-estimator in q depends exclusively on G(q) and Diy 0,q) 
and in particular: 
(23) 
The value of E(D2(E>..( q I en ),q) I q) in a 'possible world' q where 
G(q) = Est[G(q)] and D2(y 0,q) = Est[D2(y 0,q)] is given by the 
formula: 
nEst[G(q)] + A.2Est[D2(y 0,q)] 
(n + A.)2 
From formulae (20) and (21) it follows that the A.-value A~ which minimizes 
E(DlE>..(q I en ),q) I q) in (23) - i.e., the logically optimum value of A for a 
value of q where G(q) = Est[G(q)] and D2(y 0,q) = Est[Dly 0,q)]- is given by: 

THE EPISTEMIC PROBLEM OF OPTIMAUTY 
117 
(24) 
Est[G(q)] 
Formula (24) reveals that the epistemically optimum value A.~ is independent 
of n. Therefore X's choice of the optimum prior distribution is independent 
of the length n of the experiment en to be performed. 
Formulae (24) and (14) illustrate that, somewhat surprisingly, the 
Vcoptimum A.-value A.~ is identical to the CC-optimum A.-value Ace· While at 
this stage of the analysis the identity A.~ = Ace hardly appears more than a 
lucky coincidence, the reasons for this will be explained later (see Section 6). 
It could be argued that V1 neglects the inevitably conjectural nature of any 
estimate. In fact step (i) or" V1 seems to suggest that X is certain that the 
external estimates Est[G(q)] and Est[Dz{y 0,q)] are correct. However, this 
conflicts with the generally held idea that any inductive estimate is no more 
than reasonable conjecture. 
Even though V1 may be considered as somewhat 'naive', the V-approach 
to EPO need not be dismissed. Indeed a more 'sophisticated' V -solution to 
EPO - henceforth referred to as V2 - can be obtained. 
For this purpose remember that, given an arbitrary distribution Dir(y 0,A) E 
DIR(y"), the prospect of failure of the Dir(y 0,A)-based M-estimator E~.. in a 
possible state of nature q (a possible value q of q), relative to a certain 
experiment en, is referred to as E(Dz(E~..(q I en),q) I q). Accordingly, an external 
estimate of the prospect of failure of E~.. in the unknown actual state of nature 
(the true value of q) may be referred to as Est[E(Dz(E~..(q I en),q)]. 
As in V1, in V2 it is assumed that a researcher X should use the external 
estimates Est[G(q)] and Est[Dz(y,q)] available in the GO-context C of his 
empirical research on q in his attempt to approach the truth. More 
specifically, according to the V-solution V2 to EPO the following two step 
procedure should be adopted: 
(i) X uses Est[G(q)] and Est[Dz(y 0,q) to derive an external estimate 
Est[E(D2(E~..(q I en),q)) for any distribution Dir(y 0,A) E DIR(y"), 
(ii) 
X then takes the distribution in DIR(y") which minimizes the above 
mentioned external estimate as the epistemically optimum prior. 
Contrary to how it appears, step (i) is not particularly problematical since it 
follows from (20) and (Ext)(a) (Chapter 7.6, p. 98) that the value of 
Est[E(Dz(E~..(eJ,q)] is given by: 

118 
(25) 
CHAPTER 8 
nEst[G(q)] + t-.2Est[Dz(y 0,q)] 
(n + '-Y 
Formula (25) shows that the external estimate Est[E(Dz(E1..(q I e0 ),q)] of the 
prospect of failure of E'A in the 'actual world' is identical to the prospect of 
failure E(Dz(E'A(q I eo ),q) I q) of E'A in a 'possible world' q where G(q) = 
Est[G(q)] and Dz(y 0,q) = Est(Dz(y 0,q)] (see (23)). From this it follows that the 
(unique) V2-optimum A.-value minimizing Est[E(D2(EJ..(q I e0 ),q)], which may 
be referred to as 
A~, is identical to the A.-value 
A~ minimizing 
E(Dz(E')..(q I eo ),q) I q) (see (24)): A~= A~= Est[G(q)]/Est[Dz(y 0,q)]. 
This signifies that the sophisticated V -solution V2 and the naive V -solution 
V1 are two different formulations of a unique V -solution to EPO for 
GD-contexts. The V-optimum A.-value A.~= A~ may be referred to as A~. 
8.6 THE EQUIVALENCE BETWEEN THE V-SOLUTION 
AND THE CC-SOLUTION TO EPO 
Although, as remarked above, the equality "-cc = A~ may appear somewhat 
fortuitous, it can be shown that it is a necessary consequence of the fact that 
the V- and the CC-solutions to EPO are, contrary to appearances, logically 
equivalent. An outline of my argument is as follows: 
(i) the V2-optimum A.-value A~(= A.~) is defined as the (unique) A.-value 
minimizing Est[E(Dz(EJ..(q I eJ,q)] within the set of Dir(y 0,A)-based 
M-estimators E'A (see above); 
(ii) the 
definition 
of the 
CC-solution 
to 
EPO 
implies 
that 
Est[E(Dz(E,.( q I e0 ),q)] is also minimized by the CC-optimum 
A.-value "-c6 
(iii) therefore Ace is necessarily identical to A~. 
The core of the argument is represented by point (ii) which can be proved 
using theorem ( 4.12) which concerns the immodesty of any prior distribution 
on q. 
Before developing the argument in detail, three lemmas must be proved. 
Given a value q of q, an experiment e0
, a prior Dir(y 0,A') on q, the 
expected (w.r.t. Pq (en)) quadratic distance between the Dir(y 0,A')-based 
M-estimate E'A. (q I e0 ) and q is referred to as E(Dz(E'A. (q I eo ),q) I q) (see 
Section 4). Moreover, given a prior Dir(y 0,A) on q, the expected (w.r.t. 
Dir(y 0,A)) 
value of E(Dz(E'A. (q I eJ,q) I q) will 
be referred to 
as 

THE EPISTEMIC PROBLEM OF OPTIMAUT)' 
119 
E~..[E(Dz(E;.: (q I en ),q) I q)]. Lastly, the expected value of the quadratic 
distance Dz(E~... ( q I en ),q) - w .r.t. the joint probability distribution on ( q,eJ 
derived from Dir(y 0,A) and the likelihood function p(en !q) = Pq (en)) - is 
referred to as E~..[D2(E...(q I en ),q)]. 
The first lemma states the following equality: 
Equality (26) is a particular case of (3.19)_15 
Given the CC-optimum prior Dir(y 0,Acc) and the correspondingM-estimator 
Ecc• the second lemma states that: 
whereEcc[DzCEcc(q I en),q)] andEcc[Dz(E~...(q I en),q)] are the expected values 
of DzCEcc(q I en),q) andDz(E~...(q I en),q) calculated w.r.t. the joint probability 
distribution on (q,en) derived from Dir(y 0,Acc) andp(en/q) = Pq(en)· 
Inequality (27) is a particular case of ( 4.12). The intuitive meaning of (27) 
is that Dir(y 0,Acc) is immodest as is any other prior distribution on q (cf. 
(4.10)). 
Lastly, the third lemma can be stated thus: 
(28) 
nE~..[G(q)] + (A.')2E~..[Dz(y 0,q)] 
(n + A.')2 
where E~..[G(q)] and E~..[Dz(y 0,q)] denote the expected (w.r.t. Dir(y 0,A.)) values 
of G(q) and Dz{y 0,q). 
Proof of (28): 
By replacing A. with A.' in formula (20) the equality 
EDz(E~...(q I en),q) = [nG(q) + (A.')2Dz(y 0,q)]/(n +A. ')2 is obtained. From this 
equality and from the property (3.16.a) of expectations, it follows that 
E~..[E(Dz(E~...(q I en),q) I q)] = [nE~..[G(q)] + (A.') 2E~..(Dz(y 0,q)]/(n +A. ')2, Q.E.D. 
My argument develops along these lines. 
It follows from (28) that the expected (w.r.t. Dir(y 0,Acc)) value 
Ecc[E(D2(E~...(q I eJ,q) I q)] of the prospect offailureE(Dz(E~...(q I eJ,q) I q) of 
an arbitrary Dir(y 0,A.')-based M-estimator E~... is given by: 

120 
CHAPTER 8 
where Ecc denotes the expectation w.r.t. Dir(y 0,Acc)· 
Remember that the definition of the CC-solution implies that Ecc[G(q)] = 
Est[G(q)] and Ecc[Dz(y 0,q)] = Est[Dz(y 0,q)] (see Section 3). Hence, equality 
(29) can be rewritten as follows: 
(30) 
nEst(G(q) + (f...')2Est[D (yo q)] 
E [EW (E .( I e ) q) I q)] = 
2 
' 
cc ' 2 1. q 
o ' 
(n + /...')2 
By 
replacing 
A. 
with 
A.' 
in formula (25) 
the 
external estimate 
Est[E(Dz(E~... (q I e0 ),q)] of the prospect of failure E(Dz(E~...(q I e0 ),q) of E~... in 
the 'actual world' is obtained thus: 
(31) 
nEst[G(q)] + (A.')2Est[Dz(y 0,q)] 
Est[E(Dz(E~..(q I e0 ),q)] = ---------
(n + A.Y 
Formulae (30) and (31) imply that: 
Note that two particular cases of (26) are given by: 
and 
From these equalities and (27) it follows that: 
Inequality (33) signifies that Ecc[E(Dz(E~...(q I e0 ),q) I q)] is minimized by A.' 
= Ace· Given (32), Est[E(Dz(E~... (q I e0 ),q)] is also minimized by A.' = Ace· 
Recalling that the V2-optimum A.-value A.~ (= A.~) is defined as the (unique) 
A.-value minimizing Est[E(Dz(E~... ( q I e0 ),q)] (see the remarks below (25)), this 
implies that Ace is necessarily identical to A.~. 

THE EPISTEMIC PROBLEM OF OPTIMAUTY 
121 
8.7 CARNAP'S OPTIMUM INDUCfiVE METHODS 
Given the equivalence - mentioned in the introductory paragraphs - between 
a Dirichlet prior Dir(y,A.) and the corresponding GC-system (y,A.), my 
V-solution to LPO for DIR(y) (see Section 4) can be reinterpreted as a 
V-solution to LPO for the set GC(y) of GC-systems with prior vector y. 
Two principal objectives of this section are: (i) to provide a 'verisimilitude 
re-interpretation' of Camap's approach (1952) to optimum C-systems and (ii) 
to show that my V-solution to LPO for GC-system can be seen as a 
generalization of Camap's solution to LPO for C-systems. 
My V-solution equates the optimum A.-value to A, A= G(q)!Db,q) (see (21)). 
In particular, regarding the set DIR(1/k) of symmetrical Dirichlet distributions 
-or, equivalently, the set GC(l!k) of C-systems- the value of A, A is derived 
from (21) by replacing y with the indifference vector 1/k=(1lk, ... ,1/k): A,A = 
G(q)IDillk,q). Given the equalities G(q) = 1 - 'l:q7 = 1 - C(q) and Di1/k,q) 
= 'l:(q;- llk? = 'l:q7- 1/k = C(q) - 11k, the formula for A,A can be rewritten as 
follows: 
(34) 
(1 - L q'f) 
1 - C(q) 
= 
(L trl- 1lk) 
C(q) - 1lk 
Formula (34) is identical to the formula for the optimum A.-value found by 
Carnap (1952, p. 69).16 Below it is shown that this identity derives from the 
conceptual relationships existing between my approach to LPO and that of 
Carnap. 17 
Let E~.( ~len) = p~.(QJen) be the expected value of the objective probability 
~ of Q; in a given population, where p~.(Q; len) is a special value of the 
C-system (1/k,A.) and en is a possible result of the experiment en. Carnap 
refers to the estimates E~.( q1 len), relative to the possible results of en , and 
proposes the following four step strategy to identify the (logically) optimum 
C-system: 
(S.1) 
A measure ser(E~.( ~I en ),q;) of the seriousness of the error made 
by E~.(q1
l en) w.r.t. a value q of q is established. 
(S.2) 
The (local) Q; -prospect of failure of (1/k,A.) in q, relative to en, is 
defined as the expected (w.r.t. Pq(en)) valueE(ser(E~.(q1
l en),q;) I q) 
of ser(E~.( ~ I en ),q; ). 
(S.3) 
The (global) prospect of failure of (1/k,A.) in q, relative to en, is 
defined as the arithmetic average 'l:E(ser(E~.( q1 I en ),q;) I q)lk of the 
k Q; -prospects of failure. 

122 
CHAPTER 8 
(S.4) 
The optimum A.-value in q, relative to e.,, is defined as the A.-value 
minimizing the global prospect of failure. 
The following 'verisimilitude re-interpretation' of Carnap's strategy is here 
suggested. The seriousness of the error ser(E).( <L I en ),q;) can be seen as a 
measure of the local distance between the estimate E).( q1 I en ) and the true 
value q; of <L· Accordingly, the optimum A.-value may be seen as the A.-value 
minimizing the arithmetic average l'.E(ser(E).( <L I e.,),q;) I q)lk of the expected 
local distances E(ser(E).(<L I e.,),q;) I q). 
To apply Carnap's strategy, (S.1) should be specified by selecting an 
appropriate definition of ser(E).(<L I en),q;). In particular, Carnap (ibid., pp. 61-
62) defines ser(E).( <L I en ),q;) as the square error of E).( <L I en) relative to the 
value q; of <1J: 18 
It follows from (35) - in combination with (S.2), (S.3) and (3.16.b) - that the 
prospect of failure l'.E(ser(E).(<L I e.,),q;) I q)/k in (S.3) is equal to the expected 
average square error: 
Note that the average square error l:(E).(q1l en) - q; )2/k is a simple linear 
transformation of the quadratic distance D2(E).( q I en ),q) = l:(E).( <L I en) - q; )2 
used in my V -approach to LPO. Hence, E(l:(Ei q1l e.,) - qYik I q) is a simple 
linear transformation of E(DiE).(q I e.,),q I q): accordingly both quantities are 
minimized by ').." = (1 - l:c/[)l(l:c/i- llk). This signifies that my V-solution to 
LPO for GC-systems is logically equivalent to a generalization of Carnap's 
solution to LPO for C-systems. 19 
However, there are also some important differences between my 
V -approach to LPO and that of Carnap. 
Firstly, while my V-solution to LPO for the set of symmetrical Dirichlet 
distributions and the corresponding set of C-systems is worked out by making 
a substantial usage of the mathematical properties of Dirichlet distributions, 
Carnap fails to mention Dirichlet distributions at all. 
Secondly, while my V-solution to LPO is based on the explicit adoption of 
a verisimilitude view of the aims of science, Carnap does not state any formal 
notion of verisimilitude and, as a matter of fact, this term does not appear in 
The Continuum of Inductive Methods (Carnap, 1952).20 

CHAPTER 9 
THE CONTEXTUAL APPROACH TO EPO: 
COMPARISONS WITH OTHER VIEWS 
In this chapter some features of the contextual view of optimum inductive 
methods (optimum prior distributions) will be elucidated by comparing this 
view with others. 
In Section 1 it will be seen that, underlying Camap's oscillations between 
subjectivism and apriorism, there is a constant assumption, i.e., his 
universalistic view according to which a person at any given time should 
select a unique epistemically optimum inductive method for all those 
scientific and practical problems which he has to deal with. 
In Section 2 the solutions to EPO for C-systems proposed by Lewis (1972) 
and Kuipers (1986) will be examined. They are seen as different formulations 
of a hyperempiricist view where the experimental evidence en alone is 
sufficient to identify the optimum C-system. 
In Section 3 the presupposition view of induction will be considered. 
According to this view the selection of the inductive method should reflect 
certain presuppositions regarding the degree of order of the 'sector of the 
world' under examination. 
Finally, in Section 4 the verisimilitude view, according to which an 
inductive method should help to approach the truth, will be examined. 
From the above mentioned comparisons it results that the contextual view 
differs greatly from both the universalistic view and the hyperempiricist view, 
while it can be seen as a development and application of both the 
presupposition and verisimilitude views. 
9.1 THE UNIVERSALISTIC VIEW 
Regarding the choice of the inductive method Carnap (1952, p. 53), points out 
that 
... the question as to which of the available methods a man X ought to choose ... is 
fundamentally not a theoretical question. A possible answer to a theoretical question is an 
123 

124 
CHAPTER 9 
assertion; as such it can be judged as true or false, and, if it is true, it demands the assent of 
all. Here, however, the answer consists in a practical decision to be made by X. A decision 
cannot be judged as true or false but only as more or less adequate, that is, suitable for given 
purposes. However, the adequacy of the choice depends, of course, on many theoretical 
results concerning the properties of the various inductive methods; and therefore the 
theoretical results may influence the decision. Nevertheless, the decision itself still remains 
a practical matter, a matter of X making up his mind, like choosing an instrument for a 
certain kind of work. [Author's italics 1 
Carnap (ibid., pp. 54-55) develops his comparison between inductive methods 
and instruments as follows: 
Suppose that X has chosen a certain inductive method and used it during a certain period for 
the inductive problems which occurred. If, in view of the services this method has given him, 
he is not satisfied with it, he may at any time abandon it and adopt another method which 
seems to him preferable. This is not the same as a change in method from problem to 
problem. Once he adopts the new method, he will apply it to all inductive problems, problems 
of confirmation for all kinds of hypotheses ... One inductive method is here envisaged as 
covering all inductive problems . ... An inductive method is ... an instrument for the task of 
constructing a picture of the world on the basis of observational data and especially of 
forming expectations of future events as a guidance for practical conduct. X may change this 
instrument just as he changes a saw or an automobile, and for similar reasons. If X, after 
using his car for some time, is no longer satisfied with it, he will consider taking another one, 
provided that he finds one that seems to him preferable. Relevant points of view for his 
preference might be: performance, economy, aesthetic satisfaction, and others. Similarly, after 
working with a particular inductive method for a time, he may not be quite satisfied and 
therefore look around for another method. He will take into consideration the performance 
of a method, that is, the values it supplies and their relation to later empirical results, e.g. 
the truth-frequency of predictions and the error of estimates; further, the economy in use, 
measured by the simplicity of the calculations required; maybe also aesthetic features, like 
the logical elegance of the defmitions and rules involved .... Here, as anywhere else, life is 
a process of never ending adjustment; there are no absolutes, neither absolutely certain 
knowledge about the world nor absolutely perfect methods of working in the world. [Author's 
italics 1 
Some remarks are called for. Firstly, the choice of an inductive method is 
considered by Carnap as a ''practical decision". In one sense, Carnap is right 
since the adoption of an inductive method may have many practical 
consequences: for instance, the predictive probabilities derived from an 
inductive method can be used, in combination with suitable utilities, for 
making several kinds of practical decisions. In other words, the selection of 
an inductive method can be seen as a kind of far-reaching practical 'meta-
decision', since it leads to the adoption of a general strategy for making 
different types of decisions. 

THE CONTEXTUAL APPROACH: COMPARISONS WITH OTHER VIEWS 125 
However, I would argue that the ultimate roots of such a meta-decision are 
cognitive. In fact, as illustrated by the verisimilitude approach to EPO, the 
choice of an inductive method can be seen as a cognitive decision aimed at 
achieving a particular cognitive utility, i.e., approximation to the truth. 
On the other hand, in the above quotations, Carnap himself appears to 
recognize that at least some of the "given purposes" w .r.t. which an inductive 
method is judged as "more or less adequate" are cognitive purposes and that, 
consequently, an inductive method is not only a practical instrument but also, 
as it were, a cognitive instrument. Carnap, indeed, points out that "an 
inductive method is ... an instrument for the task of constructing a picture of 
the world on the basis of observational data" and that the "performance" of 
an inductive method depends on "the values it supplies and their relation to 
later empirical results, e.g. the truth-frequency of predictions and the error of 
estimates". It seems quite clear that Carnap is specifically referring here to the 
cognitive performance of a given inductive method and that, in addition, this 
performance is somehow related to its effectiveness in approaching the truth. 
Moreover, Carnap seems to suggest that the 'past performance' of a given 
inductive method is the principal basis for deciding between retaining such 
a method or adopting another one. This view could be called 'empiricist' 
since the adoption of an inductive method depends essentially on the 
empirical evaluation of its past performance. The empiricist approach to EPO 
represents a possible alternative to the subjectivistic and aprioristic approaches 
advocated by Carnap in other works. 
However, there is a constant assumption underlying Carnap's oscillations 
among apriorism, subjectivism and empiricism. This assumption, which might 
be called universalistic, presumes that any subject X, at any time t, can select 
a unique epistemically optimum inductive method which is universally 
applicable to all scientific inquiries and practical problems facing X at t. 
Carnap's universalistic approach is precisely the opposite of our contextual 
approach to EPO where the selection of the optimum inductive method 
depends on the specific cognitive context. 
EPO corresponds approximately to Carnap's "problem of the success [of 
the different C-systems] in the actual universe" (1952, p. 75). Carnap is aware 
that this problem is different from LPO and that its solution cannot be 
directly obtained from a solution to LPO. In fact, "since an observer knows 
at any time only a sample, not the whole universe, he cannot determine the 
optimum inductive method for the actual universe" (Carnap, 1952, p. 2). 
On the other hand, it might be suggested that X can estimate, in some way, 
the success of the different C-systems in the actual universe and that, using 

126 
CHAPTER 9 
such estimates, he can also 'estimate' the optimum value A11• Contrary to this 
suggestion, Carnap (ibid., pp. 59-60) maintains that: 
Questions concerning the success of a given inductive method in the actual world would be 
of a factual, nonlogical nature. And if they concerned not merely that part of the world which 
is known to us by past observation but also a part or the whole of the future, then the answer 
could be given with certainty only after all observations reports were in, if that were ever 
possible. And if our question concerned not the actual success but the probability of success 
or an estimate of success, then it would make sense only on the basis of a chosen inductive 
method. [Author's italics] 
The last point would signify that the success (failure) of any inductive method 
in the actual universe cannot be estimated without presupposing a given 
inductive method "A. 1 In fact Carnap (ibid., p.60) remarks that: 
... any judgment about the success of an inductive method in the total actual world ... is 
obviously impossible from an inductively neutral standpoint. 
I believe there are good grounds for opposing Carnap's statement. In fact, 
from the external estimates Est(q), Est[G(q)], and Est[Db 0,q)] made in 
GD-contexts (see Chapter 8.2) an external estimate of the cognitive failure of 
any GC-system in the actual universe can be derived (see (8.25)). It seems 
clear that, while this estimate is an inductive estimate (based on the 
background knowledge BK), it is not a A.-based inductive estimate. To use 
Carnap's words, it is a "judgment about the success of [a GC-system] in the 
total actual world" which is made "from an inductively neutral standpoint". 
Given the inductive neutrality of the above external estimates they would 
appear to provide an 'unprejudiced criterium' for selecting the epistemically 
optimum GC-system. 
Camap (1950, pp. 177-182) also rejects the idea that selection of an 
inductive method might be justified using the so-called principle of 
uniformity. For instance, Carnap (ibid., p. 180) considers the following 
probabilistic formulation for the principle of uniformity: 
On the basis of the available evidence, the estimate of the degree of uniformity of the world 
is high. 
Carnap assumes that the degree of uniformity of the world cannot be 
estimated without presupposing a given inductive method; hence the principle 
of uniformity cannot be used to justify the choice of the inductive method. 
However I believe that Camap's assumption is mistaken since - at least in 
an important type of multinomial context, i.e. GD-contexts - inductively 

1HE CONlEXTUAL APPROACH: COMPARISONS WITII OTIIER VIEWS 127 
neutral estimates Est[G( q)] of the degree of disorder in the "actual world" can 
be made (see Chapter 8.2). Moreover, contrary to Camap's conclusion, I hold 
that such estimates can be used for the choice of the optimum GC-system 
(see Chapter 8, Sections 3-5). 
9.2 THE HYPEREMPIRICIST VIEW 
Lewis's approach to EPO 
According to Lewis's solution to EPO for C-systems (1971) the experimental 
evidence en alone is sufficient to identify the optimum C-system. 
In Lewis's paper the requirement of immodesty plays a fundamental role 
(see Chapter 4.2). According to Lewis (1971, p. 56): 
The requirement of immodesty will not help you much in choosing an inductive method 
unless few of the otherwise adequate methods are immodest. I might expect all methods to 
be immodest; in that case, it will get you nowhere to require immodesty as a condition of 
adequacy. How many methods are immodest? 
To answer this last question the class of competing inductive methods one 
wishes to choose from must be specified. For instance, given the class of 
C-systems, a C-system A is immodest, on the evidence en, "iff its estimate, 
on [en], of its own accuracy is higher than its estimate, on [en], of the 
accuracy of any rival method [A.']" (Lewis, 1974, p. 84). 
As has already been pointed out (see Chapter 4.2), Lewis (1971) fails to 
understand that all C-systems and, more generally, all exchangeable inductive 
systems are immodest. On the contrary - due to a subtle conceptual mistake 
in formulating his equations - Lewis (ibid., p. 59) believes to have proved the 
surprising theorem that, for any evidence en , exactly one C-system is 
immodest.2 
Lewis (ibid., pp. 59-60) points out that his 'theorem' provides a complete 
solution to EPO: 
Our new condition of adequacy, immodesty, and the conditions of adequacy that restrict you 
to the [C-systems] are enough to solve completely your problem of choosing an inductive 
method. You need only choose the one remaining adequate method. The conditions of 
adequacy thereby determine the degrees to which you should believe all propositions of 
interest to you, given total evidence consisting of a complete description of a sample. 
Although the 'immodesty-based' solution to EPO proposed by Lewis (1971) 
is erroneous, it may be asked whether Lewis's more general 'hyperempiricist 
program' - according to which a suitable set of conditions of adequacy could 

128 
CHAPTER 9 
"determine the degrees to which you should believe all propositions of 
interest to you, given total evidence consisting of a complete description of 
a sample" (Lewis, 1971, p. 60) - can be carried out. 
I suspect that the hyperempiricist program is undermined by some intrinsic 
difficulties as will be shown after examining Kuipers's hyperempiricist 
proposal (1986). Here I only want to suggest that, contrary to initial 
impressions, the hyperempiricist approach is a (disguised) form of apriorism. 
Indeed, to all appearances, Lewis's approach to EPO is "the other extreme" 
w.r.t. Carnap's apriorism, as pointed out, for instance, by Pietarinen (1974, 
p. 198): 
Camap's latest view is that there is (ideally) only one rational A.-method, the choice of which 
is not based on observed facts but only on a priori considerations ... let this be [A.R]. Then the 
actual belief in a hypothesis hat any two points of time T1 and T2 is IP(h/en)] and IP(h/en)] 
respectively, where er, is the total evidence at T, . ... Lewis takes the other extreme: he makes 
the choice of a A.-method depend essentially on the total evidence er,· At T1 and T2 the 
rational belief in h is [p).,(h/en)] and (p).2(h/erJJ respectively, where the values ~ and ~ 
depend on en and e:rz. 
However, careful scrutiny reveals that Lewis's "other extreme" is simply a 
modified form of apriorism. Indeed, according to Carnap's 'traditional' 
aprioristic approach to EPO the prior disti'ibution p should be chosen on the 
grounds of appropriate a priori conditions of adequacy. Hence, the posterior 
probabilities p(Qi len) - derived from p and en by Bayes's theorem - should 
depend only on these a priori conditions and en. But, after all, the same 
happens within the hyperempiricist approach where the posterior probabilities 
p(Qjen) are determined only by the experimental evidence en and certain a 
priori conditions of adequacy, such as the principle of immodesty. 
The only difference between the two approaches is that in the 'traditional' 
aprioristic approach the prior distribution pis determined before observing en 
and only after is it updated relative to en, while in the 'modified' aprioristic 
approach suggested by Lewis and Kuipers pis determined after obtaining en-
and, so to say, from en - by suitable a priori conditions of adequacy.3 
Both Carnap's and the Lewis-Kuipers approaches attempt to determine the 
posterior probabilities p(QJen) using only experimental evidence en. On the 
contrary, in the contextual approach to EPO as proposed herein the 
background information concerning a given population (or process) available 
to researchers before commencing collection of experimental data plays a 
crucial role in determining the prior probabilities and, consequently, the 
posterior probabilities p(Qi /en). 4 

THE CONTEXTUAL APPROACH: COMPARISONS WITH OTHER VIEWS 129 
Kuipers's approach to EPO 
An interesting formulation of the hyperempiricist program is given by Kuipers 
(1986). Kuipers (1986, p. 39) assumes that, although the logically optimum 
value A" for investigating a given process Ex cannot be known since it 
depends on the (typically unknown) degree of homogeneity C(q) of Ex, it is 
possible to estimate A" on the basis of en and, therefore, this estimated value 
of A" can be adopted as the epistemically optimum A-value. 
Kuipers proposes two different procedures to 'estimate' A": an 'estimation 
by a limit-process' and a 'one-step estimation'. 
Using the first procedure the en -based estimate of A" is made thus (ibid., 
p. 39): 
(i) starting from an arbitrary (finite) value A0 of A, the value of the 
unknown C( q) is estimated, 
(ii) the estimated value of C( q) is used to obtain a 'tentative estimate' A1 
of A", 
(iii) the estimated value A1 of A"' is used to restart this cycle and so on for 
an infinite number of times. 
Kuipers succeeds in proving that the above cycle is a limit-process. In other 
words, for any experimental evidence en and for any 'starting' A-value A0, the 
sequence of 'tentative estimates' A1, ... ,Am, ... of the cycle has a limit - let us 
call it "AKu" - which can be seen as the 'ultimate estimate' of A" (ibid., pp. 
40-41). 
Kuipers (ibid., pp. 39-40) characterizes the steps (i)-(ii) of the above 
procedure as follows. Given the starting value J...o, the 'corresponding' estimate 
estf.o[C( q)] of C( q) is drawn from the inductive probabilities pf.o(QJen) by the 
following formula: 
Afterwards A1 is defined as the A-value that would be the logically optimum 
value if the estimate estf.o[C(q)] were correct: 
(2) 
1 - estf.o[C(q)] 
estM[C(q)] - 1/k 
At any moment in the cycle the same procedure should be applied when an 
estimate Am-l is used to establish the corresponding estimate est~.m. 1 [C( q)] and 
the next estimate Am.5 

130 
CHAPTER 9 
A neat feature of the above limit-process is the restricted role played by the 
starting A-value A0 in establishing the epistemically optimum value AKu. In 
fact, Kuipers proves that, for any evidence en, there are no more than two 
possible non-extreme results of the limit-process.6 
This means that the effects of the inevitably arbitrary choice of the starting 
A-value A0 are to a large extent eliminated by the limit-process and 
consequently the freedom of researchers in selecting the epistemically 
optimum A-value is restricted, at most, to the choice between the two values 
of A. Hence, Kuipers's first proposal can be considered an 'almost complete' 
solution to EPO for C-systems. 
Let us now consider one interesting feature of the above limit-process 
overlooked by Kuipers himself. Taking one of the possible limit-values AKu 
as the starting A-value A0 of the cycle, then the cycle reaches its final result 
immediately since in this case AKu = A0 = A1 = "-z = ... = Am = .... 
Therefore I would propose the above solution to EPO be restated using a 
concept of immodesty analogous to, but different from, that introduced by 
Lewis. Kuipers's limit-process can be seen, indeed, as a tool for arriving at 
an 'immodest' inductive method AKu through an infinite sequence of 'modest' 
inductive methods A0,A1, ... ,Am, ... , starting from an arbitrary method A0• Each 
method Am_1 in the sequence is modest in the sense that - through the 
inductive probabilities Piv.-tCQ; len ) and the 'corresponding' estimates 
estiv._,[C(q)] of C(q) -it leads to a different and more appropriate estimate Am 
of the optimum value AA. However, 'at the end' of this infinite cycle an 
'immodest' method AKu is obtained indicating itself as the most appropriate 
estimate of AA.7 
Hence, Kuipers's first solution to EPO for C-systems can be reformulated 
as follows: given the evidence en, the immodest method AKu which indicates 
itself as the most appropriate estimate of A A should be selected as the 
epistemically optimum inductive method. 
Note that, while any inductive method is immodest relative to my notion 
of immodesty (see 4.12), only one method for each en is immodest if 
Kuipers's notion of immodesty is applied. The highly selective character of 
Kuipers's notion of immodesty depends on the specific procedure he proposes 
for passing from Am_1 to Am. 
Let us accept, for the sake of argument, Kuipers's two-step procedure (i)-
(ii) for determining the first tentative estimate A1 of AA starting from a given 
evidence en and an initial A-value A0• Yet it could be argued that formula (1) 
does not represent the best procedure for determining the "-o-based estimate 
est~.n[C(q)] requested in step (i). Initially (1) seems plausible enough since 
p/._Q; /en) = E~.n(q1 ) appears an appropriate A0-based estimate of q1 and, 

THE CONTEXTUAL APPROACH: COMPARISONS WITH OTHER VIEWS 131 
consequently, it might be thought that an appropriate estimate est)J)[C(q)] of 
C( q) may be obtained by replacing all the q1 in C( q) = ~qi with the 
corresponding estimates EM( q1) = p0(Q; len). 
However, from a Bayesian point of view, (1) would not appear acceptable. 
In fact, while p 0(Q; /en) is an appropriate "-o-based Bayesian estimate of q1 
since it is equal to the expected value E)J)( q1) of q1 , the estimate estM[C( q)] 
= ~[p)J)(Q; /en )]2 proposed in (1) is not an appropriate A.o-based Bayesian 
estimate of C( q). In fact ~[p(Q;/en )]2 is different from (and typically higher 
than) the expected value est)J)(C(q) I en)= ~(p)J)(Q;QJen) which is the usual 
A.0-based Bayesian estimate of C(q).8 
This argument assumes that a C-system A., being equivalent to a prior 
symmetrical Dirichlet on q, should be treated as any other 'Bayesian method' 
and, accordingly, a A.-based estimate of a given quantity should be made by 
applying the M-estimator or another Bayesian procedure. Against this 
assumption, however, it might be argued that using non-Bayesian A.-based 
estimates for selecting the optimum C-system is a perfectly legitimate 
'Bayes/non-Bayes compromise' of the type welcomed by Good (1965, pp. 28 
and 34-41; 1983a, pp. 100-103). 
A 'Bayes/non-Bayes interpretation' applies clearly to Kuipers's second 
solution to EPO for C-systems (ibid., pp. 43-44). Indeed, the 'one-step 
estimation' of A.& proposed does not require any starting method A.o to estimate 
the value of C( q) relative to a given evidence en. For this purpose, one of the 
usual orthodox (non-Bayesian) statistical techniques is suggested such as the 
following simple estimation procedure: est(C(q) I en) = ~(n; !nf After 
establishing est(C(q) I en), the epistemically optimum inductive method is 
equated to the method/..* which would be logically optimum if est(C(q) I en) 
were correct.9 
From a Bayesian standpoint, the selection of a given prior Dirichlet - or, 
equivalently, the corresponding GC-system- implies obligation to update such 
a prior Dirichlet by Bayes's theorem. Such a commitment conflicts with the 
hyperempiricist solutions to EPO since they imply a completely different 
'probability kinematics'. According to the hyperempiricist solutions, indeed, 
as new evidence en is gathered it is not used to update the selected prior 
Dirichlet by Bayes's theorem but to replace the prior Dirichlet with another 
prior (which, subsequently, is updated relative to en). This means that in the 
hyperempiricist approach probability kinematics is not governed by any prior 
Dirichlet. 10 
Hence, what initially appears as a solution to the problem of the choice of 
the optimum C-systems may be seen more properly as the creation of a new 

132 
CHAPTER 9 
inductive system. Kuipers (ibid., p. 44), for instance, seems aware that this is 
the case since he asks: 
what kind of probability system results if I substitute for A. in (the formula for the special 
values of a A.-method) an estimate of A.'? From the definition of a A.-method it is already clear 
that the result is not a A.-method, for an estimate of A. • on the basis of e. cannot be a fixed 
real number. 11 
9.3 THE PRESUPPOSITION VIEW 
A clear formulation of the presupposition view of induction is given by Burks 
(1963, p. 758): 
There is on the one hand, so to speak, the universe; it is this that we get information about, 
make predictions, adjust to, etc. There is, on the other hand, the (inductive method) which 
we use in learning about and adapting to the universe. Now the presupposition view about 
the relation of these two is roughly as follows. There are some quite general synthetic 
presuppositions about the universe which the use of the (inductive method) presupposes in 
the sense that if these presuppositions are in fact false the (inductive method) is not correctly 
applicable to the universe. 
Although agreeing with Burks to some extent, I am inclined to think that 
certain 'local' presuppositions concerning specific universes (populations, 
processes) play a much more important role than the "quite general synthetic 
presuppositions about the universe" mentioned by Burks. In other words my 
approach to EPO emphasizes the 'local' and contextual character of the 
synthetic presuppositions used in selecting the epistemically optimum 
inductive method for investigating a given universe. For this reason my 
approach to EPO could be defined as a form of contextual presuppositionism. 
The first systematic exposition of the 'presuppositionist view' of induction 
was given by John Stuart Mill who believed that the principle of uniformity 
of nature is an indispensable presupposition for induction. In his interpretation 
of Mill's views, Graves (1974, pp. 304-305) points out: 
(Mill) certainly knows very well that many inductions do prove false, although a remarkable 
number succeed. In fact, he takes great pains to emphasize the diversity and complexity of 
nature as well its uniformity ... Furthermore, it is much too crude to say that nature either is 
or is not uniform, as if we had a simple dichotomy. What we have is a great many related 
particular uniformities, and the overall uniformity of nature is nothing more than the 'tissue' 
of these partial uniformities, or laws. We know from past experience that some uniformities 
do exist, and whenever we do argue inductively we are assuming (or believing, or hoping) 

THE CONTEXTUAL APPROACH: COMPARISONS WITH OTHER VIEWS 133 
that the case in question is one of them. We may of course be wrong; but to assume 
beforehand that we will be wrong is indeed inconsistent with the principle of uniformity. 
On the basis of Graves's account of Mill's views, I would consider the 
contextual approach to EPO as a 'formal development' of Mill's idea that 
inductive procedures should be based on suitable assumptions about the 
"partial uniformities" of nature. 
Graves (ibid., p. 301) points out that ''while Mill's own formulation [of the 
principle of uniformity of nature] is vague, it is not hopelessly so, and at least 
in special cases it can be made quite precise by using some of Carnap's 
ideas." More precisely, Graves (ibid., pp.313-318) considers "Carnap's 
grandiose attempt to develop a fully quantitative inductive logic" and tries to 
"see how his ideas and Mill's might be used to clarify and supplement each 
other". 
In particular, Graves (ibid., p. 316) asks: 
... what is the significance of A. ••• and how are we to determine, estimate, or choose its 
value? 
His answer (ibid.) is that: 
Ontologically, A. provides ... a precise, quantitative, numerical scalar measure of the degree 
of uniformity of nature. The value we choose represents my estimate of how uniform nature 
is, at least in the area under investigation. We may make a poor choice, but the ontologically 
correct value determines the best rule to use. This value can be discovered only inductively, 
for it reflects my actual world and would be different in other possible worlds. 
Afterwards, Graves (ibid., p. 317) points out that: 
... any justification of induction, or of a general inductive logic leading to specific rules of 
inference, must be based on non-logical facts about the constitution of the actual world and 
man's need to act in it. The world does have a degree of uniformity and if I knew just what 
it was in some a priori way I could use this to satisfy simultaneously the ideals of accuracy 
and efficiency in the best way possible .... Since I do not know the true value, I still can and 
implicitly do make estimates. These are based largely on my previous experience of particular 
uniformities .... Of course, even this picture is oversimplified. I may fmd that nature is non-
uniform in its uniformities, so that in different areas of investigation I may get better results 
by choosing different values of A. •••• 
... in some form there is always an expectation, or at least a hope of some particular 
degree of uniformity. [Author's italics] 
Unfortunately Graves does not provide any concrete indication on how to 
estimate the "degree of uniformity" of the actual world. Therefore, he is 

134 
CHAPTER 9 
unable to explain how to solve the related problem of 'estimating' the 
optimum value of J.... 
It would appear that the presupposition view is also shared by Rosenkrantz 
(1981, Ch. 1.3, p. 1) who, after asking: 
Is there any reason for preferring one A.-method to another? In particular, can we single out 
any of these methods as a uniquely reasonable inductive method? 
suggests a negative answer to these questions (ibid., Chapter 1.3., pp. 3-5): 
... all that we can conclude is that ... methods with small A. are good in homogeneous 
universes, while ... methods with large A. perform well in heterogeneous universes. And, in 
practice, we do not know exactly what kind of universe we have by the tail. 
... one's inclination is to conclude that [the] rational choice of a A.-method should depend 
on how homogeneous one takes the population under consideration to be . ... Yet, despite the 
fact that the optimal A.-method varies with the homogeneity of the considered population, 
Camap continues to speak (1952, § 18) as if one chooses a A.-method for life ... But how we 
are to reconcile the claim that a A.-method is a long term policy to be applied across the board 
with the demonstrated dependence of the best method on the homogeneity of the considered 
population?12 (Author's italics] 
Also Hintikka stresses that certain synthetic assumptions play an important 
role in the choice of the optimum inductive method. For instance, referring 
to C-systems, Hintikka (1987, p. 303-304) states that "the optimal choice" of 
J... is determined by "the amount of order or disorder" in the investigated 
universe: 
Hence, one's choice of the value of A. means guessing the relative degree of disorder in the 
world. The choice therefore cannot be made on logical grounds alone; it amounts to a 
synthetic assumption which nevertheless has to be made a priori. ... Indeed, my observation 
fits very well into the overall Bayesian strategy ofbuilding one's background information into 
the choice of prior probabilities. The use of parameters like A. may in suitable situations be 
an aid in so codifying one's background knowledge concerning the relative degree of order 
in the world. 13 
A presuppositionist view of TIP has also been advanced by Pietarinen (1972) 
- a follower of Hintikka - who holds (ibid., p. 40) that a given inductive 
method can be considered rational only if it incorporates certain 
presuppositions about "the degree of uniformity of the phenomena in 
question". 
Pietarinen (ibid., p. 128) points out that "much of the old problem of 
justifying induction" is contained in the question about the rationality of the 

THE CONTEXTUAL APPROACH: COMPARISONS WITH OTHER VIEWS 135 
choice of a given inductive method. In particular, concerning the choice of A., 
Pietarinen (ibid., pp. 111-112) points out that: 
... a rational choice of [the A.-value] ... accords with the presuppositions on the degree of 
'regularity' of 'order' of the universe. 
Having remarked that "the procedure of determining the values of the 
inductive parameters is estimation" (ibid., p. 129) - in the sense that such 
determination is based on an estimation of the degree of order of a given 
universe - Pietarinen (ibid., p. 131) recognizes that "it is difficult to spell out 
the exact principles that would yield an adequate method of estimation taking 
all relevant information into account" and, in fact, he does not suggest any 
specific procedure for estimating the degree of (dis)order in a given universe. 
With reference to this, I wish to point out that the procedures illustrated in 
Chapter 8.2, for making external estimates of the degree of disorder G( q) of 
a given population, represent a possible fulfillment of Pietarinen's request to 
specify the nature of the estimates which may be used as presuppositions in 
choosing a given inductive method. 
I would also point out that the scientific applications of G( q) and other 
diversity measures illustrated in Chapter 10 suggest that researchers, dealing 
with realistic problems in their area of expertise, before sampling a given 
population U under investigation can indeed produce meaningful 'informal 
estimates' of the Gini diversity G(q) of U. Among other things, in Chapter 
10 it is shown that G( q) is equal to the physical probability that two 
individuals chosen at random and independently from U belong to different 
categories (see below formula (10.12)): hence an estimate of G(q) amounts 
to an estimate of such a physical probability. Now it would appear plausible 
to assume that typically a scientist working in a certain area may informally 
estimate this probability even before sampling U. Lastly I would point out 
that- given the equalities Est[G(q)] = E[G(q)] (see the requirement (CC), p. 
98) and E[G(q)] = 1 - '1:.p(Q;QJ (see (3.44.b))- making an external estimate 
Est[G( q)] amounts to establishing the value of the predictive probability that 
the next two individuals drawn at random and independently from U belong 
to different groups. Once again it seems that a researcher dealing with 
population or processes in his area of expertise can assess this predictive 
probability. In any way this assessment is not qualitatively different from -
although usually more difficult than - the assessment of the predictive 
probability Y; = p(Q;) that the next observed individual will belong to a given 
group Q;. 

136 
CHAPTER 9 
9.4 THE VERISIMILITUDE VIEW 
My V-solution to EPO is based on the assumption that researchers, in 
selecting an inductive method, attempt to maximize the possibility of 
approaching the truth. Such an assumption can be seen as a concrete 
application of a more general claim about the relationships between the 
scientific method and the truth made by Friedman (1978, p. 361): 
It would seem that some nice connection between confirmation and truth must exist if 
scientific method is to be justified as a rational activity. For one of the important ends of 
scientific method is the construction of theories that are true, or at least approximately true. 
Obviously, then, scientific method will be an effective means to this end only if it does 
indeed tend to produce true, or at least approximately true, theories.14 
According to Friedman (ibid., pp. 369) the requested connection between 
confirmation and the truth is guaranteed only by reliable inductive methods 
where the concept of reliability is informally elucidated thus: 
... although a reliable method does not necessarily produce true conclusions, or even 
conclusion with a high probability of truth, it does tend toward truth: as time goes on, the 
conclusions that it accepts acquire ever increasing probabilities of truth. My suggestion is that 
it is the task of confirmation theory to show that the methods scientists actually use are 
reliable in this sense.15 
From this concept of reliability it follows that there can not be any a priori 
justification for the scientific method. According to Friedman (ibid., pp. 370-
371) 
The impossibility of such a justification follows ... from two simple and fundamental facts: 
(i) there has to be some kind of link between justification and truth; a justification of 
scientific method must say something about its propensity to lead to truth; (ii) scientific 
method is not logically guaranteed of reaching true conclusions; it is an incurably 
nondeductive method. There is no inductive method that is more reliable in every logically 
possible world than every other method; consequently, there is no method that is a priori best, 
there is no method that is a priori the most reliable. We have to know facts about the actual 
world if we are to know which method is best; and we have to know facts about the actual 
world to know even that any given method has any chance at all of leading to truth. 
Furthermore, Friedman (ibid., p. 367) argues that the "superiority" of one 
inductive method over another is not an 'intrinsic feature' of that method; on 
the contrary, the fact that one of the two methods is a more effective tool for 
approaching the truth depends, to a large extent, on the structure of the 
external world: 

THE CONTEXTUAL APPROACH: COMPARISONS WITH OTHER VIEWS 
137 
How ... can ... one particular ... method be superior to the others? I think the answer is 
obvious: a method is superior if, when it is used in the inference I actually do make, it leads 
to true conclusions sooner and more often than the other methods. And which method actually 
enjoys. this kind of superiority depends on empirical facts. 
Consider, for instance, C-systems. Friedman (ibid., p. 368, n. 3), having 
recalled that when A is small (compared ton) the predictive probability of Q; 
is closer to n;fn, whereas when A is large (compared to n) the predictive 
probability of Q; is nearer to 1/k, points out that: 
Thus, the larger A we choose, the more reluctant we are to extrapolate from my observed 
samples. Smaller values of A work better in worlds in which observed samples (of almost any 
size) tend to be representative of the entire universe. Large values of A work better in worlds 
in which observed samples (unless they are very big) tend not to be so representative. 16 
Note that, given theorem (8.22), it would appear quite natural to identify the 
"worlds in which observed samples ... tend to be representative of the entire 
universe" as the worlds with a low value of G(q). If this assumption is 
correct, Friedman's assertion would imply that whenever a scientist expects 
G(q) to be low - and he should accordingly expect the observed samples to 
tend to be representative of the entire universe - he should choose a small 
value of A. 
The importance of the concept of verisimilitude for comparing and 
selecting inductive methods is also stressed by Rosenkrantz (1980, p. 483): 
The real importance of verisimilitude ... is that it constitutes a well-defined cognitive goal, 
leaving us free to compare inductive methods by the efficiency with which they realize this 
goal - that is by the expected truthlikeness of their outputs. 
In particular, Rosenkrantz (ibid., p. 485) claims that, from the concept of 
verisimilitude or truthlikeness: 
... an interesting rationale can be given for adopting a diffuse prior where relevant data are 
lacking. For ... I can compare different priors by the expected truthlikeness of the posterior 
distributions to which they lead, conditional to the truth. 
Rosenkrantz (ibid., p. 485-87) explains the intuitive content of his proposal 
by referring to the case of "various beta priors for an unknown population 
proportion [ q1]": 
If your prior is heavily concentrated about the true value [of q1) (which amounts to a 'lucky 
guess' in the absence of pertinent data), you stand to be slightly closer to the truth after 
sampling than someone who adopts a diffuse prior, your advantage dissipating rapidly with 

138 
CHAPTER 9 
sample size. If, however, your initial estimate is in error, you will be farther from the truth 
after sampling, and if the error is substantial, you will be much farther from the truth. I can 
express this by saying that a diffuse prior is a better choice at 'almost all' values of [q1) or, 
better, that itsemi~dominates any highly peaked (or 'opinionated') prior. In practice, a diffuse 
prior never does much worse than a peaked one and 'generally' does much better .... 
It is well known that symmetric Beta priors, [Beta(a*,a*)] correspond to the 'methods' 
of Camap's A.-continuum with [A.= 2a*] and that Camap expressed a preference for small 
values of A. (i.e., for diffuse priors) without being able to justify that preference. He compared 
the A.-methods in terms of their mean-squared errors and showed that the optimal method is 
a function of the homogeneity of the considered population. It is not easy to reconcile this 
result with a preference for small values of A. ... to be used across the board in all estimation 
problems. A reconciliation can be effected, however, if we regard this preference as 
governing 'barren' contexts where no information about the population is at hand. In that case 
we can justify preference for diffuse priors (hence for estimates of the population proportion 
that are close to the observed sample proportion) by appeal to the average truthlikeness of the 
estimates to which they lead. The argument sketched here therefore fills a gap in Camap's 
account. 17 
The assumption that verisimilitude provides the rationale for selecting the 
optimum inductive method is common to both my V-solution to EPO and that 
of Rosenkrantz. However, there is an important difference between the two 
approaches. Rosenkrantz's main objective is to justify the preference for 
"diffuse priors" (C-systems with a low A.) "in the absence of pertinent 
data". 18 On the contrary, I think that, in the analysis of scientific 
methodology, the most interesting inductive problems arise w .r.t. situations 
of partial information. As a matter of fact, my V -solution to EPO can be 
applied both to the cognitive contexts which include poor background 
knowledge and to those which include ample background knowledge. 19 

CHAPTER 10 
DISORDERED UNIVERSES: DIVERSITY MEASURES IN 
STATISTICS AND THE EMPIRICAL SCIENCES 
The contextual solution to EPO proposed in Chapter 8 can be applied in any 
G-context, i.e., in any multinomial context where an external estimate 
Est[G(q)] of the Gini diversity G(q) = 1 - ~qi of the population or process 
under investigation can be made ( cf. Chapter 8.2). One of the principal aims 
of this chapter is to show that G-contexts, far from being merely theoretical 
possibilities, provide a realistic description of certain cognitive situations 
actually occurring in several empirical sciences. If this claim is true then the 
contextual solution to EPO can be concretely applied in several types of 
scientific research. 
Among other things this Chapter should indicate that: (1) G(q) is not an 
arbitrary or ad hoc index but provides a measure of the degree of disorder in 
a given population or process which appears to be useful and appropriate in 
several scientific disciplines; (2) in such disciplines G( q) can be handled as 
any other 'respectable' scientific quantity and, in particular, the value of G( q) 
in a given population or process can be conjecturally estimated before 
sampling on the basis of the background knowledge available to the 
researchers working in any given field. For instance, it seems clear that, as 
mentioned in Chapter 8.2, an estimate Est[G(q)] of the Gini diversity of a 
given population could be made on the grounds of appropriate hypotheses 
concerning the relations between G( q) and other characteristics of the 
population, such as stability, or its environment, such as pollution (see final 
page of Chapter). 
Having supplied some historical remarks on Gini diversity (Section 1), 
several explications of diversity will be illustrated (Section 2) and, lastly, 
some scientific uses of Gini diversity and other diversity indices will be 
examined (Section 3). 
139 

140 
CHAPTER 10 
10.1 GINI DIVERSITY 
Pielou (1981, p. 408) defines a diversity index as 
... a measure of the 'qualitative dispersion' of a population of individuals belonging to several 
qualitative different categories. In the same way that variance, standard deviation, mean, and 
range serve to measure quantitative variability, diversity indices measure qualitative 
variability. 
It would appear that the Italian statistician Corrado Gini (1912)1 was the first 
to make a systematic analysis of qualitative variability. More specifically, he 
elaborated a unitary conceptual framework for the analysis of both 
quantitative and qualitative variability. 
A key role in Gini's analysis is played by the concept of mean difference 
which is defined as follows. 
Let theN individuals a1, ... ,aN of a given population U be classified on the 
basis of a given quantitative characteristic - for instance weight - and Jet X; 
denote the value of a;, for i = 1, ... ,N. Then the mean difference g(U) is 
defined as follows: 
(1) 
g(U) = 
1 
n 
L 
N2 
i, j=l 
where the sum extends over all N2 couples (x; ,xi). 2 
The concept of mean difference may be adapted so that it may also be 
applied to various types of qualitative characteristics (see Gini, 1912, pp. 3-
12). Consider, for instance, the family Q=(Q1, ... ,Qk) for which a 'trivial' 
similarity relation holds, in the sense that, for any four categories, or 
properties, Q;, Qi, Qh, Q1 E Q the degree of similarity between Q; and Qi is 
identical to that between Qh and Q1. Gini (ibid., pp. 142-144) assumes that in 
this case the "difference" - or dissimilarity - between two individuals a, and 
a1 in U is 0 if they have the same property Q;, while it is 1 if they have two 
different properties Q; and Qt 
Let N; be the number of Q; -individuals in U. Then the sum of the 
differences between a given Q; -individual a, and each of the N members of 
U, including a,, is given by: N;..O + (N-NJ*1 = N- N;. Hence, the sum of 
all the N2 differences between couples of members of U is given by 
'l:.N; (N -NJ. Therefore the mean difference among the members of U is given 
by: 

DISORDERED UNIVERSES 
141 
(2) 
G(U) = 
Consider the vector q=(q1, ••• ,q,J where q; = NJN. Given that 'DV; = N, formula 
(2) can be rewritten as follows: 
(3) 
G(U) = G(q) = 1 - l: if; 
which is the usual definition of G(q). 
For a long period Gini's research on qualitative variability was little known 
outside Italy and even today the historical importance of Gini's work on the 
heterogeneity of populations has still to be fully recognized.3 As a 
consequence of the relative lack of knowledge of Gini's work on this subject, 
the concept of mean difference has been 'rediscovered' by several statisticians 
and scientists. For instance, according to Patil and Taillie (1982, p. 556) the 
diversity of a given community can be defined as "the average differentness 
[i.e., mean difference] between two randomly selected members of the 
community". 
Analogous to the mean difference explications of diversity proposed by 
Gini and other authors, mean similarity explications of uniformity have been 
advanced. For instance, given a measure of similarity S "between any pair of 
animals (or more in general units)" in a given population, the mean similarity 
E(S) among the units in the population could be considered as the "weighted 
index of homogeneity" or the "index of uniformity" of the population (Good, 
1982, p. 562). Note that if S is a trivial similarity function which can only 
assume the values 0 and 1 then E(S) is equal to ~if; (see Good, ibid.). This 
means that Camap's uniformity index C(q) =~if; can be seen as the result of 
applying Good's mean similarity explication of uniformity to a 'trivial-
similarity' family Q=(Q1, ... ,Qk).4 
10.2 EXPLICATING DIVERSITY 
Average rarity explications 
Average rarity explications of diversity are based on "the view that diversity 
is an average property of a community" where such a property is given by 
"species rarity" (Patil and Taillie, 1982, p. 548). Let Rar(Q; ,U) be a measure 
of the rarity of the species Q; in a given community U. Then, the average 
rarity explicatum Div(U) can be defined as follows: 
( 4) 
Div(U) = l: q; Rar(Qi ,U) 

142 
CHAPTER 10 
Different diversity indices can be obtained from ( 4) depending on the selected 
rarity measure Rar(Q; ,U). 
Consider a "dichotomous-type rarity measure" Rar(Q;,U) = Rar(q;) which 
depends only on the "relative abundance" q; of Q; in U (Patil and Taillie, 
1979, p. 6).5 Then the intuitive requirement that rarer species correspond to 
smaller values of qi can be expressed as follows: 
(RAR) Rar(q;) is a decreasing function of q/ 
The requirement (RAR) is satisfied, for instance, by the following definition 
of species rarity: 
(5) 
Rar(q) = 1 - q; 
If the average rarity explicatum ( 4) and the definition (5) of species rarity are 
adopted then 
Gini 
diversity is 
immediately obtained: Div(U) 
= 
~q; (1 -· q;) = 1 - ~cfl = G(q). 
Similarly some homogeneity indices have been interpreted as measures of 
the 'average commonness' of the different species present in a given 
population. Let Com(Q; ,U) be a measure of the commonness of the species 
Q; in a given community U. Then the degree of homogeneity Hom(U) can be 
defined as: 
(6) 
Hom(U) = L q;Com(Q;,U) 
Note that C(q) can be interpreted as a measure of average commonness, 
provided that Com(Q;,U) is equated to the relative frequency qi of Q; in U:7 
(7) 
Hom(U) = L q; ·q; = L ci = C(q) 
Hill (1973, p. 428) introduces the following parametric family of diversity 
indices based on the concept of "proportional abundance", or commonness, 
of the different species in population U: 
(8) 
N.(q) 
= 
[ 
wlq:-1 + WJth-1 + ... + wJ!Jk-1 
WI+ Wz+ ... + Wk 
] V(l-•) 
where Na(q) is called the "diversity number of order a" (ibid.) 

DISORDERED UNIVERSES 
143 
Hill (ibid.) points out that Na(q) "is the reciprocal of the (a-1)th root of a 
weighted mean of the (a-1Yh powers of the proportional abundances" of the 
k species Q1, ... ,Qk present in U. Therefore Na(q)-diversities can be 
"characterized as reciprocals of mean proportional abundances". 
Assuming that each of the weights W; occurring in (8) is equal to q; , then 
(8) can be rewritten as follows: 
(9) 
Na{q) = (q~ + clz ... + q{Jl/(1-a) = (L qJl/(1-a) 
The family Na(q) and the related family of uniformity measures l!Na(q) 
include several well known indices. For instance, it follows from (9) that 
N2(q) is equal to ll(~q7); hence the uniformity measure l!Nz(q) is equal to 
Carnap's homogeneity index ~q;.s 
Distance-from-maximal-heterogeneity explications 
A possible explication of the notion of homogeneity is based on the following 
two step strategy (cf. Bhargava and Doyle, 1974, p. 243): 
(i) 
identify the 'even' vector q* which represents the case of maximal 
heterogeneity, 
(ii) 
define the homogeneity Hom(q) of a given population U with vector 
q as the distance between q and q*: 
(10) 
Hom(q) = D(q,q*) 
If the quadratic distance Dz(q,q*) is applied and q* is equated to the 
indifference vector l!k=(l/k, ... ,1/k),9 then the homogeneity index derived 
from (10) is a linear function of Carnap's index C(q): 
(11) 
Hom(q) = Dz(q,l!k) = C(q)- 1/k.10 
A similar strategy could be used for explicating diversity. Indeed, given an 
appropriate measure of the 'proximity' between two probability vectors, the 
diversity Div( q) of U could be defined as the proximity between q and q* .11 
Probabilistic and encounter-theoretic explications 
The probabilistic explications of homogeneity and heterogeneity date back to 
at least to Simpson (1949, p. 688) whose "measure [~q7] of the concentration 

144 
CHAPTER 10 
of the classification" of a given population U in groups derived from his 
preceding collaboration with Turing (cf. n. 4). 
According to Simpson (ibid., p. 688), indeed, the homogeneity index ~q7 
"can be simply interpreted as the probability that two individuals chosen at 
random and independently from the population will be found to belong to the 
same group", as emerges from the equality: 
Similarly, the Gini diversity 1 - ~i; of U is equal to the probability 
~q; (1-q;) that two individuals chosen at random and independently from U 
belong to different groups. 12 
More recently, Good (1982, p. 562) proposed the following parametric 
family of homogeneity indices: 
(13) 
Cs(q)= Lqf 
where s 2:: 2 
where Cs (q) is the probability that s units, randomly selected with 
replacement from a given population, will belong to the same category. Note 
that Carnap's index is included in Good's family: C(q) = ~q; = C2(q). 
In some scientific disciplines an encounter-theoretic approach to diversity 
is proposed which provides a kind of 'operational interpretation' of the 
probabilistic explications described above. While such explications make 
reference to the probabilities of 'artificial events' such as the random 
selection of two or more individuals from a given population, the encounter-
theoretic approach to diversity makes explicit reference to the probabilities of 
'natural events' such as the spontaneous intraspecific and interspecific 
encounters between couples of individuals within a certain communityY 
For instance, in the biology of populations, the encounter-theoretic concept 
of diversity has been considered by Hurlbert (1971, p. 577) who argues that 
many commonly used diversity indices lack any evident "empirical 
interpretation" and suggests 
an alternative set of indices 
having 
"straightforward biological interpretations". 
In particular, the interpretation advanced by Hurlbert makes reference to 
"the probability of interspecific encounters (PIE)" (ibid., p. 579). The intuitive 
meaning of PIE is illustrated by Alfred Russell Wallace's vivid description 
of an Amazonian forest (1876, p. 75: quoted by Hurlbert, ibid.): 
If the traveller notices a particular species and wishes to find more like it, he may tum his 
eyes in vain in any direction. Trees of varied forms, dimensions and colors are around him, 
but he rarely sees any one of them repeated. Time after time he goes towards a tree which 
looks like the one he seeks, but a closer examination proves it to be distinct. He may at 

DISORDERED UNIVERSES 
145 
length, perhaps, meet with a second specimen half a mile off, or may fail altogether, till on 
another occasion he stumbles on one by accident. 
Wallace's traveller may be seen as an intruder- for instance a biologist or an 
other organism- entering a community and randomly encountering individuals 
belonging to it. 
Referring to a community with N individuals and k species Q1, ... ,Qk- where 
q; = N;IN is the relative frequency, or "abundance", of Q; - Hurlbert (ibid.) 
points out that "when the first individual encountered risks being the subject 
of the second encounter also, as in non lethal encounters" the probability that 
two individuals encountered by the intruder belong to different species is 
simply !l.Z = 1 - I.q;. Notice that Hurlbert's index i::J? is the same as Gini 
diversity .14 
Hurlbert (1971, p. 581) also considers the following parametric family of 
diversity indices E(S~): 
(14) 
E(S~) = }."; [1- (1- q;t] 
where E(S~) is the number of species which, on average, would be met by an 
individual ''which enters a community and in a certain period of time 
encounters n individuals at random", provided that "the intruder eats none of 
the individuals encountered" (Hurlbert, ibid.). 
It can easily be proved that the member E(S;) of the family E(S~) is linked 
to G(q) by the following relation: 
(15) 
E(S2) = 1 + G(q) 
Requirements of adequacy for diversity 
Whenever the members of a given population U are classified w.r.t. a 'trivial-
similarity' family Q=(Q1, ... ,Qk), it would seem intuitively plausible that at 
least the following 'minimal' requirements of adequacy are satisfied by a 
diversity measure Div(U) for U: 
Requirement of q-sufficiency 
Div(U) is a function, at most, of q=(q1, ••• ,qk). Hence, Div(U) can be 
referred to as Div(q). 
Requirement of symmetry 
Div( q) is a symmetrical function of q, i.e., if :n; is a permutation of 
(q 1, ••• ,qk) and q'=(:n:(q1), ••• ,:n:(qJ), then Div(q') = Div(q). 

146 
CHAPTER 10 
Requirement of maximization 
Div(q) reaches its maximum value when q is identical 
to the 
indifference vector 1/k=(1/k, ... ,1/k). 
Requirement of minimization 
Div(q) reaches its minimum value when q is one of the k 'maximally 
homogeneous' vectors: (1,0, ... ,0), (0,1,0, ... ,0), ... , (0, ... ,0,1). 
Requirement of evenness 
Div(q) increases when two components q; and% of q, are 'flattened' 
towards l!k in the following sense. Let q=(q1, ... ,qJ and q'=(q~, ... ,qk) 
be two vectors so that q; < q; s 1/k s qj < qj and q~ = qh for all 
h -:/:. i,j. Then Div(q') > Div(q). 
Indeed, these minimal requirements - which may be seen as the core of 
presystematic intuitions concerning diversity - appear to be satisfied by all the 
diversity measures mentioned in this Section, including Gini diversity. 
However, they do not seem sufficient for the axiomatic derivation of Gini 
diversity or other diversity measures. For instance, the few available 
axiomatic characterizations of Gini diversity ( cf. Bhargava and Uppuluri, 
1975; Rao, 1982) make use - in addition to some of the above minimal 
requirements - of other and stronger principles (where the intuitive meaning 
and plausibility of some of these principles are not completely clear). 
Several authors after Gini (1912) pointed out a number of 'plausible 
properties' of diversity measures which can be interpreted as more or less 
explicit (minimal) requirements of adequacy for diversity (see, for instance, 
Agresti and Agresti, 1977, pp. 206-207). It may be asked whether these 
requirements could be used to explicate diversity on the basis of the following 
two step approach widely adopted in the analytical philosophy to explicate 
methodological concepts: 
(i) 
presystematic intuitions about the diversity of a population are 
formally expressed by an appropriate set of requirements of 
adequacy; 
(ii) 
the most simple explicatum satisfying such requirements is adopted. 
Of course this approach does not exclude the following two possibilities: (i) 
the adopted explicatum is given by a continuum of diversity measures; (ii) not 
only is such a continuum compatible with the requirements of adequacy but 
it is axiomatically derivable from them. 

DISORDERED UNIVERSES 
147 
Although this approach - as opposed to the above described explications of 
diversity derived from a single 'monolithic' intuition - has not yet been 
systematically investigated, I believe it merits serious consideration. 
10.3 DIVERSITY MEASURES IN THE EMPIRICAL SCIENCES 
According to Sugihara (1984, p. 564), the notion of diversity has both a 
philosophical and a scientific interest: 
There are two broad reasons why attention has been focused on characterizing the diversity 
of an ecological assemblage. These depend on whether one treats diversity as an abstract 
phenomenological property or a specific biological one. That is, whether one treats diversity 
in its general sense, as a property in itself, or as an indicator of the functioning and 
organization of communities. First, diversity per se, as a phenomenological property, contains 
intrinsic interest, and as a fundamental quality of perception seems to demand quantification. 
It is not surprising, therefore, that one finds diversity measures emerging in such varied 
disciplines as genetics, linguistics, and economics .... 
Here justification for a particular index rests more on philosophy than science, since the 
primary purpose of the index is to reflect a human value rather than to capture an important 
property of state in the functioning of communities. 
As seen above several diversity measures may be obtained as the result of 
'philosophical explication' of the presystematic concept of diversity. A purely 
theoretical analysis of diversity may also be useful in the empirical sciences 
since it reveals the 'logical structure' of diversity measures. 
However, the choice of a given diversity measure Div(q) is entirely up to 
the scientist. Indeed, the scientific usefulness of Div(q) depends on the 
discovery of certain relations between the diversity Div(q) of a population and 
further characteristics of the population or of other entities related to it. 15 In 
other 
words, 
the 
scientific usefulness of Div(q) depends on the 
scientists' ability to link Div(q) with other scientific quantities in a network 
of well confirmed empirical laws. 16 
More specifically, Sugihara (ibid., p. 564), referring to diversity as a 
biological property, remarks that: 
To the ecologist, diversity is interesting as a property of state in so far as it has the potential 
to reflect the nature of the underlying processes and organization that structure the 
community. Therefore, beyond arbitrary or weakly motivated definitions, the scientific interest 
in and importance of ecological diversity hinges directly on its possible connections with the 
functioning and organization of communities. The principal aim of such scientific study, 
therefore, is to find a characterization of species diversity that most clearly reveals this 
functional connection.17 [Author's italics] 

148 
CHAPTER 10 
With reference to this, remember that the encounter-theoretic interpretations 
of Gini diversity and other indices (see Section 2) were forwarded in an 
attempt to identify diversity measures potentially useful for understanding the 
"functioning and organization of communities" where interactions between 
members of different categories visibly play an important role in the life of 
the community. 18 
Concerning the scientific applications of Gini diversity G(q) and other 
diversity measures in the empirical sciences, it should be remembered that 
G( q) is applied in several human sciences such as sociology (Lieberson, 
1969), economics (Amemiya, 1963) and linguistics (Yule, 1944; Greenberg, 
1956; Guiraud, 1959; Herdan, 1964, 1966).19 
G(q) and other diversity and uniformity measures have also been used in 
several natural sciences such as genetics (Lewontin, 1972), biology (Gatlin, 
1972) and ecology (see, for instance, Pielou, 1969, 1975, 1977 and Grassle, 
Patil, Smith and Taillie, 1979). 
Some interesting comments on the 'intrinsic' importance of the ecological 
concept of diversity are made by Dennis, Patil, Rossi, Stehman and Taillie 
(1979), in the introduction to an exhaustive bibliography - including 1046 
titles - on the literature on ecological diversity: 
"Why diversity?" Recently a statistician noticed tltat he never received a simple 
straightforward answer to tltis question from ecologists. But imagine the look of 
disappointment in an ecologist hearing this question. The diversity, or variety, of plants and 
animals on tltis planet is tlte very basis of tlte ecologist's profession. 
Ecology is rooted in natural history. Perhaps the fundamental working craft of ecologist 
is taxonomy; possibly tlte most elemental quantities recorded in ecological work are tlte 
numbers and abundances of species. Why are tltere so many species in the abundances tltat 
we observe? Indeed, almost any ecological study has some bearing on this question, as these 
quantities will be explained only after tlte interrelationships of tlte organisms and 
environments are fully understood. 
So if you ask an ecologist "why diversity?" the response, if any, will likely echo the retort 
of the mountaineer Mallory when questioned about his own pursuits: "Because it is there!". 
This 
quotation 
clearly 
illustrates 
that 
an 
intrinsically 
interesting 
'phenomenological parameter' of an ecosystem is its degree of diversity: as 
such, diversity should be the explanandum of appropriate scientific 
explanations. 
On the other hand, a certain diversity measure could also be used as an 
explanans to explain, or predict, other features of a given environment. With 
reference to this, Patil and Taillie (1982, p. 566) remark that: 
Research on diversity as an input parameter is relatively in its embryonic stage. A major 
question is, What of consequence, if any, does tlte diversity of a system help predict or 
determine? Some of the questions tltat have been pursued are, Does the diversity of a 

DISORDERED UNIVERSES 
149 
community help predict its stability? Does the biotic diversity of a habitat help predicting its 
abiotic classification? Does the geological diversity of a region help predict its mineral 
diversity?20 
The importance of the relationships between diversity and other scientifically 
interesting parameters is also pointed out by Pielou (1975, p. 6): 
The purpose of measuring a community diversity is usually to judge its relationship either 
with other community properties such as productivity and stability, or to the environmental 
conditions that the community is exposed to. 
With reference to this, it should be pointed out that over the last few years 
there has been an increasing theoretical and practical concern with the 
possible impact of environmental pollution on the biological diversity of 
ecosystems21 and, consequently, the possible relationships between pollution 
and diversity have been extensively investigated.22 

CHAPTER 11 
CONCLUDING REMARKS 
In this chapter the relationships between the contextual view of prior 
probabilities and some recent methodological programmes of research are 
considered; moreover some possible developments of the contextual view are 
suggested. 
Relationships between the contextual view of prior probabilities and other 
methodological programmes of research 
In spite of the increasing attention paid to scientific revolutions over the last 
thirty years, it is generally recognized that most researchers are concerned 
with what Kuhn calls normal science. Therefore the methodology of normal 
science continues to be of great importance in epistemology. Moreover, the 
analysis of the descriptive and inductive methods of normal science appears 
to be a primary goal of statistics. A number of recent studies on normal 
science attempt to combine some conceptual tools borrowed from 
epistemology with others borrowed from statistics (see Rosenkrantz (1977, 
1981), Good (1983) and Howson and Urbach (1989)). The contextual 
approach to prior probabilities is part of this methodological tendency since 
it suitably integrates some conceptual tools borrowed from epistemological 
studies on the optimum inductive methods and others borrowed from 
statistical studies on the choice of prior distributions. 
Prior probabilities denote the initial probabilities which scientists attribute 
to the hypotheses under consideration before performing a given experiment 
(cf. Chapter 3.1). The subjective view and the aprioristic view represent the 
'traditional' standpoints on the choice of prior probabilities. According to the 
subjective view the choice of prior probabilities is restricted only by the 
probability axioms. Since such axioms are compatible with an extraordinary 
variety of prior distributions subjectivists think that there cannot be any 
rational consensus among scientists before performing the experiment ( cf. 
Chapter 7.1 and 7.3). According to the aprioristic view, on the contrary, the 
choice of prior probabilities is restricted not only by the probability axioms 
but also by certain a priori principles which always allow identification of a 
150 

CONCLUDING REMARKS 
151 
unique admissible prior distribution; hence there cannot be any rational 
disagreement among scientists before carrying out the experiment ( cf. Chapter 
7.2 and 7.4). The choice of prior probabilities is one aspect of the more 
general issue concerning the rational (dis)agreement in science. While the 
subjective view of prior probabilities explains the possibility of rational 
disagreement and the aprioristic view that of rational consensus, neither view 
succeeds in explaining how both rational disagreement and rational consensus 
can occur before performing the experiment. 1 
As pointed out by Laudan (1984, p. 3), an important task of methodological 
analysis is to delineate "some machinery that explains how both consensus 
and dissensus can arise". It seems to me that the contextual view of prior 
probabilities helps to tackle this task. Indeed according to this view the choice 
of prior probabilities is restricted not only by the probability axioms but also 
by certain contextual principles and other 'contextual constraints' which 
depend on the cognitive context of the researcher (cf. Chapter 7.5). This 
implies that if the cognitive contexts of several researchers are identical, there 
should be complete rational consensus among them, while if such cognitive 
contexts are to some extent dissimilar, a corresponding degree of rational 
disagreement may arise. 
According to an epistemological view called local induction ( cf. Bogdan, 
1976) the appropriate rule of inductive acceptance for a given inquiry can be 
seen as the 'output' of a number of different 'contextual inputs' (such as the 
problem under consideration and the cognitive goals of the inquiry). Although 
local induction has so far dealt almost exclusively with the rules of inductive 
acceptance, it might more generally be defined as a methodological 
programme of research which aims to provide a contextual justification for 
the inductive procedures used in scientific research. The contextual approach 
to prior probabilities can be seen as a possible development of local 
induction. Indeed the optimum prior distribution for a given inquiry is seen 
as the output of certain contextual inputs such as the available background 
knowledge and the cognitive goals of the inquiry (cf. Chapters 7.5 and 8). 
The verisimilitude theory might be seen as a particular formulation of local 
induction. In other words VT can be seen as a methodological programme of 
research which aims to justify inductive procedures w.r.t. to a specific 
cognitive goal represented by the achievement of a high degree of 
verisimilitude. The 'verisimilitude programme' can deal with a wide variety 
of cognitive contexts since the verisimilitude thesis (VER) underlying VT 
leaves considerable freedom to select the measure of verisimilitude (distance 
from the truth) to be adopted in a given inquiry.2 The V-solution to EPO 
proposed in Chapter 8.5 can be seen as a contribution to the verisimilitude 

152 
CHAPTER 11 
programme since the optimum prior Dirichlet distribution Dir(y 0,A o) is 
intended to minimize the distance between certain Dir(y 0,A o )-based point 
estimates of the investigated parameter vector and the truth. 
Some possible developments of the contextual view 
In Chapter 7.5 a contextual justification of the requirement of neutrality (CM) 
and, accordingly, of the decision to use a Dirichlet prior (a GC-system) was 
given. Some possible developments of the contextual view of prior 
probabilities are: (i) to provide a contextual justification for appropriate 
families of prior distributions (inductive methods) for various kinds of 
multinomial contexts where (CM) does not hold;3 (ii) to provide a contextual 
justification for appropriate families of prior distributions (inductive methods) 
for Markov processes and other multicategorical processes different from 
multivariate Bernoulli processes;4 (iii) to provide a contextual justification for 
appropriate families of prior distributions for various types of 'quantitative 
processes' such as the normal process and the Poisson process.5 
The CC-solution to EPO for a given set of prior Dirichlet proposed in 
Chapter 8.3 was based on an external estimate of the Gini diversity of the 
investigated 
multivariate 
Bernoulli 
process. 
However 
'alternative 
CC-solutions' to EPO based on external estimates of other diversity measures 
deserve to be studied.6 The problem of identification of the epistemically 
optimum prior for a given inquiry (EPO) may be defined not only w.r.t. a 
given set of Dirichlet priors but also w.r.t. other sets of priors applicable to 
various kinds of quantitative processes. The possibility of using external 
estimates of the 'quantitative variability' of the considered processes to work 
out adequate CC-solutions to such EPO's deserves careful examination.7 
The V-solution to LPO for a given set of Dirichlet priors was defined w.r.t. 
M-estimators and the quadratic distance used in the verisimilitude 
interpretation of such estimators (see Chapter 8.4). Moreover the V-solution 
to LPO was used- in combination with (VER) -to work out a V-solution to 
EPO (see Chapter 8.5). It seems to me that two important problems deserve 
to be carefully examined: (i) the possibility to work out alternative 
V-solutions to LPO, defined w.r.t. other Bayesian (point or interval) 
estimators and other distance functions,8 (ii) the possibility to use such 
V-solutions to LPO for developing corresponding 'alternative V-solutions' to 
EP0.9 
Remember that a V-solution to LPO can be defined not only w.r.t. a given 
set of prior Dirichlet but, more generally, w.r.t. any set of prior distributions 
on the parameter (vector) of a given experimental process (see Chapter 8.4). 

CONCLUDING REMARKS 
153 
It seems to me that a reason for carefully exploring certain V-solutions to 
such LPO's is to ascertain whether they can help work out adequate 
V-solutions to the corresponding EPO's. 

NOTES 
CHAPTER 1 
After World War II the Bayesian approach has gained increasing support among 
statisticians and epistemologists. In particular, in the last fifteen years the Bayesian approach 
to epistemology has been developed by several authors such as Rosenkrantz (1977,1981), 
Horwich (1982), Levi (1980), Howson and Urbach (1989). Although the contentiol! between 
the Bayesian approach and other statistical and epistemological approaches is an intriguing 
topic (see, for instance, Barnett (1973), Howson and Urbach (1989), Barman (1992)) it is not 
my topic since my concern in this book is with problems arising within the Bayesian 
approach. 
2 The theory of inductive probabilities (developed by Carnap and other epistemologists) 
deals with certain types of inductive inferences, such as prediction of future events, which are 
also typical subjects in philosophical research on induction. In particular, the problem of 
assessing the probability of a future event - which had already been considered by Hobbes 
(1650) - has received much attention after it was studied by Hume (1739): see Hacking 
(1975, p. 48 and 178). 
3 Following a common epistemological usage, here "hypothesis" refers to whatever factual 
statement, from specific predictions to highly general theories. On the contrary, in statistics 
"hypothesis" is typically used with a much narrower meaning. Unfortunately, many other 
terminological conflicts occur between epistemology and statistics. Although I tried to make 
the text sufficiently clear to readers with a background in any of the two fields, a bias 
towards the epistemological jargon was inevitable given my own background. Hence, some 
tolerance is requested to those readers who will fmd familiar terms employed with a 
unfamiliar meaning (or vice versa). 
4 This term is used, among others, by Swinburne (1973) and Skyrms (1966). 
5 Of course scientists qua human beings pursue non-cognitive goals even within their 
scientific activity. For instance, "many people called scientists regard science as some 
battlefield where 'being regarded as clever and correct' is more important than 'really having 
done all the work'" (Prof. W. Schaafsma, private communication). 
6 The notion of cognitive context is borrowed from Levi (1967). 
154 

NOTES 
155 
Empiricist philosophers such as Francis Bacon and rationalist philosophers such as 
Descartes, although holding different views about the nature of scientific method, shared the 
same infallibilistic view - which Watkins (1978, p. 25) calls the "Bacon-Descartes ideal" -
about the goal of science. The infallibilistic ideal was also advocated by scientists and 
philosophers such as Boyle, Locke and Newton. 
8 Here "the truth" is intended in the sense of the correspondence theory of truth suggested 
by Aristotle and accepted, more or less explicitly, by most infallibilists: a statement is true 
if and only if it corresponds with reality, i.e., with the way things really are. According to 
the infallibilistic view the scientific method, if properly used, infallibly guarantees the 
discovery of true theories (see Laudan, 1973, p. 277). 
9 Indeed certainty about a given statement may be seen as the maximum degree of belief 
in the truth of the statement. 
1° For instance, Descartes (ca. 1628) maintains that we should "reject ... merely probable 
knowledge and make it a rule to trust only what is completely known and incapable to be 
doubted." 
11 A fascinating inquiry into the 'emergence of probability' in modem thought is made by 
Hacking (1975). 
12 
The origins of the concept of verisimilitude and, more genera]] y, the fallibilistic 
methodologies are traced by Niiniluoto (1987, Ch. 5). 
13 It would appear that in the last century a number of philosophers had already recognized 
that the probabilistic and verisimilitude views are not incompatible (see Laudan, 1973, pp. 
285-286 and 2<,5). However, according to Laudan (ibid., p. 286) "these two approaches did 
... represent different emphases, and were to give rise in the twentieth century to two very 
different strains in philosophy of science (Camap and Keynes being the descendants of the 
progress by probabilification school, and Popper and Reichenbach focusing primarily on 
progress by self-correction)". 
14 
Another example of a co-ordinated development of the Bayesian approach and the 
verisimilitude theory is given by the notions of expected verisimilitude and probable 
verisimilitude which can be defined using the concepts of epistemic probability and 
verisimilitude (see Chapter 4.3). 
15 Following a common epistemological usage, here "inference" is employed in the sense 
"argument". This conflicts with the more frequent statistical usage of "inference" in the sense 
of "conclusion" ( cf. note 3). 
16 
The conclusion of an inductive inference, indeed, is frequently termed "hypothesis" 
because of its conjectural character. 
17 As pointed out by Hacking (1975, Chapter 2), since its emergence in the Western thought 
probability was essentially dual, on the one hand having to do with degrees of belief 

156 
NOTES 
(epistemic probability), on the other, with devices tending to produce stable long-run 
frequencies (physical or objective probability). 
18 See Chapter 2.1 and Chapter 11, note 4. 
19 The term "Generalized Carnapian (GC-)systems" is borrowed from Kuipers (1978). 
20 See Chapter 7, note 18. 
21 Gini diversity is a measure of the degree of disorder of a population or process (for a 
more detailed description see Chapter 10). Contrary to what one might believe at first sight, 
Est[G(q)] is different from Gini diversity G(y") of the prior vector y0 : see formula (8.7) and 
Chapter 8, note 5. 
22 On the other hand idealizations - which play a basic role in empirical sciences - may be 
very useful also in methodological analysis. This basically depends on the possibility of 
finding interesting generalizations and 'concretizations' of the proposed idealizations (cf. 
Chapter 11 where some possible extensions of the present approach to EPO are suggested). 
CHAPTER 2 
1 All non-quantified variables - such as n, e. and e~ - contained in the formulae and 
requirements of adequacy stated herein should be interpreted as universally quantified. 
2 The axiom of convergence was suggested to Camap in 1953 by Hilary Putnam, who 
proposed for it the name "Reichenbach axiom" (cf. Camap, 1980, p. 120). 
It can also be proved that (Exc) is logically equivalent to the conjunction of two 
requirements given in terms of special values (see Kuipers, 1978, pp. 40-41). 
4 Later this view was defended by Carnap (1950, pp. 208 and 571-575). 
5 Apart from some slight terminological modification, this classification is borrowed from 
Carnap (1950, pp. 205-208). 
6 
The inference from a sample to the population is usually referred to as the inverse 
inference (cf. Carnap, 1950, pp. 207-208). 
1 On the whole, Camap devotes little attention to statistical inferences. He only provides 
some formulae to calculate the probability of point hypotheses such as "q1 = 0.3" for a finite 
population (Carnap, 1950, p. 570 and 1952, pp. 30-32). As far as I know during the fifties 
one of the few attempts to investigate the possible statistical implications ofCarnap's TIP was 
made by Tintner (1949) who referred to the manuscript of The Logical Foundations of 
Probability (Carnap, 1950). 

NOlES 
157 
8 For a more detailed description of (multivariate) Bernoulli processes see Feller (1968, Ch. 
6) and La Valle (1970, Ch. 11). 
The notions of "multivariate Bernoulli process/multinomial context" used herein 
correspond approximately to the notions of "repeatable experiment/multinomial context" as 
defined by Kuipers (1978, pp. 114-115). 
CHAPTER 3 
1 Various rules of inductive acceptance have been proposed in statistics and epistemology. 
Furthermore, the meaning of the inductive acceptance of a hypothesis has been interpreted 
in different ways. While the 'weakest' type of acceptance presumably involves selection of 
a hypothesis for further scrutiny (see, for instance, Popper's methodological rules and Fisher's 
significance tests), the 'strongest' type presumably involves the incorporation of the accepted 
hypothesis within the available background knowledge which will be taken as guaranteed in 
subsequent inquiries (see, for instance, Levi's acceptance rules (1976)). Other rules of 
inductive acceptance seem to lie somewhere between these two 'extremes'. For instance, point 
estimation could be seen as a type of 'as if-acceptance' where the estimated value of a given 
parameter is tentatively used, especially in calculations. A stronger type of inductive 
acceptance appears to be represented by interval estimation, and an even stronger type by the 
Neyman-Pearson hypothesis testing. 
2 The probabilistic approach has been supported by Camap (1968a), Jeffrey (1956, 1968, 
1970), Bar-Hillel (1968), Box and Tiao (1973, Appendix A5.6) and others. The supporters 
of the acceptational approach include, among others, Hempel (1962), Hintikka and Pietarinen 
(1966), Hilpinen (1968), Kyburg (1974), Levi (1967, 1976, 1980), Kaplan (1981), Harsanyi 
(1985), La Valle (1970) and Zellner (1971). 
3 The origins of the predictivistic approach to statistical inference date back at least to 1774 
with the famous Laplace's rule of succession (cf. Zabell, 1989). The predictivistic approach 
received strong backing from Roberts (1965). A systematic exposition of the predictive 
methods used in statistics is given by Aitchison and Dunsmore (1975). 
In conflict with the monistic approaches mentioned in the text - which emphasize the 
methodological relevance of one of the different kinds of inferences - it could be argued that 
different kinds of inductive inferences are appropriate in different situations. For instance, it 
seems clear that predictive inferences play a crucial role in risk analysis while in theoretical 
physics global inferences are much more important. This suggests that a pluralistic approach 
would provide a more adequate image of the inductive inferences demanded in scientific 
practice. 
4 The predictive distribution p(y!z) specifies the epistemic probabilities of the possible results 
of a future experiment y relative to the result z of a previously performed experiment z. 
s As Winkler (1972, p. 143) points out, continuous distributions "are studied because (1) 
they are often mathematically easier to work than discrete models, and (2) they provide an 
excellent approximation to numerous discrete models." 

158 
NOTES 
6 In the first case the probability centres on single values of the parameter vector and can 
be based on a joint probability function p(()1,()2) analogous to the one described in the 
univariate case. In the second case it covers the entire plane or some subregion of it and no 
single point receives a positive probability. 
7 
Here p(Q/y) and p(Q) are either both probability functions or both density functions, 
according to whether 0 is discrete or continuous, and p(y/Q) and p(y) are likewise either both 
probability functions or density functions according to whether y is discrete or continuous. 
8 Cf. Aitchison and Dunsmore (1975, pp. 1 and 17-23). 
9 Formula (36) can be proved as follows. 
From (22)(b) and the equalities E(q1 'll) = p(Q,Qi ), E(q1) = p(Q,) and E(IJ.J) = p(Q) (see 
(34) and (32)), it follows that cov(q1 ,IJ.J) = p(Q,Qi)- p(Q,)p(Qi) = p(Q,)p(Q/Q;) -
p(Q,)p(Qi) = p(Q;)IP(Q/Q,) - p(Qi)], Q.E.D. 
10 Formula (40) follows from var(q1l e) = p(Q,QJe) - IP(QJe)f (see (35)), E(q_, I e)= 
p(Q)e) and the sequence of equalities: p(Q,QJe) - IP(QJe)]2 = p(QJe)p(QJeQ,) -
p(QJe)p(QJe) = p(QJe)IP(QJeQ,)- p(QJe)]. 
11 Formula (41) follows from the equalities E(q1 l e) = p(QJe) and cov(q1,1J.JI e) 
p(QJe)IP(Qi!Eq,)- p(Q)e)] (cf. (36)). 
12 Formula (42) follows from the sequence of equalities: V AR(q) = ~var(q 1 ) (from (37)) = 
~(E(qi) - [E(q1 W} (from (20)(b)) = ~(qi) - ~[E(q1 )]2 = E(~qi) - C[E(q)] = E[C(q)] -
C[E(q)]. Taking into account that E(G(q)] = 1- E(C(q)] and G[E(q)] = 1 - C[E(q)] it follows 
that VAR(q) = G[(E(q)] - E[G(q)]). 
13 The pdf's f(q1) and f'(q1) correspond to two different cognitive contexts. A researcher X 
adopting f(q1) is inclined to believe that one of the objective probabilities q1, q 2 is far higher 
than the other, though he is completely uncertain which one is higher. This uncertainty is 
revealed by the equality E(q1) = E'(q2). For instance, consider the case where the objective 
probabilities that a tossed coin turns up heads or tails are represented by q1 and q2, 
respectively. If the coin appears noticeably irregular and X is totally uncertain whether the 
heads or the tails will profit from this irregularity, then he would presumably adopt a cdf such 
as f(q1). On the contrary,Xwill adopt a prior such as f'(q1) whenever he is inclined to believe 
that q1 and q 2 have very close values. 
14 Clause (a) follows from the equalities E[C(q)] = E(~qD = ~(qi) and E(qi) = p(Q,Q;) 
( cf. (33)). 
Clause (b) follows from (a) taking into account that E[G(q)] = 1 - E(C(q)). 
15 Analogously, the uniform distribution on the parameter vector q- which attributes equal 
density f(q" ... ,qk_1) = 1 to all the values of q- determines a direct generalization of Laplace's 
rule, i.e., the inductive method characterized by the special values: p(QJe.) = 
(n, + 1)/(n + k). The evolution of the rule of succession from its original formulation 
(Laplace, 1774) to its recent generalizations is masterly traced by Zabell (1989). 

NOTES 
159 
16 Clause (a) follows directly from (48). 
Clause (b) can be proved as follows. 
Given the non-negativity of the marginal variances var(q1l e) derived fromF(q ..... ,qk_1) (cf. 
(20)(a)), it follows from the 'conditional formulation' of (35) that p(Q,QJe)- [p(QJe)f :2: 0. 
From this and the equality p(Q,QJe) - [p(QJe)]2 = p(QJe)[p(QJeQ.) - p(QJe)] it follows 
thatp(QJe)[p(QJeQ,)- p(QJe)] :2:0. From this inequality, assuming thatp(QJe) is positive, 
it follows thatp(QJeQ.)- p(QJe) :2:0 and, equivalently,p(QJeQ,) :2: p(QJe.) where the final 
inequality expresses the requirement of non-negative relevance (NNR). Lastly, when p(QJe) 
= 0, then (NNR) is trivially satisfied. Q.E.D. 
CHAPTER 4 
1 More generally, if a loss function such ash(()- a)2 - where h > 0- is used, the solution 
to the decision problem remains unaltered. 
2 See Niiniluoto (1986; 1987, Ch. 12.5) and Festa (1986). 
3 The use of the quadratic distance D 2(a,()) appears to be appropriate in situation where the 
cognitive loss of accepting a, when the truth is (), is not equal to the geometrical distance 
I a -()I but increases in a way more than proportional to I a-() 1. 
4 According to Lewis (ibid., p. 62), the principle of immodesty is a necessary condition for 
a "stable trust'' in a given inductive method: "If JOU wish to maximize accuracy in choosing 
an inductive method, and you have knowingly given your trust to any but an immodest 
method, how can you justify staying with the method you have chosen? If you really trust 
your method, and you really want to maximize accuracy, you should take your method's 
advice and maximize accuracy by switching to some other method that your method 
recommends. If that method also is not immodest, and you trust it, and you still want to 
maximize accuracy, you should switch again; and so on, unless you happen to hit upon an 
immodest method. Immodesty is a condition of adequacy because it is a necessary condition 
for stable trust." [Author's italics]. 
5 For more details on Lewis's approach to EPO for C-systerns see Chapter 9.2. 
6 Note that the expected value E[DiE(y),O)] may be interpreted as the p({})-based estimate 
of the distance between the future estimates of 0 made by the p({})-based M-estimator E and 
the truth. Similarly, E[D2(T(y),O)] may be interpreted as the p({})-based estimate of the 
distance between the future estimates of 0 made by the estimator T and the truth. 
Moreover, it should be pointed out that, if the immodesty of priors is defined w.r.t. the 
corresponding p({})-based M-estimates E(y) (see definition (7)), then the quadratic distance 
D2(E(y),O) between such M-estimates and the truth may be justifiably used since such a 
distance function is the rationale for using M-estimators (see Section 1). 
7 Recall that the prior predictive probability p(y) is derived from the available p(()) and 
p(yf()) applying formula (3.27). 

160 
NOTES 
8 This is because all the properties used to prove (9) apply to any cdf Similarly, definition 
(7) and theorems (9) and (10) can be extended to any conditional cdf F((}fe) where e denotes 
an arbitrary background knowledge. 
9 Indeed Spielman proves that all C-systems are immodest Moreover, Spielman (ibid., p. 
377) argues that "it can be shown by similar reasoning that all inductive methods that satisfy 
de Finetti's condition of exchangeability are immodest." More generally, Horwich (1982, p. 
87) argues that "every inductive practice is immodest." However, Horwich's results apply 
only to "inductive practices" concerning discrete parameters. 
10 Indeed there is now an extensive and ever increasing literature on verisimilitude. To 
reach a rather exhaustive knowledge of this topic one might read the three books on the 
subject published so far: two treatises (Niiniluoto, 1987a; Oddie, 1987) and a collection of 
papers containing "a parade of approaches to truthlikeness" (Kuipers, 1987a). For a short 
survey of the current state of the debate concerning verisimilitude see Brink (1989) and Miller 
(1990). 
11 An exception is represented by Schurz and Weingartner (1987) who show that Popper's 
consequence approach can be rehabilitated by imposing appropriate restrictions on the 
classical notion of consequence-class of a theory used in Popper's original definition. More 
specifically, they (1) eliminate 'irrelevant' and 'redundant' consequences from the 
consequence-class so to obtain the so-called class of relevant consequence-elements and (2) 
apply to this class Popper's defmition of closer-to-the-truth. They prove that this strategy 
blocks the otherwise fatal objections of Tichy and Miller against Popper. 
12 The above mentioned approaches to the explication of verisimilitude have been applied 
to many kinds of hypotheses considered in empirical sciences. For instance, the similarity 
approach initially was applied to hypotheses stated in propositional logic and in monadic first-
order logic but later extensions covered full first-order logic, higher-order and intensional 
logics, probabilistic hypotheses, and hypotheses expressed in quantitative languages (for more 
information see Niiniluoto (1987, p. 198). 
13 The 'Bayesian estimation-approach' has also been applied to the analysis of appropriate 
procedures for point and interval estimation (see Niiniluoto 1986, 1987, Ch. 12.5; Festa, 
1986). 
14 
For a detailed exposition of the Bayesian approach to the epistemic problem of 
verisimilitude see Niiniluoto (1987, Ch. 7 and 12). 
15 In particular, see Kuipers (1992). 
CHAPTER 5 
1 De Finetti (1937, pp. 124-129) proved his representation theorem for the case where 
k = 2. Subsequently, DFRT has been generalized to k > 2 (cf. Good, 1965, pp. 21-23). 

NOTES 
161 
2 Indeed the equality used in (2)(a) to derive p1 from F1 (q1• ••• ·qk_1) is the same as that 
used in (3.48) to derive a predictive distribution from a probability distribution on q. 
3 Cf. Hintikka (1971, pp. 329-336). 
4 Cf. Gaifrnan (1971, § 3) and Fine (1973, p. 194). See also Jeffrey's Editor's Note to 
Carnap (1980, § 20, p. 120). 
5 De Finetti conceived his representation theorem as a tool to get rid of unknown objective 
probabilities and, accordingly, considered probability distributions on unknown objective 
probabilities as "mere mathematical fictions" (Hintikka, 1971, p. 333). 
6 It may also be assumed that, in certain cases, X believes that the 'multivariate Bernoulli 
model' provides a convenient approximation to (idealization ot) the real 'structure' of the 
multicategorical process Ex in examination. 
7 For the 'convergence towards the truth' of predictive probabilities see Hintikka (1971, pp. 
336-339). See also Niiniluoto (1980, pp. 433-434 and p. 454, note 34). 
CHAPTER 6 
1 The terms "Generalized Carnapian systems" and "GC-systems" - together with many other 
terms, symbols and definitions used in this section- are borrowed from Kuipers (1978, 1980). 
2 As Pietarinen (1972, pp. 60-61) points out, C-systems were anticipated in 1925 by W. E. 
Johnson in a paper published posthumously in 1932. 
3 The requirement (RR) also implies that the predictive probabilities do not depend upon the 
variety of evidence e •. More precisely, (RR) implies that the special values p(QJe.) do not 
depend upon the number c of different kinds of individuals exemplified in e., i.e., the number 
c of categories Q, such that n, > 0. 
A universal hypothesis h = "All the individuals in the population U belong to one of the 
categories Qi1, ... ,Q,g'' can be called a genuine universal generalization when Qi1, ... ,Q,g is a 
proper subset of {Q1, ••• ,Qk}. It is well known that in an infmite population U the prior 
probability p(h) and, therefore, the posterior probability p(hle.) assigned to a genuine 
universal generalization h by C- and GC-systems are zero. As pointed out by Niiniluoto 
(1976, p. 428ff.), the construction of inductive methods which attribute non-zero prior 
probabilities to genuine universal generalizations requires the rejection of (RR) and the 
adoption of requirements of adequacy whereby the predictive probabilities p(QJe.) depend 
not only on n, and n but also on the variety of evidence c. Indeed requirements of this kind 
have been adopted by Hintikka and Niiniluoto (1974), Niiniluoto (1976), and Kuipers (1978, 
Ch. 6). 
4 At the turn of the century several students- including Hardy (1889), Withworth (1897, pp. 
224-225), Gini (1911) and Lidstone (1920) - suggested that Beta distributions might be the 
appropriate priors for the Bayesian analysis of binomial inferences: see Good, 1965, p. 17; 

162 
NOTES 
Zabell, 1982, p. 1092; and Costantini, 1978, pp. 313-314; 1984, p. 154. For a description of 
the main features of Beta distributions see Wilks (1962, pp. 173-175). 
5 The uniform distribution for q1 is represented by the pdf"f(_q1) = 1" which attributes equal 
density to all possible values of qr 
Lidstone (1920) was probably the first to suggest to use Dirichlet distribution in 
multinomial inferences (see Good, 1965, p. 25). For an exposition of the main features of 
Dirichlet distributions see Wilks (1962, pp. 177-182). 
7 The uniform distribution on q is represented by the pdf"f(q1, ... ,qk.1) = 1" which attributes 
equal density to all possible values of q. 
8 For a long time the equivalence between GC-systems on the one hand and Beta and 
Dirichlet distributions on the other, apparently was not noticed. As far as I know this 
equivalence was pointed out first by Good (1965, p. 17): "It seems possible that G. F. Hardy 
[who defined Beta distributions] was the first to suggest a 'continuum of inductive methods', 
to use Camap's phrase." Later, the equivalence between GC-systems and Dirichlet (Beta) 
distributions was proved by Jamison (1970, pp. 47-49), Rosenkran1Z (1977, pp. 71-73), and 
others. 
9 The only difference is that pJQJe.) = nJn is not defined in the case where n = 0, whereas 
it follows from (34) that Py.o(QJe0) = Py.o(Q;) = y,. 
10 Several arguments against the straight rule and (y,O) have been given (see Camap, 1952, 
§ 14; 1980, pp. 85-86 and 95; Good 1965, pp. 15-18, 23, 28 and 36; Kuipers, 1978, p. 52). 
Note, for instance, that the apparently plausible requirement of adequacy (Reg) (see Chapter 
2.1) is not satisfied by extreme inductive methods (y,O) since they attribute prior probability 
zero to any 'heterogeneous' sequence e., i.e., to any sequence containing more than one kind 
of trial. 
11 In (38) the term "GC-systems" is interpreted to include the extreme GC-systems (y,O) 
and (y,oo) with A.-values A.= 0 and A.= oo. The class of 'proper' GC-systems, where 0 < A.< 
oo, is obtained by adding (i) the axiom of regularity (Reg), which leads to the exclusion of 
A. = 0, and (ii) the axiom of positive relevance (PR), which leads to the exclusion of A. = oo. 
12 Johnson (1932, pp. 421-423) anticipated not only Camap's C-systems (cf. note 2) but 
also their axiomatizations: see Good (1965, pp. 25-26) and Zabell (1982). 
13 Indeed (RL) - as opposed to (RR), which becomes tautological when k = 2- imposes a 
genuine restriction on the predictive probabilities p(QJe.) also when k = 2 (cf. Good, 1965, 
p. 26). 
14 
It follows from (39) that a unified axiomatization of GC-systems would consist in 
adopting the requirement of linearity (RL) for any k ~ 2. However, a serious defect of this 
axiomatization is that the intuitive meaning of (RL) is much less clear than that of the 
requirement of restricted relevance (RR) used in (38) (cf. Camap, 1980, pp. 101-102). 

NOTES 
163 
15 
Note that (CIRQ), if applied when k > 2, immediately excludes any analogy by 
similarity. However (CIRQ), as opposed to (RR), enacts a genuine restriction on the 
predictive probabilities p(QJe.) also when k = 2. 
16 A possible criticism of Costantini's axiomatization is that the new axiom (CIRQ) is less 
intuitively plausible than the 'old' axiom (RR). 
17 Besides the objective probabilities governing a multivariate Bernoulli process, many other 
types of proportions are considered in the empirical sciences. In fact, proportions are of 
interest whenever "some unit is broken into parts, such as proportions (by weight) of various 
chemical constituents of a substance" (Connor and Mosimann, 1969, p. 194). For some 
examples concerning the biological sciences see Mosimann (1962, pp. 77-81) and Connor and 
Mosimann (1969, pp. 200-205). 
Given the constraint kq1 = 1, the proportions q1, ••• ,qk cannot be independent of each other 
in the usual sense (as defined in (3.4)); indeed "statisticians and biologists alike have been 
wary of the correlations existing among proportions or percentages since Pearson's (1897) 
paper on spurious correlations" (Mosimann, 1962, p. 65). However, very often these 
correlations do not denote any 'genuine' correlation among the proportions but simply "serve 
as measures of the correlation due to the constraint [kq1 = 1 ]"(Connor and Mosimann, 1968, 
p. 78). 
The concept of neutrality aims to individuate those cases where there is no genuine 
correlation among the proportions q1, .•. ,qk. More precisely: when all the proportions q1 are 
neutral w.r.t. q=(q., ... ,qJ, then the correlations existing between the different couples of 
proportions depend uniquely on the constraint kq1 = 1. 
18 Other 'neutrality-based' parametric requirements conceptually akin to (CM) have been 
used to formulate other axiomatizations of Dirichlet distributions (Darroch and Ratcliff, 1971; 
Doksum, 1974, pp. 187-188; and James and Mosimann, 1980). 
CHAPTER 7 
1 For a survey of interviewing procedures see Winkler (1972, pp. 182-192). 
2 The mean of q1 can be seen as X's estimate of q1 and the variance of q1 as X's uncertainty 
about his own estimate ( cf. Chapter 4.1 ). Since "it is very difficult to assess the variance of 
a distribution intuitively" (Winkler, ibid., p. 189), more feasible methods of fitting a Beta 
distribution should be considered (see Winkler, ibid., p. 188-189 and Novick and Jackson, 
1974, pp. 167-170). 
A well known example of such paradoxes is the so-called Bertrand's Paradox (see 
Rosenkrantz, 1977, pp. 73-74). 
4 A concise exposition of Jeffreys's theory of invariance and related results is given by 
Zellner (1971, pp. 41-53). See also Barnett (1974, p. 178-179) and Seidenfeld (1979a, p. 417-
423). 

164 
NOTES 
5 In one sense the problem of 'arbitrariness of the group of transformations' is analogous 
to the problem of arbitrariness of parametrization: see Fine (1973, p. 172) and Milne (1983, 
p. 52). 
6 Indeed, in principle it cannot be excluded that certain invariance requirements are also 
satisfied by intuitively 'very informative' priors. This would imply that invariance properties 
are not sufficient to identify appropriate ignorance priors. 
7 See, for instance, Perks (1947), Hartigan (1964), Villegas (1977), Novick and Hall (1965) 
and Novick (1977). 
8 An excellent account of Jeffreys's theory of Invariants is given by Huzurbazar (1976, part 
1). 
9 
Box and Tiao (1973, p. 25) argue that "a prior distribution is supposed to represent 
knowledge about parameters before the outcome of a projected experiment is known. Thus, 
the main issue is how to select a prior which provides little information relative to what is 
expected to be provided by the intended experiment." An even more radical formulation of 
the view that prior distributions should be experiment-dependent is given by Skilling (1985, 
p. 13) who points out that "different experiments can demand different priors for their 
interpretation. It ... follows that the prior cannot be identified with the current state of 
knowledge, as we have assumed until now." Rather the prior should be "identified with the 
question being asked of the experiment" (ibid., p. 15). 
10 
Bernardo (1979) offers an interesting analysis of reference priors and also gives an 
extensive bibliography on the subject. 
11 Usually a significant proportion of the background knowledge BK is 'tacit knowledge' 
which can be made explicit by self-interrogation and critical discussion. 
12 For an introductory account of the 'meaning' and the properties of cross entropy see 
Rosenkrantz (1981, Ch. 4.3 and 4*.3). As pointed out by Williams (1980, p. 133, note 1), the 
quantity IK (/2,/1) and other expressions of this type have already been studied by Good 
(1950), Kullback and Leibler (1951), Savage (1954) and Lindley (1956). 
13 A formulation of this principle has already been given by Good (1963): cf. Williams 
(1980, p. 134, note 1). On the origins of MAXENT and, more generally, the so-called 
information-theoretic statistics see, for example, Guiasu (1977, Ch. 17), Jaynes (1979, pp. 16-
44) and Rosenkrantz's introduction to Jaynes (1983). 
14 For the interpretation of distributional constraints, see Seidenfeld (1979a, pp. 423 and 
438, n. 20). 
15 For instance, Shimony (1985), Seidenfeld (1979a, 1986) and other authors claim that 
(PME) may leads to the selection of unacceptable prior distributions. 

NOTES 
165 
16 Claim (1) can be seen as a particular case of a more general 'least change principle' 
proposed for dealing with the so-called probability kinematics (cf. May and Harper, 1976, p. 
139-140 and Domotor, 1985, pp. 75-76). 
11 
For example, alternative measures of distance between (epistemic) probability 
distributions are considered by May and Harper (1976, p. 145), Domotor (1985, pp. 88-90) 
and Gaifman (1986, pp. 325-331). 
18 The larger X's inductive inertia A. the slower X modifies his a priori vector yin response 
to empirical evidence e. (cf. (6.4)). 
19 Some ambiguities of Carnap's views concerning the nature and role of inductive intuition 
are pointed out by Giere (1975, p. 195): "The axioms are justified to the extent that the 
derived consequences agree with 'our' (Whose? How determined?) intuitions concerning 
which actions, beliefs, etc. are rational in these possible cases .... The appeal to such intuitive 
principles raises several important philosophical questions. For example, What gives such 
principles normative rather than merely descriptive force? ... Can one not ... ask whether the 
principles 'we' in fact accept are really correct principles of rational behaviour?" 
20 Carnap's argument includes the following weak point. Although (RR) implies T]-equality, 
(RR) is not implied by T]-equality. Hence, Carnap's assumption that (RR) should be adopted 
whenever T]-equality is assumed is not compelling. 
Consider, for instance, the class of inductive methods introduced by Hintikka and 
Niiniluoto (1976). While they do not satisfy (RR) it would appear that they satisfy T]-equality. 
For a concise description of such methods see Kuipers (1978, pp. 71-75; 1980, pp.188-189). 
21 It is clear from the context that Carnap's preference for A. = 1 also applies to equal-
distance families. 
22 
Let the properties Yellow and Orange be denoted by Y and 0. Then, the 'negative 
(inductive) analogy' between Y and 0 could be expressed by the inequality: p(O!e. Y) < 
p(Oie. ), where "0" denotes the hypothesis that the next observed car will be Orange, 
and "e. Y'' denotes the evidence obtained by adding the observation of a yellow car to the 
previously observed sample e. randomly drawn from the population of cars. Analogously, the 
'positive (inductive) analogy' between Yellow and Orange could be expressed by the 
inequality p(Oie. Y) > p(O!e. ). 
23 I maintain that in the example described herein, requirement (RR) should not be accepted 
given that BK reveals the existence of a 'negative analogy' between Yellow and Orange. Note 
that the reasons for rejecting (RR) are the opposite to those which would be suggested by 
Carnap who would have referred to the (alleged) universally valid 'positive analogy' between 
Yellow and Orange. 
24 It should also be pointed out that if the value A. = 1 - tentatively proposed by Carnap as 
the optimum A.-value- is adopted then E[G(q)] = [A./(A. + l)]G(y) = (1/2)G(y). In other words, 
if A. = 1 is adopted, then the expected disorder of the population under investigation is equal 

166 
NOTES 
to half the maximum value G(y). Once again it is difficult to see why this strong a priori 
belief should be adopted in any empirical research. 
25 Note that this distribution, being the only one to satisfy the constraints included in /, is 
also - very trivially - the MAXENT distribution. This suggests that a priori inductive 
principles such as (PME) or (MCB) are dispensable whenever appropriate distributional and 
formal constraints are adopted. A different example where a formal constraint is sufficient 
to determine the uniquely admissible probability distribution is considered by Spielman (1981, 
p. 358). 
26 In clause (i) it is assumed that the background knowledge BK shared by a given group 
of researchers also includes some more or less explicitly stated 'standards of similarity' to 
be used in the identification of the experimental processes 'sufficiently similar' to Ex. 
27 
The term "external" is used because Ein(O,Ex) does not describe any experimental 
evidence obtained from any experiment y related to Q (in Ex) by a likelihood function p(y/Q). 
28 Note that the two formal properties stated in (Ext) are also satisfied by expected values 
and several other 'formal' estimates used in statistics. 
29 This involves, among other things, the supposition that the ratios aj /(1 - a:) do not 
display any appreciable tendency to increase - or decrease - when the corresponding values 
of a: increase. 
30 Although a formal definition of "H1, ••• ,Hm-independence of Qi from Qi' could be given, 
it is not indispensable for our purposes, since a careful inspection of the two sets a), ... ,a; and 
a)/(1 - al), ... ,aj/(1 - ai) appears sufficient to recognize the presence of an appreciable 
dependence between them. 
31 
Recall that 'll /(1 - q1 ) is independent of q 1 when the probabilities attributed to the 
possible values of 'll /(1 - q1 ) are not affected by information about the value of q1 (see 
Chapter 6.6). 
32 
Given that all GC-systems satisfy the requirement of restricted relevance (RR), my 
contextual justification of (CM) (indirectly) provides a contextual justification of (RR). 
CHAPTER 8 
1 The restriction "if m is not too low" on the estimation procedures (1) and (2) for Est(q1) 
can be justified as follows. 
When the only external information Eln(q,Ex) in BK is given by the experimental 
information H 1, ••• ,Hm, where m is very low - let us say m = 1, 2, or 3 - then there are no 
grounds for believing that the specific values of q specified by H1, ••• ,Hm are the result of some 
'causal mechanism' producing Ex1, Ex2, ••• , Exm and the other Ex-similar processes and not 
just a coincidence. Hence, any external estimate Est(qJ derived from H1, ••• ,Hm - by the 
application of procedures such as (1) or (2) - is doomed to be extremely unreliable. (Cf. the 

NOTES 
167 
analogous considerations made in Chapter 7.6, pp. 100-101, where it was claimed that the 
H" ... ,Hm-independence of Qi and Q, suggests the acceptance of "the independence of Q, and 
Q/' under the restriction that m is sufficiently high.) 
In cases such as those above - and, more generally, in any case where the external 
information Eln(q,Ex) included in BK is very poor- it would seem reasonable to select the 
symmetrical prior vector 1/k=(l/k, ... ,l/k) as the external estimate Est(q)=(Est(q1), ••• ,Est(q.J) 
of q. Presumably, the same decision should be adopted in the cases of maximum ignorance, 
where BK does not contain any external information Eln(q,Ex) 
2 
Already this decision can be seen as an application of the requirement of cognitive 
coherence (CC) which -given the external estimates Est(q1), ••• , Est(q.J- imposes adoption 
of the following m-constraints c 1, ••• ,ck on Dir(y,A.): c 1 = "E(q1) = Est(q1)", ••• , ck = "E(q.J = 
Est(q.J". 
3 The restriction "if m is not too low" on the estimation procedures (3) and (4) for Est[G(q)] 
is identical to the restriction suggested w.r.t. the procedures (1) and (2) for Est(q1) and can 
be justified along the same lines ( cf. note 2). Indeed, this restriction can be seen as the 
particular case of a more general restriction applicable to any procedure of the same type as 
(1)-(4), to be used for making an external estimate Est(IJ) of the parameter IJ of a given 
process Ex. 
The above mentioned restriction on (1)-(4) could be replaced by a more precise restriction 
such as: ''The estimation procedures (1)-(4) can be applied only if m « t" where t denotes a 
certain preselected threshold value. 
4 The remarks on the procedures (1 )-( 4) (see notes 2 and 3) apply also to the restriction "if 
m is not too low" on the procedures (5) and (6) for Est[Dly 0,q)]. 
Moreover, in the particular case where m = 1 - i.e., in the case where the available external 
information Eln(q,Ex) in BK is given by the proposition H1 which specifies the value 
a1=(a:, ... ,a~) of q in a certain Ex-similar process Ex1 there is another reason for not using the 
estimation procedures (1) and (5). 
Indeed, in the case where m = 1, one can see that: (i) the estimation procedure (1) leads 
to the external estimates Est( q1) = al for any q1 and, consequently, to the selection of the 
optimum prior yo= Est(q) = a1; (ii) the estimation procedure (5) implies that Est[Dly 0,q)] 
= D 2(y 0,a1). From the equalities yo = a1 
and Est[Dly 0,q)] = D 2(y 0,a1) it follows that 
Est[D2(y 0,q)] = Dla1,a1) = 0. The external estimate Est[Db 0,q)] = 0 can be seen as a formal 
representation of the guess that the external estimate Est(q) (= y") is exactly true. However, 
it 
is 
pretty 
clear 
that 
the 
above 
guess 
is 
unjustifiably 
optimistic, 
since 
Est(q)=(Est(q1), ••• ,Est(q.J) is based on the extremely poor experimental information HI" 
(Analogously, one can prove that, in the case where m = 1, the procedures (2) and (6) also 
lead to the unreliable external estimate Est[Db 0,q)] = 0.) 
One may wonder how an external estimate Est[D2(y 0,q)] could be made in the cases where 
m is very low and, more generally, in the cases where the external information Eln(q,Ex) 
included in BK is very poor. An apparently plausible possibility is suggested by the 
circumstance that Est[Dz(y 0,q)] = Est[DlEst(q),q)] may be interpreted as an external estimate 
of the 'inaccuracy' (distance from the truth) of the external estimate Est(q) = y0• Indeed, this 
interpretation appears to suggest that the value of Est[D2(y 0,q)] should increase when the 
'size' (and the 'reliability') of the external information Eln(q,Ex) - from which Est(q) is 

168 
NOTES 
derived - decreases. In particular, Est[D2(y 0,q)] should reach its maximum value in those 
situations where the 'size' of Eln( q,Ex) is minimum (see, for instance, the case of maximum 
ignorance described in the last paragraph of note 2). 
The above remark leads to the conclusion that any multinomial context where an external 
estimate Est(q)=Est(q1), ••• ,Est(qJ of q can be made- i.e., virtually any multinomial context 
( cf. note 2) - is a D-context. 
5 
Since all the possible values of the quadratic distance Diy 0,q) (= l:(y~ - q1 )~ are 
non-negative, it follows from (Ext)(b) that Est[D2(y 0,q)] :2: 0, i.e., that the minimum value of 
Est[D2(y 0,q)] is 0. Analogously, since all the possible values of G(q) (= 1 - l:qi 
= l:q; (1 - q; ): cf. the paragraph under formula (10.12), p. 142) are non-negative, it follows 
from (Ext)(b) that Est[G(q)] :2: 0, i.e., that the minimum value of Est[G(q)] is 0. Now, from 
the fact that the minimum value of Est[D2(y 0,q)] and Est[G(q)] is 0, it follows- taking into 
account that, due to (7), Est[D2(y 0,q)] + Est[G(q)] = G(y")- that the maximum value of both 
the external estimates Est[Db 0,q)] and Est[G(q)] is G(y"). 
6 It can be proved, indeed, that the equality in (7) is implied by (1 ), (3) and (5). To this 
purpose consider the following chain of equalities: 
m 
k 
k 
m 
~ ~ (y~- aDz 
1=1 
i=l 
~ ~(y~-a:l 
i=l t=l 
(from (5)) 
m 
m 
k 
m 
m 
k 
m 
~ [m(y~)2 + ~ (aD2 - 2y~ ~ a:J 
i=l 
t=l 
t=l 
~ [mM )2 + ~(aD2- 2m(y~ )2] 
i=l 
t=l 
m 
m 
(since it follows from (1) that~ a:= mEst(q1) =my~) 
t=l 
m 
k 
k 
~ ~ (aD2 - m ~ (YD2 
1=1 i=l 
i=l 
~ [C(a' )]- mC(y 0 ) 
t=l 
m 
m 
~ [1 - G(a')]- m[1 - G(y 0 )] 
t=l 
mG(y)- mEst[G(q)] 
(from (3)) 
m 
m 
G(yo)- Est[G(q)], Q.E.D. 
7 In the proof, which is similar to that given in the note above, it is assumed that the same 
weights w1, ••• ,wm are used in (2), (4) and (6). This assumption is plausible since in all three 

NOTES 
169 
cases the same experimental information H1, ••• ,H .. is used and the weights wt>····w .. are related 
to certain features of H 1, ••• ,H .. (cf. the remark below (2)). 
8 In the last paragraph of note 4 it was claimed that virtually any multinomial context is a 
0-context Since any 0-context is also a GO-context, this claim implies that virtually any 
multinomial context is a GO-context 
9 However, a statistician working on Bayesian elicitation might remark that a GO-context 
provides 'better' (i.e., more stable) results, because the researcher who finds that (7) is 
violated may wish to reconsider his external estimates- at least for D£y 0,q)) and G(q)) and 
possibly also for q1 - in order to come to a compromise fulfilling (3). This strategy is in line 
with the results obtained by many researchers (such as Novick and Jackson (1974), Hogarth 
(1980), Lourens (1984), Terlouw (1989)) who found that some overdetermination of the prior 
is very useful: inconsistencies lead to reconsideration, and consistencies give more trust in 
the results. This remark was suggested by Prof. Molenaar. 
10 
Note, in particular, that the CC-optimum value Ace becomes equal to 0 when 
Est[D2(y 0,q)) reaches its maximum value G(yj (cf. note 5) while it tends to oo when 
Est[D2(y 0,q)] tends to its minimum value 0. This suggests the integration of the CC-solution 
by putting Ace = oo in the case where Est[Dz(y 0,q)) = 0. 
11 Note that the solution for high values of Est[D2(y 0,q)) implied by (9) can be seen as a 
particular case of the choice of diffuse priors, usually suggested in Bayesian statistics, for 
those situations where there is very scarce background knowledge about the value of the 
parameter under investigation. 
12 Notice that D(T(Q I y),Q) depends both on the value (}of Q and on T(Q I y) which, in tum, 
depends both on the estimator T and the result y of the experiment y which has been 
performed. Whilst y and T are selected by X, the actual state of nature (} and the actual 
experimental result y are, as it were, selected by 'the world'. 
13 See the justification for using the quadratic distance as provided in Chapter 4, note 6. 
14 Theorem (22) can be proved as follows. 
E(D£v(e. ),q) I q) = £[~ (nJn - qj) = ~[E(nJn - q, )2)) = ~[var(nJn) + [E(nJn - q.))2) 
(from (3.20.b) and (3.20.c)). From these equalities, taking into account that E(nJn) = q, (from 
(3.24) and (3.16.a)) and, consequently, [E(nJn- q,)] = 0 = [E(nJn- q,)f (from (3.16.a)), it 
follows that E(D2(v(e. ),q) I q) = ~var(nJn) = ~[nq,(1- q,)!n2) (from (3.25)) = ~[q,(l- q.)!n] 
(1 - ~qf)ln = G(q)ln. 
15 It should be recalled that, analogously to all formulae in Chapter 3.2, formula (3.19) does 
not apply only to parameters and parameter vectors but also to random variables and mixtures 
of parameter vectors and random variables (cf. p. 23). Hence, it also applies to any 
probability distribution on ( q,e.) where q is a parameter vector and e. a random variable with 
a probability distribution which depends on the value of q. 

170 
NOTES 
16 According to Camap (ibid., pp. 70-71) the quantity Iqf = C(q) occurring in (30) can be 
seen as a measure of "the degree of order or uniformity or regularity" of a population. In this 
connection, he points out that (i) in the case of a completely homogeneous population- i.e., 
when one q, is 1 and all others are 0- the value of Iqf is maximum (Iqf = 1) while (ii) in 
the case of a maximally heterogeneous population - i.e., when q, = l!k for all q, - the value 
of Iqf is minimum (Iqf = l!k). 
Apparently Camap stated his interpretation of Iqf without being aware that Iqf had already 
been proposed as a plausible uniformity index by several statisticians and scientists ( cf. 
Chapter 10). 
17 A number of modifications to Camap's terminology are made to simplify the analysis 
of such relationships. 
18 Camap 's decision to use the square error as a measure of the seriousness of the error was 
in keeping with a long statistical tradition which, in tum, was motivated to a large extent by 
the mathematical tractability of the square error. However, Camap did not provide any 
theoretical reason for his proposal. For a possible 'justification' of the square error, or 
quadratic distance from the truth, see Chapter 4, note 6. 
19 Carnap and Stegmiiller introduced GC-systems only in (1959). 
20 Concerning these differences between my approach. and that of Camap it should be 
recalled that in 1952 the equivalence between symmetrical Dirichlet distributions and 
C-systems was still to be recognized and the theory of verisimilitude was still to be 
developed. 
CHAPTER 9 
1 Here Camap would appear to imply that A.-based inductive estimates of the success of 
inductive methods cannot be used to select the optimum inductive method. Camap may have 
supposed, rightly, that any inductive method would immodestly evaluate itself as optimum. 
2 This mistake was identified by Spielman (1972, p. 376) and immediately accepted by 
Lewis (1974, p. 84). 
3 This is somewhat problematical: if the prior depends on the data, then the posterior is no 
longer proportional to the product of the prior and the likelihood: therefore Bayes's theorem 
no longer holds. This observation was made by Prof. Molenaar. 
4 This particular aspect of my approach to EPO appears to meet a desideratum expressed 
by Pietarinen (1974). Indeed, concerning Lewis's hyperempiricist view, Pietarinen (1974, p. 
197) remarks: "When the relative frequency of a kind [Q;] in a sample is [n;/n], the 
estimation formula [suggested by Lewis] recommends a certain method [A.]. But whether or 
not the method is rational depends on the situation, in particular, on the question of [the 
degree of uniformity of the universe]. This cannot be decided on the basis of the frequencies 
in the sample alone; it is tempting to say that somehow it must be presupposed .... In Lewis's 

NOTES 
171 
procedure ... the only relevant evidential information consists of the size and the degree of 
uniformity of the sample. But ... to know these things is not enough to estimate the degree 
of uniformity of the universe. Ordinary inductive situations ... may differ in respect of [the 
degree of uniformity], and this is relevant for the choice of the value of I.." According to 
Pietarinen (1974, p. 198) both Camap's aprioristic view and Lewis's hyperempiricist view 
"are defective" since they are unable to take account of "the degree of uniformity of the 
things under study". On the contrary, my contextual approach to EPO shows a possible way 
of considering the estimated degree of (dis )order of the universe under examination. 
This procedure had already been described by Kuipers (1984) who analyzed the 
limit-process in the special case where "-o = 0. 
6 Remember that the 'extreme' I.-values are I. = 0 and I. = oo (see Chapter 6.4). Kuipers 
(ibid., pp. 40-41) also shows that, for some possible e. and "-a, the result of the limit-process 
is an extreme I.-value. Kuipers leaves unanswered the question whether or not this is a reason 
not to use certain values of 1.0 as possible starting values of the process. 
7 There is a strong resemblance between my immodesty-based interpretation of Kuipers's 
limit-process and Lewis's description (1971, p. 62) of the behaviour of a person using a 
modest inductive method (cf. Chapter 4.2). 
8 Remember that, for any I.-# oo,p~.(Q,QJe.) > [p~.(Qi/e.)f: cf. Chapter 3, note 16. It would 
be interesting to ascertain whether some limit-process of the kind described by Kuipers could 
also be generated by the 1..0-based Bayesian estirnatesEw(C(q) I e.). However, even if the non-
Bayesian estimate in (1) were replaced by EM(C(q) I e.), the whole procedure (1)-(2) would 
remain non-Bayesian. In fact, the usual Bayesian procedures allow calculation of the expected 
error- w.r.t. an inductive method 1.0 - of all inductive methods (including A0) relative to any 
evidence e •. Hence, a Bayesian who 'starts' with method "-o and evidence e. would select that 
method A1 which minimizes the expected error - relative to A0 and e. - as the most 
appropriate inductive method. But theorem (4.11) implies that any inductive method "-o 
immodestly indicates itself as being maximally appropriate. This means, in Kuipers's 
terminology, that the 'estimate I./ suggested by the starting method "-o is ... f.o! 
9 Kuipers's second solution to EPO is formally similar to my Vcsolution (see Chapter 8.5). 
However, there is a substantial difference between the two solutions. Whilst est(C(q) I e.) is 
a 'formal' e.-based estimate, my external estimate Est[C(q)] is an 'informal' estimate based 
on the background information BK. 
10 A hyperempiricist view also appears to underlie Hintikka's assumption (1987, p. 305) 
that a researcher "might initially choose a large Camapian index of caution A. But if the 
overwhelming majority of observed individuals belongs to a small number of cells, [he] might 
very well be led to acknowledge that his caution was excessive and that he or she ought to 
have opted for a smaller A in the ftrst place." 
Several proposals in line with the hyperempiricist view are advanced by Good (1965, pp. 
28 and 35-38). They involve estimating the parameter a* of a symmetrical Dirichlet 
distribution Dir(a*, ... ,a*) on the basis of the empirical sample e •. For instance, if I 

172 
NOTES 
understand Good (ib., p. 35) correctly, the value a* might be selected so as to maximize the 
prior predictive probability p(e.) derived using formula (3.48) from Dir(a*, ... ,a*). 
11 Regarding the nature of the new probability system, Kuipers (ibid.) points out that "it is 
equally easy to see that the resulting system, for k :2: 3, cannot have the property that 
p(QJe.) is only a function of n and n,, being a structural property of all A.-systems". It is not 
clear to me whether such a system is exchangeable although I guess it is not Note that in 
Kuipers's new system the special values p(QJe.) depend, besides on n and n,, only on the 
degree of order 'I:.(n.fn)2 of e •. 
12 Rosenkrantz (1982, p. 93), referring to the family Q = (Q,non-Q), points out that even 
the extreme inductive methods A. = 0 and A. = oo can be adopted when certain 
'presuppositions' about the degree of homogeneity of the considered population are made. 
Indeed he points out that "(A. = 0) does seem appropriate for somebody who knows that the 
population is completely homogeneous, and ... [A.= oo] seems apt for one who knows that half 
the population is Q and half non-Q.". (Cf. the proposal made in Chapter 8, note 10.) 
13 For this purpose one could use some technical results obtained by Walk (1963, pp. 523-
528 and 1966, pp. 76-78) who showed that "the optimal value of A. is a monotonic function 
of the entropy in one's universe" (Hintikka, 1987, p. 303). 
14 This view appears to be shared by Horwich (1982, p. 81) who claims that "a reasonable 
inductive practice must be demonstrably reliable: there must be reason to think that it is good 
at directing us towards the truth .... So it can be reasonable to adopt one practice rather than 
another only if it can be shown that the one is more likely to be successful". 
15 
Note that according to Friedman (ibid., p. 369), ''the kind of probability at issue is 
objective or physical probability. We are requiring that there be a lawlike statistical 
correlation between the property of being reached by [a given inductive method) and the 
property of being true, in the very same sense in which there is a lawlike statistical 
correlation ... between smoking and lung cancer." A strong argument against Friedman's 
formulation of the verisimilitude approach to scientific method is given by Niiniluoto (1980, 
pp. 453-454). 
16 Here "representative" is not used in the standard statistical sense regarding the sampling 
procedure used for drawing the sample from the population. Rather the sample is 
"representative of the entire universe" if its composition is similar to that of the universe. 
17 Rosenkrantz (1982, p. 93) points out that Camap's preference for small values of A. can 
be justified only "by confining it to contexts where prior information about the population is 
lacking. It then becomes a special case of the principle favoring adoption of a diffuse prior 
in the absence of prior information." 
18 Rosenkrantz (1982, p. 93), indeed, points out that ''the problem of induction ... is to 
justify that preference." 

NOTES 
173 
19 Regarding the cases where the available background knowledge BK is very poor, see 
Chapter 8, note 4. 
CHAPTER 10 
1 Already, between the end of the 1880's and the beginning of the 1900's, a number of 
scientists and statisticians - such as Quetelet, Galton, Karl Pearson, and Yule - were 
concerned with the problem of qualitative variability. 
2 David (1968, p. 573; 1981, p. 436) provides some details on the history of mean difference 
and related indices. For instance, he points out that mean difference was already being used 
as a measure of quantitative variability by German astronomers even as early as 1869. 
3 For instance, Good (1982, p. 562)- although being aware that "both the quadratic and the 
logarithmic index of heterogeneity [i.e., Gini diversity 1 - Iqi and Shannon entropy H(q) = 
Iq)ogq;] have histories dating back a hundred years" - appears to ignore the importance of 
Gini's work and mentions only that "recently Bhargava and Uppuluri (1975) called [1- II,] 
the Gini index and cited Gini (1912)". 
4 Good (1982), having provided much information on the history of the homogeneity index 
Iqi, maintains that "it is unjust to associate [II,] with any one person" since "if [qt> ... ,qk) are 
the probabilities of [ k) mutually exclusive and exhaustive events, any statistician of this 
century who wanted a measure of homogeneity would have taken about two seconds to 
suggest [II,]" (ibid., p. 562). However, some interesting historical circumstances told by 
Good himself appear to be in conflict with his idea that the discovery of Iq7 as a 
homogeneity index is totally trivial. Indeed, he states that the famous mathematician A. M. 
Turing - when, as head of the section of the Government Code and Cipher School in 
Bletchley, Buckinghamshire was working on the cryptanalysis of the German cryptographic 
machine Enigma- being "probably unaware of the previous uses [of II,], gave [Iqr) its most 
natural name, 'the repeat rate' ... and made very important use of [II,] in cryptanalysis" 
(ibid.). Moreover, Good (ibid.) informs us that the biologist E. H. Simpson also worked at 
Bletchley: for this reason, when "he later suggested the use of 1 minus the repeat rate as an 
index of diversity he did not acknowledge Turing, fearing that to acknowledge him would be 
regarded as a breach of security". 
The term "dichotomous" is used "because the rarity of the species [Q;] would be 
unchanged if the other species were grouped into a single complementary category" (Patil and 
Taillie, 1982, p. 548). 
6 Cf. Patil and Taillie (1982, p. 551). 
7 For instance, Hill (1973, p. 428) remarks that II, represents a "weighted mean of the 
proportional abundances" qi of the different species Qi in a given population. Note that 
Weaver (1948), dealing with a process characterized by k possible events, already took II, 
as a measure of the "expected commonness" of an event (see Pielou, 1975, p. 95). 

174 
NOTES 
8 The family N.(q) is strictly connected to the so-called generalized entropy of order a, or 
H.(q)- defined as H.(q) =(In ~if,)/(1- a)- which was introduced by Renyi (1961) in the 
theory of information. Indeed, it can be proved that H.(q)= In(N.(q)) (cf. Hill, ibid.). It has 
been suggested that H.(q) can be used as a diversity measure (Pielou, 1975, p. 8). 
9 Although this assumption appears to be universally valid, Bhargava and Doyle (1974, p. 
243) suggest that in certain cases the "even" population vector may be different from 1/k. 
10 This follows from the equalities D 2(1/k,q) = ~(q, - llk)2 = ~q/ - llk = C(q) - 1/k. 
11 
Cf. Rao (1982, p. 275). Note that Div(q) cannot be directly defined as the distance 
between q and the maximally homogeneous vector since there are k maximally homogeneous 
vectors: (1,0, ... ,0), (0,1,0, ... ,0), ... ,(0, ... ,0,1). 
12 Note that "this interpretation strictly applies if the population size is infinite or if the 
sampling is done with replacement" (Agresti and Agresti, 1977, p. 206). More generally, this 
interpretation of G(q) applies to any multivariate Bernoulli process. Probabilistic explications 
ofGini diversity have been suggested by Greenberg (1956) and Lieberson (1969, p. 851) and 
others. 
13 To describe such encounters some theoretical model including appropriate idealizations 
should be used. For instance, when dealing with animal communities, 'disturbing factors' 
such as "spatial patchiness and clumping in species distribution, differential mobility" and so 
on should be ignored (Sugihara, 1982, p. 565). 
14 
Patil and Taillie (1979, pp. 7-11; 1982 pp. 549-550) develop a somewhat general 
encounter-theoretic explication of diversity and show that most 'traditional' diversity indices 
have plausible encounter-theoretic interpretations. 
15 For example, when the population is an animal community then its natural habitat might 
be one such entity. 
16 Similar remarks apply to any other quantity - such as the mean, variance, covariance -
considered in statistics and, more generally, also apply to any scientific concept. 
17 
Sugihara (1982, p. 565), referring to the diversity measures used in the biology of 
populations, remarks that "truly ground-breaking contributions to the theory of species 
diversity are not likely to arise in vitro from a mathematical analysis of indices but will most 
probably depend on an interplay of analysis with real data." 
18 This occurs, for instance, in a human community whose members are classified relative 
to a qualitative variable such as religion, ethnic origin, political party, etc. 
19 Further references on the sociological and economical application of diversity measures 
are given by Sen (1973) and Agresti and Agresti (1977). 
20 Patil and Taillie (ibid.) provide several references on these arguments. 

NOTES 
175 
21 
For instance, Patil and Taillie (1982, p. 565) report that the Committee to Review 
Methods for Ecotoxicology of the Environmental Sciences Board of the U.S. National 
Academy of Sciences (1981) concluded that "diversity is a system property that is likely to 
be a sensitive measure of ecosystem contamination." 
22 See, for instance, Taillie (1979, Section IV). 
CHAPTER 11 
1 This reflects a limitation common to most studies on scientific enterprise. With reference 
to this, Laudan (1983, p. 2) points out that "students of the development of science, whether 
sociologists or philosophers, have alternatively been preoccupied with explaining consensus 
in science or with highlighting disagreement and divergence. Those contrasting focuses would 
be harmless if all they represented were differences of emphasis or interest .... What creates 
tension is that neither approach has shown itself to have the explanatory resources for dealing 
with both." 
2 
Niiniluoto (1987, p. 473) points out that "measures of verisimilitude always have a 
pragmatic dimension, by being dependent on our cognitive interests". More generally, he 
remarks that "science is a fallible and progressive enterprise which is run by historically 
developing scientific communities. Our tools for analyzing science should be flexible enough 
to take into account this richness of scientific practice." Other interesting remarks on the 
context-dependence of verisimilitude measures are made by Niiniluoto (ibid., Chapter 13.4). 
3 For instance, one might look for a contextual justification of certain mixtures of Dirichlet 
distributions which have been suggested as appropriate priors for some kinds of multinomial 
contexts: see Good (1965, 1983), Dalal and Hall (1983), and Skyrms (1993?). In particular, 
Skyrms (1993?) shows that certain Dirichlet mixtures can be used to build exchangeable 
inductive methods capable of dealing with the so-called analogy by similarity ( cf. Chapter 
6.1 ). Moreover, one might look for a contextual justification of certain exchangeable inductive 
methods which attribute a positive probability to generalizations: see Hintikka (1966), 
Hintikka and Niiniluoto (1976), Kuipers (1978, Chapter 6) and Jamison (1970, pp. 50-53). 
In particular, Jamison (ibid.) specifies the prior distribution (on a parameter vector q) 
equivalent to Hintikka's inductive methods. 
4 Several Bayesian statistical analyses of Markov processes have been proposed, e.g., Martin 
(1967). The problems concerning the inductive inferences relative to Markov processes and 
other 'non-Bernoulli' multicategorical processes have also been considered within the 
conceptual framework of TIP: see Achinstein (1963), Carnap (1963), Diaconis and Freedman 
(1980), Kuipers (1988), Skyrms (1991). 
5 More generally, quantitative processes include all those experimental processes where the 
result of a trial is the value of a given quantity. 
6 For instance, a CC-solution to EPO might be based on an external estimate of the entropy 
of the multivariate Bernoulli process under consideration (cf. Chapter 9, note 13). 

176 
NOTES 
7 Note that various measures of quantitative variability have been defined in statistics and 
in some empirical sciences (see, for instance, formula (10.1)). 
8 Moreover, an interesting problem concerns the analysis of LPO in those cases where the 
inductive conclusion of a Bayesian inference is equated with the posterior distribution on the 
parameter (vector). Of course a necessary presupposition for such analysis is the defmition 
of an appropriate measure of the <listance between any possible posterior distribution and any 
possible value of the parameter (vector). 
9 In Chapter 8.6 the equivalence between the CC-solution and the V-solution to EPO was 
proved and explained. The existence of similar relationships between alternative V -solutions 
to EPO and alternative CC-solutions remains an open question. 

REFERENCES 
Achinstein, P., 'Confirmation Theory, Order and Periodicity', Philosophy of Science 30 
(1963), pp. 17-35. 
Agresti, A. and Agresti, B. F., 'Statistical Analysis of Qualitative Variation', in K. F. 
Schuessler (ed.), Sociological Methodology 1978, 1977, pp. 204-237. 
Aitchison, J. and Dunsmore, I. R., Statistical Prediction Analysis, Cambridge University 
Press, Cambridge, 1975. 
Amemiya, E. C., 'Measurement of Economic Differentiation', Journal of Regional Science 
5 (1963), pp. 85-87. 
Bar-Hillel, Y., 'The Acceptance Syndrome', in Lakatos (1%8), pp. 150-161. 
Barnett, V., Comparative Statistical Inference, Wiley, New York, 1973. 
Bernardo, J. M., 'Reference Posterior Distributions for Bayesian Inference' (with Discussion), 
JRSS Ser. B 41 (1979), pp. 113-147. 
Bhargava, T. N. and Doyle, P. H., 'A Geometric Study of Diversity', Journal of Theoretical 
Biology 43 (1974), pp. 241-251. 
Bhargava, T. N. and Uppuluri, V. R. R., 'On an Axiomatic Derivation of Gini Diversity, with 
Applications', Metron 33 (1975), pp. 41-53. 
Bogdan, R. J. (ed.), Local Induction. D. Reidel, Dordrecht, 1976. 
Box, G. and Tiao, G;, Bayesian Inference in Statistical Analysis, Addison-Wesley, Reading, 
Mass., 1973. 
Brink, C., 'Verisimilitude: Views and Reviews', History and Philosophy of Logic, 10 (1989), 
pp. 181-201. 
Burks, A. W., 'On the Significance of Camap's System of Inductive Logic for the Philosophy 
of Induction, in P. A. Schilpp (1963), pp. 739-759. 
Camap, R., The Logical Foundations of Probability, The University of Chicago Press, 
Chicago, 1950. (2nd enlarged ed. 1962.) 
Camap, R., The Continuum of Inductive Metlwds, The University of Chicago Press, Chicago, 
1952. 
Carnap, R., 'Variety, Analogy and Periodicity in Inductive Logic', Philosophy of Science 30 
(1963), pp. 222-227. 
Carnap, R., 'On Rules of Acceptance', in Lakatos (1968), pp. 146-150, (1968a). 
Carnap, R., 'Inductive Logic and Inductive Intuition', in Lakatos (1968), pp. 258-267, 
(1968b). 
Camap, R., 'A Basic System of Inductive Logic, Part I', in Carnap and Jeffrey (1971), pp. 
33-165. 
Carnap, R., 'A Basic System of Inductive Logic, Part II', in Jeffrey (1980), pp. 7-155. 
Camap, R. and Jeffrey, R. C. (eds.), Studies in Inductive Logic and Probability, vol. I, 
University of California Press, Berkeley, 1971. 
Carnap, R. and Stegmtiller, W., Induktive Logic und Wahrscheinlichkeit, Springer-Verlag, 
Wien, 1959. 
Connor, R. J. and Mosimann, J. E., 'Concepts of Independence for Proportions with a 
Generalization of the Dirichlet Distribution', JASA 64 (1969), pp. 194-206. 
Costantini, D., 'Considerazioni suite Ipotesi Profonde della Distribuzione Beta', Statistic a 38 
(1978), pp. 313-322. 
177 

178 
REFERENCES 
Costantini, D., 'The Relevance Quotient', Erkenntnis 14 (1979), pp. 149-157. 
Costantini, D., 'The Role of Inductive Logic in Statistical Inference' (with Discussion), 
Epistemologia, 7 (1984), pp. 153-170. 
Dalal, S. R. and Hall, W. J., 'Approximating Priors by Mixtures of Natural Conjugate Priors', 
JRSS Ser. B 45 (1983), pp. 278-286. 
Darroch J. N. and Ratcliff, D., 'A Characterization of the Dirichlet distribution', JASA 66 
(1971), pp. 641-643. 
David, H. A., 'Gini's Mean Difference Rediscovered', Biometrika SS (1968), pp. 573-575. 
David, H. A., 'Gini's Mean Difference', inS. Kotz and N. L. Johnson (eds.), Encyclopaedia 
of Statistical Sciences, Vol. 3, Wiley, New York, 1983, pp. 436-437. 
de Finetti, B., 'La prevision: ses lois logiques, ses sources subjectives', Ann. Inst. Henri 
Poincare 7 (1937), pp. 1-68. [English translation in Kyburg H. E. Jr. and Smokier, H. 
(eds.), Studies in Subjective Probability, Wiley, New York, 1964, pp. 93-158.] 
Dennis, B., Patil, G. P., Rossi, 0., Stehman S. and Taillie, C., 'A Bibliography of Literature 
on Ecological Diversity and Related Methodology', in Grassle, Patil, Smith, and Taillie 
(1979), pp. 319-353. 
Descartes, R., Rules for the Direction of the Mind (ca. 1628), in Philosophical Works, 
vol. I (translated by E. S. Haldane and G. R. T. Ross, 1911). 
Diaconis, P. and Freedman, D., 'De Finetti's Theorem for Markov Chains', Annals of 
Probability 8 (1980), pp. 115-130. 
Doksum, K., 'Tailfree and Neutral Random Probabilities and Their Posterior Distributions', 
The Annals of Probability 2 (1974), pp. 183-201. 
Domotor, Z., 'Probability Kinematics, Conditionals, and Entropy Principles', Synthese 63 
(1985), pp. 75-114. 
Earman, J., Bayes or Bust? A Critical Examination of Bayesian Confirmation Theory, The 
MIT Press, Cambridge, Mass., 1992 
Fabius, J., 'Two Characterizations of the Dirichlet distributions', Annals of Mathematical 
Statistics 1 (1973), pp. 583-587. 
Feller, W., An Introduction to Probability Theory and Its Applications, Vol. I, 2nd. ed., Wiley, 
New York, 19682• 
Ferguson, T. S., Mathematical Statistics: A Decision-Theoretic Approach, Academic Press, 
New York, 1967. 
Festa, R., manuscript, 1982. 
Festa, R., 'Inferenza predittiva e misure di eterogeneita', inStatistica 44 (1984), pp. 111-129. 
Festa, R., 'Epistemic Utilities, Verisimilitude, and Inductive Acceptance of Interval 
Hypotheses', in Abstracts of the 7th International Congress of Logic, Methodology and 
Philosophy of Science, Vol. I, Saltzburg, 1983, pp. 212-215. 
Festa, R., 'A Measure for the Distance Between an Interval Hypothesis and the Truth', 
Synthese 67 (1986), pp. 273-320. 
Festa, R., 'Theory of Similarity, Similarity of Theories, and Verisimilitude', in T. K. Kuipers 
(1987a), 1987, pp. 145-176, (1987a). 
Festa, R., 'New Aspects of Camap's Optimum Inductive Method', in Abstracts of the 8th 
International Congress of Logic, Methodology and Philosophy of Science, Vol I, Moscow, 
1987, pp. 397-400, (1987b). 
Festa, R. and Buttasi, C., 'Generalized Camapian Systems, Dirichlet Distributions, and the 
Epistemological Problem of Optimality', in Abstracts of the 8th International Congress 
of Logic, Methodology and Philosophy of Science, Vol I, Moscow, 1987, pp. 390-393. 

REFERENCES 
179 
Fine, T., Theories of Probability, Academic Press, New York, 1973. 
Friedman, M., 'Truth and Confirmation', The Journal of Philosophy 76 (1979), pp. 361-382. 
Gaifman, H., 'Applications of de Finetti's Theorem to Inductive Logic', in Camap and Jeffrey 
(1971), pp. 235-251. 
Gaifman, H., 'Towards a Unified Concept of Probability', in B. Marcus et al. (eds.), Logic, 
Metlwdology and Philosophy of Science, North-Holland, Amsterdam, 1986, pp. 319-350. 
Gatlin, L., Information Theory and the Living System, Columbia University Press, New York, 
1972. 
Giere. R. N., Review of Camap and Jeffrey (1971), Synthese 31 (1975), pp. 187-199. 
Gini, C., 'Considerazioni sulle probabilita a posteriori e applicazioni al rapporto dei sessi nelle 
nascite umane', Studi Economico-Giuridici della Facoltil 
di Giurisprudenza 
dell'Universitil di Cagliari 3 (1911). [Reprinted in Metron 1S (1949), pp. 1-37.] 
Gini, C., 'Variabilita e Mutabilita', Studi Economico-Giuridici della Facoltil di 
Giurisprudenza dell'Universitil di Cagliari 3 (1912), pp. 3-159. (Reprinted, with few 
variations, in C. Gini, Memorie di metodologia statistica, vol. I, Veschi, Roma, 1955, pp. 
211-282.] 
Good, I. G., Probability and' the Weighing of Evidence, Charles Griffm and Co., London, 
1950. 
Good, I. J, 'Maximum Entropy for Hypotheses Formulation, especially for Multi-dimensional 
Contingency Tables', Annals of Mathematical Statistics 34 (1963), pp. 911-934. 
Good, I. J., The Estimation of Probabilities, The MIT Press, Cambridge, Mass., 1965. 
Good, I. J., 'Comment' on Patil and Taillie (1982), pp. 551-563. 
Good, I. J., Good Thinking, The University of Minnesota Press, Minneapolis, 1983, (1983a). 
Good, I. J., 'The Robustness of a Hierarchical Model for Multinomials and Contingency 
Tables', in G. E. P. Box et al. (eds.), Scientific Inference, Data Analysis, and 
Robustness, 1983, pp. 191-211, (1983b). 
Grassle, J. F., Patil, G. P., Smith, W., and Taillie, C. (eds.), Ecological Diversity in Theory 
and Practice, International Co-operative Publishing House, Burtonsville, 1979. 
Graves, J., 'Uniformity and Induction', British Journal for Philosophy of Science 24 (1974), 
pp. 301-318. 
Greenberg, J. H., 'The Measurement of Linguistic Diversity', Language 32 (1956), pp. 109-
115. 
Guiasu, R., Information Theory with Applications, McGraw-Hill, New York, 1977. 
Guiraud, P., Problemes et Methodes de Ia Statistique Linguistique, D. Reidel, Dordrecht, 
1959. 
Hacking, 1., The Emergence of Probability, Cambridge University Press, Cambridge, 1975. 
Hardy, G. F., correspondence in Insurance Records 27 (1889). [Reprinted in Trans. Fac. 
Actuar. 8 (1929), pp. 174-182.] 
Harsanyi, J. C., 'Acceptance of Empirical Statements: a Bayesian Theory Without Cognitive 
Utilities', Theory and Decision 18 (1985) pp. 1-30. 
Hartigan, J., 'Invariant Prior Distributions', Annals of Mathematical Statistics 35 (1964), pp. 
836-845. 
Hempel, C. G., 'Deductive-nomological versus Statistical Explanations', in H. Feigl and G. 
Maxwell (eds.), Scientific Explanation, Space and Time, Minnesota Studies in the 
Philosophy of Science 3, University of Minnesota Press, Minneapolis, 1962, pp. 98-169. 
Herdan, G., Quantitative Linguistics, Butterworth, Whashington, D.C, 1964. 

180 
REFERENCES 
Herdan, G., The Advanced Theory of Language as Choice and Chance, Springer-Verlag, New 
York, 1966. 
Hill, M. 0., 'Diversity and Evenness: a Unifying Notation and its Consequences', Ecology 
54 (1973), pp. 427-432. 
Hilpinen, R., Rules of Acceptance and Inductive Logic, North-Holland, Amsterdam, 1968. 
Hilpinen, R., 'Camap's New System of Inductive Logic', Synthese 25 (1973), pp. 307-333. 
Hintikka, J., 'A Two-dimensional Continuum oflnductive Methods', in Hintikka and Suppes 
(1966), pp. 113-132. 
Hintikka, J., 'Unknown probability, Bayesianism, and de Finetti's representation theorem', 
in R. C. Buck and R. S. Cohen (eds.), In Memory of Rudolf Carnap, D. Reidel, 
Dordrecht, pp. 325-341. 
Hintikka, J., 'Replies and Comments', in R J. Bogdan (ed.), Jalcko Hintiklal, D. Reidel, 
Dordrecht, 1987, pp. 277-344. 
Hintikka, J. and Niiniluoto, I., 'An Axiomatic Foundation of Inductive Generalization', in 
Przelecki, M. et al. (eds.), Formal Methods in the Methodology of Empirical Sciences, 
1976, pp. 57-81. 
Hintikka, J. and Pietarinen, J., 'Semantic Information and Inductive Logic', in Hintikka and 
Suppes (1966), pp. 96-112. 
Hintikka, J. and Suppes, P., (eds.), Aspects of Inductive Logic, D. Reidel, Dordrecht, 1966. 
Hintikka, J. and Suppes P., (eds.), Information and Inference, D. Reidel, Dordrecht, 1970. 
Hobbes, T., Humane Nature, 1650. 
Hogarth, R. M., Judgment and Choice: the Psychology of Decision, Chichester, Wiley, 1980. 
Horwich, P., Probability and Evidence, Cambridge University Press, Cambridge, 1982. 
Howson, C. and Urbach, P., ScientifiC Reasoning. The Bayesian Approach, Open Court, La 
Salle, Illinois, 1989. 
Hume, D., A Treatise of Humane Nature, 1739. 
Hurlbert, S. H., 'The Nonconcept of Species Diversity: a Critique and Alternative 
Parameters', Ecology 52 (1971), pp. 577-586. 
Huzurbazar, V. S. Sufficient Statistics, Marcel Dekker, New York, 1976. 
Huzurbazar, V. S., 'Bayesian Inference and Invariant Prior Probabilities', in Zellner (1980), 
pp. 445-449. 
James, I. R and Mosimann, J. E., 'A New Characterization of the Dirichlet Distribution 
through Neutrality', The Annals of Statistics 8 (1980), pp. 183-189. 
Jamison, D., 'Bayesian Information Usage', in Hintikka and Suppes (1970), pp. 28-57. 
Jaynes, E. T., 'Prior Probabilities', IEEE Transactions on Systems Science and Cybernetics 
SSC-4, pp. 227-241. 
Jaynes, E. T., 'Where Do we Stand with Maximum Entropy?', in R. D. Levine and M. Tribus 
(eds.), The Maximum Entropy Formalism, The MIT Press, Cambridge, Mass., 1979, pp. 
15-118. 
Jaynes, E. T., Papers on Probability, Statistics, and Statistical Physics, (ed. by R. 
Rosenkrantz), D. Reidel, Dordrecht, 1983. 
Jeffrey, R, 'Valuation and Acceptance of Scientific Hypotheses', Philosophy of Science 23 
(1956), pp. 237-246. 
Jeffrey, R., 'Probable Knowledge', in Lakatos (1968), pp. 166-180. 
Jeffrey, R, 'Dracula Meets Woolfman: Acceptance vs. Partial', in M. Swain ( ed. ), Induction, 
Acceptance, and Rational Belief, D. Reidel, Dordrecht, 1970, pp. 157-185. 
Jeffrey, R., 'Probability Measures and Integrals', in Camap and Jeffrey (1971), pp. 167-223. 

REFERENCES 
181 
Jeffrey, R, 'Carnap's Inductive Logic', Synthese 25 (1973), pp. 299-306. 
Jeffrey, R (ed.), Studies in Inductive Logic and Probability, vol. II, University of California 
Press, Berkeley, 1980. 
Jeffrey, R, 'De Finetti's Probabilism', Synthese 60 (1984), pp. 73-90. 
Jeffreys, H., Theory of Probability, 2nd ed., Clarendon Press, Oxford, 1948 (3nd ed. 1961). 
Johnson, W. E., 'Probability: Deductive and Inductive Problems (Appendix)', Mind 49 (1932), 
pp. 409-423. 
Kaplan, M., 'A Bayesian Theory of Rational Acceptance', The Journal of Philosophy 78 
(1981), pp. 305-330. 
Kemeny, J., 'Carnap's Theory of Probability and Induction', in Schilpp (1963), pp. 711-738. 
Kuipers, T. A. F., Studies in Inductive Probability and Rational Expectation, D. Reidel, 
Dordrecht, 1978. 
Kuipers, T. A. F., 'A Survey of Inductive Systems', in Jeffrey (1980), pp. 183-192. 
Kuipers, T. A. F., 'Approaching Descriptive and Theoretical Truth', Erkenntnis 18 (1982), 
pp. 343-378. 
Kuipers, T. A. F., 'An Approximation of Carnap's Optimum Estimation Method', Synthese 
61 (1984), pp. 361-362. 
Kuipers, T. A. F., 'Some Estimates of the Optimum Inductive Method', Synthese 24 (1986), 
pp. 37-46. 
Kuipers, T. A. F. (ed.), Wlult is Closer-to-the-truth?, Rodopi, Amsterdam, 1987, (1987a). 
Kuipers, T. A. F., 'A Structuralist Approach to Truthlikeness', in Kuipers (1987a ), pp. 79-99, 
(1987b). 
Kuipers, T. A. F., 'Inductive Analogy by Similarity and Proximity', in D. H. Helman (ed.), 
Analogical Reasoning, Dordrecht, Kluwer, 1988. 
Kuipers, T. A. F., 'Naive and Refined Truth Approximation', Synthese 93 (1992), pp. 299-
342. 
Kullback, S., Information Theory and Statistics, Wiley, New York, 1959. 
Kullback, S. and Leibler, R A., 'On Information and Sufficiency', Annals of Mathematical 
Statistics 22 (1951 ), pp. 79-86. 
Kyburg, H. E. Jr., The Logical Foundations of Statistical Inference, D. Reidel, Dordrecht, 
1974. 
Lakatos, 1., The Problem of Inductive Logic, North-Holland Publishing Company, Amsterdam, 
1968. 
Laplace, P. S., 'Memoire sur Ia probabilite des causes parIes evenements', Memoires de 
l'Academie Royale des sciences presentes par divers savans, 6 (1774), pp. 621-656. 
[Reprinted in Oeuvres complete de Laplace, vol. 7, Gauthiers-Paris. Translated in Stigler, 
S.M., 'Laplace's Memoir on Inverse Probability', Statistical Science, 1 (1986), pp. 359-
378.] 
Laudan, L., 'Peirce and the Trivialization of Self-Correcting Thesis', in R.N. Giere and 
R. S. Westfall (eds.), Foundations of Scientifrc Method: the Nineteenth Century, 1973, 
Indiana University Press, pp. 275-306. 
Laudan, L., Science and Values, University of California Press, 1984. 
La Valle, I.H., An Introduction to Probability, Decision and Inference, Holt, Rinehart and 
Winston, New York, 1970. 
Levi, 1., Gambling with Truth, Alfred A. Knopf, New York, 1967. 
Levi, 1., 'Acceptance Revisited', in R Bogdan (ed.), Local Induction, D. Reidel, Dordrecht, 
1976, pp. 1-71. 

182 
REFERENCES 
Levi, 1., The Enterprise of Knowledge: An Essay on Knowledge, Credal Probability, and 
Chance, The MIT Press, Cambridge, Mass. and London, 1980. 
Lewis, D., 'Immodest Inductive Methods', Philosophy of Science 38 (1971 ), pp. 54-63. 
Lewis, D., 'Spielman and Lewis on Inductive Immodesty', Philosophy of Science 41 (1974), 
pp. 84-85. 
Lewontin, R. C., 'The Apportionment of Human Diversity', Evolutionary Biology 6 (1972), 
pp. 381-398. 
Lidstone, G. J., 'Note on the General Case of the Bayes-Laplace Formula for Inductive or 
Posterior probabilities', Trans. Fac. Actuar. 8 (1920), pp. 182-192. 
Lieberson, S., 'Measuring Population Diversity', American Sociological Review 34 (1969), 
pp. 850-862. 
Lindgren, B. W., Statistical Theory, MacMillan Publishing Co., New York, 19762• 
Lindley, D. V., 'On a Measure of Information Provided by an Experiment', Annals of 
Mathematical Statistics 29 (1956), pp. 986-1005. 
Lindley, D. V., Bayesian Statistics, A Review, S.I.A.M., Philadelphia, 1972. 
Lourens, P. F., The Formalization of Knowledge by Specification of Subjective Probability 
Distributions, Dissertation, University of Groningen, 1984. 
Martin, J. J., Bayesian Decision Problems and Markov Chains, Wiley, New York, 1967. 
May, S., and Harper, W., 'Towards an Optimization Procedure for Applying Minimum 
Change Principles in Probability Kinematics', in W. L. Harper and C. A. Hooker (eds.), 
Foundations of Probability Theory, Statistical Inference, and Statistical Theories of 
Science, vol. I, D. Reidel, Dordrecht, 1976, pp. 137-166. 
Mill, J. S., A System of Logic, London, 1843. 
Miller, D., 'On the Distance from the Truth as a True Distance', in J. Hintikka et al. (eds.), 
Essays on Mathematical and Philosophical Logic, North-Holland, Amsterdam, 1978, pp. 
415-435. 
Miller, D., Review of Rosenkrantz (1977), Philosophical Quarterly 30 (1980), pp. 264-266. 
Miller, D., 'Some Logical Mensuration', The British Journal for the Philosophy of Science 
41 (1990), pp. 281-290. 
Milne, P., 'A Note on Scale Invariance', The British Journal for the Philosophy of Science 
34 (1983), pp. 49-55. 
Mosimann, J. E., 'On the Compound Multinomial Distribution, the Multivariate 
8-distribution, and Correlations among Proportions', Biometrika 49 (1962), pp. 65-82. 
Niiniluoto, 1., 'Scientific Progress', Synthese 45 (1980), pp. 427-462. 
Niiniluoto, 1., 'Inductive Logic as a Methodological Research Programme', Scientia: Logic 
in the 20th Century, Milano, 1983, pp. 77-100. 
Niiniluoto, 1., 'Truthlikeness and Bayesian Estimation', Synthese 67 (1986), pp. 321-346. 
Niiniluoto, 1., Truthlikeness, D. Reidel, Dordrecht, 1987. 
Niiniluoto, 1., 'Corroboration, Verisimilitude, and the Success of Science', inK. Gavroglu, 
Y. Goudaroulis and P. Nicolacopoulos (eds), Imre Lakatos and Theories of Scientific 
Change, 1989, pp. 229-243. 
Novick, M. R. and Hall, W. J., 'A Bayesian Indifference Procedure', JASA 60 (1965), pp. 
1104-117. 
Novick, M. R. and Jackson, P. H., Statistical Methods for Educational and Psychological 
Research, McGraw Hill, New York, 1974. 
Oddie, G., Likeness to the Truth, D. Reidel, Dordrecht, 1986. 

REFERENCES 
183 
Patil, G. P. and Taillie, C., 'An Overview of Diversity', in Grassle, Patil, Smith, and Taillie, 
(1979), pp. 3-27. 
Patil, G. P. and Taillie, C., 'Diversity as a Concept and its Measurement', JASA 77 (1982), 
pp. 548-561. 
Pearson, K., 'Mathematical Contributions to the Theory of Evolution. On a form of spurious 
correlation which may arise when indices are used in the measurement of organs', 
Proceedings of Royal Society, 1897, pp. 489-498. 
Perks, W., 'Some Observations on Inverse Probability Including a New Indifference Rule' 
(with Discussion), Journal of the Institute of Actuaries 73 (1947), pp. 285-334. 
Pielou, E. C., An Introduction to Mathematical Ecology, Wiley, New York, 1969. 
Pielou, E. C., Ecological Diversity, Wiley, New York, 1975. 
Pielou, E. C., Mathematical Ecology, Wiley, New York, 1977. 
Pielou, E. C., 'Diversity Indices', in S. Kotz and N. L. Johnson (eds.), Encyclopaedia of 
Statistical Sciences, Vol. 2, Wiley, New York, 1982, pp. 408-412. 
Pietarinen, J., Lawlikeness, Analogy and Inductive Logic, (Acta Philosophica Fennica 26), 
North-Holland Publishing Co., Amsterdam, 1972. 
Pietarinen, J., 'Inductive Immodesty and Lawlikeness', Philosophy of Science 41 (1974), pp. 
196-198. 
Popper, K. R., Conjectures and Refutations: The Growth of Scientific Knowledge, Routledge 
and Kegan Paul, London, 1963. 
Rao, C. R., 'Gini-Simpson Index of Diversity: A Characterization, Generalization and 
Applications', Utilitas Matematica 218 (1982), pp. 273-282. 
Renyi, A., 'On Measures of Entropy and information', Proceedings of the Fourth Berkeley 
Symposium on Mathematical Statistics and Probability, Vol. I, 1961, pp. 547-561. 
Roberts, H. V., 'Probabilistic Prediction', JASA 60 (1965), pp. 50-62. 
Rosenkrantz, R. D., Inference, Method and Decision, D. Reidel, Dordrecht, 1977. 
Rosenkrantz, R. D., 'Measuring Truthlikeness', Synthese 45 (1980), pp. 463-487. 
Rosenkrantz, R. D., Foundations and Applications of Inductive Probability, Ridgeview 
Publishing Company, Atascadero, California, 1981. 
Rosenkrantz, R. D., 'Does the Philosophy of Induction Rest on a Mistake?', Journal of 
Philosophy 79 (1982), pp. 78-97. 
Savage, L.J., The Foundations of Statistics, Wiley, New York, 1954 (2nd ed., Dover, New 
York, 1972.) 
Schilpp, P. A., The Philosophy of Rudolf Carnap, Open Court, Lassalle, Illinois, 1963. 
Schurz, G. and Weingartner, P., Verisimilitude defined by relevant consequence-elements, in 
Kuipers (1987a), pp. 47-77. 
Seidenfeld, T., 'Why I am not an Objective Bayesian: some Reflections Prompted by 
Rosenkrantz', Theory and Decision 11 (1979), pp. 413-430, (1979a). 
Seidenfeld, T., Philosophical Problems of Statistical Inference. Learning from R. A. Fisher, 
D. Reidel, Dordrecht, 1979, (1979b). 
Seidenfeld, T., 'Entropy and Uncertainty', Philosophy of Science 53 (1986), pp. 467-491. 
Sen, A., On Economic Inequality, Clarendon Press, Oxford, 1973. 
Shannon, C. E., 'A Mathematical Theory of Communication', Bell System Technical Journal 
27 (1948), pp. 379-423, 623-656. [Reprinted in C. E. Shannon and W. Weaver, The 
Mathematical Theory of Communication, University of Illinois Press, Urbana, Ill., 1949.] 
Shimony, A., 'The Status of the Principle of Maximum Entropy', Synthese 63 (1985), pp. 35-
53. 

184 
REFERENCES 
Simpson, E. H., 'Measurement of Diversity', Nature 163 (1949), pp. 688. 
Skilling, J., 'Prior Probabilities', Synthese 63 (1985), pp. 1-34. 
Skyrms, B., Choice and Chance, Wadsworth Publishing Company, Inc. Belmont, Cal., 1966. 
Skyrms, B, 'Carnapian Inductive Logic for Markov Chains', Erkenntnis 35 (1991), pp. 439-
460. 
Skyrms, B., 'Analogy by Similarity in Hypercamapian Inductive Logic', forthcoming, 1993?. 
Spielman, S., 'Lewis on Immodest Inductive Methods', Philosophy of Science 39 (1972), pp. 
375-377. 
Spielman, S., Review of Rosenkrantz (1977), The Journal of Philosophy 78 (1981), pp. 356-
367. 
Sugihara, G., 'Comment' on Patil and Taillie (1982), pp. 564-565. 
Swinburne, R, An Introduction to Confirmation Theory, Metheun & Co. Ltd, London, 1973. 
Terlouw, P., Subjective Probability Distributions: a Psychometric Approach, Dissertation, 
University of Groningen, 1989. 
Tintner, G.T., 'Foundations of Probability and Statistical Inference', (with discussion), JRSS 
Ser. A 112 (1949), pp. 251-286. 
Villegas, C., 'On the Representation of Ignorance', JASA 72 (1977), pp. 650-654. 
Walk, K, 'Kumulative Information', Nachrichtentecnische Zeitschrift 16 (1963), pp. 523-528. 
Walk, K, 'Simplicity, Entropy and Inductive Logic', in Hintikka and Suppes (1966), pp. 66-
80. 
Wallace, A R, Tropical Nature and Other Essays, MacMillan, London, 1875. 
Watkins, J., 'The Popperian Approach to Scientific Knowledge', in G. Radni1zky and G. 
Andersson (eds.), Progress and Rationality in Science, D. Reidel, Dordrecht, 1978, pp. 
23-44. 
Weaver, W., 'Probability, Rarity, Interest, and Surprise', The Scientific Monthly 67 (1948), 
pp. 390-392. 
Williams, P. M., 'Bayesian Conditionalization and the Principle of Minimum Information', 
British Journal for the Philosophy of Slience 31 (1980), pp. 131-144. 
Wilks, S. S., Mathematical Statistics, Wiley, New York, 1962. 
Winkler, R L., Introduction to Bayesian Inference and Decision, Holt, Rinehart and 
Winston, New York, 1972 
Winkler, R L., 'Prior Information, Predictive Distributions, and Bayesian Model Building', 
in Zellner (1980), pp. 95-109. 
Withworth, W. A, DCC Exercises in Choice and Chance, 1897 [Reprinted 1965, Hafner, 
New York]. 
Yule, G. U. Statistical Study of Literary Vocabulary, Cambridge University Press, London, 
1944. 
Zabell, S. L., 'W.E. Johnson's "Sufficientness" Postulate',Annals of Statistics 10 (1982), pp. 
1091-1099. 
Zabell, S. L., 'The Rule of Succession', Erkenntnis 31 (1989), pp. 283-321. 
Zellner, A, An Introduction to Bayesian Inference in Econometrics, Wiley, New York, 1971. 
Zellner, A ( ed. ), Bayesian Analysis in Econometrics and Statistics, North-Holland Publishing 
Company, Amsterdam, 1980. 

INDEX OF NAMES 
Achinstein, P. 175, 177 
Agresti, A. 146, 174, 177 
Agresti, B. F.146, 174,177 
Aitchison, J. 157-8, 177 
Arnemiya, E. C. 148, 177 
Andersson, G. 184 
Antiseri, D. viii 
Aristotle, 155 
Artosi, A., viii 
Bacon, F. 155 
Bar-Hillel, Y. 157, 177 
Barnett, V. 81-2, 154, 163, 177 
Bayes, T. 79 
Bernardo, J. M. 164, 177 
Bernoulli, J. 3 
Bhargava, T. N. 143, 146, 173-4, 177 
Bogdan, R. J. 151, 177, 180-1 
Bonra, M. ix 
Box, G. E. P. 82, 157, 164, 177, 179 
Boyle, R. 155 
Brink, C. 160, 177 
Buck, R. C. 180 
Burks, A. W. 132, 177 
Buttasi, C. vii, 174 
Carnap, R. vii, xiii, 5, 8, 14, 33, 41, 57-9, 
66-9, 88-94, 103, 121-8, 133-4, 138, 
141, 144, 154-7, 161-62, 165, 170-2, 175, 
177-81 
Carneades, 3 
Cohen, R. S. 180 
Connor, R. J. 70, 163, 177 
Cooke, R. viii 
Costantini, D. viii, 69, 162-3, 177-8 
Dalal, S. R. 175, 177 
Darroch, J. N. 163, 178 
David. H. A. 173, 178 
185 
de Finetti, B. xii, 7, 51, 53-6, 76, 160-1, 
178, 180 
de Jager, N. viii 
de Wilde, I. ix 
Dennis, B. 148, 178 
Descartes, R. 155, 178 
Diaconis, P. 177, 178 
Doksum, K 163, 178 
Domotor, Z. 97, 165, 178 
Doyle, P. H. 143, 173, 178 
Dunsmore, I. R. 157-8, 178 
Earman, J. 154, 178 
Elzenga, M. viii 
Fabius, J. 70-1, 178 
Feigl, H. 179 
Feller, W. 157, 178 
Ferguson, T. S. 23, 178 
Festa, R. vii-viii, 159-60, 178 
Fine, T. 81, 161, 164, 179 
Fisher, R. 157 
Freedman, D. 136-7, 172, 179 
Friedman, M., 136-7, 172, 179 
Gaifman, H. 15, 161, 165, 179 
Galavotti, M. C. viii 
Galton, F. 173 
Garbolino, P. viii 
Gatlin, L. 148, 179 
Gavroglu, K 182 
Giaretta, P. viii 
Giere, R.N. 165, 179 
Gini, C. xiii, 139-41, 144, 146, 148, 161, 
173-4, 179, 183 
Giorello, G. viii 
Good, I. J. 20, 65, 67, 82, 131, 141, 144, 
150, 160-2, 171-3, 175, 179 
Goudaroulis, Y. 182 

186 
INDEX OF NAMES 
Grassle, J. F., 148, 178-9 
Graves, J. 132-3, 179 
Greenberg, J. H. 148, 174, 179 
Guiasu, R. 88, 164, 179 
Gurraud,P. 149,179 
Gwyther, C. ix 
Hacking, I. 154-5, 179 
Haldane, E. S. 178 
Hall, W. J. 67, 164, 175, 178, 182 
Hardy, G. F. 161-2, 179 
Harper, W. 165, 182 
Harsanyi, J. C. 157, 179 
Hartigan, J. 164, 179 
Hartley, D. 3 
Helman, D. H. 
Hempel, C. G. 157, 179 
Herdan, G. 148, 179-80 
Hill, M. 0. 142, 173-4, 180 
Hilpinen, R. viii, 94-5, 157, 180 
Hintikka, J. 134, 157, 161, 165, 171, 175, 
180 
Hobbes, T. 154, 180 
Hofstee, W. K. B. viii 
Hogarth, R. M. 169, 180 
Hooker, C. A. 182 
Horwich, P. 44, 154, 160, 172, 180 
Howson, C. 150, 154, 180 
Hume, D. 154, 180 
Hurlbert, S. H. 144-5, 180 
Huygens, C. 3 
Huzurbazar, V. S. 82, 164, 180 
Jackson, P. H. 163, 169, 180 
James, J. R. 163, 180 
Jamison, D. 162, 165, 180 
Jaynes, E. T. 79, 83-6, 88, 164, 180 
Jeffrey, R. 28, 53, 59, 88-9, 157, 161, 177, 
180-1 
Jeffreys, H. 79-82, 163-4, 181 
Johnson, N. L. 
Johnson, W. E. 161-2, 817 
Jonkman, H. ix 
Kaplan, M. 157, 181 
Kemeny, J. 68, 181 
Keynes, J. M. 155 
Kotz, S. 178, 183 
Kuhn, T. 150 
Kuipers, T. A. F. vii-viii, 16, 45-6, 68, 123, 
128-32, 156-7, 160-1, 165, 171-2, 175, 
181 
Kullback, S. 84, 164, 181 
Kyburg, H. E. Jr. 157, 178, 181 
Lakatos, I. 177, 180-1 
Laplace, P. S. 37, 79, 157-8, 181 
Laudan, L. 3, 151, 155, 175, 181 
La Valle, I. H. 26, 28, 157, 181 
Leibler, R. A. 164, 181 
LeSage, G. 3 
Levi, I. viii, 23, 154, 157, 181-2 
Levine, R. D. 180 
Lewis, D. 40-1, 123, 127-8, 159, 171, 182, 
184 
Lewontin, R. C. 148, 182 
Lidstone, G. J., 161-62, 182 
Lieberson, S. 148, 174, 182 
Lindgren, B. W. 23, 27, 182 
Lindley, D. 82, 164, 182 
Locke, J. 152 
Lourens, P. F. 169, 182 
Marcus, B. 179 
Martin, J. J. 175, 182 
Maxwell, G. 179 
May, S. 165, 182 
Mill, J. S. 16, 132-3, 182 
Miller, D. 45-6, 87, 160, 182 
Milne, P. 164, 182 
Mooij, A. ix 
Mooij, C. ix 
Mooij, H. ix 
Mooij, J. ix 
Mooij, S. ix 
Molenaar, W. vii, 169, 170 
Mondadori, M. viii 
Mosimann, J. E. 70, 163, 177, 180, 182 
Nauta, L. viii 
Newton, I. 155 
Neyman, J. 157 

INDEX OF NAMES 
187 
Nicolacopoulos, P. 182 
Niiniluoto, I. vii-viii, 23, 46, 155, 159-61, 
165, 172, 175, 180, 182 
Novick, M. R. 67, 163-4, 169, 178 
Oddie, G. 46, 158, 178 
Pascal, B. 3 
Pasquinelli, A. viii 
Patil, G. 141-2, 148, 173-5, 182 
Pearce, D. viii 
Pearson, E. S. 157 
Pearson, K 161, 173, 183 
Peirce, C. S. 181 
Pera, M. viii 
Perks, W. 164, 179 
Piazza, G. viii 
Pielou, E. C. 140, 148-9, 173-4, 183 
Pietarinen, J. 128, 134-5, 157, 161, 170-1, 
180, 183 
Pizzi, C. viii 
Popper, K R. vii, 1, 45, 155, 157, 160, 183 
Priestley, J. 3 
Przelecki, M. 180 
Putnam, H. 156 
Quetelet, A. 173 
Radnitzky, G. 184 
Rao, C. R. 174, 183 
Ratcliff, D. 163, 178 
Reichenbach, H. 1)5-6 
Renyi, A. 88, 174, 183 
Roberts, H. V. 155, 179 
Rosenkrantz, R. D. 84-5, 134, 137-8, 150, 
154, 162-4, 172, 182-3 
Ross, G. R. T. 178 
Rossi, 0. 148, 178 
Sandri, G. viii 
Savage, L. J. 54, 76-7, 164, 183 
Schaafsma, W. viii, 154 
Schilpp, P. A. 177, 183 
Schuessler, K F. 177 
Schurz, G. 160, 183 
Seidenfeld, T. 79, 80, 83-5, 163-4, 183 
Sen, A. 175, 183 
Shannon, C. E. 48,173,183 
Shimony, P.. 164, 183 
Siersema, I. ix 
Simpson, E. H. 143-4, 173, 183-4 
Skilling, J. 164, 184 
Skyrms, B. 152, 171-2, 179 
Smith, M. 148, 178-9 
Smokier, H. 178 
Spielman, S. 44, 160, 166, 170, 182, 184 
Stegmuller, W. 5, 57, 68, 170, 184 
Stehman, S. 148, 178 
Stigler, S. M. 181 
Sugihara, G. 147, 174, 184 
Suppes, P. 180 
Swinburne, R. 154, 184 
Tan, Y. H. viii 
Taillie. C. 141-2, 148, 173-5, 178-9, 183 
Terlouw, P. 169, 184 
Tiao, G. 82, 157, 164, 177 
Tichy, P. 45-6, 160 
Tintner, G. T. 156, 184 
Tribus, M. 180 
Tuomela, R. viii, 46 
Turing, A. M. 144, 173 
van Fraassen, B. viii 
van Heusden, B. ix 
Uppuluri, V. R. R. 144, 173, 177 
Urbach, P. 150, 154, 180 
Villegas, C. 164, 184 
Walk, K 172, 184 
Wallace, A. R. 144-5, 184 
Watkins, J. 155, 184 
Weaver, W. 173, 183-4 
Weingartner, P. 160, 183 
Westfall, R. S. 181 
Wilks, S. S. 162, 184 
Williams, P. M. 164, 184 
Winkler, R. L. 39, 61, 76-8, 157, 163, 184 
Withworth, W. A. 161, 184 
Wolf, R. 86 
Yule, G. U. 148, 173, 184 
Zabell, S. L. 157-8, 162, 184 
Zandvoort, H. ix 
Zellner, A. 21, 157, 184 

INDEX OF SUBJECTS 
analogy by similarity 58, 70, 175 
Bayes's theorem 1, 5, 20-1, 29, 35 
Bayesian approach to inductive inference 
acceptational 23 
globalistic 23 
predictivistic 23 
probabilistic 23 
Bayesian view 1-4 
Bernoulli process 4, 18-9, 33, 60, 66, 77 
multivariate 4-5, 7-8, 17, 20, 29-30, 34-5, 
38, 44, 51-3, 57, 64, 66, 77, 82, 
100, 152 
Beta distributions 60-2, 64, 66, 71, 76-8 
C-systems 58, 66, 68-9 
cognitive coherence (requirement of) 98-9, 
108-9 
cognitive context 2, 4 
cognitive decision 125 
theory 23 
cognitive failure 110-2 
prospect of 110-2, 121-2 
cognitive loss 39 
cognitive utility 23, 125 
complete description 13 
consensus (disagreement) in science 53-4, 
75, 79, 151, 175 
convergence 
of opinion 8, 51, 53-5 
towards the truth 8, 51, 55-6 
towards the empirical frequencies 15, 55-6 
correlation coefficient 27-8 
covariance 27-8 
cumulative distribution function 5, 24-5, 27-
32, 35-6, 44, 51-3 
de Finetti's representation theorem 5, 7, 51-
53, 65 
188 
Dirichlet distributions 5, 8, 57, 61-5, 77, 82, 
103-9, 111-22 
symmetrical 65-6, 121-2 
axiomatization of 8, 57, 70-1 
contextual justification of 8, 75, 100-2 
disorder, see diversity 
distance between probability vectors 39 
quadratic 39-43 
distance from the truth 39, 46, 106-22 see 
verisimilitude 
expected 39-43, 77-8, 118-20 
distributional constraints 84 
formal 97, 101 
moment constraints 84 
probabilistic 84 
propositional 84 
diversity measures 139-49, 152 (see Gini 
diversity) 
explications of 141-7 
in the empirical sciences 147-9 
minimal requirements of adequacy for 
145-6 
entropy 83 
cross-entropy 84, 87-8 
Shannon's 83-4, 87-8 
epistemic problem of optimality 8, 103-22, 
151-3 
contextual approach to 8, 103-20, 123 
hyperempiricist view of 123, 127-32 
Kuipers's approach to 129-32 
Lewis's approach to 127-8 
presupposition view of 121, 130-3 
universalistic view of 121-5 
verisimilitude view of 123, 132-5 
V-solution to 8, 103-4, 116-20, 152-3 
CC-solution to 103-4, 108-9, 118-20, 152 

INDEX OF SUBJECfS 
189 
error 113 
seriousness of 121-2 
exchangeability (requirement of) 37, 51-6, 
58,76 
expectation, see mean 
Ex-similar process 100-1 
external estimates 98-9, 104-9 
of Gini diversity 105-7, 126, 135 
of the distance between the prior vector 
and the truth 106-9 
of the prospect of failure of mean 
estimators 117-8 
fallibilism 3-4 
probabilistic fallibilism 3-4 
verisimilitude fallibilism 3-4 
mature fallibilism 3-4 
family 
of properties (categories) 13 
of attributes 91 
GC-systems 5-8, 57-9, 89, 91, 102 
axiomatization of 8, 68-9 
equivalence between Dirichlet distributions 
and 5-6, 8, 65-6 
extreme 66-7 
GO-context 103, 105-7, 126 
Gini diversity 5-6, 8, 33, 139-46 
expected 7, 20, 33, 64, 77 
heterogeneity, see uiversity 
homogeneity, see uniformity 
ignorance 
complete 79-82 
immodesty (principle of) 7, 38, 40-4, 157-8 
immodest 
inductive methods 40-1, 44, 127, 131 
probability distributions 38, 42-4 
independence 25, 28, 30 
independence of Q; and Qi (hypothesis of) 
101 
induction 
de Finetti's requirement of 54-5, 76 
local 151 
inductive 
inference 4, 16, 20 
intuition 88-90, 162 
acceptance 22, 157 
inductive method 5-8, 13-17, 36-7, 57-8 
exchangeable 5, 8, 15, 41, 44 
inductive principles 2, 
a priori 2, 90, 95-6 
contextual 2, 95 
descriptive 76, 88 
normative 76, 90 
universal 95 
infallibilism 3 
information 
experimental 98 
external 98 
partial 82 
prior 75, 78, 83, 85-6, 97 
theoretical 98 
initial equipossibility (requirement of) 58, 
68-9 
initial possibility (requirement of) 58, 68-9 
insufficient reason (principle of) 79-81, 84 
interval estimate 152, 155 
interviewing 77 
direct methods of 78 
indirect methods of 78 
invariance 80 
Invariants, theory of 82 
Laplace's succession rule 37 
learning from experience 51, 55-6, 66-7 
linearity (requirement of) 68-9, 160 
likelihood 21, 30, 35 
logical problem of optimality 8, 109-15, 125 
V-solution to 8, 110-5, 152-3 
loss 22, 38 
expected 39 
quadratic 39 
verisimilitude interpretation of 39-40 
maximum entropy 
approach 83-88, 96 
principle of 83, 85-7, 97 
mean 26,29 
conditional 27 
marginal 27, 31 

190 
INDEX OF SUBJECfS 
mean vector 27, 31, 33, 38 
mean difference 140-1 
mean estimators 7, 38-40, 112-20 
decision-theoretic justification of 38-9 
verisimilitude interpretation of7, 39-40, 45 
minimal change of belief (principle of) 85, 
87, 97 
moment 26-7,31, 
central 26-7 
moment constraints 84 
multicategorical inferences 13, 16-7 
global 16 
predictive 16 
statistical 17 
universal 16 
multinomial context 4, 6-7, 13, 17-19, 30, 
34 
multinomial inferences 1, 4-5, 7, 19 
Bayesian 20, 34, 51-2 
neutrality (requirement of) 70, 100-1 
'no gap' requirement 53 
non-negative relevance (requirement of) 15, 
37 
optimum estimator 110-2 
optimum inductive methods 6, 90 
Camap's 8, 121-2 
optimum GC-systems 6, 8, 121 
optimum Dirichlet distribution 8, 109-22 
optimum prior vector 6, 104-5 
optimum A.-value 93 
epistemically 116-20, 132-5 
logically 114-5, 121-2 
order, see uniformity 
parametric requirements 52-3 
contextual justification of 95-7, 100-2 
physical constraints 85-6 
point estimate 38-40, 113 
positive relevance (requirement of) 15, 58 
predictive distribution 14, 16, 30, 35-6 
predictive requirements 16, 52-3 
equivalence between parametric and 53 
principles of rationality 2, 76 
see inductive principles 
prior (probability) distribution 5, 8, 29, 34, 
51-2 
partial information 83, 85 
improper 67 
informationless 80, 82 
MAXENT 83-5, 164 
objective 
reference 82 
uniform 36, 60, 65 
prior inference 98-9 
prior probabilities 1, 2, 4, 6, 8, 21, 75-102 
aprioristic view of 2, 8, 75, 79-88, 90-5, 
150-1 
contextual view of 2, 4, 6, 8-9, 75, 95-
102, 150-3 
objective view of 2 
subjective view of 2, 8, 75-9, 88-90, 150 
probability 
axioms of 2, 14, 17, 
epistemic 2, 4-5, 7, 21, 24, 30, 
inductive 2, 13-17 
objective (or physical) 4, 30, 110, 112, 
115 
predictive 8, 31, 36 
probability density function 24-6, 28 
probability distribution 24 
conditional 25 
joint 24-5 
marginal 25 
probability function 24, 29 
random variable 23 
regularity (requirement of) 15 
Reichenbach axiom 15, 51, 54, 56, 58 
relevance quotient 59 
condition of invariance of the 69 
re-parametrization (paradoxes of) 81 
restricted relevance (requirement of) 58, 68 
sampling distribution 29, 113 
sequence of outcomes 13 
structurally identical 13, 15, 18, 
special values 14, 16-7, 36, 57-9 
statistic 29, 112 
statistical inference 20-4 

INDEX OF SUBJECTS 
191 
multivariate 24 
univariate 23-4 
straight rule 59, 66 
structural description 18, 92 
theory of inductive probabilities 1-2, 4-5, 7, 
13-6 
aprioristic interpretation of 90-5 
contextual i'nterpretation of 100-2 
subjective interpretation of 88-90 
truthlikeness, see verisimilitude 
typical values 7, 20, 25, 27-9, 33, 60, 63-4, 
84 
predictive interpretation of 7, 20, 31-2, 34 
uniformity 
Camap's uniformity index 32, 170 
expected 33 
requirement of 92-5 
uniformity of nature (principle of) 126-7, 
132-3 
variance 25, 27-9 
conditional 32 
epistemic interpretation of 39-40 
marginal 27, 32 
total 32-3 
vector 
empirical 58, 115 
posterior 58 
prior 6, 8, 57-8, 63-4, 66, 68, 103-4, 115 
verisimilitude 1-3, 6, 23, 45-7, 136-8, 151-2 
expected 46 
probable 46-7 
theory 1-3, 7, 38-9, 45-7, 151-52 
thesis 2, :10~-4, 116 

LIST OF REQUIREMENTS AND ACRONYMS 
Requirements of adequacy/principles/condiJions/properties 
(Al)-(A4): the axioms of probability calculus 14 
(CC): requirement of cognitive coherence 98-9 
(CIRQ): condition of invariance of the relevance quotient 69 
(CM): requirement of neutrality 70 
(Exc): requirement of exchangeability 15 
(Ext): requirement of adequacy for external estimates 98 
(IE): requirement of initial equipossibility 58 
(IP): requirement of initial possibility 58 
(MCB): rule of minimization of the change of belief 85 
Minimal requirements of adequacy for diversity indices 145-6 
(NG): 'no gap' requirement 53 
(NNR): requirement of non-negative relevance 15 
(PIR): principle of insufficient reason 79 
(PME): principle of maximum entropy 83 
(PR) : requirement of positive relevance 15 
principle of immodesty 43-4 (formulae (8)-(11)) 
property of 'Y]-equality 59 
(RAR): requirement of rarity 142 
(Reg): requirement of regularity 15 
(Reg)*: a requirement equivalent to (Reg) 15 
(Reich): Reichenbach axiom 15 
(RL): requirement of linearity 68 
(RR): requirement of restricted relevance 58 
(UR): uniformity requirement 92 
Acronyms 
BK: background knowledge 83 
BS: Bayesian statistics 1 
cdf: cumulative distribution function 24 
C-systems: Carnapian systems 58 
(CM), -neutrality 70 
DFRT: de Finetti's representation theorem 51 
Eln(Q,Ex): external information on the parameter vector 9 in the experimental process Ex 98 
192 

UST OF REQUIREMENTS AND ACRONYMS 
EPO: the epistemic problem of optimality 6 
GC-systems: Generalized Camapian systems 5 
I: prior information 83 
LPO: the logical problem of optimality 8 
MAXENT: maximum entropy approach 83 
MAXENT distribution: maximum entropy distribution 83 
m-constraints: moment constraints 84 
M-estimator: mean estimator 38 
pdf: probability density function 24 
PI-distribution: partial information distribution 83 
TIP: theory of inductive probabilities 1 
VER: verisimilitude thesis 2 
VT: verisimilitude theory 1 
193 

SYNTHESE LffiRARY 
223. A. Garcia de la Sienra, The Logical Foundations of the Marxian Theory of Value. 
1992 
ISBN 0-7923-1778-5 
224. D.S. Shwayder, Statement and Referent. An Inquiry into the Foundations of our 
Conceptual Order. 1992 
ISBN 0-7923-1803-X 
225. M. Rosen, Problems of the Hegelian Dialectic. Dialectic Reconstructed as a Logic 
of Human Reality. 1993 
ISBN 0-7923-2047-6 
226. P. Suppes, Models and Methods in the Philosophy of Science: Selected Essays. 1993 
ISBN 0-7923-2211-8 
227. R. M. Dancy (ed.), Kant and Critique: New Essays in Honor ofW. H. Werkmeister. 
1993 
ISBN 0-7923-2244-4 
228. J. Woleriski (ed.), Philosophical Logic in Poland. 1993 
ISBN 0-7923-2293-2 
229. M. De Rijke (ed.), Diamonds and Defaults. Studies in Pure and Applied Intensional 
Logic. 1993 
ISBN 0-7923-2342-4 
230. B.K. Matilal and A. Chakrabarti (eds.). Knowing from Words. 1993 
ISBN 0-7923-2345-9 
231. S.A. Kleiner, The Logic of Discovery. A Theory of the Rationality of Scientific 
Research. 1993 
ISBN 0-7923-2371-8 
232. R. Festa, Optimum Inductive Methods. A Study in Inductive Probability, Bayesian 
Statistics, and Verisimilitude. 1993 
ISBN 0-7923-2460-9 
Previous volumes are still available. 
KLUWER ACADEMIC PUBLISHERS- DORDRECHT I BOSTON I LONDON 

