Intelligent Systems Reference Library 126
Qiang Yu
Huajin Tang
Jun Hu
Kay Chen Tan
Neuromorphic 
Cognitive
Systems
A Learning and Memory Centered 
Approach

Intelligent Systems Reference Library
Volume 126
Series editors
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl
Lakhmi C. Jain, University of Canberra, Canberra, Australia;
Bournemouth University, UK;
KES International, UK
e-mail: jainlc2002@yahoo.co.uk; jainlakhmi@gmail.com;
URL: http://www.kesinternational.org/organisation.php

About this Series
The aim of this series is to publish a Reference Library, including novel advances
and developments in all aspects of Intelligent Systems in an easily accessible and
well structured form. The series includes reference works, handbooks, compendia,
textbooks, well-structured monographs, dictionaries, and encyclopedias. It contains
well integrated knowledge and current information in the ﬁeld of Intelligent
Systems. The series covers the theory, applications, and design methods of
Intelligent Systems. Virtually all disciplines such as engineering, computer science,
avionics, business, e-commerce, environment, healthcare, physics and life science
are included.
More information about this series at http://www.springer.com/series/8578

Qiang Yu
• Huajin Tang
• Jun Hu
Kay Chen Tan
Neuromorphic Cognitive
Systems
A Learning and Memory Centered Approach
123

Qiang Yu
Institute for Infocomm Research
Singapore
Singapore
Huajin Tang
College of Computer Science
Sichuan University
Chengdu
China
Jun Hu
AGI Technologies
Singapore
Singapore
Kay Chen Tan
Department of Computer Science
City University of Hong Kong
Kowloon Tong
Hong Kong
ISSN 1868-4394
ISSN 1868-4408
(electronic)
Intelligent Systems Reference Library
ISBN 978-3-319-55308-5
ISBN 978-3-319-55310-8
(eBook)
DOI 10.1007/978-3-319-55310-8
Library of Congress Control Number: 2017933943
© Springer International Publishing AG 2017
Reuse of the materials of “Cheu, E.Y., Yu, J., Tan, C.H. and Tang, H. Synaptic conditions for
auto-associative memory storage and pattern completion in Jensen et al.’s model of hippocampal area
CA3. Journal of Computational Neuroscience, 2012, 33(3): 435–447”, with permission from Springer
Publisher.
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

To our family, for their loves and supports.
Qiang Yu
Huajin Tang
Jun Hu
Kay Chen Tan

Preface
The powerful and yet mysterious human brain system attracts numerous researchers
devoting themselves to characterizing what nervous systems do, determining how
they
function,
and
understanding
why
they
operate
in
particular
ways.
Encompassing various studies of biology, physics, psychology, mathematics, and
computer science, theoretical neuroscience provides a quantitative basis for
uncovering the general principles by which the nervous systems operate. Based on
these principles, neuromorphic cognitive systems introduce some basic mathe-
matical and computational methods to describe and utilize schemes at a cognitive
level. Since the mechanisms how human memory cognitively operates and how to
utilize the bioinspired mechanisms to practical applications are rarely known, the
study of neuromorphic cognitive systems is urgently demanded.
This book presents the computational principles underlying spike-based infor-
mation processing and cognitive computation with a speciﬁc focus on learning and
memory. Speciﬁcally, the action potential timing is utilized for sensory neuronal
representations and computation, and spiking neurons are considered as the basic
information processing unit. The topics covered in this book vary from neuronal
level to system level, including neural coding, learning in both single- and multi-
layered networks, cognitive memory, and applied developments of information
processing systems with spiking neurons. From the neuronal level, synaptic
adaptation plays an important role on learning patterns. In order to perform higher
level cognitive functions such as recognition and memory, spiking neurons with
learning abilities are consistently integrated with each other, building a system with
the functionality of encoding, learning, and decoding. All these aspects are
described with details in this book.
Theories, concepts, methods, and applications are provided to motivate
researchers in this exciting and interdisciplinary area. Theoretical modeling and
analysis are tightly bounded with practical applications, which would be potentially
vii

beneﬁcial for readers in the area of neuromorphic computing. This book presents
the computational ability of bioinspired systems and gives a better understanding
of the mechanisms by which the nervous system might operate.
Singapore
Qiang Yu
Chengdu, China
Huajin Tang
Singapore
Jun Hu
Kowloon Tong, Hong Kong
Kay Chen Tan
December 2016
viii
Preface

Contents
1
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Spiking Neurons. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2.1
Biological Background . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2.2
Generations of Neuron Models . . . . . . . . . . . . . . . . . . . . . .
3
1.2.3
Spiking Neuron Models . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Neural Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3.1
Rate Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3.2
Temporal Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.3
Temporal Code Versus Rate Code . . . . . . . . . . . . . . . . . . .
7
1.4
Cognitive Learning and Memory in the Brain . . . . . . . . . . . . . . . .
8
1.4.1
Temporal Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4.2
Cognitive Memory in the Brain . . . . . . . . . . . . . . . . . . . . .
11
1.5
Objectives and Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.6
Outline of the Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2
Rapid Feedforward Computation by Temporal Encoding
and Learning with Spiking Neurons . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2
The Spiking Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.3
Single-Spike Temporal Coding. . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.4
Temporal Learning Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4.1
The Tempotron Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.4.2
The ReSuMe Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.4.3
The Tempotron-Like ReSuMe Rule . . . . . . . . . . . . . . . . . .
28
2.5
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.5.1
The Data Set and the Classiﬁcation Problem. . . . . . . . . . . .
29
2.5.2
Encoding Images. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.5.3
Choosing Among Temporal Learning Rules . . . . . . . . . . . .
30
ix

2.5.4
The Properties of Tempotron Rule . . . . . . . . . . . . . . . . . . .
32
2.5.5
Recognition Performance . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.6.1
Encoding Beneﬁts from Biology . . . . . . . . . . . . . . . . . . . . .
37
2.6.2
Types of Synapses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.6.3
Schemes of Readout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.6.4
Extension of the Network for Robust Sound
Recognition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3
A Spike-Timing Based Integrated Model for Pattern
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.2
The Integrated Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2.1
Neuron Model and General Structure . . . . . . . . . . . . . . . . .
45
3.2.2
Latency-Phase Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.2.3
Supervised Spike-Timing Based Learning. . . . . . . . . . . . . .
49
3.3
Numerical Simulations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.3.1
Network Architecture and Encoding
of Grayscale Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.3.2
Learning Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3.3
Generalization Capability . . . . . . . . . . . . . . . . . . . . . . . . . .
53
3.3.4
Parameters Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.3.5
Capacity of the Integrated System. . . . . . . . . . . . . . . . . . . .
57
3.4
Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.5
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4
Precise-Spike-Driven Synaptic Plasticity for Hetero
Association of Spatiotemporal Spike Patterns . . . . . . . . . . . . . . . . . . .
65
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.2
Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.2.1
Spiking Neuron Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.2.2
PSD Learning Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.3
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.3.1
Association of Single-Spike and Multi-spike Patterns . . . . .
71
4.3.2
Generality to Different Neuron Models . . . . . . . . . . . . . . . .
76
4.3.3
Robustness to Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.3.4
Learning Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.3.5
Effects of Learning Parameters . . . . . . . . . . . . . . . . . . . . . .
81
4.3.6
Classiﬁcation of Spatiotemporal Patterns. . . . . . . . . . . . . . .
83
4.4
Discussion and Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
x
Contents

5
A Spiking Neural Network System for Robust Sequence
Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
5.2
The Integrated Network for Sequence Recognition. . . . . . . . . . . . .
91
5.2.1
Rationale of the Whole System. . . . . . . . . . . . . . . . . . . . . .
91
5.2.2
Neural Encoding Method . . . . . . . . . . . . . . . . . . . . . . . . . .
92
5.2.3
Item Recognition with the PSD Rule . . . . . . . . . . . . . . . . .
94
5.2.4
The Spike Sequence Decoding Method. . . . . . . . . . . . . . . .
94
5.3
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.3.1
Learning Performance Analysis of the PSD Rule . . . . . . . .
97
5.3.2
Item Recognition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
5.3.3
Spike Sequence Decoding. . . . . . . . . . . . . . . . . . . . . . . . . .
105
5.3.4
Sequence Recognition System. . . . . . . . . . . . . . . . . . . . . . .
107
5.4
Discussions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
5.4.1
Temporal Learning Rules and Spiking Neurons . . . . . . . . .
110
5.4.2
Spike Sequence Decoding Network . . . . . . . . . . . . . . . . . .
110
5.4.3
Potential Applications in Authentication . . . . . . . . . . . . . . .
111
5.5
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
6
Temporal Learning in Multilayer Spiking Neural Networks
Through Construction of Causal Connections. . . . . . . . . . . . . . . . . . .
115
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
6.2
Multilayer Learning Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
6.2.1
Spiking Neuron Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
6.2.2
Multilayer PSD Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
6.2.3
Multilayer Tempotron Rule. . . . . . . . . . . . . . . . . . . . . . . . .
119
6.3
Heuristic Discussion on the Multilayer Learning Rules . . . . . . . . .
120
6.4
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
6.4.1
Construction of Causal Connections . . . . . . . . . . . . . . . . . .
121
6.4.2
The XOR Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
6.4.3
The Iris Benchmark. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
6.5
Discussion and Conclusion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
7
A Hierarchically Organized Memory Model with Temporal
Population Coding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
7.2
The Hierarchical Organized Memory Model. . . . . . . . . . . . . . . . . .
133
7.2.1
Neuron Models and Neural Oscillations . . . . . . . . . . . . . . .
133
7.2.2
Temporal Population Coding. . . . . . . . . . . . . . . . . . . . . . . .
135
7.2.3
The Tempotron Learning and STDP . . . . . . . . . . . . . . . . . .
135
Contents
xi

7.3
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
7.3.1
Auto-Associative Memory. . . . . . . . . . . . . . . . . . . . . . . . . .
142
7.3.2
Episodic Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
7.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
7.4.1
Information Flow and Emergence of Neural Cliques. . . . . .
147
7.4.2
Storage, Recall and Organization of Memory . . . . . . . . . . .
148
7.4.3
Temporal Compression and Information Binding . . . . . . . .
149
7.4.4
Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
7.5
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
8
Spiking Neuron Based Cognitive Memory Model . . . . . . . . . . . . . . . .
153
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
8.2
SRM-Based CA3 Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
8.2.1
Spike Response Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
8.2.2
SRM-Based Pyramidal Cell. . . . . . . . . . . . . . . . . . . . . . . . .
156
8.2.3
SRM-Based Interneuron . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
8.3
Convergence of Synaptic Weight . . . . . . . . . . . . . . . . . . . . . . . . . .
160
8.4
Maximum Synaptic Weight to Prevent Early Activation. . . . . . . . .
161
8.5
Pattern Completion of Auto-Associative Memory. . . . . . . . . . . . . .
163
8.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
8.7
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
xii
Contents

Acronyms
AMPA
Alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid
ANN
Artiﬁcial neural network
CCs
Complex cells
DoG
Difference of gaussian
EPSP
Excitatory post-synaptic potential
FSM
Finite state machine
GABA
Gamma-amino-butyric-acid
GCs
Ganglion cells
HH
Hodgkin–Huxley model
IM
Izhikevich model
IPSP
Inhibitory post-synaptic potential
LIF
Leaky integrate-and-ﬁre neuron
LSF
Local spectrogram feature
LSM
Liquid state machine
LTD
Long-term depression
LTP
Long-term potentiation
MutPSD
Multilayer PSD
MutTmptr
Multilayer tempotron
NMDA
N-methyl-D-aspartate
OCR
Optical character recognition
PSC
Post-synaptic current
PSD
Precise-spike-driven
ReSuMe
Remote supervised method
SMO
Subthreshold membrane potential oscillation
SNN
Spiking neural network
SOM
Self-organizing map
SPAN
Spike pattern association neuron
SRM
Spike response model
xiii

STDP
Spike-timing-dependent plasticity
SVM
Support vector machine
VLSI
Very large scale integration
WH
Widrow–Hoff
XOR
Exclusive OR
xiv
Acronyms

Chapter 1
Introduction
Abstract Since the emergence of the ﬁrst digital computer, people are set free from
heavy computing works. Computers can process a large amount of data with high
precision and speed. However, compared to the brain, the computer still cannot
approach a comparable performance considering cognitive functions such as per-
ception, recognition and memory. For example, it is easy for human to recognize the
face of a person, read papers and communicate with others, but hard for comput-
ers. Mechanisms that utilized by the brain for such powerful cognitive functions still
remain unclear. Neural networks are developed for providing a brain-like information
processing and cognitive computing. Theoretical analysis on neural networks could
offer a key approach to revealing the secret of the brain. The subsequent sections
provide detailed background information, as well as the objectives of this book.
1.1
Background
The computational power of the brain has attracted many researchers to reveal its
mystery in order to understand how it works and to design human-like intelligent sys-
tems. The human brain is constructed with around 100 billion highly interconnected
neurons. These neurons transmit information between each other to perform cog-
nitive functions. Modeling neural networks facilitates investigation of information
processing and cognitive computing in the brain from a mathematical point of view.
Artiﬁcial neural networks (ANNs), or simply called neural networks, are the earliest
work for modeling the computational ability of the brain. The research on ANNs
has achieved a great deal in both theories and engineering applications. Typically, an
ANN is constructed with neurons which have real-valued inputs and outputs.
However, biological neurons in the brain utilize spikes (or called as action poten-
tials) for information transmission between each other. This phenomenon of the
spiking nature of neurons has been known since the ﬁrst experiments conducted by
Adrian in the 1920s [1]. Neurons will send out short pulses of energy (spikes) as
signals, if they have received enough input from other neurons. Based on this mech-
anism, spiking neurons are developed with a same capability of processing spikes
as biological neurons. Thus, spiking neural networks (SNNs) are more biologically
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_1
1

2
1
Introduction
plausible than ANNs since the concept of spikes, rather than real values, is consid-
ered in the computation. SNNs are widely studied in recent years, but questions of
how information is represented by spikes and how the neurons process these spikes
are still unclear. These two questions demand further studies on neural coding and
learning in SNNs.
Spikes are believed to be the principal feature in the information processing of
neural systems, though the neural coding mechanism remains unclear. In 1920s,
Adrian also found that sensory neurons ﬁre spikes at a rate monotonically increasing
with the intensity of stimulus. This observation led to the widespread adoption of
the hypothesis of a rate coding, where neurons communicate purely through their
ﬁring rates. Recently, an increasing body of evidence shows that the precise timing of
individual spikes also plays an important role [2]. This ﬁnding supports the hypoth-
esis of a temporal coding, where the precise timing of spikes, rather than the rate,
is used for encoding information. Within a temporal coding framework, temporal
learning describes how neurons process precise-timing spikes. Further research on
temporal coding and temporal learning would provide a better understanding of the
biological systems, and would also explore potential abilities of SNNs for informa-
tion processing and cognitive computing. Moreover, beyond independently studying
the temporal coding and learning, it would be more important and useful to consider
both in a consistent system.
1.2
Spiking Neurons
The rough concept of how neurons work is understood: neurons send out short pulses
of electrical energy as signals, if they have received enough of these themselves.
This principal mechanism has been modeled into various mathematical models for
computer use. These models are built under the inspiration of how real neurons work
in the brain.
1.2.1
Biological Background
A neuron is an electrically excitable cell that processes and transmits information by
electrical and chemical signaling. Chemical signaling occurs via synapses, special-
ized connections with other cells. Neurons form neural networks through connecting
with each other.
Computers communicate with bits; neurons use spikes. Incoming signals change
the membrane potential of the neuron and when it reaches above a certain value the
neuron sends out an action potential (spike).
As is shown in Fig.1.1, a typical neuron possesses a cell body (often called soma),
dendrites, and an axon. The dendrites serve as the inputs of the neuron and the axon

1.2 Spiking Neurons
3
Fig. 1.1 Structure of a
typical neuron. A neuron
typically possesses a soma,
dendrites and an axon. The
neuron receives inputs via
dendrites and sends output
through the axon
acts as the output. The neuron collects information through its dendrites and sends
out the reaction through the axon.
Spikes cannot cross the gap between one neuron and the other. Connections
between neurons are formed via cellular interfaces, so called synapses. An incoming
pre-synaptic action potential triggers the release of neurotransmitter chemicals in
vesicles. These neurotransmitters cross the synaptic gap and bind to receptors on the
dendritic side of the synapse. Then a post-synaptic potential will be generated [3, 4].
The type of synapse and the amount of released neurotransmitter determine the
type and strength of the post-synaptic potential. The membrane potential would be
increased by excitatory post-synaptic potential (EPSP) or decreased by inhibitory
post-synaptic potential (IPSP). Real neurons only use one type of neurotransmit-
ter in all their outgoing synapses. This makes the neuron either be excitatory or
inhibitory [3].
1.2.2
Generations of Neuron Models
From the conceptual point of view, all neuron models share the following common
features:
1. Multiple inputs and single output: The neuron receives many inputs and pro-
duces a single output signal.
2. Different types of inputs: The output activities of neurons are characterized by
at least one state variable that usually corresponding to the membrane poten-
tial. An input from the excitatory/inhibitory synapses will increase/decrease the
membrane potential.
Based on these conceptual features, various neuron models are developed. Arti-
ﬁcial neural networks are already becoming a fairly old technique within computer
science. The ﬁrst ideas and models are over ﬁfty years old. The ﬁrst generation of
artiﬁcial neuron is the one with McCulloch-Pitts threshold. These neurons can only
give digital output. Neurons of the second generation do not use a threshold func-
tion to compute their output signals, but a continuous activation function, making

4
1
Introduction
them suitable for analog input and output [5]. Typical examples of neural networks
consisting of these neurons are feedforward and recurrent neural networks. They are
more powerful than their ﬁrst generation [6].
Neuron models of the ﬁrst two generations do not employ the individual pulses.
The third generation of neuron models raises the level of biological realism by using
individual spikes. This allows incorporating spatiotemporal information in commu-
nication and computation, like real neurons do.
1.2.3
Spiking Neuron Models
For the reasons of greater computational power and more biological plausibility,
spiking neurons are widely studied in recent years. As the third generation of neuron
models, spiking neurons increase the level of realism in a neural simulation.
Spiking neurons have an inherent notion of time that makes them seemingly
particularly suited for processing temporal input data [7]. Their nonlinear reaction
to input provides them with strong computational qualities, theoretically requiring
just small networks for complex tasks.
1.2.3.1
Leaky Integrate-and-Fire Neuron (LIF)
The leaky integrate-and-ﬁre neuron [4] is the most widely used and best-known
model of threshold-ﬁre neurons. The membrane potential of the neuron Vm(t) is
dynamically changing over time, as:
τm
dVm
dt
= −Vm + I (t)
(1.1)
where τm is the membrane time constant in which voltage leaks away. A bigger τm
can result in a slower decaying process of Vm(t). I (t) is the input current which is a
weighted sum from all incoming spikes.
Once a spike arrives, it is multiplied by corresponding synaptic efﬁcacy factor to
form the post-synaptic potential that changes the potential of the neuron. When the
membrane potential crosses a certain threshold value, the neuron will elicit a spike;
after which the membrane potential goes back to a reset value and holds there for a
refractory period. Within the refractory time, the neuron is not allowed to ﬁre.
From both the conceptual and computational points of view, the LIF model is
relatively simple comparing to other spiking neuron models. An advantage of the
model is that it is relatively easy to integrate it in hardware, achieving a very fast
operation. Various generalizations of the LIF model have been developed. One pop-
ular generalization of the LIF model is the Spike Response Model (SRM), where a
kernel approach is used in neuron’s dynamics. The SRM is widely used due to its
simplicity in analysis.

1.2 Spiking Neurons
5
1.2.3.2
Hodgkin-Huxley Model (HH) and Izhikevich Model (IM)
The Hodgkin-Huxley (HH) model was based on experimental observations with the
large neurons found in squid [8]. It is by far the most detailed and complex neuron
model. However, this model is less suited for simulations of large networks since the
realism of neuron model comes at a large computational cost.
The Izhikevich model (IM) was proposed in [9]. By choosing different parameter
values in the dynamic equations, the neuron model can function differently, like
bursting or single spiking.
1.3
Neural Codes
The world around us is extremely dynamic, that everything changes continuously
over time. The information of the external world goes into our brain through the
sensory systems. Determining how neuronal activity represents sensory information
is central for understanding perception. Besides, understanding the representation of
external stimuli in the brain directly determines what kind of information mechanism
should be utilized in the neural network.
Neurons are remarkable among the cells of the body in their ability to propagate
signals rapidly over large distances. They do this by generating characteristic elec-
trical pulses called action potentials or, more simply, spikes that can travel down
nerve ﬁbers. Sensory neurons change their activities by ﬁring sequences of action
potentials in various temporal patterns, with the presence of external sensory stim-
uli, such as light, sound, taste, smell and touch. It is known that information about
the stimulus is encoded in this pattern of action potentials and transmitted into and
around the brain.
Although action potentials can vary somewhat in duration, amplitude and shape,
they are typically treated as identical stereotyped events in neural coding studies.
Action potentials are all very similar. In addition, neurons in the brain work together,
rather than individually, to transfer the information.
Figure1.2 shows a typical spatiotemporal spike pattern. This pattern contains
both spatial and temporal information of a neuron group. Each neuron ﬁres a spike
train within a time period. The spike trains of the whole neuron group form the
spatiotemporal pattern. The spiking neurons inherently aim to process and produce
this kind of spatiotemporal spike patterns.
The question is still not clear that how this kind of spike trains could convey
information of the external stimuli. A spike train may contain information based on
different coding schemes. In motor neurons, for example, the strength at which an
innervated muscle is ﬂexed depends solely on the ‘ﬁring rate’, the average number
of spikes per unit time (a ‘rate code’). At the other end, a complex ‘temporal code’
is based on the precise timing of single spikes. They may be locked to an external
stimulus such as in the auditory system or be generated intrinsically by the neural
circuitry [10].

6
1
Introduction
Fig. 1.2 A typical spatiotemporal spike pattern. A group of neurons (Neuron Group) works together
to transfer the information, with each neuron ﬁring a spike train in time. All spike trains from
the group form a pattern with both spatio- and temporal-dimension information. This is called
spatiotemporal spike pattern. The vertical lines denote spikes
Whether neurons use the rate code or the temporal code is a topic of intense debate
within the neuroscience community, even though there is no clear deﬁnition of what
these terms mean. The followings further present a detailed overview of the rate code
and the temporal code.
1.3.1
Rate Code
Rate code is a traditional coding scheme, assuming that most, if not all, information
about the stimulus is contained in the ﬁring rate of the neuron. Because the sequence
of action potentials generated by a given stimulus varies from trial to trial, neuronal
responses are treated statistically or probabilistically. They may be characterized by
ﬁring rates, rather than by speciﬁc spike sequences. In most sensory systems, the
ﬁring rate increases, generally non-linearly, with increasing stimulus intensity [3].
Any information possibly encoded in the temporal structure of the spike train is
ignored. Consequently, the rate code is inefﬁcient but highly robust with respect to
input noise.
Before encoding external information into ﬁring rates, precise calculation of the
ﬁring rates is required. In fact, the term ‘ﬁring rate’ has a few different deﬁnitions,
which refer to different averaging procedures, such as an average over time or an
average over several repetitions of experiment. For most cases in the coding scheme,
it considers the spike count within an encoding window [11]. The encoding window
is deﬁned as the temporal window that contains the response patterns that are con-
sidered as the basic information-carrying units of the code. The hypothesis of the
rate code receives support from the ubiquitous correlation of ﬁring rates with sensory
variables [1].

1.3 Neural Codes
7
1.3.2
Temporal Code
When precise spike timing or high-frequency ﬁring-rate ﬂuctuations are found to
carry information, the neural code is often identiﬁed as a temporal code [12]. A
number of studies have found that the temporal resolution of the neural code is on a
millisecond time scale, indicating that precise spike timing is a signiﬁcant element
in neural coding [13, 14].
Neurons, in the retina [15, 16], the lateral geniculate nucleus (LGN) [17] and the
visual cortex [14, 18] as well as in many other sensory systems [19, 20], are observed
to precisely respond to the stimulus on a millisecond timescale. These experiments
support hypothesis of the temporal code, in which precise timings of spikes are taken
into account for conveying information.
Like real neurons, communication is based on individually timed pulses. The
temporal code is potentially much more powerful for encoding information with
respect to the rate code. It is possible to multiplex much more information into a
single stream of individual pulses than you can transmit using just the average ﬁring
rates of a neuron. For example, the auditory system can combine the information of
amplitude and frequency very efﬁciently over one single channel [21].
Another advantage of the temporal code is speed. Neurons can be made to react
to single spikes, allowing for extremely fast binary calculation. The human brain,
for example, can recognize faces in as little as 100 ms [22, 23].
There are several kinds of temporal code that have been proposed, like latency
code, interspike intervals code and phase of ﬁring code [11]. Latency code is a speciﬁc
form of temporal code, that encoding information in the timing of response relative
to the encoding window, which is usually deﬁned with respect to stimulus onset. The
latencyofaspikeisdeterminedbytheexternalstimuli.Astrongerinputcouldresultin
an earlier spike. In the interspike intervals code, the temporally encoded information
is carried by the relative time between spikes, rather than by their absolute time
with respect to stimulus onset. In the phase of ﬁring code, information is encoded
by the relative timing of spikes regarding to the phase of subthreshold membrane
oscillations [11, 24].
1.3.3
Temporal Code Versus Rate Code
Intheratecode,ahighersensoryvariablecorrespondstoahigherﬁringrate.Although
there are few doubts as to the relevance of this ﬁring rate code, it neglects the extra
information embedded in the temporal structure.
Recent studies have shown neurons in the vertebrate retina ﬁre with remarkable
temporal precision. In addition, temporal patterns in spatiotemporal spikes can carry
more information than the rate-based code [25–27]. Thus, temporal code serves as
an important component in neural system.

8
1
Introduction
Since the temporal code is more biologically plausible and computationally pow-
erful, a temporal framework is considered throughout this study.
1.4
Cognitive Learning and Memory in the Brain
In a biological nervous system, learning and memory are two indispensable com-
ponents of all mental processes, especially for cognition functions. Learning is the
acquisition of new knowledge or modiﬁcation of existing knowledge through study
and experience. And memory is the process in which information is encoded, stored,
and retrieved. Therefore, an intelligent machine is supposed to be able to obtain
knowledge from external stimulation and store them in the form of memory. When
encountering new problems, it would response relying on the stored knowledge.
1.4.1
Temporal Learning
Researchers have gone a long way to explore the secret of learning mechanisms in
the brain. In neuroscience, the learning process is found to be related to synaptic
plasticity, where the synaptic weights are adjusted along the learning.
In 1949, Donald Hebb introduced a basic mechanism that explained the adaptation
of neurons in the brain during the learning process [28]. It is called the Hebbian
learning rule, where a change in the strength of a connection is a function of the pre-
and post-synaptic neural activities. When neuron A repeatedly participates in ﬁring
neuron B, the synaptic weight from A to B will be increased.
The Hebbian mechanism has been the primary basis for learning rules in spiking
neural networks, though detailed processes of the learning occurring in biological
systems are still unclear. According to the schemes on how information is encoded
with spikes, learning rules in spiking neural networks can be generally assorted into
two categories: rate learning and temporal learning.
The rate learning algorithms, such as the spike-driven synaptic plasticity rule [29,
30], are developed for processing spikes presented in a rate-based framework, where
mean ﬁring rates of the spikes are used for carrying information. However, since
the rate learning algorithms are formulated in a rate-based framework, this group of
rules cannot be applied to process precise-time spike patterns.
To process spatiotemporal spike patterns with a temporal framework, the tem-
poral learning rule is developed. This kind of learning rule can be used to process
information that is encoded with a temporal code, where precise timing of spikes acts
as the information carrier. Development of the temporal learning rule is imperative
considering an increasing body of evidence supporting the temporal code.
Among various temporal rules, several rules have been widely studied, includ-
ing: spike-timing-dependent plasticity (STDP) [31, 32], the tempotron rule [33], the

1.4 Cognitive Learning and Memory in the Brain
9
SpikeProp rule [34], the SPAN rule [35], the Chronotron rule [36] and the ReSuMe
rule [37].
STDP is one of the most commonly and experimentally studied rules in recent
years. STDP is in agreement with Hebbs postulate because it reinforces the con-
nections with the pre-synaptic neurons that ﬁred slightly before the postsynaptic
neuron, which are those that took part in ﬁring it. STDP describes the learning
process depending on the precise spike timing, which is more biologically plausible.
The STDP modiﬁcation rule is shown as the following equation:
Δwi j =
⎧
⎪⎨
⎪⎩
A+ · exp(Δt
τ + )
: Δt ≤0
−A−· exp(−Δt
τ −) : Δt > 0
(1.2)
where Δt denotes the time difference between the pre- and post-synaptic spikes. A+,
A−and τ +, τ −are parameters of learning rates and time constants, respectively.
As is shown in Fig.1.3a, if pre-synaptic spike ﬁre before the post-synpatic spike,
long-term potentiation (LTP) will happen. Long-term depression (LTD) occurs when
the ﬁring order is reversed.
Figure1.3b shows that neurons equipped with STDP can automatically ﬁnd the
repeating pattern which is embedded in a background. The neuron will emit a spike
at the presence of this pattern [38–40].
However, STDP characterizes synaptic changes solely in terms of the temporal
contiguity of the pre-synaptic spike and the post-synaptic potential or spike. This is
not enough for learning spatiotemporal patterns since it would cause silent response
sometimes.
(a)
(b)
Fig. 1.3 Spike-Timing-Dependent Plasticity (STDP). a is a typical asymmetric learning window of
STDP. Pre-synaptic spike ﬁring before post-synaptic spike will cause long-term potentiation (LTP).
Long-term depression (LTD) occurs if the order of these two spikes is reversed. b shows the ability
of STDP to learn and detect repeating patterns that embedded in continuous spike trains. Shaded
areas denote the embedded repeating patterns, and the blue line shows the potential trace of the
neuron. Along the learning with STDP, the neuron gradually detects the target pattern by ﬁring a
spike

10
1
Introduction
The tempotron rule [33] is one such temporal learning rule where neurons are
trained to discriminate between two classes of spatiotemporal patterns. This learning
rule is based on a gradient descent approach. In the tempotron rule, the synaptic
plasticity is governed by the temporal contiguity of pre-synaptic spike, post-synaptic
depolarization and a supervisory signal. The neurons could be trained to successfully
distinguish two classes by ﬁring a spike or by remaining quiescent.
The tempotron rule is an efﬁcient rule for the classiﬁcation of spatiotemporal
patterns. However, the neurons do not learn to ﬁre at precise time. Since the tempotron
rule mainly aims at decision-making tasks, it cannot support the same coding scheme
used in both the input and output spikes. The time of the output spike seems to be
arbitrary, and does not carry information. To support the same coding scheme through
the input and output, a learning rule is needed to let the neuron not only ﬁre but also
ﬁre at the speciﬁed time. In addition, the tempotron rule is designed for a speciﬁc
neuron model, which might limit its usage on other spiking neuron models.
By contrast, the SpikeProp rule [34] can train neurons to perform a spatiotemporal
classiﬁcation by emitting single spikes at the desired ﬁring time. The SpikeProp rule
is a supervised learning rule for SNNs that based on gradient descent approach.
The major limitation is that the SpikeProp rule and its extension in [41] do not allow
multiple spikes in the output spike train. To solve this problem, several other temporal
learning rules, such as the SPAN rule, the Chronotron rule and the ReSuMe rule, have
been developed to train neurons to produce multiple output spikes in response to a
spatiotemporal stimulus.
In both the SPAN rule and the Chronotron E-learning rule, the synaptic weights
are modiﬁed according to a gradient descent approach in an error landscape. The
error function in the Chronotron rule is based on the Victor and Purpura distance
[42] in which the distance between two spike trains is deﬁned as the minimum cost
required to transform one into the other, while in the SPAN rule the error function
is based on a metric similar to the van Rossum metric [43] where spike trains are
converted into continuous time series for evaluating the difference. These arithmetic
calculations can easily reveal why and how networks with spiking neurons can be
trained, but the arithmetic-based rules are not a good choice for designing networks
with biological plausibility. The biological plausibility of error calculation is at least
questionable.

1.4 Cognitive Learning and Memory in the Brain
11
From the perspective of increased biological plausibility, the Chronotron
I-learningruleandtheReSuMeruleareconsidered.TheI-learningruleisheuristically
deﬁned in [36] where synaptic changes depend on the synaptic currents. According
to the I-learning rule, its development seems to be based on a particular case of the
Spike Response Model [4], which might also limit its usage on other spiking neu-
ron models or at least is not clearly demonstrated. Moreover, those synapses with
zero initial weights will never be updated according to the I-learning rule. This will
inevitably lead to information loss from those afferent neurons.
In view of the two aspects presented above, i.e., the biological plausibility and
the computational efﬁciency, it is important to combine the two aspects for a new
temporal learning rule and develop a comprehensive research framework within a
system where information is carried by precise-timing spikes.
1.4.2
Cognitive Memory in the Brain
From the perspective of psychology, memory can be generally divided into sensory
memory, short-term memory (STM) and long term memory (LTM). Memory models
proposed by psychologists provide abstract descriptions of how memory works. For
example,Atkinson-ShiffrinmodelsimpliﬁesthememorysystemasshowninFig.1.4.
In order to explore the memory function in biological systems, different parts of
biological nervous system have been studied [45, 46]. Researches on sensory system,
especially vision system, advance our understanding of sensory encoding. Indicated
by the study on patient H.M., the hippocampus is believed to be the most essential
part involved in consolidation of memory.
Fig. 1.4 Multi-store model
of Memory [44]
Sensory Memory 
(millisecond –1 second)
Short-term 
Memory
(< 1 minute)
Long-term 
Memory
(days, months, years)
retrieval
rehearsal

12
1
Introduction
DG
CA3
CA1
EC
Fig. 1.5 Diagram of the hippocampal circuit. Entorhinal cortex (EC), dentate gyrus (DG) and
Cornu Ammonis areas CA3 and CA1 are the main components of hippocampus
Real World
Stimulus
(Continuous Signal)
Neural Spikes
(Discrete Signal)
Sensory
Encoding
Learn/Test
Neural
 Patterns
Fig. 1.6 Information ﬂow in a typical neural network. Analogue stimulus is encoded by sensory
organs into neural signals and propagate in the neural network
The hippocampus is one of the most widely studied regions of the brain. It resides
within the medial temporal lobe of the brain and is believed to be responsible for
the storage of declarative memories. At the macroscopic level, highly processed
neocortical information from all sensory inputs converges onto the medial tempo-
ral lobe. These processed signals enter the hippocampus via the entorhinal cortex
(EC). Within the hippocampus, there are connections from the EC to all parts of the
hippocampus, including the dentate gyrus (DG), Cornu Ammonis areas CA3 and
CA1 through perforant pathway. Connections from the DG are connected to CA3
through mossy ﬁbers, from CA3 to CA1 through schaffer collaterals, and then from
CA1 back to EC. In addition, there are also strong recurrent connections within the
DG and CA3 regions. Figure1.5 depicts the overview of hippocampus based on its
anatomic structure.
A few computational models simulating different regions of the hippocampus
were proposed and studied [47–49]. Inspired by the structure of hippocampus, mem-
ory function have been demonstrated in these models. However, due to insufﬁcient
knowledge about mechanisms underlying neural computation in biological systems,
limited function of the brain can be artiﬁcially reproduced with current technology.
By simulating artiﬁcial neuron models, neural networks are inherently close to the
nature of biological nervous systems and possible to mimic their functions. Figure1.6
illustrates the information ﬂow in typical artiﬁcial neural networks. Similar to bio-
logical nervous systems, encoding and learning are the most important components
of a memory model using neural networks. Encoding deﬁnes the form of neural rep-
resentation of information, while learning plays a pivotal role in the development of
neural systems and formation of memory.

1.4 Cognitive Learning and Memory in the Brain
13
Due to their more biological realistic properties, spiking neural networks have
enhanced our understanding of information processing in the biological system and
advanced research of computational neuroscience over the past few decades. More-
over, increasing experimental ﬁndings in neurobiology and research achievements in
computational intelligence using spiking neural networks lead to growing research
interests in designing learning and memory models with spiking neurons.
1.5
Objectives and Contributions
Even though many attempts have been devoted to exploring mechanisms used in
the brain, a majority of facts about spiking neurons for information processing and
cognitive computing still remain unclear. The research gaps for current studies on
SNNs are summarized below:
1. Temporal coding and temporal learning are two of the major areas in SNNs.
Various mechanisms are proposed based on inspirations from biological obser-
vations. However, most studies on these two areas are independent. There are
few studies considering both the coding and the learning in a consistent system
[30, 34, 50–52].
2. Over the rate-based learning algorithms, the temporal learning algorithms are
developedforprocessingprecise-timingspikes.However,thesetemporallearning
algorithmsfocusmoreontheaspectsofeitherarithmeticorbiologicalplausibility.
Either side of these two aspects would not be a good choice considering both the
computational efﬁciency and the biological plausibility.
3. Currently, there are few studies on the practical applications of SNNs [30, 34,
51–53]. Most studies only focus on theoretical explorations of SNNs.
4. Learningmechanismsforbuildingcausalconnectionshavenotbeenclearlyinves-
tigated.
5. Research on memory models carried out so far mainly focuses on speciﬁc regions
of the brain, ignoring the underlying memory organization principle at a network
level.
This book mainly focuses on how to explore and develop cognitive computations
with spiking neurons under a temporal framework. The speciﬁc objectives covered
are as follows:
1. To develop integrated consistent systems of spiking neurons, where both the
coding and the learning are considered from a systematic level.
2. To develop a new temporal learning algorithm that is both simple for computation
and also biologically plausible.
3. To investigate various properties of the proposed algorithm, such as memory
capacity, robustness to noise and generality to different neuron models, etc.

14
1
Introduction
4. To investigate the ability of the proposed SNNs applying to different cognitive
tasks, such as image recognition, sound recognition and sequence recognition,
etc.
5. To investigate the temporal learning in multilayer spiking neural networks.
6. To develop computational neural network models emphasizing the generation
and organization of memory.
The signiﬁcance of this book is two-fold. On one hand, such models introduced
in the book may contribute to a better understanding of the mechanisms by which
the real brains operate. On the other hand, the computational models inspired from
biology are interesting in their own right, and could provide meaningful techniques
for developing real-world applications.
The computations of spiking neurons in this book are considered in a temporal
framework rather than a rate-based framework. This is because mounting evidence
shows that precise timing of individual spikes plays an important role. In addition, the
temporal framework offers signiﬁcant computational advantages than the rate-based
framework.
1.6
Outline of the Book
In the area of theoretical neuroscience, the general target is to provide a quantitative
basis for describing what nervous systems do, understanding how they function, and
uncovering the general principles by which they operate. It is a challenging area
since multidisciplinary knowledges are required for building models. Investigating
spike-based computation serves as a main focus in this book. To further specify
the research scope, the temporal framework is considered in this book. In order to
achieve the aforementioned objectives, a general system structure has been devised.
Further investigations on individual functional parts of the system are conducted.
The organization of the remaining chapters of this book is as follows.
Chapter 2 presents a brain-inspired spiking neural network system with sim-
ple temporal encoding and learning. With a biologically plausible supervised learn-
ing rule, the system is applied to various pattern recognition tasks. The proposed
approach is also benchmarked with the nonlinearly separable task. The encoding sys-
tem provides different levels of robustness, and enables the spiking neural networks
to process real-world stimuli, such as images and sounds. Detailed investigations on
the encoding and learning are also provided.
Chapter 3 introduces a computational model with spike-timing-based encoding
schemes and learning algorithms in order to bridge the gap between sensory encoding
and synaptic information processing. By treating sensory coding and learning as
a systematic process, the integrated model performs sensory neural encoding and
supervised learning with precisely timed spikes. We show that with a supervised
spike-timing-based learning, different spatiotemporal patterns can be recognized by
reproducing spike trains with a high time precision in milliseconds.

1.6 Outline of the Book
15
In Chap.4, a novel learning rule, namely Precise-Spike-Driven (PSD) synaptic
plasticity, is proposed for training the neuron to associate spatiotemporal spike pat-
terns. The PSD rule is simple, efﬁcient, and biologically plausible. Various properties
of this rule are investigated.
Chapter 5 presents the application of the PSD rule on sequence recognition. In
addition, the classiﬁcation ability of the PSD rule is investigated and benchmarked
against other learning rules.
In Chap.6, the learning in multilayer spiking neural networks is investigated.
Causalconnectionsarebuilttofacilitatethelearning.Severaltasksareusedtoanalyze
the learning performance of the multilayer network.
Chapter 7 describes a hierarchically organized memory model using spiking neu-
rons, in which temporal population codes are considered as the neural representation
of information and spike-timing-based learning methods are employed to train the
network. It has been demonstrated that neural cliques representing patterns are gen-
erated and input patterns are stored in the form of associative memory within gamma
cycles. Moreover, temporally separated patterns can be linked and compressed via
enhanced connections among neural groups forming episodic memory.
Finally, Chap.8 presents spiking neuron based cognitive memory models.
References
1. Adrian, E.: The Basis of Sensation: The Action of the Sense Organs. W. W. Norton, New York
(1928)
2. VanRullen, R., Guyonneau, R., Thorpe, S.J.: Spike times make sense. Trends Neurosci. 28(1),
1–4 (2005)
3. Kandel, E.R., Schwartz, J.H., Jessell, T.M., et al.: Principles of Neural Science, vol. 4. McGraw-
Hill, New York (2000)
4. Gerstner, W., Kistler, W.M.: Spiking Neuron Models: Single Neurons, Populations, Plasticity,
1st edn. Cambridge University Press, Cambridge (2002)
5. Vreeken, J.: Spiking neural networks, an introduction. Institute for Information and Computing
Sciences, Utrecht University Technical Report UU-CS-2003-008 (2002)
6. Maass, W., Schnitger, G., Sontag, E.D.: On the computational power of sigmoid versus boolean
threshold circuits. In: 32nd Annual Symposium on Foundations of Computer Science, 1991.
Proceedings, pp. 767–776. IEEE (1991)
7. Hopﬁeld, J.J., Brody, C.D.: What is a moment? transient synchrony as a collective mechanism
for spatiotemporal integration. Proc. Natl. Acad. Sci. 98(3), 1282–1287 (2001)
8. Hodgkin, A.L., Huxley, A.F.: A quantitative description of membrane current and its application
to conduction and excitation in nerve. J. Physiol. 117(4), 500–544 (1952)
9. Izhikevich, E.M.: Simple model of spiking neurons. IEEE Trans. Neural Netw. 14(6), 1569–
1572 (2003)
10. Gerstner, W., Kreiter, A.K., Markram, H., Herz, A.V.: Neural codes: ﬁring rates and beyond.
Proc. Natl. Acad. Sci. 94(24), 12740–12741 (1997)
11. Panzeri, S., Brunel, N., Logothetis, N.K., Kayser, C.: Sensory neural codes using multiplexed
temporal scales. Trends Neurosci. 33(3), 111–120 (2010)
12. Dayan, P., Abbott, L.: Theoretical neuroscience: computational and mathematical modeling of
neural systems. J. Cognit. Neurosci. 15(1), 154–155 (2003)

16
1
Introduction
13. Butts, D.A., Weng, C., Jin, J., Yeh, C.I., Lesica, N.A., Alonso, J.M., Stanley, G.B.: Temporal
precision in the neural code and the timescales of natural vision. Nature 449(7158), 92–95
(2007)
14. Bair, W., Koch, C.: Temporal precision of spike trains in extrastriate cortex of the behaving
macaque monkey. Neural Comput. 8(6), 1185–1202 (1996)
15. Berry, M.J., Meister, M.: Refractoriness and neural precision. J. Neurosci. 18(6), 2200–2211
(1998)
16. Uzzell, V.J., Chichilnisky, E.J.: Precision of spike trains in primate retinal ganglion cells. J.
Neurophysiol. 92(2), 780–789 (2004)
17. Reinagel, P., Reid, R.C.: Temporal coding of visual information in the thalamus. J. Neurophys-
iol. 20(14), 5392–5400 (2000)
18. Mainen, Z., Sejnowski, T.: Reliability of spike timing in neocortical neurons. Science
268(5216), 1503–1506 (1995)
19. Gabbiani, F., Metzner, W., Wessel, R., Koch, C.: From stimulus encoding to feature extraction
in weakly electric ﬁsh. Nature 384(6609), 564–567 (1996)
20. Wehr, M., Zador, A.M.: Balanced inhibition underlies tuning and sharpens spike timing in
auditory cortex. Nature 426(6965), 442–446 (2003)
21. Maass, W., Bishop, C.M.: Pulsed Neural Networks. MIT Press (2001)
22. Serre, T., Oliva, A., Poggio, T.: A feedforward architecture accounts for rapid categorization.
Proc. Natl. Acad. Sci. 104(15), 6424–6429 (2007)
23. Gollisch, T., Meister, M.: Rapid neural coding in the retina with relative spike latencies. Science
319(5866), 1108–1111 (2008)
24. Nadasdy, Z.: Information encoding and reconstruction from the phase of action potentials.
Front. Syst. Neurosci. 3, 6 (2009)
25. Kempter, R., Gerstner, W., van Hemmen, J.L.: Spike-based compared to rate-based Hebbian
learning. In: NIPS’98, pp. 125–131 (1998)
26. Borst, A., Theunissen, F.E.: Information theory and neural coding. Nat. Neurosci. 2(11), 947–
957 (1999)
27. Hopﬁeld, J.J.: Pattern recognition computation using action potential timing for stimulus rep-
resentation. Nature 376(6535), 33–36 (1995)
28. Hebb, D.: The Organization of Behavior: A Neuropsychological Theory. Taylor & Francis
Group (2002)
29. Fusi, S.: Spike-driven synaptic plasticity for learning correlated patterns of mean ﬁring rates.
Rev. Neurosci. 14(1–2), 73–84 (2003)
30. Brader, J.M., Senn, W., Fusi, S.: Learning real-world stimuli in a neural network with spike-
driven synaptic dynamics. Neural Comput. 19(11), 2881–2912 (2007)
31. Bi, G.Q., Poo, M.M.: Synaptic modiﬁcation by correlated activity: Hebb’s postulate revisited.
Ann. Rev. Neurosci. 24, 139–166 (2001)
32. Froemke, R.C., Poo, M.M., Dan, Y.: Spike-timing-dependent synaptic plasticity depends on
dendritic location. Nature 434(7030), 221–225 (2005)
33. Gütig, R., Sompolinsky, H.: The tempotron: a neuron that learns spike timing-based decisions.
Nat. Neurosci. 9(3), 420–428 (2006)
34. Bohte, S.M., Kok, J.N., Poutré, J.A.L.: Error-backpropagation in temporally encoded networks
of spiking neurons. Neurocomputing 48(1–4), 17–37 (2002)
35. Mohemmed, A., Schliebs, S., Matsuda, S., Kasabov, N.: SPAN: spike pattern association neuron
for learning spatio-temporal spike patterns. Int. J. Neural Syst. 22(04), 1250,012 (2012)
36. Florian, R.V.: The Chronotron: a neuron that learns to ﬁre temporally precise spike patterns.
PLoS One 7(8), e40,233 (2012)
37. Ponulak, F.: ReSuMe-new supervised learning method for spiking neural networks. Institute
of Control and Information Engineering, Pozno´n University of Technology, Tech. rep. (2005)
38. Guyonneau, R., van Rullen, R., Thorpe, S.J.: Neurons tune to the earliest spikes through STDP.
Neural Comput. 17(4), 859–879 (2005)
39. Masquelier, T., Guyonneau, R., Thorpe, S.J.: Spike timing dependent plasticity ﬁnds the start
of repeating patterns in continuous spike trains. PloS One 3(1), e1377 (2008)

References
17
40. Masquelier, T., Guyonneau, R., Thorpe, S.J.: Competitive stdp-based spike pattern learning.
Neural Comput. 21(5), 1259–1276 (2009)
41. Booij, O., et al.: A gradient descent rule for spiking neurons emitting multiple spikes. Inf.
Process. Lett. 95(6), 552–558 (2005)
42. Victor, J.D., Purpura, K.P.: Metric-space analysis of spike trains: theory, algorithms and appli-
cation. Netw.: Comput. Neural Syst. 8(2), 127–164 (1997)
43. Van Rossum, M.C., Bi, G., Turrigiano, G.: Stable Hebbian learning from spike timing-
dependent plasticity. J. Neurosci. 20(23), 8812–8821 (2000)
44. Atkinson, R.C., Shiffrin, R.M.: Human memory: A proposed system and its control processes.
In: Spence, K.W., Spence, J.T. (eds.) The Psychology of Learning and Motivation: Advances
in Research and Theory, vol. 2, pp. 89–105 (1968)
45. Hawkins, J., Blakeslee, S.: On Intelligence. Macmillan (2004)
46. He, H.: Self-adaptive Systems for Machine Intelligence. Wiley (2011)
47. Jensen, O., Lisman, J.E.: Hippocampal sequence-encoding driven by a cortical multi-item
working memory buffer. Trends Neurosci. 28(2), 67–72 (2005)
48. Kunec, S., Hasselmo, M.E., Kopell, N.: Encoding and retrieval in the ca3 region of the hip-
pocampus: a model of theta-phase separation. J. Neurophysiol. 94(1), 70–82 (2005)
49. Cutsuridis, V., Wennekers, T.: Hippocampus, microcircuits and associative memory. Neural
Netw. 22(8), 1120–1128 (2009)
50. Dennis, J., Yu, Q., Tang, H., Tran, H.D., Li, H.: Temporal coding of local spectrogram features
for robust sound recognition. In: 2013 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 803–807 (2013)
51. Yu, Q., Tang, H., Tan, K.C., Li, H.: Rapid feedforward computation by temporal encoding
and learning with spiking neurons. IEEE Trans. Neural Netw. Learn. Syst. 24(10), 1539–1552
(2013)
52. Wade, J.J., McDaid, L.J., Santos, J.A., Sayers, H.M.: SWAT: a spiking neural network training
algorithm for classiﬁcation problems. IEEE Trans. Neural Netw. 21(11), 1817–1830 (2010)
53. Ponulak, F., Kasinski, A.: Supervised learning in spiking neural networks with resume:
sequence learning, classiﬁcation, and spike shifting. Neural Comput. 22(2), 467–510 (2010)

Chapter 2
Rapid Feedforward Computation
by Temporal Encoding and Learning
with Spiking Neurons
Abstract As we know, primates perform remarkably well in cognitive tasks such as
pattern recognition. Motivated from recent ﬁndings in biological systems, a uniﬁed
and consistent feedforward system network with a proper encoding scheme and
supervised temporal rules is built for processing real-world stimuli. The temporal
rules are used for processing the spatiotemporal patterns. To utilize these rules on
images or sounds, a proper encoding method and a uniﬁed computational model with
consistent and efﬁcient learning rule are required. Through encoding, external stimuli
are converted into sparse representations which also have properties of invariance.
These temporal patterns are then learned through biologically derived algorithms in
the learning layer, followed by the ﬁnal decision presented through the readout layer.
The performance of the model is also analyzed and discussed. This chapter presents
a general structure of SNN for pattern recognition, showing that the SNN has the
ability to learn the real-world stimuli.
2.1
Introduction
Primates are remarkably good at cognitive skills such as pattern recognition. Despite
decades of engineering effort, the performance of the biological visual system still
outperforms the best computer vision systems. Pattern recognition is a general task
that assigns an output value to a given input pattern. It is an information-reduction
process which aims to classify patterns based on a priori knowledge or statistical
information extracted from the patterns. Typical applications of pattern recogni-
tion includes automatic speech recognition, handwritten postal codes recognition,
face recognition and gesture recognition. There are several conventional methods
to implement pattern recognition, such as maximum entropy classiﬁer, Naive Bayes
classiﬁer, decision trees, support vector machines (SVM) and perceptrons. We refer
these methods as traditional rules since they are less biologically plausible compared
to spiking time involved rules described later. Compared to human brain, these meth-
ods are far from reaching comparable recognition. Humans can easily discriminate
different categories within a very short time. This motivates us to investigate com-
putational models for rapid and robust pattern recognition from a biological point of
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_2
19

20
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
view. At the same time, inspired by biological ﬁndings, researchers have come up
with different theories of encoding and learning. In order to bridge the gap between
those independent studies, a uniﬁed systematic model with consistent rules is desired.
A simple feedforward architecture might account for rapid recognition as reported
recently [1]. Anatomical back projections abundantly appear almost every area in
the visual cortex, which puts the feedforward architecture under debate. However,
the observation of a quick response appeared in inferotemporal cortex (IT) [2] most
directly supports the hypothesis of the feedforward structure. The activity of neurons
in monkey IT appears quite soon (around 100ms) after stimulus onset [3]. For the
purpose of rapid recognition, a core feedforward architecture might be a reasonable
theory of visual computation.
How information is represented in the brain still remains unclear. However, there
are strong reasons to believe that using pulses is the optimal way to encode the infor-
mation for transmission [4]. Increasing number of observations show that neurons in
the brain precisely response to a stimulus. This support the hypothesis of the temporal
coding.
There are many temporal learning rules proposed for processing spatiotempo-
ral patterns, including both supervised and unsupervised rules. As opposed to the
unsupervised rule, a supervised one could potentially facilitate the learning speed
with the help of an instructor signal. Although so far there is no strong experimental
conﬁrmation of the supervisory signal, an increasing body of evidence shows that
this kind of learning is also exploited by the brain [5].
Learning schemes focusing on processing spatiotemporal spikes in a supervised
manner have been widely studied. With proper encoding methods, these schemes
could be applied to image categorization. In [6], the spike-driven synaptic plasticity
mechanism is used to learn patterns encoded by mean ﬁring rates. A rate coding is
used to encode images for categorization. The learning process is supervised and
stochastic, in which a teacher signal steers the output neuron to a desired ﬁring
rate. According to this algorithm, synaptic weights are modiﬁed upon the arrival of
pre-synaptic spikes, considering the state of post-synaptic neuron’s potential and its
recent ﬁring activity. One of the major limitations of this algorithm is that it could
not be used to learn patterns presented in the form of precise timing spikes. Different
from the spike-driven synaptic plasticity, the tempotron learning rule [7] is efﬁcient
to learn spike patterns in which information is embedded in precise timing spikes
as well as in mean ﬁring rates. This learning rule modiﬁes the synaptic weights
such that a trained neuron ﬁres once for patterns of corresponding category and
keeps silent for patterns of other categories. The ReSuMe learning rule [8, 9] is
also a supervised rule in which the trained neuron can ﬁre at desired times when
corresponding spatiotemporal patterns are presented. It has been demonstrated that
the tempotron rule and the ReSuMe rule are equivalent under certain conditions [10].
Although SNNs show promising capabilities in achieving a performance similar
to living brains due to their more faithful similarity to biological neural networks,
one of the main challenges of dealing with SNNs is getting data into and out of
them, which requires proper encoding and decoding methods. The temporal learning
algorithms are based on spatiotemporal spike patterns. However, the problem remains

2.1 Introduction
21
how to represent real-world stimuli (like images) by spatiotemporal spikes for further
computation in the spiking network. To deal with these problems, a uniﬁed systematic
model, with consistent encoding, learning and readout, is required.
The main contribution of this chapter lies in the design of a uniﬁed system model
of spiking neural network for solving pattern recognition problems. To the best of our
knowledge,thisistheﬁrstworkinwhichcomplexclassiﬁcationtaskissolvedthrough
combination of biologically plausible encoding and supervised temporal learning.
The system contains consistent encoding, learning and readout parts. Through the
network, we ﬁll the gap between real-world problem (image encoding) and theoret-
ical studies of different learning algorithms for spatiotemporal patterns. Finally, our
approach suggests a plausibility proof for a class of feedforward models of rapid and
robust recognition in the brain.
2.2
The Spiking Neural Network
In this section, the feedforward computational model for pattern recognition is
described. The model composes of 3 functional parts: the encoding part, the learning
part and the readout part. Figure2.1 shows the general architecture of the system
model.
Considering the encoding, the latency code is a simple example of temporal cod-
ing. It encodes information in the timing of response relative to the encoding win-
dow, which is usually deﬁned with respect to the stimulus onset. The external stimuli
would trigger neurons to ﬁre several spikes in different times. From biological obser-
vations, visual system can analyze a new complex scene in less than 150ms [11, 12].
This period of time is impressive for information processing considering billions of
Fig. 2.1 A schematic of the
feedforward computational
model for pattern
recognition. It contains three
functional parts: encoding,
learning and readout. A
stimulus is converted into
spatiotemporal spikes by the
encoding neurons. This
spiking pattern is passed to
the next layer for learning.
The ﬁnal decision is
represented by the readout
layer

22
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
neurons involved. This suggests that neurons exchange only one or few spikes. In
addition, it is shown that subsequent brain region may learn more and earlier about
the stimuli from the time of ﬁrst spike than from the ﬁring rate [11].
Therefore, we use single spike code as the encoding mechanism. Within the encod-
ing window, each input neuron ﬁres only once. This code is simple and efﬁcient, and
the capability of encoding information in the timing of single spikes to compute
and learn realistic data has been shown in [13]. Compared to rate coding as used in
[6], this single spike coding would potentially facilitate computing speed since less
spikes are involved in the computation.
Our single spike coding is similar to the rank order coding in [14, 15] but taking
into consideration of the precise latency of the spikes. In the rank order coding,
the rank order of neurons’ activations is used to represent the information. This
coding scheme is still under research. Taking the actual neurons’ activations into
consideration but not their rank orders, our proposed encoding method could convey
more information than the rank order coding. Since this coding utilizes only a single
spike to transmit information, it could also potentially be beneﬁcial for efﬁcient very
large scale integration (VLSI) implementations.
In the learning layer, supervised rules are used since they could improve the
learning speed with the help of the instructor signal. In this chapter, we investigate
the tempotron rule and the ReSuMe rule.
The aim of the readout part is to extract information about the stimulus from the
responses of learning neurons. As an example, we could use a binary sequence to
represent a certain class of patterns in the case that each learning neuron can only
discriminate two groups. Each learning neuron responds to a stimulus by ﬁring (1)
or not ﬁring (0). Thus, the total N learning neurons as the output can represent a
maximum number of 2N classes of patterns.
A more suitable scheme for readout would be using population response. In this
scheme, several groups are used and each group, containing several neurons, is one
particular representation of the external stimuli. Different groups compete with each
other by a voting scheme in which the group with the most amount of ﬁring neurons
would be the winner. This scheme is more compatible with the real brain since the
information is presented by the cooperation of a group of neurons rather than one
single neuron [16].
2.3
Single-Spike Temporal Coding
We have mentioned the function of the encoding layer is to convert stimulus into
spatiotemporal spikes. In this section, we illustrate our encoding model of single-
spike temporal coding, which is inspired from biological agents.
The retina is a particular interesting sensory area to study neural information
processing, since its general structure and functional organization are remarkably
well known. It is widely believed that information transmitted from retina to brain
codes the intensity of the visual stimuli at every place in visual ﬁeld. The ganglion

2.3 Single-Spike Temporal Coding
23
cells (GCs) collect the information from their receptive ﬁelds which could best drive
spiking responses [17]. In addition, different ganglion cells might have overlapped
centers of receptive ﬁelds [18]. A simple encoding model of retina is described in
[14] and is used in [15]. The GCs are used as the ﬁrst layer in our model to collect
information from original stimuli.
Focusing on emulating the processing in visual cortex, a realistic model (HMAX)
for recognition has been proposed in [19] and widely studied [1, 20, 21]. It is a
hierarchical system that closely follows the organization of visual cortex. The HMAX
performs remarkably well with natural images by using alternate simple cells (S) and
complex cells (C). Simple cells (S) gain their selectivity from a linear sum operation,
while complex cells (C) gain invariance through a nonlinear max pooling operation.
Like the HMAX model, in order to obtain an invariant encoding model to some
extent, a complex cells (CCs) layer is used in our model. In the brain, equivalents of
CCs may be in V1 and V4 (see [22] for more details).
In our model (see Fig.2.2), the image information (intensity) is transmitted to GCs
through photo-receptors. Each GC linearly integrates at its soma the information from
its receptive ﬁeld. Their receptive ﬁelds are overlapping and their scales are generally
distributed non-uniformly over the visual ﬁeld. DoG (difference of gaussian) ﬁlters
are used in the GCs layer since this ﬁlter is believed to mimic how neural processing
in the retina of the eye extracts details from external stimuli [23, 24]. Several differ-
ent scales of DoG would construct different GCs images. The CCs unit would oper-
Fig. 2.2 Architecture of the visual encoding model. A gray-scale image (as the stimuli) is presented
to the encoding layer. The photo-receptors transmit the image information analogically and linearly
to the corresponding ganglion cells (GCs). Each ganglion cell collects information from its receptive
ﬁeld (an example shown as the red dashed box). There are several layers of GCs and each has a
different scale of receptive ﬁeld. The complex cells (CCs) collect information from a local position
of GCs and a MAX operation among these GCs determines the activation value of CC unit. Each
CC neuron would ﬁre a spike according to their activations. These spikes are transmitted to the next
layer as the spatiotemporal pattern in particular time window (T)

24
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
ate a nonlinear max pooling to obtain an amount of invariance. Max pooling over the
two polarities, different scales and different local positions provides contrast reverse
invariance, scale invariance and position invariance, respectively. Biophysically plau-
sible implementations of the MAX operation have been proposed in [25], and biologi-
cal evidences of neuron performing MAX-like behavior have been found in a subclass
of complex cells in V1 [26] and cells in V4 [27].
TheactivationvalueofCCunitwouldtriggeraﬁringspike.StronglyactivatedCCs
will ﬁre earlier, whereas weakly activated will ﬁre later or not at all. The activation
of the GC is computed through the dot product as:
GCi :=< I, φi >=

l∈Ri
I (l) · φi(l)
(2.1)
where I (l) is the luminance of pixel l which is sensed by the photo-receptor. Ri is
the receptive ﬁeld region of neuron i. φi is the weight of the ﬁlter.
The GCs compute the local contrast intensities at different spatial scales and for
two different polarities: ON- and OFF-center ﬁlters. We use the simple DoG as our
ﬁlter where the surround has three times the width of the center. The DoG has the
form as:
DoG{s,lc}(l) = Gσ(s)(l −lc) −G3·σ(s)(l −lc)
(2.2)
Gσ(s)(l) =
1
2π · σ(s)2 · exp(−
∥l∥2
2 · σ(s)2 )
(2.3)
where Gσ(s) is the 2D Gaussian function with variance σ(s) which depends on the
scale s. lc is the center position of the ﬁlter.
An example of the DoG ﬁlter is shown in Fig.2.3. An OFF-center ﬁlter is simply
an inverted version of an ON-center receptive ﬁeld. All the ﬁlters are sum-normalized
to zero and square-normalized to one so that when there is no contrast change in the
image the neuron’s activation would be zero and when the image is same with the
ﬁlter the neuron’s activation would be 1. Therefore, all the activations of the GCs are
scaled to the same range ([−1, 1]).
The CCs max over different polarities according to their absolute values at same
scale and same position. Through this max operation, the model gain a contrast
reverse invariance (Fig.2.4a). From the property of the polar ﬁlters, only one could
be positive activated for a given image. Similarly, the scale invariance is increased
by max pooling over different scales at the same position (Fig.2.4b). Finally, the
position invariance is increased by pooling over different local positions (Fig.2.4c).
The dimension of images is reduced since only the max activated value in a local
position is preserved.
Figure2.5 shows the basic processing procedures in different encoding layers.
Through the encoding, the original image is sparsely presented in the CCs (Fig.2.5f).
The ﬁnal activations of CCs are used to produce spikes. Strongly activated neurons
would ﬁre earlier, whereas weakly activated ones would ﬁre later or not at all. The

2.3 Single-Spike Temporal Coding
25
Fig. 2.3 Linear ﬁlters in
retina. a is an image of the
ON-center DoG ﬁlter,
whereas (b) is an image of
the OFF-Center ﬁlter. c is the
one-dimensional show of the
DoG weights and (d) is the
2-dimensional show
(a)
(b)
(d)
(c)
Fig. 2.4 Illustration of
invariance gained from max
pooling operation. a the
contrast reverse invariance
by max pooling over
polarities. b the scale
invariance by max pooling
over different scales. c the
local position invariance by
max pooling over local
positions. The red circle
denotes the maximally
activated one
(a)
(b)
(c)
spike latencies are then linearly mapped into a predeﬁned encoding time window.
These spatiotemporal spikes are transmitted to the next layer for computation.
In our encoding scheme, we consider the actual values of neurons’ activations to
generate spikes but not the rank order of these activations as used in [14, 15]. This
could carry more information than the rank order coding which only considers the
rank order of different activations and ignores the exact differences between different
activations. For example, there are 3 neurons (n1, n2 and n3) having their activations
(C1, C2 and C3) in the range of [0, 1]. Pattern P1 is represented by (C1 = 0.1,
C2 = 0.3 and C3 = 0.9); pattern P2 is represented by (C1 = 0.29, C2 = 0.3 and
C3 = 0.32). In rank order coding, it will treat P1 and P2 as same patterns since the

26
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
Fig. 2.5 Illustration of the processing results in different encoding procedures. a is the original
external stimulus. b and c are the processing results in layer GCs with different scales. d, e and f
are the processes in the CCs layer. d is the result of max pooling over different scales. e is max
pooling over different local positions. f is the sub-sample from (e)
rank orders are same. For our encoding, in contrast, P1 and P2 would be treated as
totally different patterns. In addition, the rank order coding would be very sensitive
to the noise since the encoding time of one neuron depends on other neurons’ rank.
For example, if the least activated value is changed to a max activated value because
of a disturbance, the rank of all the other neurons would be changed. However in our
proposed algorithm only the information of the disturbed neuron would be affected.
2.4
Temporal Learning Rule
Temporal learning rule aims at dealing with information encoded by precise timing
spikes. In this section, we consider supervised mechanisms like the tempotron rule
and the ReSuMe rule that could be used for training neurons to discriminate between
different spike patterns. Whether a LTP or LTD process occurs depends on the super-
visory signal and the neuron’s activity. This kind of supervisory signal can facilitate
the learning speed compared to the unsupervised method.
2.4.1
The Tempotron Rule
In [7], the tempotron learning rule is proposed. According to this rule, the synaptic
plasticity is governed by the temporal contiguity of a presynaptic spike and a postsy-
naptic depolarization, and a supervisory signal. The tempotron can make appropriate
decisions under a supervisory signal by tuning fewer parameters.
In binary classiﬁcation problem, each input pattern presented to the neuron
belongs to one of two classes (which are labeled by P+ and P−). One neuron can
make decision by ﬁring or not. When a P+ pattern is presented to the neuron, it

2.4 Temporal Learning Rule
27
should elicit a spike; when a P−pattern is presented, it should keep silent by not
ﬁring. The tempotron rule modiﬁes the synaptic weights (wi) whenever there is an
error. This rule performs like gradient-descent rule that minimizes a cost function as:
C =
 Vthr −Vtmax, if the presented pattern is P+;
Vtmax −Vthr, if the presented pattern is P−.
(2.4)
where Vtmax is the maximal value of the post-synaptic potential V .
Applying the gradient descent method to minimize the cost leads to the tempotron
learning rule:
Δwi =
⎧
⎨
⎩
λ 
ti<tmax K(tmax −ti), if P+ error;
−λ 
ti<tmax K(tmax −ti), if P−error;
0, otherwise.
(2.5)
where tmax denotes the time at which the neuron reaches its maximum potential value
in the time domain. λ > 0 is a constant representing the learning rate. It denotes
the maximum change on the synaptic efﬁcacies. P+ error denotes that the neuron
should ﬁre but it did not; P−error denotes that the neuron should not ﬁre but it
did. According to the kernel shape, the efﬁcacies of afferents that spike near to tmax
change more than that are away from it. In this rule, tmax is the reference time for
updating synaptic weights.
2.4.2
The ReSuMe Rule
The ReSuMe described in [9] is a supervised method that aims to produce desired
spike trains in response to the given input sequence. According to this rule, the
synaptic weights are modiﬁed according to the following equation:
dωi(t)
dt
= λ[Sd(t) −Sout(t)][a +
 ∞
0
W(s)Si(t −s)ds]
(2.6)
where λ is the learning rate, a is a constant, W is a learning window with a exponential
form (W(s) = Ae−s/τE). Sd(t), Sout(t) and Si(t) are the target, post- and pre-synaptic
spike trains, respectively. Although the shape of learning window is not restricted to
exponential form, this shape can result in a better performance of convergence [28].
The spike trains have the following form:
S(t) =
n

f =1
δ(t −t f )
(2.7)

28
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
(a)
(b)
(c)
Fig. 2.6 Illustration of the ReSuMe learning rule. a demonstrates that the synaptic plasticity
depends on the correlation between the pre- and postsynaptic ﬁring times, and on the correla-
tion between pre- and desired ﬁring times. b demonstrates that the synaptic weight is potentiated
whenever a desired spike is observed. c shows that the synaptic weight is depressed whenever the
trained neuron ﬁres. This ﬁgure is revised from [8]
where t f denotes the moment of the f -th spike in the train, n denotes the total
number of spikes in the train, δ(x) is the impulse function δ(x) = 1 if x = 0 (or 0
otherwise).
Figure2.6 illustrates the ReSuMe learning rule. The synaptic efﬁcacy depends not
only on the correlation between the pre-synaptic and post-synaptic ﬁring times but
also on the correlation between the pre-synaptic and desired ﬁring times. A desired
spike would result in synaptic potentiation, and a post-synaptic spike would result
in synaptic depression.
After a learning trial, the total synaptic change is:
Δωi = λa(nd −nout) + λ
td
ti≤td W(td −ti)
(2.8)
−λ
tout
ti≤tout W(tout −ti)
where nd and nout are the number of spikes from the desired and the actual output
spike trains respectively. ti is the pre-synaptic spike time.
The ReSuMe rule could be used for both the batch learning and the online learning.
2.4.3
The Tempotron-Like ReSuMe Rule
As proposed in [10], the tempotron learning rule is a particular case of ReSuMe
rule under certain conditions. The rule discussed here is a connection between the
tempotron rule and the ReSuMe rule.
Considering to apply ReSuMe to the tempotron setup, the combined rule can be
approached. The neuron is only allowed to ﬁre once or not. After a spike is emitted,
the neuron shunts all its incoming spikes immediately. If there is only one spike,

2.4 Temporal Learning Rule
29
regardless of its time, it is reasonable to consider the neuron ﬁring at tmax. This
learning rule follows [10]:
Δwi =
⎧
⎨
⎩
λa + λ 
ti≤tmax W(tmax −ti), if nd = 1, nout = 0;
−λa −λ 
ti<tout W(tout −ti), if nd = 0, nout = 1;
0,
if nd = nout.
(2.9)
When a = 0 and W(s) = K(s), the combined rule is equivalent to the tempotron
learning rule. This implicates that the tempotron rule is a particular case of the
ReSuMe rule.
2.5
Simulation Results
In this section, several simulations are performed to test the performance of the
network and different learning rules.
2.5.1
The Data Set and the Classiﬁcation Problem
The stimuli from real world typically have a complex statistical structure. It is quite
different from idealized case of random patterns often considered. In the real world,
the stimuli hold large variability in a given class and have a high level of correlation
between members of different classes. The data set we considered here is the MNIST
digits (see Fig.2.7).
The MNIST data set contains a large number of examples of hand-written digits,
which consists of ten classes (digits 0 to 9) of examples and each example is an
image of 28 × 28 pixels. The MNIST data set is available from http://yann.lecun.
com/exdb/mnist, where many classiﬁcation results from different methods are also
listed. All images from this data set are gray-scale.
Fig. 2.7 Examples of
handwritten digits from
MNIST dataset

30
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
2.5.2
Encoding Images
Each image is presented to the encoding layer, and is then converted into spatiotem-
poral pattern. We use the coding strategy discussed previously through which the
output is sparse, as is observed in biological agents [29].
For simplicity of applying the encoding algorithm to the data set, we distribute
GCs with different receptive ﬁelds all over the image (each pixel). The image size
in GCs is same as the input image. Considering examples of 28-by-28 images, we
choose two scales for the ﬁlters (σ = 1 for 5 × 5 pixels as scale 1, and σ = 2 for
7 × 7 pixels as scale 2). The CCs layer performs the max pooling operation on the
previous GCs layer. For local position operation we choose 6 × 6 pixels and we set
the overlap pixels to be 3in one axis (x or y) for sub-sampling operation. A detailed
process of max operation is described in [19].
The application of all these processes produces a set of analog values, corre-
sponding to the activation levels of our CCs unit. The strongly activated cell will ﬁre
earlier, whereas the weakly activated will ﬁre later or not at all. The spike latencies
are linearly mapped into a predeﬁned encoding time window (100ms in this study).
The activation values are linearly converted to delay times, associating t = 0 with
activation value 1 and later times up to 100ms with lower activation values. The
neurons with activation value of 0 (or below a chosen small value) will not ﬁre due
to the weak activation.
An illustration of encoding an image is shown in Fig.2.5. Our scheme is to extract
the basic information and encode it to a spatiotemporal spike pattern. Through the
whole encoding structure, a sparse representation of the original incoming image is
ﬁnally obtained. Using this sparse representation to generate the spike pattern would,
to some extent, be compatible with biological observations in retina.
2.5.3
Choosing Among Temporal Learning Rules
In the tempotron rule, we specify the following parameters. The ratio between the
membrane and the synaptic constants is ﬁxed at τm/τs = 4. The threshold Vthr is set
to 1 and Vrest is set to 0. We use τm = 10ms and λ = 0.002.
For comparison purpose, in the ReSuMe we use the similar neuron model as the
one in the tempotron rule. However, the difference is that when the neuron emits a
spike, its potential is reset to a rest value (0 here) and is hold there for a refractory
period (3ms here).
Since the ReSuMe rule is based only on the spiking times, it could work indepen-
dently on the used spiking neuron models [9]. To verify the suitability of this rule
for our chosen neuron model, we generate a spike pattern and force the neuron to
respond at desired times. We choose 300 afferent synapses and each ﬁres only once
in the time window. The timing of each spike is generated randomly with uniform

2.5 Simulation Results
31
Fig.2.8 IllustrationofthesuitabilityofReSuMeruleforthechosenneuronmodel.Theinputpattern
contains 300 afferent synapses and each ﬁres once only. These spikes are generated randomly with
uniform distribution. For the desired spike, 3 random spiking times are chosen
(a)
(b)
(c)
Fig. 2.9 The number of iterations needed for the correct classiﬁcation of spike patterns, through
different learning rules. a is the tempotron learning rule. b is the tempotron-like ReSuMe rule. c is
the ReSuMe rule in which if the neuron ﬁres, it should spike at a desired time. Over 100 experiments
with different initial conditions, the averages (4.95, 7.36 and 14.48) and standard deviations (0.8454,
1.7438 and 12.014) are obtained for (a), (b) and (c), respectively
distribution between 0 and T . After learning, the neuron could perform as the desired
way (see Fig.2.8).
To compare the learning speed of different learning rules, we generate 30 spa-
tiotemporal patterns and each pattern contains 120 afferent synapses. The spiking
times are generated randomly with a uniform distribution between 0 and T . We ran-
domly choose 3 patterns as one category that is needed to be discriminated from
others. We record the minimum times of iterations for different rules to learn these
patterns correctly. We perform this experiment for 100 times and the results are
shown in Fig.2.9.
According to Fig.2.9, there is no signiﬁcant difference of learning speed between
the tempotron rule and tempotron-like ReSuMe rule. This is because the only differ-
ence between these two rules is the kernel windows which have a similar effect on

32
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
the synaptic change. However, compared to the ReSuMe rule, the tempotron rule is
much faster (about 3 times as the ReSuMe rule). Besides this, the learning speed of
the ReSuMe varies signiﬁcantly for different initial conditions (such as the number
of patterns, the initial weights and the learning rate). For the sake of fast recognition,
we choose the tempotron rule as our learning rule.
2.5.4
The Properties of Tempotron Rule
Since the tempotron rule is chosen, a test on its properties is needed.
2.5.4.1
Capacity
As is used for perceptron [30], the ratio of the number of random patterns (Np) that
correctly classiﬁed by the neuron over the number of its synapses (Nin), α = Np/Nin,
is used to measure the load of the neuron. An important characteristic of neuron’s
capacity is the maximum load that it can learn. As studied in [7], the maximum
recognition load of a tempotron can reach 3 approximately, which means that the
number of patterns the neuron can learn could roughly approach to 3 times the number
of synapses connected to it.
For our chosen neuron, a test on its load is shown in Fig.2.10. We set Nin = 100
and generate different number of spike patterns within a ﬁxed time window (T =
100ms). Each afferent ﬁres only once and the spiking time is randomly chosen from
uniform distribution within T . The mean number of cycles of pattern presentations
for error-free classiﬁcation is shown versus the load (α). Although a more robust
estimation of the load is feasible by allowing a small percentage of false alarms, the
Fig. 2.10 The mean number
of iterations of pattern
presentations for error-free
classiﬁcation versus neuron
load. The patterns are
randomly generated within
the ﬁxed time window
(100ms). The number of
synapses is 100. Data are
averaged over 20 runs
0.5
1
1.5
2
2.5
0
200
400
600
800
Iterations
Load (α)

2.5 Simulation Results
33
rigorous condition of error-free classiﬁcation is useful to testify the neuron’s ability
of classifying all assigned patterns successfully.
According to Fig.2.10, the neuron could successfully learn the patterns within
several tens of iterations if the load is not very high (below 1.5), but the number of
iterations would increase sharply when the load is over 1.5. This means that under a
higher load the neuron needs more time to learn the patterns or the learning process
could never converge.
This load test, to some extent, could guarantee the learning convergence when the
tempotron neuron is applied to our chosen recognition task. In our task, there are only
ten categories and patterns in each category share some common features. Compared
to the randomly generated patterns, the neuron’s capacity might be sufﬁcient to learn
these real-world stimuli.
2.5.4.2
Robustness
In some cases, the external noise might change the encoded spike patterns more
or less. The tempotron rule should hold some level of robustness to tolerate the
noise. To assess the robustness of the learning rule, we trained the neuron with a
number of patterns (α = 1). Then we tested the performance of the neuron when
facing with jittered versions of previous learned patterns. The jittered pattern was
generated by adding a Gaussian noise to all spike times of a template pattern. The
robust performance of the neuron is shown in Fig.2.11.
According to Fig.2.11, the performance of correct recognition decreases with
increasing jitter. Within a limited jitter range (0–3ms), the performance stays in
a relatively high level (over 0.8). This indicates the learning rule is robust to the
presence of temporal noise to some extent.
Fig. 2.11 The mean correct
rate of classiﬁcation on
jittered spike patterns. The
jittered pattern is generated
by adding Gaussian noise
with standard deviation to all
spike times of a template
pattern
0
2
4
6
8
10
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Standard Deviation (ms)
Correct Rate

34
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
2.5.5
Recognition Performance
The combined system is applied to recognize different patterns. To see the ability of
our system network on the recognition task, we use a small data set from the MNIST
(50 digits and 5 for each category). And we choose four neurons as the readout. We
call this readout as the fully distributed scheme with no redundancy (each neuron
codes for one bit). After several iterations of training, the network can recognize all
the patterns in this data set. Here, we take the recognition results of several digits as
an example (Fig.2.12). If the potential of the learning neuron crosses the threshold,
namely it ﬁres, the value of this neuron is considered as 1, otherwise it is 0. In
Fig.2.12, when image “0” shows up to the network, none of the learning neurons
ﬁre, so the result is [0000]. For image “3”, the result is [0011], and for “9” it’s [1001].
This indicates that the tempotron rule applied in our model could recognize different
classes of images successfully.
However, using only four neurons as the readout in a binary format might be very
sensitive to changes of input images, especially considering the real-world stimuli
in which samples hold large variability in a given class and overlap with members in
different classes. If only one neuron misclassiﬁed the incoming pattern while others
correctly responded, the pattern was still wrongly classiﬁed. Researchers have found
that neighboring neurons have similar response properties. Depending on this, neural
groups are used for assembly computing [16].
Thus, we use several grouped pools as our readout. We ﬁrstly consider a distributed
code with redundancy: 4 pools of 20 neurons each. Each pool codes for one binary
Fig. 2.12 Recognition results of digits by Tempotron learning rule. Here shows 4 learning neurons
(Neuron 1–4) and 3 images. The neuron responds to an image by ﬁring (1) or not (0). The results
for “0”, “3” and “9” are [0000], [0011] and [1001], respectively

2.5 Simulation Results
35
feature as in Fig.2.12. A voting system decides if the binary feature is 0 or 1 based on
the voting majority in this pool. Then we consider a localist scheme with redundancy,
where each pool of 20 neurons codes for only one category. For an incoming stimulus,
it is classiﬁed into a category according to the pool which has the most amount of
voting neurons ﬁred. If two or more pools have the same maximal ﬁring number, the
incoming stimulus is classiﬁed as unknown pattern.
These two schemes of readout with redundancy are used. For cross-validation,
we choose 500 digits (50 images for each category) as our training set and randomly
choose other 100 images from the MNIST data set as the testing set. In the training
phase, each neuron is trained with a sub-training set chosen from the training set.
This sub-training set consists of examples randomly chosen from the corresponding
category and also other categories. After training, the performance is tested on both
training set and testing set. The correct rate on the testing set is around 50% for the
distributed code with no redundancy, and is around 79% for the localist code with
redundancy. For distributed code, although the robustness for coding one bit feature
is improved comparing to single neuron code, it is still not comparable to the localist
one. This is due to that in the distributed code the ﬁnal decision highly depends on
correct reaction of each pool, but in the localist code it only depends on a correct
major voting of one corresponding pool. Thus, in the localist scheme, the robustness
is not only due to the redundancy but also to the localist aspect. This localist scheme
is considered in our following experiments.
To make a comparison with the benchmark machine learning method, SVMs are
chosen to perform the classiﬁcation on the CCs activation values. Since SVM also
has a binary decision behavior, we set the same classiﬁcation condition on training
and testing as for tempotron. The performances of both the tempotron and SVM on
the training set and testing set are shown in Fig.2.13. The corresponding recognition
rates are shown in Table2.1.
According to Fig.2.13, our network with the tempotron rule performs at a high
correct rate (around 93.7%) on the training set and at an acceptable correct rate
(around79%)onthetestingset,especiallyconsideringthesmalldataset(500images)
used for training. Comparing with SVM under the same condition of our encoding
model, the performances of spiking neurons are better than SVM for the training set
andcomparabletoSVMforthetestingset.Fromabiologicalpointofview,oursystem
attempts to perform robust and rapid recognition with a brain-like architecture.
Table 2.1 The classiﬁcation performance of tempotron and SVM on MNIST
Tempotron rule
SVM
Percentage(%)
Training
Testing
Training
Testing
Correct rate
93.67 ± 0.67
78.5 ± 1.85
90.24 ± 0.98
79.33 ± 2.03
Wrong rate
4.48 ± 0.58
18.35 ± 1.85
6.88 ± 0.78
18.15 ± 1.69
Unknown rate
1.86 ± 0.61
3.15 ± 1.64
2.89 ± 0.86
2.53 ± 2.04

36
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
Fig. 2.13 The classiﬁcation performance of tempotron and SVM. The system is trained 40 times
each for tempotron and SVM. After each training time, the generalization is performed on both the
training and testing set. The averages and standard deviations are plotted
Fig. 2.14 Average weights of the spiking neurons in the pool representing digit 0 and 3. Left Image
samples of digit 0 and 3 from MNIST are listed. Right The picture of the average weight of the
spiking neurons in corresponding group. Inhibited afferents are plotted black, while excited ones
are plotted gray-scale according to their weights

2.5 Simulation Results
37
To investigate the states of the spiking neurons in one grouped pool after learning,
a picture of the average weights is shown in Fig.2.14. According to Fig.2.14, the
grouped neurons, cooperating together, roughly grab a general and basic feature of
the learned category. Taking digit 0 as an example, the center weights are mostly
inhibited since these neurons are rarely activated by the incoming stimulus 0 through
our encoding model.
2.6
Discussion
Discussions on the proposed system are given as follows.
2.6.1
Encoding Beneﬁts from Biology
Through the layers of GCs and CCs the external stimuli are sparsely represented
in the activation values of CCs units. These activation values are used to generate
spiking patterns in a time domain. It already has been shown that coding schemes
based on the ﬁring rates are unlikely to be efﬁcient enough for fast information
processing [14, 31]. Considering the rapid processing in the brain and billions of
neurons involved, a temporal code which uses single spikes is, in principle, capable
of carrying substantial information about the external stimuli [11] and facilitating the
computational speed. In several sensory systems, shorter latencies of spikes result
from stronger stimulation [32, 33]. In our encoding layer, the strongly activated
neurons would ﬁre earlier, whereas the weakly activated neurons would ﬁre later or
not at all. The chosen encoding window of the temporal patterns is on a scale of
hundreds of milliseconds, which matches the biologically experimental results as
mentioned in [34–36]. In addition, our encoding is efﬁcient and the spiking output
is sparse as observed in biological retinas [29, 37].
2.6.2
Types of Synapses
The types of synapses are determined by the signs of their efﬁcacies, with posi-
tive values corresponding to excitatory synapses and negative values to inhibitory
synapses. Although this model is far from biological realism, it is proved to be a
useful computational approach [9]. In the neuron model, the sign of synapse could
change by learning. The learning also works when the signs of synapses are not
allowed to change, but the capacity is reduced. For a practical usage for multiple-
class problem, changing sign is allowed in the neuron model. This can be realized
by altering the balance between excitatory and inhibitory pathways [7].

38
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
2.6.3
Schemes of Readout
Using a binary version of readout, the network is shown to be capable to ﬁnish a
simple recognition task on a small data set. However, this kind of readout would be
very sensitive to each neuron’s performance in the readout. If only one neuron mis-
classiﬁes the pattern while others do a correct classiﬁcation, the ﬁnal readout would
also be wrong since it depends on all the neurons in a binary form. Using grouped
pools could effectively compensate this. In nervous systems such as visual corti-
cal areas [38] and hippocampus [39], information is commonly expressed through
populations or clusters of cells rather than through single cell [40]. This strategy is
robust since damage to a single cell will not have a catastrophic effect on the whole
population. Through learning, neurons in the same group try to ﬁnd the common fea-
tures discriminating that category, and through voting, the most active group would
be chosen. Another meaningful aspect of the readout is that there is an unknown
decision. Since some samples in one category are quite similar to other categories
(for example the digit “5” in the second row of Fig.2.7), it is reasonable to label
them as unknown rather than wrong. A further processing could be done for these
unknown samples.
2.6.4
Extension of the Network for Robust Sound
Recognition
In addition to the recognition on images, we also proposed a SNN for recognizing
sounds. The general structure remains the same, where functional parts of encoding,
learning and readout are involved. The major difference of the two systems is the
encoding part. With a proper encoding scheme for sounds, the SNN can perform the
Fig. 2.15 The proposed LSF-SNN system for sound recognition. Firstly the keypoints are detected
and the corresponding LSFs are extracted. Then, the SOM map is used to produce the output
spatiotemporal spike patterns. These patterns are then learnt by the tempotrons for recognition

2.6 Discussion
39
recognition well. We propose a novel approach based on the temporal coding of Local
Spectrogram Features (LSF) [41], which generates spikes that are used to train the
following neurons. The general structure for sound recognition is shown in Fig.2.15.
Our experiments demonstrate the robust performance of this system across a variety
of noise conditions, such that it is able to outperform the conventional frame-based
baseline methods. More details can be found in [41].
2.7
Conclusion
A systematic computational model by using consistent temporal encoding, learning
and readout has been presented to explore brain-based computation especially in
the regime of pattern recognition. It is a preliminary attempt to perform rapid and
robust pattern recognition from a biological point of view. The schemes used in
this model are efﬁcient and biologically plausible. The external stimuli are sparsely
represented after our encoding and the representations have properties of selectivity
and invariance. Through the network, the temporal learning rules can be applied to
processing real-world stimuli.
References
1. Serre, T., Oliva, A., Poggio, T.: A feedforward architecture accounts for rapid categorization.
Proc. Natl. Acad. Sci. 104(15), 6424–6429 (2007)
2. Perrett, D.I., Hietanen, J.K., Oram, M.W., Benson, P.J.: Organization and functions of cells
responsive to faces in the temporal cortex. Philos. Trans. R. Soc. Lond. Ser. B 335, 23–30
(1992)
3. Hung, C.P., Kreiman, G., Poggio, T., DiCarlo, J.J.: Fast readout of object identity from macaque
inferior temporal cortex. Science 310(5749), 863–866 (2005)
4. Tsukada, M., Pan, X.: The spatiotemporal learning rule and its efﬁciency in separating spa-
tiotemporal patterns. Biol. Cybern. 92, 139–146 (2005)
5. Knudsen, E.I.: Supervised learning in the brain. J. Neurosci. 14(7), 3985–3997 (1994)
6. Brader, J.M., Senn, W., Fusi, S.: Learning real-world stimuli in a neural network with spike-
driven synaptic dynamics. Neural Comput. 19(11), 2881–2912 (2007)
7. Gütig, R., Sompolinsky, H.: The tempotron: a neuron that learns spike timing-based decisions.
Nature Neurosci. 9(3), 420–428 (2006)
8. Ponulak, F.: ReSuMe-new supervised learning method for spiking neural networks. Institute
of Control and Information Engineering, Pozno´n University of Technology, Technical Report
(2005)
9. Ponulak, F., Kasinski, A.: Supervised learning in spiking neural networks with resume:
sequence learning, classiﬁcation, and spike shifting. Neural Comput. 22(2), 467–510 (2010)
10. Florian, R.V.: Tempotron-Like Learning with ReSuMe. In: Proceedings of the 18th Interna-
tional Conference on Artiﬁcial Neural Networks. Part II, ICANN’08, pp. 368–375. Springer,
Heidelberg (2008)
11. Gollisch, T., Meister, M.: Rapid neural coding in the retina with relative spike latencies. Science
319(5866), 1108–1111 (2008)
12. Thorpe, S., Fize, D., Marlot, C.: Speed of processing in the human visual system. Nature
381(6582), 520–522 (1996)

40
2
Rapid Feedforward Computation by Temporal Encoding and Learning …
13. Bohte, S.M., Bohte, E.M., Poutr, H.L., Kok, J.N.: Unsupervised clustering with spiking neurons
by sparse temporal coding and multi-layer RBF networks. IEEE Trans. Neural Netw. 13, 426–
435 (2002)
14. Van Rullen, R., Thorpe, S.J.: Rate coding versus temporal order coding: what the retinal gan-
glion cells tell the visual cortex. Neural Comput. 13(6), 1255–1283 (2001)
15. Perrinet, L., Samuelides, M., Thorpe, S.J.: Coding static natural images using spiking event
times: do neurons cooperate? IEEE Trans. Neural Netw. 15(5), 1164–1175 (2004)
16. Ranhel, J.: Neural assembly computing. IEEE Trans. Neural Netw. Learn. Syst. 23(6), 916–927
(2012)
17. Hubel, D.H., Wiesel, T.N.: Receptive ﬁelds and functional architecture of monkey striate cortex.
J. physiol 195(1), 215–243 (1968)
18. Burkart, Fischer: Overlap of receptive ﬁeld centers and representation of the visual ﬁeld in the
cat’s optic tract. Vis. Res. 13(11), 2113–2120 (1973)
19. Riesenhuber, M., Poggio, T.: Hierarchical models of object recognition in cortex. Nature Neu-
rosci. 2(11), 1019–1025 (1999)
20. Masquelier, T., Thorpe, S.J.: Unsupervised learning of visual features through spike timing
dependent plasticity. PLoS Comput. Biol. 3(2) (2007)
21. Serre, T., Wolf, L., Bileschi, S., Riesenhuber, M., Poggio, T.: Robust object recognition with
cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach. Intell. 29, 411–426 (2007)
22. Serre, T., Kouh, M., Cadieu, C., Knoblich, U., Kreiman, G., Poggio, T.: A theory of object
recognition: computations and circuits in the feedforward path of the ventral stream in primate
visual cortex. In: AI Memo (2005)
23. Enroth-Cugell, C., Robson, J.G.: The contrast sensitivity of retinal ganglion cells of the cat. J.
Physiol. 187(3), 517–552 (1966)
24. McMahon, M.J., Packer, O.S., Dacey, D.M.: The classical receptive ﬁeld surround of primate
parasol ganglion cells is mediated primarily by a non-GABAergic pathway. J. Neurosci. 24(15),
3736–3745 (2004)
25. Yu, A.J., Giese, M.A., Poggio, T.: Biophysiologically plausible implementations of the maxi-
mum operation. Neural Comput. 14(12), 2857–2881 (2002)
26. Lampl, I., Ferster, D., Poggio, T., Riesenhuber, M.: Intracellular measurements of spatial inte-
gration and the MAX operation in complex cells of the cat primary visual cortex. J. Neuro-
physiol. 92(5), 2704–2713 (2004)
27. Gawne, T.J., Martin, J.M.: Responses of primate visual cortical neurons to stimuli presented
by ﬂash, saccade, blink, and external darkening. J. Neurophysiol. 88(5), 2178–2186 (2002)
28. Ponulak, F.: Analysis of the resume learning process for spiking neural networks. Appl. Math.
Comput. Sci. 18(2), 117–127 (2008)
29. Olshausen, B.A., Field, D.J.: Sparse coding with an overcomplete basis set: a strategy employed
by V1? Vis. Res. 37(23), 3311–3325 (1997)
30. Gardner, E.: The space of interactions in neural networks models. J. Phys. A21, 257–270 (1988)
31. Gautrais, J., Thorpe, S.: Rate coding versus temporal order coding: a theoretical approach.
Biosystems 48(1–3), 57–65 (1998)
32. Reich, D.S., Mechler, F., Victor, J.D.: Independent and redundant information in nearby cortical
neurons. Science 294, 2566–2568 (2001)
33. Greschner, M., Thiel, A., Kretzberg, J., Ammermüller, J.: Complex spike-event pattern of
transient ON-OFF retinal ganglion cells. J. Neurophysiol. 96(6), 2845–2856 (2006)
34. Panzeri, S., Brunel, N., Logothetis, N.K., Kayser, C.: Sensory neural codes using multiplexed
temporal scales. Trends Neurosci. 33(3), 111–120 (2010)
35. Butts, D.A., Weng, C., Jin, J., Yeh, C.I., Lesica, N.A., Alonso, J.M., Stanley, G.B.: Temporal
precision in the neural code and the timescales of natural vision. Nature 449(7158), 92–95
(2007)
36. Borst, A., Theunissen, F.E.: Information theory and neural coding. Nature Neurosci. 2(11),
947–957 (1999)
37. Hunt, J.J., Ibbotson, M.R., Goodhill, G.J.: Sparse coding on the spot: spontaneous retinal waves
sufﬁce for orientation selectivity. Neural Comput. 24(9), 2422–2433 (2012)

References
41
38. Usrey, W., Reid, R.: Synchronous activity in the visual system. Annu. Rev. Physiol. 61(1),
435–456 (1999)
39. Wilson, M., McNaughton, B.: Dynamics of the hippocampal ensemble code for space. Science
261(5124), 1055–1058 (1993)
40. Pouget, A., Dayan, P., Zemel, R.: Information processing with population codes. Nature Rev.
Neurosci. 1(2), 125–132 (2000)
41. Dennis, J., Yu, Q., Tang, H., Tran, H.D., Li, H.: Temporal coding of local spectrogram features
for robust sound recognition. In: 2013 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 803–807 (2013)

Chapter 3
A Spike-Timing Based Integrated Model
for Pattern Recognition
Abstract During the last few decades, remarkable progress has been made in solv-
ing pattern recognition problems using network of spiking neurons. However, the
issue of pattern recognition involving computational process from sensory encoding
to synaptic learning remains underexplored, as most existing models or algorithms
only target part of the computational process. Furthermore, many learning algorithms
proposed in literature neglect or pay little attention to sensory information encoding,
which makes them incompatible with neural-realistic sensory signals encoded from
real-world stimuli. By treating sensory coding and learning as a systematic process,
we attempt to build an integrated model based on spiking neural networks (SNNs),
which performs sensory neural encoding and supervised learning with precisely
timed sequences of spikes. With emerging evidence of precise spike-timing neural
activities, the view that information is represented by explicit ﬁring times of action
potentials rather than mean ﬁring rates has received increasing attention recently.
The external sensory stimulation is ﬁrst converted into spatiotemporal patterns using
latency-phase encoding method and subsequently transmitted to the consecutive net-
work for learning. Spiking neurons are trained to reproduce target signals encoded
with precisely timed spikes. It is shown that using a supervised spike-timing based
learning, different spatiotemporal patterns are recognized by different spike patterns
with a high time precision in milliseconds.
3.1
Introduction
Everyday we recognize plenty of familiar and novel objects. However, we know little
about the underlying mechanism of the sophisticated computation involved in the
recognition process of human nervous system. Throughout our brain, neurons propa-
gate information by generating clusters of electrical impulses called action potentials
(APs) [1]. Analogue stimuli are encoded into spatiotemporal patterns and the neural
representation of external world is the basis for perception and reaction [2]. Different
encoding methods have been proposed by researchers, and among these approaches
rate-based encoding (rate codes) and spike-based encoding (temporal codes) are
the most widely studied coding schemes [3, 4]. Traditionally, it is believed that
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_3
43

44
3
A Spike-Timing Based Integrated Model …
information is carried by the temporal average of spikes [5–7], and rate-based cod-
ing has been widely used in previous learning models such as performing stochastic
gradient learning [8] and solving recognition problem relying on variance of input
currents [9]. Although rate codes work well when the stimulus is constant or varying
slowly, which is not common in real-world stimulations. Unlike the rate coding, tem-
poral encoding schemes assume that information is carried by the precisely timed
spikes, which provides more information capacity than the mean ﬁring rate of neu-
rons [10, 11]. It has been found that temporally varying sensory information such
as visual and auditory signals is processed and stored with high precision in brain
[12, 13], and precisely timed spikes are important for the integration process of corti-
cal neurons [14]. Therefore, temporal codes can describe neural signal more precisely
which enable us to exploit time as a resource for communication and computation
in spiking neural networks.
Recent neurophysiological results show that the precision of temporal spikes may
be triggered by the rapid intensity transients [15] and even a single spike can carry
substantial information about visual stimuli [16]. The low response variability of
retinal ganglion cells shows that the most important information of a ﬁring event
generated by visual neurons may be reserved by the time of the ﬁrst spike and the
number of spikes [17]. Furthermore, experimental results show that most information
carried by spikes is the timing of the ﬁrst spike after stimulus onset [16]. In human
retina, visual signal from 108 photoreceptor cells are projected to 106 retinal ganglion
cells (RGCs) in the form of spike trains [15]. Hence the information compression is
indispensable during the projection. In addition, action potentials have been shown to
be related to the phases of the intrinsic sub-threshold membrane potential oscillations
[18, 19]. The phase locking between action potential and gamma oscillation has also
been discovered in electric ﬁsh [20] and the entorhinal cortex [21]. Phase coding has
been successfully utilized to perform sequences learning and episodic memory in
hippocampus via phase precession in previous works [22–24]. The phase information
of spikes is exploited within each receptive ﬁeld. As each ganglion cell receives
information from the photoreceptor cells in its receptive ﬁeld, phase coding is used
to reserve spatial information during compression as described in Sect.2.2. Thus,
we believe that the combination of temporal and phase coding offers a new way to
implement the compression as well as to explain the compression process.
After sensory encoding, the neural system needs to learn neural signals that rep-
resent external sensory stimulation. Spike-based learning algorithms compute with
ﬁring times and make use of the inter spike intervals so that they are compatible with
temporal codes. Hebbian synaptic plasticity has been viewed as the basic mechanism
for learning and memory [25, 26], in which the synaptic efﬁcacy is increased if the
presynaptic neuron repeatedly contributes to the ﬁring of postsynaptic neuron. As
precise spike timing [27] and relative timing between pre- and post-synaptic ﬁring
[28] are discovered, learning with millisecond precision has received intensive inter-
ests. Spike-timing-dependent plasticity (STDP) is believed to play an important role
in learning, memory and the development of neural circuits [29]. However, many
existing learning models use rate codes as the neural representation of information,
and learning with temporal codes remains an open research topic. The objective

3.1 Introduction
45
of learning is to train output neurons to respond selectively to inputs and generate
desired output spike patterns by adjusting synaptic plasticity. Since the membrane
potential of postsynaptic neuron is determined by the spikes of afferent neurons,
the generation of postsynaptic spike is the result of the cooperative integration and
synchronization of presynaptic input spikes [30, 31]. When the input spikes arrive in
synchrony and a sufﬁciently large depolarization of postsynaptic membrane poten-
tial is achieved, a ﬁring event will be triggered. Since we consider explicit desired
patterns for recognition task, supervised learning is preferred due to its efﬁciency
and accuracy. Moreover, growing evidences indicate that supervised learning is also
employed in cerebellum and cerebellar cortex [32, 33]. It has also been demonstrated
to be a successful form of learning to establish network with cognition functions [34,
35]. We adopt a spike-timing based supervised learning algorithm recently devel-
oped by [36], in which the error between the target spike train and the actural one
is used as the supervisory signal. In addition, the ﬁring intervals between pre- and
postsynaptic spikes are recorded for synaptic plasticity modiﬁcation, through which
the actual output patterns approximate the desired output patterns gradually.
The contribution of this work is to bridge the gap between sensory encoding and
synaptic information processing by proposing an integrated computational model
with spike-timing based encoding scheme and learning algorithm. This helps to
reveal the neural mechanisms starting from visual encoding to synaptic learning and
the computational process in central nervous system. Such an encoding and learning
algorithms in the proposed spike-based model are integrated in a consistent scheme:
temporal codes. The encoding method provides a possible mechanism for converting
visual information into neural signals. The spiking neurons are trained to classify
spatiotemporal patterns based on the temporal conﬁguration of spikes rather than
ﬁring rates of neurons.
This chapter is organized as follows: In Sect.3.2, we introduce the general struc-
ture, encoding method and learning algorithm of the proposed integrated model. In
Sect.3.3, the performance and properties of the integrated model are demonstrated
by numerical simulations. Section3.4 reviews the related works while Sect.3.5
concludes and discusses the limitations and extensions of the integrated model
proposed in this work.
3.2
The Integrated Model
3.2.1
Neuron Model and General Structure
In our proposed integrated model, all neurons are modeled with the leaky integrate-
and-ﬁre (LIF) model [37], which is deﬁned as:
τ dV
dt = −(V −Vr) + R(I0 + Iin + In)
(3.1)

46
3
A Spike-Timing Based Integrated Model …
Image 1
Image 2
Image 3
Pattern 1
Pattern 3
Pattern 2
Target pattern 2
Target pattern 1
Target pattern 3
N-1 network
N 
Spike trains
Encoding
Learning
Fig. 3.1 General structure and information process of the integrated model. The main components
of the model are the encoding part and the learning part. The spike-based model employs temporal
codes as the neural representation of external information. The latency-phase encoding as discussed
in Sect.2.2 is used to convert the image into spatiotemporal patterns consisting of N spike trains.
After sensory encoding, each spike train is received by one input neuron of the spiking neural
network. With a predeﬁned target pattern for each input pattern, the spiking neural network equipped
with a supervised spike-timing based learning as described in Sect.2.3 is trained to recognize the
different spatiotemporal patterns
where τ = RC is the membrane time constant, C = 1nF is the membrane conduc-
tance, R = 10 MΩ is the membrane resistance, V is the membrane potential and
Vr = −60mV is the rest potential, I0 = 0.1nA is the constant inject current, Iin is the
summation of presynaptic input currents, and In is a background noise modeled as a
Gaussian process with zero mean and variance 1 nA. Once the membrane potential
reaches the threshold Vthr = −55mV, it will be reset to Vres = −65mV and held
there for the refractory period.
The spike-based model presented here consists of two components: the latency-
phase encoding and the supervised spike-timing based learning. Starting from envi-
ronmental stimuli, we ﬁrst encode images into spatiotemporal patterns and then
transmit them to a spiking neural network for learning. The entire structure of the
model is illustrated in Fig.3.1.
3.2.2
Latency-Phase Encoding
With a combination of temporal encoding and phase encoding, a feature-dependent
phase encoding algorithm has been proposed in [38]. Inspired by the information
processing in the retina, the visual information is encoded into the responses of
neurons using precisely timed action potentials. The intensity value of each pixel is
converted to a precisely timed spike via a latency encoding scheme. Various exper-
iments show that a strong stimulation leads to a short spike latency, and a weak

3.2 The Integrated Model
47
stimulation results in a long reaction time [39–41]. Therefore, a monotone decreas-
ing function could be used for the conversion from sensory stimuli to spatiotemporal
patterns. Here, a logarithmic intensity transformation is adopted, which is similar to
that used in [42].
ti = f (si) = tmax −ln(α · si + 1)
(3.2)
where ti is the ﬁring time of neuron i, tmax is the maximum time of encoding window,
α is a scaling factor, and si is the intensity of the analog stimulation. One advantage
of the logarithmic function is that the time differences of spike latencies are invariant
with different contrast level, e.g., it depends on the relative strength of the stimulation.
Ganglion cells have been observed to be ﬁring in synchrony in several species [43–
45], which illustrates the involvement of oscillations in the retina. We assume that
the phases of oscillations are related to action potentials and contribute to the infor-
mation compression from photoreceptor cells to ganglion cells. To take advantage
of the phase information, spikes are assigned with phases related to their respective
oscillations. Since each ganglion cell receives spikes from a group of photoreceptor
cells, which is deﬁned as the receptive ﬁeld of this ganglion cell, we assign different
initial phases to their subthreshold membrane oscillations. The periodic oscillation
is described as cosine function for simplicity,
iosc = A cos(ωt + φi)
(3.3)
where A is the magnitude of the subthreshold membrane oscillations, ω is the phase
angular velocity of the oscillation, and φi is the phase shift of the ith neuron in the
receptive ﬁeld.
In order to distinguish photoreceptor cells in the same receptive ﬁeld, we set a
constant phase gradient among photoreceptor neurons. The phase of subthreshold
membrane oscillation for the ith photoreceptor neuron φi is deﬁned as:
φi = φ0 + (i −1) · Δφ
(3.4)
where φ0 is the reference initial phase, and Δφ is the constant phase difference
between nearby photoreceptor cells (Δφ <
2π
NRF , NRF is the number of photoreceptor
cells in each receptive ﬁeld).
The spikes generated by the photoreceptor cells in each receptive ﬁeld are com-
pressed into one spike train by the ganglion cell. In order to utilize the phase infor-
mation of spikes to reconstruct the original visual stimuli, the alignment operation
is required to link each spike in the spike train with the corresponding photorecep-
tor cell in the receptive ﬁeld. The alignment procedure is implemented by forcing
photoreceptor cells to ﬁre only when the subthreshold membrane potentials reach
their nearest peaks as illustrated in Fig.3.2b, c. After compression as shown in
Fig.3.2c, d, each spike in the compressed spike train is linked to one particular
photoreceptor cell in the receptive ﬁeld according to the phase of the subthreshold

48
3
A Spike-Timing Based Integrated Model …
oscillations. Consequently, the phase information and the alignment together build
an one-to-one relationship between the photoreceptor cells and spikes generated by
the corresponding ganglion cell. With the latency-phase coding scheme, external
stimulation is encoded into precisely timed spikes and then compressed into spike
trains. The intensity information is encoded into ﬁring times while the spatial infor-
mation is reserved by the phases of spikes. When the spike trains are transmitted to
coupled networks with respect to the encoding area, latency-phase encoded spikes
generated by photoreceptor cells can be reconstructed from the compressed spike
trains with a same phase reference as shown in Fig.3.2d, e. The visual stimulus can
then be reconstructed via a simple latency decoding process as shown in Fig.3.2e, f.
The complete latency-phase scheme is illustrated in Fig.3.2.
0
5
10
15
20
25
30
35
40
time steps
# neuron
0
5
10
15
20
25
30
35
40
time steps
# neuron
latency 
encoding
alignment
(a)
(b)
(c)
compression
(d)
(e)
(f)
phase
reconstruction
latency 
decoding
0
5
10
15
20
25
30
35
40
time steps
# neuron
Fig. 3.2 Flowchart of the latency-phase encoding scheme. (a) Original stimuli. Stimulations with
different intensities are the inputs to the photoreceptor cells. (b) The latency-encoded pattern. The
visual information carried by the intensities is converted into the latencies of spikes. The spikes
are assigned with phase information according to their corresponding oscillations. (c) Encoded
spikes after latency encoding and alignment operation. The spikes are forced to be generated at
peaks of the sub-threshold oscillations. (d) Compressed spike train. The spikes generated by the
photoreceptor cells from the same receptive ﬁeld are compressed into a spike train. (e) Reconstructed
latency-encoded spikes. Spatial information within the receptive ﬁeld could be retrieved from the
compressed spike train via a phase reconstruction. (f) Decoded stimuli. By an inverse latency
transformation, the original stimuli are reconstructed from the reconstructed spikes [38]

3.2 The Integrated Model
49
3.2.3
Supervised Spike-Timing Based Learning
It is known that learning from instructions is an important way for our brain to obtain
new knowledge. As proposed in [36], the remote-supervised-method (ReSuMe) is
compatible with temporal codes and is capable of performing spike-timing based
learning precisely with millisecond timescale. The learning algorithm is based on
a STDP-like process and synaptic modiﬁcation during training depends on the pre-
and postsynaptic ﬁring times. After the training is successful, responses of output
neurons will converge to the target patterns with a high time precision.
It is common that error signal between the target and the actual output is used
in supervised learning. Similar to Widrow-Hoff rule applied in rate-based neuron
models [46], the modiﬁcation of synaptic efﬁcacy in ReSuMe is triggered by either
the target output (Sd(t)) or the actual output (So(t)). At the same time, the sign of
error signal (Sd(t) −So(t)) decides the direction of the modiﬁcation. To take the
spike-timing into consideration, a STDP-like term is incorporated in the kernel adi:
adi(−s) = A · exp( s
τ ),
if s < 0
(3.5)
where A is the maximal magnitude of the STDP window, and s is the delay between
the pre- and postsynaptic ﬁring. Similar to the STDP process, if a presynaptic spike
precedes a postsynaptic spike within a time interval, the synapse is strengthened.
When the phase relation is reversed, the synapse is weaken. The magnitude of mod-
iﬁcation is determined by the lag s between pre- and postsynaptic spikes and is
calculated by the convolution adi(t) ∗Si(t). The complete learning rule is described
as in Ponulak and Kasinski [36],
d
dt woi(t) = [Sd(t) −So(t)][ad +
 ∞
0
adi(s)Si(t −s)ds]
(3.6)
where woi is the synaptic weight from the presynaptic neuron i to the postsynaptic
neuron o. Sd(t), So(t) and Si(t) are the desired output, actual output and input spike
train, respectively. ad is a constant that helps speed up the learning process. From
Eq.(3.6), we can see that the synaptic weights are updated when Sd(t) ̸= So(t), and
the direction of modiﬁcation is determined by the sign of the error signal Sd(t) −
So(t). No modiﬁcation is induced when the actual output pattern is in agreement with
the desired output pattern (Sd(t) = So(t)), which is used as the stopping criterion.
The magnitude of modiﬁcation is determined by the convolution term adi(t) ∗Si(t).
Thus, Si(t), Sd(t) and So(t) together are responsible for the synaptic modiﬁcation.
The learning rule is illustrated in Fig.3.3.
The supervised signal is generated by the remote supervision scheme. Therefore,
the target spike train is not directly delivered to the postsynaptic learning neuron
and it determines the change of the synaptic efﬁcacy from the presynaptic neuron

50
3
A Spike-Timing Based Integrated Model …
presynaptic spikes
eligibility trace
desired output
output spikes
synaptic weight
(a)
(b)
(c)
(d)
Fig. 3.3 Learning rule of ReSuMe. (a) The presynaptic input spikes, (b) The eligibility trace,
(c) The desired output and actual output spikes, (d) The synaptic weight. The eligibility trace in
(b) records the status of neuron according to the presynaptic spikes in (a). The desired output
(positive direction) and the actual output (negative direction) in (c) together determine the sign of
the supervisory signal. There is no other modiﬁcation when the actual output spikes are generated
at the desired times. The synaptic weight is updated when either a actual spike is generated or a
desired spike should be induced. Meanwhile, the amount of synaptic weight change depends on the
lag between pre- and postsynaptic spikes and the eligibility trace in (b) [36]
to postsynaptic neuron. It should be noted that both the excitatory synapses and
inhibitory synapses exist in the model. During the learning, the synaptic weight is
modiﬁed when either a target spike is needed or the postsynaptic learning neuron ﬁres
at the wrong time. When the modiﬁcation occurs, the sign of error signal (Sd(t) −
So(t)) decides the direction of change and the kernel ad +
 ∞
0 adi(s)Si(t −s)ds
decides the amount of weight change. The synapses contributing to the ﬁring of
desired spikes are excitatory and adjusted to bring forward or hold off the ﬁring
times. On the other hand, the inhibitory synapses are used to suppress the ﬁrings at
undesired times. The learning process stops as soon as the actual output patterns are
identical to the target patterns.
3.3
Numerical Simulations
Real-world visual stimuli are often complex and contain a large amount of informa-
tion. In this section, three 256 × 256 grayscale images are used to demonstrate the
classiﬁcation capability and the robustness of the integrated model. Images from the
Urban and Natural Scene Categories of the LabelMe data set [47] are used here to
explore the inﬂuence of parameter variations and the memory capacity of the system.

3.3 Numerical Simulations
51
3.3.1
Network Architecture and Encoding of Grayscale
Images
The receptive ﬁeld (RF) of a sensory neuron is deﬁned as a spatial region where the
presence of stimulus affects the ﬁring of that neuron. During the encoding phase,
visual information from photoreceptor cells in the same RF is projected to retinal
ganglion cells. Each ganglion cell then compresses the received spikes into a spike
train. Therefore, the number of spikes in each spike train is determined by the number
of pixels in each input image and the number of RFs.
Nspike =
n
NRF
(3.7)
where Nspike is the number of spikes in each spike train (number of pixels in each sub-
ﬁeld assigned with an RF), n is the number of photoreceptor cells (number of pixels
of each image), and NRF is the number of retinal ganglion cells (i.e., the number
of RFs). Since each ganglion cell connects to one input neuron of the consecutive
spiking neural network, the number of input neurons N is equal to NRF. The number
of output neurons depends on the size of data sets and the readout strategy. Intuitively,
for large database with a large number of classes and complex target patterns with
more spikes, more output neurons are required to perform the learning task. A two
layer spiking neural network with 1024 input neurons and a single output neuron is
used to illustrate the recognition capability of this model.
RF1
input latency code
gamma alignment
250
300
350
400
450
time step
(a)
(b)
(c)
Fig. 3.4 The latency-phase encoding. The original image (256 × 256 pixels) in (a) is partitioned
into 1024 RFs with the size of 8 × 8. The left pattern in (b) is the spike pattern of RF1 after latency
encoding and the right one is the pattern further processed by the alignment operation (spikes
are denoted by the dot markers). The compressed spike train of RF1 is given in (c). For better
visualization, only part of the encoded spatiotemporal pattern is illustrated

52
3
A Spike-Timing Based Integrated Model …
Here, grayscale images with the size of 256 × 256 pixels are used as the external
stimulation. Each pixel value is regarded as the intensity of the visual stimulation
received by the photoreceptor cell in the retina. Thus there are 1024 RFs with the size
of 8 × 8 pixels as shown in Fig.3.4a. After the alignment as shown in Fig.3.4b, each
ganglion cell receives 64 spikes from 64 photoreceptor cells in its receptive ﬁeld and
compresses them into one spike train as shown in Fig.3.4c. Therefore, information
of the 256 × 256 pixel image is encoded into 1024 spike trains and each spike train
contains 64 spikes. As the encoding method converts the intensity values into ﬁring
times of spikes, the visual information is preserved by the temporal conﬁguration of
the spike trains.
3.3.2
Learning Performance
To recognize images, we predeﬁne different target spike patterns for input patterns.
For simplicity, each target pattern is deﬁned as a sequence of three spikes (each target
pattern is denoted by a different marker type, as shown in Fig.3.5a). After sensory
encoding, three spatiotemporal patterns of length 640ms are repetitively presented
to the network in a random sequence. The number of epoch is increased when one
pattern has been presented to the network, while the number of iteration is increased
when all patterns have been presented to the network once. The responses of the
output neuron for different input patterns are shown in Fig.3.5a. To quantitively
evaluate the learning performance, a correlation-based measure of spike timing [48]
is adopted to measure the distance between the output pattern and the target pattern.
The correlation C is close to unity when the output pattern matches the target pattern
and equals to zero when the two patterns are unrelated. The spike trains (So and
Sd) are convolved with a low pass Gaussian ﬁlter of a given width σ = 2ms. If the
ﬁltered spike trains are −→
s1 and −→
s2 , the correlation measure is
C =
−→
s1 · −→
s2
|−→
s1 ||−→
s2 |
(3.8)
The typical results of the training are shown in Fig.3.5. Within 20 presentations
of each input pattern, the output neuron is able to reproduce the target pattern as
shown in Fig.3.5. At ﬁrst, the output neuron ﬁres at random times. After several
iterations, extra spikes ﬁring at undesired times disappear, and the actual output
patterns approach to the corresponding target patterns. When successful learning
is achieved, the output neuron is able to reproduce different target patterns when
different input patterns are given. We repeated the training for dozens of times and
observed that the spiking neuron is able to learn the training pairs successfully.

3.3 Numerical Simulations
53
3.3.3
Generalization Capability
The integrated model recognizes each image as a certain spatiotemporal pattern, in
which the intensities of individual pixels are encoded into precisely timed spikes.
Therefore, the generalization of the system is expected to be related to the pixel-level
features of the input images. To study the generalization capability of the model,
we add different levels of Gaussian, speckle and salt-and-pepper noise to the input
images during the testing phase. The Gaussian noise is speciﬁed by its mean m and
variance v, the speckle noise is speciﬁed by its variance v, and the salt-and-pepper
noise is speciﬁed by the noise density d. For each kind of the noise with different
intensities, we test the trained network with one hundred noisy images. The test
0
100
200
300
400
500
600
0
10
20
30
40
50
60
70
t(ms)
# epoch
0
2
4
6
8
10
12
14
16
18
20
0
0.2
0.4
0.6
0.8
1
# iteration n
C(n)
image 1
image 2
image 3
Illurstration of learning process
desired
output
image 1
image 3
image 2
actual
output
image 1
image 3
image 2
Learning performance
(a)
(b)
Fig. 3.5 Illustration of the learning process and performance. (a) Raster plot of the output spikes.
When presented with different input patterns, the output patterns converge to the corresponding
target patterns. Given different input patterns, spikes generated by the output neuron are denoted
by different marker types. (b) The correlations C between output spike trains and the target spike
trains against learning iterations. At ﬁrst, the output neuron ﬁres at random times. After several
iterations, the output patterns begin to approach to the target patterns and the learning is converged
within twenty iterations

54
3
A Spike-Timing Based Integrated Model …
results are shown in Fig.3.6b. By analyzing the learning process, we can see that
the pixel-feature dependent generalization is related to temporally local learning
algorithm. During the learning process, only the synaptic weights associated with
input spikes evoking the postsynaptic spikes within the learning window are updated.
The decaying learning window makes the optimization process to be focused on a
limited number of synapses, which affects the ﬁring time of the nearest postsynaptic
neuron. At the same time, noise added to input images shifts part of the ﬁring times
of the encoded spatiotemporal pattern. Therefore, the spiking neuron should be able
to reproduce target spikes with a small temporal error in response to the input images
with pixel noise, but fail to recognize images in the presence of other type of noises.
As expected, the test results in Fig.3.6b show that the system is more resistant to
salt-and-pepper noise than speckle noise or Gaussian noise.
We also add the different type of noises to the input images during the training
phase. For each type of noise, 100 × 3 noisy images are used as the training set. After
0
2
4
6
8
10x 10
−4
0.2
0.4
0.6
0.8
1
img 1
img 2
img 3
Correlation vs. Gaussian noise
(deterministic training)
C(v)
noise intensity (v)
0
2
4
6
8
10x 10
−4
0.75
0.8
0.85
0.9
0.95
1
1.05
1.1
Correlation vs. Gaussian noise
(noisy training)
noise intensity (v)
C(v)
img 1
img 2
img 3
0
2
4
6
8
10
x 10
−3
0.4
0.5
0.6
0.7
0.8
0.9
1
noise intensity (v)
Correlation vs. speckle noise
(deterministic training)
C(v)
img 1
img 2
img 3
0
2
4
6
8
10 x 10
−3
0.75
0.8
0.85
0.9
0.95
1
1.05
1.1
noise intensity (v)
C(v)
Correlation vs. speckle noise
(noisy training)
img 1
img 2
img 3
0
0.05 0.1 0.15 0.2 0.25 0.3
0.6
0.7
0.8
0.9
1
1.1
noise intensity (d)
C(d)
Correlation vs. salt−and−pepper noise
(deterministic training)
img 1
img 2
img 3
0
0.05 0.1 0.15 0.2 0.25 0.3
0.75
0.8
0.85
0.9
0.95
1
1.05
1.1
noise intensity (d)
C(d)
Correlation vs. salt−and−pepper noise
(noisy training)
img 1
img 2
img 3
Gaussian noise
m=0,v=0.001
Speckle noise
v=0.01
Salt-and-pepper noise
d=0.3
(a)
(b)
(c)
Fig. 3.6 The test results with different type of noises added to the input images. (a) Examples
of images with different type of noises, such as Gaussian, speckle and salt-and-pepper noise. The
correlation C between the output spike pattern and the target pattern is used to evaluate the precision
of the neural responses. (b) Reliable responses can be reproduced by the spiking neural networks
for noisy images (e.g., deterministic training). (c) The robustness to noise is improved when the
noise information is included during the training phase (e.g., noisy training)

3.3 Numerical Simulations
55
training, another 100 × 3 images with noise of the same type and intensity level are
used to examine the reliability of the neural responses after noisy training. As shown
in Fig.3.6c, when the noise information is learned by the classiﬁer during training
phase, the robustness of the system due to the effect of noise has been improved.
It can also be observed that the maximum level of salt-and-pepper noise that the
system can tolerate is much higher than that of the other two type of noises, which
is consistent with our analysis.
3.3.4
Parameters Evaluation
To examine the inﬂuence of parameter variations in the encoded patterns, 100 images
(256 × 256 pixel, 8-bit grayscale) from the Urban and Natural Scene Categories of
theLabelMedatabaseareencodedwithvariousparameterconﬁgurations.Theimages
from LabelMe data set are used here to study the properties of the integrated model
due to their distributed intensity values and their closeness to real-world stimulation.
A few sample images from the data set are given in Fig.3.7.
The size of receptive ﬁeld, encoding cycles and phase shift constant are impor-
tant parameters for the encoding method. Since photoreceptor cells of the same RF
convey visual information to the corresponding retinal ganglion cell, the number of
photoreceptor cells in each RF affects the number of spikes in the compressed spike
train. If the length of encoding window is ﬁxed, increasing the RF size would result
in a higher average ﬁring rate of the compressed spike trains.
Fig.3.7 Sampleimagesof“buildingsinsidecity”categoryfromtheLabelMedatabase.Theoriginal
256 × 256 color images are converted into 8-bit grayscale images

56
3
A Spike-Timing Based Integrated Model …
0
5
10
15
20
25
0
200
400
600
800
1000
1200
1400
1600
1800
2000
14
16
18
20
22
24
26
5
10
15
20
25
30
35
Square error per pixel
Number of oscillation cycles
Number of oscillation cycles
Square error per pixel
Encoding error vs. oscillation cycles
0
1
2
3
4
5
6
0
10
20
30
40
50
60
70
80
90
100
Phase shift constant (degree)
Square error per pixel
Encoding error vs. phase shift constant
(a)
(b)
Fig. 3.8 The encoding error with different encoding cycles and phase shift constants on natural
images from the LabelMe database. The average square error per pixel (vertical axis) is employed to
estimate the encoding accuracy of the test images. (a) The encoding error drops when the number of
oscillation cycles increases. With more subthreshold membrane oscillation cycles, more oscillation
peaks provide more sampling points to encode input intensities (the tail of the curve is enlarged in
the inset). (b) The phase shift constant Δφ slightly affects the encoding accuracy
Consideringtheaccuracyofencodingprocess,noerrorisintroducedbythelatency
encoding scheme. The distortion of information is resulted from the alignment oper-
ation. As the alignment operation moves spikes to the peaks of the subthreshold
oscillations, the encoding accuracy is affected by the number of oscillation cycles
within the encoding period as shown in Fig.3.8a. To estimate the accuracy of encod-
ing, we compare the reconstructed images with the original images using the average
square of error per pixel,
e =
n
i=1
(si −s′
i)2
n
(3.9)
where si and s′
i are the intensities of the ith pixel in the original image and the
reconstructed image, respectively.
Since the intensity information is carried by the temporal spikes, the distribution of
the original images as well as the encoding parameters such as phase shift resolution
Δφ may affect the temporal distribution of the encoded spatiotemporal patterns. The
experiment results illustrate that the phase shift constant hardly affects the encoding
accuracy as shown in Fig.3.8b. However, it will determine the spike distribution of
the compressed spike train as shown in Fig.3.9. The encoded spikes concentrate in
the time domain with a small shift constant as shown in Fig.3.9a and spread out with
a large shift constant as shown in Fig.3.9b.
Therefore, the choice of encoding cycles depends on the precision requirement
for a speciﬁc application. Since the phase shift resolution Δφ affects the distribution
of encoded spatiotemporal patterns, it should be tuned according to the learning
algorithm adopted in the posterior neural network.

3.3 Numerical Simulations
57
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
5
10
15
20
25
30
35
40
45
50
time (s)
neuron number
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0
5
10
15
20
25
30
35
40
45
50
time (s)
neuron number
phase shift = 0.01 (rad)
phase shift = 0.1 (rad)
(a)
(b)
Fig. 3.9 The encoded patterns with a different phase shift constant. The phase shift constant is
the phase difference between nearby photoreceptor cells in the same receptive ﬁeld and affects the
ﬁring times within each receptive ﬁeld. With a small phase shift constant, neurons within the same
receptive ﬁeld tend to ﬁre simultaneously as shown in (a). With a large phase shift constant, the
temporal distribution of spikes is scattered as shown in (b)
Since the postsynaptic depolarization is determined by the integration of presy-
naptic input spikes, temporal distribution of input spatiotemporal patterns and the
complexity of target patterns will affect the learning performance. On one hand,
because a target spike requires one or more preceding input spikes to excite the out-
put neuron to ﬁre at the desired time, enough presynaptic input spikes are needed for
the generation of spikes. On the other hand, increasing the number of target spikes
will result in competition for limited available synapses between the target spikes
ﬁring at different times and impose restriction on the behavior of the output neuron.
We tested the system on 100 images (128 × 128 8-bit grayscale images from Urban
and Natural Scene Categories of LabelMe database) to examine the inﬂuence of
target patterns on the learning performance. For each number of target spikes, the
network was trained with one randomly generated target pattern. It is observed that
the spiking neuron needs more iterations to achieve a successful learning for a more
complex target patterns as discussed in our analysis.
3.3.5
Capacity of the Integrated System
The spiking neural network with the same settings in previous experiments is used to
explore the memory capacity of the integrated system. From a computational point of
view, precisely timed spikes have a remarkable encoding capacity, i.e., the memory
capacity of the system is often limited by the learning scheme employed. Since most
of the information is reserved by the temporal code, the design of target patterns plays
a pivot role in exploiting the information carried by the encoded spatiotemporal

58
3
A Spike-Timing Based Integrated Model …
Fig. 3.10 Memory (or
recognition) capacity of the
integrated model. The
average percentage of
successful recall of patterns
is plotted as a function of
training pairs. The successful
recall percentage drops
dramatically after the
number of training pairs is
larger than 11
0
5
10
15
20
25
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Number of training pairs
Percentage of successful recall
Capacity test of the spiking neural network
patterns. We train the network with different number of input patterns and deﬁne
the percentage of successful recall of trained pairs as an evaluation of the memory
capacity. A successful recall of one trained pattern is achieved when the distance
between the output pattern of the trained network and the target pattern is close
enough, i.e., C > 0.95 as the threshold. To simplify the problem for a classiﬁcation
task, we randomly generated one target spike train containing ten spikes for all input
images every time and repeat the experiment for 20 times.
As shown in Fig.3.10, for the 1024-1 spiking neural network with ten spikes in the
target patterns and the selected parameter settings, around 11 training pairs can be
successfully stored and recalled with a slight time shift. The percentage of successful
recall decreases quickly when the number of training pairs is increased. Apparently, it
can be inferred that decreasing the number of target spikes (complexity) or increasing
the free tunable parameters will lead to a larger amount of information capacity.
However, this would also allow less information of the spatiotemporal patterns to be
learned. Although it is not mathematically analyzed, the presented simulation results
for the speciﬁc case provide some insight into the information capacity of the system.
To summarize it from a system level, temporally distributed input spatiotemporal
patterns and simple target patterns are preferred for better generalization capabilities
and memory capacity of the integrated model. The scattered distribution of input
patterns enables the output neuron to generate spikes at arbitrary times. Although the
network can learn more about the original images with more complex target patterns,
the computational efforts will also be increased and the information capacity will
be limited. Therefore, the tradeoff between the learning level of input patterns and
the computational efforts as well as memory capacity should be considered for any
speciﬁc applications.

3.4 Related Works
59
3.4
Related Works
Spiking neural networks have been applied to solve different classiﬁcation tasks
[31, 49–52]. Hopﬁeld and Brody [30] proposed a computational model for pattern
recognition, in which analog signal is employed as neural representation of sensory
stimuli. The transient synchronization of decaying delay activity of a speciﬁc subset
of input neurons are used for recognition. Although it has been successfully applied
to speech recognition [31] and olfactory recognition [49], the unknown mechanism of
encoding input stimulation into decay ﬁring activities makes the model questionable.
Bohte et al. [50] proposed a temporal version of error-backpropagation, SpikeProp.
The SpikeProp was demonstrated to be able to classify images with a three-layer
spiking neural network. However, the adaptive learning can only be applied to ana-
lytically tractable neuron models, and the weights with mixed signs are suspected to
cause failures of training [53]. Gütig and Sompolinsky [51] proposed a supervised
learning algorithm, temptron, to classify spatiotemporal patterns by generating at
least one spike or staying quiescent.
Brader, Senn and Fusi [52] proposed an alternative approach, in which a spike-
driven model is able to perform binary image classiﬁcation with spiking neurons
using rate codes. In this approach, grayscale value of each pixel of input images is
normalized to a binary value such that the largest element is unity. Then each element
was encoded by Poisson spike trains at different frequencies. After learning, images
from different classes can be distinguished by the ﬁring rates of output neurons.
However, the spike-driven model only focuses on the learning part and pay little
attention to the sensory encoding. By transforming 8-bit grayscale images into binary
images, a large amount of the images have been discarded. Therefore, the actual
information carried by the input patterns are far less than that of the original images.
Moreover, the spike-driven learning relies on a stochastic process, which makes the
learning algorithm less efﬁcient and computational demanding.
Due to the use of different encoding scheme and learning strategy, the proposed
integrated model has several advantages over existing approaches. First, we look
at the pattern recognition process at a system level. Rather than considering sen-
sory encoding and learning as isolated processes, we integrated biological plausible
encoding and learning processes using consistent neural codes. The latency-phase
encoding scheme retains almost all information of the input images with high pre-
cision and links up the sensory encoding with learning process. Second, in the inte-
grated spike-based model, we demonstrated that input patterns can be classiﬁed by
precisely timed spike trains rather than the mean ﬁring rates or single spike code.
With the rich capacity of temporal codes, detailed information of the inputs can be
exploited by designing the target pattern and precisely timed spikes can be generated.
Furthermore, the supervised spike-timing based learning allows an efﬁcient compu-
tation and fast convergence, such that the system can be applied to real-life tasks,
such as movement control [54] and neuroprostheses control [55].

60
3
A Spike-Timing Based Integrated Model …
The input neurons are supposed to ﬁre more than once in our model, which makes
better use of the synaptic weights and generalization performance. Although the
temporal codes provide a large amount of information, multi-spike signal results
in the competition among target spikes ﬁring at different times for the available
resources. This leads to limited memory capacity and slow convergence as shown
in the simulation results. Therefore the removal of the conﬂicts among the target
spikes remains a challenging but interesting issue for the spike-timing based learning
algorithm. One approach is to employ multiple layer and recurrent neural structures,
such as liquid state machine [56], so as to increase the computational capability of
the system and to absorb the inﬂuence of multiple spikes.
There are a few limitations in our current model. The encoding scheme in the
model does not incorporate any information extraction to preprocess the input pat-
terns, which is viewed as a necessary procedure in traditional pattern recognition
models. By using ﬁltering techniques as proposed in HMAX model [57] or local
edge detectors [58], it is believed that the performance and memory capacity in the
proposed model will be improved with an efﬁcient neural code in a more concise
and abstract manner.
3.5
Conclusions
In this chapter, an integrated computational model with latency-phase encoding
method and supervised spiking-timing based learning algorithm has been proposed.
Stimuli were ﬁrst encoded into spatiotemporal patterns with latency-phase scheme,
which builds up a bridge between real-world stimuli to neural signals in a biological
plausible way. Then the patterns were learned by spiking neurons using a spike-
timing based supervised method with millisecond time precision. As shown in the
simulation results, the spike-timing based neural networks with temporal codes are
capable of solving pattern recognition task by computing with action potentials.
Although the current model has limitations in the recognition capacity, our study
exploits the computational mechanisms employed by neural systems in two respects:
First, our model was built at a system level emphasizing both the sensory encoding
and learning process. It is an integrated system based on a uniﬁed temporal coding
schemeandconsistentwiththeknownneurobiologicalmechanisms.Second,wehave
demonstratedtheclassiﬁcationcapabilityofthesystemthatcomputespreciselytimed
spikes and realistic stimuli, analogously to cognitive computation in human brain.
The approaches based on cognitive computation will play a leading role in many
applications spanning across signal processing, autonomous systems and robotics
[59–61].

References
61
References
1. Du Bois-Reymond, E.: Untersuchungen uer thierische elektricita. G. Reimer (1848)
2. Panzeri, S., Brunel, N., Logothetis, N.K., Kayser, C.: Sensory neural codes using multiplexed
temporal scales. Trends Neurosci. 33(3), 111–120 (2010)
3. Softky, W.R.: Simple codes versus efﬁcient codes. Curr. Opin. Neurobiol. 5(2), 239–247 (1995)
4. Rullen, R.V., Thorpe, S.J.: Rate coding versus temporal order coding: what the retinal ganglion
cells tell the visual cortex. Neural Comput. 13(6), 1255–1283 (2001)
5. Adrian, E.: The Basis of Sensation: The Action of the Sense Organs. W. W. Norton, New York
(1928)
6. Shadlen, M.N., Newsome, W.T.: Noise, neural codes and cortical organization. Curr. Opin.
Neurobiol. 4(4), 569–579 (1994)
7. Litvak, V., Sompolinsky, H., Segev, I., Abeles, M.: On the transmission of rate code in long
feedforward networks with excitatory-inhibitory balance. J. Neurosci.23(7), 3006–3015 (2003)
8. Seung, H.S.: Learning in spiking neural networks by reinforcement of stochastic synaptic
transmission. Neuron 40(6), 1063–1073 (2003)
9. Barak, O., Tsodyks, M.: Recognition by variance: learning rules for spatiotemporal patterns.
Neural Comput. 18, 2343–2358 (2006)
10. Bialek, W., Rieke, F., de Ruyter van Steveninck, R., Warland, D.: Reading a neural code.
Science 252(5014), 1854–1857 (1991)
11. Victor, J.D.: How the brain uses time to represent and process visual information. Brain Res.
886(1–2), 33–46 (2000)
12. Carr, C.E.: Processing of temporal information in the brain. Annu. Rev. Neurosci. 16(1), 223–
243 (1993)
13. Singer, W., Gray, C.M.: Visual feature integration and the temporal correlation hypothesis.
Annu. Rev. Neurosci. 18(1), 555–586 (1995)
14. Kayser, C., Montemurro, M.A., Logothetis, N.K., Panzeri, S.: Spike-phase coding boosts and
stabilizes information carried by spatial and temporal spike patterns. Neuron 61(4), 597–608
(2009)
15. Meister, M., II, M.J.B.: The neural code of the retina. Neuron 22(3), 435–450 (1999)
16. Gollisch, T., Meister, M.: Rapid neural coding in the retina with relative spike latencies. Science
319(5866), 1108–1111 (2008)
17. Keat, J., Reinagel, P., Reid, R., Meister, M.: Predicting every spike: A model for the responses
of visual neurons. Neuron 30(3), 803–817 (2001)
18. Llinas, R.R., Grace, A.A., Yarom, Y.: In vitro neurons in mammalian cortical layer 4 exhibit
intrinsic oscillatory activity in the 10-to 50-Hz frequency range. Proc. Natl. Acad. Sci. 88(3),
897–901 (1991)
19. Koepsell, K., Wang, X., Vaingankar, V., Wei, Y., Wang, Q., Rathbun, D.L., Usrey, W.M.,
Hirsch, J.A., Sommer, F.T.: Retinal oscillations carry visual information to cortex. Front. Syst.
Neurosci. 3, 4 (2009)
20. Heiligenberg, W.: Neural Nets in Electric Fish. MIT Press, Cambridge (1991)
21. Chrobak, J.J., Buzsáki, G.: Gamma oscillations in the entorhinal cortex of the freely behaving
rat. J. Neurosci. 18(1), 388–398 (1998)
22. O’Keefe, J., Burgess, N.: Dual phase and rate coding in hippocampal place cells: theoretical
signiﬁcance and relationship to entorhinal grid cells. Hippocampus 15(7), 853–866 (2005)
23. Tsodyks, M.V., Skaggs, W.E., Sejnowski, T.J., McNaughton, B.L.: Population dynamics and
theta rhythm phase precession of hippocampal place cell ﬁring: a spiking neuron model. Hip-
pocampus 6(3), 271–280 (1996)
24. Jensen, O.: Information transfer between rhythmically coupled networks: reading the hip-
pocampal phase code. Neural Comput. 13(12), 2743–2761 (2001)
25. Blumenfeld, B., Preminger, S., Sagi, D., Tsodyks, M.: Dynamics of memory representations
in networks with novelty-facilitated synaptic plasticity. Neuron 52(2), 383–394 (2006)
26. Tang, H., Li, H., Yan, R.: Memory dynamics in attractor networks with saliency weights. Neural
Comput. 22(7), 1899–1926 (2010)

62
3
A Spike-Timing Based Integrated Model …
27. Mainen, Z., Sejnowski, T.: Reliability of spike timing in neocortical neurons. Science
268(5216), 1503–1506 (1995)
28. Bi, G.Q., Poo, M.M.: Synaptic modiﬁcations in cultured hippocampal neurons: dependence on
spike timing, synaptic strength, and postsynaptic cell type. J. Neurosci. 18(24), 10464–10472
(1998)
29. Bi, G.Q., Poo, M.M.: Synaptic modiﬁcation by correlated activity: Hebb’s postulate revisited.
Annu. Rev. Neurosci. 24, 139–166 (2001)
30. Hopﬁeld, J.J., Brody, C.D.: What is a moment? “cortical” sensory integration over a brief
interval. Proc. Natl. Acad. Sci. 97(25), 13919–13924 (2000)
31. Hopﬁeld, J.J., Brody, C.D.: What is a moment? transient synchrony as a collective mechanism
for spatiotemporal integration. Proc. Natl. Acad. Sci. 98(3), 1282–1287 (2001)
32. Ito, M.: Mechanisms of motor learning in the cerebellum. Brain Res. 886(1–2), 237–245 (2000)
33. Montgomery, J., Carton, G., Bodznick, D.: Error-driven motor learning in ﬁsh. Biol. Bull.
203(2), 238–239 (2002)
34. Knudsen, E.I.: Supervised learning in the brain. J. Neurosci. 14(7), 3985–3997 (1994)
35. Ito, M.: Control of mental activities by internal models in the cerebellum. Nat. Rev. Neurosci.
9(4), 304–313 (2008)
36. Ponulak, F., Kasinski, A.: Supervised learning in spiking neural networks with resume:
sequence learning, classiﬁcation, and spike shifting. Neural Comput. 22(2), 467–510 (2010)
37. Gerstner, W., Kistler, W.M.: Spiking Neuron Models: Single Neurons, Populations, Plasticity.
Cambridge University Press, Cambridge (2002)
38. Nadasdy, Z.: Information encoding and reconstruction from the phase of action potentials.
Front. Syst. Neurosci. 3, 6 (2009)
39. Gawne, T.J., Kjaer, T.W., Richmond, B.J.: Latency: another potential code for feature binding
in striate cortex. J. Neurophysiol. 76(2), 1356–1360 (1996)
40. Reich, D.S., Mechler, F., Victor, J.D.: Temporal coding of contrast in primary visual cortex:
when, what, and why. J. Neurophysiol. 85(3), 1039–1050 (2001)
41. Greschner, M., Thiel, A., Kretzberg, J., Ammermüller, J.: Complex spike-event pattern of
transient on-off retinal ganglion cells. J. Neurophysiol. 96(6), 2845–2856 (2006)
42. Hopﬁeld, J.J.: Pattern recognition computation using action potential timing for stimulus rep-
resentation. Nature 376(6535), 33–36 (1995)
43. Arnett, D.: Statistical dependence between neighboring retinal ganglion cells in goldﬁsh. Exp.
Brain. Res. 32(1) (1978)
44. DeVries, S.H.: Correlated ﬁring in rabbit retinal ganglion cells. J. Neurophysiol.81(2), 908–920
(1999)
45. Meister, M., Lagnado, L., Baylor, D.A.: Concerted signaling by retinal ganglion cells. Science
270(5239), 1207–1210 (1995)
46. Widrow, B., Hoff, M.E., et al.: Adaptive switching circuits (1960)
47. Russell, B.C., Torralba, A., Murphy, K.P., Freeman, W.T.: Labelme: a database and web-based
tool for image annotation. Int. J. Comput. Vis. 77(1–3), 157–173 (2008)
48. Schreiber, S., Fellous, J., Whitmer, D., Tiesinga, P., Sejnowski, T.: A new correlation-based
measure of spike timing reliability. Neurocomputing 52–54, 925–931 (2003)
49. Brody, C.D., Hopﬁeld, J.: Simple networks for spike-timing-based computation, with applica-
tion to olfactory processing. Neuron 37(5), 843–852 (2003)
50. Bohte, S.M., Bohte, E.M., Poutr, H.L., Kok, J.N.: Unsupervised clustering with spiking neurons
by sparse temporal coding and multi-layer RBF networks. IEEE Trans. Neural Netw. 13, 426–
435 (2002)
51. Gütig, R., Sompolinsky, H.: The tempotron: a neuron that learns spike timing-based decisions.
Nat. Neurosci. 9(3), 420–428 (2006)
52. Brader, J.M., Senn, W., Fusi, S.: Learning real-world stimuli in a neural network with spike-
driven synaptic dynamics. Neural Comput. 19(11), 2881–2912 (2007)
53. Haruhiko, T., Masaru, F., Hiroharu, K., Shinji, T., Hidehiko, K., Terumine, H.: Obstacle to
training spikeprop networks: cause of surges in training process. In: Proceedings of the 2009
International Joint Conference on Neural Networks, pp. 1225–1229. IEEE Press, Piscataway
(2009)

References
63
54. Manette,O.,Maier,M.:Temporalprocessinginprimatemotorcontrol:relationbetweencortical
and EMG activity. IEEE Trans. Neural Netw. 15(5), 1260–1267 (2004)
55. Müller-Putz, G.R., Scherer, R., Pfurtscheller, G., Neuper, C.: Temporal coding of brain patterns
for direct limb control in humans. Front. Neurosci. 4 (2010)
56. Maass, W., Natschläger, T., Markram, H.: Real-time computing without stable states: a new
framework for neural computation based on perturbations. Neural Comput. 14(11), 2531–2560
(2002)
57. Riesenhuber, M., Poggio, T.: Hierarchical models of object recognition in cortex. Nature
Nurosci. 2(11), 1019–1025 (1999)
58. van Wyk, M., Taylor, W.R., Vaney, D.I.: Local edge detectors: a substrate for ﬁne spatial vision
at low temporal frequencies in rabbit retina. J. Neurosci. 26(51), 13250–13263 (2006)
59. Perlovsky, L.: Computational intelligence applications for defense [research frontier]. Comput.
Intell. Mag. IEEE 6(1), 20–29 (2011)
60. Meng, Y., Zhang, Y., Jin, Y.: Autonomous self-reconﬁguration of modular robots by evolving
a hierarchical mechanochemical model. Comput. Intell. Mag. IEEE 6(1), 43–54 (2011)
61. Yan, R., Tee, K.P., Chua, Y., Li, H., Tang, H.: Gesture recognition based on localist attractor
networks with application to robot control [application notes]. Comput. Intell. Mag. IEEE 7(1),
64–74 (2012)

Chapter 4
Precise-Spike-Driven Synaptic Plasticity
for Hetero Association of Spatiotemporal
Spike Patterns
Abstract This chapter introduces a new temporal learning rule, namely the Precise-
Spike-Driven (PSD) Synaptic Plasticity, for processing and memorizing spatiotem-
poral patterns. PSD is a supervised learning rule that is analytically derived from
the traditional Widrow-Hoff (WH) rule and can be used to train neurons to associate
an input spatiotemporal spike pattern with a desired spike train. Synaptic adapta-
tion is driven by the error between the desired and the actual output spikes, with
positive errors causing long-term potentiation and negative errors causing long-term
depression. The amount of modiﬁcation is proportional to an eligibility trace that is
triggered by afferent spikes. The PSD rule is both computationally efﬁcient and bio-
logically plausible. The properties of this learning rule are investigated extensively
through experimental simulations, including its learning performance, its general-
ity to different neuron models, its robustness against noisy conditions, its memory
capacity, and the effects of its learning parameters.
4.1
Introduction
With the same capability of processing spikes as biological neural systems, SNNs
[1–3] are more biologically realistic and computationally powerful than the tradi-
tional artiﬁcial neural networks. Spikes are believed to be the principal feature in
the information processing of neural systems, though the neural coding mechanism,
i.e., how information is encoded in spikes still remains unclear. The temporal codes
describe one possibility, where information is conveyed through precise times of
spikes. However, the complexity of processing temporal codes [4, 5] might limit
their usage in SNNs, which demands the development of efﬁcient learning algo-
rithms.
Supervised learning was proposed as a successful concept of information process-
ing [6]. Neurons are driven to respond at desired states under a supervisory signal,
and an increasing body of evidence shows that this kind of learning is exploited by
the brain [7–10]. Supervised mechanism has been widely used to develop various
learning algorithms for processing spatiotemporal spike patterns in SNNs [5, 11–16].
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_4
65

66
4
Precise-Spike-Driven Synaptic Plasticity …
SpikeProb [12] is one of the ﬁrst supervised learning algorithms for processing
precise spatiotemporal patterns in SNNs. However, in its original form, SpikeProb
cannot learn to reproduce a multi-spike train. The tempotron rule [5], another gradient
descent approach that is evaluated to be efﬁcient for binary temporal classiﬁcation
tasks, cannot output multiple spikes either. As the tempotron is designed mainly
for pattern recognition, it is unable to produce precise spikes. To produce a desired
spike train, several learning algorithms have been proposed such as ReSuMe [13,
17], Chronotron [14] and SPAN [15]. These three learning rules are all capable of
training a neuron to generate a desired spike train in response to an input stimulus.
In the Chronotron, two learning rules are introduced. One is analytically-derived (E-
learning) and another one is heuristically-deﬁned (I-learning). The I-learning rule is
more biologically plausible but comes with less memory capacity than the E-learning
rule. The performance of the I-learning rule depends on the weight initialization,
where initial zero values can cause information loss from the corresponding afferent
neurons. The E-learning rule and the SPAN rule are both based on an error function of
the difference between the actual output spike train and the desired spike train. Their
applicability is therefore limited to the tractable error evaluation, which might be
unavailable in actual biological networks and inefﬁcient from a computational point
of view. These arithmetic-based rules can reveal explicitly how SNNs can be trained
but the biological plausibility of the error calculation is somewhat questionable.
In this chapter, we propose an alternative learning mechanism called Precise-
Spike-Driven (PSD) synaptic plasticity, that is able to learn the association between
precisespikepatterns.SimilartoReSuMe[13]andSPAN[15],thePSDruleisderived
from the Widrow-Hoff (WH) rule but based on a different interpretation. The PSD
rule is derived analytically based on converting the spike trains into analog signals
by applying the spike convolution method. Such an approach is rarely reported in
the existing learning rule studies [15]. Synaptic adaptation in the PSD is driven by
the error between the desired and the actual output spikes, with positive errors caus-
ing long-term potentiation (LTP) and negative errors causing long-term depression
(LTD). The amount of adaptation depends on an eligibility trace determined by the
afferent spikes. Without complex error calculation, the PSD rule provides an efﬁ-
cient way for processing spatiotemporal patterns. We show that the PSD rule inherits
the advantageous properties of both arithmetic-based and biologically realistic rules,
being simple and efﬁcient for computation, and yet biologically plausible. Further-
more, the PSD is an independent plasticity rule that can be applied to different neuron
models. This straightforward interpretation of the WH rule also provides a possible
direction for further exploitation of the rich theory of ANNs, and minimizes the gap
between the learning algorithms of SNNs and the traditional ANNs.
Various properties of the PSD rule are investigated through an extensive exper-
imental analysis. In the ﬁrst experiment, the basic concepts of the PSD rule are
demonstrated, and its learning ability on hetero-association of spatiotemporal spike
pattern is investigated. In the second experiment, the PSD rule is shown to be
applicable to different neuron models. Thereafter, experiments are conducted to ana-
lyze the learning rule regarding its robustness against noisy conditions, its memory

4.1 Introduction
67
capacity,effectsofthelearningparametersanditsclassiﬁcationperformance.Finally,
a detailed discussion about the PSD rule and several related algorithms is presented.
4.2
Methods
In this section, we begin by presenting the spiking neuron models. We then describe
the PSD rule for learning hetero-association between the input spatiotemporal spike
patterns and the desired spike trains.
4.2.1
Spiking Neuron Model
The LIF model is ﬁrstly considered. The dynamics of each neuron evolves according
to the following equation:
τm
dVm
dt
= −(Vm −E) + (Ins + Isyn) · Rm
(4.1)
where Vm is the membrane potential, τm = RmCm is the membrane time constant,
Rm = 1MΩ and Cm = 10nF are the membrane resistance and capacitance, respec-
tively, E is the resting potential, Ins and Isyn are the background current noise
and synaptic current, respectively. When Vm exceeds a constant threshold Vthr, the
neuron is said to ﬁre, and Vm is reset to Vreset for a refractory period tref . We
set E = Vreset = 0mV and Vthr = E + 18mV for clarity, but any other values as
E = −60mV will result in equivalent dynamics as long as the relationships among
E, Vreset and Vthr are kept.
For the post-synaptic neuron, we model the input synaptic current as:
Isyn(t) =

i
wi I i
PSC(t)
(4.2)
where wi is the synaptic efﬁcacy of the i-th afferent neuron, and I i
PSC is the un-
weighted postsynaptic current from the corresponding afferent.
I i
PSC(t) =

t j
K(t −t j)H(t −t j)
(4.3)
where t j is the time of the j-th spike emitted from the i-th afferent neuron, H(t)
refers to the Heaviside function, K denotes a normalized kernel and we choose it as:
K(t −t j) = V0 ·

exp(−(t −t j)
τs
) −exp(−(t −t j)
τ f
)

(4.4)

68
4
Precise-Spike-Driven Synaptic Plasticity …
Fig. 4.1 Illustration of the neuron structure. The afferent neurons are connected to the post-synaptic
neuron through synapses. Each emitted spike from afferent neurons will trigger a post-synaptic
current(PSC).Themembranepotentialofthepost-synapticneuronisaweightedsumofallincoming
PSCs from all afferent neurons. The yellow neuron denotes the instructor which is used for learning
where V0 is a normalization factor such that the maximum value of the kernel is 1,
τs and τ f are the slow and fast decay constants respectively, and their ratio is ﬁxed
at τs/τ f = 4.
Figure4.1 illustrates the neuron structure. Each spike from the afferent neuron will
result in a post-synaptic current (PSC). The membrane potential of the post-synaptic
neuron is a weighted sum of all incoming PSCs over all afferent neurons.
In addition to the LIF model, we also investigate the ﬂexibility of the PSD rule to
different neuron models. For this, we use the IM model [18], where the dynamics of
the IM model is described as:
⎧
⎪⎪⎨
⎪⎪⎩
dVm/dt = 0.04V 2
m + 5Vm + 140 −U + Isyn + Ins
dU/dt = a(bVm −U)
if Vm ≥30 mV ,
then Vm ←c, U ←U + d
(4.5)
where Vm again represents the membrane potential. U is the membrane recovery
variable. The synaptic current (Isyn) is in the same form as described before, and Ins
again represents the background noise. The parameters a = 0.02, b = 0.2, c = −65
and d = 8 are chosen such that the neuron exhibits a regular spiking behavior which
is the most typical behavior observed in cortex [18].
For computational efﬁciency, the LIF model is used in the following studies of
this chapter, unless otherwise stated.
4.2.2
PSD Learning Rule
In this section we describe in detail the PSD learning rule. Note that the spiking
neuron models were developed from the traditional neuron models. In a similar way,
we develop the learning rule for spiking neurons from traditional algorithms. Inspired

4.2 Methods
69
by [15], we derive the proposed rule from the common WH rule. The WH rule is
described as:
Δwi = ηxi(yd −yo)
(4.6)
where η is a positive constant referring to the learning rate, xi, yd and yo refer to the
input, the desired output and the actual output, respectively.
Note that because the WH rule was introduced for the traditional neuron models
such as perceptron, the variables in the WH rule are regarded as real-valued vectors.
In the case of spiking neurons, the input and output signals are described by the
timing of spikes. Therefore, a direct implementation of the WH rule does not work
for spiking neurons. This motivates the development of the PSD rule.
A spike train is deﬁned as a sequence of impulses triggered by a particular neuron
at its ﬁring time. A spike train is expressed in the form of:
s(t) = Σ f δ(t −t f )
(4.7)
where t f is the f -th ﬁring time, and δ(x) is the Dirac function: δ(x) = 1 (if x = 0) or
0 (otherwise). Thus, the input, the desired output and the actual output of the spiking
neuron are described as:
⎧
⎨
⎩
si(t) = Σ f δ(t −t f
i )
sd(t) = Σgδ(t −t g
d )
so(t) = Σhδ(t −th
o )
(4.8)
The products of Dirac functions are mathematically problematic. To solve this
difﬁculty, we apply an approach called spike convolution. Unlike the method used
in [15], which needs a complex error evaluation and requires spike convolution on
all the spike trains of the input, the desired output and the actual output, we only
convolve the input spike trains.
˜si(t) = si(t) ∗κ(t)
(4.9)
where κ(t) is the convolving kernel, which we choose to be the same as Eq.(4.4). In
this case, the convolved signal is in the same form as IPSC in Eq.(4.3). Thus, we use
IPSC as the eligibility trace for the weight adaptation. The learning rule becomes:
dwi(t)
dt
= η[sd(t) −so(t)]I i
PSC(t)
(4.10)
Equation(4.10) formulates an online learning rule. The dynamics of this learning
rule is illustrated in Fig.4.2. It can be seen that the polarity of the synaptic changes
depends on three cases: (1) a positive error (corresponding to a miss of the spike)
where the neuron does not spike at the desired time, (2) a zero error (correspond-
ing to a hit) where the neuron spikes at the desired time, and (3) a negative error
(corresponding to a false-alarm) where the neuron spikes when it is not supposed to.

70
4
Precise-Spike-Driven Synaptic Plasticity …
Fig. 4.2 Demonstration of the weight adaptation in PSD. Si(t) is the presynaptic spike train.
Sd(t) and So(t) are the desired and the actual postsynaptic spike train, respectively. I i
PSC(t) is the
postsynaptic current and can be referred to as the eligibility trace for the adaptation of wi(t). A
positive error, where the neuron does not spike at the desired time, causes synaptic potentiation. A
negative error, where the neuron spikes when it is not supposed to, results in synaptic depression.
The amount of adaptation is proportional to the postsynaptic current. There will be no modiﬁcation
when the actual output spike ﬁres exactly at the desired time
Thus, the weight adaptation is triggered by the error between the desired and the
actual output spikes, with positive errors causing long-term potentiation and negative
errors causing long-term depression. No synaptic change will occur if the actual
output spike ﬁres at the desired time. The amount of synaptic changes is determined
by the current I i
PSC(t).
With the PSD learning rule, each of the variables involved has its own physical
meaning. Moreover, the weight adaptation only depends on the current states. This is
different from rules involving STDP, where both the pre- and post-synaptic spiking
times are stored and used for adaptation.
By integrating Eq.(4.10), we get:
Δwi = η
	 ∞
0
[sd(t) −so(t)]I i
PSC(t)dt
(4.11)
= η

 
g

f
K(t g
d −t f
i )H(t g
d −t f
i ) −

h

f
K(th
o −t f
i )H(th
o −t f
i )

This equation could be used for trial learning where the weight modiﬁcation is
performed at the end of the pattern presentation.
In order to measure the distance between two spike trains, we use the van Rossum
metric [19] but with a different ﬁlter function as described in Eq.(4.4). This ﬁlter is
used to compensate for the discontinuity of the original ﬁlter function. The distance
can be written as:
Dist = 1
τ
	 ∞
0
[ f (t) −g(t)]2dt
(4.12)
where τ is a free parameter (we set τ = 10ms here), f (t) and g(t) are ﬁltered signals
of the two spike trains that are considered for distance measurement.

4.2 Methods
71
Noteworthily, this distance parameter Dist is not involved in the PSD learning
rule, but is used for measuring and analyzing the performance of the learning rule,
which reﬂects the dissimilarity between the desired and the actual spike trains. In the
following experiments, different values of Dist are used for analysis depending on
the problems. For single-spike and multi-spike target trains, we set Dist to be 0.2
and 0.5, respectively, corresponding to an average time difference of around 2.5ms
for each pair of the actual and desired spikes. Smaller Dist can be used if exact
association is the main focus, e.g., Dist = 0.06 corresponds to a time difference
about 0.6ms, where no obvious dissimilarity can be seen between the two spike
trains.
4.3
Results
In this section, several experiments are presented to demonstrate the characteristics
of the PSD rule. The basic concepts of the PSD rule are ﬁrst examined, by demon-
strating its ability to associate a spatiotemporal spike pattern with a target spike train.
Furthermore, we show that the PSD has desirable properties, such as generality to
different neuron models, robustness against noise and learning capacity. The effects
of the parameters on the learning are also investigated. Then, the application of the
proposed algorithm to the classiﬁcation of spike patterns is also shown.
4.3.1
Association of Single-Spike and Multi-spike Patterns
This experiment is devised to demonstrate the ability of the proposed PSD rule for
learning a spatiotemporal spike pattern. The neuron is trained to reproduce spikes
that ﬁre at the same spiking time of a target train.
4.3.1.1
Experiment Setup
The neuron is connected with n afferent neurons, and each ﬁres a single spike within
the time interval of (0, T ). Each spike is randomly generated with a uniform distri-
bution. We set n = 1000, T = 200ms here. To avoid a single synapse dominating
the ﬁring of the neuron, we limit the weight below wmax = 6nA. The initial synaptic
weights are drawn randomly from a normal distribution with mean value of 0.5nA
and a standard deviation of 0.2nA. For the learning parameters, we set η = 0.01wmax
and τs = 10ms. The target spike train can be randomly generated, but for simplicity,
we specify it as [40, 80, 120, 160]ms to evenly distribute the spikes over the whole
time interval T .

72
4
Precise-Spike-Driven Synaptic Plasticity …
4.3.1.2
Learning Process
Figure4.3 illustrates a typical run of the learning. Initially, the neuron is observed to
ﬁre at any arbitrary time and with a ﬁring rate different from the target train, resulting
in a large distance value. The actual output spike train is quite different from the target
train at the beginning. During the learning process, the neuron gradually learns to
produce spikes at the target time, and that is also reﬂected by the decreasing distance.
After ﬁnishing the ﬁrst 10 epochs of learning, both the ﬁring rate and the ﬁring time
of the output spikes match those in the target spike train. The dynamics of neuron’s
membrane potential is also shown in Fig.4.3. Whenever the membrane potential
exceeds the threshold, a spike is emitted and the potential is kept at reset level for
a refractory period. The detailed mathematical description governing this behavior
was presented previously in the section on the Spiking Neuron Model.
This experiment shows the feasibility of the PSD rule to train the neuron to repro-
duce a desired spike train. After several learning epochs, the neuron can successfully
spike at the target time. In other words, the proposed rule is able to train the neuron
to associate the input spatiotemporal pattern with a desired output spike train within
Fig. 4.3 Illustration of the temporal sequence learning of a typical run. The neuron is connected
with n = 1000 synapses, and is trained to reproduce spikes at the target time (denoted as light blue
bars in the middle). The bottom and top show the dynamics of the neuron’s potential before and
after learning, respectively. The dashed red lines denote the ﬁring threshold. In the middle, each
spike is denoted as a dot. The right ﬁgure shows the spike distance between the actual output spike
train and the target spike train

4.3 Results
73
several training epochs. The information of the input pattern is stored by a speciﬁed
spike train.
4.3.1.3
Causal Weight Distribution
We further examine how the PSD rule drives the synaptic weights and the evolution
of the distance between the actual and the target spike trains. In order to guarantee
statistical signiﬁcance, the task described in Fig.4.3 is repeated 100 times. Each time
is referred to as one run. At the initial point of each run, different random weights
are used for training.
As can be seen from Fig.4.4, the initial weights are normally distributed around
0.5nA, which reﬂects the fact that there are no signiﬁcant differences among the
input synapses. This initial distribution of weights is expected due to the experimental
setup. After learning, a causal connectivity is established. According to the learning
rule, the synapses that ﬁre temporally close to the time of the target spikes are
potentiated. Those synapses that result in undesired output spikes are depressed. This
temporal causality is clearly reﬂected on the distribution of weights after learning
(Fig.4.4). Among those causal synapses, the one with a closer spiking time to the
desired time normally has a relatively higher synaptic strength. The synapses ﬁring
Fig. 4.4 Effect of the learning on synaptic weights and the evolution of distance along the learning
process. The top and the middle show the averaged weights before and after learning, respectively.
The height of each bar in the ﬁgure reﬂects the corresponding synaptic strength. All the afferent
neurons are chronologically sorted according to their spike time. The target spikes are overlayed
on the weights ﬁgure according to their time, and are denoted as red lines. The bottom shows the
averaged distance between the actual spike train and the desired spike train along the learning
process. All the data are averaged over 100 runs

74
4
Precise-Spike-Driven Synaptic Plasticity …
far from the desired time will have lower causal effects. Additionally, the evolution
of distance along the learning shows that the PSD rule successfully trains the neuron
to reproduce the desired spikes in around ten epochs. The results also validate the
efﬁciency of the PSD learning rule in accomplishing the single association task.
4.3.1.4
Adaptive Learning Performance
At the beginning, the neuron is trained to learn a target train as in the previous tasks.
After one successful learning, the target spike train is changed to another arbitrarily
generated train, where the precise spike time and the ﬁring rate are different from
the previous target. We discover that, with the PSD learning rule, we successfully
train the neuron to learn the new target within several epochs. As shown in Fig.4.5,
during learning, the neuron gradually adapts its ﬁring status from the old target to
the new target.
4.3.1.5
Learning Multiple Spikes
In the scenario considered above, all afferent neurons are supposed to ﬁre only once
during the entire time window. The applicability of the PSD rule is not limited to this
single spike code. We further illustrate the case where each synaptic input transmits
multiple spikes during the time window. We again use the same setup as above,
Fig. 4.5 Illustration of the adaptive learning of the changed target trains. Each dot denotes a spike.
At the beginning, the neuron is trained to learn one target (denoted by the light blue bars). After 25
epochs of learning (the dashed red line), the target is changed to another randomly generated train
(denoted by the green bars). The right ﬁgure shows the distance between the actual output spike
train and the target spike train along the learning process

4.3 Results
75
Ti
Fig. 4.6 Illustration of a typical run for learning multi-spike pattern. Each dot denotes a spike. The
top left shows the input spikes from the ﬁrst 50 afferent neurons out of 1000. Each synaptic input
is generated by a homogeneous Poisson process with a random rate from 5–25Hz. The bottom left
shows the neuron’s output spikes. The right column shows the distance between the actual output
spike train and the target spike train along learning
but each synaptic input is now generated by a homogeneous Poisson process with
a random rate ranging from 5–25Hz. Multiple spikes increase the difﬁculty of the
learning since these spikes interfere with the local learning processes [17].
As shown in Fig.4.6, the learning although slower, is again successful. The inter-
ference of local learning processes results in ﬂuctuations of the output spikes around
the target time. In the subsequent learning epochs, the neuron gradually converges
to spiking at the target time. This experiment demonstrates that the PSD rule deals
with multiple spikes quite well. Compared to multiple spikes, the single spike code
is simple for analysis and efﬁcient for computation. Thus, for simplicity, we use the

76
4
Precise-Spike-Driven Synaptic Plasticity …
single spike code in the following experiments where each afferent neuron ﬁres only
once during the time window.
The above experiments clearly demonstrate that the PSD rule is capable of training
the neuron to ﬁre at the desired time. The causal connectivity is established after
learning with this rule. In the following sections, some more challenging learning
scenarios are taken into consideration to further investigate the properties of the PSD
rule.
4.3.2
Generality to Different Neuron Models
We carry out this experiment to demonstrate that the PSD learning rule is independent
of the neuron model. In this experiment, we only compare the results of learning
association for the LIF and IM neuron models that were described previously. For a
faircomparison,bothneuronsareconnectedtothesameafferentneurons,andtheyare
trained to reproduce the same target spike train. The setup for generating the input
spatiotemporal patterns is the same as the experiment in Fig.4.5. The connection
setup is illustrated in Fig.4.7. Except for the neuron dynamics described in Eqs.(4.1)
and (4.5) respectively, all the other parameters are the same for the two neurons.
The dynamic difference between the two types of spiking neuron models is clearly
demonstrated in Fig.4.7. Although the neuron models are different, both of the neu-
rons can be trained to successfully reproduce the target spike train with the proposed
Fig. 4.7 Learning with different spiking neuron models. The LIF and IM neuron models are
considered. The left panel shows the connection setup of the experiment. Both the two neurons
are connected to the same n = 1000 afferent neurons, and are trained to reproduce target spikes
(denoted by the yellow parts). The right panel shows the dynamics of neurons’ potential before and
after learning. The dashed red lines denote the ﬁring threshold

4.3 Results
77
PSD learning rule. It is seen that the two neurons ﬁre at arbitrary time before learning,
while after learning they ﬁre spikes at the desired time.
In the PSD rule, synaptic adaptation is triggered by both the desired spikes and
the actual output spikes. The amount of updating depends on the presynaptic spikes
ﬁring before the triggering spikes. That is to say, the weight adaptation of our rule is
based on the correlation between the spiking time only. This suggests the PSD has
the generality to work with various neuron models, a capability similar to that of the
ReSuMe rule [17].
4.3.3
Robustness to Noise
In previous experiments, we only consider the simple case where the neuron is trained
to learn a single pattern under noise-free condition. However, the reliability of the
neuron response could be signiﬁcantly affected by noise. In this experiment, two
noisy cases are considered: stimuli noise and background noise.
4.3.3.1
Experiment Setup
In this experiment, a single LIF neuron with n = 500 afferent neurons is tested.
Initially, a set of 10 spike patterns are randomly generated as in previous experiments.
These 10 spike patterns are ﬁxed as the templates. The neuron is trained for 400
epochs to associate all patterns in the training set with a desired spike train (the same
train as is used before). Two training scenarios are considered in this experiment,
i.e., deterministic training (in the noise-free condition) and noisy training. In the
testing phase, a total number of 200 noise patterns are used. Each template is used
to construct 20 testing patterns. We determine the association to be correct, if the
distance between the output spike train and the desired spike train is lower than a
speciﬁed level (0.5 is used here).
4.3.3.2
Input Jittering Noise
In the case of input jittering noise, a Gaussian jitter with a standard deviation (σInp)
is added to each input spike to generate the noise patterns. The strength of the jitter
is controlled by the standard deviation of the Gaussian. The top row in Fig.4.8
shows the learning performance. In the deterministic training, the neuron is trained
purely with the initial templates. In the noisy training, a noise level of 3ms is used.
Different levels of noise are used in the testing phase to evaluate the generalization
ability. For the deterministic training, the output stabilizes quickly and can exactly
converge to the desired spike train within tens of learning epochs. However, the
generalization accuracy decreases quickly with the increasing jitter strength. In the
scenario of noisy training, although the training error cannot become zero, a better

78
4
Precise-Spike-Driven Synaptic Plasticity …
n
Fig. 4.8 Robustness of the learning rule against jittering noise of input stimuli and background
noise. The top row presents the case where the noise comes from the input spike jitters. The bottom
row presents the case of background noise. The neuron is trained under noise-free conditions
(denoted as deterministic training), or is trained under noisy conditions (denoted as noisy training).
In the training phase (left two columns), the neuron is trained for 400 epochs. Along the training
process, the average distance between the actual output spike train and the desired spike train is
shown. The standard deviation is denoted by the shaded area. In the testing phase (right column), the
generalization accuracies of the trained neuron on different levels of noise patterns are presented.
Both the average value and the standard deviation are shown. All the data are averaged over 100
runs
generalization ability is obtained. The neuron can successfully reproduce the desired
spike train with a relatively high accuracy when the noise strength is not higher than
the one used in the training. In conclusion, the neuron is less sensitive to the noise if
the noisy training is performed.
4.3.3.3
Background Current Noise
In this case, the background current noise (Ins) is considered as the noise source. The
mean value of Ins is assumed zero, and the strength of the noise is determined by
its variance (σIns). A strength of 10nA noise is used in the noisy training. We report
the results in the bottom row of Fig.4.8. Similar results are obtained as with the ﬁrst
case. Although the output can quickly converge to zero error in the deterministic
training, the generalization performance is quite sensitive to the noise. The associa-
tion accuracy drops quickly when the noise strength increases. When the neuron is
trained with noise patterns, it becomes less sensitive to the noise. A relatively high
accuracy can be obtained with a noise level up to 14nA.

4.3 Results
79
This experiment shows that the neuron trained under noise-free conditions will
be signiﬁcantly affected by noise in the testing phase. Such an inﬂuence of noise on
the timing accuracy and reliability of the neuron response has been considered in
many studies [5, 14, 15, 17, 20, 21]. Under the noisy training, the trained neuron
demonstrates high robustness against the noise. The noisy training enables the neuron
to reproduce desired spikes more reliably and precisely.
4.3.4
Learning Capacity
As used for the perceptron [22] and tempotron [5, 16] learning rules, the ratio of the
number of random patterns (p) that a neuron can correctly classify over the number
of its synapses (n), α = p/n, is used to measure the memory load. An important
characteristic of a neuron’s capacity is the maximum load that it can learn. In this
experiment, the memory capacity of the PSD rule is investigated.
4.3.4.1
Experiment Setup
We devise an experiment that has a similar setup to that in [15]. A number of p
patterns are randomly generated in the same process as previous experiments, where
each pattern contains n spike trains and each train has a single spike. The patterns
are randomly and evenly assigned to c different categories. Here we choose c = 4
for this experiment. A single LIF neuron is trained to memorize all patterns correctly
in a maximum number of 500 training epochs. The neuron is trained to emit a single
spike at a speciﬁed time for patterns from each category. The desired spikes for the
4 generated categories are set to the time of 40, 80, 120 and 160ms, respectively. A
pattern is considered to have been correctly memorized by the neuron if the distance
between the actual spike train and the desired train is below 0.2. The learning process
is considered a failure if the number of training epochs reaches the maximum number.
4.3.4.2
Maximum Load Factor
Figure4.9 shows the results of the experiment for the case of 500, 750 and 1000
afferent neurons, respectively. All the data are averaged over 100 runs. In each run,
differentinitialweightsareused.AsseenfromFig.4.9,thenumberofepochsrequired
for the training increases slightly as the number of patterns increases when the load
is not too high, but a sharp increase of learning epochs occurs after a certain high
load. This suggests that the task becomes tougher with an increasing load. It is also
noted that a larger number of synapses leads to a bigger memory capacity for the
same neuron. It is reported that the maximum load factors for 500, 750 and 1000
synapses are 0.144, 0.133 and 0.124, respectively.

80
4
Precise-Spike-Driven Synaptic Plasticity …
Fig. 4.9 The memory capacity of the PSD rule with different numbers of synapses. The neuron is
trained to memorize all patterns correctly in a maximum number of 500 epochs. The reaching points
of 500 epochs are regarded as failure of the learning. The marked lines denote average learning
epochs and the shaded areas show the standard deviation. The dashed line at 100 epochs is used for
evaluating the efﬁcient load αe described in the main text. All the data are averaged over 100 runs
4.3.4.3
Efﬁcient Load Factor
Besides the maximum load factor, we heuristically deﬁne another factor, the efﬁcient
load αe. The neuron can learn patterns efﬁciently with a relatively high load when the
number of patterns does not exceed a certain value (pe). The efﬁcient load factor is
denote as αe = pe/n. When the load is below αe, the neuron can reliably memorize
all patterns with a small number of training epochs. There are different ways to deﬁne
αe. We show two possible ways. One is to derive the deﬁnition from a mathematical
calculation such as (dEpochs/dp)pe = δ, where δ is a speciﬁed value (for example
δ = 0.5). A simpler method is where a speciﬁed number of training epochs is used.
The corresponding number of patterns that can be correctly learnt is considered as
pe. For simplicity, we use the latter as an example for demonstration and the speciﬁed
number of epochs is set to 100. As seen from Fig.4.9, the efﬁcient load factors for
500, 750 and 1000 synapses are 0.112, 0.109 and 0.108, respectively. Surprisingly,
these efﬁcient load factors seem to all be around a stable value which only changes
slightly across different numbers of synapses. This ﬁxed value of efﬁcient load factor
for different values of n indicates that the number of patterns that a neuron can
efﬁciently memorize grows linearly with the number of afferent synapses. It is worth
noting that the concept of efﬁcient load factor αe provides an important guideline
for choosing the load of patterns when a reliable and efﬁcient training is required.

4.3 Results
81
4.3.5
Effects of Learning Parameters
Two of the major parameters involved in the PSD learning rule are the learning rate
η and the decay constant τs. In this section, we aim to investigate the effects of these
parameters on the learning process.
4.3.5.1
Small τs Results in Strong Causal Weight Distribution
As a decay constant, τs is an important parameter involved in the postsynaptic cur-
rent. It determines how long a presynaptic spike will still have causal effect on the
postsynaptic neuron. In the phase of synaptic adaptation, τs also determines the mag-
nitude of modiﬁcation on the synaptic weights at the time of a triggering spike. Thus,
τs will affect the distribution of weights after the training. To look into this effect, we
conduct an experiment with a similar setup as in Fig.4.4 but with different values of
τs. Here we choose τs = 3, 10 and 30ms. As can be seen from Fig.4.10, a smaller
τs (3ms) can result in a very uneven distribution with only a few synapses being
given relatively higher weights. A ﬂat distribution is obtained with an increasing τs.
This is because τs determines how long the causal effect of an afferent spike will
sustain. A smaller τs means that only the nearer neighbors are involved in generating
the desired spikes, hence resulting in a smaller number of causal synapses. With a
smaller number of causal synapses, a higher synaptic strength will be required to
generate spikes at the desired time. On the other hand, with a larger τs, a wider range
Fig. 4.10 Effect of decay constant τs on the distribution of weights. The averaged weights after
learning are shown. The height of each bar reﬂects the synaptic strength. The afferent neurons are
chronologically sorted according to their spike time. The target spikes are overlayed and denoted
as red lines. Cases of τs = 3, 10 and 30ms are depicted. All the data are averaged over 100 runs

82
4
Precise-Spike-Driven Synaptic Plasticity …
of causal neighbors can contribute to generating the desired spikes, and therefore a
lower synaptic strength will be sufﬁcient. The synaptic strength and distribution for
different values of τs are obtained as in Fig.4.10.
4.3.5.2
Effects of both η and τs on the Learning
We further conduct another experiment to evaluate the effects of both η and τs on
the learning. In this experiment, a single LIF neuron with n = 500 afferent neurons
is considered. The neuron is trained to correctly memorize a set of 10 spike patterns
randomly generated over a time window of 200ms. The neuron is trained in a max-
imum number of 500 epochs to correctly associate all these patterns with a desired
spike train of [40, 80, 120, 160]ms. We denote that a pattern is correctly memorized
if the distance between the output spike train and the desired spike train is below
0.06. If the number of training epochs exceeds 500, we regard it as a failure. We
conduct an exhaustive search over a wide range of η and τs. Figure4.11 shows how
η and τs jointly affect the learning performance, which can be used as a guidance to
select the learning parameters. With a ﬁxed τs, a larger η results in a faster learning
speed (shown in Fig.4.11, right panel), but when η is increased above a critical value
(e.g., 0.1 for τs = 30ms in our experiments), the learning will slow down or even
fail. For small η, a larger τs leads to a faster learning, however, for large η, a larger
τs has the opposite effect. As a consequence, when τs is set in a suitable range (e.g.,
[5, 15]ms), a wide range of η can result in a fast learning speed (e.g., below 100
epochs).
Fig. 4.11 Effects of η and τs on the learning. The neuron is trained in a maximum number of 500
epochs to correctly memorize a set of 10 spike patterns. The average learning epochs are recorded
for each pair of η and τs. The reaching points of 500 epochs are regarded as failure of the learning.
The left shows an exhaustive investigation of a wide range of η and τs, and the data are averaged
over 30 runs. A small number of learning parameters are examined in the right ﬁgure, and the data
are averaged over 100 runs

4.3 Results
83
4.3.6
Classiﬁcation of Spatiotemporal Patterns
In this experiment, the ability of the proposed PSD rule for classifying spatiotemporal
patterns is investigated by using a multi-category classiﬁcation task. The setup of
this experiment is similar to that in [15]. Three random spike patterns representing
three categories are generated in a similar fashion to that in the previous experiments,
and they are ﬁxed as the templates. A Gaussian jitter with a standard deviation of
3ms is used to generate training and testing patterns. The training set and the testing
set contain 3 × 25 and 3 × 100 samples, respectively. Three neurons are trained to
classify these three categories, with each neuron representing one category. Different
neurons for each category can be speciﬁed to ﬁre different spike trains. However,
for simplicity, all the neurons in this experiment are trained to ﬁre the same spike
train ([40, 80, 120, 160]ms). The experiment is repeated 100 times, with each run
having different initial conditions.
Aftertraining,classiﬁcationisperformedonboththetrainingandthetestingset.In
the classiﬁcation task, we propose two decision-making criteria: absolute conﬁdence
and relative conﬁdence. With the absolute conﬁdence criterion, only if the distance
between the desired spike train and the actual output spike train of the corresponding
neuron is smaller than a speciﬁed value (0.5 is used here), then the input pattern will
be regarded as being correctly classiﬁed. As for the relative conﬁdence criterion, a
scheme of competition is used. The incoming pattern will be labeled by the winning
neuron that produces the closest spike train to its desired spike train.
Figure4.12 shows the average classiﬁcation accuracy for each category under the
two proposed decision criteria. From the absolute conﬁdence criterion, we see that the
neuron successfully classiﬁes the training set with an average accuracy of 99.65%.
Fig. 4.12 The average accuracies for the classiﬁcation of spatiotemporal patterns. There are 3
categories to be classiﬁed. The average accuracies are represented by shaded bars. Two types of
criteria for making decision are proposed and investigated. The left is the absolute conﬁdence
criterion, and the right is the relative conﬁdence criterion. All the data are averaged over 100 runs

84
4
Precise-Spike-Driven Synaptic Plasticity …
Table 4.1 Multi-category classiﬁcation of spatiotemporal patterns
Accuracy
(%)
Category 1
Category 2
Category 3
Training
Testing
Training
Testing
Training
Testing
Absolute
conﬁdence
99.6
83.15
99.68
80.06
99.68
68.12
±1.21
±6.79
±1.09
±4.73
±1.23
±6.09
Relative
conﬁdence
100
100
100
100
100
100
Tempotron
100
99.65
100
99.74
100
99.61
±1.21
±1.01
±1.0
The average accuracy for the testing set is 77.11%. Noteworthily, under the relative
conﬁdence, both the average accuracies for the training and the testing set reach
100%. The performance for the classiﬁcation task is therefore signiﬁcantly improved
by the relative conﬁdence decision making criterion. With the absolute conﬁdence
criterion, the trained neuron strives to ﬁnd a good match with the memorized patterns.
However, with the relative conﬁdence criterion, the trained neuron attempts to ﬁnd
the most likely category through competition.
For the classiﬁcation of spatiotemporal patterns, the tempotron is an efﬁcient
rule [5] in training LIF neurons to distinguish two classes of patterns by ﬁring one
spike or by keeping quiescent. We use the tempotron rule to benchmark the PSD
rule in the classiﬁcation of spatiotemporal patterns. The tempotron rule is applied
to perform the same classiﬁcation task as above. The classiﬁcation accuracies are
shown in Table4.1. As can be seen from Table4.1, our proposed rule with the relative
conﬁdence criterion has a comparable performance to the tempotron rule. Moreover,
the PSD rule is advantageous in that it is not limited to performing classiﬁcation, but
it is also able to memorize patterns by ﬁring desired spikes at precise time.
4.4
Discussion and Conclusion
The PSD rule is proposed for the association and recognition of spatiotemporal
spike patterns. In summary, the PSD rule transforms the input spike trains into analog
signalsbyconvolvingthespikeswithakernelfunction.Byusingakernelfunction,the
analog signals are presented in the simple form of synaptic currents. It is biologically
plausible because it allows us to interpret the signals with physical meaning. Synaptic
adaptation is driven by the error between the desired and the actual output spikes, with
positive errors causing LTP and negative errors causing LTD. The amount of synaptic
adaptation is determined by the transformed signal of the input spikes (postsynaptic
currents here) at the time of modiﬁcation occurrence. When the actual spike train is
the same as the desired spike train, the adaptation of the weights will be terminated.

4.4 Discussion and Conclusion
85
There is a supervisory signal involved in the PSD rule. The most documented
evidence for supervised rules comes from studies of the cerebellum and the cerebellar
cortex [8, 9]. It is shown that supervisory signals are provided to the learning modules
by sensory feedback [10] or other supervisory neural structures in the brain [9].
A neuromodulator released by the supervisory system can induce the control of
the adaptation. This control occurs for several neuromodulatory pathways, such as
dopamine and acetylcholine [23, 24]. Experimental evidence shows that N-methyl-
D-aspartate (NMDA) receptors are critically involved in the processes of LTP and
LTD [25–27]. After opening the NMDA channels, the resulting Ca2+ entry then
activates the biochemistry of potentiation which leads to LTP [27]. Suppression of
NMDA receptors by spike-mediated calcium entry may be a necessary step in the
induction of LTD [27, 28]. The synaptic modiﬁcation can be implemented through
a supervisory control of opening or suppression of these NMDA channels.
The PSD rule is simple and efﬁcient in synaptic adaptation. Utilizing the postsy-
naptic current as the eligibility trace for weight adaptation is a simple and efﬁcient
choice. The same signals of postsynaptic currents are also used in the synaptic adap-
tation as in the neuron dynamics, unlike the learning rules such as [12, 15, 17] where
different sources of signals were used. Thus, the number of signal sources involved
in the learning is reduced, which will directly beneﬁt the computation. Secondly,
unlike the arithmetic-based rules [12, 14, 15], where a complex error calculation
is required for the synaptic adaptation, the PSD rule is based on a simple form of
spike error between the actual and the desired spikes. The synaptic adaptation is
driven by these precise spikes without complex error calculation. As a matter of fact,
the weight modiﬁcation only depends on currently available information (shown as
Fig.4.2). Additionally, due to the ability of the PSD rule to operate online, it is suit-
able for real-time applications. According to the PSD rule, different kernels, such
as the exponential kernel and α kernel, can also be used in convolving the spikes to
provide different eligibility traces.
The PSD rule is designed for processing spatiotemporal patterns, where the exact
time of each spike is used for information transmission. The PSD rule is unsuitable
for learning patterns under the rate code because this rule is designed to process
precise-timing spikes by its nature. The rate code uses the spike count but not the
precise time to convey information. Like other spatiotemporal mapping algorithms,
including ReSuMe [13], Chronotron [14] and SPAN [15], the PSD rule cannot guar-
antee successful learning of an arbitrary spatiotemporal spike pattern. A sufﬁcient
number of input spikes around the desired time are required for establishing causal
connections. In other words, the temporal range covered by the desired spikes should
be covered by the input spikes.
In most of the experiments, a single spike code is used for afferent neurons, where
each input neuron only ﬁres a single spike during the entire time window. This single
spike code is chosen for various reasons but more than one spike is also allowed
for the PSD rule. Firstly, a single spike code is simple for analysis and efﬁcient for
computation.Secondly,thereisstrongbiologicalevidencesupportingthesinglespike
code. The PSD rule is also suitable for multi-spike train (results shown in Fig.4.6).
When the number of spikes from each afferent neuron is not high enough, the neuron

86
4
Precise-Spike-Driven Synaptic Plasticity …
can produce the desired spike train after several epochs. When the number of spikes
increases, the learning becomes slower and more difﬁcult to converge. Additionally,
the biological plausibility of an encoding scheme that can use multiple spikes to code
information is still unclear.
References
1. Gerstner, W., Kistler, W.M.: Spiking Neuron Models: Single Neurons, Populations, Plasticity,
1st edn. Cambridge University Press, Cambridge (2002)
2. Ghosh-Dastidar, S., Adeli, H.: Spiking neural networks. Int. J. Neural Syst. 19(04), 295–308
(2009)
3. Maass, W.: Networks of spiking neurons: the third generation of neural network models. Neural
Netw. 10(9), 1659–1671 (1997)
4. Shadlen, M.N., Movshon, J.A.: Synchrony unbound: review a critical evaluation of the temporal
binding hypothesis. Neuron 24, 67–77 (1999)
5. Gütig, R., Sompolinsky, H.: The tempotron: a neuron that learns spike timing-based decisions.
Nat. Neurosci. 9(3), 420–428 (2006)
6. Widrow, B., Lehr, M.: 30 years of adaptive neural networks: perceptron, madaline, and back-
propagation. Proc. IEEE 78(9), 1415–1442 (1990)
7. Knudsen, E.I.: Supervised learning in the brain. J. Neurosci. 14(7), 3985–3997 (1994)
8. Thach, W.T.: On the speciﬁc role of the cerebellum in motor learning and cognition: clues from
PET activation and lesion studies in man. Behav. Brain Sci. 19(3), 411–431 (1996)
9. Ito, M.: Mechanisms of motor learning in the cerebellum. Brain Res. 886(1–2), 237–245 (2000)
10. Carey, M.R., Medina, J.F., Lisberger, S.G.: Instructive signals for motor learning from visual
cortical area MT. Nat. Neurosci. 8(6), 813–819 (2005)
11. Brader, J.M., Senn, W., Fusi, S.: Learning real-world stimuli in a neural network with spike-
driven synaptic dynamics. Neural Comput. 19(11), 2881–2912 (2007)
12. Bohte, S.M., Kok, J.N., Poutré, J.A.L.: Error-backpropagation in temporally encoded networks
of spiking neurons. Neurocomputing 48(1–4), 17–37 (2002)
13. Ponulak, F.: ReSuMe-new supervised learning method for spiking neural networks. Institute
of Control and Information Engineering, Pozno´n University of Technology, Technical report
(2005)
14. Florian, R.V.: The chronotron: a neuron that learns to ﬁre temporally precise spike patterns.
PLoS One 7(8), e40,233 (2012)
15. Mohemmed, A., Schliebs, S., Matsuda, S., Kasabov, N.: SPAN: spike pattern association neuron
for learning spatio-temporal spike patterns. Int. J. Neural Syst. 22(04), 1250,012 (2012)
16. Yu, Q., Tang, H., Tan, K.C., Li, H.: Rapid feedforward computation by temporal encoding
and learning with spiking neurons. IEEE Trans. Neural Netw. Learn. Syst. 24(10), 1539–1552
(2013)
17. Ponulak, F., Kasinski, A.: Supervised learning in spiking neural networks with resume:
sequence learning, classiﬁcation, and spike shifting. Neural Comput. 22(2), 467–510 (2010)
18. Izhikevich, E.M.: Simple model of spiking neurons. IEEE Trans. Neural Netw. 14(6), 1569–
1572 (2003)
19. Rossum, M.: A novel spike distance. Neural Comput. 13(4), 751–763 (2001)
20. Rieke, F., Warland, D., van Steveninck, R.D., Bialek, W.: Spikes: Exploring the Neural Code,
1st edn. MIT Press, Cambridge (1997)
21. Hu, J., Tang, H., Tan, K.C., Li, H., Shi, L.: A spike-timing-based integrated model for pattern
recognition. Neural Comput. 25(2), 450–472 (2013)
22. Gardner, E.: The space of interactions in neural networks models. J. Phys. A21, 257–270 (1988)
23. Foehring, R.C., Lorenzon, N.M.: Neuromodulation, development and synaptic plasticity. Can.
J. Exp. Psychol./Rev. Canadienne de Psychologie Expérimentale 53(1), 45–61 (1999)

References
87
24. Seamans, J.K., Yang, C.R., et al.: The principal features and mechanisms of dopamine modu-
lation in the prefrontal cortex. Prog. Neurobiol. 74(1), 1–57 (2004)
25. Artola, A., Bröcher, S., Singer, W.: Different voltage-dependent thresholds for inducing long-
term depressiona and long-term potentiation in slices of rat visual cortex. Nature 347, 69–72
(1990)
26. Ngezahayo, A., Schachner, M., Artola, A.: Synaptic activity modulates the induction of bidi-
rectional synaptic changes in adult mouse hippocampus. J. Neurosci. 20(7), 2451–2458 (2000)
27. Lisman, J., Spruston, N.: Postsynaptic depolarization requirements for LTP and LTD: a critique
of spike timing-dependent plasticity. Nat. Neurosci. 8(7), 839–841 (2005)
28. Froemke, R.C., Poo, M.M., Dan, Y.: Spike-timing-dependent synaptic plasticity depends on
dendritic location. Nature 434(7030), 221–225 (2005)

Chapter 5
A Spiking Neural Network System
for Robust Sequence Recognition
Abstract This chapter presents a biologically plausible network architecture with
spiking neurons for sequence recognition. This architecture is a uniﬁed and con-
sistent system with functional parts of sensory encoding, learning and decoding.
This system is the ﬁrst attempt that helps to reveal the systematic neural mech-
anisms considering both the upstream and the downstream neurons together. The
whole system is consistently combined in a temporal framework, where the precise
timing of spikes is considered for information processing and cognitive comput-
ing. Experimental results show that our system can properly perform the sequence
recognition task with the integration of all three functional parts. The recognition
scheme is robust to noisy sensory inputs and it is also invariant to changes in the
intervals between input stimuli within a certain range. The classiﬁcation ability of
the temporal learning rule used in our system is investigated through two benchmark
tasks including an XOR task and an optical character recognition (OCR) task. Our
temporal learning rule outperforms other two benchmark rules that are widely used
for classiﬁcation. Our results also demonstrate the computational power of spiking
neurons over perceptrons for processing spatiotemporal patterns.
5.1
Introduction
As one of the cognitive abilities, sequence recognition refers to the ability to detect
and recognize the temporal order of discrete elements occurring in sequence. Such
sequence decoding operations are required for processing temporally complex stim-
uli such as speech where important information is embedded in patterns over time.
However, the biophysical mechanisms by which neural circuits detect and recognize
sequences of external stimuli are poorly understood.
Sequence information processing is a general problem that the brain needs to
solve. Several approaches with the design of traditional artiﬁcial neural network
structures [1, 2] have been considered and implemented for processing temporal
information. The functionality of the brain for sequence recognition is mimicked
through the artiﬁcial structures. However, these neural structures do not consider the
building units of spiking neurons. Recognizing sequences of external stimuli with
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_5
89

90
5
A Spiking Neural Network System for Robust Sequence Recognition
spiking features in the brain still remains an open question. Numerous studies have
put efforts separately to computational mechanisms with spiking neurons, where
some focus on neural representations of the external information [3] while others
focus on the internal procession of either upstream or downstream neurons [4–12].
Relatively few proposals exist for recognizing the sequence of incoming stimuli
from a systematic level of view. Thus, a structure based on spiking neural networks
is demanded. Such a spiking neural system for sequence recognition should contain
several functional parts including neural coding, learning and decoding. With these
functional parts integrating with each other, the system could process information
from levels of upstream encoding neurons to levels of downstream decoding neurons.
Among several different temporal learning rules, without complex error calcu-
lation, the PSD rule is simple and efﬁcient from the computational point of view,
and yet biologically plausible [9]. In the classiﬁcation of spatiotemporal patterns, the
PSD rule can even outperform the efﬁcient tempotron rule [9]. Moreover, the PSD
rule is not limited to the classiﬁcation, but can also train the neuron to associate the
spatiotemporal spike patterns with the desired spike trains.
Recently, a new decoding scheme with spiking neurons has been proposed to
describe how downstream neurons with dendritic bistable plateau potentials can
perform the decoding of spike sequences [10, 11]. The transition dynamics of this
downstream decoding network is demonstrated to be equivalent to that of a ﬁnite
state machine (FSM). This decoding scheme has the same computational power as
the FSM. It is capable of recognizing an arbitrary number of spike sequences [11].
However, as a part of a whole system, this decoding only describes the behavior of
the downstream neurons. How the upstream neurons behave and communicate with
the downstream neurons remains unclear.
In this chapter, a uniﬁed and consistent system with spiking neurons is proposed
for sequence recognition. To the best of our knowledge, this is the ﬁrst attempt to
consider a spiking system for sequence recognition with functional parts of sen-
sory coding, learning and decoding. This work helps to reveal the systematic neural
mechanisms considering all the processes of sensory coding, learning and down-
stream decoding. Such a system bridges the gap between these independently studied
processes. The system is integrated in a consistent scheme by processing precise-
timing spikes, where temporal coding and learning are involved. The sensory coding
describes how external information is converted into neural signals. Through learn-
ing, the neurons adapt their synaptic efﬁcacies for processing the input neural signals.
The decoding describes how the output neurons extract information from the neural
responses. The sequence recognition of the proposed biologically plausible system
is realized through the combination of item recognition and sequence order recogni-
tion. Identifying the input stimuli is required before recognizing the sequence order.
The recognition scheme is robust to noisy sensory input and it is also insensitive to
changes in the intervals between input stimuli within a certain range. The experiments
present spiking neural networks as a paradigm which can be used for recognizing
sequences of incoming stimuli.
The rest of this chapter is organized as follows. In Sect.5.2, detailed descriptions
are presented about the methods used in our integrated system. Section5.3 shows the

5.1 Introduction
91
performances of our system through numerical simulations. Detailed investigation
and analysis on different parts of the system are presented. The ability of the applied
temporal learning rule is isolated for investigation using the XOR benchmark task.
Then a practical optical character recognition (OCR) task is applied to investigate
the functionality of our system on item recognition. The performance of the spike
sequence decoding system is investigated by using a synthetic sequence of spikes.
Finally, the ability of the whole system on item sequence recognition is demonstrated.
Discussions about our system are presented in Sect.5.4, followed by a conclusion in
Sect.5.5.
5.2
The Integrated Network for Sequence Recognition
In this section, the whole system for sequence recognition is described, as well as
the corresponding schemes used in different subsystems.
5.2.1
Rationale of the Whole System
The whole system model contains three functional parts including sensory encoding,
learning and decoding (see Fig.5.1). They are the essential parts for a system of
spiking neurons to fulﬁll the sequence recognition task.
In order to utilize the spiking neurons for processing external information, the
ﬁrst step is to get the data into them, where proper encoding methods are required
[13, 14]. The components of the stimuli are connected to the encoding neurons.
These encoding neurons are used to generate spatiotemporal spike patterns which
represent the external stimuli. For example, each item in the sequence (denoted as ‘?’
in Fig.5.1) will be converted into a particular form of spike patterns by the encoding
neurons. For our choice, single spikes are used, with each encoding neuron only ﬁring
once within the presence of an input item. The scheme of single spikes is simple and
efﬁcient, which would potentially facilitate computing speed since fewer spikes are
involved in the computation [13, 15].
Before the recognition of the sequence order, another important step is to recog-
nize each input item. We call this recognition process as item recognition. Without
successful item recognition, it is impossible to detect the order of the incoming items
since each one will be an unknown item to the system. The learning neurons in our
system are used to perform the item recognition. These learning neurons are trained
to perform the recognition on the encoded spike patterns that sent from the encod-
ing neurons. During the training, the synaptic efﬁcacies of the learning neurons are
adapted to memorize the items. After training, the item recognition memory is stored
in the synaptic efﬁcacies, and thus the learning neurons can be applied in the sys-
tem structure. Whenever a new incoming item comes into the system, the learning
neurons can properly make a decision based on previously obtained memory.

92
5
A Spiking Neural Network System for Robust Sequence Recognition
Se
Fig. 5.1 Schematic of the system structure for sequence recognition. The system contains three
functional parts which are used for sensory encoding, item recognition and spike sequence recog-
nition, respectively. The encoding neurons convert the external stimuli into spatiotemporal spike
patterns. The learning neurons would recognize the content of each input item based on the corre-
sponding spatiotemporal spike pattern. The sequence order of the input stimuli would be recognized
through the decoding neurons
After successful item recognition, the ﬁnal step is to recognize the speciﬁc
sequence of the incoming items. The learning neurons send spikes to the decod-
ing neurons, where each spike represents a decision result for an input item. The
target of the decoding neurons is to successfully recognize the spike sequence order
of the learning neurons, and we call this recognition process as spike sequence recog-
nition. The memory of the sequence order is stored in the connection structure of the
decoding neurons.
Therefore, as is shown in Fig.5.1, our system will process the incoming items
in three main steps: (1) each item is ﬁrstly converted into spike patterns through
the encoding neurons; (2) the encoded pattern will be recognized by the learning
neurons, and a decision spike will be sent to the following network; (3) the desired
sequence order will be recognized by the decoding neurons. In order to make the
whole system function properly, the three subsystems need to communicate consis-
tently. The functionalities of the three subsystems are encoding, item recognition
and spike sequence recognition. With all these functionalities combined together,
the whole system could perform the item sequence recognition. The schemes used
in each subsystem are presented as follows.
5.2.2
Neural Encoding Method
Neural encoding considers how to generate a set of speciﬁc activity patterns that
represent the information of external stimuli. The speciﬁc activity patterns considered

5.2 The Integrated Network for Sequence Recognition
93
Fig. 5.2 Illustration of the phase encoding method with one encoding unit being presented. An
encoding unit is composed of a positive neuron (Pos), a negative neuron (Neg) and an output
neuron (Eout). The encoding unit receives signals from an input (x) and a subthreshold membrane
potential oscillation (SMO). The value of x can be negative as well as positive. The rectangle
boxes show the potential dynamics of the Pos and Neg neurons. The polarities of the synapses,
being either positive or negative, are denoted by + and −, respectively. Whenever the membrane
potential crosses the threshold (θE), the neuron (Pos or Neg) will ﬁre a spike immediately. Eout is
strongly connected to the Pos and Neg neurons, where the ﬁring of either the Pos neuron or the
Neg neuron will immediately cause a spike from the Eout neuron
in this chapter are in a spatiotemporal form where precise timing of spikes is used for
carrying information. Any encoding methods, that can generate spatiotemporal spike
patterns with spikes being sparsely distributed within the time window, could be a
proper choice for the encoding in our system. Here, we present a phase encoding
method, and use it as our encoding part in the system due to its high spatial and
temporal selectivity [16].
An increasing body of evidence shows that action potentials are related to the
phases of the intrinsic subthreshold membrane potential oscillations (SMOs) [17–
19]. These observations support the hypothesis of a phase code [16, 20, 21]. Such
a coding method can encode and retain information with high spatial and temporal
selectivity [16]. Following the coding methods presented in [16, 21], we propose
a new phase encoding method. In the coding methods of [16, 21], the artiﬁcial
steps of alignment and compression are questionable for biological implementation.
Differently, we provide a more biologically plausible coding scheme with a clearer
picture about how spikes would be generated. This would be helpful for further
implementing the coding scheme on the hardware systems. Our encoding mechanism
is presented in Fig.5.2.
As can be seen from Fig.5.2, each encoding unit contains a positive neuron (Pos),
a negative neuron (Neg) and an output neuron (Eout). The coding units compose the
encoding neurons in Fig.5.1. Each encoding unit is connected to an input signal
x and a SMO. The potentials of the Pos and Neg neurons are the summation of

94
5
A Spiking Neural Network System for Robust Sequence Recognition
x and SMO. The direction of the summation is determined by the polarities of
synapses. Whenever the membrane potential ﬁrstly crosses the threshold (θE), the
neuron will ﬁre a spike. In order to utilize single spikes, the neuron is only allowed
to ﬁre once within the whole oscillation period T . This can be implemented through
resetting the neuron’s potential to prevent it from ﬁring again within T . Due to
the strong connections, the ﬁring of either the Pos neuron or the Neg neuron will
immediately cause a spike from the Eout neuron. The SMO for the i-th encoding
unit is described as:
SMOi = M cos(ωt + φi)
(5.1)
where M is the magnitude of the SMO, ω = 2π/T is the phase angular velocity
and φi is the initial phase. φi is deﬁned as:
φi = φ0 + (i −1) · Δφ
(5.2)
where φ0 = 0 is the reference phase and Δφ is the phase difference between nearby
encoding units. We set Δφ = 2π/Nen where Nen is the number of encoding units.
5.2.3
Item Recognition with the PSD Rule
The PSD rule [9] is recently proposed for processing spatiotemporal spike patterns.
This rule is not only able to train the neurons to associate spatiotemporal spike
patterns with desired spike trains, but also able to train the neurons to perform the
classiﬁcation of spatiotemporal patterns. As the PSD rule is simple and efﬁcient,
we use it to train the learning neurons in the proposed system for item recognition.
Detailed descriptions about the PSD rule could be referred in Chap.4.
5.2.4
The Spike Sequence Decoding Method
In this part, we describe the sequence decoding method used for the decoding neurons
in our system. A network of neurons with dendritic bistable plateau potentials can be
used to recognize spike sequences [11]. Based on this idea, we build our decoding
system as presented in Fig.5.3. This decoding network can recognize a speciﬁc
sequence order of the spike inputs from the excitatory input neurons. The sequence
scale of this network could be easily modiﬁed through adding or deleting the basic
building blocks as in Fig.5.3.
In Fig.5.3, the dendrites have transient bistable plateau potentials, which pro-
duce the UP and DOWN states of the soma. The transitions between the UP and
DOWN states are controlled by the feedforward excitation from the input neurons,

5.2 The Integrated Network for Sequence Recognition
95
Fig. 5.3 The neural
structure for spike sequence
recognition. E0-5 denote the
excitatory input neurons.
S1-5 and D1-5 denote the
soma and the dendrite
respectively. Inh denotes the
global inhibitory neuron.
The dashed box shows the
basic building block for
scaling the network to
sequences of different size
feedforward inhibition from the global inhibitory neuron, as well as the lateral exci-
tations between the neurons in the network. There are only one global inhibitory
neuron that receives input from the excitatory input neurons and sends inhibition
to all the dendrite and soma. The inhibitory neuron will send a spike with a short
delay after receiving an excitatory input. At the beginning of the dendrites entering
the plateau potentials, they are transiently stable for a time and thus resistant to the
inhibition during this time. In order to make a soma ﬁre, there are two necessary con-
ditions: (1) the soma is in the UP state; (2) the corresponding excitatory neuron ﬁres.
The ﬁrst condition reﬂects the correct sequential inputs from the past and the second
condition requires that the current input must be the desired one in the sequence.
More detailed descriptions about the dynamics of Fig.5.3 can be seen in Sect.5.3.3.
The dynamics of the membrane potential of the soma is described as:
τsm
dVsm
dt
= −(Vsm −Er) + gds(Vdr −Vsm)
(5.3)
+ Is + IA + Ins
where Vsm and Vdr denote the potential of the soma and the dendrite respectively;
τsm = 20 ms is the membrane time constant; Er = −70 mV is the resting membrane
potential; gds = 0.35 is the conductance from the dendrite to the soma; Is is the
synaptic current on the soma; IA is the A-type potassium current; Ins is a background
current, and is set to zero here.
The A-type potassium current [22, 23] is activated near the resting potential and
inactivated at more depolarized potentials. IA in the soma is given by:
IA = −gA · a∞· V 3
sm · b(t) · (Vsm −EK)
(5.4)
where gA = 10 is the conductance; EK = −90 mV is the reversal potential of the
potassium current; a∞and b(t) are the activation and inactivation variables respec-
tively, and they are given by:

96
5
A Spiking Neural Network System for Robust Sequence Recognition
a∞=
1
1 + exp

−(Vsm + 70)/5

(5.5)
τA
db
dt = −b +
1
1 + exp

(Vsm + 80)/6

(5.6)
where τA = 5 ms is a time constant.
The synaptic current on the soma is given by:
Is = −gAs · (Vsm −EE) −gGs · (Vsm −EI)
(5.7)
where gAs and gGs are the alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic
acid (AMPA) and gamma-amino-butyric-acid (GABA) synaptic conductances
respectively. The AMPA and GABA synaptic conductances mediate synaptic exci-
tation and inhibition respectively. EE = 0 mV and EI = −75 mV are the reversal
potential of excitatory and inhibitory synapses respectively.
The dynamics of the membrane potential of the dendrite is described as:
τdr
dVdr
dt
= −(Vdr −Er) + gsd · (Vsm −Vdr) + Idr
(5.8)
where τdr = 10 ms is the time constant of the dendrite; gsd = 0.05 is the conductance
from the soma to the dendrite; Idr is the synaptic current on the dendrite, and is given
by:
Idr = −gAd · (Vdr −EE) −gGd · (Vdr −EI)
(5.9)
−
gNd · Vdr
1 + exp

−(Vdr + 30)/5

where gAd and gGd are the AMPA and GABA synaptic conductances respectively;
gNd is the N-methyl-D-aspartate (NMDA) synaptic conductance that is responsible
for the transient bistable plateau potential.
An incoming spike arrives at a synapse with strength G will cause changes
on synaptic conductances g: g →g + G. On the dendrite, a spike to an excita-
tory synapse will cause gAd →gAd + G and gNd →gNd + 5G. Without incoming
spikes, all the synaptic conductances will decay exponentially. The decay time con-
stants for both the AMPA and GABA conductances are 5 ms. For the NMDA con-
ductance, the decay time constant is 150 ms. gNd is not allowed to exceed 10 due to
a saturation.
The inhibitory neuron could be modeled as a single compartment quadratic LIF
neuron [10, 11] such that it can respond with a short latency to an excitatory spike
input. Different from this complex model, we simplify the model by setting the
inhibitory neuron to spike once with a delay of 2 ms at each input spike. This choice
makes the model simple and the functionality of the global feedforward inhibition
remains the same.

5.3 Experimental Results
97
5.3
Experimental Results
In this section, several experiments are presented to demonstrate the characteristics
of our model. Through simulations, we investigate the abilities of our system mainly
for item recognition and sequence recognition. A correct recognition on the input
items is an essential step for further recognizing the sequence.
Firstly, in Sect.5.3.1, the exclusive OR (XOR) problem is used to preliminar-
ily analyze the classiﬁcation ability of the temporal learning rule on spike patterns.
Through the XOR task, we want to isolate the PSD rule for testing before applying
it in our system. In Sect.5.3.2, we present the ability of our system for item recog-
nition. A set of optical characters with images of digits 0–9 are used. Section5.3.3
shows the performance of our spike sequence decoding subsystem where the down-
stream neurons could recognize a speciﬁc spike sequence. Finally, in Sect.5.3.4, the
functionalities of both the item recognition and the spike sequence recognition are
combined together for the item sequence recognition, which shows the performance
of the whole system.
5.3.1
Learning Performance Analysis of the PSD Rule
In this part, we isolate the PSD rule from the whole system for testing with the XOR
task. The XOR problem is a linearly nonseparable task, and it is a benchmark widely
used for investigating the classiﬁcation ability of SNNs recently [5, 24–26]. Thus,
we also use the XOR problem to investigate the performance of the PSD rule ﬁrstly.
Similar to the setup in [5], we directly map the XOR inputs to spike times, with
the symbol 0/1 being associated with a spike at 0/10 ms. Table5.1 shows the input
spike patterns and desired response for the XOR task. Noteworthily, the two patterns
(0–0 ms and 10–10 ms) are actually identical without considering the delay of 10 ms.
In order to have causal response to both inputs, a reasonable choice for the output
spike should be later than 10 ms. Therefore, we use the PSD to train the neuron to
ﬁre at 12 and 22 ms for pattern (0, 0) and (1, 1), respectively. The neuron is also
trained to be silent for patterns of (0, 1) and (1, 0). The PSD learning parameters are
set as η = 0.01 and τs = 10 ms.
Table 5.1 The XOR problem description
XOR input
Encoded spike input (ms)
Desired response
(0, 0)
(0, 0)
Fire
(0, 1)
(0, 10)
Silence
(1, 0)
(10, 0)
Silence
(1, 1)
(10, 10)
Fire

98
5
A Spiking Neural Network System for Robust Sequence Recognition
Fig. 5.4 The performance of
the PSD rule on the XOR
task with direct spike
mapping. The top panel
shows the membrane
potentials of the neuron after
training. The bottom shows
the spike time of the neuron
along the training. The
shaded bars denote the
desired spike time
Figure5.4 shows the PSD rule can train the neuron to solve the XOR task suc-
cessfully. When patterns of (0, 1) and (1, 0) present, the neuron keeps silent without
ﬁring. For patterns of (0, 0) and (1, 1), the neuron will ﬁre a spike as desired. How-
ever, it would be problematic if we want the neuron to ﬁre a same desired spike train
for patterns in the same group. This is due to the low dimensionality of the XOR
task. Both the temporal and spatial dimensions are very limited (2 here). In order
to train the neuron to ﬁre multiple precise-timing spikes, the dimensionality of the
problem should be enhanced.
Similar to the setup in [6, 26], we randomly generate two homogeneous poisson
spike trains with a ﬁring rate of 50 Hz in a time window of 200 ms. These two
spike trains represent 0 or 1 respectively, and they are used to form the four inputs
of the XOR problem: (0, 0), (0, 1), (1, 0) and (1, 1) (see Fig.5.5a). We also employ
the concept of reservoir computing with a network of Liquid State Machine (LSM)
like in [6, 26, 27]. The LSM uses spiking neurons connected by dynamic synapses
to project the inputs to a higher-dimensional feature space, which can facilitate the
classiﬁcation. The network used in this experiment consists of two input neurons, a
noise-free reservoir with 500 LIF neurons and one readout neuron.
The target spike train could be randomly generated over the time window. For
simplicity, we specify the target spike train for each category. For inputs of (0, 0)
and (1, 1), the output neuron is trained to spike at [110, 190] ms, while for (0, 1)
and (1, 0), it is trained to ﬁre another target train of [70, 150] ms. Other choices of
the target spikes could also be acceptable. The initial synaptic weights of the output
neuron are randomly drawn from a normal distribution with a mean value of 0.5 nA
and a standard deviation of 0.2 nA. This initial condition of synaptic weights is also
used for other experiments in this chapter. These synaptic weights are adjusted by
the PSD rule with a set of learning parameters η = 0.01 and τs = 10 ms. The results
are averaged over 100 runs.
Figure5.5b shows the results of a typical run, with the actual output spikes for each
of the four input patterns during the learning. At the beginning, both the ﬁring rates

5.3 Experimental Results
99
Fig. 5.5 The performance of
the PSD rule on the XOR
task with the LSM. a is a
general illustration of the
four inputs of the XOR task.
The values of 0 and 1 are
represented by different
spike trains. b shows the
output spike signals for each
of the four input patterns
during learning in a typical
run. ‘×’ denotes the desired
spike time. c and d are the
results of the output neuron
after 100 runs. c is the
average spike distance
between the desired and the
actual output spike trains.
The average spike distance
for each input pattern is
presented. d is the spike
histogram showing the
distribution of the actual
output spikes
(a)
(b)
(c)
(d)
and the precise timings of the output spike trains are different from those of the target
spike trains. After tens of learning epochs, the readout neuron can gradually learn
to ﬁre the target spike trains according to different input patterns. After hundreds
of learning epochs, the readout neuron stabilizes at the target spike trains. This
phenomenon can be also seen from the spike distance between the actual and the
target spike trains (see Fig.5.5c). A larger spike distance occurs at the beginning due
to the initial conditions, followed by a gradually decreasing spike distance along the
learning, and it ﬁnally converges to zero. Figure5.5d shows the distribution of the
actual output spikes corresponding to the four input patterns. From these histograms,
we can see our approach with the PSD rule obtains better performance than that in
[26]. Firstly, there are no undesired extra spikes or missing desired spikes in our
approach. In the 100 runs of experiments, the trained neuron ﬁres exactly 100 spikes
around each desired time. Secondly, the actual output spikes are precisely and reliably

100
5
A Spiking Neural Network System for Robust Sequence Recognition
Fig. 5.6 The convergent
performance. a shows the
average spike distance over
all the four input patterns. b
is the Euclidean distance
between the weights before
and after each learning
epoch. All the results are
averaged over 100 runs
(a)
(b)
close to the desired time. The maximum error of spike time is around 1 ms. Thus,
the learning success rate of our approach is higher than that in [26].
Figure5.6 shows the convergent performance during the learning process. The
averagespikedistanceover all four input patterns is presentedas well as theEuclidean
distance between the weights before and after each learning epoch. As can be seen
from Fig.5.6, irregular distances occur at the ﬁrst several learning epochs because
of the random initial conditions. After that, the distances gradually decrease and
converge to zero. The zero spike distance corresponds to the readout neuron ﬁring
exactly the target spike train, and the zero weight distance implies that there are no
more changes occurring on the weights. These two distance graphs also show the
ability of the PSD rule to modify the weights in order to produce the desired output
spikes. Either of these two types of distance can be used as a stopping criterion for
the learning process.
This experiment with the XOR problem demonstrates the ability of the PSD rule
for classifying spatiotemporal patterns. The PSD rule can perform the task as desired,
as long as enough dimensionality is provided by the input patterns. Considering the
real-world stimuli such as images, normally, the dimensionality of the input space is
not an issue. In the following, we apply the PSD rule in our system, and investigate
the performance of our system for item recognition and sequence order recognition.
5.3.2
Item Recognition
In this section, the functionality of our system for the item recognition is considered.
The encoding neurons and learning neurons in Fig.5.1 are involved for the item
recognition. A set of optical characters of 0–9 are used. Each image has a size of
20 × 20 black/white pixels, and each would be corrupted by a reversal noise where

5.3 Experimental Results
101
each pixel is randomly reversed with a probability denoted as the noise level. Some
clean and noisy samples are demonstrated in Fig.5.7a, b.
The phase encoding method illustrated in Fig.5.2 is used to convert the images
into spatiotemporal spike patterns. Each pixel acts as an input x to each encoding
unit. x is normalized into the range of [−0.5θE, 0.5θE] where θE = 1 is the threshold
of the encoding neurons. We set M = 0.5θE, thus the encoded spikes will only occur
at peaks of the SMOs. The number of encoding units Nen is equal to the number
of pixels which is 400 here. We set the oscillation period T of the SMOs to be
200 ms. The chosen scale of hundreds of milliseconds matches with the biological
experimental results [3, 28]. Figure5.7c demonstrates an encoding result with our
phase coding method. The output spikes are sparsely distributed over the time win-
dow. This sparseness is compatible with the biology [29]. In addition, the sparseness
could also beneﬁt the PSD rule for constructing causal connections. However, the
sparse spikes will not be obtained if we use the rank-order coding [30, 31] for the
given task. All the white or black pixels will result in highly synchronized spikes.
With the advantage of an additional phase coding dimension, the encoded spikes are
sparsely distributed by our coding method.
We select 10 learning neurons trained by the PSD rule, with each learning neuron
corresponding to one category. The learning parameters in the PSD rule are set to be
η = 0.06 and τs = 10 ms. All the learning neurons are trained to ﬁre a target spike
train with the corresponding category. The target spike train is set to be evenly distrib-
uted over the time window T (200 ms here) with a speciﬁed number of spikes n. The
Fig. 5.7 Illustration of the
OCR samples. a shows the
template images. b shows
some image samples with
different levels of reversal
noise. c demonstrates the
phase encoding result of a
given image sample. Each
dot denotes a spike
(b)
(a)
(c)

102
5
A Spiking Neural Network System for Robust Sequence Recognition
ﬁring time of the i-th target spike is expressed as: ti = i/(n + 1) · T , i = 1, 2, . . . n.
We choose n = 4 by default, otherwise will be stated. In the item recognition, the
relative conﬁdence criterion [9] is used for the PSD rule, where the incoming pattern
is represented by the neuron that ﬁres the most closest spike train to its target spike
train.
In this section, several noisy scenarios are considered to evaluate the robustness
of our system for item recognition: (1) spike jitter noise where a Gaussian jitter with
a standard deviation (denoted as the jitter strength) is added into each encoded spike;
(2) reversal noise as illustrated in Fig.5.7b where each pixel is randomly reversed
with a probability denoted as the noise level; (3) combined noise where both the jitter
and the reversal noises are involved.
5.3.2.1
Spike Jitter Noise
In this scenario, the image templates are ﬁrstly encoded into spatiotemporal spike
patterns. After that, jitter noises are added to generate noisy patterns. The learning
neurons are trained for 100 epochs with a jitter strength of 2 ms. In each learning
epoch, a training set of 100 patterns, with 10 for each category, is generated. After
training, a jitter range of 0–8 ms is used to investigate the generalization ability. The
number of the testing patterns for each jitter strength is set to 200. The PSD rule is
applied with different numbers of target spikes (n = 1, 2, 4, 6, 8, 10). All the results
are averaged over 100 runs.
Figure5.8 shows the effects of the number of the target spikes on the learning
performance of the PSD rule. As can be seen from Fig.5.8, when n is low (e.g. 1, 2),
the recognition performance is also relatively low. An increasing number of the target
spikes can improve the recognition performance signiﬁcantly (see n = 1, 2 →n =
4, 6).However,afurtherincreaseinthenumberoftargetspikes(n = 6 →n = 8, 10)
Fig. 5.8 The performance of
the PSD rule with different
numbers of target spikes
under the case of jitter noise
0
1
2
3
4
5
6
7
8
20
30
40
50
60
70
80
90
100
Jitter Noise Level (ms)
Classification Accuracy (%)
n=1
n=2
n=4
n=6
n=8
n=10

5.3 Experimental Results
103
Fig. 5.9 Robustness of
different rules against the
jitter noise. The PSD rule
uses n = 4 target spikes. The
PSD rule outperforms the
other two rules in the
considered task
would reduce the recognition performance. The reasons for this phenomenon are due
to the local temporal features associated with each target spike. For small number of
target spikes, the neurons make decision based on a relatively less number of temporal
features. This small number of features only covers a part range of the whole time
window, which inevitably leads to a lower performance compared to a more number
of spikes. However, when the number of spikes continues increasing, an interference
of local learning processes [6] occurs and increases the difﬁculty of the learning.
Thus, a higher number of spikes normally cannot lead to a better performance due
to the interference. Noteworthily, compared to the idealized case of random patterns
that normally considered, the effect of the local interference is more obvious for the
learning of the practical patterns since these patterns share more common features
than random patterns.
Figure5.9 shows the performance of different learning rules for the same classiﬁ-
cation task. We use a similar approach for the perceptron rule as in [26, 27], where the
spatiotemporal spike patterns are transformed into continuous states by a low-pass
ﬁlter. The target spike trains are separated into bins of size tsmp, with tsmp = 2 ms
being the sampling time. The target vectors for the perceptron contain values of 0
and 1, with 1 (or 0) corresponding to those bins that contain (or not contain) a target
spike in the bin. The input vectors for the perceptron are sampled from the continuous
states with tsmp. The input pattern will be classiﬁed by the winning perceptron that
has the closest output vector to the target vector.
As can be seen from Fig.5.9, the PSD rule outperforms both the tempotron rule
and the perceptron rule. The inferior performance of the perceptron rule can be
explained. The complexity of the classiﬁcation for the perceptron rule depends on
the dimensionality of the feature space and the number of input vectors for decisions.
A value of tsmp = 2 ms will generate 100 input vectors for each input pattern. These
100 points in 400-dimensional space are to be classiﬁed into 1 or 0. Learning a single
spike pattern can increase the difﬁculty for the perceptron rule, let alone considering

104
5
A Spiking Neural Network System for Robust Sequence Recognition
a large number of input patterns from different categories. Without separating the
time window into bins, the spiking neurons by their nature are more powerful than
the traditional neurons such as the perceptron. Both the PSD rule and the tempotron
rule are better than the perceptron rule. The PSD rule is better than the tempotron
rule since the PSD rule makes a decision based on a combination of several local
temporal features over the entire time window, but the tempotron rule only makes a
decision by ﬁring one spike or not based on one local temporal feature.
5.3.2.2
Reversal Noise
In this scenario, the reversal noise is used for generating noisy patterns as illustrated
in Fig.5.7b. The learning neurons are trained for 100 epochs with a reversal noise
level randomly drawn from the range of 0–10% in each learning epoch. Meanwhile,
a training set of 100 noisy patterns, with 10 for each category, is generated for each
learning epoch. After training, another number of 100 noisy patterns are generated
and used for each reversal noise level to test the generalization ability. The noise
range for testing covers 0–25% as shown in Fig.5.10.
As can be seen from Fig.5.10, the performances of all the three rules decrease with
the increasing noise level. The performance of the PSD rule again outperforms the
other two rules as in the previous scenario. Spiking neurons trained by the PSD rule
can obtain a high classiﬁcation accuracy (around 85%) even when the reversal noise
reaches a high level (15%). The performance of the perceptron rule in this scenario
is much better than that in the previous scenario. This is because of the type of the
noise. The performance of the perceptron rule is quite susceptible to the changes in
state vectors. Every spike of the input spatiotemporal spike patterns in the case of
spike jitter noise suffers a change, while in the case of reversal noise, a change only
occurs with a probability of the reversal noise level. That is to say, the elements in a
Fig. 5.10 Robustness of
different rules against the
reversal noise. The PSD rule
uses n = 4 target spikes. The
PSD rule outperforms the
other two rules even when
the noise level is high

5.3 Experimental Results
105
ﬁltered state vector have a less chance to change under the reversal noise than that
under the jitter noise. Thus, the performance of the perceptron rule [26] under the
reversal noise is better than that under the jitter noise.
5.3.2.3
Combined Noise
In this scenario, the jitter noise and the reversal noise are combined together to
evaluate the robustness of our item recognition. Again, the learning neurons are
trained for 100 epochs. In each epoch, a random reversal noise level chosen from 0
to 10% is used, as well as a jitter noise level of 2 ms. After training, a reversal noise
level of 10% and a jitter noise level of 4 ms is used to investigate the generalization
ability.
Figure5.11 shows that the combined noise has a stronger impact on the perfor-
mance than each single noise alone on the performance. This is expected since the
effects of the two noises are combined. The perceptron rule still has a poor perfor-
mance due to the jitter noise. The PSD rule still performs the best with a high average
accuracy and a low deviation.
The results in this section demonstrate our item recognition with the PSD rule
is robust to different noisy sensory inputs. A reliable recognition on the incoming
items is essential for further sequence recognition.
5.3.3
Spike Sequence Decoding
In this section, we investigate the performance of our decoding system for spike
sequence recognition. The structure of this decoding system is presented in Fig.5.3.
This decoding structure can recognize a speciﬁc sequence of E0, E1...E5. The size
of the sequence can be easily extended to different scales through modiﬁcation of
the decoding network structure. We denote the synaptic connections as: E0→D1
Fig. 5.11 Robustness of
different rules against the
combination of the jitter and
reversal noises. A 10%
reversal noise and a 4 ms
jitter noise are used for
testing
PSD
Tempotron
Perceptron
0
20
40
60
80
100
Classification Accuracy (%)

106
5
A Spiking Neural Network System for Robust Sequence Recognition
(G0), E1-5→S1-5 (G1), E1-5→D2-5 (G2), S1-5→D2-5 (G3), Inh→D1-5 (G4),
and Inh→S1-5 (G5). We set G0 = 5, G1 = 2.5, G2 = G3 = 3, G4 = 5, G5 = 6.
We generate a spike input feeding into our decoding system, with Fig.5.12 showing
a 200 ms interval between nearby spikes and 230 ms for Fig.5.13.
As can be seen from Fig.5.12, the decoding system successfully recognizes the
sequence through a ﬁring from S5. A strong, excitatory input to the dendrite can make
its potential go to a plateau potential that is transiently stable for a time. The plateau
potential of the dendrite then drives the potential of the soma to a high depolarized
state. Without the plateau potential of the dendrite, the potential of the soma stays
near the resting potential. We refer the high depolarized state of the soma as the UP
state, and the state near the resting potential as the DOWN state. Two conditions are
required to make a soma ﬁre: (1) the potential of the soma sustains in the UP state
(2) when an excitatory spike input comes to this soma.
Under the experimental setup of our decoding system, the UP state of the soma
can sustain for a period around 225 ms, during which the soma can reliably ﬁre a
desired spike when corresponding excitatory neuron ﬁres. We refer this period as the
reliable period. When the time interval between spikes is shorter than the reliable
Fig. 5.12 A reliable
response of the spike
sequence decoding system.
A synthetic spike sequence is
used as the input (denoted as
‘Seq’). The target sequence
pattern of E0, E1...E5 is
highlighted by the shaded
area. The potentials of the
somas (S1 −5) and the
dendrites (D1 −5) are
shown. The interval spike
time in the input sequence is
200 ms. The neurons can be
successfully activated to ﬁre
when the target sequence
presents

5.3 Experimental Results
107
Fig. 5.13 An unreliable
response of the spike
sequence decoding system.
The interval spike time in the
input sequence is 230 ms.
When the interval time is
over a certain range (225 ms
for this experimental setup),
the neurons cannot be
activated to ﬁre even when
the target sequence presents.
This is because that the
potential of the soma cannot
sustain in the UP state for
such a long interval
period, the decoding system can perform the recognition well (see Fig.5.12). When
the time interval between input spikes is longer than the reliable period, the UP state
of a soma no longer sustains at a reliably high state. This leads to that a corresponding
excitatory input spike no longer reliably drives a spike on the soma (see Fig.5.13).
The experimental results indicate that our spike sequence decoding system is
invariant to changes in the intervals between input spikes within a certain range
(0–225 ms here).
5.3.4
Sequence Recognition System
In this section, the performance of the proposed whole system is investigated. The
sensory encoding, temporal learning and spike sequence decoding are consistently
combined together for sequence recognition. We perform the experiment with the
previous digits used in Sect.5.3.2.
These optical digits are used to form a sequence pattern, with each digit image
in the sequence being corrupted by a reversal noise level of 15%. We can specify

108
5
A Spiking Neural Network System for Robust Sequence Recognition
a target sequence through building connections between the output neurons of the
item recognition network and the excitatory input neurons of the spike sequence
decoding network. For simplicity, we specify a target sequence order of digits as:
012345. Thus, the learning neurons corresponding to the categories in this target
sequence are connected to the excitatory input neurons in the sequence decoding
network one by one. Each digit image is presented for 200 ms. Additionally, the
interval between two successive images is not allowed to exceed 25 ms, guaranteeing
a reliable performance of the spike decoding system.
We construct a sequence pattern of 6 segments, with 6 images for each segment.
Every image in this sequence is randomly chosen from the 10 categories. Then the
target sequence of 012345 is embedded into this sequence, with a probability of 1/3
replacing each initial segment in the sequence. After this, we feed the whole sequence
to our system. The target of our system is to detect and recognize the target sequence
embedded in the whole sequence.
Figure5.14 shows the performance of our system for sequence recognition. An
important step before recognizing the sequence order is to correctly recognize each
input item. Only after knowing what is what, a recognition on the sequence order can
be applied. The detected target sequence is represented by the ﬁring of S5. As can
be seen from Fig.5.14, the ﬁrst target sequence is successfully recognize through the
sequential ﬁring of S1-5, while the second target sequence is not correctly recognized
due to a failure recognition on image ‘4’.
Fig. 5.14 The performance of the combined sequence recognition system. An image sequence
input is fed into the sequence recognition system. Each image suffers a reversal noise of 15%. The
target of this system is to detect and recognize a speciﬁed target sequence of 012345 (the shaded
areas). ‘Item Seq’ denotes the input sequence of the images. ‘Item Recog’ is the output results of the
learning neurons, with the blue/red color representing correct/incorrect recognition. Each output
of the learning neurons results a spike in the corresponding excitatory input neurons of the spike
decoding network (‘Spks E’). S1-5 denote the spike output of neurons in the sequence decoding
network

5.3 Experimental Results
109
Fig. 5.15 Performance on a target sequence with one semi-blind item. The input sequence is
considered in a noise-free condition. The target of this system is to detect and recognize a speciﬁed
target sequence of 012?45 where ‘?’ is from a speciﬁc range of 5–9 (illustrated in the shaded light-
cyan areas). The shaded light-pink areas show some interference sequence patterns where ‘?’ is not
chosen from the allowed range
In addition, we conduct another experiment, where one item in the target sequence
is semi-blind. This semi-blind item is conditioned to a speciﬁc range. We specify a
target sequence of 012?45, where ‘?’ is restricted to the range of 5–9. Other digits
of ‘?’ being out of this speciﬁc range lead to non-target sequences. For the sake
of simplicity, this experiment is conducted in a noise-free condition. We randomly
construct an input sequence with 48 items, and then embed the target sequences, as
well as some interference sequences, into the input sequence. In order to detect and
recognize the semi-blind target sequences, we reconstruct the connections between
the output neurons of the item recognition network and the excitatory input neurons
of the spike sequence decoding network, with all learning neurons for digit 5–9
connecting to E3 in Fig.5.3. Other connections are not changed. As can be seen
from Fig.5.15, the semi-blind target sequences are successfully recognized, and
those interference sequences are also successfully declined. Our system successfully
recognizes the target sequence of 012?45 with ‘?’ only belonging to 5–9.
These experiments show that our system with spiking neurons can perform the
sequence recognition well, even under some noisy conditions. Item recognition is an
essential step for a successful recognition of the target sequence. The step before
recognizing the sequence order is to recognize what are the items in the input
sequence. A failure recognition of the item in the target sequence would directly
affect the further recognition on the sequence order.

110
5
A Spiking Neural Network System for Robust Sequence Recognition
5.4
Discussions
In this chapter, a biologically plausible system with spiking neurons is presented for
sequence recognition. Discussions based on the simulation results are as follows.
5.4.1
Temporal Learning Rules and Spiking Neurons
The PSD rule [9], proposed in the concept of processing and memorizing spatiotem-
poral spike patterns, is applied in our system for item recognition. In the PSD rule,
the synaptic adaptation is driven by the precise timing of the actual and the target
spikes. Without a complex error calculation, the PSD rule is simple and beneﬁcial
for computation [9]. According to the classiﬁcation tasks considered in this chapter,
the PSD rule outperforms both the tempotron rule [4, 13] and the perceptron rule
[26, 27].
The computational power of the spiking neurons over the traditional neurons
(perceptrons) is reﬂected by the better performance of both the PSD rule and the
tempotron rule than the perceptron rule (see Figs.5.9 and 5.10). This is because that
the spiking neurons, by their nature, are designed for processing in a time domain
with a complex evolving dynamics on the membrane potential. A major difference
between the perceptrons and the spiking neurons is this dynamic membrane potential.
The perceptrons calculate current states in a static manner that only based on the
current inputs, while the spiking neurons evolve current states in a dynamic manner
that not only based on the current inputs but also the past states. Additionally, due to
the ability of the spiking neurons to operate online, it can beneﬁt the computation of
a sequential procession with time elapsing.
Between the two temporal learning rules for spiking neurons, the performance of
the PSD rule is better than the tempotron rule. The decisions made by the neurons
under the PSD rule are based on a combination of several local temporal features
over the whole time window. By contrast, the tempotron rule trains a neuron to make
a decision only based on one local temporal feature if the neuron is supposed to ﬁre
a spike. A decision based on several local temporal features would result in a better
performance than that only based on one local temporal feature. In addition, the PSD
rule is not limited to a classiﬁcation task, but it can also train a neuron to associate
spatiotemporal patterns with the speciﬁed desired spike trains.
5.4.2
Spike Sequence Decoding Network
Our spike sequence decoding network is biologically realistic that can behave like
FSM to recognize spike sequences [10, 11]. The functionality of this network is
achieved through transitions between the UP and DOWN states of neurons. Tran-

5.4 Discussions
111
sitions between bistable membrane potentials are widely observed through various
experiments in cortical pyramidal neurons in vivo [32, 33]. The transitions between
the states are controlled by feedforward excitation, lateral excitation and feedforward
inhibition. The neurons enter the UP state if their dendrites have a plateau potential.
The neurons will return to the DOWN state from the UP state when enough long time
elapses without excitatory input spikes. In addition, the recognition is robust to time
warping of the sequence. The recognition is intact as long as the interval between
input spikes lies in a speciﬁc range which can be quite broad (see Figs.5.12 and 5.13).
Invariance to time warping is beneﬁcial for tasks like speech recognition [34, 35].
5.4.3
Potential Applications in Authentication
Our system provides a general structure for sequence recognition. With proper encod-
ing mechanisms, this system could also be applied to acoustic, tactual and olfactory
signals in addition to visual signals. The processes of the item recognition and the
sequence order recognition in our system could be used for user authentication to
access approval. It provides a double-phase checking scheme for gaining access.
Only if both the items and also their orders are correct, the person would be allowed
to access.
We preliminarily applied these concepts to the speech task with our previously
proposed encoding scheme [36] for sounds. The voices of ten digits were considered.
It is still a very challenging task for spiking neurons to process audio signals due to
variations of speed, pitch, tone and volume. Our system could be successful in the
case where words are spoken in a similar manner such as samples (a)–(c) in Fig.5.16,
but it would be failed if the voice is changed signiﬁcantly like (d) and (e) in Fig.5.16.
Further study is required for speech recognition with spiking neurons, and further
results would be presented in our next stage.
Fig. 5.16 Voice samples of
‘Zero’. a, b and c are
samples spoken by a person
in clean conditions with a
similar manner for each
recording. d is a sample
under a 5dB noise and e is a
warped sample spoken in a
different manner. The top
panel and the bottom panel
show the sound waves and
the corresponding
spectrograms respectively
(a)
(b)
(c)
(d)
(e)

112
5
A Spiking Neural Network System for Robust Sequence Recognition
5.5
Conclusion
In this chapter, a biologically plausible network is proposed for sequence recognition.
This is the ﬁrst attempt to solve the sequence recognition with the network of spiking
neurons by considering both the upstream and the downstream neurons together. The
system is consistently integrated with functionalities of sensory encoding, learning
anddecoding.Thesystemoperatesinatemporalframework,wheretheprecisetiming
of spikes is considered for information processing and cognitive computing. The
recognition performance of the system is robust to different noisy sensory inputs,
and it is also invariant to changes in the intervals between input stimuli within a
certain range. Our system would also be beneﬁcial for applied developments in both
hardware and software.
References
1. Starzyk, J.A., He, H.: Spatio-temporal memories for machine learning: a long-term memory
organization. IEEE Trans. Neural Netw. 20(5), 768–780 (2009)
2. Nguyen, V.A., Starzyk, J.A., Goh, W.B., Jachyra, D.: Neural network structure for spatio-
temporal long-term memory. IEEE Trans. Neural Netw. Learn. Syst. 23(6), 971–983 (2012)
3. Panzeri, S., Brunel, N., Logothetis, N.K., Kayser, C.: Sensory neural codes using multiplexed
temporal scales. Trends Neurosci. 33(3), 111–120 (2010)
4. Gütig, R., Sompolinsky, H.: The tempotron: a neuron that learns spike timing-based decisions.
Nature Neurosci. 9(3), 420–428 (2006)
5. Bohte, S.M., Kok, J.N., Poutré, J.A.L.: Error-backpropagation in temporally encoded networks
of spiking neurons. Neurocomputing 48(1–4), 17–37 (2002)
6. Ponulak, F., Kasinski, A.: Supervised learning in spiking neural networks with resume:
sequence learning, classiﬁcation, and spike shifting. Neural Comput. 22(2), 467–510 (2010)
7. Florian, R.V.: The Chronotron: a neuron that learns to ﬁre temporally precise spike patterns.
PLoS One 7(8), e40,233 (2012)
8. Mohemmed, A., Schliebs, S., Matsuda, S., Kasabov, N.: SPAN: spike pattern association neuron
for learning spatio-temporal spike patterns. Int. J. Neural Syst. 22(04), 1250,012 (2012)
9. Yu, Q., Tang, H., Tan, K.C., Li, H.: Precise-spike-driven synaptic plasticity: Learning hetero-
association of spatiotemporal spike patterns. PLoS One 8(11), e78,318 (2013)
10. Jin, D.Z.: Spiking neural network for recognizing spatiotemporal sequences of spikes. Phys.
Rev. E 69(2), 021,905 (2004)
11. Jin, D.Z.: Decoding spatiotemporal spike sequences via the ﬁnite state automata dynamics of
spiking neural networks. New J. Phys. 10(1), 015,010 (2008)
12. Byrnes, S., Burkitt, A.N., Grayden, D.B., Mefﬁn, H.: Learning a sparse code for temporal
sequences using STDP and sequence compression. Neural Comput. 23(10), 2567–2598 (2011)
13. Yu, Q., Tang, H., Tan, K.C., Li, H.: Rapid feedforward computation by temporal encoding
and learning with spiking neurons. IEEE Trans. Neural Netw. Learn. Syst. 24(10), 1539–1552
(2013)
14. Yu, Q., Tang, H., Tan, K.C., Yu, H.: A brain-inspired spiking neural network model with
temporal encoding and learning. Neurocomputing 138, 3–13 (2014)
15. Bohte, S.M., Bohte, E.M., Poutr, H.L., Kok, J.N.: Unsupervised clustering with spiking neurons
by sparse temporal coding and multi-layer RBF networks. IEEE Trans. Neural Netw. 13, 426–
435 (2002)
16. Nadasdy, Z.: Information encoding and reconstruction from the phase of action potentials.
Front. Syst. Neurosci. 3, 6 (2009)

References
113
17. Llinas, R.R., Grace, A.A., Yarom, Y.: In vitro neurons in mammalian cortical layer 4 exhibit
intrinsic oscillatory activity in the 10-to 50-Hz frequency range. Proc. Natl. Acad. Sci. 88(3),
897–901 (1991)
18. Jacobs, J., Kahana, M.J., Ekstrom, A.D., Fried, I.: Brain oscillations control timing of single-
neuron activity in humans. J. Neurosci. 27(14), 3839–3844 (2007)
19. Koepsell, K., Wang, X., Vaingankar, V., Wei, Y., Wang, Q., Rathbun, D.L., Usrey, W.M.,
Hirsch, J.A., Sommer, F.T.: Retinal oscillations carry visual information to cortex. Front. Syst.
Neurosci. 3, 4 (2009)
20. Kayser, C., Montemurro, M.A., Logothetis, N.K., Panzeri, S.: Spike-phase coding boosts and
stabilizes information carried by spatial and temporal spike patterns. Neuron 61(4), 597–608
(2009)
21. Hu, J., Tang, H., Tan, K.C., Li, H., Shi, L.: A spike-timing-based integrated model for pattern
recognition. Neural Comput. 25(2), 450–472 (2013)
22. Schoppa, N., Westbrook, G.: Regulation of synaptic timing in the olfactory bulb by an A-type
potassium current. Nature Neurosci. 2(12), 1106–1113 (1999)
23. Shriki, O., Hansel, D., Sompolinsky, H.: Rate models for conductance based cortical neuronal
networks. Neural Comput. 15(8), 1809–1841 (2003)
24. Ghosh-Dastidar, S., Adeli, H.: A new supervised learning algorithm for multiple spiking neural
networks with application in epilepsy and seizure detection. Neural Netw. 22(10), 1419–1431
(2009)
25. Sporea, I., Grüning, A.: Supervised learning in multilayer spiking neural networks. Neural
Comput. 25(2), 473–509 (2013)
26. Xu, Y., Zeng, X., Zhong, S.: A new supervised learning algorithm for spiking neurons. Neural
Comput. 25(6), 1472–1511 (2013)
27. Maass, W., Natschläger, T., Markram, H.: Real-time computing without stable states: a new
framework for neural computation based on perturbations. Neural Comput. 14(11), 2531–2560
(2002)
28. Butts, D.A., Weng, C., Jin, J., Yeh, C.I., Lesica, N.A., Alonso, J.M., Stanley, G.B.: Temporal
precision in the neural code and the timescales of natural vision. Nature 449(7158), 92–95
(2007)
29. Olshausen, B.A., Field, D.J.: Sparse coding with an overcomplete basis set: a strategy employed
by V1? Vision Res. 37(23), 3311–3325 (1997)
30. Van Rullen, R., Thorpe, S.J.: Rate coding versus temporal order coding: what the retinal gan-
glion cells tell the visual cortex. Neural Comput. 13(6), 1255–1283 (2001)
31. Perrinet, L., Samuelides, M., Thorpe, S.J.: Coding static natural images using spiking event
times: do neurons cooperate? IEEE Trans. Neural Netw. 15(5), 1164–1175 (2004)
32. Lewis, B.L., O’Donnell, P.: Ventral tegmental area afferents to the prefrontal cortex maintain
membrane potential ‘up’states in pyramidal neurons via D1 dopamine receptors. Cerebral
Cortex 10(12), 1168–1175 (2000)
33. Anderson, J., Lampl, I., Reichova, I., Carandini, M., Ferster, D.: Stimulus dependence of two-
state ﬂuctuations of membrane potential in cat visual cortex. Nat. Neurosci. 3(6), 617–621
(2000)
34. Hopﬁeld, J.J., Brody, C.D.: What is a moment? transient synchrony as a collective mechanism
for spatiotemporal integration. Proc. Natl. Acad. Sci. 98(3), 1282–1287 (2001)
35. Gütig, R., Sompolinsky, H.: Time-warp-invariant neuronal processing. PLoS Biol. 7(7),
e1000,141 (2009)
36. Dennis, J., Yu, Q., Tang, H., Tran, H.D., Li, H.: Temporal coding of local spectrogram features
for robust sound recognition. In: 2013 IEEE international conference on acoustics, speech and
signal processing (ICASSP), pp. 803–807 (2013)

Chapter 6
Temporal Learning in Multilayer Spiking
Neural Networks Through Construction
of Causal Connections
Abstract This chapter presents a new supervised temporal learning rule for multi-
layer spiking neural networks. We present and analyze the mechanisms utilized in
the network for the construction of causal connections. Synaptic efﬁcacies are ﬁnely
tuned for resulting in a desired post-synaptic ﬁring status. Both the PSD rule and the
tempotron rule are extended to multiple layers, leading to new rules of multilayer
PSD (MutPSD) and multilayer tempotron (MutTmptr). The algorithms are applied
successfully to classic linearly non-separable benchmarks like the XOR and the Iris
problems.
6.1
Introduction
In biological nervous systems, neurons communicate with others through action
potentials (spikes). To emulate this phenomenon, spiking neurons are introduced to
process spike information. Due to the spiking feature, the spiking neurons are more
biologically plausible and computationally powerful than traditional neuron models
like perceptron.
Information could be carried by spikes either in a rate-based form or a precise
spike-based form. Increasing evidence shows that individual spikes with precise time
play a signiﬁcant role in transmitting information. Neurons can learn more and faster
from the spike-based code than the rate-based code.
Considering the spatiotemporal spike patterns, many learning rules have been
proposed to understand how neurons process the information. Most of temporal
learning methods, such as tempotron [1], ReSuMe [2], Chronotron [3], SPAN [4]
and PSD [5], only focus on the learning of single spiking neurons or single-layer
SNNs. These learning rules are biologically plausible to some extent. However, the
real nervous systems are extremely complex network with a large number of neurons
interconnecting with each other. Investigations on the level of single neurons or
single-layer networks might be insufﬁcient to simulate the cognitive functions of the
brain. Therefore, research on multilayer SNNs is demanded.
Some gradient-descent-based learning rules such as SpikeProp [6] and its exten-
sions [7, 8] are proposed to train the network with hidden neurons to output a target
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_6
115

116
6
Temporal Learning in Multilayer Spiking Neural Networks …
spike train. The derivations of these rules are based on the explicit dynamics of the
SRM model, which limit the applicability of these rules to other neuron models.
The same problem is also involved in another gradient-descent-based rule proposed
in [9]. Although the gradient-descent-based rules are effective, they lack biological
explanation. The complex error calculation involved in the learning is at least ques-
tionable. In [10], an extension of the ReSuMe rule is proposed for multilayer SNNs,
where the weights are updated according to STDP and anti-STDP processes. This
ReSuMe-based multilayer learning rule requires back propagation of the network
error. When the number of layers increases, the evaluation of the network error will
become more complex. Again, such a complex error evaluation is also debatable
considering the real nervous systems.
In this chapter, we propose a new supervised learning rule for multilayer spik-
ing neural networks. This rule is an extension of the PSD rule introduced in Chap.4.
Without complex error evaluation, the learning is simple and efﬁcient, and yet biolog-
ically plausible. In addition, we also proposed a multilayer learning for the tempotron
rule. Through our multilayer learning, causal connections are constructed between
layers of spiking neurons.
The rest of this chapter is organized as follows. In Sect.6.2, the proposed learning
rules for multilayer SNNs are presented, including multilayer PSD (MutPSD) rule
and multilayer tempotron (MutTmptr) rule. Heuristic discussions about our multi-
layer learning rules are presented in Sect.6.3. Section6.4 presents the simulation
results. Construction of the causal connections is ﬁrstly demonstrated. The prop-
erties and power of the MutPSD and MutTmptr rules are showcased by linearly
non-separable tasks including the XOR problem and the Iris dataset task. Finally,
discussions of the multilayer learning rules, as well as the conclusion, are presented
in Sect.6.5.
6.2
Multilayer Learning Rules
In this section, we describe the learning schemes in the feedforward multilayer spik-
ing neural networks. Firstly, the neuron model used in this chapter is introduced.
Then, the multilayer PSD (MutPSD) rule is described, followed by the introduction
of multilayer tempotron (MutTmptr) rule.
6.2.1
Spiking Neuron Model
For the sake of simplicity, our neuron model consists of a leaky integrate-and-ﬁre
neuron driven by synaptic currents generated by its afferents. The potential of the
neuron is a weighted sum of postsynaptic currents (PSCs) from all incoming spikes:

6.2 Multilayer Learning Rules
117
V (t) =

i
wi I i
PSC(t) + Vrest
(6.1)
where wi and I i
PSC are the synaptic efﬁcacy and the PSC of the i-th afferent. Vrest is
the rest potential of the neuron. The dynamics of the I i
PSC is as follow:
I i
PSC(t) =

t j
K(t −t j)H(t −t j)
(6.2)
where t j is the time of the j-th spike emitted from the i-th afferent neuron, H(t)
refers to the Heaviside function, K denotes a normalized kernel and we choose it as:
K(t −t j) = V0 ·

exp(−(t −t j)
τs
) −exp(−(t −t j)
τ f
)

(6.3)
where V0 is a normalization factor such that the maximum value of the kernel is 1,
τs and τ f are the slow and fast decay constants respectively, and their ratio is ﬁxed
at τs/τ f = 4.
For the neurons in the hidden layers, we utilize a ﬁre-and-shutdown scheme as
in [1]. This can guarantee a single spike scheme in the hidden neurons if the neu-
rons receive enough strong stimulus. Increasing experimental evidence suggests that
neural systems use exact time of single spikes to transmit information [11–13]. Visual
system can analyze a new complex scene in less than 150 ms [11, 12]. This period
of time is impressive for information processing considering billions of neurons
involved. This suggests that neurons exchange only one or few spikes. In the tactile
system, it is shown that the time of the ﬁrst spike contains important information
about the external stimuli [13]. In addition to the biological plausibility, ﬁrst spikes
also serve as an efﬁcient way to transmit information. Subsequent brain region may
learn more and earlier about the stimuli from the time of the ﬁrst spikes [11]. The
beneﬁts of the ﬁrst spike suit the role of hidden neurons acting as the information
transmitter between the input and output neurons.
6.2.2
Multilayer PSD Rule
The PSD rule [5] for single neurons or single layer network is described as:
dwi(t)
dt
= η[sd(t) −so(t)]I i
PSC(t)
(6.4)
The polarity of the synaptic changes depends on three cases: (1) a positive error
(corresponding to a miss of the spike) where the neuron does not spike at the desired
time, (2) a zero error (corresponding to a hit) where the neuron spikes at the desired

118
6
Temporal Learning in Multilayer Spiking Neural Networks …
(a)
(b)
(c)
Fig. 6.1 Structure and plasticity of multilayer PSD. a is the structure of the multilayer network
where input neurons are connected to the output neuron through hidden neurons. b shows the
synaptic structure. The synaptic plasticity in the multilayer network is driven by the desired signal
(d) and the actual output signal (o). c demonstrates the scheme for synaptic plasticity. A desired
spike will result in potentiation, while an actual output spike will lead to depression. The amount
of synaptic modiﬁcation depends on the PSC signal
time, and (3) a negative error (corresponding to a false-alarm) where the neuron
spikes when it is not supposed to.
In the single-layer PSD, only the direction of synaptic modiﬁcation is used. The
amount of modiﬁcation depends on the current input PSC. Based on this idea, a
multilayer PSD (MutPSD) rule can be developed. The instructor signals that only
containing directions of modiﬁcations are back propagated to all synapses in the
multilayer feedforward network, while the amount of synaptic change depends on
the corresponding PSC received by each synapse.
Figure6.1a shows the multilayer structure. For the reason of simplicity, one layer
of hidden neurons is considered, but the algorithm can be extended to networks with
more hidden layers similarly. The instructor signals are used to guide the synap-
tic modiﬁcation direction of all synapses. Considering the synaptic delays d, the
MutPSD rule can be described as:
Δwi = η
 ∞
0
[sd(t) −so(t)]I i
PSC(t −d)dt
(6.5)
= η
 
g
I i
PSC(t g
d −d) −

h
I i
PSC(th
o −d)

where t g
d and th
o denotes the g-th desired spike and the h-th actual output spike,
respectively. The synaptic structure is shown in Fig.6.1b.
The dynamics of synaptic plasticity is demonstrated in Fig.6.1c. Similar to the
single-layer PSD, the weight adaptation in the MutPSD is triggered by the error
between the desired and the actual output spikes, with positive errors causing long-
term potentiation (LTP) and negative errors causing long-term depression (LTD). No
synaptic change will occur if the actual output spike ﬁres at the desired time. The
amount of synaptic changes is determined by the signal I i
PSC.

6.2 Multilayer Learning Rules
119
6.2.3
Multilayer Tempotron Rule
Thetempotronlearningrule[1]wasintroducedtotrainasingleneurontodiscriminate
between spatiotemporal spike patterns. Neurons are trained to distinguish between
two classes by ﬁring at least one spike or remaining quiescent. Whenever a neuron
failed to ﬁre a spike corresponding to a positive pattern, LTP will occur; if the neuron
ﬁred a spike to a negative pattern, LTD will happen.
The tempotron rule and the PSD rule are similar to some extent. In both rules,
the instructor signals are used to guide the direction of the synaptic modiﬁcation,
either potentiation or depression. The amount of synaptic change depends on the time
difference between the afferent spikes and the reference time tref . Figure6.2 shows
the learning windows. In the tempotron rule, tmax is the reference time for updating
synaptic weights. In the PSD rule, tref refers to td or to. In the tempotron rule, it
refers to tmax. In both the tempotron rule and the PSD rule, only the pre-synaptic
spikes that precede the reference time can induce the change of synaptic weights,
resulting in a construction of causal connections.
Based on the similarity with the PSD rule, a multilayer tempotron (MutTmptr)
rule can be developed as an extension of the single layer tempotron. The synaptic
plasticity for MutTmptr is described as follow:
Δwi =
⎧
⎨
⎩
η 
ti<tmax K(tmax −ti −d), if P+ error;
−η 
ti<tmax K(tmax −ti −d), if P−error;
0, otherwise.
(6.6)
where tmax denotes the time at which the neuron reaches its maximum potential value
in the time domain, and d denotes the synaptic time delay. The above equation is
equivalent to the follow:
Δwi =
⎧
⎨
⎩
η · I i
PSC(tmax −ti −d), if P+ error;
−η · I i
PSC(tmax −ti −d), if P−error;
0, otherwise.
(6.7)
Fig. 6.2 Similarity between
the PSD rule and the
tempotron rule on learning
windows. The amount of
synaptic change depends on
the time difference Δt
between the afferent spikes
tpre and the reference time
tref

120
6
Temporal Learning in Multilayer Spiking Neural Networks …
where I i
PSC denotes the post-synaptic current (PSC) of the corresponding synapse.
The instructor signal, containing the modiﬁcation direction, is back propagated to
all synapses in the multilayer network. The amount of synaptic change depends on
the PSC signal. Equations6.5 and 6.7 are used to conduct the following simulations.
6.3
Heuristic Discussion on the Multilayer Learning Rules
In this section, we use a simple three-layer network (see Fig.6.1) to analyze the
process of the synaptic modiﬁcation in our MutPSD and MutTmptr rules. For sim-
plicity, neurons are connected through synapses without delays. Synaptic change
between the input and the hidden neurons is denoted as Δwih. Δwho refers to the
change between the hidden and the output neurons.
The following cases would occur along the learning.
1. The output neuron ﬁres a spike at to in MutPSD or ﬁres a spike to negative
patterns in MutTmptr:
The LTD process will occur. The depression is back propagated to all synapses,
resulting in Δwih < 0 and Δwho < 0. The excitatory synapses will become less
excitatory and the inhibitory synapses will become more inhibitory. This could
eliminate the wrong spike of the output neuron. A decrease in who could cause the
decrease in the potential of the output neuron, thus the spike could be eliminated.
The decrease in wih would result in a silent response of the hidden neuron.
Withoutthestimulatingsignal(spikes)fromthehiddenneuron,theoutputneuron
could become silent as desired.
2. The output neuron fails to ﬁre a spike at td in MutPSD or keeps silent to a positive
pattern in MutTmptr:
The LTP process will occur. Similar to the depression process, the potentiation
is back propagated to all synapses, resulting in Δwih > 0 and Δwho > 0. The
excitatory synapses will become more excitatory and the inhibitory synapses
will become less inhibitory. As a result, the potential of the output neuron could
be increased, leading to a spike correspondingly.
3. The output neuron reacts correctly as desired:
In the MutPSD rule, this means the output neuron only ﬁres at the time of td. In
the MutTmptr rule, it means the output neuron ﬁres at least one spike to positive
patterns and keeps silent to negative patterns. If the output neuron responds as
desired, no synaptic modiﬁcation occurs.
The instructor signal guides the direction of the synaptic modiﬁcation, leading
the output neuron to a desired response if such a solution exists.

6.4 Simulation Results
121
6.4
Simulation Results
In this section, several simulation experiments are conducted to demonstrate the
capabilities of the algorithm. Firstly, through the association of spatiotemporal spike
pattern by the MutPSD rule, we demonstrate how the causal connections are con-
structed. Both the MutPSD and the MutTmptr rules are then applied to classic bench-
marks, including the XOR problem and the Iris dataset.
6.4.1
Construction of Causal Connections
In order to demonstrate the construction of causal connections, the MutPSD rule is
used to train the neuron to associate the input spatiotemporal spike pattern with a
desired spike train.
6.4.1.1
Technical Details
We construct the network in the structure of 50 × 100 × 1, without the synaptic
delay. The input spatiotemporal spike pattern connects with the network through
the input neurons. The spatiotemporal spike pattern is designed in a single-spike
manner, where each input neuron only ﬁres once within a time window of 30 ms.
The output neuron is trained, within a max number of training epochs (150), to ﬁre
a desired spike train of [10, 20, 30] ms. The initial weights are uniformly distributed
in the range of [0, 0.5]. We set η = 0.01 and τs = 7 ms. The learning is considered
converged when each of the actual output spike approaches to the corresponding
desired spike within a precision of 0.1 ms.
6.4.1.2
Analysis of the Learning
Figure6.3a shows the input spatiotemporal spike pattern. The network is trained to
associate this spike pattern with the desired spike train. As is shown in Fig.6.3c,
the output neuron gradually learns to ﬁre at the desired times. At the begin, both
the ﬁring rate and the precise time of the output spikes are different from those of
the desired spikes. Along the learning, the output neuron can successfully ﬁre the
desired spikes. This can also be reﬂected through the spike distance graph, where
a small distance denotes a big similarity between the desired and the actual output
spike trains.
Figure6.3b shows the ﬁring behavior of the hidden neurons. Before learning, the
spikes of the hidden neurons are far away from the desired time, thus it is difﬁcult
for these hidden spikes causing desired output spikes. After learning, a sufﬁcient
number of hidden spikes appear before each desired spike. These hidden spikes are

122
6
Temporal Learning in Multilayer Spiking Neural Networks …
(a)
(b)
(c)
(e)
(d)
Fig. 6.3 Construction of causal connections. The multilayer network is trained to output a desired
spike train associating with the input pattern. a is the demonstration of the input spatiotemporal
spike pattern. b shows the spikes of the hidden neurons before learning (blue) and after learning
(magenta). Vertical red lines denote the target time. c shows the actual output spikes along the
learning epochs. The spike distance between the desired and actual output spike trains is also
shown. Shaded bars denote the desired spike time. d demonstrates the weight matrix of the network
that relating to the target time of 10 ms. The intensity reﬂects the weight value, and white boxes
mean the corresponding connections do not ﬁre before this target time. e demonstrates a connection
view of the corresponding part in d. Shaded neurons mean that they ﬁred before 10 ms, thus they
are wired up to construct causal connections. The weight strength is denoted by the line width
necessary for resulting in spikes at the desired times. We denote those pre-synaptic
spikes that resulting in a post-synaptic spike as the causal spikes. Another necessary
factor for causing desired spikes is that the synaptic weights corresponding to the
causal spikes should be ﬁne tuned.
Figure6.3b demonstrates one necessary factor with respect to the causal spikes.
The other necessary factor regarding to the weights are shown in Fig.6.3d, e. For
simplicity, only the causal connections for ﬁring a target spike at 10 ms are shown.
Figure6.3d shows the weight matrix of the network. For example, the red rectangle
reﬂects the weights relating to a speciﬁc hidden neuron, with upper ﬁgure showing
connections from input neurons to this hidden neuron, and lower ﬁgure representing
the weight from this hidden neuron to the output neuron. In the weight matrix ﬁgure,
the white boxes mean the corresponding connections do not have causal spikes.
Figure6.3e shows the connection structure corresponding to the part in Fig.6.3d.
Neurons without causal spikes do not have effect on the desired spikes. As can be

6.4 Simulation Results
123
seen from the ﬁgure, causal neurons are connected with ﬁne tuned weights, including
both the excitatory and inhibitory synapses.
6.4.2
The XOR Benchmark
The XOR problem is a linearly nonseparable task, and it is a classic benchmark
problem widely used for investigating the classiﬁcation ability of spiking neural
networks recently [6, 8, 10, 14]. Thus, we also use the XOR task to investigate the
ability of our MutPSD and MutTmptr rules in this section. Detailed experimental
setup and results are presented as follows.
6.4.2.1
Technical Details
Similar to [6], the input and output patterns for the XOR task are encoded into spikes
(as can be seen in Table6.1). The XOR input of 0/1 is directly converted to the spike
input of 0/6 ms. In addition to these two inputs, a third neuron with an input spike
at 0 ms is used to serve as the time reference. Without this time reference, pattern
(0, 0) and (1, 1) would be identical in the view of spikes, thus the network would be
unable to distinguish them.
We choose the network structure as 3 × 5 × 1. Additionally, multiple sub con-
nections (mSub) with different delays were used. We set mSub = 15, with delays
ranging from 0 to 12 ms. The network was trained with η = 0.01 and τs = 7 ms,
otherwise will stated. The network was simulated with a time window of 30 ms and
a time step of 0.1 ms.
6.4.2.2
Demonstration of the Learning
The capabilities of both the MutPSD and the MutTmptr rules on the XOR task are
demonstrated here. In the MutPSD rule, the output neuron is required to ﬁre desired
spikes with a precision of 0.2 ms corresponding to different input patterns. In the
Table 6.1 XOR problem description for multilayer SNNs
XOR input
Encoded spike input (ms)
MutPSD
output (ms)
MutTmptr
output
(0, 0)
0
0
0
16
Fire
(0, 1)
0
6
0
10
Silent
(1, 0)
6
0
0
10
Silent
(1, 1)
6
6
0
16
Fire

124
6
Temporal Learning in Multilayer Spiking Neural Networks …
Fig. 6.4 Demonstration of the MutPSD rule for the XOR task. The top row shows the four spike
input patterns of the XOR task. The middle row shows the actual output spikes according to each
input pattern. The shaded bars denote the desired spike time. The average spike distance is also
shown on the right. The bottom row shows the output spikes of the hidden neurons for the input
pattern of (0, 0)
MutTmptr rule, instead of ﬁring precisely, the output neuron is only required to
correctly ﬁre or not ﬁre corresponding to an input pattern.
Figure6.4 demonstrates the MutPSD rule can successfully train the network to
learn the XOR task. As is shown in Chap.5, the single-layer PSD cannot directly
learn this task, unless a reservoir network is used to enrich the dimension of the input
space. In our MutPSD rule, a small number of hidden neurons with adjustable weights
are sufﬁcient for the XOR task. Along the learning, the output neuron gradually
learns to ﬁre desired spikes corresponding to different input patterns. The spike
distance between the desired and the actual output spikes decreases gradually. The
synaptic efﬁcacies of the hidden neurons are also modiﬁed along the learning, which
is reﬂected from the adjustment of their spike times. The adjusted spike time of the
hidden neurons can facilitate the output neuron to ﬁre desired spikes. These hidden
spikes serve as the stimulating sources for the output neuron.

6.4 Simulation Results
125
Fig. 6.5 Demonstration of the MutTmptr rule for the XOR task. The top row shows the four spike
input patterns of the XOR task. The middle row shows the actual output spikes of the hidden neurons
according to each input pattern. The bottom row shows the membrane potentials of the output neuron
for each corresponding input pattern
As can be seen from Fig.6.5, the MutTmptr rule can also train the network to
perform the XOR task well, with the output neuron ﬁring a spike for patterns of
(0, 0) and (1, 1), and keeping silent for (0, 1) and (1, 0). The hidden neurons ﬁre
differently for each input pattern. Again, these spikes from the hidden neurons serve
as the stimulating sources for the output neuron. Noteworthily, although 5 hidden
neurons are chosen for the XOR task, only a small number of these hidden neurons
(#1, #4, #5) are utilized. Therefore, our multilayer learning rule can effectively select
a sufﬁcient number of resources that are enough to fulﬁll the task.
6.4.2.3
Convergence of the Learning
In order to investigate the convergent performance of our multilayer learning rules,
the previous demonstration experiment is conducted for 50 runs. For the MutPSD
rule, a precision of 1 ms is used as in other studies [10, 15]. The average results are
reported in Table6.2.

126
6
Temporal Learning in Multilayer Spiking Neural Networks …
Table6.2 shows our multilayer learning rules are more efﬁcient for the XOR task.
To train the output neuron to spike precisely corresponding to different patterns,
our MutPSD rule has a faster convergent speed compared to other rules. A higher
successful rate of runs is also obtained compared to that in [10]. In addition, with less
number of learning parameters, our MutPSD rule is simpler compared to multilayer
ReSuMe rule in [10]. Regardless of ﬁring precisely, the MutTmptr rule converges
even faster. This is expected since only a response of ﬁre or not ﬁre is considered for
the MutTmptr rule. Such a binary response can simplify the learning compared to
those rules for precise response in time.
Figure6.6 shows the effect of the learning rate η on our multilayer learning rules.
As can be seen in this ﬁgure, a smaller η results in a slower learning speed. The
learning becomes faster with an increasing η. However, a further increase in η cannot
beneﬁt the learning. The successful rate of runs can be decreased with a larger η
(results are not shown here). Additionally, the learning speed of the MutTmptr rule
is always faster than that of the MutPSD rule. As discussed before, this is because
the MutTmptr rule only needs to train the neuron to have a binary response of ﬁre or
not, regardless of the precise time of the response.
Table 6.2 Convergent results for the XOR problem
Precision of
convergence (ms)
No. of epochs for
convergence
Successful rate (%)
Bohte [6]
0.71
250
–
McKennoch [15]
1.0
127
–
Sporea [10]
1.0
137
98
MutPSD
1.0
86
100
MutTmptr
–
37
100
Fig. 6.6 Effect of the
learning rate on the
convergence of the XOR
task. Results are averaged
over 50 runs

6.4 Simulation Results
127
6.4.3
The Iris Benchmark
In order to investigate the recognition performance of our multilayer rules, the classic
Iris benchmark task is considered. The dataset consists of three classes of Iris ﬂowers,
with one class being linearly separable from the other two classes, and two classes
being nonlinearly separable with each other. Each class contains 50 samples and
each sample is represented by 4 variables.
6.4.3.1
Technical Details
To encode the variables of Iris, we use the same population encoding scheme as
in [6, 9], where each feature is encoded separately by an array of Gaussian func-
tions with different centers. For a variable x in a range [xmin, xmax], n neurons
with different Gaussian receptive ﬁelds are used to encode. The center and width
of the i-th neuron are set to μi = xmin + (2 · i −3)/2 · (xmax −xmin)/(n −2) and
σi = 1/1.5 · (xmax −xmin)/(n −2), respectively. Each feature is encoded as n (set
as 5) spike times between 0 and 10 ms. Thus, the total number of input neurons is
4 × 5 + 1 = 21. The number of hidden neurons is selected as 8. The number of sub
connections is set to 5, and each synapse has a synaptic delay between 0 and 10 ms.
Three networks of 21 × 8 × 1 are constructed with each network for one class. The
upper limit of training epochs is set to 300. For the MutPSD rule, each network is
trained to ﬁre a desired train of [15, 25] ms corresponding to the correct input class,
and to keep silent for other classes. In the MutTmptr rule, each network is trained to
ﬁre a spike for the positive class, and to keep silent for other classes.
6.4.3.2
Analysis of the Learning
We use a winner-take-all scheme for the readout. For the MutPSD rule, the network
with closest spike distance dominates the class of the input pattern. For the MutTmptr
rule, two different winner-take-all readout schemes are investigated. One regards to
the ﬁre status (denoted as MutTmptr_Fire), and the other one regards to the maximum
potential (denoted as MutTmptr_Vmax).
As can be seen from Fig.6.7, the MutTmptr rule can learn the training set better
than the MutPSD rule, while the MutPSD rule has a better generalization perfor-
mance. It can be seen from Fig.6.7b, the testing accuracy tends to increase with the
increasing number of samples used for training. If only output spikes are considered
for the readout, the MutPSD rule performs better than the MutTmptr_Fire rule. This
is because the MutPSD rule makes decision based on a combination of several local
temporal features, but the MutTmptr rule only uses single temporal feature for the
decision. In addition, the MutTmptr rule requires all the three nets to response cor-

128
6
Temporal Learning in Multilayer Spiking Neural Networks …
(a)
(b)
Fig. 6.7 Performance of multilayer learning rules on the Iris task. a and b show the training and
testing accuracy, respectively. Results are averaged over 10 runs
rectly for a correct decision. This is another factor affects its performance. If we use
the maximum potential for the decision, the performance is improved signiﬁcantly
(see MutTmptr_Vmax in Fig.6.7b).
6.5
Discussion and Conclusion
In this chapter, we proposed two learning rules for multilayer SNNs, namely the
multilayer PSD rule (MutPSD) and the multilayer tempotron rule (MutTmptr). These
two rules are similar, where a supervisor signal, containing the synaptic modiﬁcation
direction, is back propagated to the synapses in the network. Without complex error
evaluationas in[6, 9, 10], our multilayer learningrules aresimpler andmoreefﬁcient.
In addition, it is not biologically plausible for the neurons to back propagated a
calculated error, or it is at least questionable. A global neuromodulatory signal,
determining the polarity of the synaptic changes, would be more feasible [1].
Theamountofsynapticchangedependsonthepre-synapticcurrents.Thisscheme,
combined with the supervisor signal, can help to construct the causal connections
between neurons. Correlated neurons are connected with ﬁne tuned weights, result-
ing in a desired response at the output neuron. The hidden neurons serve as the
information transmitter between the input and output neurons.
The MutTmptr rule has a faster convergent speed than the MutPSD rule. This
is because the MutTmptr rule only trains the network to respond correctly with
a binary status, either ﬁre or not. For the MutPSD rule, the precise spike time of
the output neuron is also considered. This makes the learning more difﬁcult than
the MutTmptr rule. However, the MutPSD rule has a better generalization ability
compared to the MutTmptr rule. This is due to that, the MutPSD makes a decision

6.5 Discussion and Conclusion
129
based on a combination of several local temporal features, while the MutTmptr uses
only a single local temporal feature for a decision.
In summary, both the MutPSD and the MutTmptr rules are simple, efﬁcient and
yet biologically plausible. We demonstrate the mechanisms that how the causal con-
nections are constructed in the multilayer spiking neural networks. The performances
of our multilayer learning rules are investigated through the two classic benchmark
tasks, that is the XOR task and the Iris dataset problem. The MutTmptr rule can
provide a faster learning speed, while the MutPSD rule gives a better generalization
ability.
References
1. Gütig, R., Sompolinsky, H.: The tempotron: a neuron that learns spike timing-based decisions.
Nat. Neurosci. 9(3), 420–428 (2006)
2. Ponulak, F.: ReSuMe-new supervised learning method for spiking neural networks. Institute
of Control and Information Engineering, Pozno´n University of Technology, Tech. rep. (2005)
3. Florian, R.V.: The Chronotron: a neuron that learns to ﬁre temporally precise spike patterns.
PLoS One 7(8), e40,233 (2012)
4. Mohemmed, A., Schliebs, S., Matsuda, S., Kasabov, N.: SPAN: spike pattern association neuron
for learning spatio-temporal spike patterns. Int. J. Neural Syst. 22(04), 1250,012 (2012)
5. Yu, Q., Tang, H., Tan, K.C., Li, H.: Precise-spike-driven synaptic plasticity: Learning hetero-
association of spatiotemporal spike patterns. PLoS One 8(11), e78,318 (2013)
6. Bohte, S.M., Kok, J.N., Poutré, J.A.L.: Error-backpropagation in temporally encoded networks
of spiking neurons. Neurocomputing 48(1–4), 17–37 (2002)
7. Ghosh-Dastidar, S., Adeli, H.: Improved spiking neural networks for EEG classiﬁcation and
epilepsy and seizure detection. Integr. Comput.-Aided Eng. 14(3), 187–212 (2007)
8. Ghosh-Dastidar, S., Adeli, H.: A new supervised learning algorithm for multiple spiking neural
networks with application in epilepsy and seizure detection. Neural Netw. 22(10), 1419–1431
(2009)
9. Xu, Y., Zeng, X., Han, L., Yang, J.: A supervised multi-spike learning algorithm based on
gradient descent for spiking neural networks. Neural Netw. 43, 99–113 (2013)
10. Sporea, I., Grüning, A.: Supervised learning in multilayer spiking neural networks. Neural
Comput. 25(2), 473–509 (2013)
11. Gollisch, T., Meister, M.: Rapid neural coding in the retina with relative spike latencies. Science
319(5866), 1108–1111 (2008)
12. Thorpe, S., Fize, D., Marlot, C.: Speed of processing in the human visual system. Nature
381(6582), 520–522 (1996)
13. Johansson, R.S., Birznieks, I.: First spikes in ensembles of human tactile afferents code complex
spatial ﬁngertip events. Nat. Neurosci. 7(2), 170–177 (2004)
14. Xu, Y., Zeng, X., Zhong, S.: A new supervised learning algorithm for spiking neurons. Neural
Comput. 25(6), 1472–1511 (2013)
15. McKennoch, S., Liu, D., Bushnell, L.G.: Fast modiﬁcations of the spikeprop algorithm. In:
International Joint Conference on Neural Networks, 2006. IJCNN’06, pp. 3970–3977. IEEE
(2006)

Chapter 7
A Hierarchically Organized Memory
Model with Temporal Population Coding
Abstract Memory is a critical process in the brain to many cognitive behaviors. It is
a complex process operating across different brain regions. However, the organizing
principles of memory systems remain unclear. Emerging experiment results show
that memories are represented by population of neurons and organized in a categor-
ical and hierarchical manner. In this work, we describe a hierarchically organized
memory (HOM) model using spiking neurons, in which temporal population codes
are considered as the neural representation of information and spike-timing-based
learning methods are employed to train the network. The results have demonstrated
that memory coding units are formed into neural cliques, and information are stored
in the form of associative memory within gamma cycles. Moreover, temporally sepa-
rated patterns can be linked and compressed via enhanced connections among neural
groups forming episodic memory. Our model provides a computational interpretation
of memory organization at a system level.
7.1
Introduction
Memory is an extremely complex and brain-wide process, which is an indispensable
part of what makes various intelligence. Researchers have devoted signiﬁcant effort
to investigating what is memory and how it works. In order to understand amazing
functions of human memory, two basic questions should be answered in the ﬁrst
place: what is the internal representation of memory and how memory is organized
in the brain?
During the last few decades, a great deal of works have been conducted to inves-
tigate how information is represented in the nervous system. As a traditional coding
scheme, rate coding assumes that the most important information about a stimulus
can be described by the ﬁring rates of sensory neurons. However, rate codes fail
to describe rapidly varying real-world stimuli. Recent experimental studies show
that spike timing makes sense in visual [1], auditory [2], olfactory [3] pathways and
hippocampus [4] in various neuronal systems [5]. It has been reported that precisely
timed spikes play a pivotal role during the integration process of cortical neurons [6].
In addition, studies of population coding suggest that information can be encoded by
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_7
131

132
7
A Hierarchically Organized Memory Model with Temporal Population Coding
clusters of cells rather than single cells. Population coding has been found existing
throughout the nervous system. Visual features are encoded with population codes
in the visual cortex [7], movement directions are found to be related to populations
of motor cortical neurons [8], and place cells have been identiﬁed when an animal
passes by a speciﬁc location in an environment [9, 10]. Recently, temporal population
coding has been demonstrated to be capable of encoding visual stimuli invariantly
[11]. We believe that the combination of both temporal codes and population codes
provides an alternative approach to achieving neural representation of information
in the nervous system.
The organization of memory is closely associated with learning process that never
ceases throughout the brain. With the development of large-scale ensemble recording
techniques, network-level functional coding units have been identiﬁed in the hip-
pocampus [12]. Moreover, a recent study on population response patterns in monkey
inferiortemporalcortexsuggeststhatexternalstimulicanberepresentedbyresponses
of neural populations in monkey inferior temporal (IT) cortex, and encoded memory
patterns are organized in a hierarchical order with combination of neural cliques [13,
14]. As a special form of Hebbian learning, STDP process and other spike-timing
based learning schemes are believed to be involved in the formation of neural cliques
and associative learning. Although different spiking neural network memory models
and learning algorithms have been proposed, few of them employ temporal codes as
the neural representation or pay enough attention to memory organization.
Researchers have proposed various working memory models exploring different
mechanisms to achieve persistent neural activity (for review please refer to [15,
16]). Different learning algorithms for spiking neuron have been proposed to study
hetero-association [17–22]. Whereas, memory models concerning auto-associative
memory and episodic memory remains underexplored. Moreover, among existing
memory models using spiking neural network, how to realize memory function with
temporal codes and how memory is organized in nervous system still need more
investigation.
The Cortext model [23], which is inspired by the anatomical structure of the
cerebral cortex, is known as a hierarchical model for word recognition. The Cortext
model consists of three layers mimicking V1, V2 and IT, while the information
processing mechanism is based on a predictive coding scheme. In one macrocolumn,
which is composed of a group of minicolumns, only one minicolumn is tuned to
a particular feature that could occur in its receptive ﬁeld and the input patterns
are manually fed to different columns. Therefore, due to unused minicolumns, the
memory capability of the model and its scaling up ability could be limited. Although
the timing of spikes are employed to make the decision that which candidate is
most likely to be the input pattern, less attention has been paid to the issue that how
temporal information is exploited in the model.
We propose an hierarchically organized memory (HOM) model aiming to inves-
tigate the formation of neural cliques and the organization principle followed by
learning with precisely timed spikes in a hierarchically structured spiking neural
network. Sensory information travels upwards along the hierarchical network during
the bottom-up information processing (Fig.7.1). With a spike-timing based learning

7.1 Introduction
133
algorithm during the storing phase, the spiking neural network is able to map sensory
information into neural clique activities. Meanwhile, auto-associative memory can
be generated via fast NMDA dominated STDP process in Layer I, and sequence
learning can be performed by slow NMDA dominated STDP in Layer II. The main
contribution of this work is to propose a cognitive memory model focusing on mem-
ory organization using temporal population codes. We believe that the HOM model is
a good attempt to explore the underlying mechanisms of formation and organization
of memories in the brain.
The structure of this chapter is as follows: in the next section, we introduce the
general structure of the HOM model, neural representation of sensory information,
learning algorithm and etc. In Sect.7.3, the performance of the proposed model is
demonstrated by numeric simulation results. In Sect.7.4, important issues of the
HOM model and relations to other methods are discussed. At last, a summary of
results is presented in Sect.7.5.
7.2
The Hierarchical Organized Memory Model
The basic HOM model is composed of three layers: input layer, Layer I and Layer II.
As shown in Fig.7.1, neurons in the lower layer are fully connected to its next higher
layer,whereaslateralconnectionsexistinLayerIandLayerII.Theinterneuronsaside
provide feedback inhibition to prevent continuous ﬁring and temporally separate
ﬁring events representing different memory items into gamma cycles.
7.2.1
Neuron Models and Neural Oscillations
The spike response model (SRM) [24], which provides a simple description of the
spikingneuron,hasbeenwidelyusedinvariousstudies.Pyramidalcellsareemployed
in Layer I simulating short-term memory (STM). By utilizing the slow build-up ramp
of after-depolarizing potential (ADP) of pyramidal cells [25], the status of neurons
can be maintained. We plug ADP kernel into the generic spike response model, so
that the state of pyramidal neuron i at time t is described as
Vi(t) = ηADP(t −ti) +

j
wi jεi j(t −t j) + Vrest
(7.1)
where ti and t j denote ﬁring times of the presynaptic neuron i and the post-synaptic
neuron j, respectively. ηADP(t −ti) is the ADP of neuron i, and wi j is the synaptic
efﬁcacy from neuron j to neuron i. A spike is generated when the membrane potential
reaches its threshold Vthr = 1. The membrane potential is set to Vrest = 0 after each
ﬁring, while the ADP is reset and build up again.

134
7
A Hierarchically Organized Memory Model with Temporal Population Coding
... ...
......
Encoded Stimuli
Input Layer
Layer I
Layer II
interneurons
Fig. 7.1 The architecture of the three-layer HOM model. Neurons forming neural cliques and
enhanced lateral connections are illustrated in different colors. Two groups of interneurons feed
inhibition back to neurons in the same layer
The response of pyramid neuron after ﬁring is modeled as an α function as follows:
ηADP(t −ti) = AADP
t −ti
τADP
exp(1 −t −ti
τADP
)
(7.2)
where AADP is the amplitude of ADP, and τADP is the time constant affecting the
duration of excitatory ramp.
Theta and gamma oscillations are two important types of brain wave in syn-
chronizing the neural activity [9, 26]. They are believed to be critical for temporal
coding/decoding of active neuronal ensembles, learning and memory formation [27–
29]. An external theta oscillatory source, which injects current to neurons in Layer
I, is modeled as a cosine wave
iθ = Aθ cos(2π fθ + φ0)
(7.3)
where Aθ is the amplitude of sub-threshold membrane potential oscillation, fθ is
the frequency of theta oscillation, and φ0 is the initial phase. It has been found that
the memory capacity depends on the theta/gamma cycle length ratio, suggesting that
short-term memory is reserved within individual gamma cycles [30]. As the recent

7.2 The Hierarchical Organized Memory Model
135
direct evidence from human EEG scalp recordings supports the STM model [31],
each memory item is represented by ﬁrings in different gamma cycles in response to
speciﬁc stimulus. Meanwhile, inhibition from interneurons suppresses individuals
of other neural cliques.
7.2.2
Temporal Population Coding
With the interests in the functional role of temporal information carried by neural
activities, temporal codes have received increasing attention [6, 32]. The information
about stimulation is thought to be encoded by the time of spikes generated by a
speciﬁc population of neurons, and each input pattern is coded by a particular group
of neurons. Grayscale images are converted into single-spike spatiotemporal patterns
with temporal population codes mimicking the visual sensory encoding. A original
image is fed into Garbor ﬁlters and the output of Garbor ﬁlters are converted into
neural ﬁrings according to the following equation.
ti = f (si) = tmax −ln(α · si + 1)
(7.4)
where ti is the ﬁring time of neuron i, tmax is the width of encoding window, α is
a scaling factor, and si is the intensity of output of Garbor ﬁlter (see Fig.7.2). As a
result, each spike codes orientation components of the image and the latency denotes
the weight of corresponding component.
7.2.3
The Tempotron Learning and STDP
Thelearningrulesemployedinthemodelareexpectedtobecompatiblewithtemporal
codes. Among the existing spike-timing based learning approaches, the tempotron
rule has been shown to be a biologically plausible supervised synaptic learning
scheme [17].
Fig. 7.2 Encoding scheme. A grayscale image is convolved with Garbor ﬁlters to extract orientation
related features and then converted into a spike pattern by latency coding method

136
7
A Hierarchically Organized Memory Model with Temporal Population Coding
The tempotron learning has been employed in adjusting neural connectivities
between layers. When presented a pattern, each neuron needs to make a decision
on that whether the stimulus contains certain features that have been learned before.
The connections from neurons that contribute to the integrated postsynaptic mem-
brane potential during the presentation will be enhanced according to the tempotron
learning rule (Eq.7.5).
Δwi = λd

si<0
exp(si)
(7.5)
where wi is the synaptic weight from afferent i to the postsynaptic neuron, λ is the
learning rate, d is the desired output label (either 0 or 1), and si = ti −tmax is the delay
between presynaptic ﬁring (Si) and the time when postsynaptic membrane potential
V (t) reaches its maximal value Vmax. The tempotron learning rule is illustrated in
Fig.7.3.
Because NMDA receptor (NMDAR) is the predominant molecular device for
controlling synaptic plasticity, synaptic modiﬁcations (LTP and LTD) varies with
different postsynaptic NMDARs [33, 34]. Although the biophysical and biochemical
mechanisms that underpin STDP still need further investigation, these existing results
suggest that STDP is a NMDAR-dependent mechanism. In the proposed model, fast
andslowNMDAchannels(fast:τ 25msec,slow:τ 150msec)areadoptedtodominate
the synaptic transmission and plasticity in Layer I and Layer II, respectively. The
repetitive ﬁrings contribute to the enhancement of connections between activated
neuronsviaSTDPlearning.AsthetimecourseoftheactivationofNMDARscrucially
affects long term modiﬁcation, STDP learning performs differently with NMDA
channel in different states.
One should also note that feedback inhibition from interneurons leads to the restult
that spike volleys are temporally separated into individual gamma cycles. Therefore,
fast NMDA channel contributes to the formation of intra-clique connections (auto-
associative memory) in Layer I, and slow NMDA channel spanning over several
gamma cycles enhance inter-clique connections (episodic memory) in Layer II.
7.3
Simulation Results
In this section, we demonstrate that the proposed HOM model is capable of learning
input patterns, storing them into individual gamma cycles with population ﬁrings,
and performing sequence learning. Several experiments are conducted to illustrate
and analyze these processes.
As shown in Fig.7.1, the spiking neural network used to implement the HOM
model is composed of three layers. The synaptic weights are initialized according to
the population size of each layer.
Each input pattern is represented by tens of spikes using temporal population
codes as shown in Fig.7.2, and they are introduced to the network during troughs

7.3 Simulation Results
137
Vthr
Vrest
tmax
Vmax
ti
Neuron Number
Time
0
T
8
7
6
5
4
3
2
1
Afferent i
Postsynaptic Neuron
d
si
Synaptic
Change
s
(a)
(b)
Fig. 7.3 Illustration of the tempotron learning rule. a Typical input pattern using temporal popula-
tion codes. b Membrane potential of the postsynaptic neuron. The maximum value of the membrane
potential is reached at tmax. (inset) The synaptic weight wi changes accordingly to the time differ-
ence between s and the desired signal d. If d = 1, Δwi ≥0 (solid line), or if d = −1, Δwi < 0
(dashed line)
of the theta oscillation. Fast NMDA channels maintain activated state around 10ms
after the binding of glutamate to postsynaptic cells, while slow NMDA channels with
a slow deactivation time constant dominate the STDP process in Layer II (Fig.7.4).
The inter-layer synaptic weights are updated according to the tempotron learning rule
during the representation of input patterns (gray strips in Fig.7.5), while intra-layer
synaptic plasticity are modiﬁed by STDP.
Driven by input synaptic currents from afferents, increasing number of pyramidal
neurons in Layer I start to ﬁre and form different neural cliques iteration by iteration.
When enough stimulation are generated by neural activities in Layer I, similar phe-
nomena emergence in Layer II. After dozens of iterations, neural cliques response

138
7
A Hierarchically Organized Memory Model with Temporal Population Coding
Synaptic Change
s
0
1
2
3
4
5
6
7
Theta Oscillation
Gamma Oscillation
(a)
(b)
6-10 Hz
25-100 Hz
Fig. 7.4 LTP induced by STDP learning for different memory items. a Firings within each gamma
cycle represent memory items 0–7. b Synaptic changes depend on the relative time between ﬁrings.
Strong connections within the same neural cliques can be formed via the fast NMDA channel
(1 →0), while weaker LTPs are induced between neural cliques in different gamma cycles via the
slow NMDA channel (7 →0)
to speciﬁc patterns and repeat to ﬁre periodically during the subsequent theta cycles
(Fig.7.5).
As shown in Fig.7.5, we can identify groups of neurons ﬁring as neural cliques
in Layer I and II, respectively (Fig.7.5b, c). Within each theta cycle, neural cliques
respond selectively and repetitively to the stimulation as the same order that input
patterns are introduced to the network. As can be seen from Fig.7.5b, individual
letters (‘L’, ‘O’, ‘V’ and ‘E’) are separately encoded by the volley activities of
corresponding neural clique in Layer I. While neural activities generated by all the
four neural cliques in Layer II are coding for the word ‘LOVE’. The memory coding
principle is explained in detail in section III A and B, respectively.
Figure7.6 reveals the mechanism underlying repetitive ﬁring of pyramidal neu-
rons. After generation of the ﬁrst spike by a particular neuron, its ADP starts to build
up. When the slowly ramping up ADP meets near-peak theta current, the pyramidal
neuron will ﬁre again in the following theta cycle. Meanwhile, inhibitory feedback
from interneurons prevents neurons coding for other patterns from ﬁring right after
the volley spikes. As a result, spike volleys are temporally separated into individual
gamma cycles (Fig.7.5a).
When neurons initially start to ﬁre, the spike times are randomly distributed in
gamma cycles. With repetitive ﬁrings, synaptic weights between individual members

7.3 Simulation Results
139
0
100
200
300
400
500
600
700
800
900
1000
0
100
200
300
400
500
600
700
800
900
1000
0
20
40
60
80
100
0
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
- 1
- 0.8
- 0.6
- 0.4
- 0.2
0
Neuron #
Neuron #
Theta Oscillation
Theta Oscillation
0
100
200
300
400
500
600
700
800
900
1000
0
100
200
300
400
500
L
O
V
E
Neuron #
Pattern 1
Pattern 2
Pattern 3
Pattern 4
(a)
Raster Plot of Neurons of Layer I
Input Layer
(b)
(c)
Raster Plot of Neurons of Layer II
Time (ms)
‘L’, ‘O’, ‘V’, ‘E’
‘LOVE’
Fig. 7.5 Neural activity propagates through the system. a Encoded input patterns. Each pattern
consists of tens of neurons ﬁring within an encoding window (gray strips). b and c are the raster
plots of the neural activities in Layer I and II, respectively. Colored dots denote spikes generated
by neurons coding for different input patterns
of the same neural clique are strengthened with STDP learning, resulting in their
synchronized ﬁring as shown in Fig.7.5b.
Since fast and slow NMDA channels dominate the STDP process in Layer I and
II respectively, the resulting lateral connectivities are quite different. To verify it,
we examine the synaptic weights, especially intra-clique connections in Layer I and
inter-clique connections in Layer II as presented in Figs.7.7 and 7.8.
When exposed to external stimuli, neurons in Layer I start to ﬁre due to the
enhancement of connections from input layer to Layer I as shown in Fig.7.7. Once

140
7
A Hierarchically Organized Memory Model with Temporal Population Coding
0
100
200
300
400
500
600
700
800
900
1000
-0.5
0
0.5
1
1.5
-0.5
0
0.5
1
1.5
membrane potential
time (ms)
0
100
200
300
400
500
600
700
800
900
1000
time (ms)
ADP & inhibition
(a)
(b)
L
O
V
E
Neuron 1
Neuron 4
Neuron 3
Neuron 2
Fig. 7.6 Typical neural responses of pyramidal neurons in the same layer. a Membrane potentials
of neurons coding for different patterns. b Slow built-up ADP of pyramidal neurons (positive) and
inhibition from interneurons (negative)
neurons in Layer II receive enough stimulation from Layer I, they begin to generate
spikes. At the same time, activated neurons within the same layer wire together to
form neural cliques as shown in Figs.7.7b (W22) and 7.8b (W33).
To further study the resulted neural cliques and their connectivities, we take a close
look at synaptic connections within Layer I and II. Generally, lateral connections fall
into intra-clique, inter-clique and weak connections. The connectivity developed
after learning is illustrated in Fig.7.9. As non-activated neurons wires weakly to all
the other neurons, only intra-clique and inter-clique connections are drawn in the
following ﬁgure.
Since fast NMDA channels stay activated for several milliseconds, only ﬁrings of
neurons forming the same clique fall within this narrow time window. Consequently,
intra-clique connections are enhanced via STDP process and salient as shown in
Fig.7.7b. The highlighted weights matrices show that each neural assembly forms
a recurrent subnetwork with auto-associative memory coded in the enhanced lateral
connections.
Although lateral connections in Layer II were strengthened as those in Layer I,
the resulting connectivity is different from that in Layer I. As slow NMDARs have
a longer activation period spanning over several gamma cycles, spikes in different
cycles would induce enhancement of inter-clique connections. The salient weight
elements along the diagonal are similar to those in Layer I, in which auto-associative
memory is stored. While elements in the colored boxes denote connections between
neural cliques, in which episodic memories are encoded.

7.3 Simulation Results
141
20
40
60
80
100
50
100
150
200
250
300
350
400
450
500
550
12
14
16
18
20
22
24
26
28
20
40
60
80
100
50
100
150
200
250
300
350
400
450
500
550
0.02
0.04
0.06
0.08
0.1
0.12
0.14
W21 (Hetero-associative memry)
Neuron # (Layer I)
Neuron # (Layer I, rearranged)
Neuron # (Input Layer)
Neuron # (Input Layer)
W22 (Auto-associative memory)
Neuron # (Layer II)
Neuron # (Layer I)
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
×10-6
0
1
2
3
4
5
6
7
8
9
Neuron # (Layer II, rearranged)
Neuron # (Layer I)
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
0
2
4
6
8
10
12
14
16
18
20
×10-3
×10-3
(b)
(a)
Fig. 7.7 Neural connectivity from input layer to Layer I (a) and within Layer I (b) before (left)
and after learning (right). The activated neurons are picked out and rearranged for clear illustration
in the right column. Intra-neural clique connections are highlighted by colored boxes
In sum, neurons forming the same neural clique tend to ﬁre in synchrony, while
neural cliques coding for successive patterns are temporally compressed. The former
is caused by fast NMDA mediated STDP in Layer I, while the latter is a result of slow
NMDA mediated STDP in Layer II. As demonstrated in Fig.7.5, neurons coding for
different memories ﬁres in different gamma cycles in Layer I represent the detection
of individual patterns, while neural responses within each theta cycle in Layer II
representing the recognition of sequence of patterns. To understand from another
angle, the neural cliques identiﬁed in Layer II can be considered as a assembly
coding for the particular combination of patterns with their presentation sequence
encoded. Therefore, hetero-association between layers abstract information during
the bottom-up process, while inter-clique episodic memory binds information about
several temporally separated patterns (individual letters) into a compressed pattern
(a complete word).

142
7
A Hierarchically Organized Memory Model with Temporal Population Coding
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
0
2
4
6
8
10
12
14
16
18
20
W32 (Hetero-associative memory)
Neuron # (Layer II)
Neuron # (Layer II, rearranged)
Neuron # (Layer I)
Neuron # (Layer I)
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
11
12
13
14
15
16
17
18
19
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
0.02
0.04
0.06
0.08
0.1
0.12
0.14
×10-3
×10-3
Neuron # (Layer II,rearranged)
Neuron # (Layer II,rearranged)
20
40
60
80
100
10
20
30
40
50
60
70
80
90
100
×10-7
0
1
2
3
4
5
6
7
8
9
Neuron # (Layer II)
Neuron # (Layer II)
W33 (Episodic memory)
(b)
(a)
Fig. 7.8 Evolution of the neural connectivity from Layer I to Layer II (a) and within Layer II (b).
Inter-neural clique connections in Layer II are highlighted by colored boxes
7.3.1
Auto-Associative Memory
Our brain has a remarkable ability of association, despite constant changes in real-
world circumstance. During perception along sensory pathways, information about
external stimulation is abstracted and encoded into reliable neural activities. After
training, both hetero-associative memory and auto-associative memory are stored in
the connections between neurons. Input patterns are hetero-associated with neural
responses in Layer I via synaptic weights between input layer and Layer I, while
auto-associative memory is represented by intra-clique connections.
As neural activities can be observed as an explicit expression of stored memory,
pattern completion may refer to the ability that a subset of neurons from a partic-
ular neural clique are able to arouse the rest of that clique. The trained network
is expected to be competent for recalling stored neural activities upon presentation
of input patterns and retaining invariant responses in presence of noises and even
corruption of information. As information is distributed over neurons with popu-
lation coding scheme, the information loss caused by shifting or removing spikes

7.3 Simulation Results
143
Intra-clique connection
Inter-clique connection
Parymid Neurons
Fig. 7.9 Schematic diagram of developed lateral connectivity. Lateral connections within the same
layer are divided into intra-clique and inter-clique connections. Only inter-clique connections from
neural clique 1 to the rest are drawn for clear illustration
can be complemented with the aid of other contributing neurons. In order to investi-
gate this capability of reproducing neural activities, time jitter and missing of spikes
are considered in the following experiments. Hence, a correlation-based measure of
spike timing [35] is used to calculate the distance between an output pattern and its
corresponding target pattern.
C =
−→
s1 · −→
s2
|−→
s1 ||−→
s2 |
(7.6)
where C is the correlation denoting closeness between two temporal coded patterns
(s1 and s2). They are convolved with a low pass Gaussian ﬁlter of a width σ = 2ms.
By shifting ﬁring times of input spikes, variability of input patterns was simulated
as shown in Fig.7.10a. The shifting intervals were randomly drawn from a gaussian
distribution with mean 0 and variance [0, 5]ms. The correlation between reproduced
neural responses and the desired patterns are presented in Fig.7.10. Every simulation
has been repeated for 30 times to generate the averaged performance. Figure7.10b
shows that the network reproduces reliable neural patterns in the presence of shifting
of input spikes up to 3ms in both Layer I and II. However, the performance dramati-
cally drops to around 0.3 as the shifting interval increases to 5ms. Neural responses

144
7
A Hierarchically Organized Memory Model with Temporal Population Coding
(a)
Neuron number
8
7
6
5
4
3
2
1
9
10
t
(b)
0
1
2
3
4
5
0
0.2
0.4
0.6
0.8
1
j (ms)
C (j)
Layer I
Layer II
Fig. 7.10 a Illustration of shifted spatiotemporal patterns. Firing times of original input spikes
(black bars) are randomly shifted with random jitters (gray bars). b Reliability of retrieved neural
responses under different noise levels
in Layer I are slightly more robust than those in Layer II due to error accumulation
during the upwards information transmission.
Anadditionalexperimentisconductedtofurtherinvestigatethelinkbetweenintra-
clique connections and auto-associative memory. Since neurons in the same clique
may provide supplementary stimulation to sustain an united activity, corruption of
input patterns may not be a catastrophic error. All settings are the same as the previous
experiment, whereas one out of ten spikes is removed from each training pattern.
The experiment has been repeated for 20 times and the mean value of the correlation
between actual output and desired pattern is calculated for each trial.
As illustrated in Fig.7.11a, intra-clique connections are enhanced during learning,
while non-selective neurons are weakly connected to all the other neurons. To verify
that connections within neural cliques are responsible for the completion of patterns
in Layer I, lateral connection are removed. As shown in Fig.7.11b, the experimental
results are consistent with our analysis.
Knowledge stored in the synaptic weights from the input layer to Layer I provides
the capability of hetero-association (Fig.7.7a) by recognizing input patterns via their
speciﬁc features. At the same time, intra-clique connections (Fig.7.7b) contribute to
the pattern completion which is one of the most important features of auto-associative
memory, in Layer I. Since corrupted patterns provide insufﬁcient stimulation to
neurons in the next layer, some of the trained neurons that should have been activated
may not be triggered. Fortunately, lateral inputs from excited neurons can provide
supplementary information to recall desired neural responses. Therefore, associative
memory, which can be understood as the ability to retrieve invariant responses with
partial information, relies on both the distributed storage of knowledge in synaptic
connectivity between layers and within Layer I.

7.3 Simulation Results
145
1
2
0.8
0.6
0.2
0.4
1
0
1 - with latteral connection
2 - no latteral connection
Cm
Nonselective
Selective 
(a)
(b)
Enhanced
Weak
Fig. 7.11 a Illustration of neural cliques in Layer I coding for different input patterns (letters) after
learning. Intra-clique connections of selective neurons are enhanced to form recurrent networks
and non-selective neuron are weakly connected to other neurons. b Test results of the associative
memory based on the correlation between retrieved and corresponding desired patterns in response
to corrupted input patterns
7.3.2
Episodic Memory
In the previous experiment, fast NMDA channel contributes to the formation of
neural clique together with the auto-associative memory. While slow NMDA channel
dominates the STDP process in Layer II, which leads to different postsynaptic neural
responsesanddifferentconnectivity.TheslowdecayingtimeconstantofslowNMDA
channel leads to accumulation of EPSPs from different neural cliques. Meanwhile,
slow NMDA receptors sustain activation state over several gamma cycles, which
enables STDP learning to link sequence of memory items by building up inter-clique
connections. When lateral connections are sufﬁciently developed, the accumulated
EPSPs of occurred memory items would be able to trigger subsequent items without
the presentation of expected upcoming input stimulation during memory recall.
As demonstrated in Fig.7.12, stimulation caused by neural cliques coding for the
ﬁrst three memory items in the sequence is strong enough to trigger the particular
neural clique coding for the missing item. The inter-clique connections may lead to
the result that consecutive memory items are temporally compressed as a group of
neuron coding for the combination of several patterns in the sequence. This charac-
teristic is crucial for a spike-timing based hierarchical model, which contributes to
pattern/information binding process.
Figure7.13 shows how EPSPs of consecutive memories lead to the activation of
neural cliques coding for the next upcoming pattern, which is not presented to the
network. Since only the ﬁrst three letters were inserted during recall of episodic
memory, neural cliques coding for them in Layer I and Layer II ﬁred in response to
the stimulation as shown in Fig.7.13a, c. However, due to the slow-NMDA medi-
ated synaptic transmission and enhanced inter-clique connections in Layer II, neural

146
7
A Hierarchically Organized Memory Model with Temporal Population Coding
raster plot of neurons in Layer II
raster plot of neurons in Layer I
time (ms)
neuron no.
neuron no.
(a)
(b)
neural clique 1
neural clique 3
neural clique 2
(c)
0
100
200
300
400
500
600
700
800
900
1000
0
100
200
300
400
500
600
700
800
900
1000
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
neural clique 4
Fig.7.12 a IllustrationofgeneratedconnectivityinLayerII.Synapticweightsamongneuralcliques
are strengthened via slow NMDA dominated STDP learning. b Raster plot of neural activities in
Layer I during recall. Neurons coding for letters ‘L’, ‘O’ and ‘V’ detect the presentation of them and
trigger corresponding ﬁrings in Layer II. c Episodic memory stored in Layer II enables the recall
of missing item (‘E’) by triggering ﬁring of neurons coding for it
clique coding for the next “missing” pattern were triggered by the accumulation of
EPSPs induced by the neural cliques coding for preceding patterns (Fig.7.13b). As a
result, we can retrieve the full sequence by observing the neural activities in Layer II
as shown in Fig.7.13c. In addition, neural activities can be converted to binary codes
according to their behaviors within a certain coding time window (gray strips in
Fig.7.13c). By reading out these binary codes, we can identify the presence of indi-
vidual features/patterns in Layer I and combination of features/patterns (sequence)
in Layer II.

7.4 Discussion
147
620
625
630
635
640
645
650
655
660
665
670
0
20
40
60
80
100
Neuron no.
Neuron no.
Membrane Potential
(a)
(b)
(c)
620
625
630
635
640
645
650
655
660
665
670
20
40
60
80
100
1
1
1
0
...
0
0
L
...
0
0
1
1
1
1
O
0
0
0
0
1
1
...
Time (ms)
...
1
1
1
0
0
0
V
620
625
630
635
640
645
650
655
660
665
670
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Fig. 7.13 Recall of neural clique activities induced by accumulated EPSPs. a Response of neural
cliques coding for the presented patterns in Layer I. b Membrane potential of an activated neuron
coding for the expected upcoming pattern (‘E’) in Layer II. c Raster plot of neural activities in Layer
II and their corresponding binary codes
7.4
Discussion
7.4.1
Information Flow and Emergence of Neural Cliques
Basically, the information ﬂow in the model is unidirectional from bottom to top.
Information between layer travels upward along the network with a ﬁltering process,
while recurrent subnetworks (neural cliques) exist in both Layer I and Layer II. Input

148
7
A Hierarchically Organized Memory Model with Temporal Population Coding
stimulation trigger repetitive neural activities in Layer I, these activities further drive
neural responses in Layer II. In addition, interneurons aside each layer feed inhibition
back to neurons that trigger them.
During learning, neurons compete with each other to respond selectively to spe-
ciﬁc stimulus. Synaptic weights between layers stop changing when the population
sizes of evoked neurons reach predeﬁned conﬁguration. Since neural responses in
higher layer rely on the input stimulation from lower layer, the emergence of neural
cliques in higher layer lags those in lower layer. Although bounded synapses have
limited memory storage capacity [36], they are used in our model to ensure a certain
number of presynaptic neurons contributing the generation of postsynaptic spikes.
7.4.2
Storage, Recall and Organization of Memory
After memory representation is deﬁned as neural clique response, storage and recall
are the rest two key issues for a memory model. During learning input patterns,
hetero-association is achieved by enhanced synaptic weights between layers. Once
pyramidal neurons are triggered to ﬁre, cooperation of ADP and theta oscillation
results in the repetitive ﬁring of neurons coding for memory items every theta cycle.
While STDP learning with fast/slow NMDA channels contributes to the enhancement
of intra- and inter-clique connections, respectively.
Theta oscillation has been recorded in hippocampus involved of memory function.
A recent model [37] suggests that memories might be encoded and recalled during
different portions of the theta cycle. Similar scheme is employed in our model, hetero-
associative memory storage occurs during troughs of theta oscillation, while stored
memories are retrieved during portions near the maximums of theta oscillation. Since
activation of neural activities at troughs requires strong excitation, the resulted synap-
tic efﬁcacies are stronger than required at the maximums. Redundant excitation and
distributed information over neurons improves the robustness of recalling hetero-
associative memories. Environmental noises and even information loss will not lead
to a catastrophic retrieval failure as demonstrated in the simulation results. In addi-
tion, inhibition of negative half of theta oscillation suppresses excitation induced by
enhanced lateral connections when storage. On the other hand, if insufﬁcient stim-
ulation is injected into the network during retrieval, excitation of positive half may
provide supplementary stimulation.
As demonstrated in simulation results, multiple patterns are encoded and stored
into associative and episodic memory following a hierarchical organization principle.
Hereto-associative memory is encoded by the connectivity between layers, which
plays a pivotal role in recognizing speciﬁc patterns. Along with the development
of neural cliques, lateral connections are enhanced by STDP process in Layer I
and II. Intra-clique connections represent auto-associative memory, while episodic
memory about the sequence of input patterns are encoded in the form of inter-clique
connections.

7.4 Discussion
149
7.4.3
Temporal Compression and Information Binding
The discovery of place cells suggests that spatial information can be encoded by the
cellular activities of hippocampus in the brain. Moreover, dual oscillations have been
observedinthebraininvolvedinmemoryfunction.IntheHOMmodel,memoryitems
are coded by neuron assemblies ﬁring within different gamma cycles, while past and
present events are chunked by the theta oscillation. When arriving a particular place
that has been visited before, neural clique coding for it will be activated. Later when
coming up with another landmark, another group of neurons will be activated. The
temporal compression of neural ﬁring volleys contributes to the generation of inter-
clique connections and ability to predict upcoming patterns. Due to the repetitive
ﬁring of neurons coding for the previous and current landmarks, the superimposed
impact of these neural cliques will trigger the third clique, predicting the upcoming
landmark in the learned sequence.
Since neural responses in Layer II are linked with inter-clique connections, infor-
mation about different stimuli is binded. As shown in Fig.7.5c, neural activities of
different cliques in Layer II are interconnected. Therefore, temporally compressed
neural patterns can be understood as a new neural clique in Layer II, and fed into
other basic three-layer networks as input. By duplicating this basic network into a
larger network with hierarchical structure, more powerful ability to organize neural
activities representing features with different speciﬁcity along the hierarchy can be
achieved. Each basic networks binds several features into a combined feature and
transmits it to higher level network as its input stimulus. Therefore, the neural cliques
representing the most general features are at the bottom. Moving up along the hier-
archy, neural activities represents more speciﬁc and complex patterns.
7.4.4
Related Works
Jensen and colleagues have demonstrated that simultaneous ﬁrings of a group of
neurons can be stored in a ﬁxed recurrent network modeling hippocampual CA3
area [38]. The idea that dual oscillation interacts with pyramidal neurons has been
implemented in the model. Although ﬁring times of spike are considered in the model,
the external inputs exciting speciﬁc pyramidal neuron are presumed ﬁre in synchrony,
which ignores sensory encoding as well as the hetero-association process. In addition,
recurrent subnetworks are predeﬁned in the model and input patterns are presented
to speciﬁc recurrent networks. These assumptions restrict the generalization and
adaptability of the proposed model.
The Cortext model focuses on hetero-associative memory for word recognition,
while auto-associative memory is implemented in Jensen et al.’s CA3 model. Sim-
ilar to them, most existing memory models consider speciﬁc memory function and
rarely consider the memory organization issue. In contrast, both associative memory
(including hetero- and auto-associative) and episodic memory are involved in HOM

150
7
A Hierarchically Organized Memory Model with Temporal Population Coding
model. The functional roles are clearly assigned and they have been hierarchically
organized. Hereto-associative memory encoded by inter-layer connections to trigger
neurons in upper layers, auto-associative memory is encoded by intra-clique connec-
tions to complete corrupted spatial temporal patterns, and episodic memory is stored
in intra-clique connections to evoke upcoming memories in a sequence during recall.
In contrast to ﬁxed structures, the HOM model can generate neural cliques during
the learning process. The plasticity of network structure not only provides generaliza-
tion and scale-up capability, but fully exploit available coding units of the network.
Therefore, the proposed network architecture is a structure that evolves to a complex
connectivity after training and adapt accordingly to speciﬁc input stimuli.
From the perspective of biological plausibility, all information processing phases
in HOM purely rely on the ﬁring time of spikes. Temporal population coding scheme
together with spike-timing based learning are employed to investigate how time
information might be exploited in memory system. Spatiotemporal patterns are fed to
model sequentially as inputs and neural cliques activities can be observed as internal
representation of memory items. In addition, the HOM model employ spike-timing
dependent learning schemes to adapt both inter-layer connections and intra-layer
connections.
One should note that the proposed HOM model share some similar ideas with
several existing studies. The HOM model simulates neural clique activities reported
in [14]. We agree with the separation of encoding and retrieval theory suggested by
[37, 39]. The mechanism sustains STM was used in Jensen et al.’s model, while
tempotron learning and STDP learning have be employed in contributing long-term
memory formation in other models. However, some important issues need to be
further studied. Since bounded synapses are used in the model, overlapping of neural
cliques and soft limits of weight may contribute to improve the memory capacity of
the model. In addition, more investigation is needed to study how to organize similar
features together with a more complex model by duplicating and connecting several
basic networks together.
7.5
Conclusion
In this chapter, a hierarchically organized memory model is described, demonstrated
and analyzed. The HOM model advances our understanding of the relationship
between spike-timing based learning and formation of memory. Dual oscillation
theory has been applied to achieve short-term memory. Fast NMDA channel, which
activates within the following gamma cycle, contributes to the formation of auto-
associative memory in Layer I. While slow NMDA channel spanning over several
gamma cycles enables the synaptic facilitation among different neural cliques, result-
ing in the development of episodic memory. In addition, individual patterns can be
recognized by observing neural responses of neural cliques in Layer I, while sequence
of them can be detected by observing the collective activities in Layer II.

7.5 Conclusion
151
We believe that this is a good attempt to investigate memory generation and
organization with temporal based coding and learning schemes in spiking neural
network. Different functional memory types are involved in the model and combina-
tion of memories occurs in the top layer as demonstrated. With a more complicated
network, the capability of generating neural cliques representing memories with dif-
ferent speciﬁcity in hierarchy can be further explored. Moreover, real-world stimuli
such as visual and auditory signal can be employed as the sensory information to
investigate the application potential of the HOM model. We hope that our study can
advance our understanding of the basic memory organizing principles and contributes
to the development of artiﬁcial cognitive memory.
References
1. Meister, M., Berry II, M.J.: The neural code of the retina. Neuron 22(3), 435–450 (1999)
2. Heil, P.: Auditory cortical onset responses revisited. i. ﬁrst-spike timing. J. Neurophysiol. 77(5),
2616–2641 (1997)
3. Perez-Orive, J., Mazor, O., Turner, G.C., Cassenaer, S., Wilson, R.I., Laurent, G.: Oscillations
and sparsening of odor representations in the mushroom body. Science 297(5580), 359–365
(2002)
4. Mehta, M.R., Lee, A.K., Wilson, M.A.: Role of experience and oscillations in transforming a
rate code into a temporal code. Nature 417, 741–746 (2002)
5. VanRullen, R., Guyonneau, R., Thorpe, S.J.: Spike times make sense. Trends Neurosci. 28(1),
1–4 (2005)
6. Kayser, C., Montemurro, M.A., Logothetis, N.K., Panzeri, S.: Spike-phase coding boosts and
stabilizes information carried by spatial and temporal spike patterns. Neuron 61(4), 597–608
(2009)
7. Samonds, J.M., Zhou, Z., Bernard, M.R., Bonds, A.B.: Synchronous activity in cat visual cortex
encodes collinear and cocircular contours. J. Neurophysiol. 95(4), 2602–2616 (2006)
8. Georgopoulos, A., Schwartz, A., Kettner, R.: Neuronal population coding of movement direc-
tion. Science 233(4771), 1416–1419 (1986)
9. O’Keefe, J., Dostrovsky, J.: The hippocampus as a spatial map: preliminary evidence from unit
activity in the freely-moving rat. Brain Res. 34, 171–175 (1971)
10. Leutgeb, S., Leutgeb, J., Moser, M.B., Moser, E.: Place cells, spatial maps and the population
code for memory. Curr. Opin. Neurobiol. 15(6), 738–746 (2005)
11. Wyss, R., König, P., Verschure, P.F.M.J.: Invariant representations of visual patterns in a tem-
poral population code. Proc. Natl. Acad. Sci. 100(1), 324–329 (2003)
12. Lin, L., Osan, R., Shoham, S., Jin, W., Zuo, W., Tsien, J.Z.: Identiﬁcation of network-level
coding units for real-time representation of episodic experiences in the hippocampus. Proc.
Natl. Acad. Sci. 102(17), 6125–6130 (2005)
13. Kiani, R., Esteky, H., Mirpour, K., Tanaka, K.: Object category structure in response patterns
of neuronal population in monkey inferior temporal cortex. J. Neurophysiol. 97, 4296–4309
(2007)
14. Lin, L., Osan, R., Tsien, J.Z.: Organizing principles of real-time memory encoding: neural
clique assemblies and universal neural codes. Trends Neurosci. 29(1), 48–57 (2006)
15. Durstewitz, D., Seamans, J.K., Sejnowski, T.J.: Neurocomputational models of working mem-
ory. Nature Neurosci. 3, 1184–1191 (2000)
16. Tang, H., Ramanathan, K., Ning, N.: Guest editorial: special issue on brain inspired models of
cognitive memory. Neurocomputing 138, 1–2 (2014)

152
7
A Hierarchically Organized Memory Model with Temporal Population Coding
17. Gütig, R., Sompolinsky, H.: The tempotron: a neuron that learns spike timing-based decisions.
Nature Neurosci. 9(3), 420–428 (2006)
18. Bohte, S.M., Bohte, E.M., Poutr, H.L., Kok, J.N.: Unsupervised clustering with spiking neurons
by sparse temporal coding and multi-layer RBF networks. IEEE Trans. Neural Netw. 13, 426–
435 (2002)
19. Ponulak, F., Kasinski, A.: Supervised learning in spiking neural networks with resume:
sequence learning, classiﬁcation, and spike shifting. Neural Comput. 22(2), 467–510 (2010)
20. Florian, R.V.: The chronotron: a neuron that learns to ﬁre temporally precise spike patterns.
PLoS One 7(8), e40233 (2012)
21. Hu, J., Tang, H., Tan, K.C., Li, H., Shi, L.: A spike-timing-based integrated model for pattern
recognition. Neural comput. 25(2), 450–472 (2013)
22. Yu, Q., Tang, H., Tan, K.C., Li, H.: Precise-spike-driven synaptic plasticity: learning hetero-
association of spatiotemporal spike patterns. PLoS One 8(11), e78318 (2013)
23. Schrader, S., Gewaltig, M.O., Körner, U., Körner, E.: Cortext: a columnar model of bottom-up
and top-down processing in the neocortex. Neural Netw. 22(8), 1055–1070 (2009)
24. Maass, W., Bishop, C.M.: Pulsed Neural Networks. MIT Press, Cambridge (2001)
25. Jensen, M.S., Azouz, R., Yaari, Y.: Spike after-depolarization and burst generation in adult rat
hippocampal ca1 pyramidal cells. J. Physiol. 492, 199–210 (1996)
26. Nicolelis, M., Baccala, L., Lin, R., Chapin, J.: Sensorimotor encoding by synchronous neural
ensemble activity at multiple levels of the somatosensory system. Science 268(5215), 1353–
1358 (1995)
27. Buzsáki, G.: Theta oscillations in the hippocampus. Neuron 33, 325–340 (2002)
28. Lega, B.C., Jacobs, J., Kahana, M.: Human hippocampal theta oscillations and the formation
of episodic memories. Hippocampus 22, 748–761 (2012)
29. Axmacher,N.,Mormann,F.,Fernández,G.,Elger,C.E.,Fell,J.:Memoryformationbyneuronal
synchronization. Brain Res. Rev. 52(1), 170–182 (2006)
30. Lisman, J.E., Idiart, M.A.: Storage of 7 +/- 2 short-term memories in oscillatory subcycles.
Science 267(5203), 1512–1515 (1995)
31. Kami´nski, J., Brzezicka, A., Wróbel, A.: Short-term memory capacity (7+/-2) predicted by
theta to gamma cycle length ratio. Neurobiol. Learn. Mem. 95(1), 19–23 (2011)
32. Singer, W., Gray, C.M.: Visual feature integration and the temporal correlation hypothesis.
Annu. Rev. Neurosci. 18(1), 555–586 (1995)
33. Bear, M.F., Malenka, R.C.: Synaptic plasticity: LTP and LTD. Curr. Opin. Neurobiol. 4(3),
389–399 (1994)
34. Malenka, R.C., Bear, M.F.: LTP and LTD: an embarrassment of riches. Neuron 44(1), 5–21
(2004)
35. Schreiber, S., Fellous, J., Whitmer, D., Tiesinga, P., Sejnowski, T.: A new correlation-based
measure of spike timing reliability. Neurocomputing 52–54, 925–931 (2003)
36. Fusi, S., Abbott, L.F.: Limits on the memory storage capacity of bounded synapses. Nature
Neurosci. 10(4), 485–493 (2007)
37. Hasselmo, M.E., Bodelón, C., Wyble, B.P.: A proposed function for hippocampal theta rhythm:
separate phases of encoding and retrieval enhance reversal of prior learning. Neural Comput.
14(4), 793–817 (2002)
38. Jensen, O., Idiart, M., Lisman, J.E.: Physiologically realistic formation of autoassociative mem-
ory in networks with theta/gamma oscillations: role of fast nmda channels. Learn. Mem. 3(2–3),
243–256 (1996)
39. Kunec, S., Hasselmo, M.E., Kopell, N.: Encoding and retrieval in the ca3 region of the hip-
pocampus: a model of theta-phase separation. J. Neurophysiol. 94(1), 70–82 (2005)

Chapter 8
Spiking Neuron Based Cognitive
Memory Model
Abstract Jensen et al. (Learn. Mem. 3(2–3), 245–246, 1996 [1]) proposed an auto-
associative memory model using an integrated short-term memory (STM) and long-
term memory (LTM) spiking neural network. Their model requires that distinct pyra-
midal cells encoding different STM patterns are ﬁred in different high-frequency
gamma subcycles within each low-frequency theta oscillation. Auto-associative LTM
is formed by modifying the recurrent synaptic efﬁcacy between pyramidal cells. In
order to store auto-associative LTM correctly, the recurrent synaptic efﬁcacy must
be bounded. The synaptic efﬁcacy must be upper bounded to prevent re-ﬁring of
pyramidal cells in subsequent gamma subcycles. If cells encoding one memory item
were to re-ﬁre synchronously with other cells encoding another item in subsequent
gamma subcycle, LTM stored via modiﬁable recurrent synapses would be corrupted.
The synaptic efﬁcacy must also be lower bounded so that memory pattern comple-
tion can be performed correctly. This chapter uses the original model by Jensen et al.
as the basis to illustrate the following points. Firstly, the importance of coordinated
long-term memory (LTM) synaptic modiﬁcation. Secondly, the use of a generic
mathematical formulation (spiking response model) that can theoretically extend
the results to other spiking network utilizing threshold-ﬁre spiking neuron model.
Thirdly, the interaction of long-term and short-term memory networks that possi-
bly explains the asymmetric distribution of spike density in theta cycle through the
merger of STM patterns with interaction of LTM network.
8.1
Introduction
A key functional role of the hippocampus is the storage and recall of associative
memories [2, 3]. Auto-association refers to the retrieval or completing of a memory
from a partial or noisy sample of itself. Hetero-association refers to the recall of a
memory from one category as a result of a cue from another category. The CA1 region
of the hippocampus has been proposed to be hetero-associator [4]. Depending on the
kinetics of NDMA channels, CA3 region of hippocampus can function as either
hetero-associator [5] or auto-associator [1] for the storage of declarative memories.
© Springer International Publishing AG 2017
Q. Yu et al., Neuromorphic Cognitive Systems, Intelligent Systems
Reference Library 126, DOI 10.1007/978-3-319-55310-8_8
153

154
8
Spiking Neuron Based Cognitive Memory Model
Dual oscillations have been recorded in hippocampus in which a low frequency
theta oscillation is subdivided into about seven subcycles of high frequency gamma
oscillation [6]. The theta rhythm in the hippocampus refers to the regular oscilla-
tions of the local ﬁeld potential at frequencies of 4–12Hz which has been observed
in rodents [7]. In humans, the theta rhythm typically refers to oscillations in the
frequencies of 4–7Hz [8], while gamma rhythm typically refers to oscillations in
the frequencies of 25–100Hz [9]. It is thought that different information can be
stored at different phases of a theta cycle [10]. This type of neural information
representation is commonly known as phase encoding. It is also proposed that the
theta rhythm could work in combination with another brain rhythm known as the
gamma rhythm, of frequencies 40–100Hz [6], to actively maintain auto-associative
memories [1, 11]. Theta rhythm may also have a role to play in the formation of a
cognitive map in the hippocampus [12, 13].
In this chapter, the auto-associative memory model proposed by [1] is presented
using the Spiking Response Model (SRM) neurons [14, 15]. With SRM, the char-
acteristic of a neuron is deﬁned by kernel functions which describe the responses to
presynaptic spike as well as refractory function that characterizes its response after
it has ﬁred. In this way, the membrane potential of a neuron contributed by these
functions at different time instances can separately be identiﬁed and analyzed.
The auto-associative LTM is formed by modifying the recurrent synaptic efﬁcacy
between pyramidal cells. If the weights are not upper bounded, pyramidal cells
that have ﬁred would be re-triggered into subsequent gamma subcycle by the spikes
feedback via the recurrent synaptic connections between these cells. This will conﬂict
with cells encoding another memory item that ﬁre in that gamma subcycle, and
subsequently corrupt the LTM stored in the recurrent collaterals. If the synaptic
weights of the recurrent collaterals are not lower bounded, pattern completion can
not be carried out.
In addition, the analysis on the synaptic bounds of the original model reveals
addition evidence for rejecting the view that STM and LTM are two separate entities.
The interaction between LTM and STM network can cause evenly distribution spike
density in theta cycle to become asymmetric distributed similar to the experimental
result in [16]. Persistent burst of one STM pattern is merged with subsequent memory
pattern to result in uneven distribution of the number of simultaneous spikes to
represent the memory patterns maintained in STM network. The merger of patterns
is accomplished in two steps. Firstly, sufﬁcient inhibition terminates the continual
burst of the ﬁrst group of cells via the activation of another subset of cells. Secondly,
the after-depolarization (ADP) intrinsic to cells causes the two groups of cells to
subsequently ﬁre in synchrony.
This chapter uses the original model by [1] as the basic model to illustrate the
following points.
• The importance of coordinated long-term memory (LTM) synaptic modiﬁcation
[17].

8.1 Introduction
155
• The use of a generic mathematical formulation (spiking response model
[14, 15]) that can theoretically extend the results to other spiking network uti-
lizing threshold-ﬁre spiking neuron model.
• The interaction of long-term and short-term memory networks that possibly
explains the asymmetric distribution of spike density in theta cycle [16] through
the merger of STM patterns with interaction of LTM network.
8.2
SRM-Based CA3 Model
An overview of the SRM-based auto-associative memory model of Jensen et al. [1] is
shown in Fig.8.1 (see [18] for more details). The recurrent network in Fig.8.1 mod-
els the hippocampal CA3 region. The CA3 system operates as an auto-association
network and provides for the completion of stored memories during recall from a
partial cue via entorhinal cortex (EC) [19]. All pyramidal cells in the CA3 also accept
an oscillatory input that is used to model the theta rhythm. Feedback inhibition from
interneurons is applied to all pyramidal cells.
Fig. 8.1 Overview of CA3 model using SRM. The network consists of a short-term memory
(STM) and long-term memory (LTM) network. STM repeats the memory items in every theta
cycle. Interneurons provide feedback inhibition which generates the gamma subcycles within the
positive portion of each theta cycle. LTM network encodes information in modiﬁable recurrent
synapses. Adapted from [18]

156
8
Spiking Neuron Based Cognitive Memory Model
8.2.1
Spike Response Model
Mathematically, the membrane potential of a neuron i under the SRM model is
described by a state variable ui. A spike is modelled as an instantaneous event that
occurs when the membrane potential ui exceeds a threshold Vthres. The time at which
ui crosses Vthres is said to be the ﬁring time t( f )
i
. The set of all ﬁring times of neuron
i is denoted by
Fi =

t( f )
i
; 1 ≤f ≤n

=

t|ui(t) = Vthres ∧u′
i(t) > 0

,
(8.1)
where n is the length of the simulation. After a spike has occurred at t( f )
i
, the state
variable ui will be reset by adding a negative contribution ηi(t −t( f )
i
) to ui. The
kernel ηi(s), known as the refractory function, vanishes for s ≤0 and decays to zero
for s →∞. The refractory kernel deﬁnes a refractory period immediately following
a spike during which the neuron will be incapable of ﬁring another spike. The neuron
may also receive input from presynaptic neurons j ∈Γi where
Γi = { j| j presynaptic to i} .
(8.2)
A presynaptic spike increases or decreases the state variable ui of neuron i by an
amount wi jεi j(t −t( f )
j
). The weight wi j is known as the synaptic weight and it
characterises the strength of the connection from neuron j to neuron i. The kernel
εi j(s) models the response of neuron i to presynaptic spikes from neurons j ∈Γi
and vanishes for s ≤0. In addition to spike input from other neurons, a neuron may
receive external input hext, for example from non-spiking sensory neuron. Under the
SRM model [14], the state ui(t) of neuron i at time t is hence modelled by (8.3).
ui(t) = ηi

t −ˆti

+

j∈Γi
wi jεi j

t −ˆt j

+ hext(t) ,
(8.3)
where ˆti = t(n)
i
< t denotes the most recent ﬁring of neuron i.
8.2.2
SRM-Based Pyramidal Cell
Using SRM, the dynamic of the pyramidal cells in Fig.8.1 is modelled as fol-
lows. A pyramidal cell is ﬁred when its membrane potential uPC
i
exceeds threshold
V PC
thres = 10mV (see [20] for approximate value). In hippocampal pyramidal cells,
action potentials are followed by after-hyperpolarizations (AHPs) [21, 22]. In addi-
tion, pyramidal cells exhibit an after-depolarization (ADP) after spike during cholin-
ergic [21] or serotonergic modulation [23] or as a result of metabotropic glutamate
receptors involved in the conversion of AHP to ADP [22]. Within the framework of

8.2 SRM-Based CA3 Model
157
the SRM neuron model, the refractory kernel ηi(t −t( f )
i
) of a pyramidal cell i is
hence modelled to incorporate AHP and ADP (see (8.4)).
ηPC
i (s) = V PC
refr(s) =
(8.4)
AAHP exp
	
−
s
τAHP

+ AADP
s
τADP
exp
	
1 −
s
τADP

,
where AAHP = −3.96mV, τAHP = 5ms (see [21] for approximate value), AADP =
9.9mV, and τADP = 200ms (see [22] for approximate value). The AHP prevents the
pyramidal cells from fast repetitive ﬁring whereas ADP provides a ramp of excitation
thatbuildsupafteraspikewithinatimeof200msandthenfalls.Figure8.2aillustrates
the refractory kernel for the pyramidal cells.
Theta oscillations have been recorded in hippocampus [6]. Temporal correlations
between active cells in medial septum and the hippocampal system indicate that the
medial septum provides a constant cholinergic modulation that facilitates oscillations
and induces a phasic drive [24]. Successful memory formation is correlated with tight
coordination of spike timing with the local theta oscillation [25]. This external theta
oscillatory signal is modelled by (8.5).
hext(t) = Vtheta(t) = Atheta sin (2π f t + φ) ,
(8.5)
where Atheta = 4.95mV, f = 6Hz, and φ = −π/2.
The spikes from EC or DG input serve to excite the CA3 pyramidal cells. The
spikes from EC and DG layer are phase modulated by the local ﬁeld potential with
theta rhythm (LFP theta). The theta rhythm of EC has about π/4 phase shift from
that of the LFP theta of the CA3 [26–29]. In this chapter, the LFP theta oscillation
to the EC and DG layer is not modelled. For simplicity, the spikes from the input
neurons are introduced into pyramidal cells at the troughs of the LFP theta cycles
Fig. 8.2 Kernel functions. a The refractory function of each pyramidal cell in which AHP proceeds
before ADP after each spike of pyramidal cell. b The response of each pyramidal cell to each spike
from other presynaptic pyramidal cells. c The response of pyramidal cell to each spike from the
inhibitory interneurons

158
8
Spiking Neuron Based Cognitive Memory Model
of CA3. In addition, the response εPC←I
i j
of pyramidal cells i to presynaptic spikes
from EC/DG input neuron j is modelled by (8.6). The response would ensure that
membrane potential uPC
i
of pyramidal cell i reaches V PC
thres at the troughs of the theta
cycles. The synaptic transmission wPC←I
i j
from the input neuron j to pyramidal CA3
cell i is assumed with unit weight.
εPC←I
i j
(t −t( f )
j
) = Vin(t −t( f )
j
) =
(8.6)

14.982 mV
if (t −t( f )
j
) = 0 ,
0 mV
if (t −t( f )
j
) ̸= 0 .
For synaptic transmission between pyramidal cells in the recurrent collaterals, the
response kernel ε
PC←PC
i j
denotes an excitatory postsynaptic potential (EPSP) VEPSP.
The synaptic transmission is mediated by synaptically released glutamate binding to
AMPA (α-amino-3-hydroxy-5-methyl-4-isoazole-propionic acid) and N-methyl-D-
aspartate (NMDA) receptors [30]. AMPA receptors activate and deactivate within a
few milliseconds of presynaptic glutamate release, whereas the open probability of
NMDA receptors typically reaches a peak after 20–30ms, and decays over hundreds
of milliseconds [31, 32]. This synaptic input is modelled by (8.7).
εPC←PC
i j
(t) = VEPSP(t) = VAMPA(t) + VNMDA(t) .
(8.7)
VAMPA(t) is the AMPA potential governed in (8.8),
VAMPA(t) =
(8.8)
AAMPA
aN

t −t( f )
j
−tdelay
τAMPA

exp

1 −
t −t( f )
j
−tdelay
τAMPA

,
where AAMPA = 23.1mV, τAMPA = 1.5ms (see [1] for approximate value), and
VNMDA(t) is the NMDA potential governed in (8.9),
VNMDA(t) = ANMDA
aN
·
(8.9)
exp

−
t −t( f )
j
−tdelay
τNMDA,f
 
1 −exp

−
t −t( f )
j
−tdelay
τNMDA,r

,
where N denotes the number of pyramidal cells or interneurons in the network,
and the delay in the recurrent feedback is tdelay = 0.5ms. The constant a = M/N
denotes the sparseness of pyramidal cells for information coding with M denoting
the number of cells representing a memory pattern. In this chapter, the slow NMDA-
mediated component is ignored (ANMDA = 0mV). Slow NMDA channels have a time
constant that spans several gamma cycles which will allow synaptic connections to be
formedbetweenpyramidal cells that represent different memories. As aconsequence,

8.2 SRM-Based CA3 Model
159
slow NDMA channels enable the storage of heteroassociative sequence information
in long-term memory [5], in which NMDA receptors are responsible for learning
novel paired associations [33]. Figure8.2b illustrates the EPSP kernel function that
describes the recurrent collaterals.
For synaptic transmission from an interneuron j to pyramidal cell i, the response
kernel εPC←IN
i j
denotes an inhibitory postsynaptic potential (IPSP) VIPSP. VIPSP rep-
resents the net GABAergic inhibitory feedback to the pyramidal cells from one
interneuron. Recent evidence from cholinergically induced gamma-frequency net-
work oscillations in vitro, shows that perisomatic-targeting GABAergic interneu-
rones provide prominent rhythmic inhibition in pyramidal cells, and that these
interneurones are synchronized by recurrent excitation [34, 35]. This excitatory-
inhibitory feedback loop is sufﬁcient to generate the intrahippocampal gamma-
frequency oscillations [36]. The recruitment of this recurrent excitatoryinhibitory
feedback loop during hippocampal gamma oscillations suggests that local gamma
oscillations not only control when, but also how many and which pyramidal cells will
ﬁre during each gamma cycle [35]. In addition, interneurons ﬁre at gamma frequency
on positive portion of the theta oscillation [37]. This inhibitory effect is modelled in
(8.10) by assuming the interneurons ﬁre in synchrony by recurrent excitation of the
pyramidal cells.
εPC←IN
i j
(t) = VIPSP(t)
(8.10)
= AGABA
aN

t −t( f )
j
τGABA

exp

1 −
t −t( f )
j
τGABA

,
where AGABA = −5.94mV, t( f )
j
refers to the spike time of interneuron j, τGABA =
4ms (see [38] for approximate value). Figure8.2c illustrates the feedback inhibition
from one interneuron.
8.2.3
SRM-Based Interneuron
Action potentials ﬁred by CA3 pyramidal cells could initiate inhibitory postsynaptic
potentials (IPSPs) in nearby pyramidal cells [38]. One pyramidal cell could activate
several disynaptic inhibitory pathways terminating on another pyramidal cell. This is
suggestive of a diverse excitation of inhibitory cells to ensure recurrent inhibition is
sufﬁciently widespread, rapid and potent to control the spread of activity by recurrent
excitatory connections between CA3 pyramidal cells.
For simplicity, the response kernel εIN←PC
i j
of interneuron i to presynaptic spikes
from pyramidal cell j is modelled by (8.11). The synaptic transmission wIN←PC
i j
from
pyramidal cell j to interneuron i is assumed with unit weight. This set-up simply
serves as a signal from a pyramidal cell to initiate IPSP to other pyramidal cells via
the inhibitory feedback from the interneurons.

160
8
Spiking Neuron Based Cognitive Memory Model
εIN←PC
i j
(t −t( f )
j
) =
(8.11)

4 mV (see [38])
if (t −t( f )
j
) = 0 ,
0 mV
if (t −t( f )
j
) ̸= 0 .
Thus, the ﬁring threshold of the interneurons is assumed V IN
thres = 4mV (see [38] for
approximate value). The interneuron is also assumed with no refractory (η(s) = 0)
and no other external signal (hext(t) = 0) to simplify the kinetics of the interneurons.
8.3
Convergence of Synaptic Weight
Auto-associative LTM is formed by storing the associative information in synapses
between pyramidal cells at the recurrent collaterals [1]. Jensen et al. proposed a
synaptic modiﬁcation methodology at the recurrent collaterals as follows in (8.12).
dwPC←PC
i j
dt
=

i post(t −t( f )
j
) · bglu(t −t( f )
i
−tdelay)
τpp

(1 −wPC←PC
i j
)
(8.12)
+

i post(t −t( f )
j
)
τnpp
+ bglu(t −t( f )
i
−tdelay)
τpnp

(0 −wPC←PC
i j
)
where i post(.) denotes the postsynaptic depolarization that is attributed to back-
propagating action potentials or other dentritic depolarizing events that occur after
the spike initiation in the somatic region and is deﬁned as
i post(s) =
s
τpost
exp
	
1 −
s
τpost

.
bglu(.) models the time course of the glutamate bound to the NMDA receptors and
is deﬁned as
bglu(s) = exp
	
s
τNMDA,f

 	
1 −exp
	
−
s
τNMDA,r


.
Equation (8.12) is a ﬁrst order linear differential equation and can be rewritten as
dwPC←PC
i j
dt
= −(A(t) + B(t)) wPC←PC
i j
+ A(t) ,

8.3 Convergence of Synaptic Weight
161
where
A(t) =
i post(t −t( f )
j
) · bglu(t −t( f )
i
−tdelay)
τpp
,
and
B(t) =
i post(t −t( f )
j
)
τnpp
+ bglu(t −t( f )
i
−tdelay)
τpnp
.
It is easy to see that the synaptic dynamics is stable and it will converge to (8.13).
w∗PC←PC
i j
= exp
	
−
 +∞
0
(A(s) + B(s)) ds

(8.13)
·
	
wPC←PC
i j
(0) +
 +∞
0
A(s) exp
	 s
0
(A(ξ) + B(ξ)) dξ

ds

Equation (8.13) is not mathematically solvable. In order to estimate the limit of the
synaptic weight between any two cells encoding the same memory, the integration
terms in (8.13) are pre-computed at discrete time step of 0.1ms. This is possible
since the kernel functions A(t) and B(t) are known. The synaptic weight modiﬁcation
method proposed by Jensen et al. would only adjust synaptic weight wPC←PC
i j
between
cell i and cell j encoding the same memory item to approach 0.534. Figure8.3a
shows weight wPC←PC
i j
converges to 0.534 from different initial values. Figure8.3b
illustrates the synaptic weight converges to 0.431 when the kinetics of fast NMDA
receptors are modiﬁed. Here, the parameters τNMDA,f and τNMDA,r that characterize
the kinetics of the NMDA receptors are doubled and are respectively 14 and 2ms.
The two examples illustrate synaptic weight modiﬁcation rule is stable. However,
changing the parameters of the learning rule resulted in synaptic weight converging to
another value. Should a different number of cells ﬁred, a static synaptic weight mod-
iﬁcation rule will certainly fail. The following two sections describe the conditions
under which this learning rule will fail.
8.4
Maximum Synaptic Weight to Prevent Early Activation
The network incorporates a STM network where each memory item is repeated
within a gamma subcycle of every theta cycle. The pyramidal cells are reactivated
by the ADP that is intrinsic to each cell. If the cells that encode a memory item were
to ﬁre in another gamma cycle within the same theta cycle, the ﬁring of these cells
will potentially corrupt and interfere with the memory item in that gamma cycle. The

162
8
Spiking Neuron Based Cognitive Memory Model
Fig. 8.3 Convergence of
synaptic strength between
synchronously-ﬁred
pyramidal cells. a LTM
synaptic modiﬁcation using
the original parameters in
[1]. b Original LTM synaptic
modiﬁcation with τNMDA,f =
14ms and τNMDA,r = 2ms
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Theta cycle
wPC←PC
ij
wPC←PC
ij
(0)=0
wPC←PC
ij
(0)=0.5
wPC←PC
ij
(0)=1
(a)
0
50
100
150
200
0
0.2
0.4
0.6
0.8
1
Theta cycle
wPC←PC
ij
wPC←PC
ij
(0)=0
wPC←PC
ij
(0)=0.5
wPC←PC
ij
(0)=1
(b)
ﬁring of the cells that encode distinct memory items would thus be “link up” by the
LTM synaptic modiﬁcation proposed by Jensen et al. [1].
In order to prevent auto-associative LTM corruption, cells must be prevented
from ﬁring in other cycles other than in its designated gamma cycle. The membrane
potential of the pyramidal cells is governed by (8.3). After a pyramidal cell has
ﬁred, its membrane potential will be reset. A pyramidal cell will only ﬁre after its
membrane potential uPC
i (t) exceeds V PC
thres which is governed by (8.14).
uPC
i (t) = ηPC
i

t −ˆti

+

j∈Γi
wi jεi j

t −ˆt j

+ hext(t)
(8.14)
≥V PC
thres .
The pyramidal cells encoding a memory item will ﬁre much earlier when the
synaptic weight wPC←PC
i j
between cell i and cell j of the recurrent collaterals is high
enough to trigger the same set of cells to ﬁre in the next gamma cycle. In order
to prevent cells from reactivating within the same theta cycle and corrupting other

8.4 Maximum Synaptic Weight to Prevent Early Activation
163
memory items, the weight of the synapses in the recurrent collaterals that encode the
LTM for the same memory item must be upper bounded. The maximum synaptic
weight wmax between cell i and cell j of the recurrent collaterals is determined by
solving (8.15).
uPC
i (t) = ηPC
i

t −ˆti

+

j∈Γi
wi jεi j

t −ˆt j

+ hext(t)
(8.15)
< V PC
thres .
A memory item is encoded by a set of M pyramidal cells associated by their
synaptic weights (wPC←PC
i j
between cell i and cell j of the recurrent collaterals).
Ideally, wPC←PC
i j
≤wmax for i ̸= j, |i −j| < M and pM < i, j ≤(p + 1)M
where p ∈ZP (ZP = ﬁnite set of integers modulo P, where P is the number of
memory patterns), and wPC←PC
i j
= 0 otherwise. Given that M pyramidal cells ﬁre
for each memory item, M interneurons will also ﬁre. Thus (8.15) can further be
simpliﬁed to solve for wmax. In order to prevent the set of M pyramidal cells from
early reactivation, the membrane potential of pyramidal cell i must satisfy (8.16).
uPC
i (t) =ηPC
i

t −ˆti

+ (M −1)wmaxVEPSP(t)
(8.16)
+ MVIPSP(t) + hext(t) < V PC
thres .
Thus, the maximum synaptic weight wmax between cell i and j of the recurrent
collaterals is determined by (8.17).
wmax < V PC
thres −hext(t) −ηPC
i

t −ˆti

−MVIPSP(t)
(M −1)VEPSP(t)
.
(8.17)
The maximum weight wmax is a dynamic synaptic weight upper bound deﬁned
by the different kernel functions with time varying characteristics, as well as, the
time at which pyramidal cells ﬁre. After a group of pyramidal cells has ﬁred, a cell
i belonging to the same group will not re-ﬁre as long as its membrane potential uPC
i
does not exceed its ﬁring threshold V PC
thres. If synaptic weight of synapses between cells
encoding the same memory pattern were to exceed the upper bound, the membrane
potential of the cell will exceed the ﬁring threshold and the cell will ﬁre.
8.5
Pattern Completion of Auto-Associative Memory
Pyramidal cells encoding the same memory item are auto-associated by the synapses
within the recurrent collaterals. A memory item can be recalled when a smaller set of
cells are ﬁred [19]. However, a minimum number of cells encoding the same memory
must ﬁre to trigger the other cells and retrieve the memory item.

164
8
Spiking Neuron Based Cognitive Memory Model
ThemembranepotentialuPC
i
ofapyramidalcelli isgovernedby(8.3).Aminimum
number of cells is needed to ﬁre other pyramidal cells of the same auto-associative
memory. It is required that the membrane potential of these other cells exceed V PC
thres.
The ﬁring of the other pyramidal cells is governed by the summation of all the
VEPSP due to ﬁring of the minimum number of pyramidal cells encoding the same
memory item and the inhibitory feedback from interneurons due to the ﬁring of these
pyramidal cells.
Let M( f ) denotes the minimum number of pyramidal cells needed to ﬁre in order
to trigger the other cells encoding the same memory item, and retrieve the stored
memory item. The membrane potential uPC
i
of other cell i within the set of cells
encoding the same memory item must exceed V PC
thres is governed by (8.18).
uPC
i (t) =
M( f )

j
wPC←PC
i j
VEPSP(t) +
M( f )

j
VIPSP(t) + hext(t)
(8.18)
≥V PC
thres; pM < i, j ≤(p + 1)M; p ∈ZP .
Thus, the synaptic weight wPC←PC
i j
between cell i and cell j of the recurrent
collaterals must be lower bounded to ensure that cells that encode the same memory
item are triggered. If pyramidal cells encoding the same memory item are connected
with synapses in the recurrent collaterals with weight wmin (wPC←PC
i j
= wmin; i ̸= j;
|i −j| < M; pM < i, j ≤(p + 1)M; p ∈ZP), the minimum synaptic weight
wmin between cell i and cell j is determined by (8.19).
wmin =
V PC
thres −hext(t) −M( f )
j
VIPSPi j(t)
M( f )
j
VEPSP(t)
.
(8.19)
Similar to the synaptic weight upper bound, the lower bound wmin is also time
varyinginnatureasitdependsonthedifferentkernelfunctions.Thedynamicsynaptic
weight lower bound wmin is also calculated with respects to the last ﬁring time of
the M( f ) presynaptic pyramidal cells. Here, it is assumed that the M( f ) presynaptic
pyramidal cells ﬁre together. The ﬁrings of these pyramidal cells act as a cue to trigger
the other cells which together encode the same memory item. Pattern completion is
successfully when these “missing cells” are triggered.
8.6
Discussion
Figure8.4 illustrates the process of pattern completion for M = 5 when 80% of
every memory pattern are presented to the network. The ﬁgure shows the result
for simulation with 0.1ms time step. The weight of the recurrent synapses between
cells encoding the same memory patterns are set to 0.534 (i.e. wPC←PC
i j
= 0.534 for
pM < i, j ≤(p +1)M; p ∈Z7; i ̸= j). This is the weight recurrent synapses will

8.6 Discussion
165
0
5
10
15
20
25
30
35
Pyramidal Cell No.
0
10
20
30
40
0
10
20
30
40
0
0.5
1
1.5 1.8
0.5
0.6
0.7
0.8
0.9
1
time (second)
wmin & wmax
0.23 0.235 0.24
0.5
0.6
0.7
0.8
0.9
1
time (second)
1.71
1.72
1.73
1.74
1.75
1.76
1.77
0.5
0.6
0.7
0.8
0.9
1
time (second)
wmax
wmin
wPC←PC
ij
Fig. 8.4 Pattern completion when wPC←PC
i j
= 0.534 for pM < i, j ≤(p + 1)M; p ∈Z7
and M = 5. Left to right column: ﬁring time of pyramidal cells on top row and corresponding
minimum and maximum synaptic weight for each group of cells representing the same memory
item at different time scale. Top row ﬁring time of each pyramidal cell. Short line denotes the ﬁring
of the corresponding pyramidal cell. Red lines indicate cell ﬁrings are unsynchronized or pattern
completion is unsuccessful. Blue lines indicate pattern completion is successful. Pattern completion
can only be carried out for memory pattern 1 (cell 1 to cell 5 ﬁre in synchrony). Bottom row lines
with same colour represent the maximum and minimum synaptic weight for synapses between each
subset of cells encoding the same memory. Bold lines represent the maximum weight, and regular
lines represents the minimum recurrent synaptic weight
converge to using the synaptic modiﬁcation method presented in [1]. The top row of
Fig.8.4 shows the ﬁring time of each pyramidal cell with increasing time resolution
from left to right. The top-left plot shows the ﬁring time of pyramidal cells from
time t = 0s to t = 1.8s, the top-middle plot shows the ﬁring time of pyramidal cells
from t = 0.23s to t = 0.24s, and the top-right plot shows the ﬁring time of pyramidal
cells from t = 1.7s to t = 1.77s. The bottom row of Fig.8.4 shows the corresponding
minimum and maximum synaptic weights between cells in each group encoding the
same memory item at different time. Each bold line in these plots represents the
maximum synaptic weight for synapses between cells encoding one memory item.
Each regular line represents the minimum synaptic weight for synapses between cells
encoding a memory item. Red lines in the lower plots represent the weight bounds
for synapses between cells encoding the ﬁrst memory item. Green lines represent
the synaptic bounds for memory item 2, and so forth for memory item 3 to memory
item 7. Synaptic weight is capped between [0, 1]. The bottom-left plot shows the

166
8
Spiking Neuron Based Cognitive Memory Model
dynamic lower and upper weights for synapses between cells encoding the same
memory for t = 0s to t = 1.8s. This plot corresponds to the cell ﬁring time in the
top-left plot. The bottom-middle and bottom-right plots respectively correspond to
the ﬁring time of pyramidal cells in the top-middle and top-right plots.
The ﬁrst partial memory pattern is injected into cell 2 to cell 5 (M( f ) = 4) at time
t = 166.6ms. Lines are drawn at time t = 166.6ms to indicate the ﬁring of cell 2 to
cell 5 as shown in top-left plot. At this time, pattern completion is still unsuccessful
for memory item 1 as cell 1 has not been ﬁred. These lines are drawn in red to indicate
incomplete recall of this memory item. ADP mechanism intrinsic to every pyramidal
cell kicks in and couples with theta oscillation, the membrane potentials of cell 2 to
cell 5 subsequently exceed the ﬁring threshold. This is shown in the top-middle plot
where cell 2 to cell 5 are synchronously activated at time t = 231.0ms. Similarly red
lines are drawn at time t = 231.0ms in the top-middle plot of Fig.8.4 to indicate the
ﬁring of these cells. At this time, completion of memory pattern 1 is still unsuccessful
since cell 1 has not been ﬁred.
Cell 1 is associated with cell 2 to cell 5 as these cells together encode memory item
1. The activation of cell 2 to cell 5 subsequently triggers cell 1 to ﬁre by the excitatory
postsynaptic response to spikes from cell 2 to cell 5 via the recurrent collaterals. A
red line is drawn at time t = 232.8ms to indicate the ﬁring of cell 1. ADP and theta
oscillation together causes cell 2 to cell 5 to ﬁre in the next theta cycle at time t =
377.8ms, and cell 1 to ﬁre at t = 378.4ms. At this time, pattern completion is still
unsuccessful. ADP of every cell is activated and together with theta oscillation, these
cells ﬁre again in the next theta cycle. However, pattern completion of memory item
1 is successful at this cycle. All cells of memory item 1 are synchronously ﬁred at
t = 543.2ms. Blue lines are drawn to indicate the successful completion of memory
item. At t = 543.2ms, cell 1 to cell 5 ﬁre in synchrony.
Pattern completion for memory item 1 is possible because the synaptic weight
of 0.534 exceeds the minimum synaptic weight for synapses between cell 1 and
the other cells (cell 2 to cell 5) encoding memory item 1. The minimum synaptic
weight to allow cell 2 to cell 5 to trigger cell 1 is shown in the bottom-middle
plot. The plot shows a valley-like minimum synaptic weight. At t = 232.8ms, the
minimum synaptic weight decreases below 0.534. At this point, the potential of cell 1
exceeds its ﬁring threshold and cell 1 ﬁres (see top-middle plot). After cell 1 ﬁres, the
maximum synaptic weight for synapses among this group of cells for memory item 1
decreases. Should the weight of these synapses exceeds 0.7542 (the smallest value of
the maximum synaptic weight curve represented by red bold line in bottom-middle
plot), cell 1 to cell 5 would be ﬁred. Since weight of 0.534 is below this mark, these
cells did not ﬁre prematurely.
The second partial memory pattern is injected into cell 7 to cell 10 at time
t = 333.3ms. Similarly, ADP and theta oscillation causes cell 7 to cell 10 to re-
ﬁre at t = 399.7ms. Thereafter, the minimum synaptic weight that is able to trigger
cell 6 decreases due to the increasing excitatory postsynaptic potential at cell 6in

8.6 Discussion
167
response to spikes from cell 7 to cell 10. However, the weight of 0.534 is below
the minimum allowable weight to trigger cell 6. Thus, pattern completion for mem-
ory item 2 is unsuccessful. This phenomenon also happens to pattern completion of
memory item 3 to memory item 7. Generally, a higher weight is required to ensure the
successful recall of subsequent memory item. This is due to stronger inhibition from
interneurons accumulated due to the activation of other pyramidal cells. Figure8.4
shows that pattern completion can only be carried out for memory pattern 1. For the
other patterns, the recurrent synaptic weights are not high enough to trigger the other
missing cell that also encodes the same pattern.
Figure8.5a illustrates the mechanism of STM network when the recurrent synap-
tic weight exceeds the maximum allowable value. Figure8.5b shows a portion of
the simulation result. There are periods of time within the positive cycle of the theta
oscillation when pyramidal cell continues to re-ﬁre. For example, the continual acti-
vation of cell 1 to cell 5 within the ﬁrst theta cycle. This phenomenon is coupled
by the high feedback of EPSP in response to spikes from other pyramidal cells and
low IPSP in response to spikes from the interneurons. The high excitatory response
is due to the high recurrent synaptic weight factoring onto the excitatory responses
to spikes from associated pyramidal cells. The low inhibitory response is due to
the use of the short-term variant of SRM model that relies only on the most recent
spike to compute the inhibitory effect. Since the spiking of cell is maintained by the
slow-ramping ADP of the refractory kernel, this phenomenon will repeat for every
theta cycle. However the continual burst of activation is terminated when there is
sufﬁcient inhibition. For example, the continuous ﬁrings of cells for memory pattern
1 are terminated after new input from cell 7 to cell 10 produces sufﬁcient inhibition.
However, cells encoding memory pattern 1 subsequently ﬁre in synchrony with cells
encoding memory pattern 2, which results in the merger of these memory patterns in
the STM network. Memory pattern 3 and 4 are also subsequently merged in the STM
network. Should LTM synaptic modiﬁcation be carried out, auto-associative LTM
would be adapted and cells encoding different memory patterns would be associated.
This mechanism could help explain the asymmetric distribution of spike density in
theta cycle [16].
In general, the future state of the network is dependent on the state at which the
network is presently in. For example, if a different synaptic weight were used, the
minimum and maximum synaptic weights would differ since both are dependent
on the amount of excitatory response to spikes from presynaptic pyramidal cells
(see (8.17) and (8.19)). The activation of one STM pattern would produce excitatory
responsetoothercellswhichwouldaffecttheactivationofSTMpatterninsubsequent
gamma subcycles.

168
8
Spiking Neuron Based Cognitive Memory Model
0
5
10
15
20
Pyramidal Cell No.
−20
−10
0
10
20
Potential
−10
0
10
V PC
refr
−5
0
5
Vtheta
−20
−10
0

VIPSP
0
10
20

VEPSP
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1.1
0.2
0.4
0.6
0.8
1
time (second)
wmin& wmax
(a)
(b)
Fig. 8.5 STM mechanism when wPC←PC
i j
= 0.9 for pM < i, j ≤(p + 1)M; p ∈Z4 and
M = 5. a Top row ﬁring time of each pyramidal cell. Short line indicates the ﬁring of corresponding
pyramidal cell. Red line indicates cell ﬁrings are unsynchronized within the group of cells encoding
the same memory pattern or pattern completion is unsuccessful. Second row red, green, blue and
cyan lines respectively represent the membrane potential of cell 1, cell 6, cell 11 and cell 16. Third
row red, green, blue and cyan lines respectively represent the refractory response of cell 1, cell 6,
cell 11 and cell 16. Fourth row theta oscillation. Fifth row the total IPSP received by pyramidal cells.
Sixth row red, green, blue and cyan lines respectively represent the EPSP received by cell 1, cell 6,
cell 11 and cell 16. Last row minimum and maximum synaptic weight. Red, green, blue and cyan
coloured bold lines respectively represent the maximum synaptic weight for synapses between cells
encoding memory item 1, 2, 3 and 4. Red, green, blue and cyan coloured regular lines respectively
represent the minimum synaptic weight for synapses between cells encoding memory item 1, 2, 3,
and 4. Here, the minimum and maximum synaptic weights are computed without merger of STM
patterns. b Enlarged plot of Fig.8.5a from time 0.385–0.405s

8.7 Conclusion
169
8.7
Conclusion
The hypothesis of associative memory storage and recall in the different components
of the hippocampus is currently not possible to be evaluated by direct tests. This is
because it is technically not possible to directly generate speciﬁc memory patterns of
neural activity to demonstrate either storage or subsequent recall. The hypothesis that
such a mechanism of associative memory formation and recall does occur is based
on the suitability of the neural network architecture and experiments of Hebbian
induction of long-lasting changes in synaptic strength at relevant synapses on tissue
slice [39, 40].
Here, computational model is used to assess the auto-associative memory storage
and recall abilities of the hippocampal CA3 subsystem. The computational model
presented here is based on Jensen et al.’s recurrent spiking neural network as model
of subregion of hippocampal CA3 [1]; sufﬁcient recurrent connectivity for auto-
associative memory function may be restricted to subregion of CA3a [41]. Speciﬁ-
cally, the computational model is analyzed to present the speciﬁc synaptic conditions
that allow successful storage of different memory patterns and recall of previously
stored patterns. This model has similarities between variety of models that consist of
different levels of biological realism in the investigation of auto-associative memory
function [1, 3, 14, 15, 41–46].
The
present
computational
model
holds
memory
patterns
within
its
auto-associative network in the recurrent collaterals through STDP learning. Suc-
cessful retrieval of correctly stored patterns is achievable if the synaptic conditions
for the recurrent collaterals are satisﬁed. By connecting this auto-associative network
with another similar recurrent network that functions as hetero-associative memory
network [5, 19], it is possible to encode episodic memories and hold these data
sequence for later retrieval within its auto- and hetero-associative networks. This
combined network can effectively perform both of these functions within the same
learning task, a mechanism suggested by [10]. This dual operating modes are possible
with theta-rhythmic oscillation that naturally parses the CA3 function into encoding
and retrieval cycles, which are also demonstrated in the model by [44]. Anatomical
ﬁndings of the recurrent collaterals in CA3 subregions and physiological ﬁndings of
STDP in dendrites also suggest the dual auto- and hetero-associative functionalities
of CA3 [47].
Within the auto-associative memory model, it was also illustrated that feedback
inhibition from the interneurons needs to accurately reﬂect pyramidal cell activity
[48]. This requirement is similar to the working principle of some network models
in [3, 42, 43]. The presented network is similar to the “pseudo-inhibition” models
by [3, 43]. Each pyramidal cell provides an inhibitory connection via an interneuron
onto all other pyramidal cells. However, a low inhibitory response due to the use
of STM-variant of the SRM spiking neuron model in current network is unable to
prevent the bursting of pyramidal cells. Thus, the ﬁring rate of the interneurons should
proportionally reﬂect the neural activities of the pyramidal cell population.

170
8
Spiking Neuron Based Cognitive Memory Model
The results of this chapter are based on the generic SRM neuron model. SRM
neuron model is able to describe other threshold-ﬁre models like integrate-and-ﬁre
(I&F) and approximate the Hodgkin–Huxley conductance-based neuron model [14,
15]. Thus, the results of this chapter are extensible to spiking recurrent network
that utilizes threshold-ﬁre spiking neuron [14, 15]. In addition, other aspects of the
network under different conditions can be analysed using the generic mathematical
formulation (Eq.8.3). This work ﬁnds the synaptic bounds of a recurrent collateral
network for auto-associative memory formation. The same method can be applied
onto spiking network that realizes hetero-associative memory formation.
The analysis on the bounds of the recurrent network reveals addition evidence
for rejecting the view that STM and LTM are two separate entities. Through the
interaction between LTM and STM network, evenly distribution spike density in
theta cycle can become asymmetric, similar to the experimental result in [16]. Per-
sistent burst of one STM pattern is merged with subsequent memory pattern. This
is accomplished in two steps. Firstly, sufﬁcient inhibition terminates the continual
burst of the ﬁrst group of cells via the activation of another subset of cells. Secondly,
the after-depolarization (ADP) intrinsic to cells causes the two groups of cells to
subsequently ﬁre in synchrony.
The original work by [1] has been signiﬁcant to the understanding of the recurrent
spiking neural network dynamics. The present work relies on their model to illustrate
the following points. Firstly, it is important that long-term memory (LTM) synaptic
modiﬁcation is coordinated to maintain network stability [17]. If a different number
of cells other than the designed number of pyramidal cells were to be used to encode
the memory patterns, the static LTM synaptic modiﬁcation technique that updates
synapses without the consideration of this factor will certainly not able to adapt
synaptic weight within the new synaptic weight lower and upper bounds. Secondly,
the analysis is applicable to other spiking network utilizing threshold-ﬁre spiking
neuron model by the use of a generic mathematical formulation (spiking response
model [14, 15]). Thirdly, asymmetric distribution of spike density in theta cycle [16]
can be explained by the the merger of STM patterns through LTM and STM networks
interaction.
References
1. Jensen, O., Idiart, M., Lisman, J.E.: Physiologically realistic formation of autoassociative mem-
ory in networks with theta/gamma oscillations: role of fast nmda channels. Learn. Mem. 3(2–3),
243–256 (1996)
2. Rolls, E.: Computational models of hippocampal functions. Learning and memory: a compre-
hensive reference, pp. 641–665 (2008)
3. Cutsuridis, V., Wennekers, T.: Hippocampus, microcircuits and associative memory. Neural
Netw. 22(8), 1120–1128 (2009)
4. Rolls, E.T.: A computational theory of episodic memory formation in the hippocampus. Behav.
Brain Res. 215(2), 180–196 (2010)

References
171
5. Jensen, O., Lisman, J.E.: Theta/gamma networks with slow nmda channels learn sequences
and encode episodic memory: role of nmda channels in recall. Learn. Mem. 3(2–3), 264–278
(1996)
6. Bragin, A., Jandó, G., Nádasdy, Z., Hetke, J., Wise, K., Buzsáki, G.: Gamma (40–100 hz)
oscillation in the hippocampus of the behaving rat. J. Neurosci. 15(1), 47–60 (1995)
7. Vanderwolf, C.H.: Hippocampal electrical activity and voluntary movement in the rat. Elec-
troencephalogr. Clin. Neurophysiol. 26(4), 407–418 (1969)
8. Cantero, J.L., Atienza, M., Stickgold, R., Kahana, M.J., Madsen, J.R., Kocsis, B.: Sleep-
dependent θ oscillations in the human hippocampus and neocortex. J. Neurosci. 23(34), 10897–
10903 (2003)
9. Hughes, J.R.: Gamma, fast, and ultrafast waves of the brain: their relationships with epilepsy
and behavior. Epilepsy Behav. 13(1), 25–31 (2008)
10. Hasselmo, M.E., Bodelón, C., Wyble, B.P.: A proposed function for hippocampal theta rhythm:
separate phases of encoding and retrieval enhance reversal of prior learning. Neural Comput.
14(4), 793–817 (2002)
11. Lisman, J.E., Idiart, M.A.: Storage of 7 +/−2 short-term memories in oscillatory subcycles.
Science 267(5203), 1512–1515 (1995)
12. Wagatsuma, H., Yamaguchi, Y.: Neural dynamics of the cognitive map in the hippocampus.
Cognit. Neurodyn. 1(2), 119–141 (2007)
13. Yamaguchi, Y., Sato, N., Wagatsuma, H., Wu, Z., Molter, C., Aota, Y.: A uniﬁed view of theta-
phase coding in the entorhinal-hippocampal system. Curr. Opin. Neurobiol. 17(2), 197–204
(2007)
14. Maass, W., Bishop, C.M.: Pulsed Neural Networks. MIT Press, Cambridge (2001)
15. Gerstner, W., Kistler, W.M.: Spiking Neuron Models: Single Neurons, Populations, Plasticity,
1st edn. Cambridge University Press, Cambridge (2002)
16. Koene, R.A., Hasselmo, M.E.: First-in-ﬁrst-out item replacement in a model of short-term
memory based on persistent spiking. Cereb. Cortex 17(8), 1766–1781 (2007)
17. Abbott,L.F.,Nelson,S.B.: Synaptic plasticity: tamingthe beast.Nature Neurosci.3,1178–1183
(2000)
18. Tan, C.H., Cheu, E.Y., Hu, J., Yu, Q., Tang, H.: Associative memory model of hippocampus ca3
using spike response neurons. In: Processing of the Neural Information, pp. 493–500. Springer,
Heidelberg (2011)
19. Jensen, O., Lisman, J.E.: Novel lists of 7+/−2 known items can be reliably stored in an
oscillatory short-term memory network: interaction with long-term memory. Learn. Mem.
3(2–3), 257–263 (1996)
20. Jensen, M.S., Azouz, R., Yaari, Y.: Spike after-depolarization and burst generation in adult rat
hippocampal ca1 pyramidal cells. J. Physiol. 492, 199–210 (1996)
21. Storm, J.F.: An after-hyperpolarization of medium duration in rat hippocampal pyramidal cells.
J. Physiol. 409(1), 171–190 (1989)
22. Park, J.Y., Remy, S., Varela, J., Cooper, D.C., Chung, S., Kang, H.W., Lee, J.H., Spruston,
N.: A post-burst afterdepolarization is mediated by group i metabotropic glutamate receptor-
dependent upregulation of cav2. 3 r-type calcium channels in ca1 pyramidal neurons. PLoS
Biol. 8(11), e1000534 (2010)
23. Araneda, R., Andrade, R.: 5-hydroxytryptamine 2 and 5-hydroxytryptamine 1a receptors medi-
ate opposing responses on membrane excitability in rat association cortex. Neuroscience 40(2),
399–412 (1991)
24. Alonso, A., Gaztelu, J., Bun, W., Garcia-Austt, E., et al.: Cross-correlation analysis of septo-
hippocampal neurons during≡-rhythm. Brain Res. 413(1), 135–146 (1987)
25. Rutishauser, U., Ross, I.B., Mamelak, A.N., Schuman, E.M.: Human memory strength is pre-
dicted by theta-frequency phase-locking of single neurons. Nature 464(7290), 903–907 (2010)
26. Skaggs, W.E., McNaughton, B.L.: Theta phase precession in hippocampal. Hippocampus 6,
149–172 (1996)
27. Yamaguchi, Y., Aota, Y., McNaughton, B.L., Lipa, P.: Bimodality of theta phase precession in
hippocampal place cells in freely running rats. J. Neurophysiol. 87(6), 2629–2642 (2002)

172
8
Spiking Neuron Based Cognitive Memory Model
28. Wagatsuma, H., Yamaguchi, Y.: Cognitive map formation through sequence encoding by theta
phase precession. Neural Comput. 16(12), 2665–2697 (2004)
29. Mizuseki, K., Sirota, A., Pastalkova, E., Buzsáki, G.: Theta oscillations provide temporal
windows for local circuit computation in the entorhinal-hippocampal loop. Neuron 64(2),
267–280 (2009)
30. Ozawa, S., Kamiya, H., Tsuzuki, K.: Glutamate receptors in the mammalian central nervous
system. Prog. Neurobiol. 54(5), 581–618 (1998)
31. Forsythe, I.D., Westbrook, G.L.: Slow excitatory postsynaptic currents mediated by n-methyl-
d-aspartate receptors on cultured mouse central neurones. J. Physiol. 396(1), 515–533 (1988)
32. Stern, P., Edwards, F.A., Sakmann, B.: Fast and slow components of unitary epscs on stellate
cells elicited by focal stimulation in slices of rat visual cortex. J. Physiol. 449(1), 247–278
(1992)
33. Rajji, T., Chapman, D., Eichenbaum, H., Greene, R.: The role of ca3 hippocampal nmda
receptors in paired associate learning. J. Neurosci. 26(3), 908–915 (2006)
34. Mann, E.O., Radcliffe, C.A., Paulsen, O.: Hippocampal gamma-frequency oscillations: from
interneurones to pyramidal cells, and back. J. physiol. 562(1), 55–63 (2005)
35. Hájos, N., Paulsen, O.: Network mechanisms of gamma oscillations in the ca3 region of the
hippocampus. Neural Netw. 22(8), 1113–1119 (2009)
36. Caillard, O., Debanne, D.: Cell-speciﬁc contribution to gamma oscillations. J. Physiol. 588(5),
751–751 (2010)
37. Sik, A., Penttonen, M., Ylinen, A., Buzsáki, G.: Hippocampal ca1 interneurons: an in vivo
intracellular labeling study. J. Neurosci. 15(10), 6651–6665 (1995)
38. Miles, R.: Synaptic excitation of inhibitory cells by single ca3 hippocampal pyramidal cells of
the guinea-pig in vitro. J. Physiol. 428(1), 61–77 (1990)
39. Bliss, T.: Synaptic plasticity in the hippocampus. Trends Neurosci. 2, 42–45 (1979)
40. Neves, G., Cooke, S.F., Bliss, T.V.: Synaptic plasticity, memory and the hippocampus: a neural
network approach to causality. Nature Rev. Neurosci. 9(1), 65–75 (2008)
41. de Almeida, L., Idiart, M., Lisman, J.E.: Memory retrieval time and memory capacity of the
ca3 network: role of gamma frequency oscillations. Learn. Mem. 14(11), 795–806 (2007)
42. Marr, D.: Simple memory: a theory for archicortex. Philos. Trans. R. Soc. B Biol. Sci. 262(841),
23–81 (1971)
43. Sommer, F.T., Wennekers, T.: Associative memory in networks of spiking neurons. Neural
Netw. 14(6), 825–834 (2001)
44. Kunec, S., Hasselmo, M.E., Kopell, N.: Encoding and retrieval in the ca3 region of the hip-
pocampus: a model of theta-phase separation. J. Neurophysiol. 94(1), 70–82 (2005)
45. Bush,D.,Philippides,A.,Husbands,P.,O’Shea,M.:Dualcodingwithstdpinaspikingrecurrent
neural network model of the hippocampus. PLoS Comput. Biol. 6(7), e1000839 (2010)
46. Cutsuridis, V., Cobb, S., Graham, B.P.: Encoding and retrieval in a model of the hippocampal
ca1 microcircuit. Hippocampus 20(3), 423–446 (2010)
47. Samura, T., Hattori, M., Ishizaki, S.: Autoassociative and heteroassociative hippocampal ca3
model based on location dependencies derived from anatomical and physiological ﬁndings. In:
International Congress Series, vol. 1301, pp. 140–143. Elsevier (2007)
48. Hunter, R., Cobb, S., Graham, B.P.: Improving associative memory in a network of spiking
neurons. In: Artiﬁcial neural networks-ICANN 2008, pp. 636–645. Springer (2008)

