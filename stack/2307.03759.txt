JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
1
A Survey on Graph Neural Networks for Time
Series: Forecasting, Classification, Imputation,
and Anomaly Detection
Ming Jin, Huan Yee Koh, Qingsong Wen, Daniele Zambon, Cesare Alippi, Fellow, IEEE,
Geoffrey I. Webb, Fellow, IEEE, Irwin King, Fellow, IEEE, Shirui Pan, Senior Member, IEEE
Abstract—Time series are the primary data type used to record dynamic system measurements and generated in great volume by
both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of
information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in
GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which
traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph
neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly
detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of
GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present and discuss representative
research works and, finally, discuss mainstream applications of GNN4TS. A comprehensive discussion of potential future research
directions completes the survey. This survey, for the first time, brings together a vast array of knowledge on GNN-based time series
research, highlighting both the foundations, practical applications, and opportunities of graph neural networks for time series analysis.
Index Terms—Time series, graph neural networks, deep learning, forecasting, classification, imputation, anomaly detection.
✦
CONTENTS
1
Introduction
1
2
Definition and Notation
3
3
Framework and Categorization
5
3.1
Task-Oriented Taxonomy . . . . . . . . .
5
3.2
Unified Methodological Framework
. .
8
4
GNNs for Time Series Forecasting
8
4.1
Modeling Inter-Variable Dependencies .
9
4.2
Modeling Inter-Temporal Dependencies
10
4.3
Forecasting Architectural Fusion
. . . .
11
5
GNNs for Time Series Anomaly Detection
12
5.1
General Framework for Anomaly De-
tection
. . . . . . . . . . . . . . . . . . .
12
•
Ming Jin, Huan Yee Koh, and Geoff Webb are with the Department of
Data Science and AI, Monash University, Melbourne, Australia. E-mail:
{ming.jin, huan.koh, geoff.webb}@monash.edu;
•
Qingsong Wen is with Alibaba DAMO Seattle, WA, USA. E-mail:
qingsongedu@gmail.com;
•
Daniele Zambon and Cesare Alippi are with Swiss AI Lab ID-
SIA, Universit`a della Svizzera italiana, Lugano, Switzerland. E-mail:
{daniele.zambon, cesare.alippi}@usi.ch;
•
Irwin King is with the Department of Computer Science & Engineering,
The Chinese University of Hong Kong. E-mail: king@cse.cuhk.edu.hk;
•
Shirui Pan is with the School of Information and Communication Tech-
nology and Institute for Integrated and Intelligent Systems (IIIS), Griffith
University, Queensland, Australia. E-mail: s.pan@griffith.edu.au.
•
M. Jin and H. Y. Koh contributed equally to this work.
Corresponding Author: Shirui Pan.
GitHub Page: https://github.com/KimMeen/Awesome-GNN4TS
Version date: July 11, 2023
5.2
Discrepancy Frameworks for Anomaly
Detection . . . . . . . . . . . . . . . . . .
13
6
GNNs for Time Series Classification
15
6.1
Univariate Time Classification . . . . . .
15
6.2
Multivariate Time Series Classification .
16
7
GNNs for Time Series Imputation
17
7.1
In-Sample Imputation
. . . . . . . . . .
17
7.2
Out-of-Sample Imputation . . . . . . . .
17
8
Practical Applications
17
9
Future Directions
19
10
Conclusions
20
References
21
1
INTRODUCTION
T
He advent of advanced sensing and data stream pro-
cessing technologies has led to an explosion of time
series data, one of the most ubiquitous data types that
captures and records activity across a wide range of do-
mains [1], [2], [3]. The analysis of time series data not
only provides insights into past trends but also facilitates a
multitude of tasks such as forecasting [4], classification [5],
anomaly detection [6], and data imputation [7]. This lays
the groundwork for time series modeling paradigms that
leverage on historical data to understand current and future
possibilities. Time series analytics have become increasingly
crucial in various fields, including but not limited to cloud
arXiv:2307.03759v1  [cs.LG]  7 Jul 2023

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
2
Forecasting
%-&:%
%+1:%+ℎ
Imputation
%-2
%-1
%
Missing data point
Wind Speed
Wind
Power
Plant 1
Wind
Power
Plant 2
Anomaly Detection
%-2
%-1
%
❌
❌Anomaly Time Point
Classification
“Downward”
“Upward”
❌
Fig. 1: Graph neural networks for time series analysis
(GNN4TS). In this example of wind farm, different ana-
lytical tasks can be categorized into time series forecasting,
classification, anomaly detection, and imputation.
computing, transportation, energy, finance, social networks,
and the Internet-of-Things [8], [9], [10], [11].
Many time series involve complex interactions across
time (such as lags in propagation of effects) and variables
(such as the relationship among the variables representing
neighboring traffic sensors). By treating time points or vari-
ables as nodes and their relationships as edges, a model
structured in the manner of a network or graph can effec-
tively learn the intricacies of these relationships. Indeed,
much time series data is spatial-temporal in nature, with
different variables in the series capturing information about
different locations – space – too, meaning it encapsulates
not only time information but also spatial relationships [12].
This is particularly evident in scenarios such as urban
traffic networks, population migration, and global weather
forecasting. In these instances, a localized change, such as a
traffic accident at an intersection, an epidemic outbreak in a
suburb, or extreme weather in a specific area, can propagate
and influence neighboring regions. This might manifest as
increased traffic volume on adjacent roads, the spread of
disease to neighboring suburbs, or altered weather condi-
tions in nearby areas. This spatial-temporal characteristic
is a common feature of many dynamic systems, including
another example of the wind farm in Fig. 1, where the
underlying time series data displays a range of correlations
and heterogeneities [13]. These factors contribute to the
formation of complex and intricate patterns, posing signif-
icant challenges for effective modeling. Traditional analytic
tools, such as support vector regression (SVR) [14], [15],
gradient boosting decision tree (GBDT) [16], [17], vector au-
toregressive (VAR) [18], [19], and autoregressive integrated
moving average (ARIMA) [20], [21], struggle to handle
complex time series relations (e.g., nonlinearities and inter-
series relationships), resulting in less accurate prediction
results [22]. The advent of deep learning technologies in
the past decade has led to the development of different
neural networks based on convolutional neural networks
(CNN) [23], [24], recurrent neural networks (RNN) [25], [26],
and Transformers [27], which have shown significant ad-
vantages in modeling real-world time series data. However,
one of the biggest limitations of the above methods is that
they do not explicitly model the spatial relations existing
between time series in non-Euclidean space [13], [28], which
limits their expressiveness [28].
In recent years, graph neural networks (GNNs) have
emerged as a powerful tool for learning non-Euclidean
data representations [29], [30], [31], paving the way for
modeling real-world time series data. This enables the
capture of diverse and intricate relationships, both inter-
variable (connections between different variables within
a multivariate series) and inter-temporal (dependencies
between different points in time). Considering the complex
spatial-temporal
dependencies
inherent
in
real-world
scenarios, a line of studies has integrated GNNs with
various
temporal
modeling
frameworks
to
capture
both spatial and temporal dynamics and demonstrate
promising results [13], [32], [33], [34], [35]. This modeling
approach has also been widely adopted in many real-
world application sectors with different time series data,
including transportation [36], on-demand services [37],
[38], energy [39], healthcare [40], [41], economy [42],
and other fields [43], [44], [45]. While early research
efforts were primarily concentrated on various forecasting
scenarios [13], [33], [34], recent advancements in time series
analysis utilizing GNNs have demonstrated promising
outcomes
in
other
mainstream
tasks.
These
include
classification [46], [47], anomaly detection [48], [49], and
imputation [50], [51]. In Fig. 1, we provide an overview
of graph neural networks for time series analysis (GNN4TS).
Related Surveys. Despite the growing body of research
related to performing various time series analytic tasks
with GNNs, existing surveys are relatively limited in
number and tend to focus on specific perspectives within
a restricted scope. For instance, the survey by Wang et
al. [12] offers a review of deep learning techniques for
spatial-temporal data mining, but it does not specifically
concentrate on GNN-based methods and fails to reflect
the most recent advancements in the field. The survey
by Ye et al. [35] zeroes in on graph-based deep learning
architectures in the traffic domain, primarily considering
different forecasting scenarios. A recent survey by Jin et al.
[13] offers an overview of GNNs for predictive learning in
urban computing, but neither extends its coverage to other
application domains nor thoroughly discusses other tasks
related to time series analysis. Finally, we mention the work
by Rahmani et al. [36], which expands the survey of GNNs
to many intelligent transportation systems, but tasks other
than forecasting remain overlooked. A detailed comparison
between our survey and others is presented in Tab. 1.
To fill the gap, this survey offers a comprehensive and
up-to-date review of graph neural networks for time series
analysis, encompassing mainstream tasks ranging from time
series forecasting, classification, anomaly detection, and im-
putation. Specifically, we first provide two broad views
to classify and discuss existing works from the task- and

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
3
TABLE 1: Comparison between our survey and other related surveys.
Survey
Domain
Scope
Specific
General
Forecasting
Classification
Anomaly
Detection
Imputation
Wang et al. [12]
Ye et al. [35]
Jiang and Luo [33]
Bui et al. [34]
Jin et al. [13]
Al Sahili and Awad [32]
Rahmani et al. [36]
Our Survey
* Specifically,
represents “Not Covered”,
signifies “Partially Covered”, and
corresponds to “Fully Covered”.
methodology-oriented perspectives. Then, we delve into
six popular application sectors within the existing research
of GNN4TS, and propose several potential future research
directions. Our survey is intended for general machine
learning practitioners interested in exploring and keeping
abreast of the latest advancements in graph neural networks
for time series analysis. It is also suitable for domain experts
seeking to apply GNN4TS to new applications or explore
novel possibilities building on recent advancements. The
key contributions of our survey are summarized as follows:
•
First Comprehensive Survey. To the best of our
knowledge, this is the first comprehensive survey
that reviews the recent advances in mainstream time
series analysis tasks with graph neural networks. It
covers a wide range of recent research and provides a
broad view of the development of GNN4TS without
restricting to specific tasks or domains.
•
Unified and Structured Taxonomy. We present a
unified framework to structurally categorize existing
works from task- and methodology-oriented per-
spectives. In the first classification, we provide an
overview of tasks in time series analysis, includ-
ing various forecasting, classification, anomaly de-
tection, and imputation settings that are targeted in
most GNN-based related work. We further present
a structured taxonomy in the second classification
to dissect the graph neural networks for time series
analysis from the perspective of spatial and temporal
dependencies modeling, as well as the overall model
architecture.
•
Detailed and Current Overview. We conduct a com-
prehensive review that not only covers the breadth of
the field but also delves into the depth of individual
studies with fine-grained classification and detailed
discussion, providing readers with an up-to-date un-
derstanding of the state-of-the-art in GNN4TS.
•
Broadening Applications. We discuss the expanding
applications of GNN4TS across various sectors, high-
lighting its versatility and potential for future growth
in diverse fields.
•
Insight into Future Research Directions. We provide
an overview of potential future research directions,
offering insights and suggestions that could guide
and inspire future work in the field of GNN4TS.
The remainder of this survey is organized as follows:
Sec. 2 provides important notations and related definitions
used throughout the paper. Sec. 3 presents the taxonomy of
GNN4TS from different perspectives, along with a general
pipeline. Sec. 4, Sec. 5, Sec. 6, and Sec. 7 review four
mainstream analytic tasks in the GNN4TS literature. Sec. 8
surveys popular applications of GNN4TS across various
fields, while Sec. 9 examines open questions and potential
future directions. Finally, Sec. 10 concludes this survey.
2
DEFINITION AND NOTATION
In this section, we provide the definitions and notations
used throughout the paper. In the subsequent text, we em-
ploy bold uppercase letters (e.g., X), bold lowercase letters
(e.g., x), and calligraphic letters (e.g., V) to denote matrices,
vectors, and sets, respectively.
In this survey, we start by defining time series data,
which serves as a fundamental basis for abstracting var-
ious real-world systems, such as photovoltaic and traffic
networks. Time series data comprises a sequence of obser-
vations gathered or recorded over a period of time. This
data can be either regularly or irregularly sampled, with the
latter also referred to as time series data with missing values.
Within each of these cases, the data can be further classified
into two primary types: univariate and multivariate time series.
Definition 1 (Univariate Time Series). A univariate time
series is a sequence of scalar observations collected over
time, which can be regularly or irregularly sampled. A
regularly-sampled univariate time series is defined as
X = {x1, x2, ..., xT } ∈RT , where xt ∈R for t =
1, 2, . . . , T. For an irregularly-sampled univariate time
series, the observations are collected at non-uniform time
intervals, such as X = {(t1, x1), (t2, x2), ..., (tT , xT )} ∈
RT , where time points are non-uniformly spaced.
Definition 2 (Multivariate Time Series). A multivariate time
series is a sequence of vector observations collected over
time, i.e., X ∈RN×T , where each vector comprises
N variables. Similarly, it can either be regularly or
irregularly sampled. A regularly-sampled multivariate
time series has vector observations collected at uniform
time intervals, i.e., xt ∈RN. In an irregularly-sampled
multivariate time series, there may be N unaligned time
series with respect to time steps, which means that there
are 0 ≤n ≤N observations available at each time step.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
4
The majority of research based on GNNs focuses on
modeling multivariate time series, as they can be natu-
rally abstracted into spatial-temporal graphs. This abstraction
allows for an accurate characterization of dynamic inter-
temporal and inter-variable dependencies. The former de-
scribes the relations between different time steps within
each time series (e.g., the temporal dynamics of red nodes
between t1 and t3 in Fig. 2), while the latter captures
dependencies between time series (e.g., the spatial relations
between four nodes at each time step in Fig. 2), such as the
geographical information of the sensors generating the data
for each variable. We first define static attributed graphs as
follows.
Definition 3 (Attributed Graph). An attributed graph is
a graph that associates each node with a set of at-
tributes, representing node features. Formally, an at-
tributed graph is defined as G = (A, X), which consists
of a (weighted) adjacency matrix A ∈RN×N and a node-
feature matrix X ∈RN×D. The adjacency matrix repre-
sents the graph topology, which can be characterized by
node and edge sets V and E, where V = {v1, v2, . . . , vN}
is the set of N nodes, and E = {eij := (vi, vj) ∈V × V |
Aij ̸= 0} is the set of edges, with Aij being the (i, j)-th
entry in the adjacency matrix A. The feature matrix X
contains the node attributes, where the i-th row xi ∈RD
represents the D-dimensional feature vector of node vi.
In attributed graphs, multi-dimensional edge features can
be considered too, however, this paper assumes only scalar
weights encoded in the adjacency matrix to avoid over-
whelming notations.
In light of this, a spatial-temporal graph can be described
as a series of attributed graphs, which effectively represent
(multivariate) time series data in conjunction with either
evolving or fixed structural information over time.
Definition 4 (Spatial-Temporal Graph). A spatial-temporal
graph
can
also
be
interpreted
as
discrete-time
dy-
namic graphs [52]. Formally, we define it as G
=
{G1, G2, · · · , GT }, where Gt = (At, Xt) for each time
step t. At ∈RN×N is the adjacency matrix representing
the graph topology at time t, and Xt ∈RN×D is the
feature matrix containing the node attributes at time
t. The adjacency matrix may either evolve over time
or remain fixed, depending on specific settings. When
abstracting time series data, we let Xt := xt ∈RN.
We introduce graph neural networks as modern deep
learning models to process graph-structured data. The core
operation in typical GNNs, often referred to as graph convo-
lution, involves exchanging information across neighboring
nodes. In the context of time series analysis, this operation
enables us to explicitly rely on the time series dependencies
represented by the graph edges. Aware of the different
nuances, we define GNNs in the spatial domain, which
involves transforming the input signal with learnable func-
tions along the dimension of N.
Definition 5 (Graph Neural Network). We adopt the def-
inition presented in [30]. Given an attributed graph
G = (A, X), we define xi = X[i, :] ∈RD as the D-
dimensional feature vector of node vi. A GNN learns
node representations through two primary functions:
𝑡!
𝑡"
𝑡#
Time
A spatial-temporal graph with fixed graph structure
𝑡!
𝑡"
𝑡#
Time
A spatial-temporal graph with time-evolving graph structure
Fig. 2: Examples of spatial-temporal graphs.
AGGREGATE(·) and COMBINE(·). The AGGREGATE(·)
function computes and aggregates messages from neigh-
boring nodes, while the COMBINE(·) function merges the
aggregated and previous self-information to transform
node embeddings. Formally, the k-th layer in a GNN is
defined by convolution
a(k)
i
= AGGREGATE(k) n
h(k−1)
j
: vj ∈N(vi)
o
,
h(k)
i
= COMBINE(k) 
h(k−1)
i
, a(k)
i

,
(1)
or, more generally, aggregating messages computed from
both sending and receiving nodes vj and vi, respectively.
Here, a(k)
i
and h(k)
i
represent the aggregated message
from neighbors and the transformed node embedding
of node vi in the k-th layer, respectively. The input and
output of a GNN are h(0)
i
:= xi and h(K)
i
:= hi.
The above formulation in Eq. 1 is referred to as spatial
GNNs, as opposed to spectral GNNs which defines convo-
lution from the lens of spectral graph theory. We refer the
reader to recent publication [28] for a deeper analysis of
spectral versus spatial GNNs, and [29], [53] for a compre-
hensive review of GNNs.
To employ GNNs for time series analysis, it is implied
that a graph structure must be provided. However, not all
time series data have readily available graph structures and,
in practice, two types of strategies are utilized to generate
the missing graph structures from the data: heuristics or
learned from data.
Heuristic-based graph. This group of methods extracts
graph structures from data based on various heuristics, such
as:
•
Spatial Proximity: This approach defines the graph
structure by considering the proximity between pairs
of nodes based on, e.g., their geographical location. A
typical example is the construction of the adjacency
matrix A based on the shortest travel distance be-
tween nodes when the time series data have geospa-
tial properties:
Ai,j =
( 1
dij ,
if dij ̸= 0,
0,
otherwise,
(2)
where dij denotes the shortest travel distance be-
tween node i and node j. Some common kernel

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
5
functions, e.g., Gaussian radial basis, can also be
applied [13].
•
Pairwise Connectivity: In this approach, the graph
structure is determined by the connectivity between
pairs of nodes, like that determined by transportation
networks where the adjacency matrix A is defined as:
Ai,j =
(
1,
if vi and vj are directly linked,
0,
otherwise.
(3)
Typical scenarios include edges representing roads,
railways, or adjacent regions [54], [55]. In such cases,
the graph can be undirected or directed, resulting in
symmetric and asymmetric adjacency matrices.
•
Pairwise Similarity: This method constructs the graph
by connecting nodes with similar attributes. A simple
example is the construction of adjacency matrix A
based on the cosine similarity between time series:
Ai,j =
x⊤
i xj
∥xi∥∥xj∥,
(4)
where ∥·∥denotes the Euclidean norm. There are also
several variants for creating similarity-based graphs,
such as Pearson Correlation Coefficient (PCC) [56]
and Dynamic Time Warping (DTW) [57].
•
Functional Dependence: This approach defines the
graph structure based on the functional dependence
between pairs of nodes. These include the construc-
tion of adjacency matrix A based on Granger causal-
ity [58]:
Ai,j =





1,
if node j Granger-causes
node i at a significance level α,
0,
otherwise.
(5)
Other examples involve transfer entropy (TE) [59]
and directed phase lag index (DPLI) [60].
Learning-based graph. In contrast to heuristic-based meth-
ods, learning-based approaches aim to directly learn the
graph structure from the data end-to-end with the down-
stream task. These techniques typically involve optimizing
the graph structure alongside model parameters during
the training process, e.g., embedding-based [61], attention-
based [62], [63], sampling-based [64], [65]. These learning-
based approaches enable the discovery of more complex and
potentially more informative graph structures compared to
heuristic-based methods.
3
FRAMEWORK AND CATEGORIZATION
In this section, we present a comprehensive task-oriented
taxonomy for GNNs within the context of time series analy-
sis (Sec. 3.1). Subsequently, we elucidate the foundational
principles for encoding time series data across various
tasks by introducing a unified methodological framework of
GNN architectures (Sec. 3.2). According to the framework,
all architectures are composed of a similar graph-based
processing module fθ and a second module, pϕ, specialized
for the downstream task. Here, we also provide a general
pipeline for analyzing time series data using GNNs. The
combination of these perspectives offers a comprehensive
overview of GNN4TS.
3.1
Task-Oriented Taxonomy
In Fig. 3, we illustrate a task-oriented taxonomy of GNNs
encompassing the primary tasks and mainstream modeling
perspectives for time series analysis, and showcasing
the potential of GNN4TS. To summarize, our survey
emphasizes four categories: Time series forecasting, anomaly
detection, imputation, and classification. These tasks are
performed on top of the time series representations learned
by spatial-temporal graph neural networks (STGNNs), which
serve as the foundation for encoding time series data in
existing literature across various tasks. We detail this in
Sec. 3.2.
Time Series Forecasting. This task is centered around pre-
dicting future values of the time series based on historical
observations, as depicted in Fig. 4a. Depending on applica-
tion needs, we categorize this task into two types: single-step
forecasting and multi-step forecasting. The former is meant to
predict single future observations of the time series once at
a time, i.e., the target at time t is Y := Xt+H for H ∈N
steps ahead, while the latter makes predictions for a time
interval, e.g., Y := Xt+1:t+H. Solutions to both predictive
cases can be cast in the optimization form:
θ∗, ϕ∗= arg min
θ,ϕ
LF

pϕ
 fθ(Xt−T :t, At−T :t)
, Y

,
(6)
where fθ(·) and pϕ(·) represent a spatial-temporal GNN
and the predictor, respectively. Details regarding the fθ(·)
architecture are given in Sec. 3.2 while the predictor
is, normally, a multi-layer perceptron. In the sequel,
we denote by Xt−T :t
and At−T :t
a spatial-temporal
graph G = {Gt−T , Gt−T +1, · · · , Gt} with length T. If the
underlying graph structure is fixed, then At := At−1. LF(·)
denotes the forecasting loss, which is typically a squared
or absolute loss function in most works, e.g., STGCN [66]
and MTGNN [61]. Most existing works minimize the
error between the forecasting and the ground truth Y
through the Eq. 6; this process is known as deterministic
time series forecasting. Besides, we have probabilistic time
series forecasting methods, such as DiffSTG [67], that share
the same objective Eq. 6 function though it is not directly
optimized. Based on the size of the forecasting horizon H,
we can conduct either short-term or long-term forecasting.
Time Series Anomaly Detection. This task focuses on
detecting irregularities and unexpected event in time series
data (see Fig. 4b). Detecting anomalies requires determining
when the anomalous event occurred, while diagnosing them
requests gaining insights about how and why the anomaly
occurred. Due to the general difficulty of acquiring anomaly
events, current research commonly treats anomaly detection
as an unsupervised problem that involves the design of a
model describing normal, non-anomalous data. The learned
model is then used to detect anomalies by generating a high
score whenever an anomaly event occurs. The optimization
process can be formulated as
θ∗, ϕ∗= arg min
θ,ϕ
LR

pϕ
 fθ(Xt−T :t, At−T :t)
, Y

,
(7)
where fθ(·) and pϕ(·) denote the spatial-temporal GNN and
predictor, respectively. Similar to the forecasting task, the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
6
Graph Neural
Networks for
Time Series
Analysis
A
n
o
m
a
l
y
 
D
e
t
e
c
t
i
o
n
Anomaly
Diagnosis
Reconstruction
Discrepancy
Forecast
Discrepancy
Relational
Discrepancy
Anomaly
Detection
Reconstruction
Discrepancy
Forecast
Discrepancy
Relational
Discrepancy
C
l
a
s
s
i
f
i
c
a
t
i
o
n
Multivariate
Classiﬁcation
Series2Node
Others
Univariate
Classiﬁcation
Series2Node
Series2Graph
F
o
r
e
c
a
s
t
i
n
g
Multi-Step
Forecasting
Short-Term
Forecasting
Long-Term
Forecasting
Single-Step
Forecasting
Short-Term
Forecasting
I
m
p
u
t
a
t
i
o
n
Out-of-Sample
Imputation
Deterministic
In-Sample
Imputation
Probabilistic
Deterministic
Fig. 3: Task-oriented taxonomy of graph neural networks for time series analysis in the existing literature.
detector can simply be a multi-layer perceptron. Commonly
[48], [68], we optimize Eq. 7 on non-anomalous training
data to minimize the residual error between the input series
and the predicted (reconstructed) one, where Y := Xt;
some methods intrinsically deal with training data possibly
contaminated by anomalous observations [69]. Existing
works [49], [70] also formulate this task as a one-step-
ahead forecasting task by dropping the last observation
as Xt−T :t−1, At−T :t−1. In both cases, the reconstruction or
forecast discrepancy should be large when provided with
anomalous instances. The threshold discriminating between
normal and anomalous data is a critical issue and should
account for the fact that anomalies are typically rare events.
It follows that the threshold is calibrated to align with a
desired false alarm rate [71]. As it is important to diagnose
the cause of anomaly events a common strategy envisages
to design the statistics so that it computes discrepancies
for each individual channel node before aggregating scores
into a single anomaly score value [72]. In this way, the
channel variables responsible for the anomaly events can
be identified by computing their respective contributions to
the final score.
Time Series Imputation. This task is centered around es-
timating and filling in missing or incomplete data points
within a time series dataset (Fig. 4c). Current research in this
domain can be broadly classified into two main approaches:
In-sample imputation and out-of-sample imputation. In-sample
imputation involves filling missing values in a given time
series, while out-of-sample imputation pertains to inferring
missing data not present in the training dataset. We formu-
late the learning objective as follows:
θ∗, ϕ∗= arg min
θ,ϕ
LI

pϕ
 fθ( ˜Xt−T :t, At−T :t)
, Xt−T :t

, (8)
where fθ(·) and pϕ(·) denote the spatial-temporal GNN
and imputation module to be learned, respectively. The
imputation module can e.g., be a multi-layer perceptron.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
7
Input Series
Target Series
Spatial-Temporal GNN
!!
Predictions
Forecaster
"∅
Loss
Forecasting
Representations
(a) Graph neural networks for time series forecasting.
Reconstruction
Reconstructor
"∅
Discrepancies
Reconstruction
Relational
Discrepancies
Representations
Spatial-
Temporal GNN
!!
Input Series
Target Series
(b) Graph neural networks for time series anomaly detec-
tion.
Input Series
Ground Truth
Loss
Imputation
Predictions
Spatial-Temporal GNN
!!
Representations
Imputer
"∅
(c) Graph neural networks for time series imputation.
Series2Graph
GNN !!
Univariate Subsequences
Pooling
Classifier
"∅
#$
$
Loss
Classification
Multiple
Univariate Series
Series2Node
GNN !!
Classifier
"∅
#$
$
Loss
Classification
(d) Graph neural networks for time series classification:
Convert green series classification into a graph (top) or
node (bottom) classification task.
Fig. 4: Four categories of graph neural networks for time series analysis. For the sake of simplicity and illustrative purposes,
we assume the graph structures are fixed in all subplots.
In this task,
˜Xt−T :t represents input time series data
with missing values (reference time series), while Xt−T :t
denotes the same time series without missing values. As
it is impossible to access the reference time series during
training, a surrogate optimization objective is considered,
such
as
generating
synthetic
missing
values
[50].
In
Eq. 8, LI(·) refers to the imputation loss, which can be,
for instance, an absolute or a squared error, similar to
forecasting tasks. For in-sample imputation, the model
is trained and evaluated on ˜Xt−T :t and Xt−T :t. Instead,
for out-of-sample imputation, the model is trained and
evaluated on disjoint sequences, e.g., trained on ˜Xt−T :t but
evaluated on Xt:t+H, where the missing values in ˜Xt:t+H
will be estimated. Similar to time series forecasting and
anomaly detection, the imputation process can be either
deterministic or probabilistic. The former predicts the missing
values directly (e.g., GRIN [50]), while the latter estimates
the missing values from data distributions (e.g., PriSTI [51]).
Time Series Classification. This task aims to assign a cate-
gorical label to a given time series based on its underlying
patterns or characteristics. Rather than capturing patterns
within a time series data sample, the essence of time series
classification resides in discerning differentiating patterns
that help separate samples based on their class labels. The
objective of classifying a time series can be expressed as:
θ∗, ϕ∗= arg min
θ,ϕ
LC

pϕ
 fθ(X, A)
, Y

,
(9)
where fθ(·) and pϕ(·) denote a vanilla GNN and classifier
to be learned, respectively. Using univariate time series
classification as an example Fig. 4d, the task can be
formulated as either a graph or node classification task.
In the case of graph classification (Series2Graph) [73],
each series is transformed into a graph, and the graph
will input a GNN to generate a classification output.
This can be achieved by dividing a complete series into
multiple subsequences with a window size, W, serving

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
8
‎Graph Neural 
‎Networks for Time 
‎Series Analysis
‎Overall 
‎Architecture
‎Discrete
‎Factorized
‎Coupled
‎Continuous
‎Factorized
‎Coupled
‎Temporal 
‎Module
‎Time Domain
‎Recurrence-Based
‎Convolution-Based
‎Attention-Based
‎Hybrid
‎Frequency 
‎Domain
‎Convlution-Based
‎Attention-Based
‎Hybrid
‎Spatial 
‎Module
‎Spatial GNNs
‎Message Passing-Based
‎Graph Diffusion-Based
‎Spectral GNNs
‎Polynomial Approximation-Based
‎Hybrid
Fig. 5: Methodology-oriented taxonomy of graph neural
network for time series analysis.
Predictions
Spatial-Temporal Graph Neural Networks
Spatial Module
Spatial GNNs
Spectral GNNs
Hybrid
Model Architecture
Discrete
Continuous
Temporal Module
Time Domain
Freq. Domain
Recurrence
-Based
…
Convolution-
Based
…
Data Processing Module
Graph Structure
Learning
Data Denoising
Data
Normalization
Downstream Task Prediction Module
Forecas-
ting
Classific-
ation
Anomaly 
Detection
Imputa-
tion
Fig. 6: General pipeline for time series analysis using graph
neural networks.
as graph nodes, X ∈RN×W , and an adjacency matrix,
A, describing the relationships between subsequences.
A simple GNN, fθ(·), then employs graph convolution
and pooling to obtain a condensed graph feature to be
exploited by a classifier pϕ(·) which assigns a class label to
the graph. Alternatively, the node classification formulation
(Series2Node), treats each series as a node in a dataset graph.
Unlike Series2Graph, it constructs an adjacency matrix
representing the relationships between multiple distinct
series in a given dataset [74]. With several series of length
T stacked into a matrix X ∈RN×T as node features and
A representing pairwise relationships, the GNN operation,
fθ(·), aims at leveraging the relationships across different
series for accurate node series classification [75]. In all cases,
Y is typically a one-hot encoded vector representing the
categorical label of a univariate or multivariate time series.
3.2
Unified Methodological Framework
In Fig. 5, we present a unified methodological framework
of STGNNs mentioned in Sec. 3.1 for time series analysis.
Specifically, our framework serves as the basis for encoding
time series data in the existing literature across various
downstream tasks (Fig. 3). As an extension, STGNNs incor-
porate spatial information by considering the relationships
between nodes in the graph and temporal information by
taking into account the evolution of node attributes over
time. Similar to [13], we systematically categorize STGNNs
from three perspectives: Spatial module, temporal module, and
overall model architecture.
•
Spatial Module. To model dependencies between
time series over time, STGNNs employ the design
principles of GNNs on static graphs. These can be
further categorized into three types: Spectral GNNs,
spatial GNNs, and a combination of both (i.e., hybrid)
[29]. Spectral GNNs are based on spectral graph the-
ory and use the graph shift operator (like the graph
Laplacian) to capture node relationships in the graph
frequency domain [28], [76], [77]. Differently, spatial
GNNs simplify spectral GNNs by directly designing
filters that are localized to each node’s neighborhood.
Hybrid approaches combine both spectral and spa-
tial methodologies to capitalize on the strengths of
each method.
•
Temporal Module. To account for temporal depen-
dencies in time series, STGNNs incorporate tem-
poral modules that work in tandem with spatial
modules to model intricate spatial-temporal patterns.
Temporal dependencies can be represented in either
the time or frequency domains. In the first category
of methods, approaches encompass recurrence-based
(e.g., RNNs [25]), convolution-based (e.g., TCNs [78]),
attention-based (e.g., Transformers [27]), and a combi-
nation of these (i.e., hybrid). For the second category,
analogous techniques are employed, followed by
orthogonal space projections [28], such as the Fourier
transform.
•
Model Architecture. To integrate the two modules,
existing STGNNs are either discrete or continuous
in terms of their overall neural architectures. Both
types can be further subdivided into two subcate-
gories: Factorized and coupled. With typical factorized
STGNN model architectures, the temporal process-
ing is performed either before or after the spatial pro-
cessing, whether in a discrete (e.g., STGCN [66]) or
continuous manner (e.g., STGODE [79]). Conversely,
the coupled model architecture refers to instances
where spatial and temporal modules are interleaved,
such as DCRNN [80] (discrete) and MTGODE [22]
(continuous). Other authors refer to very related cat-
egories as time-then-space and time-and-space [81].
General Pipeline. In Fig. 6, we showcase a general pipeline
that demonstrates how STGNNs can be integrated into
time series analysis. Given a time series dataset, we first
process it using the data processing module, which performs
essential data cleaning and normalization tasks, including
the extraction of time series topology (i.e., graph structures).
Subsequently, STGNNs are utilized to obtain time series rep-
resentations, which can then be passed to different handlers
(i.e., downstream task prediction module) to execute various
analytical tasks, such as forecasting and anomaly detection.
4
GNNS FOR TIME SERIES FORECASTING
Time series forecasting aims to predict future time series
values based on historical observations. The origin of time

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
9
TABLE 2: Summary of representative graph neural networks for time series forecasting. Task notation: The first letter, “M”
or “S”, indicates multi-step or single-step forecasting, and the second letter, “S” or “L”, denotes short-term or long-term
forecasting. Architecture notation: “D” and “C” represent “Discrete” and “Continuous”; “C” and “F” stand for “Coupled”
and “Factorized”. Temporal module notation: “T” and “F” signify “Time Domain” and “Frequency Domain”; “R”, “C”, “A”,
and “H” correspond to “Recurrence”, “Convolution”, “Attention”, and “Hybrid”. Input graph notation: “R” indicates that
a pre-calculated graph structure (with a certain graph heuristic) is a required input of the model, “NR” that such graph is
not required (not a model’s input), while “O” signifies that the model can optionally exploit given input graphs. Notation of
learned graph relations: “S” and “D” indicate “Static” and “Dynamic”. Notation of adopted graph heuristics: “SP”, “PC”, “PS”,
and “FD” denote “Spatial Proximity”, “Pairwise Connectivity”, “Pairwise Similarity”, and “Functional Dependency”,
respectively. The “Missing Values” column indicates whether corresponding methods can handle missing values in input
time series.
Approach
Year
Venue
Task
Architecture
Spatial
Module
Temporal
Module
Missing
Values
Input
Graph
Learned
Relations
Graph
Heuristics
DCRNN [80]
2018
ICLR
M-S
D-C
Spatial GNN
T-R
No
R
-
SP
STGCN [66]
2018
IJCAI
M-S
D-F
Spectral GNN
T-C
No
R
-
SP
ST-MetaNet [82]
2019
KDD
M-S
D-F
Spatial GNN
T-R
No
R
-
SP, PC
NGAR [83]
2019
IJCNN
S-S
D-F
Spatial GNN
T-R
No
R
-
-
ASTGCN [84]
2019
AAAI
M-S
D-F
Spectral GNN
T-H
No
R
-
SP, PC
ST-MGCN [54]
2019
AAAI
S-S
D-F
Spectral GNN
T-R
No
R
-
SP, PC, PS
Graph WaveNet [85]
2019
IJCAI
M-S
D-F
Spatial GNN
T-C
No
O
S
SP
MRA-BGCN [86]
2020
AAAI
M-S
D-C
Spatial GNN
T-R
No
R
-
SP
MTGNN [61]
2020
KDD
S-S, M-S
D-F
Spatial GNN
T-C
No
NR
S
-
STGNN* [87]
2020
WWW
M-S
D-C
Spatial GNN
T-H
No
R
-
SP
GMAN [88]
2020
AAAI
M-S
D-C
Spatial GNN
T-A
No
R
-
SP
SLCNN [89]
2020
AAAI
M-S
D-F
Hybrid
T-C
No
NR
S
-
STSGCN [90]
2020
AAAI
M-S
D-C
Spatial GNN
T
No
R
-
PC
StemGNN [62]
2020
NeurIPS
M-S
D-F
Spectral GNN
F-C
No
NR
S
-
AGCRN [91]
2020
NeurIPS
M-S
D-C
Spatial GNN
T-R
No
NR
S
-
LSGCN [92]
2020
IJCAI
M-S
D-F
Spectral GNN
T-C
No
R
-
SP
STAR [93]
2020
ECCV
M-S
D-F
Spatial GNN
T-A
No
R
-
PC
GTS [64]
2021
ICLR
M-S
D-C
Spatial GNN
T-R
No
NR
S
-
GEN [94]
2021
ICLR
S-S
D-F
Spatial GNN
T-R
No
R
-
-
Z-GCNETs [95]
2021
ICML
M-S
D-C
Spatial GNN
T-C
No
NR
S
-
STGODE [79]
2021
KDD
M-S
C-F
Spatial GNN
T-C
No
R
-
SP, PS
STFGNN [57]
2021
AAAI
M-S
D-F
Spatial GNN
T-C
No
R
-
SP, PS
DSTAGNN [96]
2022
ICML
M-S
D-F
Spectral GNN
T-H
No
R
-
PC, PS
TPGNN [97]
2022
NeurIPS
S-S, M-S
D-F
Spatial GNN
T-A
No
NR
D
-
MTGODE [22]
2022
IEEE TKDE
S-S, M-S
C-C
Spatial GNN
T-C
No
NR
S
-
STG-NCDE [98]
2022
AAAI
M-S
C-C
Spatial GNN
T-C
Yes
NR
S
-
STEP [99]
2022
KDD
M-S
D-F
Spatial GNN
T-A
No
NR
S
-
Chauhan et al. [100]
2022
KDD
M-S
-
-
-
Yes
O
S
SP
RGSL [101]
2022
IJCAI
M-S
D-C
Spectral GNN
T-R
No
R
S
SP, PC
FOGS [102]
2022
IJCAI
M-S
-
-
-
No
NR
S
-
METRO [103]
2022
VLDB
M-S
D-C
Spatial GNN
T
No
NR
D
-
SGP [104]
2023
AAAI
M-S
D-F
Spatial GNN
T-R
No
R
-
SP
Jin et al. [28]
2023
arXiv
M-S, M-L
D-F
Spectral GNN
F-H
No
NR
S
-
series forecasting can be traced back to statistical auto-
regressive models [105], which forecast future values in
a time series based on a linear combination of its past
values. In recent years, deep learning-based approaches
have demonstrated considerable success in forecasting time
series by capturing nonlinear temporal and spatial patterns
more effectively [22]. Techniques such as recurrent neural
networks (RNNs), convolutional neural networks (CNNs),
and attention-based neural networks have been employed.
However, many of these approaches, such as LSTNet [106]
and TPA-LSTM [107], overlook and implicitly model the
rich underlying dynamic spatial correlations between time
series. Recently, graph neural network (GNN)-based meth-
ods have shown great potential in explicitly and effectively
modeling spatial and temporal dependencies in multivari-
ate time series data, leading to enhanced forecasting perfor-
mance.
GNN-based forecasting models can be categorized and
examined from multiple perspectives. In terms of forecast-
ing tasks, while many models focus on multi-step forecasting
(i.e., predicting multiple consecutive steps ahead based on
historical observations), a minority also discuss single-step
forecasting (i.e., predicting the next or one arbitrary step
ahead). From a methodological standpoint, these models
can be dissected from three aspects: (1) Modeling spa-
tial (i.e., inter-variable) dependencies, (2) modeling inter-
temporal dependencies, and (3) the architectural fusion of
spatial and temporal modules for time series forecasting. A
summary of representative works is in Tab. 2.
4.1
Modeling Inter-Variable Dependencies
Spatial dependencies, or inter-time series relationships, play
a pivotal role in affecting a model’s forecasting capabil-
ity [28]. When presented with time series data and cor-
responding graph structures that delineate the strength of
interconnections between time series, current studies typ-
ically employ (1) spectral GNNs, (2) spatial GNNs, or (3) a
hybrid of both to model these spatial dependencies. At a high
level, these methods all draw upon the principles of graph
signal processing (as detailed in Def. 5 and subsequent
discussion). Considering input variables Xt and At at a

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
10
given time t, the goal here is to devise an effective GNN-
based model, termed SPATIAL(·), to adeptly capture salient
patterns between data points from different time series at
time t. This can be expressed as ˆXt = SPATIAL(Xt, At),
where ˆXt collects all time series representations at time t
with spatial dependencies embedded.
Spectral GNN-Based Approaches. Early GNN-based
forecasting models predominantly utilized ChebConv [108]
to approximate graph convolution with Chebyshev polyno-
mials, thereby modeling inter-time series dependencies. For
instance, STGCN [66] intersects temporal convolution [109]
and ChebConv layers to capture both spatial and temporal
patterns. StemGNN [62] further proposes spectral-temporal
graph neural networks to extract rich time series patterns
by leveraging ChebConv and frequency-domain convolu-
tion neural networks. Other relevant research has largely
followed suit, employing ChebConv to model spatial time
series dependencies, while introducing innovative modifica-
tions. These include attention mechanisms [84], [92], multi-
graph construction [54], [101], and combinations of the
two [96]. Recently, building upon StemGNN, Jin et al. [28]
have theoretically demonstrated the benefits of using spec-
tral GNNs to model different signed time series relations,
such as strongly positive and negative correlated variables
within a multivariate time series. They also observed that
any orthonormal family of polynomials could achieve com-
parable expressive power for such tasks, albeit with varying
convergence rates and empirical performances.
Spatial GNN-Based Approaches. Inspired by the recent
success of spatial GNNs [29], another line of research has
been modeling inter-time series dependencies using mes-
sage passing [110] or graph diffusion [111], [112]. From the
graph perspective, these methods are certain simplifications
compared to those based on spectral GNNs, where strong
local homophilies are emphasised [28], [113]. Early methods
such as DCRNN [80] and Graph WaveNet [85] incorporated
graph diffusion layers into GRU [114] or temporal convolu-
tion to model time series data. Subsequent works including
GTS [64] and ST-GDN [115] also applied graph diffusion.
In contrast, STGCN(1st) (a second version of STGCN [66])
and ST-MetaNet [82] modelled spatial dependencies with
GCN [116] and GAT [117] to aggregate information from ad-
jacent time series. Related works, such as MRA-BGCN [86],
STGNN* [87], GMAN [88], and AGCRN [91], proposed
variants to model inter-time series relations based on mes-
sage passing. To enhance learning capabilities, STSGCN [90]
proposed spatial-temporal synchronous graph convolution,
extending GCN to model spatial and temporal dependen-
cies on localized spatial-temporal graphs. STFGNN [57]
constructed spatial-temporal fusion graphs based on dy-
namic time wrapping (DTW) before applying graph and
temporal convolutions. Z-GCNETs [95] enhanced existing
methods with salient time-conditioned topological informa-
tion, specifically zigzag persistence images. METRO [103]
introduced multi-scale temporal graphs to characterize dy-
namic spatial and temporal interactions in time series data,
together with the single-scale graph update and cross-scale
graph fusion modules to unify the modeling of spatial-
temporal dependencies. Another line of improvements in-
corporates graph propagation, allowing for the mixing of
neighborhood information from different hops to learn
high-order relations and substructures in the graph. Ex-
ample are MTGNN [61] and SGP [104]. In particular, SGP
exploits reservoir computing and multi-hop spatial process-
ing to precompute spatio-temporal representations yielding
effective, yet scalable predictive models. Follow-up works
such as MTGODE [22] and TPGNN [97] proposed con-
tinuous graph propagation and graph propagation based
on temporal polynomial coefficients. Other similar works
include STGODE [79] and STG-NCDE [98]. Distinct from
GAT-based methods, approaches based on graph trans-
former [118] can capture long-range spatial dependencies
due to their global receptive field, making them a separate
branch of enhanced methods. Examples include STAR [93],
ASTTN [119], and ASTTGN [120]. At last, we mention
NGAR [83] and GEN [94] as works that can predict the
graph topology alongside node-level signals.
Hybrid Approaches. Some hybrid methodologies also
exist, integrating both spectral and spatial GNNs. For in-
stance, SLCNN [89] employs ChebConv [108] and localized
message passing as global and local convolutions to cap-
ture spatial relations at multiple granularities. Conversely,
Auto-STGNN [121] integrates neural architecture search to
identify high-performance GNN-based forecasting models.
In this approach, various GNN instantiations, such as Cheb-
Conv, GCN [116], and STSGCN [90], can be simultaneously
implemented in different spatial-temporal blocks.
4.2
Modeling Inter-Temporal Dependencies
The modeling of temporal dependencies within time series
represents another important element in various GNN-
based forecasting methods. These dependencies (i.e., tem-
poral patterns) are capable of being modeled in the time
or/and frequency domains. A summary of representative
methods, along with their temporal module classifications,
is presented in Tab. 2. Given a univariate time series Xn
with length T, the primary goal here is to learn an effective
temporal model, referred to as TEMPORAL(·). This model
is expected to accurately capture the dependencies between
data points within Xn, such that ˆXn = TEMPORAL(Xn),
where ˆXn symbolizes the representation of time series Xn.
In the construction of TEMPORAL(·), both the time and
frequency domains can be exploited within convolutional
and attentive mechanisms. Recurrent models can also be
employed for modeling in the time domain specifically. Ad-
ditionally, hybrid models exist in both domains, integrating
different methodologies such as attention and convolution
neural networks.
Recurrent Models. Several early methodologies rely on
recurrent models for understanding inter-temporal depen-
dencies in the time domain. For instance, DCRNN [80]
integrates graph diffusion with gated recurrent units
(GRU) [114] to model the spatial-temporal dependencies in
traffic forecasting. ST-MetaNet [82] incorporates two types
of GRU to encode historical observations and capture di-
verse temporal correlations that are tied to geographical in-
formation. Inspired by [80], MRA-BGCN [86] combines the
proposed multi-range attention-based bicomponent graph
convolution with GRU. This model is designed to better cap-
ture spatial-temporal relations by modeling both node and

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
11
edge interaction patterns. On a different note, AGCRN [91]
merges GRU with a factorized variant of GCN [116] and
a graph structure learning module. Some studies, such as
GTS [64] and RGSL [101], share similar designs but pri-
marily emphasize different graph structure learning mecha-
nisms. Recently, echo state networks (ESN) [122] – a type of
RNN with sparse and randomized connectivity producing
rich dynamics – have been employed to design scalable
models without compromising the performance [104], [123].
Convolution Models. Convolutional Neural Networks
(CNNs), on the other hand, provide a more efficient per-
spective for modeling inter-temporal dependencies, with the
bulk of existing studies in the time domain. An instance
of this is STGCN [66], which introduces temporal gated
convolution that integrates 1-D convolution with gated
linear units (GLU) to facilitate tractable model training.
Works that adopt a similar approach include DGCNN [124],
SLCNN [89], and LSGCN [92]. Building on these foun-
dations, Graph WaveNet [85] incorporated dilated causal
convolution, which notably expands the receptive field with
only a minimal increase in model layers. STGODE [79] and
STFGNN [57] have produced similar designs in capturing
temporal dependencies. MTGNN [61] also uses these un-
derpinning concepts, but it enhances temporal convolution
by utilizing multiple kernel sizes. Further expanding on
this, MTGODE [22] adopts a neural ordinary differential
equation [125] to generalize this modeling process. There
are some other studies, such as Z-GCNETs [95], that directly
apply canonical convolution to capture temporal patterns
within the time domain, albeit with other focuses. An
alternative strand of methodologies, including StemGNN
[62] and TGC [28], focuses on modeling temporal clues
in the frequency domain. StemGNN applies gated convo-
lution to filter the frequency elements generated by the
discrete Fourier transform of the input time series. In con-
trast, TGC convolves frequency components individually
across various dimensions to craft more expressive temporal
frequency-domain models.
Attention Models. Recently, a growing number of
methodologies are turning towards attention mechanisms,
such as the self-attention used in the Transformer model
[126],
to
embed
temporal
correlations.
For
instance,
GMAN [88] attentively aggregates historical information
by considering both spatial and temporal features. ST-
GRAT [127] mirrors the Transformer’s architecture, employ-
ing multi-head self-attention layers within its encoder to em-
bed historical observations in conjunction with its proposed
spatial attention mechanism. STAR [93], TPGNN [97], and
STEP [99] similarly employ Transformer layers to model the
temporal dependencies within each univariate time series.
There are also variations on this approach, like the multi-
scale self-attention network proposed by ST-GDN [115],
aiming to model inter-temporal dependencies with higher
precision.
Hybrid Models. Hybrid models also find application in
modeling inter-temporal dependencies. For example, AST-
GCN [84], HGCN [128], and DSTAGNN [96] concurrently
employ temporal attention and convolution in learning
temporal correlations. STGNN* [87] amalgamates both GRU
and Transformer to capture local and global temporal de-
pendencies. Auto-STGCN [121], on the other hand, poten-
tially facilitates more diverse combinations when searching
for high-performance neural architectures. In the frequency
domain, the nonlinear variant of TGC [28] is currently the
only hybrid model proposing to capture temporal relations
through the combination of spectral attention and convolu-
tion models.
4.3
Forecasting Architectural Fusion
Given the spatial and temporal modules discussed, denoted
as SPATIAL(·) and TEMPORAL(·), four categories of neural
architectural fusion have been identified as effective means
to capture spatial-temporal dependencies within time series
data: (1) Discrete factorized, (2) discrete coupled, (3) continuous
factorized, and (4) continuous coupled. In discrete factorized
models, spatial and temporal dependencies are usually
learned and processed independently. This approach may
involve stacking and interleaving spatial and temporal mod-
ules within a model building block [61], [66], [85]. Discrete
coupled models, on the other hand, explicitly or implicitly
incorporate spatial and temporal modules into a singu-
lar process when modeling spatial-temporal dependencies,
such as in [129], [86], and [90]. Different from discrete
models, some methods abstract the underlying modeling
processes with neural differential equations, which we cat-
egorize as continuous models. Specifically, continuous fac-
torized models involve distinct processes, either partially or
entirely continuous (e.g., [79]), to model spatial and tempo-
ral dependencies. In contrast, continuous coupled models
employ a single continuous process to accomplish this task,
such as [22] and [98].
Discrete Architectures. Numerous existing GNN-based
time series forecasting methods are discrete models. For
instance, factorized approaches like STGCN [66] employ
a sandwich structure of graph and temporal gated con-
volution layers as its fundamental building block, facil-
itating the modeling of inter-variable and inter-temporal
relations. Subsequent works, such as DGCNN [124], LS-
GCN [92], STHGCN [130], and HGCN [128], retain this
model architecture while introducing enhancements such
as dynamic graph structure estimation [124], hypergraph
convolution [128], and hierarchical graph generation [128].
A multitude of other studies adhere to similar principles,
stacking diverse spatial and temporal modules in their
core building blocks. For example, ST-MetaNet [82] inter-
weaves RNN cells and GAT [117] to model evolving traffic
information. Comparable works include ST-MGCN [54],
DSATNET [131], and EGL [132]. In contrast, ASTGCN [84],
DSTAGNN [96], and GraphSleepNet [133] are constructed
upon spatial-temporal attention and convolution mod-
ules, with the latter module comprising of stacking Cheb-
Conv [108] and the convolution in the temporal dimen-
sion. Graph WaveNet [85], SLCNN [89], StemGNN [62],
MTGNN [61], STFGNN [57], and TGC [28] share a similar
model architecture without the attention mechanism. There
are also alternative designs within the realm of discrete
factorized forecasting models. For instance, STAR [93] in-
tegrates the proposed spatial and temporal Transformers,
while ST-GDN [115] initially performs attention-based tem-
poral hierarchical modeling before applying various graph
domain transformations. TPGNN [97] employs temporal

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
12
attention and the proposed temporal polynomial graph
module to more effectively capture time-evolving patterns
in time series data. MTHetGNN [59] stacks the proposed
temporal, relational, and heterogeneous graph embedding
modules to jointly capture spatial-temporal patterns in time
series data. CausalGNN [41] models multivariate time series
with causal modeling and attention-based dynamic GNN
modules. Auto-STGCN [121] explores high-performance
discrete combinations of different spatial and temporal
modules.
In the realm of discrete coupled models, early works
such as DCRNN [80] and Cirstea et al. [134] straight-
forwardly incorporate graph diffusion or attention mod-
els
into
RNN
cells.
This
approach
models
spatial-
temporal dependencies in historical observations for fore-
casting. Subsequent works, including ST-UNet [135], MRA-
BGCN [86], STGNN* [87], AGCRN [91], RGSL [101], and
MegaCRN [136], are based on similar concepts but with
varying formulations of graph convolutional recurrent
units. Some studies integrate spatial and temporal convo-
lution or attention operations into a single module. For
instance, GMAN [88] proposes a spatial-temporal attention
block that integrates the spatial and temporal attention
mechanisms in a gated manner. Z-GCNETs [95] initially
learns time-aware topological features that persist over time
(i.e., zigzag persistence representations), then applies spatial
and temporal graph convolutions to capture salient patterns
in time series data. TAMP-S2GCNETS [137] is slightly more
complex, modeling the spatial-temporal dependencies in
time series by coupling two types of GCN layers, Dynamic
Euler-Poincare Surface Representation Learning (DEPSRL)
modules, and CNNs. Another line of research direction
models spatial-temporal dependencies by convolving on
specially crafted graph structures (e.g., STSGCN [90]), per-
forming graph convolutions in a sliding window manner
(e.g., STSGCN [90] and STG2Seq [138]), or utilizing the
concept of temporal message passing (e.g., METRO [103]
and ASTTN [119]).
Continuous Architectures. To date, only a handful of ex-
isting methods fall into the category of continuous models.
For factorized methods, STGODE [79] proposes to depict the
graph propagation as a continuous process with a neural or-
dinary differential equation (NODE) [125]. This approach al-
lows for the effective characterization of long-range spatial-
temporal dependencies in conjunction with dilated con-
volutions along the time axis. For coupled methods, MT-
GODE [22] generalizes both spatial and temporal modeling
processes found in most related works into a single unified
process that integrates two NODEs. STG-NCDE [98] shares
a similar idea but operates under the framework of neural
controlled differential equations (NCDEs) [139]. Similarly,
a recent work, TGNN4I [140], integrates GRU [114] and
MPNN [110] as the ODE function to model continuous-time
latent dynamics.
5
GNNS FOR TIME SERIES ANOMALY DETECTION
Time series anomaly detection aims to identify data obser-
vations that do not conform with the nominal regime of the
data-generating process [141]. We define anomaly as any
such data point, and use the term normal data otherwise;
we note however that different terminologies, like novelty
and outlier, are used almost interchangeably to anomaly in
the literature [142]. These deviations from the nominal con-
ditions could take the form of a single observation (point)
or a series of observations (subsequence) [143]. However,
unlike normal time series data, anomalies are difficult to
characterize for two main reasons. First, they are typically
associated with rare events, so collecting and labeling them
is often a daunting task. Secondly, establishing the full
range of potential anomalous events is generally impossible,
spoiling the effectiveness of supervised learning techniques.
Consequently, unsupervised detection techniques have been
widely explored as a practical solution to challenging real-
world problems.
Traditionally,
methods
[144]
such
as
distance-
based [145], [146], [147], and distributional techniques [148]
have been widely used for detecting irregularities in
time
series
data.
The
former
family
uses
distance
measures to quantify the discrepancy of observations
from representative data points, while the latter looks
at points of low likelihood to identify anomalies. As the
data-generating process becomes more complex and the
dimensionality of the multivariate time series grows these
methods become less effective [149].
With the advancement of deep learning, early works
proposed recurrent models with reconstruction [150] and
forecasting [151] strategies respectively to improve anomaly
detection in multivariate time series data. The forecasting
and reconstruction strategies rely on forecast and recon-
struction errors as discrepancy measures between antici-
pated and real signals. These strategies rely on the fact
that, if a model trained on normal data fails to forecast or
reconstruct some data, then it is more likely that such data
is associated with an anomaly. However, recurrent models
[152] are found to lack explicit modeling of pairwise inter-
dependence among variable pairs, limiting their effective-
ness in detecting complex anomalies [48], [153]. Recently,
GNNs have shown promising potential to address this gap
by effectively capturing temporal and spatial dependencies
among variable pairs [49], [70], [154].
5.1
General Framework for Anomaly Detection
Treating anomaly detection as an unsupervised task relies
on models to learn a general concept of what normality
is for a given dataset [164], [165]. To achieve this, deep
learning architectures deploy a bifurcated modular frame-
work, constituted by a backbone module and a scoring
module [149]. Firstly, a backbone model, BACKBONE(·), is
trained to fit given training data, assumed to be nomi-
nal, or to contain very few anomalies. Secondly, a scoring
module, SCORER(X, ˆX), produces a score used to identify
the presence of anomalies by comparing the output ˆX =
BACKBONE(X) of the backbone module with the observed
time series data X. The score is intended as a measure of the
discrepancy between the expected signals under normal and
anomalous circumstances. When there is a high discrepancy
score, it is more likely that an anomaly event has occurred.
Furthermore, it is also important for a model to diagnose
anomaly events by pinpointing the responsible variables.
Consequently, a scoring function typically computes the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
13
TABLE 3: Summary of representative graph neural networks for time series anomaly detection. The strategy notation:
“CL”, “FC”, “RC”, and “RL” indicate “Class”, “Forecast”, “Reconstruction”, and “Relational Discrepancies”, respectively.
The remaining notations are shared with Table 2.
Approach
Year
Venue
Strategy
Spatial
Module
Temporal
Module
Missing
Values
Input
Graph
Learned
Relations
Graph
Heuristics
CCM-CDT [60]
2019
IEEE TNNLS
RC
Spatial GNN
T-R
No
R
-
PC, FD
MTAD-GAT [48]
2020
IEEE ICDM
FC+RC
Spatial GNN
T-A
No
NR
-
-
GDN [49]
2021
AAAI
FC
Spatial GNN
-
No
NR
S
-
GTA [154]
2021
IEEE IoT
FC
Spatial GNN
T-H
No
NR
S
-
EvoNet [155]
2021
WSDM
CL
Spatial GNN
T-R
No
R
-
PS
Event2Graph [156]
2021
arXiv
RL
Spatial GNN
T-A
No
R
-
PS
GANF [68]
2022
ICLR
RC+RL
Spatial GNN
T-R
No
NR
S
-
Grelen [157]
2022
IJCAI
RC+RL
Spatial GNN
T-H
No
NR
D
-
VGCRN [158]
2022
ICML
FC+RC
Spatial GNN
T-R
No
NR
S
-
FuSAGNet [70]
2022
KDD
FC+RC
Spatial GNN
T-R
No
NR
S
-
GTAD [159]
2022
Entropy
FC+RC
Spatial GNN
T-C
No
NR
-
-
HgAD [160]
2022
IEEE BigData
FC
Spatial GNN
-
No
NR
S
HAD-MDGAT [161]
2022
IEEE Access
FC+RC
Spatial GNN
T-A
No
NR
-
-
STGAN [161]
2022
IEEE TNNLS
RC
Spatial GNN
T-R
No
R
-
SP
GIF [69]
2022
IEEE IJCNN
RC
Spatial GNN
-
No
R
-
SP, PC, FD
DyGraphAD [162]
2023
arXiv
FC+RL
Spatial GNN
T-C
No
R
-
PS
GraphSAD [163]
2023
arXiv
CL
Spatial GNN
T-C
No
R
-
PS, PC
discrepancy for each individual channel first, before consol-
idating these discrepancies across all channels into a single
anomaly value.
To provide a simple illustration of the entire process, the
backbone can be a GNN forecaster that makes a one-step-
ahead forecast for the scorer. The scorer then computes the
anomaly score as the sum of the absolute forecast error for
each channel variable, represented as PN
i |xi
t −ˆxi
t| across N
channel variables. Since the final score is computed based on
the summation of channel errors, an operator can determine
the root cause variables by computing the contribution of
each variable to the summed error.
Advancements in the anomaly detection and diagnosis
field have led to the proposal of more comprehensive back-
bone and scoring modules [143], [149], primarily driven by
the adoption of GNN methodologies [48], [49], [70], [166].
5.2
Discrepancy Frameworks for Anomaly Detection
All the proposed anomaly detection methods follow the
same backbone-scorer architecture. However, the way the
backbone module is trained to learn data structure from
nominal data and the implementation of the scoring module
differentiate these methods into three categories: Recon-
struction, forecast, and relational discrepancy frameworks.
Reconstruction Discrepancy. Reconstruction discrep-
ancy frameworks rely on the assumption that reconstructed
error should be low during normal periods, but high during
anomalous periods. From a high-level perspective, they
are fundamentally designed to replicate their inputs as
outputs [167]. However, the assumption is that the back-
bone is sufficiently expressive to model and reconstruct
well the training data distribution, but not out-of-sample
data. Therefore, a reconstruction learning framework often
incorporates certain constraints and regularization terms,
e.g., to enforce a low-dimensional embedded code [168] or
applying variational objectives [169].
Once the data structure has been effectively learned,
the backbone model should be able to approximate the
input during non-anomalous periods, as this input would
closely resemble the normal training data. In contrast, dur-
ing an anomalous event, the backbone model is expected
to struggle with reconstructing the input, given that the
input patterns deviate from the norm and is situated outside
of the manifold. Using the reconstructed outputs from the
backbone module, a discrepancy score is computed by the
SCORER(·) to determine whether an anomalous event has
occurred. Although deep reconstruction models generally
follow these principles for detecting anomalies, a key dis-
tinction between GNNs and other architectural types rests
in the backbone reconstructor, BACKBONE(·), which is char-
acterized by its spatiotemporal GNN implementation.
MTAD-GAT [48] utilizes a variational objective [169]
to train the reconstructor module. During inference, the
reconstructor module will provide the likelihood of ob-
serving each input channel signal to the scorer. The scorer
then summarizes the likelihoods into a single reconstruction
discrepancy as an anomaly score. With the availability of
reconstruction probability for each channel variable, MTAD-
GAT can diagnose the anomaly scorer by computing the
contribution of each variable to the discrepancy score. While
MTAD-GAT shares the same variational objective as LSTM-
VAE [150], MTAD-GAT differs by employing graph atten-
tion network as a spatial-temporal encoder to learn inter-
variable and inter-temporal dependencies. Empirically, it is
shown to outperform LSTM on the same VAE objective1.
Interestingly, MTAD-GAT also shows that attention scores
in the graph attention network reflect substantial differences
between normal and anomalous periods.
GNNs require knowledge of graph structures that is
often not readily available for time series anomaly detection
data [152], [170]. To solve this issue, MTAD-GAT plainly
assumes a fully-connected graph between the spatial vari-
ables in a multivariate time series. This assumption may
not necessarily hold true in real-world scenarios and can
1. While MTAD-GAT also optimizes their network using forecasting
objective, its ablation shows that using reconstruction alone on GNN
can outperform its LSTM counterpart.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
14
potentially create unnecessary noise that weakens the ability
to learn the underlying structure of normal data.
In response, VGCRN [158] first assigns learnable em-
beddings to the channel variates of a time series data.
VGCRN then generates the channel-wise similarity matrix
as the learned graph structure by computing the dot product
between the embeddings. Under the same graph structural
learning framework, FuSAGNet [70] proposes to learn a
static and sparse directed graph by taking only the top-k
neighbors for each node in the similarity matrix. FuSAGNet
however differs in the reconstruction framework by learning
a sparse embedded code [168] rather than optimizing for
variational objectives.
In the category of reconstruction-based methods, a re-
search direction focused on graph-level embeddings to rep-
resent the input graph data as vectors to enable the applica-
tion of well-established and sophisticated detection meth-
ods designed for multivariate time series. The works by
Zambon et al. [71], [171] laid down the general framework
which has been instantiated in different nuances. Some pa-
pers design low-dimensional embedding methods trained
so that the distance between any two vector representations
best preserves the graph distance between their respective
input graphs [172], [173]. Conversely, GIF [69] employs a
high-dimension graph-level embedding method based on
the idea of random Fourier feature to discover anomalous
observations. CCM-CDT [60] is a graph autoencoder with
Riemannian manifolds as latent spaces. The autoencoder,
trained adversarially to reconstruct the input, produces
a sequence of manifold points where statistical tests are
performed to detect anomalies and changes in the data
distribution.
Forecast Discrepancy. Forecast discrepancy frameworks
rely on the assumption that forecast error should be low
during normal periods, but high during anomalous periods.
Here, the backbone module is substituted with a GNN fore-
caster that is trained to predict a one-step-ahead forecast.
During model deployment, the forecaster makes a one-step-
ahead prediction, and the forecast values are given to the
scorer. The scorer compares the forecast against the real
observed signals to compute forecast discrepancies such as
absolute error [49] or mean-squared error [154]. Importantly,
it is generally assumed that a forecasting-based model will
exhibit erratic behavior during anomaly periods when the
input data deviates from the normal patterns, resulting in a
significant forecasting discrepancy.
A seminal work in applying forecasting-based GNN
to detect anomalies in time series data is GDN [49]. The
forecaster of GDN consists of two main parts: first, a graph
structure module that learns an underlying graph structure,
and, second, a graph attention network that encodes the
input series representation. The graph structure module
computes the graph adjacency matrix for the graph atten-
tion network on the learned graph to obtain an expressive
representation for making a one-step-ahead forecast. Finally,
the scorer computes the forecast discrepancy as the maxi-
mum absolute forecast error among the channel variables to
indicate whether an anomaly event has occurred.
Interestingly, GDN illustrates that an anomaly event may
be manifested in a single variable, which acts as a symptom,
while the underlying cause may be traced to a separate,
root-cause variable. Hence, GDN proposed to utilize the
learned relationships between variables for diagnosing the
root causes of such events rather than relying solely on the
individual contributions of each variable to the discrepancy
score for diagnosing the root cause of anomaly events. This
is accomplished by identifying the symptomatic variable
that results in the maximum absolute error, followed by
pinpointing its neighbour variables. The ability of GDN
to discern these associations underscores the potential of
GNNs in offering a more holistic solution to anomaly detec-
tion and diagnosis through the automated learning of inter-
time series relationships.
Within the context of statistical methods, the AZ-
whiteness test [174] operates on the prediction residuals
obtained by forecasting models. Assuming that a forecasting
model is sufficiently good in modeling the nominal data-
generating process, the statistical test is able to identify
unexpected correlations in the data that indicate shifts in the
data distribution. The AZ-test is also able to distinguish be-
tween serial correlation, i.e., along the temporal dimension,
and spatial correlation observed between the different graph
nodes. In a similar fashion, the AZ-analysis of residuals [72]
expands the set of analytical tools to identify anomalous
nodes and time steps, thus providing a finer inspection and
diagnosis.
Relational Discrepancy. Relational discrepancy frame-
works rely on the assumption that the relationship between
variables should exhibit significant shifts from normal to
anomalous periods. This direction has been alluded to in
the MTAD-GAT work, where it was observed that attention
weights in node neighborhoods tend to deviate substan-
tially from normal patterns during anomaly periods. Conse-
quently, the logical evolution of using spatiotemporal GNN
involves leveraging its capability to learn graph structures
for both anomaly detection and diagnosis. In this context,
the backbone module serves as a graph learning module
that constructs the hidden evolving relationship between
variables. The scorer, on the other hand, is a function that
evaluates changes in these relationships and assigns an
anomaly or discrepancy score accordingly.
GReLeN [157] was the first to leverage learned dynamic
graphs to detect anomalies from the perspective of relational
discrepancy. To achieve this, the reconstruction module of
GReLeN learns to dynamically construct graph structures
that adapt at every time point based on the input time
series data. The constructed graph structures serve as the
inputs for a scorer that computes the total changes in the in-
degree and out-degree values for channel nodes. GReLeN
discovered that by focusing on sudden changes in structural
relationships, or relational discrepancy, at each time point,
they could construct a robust metric for the detection of
anomalous events.
On the other hand, DyGraphAD adopts a forecasting ap-
proach to compute relational discrepancy [162]. The method
begins by dividing a multivariate series into subsequences
and converting these subsequences into a series of dy-
namically evolving graphs. To construct a graph for each
subsequence, DyGraphAD employs the DTW distance be-
tween channel variables based on their respective values
in the subsequence. Following this preprocessing step, the
DTW distance graphs are treated as ground truth or target

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
15
and the network is trained to predict one-step-ahead graph
structures. The scorer of DyGraphAD computes the forecast
error in the graph structure as the relational discrepancy for
anomaly detection.
Hybrid and Other Discrepancies. Each type of discrep-
ancy framework often possesses unique advantages for de-
tecting and diagnosing various kinds of anomalous events.
As demonstrated in GDN [49], the relational discrepancy
framework can uncover spatial anomalies that are concealed
within the relational patterns between different channels.
In contrast, the forecast discrepancy framework may be
particularly adept at identifying temporal anomalies such
as sudden spikes or seasonal inconsistencies. Therefore,
a comprehensive solution would involve harnessing the
full potential of spatiotemporal GNNs by computing a
hybridized measure that combines multiple discrepancies as
indicators for anomaly detection. For instance, MTAD-GAT
[48] and FuSAGNet [70] employ both reconstruction and
forecast discrepancy frameworks, while DyGraphAD [162]
utilizes a combination of forecast and relational discrepancy
frameworks to enhance the detection of anomalous events.
In these cases, the scoring function should be designed to
encapsulate the combination of either reconstruction, fore-
cast or relational discrepancy. In general, the anomaly score
can be represented as St = ∥Xt−ˆXt∥2
2+∥At−ˆAt∥2
F , which
captures the reconstruction and relational discrepancies, re-
spectively. Here, Xt and At represent the target signals and
inter-series relations, while ˆXt and ˆAt denote the predicted
signal and inter-series relations.
Apart from learning the underlying structures of normal
training data, another approach to detecting time series
anomalies involves incorporating prior knowledge of how a
time series might behave during anomalous events. With
this aim in mind, GraphSAD [163] considers six distinct
types of anomalies, including spike and dip, resizing, warp-
ing, noise injection, left-side right, and upside down, to
create pseudo labels on the training data. By doing so,
unsupervised anomaly detection tasks can be transformed
into a standard classification task, with the class discrepancy
serving as an anomaly indicator.
6
GNNS FOR TIME SERIES CLASSIFICATION
Time series classification task seeks to assign a categorical
label to a given time series based on its underlying patterns
or characteristics. As outlined in a recent survey [177], early
literature in time series classification primarily focused on
distance-based approaches for assigning class labels to time
series [178], [179], [180], and ensembling methods such as
Hierarchical Vote Collective of Transformation-based En-
sembles (HIVE-COTE) [181], [182]. However, despite their
state-of-the-art performances, the scalability of both ap-
proaches remains limited for high-dimensional or large
datasets [183], [184].
To address these limitations, researchers have begun
to explore the potential of deep learning techniques for
enhancing the performance and scalability of time series
classification methods. Deep learning, with its ability to
learn complex patterns and hierarchies of features, has
shown promise in its applicability to time series classi-
fication problems, especially for datasets with substantial
training labels [185], [186]. For a comprehensive discussion
on deep learning-based time series classification, we direct
readers to the latest survey by Foumani et al. [177].
One particularly intriguing development in this area not
covered by the aforementioned survey [177] is the appli-
cation of GNN to time series classification tasks. By trans-
forming time series data into graph representations, one can
leverage the powerful capabilities of GNNs to capture both
local and global patterns. Furthermore, GNNs are capable
of mapping the intricate relationships among different time
series data samples within a particular dataset.
In the following sections, we provide a fresh GNN
perspective on the univariate and multivariate time series
classification problem.
6.1
Univariate Time Classification
Inherent in the study of time series classification lies a dis-
tinct differentiation from other time series analyses; rather
than capturing patterns within time series data, the essence
of time series classification resides in discerning differenti-
ating patterns that help separate time series data samples
based on their class labels.
For example, in the healthcare sector, time series data in
the form of heart rate readings can be utilized for health
status classification. A healthy individual may present a
steady and rhythmic heart rate pattern, whereas a patient
with cardiovascular disease may exhibit patterns indicating
irregular rhythms or elevated average heart rates. Unlike
forecasting future points or detecting real-time anomalies,
the classification task aims to distinguish these divergent
patterns across series, thereby enabling health status classifi-
cation based on these discerned patterns.
In the following, we delve into two novel graph-based
approaches to univariate time series classification, namely
Series2Graph and Series2Node.
Series2Graph. The Series2Graph approach transforms a
univariate time series into a graph to identify unique pat-
terns that enable accurate classification using a GNN. In this
manner, each series is treated as a graph, and the graph will
be the input for a GNN to make classification outputs.
Firstly, each series is broken down into subsequences as
nodes, and the nodes are connected with edges illustrating
their relationships. Following this transformation, a GNN
is applied to make graph classification. This procedure is
represented in the upper block of Fig. 4d. Fundamentally, it
seeks to model inter-temporal dependencies under a GNN
framework to identify temporal patterns that differentiate
series samples into their respective classes.
The Series2Graph perspective was first proposed by the
Time2Graph+ [73], [187] technique. The Time2Graph+ mod-
eling process can be described as a two-step process: first, a
time series is transformed into a shapelet graph, and second,
a GNN is utilized to model the relations between shapelets.
To construct a shapelet graph, the Time2Graph algorithm
partitions each time series into successive segments. It then
employs data mining techniques to assign representative
shapelets to the subsequences. These shapelets serve as
graph nodes. Edges between nodes are formed based on
the conditional probability of a shapelet occurring after
another within a time series. Consequently, each time se-
ries is converted into a graph where shapelets form nodes

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
16
TABLE 4: Summary of graph neural networks for time series classification. Task notation: “U” and “M” refer to univariate
and multivariate time series classification tasks. Conversion represents the transformation of a time series classification
task into a graph-level task as either graph or node classification task, represented as “Series2Graph” and “Series2Node”,
respectively. The remaining notations are shared with Table 2.
Approach
Year
Venue
Task
Conversion
Spatial
Module
Temporal
Module
Missing
Values
Input
Graph
Learned
Relations
Graph
Heuristics
MTPool [175]
2021
NN
M
-
Spatial GNN
T-C
No
NR
S
-
Time2Graph+ [73]
2021
TKDE
U
Series2Graph
Spatial GNN
-
No
R
-
PS
RainDrop [46]
2022
ICLR
M
-
Spatial GNN
T-A
Yes
NR
S
-
SimTSC [74]
2022
SDM
U+M
Series2Node
Spatial GNN
T-C
No
R
-
PS
LB-SimTSC [75]
2023
arXiv
U+M
Series2Node
Spatial GNN
T-C
No
R
-
PS
TodyNet [176]
2023
arXiv
M
-
Spatial GNN
T-C
No
NR
D
-
and transition probabilities create edges. After graph con-
struction, Time2Graph+ utilizes Graph Attention Network
along with a graph pooling operation to derive the global
representation of the time series. This representation is then
fed into a classifier to assign class labels to the time series.
While Time2Graph+ is the only GNN method within this
framework, we use the term Series2Graph to describe the
process of converting a time series classification task into a
graph classification task.
Series2Node. As capturing differentiating class patterns
across different series data samples is important, leveraging
relationships across the different series data samples in a
given dataset can be beneficial for classifying a time series.
To achieve this, one can take the Series2Node approach,
where each series sample is seen as a separate node. These
series nodes are connected with edges that represent the
relationships between them, creating a large graph that
provides a complete view of the entire dataset.
The
Series2Node
was
originally
proposed
by
SimTSC [74] approach. With SimTSC, series nodes are
connected using edges, which are defined by their pairwise
DTW distance, to construct a graph. During the modeling
process, a primary network is initially employed to encode
each time series into a feature vector, thus creating a node
representation. Subsequently, a standard GNN operation
is implemented to derive expressive node representations,
capturing the similarities between the series. These node
representations are then inputted into a classifier, which
assigns class labels to each time series node in the dataset.
LB-SimTSC [75] extends on SimTSC to improve the DTW
preprocessing efficiency by employing the widely-used
lower bound for DTW, known as LB Keogh [188]. This
allows for a time complexity of O(L) rather than O(L2),
dramatically reducing computation time.
The Series2Node process essentially transforms a time
series classification task into a node classification task. As
illustrated in the lower block of Fig. 4d, the Series2Node
perspective aims to leverage the relationships across differ-
ent series samples for accurate time series node classifica-
tion [75]. It is also an attempt to marry classical distance-
based approaches with advanced GNN techniques. While
not explicitly depicted in the figure, it is important to note
that the same concept can be applied to classify multivariate
time series by modifying the backbone network [74].
6.2
Multivariate Time Series Classification
In essence, multivariate time series classification maintains
fundamental similarities with its univariate counterpart,
however, it introduces an additional layer of complexity: the
necessity to capture intricate inter-time series dependencies.
For example, instead of solely considering heart rate,
patient data often incorporate time series from a multitude
of health sensors, including blood pressure sensors, blood
glucose monitors, pulse oximeters, and many others. Each
of these sensors provides a unique time series that reflects
a particular aspect of the health of a patient. By considering
these time series together in a multivariate analysis, we can
capture more complex and interrelated health patterns that
would not be apparent from any single time series alone.
Analogously, each node in an electroencephalogram
(EEG) represents electrical activity from a distinct brain
region. Given the interconnectedness of brain regions, an-
alyzing a single node in isolation may not fully capture
the comprehensive neural dynamics [189]. By employing
multivariate time series analysis, we can understand the re-
lationships between different nodes, thereby offering a more
holistic view of brain activity. This approach facilitates the
differentiation of intricate patterns that can classify patients
with and without specific neurological conditions.
In both examples, the relationships between the vari-
ables, or inter-time series dependencies, can be naturally
thought of as a network graph. Hence, they ideally suit the
capabilities of GNNs as illustrated in forecasting Sec. 4. As
such, spatiotemporal GNNs, exemplified by those utilized
in forecasting tasks [61], are conveniently adaptable for mul-
tivariate time series classification tasks. This adaptation can
be achieved through the replacement of the final layer with
a classification component. The unique design of these GNN
architectures allows for the successful detection and capture
of both inter-variable and inter-temporal dependencies. The
primary aim here is to effectively distill the complexity of
high-dimensional series data into a more comprehensible,
yet equally expressive, representation that enables differen-
tiation of time series into their representative classes [175],
[176].
The proficiency of spatiotemporal GNNs in decoding
the complexities of multivariate time series is demonstrably
showcased in the Raindrop architecture [46]. To classify
irregularly sampled data where subsets of variables have
missing values at certain timestamps, Raindrop adaptively
learns a graph structure. It then dynamically interpolates
missing observations within the embedding space, based on

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
17
any available recorded data. This flexible approach ensures
that the data representation remains both comprehensive
and accurate, despite any irregularities in the sampling.
Empirical studies provide evidence that Raindrop can
maintain robust, high-performance classification, even in
the face of such irregularities [46]. These findings further
reinforce the versatility of spatiotemporal GNNs in time
series classification, highlighting their effectiveness even
in scenarios characterized by missing data and irregular
sampling patterns.
7
GNNS FOR TIME SERIES IMPUTATION
Time series imputation, a crucial task in numerous real-
world applications, involves estimating missing or cor-
rupted values within one or more data point sequences. Tra-
ditional time series imputation approaches have relied on
statistical methodologies, such as mean imputation, spline
interpolation [200], and regression models [201]. However,
these methods often struggle to capture complex temporal
dependencies and non-linear relationships within the data.
While some deep neural network-based works, such as
[202], [203], [204], have mitigated these limitations, they
have not explicitly considered inter-time series dependen-
cies. The recent emergence of graph neural networks has
introduced new possibilities for time series imputation.
GNN-based methods better characterize intricate spatial
and temporal dependencies in time series data, making
them particularly suitable for real-world scenarios arising
from the increasing complexity of data. From a task per-
spective, GNN-based time series imputation can be broadly
categorized into two types: In-sample imputation and out-
of-sample imputation. The former involves filling in missing
values within the given time series data, while the latter
predicts missing values in disjoint sequences [50]. From a
methodological perspective, GNN for time series imputa-
tion can be further divided into deterministic and probabilis-
tic imputation. Deterministic imputation provides a single
best estimate for the missing values, while probabilistic
imputation accounts for the uncertainty in the imputation
process and provides a distribution of possible values. In
Tab. 5, we summarize most of the related works on GNN
for time series imputation to date, offering a comprehensive
overview of the field and its current state of development.
7.1
In-Sample Imputation
The majority of existing GNN-based methods primarily
focus on in-sample time series data imputation. For in-
stance, GACN [191] proposes to model spatial-temporal
dependencies in time series data by interleaving GAT [117]
and temporal convolution layers in its encoder. It then
imputes the missing data by combining GAT and temporal
deconvolution layers that map latent states back to original
feature spaces. Similarly, SPIN [193] first embeds historical
observations and sensor-level covariates to obtain initial
time series representations. These are then processed by
multi-layered sparse spatial-temporal attention blocks be-
fore the final imputations are obtained with a nonlinear
transformation. GRIN [50] introduces the graph recurrent
imputation network, where each unidirectional module con-
sists of one spatial-temporal encoder and two different im-
putation executors. The spatial-temporal encoder adopted
in this work combines MPNN [110] and GRU [114]. After
generating the latent time series representations, the first-
stage imputation fills missing values with one-step-ahead
predicted values, which are then refined by a final one-
layer MPNN before passing to the second-stage imputation
for further processing. Similar works using bidirectional
recurrent architectures include AGRN [195], DGCRIN [197],
GARNN [198], and MDGCN [199], where the main differ-
ences lie in intermediate processes. For example, AGRN
and DGCRIN propose different graph recurrent cells that
integrate graph convolution and GRU to capture spatial-
temporal relations, while GARNN involves the use of GAT
and different LSTM [205] cells to compose a graph at-
tention recurrent cell in its model architecture. MDGCN
models time series as dynamic graphs and captures spatial-
temporal dependencies by stacking bidirectional LSTM and
graph convolution. Recently, a few research studies have ex-
plored probabilistic in-sample time series imputation, such
as PriSTI [51], where the imputation has been regarded as a
generation task. In PriSTI, a similar architecture of denoising
diffusion probabilistic models [206] has been adopted to
effectively sample the missing data with a spatial-temporal
denoising network composed of attentive MPNN and tem-
poral attention.
7.2
Out-of-Sample Imputation
To date, only a few GNN-based methods fall into the
category of out-of-sample imputation. Among these works,
IGNNK [190] proposes an inductive GNN kriging model to
recover signals for unobserved time series, such as a new
variable or “virtual sensor” in a multivariate time series.
In IGNNK, the training process involves masked subgraph
sampling and signal reconstruction with the diffusion graph
convolution network presented in [129]. Another similar
work is SATCN [192], which also focuses on performing
real-time time series kriging. The primary difference be-
tween these two works lies in the underlying GNN architec-
tures, where SATCN proposes a spatial aggregation network
combined with temporal convolutions to model the under-
lying spatial-temporal dependencies. It is worth noting that
GRIN [50] can handle both in-sample and out-of-sample
imputations, as well as a similar follow-up work [194].
8
PRACTICAL APPLICATIONS
Graph neural networks have been applied to a broad range
of disciplines related to time series analysis. We categorize
the mainstream applications of GNN4TS into six areas:
Smart transportation, on-demand services, environment &
sustainable energy, internet-of-things, and healthcare.
Smart
Transportation.
The
domain
of
transportation
has been significantly transformed with the advent of
GNNs, with typical applications spanning from traffic
prediction to flight delay prediction. Traffic prediction,
specifically in terms of traffic speed and volume prediction,
is a critical component of smart transportation systems.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
18
TABLE 5: Summary of graph neural networks for time series imputation. Task notation: “Out-of-sample”, “In-sample”, and
“Both” refer to the types of imputation problems addressed by the approach. “Type” represents the imputation method as
either deterministic or probabilistic. “Inductiveness” indicates if the method can generalize to unseen nodes. The remaining
notations are shared with Table 2.
Approach
Year
Venue
Task
Type
Spatial
Module
Temporal
Module
Inductiveness
Input
Graph
Learned
Relations
Graph
Heuristics
IGNNK [190]
2021
AAAI
Out-of-sample
Deterministic
Spectral GNN
-
Yes
R
-
SP, PC
GACN [191]
2021
ICANN
In-sample
Deterministic
Spatial GNN
T-C
No
R
-
PC
SATCN [192]
2021
arXiv
Out-of-sample
Deterministic
Spatial GNN
T-C
Yes
R
-
SP
GRIN [50]
2022
ICLR
Both
Deterministic
Spatial GNN
T-R
Yes
R
-
SP
SPIN [193]
2022
NIPS
In-sample
Deterministic
Spatial GNN
T-A
No
R
-
SP
FUNS [194]
2022
ICDMW
Out-of-sample
Deterministic
Spatial GNN
T-R
Yes
R
-
-
AGRN [195]
2022
ICONIP
In-sample
Deterministic
Spatial GNN
T-R
No
NR
S
-
MATCN [196]
2022
IEEE IoT-J
In-sample
Deterministic
Spatial GNN
T-A
No
R
-
-
PriSTI [51]
2023
arXiv
In-sample
Probabilistic
Spatial GNN
T-A
No
R
-
SP
DGCRIN [197]
2023
KBS
In-sample
Deterministic
Spatial GNN
T-R
No
NR
D
-
GARNN [198]
2023
Neurocomputing
In-sample
Deterministic
Spatial GNN
T-R
No
R
-
PC
MDGCN [199]
2023
Transp. Res. Part C
In-sample
Deterministic
Spatial GNN
T-R
No
R
-
SP, PS
By leveraging advanced algorithms and data analytics
related to spatial-temporal GNNs, traffic conditions can
be accurately predicted [66], [80], [82], [86], [96], [131],
[207], thereby facilitating efficient route planning and
congestion management. Another important application
is traffic data imputation, which involves the estimation
of missing or incomplete traffic data. This is crucial for
maintaining the integrity of traffic databases and ensuring
the accuracy of traffic analysis and prediction models [196],
[197], [198], [199], [208]. There is also existing research
related to autonomous driving with 3D object detection
and motion planners based on GNNs [209], [210], [211],
which has the potential to drastically improve road safety
and traffic efficiency. Lastly, flight delay prediction is
another significant application that can greatly enhance
passenger experience and optimize airline operations. This
is achieved through the analysis of various factors such
as weather conditions, air traffic, and aircraft maintenance
schedules [212], [213]. In summary, smart transportation,
through its diverse applications, is paving the way for a
more efficient, safe, and convenient transportation system.
The integration of advanced technologies, such as GNNs, in
these applications underscores the transformative potential
of smart transportation, highlighting its pivotal role in
shaping the future of transportation.
On-Demand Services. For systems providing goods or
services upon request, GNNs have emerged as powerful
tools for modeling time series data to accurately predict
personalized real-time demands, including transportation,
energy, tourism, and more. For instance, in ride-hailing
services, GNNs capture the complex, temporal dynamics
of ride demand across different regions, enabling accurate
prediction of ride-hailing needs and thereby facilitating
efficient fleet management [54], [138], [214], [215], [216],
[217], [218]. Similarly, in bike-sharing services, GNNs
leverage the spatial-temporal patterns of bike usage to
accurately predict demand, contributing to the optimization
of
bike
distribution
and
maintenance
schedules
[55],
[219], [220], [221], [222], [223]. In the energy sector,
GNNs model the intricate relationships between various
factors influencing energy demand, providing accurate
predictions
that
aid
in
the
efficient
management
of
energy resources [224]. In the tourism industry, GNNs
capture the temporal trends and spatial dependencies
in
tourism
data,
providing
accurate
predictions
of
tourism demand and contributing to the optimization
of tourism services and infrastructure [37], [225], [226],
[227]. There are also GNN-based works that model the
complex spatial-temporal dynamics of delivery demand,
accurately
predicting
delivery
needs
and
facilitating
efficient logistics planning and operations [228]. The advent
of GNN4TS has significantly improved the accuracy of
demand prediction in on-demand services, enhancing their
efficiency and personalization. The integration of GNNs
in these applications underscores their transformative
potential, highlighting their pivotal role in shaping the
future of on-demand services.
Environment & Sustainable Energy. In the sector related
to
environment
and
sustainable
energy,
GNNs
have
been instrumental in wind speed and power prediction,
capturing the complex spatial-temporal dynamics of wind
patterns to provide accurate predictions that aid in the
efficient management of wind energy resources [229], [230],
[231], [232], [233], [234], [235]. Similarly, in solar energy,
GNNs have been used for solar irradiance and photovoltaic
(PV) power prediction, modeling the intricate relationships
between various factors influencing solar energy generation
to provide accurate predictions [236], [237], [238], [239],
[240]. In terms of system monitoring, GNNs have been
applied to wind turbines and PV systems. For wind
turbines,
GNNs
can
effectively
capture
the
temporal
dynamics of turbine performance data, enabling efficient
monitoring and maintenance of wind turbines [241]. For
PV systems, GNNs have been used for fault detection,
leveraging the spatial dependencies in PV system data
to
accurately
identify
faults
and
ensure
the
efficient
operation of PV systems [242]. Furthermore, GNNs have
been employed for air pollution prediction and weather
forecasting. By modeling the spatial-temporal patterns
of air pollution data, GNNs can accurately predict air
pollution levels, contributing to the formulation of effective
air quality management strategies [43], [243], [244]. In
weather forecasting, GNNs capture the complex, temporal
dynamics of weather patterns, providing accurate forecasts
that are crucial for various sectors, including agriculture,
energy, and transportation [44], [245].

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
19
Internet-of-Things (IoTs). IoT refers to intricately linked
devices that establish a network via wireless or wired
communication protocols, operating in concert to fulfill
shared objectives on behalf of their users [246]. These
IoT networks generate substantial quantities of high-
dimensional,
time
series
data
that
are
increasingly
complex
and
challenging
for
manual
interpretation
and understanding. Recent advancements have seen the
application of GNNs as a powerful tool for encoding the
intricate spatiotemporal relationships and dependencies
inherent in IoT networks [45], [154], [170]. GNNs leverage
their ability to unravel these convoluted relationships,
allowing for greater insights into the structure and behavior
of these networks. This approach has garnered attention
across various industrial sectors including robotics and
autonomous systems [247], [248], [249], utility plants [49],
public services [250], and sports analytics [251], [252], [253],
[254], expanding the breadth of IoT applications. With
a consistent track record of state-of-the-art results [45],
GNNs have proven integral in numerous IoT applications,
underpinning our understanding of these increasingly
complex systems.
Healthcare. Healthcare systems, spanning from individual
medical
diagnosis
and
treatment
to
broader
public
health
considerations,
present
diverse
challenges
and
opportunities that warrant the application of GNNs. In the
sphere of medical diagnosis and treatment, graph structures
can effectively capture the complex, temporal dynamics
of diverse medical settings including electrical activity
such as electronic health data [255], [256], [257], [258],
patient monitoring sensors [46], [259], [260], EEG [133],
[189], [261], brain functional connectivity such as magnetic
resonance imaging (MRI) [262], [263] and neuroimaging
data [264]. Simultaneously, for public health management,
GNNs have been proposed to predict health equipment
useful life [265] and forecasting ambulance demand [266].
More recently, GNNs have been proposed to manage
epidemic disease outbreaks as temporal graphs can provide
invaluable insights into disease spread, facilitating the
formulation of targeted containment strategies [40], [41],
[267]. In summary, the integration of GNNs with time
series data holds substantial potential for transforming
healthcare, from refining medical diagnosis and treatment
to strengthening population health strategies, highlighting
its critical role in future healthcare research.
Fraud Detection. As elucidated in the four-element fraud
diamond [268], the perpetration of fraud necessitates not
just the presence of incentive and rationalization, but also
a significant degree of capability - often attainable only
via coordinated group efforts undertaken at opportune
moments. This suggests that fraud is typically committed
by entities possessing sufficient capability, which can be
primarily achieved through collective endeavors during
suitable periods. Consequently, it is rare for fraudsters
to operate in isolation [269], [270]. They also frequently
demonstrate unusual temporal patterns in their activities,
further supporting the necessity for sophisticated fraud
detection measures [271], [272]. To this end, GNNs have
been proposed to capture these complex relational and
temporal dynamics inherent to fraud network activities.
They
have
found
successful
applications
in
various
domains, such as detecting frauds and anomalies in social
networks [271], [272], [273], financial networks and systems
[274], [275], [276], [277], and in several other sectors [270],
[278], [279], [280], [281].
Other Applications. Beyond the aforementioned sectors, the
application of GNNs for time series analysis has also been
extended to various other fields, such as finance [282], urban
planning [283], [284], epidemic control [285], [286], [287],
and particle physics [288]. As research in this area continues
to evolve, it is anticipated that the application of GNNs will
continue to expand, opening up new possibilities for data-
driven decision making and system optimization.
9
FUTURE DIRECTIONS
Pre-training, Transfer Learning, and Large Models. Pre-
training, transfer learning, and large models are emerging
as potent strategies to bolster the performance of GNNs
in time series analysis, especially when data is sparse
or diverse. These techniques hinge on utilizing learned
representations from one or more domains to enhance
performance in other related domains [3], [289]. Despite the
challenges, recent advancements, such as Panagopoulos et
al.’s model-agnostic meta-learning schema for COVID-19
spread prediction in data-limited cities, and Shao et al.’s
pre-training
enhanced
framework
for
spatial-temporal
GNNs, demonstrate the potential of these strategies [99],
[290]. The exploration of pre-training strategies and GNN
transferability for time series tasks is a burgeoning research
area, especially in the current era of generative AI and
large models, which showcase the potential for a single,
multimodal model to address diverse tasks [291]. However,
several challenges remain, including the limited availability
of time series data for large-scale pre-training compared
to language data for large language models (LLMs) [292],
ensuring the generalizability of learned knowledge to
prevent negative transfers, and designing effective pre-
training strategies that capture complex spatial-temporal
dependencies. Addressing these challenges is pivotal for
the future development and application of GNN4TS.
Robustness and Interpretability. Robustness of GNNs
refers to their ability to maintain stability under the
influence of perturbations, particularly those that are
deliberately engineered by adversaries [293]. This quality
becomes critical when dealing with time series data
generated by rapidly evolving systems. Any operational
failures within GNNs can potentially precipitate adverse
consequences on the integrity of the entire system [45],
[294]. For instance, if a GNN fails to adequately handle
noise or perturbations in a smart city application, it
might
disrupt
essential
traffic
management
functions.
Similarly, in healthcare applications, the inability of a
GNN to remain robust amidst disturbances could lead
to healthcare providers missing out on critical treatment
periods, potentially having serious health implications
for patients. While GNNs have demonstrated superior

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
20
performance
across
numerous
applications,
improving
their robustness and creating effective failure management
strategies remains vital. This not only enhances their
reliability but also widens their potential usage across
contexts.
The interpretability of GNNs plays an equally pivotal
role in facilitating the transparent and accountable use of
these sophisticated tools. This attribute sheds light on the
opaque decision-making processes of GNNs, allowing users
to comprehend the reasoning behind a given output or
prediction. Such understanding fosters trust in the system
and enables the discovery of latent patterns within the
data [293], [295]. For example, in healthcare and financial
time series analyses, interpretability may illuminate causal
factors [296], [297], facilitating more informed decision-
making. As we strive to harness the full potential of
GNN4TS, advancing their interpretability is paramount
to ensuring their ethical and judicious application in
increasingly complex environments.
Uncertainty
Quantification.
Time
series
data,
by
its
nature, is dynamic and often fraught with unpredictable
fluctuations and noise. In such environments, the ability
of a model to account for and quantify uncertainty can
greatly enhance its reliability and utility [298], [299].
Uncertainty quantification provides a probabilistic measure
of the confidence in the predictions made by the model,
aiding in the understanding of the range and likelihood
of potential outcomes [300]. This becomes particularly
important when GNNs are used for decision-making
processes in fields where high stakes are involved, such
as
financial
forecasting,
healthcare
monitoring
[255],
[258], or traffic prediction in smart cities [54], [82], [84],
[228]. Despite progress, a gap remains in the current
GNN models, which largely provide point estimates [46],
[49], [61], [74], [82], [84], [154], inadequately addressing
the potential uncertainties. This underlines an essential
research direction: developing sophisticated uncertainty
quantification methods for GNNs to better navigate the
complexities of time series data. This endeavor not only
enhances the interpretability and reliability of predictions
but also fosters the development of advanced models
capable of learning from uncertainty. Thus, uncertainty
quantification, albeit nascent, represents a promising and
pivotal pathway in the ongoing advancement of GNN4TS.
Privacy Enhancing. GNNs have established themselves as
invaluable tools in time series analysis, playing crucial roles
in diverse, interconnected systems across various sectors
[210], [212], [214], [255], [271], [276]. As these models gain
broader adoption, particularly in fields that require the
powerful data forecasting [49], [189] and reconstruction [48],
[50] capabilities of GNNs, the need for stringent privacy
protection becomes increasingly apparent. Given the ability
of GNNs to learn and reconstruct the relationships between
entities within complex systems [128], [162], it is essential
to safeguard not only the privacy of individual entities
(nodes), but also their relationship (edges) within the time
series data [293], [301]. Furthermore, the interpretability of
GNNs can serve as a double-edged sword. While it can help
identify and mitigate areas vulnerable to malicious attacks,
it could also expose the system to new risks by revealing
sensitive information [302]. Therefore, maintaining robust
privacy defenses while capitalizing on the benefits of
GNN models for time series analysis requires a delicate
balance, one that calls for constant vigilance and continual
innovation.
Scalability. GNNs have emerged as powerful tools to
model and analyze increasingly large and complex time
series data, such as large social networks consisting of
billions of users and relationships [271], [273]. However,
the adaptation of GNNs to manage vast volumes of time-
dependent data introduces unique challenges. Traditional
GNN models often require the computation of the entire
adjacency matrix and node embeddings for the graph,
which can be extraordinarily memory-intensive [303]. To
counter these challenges, traditional GNN methods utilize
sampling strategies like node-wise [304], [305], layer-wise
[306], and graph-wise [307] sampling. Yet, incorporating
these methods while preserving consideration for temporal
dependencies
remains
a
complex
task.
Additionally,
improving scalability during the inference phase to allow
real-time application of GNNs for time series analysis is
vital, especially in edge devices with limited computational
resources.
This
intersection
of
scalability,
time-series
analysis, and real-time inference presents a compelling
research frontier, ripe with opportunities for breakthroughs.
Hence, exploring these areas can be a pivotal GNN4TS
research direction.
AutoML and Automation. Despite the notable success of
GNNs in temporal analytics [46], [48], [50], [61], their em-
pirical implementations often necessitate meticulous archi-
tecture engineering and hyperparameter tuning to accom-
modate varying types of graph-structured data [308], [309].
A GNN architecture is typically instantiated from its model
space and evaluated in each graph analysis task based on
prior knowledge and iterative tuning processes [303]. Fur-
thermore, with the plethora of architectures being proposed
for different use cases [33], [50], [74], [166], [177], discerning
the most suitable option poses a significant challenge for
end users.
AutoML and automation in time series analysis using
GNNs thus plays a pivotal role in overcoming the com-
plexities associated with diverse model architectures. It can
simplify the selection process, enhancing efficiency and scal-
ability while fostering effective model optimization [308],
[310], [311]. Furthermore, it is important to note that GNNs
may not always be the optimal choice compared to other
methods [312], [313], [314]. Their role within the broader
landscape of AutoML must therefore be thoughtfully evalu-
ated. By encouraging reproducibility and broadening acces-
sibility, automation democratizes the benefits of GNNs for
advanced temporal analytics.
10
CONCLUSIONS
This comprehensive survey bridges the knowledge gap in
the field of graph neural networks for time series anal-
ysis (GNN4TS) by providing a detailed review of recent

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
21
advancements and offering a unified taxonomy to catego-
rize existing works from task- and methodology-oriented
perspectives. As the first of its kind, it covers a wide range
of tasks including forecasting, classification, anomaly detec-
tion, and imputation, providing a detailed understanding
of the state-of-the-art in GNN4TS. We also delve into the
intricacies of spatial and temporal dependencies modeling
and overall model architecture, offering a fine-grained clas-
sification of individual studies. Highlighting the expanding
applications of GNN4TS across various sectors, we demon-
strate its versatility and potential for future growth. This
survey serves as a valuable resource for machine learning
practitioners and domain experts interested in the latest
advancements in this field. Lastly, we propose potential
future research directions, offering insights to guide and
inspire future work in GNN4TS.
REFERENCES
[1]
Q. Wen, L. Yang, T. Zhou, and L. Sun, “Robust time series analysis
and applications: An industrial perspective,” in KDD, 2022, pp.
4836–4837.
[2]
P. Esling and C. Agon, “Time-series data mining,” ACM Comput-
ing Surveys, vol. 45, no. 1, pp. 1–34, 2012.
[3]
K. Zhang, Q. Wen, C. Zhang, R. Cai, M. Jin, Y. Liu, J. Zhang,
Y. Liang, G. Pang, D. Song, and S. Pan, “Self-supervised learning
for time series analysis: Taxonomy, progress, and prospects,”
arXiv preprint, vol. abs/2306.10125, 2023.
[4]
B. Lim and S. Zohren, “Time-series forecasting with deep learn-
ing: a survey,” Philos. Trans. Royal Soc. A PHILOS T R SOC A, vol.
379, no. 2194, p. 20200209, 2021.
[5]
H. Ismail Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A.
Muller, “Deep learning for time series classification: a review,”
DMKD, vol. 33, no. 4, pp. 917–963, 2019.
[6]
A. Bl´azquez-Garc´ıa, A. Conde, U. Mori, and J. A. Lozano, “A
review on outlier/anomaly detection in time series data,” ACM
Computing Surveys, vol. 54, no. 3, pp. 1–33, 2021.
[7]
C. Fang and C. Wang, “Time series data imputation: A survey on
deep learning approaches,” arXiv preprint, vol. abs/2011.11347,
2020.
[8]
J. Gao, X. Song, Q. Wen, P. Wang, L. Sun, and H. Xu, “RobustTAD:
Robust time series anomaly detection via decomposition and
convolutional neural networks,” KDD Workshop on Mining and
Learning from Time Series, 2020.
[9]
J. Zhang, F.-Y. Wang, K. Wang, W.-H. Lin, X. Xu, and C. Chen,
“Data-driven intelligent transportation systems: A survey,” IEEE
TITS, vol. 12, no. 4, pp. 1624–1639, 2011.
[10]
Y. Zhou, Z. Ding, Q. Wen, and Y. Wang, “Robust load forecasting
towards adversarial attacks via bayesian learning,” IEEE TPS,
vol. 38, no. 2, pp. 1445–1459, 2023.
[11]
A. A. Cook, G. Mısırlı, and Z. Fan, “Anomaly detection for iot
time-series data: A survey,” IEEE IoT Journal, vol. 7, no. 7, pp.
6481–6494, 2019.
[12]
S. Wang, J. Cao, and S. Y. Philip, “Deep learning for spatio-
temporal data mining: A survey,” IEEE TKDE, vol. 34, no. 8, pp.
3681–3700, 2020.
[13]
G. Jin, Y. Liang, Y. Fang, J. Huang, J. Zhang, and Y. Zheng,
“Spatio-temporal graph neural networks for predictive learn-
ing
in
urban
computing:
A
survey,”
arXiv
preprint,
vol.
abs/2303.14483, 2023.
[14]
L.-J. Cao and F. E. H. Tay, “Support vector machine with adaptive
parameters in financial time series forecasting,” IEEE TNNLS,
vol. 14, no. 6, pp. 1506–1518, 2003.
[15]
C.-J. Lu, T.-S. Lee, and C.-C. Chiu, “Financial time series forecast-
ing using independent component analysis and support vector
regression,” DSS, vol. 47, no. 2, pp. 115–125, 2009.
[16]
Y. Xia and J. Chen, “Traffic flow forecasting method based on
gradient boosting decision tree,” in FMSMT.
Atlantis Press,
2017, pp. 413–416.
[17]
E. Rady, H. Fawzy, and A. M. A. Fattah, “Time series forecasting
using tree based methods,” J. Stat. Appl. Probab, vol. 10, pp. 229–
244, 2021.
[18]
B. Biller and B. L. Nelson, “Modeling and generating multivariate
time-series input processes using a vector autoregressive tech-
nique,” ACM TOMACS, vol. 13, no. 3, pp. 211–237, 2003.
[19]
E. Zivot and J. Wang, “Vector autoregressive models for multi-
variate time series,” Modeling financial time series with S-PLUS®,
pp. 385–429, 2006.
[20]
G. E. Box and D. A. Pierce, “Distribution of residual autocorre-
lations in autoregressive-integrated moving average time series
models,” JASA, vol. 65, no. 332, pp. 1509–1526, 1970.
[21]
A. Guin, “Travel time prediction using a seasonal autoregressive
integrated moving average time series model,” in ITSC, 2006, pp.
493–498.
[22]
M. Jin, Y. Zheng, Y.-F. Li, S. Chen, B. Yang, and S. Pan, “Multi-
variate time series forecasting with dynamic graph neural odes,”
IEEE TKDE, 2022.
[23]
B. Zhao, H. Lu, S. Chen, J. Liu, and D. Wu, “Convolutional neural
networks for time series classification,” J. Syst. Eng. Electron.,
vol. 28, no. 1, pp. 162–169, 2017.
[24]
A. Borovykh, S. Bohte, and C. W. Oosterlee, “Conditional time
series forecasting with convolutional neural networks,” arXiv
preprint, vol. abs/1703.04691, 2017.
[25]
J. T. Connor, R. D. Martin, and L. E. Atlas, “Recurrent neural
networks and robust time series prediction,” IEEE TNNLS, vol. 5,
no. 2, pp. 240–254, 1994.
[26]
Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang, and G. W. Cottrell,
“A dual-stage attention-based recurrent neural network for time
series prediction,” in IJCAI, 2017, pp. 2627–2633.
[27]
Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun,
“Transformers in time series: A survey,” in IJCAI, 2023.
[28]
M. Jin, G. Shi, Y.-F. Li, Q. Wen, B. Xiong, T. Zhou, and S. Pan,
“How expressive are spectral-temporal graph neural networks
for time series forecasting?” arXiv preprint, vol. abs/2305.06587,
2023.
[29]
Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A
comprehensive survey on graph neural networks,” IEEE TNNLS,
vol. 32, no. 1, pp. 4–24, 2020.
[30]
Y. Liu, M. Jin, S. Pan, C. Zhou, Y. Zheng, F. Xia, and S. Y. Philip,
“Graph self-supervised learning: A survey,” IEEE TKDE, vol. 35,
no. 6, pp. 5879–5900, 2022.
[31]
M. Jin, Y. Zheng, Y.-F. Li, C. Gong, C. Zhou, and S. Pan, “Multi-
scale contrastive siamese networks for self-supervised graph
representation learning,” in IJCAI, 2021, pp. 1477–1483.
[32]
Z. A. Sahili and M. Awad, “Spatio-temporal graph neural net-
works: A survey,” arXiv preprint, vol. abs/2301.10569, 2023.
[33]
W. Jiang and J. Luo, “Graph neural network for traffic forecasting:
A survey,” Expert Syst. Appl., p. 117921, 2022.
[34]
K.-H. N. Bui, J. Cho, and H. Yi, “Spatial-temporal graph neural
network for traffic forecasting: An overview and open research
issues,” APIN, vol. 52, no. 3, pp. 2763–2774, 2022.
[35]
J. Ye, J. Zhao, K. Ye, and C. Xu, “How to build a graph-based deep
learning architecture in traffic domain: A survey,” IEEE TITS,
vol. 23, no. 5, pp. 3904–3924, 2020.
[36]
S. Rahmani, A. Baghbani, N. Bouguila, and Z. Patterson, “Graph
neural networks for intelligent transportation systems: A sur-
vey,” IEEE TITS, 2023.
[37]
D. Zhuang, S. Wang, H. Koutsopoulos, and J. Zhao, “Uncertainty
quantification of sparse travel demand prediction with spatial-
temporal graph neural networks,” in KDD, 2022, pp. 4639–4647.
[38]
A. Zanfei, B. M. Brentan, A. Menapace, M. Righetti, and M. Her-
rera, “Graph convolutional recurrent neural networks for water
demand forecasting,” WRR, vol. 58, no. 7, p. e2022WR032299,
2022.
[39]
W. Liao, B. Bak-Jensen, J. R. Pillai, Y. Wang, and Y. Wang, “A
review of graph neural networks and their applications in power
systems,” J. Mod. Power Syst. Clean Energy., vol. 10, no. 2, pp.
345–360, 2021.
[40]
C. Fritz, E. Dorigatti, and D. R¨ugamer, “Combining graph neural
networks and spatio-temporal disease models to improve the
prediction of weekly covid-19 cases in germany,” Scientific Re-
ports, vol. 12, no. 1, p. 3930, 2022.
[41]
L. Wang, A. Adiga, J. Chen, A. Sadilek, S. Venkatramanan, and
M. V. Marathe, “Causalgnn: Causal-based graph neural networks
for spatio-temporal epidemic forecasting,” in AAAI, 2022, pp.
12 191–12 199.
[42]
J. Wang, S. Zhang, Y. Xiao, and R. Song, “A review on graph
neural network methods in financial applications,” arXiv preprint,
vol. abs/2111.15367, 2021.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
22
[43]
H. Zhou, F. Zhang, Z. Du, and R. Liu, “Forecasting pm2. 5 us-
ing hybrid graph convolution-based model considering dynamic
wind-field to offer the benefit of spatial interpretability,” Environ.
Pollut., vol. 273, p. 116473, 2021.
[44]
R. Keisler, “Forecasting global weather with graph neural net-
works,” arXiv preprint, vol. abs/2202.07575, 2022.
[45]
G. Dong, M. Tang, Z. Wang, J. Gao, S. Guo, L. Cai, R. Gutierrez,
B. Campbel, L. E. Barnes, and M. Boukhechba, “Graph neural
networks in iot: A survey,” ACM TOSN, vol. 19, no. 2, pp. 1–50,
2023.
[46]
X. Zhang, M. Zeman, T. Tsiligkaridis, and M. Zitnik, “Graph-
guided network for irregularly sampled multivariate time se-
ries,” in ICLR, 2022.
[47]
Z. Wang, T. Jiang, Z. Xu, J. Gao, and J. Zhang, “Irregularly
sampled multivariate time series classification: A graph learning
approach,” IEEE Intelligent Systems, 2023.
[48]
H. Zhao, Y. Wang, J. Duan, C. Huang, D. Cao, Y. Tong, B. Xu,
J. Bai, J. Tong, and Q. Zhang, “Multivariate time-series anomaly
detection via graph attention network,” in ICDM, 2020, pp. 841–
850.
[49]
A. Deng and B. Hooi, “Graph neural network-based anomaly
detection in multivariate time series,” in AAAI, 2021, pp. 4027–
4035.
[50]
A. Cini, I. Marisca, and C. Alippi, “Filling the g ap s: Multivari-
ate time series imputation by graph neural networks,” in ICLR,
2022.
[51]
M. Liu, H. Huang, H. Feng, L. Sun, B. Du, and Y. Fu, “Pristi: A
conditional diffusion framework for spatiotemporal imputation,”
arXiv preprint, vol. abs/2302.09746, 2023.
[52]
M. Jin, Y.-F. Li, and S. Pan, “Neural temporal walks: Motif-aware
representation learning on continuous-time dynamic graphs,” in
NeurIPS, 2022.
[53]
D. Bacciu, F. Errica, A. Micheli, and M. Podda, “A gentle intro-
duction to deep learning for graphs,” Neural Networks, 2020.
[54]
X. Geng, Y. Li, L. Wang, L. Zhang, Q. Yang, J. Ye, and Y. Liu, “Spa-
tiotemporal multi-graph convolution network for ride-hailing
demand forecasting,” in AAAI, 2019, pp. 3656–3663.
[55]
S. He and K. G. Shin, “Towards fine-grained flow forecasting: A
graph attention approach for bike sharing systems,” in WWW,
2020, pp. 88–98.
[56]
X. Zhang, R. Cao, Z. Zhang, and Y. Xia, “Crowd flow forecasting
with multi-graph neural networks,” in IJCNN, 2020, pp. 1–7.
[57]
M. Li and Z. Zhu, “Spatial-temporal fusion graph neural net-
works for traffic flow forecasting,” in AAAI, 2021, pp. 4189–4196.
[58]
S. Yi and V. Pavlovic, “Sparse granger causality graphs for human
action classification,” in ICPR, 2012, pp. 3374–3377.
[59]
Y. Wang, Z. Duan, Y. Huang, H. Xu, J. Feng, and A. Ren,
“Mthetgnn: A heterogeneous graph embedding framework for
multivariate time series forecasting,” Pattern Recognition Letters,
vol. 153, pp. 151–158, 2022.
[60]
D. Grattarola, D. Zambon, C. Alippi, and L. Livi, “Change
detection in graph streams by learning graph embeddings on
constant-curvature manifolds,” IEEE TNNLS, 2019.
[61]
Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang,
“Connecting the dots: Multivariate time series forecasting with
graph neural networks,” in KDD, 2020, pp. 753–763.
[62]
D. Cao, Y. Wang, J. Duan, C. Zhang, X. Zhu, C. Huang,
Y. Tong, B. Xu, J. Bai, J. Tong, and Q. Zhang, “Spectral temporal
graph neural network for multivariate time-series forecasting,”
in NeurIPS, 2020.
[63]
V. G. Satorras, S. S. Rangapuram, and T. Januschowski, “Multi-
variate time series forecasting with latent graph inference,” arXiv
preprint, vol. abs/2203.03423, 2022.
[64]
C. Shang, J. Chen, and J. Bi, “Discrete graph structure learning
for forecasting multiple time series,” in ICLR, 2021.
[65]
A. Cini, D. Zambon, and C. Alippi, “Sparse graph learning for
spatiotemporal time series,” arXiv preprint, vol. abs/2205.13492,
2022.
[66]
B. Yu, H. Yin, and Z. Zhu, “Spatio-temporal graph convolutional
networks: A deep learning framework for traffic forecasting,” in
IJCAI, 2018, pp. 3634–3640.
[67]
H. Wen, Y. Lin, Y. Xia, H. Wan, R. Zimmermann, and Y. Liang,
“Diffstg: Probabilistic spatio-temporal graph forecasting with
denoising diffusion models,” arXiv preprint, vol. abs/2301.13629,
2023.
[68]
E. Dai and J. Chen, “Graph-augmented normalizing flows for
anomaly detection of multiple time series,” in ICLR, 2022.
[69]
D. Zambon, L. Livi, and C. Alippi, “Graph iForest: Isolation of
anomalous and outlier graphs,” in IJCNN, 2022, pp. 1–8.
[70]
S. Han and S. S. Woo, “Learning sparse latent graph representa-
tions for anomaly detection in multivariate time series,” in KDD,
2022, pp. 2977–2986.
[71]
D. Zambon, C. Alippi, and L. Livi, “Concept drift and anomaly
detection in graph streams,” IEEE TNNLS, pp. 1–14, 2018.
[72]
D. Zambon and C. Alippi, “Where and how to improve
graph-based spatio-temporal predictors,” arXiv preprint, vol.
abs/2302.01701, 2023.
[73]
Z. Cheng, Y. Yang, S. Jiang, W. Hu, Z. Ying, Z. Chai, and C. Wang,
“Time2graph+: Bridging time series and graph representation
learning via multiple attentions,” IEEE TKDE, 2021.
[74]
D. Zha, K.-H. Lai, K. Zhou, and X. Hu, “Towards similarity-aware
time-series classification,” in SDM, 2022, pp. 199–207.
[75]
W. Xi, A. Jain, L. Zhang, and J. Lin, “Lb-simtsc: An efficient
similarity-aware graph neural network for semi-supervised time
series classification,” arXiv preprint, vol. abs/2301.04838, 2023.
[76]
D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-
dergheynst, “The emerging field of signal processing on graphs:
Extending high-dimensional data analysis to networks and other
irregular domains,” IEEE Signal Process. Mag., vol. 30, no. 3, pp.
83–98, 2013.
[77]
A. Sandryhaila and J. M. Moura, “Discrete signal processing on
graphs,” IEEE TIP, vol. 61, no. 7, pp. 1644–1656, 2013.
[78]
R. Wan, S. Mei, J. Wang, M. Liu, and F. Yang, “Multivariate tem-
poral convolutional network: A deep neural networks approach
for multivariate time series forecasting,” Electronics, vol. 8, no. 8,
p. 876, 2019.
[79]
Z. Fang, Q. Long, G. Song, and K. Xie, “Spatial-temporal graph
ode networks for traffic flow forecasting,” in KDD, 2021, pp. 364–
373.
[80]
Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional
recurrent neural network: Data-driven traffic forecasting,” in
ICLR, 2018.
[81]
J. Gao and B. Ribeiro, “On the Equivalence Between Temporal
and Static Equivariant Graph Representations,” in ICML, 2022,
pp. 7052–7076.
[82]
Z. Pan, Y. Liang, W. Wang, Y. Yu, Y. Zheng, and J. Zhang, “Urban
traffic prediction from spatio-temporal data using deep meta
learning,” in KDD, 2019, pp. 1720–1730.
[83]
D. Zambon, D. Grattarola, C. Alippi, and L. Livi, “Autoregressive
models for sequences of graphs,” in IJCNN, 2019.
[84]
S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, “Attention based
spatial-temporal graph convolutional networks for traffic flow
forecasting,” in AAAI, 2019, pp. 922–929.
[85]
Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, “Graph wavenet
for deep spatial-temporal graph modeling,” in IJCAI, 2019, pp.
1907–1913.
[86]
W. Chen, L. Chen, Y. Xie, W. Cao, Y. Gao, and X. Feng, “Multi-
range attentive bicomponent graph convolutional network for
traffic forecasting,” in AAAI, 2020, pp. 3529–3536.
[87]
X. Wang, Y. Ma, Y. Wang, W. Jin, X. Wang, J. Tang, C. Jia, and
J. Yu, “Traffic flow prediction via spatial temporal graph neural
network,” in WWW, 2020, pp. 1082–1092.
[88]
C. Zheng, X. Fan, C. Wang, and J. Qi, “GMAN: A graph multi-
attention network for traffic prediction,” in AAAI, 2020, pp. 1234–
1241.
[89]
Q. Zhang, J. Chang, G. Meng, S. Xiang, and C. Pan, “Spatio-
temporal graph structure learning for traffic forecasting,” in
AAAI, 2020, pp. 1177–1185.
[90]
C. Song, Y. Lin, S. Guo, and H. Wan, “Spatial-temporal syn-
chronous graph convolutional networks: A new framework for
spatial-temporal network data forecasting,” in AAAI, 2020, pp.
914–921.
[91]
L. Bai, L. Yao, C. Li, X. Wang, and C. Wang, “Adaptive graph con-
volutional recurrent network for traffic forecasting,” in NeurIPS,
2020.
[92]
R. Huang, C. Huang, Y. Liu, G. Dai, and W. Kong, “LSGCN:
long short-term traffic prediction with graph convolutional net-
works,” in IJCAI, 2020, pp. 2355–2361.
[93]
C. Yu, X. Ma, J. Ren, H. Zhao, and S. Yi, “Spatio-temporal graph
transformer networks for pedestrian trajectory prediction,” in
ECCV.
Springer, 2020, pp. 507–523.
[94]
B. Paassen, D. Grattarola, D. Zambon, C. Alippi, and B. E.
Hammer, “Graph edit networks,” in ICLR, 2021.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
23
[95]
Y. Chen, I. Segovia-Dominguez, and Y. R. Gel, “Z-gcnets: Time
zigzags at graph convolutional networks for time series forecast-
ing,” in ICML, ser. Proceedings of Machine Learning Research,
vol. 139, 2021, pp. 1684–1694.
[96]
S. Lan, Y. Ma, W. Huang, W. Wang, H. Yang, and P. Li,
“DSTAGNN: dynamic spatial-temporal aware graph neural net-
work for traffic flow forecasting,” in ICML, ser. Proceedings of
Machine Learning Research, vol. 162, 2022, pp. 11 906–11 917.
[97]
Y. Liu, Q. Liu, J.-W. Zhang, H. Feng, Z. Wang, Z. Zhou, and
W. Chen, “Multivariate time-series forecasting with temporal
polynomial graph neural networks,” in NeurIPS, 2022.
[98]
J. Choi, H. Choi, J. Hwang, and N. Park, “Graph neural controlled
differential equations for traffic forecasting,” in AAAI, 2022, pp.
6367–6374.
[99]
Z. Shao, Z. Zhang, F. Wang, and Y. Xu, “Pre-training enhanced
spatial-temporal graph neural network for multivariate time
series forecasting,” in KDD, 2022, pp. 1567–1577.
[100] J. Chauhan, A. Raghuveer, R. Saket, J. Nandy, and B. Ravindran,
“Multi-variate time series forecasting on variable subsets,” in
KDD, 2022, pp. 76–86.
[101] H. Yu, T. Li, W. Yu, J. Li, Y. Huang, L. Wang, and A. Liu,
“Regularized graph structure learning with semantic knowledge
for multi-variates time-series forecasting,” in IJCAI, 2022, pp.
2362–2368.
[102] X. Rao, H. Wang, L. Zhang, J. Li, S. Shang, and P. Han, “Fogs:
First-order gradient supervision with learning-based graph for
traffic flow forecasting,” in IJCAI, 2022.
[103] Y. Cui, K. Zheng, D. Cui, J. Xie, L. Deng, F. Huang, and X. Zhou,
“Metro: a generic graph neural network framework for multi-
variate time series forecasting,” VLDB, vol. 15, no. 2, pp. 224–236,
2021.
[104] A. Cini*, I. Marisca*, F. Bianchi, and C. Alippi, “Scalable spa-
tiotemporal graph neural networks,” in AAAI, 2023.
[105] H. L¨utkepohl, “Vector autoregressive models,” in Handbook of
research methods and applications in empirical macroeconomics, 2013,
pp. 139–164.
[106] G. Lai, W. Chang, Y. Yang, and H. Liu, “Modeling long- and short-
term temporal patterns with deep neural networks,” in SIGIR,
2018, pp. 95–104.
[107] S.-Y. Shih, F.-K. Sun, and H.-y. Lee, “Temporal pattern attention
for multivariate time series forecasting,” Machine Learning, vol.
108, pp. 1421–1441, 2019.
[108] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional
neural networks on graphs with fast localized spectral filtering,”
in NeurIPS, 2016, pp. 3837–3845.
[109] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager,
“Temporal convolutional networks for action segmentation and
detection,” in CVPR, 2017, pp. 1003–1012.
[110] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
“Neural message passing for quantum chemistry,” in ICML, ser.
Proceedings of Machine Learning Research, vol. 70, 2017, pp.
1263–1272.
[111] J. Atwood and D. Towsley, “Diffusion-convolutional neural net-
works,” in NeurIPS, 2016, pp. 1993–2001.
[112] J. Klicpera, S. Weißenberger, and S. G¨unnemann, “Diffusion
improves graph learning,” in NeurIPS, 2019, pp. 13 333–13 345.
[113] Y. Zheng, H. Zhang, V. Lee, Y. Zheng, X. Wang, and S. Pan,
“Finding the missing-half: Graph complementary learning for
homophily-prone and heterophily-prone graphs,” in ICML, 2023.
[114] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evalua-
tion of gated recurrent neural networks on sequence modeling,”
in NeurIPS 2014 Workshop on Deep Learning, 2014.
[115] X. Zhang, C. Huang, Y. Xu, L. Xia, P. Dai, L. Bo, J. Zhang, and
Y. Zheng, “Traffic flow forecasting with spatial-temporal graph
diffusion network,” in AAAI, 2021, pp. 15 008–15 015.
[116] T. N. Kipf and M. Welling, “Semi-supervised classification with
graph convolutional networks,” in ICLR, 2017.
[117] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and
Y. Bengio, “Graph attention networks,” in ICLR, 2018.
[118] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and
T. Liu, “Do transformers really perform badly for graph repre-
sentation?” in NeurIPS, 2021, pp. 28 877–28 888.
[119] A. Feng and L. Tassiulas, “Adaptive graph spatial-temporal
transformer network for traffic forecasting,” in CIKM, 2022, pp.
3933–3937.
[120] B. Huang, H. Dou, Y. Luo, J. Li, J. Wang, and T. Zhou, “Adap-
tive spatiotemporal transformer graph network for traffic flow
forecasting by iot loop detectors,” IEEE IoT Journal, 2022.
[121] C. Wang, K. Zhang, H. Wang, and B. Chen, “Auto-stgcn:
Autonomous spatial-temporal graph convolutional network
search,” ACM TKDD, vol. 17, no. 5, pp. 1–21, 2023.
[122] M. Lukoˇseviˇcius and H. Jaeger, “Reservoir computing ap-
proaches to recurrent neural network training,” Comput. Sci. Rev.,
vol. 3, no. 3, pp. 127–149, Aug. 2009.
[123] A. Micheli and D. Tortorella, “Discrete-time dynamic graph echo
state networks,” Neurocomputing, vol. 496, pp. 85–95, Jul. 2022.
[124] Z. Diao, X. Wang, D. Zhang, Y. Liu, K. Xie, and S. He, “Dynamic
spatial-temporal graph convolutional neural networks for traffic
forecasting,” in AAAI, 2019, pp. 890–897.
[125] T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud,
“Neural ordinary differential equations,” in NeurIPS, 2018, pp.
6572–6583.
[126] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”
in NeurIPS, 2017, pp. 5998–6008.
[127] C. Park, C. Lee, H. Bahng, Y. Tae, S. Jin, K. Kim, S. Ko, and J. Choo,
“ST-GRAT: A novel spatio-temporal graph attention networks
for accurately forecasting dynamically changing road speed,” in
CIKM, 2020.
[128] K. Guo, Y. Hu, Y. Sun, S. Qian, J. Gao, and B. Yin, “Hierarchical
graph convolution network for traffic forecasting,” in AAAI, 2021,
pp. 151–159.
[129] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional
recurrent neural network: Data-driven traffic forecasting,” in
ICLR, 2018.
[130] R. Sawhney, S. Agarwal, A. Wadhwa, and R. R. Shah, “Spatiotem-
poral hypergraph convolution network for stock movement fore-
casting,” in ICDM, 2020, pp. 482–491.
[131] Y. Tang, A. Qu, A. H. Chow, W. H. Lam, S. Wong, and W. Ma,
“Domain adversarial spatial-temporal network: A transferable
framework for short-term traffic forecasting across cities,” in
CIKM, 2022, pp. 1905–1915.
[132] J. Ye, Z. Liu, B. Du, L. Sun, W. Li, Y. Fu, and H. Xiong, “Learning
the evolutionary and multi-scale graph structure for multivariate
time series forecasting,” in CIKM, 2022, pp. 2296–2306.
[133] Z. Jia, Y. Lin, J. Wang, R. Zhou, X. Ning, Y. He, and Y. Zhao,
“Graphsleepnet: Adaptive spatial-temporal graph convolutional
networks for sleep stage classification,” in IJCAI, 2020, pp. 1324–
1330.
[134] R.-G. Cirstea, B. Yang, and C. Guo, “Graph attention recurrent
neural networks for correlated time series forecasting,” in 5th
SIGKDD Workshop on Mining and Learning from Time Series, 2019,
pp. 1–6.
[135] B. Yu, H. Yin, and Z. Zhu, “St-unet: A spatio-temporal u-network
for graph-structured time series modeling,” arXiv preprint, vol.
abs/1903.05631, 2019.
[136] R. Jiang, Z. Wang, J. Yong, P. Jeph, Q. Chen, Y. Kobayashi, X. Song,
S. Fukushima, and T. Suzumura, “Spatio-temporal meta-graph
learning for traffic forecasting,” in AAAI, 2023.
[137] Y. Chen, I. Segovia-Dominguez, B. Coskunuzer, and Y. R. Gel,
“Tamp-s2gcnets: Coupling time-aware multipersistence knowl-
edge representation with spatio-supra graph convolutional net-
works for time-series forecasting,” in ICLR, 2022.
[138] L. Bai, L. Yao, S. S. Kanhere, X. Wang, and Q. Z. Sheng, “Stg2seq:
Spatial-temporal graph to sequence model for multi-step passen-
ger demand forecasting,” in IJCAI, 2019, pp. 1981–1987.
[139] P. Kidger, J. Morrill, J. Foster, and T. J. Lyons, “Neural controlled
differential equations for irregular time series,” in NeurIPS, 2020.
[140] J. Oskarsson, P. Sid´en, and F. Lindsten, “Temporal graph neural
networks for irregular data,” in AISTATS, 2023, pp. 4515–4531.
[141] D. M. Hawkins, Identification of outliers.
Springer, 1980, vol. 11.
[142] M. A. Pimentel, D. A. Clifton, L. Clifton, and L. Tarassenko, “A
review of novelty detection,” Signal Processing, vol. 99, pp. 215–
249, 2014.
[143] Z. Z. Darban, G. I. Webb, S. Pan, C. C. Aggarwal, and M. Salehi,
“Deep learning for time series anomaly detection: A survey,”
arXiv preprint, vol. abs/2211.05244, 2022.
[144] E. M. Knox and R. T. Ng, “Algorithms for mining distance based
outliers in large datasets,” in VLDB.
Citeseer, 1998, pp. 392–403.
[145] E. Keogh, J. Lin, and A. Fu, “Hot sax: Efficiently finding the most
unusual time series subsequence,” in ICDM, 2005.
[146] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, “Lof:
identifying density-based local outliers,” in SIGMOD, 2000, pp.
93–104.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
24
[147] M. Hahsler and M. Bola˜nos, “Clustering data streams based on
shared density between micro-clusters,” IEEE TKDE, vol. 28,
no. 6, pp. 1449–1461, 2016.
[148] K. M. Ting, B.-C. Xu, T. Washio, and Z.-H. Zhou, “Isolation
distributional kernel a new tool for point & group anomaly
detection,” IEEE TKDE, 2021.
[149] A. Garg, W. Zhang, J. Samaran, R. Savitha, and C.-S. Foo, “An
evaluation of anomaly detection and diagnosis in multivariate
time series,” IEEE TNNLS, 2021.
[150] D. Park, Y. Hoshi, and C. C. Kemp, “A multimodal anomaly de-
tector for robot-assisted feeding using an lstm-based variational
autoencoder,” IEEE Robot, vol. 3, no. 3, pp. 1544–1551, 2018.
[151] K. Hundman, V. Constantinou, C. Laporte, I. Colwell, and
T. S¨oderstr¨om, “Detecting spacecraft anomalies using lstms and
nonparametric dynamic thresholding,” in KDD, 2018, pp. 387–
395.
[152] Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, “Robust
anomaly detection for multivariate time series through stochastic
recurrent neural network,” in KDD, 2019, pp. 2828–2837.
[153] J. Xu, H. Wu, J. Wang, and M. Long, “Anomaly transformer: Time
series anomaly detection with association discrepancy,” in ICLR,
2022.
[154] Z. Chen, D. Chen, X. Zhang, Z. Yuan, and X. Cheng, “Learning
graph structures with transformer for multivariate time-series
anomaly detection in iot,” IEEE IoT Journal, vol. 9, no. 12, pp.
9179–9189, 2021.
[155] W. Hu, Y. Yang, Z. Cheng, C. Yang, and X. Ren, “Time-series
event prediction with evolutionary state graph,” in WSDM, 2021,
pp. 580–588.
[156] Y. Wu, M. Gu, L. Wang, Y. Lin, F. Wang, and H. Yang,
“Event2graph:
Event-driven
bipartite
graph
for
multivari-
ate
time-series
anomaly
detection,”
arXiv
preprint,
vol.
abs/2108.06783, 2021.
[157] W. Zhang, C. Zhang, and F. Tsung, “Grelen: Multivariate time
series anomaly detection from the perspective of graph relational
learning,” in IJCAI, 2022, pp. 2390–2397.
[158] W. Chen, L. Tian, B. Chen, L. Dai, Z. Duan, and M. Zhou,
“Deep variational graph convolutional recurrent network for
multivariate time series anomaly detection,” in ICML, vol. 162,
2022, pp. 3621–3633.
[159] S. Guan, B. Zhao, Z. Dong, M. Gao, and Z. He, “Gtad: Graph and
temporal neural network for multivariate time series anomaly
detection,” Entropy, vol. 24, no. 6, p. 759, 2022.
[160] S. S. Srinivas, R. K. Sarkar, and V. Runkana, “Hypergraph learn-
ing based recommender system for anomaly detection, control
and optimization,” in Big Data, 2022, pp. 1922–1929.
[161] L. Zhou, Q. Zeng, and B. Li, “Hybrid anomaly detection via
multihead dynamic graph attention networks for multivariate
time series,” IEEE Access, vol. 10, pp. 40 967–40 978, 2022.
[162] K. Chen, M. Feng, and T. S. Wirjanto, “Multivariate time se-
ries anomaly detection via dynamic graph forecasting,” arXiv
preprint, vol. abs/2302.02051, 2023.
[163] W. Chen, Z. Zhou, Q. Wen, and L. Sun, “Time series subsequence
anomaly detection via graph neural networks,” 2023.
[164] M. Jin, Y. Liu, Y. Zheng, L. Chi, Y.-F. Li, and S. Pan, “Anemone:
Graph anomaly detection with multi-scale contrastive learning,”
in CIKM, 2021, pp. 3122–3126.
[165] Y. Zheng, M. Jin, Y. Liu, L. Chi, K. T. Phan, and Y.-P. P. Chen,
“Generative and contrastive self-supervised learning for graph
anomaly detection,” IEEE TKDE, 2021.
[166] T. K. K. Ho, A. Karami, and N. Armanfard, “Graph-based
time-series anomaly detection: A survey,” arXiv preprint, vol.
abs/2302.00058, 2023.
[167] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning.
MIT
press, 2016.
[168] M. Ranzato, Y. Boureau, and Y. LeCun, “Sparse feature learning
for deep belief networks,” in NeurIPS, 2007, pp. 1185–1192.
[169] D. P. Kingma, M. Welling et al., “An introduction to variational
autoencoders,” Found. Trends Mach. Learn., vol. 12, no. 4, pp. 307–
392, 2019.
[170] Z. Li, Y. Zhao, J. Han, Y. Su, R. Jiao, X. Wen, and D. Pei,
“Multivariate time series anomaly detection and interpretation
using hierarchical inter-metric and temporal embedding,” in
KDD, 2021, pp. 3220–3230.
[171] D. Zambon, C. Alippi, and L. Livi, “Change-point methods on a
sequence of graphs,” IEEE TSP, 2019.
[172] D. Zambon, L. Livi, and C. Alippi, “Detecting changes in se-
quences of attributed graphs,” in IEEE SSCI, 2017.
[173] ——, “Anomaly and change detection in graph streams through
constant-curvature manifold embeddings,” in IJCNN, 2018.
[174] D. Zambon and C. Alippi, “AZ-whiteness test: A test for signal
uncorrelation on spatio-temporal graphs,” in NeurIPS, 2022.
[175] Z. Duan, H. Xu, Y. Wang, Y. Huang, A. Ren, Z. Xu, Y. Sun, and
W. Wang, “Multivariate time-series classification with hierarchi-
cal variational graph pooling,” Neural Networks, vol. 154, pp. 481–
490, 2022.
[176] H. Liu, X. Liu, D. Yang, Z. Liang, H. Wang, Y. Cui, and J. Gu, “To-
dynet: Temporal dynamic graph neural network for multivariate
time series classification,” arXiv preprint, vol. abs/2304.05078,
2023.
[177] N. M. Foumani, L. Miller, C. W. Tan, G. I. Webb, G. Forestier,
and M. Salehi, “Deep learning for time series classification
and extrinsic regression: A current survey,” arXiv preprint, vol.
abs/2302.02515, 2023.
[178] J. Lines and A. Bagnall, “Time series classification with ensembles
of elastic distance measures,” DMKD, vol. 29, pp. 565–592, 2015.
[179] C. W. Tan, F. Petitjean, and G. I. Webb, “Fastee: Fast ensembles
of elastic distances for time series classification,” DMKD, vol. 34,
no. 1, pp. 231–272, 2020.
[180] M. Herrmann and G. I. Webb, “Amercing: An intuitive, ele-
gant and effective constraint for dynamic time warping,” arXiv
preprint, vol. abs/2111.13314, 2021.
[181] J. Lines, S. Taylor, and A. Bagnall, “Time series classification
with hive-cote: The hierarchical vote collective of transformation-
based ensembles,” ACM TKDD, vol. 12, no. 5, 2018.
[182] M. Middlehurst, J. Large, M. Flynn, J. Lines, A. Bostrom, and
A. Bagnall, “Hive-cote 2.0: a new meta ensemble for time series
classification,” Machine Learning, vol. 110, no. 11-12, pp. 3211–
3243, 2021.
[183] A. Dempster, F. Petitjean, and G. I. Webb, “Rocket: exceptionally
fast and accurate time series classification using random convo-
lutional kernels,” DMKD, vol. 34, no. 5, pp. 1454–1495, 2020.
[184] C. W. Tan, A. Dempster, C. Bergmeir, and G. I. Webb, “Multi-
rocket: multiple pooling operators and transformations for fast
and effective time series classification,” DMKD, vol. 36, no. 5, pp.
1623–1646, 2022.
[185] X. Zhang, Y. Gao, J. Lin, and C. Lu, “Tapnet: Multivariate time
series classification with attentional prototypical network,” in
AAAI, 2020, pp. 6845–6852.
[186] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eick-
hoff, “A transformer-based framework for multivariate time se-
ries representation learning,” in KDD, 2021, pp. 2114–2124.
[187] Z. Cheng, Y. Yang, W. Wang, W. Hu, Y. Zhuang, and G. Song,
“Time2graph: Revisiting time series modeling with dynamic
shapelets,” in AAAI, 2020, pp. 3617–3624.
[188] E. Keogh and C. A. Ratanamahatana, “Exact indexing of dynamic
time warping,” KAIS, vol. 7, pp. 358–386, 2005.
[189] S. Tang, J. Dunnmon, K. K. Saab, X. Zhang, Q. Huang, F. Dubost,
D. Rubin, and C. Lee-Messer, “Self-supervised graph neural net-
works for improved electroencephalographic seizure analysis,”
in ICLR, 2022.
[190] Y. Wu, D. Zhuang, A. Labbe, and L. Sun, “Inductive graph neural
networks for spatiotemporal kriging,” in AAAI, 2021, pp. 4478–
4485.
[191] Y. Ye, S. Zhang, and J. J. Yu, “Spatial-temporal traffic data im-
putation via graph attention convolutional network,” in ICANN,
2021, pp. 241–252.
[192] Y. Wu, D. Zhuang, M. Lei, A. Labbe, and L. Sun, “Spatial aggre-
gation and temporal convolution networks for real-time kriging,”
arXiv preprint, vol. abs/2109.12144, 2021.
[193] I. Marisca, A. Cini, and C. Alippi, “Learning to reconstruct miss-
ing data from spatiotemporal graphs with sparse observations,”
in NeurIPS, 2022.
[194] A. Roth and T. Liebig, “Forecasting unobserved node states
with spatio-temporal graph neural networks,” arXiv preprint, vol.
abs/2211.11596, 2022.
[195] Y. Chen, Z. Li, C. Yang, X. Wang, G. Long, and G. Xu, “Adaptive
graph recurrent network for multivariate time series imputa-
tion,” in ICONIP, 2023, pp. 64–73.
[196] X. Wu, M. Xu, J. Fang, and X. Wu, “A multi-attention tensor
completion network for spatiotemporal traffic data imputation,”
IEEE IoT Journal, vol. 9, no. 20, pp. 20 203–20 213, 2022.
[197] X. Kong, W. Zhou, G. Shen, W. Zhang, N. Liu, and Y. Yang,
“Dynamic graph convolutional recurrent imputation network for
spatiotemporal traffic missing data,” KBS, vol. 261, p. 110188,
2023.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
25
[198] G. Shen, W. Zhou, W. Zhang, N. Liu, Z. Liu, and X. Kong,
“Bidirectional spatial-temporal traffic data imputation via graph
attention recurrent neural network,” Neurocomputing, vol. 531,
pp. 151–162, 2023.
[199] Y. Liang, Z. Zhao, and L. Sun, “Memory-augmented dynamic
graph convolution networks for traffic data imputation with
diverse missing patterns,” Transp. Res. Part C Emerg., vol. 143,
p. 103826, 2022.
[200] S. Moritz and T. Bartz-Beielstein, “imputets: time series missing
value imputation in r.” R J., vol. 9, no. 1, p. 207, 2017.
[201] M. Saad, M. Chaudhary, F. Karray, and V. Gaudet, “Machine
learning based approaches for imputation in time series data and
their impact on forecasting,” in SMC, 2020, pp. 2621–2627.
[202] Z. Che, S. Purushotham, K. Cho, D. Sontag, and Y. Liu, “Recur-
rent neural networks for multivariate time series with missing
values,” Scientific reports, vol. 8, no. 1, p. 6085, 2018.
[203] J. Yoon, J. Jordon, and M. van der Schaar, “GAIN: missing data
imputation using generative adversarial nets,” in ICML, ser.
Proceedings of Machine Learning Research, vol. 80, 2018, pp.
5675–5684.
[204] X. Miao, Y. Wu, J. Wang, Y. Gao, X. Mao, and J. Yin, “Generative
semi-supervised learning for multivariate time series imputa-
tion,” in AAAI, 2021, pp. 8983–8991.
[205] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”
Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.
[206] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic
models,” in NeurIPS, 2020.
[207] H. Li, Z. Lv, J. Li, Z. Xu, Y. Wang, H. Sun, and Z. Sheng, “Traffic
flow forecasting in the covid-19: A deep spatial-temporal model
based on discrete wavelet transformation,” ACM TKDD, vol. 17,
no. 5, pp. 1–28, 2023.
[208] Y. Liang, Z. Zhao, and L. Sun, “Dynamic spatiotemporal graph
convolutional neural networks for traffic data imputation with
complex missing patterns,” arXiv preprint, vol. abs/2109.08357,
2021.
[209] L. Tang, F. Yan, B. Zou, W. Li, C. Lv, and K. Wang, “Trajectory
prediction for autonomous driving based on multiscale spatial-
temporal graph,” IET Intell. Transp. Syst., vol. 17, no. 2, pp. 386–
399, 2023.
[210] X. Mo and C. Lv, “Predictive neural motion planner for au-
tonomous driving using graph networks,” IEEE TIV, 2023.
[211] L. Wang, Z. Song, X. Zhang, C. Wang, G. Zhang, L. Zhu, J. Li,
and H. Liu, “Sat-gcn: Self-attention graph convolutional network-
based 3d object detection for autonomous driving,” KBS, vol. 259,
p. 110080, 2023.
[212] K. Cai, Y. Li, Y.-P. Fang, and Y. Zhu, “A deep learning approach
for flight delay prediction through time-evolving graphs,” IEEE
TITS, vol. 23, no. 8, pp. 11 397–11 407, 2021.
[213] Z. Guo, G. Mei, S. Liu, L. Pan, L. Bian, H. Tang, and D. Wang,
“Sgdan—a spatio-temporal graph dual-attention neural network
for quantified flight delay prediction,” Sensors, vol. 20, no. 22, p.
6433, 2020.
[214] H. Yao, F. Wu, J. Ke, X. Tang, Y. Jia, S. Lu, P. Gong, J. Ye,
and Z. Li, “Deep multi-view spatial-temporal network for taxi
demand prediction,” in AAAI, 2018, pp. 2588–2595.
[215] M. Wu, C. Zhu, and L. Chen, “Multi-task spatial-temporal graph
attention network for taxi demand prediction,” in ICMAI, 2020,
pp. 224–228.
[216] L. Xu, L. Xia, and S. Pan, “Multi-attribute spatial-temporal graph
convolutional network for taxi demand forecasting,” in ICBDT,
2022, pp. 62–68.
[217] L. Bai, L. Yao, S. S. Kanhere, X. Wang, W. Liu, and Z. Yang,
“Spatio-temporal graph convolutional and recurrent networks
for citywide passenger demand prediction,” in CIKM, 2019, pp.
2293–2296.
[218] J. Tang, J. Liang, F. Liu, J. Hao, and Y. Wang, “Multi-community
passenger demand prediction at region level based on spatio-
temporal graph convolutional network,” Transp. Res. Part C
Emerg., vol. 124, p. 102951, 2021.
[219] T. S. Kim, W. K. Lee, and S. Y. Sohn, “Graph convolutional
network approach applied to predict hourly bike-sharing de-
mands considering spatial, temporal, and global effects,” PloS
one, vol. 14, no. 9, p. e0220782, 2019.
[220] Z. Chen, H. Wu, N. E. O’Connor, and M. Liu, “A comparative
study of using spatial-temporal graph convolutional networks
for predicting availability in bike sharing schemes,” in ITSC,
2021, pp. 1299–1305.
[221] G. Xiao, R. Wang, C. Zhang, and A. Ni, “Demand prediction
for a public bike sharing program based on spatio-temporal
graph convolutional networks,” Multimed. Tools Appl., vol. 80, pp.
22 907–22 925, 2021.
[222] X. Ma, Y. Yin, Y. Jin, M. He, and M. Zhu, “Short-term prediction of
bike-sharing demand using multi-source data: a spatial-temporal
graph attentional lstm approach,” Appl. Sci., vol. 12, no. 3, p. 1161,
2022.
[223] G. Li, X. Wang, G. S. Njoo, S. Zhong, S.-H. G. Chan, C.-C. Hung,
and W.-C. Peng, “A data-driven spatial-temporal graph neural
network for docked bike prediction,” in ICDE, 2022, pp. 713–726.
[224] W. Lin and D. W. 0044, “Residential electric load forecasting via
attentive transfer of graph neural networks.” in IJCAI, 2021, pp.
2716–2722.
[225] B. Zhou, Y. Dong, G. Yang, F. Hou, Z. Hu, S. Xu, and S. Ma, “A
graph-attention based spatial-temporal learning framework for
tourism demand forecasting,” KBS, p. 110275, 2023.
[226] T. Zhao, Z. Huang, W. Tu, B. He, R. Cao, J. Cao, and M. Li,
“Coupling graph deep learning and spatial-temporal influence of
built environment for short-term bus travel demand prediction,”
Comput. Environ. Urban Syst., vol. 94, p. 101776, 2022.
[227] J. Liang, J. Tang, F. Gao, Z. Wang, and H. Huang, “On region-
level travel demand forecasting using multi-task adaptive graph
attention network,” Inf. Sci., vol. 622, pp. 161–177, 2023.
[228] H. Wen, Y. Lin, X. Mao, F. Wu, Y. Zhao, H. Wang, J. Zheng, L. Wu,
H. Hu, and H. Wan, “Graph2route: A dynamic spatial-temporal
graph neural network for pick-up and delivery route prediction,”
in KDD, 2022, pp. 4143–4152.
[229] M. Khodayar and J. Wang, “Spatio-temporal graph deep neu-
ral network for short-term wind speed forecasting,” IEEE TSE,
vol. 10, no. 2, pp. 670–681, 2018.
[230] Q. Wu, H. Zheng, X. Guo, and G. Liu, “Promoting wind energy
for sustainable development by precise wind speed prediction
based on graph neural networks,” Renewable Energy, vol. 199, pp.
977–992, 2022.
[231] X. Pan, L. Wang, Z. Wang, and C. Huang, “Short-term wind
speed forecasting based on spatial-temporal graph transformer
networks,” Energy, vol. 253, p. 124095, 2022.
[232] H. Fan, X. Zhang, S. Mei, K. Chen, and X. Chen, “M2gsnet: Multi-
modal multi-task graph spatiotemporal network for ultra-short-
term wind farm cluster power prediction,” Appl. Sci., vol. 10,
no. 21, p. 7915, 2020.
[233] M. Yu, Z. Zhang, X. Li, J. Yu, J. Gao, Z. Liu, B. You, X. Zheng, and
R. Yu, “Superposition graph neural network for offshore wind
power prediction,” FGCS, vol. 113, pp. 145–157, 2020.
[234] H. Li, “Short-term wind power prediction via spatial temporal
analysis and deep residual networks,” Front. Energy Res., vol. 10,
p. 662, 2022.
[235] Y. He, S. Chai, J. Zhao, Y. Sun, and X. Zhang, “A robust spatio-
temporal prediction approach for wind power generation based
on spectral temporal graph neural network,” IET Renew. Power
Gener., vol. 16, no. 12, pp. 2556–2565, 2022.
[236] X. Jiao, X. Li, D. Lin, and W. Xiao, “A graph neural network
based deep learning predictor for spatio-temporal group solar
irradiance forecasting,” IEEE TII, vol. 18, no. 9, pp. 6142–6149,
2021.
[237] Y. Gao, S. Miyata, and Y. Akashi, “Interpretable deep learning
models for hourly solar radiation prediction based on graph
neural network and attention,” Appl. Energy, vol. 321, p. 119288,
2022.
[238] J. Simeunovi´c, B. Schubnel, P.-J. Alet, and R. E. Carrillo, “Spatio-
temporal graph neural networks for multi-site pv power forecast-
ing,” IEEE TSE, vol. 13, no. 2, pp. 1210–1220, 2021.
[239] A. M. Karimi, Y. Wu, M. Koyut¨urk, and R. H. French, “Spa-
tiotemporal graph neural network for performance prediction of
photovoltaic power systems,” in AAAI, 2021, pp. 15 323–15 330.
[240] M. Zhang, Z. Zhen, N. Liu, H. Zhao, Y. Sun, C. Feng, and F. Wang,
“Optimal graph structure based short-term solar pv power fore-
casting method considering surrounding spatio-temporal corre-
lations,” IEEE TIA, 2022.
[241] J. Liu, X. Wang, F. Xie, S. Wu, and D. Li, “Condition monitoring of
wind turbines with the implementation of spatio-temporal graph
neural network,” Eng. Appl. Artif. Intell., vol. 121, p. 106000, 2023.
[242] J. Van Gompel, D. Spina, and C. Develder, “Cost-effective fault
diagnosis of nearby photovoltaic systems using graph neural
networks,” Energy, vol. 266, p. 126444, 2023.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
26
[243] J. Tan, H. Liu, Y. Li, S. Yin, and C. Yu, “A new ensemble spatio-
temporal pm2. 5 prediction method based on graph attention
recursive networks and reinforcement learning,” Chaos, Solitons
& Fractals, vol. 162, p. 112405, 2022.
[244] V.
Oliveira
Santos,
P.
A.
Costa
Rocha,
J.
Scott,
J.
Van
Griensven Th´e, and B. Gharabaghi, “Spatiotemporal air pollution
forecasting in houston-tx: a case study for ozone using deep
graph neural networks,” Atmosphere, vol. 14, no. 2, p. 308, 2023.
[245] G. Singh, S. Durbha et al., “Maximising weather forecasting
accuracy through the utilisation of graph neural networks and
dynamic gnns,” arXiv preprint, vol. abs/2301.12471, 2023.
[246] S. Madakam, V. Lake, V. Lake, V. Lake et al., “Internet of things
(iot): A literature review,” J. comput. commun., vol. 3, no. 05, p.
164, 2015.
[247] D. Lee, Y. Gu, J. Hoang, and M. Marchetti-Bowick, “Joint inter-
action and trajectory prediction for autonomous driving using
graph neural networks,” arXiv preprint, vol. abs/1912.07882, 2019.
[248] P. Cai, H. Wang, Y. Sun, and M. Liu, “Dignet: Learning scalable
self-driving policies for generic traffic scenarios with graph neu-
ral networks,” in IROS, 2021, pp. 8979–8984.
[249] K. Li, S. Eiffert, M. Shan, F. Gomez-Donoso, S. Worrall, and
E. Nebot, “Attentional-gcnn: Adaptive pedestrian trajectory pre-
diction towards generic autonomous vehicle use cases,” in ICRA,
2021, pp. 14 241–14 247.
[250] W. Zhang, H. Liu, Y. Liu, J. Zhou, and H. Xiong, “Semi-
supervised hierarchical recurrent graph neural network for city-
wide parking availability prediction,” in AAAI, 2020, pp. 1186–
1193.
[251] M. Li, S. Chen, Y. Zhao, Y. Zhang, Y. Wang, and Q. Tian, “Mul-
tiscale spatio-temporal graph neural networks for 3d skeleton-
based motion prediction,” IEEE TIP, vol. 30, pp. 7760–7775, 2021.
[252] M. St¨ockl, T. Seidl, D. Marley, and P. Power, “Making offensive
play predictable-using a graph convolutional network to under-
stand defensive performance in soccer,” in Proceedings of the 15th
MIT Sloan Sports Analytics Conference, vol. 2022, 2021.
[253] G. Anzer, P. Bauer, U. Brefeld, and D. Faßmeyer, “Detection of
tactical patterns using semi-supervised graph neural networks,”
in SSAC, vol. 16, 2022, pp. 1–3.
[254] R. Luo and V. Krishnamurthy, “Who you play affects how
you play: Predicting sports performance using graph atten-
tion networks with temporal convolution,” arXiv preprint, vol.
abs/2303.16741, 2023.
[255] Y. Li, B. Qian, X. Zhang, and H. Liu, “Graph neural network-
based diagnosis prediction,” Big Data, vol. 8, no. 5, pp. 379–390,
2020.
[256] S. Liu, T. Li, H. Ding, B. Tang, X. Wang, Q. Chen, J. Yan, and
Y. Zhou, “A hybrid method of recurrent neural network and
graph neural network for next-period prescription prediction,”
IJMLC, vol. 11, pp. 2849–2856, 2020.
[257] C. Su, S. Gao, and S. Li, “Gate: graph-attention augmented
temporal neural network for medication recommendation,” IEEE
Access, vol. 8, pp. 125 447–125 458, 2020.
[258] Y. Li, B. Qian, X. Zhang, and H. Liu, “Knowledge guided diag-
nosis prediction via graph spatial-temporal network,” in SDM,
2020, pp. 19–27.
[259] J. Han, Y. He, J. Liu, Q. Zhang, and X. Jing, “Graphconvlstm:
spatiotemporal learning for activity recognition with wearable
sensors,” in GLOBECOM, 2019, pp. 1–6.
[260] X. Shi, H. Su, F. Xing, Y. Liang, G. Qu, and L. Yang, “Graph
temporal ensembling based semi-supervised convolutional neu-
ral network with noisy labels for histopathology image analysis,”
MedIA, vol. 60, p. 101624, 2020.
[261] Z. Jia, Y. Lin, J. Wang, X. Ning, Y. He, R. Zhou, Y. Zhou, and
H. L. Li-wei, “Multi-view spatial-temporal graph convolutional
networks with domain generalization for sleep stage classifica-
tion,” IEEE TNSRE, vol. 29, pp. 1977–1986, 2021.
[262] S. Parisot, S. I. Ktena, E. Ferrante, M. Lee, R. Guerrero, B. Glocker,
and D. Rueckert, “Disease prediction using graph convolu-
tional networks: application to autism spectrum disorder and
alzheimer’s disease,” MedIA, vol. 48, pp. 117–130, 2018.
[263] Y. Zhang and P. Bellec, “Functional annotation of human cogni-
tive states using graph convolution networks,” in Real Neurons
{\&} Hidden Units: Future directions at the intersection of neuro-
science and artificial intelligence @ NeurIPS 2019, 2019.
[264] M. Kim, J. Kim, J. Qu, H. Huang, Q. Long, K.-A. Sohn, D. Kim,
and L. Shen, “Interpretable temporal graph neural network for
prognostic prediction of alzheimer’s disease using longitudinal
neuroimaging data,” in BIBM, 2021, pp. 1381–1384.
[265] Z. Kong, X. Jin, Z. Xu, and B. Zhang, “Spatio-temporal fusion
attention: A novel approach for remaining useful life prediction
based on graph neural network,” IEEE TIM, vol. 71, pp. 1–12,
2022.
[266] Z. Wang, T. Xia, R. Jiang, X. Liu, K.-S. Kim, X. Song, and
R. Shibasaki, “Forecasting ambulance demand with profiled hu-
man mobility via heterogeneous multi-graph neural networks,”
in ICDE.
IEEE, 2021, pp. 1751–1762.
[267] T. S. Hy, V. B. Nguyen, L. Tran-Thanh, and R. Kondor, “Temporal
multiresolution graph neural networks for epidemic prediction,”
in Workshop on Healthcare AI and COVID-19, 2022, pp. 21–32.
[268] D. T. Wolfe and D. R. Hermanson, “The fraud diamond: Consid-
ering the four elements of fraud,” The CPA Journal, vol. 74, no. 12,
p. 38, 2004.
[269] Z. Li, P. Hui, P. Zhang, J. Huang, B. Wang, L. Tian, J. Zhang,
J. Gao, and X. Tang, “What happens behind the scene? towards
fraud community detection in e-commerce from online to of-
fline,” in WWW, 2021, pp. 105–113.
[270] J.-P. Chen, P. Lu, F. Yang, R. Chen, and K. Lin, “Medical insur-
ance fraud detection using graph neural networks with spatio-
temporal constraints,” JNI, vol. 7, no. 2, 2022.
[271] N. Noorshams, S. Verma, and A. Hofleitner, “TIES: temporal
interaction embeddings for enhancing social media integrity at
facebook,” in KDD, 2020, pp. 3128–3135.
[272] T. Zhao, B. Ni, W. Yu, and M. Jiang, “Early anomaly detec-
tion by learning and forecasting behavior,” arXiv preprint, vol.
abs/2010.10016, 2020.
[273] D. Huang, J. Bartel, and J. Palowitch, “Recurrent graph neural
networks for rumor detection in online forums,” arXiv preprint,
vol. abs/2108.03548, 2021.
[274] D. Cheng, X. Wang, Y. Zhang, and L. Zhang, “Graph neural
network for fraud detection via spatial-temporal attention,” IEEE
TKDE, vol. 34, no. 8, pp. 3800–3813, 2020.
[275] Y. Li, S. Xie, X. Liu, Q. F. Ying, W. C. Lau, D. M. Chiu, S. Z.
Chen et al., “Temporal graph representation learning for detecting
anomalies in e-payment systems,” in ICDMW, 2021, pp. 983–990.
[276] D. Wang, Z. Zhang, J. Zhou, P. Cui, J. Fang, Q. Jia, Y. Fang,
and Y. Qi, “Temporal-aware graph neural network for credit risk
prediction,” in SDM, 2021, pp. 702–710.
[277] S. Reddy, P. Poduval, A. V. S. Chauhan, M. Singh, S. Verma,
K. Singh, and T. Bhowmik, “Tegraf: temporal and graph based
fraudulent transaction detection framework,” in ICAIF, 2021, pp.
1–8.
[278] G. Chu, J. Wang, Q. Qi, H. Sun, S. Tao, H. Yang, J. Liao, and
Z. Han, “Exploiting spatial-temporal behavior patterns for fraud
detection in telecom networks,” IEEE TDSC, 2022.
[279] Y. Jin, X. Wang, R. Yang, Y. Sun, W. Wang, H. Liao, and X. Xie,
“Towards fine-grained reasoning for fake news detection,” in
AAAI, 2022, pp. 5746–5754.
[280] M. Lu, Z. Han, S. X. Rao, Z. Zhang, Y. Zhao, Y. Shan, R. Raghu-
nathan, C. Zhang, and J. Jiang, “Bright-graph neural networks in
real-time fraud detection,” in CIKM, 2022, pp. 3342–3351.
[281] Z. Xu, Q. Sun, S. Hu, J. Qiu, C. Lin, and H. Li, “Multi-view het-
erogeneous temporal graph neural network for “click farming”
detection,” in PRICAI.
Springer, 2022, pp. 148–160.
[282] J. WANG, S. ZHANG, Y. XIAO, and R. SONG, “A review on
graph neural network methods in financial applications.” Journal
of Data Science, vol. 20, no. 2, 2022.
[283] X. Xiao, Z. Jin, Y. Hui, Y. Xu, and W. Shao, “Hybrid spatial–
temporal graph convolutional networks for on-street parking
availability prediction,” Remote Sensing, vol. 13, no. 16, p. 3338,
2021.
[284] M. Hou, F. Xia, H. Gao, X. Chen, and H. Chen, “Urban region
profiling with spatio-temporal graph neural networks,” IEEE
TCSS, vol. 9, no. 6, pp. 1736–1747, 2022.
[285] A. Kapoor, X. Ben, L. Liu, B. Perozzi, M. Barnes, M. Blais,
and
S.
O’Banion,
“Examining
covid-19
forecasting
using
spatio-temporal graph neural networks,” arXiv preprint, vol.
abs/2007.03113, 2020.
[286] S. Yu, F. Xia, S. Li, M. Hou, and Q. Z. Sheng, “Spatio-temporal
graph learning for epidemic prediction,” ACM TIST, 2023.
[287] R. Geng, Y. Gao, H. Zhang, and J. Zu, “Analysis of the spatio-
temporal dynamics of covid-19 in massachusetts via spectral
graph wavelet theory,” IEEE TSIPN, vol. 8, pp. 670–683, 2022.
[288] G. Shi, D. Zhang, M. Jin, and S. Pan, “Towards complex dy-
namic physics system simulation with graph neural odes,” arXiv
preprint, vol. abs/2305.12334, 2023.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
27
[289] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu, “Unifying
large language models and knowledge graphs: A roadmap,”
arXiv preprint, vol. abs/2306.08302, 2023.
[290] G. Panagopoulos, G. Nikolentzos, and M. Vazirgiannis, “Transfer
graph neural networks for pandemic forecasting,” in AAAI, 2021,
pp. 4838–4845.
[291] X. Wang, G. Chen, G. Qian, P. Gao, X.-Y. Wei, Y. Wang, Y. Tian,
and W. Gao, “Large-scale multi-modal pre-trained models: A
comprehensive survey,” arXiv preprint, vol. abs/2302.10035, 2023.
[292] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language
models,” arXiv preprint, vol. abs/2303.18223, 2023.
[293] H. Zhang, B. Wu, X. Yuan, S. Pan, H. Tong, and J. Pei, “Trustwor-
thy graph neural networks: Aspects, methods and trends,” arXiv
preprint, vol. abs/2205.07424, 2022.
[294] S. J. Moore, C. D. Nugent, S. Zhang, and I. Cleland, “Iot relia-
bility: a review leading to 5 key research directions,” CCF TPCI,
vol. 2, pp. 147–163, 2020.
[295] D. Luo, W. Cheng, D. Xu, W. Yu, B. Zong, H. Chen, and X. Zhang,
“Parameterized explainer for graph neural network,” in NeurIPS,
2020.
[296] C. Abrate and F. Bonchi, “Counterfactual graphs for explainable
classification of brain networks,” in KDD, 2021, pp. 2495–2504.
[297] S. Yang, Z. Zhang, J. Zhou, Y. Wang, W. Sun, X. Zhong, Y. Fang,
Q. Yu, and Y. Qi, “Financial risk analysis for smes with graph-
based supply chain mining,” in IJCAI, 2020, pp. 4661–4667.
[298] A. D. Richardson, M. Aubinet, A. G. Barr, D. Y. Hollinger,
A. Ibrom, G. Lasslop, and M. Reichstein, “Uncertainty quantifi-
cation,” Eddy covariance: A practical guide to measurement and data
analysis, pp. 173–209, 2012.
[299] E. H¨ullermeier and W. Waegeman, “Aleatoric and epistemic
uncertainty in machine learning: An introduction to concepts and
methods,” Machine Learning, vol. 110, pp. 457–506, 2021.
[300] M. Abdar, F. Pourpanah, S. Hussain, D. Rezazadegan, L. Liu,
M. Ghavamzadeh, P. Fieguth, X. Cao, A. Khosravi, U. R. Acharya
et al., “A review of uncertainty quantification in deep learn-
ing: Techniques, applications and challenges,” Information Fusion,
vol. 76, pp. 243–297, 2021.
[301] E. Zheleva and L. Getoor, “Preserving the privacy of sensitive
relationships in graph data,” in Privacy, Security, and Trust in
KDD, 2008, pp. 153–171.
[302] J. Xu, M. Xue, and S. Picek, “Explainability-based backdoor
attacks against graph neural networks,” in Proceedings of the 3rd
ACM Workshop on Wireless Security and Machine Learning, 2021,
pp. 31–36.
[303] L. Wu, P. Cui, J. Pei, L. Zhao, and L. Song, Graph neural networks.
Springer, 2022.
[304] W. L. Hamilton, Z. Ying, and J. Leskovec, “Inductive representa-
tion learning on large graphs,” in NeurIPS, 2017, pp. 1024–1034.
[305] J. Chen, J. Zhu, and L. Song, “Stochastic training of graph con-
volutional networks with variance reduction,” in ICML, vol. 80,
2018, pp. 941–949.
[306] J. Chen, T. Ma, and C. Xiao, “Fastgcn: Fast learning with graph
convolutional networks via importance sampling,” in ICLR, 2018.
[307] W. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C. Hsieh, “Cluster-
gcn: An efficient algorithm for training deep and large graph
convolutional networks,” in KDD, 2019, pp. 257–266.
[308] C. Guan, Z. Zhang, H. Li, H. Chang, Z. Zhang, Y. Qin, J. Jiang,
X. Wang, and W. Zhu, “Autogl: A library for automated graph
learning,” in ICLR 2021 Workshop on Geometrical and Topological
Representation Learning, 2021.
[309] X. Zheng, M. Zhang, C. Chen, Q. Zhang, C. Zhou, and S. Pan,
“Auto-heg: Automated graph neural network on heterophilic
graphs,” in WWW, 2023, pp. 611–620.
[310] E. Rapaport, O. Shriki, and R. Puzis, “Eegnas: Neural architecture
search for electroencephalography data analysis and decoding,”
in International Workshop on Human Brain and Artificial Intelligence,
2019, pp. 3–20.
[311] T. Li, J. Zhang, K. Bao, Y. Liang, Y. Li, and Y. Zheng, “Autost: Ef-
ficient neural architecture search for spatio-temporal prediction,”
in KDD, 2020, pp. 794–802.
[312] A. Bagnall, J. Lines, A. Bostrom, J. Large, and E. Keogh, “The
great time series classification bake off: a review and experimen-
tal evaluation of recent algorithmic advances,” DMKD, vol. 31,
pp. 606–660, 2017.
[313] A. Alsharef, K. Aggarwal, Sonia, M. Kumar, and A. Mishra,
“Review of ml and automl solutions to forecast time-series data,”
Arch. Comput. Methods Eng., vol. 29, no. 7, pp. 5297–5311, 2022.
[314] X. Wang, R. J. Hyndman, F. Li, and Y. Kang, “Forecast combina-
tions: an over 50-year review,” Int. J. Forecast., 2022.

