
Information Security and Cryptography
Texts and Monographs
Series Editor
Ueli Maurer
Associate Editors
Martin Abadi
Ross Anderson
Mihir Bellare
Oded Goldreich
Tatsuaki Okamoto
Paul van Oorschot
Birgit Pfitzmann
Aviel D. Rubin
Jacques Stern

Hans Delfs
Helmut Knebl
Introduction
to Cryptography
Principles and Applications
Second Edition

Authors
Series Editor
Library of Congress Control Number: 2007921676
ACM Computing Classification: E.3
ISBN-13
978-3-540-49243-6 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material
is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broad-
casting, reproduction on microfilm or in any other way, and storage in data banks. Duplication of
this publication or parts thereof is permitted only under the provisions of the German Copyright
Law of September 9, 1965, in its current version, and permission for use must always be obtained
from Springer. Violations are liable for prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
© Springer-Verlag Berlin Heidelberg 2007
The use of general descriptive names, registered names, trademarks, etc. in this publication does not
imply, even in the absence of a specific statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
Typesetting: Integra, India
Printed on acid-free paper
SPIN: 11929970
ISSN 1619-7100
Cover design: KünkelLopka, Heidelberg
45/3100/Integra
5 4 3 2 1 0
Prof. Dr. Hans Delfs
Georg-Simon-Ohm University
of Applied Sciences N¨urnberg
Department of Computer Science
Keßlerplatz 12
90489 N¨urnberg
Germany
Hans.Delfs@fh-nuernberg.de
Prof. Dr. Helmut Knebl
Georg-Simon-Ohm University
of Applied Sciences N¨urnberg
Department of Computer Science
Keßlerplatz 12
90489 N¨urnberg
Germany
Helmut.Knebl@fh-nuernberg.de
Prof. Dr. Ueli Maurer
Inst. f¨ur Theoretische Informatik
ETH Z¨urich, 8092 Z¨urich
Switzerland

Preface to the Second, Extended Edition
New topics have been included in the second edition. They reﬂect recent
progress in the ﬁeld of cryptography and supplement the material covered in
the ﬁrst edition. Major extensions and enhancements are the following.
• A complete description of the Advanced Encryption Standard AES is given
in Chapter 2 on symmetric encryption.
• In Appendix A, there is a new section on polynomials and ﬁnite ﬁelds.
There we oﬀer a basic explanation of ﬁnite ﬁelds, which is necessary to
understand the AES.
• The description of cryptographic hash functions in Chapter 3 has been
extended. It now also includes, for example, the HMAC construction of
message authentication codes.
• Bleichenbacher’s 1-Million-Chosen-Ciphertext Attack against schemes
that implement the RSA encryption standard PKCS#1 is discussed in
detail in Chapter 3. This attack proves that adaptively-chosen-ciphertext
attacks can be a real danger in practice.
• In Chapter 9 on provably secure encryption we have added typical secu-
rity proofs for public-key encryption schemes that resist adaptively-chosen-
ciphertext attacks. Two prominent examples are studied – Boneh’s simple-
OAEP, or SAEP for short, and Cramer-Shoup’s public key encryption.
• Security proofs in the random oracle model are now included. Full-domain-
hash RSA signatures and SAEP serve as examples.
Furthermore, the text has been updated and clariﬁed at various points.
Errors and inaccuracies have been corrected.
We thank our readers and our students for their comments and hints, and
we are indebted to our colleague Patricia Shiroma-Brockmann and Ronan
Nugent at Springer for proof-reading the English copy of the new and revised
chapters.
N¨urnberg, December 2006
Hans Delfs, Helmut Knebl

Preface
The rapid growth of electronic communication means that issues in infor-
mation security are of increasing practical importance. Messages exchanged
over worldwide publicly accessible computer networks must be kept conﬁden-
tial and protected against manipulation. Electronic business requires digital
signatures that are valid in law, and secure payment protocols. Modern cryp-
tography provides solutions to all these problems.
This book originates from courses given for students in computer science
at the Georg-Simon-Ohm University of Applied Sciences, N¨urnberg. It is in-
tended as a course on cryptography for advanced undergraduate and graduate
students in computer science, mathematics and electrical engineering.
In its ﬁrst part (Chapters 1–4), it covers – at an undergraduate level – the
key concepts from symmetric and asymmetric encryption, digital signatures
and cryptographic protocols, including, for example, identiﬁcation schemes,
electronic elections and digital cash. The focus is on asymmetric cryptography
and the underlying modular algebra. Since we avoid probability theory in
the ﬁrst part, we necessarily have to work with informal deﬁnitions of, for
example, one-way functions and collision-resistant hash functions.
It is the goal of the second part (Chapters 5–10) to show, using prob-
ability theory, how basic notions like the security of cryptographic schemes
and the one-way property of functions can be made precise, and which as-
sumptions guarantee the security of public-key cryptographic schemes such
as RSA. More advanced topics, like the bit security of one-way functions,
computationally perfect pseudorandom generators and the close relation be-
tween the randomness and security of cryptographic schemes, are addressed.
Typical examples of provably secure encryption and signature schemes and
their security proofs are given.
Though particular attention is given to the mathematical foundations
and, in the second part, precise deﬁnitions, no special background in math-
ematics is presumed. An introductory course typically taught for beginning
students in mathematics and computer science is suﬃcient. The reader should
be familiar with the elementary notions of algebra, such as groups, rings and
ﬁelds, and, in the second part, with the basics of probability theory. Appendix
A contains an exposition of the results from algebra and number theory nec-
essary for an understanding of the cryptographic methods. It includes proofs

VIII
Preface
and covers, for example, basics like Euclid’s algorithm and the Chinese Re-
mainder Theorem, but also more advanced material like Legendre and Jacobi
symbols and probabilistic prime number tests. The concepts and results from
probability and information theory that are applied in the second part of the
book are given in full in Appendix B. To keep the mathematics easy, we
do not address elliptic curve cryptography. We illustrate the key concepts of
public-key cryptography by the classical examples like RSA in the quotient
rings Zn of the integers Z.
The book starts with an introduction into classical symmetric encryption
in Chapter 2. The principles of public-key cryptography and their use for
encryption and digital signatures are discussed in detail in Chapter 3. The
famous and widely used RSA, ElGamal’s methods and the digital signature
standard, Rabin’s encryption and signature schemes serve as the outstand-
ing examples. The underlying one-way functions – modular exponentiation,
modular powers and modular squaring – are used throughout the book, also
in the second part.
Chapter 4 presents typical cryptographic protocols, including key ex-
change, identiﬁcation and commitment schemes, electronic cash and elec-
tronic elections.
The following chapters focus on a precise deﬁnition of the key concepts
and the security of public-key cryptography. Attacks are modeled by prob-
abilistic polynomial algorithms (Chapter 5). One-way functions as the basic
building blocks and the security assumptions underlying modern public-key
cryptography are studied in Chapter 6. In particular, the bit security of the
RSA function, the discrete logarithm and the Rabin function is analyzed in
detail (Chapter 7). The close relation between one-way functions and com-
putationally perfect pseudorandom generators meeting the needs of cryptog-
raphy is explained in Chapter 8. Provable security properties of encryption
schemes are the central topic of Chapter 9. It is clariﬁed that randomness is
the key to security. We start with the classical notions of provable security
originating from Shannon’s work on information theory. Typical examples
of more recent results on the security of public-key encryption schemes are
given, taking into account the computational complexity of attacking algo-
rithms. A short introduction to cryptosystems, whose security can be proven
by information-theoretic methods without any assumptions on the hardness
of computational problems (“unconditional security approach”), supplements
the section. Finally, we discuss in Chapter 10 the levels of security of dig-
ital signatures and give examples of signature schemes, whose security can
be proven solely under standard assumptions like the factoring assumption,
including a typical security proof.
Each chapter (except Chapter 1) closes with a collection of exercises.
Answers to the exercises are provided on the Web page for this book:
www.informatik.fh-nuernberg.de/DelfsKnebl/Cryptography.

Preface
IX
We thank our colleagues and students for pointing out errors and sug-
gesting improvements. In particular, we express our thanks to J¨org Schwenk,
Harald Stieber and Rainer Weber. We are grateful to Jimmy Upton for
his comments and suggestions, and we are very much indebted to Patricia
Shiroma-Brockmann for proof-reading the English copy. Finally, we would
like to thank Alfred Hofmann at Springer-Verlag for his support during the
writing and publication of this book.
N¨urnberg, December 2001
Hans Delfs, Helmut Knebl

Contents
1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Encryption and Secrecy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
The Objectives of Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4
Cryptographic Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.5
Provable Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.
Symmetric-Key Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1
Stream Ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.2
Block Ciphers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2.1
DES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.2.2
AES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2.3
Modes of Operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.
Public-Key Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3.1
The Concept of Public-Key Cryptography . . . . . . . . . . . . . . . . .
33
3.2
Modular Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2.1
The Integers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2.2
The Integers Modulo n . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3
RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.3.1
Key Generation and Encryption . . . . . . . . . . . . . . . . . . . .
41
3.3.2
Digital Signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.3.3
Attacks Against RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.3.4
Probabilistic RSA Encryption . . . . . . . . . . . . . . . . . . . . . .
51
3.4
Cryptographic Hash Functions . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.4.1
Security Requirements for Hash Functions . . . . . . . . . . .
54
3.4.2
Construction of Hash Functions . . . . . . . . . . . . . . . . . . . .
56
3.4.3
Data Integrity and Message Authentication . . . . . . . . . .
62
3.4.4
Hash Functions as Random Functions . . . . . . . . . . . . . . .
64
3.4.5
Signatures with Hash Functions . . . . . . . . . . . . . . . . . . . .
65
3.5
The Discrete Logarithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.5.1
ElGamal’s Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.5.2
ElGamal’s Signature Scheme . . . . . . . . . . . . . . . . . . . . . . .
72
3.5.3
Digital Signature Algorithm . . . . . . . . . . . . . . . . . . . . . . .
73

XII
Contents
3.6
Modular Squaring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.6.1
Rabin’s Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.6.2
Rabin’s Signature Scheme . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.
Cryptographic Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
4.1
Key Exchange and Entity Authentication . . . . . . . . . . . . . . . . . .
81
4.1.1
Kerberos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4.1.2
Diﬃe-Hellman Key Agreement . . . . . . . . . . . . . . . . . . . . .
85
4.1.3
Key Exchange and Mutual Authentication . . . . . . . . . . .
86
4.1.4
Station-to-Station Protocol . . . . . . . . . . . . . . . . . . . . . . . .
88
4.1.5
Public-Key Management Techniques . . . . . . . . . . . . . . . .
89
4.2
Identiﬁcation Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.2.1
Interactive Proof Systems . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.2.2
Simpliﬁed Fiat-Shamir Identiﬁcation Scheme . . . . . . . . .
93
4.2.3
Zero-Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
4.2.4
Fiat-Shamir Identiﬁcation Scheme . . . . . . . . . . . . . . . . . .
97
4.2.5
Fiat-Shamir Signature Scheme . . . . . . . . . . . . . . . . . . . . .
99
4.3
Commitment Schemes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.3.1
A Commitment Scheme Based on Quadratic Residues . 101
4.3.2
A Commitment Scheme Based on Discrete Logarithms 102
4.3.3
Homomorphic Commitments . . . . . . . . . . . . . . . . . . . . . . . 103
4.4
Electronic Elections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
4.4.1
Secret Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
4.4.2
A Multi-Authority Election Scheme . . . . . . . . . . . . . . . . . 107
4.4.3
Proofs of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
4.4.4
Non-Interactive Proofs of Knowledge. . . . . . . . . . . . . . . . 112
4.4.5
Extension to Multi-Way Elections . . . . . . . . . . . . . . . . . . 112
4.4.6
Eliminating the Trusted Center . . . . . . . . . . . . . . . . . . . . 113
4.5
Digital Cash . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
4.5.1
Blindly Issued Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
4.5.2
A Fair Electronic Cash System . . . . . . . . . . . . . . . . . . . . . 123
4.5.3
Underlying Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.
Probabilistic Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5.1
Coin-Tossing Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
5.2
Monte Carlo and Las Vegas Algorithms . . . . . . . . . . . . . . . . . . . 140
6.
One-Way Functions and the Basic Assumptions. . . . . . . . . . . 147
6.1
A Notation for Probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
6.2
Discrete Exponential Function . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
6.3
Uniform Sampling Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
6.4
Modular Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
6.5
Modular Squaring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
6.6
Quadratic Residuosity Property . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6.7
Formal Deﬁnition of One-Way Functions . . . . . . . . . . . . . . . . . . 163

Contents
XIII
6.8
Hard-Core Predicates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
7.
Bit Security of One-Way Functions . . . . . . . . . . . . . . . . . . . . . . . 175
7.1
Bit Security of the Exp Family . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
7.2
Bit Security of the RSA Family . . . . . . . . . . . . . . . . . . . . . . . . . . 182
7.3
Bit Security of the Square Family . . . . . . . . . . . . . . . . . . . . . . . . 190
8.
One-Way Functions and Pseudorandomness . . . . . . . . . . . . . . 199
8.1
Computationally Perfect Pseudorandom Bit Generators . . . . . 199
8.2
Yao’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
9.
Provably Secure Encryption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
9.1
Classical Information-Theoretic Security . . . . . . . . . . . . . . . . . . . 216
9.2
Perfect Secrecy and Probabilistic Attacks . . . . . . . . . . . . . . . . . . 220
9.3
Public-Key One-Time Pads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
9.4
Passive Eavesdroppers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
9.5
Chosen-Ciphertext Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
9.5.1
A Security Proof in the Random Oracle Model . . . . . . . 236
9.5.2
Security Under Standard Assumptions . . . . . . . . . . . . . . 245
9.6
Unconditional Security of Cryptosystems . . . . . . . . . . . . . . . . . . 250
9.6.1
The Bounded Storage Model . . . . . . . . . . . . . . . . . . . . . . . 251
9.6.2
The Noisy Channel Model . . . . . . . . . . . . . . . . . . . . . . . . . 260
10. Provably Secure Digital Signatures
. . . . . . . . . . . . . . . . . . . . . . 265
10.1 Attacks and Levels of Security . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
10.2 Claw-Free Pairs and Collision-Resistant Hash Functions . . . . . 268
10.3 Authentication-Tree-Based Signatures . . . . . . . . . . . . . . . . . . . . . 271
10.4 A State-Free Signature Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
A.
Algebra and Number Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
A.1 The Integers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
A.2 Residues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
A.3 The Chinese Remainder Theorem. . . . . . . . . . . . . . . . . . . . . . . . . 299
A.4 Primitive Roots and the Discrete Logarithm . . . . . . . . . . . . . . . 301
A.5 Polynomials and Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
A.5.1 The Ring of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . 305
A.5.2 Residue Class Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
A.5.3 Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
A.6 Quadratic Residues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
A.7 Modular Square Roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
A.8 Primes and Primality Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319

XIV
Contents
B.
Probabilities and Information Theory . . . . . . . . . . . . . . . . . . . . . 325
B.1 Finite Probability Spaces and Random Variables . . . . . . . . . . . 325
B.2 The Weak Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . . . . 333
B.3 Distance Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
B.4 Basic Concepts of Information Theory . . . . . . . . . . . . . . . . . . . . 340
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361

Notation
Page
M ∗
set of words m1m2 . . . ml, l ≥0, over M
{0, 1}∗
set of bit strings of arbitrary length
1k
constant bit string 11 . . . 1 of length k
157
a ⊕b
bitwise XOR of bit strings a, b ∈{0, 1}l
13
a||b
concatenation of strings a and b
N
set of natural numbers: {1, 2, . . .}
35
Z
set of integers
35
Q
set of rational numbers
R
set of real numbers
ln(x)
natural logarithm of a real x > 0
log(x)
base-10 logarithm of a real x > 0
log2(x)
base-2 logarithm of a real x > 0
logg(x)
discrete base-g logarithm of x ∈Z∗
p
a | b
a ∈Z divides b ∈Z
289
|x|
absolute value of x ∈R
|x|
length of a bit string x ∈{0, 1}∗
|x|
binary length of x ∈N
|M|
number of elements in a set M
296
g ◦f
composition of maps: g ◦f(x) = g(f(x))
idX
identity map: idX(x) = x for all x ∈X
f −1
inverse of a bijective map f
x−1
inverse of a unit x in a ring
296
Zn
residue class ring modulo n
295
Z∗
n
units in Zn
296
a div n
integer quotient of a and n
290
a mod n
remainder of a modulo n
290, 306
a ≡b mod n
a congruent b modulo n
295, 307

XVI
Notation
Page
gcd(a, b)
greatest common divisor of integers
289
ϕ(n)
Euler phi function
296
Fq, GF(q)
ﬁnite ﬁeld with q elements
309
ord(x)
order of an element x in a group
301
QRn
quadratic residues modulo n
311
QNRn
quadratic non-residues modulo n
311
¡ x
n
¢
Legendre or Jacobi symbol
311, 312
J+1
n
units in Zn with Jacobi symbol 1
313
[a, b]
interval a ≤x ≤b in R
⌊x⌋
greatest integer ≤x
293
⌈x⌉
smallest integer ≥x
293
O(n)
Big-O notation
293
Primesk
set of primes of binary length k
157
P or P(X)
positive polynomial
141
prob(E)
probability of an event E
325
prob(x)
probability of an element x ∈X
325
prob(E, F)
probability of E AND F
325
prob(E |F)
conditional probability of E assuming F
327
prob(y|x)
conditional probability of y assuming x
328
E(R)
expected value of a random variable R
326
X ▷◁W
join of a set X with W = (Wx)x∈X
328
XW
joint probability space
327, 328
x
pX
←X
x randomly selected according to pX
148, 329
x ←X
x randomly selected from X
148, 329
x
u←X
x uniformly selected from X
148, 329
x ←X, y ←Yx
ﬁrst x, then y randomly selected
148, 330
prob(. . . : x ←X)
probability of . . . for randomly chosen x
148, 329
{A(x) : x ←X}
image of a distribution under A
139, 330
y ←A(x)
y randomly generated by A on input x
149
dist(p, ˜p)
statistical distance between distributions
336
H(X)
uncertainty (or entropy) of X
340
H(X|Y )
conditional uncertainty (entropy)
342
I(X; Y )
mutual information
342

1. Introduction
Cryptography is the science of keeping secrets secret. Assume a sender re-
ferred to here and in what follows as Alice (as is commonly used) wants to
send a message m to a receiver referred to as Bob. She uses an insecure com-
munication channel. For example, the channel could be a computer network
or a telephone line. There is a problem if the message contains conﬁdential
information. The message could be intercepted and read by an eavesdropper.
Or, even worse, the adversary, as usual referred to here as Eve, might be able
to modify the message during transmission in such a way that the legitimate
recipient Bob does not detect the manipulation.
One objective of cryptography is to provide methods for preventing such
attacks. Other objectives are discussed in Section 1.2.
1.1 Encryption and Secrecy
The fundamental and classical task of cryptography is to provide conﬁdential-
ity by encryption methods. The message to be transmitted – it can be some
text, numerical data, an executable program or any other kind of information
– is called the plaintext. Alice encrypts the plaintext m and obtains the ci-
phertext c. The ciphertext c is transmitted to Bob. Bob turns the ciphertext
back into the plaintext by decryption. To decrypt, Bob needs some secret
information, a secret decryption key.1 Adversary Eve still may intercept the
ciphertext. However, the encryption should guarantee secrecy and prevent
her from deriving any information about the plaintext from the observed
ciphertext.
Encryption is very old. For example, Caesar’s shift cipher 2 was introduced
more than 2000 years ago. Every encryption method provides an encryption
algorithm E and a decryption algorithm D. In classical encryption schemes,
both algorithms depend on the same secret key k. This key k is used for both
encryption and decryption. These encryption methods are therefore called
1 Sometimes the terms encipher and decipher are used instead of encrypt and
decrypt.
2 Each plaintext character is replaced by the character 3 to the right modulo 26,
i.e., a is replaced by d, b by e,. . ., x by a, y by b and z by c.

2
1. Introduction
symmetric. For example, in Caesar’s cipher the secret key is the oﬀset 3 of
the shift. We have
D(k, E(k, m)) = m for each plaintext m.
Symmetric encryption and the important examples DES (data encryption
standard) and AES (advanced encryption standard) are discussed in Chap-
ter 2.
In 1976, W. Diﬃe and M.E. Hellman published their famous paper, New
Directions in Cryptography ([DifHel76]). There they introduced the revo-
lutionary concept of public-key cryptography. They provided a solution to
the long standing problem of key exchange and pointed the way to digital
signatures. The public-key encryption methods (comprehensively studied in
Chapter 3) are asymmetric. Each recipient of messages has his personal key
k = (pk, sk), consisting of two parts: pk is the encryption key and is made
public, sk is the decryption key and is kept secret. If Alice wants to send a
message m to Bob, she encrypts m by use of Bob’s publicly known encryption
key pk. Bob decrypts the ciphertext by use of his decryption key sk, which
is known only to him. We have
D(sk, E(pk, m)) = m.
Mathematically speaking, public-key encryption is a so-called one-way
function with a trapdoor. Everyone can easily encrypt a plaintext using the
public key pk, but the other direction is diﬃcult. It is practically impossible
to deduce the plaintext from the ciphertext, without knowing the secret key
sk (which is called the trapdoor information).
Public-key encryption methods require more complex computations and
are less eﬃcient than classical symmetric methods. Thus symmetric methods
are used for the encryption of large amounts of data. Before applying sym-
metric encryption, Alice and Bob have to agree on a key. To keep this key
secret, they need a secure communication channel. It is common practice to
use public-key encryption for this purpose.
1.2 The Objectives of Cryptography
Providing conﬁdentiality is not the only objective of cryptography. Cryptog-
raphy is also used to provide solutions for other problems:
1. Data integrity. The receiver of a message should be able to check whether
the message was modiﬁed during transmission, either accidentally or de-
liberately. No one should be able to substitute a false message for the
original message, or for parts of it.
2. Authentication. The receiver of a message should be able to verify its
origin. No one should be able to send a message to Bob and pretend to

1.2 The Objectives of Cryptography
3
be Alice (data origin authentication). When initiating a communication,
Alice and Bob should be able to identify each other (entity authentica-
tion).
3. Non-repudiation. The sender should not be able to later deny that she
sent a message.
If messages are written on paper, the medium – paper – provides a certain se-
curity against manipulation. Handwritten personal signatures are intended to
guarantee authentication and non-repudiation. If electronic media are used,
the medium itself provides no security at all, since it is easy to replace some
bytes in a message during its transmission over a computer network, and it
is particularly easy if the network is publicly accessible, like the Internet.
So, while encryption has a long history,3 the need for techniques provid-
ing data integrity and authentication resulted from the rapidly increasing
signiﬁcance of electronic communication.
There are symmetric as well as public-key methods to ensure the integrity
of messages. Classical symmetric methods require a secret key k that is shared
by sender and receiver. The message m is augmented by a message authenti-
cation code (MAC). The code is generated by an algorithm and depends on
the secret key. The augmented message (m, MAC(k, m)) is protected against
modiﬁcations. The receiver may test the integrity of an incoming message
(m, m) by checking whether
MAC(k, m) = m.
Message authentication codes may be implemented by keyed hash functions
(see Chapter 3).
Digital signatures require public-key methods (see Chapter 3 for examples
and details). As with classical handwritten signatures, they are intended to
provide authentication and non-repudiation. Note that non-repudiation is an
indispensable feature if digital signatures are used to sign contracts. Digital
signatures depend on the secret key of the signer – they can be generated only
by him. On the other hand, anyone can check whether a signature is valid,
by applying a publicly known veriﬁcation algorithm Verify, which depends
on the public key of the signer. If Alice wants to sign the message m, she
applies the algorithm Sign with her secret key sk and gets the signature
Sign(sk, m). Bob receives a signature s for message m, and may then check
the signature by testing whether
Verify(pk, s, m) = ok,
with Alice’s public key pk.
It is common not to sign the message itself, but to apply a cryptographic
hash function (see Section 3.4) ﬁrst and then sign the hash value. In schemes
3 For the long history of cryptography, see [Kahn67].

4
1. Introduction
like the famous RSA (named after its inventors: Rivest, Shamir and Adle-
man), the decryption algorithm is used to generate signatures and the encryp-
tion algorithm is used to verify them. This approach to digital signatures is
therefore often referred to as the “hash-then-decrypt” paradigm (see Section
3.4.5 for details). More sophisticated signature schemes, like the probabilis-
tic signature scheme (PSS), require more steps. Modifying the hash value
by pseudorandom sequences turns signing into a probabilistic procedure (see
Section 3.4.5).
Digital signatures depend on the message. Distinct messages yield dif-
ferent signatures. Thus, like classical message authentication codes, digital
signatures can also be used to guarantee the integrity of messages.
1.3 Attacks
The primary goal of cryptography is to keep the plaintext secret from eaves-
droppers trying to get some information about the plaintext. As discussed
before, adversaries may also be active and try to modify the message. Then,
cryptography is expected to guarantee the integrity of the messages. Adver-
saries are assumed to have complete access to the communication channel.
Cryptanalysis is the science of studying attacks against cryptographic
schemes. Successful attacks may, for example, recover the plaintext (or parts
of the plaintext) from the ciphertext, substitute parts of the original mes-
sage, or forge digital signatures. Cryptography and cryptanalysis are often
subsumed by the more general term cryptology.
A fundamental assumption in cryptanalysis was ﬁrst stated by A. Kerkhoﬀ
in the nineteenth century. It is usually referred to as Kerkhoﬀ’s Principle. It
states that the adversary knows all the details of the cryptosystem, includ-
ing algorithms and their implementations. According to this principle, the
security of a cryptosystem must be entirely based on the secret keys.
Attacks on the secrecy of an encryption scheme try to recover plaintexts
from ciphertexts, or even more drastically, to recover the secret key. The fol-
lowing survey is restricted to passive attacks. The adversary, as usual we call
her Eve, does not try to modify the messages. She monitors the communica-
tion channel and the end points of the channel. So she may not only intercept
the ciphertext, but (at least from time to time) she may be able to observe
the encryption and decryption of messages. She has no information about
the key. For example, Eve might be the operator of a bank computer. She
sees incoming ciphertexts and sometimes also the corresponding plaintexts.
Or she observes the outgoing plaintexts and the generated ciphertexts. Per-
haps she manages to let encrypt plaintexts or decrypt ciphertexts of her own
choice.
The possible attacks depend on the actual resources of the adversary Eve.
They are usually classiﬁed as follows:

1.4 Cryptographic Protocols
5
1. Ciphertext-only attack. Eve has the ability to obtain ciphertexts. This
is likely to be the case in any encryption situation. Even if Eve cannot
perform the more sophisticated attacks described below, one must assume
that she can get access to encrypted messages. An encryption method
that cannot resist a ciphertext-only attack is completely insecure.
2. Known-plaintext attack. Eve has the ability to obtain plaintext-ciphertext
pairs. Using the information from these pairs, she attempts to decrypt a
ciphertext for which she does not have the plaintext. At ﬁrst glance, it
might appear that such information would not ordinarily be available to
an attacker. However, it very often is available. Messages may be sent in
standard formats which Eve knows.
3. Chosen-plaintext attack. Eve has the ability to obtain ciphertexts for
plaintexts of her choosing. Then she attempts to decrypt a ciphertext
for which she does not have the plaintext. While again this may seem
unlikely, there are many cases in which Eve can do just this. For example,
she sends some interesting information to her intended victim which she
is conﬁdent he will encrypt and send out. This type of attack assumes
that Eve must ﬁrst obtain whatever plaintext-ciphertext pairs she wants
and then do her analysis, without any further interaction. This means
that she only needs access to the encrypting device once.
4. Adaptively-chosen-plaintext attack. This is the same as the previous at-
tack, except now Eve may do some analysis on the plaintext-ciphertext
pairs, and subsequently get more pairs. She may switch between gather-
ing pairs and performing the analysis as often as she likes. This means
that she has either lengthy access to the encrypting device or can some-
how make repeated use of it.
5. Chosen- and adaptively-chosen-ciphertext attack. These two attacks are
similar to the above plaintext attacks. Eve can choose ciphertexts and
gets the corresponding plaintexts. She has access to the decryption de-
vice.
1.4 Cryptographic Protocols
Encryption and decryption algorithms, cryptographic hash functions or
pseudorandom generators (see Section 2.1, Chapter 8) are the basic building
blocks (also called cryptographic primitives) for solving problems involving
secrecy, authentication or data integrity.
In many cases a single building block is not suﬃcient to solve the given
problem: diﬀerent primitives must be combined. A series of steps must be
executed to accomplish a given task. Such a well-deﬁned series of steps is
called a cryptographic protocol. As is also common, we add another condition:
we require that two or more parties are involved. We only use the term
protocol if at least two people are required to complete the task.

6
1. Introduction
As a counter example, take a look at digital signature schemes. A typical
scheme for generating a digital signature ﬁrst applies a cryptographic hash
function h to the message m and then, in a second step, computes the signa-
ture by applying a public-key decryption algorithm to the hash value h(m).
Both steps are done by one person. Thus, we do not call it a protocol.
Typical examples of protocols are protocols for user identiﬁcation. There
are many situations where the identity of a user Alice has to be veriﬁed.
Alice wants to log in to a remote computer, for example, or to get access
to an account for electronic banking. Passwords or PIN numbers are used
for this purpose. This method is not always secure. For example, anyone
who observes Alice’s password or PIN when transmitted might be able to
impersonate her. We sketch a simple challenge-and-response protocol which
prevents this attack (however, it is not perfect; see Section 4.2.1).
The protocol is based on a public-key signature scheme, and we assume
that Alice has a key k = (pk, sk) for this scheme. Now, Alice can prove her
identity to Bob in the following way.
1. Bob randomly chooses a “challenge” c and sends it to Alice.
2. Alice signs c with her secret key, s := Sign(sk, c), and sends the “re-
sponse” s to Bob.
3. Bob accepts Alice’s proof of identity, if Verify(pk, s, c) = ok.
Only Alice can return a valid signature of the challenge c, because only she
knows the secret key sk. Thus, Alice proves her identity, without showing her
secret. No one can observe Alice’s secret key, not even the veriﬁer Bob.
Suppose that an eavesdropper Eve observed the exchanged messages.
Later, she wants to impersonate Alice. Since Bob selects his challenge c at
random (from a huge set), the probability that he uses the same challenge
twice is very small. Therefore, Eve cannot gain any advantage by her obser-
vations.
The parties in a protocol can be friends or adversaries. Protocols can be
attacked. The attacks may be directed against the underlying cryptographic
algorithms or against the implementation of the algorithms and protocols.
There may also be attacks against a protocol itself. There may be passive
attacks performed by an eavesdropper, where the only purpose is to obtain
information. An adversary may also try to gain an advantage by actively
manipulating the protocol. She might pretend to be someone else, substitute
messages or replay old messages.
Important protocols for key exchange, electronic elections, digital cash
and interactive proofs of identity are discussed in Chapter 4.
1.5 Provable Security
It is desirable to design cryptosystems that are provably secure. Provably se-
cure means that mathematical proofs show that the cryptosystem resists cer-

1.5 Provable Security
7
tain types of attacks. Pioneering work in this ﬁeld was done by C.E. Shannon.
In his information theory, he developed measures for the amount of informa-
tion associated with a message and the notion of perfect secrecy. A perfectly
secret cipher perfectly resists all ciphertext-only attacks. An adversary gets
no information at all about the plaintext, even if his resources in comput-
ing power and time are unlimited. Vernam’s one-time pad (see Section 2.1),
which encrypts a message m by XORing it bitwise with a truly random bit
string, is the most famous perfectly secret cipher. It even resists all the pas-
sive attacks mentioned. This can be mathematically proven by Shannon’s
theory. Classical information-theoretic security is discussed in Section 9.1;
an introduction to Shannon’s information theory may be found in Appendix
B. Unfortunately, Vernam’s one-time pad and all perfectly secret ciphers are
usually impractical. It is not practical in most situations to generate and
handle truly random bit sequences of suﬃcient length as required for perfect
secrecy.
More recent approaches to provable security therefore abandon the ideal
of perfect secrecy and the (unrealistic) assumption of unbounded computing
power. The computational complexity of algorithms is taken into account.
Only attacks that might be feasible in practice are considered. Feasible means
that the attack can be performed by an eﬃcient algorithm. Of course, here
the question about the right notion of eﬃciency arises. Certainly, algorithms
with non-polynomial running time are ineﬃcient. Vice versa algorithms with
polynomial running time are often considered as the eﬃcient ones. In this
book, we also adopt this notion of eﬃciency.
The way a cryptographic scheme is attacked might be inﬂuenced by ran-
dom events. Adversary Eve might toss a coin to decide which case she tries
next. Therefore, probabilistic algorithms are used to model attackers. Break-
ing an encryption system, for example by a ciphertext-only attack, means that
a probabilistic algorithm with polynomial running time manages to derive in-
formation about the plaintext from the ciphertext, with some non-negligible
probability. Probabilistic algorithms can toss coins, and their control ﬂow
may be at least partially directed by these random events. By using random
sources, they can be implemented in practice. They must not be confused
with non-deterministic algorithms. The notion of probabilistic (polynomial)
algorithms and the underlying probabilistic model are discussed in Chap-
ter 5.
The security of a public-key cryptosystem is based on the hardness of
some computational problem (there is no eﬃcient algorithm for solving the
problem). For example, the secret keys of an RSA scheme could be easily
ﬁgured out if computing the prime factors of a large integer were possible.4
4 What “large” means depends on the available computing power. Today, a 1024-
bit integer is considered as large.

8
1. Introduction
However, it is believed that factoring large integers is infeasible.5 There are
no mathematical proofs for the hardness of the computational problems used
in public-key systems. Therefore, security proofs for public-key methods are
always conditional: they depend on the validity of the underlying assumption.
The assumption usually states that a certain function f is one way; i.e., f
can be computed eﬃciently, but it is infeasible to compute x from f(x). The
assumptions, as well as the notion of a one-way function, can be made very
precise by the use of probabilistic polynomial algorithms. The probability of
successfully inverting the function by a probabilistic polynomial algorithm
is negligibly small, and negligibly small means that it is asymptotically less
than any given polynomial bound (see Chapter 6, Deﬁnition 6.12). Important
examples, like the factoring, discrete logarithm and quadratic residuosity
assumptions, are included in this book (see Chapter 6).
There are analogies to the classical notions of security. Shannon’s perfect
secrecy has a computational analogy: ciphertext indistinguishability (or se-
mantic security). An encryption is perfectly secret if and only if an adversary
cannot distinguish between two plaintexts, even if her computing resources
are unlimited: if adversary Eve knows that a ciphertext c is the encryption of
either m or m′, she has no better chance than 1/2 of choosing the right one.
Ciphertext indistinguishability – also called polynomial-time indistinguisha-
bility – means that Eve’s chance of successfully applying a probabilistic poly-
nomial algorithm is at most negligibly greater than 1/2 (Chapter 9, Deﬁnition
9.14).
As a typical result, it is proven in Section 9.4 that public-key one-time
pads are ciphertext-indistinguishable. This means, for example, that the RSA
public-key one-time pad is ciphertext-indistinguishable under the sole as-
sumption that the RSA function is one way. A public-key one-time pad is
similar to Vernam’s one-time pad. The diﬀerence is that the message m is
XORed with a pseudorandom bit sequence which is generated from a short
truly random seed, by means of a one-way function.
Thus, one-way functions are not only the essential ingredients of public-
key encryption and digital signatures. They also yield computationally perfect
pseudorandom bit generators (Chapter 8). If f is a one-way function, it is not
only impossible to compute x from f(x), but certain bits (called hard-core
bits) of x are equally diﬃcult to deduce. This feature is called the bit security
of a one-way function. For example, the least-signiﬁcant bit is a hard-core bit
for the RSA function x 7→xe mod n. Starting with a truly random seed,
repeatedly applying f and taking the hard-core bit in each step, you get
a pseudorandom bit sequence. These bit sequences cannot be distinguished
from truly random bit sequences by an eﬃcient algorithm, or, equivalently
(Yao’s Theorem, Section 8.2), it is practically impossible to predict the next
bit from the previous ones. So they are really computationally perfect.
5 It is not known whether breaking RSA is easier than factoring the modulus. See
Chapters 3 and 6 for a detailed discussion.

1.5 Provable Security
9
The bit security of important one-way functions is studied in detail in
Chapter 7 including an in-depth analysis of the probabilities involved.
Randomness and the security of cryptographic schemes are closely related.
There is no security without randomness. An encryption method provides se-
crecy only if the ciphertexts appear random to the adversary Eve. Vernam’s
one-time pad is perfectly secret, because, due to the truly random key string
k, the encrypted message m ⊕k 6 is a truly random bit sequence for Eve.
The public-key one-time pad is ciphertext-indistinguishable, because if Eve
applies an eﬃcient probabilistic algorithm, she cannot distinguish the pseudo-
random key string and, as a consequence, the ciphertext from a truly random
sequence.
Public-key one-time pads are secure against passive eavesdroppers, who
perform a ciphertext-only attack (see Section 1.3 above for a classiﬁcation
of attacks). However, active adversaries, who perform adaptively-chosen-
ciphertext attacks, can be a real danger in practice – as demonstrated by Ble-
ichenbacher’s 1-Million-Chosen-Ciphertext Attack (Section 3.3.3). Therefore,
security against such attacks is also desirable. In Section 9.5, we study two ex-
amples of public-key encryption schemes which are secure against adaptively-
chosen-ciphertext attacks, and their security proofs. One of the examples,
Cramer-Shoup’s public key encryption scheme, was the ﬁrst practical scheme
whose security proof is based solely on a standard number-theoretic assump-
tion and a standard assumption of hash functions (collision-resistance).
The ideal cryptographic hash function is a random function. It yields hash
values which cannot be distinguished from randomly selected and uniformly
distributed values. Such a random function is also called a random oracle.
Sometimes, the security of a cryptographic scheme can be proven in the
random oracle model. In addition to the assumed hardness of a computational
problem, such a proof relies on the assumption that the hash functions used
in the scheme are truly random functions. Examples of such schemes include
the public-key encryption schemes OAEP (Section 3.3.4) and SAEP (Section
9.5.1), the above mentioned signature scheme PSS and full-domain-hash RSA
signatures (Section 3.4.5). We give the random-oracle proofs for SAEP and
full-domain-hash signatures.
Truly random functions can not be implemented, nor even perfectly ap-
proximated in practice. Therefore, a proof in the random oracle model can
never be a complete security proof. The hash functions used in practice are
constructed to be good approximations to the ideal of random functions.
However, there were surprising errors in the past (see Section 3.4).
We distinguished diﬀerent types of attacks on an encryption scheme. In a
similar way, the attacks on signature schemes can be classiﬁed and diﬀerent
levels of security can be deﬁned. We introduce this classiﬁcation in Chap-
ter 10 and give examples of signature schemes whose security can be proven
solely under standard assumptions (like the factoring or the strong RSA as-
6 ⊕denotes the bitwise XOR operator, see page 13.

10
1. Introduction
sumption). No assumptions on the randomness of a hash function have to be
made, in contrast, for example, to schemes like PSS. A typical security proof
for the highest level of security is included. For the given signature scheme,
we show that not a single signature can be forged, even if the attacker Eve
is able to obtain valid signatures from the legitimate signer, for messages she
has chosen adaptively.
The security proofs for public-key systems are always conditional and de-
pend on (widely believed, but unproven) assumptions. On the other hand,
Shannon’s notion of perfect secrecy and, in particular, the perfect secrecy
of Vernam’s one-time pad are unconditional. Although perfect unconditional
security is not reachable in most practical situations, there are promising at-
tempts to design practical cryptosystems which provably come close to perfect
information-theoretic security. The proofs are based on classical information-
theoretic methods and do not depend on unproven assumptions. The security
relies on the fact that communication channels are noisy or on the limited
storage capacity of an adversary. Some results in this approach are reviewed
in the chapter on provably secure encryption (Section 9.6).

2. Symmetric-Key Encryption
In this chapter, we give an introduction to symmetric-key encryption. We
explain the notions of stream and block ciphers. The operation modes of
block ciphers are studied and, as prominent examples for block ciphers, DES
and AES are described.
Symmetric-key encryption provides secrecy when two parties, say Alice
and Bob, communicate. An adversary who intercepts a message should not
get any signiﬁcant information about its content.
To set up a secure communication channel, Alice and Bob ﬁrst agree on
a key k. They keep their shared key k secret. Before sending a message m
to Bob, Alice encrypts m by using the encryption algorithm E and the key
k. She obtains the ciphertext c = E(k, m) and sends c to Bob. By using the
decryption algorithm D and the same key k, Bob decrypts c to recover the
plaintext m = D(k, c).
We speak of symmetric encryption, because both communication part-
ners use the same key k for encryption and decryption. The encryption and
decryption algorithms E and D are publicly known. Anyone can decrypt a
ciphertext, if he or she knows the key. Thus, the key k has to be kept secret.
A basic problem in a symmetric scheme is how Alice and Bob can agree
on a shared secret key k in a secure and eﬃcient way. For this key exchange,
the methods of public-key cryptography are needed, which we discuss in the
subsequent chapters. There were no solutions to the key exchange problem,
until the revolutionary concept of public-key cryptography was discovered 30
years ago.
We require that the encrypted plaintext m can be uniquely recovered
from the ciphertext c. This means that for a ﬁxed key k, the encryption map
must be bijective. Mathematically, symmetric encryption may be considered
as follows.
Deﬁnition 2.1. A symmetric-key encryption scheme consists of a map
E : K × M −→C,
such that for each k ∈K, the map
Ek : M −→C, m 7−→E(k, m)

12
2. Symmetric-Key Encryption
is invertible. The elements m ∈M are the plaintexts (also called messages).
C is the set of ciphertexts or cryptograms, the elements k ∈K are the keys.
Ek is called the encryption function with respect to the key k. The inverse
function Dk := E−1
k
is called the decryption function. It is assumed that
eﬃcient algorithms to compute Ek and Dk exist.
The key k is shared between the communication partners and kept se-
cret. A basic security requirement for the encryption map E is that, without
knowing the key k, it should be impossible to successfully execute the decryp-
tion function Dk. Important examples of symmetric-key encryption schemes
– Vernam’s one-time pad, DES and AES – are given below.
Among all encryption algorithms, symmetric-key encryption algorithms
have the fastest implementations in hardware and software. Therefore, they
are very well-suited to the encryption of large amounts of data. If Alice and
Bob want to use a symmetric-key encryption scheme, they ﬁrst have to ex-
change a secret key. For this, they have to use a secure communication chan-
nel. Public-key encryption methods, which we study in Chapter 3, are often
used for this purpose. Public-key encryption schemes are less eﬃcient and
hence not suitable for large amounts of data. Thus, symmetric-key encryp-
tion and public-key encryption complement each other to provide practical
cryptosystems.
We distinguish between block ciphers and stream ciphers. The encryption
function of a block cipher processes plaintexts of ﬁxed length. A stream ci-
pher operates on streams of plaintext. Processing character by character, it
encrypts plaintext strings of arbitrary length. If the plaintext length exceeds
the block length of a block cipher, various modes of operation are used. Some
of them yield stream ciphers. Thus, block ciphers may also be regarded as
building blocks for stream ciphers.
2.1 Stream Ciphers
Deﬁnition 2.2. Let K be a set of keys and M be a set of plaintexts. In this
context, the elements of M are called characters.
A stream cipher
E∗: K∗× M ∗−→C∗, E∗(k, m) := c := c1c2c3 . . .
encrypts a stream m := m1m2m3 . . . ∈M ∗of plaintext characters mi ∈M
as a stream c := c1c2c3 . . . ∈C∗of ciphertext characters ci ∈C by using a
key stream k := k1k2k3 . . . ∈K∗, ki ∈K.
The plaintext stream m = m1m2m3 . . . is encrypted character by character.
For this purpose, there is an encryption map
E : K × M −→C,

2.1 Stream Ciphers
13
which encrypts the single plaintext characters mi with the corresponding key
character ki:
ci = Eki(mi) = E(ki, mi), i = 1, 2, . . . .
Typically, the characters in M and C and the key elements in K are binary
digits or bytes.
Of course, encrypting plaintext characters with Eki must be a bijective
map, for every key character ki ∈K. Decrypting a ciphertext stream c :=
c1c2c3 . . . is done character by character by applying the decryption map D
with the same key stream k = k1k2k3 . . . that was used for encryption:
c = c1c2c3 . . . 7→D(k, c) := Dk1(c1)Dk2(c2)Dk3(c3) . . . .
A necessity for stream ciphers comes, for example, from operating sys-
tems, where input and output is done with so-called streams.
Of course, the key stream in a stream cipher has to be kept secret. It is
not necessarily the secret key which is shared between the communication
partners; the key stream might be generated from the shared secret key by a
pseudorandom generator (see below).
Notation. In most stream ciphers, the binary exclusive-or operator XOR
for bits a, b ∈{0, 1} – considered as truth values – is applied. We have
a XOR b = 1, if a = 0 and b = 1 or a = 1 and b = 0, and a XOR b = 0,
if a = b = 0 or a = b = 1. XORing two bits a and b means to add them
modulo 2, i.e., we have a XOR b = a + b mod 2. As is common practice, we
denote the XOR-operator by ⊕, a ⊕b := a XOR b, and we use ⊕also for
the binary operator that bitwise XORs two bit strings. If a = a1a2 . . . an and
b = b1b2 . . . bn are bit strings, then
a ⊕b := (a1 XOR b1)(a2 XOR b2) . . . (an XOR bn).
Vernam’s One-Time Pad. The most famous example of a stream ci-
pher is Vernam’s one-time pad (see [Vernam19] and [Vernam26]). It is
easy to describe. Plaintexts, keys and ciphertexts are bit strings. To en-
crypt a message m := m1m2m3 . . ., where mi ∈{0, 1}, a key stream
k := k1k2k3 . . ., with ki ∈{0, 1}, is needed. Encryption and decryption are
given by bitwise XORing with the key stream:
E∗(k, m) := k ⊕m and D∗(k, c) := k ⊕c.
Obviously, encryption and decryption are inverses of each other. Each bit
in the key stream is chosen at random and independently, and the key stream
is used only for the encryption of one message m. This fact explains the name
“one-time pad”. If a key stream k were used twice to encrypt m and m, we
could derive m⊕m from the cryptograms c and c and thus obtain information
about the plaintexts, by computing c⊕c = m⊕k ⊕m⊕k = m⊕m⊕k ⊕k =
m ⊕m.

14
2. Symmetric-Key Encryption
There are obvious disadvantages to Vernam’s one-time pad. Truly random
keys of the same length as the message have to be generated and securely
transmitted to the recipient. There are very few situations where this is prac-
tical. Reportedly, the hotline between Washington and Moscow was encrypted
with a one-time pad; the keys were transported by a trusted courier.
Nevertheless, most practical stream ciphers work as Vernam’s one-time
pad. The diﬀerence is that a pseudorandom key stream is taken instead of the
truly random key stream. A pseudorandom key stream looks like a random
key stream, but actually the bits are generated from a short (truly) random
seed by a deterministic algorithm. In practice, such pseudorandom generators
can be based on speciﬁc operation modes of block ciphers or on feedback shift
registers. We study the operation modes of block ciphers in Section 2.2.3 (e.g.
see the cipher and output feedback modes). Feedback shift registers can be
implemented to run very fast on relatively simple hardware. This fact makes
them especially attractive. More about these generators and stream ciphers
can be found, for example, in [MenOorVan96]. There are also public-key
stream ciphers, in which the pseudorandom key stream is generated by using
public-key techniques. We discuss these pseudorandom bit generators and the
resulting stream ciphers in detail in Chapters 8 and 9.
Back to Vernam’s one-time pad. Its advantage is that one can prove that
it is secure – an adversary observing a cryptogram does not have the slightest
idea what the plaintext is. We discuss this point in the simplest case, where
the message m consists of a single bit. Alice and Bob want to exchange one
of the messages yes = 1 or no = 0. Previously, they exchanged the key bit k,
which was the outcome of an unbiased coin toss.
First, we assume that each of the two messages yes and no is equally
likely. The adversary, we call her Eve, intercepts the cryptogram c. Since
the key bit is truly random, Eve can only derive that c encrypts yes or no
with probability 1/2. Thus, she has not the slightest idea which of the two is
encrypted. Her only chance of making a decision is to toss a coin. She can do
this, however, without seeing the cryptogram c.
If one of the two messages has a greater probability, Eve also cannot
gain any advantage by intercepting the cryptogram c. Assume, for example,
that the probability of a 1 is 3/4 and the probability of a 0 is 1/4. Then
the cryptogram c encrypts 0 with probability 1/4 and 1 with probability 3/4,
irrespective of whether c = 0 or c = 1. Thus, Eve cannot learn more from
the cryptogram than she has learned a priori about the distribution of the
plaintexts.
Our discussion for one-bit messages may be transferred to the general
case of n-bit messages. The amount of information an attacker may obtain is
made precise by information theory. The level of security we achieve with the
one-time pad is called perfect secrecy (see Chapter 9 for details). Note that we
have to assume that all messages have the same length n (if necessary, they

2.2 Block Ciphers
15
are padded out). Otherwise, some information – the length of the message –
would leak to the attacker.
The Vernam one-time pad not only resists a ciphertext-only attack as
proven formally in Chapter 9, but it resists all the attacks deﬁned in Chap-
ter 1. Each cryptogram has the same probability. Eve does not learn anything,
not even about the probabilities of the plaintexts, if she does not know them
a priori. For each message, the key is chosen at random and independently
from the previous ones. Thus, Eve cannot gain any advantage by observing
plaintext-ciphertext pairs, not even if she has chosen the plaintexts adap-
tively.
The Vernam one-time pad ensures the conﬁdentiality of messages, but it
does not protect messages against modiﬁcations. If someone changes bits in
the cryptogram and the decrypted cryptogram makes sense, the receiver will
not notice it.
2.2 Block Ciphers
Deﬁnition 2.3. A block cipher is a symmetric-key encryption scheme with
M = C = {0, 1}n and key space K = {0, 1}r:
E : {0, 1}r × {0, 1}n −→{0, 1}n, (k, m) 7−→E(k, m).
Using a secret key k of binary length r, the encryption algorithm E encrypts
plaintext blocks m of a ﬁxed binary length n and the resulting ciphertext
blocks c = E(k, m) also have length n. n is called the block length of the
cipher.
Typical block lengths are 64 (as in DES) or 128 (as in AES), typical key
lengths are 56 (as in DES) or 128, 192 and 256 (as in AES).
Let us consider a block cipher E with block length n and key length r.
There are 2n plaintext blocks and 2n ciphertext blocks of length n. For a ﬁxed
key k, the encryption function Ek : m 7→E(k, m) maps {0, 1}n bijectively
to {0, 1}n – it is a permutation1 of {0, 1}n. Thus, to choose a key k, means
to select a permutation Ek of {0, 1}n, and this permutation is then used
to encrypt the plaintext blocks. The 2r permutations Ek, with k running
through the set {0, 1}r of keys, form an almost negligibly small subset in the
tremendously large set of all permutations of {0, 1}n, which consists of 2n!
elements. So, when we randomly choose an r-bit key k for E, then we restrict
our selection of the encryption permutation to an extremely small subset.
From these considerations, we conclude that we cannot have the ideal
block cipher with perfect secrecy in practice. Namely, in the preceding Section
2.1, we discussed a stream cipher with perfect secrecy, the Vernam one-time
pad. Perfect secrecy results from a maximum amount of randomness: for each
1 A map f : D −→D is called a permutation of D, if f is bijective.

16
2. Symmetric-Key Encryption
message bit, a random key bit is chosen (we will prove in Chapter 9 that less
randomness in key generation destroys perfect secrecy, see Theorem 9.6). We
conclude that the maximal level of security in a block cipher also requires a
maximum of randomness, and this in turn means that – when choosing a key
– we would have to select a random element from the set of all permutations
of {0, 1}n. Unfortunately, this turns out to be completely impractical. We
could try to enumerate all permutations π of {0, 1}n, π1, π2, π3, . . ., and then
randomly select one by randomly selecting an index (this index would be
the key). Since there are 2n! permutations, we need log2(2n!)-bit numbers to
encode the indexes. By Stirling’s approximation formula k! ≈
√
2πk
¡k/e
¢k,
we derive that log2(2n!) ≈(n −1.44)2n. This is a huge number. For a block
length n of 64 bits, we would need approximately 267 bytes to store a single
key. There is no storage medium with such capacity.
Thus, in a real block cipher, we have to restrict ourselves to much smaller
keys and choose the encryption permutation Ek for a key k from a much
smaller set of 2r permutations, with r typically in the range of 56 to 256.
Nevertheless, the designers of a block cipher try to approximate the ideal.
The idea is to get an encryption function which behaves like a randomly
chosen function from the very huge set of all permutations.
2.2.1 DES
The data encryption standard (DES), originally speciﬁed in [FIPS46 1977],
was previously the most widely used symmetric-key encryption algorithm.
Governments, banks and applications in commerce took the DES as the basis
for secure and authentic communication.
We give a high-level description of the DES encryption and decryption
functions. The DES algorithm takes 56-bit keys and 64-bit plaintext messages
as inputs and outputs a 64-bit cryptogram:2
DES : {0, 1}56 × {0, 1}64 −→{0, 1}64
If the key k is chosen, we get
DESk : {0, 1}64 −→{0, 1}64, x 7−→DES(k, x).
An encryption with DESk consists of 16 major steps, called rounds. In
each of the 16 rounds, a 48-bit round key ki is used. The 16 round keys
k1, . . . , k16 are computed from the 56-bit key k by using an algorithm which
is studied in Exercise 1 at the end of this chapter.
In the deﬁnition of DES, one of the basic building blocks is a map
f : {0, 1}48 × {0, 1}32 −→{0, 1}32,
2 Actually, the 56 bits of the key are packed with 8 bits of parity.

2.2 Block Ciphers
17
which transforms a 32-bit message block x with a 48-bit round key ˜k. f is
composed of a substitution S and a permutation P:
f(˜k, x) = P(S(E(x) ⊕˜k)).
The 32 message bits are extended to 48 bits, x 7→E(x) (some of the 32 bits
are used twice), and XORed with the 48-bit round key ˜k. The resulting 48
bits are divided into eight groups of 6 bits, and each group is substituted by 4
bits. Thus, we get 32 bits which are then permuted by P. The cryptographic
strength of the DES function depends on the design of f, especially on the
design of the eight famous S-boxes which handle the eight substitutions (for
details, see [FIPS46 1977]).
We deﬁne for i = 1, . . . , 16
φi : {0, 1}32 × {0, 1}32 −→{0, 1}32 × {0, 1}32, (x, y) 7−→(x ⊕f(ki, y), y).
φi transforms 64-bit blocks and for this transformation, a 64-bit block is split
into two 32-bit halves x and y. We have
φi ◦φi(x, y) = φi(x ⊕f(ki, y), y) = (x ⊕f(ki, y) ⊕f(ki, y), y) = (x, y).3
Hence, φi is bijective and φ−1
i
= φi.4 The fact that φi is bijective does not
depend on any properties of f.
The DESk function is obtained by composing φ1, . . . , φ16 and the map
µ : {0, 1}32 × {0, 1}32 −→{0, 1}32 × {0, 1}32, (x, y) 7−→(y, x),
which interchanges the left and the right half of a 64-bit block (x, y).
Namely,
DESk : {0, 1}64 −→{0, 1}64,
DESk(x) := IP−1 ◦φ16 ◦µ ◦φ15 ◦. . . µ ◦φ2 ◦µ ◦φ1 ◦IP(x).
Here, IP is a publicly known permutation without cryptographic signiﬁcance.
We see that a DES cryptogram is obtained by 16 encryptions of the same
type using 16 diﬀerent round keys that are derived from the original 56-
bit key. φi is called the encryption of round i. After each round, except
the last one, the left and the right half of the argument are interchanged.
A block cipher that is computed by iteratively applying a round function
to the plaintext is called an iterated cipher. If the round function has the
form of the DES round function φi, the cipher is called a Feistel cipher. H.
Feistel developed the Lucifer algorithm, which was a predecessor of the DES
algorithm. The idea of using an alternating sequence of permutations and
substitutions to get an iterated cipher can be attributed to C.E. Shannon
(see [Shannon49]).
3 g ◦h denotes the composition of maps: g ◦h(x) := g(h(x)).
4 As usual, if f : D −→R is a bijective map, we denote the inverse map by f −1.

18
2. Symmetric-Key Encryption
Notation. We also write DESk1...k16 for DESk to indicate that the round
keys, derived from k, are used in this order for encryption.
The following Proposition 2.4 means that the DES encryption function
may also be used for decryption. For decryption, the round keys k1 . . . k16 are
supplied in reverse order.
Proposition 2.4. For all messages x ∈{0, 1}64
DESk16...k1(DESk1...k16(x)) = x.
In other words,
DESk16...k1 ◦DESk1...k16 = id.
Proof. Since φi = φ−1
i
(see above) and, obviously, µ = µ−1, we get
DESk16...k1 ◦DESk1...k16
= IP−1 ◦φ1 ◦µ ◦φ2 ◦. . . µ ◦φ16 ◦IP ◦IP−1 ◦φ16 ◦µ ◦φ15 ◦. . . µ ◦φ1 ◦IP
= id.
This proves the proposition.
2
Shortly after DES was published, W. Diﬃe and M.E. Hellman criti-
cized the short key size of 56 bits in [DifHel77]. They suggested using
DES in multiple encryption mode. In triple encryption mode with three
independent 56-bit keys k1, k2 and k3, the cryptogram c is computed by
DESk3(DESk2(DESk1(m))). This can strengthen the DES because the set of
DESk functions is not a group (i.e., DESk2 ◦DESk1 is not a DESk function), a
fact which was shown in [CamWie92]. Moreover, it is shown there that 102499
is a lower bound for the size of the subgroup generated by the DESk functions
in the symmetric group. A small order of this subgroup would imply a less
secure multiple encryption mode.
The DES algorithm is well-studied and a lot of cryptanalysis has been
performed. Special methods like linear and diﬀerential cryptanalysis have
been developed and applied to attempt to break DES. However, the best
practical attack known is an exhaustive key search. Assume some plaintext-
ciphertext pairs (mi, ci), i = 1, . . . , n, are given. An exhaustive key search
tries to ﬁnd the key by testing DES(k, mi) = ci, i = 1, . . . , n, for all possible
k ∈{0, 1}56. If such a k is found, the probability that k is really the key
is very high. Special computers were proposed to perform an exhaustive key
search (see [DifHel77]). Recently a specially designed supercomputer and a
worldwide network of nearly 100 000 PCs on the Internet were able to ﬁnd out
the key after 22 hours and 15 minutes (see [RSALabs]). This eﬀort recovered
one key. This work would need to be repeated for each additional key to be
recovered.
The key size and the block size of DES have become too small to resist the
progress in computer technology. The U.S. National Institute of Standards

2.2 Block Ciphers
19
and Technology (NIST) had standardized DES in the 1970s. After more than
20 “DES years” the search for a successor, the AES, was started.
2.2.2 AES
In January 1997, the National Institute of Standards and Technology started
an open selection process for a new encryption standard – the advanced en-
cryption standard, or AES for short. NIST encouraged parties worldwide to
submit proposals for the new standard. The proposals were required to sup-
port a block size of at least 128 bits, and three key sizes of 128, 192 and 256
bits.
The selection process was divided into two rounds. In the ﬁrst round, 15
of the submitted 21 proposals were accepted as AES candidates. The candi-
dates were evaluated by a public discussion. The international cryptographic
community was asked for comments on the proposed block ciphers. Five
candidates were chosen for the second round: MARS (IBM), RC6 (RSA), Ri-
jndael (Daemen and Rijmen), Serpent (Anderson, Biham and Knudsen) and
Twoﬁsh (Counterpane). Three international “AES Candidate Conferences”
were held, and in October 2000 NIST selected the Rijndael cipher to be the
AES (see [NIST2000]).
Rijndael (see [DaeRij02]) was developed by J. Daemen and V. Rijmen. It is
an iterated block cipher and supports diﬀerent block and key sizes. Block and
key sizes of 128, 160, 192, 224 and 256 bits can be combined independently.
The only diﬀerence between Rijndael and AES is that AES supports only
a subset of Rijndael’s block and key sizes. The AES ﬁxes the block length to
128 bits, and uses the three key lengths 128, 192 and 256 bits.
Besides encryption, Rijndael (like many block ciphers) is suited for other
cryptographic tasks, for example, the construction of cryptographic hash
functions (see Section 3.4.2) or pseudorandom bit generators (see Section
2.2.3). Rijndael can be implemented eﬃciently on a wide range of processors
and on dedicated hardware.
Structure of Rijndael. Rijndael is an iterated block cipher. The itera-
tions are called rounds. The number of rounds, which we denote by Nr, de-
pends on the block length and the key length. In each round except the ﬁnal
round, the same round function is applied, each time with a diﬀerent round
key. The round function of the ﬁnal round diﬀers slightly. The round keys
key1, . . . , keyNr are derived from the secret key k by using the key schedule
algorithm, which we describe below.
We use the terminology of [DaeRij02] in our description of Rijndael. A
byte, as usual, consists of 8 bits, and by a word we mean a sequence of 32
bits or, equivalently, 4 bytes.
Rijndael is byte-oriented. Input and output (plaintext block, key, cipher-
text block) are considered as one-dimensional arrays of 8-bit-bytes. Both
block length and key length are multiples of 32 bits. We denote by Nb the

20
2. Symmetric-Key Encryption
block length in bits divided by 32 and by Nk the key length in bits divided
by 32. Thus, a Rijndael block consists of Nb words (or 4 · Nb bytes), and a
Rijndael key consists of Nk words (or 4 · Nk bytes).
The following table shows the number of rounds Nr as a function of Nk
and Nb:
Nb
Nk 4 5
6
7 8
4 10 11 12 13 14
5 11 11 12 13 14
6 12 12 12 13 14
7 13 13 13 13 14
8 14 14 14 14 14
In particular, AES with key length 128 bits (and the ﬁxed AES block length
of 128 bits) consists of 10 rounds.
The round function of Rijndael, and its steps, operate on an intermediate
result, called the state. The state is a block of Nb words (or 4 · Nb bytes).
At the beginning of an encryption, the variable state is initialized with the
plaintext block, and at the end, state contains the ciphertext block.
The intermediate result state is considered as a 4-row matrix of bytes with
Nb columns. Each column contains one of the Nb words of state.
The following table shows the state matrix in the case of block length 192
bits. We have 6 state words. Each column of the matrix represents a state
word consisting of 4 bytes.
a0,0 a0,1 a0,2 a0,3 a0,4 a0,5
a1,0 a1,1 a1,2 a1,3 a1,4 a1,5
a2,0 a2,1 a2,2 a2,3 a2,4 a2,5
a3,0 a3,1 a3,2 a3,3 a3,4 a3,5
The Rijndael Algorithm. An encryption with Rijndael consists of an ini-
tial round key addition, followed by applying the round function (Nr −1)-
times, and a ﬁnal round with a slightly modiﬁed round function. The round
function is composed of the SubBytes, ShiftRows and MixColumns steps and
an addition of the round key (see next section). In the ﬁnal round, the Mix-
Columns step is omitted. A high level description of the Rijndael algorithm
follows:

2.2 Block Ciphers
21
Algorithm 2.5.
byteString Rijndael(byteString plaintextBlock, key)
1
InitState(plaintextBlock, state)
2
AddKey(state, key0)
3
for i ←1 to Nr −1 do
4
SubBytes(state)
5
ShiftRows(state)
6
MixColumns(state)
7
AddKey(state, keyi)
8
SubBytes(state)
9
ShiftRows(state)
10
AddKey(state, keyNr)
11
return state;
The input and output blocks of the Rijndael algorithm are byte strings of
4·Nb bytes. In the beginning, the state matrix is initialized with the plaintext
block. The matrix is ﬁlled column by column. The ciphertext is taken from
the state matrix after the last round. Here, the matrix is read column by
column.
All steps of the round function – SubBytes, ShiftRows, MixColumns, Ad-
dKey – are invertible. Therefore, decrypting with Rijndael means to apply
the inverse functions of SubBytes, ShiftRows, MixColumns and AddKey, in
the reverse order.
The Round function. We describe now the steps – SubBytes, ShiftRows,
MixColumns and AddKey – of the round function. The Rijndael algorithm
and its steps are byte-oriented. They operate on the bytes of the state matrix.
In Rijndael, bytes are usually considered as elements of the ﬁnite ﬁeld F28
with 28 elements, and F28 is constructed as an extension of the ﬁeld F2 with
2 elements by using the irreducible polynomial X8 + X4 + X3 + X + 1 (see
Appendix A.5.3). Then adding (which is the same as bitwise XORing) and
multiplying bytes means to add and multiply them as elements of the ﬁeld
F28.
The SubBytes Step. SubBytes is the only non-linear transformation of
Rijndael. It substitutes the bytes of the state matrix byte by byte, by applying
the function SRD5 to each element of the matrix state. The function SRD is
also called the S-box; it does not depend on the key. The same S-box is used
for all byte positions. The S-box SRD is composed of two maps, f and g. First
f and then g is applied:
SRD(x) = g ◦f(x) = g(f(x))
(x a byte).
Both maps, f and g, have a simple algebraic description.
To understand f, we consider a byte x as an element of the ﬁnite ﬁeld
F28. Then f simply maps x to its multiplicative inverse x−1:
5 Rijmen and Daemen’s S-box.

22
2. Symmetric-Key Encryption
f : F28 −→F28, x 7−→
½
x−1 if x ̸= 0,
0
if x = 0.
To understand g, we consider a byte x as a vector of 8 bits or, more precisely,
as a vector of length 8 over the ﬁeld F2 with 2 elements6. Then g is the
F2-aﬃne map
g : F8
2 −→F8
2, x 7−→Ax + b,
composed of a linear map x 7→Ax and a translation with vector b. The
matrix A of the linear map and b are given by
A :=












1 0 0 0 1 1 1 1
1 1 0 0 0 1 1 1
1 1 1 0 0 0 1 1
1 1 1 1 0 0 0 1
1 1 1 1 1 0 0 0
0 1 1 1 1 1 0 0
0 0 1 1 1 1 1 0
0 0 0 1 1 1 1 1












and b :=












1
1
0
0
0
1
1
0












.
The S-box SRD operates on each of the state bytes of the state matrix
independently. For a block length of 128 bits, we have:
a0,0 a0,1 a0,2 a0,3
a1,0 a1,1 a1,2 a1,3
a2,0 a2,1 a2,2 a2,3
a3,0 a3,1 a3,2 a3,3
7−→
SRD(a0,0) SRD(a0,1) SRD(a0,2) SRD(a0,3)
SRD(a1,0) SRD(a1,1) SRD(a1,2) SRD(a1,3)
SRD(a2,0) SRD(a2,1) SRD(a2,2) SRD(a2,3)
SRD(a3,0) SRD(a3,1) SRD(a3,2) SRD(a3,3)
.
Both maps f and g are invertible. We even have f = f −1. Thus the S-box
SRD is invertible and SRD
−1 = f −1 ◦g−1 = f ◦g−1.
The ShiftRows Step. The ShiftRows transformation performs a cyclic left
shift of the rows of the state matrix. The oﬀsets are diﬀerent for each row
and depend on the block length Nb.
Nb 1. row 2. row 3. row 4. row
4
0
1
2
3
5
0
1
2
3
6
0
1
2
3
7
0
1
2
4
8
0
1
3
4
For a block length of 128 bits (Nb = 4), as in AES, ShiftRows is the map
6 Recall that the ﬁeld F2 with 2 elements consists of the residues modulo 2, i.e.,
F2 = Z2 = {0, 1}.

2.2 Block Ciphers
23
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
7−→
a
b
c
d
f
g
h
e
k
l
i
j
p
m
n
o
Obviously, ShiftRows is invertible. The inverse operation is obtained by
cyclic right shifts with the same oﬀsets.
The MixColumns Step. The MixColumns transformation operates on
each column of the state matrix independently. We consider a column
a = (a0, a1, a2, a3) as a polynomial a(X) = a3X3 + a2X2 + a1X + a0 of
degree ≤3, with coeﬃcients in F28.
Then MixColumns transforms a column a by multiplying it with the ﬁxed
polynomial
c(X) := 03 X3 + 01 X2 + 01 X + 02
and taking the residue of the product modulo X4 + 1:
a(X) 7→a(X) · c(X) mod (X4 + 1).
The coeﬃcients of c are elements of F28. Hence, they are represented as bytes,
and a byte is given by two hexadecimal digits, as usual.
The transformations of MixColumns, multiplying by c(X) and taking the
residue modulo X4 + 1, are F28-linear maps. Hence MixColumns is a linear
map of vectors of length 4 over F28. It is given by the following 4 × 4-matrix
over F28:




02 03 01 01
01 02 03 01
01 01 02 03
03 01 01 02



.
Again, bytes are represented by two hexadecimal digits.
MixColumns transforms each column of the state matrix independently.
For a block length of 128 bits, as in AES, we get
a0,0 a0,1 a0,2 a0,3
a1,0 a1,1 a1,2 a1,3
a2,0 a2,1 a2,2 a2,3
a3,0 a3,1 a3,2 a3,3
7−→
b0,0 b0,1 b0,2 b0,3
b1,0 b1,1 b1,2 b1,3
b2,0 b2,1 b2,2 b2,3
b3,0 b3,1 b3,2 b3,3
,
where




b0,j
b1,j
b2,j
b3,j



=




02 03 01 01
01 02 03 01
01 01 02 03
03 01 01 02



·




a0,j
a1,j
a2,j
a3,j



, j = 0, 1, 2, 3.
The polynomial c(X) is relatively prime to X4 + 1. Therefore c(X) is a
unit modulo X4 + 1. Its inverse is

24
2. Symmetric-Key Encryption
d(X) = 0B X3 + 0D X2 + 09 X + 0E,
i.e., c(X)·d(X) mod (X4+1) = 1. This implies that MixColumns is invertible.
The inverse operation is to multiply each column of the state matrix by d(X)
modulo X4 + 1.
AddKey. The operation AddKey is the only operation in Rijndael that de-
pends on the secret key k, which is shared by the communication partners.
It adds a round key to the intermediate result state. The round keys are de-
rived from the secret key k by applying the key schedule algorithm, which is
described in the next section. Round keys are bit strings and, as the interme-
diate results state, they have block length, i.e., each round key is a sequence
of Nb words. AddKey simply bitwise XORs the state with the roundkey to
get the new value of state:
(state, roundkey) 7→state ⊕roundkey.
Since we arrange state as a matrix, a round key is also represented as
a round key matrix of bytes with 4 rows and Nb columns. Each of the Nb
words of the round key yields a column. Then the corresponding entries of
the state matrix and the round key matrix are bitwise XORed by AddKey
to get the new state matrix. Note that bitwise XORing two bytes means to
add two elements of the ﬁeld F28.
Obviously, AddKey is invertible. It is inverse to itself. To invert it, you
simply apply AddKey a second time with the same round key.
The Key Schedule. The secret key k consists of Nk 4-byte-words. The
Rijndael algorithm needs a round key for each round and one round key for
the initial key addition. Thus we have to generate Nr + 1 round keys (as
before, Nr is the number of rounds). A round key consists of Nb words. If we
concatenate all the round keys, we get a string of Nb(Nr + 1) words. We call
this string the expanded key.
The expanded key is derived from the secret key k by the key expansion
procedure, which we describe below. The round keys
key0, key1, key2, . . . , keyNr
are then selected from the expanded key ExpKey: key0 consists of the ﬁrst
Nb words of ExpKey, key1 consists of the next Nb words of ExpKey, and
so on.
To explain the key expansion procedure, we use functions fj, deﬁned
for multiples j of Nk, and a function g. All these functions map words
(x0, x1, x2, x3), which each consist of 4 bytes, to words.
g simply applies the S-box SRD (see SubBytes above) to each byte:
(x0, x1, x2, x3) 7→(SRD(x0), SRD(x1), SRD(x2), SRD(x3)).
If j is a multiple of Nk, i.e., j ≡0 mod Nk, we deﬁne fj by

2.2 Block Ciphers
25
(x0, x1, x2, x3) 7→
¡
SRD(x1) ⊕RC
£j/Nk
¤
, SRD (x2) , SRD (x3) , SRD (x0)
¢
.
Here, so-called round constants RC[i] are used. They are deﬁned as follows.
First, recall that in our representation, the elements of F28 are the residues of
polynomials with coeﬃcients in F2 modulo P(X) = X8 + X4 + X3 + X + 1.
Now, the round constant RC[i] ∈F28 is deﬁned by RC[i] := Xi−1 mod P(X).
Relying on the non-linear S-box SRD, the functions fj and g are also
non-linear.
We are ready to describe the key expansion. We denote by
ExpKey[j], 0 ≤j < Nb(Nr + 1),
the words of the expanded key. The ﬁrst Nk words are initialized with the
secret key k. The following words are computed recursively. ExpKey[j] de-
pends on ExpKey[j −Nk] and on ExpKey[j −1].
Depending on the key length Nk, there are two versions of the key expan-
sion procedure, one for Nk ≤6, the other for Nk > 6. We have for Nk ≤6:
ExpKey[j] :=
½ExpKey[j −Nk] ⊕fj(ExpKey[j −1]) if j ≡0 mod Nk,
ExpKey[j −Nk] ⊕ExpKey[j −1]
if j ̸≡0 mod Nk.
If Nk > 6, we have:
ExpKey[j] :=







ExpKey[j −Nk] ⊕fj(ExpKey[j −1]) if j ≡0 mod Nk,
ExpKey[j −Nk] ⊕g(ExpKey[j −1]) if j ̸≡0 mod Nk
and j ≡4 mod Nk,
ExpKey[j −Nk] ⊕ExpKey[j −1]
else.
2.2.3 Modes of Operation
Block ciphers need some extension, because in practice most of the messages
have a size that is distinct from the block length. Often the message length
exceeds the block length. Modes of operation handle this problem. They were
ﬁrst speciﬁed in conjunction with DES, but they can be applied to any block
cipher.
We consider a block cipher E with block length n. We ﬁx a key k and, as
usual, we denote the encryption function with this key k by
Ek : {0, 1}n −→{0, 1}n,
for example, Ek = DESk. To encrypt a message m that is longer than n bits
we apply a mode of operation: The message m is decomposed into blocks
of some ﬁxed bit length r, m = m1m2 . . . ml, and then these blocks are
encrypted iteratively. The length r of the blocks mi is not in all modes of
operation equal to the block length n of the cipher. There are modes of
operation, where r can be smaller than n, for example, the cipher feedback

26
2. Symmetric-Key Encryption
and the output feedback modes below. In electronic code book mode and
cipher-block chaining mode, which we discuss ﬁrst, the block length r is
equal to the block length n of the block cipher.
If the block length r does not divide the length of our message, we have
to complete the last block. The last block is padded out with some bits.
After applying the decryption function, the receiver must remove the padding.
Therefore, he must know how many bits were added. This can be achieved,
for example, by storing the number of padded bits in the last byte of the last
block.
Electronic Codebook Mode. The electronic code book mode is the
straightforward mode. The encryption is deterministic – identical plaintext
blocks result in identical ciphertext blocks. The encryption works like a code-
book. Each block of m is encrypted independently of the other blocks. Trans-
mission bit errors in a single ciphertext block aﬀect the decryption only of
that block.
In this mode, we have r = n. The electronic codebook mode is imple-
mented by the following algorithm:
Algorithm 2.6.
bitString ecbEncrypt(bitString m)
1
divide m into m1 . . . ml
2
for i ←1 to l do
3
ci ←Ek(mi).
4
return c1 . . . cl
For decryption, the same algorithm can be used with the decryption function
E−1
k
in place of Ek.
If we encrypt many blocks, partial information about the plaintext is
revealed. For example, an eavesdropper Eve detects whether a certain block
repeatedly occurs in the sequence of plaintext blocks, or, more generally, she
can ﬁgure out how often a certain plaintext block occurs. Therefore, other
modes of operation are preferable.
Cipher-Block Chaining Mode. In this mode, we have r = n. Encryption
in the cipher-block chaining mode is implemented by the following algorithm:
Algorithm 2.7.
bitString cbcEncrypt(bitString m)
1
select c0 ∈{0, 1}n at random
2
divide m into m1 . . . ml
3
for i ←1 to l do
4
ci ←Ek(mi ⊕ci−1)
5
return c0c1 . . . cl
Choosing the initial value c0 at random prevents almost with certainty
that the same initial value c0 is used for more than one encryption. This is
important for security. Suppose for a moment that the same c0 is used for

2.2 Block Ciphers
27
two messages m and m′. Then, an eavesdropper Eve can immediately detect
whether the ﬁrst l blocks of m and m′ coincide, because in this case the ﬁrst
l ciphertext blocks are the same.
If a message is encrypted twice, then, with a very high probability, the
initial values are diﬀerent, and hence the resulting ciphertexts are distinct.
The ciphertext depends on the plaintext, the key and a randomly chosen
initial value. We obtain a randomized encryption algorithm.
Decryption in cipher-block chaining mode is implemented by the following
algorithm:
Algorithm 2.8.
bitString cbcDecrypt(bitString c)
1
divide c into c0c1 . . . cl
2
for i ←1 to l do
3
mi ←E−1
k (ci) ⊕ci−1
4
return m1 . . . ml
The cryptogram c = c0c1 . . . cl has one block more than the plaintext.
The initial value c0 needs not be secret, but its integrity must be guaranteed
in order to decrypt c1 correctly.
A transmission bit error in block ci aﬀects the decryption of the blocks ci
and ci+1. The block recovered from ci will appear random (here we assume
that even a small change in the input of a block cipher will produce a random-
looking output), while the plaintext recovered from ci+1 has bit errors pre-
cisely where ci did. The block ci+2 is decrypted correctly. The cipher-block
chaining mode is self-synchronizing, even if one or more entire blocks are lost.
A lost ciphertext block results in the loss of the corresponding plaintext block
and errors in the next plaintext block.
In both the electronic codebook mode and cipher-block chaining mode,
E−1
k
is applied for decryption. Hence, both modes are also applicable with
public-key encryption methods, where the computation of E−1
k
requires the
recipient’s secret, while Ek can be easily computed by everyone.
Cipher Feedback Mode. Let lsbl denote the l least signiﬁcant (rightmost)
bits of a bit string, msbl the l most signiﬁcant (leftmost) bits of a bit string,
and let || denote the concatenation of bit strings.
In the cipher feedback mode, we have 1 ≤r ≤n (recall that the plaintext
m is divided into blocks of length r). Let x1 ∈{0, 1}n be a randomly cho-
sen initial value. The cipher feedback mode is implemented by the following
algorithm:

28
2. Symmetric-Key Encryption
Algorithm 2.9.
bitString cfbEnCrypt(bitString m, x1)
1
divide m into m1 . . . ml
2
for i ←1 to l do
3
ci ←mi ⊕msbr(Ek(xi))
4
xi+1 ←lsbn−r(xi)||ci
5
return c1 . . . cl
We get a stream cipher in this way. The key stream is computed by using
Ek, and depends on the key underlying Ek, on an initial value x1 and on the
ciphertext blocks already computed. Actually, xi+1 depends on the ﬁrst ⌈n/r⌉
members7 of the sequence ci, ci−1, . . . , c1, x1. The key stream is obtained in
blocks of length r. The message can be processed bit by bit and messages
of arbitrary length can be encrypted without padding. If one block of the
key stream is consumed, the next block is computed. The initial value x1
is transmitted to the recipient. It does not need to be secret if Ek is the
encryption function of a symmetric cryptosystem (an attacker does not know
the key underlying Ek). The recipient can compute Ek(x1) – hence m1 and
x2 – from x1 and the cryptogram c1, then Ek(x2), m2 and x3, and so on.
For each encryption, a new initial value x1 is chosen at random. This
prevents almost with certainty that the same initial value x1 is used for
more than one encryption. As in every stream cipher, this is important for
security. If the same initial value x1 is used for two messages m and m′, then
an eavesdropper Eve immediately ﬁnds out whether the ﬁrst l blocks of m
and m′ coincide. In this case, the ﬁrst l blocks of the generated key stream,
and hence the ﬁrst l ciphertext blocks are the same for m and m′.
A transmission bit error in block ci aﬀects the decryption of that block and
the next ⌈n/r⌉ciphertext blocks. The block recovered from ci has bit errors
precisely where ci did. The next ⌈n/r⌉ciphertext blocks will be decrypted
into random-looking blocks (again we assume that even a small change in the
input of a block cipher will produce a random-looking output). The cipher
feedback mode is self-synchronizing after ⌈n/r⌉steps, even if one or more
entire blocks are lost.
Output Feedback Mode. As in the cipher feedback mode, we have 1 ≤
r ≤n. Let x1 ∈{0, 1}n be a randomly chosen initial value. The output
feedback mode is implemented by the following algorithm:
Algorithm 2.10.
bitString ofbEnCrypt(bitString m, x1)
1
divide m into m1 . . . ml
2
for i ←1 to l do
3
ci ←mi ⊕msbr(Ek(xi))
4
xi+1 ←Ek(xi)
5
return c1 . . . cl
7 ⌈x⌉denotes the smallest integer ≥x.

2.2 Block Ciphers
29
There are two diﬀerent output feedback modes discussed in the literature.
The one we introduced is considered to have better security properties and
was speciﬁed in [ISO/IEC 10116]. In the output feedback mode, plaintexts of
arbitrary length can be encrypted without padding. As in the cipher feedback
mode, the plaintext is considered as a bit stream and each bit is XORed with
a bit of a key stream. The key stream depends only on an initial value x1 and
is iteratively computed by xi+1 = Ek(xi). The initial value x1 is transmitted
to the recipient. It does not need to be secret if Ek is the encryption function
of a symmetric cryptosystem (an attacker does not know the key underlying
Ek). For decryption, the same algorithm can be used.
It is essential for security that the initial value is chosen randomly and
independently from the previous ones. This prevents almost with certainty
that the same initial value x1 is used for more than one encryption. If the same
initial value x1 is used for two messages m and m′, then identical key streams
are generated for m and m′, and an eavesdropper Eve immediately computes
the diﬀerence between m and m′ from the ciphertexts: m⊕m′ = c⊕c′. Thus,
it is strongly recommended to choose a new random initial value for each
message.
A transmission bit error in block ci only aﬀects the decryption of that
block. The block recovered from ci has bit errors precisely where ci did.
However, the output feedback mode will not recover from a lost ciphertext
block – all following ciphertext blocks will be decrypted incorrectly.
Security. We mentioned before that the electronic codebook mode has some
shortcomings. The question arises as to what amount the mode of operation
weakens the cryptographic strength of a block cipher. A systematic treatment
of this question can be found in [BelDesJokRog97]. First, a model for the
security of block ciphers is developed, the so-called pseudorandom function
model or pseudorandom permutation model.
As discussed in Section 2.2, ideally we would like to choose the encryption
function of a block cipher from the huge set of all permutations on {0, 1}n
in a truly random way. This approach might be called the “truly random
permutation model”. In practice, we have to follow the “pseudorandom per-
mutation model”: the encryption function is chosen randomly, but from a
much smaller family (Fk)k∈K of permutations on {0, 1}n, like DES.
In [BelDesJokRog97], the security of the cipher-block chaining mode is re-
duced to the security of the pseudorandom family (Fk)k∈K. Here, security of
the family (Fk)k∈K means that no eﬃcient algorithm is able to distinguish el-
ements randomly chosen from (Fk)k∈K from elements randomly chosen from
the set of all permutations. This notion of security for pseudorandom func-
tion families is analogously deﬁned as the notion of computationally perfect
pseudorandom bit generators, which will be studied in detail in Chapter 8.
[BelDesJokRog97] also consider a mode of operation similar to the output
feedback mode, called the XOR scheme, and its security is also reduced to
the security of the underlying pseudorandom function family.

30
2. Symmetric-Key Encryption
Exercises
1. The following algorithm computes the round keys ki, i = 1, . . . , 16, for
DES from the 64-bit key k. Only 56 of the 64 bits are used and permu-
tated. This is done by a map PC1. The result PC1(k) is divided into two
halves, C0 and D0, of 28 bits.
Algorithm 2.11.
bitString DESKeyGenerator(bitString k)
1
(C0, D0) ←PC1(k)
2
for i ←1 to 16 do
3
(Ci, Di) ←(LSi(Ci−1), LSi(Di−1))
4
ki ←PC2(Ci, Di)
5
return k1 . . . k16
Here LSi is a cyclic left shift by one position if i = 1, 2, 9 or 16, and by
two positions otherwise. The maps
PC1 : {0, 1}64 −→{0, 1}56, PC2 : {0, 1}56 −→{0, 1}48
are deﬁned by the tables
PC1
PC2
57
49
41
33
25
17
9
14
17
11
24
1
5
1
58
50
42
34
26
18
3
28
15
6
21
10
10
2
59
51
43
35
27
23
19
12
4
26
8
19
11
3
60
52
44
36
16
7
27
20
13
2
63
55
47
39
31
23
15
41
52
31
37
47
55
7
62
54
46
38
30
22
30
40
51
45
33
48
14
6
61
53
45
37
29
44
49
39
56
34
53
21
13
5
28
20
12
4
46
42
50
36
29
32
The tables are read line by line and describe how to get the images, i.e.,
PC1(x1, . . . , x64) = (x57, x49, x41, . . . , x12, x4),
PC2(x1, . . . , x56) = (x14, x17, x11, . . . , x29, x32).
The bits 8, 16, 24, 32, 40, 48, 56 and 64 of k are not used. They are
deﬁned in such a way that odd parity holds for each byte of k. A key k
is deﬁned to be weak if k1 = k2 = . . . = k16.
Show that exactly four weak keys exist, and determine these keys.
2. In this exercise, x denotes the bitwise complement of a bit string x. Let
DES : {0, 1}64 × {0, 1}56 −→{0, 1}64 be the DES function.
a. Show that DES(k, x) = DES(k, x), for k ∈{0, 1}56, x ∈{0, 1}64.
b. Let (m, DESk(m)) be a plaintext-ciphertext pair. We try to ﬁnd the
unknown key k by an exhaustive key search. Show that the number
of encryptions we have to compute can be reduced from 256 to 255 if
the pair (m, DESk(m)) is also known.

Exercises
31
3. The key stream in the output feedback mode is periodic, i.e., there exists
an i ∈N such that xi = x1. The lowest positive integer with this property
is called the period of the key stream. Let f be randomly chosen from
the set of all permutations on {0, 1}n. Show that the average period of
the key stream is 2n−1 + 1/2 if the initial value x1 ∈{0, 1}n is chosen at
random.

3. Public-Key Cryptography
The basic idea of public-key cryptography are public keys. Each person’s key
is separated into two parts: a public key for encryption available to everyone
and a secret key for decryption which is kept secret by the owner. In this
chapter we introduce the concept of public-key cryptography. Then we discuss
some of the most important examples of public-key cryptosystems, such as
the RSA, ElGamal and Rabin cryptosystems. These all provide encryption
and digital signatures.
3.1 The Concept of Public-Key Cryptography
Classical symmetric cryptography provides a secure communication channel
to each pair of users. In order to establish such a channel, the users must
agree on a common secret key. After establishing a secure communication
channel, the secrecy of a message can be guaranteed. Symmetric cryptography
also includes methods to detect modiﬁcations of messages and methods to
verify the origin of a message. Thus, conﬁdentiality and integrity can be
accomplished using secret key techniques.
However, public key techniques have to be used for a secure distribution
of secret keys, and at least some important forms of authentication and non-
repudiation also require public-key methods, such as digital signatures. A
digital signature should be the digital counterpart of a handwritten signa-
ture. The signature must depend on the message to be signed and a secret
known only to the signer. An unbiased third party should be able to verify
the signature without access to the signer’s secret.
In a public-key encryption scheme, the communication partners do not
share a secret key. Each user has a pair of keys: a secret key sk known only
to him and a public key pk known to everyone.
Suppose Bob has such a key pair (pk, sk) and Alice wants to encrypt a
message m for Bob. Like everyone else, Alice knows Bob’s public key pk. She
computes the ciphertext c = E(pk, m) by applying the encryption function
E with Bob’s public key pk. As before, we denote encrypting with a ﬁxed key
pk by Epk, i.e., Epk(m) := E(pk, m). Obviously, the encryption scheme can
only be secure if it is practically infeasible to compute m from c = Epk(m).

34
3. Public-Key Cryptography
But how can Bob then recover the message m from the ciphertext c? This
is where Bob’s secret key is used. The encryption function Epk must have
the property that the pre-image m of the ciphertext c = Epk(m) is easy to
compute using Bob’s secret key sk. Since only Bob knows the secret key, he
is the only one who can decrypt the message. Even Alice, who encrypted the
message m, would not be able to get m from Epk(m) if she lost m. Of course,
eﬃcient algorithms must exist to perform encryption and decryption.
We summarize the requirements of public-key cryptography. We are look-
ing for a family of functions (Epk)pk∈PK such that each function Epk is
computable by an eﬃcient algorithm. It should be practically infeasible to
compute pre-images of Epk. Such families (Epk)pk∈PK are called families of
one-way functions or one-way functions for short. Here, PK denotes the set
of available public keys.1 For each function Epk in the family, there should be
some information sk to be kept secret which enables an eﬃcient computation
of the inverse of Epk. This secret information is called the trapdoor informa-
tion. One-way functions with this property are called trapdoor functions.
In 1976, W. Diﬃe and M.E. Hellman published the idea of public-
key cryptography in their famous paper “New Directions in Cryptography”
([DifHel76]). They introduced a public-key method for key agreement which
is in use to this day. In addition, they described how digital signatures would
work, and proposed, as an open question, the search for such a function. The
ﬁrst public-key cryptosystem that could function as both a key agreement
mechanism and as a digital signature was the RSA cryptosystem published
in 1978 ([RivShaAdl78]). RSA is named after the inventors: R. Rivest, A.
Shamir and L. Adleman. The RSA cryptosystem provides encryption and
digital signatures and is the most popular and widely used public-key cryp-
tosystem today. We shall describe the RSA cryptosystem in Section 3.3. It is
based on the diﬃculty of factoring large numbers, which enables the construc-
tion of one-way functions with a trapdoor. Another basis for one-way func-
tions is the diﬃculty of extracting discrete logarithms. These two problems
from number theory are the foundations of most public-key cryptosystems
used today.
Each participant in a public-key cryptosystem needs his personal key
k = (pk, sk), consisting of a public and a secret (also called private) part. To
guarantee the security of the cryptosystem, it must be infeasible to compute
the secret key sk from the public key pk, and it must be possible to randomly
choose the keys k from a huge parameter space. An eﬃcient algorithm must
be available to perform this random choice. If Bob wants to participate in
the cryptosystem, he randomly selects his key k = (pk, sk), keeps sk secret
and publishes pk. Now everyone can use pk in order to encrypt messages for
Bob.
To discuss the basic idea of digital signatures, we assume that we have
a family (Epk)pk∈PK of trapdoor functions and that each function Epk is
1 A rigorous deﬁnition of one-way functions is given in Deﬁnition 6.12.

3.2 Modular Arithmetic
35
bijective. Such a family of trapdoor permutations can be used for digital
signatures. Let pk be Alice’s public key. To compute the inverse E−1
pk of Epk,
the secret key sk of Alice is required. So Alice is the only one who is able
to do this. If Alice wants to sign a message m, she computes E−1
pk (m) and
takes this value as signature s of m. Everyone can verify Alice’s signature s
by using Alice’s public key pk and computing Epk(s). If Epk(s) = m, Bob is
convinced that Alice really signed m because only Alice was able to compute
E−1
pk (m).
An important straightforward application of public-key cryptosystems is
the distribution of session keys. A session key is a secret key used in a classical
symmetric encryption scheme to encrypt the messages of a single communica-
tion session. If Alice knows Bob’s public key, then she may generate a session
key, encrypt it with Bob’s public key and send it to Bob. Digital signatures
are used to guarantee the authenticity of public keys by certiﬁcation author-
ities. The certiﬁcation authority signs the public key of each user with her
secret key. The signature can be veriﬁed with the public key of the certiﬁca-
tion authority. Cryptographic protocols for user authentication and advanced
cryptographic protocols, like bit commitment schemes, oblivious transfer and
zero-knowledge interactive proof systems, have been developed. Today they
are fundamental to Internet communication and electronic commerce.
Public-key cryptography is also important for theoretical computer sci-
ence: theories of security were developed and the impact on complexity theory
should be mentioned.
3.2 Modular Arithmetic
In this section, we give a brief overview of the modular arithmetic necessary
to understand the cryptosystems we discuss in this chapter. Details can be
found in Appendix A.
3.2.1 The Integers
Let Z denote the ordered set {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}. The elements
of Z are called integers or numbers. The integers greater than 0 are called
natural numbers and are denoted by N. The sum n+m and the product n·m
of integers are deﬁned. Addition and multiplication satisfy the axioms of a
commutative ring with a unit element. We call Z the ring of integers.
Addition, Multiplication and Exponentiation. Eﬃcient algorithms ex-
ist for the addition and multiplication of numbers.2 An eﬃcient algorithm is
an algorithm whose running time is bounded by a polynomial in the size of its
2 Simplifying slightly, we only consider non-negative integers, which is suﬃcient
for all our purposes.

36
3. Public-Key Cryptography
input. The size of a number is the length of its binary encoding, i.e., the size
of n ∈N is equal to ⌊log2(n)⌋+1.3 It is denoted by |n|. Let a, b ∈N, a, b ≤n,
and k := ⌊log2(n)⌋+ 1. The number of bit operations for the computation of
a + b is O(k), whereas for the multiplication a · b it is O(k2). Multiplication
can be improved to O(k log2(k)) if a fast multiplication algorithm is used.
Exponentiation is also an operation that occurs often. The repeated squar-
ing method (Algorithm A.26) yields an eﬃcient algorithm for the computa-
tion of an. It requires at most 2 · |n| modular multiplications. We compute,
for example, a16 by (((a2)2)2)2, which are four squarings, in contrast to the
15 multiplications that are necessary for the naive method. If the exponent
is not a power of 2, the computation has to be modiﬁed a little. For example,
a14 = ((a2a)2a)2 is computable by three squarings and two multiplications,
instead of by 13 multiplications.
Division with Remainder. If m and n are integers, m ̸= 0, we can divide
n by m with a remainder. We can write n = q · m + r in a unique way such
that 0 ≤r < abs(m).4 The number q is called the quotient and r is called the
remainder of the division. They are unique. Often we denote r by a mod b.
An integer m divides an integer n if n is a multiple of m, i.e., n = mq
for an integer q. We say, m is a divisor or factor of n. The greatest common
divisor gcd (m, n) of numbers m, n ̸= 0 is the largest positive integer dividing
m and n. gcd(0, 0) is deﬁned to be zero. If gcd(m, n) = 1, then m is called
relatively prime to n, or prime to n for short.
The Euclidean algorithm computes the greatest common divisor of two
numbers and is one of the oldest algorithms in mathematics:
Algorithm 3.1.
int gcd(int a, b)
1
while b ̸= 0 do
2
r ←a mod b
3
a ←b
4
b ←r
5
return abs(a)
The algorithm computes gcd(a, b) for a ̸= 0 and b ̸= 0. It terminates
because the non-negative number r decreases in each step. Note that gcd(a, b)
is invariant in the while loop, because gcd(a, b) = gcd(b, a mod b). In the last
step, the remainder r becomes 0 and we get gcd(a, b) = gcd(a, 0) = abs(a).
Primes and Factorization. A natural number p ̸= 1 is a prime number,
or simply a prime, if 1 and p are the only divisors of p. If a number n ∈N
is not prime, it is called composite. Primes are essential for setting up the
public-key cryptosystems we describe in this chapter. Fortunately, there are
very fast algorithms (so-called probabilistic primality tests) for ﬁnding – at
3 ⌊x⌋denotes the greatest integer less than or equal to x.
4 abs(m) denotes the absolute value of m.

3.2 Modular Arithmetic
37
least with a high probability (though not with mathematical certainty) –
the correct answer to the question whether a given number is prime or not
(see Appendix A.8). Primes are the basic building blocks for numbers. This
statement is made precise by the Fundamental Theorem of Arithmetic.
Theorem 3.2 (Fundamental Theorem of Arithmetic). Let n ∈N, n ≥2.
There exist pairwise distinct primes p1, . . . , pk and exponents e1, . . . , ek ∈
N, ei ≥1, i = 1, . . . , k, such that
n =
k
Y
i=1
pei
i .
The primes p1, . . . , pk and exponents e1, . . . , ek are unique.
It is easy to multiply two numbers, but the design of an eﬃcient algorithm
for calculating the prime factors of a number is an old mathematical problem.
For example, this was already studied by the famous mathematician C.F.
Gauß about 200 years ago (see, e.g., [Riesel94] for details on Gauß’ factoring
method). However, to this day, we do not have a practical algorithm for
factoring extremely large numbers.
3.2.2 The Integers Modulo n
The Residue Class Ring Modulo n. Let n be a positive integer. Let a
and b be integers. Then a is congruent to b modulo n, written a ≡b mod n,
if a and b leave the same remainder when divided by n or, equivalently, if n
divides a −b. We obtain an equivalence relation. The equivalence class of a
is the set of all numbers congruent to a. It is denoted by [a] and called the
residue class of a modulo n. The set of residue classes {[a] | a ∈Z} is called
the set of integers modulo n and is denoted by Zn.
Each number is congruent to a unique number r in the range 0 ≤r ≤
n −1. Therefore the numbers 0, . . . , n −1 form a set of representatives of the
elements of Zn. We call them the natural representatives.
The equivalence relation is compatible with addition and multiplication
in Z, i.e., if a ≡a′ mod n and b ≡b′ mod n, then a + b ≡(a′ + b′) mod n
and a · b ≡(a′ · b′) mod n. Consequently, addition and multiplication on Z
induce an addition and multiplication on Zn:
[a] + [b] := [a + b],
[a] · [b] := [a · b].
Addition and multiplication satisfy the axioms of a commutative ring with a
unit element. We call Zn the residue class ring modulo n.
Although we can calculate in Zn as in Z, there are some important diﬀer-
ences. First, we do not have an ordering of the elements of Zn which is compat-
ible with addition and multiplication. For example, if we assume that we have

38
3. Public-Key Cryptography
such an ordering in Z5 and that [0] < [1], then [0] < [1]+[1]+[1]+[1]+[1] = [0],
which is a contradiction. A similar calculation shows that the assumption
[1] < [0] also leads to a contradiction.
Another fact is that [a] · [b] can be [0] for [a] ̸= [0] and [b] ̸= [0]. For
example, [2] · [3] = [0] in Z6. Such elements – [a] and [b] – are called zero
divisors.
The Prime Residue Class Group Modulo n. In Z, elements a and b
satisfy a·b = 1 if and only if both a and b are equal to 1, or both are equal to
-1. We say that 1 and -1 have multiplicative inverse elements. In Zn, this can
happen more frequently. In Z5, for example, every class diﬀerent from [0] has
a multiplicative inverse element. Elements in a ring which have multiplicative
inverses are called units and form a group under multiplication.
An element [a] in Zn has the multiplicative inverse element [b], if ab ≡
1 mod n or, equivalently, n divides 1 −ab. This means we have an equation
nm + ab = 1, with suitable m. The equation implies that gcd(a, n) = 1. On
the other hand, if numbers a, n with gcd(a, n) = 1 are given, an equation
nm + ab = 1, with suitable b and m, can be derived from a and n by the
extended Euclidean algorithm (Algorithm A.5). Hence, [a] is a unit in Zn and
the inverse element is [b]. Thus, the elements of the group of units of Zn are
represented by the numbers prime to n.
Z∗
n := {[a] | 1 ≤a ≤n −1 and gcd(a, n) = 1}
is called the prime residue class group modulo n. The number of elements
in Z∗
n (also called the order of Z∗
n) is the number of integers in the interval
[1, n−1] which are prime to n. This number is denoted by ϕ(n). The function
ϕ is called the Euler phi function or the Euler totient function.
For every element a in a ﬁnite group G, we have a|G| = e, with e being
the neutral element of G.5 This is an elementary and easy to prove feature
of ﬁnite groups. Thus, we have for a number a prime to n
aϕ(n) ≡1 mod n.
This is called Euler’s Theorem or, if n is a prime, Fermat’s Theorem.
If Qk
i=1 pei
i
is the prime factorization of n, then the Euler phi function
can be computed by the formula (see Corollary A.30)
ϕ(n) = n
k
Y
i=1
µ
1 −1
pi
¶
.
If p is a prime, then every integer in {1, . . . , p −1} is prime to p. Therefore,
every element in Zp \ {0} is invertible and Zp is a ﬁeld. The group of units
Z∗
p is a cyclic group with p −1 elements, i.e., Z∗
p = {g, g2, . . . , gp−1 = [1]}
5 |G| denotes the number of elements of G (called the cardinality or order of G).

3.2 Modular Arithmetic
39
for some g ∈Zp−1. Such a g is called a generator of Z∗
p. Generators are also
called primitive elements modulo p or primitive roots modulo p (see Deﬁnition
A.37).
We can now introduce three functions which may be used as one-way
functions and which are hence very important in cryptography.
Discrete Exponentiation. Let p denote a prime number and g be a prim-
itive root in Zp.
Exp : Zp−1 −→Z∗
p, x 7−→gx
is called the discrete exponential function. Exp is a homomorphism from
the additive group Zp−1 to the multiplicative group Z∗
p, i.e., Exp(x + y) =
Exp(x) · Exp(y), and Exp is bijective. In other words, Exp is an isomorphism
of groups. This follows immediately from the deﬁnition of a primitive root.
The inverse function
Log : Z∗
p −→Zp−1
is called the discrete logarithm function. We use the adjective “discrete” to
distinguish Exp and Log for ﬁnite groups from the classical functions deﬁned
for the reals.
Exp is eﬃciently computable, for example by the repeated squaring
method (see Section 3.2.1), whereas no eﬃcient algorithm is known to ex-
ist for computing the inverse function Log for suﬃciently large primes p.
This statement is made precise by the discrete logarithm assumption (see
Deﬁnition 6.1).
Modular Powers. Let n denote the product of two distinct primes p and q
and let e be prime to ϕ(n).
RSAe : Zn −→Zn, x 7−→xe
is called the RSA function.
Proposition 3.3. Using the same notation as above, let d be a multiplicative
inverse element of e modulo ϕ(n) (note that d is also prime to ϕ(n) and RSAd
is deﬁned). Then
RSAd ◦RSAe = RSAe ◦RSAd = idZn.
Proof. We show xed = x, for x ∈Zn. First let x ∈Z∗
n. The group Z∗
n has order
ϕ(n), hence xϕ(n) = [1] and therefore xed = xed mod ϕ(n) = x. In the case x ̸∈
Z∗
n, p or q is a factor of x. If both divide x, we have x = 0 and xed = 0. Thus
the equalities hold. Observe that ϕ(n) = (p−1)(q−1) (see Corollary A.30). If
p divides x and q does not divide x, then (xe)d mod p = 0, x mod p = 0 and
(xe)d ≡xed mod (q−1) ≡x mod q, because ed ≡1 mod (q −1). This shows
that (xe)d ≡x mod n. The case where p does not divide x and q divides x

40
3. Public-Key Cryptography
follows analogously. Thereby (xe)d = xed = x for all x ∈Zn, and we have
proven our assertion.
2
We see that RSAe is an (easily computable) permutation of Zn. Knowing
d, it is also easy to compute the inverse, which is simply RSAd. However, if
d is a secret, it is believed to be infeasible to invert RSAe (provided that p
and q are very large).
Modular Squares. Let p and q denote distinct prime numbers and n = pq.
Square : Zn −→Zn, x 7−→x2
is called the Square function. Each element y ∈Zn, y ̸= 0, either has 0, 2
or 4 pre-images. If both p and q are primes ≡3 mod 4, then -1 is not a
square modulo p and modulo q, and it easily follows that Square becomes a
bijective map by restricting the domain and range to the subset of squares in
Z∗
n (for details see Appendix A.6). If the factors of n are known, pre-images
of Square (called square roots) are eﬃciently computable (see Proposition
A.62). Again, without knowing the factors p and q, computing square roots
is practically impossible for p, q suﬃciently large.
On the Diﬃculty of Extracting Roots and Discrete Logarithms. Let
g, k ∈N, g, k ≥2, and let
F : Z −→Z
denote one of the maps x 7−→x2, x 7−→xk or x 7−→gx. Usually these maps
are considered as real functions. Eﬃcient algorithms for the computation of
values and pre-images are known. These algorithms rely heavily on the order-
ing of the reals and the fact that F is, at least piecewise, monotonic. The map
F is eﬃciently computable by integer arithmetic with a fast exponentiation
algorithm (see Section 3.2.1). If we use such an algorithm to compute F, we
get the following algorithm for computing a pre-image x of y = F(x).
For simplicity, we restrict the domain to the positive integers. Then all
three functions are monotonic and hence injective. It is easy to ﬁnd integers
a and b with a ≤x ≤b. If F(a) ̸= y and F(b) ̸= y, we call FInvers(y, a, b).
Algorithm 3.4.
int FInvers(int y, a, b)
1
repeat
2
c ←(a + b) div 2
3
if F(c) < y
4
then a ←c
5
else b ←c
6
until F(c) = y
7
return c

3.3 RSA
41
F
is eﬃciently computable. The repeat-until loop terminates after
O(log2(|b −a|)) steps. Hence, FInvers is also eﬃciently computable, and we
see that F considered as a function from Z to Z can easily be inverted in an
eﬃcient way.
Now consider the same maps modulo n. The function F is still eﬃciently
computable (see above or Algorithm A.26). The algorithm FInvers, however,
does not work in modular arithmetic. The reason is that in modular arith-
metic, it does not make sense to ask in line 3 whether F(c) < y. The ring Zn
has no order which is compatible with the arithmetic operations. The best we
could do to adapt the algorithm above for modular arithmetic is to test all
elements c in Zn until we reach F(c) = y. However, this leads to an algorithm
with exponential running time (n is exponential in the binary length |n| of
n).
If the factors of n are kept secret, no eﬃcient algorithm is known today
to invert RSA and Square. The same holds for Exp. It is widely believed
that no eﬃcient algorithms exist to compute the pre-images. No one could
prove, however, this statement in the past. These assumptions are deﬁned
in detail in Chapter 6. They are the basis for security proofs in public-key
cryptography (Chapters 9 and 10).
3.3 RSA
The RSA cryptosystem is based on facts from elementary number theory
which have been known for 250 years. To set up an RSA cryptosystem, we
have to multiply two very large primes and make their product n public. n is
part of the public key, whereas the factors of n are kept secret and are used
as the secret key. The basic idea is that the factors of n cannot be recovered
from n. In fact, the security of the RSA encryption function depends on the
tremendous diﬃculty of factoring, but the equivalence is not proven.
We now describe in detail how RSA works. We discuss key generation,
encryption and decryption as well as digital signatures.
3.3.1 Key Generation and Encryption
Key Generation. Each user Alice of the RSA cryptosystem has her own
public and secret keys. The key generation algorithm proceeds in three steps
(see also Section 6.4):
1. Choose large distinct primes p and q, and compute n = p · q.
2. Choose e that is prime to ϕ(n). The pair (n, e) is published as the public
key.
3. Compute d with ed ≡1 mod ϕ(n). (n, d) is used as the secret key.

42
3. Public-Key Cryptography
Recall that ϕ(n) = (p −1)(q −1) (Corollary A.30). The numbers n, e and
d are referred to as the modulus, and encryption and decryption exponents,
respectively. To decrypt a ciphertext or to generate a digital signature, Alice
only needs her decryption exponent d, she does not need to know the primes
p and q. Nevertheless, knowing p and q can be helpful for her (e.g. to speed
up decryption, see below). At any time, Alice can derive the primes from n, e
and d by an eﬃcient algorithm with very high probability (see Exercise 4).
An adversary should not have the slightest idea what Alice’s primes are.
Therefore, we proceed as follows to get the primes p and q. First we choose
a large number x at random. If x is even, we replace x by x + 1 and apply
a probabilistic primality test to check whether x is a prime (see Appendix
A.8). If x is not a prime number, we replace x with x + 2, and so on until
the ﬁrst prime is reached. We expect to test O(ln(x)) numbers for primality
before reaching the ﬁrst prime (see Corollary A.69). The method described
does not produce primes with mathematical certainty (we use a probabilistic
primality test), but it is suﬃcient for practical purposes. At the moment, it
is suggested to take 512-bit prime numbers. No one can predict for how long
such numbers will be secure, because it is diﬃcult to predict improvements
in factorization algorithms and computer technology.
The number e can also be chosen at random. Whether e is prime to
ϕ(n) is tested with Euclid’s algorithm (Algorithm A.4). Another method for
obtaining e is to choose a prime between max(p, q) and ϕ(n) which guarantees
that it will be relatively prime to ϕ(n). We can do this in the same way
as choosing p and q. The number d can be computed with the extended
Euclidean algorithm (Algorithm A.5).
To choose a number at random, we may use a pseudorandom number
generator. This is an algorithm that generates a sequence of digits which
look like a sequence of random digits. There is a wide array of literature
concerning eﬃcient and secure generation of pseudorandom numbers (see,
e.g., [MenOorVan96]). We also discuss the subject in Chapter 8.
Encryption and Decryption. We encrypt messages in {0, . . . , n−1}, con-
sidered as elements of Zn.
1. The encryption function is deﬁned by
E : Zn −→Zn, x 7−→xe.
2. The decryption function is of the same type, and is deﬁned by
D : Zn −→Zn, x 7−→xd.
E and D are bijective maps and inverse to each other: E ◦D = D ◦E = idZn
(see Proposition 3.3). Encryption and decryption can be implemented using
an eﬃcient algorithm (Algorithm A.26).
With the basic encryption procedure, we can encrypt bit sequences up
to k := ⌊log2(n)⌋bits. If our messages are longer, we may decompose them

3.3 RSA
43
into blocks of length k and apply the scheme described in Section 3.3.4 or a
suitable mode of operation, for example the electronic codebook mode or the
cipher-block chaining mode (see Section 2.2.3). The cipher feedback mode
and the output feedback mode are not immediately applicable. Namely, if
the initial value is not kept secret everyone can decrypt the cryptogram. See
Chapter 9 for an application of the output feedback mode with RSA.
Security. An adversary knowing the factors p and q of n also knows ϕ(n) =
(p −1)(q −1), and then derives d from the public encryption key e using the
extended Euclidean algorithm (Algorithm A.5). Thus, the security of RSA
depends on the diﬃculty of ﬁnding the factors p and q of n, if p and q are
large primes. It is widely believed that it is impossible today to factor n by an
eﬃcient algorithm if p and q are suﬃciently large. This fact is known as the
factoring assumption (for a precise deﬁnition, see Deﬁnition 6.9). An eﬃcient
factoring algorithm would break RSA. It is not proven whether factoring is
necessary to decrypt RSA ciphertexts, but it is also believed that inverting
the RSA function is intractable. This statement is made precise by the RSA
assumption (see Deﬁnition 6.7).
In the construction of the RSA keys, the only inputs for the computation
of the exponents e and d are ϕ(n) and n. Since ϕ(n) = (p −1)(q −1), we
have
p + q = n −ϕ(n) + 1 and p −q =
p
(p + q)2 −4n (if p > q).
Therefore, it is easy to compute the factors of n if ϕ(n) is known.
The factorization of n can be reduced to an algorithm A that computes d
from n and e (see Exercise 4). The resulting factoring algorithm A′ is prob-
abilistic and of the same complexity as A. (This fact was already mentioned
in [RivShaAdl78]).
It is an open question as to whether an eﬃcient algorithm for factoring can
be derived from an eﬃcient algorithm inverting RSA, i.e., an eﬃcient algo-
rithm that on inputs n, e and xe outputs x. A result of Boneh and Venkatesan
(see [BonVen98]) provides evidence that, for a small encryption exponent e,
inverting the RSA function might be easier than factoring n. They show that
an eﬃcient factoring algorithm A which uses, as a subroutine, an algorithm
for computing e-th roots – called oracle for e-th roots – can be converted into
an eﬃcient factoring algorithm B which does not call the oracle. However,
they use a restricted computation model for the algorithm A – only alge-
braic reductions are allowed. Their result says that factoring is easy, if an
eﬃcient algorithm like A exists in the restricted computational model. The
result of Boneh and Venkatesan does not expose any weakness in the RSA
cryptosystem.
The decryption exponent d should be greater than n1/4. For d < n1/4, a
polynomial-time algorithm to compute d has been developed ([Wiener90]).
The algorithm uses the continued fraction expansion of e/n.

44
3. Public-Key Cryptography
Eﬃcient factoring algorithms are known for special types of primes p and
q. To give these algorithms no chance, we have to avoid such primes. First
we require that the absolute value |p−q| is large. This prevents the following
attack: We have (p + q)2/4−n = (p + q)2/4−pq = (p −q)2/4. If |p−q| is small,
then (p −q)2/4 is also small and therefore (p + q)2/4 is slightly larger than n.
Thus p + q/2 is slightly larger than √n and the following factoring method
could be successful:
1. Choose successive numbers x > √n and test whether x2 −n is a square.
2. In this case, we have x2 −n = y2. Thus x2 −y2 = (x −y)(x + y) = n,
and we have found a factorization of n.
This idea for factoring numbers goes back to Fermat.
To prevent other attacks on the RSA cryptosystem, the notion of strong
primes has been deﬁned. A prime number is called strong if the following
conditions are satisﬁed:
1. p −1 has a large prime factor, denoted by r.
2. p + 1 has a large prime factor.
3. r −1 has a large prime factor.
What “large” means can be derived from the attacks to prevent (see below).
Strong primes can be generated by Gordon’s algorithm (see [Gordon84]). If
used in conjunction with a probabilistic primality test, the running time of
Gordon’s algorithm is only about 20% more than the time needed to generate
a prime factor of the RSA modulus in the way described above. Gordon’s
algorithm yields a prime with high probability. The size of the resulting prime
p can be controlled to guarantee a large absolute value |p −q|.
Strong primes are intended to prevent the p −1 and the p + 1 factoring
attacks. These are eﬃcient if p −1 or p + 1 have only small prime factors
(see, e.g., [Forster96]; [Riesel94]). Note that p −1 and p + 1 can be expected
to have a large prime factor if the prime p is chosen large and at random.
Moreover, choosing strong primes does not increase the protection against
factoring attacks with a modern algorithm like the number-ﬁeld sieve (see
[Cohen95]). Thus, the notion of strong primes has lost signiﬁcance.
There is another attack which should be prevented by strong primes:
decryption by iterated encryption. The idea is to repeatedly apply the en-
cryption algorithm to the cryptogram until c = cei. Then c =
³
cei−1´e
, and
the plaintext m = cei−1 can be recovered. Condition 1 and 3 ensure that
this attack fails, since the order of c in Z∗
n and the order of e in Z∗
ϕ(n) are,
with high probability, very large (see Exercises 6 and 7). If p and q are cho-
sen at random and are suﬃciently large, then the probability of success of a
decryption-by-iterated-encryption attack is negligible (see [MenOorVan96], p.
313). Thus, to prevent this attack, there is no compelling reason for choosing
strong primes, too.

3.3 RSA
45
Speeding Up Encryption and Decryption. The modular exponentia-
tion algorithm is especially eﬃcient, if the exponent has many zeros in its
binary encoding. For each zero we have one less multiplication. We can take
advantage of this fact by choosing an encryption exponent e with many zeros
in its binary encoding. The primes 3, 17 or 216 + 1 are good examples, with
only two ones in their binary encoding.
The eﬃciency of decryption can be improved by use of the Chinese Re-
mainder Theorem (Theorem A.29). The receiver of the message knows the
factors p and q of the modulus n. Let φ be the isomorphism
φ : Zn −→Zp × Zq, [x] 7−→([x mod p], [x mod q]).
Compute cd = φ−1(φ(cd)) = φ−1((c mod p)d, (c mod q)d). The computation
of (c mod p)d and (c mod q)d is executed in Zp and Zq, respectively. In Zp and
Zq we have much smaller numbers than in Zn. Moreover, the decryption expo-
nent d can be replaced by d mod (p−1) and d mod (q −1), respectively, since
(c mod p)d mod p = (c mod p)d mod (p−1) mod p and (c mod q)d mod q =
(c mod q)d mod (q−1) mod q (by Proposition A.24, also see “Computing mod-
ulo a prime” on page 303).
3.3.2 Digital Signatures
The RSA cryptosystem may also be used for digital signatures. Let (n, e) be
the public key and d be the secret decryption exponent of Alice. We ﬁrst
discuss signing messages that are encoded by numbers m ∈{0, . . . , n−1}. As
usual, we consider those numbers as the elements of Zn, and the computations
are done in Zn.
Signing. If Alice wants to sign a message m, she uses her secret key and
computes her signature σ = md of m by applying her decryption algorithm.
We call (m, σ) a signed message.
Veriﬁcation. Assume that Bob received a signed message (m, σ) from Alice.
To verify the signature, Bob uses the public key of Alice and computes σe.
He accepts the signature if σe = m.
If Alice signed the message, we have (md)e = (E ◦D)(m) = m and Bob
accepts (see Proposition 3.3). However, the converse is not true. It might
happen that Bob accepts a signature not produced by Alice. Suppose Eve
uses Alice’s public key, computes me and says that (me, m) is a message
signed by Alice. Everyone verifying Alice’s signature gets me = me and is
convinced that Alice really signed the message me. The message me is not
likely to be meaningful if the message belongs to some natural language.
This example shows that RSA signatures can be existentially forged. This
means that an adversary can forge a signature for some message, but not for
a message of his choice.

46
3. Public-Key Cryptography
Another attack uses the fact that the RSA encryption and decryption
functions are ring homomorphisms (see Appendix A.3). The image of a prod-
uct is the product of the images and the image of the unit element is the unit
element. If Alice signed m1 and m2, then the signatures for m1m2 and m−1
1
are σ1σ2 and σ−1
1 . These signatures can easily be computed without the secret
key. We will now discuss how to overcome these diﬃculties.
Signing with Redundancy and Hash Functions. If the messages to be
signed belong to some natural language, it is very unlikely that the above
attacks will succeed. The messages me and m1m2 will rarely be meaningful.
When embedding the messages into {0, 1}∗, the message space is sparse. The
probability that a randomly chosen bit string belongs to the message space
is small. By adding redundancy to each message we can always guarantee
that the message space is sparse, even if arbitrary bit strings are admissible
messages. A possible redundancy function is
R : {0, 1}∗−→{0, 1}∗, x 7−→x||x.
This principle is also used in error-detection and error-correction codes. Dou-
bling the message we can detect transmission errors if the ﬁrst half of the
transmitted message does not match the second half. The redundancy func-
tion R has the additional advantage that the composition of R with the RSA
function no longer preserves products.
If the message does not need to be recovered from the signature, another
approach to prevent the attacks is the use of a hash function (see Section
3.4).
3.3.3 Attacks Against RSA
We now describe attacks not primarily directed against the RSA algorithm
itself, but against the environment in which the RSA cryptosystem is used.
The Common-Modulus Attack. Suppose two users Bob and Bridget of
the RSA cryptosystem have the same modulus n. Let (n, e1) be the public key
of Bob and (n, e2) be the public key of Bridget, and assume that e1 and e2 are
relatively prime. Let m ∈Zn be a message sent to both Bob and Bridget and
encrypted as ci = mei, i = 1, 2. The problem now is that the plaintext can be
computed from c1, c2, e1, e2 and n. Since e1 is prime to e2, integers r and s
with re1+se2 = 1 can be derived by use of the extended Euclidean algorithm
(see Algorithm A.5). Either r or s, say r, is negative. If c1 /∈Z∗
n, we can factor
n by computing gcd(c1, n), thereby breaking the cryptosystem. Otherwise we
again apply the extended Euclidean algorithm and compute c−1
1 . We can
recover the message m using (c−1
1 )−rcs
2 = (me1)r(me2)s = mre1+se2 = m.
Thus, the cryptosystem fails to protect a message m if it is sent to two users
with common modulus whose encryption exponents are relatively prime.
With common moduli, secret keys can be recovered. If Bob and Brid-
get have the same modulus n, then Bob can determine Bridget’s secret key.

3.3 RSA
47
Namely, either Bob already knows the prime factors of n or he can compute
them from his encryption and decryption exponents, with a very high proba-
bility (see Exercise 4). Therefore, common moduli should be avoided in RSA
cryptosystems – each user should have his own modulus. If the prime factors
(and hence the modulus) are randomly chosen, as described above, then the
probability that two users share the same modulus is negligibly small.
Low-Encryption-Exponent Attack. Suppose that the RSA cryptosystem
will be used for k users, and each user has a small encryption exponent. We
discuss the case of three users, Bob, Bridget and Bert, with public keys (ni, 3),
i = 1, 2, 3. Of course, the moduli ni and nj must satisfy gcd(ni, nj) = 1,
for i ̸= j, since otherwise factoring of ni and nj is possible by computing
gcd(ni, nj). We assume that Alice sends the same message m to Bob, Bridget
and Bert. The following attack is possible: let c1 := m3 mod n1, c2 := m3 mod
n2 and c3 := m3 mod n3. The inverse of the Chinese Remainder isomorphism
(see Theorem A.29)
φ : Zn1n2n3 −→Zn1 × Zn2 × Zn3
can be used to compute m3 mod n1n2n3. Since m3 < n1n2n3, we can get m
by computing the ordinary cube root of m3 in Z.
Small-Message-Space Attack. If the number of all possible messages is
small, and if these messages are known in advance, an adversary can encrypt
all messages with the public key. He can decrypt an intercepted cryptogram
by comparing it with the precomputed cryptograms.
A Chosen-Ciphertext Attack. In the ﬁrst phase of a chosen-ciphertext
attack against an encryption scheme (see Section 1.3), adversary Eve has
access to the decryption device. She obtains the plaintexts for ciphertexts of
her choosing. Then, in the second phase, she attempts to decrypt another
ciphertext for which she did not request decryption in the ﬁrst phase.
Basic RSA encryption does not resist the following chosen-ciphertext at-
tack. Let (n, e) be Bob’s public RSA key, and let c be any ciphertext, en-
crypted with Bob’s public key.
To ﬁnd the plaintext m of c, adversary Eve ﬁrst chooses a random unit
r ∈Z∗
n and requests the decryption of the random-looking message
rec mod n.
She obtains the plaintext ˜m = rm mod n. Then, in the second phase of the
attack, Eve easily derives the plaintext m, because we have in Zn
r−1 ˜m = r−1rm = m.
The attack relies on the fact that the RSA function is a ring isomorphism.

48
3. Public-Key Cryptography
Analogously, there is a chosen-plaintext attack against digital signatures6
generated with basic RSA (i.e., RSA without a hash function). Eve can suc-
cessfully forge Bob’s signature for a message m, if she is ﬁrst supplied with
a valid signature for rem mod n, where r is randomly chosen by Eve.
A setting in which the chosen-ciphertext attack against RSA encryption
may work is described in the following attack.
Attack on Encryption and Signing with RSA. The attack is possible if
Bob has only one public-secret RSA key pair and uses this key pair for both
encryption and digital signatures. Assume that the cryptosystem is also used
for mutual authentication. On request, Bob proves his identity to Eve by
signing a random number, supplied by Eve, with his secret key. Eve veriﬁes
the signature of Bob with his public key. In this situation, Eve can successfully
attack Bob as follows. Suppose Eve intercepts a ciphertext c intended for Bob:
1. Eve selects r ∈Z∗
n at random.
2. Eve computes x = rec mod n, where (n, e) is the public key of Bob. She
sends x to Bob to get a signature xd (d the secret key of Bob). Note that
x looks like a random number to Bob.
3. Eve computes r−1xd, which is the plaintext of c.
Bleichenbacher’s 1-Million-Chosen-Ciphertext Attack. The 1-Million-
Chosen-Ciphertext Attack of Bleichenbacher ([Bleichenbacher98]) is an at-
tack against PKCS#1 v1.5 ([RFC 2313]).7 The widely used PKCS#1 is part
of the Public-Key Cryptography Standards series PKCS. These are de facto
standards that are developed and published by RSA Security ([RSALabs]) in
conjunction with system developers worldwide. The RSA standard PKCS#1
deﬁnes mechanisms for encrypting and signing data using the RSA public
key system. We explain Bleichenbacher’s attack against encryption.
Let (n, e) be a public RSA key with encryption exponent e and modulus
n. The modulus n is assumed to be a k-byte integer, i.e., 256k−1 < n <
256k. PKCS#1 deﬁnes a padding format. Messages (which are assumed to
be shorter than k bytes) are padded out to obtain a formatted plaintext block
m consisting of k bytes, and this plaintext block is then encrypted by using
the RSA function. The ciphertext is me mod n, as usual. The ﬁrst byte of the
plaintext block m is 00, and the second byte is 02 (in hexadecimal notation).
Then a padding string follows. It consists of at least 8 randomly chosen bytes,
diﬀerent from 00. The end of the padding block is marked by the zero byte
00. Then the original message bytes are appended. After the padding, we get
m = 00||02||padding string||00||original message.
The leading 00-byte ensures that the plaintext block, when converted to an
integer, is less than the modulus.
6 A detailed discussion of types of attacks against digital signature schemes is
given in Section 10.1.
7 PKCS#1 has been updated. The current version is Version 2.1 ([RFC 3447]).

3.3 RSA
49
We call a k-byte message m PKCS conforming, if it has the above format.
A message m ∈Z is PKCS conforming, if and only if
2B ≤m ≤3B −1,
with B = 256k−2.
Adversary Eve wants to decrypt a ciphertext c. The attack is an adaptively-
chosen-ciphertext attack (see Section 1.3). Eve chooses ciphertexts c1, c2, . . .,
diﬀerent from c, and gets information about the plaintexts from a “decryption
oracle” (imagine that Eve can supply ciphertexts to the decryption device and
obtain some information on the decryption results). Adaptively means that
Eve can choose a ciphertext, get information about the corresponding plain-
text and do some analysis. Depending on the results of her analysis, she can
choose a new ciphertext, and so on. With the help of the oracle, she computes
the plaintext m of the ciphertext c. If Eve does not get the full plaintexts
of the ciphertexts c1, c2, . . ., as in Bleichenbacher’s attack, such an attack is
also called, more precisely, a partial chosen-ciphertext attack.
In Bleichenbacher’s attack, on input of a ciphertext, the decryption oracle
answers whether the corresponding plaintext is PKCS conforming or not.
There were implementations of the SSL/TLS-protocol that contained such
an oracle in practice (see below).
Eve wants to decrypt a ciphertext c which is the encryption of a PKCS
conforming message m. She successively constructs intervals [ai, bi] ⊂Z, i =
0, 1, 2, . . . , which all contain m and become shorter in each step, usually by a
factor of 2. Eve ﬁnds m as soon as the interval has become suﬃciently small
and contains only one integer.
Eve knows that m is PKCS conforming and hence in [2B, 3B −1]. Thus,
she starts with the interval [a0, b0] = [2B, 3B −1]. In each step, she chooses
integers s, computes the ciphertext
˜c := sec mod n
of sm mod n and queries the oracle with input ˜c. The oracle outputs whether
sm mod n
is PKCS conforming or not. Whenever sm mod n is PKCS conforming, Eve
can narrow the interval [a, b]. The choice of the multipliers s depends on
the output of the previous computations. So, the ciphertexts ˜c are chosen
adaptively.
We now describe the attack in more detail.
Let [a0, b0] = [2B, 3B −1]. We have m ∈[a0, b0], and the length of [a0, b0] is
B −1.
Step 1: Eve searches for the smallest integer s1 > 1, such that s1m mod n
is PKCS conforming. Since 2m ≥4B, the residue s1m mod n can be PKCS
conforming, only if s1m ≥n + 2B. Therefore, Eve can start her search with
s1 ≥
§n + 2B/3B −1
¨
.

50
3. Public-Key Cryptography
We have s1m ∈[s1a0, s1b0]. If s1m mod n is PKCS conforming, then
a0 + tn ≤s1m ≤b0 + tn
for some t ∈N with s1a0 ≤b0 + tn and a0 + tn ≤s1b0.
This means that m is contained in one of the intervals
[a1,t, b1,t] := [a0, b0] ∩
£a0 + tn/s1, b0 + tn/s1
¤
,
with ⌈s1a0 −b0/n⌉≤t ≤⌊s1b0 −a0/n⌋. We call the intervals [a1,t, b1,t] the
candidate intervals of step 1. They are pairwise disjoint and have length
< B/s1.
Step 2: Eve searches for the smallest integer s2, s2 > s1, such that s2m mod
n is PKCS conforming. With high probability (see [Bleichenbacher98]), only
one of the candidate intervals [a1,t, b1,t] of step 1 contains a message x, such
that s2x mod n is PKCS conforming. Eve can easily ﬁnd out whether an
interval [a, b] contains a message x, such that s2x mod n is PKCS conforming.
By comparing the interval boundaries, she simply checks whether
[s2a, s2b] ∩[a0 + rn, b0 + rn] ̸= ∅for some r with ⌈s2a/n⌉≤r ≤
¥s2b/n
¦
.
By performing this check for all of the candidate intervals [a1,t, b1,t] of step
1, Eve ﬁnds the candidate interval containing m. We denote this interval by
[a1, b1]. With a high probability, we have s2(b1 −a1) < n −B and then,
[s2a1, s2b1] is suﬃciently short to meet only one of the intervals [a0 +rn, b0 +
rn], r ∈N, say for r = r2. Now, Eve knows that
m ∈[a2, b2] := [a1, b1] ∩
£a0 + r2n/s2, b0 + r2n/s2
¤
.
The length of this interval is < B/s2.
In the rare case that more than one of the candidate intervals of step 1
contains a message x, such that s2x is PKCS conforming, or that more than
one values for r2 exist, Eve is left with more than one interval [a2, b2]. Then,
she repeats step 2, starting with the candidate intervals [a2, b2] in place of the
candidate intervals [a1,t, b1,t] (and searching for s′
2 > s2, such that s′
2m mod n
is PKCS conforming).
Step 3: Step 3 is repeatedly executed, until the plaintext m is determined.
Eve starts with [a2, b2] and (r2, s2), which she has computed in step 2. She
iteratively computes pairs (ri, si) and intervals [ai, bi] of length ≤B/si, such
that m ∈[ai, bi]. The numbers ri, si are chosen, such that
1. si ≈2si−1,
2. [siai−1, sibi−1] ∩[a0 + rin, b0 + rin] ̸= ∅,
3. sim is PKCS conforming.
The number of multipliers s that Eve has to test by querying the decryption
oracle is much smaller than in steps 1 and 2. She searches for si only in

3.3 RSA
51
the neighborhood of 2si−1. This kind of choosing si works, because, after
step 2, the intervals [ai, bi] are suﬃciently small. The length of [ai−1, bi−1]
is < B/si−1 and si ≈2si−1. Hence, the length of the interval [siai−1, sibi−1]
is less than ≈2B, and it therefore meets at most one of the intervals [a0 +
rn, b0 + rn], r ∈N, say for r = ri. From properties 2 and 3, we conclude that
sim ∈[a0 + rin, b0 + rin]. Eve sets
[ai, bi] := [ai−1, bi−1] ∩
£a0 + rin/si, b0 + rin/si
¤
.
Then, [ai, bi] contains m and its length is < B/si.
The upper bound B/si for the length of [ai, bi] decreases by a factor of
two in each iteration (si ≈2si−1). Step 3 is repeated, until [ai, bi] contains
only one integer. This integer is the searched plaintext m.
The analysis in [Bleichenbacher98] shows that for a 1024-bit modulus n,
the total number of ciphertexts, for which Eve queries the oracle, is typically
about 220.
Chosen-ciphertext attacks were considered to be only of theoretical inter-
est. Bleichenbacher’s attack proved the contrary. It was used against a web
server with the ubiquitous SSL/TLS protocol ([RFC 4346]). In the interac-
tive key establishment phase of SSL/TLS, a secret session key is encrypted
by using RSA and PKCS#1 v1.5. For a communication server it is natural to
process many messages, and to report the success or failure of an operation.
Some implementations of SSL/TLS reported an error to the client, when the
RSA-encrypted message was not PKCS conforming. Thus, they could be used
as the oracle. The adversary could anonymously attack the server, because
SSL/TLS is often applied without client authentication. To prevent Bleichen-
bacher’s attack, the implementation of SSL/TLS-servers was improved and
the PKCS#1 standard was updated. Now it uses the OAEP padding scheme,
which we describe below in Section 3.3.4.
Encryption schemes that are provably secure against adaptively-chosen-
ciphertext attacks are studied in more detail in Section 9.5.
The predicate “PKCS conforming” reveals one bit of information about
the plaintext. Bleichenbacher’s attack shows that, if we were able to com-
pute the bit “PKCS conforming” for RSA-ciphertexts, then we could easily
compute complete (PKCS conforming) plaintexts from the ciphertexts. To
compute the bit “PKCS conforming” from c is as diﬃcult as to compute m
from c. Such a bit is called a secure bit. The bit security of one-way functions
is carefully studied in Chapter 7. There we show, for example, that the least
signiﬁcant bit of an RSA-encrypted message is as secure as the whole mes-
sage. In particular, we develop an algorithm that inverts the RSA function
given an oracle with only a small advantage on the least signiﬁcant bit.
3.3.4 Probabilistic RSA Encryption
Before applying an encryption algorithm such as RSA, preprocessing of the
message is necessary. The message is divided into blocks and then some

52
3. Public-Key Cryptography
padding or formatting mechanisms are performed. Such preprocessing is pro-
vided by the OAEP (optimal asymmetric encryption padding) scheme. It is
not only applicable with the RSA cryptosystem; it can be used with any en-
cryption scheme based on a bijective trapdoor function f, such as the RSA
function or the modular squaring function of Rabin’s cryptosystem (see Sec-
tion 3.6.1).
In addition to the trapdoor function
f : D −→D, D ⊂{0, 1}n,
a pseudorandom bit generator
G : {0, 1}k −→{0, 1}l
and a hash function
h : {0, 1}l −→{0, 1}k
are used, with n = l + k. Given a random seed s ∈{0, 1}k as input, G
generates a pseudorandom bit sequence of length l (see Chapter 8 for more
details on pseudorandom bit generators). Hash functions will be discussed in
Section 3.4.
Encryption. To encrypt a message m ∈{0, 1}l, we proceed in three steps:
1. We choose a random bit string r ∈{0, 1}k.
2. We set x = (m ⊕G(r))||(r ⊕h(m ⊕G(r))).
(If x /∈D we return to step 1.)
3. We compute c = f(x).
As always, let || denote the concatenation of strings and ⊕the bitwise XOR
operator.
OAEP is an embedding scheme. The message m is embedded into the
input x of f such that all bits of x depend on the bits of m. The length of
the message m is l. Shorter messages are padded with some additional bits
to get length l. The ﬁrst l bits of x, namely m ⊕G(r) are obtained from m
by masking with the pseudorandom bits G(r). The seed r is encoded in the
last k bits masked with h(m⊕G(r)). The encryption depends on a randomly
chosen r. Therefore, the resulting encryption scheme is not deterministic –
encrypting a message m twice will produce diﬀerent ciphertexts.
Decryption. To decrypt a ciphertext c, we use the function f −1, the same
pseudorandom random bit generator G and the same hash function h as
above:
1. Compute f −1(c) = a||b, with |a| = l and |b| = k.
2. Set r = h(a) ⊕b and get m = a ⊕G(r).
To compute the plaintext m from the ciphertext c = f(x), an adversary
must ﬁgure out all the bits of x from c = f(x). He needs the ﬁrst l bits to

3.3 RSA
53
compute h(a) and the last k bits to get r. Therefore, an adversary cannot
exploit any advantage from some partial knowledge of x.
The OAEP scheme is published in [BelRog94]. OAEP has great practical
importance. It has been adopted in PKCS#1 v2.0, a widely used standard
which is implemented by Internet browsers and used in the secure socket layer
protocol (SSL/TLS, [RFC 4346]). Using OAEP prevents Bleichenbacher’s at-
tack, which we studied in the preceding section. Furthermore, OAEP is in-
cluded in electronic payment protocols to encrypt credit card numbers, and
it is part of the IEEE P1363 standard.
For practical purposes, it is recommended to implement the hash function
h and the random bit generator G using the secure hash algorithm SHA-1
(see Section 3.4.2 below) or some other cryptographic hash algorithm which
is considered secure (for details, see [BelRog94]).
If h and G are implemented with eﬃcient hash algorithms, the time to
compute h and G is negligible compared to the time to compute f and f −1.
Formatting with OAEP does not increase the length of the message substan-
tially.
Using OAEP with RSA encryption no longer preserves the multiplicative
structure of numbers, and it is probabilistic. This prevents the previously
discussed small-message-space attack. The low encryption exponent attack
against RSA is also prevented, provided that the plaintext is individually
re-encoded by OAEP for each recipient before it is encrypted with RSA.
We explained the so-called basic OAEP scheme. A slight modiﬁcation of
the basic scheme is the following. Let k, l be as before and let k′ be another
parameter, with n = l + k + k′. We use a pseudorandom generator
G : {0, 1}k −→{0, 1}l+k′
and a cryptographic hash function
h : {0, 1}l+k′ −→{0, 1}k.
To encrypt a message m ∈{0, 1}l, we ﬁrst append k′ 0-bits to m, and then
we encrypt the extended message as before, i.e., we randomly choose a bit
string r ∈{0, 1}k and the encryption c of m is deﬁned by
c = f(((m||0k′) ⊕G(r))||(r ⊕h((m||0k′) ⊕G(r)))).
Here, we denote by 0k′ the constant bit string 000 . . . 0 of length k′.
In [BelRog94], the modiﬁed scheme is proven to be secure in the random
oracle model against adaptively-chosen-ciphertext attacks. The proof assumes
that the hash function and the pseudorandom generator used behave like
truly random functions. We describe the random oracle model in Section
3.4.5.
In [Shoup2001], it was observed that there is a gap in the security proof of
OAEP. This does not imply that a particular instantiation of OAEP, such as

54
3. Public-Key Cryptography
OAEP with RSA, is insecure. In the same paper, it is shown that OAEP with
RSA is secure for an encryption exponent of 3. In [FujOkaPoiSte2001], this
result is generalized to arbitrary encryption exponents. In Section 9.5.1, we
describe SAEP – a simpliﬁed OAEP – and we give a security proof for SAEP
in the random oracle model against adaptively-chosen-ciphertext attacks.
3.4 Cryptographic Hash Functions
Cryptographic hash functions such as SHA-1 or MD5 are widely used in
cryptography. In digital signature schemes, messages are ﬁrst hashed and
the hash value h(m) is signed in place of m. Hash values are used to check
the integrity of public keys. Pseudorandom bit strings are generated by hash
functions. When used with a secret key, cryptographic hash functions become
message authentication codes (MACs), the preferred tool in protocols like SSL
and IPSec to check the integrity of a message and to authenticate the sender.
A hash function is a function that takes as input an arbitrarily long string
of bits (called a message) and outputs a bit string of a ﬁxed length n. Math-
ematically, a hash function is a function
h : {0, 1}∗−→{0, 1}n, m 7−→h(m).
The length n of the output is typically between 128 and 512 bits8. Later,
when discussing the birthday attack, we will see why the output lengths are
in this range.
One basic requirement is that the hash values h(m) are easy to compute,
making both hardware and software implementations practical.
3.4.1 Security Requirements for Hash Functions
A classical application of cryptographic hash functions is the “encryption”
of passwords. Rather than storing the cleartext of a user password pwd in
the password ﬁle of a system, the hash value h(pwd) is stored in place of
the password itself. If a user enters a password, the system computes the
hash value of the entered password and compares it with the stored value.
This technique of non-reversible “encryption” is applied in operating systems.
It prevents, for example, passwords becoming known to privileged users of
the system such as administrators, provided it is not possible to compute a
password pwd from its hash value h(pwd). This leads to our ﬁrst security
requirement.
A cryptographic hash function must be a one-way function: Given a value
y ∈{0, 1}n, it is computationally infeasible to ﬁnd an m with h(m) = y.
If a hash function is used in conjunction with digital signature schemes,
the message is hashed ﬁrst and then the hash value is signed in place of
8 The output lengths of MD5 and SHA-1 are 128 and 160 bits.

3.4 Cryptographic Hash Functions
55
the original message. Suppose Alice signs h(m) for a message m. Adversary
Eve should have no chance to ﬁnd a message m′ ̸= m with h(m′) = h(m).
Otherwise, she could pretend that Alice signed m′ instead of m.
Thus, the hash function must have the property that given a message m,
it is computationally infeasible to obtain a second message m′ with m ̸= m′
and h(m) = h(m′). This property is called the second pre-image resistance.
When using hash functions with digital signatures, we require an even
stronger property. The legal user Alice of a signature scheme with hash func-
tion h should have no chance of ﬁnding two distinct messages m and m′ with
h(m) = h(m′). If Alice ﬁnds such messages, she could sign m and say later
that she has signed m′ and not m.
Such a pair (m, m′) of messages, with m ̸= m′ and h(m) = h(m′), is
called a collision of h. If it is computationally infeasible to ﬁnd a collision
(m, m′) of h, then h is called collision resistant.
Sometimes, collision resistant hash functions are called collision free, but
that’s misleading. The function h maps an inﬁnite number of elements to a
ﬁnite number of elements. Thus, there are lots of collisions (in fact, inﬁnitely
many). Collision resistance merely states that they cannot be found.
Proposition 3.5. A collision-resistant hash function h is second-pre-image
resistant.
Proof. An algorithm computing second pre-images can be used to compute
collisions in the following way: Choose m at random. Compute a pre-image
m′ ̸= m of h(m). (m, m′) is a collision of h.
2
Proposition 3.5 says that collision resistance is the stronger property.
Therefore, second pre-image resistance is sometimes also called weak collision
resistance, and collision resistance is referred to as strong collision resistance.
Proposition 3.6. A second-pre-image-resistant hash function is a one-way
function.
Proof. If h were not one-way, there would be a practical algorithm A that on
input of a randomly chosen value v computes a message ˜m with h( ˜m) = v,
with a non-negligible probability. Given a random message m, attacker Eve
could ﬁnd, with a non-negligible probability, a second pre-image of h(m) in
the following way: She applies A to the hash value h(m) and obtains ˜m with
h( ˜m) = h(m). The probability that ˜m ̸= m is high.
2
Our deﬁnitions and the argument in the previous proof lack some precision
and are not mathematically rigorous. For example, we do not explain what
“computationally infeasible” and a “non-negligible probability” mean. It is
possible to give precise deﬁnitions and a rigorous proof of Proposition 3.6
(see Chapter 10, Exercise 2).

56
3. Public-Key Cryptography
Deﬁnition 3.7. A hash function is called a cryptographic hash function if it
is collision resistant.
Sometimes, hash functions used in cryptography are referred to as one-
way hash functions. We have seen that there is a stronger requirement, col-
lision resistance, and the one-way property follows from it. Therefore, we
prefer to speak of collision-resistant hash functions.
3.4.2 Construction of Hash Functions
Merkle-Damg˚ard’s construction. There are no known examples of hash
functions whose collision resistance can be proven without any assumptions.
In Section 10.2, we give examples of (rather ineﬃcient) hash functions that
are provably collision resistant under standard assumptions in public-key
cryptography, such as the factoring assumption.
Many cryptographic hash functions used in practice are obtained by the
following method, known as Merkle-Damg˚ard’s construction
or Merkle’s
meta method. The method reduces the problem of constructing a collision-
resistant hash function h : {0, 1}∗−→{0, 1}n to the problem of constructing
a collision-resistant function
f : {0, 1}n+r −→{0, 1}n
(r ∈N, r > 0)
with ﬁnite domain {0, 1}n+r. Such a function f is called a compression func-
tion. A compression function maps messages m of a ﬁxed length n + r to
messages f(m) of length n. We call r the compression rate.
We discuss Merkle-Damg˚ard’s construction. Let f : {0, 1}n+r −→{0, 1}n
be a compression function with compression rate r. By using f, we deﬁne a
hash function
h : {0, 1}∗−→{0, 1}n.
Let m ∈{0, 1}∗be a message of arbitrary length. The hash function h works
iteratively. To compute the hash value h(m), we start with a ﬁxed initial n-
bit hash value v = v0 (the same for all m). The message m is subdivided into
blocks of length r. One block after the other is taken from m, concatenated
with the current value v and compressed by f to get a new v. The ﬁnal v is
the hash value h(m).
More precisely, we pad m out, i.e., we append some bits to m, to obtain
a message ˜m, whose bit length is a multiple of r. We apply the following
padding method: A single 1-bit followed by as few (possibly zero) 0-bits as
necessary are appended. Every message m is padded out with such a string
100 . . . 0, even if the length of the original message m is a multiple of r. This
guarantees that the padding can be removed unambiguously – the bits which
are added during padding can be distinguished from the original message
bits.9
9 There are also other padding methods which may be applied. See, for example,
[RFC 3369].

3.4 Cryptographic Hash Functions
57
After the padding, we decompose
˜m = m1|| . . . ||mk, mi ∈{0, 1}r, 1 ≤i ≤k,
into blocks mi of length r.
We add one more r-bit block mk+1 to ˜m and store the original length of
m (i.e., the length of m before padding it out) into this block right-aligned.
The remaining bits of mk+1 are ﬁlled with zeros:
˜m = m1||m2|| . . . ||mk||mk+1.
Starting with the initial value v0 ∈{0, 1}n, we set recursively
vi := f(vi−1||mi), 1 ≤i ≤k + 1.
The last value of v is taken as hash value h(m):
h(m) := vk+1.
The last block mk+1 is added to prevent certain types of collisions. It
might happen that we obtain vi = v0 for some i. If we had not added
mk+1, then (m, m′) would be a collision of h, where m′ is obtained from
mi+1|| . . . ||mk by removing the padding string 10 . . . 0 from the last block mk.
Since m and m′ have diﬀerent lengths, the additional length blocks diﬀer and
prevent such collisions.10
Proposition 3.8. Let f be a collision-resistant compression function. The
hash function h constructed by Merkle’s meta method is also collision resis-
tant.
Proof. The proof runs by contradiction. Assume that h is not collision re-
sistant, i.e., that we can eﬃciently ﬁnd a collision (m, m′) of h. Let ( ˜m, ˜m′)
be the modiﬁed messages as above. The following algorithm eﬃciently com-
putes a collision of f from ( ˜m, ˜m′). This contradicts our assumption that f
is collision resistant.
10 Sometimes, the padding and the length block are combined: the length of the
original message is stored into the rightmost bits of the padding string. See, for
example, SHA-1 ([RFC 3174]).

58
3. Public-Key Cryptography
Algorithm 3.9.
collision FindCollision(bitString ˜m, ˜m′)
1
˜m = m1|| . . . ||mk+1, ˜m′ = m′
1|| . . . ||m′
k′+1 decomposed as above
2
v1, . . . , vk+1, v′
1, . . . , v′
k′+1 constructed as above
3
if |m| ̸= |m′|
4
then return (vk||mk+1, v′
k′||m′
k′+1)
5
for i ←1 to k do
6
if vi ̸= v′
i and vi+1 = v′
i+1
7
then return (vi||mi+1, v′
i||m′
i+1)
8
for i ←0 to k −1 do
9
if mi+1 ̸= m′
i+1
10
then return (vi||mi+1, v′
i||m′
i+1)
Note that h(m) = vk+1 = v′
k′+1 = h(m′). If |m| ̸= |m′| we have mk+1 ̸=
m′
k′+1, since the length of the string is encoded in the last block. Hence
vk||mk+1 ̸= v′
k′||m′
k′+1. We obtain a collision (vk||mk+1, v′
k′||m′
k′+1), because
f(vk||mk+1) = h(m) = h(m′) = f(v′
k′||m′
k′+1). On the other hand, if |m| =
|m′|, then k = k′, and we are looking for an index i with vi ̸= v′
i and vi+1 =
v′
i+1. (vi||mi+1, v′
i||m′
i+1) is then a collision of f, because f(vi||mi+1) = vi+1 =
v′
i+1 = f(v′
i||m′
i+1). If no index with the above condition exists, we have
vi = v′
i, 1 ≤i ≤k+1. In this case, we search for an index i with mi+1 ̸= m′
i+1.
Such an index exists, because m ̸= m′. (vi||mi+1, v′
i||m′
i+1) is then a collision
of f, because f(vi||mi+1) = vi+1 = v′
i+1 = f(v′
i||m′
i+1).
2
The Birthday Attack. One of the main questions when designing a hash
function h : {0, 1}∗−→{0, 1}n is how large to choose the length n of the hash
values. A lower bound for n is obtained by analyzing the birthday attack.
The birthday attack is a brute-force attack against collision resistance.
Adversary Eve randomly generates messages m1, m2, m3, . . .. For each newly
generated message mi, Eve computes and stores the hash value h(mi) and
compares it with the previous hash values. If h(mi) coincides with one of
the previous hash values, h(mi) = h(mj) for some j < i, Eve has found a
collision (mi, mj)11. We show below that Eve can expect to ﬁnd a collision
after choosing about 2n/2 messages. Thus, it is necessary to choose n so large
that it is impossible to calculate and store 2n/2 hash values. If n = 128 (as
with MD5), about 264 ≈1020 messages have to be chosen for a successful
attack.12 Many people think that today a hash length of 128 bits is no longer
large enough, and that 160 bits (as in SHA-1 and RIPEMD-160) should be
the lower bound (also see Section 3.4.2 below).
11 In practice, the messages are generated by a deterministic pseudorandom gener-
ator. Therefore, the messages themselves can be reconstructed and need not be
stored.
12 To store 264 16-byte hash values, you need 228 TB of storage. There are mem-
oryless variations of the birthday attack which avoid these extreme storage re-
quirements, see [MenOorVan96].

3.4 Cryptographic Hash Functions
59
Attacking second-pre-image resistance or the one-way property of h with
brute force would mean to generate, for a given hash value v ∈{0, 1}n,
random messages m1, m2, m3, . . . and check each time whether h(mi) = v.
Here we expect to ﬁnd a pre-image of v after choosing 2n messages (see
Exercise 9). For n = 128, we need 2128 ≈1039 messages. To protect against
this attack, a smaller n would be suﬃcient.
The surprising eﬃciency of the birthday attack is based on the birthday
paradox. It says that the probability of two persons in a group sharing the
same birthday is greater than 1/2, if the group is chosen at random and has
more than 23 members. It is really surprising that this happens with such a
small group.
Considering hash functions, the 365 days of a year correspond to the
number of hash values. We assume in our discussion that the hash function
h : {0, 1}∗−→{0, 1}n behaves like the birthdays of people. Each of the s = 2n
values has the same probability. This assumption is reasonable. It is a basic
design principle that a cryptographic hash function comes close to a random
function, which yields random and uniformly distributed values (see Section
3.4.4).
Evaluating h, k times with independently chosen inputs, the probability
that no collisions occur is
p = p(s, k) = 1
sk
k−1
Y
i=0
(s −i) =
k−1
Y
i=1
µ
1 −i
s
¶
.
We have 1 −x ≤e−x for all real numbers x and get
p ≤
k−1
Y
i=1
e−i/s = e−(1/s) Pk−1
i=1 i = e−k(k−1)/2s.
The probability that a collision occurs is 1 −p, and 1 −p ≥1/2 if k ≥
1/2
¡√
1 + 8 ln 2 · s + 1
¢
≈1.18 √s.
For s = 365, we get an explanation for the original birthday paradox,
since 1.18 · √s = 22.54.
For the hash function h, we conclude that it suﬃces to choose about 2n/2
many messages at random to obtain a collision with probability > 1/2.
In a hash-then-decrypt digital signature scheme, where the hash value is
signed in place of the message (see Section 3.4.5 below), the birthday attack
might be practically implemented in the following way. Suppose that Eve
and Bob want to sign a contract m1. Later, Eve wants to say that Bob has
signed a diﬀerent contract m2. Eve generates O(2n/2) minor variations of m1
and m2. In many cases, for example, if m1 includes a bitmap, Bob might not
observe the slight modiﬁcation of m1. If the birthday attack is successful,
Eve gets messages em1 and em2 with h( em1) = h( em2). Eve lets Bob sign the
contract em1. Later, she can pretend that Bob signed em2.

60
3. Public-Key Cryptography
Compression Functions from Block Ciphers. We show in this section
how to derive compression functions from a block cipher, such as DES or
AES. From these compression functions, cryptographic hash functions can
be obtained by using Merkle-Damg˚ard’s construction.
Symmetric block ciphers are widely used and well studied. Encryption
is implemented by eﬃcient algorithms (see Chapter 2). It seems natural to
also use them for the construction of compression functions. Though no rig-
orous proofs exist, the hope is that a good block cipher will result in a good
compression function. Let
E : {0, 1}r × {0, 1}n −→{0, 1}n, (k, x) 7−→E(k, x)
be the encryption function of a symmetric block cipher, which encrypts blocks
x of bit length n with r-bit keys k.
First, we consider constructions where the bit length of the hash value
is equal to the block length of the block cipher. These schemes are called
single-length MDCs13. To obtain a collision-resistant compression function,
the block length n of the block cipher should be at least 128 bits.
The compression function
f1 : {0, 1}n+r −→{0, 1}n, (x||y) 7−→E(y, x)
maps bit blocks of length n + r to blocks of length n. A block of length n + r
is split into a left block x of length n and a right block y of length r. The
right block y is used as key to encrypt the left block x.
The second example of a compression function – it is the basis of the
Matyas-Meyer-Oseas hash function ([MatMeyOse85]) – has been included in
[ISO/IEC 10118-2]. Its compression rate is n. A block of bit length 2n is
split into two halves, x and y, each of length n. Then x is encrypted with a
key g(y) which is derived from y by a function g : {0, 1}n −→{0, 1}r.14 The
resulting ciphertext is bitwise XORed with x:
f2 : {0, 1}2n −→{0, 1}n, (x||y) 7−→E(g(y), x) ⊕x.
If the block length of the block cipher is less than 128, double-length MDCs
are used. Compression functions whose output length is twice the block length
can be obtained by combining two types of the above compression functions
(for details, see [MenOorVan96]).
Real Hash Functions. Most cryptographic hash functions used in practice
today do not rely on other cryptographic primitives such as block ciphers.
They are derived from custom-designed compression functions by applying
13 The acronym MDC is explained in Section 3.4.3 below.
14 For example, we can take g(y) = y, if the block length of E is equal to the key
length, or, more generally, we can compute r key bits g(y) from y by using a
Boolean function.

3.4 Cryptographic Hash Functions
61
Merkle-Damg˚ard’s construction. The functions are especially designed for the
purpose of hashing, with performance eﬃciency in mind.
In [Rivest90], R. Rivest proposed MD4, which is algorithm number 4 in a
family of hash algorithms. MD4 was designed for software implementation on
a 32-bit processor. The MD4 algorithm is not strong enough, as early attacks
showed. However, the design principles of the MD4 algorithm were subse-
quently used in the construction of hash functions. These functions are often
called the MD4 family. The family contains the most popular hash functions
in use today, such as MD5, SHA-1 and RIPEMD-160. The hash values of
MD5 are 128 bits long, those of RIPEMD-160 and SHA-1 160 bits. All of
these hash functions are iterative hash functions; they are constructed with
Merkle-Damg˚ard’s method. The compression rate of the underlying compres-
sion functions is 512 bits.
SHA-1 is included in the Secure Hash Standard FIPS 180 of NIST
([RFC 1510]; [RFC 3174]). It is an improvement of SHA-0, which turned out
to have a weakness. The standard was updated in 2002 ([FIPS 180-2]). Now
it includes additional algorithms that produce 256-bit, 384-bit and 512-bit
outputs.
Since no rigorous mathematical proofs for the security of these hash func-
tions exist, there is always the chance of a surprise attack.
For example, the MD5 algorithm is very popular, but there have been
very successful attacks.
In 1996, H. Dobbertin detected collisions (v0||m, v0||m′) of the underly-
ing compression function, where v0 is a common 128-bit string and m, m′
are distinct 512-bit messages ([Dobbertin96a]; [Dobbertin96]). Dobbertin’s
v0 is diﬀerent from the initial value that is speciﬁed for MD5 in the Merkle-
Damg˚ard iteration. Otherwise, the collision would have immediately implied
a collision of MD5 (note that the same length block is appended to both
messages). Already in 1993, B. den Boer and A. Bosselaers had detected
collisions of MD5’s compression function. Their collisions (v0||m, v′
0||m) were
made of distinct initial values v0 and the same message m. Thus, they did not
ﬁt Merkle-Damg˚ard’s method and were sometimes called pseudocollisions.
The recent attacks by the Chinese researchers X. Wang, D. Feng, X. Lai
and H. Yu showed that MD5 can no longer be considered collision-resistant.
In August 2004, they published collisions for the hash functions MD4, MD5,
HAVAL-128, RIPEMD-128 ([WanFenLaiYu04]). V. Klima published an algo-
rithm which works for any initial value and computes collisions of MD5 on a
standard PC within a minute ([Klima06]). MD5 is really broken.
Moreover, in February 2005, X. Wang, Y. L. Yin and H. Yu cast serious
doubts on the security of SHA-1 ([WanYinYu05]). They announced that they
found an algorithm which computes collisions with 269 hash operations. This
is much less than the expected 280 steps of the brute-force birthday attack.
With current technology, 269 steps are still on the far edge of feasibility. For
example, the RC5-64 Challenge was ﬁnished in 2002. A worldwide network of

62
3. Public-Key Cryptography
Internet users was able to ﬁgure out a 64-bit RC5 key by a brute-force search.
The search took almost 5 years, and more than 300,000 users participated
(see [RSALabs]; [DistributedNet]).
All of these attacks are against collision resistance, and they are rele-
vant for digital signatures. They are not attacks against second-pre-image
resistance or the one-way property. Therefore, applications like HMAC (see
Section 3.4.3), whose security is based on these properties, are not yet af-
fected.
In the future, hash functions with longer hash values, such as SHA-256
or SHA-512, will be used in place of MD5 and SHA-1.
3.4.3 Data Integrity and Message Authentication
Modiﬁcation Detection Codes. Cryptographic hash functions are also
known as message digest functions, and the hash value h(m) of a message
m is called the digest or ﬁngerprint or thumbprint of m 15. The hash value
h(m) is indeed a “ﬁngerprint”of m. It is a very compact representation of
m, and, as an immediate consequence of second-pre-image resistance, this
representation is practically unique. Since it is computationally infeasible to
obtain a second message m′ with m ̸= m′ and h(m′) = h(m), a diﬀerent hash
value would result, if the message m were altered in any way.
This implies that a cryptographic hash function can be used to control
the integrity of a message m. If the hash value of m is stored in a secure
place, a modiﬁcation of m can be detected by calculating the hash value and
comparing it with the stored value. Therefore, hash functions are also called
modiﬁcation detection codes (MDCs).
Let us consider an example. If you install a new root certiﬁcate in your
Internet browser, you have to make sure (among other things) that the source
of the certiﬁcate is the one you think it is and that your copy of the certiﬁcate
was not modiﬁed. You can do this by checking the certiﬁcate’s thumbprint.
For this purpose, you can get the ﬁngerprint of the root certiﬁcate from the
issuing certiﬁcation authority’s web page or even on real paper by ordinary
mail (certiﬁcates and certiﬁcation authorities are discussed in Section 4.1.5).
Message Authentication Codes. A very important application of hash
functions is message authentication, which means to authenticate the origin
of the message. At the same time, the integrity of the message is guaranteed.
If hash functions are used for message authentication, they are called message
authentication codes, or MACs for short.
MACs are the standard symmetric technique for message authentication
and integrity protection and widely used, for example, in protocols such as
SSL/TLS ([RFC 4346]) and IPSec. They depend on secret keys shared be-
tween the communicating parties. In contrast to digital signatures, where
15 MD5 is a “message digest function”.

3.4 Cryptographic Hash Functions
63
only one person knows the secret key and is able to generate the signature,
each of the two parties can produce the valid MAC for a message.
Formally, the secret keys k are used to parameterize hash functions. Thus,
MACs are families of hash functions
(hk : {0, 1}∗−→{0, 1}n)k∈K.
MACs may be derived from block ciphers or from cryptographic hash func-
tions. We describe two methods to obtain MACs.
The standard method to convert a cryptographic hash function into
a MAC is called HMAC. It is published in [RFC 2104] (and [FIPS 198],
[ISO/IEC 9797-2]) and can be applied to a hash function h that is derived
from a compression function f by using Merkle-Damg˚ard’s method. You can
take as h, for example, MD5, SHA-1 or RIPEMD-160 (see Section 3.4.2).
We have to assume that the compression rate of f and the length of the
hash values are multiples of 8, so we can measure them in bytes. We denote
by r the compression rate of f in bytes. The secret key k can be of any length,
up to r bytes. By appending zero bytes, the key k is extended to a length of
r bytes (e.g., if k is of length 20 bytes and r = 64, then k will be appended
with 44 zero bytes 0x00).
Two ﬁxed and distinct strings ipad and opad are deﬁned (the ‘i’ and ‘o’
are mnemonics for inner and outer):
ipad := the byte 0x36 repeated r times,
opad := the byte 0x5C repeated r times.
The keyed hash value HMAC of a message m is calculated as follows:
HMAC(k, m) := h((k ⊕opad)||h((k ⊕ipad)||m)).
The hash function h is applied twice in order to guarantee the security of the
MAC. If we apply h only once and deﬁne HMAC(k, m) := h((k ⊕ipad)||m),
an adversary Eve could take a valid MAC value, modify the message m and
compute the valid MAC value of the modiﬁed message, without knowing the
secret key. For example, Eve may take any message m′ and compute the hash
value v of m′ by applying Merkle-Damg˚ard’s iteration with HMAC(k, m) =
h((k⊕ipad)||m) as initial value v0. Before iterating, Eve appends the padding
bits and the additional length block to m′. She does not store the length of
m′ into the length block, but the length of ˜m||m′, where ˜m is the padded
message m (including the length block for m, see Section 3.4.2). Then v is
the MAC of the extended message ˜m||m′, and Eve has computed it without
knowing the secret key k. This problem is called the length extension problem
of iterated hash functions. Applying the hash function twice prevents the
length extension attack.
MACs can also be constructed from block ciphers. The most important
construction is CBC-MAC. Let E be the encryption function of a block ci-
pher, such as DES or AES. Then, with k a secret key, the MAC value for

64
3. Public-Key Cryptography
a message m is the last ciphertext block when encrypting m, with E in the
Cipher-Block Chaining Mode CBC and key k (see Section 2.2.3). We need
an initialization vector IV for CBC. For encryption purposes, it is impor-
tant not to use the same value twice. Here, the IV is ﬁxed and typically
set to 0 . . . 0. If the block length of E is n, then m is split into blocks of
length n, m1||m2|| . . . ||ml (pad out the last block, if necessary, for example,
by appending zeros), and we compute
c0 := IV
ci := E(k, mi ⊕ci−1)
CBC-MAC := cl
Sometimes, the output of the CBC-MAC function is taken only to be a part
of the last block. There are various standards for CBC-MAC, for example,
[FIPS 113] and [ISO/IEC 9797-1]. A comprehensive discussion of hash func-
tions and MACs can be found in [MenOorVan96].
3.4.4 Hash Functions as Random Functions
A random function would be the perfect cryptographic hash function h. Ran-
dom means that for all messages m, each of the n bits of the hash value h(m)
is determined by tossing a coin. Such a perfect cryptographic hash function
is also called a random oracle16. Unfortunately, it is obvious that a perfect
random oracle can not be implemented. To determine only the hash values
for all messages of ﬁxed length l would require exponentially many (n · 2l)
coin tosses and storage of all the results, which is clearly impossible.
Nevertheless, it is a design goal to construct hash functions which approxi-
mate random functions. It should be computationally infeasible to distinguish
the hash function from a truly random function. Recall that there is a sim-
ilar design goal for symmetric encryption algorithms. The ciphertext should
appear random to the attacker. That is the reason why we hoped in Section
3.4.2 that a good block cipher induces a good compression function for a
good hash function.
If we assume that the designers of a hash function h have done a good job
and h comes close to a random oracle, then we can use h as a generator of
pseudorandom bits. Therefore, we often see popular cryptographic hash func-
tions such as SHA-1 or MD5 as sources of pseudorandomness. For example,
in the Transport Layer Security (TLS) protocol ([RFC 4346]), also known as
Secure Socket Layer (SSL), client and server agree on a shared 48-bit mas-
ter secret, and then they derive further key material (for example, the MAC
keys, encryption keys and initialization vectors) from this master secret by
16 Security proofs in cryptography sometimes rely on the assumption that the hash
function involved is a random oracle. An example of such a proof is given in
Section 3.4.5

3.4 Cryptographic Hash Functions
65
using a pseudorandom function. The pseudorandom function of TLS is based
on the HMAC construction, with the hash functions SHA-1 or MD5.
3.4.5 Signatures with Hash Functions
Let (n, e) be the public RSA key and d be the secret decryption exponent
of Alice. In the basic RSA signature scheme (see Section 3.3.2), Alice can
sign messages that are encoded by numbers m ∈{0, . . . , n −1}. To sign m,
she applies the RSA decryption algorithm and obtains the signature σ =
md mod n of m.
Typically, n is a 1024–bit number. Alice can sign a bit string m that, when
interpreted as a number, is less than n. This is a text string of at most 128
ASCII-characters. Most documents are much larger, and we are not able to
sign them with basic RSA. This problem, which exists in all digital signatures
schemes, is commonly solved by applying a collision resistant hash function
h.
Message m is ﬁrst hashed, and the hash value h(m) is signed in place of
m. Alice’s RSA signature of m is
σ = h(m)d mod n.
To verify Alice’s signature σ for message m, Bob checks whether
σe = h(m) mod n.
This way of generating signatures is called the hash-then-decrypt paradigm.
This term is even used for signature schemes, where the signing algorithm is
not the decryption algorithm as in RSA (see, for example, ElGamal’s Signa-
ture Scheme in Section 3.5.2).
Messages with the same hash value have the same signature. Collision
resistance of h is essential for non-repudiation. It prevents Alice from ﬁrst
signing m and pretending later that she has signed a diﬀerent message m′ and
not m. To do this, Alice would have to generate a collision (m, m′). Collision
resistance also prevents that an attacker Eve takes a signed message (m, σ)
of Alice, generates another message m′ with the same hash value and uses
σ as a (valid) signature of Alice for m′. To protect against the latter attack,
second-pre-image resistance of h would be suﬃcient.
The hash-then-decrypt paradigm has two major advantages. Messages of
any length can be signed by applying the basic signature algorithm, and
the attacks, which we discussed in Section 3.3.2, are prevented. Recall that
the hash function reduces a message of arbitrary length to a short digital
ﬁngerprint of less than 100 bytes.
The schemes which we discuss now implement the hash-then-decrypt
paradigm.

66
3. Public-Key Cryptography
Full-Domain-Hash RSA signatures. We apply the hash-then-decrypt
paradigm in an RSA signature scheme with public key (n, e) and secret key
d and use a hash function
h : {0, 1}∗−→{0, . . . , n −1},
whose values range through the full set {0, . . . , n −1} rather than a smaller
subset. Such a hash function h is called a full-domain hash function, because
the image of h is the full domain {0, . . . , n −1} of the RSA function17. The
signature of a message m ∈{0, 1}∗is h(m)d mod n.
The hash functions that are typically used in practical RSA schemes, like
SHA, MD5 or RIPEMD, are not full-domain hash functions. They produce
hash values of bit length between 128 and 512 bits, whereas the typical bit
length of n is 1024 or 2048.
It can be mathematically proven that full-domain-hash RSA signatures
are secure in the random oracle model ([BelRog93]), and we will give such a
proof in this section.
For this purpose, we consider an adversary F, who attacks Bob, the le-
gitimate owner of an RSA key pair, and tries to forge at least one signature
of Bob, without knowing Bob’s private key d. More precisely, F is an eﬃ-
ciently computable algorithm that, with some probability of success, on input
of Bob’s public RSA key (n, e) outputs a message m together with a valid
signature σ of m.
The random oracle model. In this model, the hash function h is assumed
to operate as a random oracle. This means that
1. the hash function h is a random function (as explained in Section 3.4.4),
and
2. whenever the adversary F needs the hash value for a message m, it has
to call the oracle h with m as input. Then it obtains the hash value h(m)
from the oracle.
Condition 2 means that F always calls h as a “black box” (for example,
by calling it as a subroutine or by communicating with another computer
program), whenever it needs a hash value, and this may appear as a trivial
condition. But it includes, for example, that the adversary has no algorithm
to compute the hash values by itself; it has no knowledge about the internal
structure of h, and it is stateless with respect to hash values. It does not store
and reuse any hash values from previous executions. The hash values h(m)
appear as truly random values to him.
We assume from now on that our full-domain hash function h is a random
oracle. Given m, each element of Zn has the same probability 1/n of being
the hash value h(m).
The security of RSA signatures relies, of course, on the RSA assumption,
which states that the RSA function is a one-way function. Without knowing
17 As often, we identify Zn with {0, . . . , n −1}.

3.4 Cryptographic Hash Functions
67
the secret exponent d, it is infeasible to compute e-th roots modulo n, i.e.,
for a randomly chosen e-th power y = xe mod n, it is impossible to compute
x from y with more than a negligible probability (see Deﬁnition 6.7 for a
precise statement).
Our security proof for full-domain-hash RSA signatures is a typical one.
We develop an eﬃcient algorithm A which attacks the underlying assumption
– here the RSA assumption. In our example, A tries to compute the e-th
root of a randomly chosen y ∈Zn. The algorithm A calls the forger F as a
subroutine. If F is successful in its forgery, then A is successful in computing
the e-th root. Now, we conclude: since it is infeasible to compute e-th roots
(by the RSA assumption), F can not be successful, i.e., it is impossible to
forge signatures. By A, the security of the signature scheme is reduced to
the security of the RSA trapdoor function. Therefore, such proofs are called
security proofs by reduction.
The security of full-domain-hash signatures is guaranteed, even if forger
F is supplied with valid signatures for messages m′ of its choice. Of course,
to be successful, F has to produce a valid signature for a message m which
is diﬀerent from the messages m′. F can request the signature for a message
m′ at any time during its attack, and it can choose the messages m′ adap-
tively, i.e., F can analyze the signatures that it has previously obtained, and
then choose the next message to be signed. F performs an adaptively-chosen-
message attack (see Section 10.1 for a more detailed discussion of the various
types of attacks against signature schemes).
In the real attack, the forger F interacts with Bob, the legitimate owner
of the secret key, to obtain signatures, and with the random oracle to obtain
hash values. Algorithm A is constructed to replace both, Bob and the random
oracle h, in the attack. It “simulates” the signer Bob and h.
Since A has no access to the secret key, it has a problem to produce a
valid signature, when F issues a signature request for message m′. Here, the
random oracle model helps A. It is not the message that is signed, but its
hash value. To check if a signature is valid, the forger F must know the hash
value, and to get the hash value it has to ask the random oracle. Algorithm
A answers in place of the oracle. If asked for the hash value of a message m′,
it selects s ∈Zn at random and supplies se as the hash value. Then, it can
provide s as the valid signature of m′.
Forger F can not detect that A sends manipulated hash values. The ele-
ments s, se and the real hash values (generated by a random oracle) are all
random and uniformly distributed elements of Zn. This means that forger
F, when interacting with A, runs in the same probabilistic setting as in the
real attack. Therefore, its probability of successfully forging a signature is
the same as in the real attack against Bob.
A takes as input the public key (n, e) and a random element y ∈Zn. Let
F query the hash values of the r messages m1, m2, . . . , mr. The structure of
A is the following.

68
3. Public-Key Cryptography
Algorithm 3.10.
int A(int n, e, y)
1
choose t ∈{1, . . . , r} at random and set ht ←y
2
choose si ∈Zn at random and set hi ←se
i, i = 1, . . . , r, i ̸= t
3
call F(n, e)
4
if F queries the hash value of mi, then respond with hi
5
if F requests the signature of mi, i ̸= t, then respond with si
6
if F requests the signature of mt, then terminate with failure
7
if F requests the signature of m′, m′ ̸= mi for i = 1, . . . , r,
8
then respond with a random element of Zn
9
if F returns (m, s), return s
In step 1, A tries to guess the message m ∈{m1, . . . , mr}, for which F
will output a forged signature. F must know the hash value of m. Otherwise,
the hash value h(m) of m would be randomly generated independently from
F’s point of view. Then, the probability that a signature s, generated by
F, satisﬁes the veriﬁcation condition se mod n = h(m) is 1/n, and hence
negligibly small. Thus, m is necessarily one of the messages mi, for which F
queries the hash value.
If F requests the signature of mi, i ̸= t, then A responds with the valid
signature si (line 5). If F requests the signature of m′, m′ ̸= mi for i =
1, . . . , r, and F never asks for the hash value of m′, then A can respond with
a random value (line 7) – F is not able to check the validity of the answer.
Suppose that A guesses the right mt in step 1 and F forges successfully.
Then, F returns a valid signature s for mt, which is an e-th root of y, i.e.,
se = h(mt) = y. In this case, A returns s. It has successfully computed an
e-th root of y modulo n.
The probability that A guesses the right mt in step 1 is 1/r. Hence, the
success probability of A is 1/r · α, where α is the success probability of forger
F. Assume for a moment that forger F is always successful, i.e., α = 1. By
independent repetitions of A, we then get an algorithm which successfully
computes e-th roots with a probability close to 1. In general, we get an
algorithm to compute e-th roots with about the same success probability α
as F.
We described the notion of provable security in the random oracle model
by studying full-domain hash RSA signatures. The proof says that a suc-
cessful forger can not exist in the random oracle model. But since real hash
functions are never perfect random oracles (see Section 3.4.4 above), our
argument can never be completed to a security proof of the real signature
scheme, where a real implementation of the hash function has to be used.
In Section 9.5, we will give a random-oracle proof for Boneh’s SAEP
encryption scheme.
Our proof requires a full-domain hash function h – it is essential that each
element of Zn has the same probability of being the hash value. The hash
functions used in practice usually are not full-domain hash functions, as we

3.4 Cryptographic Hash Functions
69
observed above. The scheme we describe in the next section does not rely on
a full-domain hash function. It provides a clever embedding of the hashed
message into the domain of the signature function.
PSS. The probabilistic signature scheme (PSS) was introduced in [BelRog96].
The signature of a message depends on the message and some randomly cho-
sen input. The resulting signature scheme is therefore probabilistic. To set
up the scheme, we need the decryption function of a public-key cryptosys-
tem like the RSA decryption function or the decryption function of Rabin’s
cryptosystem (see Section 3.6). More generally, it requires a trapdoor permu-
tation
f : D −→D, D ⊂{0, 1}n ,
a pseudorandom bit generator
G : {0, 1}l −→{0, 1}k × {0, 1}n−(l+k), w 7−→(G1(w), G2(w))
and a hash function
h : {0, 1}∗−→{0, 1}l.
The PSS is applicable to messages of arbitrary length. The message m
cannot be recovered from the signature σ.
Signing. To sign a message m ∈{0, 1}∗, Alice proceeds in three steps:
1. Alice chooses r ∈{0, 1}k at random and calculates w := h(m||r).
2. She computes G(w) = (G1(w), G2(w)) and y := w||(G1(w) ⊕r)||G2(w).
(If y /∈D, she returns to step 1.)
3. The signature of m is σ := f −1(y).
As usual, || denotes the concatenation of strings and ⊕the bitwise XOR
operator. If Alice wants to sign message m, she concatenates a random seed
r to the message and applies the hash function h to m||r. Then Alice applies
the generator G to the hash value w. The ﬁrst part G1(w) of G(w) is used
to mask r; the second part of G(w), G2(w), is appended to w||G1(w) ⊕r to
obtain a bit string y of appropriate length. All bits of y depend on the message
m. The hope is that mapping m into the domain of f by m 7−→y behaves
like a truly random function. This assumption guarantees the security of the
scheme. Finally, y is decrypted with f to get the signature. The random seed
r is selected independently for each message m – signing a message twice
yields distinct signatures.
Veriﬁcation. To verify the signature of a signed message (m, σ), we use the
same trapdoor function f, the same random bit generator G and the same
hash function h as above, and proceed as follows:
1. Compute f(σ) and decompose f(σ) = w||t||u,
where |w| = l, |t| = k and |u| = n −(k + l).
2. Compute r = t ⊕G1(w).

70
3. Public-Key Cryptography
3. We accept the signature σ if h(m||r) = w and G2(w) = u; otherwise we
reject it.
PSS can be proven to be secure in the random oracle model under the
RSA assumption. The proof assumes that the hash functions G and h are
random oracles.
For practical applications of the scheme, it is recommended to implement
the hash function h and the random bit generator G with the secure hash
algorithm SHA-1 or some other cryptographic hash algorithm that is con-
sidered collision resistant. Typical values of the parameters n, k and l are
n = 1024 bits and k = l = 128 bits.
3.5 The Discrete Logarithm
In Section 3.3 we discussed the RSA cryptosystem. The RSA function raises
an element m to the e-th power. It is a bijective function and is eﬃcient to
compute. If the factorization of n is not known, there is no eﬃcient algorithm
for computing the e-th root. There are other functions in number theory
that are easy to compute but hard to invert. One of the most important is
exponentiation in ﬁnite ﬁelds. Let p be a prime and g be a primitive root in
Z∗
p (see Appendix A.4). The discrete exponential function
Exp : Zp−1 −→Z∗
p, x 7−→gx,
is a one-way function. It can be eﬃciently computed, for example, by the
repeated squaring algorithm (Section 3.2). No eﬃcient algorithm for com-
puting the inverse function Log of Exp, i.e., for computing x from y = gx, is
known, and it is widely believed that no such algorithm exists. This assump-
tion is called the discrete logarithm assumption (for a precise deﬁnition, see
Deﬁnition 6.1).
3.5.1 ElGamal’s Encryption
In contrast to the RSA function, Exp is a one-way function without a trap-
door. It does not have any additional information, which makes the computa-
tion of the inverse function easy. Nevertheless, Exp is the basis of ElGamal’s
cryptosystem ([ElGamal84]).
Key Generation. The recipient of messages, Bob, proceeds as follows:
1. He chooses a large prime p, such that p −1 has a big prime factor and a
primitive root g ∈Z∗
p.
2. He chooses at random an integer x in the range 0 ≤x ≤p −2.
The triple (p, g, x) is the secret key of Bob.
3. He computes y = gx in Zp. The public key of Bob is (p, g, y), and x is
kept secret.

3.5 The Discrete Logarithm
71
The number p−1 will have a large prime factor if Bob is looking for primes
p of the form 2kq + 1, where q is a large prime. Thus, Bob ﬁrst chooses a
large prime q. Here he proceeds in the same way as in the RSA key generation
procedure (see Section 3.3.1). Then, to get p, Bob randomly generates a k of
appropriate bit length and applies a probabilistic primality test to z = 2kq+1.
He replaces k by k+1 until he succeeds in ﬁnding a prime. He expects to test
O(ln z) numbers for primality before reaching the ﬁrst prime (see Corollary
A.71). Having found a prime p = 2kq + 1, he randomly selects elements g in
Z∗
p and tests whether g is a primitive root. The factorization of k is required
for this test (see Algorithm A.39). Thus q must be chosen to be suﬃciently
large, such that k is small enough to be factored eﬃciently.
Bob has to avoid that all prime factors of p−1 are small. Otherwise, there
is an eﬃcient algorithm for the computation of discrete logarithms developed
by Silver, Pohlig and Hellman (see [Koblitz94]).
Encryption and Decryption. Alice encrypts messages for Bob by using
Bob’s public key (p, g, y). She can encrypt elements m ∈Zp. To encrypt a
message m ∈Zp, Alice chooses at random an integer k, 1 ≤k ≤p −2. The
encrypted message is the following pair (c1, c2) of elements in Zp:
(c1, c2) := (gk, ykm).
The computations are done in Zp. By multiplying m with yk, Alice hides the
message m behind the random element yk.
Bob decrypts a ciphertext (c1, c2) by using his secret key x. Since yk =
(gx)k = (gk)x = cx
1, he obtains the plaintext m by multiplying c2 with the
inverse c−x
1
of cx
1:
c−x
1 c2 = y−kykm = m.
Recall that c−x
1
= cp−1−x
1
, because cp−1
1
= [1] (see “Computing modulo a
prime” on page 303). Therefore, Bob can decrypt the ciphertext by raising
c1 to the (p−1−x)-th power, m = cp−1−x
1
c2. Note that p−1−x is a positive
number.
The encryption algorithm is not a deterministic algorithm. The cryp-
togram depends on the message, the public key and on a randomly chosen
number. If the random number is chosen independently for each message, it
rarely happens that two plaintexts lead to the same ciphertext.
The security of the scheme depends on the following assumption: it is im-
possible to compute gxk (and hence g−xk = (gxk)−1 and m) from gx and gk,
which is called the Diﬃe-Hellman problem. An eﬃcient algorithm to compute
discrete logarithms would solve the Diﬃe-Hellman problem. It is unknown
whether the Diﬃe-Hellman problem is equivalent to computing discrete log-
arithms, but it is believed that no eﬃcient algorithm exists for this problem
(also see Section 4.1.2).
Like basic RSA (see Section 3.3.3), ElGamal’s encryption is vulnerable to
a chosen-ciphertext attack. Adversary Eve, who wants to decrypt a ciphertext

72
3. Public-Key Cryptography
c = (c1, c2), with c1 = gk and c2 = myk, chooses random elements ˜k and ˜m
and gets Bob to decrypt ˜c = (c1g˜k, c2 ˜my˜k). Bob sends m ˜m, the plaintext
of ˜c = (gk+˜k, m ˜myk+˜k), to Eve. Eve simply divides by ˜m and obtains the
plaintext m of c: m = (m ˜m) ˜m−1. Bob’s suspicion is not aroused, because the
plaintext m ˜m looks random to him.
3.5.2 ElGamal’s Signature Scheme
Key Generation. To generate a key for signing, Alice proceeds as Bob in
the key generation procedure above to obtain a public key (p, g, y) and a
secret key (p, g, x) with y = gx.
Signing. We assume that the message m to be signed is an element in Zp.
In practice, a hash function h is used to map the messages into Zp. Then
the hash value is signed. The signed message is produced by Alice using the
following steps:
1. She selects a random integer k, 1 ≤k ≤p −2, with gcd(k, p −1) = 1.
2. She sets r := gk and s := k−1(m −rx) mod (p −1).
3. (m, r, s) is the signed message.
Veriﬁcation. Bob veriﬁes the signed message (m, r, s) as follows:
1. He veriﬁes whether 1 ≤r ≤p −1. If not, he rejects the signature.
2. He computes v := gm and w := yrrs, where y is Alice’s public key.
3. The signature is accepted if v = w; otherwise it is rejected.
Proposition 3.11. If Alice signed the message (m, r, s), we have v = w.
Proof.
w = yrrs = (gx)r(gk)s = grxgkk−1(m−rx) = gm = v.
Here, recall that exponents of g can be reduced modulo (p −1), since gp−1 =
[1] (see “Computing modulo a prime” on page 303).
2
Remarks. The following observations concern the security of the system:
1. The security of the system depends on the discrete logarithm assump-
tion. Someone who can compute discrete logarithms can get everyone’s
secret key and thereby break the system totally. To ﬁnd an s, such that
gm = yrrs, on given inputs m and r, is equivalent to the computation of
discrete logarithms.
To forge a signature for a message m, one has to ﬁnd elements r and s,
such that gm = yrrs. It is not known whether this problem is equivalent
to the computation of discrete logarithms. However, it is also believed
that no eﬃcient algorithm for this problem exists.

3.5 The Discrete Logarithm
73
2. If adversary Eve succeeds in getting the chosen random number k for
some signed message m, she can compute rx ≡(m−sk) mod (p−1) and
the secret key x, because with high probability gcd(r, p −1) = 1. Thus,
the random number generator used to get k must be of superior quality.
3. It is absolutely necessary to choose a new random number for each mes-
sage. If the same random number is used for diﬀerent messages m1 ̸= m2,
it is possible to compute k: s −s′ ≡(m −m′)k−1 mod (p −1) and hence
k ≡(s −s′)−1(m −m′) mod (p −1).
4. When used without a hash function, ElGamal’s signature scheme is exis-
tentially forgeable; i.e., an adversary Eve can construct a message m and
a valid signature (m, r, s) for m.
This is easily done. Let b and c be numbers such that gcd(c, p −1) = 1.
Set r = gbyc, s = −rc−1 mod (p−1) and m = −rbc−1 mod (p−1). Then
(m, r, s) satisﬁes gm = yrrs. Fortunately in practice, as observed above,
a hash function h is applied to the original message, and it is the hash
value that is signed. Thus, to forge the signature for a real message is
not so easy. Adversary Eve has to ﬁnd some meaningful message ˜m with
h( ˜m) = m. If h is a collision-resistant hash function, her probability of
accomplishing this is very low.
5. D. Bleichenbacher observed in [Bleichenbacher96] that step 1 in the veri-
ﬁcation procedure is essential. Otherwise Eve would be able to sign mes-
sages of her choice, provided she knows one valid signature (m, r, s), where
m is a unit in Zp−1.
Let m′ be a message of Eve’s choice, u
=
m′m−1 mod (p −1),
s′
=
su mod (p −1), r′
∈
Z, such that r′
≡
r mod p and
r′ ≡ru mod (p −1). r′ is obtained by the Chinese Remainder Theo-
rem (see Theorem A.29). Then (m′, r′, s′) is accepted by the veriﬁcation
procedure.
3.5.3 Digital Signature Algorithm
In 1991 NIST proposed a digital signature standard (DSS) (see [NIST94]).
DSS was intended to become a standard digital signature method for use
by government and ﬁnancial organizations. The DSS contains the digital
signature algorithm (DSA), which is very similar to ElGamal’s algorithm.
Key Generation. The keys are generated in a similar way as in ElGamal’s
signature scheme. As above, a prime p, an element g ∈Z∗
p and an exponent
x are chosen. x is kept secret, whereas p, g and y = gx are published. The
diﬀerence is that g is not a primitive root in Z∗
p, but an element of order q,
where q is a prime divisor of p−1.18 Moreover, the binary size of q is required
to be 160 bits.
To generate a public and a secret key, Alice proceeds as follows:
18 The order of g is the smallest e ∈N with ge = [1]. The order of a primitive root
in Z∗
p is p −1.

74
3. Public-Key Cryptography
1. She chooses a 160-bit prime q and a prime p, such that q divides p −1
(p should have the binary length |p| = 512 + 64t, 0 ≤t ≤8). She can do
this in a way analogous to the key generation in ElGamal’s encryption
scheme. First, she selects q at random, and then she looks for a prime p
in {2kq + 1, 2(k + 1)q + 1, 2(k + 2)q + 1, . . .}, with k a randomly chosen
number of appropriate size.
2. To get an element g of order q, she selects elements h ∈Z∗
p at random
until g := h(p−1)/q ̸= [1]. Then g has order q, and it generates the unique
cyclic group Gq of order q in Z∗
p.19 Note that in Gq elements are computed
modulo p and exponents are computed modulo q.20
3. Finally, she chooses an integer x in the range 1 ≤x ≤q −1 at random.
4. (p, q, g, x) is the secret key, and the public key is (p, q, g, y), with y := gx.
Signing. Messages m to be signed by DSA must be elements in Zq. In DSS,
a hash function h is used to map real messages to elements of Zq. The signed
message is produced by Alice using the following steps:
1. She selects a random integer k, 1 ≤k ≤q −1.
2. She sets r := (gk mod p) mod q and s := k−1(m + rx) mod q. If s = 0,
she returns to step 1, but it is extremely unlikely that this occurs.
3. (m, r, s) is the signed message.
Recall the veriﬁcation condition of ElGamal’s signature scheme. It says
that (m, ˜r, ˜s) with ˜r = gk mod p and ˜s = k−1(m −˜rx) mod (p −1) can be
veriﬁed by
y˜r˜r˜s ≡gm mod p.21
Now suppose that, as in DSA, ˜s is deﬁned by use of (m + ˜rx), ˜s = (m +
˜rx)k−1 mod (p −1), and not by use of (m −˜rx), as in ElGamal’s scheme.
Then the equation remains valid if we replace the exponent ˜r of y by −˜r:
y−˜r˜r˜s ≡gm mod p.
(3.1)
In the DSA, g and hence ˜r and y are elements of order q in Z∗
p. Thus, we can
replace the exponents ˜s and ˜r in (3.1) by ˜s mod q = s and ˜r mod q = r. So,
we have the idea that a veriﬁcation condition for (r, s) may be derived from
(3.1) by reducing ˜r and ˜s modulo q. This is not so easy, because the exponent
˜r also appears as a base on the left-hand side of (3.1). The base cannot be
reduced without destroying the equality. To overcome this diﬃculty, we ﬁrst
transform (3.1) to
˜rs ≡gmyr mod p.
19 There is a unique subgroup Gq of order q of Z∗
p. Gq consists of the unit element
and all elements x ∈Z∗
p of order q. It is cyclic and each member except the unit
element is a generator, see Lemma A.40.
20 See “Computing modulo a prime” on page 303.
21 We write “mod p” to make clear that computations are done in Zp.

3.5 The Discrete Logarithm
75
Now, the idea of DSA is to remove the exponentiation on the left-hand side.
This is possible because s is a unit in Z∗
q. For t = s−1 mod q, we get
˜r ≡(gmyr)t mod p.
Now we can reduce by modulo q on both sides and obtain the veriﬁcation
condition of DSA:
r = ˜r mod q ≡((gmyr)t mod p) mod q.
A complete proof of the veriﬁcation condition is given below (Proposition
3.12). Note that the exponentiations on the right-hand side are done in Zp.
Veriﬁcation. Bob veriﬁes the signed message (m, r, s) as follows:
1. He veriﬁes that 1 ≤r ≤q −1 and 1 ≤s ≤q −1; if not, then he rejects
the signature.
2. He computes the inverse t := s−1 of s modulo q and v := ((gmyr)t mod
p) mod q, where y is the public key of Alice.
3. The signature is accepted if v = r; otherwise it is rejected.
Proposition 3.12. If Alice signed the message (m, r, s), we have v = r.
Proof.
v = ((gmyr)t mod p) mod q = (gmtgrxt mod p) mod q
= (g(m+rx)t mod q mod p) mod q = (gk mod p) mod q
= r.
Note that exponents can be reduced by modulo q, because gq ≡1 mod p. 2
Remarks:
1. Compared with ElGamal, the DSA has the advantage that signatures are
fairly short, consisting of two 160-bit numbers.
2. In DSA, most computations – in particular the exponentiations – take
place in the ﬁeld Z∗
p. The security of DSA depends on the diﬃculty of
computing discrete logarithms. So it relies on the discrete logarithm as-
sumption. This assumption says that it is infeasible to compute the dis-
crete logarithm x of an element y = gx randomly chosen from Z∗
p, where p
is a suﬃciently large prime and g is a primitive root of Z∗
p (see Deﬁnition
6.1 for a precise statement). Here, as in some cryptographic protocols
discussed in Chapter 4 (commitments, electronic elections and digital
cash), the base g is not a primitive root (with order p −1), but an ele-
ment of order q, where q is a large prime divisor of p−1. To get the secret
key x, it would suﬃce to ﬁnd discrete logarithms for random elements
y = gx from the much smaller subgroup Gq generated by g. Thus, the se-
curity of DSA (and some protocols discussed in Chapter 4) requires the
(widely believed) stronger assumption that ﬁnding discrete logarithms
for elements randomly chosen from the subgroup Gq is infeasible.

76
3. Public-Key Cryptography
3. The remarks on the security of ElGamal’s signature scheme also apply
to DSA and DSS.
4. In the DSS, messages are ﬁrst hashed before signed by DSA. The DSS
suggests taking SHA-1 for the hash function.
3.6 Modular Squaring
Breaking the RSA cryptosystem might be easier than solving the factoring
problem. It is widely believed to be equivalent to factoring, but no proof
of this assumption exists. Rabin proposed a cryptosystem whose underlying
encryption algorithm is provably as diﬃcult to break as the factorization of
large numbers (see [Rabin79]).
3.6.1 Rabin’s Encryption
Rabin’s system is based on the modular squaring function
Square : Zn −→Zn, m 7−→m2.
This is a one-way function with trapdoor, if the factoring of large numbers
is assumed to be infeasible (see Section 3.2.2).
Key Generation. As in the RSA scheme, we randomly choose two large
distinct primes, p and q, for Bob. The scheme works with arbitrary primes,
but primes p and q, such that p, q ≡3 mod 4 speed up the decryption algo-
rithm. Such primes are found as in the RSA key generation procedure. We
are looking for primes p and q of the form 4k +3 in order to get the condition
p, q ≡3 mod 4. Then n = pq is used as the public key and p and q are used
as secret key.
Encryption and Decryption. We suppose that the messages to be en-
crypted are elements in Zn. The modular squaring one-way function is used
as the encryption function E:
E : Zn −→Zn, m 7−→m2.
To decrypt a ciphertext c, Bob has to compute the square roots of c in Zn.
The computation of modular square roots is discussed in detail in Appendix
A.7. For example, square roots modulo n can be eﬃciently computed, if and
only if the factors of n can be eﬃciently computed. Bob can compute the
square roots of c because he knows the secret key p and q.
Using the Chinese Remainder Theorem (Theorem A.29), he decomposes
Zn:
φ : Zn −→Zp × Zq, c 7−→(c mod p, c mod q).
Then he computes the square roots of c mod p and the square roots of c mod
q. Combining the solutions, he gets the square roots modulo n. If p divides

3.6 Modular Squaring
77
c (or q divides c), then the only square root modulo p (or modulo q) is 0.
Otherwise, there are two distinct square roots modulo p and modulo q. Since
the primes are ≡3 mod 4, the square roots can be computed by one modular
exponentiation (see Algorithm A.61). Bob combines the square roots modulo
p and modulo q by using the Chinese Remainder Theorem, and obtains four
distinct square roots of c (or two roots in the rare case that p or q divides c, or
the only root 0 if c = 0). He has to decide which of the square roots represents
the plaintext. There are diﬀerent approaches. If the message is written in some
natural language, it should be easy to choose the right one. If the messages are
unstructured, one way to solve the problem is to add additional information.
The sender, Alice, might add a header to each message consisting of the
Jacobian symbol
¡ m
n
¢
and the sign bit b of m. The sign bit b is deﬁned as 0
if 0 ≤m < n/2, and 1 otherwise. Now Bob can easily determine the correct
square root (see Proposition A.66).
Remark. The diﬃculty of computing square roots modulo n is equivalent to
the diﬃculty of computing the factors of n (see Proposition A.64). Hence,
Rabin’s encryption resists ciphertext-only attacks as long as factoring is im-
possible. The basic scheme, however, is completely insecure against a chosen-
ciphertext attack. If adversary Eve can use the decryption algorithm as a
black box, she can determine the secret key using the following attack. She
selects m in the range 0 < m < n and computes c = m2 mod n. Decrypting c
delivers y. There is a 50% chance that m ̸≡±y mod n, and in this case Eve
can easily compute the prime factors of n from m and y (see Lemma A.63).
If the Jacobian symbol
¡ m
n
¢
is added to each message m as sketched above,
Eve may choose m with
¡ m
n
¢
= −1, but add +1 to the header. Then she
will be certain to get a square root y with m ̸≡±y. Applying some proper
formatting, as in the OAEP scheme, can prevent this attack.
3.6.2 Rabin’s Signature Scheme
The decryption function in Rabin’s cryptosystem is only applicable to quad-
ratic residues modulo n. Therefore, the system is not immediately applicable
as a digital signature scheme. Before applying the decryption function, we
usually apply a hash function to the message to be signed. Here we need a
collision-resistant hash function whose values are quadratic residues modulo
n. Such a hash function is obtained by the following construction. Let M be
the message space and let
h : M × {0, 1}k −→Zn, (m, x) 7−→h(m, x)
be a hash function. To sign a message m, Bob generates pseudorandom bits
x using a pseudorandom bit generator and computes h(m, x). Knowing the
factors of n, he can easily test whether z := h(m, x) ∈QRn. z is a square if
and only if z mod p and z mod q are squares, and z mod p is a square in Zp,

78
3. Public-Key Cryptography
if and only if z(p−1)/2 ≡1 mod p (Proposition A.52). He repeatedly chooses
pseudorandom bit strings x until h(m, x) is a square in Zn. Then he computes
a square root y of h(m, x) (e.g. using Proposition A.62). The signed message
is deﬁned as
(m, x, y).
A signed message (m, x, y) is veriﬁed by testing
h(m, x) = y2.
If an adversary Eve is able to make Bob sign hash values of her choice, she
can ﬁgure out Bob’s secret key (see above).
Exercises
1. Set up an RSA encryption scheme by generating a pair of public and
secret keys. Choose a suitable plaintext and a ciphertext. Encrypt and
decrypt them.
2. Let n denote the product of two distinct primes p and q, and let e ∈N.
Show that e is prime to ϕ(n) if
µ : Z∗
n −→Z∗
n, x 7−→xe
is bijective.
3. Let RSAe : Z∗
n −→Z∗
n, x 7−→xe. Show that
|{x ∈Z∗
n | RSAe(x) = x}| = gcd(e −1, p −1) · gcd(e −1, q −1).
Hint: Show that |{x ∈Z∗
p | xk = 1}| = gcd(k, p −1), where p is a prime,
and use the Chinese Remainder Theorem (Theorem A.29).
4. Let (n, e) be the public key of an RSA cryptosystem and d be the associ-
ated decryption exponent. Construct an eﬃcient probabilistic algorithm
A which on input n, e and d computes the prime factors p and q of n
with very high probability.
Hint: Use the idea of the Miller-Rabin primality test, especially case 2 in
the proof of Proposition A.78.
5. Consider RSA encryption. Discuss, in detail, the advantage you get using,
for encryption, a public exponent which has only two 1 digits in its binary
encoding and using, for decryption, the Chinese Remainder Theorem.
6. Let p, p′, q and q′ be prime numbers, with p′ ̸= q′, p = ap′ +1, q = bq′ +1
and n := pq:
a. Show |{x ∈Z∗
p | p′ does not divide ord(x)}| = a.

Exercises
79
b. Assume that p′ and q′ are large (compared to a and b). Let x ∈Z∗
n
be a randomly chosen element. Show that the probability that x has
large order is ≥1 −
¡1/p′ + 1/q′ −1/p′q′¢
. More precisely, show
|{x ∈Z∗
n | p′q′ does not divide ord(x)}| = a(q −1) + b(p −1) −ab.
7. Consider RSA encryption RSAe : Z∗
n −→Z∗
n, x 7−→xe:
a. Show that RSAl
e = idZ∗n for some l ∈N.
b. Consider the decryption-by-iterated-encryption attack (see Section
3.3.1). Let p, p′, p′′, q, q′ and q′′ be prime numbers, with p′ ̸= q′,
p = ap′ + 1, q = bq′ + 1, p′ = a′p′′ + 1, q′ = b′q′′ + 1, n := pq and
n′ := p′q′. Assume that p′ and q′ are large (compared to a and b)
and that p′′ and q′′ are large (compared to a′ and b′). (This means
that the factors of n satisfy the conditions 1 and 3 required for strong
primes.)
Show that the number of iterations necessary to decrypt a ciphertext
c is ≥p′′q′′ (and thus very large) for all but an exponentially small
fraction of ciphertexts. By exponentially small, we mean ≤2−|n|/k
(k, constant).
8. Let p be a large prime, such that q := (p −1)/2 is also prime. Let Gq be
the subgroup of order q in Z∗
p. Let g and h be randomly chosen generators
in Gq. We assume that it is infeasible to compute discrete logarithms in
Gq. Show that
f : {0, . . . , q −1}2 −→Gq, (x, y) 7−→gxhy
can be used to obtain a collision-resistant compression function.
9. Let h : {0, 1}∗−→{0, 1}n be a cryptographic hash function. We assume
that the hash values are uniformly distributed, i.e., each value v ∈{0, 1}n
has the same probability 1/2n. How many steps do you expect until you
succeed with the brute-force attacks against the one-way property and
second pre-image resistance?
10. Set up an ElGamal encryption scheme by generating a pair of public and
secret keys.
a. Choose a suitable plaintext and a ciphertext. Encrypt and decrypt
them.
b. Generate ElGamal signatures for suitable messages. Verify the sig-
natures.
c. Forge a signature without using the secret key.
d. Play the role of an adversary Eve, who learns the random number k
used to generate a signature, and break the system.
e. Demonstrate that checking the condition 1 ≤r ≤p −1 is necessary
in the veriﬁcation of a signature (r, s).
11. Weak generators (see [Bleichenbacher96]; [MenOorVan96]).
Let p be a prime, p ≡
1 mod 4, and g ∈Z, such that g mod p is a

80
3. Public-Key Cryptography
primitive root in Z∗
p. Let (p, g, x) be Bob’s secret key and (p, g, y = gx)
be Bob’s public key in an ElGamal signature scheme. We assume that: (1)
p −1 = gt and (2) discrete logarithms can be eﬃciently computed in the
subgroup H of order g in Z∗
p (e.g. using the Pohlig-Hellman algorithm).
To sign a message m, adversary Eve does the following: (1) she sets
r = t; (2) she computes z, such that gtz = yt; and (3) she sets s =
1
2(p −3)(m −tz) mod (p −1).
Show:
a. That it is possible to compute z in step 2.
b. That (r, s) is accepted as Bob’s signature for m.
c. How the attack can be prevented.
12. Set up a DSA signature scheme by generating a pair of public and secret
keys. Generate and verify signatures for suitable messages. Take small
primes p and q.
13. Set up a Rabin encryption scheme by generating a pair of public and
secret keys. Choose a suitable plaintext and a ciphertext. Encrypt and
decrypt them. Then play the role of an adversary Eve, who succeeds in
a chosen-ciphertext attack and recovers the secret key.

4. Cryptographic Protocols
One of the major contributions of modern cryptography is the development of
advanced protocols providing high-level cryptographic services, such as secure
user identiﬁcation, voting schemes and digital cash. Cryptographic protocols
use cryptographic mechanisms – such as encryption algorithms and digital
signature schemes – as basic components.
A protocol is a multi-party algorithm which requires at least two par-
ticipating parties. Therefore, the algorithm is distributed and invoked in at
least two diﬀerent places. An algorithm that is not distributed, is not called
a protocol. The parties of the algorithm must communicate with each other
to complete the task. The communication is described by the messages to be
exchanged between the parties. These are referred to as the communication
interface. The protocol requires precise deﬁnitions of the interface and the
actions to be taken by each party.
A party participating in a protocol must fulﬁll the syntax of the com-
munication interface, since not following the syntax would be immediately
detected by the other parties. The party can behave honestly and follow the
behavior speciﬁed in the protocol. Or she can behave dishonestly, only ful-
ﬁll the syntax of the communication interface and do completely diﬀerent
things otherwise. These points must be taken into account when designing
cryptographic protocols.
4.1 Key Exchange and Entity Authentication
Public- and secret-key cryptosystems assume that the participating parties
have access to keys. In practice, one can only apply these systems if the
problem of distributing the keys is solved.
The security concept for keys, which we describe below, has two levels.
The ﬁrst level embraces long-lived, secret keys, called master keys. The keys
of the second level are associated with a session, and are called session keys.
A session key is only valid for the short time of the duration of a session. The
master keys are usually keys of a public-key cryptosystem.
There are two main reasons for the two-level concept. The ﬁrst is that
symmetric key encryption is more eﬃcient than public-key encryption. Thus,
session keys are usually keys of a symmetric cryptosystem, and these keys

82
4. Cryptographic Protocols
must be exchanged in a secure way, by using other keys. The second, probably
more important reason is that the two-level concept provides more security.
If a session key is compromised, it aﬀects only that session; other sessions
in the past or in the future are not concerned. Given one session key, the
number of ciphertexts available for cryptanalysis is limited. Session keys are
generated when actually required and discarded after use; they need not be
stored. Thus, there is no need to protect a large amount of stored keys.
A master key is used for the generation of session keys. Special care is
taken to prevent attacks on the master key. The access to the master key is
severely limited. It is possible to store the master key on protected hardware,
accessible only via a secure interface. The main focus of this section is to
describe how to establish a session key between two parties.
Besides key exchange, we introduce entity authentication. Entity authenti-
cation prevents impersonation. By entity authentication, Alice can convince
her communication partner Bob that, in the current communication, her
identity is as declared. This might be achieved, for example, if Alice signs a
speciﬁc message. Alice proves her identity by her signature on the message. If
an adversary Eve intercepts the message signed by Alice, she can use it later
to authenticate herself as Alice. Such attacks are called replay attacks. A re-
play attack can be prevented if the message to be signed by Alice varies. For
this purpose we introduce two methods. In the ﬁrst method, Alice puts Bob’s
name and a time stamp into the message she signs. Bob accepts a message
only if it appears the ﬁrst time. The second method uses random numbers. A
random number is chosen by Bob and transmitted to Alice. Then Alice puts
the random number into the message, signs the message and returns it to
Bob. Bob can check that the returned random number matches the random
number he sent and the validity of Alice’s signature. The random number
is viewed as a challenge. Bob sends a challenge to Alice and Alice returns a
response to Bob’s challenge. We speak of challenge-response identiﬁcation.
Some of the protocols we discuss provide both entity authentication and
key exchange.
4.1.1 Kerberos
Kerberos denotes the distributed authentication service originating from
MIT’s project Athena. Here we use the term Kerberos in a restricted way: we
deﬁne it as the underlying protocol that provides both entity authentication
and key establishment, by use of symmetric cryptography and a trusted third
party.
We continue with our description in [NeuTs’o94]. In that overview article,
a simpliﬁed version of the basic protocol is introduced to make the basic
principles clear. Kerberos is designed to authenticate clients who try to get
access to servers in a network. A central role is played by a trusted server
called the Kerberos authentication server.

4.1 Key Exchange and Entity Authentication
83
The Kerberos authentication server T shares a secret key of a symmetric
key encryption scheme E with each client A and each server B in the network,
denoted by kA and kB respectively. Now, assume that client A wants to access
server B. At the outset, A and B do not share any secrets. The execution of
the Kerberos protocol involves A, B and T, and proceeds as follows:
Client A sends a request to the authentication server T, requesting creden-
tials for server B. T responds with these credentials. The credentials consist
of:
1. A ticket t for the server B, encrypted with B’s secret key kB.
2. A session key k, encrypted with A’s key kA.
The ticket t contains A’s identity and a copy of the session key. It is intended
for B. A will forward it to B. The ticket is encrypted with kB, which is
known only to B and T. Thus, it is not possible for A to modify the ticket
without being detected. A creates an authenticator which also contains A’s
identity, encrypts it with the session key k (by this encryption A proves to
B that she knows the session key embedded in the ticket) and transmits
the ticket and the authenticator to B. B trusts the ticket (it is encrypted
with kB, hence it originates from T), decrypts it and gets k. Now B uses
the session key to decrypt the authenticator. If B succeeds, he is convinced
that A encrypted the authenticator, because only A and the trusted T can
know k. Thus A is authenticated to B. Optionally, the session key k can also
be used to authenticate B to A. Finally, k may be used to encrypt further
communication between the two parties in the current session.
Kerberos protects the ticket and the authenticator against modiﬁcation by
encrypting it. Thus, the encryption algorithm E is assumed to have built-in
data integrity mechanisms.
Protocol 4.1.
Basic Kerberos authentication protocol (simpliﬁed):
1. A chooses rA at random1 and sends (A, B, rA) to T.
2. T generates a new session key k, and creates a ticket
t := (A, k, l). Here l deﬁnes a validity period (consisting of a
starting and an ending time) for the ticket. T sends
(EkA(k, rA, l, B), EkB(t)) to A.
3. A recovers k, rA, l and B, and veriﬁes that rA and B match those
sent in step 1. Then she creates an authenticator
a := (A, tA), where tA is a time stamp from A’s local clock, and
sends (Ek(a), EkB(t)) to B.
4. B recovers t = (A, k, l) and a = (A, tA), and checks that:
a. The identiﬁer A in the ticket matches the one in the authen-
ticator.
1 In this chapter all random choices are with respect to the uniform distribution.
All elements have the same probability (see Appendix B.1).

84
4. Cryptographic Protocols
b. The time stamp tA is fresh, i.e., within a small time interval
around B’s local time.
c. The time stamp tA is in the validity period l.
If all checks pass, B considers the authentication of A as success-
ful.
If, in addition, B is to be authenticated to A, steps 5 and 6 are
executed:
5. B takes tA and sends Ek(tA) to A.
6. A recovers tA from Ek(tA) and checks whether it matches with
the tA sent in step 3. If yes, she considers B as authenticated.
Remarks:
1. In step 1, a random number is included in the request. It is used to
match the response in step 2 with the request. This ensures that the
Kerberos authentication server is alive and created the response. In step
3, a time stamp is included in the request to the server. This prevents
replay attacks of such requests. To avoid perfect time synchronization, a
small time interval around B’s local time (called a window) is used. The
server accepts the request if its time stamp is in the current window and
appears the ﬁrst time. The use of time stamps means that the network
must provide secure and synchronized clocks. Modiﬁcations of local time
clocks by adversaries must be prevented to guarantee the security of the
protocol.
2. The validity period of a ticket allows the reuse of the ticket in that period.
Then steps 1 and 2 in the protocol can be omitted. The client can use
the ticket t to repeatedly get access to the server for which the ticket was
issued. Each time, she creates a new authenticator and executes steps 3
and 4 (or steps 3–6) of the protocol.
3. Kerberos is a popular authentication service. Version 5 of Kerberos (the
current version) was speciﬁed in [RFC 1510]. Kerberos is based in part
on Needham and Schroeder’s trusted third-party authentication protocol
[NeeSch78].
4. In the non-basic version of Kerberos, the authentication server is only
used to get tickets for the ticket-granting server. These tickets are called
ticket-granting tickets. The ticket-granting server is a specialized server,
granting tickets (server tickets) for the other servers (the ticket-granting
server must have access to the servers’ secret keys, so usually the authen-
tication server and the ticket granting server run on the same host).
Client A executes steps 1 and 2 of Protocol 4.1 with the authentication
server in order to obtain a ticket-granting ticket. Then A uses the ticket-
granting ticket – more precisely, the session key included in the ticket
granting ticket – to authenticate herself to the ticket-granting server and
to get server tickets. The ticket-granting ticket can be reused during its
validity period for the intended ticket-granting server. As long as the
same ticket-granting ticket is used, the client’s secret key kA is not used

4.1 Key Exchange and Entity Authentication
85
again. This reduces the risk of exposing kA. We get a three-level key
scheme. The ﬁrst embraces the long-lived secret keys of the participating
clients and servers. The keys of the second level are the session keys of
the ticket-granting tickets, and the keys of the third level are the session
keys of the server tickets.
A ticket-granting ticket is veriﬁed by the ticket-granting server in the
same way as any other ticket (see above). The ticket-granting server de-
crypts the ticket, extracts the session key and decrypts the authenticator
with the session key. The client uses a ticket from the ticket-granting
server as in the basic model to authenticate to a service in the network.
4.1.2 Diﬃe-Hellman Key Agreement
Diﬃe-Hellman key agreement (also called exponential key exchange) pro-
vided the ﬁrst practical solution to the key distribution problem. It is based
on public-key cryptography. W. Diﬃe and M.E. Hellman published their
fundamental technique of key exchange together with the idea of public-key
cryptography in the famous paper, “New Directions in Cryptography”, in
1976 in [DifHel76]. Exponential key exchange enables two parties that have
never communicated before to establish a mutual secret key by exchanging
messages over a public channel. However, the scheme only resists passive
adversaries.
Let p be a suﬃciently large prime, such that it is intractable to compute
discrete logarithms in Z∗
p. Let g be a primitive root in Z∗
p. p and g are publicly
known. Alice and Bob can establish a secret shared key by executing the
following protocol:
Protocol 4.2.
Diﬃe-Hellman key agreement:
1. Alice chooses a, 0 ≤a ≤p−2, at random, sets c := ga and sends
c to Bob.
2. Bob chooses b, 0 ≤b ≤p −2, at random, sets d := gb and sends
d to Alice.
3. Alice computes the shared key k = da = (gb)a.
4. Bob computes the shared key k = cb = (ga)b.
Remarks:
1. If an attacker can compute discrete logarithms in Z∗
p, he can compute a
from c and then k = da. However, to get the secret key k, it would suﬃce
to compute gab from ga and gb. This problem is called the Diﬃe-Hellman
problem. The security of the protocol relies on the assumption that no
eﬃcient algorithms exist to solve this problem. This assumption is called
the Diﬃe-Hellman assumption. It implies the discrete logarithm assump-
tion. For certain primes, the Diﬃe-Hellman and the discrete logarithm

86
4. Cryptographic Protocols
assumption have been proven to be equivalent ([Boer88]; [Maurer94];
[MauWol96]; [MauWol98]; [MauWol2000]).
2. Alice and Bob can use the randomly chosen element k = gab ∈Z∗
p as
their session key. The Diﬃe-Hellman assumption does not guarantee that
individual bits or groups of bits of k cannot be eﬃciently derived from
ga and gb – this would be a stronger assumption.
It is recommended to make prime p 1024 bits long. Usually, the length
of a session key in a symmetric key encryption scheme is much smaller,
say 128 bits. If we take, for example, the 128 most-signiﬁcant bits of k
as a session key ˜k, then ˜k is hard to compute from ga and gb. However,
we do not know that all the individual bits of ˜k are secure (on the other
hand, none of the more signiﬁcant bits is known to be easy to compute).
In [BonVen96] it is shown that computing the
p
|p| most-signiﬁcant bits
of gab from ga and gb is as diﬃcult as computing gab from ga and gb. For
a 1024-bit prime p, this result implies that the 32 most-signiﬁcant bits of
gab are hard to compute. The problem of ﬁnding a more secure random
session key can be solved by applying an appropriate hash function h to
gab, and taking ˜k = h(gab).
3. This protocol provides protection against passive adversaries. An active
attacker Eve can intercept the message sent to Bob by Alice and then play
Bob’s role. The protocol does not provide authentication of the opposite
party. Combined with authentication techniques, the Diﬃe-Hellman key
agreement can be used in practice (see Section 4.1.4).
4.1.3 Key Exchange and Mutual Authentication
The problem of spontaneous key exchange, like Diﬃe-Hellman’s key agree-
ment, is the authenticity of the communication partners in an open net-
work. Entity authentication (also called entity identiﬁcation) guarantees the
identity of the communicating parties in the current communication ses-
sion, thereby preventing impersonation. Mutual entity authentication re-
quires some mutual secret, which has been exchanged previously, or access to
predistributed authentic material, like the public keys of a digital signature
scheme.
The protocol we describe is similar to the X.509 strong three-way au-
thentication protocol (see [ISO/IEC 9594-8]). It provides entity authentica-
tion and key distribution, two diﬀerent cryptographic mechanisms. The term
“strong” distinguishes the protocol from simpler password-based schemes.
To set up the scheme, a public-key encryption scheme (E, D) and a digital
signature scheme (Sign, Verify) are needed. Each user Alice has a key pair
(eA, dA) for encryption and another key pair (sA, vA) for digital signatures.
It is assumed that everyone has access to Alice’s authentic public keys, eA
and vA, for encryption and the veriﬁcation of signatures.

4.1 Key Exchange and Entity Authentication
87
Executing the following protocol, Alice and Bob establish a secret session
key. Furthermore, the protocol guarantees the mutual authenticity of the
communication parties.
Protocol 4.3.
Strong three-way authentication:
1. Alice chooses rA at random, sets t1 := (B, rA) (where B repre-
sents Bob’s identity), s1 := SignsA(t1) and sends (t1, s1) to Bob.
2. Bob veriﬁes Alice’s signature, checks that he is the intended
recipient, chooses rB and a session key k at random, encrypts
the session key with Alice’s public key, c := EeA(k), sets t2 :=
(A, rA, rB, c), signs t2 to get s2 := SignsB(t2) and sends (t2, s2)
to Alice.
3. Alice veriﬁes Bobs signature, checks that she is the intended re-
cipient and that the rA she received matches the rA from step 1
(this prevents replay attacks). If both veriﬁcations pass, she is
convinced that her communication partner is Bob. Now Alice de-
crypts the session key k, sets t3 := (B, rB), s3 := SignsA(t3) and
sends (t3, s3) to Bob.
4. Bob veriﬁes Alice’s signature and checks that the rB he received
matches the rB from step 2 (this again prevents replay attacks).
If both veriﬁcations pass, Bob and Alice use k as the session key.
Remarks:
1. The protocol identiﬁes the communication partner by checking that she
possesses the secret key of the signature scheme. The check is done by
the challenge-response principle. First the challenge, a random number,
used only once, is submitted. If the partner can return a signature of this
random number, he necessarily possesses the secret key, thus proving his
identity. The messages exchanged (the communication tokens) are signed
by the sender and contain the recipient. This guarantees that the token
was constructed for the intended recipient by the sender. Three messages
are exchanged in the above protocol. Therefore, it is called the three-way
or three-pass authentication protocol.
There are also two-way authentication protocols. To prevent replay at-
tacks, the communication tokens must be stored. Since these commu-
nication tokens have to be deleted from time to time, they are given a
time stamp and an expiration time. This requires a network with secure
and synchronized clocks. A three-way protocol requires more messages
to be exchanged, but avoids storing tokens and maintaining secure and
synchronized clocks.
2. The session key is encrypted with a public-key cryptosystem. Suppose
adversary Eve records all the data that Alice and Bob have exchanged,

88
4. Cryptographic Protocols
hoping that Alice’s secret encryption key will be compromised in the fu-
ture. If Eve really gets Alice’s secret key, she can decrypt the data of all
sessions she recorded. A key-exchange scheme which resists this attack
is said to have forward secrecy. The Diﬃe-Hellman key agreement does
not encrypt a session key. Thus, the session key cannot be revealed by a
compromised secret key. If we combine the Diﬃe-Hellman key agreement
with the authentication technique of the previous protocol, as in Sec-
tion 4.1.4, we achieve a key-exchange protocol with authentication and
forward secrecy.
4.1.4 Station-to-Station Protocol
The station-to-station protocol, combines Diﬃe-Hellman key agreement with
authentication. It goes back to earlier work on ISDN telephone security, as
outlined by W. Diﬃe in [Diﬃe88], in which the protocol is executed between
two ISDN telephones (stations). The station-to-station protocol enables two
parties to establish a shared secret key k to be used in a symmetric encryption
algorithm E. Additionally, it provides mutual authentication.
Let p be a prime such that it is intractable to compute discrete logarithms
in Z∗
p. Let g be a primitive root in Z∗
p. p and g are publicly known. Further,
we assume that a digital signature scheme (Sign, Verify) is available. Each
user Alice has a key pair (sA, vA) for this signature scheme. sA is the secret
key for signing and vA is the public key for verifying Alice’s signatures. We
assume that each party has access to authentic copies of the other’s public
key.
Alice and Bob can establish a secret shared key and authenticate each
other if they execute the following protocol:
Protocol 4.4.
Station-to-station protocol:
1. Alice chooses a, 0 ≤a ≤p−2, at random, sets c := ga and sends
c to Bob.
2. Bob chooses b, 0 ≤b ≤p −2, at random, computes the shared
secret key k = gab, takes his secret key sB and signs the con-
catenation of ga and gb to get s := SignsB(ga||gb). Then he sends
(gb, Ek(s)) to Alice.
3. Alice computes the shared key k = gab, decrypts Ek(s) and ver-
iﬁes Bob’s signature. If this veriﬁcation succeeds, Alice is con-
vinced that the opposite party is Bob. She takes her secret key
sA, generates the signature s := SignsA(gb||ga) and sends Ek(s)
to Bob.
4. Bob decrypts Ek(s) and veriﬁes Alice’s signature. If the veriﬁca-
tion succeeds, Bob accepts that he actually shares k with Alice.

4.1 Key Exchange and Entity Authentication
89
Remarks:
1. Both Alice and Bob contribute to the random strings ga||gb and gb||ga.
Thus each string can serve as a challenge.
2. Encrypting the signatures with the key k guarantees that the party who
signed also knows the secret key k.
4.1.5 Public-Key Management Techniques
In the public-key-based key-exchange protocols discussed in the previous sec-
tions, we assumed that each party has access to the other parties’ (authentic)
public keys. This requirement can be met by public-key management tech-
niques. A trusted third party C is needed, similar to the Kerberos authenti-
cation server in the Kerberos protocol. However, in contrast to Kerberos, the
authentication transactions do not include an online communication with C.
C prepares information in advance, which is then available to Alice and Bob
during the execution of the authentication protocol. We say that C is oﬀline.
Oﬀline third parties reduce network traﬃc, which is advantageous in large
networks.
Certiﬁcation Authority. The oﬄine trusted party is referred to as a cer-
tiﬁcation authority. Her tasks are:
1. To verify the authenticity of the entity to be associated with a public
key.
2. To bind a public key to a distinguished name and to register it.
3. (Optionally) to generate a party’s private and public key.
Certiﬁcates play a fundamental role. They enable the storage and forward-
ing of public keys over insecure media, without danger of undetected manipu-
lation. Certiﬁcates are signed by a certiﬁcation authority, using a public-key
signature scheme. Everyone knows the certiﬁcation authority’s public key.
The authenticity of this key may be provided by non-cryptographic means,
such as couriers or personal acquisition. Another method would be to publish
the key in all newspapers. The public key of the certiﬁcation authority can
be used to verify certiﬁcates signed by the certiﬁcation authority. Certiﬁcates
prove the binding of a public key to a distinguished name. The signature of
the certiﬁcation authority protects the certiﬁcate against undetected manip-
ulation. We list some of the most important data stored on a certiﬁcate:
1. A distinguished name (the real name or a pseudonym of the owner of the
certiﬁcate).
2. The owner’s public key.
3. The name of the certiﬁcation authority.
4. A serial number identifying the certiﬁcate.
5. A validity period of the certiﬁcate.

90
4. Cryptographic Protocols
Creation of Certiﬁcates. If Alice wants to get a certiﬁcate, she goes to
a certiﬁcation authority C. To prove her identity, Alice shows her passport.
Now, Alice needs public- and private-key pairs for encryption and digital
signatures. Alice can generate the key pair and hand over a copy of the
public key to C. This alternative might be taken if Alice uses a smart card to
store her secret key. Smart cards often involve key generation functionality.
If the keys are generated inside the smart card, the private key never leaves
it. This reduces the risk of theft. Another model is that C generates the key
pair and transmits the secret key to Alice. The transmission requires a secret
channel. C must of course be trustworthy, because she has the opportunity
to steal the secret key. After the key generation, C puts the public key on
the certiﬁcate, together with all the other necessary information, and signs
the certiﬁcate with her secret key.
Storing Certiﬁcates. Alice can take her certiﬁcate and store it at home.
She provides the certiﬁcate to others when needed, for example for signature
veriﬁcation. A better solution in an open system is to provide a certiﬁcate
directory, and to store the certiﬁcates there. The certiﬁcate directory is a
(distributed) database, usually maintained by the certiﬁcation authority. It
enables the search and retrieval of certiﬁcates.
Usage of Certiﬁcates. If Bob wants to encrypt a message for Alice or to
verify a signature allegedly produced by Alice, he retrieves Alice’s certiﬁcate
from the certiﬁcate directory (or from Alice) and veriﬁes the certiﬁcation
authority’s signature. If the veriﬁcation is successful, he can be sure that he
really receives Alice’s public key from the certiﬁcate and can use it.
For reasons of operational eﬃciency, multiple certiﬁcation authorities are
needed in large networks. If Alice and Bob belong to diﬀerent certiﬁcation
authorities, Bob must access an authentic copy of the public key of Alice’s
certiﬁcation authority. This is possible if Bob’s certiﬁcation authority CB has
issued a certiﬁcate for Alice’s certiﬁcation authority CA. Then Bob retrieves
the certiﬁcate for CA, veriﬁes it and can then trust the public key of CA.
Now he can retrieve and verify Alice’s certiﬁcate.
It is not necessary that each certiﬁcation authority issues a certiﬁcate
for each other certiﬁcation authority in the network. We may use a directed
graph as a model. The vertices correspond to the certiﬁcation authorities,
and an edge from CA to CB corresponds to a certiﬁcate of CA for CB. Then,
a directed path should connect any two certiﬁcation authorities. This is the
minimal requirement which guarantees that each user in the network can
verify each other user’s certiﬁcate.
However, the chaining of authentications may reduce the trust in the ﬁnal
result: the more people you have to trust, the greater the risk that you have
a cheater in the group.
Revocation of Certiﬁcates. If Alice’s secret key is compromised, the cor-
responding public key can no longer be used for encrypting messages. If the
key is used in a signature scheme, Alice can no longer sign messages with

4.2 Identiﬁcation Schemes
91
this key. Moreover, it should be possible for Alice to deny all signatures pro-
duced with this key from then on. Therefore, the fact that Alice’s secret key
is compromised must be publicized. Of course, the certiﬁcation authority will
remove Alice’s certiﬁcate from the certiﬁcate directory. However, certiﬁcates
may have been retrieved before and may not yet have expired. It is not pos-
sible to notify all users possessing copies of Alice’s certiﬁcate: they are not
known to the certiﬁcation authority. A solution to this problem is to main-
tain certiﬁcate revocation lists. A certiﬁcate revocation list is a list of entries
corresponding to revoked certiﬁcates. To guarantee authenticity, the list is
signed by the certiﬁcation authority.
4.2 Identiﬁcation Schemes
There are many situations where it is necessary to “prove” one’s identity.
Typical scenarios are to login to a computer, to get access to an account
for electronic banking or to withdraw money from an automatic teller ma-
chine. Older methods use passwords or PINs to implement user identiﬁcation.
Though successfully used in certain environments, these methods also have
weaknesses. For example, anyone to whom you must give your password to
be veriﬁed has the ability to use that password and impersonate you. Zero-
knowledge (and other) identiﬁcation schemes provide a new type of user
identiﬁcation. It is possible for you to authenticate yourself without giving
to the authenticator the ability to impersonate you. We will see that very
eﬃcient implementations of such schemes exist.
4.2.1 Interactive Proof Systems
There are two participants in an interactive proof system, the prover and the
veriﬁer. It is common to call the prover Peggy and the veriﬁer Vic. Peggy
knows some fact (e.g. a secret key sk of a public-key cryptosystem or a
square root of a quadratic residue x), which we call the prover’s secret. In an
interactive proof of knowledge, Peggy wishes to convince Vic that she knows
the prover’s secret. Peggy and Vic communicate with each other through a
communication channel. Peggy and Vic alternately perform moves consisting
of:
1. Receive a message from the opposite party.
2. Perform some computation.
3. Send a message to the opposite party.
Usually, Peggy starts and Vic ﬁnishes the protocol. In the ﬁrst move, Peggy
does not receive a message. The interactive proof may consist of several
rounds. This means that the protocol speciﬁes a sequence of moves, and
this sequence is repeated a speciﬁed number of times. Typically, a move con-
sists of a challenge by Vic and a response by Peggy. Vic accepts or rejects

92
4. Cryptographic Protocols
Peggy’s proof, depending on whether Peggy successfully answers all of Vic’s
challenges.
Proofs in interactive proof systems are quite diﬀerent from proofs in math-
ematics. In mathematics, the prover of some theorem can sit down and prove
the statement by himself. In interactive proof systems, there are two com-
putational tasks, namely producing a proof (Peggy’s task) and verifying its
validity (Vic’s task). Additionally, communication between the prover and
veriﬁer is necessary.
We have the following requirements for interactive proof systems.
1. (Knowledge) completeness. If Peggy knows the prover’s secret, then Vic
will always accept Peggy’s proof.
2. (Knowledge) soundness. If Peggy can convince Vic with reasonable prob-
ability, then she knows the prover’s secret.
If the prover and the veriﬁer of an interactive proof system follow the
behavior speciﬁed in the protocol, they are called an honest veriﬁer and an
honest prover. A prover who does not know the prover’s secret and tries to
convince the veriﬁer is called a cheating or dishonest prover. A veriﬁer who
does not follow the behavior speciﬁed in the protocol is called a cheating
or dishonest veriﬁer. Sometimes, the veriﬁer can get additional information
from the prover if he does not follow the protocol. Note that each prover (or
veriﬁer), whether she is honest or not, fulﬁlls the syntax of the communication
interface, because not following the syntax is immediately detected. She may
only be dishonest in her private computations and the resulting data that
she transmits.
Password Scheme. In a simple password scheme, Peggy uses a secret pass-
word to prove her identity. The password is the only message, and it is sent
from the prover Peggy to the veriﬁer Vic. Vic accepts Peggy’s identity if
the transmitted password and the stored password are equal. Here, only one
message is transmitted, and obviously the scheme meets the requirements. If
Peggy knows the password, Vic accepts. If a cheating prover Eve does not
know the password, Vic does not accept. The problem is that everyone who
observed the password during communication can use the password.
Identiﬁcation Based on Public-Key Encryption. In Section 4.1, we
considered an identiﬁcation scheme based on a public-key cryptosystem. We
recall the basic scenario. Each user Peggy has a secret key sk only known
to her and a public key pk known to everyone. Suppose that everyone who
can decrypt a randomly chosen encrypted message must know the secret key.
This assumption should be true if the cryptosystem is secure. Hence, the
secret key sk can be used to identify Peggy.
Peggy proves her identity to Vic using the following steps:
1. Vic chooses a random message m, encrypts it with the public key pk and
sends the cryptogram c to Peggy.

4.2 Identiﬁcation Schemes
93
2. Peggy decrypts c with her secret key sk and sends the result m′ back to
Vic.
3. Vic accepts the identity of Peggy if and only if m = m′.
Two messages are exchanged: it is a two-move protocol. The complete-
ness of the scheme is obvious. On the other hand, a cheating prover who only
knows the public key and a ciphertext should not be able to ﬁnd the plain-
text better than guessing at random. The probability that Vic accepts if the
prover does not know the prover’s secret is very small. Thus, the scheme is
also sound. This reﬂects Vic’s security requirements. Suppose that an adver-
sary Eve observed the exchanged messages and later wants to impersonate
Peggy. Vic chooses m at random and computes c. The probability of obtain-
ing the previously observed c is very small. Thus, Eve cannot take advantage
of observing the exchanged messages. At ﬁrst glance, everything seems to
be all right. However, there is a security problem if Vic is not honest and
does not follow the protocol in step 1. If, instead of a randomly chosen en-
crypted message, he sends a cryptogram intended for Peggy, then he lets
Peggy decrypt the cryptogram. He thereby manages to get the plaintext of
a cryptogram which he could not compute by himself. This violates Peggy’s
security requirements.
4.2.2 Simpliﬁed Fiat-Shamir Identiﬁcation Scheme
Let n := pq, where p and q are distinct primes. As usual, QRn denotes the
subgroup of squares in Z∗
n (see Deﬁnition A.48). Let x ∈QRn, and let y be a
square root of x. The modulus n and the square x are made public, while the
prime factors p, q and y are kept secret. The square root y of x is the secret
of prover Peggy. Here we assume that it is intractable to compute a square
root of x, without knowing the prime factors p and q. This is guaranteed by
the factoring assumption (see Deﬁnition 6.9) if p and q are suﬃciently large
randomly chosen primes, and x is also randomly chosen. Note that the ability
to compute square roots is equivalent to the ability to factorize n (Proposition
A.64). We assume that Peggy chooses n and y, computes x = y2 and publishes
the public key (n, x) to all participants. y is Peggy’s secret. Then Peggy may
prove her identity by an interactive proof of knowledge by proving that she
knows a square root y of x. The computations are done in Z∗
n.
Protocol 4.5.
Fiat-Shamir identiﬁcation (simpliﬁed):
1. Peggy chooses r ∈Z∗
n at random and sets a := r2. Peggy sends
a to Vic.
2. Vic chooses e ∈{0, 1} at random. Vic sends e to Peggy.
3. Peggy computes b := rye and sends b to Vic, i.e., Peggy sends r
if e = 0, and ry if e = 1.
4. Vic accepts if and only if b2 = axe.

94
4. Cryptographic Protocols
In the protocol, three messages are exchanged – it is a three-move protocol:
1. The ﬁrst message is a commitment by Peggy that she knows a square
root of a.
2. The second message is a challenge by Vic. If Vic sends e = 0, then Peggy
has to open the commitment and reveal r. If e = 1, she has to show her
secret in encrypted form (by revealing ry).
3. The third message is Peggy’s response to the challenge of Vic.
Completeness. If Peggy knows y, and both Peggy and Vic follow the pro-
tocol, then the response b = rye is a square root of axe, and Vic will accept.
Soundness. A cheating prover Eve can convince Vic with a probability of
1/2 if she behaves as follows:
1. Eve chooses r ∈Z∗
n and ˜e ∈{0, 1} at random, and sets a := r2x−˜e.
Eve sends a to Vic.
2. Vic chooses e ∈{0, 1} at random. Vic sends e to Eve.
3. Eve sends r to Vic.
If e = ˜e, Vic accepts. The event e = ˜e occurs with a probability of 1/2.
Thus, Eve succeeds in cheating with a probability of 1/2.
On the other hand, 1/2 is the best probability of success that a cheating
prover can reach. Namely, assume that a cheating prover Eve can convince
Vic with a probability > 1/2. Then Eve knows an a for which she can correctly
answer both challenges. This means that Eve can compute b1 and b2, such
that
b2
1 = a and b2
2 = ax.
Hence, she can compute the square root y = b2b1
−1 of x. Recall that x is
a random quadratic residue. Thus Eve has an algorithm A that on input
x ∈QRn outputs a square root y of x. Then Eve can use A to factor n (see
Proposition A.64). This contradicts our assumption that the factorization of
n is intractable.
Security. We have to discuss the security of the scheme from the prover’s
and from the veriﬁer’s points of view.
The veriﬁer accepts the proof of a cheating prover with a probability
of 1/2. The large probability of success of a cheating prover is too high in
practice. It might be decreased by performing t rounds, i.e., by iterating
the basic protocol t times sequentially and independently. In this way, the
probability of cheating is reduced to 2−t. In Section 4.2.4 we will give a
generalized version of the protocol, which decreases the probability of success
of a cheating prover.
Now we look at the basic protocol from an honest prover’s point of view,
and study Peggy’s security requirements. Vic chooses his challenges from
the small set {0, 1}. He has no chance of producing side eﬀects, as in the
identiﬁcation scheme based on public-key cryptography given above. The

4.2 Identiﬁcation Schemes
95
only information Peggy “communicates” to Vic is the fact that she knows a
square root of x. The protocol has the zero-knowledge property studied in
Section 4.2.3.
4.2.3 Zero-Knowledge
In the interactive proof system based on a public-key cryptosystem, which
we discussed above, a dishonest veriﬁer Vic can decrypt Peggy’s cryptograms
by interacting with Peggy. Since Vic is not able to decrypt them without
interaction, he learns something new by interacting with Peggy. He obtains
knowledge from Peggy. This is not desirable, because it might violate Peggy’s
security requirements as our example shows. It is desirable that interactive
proof systems are designed so that no knowledge is transferred from the prover
to the veriﬁer. Such proof systems are called zero-knowledge. Informally, an
interactive proof system is called zero-knowledge if whatever the veriﬁer can
eﬃciently compute after interacting with the prover, can be eﬃciently simu-
lated without interaction. Below we deﬁne the zero-knowledge property more
formally.
We denote the algorithm that the honest prover Peggy executes by P,
the algorithm of an honest veriﬁer by V and the algorithm of a general
(possibly dishonest) veriﬁer by V ∗. The interactive proof system (including
the interaction between P and V ) is denoted by (P, V ). Peggy knows a secret
about some object x (e.g. as in the Fiat-Shamir example in Protocol 4.5, the
root of a square x). This object x is the common input to P and V .
Each algorithm is assumed to have polynomial running time. It may be
partly controlled by random events, i.e., it has access to a source of random
bits and thus can make random choices. Such algorithms are called proba-
bilistic algorithms. We study this notion in detail in Chapter 5.
Let x be the common input of (P, V ). Suppose, the interactive proof takes
n moves. A message is sent in each move. For simplicity, we assume that the
prover starts with the ﬁrst move. We denote by mi the message sent in the
i-th move. The messages m1, m3, . . . are sent from the prover to the veriﬁer
and the messages m2, m4, . . . are sent from the veriﬁer to the prover. The
transcript of the joint computation of P and V ∗on input x is deﬁned by
trP,V ∗(x) := (m1, . . . , mn),
where trP,V ∗(x) is called an accepting transcript if V ∗accepts after the last
move. Note that the transcript trP,V ∗(x) depends on the random bits that
the algorithms P and V ∗choose. Thus, it is not determined by the input x.
Deﬁnition 4.6. An interactive proof system (P, V ) is (perfect) zero-know-
ledge if there is a probabilistic simulator S(V ∗, x), running in expected poly-
nomial time, which for every veriﬁer V ∗(honest or not) outputs on input x an
accepting transcript t of P and V ∗, such that these simulated transcripts are

96
4. Cryptographic Protocols
distributed in the same way as if they were generated by the honest prover
P and V ∗.
Remark. The deﬁnition of zero-knowledge includes all veriﬁers (also the dis-
honest ones). Hence, zero-knowledge is a property of the prover P. It captures
the prover’s security requirements against attempts to gain “knowledge” by
interacting with him.
To understand the deﬁnition, we have to clarify what a simulator is. A
simulator S is an algorithm which, given some veriﬁer V ∗, honest or not, gen-
erates valid accepting transcripts for (P, V ∗), without communicating with
the real prover P. In particular, S does not have any access to computations
that rely on the prover’s secret. Trying to produce an accepting transcript,
S plays the role of P in the protocol and communicates with V ∗. Thus, he
obtains outgoing messages of V ∗which are compliant with the protocol. His
task is to ﬁll into the transcript the messages going out from P. Since P com-
putes these messages by use of her secret and S does not know this secret,
S applies his own strategy to generate the messages. Necessarily, his proba-
bility of obtaining a valid transcript in this way is signiﬁcantly less than 1.
Otherwise, with high probability, S could falsely convince V ∗that he knows
the secret, and the proof system is not sound. Thus, not every attempt of S
to produce an accepting transcript is successful; he fails in many cases. Nev-
ertheless, by repeating his attempts suﬃciently often, the simulator is able
to generate a valid accepting transcript. It is required that the expectation
value of the running time which S needs to get an accepting transcript is
bounded by a polynomial in the binary length |x| of the common input x.2
To be zero-knowledge, the ability to produce accepting transcripts by a
simulation is not suﬃcient. The generation of transcripts, real or simulated,
includes random choices. Thus, we have a probability distribution on the set
of accepting transcripts. The last condition in the deﬁnition means that the
probability distribution of the transcripts that are generated by the simulator
S and V ∗is the same as if they were generated by the honest prover P and V ∗.
Otherwise, the distribution of transcripts might contain information about
the secret and thus reveal some of P’s knowledge.
In the following, we will illustrate the notion of zero-knowledge and the
simulation of a prover by the simpliﬁed version of the Fiat-Shamir identiﬁ-
cation (Protocol 4.5).
Proposition 4.7. The simpliﬁed version of the Fiat-Shamir identiﬁcation
scheme is zero-knowledge.
Proof. The set of accepting transcripts is
T (x) := {(a, e, b) ∈QRn × {0, 1} × Z∗
n | b2 = axe}.
Let V ∗be a general (honest or cheating) veriﬁer. Then, a simulator S with
the desired properties is given by the following algorithm.
2 In other words, S is a Las Vegas algorithm (see Section 5.2).

4.2 Identiﬁcation Schemes
97
Algorithm 4.8.
transcript S(algorithm V ∗, int x)
1
repeat
2
select ˜e ∈{0, 1} and ˜b ∈Z∗
n uniformly at random
3
˜a ←˜b2x−˜e
4
e ←V ∗(˜a)
5
until e = ˜e
6
return (˜a, ˜e,˜b)
The simulator S uses the veriﬁer V ∗as a subroutine to get the challenge
e. S tries to guess e in advance. If S succeeded in guessing e, he can produce
a valid transcript (˜a, ˜e,˜b). S cannot produce e by himself, because V ∗is an
arbitrary veriﬁer. Therefore, V ∗possibly does not generate the challenges e
randomly, as it is speciﬁed in (P, V ), and S must call V ∗to get e. Independent
of the strategy that V ∗uses to output e, the guess ˜e coincides with e with
a probability of 1/2. Namely, if V ∗outputs 0 with a probability of p and 1
with a probability of 1 −p, the probability that e = 0 and ˜e = 0 is p/2, and
the probability that e = 1 and ˜e = 1 is (1 −p)/2. Hence, the probability that
one of both events occurs is 1/2.
The expectation is that S will produce a result after two iterations of the
while loop (see Lemma B.12). An element (˜a, ˜e,˜b) ∈T returned by S cannot
be distinguished from an element (a, e, b) produced by (P, V ∗):
1. a and ˜a are random quadratic residues in QRn.
2. e and ˜e are distributed according to V ∗.
3. b and ˜b are random square roots.
This concludes the proof of the proposition.
2
4.2.4 Fiat-Shamir Identiﬁcation Scheme
As in the simpliﬁed version of the Fiat-Shamir identiﬁcation scheme, let n :=
pq, where p and q are distinct primes. Again, computations are performed in
Zn, and we assume that it is intractable to compute square roots of randomly
chosen elements in QRn, unless the factorization of n is known (see Section
4.2.2). In the simpliﬁed version of the Fiat-Shamir identiﬁcation scheme, the
veriﬁer accepts the proof of a cheating prover with a probability of 1/2. To
reduce this probability of success, now the prover’s secret is a vector y :=
(y1, . . . , yt) of randomly chosen square roots. The modulus n and the vector
x := (y2
1, . . . , y2
t ) are made public. As above, we assume that Peggy chooses
n and y, computes x and publishes the public key (n, x) to all participants.
Peggy’s secret is y.

98
4. Cryptographic Protocols
Protocol 4.9.
Fiat-Shamir Identiﬁcation:
Repeat the following k times:
1. Peggy chooses r ∈Z∗
n at random and sets a := r2. Peggy sends
a to Vic.
2. Vic chooses e := (e1, . . . , et) ∈{0, 1}t at random. Vic sends e to
Peggy.
3. Peggy computes b := r Qt
i=1 yei
i . Peggy sends b to Vic.
4. Vic rejects if b2 ̸≡a Qt
i=1 xei
i , and stops.
Security. The Fiat-Shamir identiﬁcation scheme extends the simpliﬁed
scheme in two aspects. First, a challenge e ∈{0, 1} in the basic scheme
is replaced by a challenge e ∈{0, 1}t. Then the basic scheme is iterated k
times. A cheating prover Eve can convince Vic if she guesses Vic’s challenge
e correctly for each iteration; i.e., if she manages to select the right element
from {0, 1}kt. Her probability of accomplishing this is 2−kt. It can be shown
that for t = O(log2(|n|)) and k = O(|n|l), the interactive proof system is still
zero-knowledge. Observe here that the expected running time of a simulator
that is constructed in a similar way as in the proof of Proposition 4.7 is no
longer polynomial if t or k are too large.
Completeness. If the legitimate prover Peggy and Vic follow the protocol,
then Vic will accept.
Soundness. Suppose a cheating prover Eve can convince (the honest veri-
ﬁer) Vic with a probability > 2−kt, where the probability is taken over all
the challenges e. Then Eve knows a vector A = (a1, . . . , ak) of commitments
aj (one for each iteration j, 1 ≤j ≤k) for which she can correctly answer
two distinct challenges E = (e1, . . . , ek) and F = (f 1, . . . , f k), E ̸= F, of
Vic. There is an iteration j, such that ej ̸= f j, and Eve can answer both
challenges e := ej and f := f j for the commitment a = aj. This means that
Eve can compute b1 and b2, such that
b2
1 = a
tY
i=1
xei
i and b2
2 = a
tY
i=1
xfi
i .
As in Section 4.2.2, this implies that Eve can compute the square root y =
b2b−1
1
of the random square x = Qt
i=1 xfi−ei
i
. This contradicts our assumption
that computing square roots is intractable without knowing the prime factors
p and q of n.
Remark. The number ν of exchanged bits is k(2|n| + t), the average number
µ of multiplications is k(t + 2) and the key size s (equal to the size of the
prover’s secret) is t|n|. Choosing k and t appropriately, diﬀerent values for
the three numbers can be achieved. All choices of k and t, with kt constant,
lead to the same level of security 2−kt. Keeping the product kt constant and

4.2 Identiﬁcation Schemes
99
increasing t, while decreasing k, yields smaller values for ν and µ. However,
note that the scheme is proven to be zero-knowledge only for small values of
t.
4.2.5 Fiat-Shamir Signature Scheme
[FiaSha86] gives a standard method for converting an interactive identiﬁca-
tion scheme into a digital signature scheme. Digital signatures are produced
by the signer without interaction. Thus, the communication between the
prover and the veriﬁer has to be eliminated. The basic idea is to take the
challenge bits, which in the identiﬁcation scheme are generated by the ver-
iﬁer, from the message to be signed. It must be guaranteed that the signer
makes his commitment before he extracts the challenge bits from the mes-
sage. This is achieved by the clever use of a publicly known, collision-resistant
hash function (see Section 3.4):
h : {0, 1}∗−→{0, 1}kt.
As an example, we convert the Fiat-Shamir identiﬁcation scheme (Section
4.2.4) into a signature scheme.
Signing. To sign a message m ∈{0, 1}∗, Peggy proceeds in three steps:
1. Peggy chooses (r1, . . . , rk) ∈Z∗
n
k at random and sets aj := r2
j, 1 ≤j ≤k.
2. She computes h(m||a1|| . . . ||ak) and writes these bits into a matrix, column
by column:
e := (ei,j) 1≤i≤t
1≤j≤k .
3. She computes
bj := rj
tY
i=1
yeij
i
,
1 ≤j ≤k,
and sets b = (b1, . . . , bk). The signature of m is s = (b, e).
Veriﬁcation. To verify the signature s = (b, e) of the signed message (m, s),
we compute
cj := b2
j
tY
i=1
x−eij
i
,
1 ≤j ≤k,
and accept if e = h(m||c1|| . . . ||ck).
Here, the collision resistance of the hash function is needed. The veriﬁer
does not get the original values aj – from step 1 of the protocol – to test
aj = b2
j
Qt
i=1 x−eij
i
, 1 ≤j ≤k.
Remark. The key size is t|n| and the signature size is k(t + |n|). The scheme
is proven to be secure in the random oracle model, i.e., under the assumption
that the hash function h is a truly random function (see Section 10.1 for a
detailed discussion of what the security of a signature scheme means).

100
4. Cryptographic Protocols
4.3 Commitment Schemes
Commitment schemes are of great importance in the construction of crypto-
graphic protocols for practical applications (see Section 4.4.6), as well as for
protocols in theoretical computer science. They are used, for example, to con-
struct zero-knowledge proofs for all languages in NP (see [GolMicWid86]).
This result is extended to the larger class of all languages in IP, which is the
class of languages that have interactive proofs (see [BeGrGwH˚aKiMiRo88]).
Commitment schemes enable a party to commit to a value while keeping
it secret. Later, the committer provides additional information to open the
commitment. It is guaranteed that after committing to a value, this value
cannot be changed. No other value can be revealed in the opening step: if
you have committed to 0, you cannot open 1 instead of 0, and vice versa. For
simplicity (and without loss of generality), we only consider the values 0 and
1 in our discussion.
The following example demonstrates how to use commitment schemes.
Suppose Alice and Bob are getting divorced. They have decided how to split
their common possessions. Only one problem remains: who should get the
car? They want to decide the question by a coin toss. This is diﬃcult, be-
cause Alice and Bob are in diﬀerent places and can only talk to each other by
telephone. They do not trust each other to report the correct outcome of a
coin toss. This example is attributable to M. Blum. He introduced the prob-
lem of tossing a fair coin by telephone and solved it using a bit-commitment
protocol (see [Blum82]).
Protocol 4.10.
Coin tossing by telephone:
1. Alice tosses a coin, commits to the outcome bA (heads = 0, tails
= 1) and sends the commitment to Bob.
2. Bob also tosses a coin and sends the result bB to Alice.
3. Alice opens her commitment by sending the additional informa-
tion to Bob.
Now, both parties can compute the outcome bA ⊕bB of the joint coin toss by
telephone. If at least one of the two parties follows the protocol, i.e., sends
the result of a true coin toss, the outcome is indeed a truly random bit.
In a commitment scheme, there are two participants, the committer (also
called the sender) and the receiver. The commitment scheme deﬁnes two
steps:
1. Commit. The sender sends the bit b he wants to commit to, in encrypted
form, to the receiver.
2. Reveal or open. The sender sends additional information to the receiver,
enabling the receiver to recover b.

4.3 Commitment Schemes
101
There are three requirements:
1. Hiding property. In the commit step, the receiver does not learn anything
about the committed value. He cannot distinguish a commitment to 0
from a commitment to 1.
2. Binding property. The sender cannot change the committed value after
the commit step. This requirement has to be satisﬁed, even if the sender
tries to cheat.
3. Viability. If both the sender and the receiver follow the protocol, the
receiver will always recover the committed value.
4.3.1 A Commitment Scheme Based on Quadratic Residues
The commitment scheme based on quadratic residues enables Alice to commit
to a single bit. Let QRn be the subgroup of squares in Z∗
n (see Deﬁnition
A.48), and let J+1
n
:= {x ∈Z∗
n |
¡ x
n
¢
= 1} be the units modulo n with Jacobi
symbol 1 (see Deﬁnition A.55). Let QNR+1
n
:= J+1
n \QRn be the non-squares
in J+1
n .
Protocol 4.11.
QRCommitment:
1. System setup. Alice chooses distinct large prime numbers p and
q, and v ∈QNR+1
n , n := pq.
2. Commit. To commit to a bit b, Alice chooses r ∈Z∗
n at random,
sets c := r2vb and sends n, c and v to Bob.
3. Reveal. Alice sends p, q, r and b to Bob. Bob can verify that p
and q are primes, n = pq, r ∈Z∗
n, v ̸∈QRn and c = r2vb.
Remarks:
1. There is an eﬃcient deterministic algorithm which computes the Jacobi
symbol
¡ x
n
¢
of x modulo n, without knowing the prime factors p and q of
n (Algorithm A.59). Thus, it is easy to determine whether a given x ∈Z∗
n
is in J+1
n . However, if the factors of n are kept secret, no eﬃcient algorithm
is known that can decide whether a randomly chosen element in J+1
n
is
a square, and it is assumed that no eﬃcient algorithm exists for this
question of quadratic residuosity (a precise deﬁnition of this quadratic
residuosity assumption is given in Deﬁnition 6.11). On the other hand, if
p and q are known it is easy to check whether v ∈J+1
n
is a square. Namely,
v is a square if and only if v mod p and v mod q are squares, and this in
turn is true if and only if the Legendre symbols
³
v
p
´
= v(p−1)/2 mod p
and
³
v
q
´
= v(q−1)/2 mod q are equal to 1 (see Proposition A.52).
2. If Bob could distinguish a commitment to 0 from a commitment to 1, he
could decide whether a randomly chosen element in J+1
n
is a square. This

102
4. Cryptographic Protocols
contradicts the quadratic residuosity assumption stated in the preceding
remark.
3. The value c is a square if and only if vb is a square, i.e., if and only if
b = 0. Since c is either a square or a non-square, Alice cannot change her
commitment after the commit step.
4. Bob needs p and q to check that v is not a square. By not revealing the
primes p and q, Alice could use them for several commitments. Then,
however, she has to prove that v is not a square. She could do this by an
interactive zero-knowledge proof (see Exercise 3).
5. Bob can use Alice’s commitment c and commit to the same bit b as Alice,
without knowing b. He chooses ˜r ∈Z∗
n at random and sets ˜c = c˜r2. Bob
can open his commitment after Alice has opened her commitment. If
commitments are used as subprotocols, it might cause a problem if Bob
blindly commits to the same bit as Alice. We can prevent this by asking
Bob to open his commitment before Alice does.
4.3.2 A Commitment Scheme Based on Discrete Logarithms
The commitment scheme based on discrete logarithms enables Alice to com-
mit to a message m ∈{0, . . . , q −1}.
Protocol 4.12.
LogCommitment:
1. System setup. Bob randomly chooses large prime numbers p and
q such that q divides p −1. Then he randomly chooses g and v
from the subgroup Gq of order q in Z∗
p, g, v ̸= [1].3 Bob sends
p, q, g and v to Alice.
2. Commit. Alice veriﬁes that p and q are primes, that q divides
p −1 and that g and v are elements of order q. To commit to
m ∈{0, . . . , q −1}, she chooses r ∈{0, . . . , q −1} at random, sets
c := grvm and sends c to Bob.
3. Reveal. Alice sends r and m to Bob. Bob veriﬁes that c = grvm.
Remarks:
1. Bob can generate p, q, g and v as in the DSA (see Section 3.5.3).
2. If Alice committed to m and could open her commitment as ˜m, ˜m ̸= m,
then grvm = g˜rv ˜m and logg(v) = ( ˜m −m)−1(r −˜r).4 Thus, Alice could
compute logg(v) of a randomly chosen element v ∈Gq, contradicting
the assumption that discrete logarithms of elements in Gq are infeasible
3 There is a unique subgroup of order q in Z∗
p. It is cyclic and each element x ∈Z∗
p
of order q is a generator (see Lemma A.40).
4 Note that we compute in Gq ⊂Z∗
p. Hence, computations like grvm are done
modulo p, and since the elements in Gq have order q, exponents and logarithms
are computed modulo q (see “Computing modulo a prime” on page 303).

4.3 Commitment Schemes
103
to compute (see the remarks on the security of the DSA at the end of
Section 3.5.3, and Proposition 4.21).
3. g and v are generators of Gq. gr is a uniformly chosen random element
in Gq, perfectly hiding vm and m in grvm, as in the encryption with a
one-time pad (see Section 2.1).
4. Bob has no advantage if he knows logg(v), for example by choosing v = gs
with a random s.
In the commitment scheme based on quadratic residues, the hiding property
depends on the infeasibility of computing the quadratic residues property.
The binding property is unconditional. The commitment scheme based on
discrete logarithms has an unconditional hiding property, whereas the binding
property depends on the diﬃculty of computing discrete logarithms.
If the binding or the hiding property depends on the complexity of a com-
putational problem, we speak of computational hiding or computational bind-
ing. If the binding or the hiding property does not depend on the complexity
of a computational problem, we speak of unconditional hiding or uncondi-
tional binding. The deﬁnitions for the hiding and binding properties, given
above, are somewhat informal. To deﬁne precisely what “cannot distinguish”
and “cannot change” means, probabilistic algorithms have to be used, which
we introduce in Chapter 5.
It would be desirable to have a commitment scheme which features uncon-
ditional hiding and binding. However, as the following considerations show
this is impossible. Suppose a deterministic algorithm
C : {0, 1}n × {0, 1} −→{0, 1}s
deﬁnes a scheme with both unconditionally hiding and binding. Then when
Alice sends a commitment c = C(r, b) to Bob, there must exist an ˜r such that
c = C(˜r, 1 −b). Otherwise, Bob could compute (r, b) if he has unrestricted
computational power, violating the unconditional hiding property. However,
if Alice also has unrestricted computing power, then she can also ﬁnd (˜r, 1−b)
and open the commitment as 1 −b, thus violating the unconditional binding
property.
4.3.3 Homomorphic Commitments
Let Com(r, m) := grvm denote the commitment to m in the commitment
scheme based on discrete logarithms. Let r1, r2, m1, m2 ∈{0, . . . , q−1}. Then
Com(r1, m1) · Com(r2, m2) = Com(r1 + r2, m1 + m2).
Commitment schemes satisfying such a property are called homomorphic
commitment schemes.
Homomorphic commitment schemes have an interesting application in
distributed computation: a sum of numbers can be computed without re-
vealing the single numbers. This feature can be used in electronic voting

104
4. Cryptographic Protocols
schemes (see Section 4.4). We give an example using the Com function just
introduced. Assume there are n voters V1, . . . , Vn. For simplicity, we assume
that only “yes-no” votes are possible. A trusted center T is needed to com-
pute the outcome of the election. The center T is assumed to be honest. If
T was dishonest, it could determine each voter’s vote. Let ET and DT be
ElGamal encryption and decryption functions for the trusted center T. To
vote on a subject, each voter Vi chooses mi ∈{0, 1} as his vote, a random
ri ∈{0, . . . , q −1} and computes ci := Com(ri, mi). Then he broadcasts ci
to the public and sends ET (gri) to the trusted center T. T computes
DT (
n
Y
i=1
ET (gri)) =
n
Y
i=1
gri = gr,
where r = Pn
i=1 ri, and broadcasts gr.
Now, everyone can compute the result s of the vote from the publicly
known ci, i = 1, . . . , n, and gr:
vs = g−r
n
Y
i=1
ci ,
with s := Pn
i=1 mi. s can be derived from vs by computing v, v2, . . . and
comparing with vs in each step, because the number of voters is not too
large. If the trusted center is honest, the factor gri – which hides Vi’s vote –
is never computed. Although an unconditional hiding commitment is used,
the hiding property is only computational because gri is encrypted with a
cryptosystem that provides at most computational security.
4.4 Electronic Elections
In an electronic voting scheme there are two distinct types of participants: the
voters casting the votes and the voting authority (for short, the authority)
that collects the votes and computes the ﬁnal tally.
Usually the following properties are required: (1) universal veriﬁability
ensures that the correctness of the election, especially the correct computa-
tion of the tally, can be checked by everyone; (2) privacy ensures that the
secrecy of an individual vote is maintained; and (3) robustness ensures that
the scheme works even in the presence of a coalition of parties with faulty
behavior. Naturally, only authorized voters should be allowed to cast their
votes.
There seems to be a conﬂict between the last requirement and privacy.
The scheme we describe resolves this conﬂict by establishing a group of au-
thorities and a secret sharing scheme. It guarantees privacy, even if some of
the authorities collaborate.

4.4 Electronic Elections
105
4.4.1 Secret Sharing
The idea of secret sharing is to start with a secret s and divide it into n pieces
called shares. These shares are distributed among n users in a secure way.
A coalition of some of the users is able to reconstruct the secret. The secret
could, for example, be the password to open a safe shared by ﬁve people,
with any three of them being able to reconstruct the password and open the
safe.
In a (t, n)-threshold scheme (t ≤n), a trusted center computes the shares
sj of a secret s and distributes them among n users. Each t of the n users
are able to recover s from their shares. It is impossible to recover the secret
from t −1 or fewer shares.
Shamir’s Threshold Scheme. Shamir’s threshold scheme is based on the
following properties of polynomials over a ﬁnite ﬁeld k. For simplicity, we
take k = Zp, where p is a prime.
Proposition 4.13. Let f(X) = Pt−1
i=0 aiXi ∈Zp[X] be a polynomial of de-
gree t−1, and let P := {(xi, f(xi)) | xi ∈Zp, i = 1, . . . , t, xi ̸= xj, i ̸= j}. For
Q ⊆P, let PQ := {g ∈Zp[X] | deg(g) = t −1, g(x) = y for all (x, y) ∈Q}.
1. PP = {f(X)}, i.e., f is the only polynomial of degree t −1, whose graph
contains all t points in P.
2. If Q ⊂P is a proper subset and x ̸= 0 for all (x, y) ∈Q, then each
a ∈Zp appears with the same frequency as the constant coeﬃcient of a
polynomial in PQ.
Proof. To ﬁnd all polynomials g(X) = Pt−1
i=0 biXi ∈Zp[X] of degree t −1
through m given points (xi, yi), 1 ≤i ≤m, you have to solve the following
linear equations:



1 x1 . . . xt−1
1
...
1 xm . . . xt−1
m






b0
...
bt−1


=



y1
...
ym



If m = t, then the above matrix (called A) is a Vandermonde matrix and its
determinant
det A =
Y
1≤i<j≤t
(xj −xi) ̸= 0,
if xi ̸= xj for i ̸= j. Hence,the system of linear equations has exactly one
solution and part 1 of Proposition 4.13 follows.
Now let Q ⊂P be a proper subset. Without loss of generality, Q consists
of the points (x1, y1), . . . , (xm, ym), 1 ≤m ≤t −1. We consider the following
system of linear equations:

106
4. Cryptographic Protocols





1 0 . . .
0
1 x1 . . . xt−1
1
...
1 xm . . . xt−1
m










b0
b1
...
bt−1




=





a
y1
...
ym





(4.1)
The matrix of the system consists of rows of a Vandermonde matrix (note
all xi ̸= 0 by assumption). Thus, the rows are linearly independent and
the system (4.1) has solutions for all a ∈Zp. The matrix has rank m + 1
independent of a. Hence, the number of solutions is independent from a, and
we see that each a ∈Zp appears as the constant coeﬃcient of a polynomial
in PQ with the same frequency.
2
Corollary 4.14. Let f(X) = Pt−1
i=0 aiXi ∈Zp[X] be a polynomial of degree
t −1, and let P = {(xi, f(xi)) | i = 1, . . . , t, xi ̸= xj, i ̸= j}. Then
f(X) =
t
X
i=1
f(xi)
Y
1≤j≤t,j̸=i
(X −xj)(xi −xj)−1.
(4.2)
This formula is called the Lagrange interpolation formula.
Proof. The right-hand side of (4.2) is a polynomial g of degree t −1. If we
substitute X by xi in g, we get g(xi) = f(xi). Since the polynomial f(X) is
uniquely deﬁned by P, the equality holds.
2
Now we describe Shamir’s (t, n)-threshold scheme. A trusted center T
distributes n shares of a secret s ∈Z among n users P1, . . . , Pn. To set up
the scheme, the trusted center T proceeds as follows:
1. T chooses a prime p > max(s, n) and sets a0 := s.
2. T selects a1, . . . , at−1 ∈{0, . . . , p−1} independently and at random, and
gets the polynomial f(X) = Pt−1
i=0 aiXi.
3. T computes si := f(i), i = 1, . . . , n (we use the values i = 1, . . . , n for
simplicity; any n pairwise distinct values xi ∈{1, . . . , p −1} could also
be used) and transfers (i, si) to the user Pi in a secure way.
Any group of t or more users can compute the secret. Let J ⊂{1, . . . , n},
|J| = t. From Corollary 4.14 we get
s = a0 = f(0) =
X
i∈J
f(i)
Y
j∈J,j̸=i
j(j −i)−1 =
X
i∈J
si
Y
j∈J,j̸=i
j(j −i)−1.
If only t−1 or fewer shares are available, then each a ∈Zp is equally likely to
be the secret (by Proposition 4.13). Thus, knowing only t −1 or fewer shares
provides no advantage over knowing none of them.

4.4 Electronic Elections
107
Remarks:
1. Suppose that each a ∈Zp is equally likely as the secret for someone
knowing only t−1 or fewer shares, as in Shamir’s scheme. Then the (t, n)-
threshold scheme is called perfect: the scheme provides perfect secrecy in
the information-theoretic sense (see Section 9.1). The security does not
rely on the assumed hardness of a computational problem.
2. Shamir’s threshold scheme is easily extendable for new users. New shares
may be computed and distributed without aﬀecting existing shares.
3. It is possible to implement varying levels of control. One user can hold
one or more shares.
4.4.2 A Multi-Authority Election Scheme
For simplicity, we only consider election schemes for yes-no votes. The vot-
ers want to get a majority decision on some subject. In the scheme that we
describe, each voter selects his choice (yes or no), encrypts it with a homo-
morphic encryption algorithm and signs the cryptogram. The signature shows
that the vote is from an authorized voter. The votes are collected in a single
place, the bulletin board. After all voters have posted their votes, an author-
ity can compute the tally without decrypting the single votes. This feature
depends on the fact that the encryption algorithm used is a homomorphism.
It guarantees the secrecy of the votes. But if the authority was dishonest,
she could decrypt the single votes. To reduce this risk, the authority con-
sists of several parties and the decryption key is shared among these parties,
by use of a Shamir (t, n)-threshold scheme. Then at least t of the n parties
must be dishonest to reveal a vote. First, we assume in our discussion that a
trusted center T sets up the scheme. However, the trusted center is not really
needed. In Section 4.4.6 we show that it is possible to set up the scheme by
a communication protocol which is executed by the parties constituting the
authority.
The election scheme that we describe was introduced in [CraGenSch97].
The time and communication complexity of the scheme is remarkably low.
A voter simply posts a single encrypted message, together with a compact
proof that it contains a valid vote.
The Communication Model. The members of the voting scheme commu-
nicate through a bulletin board. The bulletin board is best viewed as publicly
accessible memory. Each member has a designated section of the memory to
post messages. No member can erase any information from the bulletin board.
The complete bulletin board can be read by all members (including passive
observers). We assume that a public-key infrastructure for digital signatures
is used to guarantee the origin of posted messages.
Setting up the Scheme. To set up the scheme, we assume for now that
there is a trusted center T. The trusted center T chooses primes p and q,

108
4. Cryptographic Protocols
such that q is a large divisor of p −1, and an element g ∈Z∗
p of order q, as
in the key generation procedure of the DSA (see Section 3.5.3). g generates
the subgroup Gq of order q of Z∗
p.5
Further, we assume that T chooses a secret key s ∈{0, . . . , q −1} at
random and publishes the public key h := gs to be used for ElGamal en-
cryption with respect to the base g. A message m ∈Gq is encrypted as
(c1, c2) = (gα, hαm), where α is a randomly chosen element in {0, . . . , q −1}.
Using the secret key s the plaintext m can be recovered as m = c2c−s
1
(see
Section 3.5.1). The encryption is homomorphic: if (c1, c2) and (c′
1, c′
2) are
encryptions of m and m′, then
(c1, c2) · (c′
1, c′
2) = (c1c′
1, c2c′
2) = (gαgα′, hαmhα′m′) = (gα+α′, hα+α′mm′)
is an encryption of mm′.
Let A1, . . . , An be the authorities and V1, . . . , Vm be the voters in the elec-
tion scheme. The trusted center T chooses a Shamir (t, n)-threshold scheme.
The secret encryption key s is shared among the n authorities. Aj keeps her
share (j, sj) secret. The values hj := gsj are published on the bulletin board
by the trusted center.
Decryption. Everyone can decrypt a cryptogram c := (c1, c2) := (gα, hαm)
with some help of the authorities, but without reconstructing the secret key
s. Namely, the following steps are executed:
1. Each authority Aj posts wj := c1sj to the bulletin board. Here, we assume
that the authority Aj is honest and follows the protocol. In Section 4.4.3
we will see how to check that she really does (by a proof of knowledge).
2. Let J be the index set of a subset of t honest authorities. Then, everyone
can recover m = c2c−s
1
as soon as all Aj, j ∈J, have ﬁnished step 1:
c1
s = c1
P
j∈J sjλj,J =
Y
j∈J
(c1
sj)λj,J =
Y
j∈J
wj
λj,J,
where
λj,J =
Y
l∈J\{j}
l(l −j)−1 .
Vote Casting. Each voter Vi selects his vote vi ∈{−1, 1}, encodes vi as gvi
and encrypts gvi by the ElGamal encryption:
ci := (ci,1, ci,2) := (gαi, hαigvi).
He then signs it to guarantee the origin of the message and posts it to the
bulletin board. Here we assume that Vi follows the protocol and correctly
forms ci. He has to perform a proof of knowledge which shows that he really
does (see Section 4.4.3); otherwise his vote is invalid.
5 There is a unique subgroup of order q of Z∗
p. It is cyclic and each element x ∈Z∗
p
of order q is a generator (see Lemma A.40 and “Computing modulo a prime” on
page 303).

4.4 Electronic Elections
109
Tally Computing. Assume that m votes were cast:
1. Everyone can compute
c = (c1, c2) =
Ã m
Y
i=1
ci,1,
m
Y
i=1
ci,2
!
.
Note that c = (c1, c2) is the encryption of gd, where d is the diﬀerence
between the number of yes votes and no votes, since the encryption is
homomorphic.
2. The decryption protocol from above is executed to get gd. After suﬃ-
ciently many authorities Aj have posted wj = csj
1 to the bulletin board,
everyone can compute gd.
3. Now d can be found by computing the sequence g−m, g−m+1, . . ., and
comparing with gd in each step.
Remarks:
1. Everyone can check whether a voter or an authority was honest (see
Section 4.4.3), and discard invalid votes. If he ﬁnds a subset of t honest
authorities, he can compute the tally. This implies universal veriﬁability.
2. No coalition of t −1 or fewer authorities can recover the secret key. This
guarantees the robustness of the scheme.
3. Privacy depends on the security of the underlying ElGamal encryption
scheme and, hence, on the assumed diﬃculty of the Diﬃe-Hellman prob-
lem. The scheme provides only computational privacy. A similar scheme
is introduced in [CraFraSchYun96] which even provides perfect privacy
(in the information-theoretic sense). This is achieved by using a com-
mitment scheme with information-theoretic secure hiding to encrypt the
votes.
4. The following remarks concern the communication complexity of the
scheme:
a. Each voter only has to send one message together with a compact
proof that the message contains a valid vote (see below). His activities
are independent of the number n of authorities.
b. Each authority has to read m messages from the bulletin board,
verify m interactive proofs of knowledge and post one message to the
bulletin board.
c. To compute the tally, you have to read t messages from the bulletin
board and to verify t interactive proofs of knowledge.
5. It is possible to prepare an election beforehand. The voter Vi chooses
vi ∈{−1, 1} at random. The voting protocol is executed with the random
vi values. Later, the voter decides the alternative to choose. He selects
˜vi ∈{−1, 1}, such that ˜vivi is his vote, and posts ˜vi to the bulletin board.
The tally is computed with ˜ci = (c˜vi
i,1, c˜vi
i,2), i = 1, . . . , m.

110
4. Cryptographic Protocols
4.4.3 Proofs of Knowledge
Authority’s Proof. In the decryption protocol above, each authority Aj
has to prove that she really posts wj = csj
1 , where sj is her share of the secret
key s. Recall that hj = gsj is published on the bulletin board. The authority
has to prove that wj and hj have the same logarithm with respect to the
bases c1 and g and that she knows this logarithm. We simplify the notation
and describe an interactive proof of knowledge of the common logarithm x
of y1 = gx
1 and y2 = gx
2, where x is a random element from {0, . . . , q −1}. As
usual, we call the prover Peggy and the veriﬁer Vic.
Of course, in our voting scheme it is desirable for practical reasons that
the authority proves, in a non-interactive way, to be honest. However, it is
easy to convert the interactive proof into a non-interactive one (see Section
4.4.4). Thus, we ﬁrst give the interactive version of the proof.
Protocol 4.15.
ProofLogEq(g1, y1, g2, y2):
1. Peggy chooses r ∈{0, . . . , q −1} at random and sets a :=
(a1, a2) = (gr
1, gr
2). Peggy sends a to Vic.
2. Vic chooses c ∈{0, . . . , q −1} uniformly at random and sends c
to Peggy.
3. Peggy computes b := r −cx and sends b to Vic.
4. Vic accepts if and only if a1 = gb
1yc
1 and a2 = gb
2yc
2.
The protocol is a three-move protocol. It is very similar to the protocol used
in the simpliﬁed Fiat-Shamir identiﬁcation scheme (see Section 4.2.2):
1. The ﬁrst message is a commitment by Peggy. She commits that two
numbers have the same logarithm with respect to the diﬀerent bases g1
and g2.
2. The second message c is a challenge by Vic.
3. The third message is Peggy’s response. If c = 0, Peggy has to open
the commitment (reveal r). If c ̸= 0, Peggy has to show her secret in
encrypted form (reveal r −cx).
Completeness. If Peggy knows a common logarithm for y1 and y2, and
both Peggy and Vic follow the protocol, then a1 = gb
1yc
1 and a2 = gb
2yc
2, and
Vic will accept.
Soundness. A cheating prover Eve can convince Vic with a probability of
1/q in the following way:
1. Eve chooses r, ˜c ∈{0, . . . , q −1} at random, sets a := (gr
1y˜c
1, gr
2y˜c
2) and
sends a to Vic.
2. Vic chooses c ∈{0, . . . , q −1} at random and sends c to Eve.
3. Eve sends r to Vic.

4.4 Electronic Elections
111
Vic accepts if and only if c = ˜c. The event c = ˜c occurs with a probability of
1/q. Thus, Eve succeeds in cheating with a probability of 1/q.
If Eve can convince Vic with a probability greater than 1/q (the probabil-
ity is taken over the challenges c), she has to answer at least two challenges
correctly, for a given commitment a.
Suppose Eve knows an a = (a1, a2) for which she can answer two distinct
challenges c and ˜c. This means that Eve can compute b and ˜b, such that
a1 = gb
1yc
1, a2 = gb
2yc
2,
a1 = g˜b
1y˜c
1, a2 = g˜b
2y˜c
2.
Then she can compute
g
˜b−b
1
= yc−˜c
1
and g
˜b−b
2
= yc−˜c
2
and gets
(˜b −b)(c −˜c)−1 = logg1(y1) and (˜b −b)(c −˜c)−1 = logg2(y2).
Thus, she can compute the secret x. This contradicts the assumption that it
is infeasible to compute x from gx (for randomly chosen p, g and x). We see
that the probability of success of a cheating prover is bounded by 1/q.
Honest Veriﬁer Zero-Knowledge. The above protocol is not known to be
zero-knowledge. However, it is zero-knowledge if the veriﬁer is an honest one.
An interactive proof system (P, V ) is called honest-veriﬁer zero-knowledge
if Deﬁnition 4.6 holds for the honest veriﬁer V , but not necessarily for an
arbitrary Veriﬁer V ∗; i.e., there is a simulator S that produces correctly
distributed accepting transcripts for executions of the protocol with (P, V ).
To simulate an interaction with the honest veriﬁer is quite simple. The
key point is that the honest veriﬁer V chooses the challenge c ∈{0, . . . , q−1}
independently from (a1, a2), uniformly and at random, and this can also be
done by S.
Algorithm 4.16.
int S(int g1, g2, y1, y2)
1
select ˜b ∈{0, . . . , q −1} uniformly at random
2
select ˜c ∈{0, . . . , q −1} uniformly at random (this is V ’s task)
3
˜a1 ←g˜b
1y˜c
1, ˜a2 ←g˜b
2y˜c
2
4
return (˜a1, ˜a2, ˜c,˜b)
The transcript (˜a1, ˜a2, ˜c,˜b) returned by S is an accepting transcript and not
distinguishable from a transcript (a1, a2, c, b) produced by (P, V ):
1. a1, a2 and ˜a1, ˜a2 are randomly chosen elements in Zq.
2. c and ˜c are randomly chosen elements in {0, . . . , q −1}.
3. b and ˜b are randomly chosen elements in {0, . . . , q −1}.

112
4. Cryptographic Protocols
Voter’s Proof. In the vote-casting protocol, each voter has to prove that he
really encrypted a vote gv
∈
{g, g−1}; i.e., he has to prove that
c = (c1, c2) = (gα, hαm) and m ∈{g, g−1}. For this purpose, he performs
a proof of knowledge. He proves that he knows α for either c1 = gα and
c2g−1 = hα, or for c1 = gα and c2g = hα. Each of the two alternatives could
be proven as in the authority’s proof. Here, however, the prover’s task is more
diﬃcult. The proof must not reveal which of the two alternatives is proven.
An interactive three-move proof that convinces the veriﬁer without revealing
anything about the prover’s choice is the subject of Exercise 9.
4.4.4 Non-Interactive Proofs of Knowledge
It is easy to convert an interactive three-move proof into a non-interactive
one using the standard method of Fiat-Shamir, which we demonstrated in
Section 4.2.5. Let h : {0, 1}∗−→Zq be a collision-resistant hash function. We
get a non-interactive proof
(c, b) = ProofLogEqh(g1, y1, g2, y2),
for y1 = gx
1 and y2 = gx
2 in the following way. The prover Peggy chooses
r ∈{0, . . . , q −1} at random and sets a := (a1, a2) = (gr
1, gr
2). Then she
computes the challenge c := h(g1||y1||g2||y2||a1||a2) and sets b := r −cx. The
veriﬁcation condition is
c = h(g1||y1||g2||y2||gb
1yc
1||gb
2yc
2).
The veriﬁer needs not know a to compute the veriﬁcation condition. If we
trust the collision resistance of h, we can conclude that u = v from h(u) =
h(v).
If we convert our proofs of knowledge into non-interactive proofs, honest-
veriﬁer zero-knowledge is suﬃcient, because here, the veriﬁer is always honest.
In our election protocol, each authority and each voter completes his message
with a non-interactive proof which convinces everyone that he followed the
protocol.
4.4.5 Extension to Multi-Way Elections
We describe how to extend the scheme if a choice between several, say l > 2,
options should be possible.
To encode the votes v1, . . . , vl, we independently choose l generators gj,
j = 1, . . . , l, of Gq, and encode vj by gj. Voter Vi encrypts his vote vj as
ci = (ci,1, ci,2) = (gα, hαgj).
Each voter shows – by an interactive proof of knowledge – that he knows α
with

4.4 Electronic Elections
113
ci,1 = gα and g−1
j ci,2 = hα,
for exactly one j. We refer the interested reader to [CraGenSch97] for some
hints about the proof technique used.
The problem of computing the ﬁnal tally turns out to be more compli-
cated. The result of the tally computation is
c2c−s
1
= gd1
1 gd2
2 . . . gdl
l ,
where dj is the number of votes for vj. The exponents dj are uniquely
determined by c2c−s
1
in the following sense: computing a diﬀerent solution
( ˜d1, . . . , ˜dl) would contradict the discrete logarithm assumption, because the
generators were chosen independently (see Proposition 4.21 in Section 4.5.3).
Again as above, d = (d1, . . . , dl) can be found for small values of l and di by
searching.
4.4.6 Eliminating the Trusted Center
The trusted center sets up an ElGamal cryptosystem, generates a secret key,
publishes the corresponding public key and shares the secret key among n
users using a (t, n)-threshold scheme. To eliminate the trusted center, all
these activities must be performed by the group of users (in our case, the
authorities). For the communication between the participants, we assume
below that a bulletin board, mutually secret communication channels and a
commitment scheme exist.
Setting Up an ElGamal Cryptosystem. We need large primes p, q, such
that q is a divisor of p −1, and a generator g of the subgroup Gq of or-
der q of Z∗
p. They are generated jointly by the group of users (in our case
the group of authorities). This can be achieved if each party runs the same
generation algorithm (see Section 3.5.1). The random input to the generation
algorithm, must be generated jointly. To do this, the users P1, . . . , Pn execute
the following protocol:
1. Each user Pi chooses a string ri of random bits of suﬃcient length, com-
putes a commitment ci = C(ri) and posts the result to the bulletin
board.
2. After all users have posted their commitments, each user opens his com-
mitment.
3. They take r = ⊕n
i=1ri as the random input.
Publish the Public Key. To generate and distribute the public key the
users P1, . . . , Pn execute the following protocol:
1. Each user Pi chooses xi ∈{0, . . . , q −1} at random, computes hi := gxi
and a commitment ci := C(hi) for hi. Then he posts ci to the bulletin
board.

114
4. Cryptographic Protocols
2. After all users have posted their commitments, each user opens his com-
mitment.
3. Everyone can compute the public key h := Qn
i=1 hi.
Share the Secret Key. The corresponding secret key
x :=
n
X
i=1
xi
must be shared. The basic idea is that each user constructs a Shamir (t, n)-
threshold scheme to share his part xi of the secret key. These schemes are
combined to get a scheme for sharing x.
Let fi(X) ∈Zp[X] be the polynomial of degree t −1 used for sharing xi,
and let
f(X) =
n
X
i=1
fi(X).
Then f(0) = x and f can be used for a (t, n)-threshold scheme, provided
deg(f) = t −1. The shares f(j) can be computed from the shares fi(j):
f(j) =
n
X
i=1
fi(j).
The group of users executes the following protocol to set up the threshold
scheme:
1. The group chooses a prime p > max(x, n).
2. Each user Pi randomly chooses fi,j ∈{0, . . . , p −1}, j = 1, . . . , t −1,
and sets fi,0 = xi and fi(X) = Pt−1
j=0 fi,jXj. He posts Fi,j := gfi,j,
j = 1, . . . , t −1, to the bulletin board. Note that Fi,0 = hi is Pi’s piece
of the public key and hence is already known.
3. After all users have posted their encrypted coeﬃcients, each user tests
whether Pn
i=1 fi(X) has degree t −1, by checking
n
Y
i=1
Fi,t−1 ̸= [1].
In the rare case that the test fails, they return to step 2. If f(X) passes
the test, the degree of f(X) is t −1.
4. Each user Pi distributes the shares si,l = fi(l), l = 1, . . . , n, of his piece xi
of the secret key to the other users over secure communication channels.
5. Each user Pi veriﬁes for l = 1, . . . , n that the share sl,i received from
Pl is consistent with the previously posted committed coeﬃcients of Pl’s
polynomial:
gsl,i =
t−1
Y
j=0
(Fl,j)ij.
If the test fails, he stops the protocol by broadcasting the message

4.5 Digital Cash
115
“failure,(l, i), sl,i”.
6. Finally, Pi computes his share si of x:
si =
n
X
l=1
sl,i,
signs the public key h and posts his signature to the bulletin board.
After all members have signed h, the group will use h as their public key. If
all participants followed the protocol, the corresponding secret key is shared
among the group. The protocol given here is described in [Pedersen91]. It only
works if all parties are honest. [GenJarKraRab99] introduces an improved
protocol which works if a majority of the participants is honest.
4.5 Digital Cash
The growth of electronic commerce in the Internet requires digital cash. To-
day, credit cards are used to pay on the Internet. Transactions are online; i.e.,
all participants – the customer, the shop and the bank – are involved at the
same time (for simplicity, we assume only one bank). This requires that the
bank is available even during peak traﬃc time, which makes the scheme very
costly. Exposing the credit card number to the vendor provides him with the
ability to impersonate the customer in future purchases. The bank can easily
observe who pays which amount to whom and when, so the customer cannot
pay anonymously, as she can with ordinary money.
A payment with ordinary money requires three diﬀerent steps. First, the
customer fetches some money from the bank, and his account is debited.
Then he can pay anonymously in a shop. Later, the vendor can bring the
money to the bank, and his account is credited.
Ordinary money has an acceptable level of security and functions well for
its intended task. Its security is based on a complicated and secret manufac-
turing process. However, it is not as secure in the same mathematical sense
as some of the proposed digital cash schemes.
Digital cash schemes are modeled on ordinary money. They involve three
interacting parties: the bank, the customer and the shop. The customer and
the shop have accounts with the bank. A digital cash system transfers money
in a secure way from the customer’s account to the shop’s account. In the
following, the money is called an electronic coin, or coin for short.
As with ordinary money, paying with digital cash requires three steps:
1. The customer fetches the coin from the bank: customer and bank execute
the withdrawal protocol.
2. The customer pays the vendor: customer and vendor execute the payment
protocol.

116
4. Cryptographic Protocols
3. The vendor deposits the coin on his account: vendor and bank execute
the deposit protocol.
In an oﬄine system, each step occurs in a separate transaction, whereas in
an online system, steps 2 and 3 take place in a single transaction involving
all three parties.
The bank, the shop and the customer have diﬀerent security requirements:
1. The bank is assured that only a previously withdrawn coin can be de-
posited. It must be impossible to deposit a coin twice without being
detected.
2. The customer is assured that the shop will accept previously withdrawn
coins and that he can pay anonymously.
3. In an oﬄine system, the vendor is assured that the bank will accept a
payment he has received from the customer.
It is not easy to enable anonymous payments, for which it must be im-
possible for the bank to trace a coin, i.e., to link a coin from a withdrawal
with the corresponding coin in the deposit step. This requirement protects
an honest customer’s privacy, but it also enables the misuse by criminals.
Thus, to make anonymous payment systems practical, they must imple-
ment a mechanism for tracing a coin. It must be possible to revoke the cus-
tomer’s anonymity under certain well-deﬁned conditions. Such systems are
sometimes called fair payment systems.
In the scheme we describe, anonymity may be revoked by a trusted third
party called the trusted center. During the withdrawal protocol, the customer
has to provide data which enables the trusted center to trace the coin. The
trusted center is only needed if someone asks to revoke the anonymity of
a customer. The trusted center is not involved if an account is opened or
a coin is withdrawn, paid or deposited. Using the secret sharing techniques
from Section 4.4.1, it is easy to distribute the ability to revoke a customer’s
anonymity among a group of trusted parties.
A customer’s anonymity is achieved by using blind signatures (see Section
4.5.1). The customer has to construct a message of a special form, and then
he hides the content. The bank signs the hidden message, without seeing its
content. Older protocols use the “cut and choose method” to ensure that the
customer formed the message correctly: the customer constructs, for example,
1000 messages and sends them to the bank. The bank selects one of the 1000
messages to sign it. The customer has to open the remaining 999 messages.
If all these messages are formed correctly, then, with high probability, the
message selected and signed by the bank is also correct. In the system that
we describe, the customer proves that he constructed the message m correctly.
Therefore, only one message is constructed and sent to the bank. This makes
digital cash eﬃcient.
Ordinary money is transferable: the shop does not have to return a coin
to the bank after receiving it. He can transfer the money to a third person.

4.5 Digital Cash
117
The digital cash system we introduce does not have this feature, but there
are electronic cash systems which do implement transferable electronic coins
(e.g. [OkaOht92]; [ChaPed92]).
4.5.1 Blindly Issued Proofs
The payment system which we describe provides customer anonymity by
using a blind digital signature.6 Such a signature enables the signer (the
bank) to sign a message without seeing its content (the content is hidden).
Later, when the message and the signature are revealed, the signer is not able
to link the signature with the corresponding signing transaction. The bank’s
signature can be veriﬁed by everyone, just like an ordinary signature.
The Basic Signature Scheme. Let p and q be large primes, such that q
divides p −1. Let Gq be the subgroup of order q in Z∗
p, g a generator of
Gq7 and h : {0, 1}∗−→Zq be a collision-resistant hash function. The signer’s
secret key is a randomly chosen x ∈{0, . . . , q −1}, and the public key is
(p, q, g, y), where y = gx.
The basic protocol in this section is Schnorr’s identiﬁcation protocol. It
is an interactive proof of knowledge. The prover Peggy proves to the veriﬁer
Vic that she knows x, the discrete logarithm of y.
Protocol 4.17.
ProofLog(g,y):
1. Peggy randomly chooses r ∈{0, . . . , q −1}, computes a := gr
and sends it to Vic.
2. Vic chooses c ∈{0, . . . , q −1} at random and sends it to Peggy.
3. Peggy computes b := r −cx and sends it to Vic.
4. Vic accepts the proof if a = gbyc; otherwise he rejects it.
To achieve a signature scheme, the protocol is converted into a non-
interactive proof of knowledge ProofLogh using the same method as shown
in Sections 4.4.4 and 4.2.5: the challenge c is computed by means of the
collision-resistant hash function h.
A signature σ(m) of a message m consists of a non-interactive proof that
the signer (prover) knows the secret key x. The proof depends on the message
m, because when computing the challenge c, the message m serves as an
additional input to h:
σ(m) = (c, b) = ProofLogh(m, g, y),
6 Blind signatures were introduced by D. Chaum in [Chaum82] to enable untrace-
able electronic cash.
7 As stated before, there is a unique subgroup of order q of Z∗
p. It is cyclic and
each element x ∈Z∗
p of order q is a generator (see Lemma A.40 and “Computing
modulo a prime” on page 303).

118
4. Cryptographic Protocols
where c := h(m||a), a := gr, r ∈{0, . . . , q −1} chosen at random, and b :=
r −cx. The signature is veriﬁed by checking the condition
c = h(m||gbyc).
Here, the collision resistance of the hash function is needed. The veriﬁer does
not get a to test a = gbyc. If m is the empty string, we omit m and write
(c, b) = ProofLogh(g, y).
In this way, we attain a non-interactive proof that the prover knows x =
logg(y).
Remarks:
1. As in the commitment scheme in Section 4.3.2 and the election protocol in
Section 4.4, the security relies on the assumption that discrete logarithms
of elements in Gq are infeasible to compute (also see the remarks on the
security of the DSA at the end of Section 3.5.3).
2. If the signer uses the same r (i.e., he uses the same commitment a = gr)
to sign two diﬀerent messages m1 and m2, then the secret key x can
easily be computed:8
Let σ(mi) := (ci, bi), i = 1, 2. We have gr = gb1yc1 = gb2yc2 and derive
x = (b1 −b2)(c2 −c1)−1. Note that c1 ̸= c2 for m1 ̸= m2, since h is
collision resistant.
The Blind Signature Scheme. The basic signature scheme can be trans-
formed into a blind signature scheme. To understand the ideas, we ﬁrst recall
our application scenario. The customer (Vic) would like to submit a coin
to the shop (Alice). The coin is signed by the bank (Peggy). Alice must be
able to verify Peggy’s signature. Later, when Alice brings the coin to the
bank, Peggy should not be able to recognize that she signed the coin for Vic.
Therefore, Peggy has to sign the coin blindly. Vic obtains the blind signature
for a message m from Peggy by executing the interactive protocol ProofLog
with Peggy. In step 2 of the protocol, he deviates a little from the original
protocol: as in the non-interactive version ProofLogh, the challenge is not
chosen randomly, but computed by means of the hash function h (with m as
part of the input). We denote the transcript of this interaction by (a, c, b);
i.e., a is Peggy’s commitment in step 1, c is Vic’s challenge in step 2 and b is
sent from Peggy to Vic in step 3. (c, b) is a valid signature with a = gbyc.
Now Peggy may store Vic’s identity and the transcript τ = (a, c, b), but
later she should not be able to recognize the signature σ(m) of m. Therefore,
Vic must transform Peggy’s signature (c, b) into another valid signature (c, b)
of Peggy that Peggy is not able to link with the original transcript (a, c, b).
8 The elements in Gq have order q, hence all exponents and logarithms are com-
puted modulo q. See “Computing modulo a prime” on page 303.

4.5 Digital Cash
119
The idea is that Vic transforms the given transcript τ into another accepting
transcript τ = (a, c, b) of ProofLog by the following transformation:
β(u,v,w) : Gq × Z2
q −→Gq × Z2
q, (a, c, b) 7−→(a, c, b), where
a := augvyw,
c := uc + w,
b := ub + v.
We have
a = augvyw = (gbyc)ugvyw = gub+vyuc+w = gbyc.
Thus, τ is indeed an accepting transcript of ProofLog. If Vic chooses u, v, w ∈
{0, . . . , q} at random, then, τ and τ are independent, and Peggy cannot get
any information about τ by observing the transcript τ. Namely, given τ =
(a, c, b), each (a, c, b) occurs exactly q times among the β(u,v,w)(a, c, b), where
(u, v, w) ∈Z3
q. Thus, the probability that τ = (a, b, c) is Vic’s transformation
of (a, c, b) is
|{(u, v, w) ∈Z3
q | β(u,v,w)(a, c, b) = (a, c, b)}|
|Z3q|
= q−2,
and this is the same as the probability of τ = (a, b, c) if we randomly (and
uniformly) select a transcript from the set T of all accepting transcripts of
ProofLog. We see that Peggy’s signature is really blind – she has no better
chance than guessing at random to link her signature (c, b) with the original
transcript (a, c, b). Peggy receives no information about the transcript τ from
knowing τ (in the information-theoretic sense, see Appendix B.4).
On the other hand, the following arguments give evidence that for Vic,
the only way to transform (a, c, b) into another accepting, randomly looking
transcript (a, b, c) is to randomly choose (u, v, w) and to apply β(u,v,w). First,
we observe that Vic has to set a = augvyw (for some u, v, w). Namely, assume
that Vic sets a = augvywg′ with some randomly chosen g′. Then he gets
a = augvywg′ =
³
gbyc´u
gvywg′ = gub+vyuc+wg′,
and, since (a, c, b) is an accepting transcript, it follows that
gub+vyuc+wg′ = gbyc
or
gb−(ub+v)+x(c−(uc+w)) = g′.
This equation shows that Peggy and Vic together could compute logg(g′).
This contradicts the discrete logarithm assumption, because g′ was chosen at
random.

120
4. Cryptographic Protocols
If (a, b, c) is an accepting transcript, we get from a = augvyw that
gbyc = a = augvyw =
³
gbyc´u
gvyw = gub+vyuc+w,
and conclude that
g(ub+v)−b = yc−(uc+w).
This implies that
b = ub + v and c = uc + w,
because otherwise Vic could compute Peggy’s secret key x as
(ub + v −b)(c −uc −w)−1.
Our considerations lead to Schnorr’s blind signature scheme. In this
scheme, the veriﬁer Vic gets a blind signature for m from the prover Peggy
by executing the following protocol.
Protocol 4.18.
BlindLogSigh(m):
1. Peggy randomly chooses r ∈{0, . . . , q −1}, computes a := gr
and sends it to Vic.
2. Vic chooses u, v, w ∈{0, . . . , q −1}, u ̸= 0, at random and com-
putes a = augvyw, c := h(m||a) and c := (c −w)u−1. Vic sends c
to Peggy.
3. Peggy computes b := r −cx and sends it to Vic.
4. Vic veriﬁes whether a = gbyc, computes b := ub + v and gets the
signature σ(m) := (c, b) of m.
The veriﬁcation condition for a signature (c, b) is c = h(m||gbyc).
A dishonest Vic may try to use a blind signature (c, b) for more than
one message m. This is prevented by the collision resistance of the hash
function h. In Section 4.5.2 we will use the blind signatures to form digital
cash. A coin is simply the blind signature (c, b) issued by the bank for a
speciﬁc message m. Thus, there is another basic security requirement for blind
signatures: Vic should have no chance of deriving more than one (c, b) from
one transcript (a, c, b), i.e., from one execution of the protocol. Otherwise, in
our digital cash example, Vic could derive more than one coin from the one
coin issued by the bank. The Schnorr blind signature scheme seems to fulﬁll
this security requirement. Namely, since h is collision resistant, Vic can work
with at most one a. Moreover, Vic can know at most one triplet (u, v, w), with
a = augvyw. This follows from Proposition 4.21 below, because a, g and y are
chosen independently. Finally, the transformations β(u,v,w) are the only ones
Vic can apply, as we saw above. However, we only gave convincing evidence
for the latter statement, not a rigorous mathematical proof.

4.5 Digital Cash
121
There is no mathematical security proof for either Schnorr’s blind sig-
nature scheme or the underlying Schnorr identiﬁcation scheme. A modiﬁca-
tion of the Schnorr identiﬁcation scheme, the Okamoto-Schnorr identiﬁca-
tion scheme, is proven to be secure under the discrete logarithm assump-
tion ([Okamoto92]). [PoiSte2000] gives a security proof for the Okamoto-
Schnorr blind signature scheme, derived from the Okamoto-Schnorr identiﬁ-
cation scheme. It is shown that no one can derive more than l signed messages
after receiving l blind signatures from the signer. The proof is in the so-called
random oracle model: the hash function is assumed to behave like a truly ran-
dom function. Below we use Schnorr’s blind signature scheme, because it is
a bit easier.
If we use BlindLogSigh as a subprotocol, we simply write
(c, b) = BlindLogSigh(m).
The Blindly Issued Proof that Two Logarithms are Equal. As before,
let p and q be large primes, such that q divides p −1, and let Gq be the
subgroup of order q in Z∗
p, g a generator of Gq and h : {0, 1}∗−→Zq be a
collision-resistant hash function. Peggy’s secret is a randomly chosen x ∈
{0, . . . , q −1}, whereas y = gx is publicly known.
In Section 4.4.3 we introduced an interactive proof ProofLogEq(g, y, ˜g, ˜y)
that two logarithms are equal. Given a ˜y with ˜y = ˜gx, Peggy can prove to the
veriﬁer Vic that she knows the common logarithm x of y and ˜y (with respect
to the bases g and ˜g). This proof can be transformed into a blindly issued
proof. Here it is the goal of Vic to obtain, by interacting with Peggy, a z
with z = mx for a given message m ∈Gq, together with a proof (c, b) of this
fact which he may then present to somebody else (recall that x is Peggy’s
secret and is not revealed). Now, Peggy should issue this proof blindly, i.e., she
should not see m and z during the interaction, and later she should not be able
to link (m, z, c, b) with this issuing transaction. We proceed in a similar way
as in the blind signature scheme. Vic must not give m to Peggy, so he sends
a cleverly transformed m. Peggy computes z = mx, and then both execute
the ProofLogEq protocol (Section 4.4.3), with the analogous modiﬁcation
as above: Vic does not choose his challenge randomly, but computes it by
means of the hash function h (with m as part of the input). This interaction
results in a transcript τ = (m, z, a1, a2, c, b). Finally, Vic transforms τ into
an accepting transcript τ = (m, z, a1, a2, c, b) of the non-interactive version of
ProofLogEq (see Section 4.4.4). Since τ appears completely random to Peggy,
she cannot link τ to the original τ. Vic uses the following transformation:
a1 := au
1gvyw,
a2 := (au
2mvzw)s(au
1gvyw)t,
c := uc + w,
b := ub + v,

122
4. Cryptographic Protocols
m := msgt,
z := zsyt.
As above, a straightforward computation shows that a1 = gbyc and a2 =
mbzc. Thus, the transcript τ is indeed an accepting one. Analogous argu-
ments as those given for the blind signature scheme show that the given
transformation is the only way for Vic to obtain an accepting transcript, and
that τ and τ are independent if Vic chooses u, v, w, s, t ∈{0, . . . , q −1} at
random. Hence, the proof is really blind – Peggy gets no information about
the transcript τ from knowing τ.
Our considerations lead to the following blind signature scheme. The sig-
nature is issued blindly by Peggy. If Vic wants to get a signature for m, he
transforms m to m and sends it to Peggy. Peggy computes z = mx and ex-
ecutes the ProofLogEq protocol with Vic to prove that logm(z) = logg(y).
Vic derives z and a proof that logm(z) = logg(y) from z and the proof that
logm(z) = logg(y). The signature of m consists of z = mx and the proof that
logm(z) = logg(y).
Protocol 4.19.
BlindLogEqSigh(g,y,m):
1. Vic chooses s, t ∈{0, . . . , q −1}, s ̸= 0, at random, computes
m := m1/sg−t/s and sends m to Peggy.9
2. Peggy randomly chooses r ∈{0, . . . , q−1} and computes z := mx
and a := (a1, a2) = (gr, mr). Peggy sends (z, a) to Vic.
3. Vic chooses u, v, w ∈{0, . . . , q −1}, u ̸= 0, at random, and
computes a1 := au
1gvyw, a2 := (au
2mvzw)s(au
1gvyw)t and z :=
zsyt. Then Vic computes c := h(m||z||a1||a2) and c := (c−w)u−1,
and sends c to Peggy.
4. Peggy computes b := r −cx and sends it to Vic.
5. Vic veriﬁes whether a1 = gbyc and a2 = mb zc, computes b :=
ub + v and receives (z, c, b) as the ﬁnal result.
If Vic presents the proof to Alice, then Alice may verify the proof by checking
the veriﬁcation condition c = h(m||z||gbyc||mbzc).
Below we will use BlindLogEqSigh as a subprotocol. Then we simply write
(z, c, b) = BlindLogEqSigh(g, y, m).
Remarks:
1. Again the collision resistance of h implies that Vic can use the proof
(z, c, b) for only one message m. As before, in the blind signature proto-
col BlindLogSig, we see that Vic cannot derive two diﬀerent signatures
9 As is common practice, we denote the s-th root xs−1 of an element x of order
q by x1/s. Here, s−1 is the inverse of s modulo q. See “Computing modulo a
prime” on page 303.

4.5 Digital Cash
123
(z, c, b) and (˜z, ˜c,˜b) from one execution of the protocol, i.e., from one
transcript (m, z, a1, a2, c, b).
2. The protocols BlindLogEqSigh and BlindLogSigh may be merged to yield
not only a signature of m but also a signature of an additionally given
message M. Namely, if Vic computes c = h(M||m||z||a1||a2) in step 3,
then (c, b) is also a blind signature of M, formed in the same way as the
signatures of BlindLogSigh. We denote this merged protocol by
(z, c, b) = BlindLogEqSigh(M, g, y, m),
and call it the proof BlindLogEqSigh dependent on the message M. It
simultaneously gives signatures of M and m (consisting of z and a proof
that logg(y) = logm(z)).
4.5.2 A Fair Electronic Cash System
The payment scheme we describe is published in [CamMauSta96]. A coin
is a bit string that is (blindly) signed by the bank. For simplicity, we re-
strict the scheme to a single denomination of coins: the extension to multiple
denominations is straightforward, with the bank using a separate key pair
for each denomination. As discussed in the introduction of Section 4.5, in a
fair electronic cash system the tracing of a coin and the revoking of the cus-
tomer’s anonymity must be possible under certain well-deﬁned conditions; for
example to track kidnappers who obtain a ransom as electronic cash. Here,
anonymity may be revoked by a trusted third party, the trusted center.
System Setup. As before, let p and q be large primes such that q divides
p −1. Let Gq be the subgroup of order q in Z∗
p. Let g, g1 and g2 be randomly
and independently chosen generators of Gq. The security of the scheme re-
quires that the discrete logarithm of none of these elements with respect to an-
other of these elements is known. Since g, g1 and g2 are chosen randomly and
independently, this is true with a very high probability. Let h : {0, 1}∗−→Zq
be a collision-resistant hash function.
1. The bank chooses a secret key x ∈{1, . . . , q−1} at random and publishes
y = gx.
2. The trusted center T chooses a secret key xT ∈{1, . . . , q −1} at random
and publishes yT = gxT
2 .
3. The customer – we call her Alice – has the secret key xC ∈{1, . . . , q −1}
(randomly selected) and the public key yC = gxC
1 .
4. The shop’s secret key xS is randomly selected in {1, . . . , q −1} and yS =
gxS
1
is the public key.
By exploiting data observed by the bank in the withdrawal protocol, the
trusted center can provide information which enables the recognition of a coin
withdrawn by Alice in the deposit step. This trace mechanism is called coin
tracing. Moreover, from data observed by the bank in the deposit protocol,

124
4. Cryptographic Protocols
the trusted center can compute information which enables the identiﬁcation
of the customer. This tracing mechanism is called owner tracing.
Opening an Account. When the customer Alice opens an account, she
proves her identity to the bank. She can do this by executing the protocol
ProofLog(g1, yC) with the bank. The bank then opens an account and stores
yC in Alice’s entry in the account database.
The Online Electronic Cash System. We ﬁrst discuss the online system.
The trusted center is involved in every withdrawal transaction, and the bank
is involved in every payment.
The Withdrawal Protocol. As before, Alice has to authenticate herself
to the bank. She can do this by proving that she knows xC = logg1(yC). To
get a coin, Alice executes the withdrawal protocol with the bank. It is a
modiﬁcation of the BlindLogSigh protocol given in Section 4.5.1. Essentially,
a coin is a signature of the empty string blindly issued by the bank.
Protocol 4.20.
Withdrawal:
1. The bank randomly chooses r ∈{0, . . . , q −1} and computes
a = gr. The bank sends a to Alice.
2. Alice chooses u, v, w ∈{0, . . . , q −1}, u ̸= 0, at random, and
computes a := augvyw and c := h(a), c := (c −w)u−1. Alice
sends (u, v, w) encrypted with the trusted center’s public key
and c to the bank.
3. The bank sends a and c and the encrypted (u, v, w) to the trusted
center T.
4. T checks whether uc + w = h(augvyw), and sends the result to
the bank.
5. If the result is correct, the bank computes b := r −cx and sends
it to Alice. Alice’s account is debited.
6. Alice veriﬁes whether a = gbyc, computes b := ub + v and gets
as the coin the signature σ := (c, b) of the empty message.
Payment and Deposit. In online payment, the shop must be connected to
the bank when the customer spends the coin. Payment and deposit form one
transaction. Alice spends a coin by sending it to a shop. The shop veriﬁes
the coin, i.e., it veriﬁes the signature by checking c = h(gbyc). If the coin is
valid, it passes the coin to the bank. The bank also veriﬁes the coin and then
compares it with all previously spent coins (which are stored in the database).
If the coin is new, the bank accepts it and inserts it into the database. The
shop’s account is credited.
Coin and Owner Tracing. The trusted center can link (a, c, b) and (c, b),
which enables coin and owner tracing.

4.5 Digital Cash
125
Customer Anonymity. The anonymity of the customer relies on the fact
that the used signature scheme is blind and on the security of the encryption
scheme used to encrypt the blinding factors u, v and w.
The Oﬄine Electronic Cash System. In the oﬄine system, the trusted
center is not involved in the withdrawal transaction, and the bank is not
involved in the payment protocol. To achieve an oﬄine trusted center, the
immediate check that is performed by the trusted center in the withdrawal
protocol above is replaced by a proof that Alice correctly provides the in-
formation necessary for tracing the coin. This proof can be checked by the
bank. To obtain such a proof, the BlindLogSigh-protocol is replaced by the
BlindLogEqSigh protocol (see Section 4.5.1).
Essentially, a coin consists of a pair (m, z) with m = g1gs
2, where s is
chosen at random and z = mx, and a proof of this fact which is issued
blindly by the bank. For this purpose the BlindLogEqSigh protocol is exe-
cuted: the bank sees (m = m1/s, z = z1/s). We saw above that the general
blinding transformation in the BlindLogEqSigh protocol is m = m1/sg−t.
Here Alice chooses t = 0, i.e., m = ms; otherwise Alice could not per-
form ProofLogh
¡
M, mg1−1, g2
¢
as required in the payment protocol (see
below). The blinding exponent s is encrypted by d = ys
T = gxT s
2
, where
yT is the trusted center’s public key. This enables the trusted center to re-
voke anonymity later. It can get m by decrypting d (note that m = g1gs
2 =
g1d1/xT ), and the coin can be traced by linking m and m.
The Withdrawal Protocol. As before, ﬁrst Alice has to authenticate her-
self to the bank. The withdrawal protocol in the oﬄine system consists of two
steps. In the ﬁrst of these steps, Alice generates an encryption of the message
m to enable anonymity revocation by the trusted center. In the withdrawal
step, Alice executes a message-dependent proof BlindLogEqSigh with the
bank to obtain a (blind) signature on her coin:
1. Enable coin and owner tracing. Coin tracing means that, starting from
the information gathered during the withdrawal the bank recognizes a
speciﬁc coin in the deposit protocol. Owner tracing identiﬁes the with-
drawer of a coin, starting from the deposited coin. Tracings require the
cooperation of the trusted center. To enable coin and owner tracing, Al-
ice encrypts the message m with the trusted center’s public key yT and
proves that she correctly performs this encryption:
a. Alice chooses s ∈{1, . . . , q −1} at random, then computes
m = g1gs
2, d = ys
T , m = m1/s = g1/s
1
g2 and
(c, b) = ProofLogEqh
³
mg−1
2 , g1, yT , d
´
.
By the proof, Alice shows that logmg−1
2 (g1) = logyT (d) and that she
knows this logarithm. Alice sends
¡
c, b, mg−1
2 , g1, yT , d
¢
to the bank.

126
4. Cryptographic Protocols
b. The bank veriﬁes the proof. If the veriﬁcation condition holds, then
the bank stores d in Alice’s entry in the withdrawal database for a
possible later anonymity revocation.
2. The withdrawal of a coin. Alice chooses r ∈{0, . . . , q −1} at random,
computes the so-called coin number c# = gr and executes
(z, c1, b1) = BlindLogEqSigh(c#, g, y, m)
with the bank. Here, in the ﬁrst step of BlindLogEqSigh, Alice takes the s
from the coin and owner tracing step 1 and t = 0. Thus, she sends the m =
m1/s = g1/s
1
g2 from step 1 to the bank. The variant of BlindLogEqSigh
is used that, in addition to the signature of m, gives a signature of c# by
the bank (see the remarks after Protocol 4.19). The coin number c# is
needed in the payment below. The bank debits Alice’s account. The coin
consists of (c1, b1, c#, g, y, m, z) and some additional information Alice
has to supply when spending it (see the payment protocol below).
Payment. In an oﬄine system, double spending cannot be prevented. It
can only be detected by the bank in the deposit protocol. An additional
mechanism in the protocols is necessary to decide whether the customer or
the shop doubled the coin. If the coin was doubled by the shop, the bank
refuses to credit the shop’s account. If the customer doubled the coin, her
anonymity should be revoked without the help of the trusted center. This can
be achieved by the payment protocol. For this purpose, Alice, when paying
with the coin, has to sign the message M = yS||time||(c1, b1), where yS is the
public key of the shop, time is the time of the payment and (c1, b1) is the
bank’s blind signature on the coin from above. Alice has to sign by means
of the basic signature scheme. There she has to use s = logg2
¡
mg1−1¢
as her
secret and the coin number c# = gr, which she computed in the withdrawal
protocol, as her commitment a.
σ(M) = (c2, b2) = ProofLogh
³
M, mg−1
1 , g2
´
Now, if Alice spent the same coin (c1, b1, c#, g, y, m, z) twice, she would pro-
vide two signatures σ(M) and σ(M ′) of diﬀerent messages M and M ′ (at
least the times diﬀer!). Both signatures are computed with the same com-
mitment a = c#. Then, it is easy to identify Alice (see below). The coin
submitted to the shop is deﬁned by:
coin =
³
(c1, b1, c#, g, y, m, z),
³
c2, b2, M, g2, mg−1
1
´´
.
The shop veriﬁes the coin, i.e., it veriﬁes:
1. The correct form of M.
2. Whether c2 = h(M||c#).
3. The proof (z, c1, b1) = BlindLogEqSigh(c#, g, y, m).

4.5 Digital Cash
127
4. The proof (c2, b2) = ProofLogh
³
M, mg1−1, g2
´
, by testing
c2 = h
³
M||
¡
mg1−1¢b2gc2
2
´
.
Since h is collision resistant and the shop checks c2 = h(M||c#), Alice neces-
sarily has to use the coin number c# in the second proof. The shop accepts
if the coin passes the veriﬁcation.
Deposit. The shop sends the coin to the bank. The bank veriﬁes the coin and
searches in the database for an identical coin. If she ﬁnds an identical coin,
she refuses to credit the shop’s account. If she ﬁnds a coin with identical ﬁrst
and diﬀerent second component, the bank revokes the customer’s anonymity
(see below).
Coin and Owner Tracing.
1. Coin tracing. If the bank provides the trusted center T with d = ys
T
produced by Alice in the withdrawal protocol, T computes m:
g1d1/xT = g1gs
2 = m.
This value can be used to recognize the coin in the deposit protocol.
2. Owner tracing. If the bank provides the trusted center T with the m of
a spent coin, T computes d:
¡
mg1
−1¢xT = (gs
2)xT = ys
T = d.
This value can be used for searching in the withdrawal database.
Security.
1. Double spending. If Alice spends a coin twice (at diﬀerent shops, or at the
same shop but at diﬀerent times), she produces signatures of two diﬀerent
messages. Both signatures are computed with the same commitment c#
(the coin number). This reveals the signer’s secret, which is the blinding
exponent s in our case (see the remarks after Protocol 4.17). Knowing
the blinding exponent s, the bank can derive m = m1/s. Searching in the
withdrawal database yields Alice’s identity. If the shop doubled the coin,
the bank detects an identical coin in the database. Then she refuses to
credit the shop’s account.
2. Customer’s anonymity. The anonymity of the customer is ensured. Name-
ly, BlindLogEqSigh is a perfectly blind signature scheme, as we observed
above. Moreover, since ProofLogEq is honest-veriﬁer zero knowledge (see
Section 4.4.3), the bank also cannot get any information about the blind-
ing exponent s from the ProofLogEqh in the withdrawal protocol. Note
that any information about s could enable the bank to link m with m,
and hence the withdrawal of the coin with the deposit of the coin. The
bank could also establish this link, if she were able to determine (without
computing the logarithms) that

128
4. Cryptographic Protocols
logg2
¡
mg1
−1¢
= logmg−1
2 (g1).
(4.3)
Then she could link the proofs ProofLogEqh
¡
mg−1
2 , g1, yT , d
¢
(from the
withdrawal transaction) and ProofLogh
¡
M, g2, mg−1
1
¢
(from the deposit
transaction). However, to ﬁnd out (4.3) contradicts the decision Diﬃe-
Hellman assumption (see Section 4.5.3).
3. Security for the trusted center. The trusted center makes coin and owner
tracing possible. The tracings require that Alice correctly forms m = g1gs
2
and m = m1/s = g1/s
1
g2, and that it is really ys
T which she sends as d to
the bank (in the withdrawal transaction).
Now in the withdrawal protocol, Alice proves that m = g1/s
1
g2 and that
d = ys
T . In the payment protocol, Alice proves that m = g1g˜s
2 with ˜s
known to her. It is not clear a priori that ˜s = s or, equivalently, that
m = m1/s. However, as we observed before in the blind signature scheme
BlindLogEqSigh, the only way to transform m into m is to choose σ and
τ at random and to compute m = mσgτ. From this we conclude that
indeed ˜s = s and m = ms:
g1g˜s
2 = m = mσgτ = (g1/s
1
g2)σgτ = gσ/s
1
gσ
2 gτ.
Hence, τ = 0 and σ = ˜s = s (by Proposition 4.21, below).
4.5.3 Underlying Problems
The Representation Problem. Let p and q be large primes such that q
divides p −1. Let Gq be the subgroup of order q in Z∗
p.
Let r ≥2 and let g1, . . . , gr be pairwise distinct generators of Gq.10
Then g = (g1, . . . , gr) ∈Gr
q is called a generator of length r. Let y ∈Gq.
a = (a1, . . . , ar) ∈Zr
q is a representation of y (with respect to g) if
y =
r
Y
i=1
gai
i .
To represent y, the elements a1, · · · , ar−1 can be chosen arbitrarily; ar is then
uniquely determined. Therefore, each y ∈Gq has qr−1 diﬀerent representa-
tions. Given y, the probability that a randomly chosen a ∈{0, . . . , q}r is a
representation of y is only 1/q.
Proposition 4.21. Assume that it is infeasible to compute discrete loga-
rithms in Gq. Then no polynomial algorithm can exist which, on input of a
randomly chosen generator of length r ≥2, outputs y ∈Gq and two diﬀerent
representations of y.
10 Note that every element of Gq except [1] is a generator of Gq (see Lemma A.40).

4.5 Digital Cash
129
Proof. Assume that such an algorithm exists. On input of a randomly cho-
sen generator, it outputs y ∈Gq and two diﬀerent representations a =
(a1, . . . , ar) and b = (b1, . . . , br) of y. Then, a −b is a non-trivial repre-
sentation of [1]. Thus, we have a polynomial algorithm A which on input of
a randomly chosen generator outputs a non-trivial representation of [1]. We
may use A to deﬁne an algorithm B that on input of g ∈Gq, g ̸= [1], and
z ∈Gq, computes the discrete logarithm of z with respect to g.
Algorithm 4.22.
int B(int g, z)
1
repeat
2
select i ∈{1, . . . , r} and
3
uj ∈{1, . . . , q −1}, 1 ≤j ≤r, uniformly at random
4
gi ←zui, gj ←guj, 1 ≤j ̸= i ≤r
5
(a1, . . . , ar) ←A(g1, . . . , gr)
6
until aiui ̸≡0 mod q
7
return −(aiui)−1 ³P
j̸=i ajuj
´
mod q
We have
z−aiui =
Y
j̸=i
gajuj.
Hence, the returned value is indeed the logarithm of z. Since at least one aj
returned by A is ̸= 0 modulo q, the probability that ai ̸= 0 modulo q is 1/r.
Hence, we expect that the repeat until loop will terminate after r iterations.
If r is bounded by a polynomial in the binary length |p| of p, the expected
running time of B is polynomial in |p|.
2
Remark. Assume there is a polynomial algorithm which, when given as input
a generator of length r ≥2, outputs y ∈Gq and two diﬀerent representations
of y – not with certainty, but at least with some non-negligible probability.
Then, this algorithm can be used to compute discrete logarithms in Gq with
an overwhelmingly high probability (see Exercise 4 in Chapter 6).
The Decision Diﬃe-Hellman Problem. Let p and q be large primes, such
that q divides p−1. Let Gq be the subgroup of order q in Z∗
p. Let g ∈Gq and
a, b ∈{0, . . . , q−1} be randomly chosen. Then, the Diﬃe-Hellman assumption
(Section 4.1.2) says that it is impossible to compute gab from ga and gb.
Let g1 = ga, g2 = gb and g3 be given. The decision Diﬃe-Hellman problem
is to decide if
g3 = gab.
This is equivalent to deciding whether
logg(g3) = logg(g1) logg(g2), or
logg2(g3) = logg(g1).

130
4. Cryptographic Protocols
The decision Diﬃe-Hellman assumption says that no eﬃcient algorithm ex-
ists to solve the decision Diﬃe-Hellman problem if a, b and g3 (g1, g2 and g3,
respectively) are chosen at random (and independently). The decision Diﬃe-
Hellman problem is random self-reducible (see the remark on page 154). If
you can solve it with an eﬃcient probabilistic algorithm A, then you can also
solve it, if g ∈Gq is any element of Gq and only g1, g2, g3 are chosen randomly.
Namely, let g ∈Gq, then (g, g1, g2, g3) has the Diﬃe-Hellman property, if and
only if (gs, gs
1, gs
2, gs
3) has the Diﬃe-Hellman property, with s randomly chosen
in Z∗
q.
The representation problem and the decision Diﬃe-Hellman problem are
studied, for example, in [Brands93].
Exercises
1. Let p be a suﬃciently large prime such that it is intractable to compute
discrete logarithms in Z∗
p. Let g be a primitive root in Z∗
p. p and g are
publicly known. Alice has a secret key xA and a public key yA := gxA.
Bob has a secret key xB and a public key yB := gxB. Alice and Bob
establish a secret shared key by executing the following protocol (see
[MatTakIma86]):
Protocol 4.23.
A variant of the Diﬃe-Hellman key agreement protocol:
1. Alice chooses at random a, 0 ≤a ≤p −2, sets c := ga and
sends c to Bob.
2. Bob chooses at random b, 0 ≤b ≤p −2, sets d := gb and
sends d to Alice.
3. Alice computes the shared key k = dxAyBa = gbxA+axB.
4. Bob computes the shared key k = cxByAb = gaxB+bxA.
Does the protocol provide entity authentication? Discuss the security of
the protocol.
2. Let n := pq, where p and q are distinct primes and x1, x2 ∈Z∗
n. Assume
that at least one of x1 and x2 is in QRn. Peggy wants to prove to Vic
that she knows a square root of xi for at least one i ∈{1, 2} without
revealing i. Modify Protocol 4.5 to get an interactive zero-knowledge
proof of knowledge.
3. Besides interactive proofs of knowledge, there are interactive proofs for
proving the membership in a language. The completeness and soundness
conditions for such proofs are slightly diﬀerent. Let (P, V ) be an inter-
active proof system. P and V are probabilistic algorithms, but only V is
assumed to have polynomial running time. By P ∗we denote a general
(possibly dishonest) prover. Let L ⊆{0, 1}∗(L is called a language).

Exercises
131
Bit strings x ∈{0, 1}∗are supplied to (P, V ) as common input. (P, V )
is called an interactive proof system for the language L if the following
conditions are satisﬁed:
a. Completeness. If x ∈L, then the probability that the veriﬁer V
accepts, if interacting with the honest prover P, is ≥3/4.
b. Soundness. If x /∈L, then the probability that the veriﬁer V accepts,
if interacting with any prover P ∗, is ≤1/2.
Such an interactive proof system (P, V ) is (perfect) zero-knowledge if
there is a probabilistic simulator S(V ∗, x), running in expected polyno-
mial time, such that for every veriﬁer V ∗(honest or not) and for every
x ∈L the distributions of the random variables S(V ∗, x) and (P, V ∗)(x)
are equal.
The class of languages that have interactive proof systems is denoted by
IP. It generalizes the complexity class BPP (see Exercise 3 in Chap-
ter 5).
As in Section 4.3.1, let n := pq, with p and q distinct primes, and let
J+1
n
:= {x ∈Z∗
n |
¡ x
n
¢
= 1} be the units with Jacobi symbol 1. Let
QNR+1
n
:= J+1
n \ QRn be the quadratic non-residues in J+1
n .
The following protocol is an interactive proof system for the language
QNR+1
n
(see [GolMicRac89]). The common input x is assumed to be in
J+1
n
(whether or not x ∈Zn is in J+1
n
can be eﬃciently determined using
a deterministic algorithm; see Algorithm A.59).
Protocol 4.24.
Quadratic non-residuosity:
Let x ∈J+1
n .
1. Vic chooses r ∈Z∗
n and σ ∈{0, 1} uniformly at random and
sends a = r2xσ to Peggy.
2. Peggy computes τ :=
½
0 if a ∈QRn
1 if a /∈QRn
and sends τ to Vic.
(Note that it is not assumed that Peggy can solve this in
polynomial time. Thus, she can ﬁnd out whether a ∈QRn,
for example, by an exhaustive search.)
3. Vic accepts if and only if σ = τ.
Show:
a. If x ∈QNR+1
n
and both follow the protocol, Vic will always accept.
b. If x /∈QNR+1
n , then Vic accepts with probability ≤1/2.
c. Show that the protocol is not zero-knowledge (under the quadratic
residuosity assumption; see Remark 1 in Section 4.3.1).
d. The protocol is honest-veriﬁer zero-knowledge.
e. Modify the protocol to get a zero-knowledge proof for quadratic non-
residuosity.

132
4. Cryptographic Protocols
4. We consider the identiﬁcation scheme based on public-key encryption
introduced in Section 4.2.1. In this scheme a dishonest veriﬁer can obtain
knowledge from the prover. Improve the scheme.
5. We modify the commitment scheme based on quadratic residues.
Protocol 4.25.
QRCommitment:
1. System setup. Alice chooses distinct large prime numbers
p, q ≡3 mod 4 and sets n := pq. (Note −1 ∈J+1
n
\ QRn,
see Proposition A.53.)
2. Commit to b ∈{0, 1}. Alice chooses r ∈Z∗
n at random, sets
c := (−1)br2 and sends c to Bob.
3. Reveal. Alice sends p, q, r and b to Bob. Bob can verify that
p and q are primes ≡3 mod 4, r ∈Z∗
n, and c := (−1)br2.
Show:
a. If c is a commitment to b, then −c is a commitment to 1 −b.
b. If ci is a commitment to bi, i = 1, 2, then c1c2 is a commitment to
b1 ⊕b2.
c. Show how Alice can prove to Bob that two commitments c1 and c2
commit to equal or distinct values, without opening them.
6. Let P = {Pi | i = 1, . . . , 6}. Set up a secret sharing system, such that
exactly the groups {P1, P2}, {Q ⊂P | |Q| ≥3, P1 ∈Q} and {Q ⊂P |
|Q| ≥4, P2 ∈Q} are able to reconstruct the secret.
7. Let P = {P1, P2, P3, P4}. Is it possible to set up a secret sharing system
by use of Shamir’s threshold scheme, such that the members of a group
Q ⊂P are able to reconstruct the secret if and only if {P1, P2} ⊂Q or
{P3, P4} ⊂Q?
8. In the voting scheme of Section 4.4, it is necessary that each authority
and each voter proves that he really follows the protocol. Explain why.
9. Let p and q be large primes such that q divides p −1. Let G be the
subgroup of order q in Z∗
p, g, h, yi, zi ∈G, i = 1, 2. Peggy wants to prove
to Vic that she knows an x, such that yi := gx and zi := hx for at
least one i ∈{1, 2}, without revealing i. Modify Protocol 4.15 to get an
interactive proof of knowledge. Show how the interactive proof can be
converted into a non-interactive one.
10. We consider the problem of vote duplication. This means that a voter can
duplicate the vote of another voter who has previously posted his vote.
He can do this without knowing the content of the other voter’s ballot.
Discuss this problem for the voting scheme of Section 4.4.
11. Blind RSA signatures. Construct a blind signature scheme based on
the fact that the RSA function is a homomorphism.

Exercises
133
12. Nyberg-Rueppel Signatures. Let p and q be large primes such that
q divides p −1. Let G be the subgroup of order q in Z∗
p, and let g be a
generator of G. The secret key of the signer is a randomly chosen x ∈Zq,
the public key is y := gx.
Signing. We assume that the message m to be signed is an element in
Z∗
p. The signed message is produced using the following steps:
1. Select a random integer k, 1 ≤k ≤q −1.
2. Set r := mgk and s := xr + k mod q.
3. (m, r, s) is the signed message.
Veriﬁcation. If 1 ≤r ≤p −1, 1 ≤s ≤q −1 and m = ryrg−s, accept
the signature. If not, reject it.
a. Show that the veriﬁcation condition holds for a signed message.
b. Show that it is easy to produce forged signatures.
c. How can you prevent this attack?
d. Show that the condition 1 ≤r ≤p −1 has to be checked to detect
forged signatures, even if the scheme is modiﬁed as in item c.
13. Blind Nyberg-Rueppel Signatures (see also [CamPivSta94]). In the
situation of Exercise 12, Bob gets a blind signature for a message m ∈
{1, . . . , q −1} from Alice by executing the following protocol:
Protocol 4.26.
BlindNybergRueppelSig(m):
1. Alice chooses ˜k at random, 1 ≤˜k ≤q −1, and sets ˜a := g˜k.
Alice sends ˜a to Bob.
2. Bob chooses α, β uniformly at random with 1 ≤α ≤q −1
and 0 ≤β ≤q −1, sets ˜m := m˜aα−1gβα−1, and sends ˜m to
Alice.
3. Alice computes ˜r := ˜mg˜k, ˜s := ˜rx + ˜k mod q, and sends ˜r
and ˜s to Bob.
4. Bob checks whether ( ˜m, ˜r, ˜s) is a valid signed message. If it
is, then he sets r := ˜rα and s := ˜sα + β.
Show that (m, r, s) is a signed message and that the protocol is really
blind.
14. Proof of Knowledge of a Representation (see [Okamoto92]). Let p
and q be large primes, such that q divides p −1. Let G be the subgroup
of order q in Z∗
p, and g1 and g2 be independently chosen generators. The
secret is a randomly chosen (x1, x2) ∈{0, . . . , q −1}2, and the public key
is (p, q, g1, g2, y), where y := g1x1g2x2 of G.
How can Peggy convince Vic by an interactive proof of knowledge that
she knows (x1, x2), which is a representation of y with respect to (g1, g2)?
15. Convert the interactive proof of Exercise 14 into a blind signature scheme.

5. Probabilistic Algorithms
Probabilistic algorithms are important in cryptography. On the one hand,
the algorithms used in encryption and digital signature schemes often include
random choices (as in Vernam’s one-time pad or the DSA) and therefore are
probabilistic. On the other hand, when studying the security of cryptographic
schemes, adversaries are usually modeled as probabilistic algorithms. The
subsequent chapters, which deal with provable security properties, require a
thorough understanding of this notion. Therefore, we clarify what is meant
precisely by a probabilistic algorithm, and discuss the underlying probabilistic
model.
The output y of a deterministic algorithm A is completely determined by
its input x. In a deterministic way, y is computed from x by a sequence of
steps decided in advance by the programmer. A behaves like a mathematical
mapping: applying A to the same input x several times always yields the same
output y. Therefore, we may use the mathematical notation of a mapping,
A : X −→Y , for a deterministic algorithm A, with inputs from X and out-
puts in Y . There are various equivalent formal models for such algorithms.
A popular one is the description of algorithms by Turing machines (see, for
example, [HopUll79]). Turing machines are state machines, and deterministic
algorithms are modeled by Turing machines with deterministic behavior: the
state transitions are completely determined by the input.
A probabilistic algorithm A is an algorithm whose behavior is partly con-
trolled by random events. The computation of the output y on input x de-
pends on the outcome of a ﬁnite number of random experiments. In partic-
ular, applying A to the same input x twice may yield two diﬀerent outputs.
5.1 Coin-Tossing Algorithms
Probabilistic algorithms are able to toss coins. The control ﬂow depends on
the outcome of the coin tosses. Therefore, probabilistic algorithms exhibit
random behavior.
Deﬁnition 5.1. Given an input x, a probabilistic (or randomized) algorithm
A may toss a coin a ﬁnite number of times during its computation of the
output y, and the next step may depend on the results of the preceding coin

136
5. Probabilistic Algorithms
tosses. The number of coin tosses may depend on the outcome of the previous
ones, but it is bounded by some constant tx for a given input x. The coin
tosses are independent and the coin is a fair one, i.e., each side appears with
probability 1/2.
Examples. The encryption algorithms in Vernam’s one-time pad (Section
2.1), OAEP (Section 3.3.4) and ElGamal’s scheme (Section 3.5) include ran-
dom choices, and thus are probabilistic, as well as the signing algorithms in
PSS (Section 3.4.5), ElGamal’s scheme (Section 3.5.2) and the DSA (Section
3.5.3). Other examples of probabilistic algorithms are the algorithm for com-
puting square roots in Z∗
p (see Algorithm A.61) and the probabilistic primality
tests discussed in Appendix A.8. Many examples of probabilistic algorithms
in various areas of application can be found, for example, in [MotRag95].
Remarks and Notations:
1. A formal deﬁnition of probabilistic algorithms can be given by the no-
tion of probabilistic Turing machines ([LeeMooShaSha55]; [Rabin63];
[Santos69]; [Gill77]; [BalDiaGab95]).1 In a probabilistic Turing machine,
the state transitions are determined by the input and the outcome of coin
tosses. Probabilistic Turing machines should not be confused with non-
deterministic machines. A non-deterministic Turing machine is “able to
simply guess the solution to the given problem” and thus, in general, is
not something that can be implemented in practice. A probabilistic ma-
chine (or algorithm) is able to ﬁnd the solution by use of its coin tosses,
with some probability. Thus, it is something that can be implemented in
practice.
Of course, we have to assume (and will assume in the following) that a
random source of independent fair coin tosses is available. To implement
such a source, the inherent randomness in physical phenomena can be
exploited (see [MenOorVan96] and [Schneier96] for examples of sources
which might be used in a computer).
To derive perfectly random bits from a natural source is a non-trivial task.
The output bits may be biased (i.e., the probability that 1 is emitted is
diﬀerent from 1/2) or correlated (the probability of 1 depends on the
previously emitted bits). The outcomes of physical processes are often
aﬀected by previous outcomes and the circumstances that led to these
outcomes. If the bits are independent, the problem of biased bits can be
easily solved using the following method proposed by John von Neumann
([von Neumann63]): break the sequence of bits into pairs, discard pairs
00 and 11, and interpret 01 as 0 and 10 as 1 (the pairs 01 and 10 have
the same probability). Handling a correlated bit source is more diﬃcult.
However, there are eﬀective means of generating truly random sequences
1 All algorithms are assumed to have a ﬁnite description (as a Turing machine)
which is independent of the size of the input. We do not consider non-uniform
algorithms in this book.

5.1 Coin-Tossing Algorithms
137
of bits from a biased and correlated source. For example, Blum developed
a method for a source which produces bits according to a known Markov
chain ([Blum84]). Vazirani ([Vazirani85]) shows how almost independent,
unbiased bits can be derived from two independent “slightly-random”
sources. For a discussion of slightly random sources and their use in
randomized algorithms, see [Papadimitriou94], for example.
2. The output y of a probabilistic algorithm A depends on the input x
and on the binary string r, which describes the outcome of the coin
tosses. Usually, the coin tosses are considered as internal operations of the
probabilistic algorithm. A second way to view a probabilistic algorithm
A is to consider the outcome of the coin tosses as an additional input,
which is supplied by an external coin-tossing device. In this view, the
model of a probabilistic algorithm is a deterministic machine. We call the
corresponding deterministic algorithm AD the deterministic extension of
A. It takes as inputs the original input x and the outcome r of the coin
tosses.
3. Given x, the output A(x) of a probabilistic algorithm A is not a single
constant value, but a random variable. “A outputs y on input x” is a
random event, and by prob(A(x) = y) we mean the probability of this
event. More precisely, we have
prob(A(x) = y) := prob({r | AD(x, r) = y}).2
Here a question arises: what probability distribution of the coin tosses is
meant? The question is easily answered if, as in our deﬁnition of prob-
abilistic algorithms, the number of coin tosses is bounded by some con-
stant tx for a given x. In this case, adding some dummy coin tosses, if
necessary, we may assume that the number of coin tosses is exactly tx.
Then the possible outcomes r of the coin tosses are the binary strings of
length tx, and since the coin tosses are independent, we have the uniform
distribution of {0, 1}tx. The probability of an outcome r is 1/2tx, and
hence
prob(A(x) = y) = |{r | AD(x, r) = y}|
2tx
.
It is suﬃcient for all our purposes to consider only probabilistic algo-
rithms with a bounded number of coin tosses, for a given x. In most parts
of this book we consider algorithms whose running time is bounded by a
function f(|x|), where |x| is the size of the input x. For these algorithms,
the assumption is obviously true.
4. Given x, the probabilities prob(A(x) = y), y ∈Y, deﬁne a probability
distribution on the range Y . We denote it by pA(x). The random variable
A(x) samples Y according to the distribution pA(x).
2 If the probability distribution is determined by the context, we often do not
specify the distribution explicitly and simply write prob(e) for the probability of
an element or event e (see Appendix B.1).

138
5. Probabilistic Algorithms
5. The setting where a probabilistic algorithm A is executed may include
further random events. Now, tossing a fair coin in A is assumed to be
an independent random experiment. Therefore, the outcome of the coin
tosses of A on input x is independent of all further random events in the
given setting. In the following items, we apply this basic assumption.
6. Suppose that the input x ∈X of a probabilistic algorithm A is ran-
domly generated. This means that a probability distribution pX is given
for the domain X (e.g. the uniform distribution). We may consider the
random experiment “Randomly choose x ∈X according to pX and com-
pute y = A(x)”. If the outputs of A are in Y , then the experiment is
modeled by a joint probability space (XY, pXY ). The coin tosses of A(x)
are independent of the random choice of x. Thus, the probability that
x ∈X is chosen and that y = A(x) is
prob(x, A(x) = y) = pXY (x, y) = pX(x) · prob(A(x) = y).
The probability prob(A(x) = y) is the conditional probability prob(y|x)
of the outcome y, assuming the input x.
7. Each execution of A is a new independent random experiment: the coin
tosses during one execution of A are independent of the coin tosses in
other executions of A. In particular, when executing A twice, with inputs
x and x′, we have
prob(A(x) = y, A(x′) = y′) = prob(A(x) = y) · prob(A(x′) = y′).
If probabilistic algorithms A and B are applied to inputs x and x′, then
the coin tosses of A(x) and B(x′) are independent, unless B(x′) is called
as a subroutine of A(x) (such that the coin tosses of B(x′) are contained
in the coin tosses of A(x)), or vice versa:
prob(A(x) = y, B(x′) = y′) = prob(A(x) = y) · prob(B(x′) = y′).
8. Let A be a probabilistic algorithm with inputs from X and outputs in Y .
Let h : X −→Z be a map yielding some property h(x) for the elements in
X (e.g. the least-signiﬁcant bit of x). Let B be a probabilistic algorithm
which on input y ∈Y outputs B(y) ∈Z. Assume that a probability
distribution pX is given on X. B might be an algorithm trying to invert
A or at least trying to determine the property h(x) from y := A(x). We
are interested in the random experiment “Randomly choose x, compute
y = A(x) and B(y), and check whether B(y) = h(x)”. The random choice
of x, the coin tosses of A(x) and the coin tosses of B(y) are independent
random experiments. Thus, the probability that x ∈X is chosen, and
that A(x) = y and B(y) correctly computes h(x) is
prob(x, A(x) = y, B(y) = h(x))
= prob(x) · prob(A(x) = y) · prob(B(y) = h(x)).

5.1 Coin-Tossing Algorithms
139
9. Let pX be a probability distribution on the domain X of a probabilistic
algorithm A with outputs in Y . Randomly selecting an x ∈X and com-
puting A(x) is described by the joint probability space XY (see above).
We can project to Y , (x, y) 7→y, and calculate the probability distribu-
tion pY on Y :
pY (y) :=
X
x∈X
pXY (x, y) =
X
x∈X
pX(x) · prob(A(x) = y).
We call pY the image of pX under A. pY is also the image of the joint
distribution of X and the coin tosses r under the deterministic extension
AD of A:
pY (y) = prob({(x, r) | AD(x, r) = y})
=
X
x∈X,r∈{0,1}tx:AD(x,r)=y
pX(x) · prob(r).
As in the deterministic case (see Appendix B.1, p. 330), we sometimes
denote the image distribution by {A(x) : x ←X}.
Let A be a probabilistic algorithm with inputs from X and outputs in Y .
Then A (as a Turing machine) has a ﬁnite binary description. In particular,
we can assume that both the domain X and the range Y are subsets of
{0, 1}∗. The time and space complexity of an algorithm A (corresponding to
the running time and memory requirements) are measured as functions of
the binary length |x| of the input x.
Deﬁnition 5.2. A probabilistic polynomial algorithm is a probabilistic algo-
rithm A, such that the running time of A(x) is bounded by P(|x|), where
P ∈Z[X] is a polynomial (the same for all inputs x). The running time is
measured as the number of steps in our model of algorithms, i.e., the number
of steps of the probabilistic Turing machine. Tossing a coin is one step in this
model.
Remark. The randomness in probabilistic algorithms is caused by random
events in a very speciﬁc probability space, namely {0, 1}tx with the uniform
distribution, and at ﬁrst glance this might look restrictive. Actually, it is a
very general model.
For example, suppose you want to control an algorithm A on input x by rx
random events with probabilities px,1, . . . , px,rx (the deterministic extension
AD takes as inputs x and one of the events).3 Assume that always one of
the events occurs (i.e., Prx
i=1 px,i = 1) and that the probabilities px,i have a
ﬁnite binary representation px,i = Ptx
j=1 ax,i,j · 2−j (ax,i,j ∈{0, 1}). Further,
3 For example, think of an algorithm A that attacks an encryption scheme. If all
possible plaintexts and their probability distribution are known, then A might
be based on the random choice of correctly distributed plaintexts.

140
5. Probabilistic Algorithms
assume that rx, tx and the probabilities px,i are computable by deterministic
(polynomial) algorithms with input x. The last assumption is satisﬁed, for
example, if the events and probabilities are the same for all x.
Then the random behavior of A can be implemented by coin tosses, i.e.,
A can be implemented as a probabilistic (polynomial) algorithm in the sense
of Deﬁnitions 5.1 and 5.2. Namely, let S be the coin-tossing algorithm which
on input x:
(1) tx times tosses the coin and obtains a binary number b := btx−1 . . . b1b0,
with 0 ≤b < 2tx, and
(2) returns S(x) := i, if 2tx Pi−1
j=1 px,j ≤b < 2tx Pi
j=1 px,j.
The outputs of S(x) are in {1, . . . , rx} and prob(S(x) = i) = px,i, for 1 ≤i ≤
rx. The probabilistic (polynomial) algorithm S can be used to produce the
random inputs for AD.
5.2 Monte Carlo and Las Vegas Algorithms
The running time of a probabilistic polynomial algorithm A is required to be
bounded by a polynomial P, for all inputs x. Assume A tries to compute the
solution to a problem. Due to its random behavior, A might not reach its goal
with certainty, but only with some probability. Therefore, the output might
not be correct in some cases. Such algorithms are also called Monte Carlo
algorithms if their probability of success is not too low. They are distinguished
from Las Vegas algorithms.
timeA(x) denotes the running time of A on input x, i.e., the number of
steps A needs to generate the output A(x) for the input x. As before, |x|
denotes the binary length of x.
Deﬁnition 5.3. Let P be a computational problem.
1. A Monte Carlo algorithm A for P is a probabilistic algorithm A, whose
running time is bounded by a polynomial Q and which yields a correct
answer to P with a probability of at least 2/3:
timeA(x) ≤Q(|x|) and prob(A(x) is a correct answer to P) > 2
3,
for all instances x of P.
2. A probabilistic algorithm A for P is called a Las Vegas algorithm if its
output is always a correct answer to P, and if the expected value for the
running time is bounded by a polynomial Q:
E(timeA(x)) =
∞
X
t=1
t · prob(timeA(x) = t) < Q(|x|),
for all instances x of P.

5.2 Monte Carlo and Las Vegas Algorithms
141
Remarks:
1. The probabilities are computed assuming a ﬁxed input x; they are only
taken over the coin tosses during the computation of A(x). The distribu-
tion of the inputs x is not considered. For example,
prob(A(x) is a correct answer to P) =
X
y∈Yx
prob(A(x) = y),
with the sum taken over the set Yx of correct answers for input x.
2. The running time of a Monte Carlo algorithm is bounded by one poly-
nomial Q, for all inputs. A Monte Carlo algorithm may sometimes fail
to produce a correct answer to the given problem. However, the prob-
ability of such a failure is bounded. In contrast, a Las Vegas algorithm
always gives a correct answer to the given problem. The running time
may vary substantially and is not necessarily bounded by a single poly-
nomial. However, the expected value of the running time is polynomial.
Examples:
1. Typical examples of Monte Carlo algorithms are the probabilistic pri-
mality tests discussed in Appendix A.8. They check whether an integer
is prime or not.
2. Algorithm A.61, which computes square roots modulo a prime p, is a Las
Vegas algorithm.
3. To prove the zero-knowledge property of an interactive proof system, it
is necessary to simulate the prover by a Las Vegas algorithm (see Section
4.2.3).
A Las Vegas algorithm may be turned into a Monte Carlo algorithm
simply by stopping it after a suitable polynomial number of steps. To state
this fact more precisely, we use the notion of positive polynomials.
Deﬁnition 5.4. A polynomial P(X) = Pn
i=0 aiXi ∈Z[X] in one variable
X, with integer coeﬃcients ai, 0 ≤i ≤n, is called a positive polynomial if
P(x) > 0 for x > 0, i.e., P has positive values for positive inputs.
Examples. The polynomials Xn are positive. More generally, each polynomial
whose non-zero coeﬃcients are positive is a positive polynomial.
Proposition 5.5. Let A(x) be a Las Vegas algorithm for a problem P with
an expected running time ≤Q(|x|) (Q a polynomial). Let P be a positive
polynomial. Let ˜A be the algorithm obtained by stopping A(x) after at most
P(|x|) steps. Then ˜A is a Monte Carlo algorithm for P, which gives a correct
answer to P with probability ≥1 −Q(|x|)/P(|x|).

142
5. Probabilistic Algorithms
Proof. We have
P(|x|) · prob(timeA(x) ≥P(|x|)) ≤
∞
X
t=P (|x|)
t · prob(timeA(x) = t)
≤E(timeA(x)) ≤Q(|x|).
Thus, ˜A gives a correct answer to P with probability ≥1 −Q(|x|)/P(|x|). 2
Remark. ˜A(x) might return “I don’t know” if A(x) did not yet terminate
after P(|x|) steps and is stopped. Then the answer of ˜A is never false, though
it is not always a solution to P.
The choice of the bound 2/3 in our deﬁnition of Monte Carlo algorithms is
somewhat arbitrary. Other bounds could be used equally well, as Proposition
5.6 below shows. We can increase the probability of success of an algorithm
A(x) by repeatedly applying the original algorithm and by making a majority
decision. This works if the probability that A produces a correct result exceeds
1/2 by a non-negligible amount.
Proposition 5.6. Let P and Q be positive polynomials. Let A be a proba-
bilistic polynomial algorithm which computes a function f : X −→Y , with
prob(A(x) = f(x)) ≥1
2 +
1
P(|x|) for all x ∈X.
Then, by repeating the computation A(x) and returning the most frequent
result, we get a probabilistic polynomial algorithm ˜A, such that
prob( ˜A(x) = f(x)) > 1 −
1
Q(|x|) for all x ∈X.
Proof. Consider the algorithms At (t ∈N), deﬁned on input x ∈X as follows:
1. Execute A(x) t times, and get the set Yx := {y1, . . . , yt} of outputs.
2. Select an i ∈{1, . . . , t}, with |{y ∈Yx | y = yi}| maximal.
3. Set At(x) := yi.
We expect that more than half of the results of A coincide with f(x), and
hence At(x) = f(x) with high probability. More precisely, deﬁne the binary
random variables Sj, 1 ≤j ≤t :
Sj :=
½
1 if yj = f(x),
0 otherwise.
The expected values E(Sj) are equal to
E(Sj) = prob(A(x) = f(x)) ≥1
2 +
1
P(|x|),

5.2 Monte Carlo and Las Vegas Algorithms
143
and we conclude, by Corollary B.18, that
prob(At(x) = f(x)) ≥prob


t
X
j=1
Sj > t
2

≥1 −P(|x|)2
4t
.
For t > 1/4 · P(|x|)2 · Q(|x|), the probability of success of At is > 1 −1/Q(|x|).
2
Remark. By using the Chernoﬀbound from probability theory, the preceding
proof can be modiﬁed to get a polynomial algorithm ˜A whose probability
of success is exponentially close to 1 (see Exercise 5). We do not need this
stronger result. If a polynomial algorithm which checks the correctness of a
solution is available, we can do even better.
Proposition 5.7. Let P, Q and R be positive polynomials. Let A be a proba-
bilistic polynomial algorithm computing solutions to a given problem P with
prob(A(x) is a correct answer to P) ≥
1
P(|x|), for all inputs x.
Let D be a deterministic polynomial algorithm, which checks whether a given
answer to P is correct. Then, by repeating the computation A(x) and by
checking the results with D, we get a probabilistic polynomial algorithm ˜A for
P, such that
prob( ˜A(x) is a correct answer to P) > 1 −2−Q(|x|), for all inputs x.
Proof. Consider the algorithms At (t ∈N), deﬁned on input x as follows:
1. Repeat the following at most t times:
a. Execute A(x), and get the answer y.
b. Apply D to check whether y is a correct answer.
c. If D says “correct”, stop the iteration.
2. Return y.
The executions of A(x) are t independent repetitions of the same experiment.
Hence, the probability that all t executions of A yield an incorrect answer is
< (1 −1/P(|x|))t, and we obtain
µ
1 −
1
P(|x|)
¶t
=
Ãµ
1 −
1
P(|x|)
¶P (|x|)!t/P (|x|)
<
¡
e−1¢t/P (|x|)
= e−t/P (|x|) ≤e−ln(2)Q(|x|) = 2−Q(|x|),
for t ≥ln(2)P(|x|)Q(|x|).
2
Remark. The iterations At in the preceding proof may be used to construct a
Las Vegas algorithm that always gives a correct answer to the given problem
(see Exercise 1).

144
5. Probabilistic Algorithms
Exercises
1. Let P be a positive polynomial. Let A be a probabilistic polynomial
algorithm which on input x ∈X computes solutions to a given problem
P with
prob(A(x) is a correct answer to P) ≥
1
P(|x|) for all x ∈X.
Assume that there is a deterministic algorithm D, which checks in poly-
nomial time whether a given solution y to P on input x is correct.
Construct a Las Vegas algorithm that always gives the correct answer
and whose expected running time is ≤P(|x|)(R(|x|) + S(|x| + R(|x|))),
where R and S are polynomial bounds for the running times of A and D
(use Lemma B.12).
2. Let L ⊂{0, 1}∗be a decision problem in the complexity class BPP of
bounded-error probabilistic polynomial-time problems. This means that
there is a probabilistic polynomial algorithm A with input x ∈{0, 1}∗
and output A(x) ∈{0, 1} which solves the membership decision problem
for L ⊂{0, 1}∗, with a bounded error probability. More precisely, there
are a positive polynomial P and a constant a, 0 < a < 1, such that
prob(A(x) = 1) ≥a +
1
P(|x|), for x ∈L, and
prob(A(x) = 1) ≤a −
1
P(|x|), for x /∈L.
Let Q be another positive polynomial. Show how to obtain a probabilistic
polynomial algorithm ˜A, with
prob( ˜A(x) = 1) ≥1 −
1
Q(|x|), for x ∈L, and
prob( ˜A(x) = 1) ≤
1
Q(|x|), for x /∈L.
(Use a similar technique as that used, for example, in the proof of Propo-
sition 5.6.)
3. A decision problem L ⊂{0, 1}∗belongs to the complexity class RP of
randomized probabilistic polynomial-time problems if there exists a prob-
abilistic polynomial algorithm A which on input x ∈{0, 1}∗outputs
A(x) ∈{0, 1}, and a positive polynomial Q, such that prob(A(x) = 1) ≥
1/Q(|x|), for x ∈L, and prob(A(x) = 1) = 0, for x /∈L.
A decision problem L belongs to the complexity class NP if there is a
deterministic polynomial algorithm M(x, y) and a polynomial L, such
that M(x, y) = 1 for some y ∈{0, 1}∗, with |y| ≤L(|x|), if and only if
x ∈L (y is called a certiﬁcate for x). Show that:

Exercises
145
a. RP ⊆BPP.
b. RP ⊆NP.
4. A decision problem L ⊂{0, 1}∗belongs to the complexity class ZPP
of zero-sided probabilistic polynomial-time problems if there exists a Las
Vegas algorithm A(x), such that A(x) = 1 if x ∈L, and A(x) = 0 if
x /∈L.
Show that ZPP ⊆RP.
5. If S1, . . . , Sn are independent repetitions of a binary random variable
X and p := prob(X = 1) = E(X), then the Chernoﬀbound holds for
0 < ε ≤p(1 −p):
prob
Ã¯¯¯¯¯
1
n
n
X
i=1
Si −p
¯¯¯¯¯ < ε
!
≥1 −2e−nε2/2,
(see, e.g., [R´enyi70], Section 7.4). Use the Chernoﬀbound to derive an
improved version of Proposition 5.6.

6. One-Way Functions and the Basic
Assumptions
In Chapter 3 we introduced the notion of one-way functions. As the examples
of RSA encryption and Rabin signatures show, one-way functions play the
key role in asymmetric cryptography.
Speaking informally, a one-way function is a map f : X −→Y which is
easy to compute but hard to invert. There is no eﬃcient algorithm that
computes pre-images of y ∈Y . If we want to use a one-way function f for
encryption in the straightforward way (applying f to the plaintext, as, for
example, in RSA encryption), then f must belong to a special class of one-
way functions. Knowing some information (”the trapdoor information”: e.g.
the factorization of the modulus n in RSA schemes), it must be easy to invert
f, and f is one way only if the trapdoor information is kept secret. These
functions are called trapdoor functions.
Our notion of one-way functions introduced in Chapter 3 was a rather
informal one: we did not specify precisely what we mean by “eﬃciently com-
putable”, “infeasible” or “hard to invert”. Now, in this chapter, we will clarify
these terms and give a precise deﬁnition of one-way functions. For example,
“eﬃciently computable” means that the solution can be computed by a prob-
abilistic polynomial algorithm, as deﬁned in Chapter 5.
We discuss three examples in some detail: the discrete exponential func-
tion, modular powers and modular squaring. The ﬁrst is not a trapdoor func-
tion. Nevertheless, it has important applications in cryptography (e.g. pseu-
dorandom bit generators, see Chapter 8; ElGamal’s encryption and signature
scheme and the DSA, see Chapter 3).
Unfortunately, there is no proof that these functions are really one way.
However, it is possible to state the basic assumptions precisely, which guar-
antee the one-way feature. It is widely believed that these assumptions are
true.
In order to deﬁne the one-way feature (and in a way that naturally
matches the examples), we have to consider not only single functions, but,
more generally, families of functions deﬁned over appropriate index sets.
In the preliminary (and very short) Section 6.1, we introduce an intuitive
notation for probabilities that will be used subsequently.

148
6. One-Way Functions and the Basic Assumptions
6.1 A Notation for Probabilities
The notation
prob(B(x) = 1 : x ←X) := prob({x ∈X | B(x) = 1}),
introduced for Boolean predicates1 B : X −→{0, 1} in Appendix B.1 (p.
329), intuitively suggests that we mean the probability of B(x) = 1 if x is
randomly chosen from X. We will use an analogous notation for probabilistic
algorithms.
Let A be a probabilistic algorithm with inputs from X and outputs in
Y , and let B : X × Y −→{0, 1}, (x, y) 7−→B(x, y) be a Boolean predicate.
Let pX be a probability distribution on X. As in Section 5.1, let AD be the
deterministic extension of A, and let tx denote the number of coin tosses of
A on input x:
prob(B(x, A(x)) = 1 : x
pX
←X)
:=
X
x∈X
prob(x) · prob(B(x, A(x)) = 1)
=
X
x∈X
prob(x) · prob({r ∈{0, 1}tx | B(x, AD(x, r)) = 1)}
=
X
x∈X
prob(x) · |{r ∈{0, 1}tx | B(x, AD(x, r)) = 1)}|
2tx
.
The notation is typically used in the following situation. A Monte Carlo algo-
rithm A tries to compute a function f : X −→Y , and B(x, y) := Bf(x, y) :=
1 if f(x) = y, and B(x, y) := Bf(x, y) := 0 if f(x) ̸= y. Then
prob(A(x) = f(x) : x
pX
←X) := prob(Bf(x, A(x)) = 1 : x
pX
←X)
is the probability that A succeeds if the input x is randomly chosen from X
(according to pX).
We write x
u←X if the distribution on X is the uniform one, and often
we simply write x ←X instead of x
pX
←X.
In cryptography, we often consider probabilistic algorithms whose domain
X is a joint probability space X1X2 . . . Xr constructed by iteratively joining
ﬁbers Xj,x1...xj−1 to X1 . . . Xj−1 (Appendix B.1, p. 328). In this case, the
notation is
prob(B(x1, . . . , xr, A(x1, . . . , xr)) = 1 :
x1 ←X1, x2 ←X2,x1, x3 ←X3,x1x2,
. . .
, xr ←Xr,x1...xr−1).
Now, the notation suggests that we mean the probability of the event
B(x1, . . . , xr, A(x1, . . . , xr)) = 1 if ﬁrst x1 is randomly chosen, then x2, then
1 Maps with values in {0, 1} are called Boolean predicates.

6.2 Discrete Exponential Function
149
x3, then . . . .
A typical example is the discrete logarithm assumption in Section 6.2 (Deﬁ-
nition 6.1).
The distribution xj ←Xj,x1...xj−1 is the conditional distribution of
xj ∈Xj,x1...xj−1, assuming x1, . . . , xj−1. The probability can be computed
as follows (we consider r = 3 and the case where A computes a function f
and B is the predicate A(x) = f(x)):
prob(A(x1, x2, x3) = f(x1, x2, x3) : x1 ←X1, x2 ←X2,x1, x3 ←X3,x1x2)
=
X
x1,x2,x3
prob(x1, x2, x3) · prob(A(x1, x2, x3) = f(x1, x2, x3))
=
X
x1∈X1
prob(x1) ·
X
x2∈X1,x1
prob(x2 |x1)
·
X
x3∈X1,x1,x2
prob(x3 |x2, x1) · prob(A(x1, x2, x3) = f(x1, x2, x3)).
Here prob(x2 |x1) (resp. prob(x3 |x2, x1)) denotes the conditional probability
of x2 (resp. x3) assuming x1 (resp. x1 and x2); see Appendix B.1 (p. 328).
The last probability, prob(A(x1, x2, x3) = f(x1, x2, x3)), is the probability
that the coin tosses of A on input (x1, x2, x3) yield the result f(x1, x2, x3).
In Section 5.1, we introduced the image pY of the distribution pX under
a probabilistic algorithm A from X to Y . We have
pY (y) = prob(A(x) = y : x
pX
←X)
for each y ∈Y .
For each x ∈X, we have the distribution pA(x) on Y :
pA(x)(y) = prob(A(x) = y).
We write y ←A(x) instead of y
pA(x)
←
Y . This notation suggests that y is
generated by the random variable A(x). With this notation, we have
prob(A(x) = f(x) : x ←X) = prob(f(x) = y : x ←X, y ←A(x)).
6.2 Discrete Exponential Function
The notion of one-way functions can be precisely deﬁned using probabilistic
algorithms. As a ﬁrst example we consider the discrete exponential function.
Let I := {(p, g) | p a prime number, g ∈Z∗
p a primitive root}. We call the
family of discrete exponential functions
Exp := (Expp,g : Zp−1 −→Z∗
p, x 7−→gx)(p,g)∈I

150
6. One-Way Functions and the Basic Assumptions
the Exp family. Since g is a primitive root, Expp,g is an isomorphism be-
tween the additive group Zp−1 and the multiplicative group Z∗
p. The family
of inverse functions
Log := (Logp,g : Z∗
p −→Zp−1)(p,g)∈I
is called the Log family.
The algorithm of modular exponentiation computes Expp,g(x) eﬃciently
(see Algorithm A.26). It is unknown whether an eﬃcient algorithm for the
computation of the discrete logarithm function exists. All known algorithms
have exponential running time, and it is widely believed that, in general,
Logp,g is not eﬃciently computable. We state this assumption on the one-
way property of the Exp family by means of probabilistic algorithms.
Deﬁnition 6.1. Let Ik := {(p, g) ∈I | |p| = k}, with k ∈N,2 and let Q(X) ∈
Z[X] be a positive polynomial. Let A(p, g, y) be a probabilistic polynomial
algorithm. Then there exists a k0 ∈N, such that
prob(A(p, g, y) = Logp,g(y) : (p, g)
u←Ik, y
u←Z∗
p) ≤
1
Q(k)
for k ≥k0.
This is called the discrete logarithm assumption.
Remarks:
1. The probabilistic algorithm A models an attacker who tries to compute
the discrete logarithm or, equivalently, to invert the discrete exponential
function. The discrete logarithm assumption essentially states that for a
suﬃciently large size k of the modulus p, the probability of A successfully
computing Logp,g(y) is smaller than 1/Q(k). This means that Exp cannot
be inverted by A for all but a negligible fraction of the inputs. There-
fore, we call Exp a family of one-way functions. The term “negligible” is
explained more precisely in a subsequent remark.
2. When we use the discrete exponential function in a cryptographic scheme,
such as ElGamal’s encryption scheme (see Section 3.5.1), selecting a func-
tion Expp,g from the family means to choose a public key i = (p, g)
(actually, i may be only one part of the key).
3. The index set I is partitioned into disjoint subsets: I = S
k∈N Ik. k may
be considered as the security parameter of i = (p, g) ∈Ik. The one-way
property requires a suﬃciently large security parameter. The security
parameter is closely related to the binary length of i. Here, k = |p| is half
the length of i.
4. The probability in the discrete logarithm assumption is also taken over
the random choice of a key i with a given security parameter k. Hence, the
2 As usual, |p| denotes the binary length of p.

6.2 Discrete Exponential Function
151
meaning of the probability statement is: choosing both the key i = (p, g)
with security parameter k and y = gx randomly, the probability that A
correctly computes the logarithm x from y is small. The statement is not
related to a particular key i. In practice, however, a public key is chosen
and then ﬁxed for a long time, and it is known to the adversary. Thus, we
are interested in the conditional probability of success, assuming a ﬁxed
public key i. Even if the security parameter k is very large, there may be
keys (p, g) such that A correctly computes Logp,g(y) with a signiﬁcant
chance. However, as we will see below, the number of such keys (p, g) is
negligibly small compared to all keys with security parameter k. Choosing
(p, g) at random (and uniformly) from Ik, the probability of obtaining
one for which A has a signiﬁcant chance of success is negligibly small (see
Proposition 6.3 for a precise statement). Indeed, if p −1 has only small
prime factors, an eﬃcient algorithm developed by Pohlig and Hellman
computes the discrete logarithm function (see [PohHel78]).
Remark. In this book, we often consider families ε = (εk)k∈N of quantities
εk ∈R, as the probabilities in the discrete logarithm assumption. We call
them negligible or negligibly small if, for every positive polynomial Q ∈Z[X],
there is a k0 ∈N, such that |εk| ≤1/Q(k) for k ≥k0. “Negligible” means that
the absolute value is asymptotically smaller than any polynomial bound.
Remark. In order to simplify, deﬁnitions and results are often stated asymp-
totically (as the discrete logarithm assumption or the notion of negligible
quantities). Polynomial running times or negligible probabilities are not spec-
iﬁed more precisely, even if it were possible. A typical situation is as follows. A
cryptographic scheme is based on a one-way function f (e.g. the Exp family).
Let g be a function that describes a property of the cryptographic scheme
(e.g. g predicts the next bit of the discrete exponential pseudorandom bit
generator; see Chapter 8). It is desirable that this property g cannot be eﬃ-
ciently computed by an adversary. Sometimes, this can be proven. Typically a
proof runs by a contradiction. We assume that a probabilistic polynomial al-
gorithm A1 which successfully computes g with probability ε1 is given. Then,
a probabilistic polynomial algorithm A2 is constructed which calls A1 as a
subroutine and inverts the underlying one-way function f, with probability
ε2. Such an algorithm is called a polynomial-time reduction of f to g. If ε2
is non-negligible, we get a contradiction to the one-way assumption (e.g. the
discrete logarithm assumption).
In our example, a typical statement would be as follows. If discrete loga-
rithms cannot be computed in polynomial time with non-negligible probabil-
ity (i.e., if the discrete logarithm assumption is true), then a polynomial-time
adversary cannot predict, with non-negligible probability, the next bit of the
discrete exponential pseudorandom bit generator.
Actually, in many cases the statement could be made more precise, by
performing a detailed analysis of the reduction algorithm A2. The running

152
6. One-Way Functions and the Basic Assumptions
time of A2 can be described as an explicit function of ε1, ε2 and the running
time of A1 (see, e.g., the results in Chapter 7).
As in the Exp example, we often meet families of functions indexed on a set
of keys which may be partitioned according to a security parameter. Therefore
we propose the notion of indexes, whose binary lengths are measured by a
security parameter as speciﬁed more precisely by the following deﬁnition.
Deﬁnition 6.2. Let I = S
k∈N Ik be an inﬁnite index set which is partitioned
into ﬁnite disjoint subsets Ik. Assume that the indexes are binarily encoded.
As always, we denote by |i| the binary length of i.
I is called a key set with security parameter k or an index set with security
parameter k, if:
1. The security parameter k of i ∈I can be derived from i by a deterministic
polynomial algorithm.
2. There is a constant m ∈N, such that
k1/m ≤|i| ≤km for i ∈Ik.
We usually write I = (Ik)k∈N instead of I = S
k∈N Ik.
Remarks:
1. The second condition means that the security parameter k is a measure
for the binary length |i| of the elements i ∈Ik. In particular, statements
such as:
(1) “There is a polynomial P with . . . ≤P(|i|)”, or
(2) “For every positive polynomial Q, there is a k0 ∈N, such that
. . . ≤1/Q(|i|) for |i| ≥k0”,
are equivalent to the corresponding statements in which |i| is replaced
by the security parameter k. In almost all of our examples, we have
k ≤|i| ≤3k for i ∈Ik.
2. The index set I of the Exp family is a key set with security parameter.
As with all indexes occurring in this book, the indexes of the Exp family
consist of numbers in N or residues in some residue class ring Zn. Unless
otherwise stated, we consider them as binarily encoded in the natural way
(see Appendix A): the binary encoding of x ∈N is its standard encoding
as an unsigned number, and the encoding of a residue class [a] ∈Zn is
the encoding of its representative x with 0 ≤x ≤n −1.
3. If I = S
k∈N Ik satisﬁes only the second condition, then we can easily
modify it and turn it into a key set with a security parameter, which also
satisﬁes the ﬁrst condition. Namely, let ˜Ik := {(i, k) | i ∈Ik} and replace
I by ˜I := S
k∈N ˜Ik.
In the discrete logarithm assumption, we do not consider a single ﬁxed
key i: the probability is also taken over the random choice of the key. In the
following proposition, we relate this average probability to the conditional
probabilities, assuming a ﬁxed key.

6.2 Discrete Exponential Function
153
Proposition 6.3. Let I = (Ik)k∈N be a key set with security parameter k.
Let f = (fi : Xi −→Yi)i∈I be a family of functions and A be a probabilistic
polynomial algorithm with inputs i ∈I and x ∈Xi and output in Yi. Assume
that probability distributions are given on Ik and Xi for all k, i (e.g. the
uniform distributions). Then the following statements are equivalent:
1. For every positive polynomial P, there is a k0 ∈N, such that for all
k ≥k0
prob(A(i, x) = fi(x) : i ←Ik, x ←Xi) ≤
1
P(k).
2. For all positive polynomials Q and R, there is a k0 ∈N, such that for all
k ≥k0
prob
µ½
i ∈Ik
¯¯¯ prob(A(i, x) = fi(x) : x ←Xi) >
1
Q(k)
¾¶
≤
1
R(k).
Proof. Let
pi := prob(A(i, x) = fi(x) : x ←Xi)
be the conditional probability of success of A assuming a ﬁxed i.
We ﬁrst prove that statement 2 implies statement 1. Let P be a positive
polynomial. By statement 2, there is some k0 ∈N such that for all k ≥k0
prob
µ½
i ∈Ik
¯¯¯ pi >
1
2P(k)
¾¶
≤
1
2P(k).
Hence
prob(A(i, x) = fi(x) : i ←Ik, x ←Xi)
=
X
i∈Ik
prob(i) · pi
=
X
pi≤1/(2P (k))
prob(i) · pi +
X
pi>1/(2P (k))
prob(i) · pi
≤
X
pi≤1/(2P (k))
prob(i) ·
1
2P(k) +
X
pi>1/(2P (k))
prob(i) · 1
= prob
µ½
i ∈Ik
¯¯¯ pi ≤
1
2P(k)
¾¶
·
1
2P(k)
+ prob
µ½
i ∈Ik
¯¯¯ pi >
1
2P(k)
¾¶
≤
1
2P(k) +
1
2P(k) =
1
P(k),
for k ≥k0.
Conversely, assume that statement 1 holds. Let Q and R be positive poly-
nomials. Then there is a k0 ∈N such that for all k ≥k0

154
6. One-Way Functions and the Basic Assumptions
1
Q(k)R(k) ≥prob(A(i, x) = fi(x) : i ←Ik, x ←Xi)
=
X
i∈Ik
prob(i) · pi
≥
X
pi>1/Q(k)
prob(i) · pi
>
1
Q(k) · prob
µ½
i ∈Ik
¯¯¯ pi >
1
Q(k)
¾¶
.
This inequality implies statement 2.
2
Remark. A nice feature of the discrete logarithm problem is that it is random
self-reducible. This means that solving the problem for arbitrary inputs can be
reduced to solving the problem for randomly chosen inputs. More precisely, let
(p, g) ∈Ik := {(p, g) | p a prime, |p| = k, g ∈Z∗
p a primitive root}. Assume
that there is a probabilistic polynomial algorithm A, such that
prob(A(p, g, y) = Logp,g(y) : y
u←Z∗
p) >
1
Q(k)
(6.1)
for some positive polynomial Q; i.e., A(p, g, y) correctly computes the discrete
logarithm with a non-negligible probability if the input y is randomly selected.
Since y is chosen uniformly, we may rephrase this statement: A(p, g, y) cor-
rectly computes the discrete logarithm for a polynomial fraction of inputs
y ∈Z∗
p.
Then, however, there is also a probabilistic polynomial algorithm ˜A which
correctly computes the discrete logarithm for every input y ∈Z∗
p, with an
overwhelmingly high probability. Namely, given y ∈Z∗
p, we apply a slight
modiﬁcation A1 of A. On input (p, g, y), A1 randomly selects r
u←Zp−1 and
returns
A1(p, g, y) := (A(p, g, ygr) −r) mod (p −1).
Then prob(A1(p, g, y) = Logp,g(y)) > 1/Q(k) for every y ∈Z∗
p. Now we can
apply Proposition 5.7. For every positive polynomial P, we obtain – by re-
peating the computation of A1(p, g, y) a polynomial number of times and by
checking each time whether the result is correct – a probabilistic polynomial
algorithm ˜A, with
prob( ˜A(p, g, y) = Logp,g(y)) > 1 −2−P (k),
for every y ∈Z∗
p. The existence of a random self-reduction enhances the
credibility of the discrete logarithm assumption. Namely, assume that the
discrete logarithm assumption is not true. Then by Proposition 6.3 there is
a probabilistic polynomial algorithm A, such that for inﬁnitely many k, the
inequality (6.1) holds for a polynomial fraction of keys (p, g); i.e.,

6.3 Uniform Sampling Algorithms
155
|{(p, g) ∈Ik | inequality (6.1) holds}|
|Ik|
>
1
R(k)
(with R a positive polynomial). For these keys ˜A computes the discrete log-
arithm for every y ∈Z∗
p with an overwhelmingly high probability, and the
probability of obtaining such a key is > 1/R(k) if the keys are selected uni-
formly at random.
6.3 Uniform Sampling Algorithms
In the discrete logarithm assumption 6.1, the probabilities are taken with
respect to the uniform distributions on Ik and Z∗
p. Stating the assumption
in this way, we tacitly assumed that it is possible to sample uniformly over
Ik (during key generation) and Z∗
p, by using eﬃcient algorithms. In practice
it might be diﬃcult to construct a probabilistic polynomial sampling algo-
rithm that selects the elements exactly according to the uniform distribution.
However, as in the present case of discrete logarithms (see Proposition 6.6),
we are often able to ﬁnd practical sampling algorithms which sample in a
“virtually uniform” way. Then the assumptions stated for the uniform distri-
bution, such as the discrete logarithm assumption, apply. This is shown by
the following considerations.
Deﬁnition 6.4. Let J = (Jk)k∈N be an index set with security parameter k
(see Deﬁnition 6.2). Let X = (Xj)j∈J be a family of ﬁnite sets:
1. A probabilistic polynomial algorithm SX with input j ∈J is called a
sampling algorithm for X if SX(j) outputs an element in Xj with a
probability ≥1−εk for j ∈Ik, where ε = (εk)k∈N is negligible; i.e., given
a positive polynomial Q, there is a k0 such that εk ≤1/Q(k) for k ≥k0.
2. A sampling algorithm SX for X is called (virtually) uniform if the distri-
butions of SX(j) and the uniform distributions on Xj are polynomially
close (see Deﬁnition B.22). This means that the statistical distance is
negligibly small; i.e., given a positive polynomial Q, there is a k0 such
that the statistical distance (see Deﬁnition B.19) between the distribu-
tion of SX(j) and the uniform distribution on Xj is ≤1/Q(k), for k ≥k0
and j ∈Jk.
Remark. If SX is a virtually uniform sampling algorithm for X = (Xj)j∈J, we
usually do not need to distinguish between the virtually uniform distribution
of SX(j) and the truly uniform distribution when we compute a probability
involving x ←SX(j). Namely, consider probabilities
prob(Bj(x, y) = 1 : x ←SX(j), y ←Yj,x),
where (Yj,x)x∈Xj is a family of probability spaces and Bj is a Boolean pred-
icate. Then for every positive polynomial P, there is a k0 ∈N such that

156
6. One-Way Functions and the Basic Assumptions
| prob(Bj(x, y) = 1 : x ←SX(j), y ←Yj,x)
−prob(Bj(x, y) = 1 : x
u←Xj, y ←Yj,x) | <
1
P(k),
for k ≥k0 and j ∈Jk (by Lemmas B.21 and B.24), and we see that the
diﬀerence between the probabilities is negligibly small.
Therefore, we usually do not distinguish between perfectly and virtually
uniform sampling algorithms and simply talk of uniform sampling algorithms.
We study an example. Suppose we want to construct a uniform sam-
pling algorithm S for (Zn)n∈N. We have Zn ⊆{0, 1}|n|, and could pro-
ceed as follows. We toss the coin |n| times and obtain a binary number
x := b|n|−1 . . . b1b0, with 0 ≤x < 2|n|. We can easily verify whether x ∈Zn,
by checking x < n. If the answer is aﬃrmative, we return S(n) := x. Oth-
erwise, we repeat the coin tosses. Since S is required to have a polynomial
running time, we have to stop after at most P(|n|) iterations (P a polyno-
mial). Thus, S(n) does not always succeed to return an element in Zn. The
probability of a failure is, however, negligibly small.3
Our construction, which derives a uniform sampling algorithm for a sub-
set, works, if the membership in this subset can be eﬃciently tested. It can
be applied in many situations. Therefore, we state the following lemma.
Lemma 6.5. Let J = (Jk)k∈N be an index set with security parameter k. Let
X = (Xj)j∈J and Y = (Yj)j∈J be families of ﬁnite sets with Yj ⊆Xj for all
j ∈J. Assume that there is a polynomial Q, such that |Yj| · Q(k) ≥|Xj| for
j ∈Jk.
Let SX be a uniform sampling algorithm for (Xj)j∈J which on input j ∈
J outputs x ∈Xj4 and some additional information aux(x) about x. Let
A(j, x, aux(x)) be a Monte Carlo algorithm which decides the membership in
Yj; i.e., on input j ∈J, x ∈Xj and aux(x), it yields 1 if x ∈Yj, and 0 if
x /∈Yj. Assume that the error probability of A is negligible; i.e., for every
positive polynomial P, there is a k0 such that the error probability is ≤1/P(k)
for k ≥k0.
Then there exists a uniform sampling algorithm SY for (Yj)j∈J.
Proof. Let SY be the probabilistic polynomial algorithm which on input j ∈J
repeatedly computes x := SX(j) until A(j, x, aux(x)) = 1. To get a polyno-
mial algorithm, we stop SY after at most ln(2)kQ(k) iterations.
We now show that SY has the desired properties. We ﬁrst assume that
SX(j) ∈Xj with certainty and that A has an error probability of 0. By
Lemma B.10, we may also assume that SY has found an element in Yj (before
3 We could construct a Las Vegas algorithm in this way, which always succeeds.
See Section 5.2.
4 Here and in what follows we use this formulation, though the sampling algorithm
may sometimes yield elements outside of Xj. However, as stated in Deﬁnition
6.4, this happens only with a negligible probability.

6.3 Uniform Sampling Algorithms
157
being stopped), because this event has a probability ≥1−(1−1/Q(k))kQ(k) >
1 −2−k, which is exponentially close to 1 (see the proof of Proposition 5.7
for an analogous estimate).
By construction, we have for V ⊂Yj that
prob(SY (j) ∈V ) = prob(SX(j) ∈V |SX(j) ∈Yj) = prob(SX(j) ∈V )
prob(SX(j) ∈Yj).
Thus, we have for all subsets V ⊂Yj that
prob(SY (j) ∈V ) =
|V |
|Xj| + εj(V )
|Yj|
|Xj| + εj(Yj)
,
with a negligibly small function εj. Then prob(SY (j) ∈V ) −|V |
|Yj| is also
negligibly small (you can see this immediately by Taylor’s formula for the
real function (x, y) 7→x/y). Hence, SY is a uniform sampling algorithm for
(Yj)j∈J.
The general case, where SX(j) ̸∈Xj with a negligible probability and A
has a negligible error probability, follows by applying Lemma B.10.
2
Example. Let Yn := Zn or Yn := Z∗
n. Then Yn is a subset of Xn :=
{0, 1}|n|, n ∈N. Obviously, {0, 1}|n| can be sampled uniformly by |n| coin
tosses. The membership of x in Zn is checked by x < n, and the Euclidean
algorithm tells us whether x is a unit. Thus, there are (probabilistic polyno-
mial) uniform sampling algorithms for (Zn)n∈N and (Z∗
n)n∈N, which on input
n ∈N output an element x ∈Zn (or x ∈Z∗
n).
To apply Lemma 6.5 in this example, let J := N and Jk := {n ∈N | |n| = k}.
Example. Let Primesk be the set of primes p whose binary length |p| is k;
i.e., Primesk := {p ∈Primes | |p| = k} ⊆{0, 1}k. The number of primes
< 2k is ≈2k/k ln(2) (Theorem A.68). By iterating a probabilistic primality
test (e.g. Miller-Rabin’s test, see Appendix A.8), we can, with a probability
> 1−2−k, correctly test the primality of an element x in {0, 1}k. Thus, there
is a (probabilistic polynomial) uniform sampling algorithm S which on input
1k yields a prime p ∈Primesk.
To apply Lemma 6.5 in this example, let Jk := {1k} and J := N = S
k∈N Jk,
i.e., the index set is the set of natural numbers. However, an index k ∈N is
not encoded in the standard way; it is encoded as the constant bit string 1k
(see the subsequent remark on 1k).
Remark. 1k denotes the constant bit string 11 . . . 1 of length k. Using it as
input for a polynomial algorithm means that the number of steps in the
algorithm is bounded by a polynomial in k. If we used k (encoded in the
standard way) instead of 1k as input, the bound would be a polynomial in
log2(k).

158
6. One-Way Functions and the Basic Assumptions
We return to the example of discrete exponentiation.
Proposition 6.6. Let I := {(p, g) | p prime number, g ∈Z∗
p primitive root}
and Ik := {(p, g) ∈I | |p| = k}. There is a probabilistic polynomial uniform
sampling algorithm for I = (Ik)k∈N, which on input 1k yields an element
(p, g) ∈Ik.
Proof. We want to apply Lemma 6.5 to the index set J := N = S
k∈N Jk, Jk :=
{1k}, and the families of sets Xk := Primesk ×{0, 1}k, Yk := Ik ⊆Xk(k ∈N).
The number of primitive roots in Z∗
p is ϕ(p−1), where ϕ is the Eulerian totient
function (see Theorem A.36). For x ∈N, we have
ϕ(x) = x
r
Y
i=1
µ
1 −1
pi
¶
= x
r
Y
i=1
pi −1
pi
,
where p1, . . . , pr are the primes dividing x (see Corollary A.30). Since
Qr
i=1
pi−1
pi
≥Qr+1
i=2
i−1
i
=
1
r+1 and r + 1 ≤|x|, we immediately see that
ϕ(x) · |x| ≥x.5 In particular, we have ϕ(p −1) · k ≥p −1 ≥2k−1 for
p ∈Primesk, and hence 2k · |Yk| ≥|Xk|.
Given a prime p ∈Primesk and all prime numbers q1, . . . , qr dividing
p −1, we can eﬃciently verify whether g ∈{0, 1}k is in Z∗
p and whether it is
a primitive root. Namely, we ﬁrst test g < p and then apply the criterion for
primitive roots (see Algorithm A.39), i.e., we check whether g(p−1)/q ̸= 1 for
all prime divisors q of p −1.
We may apply Lemma 6.5 if there is a probabilistic polynomial uniform
sampling algorithm for (Primesk)k∈N which not only outputs a prime p, but
also the prime factors of p −1. Bach’s algorithm (see [Bach88]) yields such
an algorithm: it generates uniformly distributed k-bit integers n, along with
their factorization. We may repeatedly generate such numbers n until n + 1
is a prime.
2
6.4 Modular Powers
Let I := {(n, e) | n = pq, p ̸= q primes, 0 < e < ϕ(n), e prime to ϕ(n)}. The
family
RSA := (RSAn,e : Z∗
n −→Z∗
n, x 7−→xe)(n,e)∈I
is called the RSA family.
Consider an (n, e) ∈I, and let d ∈Z∗
ϕ(n) be the inverse of e mod ϕ(n).
Then we have xed = xed mod ϕ(n) = x for x ∈Z∗
n, since xϕ(n) = 1 (see
Proposition A.25). This shows that RSAn,e is bijective and that the inverse
function is also an RSA function, namely RSAn,d : Z∗
n −→Z∗
n, x 7−→xd.
5 Actually, ϕ(x) is much closer to x. It can be shown that ϕ(x) >
x
6 log(|x|) (see
Appendix A.2).

6.4 Modular Powers
159
RSAn,e can be computed by modular exponentiation, an eﬃcient algo-
rithm. d can be easily computed by the extended Euclidean algorithm A.5,
if ϕ(n) = (p −1)(q −1) is known. No algorithm to compute RSA−1
n,e in poly-
nomial time is known, if p, q and d are kept secret. We call d (or p, q) the
trapdoor information for the RSA function.
All known attacks to break RSA, if implemented by an eﬃcient algorithm,
would deliver an eﬃcient algorithm for factoring n. All known factoring al-
gorithms have exponential running time. Therefore, it is widely believed that
RSA cannot be eﬃciently inverted. The following assumption makes this
more precise.
Deﬁnition 6.7. Let Ik := {(n, e) ∈I | n = pq, |p| = |q| = k}, with k ∈N,
and let Q(X) ∈Z[X] be a positive polynomial. Let A(n, e, y) be a probabilis-
tic polynomial algorithm. Then there exists a k0 ∈N, such that
prob(A(n, e, y) = RSAn,d(y) : (n, e)
u←Ik, y
u←Z∗
n) ≤
1
Q(k)
for k ≥k0.
This is called the RSA assumption.
Remarks:
1. The assumption states the one-way property of the RSA family. The algo-
rithm A models an adversary, who tries to compute x = RSAn,d(y) from
y = RSAn,e(x) = xe (in Z∗
n) without knowing the trapdoor information
d. By using Proposition 6.3, we may interpret the RSA assumption in
an analogous way to the discrete logarithm assumption (Deﬁnition 6.1).
The fraction of keys (n, e) in Ik, for which the adversary A has a signiﬁ-
cant chance to succeed, is negligibly small if the security parameter k is
suﬃciently large.
2. RSAn,e is bijective, and its range and domain coincide. Therefore, we
also speak of a family of one-way permutations (or a family of trapdoor
permutations).
3. Here and in what follows, we restrict the key set I of the RSA family and
consider only those functions RSAn,e, where n = pq is the product of two
primes of the same binary length. Instead, we could also deﬁne a stronger
version of the assumption, where Ik is the set of pairs (n, e), with |n| = k
(the primes may have diﬀerent length). However, our statement is closer
to normal practice. To generate keys with a given security parameter k,
usually two primes of length k are chosen and multiplied.
4. The RSA problem – computing x from xe – is random self-reducible (see
the analogous remark on the discrete logarithm problem on p. 154).
Stating the RSA assumption as above, we assume that the set I of keys
can be uniformly sampled by an eﬃcient algorithm.

160
6. One-Way Functions and the Basic Assumptions
Proposition 6.8. There is a probabilistic polynomial uniform sampling al-
gorithm for I = (Ik)k∈N, which on input 1k yields a key (n, e) ∈Ik along
with the trapdoor information (p, q, d).
Proof. Above (see the examples after Lemma 6.5), we saw that Primesk can
be uniformly sampled by a probabilistic polynomial algorithm. Thus, there
is a probabilistic polynomial uniform sampling algorithm for
(Xk := {n = pq | p, q distinct primes , |p| = |q| = k} × {0, 1}2k)k∈N.
In the proof of Proposition 6.6, we observed that |x| · ϕ(x) ≥x, and we
immediately conclude that
|Z∗
ϕ(n)| = ϕ(ϕ(n)) ≥ϕ(n)
|ϕ(n)| ≥
n
|n| · |ϕ(n)| ≥
n
4k2 ≥22k−2
4k2
= 22k
16k2 .
Thus, we can apply Lemma 6.5 to Yk := Ik ⊆Xk and obtain the desired
sampling algorithm. It yields (p, q, e). The inverse d of e in Z∗
ϕ(n) can be
computed using the extended Euclidean algorithm (Algorithm A.5).
2
Remarks:
1. The uniform sampling algorithm for (Ik)k∈N which we derived in the
proof of Proposition 6.8 is constructed by the method given in Lemma
6.5. Thus, it chooses triples (p, q, e) uniformly and then tests whether
e < ϕ(n) = (p −1)(q −1) and whether e is prime to ϕ(n). If this test
fails, a new triple (p, q, e) is selected. It would be more natural and more
eﬃcient to ﬁrst choose a pair (p, q) uniformly, and then, with n = pq
ﬁxed, to choose an exponent e uniformly from Z∗
ϕ(n). Then, however, the
statistical distance between the distribution of the elements (n, e) and
the uniform distribution is not negligible. The sampling algorithm is not
uniform. Note that even for ﬁxed k, there is a rather large variance of the
cardinalities |Z∗
ϕ(n)|. Nevertheless, this more natural sampling algorithm
is an admissible key generator for the RSA family; i.e., the one-way con-
dition is preserved if the keys are sampled by it (see Deﬁnition 6.13, of
admissible key generators, and Exercise 1).
An analogous remark applies to the sampling algorithm, given in the
proof of Proposition 6.6.
2. We can generate the primes p and q by uniformly selecting numbers of
length k and testing their primality by using a probabilistic prime num-
ber test (see the examples after Lemma 6.5). There are also other very
eﬃcient algorithms for the generation of uniformly distributed primes
(see, e.g., [Maurer95]).

6.5 Modular Squaring
161
6.5 Modular Squaring
Let I := {n | n = pq, p, q distinct prime numbers, |p| = |q|}. The family
Sq := (Sqn : Z∗
n −→Z∗
n, x 7−→x2)n∈I
is called the Square family.6 Sqn is neither injective nor surjective. If
Sq−1
n (x) ̸= ∅, then |Sq−1
n (x)| = 4 (see Proposition A.62).
Modular squaring can be done eﬃciently. Square roots modulo p are com-
putable by a probabilistic polynomial algorithm if p is a prime number (see Al-
gorithm A.61). Applying the Chinese Remainder Theorem (Theorem A.29),
it is then easy to derive an eﬃcient algorithm that computes square roots in
Z∗
n if n = pq (p and q are distinct prime numbers) and if the factorization of
n is known.
Conversely, given an eﬃcient algorithm for computing square roots in Z∗
n,
an eﬃcient algorithm for the factorization of n can be derived (see Proposition
A.64).
All known factoring algorithms have exponential running time. Therefore,
it is widely believed that the factors of n (or, equivalently, square roots mod-
ulo n) cannot be computed eﬃciently. We make this statement more precise
by the following assumption.
Deﬁnition 6.9. Let Ik := {n ∈I | n = pq, |p| = |q| = k}, with k ∈N,
and let Q(X) ∈Z[X] be a positive polynomial. Let A(n) be a probabilistic
polynomial algorithm. Then there exists a k0 ∈N, such that
prob(A(n) = p : n
u←Ik) ≤
1
Q(k)
for k ≥k0.
This is called the factoring assumption.
Stating the factoring assumption, we again assume that the set I of keys
may be uniformly sampled by an eﬃcient algorithm.
Proposition 6.10. There is a probabilistic polynomial uniform sampling al-
gorithm for I = (Ik)k∈N, which on input 1k yields a number n ∈Ik, along
with its factors p and q.
Proof. The algorithm chooses at random integers p and q with |p| = |q| = k,
and applies a probabilistic primality test (see Appendix A.8) to check whether
p and q are prime. By repeating the probabilistic primality test suﬃciently
6 As above in the RSA family, we only consider moduli n which are the product
of two primes of equal binary length; see the remarks after the RSA assumption
(Deﬁnition 6.7).

162
6. One-Way Functions and the Basic Assumptions
often, we can, with a probability > 1−2−k, correctly test the primality of an
element x in {0, 1}k. This sampling algorithm is uniform (Lemma 6.5).
2
Restricting the range and the domain to the set QRn of squares modulo
n (called the quadratic residues modulo n, see Deﬁnition A.48), the modu-
lar squaring function can be made bijective in many cases. Of course, each
x ∈QRn has a square root. If p and q are distinct primes with p, q ≡3 mod 4
and n := pq, then exactly one of the four square roots of x ∈QRn is an ele-
ment in QRn (see Proposition A.66). Taking as key set
I := {n | n = pq, p, q distinct prime numbers, |p| = |q|, p, q ≡3 mod 4},
we get a family
Square := (Squaren : QRn −→QRn, x 7−→x2)n∈I
of bijective functions, also called the Square family. Since the range and
domain are of the same set, we speak of a family of permutations. The family
of inverse maps is denoted by
Sqrt := (Sqrtn : QRn −→QRn)n∈I.
Sqrtn maps x to the square root of x which is an element of QRn.
The same considerations as those on Sqn : Z∗
n −→Z∗
n above show that
Squaren is eﬃciently computable, and that computing Sqrtn is equivalent
to factoring n. Square is a family of trapdoor permutations with trapdoor
information p and q.
6.6 Quadratic Residuosity Property
If p is a prime and x ∈Z∗
p, the Legendre symbol
³
x
p
´
tells us whether x is a
quadratic residue modulo p :
³
x
p
´
= 1 if x ∈QRp, and
³
x
p
´
= −1 if x /∈QRp
(Deﬁnition A.51). The Legendre symbol can be easily computed using Euler’s
criterion (Proposition A.52):
³
x
p
´
= x(p−1)/2 mod p.
Now, let p and q be distinct prime numbers and n := pq. The Jacobi
symbol
¡ x
n
¢
is deﬁned as
¡ x
n
¢
:=
³
x
p
´
·
³
x
q
´
(Deﬁnition A.55). It is eﬃciently
computable for every element x ∈Z∗
n – without knowing the prime factors p
and q of n (see Algorithm A.59). The Jacobi symbol cannot be used to decide
whether x ∈QRn. If
¡ x
n
¢
= −1, then x is not in QRn. However, if
¡ x
n
¢
= 1,
both cases x ∈QRn and x ̸∈QRn are possible. x ∈Z∗
n is a quadratic residue
if and only if both x mod p ∈Z∗
p and x mod q ∈Z∗
q are quadratic residues,
which is equivalent to
³
x
p
´
=
³
x
q
´
= 1.
Let I := {n | n = pq, p, q distinct prime numbers, |p| = |q|} and let

6.7 Formal Deﬁnition of One-Way Functions
163
J+1
n
:=
n
x ∈Z∗
n
¯¯¯
³x
n
´
= +1
o
be the elements with Jacobi symbol +1. QRn is a proper subset of J+1
n .
Consider the functions
PQRn : J+1
n
−→{0, 1}, PQRn(x) :=
½ 1 if x ∈QRn,
0 otherwise.
The family PQR := (PQRn)n∈I is called the quadratic residuosity family.
It is believed that there is no eﬃcient algorithm which, without knowing
the factors of n, is able to decide whether x ∈J+1
n
is a quadratic residue. We
make this precise in the following assumption.
Deﬁnition 6.11. Let Ik := {n ∈I | n = pq, |p| = |q| = k}, with k ∈N,
and let Q(X) ∈Z[X] be a positive polynomial. Let A(n, x) be a probabilistic
polynomial algorithm. Then there exists a k0 ∈N, such that
prob(A(n, x) = PQRn(x) : n
u←Ik, x
u←J+1
n ) ≤1
2 +
1
Q(k)
for k ≥k0.
This is called the quadratic residuosity assumption.
Remark. The assumption states that there is not a signiﬁcant chance of com-
puting the predicate PQRn if the factors of n are secret. It diﬀers a little from
the previous assumptions: the adversary algorithm A now has to compute a
predicate. Since exactly half of the elements in J+1
n
are quadratic residues (see
Proposition A.65), A can always predict the correct value with probability
1/2, simply by tossing a coin. However, her probability of success is at most
negligibly more than 1/2.
Remark. The factoring assumption follows from the RSA assumption and
also from the quadratic residuosity assumption. Hence, each of these two
assumptions is stronger than the factoring assumption.
6.7 Formal Deﬁnition of One-Way Functions
As our examples show, one-way functions actually are families of functions.
We give a formal deﬁnition of such families.
Deﬁnition 6.12. Let I = (Ik)k∈N be a key set with security parameter k.
Let K be a probabilistic polynomial sampling algorithm for I, which on input
1k outputs i ∈Ik.
A family
f = (fi : Di −→Ri)i∈I
of functions between ﬁnite sets Di and Ri is a family of one-way functions
(or, for short, a one-way function) with key generator K if and only if:

164
6. One-Way Functions and the Basic Assumptions
1. f can be computed by a Monte Carlo algorithm F(i, x).
2. There is a uniform sampling algorithm S for D := (Di)i∈I, which on
input i ∈I outputs x ∈Di.
3. f is not invertible by any eﬃcient algorithm if the keys are generated
by K. More precisely, for every positive polynomial Q ∈Z[X] and every
probabilistic polynomial algorithm A(i, y) (i ∈I, y ∈Ri), there is a
k0 ∈N, such that for all k ≥k0
prob(fi(A(i, fi(x))) = fi(x) : i ←K(1k), x
u←Di) ≤
1
Q(k).
If K is a uniform sampling algorithm for I, then we call f a family of
one-way functions (or a one-way function), without explicitly referring to a
key generator.
If fi is bijective for all i ∈I, then f is called a bijective one-way function,
and if, in addition, the domain Di coincides with the range Ri for all i, we
call f a family of one-way permutations (or simply a one-way permutation).
Examples: The examples studied earlier in this chapter are families of one-
way functions, provided Assumptions 6.1, 6.7 and 6.9 are true:
1. Discrete exponential function.
2. Modular powers.
3. Modular squaring.
Our considerations on the “random generation of the key” above (Proposi-
tions 6.6 and 6.8, 6.10) show that there are uniform key generators for these
families. There are uniform sampling algorithms S for the domains Zp−1 and
Z∗
n, as we have seen in the examples after Lemma 6.5. Squaring a uniformly
selected x ∈Z∗
n, x 7→x2, we get a uniform sampling algorithm for the do-
mains QRn of the Square family.
Modular powers is a one-way permutation, as well as the modular squaring
function Square. The discrete exponential function is a bijective one-way
function.
Remarks:
1. Selecting an index i, for example, to use fi as an encryption function, is
equivalent to choosing a public key. Recall from Deﬁnition 6.2 that for
i ∈Ik, the security parameter k is a measure of the key length in bits.
2. Condition 3 means that pre-images of y := fi(x) cannot be computed in
polynomial time if x is randomly and uniformly chosen from the domain
(all inputs have the same probability), or equivalently, if y is random
with respect to the image distribution induced by f. f is only called a
one-way function if the random and uniform choice of elements in the
domain can be accomplished by a probabilistic algorithm in polynomial
time (condition 2).
Deﬁnition 6.12 can be immediately generalized to the case of one-way

6.7 Formal Deﬁnition of One-Way Functions
165
functions, where the inputs x ∈Di are generated by any probabilistic
polynomial – not necessarily uniform – sampling algorithm S (see, e.g.,
[Goldreich01]). The distribution x
u←Di is replaced by x ←S(i).
In this book, we consider only families of one-way functions with uni-
formly distributed inputs. The keys generated by the key generator K
may be distributed in a non-uniform way.
3. The deﬁnition can be easily extended to formally deﬁne families of trap-
door functions (or, for short, trapdoor functions). We only sketch this
deﬁnition. A bijective one-way function f = (fi)i∈I is a trapdoor func-
tion if the inverse family f −1 := (f −1
i
)i∈I can be computed by a Monte
Carlo algorithm F −1(i, ti, y), which takes as inputs the (public) key i, the
(secret) trapdoor information ti for fi and a function value y := fi(x). It
is required that the key generator K generates the trapdoor information
ti along with i.
The RSA and the Square families are examples of trapdoor functions (see
above).
4. The probability of success of the adversary A in the “one-way condition”
(condition 3) is taken over the random choice of a key of a given security
parameter k. It says that over all possibly generated keys, A has on aver-
age only a small probability of success. An adversary A usually knows the
public key i when performing her attack. Thus, in a concrete attack the
probability of success is given by the conditional probability assuming a
ﬁxed i, and this conditional probability might be high even if the average
probability is negligibly small, as stated in condition 3. However, accord-
ing to Proposition 6.3, the probability of such insecure keys is negligibly
small. Thus, when randomly generating a key i by K, the probability
of obtaining one for which A has a signiﬁcant chance of succeeding is
negligibly small (see Proposition 6.3 for a precise statement).
5. Condition 2 implies, in particular, that the binary length of the elements
in Di is bounded by the running time of S(i), and hence is ≤P(|i|) if P
is a polynomial bound for the running time of S.
6. In all our examples, the one-way function can be computed using a deter-
ministic polynomial algorithm. Computable by a Monte Carlo algorithm
(see Deﬁnition 5.3) means that there is a probabilistic polynomial algo-
rithm F(i, x) with
prob(F(i, x) = fi(x)) ≥1 −2−k
(i ∈Ik)
(see Proposition 5.6 and Exercise 5 in Chapter 5).
7. Families of one-way functions, as deﬁned here, are also called collections
of strong one-way functions. They may be considered as a single one-way
function {0, 1}∗−→{0, 1}∗, deﬁned on the inﬁnite domain {0, 1}∗(see
[GolBel01]; [Goldreich01]). For the notion of weak one-way functions, see
Exercise 3.

166
6. One-Way Functions and the Basic Assumptions
The key generator of a one-way function f is not uniquely determined:
there are more suitable key generation algorithms (see Proposition 6.14 be-
low). We call them “admissible generators”.
Deﬁnition 6.13. Let f = (fi : Di −→Ri)i∈I, I = (Ik)k∈N, be a family of
one-way functions with key generator K. A probabilistic polynomial algo-
rithm ˜K that on input 1k outputs a key i ∈Ik is called an admissible key
generator for f if the one-way condition 3 of Deﬁnition 6.12 is satisﬁed for
˜K.
Proposition 6.14. Let f = (fi : Di −→Ri)i∈I, I = (Ik)k∈N, be a family of
one-way functions with key generator K. Let ˜K be a probabilistic polynomial
sampling algorithm for I, which on input 1k yields i ∈Ik. Assume that the
family of distributions i ←˜K(1k) is polynomially bounded by the family of
distributions i ←K(1k) (see Deﬁnition B.25).
Then ˜K is also an admissible key generator for f.
Proof. This is a consequence of Proposition B.26. Namely, apply Proposition
B.26 to J := N := S
k∈N Jk, Jk := {1k}, Xk := {(i, x) | i ∈Ik, x ∈Di} and
the probability distributions (i ←˜K(1k), x
u←Di) and (i ←K(1k), x
u←Di),
k ∈N. The ﬁrst family of distributions is polynomially bounded by the
second. Assume as event Ek that fi(A(i, fi(x))) = fi(x).
2
Example. Let f = (fi : Di −→Ri)i∈I be a family of one-way functions (with
uniform key generation), and let J ⊆I with |Jk| · Q(k) ≥|Ik|, for some
polynomial Q. Let ˜K be a uniform sampling algorithm for J. Then i ←˜K(1k)
is polynomially bounded by the uniform distribution i
u←Ik. Thus, ˜K is an
admissible key generator for f. This fact may be restated as:
f = (fi : Di −→Ri)i∈J is also a one-way function.
Example. As a special case of the previous example, consider the RSA one-
way function (Section 6.4). Take as keys only pairs (n, e) ∈Ik (notation as
above), with e a prime number in Z∗
ϕ(n). Since the number of primes in Zϕ(n)
is of the same order as ϕ(n)/k (by the Prime Number Theorem, Theorem
A.68), we get an admissible key generator in this way. In other words, the
classical RSA assumption (Assumption 6.7) implies an RSA assumption, with
(n, e)
u←Ik replaced by (n, e)
u←Jk, where Jk := {(n, e) ∈Ik | e prime}.
Example. As already mentioned in Section 6.4, the key generator that ﬁrst
uniformly chooses an n = pq and then, in a second step, uniformly chooses
an exponent e ∈Z∗
ϕ(n) is an admissible key generator for the RSA one-way
function. The distribution given by this generator is polynomially bounded
by the uniform distribution.
Similarly, we get an admissible key generator for the discrete exponential
function (Section 6.2) if we ﬁrst uniformly generate a prime p (together with
a factorization of p −1) and then, for this ﬁxed p, repeatedly select g
u←Z∗
p
until g happens to be a primitive root (see Exercise 1).

6.8 Hard-Core Predicates
167
6.8 Hard-Core Predicates
Given a one-way function f, it is impossible to compute a pre-image x from
y = f(x) using an eﬃcient algorithm. Nevertheless, it is often easy to derive
single bits of the pre-image x from f(x). For example, if f is the discrete
exponential function, the least-signiﬁcant bit of x is derived from f(x) in a
straightforward way (see Chapter 7). On the other hand, since f is one way
there should be other bits of the pre-image x, or more generally, properties
of x stated as Boolean predicates, which are very hard to derive from f(x).
Examples of such hard-core predicates are studied thoroughly in Chapter 7.
Deﬁnition 6.15. Let I = (Ik)k∈N be a key set with security parameter k. Let
f = (fi : Di −→Ri)i∈I be a family of one-way functions with key generator
K, and let B = (Bi : Di −→{0, 1})i∈I be a family of Boolean predicates.
B is called a family of hard-core predicates (or, for short, a hard-core predi-
cate) of f if and only if:
1. B can be computed by a Monte Carlo algorithm A1(i, x).
2. B(x) is not computable from f(x) by an eﬃcient algorithm; i.e., for
every positive polynomial Q ∈Z[X] and every probabilistic polynomial
algorithm A2(i, y), there is a k0 ∈N such that for all k ≥k0
prob(A2(i, fi(x)) = Bi(x) : i ←K(1k), x
u←Di) ≤1
2 +
1
Q(k).
Remarks:
1. As above in the one-way conditions, we do not consider a single ﬁxed
key in the probability statement of the deﬁnition. The probability is
also taken over the random generation of a key i, with a given security
parameter k. Hence, the meaning of statement 2 is: choosing both the key
i with security parameter k and x ∈Di randomly, the adversary A2 does
not have a signiﬁcantly better chance of ﬁnding out the bit Bi(x) from
fi(x) than by simply tossing a coin, provided the security parameter is
suﬃciently large. In practice, the public key is known to the adversary,
and we are interested in the conditional probability of success assuming
a ﬁxed public key i. Even if the security parameter k is very large, there
may be keys i, such that A2 has a signiﬁcantly better chance than 1/2
of determining Bi(x). The probability of such insecure keys is, however,
negligibly small. This is shown in Proposition 6.17.
2. Considering a family of one-way functions with hard-core predicate B,
we call a key generator K admissible only if it guarantees the “hard-core”
condition 2 of Deﬁnition 6.15, in addition to the one-way condition 3 of
Deﬁnition 6.12. Proposition 6.14 remains valid for one-way functions with
hard-core predicates. This immediately follows from Proposition 6.17.
The inner product bit yields a generic hard-core predicate for all one-way
functions.

168
6. One-Way Functions and the Basic Assumptions
Theorem 6.16. Let f = (fi : Di −→Ri)i∈I be a family of one-way func-
tions, Di ⊂{0, 1}∗for all i ∈I. Extend the functions fi to functions ˜fi
which on input x ∈Di and y ∈{0, 1}|x| return the concatenation fi(x)||y. Let
Bi(x, y) :=


l
X
j=1
xj · yj

mod 2,
where l = |x|, x = x1 . . . xl, y = y1 . . . yl, xj, yj ∈{0, 1},
be the inner product modulo 2. Then B := (Bi)i∈I is a hard-core predicate of
˜f := ( ˜fi)i∈I.
For a proof, see [GolLev89], [Goldreich99] or [Luby96].
Proposition 6.17. Let I = (Ik)k∈N be a key set with security parameter k.
Let f = (fi : Xi −→Yi)i∈I be a family of functions between ﬁnite sets, and
let B = (Bi : Xi −→{0, 1})i∈I be a family of Boolean predicates. Assume
that both f and B can be computed by a Monte Carlo algorithm, with inputs
i ∈I and x ∈Xi. Let probability distributions be given on Ik and Xi for
all k and i. Assume there is a probabilistic polynomial sampling algorithm S
for X := (Xi)i∈I which, on input i ∈I, randomly chooses an x ∈Xi with
respect to the given distribution on Xi, i.e., prob(S(i) = x) = prob(x). Then
the following statements are equivalent:
1. For every probabilistic polynomial algorithm A with inputs i ∈I and
y ∈Yi and output in {0, 1}, and every positive polynomial P, there is a
k0 ∈N, such that for all k ≥k0
prob(A(i, fi(x)) = Bi(x) : i ←Ik, x ←Xi) ≤1
2 +
1
P(k).
2. For every probabilistic polynomial algorithm A with inputs i ∈I and
x ∈Xi and output in {0, 1}, and all positive polynomials Q and R, there
is a k0 ∈N, such that for all k ≥k0
prob
µ½
i ∈Ik
¯¯¯ prob(A(i, fi(x)) = Bi(x) : x ←Xi) > 1
2 +
1
Q(k)
¾¶
≤
1
R(k).
Statement 2 implies statement 1, even if we omit “for every algorithm” in both
statements and instead consider a ﬁxed probabilistic polynomial algorithm A.

6.8 Hard-Core Predicates
169
Proof. An analogous computation as in the proof of Proposition 6.3 shows
that statement 2 implies statement 1.
Now, assume that statement 1 holds, and let A(i, y) be a probabilistic
polynomial algorithm with output in {0, 1}. Let Q and R be positive polyno-
mials. We abbreviate the probability of success of A, conditional on a ﬁxed
i, by pi:
pi := prob(A(i, fi(x)) = Bi(x) : x ←Xi).
Assume that for k in an inﬁnite subset K ⊆N,
prob
µ½
i ∈Ik
¯¯¯ pi > 1
2 +
1
Q(k)
¾¶
>
1
R(k).
(6.2)
If all pi, i ∈Ik, were ≥1/2, we could easily conclude that their average also
signiﬁcantly exceeds 1/2 and obtain a contradiction to statement 1. Unfor-
tunately, it might happen that many of the probabilities pi are signiﬁcantly
larger than 1/2 and many are smaller than 1/2, whereas their average is close
to 1/2. The basic idea now is to modify A and replace A by an algorithm ˜A.
We want to replace the output A(i, y) by its complementary value 1−A(i, y)
if pi < 1/2. In this way we could force all the probabilities to be ≥1/2. At
this point we face another problem. We want to deﬁne ˜A as
(i, y) 7→
½
A(i, y)
if pi ≥1/2,
1 −A(i, y) if pi < 1/2,
and see that we have to determine by a polynomial algorithm whether pi ≥
1/2. At least we can compute the correct answer to this question with a high
probability in polynomial time. By Proposition 6.18, there is a probabilistic
polynomial algorithm C(i), such that
prob
µ
|C(i) −pi| <
1
4Q(k)R(k)
¶
≥1 −
1
4Q(k)R(k).
We deﬁne that Sign(i) := +1 if C(i) ≥1/2, and Sign(i) := −1 if C(i) <
1/2. Then Sign computes the sign σi ∈{+1, −1} of (pi −1/2) with a high
probability if the distance of pi from 1/2 is not too small:
prob(Sign(i) = σi) ≥1 −
1
4Q(k)R(k) for all i with
¯¯¯¯pi −1
2
¯¯¯¯ ≥
1
4Q(k)R(k).
Now the modiﬁed algorithm ˜A = ˜A(i, y) is deﬁned as follows. Let
˜A(i, y) :=
½
A(i, y)
if Sign(i) = +1,
1 −A(i, y) if Sign(i) = −1,
for i ∈I and y ∈Yi. Similarly as before, we write
˜pi := prob( ˜A(i, fi(x)) = Bi(x) : x ←Xi)

170
6. One-Way Functions and the Basic Assumptions
for short. By the deﬁnition of ˜A, ˜pi = pi if Sign(i) = +1, and ˜pi = 1 −pi
if Sign(i) = −1. Hence |pi −1/2| = |˜pi −1/2|. Moreover, we have ˜pi ≥1/2,
if Sign(i) = σi, and prob(Sign(i) ̸= σi) ≤1/(4Q(k)R(k)) if |pi −1/2| ≥
1/(4Q(k)R(k)).
Let
I1,k :=
½
i ∈Ik
¯¯¯ pi −1
2 >
1
Q(k)
¾
,
I2,k :=
½
i ∈Ik
¯¯¯
¯¯¯¯pi −1
2
¯¯¯¯ >
1
4Q(k)R(k)
¾
.
We compute
prob( ˜A(i, fi(x)) = Bi(x) : i ←Ik, x ←Xi) −1
2
=
X
i∈Ik
prob(i) ·
µ
˜pi −1
2
¶
=
X
i∈I2,k
prob(i) ·
µ
˜pi −1
2
¶
+
X
i̸∈I2,k
prob(i) ·
µ
˜pi −1
2
¶
=: (1).
For i /∈I2,k, we have |˜pi −1/2| = |pi −1/2| ≤1/(4Q(k)R(k)) and hence
˜pi −1/2 ≥−1/(4Q(k)R(k)). Thus
(1) ≥
X
i∈I2,k
prob(i) ·
µ
˜pi −1
2
¶
−
1
4Q(k)R(k)
=
X
i∈I2,k
prob(Sign(i) = σi) · prob(i) ·
µ
˜pi −1
2
¶
+
X
i∈I2,k
prob(Sign(i) ̸= σi) · prob(i) ·
µ
˜pi −1
2
¶
−
1
4Q(k)R(k)
=: (2).
We observed before that ˜pi ≥1/2 if Sign(i) = σi, and prob(Sign(i) ̸= σi) ≤
1/(4Q(k)R(k)) for i ∈I2,k. Moreover, we obviously have I1,k ⊆I2,k, and our
assumption (6.2) means that
prob(I1,k) =
X
i∈I1,k
prob(i) >
1
R(k),
for the inﬁnitely many k ∈K. Therefore, we may continue our computation
for k ∈K in the following way:

6.8 Hard-Core Predicates
171
(2) ≥
µ
1 −
1
4Q(k)R(k)
¶ X
i∈I2,k
prob(i) ·
¯¯¯¯pi −1
2
¯¯¯¯ −
1
4Q(k)R(k) −
1
4Q(k)R(k)
≥
µ
1 −
1
4Q(k)R(k)
¶ X
i∈I1,k
prob(i) ·
µ
pi −1
2
¶
−
1
4Q(k)R(k) −
1
4Q(k)R(k)
≥
µ
1 −
1
4Q(k)R(k)
¶
·
1
R(k) ·
1
Q(k) −
1
4Q(k)R(k) −
1
4Q(k)R(k)
=
1
2Q(k)R(k) −
1
4Q2(k)R2(k)
≥
1
4Q(k)R(k).
Since K is inﬁnite, we obtain a contradiction to statement 1 applied to ˜A and
P = 4QR, and the proof that statement 1 implies statement 2 is ﬁnished. 2
As in the preceding proof, we sometimes want to compute probabilities
with a probabilistic polynomial algorithm, at least approximately.
Proposition 6.18. Let B(i, x) be a probabilistic polynomial algorithm that
on input i ∈I and x ∈Xi outputs a bit b ∈{0, 1}. Assume that a probability
distribution is given on Xi, for every i ∈I. Further assume that there is a
probabilistic polynomial sampling algorithm S which on input i ∈I randomly
chooses an element x ∈Xi with respect to the distribution given on Xi, i.e.,
prob(S(i) = x) = prob(x). Let P and Q be positive polynomials.
Let pi := prob(B(i, x) = 1 : x ←Xi) be the probability of B(i, x) = 1,
assuming that i is ﬁxed. Then there is a probabilistic polynomial algorithm A
that approximates the probabilities pi, i ∈I, with high probability; i.e.,
prob
µ
|A(i) −pi| <
1
P(|i|)
¶
≥1 −
1
Q(|i|).
Proof. We ﬁrst observe that prob(B(i, S(i)) = 1) = pi for every i ∈I, since
S(i) samples (by use of its coin tosses) according to the distribution given on
Xi. We deﬁne the algorithm A on input i as follows:
1. Let t be the smallest n ∈N with n ≥1/4 · P(|i|)2 · Q(|i|).
2. Compute B(i, S(i)) t times, and obtain the results
b1, . . . , bt ∈{0, 1}.
3. Let
A(i) := 1
t
t
X
i=1
bi.
Applying Corollary B.17 to the t independent computations of the random
variable B(i, S(i)), we get
prob
µ
|A(i) −pi| <
1
P(|i|)
¶
≥1 −P(|i|)2
4t
≥1 −
1
Q(|i|),
as desired.
2

172
6. One-Way Functions and the Basic Assumptions
Exercises
1. Let S be the key generator for
a. the RSA family, which on input 1k ﬁrst generates random prime
numbers p and q of binary length k, and then repeatedly generates
random exponents e ∈Zϕ(n), n := pq, until it ﬁnds an e prime to
ϕ(n).
b. the Exp family, which on input 1k ﬁrst generates a random prime p
of binary length k, together with the prime factors of p−1, and then
randomly chooses elements g ∈Z∗
p, until it ﬁnds a primitive root g
(see the proof of Proposition 6.6).
Show that S is an admissible key generator for the RSA family and the
Exp family (see the remark after Proposition 6.8).
2. Compare the running times of the uniform key generator for RSA keys
constructed in Proposition 6.8 and the (more eﬃcient) admissible key
generator S in Exercise 1.
3. Consider the Deﬁnition 6.12 of “strong” one-way functions. If we replace
the probability statement in condition 3 by “There is a positive polyno-
mial Q, such that for all probabilistic polynomial algorithms A
prob(fi(A(i, fi(x))) = fi(x) : i ←K(1k), x
u←Di) ≤1 −
1
Q(k),
for suﬃciently large k”, then f = (fi)i∈I is called a family of weak one-
way functions.7
Let Xj := {2, 3, . . . , 2j −1} be the set of numbers > 1 of binary length
≤j. Let Dn := Sn−2
j=2 Xj × Xn−j (disjoint union), and let Rn := Xn.
Show that
f := (fn : Dn −→Rn, (x, y) 7−→x · y)n∈N
is a weak – not a strong – one-way function, if the factoring assumption
(Deﬁnition 6.9) is true.
4. We consider the representation problem (Section 4.5.3).
Let A(p, q, g1, . . . , gr) be a probabilistic polynomial algorithm (r ≥2).
The inputs are primes p and q, such that q divides p −1, and ele-
ments g1, . . . , gr ∈Z∗
p of order q. A outputs two tuples (a1, . . . , ar) and
(a′
1, . . . , a′
r) of integer exponents. Assume that r is polynomially bounded
by the binary length of p, i.e., r ≤T(|p|) for some polynomial T. We de-
note by Gq the subgroup of order q of Z∗
p. Recall that Gq is cyclic and
each g ∈Gq, g ̸= 1, is a generator (Lemma A.40). Let P be a positive
polynomial, and let K be the set of pairs (p, q), such that
7 A weak one-way function f yields a strong one-way function by the following
construction: (x1, . . . , x2kQ(k)) 7→(fi(x1), . . . , fi(x2kQ(k))) (where i ∈Ik). See
[Luby96] for a proof.

Exercises
173
prob
¡
A(p, q, g1, . . . , gr) = ((a1, . . . , ar), (a′
1, . . . , a′
r)),
(a1, . . . , ar) ̸= (a′
1, . . . , a′
r),
r
Y
j=1
gaj
j
=
r
Y
j=1
g
a′
j
j
:
gj
u←Gq \ {1}, 1 ≤j ≤r
¢
≥1/P(|p|).
Show that for every positive polynomial Q there is a probabilistic poly-
nomial algorithm ˜A = ˜A(p, q, g, y), such that
prob( ˜A(p, q, g, y) = Logp,g(y)) ≥1 −2−Q(|p|),
for all (p, q) ∈K, g ∈Gq \ {1} and y ∈Gq.
5. Let Jk := {n ∈N | n = pq, p, q distinct primes, |p| = |q| = k} be the
set of RSA moduli with security parameter k. For a prime ˜p, we denote
by Jk,˜p := {n ∈Jk | ˜p does not divide ϕ(n)} the set of those moduli for
which ˜p may serve as an RSA exponent. Let Primes≤2k be the primes of
binary length ≤2k.
Show that the RSA assumption remains valid if we ﬁrst choose a prime
exponent and then a suitable modulus. More precisely, show that the
classical RSA assumption (Deﬁnition 6.7) implies that:
For every probabilistic polynomial algorithm A and every positive poly-
nomial P there is a k0 ∈N, such that for all k ≥k0
prob(A(n, ˜p, x˜p) = x | ˜p
u←Primes≤2k, n
u←Jk,˜p, x
u←Z∗
n) ≤
1
P(k).
6. Let f = (fi : Di −→Ri)i∈I, I = (Ik)k∈N, be a family of one-way func-
tions with key generator K, and let B = (Bi : Di −→{0, 1})i∈I be a
hard-core predicate for f.
Show that for every positive polynomial P, there is a k0 ∈N such that
for all k ≥k0
¯¯¯¯ prob(Bi(x) = 0 : i ←K(1k), x
u←Di) −1
2
¯¯¯¯ ≤
1
P(k).
7. Let f = (fi : Di −→Ri)i∈I, I = (Ik)k∈N, be a family of one-way func-
tions with key generator K, and let B = (Bi : Di −→{0, 1})i∈I be a
family of predicates which is computable by a Monte Carlo algorithm.
Show that B is a hard-core predicate of f if and only if for every proba-
bilistic polynomial algorithm A(i, x, y) and every positive polynomial P
there is a k0, such that for all k ≥k0
| prob(A(i, fi(x), Bi(x)) = 1 : i ←K(1k), x
u←Di)
−prob(A(i, fi(x), z) = 1 : i ←K(1k), x
u←Di, z
u←{0, 1}) | ≤
1
P(k).

174
6. One-Way Functions and the Basic Assumptions
If the functions fi are bijective, then the latter statement means that
the family ({(fi(x), Bi(x)) : x
u←Di})i∈I of distributions cannot be
distinguished from the uniform distributions on (Ri × {0, 1})i∈I by a
probabilistic polynomial algorithm.
8. Let I = (Ik)k∈N be a key set with security parameter k. Consider prob-
abilistic polynomial algorithms A which on input i ∈I and x ∈Xi com-
pute a Boolean value A(i, x) ∈{0, 1}. Assume that one family of probabil-
ity distributions is given on (Ik)k∈N, and two families of probability distri-
butions p := (pi)i∈I and q := (qi)i∈I are given on X := (Xi)i∈I. Further
assume that there are probabilistic polynomial sampling algorithms S1(i)
and S2(i) which randomly choose an x ∈Xi with prob(S1(i) = x) = pi(x)
and prob(S2(i) = x) = qi(x), for all i ∈I and x ∈Xi.
Prove a result that is analogous to the statements of Propositions 6.3
and 6.17 for
| prob(A(i, x) = 1 : i ←Ik, x
pi
←Xi)
−prob(A(i, x) = 1 : i ←Ik, x
qi←Xi) | ≤
1
P(k).
9. Let n := pq, with distinct primes p and q. The quadratic residuosity
assumption (Deﬁnition 6.11) states that it is infeasible to decide whether
a given x ∈J+1
n
is a quadratic residue or not. If p and q are kept secret,
no eﬃcient algorithm for selecting a quadratic non-residue modulo n
is known. Thus, it might be easier to decide quadratic residuosity if,
additionally, a random quadratic non-residue with Jacobi symbol 1 is
revealed. In [GolMic84] (Section 6.2) it is shown that this is not the case.
More precisely, let Ik := {n | n = pq, p, q distinct primes , |p| = |q| = k}
and I := (Ik)k∈N. Let QNRn := Z∗
n \ QRn be the set of quadratic non-
residues (see Deﬁnition A.48) and QNR+1
n
:= QNRn ∩J+1
n . Then the
following statements are equivalent:
a. For all probabilistic polynomial algorithms A, with inputs n ∈I and
x ∈J+1
n
and output in {0, 1}, and every positive polynomial P, there
exists a k0 ∈N such that for all k ≥k0
prob(A(n, x) = PQRn(x) : n
u←Ik, x
u←J+1
n ) ≤1
2 +
1
P(k).
b. For all probabilistic polynomial algorithms A, with inputs n ∈I and
z, x ∈J+1
n
and output in {0, 1}, and every positive polynomial P,
there exists a k0 ∈N such that for all k ≥k0
prob(A(n, z, x) = PQRn(x) : n
u←Ik, z
u←QNR+1
n , x
u←J+1
n )
≤1
2 +
1
P(k).
Study [GolMic84] and give a proof.

7. Bit Security of One-Way Functions
Let f : X −→Y be a bijective one-way function and let x ∈X. Sometimes
it is possible to compute some bits of x from f(x) without inverting f. A
function f does not necessarily hide everything about x, even if f is one way.
Let b be a bit of x. We call b a secure bit of f if it is as diﬃcult to compute b
from f(x) as it is to compute x from f(x). We prove that the most-signiﬁcant
bit of x is a secure bit of Exp, and that the least-signiﬁcant bit is a secure
bit of RSA and Square.
We show how to compute x from Exp(x), assuming that we can compute
the most-signiﬁcant bit of x from Exp(x). Then we show the same for the
least-signiﬁcant bit and RSA or Square. First we assume that deterministic
algorithms are used to compute the most- or least-signiﬁcant bit. In this much
easier case, we demonstrate the basic ideas. Then we study the probabilistic
case. We assume that we can compute the most- or least-signiﬁcant bit with
a probability p ≥1/2 + 1/P(|x|), for some positive polynomial P, and derive
that then x can be computed with an overwhelmingly high probability.
As a consequence, the discrete logarithm assumption implies that the
most-signiﬁcant bit is a hard-core predicate for the Exp family. Given the
RSA or the factoring assumption, the least-signiﬁcant bit yields a hard-core
predicate for the RSA or the Square family.
Bit security is not only of theoretical interest. Bleichenbacher’s 1-Million-
Chosen-Ciphertext Attack against PKCS#1-based schemes shows that a
leaking secure bit can lead to dangerous practical attacks (see Section 3.3.3).
Let n, r ∈N such that 2r−1 ≤n < 2r. As usual, the binary encoding of
x ∈Zn is the binary encoding in {0, 1}r of the representative of x between
0 and n −1 as an unsigned number (see Appendix A.2). This deﬁnes an
embedding Zn ⊂{0, 1}r. Bits of x and properties of x that depend on a
representative of x are deﬁned relative to this embedding. The property “x
is even”, for example, is such a property.
7.1 Bit Security of the Exp Family
Let p be an odd prime number and let g be a primitive root in Z∗
p. We consider
the discrete exponential function

176
7. Bit Security of One-Way Functions
Expp,g : Zp−1 −→Z∗
p, x 7−→gx,
and its inverse
Logp,g : Z∗
p −→Zp−1,
which is the discrete logarithm function.
Logp,g(x) is even if and only if x is a square (Lemma A.49). The square
property modulo a prime can be computed eﬃciently by Euler’s criterion
(Proposition A.52). Hence, we have an eﬃcient algorithm that computes the
least-signiﬁcant bit of x from Expp,g(x). The most-signiﬁcant bit, however,
is as diﬃcult to compute as the discrete logarithm.
Deﬁnition 7.1. Let p be an odd prime, and let g be a primitive root in Z∗
p:
1. The predicate Msbp is deﬁned by
Msbp : Zp−1 −→{0, 1}, x 7−→
½0 if 0 ≤x < p−1
2
,
1 if p−1
2
≤x < p −1.
2. The predicate Bp,g is deﬁned by
Bp,g : Z∗
p −→{0, 1}, Bp,g(x) = Msbp(Logp,g(x)).
Remark. If p−1 is a power of 2, then the predicate Msbp is the most-signiﬁcant
bit of the binary encoding of x.
Let x ∈QRp, x ̸= 1. There is a probabilistic polynomial algorithm which
computes the square roots of x (Algorithm A.61). Let y := yn . . . y0, yi ∈
{0, 1}, be the binary representation of y := Logp,g(x). As we observed before,
y0 = 0. Therefore, w1 := g˜y with ˜y := yn . . . y1 is a root of x with Bp,g(w1) =
0. The other root w2 is g˜y+(p−1)/2, and obviously Bp,g(w2) = 1. Thus, exactly
one of the two square roots w satisﬁes the condition Bp,g(w) = 0.
Deﬁnition 7.2. Let x ∈QRp. The square root w of x which satisﬁes the
condition Bp,g(w) = 0 is called the principal square root of x. The map
PSqrtp,g is deﬁned by
PSqrtp,g : QRp −→Z∗
p, x 7−→principal square root of x.
Remark. The ability to compute the principal square root in polynomial time
is equivalent to the ability to compute the predicate Bp,g in polynomial time.
Namely, let A be a deterministic polynomial algorithm for the computation
of PSqrtp,g. Then the algorithm
Algorithm 7.3.
int B(int p, g, x)
1
if A(p, g, x2) = x
2
then return 0
3
else return 1

7.1 Bit Security of the Exp Family
177
computes Bp,g and is deterministic polynomial.
Conversely, let B be a deterministic polynomial algorithm which computes
Bp,g. If Sqrt is a polynomial algorithm for the computation of square roots
modulo p, then the polynomial algorithm
Algorithm 7.4.
int A(int p, g, x)
1
{u, v} ←Sqrt(x, p)
2
if B(p, g, u) = 0
3
then return u
4
else return v
computes PSqrtp,g. It is deterministic if Sqrt is deterministic. We have a poly-
nomial algorithm Sqrt (Algorithm A.61). It is deterministic for p ≡3 mod 4.
In the other case, the only non-deterministic step is to ﬁnd a quadratic non-
residue modulo p, which is an easy task. If we select t numbers in {1, p −1}
at random, then the probability of ﬁnding a quadratic non-residue is 1−1/2t.
Thus, the probability of success of A can be made almost 1, independent of
the size of the input x.
In the following proposition and theorem, we show how to reduce the com-
putation of the discrete logarithm function to the computation of PSqrtp,g.
Given an algorithm A1 for computing PSqrtp,g, we develop an algorithm A2
for the discrete logarithm which calls A1 as a subroutine.1 The resulting algo-
rithm A2 has the same complexity as A1. Therefore, PSqrtp,g is also believed
to be not eﬃciently computable.
Proposition 7.5. Let A1 be a deterministic polynomial algorithm, such that
A1(p, g, x) = PSqrtp,g(x) for all x ∈QRp,
with p an odd prime and g a primitive root in Z∗
p. Then there is a deterministic
polynomial algorithm A2, such that
A2(p, g, x) = Logp,g(x) for all x ∈QRp.
Proof. The basic idea of the reduction is the following:
1. Let x = gy ∈Z∗
p and y = yk . . . y0, yi ∈{0, 1}, i = 0, . . . , k, be the binary
encoding of y. We compute the bits of y from right to left. Bit y0 is 0 if
and only if x ∈QRp (Lemma A.49). This condition can be tested by use
of Euler’s criterion for quadratic residuosity (Proposition A.52).
2. To get the next bit y1, we replace x by xg−1 = gyk...y10 if y0 = 1. Then
bit y1 can be obtained from PSqrtp,g(x) = gyk...y1 as in step 1.
The following algorithm A2, which calls A1 as a subroutine, computes Logp,g.
1 In our construction we use A1 as an “oracle” for PSqrtp,g. Therefore, algorithms
such as A1 are sometimes called “oracle algorithms”.

178
7. Bit Security of One-Way Functions
Algorithm 7.6.
int A2(int p, g, x)
1
y ←empty word, k ←|p|
2
for c ←0 to k −1 do
3
if x ∈QRp
4
then y ←y||0
5
else y ←y||1
6
x ←xg−1
7
x ←A1(p, g, x)
8
return y
This completes the proof.
2
Theorem 7.7. Let P, Q ∈Z[X] be positive polynomials and A1 be a proba-
bilistic polynomial algorithm, such that
prob(A1(p, g, x) = PSqrtp,g(x) : x
u←QRn) ≥1
2 +
1
P(k) ,
where p is an odd prime number, g is a primitive root in Z∗
p and k = |p| is
the binary length of p. Then there is a probabilistic polynomial algorithm A2,
such that for every x ∈Z∗
p,
prob(A2(p, g, x) = Logp,g(x)) ≥1 −2−Q(k).
Proof. Let ε := 1/P(k). In order to reduce Logp,g to PSqrtp,g, we intend to
proceed as in the deterministic case (Proposition 7.5). There, A1 is applied
k times by A2. A2 correctly yields the desired logarithm if PSqrt is correctly
computed by A1 in each step. Now the algorithm A1 computes the function
PSqrtp,g with a probability of success of only ≥1/2+ε. Thus, the probability
of success of A2 is ≥(1/2 + ε)k. This value is exponentially close to 0, and
hence is too small.
The basic idea now is to replace A1 by an algorithm B which computes
PSqrtp,g(x) with a high probability of success, for a polynomial fraction of
inputs.
Lemma 7.8. Under the assumptions of the theorem, let t := kε−2. Then
there is a probabilistic polynomial algorithm B, such that
prob(B(x) = PSqrtp,g(x)) ≥1 −1
k , for x = g2s, 0 ≤s ≤p −1
2t
.
Proof (of the Lemma). Let x = g2s, 0 ≤s ≤(p −1)/2t. In our algorithm B we
want to increase the probability of success on input x by computing A1(x)
repeatedly. By assumption, we have
prob(A1(p, g, x) = PSqrtp,g(x) : x
u←QRn) ≥1
2 + ε.

7.1 Bit Security of the Exp Family
179
Here the probability is also taken over the random choice of x ∈QRn. There-
fore, we must modify x randomly each time we apply A1. For this pur-
pose, we iteratively select r ∈
n
0, . . . , (p−1)
2
−1
o
at random and compute
A1(p, g, xg2r). If A1(p, g, xg2r) successfully computes PSqrtp,g(xg2r), then
PSqrtp,g(x) = A1(p, g, xg2r) · g−r, at least if s + r < (p −1)/2. The latter
happens with a high probability, since s is assumed to be small. Since the
points xg2r are sampled randomly and independently, we can then compute
the principal square root PSqrtp,g(x) with a high probability, using a ma-
jority decision on the values A1(p, g, xg2r) · g−r. The probability of success
increases linearly with the number of sampled points, and it can be computed
by Corollary B.18, which is a consequence of the weak law of large numbers.
Algorithm 7.9.
int B(int x)
1
C0 ←0, C1 ←0
2
{u, v} ←Sqrt(x)
3
for i ←1 to t do
4
select r ∈
©
0, . . . , p−1
2
−1
ª
at random
5
if A1(p, g, xg2r) = ugr
6
then C0 ←C0 + 1
7
else C1 ←C1 + 1
8
if C0 > C1
9
then return u
10
else return v
We now show that
prob(B(x) = PSqrtp,g(x)) ≥1 −1
k , for x = g2s, 0 ≤s ≤p −1
2t
.
Let ri be the randomly selected elements r and xi = xg2ri, i = 1, . . . , t. Every
element z ∈QRp has a unique representation z = xg2r, 0 ≤r < (p −1)/2.
Therefore, the uniform and random choice of ri implies the uniform and
random choice of xi. Let x = g2s, 0 ≤s ≤(p −1)/2t, and r < (t −1)(p −1)/2t.
Then
2s + 2r < p −1
t
+ (t −1)(p −1)
t
= p −1 ,
and hence
PSqrtp,g(x) · gr = PSqrtp,g(xg2r) .
Let E1 be the event r < (t −1)(p −1)/2t and E2 be the event A1(p, g, xg2r) =
PSqrtp,g(xg2r). We have prob(E1) ≥1 −1/t and prob(E2) ≥1/2 + ε, and we
correctly compute PSqrtp,g(x) · gr, if both events E1 and E2 occur. Thus, we
have (denoting by E2 the complement of event E2)
prob(A1(p, g, xg2r) = PSqrtp,g(x) · gr)

180
7. Bit Security of One-Way Functions
≥prob(E1 and E2) ≥prob(E1) −prob(E2)
≥1 −1
t −
µ1
2 −ε
¶
= 1
2 + ε −1
k ε2 ≥1
2 + 1
2ε .
In each of the t iterations of B, PSqrtp,g(x) · gr is computed with prob-
ability > 1/2. Taking the most frequent result, we get PSqrtp,g(x) with a
very high probability. More precisely, we can apply Corollary B.18 to the
independent random variables Si, i = 1, . . . , t, deﬁned by
Si =
½ 1 if A1(p, g, xg2ri) = PSqrtp,g(x) · gri,
0 otherwise.
We have E(Si) = prob(Si = 1) ≥1/2 + ε.
If u = PSqrtp,g(x), then we conclude by Corollary B.18 that
prob(B(x) = PSqrtp,g(x)) = prob
µ
C0 > t
2
¶
≥1 −
4
4tε2 = 1 −1
k .
The case v = PSqrtp,g(x) follows analogously. This completes the proof of
the lemma.
2
We continue with the proof of the theorem. The following algorithm com-
putes the discrete logarithm.
Algorithm 7.10.
int A(int p, g, x)
1
y ←empty word, k ←|p|, t ←kε−2
2
guess j ∈{0, . . . , t −1} satisfying j p−1
t
≤Logp,g(x) < (j + 1) p−1
t
3
x = xg−⌈j(p−1)/t⌉
4
for c ←1 to k do
5
if x ∈QRp
6
then y ←y||0
7
else y ←y||1
8
x ←xg−1
9
x ←B(x)
10
return y +
l
j(p−1)
t
m
In algorithm A we use “to guess” as a basic operation. To guess the right
alternative means to ﬁnd out the right choice by computation.
Here, to guess the correct j means to carry out lines 3–9 and then test
whether y +
l
j(p−1)
t
m
is equal to Logp,g(x), for j = 0, 1, . . .. The test is done
by modular exponentiation. We stop if the test succeeds, i.e., if Logp,g(x) is
computed. We have to consider at most t = kε−2 = kP 2(k) many intervals.
Hence, in this way we get a polynomial algorithm. This notion of “to guess”
will also be used in the subsequent sections.

7.1 Bit Security of the Exp Family
181
We have A(p, g, x) = Logp,g(x) if B correctly computes PSqrtp,g(x) in
each iteration of the for loop. Thus, we get
prob(A(p, g, x) = Logp,g(x)) ≥
µ
1 −1
k
¶k
.
As (1 −1/k)k increases monotonously (it converges to e−1), this implies
prob(A(p, g, x) = Logp,g(x)) ≥
µ
1 −1
2
¶2
= 1
4.
By Proposition 5.7, we get, by repeating the computation A(p, g, x) indepen-
dently, a probabilistic polynomial algorithm A2(p, g, x) with
prob(A2(p, g, x) = Logp,g(x)) > 1 −2−Q(k).
This concludes the proof of the theorem.
2
Remarks:
1. The expectation is that A will compute Logp,g(x) after four repetitions
(see Lemma B.12). Thus, we expect that we have to call A1 at most
4k3ε−4 times to compute Logp,g(x).
2. In [BluMic84], Blum and Micali introduced the idea to reduce the discrete
logarithm problem to the problem of computing principal square roots.
They developed the techniques we used to prove Theorem 7.7. In that
paper they also constructed cryptographically strong pseudo-random bit
generators using hard-core bits. They proved that the most-signiﬁcant bit
of the discrete logarithm is unpredictable and achieved as an application
the discrete exponential generator (see Section 8.1).
Corollary 7.11. Let I = {(p, g) | p prime, g ∈Z∗
p a primitive root}.
Provided the discrete logarithm assumption is true,
Msb :=
µ
Msbp : Zp−1 −→{0, 1}, x 7−→
½
0 if 0 ≤x < p−1
2 ,
1 if p−1
2
≤x < p −1
¶
(p,g)∈I
is a family of hard-core predicates for the Exp family
Exp = (Expp,g : Zp−1 −→Z∗
p, x 7−→gx mod p)(p,g)∈I .
Proof. Assume Msb is not a family of hard-core predicates for Exp. Then,
there is a positive polynomial P ∈Z[X] and an algorithm A1, such that for
inﬁnitely many k
prob(A1(p, g, gx) = Msbp(x) : (p, g) ←Ik, x
u←Zp−1) > 1
2 +
1
P(k) .
By Proposition 6.17, there are positive polynomials Q, R, such that

182
7. Bit Security of One-Way Functions
prob
µ½
(p, g) ∈Ik
¯¯¯ prob(A1(p, g, gx) = Msbp(x) : x
u←Zp−1)
> 1
2 +
1
Q(k)
¾¶
>
1
R(k) ,
for inﬁnitely many k. From the theorem and the remark after Deﬁnition 7.2
above, we conclude that there is an algorithm A2 and a positive polynomial
S, such that
prob
µ½
(p, g) ∈Ik
¯¯¯ prob(A2(p, g, gx) = x : x
u←Zp−1) ≥1 −
1
S(k)
¾¶
>
1
R(k)
for inﬁnitely many k. By Proposition 6.3, there is a positive polynomial T
such that
prob(A2(p, g, gx) = x : (p, g) ←Ik, x
u←Zp−1) >
1
T(k)
for inﬁnitely many k, a contradiction to the discrete logarithm assumption
(Deﬁnition 6.1).
2
Remark. Suppose that p −1 = 2ta, where a is odd. The t least signiﬁcant
bits of x can be easily computed from gx = Expp,g(x) (see Exercise 3 of
this chapter). But all the other bits of x are secure bits, i.e., each of them
yields a hard-core predicate, as shown by H˚astad and N¨aslund ([H˚asN¨as98];
[H˚asN¨as99]).
7.2 Bit Security of the RSA Family
Let Lsb(x) := x mod 2 be the least-signiﬁcant bit of x ∈Z, x ≥0, with
respect to the binary encoding of x as an unsigned number. In order to
compute Lsb(x) for an element x in Zn, we apply Lsb to the representative
of x between 0 and n −1.
In this section, we study the RSA function
RSAn,e : Z∗
n −→Z∗
n, x 7−→xe
and its inverse RSA−1
n,e, for n = pq, with p and q odd, distinct primes, and e
prime to ϕ(n).
To compute Lsb(x) from y = xe is as diﬃcult as to compute x from y.
The following Proposition 7.12 and Theorem 7.14 make this statement more
precise. The proofs show how to reduce the computation of x from y to the
computation of Lsb(x) from y.
First, we study the deterministic case where Lsb(x) can be computed from
y by a deterministic algorithm in polynomial time.

7.2 Bit Security of the RSA Family
183
Proposition 7.12. Let A1 be a deterministic polynomial algorithm, such that
A1(n, e, xe) = Lsb(x) for all x ∈Z∗
n ,
where n := pq, p and q are odd distinct prime numbers, and e is relatively
prime to ϕ(n). Then there is a deterministic polynomial algorithm A2, such
that
A2(n, e, xe) = x for all x ∈Z∗
n .
Proof. Let x ∈Z∗
n and y = xe. The basic idea of the inversion algorithm is
to compute a ∈Z∗
n and a rational number u ∈Q, 0 ≤u < 1, with
|ax mod n −un| < 1
2 .
Then we have ax mod n = ⌊un + 1/2⌋, and hence x = a−1 ⌊un + 1/2⌋mod n.
This method to invert the RSA function is called rational approximation. We
approximate ax mod n by the rational number un.
For z ∈Z, let z := z mod n. Let 2−1 denote the inverse element of 2 mod n
in Z∗
n. We start with u0 = 0 and a0 = 1 to get an approximation for a0x with
|a0x −u0n| < n.
We deﬁne
at := 2−1at−1 and
ut := 1
2(ut−1 + Lsb(at−1x))
(the last computation is done in Q). In each step, we replace at−1 by at and
ut−1 by ut, and we observe that
atx = 2−1at−1x =
(
1
2at−1x
if at−1x is even,
1
2(at−1x + n) if at−1x is odd,
and hence
|atx −utn| = 1
2 |at−1x −ut−1n|.
After r = |n| + 1 steps, we reach
|arx −urn| < n
2r < 1
2.

184
7. Bit Security of One-Way Functions
Since Lsb(atx) = A1(n, e, ae
ty mod n), we can decide whether atx is even
without knowing x. Thus, we can compute at and ut in each step, and ﬁnally
get x. The following algorithm inverts the RSA function.
Algorithm 7.13.
int A2(int n, e, y)
1
a ←1, u ←0, k ←|n|
2
for t ←0 to k do
3
u ←1
2(u + A1(n, e, aey mod n))
4
a ←2−1a mod n
5
return a−1 ¥
un + 1
2
¦
mod n
This completes the proof of the Proposition.
2
Next we study the probabilistic case. Now the algorithm A1 does not
compute the predicate Lsb(x) deterministically, but only with a probability
slightly better than guessing it at random. Nevertheless, RSA can be inverted
with a high probability.
Theorem 7.14. Let P, Q ∈Z[X] be positive polynomials and A1 be a prob-
abilistic polynomial algorithm, such that
prob(A1(n, e, xe) = Lsb(x) : x
u←Z∗
n) ≥1
2 +
1
P(k) ,
where n := pq, k := |n|, p and q are odd distinct primes and e is relatively
prime to ϕ(n). Then there is a probabilistic polynomial algorithm A2, such
that
prob(A2(n, e, xe) = x) ≥1 −2−Q(k) for all x ∈Z∗
n .
Proof. Let y := xe and let ε := 1/P(k). As in the deterministic case, we use
rational approximation to invert the RSA function. We try to approximate
ax mod n by a rational number un. To invert RSA correctly, we have to
compute Lsb(ax) correctly in each step. However, now we only know that
the probability for k correct computations of Lsb is ≥(1/2 + ε)k, which is
exponentially close to 0 and thus too small. In order to increase the proba-
bility of success, we develop the algorithm L. The probability of success of L
is suﬃciently high. This is the statement of the following lemma.
Lemma 7.15. Under the assumptions of the theorem, there is a probabilistic
polynomial algorithm L with the following properties: given y := xe, randomly
chosen a, b ∈Z∗
n,2 α := Lsb(ax mod n), β := Lsb(bx mod n), u ∈Q with
|ax mod n −un| ≤ε3n/8 and v ∈Q with |bx mod n −vn| ≤εn/8, then L
successively computes values lt, t = 0, 1, 2, . . . , k, such that
2 Actually we randomly choose a, b ∈Zn. In the rare case that a, b /∈Z∗
n, we can
factor n using Euclid’s algorithm and compute x from xe.

7.2 Bit Security of the RSA Family
185
prob(lt = Lsb(atx mod n)| ∧t−1
j=0 lj = Lsb(ajx mod n) : a, b
u←Z∗
n) ≥1 −1
2k ,
where a0 := a, at := 2−1at−1.
Proof (of the Lemma). Let m := min(2tε−2, 2kε−2). We may assume that
both primes p and q are > m. Namely, if one of the primes is ≤m, then
we may factorize n in polynomial time (and then easily compute the inverse
of the RSA function), simply by checking whether one of the polynomially
many numbers ≤m divide n.
To compute Lsb(atx mod n), L will apply A1 m times. In each step, a
least-signiﬁcant bit is computed and the return value of L is the more fre-
quently occurring bit. In the theorem, we assume that
prob(A1(n, e, xe) = Lsb(x) : x
u←Z∗
n) ≥1
2 + ε.
The probability is also taken over the random choice of x ∈Z∗
n. Thus, we
cannot simply repeat the execution of A1 with the same input (atx)e, but
have to modify the input randomly. We use the modiﬁers at + iat−1 + b,
i ∈Z, −m/2 ≤i ≤m/2 −1, and compute Lsb((at + iat−1 + b)x mod n).3 As
we will see below, the assumptions of the lemma guarantee that we can infer
Lsb(atx mod n) from Lsb((at+iat−1+b)x mod n) with high probability (here
suﬃciently good rational approximations ut of atx mod n and v of bx mod n
are needed). a and b are chosen independently and at random, because then
the modiﬁers at + iat−1 + b are pairwise independent. Then Corollary B.18,
which is a consequence of the weak law of large numbers, applies. This implies
that the probability of success of L is as large as desired.
We now describe how L works on input of y = xe, a, b, α, β, u and v to com-
pute lt, t = 0, 1, . . . , k. In its computation, L uses the variable at to store the
current at, and the variable at−1 to store the at from the preceding iteration.
Analogously, we use variables ut and ut−1. utn is a rational approximation
of atx. We have u0 = u and ut = 1/2 (ut−1 + lt−1).
It is the goal of L to return lt = Lsb(atx mod n) for t = 0, 1, . . . , k. The
ﬁrst iteration t = 0 is easy: l0 is the given α. From now on, the variable α is
used to store the last computed lt.
Before L starts to compute l1, l2, . . . , lk, its variables are initialized by
at−1 := a0 = a, ut−1 := u.
To compute lt, t ≥1, L repeats the following subroutine.
3 If at + iat−1 + b /∈Z∗
n, we factor n using Euclid’s algorithm and compute x from
xe.

186
7. Bit Security of One-Way Functions
Algorithm 7.16.
L′( )
1
C0 ←0; C1 ←0
2
at ←2−1at−1; ut ←1
2(ut−1 + α)
3
for i ←−m
2 to m
2 −1 do
4
A ←at + iat−1 + b
5
W ←⌊ut + iut−1 + v⌋
6
B ←(iα + β + W) mod 2
7
if A1(n, e, Aey mod n) ⊕B = 0
8
then C0 ←C0 + 1
9
else C1 ←C1 + 1
10
ut−1 ←ut, at−1 ←at
11
if C0 > C1
12
then α ←0
13
else α ←1
14
return α
For z ∈Z, we denote by z the remainder of z modulo n.
For i ∈{−m/2, . . . , m/2 −1}, let
At,i := at + iat−1 + b,
W ′
t,i := ut + iut−1 + v, Wt,i =
¥
W ′
t,i
¦
,
Bt,i := (i · Lsb(at−1x) + Lsb(bx) + Lsb(Wt,i)) mod 2.
We want to compute Lsb(atx) from Lsb(At,ix), Lsb(at−1x) and Lsb(bx).
For this purpose, let λt,i := atx+i·at−1x+bx = qn+At,ix with q =
¥λt,i/n
¦
.
Then Lsb(λt,i) = (Lsb(atx) + i · Lsb(at−1x) + Lsb(bx)) mod 2 and
Lsb(At,ix) = (Lsb(λt,i) + Lsb(q)) mod 2
= (Lsb(atx) + i · Lsb(at−1x) + Lsb(bx) + Lsb(q)) mod 2,
and we obtain
Lsb(atx) = (Lsb(At,ix) + i · Lsb(at−1x) + Lsb(bx) + Lsb(q)) mod 2.
The problem is to get q and Lsb(q). We will show that Wt,i is equal to
q with a high probability, and Wt,i is easily computed from the rational
approximations ut of atx, ut−1 of at−1x and v of bx. If Wt,i = q, we have
Lsb(atx) = Lsb(At,ix) ⊕Bt,i.
We assume from now on that L computed the least-signiﬁcant bit correctly
in the preceding steps:
Lsb(ajx) = lj, 0 ≤j ≤t −1.
Next, we give a lower bound for the probability that Wt,i = q. Let Z =
|λt,i −W ′
t,in|.

7.2 Bit Security of the RSA Family
187
Z = |atx −utn + i(at−1x −ut−1n) + bx −vn|
≤
¯¯¯¯
1
2(at−1x −ut−1n)(1 + 2i)
¯¯¯¯ + |bx −vn|
≤n
2t
ε3
8 |1 + 2i| + ε
8n ≤ε
8n
µε2m
2t
+ 1
¶
≤ε
4n.
Note that |ajx −ujn| = 1/2 |(aj−1x −uj−1n)| for 1 ≤j ≤t under our
assumption lj = Lsb(ajx), 0 ≤j ≤t −1 (see the proof of Proposition
7.12). Moreover, |1 + 2i| ≤m, because −m/2 ≤i ≤m/2 −1, and m =
min(2tε−2, 2kε−2).
Now Wt,i ̸= q if and only if there is a multiple of n between λt,i and W ′
t,in.
There is no multiple of n between λt,i and W ′
t,in if
ε
4n < λt,i = At,ix < n −ε
4n ,
because Z ≤ε/4 n.
Since a, b ∈Z∗
n are selected uniformly and at random, the remainders
λt,i = (at + iat−1 + b)x mod n = ((2−1 + i)at−1 + bx) mod n are also selected
uniformly and at random.
This implies
prob(Wt,i = q) ≥prob
³ε
4n < At,ix < n −ε
4n
´
≥1 −ε
2.
We are now ready to show
prob(lt = Lsb(atx)| ∧t−1
j=0 lj = Lsb(ajx)}) ≥1 −1
2k .
Let E1,i be the event A1(n, e, Ae
t,iy) = Lsb(At,ix) (here recall that y = xe),
and let E2,i be the event that At,ix satisﬁes the condition
ε
4n < At,ix < n −ε
4n.
We have prob(E1,i) ≥1/2 + ε and prob(E2,i) = 1 −ε/2. We deﬁne random
variables
Si :=
½
1 if E1,i and E2,i occur,
0 otherwise.
We do not err in computing Lsb(atx) in the i-th step of our algorithm L if
both events E1,i and E2,i occur, i.e., if Si = 1.
We have (denoting by E1,i the complement of event E1,i)
prob(Si = 1) = prob(E1,i and E2,i) ≥prob(E2,i) −prob(E1,i)
>
³
1 −ε
2
´
−
µ1
2 −ε
¶
= 1
2 + ε
2.

188
7. Bit Security of One-Way Functions
Let i ̸= j. The probabilities prob(Si = d) and prob(Sj = d) (d ∈{0, 1}) are
taken over the random choice of a, b ∈Z∗
n and the coin tosses of A1(n, e, Ae
t,iy)
and A1(n, e, Ae
t,jy). The elements a0 = a and b are chosen independently
(and uniformly), and we have (At,i, At,j) = (at−1, b)∆= (2−t+1a, b)∆with
the invertible matrix
∆=
µ
2−1+i
2−1+j
1
1
¶
over Z∗
n. Thus, At,i and At,j are also independent. This implies that the
events E2,i and E2,j are independent and that the inputs Ae
t,iy and Ae
t,jy are
independent random elements. Since the coin tosses during an execution of A1
are independent of all other random events (see Chapter 5), the events E1,i
and E1,j are also independent. We see that Si and Sj are indeed independent.
Note that the determinant i −j of ∆is in Z∗
n, since |i −j| < m < min{p, q}.
The number of i, −m/2 ≤i ≤m/2 −1, and hence the number of random
variables Si, is m. By Corollary B.18, we conclude
prob
ÃX
i
Si > m
2
!
≥1 −
1
mε2 ≥1 −1
2k .
Recall that we do not err in computing Lsb(atx) in the i-th step of our
algorithm L, if both events E1,i and E2,i occur, i.e., if Si = 1. Thus, we
have C0 ≥P
i Si and hence prob(C0 > C1) ≥1 −1/2k if Lsb(atx) = 0, and
C1 ≥P
i Si and hence prob(C1 > C0) ≥1 −1/2k if Lsb(atx) = 1. Therefore,
we have shown that
prob(lt = Lsb(atx)| ∧t−1
j=0 lj = Lsb(ajx)) ≥1 −1
2k .
The proof of the lemma is complete.
2
We continue in the proof of the theorem. The following algorithm A in-
verts the RSA function by the method of rational approximation. The basic
structure of A is the same as that of Algorithm 7.13. Now we call L to com-
pute Lsb(ax). Therefore, we must meet the assumptions of Lemma 7.15. This
is done in lines 1–4.

7.2 Bit Security of the RSA Family
189
Algorithm 7.17.
int A(int n, e, y)
1
select a, b ∈Z∗
n at random
2
guess u, v ∈Q ∩[0, 1[ satisfying
3
|ax mod n −un| ≤ε3n
8 , |bx mod n −vn| ≤εn
8
4
guess α ←Lsb(ax mod n), guess β ←Lsb(bx mod n)
5
Compute l0, l1, . . . , lk by L
6
for t ←0 to k do
7
u ←1
2(u + lt)
8
a ←2−1a mod n
9
return a−1 ¥
un + 1
2
¦
mod n
Algorithm L computes l0, . . . , lk in advance. Lines 7 and 8 of A also ap-
pear in L. In a real and eﬃcient implementation, it is possible to avoid this
redundancy.
As above in Algorithm 7.10, we can “guess” the right alternative. This
means we can ﬁnd out the right alternative in polynomial time. There are
only a polynomial number of alternatives, and both the computation for each
alternative as well as checking the result can be done in polynomial time. In
order to guess u or v, we have to consider 8/ε3 = 8P(k)3 and 8/ε = 8P(k)
many intervals. There are only two alternatives for Lsb(ax) and Lsb(bx).
A(n, e, y) = RSA−1
n,e(y) = x for y = xe if L correctly computes lt =
Lsb(atx) for t = 1, . . . , k. Thus, we have
prob(A(n, e, y) = x) ≥
µ
1 −1
2k
¶k
.
Since (1 −1/2k)k increases monotonously (converging to e−1/2), we conclude
prob(A(n, e, y) = x) ≥1
2.
Repeating the computation A(n, e, y) independently, we get, by Proposi-
tion 5.7, a probabilistic polynomial algorithm A2(n, e, y) with
prob(A2(n, e, y) = RSA−1
n,e(y)) ≥1 −2−Q(k),
and Theorem 7.14 is proven.
2
Remarks:
1. The expectation is that A will compute RSA−1
n,e(y) after two repetitions
(see Lemma B.12). The input of A1 does not depend on the guessed
elements u, v, α and β (it only depends on a and b). Thus, we can also
use the return values of A1, computed for the ﬁrst guess of u, v, α and
β, for all subsequent guesses. Then we expect that we have to call A1 at
most 4k2ε−2 times to compute RSA−1
n,e(y).

190
7. Bit Security of One-Way Functions
2. The bit security of the RSA family was ﬁrst studied in [GolMicTon82],
in which a method for inverting RSA by guessing the least-signiﬁcant bit
was introduced (see Exercise 11).
The problem of inverting RSA, if the least-signiﬁcant bit is predicted only
with probability ≥1/2 + 1/P(k), is studied in [SchnAle84], [VazVaz84],
[AleChoGolSch88] and [FisSch2000]. The technique we used to prove
Theorem 7.14 is from [FisSch2000].
Corollary 7.18. Let
I := {(n, e) | n = pq, p and q odd distinct primes, |p| = |q|, e prime to ϕ(n)}.
Provided the RSA assumption is true, then
Lsb = (Lsbn,e : Z∗
n −→{0, 1}, x 7−→Lsb(x))(n,e)∈I
is a family of hard-core predicates for the RSA family
RSA = (RSAn,e : Z∗
n −→Z∗
n, x 7−→xe)(n,e)∈I.
Proof. The proof is analogous to the proof of Corollary 7.11.
2
Remark. H˚astad and N¨aslund have shown that all the plaintext bits are secure
bits of the RSA function, i.e., each of them yields a hard-core predicate
([H˚asN¨as98]; [H˚asN¨as99]).
7.3 Bit Security of the Square Family
Let n := pq, with p and q distinct primes, and p, q ≡3 mod 4. We consider
the (bijective) modular squaring function
Squaren : QRn −→QRn, x 7−→x2
and its inverse, the modular square root function
Sqrtn : QRn −→QRn, y 7−→Sqrtn(y)
(see Section 6.5). The computation of Sqrtn(y) can be reduced to the com-
putation of the least-signiﬁcant bit Lsb(Sqrtn(y)) of the square root. This is
shown in Proposition 7.19 and Theorem 7.22. In the proposition, the algo-
rithm that computes the least-signiﬁcant bit is assumed to be deterministic
polynomial. Then the algorithm which we obtain by the reduction is also
deterministic polynomial. It computes Sqrtn(y) for all y ∈QRn (under the
additional assumption n ≡1 mod 8).
Proposition 7.19. Let A1 be a deterministic polynomial algorithm, such that
A1(n, x2) = Lsb(x) for all x ∈QRn ,
where n = pq, p and q are distinct primes, p, q ≡3 mod 4 and n ≡1 mod 8.
Then there exists a deterministic polynomial algorithm A2, such that
A2(n, y) = Sqrtn(y) for all y ∈QRn .

7.3 Bit Security of the Square Family
191
Proof. As in the RSA case, we use rational approximation to invert the
Square function. Let y = x2, x ∈QRn. Since n is assumed to be ≡1 mod 8,
either 2 ∈QRn or −2 ∈QRn (see the remark following this proof).
First, let 2 ∈QRn. Then 2−1 ∈QRn. We deﬁne a0 := 1, at :=
2−1at−1 mod n for t ≥1, and u0 := 1, ut := 1/2 (ut−1 + Lsb(at−1x mod n))
for t ≥1. Since 2−1 ∈QRn, we have at ∈QRn and hence atx ∈QRn
for all t ≥1. Thus, Sqrtn(a2
tx2) = atx, and hence we can compute
Lsb(atx mod n) = A1(n, a2
tx2) = A1(n, a2
ty) by A1 for all t ≥1. The ra-
tional approximation works as in the RSA case.
Now, let −2 ∈QRn. Then −2−1 ∈QRn. We modify the method of
rational approximation and deﬁne a0 = 1, at = −2−1at−1 mod n for t ≥1,
and u0 = 1, ut = 1/2 (2 −Lsb(at−1x mod n) −ut−1) for t ≥1. Then, we get
|atx mod n −utn| = 1
2 |(at−1x mod n −ut−1n)|,
because
atx mod n = −2−1at−1x mod n = 2−1(n −at−1x mod n)
=
(
1
2(n −at−1x mod n)
if at−1x mod n is odd,
1
2(n −at−1x mod n + n) otherwise.
After r = |n| + 1 steps we reach
|arx −urn| ≤n
2r < 1
2.
Since −2−1 ∈QRn, we have at ∈QRn and hence atx ∈QRn for t ≥1.
Thus, Sqrtn(a2
tx2) = atx, and hence we can compute Lsb(atx mod n) =
A1(n, a2
tx2) = A1(n, a2
ty) by A1 for all t ≥1.
2
Remarks:
1. As p, q ≡3 mod 4 is assumed, p and q are ≡3 mod 8 or ≡−1 mod 8,
and hence either n ≡1 mod 8 or n ≡5 mod 8. We do not consider the
case n ≡5 mod 8 in the proposition.
2. The proof actually works if we have 2 ∈QRn or −2 ∈QRn. This is
equivalent to n ≡
1 mod 8, as follows from Theorem A.53. We have
2 ∈QRn if and only if 2 ∈QRp and 2 ∈QRq, and this in turn is
equivalent to p ≡q ≡−1 mod 8 (by Theorem A.53). On the other hand,
−2 ∈QRn if and only if −2 ∈QRp and −2 ∈QRq, and this in turn is
equivalent to p ≡q ≡3 mod 8 (by Theorem A.53).
Studying the Square function, we face, in the probabilistic case, an addi-
tional diﬃculty compared to the reduction in the RSA case. Membership in
the domain of the Square function – the set QRn of quadratic residues – is not
eﬃciently decidable without knowing the factorization of n. To overcome this

192
7. Bit Security of One-Way Functions
diﬃculty, we develop a probabilistic polynomial reduction of the quadratic
residuosity property to the predicate deﬁned by the least-signiﬁcant bit of
Sqrt. Then the same reduction as in the RSA case also works for the Sqrt
function.
Let J+1
n
= {x ∈Z∗
n |
¡ x
n
¢
= +1}. The predicate
PQRn : J+1
n
−→{0, 1}, PQRn(x) =
(
1 if x ∈QRn,
0 otherwise,
is believed to be a trapdoor predicate (see Deﬁnition 6.11). In Proposition
7.20, we show how to reduce the computation of PQRn(x) to the computation
of the least-signiﬁcant bit of a Sqrtn(x).
Proposition 7.20. Let P, Q ∈Z[X] be positive polynomials, and let A1 be a
probabilistic polynomial algorithm, such that
prob(A1(n, x2) = Lsb(x) : x
u←QRn) ≥1
2 +
1
P(k) ,
where n = pq, p and q are distinct primes, p, q ≡3 mod 4, and k = |n|.
Then there exists a probabilistic polynomial algorithm A2, such that
prob(A2(n, x) = PQRn(x)) ≥1 −
1
Q(k) for all x ∈J+1
n .
Proof. Let x ∈J+1
n . If x ∈QRn, then x = Sqrtn(x2) and therefore
Lsb(x) = Lsb(Sqrtn(x2)). If x /∈QRn, then −x mod n = n −x = Sqrtn(x2)
and Lsb(x) ̸= Lsb(Sqrtn(x2)) (note that −1 ∈J+1
n \ QRn for p, q ≡3 mod 4,
by Theorem A.53). Consequently, we get
PQRn(x) = Lsb(x) ⊕Lsb(Sqrtn(x2)) ⊕1.
Since for each y ∈QRn there are exactly two elements x ∈J+1
n
with
x2 = y and because |J+1
n | = 2|QRn|, we conclude
prob(A1(n, x2) = Lsb(Sqrtn(x2)) : x
u←J+1
n )
= prob(A1(n, y) = Lsb(Sqrtn(y)) : y
u←QRn).
Hence, we get
prob(PQRn(x) = A1(n, x2) ⊕Lsb(x) ⊕1 : x
u←J+1
n ) ≥1
2 +
1
P(k).
We construct an algorithm A2, such that for every x ∈J+1
n
prob(A2(n, x) = PQRn(x)) ≥1 −
1
Q(k).

7.3 Bit Security of the Square Family
193
Algorithm 7.21.
int A2(int n, x)
1
c ←0, l ←
l
Q(k)P 2(k)
4
m
2
for i ←1 to l do
3
select r ∈QRn at random
4
c ←c + (A1(n, (rx)2 mod n) ⊕Lsb(rx mod n) ⊕1)
5
if c > l
2
6
then return 1
7
else return 0
Let x ∈J+1
n . PQRn(x) = PQRn(rx) is computed l times by applying
A1 to the l independent random inputs rx. We compute PQRn(rx) in each
step with a probability > 1/2. The weak law of large numbers guarantees
that, for suﬃciently large l, we can compute PQRn(x) by the majority of the
results, with a high probability. More precisely, let the random variable Si,
i = 1, . . . , l, be deﬁned by
Si =
(
1 if PQRn(rx) = A1(n, (rx)2) ⊕Lsb(rx) ⊕1,
0 otherwise,
with r ∈QRn randomly chosen, as in the algorithm. Then we have
prob(Si = 1) ≥1/2 + 1/P(k). The random variables Si, i = 1, . . . , l, are
independent. If PQRn(x) = 1, we get by Corollary B.18
prob(c > l
2) ≥1 −P 2(k)
4l
≥1 −
1
Q(k) .
The case PQRn(x) = 0 follows analogously. Thus, we have shown
prob(A2(n, x) = PQRn(x)) ≥1 −
1
Q(k) ,
as desired.
2
Theorem 7.22. Let P, Q ∈Z[X] be positive polynomials and A1 be a prob-
abilistic polynomial algorithm, such that
prob(A1(n, x2) = Lsb(x) : x
u←QRn) ≥1
2 +
1
P(k) ,
where n := pq, p and q are distinct primes, p, q ≡3 mod 4, and k := |n|.
Then there is a probabilistic polynomial algorithm A2, such that
prob(A2(n, x) = Sqrtn(x)) ≥1 −2−Q(k) for all x ∈QRn .

194
7. Bit Security of One-Way Functions
Proof. The proof runs in the same way as the proof of Theorem 7.14. We only
describe the diﬀerences to this proof. Here, the algorithm A1 is only applicable
to quadratic residues. However, it is easy to compute
¡ x
n
¢
for x ∈Z∗
n, and we
can use algorithm A2 from Proposition 7.20 to check whether a given element
x ∈J+1
n
is a quadratic residue. Assume that prob(A2(n, x) = PQRn(x)) ≥
1 −1/P 2(k).
If p, q ≡3 mod 4, we have −1 /∈QRn (see Theorem A.53). Therefore,
either a or −a ∈QRn for
¡ a
n
¢
= 1. We are looking for m multipliers ˜a of the
form at +iat−1 +b with
¡ ˜a
n
¢
= 1, where m := min(2tε−2, 2kε−2). If ˜a ∈QRn,
Lsb(˜ax) can be computed with algorithm A1, and if −˜a = n −˜a ∈QRn,
Lsb(˜ax) = 1 −Lsb(−˜ax) and Lsb(−˜ax) can be computed with algorithm
A1. Lsb(˜ax) is correctly computed if A1 correctly computes the predicate
Lsb and A2 from Proposition 7.20 correctly computes the predicate PQRn.
Both events are independent. Thus Lsb(˜ax) is computed correctly with a
probability >
¡1/2 + 1/P(k)
¢ ¡
1 −1/P 2(k)
¢
>
¡1/2 + 1/2P(k)
¢
. Thus we set
ε = 1/2P(k).
With i varying in an interval, the fraction of the multipliers at + iat−1 +
b which are in J+1
n
diﬀers from 1/2 only negligibly, because J+1
n
is nearly
uniformly distributed in Z∗
n (see [Peralta92]).
We double the range for i, take i ∈[−m, m −1], and halve the distances
of the initial rational approximations:
|ax mod n −un| ≤ε3n
16 and |bx mod n −vn| ≤εn
16 .
Now we obtain the same estimates as in the proof of Theorem 7.14.
2
Corollary 7.23. Let I := {n | n = pq, p and q distinct primes, |p| = |q|,
p, q ≡3 mod 4}. Provided that the factorization assumption is true,
QRLsb = (QRLsbn : QRn −→{0, 1}, x 7−→Lsb(x))n∈I
is a family of hard-core predicates for
Square =
¡
Squaren : QRn −→QRn, x 7−→x2¢
n∈I .
Proof. The proof is analogous to the proof of Corollary 7.11. Observe that
the ability to compute square roots modulo n is equivalent to the ability to
compute the prime factors of n (Proposition A.64).
2

Exercises
195
Exercises
1. Compute Logp,g(17) using Algorithm 7.6, for p := 19 and g := 2 (note
that 2, 4 and 13 are principal square roots).
2. Let p be an odd prime number, and g be a primitive root in Z∗
p.
For y := Expp,g(x), we have
Bp,g(y) = 0 if and only if 0 ≤x < p −1
2
,
Bp,g(y2) = 0 if and only if 0 ≤x < p −1
4
and p −1
2
≤x < 3(p −1)
4
,
and so on. Let A1 be a a deterministic polynomial algorithm such that
A1(p, g, y) = Bp,g(y) for all y ∈Z∗
p.
By using a binary search technique, prove that there is a deterministic
polynomial algorithm A2 such that
A2(p, g, y) = Logp,g(y)
for all y ∈Z∗
p.
3. Let p be an odd prime number. Suppose that p−1 = 2ta, where a is odd.
Let g be a primitive root in Z∗
p:
a. Show how the t least-signiﬁcant bits (bits at the positions 0 to t −1)
of x ∈Zp−1 can be easily computed from gx = Expp,g(x).
b. Denote by Lsbt(x) the t-th least-signiﬁcant bit of x (bit at position t
counted from the right, beginning with 0). Let A1 be a deterministic
polynomial algorithm such that
A1(p, g, gx) = Lsbt(x)
for all x ∈Zp−1. By using A1, construct a deterministic polynomial
algorithm A2 such that
A2(p, g, y) = Logp,g(y)
for all y ∈Z∗
p. (Here assume that a deterministic algorithm for com-
puting square roots exists.)
c. Show that Lsbt yields a hard-core predicate for the Exp family.
4. As in the preceding exercise, Lsbj denotes the j-th least-signiﬁcant bit.
a. Let A1 be a deterministic polynomial algorithm such that for all
x ∈Zp−1
A1(p, g, gx, Lsbt(x), . . . , Lsbt+j−1(x)) = Lsbt+j(x),
where p is an odd prime, g ∈Z∗
p is a primitive root, p −1 = 2ta, a is
odd, k = |p| and j ∈{0, . . . , ⌊log2(k)⌋}.
Construct a deterministic polynomial algorithm A2 such that

196
7. Bit Security of One-Way Functions
A2(p, g, y) = Logp,g(y)
for all y ∈Z∗
p.
b. Let P, Q ∈Z[X] be positive polynomials and A1 be a probabilistic
polynomial algorithm such that
prob(A1(p, g, gx, Lsbt(x), . . . , Lsbt+j−1(x))
= Lsbt+j(x) : x
u←Zp−1) ≥1
2 +
1
P(k) ,
where p is a an odd prime, g ∈Z∗
p is a primitive root, p −1 = 2ta, a
is odd, k = |p| and j ∈{0, . . . , ⌊log2(k)⌋}.
Construct a probabilistic polynomial algorithm A2 such that
prob(A2(p, g, y) = Logp,g(y)) ≥1 −2−Q(k)
for all y ∈Z∗
p.
5. Let I := {(p, g) | p an odd prime, g ∈Z∗
p a primitive root} and Ik :=
{(p, g) ∈I | |p| = k}. Assume that the discrete logarithm assumption
(see Deﬁnition 6.1) is true.
Show that for every probabilistic polynomial algorithm A, with inputs
p, g, y, b0, . . . , bj−1, 1 ≤j ≤⌊log2(k)⌋, and for every positive polynomial
P, there is a k0 ∈N such that
prob(A(p, g, gx, Lsbt(x), . . . , Lsbt+j−1(x))
= Lsbt+j(x) : (p, g)
u←Ik, x
u←Zp−1) ≤1
2 +
1
P(k)
for k ≥k0 and for all j ∈{0, . . . , ⌊log2(k)⌋}. t is deﬁned by p −1 = 2ta,
with a odd.
In particular, the predicates Lsbt+j, 0 ≤j ≤⌊log2(k)⌋, are hard-core
predicates for Exp.
6. Compute the rational approximation (a, u) for 13 ∈Z29.
7. Let p := 17, q := 23, n := pq and e := 3. List the least-signiﬁcant bits
that A1 will return if you compute RSA−1
n,e(49) using Algorithm 7.12
(note that 49 = 1963 mod n).
8. Let n := pq, with p and q distinct primes and e relatively prime to ϕ(n),
x ∈Z∗
n and y := RSAn,e(x) = xe mod n. Msb is deﬁned analogously to
Deﬁnition 7.1. Show that you can compute Msb(x) from y if and only if
you can compute Lsb(x) from y.
9. Show that the most-signiﬁcant bit of x is a hard-core predicate for the
RSA family, provided that the RSA assumption is true.
10. Use the most signiﬁcant bit and prove Proposition 7.12 using a binary
search technique (analogous to Exercise 2).

Exercises
197
11. RSA inversion by binary division ([GolMicTon82]). Let y := RSAn,e(x) =
xe mod n. Let A be an algorithm that on input y outputs Lsb(x). Let
k := |n|, and let 2−e ∈Z∗
n be the inverse of 2e ∈Z∗
n. Compute x from y
by using the bit vector (bk−1, . . . , b0), deﬁned by
y0 = y,
b0 = A(y0),
yi =
(
yi−12−e mod n
if bi−1 = 0,
(n −yi−1)2−e mod n otherwise,
bi = A(yi), for 1 ≤i ≤k.
Describe the algorithm inverting RSA and show that it really does invert
RSA.
12.
a. Let A1 be a deterministic polynomial algorithm such that
A1(n, e, xe, Lsb0(x), . . . , Lsbj−1(x)) = Lsbj(x)
for all x ∈Z∗
n, where n := pq, p and q are odd distinct primes, e is
relatively prime to ϕ(n), k := |n| and j ∈{0, . . . , ⌊log2(k)⌋}.
Construct a deterministic polynomial algorithm A2 such that
A2(n, e, xe) = x
for all x ∈Z∗
n.
b. Let P, Q ∈Z[X] be positive polynomials and A1 be a probabilistic
polynomial algorithm, such that
prob(A1(n, e, xe, Lsb0(x), . . . , Lsbj−1(x))
= Lsbj(x) : x
u←Z∗
n) ≥1
2 +
1
P(k) ,
where n := pq, p and q are odd distinct primes, e is relatively prime
to ϕ(n), k := |n| and j ∈{0, . . . , ⌊log2(k)⌋}.
Construct a probabilistic polynomial algorithm A2 such that
prob(A2(n, e, xe) = x ≥1 −2−Q(k)
for all x ∈Z∗
n.
13. Let I := {(n, e) | n = pq, p ̸= q prime numbers, |p| = |q|, e < ϕ(n), e
prime to ϕ(n)} and Ik := {(n, e) ∈I | n = pq, |p| = |q| = k}. Assume
that the RSA assumption (see Deﬁnition 6.7) is true.
Show that for every probabilistic polynomial algorithm A, with inputs
n, e, y, b0, . . . , bj−1, 0 ≤j ≤⌊log2(|n|)⌋, and every positive polynomial P,
there is a k0 ∈N such that

198
7. Bit Security of One-Way Functions
prob(A(n, e, xe, Lsb0(x), . . . , Lsbj−1(x))
= Lsbj(x) : (n, e)
u←Ik, x
u←Z∗
n) ≤1
2 +
1
P(k)
for k ≥k0 and for all j ∈{0, . . . , ⌊log2(|n|)⌋}. In particular, the predicates
Lsbj, 0 ≤j ≤⌊log2(|n|)⌋, are hard-core predicates for RSA.

8. One-Way Functions and Pseudorandomness
There is a close relationship between encryption and randomness. The se-
curity of encryption algorithms usually depends on the random choice of
keys and bit sequences. A famous example is Shannon’s result. Ciphers with
perfect secrecy require randomly chosen key strings that are of the same
length as the encrypted message. In Chapter 9, we will study the classical
Shannon approach to provable security, together with more recent notions of
security. One main problem is that truly random bit sequences of suﬃcient
length are not available in most practical situations. Therefore, one works
with pseudorandom bit sequences. They appear to be random, but actually
they are generated by an algorithm. Such algorithms are called pseudoran-
dom bit generators. They output, given a short random input value (called the
seed), a long pseudorandom bit sequence. Classical techniques for the genera-
tion of pseudorandom bits or numbers (see [Knuth98]) yield well-distributed
sequences. Therefore, they are well-suited for Monte Carlo simulations. How-
ever, they are often cryptographically insecure. For example, in linear con-
gruential pseudorandom number generators or linear feedback shift registers
(see, e.g., [MenOorVan96]), the secret parameters and hence the complete
pseudorandom sequence can be eﬃciently computed from a small number of
outputs.
It turns out that computationally perfect (hence cryptographically secure)
pseudorandom bit generators can be derived from one-way permutations with
hard-core predicates. We will discuss this close relation in this chapter. The
pseudorandom bit generators G studied are families of functions whose in-
dexes vary over a set of keys. Before we can use G, we have to select such a
key with a suﬃciently large security parameter.
Of course, even applying a perfect pseudorandom generator requires start-
ing with a truly random seed. Thus, in any case you need some “natural”
source of random bits, such as independent fair coin tosses (see Chapter 5).
8.1 Computationally Perfect Pseudorandom Bit
Generators
In the deﬁnition of pseudorandom generators, we use the notion of polynomial
functions.

200
8. One-Way Functions and Pseudorandomness
Deﬁnition 8.1. We call a function l : N −→N a polynomial function if it is
computable by a polynomial algorithm and if there is a polynomial Q ∈Z[X],
such that l(k) ≤Q(k) for all k ∈N.
Now we are ready to deﬁne pseudorandom bit generators.
Deﬁnition 8.2. Let I = (Ik)k∈N be a key set with security parameter k, and
let K be a probabilistic polynomial sampling algorithm for I, which on input
1k outputs an i ∈Ik. Let l be a polynomial function.
A pseudorandom bit generator with key generator K and stretch function
l is a family G = (Gi)i∈I of functions
Gi : Xi −→{0, 1}l(k)
(i ∈Ik),
such that
1. G is computable by a deterministic polynomial algorithm G:
G(i, x) = Gi(x) for all i ∈I and x ∈Xi.
2. There is a uniform sampling algorithm S for X := (Xi)i∈I, which on
input i ∈I outputs x ∈Xi.
The generator G is computationally perfect (or cryptographically secure), if
the pseudorandom sequences generated by G cannot be distinguished from
true random sequences by an eﬃcient algorithm; i.e., for every positive poly-
nomial P ∈Z[X] and every probabilistic polynomial algorithm A with inputs
i ∈Ik, z ∈{0, 1}l(k) and output in {0, 1}, there is a k0 ∈N such that for all
k ≥k0
| prob(A(i, z) = 1 : i ←K(1k), z
u←{0, 1}l(k))
−prob(A(i, Gi(x)) = 1 : i ←K(1k), x
u←Xi) | ≤
1
P(k).
Remarks:
1. The probabilistic polynomial algorithm A in the deﬁnition may be con-
sidered as a statistical test trying to compute some property which dis-
tinguishes truly random sequences in {0, 1}l(k) from the pseudorandom
sequences generated by G. Classical statistical tests for randomness, such
as the Chi-square test ([Knuth98], Chapter 3), can be considered as such
tests and can be implemented as polynomial algorithms. Thus, “compu-
tationally perfect” means that no statistical test – which can be imple-
mented as a probabilistic algorithm with polynomial running time – can
signiﬁcantly distinguish between true random sequences and sequences
generated by G, provided a suﬃciently large key i is chosen.
2. By condition 2, we can randomly generate uniformly distributed seeds x ∈
Xi for the generator G. We could (but do not) generalize the deﬁnition
and allow non-uniform seed generators S (see the analogous remark after

8.1 Computationally Perfect Pseudorandom Bit Generators
201
the formal deﬁnition of one-way functions – Deﬁnition 6.12, remark 2).
The constructions and proofs given below also work in this more general
case.
3. We study only computationally perfect pseudorandom generators. There-
fore, we do not specify any other level of pseudorandomness for the se-
quences generated by G. In the literature, the term “pseudorandom gen-
erator” is sometimes only used for generators that are computationally
perfect (see, e.g., [Goldreich99]; [Goldreich01]).
4. Our deﬁnition of computationally perfect pseudorandom generators is
a deﬁnition in the “public-key model”. The key i is an input to the
statistical tests A (which are the adversaries). Thus, the key i is assumed
to be public and available to everyone. Deﬁnition 8.2 can be adapted
to the “private-key model”, where the selected key i is kept secret, and
hence is not known to the adversaries. The input i of the statistical tests
A has to be omitted. We only discuss the public-key model.
5. Admissible key generators can be analogously deﬁned as in Deﬁnition
6.13.
6. The probability in the deﬁnition is also taken over the random generation
of a key i, with a given security parameter k. Even for very large k, there
may be keys i such that A can successfully distinguish pseudorandom
from truly random sequences. However, when generating a key i by K,
the probability of obtaining one for which A has a signiﬁcant chance of
success is negligibly small (see Exercise 8 in Chapter 6).
7. As is common, we require that the pseudorandom generator can be im-
plemented by a deterministic algorithm. However, if the sequences can be
computed probabilistically in polynomial time, then we can also compute
them almost deterministically: for every positive polynomial Q, there is
a probabilistic polynomial algorithm G(i, x) with
prob(G(i, x) = Gi(x)) ≥1 −2−Q(k)
(i ∈Ik)
(see Proposition 5.6 and Exercise 5 in Chapter 5). Thus, a modiﬁed def-
inition, which relaxes condition 1 to “Monte-Carlo computable”, would
also work. In all our examples, the pseudorandom sequences can be eﬃ-
ciently computed by deterministic algorithms.
We will now derive pseudorandom bit generators from one-way permuta-
tions with hard-core predicates (see Deﬁnition 6.15). These generators turn
out to be computationally perfect. The construction was introduced by Blum
and Micali ([BluMic84]).
Deﬁnition 8.3. Let I = (Ik)k∈N be a key set with security parameter k, and
let Q ∈Z[X] be a positive polynomial.
Let f = (fi : Di −→Di)i∈I be a family of one-way permutations with hard-
core predicate B = (Bi : Di −→{0, 1})i∈I and key generator K. Then we
have the following pseudorandom bit generator with stretch function Q and
key generator K:

202
8. One-Way Functions and Pseudorandomness
G := G(f, B, Q) := (Gi : Di −→{0, 1}Q(k))k∈N,i∈Ik,
x ∈Di 7−→(Bi(x), Bi(fi(x)), Bi(f 2
i (x)), . . . , Bi(f Q(k)−1
i
(x))).
We call this generator the pseudorandom bit generator induced by f, B and
Q.
Remark. We obtain the pseudorandom bit sequence by a very simple construc-
tion: choose some random seed x
u←Di. Compute the ﬁrst pseudorandom bit
as Bi(x), apply fi and get y := fi(x). Compute the next pseudorandom bit
as Bi(y), apply fi to y and get a new y := fi(y). Compute the next pseudo-
random bit as Bi(y), and so on.
Examples:
1. Provided that the discrete logarithm assumption is true, the discrete
exponential function
Exp = (Expp,g : Zp−1 −→Z∗
p, x 7−→gx)(p,g)∈I
with I := {(p, g) | p prime, g ∈Z∗
p a primitive root} is a bijective one-
way function, and the most-signiﬁcant bit Msbp(x) deﬁned by
Msbp(x) =
(
0 for 0 ≤x < p−1
2 ,
1 for p−1
2
≤x < p −1,
is a hard-core predicate for Exp (see Section 7.1). Identifying Zp−1 with
Z∗
p in the straightforward way,1 we may consider Exp as a one-way per-
mutation. The induced pseudorandom bit generator is called the discrete
exponential generator (or Blum-Micali generator).
2. Provided that the RSA assumption is true, the RSA family
RSA = (RSAn,e : Z∗
n −→Z∗
n, x 7−→xe)(n,e)∈I
with I := {(n, e) | n = pq, p, q distinct primes, |p| = |q|, e prime to ϕ(n)}
is a one-way permutation, and the least-signiﬁcant bit Lsbn(x) is a hard-
core predicate for RSA (see Section 7.2). The induced pseudorandom bit
generator is called the RSA generator.
3. Provided that the factorization assumption is true, the modular squaring
function
Square = (Squaren : QRn −→QRn, x 7−→x2)n∈I
with I := {n | n = pq, p, q distinct primes, |p| = |q|, p, q ≡3 mod 4} is a
one-way permutation, and the least-signiﬁcant bit Lsbn(x) is a hard-core
predicate for Square (see Section 7.3). The induced pseudorandom bit
1 Zp−1 = {0, . . . , p−2} −→Z∗
p = {1, . . . , p−1}, 0 7→p−1, x 7→x for 1 ≤x ≤p−2.

8.1 Computationally Perfect Pseudorandom Bit Generators
203
generator is called the (x2 mod n) generator or Blum-Blum-Shub gener-
ator.
Here the computation of the pseudorandom bits is particularly simple:
choose a random seed x ∈Z∗
n, repeatedly square x and reduce it by
modulo n, take the least-signiﬁcant bit after each step.
We will now show that the pseudorandom bit generators, induced by one-
way permutations, are computationally perfect. For later applications, it is
useful to prove a slightly more general statement that covers the pseudoran-
dom bits and the seed, which is encrypted using f Q(k)
i
.
Theorem 8.4. Let I = (Ik)k∈N be a key set with security parameter k, and
let Q ∈Z[X] be a positive polynomial. Let f = (fi : Di −→Di)i∈I be a family
of one-way permutations with hard-core predicate B = (Bi : Di −→{0, 1})i∈I
and key generator K. Let G := G(f, B, Q) be the induced pseudorandom bit
generator.
Then, for every probabilistic polynomial algorithm A with inputs i ∈Ik, z ∈
{0, 1}Q(k), y ∈Di and output in {0, 1}, and every positive polynomial P ∈
Z[X], there is a k0 ∈N such that for all k ≥k0
| prob(A(i, Gi(x), f Q(k)
i
(x)) = 1 : i ←K(1k), x
u←Di)
−prob(A(i, z, y) = 1 : i ←K(1k), z
u←{0, 1}Q(k), y
u←Di) | ≤
1
P(k).
Remark. The theorem states that for suﬃciently large keys, the probability
of distinguishing successfully between truly random sequences and pseudo-
random sequences – using a given eﬃcient algorithm – is negligibly small,
even if the encryption f Q(k)
i
(x) of the seed x is known.
Proof. Assume that there is a probabilistic polynomial algorithm A, such
that the inequality is false for inﬁnitely many k. Replacing A by 1 −A if
necessary, we may drop the absolute value and assume that
prob(A(i, Gi(x), f Q(k)
i
(x)) = 1 : i ←K(1k), x
u←Di)
−prob(A(i, z, y) = 1 : i ←K(1k), z
u←{0, 1}Q(k), y
u←Di) >
1
P(k),
for k in an inﬁnite subset K of N.
For k ∈K and i ∈Ik, we consider the following sequence of distributions
pi,0, pi,1, . . . , pi,Q(k) on Zi := {0, 1}Q(k) × Di:2
2 We use the notation for image distributions introduced in Appendix B.1 (p.
330): pi,r is the direct product of the uniform distribution on {0, 1}Q(k)−r
with the image of the uniform distribution on Di under the mapping x 7→
(Bi(x), Bi(fi(x)), . . . , Bi(f r−1
i
(x)), f r
i (x)).

204
8. One-Way Functions and Pseudorandomness
pi,0 := {(b1, . . . , bQ(k), y) : (b1, . . . , bQ(k))
u←{0, 1}Q(k), y
u←Di}
pi,1 := {(b1, . . . , bQ(k)−1, Bi(x), fi(x)) :
(b1, . . . , bQ(k)−1)
u←{0, 1}Q(k)−1, x
u←Di}
pi,2 := {(b1, . . . , bQ(k)−2, Bi(x), Bi(fi(x)), f 2
i (x)) :
(b1, . . . , bQ(k)−2)
u←{0, 1}Q(k)−2, x
u←Di}
...
pi,r := {(b1, . . . , bQ(k)−r, Bi(x), Bi(fi(x)), . . . , Bi(f r−1
i
(x)), f r
i (x)) :
(b1, . . . , bQ(k)−r)
u←{0, 1}Q(k)−r, x
u←Di}
...
pi,Q(k) := {(Bi(x), Bi(fi(x)), . . . , Bi(f Q(k)−1
i
(x)), f Q(k)
i
(x)) : x
u←Di}.
We start with truly random bit sequences. In each step, we replace one
more truly random bit from the right with a pseudorandom bit. The seed
x encrypted by f r
i is always appended on the right. Note that the image
{fi(x) : x
u←Di} of the uniform distribution under fi is again the uniform
distribution, since fi is bijective. Finally, in pi,Q(k) we have the distribution
of the pseudorandom sequences supplemented by the encrypted seed. We
observe that
prob(A(i, z, y) = 1 : i ←K(1k), z
u←{0, 1}Q(k), y
u←Di)
= prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,0
←Zi)
and
prob(A(i, Gi(x), f Q(k)
i
(x)) = 1 : i ←K(1k), x
u←Di)
= prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,Q(k)
←
Zi).
Thus, our assumption says that for k ∈K, the algorithm A is able to dis-
tinguish between the distribution pi,Q(k) (of pseudorandom sequences) and
the (uniform) distribution pi,0. Hence, A must be able to distinguish between
two subsequent distributions pi,r and pi,r+1, for some r.
Since fi is bijective, we have the following equation (8.1):
pi,r = {(b1, . . . , bQ(k)−r, Bi(x), Bi(fi(x)), . . . , Bi(f r−1
i
(x)), f r
i (x)) :
(b1, . . . , bQ(k)−r)
u←{0, 1}Q(k)−r, x
u←Di}
= {(b1, . . . , bQ(k)−r, Bi(fi(x)), Bi(f 2
i (x)), . . . , Bi(f r
i (x)), f r+1
i
(x)) :
(b1, . . . , bQ(k)−r)
u←{0, 1}Q(k)−r, x
u←Di}.
We see that pi,r diﬀers from pi,r+1 only at one position, namely at position
Q(k) −r. There, the hard-core bit Bi(x) is replaced by a truly random bit.

8.1 Computationally Perfect Pseudorandom Bit Generators
205
Therefore, algorithm A, which distinguishes between pi,r and pi,r+1, can also
be used to compute Bi(x) from fi(x).
More precisely, we will derive a probabilistic polynomial algorithm ˜A(i, y)
from A that on inputs i ∈Ik and y := fi(x) computes Bi(x) with probability
> 1/2 + 1/P(k)Q(k), for the inﬁnitely many k ∈K. This contradiction to the
hard-core property of B will ﬁnish the proof of the theorem.
For k ∈K, we have
1
P(k) < prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,Q(k)
←
Zi)
−prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,0
←Zi)
=
Q(k)−1
X
r=0
(prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,r+1
←
Zi)
−prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,r
←Zi)).
Randomly choosing r, we expect that the r-th term in the sum is
> 1/P(k)Q(k).
On inputs i ∈Ik, y ∈Di, the algorithm ˜A works as follows:
1. Choose r, with 0 ≤r < Q(k), uniformly at random.
2. Independently choose random bits b1, b2, . . . , bQ(k)−r−1 and another ran-
dom bit b.
3. For y = fi(x) ∈Di, let
˜A(i, y) = ˜A(i, fi(x))
:=







b
if A(i, b1, . . . , bQ(k)−r−1, b,
Bi(fi(x)), . . . , Bi(f r
i (x)), f r+1
i
(x)) = 1,
1 −b otherwise.
If A distinguishes between pi,r and pi,r+1, it yields 1 with higher probability
if the (Q(k) −r)-th bit of its input is Bi(x) and not a random bit. Therefore,
we guess in our algorithm that the randomly chosen b is the desired hard-core
bit if A outputs 1.
We now check that ˜A indeed computes the hard-core bit with a non-
negligible probability. Let R be the random variable describing the choice
of r in the ﬁrst step of the algorithm. Since r is selected with respect to
the uniform distribution, we have prob(R = r) = 1/Q(k) for all r. Applying
Lemma B.13, we get

206
8. One-Way Functions and Pseudorandomness
prob( ˜A(i, fi(x)) = Bi(x) : i ←K(1k), x
u←Di)
= 1
2 + prob( ˜A(i, fi(x)) = b|Bi(x) = b) −prob( ˜A(i, fi(x)) = b)
= 1
2 +
Q(k)−1
X
r=0
prob(R = r) · (prob( ˜A(i, fi(x)) = b|Bi(x) = b, R = r)
−prob( ˜A(i, fi(x)) = b|R = r))
= 1
2 +
1
Q(k)
Q(k)−1
X
r=0
(prob( ˜A(i, fi(x)) = b|Bi(x) = b : i ←K(1k), x
u←Di)
−prob( ˜A(i, fi(x)) = b : i ←K(1k), x
u←Di))
= 1
2 +
1
Q(k)
Q(k)−1
X
r=0
(prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,r+1
←
Zi)
−prob(A(i, z, y) = 1 : i ←K(1k), (z, y)
pi,r
←Zi))
> 1
2 +
1
Q(k)P(k),
for the inﬁnitely many k ∈K. The probabilities in lines 2 and 3 are computed
with respect to i ←K(1k) and x
u←Di (and the random choice of the
elements bi, b, r). Since r is chosen independently, we can omit the conditions
R = r. Taking the probability prob( ˜A(i, fi(x)) = b|Bi(x) = b) conditional
on Bi(x) = b just means that the inputs to A in step 3 of the algorithm ˜A
are distributed according to pi,r+1. Finally, recall equation (8.1) for pi,r from
above.
Since B is a hard-core predicate, our computation yields the desired con-
tradiction, and the proof of Theorem 8.4 is complete.
2
Corollary 8.5 (Theorem of Blum and Micali). Pseudorandom bit generators
induced by one-way permutations with hard-core predicates are computation-
ally perfect.
Proof. Let A(i, z) be a probabilistic polynomial algorithm with inputs i ∈
Ik, z ∈{0, 1}Q(k) and output in {0, 1}. We deﬁne ˜A(i, z, y) := A(i, z), and
observe that
prob( ˜A(i, z, y) = 1 : i ←Ik, z ←{0, 1}Q(k), y ←Di)
= prob(A(i, z) = 1 : i ←Ik, z ←{0, 1}Q(k)),
and the corollary follows from Theorem 8.4 applied to ˜A.
2

8.2 Yao’s Theorem
207
8.2 Yao’s Theorem
Computationally perfect pseudorandom bit generators such as the ones in-
duced by one-way permutations are characterized by another unique feature:
it is not possible to predict the next bit in the pseudorandom sequence from
the preceding bits.
Deﬁnition 8.6. Let I = (Ik)k∈N be a key set with security parameter k, and
let G = (Gi : Xi −→{0, 1}l(k))i∈I be a pseudorandom bit generator with
polynomial stretch function l and key generator K:
1. A next-bit predictor for G is a probabilistic polynomial algorithm
A(i, z1 . . . zr) which, given i ∈Ik, outputs a bit (“the next bit”) from r
input bits zj (0 ≤r < l(k)).
2. G passes all next-bit tests if and only if for every next-bit predictor A
and every positive polynomial P ∈Z[X], there is a k0 ∈N such that for
all k ≥k0 and all 0 ≤r < l(k)
prob(A(i, Gi,1(x) . . . Gi,r(x)) = Gi,r+1(x) : i ←K(1k), x
u←Xi)
≤1
2 +
1
P(k) .
Here and in what follows we denote by Gi,j, 1 ≤j ≤l(k), the j-th bit
generated by Gi:
Gi(x) = (Gi,1(x), Gi,2(x), . . . , Gi,l(k)(x)).
Remarks:
1. A next-bit predictor has two inputs: the key i and a bit string z1 . . . zr
of variable length.
2. As usual, the probability in the deﬁnition is also taken over the random
choice of a key i with security parameter k. This means that when ran-
domly generating a key i, the probability of obtaining one for which A
has a signiﬁcant chance of predicting a next bit is negligibly small (see
Proposition 6.17).
Theorem 8.7 (Yao’s Theorem). Let I = (Ik)k∈N be a key set with security
parameter k, and let G = (Gi : Xi −→{0, 1}l(k))i∈I be a pseudorandom bit
generator with polynomial stretch function l and key generator K.
Then G is computationally perfect if and only if G passes all next-bit tests.

208
8. One-Way Functions and Pseudorandomness
Proof. Assume that G is computationally perfect and does not pass all
next-bit tests. Then there is a next-bit predictor A and a positive poly-
nomial P, such that for k in an inﬁnite subset K of N, we have a position
rk, 0 ≤rk < l(k), with qk,rk > 1/2 + 1/P(k), where
qk,r := prob(A(i, Gi,1(x) . . . Gi,r(x)) = Gi,r+1(x) : i ←K(1k), x
u←Xi).
By Proposition 6.18, we can compute the probabilities qk,r, r = 0, 1, 2, . . . ,
approximately with high probability, and we conclude that there is a proba-
bilistic polynomial algorithm R which on input 1k ﬁnds a position where the
next-bit predictor is successful:
prob
µ
qk,R(1k) > 1
2 +
1
2P(k)
¶
≥1 −
1
4P(k).
We deﬁne a probabilistic polynomial algorithm ˜A (a statistical test) for
inputs i ∈I and z = (z1, . . . , zl(k)) ∈{0, 1}l(k) as follows: Let r := R(1k) and
set
˜A(i, z) :=
(
1 if zr+1 = A(i, z1 . . . zr),
0 otherwise.
For truly random sequences, it is not possible to predict a next bit with
probability > 1/2. Thus, we have for the uniform distribution on {0, 1}l(k)
that
prob( ˜A(i, z) = 1 : i ←K(1k), z
u←{0, 1}l(k)) ≤1
2.
We obtain, for the inﬁnitely many k ∈K, that
| prob( ˜A(i, Gi(x)) = 1 : i ←K(1k), x
u←Xi)
−
prob( ˜A(i, z) = 1 : i ←K(1k), z
u←{0, 1}l(k)) |
>
µ
1 −
1
4P(k)
¶
·
µ1
2 +
1
2P(k) −1
2
¶
−
1
4P(k)
=
1
4P(k) −
1
8P 2(k) ≥
1
8P(k),
which is a contradiction to the assumption that G is computationally perfect.
Conversely, assume that the sequences generated by G pass all next-bit
tests, but can be distinguished from truly random sequences by a statistical
test A. This means that
| prob(A(i, Gi(x)) = 1 : i ←K(1k), x
u←Xi)
−prob(A(i, z) = 1 : i ←K(1k), z
u←{0, 1}l(k)) | >
1
P(k),
for some positive polynomial P and k in an inﬁnite subset K of N. Replacing
A by 1 −A, if necessary, we may drop the absolute value.

8.2 Yao’s Theorem
209
The proof now runs in a similar way to the proof of Theorem 8.4. For
k ∈K and i ∈Ik, we consider a sequence pi,0, pi,1, . . . , pi,l(k) of distributions
on {0, 1}l(k):
pi,0 := {(b1, . . . , bl(k)) : (b1, . . . , bl(k))
u←{0, 1}l(k)}
pi,1 := {(Gi,1(x), b2, . . . , bl(k)) : (b2, . . . , bl(k))
u←{0, 1}l(k)−1, x
u←Xi}
pi,2 := {(Gi,1(x), Gi,2(x), b3, . . . , bl(k)) :
(b3, . . . , bl(k))
u←{0, 1}l(k)−2, x
u←Xi}
...
pi,r := {(Gi,1(x), Gi,2(x), . . . , Gi,r(x), br+1, . . . , bl(k)) :
(br+1, . . . , bl(k))
u←{0, 1}l(k)−r, x
u←Xi}
...
pi,l(k) := {(Gi,1(x), Gi,2(x), . . . , Gi,l(k)(x)) : x
u←Xi}.
We start with truly random bit sequences, and in each step we replace one
more truly random bit from the left by a pseudorandom bit. Finally, in pi,l(k)
we have the distribution of the pseudorandom sequences.
Our assumption says that for k ∈K, algorithm A is able to distinguish
between the distribution pi,l(k) (of pseudorandom sequences) and the (uni-
form) distribution pi,0. Again, the basic idea of the proof now is that A must
be able to distinguish between two subsequent distributions pi,r and pi,r+1,
for some r. However, pi,r+1 diﬀers from pi,r in one position only, and there a
truly random bit is replaced by the next bit Gi,r+1(x) of the pseudorandom
sequence. Therefore, algorithm A can also be used to predict Gi,r+1(x).
More precisely, we will derive a probabilistic polynomial algorithm
˜A(i, z1, . . . , zr) that successfully predicts the next bit Gi,r+1(x) from
Gi,1(x), Gi,2(x), . . . , Gi,r(x) for some r = rk, for the inﬁnitely many k ∈K.
This contradiction to the assumption that G passes all next-bit tests will
ﬁnish the proof of the theorem.
Since A is able to distinguish between the uniform distribution and the
distribution induced by G, we get for k ∈K that
1
P(k) < prob(A(i, z) = 1 : i ←K(1k), z
pi,l(k)
←
{0, 1}l(k))
−prob(A(i, z) = 1 : i ←K(1k), z
pi,0
←{0, 1}l(k))
=
l(k)−1
X
r=0
(prob(A(i, z) = 1 : i ←K(1k), z
pi,r+1
←
{0, 1}l(k))
−prob(A(i, z) = 1 : i ←K(1k), z
pi,r
←{0, 1}l(k))).
We conclude that for k ∈K, there is some rk, 0 ≤rk < l(k), with

210
8. One-Way Functions and Pseudorandomness
1
P(k)l(k) < prob(A(i, z) = 1 : i ←K(1k), z
pi,rk+1
←
{0, 1}l(k))
−prob(A(i, z) = 1 : i ←K(1k), z
pi,rk
←{0, 1}l(k)).
This means that A(i, z) yields 1 for
z = (Gi,1(x), Gi,2(x), . . . , Gi,rk(x), b, brk+2, . . . , bl(k))
with higher probability if b is equal to Gi,rk+1(x) and not a truly random bit.
On inputs i ∈Ik, z1 . . . zr (0 ≤r < l(k)), algorithm ˜A is deﬁned as follows:
1. Choose truly random bits b, br+2, . . . , bl(k), and set
z := (z1, . . . , zr, b, br+2, . . . , bl(k)).
2. Let
˜A(i, z1 . . . zr) :=
(
b
if A(i, z) = 1,
1 −b if A(i, z) = 0.
Applying Lemma B.13, we get
prob( ˜A(i, Gi,1(x) . . . Gi,rk(x)) = Gi,rk+1(x) : i ←K(1k), x
u←Xi)
= 1
2 + prob( ˜A(i, Gi,1(x) . . . Gi,rk(x)) = b |
Gi,rk+1(x) = b : i ←K(1k), x
u←Xi)
−prob( ˜A(i, Gi,1(x) . . . Gi,rk(x)) = b : i ←K(1k), x
u←Xi)
= 1
2 + prob(A(i, z) = 1 : i ←K(1k), z
pi,rk+1
←
{0, 1}l(k))
−prob(A(i, z) = 1 : i ←K(1k), z
pi,rk
←{0, 1}l(k))
> 1
2 +
1
P(k)l(k),
for the inﬁnitely many k ∈K. This is the desired contradiction and completes
the proof of Yao’s Theorem.
2
Exercises
1. Let I = (Ik)k∈N be a key set with security parameter k, and let
G = (Gi)i∈I be a computationally perfect pseudorandom bit generator
with polynomial stretch function l. Let π = (πi)i∈I be a family of permu-
tations, where πi is a permutation of {0, 1}l(k) for i ∈Ik. Assume that π
can be computed by a polynomial algorithm Π, i.e., πi(y) = Π(i, y). Let
π ◦G be the composition of π and G: x 7→πi(Gi(x)).
Show that π ◦G is also a computationally perfect pseudorandom bit
generator.

Exercises
211
2. Give an example of a computationally perfect pseudorandom bit gener-
ator G = (Gi)i∈I and a family of permutations π, such that π ◦G is not
computationally perfect.
(According to Exercise 1, π cannot be computable in polynomial time.)
3. Let G = (Gi)i∈I be a pseudorandom bit generator with polynomial
stretch function l and key generator K.
Show that G is computationally perfect if and only if next bits in the
past cannot be predicted; i.e., for every probabilistic polynomial algo-
rithm A(i, zr+1 . . . zl(k)) which, given i ∈Ik, outputs a bit (“the next bit
in the past”) from l(k) −r input bits zj, there is a k0 ∈N such that for
all k ≥k0 and all 1 ≤r ≤l(k)
prob(Gi,r(x) = A(i, Gi,r+1(x) . . . Gi,l(k)(x)) : i ←K(1k), x
u←Xi)
≤1
2 +
1
P(k).
4. Let Q be a positive polynomial, and let G = (Gi)i∈I be a computationally
perfect pseudorandom bit generator with
Gi : {0, 1}Q(k) −→{0, 1}Q(k)+1
(i ∈Ik),
i.e., G extends the binary length of the seeds by 1. Recursively, deﬁne
the pseudorandom bit generators Gl by
G1 := G,
Gl
i(x) := (Gi,1(x), Gl−1
i
(Gi,2(x), . . . , Gi,Q(k)+1(x))).
As before, we denote by Gl
i,j(x) the j-th bit of Gl
i(x). Let l vary with
the security parameter k, i.e., l = l(k), and assume that l : N −→N is a
polynomial function.
Show that Gl is computationally perfect.
5. Prove the following stronger version of Yao’s Theorem (Theorem 8.7).
Let I = (Ik)k∈N be a key set with security parameter k, and let
G = (Gi : Xi −→{0, 1}l(k))i∈I be a pseudorandom bit generator with
polynomial stretch function l and key generator K.
Let f = (fi : Xi −→Yi)i∈I be a Monte-Carlo computable family of maps.
Then, the following statements are equivalent:
a. For every probabilistic polynomial algorithm A(i, y, z) and every pos-
itive polynomial P, there is a k0 ∈N such that for all k ≥k0 and all
0 ≤r < l(k)
prob(Gi,r+1(x) = A(i, fi(x), Gi,1(x) . . . Gi,r(x)) :
i ←K(1k), x
u←Xi)
≤1
2 +
1
P(k).

212
8. One-Way Functions and Pseudorandomness
b. For every probabilistic polynomial algorithm A(i, y, z) and every pos-
itive polynomial P, there is a k0 ∈N such that for all k ≥k0
| prob(A(i, fi(x), z) = 1 : i ←K(1k), x
u←Xi, z
u←{0, 1}l(k))
−prob(A(i, fi(x), Gi(x)) = 1 : i ←K(1k), x
u←Xi) |
≤
1
P(k).
In this exercise a setting is modeled in which some information fi(x)
about the seed x is known to the adversary.
6. Let f = (fi : Di −→Ri)i∈I be a family of one-way functions with key
generator K, and let B = (Bi : Di −→{0, 1}l(k))i∈I be a family of l-
bit predicates which is computable by a Monte Carlo algorithm (l a
polynomial function). Let Bi = (Bi,1, Bi,2, . . . , Bi,l(k)). We call B an l-bit
hard-core predicate for f (or simultaneously secure bits of f), if for every
probabilistic polynomial algorithm A(i, y, z1, . . . , zl) and every positive
polynomial P, there is a k0 ∈N such that for all k ≥k0
| prob(A(i, fi(x), Bi,1(x), . . . , Bi,l(k)(x)) = 1 : i ←K(1k), x
u←Di)
−prob(A(i, fi(x), z) = 1 : i ←K(1k), x
u←Di, z
u←{0, 1}l(k)) |
≤
1
P(k).
For l = 1, the deﬁnition is equivalent to our previous Deﬁnition 6.15 of
hard-core bits (see Exercise 7 in Chapter 6).
Now assume that B is an l-bit hard-core predicate, and let C
=
(Ci : {0, 1}l(k) −→{0, 1})i∈I be a Monte-Carlo computable family of
predicates with prob(Ci(x) = 0 : x
u←{0, 1}l(k)) = 1/2 for all i ∈I.
Show that the composition C ◦B, x ∈Di 7→Ci(Bi(x)), is a hard-core
predicate for f.
7. Let f = (fi : Di −→Ri)i∈I be a family of one-way functions with key
generator K, and let B = (Bi : Di −→{0, 1}l(k))i∈I be a family of l-bit
predicates for f. Let Bi = (Bi,1, Bi,2, . . . , Bi,l(k)). Assume that know-
ing fi(x) and Bi,1(x), . . . , Bi,j−1(x) does not help in the computation of
Bi,j(x). More precisely, assume that for every probabilistic polynomial
algorithm A(i, y, z) and every positive polynomial P, there is a k0 ∈N
such that for all 1 ≤j ≤l(k)
prob(A(i, fi(x), Bi,1(x) . . . Bi,j−1(x)) = Bi,j(x) : i ←K(1k), x
u←Xi)
≤1
2 +
1
P(k).
(In particular, the (Bi,j)i∈I are hard-core predicates for f.)
Show that the bits Bi,1, . . . , Bi,l are simultaneously secure bits for f.

Exercises
213
Examples:
The ⌊log2(|n|)⌋least-signiﬁcant bits are simultaneously secure for the
RSA (and the Square) one-way function (Exercise 12 in Chapter 7). If
p is a prime, p −1 = 2ta and a is odd, then the bits at the positions
t, t + 1, . . . , t + ⌊log2(|p|)⌋(counted from the right, starting with 0) are
simultaneously secure for the discrete exponential one-way function (Ex-
ercise 5 in Chapter 7).
8. Let f = (fi : Di −→Di)i∈I be a family of one-way permutations with
key generator K, and let B = (Bi)i∈I be an l-bit hard-core predicate
for f. Let G be the following pseudorandom bit generator with stretch
function lQ (Q a positive polynomial):
G :=
¡
Gi : Di −→{0, 1}l(k)Q(k)¢
k∈N,i∈Ik
x ∈Di 7−→(Bi(x), Bi(fi(x)), Bi(f 2
i (x)), . . . , Bi(f Q(k)−1
i
(x))).
Prove a statement that is analogous to Theorem 8.4. In particular, prove
that G is computationally perfect.
Example:
Taking the Square one-way permutation x 7→x2 (x ∈QRn, with n a
product of distinct primes) and the ⌊log2(|n|)⌋least-signiﬁcant bits, we
get the generalized Blum-Blum-Shub generator. It is used in the Blum-
Goldwasser probabilistic encryption scheme (see Chapter 9).

9. Provably Secure Encryption
This chapter deals with provable security. It is desirable that mathematical
proofs show that a given cryptosystem resists certain types of attacks. The
security of cryptographic schemes and randomness are closely related. An en-
cryption method provides secrecy only if the ciphertexts appear suﬃciently
random to the adversary. Therefore, probabilistic encryption algorithms are
required. The pioneering work of Shannon on provable security, based on his
information theory, is discussed in Section 9.1. For example, we prove that
Vernam’s one-time pad is a perfectly secret encryption. Shannon’s notion of
perfect secrecy may be interpreted in terms of probabilistic attacking algo-
rithms that try to distinguish between two candidate plaintexts (Section 9.2).
Unfortunately, Vernam’s one-time pad is not practical in most situations. In
Section 9.3, we give important examples of probabilistic encryption algo-
rithms that are practical. One-way permutations with hard-core predicates
yield computationally perfect pseudorandom bit generators (Chapter 8), and
these can be used to deﬁne “public-key pseudorandom one-time pads”, by
analogy to Vernam’s one-time pad: the plaintext bits are XORed with pseu-
dorandom bits generated from a short, truly random (one-time) seed. More
recent notions of provable security, which include the computational complex-
ity of attacking algorithms, are considered in Section 9.4. The computational
analogue of Shannon’s perfect secrecy, ciphertext-indistinguishability, is de-
ﬁned. A typical security proof for probabilistic public-key encryption schemes
is given. We show that the public-key one-time pads, introduced in Section
9.3, provide computationally perfect secrecy against passive eavesdroppers,
who perform ciphertext-only or chosen-plaintext attacks. Encryption schemes
that are secure against adaptively-chosen-ciphertext attacks, are considered
in Section 9.5. The security proof for Boneh’s SAEP is a typical proof in the
random oracle model, the proof for Cramer-Shoup’s public key encryption
scheme is based solely on a standard number-theoretic assumption and the
collision-resistance of the hash function used. Finally, a short introduction to
some results of the “unconditional security approach” is given in Section 9.6.
In this approach, the goal is to design practical cryptosystems which prov-
ably come close to perfect information-theoretic security, without relying on
unproven assumptions about problems from computational number theory.

216
9. Provably Secure Encryption
9.1 Classical Information-Theoretic Security
A deterministic public-key encryption algorithm E necessarily leaks infor-
mation to an adversary. For example, recall the small-message-space attack
on RSA (Section 3.3.3). An adversary intercepts a ciphertext c and knows
that the transmitted message m is from a small set {m1, . . . , mr} of pos-
sible messages. Then he easily ﬁnds out m by computing the ciphertexts
E(m1), . . . , E(mr) and comparing them with c. This example shows that
randomness in encryption is necessary to ensure real secrecy. Learning the
encryption c = E(m) of a message m, an adversary should not be able to
predict the ciphertext the next time when m is encrypted by E. This obser-
vation applies also to symmetric-key encryption schemes. Thus, to obtain a
provably secure encryption scheme, we have to study randomized encryption
algorithms.
Deﬁnition 9.1. An encryption algorithm E, which on input m ∈M out-
puts a ciphertext c ∈C, is called a randomized encryption if E is a non-
deterministic probabilistic algorithm.
The random behavior of a randomized encryption E is caused by its coin
tosses. These coin tosses may be considered as the random choice of a one-
time key (for each message to be encrypted a new random key is chosen,
independently of the previous choices). Take, for example, Vernam’s one-time
pad which is the classical example of a randomized (and provably secure)
cipher. We recall its deﬁnition.
Deﬁnition 9.2. Let n ∈N and M := C := {0, 1}n. The randomized en-
cryption E which encrypts a message m ∈M by XORing it bitwise with a
randomly and uniformly chosen bit sequence k
u←{0, 1}n of the same length,
E(m) := m ⊕k, is called Vernam’s one-time pad.
As the name indicates, key k is used only once: each time a message
m is encrypted, a new bit sequence is randomly chosen as the encryption
key. This choice of the key is viewed as the coin tosses of a probabilistic
algorithm. The security of a randomized encryption algorithm is related to
the level of randomness caused by its coin tosses. More randomness means
more security. Vernam’s one-time pad includes a maximum of randomness
and hence, provably, provides a maximum of security, as we will see below
(Theorem 9.5).
The problem with Vernam’s one-time pad is that truly random keys of the
same length as the message have to be generated and securely transmitted
to the recipient. This is rarely a practical operation (for an example, see
Section 2.1). Later (in Section 9.4), we will see how to obtain practical, but
still provably secure probabilistic encryption methods, by using high quality
pseudorandom bit sequences as keys.
The classical notion of security of an encryption algorithm is based
on Shannon’s information theory and his famous papers [Shannon48] and

9.1 Classical Information-Theoretic Security
217
[Shannon49]. Appendix B.4 gives an introduction to information theory and
its basic notions, such as entropy, uncertainty and mutual information.
We consider a randomized encryption algorithm E mapping plaintexts
m ∈M to ciphertexts c ∈C. We assume that the messages to be encrypted
are generated according to some probability distribution, i.e., M is assumed
to be a probability space. The distribution on M and the algorithm E induce
probability distributions on M × C and C (see Section 5.1). As usual, the
probability space induced on M × C is denoted by MC and, for m ∈M
and c ∈C, prob(c|m) denotes the probability that c is the ciphertext if
the plaintext is m. Analogously, prob(m|c) is the probability that m is the
plaintext if c is the ciphertext.1
Without loss of generality, we assume that prob(m) > 0 for all m ∈M
and that prob(c) > 0 for all c ∈C.
Deﬁnition 9.3 (Shannon). The encryption E is perfectly secret if C and M
are independent, i.e., the distribution of MC is the product of the distribu-
tions on M and C:
prob(m, c) = prob(m) · prob(c), for all m ∈M, c ∈C.
Perfect secrecy can be characterized in diﬀerent ways.
Proposition 9.4. The following statements are equivalent:
1. E is perfectly secret.
2. The mutual information I(M; C) = 0.
3. prob(m|c) = prob(m), for all m ∈M and c ∈C.
4. prob(c|m) = prob(c), for all m ∈M and c ∈C.
5. prob(c|m) = prob(c|m′), for all m, m′ ∈M and c ∈C.
6. prob(E(m) = c) = prob(c), for all m ∈M and c ∈C.
7. prob(E(m) = c) = prob(E(m′) = c), for all m, m′ ∈M and c ∈C;
i.e., the distribution of E(m) does not depend on m.
Proof. All statements of Proposition 9.4 are contained in, or immediately
follow from Proposition B.32. For the latter two statements, observe that
prob(c|m) = prob(E(m) = c),
by the deﬁnition of prob(E(m) = c) (see Chapter 5).
2
Remarks:
1. The probabilities in statement 7 only depend on the coin tosses of E. This
means, in particular, that the perfect secrecy of an encryption algorithm
E does not depend on the distribution of the plaintexts.
1 The notation is introduced on p. 328 in Appendix B.1.

218
9. Provably Secure Encryption
2. Let Eve be an attacker trying to discover information about the plain-
texts from the ciphertexts that she is able to intercept. Assume that Eve
is well informed and knows the distribution of the plaintexts. Then per-
fect secrecy means that her uncertainty about the plaintext (as precisely
deﬁned in information theory, see Appendix B.4, Deﬁnition B.27) is the
same whether or not she observes the ciphertext c: learning the ciphertext
does not increase her information about the plaintext m. Thus, perfect
secrecy really means unconditional security against ciphertext-only at-
tacks.
A perfectly secret randomized encryption E also withstands the other
types of attacks, such as the known-plaintext attacks and adaptively-
chosen-plaintext/ciphertext attacks discussed in Section 1.3. Namely, the
security is guaranteed by the randomness caused by the coin tosses of E.
Encrypting, say, r messages, means applying E r times. The coin tosses
within one of these executions of E are independent of the coin tosses in
the other executions (in Vernam’s one-time pad, this corresponds to the
fact that an individual key is chosen independently for each message).
Knowing details about previous encryptions does not help the adver-
sary. Each encryption is a new and independent random experiment and,
hence, the probabilities prob(c|m) are the same, whether we take them
conditional on other plaintext-ciphertext pairs (m′, c′) or not. Note that
additional knowledge of the adversary is included by conditioning the
probabilities on this knowledge.
3. The mutual information is a typical measure deﬁned in information the-
ory (see Deﬁnition B.30). It measures the average amount of information
Eve obtains about the plaintext m when learning the ciphertext c.
Vernam’s one-time pad is a perfectly secret encryption. More generally,
we prove the following theorem.
Theorem 9.5 (Shannon). Let M := C := K := {0, 1}n, and let E be a
one-time pad, which encrypts m := (m1, . . . , mn) ∈M by XORing it with a
random key string k := (k1, . . . , kn) ∈K, chosen independently from m:
E(m) := m ⊕k := (m1 ⊕k1, . . . , mn ⊕kn).
Then E is perfectly secret if and only if K is uniformly distributed.
Proof. We have
probMC(m, c) = probMK(m, m ⊕c) = probM(m) · probK(m ⊕c).
If M and C are independent, then
probM(m) · probC(c) = probMC(m, c) = probM(m) · probK(m ⊕c).
Hence

9.1 Classical Information-Theoretic Security
219
probK(m ⊕c) = probC(c), for all m ∈M.
This means that probK(k) is the same for all k ∈K. Thus, K is uniformly
distributed. Conversely, if K is uniformly distributed, then
probC(c) =
X
m∈M
probMK(m, m ⊕c) =
X
m∈M
probM(m) · probK(m ⊕c)
=
X
m∈M
probM(m) · 1
2n
= 1
2n .
Hence, C is also distributed uniformly, and we obtain:
probMC(m, c) = probMK(m, m ⊕c)
= probM(m) · probK(m ⊕c) = probM(m) · 1
2n
= probM(m) · probC(c).
Thus, M and C are independent.
2
Remarks:
1. Note that we do not consider the one-time pad as a cipher for plaintexts
of varying length: we have to assume that all plaintexts have the same
length n. Otherwise some information, namely the length of the plaintext,
leaks to adversary Eve, and the encryption could not be perfectly secret.
2. There is a high price to pay for the perfect secrecy of Vernam’s one-time
pad. For each message to be encrypted, of length n, n independent ran-
dom bits have to be chosen for the key. One might hope to ﬁnd a more
sophisticated, perfectly secret encryption method requiring less random-
ness. Unfortunately, this hope is destroyed by the following result which
was proven by Shannon ([Shannon49]).
Theorem 9.6. Let E be a randomized encryption algorithm with the de-
terministic extension ED : M × K −→C. Each time a message m ∈M is
encrypted, a one-time key k is chosen randomly from K (according to some
probability distribution on K), independently from the choice of m. Assume
that the plaintext m can be recovered from the ciphertext c and the one-time
key k (no other information is necessary for decryption). Then, if E is per-
fectly secret, the uncertainty of the keys cannot be smaller than the uncer-
tainty of the messages:
H(K) ≥H(M).
Remark. The uncertainty of a probability space M (see Deﬁnition B.27) is
maximal and equal to log2(|M|) if the distribution of M is uniform (Propo-
sition B.28). Hence, if M = {0, 1}n as in Theorem 9.5, then the entropy of

220
9. Provably Secure Encryption
any key set K – yielding a perfectly secret encryption – is at least n. Thus,
the random choice of k ∈K requires the choice of at least n truly random
bits.
Note at this point that the perfect secrecy of an encryption does not de-
pend on the distribution of the plaintexts (Proposition 9.4). Therefore, we
may assume that M is uniformly distributed and, as a consequence, that
H(M) = n.
Proof. The plaintext m can be recovered from the ciphertext c and the one-
time key k. This means that there is no uncertainty about the plaintext
if both the ciphertext and the key are known, i.e., the conditional entropy
H(M |KC) = 0 (see Deﬁnition B.30). Perfect secrecy means I(M; C) =
0 (Proposition 9.4), or equivalently, H(C) = H(C |M) (Proposition B.32).
Since M and K are assumed to be independent, I(K; M) = I(M; K) = 0
(Proposition B.32). We compute by use of Proposition B.31 and Deﬁnition
B.33 the following:
H(K) −H(M) = I(K; M) + H(K |M) −I(M; K) −H(M |K)
= H(K |M) −H(M |K)
= I(K; C |M) + H(K |CM) −I(M; C |K) −H(M |KC)
= I(K; C |M) + H(K |CM) −I(M; C |K)
≥I(K; C |M) −I(M; C |K)
= H(C |M) −H(C |KM) −H(C |K) + H(C |KM)
= H(C |M) −H(C |K)
= H(C) −H(C) + I(K; C) = I(K; C)
≥0.
The proof of Theorem 9.6 is ﬁnished.
2
Remark. In Vernam’s one-time pad it is not possible, without destroying
perfect secrecy, to use the same randomly chosen key for the encryption of two
messages. This immediately follows, for example, from Theorem 9.6. Namely,
such a modiﬁed Vernam one-time pad may be described as a probabilistic
algorithm from M × M to C × C, with the deterministic extension
M × M × K −→C × C,
(m, m′, k) 7→(m ⊕k, m′ ⊕k),
where M = K = C = {0, 1}n. Assuming the uniform distribution on M, we
have
H(K) = n < H(M × M) = 2n.
9.2 Perfect Secrecy and Probabilistic Attacks
We model the behavior of an adversary Eve by probabilistic algorithms, and
show the relation between the failure of such algorithms and perfect secrecy.

9.2 Perfect Secrecy and Probabilistic Attacks
221
In Section 9.3, we will slightly modify this model by restricting the computing
power of the adversary to polynomial resources.
As in Section 9.1, let E be a randomized encryption algorithm that maps
plaintexts m ∈M to ciphertexts c ∈C and is used by Alice to encrypt her
messages. As before, Alice chooses the messages m ∈M according to some
probability distribution. The distribution on M and the algorithm E induce
probability distributions on M × C and C. prob(m, c) is the probability that
m is the chosen message and that the probabilistic encryption of m yields c.
We ﬁrst consider a probabilistic algorithm A which on input c ∈C out-
puts a plaintext m ∈M. Algorithm A models an adversary Eve performing
a ciphertext-only attack and trying to decrypt ciphertexts. Recall that the
coin tosses of a probabilistic algorithm are independent of any other random
events in the given setting (see Chapter 5). Thus, the coin tosses of A are
independent of the choice of the message and the coin tosses of E. This is a
reasonable model, because sender Alice, generating and encrypting messages,
and adversary Eve operate independently. We have
prob(m, c, A(c) = m) = prob(m, c) · prob(A(c) = m),
for m ∈M and c ∈C (see Chapter 5). prob(A(c) = m) is the conditional
probability that A(c) yields m, assuming that m and c are ﬁxed. It is de-
termined by the coin tosses of A. The probability of success of A is given
by
probsuccess(A) :=
X
m,c
prob(m, c) · prob(A(c) = m)
=
X
m,c
prob(m) · prob(E(m) = c) · prob(A(c) = m)
= prob(A(c) = m : m ←M, c ←E(m)).
Proposition 9.7. If E is perfectly secret, then for every probabilistic algo-
rithm A which on input c ∈C outputs a plaintext m ∈M
probsuccess(A) ≤max
m∈M prob(m).
Proof.
probsuccess(A) =
X
m,c
prob(m, c) · prob(A(c) = m)
=
X
c
prob(c) ·
X
m
prob(m|c) · prob(A(c) = m)
=
X
c
prob(c) ·
X
m
prob(m) · prob(A(c) = m)
(by Proposition 9.4)

222
9. Provably Secure Encryption
≤max
m∈M prob(m) ·
X
c
prob(c) ·
X
m
prob(A(c) = m)
= max
m∈M prob(m),
and the proposition follows.
2
Remarks:
1. In Proposition 9.7, as in the whole of Section 9.2, we do not assume
any limits for the resources of the algorithms. The running time and the
memory requirements may be exponential.
2. Proposition 9.7 says that for a perfectly secret encryption, selecting a
plaintext with maximal probability from M, without looking at the ci-
phertext, is optimum under all attacks that try to derive the plaintext
from the ciphertext. If M is uniformly distributed, then randomly select-
ing a plaintext is an optimal strategy.
Perfect secrecy may also be described in terms of distinguishing algo-
rithms.
Deﬁnition 9.8. A distinguishing algorithm for E is a probabilistic algorithm
A which on inputs m0, m1 ∈M and c ∈C outputs an m ∈{m0, m1}.
Remark. A distinguishing algorithm A models an adversary Eve, who, given
a ciphertext c and two plaintext candidates m0 and m1, tries to ﬁnd out
which one of the both is the correct plaintext, i.e., which one is encrypted as
c. Again, recall that the coin tosses of A are independent of a random choice
of the messages and the coin tosses of the encryption algorithm (see Chapter
5). Thus, the adversary Eve and the sender Alice, generating and encrypting
messages, are modeled as working independently.
Proposition 9.9. E is perfectly secret if and only if for every probabilistic
distinguishing algorithm A and all m0, m1 ∈M,
prob(A(m0, m1, c) = m0 : c ←E(m0))
=
prob(A(m0, m1, c) = m0 : c ←E(m1)).
Proof. E is perfectly secret if and only if the distribution of E(m) does not
depend on m (Proposition 9.4). Thus, the equality obviously holds if E is
perfectly secret.
Conversely, assume that E is not perfectly secret. There are no limits for
the running time of our algorithms. Then there is an algorithm P which starts
with a description of the encryption algorithm E and analyzes the paths and
coin tosses of E and, in this way, computes the probabilities prob(c|m):
P(c, m) := prob(c|m), for all c ∈C, m ∈M.
We deﬁne the following distinguishing algorithm:

9.2 Perfect Secrecy and Probabilistic Attacks
223
A(m0, m1, c) :=
(
m0 if P(c, m0) > P(c, m1),
m1 otherwise.
Since E is not perfectly secret, there are m0, m1 ∈M and c0 ∈C, such that
P(c0, m0) = prob(c0 |m0) > P(c0, m1) = prob(c0 |m1) (Proposition 9.4). Let
C0 := {c ∈C | prob(c|m0) > prob(c|m1)} and
C1 := {c ∈C | prob(c|m0) ≤prob(c|m1)}.
Then A(m0, m1, c) = m0 for c ∈C0, and A(m0, m1, c) = m1 for c ∈C1. We
compute
prob(A(m0, m1, c) = m0 : c ←E(m0))
−prob(A(m0, m1, c) = m0 : c ←E(m1))
=
X
c∈C
prob(c|m0) · prob(A(m0, m1, c) = m0)
−
X
c∈C
prob(c|m1) · prob(A(m0, m1, c) = m0)
=
X
c∈C0
prob(c|m0) −prob(c|m1)
≥prob(c0 |m0) −prob(c0 |m1)
> 0,
and see that a violation of perfect secrecy causes a violation of the equality
condition. The proof of the proposition is ﬁnished.
2
Proposition 9.10. E is perfectly secret if and only if for every probabilistic
distinguishing algorithm A and all m0, m1 ∈M, with m0 ̸= m1,
prob(A(m0, m1, c) = m : m
u←{m0, m1}, c ←E(m)) = 1
2.
Proof.
prob(A(m0, m1, c) = m : m
u←{m0, m1}, c ←E(m))
= 1
2 · prob(A(m0, m1, c) = m0 : c ←E(m0))
+ 1
2 · prob(A(m0, m1, c) = m1 : c ←E(m1))
= 1
2 + 1
2 · (prob(A(m0, m1, c) = m0 : c ←E(m0))
−prob(A(m0, m1, c) = m0 : c ←E(m1)),
and the proposition follows from Proposition 9.9.
2

224
9. Provably Secure Encryption
Remark. Proposition 9.10 characterizes a perfectly secret encryption scheme
in terms of a passive eavesdropper A, who performs a ciphertext-only attack.
But, as we observed before, the statement would remain true, if we model
an (adaptively-)chosen-plaintext/ciphertext attacker by algorithm A (see the
remark after Proposition 9.4).
9.3 Public-Key One-Time Pads
Vernam’s one-time pad is provably secure (Section 9.1) and thus appears to
be a very attractive encryption method. However, there is the problem that
truly random keys of the same length as the message have to be generated
and securely transmitted to the recipient. The idea now is to use high qual-
ity (“cryptographically secure”) pseudorandom bit sequences as keys and to
obtain in this way practical, but still provably secure randomized encryption
methods.
Deﬁnition 9.11. Let I = (Ik)k∈N be a key set with security parameter
k, and let G = (Gi)i∈I, Gi : Xi −→{0, 1}l(k) (i ∈Ik), be a pseudorandom
bit generator with polynomial stretch function l and key generator K (see
Deﬁnition 8.2).
The probabilistic polynomial encryption algorithm E(i, m) which, given
(a public key) i ∈Ik, encrypts a message m ∈{0, 1}l(k) by bitwise XORing
it with the pseudorandom sequence Gi(x), generated by Gi from a randomly
and uniformly chosen seed x ∈Xi,
E(i, m) := m ⊕Gi(x),
x
u←Xi,
is called the pseudorandom one-time pad induced by G. Keys i are assumed
to be generated by K.
Example. Let f = (fi : Di −→Di)i∈I be a family of one-way permutations
with hard-core predicate B = (Bi : Di −→{0, 1})i∈I, and let Q be a poly-
nomial. f, B and Q induce a pseudorandom bit generator G(f, B, Q) with
stretch function Q (see Deﬁnition 8.3), and hence a pseudorandom one-time
pad.
We will see in Section 9.4 that computationally perfect pseudorandom
generators (Deﬁnition 8.2), such as the G(f, B, Q)s, lead to provably secure
encryption schemes. Nevertheless, one important problem remains unsolved:
how to transmit the secret one-time key – the randomly chosen seed x – to
the recipient of the message?
If G is induced by a family of trapdoor permutations with hard-core pred-
icate, there is an easy answer. We send x hidden by the one-way function
together with the encrypted message. Knowing the trapdoor, the recipient is
able to determine x.

9.3 Public-Key One-Time Pads
225
Deﬁnition 9.12. Let I = (Ik)k∈N be a key set with security parameter k,
and let Q be a positive polynomial. Let f = (fi : Di −→Di)i∈I be a family of
trapdoor permutations with hard-core predicate B and key generator K. Let
G(f, B, Q) be the induced pseudorandom bit generator with stretch function
Q. For every recipient of messages, a public key i ∈Ik (and the associated
trapdoor information) is generated by K.
The probabilistic polynomial encryption algorithm E(i, m) which en-
crypts a message m ∈{0, 1}Q(k) as
E(i, m) := (m ⊕G(f, B, Q)i(x), f Q(k)
i
(x)),
with x chosen randomly and uniformly from Di (for each message m), is
called the public-key one-time pad induced by f, B and Q.
Remarks:
1. Recall that we get G(f, B, Q)i(x) by repeatedly applying fi to x and
taking the hard-core bits Bi(f j
i (x)) of the sequence
x, fi(x), f 2
i (x), f 3
i (x), . . . , f Q(k)−1
i
(x)
(see Deﬁnition 8.3). In order to encrypt the seed x, we then apply fi once
more. Note that we cannot take f j
i (x) with j < Q(k) as the encryption
of x, because this would reveal bits from the sequence G(f, B, Q)i(x).
2. Since fi is a permutation of Di and the recipient Bob knows the trapdoor,
he has an eﬃcient algorithm for f −1
i
. He is able to compute the sequence
x, fi(x), f 2
i (x), f 3
i (x), . . . , f Q(k)−1
i
(x)
from f Q(k)
i
(x), by repeatedly applying f −1
i
. In this way, he can easily
decrypt the ciphertext.
3. In the public-key one-time pad, the basic pseudorandom one-time pad is
augmented by an asymmetric (i.e., public-key) way of transmitting the
one-time symmetric encryption key x.
4. Like the basic pseudorandom one-time pad, the augmented version is
provably secure against passive attacks (see Theorem 9.16). Supplying the
encrypted key f Q(k)
i
(x) does not diminish the secrecy of the encryption
scheme.
5. Pseudorandom one-time pads and public-key one-time pads are straight-
forward analogies to the classical probabilistic encryption method, Ver-
nam’s one-time pad. We will see in Section 9.4 that more recent notions of
secrecy, such as indistinguishability (introduced in [GolMic84]), are also
analogous to the classical notion in Shannon’s work. The statements on
the secrecy of pseudorandom and public-key one-time pads (see Section
9.4) are analogous to the classical results by Shannon.

226
9. Provably Secure Encryption
6. The notion of probabilistic public-key encryption, whose security may
be rigorously proven in a complexity theoretic model, was suggested
by Goldwasser and Micali ([GolMic84]). They introduced the hard-core
predicates of trapdoor functions (or, more generally, trapdoor predicates)
as the basic building blocks of such schemes. The implementation of
probabilistic public-key encryption, given in [GolMic84] and known as
Goldwasser-Micali probabilistic encryption (see Exercise 7), is based on
the quadratic residuosity assumption (Deﬁnition 6.11). During encryp-
tion, messages are expanded by a factor proportional to the security
parameter k. Thus, this implementation is quite wasteful in space and
bandwidth and is therefore not really practical. The public-key one-time
pads, introduced in [BluGol85] and [BluBluShu86], avoid this large mes-
sage expansion. They are the eﬃcient implementations of (asymmetric)
probabilistic encryption.
9.4 Passive Eavesdroppers
We study the security of public-key encryption schemes against passive eaves-
droppers, who perform ciphertext-only attacks. In a public-key encryption
scheme, a ciphertext-only attacker (as everybody) can encrypt messages of
his choice at any time by using the publicly known key. Therefore, secu-
rity against ciphertext-only attacks in a public-key encryption scheme also
includes security against adaptively-chosen-plaintext attacks.
The stronger notion of security against adaptively-chosen-ciphertext at-
tacks is considered in the subsequent Section 9.5.
Throughout this section we consider a probabilistic polynomial encryption
algorithm E(i, m), such as the pseudorandom or public-key one-time pads
deﬁned in Section 9.3. Here, I = (Ik)k∈N is a key set with security parameter k
and, for every i ∈I, E maps plaintexts m ∈Mi to ciphertexts c := E(i, m) ∈
Ci. The keys i are generated by a probabilistic polynomial algorithm K and
are assumed to be public. In our examples, the encryption E is derived from
a family f = (fi)i∈I of one-way permutations, and the index i is the public
key of the recipient.
We deﬁne distinguishing algorithms A for E completely analogous to Def-
inition 9.8. Now the computational resources of the adversary, modeled by
A, are limited. A is required to be polynomial.
Deﬁnition 9.13. A probabilistic polynomial distinguishing algorithm for E
is a probabilistic polynomial algorithm A(i, m0, m1, c) which on inputs i ∈
I, m0, m1 ∈Mi and c ∈Ci outputs an m ∈{m0, m1}.
Below we show that pseudorandom one-time pads induced by computa-
tionally perfect pseudorandom bit generators have computationally perfect
secrecy. This result is analogous to the classical result by Shannon that Ver-
nam’s one-time pad is perfectly secret. Using truly random bit sequences as

9.4 Passive Eavesdroppers
227
the key in the one-time pad, the probability of success of an attack with
unlimited resources – which tries to distinguish between two candidate plain-
texts – is equal to 1/2 ; so there is no use in observing the ciphertext. Using
computationally perfect pseudorandom bit sequences, the probability of suc-
cess of an attack with polynomial resources is, at most, negligibly more than
1/2 .
Deﬁnition 9.14. The encryption E is called ciphertext-indistinguishable or
(for short) indistinguishable, if for every probabilistic polynomial distinguish-
ing algorithm A(i, m0, m1, c) and every probabilistic polynomial sampling al-
gorithm S, which on input i ∈I yields S(i) = {m0, m1} ⊂Mi, and every
positive polynomial P ∈Z[X], there is a k0 ∈N such that for all k ≥k0:
prob(A(i, m0, m1, c) = m : i ←K(1k), {m0, m1} ←S(i),
m
u←{m0, m1}, c ←E(i, m)) ≤1
2 +
1
P(k).
Remarks:
1. The deﬁnition is a deﬁnition in the “public-key model”: the keys i are
public and hence available to the distinguishing algorithms. It can be
adapted to a private-key setting. See the analogous remark after the
deﬁnition of pseudorandom generators (Deﬁnition 8.2).
2. Algorithm A models a passive adversary, who performs a ciphertext-only
attack. But, everybody knows the public key i and can encrypt messages
of his choice at any time. This implies that the adversary algorithm
A may include the encryption of messages of its choice. We see that
ciphertext-indistinguishability, as deﬁned here, means security against
adaptively-chosen-plaintext attacks. Chosen-ciphertext attacks are con-
sidered in Section 9.5.
3. The output of the sampling algorithm S is a subset {m0, m1} with two
members, and therefore m0 ̸= m1.
If the message spaces Mi are unrestricted, like {0, 1}∗, then it is usually
required that the two candidate plaintexts m0, m1, generated by S, have
the same bit length. This additional requirement is reasonable. Typically,
the length of the plaintext and the length of the ciphertext are closely
related. Hence, information about the length of the plaintexts necessarily
leaks to an adversary, and plaintexts of diﬀerent length can easily be
distinguished. For the same reason, we considered Vernam’s one-time
pad as a cipher for plaintexts of a ﬁxed length in Section 9.1 (see the
remark after Theorem 9.5).
In this section, we consider only schemes where all plaintexts are of the
same bit length.
4. In view of Proposition 9.10, the deﬁnition is the computational analogy
to the notion of perfect secrecy, as deﬁned by Shannon.

228
9. Provably Secure Encryption
Non-perfect secrecy means that some algorithm A is able to distinguish
between distinct plaintexts m0 and m1 (given the ciphertext) with a prob-
ability > 1/2. The running time of A may be exponential. If an encryption
scheme is not ciphertext-indistinguishable, then an algorithm with poly-
nomial running time is able to distinguish between distinct plaintexts m0
and m1 (given the ciphertext) with a probability signiﬁcantly larger than
1/2. In addition, the plaintexts m0 and m1 can be found in polynomial
time by some probabilistic algorithm S. This additional requirement is
adequate. A secrecy problem can only exist for messages which can be
generated in practice by using a probabilistic polynomial algorithm. The
message generation is modeled uniformly by S for all keys i.2
5. The notion of ciphertext-indistinguishability was introduced by Gold-
wasser and Micali ([GolMic84]). They call it polynomial security or
polynomial-time indistinguishability. Ciphertext-indistinguishable encryp-
tion schemes are also called schemes with indistinguishable encryptions.
Another notion of security was introduced in [GolMic84]. An encryp-
tion scheme is called semantically secure, if it has the following prop-
erty: Whatever a passive adversary Eve is able to compute about the
plaintext in polynomial time given the ciphertext, she is also able to
compute in polynomial time without the ciphertext. The messages to
be encrypted are assumed to be generated by a probabilistic polyno-
mial algorithm. Semantic security is equivalent to indistinguishability
([GolMic84]; [MicRacSlo88]; [WatShiIma03]; [Goldreich04]).
6. Recall that the execution of a probabilistic algorithm is an independent
random experiment (see Chapter 5). Thus, the coin tosses of the distin-
guishing algorithm A are independent of the coin tosses of the sampling
algorithm S and the coin tosses of the encryption algorithm E. This
reﬂects the fact that sender and adversary operate independently.
7. The probability in our present deﬁnition is also taken over the random
generation of a key i, with a given security parameter k. Even for very
large k, there may be insecure keys i such that A is able to distinguish
successfully between two plaintext candidates. However, when randomly
generating keys by the key generator, the probability of obtaining an
insecure one is negligibly small (see Proposition 6.17 for a precise state-
ment).
8. Ciphertext-indistinguishable encryption algorithms are necessarily ran-
domized encryptions. When encrypting a plaintext m twice, the proba-
bility that we get the same ciphertext must be negligibly small. Other-
wise it would be easy to distinguish between two messages m0 and m1
by comparing the ciphertext with encryptions of m0 and m1.
2 By applying a (more general) non-uniform model of computation (non-uniform
polynomial-time algorithms instead of probabilistic polynomial algorithms, see,
for example, [Goldreich99], [Goldreich04]), one can dispense with the sampling
algorithm S.

9.4 Passive Eavesdroppers
229
Theorem 9.15. Let E be the pseudorandom one-time pad induced by a com-
putationally perfect pseudorandom bit generator G. Then E is ciphertext-
indistinguishable.
Proof. The proof runs in exactly the same way as the proof of Theorem 9.16,
yielding a contradiction to the assumption that G is computationally perfect.
See below for the details.
2
General pseudorandom one-time pads leave open how the secret encryp-
tion key – the randomly chosen seed – is securely transmitted to the receiver
of the message. Public-key one-time pads provide an answer. The key is en-
crypted by the underlying one-way permutation and becomes part of the en-
crypted message (see Deﬁnition 9.12). The indistinguishability is preserved.
Theorem 9.16. Let E be the public-key one-time pad induced by a family
f = (fi : Di −→Di)i∈I of trapdoor permutations with hard-core predicate B.
Then E is ciphertext-indistinguishable.
Remark. Theorem 9.16 states that public-key cryptography provides vari-
ants of the one-time pad which are provably secure and practical. XORing
the plaintext with a pseudorandom bit sequence generated from a short ran-
dom seed by a trapdoor permutation with hard-core predicate (e.g. use the
Blum-Blum-Shub generator or the RSA generator, see Section 8.1,) yields
an encryption with indistinguishability. Given the ciphertext, an adversary
is provably not able to distinguish between two plaintexts. In addition, it is
possible in this public-key one-time pad to securely transmit the key string
(more precisely, the seed of the pseudorandom sequence) to the recipient,
simply by encrypting it by means of the one-way function.
Of course, the security proof for a public-key one-time pad, such as the
RSA- or Blum-Blum-Shub-based one-time pad, is conditional. It depends on
the validity of basic unproven (though widely believed) assumptions, such as
the RSA assumption (Deﬁnition 6.7), or the factoring assumption (Deﬁnition
6.9).
Computing the pseudorandom bit sequences using a one-way permutation
requires complex computations, such as exponentiation and modular reduc-
tions. Thus, the classical private-key symmetric encryption methods, like the
DES (see Chapter 2) or stream ciphers, using shift registers to generate pseu-
dorandom sequences (see, e.g., [MenOorVan96], Chapter 6), are much more
eﬃcient than public-key one-time pads, and hence are better suited for large
amounts of data.
However, notice that the one-way function of the Blum-Blum-Shub gen-
erator (see Chapter 8) is a quite simple one. Quadratic residues x mod n
are squared: x 7→x2 mod n. A public-key one-time pad, whose eﬃciency is
comparable to standard RSA encryption, can be implemented based on this
generator.
Namely, suppose n = pq, with distinct primes p, q ≡3 mod 4 of binary
length k. Messages m of length l are to be encrypted. In order to encrypt m,

230
9. Provably Secure Encryption
we randomly choose an element from Z∗
n and square it to get a random x in
QRn. This requires O(k2) steps. To get the pseudorandom sequence and to
encrypt the random seed x we have to compute l squares modulo n, which
comes out to O(k2l) steps. XORing requires O(l) steps. Thus, encryption is
ﬁnished in O(k2l) steps.
To decrypt, we ﬁrst compute the seed x from x2l by drawing the square
root l times in QRn. We can do this by drawing the square roots modulo p and
modulo q, and applying the Chinese Remainder Theorem (see Proposition
A.62).
Assume, in addition, that we have even chosen p, q ≡7 mod 8. Then
p + 1/8 is in N, and for every quadratic residue a ∈QRp we get a square root
b which again is an element of QRp by setting
b = a(p+1)/4 =
³
a(p+1)/8´2
.
Here, note that
b2 = a(p+1)/2 = a · a(p−1)/2 = a,
since a(p−1)/2 =
³
a
p
´
= 1 for quadratic residues a ∈QRp (Proposition A.52).
Thus, we can derive x mod p from y = x2l by
x mod p = yu mod p, with u =
µp + 1
4
¶l
mod (p −1).
The exponent u can be reduced modulo p −1, since ap−1 = 1 for all a ∈Z∗
p.
We assume that the message length l is ﬁxed. Then the exponent u can
be computed in advance, and we see that ﬁguring out x mod p (or x mod
q) requires at most k squarings applying the repeated squaring algorithm
(Algorithm A.26). Thus, it can be done in O(k3) steps.
Reducing x2l mod p (and x2l mod q) at the beginning and applying the
Chinese Remainder Theorem at the end requires at most O(k2) steps. Sum-
marizing, we see that computing x requires O(k3) steps. Now, completing the
decryption essentially means performing an encryption whose cost is O(k2l),
as we saw above. Hence, the complete decryption procedure takes O(k3+k2l)
steps. If l = O(k), this is equal to O(k3) and thus of the same order as the
running time of an RSA encryption.
The eﬃciency of the Blum-Blum-Shub-based public-key one-time pad (as
well as that of the RSA-based one) can be increased even further, by modify-
ing the generation of the pseudorandom bit sequence. Instead of taking only
the least-signiﬁcant bit of x2j mod n, you may take the ⌊log2(|n|)⌋least-
signiﬁcant bits after each squaring. These bits form a ⌊log2(|n|)⌋-bit hard-
core predicate of the modular squaring function and are simultaneously secure
(see Exercise 7 in Chapter 8). The resulting public-key one-time pad is called
Blum-Goldwasser probabilistic encryption ([BluGol85]). It is also ciphertext-
indistinguishable (see Exercise 8 in Chapter 8).

9.4 Passive Eavesdroppers
231
Our considerations are not only valid asymptotically, as the O notation
might suggest. Take for example k = 512 and |n| = 1024, and encrypt 1024-
bit messages m. In the x2 mod n public-key one-time pad, always use the
log2(|n|) = 10 least-signiﬁcant bits. To encrypt a message m, about 100 mod-
ular squarings of 1024-bit numbers are necessary. To decrypt a ciphertext,
we ﬁrst determine the seed x by at most 1024 = 512+512 modular squarings
and multiplications of 512-bit numbers, and then compute the plaintext using
about 100 modular squarings, as in encryption. Encrypting and decrypting
messages m ∈Zn by RSA requires up to 1024 modular squarings and mul-
tiplications of 1024-bit (encryption) or 512-bit (decryption) numbers, with
the actual number depending on the size of the encryption and decryption
exponents. In the estimates, we did not count the (few) operations associated
with applying the Chinese Remainder Theorem during decryption.
Proof (of Theorem 9.16). Let K be the key generator of f and G :=
G(f, B, Q) be the pseudorandom bit generator (Chapter 8). Recall that
E(i, m) = (m ⊕Gi(x), f Q(k)
i
(x)),
where i ∈I = (Ik)k∈N, m ∈{0, 1}Q(k) (for i ∈Ik) and the seed x is randomly
and uniformly chosen from Di.
The image of the uniform distribution on Di under f Q(k)
i
is again the
uniform distribution on Di, because fi is bijective.
You can obtain a proof of Theorem 9.15 simply by omitting “×Di”,
y, f Q(k)
i
(x) everywhere (and by replacing Q(k) by l(k)). Our proof yields
a contradiction to Theorem 8.4. In the proof of Theorem 9.15, you get the
completely analogous contradiction to the assumption that the pseudoran-
dom bit generator G is computationally perfect.
Now assume that there is a probabilistic polynomial distinguishing algo-
rithm A(i, m0, m1, c, y), with inputs i ∈I, m0, m1 ∈Mi, c ∈{0, 1}Q(k) (if
i ∈Ik), y ∈Di, a probabilistic polynomial sampling algorithm S(i) and a
positive polynomial P, such that
prob(A(i, m0, m1, c, y) = m :
i ←K(1k), {m0, m1} ←S(i), m
u←{m0, m1}, (c, y) ←E(i, m))
= prob(A(i, m0, m1, m ⊕Gi(x), f Q(k)
i
(x)) = m :
i ←K(1k), {m0, m1} ←S(i), m
u←{m0, m1}, x
u←Di)
> 1
2 +
1
P(k),
for inﬁnitely many k. We deﬁne a probabilistic polynomial statistical test
˜A = ˜A(i, z, y), with inputs i ∈I, z ∈{0, 1}Q(k) (if i ∈Ik) and y ∈Di and
output in {0, 1}:
1. Apply S(i) and get {m0, m1} := S(i).

232
9. Provably Secure Encryption
2. Randomly choose m in {m0, m1} : m
u←{m0, m1}.
3. Let
˜A(i, z, y) :=
(
1 if A(i, m0, m1, m ⊕z, y) = m,
0 otherwise.
The statistical test ˜A will be able to distinguish between the pseudorandom
sequences produced by G and truly random, uniformly distributed sequences,
thus yielding the desired contradiction. We have to compute the probability
prob( ˜A(i, z, y) = 1 : i ←K(1k), (z, y) ←{0, 1}Q(k) × Di)
for both the uniform distribution on {0, 1}Q(k) × Di and the distribution
induced by (G, f Q(k)). More precisely, we have to compare the probabilities
pk,G := prob( ˜A(i, Gi(x), f Q(k)
i
(x)) = 1 : i ←K(1k), x
u←Di)
and
pk,uni := prob( ˜A(i, z, y) = 1 : i ←K(1k), z
u←{0, 1}Q(k), y
u←Di).
Our goal is to prove that
pk,G −pk,uni >
1
P(k),
for inﬁnitely many k. This contradicts Theorem 8.4 and ﬁnishes the proof.
From the deﬁnition of ˜A we get
prob( ˜A(i, z, y) = 1 : i ←K(1k), (z, y) ←{0, 1}Q(k) × Di)
= prob(A(i, m0, m1, m ⊕z, y) = m : i ←K(1k),
(z, y) ←{0, 1}Q(k) × Di, {m0, m1} ←S(i), m
u←{m0, m1}).
Since the random choice of (z, y) and the random choice of m in the proba-
bilistically computed pair S(i) are independent, we may switch them in the
probability and obtain
prob( ˜A(i, z, y) = 1 : i ←K(1k), (z, y) ←{0, 1}Q(k) × Di)
= prob(A(i, m0, m1, m ⊕z, y) = m : i ←K(1k),
{m0, m1} ←S(i), m
u←{m0, m1}, (z, y) ←{0, 1}Q(k) × Di).
Now consider pk,G:
pk,G = prob(A(i, m0, m1, m ⊕Gi(x), f Q(k)
i
(x)) = m :
i ←K(1k), {m0, m1} ←S(i), m
u←{m0, m1}, x
u←Di).
We assumed that this probability is > 1/2 + 1/P(k), for inﬁnitely many k.
The following computation shows that pk,uni = 1/2 for all k, thus com-
pleting the proof. We have

9.5 Chosen-Ciphertext Attacks
233
pk,uni = prob(A(i, m0, m1, m ⊕z, y) = m : i ←K(1k), {m0, m1} ←S(i),
m
u←{m0, m1}, z
u←{0, 1}Q(k), y
u←Di)
=
X
i
prob(K(1k) = i) ·
X
m0,m1
prob(S(i) = {m0, m1}) · pi,m0,m1,
with
pi,m0,m1 = prob(A(i, m0, m1, m ⊕z, y) = m :
m
u←{m0, m1}, z
u←{0, 1}Q(k), y
u←Di).
Vernam’s one-time pad is perfectly secret (Theorem 9.5). Thus, the latter
probability is equal to 1/2 by Proposition 9.10. Note that the additional input
y
u←Di of A, not appearing in Proposition 9.10, can be viewed as additional
coin tosses of A. So, we ﬁnally get the desired equation
pk,uni =
X
i
prob(K(1k) = i)
X
m0,m1
prob(S(i) = {m0, m1}) · 1
2 = 1
2,
and the proof of the theorem is ﬁnished.
2
9.5 Chosen-Ciphertext Attacks
In the preceding section, we studied encryption schemes which are ciphertext-
indistinguishable and provide security against a passive eavesdropper, who
performs a ciphertext-only or an adaptively-chosen-plaintext attack. The
schemes may still be insecure against an attacker, who manages to get tem-
porary access to the decryption device and who executes a chosen-ciphertext
or an adaptively-chosen-ciphertext attack (see Section 1.3 for a classiﬁcation
of attacks). Given a ciphertext c, such an attacker Eve tries to get informa-
tion about the plaintext. In the course of her attack, Eve can get decryptions
of ciphertexts c′ from the decryption device, with the only restriction that
c′ ̸= c. She has temporary access to a “decryption oracle”.
Consider, for example, the eﬃcient implementation of Goldwasser-Micali’s
probabilistic encryption, which we called public-key one-time pad and which
we studied in the preceding sections. A public-key one-time pad encrypts a
message m by XORing it bitwise with a pseudorandom key stream G(x),
where x is a secret random seed. The pseudorandom bit generator G is in-
duced by a family f = (fi : Di −→Di)i∈I of trapdoor permutations with
hard-core predicate B. The public-key-encrypted seed f l(x) (where l is the
bit length of the messages m) is transmitted together with the encrypted
message m ⊕G(x) (see Section 9.4 above).
These encryption schemes are provably secure against passive eavesdrop-
pers (Theorem 9.16), but they are insecure against a chosen-ciphertext at-
tacker Eve. Eve submits ciphertexts (c, y) for decryption, where y is any

234
9. Provably Secure Encryption
element in the domain of f and c = c1c2 . . . cl is any bit string of length
l. If Eve only obtains the last bit ml of the plaintext m = m1m2 . . . ml
from the decryption device, then she immediately derives the hard-core bit
B(f −1(y)) of f −1(y), since B(f −1(y)) = cl ⊕ml. Therefore, Eve has an or-
acle that provides her with the hard-core bit B(f −1(y)) for every y. Now
assume that f is the RSA function modulo a composite n = pq or the Rabin
function, which squares quadratic residues modulo n = pq, as in Blum-Blum-
Shub-encryption, and B is the least signiﬁcant bit. The hard-core-bit oracle
enables Eve to compute the inverse of the RSA or Rabin function modulo
n by using an eﬃcient algorithm, which calls the oracle as a subroutine. We
constructed these algorithms in Sections 7.2 and 7.3. Then, of course, Eve
can also compute the seed x from f l(x) and derive the plaintext m = c⊕G(x)
for every ciphertext c.
We have to worry about adaptively-chosen-ciphertext attacks. One can
imagine scenarios where Bob, the owner of the secret decryption key, might
think that decryption requests are reasonable – for example, if an incomplete
conﬁguration or control of privileges enables attacker Eve from time to time
to get access to the decryption device. If a system is secure against chosen-
ciphertext attacks, then it also resists partial chosen-ciphertext attacks. In
such an attack, adversary Eve does not get the full plaintext in response to
her decryption requests, but only some partial information. Partial-chosen-
ciphertext attacks are a real danger in practice. We just discussed a partial-
chosen-ciphertext attack against public-key one-time pads. In Section 3.3.3,
we described Bleichenbacher’s 1-Million-Chosen-Ciphertext Attack against
PKCS#1(v1.5)-based schemes, which is a practical example of a partial-
chosen-ciphertext attack.
Therefore, it is desirable to have encryption schemes which are provably
secure against adaptively-chosen-ciphertext attacks. We give two examples
of such schemes. The security proof of the ﬁrst scheme, Boneh’s SAEP, re-
lies on the random oracle model, which we described in Section 3.4.5, and
the factoring assumption (Deﬁnition 6.9). The security proof of the second
scheme, Cramer-Shoup’s public key encryption scheme, is based solely on a
standard number-theoretic assumption of the hardness of a computational
problem and on a standard hash function assumption (collision-resistance).
The stronger random oracle model is not needed.
We start with a deﬁnition of the security notion. The deﬁnition includes
a precise description of the attack model. As before, we strive for ciphertext-
indistinguishability. This notion can be extended to cover adaptively-chosen-
ciphertext attacks ([NaoYun90]; [RacSim91]).
Deﬁnition 9.17. Let E be a public-key encryption scheme.
1. An adaptively-chosen-ciphertext attack algorithm A against E is a prob-
abilistic polynomial algorithm that interacts with its environment, called
the challenger, as follows:

9.5 Chosen-Ciphertext Attacks
235
a. Setup: The challenger C randomly generates a public-secret key pair
(pk, sk) for E and calls A with the public key pk as input. The secret
key sk is kept secret.
b. Phase I: The adversary A issues a sequence of decryption requests
for various ciphertexts c′. The challenger responds with the decryp-
tion of the valid ciphertexts c′.
c. Challenge: At some point, algorithm A outputs two distinct mes-
sages m0, m1. The challenger selects a message m ∈{m0, m1} at
random and responds with the “challenge” ciphertext c, which is an
encryption E(pk, m) of m.
d. Phase II: The adversary A continues to request the decryption of
ciphertexts c′, with the only constraint that c′ ̸= c. The challenger
decrypts c′, if c′ is valid, and sends the plaintext to A. Finally, A
terminates and outputs m′ ∈{m0, m1}.
The attacker A is successful, if m′ = m.
We also call A, more precisely, an adaptively-chosen-ciphertext distin-
guishing algorithm.
2. E is ciphertext-indistinguishable against adaptively-chosen-ciphertext at-
tacks, if for every adaptively-chosen-ciphertext distinguishing algorithm
A, the probability of success is ≤1/2 + ε, with ε negligible.
We also call such an encryption scheme E indistinguishability-secure, or
secure, for short, against adaptively-chosen-ciphertext attacks.
Remarks:
1. In the real attack, the challenger is Bob, the legitimate owner of a public-
secret key pair (pk, sk). He is attacked by the adversary A.
2. If the message space is unrestricted, like {0, 1}∗, then it is usually required
that the two candidate plaintexts m0, m1 have the same bit length. This
additional requirement is reasonable. Typically, the length of the plain-
text and the length of the ciphertext are closely related. Hence, informa-
tion about the length of the plaintexts necessarily leaks to an adversary,
and plaintexts of diﬀerent length can easily be distinguished (also see the
remark on Vernam’s one-time pad after Theorem 9.5).
If the message space is a number-theoretic or geometric domain, as in
the typical public-key encryption scheme, then, usually, all messages have
the same bit length. Take, for example, Zn. The messages m ∈Zn are all
encoded as bit strings of length ⌊log2(n)⌋+ 1; they are padded out with
the appropriate number of leading zeros, if necessary.
3. In the literature, adaptively-chosen-ciphertext attacks are sometimes de-
noted by the acronym CCA2, and sometimes they are simply called
chosen-ciphertext attacks. Non-adaptive chosen-ciphertext attacks, which
can request the decryption of ciphertexts only in phase I of the attack,
are often called lunchtime attacks or midnight attacks and denoted by
the acronym CCA1.

236
9. Provably Secure Encryption
4. The notion of semantic security (see the remarks after Deﬁnition 9.14)
can also be carried over to the adaptively-chosen-ciphertext setting. It
means that whatever an adversary Eve is able to compute about the
plaintext m in polynomial time given the ciphertext c, she is also able to
compute in polynomial time without the ciphertext, even if Eve gets the
decryption of any adaptively chosen ciphertexts c′ ̸= c. As shown recently,
semantic security and indistinguishability are also equivalent in the case
of adaptively-chosen-ciphertext attacks ([WatShiIma03]; [Goldreich04]).
9.5.1 A Security Proof in the Random Oracle Model
Boneh’s Simpliﬁed OAEP – SAEP. As an example, we study Boneh’s
Simple-OAEP encryption, or SAEP for short ([Boneh01]). In the encryption
scheme, a collision-resistant hash function h is used. The security proof for
SAEP is a proof in the random oracle model. Basically, this means that h
is assumed to be a truly random function (see page 66 for a more precise
description of the random oracle model).
In contrast to Bellare’s OAEP, which we studied in Section 3.3.4, SAEP
applies Rabin encryption (Section 3.6.1) and not the RSA function. Com-
pared to OAEP, the padding scheme is considerably simpliﬁed. It requires
only one cryptographic hash function. The slightly more complex padding
scheme SAEP+ is provably secure and can be applied to the RSA and the
Rabin function (see [Boneh01]). As OAEP, it requires an additional hash
function G. We do not discuss SAEP+ here.
Key Generation. Let k ∈N be an even security parameter (e.g. k = 1024).
Bob generates a (k +2)-bit modulus n = pq, with 2k+1 < n < 2k+1 +2k (i.e.,
the two most signiﬁcant bits of n are 10), where p and q are (k/2 + 1)-bit
primes, with p ≡q ≡3 mod 4. The primes p, q are chosen randomly. The
public key is n, the private key is (p, q).
The security parameter is split into 3 parts, k = l + s0 + s1, with l ≤
k/4 and l + s0 ≤k/2. In practice, typical values for these parameters are
k = 1024, l = 256, s0 = 128, s1 = 640. The constraints on the lengths of the
security parameters are necessary for the security proof (see Theorem 9.19
below).
We make use of a collision-resistant hash function
h : {0, 1}s1 −→{0, 1}l+s0.
Notation. As usual, we denote by 0r (or 1r) the constant bit string 000 . . . 0
(or 111 . . . 1) of length r (r ∈N). As always, let || denote the concatenation
of strings and ⊕be the bitwise XOR operator.
Encryption. To encrypt an l-bit message m ∈{0, 1}l for Bob, Alice pro-
ceeds in the following steps:
1. She chooses a random bit string r ∈{0, 1}s1.

9.5 Chosen-Ciphertext Attacks
237
2. She appends s0 0-bits to m to obtain the (l + s0)-bit string x := m||0s0.
3. She sets y = (x ⊕h(r))||r.
4. She views the k-bit string y as a k-bit integer and applies the Rabin
trapdoor function modulo n to obtain the ciphertext c:
c := y2 mod n.
Note that y < 2k < n/2.
Remarks:
1. At a ﬁrst glance, the length l of the plaintexts might appear small (recall
that a typical value is l = 256). But usually we encrypt only short data
by using a public-key method, for example, session keys for a symmetric
cipher, such as Triple-DES or AES, and for this purpose 256 bits are
really suﬃcient.
2. All ciphertexts are quadratic residues modulo n. But not every quadratic
residue appears as a ciphertext. Let c be a quadratic residue modulo n.
Then c is the encryption of some plaintext in {0, 1}l, if there is a square
root y of c modulo n, such that
a. y < 2k, i.e., we may consider y as a bit string of length k, and
b. the s0 least-signiﬁcant bits of v ⊕h(r) are all 0, where y = v||r, v ∈
{0, 1}l+s0, r ∈{0, 1}s1.
In this case, the plaintext m, whose encryption is c, consists of the l
most-signiﬁcant bits of v ⊕h(r), i.e., v ⊕h(r) = m||0s0, and we call c a
valid ciphertext (or a valid encryption of m) with respect to y.
The security of SAEP is based on the factoring assumption: it is practi-
cally infeasible to compute the prime factors p and q of n (see Section 6.5
for a precise statement of the assumption). Decrypting ciphertexts requires
drawing square roots. Without knowing the prime factors of n, it is infeasible
to compute square roots modulo n. The ability to compute modular square
roots is equivalent to the ability to factorize the modulus n (Proposition A.62,
Lemma A.63). Bob can compute square roots modulo n because he knows
the secret factors p and q.
We recall from Proposition A.62 some basics on computing square roots.
Let c ∈Zn be a quadratic residue, c ̸= 0. If c is prime to n, then c has 4
distinct square roots modulo n. If c is not prime to n, i.e., if c is a multiple of
p or q, then c has only 2 square roots modulo n. In the latter case, the factors
p, q of n can be easily derived by computing gcd(c, n) with the Euclidean
algorithm. The probability for this case is negligibly small, if c is a random
quadratic residue.
If y2 mod n = c, then also (n −y)2 mod n = c. Hence, exactly two of the
4 roots (or one of the two roots) are < n/2.
The residue [0] ∈Zn has the only square root [0].
If both primes p and q are ≡3 mod 4, as here in SAEP, and if the factors
p and q are known, then the square roots of c can be easily and eﬃciently
computed as follows:

238
9. Provably Secure Encryption
1. p + 1/4 and q + 1/4 are integers, and the powers zp = c(p+1)/4 mod p of
c mod p and zq = c(q+1)/4 mod q of c mod q are square roots of c modulo
p and modulo q (see Proposition A.60). Recall that z2
p = c(p+1)/2 =
c · c(p−1)/2 and c(p−1)/2 ≡1 mod p, if c mod p ̸= 0 (see Euler’s criterion,
Proposition A.52). If c mod p ̸= 0, then ±zp are the two distinct square
roots of c mod p. If c mod p = 0, then there is the single root zp = 0 of c
modulo p.
2. To get the square roots of c modulo n, we map (±zp, ±zq) to Zn by
applying the inverse Chinese remainder map. If c is prime to n, then we
obtain 4 distinct roots. If zp = 0 or zq = 0, we get 2 distinct roots, and
if both zp and zq are 0, then there is the only root 0 (see Proposition
A.62).
Decryption. Bob decrypts a ciphertext c by using his secret key (p, q) as
follows:
1. He computes the square roots of c modulo n, as just described.
In this computation, he tests that z2
p ≡c mod p and z2
q ≡c mod q. If
either test fails, then c is not a quadratic residue and Bob rejects c.
2. Two of the four roots (or one of the two roots) are > n/2 and hence can
be discarded. Bob is left with 2 square roots y1, y2 (let y1 = y2, if there
is only one left). If neither of y1, y2 is < 2k, then Bob rejects c.
From now on, we assume that y1 < 2k. If y2 ≥2k, then Bob does not
have to distinguish between two candidates, and thus he can simplify the
following steps (omit the processing of y2). So, assume now that both y1
and y2 are < 2k. Then, we may view them as strings in {0, 1}k.
3. Bob writes y1 = v1||r1 and y2 = v2||r2, with v1, v2 ∈{0, 1}l+s0 and
r1, r2 ∈{0, 1}s1, and computes x1 = v1 ⊕h(r1) and x2 = v2 ⊕h(r2).
4. He writes x1 = m1||t1 and x2 = m2||t2, with m1, m2 ∈{0, 1}l and t1, t2 ∈
{0, 1}s0. If either none or both of t1, t2 are 00 . . . 0, then Bob rejects c.
Otherwise, let i ∈{1, 2} be the unique i with ti = 00 . . . 0. Then, mi is
the decryption of c.
Remark. It might happen that Bob rejects a valid encryption c of a message
m, because c is valid with respect to both roots y1 and y2 and hence both
t1 and t2 are 00...0. But the probability for this event is negligibly small, at
least if h comes close to a random function.
Namely, assume y1 ̸= y2 and c is a valid encryption of m with respect to
y1, i.e., y1 = ((m||0s0) ⊕h(r1))||r1. If h is assumed to be a random function,
then the probability that c is also a valid ciphertext with respect to y2 is
about 1/2s0. There are 2 cases. If r1 ̸= r2, then h(r2) is randomly generated
independently from h(r1). Hence, the probability that the s0 least-signiﬁcant
bits of h(r2) are just the s0 least-signiﬁcant bits of v2 is 1/2s0. If r1 = r2 and
c is valid with respect to both roots, then the (s0 + s1) least-signiﬁcant bits
of y1 and y2 coincide, i.e., y2 = y1 + 2s0+s1δ, with absolute value |δ| < 2l.
Since |δ| < 2l < 2k/2 < p, q, we know that δ is a unit modulo n. From

9.5 Chosen-Ciphertext Attacks
239
y2
2 = (y1 + 2s0+s1δ)2 = y2
1 + 2s0+s1+1δy1 + 22(s0+s1)δ2 ≡c ≡y2
1 mod n,
we conclude that y1 ≡−2s0+s1−1δ mod n, and the probability for that is
1/2s0+s1−1.
The rejection of valid ciphertexts can be completely avoided by a slight
modiﬁcation of the encryption algorithm. Alice repeats the random genera-
tion of r, until y has Jacobi symbol 1. Then Bob can always select the correct
square root by taking the unique square root < 2k with Jacobi symbol 1. We
described a similar approach in Section 3.6.1 on Rabin’s encryption. However,
this makes the encryption scheme less eﬃcient, and it is not necessary.
The proof of security for SAEP is based on an important result due to
Coppersmith ([Coppersmith97]).
Theorem 9.18. Let n be an integer, and let f(X) ∈Zn[X] be a monic
polynomial of degree d. Then there is an eﬃcient algorithm which ﬁnds all
x ∈Z such that the absolute value |x| < n1/d and f(x) ≡0 mod n.
For a proof, see [Coppersmith97]. The special case f(X) = Xd−c, c ∈Zn,
is easy. To get the solutions x with |x| < n1/d, you can compute the ordinary
d-th roots of c, because for 0 ≤x < n1/d we have xd mod n = xd. To compute
the ordinary d-th roots is easy, since x 7→xd (without taking residues) is
strictly monotonic (take, for example, the simple Algorithm 3.4).
Theorem 9.19. Assume that the hash function h in SAEP is a random
oracle. Let n = pq be a key for SAEP with security parameter k = l + s0 +
s1 (i.e., 2k+1 < n < 2k+1 + 2k). Assume l ≤k/4 and l + s0 ≤k/2. Let
A(n, l, s0, s1) be a probabilistic distinguishing algorithm with running time t
that performs an adaptively-chosen-ciphertext attack against SAEP and has
a probability of success ≥1/2 + ε. Let qd be the number of A’s decryption
queries, and let qh be the number of A’s queries of the random oracle h.
Then there is a probabilistic algorithm B for factoring the modulus n with
running time
t + O(qdqhtC + qdt′
C)
and
probability of success ≥1
6 · ε ·
µ
1 −2qd
2s0 −2qd
2s1
¶
.
Here, tC (resp. t′
C) is the running time of Coppersmith’s algorithm for ﬁnding
“small-size” roots of polynomials of degree 2 (resp. 4) modulo n (i.e., roots
with absolute value ≤n1/2 resp. ≤n1/4).
Remarks:
1. Recall that typical values of the parameters are k = 1024, l = 256, s0 =
128, s1 = 640. The number qd of decryption queries that an adversary
can issue in practice should be limited by 240. We see that the fractions
2qd/2s0 and 2qd/2s1 are negligibly small.

240
9. Provably Secure Encryption
2. Provided the factoring assumption is true, we conclude from Boneh’s the-
orem that a probabilistic polynomial attacking algorithm like A, whose
probability of success is ≥1/2 + 1/P(k), P a positive polynomial (i.e.,
A has a non-negligible “advantage”), cannot exist. Otherwise, B would
factor moduli n in polynomial time. Thus, SAEP is indistinguishability-
secure against adaptively-chosen-ciphertext attacks (in the random oracle
model).
But Boneh’s result is more precise: It gives a tight reduction from the
problem of attacking SAEP to the problem of factoring. If we had a
successful attacking algorithm A against SAEP, we could give precise
estimates for the running time and the probability of successfully factor-
ing n. Or, conversely, if we can state reasonable bounds for the running
time and the probability of success in factoring numbers n of a given
bit length, we can derive a concrete lower bound for the running time of
algorithms attacking SAEP with a given probability of success.
Proof. We sketch the basic ideas of the proof. In particular, we illustrate
how the random oracle assumption is applied. For more details we refer to
[Boneh01].
It suﬃces to construct an algorithm S which on inputs n, l, s0, s1 and
c = α2 mod n, for a randomly chosen α with 0 ≤α < 2k, outputs a square
root α′, 0 ≤α′ < 2k, of c with probability ≥ε · (1 −2qd/2s0 −2qd/2s1).
Namely, with probability > 1/3, a number c with 0 ≤c < n has two distinct
square roots modulo n in [0, 2k[ 3. Hence α ̸= α′ with probability ≥1/6 and
then n can be factored by computing gcd(n, α −α′) (see Lemma A.63 and
Section 3.6.1).
In the following we describe the algorithm S. The algorithm eﬃciently
computes a square root < 2k of c, without knowing p, q, in two cases, which
we study ﬁrst.
1. Let y = v||r, where v and r are bit strings of lengths (l + s0) and s1. If y
is a root of c, then v is a root of the quadratic polynomial (2s1X +r)2 −c
modulo n, with 0 ≤v < 2l+s0 ≤2k/2 < n1/2.
Thus, if S happens to know or guess correctly the s1 lower signiﬁcant
bits r of a root y, then it eﬃciently ﬁnds y by applying the following
algorithm
CompRoot1(c, r):
Compute the roots v < 2k/2 of (2s1X + r)2 −c modulo n by using
Coppersmith’s algorithm. If such a v is found, return the root y = v||r.
2. Let y = m||w be a square root modulo n of c, with m ∈{0, 1}l and
w ∈{0, 1}s0+s1. Let c′ ∈Zn be a further quadratic residue, c′ ̸= c,
and assume that c′ has a k-bit square root y′ = m′||w modulo n, whose
(s0 + s1) lower signiﬁcant bits w are the same as those of y. Then we
3 See Fact 2 in [Boneh01]; n is chosen between 2k+1 and 2k+1 + 2k to get this
estimate.

9.5 Chosen-Ciphertext Attacks
241
may write y′ = y +2s0+s1δ, where δ = m′ −m is an integer with absolute
value |δ| < 2l ≤2k/4 < n1/4. y is a common root of the polynomials
f(X) = X2 −c and g(X, δ) = (X +2s0+s1δ)2 −c′ modulo n. Therefore, δ
is a root of the resultant R of f(X) and g(X, ∆) = (X + 2s0+s1∆)2 −c′.
The resultant R is a polynomial modulo n in ∆of degree 4 (see, for
example, [Lang05], Chapter IV). Since |δ| < 2l ≤2k/4 < n1/4, we can
compute δ eﬃciently by using Coppersmith’s algorithm for polynomials
of degree 4. The greatest common divisor gcd(f(X), g(X, δ)) is X −y.
Thus, if S happens to get such an element c′, then it eﬃciently ﬁnds a
square root y of c by applying the following algorithm
CompRoot2(c, c′):
Compute the roots δ with absolute value |δ| < 2l of the resultant R mod-
ulo n by using Coppersmith’s algorithm. Compute the greatest common
divisor X −y of X2 −c and (X + 2s0+s1δ)2 −c′ modulo n. If such a δ
(and then y) is found, return the root y.
Algorithm S interacts with the attacking algorithm A. In the real at-
tack, A interacts with Bob, the legitimate owner of the public-secret key pair
(n, (p, q)), to obtain decryptions of ciphertexts of its choice, and with the ran-
dom oracle h to get hash values h(m) (in practice, interacting typically means
to communicate with another computer program). Now S is constructed to
replace both Bob and the random oracle h in the attack. It “simulates” Bob
and h.
Each time, when A issues a query, S has a chance to compute a square
root of c, and of course S terminates when a root is found.
S has no problems answering the hash queries. Since h is a random oracle,
S can assign a randomly generated v ∈{0, 1}l+s0 as hash value h(r). The
only restriction is: If the hash value of m is queried more than once, then
always the same value has to be provided. Therefore, S has to store the list
H of hash value pairs (r, h(r)) that it has given to A.
The structure of S is the following.
S calls A with the public key n and the security parameters l, s0, s1 as
input. Then it waits for the queries of A.
1. If A queries the hash value for r, then
a. if (r, h(r)) is on the list H of previous responses to hash queries, then
S again sends h(r) as hash value to A;
b. else S applies algorithm CompRoot1(c, r); if S ﬁnds a root of c mod-
ulo n in this way, then it returns the root and terminates;
c. else S picks a random bit string v of length (l + s0), puts (r, v) on its
list H of hash values and sends v = h(r) as hash value to A.
2. if A queries the decryption of a ciphertext c′, then
a. S applies algorithm CompRoot2(c, c′); if S ﬁnds a root of c modulo
n in this way, it returns the root and terminates;
b. else S applies, for each (r′, h(r′)) on the list H of previous hash values,
algorithm CompRoot1(c′, r′); if S ﬁnds a square root v′||r′ of c′ in this

242
9. Provably Secure Encryption
way, then it computes w′ = v′ ⊕h(r′); if the s0 lower signiﬁcant bits
of w′ are all 0, i.e., w′ = m′||0s0, then S sends m′ as plaintext to A,
else S rejects c′ as an “invalid ciphertext”;
c. else S could not ﬁnd a square root of c′ in the previous step b and
rejects c′ as an “invalid ciphertext”;
3. if A produces the two candidate plaintexts m0, m1 ∈{0, 1}l, then S sends
c as encryption of m0 or m1 to A.
If A terminates, then S also terminates (if it has not terminated before).
To analyze algorithm S, let y1 and y2 be the two square roots of c with
0 ≤y1, y2 < 2k (take y2 = y1, if there is only one). The goal of S is to
compute y1 or y2. We decompose yi = vi||ri, with ri ∈{0, 1}s1.
We consider several cases.
1. If A happens to query the hash value for r1 or r2, then S successfully
ﬁnds one of the roots y1, y2 in step 1b.
2. If A happens to query the decryption of a ciphertext c′, and if c′ has a
k-bit square root y′ modulo n, whose (s0 + s1) lower signiﬁcant bits are
the same as those of y1 or y2, then S successfully ﬁnds one of the roots
y1, y2 in step 2a.
We observed above that S can easily play the role of the random oracle
h and answer hash queries. Sometimes, S can answer decryption requests
correctly.
3. If A queries the decryption of a ciphertext c′ that is valid with respect
to its square root y′ modulo n, and if A has previously asked for the
hash value h(r′) of the s1 rightmost bits r′ of y′ (c′ = y′2 mod n; y′ =
v′||r′, r′ ∈{0, 1}s1), then S responds in step 2b to A with the correct
plaintext.
But S does not know the secret key, and so S cannot perfectly simulate
Bob. We now study the two cases where, from A’s point of view, the behavior
of S might appear diﬀerent from Bob’s behavior.
In the real attack, Bob sends a valid encryption of m0 or m1 as challenge
ciphertext to A. The choice of the number c, which S sends as challenge
ciphertext, has nothing to do with m0 or m1. Therefore, we have to study
the next case 4.
4. The number c, which S presents as challenge ciphertext at the end of
phase I, is, from A’s point of view, not a valid encryption of m0 or m1 with
respect to yi, where i = 1 or i = 2. This means that vi ⊕h(ri) ̸= mb||0s0
or, equivalently, h(ri) ̸= (mb||0s0) ⊕vi for b = 0, 1.
This can only happen either
• if A has asked for h(ri) in phase I, and S has responded with a non-
appropriate hash value for ri, or

9.5 Chosen-Ciphertext Attacks
243
• if A has asked in phase I for the decryption of a ciphertext c′ with
c′ = y′2 mod n, y′ = v′||r′, r′ ∈{0, 1}s1, v′ ∈{0, 1}l+s0 and r′ = ri, i.e.,
the s0 rightmost bits of y′ and yi are equal4; in this case the answer of
S might have put some restriction on h(ri).
Otherwise, the hash value h(ri), which is randomly generated by the
oracle, is independent of A’s point of view at the end of phase I. This
implies that, from A’s point of view, h(ri) = (mb||0s0) ⊕vi has the same
probability as h(ri) = w, for any other (l + s0)-bit string w. Therefore c
is a valid encryption of m0 or m1 with respect to yi.
That A has asked for h(ri) before can be excluded, because then S has
already successfully terminated in step 1b.
The number c is generated by squaring a randomly chosen α, and c is not
known to A in phase I. Therefore, the random choice of c is independent
from A′s decryption queries in phase I. Hence, the probability that a
particular decryption query in phase I involves r1 or r2 is 2 · 1/2s1. There
are at most qd decryption queries. Thus, the probability of case 4 is
≤qd · 2/2s1.
5. Attacker A asks for the decryption of a ciphertext c′ and S rejects it (in
step 2b or step 3), whereas Bob, by using his secret key, accepts c′ and
provides A with the plaintext m′. Then, we are not in case 2, because,
in case 2, S successfully terminates in step 2a before giving an answer to
A.
We assume that c is a valid ciphertext for m0 or m1 with respect to both
square roots y1 and y2, i.e., that we are not in case 4. This means that
for i = 1 and i = 2, we have v ⊕h(ri) = mb||0s0 for b = 0 or b = 1.
Decrypting c′, Bob ﬁnds:
c′ = y′2 mod n, y′ = v′||r′, v′ ∈{0, 1}l+s0, r′ ∈{0, 1}s1, v′⊕h(r′) = m′||0s0.
If r′ = ri for i = 1 or i = 2, then the s0 rightmost bits of v′ and v are
the same – they are equal to the s0 rightmost bits of h(r′) = h(ri). Then
the (s0 + s1) rightmost bits of y′ and yi coincide and we are in case 2,
which we have excluded. Hence r′ ̸= r1 and r′ ̸= r2.
The hash value h(r′) has not been queried before, because otherwise we
would be in case 3, and S would have accepted c′ and responded with
the plaintext m′.
Hence, the hash value h(r′) is a random value which is independent from
preceding hash queries and our assumption that c is a valid encryption
of m0 or m1 (which constrains h(r1) and h(r2), see above). But then the
probability that the s0 rightmost bits of h(r′) ⊕v′ are all 0 (which is
necessary for Bob to accept) is 1/2s0. Since there are at most 2 square
roots y′, and A queries the decryption of qd ciphertexts, the probability
that case 5 occurs is ≤qd · 2/2s0.
4 Note that S successfully terminates in step 2a only if there are (s0 +s1) common
rightmost bits.

244
9. Provably Secure Encryption
Now assume that neither case 4 nor case 5 occurs. Following Boneh, we
call this assumption GoodSim.
Under this assumption GoodSim, S behaves exactly like Bob and the
random oracle h. Thus, A operates in the same probabilistic setting, and it
therefore has the same probability of success 1/2+ε as in the original attack.
If, in addition, cases 1 and 2 do not occur, then the randomly generated c
(and r1, r2), and the hash values h(r1) and h(r2), which are generated by the
random oracle, are independent of all hash and decryption queries issued by A
and the responses given by S. Therefore A has not collected any information
about h(r1) and h(r2). This means that the ciphertext c hides the plaintext
mb perfectly in the information-theoretic sense – the encryption includes a
bitwise XOR with a truly random bit string h(ri). We have seen in Section
9.2 that then A’s probability of correctly distinguishing between m0 and m1
is 1/2.
Therefore, the advantage ε in A’s probability of success necessarily results
from the cases 1, 2 (we still assume GoodSim). In the cases 1 and 2, the
algorithm S successfully computes a square root of c. Hence the probability
of success probsuccess(S |GoodSim) of algorithm S assuming GoodSim is ≥
ε. The probability of the cases 4 and 5 is ≤2qd/2s1 + 2qd/2s0 and hence
prob(GoodSim) ≥1 −2qd/2s1 −2qd/2s0, and we conclude that the probability
of success of S is
≥prob(GoodSim) · probsuccess(S |GoodSim) ≥ε ·
µ
1 −2qd
2s1 −2qd
2s0
¶
.
The running time of S is essentially the running time of A plus the running
times of the Coppersmith algorithms. The algorithm for quadratic polyno-
mials is called after each of the qh hash queries (step 1b). After each of the
qd decryption queries, it may be called for all of the ≤qh known hash pairs
(r, h(r)) (step 2b). The algorithm for polynomials of degree 4 is called after
each of the qd decryption queries (step 2a). Thus we can estimate the running
time of S by
t + O(qdqhtC + qdt′
C),
and we see that the algorithm S gives a tight reduction from the problem of
attacking SAEP to the problem of factoring.
2
Remark. True random functions can not be implemented in practice. There-
fore, a proof in the random oracle model – treating hash functions as equiv-
alent to random functions – can never be a complete proof of security for
a cryptographic scheme. But, intuitively, the random oracle model seems to
be reasonable. In practice, a well-designed hash function should never have
any features that distinguish it from a random function and that an attacker
could exploit (see Section 3.4.4).
In recent years, doubts about the random oracle model have been ex-
pressed. Examples of cryptographic schemes were constructed which are prov-
ably secure in the random oracle model, but are insecure in any real-world

9.5 Chosen-Ciphertext Attacks
245
implementation, where the random oracle is replaced by a real hash function
([CanGolHal98]; [GolTau03]; [GolTau03a]; [MauRenHol04]; [CanGolHal04]).
However, the examples appear contrived and far from systems that would be
designed in the real world. The conﬁdence in the soundness of the random
oracle assumption is still high among cryptographers, and the random oracle
model is still considered a useful tool for validating cryptographic construc-
tions. See, for example, [KobMen05] for a discussion of this point.
Nevertheless, it is desirable to have encryption schemes whose security
can be proven solely under a standard assumption that some computational
problem in number theory can not be eﬃciently solved. An example of such
a scheme is given in the next section.
9.5.2 Security Under Standard Assumptions
The Cramer-Shoup Public Key Encryption Scheme. The security
of the Cramer-Shoup public key encryption scheme ([CraSho98]) against
adaptively-chosen-ciphertext attacks can be proven assuming that the deci-
sion Diﬃe-Hellman problem (Section 4.5.3) can not be solved eﬃciently and
that the hash function used is collision-resistant. The random oracle model is
not needed. In Chapter 10, we will give examples of signature schemes whose
security can be proven solely under the assumed diﬃculty of computational
problems (for example, Cramer-Shoup’s signature scheme).
First, we recall the decision Diﬃe-Hellman problem (see Section 4.5.3).
Let p and q be large prime numbers, such that q is a divisor of p −1, and
let Gq be the (unique) subgroup of order q of Z∗
p. Gq is a cyclic group, and
every element g ∈Z∗
p of order q is a generator of Gq (see Lemma A.40).
Given g1, u1 = gx
1, g2 = gy
1, u2 with random elements g1, u2 ∈Gq and
randomly chosen exponents x, y ∈Z∗
q , decide if u2 = gxy
1 . This is equivalent
to decide, for randomly (and independently) chosen elements g1, u1, g2, u2 ∈
Gq, if
logg2(u2) = logg1(u1).
If the equality holds, we say that (g1, u1, g2, u2) has the Diﬃe-Hellman prop-
erty.
The decision Diﬃe-Hellman assumption says that no probabilistic poly-
nomial algorithm exists to solve the decision Diﬃe-Hellman problem.
Notation. For pairs u = (u1, u2), x = (x1, x2) and a scalar value r we
shortly write ux := ux1
1 ux2
2 , urx := urx1
1
urx2
2
.
Key Generation. Bob randomly generates large prime numbers p and q,
such that q is a divisor of p −1. He randomly chooses a pair g = (g1, g2) of
elements g1, g2 ∈Gq.5
5 Compare, for example, the key generation in the Digital Signature Standard,
Section 3.5.3.

246
9. Provably Secure Encryption
Then Bob randomly chooses three pairs of exponents x = (x1, x2), y =
(y1, y2), z = (z1, z2), with x1, x2, y1, y2, z1, z2 ∈Z∗
q and computes modulo
p:
d = gx = gx1
1 gx2
2 , e = gy = gy1
1 gy2
2 , f = gz = gz1
1 gz2
2 .
Bob’s public key is (p, q, g, d, e, f), his private key is (x, y, z).
For encryption, we need a collision-resistant hash function
h : {0, 1}∗−→Z∗
q = {0, 1, . . . q −1}.
h outputs bit strings of length |q| = ⌊log2(q)⌋+ 1. In practice, as in DSS, we
might have |q| = 160 and h = SHA-1, see Section 3.5.3.
Encryption. Alice can encrypt messages m ∈Gq for Bob, i.e., elements m
of order q in Z∗
p. To encrypt a message m for Bob, Alice chooses a random
r ∈Z∗
q and computes the ciphertext
c = (u1, u2, w, v) := (gr
1, gr
2, f r · m, v), with v := drer·h(u1,u2,w)).
Note that the Diﬃe-Hellman property holds for (g1, u1, g2, u2).
Decryption. Bob decrypts a ciphertext c = (u1, u2, w, v) by using his pri-
vate key (x, y, z) as follows:
1. Bob checks the veriﬁcation code v. He checks if ux+y·h(u1,u2,w) = v. If
this equation does not hold, he rejects c.
2. He recovers the plaintext by computing w · u−z.
Remarks:
1. We follow here the description of Cramer-Shoup’s encryption scheme in
[KobMen05]. The actual scheme in [CraSho98] is the special case with
z2 = 0. Cramer and Shoup prove the security of the scheme using a
slightly weaker assumption on the hash function. They assume that h is
a member of a universal one-way family of hash functions (as deﬁned in
Section 10.4).
2. If c = (u1, u2, w, v) is the correct encryption of a message m, then the
veriﬁcation code v passes the check in step 1,
ux+y·h(u1,u2,w) = (gr)x · (gr)y·h(u1,u2,w)
= (gx)r · (gy)r·h(u1,u2,w) = dr · er·h(u1,u2,w) = v,
and the plaintext m is recovered in step 2,
f r = (gz)r = (gr)z = uz, hence w · u−z = m · f r · f −r = m.

9.5 Chosen-Ciphertext Attacks
247
3. The private key (x, y, z) = ((x1, x2), (y1, y2), (z1, z2)) is an element in the
Zq-vectorspace Z6
q. Publishing the public key (d, e, f) means to publish
the following conditions on (x, y, z):
(1) d = gx ,
(2) e = gy ,
(3) f = gz.
These are linear equations for x1, x2, y1, y2, z1, z2. They can also be writ-
ten as (with λ := logg1(g2))
(1) logg1(d) = x1 + λ · x2
(2) logg1(e) =
y1 + λ · y2
(3) logg1(f) =
z1 + λ · z2.
The equations deﬁne lines Lx, Ly, Lz in the plane Z2
q. For a given public
key (d, e, f), the private key element x (resp. y, z) is a (uniformly dis-
tributed) random element of Lx (resp. Ly, Lz); each of the elements in
Lx (resp. Ly, Lz) has the same probability 1/q of being x (resp. y, z).
4. Let c := (u1, u2, w, v) be a ciphertext-tuple. Then c is accepted for de-
cryption only if the veriﬁcation condition (4) v = ux+y·α, with α =
h(u1, u2, w), holds. The veriﬁcation condition is a linear equation for x
and y:
(4) logg1(v) = logg1(u1)x1+λ logg2(u2)x2+α logg1(u1)y1+αλ logg2(u2)y2.
Equations (1), (2) and (4) are linearly independent, if and only if
logg1(u1) ̸= logg2(u2).
The ciphertext-tuple c is the correct encryption of a message, if and only
if c satisﬁes the veriﬁcation condition and logg1(u1) = logg2(u2), i.e.,
(g1, u1, g2, u2) has the Diﬃe-Hellman property. In this case, we call c a
valid ciphertext.
Now assume that logg1(u1) ̸= logg2(u2). Then, the probability that c
passes the check of the veriﬁcation code and is not rejected in the de-
cryption step 1 is 1/q and hence negligibly small. Namely, (1), (2) and
(4) are linearly independent. This implies that (x, y) is an element of the
line in Z4
q which is deﬁned by (1), (2), (4). The probability for that is 1/q,
since (x, y) is a random element of the 2-dimensional space Lx × Ly.
Theorem 9.20. Let the decision Diﬃe-Hellman assumption be true and let
h be a collision-resistant hash function. Then the probability of success of any
attacking algorithm A, which on input of a random public key (p, q, g, d, e, f)
executes an adaptively-chosen-ciphertext attack and tries to distinguish be-
tween two plaintexts, is ≤1/2 + ε, with ε negligible.
Proof. We discuss the essential ideas of the proof. For more details, we refer
to [CraSho98].

248
9. Provably Secure Encryption
The proof runs by contradiction. Assume that there is an algorithm A
which on input of a randomly generated public key successfully distinguishes
between ciphertexts in an adaptively-chosen-ciphertext attack, with a prob-
ability of success of 1/2 + ε, ε non-negligible.
Then we can construct a probabilistic polynomial algorithm S which an-
swers the decision Diﬃe-Hellman problem with a probability close to 1, in
contradiction to the decision Diﬃe-Hellman assumption. The algorithm S
successfully ﬁnds out whether a random 4-tuple (g1, u1, g2, u2) has the Diﬃe-
Hellman property or not.
As in the proof of Theorem 9.19, algorithm S interacts with the attack-
ing algorithm A. In the real attack, A interacts with Bob, the legitimate
owner of the secret key, to obtain decryptions of ciphertexts of its choice (in
practice, interacting typically means to communicate with another computer
program). Now S is constructed to replace Bob in the attack. S “simulates”
Bob.
On input of (p, q, g1, u1, g2, u2) the algorithm S repeatedly generates a
random private key (x, y, z) and computes the corresponding public key d =
gx, e = gy, f = gz. Then S calls A with the public key, and A executes
its attack. The diﬀerence to the real attack is that A communicates with S
instead of Bob. At some point (end of phase I of the attack), A outputs two
plaintexts m0, m1. S randomly selects a bit b ∈{0, 1}, sets
w := uzmb, α := h(u1, u2, w), v := ux+α·y
and sends c := (u1, u2, w, v) to A, as an encryption of mb. The veriﬁcation
code v is correct by construction, even if logg1 u1 ̸= logg2 u2 and c is not a
valid encryption of mb.
At any time in phase I or phase II, A can request the decryption of a
ciphertext c′ = (u′
1, u′
2, w′, v′), c′ ̸= c. If c′ satisﬁes the veriﬁcation condition,
then equation (4) holds for c′, i.e., we have, with α′ = h(u′
1, u′
2, w′):
(5) logg1(v′) = logg1(u′
1)x1 + λ logg2(u′
2)x2 + α′ logg1(u′
1)y1 + α′λ logg2(u′
2)y2.
We observe: If logg1(u1) ̸= logg2(u2) and logg1(u′
1) ̸= logg2(u′
2), then the
equations (1), (2), (4), (5) are linearly independent, if and only if α′ ̸= α.
If c′ satisﬁes the veriﬁcation condition, then, up to negligible probability,
c′ is a valid ciphertext (see Remark 4 above) and S answers with the correct
plaintext. S can easily check the veriﬁcation condition and decrypt the ci-
phertext, because it knows the secret key. Here, S behaves exactly like Bob
in the real attack.
If (g1, u1, g2, u2) has the Diﬃe-Hellman property, then the ciphertext
c := (u1, u2, w, v), which S presents to A as an encryption of mb, is a valid
ciphertext, and the probability distribution of c is the same, as if Bob pro-
duced the ciphertext. Hence, in this case, A operates in the same setting as
in the real attack.

9.5 Chosen-Ciphertext Attacks
249
If (g1, u1, g2, u2) does not have the Diﬃe-Hellman property, then A op-
erates in a setting which does not occur in the real attack, since Bob only
produces valid ciphertexts. We do not know how attacker A responds to the
modiﬁed setting. For example, A could fail to terminate in its expected run-
ning time or it could output an error message or it could produce the usual
output and answer, whether m0 or m1 is encrypted. But fortunately the
concrete behavior of A is not relevant in this case: if (g1, u1, g2, u2) does not
have the Diﬃe-Hellman property, then it is guaranteed that up to a negligible
probability, mb is perfectly hidden in w = uzmb to A and A can not generate
any advantage from knowing w in distinguishing between m0 and m1.
To understand this, assume that (g1, u1, g2, u2) does not have the Diﬃe-
Hellman property. Then the probability that attacker A does get some infor-
mation about the private key z by asking for the decryption of ciphertexts c′
of its choice, c′ ̸= c, is negligibly small.
Namely, let A ask for the decryption of a ciphertext c′ = (u′
1, u′
2, w′, v′).
Let α′ = h(u′
1, u′
2, w′).
Since we assume that logg1(u1) ̸= logg2(u2), equations (1), (2), (4) are
linearly independent and deﬁne a line L. From A’s point of view, the private
key (x, y) = (x1, x2, y1, y2) is a random point on the line L.
We have to distinguish between several cases.
1. α′ = α. Since h is collision-resistant6, the probability that A can gen-
erate a triple (u′
1, u′
2, w′) ̸= (u1, u2, w), with h(u′
1, u′
2, w′) = α′ = α =
h(u1, u2, w), is negligibly small. Hence the case α′ = α can happen, up
to a negligible probability, only if (u′
1, u′
2, w′) = (u1, u2, w). But then
v′ ̸= v, because c′ ̸= c, and hence v′ ̸= (u′)x+α′·y = ux+α·y = v, and c′ is
rejected.
2. Now assume that α′ ̸= α and (g1, u′
1, g2, u′
2) does not have the Diﬃe-
Hellman property. If c′ is not rejected, then the following equation (5)
holds (it is equation (4) stated for c′):
(5) logg1(v′)
= logg1(u′
1)x1 + λ logg2(u′
2)x2 + α′ logg1(u′
1)y1 + α′λ logg2(u′
2)y2.
Since α′ ̸= α, equation (5) puts, in addition to (1), (2), (4), another lin-
early independent condition on (x, y). Hence, at most one point (˜x, ˜y)
satisﬁes these equations. Since (x, y) is a randomly chosen element on
the line L, the probability that (x, y) = (˜x, ˜y) is ≤1/q. Hence, up to a
negligible probability, c′ is rejected.
3. The remaining case is that α′ ̸= α and (g1, u′
1, g2, u′
2) has the Diﬃe-
Hellman property. Either the check of the veriﬁcation code v′ fails and
c′ is rejected or equation (5) holds. In the latter case, S decrypts c′
and provides A with the plaintext m′. Thus, what A learns from the
6 Second-pre-image resistance would suﬃce.

250
9. Provably Secure Encryption
decryption is the equation (6) m′ = w′ · u′−z, which can be reformulated
as
(6)
logg1(w′ · m′−1) = logg1(u′
1)z1 + λ logg2(u′
2)z2
= logg1(u′
1)(z1 + λz2).
But this equation is linearly dependent on (3), so it does not give A
any additional information on z. A learns from m′ = w′ · u′−z the same
information that it already knew from the public-secret-key equation
(3) f = ez: the private key z is a random element on the line Lz.
Summarizing, if (g1, u1, g2, u2) does not have the Diﬃe-Hellman property,
then up to a negligible probability, from attacker A’s point of view, the secret
key z is a random point on a line in Z2
q . This means that from A’s point of
view, mb is perfectly (in the information-theoretic sense) hidden in w = f zmb
by a random element of Gq. We learnt in Section 9.2 that therefore A’s
probability of distinguishing successfully between m0 and m1 is exactly 1/2.
Hence, the non-negligible advantage ε of A results solely from the case that
(g1, u1, g2, u2) has the Diﬃe-Hellman property.
In order to decide whether (g1, u1, g2, u2) has the Diﬃe-Hellman prop-
erty, S repeatedly randomly generates private key elements (x, y, z) and runs
the attack with A. If A correctly determines which of the messages mb was
encrypted by S in signiﬁcantly more than half of the iterations, then S can
be almost certain that (g1, u1, g2, u2) has the Diﬃe-Hellman property. Oth-
erwise, S is almost certain that it does not.
2
9.6 Unconditional Security of Cryptosystems
The security of many currently used cryptosystems, in particular that of all
public-key cryptosystems, is based on the hardness of an underlying computa-
tional problem, such as factoring integers or computing discrete logarithms.
Security proofs for these systems show that the ability of an adversary to
perform a successful attack contradicts the assumed diﬃculty of the com-
putational problem. A typical security proof was given in Section 9.4. We
proved that public-key one-time pads induced by one-way permutations with
a hard-core predicate are computationally secret. Thus, the security of the
encryption scheme is reduced to the one-way feature of function families,
such as the RSA or modular squaring families, and the one-way feature of
these families is, in turn, based on the assumed hardness of inverting modu-
lar exponentiation or factoring a large integer (see Chapter 6). The security
proof is conditional, and there is some risk that in the future, the underlying
condition will turn out to be false.
On the other hand, Shannon’s information-theoretic model of security
provides unconditional security. The perfect secrecy of Vernam’s one-time

9.6 Unconditional Security of Cryptosystems
251
pad is not dependent on the hardness of a computational problem, or limits
of the computational power of an adversary.
Although perfect secrecy is not reachable in most practical situations,
there are various promising attempts to design practical cryptosystems whose
security is not based on assumptions and which provably come close to perfect
information-theoretic security.
One approach is quantum cryptography, introduced by Bennett and Bras-
sard. Two parties agree on a secret key by transmitting polarized photons
over a ﬁber-optic channel. The secrecy of the key is based on the uncertainty
principle of quantum mechanics ([BraCre96]).
In other approaches, the unconditional security of cryptosystems is based
on the fact that communication channels are noisy (and hence, an eaves-
dropper never gets all the information), or on the limited storage capacity of
an adversary (see, e.g., [Maurer99] for an overview on information-theoretic
cryptography).
9.6.1 The Bounded Storage Model
We will give a short introduction to encryption schemes designed by Maurer et
al., whose unconditional security is guaranteed by a limit on the total amount
of storage capacity available to an adversary. Most of the encryption schemes
studied in this approach are similar to the one proposed in [Maurer92], which
we are going to describe now.
Alice wants to transmit messages m ∈M := {0, 1}n to Bob. She uses a
one-time pad for encryption, i.e., she XORs the message m bitwise with a
one-time key k. As usual, we have a probability distribution on M which,
together with the probabilistic choice of the keys, yields a probability distri-
bution on the set C of ciphertexts. Without loss of generality, we assume that
prob(m) > 0 for all m ∈M, and prob(c) > 0 for all c ∈C. The eavesdropper
Eve is a passive attacker: observing the ciphertext c ∈C ⊆{0, 1}n, she tries
to get information about the plaintext.
Alice and Bob extract the key k for the encryption of a message m
from a publicly accessible “table of random bits”. Security is achieved if
Eve has access only to some part of the table. This requires some clever
realization of the public table of random bits. A possibly realistic scenario
(the “satellite scenario”) is that the random bits are broadcast by some radio
source (e.g. a satellite or a natural deep-space radio source) with a suﬃciently
high data rate (see the example below) and Eve can store only parts of it.
So, we assume that there is a public source broadcasting truly (uniformly
distributed) random bits to Alice, Bob and Eve at a high speed. The com-
munication channels are assumed to be error free.
Alice and Bob select their key bits from the bit stream over some time pe-
riod T, according to some private strategy not known to Eve. The ciphertext
is transmitted later, after the end of T. Due to her limited storage resources,
Eve can store only a small part of the bits broadcast during T.

252
9. Provably Secure Encryption
To extract the one-time key k for a message m ∈{0, 1}n, Alice and Bob
synchronize on the source and listen to the broadcast bits over the time period
T. Let R (called the randomizer) be the random bits transmitted during T,
and let (ri,j | 1 ≤i ≤l, 0 ≤j ≤t −1) be these bits arranged as elements of a
matrix with l rows and t columns. Thus, R contains |R| = lt bits. Typically,
l is small (about 50) and t is huge, even when compared with the length n of
the plaintext message m.
Alice and Bob have agreed on a (short) private key (s1, . . . , sl) in ad-
vance, with si randomly chosen in {0, . . . , t−1} (with respect to the uniform
distribution), and take as key k := k0 . . . kn−1, with
kj := r1,(s1+j) mod t ⊕r2,(s2+j) mod t . . . ⊕rl,(sl+j) mod t.
In other words, Alice and Bob select from each row i the bit string bi of length
n, starting at the randomly chosen “seed” si (jumping back to the beginning
if the end of the row is reached) and then get their key k by XORing these
strings bi.
Attacker Eve also listens to the random source during T and stores some
of the bits, hoping that this will help her to extract information when the
encrypted message is transmitted after the end of T. Due to her limited
storage space, she stores only q of the bits ri,j. The bits are selected by some
probabilistic algorithm.7 For each of these q bits, she knows the value and
possibly the position in the random bit stream R. Eve’s knowledge about
the randomizer bits is summarized by the random variable S. You may also
consider S as a probabilistic algorithm, returning q positions and bit values.
As usual in the study of a one-time pad encryption scheme, Eve may know
the distribution on M. We assume that she has no further a priori knowledge
about the messages m ∈M actually sent to Bob by Alice. The following
theorem is proven in [Maurer92].
Theorem 9.21. There exists an event E, such that for all probabilistic strate-
gies S for storing q bits of the randomizer R
I(M; CS |E) = 0 and prob(E) ≥1 −nδl,
where δ := q/|R| is the fraction of randomizer bits stored by Eve.
Proof. We sketch the basic idea and refer the interested reader to [Maurer92].
Let (s1, . . . , sl) be the private key of Alice and Bob, and k := k0 . . . kn−1 be
the key extracted from R by Alice and Bob. Then bit kj is derived from R
by XORing the bits r1,(s1+j) mod t, r2,(s2+j) mod t, . . . , rl,(sl+j) mod t.
If Eve’s storage strategy missed only one of these bits, then the result-
ing bit kj appears truly random to her, despite her knowledge S of the
randomizer. The probability of the event F that Eve stores with her strat-
egy all the bits r1,(s1+j) mod t, r2,(s2+j) mod t, . . . , rl,(sl+j) mod t, for at least one
j, 0 ≤j ≤n −1, is very small – it turns out to be ≤nδl.
7 Note that there are no restrictions on the computing power of Eve.

9.6 Unconditional Security of Cryptosystems
253
The “security event” E is deﬁned as the complement of F. If E occurs,
then, from Eve’s point of view, the key extracted from R by Alice and Bob
is truly random, and we have the situation of Vernam’s one-time pad, with
a mutual information which equals 0 (see Theorem 9.5).
2
Remarks:
1. The mutual information in Theorem 9.21 is conditional on an event E.
This means that all the entropies involved are computed with the condi-
tional probabilities assuming E (see the ﬁnal remark in Appendix B.4).
2. In [Maurer92] a stronger version is proven. It also includes the case that
Eve has some a priori knowledge V of the plaintext messages, where V is
jointly distributed with M. Then the mutual information I(M, CS |V, E)
between M and CS, conditioned over V and assuming E, is 0. Condi-
tioning over V means that the mutual information does not include the
amount of information about M resulting from the knowledge of V (see
Proposition B.36).
3. Adversary Eve cannot gain an advantage from learning the secret key
(s1, . . . , sl) after the broadcast of the randomizer R. Therefore, Theorem
9.23 is a statement on everlasting security.
4. The model of attack applied here is somewhat restricted. In the ﬁrst
phase, while listening to the random source, the eavesdropper Eve does
not exploit her full computing power; she simply stores some of the trans-
mitted bits and does not use the bit stream as input for computations
at that time. The general model of attack, where Eve may compute and
store arbitrary bits of information about the randomizer, is considered
below.
Example. This example is derived from a similar example in [CachMau97]. A
satellite broadcasting random bits at a rate of 16 Gbit/s is used for one day
to provide a randomizer table R with about 1.5·1015 bits. Let R be arranged
in l := 100 rows and t := 1.5 · 1013 columns. Let the plaintexts be 6 MB, i.e.,
n ≈5 · 107 bits. Alice and Bob have to agree on a private key (s1, . . . , sl) of
100 · log2(1.5 · 1013) ≈4380 bits. The storage capacity of the adversary Eve
is assumed to be 100 TB, which equals about 8.8 · 1014 bits. Then δ ≈0.587
and
prob(not E) ≤5 · 107 · 0.587100 ≈3.7 · 10−16 < 10−15.
Thus, the probability that Eve gets any additional information about the
plaintext by observing the ciphertext and applying an optimal storage strat-
egy is less than 10−15.
Theorem 9.21 may also be interpreted in terms of distinguishing algo-
rithms (see Proposition 9.10). We denote by E the probabilistic encryption
algorithm which encrypts m as c := m⊕k, with k randomly chosen as above.
Theorem 9.22. For every probabilistic storage strategy S storing a frac-
tion δ of all randomizer bits, and every probabilistic distinguishing algorithm
A(m0, m1, c, s) and all m0, m1 ∈M, with m0 ̸= m1,

254
9. Provably Secure Encryption
prob(A(m0, m1, c, s) = m : m
u←{m0, m1}, c ←E(m), s ←S) ≤1
2 + nδl.
Remark. This statement is equivalent to
| prob(A(m0, m1, c, s) = m0 : c ←E(m0), s ←S)
−prob(A(m0, m1, c, s) = m0 : c ←E(m1), s ←S) | ≤nδl,
as the same computation as in the proof of Proposition 9.10 shows.
Proof. From Theorem 9.21 and Proposition B.32, we conclude that
prob(c, s|m0, E) = prob(c, s|m1, E)
for all m0, m1 ∈M, c ∈C and s ∈S. Computing with probabilities condi-
tional on E, we get
prob(A(m0, m1, c, s) = m0 |E : c ←E(m0), s ←S)
=
X
c,s
prob(c, s|m0, E) · prob(A(m0, m1, c, s) = m0 |E)
=
X
c,s
prob(c, s|m1, E) · prob(A(m0, m1, c, s) = m0 |E)
= prob(A(m0, m1, c, s) = m0 |E : c ←E(m1), s ←S).
Using Lemma B.9 we get
| prob(A(m0, m1, c, s) = m0 : c ←E(m0), s ←S)
−prob(A(m0, m1, c, s) = m0 : c ←E(m1), s ←S) |
≤prob(not E) ≤nδl,
as desired.
2
As we observed above, the model of attack just used is somewhat re-
stricted, because the eavesdropper Eve does not use the bit stream as input for
computations in the ﬁrst phase – while listening to the random source. Secu-
rity proofs for the general model of attack, where Eve may use her (unlimited)
computing power at any time – without any restrictions – were given only
recently in a series of papers ([AumRab99]; [AumDinRab02]; [DinRab02];
[DziMau02]; [Lu02]; [Vadhan03]).
The results obtained by Aumann and Rabin ([AumRab99]) were still re-
stricted. The randomness eﬃciency, i.e., the ratio δ = q/|R| of Eve’s storage
capacity and the size of the randomizer, was very small.
Major progress on the bounded storage model was then achieved by Au-
mann, Ding and Rabin ([AumDinRab02]; [Ding01]; [DinRab02]) proving the
security of schemes with a randomness eﬃciency of about 0.2.
Strong security results for the general model of attack were shown by
Dziembowski and Maurer in [DziMau02] (an extended version is [DziMau04a]).

9.6 Unconditional Security of Cryptosystems
255
They can prove that keys k, whose length n is much longer than the length
of the initial private key, can be securely generated with a randomness ef-
ﬁciency which may be arbitrarily close to 1. A randomness eﬃciency of 0.1
is possible for reasonable parameter sizes, which appear possible in practice
(for example, with attacker Eve’s storage capacity ≈125 TB, a derived key
k of 1 GB for the one-time pad, an initial private key < 1 KB and a statis-
tical distance < 2−29 between the distribution of the derived key k and the
uniform distribution, from Eve’s point of view).
The schemes investigated in the cited papers are all very similar to the
scheme from [Maurer92], which we explained here. Of course, security proofs
for the general model of attack require more sophisticated methods of prob-
ability and information theory.
In all of the bounded-storage-model schemes that we referred to above
one assumes that Alice and Bob share an initial secret key s, usually without
considering how such a key s is obtained by Alice and Bob. A natural way
would be to exchange the initial key by using a public-key key agreement
protocol, for example, the Diﬃe-Hellman protocol (see Section 4.1.2). At ﬁrst
glance, this approach may appear useless, since the information-theoretic
security against a computationally unbounded adversary Eve is lost – Eve
could break the public-key protocol with her unlimited resources. However,
if Eve is an attacker, who gains her inﬁnite computing power (and then the
initial secret key) only after the broadcast of the randomizer, then the security
of the scheme might be preserved (see [DziMau04b] for a detailed discussion).
In [CachMau97], another approach is discussed. In a variant of their
scheme, the key k for the one-time pad is generated within the bounded
storage model, and Alice and Bob need not share an initial secret key s. The
general model of attack is applied in [CachMau97] – adversary Eve may use
her unlimited computing power at any time8.
Some of the techniques used there are basic.9 To illustrate these tech-
niques, we give a short overview about parts of [CachMau97].
As before, there is some source for truly random bits. Alice, Bob and
Eve receive these random bits over perfect channels without any errors. We
are looking for bit sequences of length n to serve as keys in a one-time pad
for encrypting messages m ∈{0, 1}n. The random bit source generates N
bits R := (r1, . . . , rN). The storage capacity q of Eve is smaller than N, so
she is not able to store the whole randomizer. In contrast to the preceding
model, she not only stores q bits of the randomizer, but also executes some
probabilistic algorithm U while listening to the random source, to compute
q bits of information from R (and store them in her memory). As before, we
8 Unfortunately, the arising schemes are either impractical or attacker Eve’s prob-
ability of success is non-negligible, see below.
9 They are also applied, for example, in the noisy channel model, which we discuss
in Section 9.6.2.

256
9. Provably Secure Encryption
denote by δ := q/N the fraction of Eve’s storage capacity with respect to the
total number of randomizer bits.
In a ﬁrst phase, called advantage distillation, Alice and Bob extract suf-
ﬁciently many, say l, bits S := (s1, . . . , sl) from R, at randomly chosen posi-
tions P := (p1, . . . , pl):
s1 := rp1, s2 := rp2, . . . , sl := rpl.
It is necessary that the positions are chosen pairwise independently. The
positions p1, . . . , pl are kept secret, until the broadcast of the randomizer is
ﬁnished.
Alice and Bob can select the bits in two ways.
1. Private key scenario: Alice and Bob agree on the positions p1, . . . , pl
in advance and share these positions as an initial secret key.
2. Key agreement solely by public discussion: Independently, Alice
and Bob each select and store w bits of the randomizer R. The positions
of the selected bits t1, . . . , tw and u1, . . . , uw are randomly selected, pair-
wise independent and uniformly distributed. When the broadcast of the
randomizer is ﬁnished, Alice and Bob exchange the chosen positions over
the public channel. Let {p1, . . . , pl} = {t1, . . . , tw} ∩{u1, . . . , uw}. Then,
Alice and Bob share the l randomizer bits at the positions p1, . . . , pl. It is
easy to see that the expected number l of common positions is l = w2/N
(see, for example, Corollary B.17). Hence, on average, they have to select
and store
√
lN randomizer bits to obtain l common bits.
Since Eve can store at most q bits, her information about S is incom-
plete. For example, it can be proven that Eve knows at most a fraction δ
of the l bits in S (in the information-theoretic sense). Thus, Alice and Bob
have distilled an advantage. Let e be the integer part of Eve’s uncertainty
H(S |Eve’s knowledge) about S. Then Eve lacks approximately e bits of in-
formation about S.
In a second phase, Alice and Bob apply a powerful technique, called pri-
vacy ampliﬁcation or entropy smoothing, to extract f bits from S in such a
way that Eve has almost no information about the resulting string ˜S. Here,
f is given by the R´enyi entropy of order 2 (see below). Since this entropy is
less than or equal to Shannon’s entropy, we have f ≤e. Eve’s uncertainty
about ˜S is close to f, so from Eve’s point of view, ˜S appears almost truly
random. Thus, it can serve Alice and Bob as a provably secure key k in a
one-time pad.
Privacy ampliﬁcation is accomplished by randomly selecting a member
from a so-called universal class of hash functions (see below). Alice randomly
selects an element h from such a universal class H (with respect to the uniform
distribution) and sends h to Bob via a public channel. Thus, Eve may even
know H and h. Alice and Bob both apply h to S in order to obtain their key
k := h(S) for the one-time pad.

9.6 Unconditional Security of Cryptosystems
257
Let H and K be the random variables describing the probabilistic choice
of the function h and the probabilistic choice of the key k. The following
theorem is proven in [CachMau97].
Theorem 9.23. Given a ﬁxed storage capacity q of Eve and ϵ1, ϵ2 > 0, there
exists a security event E such that
prob(E) ≥1 −ϵ1 and I(K; H |U = u, P = p, E) ≤ϵ2,
and hence in particular
I(K; UHP |E) ≤ϵ2 and I(K; UH |E) ≤ϵ2,
provided the size N of the randomizer R and the number l of elements selected
from R by S are suﬃciently large.
Remarks:
1. Explicit formulae are derived in [CachMau97] which connect the bounds
ϵ1, ϵ2, the size N of the randomizer, Eve’s storage capacity q, the number
l of chosen positions and the number f of derived key bits.
2. The third inequality follows from the second by Proposition B.35, and
the fourth inequality from the third by Proposition B.36 (also observe
the ﬁnal remark in Appendix B.4).
3. I(K; H |U = u, P = p, E) ≤ϵ2 means the following. Assume Eve has
the speciﬁc knowledge U = u about the randomizer and has learned
the positions P of the bits selected from R by Alice and Bob after the
broadcast of the randomizer R. Then the average amount of information
(measured in bits in the information-theoretic sense) that Eve can derive
about the key k from learning the hash function h is less than ϵ2, provided
the security event E occurs.
Thus, in the private key scenario, the bound ϵ2 also holds if the secret key
shared by Alice and Bob (i.e., the positions of the selected randomizer
bits) is later compromised. The security is everlasting.
As we mentioned above, a key step in the proof of Theorem 9.23 is privacy
ampliﬁcation to transform almost all the entropy of a bit string into a random
bit string. For this purpose, it is not suﬃcient to work with the classical
Shannon entropy as deﬁned in Appendix B.4. Instead, it is necessary to use
more general information measures: the R´enyi entropies of order α (0 ≤α ≤
∞, see [R´enyi61]; [R´enyi70]). Here, in particular, the R´enyi entropy of order
2 – also called collision entropy – is needed.
Deﬁnition 9.24. Let S be a random variable with values in the ﬁnite set S.
The collision probability probc(S) of S is deﬁned as
probc(S) :=
X
s∈S
prob(S = s)2.

258
9. Provably Secure Encryption
The collision entropy or R´enyi entropy (of order 2) of S is
H2(S) := −log2(probc(S)) = −log2
ÃX
s∈S
prob(S = s)2
!
.
probc(S) is the probability that two independent executions of S yield the
same result. H2(S) measures the uncertainty that two independent executions
of the random experiment S yield the same result.
The mathematical foundation of privacy ampliﬁcation is the Smoothing
Entropy Theorem. It states that almost all the collision entropy of a random
variable S may be converted into uniform random bits by selecting a function
h randomly from a universal class of hash functions and applying h to S (see,
e.g., [Luby96], Lecture 8). Universal classes of hash functions were introduced
by Carter and Wegman ([CarWeg79]; [WegCar81]).
Deﬁnition 9.25. A set H of functions h : X −→Y is called a universal class
of hash functions if for all distinct x1, x2 ∈X,
prob(h(x1) = h(x2) : h
u←H) =
1
|Y |.
H is called a strongly universal class of hash functions if for all distinct
x1, x2 ∈X and all (not necessarily distinct) y1, y2 ∈Y ,
prob(h(x1) = y1, h(x2) = y2 : h
u←H) =
1
|Y |2 .
In particular, a strongly universal class is also universal. (Strongly) univer-
sal classes of hash functions behave like completely random functions with
respect to collisions (or value pairs).
Example. A straightforward computation shows that the set of linear map-
pings {0, 1}l −→{0, 1}f is a strongly universal class of hash functions. There
are smaller classes ([Stinson92]). For example, the set
H := {ha0,a1 : F2l −→F2f , x 7−→msbf(a0 · x + a1) | a0, a1 ∈F2l}
is strongly universal (l ≥f), and the set
H := {ha : F2l −→F2f , x 7−→msbf(a · x) | a ∈F2l}
is universal. Here, we consider {0, 1}m as equipped with the structure F2m of
the Galois ﬁeld with 2m elements (see Appendix A.5), and msbf denotes the
f most-signiﬁcant bits. See Exercise 8.
Remark. In the key generation scheme discussed, Alice and Bob select w (or
l) bits at pairwise independent random positions from the N bits broadcast
by the random source. They have to store the positions of these bits. At ﬁrst

9.6 Unconditional Security of Cryptosystems
259
glance, w·log2(N) bits are necessary to describe w positions. Since w is large,
a huge number of bits have to be stored and transferred between Alice and
Bob. Strongly universal classes of hash functions also provide a solution to
this problem.
Assume N = 2m, and consider {0, 1}m as F2m. Alice and Bob may work
with the strongly universal class
H = {h : F2m −→F2m, x 7−→a0 · x + a1 | a0, a1 ∈F2m}
of hash functions. They ﬁx pairwise diﬀerent elements x1, . . . , xw ∈F2m in
advance. H and the xi may be known to Eve. Now, to select w positions
randomly – uniformly distributed and pairwise independent – Alice or Bob
randomly chooses some h from H (with respect to the uniform distribution)
and applies h to the xi. This yields w uniformly distributed and pairwise
independent positions
h(x1), h(x2), . . . , h(xw).
Thus, the random choice of the w positions reduces to the random choice of
an element h in H, and this requires the random choice of 2m = 2 log2(N)
bits.
Example. Assume that Alice and Bob do not share an initial private key
and the key is derived solely by a public discussion. Using the explicit for-
mulae, you get the following example for the Cachin-Maurer scheme (see
[CachMau97] for more details). A satellite broadcasting random bits at a
rate of 40 Gbit/s is used for 2 · 105 seconds (about 2 days) to provide a
randomizer R with about N = 8.6 · 1015 bits. The storage capacity of the
adversary Eve is assumed to be 1/2 PB, which equals about 4.5 · 1015 bits.
To get l = 1.3 · 107 common positions and common random bits, Alice and
Bob each have to select and store w =
√
lN = 3.3 · 1011 bits (or about 39
GB) from R. By privacy ampliﬁcation, they get a key k of about 61 KB and
Eve knows not more than 10−20 bits of k, provided that the security event
E occurs. The probability of E is ≥1 −ε1 with ε1 = 10−3. Since l is of the
order of 1/ε2
1, the probability that the security event E does not occur can
not be reduced to signiﬁcantly smaller values, without increasing the storage
requirements for Alice to unreasonably high values. To choose the w positions
of the randomizer bits which they store, Alice and Bob each randomly select a
strongly universal hash function (see the preceding remark). So, to exchange
these positions, Alice and Bob have to transmit a strongly universal hash
function in each direction, which requires 2 log2(N) ≈106 bits. For privacy
ampliﬁcation, either Alice or Bob chooses the random universal hash function
and communicates it, which takes about l bits ≈1.5 MB. The large size of
the hash functions may be substantially reduced by using “almost universal”
hash functions.

260
9. Provably Secure Encryption
Remarks:
1. Alice and Bob need – as the example demonstrates – a very large capacity
for storing the positions and the values of the randomizer bits, and the
size of this storage rapidly increases if the probability of the security
event E is required to be signiﬁcantly closer to 1 than 10−3, which is
certainly not negligible. To restrict adversary Eve’s probability of success
to negligible values would require unreasonably high storage capacities
of Alice and Bob. This is also true for the private-key scenario.
2. If the key k is derived solely by a public discussion, then both Alice and
Bob need storage on the order of
√
N, which is also on the order of √q
(recall that N is the size of the randomizer and q is the storage size of
the attacker). It is shown in [DziMau04b] that these storage requirements
can not be reduced. The Cachin-Maurer scheme is essentially optimal in
terms of the ratio between the storage capacity of Alice and Bob and
the storage capacity of adversary Eve. The practicality of schemes in the
bounded storage model which do not rely on a shared initial secret key
is therefore highly questionable.
9.6.2 The Noisy Channel Model
An introduction to the noisy channel model is, for example, given in the
survey article [Wolf98]. As before, we use the “satellite scenario”. Random
bits are broadcast by some radio source. Alice and Bob receive these bits
and generate a key from them by a public discussion. The eavesdropper, Eve,
also receives the random bits and can listen to the communication channel
between Alice and Bob. Again we assume that Eve is a passive adversary.
There are other models including an active adversary (see, e.g., [Maurer97];
[MauWol97]). Though all communications are public, Eve gains hardly any
information about the key. Thus, the generated key appears almost random
to Eve and can be used in a provably secure one-time pad. The secrecy of
the key is based on the fact that no information channel is error-free. The
system also works in the case where Eve receives the random bits via a much
better channel than Alice and Bob.
The key agreement works in three phases. As in Section 9.6.1, it starts
with advantage distillation and ends with privacy ampliﬁcation. There is an
additional intermediate phase called information reconciliation.
During advantage distillation, Alice chooses a truly random key k, for ex-
ample from the radio source. Before transmitting it to Bob, she uses a random
bit string r to mask k and to make the transmission to Bob highly reliable
(applying a suitable error detection code randomized by r). The random bit
string r is taken from the radio source and commonly available to all par-
ticipants. If suﬃcient redundancy and randomness is built in, which means
that the random bit string r is suﬃciently long, the error probability of the

Exercises
261
adversary is higher than the error probability of the legitimate recipient Bob.
In this way, Alice and Bob gain an advantage over Eve.
When phase 1 is ﬁnished, the random string k held by Alice may still diﬀer
from the string k′ received by Bob. Now, Alice and Bob start information
reconciliation and interactively modify k and k′, such that at the end, the
probability that k ̸= k′ is negligibly small. This must be performed without
leaking too much information to the adversary Eve. Alice and Bob may, for
example, try to detect diﬀering positions in the string by comparing the parity
bits of randomly chosen substrings.
After phase 2, the same random string k of size l is available to Alice and
Bob with a very high probability, and they have an advantage over Eve. Eve’s
information about k, measured by R´enyi entropies, is incomplete. Applying
the privacy ampliﬁcation techniques sketched in Section 9.6.1 Alice and Bob
obtain their desired key.
Exercises
1. Let n ∈N. We consider the aﬃne cipher modulo n. It is a symmetric
encryption scheme. A key (a, b) consists of a unit a ∈Z∗
n and an element
b ∈Zn. A message m ∈Zn is encrypted as a · m + b.
Is the aﬃne cipher perfectly secret if we randomly (and uniformly) choose
a key for each message m to be encrypted?
2. Let E be an encryption algorithm, which encrypts plaintexts m ∈M
as ciphertexts c ∈C, and let K denote the secret key used to decrypt
ciphertexts.
Show that an adversary’s uncertainty about the secret key is at least as
great as her uncertainty about the plaintext: H(K |C) ≥H(M |C).
3. ElGamal’s encryption (Section 3.5.1) is probabilistic. Is it computation-
ally secret?
4. Consider Deﬁnition 9.14 of computationally secret encryptions. Show
that an encryption algorithm E(i, m) is computationally secret if and
only if for every probabilistic polynomial distinguishing algorithm
A(i, m0, m1, c) and every probabilistic polynomial sampling algorithm
S, which on input i ∈I yields S(i) = {m0, m1} ⊂Mi, and every positive
polynomial P ∈Z[X], there is a k0 ∈N such that for all k ≥k0,
prob(A(i, m0, m1, c) = m0 : i ←K(1k), {m0, m1} ←S(i), c ←E(m0))
−prob(A(i, m0, m1, c) = m0 : i ←K(1k), {m0, m1} ←S(i), c ←E(m1))
≤
1
P(k).

262
9. Provably Secure Encryption
5. Let Ik := {(n, e) | n = pq, p, q distinct primes, |p| = |q| = k, e ∈Z∗
ϕ(n)}
and (n, e)
u←Ik be a randomly chosen RSA key. Encrypt messages m ∈
{0, 1}r, where r ≤log2(|n|), in the following way. Pad m with leading
random bits to get a padded plaintext m with |m| = |n|. If m > n, then
repeat the padding of m until m < n. Then encrypt m as E(m) := c :=
me mod n.
Prove that this scheme is computationally secret.
Hint: use Exercise 7 in Chapter 8.
6. Let I = (Ik)k∈N be a key set with security parameter, and let f =
(fi : Di −→Di)i∈I be a family of one-way trapdoor permutations with
hard-core predicate B = (Bi : Di −→{0, 1})i∈I and key generator K.
Consider the following probabilistic public-key encryption scheme
([GolMic84]; [GolBel01]): Let Q be a polynomial and n := Q(k). A
bit string m := m1 . . . mn is encrypted as a concatenation c1|| . . . ||cn,
where cj := fi(xj) and xj is a randomly selected element of Di with
Bi(xj) = mj.
Describe the decryption procedure and show that the encryption scheme
is computationally secret.
Hints: Apply Exercise 4. Given a pair {m0, m1} of plaintexts, construct
a sequence of messages ˜m1 := m0, ˜m2, . . . , ˜mn := m1, such that ˜mj+1
diﬀers from ˜mj in at most one bit. Then consider the sequence of distri-
butions c ←E(i, ˜mj) (also see the proof of Proposition 8.4).
7. We consider the Goldwasser-Micali probabilistic encryption scheme
([GolMic84]). Let Ik := {n | n = pq, p, q distinct primes , |p| = |q| = k}
and I := (Ik)k∈N. As his public key, each user Bob randomly chooses an
n
u←Ik (by ﬁrst randomly choosing the secret primes p and q) and a
quadratic non-residue z
u←QNRn with Jacobi symbol
¡ z
n
¢
= 1 (he can
do this easily, since he knows the primes p and q; see Appendix A.6).
A bit string m = m1 . . . mn is encrypted as a concatenation c1|| . . . ||cn,
where cj = x2
j if mj = 1, and cj = zx2
j if mj = 0, with a randomly chosen
xj
u←Z∗
n. In other words: A 1 bit is encrypted by a random quadratic
residue, a 0 bit by a random non-residue.
Describe the decryption procedure and show that the encryption scheme
is computationally secret, provided the quadratic residuosity assumption
(Deﬁnition 6.11) is true.
Hint: The proof is similar to the proof of Exercise 6. Use Exercise 9 in
Chapter 6.
8. Let l ≥f. Prove that:
a. H := {ha : F2l −→F2f , x 7−→msbf(a · x) | a ∈F2l} is a universal
class of hash functions.
b. H := {ha0,a1 : F2l −→F2f , x 7−→msbf(a0 · x + a1) | a0, a1 ∈F2l} is
a strongly universal class of hash functions.

Exercises
263
Here we consider {0, 1}m as equipped with the structure F2m of the Galois
ﬁeld with 2m elements. As before, msbf denotes the f most-signiﬁcant
bits.

10. Provably Secure Digital Signatures
In previous sections, we discussed signature schemes (Full-Domain-Hash RSA
signatures and PSS in Section 3.4.5; the Fiat-Shamir signature scheme in
Section 4.2.5) that include a hash function h and whose security can be
proven in the random oracle model. It is assumed that the hash function
h is a random oracle, i.e., it behaves like a perfectly random function (see
Sections 3.4.4 and 3.4.5). Perfectly random means that for all messages m,
each of the k bits of the hash value h(m) is determined by tossing a coin,
or, equivalently, that the map h : X −→Y is randomly chosen from the set
F(X, Y ) of all functions from X to Y . In general, F(X, Y ) is tremendously
large. For example, if X = {0, 1}n and Y = {0, 1}k, then |F(X, Y )| = 2k2n.
Thus, it is obvious that perfectly random oracles cannot be implemented.
Moreover, examples of cryptographic schemes were constructed that are
provably secure in the random oracle model, but are insecure in any real-world
implementation, where the random oracle is replaced by a real hash function.
Although these examples are contrived, doubts on the random oracle model
arose (see the remark on page 244 in Section 9.5).
Therefore, it is desirable to have signature schemes whose security can
be proven solely under standard assumptions (like the RSA or the discrete
logarithm assumption). Examples of such signature schemes are given in this
chapter.
10.1 Attacks and Levels of Security
Digital signature schemes are public-key cryptosystems and are based on the
one-way feature of a number-theoretical function. A digital signature scheme,
for example the basic RSA signature scheme (Section 3.3.2) or ElGamal’s
signature scheme (Section 3.5.2), consists of the following:
1. A key generation algorithm K, which on input 1k (k being the security
parameter) produces a pair (pk, sk) consisting of a public key pk and a
secret (private) key sk.
2. A signing algorithm S(sk, m), which given the secret key sk of user Alice
and a message m to be signed, generates Alice’s signature σ for m.

266
10. Provably Secure Digital Signatures
3. A veriﬁcation algorithm V (pk, m, σ), which given Alice’s public key pk,
a message m and a signature σ, checks whether σ is a valid signature of
Alice for m. Valid means that σ might be output by S(sk, m), where sk
is Alice’s secret key.
Of course, all algorithms must be polynomial. The key generation algorithm
K is always a probabilistic algorithm. In many cases, the signing algorithm
is also probabilistic (see, e.g., ElGamal or PSS). The veriﬁcation algorithm
might be probabilistic, but in practice it usually is deterministic.
As with encryption schemes, there are diﬀerent types of attacks on signa-
ture schemes. We may distinguish between (see [GolMicRiv88]):
1. Key-only attack. The adversary Eve only knows the public key of the
signer Alice.
2. Known-signature attack. Eve knows the public key of Alice and has seen
message-signature pairs produced by Alice.
3. Chosen-message attack. Eve may choose a list (m1, . . . , mt) of messages
and ask Alice to sign these messages.
4. Adaptively-chosen-message attack. Eve can adaptively choose messages
to be signed by Alice. She can choose some messages and gets the cor-
responding signatures. Then she can do cryptanalysis and, depending
on the outcome of her analysis, she can choose the next message to be
signed, and so on.
Adversary Eve’s level of success may be described in increasing order as (see
[GolMicRiv88]):
1. Existential forgery. Eve is able to forge the signature of at least one
message, not necessarily the one of her choice.
2. Selective forgery. Eve succeeds in forging the signature of some messages
of her choice.
3. Universal forgery. Although unable to ﬁnd Alice’s secret key, Eve is able
to forge the signature of any message.
4. Retrieval of secret keys. Eve ﬁnds out Alice’s secret key.
As we have seen before, signatures in the basic RSA, ElGamal and DSA
schemes, without ﬁrst applying a suitable hash function, can be easily exis-
tentially forged using a key-only attack (see Section 3). In the basic Rabin
scheme, secret keys may be retrieved by a chosen-message attack (see Section
3.6.1). We may deﬁne the level of security of a signature scheme by the level
of success of an adversary performing a certain type of attack. Diﬀerent levels
of security may be required in diﬀerent applications.
In this chapter, we are interested in signature schemes which provide the
maximum level of security. The adversary Eve cannot succeed in an existen-
tial forgery with a signiﬁcant probability, even if she is able to perform an
adaptively-chosen-message attack. As usual, the adversary is modeled as a
probabilistic polynomial algorithm.

10.1 Attacks and Levels of Security
267
Deﬁnition 10.1. Let D be a digital signature scheme, with key generation
algorithm K, signing algorithm S and veriﬁcation algorithm V . An exis-
tential forger F for D is a probabilistic polynomial algorithm F that on
input of a public key pk outputs a message-signature pair (m, σ) := F(pk).
F is successful on pk if σ is a valid signature of m with respect to pk, i.e.,
V (pk, F(pk)) = accept. F performs an adaptively-chosen-message attack if,
while computing F(pk), F can repeatedly generate a message ˜m and then is
supplied with a valid signature ˜σ for ˜m.
Remarks. Let F be an existential forger performing an adaptively-chosen-
message attack:
1. Let (pk, sk) be a key of security parameter k. Since the running time
of F(pk) is bounded by a polynomial in k (note that pk is generated in
polynomial time from 1k), the number of messages for which F requests
a signature is bounded by T(k), where T is a polynomial.
2. The deﬁnition leaves open who supplies F with the valid signatures. If
F is used in an attack against the legitimate signer, then the signatures
˜σ are supplied by the signing algorithm S, S(sk, ˜m) = ˜σ, where sk is the
private key associated with pk. In a typical security proof, the signatures
are supplied by a “simulated signer”, who is able to generate valid sig-
natures without knowing the trapdoor information that is necessary to
derive sk from pk. This sounds mysterious and impossible. Actually, for
some time it was believed that a security proof for a signature scheme is
not possible, because it would necessarily yield an algorithm for invert-
ing the underlying one-way function. However, the security proof given
by Goldwasser, Micali and Rivest for their GMR scheme (discussed in
Section 10.3) proved the contrary. See [GolMicRiv88], Section 4: The
paradox of proving signature schemes secure. The key idea for solving
the paradox is that the simulated signer constructs signatures for keys
whose form is a very speciﬁc one, whereas their probability distribution
is the same as the distribution of the original keys (see, e.g., the proof of
Theorem 10.12).
3. The signatures σi, 1 ≤i ≤T(k), supplied to F are, besides pk, inputs to
F. The messages mi, 1 ≤i ≤T(k), for which F requests signatures, are
outputs of F. Let Mi be the random variable describing the i-th message
mi. Since F adaptively chooses the messages, message mi may depend on
the messages mj and the signatures σj supplied to F for mj, 1 ≤j < i.
Thus Mi may be considered as a probabilistic algorithm with inputs pk
and (mj, σj)1≤j<i.
The probability of success of F for security parameter k is then computed
as1
1 Unless otherwise stated, we always mean F’s probability of success when the
signatures for the adaptively chosen messages are supplied by the legitimate
signer S.

268
10. Provably Secure Digital Signatures
prob(V (pk, F(pk, (σi)1≤i≤T (k))) = accept : (pk, sk) ←K(1k),
mi ←Mi(pk, (mj, σj)1≤j<i), σi ←S(sk, mi), 1 ≤i ≤T(k)).
Deﬁnition 10.2. A digital signature scheme is secure against adaptively-
chosen-message attacks if and only if for every existential forger F performing
an adaptively-chosen-message attack and every positive polynomial P, there
is a k0 ∈N such that for all security parameters k ≥k0, the probability of
success of F is ≤1/P(k).
Remark. Fail-stop signature schemes provide an additional security feature.
If a forger – even if he has unlimited computing power and can do an ex-
ponential amount of work – succeeds in generating a valid signature, then
the legitimate signer Alice can prove with a high probability that the signa-
ture is forged. In particular, Alice can detect forgeries and then stop using
the signing mechanism (“fail then stop”). The signature scheme is based on
the assumed hardness of a computational problem, and the proof of forgery
is performed by showing that this underlying assumption has been compro-
mised. Fail-stop signature schemes were introduced by Waidner and Pﬁtz-
mann ([WaiPﬁ89]). We do not discuss fail-stop signatures here (see, e.g.,
[Pﬁtzmann96]; [MenOorVan96]; [Stinson95]; [BarPﬁ97]).
10.2 Claw-Free Pairs and Collision-Resistant Hash
Functions
In many digital signature schemes, the message to be signed is ﬁrst hashed
with a collision-resistant hash function. Provably collision-resistant hash func-
tions can be constructed from claw-free pairs of trapdoor permutations. In
Section 10.3 we will discuss the GMR signature scheme introduced by Gold-
wasser, Micali and Rivest. It was the ﬁrst signature scheme that was provably
secure against adaptively-chosen-message attacks (without depending on the
random oracle model), and it is based on claw-free pairs.
Deﬁnition 10.3. Let f0 : D −→D and f1 : D −→D be permutations of the
same domain D. A pair (x, y) is called a claw of f0 and f1 if f0(x) = f1(y).
Let I = (Ik)k∈N be a key set with security parameter k. We consider
families
f0 = (f0,i : Di −→Di)i∈I, f1 = (f1,i : Di −→Di)i∈I
of one-way permutations with common key generator K that are deﬁned on
the same domains.
Deﬁnition 10.4. (f0, f1) is called a claw-free pair of one-way permutations
if it is infeasible to compute claws; i.e., for every probabilistic polynomial
algorithm A which on input i outputs distinct elements x, y ∈Di, and for
every positive polynomial P, there is a k0 ∈N such that for all k ≥k0

10.2 Claw-Free Pairs and Collision-Resistant Hash Functions
269
prob(f0,i(x) = f1,i(y) : i ←K(1k), {x, y} ←A(i)) ≤
1
P(k) .
Claw-free pairs of one-way permutations exist, if, for example, factoring
is hard.
Proposition 10.5. Let I := {n | n = pq, p, q primes, p ≡3 mod 8, q ≡
7 mod 8}. If the factoring assumption (Deﬁnition 6.9) holds, then
CQ := (fn, gn : QRn −→QRn)n∈I,
where fn(x) := x2 and gn(x) := 4x2, is a claw-free pair of one-way permuta-
tions (with trapdoor).
Proof. Let n = pq ∈I. Since both primes, p and q, are congruent 3 modulo
4, fn is a permutation of QRn (Proposition A.66). Four is a square and it is
a unit in Zn. Thus, gn is also a permutation of QRn. Now, let x, y ∈QRn
with x2 = 4y2 mod n be a claw. From n ≡1 mod 4 and n ≡−3 mod 8,
we conclude
¡ −1
n
¢
= 1 and
¡ 2
n
¢
= −1 (Theorem A.57). We get
¡ ±2y
n
¢
=
¡ ±1
n
¢
·
¡ 2
n
¢
·
¡ y
n
¢
= 1·(−1)·1 = −1, whereas
¡ x
n
¢
= 1 since x is a square. Thus
x ̸= ±2y, and the Euclidean algorithm yields a factorization of n. Namely,
0 = x2 −4y2 = (x−2y)(x+2y) = 0 and thus gcd(x2 −4y2, n) is a non-trivial
divisor of n. We see that an algorithm which, with some probability, ﬁnds
claws of CQ yields an algorithm factoring n with the same probability, which
is a contradiction to the factoring assumption.
2
Claw-free pairs of one-way permutations can be used to construct collision-
resistant hash functions.
Deﬁnition 10.6. Let I = (Ik)k∈N be a key set with security parameter k,
and let K be a probabilistic polynomial sampling algorithm for I, which on
input 1k outputs a key i ∈Ik. Let k(i) be the security parameter of i (i.e.,
k(i) = k for i ∈Ik), and g : N −→N be a polynomial function. A family
H =
¡
hi : {0, 1}∗−→{0, 1}g(k(i))¢
i∈I
of hash functions is called a family of collision-resistant (or collision-free)
hash functions (or a collision-resistant hash function for short) with key gen-
erator K, if:
1. The hash values hi(x) can be computed by a polynomial algorithm H
with inputs i ∈I and x ∈{0, 1}∗.
2. It is computationally infeasible to ﬁnd a collision; i.e., for every proba-
bilistic polynomial algorithm A which on input i ∈I outputs messages
m0, m1 ∈{0, 1}∗, m0 ̸= m1, and for every positive polynomial P, there
is a k0 ∈N such that
prob(hi(m0) = hi(m1) : i ←K(1k), {m0, m1} ←A(i)) ≤
1
P(k),
for all k ≥k0.

270
10. Provably Secure Digital Signatures
Remark. Collision-resistant hash functions are one way (see Exercise 2).
Let (f0, f1) be a pair of one-way permutations, as above. For every m ∈
{0, 1}∗, we may derive a family fm = (fm,i : Di −→Di)i∈I as follows. For
m := m1 . . . ml ∈{0, 1}l and x ∈Di, let
fm,i(x) := fm1,i(fm2,i(. . . fml,i(x) . . .)).
If m ∈{0, 1}∗is the concatenation m := m1||m2 of strings m1 and m2, then
obviously fm,i(x) = fm1,i(fm2,i(x)).
This family may now be used to construct a family H = (hj)j∈J of hash
functions. Let Jk := {(i, x) | i ∈Ik, x ∈Di} and J := S
k∈N Jk. We deﬁne
Fj(m) := fm,i(x) ∈Di for j = (i, x) ∈J and m ∈{0, 1}∗.
Our goal is a collision-resistant family of hash functions H. To achieve this,
we have to modify our construction a little and ﬁrst encode our messages m
in a preﬁx-free way. Let [m] denote a preﬁx-free (binary) encoding of the
messages m in {0, 1}∗. Preﬁx-free means that no encoded [m] appears as a
preﬁx2 in the encoding [m′] of an m′ ̸= m. For example, we might encode
each 1 by 1, each 0 by 00 and terminate all encoded messages by 01.3 We
deﬁne
hj(m) := Fj([m]), for m ∈{0, 1}∗and j ∈J.
We will prove that H is a collision-resistant family of hash functions if the
pair (f0, f1) is claw-free. Thus, we obtain the following proposition.
Proposition 10.7. If claw-free pairs of one-way permutations exist, then
collision-resistant hash functions also exist.
Proof. Let H be the family of hash functions constructed above. Assume that
H is not collision-resistant. This means that there is a positive polynomial P
and a probabilistic polynomial algorithm A which on input j = (i, x) ∈Jk
ﬁnds a collision {m, m′} of hj with non-negligible probability ≥1/P(k)
(where P is a positive polynomial) for inﬁnitely many k. Collision means
that f[m],i(x) = f[m′],i(x). Let [m] = m1 . . . mr and [m′] = m′
1 . . . m′
r′
(mj, m′
j′ ∈{0, 1}), and let l be the smallest index u with mu ̸= m′
u.
Such an index l exists, since [m] is not a preﬁx of [m′], nor vice versa.
We have fml...mr,i(x) = fm′
l...m′
r′,i(x), since f0,i and f1,i are injective. Then
(fml+1...mr,i(x), fm′
l+1...m′
r′,i(x)) is a claw of (f0, f1). The binary lengths r
and r′ of m and m′ are bounded by a polynomial in k, since m and m′
are computed by the polynomial algorithm A. Thus, the claw of (f0, f1) can
be computed from the collision {m, m′} in polynomial time. Hence, we can
compute claws with non-negligible probability ≥1/P(k), for inﬁnitely many
k, which is a contradiction.
2
2 A string s is called a preﬁx of a string t if t = s||s′ is the concatenation of s and
another string s′.
3 Eﬃcient preﬁx-free encodings exist, such that [m] has almost the same length as
m (see, e.g., [BerPer85]).

10.3 Authentication-Tree-Based Signatures
271
Remarks:
1. The constructed hash functions are rather ineﬃcient. Thus, in practice,
custom-designed hash functions such as SHA are used, whose collision
resistance cannot be proven rigorously (see Section 3.4).
2. Larger sets of pairwise claw-free one-way permutations may be used in
the construction instead of one pair, for example sets with 2s elements.
Then s bits of the messages m are processed in one step. There are larger
sets of pairwise claw-free one-way permutations that are based on the
assumptions that factoring and the computation of discrete logarithms
are infeasible (see [Damg˚ard87]).
3. Another method of constructing provably collision-resistant hash func-
tions is given in Exercise 8 in Chapter 3. It is based on the assumed
infeasibility of computing discrete logarithms.
10.3 Authentication-Tree-Based Signatures
Again, we consider a claw-free pair (f0, f1) of one-way permutations, as above.
In addition, we assume that f0 and f1 have trapdoors. Such a claw-free pair of
trapdoor permutations and the induced functions fm, as deﬁned above, may
be used to generate probabilistic signatures. Namely, Alice randomly chooses
some i ∈I (with a suﬃciently large security parameter k) and some x ∈Di,
and publishes (i, x) as her public key. Then Alice, by using her trapdoor
information, computes her signature σ(i, x, m) for a message m ∈{0, 1}∗as
σ(i, x, m) := f −1
[m],i(x),
where [m] denotes some (ﬁxed) preﬁx-free encoding of m. Bob can verify
Alice’s signature σ by comparing f[m],i(σ) with x. Since f[m],i is one way
and m 7→f[m],i(σ) is collision resistant, as we have just seen in the proof of
Proposition 10.7, only Alice can compute the signature for m, and no one
can use one signature σ for two diﬀerent messages m and m′. Unfortunately,
this scheme is a one-time signature scheme. This means that only one mes-
sage can be signed by Alice with her key (i, x); otherwise the security is not
guaranteed.4 If two messages m ̸= m′ were signed with the same reference
value x, then a claw of f0 and f1 can be easily computed from σ(i, x, m) and
σ(i, x, m′) (see Exercise 5)5, and this can be a severe security risk. If we use,
for example, the claw-free pair of Proposition 10.5, then Alice’s secret key
(the factors of the modulus n) can be easily retrieved from the computed
claw.
4 More
examples
of
one-time
signature
schemes
may
be
found,
e.g.,
in
[MenOorVan96] and [Stinson95].
5 It is not a contradiction to the assumed claw-freeness of the pair that the claw
can be computed, because here the adversary is additionally supplied with two
signatures which can only be computed by use of the secret trapdoor information.

272
10. Provably Secure Digital Signatures
In the GMR signature scheme ([GolMicRiv88]), Goldwasser, Micali and
Rivest overcome this diﬃculty by using a new random reference value for
each message m. Of course, it is not possible to publish all these reference
values as a public key in advance, so the reference values are attached to the
signatures. Then it is necessary to authenticate these reference values, and
this is accomplished by a second claw-free pair of trapdoor permutations.
The GMR scheme is based on two claw-free pairs
(f0, f1) = (f0,i, f1,i : Di −→Di)i∈I, (g0, g1) = (g0,j, g1,j : Ej −→Ej)j∈J
of trapdoor permutations, deﬁned over I = (Ik)k∈N and J = (Jk)k∈N. Each
user Alice runs a key-generation algorithm K(1k) to randomly choose an
i ∈Ik and an j ∈Jk (and generate the associated trapdoor information).
Moreover, Alice generates a binary “authentication tree” of depth d. The
nodes of this tree are randomly chosen elements in Di. Then Alice publishes
(i, j, r) as her public key, where r is the root of the authentication tree.
The authentication tree has 2d leaves vl, 1 ≤l ≤2d. Alice can now sign
up to 2d messages. To sign the l-th message m, she takes the l-th leaf vl
and takes as the ﬁrst part σ1(m) of the signature the previously deﬁned
probabilistic signature f −1
[m],i(vl) with respect to the reference value vl.6 The
second part σ2(m) of the signature authenticates vl. It contains the elements
x0 := r, x1, . . . , xd := vl on the path from the root r to the leaf vl in the
authentication tree and authentication values for each node xm, 1 ≤m ≤d.
The authentication value of xm contains the parent xm−1 and both of its
children c0 and c1 (one of them is xm) and the signature g−1
[c0||c1],j(xm−1) of
the concatenated children with respect to the reference value xm−1, computed
by the second claw-free pair. The children of a node are authenticated jointly.
To verify Alice’s signatures, Bob has to climb up the tree from the leaf in
the obvious way. If he ﬁnally computes the correct root r, he accepts the
signature.
Theorem 10.8. The GMR signature scheme is secure against adaptively-
chosen-message attacks.
Proof. See [GolMicRiv88].
2
Remarks:
1. The full authentication tree and the authentication values for its nodes
could be constructed in advance and stored. However, it is more eﬃcient
to develop it dynamically, as it is needed for signatures, and to store only
the necessary information about its current state.
2. The size of a GMR signature is of order O(kd) if the inputs of the claw-
free pairs are of order O(k) (as in the pair given in Proposition 10.5).
In practice, this size is considerable if the desired number n of signa-
tures and hence d = log2(n) increases. For example, think only of 10000
6 Here we simplify a little and omit the “bridge items”.

10.4 A State-Free Signature Scheme
273
signatures with a security parameter k = 1024. The size of the signa-
tures and the number of computations of f – necessary to generate and
to verify signatures – could be substantially reduced if authentication
trees with much larger branching degrees could be used instead of the
binary one, thus reducing the distance from a leaf to the root. Such sig-
nature schemes have been developed, for example, by Dwork and Naor
([DwoNao94]) and Cramer and Damg˚ard ([CraDam96]). They are quite
eﬃcient and provably provide security against adaptively-chosen-message
attacks if the RSA assumption 6.7 holds. For example, taking a 1024-bit
RSA modulus, a branching degree of 1000 and a tree of depth 3 in the
Cramer-Damg˚ard scheme, Alice could sign up to 109 messages, with the
size of each signature being less than 4000 bits.
10.4 A State-Free Signature Scheme
The signing algorithm in GMR or in other authentication-tree-based signa-
ture schemes is not state free: the signing algorithm has to store the cur-
rent state of the authentication tree which depends on the already gener-
ated signatures, and the next signature depends on this state. In this sec-
tion, we will describe a provably secure and quite eﬃcient state-free digi-
tal signature scheme introduced by Cramer and Shoup ([CraSho2000]). The
scheme is secure against adaptively-chosen-message attacks, provided the so-
called strong RSA assumption (see below) holds. Another state-free signature
scheme based on the strong RSA assumption, has been, for example, intro-
duced in [GenHalRab99]. The security proof, which we will give below, shows
the typical features of such a proof. It runs with a contradiction: a successful
forging algorithm is used to construct an attacker A who successfully inverts
the underlying one-way function. The main problem is that in a chosen-
message attack, the forger F is only successful if he can request signatures
from the legitimate signer. Now the legitimate signer, who uses his secret key,
cannot be called during the execution of F, because A is only allowed to use
publicly accessible information. Thus, a major problem is to substitute the
legitimate signer by a simulation.
The moduli in the Cramer-Shoup signature scheme are deﬁned with spe-
cial types of primes.
Deﬁnition 10.9. A prime p is called a Sophie Germain prime if 2p + 1 is
also a prime.7
Remark. In the Cramer-Shoup signature scheme we have to assume that suf-
ﬁciently many Sophie Germain primes exist. Otherwise there is no guarantee
that keys can be generated in polynomial time. The security proof given below
7 Sophie Germain (1776–1831) proved the ﬁrst case of Fermat’s Last Theorem for
prime exponents p, for which 2p + 1 is also prime ([Osen74]).

274
10. Provably Secure Digital Signatures
also relies on this assumption (see Lemma 10.11). There must be a positive
polynomial P, such that the number of k-bit Sophie Germain primes p is
≥2k/P(k). Today, there is no rigorous mathematical proof for this. It is not
even known whether there are inﬁnitely many Sophie Germain primes. On the
other hand, it is conjectured and there are heuristic arguments and numerical
evidence that the number of k-bit Sophie Germain primes is asymptotically
equal to c · 2k/k2, where c is an explicitly computable constant ([Koblitz88];
[BatHor62]; [BatHor65]). Thus, there is convincing evidence for the existence
of suﬃciently many Sophie Germain primes.
The Strong RSA Assumption. The security of the Cramer-Shoup signa-
ture scheme is based on the following strong RSA assumption introduced in
[BarPﬁ97].
Let I := {n ∈N | n = pq, p ̸= q prime numbers , |p| = |q|} be the set of
RSA moduli and Ik := {n ∈I | n = pq, |p| = |q| = k}.
Deﬁnition 10.10 (strong RSA assumption). For every positive polynomial
Q and every probabilistic polynomial algorithm A which on inputs n ∈I and
y ∈Z∗
n outputs an exponent e > 1 and an x ∈Z∗
n, there exists a k0 ∈N such
that
prob(xe = y : n
u←Ik, y
u←Z∗
n, (e, x) ←A(n, y)) ≤
1
Q(k)
for k ≥k0.
Remark. The strong RSA assumption implies the classical RSA assumption
(Deﬁnition 6.7). In the classical RSA assumption, the attacking algorithm has
to ﬁnd an e-th root for y ∈Z∗
n, for a given e. Here the exponent is not given.
The adversary is successful if, given some y ∈Z∗
n, she can ﬁnd an exponent
e > 1, such that she is able to extract the e-th root x of y. Today, the only
known method for breaking either assumption is to solve the factorization
problem.
Let
ISG := {n ∈I | n = pq, p = 2˜p + 1, q = 2˜q + 1, ˜p, ˜q Sophie Germain primes}
and ISG,k := ISG ∩Ik.
Lemma 10.11. Assume that there is a positive polynomial P, such that the
number of k-bit Sophie Germain primes is ≥2k/P(k). Then the strong RSA
assumption implies that for every positive polynomial Q and every probabilis-
tic polynomial algorithm A which on inputs n ∈ISG and y ∈Z∗
n outputs an
exponent e > 1 and an x ∈Z∗
n, there exists a k0 ∈N such that
prob(xe = y : n
u←ISG,k, y
u←Z∗
n, (e, x) ←A(n, y)) ≤
1
Q(k)
for k ≥k0.

10.4 A State-Free Signature Scheme
275
Proof. The distribution n
u←ISG,k is polynomially bounded by the distribu-
tion n
u←Ik if the existence of suﬃciently many Sophie Germain primes is
assumed. Thus, we may replace n
u←Ik by n
u←ISG,k in the strong RSA
assumption (Proposition B.26).
2
The Cramer-Shoup Signature Scheme. In the key generation and in the
signing procedure, a probabilistic polynomial algorithm GenPrime(1k) with
the following properties is used:
1. On input 1k, GenPrime outputs a k-bit prime.
2. If GenPrime(1k) is executed R(k) times (R a positive polynomial), then
the probability that any two of the generated primes are equal is negli-
gibly small; i.e., for every positive polynomial P there is a k0 ∈N, such
that for all k ≥k0
prob(ej1 = ej2 for some j1 ̸= j2 : ej ←GenPrime(1k), 1 ≤j ≤R(k))
≤
1
P(k).
Such algorithms exist. For example, an algorithm that randomly and uni-
formly chooses primes of binary length k satisﬁes the requirements. Namely,
the probability that ej1 = ej2 (j1 and j2 ﬁxed, j1 ̸= j2) is about k/2k by the
Prime Number Theorem (Theorem A.68), and there are
³
R(k)
2
´
< R(k)2/2
subsets {j1, j2} of {1, . . . , R(k)}. There are suitable implementations of
GenPrime which are much more eﬃcient than the uniform sampling algo-
rithm (see [CraSho2000]).
Let N ∈N, N > 1 be a constant. To set up a Cramer-Shoup signature
scheme, we choose two security parameters k and l, with k1/N < l+1 < k−1.
Then we choose a collision-resistant hash function h : {0, 1}∗−→{0, 1}l.
More precisely, by using H’s key generator, we randomly select a hash func-
tion from Hl, where H is a collision-resistant family of hash functions and Hl
is the subset of functions with security parameter l (without loss of generality,
we assume that the functions in Hl map to {0, 1}l). We proved in Section 10.2
that such collision-resistant families exist if the RSA assumption and, as a
consequence, the factoring assumption hold. The output of h is considered as
a number in {0, . . . , 2l −1}. All users of the scheme generate their signatures
by using the hash function h.8
Given k, l and h, each user Alice generates her public and secret key.
Key Generation.
1. Alice randomly chooses a modulus n
u←ISG,k, i.e., she randomly and
uniformly chooses Sophie Germain primes ˜p and ˜q of length k −1 and
sets n := pq, p := 2˜p + 1 and q := 2˜q + 1.
8 In practice we may use, for example, k = 512, l = 160 and h =SHA-1, which is
believed to be collision resistant.

276
10. Provably Secure Digital Signatures
2. She chooses g
u←QRn and x
u←QRn at random and generates an (l +1)-
bit prime ˜e := GenPrime(1l+1).
3. (n, g, x, ˜e) is the public key; (p, q) is the secret key.
Remark. Using Sophie Germain primes ensures that the order p−1
2 · q−1
2
= ˜p· ˜q
of QRn is a product of distinct primes. Thus it is a cyclic group; it is the
cyclic subgroup of order ˜p˜q of Z∗
n.
In the following, all computations are done in Z∗
n unless otherwise stated
and, as usual, we identify Zn = {0, . . . , n −1}.
Signing. It is possible to sign arbitrary messages m ∈{0, 1}∗. To sign m,
Alice generates an (l + 1)-bit prime e := GenPrime(1l+1) and randomly
chooses ˜y
u←QRn. She computes
˜x := ˜y˜e · g−h(m),
y :=
³
x · gh(˜x)´e−1
,
where e−1 is the inverse of e in Z∗
ϕ(n) (the powers are computed in Z∗
n, which
is of order ϕ(n)). The signature σ of m is (e, y, ˜y).
Remarks:
1. Taking the e−1-th power in the computation of y means computing the
e-th root in Z∗
n. Alice needs her secret key for this computation. Since
|e| = l + 1 < k −1 = |˜p| = |˜q|, the prime e does not divide ϕ(n) = 4˜p˜q.
Hence, Alice can easily compute the inverse e−1 of e in Z∗
ϕ(n), by using her
secret ˜p and ˜q (and the extended Euclidean algorithm, see Proposition
A.16).
2. Signing is a probabilistic algorithm, because the prime e is generated
probabilistically and a random quadratic residue ˜y is chosen. After these
choices, the computation of the signature is deterministic. Therefore, we
can describe the signature σ of m as the value of a mathematical function
sign:
σ = sign(h, n, g, x, ˜e, e, ˜y, m).
To compute the function sign by an algorithm, Alice has to use her knowl-
edge about the prime factors of n.
Veriﬁcation. Recipient Bob veriﬁes a signature σ = (e, y, ˜y) of Alice for
message m as follows:
1. First, he checks whether e is an odd (l+1)-bit number that is not divisible
by ˜e.
2. Then he computes
˜x := ˜y˜e · g−h(m)
and checks whether
x = ye · g−h(˜x).
He accepts if both checks are aﬃrmative; otherwise he rejects.

10.4 A State-Free Signature Scheme
277
Remarks:
1. Note that the veriﬁcation algorithm does not verify that e is a prime;
it only checks whether e is odd. A primality test would considerably
decrease the eﬃciency of veriﬁcation, and the security of the scheme
does not require it (as the security proof shows).
2. If Alice generates a signature (e, y, ˜y) with e = ˜e, then this signature is not
accepted by the veriﬁcation procedure. However, since both e and ˜e are
generated by GenPrime, this happens only with negligible probability,
and Alice could simply generate a new prime in this case.
Theorem 10.12. If the strong RSA assumption holds, “many” Sophie Ger-
main primes exist and H is collision resistant, then the Cramer-Shoup sig-
nature scheme is secure against adaptively-chosen-message attacks.
Remark. There is a variant of the signature scheme which does not require
the collision resistance of the hash function (see [CraSho2000]). The fam-
ily H is only assumed to be a universal one-way family of hash functions
([NaoYun89]; [BelRog97]). The universal one-way property is weaker than
full collision resistance: if an adversary Eve ﬁrst chooses a message m and
then a random key i is chosen, it should be infeasible for Eve to ﬁnd m′ ̸= m
with hi(m) = hi(m′). Note that the size of the key can grow with the length
of m.
In the proof of the theorem we need the following technical lemma.
Lemma 10.13. There is a deterministic polynomial algorithm that for all k,
given n ∈ISG,k, an odd natural number e with |e| < k −1, a number f and
elements u, v ∈Z∗
n with ue = vf as inputs, computes the r-th root vr−1 ∈Z∗
n
of v for r := e/d and d := gcd(e, f).
Proof. e and hence r and d are prime to ϕ(n), since ϕ(n) = 4˜p˜q, with Sophie
Germain primes ˜p and ˜q of binary length k −1, and e is an odd number
with |e| < k −1. Thus, the inverse elements r−1 of r and d−1 of d in Z∗
ϕ(n)
exist and can be computed by the extended Euclidean algorithm (Proposition
A.16). Let s := f/d. Since r is prime to s, the extended Euclidean algorithm
(Algorithm A.5) computes integers m and m′, with sm + rm′ = 1. We have
ur = (ue)d−1
=
¡
vf¢d−1
= vs
and
(um · vm′)r = (vs)m · (vm′)r = vsm+rm′ = v.
By setting
vr−1 = um · vm′
we obtain the r-th root of v.
2

278
10. Provably Secure Digital Signatures
Proof (of Theorem 10.12). The proof runs by contradiction.
Let Forger(h, n, g, x, ˜e) be a probabilistic polynomial forging algorithm which
adaptively requests the signatures for t messages, where t = R(k) for some
polynomial R, and then produces a valid forgery with non-negligible prob-
ability for inﬁnitely many security parameters (k, l). By non-negligible, we
mean that the probability is > 1/Q(k) for some positive polynomial Q.
We will deﬁne an attacking algorithm A that on inputs n ∈ISG and
z ∈Z∗
n successfully computes an r-th root modulo n of z, without knowing
the prime factors of n (contradicting the strong RSA assumption, by Lemma
10.11).
On inputs n ∈ISG,k and z ∈Z∗
n, A works as follows:
1. Randomly and uniformly select the second security parameter l and ran-
domly choose a hash function h ∈Hl (by using H’s key generator).
2. In a clever way, generate the missing elements g, x and ˜e of a public key
(n, g, x, ˜e).
3. Interact with Forger to obtain a forged signature (m, σ) for the public
key (n, g, x, ˜e).
Since the prime factors of n are not known in this setting, Forger can-
not get the signatures he requests from the original signing algorithm.
Instead, he obtains them from A. Since g, x and ˜e were chosen in a clever
way, A is able to supply Forger with valid signatures without knowing
the prime factors of n.
4. By use of the forged signature (m, σ), compute an r-th root modulo n of
z for some r > 1.
A simulates the legitimate signer in step 3. We therefore also say that Forger
runs against a simulated signer. Simulating the signer is the core of the proof.
To ensure that Forger yields a valid signature with a non-negligible proba-
bility, the probabilistic setting where Forger operates must be identical (or
at least very close) to the setting where Forger runs against the legitimate
signer. This means, in particular, that the keys generated in step 2 must be
distributed as are the keys in the original signature scheme, and the signatures
supplied to Forger in step 3 must be distributed as if they were generated by
the legitimate signer.
We denote by mi, 1 ≤i ≤t, the messages for which signatures are
requested by Forger, and by σi = (ei, yi, ˜yi) the corresponding signatures
supplied to Forger. Let (m, σ) be the output of Forger, i.e., m is a mes-
sage ̸= mi, 1 ≤i ≤t, and σ = (e, y, ˜y) is the forged signature of m. Let
˜xi := ˜y˜e
i · g−h(mi) and ˜x := ˜y˜e · g−h(m).
We distinguish three (overlapping) types of forgery:
1. Type 1. For some i, 1 ≤i ≤t, ei divides e and ˜x = ˜xi.
2. Type 2. For some i, 1 ≤i ≤t, ei divides e and ˜x ̸= ˜xi.
3. Type 3. For all i, 1 ≤i ≤t, ei does not divide e.

10.4 A State-Free Signature Scheme
279
Here, note that the number e in the forged signature can be non-prime (the
veriﬁcation procedure does not test whether e is a prime). The numbers ei
are primes (see below).
We may deﬁne a forging algorithm Forger 1 which yields the output of
Forger if it is of type 1, and otherwise returns some message and signature
not satisfying the veriﬁcation condition. Analogously, we deﬁne Forger 2 and
Forger 3. Then the valid forgeries of Forger 1 are of type 1, those of Forger 2
are of type 2 and those of Forger 3 of type 3. If Forger succeeds with non-
negligible probability, then at least one of the three “single-type forgers”
succeeds with non-negligible probability. Replacing Forger by this algorithm,
we may assume from now on that Forger generates valid forgeries of one type.
Case 1: Forger is of type 1. To generate the public key in step 2, A
proceeds as follows:
1. Generate (l + 1)-bit primes ei := GenPrime(1l+1), 1 ≤i ≤t. Set
g := z2 Q
1≤i≤t ei.
2. Randomly choose w
u←Z∗
n and set
x := w2 Q
1≤i≤t ei.
3. Set ˜e := GenPrime(1l+1).
To generate the signature for the i-th message mi, A randomly chooses
˜yi
u←QRn and computes
˜xi := ˜y˜e
i · g−h(mi) and yi :=
³
x · gh(˜xi)´e−1
i
.
Though A does not know the prime factors of n, she can easily compute
the ei-th root to get yi, because she knows the ei-th root of x and g by
construction.
Forger then outputs a forged signature σ = (e, y, ˜y) for a message m ̸∈
{m1, . . . , mt}. If the forged signature does not pass the veriﬁcation procedure,
then the forger did not produce a valid signature. In this case, A returns a
random exponent and a random element in Z∗
n, and stops. Otherwise, Forger
yields a valid type-1 forgery. Thus, for some j, 1 ≤j ≤t, we have ej | e,
˜x = ˜xj and
˜y˜e = ˜x · gh(m),
˜y˜e
j = ˜x · gh(mj).
Now A has to compute an r-th root of z. If h(m) ̸= h(mj), which happens
almost with certainty, since H is collision resistant, we get, by dividing the
two equations, the equation
˜z˜e = ga = z2a Q
1≤i≤t ei,

280
10. Provably Secure Digital Signatures
with 0 < a < 2l. Here recall that h yields l-bit numbers and these are < 2l.
Then a := h(m) −h(mj) if h(m) −h(mj) > 0; otherwise a := h(mj) −h(m).
Since ˜e is an (l +1)-bit prime and thus ≥2l, ˜e does not divide a. Moreover, ˜e
and the ei were chosen by GenPrime. Thus, with a high probability (stated
more precisely below), ˜e ̸= ei, 1 ≤i ≤t. In this case, A can compute z˜e−1
using Lemma 10.13 (note that ˜e is an (l + 1)-bit prime and l + 1 < k −1). A
returns the exponent ˜e and the ˜e-th root z˜e−1.
We still have to compute the probability of success of A. Interacting with
the legitimate signer, Forger is assumed to be successful. This means that
there exists a positive polynomial P, an inﬁnite subset K ⊆N and an l(k) for
each k ∈K, such that Forger produces a valid signature with a probability
≥1/P(k) for all the pairs (k, l(k)), k ∈K, of security parameters. To state
this more precisely, let Mi(h, n, g, x, ˜e, (mj, σj)1≤j<i) be the random variable
describing the choice of mi by Forger (1 ≤i ≤t). The signatures σi are
additional inputs to Forger. We have for k ∈K and l := l(k):
psuccess,Forger(k, l) = prob(V erify(h, n, g, x, ˜e, m, σ) = accept :
h ←Hl, n
u←ISG,k,
g
u←QRn, x
u←QRn, ˜e ←GenPrime(1l+1),
ei ←GenPrime(1l+1), ˜yi
u←QRn,
mi ←Mi(h, n, g, x, ˜e, (mj, σj)1≤j<i),
σi = sign(h, n, g, x, ˜e, ei, ˜yi, mi), 1 ≤i ≤t,
(m, σ) ←Forger(h, n, g, x, ˜e, (σi)1≤i≤t) )
≥
1
P(k).
Here recall that the signature σi of mi can be derived deterministically from
h, n, g, x, ˜e, ei, ˜yi and mi as the value of a mathematical function sign (see p.
276).
Let ψ(z, (ei)1≤i≤t) := g = z2 Q
1≤i≤t ei and χ(w, (ei)1≤i≤t) := x =
w2 Q
1≤i≤t ei be the speciﬁc g and x constructed by A. A(n, z) succeeds in com-
puting the root z˜e−1 if the Forger produces a valid signature, if h(m) ̸= h(mi)
and if ˜e ̸= ei, 1 ≤i ≤t. All other steps in A are deterministic. Therefore, we
may compute the probability of success of A (for security parameter k) as
follows:
prob(vr = z : n
u←ISG,k, z
u←Z∗
n, (v, r) ←A(n, z))
≥prob(V erify(h, n, ψ(z, (ei)i), χ(w, (ei)i), ˜e, m, σ) = accept,
h(m) ̸= h(mi), ˜e ̸= ei, 1 ≤i ≤t :
h ←Hl, n
u←ISG,k, z
u←Z∗
n, w
u←Z∗
n, ˜e ←GenPrime(1l+1),
ei ←GenPrime(1l+1), ˜yi
u←QRn,
mi ←Mi(h, n, ψ(z, (ei)i), χ(w, (ei)i), ˜e, (mj, σj)1≤j<i),

10.4 A State-Free Signature Scheme
281
σi = sign(h, n, ψ(z, (ei)i), χ(w, (ei)i), ˜e, ei, ˜yi, mi), 1 ≤i ≤t,
(m, σ) ←Forger(h, n, ψ(z, (ei)i), χ(w, (ei)i), ˜e, (σi)1≤i≤t) )
=: p1
Let Q be a positive polynomial. GenPrime, when called polynomially times,
generates the same prime more than once only with negligible probability, and
h is randomly chosen from a collision-resistant family H of hash functions.
Thus, there is some k0 such that both the probability that ˜e ̸= ei for 1 ≤i ≤t
and the probability that h(m) ̸= h(mi) for 1 ≤i ≤t are ≥1 −1/Q(k), for
k ≥k0.9 Hence, we get for k ≥k0 that
p1 ≥prob(V erify(h, n, ψ(z, (ei)i), χ(w, (ei)i, ˜e, m, σ) = accept :
h ←Hl, n
u←ISG,k, z
u←Z∗
n, w
u←Z∗
n, ˜e ←GenPrime(1l+1),
ei ←GenPrime(1l+1), ˜yi
u←QRn,
mi ←Mi(h, n, ψ(z, (ei)i), χ(w, (ei)i), ˜e, (mj, σj)1≤j<i),
σi = sign(h, n, ψ(z, (ei)i), χ(w, (ei)i), ˜e, ei, ˜yi, mi), 1 ≤i ≤t,
(m, σ) ←Forger(h, n, ψ(z, (ei)i), χ(w, (ei)i), ˜e, (σi)1≤i≤t) )
·
µ
1 −
1
Q(k)
¶
·
µ
1 −
1
Q(k)
¶
=: p2
The ﬁrst factor in p2 is the probability that Forger successfully yields a
valid signature when interacting with the simulated signer. This probability is
equal to Forger’s probability of success, psuccess,Forger(k, l), when he interacts
with the legitimate signer. Namely, ψ(z, (ei)i) and χ(w, (ei)i) are uniformly
distributed quadratic residues, independent of the distribution of the ei, since
z
u←Z∗
n and w
u←Z∗
n.10 Thus, we may replace ψ(z, (ei)i), z
u←Z∗
n and
χ(w, (ei)i), w
u←Z∗
n by g
u←QRn and x
u←QRn. We get
p2 = psuccess,Forger(k, l) ·
µ
1 −
1
Q(k)
¶2
.
For k ∈K and l = l(k), we have psuccess,Forger(k, l) ≥1/P(k). The probability
that A chooses l(k) in her ﬁrst step is ≥1/k, and we ﬁnally obtain that
prob(vr = z : n
u←ISG,k, z
u←Z∗
n, (v, r) ←A(n, z)) ≥
1
P(k)·1
k ·
µ
1 −
1
Q(k)
¶2
,
for the inﬁnitely many k ∈K. This contradicts the strong RSA assumption.
The proof of Theorem 10.12 is ﬁnished in the case where the forger is of
type 1. The other cases are proven below.
2
9 Note that the messages m and mi are generated by a probabilistic polynomial
algorithm, namely Forger.
10 Here note that Q
i ei and ˜e are prime to ϕ(n) = 4˜p˜q, because ˜e and the ei are
(l + 1)-bit primes and l + 1 < k −1 = |˜p| = |˜q|.

282
10. Provably Secure Digital Signatures
Remark. Before we study the next cases, let us have a look back at the proof
just ﬁnished. The core of the proof is to simulate the legitimate signer and
to supply valid signatures to the forger, without knowing the prime factors
of the modulus n. We managed to do this by a clever choice of the second
parts of the public keys. Here, the key point is that given a ﬁxed ﬁrst part
of the public key, i.e., given a modulus n, the joint distribution of the second
part (g, x, ˜e) of the public key and the generated signatures is the same in
the legitimate signer and in the simulation by A. This fact is often referred
to as “the simulated signer perfectly simulates the legitimate signer”.
Proof (of cases 2 and 3).
Case 2: Forger is of type 2. We may assume that the j with ej | e and
˜x ̸= ˜xj is ﬁxed. Namely, we may guess the correct j, i.e., we iterate over the
polynomially many cases j and assume j ﬁxed in each case (see Chapter 7,
proof of Theorem 7.7, for an analogous argument).
To generate the missing elements g, x and ˜e of the public key, A proceeds
as follows:
1. Generate (l + 1)–bit primes ei := GenPrime(1l+1), 1 ≤i ≤t. Choose a
further prime ˜e := GenPrime(1l+1). Set
g := z2˜e Q
i̸=j ei.
2. Randomly choose w
u←Z∗
n and u
u←Z∗
n. Set
yj := w2 Q
i̸=j ei and ˜xj := u2˜e.
3. Let x := yej
j · g−h(˜xj).
Then g and x are uniformly distributed quadratic residues, since z, w and u
are uniformly distributed (and the exponents ˜e and ei are prime to ϕ(n), see
the footnote on p. 281).
To generate the signature (ei, yi, ˜yi) for the i-th message mi, requested
by Forger, A proceeds as follows:
1. If i ̸= j, then A randomly chooses ˜yi
u←QRn and computes
˜xi := ˜y˜e
i · g−h(mi) and yi :=
³
x · gh(˜xi)´e−1
i
.
A can compute the ei-th root, because the ei-th roots of g and x are
known to her by construction.
2. If i = j, then the value of yj has already been computed above. Moreover,
A can compute the correct value of ˜yj = (˜xjgh(mj))˜e−1, because she
knows the ˜e-th root of ˜xj and g. Note that ˜yj is uniformly distributed,
as required, since ˜xj is uniformly distributed.

10.4 A State-Free Signature Scheme
283
It is obvious from the construction that A generates signatures which satisfy
the veriﬁcation condition. Forger outputs a forged signature σ = (e, y, ˜y)
for a message m ̸∈{m1, . . . , mt}. If the forged signature does not pass the
veriﬁcation procedure, A returns a random exponent and a random element
in Z∗
n, and stops. Otherwise, Forger yields a valid type-2 forgery, such that
ej divides e, i.e., e = ej · f and ˜y˜e · g−h(m) = ˜x ̸= ˜xj. We have
ye =
¡
yf¢ej = x · gh(˜x) and yj
ej = x · gh(˜xj).
Now A has to compute an r-th root of z. If h(˜x) ̸= h(˜xj), which happens
almost with certainty, since H is collision resistant, we get, by dividing the
two equations, the equation
˜zej = ga = z2a˜e Q
i̸=j ei,
with 0 < a < 2l. Since all (l + 1)-bit primes are chosen by GenPrime(1l+1),
the probability that ej is equal to ˜e or equal to an ei, i ̸= j, is negligibly
small. If ej is diﬀerent from ˜e and the ei, i ̸= j, then we can compute ze−1
j
by Lemma 10.13. In this case, A returns the exponent ej and the ej-th root
ze−1
j .
A(n, z) succeeds in computing the root ze−1
j
if the Forger produces a valid
signature, if h(˜x) ̸= h(˜xj) and if ej is diﬀerent from ˜e and the ei, i ̸= j. As in
case 1, it follows from the construction that A perfectly simulates the choice of
the key together with the generation of signatures supplied to Forger. Thus,
as we have seen in case 1, the Forger’s probability of success if he interacts
with the simulated signer is the same as if he interacted with the legitimate
signer. Computing the probabilities in a completely analogous way as in case
1, we derive that
prob(vr = z : n
u←ISG,k, z
u←Z∗
n, (v, r) ←A(n, z)) ≥
1
S(k),
for some positive polynomial S and inﬁnitely many k. This contradicts the
strong RSA assumption (Lemma 10.11).
Case 3: Forger is of type 3. To complete the public key by g, x and ˜e, A
proceeds as follows:
1. Generate (l + 1)-bit primes ˜e and ei, 1 ≤i ≤t, by applying
GenPrime(1l+1). Set
g := z2˜e Q
i ei.
2. Choose a
u←{1, . . . , n2} and set x := ga.
A can easily generate valid signatures (ei, yi, ˜yi) for messages mi, 1 ≤i ≤t,
requested by Forger. Namely, A chooses ˜yi
u←QRn and computes ˜xi =
˜yei
i · g−h(mi) and yi = (x · gh(˜xi))e−1
i . The latter computation works, because

284
10. Provably Secure Digital Signatures
due to the construction, the ei-th roots of g and x can be immediately derived
for 1 ≤i ≤t.
Then Forger outputs a forged signature σ = (e, y, ˜y) for a message m ̸∈
{m1, . . . , mt}. The signature is of type 3, i.e., ei does not divide e, 1 ≤i ≤t.
If the signature is valid, we get the equation
ye = x · gh(˜x) = zf,
where f = 2˜e
Y
i
ei · (a + h(˜x)),
with ˜x = ˜y˜e · g−h(m).
To compute a root of z, let d = gcd(e, f). Observe that e may be a non-
prime, because the veriﬁcation algorithm only tests whether e is odd. If d < e,
i.e., e does not divide f, then r := e/d > 1, and we can compute the r-th root
zr−1 by Lemma 10.13.
Thus, A succeeds in computing a root if Forger yields a valid forgery and
if e does not divide f. To compute the probability that Forger yields a valid
forgery, we have to consider the distribution of the keys (g, x), generated by
A in step 2.
By the deﬁnition of n, QRn is a cyclic group of order ˜p˜q with distinct
Sophie Germain primes ˜p and ˜q.
Note that ˜e and the ei are (l + 1)-bit primes, and l + 1 < k −1. Thus, ˜e
and none of the ei are equal to ˜p or ˜q, which are (k −1)-bit primes. Hence,
˜e Q
i ei is prime to ϕ(n) = 4˜p˜q. We conclude that g is uniformly distributed
in QRn, since z is uniformly chosen from Z∗
n.
Let a = b˜p˜q + c, 0 ≤c < ˜p˜q (division with remainder). Now 2k−2 <
˜p, ˜q < 2k−1, n2 ≈24k, and a
u←{1, . . . , n2} is uniformly chosen. This implies
that the probability of a remainder c, 0 ≤c < ˜p˜q, diﬀers from 1/˜p˜q by at most
1/n2 ≈1/24k. This means that the distribution of c is polynomially close to the
uniform distribution. This in turn implies that the conditional distribution
of x, assuming that g is a generator of QRn, is polynomially close to the
uniform distribution on QRn. However, QRn ∼= Z˜p˜q ∼= Z˜p × Z˜q and therefore
(˜p−1)(˜q −1) of the ˜p˜q elements in QRn are generators. Thus, the probability
that g is a generator of QRn is ≥1 −1/2k−3, which is exponentially close to
1. Summarizing, we get that the distribution of x is polynomially close to the
uniform distribution on QRn.
We see that A almost perfectly simulates the legitimate signer. The dis-
tributions of the keys and the signatures supplied to Forger are polynomially
close to the distributions when Forger interacts with the legitimate signer.
By Lemmas B.21 and B.24, we conclude that the probability that Forger
produces a valid signature, if he interacts with the simulated signer in A,
cannot be polynomially distinguished from his probability of success when
interacting with the legitimate signer. Thus, Forger produces in step 3 of A
a valid signature with probability ≥1/Q(k) for some positive polynomial Q
and inﬁnitely many k.

Exercises
285
We still have to study the conditional probability that e does not divide
f, assuming that Forger produces a valid signature. It is suﬃcient to prove
that this probability is non-negligible. We will show that it is > 1/2. If we
could prove this estimate assuming h, n, g, x, ˜e and the forged signed message
(m, σ) ﬁxed, for every h ∈Hl, n ∈ISG,k, every g, x and ˜e possibly generated
by A, and every valid (m, σ) possibly output by Forger, then we are done
(take the sum over all h, n, g, x, ˜e, m and σ). Therefore, we now assume that
h, n, g, x and ˜e, and m and σ are ﬁxed. This implies that c and ˜x are also
ﬁxed.
Let s be a prime dividing e. Then s > 2 and s ̸= ˜e, because otherwise the
veriﬁcation condition was not satisﬁed. Moreover, s ̸= ei, since the forgery
is of type 3. Thus, it suﬃces to prove that s does not divide a + h(˜x) with
probability ≥1/2, assuming h, n, g, x, ˜e, m and σ ﬁxed. Let a = b˜p˜q + c, as
above. a + h(˜x) = b˜p˜q + c + h(˜x) = L(b), with L a linear function (note that
c and ˜x are ﬁxed). The probability that s divides a + h(˜x) is the same as
the probability that L(b) ≡0 mod s. Now the conditional distribution of b,
assuming c ﬁxed, is also polynomially close to the uniform distribution on
{0, . . . , ⌊n2/˜p˜q⌋}. Thus, the distribution of b mod s is polynomially close to
the uniform distribution. s does not divide ˜p˜q, because |s| ≤l + 1 < k −1
and |˜p| = |˜q| = k −1. Thus, L(b) ≡
0 mod s is a non-vanishing linear
equation over Zs. Hence, the probability that L(b) = 0 mod s is very close
to 1/s. This means that s and hence e do not divide f, with a probability
≥1 −1/(s −1) ≥1/2 (recall that s > 2).
Now the proof of case 3 is ﬁnished, and the proof of Theorem 10.12 is
complete.
2
Exercises
1. Consider the construction of collision-resistant hash functions in Section
10.2. Explain how the preﬁx-free encoding of the messages can be avoided
by applying Merkle’s meta method (Section 3.4.2).
2. Let I = (Ik)k∈N be a key set with security parameter k, and let
H =
¡
hi : {0, 1}∗−→{0, 1}g(k(i))¢
i∈I be a family of collision-resistant
hash functions. Let li ≥g(k(i)) + k(i) for all i ∈I, and let {0, 1}≤li :=
{m ∈{0, 1}∗| 1 ≤|m| ≤li} be the bit strings of length ≤li.
Show that the family
¡
hi : {0, 1}≤li −→{0, 1}g(k(i))¢
i∈I
is a family of one-way functions (with respect to H’s key generator).
3. The RSA and ElGamal signature schemes are introduced in Chapter 3.
There, various attacks against the basic schemes (no hash function is
applied to the messages) are discussed. Classify these attacks and their
levels of success (according to Section 10.1).

286
10. Provably Secure Digital Signatures
4. The following signature scheme was suggested by Ong, Schnorr and
Shamir ([OngSchSha84]). Alice chooses two random large distinct primes
p and q, and a random x ∈Z∗
n, where n := pq. Then she computes
y := −x−2 ∈Z∗
n and publishes (n, y) as her public key. Her secret is x
(she does not need to know the prime factors of n). To sign a message
m ∈Zn, she randomly selects an r ∈Z∗
n and calculates (in Zn)
s1 := 2−1 ¡
r−1m + r
¢
and s2 := 2−1x
¡
r−1m −r
¢
.
(s1, s2) is the signature of m. To verify a signature (s1, s2), Bob checks
that m = s2
1 + ys2
2 (in Zn):
a. Prove that retrieving the secret key by a key-only attack is equivalent
to the factoring of n.
b. The scheme is existentially forgeable by a key-only attack.
c. The probability that a randomly chosen pair (s1, s2) is a signature
for a given message m is negligibly small.
d. State the problem, which an adversary has to solve, of forging signa-
tures for messages of his choice by a key-only attack.
(In fact, Pollard has broken the scheme by an eﬃcient algorithm for
this problem, see [PolSch87].)
5. Let I = (Ik)k∈N be a key set with security parameter k, and let
f0 = (f0,i : Di −→Di)i∈I, f1 = (f1,i : Di −→Di)i∈I be a claw-free pair
of trapdoor permutations with key generator K. We consider the fol-
lowing signature scheme. As her public key, Alice randomly chooses an
index i ∈Ik – by computing K(1k) – and a reference value x
u←Di.
Her private key is the trapdoor information of f0,i and f1,i. We encode
the messages m ∈{0, 1}∗in a preﬁx-free way (see Section 10.2), and
denote the encoded m by [m]. Then Alice’s signature of a message m
is σ(i, x, m) := f −1
[m],i(x), where f[m],i is deﬁned as in Section 10.2. Bob
can verify Alice’s signature σ by comparing f[m],i(σ) with x. Study the
security of the scheme. More precisely, show
a. A claw of f0 and f1 can be computed from σ(i, x, m) and σ(i, x, m′)
if the messages m and m′ are distinct.
b. The scheme is secure against existential forgery by a key-only attack.
c. Assume that the message space has polynomial cardinality. More pre-
cisely, let c ∈N and assume that only messages m ∈{0, 1}c⌊log2(k)⌋
are signed by the scheme. Then the scheme is secure against
adaptively-chosen-message attacks if used as a one-time signature
scheme (i.e., Alice signs at most one message with her key).
d. Which problem do you face in the security proof in c if the scheme
is used to sign arbitrary messages in {0, 1}∗?
6. We consider the same setting as in Exercise 5. We assume that the gener-
ation of the messages m ∈{0, 1}∗to be signed can be uniformly modeled
for all users by a probabilistic polynomial algorithm M(i). In particular,

Exercises
287
this means that the messages Alice wants to sign do not depend on her
reference value x.
Let (i, x) be the public key of Alice, and let mj be the j-th mes-
sage to be signed by Alice. The signature σ(i, x, mj) of mj is deﬁned
as σ(i, x, mj) := (sj, [m1]|| . . . ||[mj−1]), with sj := f −1
[mj],i(sj−1). Here
[m1]|| . . . ||[mj−1] denotes the concatenation of the preﬁx-free encoded
messages, and s0 := x is Alice’s randomly chosen reference value (see
[GolBel01]):
a. Show by an example that in order to prevent forging by known sig-
nature attacks, the veriﬁcation procedure has to check whether the
bit string ˆm in a signature (s, ˆm) is well formed with respect to the
preﬁx-free encoding.
Give the complete veriﬁcation condition.
b. Prove that no one can existentially forge a signature by a known-
signature attack.

A. Algebra and Number Theory
Public-key cryptosystems are based on modular arithmetic. In this section, we
summarize the concepts and results from algebra and number theory which
are necessary for an understanding of the cryptographic methods. Textbooks
on number theory and modular arithmetic include [HarWri79], [IreRos82],
[Rose94], [Forster96] and [Rosen2000]. This section is also intended to es-
tablish notation. We assume that the reader is familiar with the elementary
notions of algebra, such as groups, rings and ﬁelds.
A.1 The Integers
Z denotes the ring of integers; N = {z ∈Z | z > 0} denotes the subset of
natural numbers.
We ﬁrst introduce the notion of divisors and the fundamental Euclidean
algorithm which computes the greatest common divisor of two numbers.
Deﬁnition A.1. Let a, b ∈Z:
1. a divides b if there is some c ∈Z, with b = ac. We write a | b for “a
divides b”.
2. d ∈N is called the greatest common divisor of a and b, if:
a. d | a and d | b.
b. If t ∈Z divides both a and b, then t divides d.
The greatest common divisor is denoted by gcd(a, b).
3. If gcd(a, b) = 1, then a is called relatively prime to b, or prime to b for
short.
Theorem A.2 (Division with remainder). Let z, a ∈Z, a ̸= 0. Then there
are unique numbers q, r ∈Z, such that z = q · a + r and 0 ≤r < |a|.
Proof. In the ﬁrst step, we prove that such q and r exist. If a > 0 and z ≥0,
we may apply induction on z. For 0 ≤z < a we obviously have z = 0 · a + z.
If z ≥a, then, by induction, z −a = q · a + r for some q and r, 0 ≤r < a,
and hence z = (q + 1) · a + r. If z < 0 and a > 0, then we have just shown
the existence of an equation −z = q · a + r, 0 ≤r < a. Then z = −q · a if
r = 0, and z = −q · a −r = −q · a −a + (a −r) = −(q + 1) · a + (a −r) and

290
A. Algebra and Number Theory
0 < a −r < a. If a < 0, then −a > 0. Hence z = q · (−a) + r = −q · a + r,
with 0 ≤r < |a|.
To prove uniqueness, consider z = q1 · a + r1 = q2 · a + r2. Then 0 =
(q1 −q2) · a + (r1 −r2). Hence a divides (r1 −r2). Since |r1 −r2| < |a|, this
implies r1 = r2, and then also q1 = q2.
2
Remark. r is called the remainder of z modulo a. We write z mod a for r.
The number q is the (integer) quotient of z and a. We write z div a for q.
The Euclidean Algorithm. Let a, b ∈Z, a > b > 0. The greatest common
divisor gcd(a, b) can be computed by an iterated division with remainder. Let
r0 := a, r1 := b and
r0
= q1r1 + r2,
0 < r2 < r1,
r1
= q2r2 + r3,
0 < r3 < r2,
...
rk−1 = qkrk + rk+1,
0 < rk+1 < rk,
...
rn−2 = qn−1rn−1 + rn, 0 < rn < rn−1,
rn−1 = qnrn + rn+1,
0 = rn+1.
By construction, r1 > r2 > . . .. Therefore, the remainder becomes 0 after
a ﬁnite number of steps. The last remainder ̸= 0 is the greatest common
divisor, as is shown in the next proposition.
Proposition A.3.
1. rn = gcd(a, b).
2. There are numbers d, e ∈Z with gcd(a, b) = da + eb.
Proof. 1. From the equations considered in reverse order, we conclude that
rn divides rk, k = n−1, n−2 . . .. In particular, rn divides r1 = b and r0 = a.
Now let t be a divisor of a = r0 and b = r1. Then t | rk, k = 2, 3, . . . , and
hence t | rn. Thus, rn is the greatest common divisor.
2. Iteratively substituting rk+1 by rk−1 −qkrk, we get
rn = rn−2 −qn−1 · rn−1
= rn−2 −qn−1 · (rn−3 −qn−2 · rn−2)
= (1 + qn−1qn−2) · rn−2 −qn−1 · rn−3
...
= da + eb,
with integers d and e.
2
We have shown that the following algorithm, called Euclid’s algorithm,
outputs the greatest common divisor. abs(a) denotes the absolute value of a.

A.1 The Integers
291
Algorithm A.4.
int gcd(int a, b)
1
while b ̸= 0 do
2
r ←a mod b
3
a ←b
4
b ←r
5
return abs(a)
We now extend the algorithm, such that not only gcd(a, b) but also the
coeﬃcients d and e of the linear combination gcd(a, b) = da+eb are computed.
For this purpose, we write the recursion
rk−1 = qkrk + rk+1
using matrices
µ rk
rk+1
¶
= Qk
µrk−1
rk
¶
, where Qk =
Ã
0
1
1 −qk
!
, k = 1, . . . , n.
Multiplying the matrices, we get
µ rn
rn+1
¶
= Qn · Qn−1 · . . . · Q1
µr0
r1
¶
.
The ﬁrst component of this equation yields the desired linear combination
for rn = gcd(a, b). Therefore, we have to compute Qn · Qn−1 · . . . · Q1. This is
accomplished by iteratively computing the matrices
Λ0 =
Ã
1 0
0 1
!
,
Λk =
Ã
0
1
1 −qk
!
Λk−1,
k = 1, . . . , n,
to ﬁnally get Λn = Qn · Qn−1 · . . . · Q1.
In this way, we have derived the
following algorithm, called the extended Euclidean algorithm. On inputs a
and b it outputs the greatest common divisor and the coeﬃcients d and e of
the linear combination gcd(a, b) = da + eb.

292
A. Algebra and Number Theory
Algorithm A.5.
int array gcdCoef (int a, b)
1
λ11 ←1, λ22 ←1, λ12 ←0, λ21 ←0
2
while b ̸= 0 do
3
q ←a div b
4
r ←a mod b
5
a ←b
6
b ←r
7
t21 ←λ21; t22 ←λ22
8
λ21 ←λ11 −q · λ21
9
λ22 ←λ12 −q · λ22
10
λ11 ←t21
11
λ12 ←t22
12
return (abs(a), λ11, λ12)
We analyze the running time of the Euclidean algorithm. Here we meet
the Fibonacci numbers.
Deﬁnition A.6. The Fibonacci numbers fn are recursively deﬁned by
f0 := 0,
f1 := 1,
fn := fn−1 + fn−2, for n ≥2.
Remark. The Fibonacci numbers can be non-recursively computed using the
formula
fn =
1
√
5(gn −˜gn),
where g and ˜g are the solutions of the equation x2 = x + 1:
g := 1
2
³
1 +
√
5
´
and ˜g := 1 −g = −1
g = 1
2
³
1 −
√
5
´
.
See, for example, [Forster96].
Deﬁnition A.7. g is called the Golden Ratio.1
Lemma A.8. For n ≥2, fn ≥gn−2. In particular, the Fibonacci numbers
grow exponentially fast.
Proof. The statement is clear for n = 2. By induction on n, assuming that
the statement holds for ≤n, we get
fn+1 = fn + fn−1 ≥gn−2 + gn−3 = gn−3(1 + g) = gn−3g2 = gn−1.
2
Proposition A.9. Let a, b ∈Z, a > b > 0. Assume that computing gcd(a, b)
by the Euclidean algorithm takes n iterations (i.e., using n divisions with
remainder). Then a ≥fn+1 and b ≥fn.
1 It is the proportion of length to width which the Greeks found most beautiful.

A.1 The Integers
293
Proof. Let r0 := a, r1 := b and consider
r0
= q1r1 + r2,
r1
= q2r2 + r3,
...
rn−2 = qn−1rn−1 + rn,
rn−1 = qnrn,
and
fn+1 = fn + fn−1,
fn
= fn−1 + fn−2,
...
f3
= f2 + f1,
f2
= f1.
By induction, starting with i = n and descending, we show that ri ≥fn+1−i.
For i = n, we have rn ≥f1 = 1. Now assume the inequality proven for ≥i.
Then
ri−1 = qiri + ri+1 ≥ri + ri+1 ≥fn+1−i + fn+1−(i+1) = fn+1−(i−1).
Hence a = r0 ≥fn+1 and b = r1 ≥fn.
2
Notation. As is common use, we denote by ⌊x⌋the greatest integer less
than or equal to x (the “ﬂoor” of x), and by ⌈x⌉the smallest integer greater
than or equal to x (the “ceiling” of x).
Corollary A.10. Let a, b ∈Z. Then the Euclidean algorithm computes
gcd(a, b) in at most ⌊logg(a)⌋+ 1 iterations.
Proof. Let n be the number of iterations. From a ≥fn+1 ≥gn−1 (Lemma
A.8) we conclude n −1 ≤⌊logg(a)⌋.
2
The Binary Encoding of Numbers. Studying algorithms with numbers
as inputs and outputs, we need binary encodings of numbers (and residues,
see below). We always assume that integers n ≥0 are encoded in the standard
way as unsigned integers:
The sequence zk−1zk−2 . . . z1z0 of bits zi ∈{0, 1}, 0 ≤i ≤k −1, is the
encoding of
n = z0 + z1 · 21 + . . . + zk−2 · 2k−2 + zk−1 · 2k−1 =
k−1
X
i=0
zi · 2i.
If the leading digit zk−1 is not zero (i.e., zk−1 = 1), we call n a k-bit integer,
and k is called the binary length of n. The binary length of n ∈N is usually
denoted by |n|. Of course, we only use this notation if it cannot be confused
with the absolute value. The binary length of n ∈N is ⌊log2(n)⌋+ 1. The
numbers of binary length k are the numbers n ∈N with 2k−1 ≤n ≤2k −1.
The Big-O Notation. To state estimates, the big-O notation is useful.
Suppose f(k) and g(k) are functions of the positive integers k which take
positive (not necessarily integer) values. We say that f(k) = O(g(k)) if there
is a constant C such that f(k) ≤C · g(k) for all suﬃciently large k. For

294
A. Algebra and Number Theory
example, 2k2 + k + 1 = O(k2) because 2k2 + k + 1 ≤4k2 for all k ≥1. In our
examples, the constant C is always “small”, and we use the big-O notation
for convenience. We do not want to state a precise value of C.
Remark. Applying the classical grade school methods, we see that adding and
subtracting two k-bit numbers requires O(k) binary operations. Multiplica-
tion and division with remainder can be done with O(k2) binary operations
(see [Knuth98] for a more detailed discussion of time estimates for doing
arithmetic). Thus, the greatest common divisor of two k-bit numbers can be
computed by the Euclidean algorithm with O(k3) binary operations.
Next we will show that every natural number can be uniquely decomposed
into prime numbers.
Deﬁnition A.11. Let p ∈N, p ≥2. p is called a prime (or a prime number)
if 1 and p are the only positive divisors of p. A number n ∈N which is not a
prime is called a composite.
Remark. If p is a prime and p | ab, a, b ∈Z, then either p | a or p | b.
Proof. Assume that p does not divide a and does not divide b. Then there
are d1, d2, e1, e2 ∈Z, with 1 = d1p + e1a, 1 = d2p + e2b (Proposition A.3).
Then 1 = d1d2p2 + d1e2bp + e1ad2p + e1e2ab. If p divided ab, then p would
divide 1, which is impossible. Thus, p does not divide ab.
2
Theorem A.12 (Fundamental Theorem of Arithmetic). Let n ∈N, n ≥2.
There are pairwise distinct primes p1, . . . , pr and exponents e1, . . . , er ∈
N, ei ≥1, i = 1, . . . , r, such that
n =
r
Y
i=1
pei
i .
The primes p1, . . . , pr and exponents e1, . . . , er are unique.
Proof. By induction on n we obtain the existence of such a decomposition.
n = 2 is a prime. Now assume that the existence is proven for numbers ≤n.
Either n + 1 is a prime or n + 1 = l · m, with l, m < n + 1. By assumption,
there are decompositions of l and m and hence also for n + 1.
In order to prove uniqueness, we assume that there are two diﬀerent decom-
positions of n. Dividing both decompositions by all common primes, we get
(not necessarily distinct) primes p1, . . . , ps and q1, . . . , qt, with {p1, . . . , ps} ∩
{q1, . . . , qt} = ∅and p1 · . . . · ps = q1 · . . . · qt. Since p1 | q1 · . . . · qt, we conclude
from the preceding remark that there is an i, 1 ≤i ≤t, with p1 | qi. This is a
contradiction.
2

A.2 Residues
295
A.2 Residues
In public-key cryptography, we usually have to compute with remainders
modulo n. This means that the computations take place in the residue class
ring Zn.
Deﬁnition A.13. Let n ∈N, n ≥2:
1. a, b ∈Z are congruent modulo n, written as
a ≡b mod n,
if n divides a−b. This means that a and b have the same remainder when
divided by n: a mod n = b mod n.
2. Let a ∈Z. [a] := {x ∈Z | x ≡a mod n} is called the residue class of a
modulo m.
3. Zn := {[a] | a ∈Z} is the set of residue classes modulo n.
Remark. As is easily seen, “congruent modulo n” is a symmetric, reﬂexive
and transitive relation, i.e., it is an equivalence relation. The residue classes
are the equivalence classes. A residue class [a] is completely determined by
one of its members. If a′ ∈[a], then [a] = [a′]. An element x ∈[a] is called
a representative of [a]. Division with remainder by n yields the remainders
0, . . . , n −1. Therefore, there are n residue classes in Zn:
Zn = {[0], . . . , [n −1]}.
The integers 0, . . . , n −1 are called the natural representatives. The natural
representative of [x] ∈Zn is just the remainder (x mod n) of x modulo n (see
division with remainder, Theorem A.2). If, in the given context, no confu-
sion is possible, we sometimes identify the residue classes with their natural
representatives.
Since we will study algorithms whose inputs and outputs are residue
classes, we need binary encodings of the residue classes. The binary encoding
of [x] ∈Zn is the binary encoding of the natural representative x mod n as
an unsigned integer (see our remark on the binary encoding of non-negative
integers in Section A.1).
Deﬁnition A.14. By deﬁning addition and multiplication as
[a] + [b] = [a + b] and [a] · [b] = [a · b],
Zn becomes a commutative ring, with unit element [1]. It is called the residue
class ring modulo n.
Remark. The sum [a] + [b] and the product [a] · [b] do not depend on the
choice of the representatives by which they are computed, as straightforward
computations show. For example, let a′ ∈[a] and b′ ∈[b]. Then n | a′ −a and
n | b′ −b. Hence n | a′ + b′ −(a + b), and therefore [a + b] = [a′ + b′].

296
A. Algebra and Number Theory
Doing multiplications in a ring, we are interested in those elements which
have a multiplicative inverse. They are called the units.
Deﬁnition A.15. Let R be a commutative ring with unit element e. An
element x ∈R is called a unit if there is an element y ∈R with x · y = e. We
call y a multiplicative inverse of x. The subset of units is denoted by R∗.
Remark. The multiplicative inverse of a unit x is uniquely determined, and
we denote it by x−1. The set of units R∗is a subgroup of R with respect to
multiplication.
Example. In Z, elements a and b satisfy a · b = 1 if and only if both a and b
are equal to 1, or both are equal to −1. Thus, 1 and −1 are the only units
in Z. The residue class rings Zn contain many more units, as the subsequent
considerations show. For example, if p is a prime then every residue class in
Zp diﬀerent from [0] is a unit. An element [x] ∈Zn in a residue class ring is
a unit if there is a residue class [y] ∈Zn with [x] · [y] = [1], i.e., n divides
x · y −1.
Proposition A.16. An element [x] ∈Zn is a unit if and only if gcd(x, n) =
1. The multiplicative inverse [x]−1 of a unit [x] can be computed using the
extended Euclidean algorithm.
Proof. If gcd(x, n) = 1, then there is an equation xb + nc = 1 in Z, and the
coeﬃcients b, c ∈Z can be computed using the extended Euclidean algorithm
A.5. The residue class [b] is an inverse of [x]. Conversely, if [x] is a unit, then
there are y, k ∈Z with x · y = 1 + k · n. This implies gcd(x, n) = 1.
2
Corollary A.17. Let p be a prime. Then every [x] ̸= [0] in Zp is a unit.
Thus, Zp is a ﬁeld.
Deﬁnition A.18. The subgroup
Z∗
n := {x ∈Zn | x is a unit in Zn}
of units in Zn is called the prime residue class group modulo n.
Deﬁnition A.19. Let M be a ﬁnite set. The number of elements in M is
called the cardinality or order of M. It is denoted by |M|.
We introduce the Euler phi function, which gives the number of units modulo
n.
Deﬁnition A.20.
ϕ : N −→N, n 7−→|Z∗
n|
is called the Euler phi function or the Euler totient function.
Proposition A.21 (Euler).
X
d | n
ϕ(d) = n.

A.2 Residues
297
Proof. If d is a divisor of n, let Zd := {x | 1 ≤x ≤n, gcd(x, n) = d}.
Each k ∈{1, . . . , n} belongs to exactly one Zd. Thus n = P
d | n |Zd|. Since
x 7→x/d is a bijective map from Zd to Z∗
n/d, we have |Zd| = ϕ(n/d), and hence
n = P
d | n ϕ(n/d) = P
d | n ϕ(d).
2
Corollary A.22. Let p be a prime and k ∈N. Then ϕ(pk) = pk−1(p −1).
Proof. By Euler’s result, ϕ(1) + ϕ(p) + . . . + ϕ(pk) = pk and ϕ(1) + ϕ(p) +
. . .+ϕ(pk−1) = pk−1. Subtracting both equations yields ϕ(pk) = pk −pk−1 =
pk−1(p −1).
2
Remarks:
1. By using the Chinese Remainder Theorem below (Section A.3), we will
also get a formula for ϕ(n) if n is not a power of a prime (Corollary A.30).
2. At some points in the book we need a lower bound for the fraction ϕ(n)/n
of units in Zn. In [RosSch62] it is proven that
ϕ(n) >
n
eγ log(log(n)) +
2.6
log(log(n))
, with Euler’s constant γ = 0.5772 . . . .
This inequality implies, for example, that
ϕ(n) >
n
6 log(log(n)) for n ≥1.3 · 106,
as a straightforward computation shows.
The RSA cryptosystem is based on old results by Fermat and Euler.2
These results are special cases of the following proposition.
Proposition A.23. Let G be a ﬁnite group and e be the unit element of G.
Then x|G| = e for all x ∈G.
Proof. Since we apply this result only to Abelian groups, we assume in our
proof that the group G is Abelian. A proof for the general case may be found
in most introductory textbooks on algebra.
The map µx : G −→G, g 7−→xg, multiplying group elements by x, is a
bijective map (multiplying by x−1 is the inverse map). Hence,
Y
g∈G
g =
Y
g∈G
xg = x|G| Y
g∈G
g,
and this implies x|G| = e.
2
As a ﬁrst corollary of Proposition A.23, we get Fermat’s Little Theorem.
Proposition A.24 (Fermat). Let p be a prime and a ∈Z be a number that
is prime to p (i.e., p does not divide a). Then
ap−1 ≡1 mod p.
2 Pierre de Fermat (1601–1665) and Leonhard Euler (1707–1783).

298
A. Algebra and Number Theory
Proof. The residue class [a] of a modulo p is a unit, because a is prime to p
(Proposition A.16). Since |Z∗
p| = p −1 (Corollary A.17), we have [a]p−1 = 1
by Proposition A.23.
2
Remark. Fermat stated a famous conjecture known as Fermat’s Last Theo-
rem. It says that the equation xn + yn = zn has no solutions with non-zero
integers x, y and z, for n ≥3. For more than 300 years, Fermat’s conjecture
was one of the outstanding challenges of mathematics. It was ﬁnally proven
in 1995 by Andrew Wiles.
Euler generalized Fermat’s Little Theorem.
Proposition A.25 (Euler). Let n ∈N and let a ∈Z be a number that is
prime to n. Then
aϕ(n) ≡1 mod n.
Proof. It follows from Proposition A.23, in the same way as Proposition A.24.
The residue class [a] of a modulo n is a unit and |Z∗
n| = ϕ(n).
2
Fast Modular Exponentiation. In cryptography, we often have to com-
pute a power xe or a modular power xe mod n. This can be done eﬃciently
by the fast exponentiation algorithm. The idea is that if the exponent e is a
power of 2, say e = 2k, then we can exponentiate by successively squaring:
xe = x2k = ((((. . . (x2)2)2 . . .)2)2)2.
In this way we compute xe by k squarings. For example, x16 = (((x2)2)2)2.
If the exponent is not a power of 2, then we use its binary representation.
Assume that e is a k-bit number, 2k−1 ≤e < 2k. Then
e = 2k−1ek−1 + 2k−2ek−2 + . . . + 21e1 + 20e0,
(with ek−1 = 1)
= (2k−2ek−1 + 2k−3ek−2 + . . . + e1) · 2 + e0
= (. . . ((2ek−1 + ek−2) · 2 + ek−3) · 2 + . . . + e1) · 2 + e0.
Hence,
xe = x(...((2ek−1+ek−2)·2+ek−3)·2+...+e1)·2+e0 =
= (x(...((2ek−1+ek−2))·2+ek−3)·2+...+e1)2 · xe0 =
= (. . . (((x2 · xek−2)2 · xek−3)2 · . . .)2 · xe1)2 · xe0.
We see that xe can be computed in k −1 steps, with each step consisting of
squaring the intermediate result and, if the corresponding binary digit ei of
e is 1, an additional multiplication by x. If we want to compute the modular
power xe mod n, then we take the remainder modulo n after each squaring
and multiplication:
xe mod n =
(. . . (((x2 · xek−2 mod n)2 · xek−3 mod n)2 · . . .)2 · xe1 mod n)2 · xe0 mod n.
We obtain the following algorithm for fast modular exponentiation.

A.3 The Chinese Remainder Theorem
299
Algorithm A.26.
int ModPower(int x, e, n)
1
y ←x;
2
for i ←BitLength(e) −2 downto 0 do
3
y ←y2 · xBit(e,i) mod n
4
return y
In particular, we get
Proposition A.27. Let l = ⌊log2 e⌋. The computation of xe mod n can be
done by l squarings, l multiplications and l divisions.
Proof. The binary length k of e is ⌊log2(e)⌋+ 1.
2
A.3 The Chinese Remainder Theorem
The Chinese Remainder Theorem provides a method of solving systems of
congruences. The solutions can be found using an easy and eﬃcient algorithm.
Theorem A.28. Let n1, . . . , nr ∈N be pairwise relatively prime numbers,
i.e., gcd(ni, nj) = 1 for i ̸= j. Let b1, b2, . . . , br be arbitrary integers. Then
there is an integer b such that
b ≡bi mod ni,
i = 1, . . . , r.
Furthermore, the remainder b mod n is unique, where n = n1 · . . . · nr.
The statement means that there is a one-to-one correspondence between
the residue classes modulo n and tuples of residue classes modulo n1, . . . , nr.
This one-to-one correspondence preserves the additive and multiplicative
structure. Therefore, we have the following ring-theoretic formulation of The-
orem A.28.
Theorem A.29 (Chinese Remainder Theorem). Let n1, . . . , nr ∈N be pair-
wise relatively prime numbers, i.e., gcd(ni, nj) = 1, for i ̸= j. Let n =
n1 · . . . · nr. Then the map
ψ : Zn −→Zn1 × . . . × Znr, [x] 7−→([x mod n1], . . . , [x mod nr])
is an isomorphism of rings.
Remark. Before we give a proof, we review the notion of an “isomorphism”.
It means that ψ is a homomorphism and bijective. “Homomorphism” means
that ψ preserves the additive and multiplicative structure. More precisely, a
map f : R −→R′ between rings with unit elements e and e′ is called a (ring)
homomorphism if
f(e) = e′ and f(a + b) = f(a) + f(b), f(a · b) = f(a) · f(b) for all a, b ∈R.

300
A. Algebra and Number Theory
If f is a bijective homomorphism, then, automatically, the inverse map g =
f −1 is also a homomorphism. Namely, let a′, b′ ∈R′. Then a′ = f(a) and
b′ = f(b), and g(a′ · b′) = g(f(a) · f(b)) = g(f(a · b)) = a · b = g(a′) · g(b′)
(analogously for + instead of ·).
Being an isomorphism, as ψ is, is an extremely nice feature. It means, in
particular, that a is a unit in R if and only if f(a) is a unit R′ (to see this,
compute e′ = f(e) = f(a · a−1) = f(a) · f(a−1), hence f(a−1) is an inverse
of f(a)). And the “same” equations hold in domain and range. For example,
we have a2 = b in R if and only if f(a)2 = f(b) (note that f(a)2 = f(a2)).
Thus, b is a square if and only if f(b) is a square (we will use this example
in Section A.7).
Isomorphism means that the domain and range may be considered to be
the same for all questions concerning addition and multiplication.
Proof (of Theorem A.29). Since each ni divides n, the map is well deﬁned,
and it obviously is a ring homomorphism. The domain and range of the map
have the same cardinality (i.e., they contain the same number of elements).
Thus, it suﬃces to prove that ψ is surjective.
Let ti := n/ni = Q
k̸=i nk. Then ti ≡0 mod nk for all k ̸= i, and gcd(ti, ni) =
1. Hence, there is a di ∈Z with di ·ti ≡1 mod ni (Proposition A.16). Setting
ui := di · ti, we have
ui ≡0 mod nk, for all k ̸= i, and ui ≡1 mod ni.
This means that the element (0, . . . , 0, 1, 0, . . . , 0) (the i-th component is 1,
all other components are 0) is in the image of ψ. If ([x1], . . . , [xr]) ∈Zn1 ×
. . . × Znr is an arbitrary element, then ψ(Pr
i=1 xi · ui) = ([x1], . . . , [xr]).
2
Remarks:
1. Actually, the proof describes an eﬃcient algorithm for computing a num-
ber b, with b ≡bi mod ni, i = 1, . . . , r (recall our ﬁrst formulation of
the Chinese Remainder Theorem in Theorem A.28). In a preprocessing
step, the inverse elements [di] = [ti]−1 are computed modulo ni using the
extended Euclidean algorithm (Proposition A.16). Then b can be com-
puted as b = Pr
i=1 bi · di · ti, for any given integers bi, 1 ≤i ≤r.
We mainly apply the Chinese Remainder Theorem with r = 2 (for ex-
ample, in the RSA cryptosystem). Here we simply compute coeﬃcients
d and e with 1 = d · n1 + e · n2 (using the extended Euclidean algorithm
A.5), and then b = d · n1 · b2 + e · n2 · b1.
2. The Chinese Remainder Theorem can be used to make arithmetic compu-
tations modulo n easier and (much) more eﬃcient. We map the operands
to Zn1 × . . . × Znr by ψ and do our computation there. Zn1 × . . . × Znr is
a direct product of rings. Addition and multiplication are done compo-
nentwise, i.e., we perform the computation modulo ni, for i = 1, . . . , r.

A.4 Primitive Roots and the Discrete Logarithm
301
Here we work with (much) smaller numbers.3 Finally, we map the result
back to Zn by ψ−1 (which is easily done, as we have seen in the preceding
remark).
As a corollary of the Chinese Remainder Theorem, we get a formula for
Euler’s phi function for composite inputs.
Corollary A.30. Let n ∈N and n = pe1
1 · . . . · per
r
be the decomposition of n
into primes (as stated in Theorem A.12). Then:
1. Zn is isomorphic to Zpe1
1 × . . . × Zper
r .
2. Z∗
n is isomorphic to Z∗
pe1
1 × . . . × Z∗
per
r .
In particular, we have for Euler’s phi function that
ϕ(n) = n
r
Y
i=1
µ
1 −1
pi
¶
.
Proof. The ring isomorphism of Theorem A.29 induces, in particular, an iso-
morphism on the units. Hence,
ϕ(n) = ϕ(pe1
1 ) · . . . · ϕ(per
r ),
and the formula follows from Corollary A.22.
2
A.4 Primitive Roots and the Discrete Logarithm
Deﬁnition A.31. Let G be a ﬁnite group and let e be the unit element of
G. Let x ∈G. The smallest n ∈N with xn = e is called the order of x. We
write this as ord(x).
Remark. There are exponents n ∈N, with xn = e. Namely, since G is ﬁnite,
there are exponents m and m′, m < m′, with xm = xm′. Then m′ −m > 0
and xm′−m = e.
Lemma A.32. Let G be a ﬁnite group and x ∈G. Let n ∈N with xn = e.
Then ord(x) divides n.
Proof. Let n = q ·ord(x)+r, 0 ≤r < ord(x) (division with remainder). Then
xr = e. Since 0 ≤r < ord(x), this implies r = 0.
2
Corollary A.33. Let G be a ﬁnite group and x ∈G. Then ord(x) divides
the order |G| of G.
Proof. By Proposition A.23, x|G| = e.
2
Lemma A.34. Let G be a ﬁnite group and x ∈G. Let l ∈Z and d =
gcd(l, ord(x)). Then ord(xl) = ord(x)/d.
3 For example, if n = pq (as in an RSA scheme) with 512-bit numbers p and q,
then we compute with 512-bit numbers instead of with 1024-bit numbers.

302
A. Algebra and Number Theory
Proof. Let r = ord(xl). From (xl)ord(x)/d = (xord(x))l/d = e we conclude
r ≤ord(x)/d. Choose numbers a and b with d = a · l + b · ord(x) (Proposition
A.3). From xr·d = xr·a·l+r·b·ord(x) = xl·r·a = e, we derive ord(x) ≤r · d.
2
Deﬁnition A.35. Let G be a ﬁnite group. G is called cyclic if there is an
x ∈G which generates G, i.e., G = {x, x2, x3, . . . , xord(x)−1, xord(x) = e}.
Such an element x is called a generator of G.
Theorem A.36. Let p be a prime. Then Z∗
p is cyclic, and the number of
generators is ϕ(p −1).
Proof. For 1 ≤d ≤p −1, let Sd = {x ∈Z∗
p | ord(x) = d} be the units
of order d. If Sd ̸= ∅, let a ∈Sd. The equation Xd −1 has at most d
solutions in Zp, since Zp is a ﬁeld (Corollary A.17). Hence, the solutions
of Xd −1 are just the elements of A := {a, a2, . . . , ad}. Each x ∈Sd is a
solution of Xd −1, and therefore Sd ⊂A. Using Lemma A.34 we derive
that Sd = {ac | 1 ≤c < d, gcd(c, d) = 1}. In particular, we conclude that
|Sd| = ϕ(d) if Sd ̸= ∅(and an a ∈Sd exists).
By Fermat’s Little Theorem (Proposition A.24), Z∗
p is the disjoint union of
the sets Sd, d | p −1. Hence |Z∗
p| = p −1 = P
d | p−1 |Sd|. On the other hand,
p −1 = P
d | p−1 ϕ(d) (Proposition A.21), and we see that |Sd| = ϕ(d) must
hold for all divisors d of p −1. In particular, |Sp−1| = ϕ(p −1). This means
that there are ϕ(p −1) generators of Z∗
p.
2
Deﬁnition A.37. Let p be a prime. A generator g of the cyclic group Z∗
p is
called a primitive root of Z∗
p or a primitive root modulo p.
Remark. It can be proven that Z∗
n is cyclic if and only if n is one of the
following numbers: 1, 2, 4, pk or 2pk; p a prime, p ≥3, k ≥1.
Proposition A.38. Let p be a prime. Then x ∈Z∗
p is a primitive root if and
only if x(p−1)/q ̸= [1] for every prime q which divides p −1.
Proof. An element x is a primitive root if and only if x has order p−1. Since
ord(x) divides p −1 (Corollary A.33), either x(p−1)/q = [1] for some prime
divisor q of p −1 or ord(x) = p −1.
2
We may use Proposition A.38 to generate a primitive root for those primes
p for which we know (or can eﬃciently compute) the prime factors of p −1.
Algorithm A.39.
int PrimitiveRoot(prime p)
1
Randomly choose an integer g, with 0 < g < p −1
2
if g(p−1) div q ̸≡1 mod p, for all primes q dividing p −1
3
then return g
4
else
go to 1

A.4 Primitive Roots and the Discrete Logarithm
303
Since ϕ(p −1) > (p −1)/6 log(log(p −1)) (see Section A.2), we expect to ﬁnd
a primitive element after O(log(log(p))) iterations (see Lemma B.12).
No eﬃcient algorithm is known for the computation of primitive roots for
arbitrary primes. The problem is to compute the prime factors of p−1, which
we need in Algorithm A.39. Often there are primitive roots which are small.
Algorithm A.39 is used, for example, in the key-generation procedure of
the ElGamal cryptosystem (see Section 3.5.1). There the primes p are chosen
in such a way that the prime factors of p −1 can be derived eﬃciently.
Lemma A.40. Let p be a prime and let q be a prime that divides p−1. Then
the set
Gq = {x ∈Z∗
p | ord(x) = q or x = [1]},
which consists of the unit element [1] and the elements of order q, is a sub-
group of Z∗
p. Gq is a cyclic group, and every element x ∈Z∗
p of order q, i.e.,
every element x ∈Gq, x ̸= [1], is a generator. Gq is generated, for example,
by g(p−1)/q, where g is a primitive root modulo p. Gq is the only subgroup of
G of order q.
Proof. Let x, y ∈Gq. Then (xy)q = xqyq = [1], and therefore ord(xy) divides
q. Since q is a prime, we conclude that ord(xy) is 1 or q. Thus xy ∈Gq, and
Gq is a subgroup of Z∗
p. Let h ∈Z∗
p be an element of order q, for example, h :=
gp−1/q, where g is a primitive root modulo p. Then {h0, h1, h2, . . . hq−1} ⊆Gq.
The elements of Gq are solutions of the equation Xq −1 in Zp. This equation
has at most q solutions in Zp, since Zp is a ﬁeld (Corollary A.17). Therefore
{h0, h1, h2, . . . hq−1} = Gq, and h is a generator of Gq. If H is any subgroup
of order q and z ∈H, z ̸= [1], then ord(z) divides q, and hence ord(z) = q,
because q is a prime. Thus z ∈Gq, and we conclude that H = Gq.
2
Computing Modulo a Prime. The security of many cryptographic schemes
is based on the discrete logarithm assumption, which says that x 7→gx mod p
is a one-way function. Here p is a large prime and the base element g is
1. either a primitive root modulo p, i.e., a generator of Z∗
p, or
2. it is an element of order q in Z∗
p, i.e., a generator of the subgroup Gq of
order q, and q is a (large) prime that divides p −1.
Examples of such schemes which we discuss in this book are ElGamal’s en-
cryption and digital signatures, the digital signature standard DSS (see Sec-
tion 3.5), commitment schemes (see Section 4.3.2), electronic elections (see
Section 4.4) and digital cash (see Section 4.5).
When setting up such schemes, generators g of Z∗
p or Gq have to be
selected. This can be diﬃcult or even infeasible in the ﬁrst case, because we
must know the prime factors of p−1 in order to test whether a given element
g is a primitive root (see Algorithm A.39 above). On the other hand, it is
easy to ﬁnd a generator g of Gq. We simply take a random element h ∈Z∗
p
and set g := h(p−1)/q. The order of g divides q, because gq = hp−1 = [1].

304
A. Algebra and Number Theory
Since q is a prime, we conclude that ord(g) = 1 or ord(g) = q. Therefore, if
g ̸= [1], then ord(g) = q and g is a generator of Gq.
To implement cryptographic operations, we have to compute in Z∗
p or in
the subgroup Gq. The following rules simplify these computations.
1. Let x ∈Z∗
p. Then xk = xk′, if k ≡k′ mod (p −1).
In particular, xk = xk mod (p−1), i.e., exponents can be reduced by modulo
(p −1), and x−k = xp−1−k.
2. Let x ∈Z∗
p be an element of order q, i.e., x ∈Gq. Then xk = xk′, if
k ≡k′ mod q.
In particular, xk = xk mod q, i.e., exponents can be reduced by modulo q,
and x−k = xq−k.
The rules state that the exponents are added and multiplied modulo (p −1)
or modulo q. The rules hold, because xp−1 = [1] for x ∈Z∗
p (Proposition
A.24) and xq = [1] for x ∈Gq, which implies that
xk+l·(p−1) = xkxl·(p−1) = xk ¡
xp−1¢l = xk[1]l = xk for x ∈Z∗
p
and xk+l·q = xkxl·q = xk (xq)l = xk[1]l = xk for x ∈Gq.
These rules can be very useful in computations. For example, let x ∈Z∗
p
and k ∈{0, 1, . . . , p −2}. Then you can compute the inverse x−k of xk by
raising x to the (p −1 −k)-th power, x−k = xp−1−k, without explicitly
computing an inverse by using, for example, the Euclidean algorithm. Note
that (p −1 −k) is a positive exponent. Powers of x are eﬃciently computed
by the fast exponentiation algorithm (Algorithm A.26).
In many cases it is also possible to compute the k-th root of elements in
Z∗
p.
1. Let x ∈Z∗
p and k ∈N with gcd(k, p−1) = 1, i.e., k is a unit modulo p−1.
Let k−1 be the inverse of k modulo p −1, i.e., k · k−1 ≡1 mod (p −1).
Then
³
xk−1´k
= x, i.e., xk−1 is a k-th root of x in Z∗
p.
2. Let x ∈Z∗
p be an element of order q, i.e., x ∈Gq, and k ∈N with
1 ≤k < q. Let k−1 be the inverse of k modulo q, i.e., k · k−1 ≡1 mod q.
Then
³
xk−1´k
= x, i.e., xk−1 is a k-th root of x in Z∗
p.
It is common practice to denote the k-th root xk−1 by x1/k.
You can apply analogous rules of computation to elements gk in any ﬁnite
group G. Proposition A.23, which says that g|G| is the unit element, implies
that exponents k are added and multiplied modulo the order |G| of G.
A.5 Polynomials and Finite Fields
A ﬁnite ﬁeld is a ﬁeld with a ﬁnite number of elements. In Section A.2, we
met examples of ﬁnite ﬁelds: The residue class ring Zn is a ﬁeld, if and only

A.5 Polynomials and Finite Fields
305
if n is a prime. The ﬁelds Zp, p a prime number, are called the ﬁnite prime
ﬁelds, and they are also denoted by Fp. Finite ﬁelds are extensions of these
prime ﬁelds. Field extensions are constructed by using polynomials. So we
ﬁrst study the ring of polynomials with coeﬃcients in a ﬁeld k.
A.5.1 The Ring of Polynomials
Let k[X] be the ring of polynomials in one variable X over a (not necessarily
ﬁnite) ﬁeld k. The elements of k[X] are the polynomials
F(X) = a0 + a1X + a2X2 + . . . adXd =
d
X
i=0
aiXi,
with coeﬃcients ai ∈k, 0 ≤i ≤d.
If we assume that ad ̸= 0, then the leading term adXd really appears in the
polynomial, and we call d the degree of F, deg(F) for short. The polynomials
of degree 0 are just the elements of k.
The polynomials in k[X] are added and multiplied as usual:
1. We add two polynomials F = Pd
i=0 aiXi and G = Pe
i=0 biXi, assume
d ≤e, by adding the coeﬃcients (set ai = 0 for d < i ≤e):
F + G =
e
X
i=0
(ai + bi)Xi.
2. The product of two polynomials F = Pd
i=0 aiXi and G = Pe
i=0 biXi is
F · G =
de
X
i=0
Ã
i
X
k=0
akbi−k
!
Xi.
With this addition and multiplication, k[X] becomes a commutative ring with
unit element. The unit element of k[X] is the unit element 1 of k. The ring
k[X] has no zero divisors, i.e., if F and G are non-zero polynomials, then the
product F · G is also non-zero.
The algebraic properties of the ring k[X] of polynomials are analogous to
the algebraic properties of the ring of integers.
Analogously to Deﬁnition A.1, we deﬁne for polynomials F and G what
it means that F divides G and the greatest common divisor of F and G. The
greatest common divisor is unique up to a factor c ∈k, c ̸= 0, i.e., if A is a
greatest common divisor of F and G, then c · A is also a greatest common
divisor, for c ∈k∗= k \ {0}.
A polynomial F is (relatively) prime to G if the only common divisors of
F and G are the units k∗of k.
Division with remainder works as with the integers. The diﬀerence is that
the “size” of a polynomial is measured by using the degree, whereas the
absolute value was used for an integer.

306
A. Algebra and Number Theory
Theorem A.41 (Division with remainder). Let F, G ∈k[X], G ̸= 0. Then
there are unique polynomials Q, R ∈k[X], such that F = Q · G + R and
0 ≤deg(R) < deg(G).
Proof. The proof runs exactly in the same way as the proof of Theorem A.2:
Replace the absolute value with the degree.
2
R is called the remainder of F modulo G. We write F mod G for R. The
polynomial Q is the quotient of F and G. We write F div G for Q.
You can compute a greatest common divisor of polynomials F and G by
using the Euclidean algorithm, and the extended Euclidean algorithm yields
the coeﬃcients C, D ∈k[X] of a linear combination
A = C · F + D · G,
with A a greatest common divisor of F and G.
If you have obtained such a linear combination for one greatest common
divisor, then you immediately get a linear combination for any other greatest
common divisor by multiplying with a unit from k∗.
In particular, if F is prime to G, then the extended Euclidean algorithm
computes a linear combination
1 = C · F + D · G.
We also have the analogue of prime numbers.
Deﬁnition A.42. Let P ∈k[X], P ̸∈k. P is called irreducible (or a prime) if
the only divisors of P are the elements c ∈k∗and c·P, c ∈k∗, or, equivalently,
if whenever one can write P = F · G with F, G ∈k[X], then F ∈k∗or
G ∈k∗. A polynomial Q ∈k[X] which is not irreducible is called reducible
or a composite.
As the ring Z of integers, the ring k[X] of polynomials is factorial, i.e.,
every element has a unique decomposition into irreducible elements.
Theorem A.43. Let F ∈k[X], F ̸= 0, be a non-zero polynomial. There
are pairwise distinct irreducible polynomials P1, . . . , Pr, r ≥0, exponents
e1, . . . , er ∈N, ei ≥1, i = 1, . . . , r, and a unit u ∈k∗, such that
F = u
r
Y
i=1
P ei
i .
This factorization is unique in the following sense: If
F = v
s
Y
i=1
Qfi
i
is another factorization of F, then we have r = s, and after a permutation
of the indices i we have Qi = uiPi, with ui ∈k∗, and ei = fi for 1 ≤i ≤r.

A.5 Polynomials and Finite Fields
307
Proof. The proof runs in the same way as the proof of the Fundamental
Theorem of Arithmetic (Theorem A.12).
2
A.5.2 Residue Class Rings
As in the ring of integers, we can consider residue classes in k[X] and residue
class rings.
Deﬁnition A.44. Let P ∈k[X] be a polynomial of degree ≥1:
1. F, G ∈k[X] are congruent modulo P, written as
F ≡G mod P,
if P divides F −G. This means that F and G have the same remainder
when divided by P, i.e., F mod P = G mod P.
2. Let F ∈k[X]. [F] := {G ∈k[X] | G ≡F mod P} is called the residue
class of F modulo P.
As before, “congruent modulo” is an equivalence relation, the equivalence
classes are the residue classes, and the set of residue classes
k[X]/Pk[X] := {[F] | F ∈k[X]}
is a ring. Residue classes are added and multiplied by adding and multiplying
a representative:
[F] + [G] := [F + G],
[F] · [G] := [F · G].
We also have a natural representative of [F], the remainder F mod P of
F modulo P: [F] = [F mod P]. As remainders modulo P, we get all the
polynomials which have a degree < deg(P). Therefore, we have a one-to-one
correspondence between k[X]/Pk[X] and the set of residues {F ∈k[X] |
deg(F) < deg(P)}. We often identify both sets:
k[X]/Pk[X] = {F ∈k[X] | deg(F) < deg(P)}.
Two residues F and G are added or multiplied by ﬁrst adding or multiplying
them as polynomials and then taking the residue modulo P. Since the sum
of two residues F and G has a degree < deg(P), it is a residue, and we do
not have to reduce. After a multiplication, we have, in general, to take the
remainder.
Addition : (F, G) 7−→F + G ,
Multiplication : (F, G) 7−→F · G mod P.
Let n := deg(P) be the degree of P. The residue class ring k[X]/Pk[X] is
an n-dimensional vector space over k. A basis of this vector space is given by
the elements [1], [X], [X2], . . . , [Xn−1] . If k is a ﬁnite ﬁeld with q elements,
then k[X]/Pk[X] consists of qn elements.

308
A. Algebra and Number Theory
Example. Let k = F2 = Z2 = {0, 1} be the ﬁeld with two elements 0 and 1
consisting of the residues modulo 2, and P := X8 +X4 +X3 +X +1 ∈k[X].
The elements of k[X]/Pk[X] may be identiﬁed with the binary polynomials
b7X7 + b6X6 + . . . + b1X + b0, bi ∈{0, 1}, 0 ≤i ≤7, of degree ≤7. The ring
k[X]/Pk[X] contains 28 = 256 elements. We have, for example,
(X6 + X3 + X2 + 1) · (X5 + X2 + 1)
= X11 + X7 + X6 + X4 + X3 + 1
= X3 · (X8 + X4 + X3 + X + 1) + 1
≡1 mod (X8 + X4 + X3 + X + 1).
Thus, X6+X3+X2+1 is a unit in k[X]/Pk[X], and its inverse is X5+X2+1.
We may characterize units as in the integer case.
Proposition A.45. An element [F] ∈k[X]/Pk[X] is a unit if and only if F
is prime to P. The multiplicative inverse [F]−1 of a unit [F] can be computed
using the extended Euclidean algorithm.
Proof. The proof is the same as the proof in the integer case (see Proposition
A.16). Recall that the inverse may be calculated as follows: If F is prime to
P, then the extended Euclidean algorithm produces a linear combination
C · F + D · P = 1, with polynomials C, D ∈k[X].
We see that C · F ≡1 mod P. Hence, [C] is the inverse [F]−1.
2
If the polynomial P is irreducible, then all residues modulo P, i.e., all
polynomials with a degree < deg(P), are prime to P. So we get the same
corollary as in the integer case.
Corollary A.46. Let P be irreducible. Then every [F] ̸= [0] in k[X]/Pk[X]
is a unit. Thus, k[X]/Pk[X] is a ﬁeld.
Remarks:
1. Let P be an irreducible polynomial of degree n. The ﬁeld k is a subset of
the larger ﬁeld k[X]/Pk[X]. We therefore call k[X]/Pk[X] an extension
ﬁeld of k of degree n.
2. If P is reducible, then P = F · G, with polynomials F, G of degree <
deg(P). Then [F] ̸= [0] and [G] ̸= [0], but [F] · [G] = [P] = [0]. [F] and
[G] are “zero divisors”. They have no inverse, and we see that k[X]/Pk[X]
is not a ﬁeld.
A.5.3 Finite Fields
Now, let k = Zp = Fp be the prime ﬁeld of residues modulo p, p ∈Z a
prime number, and let P ∈Fp[X] be an irreducible polynomial of degree

A.5 Polynomials and Finite Fields
309
n. Then k[X]/Pk[X] = Fp[X]/PFp[X] is an extension ﬁeld of Fp. It is an
n-dimensional vector space over Fp, and it contains pn elements.
In general, there is more than one irreducible polynomial of degree n over
Fp. Therefore there are more ﬁnite ﬁelds with pn elements. For example, if
Q ∈Fp[X] is another irreducible polynomial of degree n, Q ̸= cP for all c ∈k,
then Fp[X]/QFp[X] is a ﬁeld with pn elements, diﬀerent from k[X]/Pk[X].
But one can show that all the ﬁnite ﬁelds with pn elements are isomorphic
to each other in a very natural way. As the mathematicians state it, up to
canonical isomorphism, there is only one ﬁnite ﬁeld with pn elements. It is
denoted by Fpn or by GF(pn).4
If you need a concrete representation of Fpn, then you choose an irre-
ducible polynomial P ∈Fp[X] of degree n, and you have Fpn = Fp[X]/PFp[X].
But there are diﬀerent representations, reﬂecting your degrees of freedom
when choosing the irreducible polynomial.
One can also prove that in every ﬁnite ﬁeld k, the number |k| of elements
in k must be a power pn of a prime number p. Therefore, the ﬁelds Fpn are
all the ﬁnite ﬁelds that exist.
In cryptography, ﬁnite ﬁelds play an important role in many places. For
example, the classical ElGamal cryptosystems are based on the discrete log-
arithm problem in a ﬁnite prime ﬁeld (see Section 3.5), the elliptic curves
used in cryptography are deﬁned over ﬁnite ﬁelds, and the basic encryption
operations of the Advanced Encryption Standard AES are algebraic opera-
tions in the ﬁeld F28 with 28 elements. The AES is discussed in this book
(see Section 2.2.2). This motivates the following closer look at the ﬁelds F2n.
We identify F2 = Z2 = {0, 1}. Let P = Xn + an−1Xn−1 + . . . + a1X +
a0, ai ∈{0, 1}, 0 ≤i ≤n −1 be a binary irreducible polynomial of degree
n. Then F2n = Fp[X]/PFp[X], and we may consider the binary polynomials
A = bn−1Xn−1+bn−2Xn−2+. . .+b1X +b0 of degree ≤n−1 (bi ∈{0, 1}, 0 ≤
i ≤n −1) as the elements of F2n. Adding two of these polynomials in F2n
means to add them as polynomials, and multiplying them means to ﬁrst
multiply them as polynomials and then take the remainder modulo P.
Now we can represent the polynomial A by the n-dimensional vector
bn−1bn−2 . . . b1b0 of its coeﬃcients. In this way, we get a binary represen-
tation of the elements of F2n; the elements of F2n are just the bit strings of
length n. To add two of these elements means to add them as binary vectors,
i.e., you add them bitwise modulo 2, which is the same as bitwise XORing:
bn−1bn−2 . . . b1b0
+
cn−1cn−2 . . . c1c0
= (bn−1 ⊕cn−1)(bn−2 ⊕cn−2) . . . (b1 ⊕c1)(b0 ⊕c0).
To multiply two elements is more complicated: You have to convert the bit
strings to polynomials, multiply them as polynomials, reduce modulo P and
4 Finite ﬁelds are also called Galois ﬁelds, in honor of the French mathematician
´Evariste Galois (1811–1832).

310
A. Algebra and Number Theory
take the coeﬃcients of the remainder. The 0-element of F2n is 00 . . . 00 and
the 1-element is 00 . . . 001.
In the Advanced Encryption Standard AES, encryption depends on al-
gebraic operations in the ﬁnite ﬁeld F28. The irreducible binary polynomial
P := X8 + X4 + X3 + X + 1 is taken to represent F28 as F2[X]/PF2[X] (we
already used this polynomial in an example above). Then the elements of F28
are just strings of 8 bits. In this way, a byte is an element of F28 and vice
versa. One of the core operations of AES is the so-called S-Box. The AES
S-Box maps a byte x to its inverse x−1 in F28 and then modiﬁes the result
by an F2-aﬃne transformation (see Section 2.2.2). We conclude this section
with examples for adding, multiplying and inverting bytes in F28.
01001101 + 00100101 = 01101000,
10111101 · 01101001 = 11111100,
01001101 · 00100101 = 00000001,
01001101−1 = 00100101.
As is common practice, we sometimes represent a byte and hence an element
of F28 by two hexadecimal digits. Then the examples read as follows:
4D + 25 = 68, BD · 69 = FC, 4D · 25 = 01, 4D−1 = 25.
A.6 Quadratic Residues
We will study the question as to which of the residues modulo n are squares.
Deﬁnition A.47. Let n ∈N and x ∈Z. We call that x is a quadratic residue
modulo n if there is an element y ∈Z with x ≡y2 mod n. Otherwise, x is
called a quadratic non-residue modulo n.
Examples:
1. The numbers 0, 1, 4, 5, 6 and 9 are the quadratic residues modulo 10.
2. The numbers 0, 1, 3, 4, 5 and 9 are the quadratic residues modulo 11.
Remark. The property of being a quadratic residue depends only on the
residue class [x] ∈Zn of x modulo n. An integer x is a quadratic residue
modulo n if and only if its residue class [x] is a square in the residue class
ring Zn (i.e., if and only if there is some [y] ∈Zn with [x] = [y]2). The residue
class [x] is often also called a quadratic residue.
In most cases we are only interested in the quadratic residues x which are
units modulo n (i.e., x and n are relatively prime, see Proposition A.16).

A.6 Quadratic Residues
311
Deﬁnition A.48. The subgroup of Z∗
n that consists of the residue classes
represented by a quadratic residue is denoted by QRn:
QRn = {[x] ∈Z∗
n | There is a [y] ∈Z∗
n with [x] = [y]2}.
It is called the subgroup of quadratic residues or the subgroup of squares. The
complement of QRn is denoted by QNRn := Z∗
n \ QRn. It is called the subset
of quadratic non-residues.
We give a criterion for determining the quadratic residues modulo a prime.
Lemma A.49. Let p be a prime > 2 and g ∈Z∗
p be a primitive root of Z∗
p.
Let x ∈Z∗
p. Then [x] ∈QRp if and only if x ≡gt mod p for some even
number t, 0 ≤t ≤p −2.
Proof. Recall that Z∗
p is a cyclic group generated by g (Theorem A.36). If
[x] ∈QRp, then x ≡y2 mod p, and y ≡gs mod p for some s. Then x =
g2s mod p ≡gt mod p, with t := 2s mod (p −1) (the order of g is p −1) and
0 ≤t ≤p −2. Since p −1 is even, t is also even.
Conversely, if x ≡gt mod p, and t is even, then x ≡(gt/2)2 mod p, which
means that x ∈QRp.
2
Proposition A.50. Let p be a prime > 2. Exactly half of the elements of Z∗
p
are squares, i.e., |QRp| = (p −1)/2.
Proof. Since half of the integers x with 0 ≤x ≤p−2 are even, the proposition
follows from the preceding lemma.
2
Deﬁnition A.51. Let p be a prime > 2, and let x ∈Z be prime to p.
µx
p
¶
:=
(
+1 if [x] ∈QRp,
−1 if [x] ̸∈QRp,
is called the Legendre symbol of x mod p. For x ∈Z with p|x, we set
³
x
p
´
:= 0.
Proposition A.52 (Euler’s criterion). Let p be a prime > 2, and let x ∈Z.
Then
µx
p
¶
≡x(p−1)/2 mod p.
Proof. If p divides x, then both sides are congruent 0 modulo p. Suppose p
does not divide x. Let [g] ∈Z∗
p be a primitive element.
We ﬁrst observe that g(p−1)/2 ≡−1 mod p. Namely, [g](p−1)/2 is a solution of
the equation X2 −1 over the ﬁeld Z∗
p. Hence, g(p−1)/2 ≡±1 mod p. However,
g(p−1)/2 mod p ̸= 1, because the order of [g] is p −1.
Let [x] = [g]t, 0 ≤t ≤p −2. By Lemma A.49, [x] ∈QRp if and only if
t is even. On the other hand, x(p−1)/2 ≡gt(p−1)/2 ≡±1 mod p, and it is
≡1 mod p if and only if t is even. This completes the proof.
2

312
A. Algebra and Number Theory
Remarks:
1. The Legendre symbol is multiplicative in x:
µxy
p
¶
=
µx
p
¶
·
µy
p
¶
.
This immediately follows, for example, from Euler’s criterion. It means
that [xy] ∈QRp if and only if either both [x], [y] ∈QRp or both [x], [y] /∈
QRp.
2. The Legendre symbol
³
x
p
´
depends only on x mod p, and the map
Z∗
p −→{1, −1}, x 7−→
³
x
p
´
is a homomorphism of groups.
We do not give proofs of the following two important results. Proofs
may be found, for example, in [HarWri79], [Rosen2000], [Koblitz94] and
[Forster96].
Theorem A.53. Let p be a prime > 2. Then:
1.
³
−1
p
´
= (−1)(p−1)/2 =
(
+1 if p ≡1 mod 4,
−1 if p ≡3 mod 4.
2.
³
2
p
´
= (−1)(p2−1)/8 =
(
+1 if p ≡±1 mod 8,
−1 if p ≡±3 mod 8.
Theorem A.54 (Law of Quadratic Reciprocity). Let p and q be primes > 2,
p ̸= q. Then
µp
q
¶ µq
p
¶
= (−1)(p−1)(q−1)/4.
We generalize the Legendre symbol for composite numbers.
Deﬁnition A.55. Let n ∈Z be a positive odd number and n = Qr
i=1 pei
i be
the decomposition of n into primes. Let x ∈Z.
³x
n
´
:=
r
Y
i=1
µ x
pi
¶ei
is called the Jacobi symbol of x mod n .
Remarks:
1. The value of
¡ x
n
¢
only depends on the residue class [x] ∈Zn.

A.6 Quadratic Residues
313
2. If [x] ∈QRn, then [x] ∈QRp for all primes p that divide n. Hence,
¡ x
n
¢
= 1. The converse is not true, in general. For example, let n = pq
be the product of two primes. Then
¡ x
n
¢
=
³
x
p
´
·
³
x
q
´
can be 1, whereas
both
³
x
p
´
and
³
x
q
´
are −1. This means that x mod p (and x mod q), and
hence x mod n are not squares.
3. The Jacobi symbol is multiplicative in both arguments:
³xy
n
´
=
³x
n
´
·
³ y
n
´
and
³ x
mn
´
=
³ x
m
´
·
³x
n
´
.
4. The map Z∗
n −→{1, −1}, [x] 7−→
¡ x
n
¢
is a homomorphism of groups.
5. J+1
n
:= {[x] ∈Z∗
n|
¡ x
n
¢
= 1} is a subgroup of Z∗
n.
Lemma A.56. Let n ≥3 be an odd integer. If n is a square (in Z), then
¡ x
n
¢
= 1 for all x. Otherwise, half of the elements of Z∗
n have a Jacobi symbol
of 1, i.e., |J+1
n | = ϕ(n)/2.
Proof. If n is a square, then the exponents ei in the prime factorization of n
are all even (notation as above), and the Jacobi symbol is always 1. If n is not
a square, then there is an odd ei, say e1. By the Chinese Remainder Theorem
(Theorem A.29), we ﬁnd a unit x which is a quadratic non-residue modulo
p1 and a quadratic residue modulo pi for i = 2, . . . , r. Then
¡ x
n
¢
= −1, and
mapping [y] to [y · x] yields a one-to-one map from J+1
n
to Z∗
n \ J+1
n .
2
Theorem A.57. Let n ≥3 be an odd integer. Then:
1.
¡ −1
n
¢
= (−1)(n−1)/2 =
(
+1 if n ≡1 mod 4,
−1 if n ≡3 mod 4.
2.
¡ 2
n
¢
= (−1)(n2−1)/8 =
(
+1 if n ≡±1 mod 8,
−1 if n ≡±3 mod 8.
Proof. Let f(n) = (−1)(n−1)/2 for statement 1 and f(n) = (−1)(n2−1)/8
for statement 2. You can easily check that f(n1n2) = f(n1)f(n2) for
odd numbers n1 and n2 (for statement 2, consider the diﬀerent cases of
n1, n2 mod 8). Thus, both sides of the equations
¡ −1
n
¢
= (−1)(n−1)/2 and
¡ 2
n
¢
= (−1)(n2−1)/8 are multiplicative in n, and the proposition follows from
Theorem A.53.
2
Theorem A.58 (Law of Quadratic Reciprocity). Let n, m ≥3 be odd inte-
gers. Then
³m
n
´
= (−1)(n−1)(m−1)/4 ³ n
m
´
.

314
A. Algebra and Number Theory
Proof. If m and n have a common factor, then both sides are zero by the
deﬁnition of the symbols. So we can suppose that m is prime to n. We write
m = p1p2 . . . pr and n = q1q2 . . . qs as a product of primes. Converting from
¡ m
n
¢
= Q
i,j
³
pi
qj
´
to
¡ n
m
¢
= Q
i,j
³
qj
pi
´
, we apply the reciprocity law for
the Legendre symbol (Theorem A.54) for each of the factors. We get rs
multipliers εij = (−1)(pi−1)(qj−1)/4. As in the previous proof, we use that
f(n) = (−1)(n−1)/2 is multiplicative in n and get
Y
i,j
(−1)(pi−1)(qj−1)/4 =
Y
j
ÃY
i
(−1)(pi−1)/2
!(qj−1)/2
=
Y
j
³
(−1)(m−1)/2´(qj−1)/2
=

Y
j
(−1)(qj−1)/2


(m−1)/2
=
³
(−1)(n−1)/2´(m−1)/2
= (−1)(n−1)(m−1)/4,
as desired.
2
Remark. Computing a Jacobi symbol
¡ m
n
¢
simply by using the deﬁnition
requires knowing the prime factors of n. No algorithm is known that can
compute the prime factors in polynomial time. However, using the Law of
Quadratic Reciprocity (Theorem A.58) and Theorem A.57, we can eﬃciently
compute
¡ m
n
¢
using the following algorithm, without knowing the factoriza-
tion of n.
Algorithm A.59.
Jac(int m, n)
1 Replace m by m mod n.
2 If m = 0, then
¡ m
n
¢
= 0, and if m = 1, then
¡ m
n
¢
= 1.
3 Else, set m = 2tr, with r odd.
Compute
³
2t
n
´
=
¡ 2
n
¢t by Theorem A.57.
If r = 1, we are ﬁnished.
4 If r ≥3, we still need to compute
¡ r
n
¢
.
Apply the Law of Quadratic Reciprocity and compute
¡ r
n
¢
by
¡ n
r
¢
.
5 Now set m = n and n = r and go to 1.
We have r ≤m mod n, and (n, r) becomes the pair (m, n) in the next itera-
tion. An analysis similar to that of the Euclidean algorithm (Algorithm A.4)
shows that the algorithm terminates after at most O(log2(n)) iterations (see
Corollary A.10).
Example. We want to determine whether the prime 7331 is a quadratic
residue modulo the prime 9859. For this purpose, we have to compute the
Legendre symbol
¡ 7331
9859
¢
and could do that using Euler’s criterion (Proposi-
tion A.52). However, applying Algorithm A.59 is much more eﬃcient:

A.7 Modular Square Roots
315
µ7331
9859
¶
= −
µ9859
7331
¶
= −
µ2528
7331
¶
= −
µ
2
7331
¶5
·
µ 79
7331
¶
= −(−1)5 ·
µ 79
7331
¶
=
µ 79
7331
¶
= −
µ7331
79
¶
= −
µ63
79
¶
=
µ79
63
¶
=
µ16
63
¶
=
µ 2
63
¶4
= 1.
Thus, 7331 is a quadratic residue modulo 9859.
A.7 Modular Square Roots
We now discuss how to get the square root of a quadratic residue. Computing
square roots modulo n can be a diﬃcult or even an infeasible task if n is a
composite number (and, e.g., the Rabin cryptosystem is based on this; see
Section 3.6). However, if n is a prime, we can determine square roots using
an eﬃcient algorithm.
Proposition A.60. There is a (probabilistic) polynomial algorithm Sqrt
which, given as inputs a prime p and an a ∈QRp, computes a square root
x ∈Z∗
p of a: Sqrt(p, a) = x and x2 = a (in Zp).
Remarks:
1. The square roots of a ∈QRp are the solutions of the equation X2 −a = 0
over Zp. Hence a has two square roots (for p > 2). If x is a square root,
then −x is the other root.
2. “Probabilistic” means that random choices5 are included in the algo-
rithm. Polynomial means that the running time (the number of binary
operations) of the algorithm is bounded by a polynomial in the binary
length of the inputs. Sqrt is a so-called Las Vegas algorithm, i.e., we
expect Sqrt to return a correct result in polynomial time (for a detailed
discussion of the notion of probabilistic polynomial algorithms, see Chap-
ter 5).
Proof. Let a ∈QRp. By Euler’s criterion (Proposition A.52), a(p−1)/2 = 1.
Hence a(p+1)/2 = a. We ﬁrst consider the (easy) case of p ≡3 mod 4. Since 4
divides p + 1, (p + 1)/4 is an integer, and x := a(p+1)/4 is a square root of a.
Now assume p ≡1 mod 4. The straightforward computation of the square
root as in the ﬁrst case does not work, since (p + 1)/2 is not divisible by 2. We
choose a quadratic non-residue b ∈QNRp (here the random choices come into
play, see below). By Proposition A.52, b(p−1)/2 = −1. We have a(p−1)/2 = 1,
and (p −1)/2 is even. Let (p −1)/2 = 2lr, with r odd and l ≥1. We will
5 In this chapter, all random choices are with respect to the uniform distribution.

316
A. Algebra and Number Theory
compute an exponent s, such that arb2s = 1. Then we are ﬁnished. Namely,
ar+1b2s = a and a(r+1)/2bs is a square root of a.
We obtain s in l steps. The intermediate result after step i is a representation
a2l−ir · b2si = 1. We start with a(p−1)/2 · b0 = a(p−1)/2 = 1 and s0 = 0. Let
yi = a2l−ir · b2si. In the i-th step we take the square root y′
i := a2l−ir · bsi−1
of yi−1 = a2l−i+1r · b2si−1. The value of y′
i is either 1 or −1. If y′
i = 1, then we
take yi := y′
i. If y′
i = −1, then we set yi := y′
i · b(p−1)/2. The ﬁrst time that
b appears with an exponent > 0 in the representation is after the ﬁrst step
(if ever), and then b’s exponent is (p −1)/2 = 2lr. This implies that si−1 is
indeed an even number for i = 1, . . . , l.
Thus, we may compute a square root using the following algorithm.
Algorithm A.61.
int Sqrt(int a, prime p)
1
if p ≡3 mod 4
2
then return a(p+1) div 4 mod p
3
else
4
randomly choose b ∈QNRp
5
i ←(p −1) div 2; j ←0
6
repeat
7
i ←i div 2; j ←j div 2
8
if aib j ≡−1 mod p
9
then j ←j + (p −1) div 2
10
until i ≡1 mod 2
11
return a(i+1) div 2 b j div 2 mod p
In the algorithm we get a quadratic non-residue by a random choice. For
this purpose, we randomly choose an element b of Z∗
p and test (by Euler’s
criterion) whether b is a non-residue. Since half of the elements in Z∗
p are
non-residues, we expect (on average) to get a non-residue after 2 random
choices (see Lemma B.12).
2
Now let n be a composite number. If n is a product of distinct primes
and if we know these primes, we can apply the Chinese Remainder Theorem
(Theorem A.29) and reduce the computation of square roots in Z∗
n to the
computation of square roots modulo a prime. There we can apply Algorithm
A.61. We discuss this procedure in detail for the RSA and Rabin settings,
where n = pq, with p and q being distinct primes. The extended Euclidean
algorithm yields numbers d, e ∈Z with 1 = dp+eq. By the Chinese Remainder
Theorem, the map
ψp,q : Zn −→Zp × Zq, [x] 7−→([x mod p], [x mod q])
is an isomorphism. The inverse map is given by
χp,q : Zp × Zq −→Zn, ([x1], [x2]) 7−→[dpx2 + eqx1].

A.7 Modular Square Roots
317
Addition and multiplication in Zp × Zq are done component-wise:
([x1], [x2]) + ([x′
1], [x′
2]) = ([x1 + x′
1], [x2 + x′
2]),
([x1], [x2]) · ([x′
1], [x′
2]) = ([x1 · x′
1], [x2 · x′
2]).
For [x] ∈Zn let ([x1], [x2]) := ψp,q([x]). We have
[x]2 = [a] if and only if [x1]2 = [a1] and [x2]2 = [a2]
(see the remark after Theorem A.29). Thus, in order to compute the roots of
[a], we can compute the square roots of [a1] and [a2] and apply the inverse
Chinese remainder map χp,q. In Zp and Zq, we can eﬃciently compute square
roots by using Algorithm A.61.
Recall that Zp and Zq are ﬁelds, and over a ﬁeld a quadratic equation
X2 −[a1] = 0 has at most 2 solutions. Hence, [a1] ∈Zp has at most two
square roots. The zero-element [0] has the only square root [0]. For p = 2,
the only non-zero element [1] of Z2 has the only square root [1]. For p > 2, a
non-zero element [a1] has either no or two distinct square roots. If [x1] is a
square root of [a1], then −[x1] is also a root and for p > 2, [x1] ̸= −[x1].
Combining the square roots of [a1] in Zp with the square roots of [a2] in
Zq, we get the square roots of [a] in Zn. We summarize the results in the
following proposition.
Proposition A.62. Let p and q be distinct primes > 2 and n := pq. Assume
that the prime factors p and q of n are known. Then the square roots of a
quadratic residue [a] ∈Zn can be eﬃciently computed. Moreover, if [a] =
([a1], [a2])6, then:
1. [x] = ([x1], [x2]) is a square root of [a] in Zn if and only if [x1] is a square
root of [a1] in Zp and [x2] is a square root of [a2] in Zq.
2. If [x1] and −[x1] are the square roots of [a1], and [x2] and −[x2] are
the square roots of [a2], then [u] = ([x1], [x2]), [v] = ([x1], −[x2]), −[v] =
(−[x1], [x2]) and −[u] = (−[x1], −[x2]) are all the square roots of [a].
3. [0] is the only square root of [0] = ([0], [0]).
4. If [a1] ̸= [0] and [a2] ̸= [0], which means that [a] is a unit in Zn, then
the square roots of [a] given in 2. are pairwise distinct, i.e., [a] has four
square roots.
5. If [a1] = [0] (i.e., p divides a) and [a2] ̸= [0] (i.e., q does not divide a),
then [a] has only two distinct square roots, [u] and [v].
Remark. If one of the primes is 2, say p = 2, then statements 1–3 of Proposi-
tion A.62 are also true. Statements 4 and 5 have to be modiﬁed. [a] has only
one or two roots, because [x1] = −[x1]. If [a2] = 0, then there is only one
root. If [a2] ̸= 0, then the roots [u] and [v] are distinct.
6 We identify Zn with Zp × Zq via the Chinese remainder isomorphism ψp,q.

318
A. Algebra and Number Theory
Conversely, the ability to compute square roots modulo n implies the
ability to factorize n.
Lemma A.63. Let n := pq, with distinct primes p, q > 2. Let [u] and [v] be
square roots of [a] ∈QRn with [u] ̸= ±[v]. Then the prime factors of n can
be computed from [u] and [v] using the Euclidean algorithm.
Proof. We have n | u2 −v2 = (u + v)(u −v), but n does not divide u + v and
n does not divide u −v. Hence, the computation of gcd(u + v, n) yields one
of the prime factors of n.
2
Proposition A.64. Let I := {n ∈N | n = pq, p, q distinct primes}. Then
the following statements are equivalent:
1. There is a probabilistic polynomial algorithm A1 that on inputs n ∈I and
a ∈QRn returns a square root of a in Z∗
n.
2. There is a probabilistic polynomial algorithm A2 that on input n ∈I
yields the prime factors of n.
Proof. Let A1 be a probabilistic polynomial algorithm that on inputs n and a
returns a square root of a modulo n. Then we can ﬁnd the factors of n in the
following way. We randomly select an x ∈Z∗
n and compute y = A1(n, x2).
Since a has four distinct roots by Proposition A.62, the probability that
x ̸= ±y is 1/2. If x ̸= ±y, we easily compute the factors by Lemma A.63.
Otherwise we choose a new random x. We expect to be successful after two
iterations.
Conversely, if we can compute the prime factors of n using a polyno-
mial algorithm A2, we can also compute (all the) square roots of arbitrary
quadratic residues in polynomial time, as we have seen in Proposition A.62.
2
The Chinese Remainder isomorphism can also be used to determine the
number of quadratic residues modulo n.
Proposition A.65. Let p and q be distinct primes, and n := pq. Then
|QRn| = (p −1)(q −1)/4.
Proof. [a] = ([a1], [a2]) ∈QRn if and only if [a1] ∈QRp and [a2] ∈QRq. By
Proposition A.50, |QRp| = (p −1)/2 and |QRq| = (q −1)/2.
2
Proposition A.66. Let p and q be distinct primes with p, q ≡
3 mod 4,
and n := pq. Let [a] ∈QRn, and [u] = ([x1], [x2]), [v] = ([x1], −[x2]), −[v] =
(−[x1], [x2]) and −[u] = (−[x1], −[x2]) be the four square roots of [a] (see
Proposition A.62). Then:
1.
¡ u
n
¢
= −
¡ v
n
¢
.
2. One and only one of the four square roots is in QRn.

A.8 Primes and Primality Tests
319
Proof. 1. We have u ≡v mod p and u ≡−v mod q, hence we conclude by
Theorem A.53 that
³u
n
´
=
µu
p
¶ µu
q
¶
=
µv
p
¶ µv
q
¶ µ−1
q
¶
= −
µv
p
¶ µv
q
¶
= −
³ v
n
´
.
2. By Theorem A.53,
³
−1
p
´
=
³
−1
q
´
= −1. Thus, exactly one of the roots
[x1] or −[x1] is in QRp, say [x1], and exactly one of the roots [x2] or −[x2] is
in QRq, say [x2]. Then [u] = ([x1], [x2]) is the only square root of [a] that is
in QRn.
2
A.8 Primes and Primality Tests
Theorem A.67 (Euclid’s Theorem). There are inﬁnitely many primes.
Proof. Assume that there are only a ﬁnite number of primes p1, . . . , pr. Let
n = 1 + p1 · . . . · pr. Then pi does not divide n, 1 ≤i ≤r. Thus, either n is a
prime or it contains a new prime factor diﬀerent from pi, 1 ≤i ≤r. This is
a contradiction.
2
There is the following famous result on the distribution of primes. It is
called the Prime Number Theorem and was proven by Hadamard and de la
Vall´ee Poussin.
Theorem A.68.
Let π(x) = |{p prime | p ≤x}|. Then for large x,
π(x) ≈
x
ln(x).
A proof can be found, for example, in [HarWri79] or [Newman80].
Corollary A.69. The frequency of primes among the numbers in the mag-
nitude of x is approximately 1/ln(x).
Sometimes we are interested in primes which have a given remainder c
modulo b.
Theorem A.70 (Dirichlet’s Theorem). Let b, c ∈N and gcd(b, c) = 1.
Let πb,c(x) = |{p prime | p ≤x, p = kb + c, k ∈N}|. Then for large x,
πb,c(x) ≈
1
ϕ(b)
x
ln(x).
Corollary A.71. Let b, c ∈N and gcd(b, c) = 1. The frequency of primes
among the numbers a with a mod b = c in the magnitude of x is approximately
b/ϕ(b) ln(x).

320
A. Algebra and Number Theory
Our goal in this section is to give criteria for prime numbers which can
be eﬃciently checked by an algorithm. We will use the fact that a proper
subgroup H of Z∗
n contains at most |Z∗
n|/2 = ϕ(n)/2 elements. More generally,
we prove the following basic result on groups.
Proposition A.72. Let G be a ﬁnite group and H ⊂G be a subgroup. Then
|H| is a divisor of |G|.
Proof. We consider the following equivalence relation (∼) on G: g1 ∼g2
if and only if g1 · g−1
2
∈H. The equivalence class of an element g ∈G is
gH = {gh | h ∈H}. Thus, all equivalence classes contain the same number,
namely |H|, of elements. Since G is the disjoint union of the equivalence
classes, we have |G| = |H| · r, where r is the number of equivalence classes,
and we see that |H| divides |G|.
2
Fermat’s Little Theorem (Theorem A.24) yields a necessary condition
for primes. Let n ∈N be an odd number. If n is a prime and a ∈N with
gcd(a, n) = 1, then an−1 ≡1 mod n. If there is an a ∈N with gcd(a, n) = 1
and an−1 ̸≡1 mod n, then n is not a prime.
Unfortunately, the converse is not true: there are composite (i.e., non-prime)
numbers n, such that an−1 ≡1 mod n, for all a ∈N with gcd(a, n) = 1. The
smallest n with this property is 561 = 3 · 11 · 17.
Deﬁnition A.73. Let n ∈N, n ≥3, be a composite number. We call n a
Carmichael number if
an−1 ≡1 mod n,
for all a ∈N with gcd(a, n) = 1.
Proposition A.74. Let n be a Carmichael number, and let p be a prime that
divides n. Then p2 does not divide n. In other words, the factorization of n
does not contain squares.
Proof. Assume n = pkm, with k ≥2 and p does not divide m. Let b := 1+pm.
From bp = (1 + pm)p = 1 + p2 · α we derive that bp ≡1 mod p2. Since p does
not divide m, we have b ̸≡1 mod p2 and conclude that b has order p modulo
p2. Now, b is prime to n, because it is prime to p and m, and n is a Carmichael
number. Hence bn−1 ≡1 mod n, and then, in particular, bn−1 ≡1 mod p2.
Thus p | n −1 (by Lemma A.32), a contradiction to p | n.
2
Proposition A.75. Let n be an odd, composite number that does not contain
squares. Then n is a Carmichael number if and only if (p −1) | (n −1) for
all prime divisors p of n.
Proof. Let n = p1 · . . . · pr, with pi being a prime, i = 1, . . . , r, and pi ̸= pj
for i ̸= j. n is a Carmichael number if and only if an−1 ≡1 mod n for all
a that are prime to n and, by the Chinese Remainder Theorem this in turn
is equivalent to an−1 ≡
1 mod pi, for all a which are not divided by pi,

A.8 Primes and Primality Tests
321
i = 1, . . . , r. This is the case if and only if (pi −1) | (n −1), for i = 1, . . . , r.
The last equivalence follows from Proposition A.23 and Corollary A.33, since
Z∗
pi is a cyclic group of order pi −1 (Theorem A.36).
2
Corollary A.76. Every Carmichael number n contains at least three distinct
primes.
Proof. Assume n = p1 ·p2, with p1 < p2. Then n−1 = p1(p2 −1)+(p1 −1) ≡
(p1 −1) mod (p2 −1). However, (p1 −1) ̸≡0 mod (p2 −1), since 0 < p1 −1 <
p2 −1. Hence, p2 −1 does not divide n −1. This is a contradiction.
2
Though Carmichael numbers are extremely rare (there are only 2163
Carmichael numbers below 25 · 109), the Fermat condition an−1 ≡1 mod p
is not reliable for a primality test.7 We are looking for other criteria.
Let n ∈N be an odd number. If n is a prime, by Euler’s criterion (Propo-
sition A.52),
¡ a
n
¢
≡a(n−1)/2 mod n for every a ∈N with gcd(a, n) = 1. Here
the converse is also true. More precisely:
Proposition A.77. Let n be an odd and composite number. Let
En =
n
[a] ∈Z∗
n
¯¯¯
³ a
n
´
̸≡a(n−1)/2 mod n
o
.
Then |En| ≥ϕ(n)/2, i.e., for more than half of the [a], we have
¡ a
n
¢
̸≡a(n−1)/2 mod n.
Proof. Let
En
:=
Z∗
n \ En
be
the
complement
of
En.
We
have
En = {[a] ∈Z∗
n |
¡ a
n
¢
≡a(n−1)/2 mod n} = {[a] ∈Z∗
n | a(n−1)/2 ·
¡ a
n
¢−1 ≡
1 mod n}. Since En is a subgroup of Z∗
n, we could infer |En| ≤ϕ(n)/2 if En
were a proper subset of Z∗
n (Proposition A.72). Then |En| = |Z∗
n| −|En| ≥
ϕ(n) −ϕ(n)/2 = ϕ(n)/2.
Thus it suﬃces to prove: if En = Z∗
n, then n is a prime.
Assume En = Z∗
n and n is not a prime. From
¡ a
n
¢
≡a(n−1)/2 mod n, it follows
that an−1 ≡1 mod n. Thus n is a Carmichael number, and hence does not
contain squares (Proposition A.74). Let n = p1 · . . . · pk, k ≥3, be the decom-
position of n into distinct primes. Let [v] ∈Z∗
p1 be a quadratic non-residue,
i.e.,
³
v
p1
´
= −1. Using the Chinese Remainder Theorem, choose an [x] ∈Z∗
n
with x ≡v mod p1 and x ≡1 mod n/p1. Then
¡ x
n
¢
=
³
x
p1
´
· . . . ·
³
x
pn
´
= −1.
Since En = Z∗
n,
¡ x
n
¢
≡x(n−1)/2 mod n, and hence x(n−1)/2 ≡−1 mod n, in
particular x(n−1)/2 ≡−1 mod p2. This is a contradiction.
2
The following considerations lead to a necessary condition for primes that
does not require the computation of Jacobi symbols. Let n ∈N be an odd
number, and let n −1 = 2tm, with m odd. Suppose that n is a prime. Then
Zn is a ﬁeld (Corollary A.17), and hence ±1 are the only square roots of 1,
7 Numbers n satisfying an−1 ≡1 mod n are called pseudoprimes for the base a.

322
A. Algebra and Number Theory
i.e., the only solutions of X2 −1, modulo n. Moreover, an−1 ≡1 mod n for
every a ∈N that is prime to n (Theorem A.24). Thus
an−1 ≡1 mod n and a(n−1)/2 ≡±1 mod n, and
if a(n−1)/2 ≡1 mod n, then a(n−1)/4 ≡±1 mod n, and
if a(n−1)/4 ≡1 mod n, then . . . .
We see: if n is a prime, then for every a ∈N with gcd(a, n) = 1, either
am ≡1 mod n, or there is a j ∈{0, . . . , t −1} with a2jm ≡−1 mod n. The
converse is also true, i.e., if n is composite, then there exists an a ∈N with
gcd(a, n) = 1, such that am ̸≡1 mod n and a2jm ̸≡−1 mod n for 0 ≤j ≤
t −1. More precisely:
Proposition A.78. Let n ∈N be a composite odd number. Let n −1 = 2tm,
with m odd. Let
Wn = {[a] ∈Z∗
n | am ̸≡1 mod n and a2jm ̸≡−1 mod n for 0 ≤j ≤t −1}.
Then |Wn| ≥ϕ(n)/2.
Proof. Let Wn := Z∗
n \ Wn be the complement of Wn. We will show that Wn
is contained in a proper subgroup U of Z∗
n. Then the desired estimate follows
by Proposition A.72, as in the proof of Proposition A.77. We distinguish two
cases.
Case 1: There is an [a] ∈Z∗
n with an−1 ̸≡1 mod n.
Then U = {[a] ∈Z∗
n | an−1 ≡1 mod n} is a proper subgroup of Z∗
n, which
contains Wn, and the proof is ﬁnished.
Case 2: We have an−1 ≡1 mod n for all [a] ∈Z∗
n.
Then n is a Carmichael number. Hence n does not contain any squares
(Proposition A.74). Let n = p1 · . . . · pk, k ≥3, be the decomposition into
distinct primes. We set
W i
n = {[a] ∈Z∗
n | a2im ≡−1 mod n}.
W 0
n is not empty, since [−1] ∈W 0
n. Let r = max{i | W i
n ̸= ∅} and
U := {[a] ∈Z∗
n | a2rm ≡±1 mod n}.
U is a subgroup of Z∗
n and Wn ⊂U. Let [a] ∈W r
n. Using the Chinese
Remainder Theorem, we get a [w] ∈Z∗
n with w ≡a mod p1 and w ≡1 mod
n/p1. Then w2rm ≡
−1 mod p1 and w2rm ≡
+1 mod p2, hence w2rm ̸≡
±1 mod n. Thus w ̸∈U, and we see that U is indeed a proper subgroup of
Z∗
n.
2
Remark. The set En from Proposition A.77 is a subset of Wn, and it can even
be proven that |Wn| ≥3
4ϕ(n) (see, e.g., [Koblitz94]).

A.8 Primes and Primality Tests
323
Probabilistic Primality Tests. The preceding propositions are the basis
of probabilistic algorithms which test whether a given odd number is prime.
Proposition A.77 yields the Solovay-Strassen primality test, and Proposition
A.78 yields the Miller-Rabin primality test. The basic procedure is the same
in both tests: we deﬁne a set W of witnesses for the fact that n is composite.
Set W := En (Solovay-Strassen) or W := Wn (Miller-Rabin). If we can ﬁnd
a w ∈W, then W ̸= ∅, and n is a composite number.
To ﬁnd a witness w ∈W, we randomly choose (with respect to the uni-
form distribution) an element a ∈Z∗
n and check whether a ∈W. Since
|W| ≥ϕ(n)/2 (Propositions A.77 and A.78), the probability that we get a
witness by the random choice, if n is composite, is ≥1/2. By repeating the
random choice k times, we can increase the probability of ﬁnding a witness if
n is composite. The probability is then ≥1−1/2k. If we do not ﬁnd a witness,
n is considered to be a prime.
The tests are probabilistic and the result is not necessarily correct in all
cases. However, the error probability is ≤1/2k and hence very small, even for
moderate values of k.
Remark. The primality test of Miller-Rabin is the better choice:
1. The test condition is easier to compute.
2. A witness for the Solovay-Strassen test is also a witness for the Miller-
Rabin test.
3. In the Miller-Rabin test, the probability of obtaining a witness by one
random choice is ≥3/4 (we only proved the weaker bound of 1/2).
Here follows the algorithm for the Miller-Rabin test (with error probability
≤1/4k).
Algorithm A.79.
boolean MillerRabinTest(int n, k)
1
if n is even
2
then return false
3
m ←(n −1) div 2; t ←1
4
while m is even do
5
m ←m div 2; t ←t + 1
6
for i ←1 to k do
7
a ←Random() mod n
8
u ←am mod n
9
if u ̸= 1
10
then j ←1
11
while u ̸= −1 and j < t do
12
u ←u2 mod n; j ←j + 1
13
if u ̸= −1
14
then return false
15
return true ;

B. Probabilities and Information Theory
We review some basic notions and results using in this book concerning prob-
ability, probability spaces and random variables. This chapter is also intended
to establish notation. There are many textbooks on probability theory, in-
cluding [Bauer96], [Feller68], [GanYlv67], [Gordon97] and [R´enyi70].
B.1 Finite Probability Spaces and Random Variables
First we summarize basic concepts and notations. At the end of this section
we derive a couple of elementary results. They are useful in our computations
with probabilities. We consider only ﬁnite probability spaces.
Deﬁnition B.1.
1. A probability distribution (or simply distribution) p = (p1, . . . , pn) is a
tuple of elements pi ∈R, 0 ≤pi ≤1, called probabilities, such that
Pn
i=1 pi = 1.
2. A probability space (X, pX) is a ﬁnite set X = {x1, . . . , xn} equipped with
a probability distribution pX = (p1, . . . , pn). pi is called the probability
of xi, 1 ≤i ≤n. We also write pX(xi) := pi and consider pX as a map
X →[0, 1], called the probability measure on X, associating with x ∈X
its probability.
3. An event E in a probability space (X, pX) is a subset E of X. The prob-
ability measure is extended to events:
pX(E) =
X
y∈E
pX(y).
Example. Let X be a ﬁnite set. The uniform distribution pX,u is deﬁned by
pX,u(x) := 1/|X|, for all x ∈X. All elements of X have the same probability.
Notation. If the probability measure is determined by the context, we of-
ten do not specify it explicitly and simply write X instead of (X, pX) and
prob(x) or prob(E) instead of pX(x) or pX(E). If E and F are events, we
write prob(E, F) instead of prob(E ∩F) for the probability that both events
E and F occur. Separating events by commas means combining them with

326
B. Probabilities and Information Theory
AND. The events {x} containing a single element are simply denoted by x.
For example, prob(x, E) means the probability of {x} ∩E (which is 0 if x ̸∈E
and prob(x) otherwise).
Remark. Let X be a probability space. The following properties of the prob-
ability measure are immediate consequences of Deﬁnition B.1:
1. prob(X) = 1, prob(∅) = 0.
2. prob(A ∪B) = prob(A) + prob(B), if A and B are disjoint events in X.
3. prob(X \ A) = 1 −prob(A).
Deﬁnition B.2. Let S : X −→Y be a map from a probability space (X, pX)
to a set Y . Then S and pX induce a distribution S(pX) on Y :
S(pX)(y) := pX(S−1(y)) := pX({x ∈X | S(x) = y}).
The distribution S(pX) is called the image (distribution) of pX under S.
Deﬁnition B.3. Let (X, pX) be a probability space. A map S : X −→Y is
called a Y -valued random variable on X. The distribution pS of a random
variable S is the image of pX under S:
pS(y) := S(pX)(y) = pX({x ∈X | S(x) = y}) for y ∈Y.
We call S a real-valued random variable if Y ⊂R, and a binary random
variable if Y = {0, 1}. Binary random variables are also called Boolean pred-
icates.
Remark. Considering the distribution of a random variable S : X −→Y
means considering the distribution of the probability space induced as image
on Y by S.
Vice versa, it is sometimes convenient (and common practice in probabil-
ity theory) to look at a probability space as if it were a random variable.
Namely, consider (X, pX) as a random variable SX that is deﬁned on some
not further-speciﬁed probability space (Ω, pΩ) and samples over X according
to the given distribution:
SX : Ω−→X and pX = SX(pΩ).
Deﬁnition B.4. Let S be a real-valued random variable on a ﬁnite proba-
bility space X. The probabilistic average
E(S) :=
X
x∈X
prob(x) · S(x)
is called the expected value of S.
Remark. The expected value E(S) is a weighted average. The weight of each
value is its probability.

B.1 Finite Probability Spaces and Random Variables
327
Deﬁnition B.5. Let (X, pX) be a probability space and A, B ⊆X be events,
with pX(B) > 0. The conditional probability of A assuming B is
pX(A|B) := pX(A, B)
pX(B) .
In particular, we have
pX(x|B) =
(
pX(x)/pX(B) if x ∈B,
0
if x ̸∈B.
The conditional probabilities pX(x|B), x ∈X, deﬁne a probability distribu-
tion on X. They describe the probability of x assuming that event B occurs.
If C is a further event, then pX(A|B, C) is the conditional probability
of A assuming B ∩C. Separating events by commas in a condition means
combining them with AND.
Deﬁnition B.6. Let A, B ⊆X be events in a probability space (X, pX).
A and B are called independent if and only if prob(A, B) = prob(A)·prob(B).
If prob(B) > 0, then this condition is equivalent to prob(A|B) = prob(A).
Deﬁnition B.7. A probability space (X, pX) is called a joint probability
space with factors (X1, p1), . . . , (Xr, pr), denoted for short by X1X2 . . . Xr,
if:
1. The set X is the Cartesian product of the sets X1, . . . , Xr:
X = X1 × X2 × . . . × Xr.
2. The distribution pi, 1 ≤i ≤r, is the image of pX under the projection
πi : X −→Xi, (x1, . . . , xr) 7−→xi,
which means
pi(x) = pX(π−1
i
(x)), for 1 ≤i ≤r and x ∈Xi.
The probability spaces X1, . . . , Xr are called independent if and only if
pX(x1, . . . , xr) =
r
Y
i=1
pi(xi), for all (x1, . . . , xr) ∈X
(or, equivalently, if the ﬁbers π−1
i
(xi), 1 ≤i ≤r,1 are independent events in
X, in the sense of Deﬁnition B.6). In this case X is called the direct product
of the Xi, denoted for short by X = X1 × X2 × . . . × Xr.
1 The set π−1(x) of pre-images of a single element x under a projection map π is
called the ﬁber over x.

328
B. Probabilities and Information Theory
Analogously, random variables S1, . . . , Sr are called jointly distributed if
there is a joint probability distribution pS of S1, . . . , Sr:
prob(S1 = x1, S2 = x2, . . . , Sr = xr) = pS(x1, . . . , xr).
They are called independent if and only if
prob(S1 = x1, S2 = x2, . . . , Sr = xr) =
r
Y
i=1
prob(Si = xi),
for all x1, . . . , xr.
Remark. Recall that the distribution of a random variable may be considered
as the distribution of a probability space, and vice versa (see above). In
this way, joint probability spaces correspond to jointly distributed random
variables.
Notation. Let (XY, pXY ) be a joint probability space with factors (X, pX)
and (Y, pY ). Let x ∈X and y ∈Y . Then we denote by prob(y|x) the con-
ditional probability pXY ((x, y)|{x} × Y ) of (x, y), assuming that the ﬁrst
component is x. Thus, prob(y|x) is the probability that y occurs as the sec-
ond component if the ﬁrst component is x.
Remark. In this book we often meet joint probability spaces in the following
way: a set X and a family W = (Wx)x∈X of sets are given. Then we may
join the set X with the family W, to get
X ▷◁W := {(x, w) | x ∈X, w ∈Wx} =
[
x∈X
{x} × Wx.
The set Wx becomes the ﬁber over x.
Assume that probability distributions pX on X and pWx on Wx, x ∈X,
are given. Then we get a joint probability distribution pXW on X ▷◁W using
pXW (x, w) := pX(x) · pWx(w).
Conversely, given a distribution pXW on X ▷◁W, we can project X ▷◁W to
X. We get the image distribution pX on X using
pX(x) :=
X
w∈Wx
pXW (x, w)
and the probability distributions pWx on Wx, x ∈X, using
pWx(w) = pXW ((x, w)|{x} × Wx) = pXW (x, w)/pX(x).
The probabilities pWx are conditional probabilities.
prob(w|x) := pWx(w) is the conditional probability that w occurs as the
second component, assuming that the ﬁrst component x is given.

B.1 Finite Probability Spaces and Random Variables
329
We write XW for short for the probability space (X ▷◁W, pXW ), as
before, and we call it a joint probability space. At ﬁrst glance it does not
meet Deﬁnition B.7, because the underlying set is not a Cartesian product
(except in the case where all sets Wx are equal). However, all sets are assumed
to be ﬁnite. Therefore, we can easily embed all the sets Wx into one larger
set ˜W. Then X ▷◁W ⊆X × ˜W, and pXW may also be considered as a
probability distribution on X × ˜W (extend it by zero, so that all elements
outside X ▷◁W have a probability of 0). In this way we get a joint probability
space X ˜W in the strict sense of Deﬁnition B.7. XW and X ˜W are practically
the “same” as probability spaces (the diﬀerence has a measure of 0).
As an example, think of the domain of the modular squaring one-
way function (Section 3.2) (n, x) 7→x2 mod n, where n ∈Ik = {pq |
p, q distinct primes, |p| = |q| = k} and x ∈Zn, and assume that the moduli
n (the keys, see Rabin encryption, Section 3.6.1) and the elements x (the
messages) are selected according to some probability distributions (e.g. the
uniform ones). Here X = Ik and W = (Zn)n∈Ik.
The joining may be iterated. Let X1 = (X1, p1) be a probability space
and let Xj = (Xj,x, pj,x)x∈X1▷◁...▷◁Xj−1, 2 ≤j ≤r, be families of probability
spaces. Then by iteratively joining the ﬁbers we get a joint probability space
(X1X2 . . . Xr, pX1X2...Xr).
Notation. We introduce some notation which turns out to be very useful in
many situations.
Let (X, pX) be a probability space and B : X −→{0, 1} be a Boolean
predicate. Then
prob(B(x) = 1 : x
pX
←X) := pX({x ∈X | B(x) = 1}).
The notation suggests that pX({x ∈X | B(x) = 1}) is the probability for
B(x) = 1 if x is randomly selected from X according to pX. If the distribution
pX is clear from the context, we simply write
prob(B(x) = 1 : x ←X)
instead of prob(B(x) : x
pX
←X). If pX is the uniform distribution, we write
prob(B(x) = 1 : x
u←X).
Sometimes, we denote the probability distribution on X by x ←X and
the uniform distribution by x
u←X. We emphasize in this way that the
members of X are chosen randomly.
If Y ⊂X and pY is a distribution on Y , then the notation x ←Y is not
only used for the distribution pY , but also for the image of pY on X (under
the inclusion map). This intuitively suggests that elements x are randomly
chosen from the subset Y , whereas elements outside of Y have zero probability
and are not chosen.

330
B. Probabilities and Information Theory
As before, let S(pX) denote the image distribution under a map
S : X −→Y . Then
prob(S(x) = y : x ←X) = S(pX)(y) = pX({x ∈X | S(x) = y}), for y ∈Y.
Sometimes we denote the image distribution by {S(x) : x ←X}. This nota-
tion indicates that the probability measure is concentrated on the image of
X – only the elements S(x) in Y can have a probability > 0 – and that the
probability of y ∈Y is given by the probability for y appearing as S(x), if x
is randomly selected from X.
Now let (XW, pXW ) be a joint probability space, W = (Wx)x∈X, as in-
troduced above, and let pX and pWx, x ∈X, be the probability distributions
induced on X and the ﬁbers Wx. We write
prob(B(x, w) = 1 : x
pX
←X, w
pWx
←Wx), or simply
prob(B(x, w) = 1 : x ←X, w ←Wx)
instead of pXW ({(x, w) | B(x, w) = 1}). Here, B is a Boolean predicate on
X ▷◁W. The notation suggests that we mean the probability for B(x, w) = 1,
if ﬁrst x is randomly selected and then w is randomly selected from Wx.
More generally, if (X1X2 . . . Xr, pX1X2...Xr) is formed by iteratively join-
ing ﬁbers (see above; we also use the notation from above), then we write
prob(B(x1, . . . , xr) = 1 : x1 ←X1, x2 ←X2,x1,
x3 ←X3,x1x2,
. . .
, xr ←Xr,x1...xr−1)
instead of pX1X2...Xr({(x1, . . . , xr) | B(x1, . . . , xr) = 1}). Again we write
more precisely
u←(or
p←) instead of ←if the distribution is the uniform one
(or not clear from the context).
The distribution xj ←Xj,x1...xj−1 is the conditional distribution of xj ∈
Xj,x1...xj−1, assuming x1, . . . , xj−1; i.e., it gives the conditional probabilities
prob(xj |x1, . . . , xj−1). We have (for r = 3) that
prob(B(x1, x2, x3) = 1 : x1 ←X1, x2 ←X2,x1, x3 ←X3,x1x2)
=
X
(x1,x2,x3):B(x1,x2,x3)=1
prob(x1) · prob(x2 |x1) · prob(x3 |x2, x1).
We now derive a couple of elementary results. They are used in our com-
putations with probabilities.
Proposition B.8. Let X be a ﬁnite probability space, and let X be the dis-
joint union of events E1, . . . , Er ⊆X, with prob(Ei) > 0 for i = 1 . . . r. Then
prob(A) =
r
X
i=1
prob(Ei) · prob(A|Ei)
for every event A ⊆X.

B.1 Finite Probability Spaces and Random Variables
331
Proof.
prob(A) =
r
X
i=1
prob(A ∩Ei) =
r
X
i=1
prob(Ei) · prob(A|Ei).
2
Lemma B.9. Let A, B, E ⊆X be events in a probability space X, with
prob(E) > 0. Suppose that A and B have the same conditional probability
assuming E, i.e., prob(A|E) = prob(B|E). Then
|prob(A) −prob(B)| ≤prob(X \ E).
Proof. By Proposition B.8, we have
prob(A) = prob(E) · prob(A|E) + prob(X \ E) · prob(A|X \ E)
and an analogous equality for prob(B). Subtracting both equalities, we get
the desired inequality.
2
Lemma B.10. Let A, E ⊆X be events in a probability space X, with
prob(E) > 0. Then
| prob(A) −prob(A|E) | ≤prob(X \ E).
Proof. By Proposition B.8, we have
prob(A) = prob(E) · prob(A|E) + prob(X \ E) · prob(A|X \ E).
Hence,
| prob(A) −prob(A|E) |
= prob(X \ E) · | prob(A|X \ E) −prob(A|E) |
≤prob(X \ E),
as desired.
2
We continue with two results on expected values of random variables.
Proposition B.11. Let R and S be real-valued random variables:
1. E(R + S) = E(R) + E(S).
2. If R and S are independent, then E(R · S) = E(R) · E(S).
Proof. Let WR, WS ⊂R be the images of R and S. Since we consider only
ﬁnite probability spaces, WR and WS are ﬁnite sets. We have

332
B. Probabilities and Information Theory
E(R + S) =
X
x∈WR,y∈WS
prob(R = x, S = y) · (x + y)
=
X
x∈WR,y∈WS
prob(R = x, S = y) · x
+
X
x∈WR,y∈WS
prob(R = x, S = y) · y
=
X
x∈WR
prob(R = x) · x +
X
y∈WS
prob(S = y) · y
= E(R) + E(S),
and if R and S are independent,
E(R · S) =
X
x∈WR,y∈WS
prob(R = x, S = y) · x · y
=
X
x∈WR,y∈WS
prob(R = x) · prob(S = y) · x · y
=
Ã X
x∈WR
prob(R = x) · x
!
·

X
y∈WS
prob(S = y) · y


= E(R) · E(S),
as desired.
2
A probability space X is the model of a random experiment. n indepen-
dent repetitions of the random experiment are modeled by the direct product
Xn = X ×. . .×X. The following lemma answers the question as to how often
we have to repeat the experiment until a given event is expected to occur.
Lemma B.12. Let E be an event in a probability space X, with prob(E) =
p > 0. Repeatedly, we execute the random experiment X independently. Let
G be the number of executions of X, until E occurs the ﬁrst time. Then the
expected value E(G) of the random variable G is 1/p.
Proof. We have prob(G = t) = p · (1 −p)t−1. Hence,
E(G) =
∞
X
t=1
t · p · (1 −p)t−1 = −p · d
dp
∞
X
t=1
(1 −p)t = −p · d
dp
1
p = 1
p.
2
Remark. G is called the geometric random variable with respect to p.
Lemma B.13. Let R, S and B be jointly distributed random variables with
values in {0, 1}. Assume that B and S are independent and that B is uni-
formly distributed: prob(B = 0) = prob(B = 1) = 1/2. Then
prob(R = S) = 1
2 + prob(R = B |S = B) −prob(R = B).

B.2 The Weak Law of Large Numbers
333
Proof. We denote by B the complementary value 1−B of B. First we observe
that prob(S = B) = prob(S = B) = 1/2, since B is uniformly distributed
and independent of S:
prob(S = B)
= prob(S = 0) · prob(B = 0|S = 0) + prob(S = 1) · prob(B = 1|S = 1)
= prob(S = 0) · prob(B = 0) + prob(S = 1) · prob(B = 1)
= (prob(S = 0) + prob(S = 1)) · 1
2 = 1
2.
Now, we compute
prob(R = S)
= 1
2 · prob(R = B|S = B) + 1
2 · prob(R = B|S = B)
= 1
2 · (prob(R = B|S = B) + 1 −prob(R = B|S = B))
= 1
2 + 1
2 · (prob(R = B|S = B) −prob(R = B|S = B))
= 1
2 + 1
2 ·
Ã
prob(R = B|S = B)
−prob(R = B) −prob(S = B) · prob(R = B|S = B)
prob(S = B)
!
= 1
2 + 1
2 · (prob(R = B|S = B)
−(2 · prob(R = B) −prob(R = B|S = B)))
= 1
2 + prob(R = B|S = B) −prob(R = B),
and the lemma is proven.
2
B.2 The Weak Law of Large Numbers
The variance of a random variable S measures how much the values of S on
average diﬀer from the expected value.
Deﬁnition B.14. Let S be a real-valued random variable. The expected
value E((S −E(S))2) of (S −E(S))2 is called the variance of S or, for short,
V ar(S).
If the variance is small, the probability that the value of S is far from E(S)
is low. This fundamental fact is stated precisely in Chebyshev’s inequality:

334
B. Probabilities and Information Theory
Theorem B.15 (Chebyshev’s inequality). Let S be a real-valued random
variable with expected value α and variance σ2. Then for every ε > 0,
prob(|S −α| ≥ε) ≤σ2
ε2 .
Proof. Let WS ⊂R be the image of S. Since we consider only ﬁnite proba-
bility spaces, WS is a ﬁnite set. Computing
σ2 = E((S −α)2) =
X
x∈WS
prob(S = x) · (x −α)2
≥
X
x∈WS,|x−α|≥ε
prob(S = x) · ε2 = ε2 · prob(|S −α| ≥ε),
we obtain Chebyshev’s inequality.
2
Proposition B.16 (weak law of large numbers). Let S1, . . . , St be pairwise-
independent real-valued random variables, with a common expected value and
a common variance: E(Si) = α and V ar(Si) = σ2, i = 1, . . . , t.
Then for every ε > 0,
prob
Ã¯¯¯¯¯
1
t
t
X
i=1
Si −α
¯¯¯¯¯ < ε
!
≥1 −σ2
tε2 .
Proof. Let Z = 1
t
Pt
i=1 Si. Then we have E(Z) = α and
V ar(Z) = E((Z −α)2) = E


Ã
1
t
t
X
i=1
(Si −α)
!2

= 1
t2 E


t
X
i=1
(Si −α)2 +
X
i̸=j
(Si −α) · (Sj −α)


= 1
t2
t
X
i=1
E((Si −α)2) +
X
i̸=j
E((Si −α) · (Sj −α))
= 1
t2 σ2t = σ2
t .
Here observe that for i ̸= j, E((Si −α) · (Sj −α)) = E(Si −α) · E(Sj −α),
since Si and Sj are independent, and E(Si−α) = E(Si)−α = 0 (Proposition
B.11).
From Chebyshev’s inequality (Theorem B.15), we conclude that
prob(|Z −α| ≥ε) ≤σ2
tε2 ,
and the weak law of large numbers is proven.
2

B.2 The Weak Law of Large Numbers
335
Remark. In particular, the weak law of large numbers may be applied to t
executions of the random experiment underlying a random variable S. It says
that the mean of the observed values of S comes close to the expected value
of S if the executions are pairwise-independent (or even independent) and its
number t is suﬃciently large.
If S is a binary random variable observing whether an event occurs or
not, there is an upper estimate for the variance, and we get the following
corollary.
Corollary B.17. Let S1, . . . , St be pairwise-independent binary random vari-
ables, with a common expected value and a common variance: E(Si) = α and
V ar(Si) = σ2, i = 1, . . . , t. Then for every ε > 0,
prob
Ã¯¯¯¯¯
1
t
t
X
i=1
Si −α
¯¯¯¯¯ < ε
!
≥1 −
1
4tε2 .
Proof. Since Si is binary, we have Si = S2
i and get
V ar(Si) = E((Si −α)2) = E(S2
i −2αSi + α2)
= E(S2
i ) −2αE(Si) + α2 = α −2α2 + α2 = α(1 −α) ≤1
4.
Now apply Proposition B.16.
2
Corollary B.18. Let S1, . . . , St be pairwise-independent binary random vari-
ables, with a common expected value and a common variance: E(Si) = α and
V ar(Si) = σ2, i = 1, . . . , t. Assume E(Si) = α = 1/2 + ε, ε > 0. Then
prob
Ã
t
X
i=1
Si > t
2
!
≥1 −
1
4tε2 .
Proof.
If
¯¯¯¯¯
1
t
t
X
i=1
Si −α
¯¯¯¯¯ < ε, then 1
t
t
X
i=1
Si > 1
2.
We conclude from Corollary B.17 that
prob
Ã
t
X
i=1
Si > t
2
!
≥prob
Ã¯¯¯¯¯
1
t
t
X
i=1
Si −α
¯¯¯¯¯ < ε
!
≥1 −
1
4tε2 ,
and the corollary is proven.
2

336
B. Probabilities and Information Theory
B.3 Distance Measures
We deﬁne the distance between probability distributions. Further, we prove
some statements on the behavior of negligible probabilities when the proba-
bility distribution is varied a little.
Deﬁnition B.19. Let p and ˜p be probability distributions on a ﬁnite set X.
The statistical distance between p and ˜p is
dist(p, ˜p) := 1
2
X
x∈X
| p(x) −˜p(x) |.
Remark. The statistical distance deﬁnes a metric on the set of distributions
on X.
Lemma B.20. The statistical distance between probability distributions p and
˜p on a ﬁnite set X is the maximal distance between the probabilities of events
in X, i.e.,
dist(p, ˜p) = max
E⊆X | p(E) −˜p(E) |.
Proof. Recall that the events in X are the subsets of X. Let
E1 := {x ∈X | p(x) > ˜p(x)},
E2 := {x ∈X | p(x) < ˜p(x)},
E3 := {x ∈X | p(x) = ˜p(x)}.
Then
0 = p(X) −˜p(X) =
3
X
i=1
p(Ei) −˜p(Ei),
and p(E3) −˜p(E3) = 0. Hence, p(E2) −˜p(E2) = −(p(E1) −˜p(E1)), and we
obviously get
max
E⊆X | p(E) −˜p(E) | = p(E1) −˜p(E1) = −(p(E2) −˜p(E2)).
We compute
dist(p, ˜p) = 1
2
X
x∈X
| p(x) −˜p(x) |
= 1
2
Ã X
x∈E1
(p(x) −˜p(x)) −
X
x∈E2
(p(x) −˜p(x))
!
= 1
2(p(E1) −˜p(E1) −(p(E2) −˜p(E2)))
= max
E⊆X | p(E) −˜p(E) |.
The lemma follows.
2

B.3 Distance Measures
337
Lemma B.21. Let (XW, pXW ) be a joint probability space (see Section B.1,
p. 327 and p. 328). Let pX be the induced distribution on X, and prob(w|x)
be the conditional probability of w, assuming x (x ∈X, w ∈Wx). Let ˜pX be
another distribution on X. Setting prob(x, w) := ˜pX(x) · prob(w|x), we get
another probability distribution ˜pXW on XW (see Section B.1). Then
dist(pXW , ˜pXW ) ≤dist(pX, ˜pX).
Proof. We have
|pXW (x, w)−˜pXW (x, w)| = |(pX(x)−˜pX(x))·prob(w|x)| ≤|pX(x)−˜pX(x)|,
and the lemma follows immediately from Deﬁnition B.19.
2
Throughout the book we consider families of sets, probability distributions
on these sets (often the uniform one) and events concerning maps and proba-
bilistic algorithms between these sets. The index sets J are partitioned index
sets with security parameter k, J = S
k∈N Jk, usually written as J = (Jk)k∈N
(see Deﬁnition 6.2). The indexes j ∈J are assumed to be binarily encoded,
and k is a measure for the binary length |j| of an index j. Recall that, by def-
inition, there is an m ∈N, with k1/m ≤|j| ≤km for j ∈Jk. As an example,
think of the family of RSA functions (see Chapters 3 and 6):
(RSAn,e : Z∗
n −→Z∗
n, x 7−→xe)(n,e)∈I,
where Ik = {(n, e) | n = pq, p ̸= q primes, |p| = |q| = k, e prime to ϕ(n)}.
We are often interested in asymptotic statements, i.e., statements holding for
suﬃciently large k.
Deﬁnition B.22. Let J = (Jk)k∈N be an index set with security parameter
k, and let (Xj)j∈J be a family of sets. Let p = (pj)j∈J and ˜p = (˜pj)j∈J be
families of probability distributions on (Xj)j∈J.
p and ˜p are called polynomially close,2 if for every positive polynomial P
there is a k0 ∈N, such that for all k ≥k0 and j ∈Jk
dist(pj, ˜pj) ≤
1
P(k).
Remarks:
1. “Polynomially close” deﬁnes an equivalence relation between distribu-
tions.
2. Polynomially close distributions cannot be distinguished by a statistical
test implemented as a probabilistic polynomial algorithm (see [Luby96],
Lecture 7). We do not consider probabilistic polynomial algorithms in
this appendix. Probabilistic polynomial statistical tests for pseudoran-
dom sequences are studied in Chapter 8 (see, e.g., Deﬁnition 8.2).
2 Also called (ε)-statistically indistinguishable (see, e.g., [Luby96]) or statistically
close (see, e.g., [Goldreich01]).

338
B. Probabilities and Information Theory
The following lemma gives an example of polynomially close distributions.
Lemma B.23. Let Jk := {n | n = rs, r, s primes, |r| = |s| = k, r ̸= s} 3 and
J := S
k∈N Jk. The distributions x
u←Zn and x
u←Z∗
n are polynomially close.
In other words, uniformly choosing any x from Zn is polynomially close to
choosing only units.
Proof. Let pn be the uniform distribution on Zn and let ˜pn be the distribution
x
u←Z∗
n. Then pn(x) = 1/n for all x ∈Zn, ˜pn(x) = 1/ϕ(n) if x ∈Z∗
n,
and ˜pn(x) = 0 if x ∈Zn \ Z∗
n. We have |Z∗
n| = ϕ(n) = n Q
p|n
p−1
p , where
the product is taken over the distinct primes p dividing n (Corollary A.30).
Hence,
dist(pn, ˜pn) = 1
2
X
x∈Zn
| pn(x) −˜pn(x) |
= 1
2

X
x∈Z∗n
µ
1
ϕ(n) −1
n
¶
+
X
x∈Zn\Z∗n
1
n

= 1 −ϕ(n)
n
= 1 −
Y
p|n
p −1
p
.
If n = rs ∈Jk, then
dist(pn, ˜pn) = 1 −(r −1)(s −1)
rs
= 1
r + 1
s −1
rs ≤2 ·
1
2k−1 =
1
2k−2 ,
and the lemma follows.
2
Remark. The lemma does not hold for arbitrary n, i.e., with
˜Jk := {n | |n| = k} instead of Jk. Namely, if n has a small prime factor q,
then 1 −Q
p|n
p−1
p
≥1 −q−1
q , which is not close to 0.
Example. The distributions x
u←Zn and x
u←Zn ∩Primes are not polynomi-
ally close.4 Their statistical distance is almost 1 (for large n).
Namely, let k = |n|, and let p1 and p2 be the uniform distributions on Zn
and Zn ∩Primes. As usual, we extend p2 by 0 to a distribution on Zn. The
number π(x) of primes ≤x is approximately x/ln(x) (Theorem A.68). Thus,
we get (with c = ln(2))
dist(p1, p2) = 1
2
X
x∈Zn
| p1(x) −p2(x) |
= 1
2
µ
π(n)
µ
1
π(n) −1
n
¶
+ (n −π(n)) 1
n
¶
=
µ
1 −π(n)
n
¶
≈1 −
1
ln(n)
= 1 −
1
c log2(n) ≥1 −
1
c(k −1),
3 If r ∈N, we denote, as usual, the binary length of r by |r|.
4 As always, we consider Zn as the set {0, . . . , n −1}.

B.3 Distance Measures
339
and see that the statistical distance is close to 1 for large k.
Lemma B.24. Let J = (Jk)k∈N be an index set with security parameter k,
and let (Xj)j∈J be a family of sets. Let p = (pj)j∈J and ˜p = (˜pj)j∈J be
families of probability distributions on (Xj)j∈J which are polynomially close.
Let (Ej)j∈J be a family of events Ej ⊆Xj.
Then for every positive polynomial P, there is a k0 ∈N such that for all
k ≥k0
|pj(Ej) −˜pj(Ej)| ≤
1
P(k),
for all j ∈Jk.
Proof. This is an immediate consequence of Lemma B.20.
2
Deﬁnition B.25. Let J = (Jk)k∈N be an index set with security parameter
k, and let (Xj)j∈J be a family of sets. Let p = (pj)j∈J and ˜p = (˜pj)j∈J be
families of probability distributions on (Xj)j∈J.
˜p is polynomially bounded by p if there is a positive polynomial Q such that
pj(x)Q(k) ≥˜pj(x) for all k ∈N, j ∈Jk and x ∈Xj.
Examples. In both examples, let J = (Jk)k∈N be an index set with security
parameter k:
1. Let (Xj)j∈J and (Yj)j∈J be families of sets with Yj ⊆Xj, j ∈J. Assume
there is a polynomial Q, such that |Yj|Q(k) ≥|Xj| for all k, j ∈Jk.
Then the image of the uniform distributions on (Yj)j∈J under the inclu-
sions Yj ⊆Xj is polynomially bounded by the uniform distributions on
(Xj)j∈J. This is obvious, since 1/|Yj| ≤Q(k)/|Xj| by assumption.
For example, x
u←Zn ∩Primes is polynomially bounded by x
u←Zn,
because the number of primes ≤n is of the order n/k, with k = |n| being
the binary length of n (by the Prime Number Theorem, Theorem A.68).
2. Let (Xj)j∈J and (Yj)j∈J be families of sets. Let f = (fj : Yj −→Xj)j∈J
be a family of surjective maps, and assume that for j ∈Jk, each x ∈Xj
has at most Q(k) pre-images, Q a polynomial. Then the image of the
uniform distributions on (Yj)j∈J under f is polynomially bounded by
the uniform distributions on (Xj)j∈J.
Namely, let pu be the uniform probability distribution and pf be the
image distribution. We have for j ∈Jk and x ∈Xj that
pf(x) ≤Q(k)
|Yj| ≤Q(k)
|Xj| = Q(k)pu(x).
Proposition B.26. Let J = (Jk)k∈N be an index set with security parameter
k. Let (Xj)j∈J be a family of sets. Let p = (pj)j∈J and ˜p = (˜pj)j∈J be
families of probability distributions on (Xj)j∈J. Assume that ˜p is polynomially
bounded by p. Let (Ej)j∈J be a family of events Ej ⊆Xj, whose probability
is negligible with respect to p; i.e., for every positive polynomial P there is a

340
B. Probabilities and Information Theory
k0 ∈N, such that pj(Ej) ≤1/P(k) for k ≥k0 and j ∈Jk.
Then the events (Ej)j∈J have negligible probability also with respect to ˜p.
Proof. There is a polynomial Q, such that ˜pj ≤Q(k) · pj for j ∈Jk. Now let
R be a positive polynomial. Then there is some k0, such that for k ≥k0 and
j ∈Jk
pj(Ej) ≤
1
R(k)Q(k) and hence ˜pj(Ej) ≤Q(k) · pj(Ej) ≤
1
R(k),
and the proposition follows.
2
B.4 Basic Concepts of Information Theory
Information theory and the classical notion of provable security for encryp-
tion algorithms go back to Shannon and his famous papers [Shannon48] and
[Shannon49].
We give a short introduction to some basic concepts and facts from infor-
mation theory that are needed in this book. Textbooks on the subject include
[Ash65], [Hamming86], [CovTho92] and [GolPeiSch94].
Changing the point of view from probability spaces to random variables
(see Section B.1), all the following deﬁnitions and results can be formulated
and are valid for (jointly distributed) random variables as well.
Deﬁnition B.27. Let X be a ﬁnite probability space. The entropy or un-
certainty of X is deﬁned by
H(X) :=
X
x∈X,prob(x)̸=0
prob(x) · log2
µ
1
prob(x)
¶
= −
X
x∈X,prob(x)̸=0
prob(x) · log2(prob(x)).
Remark. The probability space X models a random experiment. The possi-
ble outcomes of the experiments are the elements x ∈X. If we execute the
experiment and observe the event x, we gain information. The amount of
information we obtain with the occurrence of x (or, equivalently, our uncer-
tainty whether x will occur) – measured in bits – is given by
log2
µ
1
prob(x)
¶
= −log2 (prob(x)) .
The lower the probability of x, the higher the uncertainty. For example,
tossing a fair coin we have prob(heads) = prob(tails) = 1/2. Thus, the amount
of information obtained with the outcome heads (or tails) is 1 bit. If you

B.4 Basic Concepts of Information Theory
341
throw a fair die, each outcome has probability 1/6. Therefore, the amount of
information associated with each outcome is log2(6) ≈2.6 bits.
The entropy H(X) measures the average (i.e., the expected) amount of
information arising from executing the experiment X. For example, toss a
coin which is unfair, say prob(heads) = 3/4 and prob(tails) = 1/4. Then we
obtain log2(4/3) ≈0.4 bits of information with outcome heads, and 2 bits
of information with outcome tails; so the average amount of information
resulting from tossing this coin – the entropy – is 3/4 · log2(4/3) + 1/4 · 2 ≈0.8
bits.
Proposition B.28. Let X be a ﬁnite probability space which contains n el-
ements, X = {x1, . . . , xn}:
1. 0 ≤H(X) ≤log2(n).
2. H(X) = 0 if and only if there is some x ∈X with prob(x) = 1 (and
hence all other elements in X have a probability of 0).
3. H(X) = log2(n) if and only if the distribution on X is uniform.
For the proof of Proposition B.28, we need the following technical lemma.
Lemma B.29. Let (p1, . . . , pn) and (q1, . . . , qn) be probability distributions
(i.e., all pi and qj are ≥0 and Pn
i=1 pi = Pn
j=1 qj = 1, see Deﬁnition B.1).
Assume pk ̸= 0 and qk ̸= 0 for k = 1, . . . , n. Then:
1.
n
X
k=1
pk log2
µ 1
pk
¶
≤
n
X
k=1
pk log2
µ 1
qk
¶
.
(B.1)
2. Equality holds in (B.1) if and only if (p1, . . . , pn) = (q1, . . . , qn).
Proof. Since log2(x) = log2(e) · ln(x) and log2(e) > 0, it suﬃces to prove
the statements for ln instead of log2. We have ln(x) ≤x −1 for all x, and
ln(x) = x −1 if and only if x = 1. Therefore,
ln
µqk
pk
¶
≤qk
pk
−1, hence pk ln
µqk
pk
¶
≤qk −pk, hence
n
X
k=1
pk ln
µqk
pk
¶
≤
n
X
k=1
(qk −pk), and this implies
n
X
k=1
pk(ln(qk) −ln(pk)) ≤
n
X
k=1
qk −
n
X
k=1
pk = 0.
Obviously, we have equality if and only if qk/pk = 1 for k = 1, . . . , n, which
means qk = pk for k = 1, . . . , n.
2

342
B. Probabilities and Information Theory
Proof (of Proposition B.28). Since −prob(x) · log2(prob(x)) ≥0 for every
x ∈X, the ﬁrst inequality in statement 1, H(X) ≥0, and statement 2 are
immediate consequences of the deﬁnition of H(X).
To prove statements 1 and 3, set
pk := prob(xk), qk := 1
n, 1 ≤k ≤n.
Applying Lemma B.29 we get
n
X
k=1
pk log2
µ 1
pk
¶
≤
n
X
k=1
pk log2
µ 1
qk
¶
=
n
X
k=1
pk log2(n) = log2(n).
Equality holds instead of ≤if and only if
pk = qk = 1
n, k = 1, . . . , n,
by statement 2 of Lemma B.29.
2
In the following we assume without loss of generality that all elements of
probability spaces have a probability > 0.
We consider joint probability spaces XY and will see how to specify the
amount of information gathered about X when learning Y . We will often
use the intuitive notation prob(y|x), for x ∈X and y ∈Y . Recall that
prob(y|x) is the probability that y occurs as the second component, if the
ﬁrst component is x (see Section B.1, p. 328).
Deﬁnition B.30. Let X and Y be ﬁnite probability spaces with joint dis-
tribution XY .
The joint entropy H(XY ) of X and Y is the entropy of the joint distribution
XY of X and Y :
H(XY ) := −
X
x∈X,y∈Y
prob(x, y) · log2 (prob(x, y)) .
Conditioning X over y ∈Y , we deﬁne
H(X |y) := −
X
x∈X
prob(x|y) · log2(prob(x|y)).
The conditional entropy (or conditional uncertainty) of X assuming Y is
H(X |Y ) :=
X
y∈Y
prob(y) · H(X |y).
The mutual information of X and Y is the reduction of the uncertainty of X
when Y is learned:
I(X; Y ) = H(X) −H(X |Y ).

B.4 Basic Concepts of Information Theory
343
H(XY ) measures the average amount of information gathered by observing
both X and Y . H(X |Y ) measures the average amount of information arising
from (an execution of) the experiment X, knowing the result of experiment Y .
I(X; Y ) measures the amount of information about X obtained by learning
Y .
Proposition B.31. Let X and Y be ﬁnite probability spaces with joint dis-
tribution XY . Then:
1. H(X |Y ) ≥0.
2. H(XY ) = H(X) + H(Y |X).
3. H(XY ) ≤H(X) + H(Y ).
4. H(Y ) ≥H(Y |X).
5. I(X; Y ) = I(Y ; X) = H(X) + H(Y ) −H(XY ).
6. I(X; Y ) ≥0.
Proof. Statement 1 is true since H(X |y) ≥0 by Proposition B.28. The other
statements are a special case of the more general Proposition B.36.
2
Proposition B.32. Let X and Y be ﬁnite probability spaces with joint dis-
tribution XY . The following statements are equivalent:
1. X and Y are independent.
2. prob(y|x) = prob(y), for x ∈X and y ∈Y .
3. prob(x|y) = prob(x), for x ∈X and y ∈Y .
4. prob(x|y) = prob(x|y′), for x ∈X and y, y′ ∈Y .
5. H(XY ) = H(X) + H(Y ).
6. H(Y ) = H(Y |X).
7. I(X; Y ) = 0.
Proof. The equivalence of statements 1, 2 and 3 is an immediate consequence
of the deﬁnitions of independence and conditional probabilities (see Deﬁnition
B.6). Statement 3 obviously implies statement 4. Conversely, statement 3
follows from statement 4 using
prob(x) =
X
y∈Y
prob(y) · prob(x|y).
The equivalence of the latter statements follows as a special case from Propo-
sition B.37.
2
Next we study the mutual information of two probability spaces condi-
tioned on a third one.
Deﬁnition B.33. Let X, Y and Z be ﬁnite probability spaces with joint
distribution XYZ. The conditional mutual information I(X; Y |Z) is given
by
I(X; Y |Z) := H(X |Z) −H(X |Y Z).

344
B. Probabilities and Information Theory
The conditional mutual information I(X; Y |Z) is the average amount of
information about X obtained by learning Y , assuming that Z is known.
When studying entropies and mutual informations for jointly distributed
ﬁnite probability spaces X, Y and Z, it is sometimes useful to consider the
conditional situation where z ∈Z is ﬁxed. Therefore we need the following
deﬁnition.
Deﬁnition B.34. Let X, Y and Z be ﬁnite probability spaces with joint
distribution XYZ and z ∈Z.
H(X |Y, z) :=
X
y∈Y
prob(y|z) · H(X |y, z),
I(X; Y |z) := H(X |z) −H(X |Y, z).
(See Deﬁnition B.30 for the deﬁnition of H(X |y, z).)
Proposition B.35. Let X, Y and Z be ﬁnite probability spaces with joint
distribution XYZ and z ∈Z:
1. H(X |Y Z) = P
z∈Z prob(z) · H(X |Y, z).
2. I(X; Y |Z) = P
z∈Z prob(z) · I(X; Y |z).
Proof.
H(X |Y Z) =
X
y,z
prob(y, z) · H(X |y, z) (by Deﬁnition B.30)
=
X
z
prob(z)
X
y
prob(y|z) · H(X |y, z) =
X
z
prob(z) · H(X |Y, z).
This proves statement 1.
I(X; Y |Z) = H(X |Z) −H(X |Y Z)
=
X
z
prob(z) · H(X |z) −
X
z
prob(z) · H(X |Y, z) (by Def. B.30 and 1.)
=
X
z
prob(z) · I(X; Y |z).
This proves statement 2.
2
Proposition B.36. Let X, Y and Z be ﬁnite probability spaces with joint
distribution XYZ.
1. H(XY |Z) = H(X |Z) + H(Y |XZ).
2. H(XY |Z) ≤H(X |Z) + H(Y |Z).
3. H(Y |Z) ≥H(Y |XZ).
4. I(X; Y |Z) = I(Y ; X |Z) = H(X |Z) + H(Y |Z) −H(XY |Z).
5. I(X; Y |Z) ≥0.

B.4 Basic Concepts of Information Theory
345
6. I(X; Y Z) = I(X; Z) + I(X; Y |Z).
7. I(X; Y Z) ≥I(X; Z).
Remark. We get Proposition B.31 from Proposition B.36 by taking Z := {z0}
with prob(z0) := 1 and XYZ := XY × Z.
Proof.
1. We compute
H(X |Z) + H(Y |XZ)
= −
X
z∈Z
prob(z)
X
x∈X
prob(x|z) · log2(prob(x|z))
−
X
x∈X,z∈Z
prob(x, z)
X
y∈Y
prob(y|x, z) · log2(prob(y|x, z))
= −
X
x,y,z
prob(z)prob(x, y|z) · log2(prob(x|z))
−
X
x,y,z
prob(x, z)prob(y|x, z) · log2(prob(y|x, z))
= −
X
x,y,z
prob(x, y, z) ·
µ
log2(prob(x|z)) + log2
µprob(x, y|z)
prob(x|z)
¶¶
= −
X
x,y,z
prob(x, y, z) · log2(prob(x, y|z))
= −
X
z
prob(z)
X
x,y
prob(x, y|z) · log2(prob(x, y|z))
= H(XY |Z).
2. We have
H(X |Z) = −
X
z∈Z
prob(z)
X
x∈X
prob(x|z) · log2(prob(x|z))
= −
X
z∈Z
prob(z)
X
y∈Y
X
x∈X
prob(x, y|z) · log2(prob(x|z)),
H(Y |Z) = −
X
z∈Z
prob(z)
X
y∈Y
prob(y|z) · log2(prob(y|z))
= −
X
z∈Z
prob(z)
X
x∈X
X
y∈Y
prob(x, y|z) · log2(prob(y|z)).
Hence,
H(X |Z) + H(Y |Z)
= −
X
z∈Z
prob(z)
X
x,y
prob(x, y|z) · log2(prob(x|z)prob(y|z)).

346
B. Probabilities and Information Theory
By deﬁnition,
H(XY |Z) = −
X
z∈Z
prob(z)
X
x,y
prob(x, y|z) · log2(prob(x, y|z)).
Since (prob(x, y|z))(x,y)∈XY
and (prob(x|z) · prob(y|z))(x,y)∈XY
are
probability distributions, the inequality follows from Lemma B.29.
3. Follows from statements 1 and 2.
4. Follows from the deﬁnition of the mutual information, since H(X |Y Z) =
H(XY |Z) −H(Y |Z) by statement 1.
5. Follows from statements 2 and 4.
6. I(X; Z) + I(X; Y |Z) = H(X) −H(X |Z) + H(X |Z) −H(X |Y Z) =
I(X; Y Z).
7. Follows from statements 5 and 6.
The proof of Proposition B.36 is ﬁnished.
2
Proposition B.37. Let X, Y and Z be ﬁnite probability spaces with joint
distribution XYZ. The following statements are equivalent:
1. X and Y are independent assuming z, i.e.,
prob(x, y|z) = prob(x|z) · prob(y|z), for all (x, y, z) ∈XYZ.
2. H(XY |Z) = H(X |Z) + H(Y |Z).
3. H(Y |Z) = H(Y |XZ).
4. I(X; Y |Z) = 0.
Remark. We get the last three statements in Proposition B.32 from Proposi-
tion B.37 by taking Z := {z0} with prob(z0) := 1 and XYZ := XY × Z.
Proof. The equivalence of statements 1 and 2 follows from the computation
in the proof of Proposition B.36, statement 2, by Lemma B.29. The equiv-
alence of statements 2 and 3 is immediately derived from Proposition B.36,
statement 1. The equivalence of statements 2 and 4 follows from Proposition
B.36, statement 4.
2
Remark. All entropies considered above may in addition be conditioned on
some event E. We make use of this fact in our overview about some results
on “unconditional security” in Section 9.6. There, mutual information of the
form I(X; Y |Z, E) appears. E is an event in XYZ (i.e., a subset of XYZ) and
the mutual information is deﬁned as usual, but the conditional probabilities
assuming E have to be used. More precisely:
H(X |Y Z, E) := −
X
(x,y,z)∈E
prob(y, z|E)·prob(x|y, z, E)·log2(prob(x|y, z, E)),
H(X |Z, E) := −
X
(x,y,z)∈E
prob(z|E) · prob(x|z, E) · log2(prob(x|z, E)),

B.4 Basic Concepts of Information Theory
347
I(X; Y |Z, E) := H(X |Z, E) −H(X |Y Z, E).
Here recall that, for example, prob(y, z|E) = prob(X × {y} × {z}|E) and
prob(x|y, z, E) = prob ({x} × Y × Z |X × {y} × {z}, E) The results on en-
tropies from above remain valid if they are, in addition, conditioned on E.
The same proofs apply, but the conditional probabilities assuming E have to
be used.

References
Textbooks
[Ash65] R.B. Ash: Information Theory. New York: John Wiley & Sons, 1965.
[BalDiaGab95] J.L. Balc´azar, J. D´ıaz, J. Gabarr´o: Structural Complexity I. Berlin,
Heidelberg, New York: Springer-Verlag, 1995.
[Bauer07] F.L. Bauer: Decrypted Secrets – Methods and Maxims of Cryptology.
4th ed. Berlin, Heidelberg, New York: Springer-Verlag, 2007.
[Bauer96] H. Bauer: Probability Theory. Berlin: de Gruyter, 1996.
[BerPer85] J. Berstel, D. Perrin: Theory of Codes. Orlando: Academic Press, 1985.
[Buchmann2000] J.A. Buchmann: Introduction to Cryptography. Berlin, Heidel-
berg, New York: Springer-Verlag, 2000.
[Cohen95] H. Cohen: A Course in Computational Algebraic Number Theory.
Berlin, Heidelberg, New York: Springer-Verlag, 1995.
[CovTho92] T.M. Cover, J.A. Thomas: Elements of Information Theory. New York:
John Wiley & Sons, 1992.
[DaeRij02] J. Daemen, V. Rijmen: The Design of Rijndael – AES – The Advanced
Encryption Standard. Berlin, Heidelberg, New York: Springer-Verlag, 2002.
[Feller68] W. Feller: An Introduction to Probability Theory and Its Applications.
3rd ed. New York: John Wiley & Sons, 1968.
[Forster96] O. Forster: Algorithmische Zahlentheorie. Braunschweig, Wiesbaden:
Vieweg, 1996.
[GanYlv67] R.A. Gangolli, D. Ylvisaker: Discrete Probability. New York: Harcourt,
Brace & World, 1967.
[Goldreich99] O. Goldreich: Modern Cryptography, Probabilistic Proofs and Pseu-
dorandomness. Berlin, Heidelberg, New York: Springer-Verlag, 1999.
[Goldreich01] O. Goldreich: Foundations of Cryptography – Basic Tools. Cam-
bridge University Press, 2001.
[Goldreich04] O. Goldreich: Foundations of Cryptography. Volume II Basic Appli-
cations. Cambridge University Press, 2004.
[GolPeiSch94] S.W. Golomb, R.E. Peile, R.A. Scholtz: Basic Concepts in Informa-
tion Theory and Coding. New York: Plenum Press, 1994.
[Gordon97] H. Gordon: Discrete Probability. Berlin, Heidelberg, New York:
Springer-Verlag, 1997.
[Hamming86] R.W. Hamming: Coding and Information Theory. 2nd ed. Englewood
Cliﬀs, NJ: Prentice Hall, 1986.
[HarWri79] G.H. Hardy, E.M. Wright: An Introduction to the Theory of Numbers.
5th ed. Oxford: Oxford University Press, 1979.
[HopUll79] J. Hopcroft, J. Ullman: Introduction to Automata Theory, Languages
and Computation. Reading, MA: Addison-Wesley Publishing Company, 1979.
[IreRos82] K. Ireland, M.I. Rosen: A Classical Introduction to Modern Number
Theory. Berlin, Heidelberg, New York: Springer-Verlag, 1982.

350
References
[Kahn67] D. Kahn: The Codebreakers: The Story of Secret Writing. New York:
Macmillan Publishing Co. 1967.
[Knuth98] D.E. Knuth: The Art of Computer Programming. 3rd ed. Volume
2/Seminumerical Algorithms. Reading, MA: Addison-Wesley Publishing Com-
pany, 1998.
[Koblitz94] N. Koblitz: A Course in Number Theory and Cryptography. 2nd ed.
Berlin, Heidelberg, New York: Springer-Verlag, 1994.
[Lang05] S. Lang: Algebra. 3rd ed. Berlin, Heidelberg, New York: Springer-Verlag,
2005.
[Luby96] M. Luby: Pseudorandomness and Cryptographic Applications. Princeton,
NJ: Princeton University Press, 1996.
[MenOorVan96] A. Menezes, P.C. van Oorschot, S.A. Vanstone: Handbook of Ap-
plied Cryptography. Boca Raton, New York, London, Tokyo: CRC Press, 1996.
[MotRag95] R. Motwani, P. Raghavan: Randomized Algorithms. Cambridge, UK:
Cambridge University Press, 1995.
[Osen74] L.M. Osen: Women in Mathematics. Cambridge, MA: MIT, 1974.
[Papadimitriou94] C.H. Papadimitriou: Computational Complexity. Reading, MA:
Addison-Wesley Publishing Company, 1994.
[R´enyi70] A. R´enyi: Probability Theory. Amsterdam: North-Holland, 1970.
[Riesel94] H. Riesel: Prime Numbers and Computer Methods for Factorization.
Boston, Basel: Birkh¨auser, 1994.
[Rose94] H.E. Rose: A Course in Number Theory. 2nd ed. Oxford: Clarendon Press,
1994.
[Rosen2000] K.H. Rosen: Elementary Number Theory and Its Applications. 4th ed.
Reading, MA: Addison-Wesley Publishing Company, 2000.
[Salomaa90] A. Salomaa: Public-Key Cryptography. Berlin, Heidelberg, New York:
Springer-Verlag, 1990.
[Schneier96] B. Schneier: Applied Cryptography. New York: John Wiley & Sons,
1996.
[Simmons92] G.J. Simmons (ed.): Contemporary Cryptology. Piscataway, NJ:
IEEE Press, 1992.
[Stinson95] D.R. Stinson: Cryptography – Theory and Practice. Boca Raton, New
York, London, Tokyo: CRC Press, 1995.
Papers
[AleChoGolSch88] W.B. Alexi, B. Chor, O. Goldreich, C.P. Schnorr: RSA/Rabin
functions: certain parts are as hard as the whole. SIAM Journal on Computing,
17(2): 194–209, April 1988.
[AumDinRab02] Y. Aumann, Y.Z. Ding, M.O. Rabin: Everlasting security in the
bounded storage model. IEEE Transactions on Information Theory 48(6):1668-
1680, 2002.
[AumRab99] Y. Aumann, M.O. Rabin: Information-theoretically secure communi-
cation in the limited storage space model. Advances in Cryptology - CRYPTO
’99, Lecture Notes in Computer Science, 1666: 65–79, Springer-Verlag, 1999.
[Bach88] E. Bach: How to generate factored random numbers. SIAM Journal on
Computing, 17(2): 179–193, April 1988.
[BarPﬁ97] N. Bari´c, B. Pﬁtzmann: Collision-free accumulators and fail-stop sig-
nature schemes without trees. Advances in Cryptology - EUROCRYPT ’97,
Lecture Notes in Computer Science, 1233: 480–494, Springer-Verlag, 1997.
[BatHor62] P. Bateman, R. Horn: A heuristic formula concerning the distribution
of prime numbers. Mathematics of Computation, 16: 363–367, 1962.

References
351
[BatHor65] P. Bateman, R. Horn: Primes represented by irreducible polynomials
in one variable. Proc. Symp. Pure Math., 8: 119–135, 1965.
[BeGrGwH˚aKiMiRo88] M. Ben-Or, O. Goldreich, S. Goldwasser, J. H˚astad, J. Kil-
ian, S. Micali, P. Rogaway: Everything provable is provable in zero-knowledge.
Advances in Cryptology - CRYPTO ’88, Lecture Notes in Computer Science,
403: 37–56, Springer-Verlag, 1990.
[Bellare99] M. Bellare: Practice oriented provable security. Lectures on Data Secu-
rity. Lecture Notes in Computer Science, 1561: 1–15, Springer-Verlag, 1999.
[BelRog93] M. Bellare, P. Rogaway: Random oracles are practical: a paradigm for
designing eﬃcient protocols, Proc. First Annual Conf. Computer and Commu-
nications Security, ACM, New York, 1993:6273, 1993.
[BelRog94] M. Bellare, P. Rogaway: Optimal asymmetric encryption. Advances in
Cryptology - EUROCRYPT ’94, Lecture Notes in Computer Science, 950: 92–
111, Springer-Verlag, 1995.
[BelRog96] M. Bellare, P. Rogaway: The exact security of digital signatures, how
to sign with RSA and Rabin. Advances in Cryptology - EUROCRYPT ’96,
Lecture Notes in Computer Science, 1070: 399–416, Springer-Verlag, 1996.
[BelRog97] M. Bellare, P. Rogaway: Collision-resistant hashing: towards making
UOWHF practical. Advances in Cryptology - CRYPTO ’97, Lecture Notes in
Computer Science, 1294: 470–484, Springer-Verlag, 1997.
[Bleichenbacher96] D. Bleichenbacher: Generating ElGamal signatures without
knowing the secret key. Advances in Cryptology - EUROCRYPT ’96, Lecture
Notes in Computer Science, 1070: 10–18, Springer-Verlag, 1996.
[Bleichenbacher98] D. Bleichenbacher: A chosen ciphertext attack against protocols
based on the RSA encryption standard PKCS #1. Advances in Cryptology -
CRYPTO ’98, Lecture Notes in Computer Science, 1462: 1–12, Springer-Verlag,
1998.
[BluBluShu86] L. Blum, M. Blum, M. Shub: A simple unpredictable pseudorandom
number generator. SIAM Journal on Computing, 15(2): 364–383, 1986.
[BluGol85] M. Blum, S. Goldwasser: An eﬃcient probabilistic public-key encryp-
tion scheme which hides all partial information. Advances in Cryptology - Pro-
ceedings of CRYPTO ’84, Lecture Notes in Computer Science, 196: 289–299,
Springer-Verlag, 1985.
[Blum82] M. Blum: Coin ﬂipping by telephone: a protocol for solving impossible
problems. Proceedings of the 24th IEEE Computer Conference, San Francisco,
Calif., February 22–25, 1982: 133–137, 1982.
[Blum84] M. Blum: Independent unbiased coin ﬂips from a correlated biased source.
Proceedings of the IEEE 25th Annual Symposium on Foundations of Computer
Science, Singer Island, Fla., October 24–26, 1984: 425–433, 1984.
[BluMic84] M. Blum, S. Micali: How to generate cryptographically strong sequences
of pseudorandom bits. SIAM Journal on Computing, 13(4): 850–863, November
1984.
[Boer88] B. Den Boer: Diﬃe-Hellman is as strong as discrete log for certain primes.
Advances in Cryptology - CRYPTO ’88, Lecture Notes in Computer Science,
403: 530–539, Springer-Verlag, 1990.
[BoeBos93] B. den Boer, A. Bosselaers: Collisions for the compression function of
MD5. Advances in Cryptology - EUROCRYPT ’93, Lecture Notes in Computer
Science, 765: 293–304, Springer-Verlag, 1994.
[Boneh01] D. Boneh: Simpliﬁed OAEP for the RSA and Rabin functions. Advances
in Cryptology - CRYPTO 2001, Lecture Notes in Computer Science, 2139: 275–
291, Springer-Verlag, 2001.

352
References
[BonVen96] D. Boneh, R. Venkatesan: Hardness of computing the most signiﬁcant
bits of secret keys in Diﬃe-Hellman and related schemes. Advances in Cryp-
tology - CRYPTO ’96, Lecture Notes in Computer Science, 1109: 129–142,
Springer-Verlag, 1996.
[BonVen98] D. Boneh, R. Venkatesan: Breaking RSA may not be equivalent to
factoring. Advances in Cryptology - EUROCRYPT ’98, Lecture Notes in Com-
puter Science, 1403: 59–71, Springer-Verlag, 1998.
[Brands93] S. Brands: An eﬃcient oﬀ-line electronic cash system based on the
representation problem. Technical Report CS-R9323. Amsterdam, NL: Centrum
voor Wiskunde en Informatica (CWI), 1993.
[BraCre96] G. Brassard, C. Cr´epeau: 25 years of quantum cryptography. SIGACT
News 27(3): 13–24, 1996.
[CachMau97] C. Cachin, U.M. Maurer: Unconditional security against memory-
bounded adversaries. Advances in Cryptology - CRYPTO ’97, Lecture Notes in
Computer Science, 1294: 292–306, Springer-Verlag, 1997.
[CamMauSta96] J. Camenisch, U.M. Maurer, M. Stadler: Digital payment systems
with passive anonymity revoking trustees. Proceedings of ESORICS ’96, Lecture
Notes in Computer Science, 1146: 33–43, Springer-Verlag, 1996.
[CamWie92] K.W. Campbell, M.J. Wiener: DES is not a group. Advances in
Cryptology - CRYPTO ’92, Lecture Notes in Computer Science, 740: 512–520,
Springer-Verlag, 1993.
[CamPivSta94] J.L. Camenisch, J.M. Piveteau, M.A. Stadler: Blind signatures
based on the discrete logarithm problem. Advances in Cryptology - EURO-
CRYPT ’94, Lecture Notes in Computer Science, 950: 428–432, Springer-Verlag,
1995.
[CanGolHal98] R. Canetti, O. Goldreich, S. Halevi: The random oracle methodol-
ogy, revisited. STOC’98, Dallas, Texas: 209–218, New York, NY: ACM, 1998.
[CanGolHal04] R. Canetti, O. Goldreich, S. Halevi: On the random-oracle method-
ology as applied to length-restricted signature schemes. First Theory of Cryp-
tography Conference, TCC 2004, Lecture Notes in Computer Science, 2951:
40–57, Springer-Verlag, 2004.
[CarWeg79] J.L. Carter, M.N. Wegman: Universal classes of hash functions. Journal
of Computer and System Sciences, 18: 143–154, 1979.
[ChaPed92] D. Chaum, T. Pedersen: Wallet databases with observers. Advances in
Cryptology - CRYPTO ’92, Lecture Notes in Computer Science, 740: 89–105,
Springer-Verlag, 1993.
[Chaum82] D. Chaum: Blind signatures for untraceable payments. Advances in
Cryptology - Proceedings of CRYPTO ’82: 199–203, Plenum Press 1983.
[Coppersmith97] D. Coppersmith: Small solutions to polynomial equations and low
exponent RSA vulnerabilities. Journal of Cryptology, 10(4): 233–260, 1997.
[CraDam96] R. Cramer, I. Damg˚ard: New generation of secure and practical RSA-
based signatures. Advances in Cryptology - CRYPTO ’96, Lecture Notes in
Computer Science, 1109: 173–185, Springer-Verlag, 1996.
[CraFraSchYun96] R. Cramer, M.K. Franklin, B. Schoenmakers, M. Yung: Multi-
authority secret-ballot elections with linear work. Advances in Cryptology -
EUROCRYPT ’96, Lecture Notes in Computer Science, 1070: 72–83, Springer-
Verlag, 1996.
[CraGenSch97] R. Cramer, R. Gennaro, B. Schoenmakers: A secure and optimally
eﬃcient multi-authority election scheme. Advances in Cryptology - EURO-
CRYPT ’97, Lecture Notes in Computer Science, 1233: 103–118, Springer-
Verlag, 1997.
[CraSho98] R. Cramer, V. Shoup: A practical public key cryptosystem provably
secure against adaptive chosen ciphertext attack. Advances in Cryptology -

References
353
CRYPTO ’98, Lecture Notes in Computer Science, 1462: 13–25, Springer-
Verlag, 1998.
[CraSho2000] R. Cramer, V. Shoup: Signature schemes based on the strong RSA
assumption. ACM Transactions on Information and System Security, 3(3): 161–
185, 2000.
[Damg˚ard87] I.B. Damg˚ard: Collision-free hash functions and public-key signature
schemes. Advances in Cryptology - EUROCRYPT ’87, Lecture Notes in Com-
puter Science, 304: 203–216, Springer-Verlag, 1988.
[Diﬃe88] W. Diﬃe: The ﬁrst ten years of public key cryptology. In: G.J. Simmons
(ed.): Contemporary Cryptology, 135–175, Piscataway, NJ: IEEE Press, 1992.
[DifHel76] W. Diﬃe, M.E. Hellman: New directions in cryptography. IEEE Trans-
actions on Information Theory, IT-22: 644–654, 1976.
[DifHel77] W. Diﬃe, M. E. Hellman: Exhaustive cryptanalysis of the NBS data
encryption standard. Computer, 10: 74–84, 1977.
[Ding01] Y.Z. Ding: Provable everlasting security in the bounded storage model.
PhD Thesis, Cambridge, MA: Harvard University, May 2001.
[DinRab02] Y.Z. Ding, M.O. Rabin: Hyper-encryption and everlasting security.
19th Annual Symposium on Theoretical Aspects of Computer Science (STACS)
2002, Lecture Notes in Computer Science, 2285: 1–26, Springer-Verlag, 2002.
[Dobbertin96] H. Dobbertin: Welche Hash-Funktionen sind f¨ur digitale Signaturen
geeignet? In: P. Horster (ed.): Digitale Signaturen, 81–92, Braunschweig, Wies-
baden: Vieweg 1996.
[Dobbertin96a] H. Dobbertin: Cryptanalysis of MD5. Presented at the rump ses-
sion, Advances in Cryptology - EUROCRYPT ’96.
[DwoNao94] C. Dwork, M. Naor: An eﬃcient unforgeable signature scheme and its
applications. Advances in Cryptology - CRYPTO ’94, Lecture Notes in Com-
puter Science, 839: 234–246, Springer-Verlag, 1994.
[DziMau02] S. Dziembowski, U. Maurer: Tight security proofs for the bounded-
storage model. Proceedings of the 34th Annual ACM Symposium on Theory of
Computing, Montr´eal, Qu´ebec, Canada, May 19–21, 2002: 341–350, 2002.
[DziMau04a] S. Dziembowski, U. Maurer: Optimal randomizer eﬃciency in the
bounded-storage model. Journal of Cryptology, 17(1): 5–26, January 2004.
[DziMau04b] S. Dziembowski, U. Maurer: On generating the initial key in the
bounded-storage model. Advances in Cryptology - EUROCRYPT 2004, Lec-
ture Notes in Computer Science, 3027: 126–137, Springer-Verlag, 2004.
[ElGamal84] T. ElGamal: A public key cryptosystem and a signature scheme based
on discrete logarithms. Advances in Cryptology - Proceedings of CRYPTO ’84,
Lecture Notes in Computer Science, 196: 10–18, Springer-Verlag, 1985.
[FiaSha86] A. Fiat, A. Shamir: How to prove yourself: practical solutions to identiﬁ-
cation and signature problems. Advances in Cryptology - CRYPTO ’86, Lecture
Notes in Computer Science, 263: 186–194, Springer-Verlag, 1987.
[FIPS46 1977] FIPS46: Data Encryption Standard. Federal Information Processing
Standards Publication 46, U.S. Department of Commerce/National Bureau of
Standards, National Technical Information Service, Springﬁeld, Virginia, 1977.
[FIPS 113] FIPS 113: Computer data authentication. Federal Information Process-
ing Standards Publication 113, U.S. Department of Commerce/National Bureau
of Standards, http://www.itl.nist.gov/ﬁpspubs/, 1985.
[FIPS 180-2] FIPS 180-2: Secure hash signature standard. Federal Information Pro-
cessing Standards Publication 180-2, U.S. Department of Commerce/National
Bureau of Standards, http://www.itl.nist.gov/ﬁpspubs/, 2002.
[FIPS 198] FIPS 198: The keyed-hash message authentication code (HMAC). Fed-
eral Information Processing Standards Publication 198, U.S. Department of

354
References
Commerce/National Bureau of Standards, http://www.itl.nist.gov/ﬁpspubs/,
2002.
[FisSch2000] R. Fischlin, C.P. Schnorr: Stronger security proofs for RSA and Rabin
bits. Journal of Cryptology, 13(2): 221–244, 2000.
[FujOkaPoiSte2001] E. Fujisaki, T. Okamoto, D. Pointcheval, J. Stern: RSA-OAEP
is secure under the RSA assumption. Advances in Cryptology - CRYPTO 2001,
Lecture Notes in Computer Science, 2139: 260–274, Springer-Verlag, 2001.
[GenHalRab99] R. Gennaro, S. Halevi, T. Rabin: Secure hash-and-sign signatures
without the random oracle. Advances in Cryptology - EUROCRYPT ’99, Lec-
ture Notes in Computer Science, 1592: 123–139, Springer-Verlag, 1999.
[GenJarKraRab99] R. Gennaro, S. Jarecki, H. Krawczyk, T. Rabin: Secure dis-
tributed key generation for discrete-log based cryptosystems. Advances in Cryp-
tology - EUROCRYPT ’99, Lecture Notes in Computer Science, 1592: 295–310,
Springer-Verlag, 1999.
[Gill77] J. Gill: Computational complexity of probabilistic Turing machines. SIAM
Journal on Computing, 6(4): 675–695, December 1977.
[GolLev89] O. Goldreich, L. Levin: A hard-core predicate for all one-way functions.
Proceedings of the 21st Annual ACM Symposium on Theory of Computing,
Seattle, Wash., May 15–17, 1989: 25–32, 1989.
[GolMic84] S. Goldwasser, S. Micali: Probabilistic encryption. Journal of Computer
and System Sciences, 28(2): 270–299, 1984.
[GolMicRac89] S. Goldwasser, S. Micali, C. Rackoﬀ: The knowledge complexity of
interactive proof systems. SIAM Journal on Computing, 18: 185–208, 1989.
[GolMicRiv88] S. Goldwasser, S. Micali, R. Rivest: A digital signature scheme se-
cure against chosen message attacks. SIAM Journal on Computing, 17(2): 281–
308, 1988.
[GolMicTon82] S. Goldwasser, S. Micali, P. Tong: Why and how to establish a
private code on a public network. Proceedings of the IEEE 23rd Annual Sym-
posium on Foundations of Computer Science, Chicago, Ill., November 3–5, 1982:
134–144, 1982.
[GolMicWid86] O. Goldreich, S. Micali, A. Wigderson: Proofs that yield nothing
but their validity and a methodology of cryptographic protocol design. Pro-
ceedings of the IEEE 27th Annual Symposium on Foundations of Computer
Science, Toronto, October 27–29, 1986: 174–187, 1986.
[GolTau03] S. Goldwasser, Y. Tauman: On the (in)security of the Fiat-Shamir
paradigm. Proceedings of the IEEE 44th Annual Symposium on Foundations of
Computer Science, Cambridge, MA, USA, October 11–14, 2003: 102–113, 2003.
[GolTau03a] S. Goldwasser, Y. Tauman: On the (in)security of the Fiat-Shamir
paradigm. Cryptology ePrint Archive, http://eprint.iacr.org, Report 034, 2003.
[Gordon84] J.A. Gordon: Strong primes are easy to ﬁnd. Advances in Cryptology -
EUROCRYPT ’84, Lecture Notes in Computer Science, 209: 216–223, Springer-
Verlag, 1985.
[H˚asN¨as98] J. H˚astad, M. N¨aslund: The security of all RSA and Discrete Log bits.
Proceedings of the IEEE 39th Annual Symposium on Foundations of Computer
Science, Palo Alto, CA, November 8–11, 1998: 510–519, 1998.
[H˚asN¨as99] J. H˚astad, M. N¨aslund: The security of all RSA and Discrete Log bits.
Electronic Colloquium on Computational Complexity, http://eccc.hpi-web.de,
ECCC Report TR99-037, 1999.
[ISO/IEC 9594-8] ISO/IEC 9594-8: Information technology - Open Systems Inter-
connection - The Directory: Authentication framework. International Organi-
zation for Standardization, Geneva, Switzerland, http://www.iso.org, 1995.

References
355
[ISO/IEC 9797-1] ISO/IEC 9797-1: Message Authentication Codes (MACs) – Part
1: Mechanisms using a block cipher. International Organization for Standard-
ization, Geneva, Switzerland, http://www.iso.org, 1999.
[ISO/IEC 9797-2] ISO/IEC 9797-2: Message Authentication Codes (MACs) – Part
2: Mechanisms using a dedicated hash-function. International Organization for
Standardization, Geneva, Switzerland, http://www.iso.org, 2002.
[ISO/IEC 10116] ISO/IEC 10116: Information processing - Modes of operation for
an n-bit block cipher algorithm. International Organization for Standardization,
Geneva, Switzerland, http://www.iso.org, 1991.
[ISO/IEC 10118-2] ISO/IEC 10118-2: Information technology - Security techniques
- Hash-functions - Part 2: Hash-functions using an n-bit block cipher algo-
rithm. International Organization for Standardization, Geneva, Switzerland,
http://www.iso.org, 1994.
[Koblitz88] N. Koblitz: Primality of the number of points on an elliptic curve over
a ﬁnite ﬁeld. Paciﬁc Journal of Mathematics, 131(1): 157–165, 1988.
[KobMen05] N. Koblitz, A.J. Menezes: Another look at “provable security”. Jour-
nal of Cryptology, Online First: OF1–OF35, November 2005.
[Klima06] V. Klima: Tunnels in Hash Functions: MD5 Collisions Within a Minute.
Cryptology ePrint Archive, http://eprint.iacr.org, Report 105, 2006.
[LeeMooShaSha55] K. de Leeuw, E.F. Moore, C.E. Shannon, N. Shapiro: Com-
putability by probabilistic machines. In: C.E. Shannon, J. McCarthy (eds.):
Automata Studies, 183–212, Princeton, NJ: Princeton University Press, 1955.
[Lu02] C. Lu: Hyper-encryption against space-bounded adversaries from on-line
strong extractors. Advances in Cryptology - CRYPTO 2002, Lecture Notes in
Computer Science, 2442: 257–271, Springer-Verlag, 2002.
[MatMeyOse85] S.M. Matyas, C.H. Meyer, J. Oseas: Generating strong one way
functions with cryptographic algorithm. IBM Techn. Disclosure Bull., 27(10A),
1985.
[MatTakIma86] T. Matsumoto, Y. Takashima, H. Imai: On seeking smart public-
key-distribution systems. The Transactions of the IECE of Japan, E69: 99–106,
1986.
[Maurer92] U.M. Maurer: Conditionally-perfect secrecy and a provably-secure ran-
domized cipher. Journal of Cryptology, 5(1): 53–66, 1992.
[Maurer94] U.M. Maurer: Towards the equivalence of breaking the Diﬃe-Hellman
protocol and computing discrete logarithms. Advances in Cryptology -
CRYPTO ’94, Lecture Notes in Computer Science, 839: 271–281, Springer-
Verlag, 1994.
[Maurer95] U.M. Maurer: Fast generation of prime numbers and secure public-key
cryptographic parameters. Journal of Cryptology, 8: 123–155, 1995.
[Maurer97] U.M. Maurer: Information-theoretically secure secret-key agreement by
not authenticated public discussion. Advances in Cryptology - EUROCRYPT
’92, Lecture Notes in Computer Science, 658: 209–225, Springer-Verlag, 1993.
[Maurer99] U.M. Maurer: Information-theoretic cryptography. Advances in Cryp-
tology - CRYPTO ’99, Lecture Notes in Computer Science, 1666: 47–65,
Springer-Verlag, 1999.
[MauRenHol04] U. Maurer, R. Renner, C. Holenstein: Indiﬀerentiability, impossi-
bility results on reductions, and applications to the Random Oracle method-
ology. First Theory of Cryptography Conference, TCC 2004, Lecture Notes in
Computer Science, 2951: 21–39, Springer-Verlag, 2004.
[MauWol96] U.M. Maurer, S. Wolf: Diﬃe-Hellman oracles. Advances in Cryptology
- CRYPTO ’96, Lecture Notes in Computer Science, 1109: 268–282, Springer-
Verlag, 1996.

356
References
[MauWol97] U.M. Maurer, S. Wolf: Privacy ampliﬁcation secure against active ad-
versaries. Advances in Cryptology - CRYPTO ’96, Lecture Notes in Computer
Science, 1109: 307–321, Springer-Verlag, 1996.
[MauWol98] U.M. Maurer, S. Wolf: Diﬃe-Hellman, decision Diﬃe-Hellman, and
discrete logarithms. Proceedings of ISIT ’98, Cambridge, MA, August 16–21,
1998, IEEE Information Theory Society: 327, 1998.
[MauWol2000] U.M. Maurer, S. Wolf: The Diﬃe-Hellman protocol. Designs, Codes,
and Cryptography, Special Issue Public Key Cryptography, 19: 147–171, Kluwer
Academic Publishers, 2000.
[MicRacSlo88] S. Micali, C. Rackoﬀ, B. Sloan: The notion of security for proba-
bilistic cryptosystems. SIAM Journal on Computing, 17: 412–426, 1988.
[NaoYun89] M. Naor, M. Yung: Universal one-way hash functions and their cryp-
tographic applications. Proceedings of the 21st Annual ACM Symposium on
Theory of Computing, Seattle, Wash., May 15–17, 1989: 33–43, 1989.
[NaoYun90] M. Naor, M. Yung: Public-key cryptosystems provably secure against
chosen ciphertext attack. Proceedings of the 22nd Annual ACM Symposium on
Theory of Computing, Baltimore, MD, May 14–16, 1990: 427–437, 1990.
[NeeSch78] R.M. Needham, M.D. Schroeder: Using encryption for authentication in
large networks of computers. Communications of the ACM, 21: 993–999, 1978.
[von Neumann63] J. von Neumann: Various techniques for use in connection with
random digits. In: von Neumann’s Collected Works, 768–770. New York: Perg-
amon, 1963.
[NeuTs’o94] B.C. Neuman, T. Ts’o: Kerberos: an authentication service for com-
puter networks. IEEE Communications Magazine, 32: 33–38, 1994.
[Newman80] D.J. Newman: Simple analytic proof of the prime number theorem.
Am. Math. Monthly 87: 693–696, 1980.
[NIST94] National Institute of Standards and Technology, NIST FIPS PUB 186,
Digital Signature Standard, U.S. Department of Commerce, 1994.
[Okamoto92] T. Okamoto: Provably secure and practical identiﬁcation schemes and
corresponding signature schemes. Advances in Cryptology - CRYPTO ’92, Lec-
ture Notes in Computer Science, 740: 31–53, Springer-Verlag, 1993.
[OkaOht92] T. Okamoto, K. Ohta: Universal electronic cash. Advances in Cryptol-
ogy - CRYPTO ’91, Lecture Notes in Computer Science, 576: 324–337, Springer-
Verlag, 1992.
[OngSchSha84] H. Ong, C.P. Schnorr, A. Shamir: Eﬃcient signature schemes based
on quadratic equations. Advances in Cryptology - Proceedings of CRYPTO ’84,
Lecture Notes in Computer Science, 196: 37–46, Springer-Verlag, 1985.
[Pedersen91] T. Pedersen: A threshold cryptosystem without a trusted party. Ad-
vances in Cryptology - EUROCRYPT ’91, Lecture Notes in Computer Science,
547: 522–526, Springer-Verlag, 1991.
[Peralta92] R. Peralta: On the distribution of quadratic residues and nonresidues
modulo a prime number. Mathematics of Computation, 58(197): 433–440, 1992.
[Pﬁtzmann96] B. Pﬁtzmann: Digital signature schemes - general framework and
fail-stop signatures. Lecture Notes in Computer Science, 1100, Springer-Verlag,
1996.
[PohHel78] S.C. Pohlig, M.E. Hellman: An improved algorithm for computing log-
arithms over GF(p) and its cryptographic signiﬁcance. IEEE Transactions on
Information Theory, IT24: 106–110, January 1978.
[PoiSte2000] D. Pointcheval, J. Stern: Security arguments for digital signatures and
blind signatures. Journal of Cryptology, 13(3): 361–396, 2000.
[PolSch87] J.M. Pollard, C.P. Schnorr: An eﬃcient solution of the congruence x2 +
ky2 = m(modn). IEEE Transactions on Information Theory, 33(5): 702–709,
1987.

References
357
[Rabin63] M.O. Rabin: Probabilistic automata. Information and Control, 6: 230–
245, 1963.
[Rabin79] M.O. Rabin: Digitalized signatures and public key functions as in-
tractable as factorization. MIT/LCS/TR-212, MIT Laboratory for Computer
Science, 1979.
[RacSim91] C. Rackoﬀ, D.R. Simon: Non-interactive zero-knowledge proof of
knowledge and chosen ciphertext attack. Advances in Cryptology - CRYPTO
’91, Lecture Notes in Computer Science, 576: 433–444, Springer-Verlag, 1992.
[R´enyi61] A. R´enyi: On measures of entropy and information. Proc. 4th Berkeley
Symposium on Mathematical Statistics and Probability, vol. 1: 547–561, Berke-
ley: Univ. of Calif. Press, 1961.
[RFC 1510] J. Kohl, C. Neuman: The Kerberos network authentication service
(V5). Internet Request for Comments 1510 (RFC 1510), http://www.ietf.org,
1993.
[RFC 2104] H. Krawczyk, M. Bellare, R. Canetti: HMAC: Keyed-hashing for
message authentication. Internet Request for Comments 2104 (RFC 2104),
http://www.ietf.org, 1997.
[RFC 2246] The TLS protocol, Version 1.0. Internet Request for Comments 2246
(RFC 2246), http://www.ietf.org, 1999.
[RFC 2313] B. Kaliski: PKCS#1: RSA encryption, Version 1.5. Internet Request
for Comments 2313 (RFC 2313), http://www.ietf.org, 1998.
[RFC 2409] The Internet Key Exchange (IKE). Internet Request for Comments
2409 (RFC 2409), http://www.ietf.org, 1998.
[RFC 3174] US Secure Hash Algorithm 1 (SHA1). Internet Request for Comments
3174 (RFC 3174), http://www.ietf.org, 2001.
[RFC 3369] R. Housley: Cryptographic Message Syntax (CMS). Internet Request
for Comments 3369 (RFC 3369), http://www.ietf.org, 2002.
[RFC 3447] J. Jonsson, B. Kaliski: Public-Key Cryptography Standards (PKCS)
#1: RSA cryptography speciﬁcations, Version 2.1. Internet Request for Com-
ments 3447 (RFC 3447), http://www.ietf.org, 2003.
[RFC 4346] T. Dierks, E. Rescorla: The Transport Layer Security (TLS) pro-
tocol,
Version
1.1.
Internet
Request
for
Comments
4346
(RFC
4346),
http://www.ietf.org, 2006.
[Rivest90] R. Rivest: The MD4 message digest algorithm. Advances in Cryptology
- CRYPTO ’90, Lecture Notes in Computer Science, 537: 303–311, Springer-
Verlag, 1991.
[RivShaAdl78] R. Rivest, A. Shamir, and L.M. Adleman: A method for obtaining
digital signatures and public key cryptosystems. Communications of the ACM,
21(2): 120–126, 1978.
[RosSch62] J. Rosser, L. Schoenﬁeld: Approximate formulas for some functions of
prime numbers. Illinois J. Math. 6: 64–94, 1962.
[Santos69] E.S. Santos: Probabilistic Turing machines and computability. Proc.
Amer. Math. Soc. 22: 704–710, 1969.
[SchnAle84] C.P. Schnorr, W. Alexi: RSA-bits are 0.5 + epsilon secure. Advances
in Cryptology - EUROCRYPT ’84, Lecture Notes in Computer Science, 209:
113–126, Springer-Verlag, 1985.
[Shannon48] C.E. Shannon: A mathematical theory of communication. Bell Sys-
tems Journal, 27: 379–423, 623–656, 1948.
[Shannon49] C.E. Shannon: Communication theory of secrecy systems. Bell Sys-
tems Journal, 28: 656–715, 1949.
[Shor94] P.W. Shor: Algorithms for quantum computation: discrete log and fac-
toring. Proceedings of the IEEE 35th Annual Symposium on Foundations of

358
References
Computer Science, Santa Fe, New Mexico, November 20–22, 1994: 124–134,
1994.
[Shoup2001] V. Shoup: OAEP Reconsidered. Advances in Cryptology - CRYPTO
2001, Lecture Notes in Computer Science, 2139: 239–259, Springer-Verlag, 2001.
[Stinson92] D.R. Stinson: Universal hashing and authentication codes. Advances
in Cryptology - CRYPTO ’91, Lecture Notes in Computer Science, 576: 74–85,
Springer-Verlag, 1992.
[Vadhan03] S. Vadhan: On constructing locally computable extractors and cryp-
tosystems in the bounded storage model. Advances in Cryptology - CRYPTO
2003, Lecture Notes in Computer Science, 2729: 61–77, Springer-Verlag, 2003.
[Vazirani85] U.V. Vazirani: Towards a strong communication complexity, or gener-
ating quasi-random sequences from slightly random sources. Proceedings of the
17th Annual ACM Symposium on Theory of Computing, Providence, RI, May
6–8, 1985: 366–378, 1985.
[VazVaz84] U.V. Vazirani, V.V. Vazirani: Eﬃcient and secure pseudorandom num-
ber generation. Proceedings of the IEEE 25th Annual Symposium on Founda-
tions of Computer Science, Singer Island, Fla., October 24–26, 1984: 458–463,
1984.
[Vernam19] G.S. Vernam: Secret signaling system. U.S. Patent #1, 310, 719, 1919.
[Vernam26] G.S. Vernam: Cipher printing telegraph systems for secret wire and
radio telegraphic communications. Journal of American Institute for Electrical
Engineers, 45: 109–115, 1926.
[WaiPﬁ89] M. Waidner, B. Pﬁtzmann: The dining cryptographers in the disco:
unconditional sender and recipient untraceability with computationally secure
serviceability. Advances in Cryptology - EUROCRYPT ’89, Lecture Notes in
Computer Science, 434: 690, Springer-Verlag, 1990.
[WanFenLaiYu04] X. Wang, D. Feng, X. Lai, H. Yu: Collisions for hash functions
MD4, MD5, HAVAL-128, RIPEMD. Rump Session, Advances in Cryptology -
CRYPTO 2004. Cryptology ePrint Archive, http://eprint.iacr.org, Report 199,
2004.
[WatShiIma03] Y. Watanabe, J. Shikata, H. Imai: Equivalence between semantic
security and indistinguishability against chosen ciphertext attacks. Proceedings
Public Key Cryptography - PKC 2003, Lecture Notes in Computer Science,
2567: 71–84, Springer-Verlag, 2003.
[WegCar81] M.N. Wegman, J.L. Carter: New hash functions and their use in au-
thentication and set equality. Journal of Computer and System Sciences, 22:
265–279, 1981.
[Wiener90] M.J. Wiener: Cryptanalysis of short RSA secret exponents. IEEE
Transactions on Information Theory, 36: 553–558, 1990.
[Wolf98] S. Wolf: Unconditional security in cryptography. In: I. Damg˚ard (ed.):
Lectures on Data Security. Lecture Notes in Computer Science, 1561: 217–250,
Springer-Verlag, 1998.
Internet
[BelDesJokRog97] M. Bellare, A. Desai, E. Jokipii, P. Rogaway: A concrete security
treatment of symmetric encryption: analysis of the DES modes of operation.
http://www-cse.ucsd.edu/users/mihir/papers/sym-enc.html, 1997.
[GolBel01] S. Goldwasser, M. Bellare: Lecture notes on cryptography. http://www-
cse.ucsd.edu/users/mihir/papers/gb.html, 2001.
[DistributedNet] Distributed Net. http://www.distributed.net.
[NIST2000] National Institute of Standards and Technology. Advanced Encryption
algorithm (AES) development eﬀort. http://www.nist.gov/aes.

References
359
[RSALabs] RSA Laboratories. http://www.rsasecurity.com/rsalabs/.
[WanYinYu05] X. Wang, Y.L. Yin, H. Yu: Collision search attacks on SHA1.
Preprint, http://theory.csail.mit.edu/˜yiqun/shanote.pdf, February 2005.

Index
algorithm
– coin-tossing, 135
– deterministic, 135
– deterministic extension, 137
– distinguishing, 222, 226
– eﬃcient, 7
– Euclid, 36, 290, 306
– extended Euclid, 291, 306
– Las Vegas, 140
– Lucifer, 17
– Miller-Rabin, 323
– Monte Carlo, 140
– oracle, 177
– polynomial algorithm, 139
– probabilistic, see probabilistic
algorithm
– randomized encryption, 27, 216
– Rijndael, 20
– Solovay-Strassen, 323
– sampling, 155
admissible key generator, 166, 201
advanced encryption standard, 19
advantage distillation, 256
AES, see advanced encryption standard
aﬃne cipher, 261
assumptions
– factoring, 161
– decision Diﬃe-Hellman, 130, 245
– Diﬃe-Hellman, 85
– discrete logarithm, 150
– quadratic residuosity, 163
– RSA, 159
– strong RSA, 274
attack
– adaptively-chosen-ciphertext, 5, 49,
234
– adaptively-chosen-message, 266
– adaptively-chosen-plaintext, 5
– birthday, 58
– Bleichenbacher’s 1-Million-Chosen-
Ciphertext, 48
– chosen-message, 266
– chosen-plaintext, 5
– ciphertext-only, 5
– common-modulus, 46
– against encryption, 4
– encryption and signing with RSA, 48
– key-only, 266
– known-plaintext, 5
– known-signature, 266
– low-encryption-exponent, 47
– partial chosen-ciphertext, 49
– replay, 82
– RSA, 46
– small-message-space, 47
– against signatures, 265
attacks and levels of security, 265
authentication, 2
big-O, 293
binary
– encoding of x ∈Z, 293
– encoding of x ∈Zn, 295
– random variable, 326
binding property, 101, 103
bit-security
– Exp family, 175
– RSA family, 182
– Square family, 190
Bleichenbacher’s 1-Million-Chosen-
Ciphertext Attack, 48
blind signatures
– Nyberg-Rueppel, 133
– RSA, 132
– Schnorr, 120
blindly issued proofs, 117
block cipher, 15
Blum-Blum-Shub generator, 203, 213
Blum-Goldwasser probabilistic
encryption, 230
Blum-Micali generator, 202
Boolean predicate, 148, 326

362
Index
Caesar’s shift cipher, 1
cardinality, 296
Carmichael number, 320
certiﬁcate, 89
certiﬁcation authority, 89
challenge-response, 6, 82, 87
Chernoﬀbound, 145
Chebyshev’s Inequality, 334
cipher-block chaining, 26
cipher feedback, 27
ciphertext, 1, 12
ciphertext-indistinguishable, 227, 235
claw, 268
claw-free pair of one-way permutations,
268
coin, see electronic coin
coin tossing by telephone, 100
collision, 55
– entropy, 257
– resistant hash function, see hash
function
– probability, 257
commitment, 100
– binding property, 101
– discrete logarithm, 102
– hiding property, 101
– quadratic residue, 101
– homomorphic, 103
completeness, 92, 131
composite, 36, 294
compression function, 56, 60
complexity class
– BPP, 144
– IP, 131
– NP, 144
– RP, 144
– ZPP, 145
computationally perfect pseudorandom
generator, 200
conditional
– entropy, 342
– mutual information, 343
– probability, 327
– uncertainty, 342
conﬁdentiality, 1
congruent modulo n, 37, 295
creation of a certiﬁcate, 90
cryptanalysis, 4
cryptogram, 12
cryptographic protocol, 5, 81, see also
protocol
cryptographically secure, 200
cryptography, 1
cryptology, 4
cyclic group, 302
data encryption standard, 16
data integrity, 2
data origin authentication, 3
decision Diﬃe-Hellman problem, 129,
245
decryption, 1
– key, 1
DES, see data encryption standard
deterministic extension, 137
Diﬃe-Hellman
– key agreement, 85
– problem, 71, 85
digital cash, 115
– coin and owner tracing, 124, 127
– customer anonymity, 125
– deposit, 127
– electronic coin, 115
– fair payment systems, 116, 123
– oﬄine, 125
– online, 124
– opening an account, 124
– owner tracing, 124
– payment and deposit protocol, 124
– payment protocol, 126
– security, 127
– withdrawal protocol, 124, 125
digital
– ﬁngerprint, 62
– digital signature algorithm, 73
– signature scheme, 265
– digital signature standard, 73
– signatures, 3, see also signatures
direct product, 327
discrete
– exponential function, 39, 149
– exponential generator, 202
– logarithm function, 39, 150
distance measures, 336
division with remainder, 36, 289
divisor, 36
DSA, see digital signature algorithm
DSS, see digital signature standard
ElGamal’s encryption, 70
ElGamal’s Signature Scheme, 72
electronic cash, see digital cash
electronic codebook mode, 26
electronic election, 107
– authority’s proof, 110
– communication model, 107

Index
363
– decryption, 108
– multi-way, 112
– setting up the scheme, 107
– tally computing, 109
– trusted center, 113
– vote casting, 108
– vote duplication, 132
– voter’s proof, 112
encryption, see also public-key
encryption
– ciphertext-indistinguishable, 227, 235
– Cramer-Shoup, 245
– methods, 1
– OAEP, 51
– perfectly secret, 217
– provably secure, 215
– randomized, 216
– SAEP, 236
– symmetric, 2
– symmetric-key, 11
– Vernam’s one-time pad, 7, 13, 216
entity authentication, 3, 81, 82, 86
entropy, 340
– conditional, 342
– entropy smoothing, 256
– joint entropy, 342
– R´enyi entropy, 257
– smoothing entropy theorem, 258
existential forger, 267
existentially forged, 45
Exp family, 150
fail-stop signature schemes, 268
family of
– claw-free one-way permutations, 268
– hard-core predicates, 167
– one-way functions, 163
– one-way permutations, 164
– trapdoor functions, 165
fast modular exponentiation, 298
feasible, 7
Feistel cipher, 17
Fibonacci numbers, 292
ﬁeld
– ﬁnite, 304
– prime, 305
forward secrecy, 88
function
– compression, 56, 60
– discrete exponential, 39, 149
– discrete logarithm, 39, 150
– Euler phi or totient, 38, 296
– hash, see hash function
– one-way, see one-way function
– polynomial function, 200
– RSA, 39, 158
– square, 40, 161, 162
– stretch function, 200
Galois ﬁeld, 309
geometric random variable, 332
GMR signature scheme, 272
Golden Ratio, 292
Goldwasser-Micali probabilistic
encryption, 226
greatest common divisor, 289, 305
hard-core predicate, 167, 212
hash function, 54
– birthday attack, 58
– construction, 56
– collision resistant, free, 55, 268, 269
– compression function, 56
– real hash functions, 60
– digital ﬁngerprint, 62
– second pre-image resistant, 55
– strongly collision resistant, 55
– strongly universal class, 258
– universal class, 258
– universal one-way family, 277
– weakly collision resistant, 55
hash-then-decrypt paradigm, 65
hiding property, 101, 103
HMAC, 63
homomorphism, 299
honest-veriﬁer zero-knowledge, 111
identiﬁcation schemes, 91
image distribution, 326
independent events, 327
index set with security parameter, 152
information
– reconciliation, 260
– theory, 340
– theoretic security, 216
inner product bit, 167
integer quotient, 290
integers modulo n, 37
interactive proof
– completeness, 92, 131
– of knowledge, 91
– move, 91, 93, 94
– prover, 91, 92
– prover’s secret, 91
– rounds, 91
– soundness, 92, 131
– system, 91

364
Index
– system for a language, 131
– veriﬁer, 91, 92
– zero-knowledge, 95, 131
isomorphism, 299
iterated cipher, 17
Jacobi symbol, 312
Kerberos, 82
– authentication protocol, 83
– authentication server, 82
– credential, 83
– ticket, 83
Kerkhoﬀ’s Principle, 4
key, 12
– exchange, 81, 86
– generator, 163, 200
– set with security parameter, 152
– stream, 12
knowledge
– completeness, 92
– soundness, 92
– zero-knowledge, 95, 131
Lagrange interpolation formula, 106
Legendre symbol, 311
bounded storage model, 251
Log family, 150
master keys, 81
MD5, 61
Merkle’s meta method, 56
Merkle-Damg˚ard construction, 56
message, 1
message authentication code, 3, 62
message digest, 62
modes of operation, 25
– cipher-block chaining, 26
– cipher feedback, 27
– electronic codebook mode, 26
– output feedback mode, 28
modiﬁcation detection codes, 62
modular
– arithmetic, 35
– powers, 39, 158
– Square Roots, 315
– Squares, 40
– Squaring, 76, 161
multiplicative inverse, 296
mutual authentication, 86
mutual information, 342
natural representatives, 295
negligible, 151
next-bit predictor, 207
next-bit test, 207
NIST, 19
noisy channel model, 260
non-interactive proofs of knowledge,
112
non-repudiation, 3
Nyberg-Rueppel signatures, 133
OAEP, 51
one-way function, 2, 34, 147, 150, 163
– bijective one-way function, 164
– bit security of, 175
– deﬁnition of, 163
– Exp family, 150
– pseudorandomness, 199
– RSA family, 158
– Square family, 162
– weak one-way function, 172
one-way permutation, 159, 164
order of x, 301
order of a set, 296
output feedback mode, 28
password scheme, 92
perfect threshold scheme, 107
perfectly secret, 7, 217
permutation, 15
plaintext, 1, 12
polynomial
– degree, 305
– irreducible, 306
– prime to, 305
– reducible, 306
polynomial-time indistinguishability, 8,
228
polynomial-time reduction, 151
polynomially bounded, 339
polynomially close, 337
positive polynomial, 141
prime number, 36, 294
prime residue class group, 38, 296
prime to, 36, 289
primality test, 319
primitive element, 39
primitive root, 302
principal square root, 176
privacy ampliﬁcation, 256
probabilistic algorithm, 135
– coin-tossing algorithm, 135
– distinguishing algorithm, 222, 226
– Las Vegas, 140
– Miller-Rabin primality test, 323

Index
365
– Monte Carlo, 140
– polynomial algorithm, 139
– Solovay-Strassen primality test, 323
probabilistic
– public-key encryption, 226
– primality test, 323
– signature scheme, 69
probability, 325
– distribution, 325
– image distribution, 326
– independent events, 327,
– joint space, 327
– measure, 325
– notation for, 148, 329
– of success, 221, 267
– space, 325
proof of knowledge, see also interactive
proof
– interactive, 91
– non-interactive, 112
– of a logarithm, 117
– of a representation, 133
– of the equality of two logarithms, 110
– of a square root, 93, 97
protocol,
– Diﬃe-Hellman key agreement, 85
– digital cash, 123
– electronic elections, 107
– Fiat-Shamir identiﬁcation, 93, 97
– Kerberos authentication, 83
– Schnorr’s identiﬁcation, 93, 117
– station-to-station, 88
– strong three-way authentication, 87
– two-way authentication protocol, 87
provable security, 6
provably secure digital signature, 265
provably secure encryption, 215
pseudorandom
– function model, 29
– one-time pad induced by G, 224
– permutation model, 29
pseudorandom bit generator, 200
– Blum-Blum-Shub generator, 203, 213
– Blum-Micali generator, 202
– discrete exponential generator, 202
– induced by f, B and Q, 202
– RSA generator, 202
– x2 mod n generator, 203
PSS, see probabilistic signature scheme
public-key, 33
– cryptography, 2, 33
– management techniques, 89
public-key encryption, 2, 33
– asymmetric encryption, 2
– Blum-Goldwasser probabilistic
encryption, 230
– ciphertext-indistinguishable, 227
– ElGamal’s encryption, 70
– OAEP, 51
– probabilistic public-key encryption,
226
– provably secure encryption, 215
– public key one-time pad, 225
– randomized encryption, 216
– Rabin’s encryption, 76
– RSA, 41
quadratic
– non-residue, 131, 310
– residue, 311
quadratic residuosity
– family, 163
– property, 162
Rabin’s encryption, 76
Rabin’s signature scheme, 77
random
– function, 9
– oracle model, 66, 236, 244
– self-reducible, 154
random variable, 326
– binary, 326
– distribution, 326
– expected value, 326
– geometric, 332
– independent, 328
– jointly distributed, 328
– variance, 333
– real-valued, 326
randomized algorithm, see probabilistic
algorithm
rational approximation, 183
real-valued random variable, 326
relatively prime to, 36, 289
representation problem, 128
residue, 295
– residue class ring, 295
– residue class, 295
retrieval of secret keys, 266
revocation of a certiﬁcate, 90
Rijndael cipher, 19
RIPEMD-160, 61
RSA, 41
– assumption, 159
– attacks, 46
– attack on encryption and signing, 48

366
Index
– encryption and decryption, 42
– encryption and decryption exponent,
42
– existentially forged, 45
– digital signatures, 45
– family, 158
– generator, 202
– function, 39
– key generation, 41, 160
– low-encryption-exponent attack, 47
– probabilistic encryption, 51
– security, 43
– speeding up encryption and
decryption, 45
SAEP, 236
sampling algorithm, 155
Schnorr’s blind signature, 120
Schnorr’s identiﬁcation protocol, 117
second pre-image resistance, 55
secret key, 33
secret key encryption, see encryption
secret sharing, 105
secure
– against adaptively-chosen-ciphertext
attacks, 235
– against adaptively-chosen-message
attacks, 268
– bit, 175
– cryptographically, 200
– indistinguishability-secure, 235
security
– everlasting, 253
– information-theoretic, 216
– in the random oracle model, 66, 236
– levels of, 265
– modes of operation, 29
– proof by reduction, 67
– RSA, 43
– unconditional, 250
– under standard assumptions, 245
seed, 199
selective forgery, 266
semantically secure, 228
session keys, 81
SHA-1, 61
Shamir’s threshold scheme, 105
signature
– authentication-tree-based signatures,
271
– blind, see blind signature
– Cramer-Shoup signature scheme, 275
– Digital Signature Algorithm, 73
– ElGamal’s signature scheme, 72
– fail-stop, 268
– Fiat-Shamir, 99
– full-domain-hash RSA, 66
– GMR signature scheme, 272
– one-time signature scheme, 271, 286
– probabilistic, see probabilistic
signature scheme
– provably secure digital, 265
– Rabin’s signature scheme, 77
– RSA signature scheme, 45
– signed message, 45
– state-free, 273
simulator, 95
simultaneously secure bits, 212
single-length MDCs, 60
soundness, 92, 131
Sophie Germain prime, 273
Square family, 162
square root, 40, 315
squares, 311
statistical distance, 336
statistically close, 337
stream ciphers, 12
strong primes, 44
Theorem
– Blum and Micali, 206
– Chinese Remainder, 299
– Fermat’s Little Theorem, 38, 297
– Fundamental Theorem of Arithmetic,
37
– Prime Number, 319
– Shannon, 218
– Yao, 207
threshold scheme, 105
tight reduction, 240
transcript, 95
trapdoor
– function, 34, 147, 165
– information, 34, 159
– permutation, 159
uncertainty, 340
uniform distribution, 325
uniform sampling algorithm, 156
unit, 296
universal forgery, 266
variance, 333
Vernam’s one-time pad, 7, 13, 216
virtually uniform, 155
voting, see electronic election

Index
367
weak law of large numbers, 334
witness, 323
zero divisor, 38
zero-knowledge, 95, 131
– honest-veriﬁer, 111

