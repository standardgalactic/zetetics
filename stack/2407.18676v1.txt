Right Now, Wrong Then: Non-Stationary Direct Preference
Optimization under Preference Drift
Seongho Son ∗†
William Bankes ∗†
Sayak Ray Chowdhury ‡
Brooks Paige †
Ilija Bogunovic†
July 29, 2024
Abstract
Reinforcement learning from human feedback (RLHF) aligns Large Language Models (LLMs) with
human preferences.
However, these preferences can often change over time due to external factors
(e.g. environment change and societal influence) – consequently, what was wrong then might be right
now. Current preference optimization algorithms do not account for temporal preference drift in their
modeling, which can lead to severe misalignment. To address this limitation, we use a Dynamic Bradley-
Terry model that models preferences via time-dependent reward functions, and propose Non-Stationary
Direct Preference Optimisation (NS-DPO). By introducing a discount parameter in the loss function,
NS-DPO applies exponential weighting, which proportionally focuses learning on more time-relevant
datapoints. We theoretically analyse the convergence of NS-DPO in the offline setting, providing upper
bounds on the estimation error caused by non-stationary preferences.
Finally, we demonstrate the
effectiveness of NS-DPO1 for fine-tuning LLMs in scenarios with drifting preferences. By simulating
preference drift using renowned reward models and modifying popular LLM datasets accordingly, we show
that NS-DPO fine-tuned LLMs remain robust under non-stationarity, significantly outperforming baseline
algorithms that ignore temporal preference changes, without sacrificing performance in stationary cases.
1
Introduction
The application of Reinforcement Learning from Human Feedback (RLHF) to fine-tune Large Language
Models (LLMs) (Christiano et al., 2017; Stiennon et al., 2020; Ziegler et al., 2019; Ouyang et al., 2022;
Bai et al., 2022b) has lead to more precise control over the behaviour they exhibit. This control is crucial
when looking to safely deploy models in the real world (Amodei et al., 2016; Hendrycks and Mazeika, 2022).
Human preference datasets enable the training of proxy reward models (see, e.g., RewardBench (Lambert
et al., 2024)) that can accurately evaluate complex human behaviour. These proxy reward models are used in
conjunction with RL to fine-tune the LLM. Recent works (Rafailov et al., 2024; Azar et al., 2024; Amini et al.,
2024; Swamy et al., 2024; Rosset et al., 2024; Ethayarajh et al., 2024; Hong et al., 2024) seek to improve the
efficiency and stability of these approaches (Chaudhari et al., 2024) by training the LLM straight from the
human preference data, avoiding the need to learn a proxy reward model.
A key assumption made in these contemporary preference optimization approaches is that human preferences
are stationary, i.e., they do not change over time. However, a sudden or gradual shift in preferences can
occur due to new information becoming available (Zafari et al., 2019; Johnson and Mayorga, 2020), changes
in the demographics of the queried audience (Caldwell, 1981), or due to social influences and cultural trends.
As more preference datasets are gathered over long periods of time, the chance of the data containing varying
preferences increases. In such cases, algorithms that do not account for these changes, view them as noise
and treat outdated data as equally important as fresh data, often leading to deteriorated performance. An
∗Equal Contribution, correspondence to seong.son.22@ucl.ac.uk and william.bankes.21@ucl.ac.uk
†University College London
‡Microsoft Research, India
1https://github.com/geronest/ns-dpo
1
arXiv:2407.18676v1  [cs.LG]  26 Jul 2024

Are GPUs necessary 
for AI research?
Year 2011
"Not really. CPUs are more
flexible and accessible
compared to GPUs."
Prompt
Preferred
response
...
How are GPUs considered
in the AI community?
"GPUs play a crucial role for
deep learning, due to their
special design."
Will GPUs particularly 
be in high demand?
"Because its design suits deep
learning research, the demand
for GPUs will keep rising."
"We cannot be sure about this,
as CPUs still have their own
advantages over GPUs."
Year 2023
Year 2024
Farther: less relevant
Recent: more relevant
Gradient updates weighted by time distance
Uniformly weighted gradient updates
Training
Test
Pref. optimization
DPO / IPO
NS-DPO (Non-Stationary)
Dynamic Bradley-Terry Model
Predicted
preference
Figure 1: Human preferences are dynamic and influenced by a variety of factors (e.g. environment change
and societal influence). However, standard preference optimization approaches (e.g., DPO and IPO (Rafailov
et al., 2024; Azar et al., 2024)) do not account for this non-stationarity. In contrast, NS-DPO robustly learns
on non-stationary data by using a Dynamic Bradley-Terry model, and adjusts the loss to discount older
datapoints and concentrate learning on the latest data.
increasing body of evidence (Zhou et al., 2024; Chen et al., 2024a) points to data quality as being a key
factor in fine-tuning performance, thus preference drift can greatly affect the alignment of models which do
not account for it (Carroll et al., 2024). The development of preference optimization algorithms and theory
to handle preference drifts are therefore crucial.
In this work, we propose Non-Stationary Direct Preference Optimization (NS-DPO), a novel approach that
uses a probabilistic Dynamic Bradley-Terry model (Cattelan et al., 2013; Bong et al., 2020; Tian et al., 2023)
to account for non-stationary drift in human preferences. NS-DPO re-weights each training datapoint by
appropriately down-weighting older data with potentially stale preferences and up-weighting more recent
ones. We empirically show the effectiveness and robustness of NS-DPO compared to stationary approaches,
using both synthetic experiments and datasets commonly used for fine-tuning LLMs. Our overall approach
is summarised in Figure 1.
Related work. One of the primary applications of the RLHF framework is fine-tuning large language models
(LLMs) (Christiano et al., 2017; Stiennon et al., 2020; Ziegler et al., 2019; Ouyang et al., 2022; Bai et al.,
2022b) using the Bradley-Terry model (Bradley and Terry, 1952) as a preference model. Rafailov et al. (2024)
propose Direct Preference Optimization (DPO), which directly fine-tunes an LLM from a human preference
dataset. This approach skips the stage of training a separate reward model, while achieving stable and
comparable performance to PPO-based RLHF methods. A variety of alternatives to DPO have been proposed
including Azar et al. (2024), which address the shortfalls of modelling preferences with rewards. Amini et al.
(2024); Meng et al. (2024) adjust the DPO objective with an offset to include the extent to which one response
is preferred over another, whilst Cen et al. (2024) learn an offset to express uncertainty in the reward model.
Ethayarajh et al. (2024) remove paired preferences all together and propose maximising a utility function.
Several approaches examine preference optimisation from a game theory perspective, where the current policy
plays against previous versions to further improve performance (Swamy et al., 2024; Rosset et al., 2024; Wu
et al., 2024b; Yuan et al., 2024; Chen et al., 2024b; Pang et al., 2024; Munos et al., 2024). Xu et al. (2023)
propose a cringe loss based objective whilst Hong et al. (2024); Pentyala et al. (2024); Hua et al. (2024) try
to combine the supervised fine-tuning and preference optimization steps. Hong et al. (2024); Hua et al. (2024)
propose a single training objective to do this and Pentyala et al. (2024) examine combining two different
models trained on an SFT and direct preference objective respectively. Finally, Lu et al. (2024) propose a
meta algorithm which uses an LLM to optimize the form of the direct preference learning objective itself.
A prominant direction of work is the online setting (Qi et al., 2024; Zhang et al., 2024; Guo et al., 2024; Xie
et al., 2024), where feedback is returned by a human labeler or superior model. Khaki et al. (2024); Liu et al.
(2024) adapt the offline settings using techniques such as rejection sampling to approximate an online setting.
In this work we only consider the offline setting for simplicity, however the approach we propose can easily be
adapted to the online setting. Other important directions of research include safety and robustness. Dai et al.
2

(2024); Ramesh et al. (2024); Wu et al. (2024a) consider robust settings where safety or group information
is known at training time and Dai et al. (2024) analyse a constrained optimization problem through the lens
of safety in LLMs. Whilst these approaches look to address a wide range of settings, our work is the first
to provide a solution to the case of non-stationary preferences.
Carroll et al. (2024) consider how to correctly align LLMs under preference drift, showing several possible
goals for alignment in an online setting. Whilst in the online non-stationary setting the LLM can adapt to
the changing preferences of the user, our setting considers aligning the model on an offline dataset before
deploying the static model to users at test time. As such our approach is most similar to the Privileged
Reward and Initial Reward settings Carroll et al. (2024) proposes, as we determine that the preferences
exhibited in the present are the most important (Privileged Reward) and future users will interact with a
model aligned to preferences from their past (Initial Reward).
A variety of work has also analysed the RLHF problem from a theoretical standpoint. Xiong et al. (2024)
provide suboptimiality bounds of policies in the offline, online and hybrid settings under linear rewards. They
do not directly analyse the performance of DPO but propose it as a practical implementation of the oracle
central to their analysis. Zhu et al. (2023); Chowdhury et al. (2024) analyse the offline preference learning and
DPO settings, respectively. Chowdhury et al. (2024) address noisy preferences with a modified version of the
DPO algorithm, presenting confidence bounds for neural policy classes and suboptimality bounds for the setting
with log-linear policies. Similarly, in this work, we provide the first theoretical guarantees for the popular
offline setting where the true reward parameter (used to label training data) is allowed to change over time.
Parameter drift has been widely studied in the bandit literature. Cheung et al. (2019) propose using a sliding
window to estimate parameters with data points close to the current timestep, whilst Bogunovic et al. (2016);
Zhao et al. (2020) investigate a restarting strategy. Similarly to the strategy of Russac et al. (2019), we use
an exponentially weighted discounting term to re-weight points close to the current timestep. Faury et al.
(2021); Wang et al. (2023) apply this approach to the case of generalised linear bandits first proposed by
Filippi et al. (2010). Pacchiano et al. (2021); Saha (2021); Mehta et al. (2023) focus on the duelling bandit
setting, where only preference feedback between two actions is provided by the environment. We utilise the
elements of their analysis in our analysis of NS-DPO.
Main contributions. To the best of our knowledge, this is the first work to present algorithms for fine-tuning
LLMs under non-stationary preferences in offline learning scenarios. Additionally, it is the first to utilize
the Dynamic Bradley-Terry model to accommodate varying preferences, a perspective that we anticipate
will have broader applicability in RLHF and continual fine-tuning of LLMs. We propose NS-DPO, a direct
preference optimization method that accounts for non-stationary preferences in the dataset via a Dynamic
Bradley-Terry model which modifies the training loss with a single exponential weighting parameter γ. In this
regard, NS-DPO represents a simple and practical extension of the popular DPO algorithm. We provide an
upper bound on the expected regret of NS-DPO for log-linear policies, given standard training data coverage
assumptions used in offline learning.
Empirically, we demonstrate that NS-DPO significantly outperforms stationary DPO and other relevant base-
lines on an offline learning problem across a range of non-stationary datasets and various degrees of preference
drift. Additionally, when preference drift is absent, NS-DPO matches the performance of DPO. We construct
non-stationary preference datasets from a variety of existing popular datasets including GlobalOpinionsQA
(Durmus et al., 2023), Helpful & Harmless (Dai et al., 2024), and UltraFeedback (Cui et al., 2023). We
simulate preference drift using facets of the datasets such as opinions from different groups (Durmus et al.,
2023) or suitable changes in reward models (Lambert et al., 2024). For example, we use popular reward
models PairRM (Jiang et al., 2023) and ArmoRM (Wang et al., 2024), which differ in various aspects such
as safety and reasoning, and switch between them when creating/labeling datasets, either through sudden
(changepoint) transitions or gradual changes. In summary:
NS-DPO is the first practical and provably efficient approach for non-stationary preference
optimization, offering simplicity in implementation and matching the computational complexity of
popular stationary direct preference optimization methods.
3

2
Preliminaries
Stationary RLHF. In the stationary RLHF setting (Ziegler et al., 2019; Ouyang et al., 2022), the goal
is to find a suitable LLM policy π, whose response a, to a prompt x, maximise a reward function r(x, a), i.e.,
J (π) = Ex∼X,a∼π
h
r(x, a) −τDKL[π(·|x)∥πref(·|x)]
i
.
(1)
Here, the KL-divergence prevents the learnt policy from deviating too far from some reference policy πref,
that has characteristics we wish to preserve in the final model. This is controlled by the parameter τ. In
practical settings, human feedback is too complex to capture in a hand designed reward model and we resort
to learning a model from human preference data.
Bradley-Terry Model. A human preference dataset consists of prompts and two possible responses
D = {(xi, ai, a′
i)}i∈[n], where ai is the response preferred to a′
i, and n the number of datapoints. To learn a
reward model from this dataset we assume the preferences are generated by a Bradley-Terry (BT) model
(Bradley and Terry, 1952) where the probability that ai is preferred to a′
i is
p(ai ≻a′
i|xi) = σ(r(xi, ai) −r(xi, a′
i)).
(2)
here σ(·) is the logistic sigmoid function and r(x, a) is the reward model of human preferences we do not
have access to and wish to learn. We parameterise the reward, typically as a single layer MLP on the last
layer of the reference policy model πref(Ziegler et al., 2019), and then learn the parameters using a maximum
likelihood estimator. An LLM can then be fine-tuned on the objective in Eq. (1) using Reinforcement Learning
(RL). It is important to note that the BT model captures many of the inherent assumptions we make about
our data which include the stationary nature of the underlying data generating process.
Direct Preference Optimization. Recent work by (Rafailov et al., 2024) avoids the training of an explicit
reward model in the stationary RLHF process by optimizing the LLM policy directly from human preference
data. To do this, the analytical solution to the stationary RLHF objective is rearranged into Eq. (1) to derive
an implicit reward
r(x, a) = τ log π(a|x)
πref(a|x) + τ log Z(x),
(3)
where Z(x) is a normalisation constant. This is substituted into the negative log likelihood of the Bradley-Terry
model (see Eq. (2)) resulting in the direct preference optimization (DPO) objective
L(π) =
X
(x,a,a′)∼D
−log σ

τ log π(a|x)
πref(a|x) −τ log π(a′|x)
πref(a′|x)

.
(4)
DPO, like the other methods introduced in this section, are all stationary as they assume the explicit or
implicit reward model does not change with time. This assumption does not hold when training on real-world
data however, and changes in preferences over time, captured in the dataset, appear as label noise.
3
Learning Under Preference Drift
To address the problem of preference drift, in datasets collected over a period of time, we propose Non-
Stationary Direct Preference Optimization (NS-DPO). NS-DPO incorporates the Dynamic Bradley-Terry
model which includes a non-stationary reward model r(x, a, t). Here t ∈{1, . . . , T −1} denotes a time step in
the past, and T ∈N+ denotes the current time step, where we are evaluating the trained policy. Under the
Dynamic Bradley-Terry model, the probability of response ai being preferred to a′
i is
p(ai ≻a′
i|xi, ti) = σ(r(xi, ai, ti) −r(xi, a′
i, ti)),
(5)
where in addition to the prompts and responses, we assume the dataset has temporal information about
when the human preference between the two responses is expressed, D = {(xi, ai, a′
i, ti}i∈[n]. For the ease of
indexing datapoints, we assume ti ≤tj if i < j.
4

Rather than making an explicit assumption on how the reward function varies over time, we consider a setting in
which the degree the reward can change is upper bounded. This is a mild assumption on the temporal variation,
and allows the reward to vary drastically at any point in time over all T −1 steps over which our training
data is recorded. We formalise this in Assumption 4.3 (Section 4), and use it to show that the convergence of
NS-DPO depends upon the upper bound of the allowed drift. An approach to learning in this setting is via
an exponentially weighted maximum likelihood estimator (Faury et al., 2021; Russac et al., 2019; Wang et al.,
2023), where points are re-weighted such that losses incurred at the most recent datapoints are prioritised.
To learn a suitable reward model in this setting, we define the reward at time step T as r(x, a, T) ∈R, where
R is the space of reward functions. We then estimate the reward function at timestep T, by maximising
the exponentially weighted negative log-likelihood of the Dynamic Bradley-Terry model as follows:
LDBT (r) =
X
(xi,ai,a′
i,ti)∼D
−γT −ti−1 log σ (r(xi, ai, T) −r(xi, a′
i, T)) .
(6)
In Eq. (6), γ ∈(0, 1) controls the rate at which older points are discounted. The loss recovers the stationary
Bradley-Terry model as γ →1.
Offline Non-Stationary Direct Preference Optimization. The derivation of NS-DPO follows as
previously shown in Section 2 for the stationary case. We first define the RLHF objective at timestep T as
JT (π) = Ex∼X,a∼π
h
r(x, a, T) −τDKL[π(·|x)∥πref(·|x)]
i
,
(7)
where we are interested in maximising the reward function r(x, a, T) that reflects human preferences in the
present (i.e., the current time step). We note the prompt distribution X and the reference model πref do
not vary with time. As we consider the reward model at a specific time T, we can derive an implicit reward
of the same form as Eq. (3). This relates the optimal policy and reward function of Eq. (7) as
r(x, a, T) = τ log π∗
T (a|x)
πref(a|x) + τ log Z∗
T (x),
(8)
where π∗
T is the optimal policy that optimises Eq. (7) and Z∗
T denotes the normalisation constant of π∗
T . We
then parameterise the policy π in Eq. (7) using the parameter θT , which enables expressing the implicit
reward with respect to the parameter as
rθT (x, a, T) = τ log πθT (a|x)
πref(a|x) + τ log ZθT (x),
(9)
where ZθT denotes the normalisation constant of πθT . We apply Eq. (9) into the exponentially weighted
negative log likelihood in Eq. (6) to derive the NS-DPO objective
LNS(θT ) =
X
(xi,ai,a′
i,ti)∼D
−γT −ti−1 log σ

τ log πθT (ai|xi)
πref(ai|xi) −τ log πθT (a′
i|xi)
πref(a′
i|xi)

.
(10)
Writing the difference in implicit reward terms as hθT (xi, ai, a′
i) = τ log
πθT (ai|xi)
πref(ai|xi) −τ log
πθT (a′
i|xi)
πref(a′
i|xi), the
gradient of the NS-DPO objective with respect to θT is
∇θT LNS(θT ) =
X
(xi,ai,a′
i,ti)∈D
−τγT −ti−1σ (−hθT (xi, ai, a′
i)) (∇θT log πθT (ai|xi) −∇θT log πθT (a′
i|xi)) . (11)
The gradient of the NS-DPO objective consists of two terms. The first term σ (−hθT (xi, ai, a′
i)) scales the
gradient update, which increases when the model incorrectly prefers response a′
i to ai and decreases when
the model correctly predicts the response preference. NS-DPO adjusts this term by discounting the scaling
term further when points are temporally far away from T, while increasing it for more recent points. The
second term, ∇θT log πθT (ai|xi) −∇θT log πθT (a′
i|xi), controls the direction of the gradient update.
5

4
Theoretical Analysis of Offline Non-stationary DPO
In this section, we analyse the performance of the NS-DPO objective (Eq. (10)), in the offline setting.
Policy Class. We use policies parameterised by θ ∈Θ ⊂Rd of the following form
Π =

πθ(a|x) =
exp(fθ(x, a))
P
a′∈A exp(fθ(x, a′))

,
(12)
where fθ(x, a) ∈R is a differentiable function. For our analysis, we consider the case of log-linear policies
where fθ is linear: fθ(x, a) = ϕ(x, a)⊺θ, and the feature map ϕ(x, a) is a d-dimensional vector. This is
motivated by the reward model introduced in Ziegler et al. (2019) where the last hidden layer of the LLM is
used as the feature embedding function ϕ(x, a).
Performance measure and Optimal Policy. Let ˜θT ∈Θ denote the parameter that minimises the
NS-DPO objective defined in Eq. (10). We assess the performance of the policy π˜θT , using the difference in
expected rewards
Roff
T = Ex∼X
h
Ea∼π∗
T (·|x)[r(x, a, T)] −Ea′∼π˜
θT (·|x)[r(x, a′, T)]
i
.
(13)
Here r(·, ·, T) denotes the true reward function at time T, and π∗
T denotes the optimal policy against which
we compare the performance of our algorithm. Given a reference policy πref, the optimal policy is defined
as the policy which optimises the RLHF objective at time step T
π∗
T = arg max
π∈Π
Ex∼X,a∼π
h
r(x, a, T) −τDKL[π(·|x)∥πref(·|x)]
i
.
(14)
Similarly, we can define the parameter θ∗
t of the optimal policy in each time step t ∈[T]
θ∗
t = arg max
θt∈Θ
Ex∼X,a∼π
h
r(x, a, t) −τDKL[πθt(·|x)∥πref(·|x)]
i
.
(15)
We now introduce further assumptions on the setting. In order to make the learning process possible, we
bound the 2-norm of the feature and parameter spaces.
Assumption 4.1. (Boundedness) The parameters and features are bounded: θ ∈Θ where Θ = {θ ∈
Rd | ∥θ∥2 ≤W} and Φ = {ϕ(x, a) ∈Rd | ∥ϕ(x, a)∥2 ≤L}.
It is known that an equivalence class of reward models leads to the same preferences under the Bradley-Terry
model (Rafailov et al., 2024). This is similarly true in the case of the Dynamic Bradley-Terry model, because
the implicit reward of NS-DPO, shown in Eq. (8), relates the reward to the policy parameters θ. We thus
construct the following constraint on the policy class to properly specify the problem (Chowdhury et al., 2024).
Assumption 4.2. (Identifiability) The optimal policy in each time step t corresponds to a single parameter
in Θ, which satisfies Eq. (15): 1⊺
dθ∗
t = 0
∀t ∈[T].
We consider the setting where the true underlying parameter θ∗
t ∈Θ, ∀t ∈[T] of the optimal policy π∗is
changing at each time step. We do not constrain how the optimal parameter changes, but instead upper
bound the possible parameter drift allowed in the environment up to time step T. This upper bound is known
as the variation budget.
Assumption 4.3. (Variation Budget Bound) The parameter drift of θ∗
t ∈Θ across T timesteps is upper
bounded as PT −1
t=1 ∥θ∗
t+1 −θ∗
t ∥2 ≤BT where BT > 0 is a known constant.
In the offline setting, our learning is constrained by the available dataset D. A standard assumption in the
offline learning literature is that of data coverage (Chowdhury et al., 2024; Zhu et al., 2023). The data coverage
assumption ensures that the reference policy πref suitably explores the space of plausible responses of the
optimal policy. We define the population covariance matrix as Σπ = E[ϕ(x, a)ϕ(x, a)⊺] −E[ϕ(x, a)]E[ϕ(x, a)]⊺
where the expectation is calculated over samples x ∼X, a ∼π(·|x). The condition number κπ compares the
coverage of the two policies π and πref
∀π ∈Π : κπ = sup
v∈Rd
v⊺Σπv
v⊺Σπrefv = λmax(Σπ)
λmin(Σπref),
(16)
6

while we use κ = maxπ κπ to denote the maximum possible value of κπ. The definition of κπ requires that
the reference policy sufficiently explores the feature space, which leads to the following assumption.
Assumption 4.4. (Feature Coverage) The reference policy πref satisfies λmin(Σπref) > 0.
In a time-varying setting, the quality of the dataset D also depends upon its temporal coverage. We use the
following assumption which also guarantees a minimal amount of data in each time step. Having enough data
in each time step is motivated by the fact that we are assuming no knowledge of the dynamics of the actual
preference drift. Note that Θ(T) in the assumption is the notation for the complexity, which is different from
the parameter set Θ in Assumption 4.1.
Assumption 4.5. (Temporal Coverage) For each time step t ∈[T −1], the number of datapoints in the
training set is between m and ¯m, where m > 0 and ¯m > m are constants (i.e., n = Θ(T)).
4.1
Theoretical Results
Estimation Error. To bound the expected regret of the policy trained with NS-DPO, bounding the
difference between the optimal and the found parameter is required. To analyse the parameter estimation
error, we define the discounted covariance matrix of the offline dataset as
ˆΣ = 1
n
n
X
i=1
γT −ti−1(ϕ(xi, ai) −ϕ(xi, a′
i))(ϕ(xi, ai) −ϕ(xi, a′
i))⊺.
(17)
Under the assumptions from Section 4, we introduce bounds on the estimation error of the parameter ˜θT
with respect to the true parameter θ∗
T , and ˆΣ
∥θ∗
T −˜θT ∥ˆΣ+λI,
(18)
where λ > 0 is introduced to guarantee the inversion of the matrix ˆΣ + λI. The detailed analysis of the
confidence bound is provided in Appendix C.1.
Our analysis differs from the stationary case (Chowdhury et al., 2024), as we also consider the discounted
datapoints in the NS-DPO loss. This is reflected in the covariance matrix ˆΣ by the inclusion of the γT −ti−1
term, which increases uncertainty about the observations that happened further in the past.
We also separate the estimation error into the learning term (Eq. (43)) and the tracking term (Eq. (44)), the
latter of which accounts for the error introduced by the non-stationary nature of the environment.
Expected Regret Bound. Starting from the definition of the expected regret in Eq. (13), the regret can
be expressed in terms of the estimation error in Eq. (28). We can then use our results in Theorem C.1 to
complete the analysis. The full details of the regret analysis proof are deferred to Appendix C.2.
Theorem 4.1. (Regret bound of ˜θT ) Let δ ∈(0, 1
2], τ > 0 and r∗
T (x, a) < rmax ∀x ∈X, a ∈A. Let ˜θT denote
the parameter in Θ which minimises the NS-DPO loss (Eq. (24)) on an offline dataset. When γ = 1−
  BT
dT
1/2,
the following bound holds with probability at least 1 −2δ:
Roff
T ≤rmaxp
¯mT(1 −γ)κ
C2
p
2m(1 −γT −1)
 
2
√
λW + 2C1
τcσ,τ
r
d + log(1/δ)
n
+ 16LRσ,τ ¯m
T(1 −γ)
3
2
r
d ¯m
n BT
!
,
where C1 > 0 and 0 < C2 < 1 denote constants. In other words, when λ = O
  d
n

, Roff
T
satisfies:
Roff
T = ˜O

d B1/2
T
n−1/4
.
We compare the complexity bound of our approach with the previous works. Standard offline RL algorithms
assuming the stationarity of the underlying reward functions achieve O(n−1/2) (Wang et al., 2021; Zhan et al.,
2024; Qiao and Wang, 2024; Cen et al., 2024). Chowdhury et al. (2024) achieves O(n−1/4) in scenarios where
preferences are subject to noises, whose frequency is known. Our regret bound is O(n−1/4), which is comparable
to that of Chowdhury et al. (2024). The complexity of our algorithm is influenced by the value of γ = 1 −
  BT
dT
1/2, which is used to address the non-stationarity in the dataset. We provide the details in Appendix C.2.1.
7

5
Experiments
In this section, we empirically evaluate NS-DPO’s ability to learn under preference drift. We first show that
NS-DPO outperforms DPO in the log-linear policy setting, supporting our theoretical results introduced
in Section 4.1. We then analyse how NS-DPO performs under different types of preference drift and under
different strengths of preference change using the Llama2 LLM (Touvron et al., 2023). Our code is available
at https://github.com/geronest/ns-dpo.
5.1
Experimental Setup
5.1.1
Synthetic Experiments
To analyse the performance of NS-DPO in the log-linear policy class, we construct a synthetic environment
with a known feature space and preference drift. We use the feature space from (Li et al., 2023), where
x ∈X = [0, 1]dx, a ∈A = [na] and ϕ(x, a) is computed as
ϕ(x, a) =
"
(a + 1) · cos(x0 · π),
1
a + 1 · sin(x0 · π), · · · , (a + 1) · cos(xdx−1 · π),
1
a + 1 · sin(xdx−1 · π)
#
.
(19)
The dimensions of the feature space and the policy parameter are both 2 · dx. We use dx = 4, dθ = 8, |A| = 16
for all synthetic experiments.
Non-stationary Dataset.
To construct a dataset D = {x, a, a′, t}n
i=1, we randomly sample x ∼X and
a1, a2 ∼A. We assign 20 datapoints per time step ∀t ∈[100]. We sample 100 datapoints for evaluation at
T = 101. To introduce preference drift, we follow an approach similar to Faury et al. (2021). We sample the
preferences over a1 and a2 from the class of log-linear policies given in Eq. (12), parameterised by θ∗
t . We
denote preferred response as a and the rejected response as a′. When t ≤33, we set the optimal parameter
as θ∗
t = (1, 0, 1, 0, 1, 0, 1, 0)⊺. Between 34 ≤t ≤66, the parameter θ∗
t varies as
θ∗
t =

cos( t−33
33
· π
2 ), sin( t−33
33
· π
2 ), . . . , cos( t−33
33
· π
2 ), sin( t−33
33
· π
2 )
⊺.
(20)
For the remaining time steps 67 ≤t ≤100, we use θ∗
t = (0, 1, 0, 1, 0, 1, 0, 1)⊺.
Algorithms for Synthetic Experiments.
We compare NS-DPO with DPO and SW-DPO in synthetic
experiments. SW-DPO uses a "sliding" window to only consider points close to the current timestep T,
which is commonly used in the non-stationary bandit literature (Garivier and Moulines, 2008). We test the
performance of NS-DPO and SW-DPO over several values of γ ∈{0.7, 0.9} and window size w ∈{33, 50}.
The regularisation coefficient is τ = 1.0 for all algorithms. We normalise the scale of the gradient for each
method to address the differences caused by the application of exponential weighting and sliding window.
For the reference policies, we use a uniform policy, whose parameter θref ∈Rd is a zero vector.
Evaluation Metrics.
To analyse the performance of the algorithms, we use the reward accuracy of the
trained policies. The reward accuracy is computed by the portion of test response pairs with correctly
estimated preferences, using the implicit rewards defined in Eq. (8). For each tested algorithm, we report
averaged results of the experiments across 10 different random seeds.
5.1.2
Large Language Model Experiments
To test the performance of NS-DPO in an LLM setting, we create three preference datasets with known and
controlled preference drift.
1) NSGO Datasets.
We modify the GlobalOpinionQA dataset2 (Durmus et al., 2023) to create a time
varying dataset. GlobalOpinionQA consists of questions regarding global issues, different responses, and
preferences from several countries represented as a probability vector. We copy the questions and responses
to create multiple time steps t ∈[100]. We then vary the preferences with time by linearly interpolating
between the preferences of two different countries. This simulates gradual preference drifts that can be caused
by demographic shift or a series of external events. We generate preference drift using three pairs of countries.
2https://huggingface.co/datasets/Anthropic/llm_global_opinions
8

0
200
400
600
800
Training Steps
0.700
0.725
0.750
0.775
0.800
0.825
0.850
0.875
0.900
Reward Accuracy
NS-DPO ( = 0.7)
NS-DPO ( = 0.9)
SW-DPO (w = 33)
SW-DPO (w = 50)
DPO
0
200
400
600
800
Training Steps
0.700
0.725
0.750
0.775
0.800
0.825
0.850
0.875
0.900
Reward Accuracy
NS-DPO ( = 0.3)
NS-DPO ( = 0.5)
NS-DPO ( = 0.7)
NS-DPO ( = 0.9)
Figure 2: Synthetic experiment results with dx = 4, |A| = 16. The shaded area represents the standard deviation of each
algorithm. [Left] NS-DPO and SW-DPO successfully addresses the non-stationarity present in the dataset, while stationary DPO
fails to do so. NS-DPO shows faster training than SW-DPO, even compared to the case where the value of the window parameter
w for SW-DPO is set to the optimal value of 33. [Right] An ablation study on how different values of the discount factor γ affect
the training of NS-DPO. As the value of γ becomes larger, the final test accuracy of the policy is achieved in fewer training steps.
In each pair the starting country is the US, and the ending country is either Brazil, Japan or Germany. The
preferences at the first and last time step correspond to either country in the pair. The last time step is held
out as a test dataset and treated as the current time T = 101. We divide the prompt-response pairs so that
training and test data do not share any prompts.
2) UltraFeedback Datasets.
Using the prompts and response candidates of UltraFeedback3 (Cui et al.,
2023), we obtain preferences from two different reward models, PairRM4(Jiang et al., 2023) and ArmoRM5
(Wang et al., 2024). The datapoints in the training set are randomly assigned to one of t ∈[100] time steps,
and assigned preferences of PairRM if the time step t is earlier than the change point tcp ∈{51, 66, 81}.
We assign the preferences of ArmoRM for the datapoints with time steps t ≥tcp and datapoints in the test
set with T = 101. We also vary the portion of datapoints with preferences that differ between PairRM and
ArmoRM, which is denoted by ρdiff. We use ρdiff ∈{0.5, 0.7, 0.9, 0.95, 1.0} to create the datasets. We use
10k datapoints for training and 500 datapoints for testing.
helpsteer-helpfulness
helpsteer-correctness
helpsteer-coherence
helpsteer-complexity
beavertails-is_safe
0.00
0.25
0.50
0.75
1.00
Correlation
Figure 3: The correlation of different
preference labels generated by rewards
from the ArmoRM reward model on
the Helpful Harmless harmless-base
dataset (Bai et al., 2022a).
We ob-
served that concepts such as safety and
helpfulness have more correlated pref-
erences, whilst the helpsteer-coherence
reward model is un-correlated with
the other models we analysed.
3) Time Varying Helpful Harmless Datasets.
Using the harmless-
base subset of the Helpful Harmless dataset6(Bai et al., 2022a), we create a
time varying preference dataset. To do so, we use two reward models, the
helpsteer-helpfulness and beavertails-is_safe outputs from the ArmoRM
model (Wang et al., 2024). Figure 3 shows that these rewards result in
different preferences on the harmless-base dataset. We then assign each
datapoint in the dataset a random time value from t ∈[100]. We construct
two methods to assign preferences using the time step information: change
point preference shift and gradual variation. Under the change point
preference shift, datapoints are assigned preferences according to helpsteer-
helpfulness before the change point tcp and beavertails-is_safe after the
change point. Under gradual variation, we use the following reward model
r(x, y, t) =







r0(x, y)
t < 33
r0(x, y) (t−33)
33
+ r1(x, y)

1 −(t−33)
33

33 ≤t < 66
r1(x, y)
t ≥66,
(21)
where r0 is the helpsteer-helpfulness reward and r1 is the beavertails-is_safe
reward. We use this type of schedule for gradual change to simulate pref-
erence drifts that happens gradually over a finite time horizon. We use
15k points for training and 2k for testing. We use reward models for
3We modify the binarized version of UltraFeedback.
4https://huggingface.co/llm-blender/PairRM
5https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1
6https://huggingface.co/datasets/Anthropic/hh-rlhf
9

0.5
0.7
0.9
0.95
1.0
Value of 
diff
0.3
0.4
0.5
0.6
0.7
Reward Accuracies
DPO
tDPO
NS-DPO
0.5
0.7
0.9
0.95
1.0
Value of 
diff
0.3
0.4
0.5
0.6
0.7
Reward Accuracies
DPO
tDPO
NS-DPO
0.5
0.7
0.9
0.95
1.0
Value of 
diff
0.3
0.4
0.5
0.6
0.7
Reward Accuracies
DPO
tDPO
NS-DPO
Figure 4: [Left] Reward model shift at tcp = 51. [Middle] Reward model shift at tcp = 66. [Right] Reward model shift at
tcp = 81. As ρdiff, the percentage of training datapoints with flipped preference increase, stationary DPO fails to learn the
preference distribution at T = 101. Meanwhile, NS-DPO shows robust performance under various values of ρdiff, maintaining
reward accuracies above 50%. As tcp, the change point of the reward model happens later in time, the gap between stationary
approaches and NS-DPO gets bigger. The experiments were run on the UltraFeedback dataset under a reward model shift from
PairRM to ArmoRM. The Llama-2-7b-chat-hf model was used and the training dataset consisted of 100 time steps.
helpfulness and safety, as these are both desired properties of an LLM but often result in differing preferences;
for example, rewarding helpfulness can often lead to unsafe outputs when an LLM is asked a dubious question,
like how to best rob a store.
Language Models.
We use Llama-2-7b-chat-hf 7 (Touvron et al., 2023) for both fine-tuning and the
reference model. To reduce the compute demands, we train LoRA weights (Hu et al., 2022) (see Section 6
for further details).
Algorithms for the LLM experiments.
We compare NS-DPO against baselines including include
stationary DPO and IPO. We also construct an In-Context Learning (ICL) algorithm referred to as tDPO,
in which information about the time step is appended to the prompts of the data. All algorithms use the
same SFT model as the reference model. We do not adjust the SFT training procedure from Rafailov et al.
(2024), training the model on the preferred responses in the dataset. We used τ = 0.1 for all models, and
NS-DPO uses γ = 0.95 for experiments with 2C NSGO dataset and UltraFeedback dataset. For Time Varying
Helpful-Harmless dataset, we adjust the value of γ as γ = 1 −(
1
100−tcp ) log(100).
Evaluation Metrics.
To compare the performance of NSDPO and the baseline algorithms in LLM datasets,
we use reward accuracy as we do in synthetic experiments.
5.2
Experiment Results
0
2000
4000
6000
8000
Trained Examples
0.3
0.4
0.5
0.6
0.7
Reward Accuracies
NS-DPO ( = 0.95)
DPO
Figure 5: Training curves of NS-DPO and
DPO on a stationary setting of the UltraFeed-
back dataset, which corresponds to tcp = 0.
Llama-2-7b-chat-hf was used for LLMs. NS-
DPO matches the performance of DPO even
in stationary settings.
How does NS-DPO perform when specialised to log-linear
policy classes?
As shown in the left image of Figure 2, when com-
pared to NS-DPO and SW-DPO, DPO shows the worst performance
with respect to the test data. Both NS-DPO and SW-DPO, which
account for the preference drift present in the data, show signifi-
cantly better performance. SW-DPO achieves similar performance
to NS-DPO in the later stages of training, but NS-DPO achieves
this performance in fewer training steps. As NS-DPO only varies the
weights of datapoints, rather than removing them entirely, it can still
leverage the information of datapoints in the earlier time steps. The
right image of Figure 2 shows a comparison of different values of γ, ranging from 0.3 to 0.9. The results
show that the performance of NS-DPO is stable in terms of the final test accuracy across a large range of
values, γ ∈[0.5, 0.9]. As the value of γ is reduced, only points closest to the current time step contribute
significantly to the gradient update of the model. Thus as γ decreases, NS-DPO requires more training steps
for the reward accuracy on the test set to converge.
NS-DPO outperforms the stationary DPO method, and achieves the same performance as other
non-stationary baseline approaches in fewer training steps. The final performance of NS-DPO is
robust to the value of γ across a wide range of values.
7https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
10

0
2000
4000
6000
8000
Trained Examples
0.3
0.4
0.5
0.6
0.7
Reward Accuracies
NS-DPO ( = 0.95)
DPO
tDPO
0
2000
4000
6000
8000
Trained Examples
0.3
0.4
0.5
0.6
0.7
Reward Accuracies
NS-DPO ( = 0.95)
DPO
tDPO
0
1000
2000
3000
4000
5000
6000
7000
8000
Trained Examples
0.3
0.4
0.5
0.6
0.7
Reward Accuracies
NS-DPO ( = 0.95)
DPO
tDPO
Figure 6: Llama-2-7b-chat-hf experiment results using 2C NSGO dataset. [Left] Opinion drift from the US to Germany.
[Middle] Opinion drift from the US to Japan. [Right] Opinion drift from the US to Brazil. NS-DPO stays robust to the
non-stationarity present in the dataset and achieves reward accuracies above 60%, while stationary methods show dropped
reward accuracies of around 55%. Including the time steps in the prompt (tDPO) does not help meaningfully improve the
performance of stationary DPO.
10
30
50
70
90
Change Point
0.3
0.4
0.5
0.6
0.7
0.8
Reward Accuracies
DPO
IPO
NS-DPO
10
30
50
70
90
Change Point
0.3
0.4
0.5
0.6
0.7
0.8
Reward Accuracies
DPO
IPO
NS-DPO
10
30
50
70
90
Change Point
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Reward Accuracies
DPO
IPO
NS-DPO
Figure 7: NS-DPO consistently outperforms DPO and IPO as the change point, tcp nears the present T = 101 for varying
strengths of preference shift on the Time Varying Helpful Harmless dataset using the Llama-2-7b-chat-hf model.
[Left]
ρdiff = 0.7. [Middle] ρdiff = 0.8. [Right] ρdiff = 0.9. We note that as the value of tcp increases, the performance difference
between NS-DPO and the baselines increases. This is because as the change point moves closer to the present time step, the
number of samples available from the updated preference distribution decreases. NS-DPO discounts samples with old preferences,
focusing learning upon the small number of samples with up-to-date preference labels.
How robust and effective is NS-DPO under varying strengths of sudden preference drift?
Preference drift in non-stationary scenarios can vary in strength. To investigate this, we conduct two different
experiments. Firstly, we vary ρdiff, the portion of datapoints with preferences that change, at three different
change points on the non-stationary UltraFeedback Dataset introduced in Section 5.1. Secondly, we vary
the change point for three different values of ρdiff on the Time Varying Helpful Harmless dataset. Stationary
preference algorithms treat non-stationary preferences as label noise in the data. As ρdiff is increased, the
level of noise observed by the stationary algorithms increase leading to worse performance. We show this
in Figure 4 and Figure 7 where for high values of ρdiff, when the change point is close to the present, the
difference in performance between NS-DPO and the baseline algorithms can be as much as 20%. Datasets
with a change point that occurs close to the present have very few examples of the new preference distribution.
Because of this, stationary algorithms learn the old preference distribution, as that is mostly represented
in the data. The low performance of the baseline algorithms on the binary classification of preferences at test
time demonstrates this empirically. Note that the performance of NS-DPO matches that of DPO even when
the preference shift in the dataset is not significant, ρdiff ≤0.7. This observation is further supported by
Figure 5, where NS-DPO matches the performance of stationary DPO in a dataset with no preference drift.
These results show that NS-DPO is robust against strong preference drift in offline datasets and matches
the performance of stationary algorithms when the preference drift is trivial.
Standard preference learning approaches fail under strong preference drift, learning equally from old
and recent preferences. NS-DPO is robust in these settings, and matches the performance of stationary
approaches when the preference drift is small or non-existent.
How does NS-DPO perform under gradual preference drifts?
As well as the strength of the preference
shift, different schedules of preference drift can also affect the performance of preference optimization algorithms.
Here we investigate cases where preference drift happens gradually over time. In Figure 8, we see that NS-DPO
outperforms the DPO reward accuracy by over 10% on the TVHH dataset with gradual preference drift. We
11

note that the performance of NS-DPO is dependent upon the value of γ chosen, however both approaches out-
perform the stationary baseline. The experiment results on the 2C NSGO dataset, which also simulates a grad-
ual drift of preferences, are given in Figure 6. NS-DPO shows significantly better performance compared to sta-
tionary DPO, showing a performance gap of nearly 10% in reward accuracy. This difference is mainly caused by
stationary methods failing to efficiently learn from datapoints at later time steps. tDPO, which trains the policy
with time step information appended to the prompt, does not show a significant difference from stationary DPO.
NS-DPO outperforms stationary approaches when preferences change gradually over multiple time
steps instead of at a specific change point.
6
Conclusion
0.85
0.95
0.35
0.40
0.45
0.50
0.55
0.60
0.65
Reward Accuracy
DPO
NS-DPO
Figure 8: NS-DPO outperforms DPO in set-
tings where preference drift occurs slowly
across multiple timesteps. Here we compare
NS-DPO and DPO on the TVHH dataset
with a gradual preference shift.
In this work we propose NS-DPO, a practical and provably efficient ap-
proach for preference optimization on non-stationary offline datasets.
With standard assumptions, we provide a theoretical analysis on the
performance of NS-DPO in the case of log-linear policies and show
that the complexity of the regret matches that of other stationary
approaches. We further support this result with a suit of empirical
results on a synthetic setting. We also investigate the application of
NS-DPO to LLMs, create several non-stationary preference datasets,
and show that NS-DPO shows superior performance to standard pref-
erence optimization algorithms and In Context Learning approaches
on these datasets. Even in stationary settings, NS-DPO matches the
performance of stationary algorithms. This motivates the usefulness
of our approach when the existence of preference drift in a dataset is unknown, as applying NS-DPO will not
hurt performance even if the preference drift is too small to matter. Our approach can be further extended
to the online setting where data is sequentially provided as time passes. NS-DPO can also be adapted to
learn at a time step that is not the present by discounting both past and future preference as a function of
their distance from the time step of interest. We leave these ideas to future work.
Compute Resources Uses
To run the LLM experiments, we used A100 GPUs with 40GB VRAM. The synthetic experiments were run
locally on a laptop without using GPUs.
Acknowledgements
IB was supported by the EPSRC New Investigator Award EP/X03917X/1; the Engineering and Physical
Sciences Research Council EP/S021566/1; and Google Research Scholar award. WB was supported by the
Engineering and Physical Sciences Research Council EP/S021566/1.
References
Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset. arXiv preprint
arXiv:2402.10571, 2024.
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete
problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.
Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko,
and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences.
In International Conference on Artificial Intelligence and Statistics, pages 4447–4455. PMLR, 2024.
12

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073, 2022b.
Ilija Bogunovic, Jonathan Scarlett, and Volkan Cevher. Time-varying gaussian process bandit optimization.
In Artificial Intelligence and Statistics, pages 314–323. PMLR, 2016.
Heejong Bong, Wanshan Li, Shamindra Shrotriya, and Alessandro Rinaldo. Nonparametric estimation in the
dynamic bradley-terry model. In International Conference on Artificial Intelligence and Statistics, pages
3317–3326. PMLR, 2020.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired
comparisons. Biometrika, pages 324–345, 1952.
John C Caldwell. The mechanisms of demographic change in historical perspective. Population studies, pages
5–27, 1981.
Micah Carroll, Davis Foote, Anand Siththaranjan, Stuart Russell, and Anca Dragan. Ai alignment with
changing and influenceable reward functions. arXiv preprint arXiv:2405.17713, 2024.
Manuela Cattelan, Cristiano Varin, and David Firth. Dynamic bradley–terry modelling of sports tournaments.
Journal of the Royal Statistical Society Series C: Applied Statistics, pages 135–150, 2013.
Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans,
Yuejie Chi, and Bo Dai. Value-incentivized preference optimization: A unified approach to online and
offline rlhf. arXiv preprint arXiv:2405.19320, 2024.
Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik
Narasimhan, Ameet Deshpande, and Bruno Castro da Silva. Rlhf deciphered: A critical analysis of
reinforcement learning from human feedback for llms. arXiv preprint arXiv:2404.08555, 2024.
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan,
Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. 2024a.
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak
language models to strong language models. 2024b.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non-stationarity. In
International Conference on Artificial Intelligence and Statistics, pages 1079–1087. PMLR, 2019.
Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language
models with noisy feedback. International Conference on Machine Learning, 2024.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement
learning from human preferences. Advances in neural information processing systems, 30, 2017.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint
arXiv:2310.01377, 2023.
Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang.
Safe rlhf: Safe reinforcement learning from human feedback. 2024.
Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen,
Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring the representation of
subjective global opinions in language models. arXiv preprint arXiv:2306.16388, 2023.
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment
as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.
13

Louis Faury, Marc Abeille, Clément Calauzènes, and Olivier Fercoq. Improved optimistic algorithms for
logistic bandits. In International Conference on Machine Learning, pages 3052–3060. PMLR, 2020.
Louis Faury, Yoan Russac, Marc Abeille, and Clément Calauzenes. Regret bounds for generalized linear
bandits under parameter drift. arXiv preprint arXiv:2103.05750, 2021.
Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The generalized
linear case. Advances in neural information processing systems, 23, 2010.
Aurélien Garivier and Eric Moulines. On upper-confidence bound policies for non-stationary bandit problems.
arXiv preprint arXiv:0805.3415, 2008.
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame,
Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback.
arXiv preprint arXiv:2402.04792, 2024.
Dan Hendrycks and Mantas Mazeika. X-risk analysis for ai research. arXiv preprint arXiv:2206.05862, 2022.
Jiwoo Hong, Noah Lee, and James Thorne. Reference-free monolithic preference optimization with odds ratio.
arXiv preprint arXiv:2403.07691, 2024.
Daniel Hsu, Sham Kakade, and Tong Zhang. A tail inequality for quadratic forms of subgaussian random
vectors. 2012.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. Lora: Low-rank adaptation of large language models. International Conference on Learning
Representations, 2022.
Ermo Hua, Biqing Qi, Kaiyan Zhang, Yue Yu, Ning Ding, Xingtai Lv, Kai Tian, and Bowen Zhou. Intuitive
fine-tuning: Towards unifying sft and rlhf into a single process. arXiv preprint arXiv:2405.11870, 2024.
Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with
pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 14165–14178, 2023.
Branden B Johnson and Marcus Mayorga. Temporal shifts in americans’ risk perceptions of the zika outbreak.
Human and Ecological Risk Assessment: An International Journal, 27(5):1242–1257, 2020.
Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, and Prathap Ramachandra. Rs-dpo: A hybrid rejection sampling
and direct preference optimization method for alignment of large language models. In Findings of the
Association for Computational Linguistics: NAACL 2024, pages 1665–1680, 2024.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha
Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language
modeling. arXiv preprint arXiv:2403.13787, 2024.
Ziniu Li, Tian Xu, and Yang Yu. Policy optimization in rlhf: The impact of out-of-preference data. arXiv
preprint arXiv:2312.10584, 2023.
Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and Jialu Liu.
Statistical rejection sampling improves preference optimization. 2024.
Chris Lu, Samuel Holt, Claudio Fanconi, Alex J Chan, Jakob Foerster, Mihaela van der Schaar, and
Robert Tjarko Lange. Discovering preference optimization algorithms with and for large language models.
arXiv preprint arXiv:2406.08414, 2024.
Viraj Mehta, Vikramjeet Das, Ojash Neopane, Yijia Dai, Ilija Bogunovic, Jeff Schneider, and Willie Neiswanger.
Sample efficient reinforcement learning from human feedback via active exploration.
arXiv preprint
arXiv:2312.00267, 2023.
Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free
reward. arXiv preprint arXiv:2405.14734, 2024.
14

Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhao-
han Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning
from human feedback. 2024.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. Advances in neural information processing systems, 35:27730–27744, 2022.
Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling rl: reinforcement learning with trajectory
preferences. arXiv preprint arXiv:2111.04850, 2021.
Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston.
Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024.
Shiva Kumar Pentyala, Zhichao Wang, Bin Bi, Kiran Ramnath, Xiang-Bo Mao, Regunathan Radhakrishnan,
Sitaram Asur, et al. Paft: A parallel training paradigm for effective llm fine-tuning. arXiv preprint
arXiv:2406.17923, 2024.
Biqing Qi, Pengfei Li, Fangyuan Li, Junqi Gao, Kaiyan Zhang, and Bowen Zhou. Online dpo: Online direct
preference optimization with fast-slow chasing. arXiv preprint arXiv:2406.05534, 2024.
Dan Qiao and Yu-Xiang Wang. Offline reinforcement learning with differential privacy. Advances in neural
information processing systems, 36, 2024.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.
Direct preference optimization: Your language model is secretly a reward model. Advances in neural
information processing systems, 36, 2024.
Shyam Sundhar Ramesh, Yifan Hu, Iason Chaimalas, Viraj Mehta, Pier Giuseppe Sessa, Haitham Bou
Ammar, and Ilija Bogunovic. Group robust preference optimization in reward-free rlhf. arXiv preprint
arXiv:2405.20304, 2024.
Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang
Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. arXiv
preprint arXiv:2404.03715, 2024.
Yoan Russac, Claire Vernade, and Olivier Cappé. Weighted linear bandits for non-stationary environments.
Advances in neural information processing systems, 32, 2019.
Aadirupa Saha.
Optimal algorithms for stochastic contextual preference bandits.
Advances in neural
information processing systems, 34:30050–30062, 2021.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario
Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in neural
information processing systems, 33:3008–3021, 2020.
Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist
approach to reinforcement learning from human feedback. 2024.
Xin-Yu Tian, Jian Shi, Xiaotong Shen, and Kai Song. A spectral approach for the dynamic bradley-terry
model. arXiv preprint arXiv:2307.16642, 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via
multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024.
Jing Wang, Peng Zhao, and Zhi-Hua Zhou. Revisiting weighted strategy for non-stationary parametric
bandits. In International Conference on Artificial Intelligence and Statistics, pages 7913–7942. PMLR,
2023.
15

Ruosong Wang, Dean P Foster, and Sham M Kakade. What are the statistical limits of offline rl with linear
function approximation? 2021.
Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jiawei Chen, Jinyang Gao, Bolin Ding, Xiang Wang,
and Xiangnan He. Towards robust alignment of language models: Distributionally robustifying direct
preference optimization. arXiv preprint arXiv:2407.07880, 2024a.
Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference
optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024b.
Tengyang Xie, Dylan J Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, and Alexander
Rakhlin. Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient
rlhf. arXiv preprint arXiv:2405.21046, 2024.
Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative
preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In
International Conference on Machine Learning, 2024.
Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others:
Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023.
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
Self-rewarding language models. 2024.
Farhad Zafari, Irene Moser, and Tim Baarslag. Modelling and analysis of temporal preference drifts using a
component-based factorised latent approach. Expert systems with applications, 116:186–208, 2019.
Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline preference-based
reinforcement learning. In International Conference on Learning Representations, 2024.
Shenao Zhang, Donghan Yu, Hiteshi Sharma, Ziyi Yang, Shuohang Wang, Hany Hassan Awadalla, and
Zhaoran Wang. Self-exploring language models: Active preference elicitation for online alignment. In
Automated Reinforcement Learning: Exploring Meta-Learning, AutoML, and LLMs, 2024.
Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary linear bandits.
In International Conference on Artificial Intelligence and Statistics, pages 746–755. PMLR, 2020.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping
Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in neural information processing systems,
36, 2024.
Banghua Zhu, Michael Jordan, and Jiantao Jiao. Principled reinforcement learning with human feedback
from pairwise or k-wise comparisons. In International Conference on Machine Learning, pages 43037–43067.
PMLR, 2023.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano,
and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593,
2019.
16

A
Appendix Contents
In Appendix B, we explain the details of experiments conducted, including the creation of non-stationary
datasets for LLM experiments and the behaviour of NS-DPO and SW-DPO in the synthetic setting. We
provide proofs of our theoretical analysis in Appendix C step by step. Some of the in-depth derivations
necessary for the theoretical analysis are separately presented in Appendix C.3 and Appendix C.4.
B
Further Experiment Details
B.1
The Two Countries (2C) Non-Stationary Global Opinions Dataset
To test NSDPO, we create a synthetic non-stationary dataset in which the temporal trends are known. To
do this, we use the GlobalOpinionsQA dataset (Durmus et al., 2023). We preprocess the dataset in three
major ways.
Binary Preferences.
We convert the dataset to a dataset of binary preferences. For each set of prompt
and responses, we create a row for each possible combination of prompt and binary response pairs. We
calculate the preference probability for these response pairs as follows. Assuming the non-binary responses
follow a Plackett-Luce preference framework, we can find the reward associated with responses (up to an
additive constant) by taking the log of the preference probability. We can then take the sigmoid of these
responses to find a normalised binary preference.
Country Filter.
We filter the dataset down to the following countries: Nigeria, Egypt, India, China, Japan,
Germany, France, Spain, United States, Canada, Brazil, Argentina, Australia and New Zealand.
Country Level Prompts.
We filter the dataset such that each row of the dataset is the prompt, response,
preference probability of a single country.
After the preprocessing, we copy the dataset and assign a different timestep to each unique instance of
(prompt, response, preference). We simulate the drift in preferences by using preference probabilities of
two countries, shifting from one to another over time. Out of 100 time steps in the training dataset, the
first 33 time steps consisted of preference probabilities from the US. Preference labels sampled from the last
33 time steps are from probabilities of the target country. We use Germany, Japan and Brazil as target
countries, creating three different datasets. In the intermediate 33 time steps, preference labels are sampled
from interpolated probabilities between these two countries. To introduce sufficient shift in preferences, we
selected responses in which probabilities for the same response from two countries differed at least by 0.2. We
subsampled prompt-response pairs down to 10,000 datapoints, allowing each time step to consist of different
prompts and responses. For evaluation, we used prompts and response candidates that are not present in the
training data.
B.2
Synthetic Experiments
In this section, we present the experiment results of NS-DPO and SW-DPO on the synthetic dataset with
varied values of hyperparameters γ and w. As shown in Figure 9, The performance of NS-DPO is robust
across varied values of γ, maintaining its reward accuracy over 80% when 0.5 ≤γ ≤0.97. In the case of
SW-DPO, the performance is more sensitive to the change of the window size w. When w = 10, it shows
similar test performance in the later stage of the training, while the process is visibly slowed down due to
the reduced amount of datapoints actually being used. On the other hand, as the window size gets bigger
and starts including datapoints where parameter shift introduces conflicting preferences, SW-DPO also
shows degrading performance. These results provide further support the advantages of using NS-DPO over
SW-DPO, as it shows faster training and less sensitivity to the hyperparameter.
17

0
200
400
600
800
Training Steps
0.700
0.725
0.750
0.775
0.800
0.825
0.850
0.875
0.900
Reward Accuracy
NS-DPO ( = 0.93)
NS-DPO ( = 0.95)
NS-DPO ( = 0.97)
NS-DPO ( = 0.99)
0
200
400
600
800
Training Steps
0.700
0.725
0.750
0.775
0.800
0.825
0.850
0.875
0.900
Reward Accuracy
SW-DPO (w = 10)
SW-DPO (w = 33)
SW-DPO (w = 50)
SW-DPO (w = 66)
Figure 9: [Left] Performance of NS-DPO with values of γ > 0.9. NS-DPO shows robust performance with respect to the value
of γ, while it starts resembling the performance of stationary DPO as the value approaches very close to 1, γ > 0.97. [Right]
Expected RLHF objective gap of SW-DPO in the same experiments. The performance of SW-DPO improves as the value of w
gets closer to 33, when the algorithm is only learning from datapoints where the preference distribution stays stationary in the
given setting. The setting with w = 10 also shows final performance similar to the case of w = 33, but it shows slower training
because of the reduced amount of data used for training.
C
Offline Learning Analysis
In this section, we provide the remaining details of the analysis on the offline learning of non-stationary dataset.
Non-Linearity Coefficients.
Following the analysis from Filippi et al. (2010); Faury et al. (2021), we
capture the non-linearity of the sigmoid function in the NS-DPO loss. We use the coefficients kσ,τ, cσ,τ, which
are the supremum and infimum of ˙σ(τ⟨ϕ(x, a) −ϕ(x, a′), θ⟩) over x ∈X, (a, a′) ∈A2, θ ∈Θ respectively:
kσ,τ =
sup
x∈X,(a,a′)∈A2,θ∈Θ
˙σ(τ⟨ϕ(x, a) −ϕ(x, a′), θ⟩),
(22)
cσ,τ =
inf
x∈X,(a,a′)∈A2,θ∈Θ ˙σ(τ⟨ϕ(x, a) −ϕ(x, a′), θ⟩),
(23)
while we use Rσ,τ = kσ,τ/cσ,τ to denote the ratio between kσ,τ and cσ,τ.
Loss and gradient.
For the sake of clarity in our analysis, we draw parallels between the NS-DPO objective
in Eq. (10) and the logistic regression objective used in the generalised linear bandit setting of (Faury et al.,
2021). We re-write the NS-DPO loss with a preference label oi ∈{0, 1} and a ℓ2 regularisation term λcσ,τ τ 2
2
∥θ∥2:
LNS(θ) = −1
n
n
X
i=1
h
γT −ti−1
oi log σ(hθ(xi, ai, a′
i)) + (1 −oi) log σ(hθ(xi, a′
i, ai))
	i
+ λcσ,ττ 2
2
∥θ∥2,
(24)
where hθ(s, a, a′) = τ log

πθ(x,a)
πref(x,a)

−τ log

πθ(x,a′)
πref(x,a′)

is used for brevity.
Here oi = 1 for ai ≻a′
i and oi = 0 when a′
i ≻ai. We assume the preference label oi is sampled from a
Dynamic Bradley-Terry model with the true unknown environment parameter θ∗
ti. Under this assumption,
the mean of the preference label is E[oi|{xi, ai, a′
i, ti}] = σ(hθ∗
ti(xi, ai, a′
i)). When there is only a unilateral
preference sampled for a given prompt-response pairs, the sigmoid function forces the implicit rewards of
DPO to have infinitely large scale, driving p(a ≻a′) to either 1 or 0 (Azar et al., 2024). The ℓ2 regularisation
term in our analysis mitigates this problem, by controlling the parameter norm. Differentiating Eq. (24) with
respect to the parameter θ results in
∇θLNS(θ) = −1
n
n
X
i=1
τγT −ti−1oi ˆϕi + 1
n
n
X
i=1
h
τγT −ti−1σ(hθ(xi, ai, a′
i))ˆϕi
i
+ λcσ,ττ 2θ
|
{z
}
:=gτ (θ)
,
(25)
18

where ˆϕi = ϕ(xi, ai) −ϕ(xi, a′
i) is also introduced for brevity. We denote the parameter-dependent part of
the gradient as gτ(θ) = 1
n
Pn
i=1
h
τγT −ti−1σ(hθ(xi, ai, a′
i))ˆϕi
i
+ λcσ,ττ 2θ which we will use to analyse the
parameter estimation error.
Parameter Projection.
Let ˆθT denote the parameter minimising the NS-DPO loss defined in Eq. (24),
ˆθT = arg minθ∈Rd LNS(θ). Due to both learning and tracking aspects of the estimation error, we cannot
guarantee that ˆθT is within the boundary of the parameter presented in Assumption 4.1, ˆθT ∈Θ. This
motivates a parameter projection method, which enables finding an admissible parameter ˜θT ∈Θ while
minimising its deviation from ˆθT (Faury et al., 2021; Wang et al., 2023). Using ˜θT in the performance
analysis of NS-DPO allows preventing the potential violation of Assumption 4.1 when ˆθT is used. We perform
parameter projection by calculating ˆθT by
˜θT = arg min
θ∈Θ
∥gτ(ˆθT ) −gτ(θ)∥(ˆΣ+λI)−1,
(26)
using ˆΣ defined in Eq. (17) and gτ(θ) defined in Eq. (25).
Covariance matrices.
In addition to ˆΣ defined in Eq. (17) we also define ˜Σ, to which squared discount
weights are applied:
˜Σ = 1
n
n
X
i=1
γ2T −2ti−2(ϕ(xi, ai) −ϕ(xi, a′
i))(ϕ(xi, ai) −ϕ(xi, a′
i))⊺.
(27)
Due to its squared application of the exponential weighting, ˆΣ ≻˜Σ.
C.1
Estimation Error
Theorem C.1. (Estimation error of ˜θT .) Let δ ∈(0, 1], λ > 0, τ > 0. Let ˆθT denote the minimiser of the
NS-DPO loss defined in Eq. (24) on an offline dataset. Let ˜θT denote the parameter obtained by performing
the parameter projection procedure on ˆθT . Then with probability at least 1 −δ:
∥˜θT −θ∗
T ∥ˆΣ+λI ≤2
√
λW + 2C1
τcσ,τ
r
d + log(1/δ)
n
|
{z
}
learning
+ 16LRσ,τ ¯m
T(1 −γ)
3
2
r
d ¯m
n BT
|
{z
}
tracking
(28)
where C1 > 0 is a constant.
Estimation errors in typical stationary settings can be considered as learning errors, which are caused by
having finite data sampled stochastically. In time-varying settings, the parameter estimation suffers from
tracking error as well, which is caused by the drift of the underlying true parameter along the time steps
(Faury et al., 2021; Wang et al., 2023). In this section, we show how these errors can be disentangled and
bounded separately. To do this, we apply the approach of (Wang et al., 2023) in contextual bandit setting to
our setting of offline preference learning.
C.1.1
Bound Decomposition
We begin with the deviation between the optimal parameter θ∗
T and ˜θT , the projected parameter of the
NS-DPO estimator ˆθT :
gτ(˜θT ) −gτ(θ∗
T ) = 1
n
n
X
i=1
τγT −1−ti 
σ(h˜θT (xi, ai, a′
i)) −σ(hθ∗
T (xi, ai, a′
i))
 ˆϕi + λcσ,ττ 2(˜θT −θ∗
T ).
(29)
19

Applying the mean value theorem to the difference of sigmoid functions in Eq. (29) we get
gτ(˜θT ) −gτ(θ∗
T ) = 1
n
n
X
i=1
τ 2γT −1−ti
Z 1
v=0
˙σ(τ⟨ˆϕi, (1 −v)θ∗
T + v˜θT ⟩)dv

ˆϕi ˆϕ⊺
i (˜θT −θ∗
T )
+ λcσ,ττ 2(˜θT −θ∗
T ).
We can now define a matrix GT to define the relation between gτ(˜θT ) −gτ(θ∗
T ) and ˜θT −θ∗
T :
GT := 1
n
n
X
i=1
γT −1−ti
Z 1
v=0
˙σ(τ⟨ˆϕi, (1 −v)θ∗
T + v˜θT ⟩)dv

|
{z
}
α(i,θ∗
T ,˜θT )
ˆϕi ˆϕ⊺
i + λcσ,τI,
(30)
gτ(˜θT ) −gτ(θ∗
T ) = τ 2 · GT · (˜θT −θ∗
T ).
(31)
We make a brief aside to show GT ⪰cσ,τ(ˆΣ + λI) ⪰0 (Faury et al., 2020; Filippi et al., 2010), as this is an
important property of GT and one we will use later in the main proof. To prove this, we first show that
α(i, θ∗
T , ˜θT ) > cσ,τ. α(i, θ1, θ2) is the mean value of ˙σ along the path between some points ⟨ˆϕ, θ1⟩and ⟨ˆϕ, θ2⟩.
This is greater than the infimum of ˙σ at a point along that path, which is in turn greater than the infimum
of ˙σ in the space of parameters θ ∈Θ. The last infimum is the definition of cσ,τ Eq. (23). Then
α(i, θ1, θ2) =
Z v=1
v=0
˙σ(τ(vϕ⊺
i θ1 −(1 −v)ϕ⊺
i θ2))dv ≥infc∈[ϕ⊺
i θ1,ϕ⊺
i θ2][ ˙σ(c)]
≥infϕ∈Φ,θ∈Θ[ ˙σ(τϕ⊺θ)] = cσ,τ > 0.
(32)
α(i, θ1, θ2) > 0 comes from the fact that the logistic sigmoid function is strictly increasing and has a gradient
greater than zero at every point. Because of this inequality, each element of GT denoted by [GT ]lk∀l, k ∈[d],
is strictly larger than each element of cσ,τ[ˆΣ]lk. We use this to prove that GT ⪰cσ,τ(ˆΣ + λI) for any
v = θ1 −θ2. We first remind the reader of the definition of ˆΣ:
ˆΣ = 1
n
n
X
i=1
γT −ti−1(ϕ(xi, ai) −ϕ(xi, a′
i))(ϕ(xi, ai) −ϕ(xi, a′
i))⊺.
We then prove the inequality, using the fact that α and γ do not depend upon the indices l, k of the vector v
to move the sum across indices within the sum over the datapoints
v⊺GT v =
X
(l,k)∈[d]2
h 1
n
n
X
i=1
γT −1−tiα(i, θ1, θ2)ˆϕi ˆϕ⊺
i + λcσ,τI
i
lkvlvk
=

1
n
n
X
i=1
γT −1−tiα(i, θ1, θ2)
X
(l,k)∈[d]2
h
ˆϕi ˆϕ⊺
i
i
lkvlvk

+ λcσ,τ
X
l∈[d]
v2
l
≥

1
n
n
X
i=1
γT −1−ticσ,τ
X
(l,k)∈[d]2
h
ˆϕi ˆϕ⊺
i
i
lkvlvk

+ λcσ,τ
X
l∈[d]
v2
l
(33)
= cσ,τ
X
(l,k)∈[d]2
h 1
n
n
X
i=1
γT −1−ti ˆϕi ˆϕ⊺
i + λI
|
{z
}
ˆΣ+λI
i
lkvlvk = cσ,τv⊺(ˆΣ + λI)v.
(34)
We now continue applying Eq. (31) to bound the estimation error term:
∥˜θT −θ∗
T ∥ˆΣ+λI = 1
τ 2 ∥G−1
T (gτ(˜θT ) −gτ(θ∗
T ))∥ˆΣ+λI.
(35)
20

We use Eq. (34) to apply GT
−1 ≺
1
cσ,τ (ˆΣ + λI)−1:
1
τ 2 ∥G−1
T (gτ(˜θT ) −gτ(θ∗
T ))∥ˆΣ+λI ≺
1
τ 2cσ,τ
∥gτ(˜θT ) −gτ(θ∗
T )∥(ˆΣ+λI)−1.
(36)
We add and subtract gτ(ˆθT ) inside Eq. (36), and apply triangle inequality to derive
1
τ 2cσ,τ
∥gτ(˜θT ) −gτ(θ∗
T )∥(ˆΣ+λI)−1
=
1
τ 2cσ,τ
∥gτ(˜θT ) −gτ(ˆθT ) + gτ(ˆθT ) −gτ(θ∗
T )∥(ˆΣ+λI)−1
≤
1
τ 2cσ,τ

∥gτ(˜θT ) −gτ(ˆθT )∥(ˆΣ+λI)−1 + ∥gτ(ˆθT ) −gτ(θ∗
T )∥(ˆΣ+λI)−1

.
(37)
We use the definition of ˜θT from Eq. (26) to derive ∥gτ(˜θT ) −gτ(ˆθT )∥(ˆΣ+λI)−1 ≤∥gτ(ˆθT ) −gτ(θ∗
T )∥(ˆΣ+λI)−1
and get
1
τ 2cσ,τ

∥gτ(˜θT ) −gτ(ˆθT )∥(ˆΣ+λI)−1 + ∥gτ(ˆθT ) −gτ(θ∗
T )∥(ˆΣ+λI)−1

≤
2
τ 2cσ,τ
∥gτ(ˆθT ) −gτ(θ∗
T )∥(ˆΣ+λI)−1.
(38)
We remind the definition of ˆθT , which minimises the gradient of the loss defined in Eq. (25), making
∇LNS(ˆθT ) = 0:
∇LNS(ˆθT ) = 1
n
n
X
i=1
τγT −1−tih
σ(τ⟨ˆϕi, ˆθT −θref⟩) −oi
i
ˆϕi + λcσ,ττ 2ˆθT = 0.
(39)
We rearrange the terms in Eq. (39) to derive gτ(ˆθT ) on one side of the equation:
1
n
n
X
i=1
τγT −1−tiσ(τ⟨ˆϕi, ˆθT −θref⟩)ˆϕi + λcσ,ττ 2ˆθT
|
{z
}
=gτ (ˆθT )
= 1
n
n
X
i=1
τγT −1−tioi ˆϕi.
(40)
We apply the result of Eq. (40) to obtain
gτ(ˆθT ) −gτ(θ∗
T ) = 1
n
n
X
i=1
τγT −1−ti[oi −σ(hθ∗
T (xi, ai, a′
i))]ˆϕi −λcσ,ττ 2θ∗
T .
(41)
Using the fact that the preference label oi is obtained from the optimal parameter at time step ti, we define
ϵi = oi −σ(τ⟨ˆϕi, θ∗
ti −θref⟩), and use oi = ϵi + σ(τ⟨ˆϕi, θ∗
ti −θref⟩) to get
1
n
n
X
i=1
τγT −1−ti[oi −σ(hθ∗
T (xi, ai, a′
i))]ˆϕi −λcσ,ττ 2θ∗
T
= 1
n
n
X
i=1
τγT −1−ti[ϵi + σ(τ⟨ˆϕi, θ∗
ti −θref⟩) −σ(hθ∗
T (xi, ai, a′
i))]ˆϕi −λcσ,ττ 2θ∗
T
= 1
n
n
X
i=1
τγT −1−ti[σ(τ⟨ˆϕi, θ∗
ti −θref⟩) −σ(hθ∗
T (xi, ai, a′
i))]ˆϕi
|
{z
}
tracking
+ 1
n
n
X
i=1
τγT −1−tiϵi ˆϕi −λcσ,ττ 2θ∗
T
|
{z
}
learning
.
(42)
21

We use terms in Eq. (42) with Eq. (38) to define learning error and tracking error:
ξlearn =
2
τ 2cσ,τ
∥1
n
n
X
i=1
τγT −1−tiϵi ˆϕi −λcσ,ττ 2θ∗
T ∥(ˆΣ+λI)−1
(43)
ξtrack =
2
τ 2cσ,τ
∥1
n
n
X
i=1
τγT −1−ti[σ(τ⟨ˆϕi, θ∗
ti −θref⟩) −σ(hθ∗
T (xi, ai, a′
i))]ˆϕi∥(ˆΣ+λI)−1.
(44)
Bounding each of Eq. (43) and Eq. (44) results in Theorem C.1. The detailed bounds for the tracking and
learning terms are provided in Appendix C.1.2 and Appendix C.1.3 respectively.
C.1.2
Confidence Sets: Learning
We begin with the definition of the learning error:
ξlearn =
2
τ 2cσ,τ
∥1
n
n
X
i=1
τγT −1−tiϵi ˆϕi −λcσ,ττ 2θ∗
T ∥(ˆΣ+λI)−1.
(45)
We bound the norm of Eq. (45) with respect to ˜Σ + λI, using the fact that ˆΣ ≻˜Σ and ˜Σ + λI ⪰λI:
 1
n
n
X
i=1
τγT −1−tiϵi ˆϕi −λcσ,ττ 2θ∗
T

(ˆΣ+λI)−1
≤
 1
n
n
X
i=1
τγT −1−tiϵi ˆϕi −λcσ,ττ 2θ∗
T

(˜Σ+λI)−1
≤∥λcσ,ττ 2θ∗
T ∥(λI)−1 +
 1
n
n
X
i=1
τγT −1−tiϵi ˆϕi

(˜Σ+λI)−1
≤τ 2√
λcσ,τW +
 1
n
n
X
i=1
τγT −1−tiϵi ˆϕi

(˜Σ+λI)−1.
(46)
We can use the ϵi’s property of being a sub-Gaussian random variable, sampled i.i.d. during the creation
of the dataset. We apply Theorem 2.1 of (Hsu et al., 2012) to Eq. (46), resulting in a bound holding with
probability at least 1 −δ:
 1
n
n
X
i=1
τγT −1−tiϵi ˆϕi

(˜Σ+λI)−1 ≤τC1
r
d + log(1/δ)
n
= βT (δ),
(47)
where C1 denotes a constant introduced for bounding purpose. We provide the details of applying (Hsu et al.,
2012)’s theorem in Appendix C.3.
We now go back to the original definition of learning error term ξlearn and bound it. We use the result in
Eq. (46) and Eq. (47) to derive
ξlearn =
2
τ 2cσ,τ
∥1
n
n
X
i=1
τγT −1−tiϵi ˆϕi −λcσ,ττ 2θ∗
T ∥(ˆΣ+λI)−1
=
2
τ 2cσ,τ
 
τ 2√
λcσ,τW + τC1
r
d + log(1/δ)
n
!
= 2
√
λW + 2C1
τcσ,τ
r
d + log(1/δ)
n
,
(48)
which finishes the bounding of the learning error.
22

C.1.3
Estimation Error: Tracking
We begin with the definition of the tracking error:
ξtrack =
2
τ 2cσ,τ
 1
n
n
X
i=1
τγT −1−ti[σ(τ⟨ˆϕi, θ∗
ti −θref⟩) −σ(hθ∗
T (xi, ai, a′
i))]ˆϕi

(ˆΣ+λI)−1
=
2
τ 2cσ,τ
 1
n
n
X
i=1
τγT −1−ti[σ(τ⟨ˆϕi, θ∗
ti −θref⟩) −σ(τ⟨ˆϕi, θ∗
T −θref⟩)]ˆϕi

(ˆΣ+λI)−1.
(49)
We remind that using Eq. (32), α(i, θ∗
ti, θ∗
T ) is
α(i, θ∗
ti, θ∗
T ) :=
Z 1
v=0
˙σ(τ⟨ˆϕi, (1 −v)θ∗
T + vθ∗
ti⟩)dv.
(50)
Applying the man value theorem to Eq. (49), we obtain
2
τ 2cσ,τ
 1
n
n
X
i=1
τγT −1−ti[σ(τ⟨ˆϕi, θ∗
ti −θref⟩) −σ(τ⟨ˆϕi, θ∗
T −θref⟩)]ˆϕi

(ˆΣ+λI)−1
=
2
τ 2cσ,τ
 1
n
n
X
i=1
τ 2γT −1−tiα(i, θ∗
ti, θ∗
T )ˆϕi ˆϕ⊺
i (θ∗
ti −θ∗
T )

(ˆΣ+λI)−1.
(51)
We apply telescopic sum, which separates θ∗
ti −θ∗
T into differences of the optimal parameters between each
datapoint:
 1
n
n
X
i=1
τ 2γT −1−tiα(i, θ∗
ti, θ∗
T )ˆϕi ˆϕ⊺
i (θ∗
ti −θ∗
T )

(ˆΣ+λI)−1
=
 1
n
n
X
i=1
τ 2γT −1−tiα(i, θ∗
ti, θ∗
T )ˆϕi ˆϕ⊺
i

n
X
p=i
(θ∗
tp −θ∗
tp+1)

(ˆΣ+λI)−1,
(52)
where we use tn+1 to denote T.
Then we use Pn
i=k
Pn
j=i ai,j = Pn
j=k
Pj
i=k ai,j to rearrange the terms inside the summation:
 1
n
n
X
i=1
τ 2γT −1−tiα(i, θ∗
ti, θ∗
T )ˆϕi ˆϕ⊺
i

n
X
p=i
(θ∗
tp −θ∗
tp+1)

(ˆΣ+λI)−1
=

n
X
p=1
1
n
p
X
i=1
τ 2γT −1−tiα(i, θ∗
ti, θ∗
T )ˆϕi ˆϕ⊺
i (θ∗
tp −θ∗
tp+1)

(ˆΣ+λI)−1.
(53)
We use α(i, θ∗
ti, θ∗
T ) ≤kσ,τ using the definition of αi in Eq. (32) to get

n
X
p=1
1
n
p
X
i=1
τ 2γT −1−tiα(i, θ∗
ti, θ∗
T )ˆϕi ˆϕ⊺
i (θ∗
tp −θ∗
tp+1)

(ˆΣ+λI)−1
≤τ 2kσ,τ

n
X
p=1
1
n
p
X
i=1
γT −1−ti ˆϕi ˆϕ⊺
i (θ∗
tp −θ∗
tp+1)

(ˆΣ+λI)−1.
(54)
We then apply triangle inequality and Cauchy-Schwarz inequality to get
τ 2kσ,τ

n
X
p=1
1
n
p
X
i=1
γT −1−ti ˆϕi ˆϕ⊺
i (θ∗
tp −θ∗
tp+1)

(ˆΣ+λI)−1
≤τ 2kσ,τ
n
X
p=1
 1
n
p
X
i=1
γT −1−ti ˆϕi∥ˆϕ⊺
i ∥2∥θ∗
tp −θ∗
tp+1∥2

(ˆΣ+λI)−1.
(55)
23

We use ∥ˆϕ∥≤2L and arrange terms to obtain
τ 2kσ,τ
n
X
p=1
 1
n
p
X
i=1
γT −1−ti ˆϕi∥ˆϕ⊺
i ∥2∥θ∗
tp −θ∗
tp+1∥2

(ˆΣ+λI)−1
≤2Lτ 2kσ,τ
n
X
p=1
1
n
p
X
i=1
γT −1−ti∥ˆϕi∥(ˆΣ+λI)−1
|
{z
}
=v1
∥θ∗
tp −θ∗
tp+1∥2.
(56)
Here we bound the term v1. We first apply Jensen’s inequality to derive
v1 ≤
v
u
u
t 1
n
p
X
i=1
γT −1−ti
v
u
u
t 1
n
p
X
i=1
γT −1−ti∥ˆϕi∥2
(ˆΣ+λI)−1
= γ
T −1
2
v
u
u
t 1
n
p
X
i=1
γ−ti
v
u
u
t 1
n
p
X
i=1
γT −1−ti∥ˆϕi∥2
(ˆΣ+λI)−1.
(57)
We then use the property of trace operation and ˆΣ ≻Pp
i=1 γT −1−ti ˆϕi ˆϕ⊺
i from Eq. (17) to get
1
n
p
X
i=1
γT −1−ti∥ˆϕi∥2
(ˆΣ+λI)−1 = 1
n
p
X
i=1
γT −1−titr

ˆϕ⊺
i (ˆΣ + λI)−1 ˆϕi

= tr
 
(ˆΣ + λI)−1 1
n
p
X
i=1
γT −1−ti ˆϕi ˆϕ⊺
i
!
≤tr (Id) = d.
(58)
We apply Assumption 4.5 here. Because each time step can have at maximum ¯m datapoints, we can upper
bound 1
n
Pp
i=1 γ−ti with
1
n
p
X
i=1
γ−ti ≤¯m
n
t
X
k=1
γ−k = ¯mγ(γ−(t+1) −1)
n(1 −γ)
,
(59)
where t =
l
|[p]|
¯m
m
. We combine Eq. (58) and Eq. (59) to obtain
2Lτ 2kσ,τ
n
X
p=1
1
n
p
X
i=1
γT −1−ti∥ˆϕi∥(ˆΣ+λI)−1∥θ∗
tp −θ∗
tp+1∥2
≤2Lτ 2kσ,τ
n
X
p=1
γ
T −1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
∥θ∗
tp −θ∗
tp+1∥2.
(60)
We apply Assumption 4.5 again to upper bound the summation as Pn
p=1 vp ≤¯m PT −1
t=1 vt, getting
2Lτ 2kσ,τ
n
X
p=1
γ
T −1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
∥θ∗
tp −θ∗
tp+1∥2
≤2Lτ 2kσ,τ ¯m
T −1
X
t=1
γ
T −1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
∥θ∗
t −θ∗
t+1∥2.
(61)
24

We apply v = 1
T
PT
k=1 v to introduce another summation:
2Lτ 2kσ,τ ¯m
T −1
X
t=1
γ
T −1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
∥θ∗
t −θ∗
t+1∥2
= 2Lτ 2kσ,τ ¯m
T
T
X
k=1
T −1
X
t=1
γ
T −1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
∥θ∗
t −θ∗
t+1∥2.
(62)
Because γ < 1, we can bound
T
X
k=1
T −1
X
t=1
γ
T −1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
≤2
T −1
X
t=1
T
X
k=t+1
γ
k−1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
(63)
and apply geometric sum to obtain
2
T −1
X
t=1
T
X
k=t+1
γ
k−1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
= 2
T −1
X
t=1
γ
t
2 −γ
T
2
1 −γ
1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
.
(64)
We use γ < 1 again to derive 1+γ
1
2
2
< 1, and get
2
T −1
X
t=1
γ
t
2 −γ
T
2
1 −γ
1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
≤2
T −1
X
t=1
γ
t
2 −γ
T
2
1 −γ
1
2 1+γ
1
2
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
= 4
T −1
X
t=1
γ
t
2 −γ
T
2
1 −γ
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
.
(65)
We then use

γ
t
2 −γ
T
2
 p
γ(γ−(t+1) −1) ≤γ
t
2 γ−t
2 = 1 to derive
4
T −1
X
t=1
γ
t
2 −γ
T
2
1 −γ
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
≤4
r
d ¯m
n
T −1
X
t=1
1
(1 −γ)
3
2
.
(66)
We use the result from Eq. (66) to Eq. (62), and use the definition of variation budget BT from Assumption
4.3 to get
2Lτ 2kσ,τ ¯m
T
T
X
k=1
T −1
X
t=1
γ
T −1
2
s
d ¯mγ(γ−(t+1) −1)
n(1 −γ)
∥θ∗
t −θ∗
t+1∥2
≤8Lτ 2kσ,τ ¯m
T
r
d ¯m
n
T −1
X
t=1
1
(1 −γ)
3
2
∥θ∗
t −θ∗
t+1∥2
≤8Lτ 2kσ,τ ¯m
T(1 −γ)
3
2
r
d ¯m
n BT .
(67)
We now combine Eq. (67) with Eq. (44) to derive the full bound of the tracking error:
ξtrack = 16LRσ,τ ¯m
T(1 −γ)
3
2
r
d ¯m
n BT .
(68)
25

We now use Eq. (68) with Eq. (48) to obtain the full estimation error:
∥ˆθT −ˆθ
∗
T ∥ˆΣ+λI ≤ξlearn + ξtrack
≤2
√
λW + 2C1
τcσ,τ
r
d + log(1/δ)
n
+ 16LRσ,τ ¯m
T(1 −γ)
3
2
r
d ¯m
n BT ,
(69)
which concludes the analysis for Theorem C.1.
C.2
Regret Bound
Theorem 4.1. (Regret bound of ˜θT ) Let δ ∈(0, 1
2], τ > 0 and r∗
T (x, a) < rmax ∀x ∈X, a ∈A. Let ˜θT denote
the parameter in Θ which minimises the NS-DPO loss (Eq. (24)) on an offline dataset. When γ = 1−
  BT
dT
1/2,
the following bound holds with probability at least 1 −2δ:
Roff
T ≤rmaxp
¯mT(1 −γ)κ
C2
p
2m(1 −γT −1)
 
2
√
λW + 2C1
τcσ,τ
r
d + log(1/δ)
n
+ 16LRσ,τ ¯m
T(1 −γ)
3
2
r
d ¯m
n BT
!
,
where C1 > 0 and 0 < C2 < 1 denote constants. In other words, when λ = O
  d
n

, Roff
T
satisfies:
Roff
T = ˜O

d B1/2
T
n−1/4
.
C.2.1
Population Covariance of Feature Differences
Let Σπref,diff define the population covariance matrix of the feature differences:
Σπref,diff = E[ˆϕˆϕ⊺],
(70)
where ˆϕ = ϕ(x, a)−ϕ(x, a′) denotes the feature difference vector, and the expectation is computed with respect
to x ∼X, t ∼T , a, a′ ∼πref(·|x). We also define the discounted population covariance matrix Σγ
πref,diff:
Σγ
πref,diff = E[γT −1−t ˆϕˆϕ⊺],
(71)
where the expectation is computed with respect to the same distributions as Σπref,diff.
We then define ωupp(T, γ):
ωupp(T, γ) = sup
v∈Rd
v⊺Σπref,diffv
v⊺Σγ
πref,diffv ,
(72)
Without any assumptions on the time distribution, ωupp(T, γ) ≤γ−(T −1), which happens when all the
datapoints come from the oldest time step. We use Assumption 4.5 to obtain a tighter upper bound of ωupp.
Using m(T −1) ≤n ≤¯m(T −1), we can get
1
n
n
X
i=1
γT −1−ti ≥m
n ·
T −1
X
t=1
γT −1−t ≥
m
¯m(T −1) ·
T −1
X
t=1
γT −1−t.
(73)
We note that the prompt distribution X and the reference policy πref are independent from the time step
distribution T . Using Eq. (73), we obtain
v⊺Σγ
πref,diffv ≥
 
m
¯m(T −1)
T −2
X
i=0
γi
!
· (v⊺Σπref,diffv) =
m(1 −γT −1)
¯m(T −1)(1 −γ) · (v⊺Σπref,diffv),
(74)
which implies ωupp(T, γ) ≤¯m(T −1)(1−γ)
m(1−γT −1) .
26

C.2.2
Decomposing Regret Bound
In order to decompose and bound the detailed elements of the regret bound, we first show the relation
between the regret and the estimation error of the model parameters.
Theorem C.2. Let δ ∈[0, 1] and r∗
T (x, a) < rmax ∀x ∈X, a ∈A. Let ˜θT denote the parameter obtained by
performing the parameter projection in Appendix C, after training with the NS-DPO loss defined in Eq. (24)
on an offline dataset. When γ = 1 −
  BT
dT
1/2, with probability at least 1 −δ:
Roff
T ≤rmaxp
¯mT(1 −γ)κ
C2
p
2m(1 −γT −1)
∥˜θT −θ∗
T ∥ˆΣ+λI,
(75)
where 0 < C2 < 1 is a constant.
Let π˜θT denote the policy we obtained by training with NS-DPO and performing parameter projection. We
use Σπ˜
θ to denote the population covariance matrix, whose expectation taken with respect to π˜θ. The initial
part of the proof follows the derivation presented in the regret analysis of (Chowdhury et al., 2024), so we
skip the intermediate steps and directly present Eq. (76):
Roff
T ≤rmax∥˜θT −θ∗
T ∥Σπ˜
θT
(Section A.5. of (Chowdhury et al., 2024)),
(76)
where rmax refers to the maximum value of the reward. We then convert the parameter difference norm into
one using ˆΣ + λI defined in Eq. (17):
rmax∥˜θT −θ∗
T ∥Σπ˜
θT ≤rmax∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t
(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺(ˆΣ + λI)(˜θT −θ∗
T )
.
(77)
We now use the following lemma from (Chowdhury et al., 2024), which relies on the matrix concentration
inequality to explain the difference between ˆΣ and Σγ
πref,diff.
Lemma C.1. (Lemma A.1. of (Chowdhury et al., 2024)) With probability at least 1 −δ, for some universal
constant C, we have
∥ˆΣ −Σγ
πref,diff∥2 ≤C
p
d log(4d/δ)/n.
(78)
We use the result of Lemma C.2 proved in Appendix C.4 with Lemma C.1 to obtain
ˆΣ ⪰C2Σγ
πref,diff
(79)
for a constant 0 < C2 < 1 and the discount weight γ set to
γ = 1 −
BT
dT
1/2
.
(80)
We use Eq. (79) to derive
rmax∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t
(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺(ˆΣ + λI)(˜θT −θ∗
T )
≤rmax
C2
∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t
(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺(Σγ
πref,diff)(˜θT −θ∗
T )
.
(81)
27

We then apply the result from Eq. (72) which implies (∥v∥Σγ
πref ,diff)−1 ≤
p
ωupp(T, γ)(∥v∥Σπref ,diff)−1:
rmax
C2
∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t
(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺(Σγ
πref,diff)(˜θT −θ∗
T )
≤rmaxp
ωupp(T, γ)
C2
∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t
(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺(Σπref,diff)(˜θT −θ∗
T )
.
(82)
From the definition of Σπref,diff in Eq. (70), a, a′ are independently sampled. We combine this fact with the
population covariance matrix Σπref, deriving Σπref,diff = 2Σπref. We use this to get
rmaxp
ωupp(T, γ)
C2
∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t
(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺(Σπref,diff)(˜θT −θ∗
T )
≤rmaxp
ωupp(T, γ)
C2
√
2
∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺Σπref(˜θT −θ∗
T )
. (83)
We use κ = maxπ∈Π κπ with the definition of κπ in Eq. (16), along with the result obtained in Eq. (74) to use
ωupp(T, γ) = (T −1)(1−γ)
1−γT −1
≤T (1−γ)
1−γT −1 :
rmaxp
ωupp(T, γ)
C2
√
2
∥˜θT −θ∗
T ∥(ˆΣ+λI)
v
u
u
t(˜θT −θ∗
T )⊺Σπ˜
θT (˜θT −θ∗
T )
(˜θT −θ∗
T )⊺Σπref(˜θT −θ∗
T )
≤rmaxp
ωupp(T, γ)κ
C2
√
2
∥˜θT −θ∗
T ∥(ˆΣ+λI)
≤rmaxp
¯mT(1 −γ)κ
C2
p
2m(1 −γT −1)
∥˜θT −θ∗
T ∥(ˆΣ+λI).
(84)
C.2.3
Complexity Analysis
In order to investigate the complexity of the regret bound, we set the value of γ using d, T, BT . We first set
γ as
γ = 1 −
BT
dT
1/2
.
We apply (1 −γ) to each term in the estimation error, with assumption of λ = O
  d
n

from Lemma C.1, while
ignoring the logarithmic factor:
2
√
λW
(= d
1
2 n−1
2 )
2C1
τcσ,τ
r
d + log(1/δ)
n
(= d
1
2 n−1
2 )
16LRσ,τ ¯m
T(1 −γ)
3
2
r
d ¯m
n BT
(= d
5
4 B
1
4
T T −3
4 )
(85)
Here, we note that from Assumption 4.5, n = Θ(T). This allows us to consider the complexity with respect
to the dataset size n and T together. We can conclude from Eq. (85) that the complexity bound of the entire
28

estimation error is O(d
5
4 B
1
4
T T −1
2 ). By setting the value of T to a sufficiently large one, making 1−γT −1 ≥1
2,
then the complexity bound of
p
ωupp(T, γ) is
p
T(1 −γ)
(= d−1
4 B
1
4
T T
1
4 ).
(86)
Finally we present the total complexity bound of the algorithm, by applying the complexity of
p
ωupp(T, γ)
in Eq. (86) to each of the estimation error term:
Roff
T = O(d B
1
2
T T −1
4 )
= O(d B
1
2
T n−1
4 ).
(87)
C.3
Details of applying Bernstein’s inequality
We restate the norm to investigate:
∥1
n
n
X
i=1
τγT −1−tiϵi ˆϕi∥(˜Σ+λI)−1.
(88)
We then define two vectors V and Z, followed by a matrix M:
V = [ϵ1, . . . , ϵn],
(89)
Z = [γT −1−t1 ˆϕ1, . . . , γT −1−tn ˆϕn],
(90)
M = 1
n2 Z(˜Σ + λI)−1Z⊺.
(91)
We then express Eq. (88) using V, Z, M:
∥1
n
n
X
i=1
τγT −1−tiϵi ˆϕi∥(˜Σ+λI)−1 =
√
τ 2V ⊺MV .
(92)
We here recall the definition of ϵi, which is a 1-sub-Gaussian random variable:
ϵi = oi −σ(τ⟨ˆϕi, θ∗
ti −θref⟩),
Eoi∼pti(ai≻a′
i|xi)[ϵi] = 0,
(93)
Varoi∼pti(ai≻a′
i|xi)[ϵi] = Eoi∼pti(ai≻a′
i|xi)[ϵ2
i ] −(Eoi∼pti(ai≻a′
i|xi)[ϵi])2 ≤1.
(94)
As stated in (Hsu et al., 2012), the Bernstein’s inequality for sub-Gaussian random variables in quadratic
form implies
τ 2V ⊺MV ≤τ 2 
tr(M) + 2
p
tr(M ⊺M) log(1/δ) + 2∥M∥log(1/δ)

≤τ 2 · C1 · d + log(1/δ)
n
,
(95)
for some C1 > 0, while ∥M∥= λmax(M). Here we used the definition of ˜Σ in Eq. (17) to show ˜Σ = 1
nZ⊺Z,
and derive for λ > 0
M ≺1
n2 Z(˜Σ)−1Z⊺= 1
nI,
(96)
tr(M) ≤d/n,
(97)
tr(M ⊺M) ≤d/n2,
(98)
∥M∥≤1/n.
(99)
29

C.4
Setting Discount Weight γ
In this section, we prove the following lemma which poses a condition on the structure of discount weight γ.
Lemma C.2. (Setting γ) Under Assumption 4.5, there exists a constant 0 < C2 < 1 which satisfies
Σγ
πref,diff −C
p
d log(4d/δ)/n ⪰C2Σγ
πref,diff,
(100)
when γ = 1 −
  BT
dT
1/2.
We rearrange terms in Eq. (100) to get
(1 −C2)Σγ
πref,diff ⪰C
p
d log(4d/δ)/n,
(101)
which implies that λmin(Σγ
πref,diff) ≥C
p
d log(4d/δ)/n. Because prompt distribution X and the behaviour
of the reference policy πref is independent from the time distribution T , combined with Assumption 4.4,
λmin(Σγ
πref,diff) > 0 is guaranteed.
Because ωupp(T, γ)−1λmax(Σπref,diff) ⪯λmin(Σγ
πref,diff) based on Eq. (72), we derive
λmin(Σγ
πref,diff) ⪰ωupp(T, γ)−1λmax(Σπref,diff)
⪰C
p
d log(4d/δ)/n.
(102)
We apply Assumption 4.5 to use ωupp(T, γ) ≤¯m(T −1)(1−γ)
m(1−γT −1)
obtained in Eq. (74), and get
2
T(1 −γ) ≥
1 −γT −1
(T −1)(1 −γ) ≥¯mC
p
d log(4d/δ)/n
mλmax(Σπref,diff) .
(103)
By setting γ = 1 −
  BT
dT
1/2, combined with n = Θ(T) from Assumption 4.5, we show that
2
T(1 −γ) = T −1/2d1/2B−1/2
T
≥¯mC
p
d log(4d/δ)/n
mλmax(Σπref,diff) ,
(104)
which concludes the proof.
30

