ERNIE-VIL 2.0: MULTI-VIEW CONTRASTIVE LEARNING FOR
IMAGE-TEXT PRE-TRAINING
Bin Shan
Weichong Yin
Yu Sun
Hao Tian
Hua Wu
Haifeng Wang
Baidu Inc., China
{shanbin01, yinweichong, sunyu02}@baidu.com
ABSTRACT
Recent Vision-Language Pre-trained (VLP) models based on dual encoder have attracted extensive
attention from academia and industry due to their superior performance on various cross-modal
tasks and high computational efﬁciency. They attempt to learn cross-modal representation using
contrastive learning on image-text pairs, however, the built inter-modal correlations only rely on a
single view for each modality. Actually, an image or a text contains various potential views, just
as humans could capture a real-world scene via diverse descriptions or photos. In this paper, we
propose ERNIE-ViL 2.0, a Multi-View Contrastive learning framework to build intra-modal and
inter-modal correlations between diverse views simultaneously, aiming at learning a more robust
cross-modal representation. Speciﬁcally, we construct multiple views within each modality to learn
the intra-modal correlation for enhancing the single-modal representation. Besides the inherent
visual/textual views, we construct sequences of object tags as a special textual view to narrow the
cross-modal semantic gap on noisy image-text pairs. Pre-trained with 29M publicly available datasets,
ERNIE-ViL 2.0 achieves competitive results on English cross-modal retrieval. Additionally, to
generalize our method to Chinese cross-modal tasks, we train ERNIE-ViL 2.0 through scaling up
the pre-training datasets to 1.5B Chinese image-text pairs, resulting in signiﬁcant improvements
compared to previous SOTA results on Chinese cross-modal retrieval. We release our pre-trained
models in https://github.com/PaddlePaddle/ERNIE/.
1
Introduction
In the past two years, the Vision-Language Pre-training (VLP) models have achieved remarkable improvements on
various cross-modal tasks such as Visual Question Answering (VQA) and cross-modal retrieval. Most existing works
[1, 2, 3], based on cross-modal transformer encoders, focus on designing multiple proxy pre-training tasks (e.g., Masked
Language Modeling (MLM) and Masked Region Modeling (MRM)) to learn joint cross-modal representation. However,
cross-modal attention layers in the encoder aim to fuse multiple token-level visual/textual features to learn the joint
representation with massive interactions, resulting in high computation costs for real-life systems such as the online
cross-modal retrieval system. To address the limitation, recent works [4, 5, 6] based on dual-encoder architecture utilize
a compute-efﬁcient framework with light cross-modal interaction, achieving comparable results on vision-language
tasks via training on large-scale image-text pairs.
However, they attempt to build the cross-modal alignment via single-view contrastive learning since the built inter-modal
correlation only relies on a single view for each modality. Indeed, the intra-modal correlation which they neglect could
enhance the single-modal representation and contribute to building a better cross-modal alignment. Besides, there often
exists weak correlations in the noisy web-crawled image-text pairs containing inherent visual/textual views, widening
the cross-modal semantic gap.
In this paper, we propose ERNIE-ViL 2.0, a multi-view contrastive learning framework for cross-modal retrieval,
aiming to learn robust cross-modal representation through modeling both inter-modal and intra-modal correlations
between diverse views. As illustrated in Figure 1, compared to the conventional single-view contrastive learning
arXiv:2209.15270v1  [cs.CV]  30 Sep 2022

Blue sky, red car,
green sign, gray 
street, sidewalk
Pedestrian 
crossing the street 
at a traffic  signal.
Single-view contrastive learning
Multi-view contrastive learning
Contrastive Loss
Pedestrian 
crossing the street 
at a traffic  signal.
Contrastive  Loss
Contrastive  Loss
Contrastive  Loss
Figure 1: Multi-view contrastive learning vs. Single-view contrastive learning. Single-view contrastive learning only
focuses on a single inter-modal correlation between a visual and a textual view. Multi-view contrastive learning could
learn on various kinds of intra-modal and inter-modal correlations through constructing diverse potential views.
methods, multi-view contrastive learning learns on both intra-modal and inter-modal correlations between various
views, improving the robustness and generalization of the model. Similarly, CMC [7] utilizes multi-view contrastive
learning for visual representation learning, resulting in a more robust representation. Our method constructs various
visual/textual views to enhance the representations within and across modalities. Speciﬁcally, for intra-modal contrastive
view pairs, we construct image-image pairs and text-text pairs to enhance representation with each modality. In addition
to the inherent visual/textual views, we construct sequences of object tags as a special textual view to mitigate the
effects of noisy multi-modal data and ease the learning of vision-language alignment.
Based on the dual-encoder architecture, we train an English model on 29M publicly available datasets and achieve
competitive results on cross-modal retrieval tasks. We further scale up the training datasets to 1.5B Chinese image-text
pairs, resulting in signiﬁcant improvements compared to previous SOTA results on Chinese cross-modal retrieval.
Overall, we summarise our contributions as three folds:
1. We propose the ﬁrst multi-view learning framework for cross-modal retrieval, which exploits diverse views to
obtain view-invariant and robust cross-modal representations.
2. We introduce object tags as special textual views, which effectively narrow the semantics gap between
image-text and ease the learning of cross-modal alignment on large-scale noisy data.
3. We establish a solid and comparable benchmark for English cross-modal retrieval using only noisy publicly
available datasets. Furthermore, trained on 1.5B Chinese image-text pairs, our model achieves SOTA results
on Chinese cross-modal retrieval.
2
Related Work
2.1
Vision-Language Pre-training
To improve the performance of downstream multi-modal tasks, Vision-Language Pre-training (VLP) models aim
at aligning the representations in different modalities to the common space. According to the interaction between
vision-language, they could be summarized into two categories: cross-encoder models and dual-encoder models.
Cross-encoder models [1, 2, 8, 3, 9, 10] build cross-modal attention between vision and language to achieve a joint
multi-modal representation. They usually concatenate region-level visual features and token-level textual features as
inputs of the transformer-based encoder and learn cross-modal joint representation using multiple proxy loss (e.g.,
MLM, MFM). These methods greatly improve the performance of various downstream tasks, especially for complexity
reasoning V+L tasks (e.g., VQA). However, as the key ingredient, cross-modal attention brings huge computation costs
2

in training and deployment. As mentioned in [10], the inference time of UNITER [2] with 12/24-layer is 48 seconds for
text query from 5K COCO [11] images.
Dual-encoder models [5, 4, 12, 6, 13] employ a light cross-modal interaction, which only compute similarity (e.g.,
cosine similarity) between visual and textual feature generated from separate encoder once, resulting in signiﬁcantly
lowering computational costs. Utilizing contrastive learning on large-scale web-crawl image-text pairs, they achieved
several promising results on cross-modal tasks and performed remarkable zero-shot abilities. Besides, recent works
[14, 15, 9, 16] attempt to combine dual-encoder and cross-encoder into an uniﬁed framework to learn cross-modal
representation. However, they mainly rely on the dual-encoder modules when applied to cross-modal retrieval tasks.
Our method falls into the dual-encoder category and we proposes a multi-view contrastive learning framework to
improve the performance of cross-modal tasks. Recent works [12, 17] are closely related to our work, which utilize a
similar pre-training framework but focus on learning visual representation. Besides, we consider object detection tags
as text views, while they use the tags as a supervisor to construct positive image samples.
2.2
Multi-view Learning
Multi-view learning exploits the consistency and complementary properties of different views to acquire the better
generalization ability of models than single-view learning [18]. It is a general term comprising many methods: co-
training [19] and multi-kernel learning [20]. Close to the self-supervised representation learning discussed in this
paper, CMC [7] and CPC [21] attempt to learn more robust single-modal representation through using multi-view
contrastive learning (e.g., future and past of sequential data in CPC, images in different color space in CMC). Whereas
these methods present multiple deﬁnitions of views, the common insights of these works is learning view-invariant
and more robust representations across various views through maximizing their co-occurrence semantics. While these
works attempt to learn single-modal representation using multi-view learning, we focus on improving the robustness
and generalization of cross-modal representation by simultaneously learning correlations between intra-modal and
inter-modal views.
3
Method
In this section, we present ERNIE-ViL 2.0, a Multi-View Contrastive learning framework based on dual-encoder
architecture for cross-modal retrieval. We ﬁrst introduce the overview of the proposed framework in Section 3.1, then
describe how to construct various views in Section 3.2. Finally, the multi-view contrastive loss will be presented in
Section 3.3.
3.1
Framework Overview
As illustrated in Figure 2, based on dual-encoder architecture, ERNIE-ViL 2.0 learns vision-language representation
via modeling intra-modal and inter-modal correlations of diverse views simultaneously using multi-view contrastive
learning. For learning the intra-modal correlations, we implement data augmentation to construct views for each
modality. In visual modality, similar to SimCLR [22], we utilize image augmentation (e.g. random cropping, jitter) to
construct visual views. In textual modality, inspired by SimCSE [23], we consider dropout noise as text augmentation
to construct textual views. For learning the inter-modal correlations, besides inherent image-caption pairs contained in
datasets, we introduce the special textual sequence (object tags) to improve the inter-modal alignment.
We propose a multi-view contrastive loss to simultaneously learn the intra-modal and inter-modal alignments across
multiple views, consisting of I2I (image-image), I2T (image-text), T2I (text-image), and T2T (text-text).
3.2
Construction of Diverse Views
Constructing effective and diverse views from image-caption pairs is the essential ingredient of our proposed framework.
Overall, we construct six views for visual and textual modality, containing four regular views for the image and caption
and two special views built from sequences of object tags.
To construct the visual views, image augmentation (e.g., random crop and jitter) as a widely used technique for visual
representation learning is utilized to generate different visual views. Thus, we implement random image augmentation
on the same image twice to get two different visual views deﬁned as Iv1 and Iv2. Since SimCSE[23] uses dropout as a
text augmentation and presents a promising and steady performance, we feed captions to a text encoder with dropout
units twice to obtain different textual views Tv1 and Tv2.
3

Caption
Caption / Tag
Tag
/
Image Encoder
Text Encoder
I2I 
I2T
T2T
Multi-view Contrastive Loss
T2I
Visual views
Textualviews
Text augmentation 
The picture contains concrete 
sidewalk , blue sky, red car,
green sign, red sign, green 
tree … …
The picture contains concrete 
sidewalk , blue sky, red car,
green sign, red sign, green 
tree … …
The picture contains concrete 
sidewalk , blue sky, red car,
green sign, red sign, green 
tree … …
The picture contains concrete 
sidewalk , blue sky, red car,
green sign, red sign, green 
tree … …
The picture contains concrete 
sidewalk , blue sky, red car,
green sign, red sign, green 
tree … …
Pedestrian are crossing 
the street at a traffic 
signal.
Pedestrian are crossing 
the street at a traffic 
signal.
Pedestrian are crossing 
the street at a traffic 
signal.
Pedestrian are crossing 
the street at a traffic 
signal.
Pedestrian are crossing 
the street at a traffic 
signal.
Image augmentation 
Figure 2: The overview of our proposed Multi-View Contrastive learning framework (ERNIE-ViL 2.0). ERNIE-ViL
2.0 contains an image encoder and a text encoder. At each pre-training iteration, two visual views (images) and two
textual views(captions or tags) are fed into the corresponding encoder. Particularly, we randomly feed one of the texts
(captions or tags) into the encoder at each iteration. Multi-view contrastive loss sums four contrastive losses (I2I:
image-image, I2T:image-text, T2I: text-image, T2T: text-text) calculated between different views with different weights.
To enhance the alignment across modalities on noisy image-text pairs, we introduce the special textual sequence (object
tags) and apply the same text augmentation to get different views. Particularly, we construct the special textual sequence
by concatenating a prompt (e.g., The picture contains) and the attribute-object phrases extracted on raw images using a
pre-trained object detector(e.g., BUTD [24]). Besides, we keep these phrases for all the detected objects-attributes,
corresponding with all the presented objects. We consider the special textual sequence holding more coarse-grained
semantic textual units, which could be treated as a bridge between ﬁne-grained (concrete) semantic units (e.g., named
entities, low-frequency alternative name) in captions and abstract visual concepts in images, resulting in easing learning
of cross-modal alignment. Besides, the model acquires more powerful image representative abilities due to the special
textual sequence could provide information missed in captions. For a better interpretation of the relations among
different views, we show some cases in Figure 3.
3.3
Multi-view Contrastive Loss
For learning correlations of the views built in section 3.2, we propose a multi-view contrastive loss. We contrast two
intra-modal view pairs and two inter-modal view pairs selected from constructed views at each iteration in a set S:
S = {(Iv1, Iv2), (Tv1, Tv2), (Iv1, Tv1), (Tv1, Iv1)}
For intra-modal pairs, (Iv1, Iv2) and (Tv1, Tv2) denote two visual views and textual views respectively. We select view
pairs augmented on the same image or text as positives and others in the batch as negatives. For inter-modal pairs,
(Iv1, Tv1) and (Tv1, Iv1) denote bi-directional visual/textual views pairs. Speciﬁcally, T represents different textual
views (caption or tag), and we randomly sample one of them (caption or tag) at each iteration. Then, the model will
learn vision-language representation through contrasting the similarity of view pairs in S. Following InfoNCE [25], we
deﬁne the loss of each type of pair in S as
L(x,y) = −1
N
N
X
i
log
exp(hi
x
⊤hi
y/τ)
PN
j=1 exp(hix
⊤hj
y/τ)
(1)
where x and y denote different views in each pair of S, and h denotes embeddings encoded by corresponding encoders,
and τ is the temperature to scale the logits, and N denotes the number of visual/textual views in the batch. Then, we
minimize the overall contrastive loss for all view pairs with
4

Dinner at the Terrace Restaurant at Maru Maru
The picture contains silver spoon, white plate, red tomato, 
small bowl, purple onions, brown food, blue tablecloth, 
white shadow … …
Mustapha<mark> Gatos Manx
The picture contains pink nose, ears, cat, green eyes, 
whiskers, head … …
The picture contains holding hand, open window, clear 
windshield, blue jeans, clear window, white car, blue 
arm … …
BMW E90 Door Lock Replacement - Removing the 
window glass
Caption views
Tag views
Image views
Additional details
Coarse-graineddescription
Fine-grained description
Figure 3: The relations among different views: 1) Colored tokens share similar semantics, but they have different
semantic granularity in captions (coarse-grained) and tags (ﬁne-grained) view. 2) Underlined tokens in tag views
provide missed information in the captions.
Lmulti−view =
X
s∈S
λsLs
(2)
, where λ are parameters to scale different losses, s belongs to S and is one type of view pairs in S.
4
Experiment
In this section, we present the pre-training datasets of our framework in Section 4.1 and settings in Section 4.2, we
evaluate our proposed model on cross-modal retrieval tasks in Section 4.3 and other vision-language tasks in Appendix
B and C, and conduct ablation studies in Section 4.4.
4.1
Pre-training Datasets
English pre-training datasets
While many existing previous works incorporate massive private datasets such as
1.8B image-text pairs in ALIGN [5], we focus on utilizing only publicly available and fully out-of-domain datasets
to build a comparable English benchmark for cross-modal retrieval. Thus, we built our pre-training data on three
large-scale publicly available English datasets with 29M unique images: Conceptual Captions (CC) [26], Conceptual
12M [27] and ﬁltered Yahoo Flickr Creative Commons 100M [28] from [4] (remaining 14M).
Chinese pre-training datasets
To generalize our framework to Chinese cross-modal retrieval, we scale the pre-
training datasets to 1.5 billion image-text pairs consisting of the 1.1 billion web-crawled Chinese image-text pairs
and 0.4 billion publicly available cross-modal datasets (CC12M, CC3M, YFCC, and LAION[29] ). We collect 400M
image-text pairs from the public datasets in the end, for some URLs were lost. We then translate translate the captions
into Chinese by public machine translation system 1. We present the details of the English and Chinese pre-training
datasets in Table 5.
4.2
Pre-training Settings
For our English models, following previous method ALIGN [5], we consider EffcientNet-L2[30] as the image encoder
and Bert-large [31] as the text encoder. For visual inputs, we resize the images to 256×256 and implement random
crop, horizontal ﬂipping, jittering, gaussian blurring, and grayscale for image augmentation. We train the English model
with a total batch size of 7168 on 112 A100 GPUs for 90K steps. We adopt Adam optimizer [32] with a base learning
rate of 10−4, which is warmed up linearly in 2K steps and decayed to 10−6 with a cosine schedule.
1We use Baidu Translation API Service at https://fanyi.baidu.com/
5

Flickr30K (1K test set)
MSCOCO (5K test set)
Method
# Pre-train
image →text
text →image
image →text
text →image
images
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
Unicoder-VL
4M
48.4 76.0
85.2
64.3 85.8
92.3
44.9 71.2
80.4
32.3 59.0
70.2
UNITER
4M
83.6 95.7
97.7
68.7 89.2
93.9
-
-
-
-
-
-
CLIP
400M
88.0 98.7
99.4
68.7 90.6
95.2
58.4 81.5
88.1
37.8 62.4
72.2
ALIGN
1.8B
88.6 98.7
99.7
75.7 93.8
96.8
58.6 83.0
89.7
45.6 69.8
78.6
FILIP
340M
89.8 99.2
99.8
75.0 93.4
96.3
61.3 84.3
90.4
45.9 70.6
79.3
Florence
900M
90.9 99.1
-
76.7 93.6
-
64.7 85.9
-
47.2 71.4
-
FLAVA
70M
67.7 94.0
-
65.2 89.4
-
42.7 76.8
-
38.4 67.5
-
CoCa†
4.8B
92.5 99.5
99.9
80.4 95.7
97.7
66.3 86.2
91.8
51.2 74.2
82.0
ERNIE-ViL 2.0
29M
91.2 99.1
99.8
77.4 93.8
96.4
63.1 85.7
91.4
46.0 71.4
80.4
FILIP*
340M
95.4 99.8 100.0 84.7 97.0
98.7
-
-
-
-
-
-
ALBEF*
4M
94.1 99.5
99.7
82.8 96.3
98.1
-
-
-
-
-
-
ERNIE-ViL 2.0*
29M
96.1 99.9 100.0 85.0 97.0
98.3
-
-
-
-
-
-
Table 1: Zero-shot English cross-modal retrieval results on Flickr30K and MSCOCO datasets, compared with previous
best VLP models with or without in-domain datasets based on different architecture. *: the model ﬁne-tuned on
MSCOCO dataset. CoCa† uses 165x more image-text pairs.
Flickr30K (1K test set)
MSCOCO (5K test set)
Method
# Pre-train
image →text
text →image
image →text
text →image
images
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
UNITER
4M
87.3 98.0
99.2
75.6 94.1
96.8
65.7 88.6
93.8
52.9 79.9
88.0
OSCAR
4M
-
-
-
-
-
-
73.5 92.2
96.0
57.5 82.8
89.8
ERNIE-ViL
4M
88.7 97.3
99.1
75.1 93.4
96.3
-
-
-
-
-
-
ALIGN
1.8B
95.3 99.8 100.0 84.9 97.4
98.6
77.0 93.5
96.9
59.9 83.3
89.8
ALBEF
14M
95.9 99.8 100.0 85.6 97.5
98.9
77.6 94.3
97.2
60.7 84.3
90.5
FILIP
340M
96.6 100.0 100.0 87.1 97.7
99.1
78.9 94.4
97.4
61.2 84.3
90.5
Florence
900M
97.2 99.9
-
87.9 98.1
-
81.8 95.2
-
63.2 85.7
-
ERNIE-ViL 2.0
29M
97.2 100.0 100.0 93.3 99.4
99.8
77.4 93.6
97.1
59.5 83.4
90.1
Table 2: Fine-tuned English cross-modal retrieval results on Flickr30K and MSCOCO datasets, compared with previous
best VLP models with or without in-domain datasets based on different architecture.
We build our Chinese model following the similar settings except initializing the textual encoder with ERNIE [33, 34, 35]
and training with a total batch size of 23200. Besides, we explore another visual backbone widely used in VLP (i.e.,
Vision Transformer (ViT) [36]) as our image encoder (the details in Appendix D ).
AIC-ICC
COCO-CN
Models
image →text
text→image
mean
image→text
text→image
mean
R@1 R@5 R@10 R@1 R@5 R@10 Recall R@1 R@5 R@10 R@1 R@5 R@10 Recall
BriVL
20.3 37.0
45.6
14.4 30.4
39.1
31.1
-
-
-
-
-
-
-
Taiyi
-
-
-
-
-
-
-
-
-
-
51.5 81.0
90.4
-
R2D2
30.7 47.2
52.9
14.9 28.1
33.4
34.5
63.3 89.3
95.7
56.4 85.0
93.1
80.5
Wukong
-
-
-
-
-
-
-
55.2 81.0
90.6
53.4 80.2
90.1
75.1
M3P
-
-
-
-
-
-
-
-
-
-
-
-
-
32.3
ERNIE-ViL 2.0 33.7 52.1
60.0
19.0 35.3
43.5
40.6
69.1 92.9
97.1
69.6 91.2
96.9
86.1
Table 3: Zero-shot Chinese cross-modal retrieval results on COCO-CN and AIC-ICC datasets, compared with the
previous Chinese VLP models.
6

C
Image encoder
Text  encoder
Inter-modal constrative learning
Intra-modal constrative learning
Image encoder
Text  encoder
Image encoder
Text  encoder
Image encoder
Text  encoder
Image
caption
Image
caption
tag
Image encoder
Text  encoder
Image
caption
Image
caption
caption
Image
Image
Image
caption/tag caption/tag
A
B
D
E
Figure 4: Ablation study with ﬁve experiments. A (baseline): conventional single-view contrastive learning. B integrates
image-tags (inter-modal) view pairs into baseline A. C and D integrate image-image view pairs and caption-caption
view pairs (intra-modal) into baseline A, respectively. E merges entire strategies (A+B+C+D), namely ERNIE-ViL 2.0.
No
Model
Types of view pairs
Image Retrieval
Text Retrieval
mR
mR
mR
A
Baseline
Inter-modal
81.9
91.6
86.8
B
+ (tag,image)
83.7
92.6
88.2
C
+ (image,image)
Intra-modal
83.6
91.3
87.5
D
+ (caption,caption)
83.8
92.1
88.0
E
ERNIE-ViL2.0ablation
Inter-modal & Inter-modal
85.1
93.0
89.1
Table 4: The results of the ablated models to study the effect of our proposed strategies. (X, Y ) donates contrasting
view pairs (e.g., (tag,image) is learn inter-modal correlations between tags and images). The metric of all experiments is
mR (meanRecall), which is the average of R@1, R@5 and R@10. ERNIE-ViL 2.0ablation uses full purposed strategies
of ERNIE-ViL 2.0 with the ablation setting.
4.3
Results on Cross-modal Retrieval
Under the zero-shot setting, we directly use the pre-trained model for image-text retrieval. In the ﬁne-tuning phase, for
a fair comparison with previous VL pre-training methods, we only use the conventional views (image-caption pairs). To
evaluate text-image retrieval, we adopt recall at K (R@K) as the metric.
English cross-modal retrieval
We evaluate our English pre-trained model for cross-modal retrieval on Flickr30K
[37] and MSCOCO [11] in zero-shot and ﬁne-tuning settings. Following the widely used splits in [38], we split
MSCOCO and Flickr30K into the train/validation/test set with 113K/5K/5K and 29K/1K/1K, respectively. As shown
in Table 1 and Table 2, ERNIE-ViL 2.0 achieves a comparable results compared with previous works. Speciﬁcally,
compared to methods [2, 1, 14, 8, 9, 3] only using publicly available datasets, ERNIE-ViL 2.0 obtain signiﬁcant
improvements both in zero-shot and ﬁne-tuning settings. For the works [4, 5, 6, 15, 16] using large-scale private
datasets, we also present superior results of zero-shot cross-modal retrieval except for CoCa [16] with 165x more
image-text pairs. Notably, we outperform ALIGN [5] with the same model size and architecture using 62x fewer
pre-training image-text pairs.
Chinese cross-modal retrieval
We evaluate our Chinese pre-training models on two Chinese cross-modal bench-
marks COCO-CN [39] and AIC-ICC [40], where we use standard 1K test splits for COCO-CN and the validation split
for AIC-ICC (the ﬁrst 10K images, following BriVL [41]). As shown in Table 3, ERNIE-ViL 2.0 outperforms all
existing methods [41, 42, 43, 44, 45, 46] and achieves SOTA results both on COCO-CN and AIC-ICC with zero-shot
settings. Compared to previous best methods [43], ERNIE-ViL 2.0 obtain an improvements of 6.1% meanRecall on
AIC-ICC and 5.6% meanRecall on COCO-CN. Besides, we also present the results of the models with ViT as visual
encoder in Appendix Table 8.
7

4.4
Ablation Study
We study the beneﬁts of the additional visual and textual views enhancing the cross-modal contrastive learning on the
English pre-training datasets. Figure 4 illustrates the detailed contrastive models used in our ablation experiments. We
treat the conventional single-view constrative learning method as our baseline (model A). We incorporate intra-modal
views for contrastive learning with model C for visual views and model D for textual views. We also try adding another
inter-modal views for contrasting using the sequence of tags as the special textual view (model B). Finally, we arrive at
our ﬁnal multi-view contrastive learning model E. We pre-train all the ablated models using CC [26] and CC12M [27]
as the pre-training datasets for 50,000 steps with a batch size of 1024. All the models follow the same architecture and
other hyper-parameters with our ﬁnal ERNIE-ViL 2.0 model. We measure the performance of the models on zero-shot
cross-modal retrieval of Flickr30K and use meanRecall as the evaluation metric.
The results of the experiments are listed in Table 4. We obtain the following observations from the performance
comparisons.
Adding intra-modal views While the conventional contrastive learning method use the image-text pair, rely only
on the cross-modal view pairs for contrastive learning. Incorporating the intra-modal views pairs, both visual view
pairs and textual view pairs, brings improvement for image-text retrieval task. Speciﬁcally, adding text-text pairs for
contrasting obtains an absolute improvement 1.2% on meanRecall (Model D compared to Model A) while adding
image-image pairs brings an absolute improvement of 0.7% (Model C compared to Model A).
Enhancing inter-modal views We consider the sequence of tags as special textual view and construct a new inter-
modal view pairs. We observe that with this type of enhanced inter-modal view pairs, the contrastive learning learning
better representations for image and text. Adding (tag, image) pairs for contrasting brings a signiﬁcant absolute
improvement of 1.6% on meanRecall (Model B compared to Model A). Actually, the sequence of tags serves as a bridge
between ﬁne-grained semantic units in captions and abstract visual concepts in images, resulting in easing learning of
cross-modal alignment.
Multi-view contrastive learning Bringing the enhanced inter-modal views and intra-modal views together, we arrive
at the ﬁnal multi-view contrastive learning method, ERNIE-ViL 2.0. Compared to the image-text contrastive methods,
ERNIE-ViL 2.0 makes a signifcant absolute improvement of 2.3% on meanRecall for image-text retrieval. For text-
to-image retrieval the absolute improvement (3.2%) on meanRecall is more signiﬁcant given that the text-to-image
task is more challenging (there is ﬁve ground-truth captions for each image and only one ground-truth for each image),
demonstrating the effectiveness of our multi-view contrastive learning framework.
5
Analysis of Learned Embeddings
We construct a Chinese image-text retrieval system and perform image retrieval and text retrieval to study the learned
embeddings by ERNIE-ViL 2.0 (Chinese model).
Image retrieval
We perform image retrieval on 300K web-crawled images separated from our training data. Figure
5 shows the top-1 text-to-image retrieval results with two categories of manual queries. The ﬁrst category of queries
consists of general words with different qualifying words(e.g., personiﬁcation style, animal facial expression, the levels
for the speciﬁed cars), and ERNIE-ViL 2.0 can retrieve the precise image given the detailed description. Besides, the
second part (right side in Figure 5) presents retrieval results with several hard queries (e.g., ﬁne-grained words, abstract
concepts, long-tail phrases, etc.) validating the superior capability of our models.
Text retrieval
We perform text retrieval on 41k tags collected from a Chinese public lexicon (e.g.,THUOCL2). We
present top-2 retrieval results for each image selected from different domains (e.g., food, landmark, pets, and famous
people). As shown in Figure 6, ERNIE-ViL 2.0 has a powerful ability to recognize the ﬁne-grained concepts and
instances in images. As mentioned in Section 3.3, we consider ERNIE-ViL 2.0 can easily learn such ﬁne-grained
concepts on noisy cross-modal data leveraging construct the contrastive learning between object tags and images.
6
Conclusion
We propose a Multi-view contrastive learning framework ERNIE-ViL 2.0 for cross-modal retrieval, to improve the
robustness and generalization of the cross-modal representation through incorporating diverse views into the contrastive
learning framework. ERNIE-ViL 2.0 attempts to build both inter-modal and intra-modal correlations simultaneously.
2http://thuocl.thunlp.org/
8

+简笔画
(stick figure)
+拟人化
(personification)
一只猫
(A cat)
+开心
(happy)
+悲伤
(sad)
大熊猫
(A panda)
+生气
(Angry)
+油画
(oil painting)
+宏伟壮观的
(large and spectacular)
+中式风格的
(traditional Chinese style)
+西式风格的
(the western style)
一座桥
(A bridge)
一件瓷器
(A piece of porcelain)
+底部
(bottom view)
+碎片
(pieces)
+俯视图
(top view)
+G级
(G class)
奔驰汽车
(Mercedes cars)
+M级
(M class)
+SL级
(SL class)
牙膏盒组成的坦克
(Tank composed of toothpaste box)
一个路牌上带有校园两字
(A street sign with the word campus on it)
一本关于中医的书籍
(A book about Chinese medicine)
一本关于爱情的西方名著
(A Western Famous Book About Love)
一个戴眼镜的猫正在学习
(A cat with glasses is studying)
手写的请假条
(handwritten leave slip)
Figure 5: Image retrieval with ﬁne-grained text queries using ERNIE-ViL 2.0’s embeddings. The left side lists general
descriptions with qualifying words (i.e., style, viewpoint, class, etc.). The right side lists the complex queries (i.e.,
The ﬁrst row requires recognizing the detailed character in images for given questions. The second row presents some
long-tail queries. The third row shows queries containing abstract concepts.)
小蛮腰,广州塔
(Guangzhou Tower)
孔子标准像,孔圣
(Confucius standard image, 
Confucius)
神经网络,网状结构
(Neural Network, Grid structure)
雪山豹,雪豹
(Snow leopard)
韩国拌饭,韩膳
(bibimbap, Korean food)
斯塔福斗牛梗,斯塔福德郡牛梗
(Staffordshire Bull Terrier)
虎百合,哥伦比亚虎百合
(tiger lily, Colombian tiger lily)
优胜美地,山柏
(Yosemite, Mountain Cypress)
Figure 6: The text retrieval with images from different domains (e.g., Landmark, famous people, artworks, ﬂowers,
pets, foods, etc.) using ERNIE-ViL 2.0’s embeddings.
9

Moreover, we construct a special textual view as a bridge to ease the learning of the challenging semantic alignment
between noisy image-text pairs. ERNIE-ViL 2.0 achieves SOTA results on Chinese cross-modal retrieval tasks, and
comparable results on English cross-modal retrieval pre-trained with fewer image-text pairs and hence build a solid and
robust benchmark with only publicly available datasets. For future work, we will study multi-view learning for the
cross-encoder architecture, which obtain better results for complex Vision-Language reasoning tasks.
References
[1] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A universal encoder for vision
and language by cross-modal pre-training. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pages 11336–11344, 2020.
[2] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.
Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104–120.
Springer, 2020.
[3] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,
Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference
on Computer Vision, pages 121–137. Springer, 2020.
[4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language
supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR, 2021.
[5] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,
and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In
International Conference on Machine Learning, pages 4904–4916. PMLR, 2021.
[6] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang,
and Chunjing Xu. FILIP: ﬁne-grained interactive language-image pre-training. CoRR, 2021.
[7] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European conference on
computer vision, pages 776–794. Springer, 2020.
[8] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced
vision-language representations through scene graphs. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 35, pages 3208–3216, 2021.
[9] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural
information processing systems, 34:9694–9705, 2021.
[10] Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei Fang, and Jingjing Liu. LightningDOT: Pre-training
visual-semantic embeddings for real-time image-text retrieval. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages
982–997, Online, June 2021. Association for Computational Linguistics.
[11] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision,
pages 740–755. Springer, 2014.
[12] Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, and Baldo Faieta.
Multimodal contrastive training for visual representation learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 6995–7004, 2021.
[13] Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, and Jason
Baldridge. Mural: Multimodal, multitask retrieval across languages. ArXiv, abs/2109.05125, 2021.
[14] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach,
and Douwe Kiela. Flava: A foundational language and vision alignment model, 2021.
[15] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong
Huang, Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu, Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng
Wang, Bin Xiao, Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and Pengchuan Zhang. Florence: A new
foundation model for computer vision, 2021.
[16] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models, 2022.
10

[17] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image
pre-training, 2021.
[18] Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. ArXiv, abs/1304.5634, 2013.
[19] Avrim Blum and Tom. Mitchell. Combining labeled and unlabeled data with co-training. In COLT’ 98, 1998.
[20] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Learning non-linear combinations of kernels.
Advances in neural information processing systems, 22, 2009.
[21] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In
International Conference on Machine Learning, pages 4116–4126. PMLR, 2020.
[22] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR,
2020.
[23] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894–6910,
Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
[24] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.
Bottom-up and top-down attention for image captioning and visual question answering.
2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 6077–6086, 2018.
[25] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.
ArXiv, abs/1807.03748, 2018.
[26] Edwin G. Ng, Bo Pang, Piyush Sharma, and Radu Soricut. Understanding guided image captioning performance
across domains. arXiv preprint arXiv:2012.02339, 2020.
[27] Soravit Changpinyo, Piyush Kumar Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale
image-text pre-training to recognize long-tail visual concepts. 2021 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3557–3567, 2021.
[28] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,
and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016.
[29] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta,
Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-ﬁltered 400 million
image-text pairs, 2021.
[30] Mingxing Tan and Quoc Le. EfﬁcientNet: Rethinking model scaling for convolutional neural networks. In
Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on
Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 6105–6114. PMLR, 09–15
Jun 2019.
[31] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. In NAACL-HLT, 2019.
[32] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.
[33] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian,
and Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223,
2019.
[34] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: A
continual pre-training framework for language understanding. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 34, pages 8968–8975, 2020.
[35] Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin
Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu,
Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 3.0: Large-scale knowledge enhanced
pre-training for language understanding and generation, 2021.
[36] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An
image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on
Learning Representations, 2021.
[37] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana
Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.
IJCV, 2017.
11

[38] Andrej Karpathy and Li Fei-Fei.
Deep visual-semantic alignments for generating image descriptions.
In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3128–3137, 2015.
[39] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. Coco-cn for
cross-lingual image tagging, captioning and retrieval, 2018.
[40] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin,
Yanwei Fu, et al. Large-scale datasets for going deeper in image understanding. In 2019 IEEE International
Conference on Multimedia and Expo (ICME), pages 1480–1485. IEEE, 2019.
[41] Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang, Jing Wen, Heng Zhang, Baogui
Xu, Weihao Zheng, Zongzheng Xi, Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Libao
Zhang, Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu Liu, Zheng Gong, Chu
Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song,
and Ji-Rong Wen. Wenlan: Bridging vision and language by large-scale multi-modal pre-training. ArXiv, 2021.
[42] IDEA-CCNL. Fengshenbang-lm. https://github.com/IDEA-CCNL/Fengshenbang-LM, 2022.
[43] Chunyu Xie, Heng Cai, Jianfei Song, Jincheng Li, Fanjing Kong, Xiaoyu Wu, Henrique Morimitsu, Lin Yao,
Dexin Wang, Dawei Leng, Xiangyang Ji, and Yafeng Deng. Zero and r2d2: A large-scale chinese cross-modal
benchmark and a vision-language framework, 2022.
[44] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Minzhe Niu, Xiaodan Liang, Lewei Yao, Runhui Huang,
Wei Zhang, Xin Jiang, Chunjing Xu, and Hang Xu. Wukong: A 100 million large-scale chinese cross-modal
pre-training benchmark, 2022.
[45] Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang, Dongdong Zhang, and Nan Duan.
M3p: Learning universal representations via multitask multilingual multimodal pre-training. 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2021.
[46] Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018.
[47] Zarana Parekh, Jason Baldridge, Daniel Cer, Austin Waters, and Yinfei Yang. Crisscrossed captions: Extended
intramodal and intermodal semantic similarity judgments for ms-coco. arXiv preprint arXiv:2004.15020, 2020.
[48] Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: A novel task for ﬁne-grained image
understanding. arXiv preprint arXiv:1901.06706, 2019.
[49] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning
about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 6418–6428, Florence, Italy, July 2019. Association for Computational
Linguistics.
12

Language
Source
# Image
English
CC3M, CC12M, YFCC
29M
Chinese
CC3M, CC12M, YFCC,LAION, ERNIE-ViL1.1B
1.5B
Table 5: The details of pre-training datasets used in ERNIE-ViL 2.0
I2T
T2I
T2T
I2I
DET2T+I2T
77.3
65.7
60.4
65.7
ALIGN
89.9
79.2
62.5
73.3
ERNIE-ViL 2.0
90.0
79.0
64.7
72.4
Table 6: Multimodal retrieval results at Crisscrossed Captions (CxC), compared with multi-modal methods: ALIGN
[5], DET2T+I2T [47]. The metric is the meanRecall (mR). I2T: text →image, I2T:image →text, I2I:image →image,
T2T: text →text
A
The Details of Pre-training Datasets
We list the details of pre-training datasets used in ERNIE-ViL 2.0 in Table 5, where ERNIE-ViL1.1B denotes 1.1 billion
image-text pairs crawled from the Chinese websites.
B
Multi-modal Retrieval
Due to that our framework learns intra-modal and inter-modal views simultaneously, we consider another datasets
Crisscrossed Caption (CxC) [47], which contains inter-modal and intra-modal retrieval tasks. CxC extends MSCOCO
datasets to 267,095 caption-caption, image-image, and caption-image pairs. Speciﬁcally, following [47], we construct
positives and negtives in evaluation for semantic textual similarity (STS), semantic image-text similarity (SITS) and
semantic image similarity (SIS). We evaluate ERNIE-ViL 2.0 (English model) on CXC using the ﬁne-tuned model on
COCO datasets. The Table 6 reports experimental results, showing that ERNIE-ViL 2.0 outperforms ALIGN (previous
best method) both in image-to-text and text-to-text retrieval.
C
Transferring to Fine-grained Understanding Tasks
For further evaluating the transferability of ERNIE-ViL 2.0, we transfer our English pre-trained model to ﬁne-grained
multimodal tasks. We consider two tasks: SNLI-VE [48] and NLVR2 [49]. SNLI-VE is a visual entailment task to
predict whether a given image entails a text. NLVR2 requires the model to predict the relations between a given sentence
and an image pair. Following UNITER’s [2] settings, classiﬁcation accuracy is adopted to measure two benchmarks
(SNLI-VE and NLVR2). Our model predicts the class probabilities on SNLI-VE using a multi-layer perception (MLP)
on a concatenated representation of image and text encoder outputs. For NLVR2, after encoding the two image into
individual embedding, we concatenate and feed them to a MLP layer to obtain the visual embedding. The textual
embedding and the visual embedding are concatenated and fed into the classiﬁer to get the ﬁnal scores. We present the
results in Table 7. Although those tasks need more ﬁne-grained interaction of visual/textual features, our method still
presents comparable results with a global cross-modal representation.
SNLI-VE
NLVR2
UNITER
78.3
-
VisualBERT
-
67.0
ERNIE-ViL 2.0
76.6
66.0
Table 7: Compared with previous methods with Fine-grained vision-language interaction on V+L downstream tasks:
SNLI-VE and NLVR2, ERNIE-ViL 2.0 present comparable results with shallow cross-modal interaction
13

Models
ERNIE-ViL 2.0large
ERNIE-ViL 2.0large
ERNIE-ViL 2.0base
Backbone
Image Encoder
ViT-L-14
EffcientNet-L2
ViT-B-16
Text Encoder
ERNIE-Large
ERNIE-Large
ERNIE-Base
Batch size
8000
23200
14000
AIC-ICC
image→text
R@1
32.3
33.7
30.4
R@5
51.6
52.1
48.9
R@10
59.9
60.0
57.5
text→image
R@1
20.2
19.0
17.9
R@5
37.0
35.3
34.2
R@10
45.4
43.5
42.5
COCO-CN
image→text
R@1
68.6
69.1
66.5
R@5
92.5
92.9
91.6
R@10
97.5
97.1
96.2
text→image
R@1
70.1
69.6
65.9
R@5
92.5
91.2
90.1
R@10
96.4
96.9
96.1
Table 8: The performance of zero-shot Chinese cross-modal retrieval with different architecture
D
The Overall Results of Chinese Cross-modal Retrieval
We list the overall results of Chinese cross-modal results on AIC-ICC and COCO-CN with different architecture in
Table 8
14

