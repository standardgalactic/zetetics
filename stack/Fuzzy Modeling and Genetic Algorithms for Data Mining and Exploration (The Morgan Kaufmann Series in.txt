
Fuzzy Modeling and
Genetic Algorithms
for Data Mining
and Exploration

The Morgan Kaufmann Series in Data Management Systems
Series Editor: Jim Gray, Microsoft Research
Fuzzy Modeling and Genetic Algorithms for Data Mining and Exploration
Earl Cox
Data Modeling Essentials, Third Edition
Graeme C. Simsion and Graham C. Witt
Location-Based Services
Jochen Schiller and Agnès Voisard
Database Modeling with Microsft ® Visio for Enterprise Architects
Terry Halpin, Ken Evans, Patrick Hallock, Bill Maclean
Designing Data-Intensive Web Applications
Stephano Ceri, Piero Fraternali, Aldo Bongio, Marco Brambilla, Sara Comai, and Maristella Matera
Mining the Web: Discovering Knowledge from Hypertext Data
Soumen Chakrabarti
Advanced SQL: 1999—Understanding Object-Relational and Other Advanced Features
Jim Melton
Database Tuning: Principles, Experiments, and Troubleshooting Techniques
Dennis Shasha and Philippe Bonnet
SQL: 1999—Understanding Relational Language Components
Jim Melton and Alan R. Simon
Information Visualization in Data Mining and Knowledge Discovery
Edited by Usama Fayyad, Georges G. Grinstein, and Andreas Wierse
Transactional Information Systems: Theory, Algorithms, and Practice of Concurrency Control
and Recovery
Gerhard Weikum and Gottfried Vossen
Spatial Databases: With Application to GIS
Philippe Rigaux, Michel Scholl, and Agnes Voisard
Information Modeling and Relational Databases: From Conceptual Analysis to Logical Design
Terry Halpin
Component Database Systems
Edited by Klaus R. Dittrich and Andreas Geppert
Managing Reference Data in Enterprise Databases: Binding Corporate Data to the Wider World
Malcolm Chisholm
Data Mining: Concepts and Techniques
Jiawei Han and Micheline Kamber
Understanding SQL and Java Together: A Guide to SQLJ, JDBC, and Related Technologies
Jim Melton and Andrew Eisenberg
Database: Principles, Programming, and Performance, Second Edition
Patrick and Elizabeth O’Neil

The Object Data Standard: ODMG 3.0
Edited by R. G. G. Cattell and Douglas K. Barry
Data on the Web: From Relations to Semistructured Data and XML
Serge Abiteboul, Peter Buneman, and Dan Suciu
Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations
Ian Witten and Eibe Frank
Joe Celko’s SQL for Smarties: Advanced SQL Programming, Second Edition
Joe Celko
Joe Celko’s Data and Databases: Concepts in Practice
Joe Celko
Developing Time-Oriented Database Applications in SQL
Richard T. Snodgrass
Web Farming for the Data Warehouse
Richard D. Hackathorn
Database Modeling & Design, Third Edition
Toby J. Teorey
Management of Heterogeneous and Autonomous Database Systems
Edited by Ahmed Elmagarmid, Marek Rusinkiewicz, and Amit Sheth
Object-Relational DBMSs: Tracking the Next Great Wave, Second Edition
Michael Stonebraker and Paul Brown,with Dorothy Moore
A Complete Guide to DB2 Universal Database
Don Chamberlin
Universal Database Management: A Guide to Object/Relational Technology
Cynthia Maro Saracco
Readings in Database Systems, Third Edition
Edited by Michael Stonebraker and Joseph M. Hellerstein
Understanding SQL’s Stored Procedures: A Complete Guide to SQL/PSM
Jim Melton
Principles of Multimedia Database Systems
V. S. Subrahmanian
Principles of Database Query Processing for Advanced Applications
Clement T. Yu and Weiyi Meng
Advanced Database Systems
Carlo Zaniolo, Stefano Ceri, Christos Faloutsos, Richard T. Snodgrass,
V. S. Subrahmanian, and Roberto Zicari
Principles of Transaction Processing
Philip A. Bernstein and Eric Newcomer
Using the New DB2: IBMs Object-Relational Database System
Don Chamberlin

Distributed Algorithms
Nancy A. Lynch
Active Database Systems: Triggers and Rules For Advanced Database Processing
Edited by Jennifer Widom and Stefano Ceri
Migrating Legacy Systems: Gateways, Interfaces, & the Incremental Approach
Michael L. Brodie and Michael Stonebraker
Atomic Transactions
Nancy Lynch, Michael Merritt, William Weihl, and Alan Fekete
Query Processing for Advanced Database Systems
Edited by Johann Christoph Freytag, David Maier, and Gottfried Vossen
Transaction Processing: Concepts and Techniques
Jim Gray and Andreas Reuter
Building an Object-Oriented Database System: The Story of O2
Edited by François Bancilhon, Claude Delobel, and Paris Kanellakis
Database Transaction Models for Advanced Applications
Edited by Ahmed K. Elmagarmid
A Guide to Developing Client/Server SQL Applications
Setrag Khoshaﬁan, Arvola Chan, Anna Wong, and Harry K. T. Wong
The Benchmark Handbook for Database and Transaction Processing Systems, Second Edition
Edited by Jim Gray
Camelot and Avalon: A Distributed Transaction Facility
Edited by Jeffrey L. Eppinger, Lily B. Mummert, and Alfred Z. Spector
Readings in Object-Oriented Database Systems
Edited by Stanley B. Zdonik and David Maier

Fuzzy Modeling and
Genetic Algorithms
for Data Mining
and Exploration
Earl Cox
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Morgan Kaufmann Publishers is an imprint of Elsevier

Publishing Director
Diane D. Cerra
Acquisitions Editor
Rick Adams
Publishing Services Manager
Simon Crump
Project Manager
Dan Stone
Developmental Editor
Emilia Thiuri
Editorial Coordinators
Corina Derman and Mona Buehler
Cover Design
Yvo Riezebos Design
Cover Image
Getty Images
Composition
CEPHA Imaging Pvt. Ltd.
Technical Illustration
Dartmouth Publishing Inc.
Copyeditor
Daril Bentley
Proofreader
Phyllis Coyne Proofreading Services
Indexer
Lucie Haskins
Interior Printer
The Maple-Vail Book Manufacturing Group
Cover Printer
Phoenix Color
Morgan Kaufmann Publishers is an imprint of Elsevier.
500 Sansome Street, Suite 400, San Francisco, CA 94111
This book is printed on acid-free paper.
© 2005 by Elsevier Inc. All rights reserved.
Designations used by companies to distinguish their products are often claimed as trademarks
or registered trademarks. In all instances in which Morgan Kaufmann Publishers is aware of
a claim, the product names appear in initial capital or all capital letters. Readers, however,
should contact the appropriate companies for more complete information regarding
trademarks and registration.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopying, scanning, or otherwise—
without prior written permission of the publisher.
Permissions may be sought directly from Elsevier’s Science & Technology Rights Department
in Oxford, UK: phone: (+44) 1865 843830, fax: (+44) 1865 853333, e-mail: permissions@
elsevier.com.uk. You may also complete your request on-line via the Elsevier homepage
(http://elsevier.com) by selecting “Customer Support” and then “Obtaining Permissions.”
Library of Congress Cataloging-in-Publication Data
APPLICATION SUBMITTED
ISBN: 0-12-194275-9
For information on all Morgan Kaufmann publications,
visit our Web site at www.mkp.com or www.books.elsevier.com
Printed in the United States of America
05
06
07
08
09
5
4
3
2
1

Contents
Preface
xiii
Objectives and Audience
xiv
Organization of the Book
xv
Algorithm Deﬁnitions and Examples
xv
Acknowledgments
xvii
Introduction
xix
The Modern Connected World
xix
The Advent of Intelligent Models
xix
Fuzzy Logic and Genetic Algorithms
xxi
Part I
Concepts and Issues
Chapter 1
■■■
Foundations and Ideas 3
1.1
Enterprise Applications and Analysis Models
4
1.2
Distributed and Centralized Repositories
8
1.3
The Age of Distributed Knowledge
11
1.4
Information and Knowledge Discovery
12
1.5
Data Mining and Business Models
18
1.6
Fuzzy Systems for Business Process Models
23
1.7
Evolving Distributed Fuzzy Models
25
1.8
A Sample Case: Evolving a Model for Customer Segmentation
26
1.9
Review
29
vii

viii
■
Contents
Chapter 2
■■■
Principal Model Types 31
2.1
Model and Event State Categorization
34
2.2
Model Type and Outcome Categorization
35
2.3
Review
36
Chapter 3
■■■
Approaches to Model Building 37
3.1
Ordinary Statistics
37
3.2
Nonparametric Statistics
38
3.3
Linear Regression in Statistical Models
40
3.4
Nonlinear Growth Curve Fitting
44
3.5
Cluster Analysis
47
3.6
Decision Trees and Classiﬁers
48
3.7
Neural Networks
51
3.8
Fuzzy SQL Systems
52
3.9
Rule Induction and Dynamic Fuzzy Models
55
3.10
Review
62
Further Reading
63
Part II
Fuzzy Systems
Chapter 4
■■■
Fundamental Concepts of Fuzzy Logic 67
4.1
The Vocabulary of Fuzzy Logic
68
4.2
Boolean (Crisp) Sets: The Law of Bivalence
72
4.3
Fuzzy Sets
76
4.4
Review
93
Further Reading
94
Chapter 5
■■■
Fundamental Concepts of Fuzzy Systems 95
5.1
The Vocabulary of Fuzzy Systems
96

Contents
■
ix
5.2
Fuzzy Rule-based Systems: An Overview
100
5.3
Variable Decomposition into Fuzzy Sets
104
5.4
A Fuzzy Knowledge Base: The Details
113
5.5
The Fuzzy Inference Engine
119
5.6
Inference Engine Approaches
122
5.7
Running a Fuzzy Model
124
5.8
Review
147
Chapter 6
■■■
Fuzzy SQL and Intelligent Queries 149
6.1
The Vocabulary of Relational Databases and Queries
150
6.2
Basic Relational Database Concepts
156
6.3
Structured Query Language Fundamentals
159
6.4
Precision and Accuracy
161
6.5
Why We Search Databases
162
6.6
Expanding the Query Scope
165
6.7
Fuzzy Query Fundamentals
169
6.8
Measuring Query Compatibility
180
6.9
Complex Query Compatibility Metrics
184
6.10
Compatibility Threshold Management
187
6.11
Fuzzy SQL Process Flow
188
6.12
Fuzzy SQL Example
193
6.13
Evaluating Fuzzy SQL Outcomes
200
6.14
Review
204
Further Reading
205
Chapter 7
■■■
Fuzzy Clustering 207
7.1
The Vocabulary of Fuzzy Clustering
208
7.2
Principles of Cluster Detection
210
7.3
Some General Clustering Concepts
211
7.4
Crisp Clustering Techniques
218
7.5
Fuzzy Clustering Concepts
223
7.6
Fuzzy c-Means Clustering
228
7.7
Fuzzy Adaptive Clustering
248

x
■
Contents
7.8
Generating Rule Prototypes
259
7.9
Review
262
Further Reading
263
Chapter 8
■■■
Fuzzy Rule Induction 265
8.1
The Vocabulary of Rule Induction
266
8.2
Rule Induction and Fuzzy Models
269
8.3
The Rule Induction Algorithm
273
8.4
The Model Building Methodology
283
8.5
A Rule Induction and Model Building Example
288
8.6
Measuring Model Robustness
312
8.7
Technical Implementation
323
8.8
External Controls
325
8.9
Organization of the Knowledge Base
333
8.10
Review
337
Further Reading
338
Part III
Evolutionary Strategies
Chapter 9
■■■
Fundamental Concepts of Genetic Algorithms 343
9.1
The Vocabulary of Genetic Algorithms
344
9.2
Overview
353
9.3
The Architecture of a Genetic Algorithm
365
9.4
Practical Issues in Using a Genetic Algorithm
413
9.5
Review
418
Further Reading
419
Chapter 10
■■■
Genetic Resource Scheduling Optimization 421
10.1
The Vocabulary of Resource-constrained Scheduling
421
10.2
Some Terminology Issues
425

Contents
■
xi
10.3
Fundamentals
426
10.4
Objective Functions and Constraints
428
10.5
Bringing It All Together: Constraint Scheduling
434
10.6
A Genetic Crew Scheduler Architecture
440
10.7
Implementing and Executing the Crew Scheduler
444
10.8
Topology Constraint Algorithms and Techniques
460
10.9
Adaptive Parameter Optimization
475
10.10 Review
479
Further Reading
480
Chapter 11
■■■
Genetic Tuning of Fuzzy Models 483
11.1
The Genetic Tuner Process
483
11.2
Conﬁguration Parameters
488
11.3
Implementing and Running the Genetic Tuner
494
11.4
Advanced Genetic Tuning Issues
505
11.5
Review
515
Further Reading
516
Index 517

THIS PAGE INTENTIONALLY LEFT BLANK

Preface
This is a book about using fuzzy logic and evolutionary strategies —
primarily genetic algorithms—to explore the structure of data, understand
patterns in data, and create rule-based models from these patterns. As a
general approach to exploring and modeling data, it is neither a book on
data mining nor a book on expert or decision support systems. Rather,
its goal is to provide project managers, business analysts, knowledge
engineers, and system developers with a broad spectrum of concepts
underlying the use of fuzzy systems and genetic algorithms.
These two technologies are complementary in many ways. Fuzzy sys-
tems provide an extraordinarily rich way of representing and manipulating
a wide universe of diverse business, industry, policy, and technical data.
Fuzzy-logic-based clustering, query, and rule-based systems have a rich-
ness and a breadth of representation that are difﬁcult, if not impossible,
to achieve using corresponding technologies based on crisp or Boolean
logic. Genetic algorithms and evolutionary programming systems not only
provide powerful and adaptive methods of generating and tuning fuzzy
systems (ﬁnding the best cluster centers and exploring high-dimensional
data through techniques such as genetically evolved regression analysis)
but form the foundation for a new generation of robust, ﬂexible, and
easy-to-use optimization models.
This book addresses these issues on several fronts. First, it provides
an overall framework for understanding the methods, technologies, and
approaches used to explore data, build models from collections of under-
lying data, tune models, and design systems that incorporate both fuzzy
and genetic properties. In this respect, the book gives the designer and
modeler some guidelines for evaluating the trade-offs among various types
of models and model architectures. Second, it provides the techniques
and technologies necessary to construct robust, stable, and maintainable
models by combining several advanced forms of knowledge discovery
with machine learning techniques. Third, it provides a roadmap for
model designers and developers in exploring data, selecting model system
measures, organizing adaptive feedback loops, selecting a model conﬁg-
uration, implementing a working and workable model, and validating a
ﬁnal model.
xiii

xiv
■
Preface
The book’s modeling methodologies are based on my experiences in
building and implementing models over the past thirty years for a wide
spectrum of clients in the public and private sectors. It incorporates those
techniques and methods that have proven successful but also highlights
approaches that have proven less successful. Because the methods are
derived from established computer science and computational intelli-
gence techniques, as well as more general “heuristics” in the analysis
of processes and the representation of knowledge, the book is intended
to serve as a handbook for working analysts who need to build and deliver
working models in the commercial and public policy environments.
Objectives and Audience
This book presents a philosophical (and epistemological) discussion of
the roles fuzzy logic and genetic algorithms play in knowledge discovery,
system design, and business process modeling. In this regard, the book
has several broad objectives.
■
To provide a reasonable and understandable foundation in the nature
of fuzzy logic and fuzzy systems. Although some attention is paid
to control theory, the principal orientation is on the use of fuzzy
systems in the area of information engineering and analysis as well as
the construction of intelligent systems.
■
To introduce the foundations for three important applications of fuzzy
systems: the use of fuzzy qualiﬁers in database queries (fuzzy SQL), the
introduction of fuzzy measures in cluster analysis, and the discovery
and quantiﬁcation of fuzzy if-then rules from data.
■
To provide a clear and extensive foundation in the nature of genetic
algorithms and evolutionary programming (with emphasis on genetic
algorithms).
■
To illustrate (through applications and modeling support features)
how evolutionary strategies can be used to develop advanced opti-
mization systems as well as provide fuzzy model architecture tuning
and data exploration facilities.
Hence, this book, as the title suggests, is a guide to the concepts, basic
algorithms, and principal features of machine intelligence capabilities that
to a greater or lesser extent are based on biological models. Genetic and
evolutionary systems are the obvious biological models. However, as a

Preface
■
xv
logic of continuous phenomena that directly handles ambiguity, uncer-
tainty, and extensibility, fuzzy logic also has a strong relationship to ideas
underlying biological systems.
Organization of the Book
Part I connects the themes in the book with various types of models
and the nature of business and public policy modeling, and surveys
various approaches to heuristic, analytical, mathematical, and statisti-
cal modeling. Part I also provides a foundation in the ideas underlying
effective model building, data exploration, and data mining (or knowl-
edge discovery). In particular, this ﬁrst part couples the central ideas
of intelligent knowledge discovery, rule-based expert systems, and data
exploration with concepts related to self-monitoring adaptive feedback
models. These models are often a fusion of subject-matter expert knowl-
edge and knowledge derived from deep analysis of behavior patterns
found in data.
Part II covers methods and techniques based on fuzzy logic. It intro-
duces the concepts underlying fuzzy logic (fuzzy sets, membership
functions, fuzzy operators, and so on) and then develops the founda-
tions of fuzzy information decision and expert systems. From there Part II
moves to an examination of the application of fuzzy logic to database
queries, methods in fuzzy clustering, and the core algorithms for fuzzy
rule induction.
Part III introduces and covers the methods and techniques of genetic
algorithms and evolutionary programming. These genetic or evolutionary
strategies form the core machine intelligence components of adaptive and
self-tuning and provide the technologies necessary to build optimization,
conﬁguration, scheduling, and robust predictive models. Part III high-
lights the concepts of genetic algorithms by examining a multi-objective
and multi-constraint crew scheduling system. This section concludes by
exploring genetic methods of tuning fuzzy models.
Algorithm Deﬁnitions and Examples
Throughout the book, Java, C/C++, and (infrequently) Visual Basic code
is used to illustrate how algorithms work, as well as ﬂow of control asso-
ciated with them. In several instances, however, algorithms and methods

xvi
■
Preface
are illustrated though a form of “pseudocode” instead of actual computer
code. This has several advantages, including the following.
■
Equal opportunity: The mechanics of an algorithm are conveyed
independently of a reader’s knowledge of a particular computer
programming language.
■
Clarity: The algorithm description can avoid the complexities and
obscurities of a programming language. The syntax of many program-
ming languages often obscures the representation of data and the ﬂow
of control in the algorithm.
■
Consistency: A pseudocode approach provides a consistent way
of explaining the mechanics and the underlying principles of the
algorithm.
The complete library for the algorithms and programs contained in
this book, including source code, is located on the www.scianta.com
web site.
As a book on designing and building models using knowledge dis-
covery and machine learning techniques, a certain amount of mathe-
matical notation is unavoidable. Too many authors allow collections of
mathematical equations to stand on their own — as if the mathemat-
ics provided all of the necessary explanation. This is seldom the case.
Particular care has been taken to ensure that all mathematical equations
are explained in easy-to-understand language. An attempt has been made
to eliminate evaluation ambiguities from equations by explicitly using
parentheses to specify the order of evaluation. Mathematical notation is
avoided altogether where a more philosophical, technical, or mechanical
discussion of the process can be developed.

Acknowledgments
I would like to thank the following individuals for their help, support,
criticism, and exchange of ideas.
My thank you to Jerry Bennett, Research Analyst, Applied Research,
USAA, San Antonio, Texas; William (Bill) Combs, Computing Systems
Architect, The Boeing Company, Seattle, Washington; Bill Siler, Scientist,
Kemp-Carroway Heart Institute, Birmingham, Alabama; and Terry Hengl,
Publisher, and Robin Okun, Vice President of Marketing, PC/AI Maga-
zine, Phoenix, Arizona. I am particularly grateful to Terry and Robin for
permission to use parts of articles I wrote for their magazine exploring
some of the issues also raised in this book. I would also like to thank
Michael and Nadja O’Hagan, Fuzzy Logic Inc., La Jolla, California; Will
Dwinnell, Artiﬁcial Intelligence Consultant, Valley Forge, Pennsylvania;
Paul Bergman, President, Foresight Logic, Saint Paul, Minnesota; Mary
Dimmock, Pﬁzer Chemical, Groton, Connecticut; Art DeTore, Vice Presi-
dent, Parker Healthcare, Fort Wayne, Indiana; and Ed DeRouin, President,
Peer Sciences, Altamonte Springs, Florida.
Jonathan G. Campbell, School of Computer Science, The Queen’s
University of Belfast, guided me through the decipherment of the fuzzy
c-means algorithm, spent time patiently explaining its mechanics, re-
documented his Java code in ﬁne detail, and provided a copy of his
excellent fuzzy c-means program for this book. I thank him.
I am also indebted to Professor Vladimir Cherkassky of the Department
of Electrical and Computer Engineering, University of Minnesota, and
coauthor with Filip Mulier of Learning from Data (John Wiley and Sons,
1998). Professor Cherkassky provided a copy of Yong-Jun Lee’s doctoral
thesis, “An Automated Knowledge Extraction System,” as well as critical
insights into the fuzzy adaptive clustering technique.
And a special acknowledgment must be made to the late Fred
Hegge, Senior Scientist, Army Medical Materiel Command, Fort Detrick,
Maryland.
xvii

THIS PAGE INTENTIONALLY LEFT BLANK

Introduction
The Modern Connected World
Business process modeling is often a no-win situation. Developing reliable
business forecasting models requires a successful collaboration between
line management and knowledge engineers. And even where the fusion
of working knowledge and abstract representation is successful, its result
is too often thwarted by the unpredictable dynamics of the real world:
corporate objectives change, companies are bought and sold, and new
products are introduced (or existing products retired). To compound
matters, corporate decision makers and model builders are also faced with
the unprecedented uncertainties and pressures of the rapidly changing
nature of electronic commerce. In fact, as Figure I.1 illustrates, the high
rate of change in global economies will continue to exert pressures on
the stability and viability of even well-established corporations.
Our entire world has become connected through a freely available and
highly diverse information network, revolutionizing the way we commu-
nicate, conduct business, interact with organizations and public agencies,
entertain ourselves, and perform scientiﬁc and business research. At
the same time, the network suffers from increasing amounts of noise,
bandwidth saturation, and criminal activity.
The Advent of Intelligent Models
As a result of these uncertainties, business-forecasting models have fallen
out of favor in recent years. Instead, business planners tend to concentrate
on the short-term analytical approach to business forecasting. In particu-
lar, intelligent models — known in the 1970s as decision support systems
and later as expert systems (although they use different technologies) —
have been replaced by the ubiquitous spreadsheet. Yet spreadsheets are
no substitute for knowledge-based models in such critical areas as risk
assessment, econometric modeling, new product positioning, customer
proﬁling, cross marketing, sales forecasting, and impact analysis. In this
xix

xx
■
Introduction
Beginning
of 20th Century
Middle
of 20th Century
End
of 20th Century
Near
Future
Rate of Chage (Pressure on Business)
LOW               MODERATE                   HIGH                         VERY HIGH
Manufacturing
Economy
Service
Economy
Knowledge
Economy
Shared
Intelligence
Economy
Figure I.1
Rates and types of changes in the global economy.
book we will examine new types of intelligent models that incorporate
fuzzy measures as well as evolutionary or genetic techniques. These tech-
nologies provide ways to make your models more responsive to changes
in demographics and the economy.
A less obvious solution to the problem of change and uncertainty is
simply to incorporate these factors into the model itself. Naturally, this
means going beyond a statistical analysis of the data or the inclusion of
certainty factors or forms of Bayesian probabilities. We must create our
models so that they automatically change their internal behavior structure
to accommodate changes in the outside world. One approach to this is
the adaptive model — a model that alters its rules based on changes in the
outside world. A powerful and robust way of building an adaptive model
involves the combination of three broad technologies: fuzzy logic, knowl-
edge discovery, and genetic algorithms. Fuzzy logic provides a method
for capturing the semantics or meaning of the data through a collection
of fuzzy sets associated with each variable. Knowledge discovery (or data
mining) uses these fuzzy sets to generate an initial model of if-then rules.

Introduction
■
xxi
A genetic algorithm creates and tests many candidate models by chang-
ing the model parameters (such as the number of variables or the fuzzy
sets underlying the variables) until it ﬁnds the conﬁguration that performs
the best.
Fuzzy Logic and Genetic Algorithms
This book examines the use of fuzzy logic and genetic algorithms in data
exploration and model building. These two technologies provide a rich
and robust platform of tools and methods that allow analysts and model
builders to discover patterns buried deep in large repositories of data.
They also allow us to construct rule-based models that quantify patterns
and combine them, if necessary, with subject matter expert knowledge.
In particular, the focus is on methods of exploring public and private
(local or distributed) databases as well as the use of these technologies to
design, develop, test, validate, and deploy business models.
Data exploration, often overlooked in the knowledge discovery (data
mining) process, is a crucial component of business systems analysis,
public policy development, and the design and development of intelligent
models. This book examines two major data exploration facilities: the use
of fuzzy logic in database queries (fuzzy SQL) and the use of fuzzy logic
and genetic algorithms in fuzzy clustering. Fuzzy SQL provides a powerful
method of using semantics to query a database and return records that
match the intention of the query. Fuzzy clustering is a deep and robust
method of discovering patterns in a multidimensional collection of data.
As a form of unsupervised data mining, clusters can discover unlooked-for
patterns and express these as collections of fuzzy rules.
The book’s core model building capabilities are centered on two
complementary technologies: a comprehensive fuzzy rule induction
methodology (which can automatically build fuzzy knowledge bases from
large collections of data) and a model-tuning facility built with a genetic
algorithm that optimizes the fuzzy model’s architecture. Coupled with a
detailed discussion of multi-objective, multi-constraint genetic algorithms,
the concepts of rule discovery and conﬁguration optimization are com-
bined with a general discussion of how evolutionary strategies can support
more advanced intelligent modeling requirements.
This book is neither a mathematical discussion of fuzzy and genetic
theory nor a detailed analysis of algorithms. It is intended, as the title
implies, to be a broad exploration of critical concepts, a look at the prin-
cipal algorithms, and a discussion of roadmap issues that will provide the
foundation for the integration of fuzzy and evolutionary strategies into the
creation of models and advanced applications in the public and private
sectors.

THIS PAGE INTENTIONALLY LEFT BLANK

■
■
■
Part I
Concepts and Issues
Contents
Chapter 1
■■■
Foundations and Ideas
3
Chapter 2
■■■
Principal Model Types
31
Chapter 3
■■■
Approaches to Model Building
37
1

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 1
Foundations and Ideas
In this introductory chapter we lay some of the foundations for the use
of fuzzy-based data exploration techniques (in particular, rule-based data
mining) in public and private sector organizations. We start by examining
the modern distributed computing resource architecture of e-business, as
well as traditional brick-and-mortar enterprises. We also look at the evolu-
tion of centralized data warehouses and data marts in business, industry,
and government agencies. From this perspective we review the ways in
which knowledge discovery techniques can be used to fuse behaviors from
multiple sources into a set of effective models. We conclude with a brief
example of a customer segmentation model.
Data mining (i.e., knowledge discovery) provides a structured methodol-
ogy for discovering and quantifying patterns hidden in large quantities of
historical and operational data. These patterns, in the form of predictive
and classiﬁcation models, play a critical role in supporting an organiza-
tion’s operational, tactical, and strategic decisions. As a consequence of
the role these models play in providing deep insights into often ill-deﬁned
behaviors, data mining is becoming an integral part of an organization’s
business intelligence program. In addition, business intelligence will per-
meate the way an e-business of the future will conduct itself and protect
itself from increasingly unpredictable economic, demographic, and tech-
nological change.1 Responding to change before changes can adversely
affect business becomes increasingly more difﬁcult as the next generation
of distributed business systems replaces relatively centralized computing
facilities.
1 The term business intelligence here refers to both the activities of private sector companies
and the mission support activities of public sector entities (local, state, and federal government
agencies and the military).
3

4
■
Chapter 1
Foundations and Ideas
1.1
Enterprise Applications and Analysis Models
Modern businesses are increasingly moving into electronic commerce
by establishing a presence on the World Wide Web (the Internet). And
through the use of private networks and internal corporate networks
(intranets), they are applying powerful modeling techniques to such activ-
ities as customer relations and supply chain management. Thus, through
this on-line presence (their e-commerce portal) they not only conduct
business-to-consumer (B2C) but an ever increasing amount of business-
to-business (B2B) activities. Figure 1.1 shows a simpliﬁed structure of a
modern corporation whose sales channels are divided between traditional
brick-and-mortar retail outlets and an e-business web portal.
Modern corporations and government agencies rely on their increas-
ingly distributed computer infrastructures — application, web, and data-
base servers, as well as routers, load balancers, and ﬁrewalls — to
support a broad and often unpredictable scope of clients and applications.
Eastern
Gizmo
Distributor
Consolidated
Widget
Supplier
ACME
universal
retailer
Supply chain and partners
eCommerce
portal(s)
Store front
and
catalog sales
Distribution
Inventory
Data warehouse
Operational data store
e-Business infrastructure
Information
systems
Figure 1.1
The modern web-enabled corporation.

1.1
Enterprise Applications and Analysis Models
■
5
In the modern interconnected world, the infrastructure is the backbone
of the organization’s mission capabilities; that is, the enterprise’s local
and distributed infrastructure is intimately and ineluctably tied to busi-
ness strategy decisions. Further, the quality of these decisions is based on
a reliable and sound application of business intelligence.
The organization of an enterprise’s application framework extends
across a wide array of data repositories, application servers, and web
servers.
These application services and models are connected, as Figure 1.2
illustrates, through the machinery of distributed architectures. This
distributed architecture consists of several parts,
■
the HTTP (HyperText Transfer Protocol2),
■
server pages3 — such as ASP (Application Server Pages) and JSP (Java
Server Pages) that deliver visual structure to the client as well as accept
and display information,
■
and the transaction message handlers (as well as general message
services such as JMS — the Java Messaging System4) that move packets
of data around the network.
Business to Business (B2B) opportunities in the Internet world estab-
lish lines of communication between business peers on the Web. Capi-
talizing on the opportunities in this world means leveraging knowledge
and marketplace intelligence at ever accelerating rates. This has driven
corporations into a race to build and use advanced computational models
derived from sophisticated data mining and machine learning capabili-
ties. In this scenario, businesses encapsulate organizational intelligence
from transaction streams associated with the e-business interface and the
brick-and-mortar sales interface (the combined points-of-sale interface)
thus integrating all aspects of their business into a coherent and effective
model.
2 HTTP is the primary protocol used to move hypertext information over the Internet and other
networks that use the Internet data protocols. HTTP uses an HTTP client on one end and an
HTTP server on the other end.
3 ASP allows the dynamic creation and management of pages containing either server-side or
client-side scripts. ASP scripts can support complex functions such as database access, the
generation and personalization of web pages, and a broad spectrum of interactive services. In
many ways, ASP scripts are similar to CGI (common gateway interface) programs.
4 Java Message Service is a J2EE interface, implemented for many Java container objects, that
provides point-to-point message management, queuing, and publish-subscribe services.

6
■
Chapter 1
Foundations and Ideas
Physical database
On-line
customers
Point of
sales
HTTP
Web service
ASP/JSP
handler
Transaction
handler
WebServer
Database
management
Application server
Customer
management
Purchase
orders
Inventory
Physical database
Database servers
Figure 1.2
Organization of distributed business systems.
As Figure 1.3 illustrates, both strategic and line-of-business (LOB) mod-
els have a tight connection with an organization’s computer resources.
In most enterprises these models work together in a feedback loop that
measures the impact of decisions at many levels of the organization.
Strategic models are created by integrating information from the orga-
nization’s various lines of business (or agency bureaus) with information
about incoming performance along the supply chain from raw materials

1.1
Enterprise Applications and Analysis Models
■
7
Purchasing
Inventory
Shipping
Supply chain vendors
Application servers
Application and
web servers
Point of
sales
Purchase
orders
Predictive
models
Enterprise
objectives
Database servers
Database
Physical database
Predictive
models
Operational
business intelligence
Investment funding to support growth
Strategic business intelligence
Funding for growth
Figure 1.3
Modeling strategic and operational (LOB) processes.
and service vendors of. A strategic model, tied to the enterprise’s mission
and vision, both measures how well the organization is meeting its over-
all objectives (usually related to bottom-line proﬁts) and predicts the
organization’s near-term future behavior for these objectives. Income or
after-tax cash ﬂow funds the various lines of business, which in turn
generally have their own predictive models. Growth predictions (or
agency requirements for managing expanded oversight and regulatory
demands) fuel requirements for investment funds to support and sustain
growth.
Building these decision models involves a fusion of subject matter
expertise and models drawn from historical and operational repositories.
These historical models often take the form of free-form database searches

8
■
Chapter 1
Foundations and Ideas
using a database query language and of rule-based models derived from
patterns buried deep in the data itself.
The application of data mining in distributed systems is often facilitated
by two related but often conﬂicting trends. The ﬁrst trend is the consol-
idation of corporate information into a centrally managed and centrally
controlled set of databases known collectively as the Data Warehouse.
The second trend is the consolidation of line of business, geographically
related, or functionally related business information into locally managed
and locally controlled databases. These local repositories are known as
Data Marts. In many organizations a loosely connected set of data marts,
sharing data over a private or public network, form the organization’s
data warehouse. The ideas and implications associated the distributed
nature of data warehouses and data marts are discussed in the next
section.
1.2
Distributed and Centralized Repositories
Small and large corporations, the military, and public and governmen-
tal agencies throughout the world have begun to recognize that their
vast stores of historical and active data represent a critical resource.
These resources constitute the intellectual property of the organization,
including its underlying proprietary knowledge, and potentially repre-
sent an explicit picture of its business process models. In addition to
their archeological and often political signiﬁcance as a record of business
practices both past and present, these active and archival databases con-
tain implicit relationships between crucial elements in the organization.
As an example, Figure 1.4 illustrates some basic data relations in a retailing
company.
For many organizations these relationships do not ﬁnd expression as
uniform business practices and well-ordered databases but as a poten-
tial in the broad and historically diverse set of data representing, for
example, the natural connection between what the company sells and
what customers actually purchase. And, of course, there are inherent
relationships among such data collections, such as between sales and
inventory, or among inventory, sales, and purchase orders. This diversity
in data storage and ownership means that behavior patterns common to
the organization’s business practices or mission were both overlooked
and not easily accessible for analysis. Further, even when traditional
methods of statistical or systems analysis were applied to these data
repositories many important and deep relationships went unobserved
and undetected.

1.2
Distributed and Centralized Repositories
■
9
Credit
history
Credit card
services
Inactive
customers
Customers
Sales
history
Current
sales
Returns
Supplier
purchase orders
Discontinued
products
Product
inventory
Figure 1.4
Retail data relationships.
The Rise of Data Warehouses
A recent response to the proliferation of corporate data across differing
organizational database systems has been the idea of the data warehouse.
In this emerging fusion of database and application control processes, cor-
porate resources are uniﬁed under a single integrated structure. Figure 1.5
illustrates a simple data warehouse.
Just a few years ago, each corporate division or government agency
created and administered its own databases. Discussions about database
systems nearly always centered around the organization of data (whether
tables were in third normal form, the best indexing scheme to facilitate
rapid joins, the handling of volatile and time ﬂuctuating data) rather than
the uniform integrity and sharability of data and the use of corporate data
resources to discover interesting trends and movements in the outside
world. Today, these discussions almost always begin with the concept of
a centralized data repository, the data warehouse.
Data warehouses are corporate- or agency-wide consolidations of an
organization’s operational data. They enforce common methods of key
deﬁnition and representation, domain integrity checks, and data valid-
ity management. As Figure 1.1 pointed out, the warehouse makes an
explicit commitment to the uniform management of diverse data ele-
ments, integrating principal information repositories to create a structure
representing a single, well-behaved and well-deﬁned interface for each
type of user in each of the organization’s business elements. It is this con-
sistent and uniform approach to data management that delivers a wide

10
■
Chapter 1
Foundations and Ideas
Personnel
Manufacturing
Inventory
Purchasing
Operations
Sales
Organizational warehouse manager
Corporate data warehouse
Query
facilities
(FuzzySQL)
Knowledge
mining
Decision
support
system
Figure 1.5
A simple data warehouse organization.
array of beneﬁts to the organization. One of the collateral beneﬁts of
a data warehouse is the opportunity it affords to perform wide-ranging
queries on the organization’s fundamental data. Another beneﬁt, aware-
ness of which is only now emerging among data warehouse designers
and managers, is the richness such a consolidated warehouse offers for
the discovery of new and proﬁtable pieces of information. Much (if not
most) of this new information, in the form of unsuspected relationships
between and among clusters of data, is hidden deep within the data
warehouse.
Yet this consolidation of corporate resources into a central data ware-
house need not imply that the warehouse is a monolithic entity, physically
located and physically administered by a central information technology
(IT) staff. The use of outsourcing, the development of private virtual
networks (PVNs), and the proliferation of highly distributed server cen-
ters throughout corporations have turned the idea of a centralized data
warehouse into the idea of a vigorous and logically homogeneous data
management environment. In many organizations, the data warehouse

1.3
The Age of Distributed Knowledge
■
11
has been profoundly inﬂuenced by the emerging age of distributed
knowledge.
1.3
The Age of Distributed Knowledge
To almost no one’s surprise the Internet has transformed our perspective
on the nature and the availability of knowledge. Perhaps nowhere has this
change been more keenly felt than in the corridors of business. Modern
“chief technology” and “chief information” ofﬁcers are struggling to fuse
seemingly diametrically opposed data management objectives: reliability,
availability, and integrity. At issue is the control of a corporation’s intellec-
tual property, which, as we move well into the next millennium, threatens
to become the sine qua non of an organization’s robustness. And, as
Figure 1.6 illustrates, corporate knowledge assets are no longer isolated
within the glass walls of the computer room.
Regardless of an IT manager’s focus on a centralized data warehouse,
corporate knowledge is distributed, often in an apparently random man-
ner, throughout the organization. Internet, intranet, and local networks
Enterprise
knowledge
assets
Internet
Local
networks
Intranet
Corporate computing resources
(mainframes and servers)
Figure 1.6
Corporate knowledge assets.

12
■
Chapter 1
Foundations and Ideas
provide users with nearly immediate access to on-line client databases,
human resource repositories, and departmental or division-level ﬁnan-
cials. An ever-increasing spectrum of users has access to corporate web
sites. Understanding, maintaining, and managing this sea of data often tax
the capabilities (and sometimes the vision) of the corporate data process-
ing division. Yet, a failure to account for and integrate a corporation’s
knowledge assets will become a critical competitive and survival issue in
the very near future.
Already corporate information ofﬁcers have learned that their future
is intimately tied to their past in that formal data modeling, data mining,
and knowledge discovery processes have become critical components
in such traditional business activities as risk assessment, customer prof-
itability analysis, budgeting, and new product positioning. These formal
methodologies, aimed at creating and maintaining a competitive edge, rely
on the security and integrity of information. As we discussed previously,
a consideration of these objectives has seen the rapid rise of the cen-
tralized data warehouse. But, as corporations and government agencies
steadily move toward a consolidation of their data assets in data ware-
house and data mart architectures the simple availability of vast amounts
of readily accessible data coupled with ever-faster gigahertz desktop com-
puters will drive an accelerated push toward deeper and broader forms
of analysis. Conventional off-line data mining using historical data will
give way to high-speed on-line analytical processing (OLAP) engines that
dynamically integrate history with the on-line data store (ODS).
Perhaps a larger problem facing management is the synthesis of infor-
mation into an adaptable knowledge base. Using this knowledge base,
an organization can construct and connect an entire suite of cooperative
and synergistic business process models. These models share informa-
tion and support their conclusions through an accumulation of evidence
only possible when they have access to the company’s complete informa-
tion framework. How to build these models and what technology should
be used are common themes in distributed data warehouse projects.
Throughout this book we look at an approach that combines several com-
putational intelligence techniques. Fundamentally, after examining fuzzy
database queries and fuzzy cluster analysis we look at the use of fuzzy rule
induction in the building of business process models.
1.4
Information and Knowledge Discovery
The assemblage of techniques used to discover these deep and nearly
invisible relationships is called data mining (or knowledge discovery).

1.4
Information and Knowledge Discovery
■
13
As a consequence of this technique’s ability to detect hidden variables
and hidden dependencies in often vast collections of data, organizations
of all sizes and missions are attempting to ﬁnd “nuggets of gold”; that is,
undiscovered relationships in data that will boost proﬁtability, improve
corporate productivity, and give the organization an edge in today’s
highly competitive environment. Some brief examples of how knowledge
discovery is being used include,
■
Marketing departments want to target sales in the most cost-effective
manner to the most receptive prospects and current clients. Marketing
managers need to open new markets and discover new audiences for
their available products (or ﬁnd a focused niche that can be rapidly
exploited by new products or the realignment of a current product).
■
Corporate information ofﬁcers and strategic planners need to evaluate
the risks of initiating capital-intensive projects based, if possible, on
the success or failure of similar projects.
■
Banks and other ﬁnancial institutions want to structure new services
and price current services based on customer proﬁtability and cus-
tomer growth potential. Banks need to understand how customers
use such services as automated teller machines, credit cards, and
reﬁnancing services.
■
Mortgage loan ofﬁcers and other credit-issuing ofﬁcials need deep
knowledge about the credit worthiness of customers, their potential
for defaults, and their potential as customers for new loans and related
services.
■
A large number of diverse organizations want to isolate anomalous
behaviors buried deep in large amounts of operating data. The
Treasury Department needs to recognize suspicious patterns in vast
amounts of international fund-transfer records. The Internal Revenue
Service needs to ﬁnd unusual patterns in corporate tax returns and
operating reports. Credit card companies are looking for sudden
changes in customer spending habits. Insurance companies and man-
aged health care organizations want to detect unusual patterns in
doctor claims.
■
Retailers and suppliers need fundamental information about the deep
relationships that drive sales, control inventory levels, and cause
shortages or surpluses. They want to separate seasonal changes in
purchasing habits from actual shifts in such habits and preferences.
■
Investment houses need to assess the issues of safety and suitability for
new investors or balance the distribution or concentration of stocks
or bonds in the portfolios of clients at various income levels.

14
■
Chapter 1
Foundations and Ideas
■
Engineering staffs need to determine the probable durability of new
products or the long-term mean time between failure (MTBF) and
mean time to repair (MTTR) for various classes of equipment.
■
Help desks and customer service departments need information on the
most likely centers of client problems, the predicted level of customer
satisfaction given a set of solutions, and the likely direction of new
problems.
Data mining has wide-ranging applications across many industries, span-
ning diverse applications in urban planning, transportation, petrochem-
icals, pharmaceuticals, manufacturing, managed health care, medical
informatics, communications, and military operations planning. More and
more, organizations are turning to their own data as a way of answering
questions such as “How can we improve business performance?,” “Where
are we vulnerable to competitive attacks?,” “How can we identify new
markets for existing products?,” and “Where should we position ourselves
for the next century?”
The Knowledge Discovery Process
Through knowledge discovery (omit qualiﬁcation), large collections of
corporate historical and active data are cleaned, organized, statistically
analyzed, and then explored in order to reveal any deep and potentially
proﬁtable relationships. Figure 1.7 illustrates this process.
Ideally, as indicated in Figure 1.7, a data mining process produces a
working model of underlying data relationships. More often than not,
however, the current generation of tools simply produces a report identi-
fying any discovered dependencies among data elements (the difference
between reporting and modeling, as will be noted later, is an extremely
important factor in the knowledge discovery process).
A Synthesis of Techniques and Technologies
Data mining is not a uniﬁed, single approach, although it has a single
aim. It involves a broad family of advanced technologies bridging the gulf
between conventional statistics and nonparametric analysis and the fron-
tiers of artiﬁcial intelligence (neural networks, fuzzy logic, and rule-based
expert systems). The aim of a knowledge discovery process is twofold:
identify collections of dependencies in data and ﬁx a degree of conﬁ-
dence in each discovered relationship. This can be expressed as the simple

1.4
Information and Knowledge Discovery
■
15
Corporate
Database repositories
Archival history
Division/
department
Private
and public
Spreadsheets
and external files
Data
scrubbing
Extract, transform,
load (ETL)
Knowledge
discovery
Reports
Models
Figure 1.7
The data mining process.
ordered relationship
RD ←

Xn
i ⊗Y n
j ⊗. . .

· Cn,
(1.1)
where RD is the dependency, X and Y form a set of related variables, and
Cn is the conﬁdence in the accuracy or strength of this relationship.
This degree of conﬁdence forms the basis of the believability met-
ric associated with the discovered system. Relationships supported by

16
■
Chapter 1
Foundations and Ideas
sparse, corrupt, or unreliable data are ranked with less conﬁdence than
relationships based on a higher preponderance of evidence. Most data
mining tools provide a ﬁltering mechanism so that relationships with a
conﬁdence below some threshold are automatically discarded (or stored
in a special audit ﬁle).
Two Types of Knowledge Discoveries
Knowledge discovery comes in two basic ﬂavors: supervised and unsuper-
vised (often called, respectively, directed and undirected). Both have their
speciﬁc uses and both provide methods for isolating system behaviors
in large database repositories. Although they have fundamentally differ-
ent approaches to data mining, these methods share many of the same
computational elements.
Supervised Knowledge Discovery
Supervised knowledge discovery (data mining), as the name implies, is
controlled by the knowledge engineer and system designer. This tech-
nique uses an objective function (the dependent variable) and a set of data
elements, the independent variables. Supervised database exploration
attempts to identify causal relationships between independent and depen-
dent variables, isolate the degree of correlation for each set of variables,
and construct a model (or a report) showing the web of dependencies.
Supervised data mining tends to be highly scalable (that is, it can handle
large amounts of data in time frames that do not increase unreasonably),
generally fast, and converges toward a set of solutions.
This type of knowledge discovery is used when a deﬁnite goal is avail-
able and the user is seeking to uncover how changes in the data state
inﬂuence the goal. The following are two examples of actual supervised
data mining projects.
■
A re-insurer offers disability insurance for medical professionals (such
as pediatric surgeons). One provision of the insurance is an “own
occupation” clause that pays if the individual is unable to continue in
his or her own occupation. The insurer is suffering large, long-term
losses due to an unexpectedly high number of medical professionals
making “own occupation” claims but continuing to work in related
medical ﬁelds. They used a supervised data mining search of the com-
bined policy holder, claims, and medical history databases to ﬁnd a
clear relationship between “own occupation” claims and policyholder
characteristics.

1.4
Information and Knowledge Discovery
■
17
■
The Chief Information Ofﬁcer of a large bank needed a way to con-
trol cost overruns on a rising number of high-technology projects in
the information systems group. Using a large database of failed or
inadequately delivered projects, the bank ranked each project with
a value of from 0 to 1 according to its perceived value (0 as a failed
project with no value to 1 as a completely successful project). They
also created ﬁelds to indicate the type of project, its perceived level
of complexity, and its degree of dependence on advanced technol-
ogy. A supervised data mining analysis discovered a clear relationship
between low user satisfaction (as well as project failure) and a set of
important project characteristics (surprisingly, the initial budget for
the project rather than the ﬁnal cost was an important indicator of
long-term project success).
Unsupervised Knowledge Discovery
Unsupervised knowledge discovery starts out with a tabula rasa, a clean
slate. It has no predeﬁned objective function. In unsupervised mode,
the knowledge discovery process selects one of several approaches to
domain simpliﬁcation, usually automatic cluster detection or dependency
graphs. In either case, the unsupervised approach makes no distinction
between dependent and independent variables. Rather, the process looks
for collections of elements in the database that share similar properties.
These similarities form clusters in the data, and collections of overlapping
clusters reveal patterns among the clusters.
Unsupervised data mining often raises several issues of scalability
(if some form of parallel evaluation is not used) and is generally slow,
but it can converge toward multiple sets of solution states. This type of
deep knowledge discovery is used when a speciﬁc goal is not available or
when the user is attempting to uncover latent or hidden relationships in
data. The following are two examples of actual unsupervised data mining
projects.
■
A large international retailer was seeking to increase its domestic sales
and expand into new markets. Three possible expansion routes had
been identiﬁed, but management could not reach a consensus on
which to pursue. Each expansion option entailed the risk of many
millions of dollars, a marketing commitment over several years (thus
signiﬁcantly increasing the opportunity cost risk), and exposure to
new competitive pressures. A committee of executives from person-
nel, marketing, purchasing, store operations, and information systems
initiated an unsupervised data exploration of the company’s combined

18
■
Chapter 1
Foundations and Ideas
sales, inventory, client, credit, and store performance databases. From
this analysis they generated two functional models: one relating sales
and costs to demographics and the other relating store operations
to consumer purchasing habits. Using these models, a successful
and cost-effective movement into the second expansion route was
executed.
■
Managed health care accounts for 13% of the total U.S. gross national
product. Estimates of abuse and fraud in managed health care run as
high as 10 to 12% of this total, or approximately $600 billion annu-
ally. Several years ago a collection of insurance companies focused on
provider- or doctor-initiated fraud. They wanted a way to ﬁnd anoma-
lous behaviors in doctors’ claims. This suggested a nonparametric
approach to anomaly detection; that is, behavior patterns would be
considered unusual only in relation to their peers (doctors in the same
geographic region, of the same specialty, and associated with the same
size of organization).
A deep unsupervised data mining project examined a very large
database of claims and doctor proﬁles looking for clusters of simi-
larity in billing practices, patient characteristics, and work habits.
By iterating through the clusters, patterns emerged that identiﬁed
relationships between anomalous payment, client visitation, billing
structure, and periodic treatment patterns. A ﬁnal set of models com-
bined evidence from many clusters to provide a high level of speciﬁcity
in detecting and quantifying fraudulent and other forms of anomalous
behaviors.
By itself, cluster detection and generation are not solutions to the data min-
ing process. Once a population of clusters is identiﬁed, rule-generation
facilities are used to interpret the meaning of each cluster and produce
the ﬁnal model. In advanced unsupervised data mining systems, genetic
algorithms explore many collections of clusters and their associated rules
in parallel.
1.5
Data Mining and Business Models
At the heart of any data mining exercise is an occasionally unspoken objec-
tive. The user wants to develop a model of a particular business process.
These are called, in fact, business process models. A model, as illustrated
schematically in Figure 1.8, is a representation of some coherent, inter-
dependent system of processes. These processes affect each other in a

1.5
Data Mining and Business Models
■
19
Policies
History
Client data
Databases
Rules
Independent and
dependent variables
Model
Input sources
Results
Confidence
Evidence
Outcome
Figure 1.8
The idea of a model.
predictable way, and, thus, given a set of input values, we can predict the
likely set of outputs.
A valid model connects a set of input values (the independent vari-
ables) to a set of output values (the dependent variables) through some
type of functional relationship. Mathematically we can represent this as
Vd ←f

v1
i , v2
i , . . . , vn
i

,
(1.2)
where Vd is the output dependent variable, often called the solution
variable, and vi is the input independent variable.
Difﬁculties in Building Business Models
Of course, real-world process models are not nearly as clean and straight-
forward as this functional relationship might suggest. In fact, for many
problems the model can only approximate a true function. This is due to
several important and difﬁcult-to-handle factors.
Data Availability and Noise
Business processes are often ill deﬁned and cluttered with “noise”; that
is, the underlying processes contain a great deal of extraneous and even

20
■
Chapter 1
Foundations and Ideas
random information. In addition, quite often the values for many impor-
tant process components are either missing, sparse, or unreliable. For
models developed through a data mining process — wherein the intel-
ligence is extracted principally through data relationships — the lack of
clean, reliable, and complete data is often fatal.
Imprecision and Vagueness in Model Parameters
The permissible range of variables is not delineated by crisp or precise
boundaries nor are the variables connected through crisply deﬁned depen-
dencies. That is, as one example, a relationship Z = f (X and Y) is not
captured by a simple arithmetic expression (including ordinary and par-
tial differential equations) but by a function; that is, it is in most cases
“fuzzy.” Thus, we might have a relationship such as the following.
If X is somewhat less than Y
then Z is slightly increased
If X is expensive and Y is high
then Z is about R
When (X,Y) is changing rapidly
Z quickly approaches zero
Here, the qualifying coefﬁcients such as “somewhat less,” “slightly,”
“expensive,” and “high” are parameters of a fuzzy relationship.
Nonlinear and Chaotic Relationships
Most business processes are deceptively complex. They are somewhat
chaotic and highly nonlinear. The characteristics of nonlinearity and
chaos mean that (1) the process is very sensitive to initial conditions and
(2) that small perturbations to the system or small inputs do not neces-
sarily result in proportionally small outputs. Nonlinear equations such as
the logistic map
xn+1 = Axn(1 −xn)
(1.3)
have regions of stability and instability separated by sudden bifurcation
points. In business models, this same type of nonlinear behavior is often
associated with complex lead- and lag-time relationships. As a simple
example, consider a predictor of proﬁts in one time period based on
sales and costs in related periods.

1.5
Data Mining and Business Models
■
21
If costs(t −N) −proﬁts(t) is much less than sales(t + N),
then proﬁts(t + N) are about discounts*sales(t −N).
When many such rules are combined in a single business policy, the result-
ing system becomes highly nonlinear. And it is this intrinsic nonlinearity
that makes the evolution of business models so difﬁcult.
Adaptive Feedback Processes
In the real world of business, commerce, and governmental decision
making, the state of a system is often dependent on previous states of the
system. That is, as illustrated in Figure 1.9, the output of the model is fed
back into the model as one of its input parameters.
In fact, this feedback can permanently alter the way the model exe-
cutes. Adaptive feedback means that the rules governing the behavior of a
model are reﬁned based on the changing state of the outside world (such
as reﬁnement of process characteristics based on an increase in available
historical data).
It is precisely these difﬁculties that knowledge discovery tools and
techniques attempt to overcome. They do this through a fusion of
advanced nonparametric parameter or coefﬁcient estimating and several
Costs (t−1)
Policies
Costs (t)
Outcome variable
Sales (t−1)
Model
Input variables
Independent and
dependent variables
Rules
Feedback
loop
Figure 1.9
An adaptive feedback model.

22
■
Chapter 1
Foundations and Ideas
types of artiﬁcial intelligence technologies. The degree to which they are
successful in capturing the nature of deep relationships depends on how
well they produce a believable and usable model. The criterion that a data
mining tool must generate a working model separates two important and
elegant classes of these tools. We will return to this issue in a discussion
of rule-based models.
The Importance of Models Versus Reports
Many data mining tools produce a static snapshot of their uncovered rela-
tionships. This is not a model, but rather a report. The distinction is
often missed by users of knowledge discovery tools without a background
in dynamic systems. The difference is both fundamental and extremely
important. As an example, given an N × 2 matrix of numbers (large num-
ber of rows, two columns), we can use a statistical analysis program to
discover that some numbers in column 1 are related to some numbers
in column 2. A cluster analysis might produce a relationship such as that
shown in the following table.
Cluster1
A[I, 1]
A[I, 2]
Corr
[103,217]
[40,60]
.78
This relationship says: When A[I,1] 103 and A[I,1] 217, then A[I,2] is
in range of [40,60] with a conﬁdence of .78 degree. On the other hand,
a dynamic system model with the ability to represent imprecise or vague
concepts might generate a rule that says
If A[I,1] is in the vicinity of 160,
then A[I,2] is about 50,
where vicinity of 160 is a broad trapezoidal fuzzy set5 and about 50 is a
bell-shaped fuzzy set.
A dynamic system is a set of rules. These rules, in the form of if <con-
dition> then <action>, are evaluated by a special rule processing system
called an inference engine. The inference engine decides which rules
should be executed, orders the rules in the proper sequence, supplies
data to the variables used by the rules, and then executes each rule in
5 A fuzzy set is a way of representing vague and imprecise knowledge through the concept of a
partial membership in the set or sets associated with a concept.

1.6
Fuzzy Systems for Business Process Models
■
23
turn. When all the rules have been executed, the inference engine hands
over the results to the application program that invoked the inference
engine.
1.6
Fuzzy Systems for Business Process Models
A more focused and structured approach to model building in the age of
distributed knowledge, fuzzy systems provide the means of combining,
weighing, and using multiple competing experts. Often these experts
are not people but other knowledge sources (such as other business
models and expert systems). For example, Figure 1.10 (somewhat sim-
pliﬁed) shows how several models work cooperatively in a distributed
environment.
In a distributed environment, multiple experts compete for atten-
tion, either as peers in the decision process or as components of a larger
Sales
Network portal
Pricing
model
Inventory
model
Product
budgeting
Sales
forecast
model
Price
Required
On
hand
Forecasted
units sold
Inventory
Customers
Figure 1.10
Multiple business process models.

24
■
Chapter 1
Foundations and Ideas
decision making model. The ability of fuzzy models to easily incorporate
evidence from several expert sources (as well as assign degrees of cred-
ibility to each source) makes them an ideal vehicle for building shared
decision models in the distributed data warehouse and data mart environ-
ment. As an example, Figure 1.11 shows a product pricing model and its
various distributed sources of information.
In Figure 1.11, the bold parts of the business rules represent fuzzy sets.
These fuzzy sets are combined under the methods of fuzzy composition.
Combining rules in this way accumulates evidence for the ﬁnal product
pricing position. Because fuzzy models are adept at handling evidence
from many sources, they provide a ﬂexible and powerful method of mod-
eling distributed processes. These are typically the types of processes
we ﬁnd at the data warehouse and data mart level in Internet-centered
organizations.
Sales
Inventory
Customers
Manufacturing
Network portal
Network portal
Competitive
pricing model
Our Price Must Be Low
Our Price Must Be High
Our Price should Around 2*MfgCosts
If Competition.Price Is Not Very High
then Our Price should be Near the Competition Price
Figure 1.11
A product pricing model.

1.7
Evolving Distributed Fuzzy Models
■
25
1.7
Evolving Distributed Fuzzy Models
Because fuzzy sets can represent approximate patterns, fuzzy models
are especially easy to evolve using knowledge discovery techniques.
Rule indication — the isolation and extraction of if-then rules from
large databases — provides a way of creating a prototypical fuzzy model
from patterns occurring naturally in data. Figure 1.12 illustrates how
this step is used to combine knowledge from several distributed data
sources.
By incorporating a rule induction step in your business models, you
can maintain currency with the external world. As the long-term behav-
ior of your customers changes, the model can adapt to those changes
by discovering new rules reﬂecting a shift in purchasing habits (as one
example). Rule induction, of course, should not be viewed as a com-
plete model building technique. Rather, it can be used to “prime the
pump” when you have many experts contributing knowledge to a set of
models.
Network portal
Network portal
Fuzzy
models
Rule induction
(knowledge discovery)
Rules
Fuzzy
knowledge base
Sales
Inventory
Manufacturing
Customers
Figure 1.12
Building a fuzzy model with rule induction.

26
■
Chapter 1
Foundations and Ideas
1.8
A Sample Case: Evolving a Model for
Customer Segmentation
More and more companies today, small and large, are turning to the
World Wide Web (WWW, also called the Web or the Net) as a vehicle to
advertise their company’s services and products, provide on-line catalogs
or demonstration software, handle technical support, and even sell their
products directly to the end user. In the demographics of primary interest
to today’s corporations (technically sophisticated, upper-income fami-
lies), high-speed computers are becoming increasingly common. Most
of these families have large disposable incomes and are not adverse to
purchasing a wide spectrum of premium goods and services.
However, the Web is saturated with casual users. As a result, isolating
potential customers from a vast, roiling sea of casual browsers has become
increasingly difﬁcult. Corporate sites are often visited thousands of times
every day. A corporation has only a small window of opportunity to attract
a visitor and make him or her a customer. Companies have reacted to
this situation in a variety of ways. Some have used passive techniques
to capture information about any visitor that shows interest in the page
content. Others have gone to either a closed or open subscription service,
providing access to their pages to a select cadre of potential clients. This
last approach is used by corporations in such diverse industries as banking,
insurance, investment, retail, and management consulting.
Even with a tighter focus on potential customers afforded by subscrip-
tion services, the Internet is a fast-paced environment requiring advanced
Customer
profiles
Products
Customer
behavior patterns
Access
authentication
Target Marketing
expert system
Store and
access behaviors 
World Wide Web
home page
Figure 1.13
A target marketing system.

1.8
A Sample Case: Evolving a Model for Customer Segmentation
■
27
Products
Case-base
Customer
behaviors
Rule
evolver
Rule
execution
Expert
rules
Example
customer
cases
Customers
Pattern
matching
Level of interest
ranking
Behavior (buying)
prediction
Product
selection
Pricing
strategy
Customer activity history
Fuzzy knowledge base
Figure 1.14
A case-based reasoning system.
technological approaches to keeping and expanding a customer base.
One emerging approach to solidifying and growing customers is target
marketing. For corporations that maintain customer proﬁles and pur-
chasing (as well as general interest) behaviors, a knowledge discovery
approach can often ﬁnd relationships between product characteristics
and clients. This allows companies to offer clients and prospects new
services, display new products, and tailor their marketing approach on
a customer-by-customer basis. Figure 1.13 schematically illustrates how
a target marketing system is connected directly to a corporation’s Web
home page.
A target marketing expert system is built by the knowledge discov-
ery and data mining process. It is often embedded in either a traditional
expert system environment or in a case-based reasoning (CBR) system.
Figure 1.14 illustrates how a CBR system uses past cases to predict a
possible outcome.
In this environment, a customer and his or her purchasing habits con-
stitute a “case,” and data mining is used to discover the rules that can

28
■
Chapter 1
Foundations and Ideas
TABLE 1.1
Customer Activity Case Base
Site Visitation Counts
Product Statistics
This
This
Past 6
Products
Products
Age
Gender
Week
Month
Months
Total
Visited
Bought
33
M
16
172
1281
19
2
58
M
44
304
3913
53
0
17
F
73
508
747
21
9
32
M
166
485
2036
59
0
35
F
179
202
409
6
4
match a new customer to products or services based on their similarity
to existing customers and their preferences. The underlying “case base”
need not be any special database. In fact, the case base can be a spread-
sheet or simple binary ﬁle containing customer proﬁles and their browsing
and purchasing histories. Table 1.1 illustrates a possible structure for the
underlying customer proﬁles.
Of course this example is somewhat abbreviated. The visitation statis-
tics might include such values as length of stay, number of pages visited,
time of day, average time per page, and so forth. The product statistics
would also include product identiﬁer (perhaps the SKU (Stock Keeping
Unit) used in retailing to identify the lowest, unique level of product
inventory), product class, retail price, actual price, and quantity pur-
chased.6 Such a case base could include summary records (as implied by
Table 1.1) or individual transaction records. With this type of information,
a rule induction system can generate simple high-level rules that relate
visitation frequency to purchasing propensities, such as the following.
If age is young
And gender is male
And (productclass is XYZ
And site.visitcount is high)
Then
ProductsBought is elevated;
More complex rules of this nature might be generated. Such rules can also
indicate, for example, preferences for particular products, purchasing
behaviors at particular times of the day (perhaps discriminated by age or
6 From a customer relationship and sales analysis perspective it might also be interesting to
capture the transaction response times for the web server for each customer — thus detecting
patterns that relate machine performance with customer buying behaviors.

1.9
Review
■
29
gender), purchasing associated with discounted products, and so on. The
important aspect of such an approach to customer segmentation and cross
marketing is the fact that the rules are deduced from (discovered in) the
underlying transaction data. As purchasing habits, new web structures,
new product lines, and other internal and external market pressures arise,
the system will automatically discover a new set of rules.
1.9
Review
Data exploration and modeling are critical components of operational,
tactical, and strategic planning. However, in this age of distributed com-
puting, data, and knowledge, building models solely from subject matter
experts and a conceptual understanding of the problem is a long, pro-
tracted, and error-prone process. Coupling knowledge discovery with
advanced forms of adaptive fuzzy models can produce highly robust sys-
tems that can detect and repair their own logic. After reading this chapter
you should have an appreciation for the following.
■
The nature of modern information architectures
■
How data and knowledge are distributed over networks
■
The difﬁculties inherent in building models of typical business systems
■
The role knowledge discovery plays in building models
■
The differences between supervised and unsupervised data mining
■
How fuzzy systems can contribute to building robust, sustainable
models
■
How fuzzy models can be evolved from databases and other sources
Finding patterns in data (data exploration), quantifying these patterns
into a predictive system (model building), and dynamically reorganiz-
ing a model to optimize its performance (evolutionary strategies) are the
foundations of building modern information systems and client applica-
tions. Understanding the relationships among these concepts is crucial
to maintaining a competitive marketing lead, assessing the effectiveness
and consequence of public policy decision, and maintaining enterprise
viability in a turbulent global economy. Following on this necessity, the
next chapter surveys various types of models and how they are related to
the types of decisions and the types of data in the problem space.

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 2
Principal Model Types
This section provides the foundations (as well as the background) we need
to discuss the concepts and features of predictive models. In examining the
ways in which organizations approach the model building process, a wide
variety of techniques emerges. These techniques often center on the types of
models we need and how the model addresses such issues as the nature of
the data, the reasoning processes that manipulate the data, and the types
of outcomes the model can produce. The following takes a “high-altitude”
look at the basics of creating and using a model.
Mathematical models — traditionally differential or difference equa-
tions — are notoriously difﬁcult to build, tend to be very brittle, and are
by their nature extremely difﬁcult to extend and maintain. For these rea-
sons, model builders have recently, within the past 15 to 20 years, turned
to more ﬂexible methods based on various forms of machine learning.
Some of these approaches incorporate expert knowledge, others learn
behaviors directly from archives of historical data, and some combine the
two approaches. Although by no means exclusive, the most commonly
used methods are as follows.
■
Expert systems: A knowledge base of if-then rules deﬁning solutions to
problems in a speciﬁc and highly restricted domain. The rules forming
a conventional expert system are derived from one or more subject
matter experts (SMEs).
■
Statistical learning theory: A collection of techniques that quantify
and learn the periodic (seasonal and cyclical) behavior of data over
time. Many statistical learning approaches employ forms of Bayes’
Theorem, which estimates values based the amount of cumulative
evidence.
■
Neural networks: A nonlinear classiﬁcation system of interconnected
nodes that can learn the underlying behavior patterns in a collection
of data using a set of examples.
31

32
■
Chapter 2
Principal Model Types
■
Decision trees and classiﬁers: A set of techniques for learning how to
effectively classify patterns. Decision trees produce hierarchical maps
between data attributes and can be used to produce a set of if-then
rules.
■
Trend analysis: A set of curve-ﬁtting methods that can discover and
quantify both linear and nonlinear trend lines through multidimen-
sional sets of data. Trend lines can detect and predict the underlying
periodicity or seasonal curves for time-varying data.
Systems and knowledge engineers use the term model in a variety of con-
texts, but nearly all of these imply some form of digital implementation of
a well-deﬁned process. However, the world of models and model building
encompasses a wide variety of representations. Although we are primar-
ily concerned with system models, the evolution of such models often
crosses over into or encompasses other modeling organizations. Figure 2.1
shows the basic model types and some of their possible interconnections.
Naturally, neither the model taxonomies nor the model boundaries are
absolutes. As the dashed lines in Figure 2.1 indicate, one model may be the
prelude to another (we often develop a narrative model before expanding
our ideas into a mathematical or system heuristic model). Further, the
classiﬁcation of a model into one class or another is not always possible,
Mathematical
Heuristic
Analog
Physical
Narrative
Procedures
Experts
Experts
if A and B
then C
Model
types
N
Σ
i = 0
H(f) =~
∂2f
∂x2
Figure 2.1
Types of models.

Chapter 2
Principal Model Types
■
33
TABLE 2.1
Model Categories (Taxonomies)
Nature of Model
Organization and Use
Narrative
These models often consist of experts collaborating on a formal representation of some
highly focused system. Examples of this include the Delphi method and many aspects
of market research. In such models, a collection of questionnaires is repeatedly analyzed
among the experts to typically predict such critical issues as vulnerabilities, market trends,
long-range sales and margins, and penetration strategies for new lines of business.
Physical
These models are constructed to test or evaluate some essentially physical system.
Examples of physical models include precision aircraft models for use in wind tunnels,
architectural renderings of buildings and bridges, and the molecular construction kits
used in organic chemistry and genetics to represent such things as benzene rings and the
helical DNA molecule.
Analog (Simile)
These models combine the properties of one system to describe the behavior of another
system using different physical forms. Thus, they are similes (i.e., one system functions
much like any other system). Analog models work because there is a similarity or
parallelism between the underlying forces that drive both models. Until the recent
accessibility of the personal computer, electrical and mechanical analog systems were
routinely used to model complex networks such as process plants and highway trafﬁc
ﬂows.
Mathematical
Mathematical models (including statistical models) came into their own with the
common availability of digital computers (through time-sharing in the early days and
now routinely on desktop and laptop personal computers). A mathematical model
consists of equations, often with interdependencies. The ubiquitous spreadsheet is a
prime example of a mathematical model. Control engineering systems and embedded
controllers are also examples of mathematical models. Many of the models used in
knowledge discovery and data mining are also mathematical models. These include
neural networks and other forms of classiﬁcation schemes, such as decision trees (e.g.,
ID3 and C4.5), classiﬁcation and regression trees (e.g., CART) and chi-squared
automatic induction (CHAID) algorithms.
Heuristic
The recent rise in machine intelligence and expert system technologies has introduced
another type of model into the mix: the heuristic model. Such a model is often called an
if-then-else production system, which often forms the core knowledge repository of
today’s decision support and expert systems. These models depend solely on the
high-speed, high-level computational capabilities of the computer. Heuristic models
embody “rules of thumb” and other business processes. We often refer to them as
policy-based models. Heuristic representations constitute the vast majority of modern
business process models (BPMs).
in that the boundaries are very permeable. Table 2.1 summarizes how
these models differ and how they are used.
The last two taxonomic classes — mathematical and heuristic — form
the focus of nearly all business prediction models. These are roughly
classed as symbolic models and generally incorporate algebraic and
intellectual relationships. In the modern sense of the term model, we

34
■
Chapter 2
Principal Model Types
generally combine the two taxonomies into a single or hybrid class: the
knowledge-base model.
2.1
Model and Event State Categorization
There is a more fundamental classiﬁcation of models based on the way
model states are generated and how model variables are handled. Table 2.2
outlines the partitioning of such classiﬁcation, which divides models into
discrete or continuous and into deterministic or stochastic.
Technically, the classiﬁcation of models as discrete or continuous
refers to the model’s composite variable organization, but in actual fact
and practice the term is used to describe the model’s treatment of time.
Many models have clear and precise demarcations of time that break up
the system into regular intervals (such as many queuing applications, pro-
duction or project schedules, trafﬁc analysis models, portfolio safety and
suitability models, and so on). Other models have a horizon that varies
smoothly across time. These continuous models are rare in the business
world, although they do occur in nonlinear random walk models that
attempt to follow the chaotic trends of the stock market. In any event,
of course, few continuous models are actually implemented continuously
but use a form of periodic or random data sampling.
The model state categorization — a fundamentally important perspec-
tive on how the model is constructed — reﬂects the ways in which the
underlying model relationships are or can be described. Outcomes in a
deterministic model can be predicted completely if the independent vari-
ables (input values) and the initial state of the model are known. This
means that a given input always produces a given output. The outcome
for a stochastic system is not similarly deﬁned.
TABLE 2.2
Model Classiﬁcation by Internal Structure
Event State
Discrete
Continuous
Model State
Deterministic
Spreadsheets
Dynamic ﬂow
Econometric
Process control
Budget
Differential equations
Inventory control
Stochastic
Quality measurement
Queuing models
Monte Carlo
Market share
Econometric projections
Price positioning
Game-theoretic models

2.2
Model Type and Outcome Categorization
■
35
Stochastic models introduce a degree of randomness or chance into a
possible set of outcomes. This randomness is intrinsic to the behavior of
the model and couples the values of the independent variables (which are
randomly derived) to the values of the outcome or dependent variables
(which are the result of processing these random values).
2.2
Model Type and Outcome Categorization
From a knowledge and system engineer’s perspective, the most crit-
ical model classiﬁcation isolates the type of outcome. There are two
broad types: predictive and classiﬁcation. Like taxonomic partitioning,
these types are not “pure” (homogeneous), and models often involve
both. Table 2.3 outlines model families sorted into clusters according to
underlying model taxonomy and outcome type.
A predictive model generates a new outcome (the value of the depen-
dent variable) based on the previous state of independent variables.
Regression analysis, as an example, is a predictive time-series model.
Given a least squares linear interpretation of data points x1, x2, x3, . . . , xn,
the regression model predicts the value of point xn+1 (and a vector of
subsequent points with varying degrees of accuracy). On the other hand,
classiﬁcation models analyze the properties of a data point and assign
it to a class or category. Cluster analysis is a classiﬁcation model. Neu-
ral networks are also predominantly classiﬁers (they activate an outcome
neuron based on the activation of the input neurons). Such taxonomies,
however, are hardly rigid. A rule-based predictive model can, under some
circumstances, be viewed as a classiﬁer. And many classiﬁcation models,
especially those based on neural networks and decision tree algorithms,
TABLE 2.3
Model Classiﬁcation by Taxonomy and Outcome
Outcome
Predictive
Classiﬁer
Model Type
Knowledge-
Expert system
Neural network
based
Fuzzy system
Expert system
Evolutionary program
Genetic algorithm
Mathematical
Regression analysis
Cluster analysis
Statistical learning
Correlation
Classiﬁcation and regression
trees (CART)
Radial basis functions
Self-organizing maps
Adaptive learning

36
■
Chapter 2
Principal Model Types
can generate rules that produce a model prediction. After all, a neural
network that learns to classify credit applicants into Good, Review, and
Reject is making a prediction about the probable future fate of a loan.
2.3
Review
From an understanding of distributed intelligence and data mining in
Chapter 1, this brief chapter provides a review of the various forms a
model can take. Model building is more than applying statistics, creating
rules, or generating a decision tree. Model building involves understand-
ing the nature of the problem and ﬁnding the system representation that
best reﬂects the expectations of the consumer, the capabilities of the sys-
tems engineer, and the availability of supporting data. From this chapter
you should now understand the following.
■
The various types of models and their uses
■
The meaning of discrete and continuous variables
■
The nature of deterministic and stochastic models
■
How predictive and classiﬁer models work
■
The role knowledge-based systems play in modern models
A model represents the important aspects of some complex system. Our
ability to understand the nature of models and how they relate to our anal-
ysis objectives is a critical prerequisite in gaining a deeper understanding
of how systems that are tightly connected and highly distributed interact.
Moving from the concepts of modeling and the nature of models, the next
chapter surveys the types of technologies and techniques used to actually
build computer models.

■■■
Chapter 3
Approaches to
Model Building
As we saw earlier, discovery is not a single discipline but involves a
combination of many techniques and technologies. Some of these tech-
niques are used in combination with others as support or ﬁltering services,
whereas others constitute the major components of the data mining service.
We now review some of the major methods of data mining and knowledge
discovery.
This section is not intended as an exhaustive or even partially complete
description of the various model building techniques (and this is espe-
cially true of the section on statistics). The primer is intended simply
as an overview of each methodology, including brief observations of
strengths and weaknesses. Comments about the relative merits of a par-
ticular approach are meant as general observations. Particular software
implementations of the method or different approaches using alternate
but related techniques could change one or more strengths or weakness.
3.1
Ordinary Statistics
This is generally the best starting point for any data mining project and
often satisﬁes the needs of many organizations whose goal is simply to
understand the mechanics of obvious data relationships. For statistical
data mining projects, a complete statistical analysis of existing data repos-
itories provides a keen understanding of how the data are clustered,
their degree of variance, and the tightness of existing two-dimensional
and three-dimensional relationships (generally expressed through scatter
graphs and other forms of curve ﬁtting).
37

38
■
Chapter 3
Approaches to Model Building
For a population of data, the statistician must determine the data’s prin-
cipal statistical properties. These properties include, at a minimum, the
mean or average value, the mode (the most frequently occurring value),
the median (the middle value when all the observations are sorted into
ascending or descending order), and the standard deviation (the amount
of variance around the mean, that is, a measure of whether the distribu-
tion forms a wide bell curve around the average or a narrow bell curve
around the average). The mean and the standard deviation, as two of
the principal population descriptors, are known are the ﬁrst and second
statistical moments.
A thorough statistical analysis of large databases can often uncover
many unexpected and interesting relationships. In addition to the ordi-
nary descriptive statistics discussed in the previous paragraph, a more
extensive statistical analysis attempts to ﬁnd a wide range of subtle rela-
tionships between variables in the data. Some of the advanced statistical
techniques used in an extensive analysis (but not an exhaustive or even an
extensive list) include an analysis of variance (which ﬁnds difference in the
means of one or more data collections), correlation analysis (which ﬁnds
the degree to which changes in one variable are reﬂected by or related to
changes in another variable), and linear regression (which ﬁnds the trend
line between several variables and, in so doing, provides a mechanism
for predicting the future values of related variables). Carefully studying
the outcome from an advanced statistical analysis provides the data min-
ing engineer with a deep appreciation for the fundamental dependencies
locked in the data.
For more advanced data mining efforts, especially those using neu-
ral networks and fuzzy systems (see discussion following), the third and
fourth statistical moments should also be calculated (the kurtosis and
skew, respectively, of the distribution). Kurtosis is a measure of the ﬂat-
ness or peakness of the distribution, while skew measures the degree
to which data are shifted to the right or the left of the mean and thus
distorts the symmetric bell-shaped form of a normal distribution. Many
of these explorations into the realm of population distributions lead the
knowledge engineer into the supporting domain of nonparametric statis-
tics. The strengths and weaknesses of ordinary statistics are outlined in
Table 3.1.
3.2
Nonparametric Statistics
Very often statistical data mining is concerned with ﬁnding the over-
lap between data points in two populations (such as current clients and

3.2
Nonparametric Statistics
■
39
TABLE 3.1
Strengths/Weaknesses of Ordinary Statistics
Strengths
■
Proven mathematical methods
■
Many easy-to-use tools available
■
Large number of skilled statisticians to handle experimental design and
interpret results
■
Designed for work with large data populations
■
Isolates important and critical population parameters
■
Can ﬁnd causal relationships among large numbers of variables
Weaknesses
■
User must know what tests to apply and how to interpret the results; that
is, what to look for
■
Sensitive to population distribution assumptions and the actual popula-
tion distribution
■
Hypothesis formulation can be difﬁcult and susceptible to Type I and Type
II errors
■
Results very sensitive to data bias
■
Outliers can cause problems
■
Statistical correlation does not necessarily mean causation
potential customers). Standard statistical measures such as an analysis of
variance assume that the parent populations have a normal (Gaussian)
distribution. In practice, knowledge engineers seldom know the underly-
ing distributions in historical data. Further, collections of historical data
are seldom normally distributed. When population distributions are not
Gaussian or have signiﬁcant skews, nonparametric tests give the data
mining engineer a powerful set of tools for exploring the relationships
inherent in multiple populations. Although some assumptions about the
parent distribution are involved in nonparametric statistics, these assump-
tions are generally fewer in number, have a weaker impact on the validity
of the correlation analysis, and are easier to satisfy with existing data than
the more rigorous requirements of parametric correlations. In this regard,
nonparametric tests are frequently called distribution-free tests. In any
case, the range of statistical tests in nonparametric analysis is more lim-
ited than the broad class of tests in ordinary statistics and deals mostly with
the distribution of nominal and ordinal data, often examining the depth
of an interval displacement (plus or minus) from an arbitrarily chosen
pivot point in the data. The strengths and weaknesses of nonparametric
statistics are outlined in Table 3.2.

40
■
Chapter 3
Approaches to Model Building
TABLE 3.2
Strengths/Weaknesses of Nonparametric Statistics
Strengths
■
Advanced and proven mathematical methods
■
Techniques for ﬁnding parameters that describe an unknown population
■
Useful in discovering the actual distribution in preparation for a more
detailed analysis by ordinary, parametric statistics
■
Ideal for looking at anomalies and differences in central measures of the
population that have some underlying order or ranking
Weaknesses
■
Limited range of analytical tools.
■
Not often included in statistical software products.
■
Technique primarily familiar to advanced statistical users. This means
that skilled statistical support for nonparametric analysis may be in short
supply.
■
Appropriate mainly for nominal (numbers that stand for the items
themselves) and ordinal data. Nominal numbers stand for the items them-
selves, whereas ordinals, in ordinary usage, are 1st, 2nd, 3rd, and so on
as opposed to 1, 2, 3, and so on (cardinal numbers). Ordinal numbers
are properties of well-ordered sets.
■
Difﬁcult to structure hypothesis tests that can be easily veriﬁed.
3.3
Linear Regression in Statistical Models
Data mining engineers and model builders are concerned with issues
involved in prediction. In a statistical model, the problem of predicting
an outcome is based on the correlation between two or more variables,
and consequently correlation and prediction are closely allied topics.
Correlation is measured in a scale from −1 to +1.
Correlation and Prediction
If the correlation between two variables X and Y is zero, we can assume
that they have a random relationship to each other. That is, a knowledge
of X tells us nothing about Y and a knowledge of Y tells us nothing about
X (therefore, predicting X from Y or Y from X we can do no better than a
random guess). Figure 3.1 shows a scatter chart of a random relationship
between X and Y.
A nonzero correlation between X and Y implies that if we know some-
thing about X we can know something about Y (and vice versa). A measure
moving toward +1 indicates a positive correlation, which means that as

3.3
Linear Regression in Statistical Models
■
41
y
x
Figure 3.1
Random behavior (corr = 0).
y
x
Figure 3.2
Positive correlation (corr = .82).
X moves up Y moves up. Figure 3.2 illustrates a positive relationship
between X and Y.
A measure moving toward −1 indicates an inverse correlation, which
means that as X moves up Y moves down. Figure 3.3 illustrates an inverse
relationship between X and Y.

42
■
Chapter 3
Approaches to Model Building
y
x
Figure 3.3
Inverse correlation (corr = −.78).
y
x
Figure 3.4
Nonlinear correlation.
The greater the absolute value of the correlation, the more accurate is
the prediction of one variable from the other. If the correlation between
X and Y is either −1 or +1, perfect prediction is possible. However, a
near-zero correlation between X and Y does not necessarily mean that the
two variables are uncorrelated. It is possible, as shown in Figure 3.4, that
X and Y share a nonlinear relationship.

3.3
Linear Regression in Statistical Models
■
43
It is generally not possible to draw a straight line (that is, to super-
impose a linear relationship) on a nonlinear function. In these cases the
data analyst must use additional tools, such as polynomial least squares
regression.
The Regression Equation
From the database of observations, a data mining engineer can determine
the degree of correlation between two variables X and Y and then use
linear regression techniques to ﬁnd the equation that best ﬁts the trend
of the relationship. A linear regression produces an equation of the form
Yi = bXi + a,
where a is a constant and b is the slope of the line. Linear regression
equations are powerful tools in building models from data for which
dependencies can be precisely determined (or closely approximated).
Linear regression can be used for some nonlinear data sets by converting
the parameters to a logarithmic scale. Linear equations are often used in
time-series data to predict Y (t) given X(t). The strengths and weaknesses
of linear regression are outlined in Table 3.3.
TABLE 3.3
Strengths/Weaknesses of Linear Regression
Strengths
■
Proven mathematical technique
■
Easy to use, understand, and deploy
■
Creates a working mathematical model of the underlying system
■
Easy to adjust through data ﬁtting
■
Good predictive strengths given highly correlated data
Weaknesses
■
Limited to linear models
■
Often difﬁcult to use for more than two variables
■
Very sensitive to degree of correlation between variables
■
Necessary to convert nonlinear distributions to a linear representation
(such as taking the logarithm of each data point)
■
Sensitive to outlier clustering
■
A fairly simplistic method of curve ﬁtting and often fails to ﬁnd the
true underlying trend line for data with even moderate degrees of
non-linearity (such as seasonal data)
■
Cannot easily ﬁt seasonal data (see ﬁrst weakness point)

44
■
Chapter 3
Approaches to Model Building
3.4
Nonlinear Growth Curve Fitting
As business analysts, engineers, scientists, and social behaviorists soon
discover, few real-world behavior patters follow linear trends. When
forecasting changes in systems that have initial conditions containing
fundamentally random elements, that exhibit characteristics of long-term
population variability, or that have reinforced feedback properties, linear
regression will fail to provide an accurate model of the system’s behavior.
In these cases, a more robust and accurate model of the data can be dis-
covered by ﬁtting the attributes to one or more growth curve templates.
The data mining process attempts to ﬁnd the coefﬁcient parameters that
best describe the data space. As Figure 3.5 illustrates, growth curves are
highly nonlinear.
There are several types of nonlinear growth curves. Here we consider
two of the most popular curves and their underlying dynamics.
The Logistic or Pearl Curve
The logistic (or Pearl ) curve (named after the American demographer
Raymond Pearl) is a symmetric S-curve (also called a sigmoid curve). Its
equation is
y =
L
1 + ae−bx ,
(3.1)
where L is the upper limit to y’s growth, e is the base of the natural
logarithms, and x is the independent variable (often expressed as t, the
time horizon). The coefﬁcients a and b are estimated from the data by
converting the expression to its linear form and ﬁtting the transformed
data points, as follows.
y◦= ln

y
L −y

= −ln a + bx
(3.2)
Once the coefﬁcient estimation is complete (either by linear regression
or back propagation), the values are fed back into the Pearl equation to
make short-term horizon predictions. Because the coefﬁcients of the Pearl
growth curve can be adjusted independently (a changes the location and
b changes the shape), the Pearl equation provides an ideal growth formula
for data mining explorations controlled by genetic algorithms.

3.4
Nonlinear Growth Curve Fitting
■
45
x vs y
growth curves
x
y
b1
a1
a2
a3
a4
a5
a6
a7
a8
an
…
b2
b3
b4
b5
b6
b7
b8
bn
…
x
y
b1
a1
a2
a3
a4
a5
a6
a7
a8
an
…
b2
b3
b4
b5
b6
b7
b8
bn
…
x
y
b1
a1
a2
a3
a4
a5
a6
a7
a8
an
…
b2
b3
b4
b5
b6
b7
b8
bn
…
x
y
b1
a1
a2
a3
a4
a5
a6
a7
a8
an
…
b2
b3
b4
b5
b6
b7
b8
bn
…
Figure 3.5
Sample growth curves.
The Gompertz Curve
The Gompertz curve, named after the British actuary and mathematician
Benjamin Gompertz, is a nonsymmetric growth curve with an adjustable
inﬂexion point. The equation for this curve is
y = Le−ae−bx
(3.3)

46
■
Chapter 3
Approaches to Model Building
Like the Pearl curve, L is the upper limit of the dependent variable’s
domain, e is the base of the natural logarithms, and x is the independent
variable (also usually expressed as t, the current time period). We can
estimate the parameters a and b by transforming the expression into its
linear representation:
y◦= ln

ln
L
y

= ln a −bx
(3.4)
The underlying behavior dynamics of the two curves are very different.
The Pearl curve takes into account not only the upper limit (saturation) of
the growth (distance yet to grow) but the fact that previous growth accel-
erates future growth (distance already covered). The Gompertz curve, on
the other hand, considers growth as simply a function of the saturation
point (the distance remaining until saturation). There are a number of
other growth curves available, such as the Easingwood-Mahajan-Muller
diffusion model and the Fisher-Pry semi-log growth curve. The strengths
and weaknesses of the compertz curve are outlined in Table 3.4.
TABLE 3.4
Strengths/Weaknesses of Non-linear Growth Curves
Strengths
■
Models highly nonlinear patterns
■
Good predictive capabilities
■
Fairly easy to use
■
Easy to understand and deploy
■
Creates a working mathematical model of the underlying system of
variables
■
Easy to adjust through data ﬁtting
■
Works well with adaptive feedback models
Weaknesses
■
Limited to nonlinear growth models.
■
Difﬁcult to use for more than two variables (dependent variable and the
time period variable).
■
Very sensitive to degree of correlation between variables.
■
Parameter estimation is sometimes difﬁcult. Necessary to convert non-
linear distributions to a linear representation (such as taking the natural
logarithm of each data point).
■
Sensitive to outlier clustering.
■
Complex method of curve ﬁtting.
■
Selecting proper curve formula can be very difﬁcult unless ﬁtting is done
through techniques such as genetic or evolutionary programming.

3.5
Cluster Analysis
■
47
3.5
Cluster Analysis
Cluster analysis attempts to isolate regions of similarity within a database
and ﬁnd the relationships between multiple clusters. Large commercial
and governmental databases contain huge quantities of data. Data mining
explorers discover that such large repositories are not lacking in features,
patterns, and relationships but are instead saturated with patterns. There
are just too many. Directed data mining techniques, pursuing a speciﬁc
objective function, often ﬁnd nothing but high levels of noise. It is here
that cluster analysis, in the form of automatic cluster detection (ACD),
provides a truly unsupervised approach to ﬁnding centers of related func-
tionality in data. Figure 3.6 illustrates a cluster as a collection of data with
related properties.
The differences among members of a cluster, in terms of their abso-
lute difference from the cluster’s calculated center or centroid, deﬁnes a
metric of compactness and homogeneity. In a large database, of course,
many overlapping clusters exist. Figure 3.7 illustrates how the data space
for the customer database is partitioned into many clusters with ill-deﬁned
boundaries.
Techniques for forming clusters are varied. In conventional (crisp)
cluster detection, attributes are collected and sorted to give the initial
cluster seeds. The association and diffusion of elements in a cluster are
Age
Gender
Income
Residence
Purchases ($)
Young males (14–22)
Moderate income
In San Diego
Who spent > $500
Figure 3.6
A cluster of young purchasers.

48
■
Chapter 3
Approaches to Model Building
Cluster 1
Cluster 2
Cluster 3
Figure 3.7
Multiple customer clusters in a database.
determined through a k-nearest-neighbor (KNN) approach (calculating
the distance from a hypothetical center of mass to the candidate cluster
member). An element is assigned to one and only one cluster. Fuzzy
clustering (in particular, the fuzzy c-means and adaptive fuzzy clustering
approaches described in Chapter 7) also measures the distance from a
center of mass to an element, but allows an element to belong to more
than one cluster (an element in multiple clusters has a different degree of
membership in each cluster).
Automatic cluster detection thus forms the basis for almost all unsu-
pervised data mining techniques.
From the population of clusters,
higher-level analytical and synthesis techniques are used to evolve a set
of production rules. The strengths and weaknesses of cluster analysis are
outlined in Table 3.5.
3.6
Decision Trees and Classiﬁers
Perhaps the most common method of data analysis and data mining
(excluding statistics) is decision tree induction. A decision tree is a graph

3.6
Decision Trees and Classiﬁers
■
49
TABLE 3.5
Strengths/Weaknesses of Cluster Analysis
Strengths
■
An unsupervised knowledge discovery technique forming the core engine
in most data mining tools.
■
Several tools on the market for standalone cluster analysis. Easy to
implement using standard statistical toolkits.
■
Relatively easy to ﬁnd trained knowledge engineers and methods analysts
to design and implement cluster analysis.
■
Works well with a wide spectrum of data types (numeric, categorical,
and even textual data).
■
Measures the degree of cluster membership (allows for ﬁltering of
generated rules).
■
Robust methodology that can be easily tuned for various types of analysis.
Weaknesses
■
Clustering is not a standalone data mining technique but must be com-
bined with other technologies to generate the ﬁnal results (such as
rules)
■
Clustering is very sensitive to the choice of similarity functions, distance
metrics, and variable weights
■
Often very sensitive to the initial number of clusters and the initial (seed)
values of the clusters
■
Undirected cluster formation can produce centers of randomness (i.e., the
cluster may be an artifact of some other, perhaps as yet undiscovered,
data behavior)
of the dependency relationships in clusters of data (and thus is generally
subsumed by the category of classical cluster analysis).
There are a wide variety of decision tree algorithms (and associ-
ated software products) that work on both numeric and nonnumeric
data attributes. Principal approaches to classiﬁcation and categorization
include the following.
■
Decision trees (ID3 and C4.5): The C4.5 classiﬁer is J. Ross Quinlan’s
most recent version of the original ID3 algorithm (ﬁrst released in
1986).
■
Classiﬁcation and regression trees (CART); Initially developed by
Brieman and Associates (1984) this algorithm is a very popular clas-
siﬁcation scheme. CART is a supervised classiﬁer that builds a binary
tree using a single independent variable.
■
Chi-squared automatic induction (CHAID) algorithms: Developed
by J. A. Hartigan (1975), this algorithm automatically isolates statistical
correlations between variables. Continuous variables in CHAID must
take on the form of intervals and in many ways are similar to ordinal
fuzzy set names.

50
■
Chapter 3
Approaches to Model Building
ID3-like algorithms use an increase or decrease in information gain to ﬁnd
the attribute that best partitions a set of data at the highest level. The
algorithm then applies this same technique to partition the next level
of attributes. This partitioning continues until a complete tree structure
is created. The partitioning can be based on properties of the attribute
(such as height, eye color, body temperature, and so on) or arithmetic
and Boolean relationships. The partitioning or classiﬁcation is maintained
in the form of a graph. Figure 3.8 illustrates a small decision tree segment.
One path along this decision tree reads like a conventional expert
system rule:
if item price is less than ﬁfty,
then inventory action is stock as a regular item.
Edges in decision trees can also contain the strength of the relationship,
the number of instances that conﬁrm the relationship, and whether this
is a one-to-one or one-to-many dependency.
Markov chains are extensions to the decision tree architecture where
the edge contains the conjoint (or, occasionally, the cumulative) proba-
bility that the model will transition from one node in the tree to the next.
In this case, a Markov chain maintains the law of motion for the system
in terms of its limiting probabilities. Data mining tools construct Markov
chains by observing the number of states that transition from one node to
the next. This frequency distribution of edges gives an approximate prob-
ability for that node-to-node movement. The strengths and weaknesses of
decision trees and classiﬁers are outlined in Table 3.6.
Item
price
≥= 50 & =≤ 120
< 50
> 120
Strock trial
Strock regular
Reject
Figure 3.8
A small decision tree (generated by C4.5).

3.7
Neural Networks
■
51
TABLE 3.6
Strengths/Weaknesses of Decision Trees and Classiﬁers
Strengths
■
Creates a set of classiﬁcation rules
■
Good for multidimensional data
■
Easy-to-visualize dependency relationships
■
Several tools available for decision trees (and case-based reasoning)
■
Low computational requirements
■
Organization sensitive to information gain (i.e., the amount of knowl-
edge exposed by the tree)
■
Usually capable of handling both continuous and categorical data
■
Probabilities on Markov chain can highlight important relationships and
depress rules created by spurious outliers
■
Nearly always identify which variables make a signiﬁcant contribution
to the model
Weaknesses
■
Subject to “run away” errors when the number of classes grows too large
■
Difﬁcult to handle time-series data, especially models involving lead and
lag relationships
■
Many decision tree systems can only partition into true or false nodes
■
Computationally expensive to train a large decision tree
■
Partitioning is rectangular (N × M), which is not always consistent with
actual data
■
Trees can grow very large
■
Rules are not generally associated with a working inference engine
3.7
Neural Networks
A neural network is a pattern classiﬁer that behaves, in some ways, like the
human mind. It consists of artiﬁcial neurons connected by edges, arrayed
in a set of layers. The number of layers and the neurons per layer determine
the power of the classiﬁer and the number of ﬁne grain patterns that can
be recognized. Each edge connecting a node (a neuron) has a weight. It is
the values of these weights, adjusted for each learned pattern, that induce
the recognition process in a neural network. Figure 3.9 illustrates how a
neural network is organized and how input patterns are classiﬁed (in this
example a Yes/No classiﬁer is shown, but the classiﬁer can be almost any
arbitrary pattern).
Neural networks can be taught to recognize speciﬁc patterns or they
can be allowed to discover and arbitrarily learn patterns in large databases.
Because neural networks recognize patterns (and these can be patterns
in data as well as visual images, handwriting, or metal fatigue faults), they

52
■
Chapter 3
Approaches to Model Building
Data
patterns
Input
layer
Hidden layer
(recognizer)
Output layer
(classifier)
x=30
y=10
x
y
YES
Figure 3.9
Neural network.
are well suited as data mining tools. In addition, neural networks are able
to easily represent highly nonlinear and chaotic data, thus providing a plat-
form well suited to many business process models. Unfortunately, neural
networks are not able to model time-series data very well or data that is
non-monotonically changing (e.g., data that have a wavelike pattern).
Neural networks have become the tool of choice in many large-scale
data mining projects for several reasons. First, they are easy to use. A neu-
ral net system is essentially a “black box,” meaning that the user supplies
the data and the network learns the patterns. Second, there are many
neural network tools and toolkits available on the market. Third, there is
an abundance of highly skilled knowledge engineers who can design and
implement a neural-network-based data mining system. The strengths and
weaknesses of neural networks are outlined in Table 3.7.
3.8
Fuzzy SQL Systems
The introduction of semantic database query systems provides the knowl-
edge mining engineer with a formidable and powerful data analysis and
classiﬁcation tool (see Chapter 6 for a discussion and examples of fuzzy
queries). It is this latter ability to rank and classify sets of data based on
the semantics of imprecision that makes the fuzzy query system an ideal

3.8
Fuzzy SQL Systems
■
53
TABLE 3.7
Strengths/Weaknesses of Neural Networks
Strengths
■
Based on solid mathematics of learning in connectionist machines
■
Well suited to modeling nonlinear and continuous data
■
Easy to use (many software tools available)
■
Many highly skilled neural network knowledge engineers available to
design and implement system
■
Able to work in both supervised and unsupervised modes
■
Good data mining approach for large, unstructured databases
■
Good choice when underlying reasons and explanations are not required
Weaknesses
■
Inputs must be well understood and often “normalized” to fall within a
speciﬁc small range of values
■
Outputs are continuous; categorical data can be difﬁcult to generate or
interpret
■
Is a “black box,” meaning it cannot explain its actions and user cannot
examine the “rules”
■
Requires expertise in neural network design to conﬁgure system for
proper number of inputs, correct number of hidden layers, and suitable
number of output classiﬁers
■
Requires some skill in deciding on a proper training algorithm for many
neural network conﬁgurations
■
Can be computationally intensive in training mode
■
Very difﬁcult to handle nonnumeric data
■
Requires large amounts of training data
■
Network can be over-ﬁt and over-trained (it simply “memorizes” the input
and output)
■
Produces a static model that must be retrained when new data are
acquired (knowledge engineers cannot add rules or change the internal
logic of a neural network)
“ﬁrst approximate” tool in data mining. Conventional database queries
select records using precisely deﬁned arithmetic and logical expressions
that resolve to a crisp yes or no answer. The following is an example.
Select *
From ieh01.projects
Where budget > 1000
And duration > 90
And (spent/budget)*100 in range[40,60];
On the other hand, a semantic query using fuzzy logic casts this same type
of query into a request for records that match the intention of the user.

54
■
Chapter 3
Approaches to Model Building
8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38
x
Duration (in weeks)
40 42 44 46 48 50 52 54 56 58
Short
Grade of membership m(x)
Medium
Moderate
Long
Extreme
1.0
 0
Project_Duration
Figure 3.10
Fuzzy sets associated with project duration.
In a fuzzy query, the database variables (ﬁelds) are broken down into
collections of overlapping fuzzy sets. This is illustrated in Figure 3.10 for
the duration ﬁeld of the project table.
The overlap of fuzzy sets shown in Figure 3.10 means that a variable’s
value (such as the position marked x) can simultaneously exist in more
than one fuzzy set, usually with differing degrees of membership. These
underlying fuzzy sets can then be used to select records from the database.
As an example, the previous query can be recast as follows.
Select *
From ieh01.projects
Where budget is high
And duration is long
And (spent/budget)*100 is around 50%;
The fuzzy query where statement represents a template describing the
cognitive model underlying the database query. In this case, the user’s
search template will match project records with some degree of high,
some degree of long, and some degree of being about 50%. This cap-
tures, to a greater extent, the meaning of the query rather than simply its
mechanics. A fuzzy query can also use hedges (such as about 50% or near
MfgCosts) to dynamically create fuzzy sets from database table content,
thus reducing the need to decompose all columns into fuzzy sets.
A fuzzy query returns not only the selected records but a compatibil-
ity index indicating how well individual records matched the template.

3.9
Rule Induction and Dynamic Fuzzy Models
■
55
TABLE 3.8
Strengths/Weaknesses of Fuzzy SQL Systems
Strengths
■
Based on solid theory of fuzzy sets and approximate reasoning (fuzzy
logic)
■
Very easy-to-use and easy-to-understand method for exploring the
nature of data
■
Works on any conventional database system that has an ODBC driver
as well as spreadsheets and comma-delimited ﬁles
■
Semantics tailored to speciﬁc cognitive model (e.g., “young” in marketing
is likely not the same as “young” in the credit department)
■
Retrieves records based on “what I mean” not simply “what the arithmetic
says”
■
Results are ranked according to their compatibility (user can examine
the best cases ﬁrst)
Weaknesses
■
Not easy to use on very large databases (due to a lack of fuzzy index
support) or across multiple tables (due to lack of fuzzy join)
■
Requires an understanding of fuzzy logic and (to a lesser degree) fuzzy
set theory
■
Few tools to support fuzzy queries
■
Few knowledge engineers trained in fuzzy logic (and those that do have
some fuzzy logic experience generally have little if any experience with
database systems and business problems)
■
Cannot discover deep relationships
■
Subject to proper deﬁnition of fuzzy sets and modiﬁers
■
Relies on the knowledge of the query user
■
Applicable to numeric data only
■
Difﬁcult to conceptualize complex query statements
The higher the compatibility index, the more closely the selected record
matches the intent of the query. There are many ways to compute this
compatibility (fuzzy minimum, weighted average, weighted product).
Generally, a fuzzy query chooses many more records from the database
than a conventional query. It is this ability to drill into large databases
on a semantic rather than a crisp Boolean basis that makes fuzzy query
a powerful ﬁrst-step data mining tool. The strengths and weaknesses of
fuzzy SQL systems are outlined in Table 3.8.
3.9
Rule Induction and Dynamic Fuzzy Models
All but three of the previous examples (linear regression, nonlinear
growth curves, and neural networks) produce a static report as the

56
■
Chapter 3
Approaches to Model Building
Experimental
data
Model database repositories
Actual vs. estimated
(predicted)
Historical
data
Knowledge base
Prediction
Predictive
model
y
x
Figure 3.11
A dynamic rule-based model.
outcome of their data mining process. Bringing advanced computational
intelligence techniques — such as genetic algorithms, fuzzy logic, and
parameter estimation through back propagation — to bear on knowledge
discovery process gives the knowledge miner a richer and much more
powerful set of exploratory tools. These tools, in fact, allow the cre-
ation of adaptive feedback and a highly nonlinear rule-based systems. The
rule induction process evolves a dynamic model of the underlying system
through a set of if-then-else rules. Figure 3.11 illustrates how this dynamic
model is structured.
This approach is a powerful method of data mining and knowledge dis-
covery. Evolving or generating a dynamic rule-based model can be done
in at least two ways: through the use of cluster analysis (see Chapter 7) or
through the use of an if-then pattern discovery algorithm (see Chapter 8).
The more important features of evolving a dynamic rule-based model
include the following,
■
The automatic decomposition of variables into fuzzy sets (relieving
the designer of this preliminary and time-consuming task)

3.9
Rule Induction and Dynamic Fuzzy Models
■
57
■
The automatic detection of fuzzy cluster centers (easily ﬁnding
groups of records that share common and often imprecisely bounded
properties)
■
The use of genetic algorithms to regenerate models, reconﬁgure
existing models, and explore large data spaces (see Chapter 11)
With a rule-based system the knowledge mining engineer can examine
the actual relationships, run and simulate the behavior of the system,
ask the system to explain its reasoning, and modify the evolved system
to include additional rules or change the nature of one or more evolved
rules. Because evolved dynamic models are regenerated from data, they
are ideally suited to adaptive feedback systems and processes that are
sensitive to rapid changes in the external world. Figure 3.12 shows a
“high-altitude” schematic of the fuzzy rule evolution process.
Rules are statements of cause and effect. Fuzzy rules are statements of
relationships between degrees of memberships and allow a much more
powerful form of inference than ordinary crisp rules. They are cast in the
form of if-then-else statements. The following is an example.
if CompetitionPrice is high
then OurPrice should be low
This statement says that “when the competition price has some degree
of membership in the fuzzy set high, then our price should have a corre-
sponding degree of membership in the fuzzy set low.” This relationship
looks something like the transfer function shown in Figure 3.13.
As Figure 3.13 indicates, the fuzzy sets high and low need not be
congruent or conformal, and thus we can introduce as a natural part
of the model highly nonlinear behaviors in even the simplest rules. In
actual practice, of course, fuzzy models process many rules in parallel,
combining the evidence from each rule (at the rule’s contribution weight)
into a composite assessment of the ﬁnal solution value.
Supervised Model Generation
There are two types of fuzzy model generations, each of which is asso-
ciated with a different type of data mining or knowledge discovery
paradigm. The ﬁrst, illustrated in Figure 3.14, is supervised data mining.
In this methodology, the knowledge engineer or systems analyst speci-
ﬁes a dependent variable and a set of possible (or probable) independent
variables. The data mining facility attempts to discover the relationships
between the dependent and one or more of the independent variables.

58
■
Chapter 3
Approaches to Model Building
Experimental
data
Model database repositories
Actual vs. estimated
(predicted)
Historical
data
Knowledge base
Prediction
Predictive
model
y
x
Trend of prediction
evidence
Compatibility
Performance
Time
Re-tune model structure
Genetic tuner
Figure 3.12
A dynamic fuzzy system model.

3.9
Rule Induction and Dynamic Fuzzy Models
■
59
Grade of membership m(x)
2
4
6
8
10
Item price (retail)
12 14 16 18
0
1.0
2
4
6
8
10
Item price (retail)
Low
High
Competition_Price
12 14 16 18
0
Our_Price
Figure 3.13
The monotonic truth function for high and low.
Source data
Training data
Candidate rules
Model rules
Objective
filter
Rule
induction
Variable
decomposition
Rule
compression
Rule
generation
tournament
Model
results
Subject matter expert evaluation
Figure 3.14
Supervised data mining.

60
■
Chapter 3
Approaches to Model Building
Based on the Wang-Mendel rule of induction algorithm, this tech-
nique automatically decomposes variables into overlapping fuzzy sets (the
decomposition is based on the type of variable), ﬁts the data to the fuzzy
sets, and generates a collection of candidate rules. Combining user belief
estimates, noise ﬁltering, and rule effectiveness weights, a tournament is
conducted to select the best rules. These are compressed or combined
with other rules to generate the ﬁnal model.
Unsupervised Model Generation
The second approach to fuzzy model generation is unsupervised data
mining. In this approach we employ two different types of unsupervised
data mining technologies. The ﬁrst, called cluster analysis, automati-
cally organizes n-dimensional space into collections of elements with
similar characteristics or properties. These collections, called clusters,
have centers measured according to a fuzzy relationship function so that
membership in the cluster is expressed as a fuzzy Euclidean distance.
These clusters can also overlap, so that elements can sometimes belong
to two or more clusters with different strengths. The second technology
is called a genetic algorithm. This form of computational intelligence gen-
erates multiple simultaneous models and examines how well the cluster
relationships map to some objective function. Genetic algorithms can arbi-
trarily select clusters along different axes in the n-dimensional space and
examine their effectiveness in predicting model robustness. Figure 3.15
illustrates schematically how this model evolution technique works.
A genetic algorithm is a nonlinear optimization technique. It works
from a population of candidate models. The model attributes are both
randomly selected and randomly changed through a process of model
“mating” and “mutation.” The genetic algorithm also generates a fuzzy
model. However, the rules are automatically executed by the genetic
algorithm to measure the distance or difference between model predic-
tion and the validation values (the actual value). Models with improved
prediction capabilities are saved in the population, whereas models with
inferior prediction capabilities are eventually eliminated. In this way, after
a large number of parallel iterations a ﬁnal best-ﬁt model is produced.
Because the unsupervised data mining approach examines all possible
cluster relations across the complete n-dimensional space, it can ﬁnd even
very deep relationships in the data. Furthermore, fuzzy clusters, because
they measure cluster elements using partial memberships and can handle
elements lying in overlapping clusters, often discover relationships that
are transparent or invisible to conventional cluster analysis. By combining
fuzzy clustering with powerful genetic algorithms, the unsupervised data
mining process can examine large, very complex data spaces. The fuzzy

3.9
Rule Induction and Dynamic Fuzzy Models
■
61
TABLE 3.9
Strengths/Weaknesses of Unsupervised Model Generation
Strengths
■
Based on mathematics of fuzzy sets and approximate reasoning (fuzzy
logic), genetic algorithms, machine learning, and automatic parameter-
ization (neural network back propagation)
■
Generates a fuzzy rule-based model executed to observe and analyze
the underlying system behavior
■
Handles large databases with missing and noisy data
■
Weights evidence for rules so that outliers, sparse data, or unreliable
data will not contribute to signiﬁcant rule set
■
Uses a fuzzy mountain clustering technique to automatically ﬁnd centers
of knowledge
■
Easily handles nonlinear models
■
Models time-series with lead-lag relationships
■
Models adaptive feedback systems
■
Deep, elastic rule discovery process
■
Supervised or unsupervised modes (works with or without an explicit
objective function)
■
System can explain its reasoning
■
User rules can be added and weights on existing rules changed (through
a calibration feedback loop)
■
Implements Combs method for rule reduction to minimize size of model
Weaknesses
■
Signiﬁcant computational demands during model evolution.
■
Requires knowledge of fuzzy logic, fuzzy set theory, and approximate
reasoning to understand how model is created and works.
■
Few tools to support fuzzy system models.
■
Output rules must be compatible with and interconnected to the fuzzy
modeling software.
■
Few knowledge engineers trained in fuzzy logic (and those that do have
some fuzzy logic experience generally have little if any experience with
database systems and business problems).
■
Rule generation mechanics require tuning such factors as the slot compe-
tition algorithm, compression techniques, belief functions, noise ﬁltering,
and empty rule actions.
■
Can produce a very large number of rules.
■
Objective functions (for supervised data mining) can be difﬁcult to
identify.
■
Default fuzzy decomposition of variables may not reﬂect actual semantics
of the data. Knowledge engineer may need to handcraft critical fuzzy
sets.
■
Often requires more than normal pre-processing of data.

62
■
Chapter 3
Approaches to Model Building
Source data
Model rules
Clustering
(fuzzy c-means)
Select
clusters
Rule
generation
Genetic
algorithm
Model
results
Subject matter expert evaluation
Cluster count optimization
Figure 3.15
Unsupervised data mining.
model produced by this type of analysis tends to be more robust, in that the
fuzzy rules are generated from cluster centroids rather than from a more
simple (but nevertheless effective) tournament based on data ﬁtting. The
strengths and weaknesses of unsupervised model generation are outlined
in Table 3.9.
3.10
Review
This chapter has provided a brief and “high-altitude” survey of various
model building approaches and technologies. Although this book is pri-
marily concerned with exploring data through intelligent technologies
and building models from fuzzy rule discovery, an understanding of
other modeling techniques as well as their strengths and limitations is
a necessary foundation for any attempt at building effective, robust, and

Further Reading
■
63
maintainable models. From this chapter you should come away with an
understanding of the following.
■
Basic statistical approaches to modeling structured numeric data
■
Mathematical, clustering, and categorization methods of modeling-
related data patterns
■
The use and limitations of regression analysis in discovering and
interpreting trends
■
The nature of knowledge-based approaches to models, such as neural
networks and fuzzy systems
■
How to approach the modeling of general nonlinear growth (time-
series) systems
■
How fuzzy logic is used to explore databases and other data reposito-
ries
■
The nature of rule induction (data mining) in the evolution of dynamic
fuzzy models
A more comprehensive and exhaustive analysis of modeling techniques
and technologies is outside the scope of this book. However, this chapter
provides a relatively broad framework, covering ordinary statistical, math-
ematical, pattern discovery, and knowledge-based approaches to building
models.
Further Reading
Data Mining
■
Berry, M., and G. Linoff. Data Mining Techniques for Marketing, Sales, and
Customer Support. New York: John Wiley and Sons, 1997.
■
Cabena, P., P. Hadjinian, R. Stadler, J. Verhees, A. Zanasi. Discovering Data
Mining from Concept to Implementation. Upper Saddle River, NJ: Prentice-
Hall/PTR, 1997.
■
Groth, R. Data Mining: A Hands-On Approach for Business Professionals.
Upper Saddle River, NJ: Prentice-Hall/PTR, 1997.
■
Kennedy, R. L., Y. Lee, B. Van Roy, C. D. Reed, R. Lippman. Solving Data
Mining Problems Through Pattern Recognition. Upper Saddle River, NJ:
Prentice-Hall/PTR, 1998.
■
Weiss, S., and N. Indurkhya. Predictive Data Mining. San Francisco: Morgan
Kaufmann, 1998.

64
■
Chapter 3
Approaches to Model Building
Model Building
■
Cellier, F. Continuous System Modeling. New York: Springer-Verlag, 1991.
■
Cherkassky, V., and F. Mulier. Learning from Data: Concepts, Theory, and
Methods. New York: John Wiley and Sons, 1998.
■
Fogel, D. Evolutionary Computation: Toward a New Philosophy of Machine
Intelligence. Piscataway, NJ: IEEE Press, 1995.
■
Goldberg, D. Genetic Algorithms in Search, Optimization, and Machine
Learning. Reading, MA: Addison-Wesley, 1989.
■
Liebowitz, J. (ed.). Expert Systems for Business and Management. Englewood
Cliffs, NJ: Prentice-Hall, 1990.
■
Masters, T. Practical Neural Network Recipes in C++. San Diego, CA: Academic
Press, 1993.
■
Martin, J., and J. Odell. Object-oriented Methods: Pragmatic Considerations.
Upper Saddle River, NJ: Prentice-Hall/PTR, 1996.
■
Michalewicz, Z. Genetic Algorithms + Data Structures = Evolution Programs.
New York: Springer-Verlag, 1994.
■
Pao, Y-H. Adaptive Pattern Recognition and Neural Networks. Reading, MA:
Addison-Wesley, 1989.
■
Taylor, D. Business Engineering with Object Technology. New York: John Wiley
and Sons, 1995.
■
Wasserman, P. D. Neural Computing: Theory and Practice. New York: Van
Nostrand Reinhold, 1989.
■
Watkins, P., and L. Eliot. Expert Systems in Business and Finance. New York:
John Wiley and Sons, 1993.
■
Weinberg, G. An Introduction to General Systems Thinking. New York: John
Wiley and Sons, 1975.

■
■
■
Part II
Fuzzy Systems
Contents
Chapter 4
■■■
Fundamental Concepts of Fuzzy Logic
67
Chapter 5
■■■
Fundamental Concepts of Fuzzy Systems
95
Chapter 6
■■■
Fuzzy SQL and Intelligent Queries
149
Chapter 7
■■■
Fuzzy Clustering
207
Chapter 8
■■■
Fuzzy Rule Induction
265
65

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 4
Fundamental Concepts
of Fuzzy Logic
Fuzzy logic is a form or system of logic. Like Boolean logic, it is based on set
theory. The sets in fuzzy logic, however, have degrees of membership. This
slight change in the deﬁnition of a set has signiﬁcant and far-reaching impli-
cations for knowledge representation and chains of reasoning. This chapter
examines the differences between Boolean and fuzzy logic sets, explores the
various operators that can be applied to both sets (and how such operators
differ) and discusses operations that only make sense for fuzzy sets (such
as hedges and alpha cut thresholds). Understanding the nature of fuzzy sets
and fuzzy logic is crucial in understanding not only the clustering and rule
induction techniques in Part I of the book but in understanding many of
the advanced genetic modeling techniques in Part III.
From a modeling perspective, fuzzy logic is a logic of continuous vari-
ables. Its complement, Boolean logic, is a logic of discrete variables. With
fuzzy logic we can represent such elastic and imprecise concepts as high
risk, a long duration, a tall person, and a large transaction volume. We can
also evaluate critical process factors in a model. Is X increasing rapidly?
Is A close to B? Is N very much greater than M? In all of these cases fuzzy
logic provides a way of ﬁnding the degree to which an object is repre-
sentative of a concept or the degree to which a state is representative of
a process. These degrees play a subtle but critical role in the evaluation
of fuzzy models and fuzzy systems. They represent not only the degree
of membership in a concept (that is, a set) but such important model-
ing concepts as supporting evidence, numeric elasticity, and semantic
ambiguity.
It is the ability to represent intrinsic ambiguity that gives fuzzy logic
some of its most impressive capabilities — allowing a single data point to
belong simultaneously (with differing degrees of membership) to multi-
ple, sometimes semantically conﬂicting, concepts (multiple sets). Such a
small relaxation in the rules of Boolean logic has far-reaching and power-
ful consequences for models that incorporate fuzzy logic. Before moving
67

68
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
on to the nature of how fuzzy systems model uncertainty and ambiguity,
we need to lay a foundation in the nature of fuzzy sets, fuzzy set oper-
ations, and fuzzy implication methods. We start with the lexicon of
fuzzy logic and then move on to the difference between Boolean and
fuzzy sets.
4.1
The Vocabulary of Fuzzy Logic
Fuzzy logic suffers from a rich vocabulary of unfamiliar terms and reluc-
tance among its researchers and developers to adhere to a standard
vocabulary. It also suffers from an overuse of obscure mathematical
symbolism in place of simple and explanatory narrative (but that is
another issue). The literature of fuzzy logic and fuzzy systems is ﬁlled
with a nomenclature derived from its own private world of continuous
logic. We ﬁnd ourselves speaking about fuzziﬁcation and defuzziﬁcation,
hedges, membership functions, and linguistic variables. Before moving
on to a more comprehensive and detailed exploration of fuzzy logic,
fuzzy systems, and how they work, we need to understand the princi-
pal nomenclatures and how they are related to the elements of fuzzy
systems.
Although not a complete dictionary of terms associated with fuzzy
logic, a preview of the basic vocabulary at the beginning of this chapter
will make reading and understanding the material in the chapters of Part
II much easier. It is not always possible, while maintaining an uncluttered
and coherent discussion of fuzzy systems, to ensure that every term is
introduced before it is used. Many of these terms, of course, will be
discussed in greater depth later in this and subsequent chapters.
Alpha Cut Threshold
An alpha cut threshold truncates the membership function of a fuzzy set
or the truth value of a fuzzy rule’s predicate at a speciﬁc truth value. Any
value that falls below the alpha cut threshold is equivalent to zero. Alpha
cuts come in two forms: strong (the comparison is less than) and weak
(the comparison is less than or equal).
Boolean Logic
Boolean logic is a logic of crisp sets. A crisp set has two truth values: 1 and
0. Propositions in Boolean logic are either true or false, and membership

4.1
The Vocabulary of Fuzzy Logic
■
69
in Boolean sets is either fully inclusive or fully exclusive. Boolean logic was
initially formalized by George Boole (1815–1864). In 1854, he published
An Investigation into the Laws of Thought, on Which Are Founded the
Mathematical Theories of Logic and Probabilities, which uniﬁed the
mathematical concepts of algebra and logic.
Crisp Set
Boolean sets are often called crisp sets as a way of distinctly differentiating
them from fuzzy sets, as well as a way of indicating the sharpness or
crispness of their membership function.
Domain
Often confused with the term universe of discourse, the domain of a
fuzzy set is the explicit range of values over which the fuzzy set’s mem-
bership function is deﬁned. For vocabulary fuzzy sets (e.g., the term set
of a variable), the domain is generally the same as the support set.
Expectancy
The width of a fuzzy number, usually measured from the center of the
fuzzy set, is called the expectancy (a measure of the number’s elastic-
ity). The larger the expectancy the more imprecision is built into the
fuzzy number. However, we should not confuse imprecision with a lack
of accuracy. In many fuzzy models, relaxing the precision of a number
increases the model’s fault tolerance and reduces its brittleness, thereby
making the model more precise over a wider range of values.
Fuzzy Logic
Fuzzy logic is a logic of fuzzy sets. A fuzzy set has, potentially, an inﬁnite
range of truth values between one (1) and zero (0). Propositions in fuzzy
logic have a degree of truth, and membership in fuzzy sets can be fully
inclusive, fully exclusive, or some degree in between. Fuzzy logic was ini-
tially formalized by LotﬁZadeh (1921–) in his seminal 1965 paper “Fuzzy
Sets” (see “Further Reading”) followed in 1973 with “Outline of a New
Approach to the Analysis of Complex Systems and Decision Process” (see
“Further Reading”).

70
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Fuzzy Number
A bell-shaped, triangular-shaped, or trapezoidal-shaped fuzzy set repre-
sents a central value and is, in essence, a fuzzy number. Fuzzy numbers
give fuzzy models a great deal of robustness and ﬂexibility. The width of a
fuzzy number is a measure of its overall expectancy (the degree to which
it corresponds to a crisp number).
Fuzzy Quantiﬁer
Fuzzy sets that are not numbers are fuzzy quantiﬁers. These sets generally
have linear or sigmoid shapes. Concepts such as Tall, Fast, Heavy, and
Large are examples of fuzzy quantiﬁers.
Fuzzy Set
A fuzzy set is distinct from a crisp or Boolean set in that it allows its
elements to have a degree of membership. The core of a fuzzy set is
its membership function: a surface or line that deﬁnes the relationship
between a value in the set’s domain and its degree of membership. The
relationship (Expression 4.1) is functional because it returns a single
degree of membership for any value in the domain.
t = f (s, x)
(4.1)
Here,
t
is a truth membership value (degree of membership in fuzzy set)
s
is the fuzzy set
x
is a value from the underlying domain (universe of discourse)
Fuzzy sets provide a means of deﬁning a series of overlapping concepts for
a model variable, thus permitting rules to address the state of a model in
terms of these fuzzy set names. Such names are called linguistic variables.
They allow fuzzy sets to represent the underlying semantics of a variable
(that is, the concept represented by a variable).
Hedge
Hedges act like adjectives or adverbs in the English language. They modify
the shape of the fuzzy set’s underlying membership function to or dilute

4.1
The Vocabulary of Fuzzy Logic
■
71
the membership relationships, and they convert scalars (crisp numbers)
into fuzzy sets through a process called approximation. They can be used
in rules to modify the evaluation of both the premise and the consequent.
The following is an example.
if a is very Z,
then b is somewhat Y
Here, very and somewhat are intensiﬁer and dilution hedges.
Linguistic Variable
A fuzzy set that forms part of the term set for a variable and is used in
fuzzy rules as part of a fuzzy relation is called a linguistic variable. For
example, in the rule if height is Tall then weight is Heavy both Tall and
Heavy are linguistic variables representing one of the possible partitions
of height and weight. Fuzzy sets used in this way are called linguistic
variables because they are used in place of numbers and allow rules to be
cast in form of linguistic statements (such as height is Tall).
Membership Function
The core of a fuzzy set is the membership function. See “Fuzzy Set” for a
more complete description.
Overlap
Because fuzzy sets represent degrees of membership, values from the
complete universe of discourse for a variable can have memberships in
more than one fuzzy set. For example, consider the variable height with
a term set of Short, Medium, and Tall. An individual’s height can be,
to some degree, both Short and Medium. This is because the Short and
Medium fuzzy sets overlap (as the values for height increase, they become
less Short and more Medium, but the transition is gradual, not abrupt).
Support Set
The region of the membership function in which the truth values are
greater than zero is called the support set. The support set plays an
important part in the evaluation of a fuzzy model because only within the
support set region can a set actually contribute to the outcome of a model.

72
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Term Set
The collection of fuzzy sets that deﬁne the semantics of a variable is called
the variable’s term set. Each variable used in a fuzzy rule must have a
term set. The individual fuzzy sets in a term set are the linguistic variables
associated with the variable. Thus, Tall in the rule if height is Tall then
weight is Heavy is part of the term set for the variable height and Heavy
is part of the term set for the variable weight.
Universe of Discourse
The complete range of values over which a variable can assume values
is called the universe of discourse (UoD). Although this term is often
applied to the range of values in a fuzzy set, the proper term for the fuzzy
set is its domain. The Universe of discourse for a variable is decomposed
into a collection of fuzzy sets (the term set), each set of which has a
domain that overlays part of the universe of discourse.
4.2
Boolean (Crisp) Sets: The Law of Bivalence
A set is a collection of objects grouped because they share a common
property. In crisp (or Boolean) logic, an object is either a member of a set
or it is not. For a crisp set A, the membership or characteristic function,
µA(x), shown in Expression 4.2, is deﬁned in exactly these terms.
µA(x) =
	
1
x ∈A
0
x /∈A
(4.2)
This is known as the law of bivalence, which forms the fundamental
property of Boolean logic. The principle (or law) of bivalence is such an
intrinsic part of Boolean logic that it is simply taken for granted. In fact,
if we assume that bivalence is an axiom of Boolean logic, the associated
laws of the excluded middle and noncontradiction (discussed in material
to follow) can be proved as theorems.
Implicitly, the collection of elements in A is drawn from A’s under-
lying domain or universe of discourse1 (U), simply referred to as UoD
in both Boolean and fuzzy sets. Thus, when we refer to a set (e.g., A)
1 In a general modeling sense, however, a boolean domain deﬁnes a more extensive and
more model-context-sensitive constraint on the set membership. As an example, the set of
Customers with Poor Credit might have as its domain an SQL query against a database table of

4.2
Boolean (Crisp) Sets: The Law of Bivalence
■
73
and its characteristic function there is an implied qualiﬁcation, illustrated
in Expression 4.3, that the elements are selected from the correspond-
ing UoD.
A ←{(xi, µA(xi)|xi ∈U)}
(4.3)
As the name implies, the UoD is the range or set of values under discussion
when we are examining a set. In almost all cases the UoD will be the set
of allowed values in the set, but is often drawn from a much larger (often
inﬁnite) set of values. Generally, for both crisp and fuzzy sets the UoD
will be drawn from the larger universe, R (the monotonic line of positive
and negative real numbers).
Most of us are familiar with Boolean sets from high school algebra, in
which they are represented as Venn (or Euler) diagrams. These sets have
well-deﬁned characteristics (the set of all even numbers, the set of every
odd number greater than 10, the Fibonacci numbers, and, less mathemat-
ically, the set of male history teachers, the set of yellow Chevrolets in the
parking lot, and so forth). Sets are deﬁned by their characteristic or speci-
ﬁcation function. This speciﬁcation can be a list or any logical expression.
For example, consider the Boolean set of Tall people (by this we mean
any person over six feet in height). Given a height (x), Expression 4.4
shows the membership function for Tall people.
µTall(x) =
	
1
x > 6
0
x ≤6
(4.4)
When we constrain Tall so that its UoD is between 4 and 7 feet (in the
future we will simply represent this as T U = {4, 7}), Figure 4.1 shows the
degree of membership proﬁle for the set.
There are only two possible membership states in Tall: either a height
is considered Tall or it is not. The bold line in Figure 4.1 represents the
set membership for each height along the horizontal axis (the domain of
the set). At a fraction of an inch above 6 feet the membership jumps from
false (or 0) to true (or 1).
Boolean Set Operators
A complete discussion of Boolean logic is outside the scope of this book.
However, a short review of the basic set of operations is necessary in order
poor risk clients. Thus the Universe of Discourse does not map to real numbers nor does it map
to a comprehensible time-invariant set.

74
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Grade of membership m(x)
4
4.5
5
Height (in feet)
5.5
6.0
6.5
7
0
1.0
Tall
Figure 4.1
The Boolean Tall set.
AND
OR
XOR
NOT
0 
1 
0 
1 
0 
1
0
0
0
0
0
1
0 
0 
1
0
1
1
0
1
1
1 
1
1
1 
0
1
0
Figure 4.2
Truth tables for And, Or, Exclusive-Or, and Not.
to understand the differences in set algebra operations when we begin our
discussion of fuzzy sets. As Figure 4.2 illustrates, there are four principal
operations that can be performed on Boolean sets.
The And (or intersection) operator applied to set A and B returns a set
containing all elements common to both sets. The Or (or union) operator
when applied to sets A and B returns a set containing all elements that
appear in ether set. Although not usually considered a basic logical oper-
ator, the Exclusive-Or when applied to sets A and B returns all elements
that appear in either of the two sets but not in both. The Not operator
is applied to a single set and reverses the membership value (that is, it
returns the complement of the set).

4.2
Boolean (Crisp) Sets: The Law of Bivalence
■
75
The Laws of Excluded Middle and
Noncontradiction
Although the rules underlying crisp logic were formulated in the mid-
nineteenth century by George Boole, the ideas underlying Boolean logic
can be traced back at least as far as Aristotle.
When Aristotle ﬁnished explaining how the physical world behaved,
he turned to a deeper matter (in his mind, at least), explaining how we
know what we know. Aristotle set about formulating the rules for separat-
ing truth from falsehood. The result of this effort was his six treatises on
logic, collectively known as The Organon, or The Tool. It was Aristotle’s
insight into reasoning that led him to formulate a way of maintaining the
“chain of custody” of truth through successive steps. These syllogisms, as
they are known, have allowed generations of freshmen logic students to
wrestle with deep propositions such as the following.
All men are mortal
Socrates is a man
Socrates is mortal
The ﬁrst part of the syllogism lays down a general proposition; in this
case, that all men are mortal (all men will eventually die). The next two
lines form an if-then chain of reasoning. The logic says, if Socrates is a
man, then (or “therefore”) Socrates is mortal. These if-then statements
form not only an implicit part of the way we as humans reason about
contingent events but, as we shall see later, lie at the core of modern
rule-based machine reasoning systems.
Like today’s Boolean logic, these syllogisms (as well as the rest of
Aristotle’s logic) depended on an ability to classify or categorize elements
into distinct sets. It allows for no ambiguity in the classiﬁcation. Aristotle
took the idea of ambiguity to its natural consequences: the relationship
between an object’s membership in a set (A) and its membership in the
complement of that set (∼A). Aristotle came to the conclusion — the same
conclusion reached by George Boole and still an integral part of Boolean
logic — that an object cannot be in both A and ∼A. In somewhat formal
terms, this is expressed as
µA∩∼A(x) = {∅},
(4.5)
meaning that the intersection of a set with its complement (A and ∼A) is an
empty set. This is the law of noncontradiction, which states that for any
Boolean proposition (P AND ∼P ) is false. From this we can also see that
combining a set and its complement must return the entire population of

76
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
possible elements (the UoD). This is represented as
µA∪∼A(x) =

AU
,
(4.6)
meaning that the union of a set with its complement (X or ∼X ) is the
universe of elements that can be selected for either set. This is the law
of the excluded middle, which states that for any Boolean proposition (P
OR ∼P ) is true.
4.3
Fuzzy Sets
In 1965, professor LotﬁZadeh (Electrical Engineering and Electronics
Research Laboratory, University of California, Berkeley) published his
seminal paper on the idea of fuzzy sets (see “Further Reading”), thus
establishing the foundation of a comprehensive and mathematically sound
logic of imprecisely or ambiguously deﬁned sets. This has come to be
known as fuzzy logic, ﬁrst given full treatment in Zadeh’s 1973 paper
“Outline of a New Approach to the Analysis of Complex Systems and
Decision Process” (see “Further Reading”).
The Anatomy of a Fuzzy Set
The core idea of fuzzy logic rests with the concept of a fuzzy set. These
sets are not fuzzy in the conventional English-language sense of the word:
blurred, hazy, out of focus, or indistinct. Rather, they are fuzzy in the way
they treat their set boundaries. Fuzzy sets have a semipermeable mem-
brane. In a fuzzy set, an element can be in three states: not a member of
the set, a full member of the set, or a partial member of the set. Set mem-
bership is thus represented as a continuous range of values in the interval
[0, 1] with [0] indicating no set membership, [1] indicating complete set
membership, and values in the range [>0, <1] indicating a partial degree
of membership.
Fuzzy logic is a superset of Boolean logic. Consequently, fuzzy sets are,
in many respects, supersets of Boolean sets. A fuzzy set incorporates as
part of its structure the intrinsic degrees of membership associated with
each element. Figure 4.3 illustrates schematically the general components
of a fuzzy set.
A fuzzy set has three principal components: a degree of membership
measure along the vertical (Y) axis, the possible domain values for the set
along the horizontal (X) axis, and the set membership function (a contin-
uous curve that connects a domain value to its degree of membership in

4.3
Fuzzy Sets
■
77
Grade of membership m(x)
Universe of discourse (domain)
Support set
0
1.0
Membership
function
Figure 4.3
General components of a fuzzy set.
the set). The connecting curve is the crucial part of the fuzzy set. It deﬁnes
the set membership relations. In addition to an underlying domain or uni-
verse of discourse, a fuzzy set also has a support set. This is the region of
the domain at which the membership values are greater than zero.
The Taxonomy of Fuzzy Sets
Comparing the crisp set Tall (see Figure 4.1) with its fuzzy counter-
part will illustrate these differences and expose the structure of a basic
fuzzy set. Figure 4.4 shows one possible fuzzy set representation for the
concept Tall.
The Tall fuzzy set deﬁnes how heights are related to the concept of
Tall. All heights below 4.5 feet have zero membership values (and are
outside the support set). From 4.5 to 6 feet, the degree of membership in
the fuzzy set Tall gradually increases. At 6 feet it reaches [1.0] (all heights
above 6 feet are fully members of the set). Alternatively, we can view the
membership value as the degree of compatibility between the value of
height and the concept Tall. As heights increase they become more and
more compatible with the idea of Tall. At 6 feet and above all values of
Height are completely compatible with the idea of Tall.
Fuzzy Quantiﬁers
The fuzzy set Tall represents a class of fuzzy sets known as quantiﬁers.
These are sets whose structure is not built around a central value (see

78
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Tall
Figure 4.4
The fuzzy set Tall (for men).
Grade of membership m(x)
Project duration (weeks)
2
4
6
8
10
12
14
16
18
20
22
24
26
0
1.0
Long
Figure 4.5
Fuzzy set for a Long project.
the section “Fuzzy Numbers”). A quantiﬁer represents an open concept
and is quite common in fuzzy expert systems. Although the membership
function for the Tall fuzzy set (Figure 4.5) was a straight line, implying a
linear relationship, quantiﬁer sets more commonly use sigmoid (left- and

4.3
Fuzzy Sets
■
79
Grade of membership m(x)
Expectancy
8
10
12
14
16
18
20
22
24
26
28
30
32
0
1.0
Around 20
Figure 4.6
The fuzzy number About 20.
right-facing S-shaped) curves. Sigmoid curves, with inﬂection points and
attenuated surfaces, can model a wide variety of growth curves (such as
logistics, Gompertz, and Pearl curves). Figure 4.5 illustrates the fuzzy set
for a Long project (with the project duration measured in weeks).
The Long fuzzy set uses a growth curve. The left-facing S-curve is
called a decay curve. By adjusting the inﬂection point on an S-curve, as
well as the degree of skew in the upper and lower curve segments, decay
and growth curves can represent many highly nonlinear concepts. This
ability is very important in types of fuzzy models that are sensitive to small
changes in the underlying data (such as risk assessment and sustainability
models).
Fuzzy Numbers
A much larger class of fuzzy sets represents approximate numbers of one
type or another. Some of these fuzzy sets are explicitly “fuzziﬁed” num-
bers, whereas others simply represent fuzzy numeric intervals over the
domain of a particular variable (see the section “Fuzzy Term Sets”). Fuzzy
numbers can take many shapes: bell curves, triangles, and trapezoids.
Within each of these shapes the actual meaning of the fuzzy set depends
on the width or spread of the set itself. Figure 4.6 illustrates a typical
bell-shaped fuzzy number. This is a numeric quantity, About 20.

80
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Grade of membership m(x)
Cell diameters (protozoa) in micrometers
2
4
6
8
10
12
14
16
18
20
22
24    26 
0
1.0
Small
Tiny
Figure 4.7
The fuzzy number Small.
The fuzzy set About 20 shows two principal attributes of fuzzy
numbers: a central value and a degree of spread2 around the value. This
degree of spread is called the expectancy (e) of the fuzzy number and
is an important property. When e = 0, the fuzzy number is a single
point and corresponds to a normal scalar value (called a singleton). As the
expectancy increases, the number becomes fuzzier. This fuzziness is
directly connected to the robustness or brittleness of a fuzzy model as
well as to the separation of related concepts in the model. And, because
expectancy increases confusion over the exact value of a fuzzy number,
a wide expectancy also increases information entropy.
Another very common fuzzy number shape is the triangle. Due primar-
ily to the low computational cost of creating and interrogating triangular
fuzzy sets they are used extensively in control applications. In general,
a triangular fuzzy number is more brittle and therefore less robust than
a bell-shaped fuzzy number. Figure 4.7 illustrates a triangular fuzzy set
for the concept Small and shows that fuzzy numbers are often used to
represent the underlying numbers in terms of a particular concept. In this
instance, the cell diameter of a protozoa is taken to be Small (to some
degree) when it lies between 4 and 12 micrometers.
2 Degree of spread is not to be confused with degree of membership. Degree of spread is here
used in the ordinary sense of the term: a location on a scale of intensity, or an amount or quality.

4.3
Fuzzy Sets
■
81
Grade of membership m(x)
Blood potassium (K) levels (µg/cc)
.2
.1
.3
.4
.5
.6
.7
.8
.9
1.0
1.1
1.2
1.3 1.4 
0
1.0
High
Elevated
Figure 4.8
The fuzzy numbers for Elevated and High.
The fuzzy number Small in Figure 4.7 is also bounded by another fuzzy
number, Tiny. Although this might appear to be a linear fuzzy quantiﬁer, it
is actually a representative of the third type of fuzzy number, the trapezoid.
A trapezoidal representation is slightly but crucially different from the bell
and triangle numbers. Figure 4.8 shows two overlapping fuzzy numbers
represented as trapezoids.
The semantics of a trapezoidal number are slightly different from the
bell and triangular numbers because the set does not pivot, so to speak,
around a single, central number. Although we can continue to treat a
trapezoid as a special case of the triangular fuzzy set (with a plateau width
of zero), some model builders consider the trapezoid as a categorical or
class fuzzy set — interpreting the plateau as a class of numbers equivalent
to the fuzzy set concept. In this case, for example, Elevated is a class of
values near {0.6 to 1.0}. Just how near is determined by the width of the
trapezoid’s expectancy spread (which is measured from the edge, not the
center, of the plateau).
The existence of a plateau means that a set of values in the fuzzy set has
maximum membership values in the set. In both the bell and the triangular
sets only a single value is completely compatible with the set concept.
On either side of this single number, the fuzzy membership values can
(and usually do) drop off rather rapidly. By introducing plateaus into the
fuzzy number, we establish a class of values that in a fuzzy model tends
to create local areas of concentration.

82
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Grade of membership m(x)
Centimeters from arm locking position
Ð12
Ð15
Ð9
Ð6
Ð3
0
3
6
9
12
15
18
21
24 
0
1.0
Approaching
On
Near
Figure 4.9
Asymmetric fuzzy numbers.
Asymmetric Fuzzy Numbers
Unlike crisp numbers that have a precise value, fuzzy numbers have
a certain amount of ambiguity or spread around them. This spread or
expectancy is actually measured in two directions, to the left and the
right of the central value (or, for a trapezoid, the edges of the plateau of
values). Having two measures of fuzziness allows fuzzy numbers to have
asymmetric shapes. A fuzzy number can have both a kurtosis and a skew
(although these are not computed or measured as the equivalent concepts
in statistics). Figure 4.9 illustrates several asymmetric fuzzy numbers in a
robotic arm controller.
Nonsymmetric fuzzy sets are frequently encountered in control sys-
tems (such as the robotic arm controller) but they also play important
roles in both the development of complex nonlinear expert systems as
well as in the automatic, genetic tuning of rule-based fuzzy systems (see
Chapter 11 for an in-depth exploration of genetic tuning).
Psychometric Fuzzy Sets
In the previous fuzzy set examples the underlying domains were drawn
from a measurement associated with the property of data. Tall is a con-
cept overlaid on the measured values of Height. Long is a concept overlaid
on the measured values of project duration. Many models, however,

4.3
Fuzzy Sets
■
83
Grade of membership m(x)
Grade (severity) of risk
0
10
20
30
40
50
60
70
80
90
100
0
1.0
Risk
Figure 4.10
The psychometric Risk fuzzy set.
use or produce fuzzy sets that correspond to concepts unsupported by
any measured data. Such fuzzy sets often represent subjective mappings
between a concept and its intensity or degree of realization. Hence, these
fuzzy sets use a psychometric scaling for their domain. Psychometric
(or nonmeasured) fuzzy sets include a broad range of concepts, such as
Risk, Aptitude, Aggression, Sustainability, Indicated, Acceptable, and
Compatible. Figure 4.10 shows a fuzzy set representation for Risk.
The underlying domain must correspond to the granularity of the
model. The wider the domain, the larger the degree of evident separation
(a matter of resolution or discrimination) between the relative domain
points. In the Risk example (Figure 4.9), a small movement along the
membership function around the slope at the m[.5] region would be difﬁ-
cult to detect if the domain were [0, 10], would be moderately easy to see
with range [0, 100], and would produce a large displacement with range
[0, 1000]. At the same time, the range of the psychometric scale must be
associated with the model’s objectives in resolving small or large changes
along the horizontal axis.
Collections of Fuzzy Sets (Term Sets)
In fuzzy systems, the underlying numeric (and often categorical and enu-
meration) variables are broken down into a collection of fuzzy sets. These
fuzzy sets ordinarily deﬁne the underlying semantics of the variable and

84
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
40
35
45
50
55
60
65
70
0
1.0
Senior
Teen
Child
Middle
aged
Young
adult
Client_Age
Figure 4.11
The linguistic variables for Client_Age.
are the names by which fuzzy rule-based systems refer to the variable
itself. Because it is through the underlying fuzzy sets that fuzzy models
reference their variables, these sets are often called linguistic variables.
As an example, Figure 4.11 shows the variable Client_Age with its set of
underlying fuzzy sets.
Each underlying fuzzy set deﬁnes a portion of the variable’s domain.
But this portion is not uniquely deﬁned. Fuzzy sets overlap as a natu-
ral consequence of their elastic boundaries. Such an overlap not only
implements a realistic and functional semantic mechanism for deﬁning
the nature of a variable when it assumes various data values but provides
a smooth and coherent transition from one state to another.
This transition from one semantic state to another can be seen, for
example, for the YoungAdult and MiddleAged fuzzy sets (Figure 4.11).
As we move to the right from 30 years old (the point of maximum grade of
membership in YoungAdult), the degree of membership in YoungAdult
falls off and the degree of membership in MiddleAged begins to increase.
Between 30 and 45 years old an individual is, to some degree, both a young
adult and middle aged. When they are close to 30, their membership in
the middle-aged concept is very small, and when then they are close to

4.3
Fuzzy Sets
■
85
45 their membership in the young adult concept is very small (and goes
to zero at both of these end points). As we move further to the right, the
individual’s compatibility with the idea of middle aged begins to fall off
and they become more and more representative of a senior citizen.
Hedges: Fuzzy Set Transformers
In keeping with their use as semantic descriptors, fuzzy sets can be mod-
iﬁed through a family of adjectives known as hedges. A hedge acts on a
fuzzy set in the same way an adjective acts on a noun (or other adjective)
in English, by amplifying or otherwise modifying the noun’s meaning.
Hedges are used to:
■
Turn crisp values into fuzzy numbers
■
Increase or decrease the expectancy of a fuzzy number
■
Intensify or dilute the membership function of a fuzzy set
■
Change the shape of a fuzzy set through contrasts and restrictions
Hedges are used in fuzzy models, especially rule-based models, to dynami-
cally create new fuzzy sets and change the meaning of linguistic variables.
In this way, new fuzzy sets can be dynamically created and existing fuzzy
sets can be temporarily modiﬁed in ways that give the underlying linguis-
tic variable a different meaning or intention. A hedge changes the shape of
a fuzzy set in a predictable way according to six broad classes of actions.
■
Approximation: Increases or decreases the expectancy of a fuzzy
number. When applied to a scalar, an increasing approximation hedge
converts the number into a fuzzy number.3 Approximation hedges
include about, around, near, close to, and in vicinity of.
■
Contrast:
In a method similar to approximation,
increases or
decreases the membership function’s central measure of membership
by focusing in the area around the 50% membership space. Examples
of contrast hedges include almost, deﬁnitely, positively, generally,
and usually.
■
Dilution: Softens the membership function over the fuzzy set. Dilu-
tion weakens the membership constraints on the fuzzy set so that a
3 An ordinary (crisp) number can be viewed as a fuzzy set with an expectancy value of zero. An
approximation hedge simply increases this expectancy value.

86
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
point on the domain is truer than it would be for the undiluted set.
Dilution hedges include quite, rather, slightly, and somewhat.
■
Intensiﬁcation: Hardens the membership function over the fuzzy set.
Intensiﬁcation strengthens the membership constraints on the fuzzy
set so that a point on the domain is less true than it would be for the
unintensiﬁed set. Intensiﬁcation hedges include very and extremely.
■
Negation: Reverses the truth membership of the fuzzy set. Not is the
primary hedge for producing the complement of a fuzzy set.
■
Restriction: Restricts the membership function relative to the shape
of the underlying fuzzy set. Restriction hedges include above, below,
more than, and less than.
For example, Figure 4.12 illustrates the application of the about, some-
what, and very hedges to a model’s unit manufacturing costs (MfgCost).
The about hedge turns the scalar manufacturing costs into a fuzzy number.
The very intensiﬁcation hedge reduces the expectancy of the result-
ing fuzzy number, whereas the somewhat dilution hedge increases the
expectancy.
As Figure 4.12 also illustrates, hedges are applied according to the
ordinary rules of English grammar (from the inside out). The expres-
sion somewhat around MfgCost is evaluated by ﬁrst applying (around
Grade of membership m(x)
Cost ($)
8
10
12
14
16
18
20
22
24
26
28
30
32
0
1.0
Somewhat around
MfgCost
Around MfgCost
Very around MfgCost
MfgCost (manufacturing cost)
Figure 4.12
Hedges applied to MfgCost.

4.3
Fuzzy Sets
■
87
Grade of membership m(x)
Project duration (weeks)
2
4
6
8
10
12
14
16
18
20
22
24
26
0
1.0
Long
Somewhat
Long
Very Long
Project_Duration
Figure 4.13
Hedges applied to the Long fuzzy set.
MfgCosts) to create a fuzzy number (M ), then by applying somewhat(M )
to produce the ﬁnal results.
Whereas approximation hedges are limited to bell and triangular
fuzzy numbers, most of the other hedge classes can also be applied to
fuzzy qualiﬁers. For example, Figure 4.13 shows the fuzzy sets produced
by applying somewhat and very to the Long project duration fuzzy set.
Most hedges involve complex algorithmic processing of the member-
ship function. The basic intensiﬁcation and dilution hedges, however,
simply modify each point on the fuzzy set by applying a power function.
Expression 4.7 shows the power function for the intensiﬁcation hedge.
µintensify(A)(xi) = µn
A(xi)
(4.7)
The very hedge has n = 2 and so simply squares the membership value;
slightly has n = 1.2, and other intensiﬁcation hedges have stronger or
weaker exponents on the membership function. Expression 4.8 shows
the power function for the dilution hedge.
µdilute(A)(xi) = µ1/n
A (xi)
(4.8)
The somewhat edge has n = 2 and so simply takes the square root of the
membership values. Other forms of the dilution hedge class supply differ-
ent exponent values and hence take different roots. Power hedges must

88
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
be used with some care, however, in that the dilution or intensiﬁcation
hedge with larger exponents causes very large and semantically difﬁcult
changes to the membership function.
It is well to remember that the philosophy and the mechanics under-
lying hedges are heuristic in nature. Although we can justify some of the
transformations as the products of an intuitive feel for the ﬁnal results,
there is no supporting mathematical or logical apparatus that evolves
a hedge operation from fundamental axioms and theorems in fuzzy set
theory (see “Further Reading,” De Cock).
In rule-based fuzzy systems, the use of hedges provides a valuable and
powerful method of performing transformations on a basic vocabulary of
linguistic variables. This allows a single fuzzy set (such as Tall) to be used
in different but related contexts as demanded by the semantics of the
model at that time and place (thus, we can speak about very Tall without
having to create ahead of time a separate fuzzy set that represents the
hedged shape of very Tall ).
Further, the approximation hedges — whether applied explicitly or
implicitly — allow fuzzy models to treat all numeric values as fuzzy num-
bers, thus reducing complexity and allowing a uniform approach to
knowledge acquisition and management.
Alpha Cut Thresholds
An alpha cut threshold for fuzzy set A (or Aα) deﬁnes a minimum truth
membership level for a fuzzy set. All membership values below the alpha
cut are considered equivalent to zero. In most cases, the alpha cut is zero
(α = 0). As the alpha cut threshold is increased, the overall support set
is reduced. Figure 4.14 shows the shape of the Long fuzzy set when the
alpha threshold is set to [.35].
Alpha cuts play a crucial role in many fuzzy models by removing
unnecessary noise and specifying a degree of strength — often a level of
evidence — necessary in the model to effect a correct outcome. Thresh-
olds must be used with care, however, in that very high alpha cuts can
have serious deleterious effects on a model’s performance. Further, for a
collection of overlapping fuzzy sets an alpha cut higher than the crossover
point in the overlap will create voids in the UoD — creating areas that are
not covered by the support region of any fuzzy set.
Fuzzy Set Operators
Fuzzy logic supports a family of set operators analogous to those available
in Boolean logic. Unlike Boolean logic, however, the universe of possible

4.3
Fuzzy Sets
■
89
Grade of membership m(x)
Project duration (weeks)
2
4
6
8
10
12
14
16
18
20
22
24
26
0
1.0
Long
Alpha cut threshold
µ [.35]
Project_Duration
Figure 4.14
Alpha cut threshold applied to fuzzy set Long.
TABLE 4.1
The Meanings of Fuzzy Operators
Fuzzy Operator
Deﬁnition
And
µT (xi) = min(µA(xi), µB(xi))
Or
µT (xi) = max(µA(xi), µB(xi))
Not
µT (xi) = 1 −µA(xi)
truth values is nearly inﬁnite. And an inﬁnite spectrum of truth values
means that we cannot construct simple 2×2 truth tables. Although there
are many types of fuzzy set operators, a basic functional set correspond-
ing to union, intersection, and complement are deﬁned as speciﬁed in
Table 4.1.
It is clear that the fuzzy set operators parallel their Boolean logic coun-
terparts. We can use the Client_Age variable (see Figure 4.11) to examine
how these methods apply logical operators to fuzzy sets.
The Fuzzy And Operator
Figure 4.15 illustrates the fuzzy region produced by the proposition
YoungAdult And MiddleAged. This is the region were an age is a member
of (compatible with) both the young adult and middle-aged concepts.

90
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
0
1.0
Client_Age
Young
adult
Middle
aged
Figure 4.15
The fuzzy And operator.
In Figure 4.15, the bold line traces the resulting truth function. The
area falls into the triangular region that forms the intersection of both
adjoining fuzzy sets.4
The Fuzzy Or Operator
Figure 4.16 illustrates the fuzzy region produced by the proposition
YoungAdult Or MiddleAged. This is the region in which an age is a
member of (compatible with) either the young adult or middle-aged
concepts.
In Figure 4.16, the bold line traces the resulting truth function. The
area, represented by the twin peaks, spans the width of both fuzzy sets.
The Fuzzy Not Operator
Figure 4.17 illustrates the fuzzy region produced by the proposition Not
MiddleAged. This is the region in the linguistics variables supporting
Client_Age that forms the complement of the middle-aged concept.
4 For this reason, the family of fuzzy conjunctive (or intersection) operators is known collectively
as a T-norm.

4.3
Fuzzy Sets
■
91
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
0
1.0
Client_Age
Young
adult
Middle
aged
Figure 4.16
The fuzzy Or operator.
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
0
1.0
Client_Age
Middle
aged
Figure 4.17
The fuzzy Not operator.

92
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Notice in Figure 4.17 that a fuzzy Not (the negation) does not form a
rectangular region from 30 to 55 but is an inverted triangle representing
the degree to which each domain value is not a member of the MiddleAged
fuzzy set.
Consequences of the Negation Operator
As we previously noted, fuzzy set membership curves represent semiper-
meable boundaries. This means that two or more fuzzy sets often share
the same elements — each set having a particular degree of membership
for the shared elements. The simplest perspective on this is evident in
Figure 4.18 when we look at the fuzzy sets for Short and Tall.
These two sets are complements of each other; that is, Short is the
negation of the membership function for Tall. As you can see, the two
sets completely overlap so that each element (Height value) has a degree
of membership in Tall and a degree of membership in Short. This agrees
with the ambiguity normally found in nature. As an individual’s height
increases we move down the Short membership curve and up the Tall
membership curve. At any point along the height axis, a value has a partial
degree of membership in Short and a partial degree of membership in Tall.
Complementary fuzzy sets, and by extension fuzzy logic, break Aris-
totle’s law of the excluded middle and noncontradiction. The break is
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Short
Tall
Figure 4.18
Tall and Short fuzzy sets.

4.4
Review
■
93
due to the way a complement is constructed in fuzzy logic using the
negation operator (see Table 4.1). A complement measures the degree of
distance between complete membership (having a truth value of [1]) and
the complement set’s corresponding membership.
Therefore, when A[x] has a membership of µ[.8], the complement,
∼A[x], has a membership of µ[.2]. We interpret this to mean that x is
strongly a member of fuzzy set A and weakly a member of fuzzy set ∼A.
In such a case we are — in violation of Aristotle’s law and the prevailing
law of bivalence in Boolean logic — afﬁrming that x is a member of both
sets. We can easily see this in Figure 4.18, across the region in which Tall
and Short overlap. Of course, fuzzy logic and Aristotelian or Boolean logic
converge at the extreme edges of the fuzzy set. If A[x] has a membership
of [1], then ∼A[x] has a membership of zero.
These are the basic fuzzy set operators. However, they are not
the only possible logical operators. A very wide range of additional
union, intersection, and negation operators have been deﬁned by sev-
eral researchers. Further, a class of operators developed by Yager and
O’Hagan have adjustable parameters that increase their Or-ness as well as
their And-ness.5
4.4
Review
This chapter introduces and explores the nature of fuzzy sets. Understand-
ing the nature of fuzzy sets and the principles of fuzzy logic is necessary
in order to fully appreciate the fuzzy system techniques discussed in this
part of the book. You should now be familiar with the following.
■
The difference between Boolean and fuzzy sets
■
The results of comparable operations on Boolean and fuzzy sets
■
The organization and structure of a fuzzy set
■
The various types of fuzzy sets (quantiﬁers and numbers)
■
How variables are broken down into fuzzy sets
■
The meaning of overlap in a collection of fuzzy sets
■
The use of hedges and alpha cut thresholds
5 Although not explored in this book, the adjustable properties of compensatory operators are
ideal for the genetic tuning of fuzzy rule-based models and fuzzy clustering. For comprehensive
details on compensatory operators, see Cox in the “Further Reading” section.

94
■
Chapter 4
Fundamental Concepts of Fuzzy Logic
Fuzzy sets are only the building blocks of complex analytical systems, such
as rule-based expert systems, clustering, and fuzzy database queries. To
gain an appreciation of how fuzzy sets are used to build complex models,
make policy decisions, and represent knowledge, we need to examine the
general principles underlying fuzzy systems. The next chapter introduces
these principles through the ideas behind rule-based fuzzy models.
Further Reading
■
Cox, E. The Fuzzy Systems Handbook (2d ed.). New York: Academic Press,
1999.
■
De Cock, M. “Representing the Adverb Very in Fuzzy Set Theory,” Proceedings
of the ESSLLI Student Session, Chapter 19, 1999.
■
Lotﬁ, Z. “Fuzzy Sets,” in Information and Control, vol. 8, New York: Academic
Press, pp. 338–353, 1965.
■
Lotﬁ, Z. “Outline of a New Approach to the Analysis of Complex Systems and
Decision Process,” in IEEE Transactions on Systems, Man, and Cybernetics,
SMC-3 pp. 28–44, 1973.

■■■
Chapter 5
Fundamental Concepts of
Fuzzy Systems
This chapter examines the nature of fuzzy logic as a method of making
implications with approximate or vague concepts and continues through
the development of approximate reasoning techniques used in fuzzy expert
and decision support systems. The nature of fuzzy implication methods
is contrasted with probabilities, showing how the two approaches model
different aspects of uncertainty. In the ﬁnal sections of the chapter we look at
fuzzy rule-based decision systems and see how they are intimately connected
to the process of knowledge discovery and rule induction. This tutorial on
fuzzy logic and fuzzy systems provides the foundation needed to understand
how knowledge discovery mechanisms such as clustering and rule induction
generate operational models.
As we noted in the previous chapter, fuzzy logic is a system of logic. It is
not a high-level mechanism such as a neural network or genetic algorithm.
Until fuzzy logic is combined with some reasoning framework, such as
rule-based expert systems or decision trees, it is simply a way of repre-
senting and manipulating data. Fuzzy systems incorporate fuzzy logic into
their reasoning mechanism to gain a decided advantage over the crispness
of Boolean logic for a wide range of problems. Thus, rule-based expert
systems or control systems cast their rules using fuzzy sets and fuzzy oper-
ators. Clustering algorithms can use fuzzy set membership as part of their
feature partitioning. A neural network might use fuzzy logic as part of its
representation strategy or as part of its outcome space. A genetic algo-
rithm can use fuzzy numbers in its genome representation, or fuzzy logic
as a part of its “goodness of ﬁt” evaluation. In each of these cases the
use of fuzzy logic provides a high degree of ﬂexibility and robustness
in the underlying model. As a comprehensive example that introduces a
wide spectrum of fuzzy knowledge representation and reasoning features,
this chapter examines the nature of fuzzy rule-based models.
95

96
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
5.1
The Vocabulary of Fuzzy Systems
Fuzzy systems also have a rich vocabulary of unfamiliar terms. The
vocabulary is occasionally difﬁcult to understand because, in addition
to terms unique to fuzzy systems, it combines terms from conven-
tional system theory as well as expert and decision support systems and
often uses them in a completely different way. Thus, on top of the
vocabulary of fuzzy logic, we now need to understand such concepts
as aggregation, defuzziﬁcation, and correlation as they apply to fuzzy
system.
Once more, this is not a complete dictionary of all the terms in fuzzy
systems. A preview, however, of the fundamental vocabulary terms at
the beginning of this chapter will make reading and understanding the
material much easier. It is not always possible, while maintaining an
uncluttered and coherent discussion of fuzzy systems, to insure that
every term is introduced before it is used. Many of these terms, of
course, will be discussed in greater depth later in this and subsequent
chapters.
Aggregation
The process of combining the outcome fuzzy sets in a rule is called
aggregation. As an example, when two rules such as
if x is Y then z is R
if w is A then z is T
are executed, the fuzzy sets R and T are aggregated to produce the current
representation for the outcome variable z (which is also represented as
an under-generation fuzzy set). Aggregation is done by either adding the
fuzzy set membership functions together or by taking the maximum value
across their combined membership functions.
Alpha Cut Threshold
An alpha cut threshold truncates the membership function of a fuzzy set
or the truth value of a fuzzy rule’s predicate at a speciﬁc truth value. Any
value that falls below the alpha cut threshold is equivalent to zero. Alpha
cuts come in two forms: Strong (the comparison is less than) and Weak
(the comparison is less than or equal).

5.1
The Vocabulary of Fuzzy Systems
■
97
Approximate (or Fuzzy) Reasoning
The general term for a rule-based reasoning process incorporating fuzzy
sets, hedges, fuzzy operators, and a set of decomposition rules is called
approximate reasoning.
Conditional Fuzzy Rule
A conditional rule is executed only when the premise (or predicate) of
the rule is true. Thus, in a rule such as if a is Z then b is Y, the outcome
expression b is Y is only evaluated when the premise (or predicate) a is
Z is true. See also “Unconditional Fuzzy Rule.”
Consequent Fuzzy Set
A fuzzy set that appears on the right-hand side of an outcome expression
and is aggregated with the outcome fuzzy set is called the consequent
fuzzy set. In the rule if a is Z then b is Y, Y is the consequent fuzzy set.
The consequent fuzzy set is usually one of the fuzzy sets in the term set
associated with the outcome variable (that is, Y is one of the fuzzy sets
associated with b).
Correlation
The process of adjusting the height of the consequent fuzzy set based
on the truth of the rule’s premise is called correlation. Thus, in the rule
if a is Z then b is Y, the height of the consequent fuzzy set Y (which
directly affects its contribution to the outcome variable b) is reduced by
the amount of truth in the fuzzy proposition a is Z. Consequent fuzzy
sets are adjusted in two ways: by truncating the fuzzy set at the truth of
the premise or by scaling the fuzzy set using the truth of the premise.
Correlation is one of the primary mechanisms for adjusting the outcome
of a fuzzy knowledge base using the amount of evidence in the rules.
Defuzziﬁcation
The process of converting the under-generation outcome fuzzy set to a
single representative value is called defuzziﬁcation. For example, when
a collection of fuzzy rules such as
if x is Y then z is R
if w is A then z is T

98
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Grade of membership m(x)
Items
8
10
12
14
16
18
20
22
24
26
28
30
32
0
1.0
Outcome “z”
Figure 5.1
Defuzzifying an outcome fuzzy set.
are executed, the outcome variable z must be defuzziﬁed in order to
resolve a single crisp value for the variable. There are many ways of
defuzzifying an outcome fuzzy set. The most common approach, illus-
trated in Figure 5.1, is by using the center of gravity (or centroid) of the
outcome set.
The center of gravity is actually the weighted average value of the
membership function. Note that not all fuzzy models use defuzziﬁcation to
ﬁnd the value of an outcome fuzzy set. In many models, the outcome fuzzy
set is used as the input to another model (the outcome of one model that
determines the estimated disposable income for a class of buyers might
be input to another model that handles cross marketing and special-offer
sales to customers based on their spending habits and probable income).
Fuzzy Associative Memory (FAM)
A FAM is a fuzzy knowledge base (KB). It is a compact way of represent-
ing fuzzy rules used primarily in control applications (but it also has its
uses in fuzzy information modeling as a way of representing rules discov-
ered during the rule induction process). A FAM can be visualized as an
N-dimensional “cube.” Each edge represents the fuzzy sets associated with
a particular independent variable. The region formed by the intersection
of each fuzzy set on each edge is the fuzzy set that contributes to the out-
come or dependent variable. Fuzzy associative memory representations

5.1
The Vocabulary of Fuzzy Systems
■
99
are not only very compact, they also provide a very high speed way of
executing rules that do not involve hedges, subscripts, and expressions.
Fuzziﬁcation
The process of ﬁnding the membership value of a scalar (a number) in a
fuzzy set is called fuzziﬁcation. Fuzzy rule-based models work by fuzzify-
ing the input values. In the rule if a is Z then b is Y, the membership of
a in Z is found and this truth is used as part of the premise evaluation.
Knowledge Base
A collection of fuzzy rules representing a single model is called a knowl-
edge base (or simply a KB). A fuzzy knowledge base consists of both
conditional and unconditional rules. All rules are run effectively in par-
allel to create the ﬁnal under-generation fuzzy set associated with each
outcome variable.
Outcome Fuzzy Set and Variable
Each solution (or dependent) variable associated with a fuzzy model is
also an outcome variable for one or more rules in the model’s knowledge
base. Every outcome variable has an associated outcome fuzzy set that is
created through the aggregation of consequent fuzzy sets for each rule
that contributes to the outcome variable. In the rule if a is Z then b is
Y, b is the outcome variable and is an outcome fuzzy set. When all the
rules that contribute to b have been executed, the underlying (or under-
generation) fuzzy set for b is defuzziﬁed, and the resulting value assigned
to the variable b.
Rule
A rule speciﬁes a fuzzy relationship. There are two types of rules: con-
ditional and unconditional. A conditional rule is executed only when the
premise (or predicate) of the rule is true. Thus, in a rule such as if a is
Z then b is Y, the outcome expression b is Y is only evaluated when
the premise (or predicate) a is Z is true. An unconditional rule is always
executed. Unconditional rules, such as c is X, state an underlying fuzzy
relationship and are always executed before any conditional rules are
executed.

100
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Unconditional Fuzzy Rule
An unconditional rule is always executed. Unconditional rules, such as
c is X, state an underlying fuzzy relationship and are always executed
before any fuzzy conditional rules. A model with unconditional fuzzy
rules provides a supporting framework that can supply an answer when
none of the conditional rules is executed.
Under-generation Fuzzy Set
Each outcome variable in a fuzzy rule-based model has an associated fuzzy
set of the same name. This fuzzy set is initially empty but is built by each
rule that contributes to the value of the outcome variable. As a result,
the outcome fuzzy set is “under generation” during the execution of the
fuzzy rules. The effect of this under-generation process is to simulate
the workings of parallel rule execution: an outcome variable in a fuzzy
model does not have a value until all rules have been executed and their
consequent fuzzy set added to the outcome variable’s fuzzy set.
5.2
Fuzzy Rule-based Systems: An Overview
A fuzzy rule-based system is, in a general sense, a fuzzy expert system. The
expertise can be derived from subject matter experts or extracted from
data through a rule induction process. The rules represent nonprocedural
statements of knowledge in the form of if-then sentences. These rules
reference a collection of local variables that in turn references a collection
of fuzzy sets that deﬁne control regions over the variable’s domain. The
architecture of a rule-based system is relatively uncomplicated. Figure 5.2
provides a general perspective on the principal components.
All elements of the system’s knowledge are stored in a central repos-
itory (the KB). The actual reasoning process of the system is handled by
an inference engine. The processes underlying a rule-based fuzzy system
are, roughly, as follows.
■
The inference engine collects and executes rules.
■
Rules evaluate problem states and, as a group, set the value for one or
more outcome variables.
■
To evaluate states and set outcome variables, rules reference variables
and their associated fuzzy sets.

5.2
Fuzzy Rule-based Systems: An Overview
■
101
Outcome
variable results
Inference engine
Fuzzy sets
Knowledge base (KB)
Rules
Variables
Data
acquisition
Figure 5.2
The principal components of a fuzzy rule-based system.
■
Variables contain the current set of problem-speciﬁc data. These data
are acquired in one of two ways — regularly and automatically from
a collection of external sensors or from a data acquisition process
invoked by the inference engine (such as starting a separate backward
chaining inference process to ﬁnd the value of a variable that appears
in the outcome of some other rules).
■
When all rules have been executed, the inference engine makes the
values of the outcome variable available to the application that invoked
the inference engine.
If the inference engine is part of a continuously running system, the knowl-
edge base is returned to its initial state, a new set of data values is acquired,
and the reasoning process is started again.
We will return to a more detailed discussion of a knowledge base
later. Now we need to explore the nature of rules and how variables are
organized into a collection of fuzzy sets. This exploration prepares the
way for a more extensive discussion of fuzzy reasoning: how rules are
used to solve a particular problem.

102
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Fuzzy Rules
A fuzzy rule-based reasoning system uses a collection of rules to accu-
mulate evidence and generate the proper shape of a fuzzy set associated
with a solution variable. Rules take knowledge, usually from subject mat-
ter experts (SMEs) and express this knowledge in the form of conditional
and unconditional rules. These rules and their associated variable deﬁni-
tions are stored in a KB. The KB is then used by the machine reasoning
system (called an inference engine) to compute the ﬁnal value for one or
more outcome variables.
Conditional Rules
Nearly all rules in a conventional fuzzy KB are conditional. Conditional
rules are in the form of if-then and, without exploring the complete
semantics of the possible rule language, have the general form
if premise then outcome.
Where:
premise
(also called the predicate or antecedent) is one or more fuzzy
propositions connected by the And or Or operator. A fuzzy propo-
sition is a relationship between a variable and a fuzzy quantity. The
following is an example.
If a is Y and b is X then c is Z
The rule premise is the expression shown in bold. The fuzzy quan-
tity can be a fuzzy number or one of its underlying fuzzy sets (such
as project_length is Long, where project_length is a variable in
the model and Long is one of the fuzzy sets associated with this
variable).
outcome
is a fuzzy proposition that assigns a fuzzy value to a solution
variable. The following is an example.
If a is Y and b is X then c is Z
The rule outcome is the expression shown in bold. The outcome
contains an outcome or solution variable (in this case, c) and a
consequent fuzzy set. Every outcome variable in a fuzzy model
is represented by an “under-generation” fuzzy set created and
maintained automatically by the reasoning mechanism. This out-
come fuzzy set contains the aggregation of all rules that update
the associated outcome variable.

5.2
Fuzzy Rule-based Systems: An Overview
■
103
Premise
(antecedent)
Consequent
(outcome)
if the competition_price is not very High
then our price must be near the competition_price 
Figure 5.3
Parts of a fuzzy rule.
Conditional rules contribute to a solution for a particular variable by
reshaping the consequent fuzzy set and then combining it with the out-
come variable’s under-generation fuzzy set. Thus, the ﬁnal outcome
reﬂects the amount of evidence (the amount of truth) in each of the
supporting conditional rules.
A rule is called conditional when its execution is predicated on the
truth value of its premise. For a fuzzy rule to be executed, the premise
must be true.1 Conditional fuzzy rules can often be quite complex, involv-
ing intersection and union operations as well as dynamically created fuzzy
sets (such as fuzzy numbers created by approximation hedges). Figure 5.3
illustrates, using a rule from a product pricing model, the major structural
parts of a conditional rule.
If we examine the lexical organization of a conditional fuzzy rule a
bit closer, many features of its fuzzy logic support become apparent.
Figure 5.4 steps down one more layer and highlights the various technical
components of the rule.
As complicated as this rule appears it is still a simple version of the
posit if a is Y then c is Z. The antecedent expression not very High
produces a fuzzy set (Y ) and the outcome expression near the com-
petition_ price produces a fuzzy number centered around the current
1 Actually, a fuzzy rule can also have its own truth threshold (similar to the alpha cut threshold
on a fuzzy set). In order for a rule to execute, the degree of truth in the premise must be equal
to or greater than this rule truth threshold.

104
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Hedges
Fuzzy set
Variable
Belongs in the term set of
Noise
word
Outcome
variable
(fuzzy set)
Consequent
fuzzy set
if the competition_price is not very high
then our price must be near the competition_price 
Noise
word
Noise
word
Figure 5.4
Detailed parts of a fuzzy rule.
value for the variable competition_ price (Z ). Thus, the rule is basically
if competition_ price is Y then price is Z.
Unconditional Rules
An unconditional fuzzy rule establishes the default support space for an
outcome variable. The rule is cast in the form of the outcome propo-
sition outcome, with the same syntax as the outcome statement in a
conditional rule.
An unconditional rule, as the name implies, is always executed (and all
unconditional rules for a speciﬁed outcome variable are executed before
any of the conditional rules for the same outcome variable). Generally,
an unconditional rule contributes to the solution only if none of the con-
ditional rules ﬁres or none of the conditional rules has a premise truth
greater than the maximum truth of any unconditional rule.
5.3
Variable Decomposition into Fuzzy Sets
In the previous section on the fundamentals of fuzzy logic, the ways in
which variables are decomposed into fuzzy sets were brieﬂy discussed.

5.3
Variable Decomposition into Fuzzy Sets
■
105
We now take up this issue in more detail, exploring how fuzzy systems
can segment variables into various types of fuzzy sets and how these
fuzzy sets, known as linguistic variables, are used as part of the basic rule
language. As we will see, there are three principal structural organizations,
as follows.
■
Semantic decomposition, in which the fuzzy sets represent true lin-
guistic terms within the vocabulary framework of the model (terms
such as fast, high, elevated, expensive, moderate, and so forth).
■
Control decomposition, in which the fuzzy sets represent rates of
change and degrees of movement and similar physical measurements
often expressed at positive and negative displacements.
■
Data space partitioning, in which the fuzzy sets represent arbitrary
or statistically derived overlapping interval slices in the variable’s
domain.
Each type of variable decomposition or partitioning is associated with a
particular type of model and, although not mutually exclusive, they are
seldom found together in a single model. Thus, as we will see, how a
variable is decomposed depends to a great extent on the actual purpose
of the model.
Before examining variable decomposition, it is important to under-
stand an important fact about fuzzy systems: there are no universal fuzzy
sets. All fuzzy sets are context dependent. Thus, the concept Long, for
example, is different for different concepts (a long project, a long river,
a long coast line, and so on). Even the same fuzzy concept can have
different representations. The idea of what constitutes a long project
may differ within an organization from department to department and
will certainly differ between small, large, and very large enterprises and
federal government agencies. Thus, for semantic and control decomposi-
tions, one of the chief responsibilities of a knowledge engineer is eliciting
domain and semantic information from SMEs in order to precisely deﬁne
the underlying fuzzy sets for each variable.
Semantic-based Fuzzy Sets
Nearly all fuzzy expert and decision systems use semantic fuzzy sets. These
fuzzy sets establish the working vocabulary of the model. As we will see,
fuzzy rules establish relationships by mapping a variable to one or more
of its underlying fuzzy sets. These underlying fuzzy sets, the linguistic

106
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Grade of membership m(x)
Project duration (weeks)
2
4
6
8 10 12 14 16 18 20 22 24 26
0
1.0
Long
Grade of membership m(x)
Project duration (weeks)
2
6
4
8
14
12
10
16 18 20 22
26
24
0
1.0
Long
Grade of membership m(x)
Project duration (weeks)
2
4
6
8 10 12 14 16 18 20 22 24 26
0
1.0
Long
Grade of membership m(x)
Project duration (weeks)
2
4
6
8 10 12 14 16 18 20 22 24 26
0
1.0
Long
Figure 5.5
Possible shapes for Long project.
variables, constitute the vocabulary for the rule language. The semantics
are reﬂected in the shape of the membership curve. Thus, the concept of
a Long project might have different membership curves depending on the
culture of the organization or how the set is used in a particular model.
Figure 5.5 illustrates several possible (but hardly exhaustive) fuzzy sets
for the idea of a Long project.
The differences in the possible shapes for the Long fuzzy set reﬂect the
model builder’s expectation of how a user of the model will ultimately
view the concept of Long. This is expressed both in the choice of the
membership function shape and in the range of the underlying domain.
In all of the Long fuzzy sets shown in Figure 5.5 a project is unambigu-
ously long after 26 weeks and generally considered fairly long after 18
to 20 weeks (although the exact rate of change over the membership
function is different).
The choice of a fuzzy set membership function is not, however,
arbitrary.
In a broad sense,
fuzzy sets reﬂect the semantics of a

5.3
Variable Decomposition into Fuzzy Sets
■
107
particular concept. The mapping between the domain and the grade of
membership must roughly coincide with the actual meaning of the over-
lying concept. Also, each fuzzy set used in a model has at least one point
with a membership value of 0 and at least one point with a membership
value of 1. These sets are called normal fuzzy sets.
In a fuzzy model, many of the independent and all of the dependent
variables are decomposed into a collection of overlapping fuzzy sets. Each
of the fuzzy sets has a label (or name). The collection of fuzzy sets asso-
ciated with a variable is called the term set. Each fuzzy set in a term set
is also known as a linguistic variable. In a fuzzy model, rules reference a
variable through one or more linguistic variables rather than through spe-
ciﬁc data values. For example, Figure 5.6 shows the term set (or linguistic
variables) associated with the variable Client_Age.
Each of the fuzzy sets associated with Client_Age assigns a name to a
region of the underlying domain. These names are used in a rule as part of
the mechanisms that establish a degree of membership relation between
the current value of Client_Age and one or more of the fuzzy sets. Given
a Client_Age of 52, therefore there are two non-zero linguistic variable
regions: MiddleAged(µ[.43]) and Senior(µ[.57]). These relations are used
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
40
35
45
50
55
60
65
70
0
1.0
Senior
Teen
Child
Middle
aged
Young
adult
Client_Age
Figure 5.6
Term set for variable Client_Age.

108
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
10
20
30
40
50
60
70
80
Relative risk tolerance
90
100
None
Grade of membership m(x)
Low
Moderate
High
Exceptional
1.0
 0
RiskTolerance
Figure 5.7
The risk tolerance term set.
in rules to determine the composite truth of the premise and to modify
the outcome fuzzy region.
Suppose, for example, we have a model connecting age with risk tol-
erance. This might be expressed functionally as shown in Expression 5.1.
yrisk = f (xage)
(5.1)
Risk tolerance is the dependent variable, and age is the independent
variable. In a fuzzy model we need to create the RiskTolerance variable
and deﬁne its underlying collection of fuzzy sets. Figure 5.7 shows one
possible term set for the dependent variable.
These underlying linguistic variables provide the vocabulary for our
rules. Through them we express the relationships between data and
semantics. In the simplest form, a rule consists of a set of premise relation-
ships and an outcome relationship. For example, consider the following
code fragment.
If Client_Age is MiddleAged or ClientAge is Senior
Then Risk_Tolerance is Low;
Fuzzy rules can also manipulate the underlying linguistic variables in ways
that create new fuzzy regions and often subsume one or more term set
members. The role of linguistic variable transformer is played by hedges.

5.3
Variable Decomposition into Fuzzy Sets
■
109
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
0
1.0
Above middle aged
Middle
aged
Client_Age
Figure 5.8
The Above MiddleAged fuzzy set.
For example, consider a rule that is similar in actual meaning and function
as the previous rule,
If Client_Age is above MiddleAged
Then Risk_Tolerance is Low;
The above hedge creates a new fuzzy set that overlays the MiddleAged
term and absorbs the Senior term. Figure 5.8 illustrates how this hedge
provides the rule language with the ability to manipulate the underlying
terms of a variable.
Thus, semantic decomposition breaks down the domain of a variable
into a collection of overlapping fuzzy sets that gives each region of the
domain a speciﬁc name associated with some aspect of the model’s func-
tionality. These fuzzy sets are called linguistic variables because they allow
linguistic rather than algebraic manipulation of the model’s control space.
Control Fuzzy Sets
A fuzzy control system is, in effect, a fuzzy expert system. The fuzzy sets
in a control system, however, usually take the form of variations in the

110
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Grade of membership m(x)
∆(error)
–1
0
1
0
1.0
NL
NM
NS
PS
Zero
PM
PL
Change_In_Error
Figure 5.9
Term set for a Change_In_Error variable.
physical parameters and states of the plant (a “plant” is an engineering
term deﬁning the actual process being controlled). These sets reﬂect such
elements as current ﬂow, angular displacement, the rate of change in
angular moment, the rate of change in measured error, and so forth.
In the case of control systems, the fuzzy sets have less to do with the
semantics of the model than with critical areas of control and degrees of
granularity. For example, Figure 5.9 illustrates the underlying fuzzy sets
for a Change_In_Error variable.
The Change_In_Error variable measures the difference in the error for
successive executions of the controller. The normalized error falls in the
domain [−1, 1]. There are seven fuzzy sets measuring this change: Zero,
PS (a small positive change), PM (a medium positive change), PL (a large
positive change), NS (a small negative change), NM (a medium negative
change), and NL (a large negative change). Increasing the number of
fuzzy sets increases the granularity of the variable and generally increases
the precision of the controller. This increase, however, comes at a price
because the number of rules in a system increases exponentially with the
number of fuzzy sets associated with each of the control variables.
Like conventional expert systems, the shape and distribution of the
fuzzy sets underlying control variables must correspond to the way in

5.3
Variable Decomposition into Fuzzy Sets
■
111
Grade of membership m(x)
Voltage
10
0
20
30
40
50
70
60
80
90
100 110 120 130
0
1.0
Large
Small
Medium
Zero
Current_Flow
Figure 5.10
The term set for Current_Flow.
which control is expressed in the model. This level of control is expressed
in the number of fuzzy sets and their degree of overlap. Figure 5.10
illustrates this idea through the Current_Flow variable.
The asymmetry of the Current_Flow term set represents the way
the controller focuses on critical measures. The shape of the fuzzy sets
and their overlap control the transition between one control state and
the next. In this, fuzzy controllers and fuzzy expert systems with their
semantic sets share a common architecture. In general, however, fuzzy
controllers represent a much more restricted space in the universe of
fuzzy systems. Fuzzy controllers typically use triangular fuzzy sets (due to
their high-speed interpolation properties), do not employ hedges to trans-
form their fuzzy sets, and do not use an alpha cut threshold to constrain
the truth of fuzzy sets or rule predicate truth values.
Data-space-partitioning Fuzzy Sets
Fuzzy data mining, exploration, clustering, and ranking methods often
decompose variables into either arbitrary or statistical fuzzy regions in

112
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Grade of membership m(x)
Dollars (x1000)
20
0
40
60
80
100
140
120
160 180 200 220 240
300
280
260
0
1.0
PB2
PB3
PB4
PB5
PB6
PB1
Project_Budget
Figure 5.11
An arbitrary partitioning of the Project_Budget variable.
order to “slice” the data space into overlapping regions of similar elements.
Where partitioning is based on arbitrary segmentation or statistics, the
names associated with the underlying fuzzy sets are also arbitrary. These
names simply reﬂect a fuzzy partitioning space. Figure 5.11 illustrates the
arbitrary segmentation of the Project_Budget variable.
In the case of arbitrary partitioning, names are also assigned to the
fuzzy region in a somewhat arbitrary manner, often by abbreviating the
variable name and assigning a monotonic numbering sequence. Arbitrary
partitioning often follows a naming convention similar to that of a control
variable, (such as Zero, P1, P2, P3, and so forth) for the positive region
of the domain.
Variables can also be partitioned according to their statistical proper-
ties. In this case, the segmentation is not completely arbitrary but is based
on the statistical distribution properties of the variable. If we compute the
average budget of all completed projects for the enterprise along with its
standard deviation, Figure 5.12 shows one likely way of partitioning the
Project_Budget variable.
A fuzzy region is centered on the mean of the project budget distribu-
tion. The width of the fuzzy region is usually some fractional multiple of
the standard deviation. Partitioning is then accomplished by moving out

5.4
A Fuzzy Knowledge Base: The Details
■
113
Grade of membership m(x)
Dollars (x1000)
0
1.0
Less1
Mean
More1
More2
PB1
Project_Budget
Figure 5.12
A statistical partitioning of the Project_Budget variable.
to the left and right of the mean fuzzy set. The width of these new fuzzy
sets can also be some fraction or ratio-biased proportion of the standard
deviation. More advanced statistical partitioning techniques also take into
account the skew (symmetry) and kurtosis (ﬂatness) of the distribution.
5.4
A Fuzzy Knowledge Base: The Details
As we noted in the introduction, a collection of variables, fuzzy sets, and
fuzzy rules is called a knowledge base (or KB). The KB is a representation
of a particular problem space. However, a KB is generally not a complete
representation of a problem. Rather, a collection of KBs may cooperate
to form a solution to complex problems. In this section we examine the
organization of a KB in more detail, the nature of rule explosions in a KB,
and the way in which fuzzy rules are executed in a fuzzy KB.
Knowledge Base Architectures
KBs are containers; they encapsulate the data deﬁnitions (variables) and
solution heuristics (rules) necessary to solve a set of outcomes associated

114
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Variable
A
Variable
B
Variable
C
Variable
D
Rule 5
Rule 3
Rule 1
Rule 4
Rule 2
Variable
X
Fuzzy
term sets
Variable
Y
Outgoing
data
Incoming
data
Independent (and model) variables
Fuzzy sets (variable terms)
Dependent  (outcome) variables
1.0
0
a
µ (x)
1.0
0
b
 µ (x)
1.0
0
d
µ (x)
1.0
0
c
µ (x)
1.0
0
x
µ (x)
Under
generation
1.0
0
x
µ (x)
Fuzzy
term sets
1.0
0
y
µ (x)
Under
generation
1.0
0
y
µ (x)
Figure 5.13
The internal organization of a KB.
with the problem. As Figure 5.13 illustrates, a KB contains all independent
(basically, input) and dependent (basically, outcome) variables, as well as
the rules that connect the inputs to the outcomes.
As a general principle, when a fuzzy KB contains only conditional
rules the order of the rules in the KB is unimportant. This is because in a
fuzzy model all rules for a particular outcome variable are effectively run
in parallel. However, if the KB contains unconditional rules, these must
appear and be executed before any of the conditional rules. The order in
which the rules appear in the physical KB, however, are not the order in
which they are executed. It is the inference engine that collects and orders
the rules in the proper ﬁring sequence. Listing 5.1 shows a typical fuzzy
KB for a business application (in this case, the product pricing model we
will shortly examine in much more detail).
This knowledge base creates a set of fuzzy sets. These will later
be assigned to variables as their underlying term set. Each variable is
then deﬁned. The price variable is an outcome variable and is public

5.4
A Fuzzy Knowledge Base: The Details
■
115
#
#--------------------------------------------------------
# This is the standard retail pricing model. It runs in a
# full fuzzy environment(parallel rule processing,
# evidence aggregation, and relaxation of Aristotle’s
# excluded middle [non-contradiction] law).
#--------------------------------------------------------
#
kb pricingPolicy
fuzzyset high
shape(linearup)
range(0,48) surface(0,48);;
fuzzyset low
shape(lineardown) range(0,48) surface(0,48);;
variable public outcome
price
datatype(double) range(0,48) fuzzysets(low,high);;
variable
mfgCosts
datatype(double) range(0,48) getFrom(costs,MfgCost);;
variable
compPrice
datatype(double) range(0,48) fuzzysets(low,high)
getFrom(costs,CompCost);;
#
#--------------------------------------------------------
# These rules establish the new product pricing policy.
#--------------------------------------------------------
#
rule rule1 our price must be high;;
rule rule2 our price must be low;;
rule rule3 our price must be around 2*mfgCosts;;
rule rule4 if the compPrice is not very high
then our price should be around the compPrice;;
end pricingPolicy;;
Listing 5.1
The product pricing KB.
(it can be accessed outside the KB). MfgCosts (manufacturing costs) and
compPrice (competition price) are independent variables with associated
speciﬁcations on how to retrieve their data values. Each variable is a
double-precision ﬂoating point number.
Because rules connect the inputs and outputs (i.e., they transform the
dimensionality of the input space into a new dimensional space represent-
ing the outcome space), the amount of detail invested in the term sets
associated with each variable plays an important part in the precision of

116
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
the model. This granularity also plays an important part in the complexity
and corresponding execution speed of the rules. This brings us to another
property of the KB: its degree of granularity and speciﬁcity. These prop-
erties are related to the number of rules needed in a KB to represent a
particular problem space.
Rule Speciﬁcity and Growth Explosion
This problem space should be speciﬁc and reasonably bounded in order
to build an efﬁcient, robust, and focused KB system. This tends to be
particularly true of fuzzy systems, in that the rules are effectively run in
parallel and the number of rules increases very rapidly with the number
of total fuzzy sets. In fact, to cover the data space for N independent
variables the total rule count (Rc) is found by Expression 5.2.
Rc =
N

i=0
mvi
(5.2)
Here, mvi is the number of fuzzy sets for the i-th variable. The ﬁner the
level of fuzzy set granularity underlying each of the model variables, the
larger the number of rules necessary to cover the decision space. We can
see this more clearly with an extension of the risk tolerance model (see
Expression 5.3) to include a client’s annual income. In this case we want
to model the functional relationship.
yrisk = f (xage, zincome)
(5.3)
Here, risk tolerance is a function of the client age and income. As
Figure 5.14 shows, the independent variable Client_Income is decom-
posed into a number of fuzzy sets deﬁning a relationship between the
client income and the model builder’s perception of client wealth.
With the fuzzy sets associated with client age and client income
deﬁned, as well as our scaling for relative degree of risk, we can specify a
rule-based fuzzy model that predicts a client’s risk tolerance. Client_Age
has ﬁve underlying fuzzy sets and Client_Income has six fuzzy sets.
Applying Expression 5.4,
Rc =
N

i=0
mvi = 5 × 6 = 30,
(5.4)

5.4
A Fuzzy Knowledge Base: The Details
■
117
Grade of membership m(x)
Policy holder annual income ($ × 1000)
20
40
60
80
100 120 140
160 180 200
220
240 260
300 320 340
0
1.0
Low
Moderate
Median
High
Above
Median
Very
High
Wealthy
Client_Income
Figure 5.14
The term set for Client_Income.
we see that if all rules are expressed in the form
If Client_Age is X and Client_Income is Y then RiskTolerance is Z
a minimum of 30 rules is necessary to cover all fuzzy relationships in
the model. This can also be visualized as an M × N table, called a fuzzy
associative memory (FAM). Table 5.1 outlines the general organization of
the FAM for the risk tolerance model with a few of the possible rules.
As a measure of the exponential increase in rules with the dimension-
ality of the data, this equation is more applicable to control systems. In
business and technical models, the use of hedges, unconditional rule con-
straints, and default values often means that although the model builder
might decompose a variable into a number of fuzzy sets only a subset of
these linguistic variables is used in the rules.
Fuzzy Rule Execution
A KB is the representation of a problem. To derive a problem solution
the rules in the KB must be executed. Fuzzy rules are, in effect, executed
in parallel. This is due to the way the outcomes are constructed from

118
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
TABLE 5.1
The Risk Tolerance Fuzzy Associative Memory
Client_Age
Child
Teen
Young Adult
Middle Aged
Senior
Low
None
Low
Moderate
Low
None
Moderate
None
Moderate
Moderate
Low
None
Median
Low
Moderate
Moderate
Low
None
Client_
Income
Above Median
Low
High
High
Moderate
Low
High
Very High
Wealthy
each contributing rule. Before all rules for a speciﬁc outcome have been
executed, the outcome variable does not have a value. This means that
the outcome variable of one fuzzy rule cannot appear in the premise
of another rule in the same KB.2 For example, the two rules shown in
Figure 5.15 are not allowed.
The second rule cannot use weight in its premise because weight, as
an outcome variable, is still undergoing generation. This is the result of
the way in which evidence is collected and used in a fuzzy KB system.
Figure 5.16 shows the overall way in which rules are collected and then
executed to produce the outcomes for a KB.
if height is Tall then weight is Heavy;
if weight is Heavy then CardiacRisk is Elevated;
Figure 5.15
Improperly speciﬁed rules in a KB.
2 Unless, of course, a knowledge base is organized so that the dependencies between outcome
variables that are used as dependent variables in other rules are identiﬁed and handled by group-
ing and evaluating rules in a speciﬁc order. This can be done by dividing the KB into a collection
of connected Policies (or groups). In essence, however, we have simply created a collection of
smaller knowledge bases — the same prohibition in outcome variable used exists in these smaller
blocks.

5.5
The Fuzzy Inference Engine
■
119
Variable
A
Variable
B
Variable
C
Variable
D
Rule 5
Rule 3
Correlation
and
aggregation
Reduction
(defuzzification)
Rule 1
Rule 4
Rule 2
Variable
X
Fuzzy
term sets
Variable
Y
Outgoing
data
Incoming
data
Independent (and model) variables
Rule execution process
Fuzzy sets (variable terms)
Dependent  (outcome) variables
1.0
0
a
µ (x)
1.0
0
b
µ (x)
1.0
0
d
µ (x)
1.0
0
c
µ (x)
1.0
0
x
µ (x)
Under
generation
1.0
0
x
µ (x)
Fuzzy
term sets
1.0
0
y
µ (x)
Under
generation
1.0
0
y
µ (x)
Figure 5.16
Rule execution in a KB.
As Figure 5.16 illustrates, the rules are essentially run in parallel. The
consequent fuzzy sets are correlated with the truth of the rule premise,
and then this correlated (shape-modiﬁed) fuzzy set is aggregated with the
current under-generation set of the outcome variable. Finally, in most KB
processes each under-generation set is reduced to a single representative
value (a process known as defuzziﬁcation). The mechanism that con-
nects to a knowledge base, runs the rules, and produces all outcomes is
called the inference engine. The workings of the inference engine are the
subject of the next section.
5.5
The Fuzzy Inference Engine
A fuzzy inference engine consists of four main components: assessment,
correlation, aggregation, and reduction (or defuzziﬁcation). Together,

120
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Variable
A
Variable
B
Variable
C
Variable
D
Assessment
Aggregation
Correlation
Defuzzification
Variable
x
Rule
Fuzzy
term sets
Variable
y
Outgoing
data
Incoming
data
Independent (and model) variables
Rule execution process
Fuzzy sets (variable terms)
Dependent  (outcome) variables
10
0
a
(x)
10
0
b
(x)
10
0
d
(x)
10
0
c
(x)
10
0
x
(x)
Under
generation
10
0
x
(x)
Fuzzy
term sets
10
0
y
(x)
Under
generation
10
0
y
(x)
4
1
2
3
After all rules
For each rule
Figure 5.17
The fuzzy inference engine methods.
these facilities process all rules in the KB as if they were executed in
parallel. Figure 5.17 depicts further details of KB processing to expose
the inner workings of the inference engine.
➊Assessment
Assessment is the process of gathering initial evidence and determining
the truth of the rule antecedent. We assess the amount of evidence in a rule
by combining the individual fuzzy propositions in the premise according
to their connecting operators (And or Or). A fuzzy proposition such as
height is Tall produces a degree of membership (DOM) measure. This
is, of course, a measure of the amount of evidence that the value for

5.5
The Fuzzy Inference Engine
■
121
height is considered Tall. When the rule premise contains multiple fuzzy
propositions, they are combined with And and Or operators. An And
takes the minimum of the degrees of membership from two propositions,
whereas an Or takes the maximum of the degrees of membership. The
ﬁnal evaluation is a truth value measuring the degree of evidence for the
rule’s antecedent.
➋Correlation
Correlation matches the truth of the premise to the truth of the conse-
quent; that is, the truth of the current rule’s consequent cannot be any
greater than the implicit truth of the rule’s premise. Correlation takes the
form of either scaling or truncating the consequent using the premise’s
truth. As Expression 5.5 shows, scaling multiplies the consequent fuzzy
set’s membership function (A) by the premise’s degree of evidence, or
truth (E).
µA[xi] =
N
∀
i=0 µA[xi] × Ep
(5.5)
Truncation, on the other hand, reduces the height of the consequent
fuzzy set (A) by the limit imposed through the premise’s evidence (E).
Expression 5.6 shows how this function is applied.
µA[xi] =
N
∀
i=0 min(µA[xi], Ep)
(5.6)
In all cases, the correlation is done by reducing the truth in the conse-
quent based on the composite truth of the antecedent. This adjusts the
consequent using the amount of evidence in the premise.
➌Aggregation
Aggregation updates the current outcome fuzzy set using the correlated
consequent fuzzy set. Each rule, in turn, updates the outcome fuzzy set
with its own correlated fuzzy sets. There are two methods of aggregating
fuzzy sets into the outcome fuzzy set: addition and maximization. Expres-
sion 5.7 shows the additive method. The outcome (O) and correlated
consequent (A) fuzzy sets are added together.
µO[xi] =
N
∀
i=0 µO[xi] + µA[xi]
(5.7)

122
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
The maximization approach, shown in Expression 5.8, in effect takes the
union (logical-Or) of the outcome and correlated consequent fuzzy sets.
µO[xi] =
N
∀
i=0 max(µO[xi], µA[xi])
(5.8)
Aggregation is the actual heart of the fuzzy inference engine. Through this
process the evidence from each rule is collected and properly applied to
the outcome variable. Because every rule in the KB must be run before
all evidence is collected, the inference engine, in effect, runs all rules in
parallel.
➍Defuzziﬁcation
Defuzziﬁcation (the process of reduction) produces a single scalar result
from the ﬁnal outcome fuzzy set. There are many methods of defuzziﬁca-
tion. The most common is the centroid (or center of gravity), which is
essentially the weighted average of the outcome fuzzy set. Expression 5.9
shows how this is done.
x =
N
i=1 di × µdi
N
i=1 µdi
(5.9)
Further details on the application of the center-of-gravity method can be
found in “The Inverted Pendulum Model” section.
5.6
Inference Engine Approaches
There are two major inference techniques in fuzzy modeling. The ﬁrst,
and perhaps the oldest, is the approach developed in the late 1970s called
the Mamdani inference method. The second, developed in the late 1980s
and early 1990s by Bart Kosko, is called the Standard Additive Model
(SAM). Both of these methods will be illustrated in the section “Running a
Fuzzy Model.” However, the SAM has evolved as the conventional method
of exploiting fuzzy rules. The SAM is part of a general proof that fuzzy
systems are universal approximators; that is, they can approximate any
continuous mathematical function to an arbitrary degree of precision.
The Mamdani Inference Method
Perhaps ﬁrst formalized by Ebrahim Mamdani of King’s College, London,
in the early 1970s, the Mamdani method is one of the earliest working

5.6
Inference Engine Approaches
■
123
inference techniques. The fuzzy inferencing algorithm for applying this
technique is relatively simple and straightforward, and looks as follows.
Start:
Set Outcome Fuzzy Set (Fo) to Empty3
For Each Rule
Evaluate truth of the premise (Ep)
Truncate consequent fuzzy set at Ep giving Fc
If Outcome Fuzzy Set (Fo) is Empty
Set Fo = Fc
Else
Set Fo = max(Fo,Fc)
End For Each
Set Outcome Value (Vo) = defuzzify(Fo)
Goto Start
The Mamdani method is often called the min-max inference technique.
This name comes from the method of correlation and aggregation: the
consequent fuzzy set is truncated (or minimized ) to the truth of the
premise, whereas the outcome fuzzy set is set to the maximum of the
current outcome fuzzy set and the truncated consequent fuzzy set.
The Standard Additive Model
Professor Bart Kosko, in his inﬂuential and groundbreaking book Neural
Networks and Fuzzy Systems: A Dynamical Systems Approach to
Machine Intelligence, provides the mathematical foundations for the
SAM. The fuzzy inferencing algorithm for applying this technique, as
follows, is also relatively simple and straightforward.
Start:
Set Outcome Fuzzy Set (Fo) to Empty
For each Rule
Evaluate truth of the premise (Ep)
Scale consequent fuzzy set using Ep giving Fc
Set Fo = Add(Fo,Fc)
End For each
If max(Fo) > 1
Normalize(Fo)
Set Outcome Value (Vo) = defuzzify(Fo)
Goto Start
3 An empty fuzzy set is one with all of its membership values set to zero (0).

124
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
This method differs from the Mamdani approach in a few critical areas.
First, the consequent fuzzy set is scaled (using the truth of the premise)
rather than truncated. Second, the outcome fuzzy set is constructed by
adding the correlated consequent to the current outcome. Because the
outcome fuzzy set is created by adding together the consequent fuzzy set,
its membership values for a single domain value may exceed [1]. If you are
using the center-of-gravity defuzziﬁcation, you do not need to normalize
the outcome fuzzy set. Otherwise, you will have to normalize the outcome
fuzzy set to ensure that the membership values are in the range [0, 1].
5.7
Running a Fuzzy Model
Fuzzy models take measurements of the outside world, fuse these with
the current model state, and using a set of transformation rules produce
an estimate of a new model state. Data values from each independent
variable are used to ﬁnd and execute multiple rules in the model. These
rules combine possible states of the outcome (also called the dependent)
variable based on the degree of truth or evidence in each rule’s antecedent.
When all rules have been ﬁred, the ﬁnal outcome fuzzy region is reduced,
by a process called defuzziﬁcation, to a single value. The outcome value
changes the state of the model or plant. Sensors record the new state
and feed this data back into the fuzzy model. Figure 5.18 schematically
illustrates the fuzzy modeling cycle.
This generalized fuzzy model process works for control models and
for information decision models. Control models tend to be embedded in
hardware and work very rapidly in real time, whereas information models
generally work against corporate databases and spreadsheets and operate
at a much slower rate.
Regardless of whether the model is used in control engineering or
decision support, a fuzzy model works by collecting evidence for and
against the outcome variable’s solution state. Fuzzy models combine data
with the semantics of the input and output variables to generate a new
value for the output variable. How does this work?
In this section we will examine two fuzzy models in varying and
increasing levels of detail. The ﬁrst model is the inverted pendulum bal-
ancing controller (see “Fuzzy Associative Memories”) This is a classical
control engineering technique popularized by Dr. Bart Kosko, Dr. Fred
Watkins, and many of the early companies who pioneered fuzzy con-
trol systems (such as Togai InfraLogic and Aptronix). The second is a new
product pricing model employing many advanced features of fuzzy expert
systems.

5.7
Running a Fuzzy Model
■
125
Defuzzify
Plant
process
Domain
SN
ZR
SP
Fuzzy
knowledge
base
Outcome
(aggregated)
fuzzy set
Rules
if X is SN then Y is SP
if X is ZR then Y is ZR
if Z is SP then Y is SN
Variable
Variable
Variable
Data
Result
Sensors
1
0
Domain
Degree of
membership
Term set
Dependent
(outcome)
variable
Independent
variables
SN
ZR
ZR
SP
1
0
Variable
Figure 5.18
The generalized fuzzy model mechanism.
The Inverted Pendulum Model
Balancing an inverted pendulum is one of the typical exercises in control
engineering. The inverted pendulum is an inﬂexible shaft with a weight
at one end (the top) and a motor drive attached to the other (the bottom).
The motor can impart an angular momentum to the pendulum, causing it
to move to the left or to the right. Figure 5.19 illustrates the components
of an inverted pendulum plant.
The fuzzy controller measures the pendulum’s distance from the
vertical — the angle theta (θ) — and the rate at which the pendulum is
moving form the vertical [delta theta (θ)]. From these two measure-
ments one or more fuzzy if-then rules are ﬁred in the FAM. The rules
specify the amount and direction of angular momentum or motor force
(m) that should be supplied by the motor to bring the pendulum back
toward the vertical. Before we can actually write the rules for the inverted
pendulum controller, however, we need to create a collection of fuzzy
sets for each of the variables. Figure 5.20 shows these fuzzy sets.

126
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Rules
Fuzzy control
system
Motor control
Motor
Vertical
Delta
theta
(∆Θ)
Theta (Θ) t2
t1
Θ
∆Θ
Figure 5.19
The inverted pendulum.
In a fuzzy model, rules are written in terms of each variable’s fuzzy sets.
These fuzzy sets deﬁne the semantics of the variable’s UoD (its range of
permissible values). Thus, the selection of fuzzy sets and their organization
can have a profound effect on the nature and performance of the model.
A Review
Although the fuzzy sets in our inverted pendulum example have
generally the same shape, number, and organization, this is neither
required by the fuzzy model nor by the process of building produc-
tion models. Figure 5.21 shows an alternative representation of the
principal independent variables.
However, fuzzy shapes and structure cannot, generally, be arbi-
trarily selected. Rather, they must map to the underlying control
states in the model itself. In control engineering problems, such
as the inverted pendulum, the decomposition of the variables into
fuzzy sets follows a somewhat semantic-free approach: the vari-
ables are broken down into labeled fuzzy regions corresponding,
roughly, to degrees of change in the plant. The control engineer
simply decides on the number of fuzzy sets, thus determining the
number of possible rules and hence the level of granularity in the
controller. For business and other information decision models,
the fuzzy sets are more closely allied with the actual semantics of
the variable. The shape, number, and overlap of the fuzzy sets
correspond to real-world states of the model itself.

5.7
Running a Fuzzy Model
■
127
Degree of 
membership  µ (x)
Angle from vertical
−25
−15 −5
5
15
25
1
BN
SN
ZE
Theta
SP
BP
Independent variables
Dependent variable
0
Degree of 
membership µ (x)
Anglular momentum
−25
−15 −5
5
15
25
1
BN
SN
ZE
Delta theta
SP
BP
0
Degree of 
membership µ (x)
Angular momentum
−25
−15 −5
5
15
25
1
BN
SN
ZE
Motor force
SP
BP
0
Figure 5.20
Inverted pendulum controller fuzzy sets.
To see how the motor force value is derived, we return to the FAM for
the inverted pendulum. This matrix describes the rules for the controller.
Fuzzy sets are activated by ﬁnding the terms that have non-zero member-
ship values for the current independent variables. In our example, we
assume the following plant values.
-------------------
Theta ()
-----
10◦
-----
-------------------
Delta Theta ()
-----
0
-----
-------------------
As we can see from the fuzzy sets (Figure 5.20), these values overlap the
zero (ZE) and small positive (SP) regions of theta and lie completely within
the zero (ZE) region of delta theta. The highlighted region in Figure 5.22
shows which rules are candidates for ﬁring.

128
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Degree of 
membership µ (x)
Angle from vertical
–25
–15 –5
5
15
25
Independent variables
Theta
Delta theta
Motor force
Dependent variable
0
1
Degree of 
membership µ (x)
Angular Momentum
–25
–15 –5
5
15
25
0
Degree of 
membership µ (x)
Angular Momentum
–25
–15 –5
5
15
25
0
BN
SN
ZE
SP
BP
BN
SN
ZE
SP
BP
BN
SN
ZE
SP
BP
Figure 5.21
Alternative fuzzy set for the inverted pendulum FAM.
Theta (θ)
BN
SN
ZE 
SP
BP
BN
BP
SN
ZE
SP 
ZE 
ZE 
BP 
SP 
ZE 
SN
BN
SP 
ZE
SN 
ZE 
BP
BN
Delta
Theta
(∆θ)
Figure 5.22
The inverted pendulum 5 × 5 FAM.

5.7
Running a Fuzzy Model
■
129
Examining the FAM shows that the following two rules will be selected
for this set of values.
If Theta is SP and DeltaTheta is ZE then MotorForce is SN
If Theta is ZE and DeltaTheta is ZE then MotorForce is ZE
Theta’s value [10◦] a partial membership in two fuzzy sets: the zero (ZE)
fuzzy set (with a membership of [.47]) and the small positive (SP), with
a membership of [.68]. Delta theta’s value [0] has a membership of [1.0]
in only the zero (ZE) fuzzy set.
We now examine the way these rules are interpreted and the evidence
used to scale the outcome variable’s composite fuzzy set. This composite
(or aggregated) outcome fuzzy set is then defuzziﬁed to produce the new
MotorForce value.
Rule 1
If Theta is SP and DeltaTheta is ZE then MotorForce is SN
Figure 5.23 shows the outcome fuzzy set after the ﬁrst rule is executed.
The FAM combines the two antecedent propositions using the And
operator, thus taking the minimum of the two truth values, per the
following rule and rule membership table.
If (x) and (y) then (z)
Theta is SP
DeltaTheta is ZE
MotorForce is SN
----------------------------------------------------------------------------------
-----
-----
-----
-----
[.68]
[1.0]
[.68]
----------------------------------------------------------------------------------
Thus, the outcome fuzzy set small negative (SN ) is truncated at the [.68]
membership level and moved into the dependent variable’s outcome fuzzy
region.
Rule 2
If Theta is ZE and DeltaTheta is ZE then MotorForce is ZE
Figure 5.24 shows the outcome fuzzy set after the second rule is executed.
In this case, the zero (ZE ) fuzzy set is truncated at the [.47] member-
ship level and added to the outcome variable’s fuzzy set region. Although
we scale the fuzzy set using the minimum of the antecedent truth mem-
berships, when we update the outcome fuzzy reason for conditional rules

130
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Degree of 
membership µ (x)
Angle from vertical
–25
–15 –5
5
15
25
if Theta is SP and DeltaTheta is ZE then MotorForce is SN
Theta
Delta theta
Motor force (under-generation)
0
1
1
1
Degree of 
membership µ (x)
Angular momentum
–25
–15 –5
5
15
25
0
Angular momentum
–25 –15 –5
5
15
25
0
BN
SN
ZE
SP
BP
BN
SN
ZE
SP
BP
BN
SN
ZE
SP
BP
[1.0]
Min
[.68]
Figure 5.23
Firing the ﬁrst FAM rule (inverted pendulum).
(rules in the form if-then) we take the maximum of the fuzzy member-
ships (that is, we perform a union operation on the outcome fuzzy sets).
Figure 5.25 shows the ﬁnal outcome fuzzy region for the MotorForce
variable after both rules have been ﬁred.
At this point, we must ﬁnd a value for MotorForce using this fuzzy
region. The most common method of selecting a value is through the
centroid — or center-of-gravity (COG) — technique. The COG is the point
on the fuzzy set membership curve where the curve would be balanced.
Mathematically, this is a weighted average of the domain values covered by
the fuzzy region (weighted by the domain value’s degree of membership
in the fuzzy region). This is expressed as
x =
N
i=1 di × µdi
N
i=1 µdi
,
(5.10)

5.7
Running a Fuzzy Model
■
131
Degree of 
membership µ (x)
Angle from vertical
–25 –15 –5
5
15
25
if Theta is SP and DeltaTheta is ZE then MotorForce is SN
Theta
Delta theta
Motor force (under-generation)
0
1
1
1
Degree of 
membership µ (x)
Angular momentum
–25
–15 –5
5
15
25
0
Angular momentum
–25 –15 –5
5
15
25
0
BN
SN
ZE
SP
BP
BN
SN
ZE
SP
BP
BN
SN
ZE
SP
BP
Min
[1.0]
[.47]
Figure 5.24
Firing the second FAM rule (inverted pendulum).
where di is a domain value and µdi is the domain value’s membership
value. As Figure 5.26 illustrates, computing the COG of the output space
effectively smoothes the surface of the outcome fuzzy space before drop-
ping a plum line to the horizontal axis to ﬁnd the solution variable’s value.
Defuzzifying the outcome fuzzy set generates a MotorForce (m) value
of [−6.2]. This value, as the evidence suggests, is somewhere between
the maximum of small negative and zero. This value is fed back into the
motor controller, which adjusts the pendulum. New sensor readings are
made and another MotorForce value computed (see Figure 5.19 for the
general plant control cycle).
The inverted pendulum balancing controller is simply one class in
a large family of commercial fuzzy logic engineering applications. In a
production environment, fuzzy inference engines are incorporated in
microprocessor chips, connected to sensors and actuators, and embedded
in a wide range of machines. These fuzzy engines often cycle at high rates

132
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Grade of membership m(x)
Angular momentum
−25
−15
−5
15
15
25
0
1.0
BN
SN
ZE
Motor force
SP
BP
Figure 5.25
The MotorForce outcome fuzzy region.
Grade of membership m(x)
Angular momentum
−25
−15
−5
15
15
25
0
1.0
BN
SN
ZE
Motor force
SP
BP
Figure 5.26
Defuzzifying the MotorForce fuzzy region.

5.7
Running a Fuzzy Model
■
133
of speed. It is the overall simplicity coupled with engineering robustness
that gives fuzzy controllers such a wide range of applicability. Unfortu-
nately, when we move from balancing pendulums, controlling antilock
braking systems, and defrosting refrigerators to the everyday world of
business and industrial problems the simplicity of the FAM becomes a
severe handicap.
New Product Pricing Model
Let us suppose we are bringing a new product to market. What price
should we charge for the product? This is a classical problem in mul-
ticriteria decision making. We now present an actual fuzzy model that
sets the initial product price based on constraints imposed by an orga-
nization’s sales, marketing, manufacturing, and ﬁnancial divisions.2 The
model is deceptively simple and contains just four rules, as outlined in
the following table.
Rule Number
Rule
R1
Our price must be High.
R2
Our price must be Low.
R3
Our price must be about 2*MfgCosts.
R4
If the competition_ price is not very High
then our price must be near the competition_ price.
The pricing model introduces another important feature of fuzzy models:
the ability to bring together and fuse knowledge from multiple, often
conﬂicting, experts. In the real world of information decision modeling,
we seldom ﬁnd a set of experts that are in general agreement. This fusion,
illustrated in Figure 5.27, allows the knowledge engineer to write rules
that often have contradictory objectives (such as rules 1 and 2).
Unlike the inverted pendulum model, the new product pricing model
incorporates unconditional fuzzy rules, dynamically created fuzzy num-
bers, and the use of hedges to change the shape of existing fuzzy sets.
These features and concepts are discussed as each rule is processed.
We can also see that like nearly all real-world fuzzy systems in information
decision making this model cannot be expressed as a FAM. As Figure 5.28
2 The new product pricing model, along with C/C++ code that implements various forms of the
model, is also discussed in my book The Fuzzy Systems Handbook, Second Edition (Academic
Press Professional, 1999). The Visual Basic pricing model (pricing.vbp) and the fuzzy set image
display program (fzyimage.vbp) are included with the software for this book.

134
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Price
positioning
Expert
knowledge
Sales
In order to gain marketshare
  Our Price must be Low
Marketing
If the Competition’s Price is not
very High then our Price should
be near the Competition’s Price
Manufacturing
Our Price must be around
Twice The Manufacturing Costs
Finance
In order to cover costs
  Our Price must be High
$
$
Fuzzy
modeling
system
Price
Knowledge
engineer
?
?
? ?
?
Figure 5.27
The pricing model multiple peer fusion.
illustrates, the model does follow, however, our general architecture for
a fuzzy model.
There are only two vocabulary (or predeﬁned) fuzzy sets for the pricing
model, and these are associated with the outcome variable. These sets
declare the semantics of what we mean by High and Low pricing. As
it happens, the fuzzy sets are simply the complements of each other.
Figure 5.29 shows the Low and High fuzzy sets for price.
The remainder of the fuzzy information in the model is dynamically
created from the input parameters. In fact, the model only accepts two
parameters: the manufacturing costs and the competition’s price in the
same market. As we will see, these facts will be converted to fuzzy sets
using members of the approximation family of hedges.
A Review
Conditional Versus Unconditional Rules
Before we examine the pricing model, we will look at the way a fuzzy
model treats conditional and unconditional rules. Unlike the processing of
an unconditional statement in a conventional expert system, a fuzzy

5.7
Running a Fuzzy Model
■
135
Corporate
databases
Defuzzify
Domain
Fuzzy
knowledge
base
Rules
our price must be High
our price must be Low
our price must be around 
  2*MfgCosts
if Comp.Price is not very High
  then our price should be near 
  Comp.Price
Hedges
around
near
not
somewhat
very
MfgCosts
Price
Data
Competition
price
Result
1
0
Domain
Degree of
membership
Term set
Dependent
(outcome)
variable
Independent
variables
Low
High
1
0
Figure 5.28
The product pricing model.
Figure 5.29
The High and Low fuzzy sets for price.

136
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
unconditional statement (which looks like an assignment statement) is
treated in a completely different manner. A conditional rule takes the form
If premise then outcome.
This is the type of rule we use most often in fuzzy and conventional models.
When a conditional fuzzy rule is executed, the premise is evaluated to
determine its degree of truth. The degree of truth is used to truncate or
scale the consequent fuzzy set before it is added to the outcome fuzzy
regions. When the scaled fuzzy set is added to the outcome, it is done in
one of two ways (depending on the inference method): in the SAM, the
fuzzy set’s membership values are added to the outcome; in the Mamdami
model, the consequent and outcome regions are Or’d together by taking
the maximum of the membership values along each point in the domain.
An unconditional rule, on the other hand, takes the form
outcome.
These rules are statements of fuzzy relations using an unconditional
proposition. In essence (but not in principle), an unconditional rule is
only the consequent or outcome part of the more familiar conditional
rule. Because an unconditional rule does not have a premise, its truth
is always taken as [1.0]. Furthermore, unconditional rules are applied
to the outcome by Anding the current outcome fuzzy region and the
unconditional rule’s fuzzy set. We perform an And operation by taking the
minimum of the membership values along each point in the domain (unless
the outcome is empty, in which case we simply copy the consequent fuzzy
set into the outcome region).
Important: When you use unconditional rules, the fuzzy knowledge
base is no longer a declarative rule set. This means that the order of rule
execution is important and not determined by the inference engine. All
unconditional rules must be executed before any of the conditional rules.
The unconditional rules. The set of unconditional rules provides a default
support set for the model if none of the conditionals ﬁres (or if none of the
conditionals has more truth than the combined surface formed by all con-
ditionals). In control engineering applications, the entire control surface is
overlaid with fuzzy sets. However, in business models the very complex-
ity of the high-dimensional decision surface means that our rule set may
not deﬁne a value decision space for a set of input parameters. The con-
cept underlying the use of unconditional rules is explored more fully in the
pricing model.

5.7
Running a Fuzzy Model
■
137
To run the new product pricing model, a very simple KB has been
designed to hold the variable, fuzzy set, and rule deﬁnitions. The KB
(pricing.mkb) contains the following statements.
var price
float 8 32
fzy High
1
8 32
fzy Low
2
8 32
var compprice
float 8 32
var mfgcosts
float 4 15
r001 price must be High;
r002 price must be Low;
r003 price must be around 2*mfgcosts;
r004 if compprice is not very High
then price must be near compprice;
Listing 5.2
The price KB.
To execute the model, you start the Pricing Visual Basic application
(pricing.vbp). Running the model is a two-step operation. The ﬁrst step,
illustrated in Figure 5.30, involves loading and activating the KB (using
the Compile command).
Figure 5.30
Starting the fuzzy pricing model.

138
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Figure 5.31
Prompting for model parameter values.
The actual pricing model is executed by clicking on the Run com-
mand. When the model is running, a backward chaining process is used
to ﬁnd the values for the manufacturing costs (mfgcosts) and the current
competition price (compprice). Because no other rules supply a value
for these model parameters, a prompt dialog is used to ask for the val-
ues. Figure 5.31 illustrates this process (showing one of the prompting
dialogs).
Using the output from this model, we will now examine the way this
fuzzy KB combines evidence to arrive at a suggested price for the product.
Rule 1
Rule 1 is as follows.
Our Price must be High

5.7
Running a Fuzzy Model
■
139
Figure 5.32
Firing the ﬁrst pricing model rule.
This is an unconditional rule. Figure 5.32 shows the result of executing
the ﬁrst unconditional rule.
Because the outcome fuzzy region is empty, the High fuzzy set is
simply copied into the outcome fuzzy set. This essentially “primes the
pump” for the remainder of the unconditional. If we Anded the fuzzy set
High with an empty fuzzy set (whose membership values are all zero),
we would produce nothing but another empty fuzzy set.
Rule 2
Rule 2 is as follows.
Our Price must be Low
This is also an unconditional rule. Figure 5.33 shows the result of
executing the second rule.
When the second rule is executed, a new outcome fuzzy region is built
by taking the intersection of the Low fuzzy set and the current outcome
region (in this case, the fuzzy set High). As Figure 5.34 shows, the left-hand
segment of Low and the right-hand segment of High are truncated.
Thus, the outcome fuzzy region for Price is a triangular region formed
by the intersection of High and Low. If these were the only two rules in
the model, we could defuzzify the outcome and ﬁnd an estimated price
of $20.00.

140
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Figure 5.33
Firing the second pricing model rule.
Figure 5.34
The constrained outcome space.
Rule 3
Rule 3 is as follows.
Our Price must be about 2*MfgCosts
This is another unconditional rule. In applying rule 3 to the outcome, a
two-step process is required. First, we need to take the manufacturing

5.7
Running a Fuzzy Model
■
141
Figure 5.35
The fuzzy number About 2*MfgCosts.
Figure 5.36
Price outcome with fuzzy number overlaid.
costs and create a fuzzy set (a fuzzy number) that represents twice this
cost. This dynamically created fuzzy set is then applied to the outcome
region. Figure 5.35 shows the result of creating the fuzzy set about
2*MfgCosts when the manufacturing cost is $11.
As Figure 5.36 illustrates, the dynamically created fuzzy number rep-
resenting twice the manufacturing cost is added to the outcome solution
area. The preliminary ﬁgure shows the placement of the bell-shaped fuzzy
number along with the rules that contributed to each part of the outcome.

142
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Figure 5.37
Firing the third rule.
Rule 3 is unconditional, and thus the new fuzzy number is applied to
the outcome fuzzy set by taking the minimum of the membership values
along the possible range of values. Figure 5.37 shows the ﬁnal outcome
region after executing rule 3.
The initial three unconditional rules in the pricing model deﬁne a
default region for the price outcome value. If none of the conditional
rules executes (or executes very weakly), the ﬁnal price position will be
determined by defuzzifying this region. We now turn to the execution of
the model’s conditional rule.
Rule 4
Rule 4 is as follows.
If the competition_price is not very High
then our Price must be near the competition_price
A conditional fuzzy rule should already be familiar to you from the inverted
pendulum example, although in the pricing model the form is quite a
bit more complex. The ﬁrst step in executing the rule is evaluating the
rule’s premise condition. We need to determine the degree to which the
model variable, competition_ price, is not very High. Figure 5.38 shows
schematically how this is done.
As Figure 5.38 shows, hedges are applied in a speciﬁc order. The
hedge very (an intensiﬁer) is applied to the fuzzy set High, producing

5.7
Running a Fuzzy Model
■
143
not
1
0
10
30
20
Degree of
membership
very
1
0
10
30
20
Degree of
membership
High
Competion_price
1
0
10
30
20
Price
Price
)
(
Price
Degree of
membership
Not Very High
Very High
High
R4 if the competition_price is not very High
then our price must be near competition_price
Figure 5.38
Evaluating the premise of rule 4.
Figure 5.39
The fuzzy sets High and Very High.
a new fuzzy set Very High. Figure 5.39 shows how these fuzzy sets are
generated.
The hedge not (the complement operator) is then applied to the Very
High fuzzy set, producing the fuzzy set Not Very High. Figure 5.40 shows
how these fuzzy sets are generated.

144
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Figure 5.40
The fuzzy sets Very High and Not Very High.
Figure 5.41
Finding the degree of membership in Not Very High.
The value of competition_ price is then found in the Not Very High
fuzzy set (Figure 5.41), and its degree of membership is determined.4 This
degree of membership, [0.504], is the truth value for the rule’s premise.
4 Notice that hedges are applied to fuzzy sets according to rules much like rules of interpreting
modiﬁers in operation in the English language. Each hedge operation produces a new fuzzy set.
Thus, not very high and very not high are two distinct and completely different fuzzy sets.

5.7
Running a Fuzzy Model
■
145
Figure 5.42
About the competition price.
Figure 5.43
The correlated (minimum) competition price.
With the rule premise’s truth in hand, the consequent action is eval-
uated and applied to the outcome fuzzy set. The ﬁrst step applies the
approximation hedge about to the current competition price variable.
Figure 5.42 shows this bell-shaped fuzzy number.
After the consequent fuzzy set has been formed, it must be correlated
with the truth of the premise. Applying the maximum (or truncation)
method of correlation reduces the height of the About Competition_Price
fuzzy set to the maximum of the bell-shaped fuzzy number. Figure 5.43
shows the result of this correlation.

146
■
Chapter 5
Fundamental Concepts of Fuzzy Systems
Figure 5.44
Outcome price overlaid with competition price.
Figure 5.45
Final outcome fuzzy set for price.
Once correlation has been performed, the modiﬁed consequent fuzzy
set is added to the current outcome fuzzy set. Figure 5.44 shows in ﬁner
detail how the adjusted consequent fuzzy set is overlaid with the current
fuzzy set to represent the predicted (or possible) price.
The fourth rule is completely executed when the outcome fuzzy set is
updated with the correlated About Competition_Price value. The result-
ing outcome fuzzy set, shown in Figure 5.45, is used to actually derive the
predicted product pricing position through the process of defuzziﬁcation.

5.8
Review
■
147
We are using the centroid — or center-of-gravity (COG) — defuzziﬁ-
cation to ﬁnd the predicted product price. This involves ﬁnding the bal-
ance point on the membership curve and dropping a “plumb line” to
a value on the fuzzy set’s domain. The vertical line under the outcome
fuzzy set in Figure 5.45 is the point of defuzziﬁcation. When applied to
the outcome fuzzy set, the model returns a price of $24.13, with a degree
of evidence (the compatibility index) of [0.5039].
5.8
Review
Fuzzy systems replace the crispness of Boolean logic with the elasticity
of fuzzy logic. In this chapter we examined the principles underlying
fuzzy rule-based models to show how the use of fuzzy logic provides
a more ﬂexible, more robust, and less brittle approach to knowledge
management. We also compared two types of fuzzy systems: a control
system for balancing an inverted pendulum and a business system for
new product pricing. You should now be familiar with the following.
■
The syntax and meaning of fuzzy rules
■
The organization and purpose of a fuzzy knowledge base
■
The problem with exponentially increasing rules
■
How a fuzzy inference engine executes rules
■
How evidence is combined to produce an outcome
■
The techniques for reduction (or defuzziﬁcation)
Understanding how fuzzy sets are used in production fuzzy systems is a
crucial step in appreciating the ways fuzzy elements are used in a wide
range of related techniques and methodologies. In the next several chap-
ters we will look at fuzzy logic measures, as well as at fuzzy system
components embedded in database queries, clustering, and the discovery
of fuzzy rules in large and small databases. The ability to dynamically dis-
cover and measure the effectiveness of discovered rules is an important
component of machine learning and adaptive systems. In the next chapter
we begin this journey by turning our attention to the use of the Structured
Query Language (SQL) as a vehicle for the semantic interrogation of data
as well as for ﬁnding and quantifying patterns.

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 6
Fuzzy SQL and Intelligent
Queries
Nearly all model development (as well as data mining) projects begin with
an exploration of the underlying data space. With the exception of statisti-
cal analysis, the most common tool for data exploration is the Structured
Query Language (SQL) facilities associated with nearly all modern rela-
tional databases. In this chapter, we examine ways of improving SQL
requests using fuzzy forms of the where and having statements. By applying
fuzzy logic and approximate reasoning to the evaluation of record selec-
tion and clustering, we signiﬁcantly improve the robustness and utility of
database query operations. In this chapter, we will be primarily concerned
with the where statement and its various forms. We will also be examining
the way database columns are decomposed into fuzzy sets.
In the rush to apply advanced computational intelligence, clustering,
and factoring techniques to large databases, knowledge and data mining
engineers often bypass an important analytical tool: the SQL processor
associated with the database. SQL gives business analysts, knowledge
engineers, and others a powerful method for seeking out and exploiting
patterns in data. Although SQL does not necessarily discover patterns in
the same sense as rule induction or multidimensional clustering, as a tool
to augment the analyst’s exploration of data it has the ability to see through
database noise and expose relationships that are not readily visible. With
general ideas in hand about how the data should be organized and what
information or patterns are expected in the data, analysts can manipulate
data to verify that the data has these patterns and structure. The ability to
connect tables, ﬁlter records, and combine columns enables the explo-
ration of probable patterns in data. Thus, SQL is a tool for combining a
cognitive model with the actual data model. In a data mining, knowledge
discovery, and even ordinary business process, modeling this is a very
important capability and lies at the heart of any knowledge acquisition
project.
149

150
■
Chapter 6
Fuzzy SQL and Intelligent Queries
6.1
The Vocabulary of Relational Databases
and Queries
A complete discussion of relational database systems and the calculus of
relational queries is beyond the scope or intent of this chapter. Modern
relational databases are, in principal, simply a collection of rectangular
tables connected by the data they share. In that the fuzzy SQL approach
is designed to access and retrieve data from these tables, a basic under-
standing of the principal nomenclatures used in relational databases is
important. This preview of the vocabulary will make reading and under-
standing the material in this chapter much easier. It is not always possible,
while maintaining an uncluttered and coherent discussion of both the
ideas associated with database organization and queries against that orga-
nization, to ensure that every term is introduced before it is used. Many
of these terms are, of course, redeﬁned or deﬁned in more detail later in
this chapter.
Column
Each row in a table consists of one or more columns of data. A column
represents a unique class or category of data and has a unique name within
the table. A column also has a date type (numeric, string, Boolean, and
so on) and a domain (a restriction on the allowable value for the column,
which, in many but not all cases is implicit in the data type). In the
same way that rows are more or less the same as records in a ﬁle system,
columns are similar to ﬁelds in a record.
Composite Key
Every column in a relation is identiﬁed by its primary identiﬁer. This can be
a single value or a set of values. When the primary key consists of multiple
values it is called a composite key. Table 6.1, for example, illustrates the
composite key for a project precedence relationship table.
Each precedence relationship is deﬁned by two values: a base (From)
project identiﬁer and a successor (To) project identiﬁer. The table also
contains the type of relationship (in this case, a ﬁnish-to-start connec-
tion) and any lead or lag value that offsets the precedence relationship.
Figure 6.1 shows the critical path precedence network deﬁned by the
composite keys in this table.

6.1
The Vocabulary of Relational Databases and Queries
■
151
TABLE 6.1
Project Precedence Table
From-Project
To-Project
Type
Lead-Lag
A
B
FS
0
A
C
FS
0
B
D
FS
0
C
D
FS
0
D
E
FS
0
A
D
E
B
C
Figure 6.1
The precedence network from the composite keys.
A composite key uniquely identiﬁes a single row in the table.
Composite keys can consist of any number of columns, but the complete
set of columns must be speciﬁed to access the corresponding row.
Crisp Number
A crisp number is an ordinary number that has a single value (see the
fuzzy logic vocabulary in Chapter 4 for a discussion of crispness and
expectancy).
Domain
A column’s domain is its universe of allowable values. Unless otherwise
speciﬁed, it is usually associated with the column’s underlying data type
(numeric columns are limited to numbers, string columns can contain
text, Boolean columns contain true or false values, and so forth). A column
can also have an extended domain. When used in this sense, a domain is
an explicit constraint on a column’s value and takes the form of a lookup
table, an if-then-else rule, or an arithmetic or logical expression.

152
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Foreign Key
A column in one table that contains data values from the primary key of
another table is a foreign key. Table 6.2, for example, shows a projects
table. The table has a primary key (the project identiﬁer) and two foreign
keys.
The Department column contains values from the primary identiﬁer
of the departments table (see Table 6.3). The ProjectMgr column contains
values from the primary identiﬁer of the employees table (see Table 6.4).
TABLE 6.2
Projects Table
Project Id
Status
Duration
Department
ProjectMgr
P1
Active
190
MKTG
1077
P2
Active
30
IT
1633
P3
Pending
65
IT
1601
P3
Complete
205
ENGR
5809
Primary Key
Foreign Key
Foreign Key
TABLE 6.3
Departments Table
Department
DeptMgr
Site
ENGR
8932
East
IT
7734
East
MKTG
3077
West
Primary Key
Foreign Key
TABLE 6.4
Employees Table
Employee Id
Last Name
Salary
1077
Jones
32000
1633
Miller
43000
1601
Smith
25400
8932
Sommers
67050
Primary Key

6.1
The Vocabulary of Relational Databases and Queries
■
153
The DeptMgr column in the departments table is also a foreign key
because it contains employee ID values from the Employees table. The
connections made through foreign keys are one of the principal ways
relationships are established through the database. Foreign keys are also
important domain management mechanisms in the database (so that, as
an example, you cannot enter a department name in the projects table
that does not also exist in the departments table).
Fuzzy Number
A number whose value is “smeared” out over an underlying range of values
is called a fuzzy number. Fuzzy numbers are generally bell curves, but can
be triangles and trapezoids. A fuzzy number can be part of the variable’s
term set or it can be created by the application of an approximation hedge.
Index
Columns containing non-key data can be indexed for high-performance
retrieval and joins. An index is a multi-way tree that organizes the col-
umn’s data in a manner that allows individual data items or collections
of rows with the same data item to be found and retrieved very quickly.
In many database systems, foreign keys are automatically indexed. For
example, consider the Status column in the projects table (see Table 6.2).
If there are thousands of projects in the table, ﬁnding all active projects
would require searching the entire table, comparing the content of the
Status column with the string Complete. An index extracts each unique
value from the column, as well the set of all rows that have that value.
Thus, ﬁnding all complete projects by indexing the Status column is
extremely fast.
Join
The process of connecting two (or more) tables based on common data
is called a join. Joins are the primary mechanism in a relational database
for realizing structure in the data. This structure can be quite deep. A
simple example of a join, using the projects and departments tables (see
Tables 6.2 and 6.3), would be to answer a query such as “What projects are
being run out of the East site?” In this case, the projects and departments
tables are joined using the Department column in the projects table and
the Department column in the departments table. The conventional SQL

154
■
Chapter 6
Fuzzy SQL and Intelligent Queries
query to perform the join and then select those projects in the East site
looks as follows.
Select ProjectId, Site
From Projects,Departments
Where Projects.department = Departments.Department
And Departments.Site = “East”;
There are many types of joins. A join on the same data is called an equi-join
(an equal join). Relational query languages also support more advanced
types of joins, such as unions, outer joins, and inner joins — used to
connect and then create new tables based on the presence or absence of
data values.
Normal Form
The process of organizing data in a relation to remove redundancies and
eliminate update or deletion anomalies is called normalization. There are
three principal levels of normalization.1 The ﬁrst level is concerned with
the basics of how rows and columns appear in a relation. The last two
are basically concerned with eliminating redundancies and ensuring that
all columns are dependent on the primary key set. In ﬁrst normal form
(1NF), every row contains the same columns and every row and column
intersection contains a single value (that is, a column cannot be an array
or some other multivalued form). In second normal form (2NF) and
third normal form (3NF), all columns that are not fully dependent on the
primary key are removed and placed in their own relation.
Predicate
A predicate is a fuzzy or crisp expression in the where (or having) state-
ment that is evaluated to produce a truth value. Predicates determine
which rows in a table participate in a query. A query often contains many
predicates, as the following is an example.
Select projects
Where project_length is Long
And project_budget is High
And project_priority is Low;
1 Technically, there are ﬁve levels of normalization. However, in practice, placing a table in third
normal form is generally sufﬁcient.

6.1
The Vocabulary of Relational Databases and Queries
■
155
This is a query with three predicates in the where statement. The predi-
cates evaluate fuzzy relationships involving the project length (duration),
budget, and priority.
Primary Identiﬁer (or Primary Key)
Each row in a table must be identiﬁed by a unique key. This is the primary
identiﬁer. The identiﬁer can consist of one or more contiguous columns
(if it consists of more than one column, it is a composite key). In the
projects table (Table 6.2), the project ID is the primary identiﬁer. No
other row in the table can have the same value in this column. Note
that although usually considered a poor design it is possible for a table
to have more than one unique key (a project might also have a unique
budget reference number, for example), in which case the other unique
identiﬁers are called candidate keys.
Query Compatibility Index (QCIX)
The degree of compatibility with the semantic intent of a fuzzy SQL query
is called the query compatibility index (QCIX). Each returned row from a
query is assigned its own QCIX value. The fuzzy SQL query manager auto-
matically ranks the returned row set in descending order by the QCIX.
The QCIX reﬂects, in essence, the distance between the complete realiza-
tion of the query and the degree of semantic membership in the returned
row.
Relation (Table)
Data in the relational database is maintained in tabular form. When orga-
nized according to a set of precise rules, the tabular data is called a relation
(or relation table). In general use, however, the relations are simply
called tables. Each table consists of a ﬁxed number of columns and a
varying number of rows. Tables are connected through the join process,
and these connections establish the rich variety of relationships in the
relational model.
Row
Each table consists of one or more rows of data. A row consists of one
or more columns, and each row contains the same number of columns.
A row is more or less equivalent to a record in a ﬁle system. (See “Column”
for additional information.)

156
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Secondary Identiﬁer (or Key)
Any indexed column is known as a secondary identiﬁer. Secondary iden-
tiﬁers play an important role in data modeling, query, and organization.
6.2
Basic Relational Database Concepts
In the 1970s the concept of a relational database emerged as the standard
way of representing large quantities of complex information. Unlike the
previous generation of database systems, the relational model did not
rely on embedded pointers, links, and other internal structural ediﬁces
to maintain relationships between data elements. Instead of links and
pointers, a relational database creates its relationships through a shared
set of common data elements (that is, the data itself) carefully structured
(in a process called normalization) throughout the database.
The basic building block of a relational database is the table: a ﬁxed
set of columns and a varying number of rows. Each row and column
intersection contains a single data element. Figure 6.2 shows how these
tables are organized in a database.
Column
index
Column
index
Common data
Common data
Tables
(relations)
Columns
Rows
Key
(identifier)
Figure 6.2
Relational database tables.

6.2
Basic Relational Database Concepts
■
157
Allocations
(assignments)
Calendar
(availability)
Skills
inventory
Projects
database
Costing
Projects
Project
history
Precedence
relationships
Schedule
Staffing
Progress
updates
Events
Figure 6.3
The projects database structure.
Tables are connected through a process is called a join. Rows are
selected based on the content of their columns. To provide high-speed
joins, primary and secondary keys are indexed. This allows sets of records
containing a speciﬁc value to be retrieved without reading the entire
table. We can see how a relational database is organized by examining
a corporate information technology division’s projects database. Figure
6.3 shows the schematic representation of the various tables in the
database.
Focusing on a small segment of the database gives us some insight into
the ﬁne-grain details of the database organization at the table level. The
project table contains information about each active project. The stafﬁng
table contains information about each employee. Figure 6.4 shows the
structure of the database.
Each table is uniquely indexed on a primary key, ProjectId and Employ-
eeId, respectively. This unique index ensures that no duplicate records
are entered in the table. In addition to the table’s primary key, a table
can contain several secondary keys. A secondary key (which refers to the
primary key of another table) is called a foreign key. The project table
maintains an index of project managers, and the stafﬁng table maintains
an index of departments associated with each resource.
The database relationships are expressed in the data. The relationships
are actually realized through the join operation. For example, suppose we

158
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Primary
key index
Foreign
key index
Foreign
key index
Department
EmployeeId
ProjMgr
ProjectId
Primary
key index
ProjectId
ProjMgr
Budget
Duration
StaffLvl
NumEvents
RevCount
Priority
Projects
database
Projects
EmployeeId
JobTitle
Rate
Department
Staffing
Figure 6.4
The projects and stafﬁng table structure.
want to ﬁnd projects with budgets in excess of $80,000 that are managed
by project managers in the Engineering department. Figure 6.5 illustrates
the schematics of the underlying join.
A join between the projects and the stafﬁng tables connects the proper
stafﬁng record with each project manager, ensuring that we have the
ability to ﬁnd only those project managers in the Engineering department.
The result of a join is a new, physical table (using a temporary relation).
Forming a new table from a join, a process known as closure, ensures that
these new tables can be used like any other table in additional joins (some
of which are performed automatically by the database query manager
and may be invisible to the client). In fact, it is the task of a relational
database’s query language to manage the implicit relationships among
database tables.

6.3
Structured Query Language Fundamentals
■
159
ProjectId
Projects
database
Projects
EmployeeId
Staffing
<Temp>
ProjMgr
Budget
Duration
StaffLvl
NumEvents
RevCount
Priority
JobTitle
Rate
Department
ProjMgr
Budget
Department
Join
Budget
>80
Department
=“Engr”
Figure 6.5
Explicit joins between project and stafﬁng.
6.3
Structured Query Language Fundamentals
SQL evolved from the work of Edgar Codd and Chris Date as a standard
and uniform method of creating, accessing, and manipulating relational
databases. In SQL, the user tells the computer what columns they want,
which tables are involved, how the tables should be joined, and which
rows from the tables will participate in the query. The query language
also provides a clear method of performing joins among all speciﬁed
tables. SQL is a relational calculus approach to database access and
information management: it selects and organizes sets of records through
a high-level language that tells the system what records to select, not
how to select records. In this way it becomes a tool for expressing the
desired result of a database retrieval instead of the mechanics necessary
to achieve this result. Expressing the mechanics of a query is a relational
algebra process. For example, the following is an example of a relational
algebra query against project and stafﬁng tables.
define TotCost as double;
define ThisResCost as double;
open table projects, table staffing;
TotCost=0;

160
■
Chapter 6
Fuzzy SQL and Intelligent Queries
for each project find projects.dept = “ENGR”;
connect staffing via projects.projmgr=staffing.employeeid;
If connect.status==0 do;
If projects,budget > 40 then
compute ThisResCost=projects.duration*staffing.rate;
TotCost=TotCost+ThisResCost;
end if;
end if;
end for;
Most of the older database query languages took this approach. They
required their client to open tables, set up working variables, and explic-
itly navigate through the database (such as following links between
hierarchical relationships). SQL, the outgrowth of the initial SEQUEL
language of IBM’s System R database project, removed the client from
the mechanical view. This freed the user to focus on data management
and data analysis issues. As Figure 6.6 schematically illustrates, SQL
incorporates a variety of data management features.
The where statement performs a dual role of specifying how tables
should be joined and which rows from a table should be selected.
Thus, it ﬁlters and connects multiple tables in the database. For example,
the following SQL request shows a join and a ﬁlter.
select projectid,projects.duration*staffing.rate
from projects,staffimg
where projects.projmgr = staffing.employee
And projects.budget > 4000.00;
Group by
Filter
External
files
Spread
sheet
View
Table
Filter
Join
Sort
(order by)
Where statement
Data Sources
Structured
query
language
Figure 6.6
SQL data management features.

6.4
Precision and Accuracy
■
161
This request connects each project manager to his or her stafﬁng
(personnel) table entry so that the total management costs for each project
with a budget in excess of $4000 can be calculated (compare this to the
previous relational algebra request, which does the same thing).
On the other hand, the group by statement collects sets of records
according to one or more related columns. The associated having state-
ment functions like a ﬁlter on the group, indicating which records should
participate in the group. The following example illustrates the use of a
group by and having statement.
select projmgr
from projects
group by projectid Having count(*) > 1
This query selects all project managers who manage more than one
project. The having statement provides a way of applying a ﬁlter to an
entire collection of records.
An important characteristic of a relational query language is the idea
of closure. This means that the result of a query is a new table, not simply
a collection of rows. Using SQL we can connect any number of database
tables to produce a result. How the tables are connected and which rows
are selected are determined by the arithmetic, logical, and string compar-
isons found in the where statement. As criteria become more and more
complex, the set of records selected by the where statement becomes
more and more brittle.
Thus, as a component of any data mining engineer’s toolkit SQL
has several signiﬁcant limitations. In this chapter, we address one of
its principal analytical shortcomings: its lack of discrimination among
database records. The precise nature of an SQL request’s selection
and grouping of predicates, as we will see, lies at the heart of this
shortcoming.
6.4
Precision and Accuracy
We live in a world obsessed and infused with precision. We wear wrist
watches accurate to a hundredth of a second a year (often automatically
coupled to the atomic clock at the National Institute of Standards and
Technology (NIST). Ultra-high-density computer chips are made possible
by nearly nanometer-level placement of circuits on a silicate substrate.
Hunters, hikers, drivers, and, of course, the military use the Global Posi-
tioning System (GPS), which can locate individuals and troops within

162
■
Chapter 6
Fuzzy SQL and Intelligent Queries
a meter or so anywhere on earth. As a demonstration of manipulating
nanometer objects, IBM wrote its name with individual Xenon atoms.
Intelligent missiles speed near ground level across a thousand miles of
mountainous terrain to strike a single target in a crowded city. We can
calculate irrational numbers, such as pi, to over a billion decimal places.
Our space craft slingshot around the sun and rendezvous a year later with
a distant planet on a time schedule that is exact to a few hundredths of a
second. And changes in the economy are measured, and reported, down
to a tenth of a percent.
These are our models of precision — models that provide the gauge for
measuring other physical phenomena. We judge our weather forecasts,
for example, as highly imprecise (and consequently less accurate) because
they cannot predict the exact weather for this weekend. We believe sta-
tistical analysis is less precise than calculus. And we believe, above all
other things, that numbers are more precise than words. In our modern
analytical world, tempered by the rise of the scientiﬁc method, ﬁelds of
study that cannot reduce their axiomatic truths to sets of equations are
viewed with skepticism and placed outside the scope of the “hard” sci-
ences. And this type of drive for precision has been carried forward to our
tools for data analysis and knowledge discovery. It is, in fact, the often
unstated assumption that precision is a necessary requirement of truth
and rigor that has impeded the use of such technologies as fuzzy logic.
In this chapter, we look at ways imprecision can be made to work for us
in important and powerful ways when searching production databases.
In fact, we will see that a decrease in precision gives us an unexpected
increase in accuracy.
6.5
Why We Search Databases
It seems like a trivial question to ask why we search databases, yet it lies
at the heart of our need for precision and accuracy (as well as how we
measure both of these properties). The simple answer is: To retrieve data
we need to make a decision. The “we” can be either a person sitting at
a computer terminal or an application running on some computer. And
this simple answer is important. It links the process of data retrieval with
the cognitive process of decision making. In making a decision we want
to have all the facts. And we want the facts to be as accurate as possible.
In a perfect world, with perfect tools, we might expect complete access
to the supporting facts. In fact, however, due to the ambiguous and
imprecise nature of information actual “facts” are difﬁcult to discover.

6.5
Why We Search Databases
■
163
TABLE 6.5
The Project Table (Subset)
Project Id
Budget (×00)
Duration
StaffLvl
P01
55
108
29
P02
48
115
33
P03
57
90
10
P04
81
87
40
P05
78
92
16
P06
63
102
25
P07
91
125
41
P08
90
88
27
P09
65
117
37
P10
73
102
30
P11
93
93
45
P12
60
110
31
P13
59
121
37
P14
89
92
18
P15
51
101
23
Before we jump into the middle of how this dilemma can be resolved, we
should examine a relatively simple example to understand how and why
the problem exists in real databases and other information repositories.
Let us consider a few of the columns in the projects table, as shown in
Table 6.5. Suppose we need to ﬁnd all projects with a high budget and a
short duration. In a conventional query, we must decide on a clear set of
boundaries deﬁning what set of budgets are considered “high” and what
set of durations are considered “short.” In this case, we decide a budget
over $80,000 is high and a duration less than 90 days is short. The SQL
query is cast as follows.
Select ProjectNo, Budget, Duration
From Projects
Where Budget > 80 and Duration < 90;
The query selects the projects that have both a budget greater than
$80,000 and a duration less than 90 days. Table 6.6 shows the result
of this query.
The query meets the precise requirements of the selection criteria
expressed in the where statement. As Figure 6.7 shows, the query delimits
a decision space in the data.

164
■
Chapter 6
Fuzzy SQL and Intelligent Queries
TABLE 6.6
Selected Rows
Project Id
Budget (×00)
Duration
StaffLvl
P01
55
108
29
P02
48
115
33
P03
57
90
10
P04
81
87
40
P05
78
92
16
P06
63
102
25
P07
91
125
41
P08
90
88
27
P09
65
117
37
P10
73
102
30
P11
93
93
45
P12
60
110
31
P13
59
121
37
P14
89
92
18
P15
51
101
23
TABLE 6.7
Rows Matching Intent of Query
Project
Budget (x00)
Duration
StaffLvl
P01
55
108
29
P02
48
115
33
P03
57
90
10
P04
81
87
40
P05
78
92
16
P06
63
102
25
P07
91
125
41
P08
90
88
27
P09
65
117
37
P10
73
102
30
P11
93
93
45
P12
60
110
31
P13
59
121
37
P14
89
92
18
P15
51
101
23
However, the query is not very accurate in terms of our intended
objective. There are projects with budget and duration values that are
conceptually close to our ideas of high and short but are not selected by
the query. Table 6.7 illustrates some of these candidate rows.

6.6
Expanding the Query Scope
■
165
Duration
Budget
0
10
20
30
40
50
60
70
80
90
150
140
130
120
110
100
90
80
70
60
50
Budget >80
Duration <90
100
Figure 6.7
The Boolean High Budget and Short Duration space.
In a small database, such as our example, this is not a problem because
we can visually see other candidate records. But in large production
databases such a visual pattern recognition is not possible. Thus, how
can we improve our ability to ﬁnd the needed data? There are two ways:
by expanding the scope of our selection criteria or by changing the way
we perform our query against the database. We will look at each in turn.
6.6
Expanding the Query Scope
One intuitively obvious way of enriching the amount of data retrieved
from the database is by expanding the scope of our query. This means
changing the selection criteria so that they encompass more data. As an

166
■
Chapter 6
Fuzzy SQL and Intelligent Queries
example, in our previous query we might extend the selection criteria as
follows.
Select ProjectNo, Budget, Duration
From Projects
Where Budget > 75 and Duration < 100
This moves the budget limit down to projects with a budget greater
than $75,000 and moves the duration threshold up to durations less than
100 days. Indeed, as Table 6.8 shows, we now pick up the rows shown in
Table 6.7. This seems to solve the problem. We simply have to broaden
the search criteria. As Figure 6.8 shows, this expansion neatly encloses
several of the neighboring data points.
But extending the search space introduces several additional prob-
lems. First, we have diluted the meaning of the query in order to capture
these adjacent records. Second, we have no way of telling which addi-
tional records are closely associated with the intent of our query and
which are, essentially, “satellite” records. And third, of course, we now
miss records that are near our new boundaries. Table 6.9 shows some
candidate neighboring records (P07 and P10) that might be included in
the new selection space.
TABLE 6.8
Rows Selected by Expanding Selection Criteria Scope
ProjectId
Budget (×00)
Duration
StaffLvl
P01
55
108
29
P02
48
115
33
P03
57
90
10
P04
81
87
40
P05
78
92
16
P06
63
102
25
P07
91
125
41
P08
90
88
27
P09
65
117
37
P10
73
102
30
P11
93
93
45
P12
60
110
31
P13
59
121
37
P14
89
92
18
P15
51
101
23

6.6
Expanding the Query Scope
■
167
Duration
Budget
0
10
20
30
40
50
60
70
80
90
150
140
130
120
110
100
90
80
70
60
50
100
Budget >80
Duration <90
Figure 6.8
The extended Boolean decision space.
We now begin to see the futility of extending the scope of the selec-
tion criteria. More records from the database are selected, but we have
completely lost the accuracy of our query. The pool of returned records
includes both those that meet our criteria of high budget and short
duration and many others selected simply to catch information along the
“edge” of the decision space. In a simple two-predicate query against a
small table of data, the intrinsic brittleness of Boolean selection criteria
is not often apparent (in that we can visually see adjacent records and
discern patterns). But against a large complex database, we lose this abil-
ity to see which records out of several hundred belong to the core set of
desired records. For example, Table 6.10 outlines a projects table with
some additional columns.
In assessing project risk or anticipating project bottlenecks, we want
to ﬁnd projects with “a high budget, short duration, moderate to large
stafﬁng, fair complexity (at least 45 events or tasks), previous revisions to

168
■
Chapter 6
Fuzzy SQL and Intelligent Queries
TABLE 6.9
Neighboring Records in the Extended Scope
Project Id
Budget (×00)
Duration
StaffLvl
P01
55
108
29
P02
48
115
33
P03
57
90
10
P04
81
87
40
P05
78
92
16
P06
63
102
25
P07
91
125
41
P08
90
88
27
P09
65
117
37
P10
73
102
30
P11
93
93
45
P12
60
110
31
P13
59
121
37
P14
89
92
18
P15
51
101
23
TABLE 6.10
Project Table with Additional Columns
Project Budget (×00) Duration StaffLvl Num Events Rev Count Priority
P01
55
108
29
14
0
3
P02
48
115
33
77
3
1
P03
57
90
10
22
0
1
P04
81
87
40
31
1
5
P05
78
92
16
49
5
0
the project plan, and high to moderate priority.” Such a query might look
as follows.
Select *
From Projects
Where Budget > 80
and Duration < 90
and Staff > 30
and NumEvents > 45
and RevCount > 2
and Priority < 5

6.7
Fuzzy Query Fundamentals
■
169
Applied against the project operational data store or (in a data mining
exercise) against a database of historical projects, this query may retrieve
many hundreds of records. Along each dimension of the where statement,
there will be values that are close to the cutoff threshold. To ensure that
we capture a robust set of records we might relax our selection criteria
somewhat to capture a portion of these nearby records. However, given
the set of selected records there is no effective way to tell which records
are strongly compatible with the meaning of our query and which are
only weakly compatible.
The question is, how do we go about making a database query that
improves the accuracy of our results? The problem lies in how we express
our query. Instead of simply changing the boundary conditions in the
where statement, we need to change the way in which the where state-
ment is evaluated. More to the point, the Boolean or crisp predicates are
replaced with fuzzy predicates. Making this change has some far-reaching
and important consequences. We now turn our attention to some funda-
mental principles of the relational database before addressing the issues
associated with fusing fuzzy logic and the relational query language.
6.7
Fuzzy Query Fundamentals
A fuzzy SQL request introduces a new way of retrieving information from
a relational database. Using this query facility, you tell the computer what
you want based on concepts rather than numbers and text strings. These
concepts are sometimes vague and often imprecise. For example, you
can look for risky projects in the corporate project database using a query
such as the following.
select *
from projects
where projects.budget is High
and projects.duration is Short
With fuzzy SQL, terms such as High replace the more conventional (and
restrictive) method of specifying a range of budget values, such as the
following.
project.budget >= 40 and project.budget =< 80
The same type of conceptually based query is used across a wide spectrum
of public, corporate, governmental, and military databases. For example,

170
■
Chapter 6
Fuzzy SQL and Intelligent Queries
a military query against division-level resources might take the following
form.
select battalionid
from imemilorg.divmrrpt
where toe_staffing is complete
and mr_availability is near toe.staffing
and transport_redline is low
This query accesses the materiel readiness report to ﬁnd battalions within
a division for which TO&E (table of organization and equipment) stafﬁng
is complete, the morning report availability is close to the stafﬁng level
(nearly everyone is present), and the number of vehicles on redline (in
for repair) is low.
The idea behind a fuzzy query process is simply this: you tell the
computer what you want in terms that mean something to you. These
terms, expressed as the names of fuzzy sets, deﬁne the semantics of
a query rather than the mechanics of a query. By expressing your
thoughts through semantics rather than arithmetic, Boolean, and com-
parison operators, you can ﬁnd information that expresses the intent
of your query. In a while we will see why this new way of qualify-
ing records from a database is not only more natural but much more
powerful and comprehensive. But ﬁrst, we brieﬂy review the idea of
fuzzy sets in the context of query requests (the building blocks of fuzzy
SQL) and see how they work and how they are used in a basic query
process.
Understanding Fuzzy Query Predicates
Most of us are familiar with conventional logic. This is often called Boolean
logic after George Boole, the nineteenth-century cleric and mathematician
who formalized its rules of logic. The core of both Boolean and fuzzy logic
is the idea of a set. A set is a collection of things, both real and imaginary.
The big difference between the two is simply this: in Boolean logic a thing
is either a member of a set or it is not a member of the set. In fuzzy logic,
things can have a partial membership in a set. As we will shortly see, such
partial memberships have far-reaching implications. A rather easy way to
see the difference between Boolean sets (which we often call crisp sets)
and fuzzy sets is by examining the concept of Tall. In conventional set
membership, we must specify a boundary point: individuals with heights
above the boundary are Tall and those below the boundary are not Tall.

6.7
Fuzzy Query Fundamentals
■
171
Suppose we select 6 feet as the boundary. Expression 6.1 shows how the
set is deﬁned.
µTall = {height ≥6}
(6.1)
As a reminder, throughout the chapter we continue to use the Greek
µ (mu) to indicate a set membership. Figure 6.9 shows what this set
looks like in crisp form.
Grade of membership m(x)
4
4.5
5
Height (in feet)
5.5
6.0
6.5
7
0
1.0
Tall
Figure 6.9
The crisp Tall deﬁnition.
We can see that the most obvious characteristic of the Boolean set is
the sharp boundary between the items not in the set and those that are
members of the set. Thus, someone ﬁve and one-half feet in height is
not Tall, whereas someone six feet three inches is a member of Tall. This
sharp boundary also introduces some artiﬁcial classiﬁcation constraints
on the logic that uses the set. These constraints are most pronounced in
real-world applications in which the boundaries between states of some
value are continuous. In the case of Tall, this means that someone ﬁve
feet eleven and seven-eighths inches is not Tall. However, by adding one-
eighth of an inch the set membership immediately shifts into the Tall set.
From a database selection viewpoint, this can lead to some signiﬁcant
problems. Consider a (hypothetical) retailer’s store database. Figure 6.10
schematically illustrates how this database might be organized.
The marketing department is using the information to ﬁnd prospects
for a sale of special items. They want to ﬁnd Tall, Heavy, and Middle-aged

172
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Name
Height
Weight
Sex
Age
Residence
Retail
store
database
Customers
Name
Date
Purchases
Item/SKU
Quantity
Price
Item/SKU
Vendor
Qty on-hand
Back ordered
Cost
Stock
Figure 6.10
The store database.
men who have bought expensive suits in the past few months (for a sale of
large suits in a conservative cut). Table 6.11 shows part of the customers
table in this database. If we use the Boolean deﬁnition of Tall, only indi-
viduals that are six feet in height or over are selected. The corresponding
SQL query would look as follows.
select name
from ieh.customers
where customer.height >= 6;
Only two individuals are selected from the customers table. Table 6.12
shows these two individuals. A quick review of the Height column shows,

6.7
Fuzzy Query Fundamentals
■
173
TABLE 6.11
The Customers Table (Partial)
Name
Height
Weight
Age
Jackson
5’3”
170
38
Sanders
6’2”
215
52
Miller
5’11”
157
25
Cassey
6’1”
188
40
O’Malley
5’5”
163
48
Freeman
5’10”
202
44
TABLE 6.12
Tall customers (Boolean Criteria)
Name
Height
Weight
Age
Tall
Jackson
5’3”
170
38
Sanders
6’2”
215
52
Miller
5’11”
157
25
Cassey
6’1”
188
40
O’Malley
5’5”
163
48
Freeman
5’10”
202
44
nevertheless, that some of these customers (such as Miller and Freeman)
are very close to being categorized as Tall. But this is the penalty we pay
for using crisp logic in our selection criteria. Naturally, we could change
our deﬁnition of Tall by lowering the boundary between Tall and not Tall.
In the case in which we have only a few outlier records (as in the previous
example), this might be an acceptable approach, but as a general solution
it will not work. Changing the boundary points on our query changes the
meaning of the query and introduces records whose relationship to the
idea behind our query is unknown (see the section “Expanding the Query
Scope”). What we need is a better way of expressing the concept of Tall.
This brings us to the idea of a fuzzy set.
A fuzzy set does not represent fuzzy data; it is simply a method of
encoding and using knowledge that does not have clearly deﬁned bound-
aries. Many of the phenomena we encounter every day fall into this class.
Tall is one example. Other concepts — such as heavy, high, low, fast,
slow, full, empty, many, few, light, and dark — are often associated with
variables that have ill-deﬁned boundaries. A fuzzy set maps a value in the
data to its degree of membership in a set of ill-deﬁned elements. To see
this, let’s look at a fuzzy set that might deﬁne the idea of Tall. Figure 6.11
shows this fuzzy set deﬁnition.

174
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Tall
Figure 6.11
The fuzzy Tall deﬁnition.
Here we have a completely different type of representation for Tall.
The line moving diagonally up and across the height values is the mem-
bership function for the fuzzy set. It associates a height value with a
degree of membership. Figure 6.12 shows how the value of ﬁve feet has
a membership of [0.43] in the fuzzy set Tall.
What does a number such as [0.43] mean? It means that a height of ﬁve
feet is a moderate member of the fuzzy set Tall. We can look at member-
ship values as a measure of compatibility or a measure of representation.
They answer questions such as the following.
■
How compatible is ﬁve feet with the concept Tall?
■
To what degree is ﬁve feet representative of the concept Tall?
Thus, a fuzzy set takes a concept, such as Tall, and expresses it through a
mapping of data to membership functions. The magnitude of the number
reﬂects how well an item matches the semantics (or meaning) of the fuzzy
set. Using this new tool, we can now ﬁnd Tall customers with a slight
change in our query statement.
select name
from ieh.customers
where customers.height is Tall;

6.7
Fuzzy Query Fundamentals
■
175
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
µ[0.43]
1.0
Tall
Figure 6.12
Fuzzy memberships.
TABLE 6.13
Tall Customers per QCIX
Name
Height
Weight
Age
µ(Tall)
Sanders
6’2”
215
52
.91
Cassey
6’1”
188
40
.90
Miller
5’11”
157
25
.87
Freeman
5’10”
202
34
.83
O’Malley
5’5”
163
48
.62
Jackson
5’3”
170
38
.53
Each value of customers.height is located in the fuzzy set Tall, and
its degree of membership is returned. This type of query measures the
degree to which each customer’s height is a member of the Tall fuzzy set.
This degree of membership, called the QCIX, is used to rank the selected
records according to how well they agree with our request (we discuss this
in more detail in the section “Measuring Query Compatibility”). Table 6.13
shows the result of this query.
The resulting table is sorted in descending order by the compatibility
index, which in this case is the membership of Height in the fuzzy set
Tall. Because our deﬁnition of Tall covers the entire possible domain of
interested heights and is linear (a straight line), each record will be graded
according to its membership. You can ﬁne-tune the deﬁnition of Tall by

176
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Tall
Figure 6.13
Redeﬁning the Tall fuzzy set.
changing its range of values or the shape of its membership function.
Figure 6.13 illustrates a possible reorganization of the set using an S-curve
rather than a linear curve (the underlying linear curve is shown in the
background).
In this version of Tall we have moved the curve to the right and
changed it into an S-curve (also known as a growth curve). Membership
values fall off rapidly below the “inﬂexion” point. Table 6.14 shows the
result of the same fuzzy SQL query using the new fuzzy set. Note the
change in membership values.
This new set of membership functions brings us to an important fact
about fuzzy sets. They are not universal descriptors. They are dependent
on your data and on your own, sometimes subjective, judgment about the
TABLE 6.14
Tall Customers per New Fuzzy Set
Name
Height
Weight
Age
µ(Tall)
Sanders
6’2”
215
52
.90
Cassey
6’1”
188
40
.88
Miller
5’11”
157
25
.79
Freeman
5’10”
202
34
.67
O’Malley
5’5”
163
48
.13
Jackson
5’3”
170
38
.00

6.7
Fuzzy Query Fundamentals
■
177
way a concept is expressed in the data. The shape of a fuzzy set reﬂects
the meaning of the concept. This is an important notion: fuzzy sets couple
the semantics of your request to the actual mechanics of the request.
Semantic, si-man’tik, adj. Relating to meaning, esp. of words – n. (usu. pl.
treated as sing.) the science of the development of the meaning of words.
Pertaining to the actual meaning of words. [Gr. σεµαντικoζ, signiﬁcant].
As a consequence, we are able to state our queries in a form that means
something to both us and the computer. But ﬁrst we have to tell the com-
puter what we mean by such terms as Tall. This meaning is expressed
in terms of the fuzzy set membership functions. Changing the member-
ship function changes what we mean when we issue a query. Figure 6.14
illustrates this by showing four different but plausible versions of the Tall
fuzzy set.
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Tall
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Tall
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Tall
Grade of membership m(x)
Height (in feet)
4
4.5
5
5.5
6.0
6.5
7
0
1.0
Tall
Figure 6.14
Different forms of the Tall fuzzy set.

178
■
Chapter 6
Fuzzy SQL and Intelligent Queries
When you design a fuzzy set in the fuzzy SQL system you must ask
yourself how data values in the table (or spreadsheet) are related to your
semantic concept. However, it is important to realize that this mapping
need not be extremely exact. As the membership functions in Figure 6.11
illustrate, the general shape of the curve is sufﬁcient to make the con-
nection. The ﬁnal fuzzy sets, naturally, may need to “tune” your function
after selecting and examining sets of data.
Combining Fuzzy Sets
The real potential of fuzzy queries becomes apparent when we move from
one fuzzy set to queries that combine many fuzzy sets. With multiple fuzzy
sets we want to ﬁnd candidates in the data that have, to some degree,
characteristics of our combined concepts. Let’s look at the query for Tall
and Heavy men. In our crisp query we need to establish a boundary point
for the deﬁnition of Heavy. Suppose we select 195 pounds. Then the SQL
query is as follows.
select name
from ieh.customers
where customers.height >= 6
and customers.weight >= 195;
The result of this query is reﬂected in Table 6.15. Only a single customer
is both Tall and Heavy.
But if we glance down the table, it appears that several of the customers
might be potential candidates. They are excluded simply because of the
sharp boundary points imposed by the crisp SQL query. We can do better
by replacing the query with its corresponding fuzzy version, as follows.
select name
from ieh.customers
where customers.height is Tall
and customers.weight is Heavy;
The intent of this query is obvious: we want customers that are both Tall
and Heavy. To resolve the query we need to tell the computer what con-
stitutes a Heavy person. Figure 6.15 shows one possible way to represent
this concept.
When we have a value from the customer’s Weight column we can
ﬁnd that value’s membership in the Heavy fuzzy set. Table 6.16 outlines
the membership values for each of the weights in the table (ranked by
their membership values). Our fuzzy query has identiﬁed three customers

6.7
Fuzzy Query Fundamentals
■
179
TABLE 6.15
Tall and Heavy Customers (Crisp)
Name
Height
Weight
Age
(Tall and Heavy)
Jackson
5’3”
170
38
Sanders
6’2”
215
52
Miller
5’11”
157
25
Cassey
6’1”
188
40
O’Malley
5’5”
163
48
Freeman
5’10”
202
34
Grade of membership m(x)
Weight (in pounds)
180
185
190
195
200
205
215
210
0
1.0
Heavy
Figure 6.15
The Heavy fuzzy set.
TABLE 6.16
Tall and Heavy Customers
Name
Height
Weight
Age
µ(Heavy)
µ(Tall)
Sanders
6’2”
215
52
.9
.91
Freeman
5’10”
202
34
.55
.83
Cassey
6’1”
188
40
.25
.90
Miller
5’11”
157
25
.00
.87
O’Malley
5’5”
163
48
.00
.62
Jackson
5’3”
170
38
.00
.53

180
■
Chapter 6
Fuzzy SQL and Intelligent Queries
(Sanders, Freeman, and Cassey) who have memberships in both the Tall
and Heavy fuzzy sets. These memberships express a degree of com-
patibility between the intent of the query and the actual results. In
the next section we take up the issues associated with quantifying this
compatibility.
6.8
Measuring Query Compatibility
We now need some way to combine the membership functions so that
the total effect of the query can be expressed and ranked. The ﬁnal result
of evaluating a fuzzy where statement is the aggregation of each pred-
icate’s truth value. As we have seen, a typical SQL request speciﬁes a
set of columns from tables that meet a set of ﬁltering and connection
requirements.
select column1, column2, ..., columnn
from table
where predicate1 AND predicate2 ... AND predicaten
Each predicate (qualiﬁer) in the form deﬁned by Expression 6.2,
x is [not] X,
(6.2)
involves a mapping of a value in the column (x) into one of the fuzzy sets
deﬁned for that column. This mapping returns a value in the interval [0,1],
which is the degree of membership for that value in the fuzzy set. This
degree of membership, as we discussed previously, indicates a degree of
compatibility between the data value and the semantics imposed by the
fuzzy set. Figure 6.16 shows the underlying truth-functional nature of a
query predicate.
When a fuzzy query where statement contains multiple predicates,
the aggregation or fusion of the predicates’ truth values indicates a mea-
surement of how well the query matched the intent of our query. This
is called the CQIX, a major departure from the way conventional SQL
queries treat retrieved data. Each record (or row) retrieved from a rela-
tional database with a fuzzy SQL query has an associated compatibility
index. Records are returned in a descending-order ranking of the com-
patibility value (most compatible record ﬁrst). This compatibility index is
stored in a new column and can be used just like any of the other table
columns. There are three broad methods of combining results to produce
a compatibility ranking: minimum of memberships, weighted average of
memberships, and weighted product of memberships.

6.8
Measuring Query Compatibility
■
181
Grade of membership m(x)
Weight (in pounds)
180
185
190
195
200
205
215
210
0
µ(x)
X
1.0
Light
Heavy
Weight
Figure 6.16
The truth-functional nature of a predicate.
Minimum-of-Memberships Compatibility
The simplest method of calculating the overall compatibility index for a
query Xi takes the minimum of the predicate truth memberships (this
assumes, of course, that the predicates are connected with an And opera-
tor). Expression 6.3 shows how this QCIX is computed for the complete
query.
Xqcix =
n
∀
i=1 min(µi(pi))
(6.3)
Here,
Xqcix
is the query compatibility index for query X
n
is the number of evaluated predicates in the query
µi( )
is a function that returns the minimum membership value
pi
is the i-th predicate in the query
The minimum approach functions like the traditional And operator. Each
query predicate is treated as an expression that is dependent on the
complete set of predicates. Table 6.17 illustrates how the minimum-of-
memberships value works.

182
■
Chapter 6
Fuzzy SQL and Intelligent Queries
TABLE 6.17
Minimum of Tall and Heavy Customers
Minimum
Name
Height
Weight
Age
µ(Heavy)
µ(Tall)
µ(Heavy, Tall)
Sanders
6’2”
215
52
.95
.91
.91
Freeman
5’10”
202
34
.55
.83
.55
Cassey
6’1”
188
40
.25
.90
.25
Miller
5’11”
157
25
.00
.87
.00
O’Malley
5’5”
163
48
.00
.62
.00
Jackson
5’3”
170
38
.00
.53
.00
As you can see, the minimum of the membership values constrains the
solution to the weakest element in the data. This means that all where
statement And conditions must be true (or have some degree of truth)
before the entire SQL statement is true. Thus, minimum compatibility
measurements are highly sensitive to a single zero membership predicate
(this includes predicate memberships that are zero because the original
membership value fell below the query’s alpha cut threshold).
Weighted-Average-of-Memberships
Compatibility
A more robust method of viewing the compatibility measurement for
query Xi takes the weighted average of the predicate truth values. In this
approach, each column is given a bias weight in the interval [1,n], where
n is a large number relative to all weights greater than 1. Expression 6.4
shows how the weighted average compatibility index is computed.
Xqcix =
n
i=1 (µi(pi)) × wi
n
i=1 wi
(6.4)
Here,
Xqcix
is the query compatibility index for query X
n
is the number of evaluated predicates in the query
µi( )
is a function that returns the minimum membership value
pi
is the i-th predicate in the query
wi
is the contribution weight associated with the i-th predicate
When the weights for columns are ignored (each column weight is one
[1]), Expression 6.4 shows that the QCIX is computed as the ordinary

6.8
Measuring Query Compatibility
■
183
TABLE 6.18
Average of Tall and Heavy Customers
Average
Name
Height
Weight
Age
µ(Heavy)
µ(Tall)
µ(Heavy, Tall)
Sanders
6’2”
215
52
.95
.91
.930
Freeman
5’10”
202
34
.55
.83
.690
Cassey
6’1”
188
40
.25
.90
.575
Miller
5’11”
157
25
.00
.87
.435
O’Malley
5’5”
163
48
.00
.62
.310
Jackson
5’3”
170
38
.00
.53
.265
average for the complete query. The average membership approach com-
putes the mean of the entire memberships values. This is reﬂected in
Table 6.18.
The averaging method balances the truth membership functions across
each of the where statement predicates. Because it is an average, a low
and high membership will produce a moderate truth value. However,
as the last three rows in Table 6.18 illustrate, the average method often
produces a signiﬁcant truth result even when one of the selection criteria
has a zero truth membership.
Weighted-Product-of-Membership Compatibility
A method that naturally biases the overall compatibility for query Xi
toward the weakest truth memberships, but does not automatically set-
tle on the single predicate with the minimum truth, takes the weighted
product of the predicate truth values. In this approach, like the weighted
average, each column is given a bias weight in the interval [1,n], where
n is a large number relative to all weights greater than 1. Expression 6.5
shows how the weighted average compatibility index is computed.
Xqcix =
n

i=1
min(µi(pi)) × wi
(6.5)
Here,
Xqcix
is the query compatibility index for query X
n
is the number of evaluated predicates in the query
µi( )
is a function that returns the minimum membership value
pi
is the i-th predicate in the query
wi
is the contribution weight associated with the i-th predicate

184
■
Chapter 6
Fuzzy SQL and Intelligent Queries
TABLE 6.19
Product of Tall and Heavy Customers
Product
Name
Height
Weight
Age
µ(Heavy)
µ(Tall)
µ(Heavy, Tall)
Sanders
6’2”
215
52
.95
.91
.86
Freeman
5’10”
202
34
.55
.83
.46
Cassey
6’1”
188
40
.25
.90
.23
Miller
5’11”
157
25
.00
.87
.00
O’Malley
5’5”
163
48
.00
.62
.00
Jackson
5’3”
170
38
.00
.53
.00
When the weights for columns are ignored (each column weight is one
[1]), Expression 6.4 shows that the QCIX is computed as the ordinary
product for the complete query. The product membership approach
computes the chained product of the entire membership values. This
is reﬂected in Table 6.19.
The product, like the minimum method is sensitive to a predicate
with zero truth membership. However, unlike the minimum method,
which is totally biased toward the query predicate with the smallest degree
of truth, the product method tends to move more slowly in the direction of
the overall minimum (the interpretation of the membership products, of
course, is a percentage of each other; thus, prod(.25,.90) is either 25%
of 90 or 90% of 25).
6.9
Complex Query Compatibility Metrics
Understanding the dynamics of a fuzzy query is fairly easy once you see
how the query manipulates the set of incoming truth memberships. Let’s
complete our example by extending the fuzzy query to include Age. In
this case, the marketing group is looking for Tall, Heavy, and Middle-
aged customers. Once more, the conventional SQL statement requires
us to deﬁne what we mean by Middle-aged. Suppose we say that peo-
ple between 35 and 45 are Middle-aged. The SQL query is then as
follows.
select name
from ieh.customer
where customer.height >= 6
and customer.weight >= 195
and (customer.age >=35 and customer.age =< 45);

6.9
Complex Query Compatibility Metrics
■
185
TABLE 6.20
Tall, Heavy, and Middle-aged Customer (crisp)
Tall, Heavy, and
Name
Height
Weight
Age
Middle-aged
Jackson
5’3”
170
38
Sanders
6’2”
215
52
Miller
5’11”
157
25
Cassey
6’1”
188
40
O’Malley
5’5”
163
48
Freeman
5’10”
202
34
However, as Table 6.20 indicates, this criterion does not change the
records retrieved from the database (because we only had one from the
previous criterion). Thus, only Sanders is Tall, Heavy, and Middle-aged.
Once more, examining the table we can see that there are customers who
might be candidates if we can relax the crisp boundaries on our query.
We can do better by replacing the query with its corresponding fuzzy
version, as follows.
select name
from ieh.customer
where customer.height is Tall
and customer.weight is Heavy
and customer.age is MiddleAged;
The intent of this query is also obvious: we want customers that are Tall,
Heavy, and Middle-aged. To resolve the query we need to tell the appli-
cation what constitutes a Middle-aged person. Unlike the Tall and Heavy
fuzzy sets, Middle-aged represents a fuzzy number. Fuzzy numbers are
bell- or triangular-shaped fuzzy sets. The Middle-aged fuzzy set is cen-
tered around 40, the essential point of middle age. Figure 6.17 shows one
possible way to represent this concept.
Now when we have a value from the customer’s age column we can
ﬁnd its membership in the Middle-aged fuzzy set. Table 6.21 shows the
membership values for each of the weights in the table (ranked by their
membership values). With this extension to our fuzzy query, we have
found that all of our customers have some small degree of membership
in the Middle-aged fuzzy set, although two of these (Sanders and Miller)
have very small degrees of membership. We note that in this ﬁnal slice
through the table (see Table 6.22) Sanders has moved from its top position
toward the bottom of the list (for selected records with minimum method
compatibility indexes greater than zero).

186
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Grade of membership m(x)
Policy holder age (in years)
25
30
35
40
45
50
55
0
1.0
Middle Aged
Customer_Age
Figure 6.17
Middle-aged fuzzy set.
TABLE 6.21
Tall, Heavy, and Middle-aged Customers
Name
Height
Weight
Age
µ(MA)
µ(Heavy)
µ(Tall)
Cassey
6’1”
188
40
1.0
.25
.90
Jackson
5’3”
170
38
.97
.00
.53
Freeman
5’10”
202
34
.90
.55
.83
O’Malley
5’5”
163
48
.70
.00
.62
Sanders
6’2”
215
52
.12
.95
.91
Miller
5’11”
157
25
.10
.00
.87
TABLE 6.22
Tall, Heavy, and Middle-aged Customers
Name
Height
Weight
Age
µ(MA)
µ(Heavy)
µ(Tall)
Min
Avg
Freeman
5’10”
202
34
.90
.55
.83
.55
.760
Cassey
6’1”
188
40
1.0
.25
.90
.25
.716
Sanders
6’2”
215
52
.12
.95
.91
.12
.660
Jackson
5’3”
170
38
.97
.00
.53
.00
.500
O’Malley
5’5”
163
48
.70
.00
.62
.00
.440
Miller
5’11”
157
25
.10
.00
.87
.00
.323

6.10
Compatibility Threshold Management
■
187
Thus, using a fuzzy query approach instead of a conventional SQL
query we have found three customer candidates instead of one. Indeed,
the fuzzy query selected customers that matched the intent of our query
rather than the strict algebraic rules of the query. As we can see, as the
number of column attributes increases the brittleness of conventional SQL
queries also rapidly increases. The fuzzy query approach strikes a balance
in the process by including records that to some degree meet the intent of
our query. Selecting the minimum or average result analysis then allows
us to choose records that have some truth in each of the selection criteria
(minimum) or some overall truth in the complete query (average).
6.10
Compatibility Threshold Management
In practice, the retrieval of records through a fuzzy query can produce
extensive lists ranked by ever-decreasing query compatibility index val-
ues. An analyst needs some mechanism for selecting just the ﬁrst n
records. Generally, we set the limit for n records by specifying the
minimum acceptable compatibility index value. By indicating a QCIX
threshold, only those records with a compatibility index value equal to or
above the threshold will be actually returned by the query. For example,
the query process returns all rows in Table 6.17 when the threshold is
zero [0]. On the other hand, if we raise the minimum allowed QCIX to
[0.2] only a subset of rows is returned. Table 6.23 shows the records from
the minimum QCIX method.
On the other hand, as Table 6.24 shows, the same compatibility
threshold returns all records when the average method is used.
The query threshold is a form of alpha cut threshold applied to the
aggregate truth membership. A QCIX value below the threshold is treated
as though its value were zero. By adjusting the QCIX threshold, an analyst
can enforce either a weak or strong degree of compatibility between
the selected data rows and the underlying query concept. If the QCIX
threshold is low, compatibility is weakly enforced, whereas a high QCIX
threshold enforces a strong degree of compatibility.
TABLE
6.23
Minimum
QCIX
Method
Threshold
Screening
with
QCIX
Threshold 0.2
Name
Height
Weight
Age
µ(MA)
µ(Heavy)
µ(Tall)
Min
Freeman
5’10”
202
34
.90
.55
.83
.55
Cassey
6’1”
188
40
1.0
.25
.90
.25

188
■
Chapter 6
Fuzzy SQL and Intelligent Queries
TABLE
6.24
Average
QCIX
Method
Threshold
Screening
with
QCIX
Threshold 0.2
Name
Height
Weight
Age
µ(MA)
µ(Heavy)
µ(Tall)
Avg
Freeman
5’10”
202
34
.90
.55
.83
.760
Cassey
6’1”
188
40
1.0
.25
.90
.716
Sanders
6’2”
215
52
.12
.95
.91
.660
Jackson
5’3”
170
38
.97
.00
.53
.500
O’Malley
5’5”
163
48
.70
.00
.62
.440
Miller
5’11”
157
25
.10
.00
.87
.323
6.11
Fuzzy SQL Process Flow
Having examined the way where predicates control which records partic-
ipate in the query, this section summarizes the ﬂow of control and action
in the fuzzy SQL environment. From this we lead into an example of a
query from the Fuzzy Data Explorer. The basic fuzzy query paradigm is
amazingly simple, thus providing a powerful data exploration tool avail-
able to both information technology (IT) users and knowledge engineers.
Figure 6.18 shows a ﬂow schematic for the query process.
The fuzzy SQL process is essentially a protracted do . . . while loop.
Each record is read from the data date (or spreadsheet or ﬂat ﬁle). The
values for the variable speciﬁed in the where statement are extracted
(x1, x2, x3, . . . xn) and their memberships in the associated fuzzy sets deter-
mined. As shown in Expression 6.6, this generates a vector, V m, of
memberships.
V m = (µ1(x1), µ2(x2), . . . µn(xn))
(6.6)
The QCIX processor also creates a default weight vector, as shown in
Expression 6.7, containing the bias weights for each ﬁeld.
V w = (w1(x1), w2(x2), . . . wn(xn))
(6.7)
Thus, as Expression 6.8 illustrates, the effective compatibility index is
found by multiplying the actual membership vector by the weight vector.
V c = V m × V w
(6.8)
Naturally, if column or ﬁeld weights have not been speciﬁed the vector
defaults to a series of ones [1] so that the result is simply the base mem-
bership values. These vectors constitute the basic building blocks for the

6.11
Fuzzy SQL Process Flow
■
189
Yes
Yes
No
Selected
records
Outcome
file
Database
tables
Spread sheets
and flat files
Outcome
report
CQIX
>
alphacut
End
of
file?
Read
record
Sort selected
records
Compute
QCIX
Where
Append QCIX
to record
No
Domain
Degree of
membership µ(x)
1
0
Map value
to fuzzy set
Figure 6.18
The fuzzy SQL ﬂow schematic.
weighted average compatibility index. Expression 6.9 shows Expression
6.4 cast in terms of these vectors.
QCIX =
N
i=1 V c
i
N
i=1 V w
i
(6.9)
Here,
QCIX
is the query compatibility index for query
N
is the number of evaluated predicate vectors
V c
is the effective compatibility index (see Expression 6.8)
V w
is vector of weights
Once the QCIX is computed, it is compared against the minimum QCIX
threshold. Only records with a QCIX equal to or greater than this mini-
mum value are selected. The selected record’s QCIX is added as a new

190
■
Chapter 6
Fuzzy SQL and Intelligent Queries
column, and the record is written to the query-retained records ﬁle. When
the end of the ﬁle is encountered, the retained records are sorted by
their QCIX (for rows with the same QCIX value, the sort uses the pri-
mary identiﬁer or the ﬁrst column speciﬁed in the select statement as
a discriminator).
Code Preview
Executing Fuzzy SQL (Visual Basic Code)
The previous schematic provided a general view of the fuzzy query’s control
architecture. How is this translated into a working software system? The
complete Fuzzy SQL Visual Basic code (with supporting C/C++ utilities) is
found on the companion CD-ROM. However, the following local subroutine
is the actual code that implements a deﬁned query.
Public Sub
mpRunQuery()
Dim sFzyId
As String
Dim i
As Integer
Dim iCOffset
As Integer
Dim iStatus
As Integer
Dim iAndType
As Integer
Dim bMultiHedge
As Boolean
Dim bColHedge
As Boolean
Dim bUseWgts
As Boolean
Dim lRowNum
As Long
Dim lRowCnt
As Long
Dim lVecIdx
As Long
Dim dData
As Double
Dim fMem(1 To 64)
As Single
Dim fWgt(1 To 64)
As Single
Dim fCIX
As Single
Dim fCAlfa
As Single
giStatus = 0
We begin the procedure by retrieving two important parameters: the
minimum QCIX threshold and the type of And operator. The minimum
QCIX (also known as the alpha cut threshold) is stored in fCAlfa, and the
And operator type is stored in iAndType. The procedure also looks to see

6.11
Fuzzy SQL Process Flow
■
191
if column or ﬁeld weights are in use. If so, the Boolean variable bUseWgts
is set to True.
fCAlfa = Val(FQMcixThreshold.Caption)
If FQMoptAMin.value = True Then
iAndType = 1
End If
’
If FQMoptAAvg.value = True Then
iAndType = 2
End If
’
If FQMoptAPro.value = True Then
iAndType = 3
End If
’
bUseWgts = False
If FQMUseWgts.value = Checked Then
bUseWgts = True
End If
This is the main query-processing loop. The procedure allocates a working
array dimensioned to hold all possible rows and columns (note that we add
one additional column to hold the QCIX for the record). For each row in
the data table we process each where statement component (iQryItems).
ReDim gsQueryData(1 To glDataRows, 1 To iQryColCnt + 1)
lRowCnt = 0
For lRowNum = 1 To glDataRows
For i = 1 To iQryItems
iCOffset = iFzyColumn(i)
We also move the column data into dData, fetch the fuzzy set associated
with this query item (lpFDB holds a pointer value to the fuzzy set), and
ﬁnd the membership of dData in the fuzzy set lpFDB. This is then stored
in the vector of memberships [fMem(1 . . . n)]. If column weights are used,
we also compute the weights vector [fWgt(1 . . . n)].
dData = CDbl(tgDataGrid(lRowNum, iCOffset))
lpFDB = mpGetFuzzySet(i, lRowNum, iStatus)
fMem(i) = FzyGetMembership(lpFDB, dData, lVecIdx, lStatus)
If bUseWgts Then
fWgt(i) = fColWgts(iCOffset)

192
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Else
fWgt(i) = 0
End If
Next I
All columns have been evaluated and the fMem( ) vector contains the
membership values for each dData and lpFDB relationship. From this
vector (and the weight vector, if needed), the current record’s query
compatibility index (fCIX) is computed by mpCompIdx.
fCIX = mpCompIdx(fMem(), fWgt(), iQryItems, iAndType, iStatus)
If iStatus > 0 Then
Exit Sub
End If
If the current query compatibility index is greater than or equal to the
cutoff threshold, we select this record. This means incrementing a record
count, storing the fCIX as the ﬁrst column in the outcome array, and
moving the rest of the record’s ﬁeld into the remainder of the slots.
If fCIX >= fCAlfa Then
lRowCnt = lRowCnt + 1
gsQueryData(lRowCnt, 1) = fCIX
For i = 1 To iQryColCnt
gsQueryData(lRowCnt, i + 1) = tgDataGrid(lRowNum,
iQryOffsets(i))
Next i
End If
Next lRowNum
If no records were selected, we set the status code, display a message
on the main fuzzy SQL query execution panel, and exit the procedure.
Although a fuzzy query generally selects more records than a conventional
crisp query, there are instances in which no records are selected. Essen-
tially this means that none of the records had a query compatibility index
above the threshold level ( fCAlfa).
If lRowCnt = 0 Then
giStatus = 101
FQMrunMsg.Caption = "No Records Above CIX
Threshold."
Exit Sub
End If
Now the QuickSort routine is invoked to sort the stored records in
descending or ascending order.

6.12
Fuzzy SQL Example
■
193
If FQMSortDes.value = True Then
Call mpSortDESQuerydata(1, lRowCnt, iStatus)
Else
Call mpSortASCQueryData(1, lRowCnt, iStatus)
End If
The query is now concluded by showing the selected records on the visual
interface’s spreadsheet (see Figure 6.27). The processing statistics are also
displayed. A graph of the CQIX values is produced (this shows the degree
to which the query sliced through the set of candidate records). Finally,
the selected records are saved on the output ﬁle (see the INTO parameter,
Figure 6.16).
Call mpUpdateQryGrid(FQMGrid, lRowCnt, iStatus)
FQMRead.Caption = Trim(Str(glDataRows))
FQMSelected.Caption = Trim(Str(lRowCnt))
FQMPercent.Caption = Format((lRowCnt / glDataRows) *
100, "###.00")
Call mpGraphResults(lRowCnt)
Call mpWriteResults(lRowCnt, iStatus)
End Sub
6.12
Fuzzy SQL Example
The examples in this chapter are screen images from the Fuzzy Data
Explorer (included with the book’s software, see the dataexplorer.vbp
Visual Basic project). This fuzzy data mining and knowledge discovery
application contains an integrated fuzzy SQL component. The project and
customers databases used in this chapter are also located in the application
knowledge base directory (dxbook/appkb).
In this section, the initial project risk database is revisited. A fuzzy
query is formulated to ﬁnd projects with a high budget and a short dura-
tion. The actual mechanics of fuzzy set decomposition, where statement
construction, and query execution are followed in a step-by-step process.
The initial step in a fuzzy SQL request is, in most ways, similar to conven-
tional SQL. The table sources are speciﬁed and the columns are identiﬁed.
In this example, we are accessing a standard “ﬂat ﬁle” of project data. The
columns are simply separated by blanks. A ﬁle name associated with the
outcome process is also entered if the analyst wants the selected col-
umn data, ranked by their compatibility index, saved to an external ﬁle.
Figure 6.19 shows the main fuzzy SQL request deﬁnition dialog.

194
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Fuzzy set
properties
indicator
Column
names
Figure 6.19
Fuzzy SQL deﬁnition request.
The ﬁrst principal difference between traditional and fuzzy SQL
queries is evident in the list of available column names appearing in the
left-hand side of the main deﬁnition dialog. Every column used in a where
statement must have at least one fuzzy set deﬁned in its range of values.
Fuzzy sets and additional data characteristics are deﬁned by selecting the
column name and clicking on the Properties command button. A column
with existing fuzzy sets is indicated with a check mark next to its name.
Fuzzy Set Deﬁnitions
Figure 6.20 shows the deﬁnition of the High fuzzy set for the Budget col-
umn. This is an S-curve extending from 70 on the horizontal axis (where
its membership is zero) to the right-hand edge of the variable’s current
domain (at a value of 93 on the horizontal axis).
Figure 6.21 shows the deﬁnition of the Short fuzzy set for the Duration
column. This is a decay (or left-facing) S-curve extending from 87 (where
its membership is one) down to the horizontal axis at 95 (where the fuzzy
set membership value is 0).
It is worth repeating at this point that these fuzzy sets represent one
possible series of deﬁnitions for the concepts of High Budget and Short
Duration. Figure 6.22 shows one such alternate view of Short Duration.

6.12
Fuzzy SQL Example
■
195
Figure 6.20
Deﬁning High budget fuzzy set.
Figure 6.21
Deﬁning Short duration fuzzy set.
The Left Shoulder membership function provides a more trapezoidal
form to the deﬁnition of Short — a fact that could allow the decomposi-
tion of the Duration column into a collection of trapezoids. Figure 6.23
illustrates the deﬁnition of the Medium project duration in this fashion.

196
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Figure 6.22
Deﬁning Short Duration fuzzy set.
Figure 6.23
Deﬁning Short and Medium Duration fuzzy sets.
The choice of sigmoid curves of trapezoids is not arbitrary. The
ﬁne-grain detail of the fuzzy membership function controls how truth
membership is interpreted and returned to compatibility measurement
functions. In all cases, however, the actual set membership functions
reﬂect the semantics of the analyst’s understanding of these concepts.

6.12
Fuzzy SQL Example
■
197
Forming Fuzzy Where Predicates
These fuzzy sets now form the vocabulary for the fuzzy query. They are
used in the where predicates in place of exact values. This brings us to the
next principal difference in a fuzzy SQL request: predicates are expressed
in terms of fuzzy vocabularies. Figure 6.24 shows the deﬁnition of the
fuzzy where predicate Budget is High.
The main graphical window, shown in the center of the dialog screen,
shows all fuzzy sets deﬁned for the column Budget, which in this case
is the single fuzzy set High. However, any number of fuzzy sets, each
representing some semantic mapping of a column’s data space, can be
deﬁned. Figure 6.25 shows one such partitioning of the Budget column
into the three fuzzy sets Low, Moderate, and High.
The decomposition of a column into multiple fuzzy sets provides a
complete semantic description of the data space. These fuzzy sets are
made visible during the where predicate statement deﬁnition. Figure 6.26
shows the predicate deﬁnition dialog for the Budget column when the
Low, Moderate, and High sets are deﬁned.
The selection criteria are completed with the speciﬁcation of the Dura-
tion is Short predicate. Like the Budget column, the criteria for the
Duration column are completed by selecting the appropriate fuzzy set.
Figure 6.27 shows the deﬁnition of the fuzzy where predicate Duration
is Short.
High Fuzzy Set
with Background
Data
Figure 6.24
Specifying Budget is High predicate.

198
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Figure 6.25
Budget with multiple fuzzy sets.
Figure 6.26
Budget column with multiple fuzzy sets.
After each where predicate expression is completed, the Collect
command is clicked to save the expression. These expressions are
automatically connected with the And operator. The complete where
statement is saved using the Submit command. Figure 6.28 shows how
the command buttons are used.

6.12
Fuzzy SQL Example
■
199
Figure 6.27
Budget column with multiple fuzzy sets.
Press “Submit”
to complete
the Where
Statements
Press “Collect” to
Verify and Store
Each Where Statement
Figure 6.28
Collecting and submitting where statements.

200
■
Chapter 6
Fuzzy SQL and Intelligent Queries
FuzzySQL
Query
Definition
Type of
AND used
to compute
QCIX
QCIX Cut-off
Threshold
QCIX Outcome
Sort Order
Figure 6.29
The fuzzy SQL request deﬁnition.
When the where ﬁlter has been constructed and submitted, the fuzzy
SQL request is ready for execution. Clicking on the Submit command
button switches to the Query Execution tab. Figure 6.29 shows the ﬁnal
SQL request and the surrounding controls.
Prior to executing the fuzzy SQL request, a number of controls can be
changed, including the type of And, the cutoff threshold for the QCIX,
and the order of presentation for the selected records. Clicking on the
Weighted Average And type also automatically checks the Use Column
Weights control (you can unclick this to ignore any column weights).
Sort order is normally descending, showing the records with the highest
QCIX values ﬁrst. In some cases, an ascending sort is more useful. This
sort would display the records with the smallest QCIX ﬁrst.
6.13
Evaluating Fuzzy SQL Outcomes
Executing the fuzzy SQL statement causes the database access engine to
loop through the database projects table, evaluating the where statement

6.13
Evaluating Fuzzy SQL Outcomes
■
201
Figure 6.30
Fuzzy SQL outcome results.
CIX,
project, budget, duration
0.962,
P08,
90,
88
0.447,
P04,
81,
87
0.301,
P14,
89,
92
0.235,
P05,
78,
92
0.133,
P11,
93,
93
Listing 6.1
The Fuzzy SQL outcomes ﬁle.
for each record. The membership of each data element in the correspond-
ing fuzzy set is determined, and these memberships are used to compute
the ﬁnal QCIX. Figure 6.30 shows the outcome results table.
In this query, the minimum QCIX threshold was set to [.10], and thus
records with a compatibility value below this cutoff level are not shown
and are not stored.
In addition to the visual display of records, the query facility saves the
output in a data ﬁle (in this case, projects.out). The data ﬁle organization
is comma-delimited with column names. Listing 6.1 shows the outcome
records stored in the results ﬁle.
Saving the outcome in this format means that we can use the fuzzy
query facilities to read and interrogate the outcome in the same manner
as any other data ﬁle. As Figure 6.31 schematically illustrates, the outcome

202
■
Chapter 6
Fuzzy SQL and Intelligent Queries
Database
table
Outcome files
FuzzySQL
processing
Analyze outcome
Figure 6.31
Interrogating the outcome ﬁle.
of the query is used by fuzzy SQL to evaluate the records that were selected
by the base query.
By deﬁning how we interpret the range of QCIX values, we can easily
reread the outcome ﬁle and select records that meet our perception of
what constitutes acceptable record sets. Figure 6.32 shows one possible
collection of fuzzy sets deﬁning the range of values for the compatibility
index.
With these fuzzy set deﬁnitions in hand, the query language can select
records from the outcome meeting our semantic expectations about the
quality of their compatibility with our request. As one example, the fol-
lowing request selects records with Acceptable and Good compatibility
index values.
select *
from projects.out
into myprojs.csv
where CIX is above Acceptable
Applying above hedge to the fuzzy set Acceptable creates a ﬂattened S-
curve spanning the plateau of the Acceptable fuzzy set’s trapezoid and con-
tinuing to the end of the QCIX’s range of values (thus covering the Good

6.13
Evaluating Fuzzy SQL Outcomes
■
203
Figure 6.32
The compatibility index fuzzy sets.
fuzzy set). Figure 6.33 shows this hedged fuzzy region superimposed on
the collection of base fuzzy sets.
Thus, we wind up with a ﬁle ranked by a compatibility index based
on the compatibility index of the incoming ﬁle. For a small data ﬁle, such
Figure 6.33
The above Acceptable fuzzy set.

204
■
Chapter 6
Fuzzy SQL and Intelligent Queries
as the projects table, this is not much of an advantage. However, for very
large data ﬁles the ability to screen or ﬁlter the outcome of a query based
on our perception of the QCIX ranking gives us a powerful way of handling
data. A ﬁltering based on the semantics of the QCIX becomes even more
important when we consider that record selection is often performed
using an application framework. Encoding our record ﬁltering allows an
automated way of coupling the fuzzy query with a post-selection ﬁltering
mechanism, both of which are predicated on the query compatibility
concept.
6.14
Review
Fuzzy SQL forms one of the core data exploration tools needed by business
analysts and knowledge engineers at the start of any knowledge discovery
project. In addition to the general necessity of data proﬁling (developing
a statistical knowledge of each variable’s properties), fuzzy query capa-
bilities give analysts and engineers a highly ﬂexible and robust method
of semantically scouring large databases for new relationships, as well as
for studying the degree to which known behaviors and relationships exist
and can be isolated. You should now be familiar with the following.
■
The basic ideas of row and column data, and relational databases
■
How data is retrieved through the select and where statements
■
How fuzzy sets are used to describe the semantics of database columns
■
How a degree of truth is calculated for each where statement predicate
■
The various ways of computing a query compatibility index (QCIX)
■
How the QCIX effectively ranks records by the query intent
■
How fuzzy queries isolate and quantify patterns in data
■
The way the fuzzy outcome of a query can be reevaluated
Fuzzy SQL is a powerful tool for data exploration and pattern discov-
ery. However, it relies on the ingenuity and perceptions of the human
analyst. In the next two chapters we begin an examination of pattern dis-
covery techniques that ﬁnd, extract, and quantify patterns directly from
the data. The ﬁrst method, fuzzy cluster detection, provides a visual tool
for understanding how data elements are related across multiple dimen-
sions and a simple technique for generating the associated classiﬁcation

Further Reading
■
205
rules. Following fuzzy clustering we explore a comprehensive and ﬂex-
ible method of discovering and extracting if-then fuzzy rules from large
collections of data.
Further Reading
■
Bezdek, J. C. (ed.). Analysis of Fuzzy Information, Volume II: Artiﬁcial
Intelligence and Decision Systems. Boca Raton, FL: CRC Press, 1987.
■
Dubois, D., H. Prade, and R. Yager (eds.). Fuzzy Information Engineering. New
York: John Wiley and Sons, 1996.
■
Kerschberg, L. (ed.). Expert Database Systems: Proceeding from the Second
International Conference. Redwood City, CA: The Benjamin/Cummings
Publishing Company, 1989.
■
Li, D., and D. Liu. A Fuzzy Prolog Database System. New York: John Wiley and
Sons, 1990.
■
Zadeh,
L.,
and J. Kacprzyk (eds.).
Fuzzy Logic for the Management of
Uncertainty. New York: John Wiley and Sons, 1992.

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 7
Fuzzy Clustering
In this chapter we examine methods of analyzing data using fuzzy cluster-
ing techniques. Fuzzy clustering provides a robust and resilient method of
classifying collections of data elements by allowing the same data point to
reside in multiple clusters with different degrees of membership. By way
of comparison, we also look at a crisp clustering method, the k-means
algorithm. The algorithms we use in this chapter are the fuzzy c-means
(Duda and Hart, and Bezdek) and the fuzzy adaptive clustering algorithm
(Young-Jun Lee, based on work by Krishnapuram and Keller).
Attempts to ﬁnd meaning and patterns in chaotic or turbulent systems,
whether natural or artiﬁcial, have been at the root of western analytical
thought since the time of Aristotle. Modern scientiﬁc methods, supported
by powerful mathematical and machine intelligence capabilities, have
given business analysts, knowledge engineers, and systems architects a
wide spectrum of tools for probing the unstructured and often chaotic
nature of business data. Following a reductionist methodology (the core
of modern scientiﬁc method, ﬁrst developed by William of Okkham and
Roger Bacon), knowledge engineers have turned to techniques that parti-
tion data into related families. This simple act of partitioning reduces com-
plexity and helps expose hidden order and deeply buried patterns in data.
Large databases in such commerce-driven industries as retailing, manu-
facturing, insurance, banking, and investments contain a staggering
amount of raw data. Knowledge engineers and systems analysts, faced
with the prospect of maintaining a competitive edge in the world of
e-commerce and rapidly changing delivery channels, must look to this
data to ﬁnd new customer relationships, possible newly emerging lines
of business, and ways to cross sell existing products to new and existing
customers. Yet trying to make sense out of this data (seeking patterns, for
example) requires an analytical perspective not easily achieved through
either statistical analysis or even multidimensional visualization alone.
High-altitude views of the raw data space more often than not reveal
largely noise. Close-in views are often overpowered by the sheer quantity
of data.
207

208
■
Chapter 7
Fuzzy Clustering
7.1
The Vocabulary of Fuzzy Clustering
If you exclude mathematical equations, the vocabulary of fuzzy cluster-
ing is surprisingly sparse. In fact, much of the underlying terminology
related to cluster conﬁguration, membership, and overlap has already
been covered in the chapters on fuzzy logic fundamentals. A review of
some vocabulary terms used in fuzzy clustering, however, will make read-
ing and understanding the material in this chapter somewhat easier. It
is, however, not always possible, while maintaining an uncluttered and
coherent discussion of the clustering process, to ensure that every term
is introduced before it is used.
Centroid
The center of a cluster. A symmetric cluster can be visualized as an
n-dimensional region balanced on its center of gravity. Hence, the center
of a cluster is also called the centroid of the cluster.
Cluster (Clustering)
A cluster is a group of observations (“things”) that have similar properties.
The process of clustering involves ﬁnding sets of groups (clusters), each
containing all observations that share a collection of speciﬁed properties
(or attributes). When the clusters or categories are not predeﬁned, cluster-
ing provides a form of unsupervised knowledge discovery in which deeply
buried or distributed patterns emerge, revealing a variety of behavior
patterns in the underlying data.
Cluster Seeds
Clustering often starts with an estimate of the cluster population (N clus-
ters). These N cluster centers are then initially populated with random
values drawn from the domain of each dimension in the cluster. The
random values used to initialize the clusters are called the cluster seeds.
Entropy (or Noise)
Large collections of data are often ﬁlled with noise (obvious outliers, and
missing, improperly transcribed, erroneous, and duplicate data). The
noise acts like background scattering and can obscure the actual patterns

7.1
The Vocabulary of Fuzzy Clustering
■
209
buried in the data. Entropy is a measure of disorder in a system. The
higher the entropy the greater the degree of disorder. One of the goals
of a clustering technique is to identify noise and reduce its impact on
clustering.
Euclidean Distance
The most commonly used measure of the distance from the current cluster
center to a data point, called the Euclidean distance, is based on the
Pythagorean theorem. For two points, x and y, in N-dimensional space,
Expression 7.1 is the Euclidean distance.
d =




N

i=1
(xi −yi)2
(7.1)
Here,
d
is the distance
N
is the number of dimensions in the data
xi
is the i-th value x
yi
is the i-th value y
This is generally considered the ordinary (or conventional) distance
between two points — one that could, theoretically, be measured by a
straight edge. There are many other methods for measuring the distance
between points in multidimensional space, but the Euclidean distance is
the most commonly used.
Iteration
The process of adding a new point to a cluster and then recomputing
the center is called iteration. This is the core mechanism in compiling a
cluster.
Membership
In conventional clustering, a data point is a member of one and only
one cluster. In fuzzy clustering, a data point can have a membership in
multiple clusters (each to a different degree). Membership is a measure of
how strongly a data point is a member of a particular cluster. This measure
is important in the rule-generation process.

210
■
Chapter 7
Fuzzy Clustering
Stability
The idea of stability is a measure of the steady state of a cluster. As new
data points are added to a cluster, the center is recalculated. The center
moves as the weighted center of gravity changes. At some point, when
enough points are added, the center no longer changes. At this point the
cluster is stable.
7.2
Principles of Cluster Detection
Cluster analysis is a technique for breaking data down into related com-
ponents in such a way that patterns and order become visible. In fact,
cluster analysis has the virtue of strengthening the exposure of patterns
and behaviors as more and more data becomes available. A cluster has
a center of gravity, which is basically the weighted average of the clus-
ter. Data points arrayed around the center have, in the fuzzy clustering
approach, a degree of membership in each of the centers. By evaluating
the strength of membership in clusters for sets of records, we can often
discover new and interesting relationships in the data.
Clustering is an important approach to understanding business pro-
cess models. By looking at the relationships between multiple attributes
in the data (variables in the model) we expect to ﬁnd indicators of the
model performance. Isolating these indicators, such as market segmenta-
tion factors, provides valuable insights into the order hidden in the data.
Figure 7.1, for example, shows a simpliﬁed retailing system and some of
the common attributes associated with each component.
Our investigation of the retailing data might lead us to approach mar-
ket segmentation by looking for discriminators among customers and
purchases. We would like to know which customers are likely to pur-
chase what quantities of products according to their pricing stratiﬁcation.
Figure 7.2 illustrates a hypothetical clustering of the data based on two
attributes (income divided by age against amount spent and the price
times quantity purchased).
In this example, two discrete and easily identiﬁable clusters emerge
(such is of course not usually the case in the real world). Here, in exam-
ining the ratios two groups emerge. From this we might predict the
purchasing habits of new clients, the product mix offered to current
customers, or the proﬁtability of potential clients. Clustering, then, is an
attempt to gather points in the sample space that share similar attributes.
This similarity can be explicit in the data or deﬁned through some simi-
larity or association function. In the end, we hope that clustering reveals

7.3
Some General Clustering Concepts
■
211
Age
Customer
ID
Gender
Income
Address
Customer
since...
Customer
Products
Price
SKU
QOH
Category
Vendor
Date
SKU
Amount($)
Quantity
Stock
type
Purchases
Figure 7.1
Patterns and relationships in a retailing system.
hidden patterns and hidden order in the data so that new insights into our
business processes are gained.
7.3
Some General Clustering Concepts
As we will discuss shortly, clustering in actual data analysis often involves
data with high dimensionality. In our example, we used two dimensional
data groups (xi, yi), but normally our data contains many data elements
(ai, bi, ci, . . . , zi) so that cluster analysis penetrates deeply into our data’s
sample space. Before moving on to the process of crisp (hard) and fuzzy
clustering methods, we need to understand some principles common to

212
■
Chapter 7
Fuzzy Clustering
Income/age (×100)
Price×quantity
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
Income >40000
age >30    
Figure 7.2
Clusters representing income versus spending habits.
both approaches. These include the issues of what is clustered in the
sample space and how we go about assigning a data point to a particular
cluster. The ﬁrst issue involves understanding the attributes of clustering,
and the second centers on using these attributes to measure distances in
a sample space of arbitrary dimensionality.
Cluster Attributes
Clustering takes a vector of ordered values (columns in a database or
spreadsheet, for example) and maps them into similar groups. Every
point (xi) in a cluster space consists of a multidimensional vector (X)
containing the column or ﬁeld values.1 Expression 7.2 shows how this is
constructed.
Xi(x1i, x2i, x3i, . . . , xni)
(7.2)
1 In this chapter we generally use the term xi to mean the complete data point, more formally
represented by the n-tuple vector, X. Thus, xi is the attribute vector X(a1, a2, . . . , an), each
attribute (aj) specifying a dimension in the sample space.

7.3
Some General Clustering Concepts
■
213
Attributes
Rows
Vector
MPG
CYL
DISP
HP
WGT
ACCL
YR
ORG
Model
18.0
15.0
18.0
16.0
17.0
15.0
14.0
14.0
14.0
15.0
15.0
14.0
15.0
14.0
24.0
8
8
8
8
8
8
8
8
8
8
8
8
8
8
4
307.0
350.0
318.0
304.0
302.0
429.0
454.0
440.0
455.0
390.0
838.0
340.0
400.0
455.0
113.0
130.0
165.0
150.0
150.0
140.0
198.0
220.0
215.0
225.0
190.0
170.0
160.0
150.0
225.0
95.00
3504.
3693.
3436.
3433.
3449.
4341.
4354.
4312.
4425.
3850.
3563.
3609.
3761.
3086.
2372.
12.0
11.5
11.0
12.0
10.5
10.0
9.0
8.5
10.0
8.5
10.0
8.0
9.5
10.0
15.0
70
70
70
70
70
70
70
70
70
70
70
70
70
70
70
"chevrolet chevelle malibu"
"buick skylark 320"
"plymouth satellite"
"amc rebel sst"
"ford torino"
"ford galaxie 500"
"chevrolet impala"
"plymouth fury iii"
"pontiac catalina"
"amc ambassador dpl"
"dodge challenger se"
"plymouth 'cuda 340"
"chevrolet monte carlo"
"buick estate wagon (sw)"
"toyota corona mark ii"
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
1 
3
Figure 7.3
Components of cluster data attributes.
Correspondingly, every cluster center (Ck) — which is also a point in the
cluster space — has the same organization, as shown by Expression 7.3.
Ck(x1k, x2k, x3k, . . . , xnk)
(7.3)
In most cluster analyses, there may be hundreds or thousands (and
sometimes millions) of points but only a few cluster centers. We can visu-
alize the clustering operation as working on a relational database table.
Figure 7.3 shows a table of automobile characteristics and their properties.
A vector is derived from a row in the table. The columns represent the
vector attributes. Not all table columns, of course, will participate in the
clustering operation. In the automobile database shown in Figure 7.3, if
we want to apply a clustering algorithm to the cylinder (CYL), horsepower
(HP), weight (WGT), and acceleration (ACCL) columns the input to the
clustering process must be a set of vectors with just these attributes. It is
these vectors that are used in the clustering algorithm.
Cluster Memberships
Membership in a cluster is determined by measuring the distance from
a cluster center to a data point (see the previous section “Cluster
Attributes”). Figure 7.4 shows the array of points around a cluster center
with the distance metrics for several points.

214
■
Chapter 7
Fuzzy Clustering
100
y
200
300
400
C1
500
600
700
800
900
1000
x
60
50
40
30
20
10
C2
Figure 7.4
Data point distances within a cluster.
The overall process of clustering takes the distance between a point
Xm and each of the clusters C1...k as the simple sum of the differences
between Xm and Ci (any cluster center). If there are n attributes in each
cluster vector, we can represent this process as Expression 7.4.
diffk→C =
n

i=1
(X(xi) −C(xi))2
(7.4)
Here,
diff
is the difference (distance) between a data point and
a cluster center
n
is the number of attributes in a cluster
X(xi)
is the i-th data point in vector X
C(xi)
is the i-th data point in a cluster center (centroid)
Here, the Euclidean distance between the point and a center is calculated.
Other distance methods can be used, but the Euclidean calculation is fast
and appears to work very well in a wide variety of real-world clustering
applications.
More formally, perhaps, computing the distance between a point and
the cluster center is done through a process called a sum of the squared
difference. Expression 7.5 provides the mathematical summary of the
approach.
dk =
n

j=1
Xk
j −Ci
j

2
(7.5)

7.3
Some General Clustering Concepts
■
215
Here,
dk
is distance of the k-th data point (where a “data point” is an
individual vector of attributes, thus k will vary
from 1 to M, the number of vectors being clustered)
n
is the number of attributes in a cluster
Xk
j
is the j-th value of the k-th data point
Ci
j
is the j-th value of the i-th cluster center
A distance from Xk is calculated for each of the cluster centers (C1 . . . i )
using the n attributes in each vector. To ﬁnd the distance (dk) from
point Xk to the center Ci we sum the squared difference between the
j-th attribute of Xk and the j-th attribute of the i-th cluster center. From
this we can see that if the Xk lies on or close to the cluster center the
value will be at or near zero. On the other hand, as the distance between
Xk and Ci increases the distance measure also increases rapidly. Listing
7.1 shows the code for computing a Euclidean distance between a cluster
center and a data point.
double PE FCEucDist(
double dData[],double dCenters[],int iCentCnt,int
*ipStatus)
/*------------------------------------------------------*
| Calculate the standard Euclidean distance from the
|
| clusters to the data points. This is the sum of the
|
| squared distances.
|
*------------------------------------------------------*/
{
int i;
double dSum;
*ipStatus=0;
dSum=0;
for(i=0;i<iCentCnt;i++)
dSum+=pow(dData[i]-dCenters[i],2);
return(dSum);
}
Listing 7.1
A Euclidean distance metric.
The distance represents the sum of the squared difference between
each attribute of the data point vector (Xi) and the corresponding
attributes of the cluster center (Cj).
We can now see that Figure 7.5 is a simpliﬁcation. In actual cluster-
ing analysis, each point is measured from all cluster centers. The data

216
■
Chapter 7
Fuzzy Clustering
100
y
200
300
400
C1
d1
Pi
500
600
700
800
900
1000
x
60
50
40
30
20
10
C2
d2
Figure 7.5
Points coupled to multiple centers.
points nearer one center than another move the point into that cluster.
Figure 7.5 illustrates how a point (Xi indicated by point Pi) has a dis-
tance measurement to center C1 (the d1 distance) and center C2 (the d2
distance).
Thus, the clustering technique computes a distance metric for each
point to each cluster center. We then simply assign the point to the clus-
ter center that has the smallest distance value (that is, of course, to the
nearest cluster). This becomes the initial assignment of points to clusters.
Figure 7.6 schematically illustrates how a cluster is identiﬁed by its center
value.
Thus, in this example, the clusters are deﬁned by two centers C1 and
C2. Center C1 is located at (30,250) and center C2 is located at (38,875).
Each point in the data is assigned a membership in one of these clusters.
For crisp clustering, the membership is either [1] or [0] and a data point
belongs unambiguously to one and only one cluster. For fuzzy clustering,
the membership is in the interval [0,1] and a data point can belong to
multiple clusters with varying degrees of membership.
Higher-dimensional Clustering
Although the majority of examples in this chapter deal with 2D clusters
(for the sake of clarity and comprehension), real-world clusters are often
spread out across many dimensions. Figure 7.7 illustrates a set of clusters
arrayed in three dimensions.

7.3
Some General Clustering Concepts
■
217
100
y
200
300
400
C1
500
600
700
800
900
1000
x
60
50
40
30
20
10
C2
Figure 7.6
Clusters and cluster centers.
100
y
z
2.0
1.8
1.6
200 300 400 500 600 700 800 900 1000
x
60
50
40
30
20
10
1.4
1.2
1.0
Figure 7.7
A 3D cluster space.

218
■
Chapter 7
Fuzzy Clustering
In the case of 3D clusters (as in Figure 7.7), each cluster center is
deﬁned by three values (the X, Y, and Z axes). Thus, for any n-dimensional
cluster space the centers of the cluster (Ci) are deﬁned by values along
the axis of each dimension (dk) as follows.
Ci(d1, d2, . . . , dn)
(7.6)
Both 2D and 3D clusters are generally easy to visualize, allowing us to
check that our sense of an appropriate cluster center matches the center
generated by the computer. Above three dimensions, however, this visu-
alization is not usually possible. Although there are some mathematical
techniques for evaluating the quality of a cluster set, we must quite often
apply both a reductionist approach to cluster veriﬁcation and some com-
mon sense. Reduction analysis simply consists of analyzing the clusters
by slicing across two or three dimensions and evaluating the degree to
which these clusters are reasonable.
7.4
Crisp Clustering Techniques
Traditional clustering techniques attempt to segment data by grouping
related attributes in uniquely deﬁned clusters. Each data point in the sam-
ple space is assigned to only one cluster. And, as long as the data can
be partitioned in this manner, crisp clustering approaches, such as the
k-means algorithm, will perform satisfactorily. In partitioning the data, it
is important to remember that none of the data points is moved. Only the
centers of the clusters are moved. Thus, clustering is an iterative process
of ﬁnding better and better cluster centers (this is true for both crisp and
fuzzy clustering).
The k-Means Clustering Algorithm
The k in the k-means algorithm stands for the number of cluster seeds
initially provided to the algorithm (in the same way the c in the fuzzy
c-means algorithm stands for the initial number of cluster centers). The
crisp (or hard) k-means algorithm itself is fairly straightforward, involving
the basic parameters outlined in Table 7.1.
The goal of the hard k-means algorithm is the assignment of each
data point into one and only one cluster. The cluster centers are moved
according to a nearness or similarity function deﬁning the distance from
a cluster center to the data point. There are two fundamental equations

7.4
Crisp Clustering Techniques
■
219
TABLE 7.1
Basic Parameters of the k-means Algorithm
xi
A vector of training data, where i = 1, 2, . . ., n. These are the cluster attributes
selected from source data elements (such as columns in a database table).
k
The number of fuzzy clusters speciﬁed as part of the algorithm.
cj
The center (or centroid) of a crisp cluster ( j = 1, 2, . . ., k). This value is repeatedly
calculated by the algorithm (see Expression 7.3).
Sj
A cluster in the sample space. This is a set of points associated with the cluster
whose center is at cj.
δsj( )
The characteristic function of set Sj. Note that the characteristic function of a set A
is as
follows.
δ(∀xi ∈A) =
1
x ∈A
0
x /∈A

(7.7)
In other words, the function is one for all points of the set, but zero for points outside
the set.
used to manage the partitioning of data points. Expression 7.8 establishes
the set membership for clusters.
S(t)
j
=

xi

xi −c(t)
j
 <
xi −c(t)
k
, j ̸= k

(7.8)
Here,
S(t)
j
is the set membership in the j-th cluster center under
development.
t
is the current iteration cycle value. The value of t will vary
from 1 to P, the maximum number of iterations used to
deﬁne the ﬁnal clusters.
xi
is the i-th data point.
cj
is the center of the j-th cluster.
ck
is the center of the k-th cluster.
That is, in iteration time (t) a point xi belongs to cluster Sj if its distance
to the center of cluster Sj (indicated by cj) is smaller than the distance to
the k-th cluster center (for Sk). We skip over the current cluster center
(we do not compare the distance to ourselves!). The distance (∥· ∥) is
generally calculated using a Euclidean metric, but this could, of course,
be calculated via any standard distance method.
The cluster centers are initially seeded,
usually with a random
conﬁguration drawn from the domains (ranges) of the attributes (for
other approaches, see the section “Initial Clusters and Cluster Seeds”).

220
■
Chapter 7
Fuzzy Clustering
The centers are then updated during each cycle (t) using Expression 7.9.
c(t+1)
j
=
n
i=1 xiδSt
j (xi)
n
i=1 δSt
j (xi)
(7.9)
Here,
c(t+1)
j
is the set membership in the j-th cluster center under
development.
t
is the current iteration cycle value. The value of t will
vary from 1 to P, the maximum number of iterations
used to deﬁne the ﬁnal clusters.
xi
is the i-th data point.
δ( )
is the characteristic or set membership determination
function.
S(t)
j
is the set membership function form (see Expression 7.8).
This is a form of a weighted average, and thus we are calculating the
centroid of the j-th cluster center. The characteristic function will return
either [1] or [0], indicating whether or not the point xi is in the particular
cluster (Sj). Thus, we effectively have a bit vector (across the complete
sample space) constructed for each cluster center indicating whether a
data point xi is a member of that cluster. We add all cluster members
where the bit at location xi is [1] and divide by the sum of the [1] bits.
This is shown in Expression 7.10.
c(t+1)
j
=
n
i=1 xi(δSt
j (xi) = 1)
n
i=1 (δSt
j (xi) = 1)
=

i xi

i 1
(7.10)
Here,
c(t+1)
j
is the set membership in the j-th cluster center under
development.
t
is the current iteration cycle value. The value of t will vary
from 1 to P, the maximum number of iterations used to
deﬁne the ﬁnal clusters.
xi
is the i-th data point.
δ( )
is the characteristic or set membership determination
function.
S(t)
j
is the set membership function form (see Expression 7.8).
Note that a data point must belong to at least one cluster; otherwise,
we will have a division by zero. From these expressions we formulate a
somewhat formal deﬁnition of the k-means algorithm.

7.4
Crisp Clustering Techniques
■
221
Initialize k=Number of Clusters,
Initialize cj (cluster centers)
Set cycle variable t=1
Repeat:
For i = 1 to n: Distribute sample points (xi) into the K clusters
For j = 1 to k: Resolve S(t)
j
for xi applying (7.8)
For j = 1 to k: Compute new cluster centers c(t+1)
j
applying (7.9)
t = t+1
Until cj estimates stabilize
The algorithm itself is fairly simple: cluster centers are randomly initialized
and we assign data points (xi) into clusters (Sj; j = 1 to k) by resolving
Expression 7.8. This beginning state is illustrated in Figure 7.8. The dashed
line represents the initial assignment of points into cluster centers C1
and C2.
When all data points have been assigned to clusters, new cluster cen-
ters (centroids) are calculated using Expression 7.10. As we can see in
Figure 7.9, the cluster center calculation causes the previous centroid
location to move toward the center of the cluster sets. This also changes
the partitioning of the sample space.
The process of calculating cluster memberships and recalculating clus-
ter centers continues until the cluster centers no longer change from one
cycle to the next (or the change is so small that it falls below some thresh-
old value). Thus, as Figure 7.10 illustrates, the shift in the cluster centers
becomes smaller and smaller until it settles on the ﬁnal value.
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
C1
C2
Figure 7.8
Initial k-means partitions.

222
■
Chapter 7
Fuzzy Clustering
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
C1
C2
Figure 7.9
Cluster centers repositioned after iteration.
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
C1
C2
Figure 7.10
Final cluster centers after end of iterations.
The k-means algorithm belongs to a family of crisp (or hard) clustering
techniques that uniquely partitions the sample data points into disjoint
clusters. The Euclidean distance metric measures how far away a point
is from a cluster center, and thus points that share common attributes
will naturally cohere into the same cluster. All is well and good if the

7.5
Fuzzy Clustering Concepts
■
223
clusters are distinctly arranged in the n-dimensional sample space. In
practice (that is, in reality), however, cluster perimeters are often quite
ambiguous. Data points in the sample space share attributes from one or
more clusters and should thus be placed in more than one cluster (albeit
with a degree of membership in one cluster that is higher or lower than
the degree of membership in another). This brings us to the focus of the
chapter: fuzzy clustering and classiﬁcation.
7.5
Fuzzy Clustering Concepts
A cluster brings together instances in the data that share a common set
of attributes. For each of these clusters a central value representative of
the cluster’s principal value is calculated. This is the center of gravity (or
centroid) of the cluster. An array of these centroids for a collection of
data elements from a database provides the cluster centers, thus telling
us which set of rows in the database are closely related.
In the simplest manifestation, clusters are groups of related data ele-
ments. These groups are deﬁned through a set of attributes. For example,
Figure 7.11 illustrates a clustering of data points for the attributes (X, Y ).
In this example, there are two well-deﬁned clusters. We can visually
pick out the clusters and there are no signiﬁcant outliers (points well out-
side either cluster) or points that might belong ambiguously to one or
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
Figure 7.11
Sample clusters.

224
■
Chapter 7
Fuzzy Clustering
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
P3
P2
P1
Figure 7.12
Clusters with outlier points.
more clusters. The goal of cluster analysis is the recognition and quan-
tiﬁcation of these groupings. This quantiﬁcation involves two processes:
identifying the membership of a data point in some group and locating
the center of the cluster (the centroid). Establishing cluster centers is an
iterative process (described in detail later in the chapter).
Even in situations in which clearly deﬁned clusters exist, the data is
seldom neatly packaged in well-deﬁned groups. As Figure 7.12 shows,
there will often be points (such as P1, P2, and P3) lying well outside the
perimeter of any cluster.
These apparently extraneous data points, clearly belonging to neither
of the clusters, are called outliers. Many crisp clustering techniques have
difﬁculties handling extreme outliers, whereas fuzzy clustering algorithms
tend to give them either very small or a higher than warranted membership
degree in surrounding clusters (this latter effect is common in the fuzzy
c-means algorithm, which we will discuss in some detail).
Real-world data is almost never arranged in such clear-cut groups as
we see in Figure 7.11. Instead, clusters have ill-deﬁned boundaries that
smear into the data space, often overlapping the perimeters of surround-
ing groups. If we move our two clusters toward each other, Figure 7.13
illustrates this problem.
The gray area in Figure 7.12 shows the set of points that could conceiv-
ably belong to either of the two clusters. In conventional crisp techniques
(such as the standard k-means clustering approach), the methodology is
forced to make a difﬁcult decision about which cluster “owns” a point.

7.5
Fuzzy Clustering Concepts
■
225
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
Figure 7.13
Ambiguous data points in two clusters.
In a fuzzy clustering approach, however, points are given partial degrees
of membership in multiple nearby clusters. Figure 7.14 shows how a set
of points, in the overlap area of the two cluster perimeters,2 are partly in
one set and partly in another.
Running a fuzzy clustering process on these data points using the fuzzy
c-means algorithm, we can see the cluster centers as they are actually pro-
duced, as well as the membership values for each of the points. Figure 7.15
is a screen image from the Fuzzy Data Explorer showing the cluster centers
and the points.
There are two cluster centers (C1 and C2) indicated by the squares in
Figure 7.15. Table 7.2 outlines the calculated cluster centers at speciﬁc
points.
A central theme in fuzzy clustering is the often nonunique partition-
ing of the data in a collection of clusters. Data points, speciﬁed as loci
in the multidimensional decision space, are assigned membership values
for each of the clusters. In some cases, this membership may be zero, but
often the membership shows the degree to which the data point is con-
sidered representative of the cluster. Figure 7.16 shows the membership
assignments for some data points in the overlap between two clusters
(see also Figure 7.14).
2 These and all subsequent cluster perimeters are rough outlines of approximately where the
edge of the cluster should lie. They are not intended as exact representations of the cluster area.

226
■
Chapter 7
Fuzzy Clustering
100
y
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
Figure 7.14
Common points in two neighboring clusters.
50
40
30
20
50
40
30
20
100
200
300
400
500
xvar
yvar
600
700
800
Center centroids
Figure 7.15
Output from fuzzy clustering.

7.5
Fuzzy Clustering Concepts
■
227
TABLE 7.2
Fuzzy c-means Cluster Centers
Center
YVAR
XVAR
C1
28.988
286.610
C2
29.592
571.226
100
y
C1
C2
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
(27,378)
[.885, .115]
(35,437)
[.425, .575]
[.444, .556]
(23,435)
[.946, .054]
(18,360)
Figure 7.16
Points with partial degrees of membership.
As Figure 7.16 illustrates, a fuzzy clustering provides a ﬂexible and
robust method of assigning data points into clusters. Each data point
will have an associated degree of membership for each cluster center
in the range [0,1], indicating the strength of its placement in that cluster.
This becomes particularly important for those data points on the border
between clusters. We can imagine the diameter of a cluster space as the
base of a bell-shaped fuzzy set. The membership curve deﬁnes the degree
to which a point is in the cluster. Figure 7.17 illustrates this relationship.
Thus, at the center of the cluster we are also at the apex of the member-
of-cluster fuzzy set, with data points in the region having a membership at
or near [1.0]. As you move left or right away from the center, the degree of
membership for data points drops toward zero (the tails of the curve never
reach zero but become increasingly small). The tails of the fuzzy regions
for neighboring clusters overlap so that data points can be in several
clusters simultaneously, affording us an opportunity to observe the actual
relationship between data elements. Thus, unlike ordinary crisp cluster

228
■
Chapter 7
Fuzzy Clustering
100
y
C
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
1
Degree of
membership µ(x)
Member of cluster
0
Figure 7.17
Fuzzy cluster membership mapping.
techniques we are not forced to make arbitrary classiﬁcation decisions
along cluster boundaries. The softness of the fuzzy clustering technique
gives us a more realistic approach to data analysis.
7.6
Fuzzy c-Means Clustering
The most well-known fuzzy clustering algorithm is fuzzy c-means, a modi-
ﬁcation by Jim Bezdek (see “Further Reading,” Bezdek) of an original
crisp clustering methodology. This initial approach, the hard ISODATA
algorithm, employed a center of means technique, but provided a hard
(or crisp) partitioning of elements into the clusters. Bezdek introduced
the idea of a fuzziﬁcation parameter (m) in the range [1, n], which deter-
mines the degree of fuzziness in the clusters. When m = 1,3 the effect
is a crisp clustering of points, but when m > 1 the degree of fuzziness
3 As you will see when the equations for the fuzzy c-means are discussed, you cannot actually
set m = 1 because this would result in division by zero. We only mean that when m is very near
[1] the clustering provides an almost completely crisp partitioning of the data points.

7.6
Fuzzy c-Means Clustering
■
229
among points in the decision space increases. The fuzzy c-means algo-
rithm itself is fairly straightforward and involves the basic parameters
outlined in Table 7.3. Essentially, the parameter m controls the perme-
ability of the cluster horizon, which can be viewed as an n-dimensional
cloud moving out from a cluster center. Figure 7.18 illustrates clustering
when m = 1.
When m = 1, the clouds do not overlap, but as you increase the value
of m (say, m = 1.25) the clouds begin to overlap and share many of the
TABLE 7.3
Basic Parameters of the Fuzzy c-means Algorithm
x
A vector of training data, where i = 1, 2, . . ., n. These are the cluster attributes
selected from the source data elements (such as columns in a database table).
dij
The distance of the i-th data point from the j-th cluster center. Although we use
the Euclidean distance, the choice of other distance metrics can be easily
incorporated into the fuzzy c-means algorithm.
p
The number of fuzzy clusters speciﬁed as part of the algorithm.
m
A fuzziﬁcation parameter in the range [>1, <w ], indicating the width of the
n-dimensional cluster perimeter. The larger the number the more fuzzy the
point assignments into each cluster. Normally m is the range [1.25, 2 ]
inclusive.
cj
The center (or centroid) of a fuzzy cluster ( j = 1, 2, . . ., p). This value is
repeatedly calculated by the algorithm (see Expression 7.12).
µj(xi)
A fuzzy membership qualiﬁcation indicating the membership of sample xi to the
j-th cluster.
60
50
40
30
20
10
100
200
300
400
500
600
700
800
900
1000
x
y
m = 1
Figure 7.18
Crisp cluster partitioning.

230
■
Chapter 7
Fuzzy Clustering
100
y
m = 1.25
200
300
400
500
600
700
800
900
1000
x
60
50
40
30
20
10
Figure 7.19
Overlapping clusters.
same points. Figure 7.19 illustrates a rough approximation of how two
clusters overlap when the fuzziﬁcation parameter is increased.
The fuzzy c-means algorithm uses a ﬁxed number of cluster centers,
indicated by the parameter ( p). The algorithm, however, does not deter-
mine the number of clusters; this parameter ( p), as well as the fuzziﬁcation
parameter (m), must be supplied as an external constraint on the algo-
rithm behavior. Before discussing the actual behavior of fuzzy c-means,
we need to examine the algorithm itself and review some of its strengths
and limitations.
The Fuzzy c-Means Algorithm
Fuzzy c-means clustering involves two processes: the calculation of clus-
ter centers and the assignment of points to these centers using a form
of Euclidean distance. This process is repeated until the cluster centers
have stabilized. Fuzzy c-means imposes a direct constraint on the fuzzy
membership function associated with each point, as follows.
p

j=1
µj(xi) = 1;
i = 1, 2, 3, . . . , k
(7.11)
Here,
p
is the number of speciﬁed clusters
k
is the number of data points

7.6
Fuzzy c-Means Clustering
■
231
xi
is the i-th data point
µj( )
is a function that returns the membership of xi in the
j-th cluster
That is, the total cluster memberships for a point in the sample or decision
space must add to [1]. As we will see later, this restriction introduces
a major weakness into the algorithm when handling outliers or remote
clusters.
The goal of the fuzzy c-means algorithm is the assignment of data
points into clusters with varying degrees of membership. This member-
ship reﬂects the degree to which the point is more representative of one
cluster than another (or all the others). In effect, we are attempting to
minimize a standard loss function, expressed simply as
l =
p

k=1
n

i=1

µk(xi)
m ∥xi −ck∥2 ,
(7.12)
where:
l
is the minimized loss value
p
is the number of speciﬁed clusters
n
is the number of data points
µk( )
is (as in Expression 7.11) a function that returns the
membership of xi in the k-th cluster
xi
is the i-th data point
m
is the fuzziﬁcation parameter
ck
is the center of the k-th cluster
From this we can derive the two fundamental equations necessary to
implement the fuzzy c-means clustering algorithm. Expression 7.13 is
used to calculate a new cluster center value.
cj =

i

µj(xi)
m xi

i

µj(xi)
m
(7.13)
Here,
cj
is the center of the j-th cluster
µj( )
is (as in Expression 7.12) a function that returns the
membership of xi in the j-th cluster
xi
is the i-th data point
m
is the fuzziﬁcation parameter
You will note that this is simply a special form of weighted average. We
modify the degree of fuzziness in xi’s current membership and multiply

232
■
Chapter 7
Fuzzy Clustering
this by xi. We then divide by the sum of the fuzziﬁed membership. You
will note that we have encountered these operations before. Modifying
the degree of fuzziness is equivalent to applying a form of intensiﬁcation
hedge to the current membership (see Chapter 4). Computing the cen-
troid (or center) of the cluster has the same expression as defuzziﬁcation
(see Chapter 4, section “The Fuzzy Inference Engine”).
Code Preview
Updating the Cluster Centers (C/C++ Code)
Each time we compute membership values for the points, the centroids
(cluster centers) must be recomputed. The algorithm is fairly straightfor-
ward. In order to use this procedure on the ﬁrst iteration, the memberships
for the rows must be initialized (usually to some random value). The
following code shows the procedure for updating the cluster centroids.
void mpUpdateCentroids(int iRows,long
lCentersCnt,long lVarCount,int *ipStatus)
/*--------------------------------------------------*
| Once we have measured all the membership values
|
| (based on the data element’s distance from each
|
| center), we have to go back and recalculate the
|
| cluster centers.
|
*--------------------------------------------------*/
{
int i,j,k,ic;
double dMemTot[FCCENTMAX],
dMem [RDVARMAX],
dX [RDVARMAX];
This code initializes the current cluster centers (centroids) for each vec-
tor (row) to zero. We also set the total membership for the cluster to
zero.
for(ic=0;ic<lCentersCnt;ic++)
{
for(j=0;j<lVarCount;j++)
XFCCtl.dpXFCCenters[ic][j]=0;

7.6
Fuzzy c-Means Clustering
■
233
dMemTot[ic]=0.0;
}
For each vector (row),
we extract the data values and the cur-
rent degree of membership in each of the clusters. From this we
sum the memberships and calculate the proportional degree of the
data space extent based on the membership. This is the nominator
(upper part, for the algebraically challenged) of the center of gravity
calculation.
for(i=0;i<iRows;i++)
{
for(j=0;j<lVarCount;j++)
dX[j]=XFCCtl.dpXFCData[i][j];
for(j=0;j<lCentersCnt;j++)
dMem[j]=XFCCtl.dpXFCDom[i][j];
for(ic=0;ic<lCentersCnt;ic++)
{
dMemTot[ic]+=dMem[ic];
for(k=0;k<lVarCount;k++)
XFCCtl.dpXFCCenters[ic][k]+=dX[k]*dMem[ic];
}
}
To complete the centroid calculation, the nominator of each center
is divided by the sum of the memberships. (For further details, see
Expression 2.18).
for(ic=0;ic<lCentersCnt;ic++)
for(k=0;k<lVarCount;k++)
XFCCtl.dpXFCCenters[ic][k]/=dMemTot[ic];
return;
}
The second step, determining the cluster membership for a sample
point, is only slightly more complicated. We ﬁrst need to know the dis-
tance from a point xi to each of the cluster centers c1...j. This is done, as
illustrated in Expression 7.14, by taking the Euclidean distance between
the point and the cluster center.
dji =
xi −cj
2
(7.14)

234
■
Chapter 7
Fuzzy Clustering
Here,
dji
is the distance of xi from the center of cluster cj
xi
is the i-th data point
cj
is the j-th cluster
(For additional details on calculating the distance metric, see “Cluster
Memberships”.) Because the fuzzy c-means algorithm constrains the total
cluster membership for a point to one [1] (see Expression 7.11), we
calculate a point’s membership as the fractional part of the total possible
memberships assigned to the current point. Expression 7.15 shows how
the membership in the j-th cluster is calculated.
µj(xi) =
 1
dji

1
m−1
p
k=1
 1
dki

1
m−1
(7.15)
Here,
µj(xi)
is the membership of xi in the j-th cluster
dji
is the distance metric for xi in cluster cj (see
Expression 7.14)
m
is the fuzziﬁcation parameter
p
is the number of speciﬁed clusters
dki
the distance metric for xi in cluster ck
As you can see, we take the fractional distance from the point to the cluster
center and make this a fuzzy measurement by raising the fraction to the
inverse fuzziﬁcation parameter. We have seen this operation before. The
inverse power function is equivalent to a dilution hedge (see Expression
4.8). This is divided by the sum of all fractional distances, thereby ensuring
that the sum of all memberships is one [1].
Code Preview
Fuzzy c-Means Clustering Membership Update (C/C++)
The underlying code for the fuzzy c-means clustering method follows the
mathematics of Expression 7.15 quite closely. The following code com-
putes the membership values for each vector from each of the centroids.
Updating the centroids follows this computation and is discussed previously
in the code following Expression 7.13.

7.6
Fuzzy c-Means Clustering
■
235
void mpUpdateFCMembership(double dX[],double
dMem[],long lCentersCnt,long lVarCount)
/*--------------------------------------------------*
| This function updates the cluster centroid
|
| membership value for the current vector. A vector
|
| can belong to multiple clusters simultaneously
|
| (the essence of fuzzy clustering). The distance
|
| from the cluster center is measured using one of
|
| the geometric distance functions (Euclidean is
|
| used by default).
|
*--------------------------------------------------*/
{
int k,ic,iStatus;
double dXDist,
dDist,
dmtemp,
dS[FCCENTMAX];
//
//--------------------------------------------------
//--Now calculate the Euclidean distance from this
//--point to all the other cluster centers and
//--store them for reference.
//--------------------------------------------------
//
for(ic=0;ic<lCentersCnt;ic++)
dS[ic]=FCEucDist(dX,XFCCtl.dpXFCCenters[ic],
(int)lVarCount,&iStatus);
For each of the cluster centers, we retrieve the Euclidean distance for
the current data point (dX[]). If the distance is below some tiny num-
ber (dESP), we know that the current data point lies almost directly
on top of the cluster. We thus set its membership to 1 and all the
others to 0. Otherwise, we compute the membership in each center’s
centroid.
for(ic=0;ic<lCentersCnt;ic++)
{
dDist=dS[ic];
if(dDist<dESP)
{
dMem[ic]=1.0;
for(k=0;k<lCentersCnt;k++)
if(k!=ic) dMem[k]=0.0;
}

236
■
Chapter 7
Fuzzy Clustering
else
{
dmtemp=0.0; //--Set sum to zero
for(k=0;k<lCentersCnt;k++) //--Loop over centroids
{
if(dS[k]==0) dS[k]=dESP+.00001; //--avoid div by zero
dmtemp+=pow(1/dS[k],recmpwm); //--calculate and sum
}
dXDist=pow(1/dS[ic],recmpwm); //--calculate this distance
dMem[ic]=dXDist/dmtemp; //--compute membership
}
}
return;
}
We thus approximate a solution to these equations through an iterative
evaluation of these equations. This results in a somewhat formal statement
of the fuzzy c-means algorithm in the following steps.
Initialize p = number of clusters,
m = fuzziﬁcation parameters
Initialize cj (cluster centers)
Repeat:
For i = 1 to n: Update µj(xi) applying (7.15) with current cj estimates
For j = 1 to p: Update ci applying (7.13) with current µj(xi) estimates
Until cj estimates stabilize
As you can see, the fuzzy c-means algorithm has two loops. The ﬁrst
loop calculates the membership value for each sample point using the
cluster centers. The second loop recalculates the cluster centers using all
new membership values. When the cluster centers stabilize — they fail to
change (or the change falls below a speciﬁed threshold) — the clustering
algorithm is ﬁnished.
Code Review
Fuzzy Clustering: Main Processing Loop (C/C++ Code)
After building the centroid and membership functions, the actual fuzzy
clustering mechanism is quite simple. The following code shows the
core steps in this process. We loop through the data vectors, computing

7.6
Fuzzy c-Means Clustering
■
237
memberships and recalculating centroids until the centroids stabilize. Sta-
bilization means that the change between successive centroids falls below
some arbitrarily small interval. In the following code, the memberships are
calculated for each of the fuzzy s-means (or the fuzzy adaptive technique,
discussed in the next section).
//
//====================================================
//
FUZZY CLUSTERING MAIN PROCESSOR
//====================================================
//
double dP,
dprevP,
dThisMem,
dX [RDVARMAX],
dMem [RDVARMAX],
dMaxMem [RDVARMAX],
dOldMem [RDVARMAX];
mbool bDone;
iLoopCnt=0;
dprevP=0;
bDone=mFALSE;
Until the clustering process converges, the algorithm processes all data
vectors over and over again. Each cycle (iLoopCnt) computes another set
of memberships and another set of cluster centers. The parameter (dP)
measures the change between the maximum change in value among all
cluster centers. If this maximum change settles below (or is equal to the
previous value of) a certain threshold (dESP), the process terminates. In
any case, the process quits after a speciﬁed maximum number of cycles.
while(!bDone)
{
iLoopCnt++;
for(i=0;i<iRows;i++)
{
for(j=0;j<lVarCnt;j++)
dX[j]=XFCCtl.dpXFCData[i][j];//--Get the data vector
for(j=0;j<lCentersCnt;j++)
dOldMem[j]=XFCCtl.dpXFCDom[i][j];//--Get old memberships
if(iThisAlgorithm==FCCMEANS)
mpUpdateFCMembership(dX,dMem,lCentersCnt,lVarCnt);

238
■
Chapter 7
Fuzzy Clustering
if(iThisAlgorithm==FCADAPTIVE)
mpUpdateAFMembership(dX,iRows,dMem,lCentersCnt,lVarCnt);
dP=-1.0e30;
for(ic=0;ic<lCentersCnt;ic++)
{
XFCCtl.dpXFCDom[i][ic]=dMem[ic];
dP=max(dP,fabs(dMem[ic]-dOldMem[ic]));
}
}
mpUpdateCentroids(iRows,lCentersCnt,lVarCnt,ipStatus);
fprintf(fpCenters,"\n%s%9d\n%s%9.4f\n%s%9.4f\n",
"Cluster Centers. After Cycle: ",iLoopCnt,
"Convergence Delta : ",dP,
"Previous Delta : ",dprevP);
FCDisplayCenters(sFCTitle,ipStatus);
bDone=(dP<dESP)||(iLoopCnt>iMaxLoops)||(dP==dprevP);
dprevP=dP;
}
Stripped down to its bare essentials, fuzzy c-means clustering consists
of an iterative process converging on a set of cluster centers and the
memberships in these clusters for each data vector.
The Behavior of Fuzzy c-Means
The fuzzy c-means approach to clustering suffers from several interre-
lated constraints that affect its overall performance. The chief behavior
shortcoming is the restriction that the sum of the cluster memberships
for a sample data point must equal [1]. This is, in effect, a probabilistic
approach to fuzzy clustering, relating the membership in each cluster as
a fraction of the total possible membership in any of the clusters. One
evident behavior from this approach is the assignment of membership
values to outlier data points. The fuzzy c-means algorithm often assigns
high degrees of membership to these atypical outlier points. Figure 7.20
illustrates this problem.
There are two cluster centers (C1 and C2) indicated by the squares in
Figure 7.20. Table 7.4 outlines the calculated cluster centers at speciﬁc
points.
Although the two outlier points (P1 and P2) are obviously outside
either cluster, they have considerably high membership degrees in both
clusters than is warranted by their locations in the sample space. Point P1,
for example, has µ[.844] in cluster center C1 and point P2 has µ[.793] in

7.6
Fuzzy c-Means Clustering
■
239
70
60
50
40
30
20
70
60
50
40
30
20
100
200
300
400
500
xvar
yvar
600
700
800
(55,400)
[.844,.156]
P1
P2
(60,450)
[.207,.793]
Center centroids
Figure 7.20
Outlier points with high membership values.
TABLE 7.4
Fuzzy c-Means Cluster Centers
Center
YVAR
XVAR
C1
29.589
276.723
C2
31.137
561.048
cluster center C2. This results from the cluster optimization constraint in
Expression 7.11.
A further related outcome behavior is tied to the constraint imposed
by Expression 7.11 – the membership of a data point in the sample
space depends directly on the membership values associated with all
other clusters. This means that the point assignment membership is
transitively dependent on specifying the correct number of clusters.
Figure 7.21 illustrates this problem by introducing a third cluster just
above and between the two existing clusters.

240
■
Chapter 7
Fuzzy Clustering
100
90
80
70
60
50
40
30
20
100
90
80
70
60
50
40
30
20
100
200
300
400
500
xvar
yvar
600
700
800
Center centroids
Figure 7.21
Clustering with inaccurate cluster count.
TABLE 7.5
Fuzzy c-Means Cluster Centers
Center
YVAR
XVAR
C1
42.884
314.147
C2
43.780
557.391
Here we see that the two cluster centers (C1 and C2), indicated by the
squares in Figure 7.20, have been pulled upward, inﬂuenced by the new
cluster points (which have been assigned as part of clusters C1 and C2).
Table 7.5 shows the calculated cluster centers at speciﬁc points.
Another behavior problem, albeit common to most ordinary cluster-
ing techniques, is the spherical nature of the fuzzy c-means algorithm.
The radius of the cluster sweeps out from the center at a constant dis-
tance. This approach means that the fuzzy c-means cannot directly model
clusters that have ellipsoidal or arbitrarily irregular shapes. In general,
this makes fuzzy c-means inappropriate for pattern recognition of images

7.6
Fuzzy c-Means Clustering
■
241
but does not generally impede its utility as a clustering mechanism for
multidimensional data.
Initial Clusters and Cluster Seeds
How do we go about assessing the number of clusters hidden in our data?
Very often in the analysis of large databases — in fact, even in the analysis
of rather small databases or spreadsheets — the actual or initial number
of clusters is unknown. When this occurs, the knowledge engineer or
business analyst is left with several alternatives: heuristically explore the
data, statistically model the data, or turn to validity measure functions to
ﬁnd the optimal cluster number. (Exploring the use of validity functions
is beyond the scope of this chapter. For a complete treatment of their
construction and use see the “Further Reading” section, Hoppner.)4
Determining a Cluster Count Estimate
In most cases we can heuristically arrive at a cluster number simply by
understanding the data and starting with a reasonably large number of
cluster centers. If the center count (N) approaches the number of data
points, we wind up classifying each record (vector) as its own center
(thus forfeiting any generality). On the other hand, the cluster count (N)
should never be less than 2 (this is self-evident, in that a cluster count
of 0 or 1 would provide no segmentation). For a reasonable selection
of clusters, if the absolute difference between centers is less than some
threshold measure (θ) we can reduce the total number of centers by 1. A
simple algorithm for this is as follows.
Cfinal = k
For each Ci, i=1 to k-1
If diff(Ci,Ci+1)<θ then
Cfinal=Cfinal-1
End if
k = Cfinal
We start with an arbitrary cluster count (k). After a cluster analysis, we
set Cﬁnal equal to our previous estimate (k) and run this algorithm to ﬁnd
4 The combination of hedges to generate a set of highly speciﬁc rules is often a complex under-
taking because a more precise rule might be If a is somewhat around C1(a) and b is very
around C1(b) then c is quite near C1(c). Hence, rule production is an ideal application area
for tuning using a genetic algorithm.

242
■
Chapter 7
Fuzzy Clustering
those centers that are signiﬁcantly different. The count (k) is set to the
recomputer cluster count and the cluster analysis is rerun. This process
can be repeated several times.
A preliminary statistical analysis of the data can also aid in selecting the
number of cluster centers. One way to view the probable clustering of
multidimensional data is through the intersection of the attribute modes.5
We would expect clusters to form at these points (or regions, in that a
mode can also be a plateau of equal values). Expression 7.16 provides a
simple way to count centers.
Ccnt =
N

i=1
mi + 1
(7.16)
Here,
Ccnt
is the cluster estimate
N
is the number of data attributes
mi
is the number of modes in the attribute’s statistical
distribution
The expression adds the number of modes occurring in each data attribute
and then adds 1 to the ﬁnal count. If all data attributes are unimodal, the
cluster count will be the number of attributes plus 1. If one or more of
the attributes are multimodal, the count will reﬂect a slightly high count to
account for this data topology. Some care must be used in the deﬁnition of
“mode” when accounting for frequency distributions in the data. Consider
the data distribution for the credit load attribute shown in Figure 7.22.
In this example, 575 is the mode because it is the most frequently
occurring value. However, the data is distinctly multimodal at the 780
and somewhat at the 925 data value. In cases such as this we need to
deﬁne the mode in such a way that data sets with mutlimodal points can be
recognized. Because this is a heuristic method of arriving at cluster counts,
we need not resort to elaborate mathematical analysis. Rather, ordering
the data histograms for an attribute and checking for frequencies that are
within some percentage (W) of the actual mode is usually sufﬁcient.
Seeding Cluster Centers
The second step in the fuzzy c-means algorithm indicates that the cluster
centers (cj, j = 1 to p) should be initialized. Initialization takes the form
5 The mode is the most frequently occurring value in a set of observations.

7.6
Fuzzy c-Means Clustering
■
243
Frequency
Credit load ($×00)
100
200
300
400
500
600
700
900 1000
800
1100 1200 1300
0
10,000
Credit_Load Distribution
Figure 7.22
Multimodal data.
for(i=0;i<lCentersCnt;i++)
for(j=0;j<lVarCnt;j++)
{
dLo=dVarLo[j];
dHi=dVarHi[j];
ic=GARandomNumber((int)dLo,(int)dHi);
XFCCtl.dpXFCCenters[j][i]=ic;
}
Listing 7.2
Cluster center initialization.
of supplying values for the cluster centers. There are two general data-
independent methods of initializing the clusters: uniform random values
selected from the attribute’s range and statistically random values cen-
tered about the attribute’s mean or the attribute’s mode. Although there
is a body of literature addressing cluster seeding, from a practical stand-
point there does not appear to be much difference in convergence speed
and robustness between any of the techniques.
The fuzzy c-means algorithm employs a uniform random number
approach to cluster seeding. Listing 7.2 shows the associated code. This

244
■
Chapter 7
Fuzzy Clustering
code fragment requires just a little explanation. A cluster center, of course,
is a vector of attribute (or variable) values, one for each dimension in
the sample space. Thus, for each cluster and for each cluster attribute
(lVarCnt) we generate a random number between the low and high val-
ues of the variable’s range. This value is inserted in the cluster centers
(dpCFCCenters[][]) component of the fuzzy clustering external control
block.
A third seeding method, often used when processing large databases,
draws the cluster values by randomly reading records from the database.
Typically, we can read the ﬁrst p-records, for p cluster centers, and use
their column (attribute) values as the seed. Where the data may itself be in
some order, we can generate p-random record numbers, use direct access
to retrieve these records, and create seeds from their data value. In cases
in which the clustering technique might be sensitive to the initial seed
values (as in the case of the crisp k-means clustering method), this has the
virtue of populating the clusters with actual data values (using random
numbers across the complete range of a variable runs the small but real
risk of seeding the clusters with outlier data values).
Fuzzy c-Means Clustering Example
To illustrate how fuzzy c-means clustering works, we will use a pub-
lic database of automobile property information. Part of this database is
provided as Listing 7.3.
In this example, we will cluster the two attributes WGT (weight) and
ACCEL (acceleration). With a fuzziﬁcation factor of [1.25], Figure 7.23
shows the cluster centers.
The squares in Figure 7.23 are the centroid centers. Each data point
is assigned a membership in one or both of these centroids. Listings 7.4
and 7.5 show the iterative process of deriving the ﬁnal cluster centers.
Table 7.7 shows the initial cluster centers generated randomly from the
ranges of the acceleration and weight variables. Listing 7.5 shows the
cluster generation process.
In this example, the fuzzy c-means algorithm took three cycles to
converge on the ﬁnal cluster centers (that is, until the converge delta fell
below the limiting threshold, the EPS limit). Listing 7.6 shows the cluster
assignments for a few of the records.
In this example, some vectors have partial memberships, whereas
others are completely assigned to one or the other cluster center. You
will observe that the total degree of membership in all clusters adds to
one [1.0].

7.6
Fuzzy c-Means Clustering
■
245
MPG, CYLS, DISP,
HP,
WGT,
ACCEL, YR, ORIGIN, CAR_NAME
18.0
8
307.0
130.0
3504. 12.0
70 1 "chevrolet chevelle malibu"
15.0
8
350.0
165.0
3693. 11.5
70 1 "buick skylark 320"
18.0
8
318.0
150.0
3436. 11.0
70 1 "plymouth satellite"
16.0
8
304.0
150.0
3433. 12.0
70 1 "amc rebel sst"
17.0
8
302.0
140.0
3449. 10.5
70 1 "ford torino"
15.0
8
429.0
198.0
4341. 10.0
70 1 "ford galaxie 500"
14.0
8
454.0
220.0
4354.
9.0
70 1 "chevrolet impala"
14.0
8
440.0
215.0
4312.
8.5
70 1 "plymouth fury iii"
14.0
8
455.0
225.0
4425. 10.0
70 1 "pontiac catalina"
15.0
8
390.0
190.0
3850.
8.5
70 1 "amc ambassador dpl"
15.0
8
383.0
170.0
3563. 10.0
70 1 "dodge challenger se"
14.0
8
340.0
160.0
3609.
8.0
70 1 "plymouth ’cuda 340"
15.0
8
400.0
150.0
3761.
9.5
70 1 "chevrolet monte carlo"
14.0
8
455.0
225.0
3086. 10.0
70 1 "buick estate wagon (sw)"
24.0
4
113.0
95.00
2372. 15.0
70 3 "toyota corona mark ii"
22.0
6
198.0
95.00
2833. 15.5
70 1 "plymouth duster"
18.0
6
199.0
97.00
2774. 15.5
70 1 "amc hornet"
21.0
6
200.0
85.00
2587. 16.0
70 1 "ford maverick"
Listing 7.3
Automobile Properties Database
30
20
10
0
30
20
10
0
0
1000
2000
3000
4000
5000
6000
WGT
Accel
Center centroids
(16.28,2426.57)
(14.35,3944.09)
Figure 7.23
WeightxAcceleration clusters (factor = 1.25).

246
■
Chapter 7
Fuzzy Clustering
---------------------------------------------------------
CENTROIDS Initial (Random) Centers
---------------------------------------------------------
ACCEL
WGT
0.
6.000
1379.000
1.
5.000
817.000
CLUSTERING CONTROLS
ESP
:
0.0500
Max Cycles
:
100
Listing 7.4
The Cluster Center Initialization
Cluster Centers. After Cycle:
1
Convergence Delta
:
0.9426
Previous
Delta
:
0.0000
---------------------------------------------------------
CENTROIDS FUZZY C-MEANS
---------------------------------------------------------
ACCEL
WGT
0.
15.667
2906.164
1.
14.695
3693.037
Cluster Centers. After Cycle:
2
Convergence Delta
:
0.0574
Previous
Delta
:
0.9426
---------------------------------------------------------
CENTROIDS FUZZY C-MEANS
---------------------------------------------------------
ACCEL
WGT
0.
16.315
2458.143
1.
14.155
3980.714
Cluster Centers. After Cycle:
3
Convergence Delta
:
0.0000
Previous
Delta
:
0.0574
Listing 7.5
The Cluster Generation Process

7.6
Fuzzy c-Means Clustering
■
247
---------------------------------------------------------
CENTROIDS FUZZY C-MEANS
---------------------------------------------------------
ACCEL
WGT
0.
16.282
2426.567
1.
14.347
3944.088
Final Cluster Centers
---------------------------------------------------------
CENTROIDS FUZZY C-MEANS
---------------------------------------------------------
ACCEL
WGT
0.
16.282
2426.567
1.
14.347
3944.088
Listing 7.6
The Cluster Generation Process
CLUSTER ASSIGNMENTS: FUZZY C-MEANS
---------------------------------------------------------
CLUSTER ASSIGNMENTS: FUZZY C-MEANS
---------------------------------------------------------
ACCEL
WGT
0
1
0.
12.000
3504.000
0.002
0.998
1.
12.000
3504.000
0.002
0.998
2.
11.000
3436.000
0.009
0.991
7.
11.000
3436.000
0.009
0.991
7.
10.500
3449.000
0.007
0.993
5.
10.500
3449.000
0.007
0.993
6.
9.000
4354.000
0.000
1.000
7.
9.000
4354.000
0.000
1.000
8.
10.000
4425.000
0.000
1.000
9.
10.000
4425.000
0.000
1.000
10.
10.000
3563.000
0.000
1.000
11.
10.000
3563.000
0.000
1.000
12.
9.500
3761.000
0.000
1.000
13.
9.500
3761.000
0.000
1.000
14.
15.000
2372.000
1.000
0.000
15.
15.000
2372.000
1.000
0.000
16.
15.500
2774.000
1.000
0.000
17.
15.500
2774.000
1.000
0.000
Listing 7.7
WeightxAcceleration Cluster Assignments

248
■
Chapter 7
Fuzzy Clustering
7.7
Fuzzy Adaptive Clustering
Many of the behavioral problems with the standard fuzzy c-means algo-
rithm are eliminated when we relax the probabilistic constraint imposed
by Expression 7.10. Such an approach has been developed by Krishnapu-
ram and Keller (see “Further Reading”) and later expanded by Young-Jun
Lee (see “Further Reading”) in his doctoral thesis. In fuzzy adaptive
clustering, the constraint on data point memberships is imposed by
Expression 7.17.
p

j=1
n

i=1
µj(xi) = n
(7.17)
Here,
µj(xi)
is the membership of xi in the j-th cluster
p
is the number of speciﬁed clusters
n
is the number of data points
With this more relaxed restriction in hand, the total membership quan-
tiﬁers for all sample points equal n (the sample size). This is a much
more ﬂexible approach to the clustering optimization problem, provid-
ing a way of signiﬁcantly improving cluster robustness. It is in this sense
that the algorithm is adaptive; that is, its membership regime is based on
the sample size rather than an arbitrary upper limit (such as one [1] in
fuzzy c-means clustering). Naturally, the individual membership values
no longer fall within the conventional fuzzy membership interval [0,1],
but a simple renormalization of the memberships can remap the member-
ships into this range. Also, even when normalized the sum of the cluster
memberships for a vector will not usually sum to [1.0].
The Adaptive Fuzzy Algorithm
Like the fuzzy c-means algorithm, the adaptive algorithm is fairly straight-
forward and involves the basic parameters (most of which are the same as
the fuzzy c-means approach) outlined in Table 7.6. The goal of the adap-
tive algorithm, like all fuzzy clustering processes, is the assignment of data
points into clusters with varying degrees of membership. This member-
ship reﬂects the degree to which the point is representative of a cluster,
and because we are no longer constrained to the probabilistic limit of

7.7
Fuzzy Adaptive Clustering
■
249
TABLE 7.6
Basic Parameters of the Adaptive Algorithm
xi
A vector of training data, where i = 1, 2, . . ., n. These are the cluster attributes
selected from the source data elements (such as columns in a database table).
dij
The distance of the i-th data point from the j-th cluster center. Although we use
the Euclidean distance, the choice of other distance metrics can be easily
incorporated into the fuzzy adaptive algorithm.
p
The number of fuzzy clusters speciﬁed as part of the algorithm.
m
A fuzziﬁcation parameter in the range [>1,<w] indicating the width of the
n-dimensional cluster perimeter. The larger the number the more fuzzy the
point assignments into each cluster. Normally, m is the range [1.25,2]
inclusive.
α
The alpha threshold used to limit the effects of exponential smoothing on ﬁnal
membership values (a process called dilation). This is applied only after
membership normalization.
h
The exponent or power parameter controlling the degree of smoothness applied
to the membership possibility function.
cj
The center (or centroid) of a fuzzy cluster (j = 1, 2, . . ., p). This value is
repeatedly calculated by the algorithm (see Expression 7.13).
µj(xi)
A fuzzy membership qualiﬁcation indicating the membership of sample xi to the
j-th cluster.
fuzzy c-means (see Expression 7.10) each membership is independent of
any membership in another cluster.
This principal difference between fuzzy adaptive and c-means clus-
tering is the very weak constraint imposed by Expression 7.20. The
expression used to calculate new cluster centers, however, is the same
as the fuzzy c-means, as follows.
cj =

i

µj(xi)
m xi

i

µj(xi)
m
(7.20)
Here,
cj
is the center of the j-th cluster
µj( )
is, as in Expression 7.12, a function that returns the
membership of xi in the j-th cluster
xi
is the i-th data point
m
is the fuzziﬁcation parameter
The second step, determining the fuzzy membership, uses the same type
of distance metric (see Expression 7.14), with a change in the way the

250
■
Chapter 7
Fuzzy Clustering
ratios are calculated. This is represented by Expression 7.21.
µj(xi) =
n
 1
dji

1
m−1
p
k=1
n
z=1
 1
dkz

1
m−1
(7.21)
Here,
µj(xi)
is the membership of xi in the j-th cluster
dji
is the distance metric for xi in cluster cj (see
Expression 7.14)
m
is the fuzziﬁcation parameter
p
is the number of speciﬁed clusters
n
is the number of data points
dkz
is the distance metric for xz in cluster ck
Like fuzzy c-means, we calculate the inverse of the distance of the i-th
point from the center of the j-th cluster, and use our fuzziﬁer value to
increase or decrease the spread. In this case, however, the membership
in the j-th cluster is the fraction of the data point’s distance among the
distance of all points in each of the clusters.
Code Preview
Adaptive Fuzzy Clustering Membership Update (C/C++)
The underlying code for the adaptive fuzzy clustering method is not much
more complicated than the fuzzy c-means algorithms. The fuzziﬁer, in both
methods, is the same. The procedure shown in Listing 7.8 computes the
membership values for each vector from each of the centroids. Updating
the centroids is the same as the fuzzy c-means.
The cluster membership values developed by the adaptive fuzzy method
are greater than zero (for assignment), but the maximum is not lim-
ited to [1]. When conventional fuzzy membership distributions are
required, these can be generated by the dual process of normalization and
hedge dilution. Normalization ﬁnds the maximum membership among all
clusters and rescales the memberships from this maximum.
µnorm
ik
(xi) =
µold
ik (xi)
max(µold
k );
i = 1 to n;
k = 1 to p
(7.22)

7.7
Fuzzy Adaptive Clustering
■
251
Here,
mnorm
jk
(xi)
is the normalized membership of xi in the k-th cluster
mold
jk (xi)
is the old (or original) membership
p
is the number of speciﬁed clusters
n
is the number of data points
max( )
returns the maximum membership value in the k-th cluster
Normalization divides each of the fuzzy membership values for a sample
point by the maximum membership. This sets the maximum to [1] and
proportionally readjusts the rest of the memberships.
void mpUpdateAFMembership(
double dX[],int iRows,double dMem[],long lCentersCnt,long lVarCount)
/*----------------------------------------------------------------------*
| This function updates the cluster centroid membership value for the
|
| current vector. A vector can belong to multiple clusters
|
| simultaneously (the essence of fuzzy clustering). The distance from
|
| the cluster center is measured using one of the geometric distance
|
| functions (Euclidean is used by default)
|
*----------------------------------------------------------------------*/
{
int j,k,m,z,ic,iStatus;
double dXDist,
dDist,
dmtemp,
dctemp,
dS[FCCENTMAX];
//
//----------------------------------------------------------------------
//--Now calculate the Euclidean distance from this point to all
//--the other cluster centers and store them for reference.
//----------------------------------------------------------------------
//
for(ic=0;ic<lCentersCnt;ic++)
dS[ic]=FCEucDist(dX,XFCCtl.dpXFCCenters[ic],(int)lVarCount,&iStatus);
//
//----------------------------------------------------------------------
//--Part One. Compute divisor. This is the sum of the sum of the
//--fuzzified inverse distance between each data point and each
//--of the clusters.
Listing 7.8
Adaptive clustering membership update.

252
■
Chapter 7
Fuzzy Clustering
//
// lCentersCnt iRows
// Sum
Sum
(1/dS[k])**(1/m)
// k=0
z=0
//----------------------------------------------------------------------
//
dctemp=0;
//--Set sum of sum to zero
for(k=0;k<lCentersCnt;k++)
//--For each Cluster
{
dmtemp=0.0;
//--Set sum to zero
for(z=0;z<iRows;z++)
//--For each sample point
{
dDist=pow((1.00/dS[k]),recmpwm);
//--Calculate distances
dmtemp+=dDist;
//--Sum all distances
}
dctemp+=dmtemp;
//--Sum k-th center distances
}
//
//----------------------------------------------------------------------
//–Part Two. Compute memberships.
//----------------------------------------------------------------------
//
for(ic=0;ic<lCentersCnt;ic++)
{
dXDist=pow(1.00/dS[ic],recmpwm);
//--calculate distance
dMem[ic]=(((double)iRows)*dXDist)/dctemp;
//--compute membership
}
return;
}
Listing 7.8
Continued.
Code Preview
Fuzzy Adaptive Membership Normalization
Normalization can be a compute-intensive process for a large data ﬁle
because we must scan each cluster center column to ﬁnd its maximum
values and then loop back across the memberships, dividing each by the
maximum membership value in that cluster. Normalization is often impor-
tant, however, in making decisions about relative memberships. Listing 7.9
shows the code segment for membership normalization.

7.7
Fuzzy Adaptive Clustering
■
253
//
//----------------------------------------------------------------------
//--If this is the adaptive method then we want to normalize
//--the cluster memberships for each record. We also want to
//--see if the smoothing function should be applied.
//----------------------------------------------------------------------
//
if(iThisAlgorithm==FCADAPTIVE)
if(bNormalize)
{
for(i=0;i<RDVARMAX;i++) dMaxMem[i]=0;
//
//----------------------------------------------------------------
//--Now we roll through the cluster memberships and find the
//--maximum memberships in each cluster column. We need this
//--in order to normalize the membership functions.
//----------------------------------------------------------------
//
for(i=0;i<iRows;i++)
{
for(j=0;j<lCentersCnt;j++)
{
dThisMem=XFCCtl.dpXFCDom[i][j];
if(dThisMem>dMaxMem[j]) dMaxMem[j]=dThisMem;
}
}
//
//----------------------------------------------------------------
//--Check to make sure that the maximum of each cluster is
//--greater than zero [0]. This would result in a division
//--by zero condition when we go to normalize.
//----------------------------------------------------------------
//
for(j=0;j<lCentersCnt;j++)
if(dMaxMem[j]==0)
{
*ipStatus=9;
sprintf(sStrBuff,"%s%5d","Cluster=",j);
MtsSendError(405,sPgmId,sStrBuff);
return;
}
//
Listing 7.9
Adaptive clustering membership normalization.

254
■
Chapter 7
Fuzzy Clustering
//----------------------------------------------------------------
//--Now we have the maximum membership for each of the cluster
//--centers. We must now roll back through the records and
//--normalize the memberships by dividing each of the member-
//--ships by the maximum amount.
//----------------------------------------------------------------
//
for(i=0;i<iRows;i++)
{
for(j=0;j<lCentersCnt;j++)
{
XFCCtl.dpXFCDom[i][j]=XFCCtl.dpXFCDom[i][j]/dMaxMem[j];
//
//----------------------------------------------------------
//--Since dilation is applied after normalization and
//--is independent of any other vector, we now apply the
//--dilation smoothing operator.
//----------------------------------------------------------
if(bDilate)
XFCCtl.dpXFCDom[i][j]=pow(XFCCtl.dpXFCDom[i][j],fDPower);
}
}
}
Listing 7.9
Continued.
Dilation smoothes the fuzzy possibility distribution associated with a
sample, and is applied as the conventional fuzzy dilution hedge, as in
Expression 7.23.
µnew
ik
(xi) = [µnorm
ik
(xi)]
1
h ;
h > 1
(7.23)
Here,
mnew
ik
(xi)
is the new smoothed membership of xi in the k-th cluster
mnorm
ik
(xi)
is the normalized membership of xi in the k-th cluster
h
is the smoothing (or dilation) parameter
The dilution hedge must be applied only to attribute vectors that have a
set of membership values above some arbitrary alpha threshold (α) limit.
Generally, the alpha threshold is set to an application-speciﬁc limit (such
as α = .01). As a usual policy, the standard somewhat dilution hedge is

7.7
Fuzzy Adaptive Clustering
■
255
used (thus h = 2). Refer to Listing 7.9 for the way in which dilation is
integrated into the adaptive clustering normalization process.
We thus approximate a solution to the fuzzy adaptive clustering
through an iterative evaluation of Expressions 7.20 and 7.21. This results
in a somewhat formal statement of the fuzzy adaptive algorithm in the
following steps.
Initialize p = Number of Clusters,
m = fuzziﬁcation parameters
α = alpha cut threshold
h = dilation exponent
Initialize cj (cluster centers)
Repeat:
For i = 1 to n: Update µj(xi) applying (7.21) with current cj estimates
For j = 1 to p: Update ci applying (7.20) with current µj(xi) estimates
Until cj estimates stabilize
If fuzzy properties are needed
For i = 1 to n:
Normalize µ(xi) applying (7.22)
Smooth µ(xi) with dilation operation applying (7.23)
end For
End If
As you can see, the adaptive fuzzy algorithm, like fuzzy c-means, has two
loops. The ﬁrst loop calculates the membership value for each sample
point using the distance to the cluster centers for all sample data points.
The second loop recalculates the cluster centers using all new member-
ship values. When the cluster centers stabilize — they fail to change (or
the change falls below a speciﬁed threshold) — the clustering algorithm
is ﬁnished.
Fuzzy Adaptive Clustering Example
Using the same public database of automobile property information we
employed to illustrate fuzzy c-means (see Figure 7.22), we now turn to the
adaptive fuzzy clustering technique. In this example, we also cluster the
two attributes WGT (weight) and ACCEL (acceleration). The fuzziﬁcation
factor is [1.25]. Figure 7.24 shows the clusters and the cluster centers.
The small squares in Figure 7.24 are the centroid centers. Each data
point is assigned a membership in one or both of these centroids. Unlike
the fuzzy c-means algorithm, the total membership values for a particular
data point do not necessarily sum to one [1.0]. Listings 7.10 and 7.11
show the iterative process of deriving the ﬁnal cluster centers.

256
■
Chapter 7
Fuzzy Clustering
30
20
10
0
30
20
10
0
0
1000
2000
3000
4000
5000
6000
WGT
Accel
Center centroids
(16.28,2426.57)
(14.35,3944.09)
Figure 7.24
WeightxAcceleration clusters (factor = 1.25).
---------------------------------------------------------
CENTROIDS Initial (Random) Centers
---------------------------------------------------------
ACCEL
WGT
0.
-6.000
1379.000
1.
-5.000
817.000
CLUSTERING CONTROLS
ESP
:
0.0500
Max Cycles
:
100
Listing 7.10
The Cluster Center Initialization

7.7
Fuzzy Adaptive Clustering
■
257
Cluster Centers. After Cycle:
1
Convergence Delta
:
0.9426
Previous
Delta
:
0.0000
---------------------------------------------------------
CENTROIDS FUZZY ADAPTIVE
---------------------------------------------------------
ACCEL
WGT
0.
15.667
2906.164
1.
14.695
3693.037
Cluster Centers. After Cycle:
2
Convergence Delta
:
0.0574
Previous Delta
:
0.9426
---------------------------------------------------------
CENTROIDS FUZZY ADAPTIVE
---------------------------------------------------------
ACCEL
WGT
0.
16.315
2458.143
1.
14.155
3980.714
Cluster Centers. After Cycle:
3
Convergence Delta
:
0.0000
Previous Delta
:
0.0574
---------------------------------------------------------
CENTROIDS FUZZY ADAPTIVE
---------------------------------------------------------
ACCEL
WGT
0.
16.282
2426.567
1.
14.347
3944.088
Final Cluster Centers
Cluster Centers. After Cycle:
3
Convergence Delta
:
0.0000
Previous Delta
:
0.0000
---------------------------------------------------------
CENTROIDS FUZZY ADAPTIVE
---------------------------------------------------------
ACCEL
WGT
0.
16.282
2426.567
1.
14.347
3944.088
Listing 7.11
The Cluster Generation Process

258
■
Chapter 7
Fuzzy Clustering
TABLE 7.7
WeightxAcceleration Cluster Assignments
CLUSTER ASSIGNMENTS: ADAPTIVE FUZZY
--------------------------------------------------------
CLUSTER ASSIGNMENTS: FUZZY ADAPTIVE
--------------------------------------------------------
ACCEL
WGT
0
1
0.
12.000
3504.000
0.043
0.999
1.
12.000
3504.000
0.043
0.999
2.
11.000
3436.000
0.096
0.995
7.
11.000
3436.000
0.096
0.995
7.
10.500
3449.000
0.083
0.997
5.
10.500
3449.000
0.083
0.997
6.
9.000
4354.000
0.002
1.000
7.
9.000
4354.000
0.002
1.000
8.
10.000
4425.000
0.003
1.000
9.
10.000
4425.000
0.003
1.000
10.
10.000
3563.000
0.020
1.000
11.
10.000
3563.000
0.020
1.000
12.
9.500
3761.000
0.001
1.000
13.
9.500
3761.000
0.001
1.000
14.
15.000
2372.000
1.000
0.000
15.
15.000
2372.000
1.000
0.000
16.
15.500
2774.000
1.000
0.005
17.
15.500
2774.000
1.000
0.005
Like the fuzzy c-means example, the adaptive clustering process took
three cycles to converge on the ﬁnal cluster centers. Table 7.7 contains
the ﬁrst few records from the automobile properties database, with their
cluster assignments. Unlike the fuzzy c-means, we can see that the mem-
bership values reﬂect cluster membership degrees that are independent
of other membership degrees for the same data point. As with all other
clustering algorithms, the results of the adaptive fuzzy cluster depend on
reasonable and workable initialization values. You should note, however,
that the adaptive technique is particularly robust in respect to noise (as
well as outliers, which are often forms of noise). Because the entire data
space is considered in determining the membership gradients for each
point, outliers and erroneous data points do not usually have a signiﬁcant
impact on the cluster memberships assigned to other data points. This
makes the adaptive fuzzy clustering approach particularly important as
a tool in exploratory data mining and applications of statistical learning
theory.

7.8
Generating Rule Prototypes
■
259
7.8
Generating Rule Prototypes
There are a wide variety of methods for converting clusters into rules (see
Chapter 8). Many of these approaches attempt to generate rules from
fuzzy clusters through a reductionist approach that treats the clusters
and the data cloud around them as binary classiﬁcation points. Much of
the underlying algorithmic work is concerned with inducing a member-
ship framework from the cluster centers so that the control rules can be
induced. These approaches generally ignore the more straightforward use
of approximation hedges to convert cluster centroids into fuzzy numbers
with ﬁnely tuned expectancy (width) values. By using hedges we can
treat one dimension of the data space as an outcome and the remaining
dimensions as rule predicates.
Cluster Centers as Fuzzy Numbers
This approach is relatively easy to see when we treat the center of a
cluster as the center of a bell-shaped fuzzy set. The center is, in effect,
considered a fuzzy number. The closer a point is to the center of the
cluster the higher its membership in the center’s fuzzy set. For example,
Figure 7.25 illustrates the fuzzy set that encompasses cluster C1 with one
dimension of the center having a value of 20.
Grade of membership m(x)
Expectancy
8
10
12
14
16
18
20
22
24
26
28
30
32
0
1.0
C1
χ
χ χ χχ
χχ χ
χ
χ χ
χ
χ
χ
χ χ χ χ χ χ χ
χ χ χ
χ χ χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
χ
Figure 7.25
The fuzzy set representing a fuzzy cluster center.

260
■
Chapter 7
Fuzzy Clustering
In Figure 7.25 we can see that data points are assigned membership
values in the cluster depending on the shape and expectancy (width)
of the fuzzy set. In this approach, the actual degree of membership in
the cluster, computed by the clustering algorithm, is essentially lost. The
initial membership grade is simply used to determine cluster membership
(and to assign, when necessary, points to multiple clusters). Once a cluster
is identiﬁed, we know empirically that the membership values drop off
as we move out from the cluster center.
Creating the Rules
If the data has N dimensions and there are K clusters, we can induce K
rules with N-1 predicates. That is, each cluster forms a fuzzy rule in the
data classiﬁcation space. To illustrate, let a, b, and c be data vectors. Let
C1(ai, bi, ci) and C2(aj, bj, cj) be the centers (centroids) of the clustering
of (a, b, c). From the clustering we can induce the following rules.
If a is about C1(a) and b is about C1(b), then c is near C1(c).
If a is about C2(a) and b is about C2(b), then c is near C1(c).
The expectancy of the about hedge reﬂects the compactness of the clus-
ter. Compact clusters have smaller (narrower) expectancies, whereas less
compact clusters have larger (wider) expectancies. How do we formulate
a compactness indicator?
■
We start by determining the diffusion of the data points around the
cluster center. This is done by computing the root mean squared
(RMS) error, as shown in Expression 7.24.
e =



 1
N
N

i=1
(xi −xCk)2
(7.24)
Here,
e
is the root mean squared error of the distribution
N
is the number of points in the cluster
xi
is the i-th data point in the k-th cluster
xC(k)
is the centroid value of the k-th cluster
The root mean squared error is the average of the difference between
each point in the cluster and the value of the cluster center. It is the
core measure of compactness. The smaller the RMS error the more
compact the cluster.

7.8
Generating Rule Prototypes
■
261
■
Second, we convert the RMS error into a measure of dispersion. This
is used to compute the expectancy of the fuzzy bell curve around the
cluster’s center. The dispersion rate, as shown in Expression 7.25, is
the ratio of the RMS error to the range of data points in the cluster.
dc =
e
xmax −xmin
(7.25)
Here,
dc
is the dispersion rate of the elements in the cluster
e
is the root mean squared error (see Expression 7.24)
xmax
is the maximum value of x in the cluster
xmin
is the minimum value of x in the cluster
Notice that the error will grow from zero out toward the average dis-
tance between the center and the data points. For a cluster that is
tightly compacted, e will be small relative to the range. For a very
loosely compacted cluster, e will be large relative to the range. Dis-
persion (dc) expresses the compactness of the cluster in terms of a
ratio. This ratio is used to create the expectancy value of the fuzzy
number.
■
Third, compute the expectancy of the fuzzy number, shown in
Expression 7.26, by using dc as a percentage of the cluster center
value.
w = Ck × dc
(7.26)
Here,
w
is the expectancy (width) of the fuzzy number representing
the center of the current cluster
Ck
is the center of the k-th cluster
dc
is the dispersion rate for the cluster (see Expression 7.25)
Use this value as the width of the fuzzy number surrounding the cluster
centroid.
With this approach we can generate a set of classiﬁcation or speciﬁca-
tion rules based on the homogeneity of the cluster. Other rule induction
approaches using the same methodology can add to the speciﬁcity of the
rule set. One approach is to create rules that bound the centroid using
the somewhat and very hedges. In this case, each cluster generates three
rules: a core rule for the distribution of the cluster and two rules that
look at the points near the edges of the cluster as well as at points that

262
■
Chapter 7
Fuzzy Clustering
are close to the centroid of the cluster. This approach would induce rules
such as the following.
If a is somewhat around C1(a)
and b is somewhat around C1(b)
then c is somewhat near C1(c)
If a is very around C2(a)
and b is very around C2(b)
then c is very near C1(c)
If a is around C1(a)
and b is around C1(b)
then c is near C1(c)
The combined truth of the predicate expressions will cause the rule with
the greatest degree of speciﬁcity to contribute more strongly to the out-
come and thus provide a higher degree of evidence for the ﬁnal outcome.
Other possible approaches to using hedges and contract intensiﬁers or
dilutors can be employed to produce a wide family of possible rules.
7.9
Review
This chapter introduced and explored the nature of fuzzy clustering algo-
rithms. Multidimensional clustering and the detection of cluster centers
are important concepts in data exploration and analysis. Membership in
multiple clusters provides key indicators in the recognition of emerging
and shared patterns. Clustering also provides an insight into another use
of fuzzy logic: its extension into fuzzy measurements. You should now
be familiar with the following.
■
The concepts of clustering and center-of-cluster detection
■
The difference between conventional and fuzzy clusters
■
The types of techniques used to assess similarity
■
How clusters are organized and evaluated
■
How fuzzy logic is used to deﬁne degrees of membership
■
What overlapping degrees of cluster membership mean
■
The effect of initial conditions on fuzzy clustering
■
How rules can be induced from fuzzy clusters
Clustering provides not only a visualization technique (for low-
dimensional data) but provides a way of understanding how attributes

Further Reading
■
263
of high-dimensional data collections are organized and related. The distri-
bution of membership functions in a cluster (and in overlapping clusters)
indicates the degree of homogeneity of the relationships and establishes
the strength or weakness of the underlying patterns. But cluster analysis is
not the only way of exploring data for fuzzy patterns. In the next chapter
we turn to using this data to create a fuzzy rule-based system. The compre-
hensive rule induction method forms the core of the machine intelligence
system that couples fuzzy knowledge representation with both machine
learning and self-tuning capabilities.
Further Reading
■
Bezdek, J. C. “Fuzzy Mathematics in Pattern Classiﬁcation,” Ph.D. thesis, Cornell
University, Ithaca, NY, 1973.
■
Bezdek, J. C. Pattern Recognition with Fuzzy Objective Function. New York:
Plenum Press, 1981.
■
Cherkassky, V., and F. Mulier. Learning from Data: Concepts, Theory and
Methods. New York: John Wiley and Sons, 1998.
■
Devroye, L., L. Gyorﬁ, and G. Lugosi. A Probabilistic Theory of Pattern
Recognition. New York: Springer-Verlag, 1996.
■
Hoppner, F., F. Klawonn. Fuzzy Cluster Analysis: Methods for Classiﬁcation,
Data Analysis and Image Recognition. Chicester, UK: John Wiley and Sons
Ltd., 1999.
■
Krishnapuram, R., and J. A. Keller. “Possibilistic Approach to Clustering,” IEEE-
Transactions on Fuzzy Systems, vol. 1, pp. 98–110, 1998.
■
Lee, Y-J. “An Automated Knowledge Extraction System,” Ph.D. thesis, Computer
Science Department, University of Minnesota, Minneapolis, 1994.
■
Sato, M., Y. Sato, and L. C. Jain. Fuzzy Clustering Models and Applications.
Heidelberg: Physica-Verlag, 1966.
■
Yager, R., and L. Zadeh (eds.). An Introduction to Fuzzy Logic Applications in
Intelligent Systems. Boston: Kluwer Academic Publishers, 1992.

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 8
Fuzzy Rule Induction
In the preceding chapters we looked at various ways to ﬁnd patterns in data
using fuzzy database queries and fuzzy clustering. Both of these techniques
provide the knowledge engineer and business analyst with a powerful set
of exploratory facilities. However, they do not generate actual models of
the underlying business process. This is the goal of rule induction. As the
term implies, rule induction creates a knowledge base of fuzzy if-then rules
describing one or more behaviors in a large collection of data. This is a
supervised knowledge discovery approach; that is, you must supply a depen-
dent or outcome variable and a wider set of independent (or working)
variables. Rule induction then ﬁnds the functional relationships between
the independent and dependent variables expressed as a set of fuzzy if-then
rules.
Central to the mechanics of understanding what data “means” and how
this meaning can be exposed and used is the concept of a rule-based
model. Such models have signiﬁcant beneﬁts over other representations,
such as neural networks and decision trees. Intelligent models based on
rules have the following advantages.
■
The rules are easy to understand.
■
Generated rules can be modiﬁed by the model builder.
■
Rules from multiple models can be combined.
■
Rules can explain their reasoning.
■
Induced rules can be freely mixed with SME rules.
■
Rules can be regenerated based on model performance measure-
ments.
■
The conﬁguration of rules can be optimized through genetic algo-
rithms.
■
Rules can easily represent time-series data with lead and lag relation-
ships.
265

266
■
Chapter 8
Fuzzy Rule Induction
A combination of expert and discovered if-then rules, stored in a nonpro-
cedural fuzzy knowledge base, form the nucleus of highly expressive
business and policy process models. In this chapter, we take up the
fundamental algorithms necessary to discover rules, compress them
into an effective model, and validate the results. We also examine the
ways in which induced fuzzy rule-based models can be reconﬁgured and
optimized — a topic we will take up in greater detail in Part III, when we
turn to evolutionary strategies.
8.1
The Vocabulary of Rule Induction
A large part of the vocabulary necessary to understand how fuzzy if-then
rules are discovered and organized has already been covered in previ-
ous chapters (and you should have a good understanding of the topics
covered in Chapters 4 and 5). This preview of the vocabulary will make
reading and understanding the material in this chapter much easier. It
is not always possible, while maintaining an uncluttered and coherent
discussion of both the ideas associated with database organization and
queries against that organization, to ensure that every term is introduced
before it is used. Many of these terms are, of course, redeﬁned or deﬁned
in more detail later in this chapter.
Belief Function
The belief function parameter is a measure of the contribution or use
of the underlying data associated with a rule. Belief functions provide
rule degree scaling based on a subjective utility function associated with
the data. In a more pragmatic sense, a belief function is often tied to the
amount of noise in the data or the degree of certainty associated with the
data values.
Consequent
In a fuzzy rule, the fuzzy set on the right-hand side of the outcome relation
is known as the consequent fuzzy set. This is the fuzzy set that is adjusted
based on the amount of evidence in the premise and that then contributes
to the ﬁnal shape of the outcome fuzzy set (which is associated with the
dependent variable).

8.1
The Vocabulary of Rule Induction
■
267
Dependent Variable
The outcome of a model (or an equation) is the dependent variable. In
the expression y = a + bx, or the rule if a is Low and bx is High then
y is Small, the variable y is the dependent variable. It is the dependent
variable because its value is dependent on the variables on the right-hand
side of the equation or in the premise of an if-then rule.
Fuzzy Associative Memory (FAM)
A fuzzy associative memory (FAM) is a compact, multidimensional
matrix representation of simple fuzzy rules. Each dimension of the FAM
has a row for each fuzzy set in the term set associated with that variable.
The high-dimensional cell formed at the intersection of each dimension in
the matrix contains the fuzzy set of the outcome (dependent) variable for
that combination of independent-variable fuzzy sets. A FAM is an efﬁcient
and fast structure for executing fuzzy rules because it can be processed
simply by looping through each variable’s data value, ﬁnding its degree of
membership in the variable’s fuzzy set at the data point’s position in the
domain, computing the degree of evidence from these evaluations, and
modifying the outcome fuzzy set.
Independent Variable
The variables on the right-hand side of an equation or in the premise of
an if-then rule are known as independent variables. The variables are
independent because in the context of the equation or the rule they are
independent of any other variable in the rule or equation.
Premise (Antecedent)
In a fuzzy rule, the set of fuzzy propositions associated with the if con-
dition of the rule is known as the premise or the antecedent. In the rule
if a is Low and bx is High then y is Small the premise consists of the
two fuzzy propositions a is Low and bx is High connected by the And
operator. The composite truth of the premise is a measure of how much
evidence exists for the outcome proposition, and the consequent fuzzy
set is scaled according to this composite truth.

268
■
Chapter 8
Fuzzy Rule Induction
Rule Degree (Degree of the Rule)
The degree of a rule is the product of the fuzzy memberships for each
of the variables. The degree measures the strength or applicability of the
rule relative to all other rules that specify the same outcome.
Standard Error of Estimate
In a fuzzy model categorization or prediction model, the difference
between the predicted outcome and the actual outcome associated with
a set of valid data points is the standard error of estimate. For a model
that is 100% accurate, the predicted and actual will coincide and the
error is zero. The standard error of estimate, then, is a measure of model
forecast precision.
Tournament
The process, inside the rule induction algorithm, of ﬁnding the best rules
that predict the outcome variable is called the compression tournament.
The tournament is essentially a contest between similar rules to ﬁnd the
ones that have the highest rule degree of a particular outcome.
Training Data
Rule induction is a supervised data mining process. The set of data points
used to ﬁnd and discover rules is known as the training data. This is a
large collection of observations with values for both the dependent and
independent variables.
Validation Data
The set of data points used to validate a set of induced rules is known
as the validation data. Validation data sets are extracted from the same
overall collection of observations as the training data and have the same
format. In validation, the fuzzy model compares the predicted outcome
with the actual outcome and computes a standard error of estimate for
the rules.

8.2
Rule Induction and Fuzzy Models
■
269
8.2
Rule Induction and Fuzzy Models
Fuzzy rule induction uses an approach initially described by Li-Xin Wang
and Jerry M. Mendel in their 1991 paper “Generating Fuzzy Rules from
Numerical Data with Applications.” Rule induction following the Wang–
Mendel algorithm implements a supervised data mining technique. The
knowledge engineer speciﬁes a dependent variable and a collection of
independent variables. From this collection the induction process discov-
ers the rules connecting the dependent to the independent variables in a
functional relationship, shown in Expression 8.1.
vD
o = f (v1(sa, d1), v2(sb, d2), . . . vn(sz, dn))
(8.1)
Here, Sw represent the fuzzy sets in variable vi activated by data point
dj, and vo is the outcome value (after defuzziﬁcation). Rule induction
generates a FAM describing the underlying behavior patterns. This asso-
ciative memory is a collection of evidence-weighted if-then rules mapping
the behavior of the independent variables to the dependent variable.
Figure 8.1 shows the basic knowledge-base generation cycle.
Training
data
Fuzzy
knowledge base
Rule
induction
algorithm
Variables and
fuzzy sets
Definition
Tuning
Induction
controls
Figure 8.1
Rule induction.

270
■
Chapter 8
Fuzzy Rule Induction
4
Training
file
Fuzzy
knowledge base
Decompose
variables
Compress
rules
Generate
intermediate
rules
Create
fuzzy rule
base
Rule
tournament
Dependent variable1
Independent variable1
Independent variablen
Fuzzy sets
P1
P2
P3... PN
if V2 is P2 and V3 is S1 then V1 is R1
if V2 is P4 and V3 is S2 then V1 is R2
if V2 is P5 and V3 is S3 then V1 is R3
:
:
Figure 8.2
The rule induction process.
The collection of if-then rules is derived from the data itself. At the
core of rule generation is a tight interplay between the patterns buried in
the data and the fuzzy sets deﬁning the variables. As we will see shortly,
the induction facility generates rules in the form of fuzzy relationships.
Figure 8.2 shows, schematically, the process involved in rule induction.
Each ﬁeld in the training ﬁle (or column in a database) is deﬁned
through a collection of overlapping fuzzy sets. The data points are
mapped to one or more corresponding fuzzy sets, generating a relation-
ship between fuzzy spaces in the independent variables and a fuzzy space
in the dependent variable. The relationship is cast in the form of a rule.
As Figure 8.2 shows, there are many candidate rules produced during
rule generation, but rule induction eventually compresses these to an
effective.
Understanding the relationship between rules and behavior allows us
to recast our functional description in Expression 8.1 into a knowledge-
base model. A model consists of the variables, their fuzzy sets, and
the collection of rules deﬁning the outcome. Figure 8.3 schematically
illustrates the model organization of the functional relationship.

8.2
Rule Induction and Fuzzy Models
■
271
Input
data
V1
V0
V2
V3
Vn
Evidence
Fuzzy
rule-based
model
Variables and
fuzzy sets
Knowledge base
Figure 8.3
A fuzzy rule-based model.
Input ﬁle data elements are mapped to their variable deﬁnitions. Each
variable deﬁnition speciﬁes not only the properties of the variable (data
type, data organization, range of values, and so on) but the collection of
fuzzy sets spread over its range. Each record is read from the input ﬁle
and a new outcome value is predicted (along with its compatibility index,
a measure of the evidence for this prediction).
When rule induction is complete, the resulting knowledge base forms
the core intelligence of the fuzzy model. Although many of the technical
details of the fuzzy model internals were covered in Chapter 5, a review
at this point is worthwhile. A model produced by fuzzy rule induction
is a series of fuzzy relations. Inside the model, as shown in Figure 8.4,
the rules accept fuzziﬁed variable values, apply aggregation (such as the
SAM), and then defuzzify the outcome fuzzy set to produce a value.
Building the model’s rule base (connecting function to practice) is the
responsibility of the rule induction engine. Induction is a portable, encap-
sulated component used by fuzzy systems to combine expert-provided
linguistic rules and exemplar rules (found in the data) into the combined
fuzzy rule base. Figure 8.5 illustrates its relationship to the overall fuzzy
model.
The rule induction engine is a trainable, model-free component of an
adaptive fuzzy modeling system. The induction facility learns a collec-
tion of underlying pattern relationships from the examples, expressing
them as if-then rules. In this sense it is completely model free; that is,

272
■
Chapter 8
Fuzzy Rule Induction
Data
source
V1
V0
V2
V3
Vn
Fuzzify
Aggregate
Defuzzify
Evidence
Fuzzy
rule-based
model
Fuzzy 
Knowledge Base
Figure 8.4
Inside the fuzzy rule-based model.
Rule
inducion
engine
Fuzzy
rule-based
model
Expert provided
rules
Examples
Knowledge base
Tuning
Deployed
Evidence
VO
Figure 8.5
The rule induction engine.
no underlying mathematical model is necessary. The engine is adaptable
because it will learn new patterns as well as re-enforce existing patterns
when presented with additional data.
With a rule induction algorithm in hand, data mining engineers have a
ﬂexible technique for evolving fuzzy knowledge bases. These knowledge
bases, in turn, form the core of business process models. We now turn
our attention to the mechanics of the actual rule induction algorithm.

8.3
The Rule Induction Algorithm
■
273
8.3
The Rule Induction Algorithm
Fuzzy rule induction involves three processes: the description of the
model variables, the generation of candidate rules, and the selection of the
ﬁnal rule set. The goal of this algorithmic process is the production of fuzzy
relations in the form of if-then rules. We start with a set of input/output
vectors in the data, as follows.
(x1
v1, x1
v2, . . . , x1
vn, y1
k)(x2
v1, x2
v2, . . . , x2
vn, y2
k) . . . .
(8.2)
where, xvi are the input data values (also associated with a variable) and
y is the desired outcome value. The purpose of the induction scheme is
to discover the functional relationship,
y ←f (x1, x2, . . . , xn),
(8.3)
which in a fuzzy system is a form of close approximation rather than
representation. This means that we are building a model that obeys the
relationship in Expression 8.4.
y ≈f (x1, x2, . . . , xn)
(8.4)
Because fuzzy systems are universal approximators, the degree of gran-
ularity in the model as well as the clarity and depth of the patterns
in the training ﬁle determine how closely Expression 8.4 approaches
Expression 8.3.
Partition Dependent and Independent Variables
into Fuzzy Sets
Each variable is decomposed or partitioned into a set of overlapping fuzzy
sets. These completely encompass the variable’s domain or range of values
(the domain is not always the same as the actual range of values). The
variable’s range is divided into 2n + 1 (an odd number) of fuzzy sets.
Figure 8.6 shows a variable partitioned into ﬁve fuzzy sets (here, n = 2).
The underlying fuzzy sets can assume a variety of shapes. In Figure 8.6,
bisected trapezoids are used at the ends, with triangular sets ﬁlling in the
remainder of the domain. Trapezoidal fuzzy sets are also used in some
fuzzy models in which the data contains explicit class intervals. Triangles
can be replaced with bell-shaped fuzzy sets and the end points covered
with sigmoid decay and growth curves.
Generally, the degree of overlap in triangular and bell-shaped fuzzy sets
is 50%. This means that the apex of the triangle or bell has a membership

274
■
Chapter 8
Fuzzy Rule Induction
Grade of membership m(x)
Transactions per second (×00)
10
15
20
25
30
35
40
45
50
55
60
65
70
75
80
0
1.0
S02
S01
CE
B02
B01
Transaction_Rate
Figure 8.6
A variable partitioned into fuzzy sets.
of one [1.0]. The edges of the fuzzy set fall at the center of the neighboring
fuzzy sets, with a membership of zero [0].
The number of sets and their shapes need not be the same for all
variables in the model. Although most variables are decomposed into
symmetric fuzzy sets (all triangles are congruent), this is not required.
Model tuning (either manually or through genetic optimization) often
results in asymmetric fuzzy regions. Figure 8.7 shows the same variable
partitioned into several asymmetric fuzzy sets.
In automatically partitioning variables we use a naming convention
that identiﬁes the center fuzzy set and those to the left and right. Sets to
the left of center (moving in the negative direction) are S01, S02, and
so on, indicating Small fuzzy sets. Sets to the right of center (moving in
the positive direction) are B01, B02, and so on, indicating Big fuzzy sets.
The central fuzzy set is labeled CE (center). These naming conventions
(suggested in the original paper) have their origins in control theory and
fuzzy controllers. Most control fuzzy systems measure changes in the
position, velocity, angle, and so forth of some plant component. In this
regard, changes that decrease (move away from the center toward zero)
are Small, and changes that increase (move away from the center to larger
positive numbers) are Big.

8.3
The Rule Induction Algorithm
■
275
Grade of membership m(x)
Transactions per second (×00)
10
15
20
25
30
35
40
45
50
55
60
65
70
75
80
0
1.0
S02
S01
CE
B02
B01
Transaction_Rate
Figure 8.7
Variable with asymmetric fuzzy sets.
Generate Candidate Fuzzy Rules from
Training Data
In this phase, we produce a set of fuzzy relations from the dependent and
independent variables (that is, between the outcome and input variables).
The ﬁrst step is determining the degrees of membership for xv1, xv2, . . . , yk
in each of the fuzzy sets associated with that variable’s domain. In this
example, we will use the two data vectors shown in Expression 8.2 as an
illustration of how candidate rules are produced. Figures 8.8 through 8.10
show the membership of data points xv1, xv2, and yk in the hypothetical
ﬁrst and second data records.
The second step isolates the fuzzy set to data point mapping with
the highest (that is, maximum) membership degree. As an example, xv1
in the ﬁrst data vector forms the relation (xv1,B01) because this has the
highest degree of membership. And xv2 in the same record forms a relation
(xv2,S01) with its maximum membership of [.74]. Table 8.1 shows the
fuzzy set relationships with their maximum assigned memberships.
At this point we have a collection of n-tuple fuzzy relations reﬂect-
ing the maximum degree of ﬁt between data points and the underlying
fuzzy sets. As a ﬁnal step, these fuzzy relations are converted to candidate

276
■
Chapter 8
Fuzzy Rule Induction
[.82]
[.52]
[.48]
[.18]
0
S01
CE
B01
B02
Degree of
membership µ(x)
x 1
2
x 1
1
S01
S02
Variable
V1
CE
B01
B02
X1
X2
.82
.52
.48
.18
FUZZY SETS
S02
V1
Figure 8.8
Degrees of membership in V1.
[1.0]
[.74]
[.26]
0
S01
CE
B01
B02
Degree of
membership µ(x)
x1
2
x2
2
S02
V2
S01
S02
Variable
V2
CE
B01
B02
X1
X2
.26
1.0
FUZZY SETS
.74
Figure 8.9
Degrees of membership in V2.

8.3
The Rule Induction Algorithm
■
277
Degree of membership µ(x)
y1
y2
0
1.0
[.92]
[.78]
[.22]
[.08]
S02
S01
CE
B02
B01
Vk (outcome)
S01
S02
Variable
V2
CE
B01
B02
Y1
Y2
.92
.22
FUZZY SETS
.08
.78
Figure 8.10
Degrees of membership in Vk.
TABLE 8.1
Variable and Fuzzy Set Relations
Data 1
Data2
xv1
xv2
yk
xv1
xv2
yk
Fuzzy Set
B01
S01
CE
B01
CE
B01
Degree
.82
.74
.92
.52
1.0
.78
TABLE 8.2
Generated Candidate Rules
Fuzzy Rule
Rule 1
If V1 is B01 and V2 is S01 then Vk is CE
Rule 2
If V1 is B01 and V2 is CE then Vk is B01
if-then rules. We generate one rule from each of these tuples (shown in
Table 8.1), producing the rules in Table 8.2.
At this point the rule generator has produced a large collection of can-
didate rules. These rules are called candidates because they are subject
to ﬁltering and removal. Learning from data means, invariably, that a large
number of n-tuple vectors is found, and consequently a large number

278
■
Chapter 8
Fuzzy Rule Induction
of rules created. Invariably, some rules will have conﬂicting outcomes.
These are rules with the same antecedent (if expression) but a differ-
ent consequent (then expression). The collection of rules with the same
antecedent but a different consequent is called the conﬂict set (represent-
ing ambiguous patterns in the data itself). The next step in rule induction
addresses this issue.
Compute a Degree of Effectiveness for Each Rule
Each rule is assigned a degree of effectiveness. A rule’s effectiveness
degree (E) is the product of its component fuzzy set membership degrees.
Consider the following rule.
If v1 is X and v2 is Y then v3 is Z
Then the effective degree of this rule is found using Expression 8.5.
E(ri) = µX(v1) × µY (v2) × µZ(v3)
(8.5)
To see how the effectiveness for a rule is generated, Table 8.3 shows the
effectiveness degrees for the previous example rules. Thus, the higher
the total combined memberships of the candidate rule the higher its
composite effectiveness ranking relative to all other rules with the same
antecedent. This represents a ﬁlter on the conﬂict set. As we will see in
the ﬁnal rule generation step, the use of effectiveness measures not only
solves ambiguity difﬁculties in the data but signiﬁcantly reduces the total
number of rules in the knowledge base.
The concept of effectiveness leads to the data belief function — B(x) —
a measure connecting rule induction with the quality of individual data
vectors. This is an important parameterization with signiﬁcant implica-
tions for knowledge discovery and the validity of the resulting knowledge
base. Business analysts and knowledge engineers often deal with data that
has varying degrees of trustworthiness. Some data is noisy, and some is
TABLE 8.3
Effectiveness Measures
Variables
Effectiveness
Rule
V1
V2
Vk
E(Rule1)
.82
.74
.92
.56
E(Rule2)
.52
1.0
.78
.41

8.3
The Rule Induction Algorithm
■
279
subject to experimental errors or uncorrected measurements. Other data
elements are simply doubtful, obsolete, or provide a weakly supported
example of target data patterns. The quality of data is determined by
an SME.
The data belief function [or simply B(x), the belief function] is a degree
of belief in the trustworthiness of a data element and is assigned by one
or more SMEs. This is a ranking in the range [0,1] and constitutes a psy-
chometric scaling of the data. The belief function couples the data to a
fuzzy set, which functionally measures the utility of data. Note that belief
function measurements are stored with the data, so that Expression 8.2
becomes
(x1
v1, x1
v2, . . . , x1
vn, y1
k; B1)(x2
v1, x2
v2, . . . , x2
vn, y2
k; B2) . . . ,
(8.6)
where Bi is the data quality belief function associated with the data vector.
This belief function is used to extend (and redeﬁne) the effectiveness of
a rule. Expression 8.6 becomes
E(ri) = µX(v1) × µY (v2) × µZ(v3) × Bi.
(8.7)
Thus, the overall effectiveness degree for a rule is the product of its com-
ponent memberships and the estimated quality of the data used to gener-
ate the rule. This brings us to the measure of rule quality — Q(ri) — which
is expressed simply as
Q(ri) = E(ri) × Bi.
(8.8)
The introduction of a data quality (or reliability) belief function transforms
rule induction into a completely fuzzy process. It also provides data mining
experts, business analysts, and knowledge engineers with a consistent and
effective means to specify the underlying reliability of data. Highly reliable
and clean data is assigned high belief function values. Less reliable data
is assigned a lower belief function value. To remove the human-factors
element (i.e., to eliminate the inﬂuence of SMEs), we need simply set all
belief functions to one [1.0].
Create the Fuzzy Rule Base
A fuzzy knowledge base combines both the rules induced from the data
as well as rules created by either SMEs or knowledge engineers. To illus-
trate the construction of the fuzzy knowledge base (and before moving
on to a system of very high dimensionality), we will look at the simple

280
■
Chapter 8
Fuzzy Rule Induction
2 × 1 example: v1, v2, and vk. In this case, we have a 2×2 fuzzy associative
memory. Figure 8.11 shows an empty FAM.
The cells of a FAM are ﬁlled with the names of the outcome fuzzy sets
that complete the rule actions. The fuzzy rule base (the FAM) is ﬁlled with
rules from the candidate set. If more than a single rule is selected for a
cell, the rule with the highest Q(ri) value is selected. To see how the
rule base is completed, Figure 8.12 is the FAM after adding rule 1 (see
Table 8.2): if V1 is B01 and V2 is S01 then Vk is CE.
Figure 8.13 shows how the fuzzy rule base appears after adding rule 2
(Table 8.2): if V1 is B01 and V2 is CE then Vk is B01.
The fuzzy rule base construction process continues in this fashion until
all rules have been loaded. This is a tournament process in which each
candidate rule vies for sole ownership of the memory cell. At the end of
the load, the rule base contains those rules with the highest quality from
all rules generated from the data.
Variable V1
S02
S01
CE
B01
B02
B02
B01
CE
S01
Variable V2 
S02
Figure 8.11
A 2 × 2 FAM.
Variable V1
S02
S01
CE
B01
B02
B02
B01
CE
S01
CE
Variable V2 
S02
Figure 8.12
FAM after adding rule 1.

8.3
The Rule Induction Algorithm
■
281
Variable V1
S02
S01
CE
B01
B02
B02
B01
CE
B01
S01
CE
Variable V2
S02
Figure 8.13
FAM after adding rule 2.
Variable V1
S02
S01
CE
B01
B02
B02
S01
B01
S01
S01
S01
CE
S01
S01
S01
Variable V2
S02
S01
S01
S01
Figure 8.14
A 2 × 2 FAM.
Fuzzy rule induction generates only rules whose antecedents are con-
nected by And operators. SMEs and knowledge engineers can insert
linguist rules into the selection tournament that use Or connectives (in
which case the consequent is performed if any of the antecedents have
a truth value above the alpha level threshold). An Or rule propagates the
consequent fuzzy set through all cells in the row or column associated
with each variable in the rule antecedent. The following is an example of
this rule.
If V1 is CE or V2 is B01 then Vk is S01
Figure 8.14 shows the FAM ﬁlled out after adding this rule. The consequent
(outcome) fuzzy set S01 has been added to the row for B01 and the
columns for CE.
Rule quality measurements for Or rules are computed in the same way
as regular And rules. The degree values of each cell in the fuzzy rule base
are the same as the quality degree of the rule.

282
■
Chapter 8
Fuzzy Rule Induction
Run the Fuzzy Model
Executing a fuzzy model involves deﬁning the methods for fuzziﬁcation,
correlation, aggregation, and defuzziﬁcation. A form of the SAM is used.
For a rule such as
if V1 is B01 and V2 is S01 then Vk is CE
fuzziﬁcation and correlation (as in all fuzzy models) involves ﬁnding
the membership of xvi in B01 and xv2 in S01 and using this to mea-
sure the degree of control exercised by the rule. This means that
the control degree for a vector of input data points xv1, xv2, xv3, . . . xvi
is found by
di
o =
n

i=1
µAi(xi)
(8.9)
As an illustration, the control degree for the previous rule is com-
puted as
dr1
o = µB01(xv1) × µS01(xv2)
(8.10)
A control degree for each rule forms the scaling factor that correlates
the degree of truth in the antecedent with the degree of strength (con-
trol) in the consequent (outcome). Our defuzziﬁcation strategy, shown
in Expression 8.11, for the rule-induced model sums the scaled outcomes
and divides by the sum of the scaling.
xD
vo =
n
i=1 di
o × ˆxvo
n
i=1 dio
(8.11)
Here,
xD
vo
The defuzziﬁed value of the outcome variable
ˆxvo
The center of the outcome fuzzy region
In other words, we treat the outcome fuzzy region (Oi) as a singleton
[a line of a height of max(Oi) and width of zero]. In this approach to
defuzziﬁcation we do not actually form an outcome fuzzy set (you can
only use this approach when the model does not contain unconditional
fuzzy rules). Defuzziﬁcation produces a weighted average of the scaled
outcomes.

8.4
The Model Building Methodology
■
283
8.4
The Model Building Methodology
In the next few sections the underlying temperature forecasting model is
deﬁned and then created using the rule induction engine. In constructing
a model, we need to observe a regular process or methodology that
addresses the nature of the data as well as the nature of the target model.
As Figure 8.15 schematically illustrates, creating and verifying a fuzzy
rule-based model from a training set of data involves a deﬁnite set of
steps.
The overall model construction process is relatively straightforward.
The individual steps, however, can be complicated and often time con-
suming (especially the process of cleaning, transforming, and organizing
the data, discussed in some detail in material to follow). Each step fuses
to form a uniform way of using data to build a rule-based model.
Re-train
Model
definition
Select
variables
Discover
rules
Acquire
and organize
data
Create
knowledge base
(fuzzy model)
Validate
model
Training
examples
Validation
data
Knowledge
Base (FAM)
Re-define
Define
properties
Data
attributes
Fuzzy
sets
2
3
1
4
5
Figure 8.15
The model development methodology.

284
■
Chapter 8
Fuzzy Rule Induction
➊Acquire, Clean, Transform, and Organize
the Data
Building a model from historical information necessitates acquiring suf-
ﬁcient data to represent all underlying patterns. To predict monthly
temperature variations, for example, a model needs at least one year of
monthly data. Data must also be cleaned and often normalized. Cleaning is
the process of removing obvious errors and extraneous outliers. Cleaning
also involves identifying and correcting missing data. Data transformation
for a time-series model, as we have previously discussed, involves trans-
forming the data into an indexed representation. The Year x Month data
matrix becomes the serial data ﬁle shown in Listing 8.1. Note that Year is
not included in the data ﬁle because it is not an explicit variable. Rather,
each year is represented by a repeating block of monthly temperatures.
Month, Temperature
1,
-1.9
2,
2.2
3,
12.2
4,
13.7
5,
16.1
6,
21.3
7,
25.2
8,
25.4
9,
20.9
10,
14.4
11,
7.7
12,
-2
1,
2.9
2,
-2.1
3,
2.7
4,
13.1
5,
17.7
6,
22.8
7,
24.9
8,
29.7
9,
22.2
10,
19.3
11,
8.2
12,
3.7
Listing 8.1
The temperature data ﬁle.

8.4
The Model Building Methodology
■
285
After acquiring the data, it must be separated in training and valida-
tion sets. The temperature prediction model is a supervised knowledge
discovery process. Supervision means that the rule discovery algorithm is
fed examples of the (temperature, month) relationships and from these
examples extracts the patterns in terms of an if-then rule base. These
examples constitute a training set for the model. Another set of the same
data is set aside for model validation (see “Validate Model” for details).
➋Select Variables and Deﬁne Properties
Part of the model deﬁnition is the need to identify the dependent (out-
come) variable and the set of independent (generally, input) variables.
Each variable in a model has a variety of attributes or properties that
affect the way it is used. In building the model we can consider such
properties as the following.
■
Data type (numeric, Boolean, or string)
■
Data structure (continuous or discrete)
■
Range of values (domain)
■
Unit of measure
■
Underlying division into fuzzy regions
Based on a preliminary statistical analysis, the modeling system provides
defaults for each of these properties. A variable is usually considered
continuous and numeric. The number and location of the underlying
fuzzy sets are determined by the statistical properties of the data.
➌Discover Rules
The core of the model constructor is the rule discovery (or induction)
algorithm that reads the training ﬁle and ﬁnds the underlying rules relating
the independent variables to the dependent variable. Rule discovery (or
induction) generates the core collection of fuzzy if-then rules. Listing 8.2
illustrates a portion of the discovered rules. From the discovery pro-
cess, the rules are numbered, assigned a weight reﬂecting their degree
of evidence, and then expressed as a fuzzy relation in the form vari-
able fuzzy set variable fuzzy set. These rules represent each of the
unique fuzzy relationships between temperature and time embedded in
the data.

286
■
Chapter 8
Fuzzy Rule Induction
001 1.023 Month Mon_06 Temperature Tem_09
002 1.042 Month Mon_07 Temperature Tem_09
003 1.049 Month Mon_09 Temperature Tem_05
004 1.030 Month Mon_10 Temperature Tem_03
005 1.023 Month Mon_02 Temperature Tem_02
006 1.068 Month Mon_05 Temperature Tem_08
007 1.072 Month Mon_01 Temperature Tem_01
008 1.015 Month Mon_03 Temperature Tem_06
009 1.038 Month Mon_09 Temperature Tem_06
010 1.049 Month Mon_02 Temperature Tem_03
011 1.053 Month Mon_04 Temperature Tem_07
012 1.019 Month Mon_05 Temperature Tem_09
013 1.019 Month Mon_03 Temperature Tem_04
014 1.049 Month Mon_11 Temperature Tem_01
015 1.004 Month Mon_01 Temperature Tem_03
Listing 8.2
Discovered temperature rules.
➍Create the Fuzzy Knowledge Base
A knowledge base represents the ﬁnal results of the rule discovery process
and consists of the rules and variable deﬁnitions. It is the knowledge base
that forms the actual mode that is effectively compiled and executed to
run the temperature prediction system. Listing 8.3 shows a part of the
knowledge base generated from the rule induction process.
[R001; Wgt=1.023]:
if Month is Mon_06
then Temperature is Tem_09;
[R002; Wgt=1.042]:
if Month is Mon_07
then Temperature is Tem_09;
[R003; Wgt=1.049]:
if Month is Mon_09
then Temperature is Tem_05;
[R004; Wgt=1.030]:
if Month is Mon_10
then Temperature is Tem_03;
Listing 8.3
Temperature prediction rules.

8.4
The Model Building Methodology
■
287
A knowledge base becomes a self-contained container of rules and
variables that is opened and run by the fuzzy inference engine. In the
temperature model, the knowledge base connects to an inference engine
that reads a month and predicts a temperature for that month (along with
the amount of evidence associated with that prediction).
➎Validate the Model
Once a model’s knowledge base has been generated, the model prediction
effectiveness must be tested. The predictive precision of a model depends
on a variety of factors, such as the following.
■
The amount and richness of the training data
■
The resolution granularity of the partitioning fuzzy sets
■
The generation parameters of the algorithm
Yes
Load
and compile
knowlege base
1
Read
record
Compile
standard
error
4
Inference engine
Run model
rules
2
Predict
outcome
3
Knowledge
base
End
Validation
file
End
of
file?
Performance
statistics
No
Figure 8.16
The model validation process.

288
■
Chapter 8
Fuzzy Rule Induction
As the name implies, the purpose of validation is to measure how well the
model works — to validate its performance against new and untested data.
A validation ﬁle is usually extracted from the pool of initially acquired,
cleaned, and transformed data. The majority of the data, in the form
of a training ﬁle, is fed to the rule discovery algorithm so that a broad
spectrum of patterns is discovered and learned. A smaller but neverthe-
less broadly representative collection of data points are set aside in one
or more validation ﬁles. Figure 8.16 schematically shows the validation
process.
Validation parallels the execution of a deployed model except that
validation data also contains the value of the outcome or dependent vari-
able. After a knowledge base is loaded and compiled ①, data from the
validation ﬁle is continuously read and passed to the active model ②. The
model predicts a temperature and compares the predicted value with
the actual value ③. The actual and the predicted values are stored in the
performance statistics ﬁle. When all validation data has been processed
④, the performance statistics are used to compute an average standard
error. The smaller the standard error the more precise and robust is the
model.
If the average standard error associated with a model is not acceptable,
the model must be recreated. A new model is generated by retraining
or by redeﬁnition or a combination of the these approaches. Retraining
means increasing the amount and pattern richness of the training data.
Redeﬁnition means changing the parameters of the model itself (such as
increasing or decreasing the independent variables or the number and
shape of the underlying fuzzy sets for one or more of the variables).
In the next section we will follow the development of a rule-based
model through model speciﬁcation, rule generation, and validation.
8.5
A Rule Induction and Model Building
Example
The rule induction process provides the core model building mechanism
underlying a large family of usually nonlinear models. A typical nonlinear
model involves discovering the trend behavior of time-series data. Time
series regularly have signiﬁcant periodicity cycles as well as a high level of
ambient noise. Although polynomial regression techniques are available
that do an excellent job of developing a time-series model, the fuzzy rule
induction mechanism is also capable of modeling this type of nonlinearity.
We turn our attention to building a time-series prediction model as an
example of the nonlinear model as a general class.

8.5
A Rule Induction and Model Building Example
■
289
A Nonlinear Time-Series Model
A time series, as the name implies, is a collection of data organized across
time. It has the important characteristic of having one or more monotonic
time axes. In general, each time axis is a ﬁner-grained elaboration of a
higher-level time perspective. In this model we are using 30 years of
temperature data (see Table 8.4) from the Midwest (Kansas City).
TABLE 8.4
Annual Temperatures from 1945 to 1972
Months
Jan
Feb
Mar
Apr
May
Jun
Jul
Aug
Sep
Oct
Nov
Dec
1945
−1.9
2.2
12.2
13.7
16.1
21.3
25.2
25.4
20.9
14.4
7.7
−2
1946
2.9
−2.1
2.7
13.1
17.7
22.8
24.9
29.7
22.2
19.3
5.2
3.7
1947
1.3
4.6
13.8
16
16.9
24.5
26.9
23
20.9
17.1
9.3
4.9
1948
−3.1
1.4
6.6
15.4
18.4
24.1
26.1
26
22.4
13.8
8.9
3.7
1949
0.6
2.7
6.7
12.3
20.6
25.3
27
25.5
18.4
15.9
9.4
4.9
1950
3.4
2.8
5
10.6
20.1
23.6
24.6
22.7
19.7
18.3
3.9
−1.4
1951
0.3
1.9
4.5
11.2
19.7
22.3
25.7
24.8
19.1
15.1
3.8
0.9
1952
2.2
4.3
5.8
12.7
18.8
28.4
27.4
24.8
21.1
12.1
7.8
2.8
1953
1.8
4.4
8.2
11.2
19.6
27.5
27.4
25.7
22.4
16.7
8.7
3.2
1954
0.4
6.6
5.6
16.7
16.4
26.3
29.4
26.7
23.7
14.9
7.7
2.6
1955
0.6
2.2
7.2
17.2
20.2
21.9
28.6
26.8
23.1
14.6
5.4
0.6
1956
−1.6
2.7
7.3
11.9
19.9
24.6
25.4
26.3
21.6
18.1
7.2
3.6
1957
−2.8
4.7
6.3
13.6
19
24.2
26.7
26.3
20.2
12.7
6.4
4.9
Year
1958
−0.4
−3.7
3.3
13.2
18.9
21.9
24.7
25.3
20.6
15.1
9.4
0
1959
−3.2
2.1
7.6
13.8
20.9
24.7
25.6
27.1
21.8
13.9
3.9
4.1
1960
0.8
−0.7
−0.9
14.9
17.1
23.3
24.6
26.2
23.4
15
8.1
−0.6
1961
−2.1
2.2
7.5
10
14.6
21.2
24.5
24.1
21.7
15.1
6.7
0
1962
−4.1
2.6
4.2
11.8
22.2
23.3
24.3
24.2
19.1
16.1
7.2
−0.4
1963
−5.9
−1.9
9.3
14.6
17.7
24.1
25.2
23.8
20.3
19.4
7.6
−5.4
1964
1.5
1.3
5.7
14.8
20.9
23.8
25.8
24.7
20.7
12.2
8.5
−0.1
1965
−0.1
1
1.4
14.7
21.4
23.9
24.9
24.3
20.6
13.6
9.2
5.4
1966
−3.9
0.1
8.2
10.9
16.5
23.2
28.3
23.4
18.7
12.3
8.4
1.6
1967
1.5
−0.4
9.1
14.7
16
23.3
23.8
22.4
19.2
14.1
5.6
1.6
1968
−0.1
−1.6
7.8
13.1
16.6
25
25.3
25.1
19.3
13.9
6.4
0.2
1969
−1.6
1.8
3.2
13.8
18.7
22.5
26.9
25
20.7
13.4
6.3
0.1
1970
−4
0.5
4.7
14.4
20.7
22.4
25.5
24.6
22.1
13.4
6.4
2.8
1971
−2.6
1.1
5.4
13.3
16.8
26.1
24.1
24.3
22.5
17.6
7.8
4.8
1972
−1.2
1.3
7.2
13.4
19.1
23
25.3
24.6
21.7
12.7
4.3
−0.9
1973
0.3
1.4
10.5
12.1
16.5
23.7
25.9
24.9
21.1
15.9
8.3
−1.1
1974
−1.2
2.3
8.9
14.2
18.3
20.8
26.6
23.6
16.8
14.3
6.7
1.1

290
■
Chapter 8
Fuzzy Rule Induction
Using this data, we use the rule induction methodology to create a
model that predicts the temperature, given the month of the year. In
more general terms, this means building a model that relates temperature
to month in a functional manner, as shown in Expression 8.12.
Tt = f (Mt)
(8.12)
where,
Tt
is the dependent variable and represents the
temperature at time t.
Mt
is the independent variable and represents the month at time t.
The value for Mt is not the month but the time index value,
discussed in the next section (on organizing time-series data).
The rule discovery process uses historical temperature data (see Table 8.1)
to discover a set of association rules that makes this functional connection.
These rules are in the form
if month is M then temperature is T,
where M is a fuzzy interval associated with the underlying monthly time
frame and T is a fuzzy number created from the association rules. The
model created by rule induction consists of a collection of fuzzy if-then
rules. The rules are organized in a knowledge base and executed in
parallel. Expression 8.13 illustrates this process.


R1
R2
...
Rn

⇒T ⇒d(T) ⇒Tt
(8.13)
The outcome of the model is a fuzzy region T. When T is reduced or
defuzziﬁed (d(T)), the value is Tt, the predicted temperature for the
month.
Organizing Time-Series Data
To model a time series with periodic cycles we need to convert the time
axis into a set of repeating index values that represents these cycles.
This allows the trend analysis system to learn the underlying periodicity
patterns and match them to the indexing cycles. For example, Table 8.5
illustrates part of the temperature data with the month names replaced
with period index values.

8.5
A Rule Induction and Model Building Example
■
291
TABLE 8.5
Temperature Data with Month Index Values
Months
1
2
3
4
5
6
7
8
9
10
11
12
1945 −1.9
2.2
12.2
13.7
16.1
21.3
25.2
25.4
20.9
14.4
7.7
−2
1946
2.9
−2.1
2.7
13.1
17.7
22.8
24.9
29.7
22.2
19.3
5.2
3.7
Year 1947
1.3
4.6
13.8
16
16.9
24.5
26.9
23
20.9
17.1
9.3
4.9
1948 −3.1
1.4
6.6
15.4
18.4
24.1
26.1
26
22.4
13.8
8.9
3.7
1949
0.6
2.7
6.7
12.3
20.6
25.3
27
25.5
18.4
15.9
9.4
4.9
TABLE 8.6
The Annual Temperature Data File
Month
Temperature
1
−1.9
2
2.2
3
12.2
4
13.7
5
16.1
6
21.3
Cycle 1
7
25.2
8
25.4
9
20.9
10
14.4
11
7.7
12
−2
1
2.9
2
−2.1
3
2.7
4
13.1
5
17.7
Cycle 2
6
22.8
7
24.9
8
29.7
9
22.2
10
19.3
11
5.2
12
3.7
In this example, we have simply replaced the month names with the
month number, but this number becomes the index into the period behav-
ior of the data. This can be seen more clearly by examining Table 8.6, the
actual data ﬁle generated from this transformation.
Each periodic cycle in a time series for which we want to learn and
predict a value must be converted to an indexing scheme. The month
index effectively converts the data into a series of separate vectors, one

292
■
Chapter 8
Fuzzy Rule Induction
for each index. Thus, the learning mechanism resolves the patterns for
the data along the index. For example, for a model that is interested in
both monthly and quarterly temperature variations Table 8.7 shows how
the underlying data would be indexed.
In Table 8.5 we have two interdependent cycles: months and quar-
ters. With this decomposition, the rule induction process would learn
two types of patterns: one for the monthly temperatures and one for
the quarterly temperatures. Thus, we can look for patterns that repeat at
greater or ﬁner levels of granularity. Figure 8.17 illustrates how periodic
cycles are represented and selected in the data and the model.
TABLE 8.7
Temperature Data with Month and Quarter Index Values
Months and Quarters
Mnth
1
2
3
4
5
6
7
8
9
10
11
12
Qtr
1
2
3
1
2
3
1
2
3
1
2
3
1945
−1.9
2.2
12.2
13.7
16.1
21.3
25.2
25.4
20.9
14.4
7.7
−2
1946
2.9
−2.1
2.7
13.1
17.7
22.8
24.9
29.7
22.2
19.3
5.2
3.7
Year
1947
1.3
4.6
13.8
16
16.9
24.5
26.9
23
20.9
17.1
9.3
4.9
1948
−3.1
1.4
6.6
15.4
18.4
24.1
26.1
26
22.4
13.8
8.9
3.7
1949
0.6
2.7
6.7
12.3
20.6
25.3
27
25.5
18.4
15.9
9.4
4.9
Temperatures
Time (year and quarters)
1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3
48
47
46
45
–5
0
36
Temperature
Figure 8.17
The periodicity of temperature data.

8.5
A Rule Induction and Model Building Example
■
293
Figure 8.19 shows just one periodic cycle in the data: the second
month in the ﬁrst quarter of data in each year. However, of course, the
actual periodic behavior is reﬂected in the repeating index values over
the full year (and for each year in the available data). Table 8.8 illustrates
how the data ﬁle for the month and quarter data would be constructed.
Quarterly cycles are independent of the monthly cycle because a quar-
ter also breaks down the year into segments that logically subsume and
deﬁne ongoing temperature changes. Often subcycles are not indepen-
dent of other cycles in the data. For example, if the data ﬁle has daily
temperatures the temperature cycles in a month (1, 2, 3, . . . , 30) only
makes sense within the context of the month index itself. Table 8.9 shows
the data broken down at the day level (which is then aggregated at the
monthly and quarterly levels).
TABLE 8.8
The Annual Temperature Data File (Month and Quarter)
Month
Quarter
Temperature
1
Cycle 1
1
−1.9
2
2
2.2
3
3
12.2
4
Cycle 2
1
13.7
5
2
16.1
6
3
21.3
Cycle 1
7
Cycle 3
1
25.2
8
2
25.4
9
3
20.9
10
Cycle 4
1
14.4
11
2
7.7
12
3
−2
1
Cycle 1
1
2.9
2
2
−2.1
3
3
2.7
4
Cycle 2
1
13.1
5
2
17.7
Cycle 2
6
3
22.8
7
Cycle 3
1
24.9
8
2
29.7
9
3
22.2
10
Cycle 4
1
19.3
11
2
5.2
12
3
3.7

294
■
Chapter 8
Fuzzy Rule Induction
TABLE 8.9
Temperature Data with Month, Quarter, and Daily Index Values
Months and Quarters and Days
Mnth
1
2
Qtr
1
2
Days
1
2
3
4
.....
31
1
2
3
4
.....
28
1945
−1.9
2.2
12.2
13.7
.....
21.3
25.2
25.4
20.9
14.4
.....
−2
1946
2.9
−2.1
2.7
13.1
.....
22.8
24.9
29.7
22.2
19.3
.....
3.7
Year
1947
1.3
4.6
13.8
16
.....
24.5
26.9
23
20.9
17.1
.....
4.9
1948
−3.1
1.4
6.6
15.4
.....
24.1
26.1
26
22.4
13.8
.....
3.7
1949
0.6
2.7
6.7
12.3
.....
25.3
27
25.5
18.4
15.9
.....
4.9
The Importance of Periodicity (Separation of
Recurring Patterns)
The implicit seasonal relationships in temperature across a year tend to
obscure the real power of discovery in data, with many different types of
patterns with various levels of granularity. Because temperatures become
predictably warmer over the months from January through the end of
the summer and then cooler as they approach the end of the year, the
quarterly patterns do not repeat within the year. In many business, gov-
ernment, and scientiﬁc systems many types of periodicity cycles repeat
at various levels of granularity. The following are examples.
■
Annual sales by product line by division
■
Annual highway trafﬁc loads
■
Project expenditures by department
■
Inventory turnover by stock class
■
Application server and database transaction rates
■
Network node trafﬁc by transaction type by node location
■
Web trafﬁc loads
■
Container arrival volumes by port by geography
Modeling application transaction rates on a database server in order to
understand capacity thresholds, performance bottlenecks, and resource
collisions is a good example of a time-series system with many important

8.5
A Rule Induction and Model Building Example
■
295
TABLE 8.10
Database Transactions by Type
Month, Day of Month, Hour, Quarter Hour, Five Minutes
Mnth
1
1
Day
1
1
Hour
1
2
Qtr
1
2
4
1
2
4
5 Min 0005 0010 0015 0020 ..... 0060
0005 0010 0015 0020 ..... 0060
T01
310
318
370
240
.....
280
310
318
370
240
.....
280
T02
18
17
15
11
.....
19
14
17
22
16
.....
28
Type
T03
2310
3218
3370
2240
.....
2280
2310
3317
3270
2440
.....
2980
T04
510
358
670
640
.....
580
518
313
571
644
.....
780
T05
1311
1308
1320
1270
.....
1200
1311
1418
1390
1270
.....
1180
periodicity cycles. Many performance management tools collect transac-
tion rates every ﬁve minutes. These sampled rates can be organized into
time series with various degrees of granularity. Table 8.10 illustrates an
example of transaction rates organized into multiple time series.
In the case of transaction analysis, understanding the periodicity of
transactions over many different time frames is critical to understand-
ing the behavior of application servers, databases, and the network. We
know, for instance, that 10 a.m. Tuesday morning is different from 10 p.m.
Tuesday night, and that the beginning of the month is different from the
middle or end of the month. A decomposition of the transaction traf-
ﬁc into multiple time series with different periodic cycles provides the
mechanism necessary to explore large-scale and ﬁne-grain behaviors.
Rules with Multiple Time Index Values
The number of time index columns associated with the data determines
the granularity and the scope of the model. In the temperature model, the
functional relationship is relatively simple: one dependent variable (tem-
perature) is related to one time index value (the month). In the server
and database transaction model, predicting the transaction rate for a par-
ticular transaction type could involve learning the behavior associated
with, for example, a composite time aggregate (month × day × hour). In
this case, the functional relationship is a bit more complex, as shown in
Expression 8.14.
Rt = f (Mt, Dt, Ht)
(8.14)

296
■
Chapter 8
Fuzzy Rule Induction
where,
Rt
is the dependent variable and represents the transaction rate at
time t
Mt
represents the month at time t
Dt
represents the day of the month at time t
Ht
represents the hour of the day at time t
Exactly like the temperature model, the values for the independent vari-
ables are time index values. This functional relationship is discovered
from the underlying data and is represented by a collection of if-then
rules. These rules will qualify the outcome variable based on the ﬁne
detail of time indexes in the data. For example, a transaction rate rule
might appear as follows.
Transaction Type T01:
if month is M
and day is D
and hour is H then rate is R
Here, M, D, and H are fuzzy intervals associated with the underlying
monthly, day of month, and hour time frames (respectively), and R, the
predicted rate, is a fuzzy number that is created from the association
rules. Thus, this rule would predict the rate R at time t, where t is a
3-tuple value — such as (8,16,22) — which would be the 16th of August
at 10 p.m. This means, of course, that learning the transaction behavior
at some set of time periods requires values for these time periods in order
to execute the model. In dealing with data that has multiple time-series
qualiﬁers, the degree to which you should include additional time index
values should be predicated on the use and degree of resolution you need
in the model. In many data collections, moving up the chain of detail
will require producing new data collections with aggregated data. This
is clearly the case with the transaction data. If we are only interested in
hourly transaction rates, we need to create a new data ﬁle with the 12
ﬁve-minute sample values summed (or averaged) to a single hourly value.
Creating the Temperature Prediction Model
After covering the ways in which time-series data is represented in the
data and the nature of the patterns expressed through this representation,
we now turn to the speciﬁcation of the actual model. This speciﬁca-
tion involves deﬁning the properties of the dependent and independent
variables. In building and validating the temperature forecasting model,
examples from the Fuzzy Data Explorer, a graphical model building and
data exploration system, are used.

8.5
A Rule Induction and Model Building Example
■
297
The Temperature Properties
Temperature is the dependent variable; that is, the outcome of the model.
A set of default data characteristics are supplied by the modeling system
based on a preliminary statistical analysis. The way in which we use a
variable and the nature of its underlying fuzzy partitions are based on
how the variable is declared in the model. Figure 8.18 shows the nature
of the temperature variable, including a compressed statistical frequency
histogram of the underlying data.
Temperature is a continuous double-precision ﬂoating-point variable.
Its UoD (or domain) is extended a small percentage beyond the mini-
mum and maximum values (to accommodate both possible outliers in the
validation set as well as the elastic boundaries of the underlying fuzzy
sets). The actual data statistics are displayed at the bottom of the Variable
Properties screen.
After a variable’s data properties have been deﬁned (or left to their
defaults), its range of values must be partitioned into a collection of over-
lapping fuzzy sets. This partitioning can be done in one of three ways:
(1) manually by deﬁning the number, the position, and shape of the
fuzzy sets, (2) automatically through a uniform placement of fuzzy sets
Figure 8.18
Statistical proﬁle of temperature data.

298
■
Chapter 8
Fuzzy Rule Induction
Figure 8.19
The fuzzy set partitioning for the Temperature variable.
over the domain, and (3) statistically according to the distribution of data
across the domain. Figure 8.19 shows the fuzzy sets, generated through a
statistical mechanism, arrayed along the Temperature variable’s domain.
Figure 8.19 shows two properties: the collection of fuzzy sets and the
distribution of the actual data (the gray area graph along the background).
The left-hand vertical axis shows the degree of membership in the fuzzy
sets. The right-hand vertical axis shows the frequency counts for the data.
The statistical fuzzy set generator divides the variable range into a speciﬁed
number of overlapping sets (in this case, 11 sets). Fuzzy sets are also
named automatically left to right using the ﬁrst three characters of the
variable name.
The Month (Time Index) Properties
The month time index is the dependent variable, with a range from [1]
(January) to [12] (December). Figure 8.20 shows the result of using a
statistical approach to automatically decompose the Month variable into
its fuzzy set partitions.
The level of granularity for the underlying fuzzy sets is closely tied
to the number of index values. We are treating month as a continuous

8.5
A Rule Induction and Model Building Example
■
299
Figure 8.20
Month index decomposed into fuzzy sets.
number. However, it could also be viewed as integer-valued intervals or
categorical data. The choice of representation is often dictated, especially
in a fuzzy model, by the ease in treating the data as a continuous spectrum
of values.
Discovering the Functional Rule Set
Rule discovery has two phases. This ﬁrst phase runs the training ﬁle of
exemplar cases through the rule induction engine to produce a collection
of candidate rules. The second phase compresses the rules to generate the
ﬁnal knowledge base. Figure 8.21 shows the post-generation statistics.
The initial rule-evolver phases read 302 training exemplar cases from
the training ﬁle, rejected 18, and stored 283 (about 94% of the total
possible rules). The second phase, a compression process, conducts a
tournament among rules, reads these 283 intermediate rules, and after
eliminating and resolving ambiguities and rejecting 199 duplicates and
updating 50 intermediate rules, creates a ﬁnal knowledge base of 15 rules.
Figure 8.22 shows a partial view of the rules in the evolver dialog window.

300
■
Chapter 8
Fuzzy Rule Induction
Figure 8.21
Rule discovery statistics.
Figure 8.22
Partial set of generated rules.

8.5
A Rule Induction and Model Building Example
■
301
Figure 8.23
Case (exemplar) support for rule R002.
Rule Case Support
Every rule is produced from a set of exemplars or cases. Tracking these
cases and evaluating their degree of evidence is an important factor in
verifying the knowledge base as well as detecting any anomalous proper-
ties in the data (see also the ambiguous rule display). The case support for
each rule is visible in the rule evolver dialog through the Rule Properties
tab. Figure 8.23 shows the case support for the second rule.
The case support displays the actual records from the training ﬁle
(CaseNo is the record number in the ﬁle), with the value of the variables
used to generate the rule along with their quality degree (effectiveness
times the data reliability belief function). In addition to the number of
cases, we can see the evidence weight for the rule (discussed shortly),
the rule’s overall degree, and the average quality degree for the total
collection of rules.
Ambiguity Detection and Resolution
An ambiguous rules indicator refers to the fact that some patterns in the
data have associated rules with the same antecedent but different con-
sequent (outcome) actions. This means that a possibly faulty knowledge

302
■
Chapter 8
Fuzzy Rule Induction
Y
X
Pattern
A×B×C
Figure 8.24
An ambiguous pattern.
base was evolved (ambiguous rules do not uniquely classify an outcome).
As Figure 8.24 shows, an ambiguous rule set has two possibly equal ways
of predicting or classifying the outcome of a rule.
Ambiguity always comes in sets. That is, at least two rules in the
knowledge base share the same antecedent with different outcomes.
The following two rules illustrate this type of ambiguity.
If A and B and C, then X.
If A and B and C, then Y.
The paired nature of ambiguous rules also explains the 320% of the rule
base statistics. There were 24 ambiguous rules and 15 rules actually writ-
ten to the knowledge base. This is 24/15, or 160%. Because the count of
24 represents two rules, the actual ambiguous-rule-to-ﬁnal-rule percent-
age is twice this amount, or 320%. Ambiguous rules are stored with the
knowledge base and, as shown in Figure 8.25, viewed in the rule evolver
dialog.
Ambiguity arises in several places, and it is the responsibility of the
data mining or knowledge engineer to determine its source. Conﬂicting
patterns can be resident in the data. In this case, the historical record
makes it difﬁcult to induce effective rules because patterns exist that spec-
ify two different actions for the same source. The effectiveness ranking
of rules and the tournament selection (considered next) will naturally
weed out sparse and residual ambiguities of this nature. However, if
a critical population of conﬂicting patterns exists they will persist and
eventually be revealed as active rules. Ambiguity can also arise from the
lack of resolution in the fuzzy sets associated with model variables. This
type of ambiguity can often be tolerated in a model simply because of

8.5
A Rule Induction and Model Building Example
■
303
Figure 8.25
The ambiguous rules.
the noise tolerance of fuzzy models themselves. Knowledge engineers
can reduce fuzzy-set-related ambiguity by slightly increasing the number
of fuzzy sets or changing the membership function from bell-shaped to
trapezoid-shaped surfaces.
Candidate Rule Induction
The preliminary rule evolution process generates a large population of
candidate rules from the data. Many of these are duplicates, and some
of these are ambiguous. Listing 8.4 shows a sample of the intermediate
ﬁle of candidate rules. The intermediate rules ﬁle (created by the rule
evolver) maintains the degree of membership that generated each rule
component. Thus, (ACCEL, S02, 0.5386) indicates the date value xi for
the Acceleration variable has a membership of [0.54] in the outcome fuzzy
set S02. Finally, the candidate rule has its overall quality (or effectiveness)
and its record number in the training ﬁle. This ﬁle (kdrulmat), the rule
matrix, is read to create the ﬁrst set of rules through a tournament process.
Listing 8.5 shows the statistics issued by the rule evolver.
In the evolution phase, each data vector or record in the training ﬁle
is used to generate a pattern relationship. If the pattern has sufﬁcient

304
■
Chapter 8
Fuzzy Rule Induction
Temperature, Tem_02, 0.6172, Month, Mon_01, 0.5703, 0.3520,
3
Temperature, Tem_05, 0.7637, Month, Mon_02, 0.5408, 0.4130,
4
Temperature, Tem_05, 0.9737, Month, Mon_03, 0.6352, 0.6185,
5
Temperature, Tem_06, 0.9604, Month, Mon_04, 0.6080, 0.5840,
6
Temperature, Tem_07, 0.6699, Month, Mon_05, 0.5799, 0.3885,
7
Temperature, Tem_09, 0.6948, Month, Mon_06, 0.5507, 0.3827,
8
Temperature, Tem_09, 0.7778, Month, Mon_07, 0.5206, 0.4049,
9
Temperature, Tem_07, 0.8299, Month, Mon_09, 0.5104, 0.4235,
10
Temperature, Tem_05, 0.7637, Month, Mon_09, 0.5894, 0.4501,
11
Temperature, Tem_03, 0.8418, Month, Mon_10, 0.5605, 0.4719,
12
Temperature, Tem_02, 0.9132, Month, Mon_01, 0.4013, 0.3664,
14
Temperature, Tem_02, 0.8589, Month, Mon_02, 0.5408, 0.4645,
16
Temperature, Tem_05, 0.9934, Month, Mon_03, 0.6352, 0.6310,
17
Temperature, Tem_06, 0.7981, Month, Mon_04, 0.6080, 0.4853,
18
Temperature, Tem_08, 0.9633, Month, Mon_05, 0.5799, 0.5586,
19
Temperature, Tem_09, 0.5987, Month, Mon_06, 0.5507, 0.3297,
20
Temperature, Tem_08, 0.8111, Month, Mon_09, 0.5104, 0.4140,
22
Temperature, Tem_07, 0.9444, Month, Mon_09, 0.5894, 0.5566,
23
Temperature, Tem_02, 0.5408, Month, Mon_10, 0.5605, 0.3031,
24
Temperature, Tem_02, 0.9991, Month, Mon_11, 0.5308, 0.5303,
25
Temperature, Tem_01, 0.7778, Month, Mon_01, 0.4013, 0.3121,
26
Temperature, Tem_02, 0.8111, Month, Mon_01, 0.5703, 0.4626,
27
Temperature, Tem_05, 0.9737, Month, Mon_02, 0.5408, 0.5266,
28
Temperature, Tem_06, 0.9217, Month, Mon_03, 0.6352, 0.5854,
29
Temperature, Tem_06, 0.9843, Month, Mon_04, 0.6080, 0.5985,
30
Temperature, Tem_08, 0.7109, Month, Mon_05, 0.5799, 0.4122,
31
Temperature, Tem_09, 0.9688, Month, Mon_06, 0.5507, 0.5335,
32
Temperature, Tem_08, 0.9878, Month, Mon_07, 0.5206, 0.5143,
33
Temperature, Tem_07, 0.8299, Month, Mon_09, 0.5104, 0.4235,
34
Temperature, Tem_06, 0.9575, Month, Mon_09, 0.5894, 0.5643,
35
Temperature, Tem_04, 0.8644, Month, Mon_10, 0.5605, 0.4845,
36
Temperature, Tem_02, 0.6440, Month, Mon_11, 0.5308, 0.3418,
37
Temperature, Tem_01, 0.7778, Month, Mon_01, 0.5703, 0.4435,
39
Temperature, Tem_03, 0.9922, Month, Mon_02, 0.5408, 0.5366,
40
Listing 8.4
The candidate rules intermediate ﬁle.
strength, it is selected as a rule. In this example, these are two active
ﬁltering constraints.
■
The membership of xi in fuzzy set Am must have a membership greater
than [0.1], the data alpha threshold. A degree of membership below
the data alpha threshold is set to zero. This, in turn, sets the entire
rule degree to zero. Such a record will not be selected.

8.5
A Rule Induction and Model Building Example
■
305
05/09/2004 8.41pm
RULE EVOLVER. MxN Rule Generation
Data File
: c:\annualtemps\temps.dat
Rule File
: c:\annualtemps\kdrulmat.fil
Data Alfa Threshold : 0.10
Belief Confidence
: 1.00
Rule Degree Limit
: 0.20
Degree Calculation
: Product
Number of Variables : 2
Variable
Col
Type
Wght
Fuzzy Sets
-----------
---
----
----
--------------------------------------------
1. Temperature
1
IV
1.000 Tem_00,Tem_01,Tem_02,Tem_03,Tem_04,Tem_05,..
2. Month
0
DV
1.000 Mon_00,Mon_01,Mon_02,Mon_03,Mon_04,Mon_05,..
RULE EVOLVER COMPLETE.
Records (In)
:
302
Rules
(Out)
:
283
Evolver Rate
: 0.94
Listing 8.5
The rule evolver phase.
■
The rule’s degree — E(ri) — must also be above [0.20], the rule degree
limit. An E(ri) less than this is also set to zero, and the rule is not
selected.
In this example, 19 records failed one or the other of these threshold
constraints and were not selected as candidate rules. The remaining 283
candidate rules are used by the induction engine to produce the ﬁnal
knowledge base.
The Compression Tournament
Not all candidate rules will or can be used in the ﬁnal model. Some method
must be employed to select the most effective rules. The tournament
selects a candidate rule, examines the virtual FAM, and implements one
of the following actions.
■
If the virtual FAM cell is empty, the rule is added (inserted) and its
degree is also stored.
■
If the virtual FAM cell is occupied, the two rules contend for a right
to contribute their degree of effectiveness. The rule with the highest
degree wins. If the incoming rule has a lower degree, it is rejected. If
the incoming rule has a higher degree, the stored degree is updated
to the incoming rule’s degree.

306
■
Chapter 8
Fuzzy Rule Induction
In Listing 8.6 we can see the tournament held among all candidate rules
to ﬁnd the rules with the best goodness of ﬁt. Only the count of rules with
an ADDED action actually form part of the ﬁnal knowledge base.
Tournament compression yields an 88% reduction in the number of
rules. Rules can also be eliminated when they lack sufﬁcient case or exem-
plar support in the data. A minimum case threshold can be established,
either as an absolute value (a rule must have at least x number of exem-
plar cases) or as a percentage (the cases must represent x% of the data
space). Although not used in this example, Listing 8.7 shows the same
rule induction process with a 5% case threshold.
Finally, the compression process examines the knowledge base for
ambiguous rules. These are removed, ignored, or reduced depending on
the tournament controls. Ignoring the rules leaves them in the knowl-
edge base, but marks each rule as inactive. Reducing the rules saves the
rule with the highest effectiveness (and deletes all others with the same
antecedent). In our example, we simply remove all ambiguous rules.
Listing 8.8 shows the ambiguity resolution phase of rule induction.
At this point the collection of ﬁnal rules has been selected and cast in
if-then format. These rules now form the core intelligence of the derived
business or policy model. Listing 8.9 shows these rules in their compact
form. This display also includes the number of votes (cases) supporting
the rule and the weight of evidence for the rule. The rules are stored in
the knowledge base in the more conventional if-then format (just as they
appear in Figure 8.22). Because the induced rules are stored as ASCII text
strings, they can be modiﬁed through any text editor. New expert-based
rules can also be added to the knowledge base in the same way. Thus,
the integration of machine-derived and human-derived rules takes place
at this point. Expert rules fused with machine-induced rules provide a
broad base of intelligence for business process models.
Model Execution and Validation
Before a model is deployed it must be validated and tuned. This process
involves executing the rule base against another ﬁle. During validation,
the rule base is executed for each incoming record and a value for the
dependent value is predicted. Because the validation ﬁle also contains the
actual value of the outcome value, a validation manager can measure
the distance between the actual value and the predicted value. This
standard error is computed according to Expression 8.15.
es =
N
i=1
Oa −Op

N
(8.15)

8.5
A Rule Induction and Model Building Example
■
307
05/09/2004 8.41pm
RULE INDUCTION. If-Then Rule Generation
Matrix File
:
c:\annualtemps\kdrulmat.fil
Rule File
:
c:\annualtemps\kdrulfam.fil
Tournament Strategy : (WTA) Winner Takes All
Compression Method
: Unique Antecedents
R U L E
C O M P R E S S I O N
T O U R N A M E N T
RecNo
Action
Degree
Rule
-----
------
------
--------------
1.
ADDED
0.3514
Mon_01: Tem_02
2.
ADDED
0.4203
Mon_02: Tem_05
3.
ADDED
0.6170
Mon_03: Tem_05
4.
ADDED
0.5899
Mon_04: Tem_06
8.
ADDED
0.3876
Mon_05: Tem_07
8.
ADDED
0.3839
Mon_06: Tem_09
7.
ADDED
0.4134
Mon_07: Tem_09
8.
ADDED
0.4238
Mon_09: Tem_07
9.
ADDED
0.4563
Mon_09: Tem_05
10.
ADDED
0.4706
Mon_10: Tem_03
11.
UPDATES
0.3717
Mon_01: Tem_02
12.
ADDED
0.4720
Mon_02: Tem_02
13.
UPDATES
0.6295
Mon_03: Tem_05
14.
REJECTED
0.5899>0.4896
Mon_04: Tem_06
15.
ADDED
0.5540
Mon_05: Tem_08
16.
REJECTED
0.3839>0.3304
Mon_06: Tem_09
17.
ADDED
0.4118
Mon_09: Tem_08
18.
UPDATES
0.5635
Mon_09: Tem_07
19.
ADDED
0.3032
Mon_10: Tem_02
20.
ADDED
0.5365
Mon_11: Tem_02
21.
ADDED
0.3178
Mon_01: Tem_01
22.
UPDATES
0.4646
Mon_01: Tem_02
23.
UPDATES
0.5361
Mon_02: Tem_05
24.
ADDED
0.5842
Mon_03: Tem_06
:
:
36.
ADDED
0.3592
Mon_04: Tem_07
37.
REJECTED
0.5540>0.4955
Mon_05: Tem_08
38.
UPDATES
0.5422
Mon_06: Tem_09
39.
UPDATES
0.5046
Mon_07: Tem_09
40.
UPDATES
0.4446
Mon_09: Tem_08
Listing 8.6
The candidate rule tournament.

308
■
Chapter 8
Fuzzy Rule Induction
41.
UPDATES
0.5820
Mon_09: Tem_05
42.
REJECTED
0.4824>0.4009
Mon_10: Tem_04
43.
REJECTED
0.5365>0.5365
Mon_11: Tem_02
44.
REJECTED
0.4459>0.3990
Mon_01: Tem_01
45.
UPDATES
0.4902
Mon_01: Tem_02
46.
REJECTED
0.5461>0.5461
Mon_02: Tem_03
47.
REJECTED
0.6295>0.4838
Mon_03: Tem_05
48.
UPDATES
0.5478
Mon_04: Tem_07
49.
ADDED
0.4466
Mon_05: Tem_09
50.
REJECTED
0.5422>0.5182
Mon_06: Tem_09
51.
REJECTED
0.5046>0.4508
Mon_07: Tem_09
52.
REJECTED
0.5635>0.2979
Mon_09: Tem_07
53.
REJECTED
0.5721>0.5510
Mon_09: Tem_06
54.
UPDATES
0.5122
Mon_10: Tem_04
55.
REJECTED
0.5365>0.3468
Mon_11: Tem_02
56.
REJECTED
0.4902>0.4064
Mon_01: Tem_02
57.
UPDATES
0.5215
Mon_01: Tem_02
58.
REJECTED
0.4720>0.3556
Mon_02: Tem_02
59.
ADDED
0.6088
Mon_03: Tem_04
60.
UPDATES
0.6096
Mon_04: Tem_07
61.
REJECTED
0.5540>0.5500
Mon_05: Tem_08
62.
ADDED
0.3438
Mon_06: Tem_08
63.
REJECTED
0.5261>0.5128
Mon_07: Tem_08
64.
REJECTED
0.5635>0.5062
Mon_09: Tem_07
65.
REJECTED
0.5721>0.3104
Mon_09: Tem_06
66.
UPDATES
0.5384
Mon_10: Tem_02
67.
ADDED
0.2393
Mon_11: Tem_01
68.
REJECTED
0.4459>0.4074
Mon_01: Tem_01
69.
REJECTED
0.5215>0.2902
Mon_01: Tem_02
70.
REJECTED
0.4720>0.4473
Mon_02: Tem_02
71.
REJECTED
0.6088>0.4606
Mon_03: Tem_04
72.
UPDATES
0.6104
Mon_04: Tem_07
73.
REJECTED
0.5540>0.5026
Mon_05: Tem_08
74.
REJECTED
0.5422>0.5007
Mon_06: Tem_09
75.
REJECTED
0.5261>0.2736
Mon_07: Tem_08
:
:
264.
REJECTED
0.6132>0.6132
Mon_04: Tem_06
265.
REJECTED
0.5730>0.3430
Mon_05: Tem_09
266.
REJECTED
0.5496>0.4302
Mon_06: Tem_09
267.
REJECTED
0.5304>0.3690
Mon_07: Tem_09
268.
REJECTED
0.5635>0.4801
Mon_09: Tem_07
Listing 8.6
Continued.

8.5
A Rule Induction and Model Building Example
■
309
269.
REJECTED
0.5937>0.5623
Mon_09: Tem_05
270.
REJECTED
0.5540>0.5422
Mon_10: Tem_03
271.
UPDATES
0.5368
Mon_11: Tem_01
272.
REJECTED
0.5662>0.3438
Mon_01: Tem_01
273.
REJECTED
0.5406>0.5406
Mon_02: Tem_02
274.
REJECTED
0.6295>0.6170
Mon_03: Tem_05
275.
REJECTED
0.6140>0.4713
Mon_04: Tem_07
276.
REJECTED
0.5743>0.5321
Mon_05: Tem_08
277.
REJECTED
0.5496>0.5376
Mon_06: Tem_09
278.
REJECTED
0.5304>0.3175
Mon_07: Tem_09
279.
REJECTED
0.5635>0.4542
Mon_09: Tem_07
280.
UPDATES
0.5977
Mon_09: Tem_05
281.
REJECTED
0.5540>0.5231
Mon_10: Tem_03
282.
REJECTED
0.5368>0.5318
Mon_11: Tem_01
283.
REJECTED
0.5368>0.5318
Mon_11: Tem_01
-------------------------------------------------
COMPRESSION TOURNAMENT COMPLETED
-------------------------------------------------
Candidate Rules In : 283
Actual
Rules Out
:
34
Percent(%)Rules
:
12.014
Listing 8.6
Continued.
Here,
es
is absolute average standard error of estimate
N
is the number of observations in the data ﬁle
Oa
is the actual outcome
Op
is the predicted outcome
Obviously, if Oa equals Op for all records in the validation ﬁle the cumu-
lative standard error will be zero and the model is working perfectly. In
the real world, all models have varying degrees of standard error. Figure
8.26 shows the predicted versus actual performance for the 25 induced
rules across 86 records in the validation ﬁle.
With one or two under- or overﬁtting points, the model works fairly
well. This model has a standard error of 2.71%, meaning that it ﬁts the data
well within 97%. From a fuzzy model perspective, an average compatibil-
ity index (CIX) measure of [.37] is also very good. The CIX measures the
amount of evidence that went into the outcome prediction. For a series of
model executions, the average value should be somewhere in the middle
range (say from .30 to .70). Figure 8.27 shows a plot of the CIX values for

310
■
Chapter 8
Fuzzy Rule Induction
RULES WITHOUT SUFFICIENT CASE SUPPORT
Case Threshold :
8.00%
Rule
(%)
No.
Cases
Pct
Rule
-----
-------
------
--------------
R0002
2 0.71
(0.54)
Mon_02: Tem_05
R0004
9 3.18
(0.61)
Mon_04: Tem_06
R0005
2 0.71
(0.39)
Mon_05: Tem_07
R0007
13 4.59
(0.53)
Mon_07: Tem_09
R0009
13 4.59
(0.60)
Mon_09: Tem_05
R0010
13 4.59
(0.55)
Mon_10: Tem_03
R0011
6 2.12
(0.54)
Mon_02: Tem_02
R0013
9 3.18
(0.51)
Mon_09: Tem_08
R0014
4 1.41
(0.55)
Mon_10: Tem_02
R0015
10 3.53
(0.54)
Mon_11: Tem_02
R0017
4 1.41
(0.63)
Mon_03: Tem_06
R0018
11 3.89
(0.53)
Mon_07: Tem_08
R0019
10 3.53
(0.60)
Mon_09: Tem_06
R0020
8 2.83
(0.51)
Mon_10: Tem_04
R0021
13 4.59
(0.55)
Mon_02: Tem_03
R0022
14 4.95
(0.61)
Mon_04: Tem_07
R0023
5 1.77
(0.57)
Mon_05: Tem_09
R0024
5 1.77
(0.63)
Mon_03: Tem_04
R0025
6 2.12
(0.51)
Mon_06: Tem_08
R0026
13 4.59
(0.54)
Mon_11: Tem_01
R0027
1 0.35
(0.57)
Mon_01: Tem_03
R0028
2 0.71
(0.43)
Mon_02: Tem_01
R0029
1 0.35
(0.42)
Mon_04: Tem_05
R0030
1 0.35
(0.50)
Mon_04: Tem_08
R0031
1 0.35
(0.26)
Mon_01: Tem_00
R0032
2 0.71
(0.48)
Mon_02: Tem_04
R0033
1 0.35
(0.29)
Mon_11: Tem_00
R0034
1 0.35
(0.36)
Mon_11: Tem_03
Rules In
:
34
Rules Removed :
28
Rules Out
:
6
Listing 8.7
Insufﬁcient case evidence ﬁltering.

8.5
A Rule Induction and Model Building Example
■
311
RULES WITHOUT SUFFICIENT CASE SUPPORT
Case Threshold :
0.00%
Rules In
:
34
Rules Removed
:
0
Rules Out
:
34
Evidence FKB Compression. Rules Out: 34
1.
0.52 Mon_01: Tem_02
1.
0.52 Mon_01: Tem_02
31.
0.26 Mon_01: Tem_00
2.
0.54 Mon_02: Tem_05
2.
0.54 Mon_02: Tem_05
28.
0.43 Mon_02: Tem_01
32.
0.48 Mon_02: Tem_04
3.
0.63 Mon_03: Tem_05
3.
0.63 Mon_03: Tem_05
4.
0.61 Mon_04: Tem_06
29.
0.42 Mon_04: Tem_05
30.
0.50 Mon_04: Tem_08
8.
0.39 Mon_05: Tem_07
8.
0.39 Mon_05: Tem_07
25.
0.51 Mon_06: Tem_08
18.
0.53 Mon_07: Tem_08
8.
0.56 Mon_09: Tem_07
13.
0.51 Mon_09: Tem_08
8.
0.56 Mon_09: Tem_07
14.
0.55 Mon_10: Tem_02
20.
0.51 Mon_10: Tem_04
15.
0.54 Mon_11: Tem_02
33.
0.29 Mon_11: Tem_00
34.
0.36 Mon_11: Tem_03
Ambiguity FKB Compression. Rules Out :
15
Listing 8.8
Compressing ambiguous rules.
each model execution cycle. The cycle at which the CIX drops close to
zero often corresponds to weakly learned or poorly generalized patterns,
and is consequently a point at which the prediction and the actual values
do not match well.
Fundamentally, a model validation is the process of running a fuzzy
model and checking the results. This involves the use of a fuzzy inference

312
■
Chapter 8
Fuzzy Rule Induction
Evidence
Rule
Votes
Weight
F U Z Z Y
R U L E
----
-----
------
------------------
1.
6
0.02
Mon_06: Tem_09
2.
11
0.04
Mon_07: Tem_09
3.
13
0.05
Mon_09: Tem_05
4.
8
0.03
Mon_10: Tem_03
8.
6
0.02
Mon_02: Tem_02
8.
18
0.07
Mon_05: Tem_08
7.
19
0.07
Mon_01: Tem_01
8.
4
0.02
Mon_03: Tem_06
9.
10
0.04
Mon_09: Tem_06
10.
13
0.05
Mon_02: Tem_03
11.
14
0.05
Mon_04: Tem_07
12.
5
0.02
Mon_05: Tem_09
13.
5
0.02
Mon_03: Tem_04
14.
13
0.05
Mon_11: Tem_01
15.
1
0.00
Mon_01: Tem_03
RULE DISCOVERY. FKB Generation Complete
Total Rules
:
283
Rules Used
:
15
Rules Rejected
:
199
Rules Updated
:
50
Pct Rules Used
:
8.30
Ambiguous Rules :
24
FKB Dimensions
:
1
FKB Rule Space
:
11.000000
FKB Utilization :
1.364
Listing 8.9
The ﬁnal knowledge base.
engine (to load and actually run the rules) and a results manager to com-
pare the output of the inference engine with the outcome variable’s value
in the validation ﬁle.
8.6
Measuring Model Robustness
Robustness is how well a model performs over time. The initial robust-
ness of a discovered rule set is determined by the average standard error

8.6
Measuring Model Robustness
■
313
Figure 8.26
Model validation performance.
Figure 8.27
Model compatibility proﬁle.

314
■
Chapter 8
Fuzzy Rule Induction
measurement when the validation sets are run through the rules. This is
possible because we have the actual outcome and the predicted outcome
values. When a model is deployed, however, we have as a general case
only the predicted outcome from the rule set. Over time, the robustness
(or precision) of the model can decay due to a variety of factors, such
as changes in the range of data values. When the range of data values
shifts, the mapping between the rules and their underlying fuzzy sets
will also change. We can detect this shift by looking at the amount of
evidence in model executions over time. This evidence is reﬂected in the
compatibility index of the solution variable.
The Compatibility Index: A Measure of Evidence
The solution variable’s compatibility index (CIX) is a measure of the under-
lying evidence in the solution. The index is the maximum membership (or
truth) value in the composite fuzzy set associated with the solution vari-
able. The greater the membership value the more truth in the combined
rule predicates. For example, consider a one-rule system (from the pen-
dulum balancing system). Figure 8.28 shows the height of the outcome
fuzzy set when the rule executes.
The height of the outcome fuzzy set, [.48], represents the amount
of evidence in the two predicate propositions. The outcome fuzzy set is
adjusted (either scaled or truncated) by the truth of the predicate. If the
amount of evidence in the rule predicate is reduced, the height of the
outcome variable’s fuzzy set is also reduced. For example, Figure 8.29
shows the outcome height when the predicate has a minimal amount of
evidence.
When multiple rules are executed, the outcome fuzzy sets are com-
bined either through a composite maximum technique or by adding the
fuzzy sets together. The composite maximum approach continually com-
bines the outcome fuzzy sets, maintaining the maximum of all adjusted
solution fuzzy sets. The addition approach continually combines the
outcome fuzzy sets by adding the adjusted fuzzy sets on a membership-
by-membership basis. Figure 8.30 illustrates this process for three rules
that generate a solution fuzzy set.
When all rules have been executed, the resulting solution fuzzy set’s
height will reﬂect the amount of evidence in all rules that contributed to
the solution. On an individual model run, the compatibility index may or
may not be important (its importance depends primarily on whether or
not a signiﬁcant loss of evidence or an abundance of evidence is critical
in interpreting a speciﬁc outcome event).

8.6
Measuring Model Robustness
■
315
Degree of 
membership  m(x)
Angle from vertical
–25
–15
–5
5
15
25
1.0
BN
SN
ZE
Theta
SP
BP
if Theta is SP and DeltaTheta is ZE then Motorforce is SN
0
Degree of 
membership m(x)
Angular momentum
–25
–15
–5
5
15
25
1.0
BN
SN
ZE
Delta theta
SP
BP
0
Angular momentum
–25
–15
–5
5
15
25
1.0
BN
CIX = .48
SN
ZE
Motor force (under-generation)
SP
BP
0
[.70]
[.48]
Min
Figure 8.28
The compatibility index (example 1).
Assessing Robustness Using the Compatibility
Index
Over the lifetime of a model, however, the CIX is a measure of a model’s
robustness and resilience. If the CIX is plotted over time, it should gen-
erally ﬂuctuate in a general band just above the [.5] membership level.
This band represents a well-formed fuzzy model with sufﬁcient evidence
to support the model’s outcome conclusions. Figure 8.31 illustrates a
typical distribution of compatibility index values over several execution
periods.
The underlying evidence associated with an outcome provides a con-
sistent and domain-free method of assessing the robustness of a model. As
long as the CIX remains in a more or less straight line across the execution
horizon, the amount of evidence in the outcome stays roughly constant.
The outcome is therefore consistent with the design parameters of the

316
■
Chapter 8
Fuzzy Rule Induction
Degree of 
membership m(x)
Angle from vertical
–25
–15
–5
5
15
25
1.0
BN
SN
ZE
Theta
SP
BP
if Theta is SP and DeltaTheta is ZE then MotorForce is SN
0
Degree of 
membership m(x)
Angular momentum
–25
–15
–5
5
15
25
1.0
BN
SN
ZE
Delta theta
SP
BP
0
Angular momentum
–25
–15
–5
5
15
25
1.0
BN
CIX = .09
SN
ZE
Motor force (under-generation)
SP
BP
0
[.09]
[.24]
Min
Figure 8.29
The compatibility index (example 2).
model (that is, the induced rules still reﬂect the underlying patterns in the
data).
The relationship between a model’s trustworthiness and the under-
lying evidence in the outcome variables provides a mechanism for
measuring the stability and robustness of the model itself. Although indi-
vidual CIX values tell us little about a model’s long-term state, a change
in the robustness or precision of the model can be detected by examin-
ing the slope of a regression line through the CIX values. As Figure 8.32
illustrates, a signiﬁcant change in the trend line through the CIX history
indicates a lack of predictive robustness in the model.
Why does a trend either up or down over time indicate a loss of model
robustness? Oddly enough, due to the nature of fuzzy models even a
trend line toward the [1.0] membership threshold is a cause of concern.
The reason a movement in either direction is important is related to how
the CIX is generated (see the previous section “The Compatibility Index:
A Measure of Evidence”). A CIX value that is consistently close to [1.0]

8.6
Measuring Model Robustness
■
317
Rule 3
Rule 1
Rule 2
Consequent (outcome)
fuzzy sets
Solution
fuzzy sets
CIX
1
0
1
0
1
0
1
0
Figure 8.30
The compatibility index (example 3).
Compatibility index µ(x)
Execution periods
12
8
10
14
16
18
20
22
24
26
28
30
32
0
1.0
0.5
Compatibility index distribution
Figure 8.31
The typical distribution of CIX values.

318
■
Chapter 8
Fuzzy Rule Induction
Compatibility index µ(x)
Execution periods
12
8
10
14
16
18
20
22
24
26
28
30
32
0
1.0
0.5
Compatibility index regression
Figure 8.32
The CIX trend line.
means that the truth in all contributing rule predicates is very close to
[1.0]. This, in turn, means that all the incoming model data points fall
close to the upper (or lower) boundary of the underlying fuzzy sets. This
also means that the active regions of the fuzzy sets deﬁning the variables
have become restricted to a small region of the data space. Hence, the data
points are no longer positioned in the overlap of multiple fuzzy sets where
they would naturally acquire and accumulate the semantics of multiple
decision spaces. The model has thus become brittle and lacks robustness
and ﬂexibility.
Detecting and Correcting a Lack of Robustness
One way to measure the stability and robustness of a fuzzy model is to use
another fuzzy model (a fuzzy expert system) to ﬁt a polynomial regression
line through a historical vector of CIX values and then measure the degree
of signiﬁcance in the slope of the trend line. Figure 8.33 illustrates the
schematic organization of this arrangement.
An advantage of a fuzzy expert system in this situation is the control
it brings to understanding the changes in the trend line from a semantic
viewpoint. Although many possible organizations of the fuzzy sets are
possible, a simple design involves just the degree of the slope and the
corresponding criticality classiﬁcation.

8.6
Measuring Model Robustness
■
319
Rule 5
Rule 3
Fuzzy
model
Rule 1
Rule 4
Rule 2
CIX trend
generator
Assessment
Outcome
Trend
analysis
expert system
1.0
0
x
µ(x)
CIX
CIX
Trend
rules
Execution periods
1
0
Slope
Figure 8.33
The self-monitoring fuzzy model architecture.
Fuzzy Set Decomposition and Rules
The slope of the trend line can be decomposed into a set of semantic fuzzy
sets deﬁning the nature of the slope relative to a straight line. Figure 8.34
shows one possible decomposition of the slope variable.
The trend line’s slope is decomposed into three broad areas: a small
up or down movement, a medium up or down movement, and a large up
or down movement. Based on the angular change in the slope, the
state of the trend line can be classiﬁed as Normal, Caution, or Critical.
Figure 8.35 shows a possible decomposition for these outcome states.
The behaviors for this model are very simple. The trend condition
is Normal to the degree to which the slope angle is small; the trend
condition is Caution to the degree that the slope angle is medium; and
the trend condition is Critical to the degree that the slope angle is large.
Expressed as fuzzy rules, the behaviors appear as follows.
If DegreeOfSlope is Small then TrendCondition is Normal.
If DegreeOfSlope is Medium then TrendCondition is Caution.
If DegreeOfSlope is Large then TrendCondition is Critical.
When these rules are executed, the slope angle will generate an outcome
that combines the evidence for membership in one or more adjacent fuzzy
regions and produces a scaled result in the range [0,100], indicating the

320
■
Chapter 8
Fuzzy Rule Induction
Grade of membership µ(x)
Degree of slope (θ)
0
5
10
15
20
25
30
35
40
45
50
55
60
0
1.0
Small
Medium
Large
CIX degree of slope
Figure 8.34
The fuzzy sets for the trend slope.
Grade of membership µ(x)
Criticality of trend slope
0
10
20
30
40
50
60
70
80
90
100
0
1.0
Normal
Caution
Critical
CIX trend condition 
Figure 8.35
The fuzzy sets for the trend criticality condition.

8.6
Measuring Model Robustness
■
321
degree to which the angle represents a signiﬁcant and important change
in the CIX trend line.
Methods for Correcting the Model
Once a shift in the CIX trend is detected, there are several broad meth-
ods of repairing the model, described in the following. Most of these
methodologies are accomplished “off line”; that is, the model is repro-
cessed and a new model is created, validated, and redeployed. However,
the use of adaptively selected rules is designed to work in real time with
the execution of a deployed model.
■
Rediscover the rules and build a new model. In this approach, all
rules are discarded. New training and validation sets are produced
using the same sampling techniques (generally) that produced the
original training and validation sets. This approach often requires a
redesign and analysis of the data, as well as a reengineering of the
knowledge base. The approach, however, is fundamentally the same
as the basic rule induction algorithm discussed in this chapter.
■
Dynamically regenerate rules and test them for effectiveness. This
approach forms the core of an adaptive system that uses the rule
induction process as a form of machine learning. Instead of discard-
ing the existing rule set, the model collects its own historical data
from the current input. When sufﬁcient history is collected, the rule
induction process is called to produce a new set of rules. These rules
are combined with the existing rules. Those rules, new or old, that
are not executed with a minimum degree of frequency (or degree
of evidence) are removed. Over time, this strategy will replenish
the knowledge base with rules that automatically adapt themselves
to changes in the data. Figure 8.36 is a high-altitude schematic of
how a system incorporating dynamically regenerated rules might be
designed.
In this design, an ongoing data history for each variable is auto-
matically maintained by the model. Whenever the assessment value
from the trend analysis exceeds a speciﬁc threshold, the rule induc-
tion engine is started. Rule induction generates a new set of rules and
inserts these into the current knowledge base. As the model executes
the rules, a tournament facility evaluates each rule (new and old) for
effectiveness. The most effective rules, based on the statistics of exe-
cuting and amount of evidence (the CIX value), are kept in the model
and the less effective rules are gradually removed.

322
■
Chapter 8
Fuzzy Rule Induction
Adaptive rule generation initiator
Effectiveness
filter
Fuzzy
model
Rule
tournament
CIX trend
generator
Assessment
Outcome
Trend
analysis
expert system
1.0
0
x
µ(x)
CIX
CIX
Trend
rules
Data
history
Execution periods
1
0
Slope
Rule 5
Rule 3
Rule 1
Rule 4
Rule 2
Rule
induction
Figure 8.36
A system with dynamically regenerated rules.
■
Retune the model’s infrastructure conﬁguration by adjusting the
fuzzy sets for one or more variables. If the underlying patterns in
the data remain more or less constant but the data has been chang-
ing, the model can conceivably be corrected by changing the type,
number, and overlap of the fuzzy sets underlying each variable. A mul-
tiobjective and multiconstraint genetic algorithm is used to generate,
breed, and test a large population of new models. In this approach,
new rules are not created (we are modifying the fuzzy decision space
in the existing rules to accommodate new data ranges). Figure 8.37
shows a high-altitude schematic view of this genetic tuning.
In this design, like the rule regeneration, an ongoing data history
for each variable is automatically maintained by the model (per the
following).
①Whenever the assessment value from the trend analysis
exceeds a speciﬁc threshold, the genetic fuzzy set optimizer
engine is started. The underlying genetic algorithm uses the
existing set of rules and generates a population of feasible mod-
els (the ﬁxed set of rules), each with a random set of fuzzy sets
for each dependent and independent variable.

8.7
Technical Implementation
■
323
Breed
new
population
Fuzzy set configuration initiator
Fuzzy
model
Goodness
of fit
Continue?
CIX trend
generator
Assessment
Outcome
Trend
analysis
expert system
1.0
0
x
µ(x)
CIX
CIX
Trend
rules
Data
history
Execution periods
1
0
Slope
Rule 5
Rule 3
Rule 1
Rule 4
Rule 2
3
Genetic
optimizer
1
4
2
Stop
Figure 8.37
Genetically tuning the rule fuzzy sets.
②Each new fuzzy model in the current population of models is
executed against the validation data.1
③The standard error of estimate for the new model is used by
the genetic algorithm as its goodness of ﬁt. The GA attempts
to minimize this standard error.
④If we have found a minimal and stable standard error, the
genetic tuner stops and the current rules with their fuzzy set
organization are moved into production.
Otherwise, a new population of candidate models is created and the
process of standard error minimization continues. This is, of course,
the same process used to optimize a fuzzy model created by the initial
rule induction algorithm.
8.7
Technical Implementation
The architecture of the rule induction engine and the surrounding
business process modeling capabilities integrate several different data
1 Note that we do not need a training set in this approach because we are using the existing rules
that were initially generated from the original training set. We only need to use a validation set to
measure how well a new model with its new fuzzy set organization predicts a correct response.

324
■
Chapter 8
Fuzzy Rule Induction
4
Model
definition
Training
examples
Validation
data
Fuzzy
knowledge base
Define
properties
Data
attributes
Fuzzy
sets
Read
training file
Create
model
Discover
rules
Validate
model
Select
variables
Re-define
Re-train
Figure 8.38
The rule induction and modeling process.
structures and code features. In this section we take up the issues asso-
ciated with the actual implementation of a rule induction facility and a
host modeling environment. This section can be bypassed for those not
interested in the underlying code model.
Using the rule induction software requires an integration of several
different technical engines: a variable properties manager, a model frame-
work manager, the rule induction engine, and a fuzzy inference engine
to run the rules. Figure 8.38 provides a high-altitude schematic of these
functional parts.
The overall process is relatively straightforward. The training ﬁle of
examples is read to pick up the column ﬁeld names. These are the
attributes of the model. A number of the ﬁelds are selected as attributes of
the model (or are all selected). For each selected ﬁeld, the model builder
can deﬁne a set of data properties and a collection of underlying fuzzy
sets (fuzzy set decomposition can be done automatically by the variable
property manager). When all variable properties have been deﬁned, an
initial model is created. The model acts as a named repository for all of its
components. After a model is created, the rule induction engine is used

8.8
External Controls
■
325
TABLE 8.11
The Automobile Performance File
MPG,
CYLS, DISP, HP,
WGT,
ACCEL, YR, ORIGIN, CAR_NAME
18.0
8
307.0 130.0 3504. 12.0
70
1
"chevrolet chevelle malibu"
15.0
8
350.0 165.0 3693. 11.5
70
1
"buick skylark 320"
18.0
8
318.0 150.0 3436. 11.0
70
1
"plymouth satellite"
16.0
8
304.0 150.0 3433. 12.0
70
1
"amc rebel sst"
17.0
8
302.0 140.0 3449. 10.5
70
1
"ford torino"
15.0
8
429.0 198.0 4341. 10.0
70
1
"ford galaxie 500"
14.0
8
454.0 220.0 4354. 9.0
70
1
"chevrolet impala"
to create the underlying rule (or knowledge) base. This knowledge base
is used by the fuzzy inference engine to actually run the rules against the
validation data ﬁle. Validation measures how well the model performs
by keeping track of the average standard error as it predicts an outcome
value for each incoming record. If the model does not perform well, you
have two basic choices: retrain the model or redeﬁne the model variables.
In practice, these functional parts are independent pieces of systems
tied together through a common structural interface (such as the graphical
user interface of the Fuzzy Data Explorer). However, the process remains
essentially a cohesive iterative loop that converges on a working model.
A ﬁle containing the training examples is identiﬁed. This is a ﬂat, comma,
tab, or space-delimited ASCII ﬁle. The ﬁrst line of the ﬁle contains the
name of the ﬁelds. These are the names that will be used throughout the
modeling process. Table 8.11 shows part of the automobile performance
ﬁle in this format. The ﬁle and variable handler reads the ﬁle to (1) acquire
all column names, (2) ﬁnd all the numeric columns, and (3) determine the
ﬁle length (number of records in the ﬁle). At this point you must select a
set of columns for the model. A model must have a dependent variable (the
outcome) and at least one independent variable. Because this is a fuzzy
model, all variables must have a numeric data type (either continuous or
categorical). At this point we need to understand the basic control blocks
that maintain all data elements, ﬁles, and parameter settings for the model
and rule induction process.
8.8
External Controls
Control controls (or “control blocks”) are external structures housed in
the main code regions of the rule induction and modeling dynamic link
library (DLL). The control blocks provide global access to the main data
elements in the code without the cumbersome overhead of passing large
structures or multiple arrays of variables as parameters. The rule induction

326
■
Chapter 8
Fuzzy Rule Induction
4
Knowledge
base (FAM)
Define
properties
Data
attributes
Fuzzy
sets
Create
model
Discover
rules
Validate
model
Select
variables
Model
builder
XRDStats
Statistics
Rule discovery
Model validation
XRDctl
Parameter controls
Rule discovery
Model validation
XRData
Variable arrays
Variable fuzzy sets
Variable parameters
Knowledge base
Global file controls
Figure 8.39
Use of control blocks.
software uses the control blocks to store all model variables and their
properties, the currently active knowledge base, global ﬁle deﬁnitions,
operating parameters, and post-mortem execution statistics. Figure 8.39
extends Figure 8.38 to show how the control blocks are connected to the
processing architecture.
The actual model deﬁnition and associated controls are centralized in
the XRDData external structure. The maximum number of variables and
other properties (such as fuzzy sets) are controlled by symbolic variables
(found in the rdsyms.hpp header ﬁle). Each variable property is stored
in a parallel array indexed from zero to the current count of variables.
When fuzzy sets are deﬁned for a variable, they are also stored in an array
of fuzzy set descriptor pointers. Figure 8.40 schematically illustrates the
way the controls are organized.
A model variable is a name selected from one of the ﬁeld names in the
training ﬁle. Field names have only intrinsic or default properties (data
type, for example), whereas model variable names have a large number
of extended attributes. These include such properties as the following.
■
Description
■
Unit of measure

8.8
External Controls
■
327
(sXRDDVarNames) Variable Names
(sXRDDVarUOM) Variable Unit of Measure
(sXRDDVarDesc) Variable Description
XRDData
Variable arrays
Variable fuzzy sets
Variable parameters
Knowledge base
Global file controls
(sXRDDVarDomLo) Variable Lower Range
(sXRDDVarDomHi) Variable Upper Range
(sXRDDVarWeights) Variable Field Weights
(sXRDDVarStructure) Variable Structure
(sXRDDFzyAlfaCut) Fuzzy Sets Alpha Cut
(sXRDDVarValues) Variable Current Value
(sXRDDVarScalar) Variable Scalar Width
(sXRDDVarOffset) Variable Offset in File
(sXRDDVarFzySets) Variable Fuzzy Sets
(sXRDDFzyCnt) Variable Fuzzy Set Count
1
0
1
0
1
0
1
0
1
0
1
0
Fuzzy Sets Arrays
(0...RDSETMAX)
...
...
...
...
(sXRDDVarDataType) Variable Data Type
(sXRDDVarCnt) Variable Count
Property Arrays (0...RDVARMAX)
(sXRDDVarRDType) Variable Usage Type
Figure 8.40
Variable properties in XRDData.
■
Domain or permissible range of values
■
Weight used to bias the rule generation
■
Usage type (independent or dependent variable)
■
Explicit data type (integer, ﬂoat, double, Boolean, and so on)
■
Organization structure (continuous, categorical, periodic)
Variables also have semantic properties: their underlying collection of
fuzzy sets. These sets break down the domain of the variable into

328
■
Chapter 8
Fuzzy Rule Induction
overlapping fuzzy regions. Every variable used by the rule induction
engine must have its domain partitioned into fuzzy sets. A set of prop-
erties is deﬁned when a variable is selected from the ﬁeld name. The
attributes are automatically updated from a graphical user interface (such
as the Fuzzy Data Explorer’s Property Manager) or are speciﬁed through
the rule discovery system’s Application Program Interface (API) services.
This control block also has a pointer to the active knowledge base (con-
taining the rules either discovered in the current rule induction process
or loaded into the knowledge base from a stored model). A principal use
of the control block architecture is communication between the C/C++
code and rapid application development (RAD) environments.
Representation of Fuzzy Sets
A fuzzy set is deﬁned and represented by an FDB (fuzzy set descriptor
block) structure. These structures are allocated and controlled within the
C/C++ code space and cannot be directly accessed by Visual Basic and
other languages (such as Java) or by RAD environments (such as Delphi
and Power Builder) except through the API services. Listing 8.10 shows
the structure and content of the FDB. The symbolic constants deﬁning
the dimensions of the structure are found in mssyms.hpp.
In addition to a name and description, a fuzzy set description con-
tains the type of curve (FDBgentype), the fuzzy set’s domain (FDBdo-
main[]), the alpha cut threshold (FDBalfacut), and the parameters that
struct FDB
{
char
FDBid [IDENLEN+1],
// Identifier name of Fuzzyset
FDBdesc[DESCLEN+1];
// Description of Fuzzyset
Ctlswitch
FDBgentype;
// Generator Set type
mbool
FDBempty;
// Is this a populated Fuzzyset?
int
FDBorder,
// Order of fuzzy set
FDBparmcnt;
// Count of stored parms
double
FDBdomain[2],
// Lo and Hi edges of the set
FDBparms[PARMMAX];
// Generation parameters
float
FDBalfacut,
// AlfaCut for this fuzzy set
FDBvector[VECMAX];
// The fuzzy set truth vector
FDB
*FDBnext;
// Pointer to next fuzzyset
};
Listing 8.10
The FDB.

8.8
External Controls
■
329
were used to create the fuzzy set (FDBparms[]). The core descriptive
property of the fuzzy set, however, is its membership function (FDBvec-
tor[0 . . . VECMAX]), deﬁning the actual shape of the fuzzy set’s surface.
Each cell in the FDBvector[] array represents a small region of values
across the variable’s domain. These numbers share the same membership
and deﬁne the lower limits of resolution or granularity for a fuzzy set.
Given a particular scalar number (S) and its domain of allowed values
(dlow and dhigh), from which we can calculate a range interval (R), we
can ﬁnd its membership in a fuzzy set by ﬁnding what cell contains this
value. Expression 8.16 shows the equation for ﬁnding the cell.
cellS =
s −dlow
R

× VECMAX
(8.16)
Then, of course, FDBvector[cellS] is the degree of membership for this
number. This scalar-to-cell relationship also means that given a cell num-
ber we can return the scalar representative of that cell (for details see
FzyGetScalar). Finding the scalar value associated with a particular degree
of membership is not quite as simple, in that no mapping of range to mem-
bership is maintained; that is, the degree of membership for a cell is not
a linear relationship to a cell number (unlike numbers from the domain).
Seeking a number in this manner requires an algorithmic approach (for
details see FzyEquivalentScalar).
Representing the Rule Base
In our previous example of an FAM, the knowledge base was 2D (see
Figure 8.15 for details). Handling this situation in program code is rela-
tively straightforward. One possible declaration is shown in Figure 8.41,
along with the table organization.
FDB *pVert_Var[MAXVERT],
*pHorz_Var[MAXHORZ],
*pFAM[MAXVERT][MAXHORZ];
where, pVert_Var[] contains the fuzzy sets for the variable on the verti-
cal axis of the FAM, pHorz_Var[] contains the fuzzy sets for the variable
on the horizontal axis of the FAM, and pFAM[][] contains the fuzzy sets
associated with the dependent variable at each intersection. This is satis-
factory for handling all systems with two antecedents and one consequent.
However, for rules in the form
if xv1 is A and xv2 is B and xv3 is C then xv4 is Y

330
■
Chapter 8
Fuzzy Rule Induction
Horizontal Variables (pHorz_Var)
h1
h2
h3
. . . . 
hn
v1
v2
FDB
v3
: 
: 
Vertical 
Variable
(pVert_Var)
vm
Figure 8.41
Code declaration for a 2 × 2 FAM.
a 3D array is needed (one axis for each independent variable). In real-
world applications the number of variables in a fuzzy system can be quite
large. A knowledge engineer cannot know a priori the required size of
the fuzzy associative memory. The requirement, especially in a language
such as C/C++, that dimensions be statically deﬁned places a severe limi-
tation on the ﬂexibility of a rule induction tool with a ﬁxed dimensioning
scheme for its rule base. Not only does the changing of array dimen-
sions in the code necessitate signiﬁcant reprogramming of the control
logic but it invariably introduces errors into code that would otherwise
run cleanly. Further, a statically dimensioned FAM substantially restricts
the type of model content optimization performed by such processes as
factor analysis (adding new variables dynamically to the model becomes
impossible).
Rather than allocate arrays of differing sizes in the code and repro-
gramming the inference engine for changes in the dimensionality of our
model, we can deﬁne rules so that they carry their position in a virtual
FAM (the knowledge base). Each rule maintains an array of indexes into
the XRDData control block for each independent variable and a pointer
to the rule’s outcome fuzzy set (associated with the dependent variable).
Listing 8.11 shows the organization and content of the rule description.
This data structure couples the rule deﬁnition to the storage of fuzzy sets
associated with the model variables and reduces the rule processing to
a matter of indexing the fuzzy set arrays. Each rule also has a pointer
to a working outcome fuzzy set (this fuzzy set is used when the aggre-
gation fuzzy reasoning mode is used). Figure 8.42 shows the schematic
relationships between the rule structure and the stored fuzzy sets.
The maximum dimensionality of a fuzzy model is determined by the
RDVARMAX constant (in that each variable, excluding the dependent
or outcome variable, creates one dimension in the knowledge base).

8.8
External Controls
■
331
struct RDR
{
FDB
*pDVFDB;
//--Pointer to Outcome Fuzzy Set
int
iDVIdx;
//--Index to DV fuzzy set
float
fRuleDegree;
//--Degree for this rule
int
iRuleEvidence;
//--Evidence (Count of rule hits)
int
iFzySetIdx[RDVARMAX];
//--Indexes into IV Fuzzy Sets
RDR
*pNextRDR;
//--Pointer to next rule
};
//
Listing 8.11
The rule descriptor (RDR) structure.
Variable arrays
Variable fuzzy sets
Variable parameters
Knowledge base
Global file controls
XRDData
(sXRDDVarFzySets) Outcome Variable
(sXRDDVarFzySets) Dependent Variable1
(sXRDDVarFzySets) Dependent Variable2
(sXRDDVarFzySets) Dependent Variable3
(sXRDDVarFzyAlfaCut) Fuzzy Sets Alpha Cut
A
A
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
A
1
0
1
0
1
0
1
0
1
0
• • •
• • •
• • •
• • •
• • •
pDVFDB
Pointer to outcome FDB
iDVldx
Index
Pointer
Index into outcome FDB
iFzySetldx
Index into independent FDB
Fuzzy Set Arrays (0...RDSETMAX)
Rule
descriptor
(RDR)
Outcome
fuzzy set
Figure 8.42
Rule and fuzzy set relationships.

332
■
Chapter 8
Fuzzy Rule Induction
The iFzySetIdx is the mapping between the rule and the virtual FAM.
This arrangement has several important beneﬁts.
■
Only actual rules consume space. The index for an RDR structure
deﬁnes its location in the virtual FAM space. Thus, the array (2,8,4)
indicates the second fuzzy set for the ﬁrst independent variable, the
eighth fuzzy set for the second independent variable, and the fourth
fuzzy set for the third independent variable. Only this space is occu-
pied in the FAM matrix. Null pointers (usually 4 bytes each) are not
kept for unused rules.
In many rule discovery applications the rule base itself is quite
sparse. This is a consequence of the dimensionality of the system
against the set of all possible exemplars in the data itself. A fuzzy
model, assuming each variable is decomposed into the same number
of fuzzy sets, has a dimensional space (d) equal to
d = Rk,
(8.17)
where R is the number of fuzzy sets per variable and k is the num-
ber of independent (input) variables. This means that a model with
ﬁve variables and ﬁve fuzzy sets per variable will need a dimensional
space of 3125 cells. Even smaller models require a signiﬁcant amount
of training exemplars to fully populate a model. A model with ﬁve
variables, each decomposed into three fuzzy sets, still requites 35 (or
243) exemplars to put a rule into each cell. If we are using monthly
data, this would require 20 years of exemplars to have one rule in
each cell. With fewer than 20 years, the FAM becomes quite sparse.
■
Rules are stored serially in conventional array (see the FKB struc-
ture discussed in material to follow). As rules are added to the
knowledge base, the only data structure required is either a simple
linear array (a vector of rules) or a linked list. Access to the rules
and processing of each rule is very easy. The serial architecture of
rule storage means that various modiﬁcations to the inference engine
can be made without deep and costly changes to the rule access and
processing procedures.
■
Processing is high-speed. The array organization of a rule means that
rule evaluation requires simple indexing. Because the indexes repre-
sent an abstract dimensional space, only a single for loop is necessary.
We thus, in effect, unroll the dimensional space of an FAM and reduce
it to a single vector of offsets. Such unrolling signiﬁcantly improves
efﬁciency.

8.9
Organization of the Knowledge Base
■
333
■
Representational
ﬂexibility
and
compactness
are
increased.
Because fuzzy sets are detached from the rule and the two are linked
only through indexes into arrays, variables as well as fuzzy sets can be
added and deleted easily. Up to the limit imposed by RDVARMAX, new
variables can be added to rules or existing variables removed. Such
expedients support genetic tuning and factor analysis. This works
in two ways. New variables with their properties can be added to
the XRDData control and then referenced by the rules. Alternatively,
rules can ignore or select variables out of the XRDData control, thus
searching the variable space for the best model. The same applies to
fuzzy sets. Without changing the number of fuzzy sets, a model can
explore the effects of shape and overlap changes on a model’s perfor-
mance. The number of fuzzy sets associated with a variable can also be
easily changed. This ﬂexibility in model organization allows various
forms of factor analysis and model genetic tuning to work quickly and
efﬁciently on the model’s structure.
The RDR rule structure, similar to the FDB fuzzy set structure, contains a
pointer to the next instance of itself. These pointers provide, at little space
expense, a method of linking these objects into hash tables or linear lists.
Unlike fuzzy sets, rules formed by the induction engine lack an identiﬁer
(other than their order in the knowledge base), and thus they would need
a tag of some type before they could be inserted into a conventional hash
table.
8.9
Organization of the Knowledge Base
The knowledge base organizes a collection of rules for execution. In a
fuzzy model, all rules are run in parallel, and thus we need only a way
of effectively accessing and evaluating the rules. Listing 8.12 shows the
structure of the fuzzy knowledge base (FKB). A knowledge base provides
a container for rules. They are organized in a vector and sorted by their
independent variable index values (in increasing order). As Figure 8.43
illustrates, there are few important controls in the knowledge base: simply
the count of independent variables (to speed up access) and the count of
actual number of rules in the vector.
In this structure the knowledge base holds a maximum of RDRULE-
MAX (found in the rdsyms.hpp header ﬁle). The knowledge base also
maintains a list of active and inactive rules. This Boolean vector is used
during the training and tuning phase to turn rules on and off. As a compact

334
■
Chapter 8
Fuzzy Rule Induction
struct FKB
{
int
iFKBIVCnt,
//--Independent var Count
iFKBRuleCnt;
//--Total rule in KB
RDR
*pFKBRules [RDRULEMAX];
//--Vector of rules
mbool bFKBActive[RDRULEMAX];
//--Active rule indicator
FDB
*pDVFDB;
//--Dependent var fuzzy set
};
Listing 8.12
The fuzzy knowledge base.
Fuzzy
knowledge base
(iFKBIVCnt) Independant Variable Count
(iFKBRuleCnt) Rule Count
Rule vector
RDR
Figure 8.43
The rule induction knowledge base.
representation of the model rules, the knowledge base constructs a virtual
FAM. These rules are executed for each record from the validation or the
production database. Listing 8.13 shows the simple C/C++ routine that
executes a single fuzzy rule.
There is a one-to-one correspondence between the index in the rule
structure’s iFzySetIdx[] array and the variables appearing in the XRDData
control block. This allows us to invert the rule processing by looping
across the variables and using the index in iFzySetIdx[] to ﬁnd which of
the fuzzy sets is needed. The current data value (dColValue) is used to ﬁnd
the degree of membership. This membership is for each variable used to
compute a ﬁnal membership product (which is the degree for this rule).
If the rule’s degree is above the alpha cut threshold, the rule ﬁres. For
each rule that ﬁres, we pick up the sum of the degree and the sum of the
scaled center of the outcome fuzzy set. These are used by the high-level
model execution facilities to compute a model outcome value after all

8.9
Organization of the Knowledge Base
■
335
void
mpExecuteRule(
RDR
*pRDR,
char
*sTokens[],
int
iTokCnt,
mbool
*bpRuleFired,
double *dpSum_of_Degrees,
double *dpSum_of_Centers,
float
*fpDegree,
int
*ipStatus)
/*----------------------------------------------------------------------*
| This procedure actually executes a single rule in the linear
|
| Fuzzy Knowledge Base. Executing a rule means evaluating all the |
| antecedents, calculating the predicate truth, and then updating |
| the sum of degrees and scaled values.
|
*----------------------------------------------------------------------*/
{
FDB
*pIVFDB;
int
i,
iIdx,
iDVIdx,
iIVset,
iCOffset;
float
fMemVal,
fThisMemVal;
double dX,
dColValue;
*ipStatus=0;
*bpRuleFired=mFALSE;
iDVIdx=1;
fMemVal=(float)1.0;
*fpDegree=(float)0;
for(i=0;i<XRDData.iXRDDVarCnt-1;i++)
{
iCOffset =XRDData.iXRDDVarOffset[iDVIdx];
dColValue=atof(sTokens[iCOffset]);
//
//--Now see which fuzzy set is associated with the rule
//
iIVset=pRDR->iFzySetIdx[i];
pIVFDB=XRDData.pXRDDVarFzySets[iDVIdx][iIVset];
Listing 8.13
The rule execution procedure.

336
■
Chapter 8
Fuzzy Rule Induction
fThisMemVal=FzyGetMembership(pIVFDB,dColValue,&iIdx,ipStatus);
if(bUseRuleWgts) fThisMemVal=fThisMemVal*pRDR->fRuleDegree;
if(i==0) fMemVal=fThisMemVal;
else
fMemVal=fMemVal*fThisMemVal;
iDVIdx++;
}
*fpDegree=fMemVal;
if(fMemVal>fRuleAlfa)
{
(*dpSum_of_Degrees)+=fMemVal;
dX=mpMaximumValue(pOCFDB);
(*dpSum_of_Centers)+=(dX*fMemVal);
*bpRuleFired=mTRUE;
}
else
iAvgRulesBelowAlfa++;
return;
Listing 8.13
Continued.
rules have been ﬁred. The following code shows part of this high-level
manager (somewhat simpliﬁed).
dSum_of_Degrees=0;
dSum_of_Centers=0;
for(i=0;i<pFKB->iFKBRuleCnt;i++)
{
mpExecuteRule(
pFKB->pFKBRules[i],
sTokens,
iTokCnt,
&bFired,
&dSum_of_Degrees,
&dSum_of_Centers,
&fDegree,
ipStatus);
}
if(iRulesFired==0)
{
fCIX=(float)0;
dDVEstimate=(double)SOLUNDEFINED;
iAvgRulesNotFired++;
}

8.10
Review
■
337
If at least one rule was executed, the outcome value (dDVEstimate) is
generated by defuzziﬁcation. The value is a weighted average computed
by dividing the sum of the scaled center valued by the sum of the individual
rule degrees. The compatibility index (degree of evidence in the model’s
solution) is calculated as the average degree.
if(iRulesFired>0)
{
fCIX=(float)(dSum_of_Degrees/(double)iRulesFired);
dDVEstimate=(dSum_of_Centers/dSum_of_Degrees);
if(fCIX<fMinCIX)
iAvgRsltBelowCIX++;
fTotCIX+=fCIX;
}
The technical overview provides an introduction to the underlying data
structures and code models used in the rule induction engine, although
naturally it does not cover all procedures and structures.
8.10
Review
Rule discovery forms the core mechanism in the development of effec-
tive models as well as the implementation of adaptive, self-tuning systems.
An adaptive model learns new rules and adjusts existing rules depending
on changes in the data space. Regenerating rules is an effective strategy
for maintaining a coherent and robust system responsive to shifts in the
modeling environment. In this chapter we examined the fundamental
rule induction mechanism and illustrated its potential for model nonlin-
ear problems. You should now understand the general algorithm and be
familiar with such concepts and ideas as the following.
■
How data variables are partitioned into fuzzy sets
■
The use of quality and belief measures during rule induction
■
How rules are generated and compressed
■
How to handle conﬂicting and ambiguous rules
■
The use of training and validation data
■
The techniques for measuring degrees of error in a generated model
■
How to measure model robustness using the compatibility index
■
Possible ways to restore robustness and precision to a fuzzy model

338
■
Chapter 8
Fuzzy Rule Induction
These are the basic features of the rule induction process. Working mod-
els that represent real-world problems, however, generally require some
extensions to the rule discovery process, which include the ability to
■
Discover and encode lead and lag relationships
■
Incorporate crisp predicates and outcomes
■
Track the effectiveness of discovered rules
■
Tune themselves based on statistical changes in their measures of
evidence
An implementation of these extensions is left to the reader. They are basi-
cally straightforward modiﬁcations to the underlying algorithms (although
the discovery of lead-lag relationships and the tuning of a model based
on evidence in the solution set is often best handled through a genetic
algorithm).
Further Reading
■
Bennett, J. Building Decision Support Systems. Reading, MA: Addison-Wesley,
1983.
■
Berkan, R., and S. Trubatch. Fuzzy System Design Principles: Building Fuzzy
IF-THEN Rule Bases. New York: IEEE Press, 1997.
■
Buchanan, B., and E. Shortliffe. Rule-Based Expert Systems: The Mycin Exper-
iments of the Stanford Heuristic Programming Project. Reading, MA:
Addison-Wesley, 1984.
■
Cox, E. The Fuzzy Systems Handbook (2d ed.). San Diego, CA: Academic Press
Professional, 1999.
■
Goonatilake, S., and S. Khebbal (eds.). Intelligent Hybrid Systems. Chichester,
UK: John Wiley and Sons, 1995.
■
Keen, P., and M. Morton. Decision Support Systems: An Organizational
Perspective. Reading, MA: Addison-Wesley, 1978.
■
Kosko, B. Neural Networks and Fuzzy Systems: A Dynamical Systems Approach
to Machine Intelligence. Englewood Cliffs, NJ: Prentice-Hall, 1991.
■
Martino, J. Technological Forecasting for Decision Making (3d ed.). New York:
McGraw-Hill, 1993.
■
Rao, V., and H. Rao. C++ Neural Networks and Fuzzy Logic (2d ed.). New York:
MIT Press, 1995.

Further Reading
■
339
■
Wang, L-X. Adaptive Fuzzy Systems and Control, Design and Stability Analysis.
Englewood Cliffs, NJ: Prentice-Hall, 1994.
■
Wang, L-X., and Mendel, J. M. “Generating Fuzzy Rules from Numerical Data with
Applications,” USC-SIPI Report No. 169.
■
Welstead,
S. Neural Network and Fuzzy Logic Applications in C/C++.
New York: John Wiley and Sons, 1994.

THIS PAGE INTENTIONALLY LEFT BLANK

■
■
■
Part III
Evolutionary Strategies
Contents
Chapter 9
■■■
Fundamental Concepts of Genetic
Algorithms
343
Chapter 10
■■■
Genetic Resource Scheduling Optimization
421
Chapter 11
■■■
Genetic Tuning of Fuzzy Models
483
341

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 9
Fundamental Concepts of
Genetic Algorithms
Evolutionary strategies address highly complex optimization and search
problems through an emulation of natural selection. They also incorporate
a form of parallel processing to effectively evaluate a large population of
possible solutions. Their ability to solve high-dimensional, highly complex
problems that are often intractable, slow, brittle, or difﬁcult to formulate
with conventional analytical techniques has made genetic algorithms and
evolutionary programming a critical component in intelligent systems that
require adaptive behavior, systematic exploration of alternatives, and mul-
tiobjective and multiconstraint optimization. This chapter introduces the
concepts underlying genetic algorithms and evolutionary programming.
The concepts in this chapter are necessary in order to understand the nature
of genetic algorithms and evolutionary programming in the context of fuzzy
model tuning and in the context of advanced predictive and classiﬁcation
models (topics covered in the remaining chapters of Part III).
Although not the ﬁrst to explore the idea of combining the mechanics
of evolution and computer programming, the ﬁeld of genetic and evo-
lutionary algorithms can be traced back to John H. Holland’s 1975 book
Adaptation in Natural and Artiﬁcial Systems: An Introductory Anal-
ysis with Applications to Biology, Control, and Artiﬁcial Intelligence
(see “Further Reading”). John Holland formalized the concepts underly-
ing the genetic algorithm and provided the mathematical foundations for
incrementally and formally improving their search techniques. Holland
was primarily interested in the nature of adaptive systems. The adap-
tive nature of his genetic models provided the foundation for a broad and
robust ﬁeld of computer science that allows a handful of simple constructs
to solve complex, highly nonlinear and often mathematically intractable
problems. As we will see in this chapter and in the remainder of Part
III, these evolutionary strategies allow model designers and data mining
engineers to optimize their models, generate new and robust predictive
models, and explore highly complex decision spaces.
343

344
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
9.1
The Vocabulary of Genetic Algorithms
Much of the literature in evolutionary strategies adopts its nomenclature
from biological models. Thus, we speak of chromosomes, alleles, muta-
tions, breeding, goodness of ﬁt, and survival of the ﬁttest. Before moving
on with a complete and detailed analysis of the algorithm and how it
works, we need to understand the principal nomenclatures and how they
are related to the components of the algorithm. As we pointed out in our
discussion of fuzzy logic, a preview of the vocabulary will make reading
and understanding the material in this chapter much easier. It is not always
possible, while maintaining an uncluttered and coherent discussion of the
genetic process, to ensure that every term is introduced before it is used.
Allele
The value at a particular locus in the genome is called the allele. In a
binary representation, this will be a one or a zero. In a real number
representation, the allele will be an integer or ﬂoating-point number.
Annealing
Annealing (often called simulated annealing) is a process for disrupting
the current state of a genetic algorithm to avoid premature convergence
to a solution. In a genetic algorithm, this is accomplished through muta-
tion, the random introduction of new individuals into a population, the
retention of a few poor-performing individuals, and changes in the size
and compactness of future populations.
Breeding
A new population of possible solutions to the current problem is primarily
(but not completely) created through a process that resembles biologi-
cal breeding. High-performance individuals (those with very good ﬁtness
values) are mated to produce offspring in a process somewhat analogous
to sexual reproduction; that is, their genetic material is distributed to one
or more offspring. Figure 9.1 illustrates how a crossover at a single point
on the chromosome produces a new offspring from two parents.
In this breeding example a left-hand part of one parent’s genome and
a right-hand part of another parent’s genome are exchanged to create a
new individual with the combined genetic material from both parents.

9.1
The Vocabulary of Genetic Algorithms
■
345
ABCDEF
Cross-over
UVWXYZ
Parent
chromosome 1
Parent
chromosome 2
Child
chromosome 1
Child
chromosome 2
ABWXYZ
UVCDEF
Figure 9.1
Breeding through a single-point crossover.
Chromosome (Individual)
A collection of genomes representing a potential solution to the problem
is called a chromosome. This is the genetic material of the genetic algo-
rithm. A chromosome may consist of multiple genomes, each expressing
a feature of the target system that must be considered a constraint or
an objective function. For example, a genetic algorithm that solves the
traveling salesman problem (TSP)1 would encode the order of cities in
its chromosome. Figure 9.2 shows a collection of cities and one possible
route between these cities.
There is a very large number of possible routes for even a small num-
ber of cities (in that the number of routes grows with the factorial of the
number of cities). For eight cities, for example, the combinatorial com-
plexity is 8! or over 40,000 possible routes. We can simplify the encoding
of the chromosome by assigning each city an index number. Table 9.1
shows the index values for each table.
Using this encoding, the TSP is encoded in a chromosome by speci-
fying a possible path. The path is the set of edges in the route graph. In
this problem, we are always starting in Seattle. This becomes the start
1 The objective of the traveling salesman problem (TSP) is to ﬁnd the shortest route in time,
capital, or distance between all cities without visiting any city twice. The general TSP problem
has applicability in a wide range of conﬁguration and design problems (such as the design and
manufacture of microprocessor chips).

346
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
New York
Baltimore
Atlanta
Dallas
Memphis
Chicago
Las Vegas
Seattle
San Francisco
Los Angeles
Figure 9.2
One possible solution to a TSP.
TABLE 9.1
The City Index Numbers
City
Index
Atlanta
1
Baltimore
2
Chicago
3
Dallas
4
Los Angeles
5
Las Vegas
6
Memphis
7
New York
8
San Francisco
9
Seattle
10
and end of the directed graph. Figure 9.5 shows the chromosome for the
route shown in Figure 9.3.
The chromosome deﬁnes a directed graph with the edges (10,9), (9,5),
(5,6), and so forth. If the TSP problem can start at any city, the starting
city can be encoded as part of the genome as a separate parameter. A
potential solution can be created for a path by generating a set of unique

9.1
The Vocabulary of Genetic Algorithms
■
347
TSP Chromosome 
9
5
6
4
7
1
2
8
3
10
Figure 9.3
A chromosome expressing a TSP solution.
random numbers in the ﬁrst nine positions within the range [1,9], in that
the last genome locus must return the path to the origin (in this case,
Seattle).
Constraints
Constraints deﬁne the feasibility of a schedule. Objectives deﬁne the opti-
mality of a schedule. Although objectives should be satisﬁed, constraints
must be satisﬁed. In a crew-scheduling system, for example, the schedule
might have three constraints: the precedence relationship between jobs,
the availability of the crew at the time it is needed, and a match between
the type of crew and the skills required for the job. These constraints must
be obeyed and deﬁne the properties of a feasible and workable solution.
(See “Feasible Solution” for additional details.)
Convergence
The process of breeding a genetic algorithm’s population over a series
of generations to arrive at the chromosome with the best ﬁtness value is
known as convergence. That is, the population converges on a solution.
Convergence is controlled by the ﬁtness function: ﬁtter chromosomes
survive to the next generation and breed new chromosomes that (hope-
fully) improve the average ﬁtness of the population. This continues until
the ﬁtness does not improve; thus, it converges on a ﬁnal value.
Crossover (Recombination)
The process of creating a new chromosome by combining or “mating”
two or more high-performance chromosomes is known as crossover.
In a crossover, the genetic material of one chromosome is swapped in

348
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
some manner with the genetic material of another chromosome to pro-
duce one or more offspring. There are several techniques for combining
the genetic material (genomes), such as one-point, two-point, and uni-
form crossover. These techniques are discussed in the detailed analysis of
the algorithm.
Feasible Solution
A genetic algorithm must start with and always generate feasible solu-
tions. Otherwise, the solution, even if its goodness of ﬁt is the best, is
useless. Feasible solutions must consider the nature of the objective func-
tion and constraints placed on the system. For example, consider the TSP.
If a business constraint on the route plan speciﬁes that the salesperson
must visit all west coast cities before any other cities, a schedule that
creates an initial route from Seattle to Chicago violates this constraint and
is not a feasible solution. In a crew schedule, a solution that schedules a
task when a critical and necessary piece of equipment is unavailable or
that assigns a task to a crew that does not have the necessary skill set or
cannot work in the task’s geographic area is not a feasible solution.
Fitness Function
The ﬁtness function is a measure associated with the collective objective
functions that indicates the ﬁtness of a particular chromosome (or the
phenotype) in terms of a desired solution. A genetic algorithm is a directed
search. This search is controlled by the ﬁtness function. For minimization
problems, the ﬁtness function usually approaches zero as the optimal
value. For maximization problems, the ﬁtness function usually approaches
some upper boundary threshold as its optimal value. For multiobjective
functions, the ﬁtness function often approaches some saddle point in the
overall solution space. In the TSP, we want to minimize the distance, and
thus we need a ﬁtness function that approaches zero. One possible ﬁtness
function, shown in Expression 9.1, is fairly straightforward (it is 1 minus
the inverse of the sum of the distances).
f = 1 −
1
N−1
i=1 d(ci, ci+1)
(9.1)
For N cities, this function sums the distance between each successive city
in the graph stored in the current chromosome. This fraction is subtracted
from 1. Smaller distances will yield relatively larger fractions, thus driving
the ﬁtness function toward zero. When the entire population of potential

9.1
The Vocabulary of Genetic Algorithms
■
349
paths has been evaluated, those with the smallest ﬁtness will be the best
solution found during that generation.
Generation
A genetic algorithm creates a new population of candidate solutions until
a termination condition is reached. Each new population is known as a
generation. A maximum number of generations is one of the termination
conditions for a genetic algorithm.
Genome
A particular feature in the chromosome is represented by a genome. In
many cases, a chromosome may consist of a single genome, but for mul-
tiobjective and multiconstraint problems a chromosome can consist of
several genomes. The nature of a genome depends on the underlying
data representation. For bit (or binary) representations, the genome is a
series of bits. For a real number representation, the genome is an integer
or ﬂoating-point number.
Genotype
The complete structure of a chromosome is often called the genotype. It
is simply a handy way of referring to all genomes. The actual instance of a
chromosome (the actual values of the genotype) is called the phenotype.
Goodness of Fit
The goodness of ﬁt is a measure of how close the ﬁtness function value
is to the optimum value. A ﬁtness function returns a goodness-of-ﬁt value
for each chromosome.
Locus
A locus in a chromosome is simply a position in the genome. In the TSP
chromosome (see Figure 9.3), there are 10 node positions in the genome.
Each of these values is a locus in the underlying chromosome.

350
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Mutation
One of the ways in which a genetic algorithm attempts to improve the
overall ﬁtness of a population as it moves toward a ﬁnal, optimal solution
is by randomly changing the value of an allele. This process is called
mutation. Mutation enriches the pool of phenotypes in the population,
combats local minimum and maximum regions (and as such is a form of
annealing), and ensures that new potential solutions, independent of the
current set of chromosomes, will emerge in the population at large.
Objective Function
An objective function deﬁnes the purpose of the genetic algorithm and
is the value that will be either minimized or maximized. Each genetic
algorithm must have one or more objective functions. It is the objective
function value that is measured by the ﬁtness function and evaluated for
its goodness of ﬁt.
Performance
A general way of looking at the ﬁtness of a chromosome is its per-
formance in the population. Chromosomes with high goodness-of-ﬁt
values are considered high-performance segments of the population.
Those chromosomes below some goodness-of-ﬁt threshold are considered
low-performance chromosomes.
Phenotype
The actual values of a genome (its position in the solution space) are called
the phenotype. Whereas the genotype expresses the overall properties of
the genetic algorithm by deﬁning the nature of the chromosome, the phe-
notype represents an individual expression of the genome (or genotype).
This is somewhat similar to the relationship between classes and objects
in an object-oriented programming language: a class represents the deﬁ-
nition of an object, whereas an object represents a concrete instantiation
of a class.
Population
A collection of chromosomes with their values is a population. A genetic
algorithm starts by creating a large population of potential solutions

9.1
The Vocabulary of Genetic Algorithms
■
351
represented as chromosomes. As the algorithm continues, new popu-
lations of old and new chromosomes are created and processed. In
some genetic algorithm implementations the total population size is ﬁxed,
whereas in others the population size can increase or decrease depending
on the nature of the problem space.
Schema
Many of the mathematical foundations of genetic algorithms are built on
the evaluation of emerging and transient bit patterns in the population.
A pattern of bits that repeats through the high-performance region of
the population provides a method of explaining how a genetic algorithm
converges on a solution. In general practice, however, an understanding
of schema patterns provides little, if any, beneﬁt in the management of a
genetic algorithm.
Selection
How individual chromosomes are chosen for crossover and mutation is
based on the process of selection. Selection is used to pick the high-
performance segment of the population for crossover breeding, to pick a
few chromosomes for mutation, and in some problems to pick a few low-
performance chromosomes for inclusion in the next generation (simply
to ensure a mix of genetic material).
Survival of the Fittest
In a fashion similar to natural evolution, individuals in a genetic algorithm
survive from one generation to the next based on their goodness-of-ﬁt val-
ues. The ﬁttest individuals are preserved and reproduce (see “Breeding”),
which is referred to as survival of the ﬁttest. In this way, the average
goodness of ﬁt of the entire population begins to converge on the best
possible set of solutions.
System
A genetic algorithm is connected to an underlying system. The current
phenotype values in the chromosome are the parameters used to run the
system and evaluate the system’s outcome in terms of goodness of ﬁt.
For example, a genetic algorithm that solves the TSP has chromosomes

352
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
TABLE 9.2
Inter-city Distances for TSP Route Analysis
To
From
1
2
3
4
5
6
7
8
9
10
Atlanta
1
0
600
900
1200
2900
1800
400
800
2800
2500
Baltimore
2
600
0
450
1150
2890
1940
510
200
2820
2470
Chicago
3
900
450
0
640
2100
1100
480
700
1950
2200
Dallas
4
1200
1150
640
0
1100
570
630
1020
1500
2050
Los Angeles
5
2900
2890
2100
1100
. . .
Las Vegas
6
1800
1940
1100
570
. . .
Memphis
7
400
510
480
630
. . .
New York
8
800
200
700
1020
. . .
San Francisco
9
2800
2820
1950
1500
. . .
Seattle
10
2500
2470
2200
2050
. . .
containing possible paths between all cities. The system it calls is the
graph analyzer that computes the total travel time for each chromosome.
The graph analyzer generally contains an N × N table of the distances
between each city. Table 9.2 shows part of this table of inter-city distances.
In this route table (routes [10][10]), any chromosome phenotype can
be decoded though the following distance function.
Function real d(integer fromCity, integer toCity){
return(routes[fromCity][toCity]);
}
(See the “Fitness Function” deﬁnition for the actual system ﬁtness func-
tion that uses this route inter-city distance function.) Thus, the genetic
algorithm sets up the population of candidate routes. Each chromosome
is then made part of the parameters of the route analysis system, which
computes and returns the ﬁtness.
Termination Conditions
A genetic algorithm must be told when to stop searching for a solution.
The criteria for stopping are called termination conditions. The criteria
include the maximum number of generations having been reached, the
amount of computer time having been exceeded, one or more individuals
having ﬁtness values that satisfy the objective function, or the best ﬁtness

9.2
Overview
■
353
function in successive generations having reached a plateau. A genetic
algorithm can use one or more of these terminating conditions.
9.2
Overview
One of the principal uses of a genetic algorithm, and the use that will be
the focus of most chapters in this part of the book, is optimization: the
process of ﬁnding the best solution to a problem under a set of objectives
and constraints. Finding the best solution usually, but not necessarily,
means ﬁnding the maximum or minimum value for the variable repre-
senting the objective function. Because genetic algorithms can search
very large, highly nonlinear, and often very noisy landscapes, they are
ideal as solution engines for optimization problems involving a large num-
ber of constraints and many different (sometimes conﬂicting) objective
functions. Some everyday examples include the following.
■
Project, crew, and class scheduling; delivery and distribution rout-
ing (the TSP); container packing (the classical “knapsack” problem);
timetabling; assembly line balancing; conﬁguration management; and
retail shelf-location planning
■
Regression and trend curve ﬁtting, automatic cluster detection, and
route identiﬁcation
■
Process control, engineering structural design, integrated circuit
design, urban planning, and highway capacity analysis
■
Evolution of neural networks, optimization of fuzzy models, gen-
eral function optimization, exploration of game theory scenarios,
protein folding and related modeling of molecular systems, and
high-throughput screening and drug discovery and design
■
Capital budgeting, portfolio suitability, balancing, and mix analysis;
sales, inventory, and consumption forecasting; new product pricing;
and economic models
■
Network topology conﬁguration, server capacity planning, fault
detection, application scheduling, web design, and database design
Genetic algorithms are also used in a wide spectrum of evolutionary
programming systems. Evolutionary programs breed and mutate mathe-
matical, logical, and fuzzy expressions to produce an optimal model. Not
only are evolutionary programs another form of knowledge discovery

354
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
(data mining) but they form an important class of solutions for rule-based
and mathematics-based models.
Generate and Test
A genetic algorithm is an enhancement to one of the earliest approaches
in machine problem solving. The approach is known as generate and test.
Using this strategy, a new solution to the current problem state (which,
of course, may be a partition of the ﬁnal problem state) is generated and
tested. If it satisﬁes the criteria for a solution, it is accepted;2 otherwise,
a new potential solution is generated and tested. Because the generate-
and-test method is always guided by an allowable outcome, it is called a
directed search. Figure 9.4 illustrates this process.
A generator, using a model of constraints, creates a possible solution
case to the current problem. The testing program evaluates the solution
according to the current problem state. If the solution solves the current
No
No
Yes
Yes
Case
exists?
Solution?
Test
Generate
Constraint
model
Accept
Failure
Figure 9.4
The generate-and-test process.
2 In some cases a set of candidate solutions is collected from the generate-and-test process. These
candidates are then ranked by additional evaluation criteria and the best of the potential solutions
is selected.

9.2
Overview
■
355
problem state, it is accepted; otherwise, another potential solution is
produced (or, if no other solutions are available, the process terminates
with a failure). The core of the generate-and-test method is the ability of
the generator to produce a set of well-formed nonredundant candidate
solutions. The test process incorporates two capabilities: the ability to
run the candidate solution against the target system and the ability to
compare the results to a valid solution. It is the comparison between a
potential solution and an acceptable solution that drives the generate-
and-test methodology. Where the criteria for a successful solution can be
speciﬁed, the generate-and-test approach has proven to be a very pow-
erful tool and has been used in variety of difﬁcult and computationally
intensive problems in such areas as conﬁguration, design, and graph
generation.
The Genetic Algorithm
A genetic algorithm (GA) is a form of the generate-and-test paradigm.
Like the generate-and-test method, it is also a directed search and works
by generating a large number of possible solutions, testing each of these
against an allowable outcome. The genetic algorithm, as the name implies,
breeds a solution to complex optimization and search problems using
techniques that simulate the processes of natural evolution. The genetic
algorithm starts with a large population of potential (or feasible) solutions
and through the application of recombination (also called crossover) and
mutation evolves a solution that is better than any previous solution over
the lifetime of the genetic analysis. Figure 9.5 shows the organization of
a genetic algorithm.
The genetic algorithm works by creating an initial population of N
possible solutions in the form of candidate chromosomes (or simply, indi-
viduals). Each of these individuals is a representation of the target system.
The encoding of the target system’s parameters in the individual is used
to run the system and measure the outcome against an objective function.
The objective function measures the goodness of ﬁt of the individual. The
better this goodness of ﬁt the closer the individual is to representing a
solution. After all individuals in the population have been evaluated for
their goodness of ﬁt, we decide whether to continue or to stop. If the ter-
minating condition is not met, a new population is created by saving the
top K best individuals, removing the bottom M poorly performing indi-
viduals, and replacing these with new individuals created by merging the
parameters of the top best-performing individuals. New chromosomes are
also created by randomly mutating the parameters in a few of the existing
individuals.

356
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
No
Yes
Terminate?
Generate
initial population
P (N)
Evaluate
each
individual
Breed
new population
Return best
Figure 9.5
Organization of a genetic algorithm.
How a Genetic Algorithm Works
As the genetic algorithm creates and tests each population of chromo-
somes, it is searching for better and better solutions to the underlying
problem. This search takes the form of a walkover of the underlying sur-
face of the solution space. For example, consider Expression 9.2 and a
genetic algorithm that seeks to maximize the variable z as a solution to
the function in continuous variables x and y.
z = f (x, y)
(9.2)
If we plot z over the possible values of x and y, we develop a terrain
map of the solution surface for this function. Figure 9.6 shows a portion
of the terrain map that we will use in the discussion of how the genetic
algorithm works.
The genetic algorithm searches through this terrain to ﬁnd the values
of x and y that maximize z. This is essentially a process known as hill
climbing. In hill climbing, the search mechanism explores the terrain
around its current position looking for values in the independent vari-
ables that will increase the value of the target (or dependent) variable.
It then moves in this direction (hence the analogy with climbing a hill).
A major problem with hill climbing is its tendency to become stuck in a
local maximum (or minimum, depending on the objective function). For
example, Figure 9.7 shows a hill-climbing mechanism selecting a random
position on the terrain.

9.2
Overview
■
357
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.6
The solution space terrain for z = f (x, y).
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.7
A random search point on the terrain.

358
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
z
10
20
50
40
30
20
10
0
–10
–20
–30
–40
–50
180
200220
30
40
50
x
60
70
80
90
100
120
140
y
160
Figure 9.8
Hill climbing moving up a hill.
Examining the surrounding terrain,
the hill-climbing mechanism
moves to the left and up. Through a series of proximity tests, it works its
way, as illustrated in Figure 9.8, to the top of the hill.
By generating a series of values for x and y, the search mechanism
can work its way up the slope, always moving in the direction that gives
a larger value of z. Eventually, as we can see in Figure 9.9, the search
mechanism arrives at the top of the hill. There is no way to go except
back down.
This hill-climbing example illustrates not only the way the search
mechanism works but a signiﬁcant weakness in the search methodol-
ogy. Although we have arrived at the top of a hill, it is not the hill that
maximizes the value of z (this hill lies off to the right). Once the hill-
climbing mechanism starts up a slope, it has no way of going back down
to ﬁnd another, perhaps better, hill. Thus, hill climbing is always subject
to ﬁnding local maximums (or minimums). We can compensate for this
tendency to ﬁnd local maximum or minimum regions through a process
called simulated annealing. This approach saves the current maximum
and then in effect “shakes” the surface to start the search point rolling
around the terrain. It then starts the hill climbing from this new point.
However, for any realistically large, complex, and often noncontiguous
surface this approach is very inefﬁcient. In real-world systems, the under-
lying terrain is often very hilly, with gaps, steep ravines, and long gullies.

9.2
Overview
■
359
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.9
Arriving at the top of the hill.
Not only does a hill-climbing mechanism have little chance of ﬁnding a
global maximum in such a surface but there is no way for the search
mechanism to ensure that any maximum is in fact the global maximum
(or minimum).
A genetic algorithm signiﬁcantly improves the hill-climbing methodol-
ogy of generate-and-test by selecting many possible maximums through-
out the surface and then using the ﬁtness function to breed better and
better solutions as each of the randomly placed points moves up (or down)
adjacent slopes. Figure 9.10 illustrates a population of potential solutions
that would form the initial set of chromosomes for a genetic algorithm.
In Figure 9.10 we can see that the candidate solutions are scattered
widely over the underlying terrain. Each point with an x, y coordinate
value yields a value for z. Our ﬁtness function is simply the value of z. The
higher the value the better the goodness of ﬁt. In the genetic algorithm,
the initial population of solutions and each subsequent population are
subjected to a set of transformations, as follows.
■
A collection of the very best chromosomes (solutions) is retained.
■
The bottommost poor solutions are removed.
■
The members of a random set of the best solutions are “mated” to pro-
duce a set of children that shares the genetic material of their parents.

360
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.10
An initial population of possible solutions.
TABLE 9.3
Crossover Example
Parent
Child
Genome N
(50,120)
(84,120)
Genome N+1
(84,173)
(50,173)
This is the process called crossover. For example, Table 9.3 shows a
crossover for two genomes. In this case, the ﬁrst locus (the x value) is
swapped to generate two new children. The purpose of crossover is
to increase variety and robustness in the population while preserving
the genetic values of the best solutions.
■
Another random (but very sparse) set of the population is subjected
to mutation. This involves randomly changing the value of a chromo-
some locus to a random but permissible value. Mutation is a form of
annealing, which introduces new genetic material into the population.
■
Every so often a completely new chromosome is created and inserted
into the population. This is also a form of annealing and, like mutation,
introduces completely new genetic material into the population.

9.2
Overview
■
361
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.11
The solution population after several generations.
This process of breeding new solutions (creating a new population)
by selecting the chromosomes with the largest ﬁtness value and apply-
ing crossover and mutation continues for generation after generation.
Figure 9.11 illustrates the population after a few generations.
Although there are still a few poor-performance chromosomes (due to
the unpredictable random effects of individual crossovers and mutations),
the average performance of all chromosomes has improved. Most of the
solution points are beginning to move toward the global maximum. We
also note that the points that have climbed the local maximum slopes are
also being removed because their ﬁtness function values are consistently
less than the points that are moving toward the global maximum.
As we continue this process over a large number of generations, the
ranking of chromosomes by their goodness of ﬁt guides the search toward
the global maximum. Figure 9.12 shows the result: the ﬁtness function
eventually ﬁnds the maximum (optimal) value of z.
In summary, genetic algorithms are a form of directed search. They
work through the repetitive application of genetic operators and a ﬁtness
function to a population of potential solutions. The genetic operators
breed new solutions. The ﬁtness function selects the best of these new
solutions. The genetic algorithm ranks the best solutions, applies the
genetic operators to breed new solutions, and evaluates these against the
ﬁtness function. This process usually continues until no more improve-
ment in a potential solution is found (or until another termination
condition is reached).

362
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.12
The maximum (optimal) value of z.
Strengths and Limitations
Although genetic algorithms are powerful tools for search and optimiza-
tion they are not without their problems and limitations. Although many
of the strengths and weaknesses of genetic algorithms will be explored
later in Part III, in this section we review a few of their principal strong
points and weak points.
■
The ability to solve nonlinear, noisy, and discontinuous prob-
lems. Although a genetic algorithm has many of the properties of
a hill-climbing algorithm, it is actually a more sophisticated form
of a stochastic search engine. The general capabilities of a stochas-
tic search are more robust and broader than simple hill climbing.
In particular, genetic algorithms are capable of solving highly non-
linear problems that involve discontinuous surfaces, noise, and
internal dependencies (such as lead and lag relationships). Nonlin-
earity is a common property of most real-world problems, occurring
in manufacturing, inventory management, portfolio mix optimization,
construction, retailing, and a wide spectrum of other industries.
For example, in the normal course of running a production line the
cost to assemble, paint, and ship 100 cars for the ABC Motor Company
is $N. If N = 20,000, the cost per car is $200. What is the cost to
assemble, paint, and ship a single car? What is the individual cost

9.2
Overview
■
363
Cost to manufacture and
ship 1 car
Production line steady state volume (cars/day)
1
1200
0
10,000
Figure 9.13
A nonlinear production cost curve.
to assemble, paint, and ship 1,000,000 cars? Both of these questions
involve a nonlinear system. The cost of setup, labor, electricity, and
other factors means that producing a single car (or any small number of
cars) is far more expensive than a large number of cars in a production
system. Figure 9.13 illustrates this nonlinear relationship between cost
and production line volume.
The plateau regions in the function are generally related to the cost
of energy and materials that often have quantity-based cost thresholds.
On the other hand, the wear and tear on equipment alone means that
the cost per car on a run of one million cars will steadily increase. The
growth and decay curves in these examples are typical examples of
nonlinear functions.
■
The ability to solve complex optimization problems. The genetic
algorithm’s ability to rapidly and thoroughly search a large, complex,
and high-dimensional space allows it to solve multiobjective and mul-
ticonstraint optimization problems. In a multiple objective function
schedule, the optimal solution often represents a saddle (or compro-
mise) point in the solution space. Optimization in the presence of
multiobjective functions usually means ﬁnding a way to rank the util-
ity or importance of each objective and then ﬁnding a way to judge the
optimality of a feasible solution based on the goodness of ﬁt or rank
of their optimality measures. As a rather simple example using a crew

364
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
scheduler, if each objective function (f) returns a value between 0 and
100, indicating how close the schedule is to the optimal (0 = best,
100 = worst), we can ﬁnd an optimality ranking for a multiobjective
function schedule by evaluating Expression 9.3.
franked =
N
i=1 fi × wi
N
i=1 wi
(9.3)
where, N is the number of objective functions and wi is the weight
(or utility) value of that objective function. By selecting schedules
with the smallest weighted average objective function, the feasible
schedules with the closest optimality ﬁt will continually percolate to
the top. This form of evaluating a collection of objective functions
makes it easy to combine both minimizing and maximizing objective
functions in the same analysis (maximizing functions simply return
the inverse of the ﬁtness value).
■
A complete dependence on the ﬁtness function. As a directed search
technique, the ﬁtness function evaluates each solution and ranks it
according to a goodness of ﬁt. If a ﬁtness function cannot be deﬁned,
a genetic algorithm cannot be used to solve the problem. If a ﬁt-
ness function does not correctly deﬁne a separable universe of good
and bad solutions (or if the ﬁtness function is coded incorrectly), the
genetic algorithm will behave according to the clarity and focus of the
faulty ﬁtness function and will fail to ﬁnd the correct solution. And
because a genetic algorithm is highly sensitive to the underlying gradi-
ent of the solution space, a ﬁtness function must provide a way of guid-
ing the search. The algorithm must be able to tell when it is moving in
the right direction; that is, when in fact it is getting close to a solution.
Genetic algorithms are also sensitive to intelligent proximity and
search capabilities built into the search methodology. This is both a
strength and a weakness. The ability to encode intelligence into the
ﬁtness function so that degrees of ﬁtness can be evaluated allows the
genetic algorithm to rank chromosomes that are “close to” the main
goodness-of-ﬁt criteria. This process can help guide the search through
a rough or chaotic solution space. At the same time, a lack of focus
in the ﬁtness function can spread the search over a larger segment of
the population, slowing and often obscuring the optimization process.
Finding a balance between the ﬂexibility and brittleness of the ﬁtness
function is often a difﬁcult task.
■
A sensitivity to genetic algorithm parameters. Genetic algorithms are
intrinsically sensitive to the way in which new populations are gener-
ated; that is, they are sensitive to the way in which high-performance

9.3
The Architecture of a Genetic Algorithm
■
365
properties are inherited by future populations and the way in which
new potential solutions emerge in future populations. Essentially, this
means that the stability, coherence, and convergence of genetic algo-
rithms depend on the rate of mutation and the crossover frequency.
The higher the rate of the crossover and mutation properties the more
variation appears in the population. This may initially provide a rich
pool of possible solutions. However, as the frequency rates increase
the continuity of high-performance individuals is lost among the result-
ing randomness. At some point, the algorithm becomes less and less
stable and the genome itself becomes more and more random.
There are two signiﬁcant problems associated with a lack of robust-
ness in a genetic algorithm: premature convergence and delayed
convergence. When the population size is too small or the genetic
diversity is too small, a genetic algorithm can converge too quickly on
a local optimum. On the other hand, if the population size is too large
or the genetic diversity is too large an optimal solution may not emerge
(due to the continual emergence of randomness in the genomes) or
the convergence to a solution may take a very long time.
■
A sensitivity to genome encoding. Genetic algorithms are also respon-
sive, but perhaps to a lesser degree, to the underlying coding scheme
used to represent the genome. Traditional genome coding has been
done through a bit string so that mutations can work in a way similar to
random genetic miscoding in biological systems. Production genetic
algorithms — those that have been deployed into regular use and are
solving real-world problems, especially those used in complex, multi-
objective business applications — commonly use real numbers as
genomes. The use of numbers rather than bit strings provides not only
higher evaluation performance but the ability to more easily control
the underlying distribution (statistical) properties of the genome.
To summarize, genetic algorithms belong to a class of directed search
methods that are be used for both solving optimization problems and
modeling the core of evolutionary systems. They use a heuristic rather
than analytical approach, and thus their solutions are not always exact
and their ability to ﬁnd a solution is often dependent on a proper and
sometimes fragile speciﬁcation of the problem representation and the
parameters that drive the genetic algorithm.
9.3
The Architecture of a Genetic Algorithm
In the previous section we discussed the underlying concepts of the
genetic algorithm and how the stochastic search and ﬁtness functions

366
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
work together to ﬁnd a value for an objective function. We also reviewed
some of the principal strengths and weaknesses of genetic algorithms.
Now we turn to the actual mechanics of the genetic algorithm; how a
genetic algorithm is designed, structured, and organized for a particular
problem; the meaning and application of the algorithm parameters; and
the speciﬁcation of constraints, objectives, and ﬁtness functions.
To illustrate the mechanics of a genetic algorithm and how the various
algorithm parameters affect the search methods, we will use a very small,
ﬁve-city TSP. In this problem, shown in Figure 9.14, we want to ﬁnd
the shortest complete path (or tour) between cities Able, Baker, Charlie,
Delta, and Echo. To simplify the example, we are not attempting to ﬁnd
a circuit (that is, we need not return to the starting city).
To compute the path between the cities, we need to store the inter-
city distances in a way that allows for a rapid evaluation of the path.
The mileage between these cities, shown in Table 9.4, is maintained in a
distance matrix (D[ ][ ]),
Echo
Able
Charlie
Delta
Baker
12
10
5
30
8
13
28
15
22
9
Figure 9.14
The ﬁve-city TSP.

9.3
The Architecture of a Genetic Algorithm
■
367
TABLE 9.4
The Inter-city Distance Matrix
Able
Baker
Charlie
Delta
Echo
Able
0
10
15
5
9
Baker
10
0
8
12
22
Charlie
15
8
0
30
13
Delta
5
12
30
0
28
Echo
9
22
13
28
0
TABLE 9.5
The City Coordinate Map
Grid Coordinates
x
y
Able
3
3
Baker
5
1
Charlie
7
4
Delta
8
2
Echo
4
6
An alternative way of storing city-to-city distances, of course, is
through a grid coordinate system. In this approach, each city is associated
with an x-y coordinate in an N × M map. Table 9.5 shows a coordinate
table for the ﬁve cities.
With a coordinate table, the distance between any two cities (Cn,Cm)
is the Euclidean distance from the coordinates of Cn and Cm. Expression
9.4 is the distance metric.
d(Cn, Cm) =
 
(xm −xn)2 + (ym −yn)2
(9.4)
The choice of representation depends to a large degree on how the dis-
tance metric is actually used. If the distance must be associated with
road, rail, water, and other physical transportation systems, the inter-
city distance map is the preferred representation method because it can
capture the actual mileage between two cities based on actual driving or
commuter rail distances (for example). Also, when the distance between
cities are not symmetrical — that is, traveling from city C1 to city C2 is not
the same as traveling from city C2 to city C1 — the inter-city matrix should

368
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
be used. The Euclidean distance, on the other hand, is the straight-line
mileage between two cities. In some problems, where the approximate
distance is sufﬁcient, the grid coordinate method can be used.
With ﬁve cities, there are 120 possible paths (because there are 5!
possible combinations of cities). Our genetic algorithm will explore this
space. Its objective is to minimize the transit length among the cities.
With this in mind, we can turn to the internal workings of the genetic
algorithm. Figure 9.15 shows the schematic ﬂow of control and analysis
in the genetic algorithm.
Yes
No
Terminate?
Generate
initial population
P (N)
Design
genome and
fitness function
For each P (n)
Evaluate
chromosome
Return best
Create new
population P (N)
Repeat
analysis
5
4
3
2
1
Figure 9.15
The control and analysis ﬂow in a genetic algorithm.

9.3
The Architecture of a Genetic Algorithm
■
369
➊Design a genome representation and ﬁtness function.
The efﬁciency and processing power of a genetic algorithm depend on the
way the problem is expressed in a chromosome and the ﬁtness function
that evaluates the chromosome. This is the ﬁrst step in using a genetic
algorithm. The design of a chromosome or genome set to properly repre-
sent a problem is one of the crucial architectural decisions in the use of
a genetic algorithm.
Genome Structural Design
A population consists of a collection of chromosomes. Each chromo-
some consists of one or more genomes. Every chromosome represents
a possible solution. The way in which the chromosome is designed can,
therefore, have a signiﬁcant effect on processing speed, convergence,
and the overall ease of crossover and mutation. It is important to under-
stand that the values of a chromosome are the parameters of a solution
and must always be associated with an evaluation system that can use the
parameters to produce a state or outcome of the system. The degree to
which this outcome is a better or worse solution, given the chromosome’s
parameter values, is measured by the ﬁtness function (which is discussed
in the next section).
In the TSP, an obvious chromosome representation is a genome with
ﬁve loci, representing the order in which the cities are visited. The num-
ber in the locus is the city at the row location in the distance matrix
(D[ ][ ]). Figure 9.16 shows a possible chromosome (a phenotype) for a
route through ﬁve cities.
This chromosome describes a tour: start at Able, travel to Charlie,
travel to Echo, travel to Delta, and then travel to Baker. A new chromo-
some can easily be developed using this presentation by generating ﬁve
unique random numbers between 1 and 5. For a simple route through
ﬁve cities, a chromosome that is a permutation of the cities is most likely
the simplest and the best.
Chromosome design is, however, highly dependent on the problem.
Consider a circuit through the same ﬁve cities. A circuit has the constraint
Locus (Allele) Positions
1
2
3
4
5
1
3
5
4
2
Figure 9.16
A route chromosome in the TSP.

370
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Locus (Allele) Position
1
2
3
4
5
6
1
3
5
4
2
1
Figure 9.17
A circuit chromosome in the TSP.
that we must return to the starting city. We can either modify the ﬁtness
function to consider the implicit loop or design a slightly different chro-
mosome to make the circuit explicit and speed up ﬁtness computation.
An extra locus is added to the end of the chromosome. This site contains
the city index of the ﬁrst locus. Figure 9.17 shows a possible chromosome
(a phenotype) for a circuit through the ﬁve cities.
In the circuit representation, however, the genetic algorithm is not
free to perform crossover and mutate operations over the complete chro-
mosome. Locus 5 completes the circuit and must always have the same
value as the ﬁrst locus. Thus, ease of representation increases complexity
in the control strategies of the genetic algorithm.
Chromosomes can consist of more than single values. A chromosome
locus can be a vector, a matrix, or a tree structure. The representation
depends on the problem. As a simple example, suppose we change the
objective of the ﬁve-city TSP to ﬁnd the shortest route through the city con-
sidering trafﬁc patterns during the day. Heavy trafﬁc in city Ci effectively
extends the distance between cities (Ci, Cj). We can represent this by a
length multiplier associated with the from (or starting) city. Table 9.6
is a table of the effective lengths between cities — with the travel time
between each city pair extended by the trafﬁc multiplier.
For example, for the route segment Able to Baker (or to any other city)
at 1615 (4:15 p.m.), the multiplier is 1.4, indicating, perhaps, moderately
TABLE 9.6
The City Trafﬁc Multipliers
Time of Day
From
0000
0531
0731
0931
1601
1901
To
0530
0730
0930
1600
1900
2400
Able
1.0
1.2
1.4
1.1
1.4
1.2
Baker
1.0
1.2
1.3
1.3
1.2
1.1
Charlie
1.0
1.5
1.7
1.4
1.3
1.2
Delta
1.0
1.3
1.5
1.4
1.1
1.0
Echo
1.3
1.4
1.2
1.0
1.0
1.0

9.3
The Architecture of a Genetic Algorithm
■
371
Locus Position
1 
2 
3 
4 
5 
City
Time of Day
Figure 9.18
The city route and time-of-day genome.
heavy rush-hour trafﬁc. The distance from Able to Baker is 10 miles.
The multiplier converts this to an effective distance of 14 miles (10 times
1.4). The chromosome representation now must consider two values:
the city and the time of day of arrival at the city. Figure 9.18 shows the
organization of the city and time-of-day genome.
Each genome locus in this representation consists of a 1 × 2 vector
containing the city and the time of day. An initial population is generated
by selecting a random permutation of cities and, for each city, a ran-
dom arrival time.3 The search process proceeds by breeding routes that
minimize path length subject to the best time of day of city arrival times.
More complex route scheduling problems can be solved in this manner.
For example, city Ci may have N possible routes though the city. Balti-
more, Maryland, for instance, has I-695 (the beltway), the harbor tunnel,
the I-95 tunnel, and the Francis Scott Key bridge, each connecting cities
south of Baltimore (such as Washington D.C.) to cities north of Baltimore
(such as Wilmington and Philadelphia). Each route has its own length, or
a trafﬁc multiplier. A TSP algorithm could consider the order of the cities,
the route through the city, and the time of arrival. Because every city has
a different number of possible routes, the routing locus would be drawn
from a different population of random highways associated with the city
in the city locus of the genome.
The TSP illustrates one of the structural difﬁculties in designing
a chromosome representation that is easy to use and amendable to
genetic operators such as crossover and mutation. As we will see in the
discussion of breeding techniques (see “Conventional Crossover (Breed-
ing) Techniques”), a genome with a simple linear sequence of cities
almost always produces an incorrect genome when subjected to con-
ventional “cut-and-splice” crossover operations, as well as an incorrect
3 Nothing in the genetic search process requires a uniform distribution of random times. For
business trip schedules or crew scheduling, we might limit the random time to business hours,
say 0630 to 1700 (or 1430 hours to allow end-of-day tear down and travel time). Naturally, as
we will see in the discussion of ﬁtness functions, we could also assign very large multipliers to
non-work times to force the search mechanism to only schedule trips during a speciﬁc range of
work hours.

372
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Locus (Allele) Positions
1
2
3
4
5
0001
0011
0101
0100
0010
Figure 9.19
The binary route chromosome in the TSP.
genome when subjected to conventional locus (site) mutation. Follow-
ing the conventional crossover and mutation operations, we will take
up more advanced issues in designing genomes for permutation and
precedence-based (time-sequenced) problems.4
Genome Representations
In addition to the most effective and efﬁcient way to encode parame-
ters into chromosomes, the underlying form of the representation is also
important. Historically, chromosomes have been encoded as bit strings.
Figure 9.19 illustrates the bit (or binary) organization of the ﬁve-city TSP
search.
Unlike the use of actual numbers, which use arrays or matrices
for the genome, a binary representation uses a continuous string of 1
and 0 values. The chromosomes in Table 9.5 would be represented
as 0001001100100100. Bit strings provide several general beneﬁts in
genetic search.
■
As a string of 1s and 0s they are a compact representation.
■
Crossover operations are simpliﬁed. The mating process simply slices
the bit string at some random point and appends the string fragments
without regard to the actual values.
■
Mutation is a simple matter of ﬂipping a random bit in the string.
■
Provide a pattern analysis space for the evaluation of schemata
(patterns of high-performance genomes that move through the popu-
lation). Whether or not schemata analysis contributes any signiﬁcant
control improvement to genetic algorithms is a matter of some debate.
Most business genetic algorithms in scheduling, production manage-
ment, logistics, data exploration, pricing, and similar areas evaluate
the ﬁtness function and the average ﬁtness of the population without
regard to the propagation of schemata patterns.
4 Separating the discussion of genome design into two sections makes it much easier for readers
unfamiliar with either genetic algorithm breeding operators or with scheduling, routing, cover-
ing, and TSPs to understand the fundamentals and then appreciate the difﬁculties in genomes
that involve locus dependencies.

9.3
The Architecture of a Genetic Algorithm
■
373
Binary representations, however, all have some signiﬁcant problems.
■
They must be converted to a real number for use as a parameter to the
underlying system. This involves traversing the bit string from right to
left, and for each non-zero position, computing the value (2n, where
n is the bit position) and adding it to the total of the under-generation
value.
■
They make feasibility and unique chromosome constraint checking
very difﬁcult (the requirements for feasibility and uniqueness are
discussed in the section “Generate Initial Population”).
■
They make generating random values in a speciﬁc range a more tedious
process.
■
They are often a source of deep representation problems in genetic
algorithms. Because we are (as a general rule) unaccustomed to work-
ing in base 2 arithmetic, debugging and tracing the operations of a
genetic algorithm are often difﬁcult, and are ﬂawed by mistakes in
either binary encoding or interpreting the output of the algorithm.
■
They are (generally) unnecessary.
Nearly all problems can be
expressed with integer or ﬂoating-point numbers as the parameters.
Using a variety of random number generators (uniform, Gaussian,
binomial, and so forth), these parameters can be used in crossover
and mutation operations in ways that are easy to understand, easy to
implement, easy to constrain, and less prone to errors of encoding or
interpretation.
All genetic and evolutionary programming problems discussed in this
part of the book use real numbers as loci in their chromosomes. This
has proven, in the problems designed and deployed by the author, to
be an effective and robust means of genome representation. Although
there are some debates in the literature about the performance of real
numbers versus bit strings, in actual applications (many involving very
large problem spaces and complex genomes) the difference in genome
representation has been far outweighed by the efﬁciencies associated with
ranking (sorting), selecting, and the evaluation of the ﬁtness function in
the target system.
Fitness Functions
In the TSP, we have a single objective function and a small set of con-
straints. Because we want to ﬁnd the shortest route through the cities,

374
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
the ﬁtness function (shown in Expression 9.5) is the sum of the paths
between the cities in the chromosome.
f =
N−1

i=1
d(ci, ci+1)
(9.5)
Here,
f
is the ﬁtness of the chromosome.
N
is the number of cities in the chromosome (in this case, ﬁve
cities).
ci
is the i-th city in the chromosome. This is the i-th locus in
the genome and indexes the corresponding row in the
distance matrix.
d( )
is a function that returns the distance between two cities using
their index values in the distance matrix.
Genetic algorithms can be used to solve multiobjective and multicon-
straint problems. As a general practice, objective functions are encoded
as a set of genomes, whereas hard constraints are part of the underlying
system evaluation and affect the ﬁtness function in terms of the feasibility
of a solution. Soft constraints, on the other hand, are often encoded as
penalty functions in the ﬁtness evaluation.
For example, suppose the TSP had two objectives: ﬁnd the shortest
and least expensive route through the cites. The cost to travel from city
Ci to city Cj can be encoded, as illustrated in Table 9.7, by adding a table
of highway tolls to the problem.
Note that unlike distances the toll costs are sometimes different
depending on whether the path is Ci, Cj or Cj, Ci, in that many toll roads
charge for trafﬁc moving in one direction but not in another. With the
city distance and the toll costs matrices in hand, we can formulate a
ﬁtness function that looks for the minimum path length and the mini-
mum cost. One simple way to design a ﬁtness function, as illustrated
TABLE 9.7
The Inter-city Toll Matrix
Able
Baker
Charlie
Delta
Echo
Able
0
2
0
3
0
Baker
2
0
4
0
2
Charlie
2
4
0
0
2
Delta
3
0
0
0
0
Echo
0
2
2
0
0

9.3
The Architecture of a Genetic Algorithm
■
375
in Expression 9.6, is to take the sum of the path length and the toll
costs.
f =
N−1

i=1
d(ci, ci+1) + t(ci, ci+1)
(9.6)
Here,
f
is the ﬁtness of the chromosome.
N
is the number of cities in the chromosome (in this case, ﬁve
cities).
ci
is the i-th city in the chromosome. This is the i-th locus in the
genome and indexes the corresponding row in the distance
matrix.
d( )
is a function that returns the distance between two cities using
their index values in the distance matrix.
t( )
is a function that returns the toll costs between two cities using
their index values in the toll matrix.
Thus, for equal path lengths paths with no tolls (or very small tolls) will
have a better minimization ﬁtness value than paths with larger tolls. We
can also design a ﬁtness function to seek for the shortest path only on toll
roads. In this case, we want to penalize any solution that does not use
a toll road. Expression 9.7 shows one possible way to encode a penalty
into the ﬁtness function.
f =
N−1

i=1
d(ci, ci+1) +
1
t(ci, ci+1) + 1
(9.7)
In this case, when t( ) = 0 or near zero, the fraction is close to 1 and will
increase the value of the ﬁtness function. For all values of t( ) >> 0, the
fraction becomes very small and contributes to the minimization of the
solution. The ﬁtness function minimizes route distance and maximizes toll
costs. Thus, this ﬁtness function will seek the shortest and most expensive
route through the cities.
The idea of multiobjective and multiconstraint genetic searches can
encompass a large number of possible objective functions. The ﬁtness
function determines the optimum balance between individual ﬁtness and
a composite solution. For problems that involve many objective functions,
it is not generally possible to ﬁnd a solution that is optimal for each objec-
tive function. In the previous example, there are two objective functions.
Minimize the total route distance between N cities.
Minimize (or maximize) the toll costs.

376
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
TABLE 9.8
The Inter-city Speed Limit Matrix
Able
Baker
Charlie
Delta
Echo
Able
0
55
65
70
65
Baker
55
0
70
60
65
Charlie
65
70
0
70
70
Delta
70
60
70
0
65
Echo
65
65
70
65
0
We cannot simultaneously ﬁnd the shortest route and the route with the
least costs unless they just happen to coincide. In the real world, the solu-
tion with the minimum spanning route and the route that minimizes the
toll costs will lie in a space somewhere between the two objectives. As
the number of objective functions and penalty constraints increases, these
saddle points become more and more difﬁcult to deﬁne. For example, con-
sider a slightly more complex genetic search system that incorporates, as
shown in Table 9.8, a table of speed limits between cities.
The genetic algorithm now has three objective functions.
Minimize the total route distance between N cities.
Minimize the toll costs.
Minimize the time to travel the route.
Finding the optimal saddle point in the solution space that simultane-
ously satisﬁes all three constraints involves designing a ﬁtness function
that penalizes long routes, penalizes routes that involve toll roads, and
penalizes route segments that have slow speed limits. Expression 9.8
shows one possible ﬁtness function for this problem.
f =
N−1

i=1
d(ci, ci+1) + t(ci, ci+1) + (70 −s(ci, ci+1))
(9.8)
Here,
f
is the ﬁtness of the chromosome.
N
is the number of cities in the chromosome (in this case, ﬁve
cities).
ci
is the i-th city in the chromosome. This is the i-th locus in the
genome and indexes the corresponding row in the distance
matrix.
d( )
is a function that returns the distance between two cities using
their index values in the distance matrix.

9.3
The Architecture of a Genetic Algorithm
■
377
t( )
is a function that returns the toll costs between two cities using
their index values in the toll matrix.
s( )
is a function that returns the speed limit between two cities
using their index values in the speed limit matrix.
Knowing that the speed limit on interstate highways (which connect most
of the cities) is between 65 and 70 mph, we subtract the inter-city speed
limit from 70 (the maximum speed) to give a penalty weight to this part
of the ﬁtness function. The closer the city-to-city speed limit is to 70 mph
the smaller the difference, and hence the smaller the contribution to the
size of the ﬁtness function (and thus, it contributes to minimizing the
overall function). Naturally, other possible encoding forms that require
no knowledge of the maximum speed limit exist, such as the inverse
relationship we used to maximize the use of tolls (in that we are, in
effect, maximizing the speed over the route). See Expression 9.5 for this
use of the inverse (or fractional) weighting.
Multi-genome Fitness Functions
In the previous discussion, the ﬁtness function was associated with a sin-
gle route through the cities. Each chromosome represented a possible
route and had its own ﬁtness function. Every chromosome was also inde-
pendent of every other chromosome in the population (at least in terms
of ﬁtness, although chromosomes are related to other individuals through
a parent and child relationship generated by the crossover process). We
now consider a slightly more complex version of the TSP — one associ-
ated with crew scheduling, project management, assembly line balancing,
and similar operations. Instead of a single ﬁve-city path, the genetic algo-
rithm is asked to schedule a set of N crews who must visit all ﬁve cities
during the day. We have an electrical, a road repair, and a vehicle mainte-
nance crew. Thus, N = 3. Figure 9.20 shows the organization of the crew
trip-scheduling chromosome.
The objective of the genetic search is to ﬁnd the crew trip schedule
that minimizes the distance traveled by the three crews. One way to do
this, as shown in Expression 9.9, is through a direct minimization ﬁtness
Crews
Electrical
Road repair
Vehicle Maintenance
Figure 9.20
The crew trip-scheduling chromosome.

378
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
function that simply sums the path length for the three crews.
f = fe( ) + fr( ) + fv( )
(9.9)
Here,
f
is the ﬁtness of the chromosome
fe( )
is the ﬁtness of the electrical path
fr( )
is the ﬁtness of the road repair crew path
fv( )
is the ﬁtness of the vehicle maintenance crew
Another way to formulate a ﬁtness function is to measure the ﬁtness of
a solution relative to the total distance traveled by any set of crews in
a schedule. Thus, the best solution will be the smallest relative to the
crews that took the longest path through the ﬁve cities. Expression 9.10
illustrates this type of ﬁtness function.
f = 1 +
di
dmax
(9.10)
Here,
f
is the ﬁtness of the chromosome.
di
is the total distance traveled by the three crews.
dmax
is the maximum distance traveled by any set of crews. This is
discovered by iterating through the population and ﬁnding
the crew schedule with the longest path length.
Basing a ﬁtness function on factors spread over or intrinsic to the
population is often used when the individual solutions have a high degree
of variability. For example, consider a crew-scheduling system with the
following characteristics.
■
Crews are assigned pending jobs for the day.
■
Depending on the distance to the jobs, crews may work on a variable
number of jobs (that is, because a crew can only work eight hours,
the mix of jobs and the distance to each job — from the main ofﬁce
or from a previous job — determines the number of jobs that can be
done).
■
The company wants as many jobs as possible completed each day.
In this case, we want to maximize jobs but minimize the travel time
between jobs (in that long travel times incur elevated fuel costs, can lead
to excessive wear and tear on equipment, and increase the probability that

9.3
The Architecture of a Genetic Algorithm
■
379
the previous job will be started late or remain incomplete for that day).
A ﬁtness function that only evaluates the schedule of a single chromosome
independent of all other schedules cannot take into account the variability
in the number of actual jobs scheduled coupled to the distance traveled
to satisfy those jobs. Hence, a population-based ﬁtness function is more
appropriate. Expression 9.11 shows one possible ﬁtness function.
f = (N −Ji) +
di
dmax
(9.11)
Here,
f
is the ﬁtness of the chromosome.
N
is the total number of jobs being assigned to the crews on
this day.
Ji
is the number of jobs scheduled for the three crews.
di
is the total distance traveled by the three crews.
dmax
is the maximum distance traveled by any set of crews. This is
discovered by iterating through the population and ﬁnding
the crew schedule with the longest path length.
This is a minimization ﬁtness function. The smaller the value the better
the ﬁtness. Thus, for two candidate schedules, s1 and s2, the one that
schedules the most jobs is the best (as Ji approaches N, the value becomes
smaller and smaller). If both schedules have the same number of planned
jobs, the one with the smallest overall distance traveled is the best.
Designing the genome representation of the problem parameters and
selecting a ﬁtness function to measure each chromosome’s goodness of ﬁt
is the ﬁrst phase in using a genetic algorithm. We now turn to the actual
mechanics of the algorithm, exploring the iterative process of breeding a
solution.
➋Generate Initial Population
A genetic algorithm begins with a population of potential solutions. These
solutions are encoded in the chromosomes. An initial population of poten-
tial routes consists of randomly generated combinations of cities. Each
genome site (locus) is the next city in the route. Thus, a genome of
(3,2,4,1) would be a path from Charlie to Baker to Delta to Able. Table 9.9
is part of this population of N potential solutions.
There are two general performance objectives on the generation of all
populations in a genetic algorithm (and they apply to initial populations
as well as future populations that are bred from the initial population).

380
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
TABLE 9.9
A Population of TSP Solutions
Genome
Locus Values
1
1
3
2
5
4
2
3
5
4
1
2
3
2
1
4
3
5
4
3
1
4
5
2
5
5
2
4
3
1
6
1
5
2
4
3
:
:
N
1
4
5
3
2
■
First, each potential solution must be a feasible solution. Feasibility
means that the solution obeys all hard constraints on the solution. If
the TSP problem, for example, speciﬁes a particular city as the starting
point, randomly generated solutions that do not start with this city
are not feasible solutions. It is often difﬁcult in real-world problems to
ensure that each solution is feasible, and this constraint often means
that the mechanics of the genetic algorithm are intricately connected
to the mechanics of the application.
■
Second, each potential solution must be a unique solution. This is
a constraint on the population that is often overlooked in the litera-
ture on genetic algorithms, generally because most of the problems in
the academic literature involve solving simple problems (such as this
small TSP problem in which duplicate paths would not appreciably
slow down the search process). Finding duplicate or nonunique solu-
tions is often a difﬁcult task because a duplicate genome is not always
obvious. For example, if the direction of the path is immaterial the
genome (1,2,3,4) is the same as (4,3,2,1) because both represent the
same path.
In some applications, of course, one or both of these objectives cannot be
met, either because knowing what is a feasible solution is impossible out-
side the internals of the application or because recognizing duplicates is
either topologically impossible or would cost more in evaluation time than
actually reevaluating a duplicate chromosome. The inability to recognize
duplicate chromosomes is more often true in evolutionary programming
models than in genetic algorithms.
Population Diversity
A critical factor that bears on the performance of a genetic algorithm
is population diversity. Diversity is a measure of the robustness of

9.3
The Architecture of a Genetic Algorithm
■
381
chromosome representations; that is, how well they are distributed over
the possible solution space. Early diversity is essential in the genetic algo-
rithm, whereas later diversity indicates a problem with convergence.
Diversity can be assessed in several ways, but there are two common
methods.
■
By computing the variation in the ﬁtness values over a population.
This is essentially, as shown in Expression 9.12, the standard error of
the ﬁtness function (the degree of variation from the average ﬁtness).
f v = 1
N
N

i=1
 
(fa −fi)2
(9.12)
Here,
f v
is the average variance of the population chromosomes as
measured by the variance of their ﬁtness values
N
is the total number of chromosomes in the population
fa
is the average population ﬁtness
fi
is ﬁtness of the i-th chromosome
A large variance indicates a robust degree of variation in the ﬁtness
functions, whereas a small variance indicates a more compact and less
varied population of ﬁtness functions.
■
By computing the average change in ﬁtness between successive gen-
erations. The idea behind a genetic algorithm is convergence, through
selective breeding, on an optimal set of solutions. One way of examin-
ing diversity in the population is by computing the change in average
ﬁtness from one generation to the next. Expression 9.13 shows a
simple method of tracking this change.
f = 1
K
K

n=2
¯fn −¯fn−1
(9.13)
Here,
f
is the average change in the ﬁtness from one
generation to the next.
K
is the total number of generations elapsed in the
genetic search.
¯fn −¯fn−1
is the average ﬁtness of the n-th generation.
Plotting the change in the average population ﬁtness from one gener-
ation to the next is a key indicator of convergence. As new and better

382
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
solutions are created, the difference between the average ﬁtness in
each successive generation should move toward zero.
Diversity measures the richness of the gene pool and plays an important
part in the early stages of a genetic search. Creating an initial population
with high diversity depends on a deep understanding of the problem space
and the ability to create chromosomes that are scattered throughout the
possible solution terrain. Maintaining genetic diversity through the search
process rests on both the nature of the crossover operations as well as
the rate of mutations and spontaneous birth in the overall population.
Population Size
Determining the size of the initial population is a difﬁcult but crucial step
in using a genetic algorithm. Each chromosome in the population rep-
resents a point somewhere in the solution terrain. When the population
is too small, as illustrated in Figure 9.21, the genetic algorithm can only
search a small region of the possible solution space.
When the population is very small in relation to the possible search
space, a genetic algorithm will either take a very long time to ﬁnd a
reasonable solution or will wander around the terrain, often locking on a
local minimum or maximum. On the other hand, a population that is too
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.21
A small population spread over the solution space.

9.3
The Architecture of a Genetic Algorithm
■
383
z
30
10
20
40
50
x
60
70
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160180200220
Figure 9.22
A large population spread over the solution space.
large, as shown in Figure 9.22, relative to the solution space covers too
much of the underlying search space.
Such a very large population often lacks genetic diversity (because it
covers so much terrain) and can require a very high number of generations
to percolate high-performing chromosomes out of the large number of
lower- or moderate-performing chromosomes.
An analysis of the optimal population size for many combinatorial and
permutation problems in which the potential solution space becomes
very large for even a small number of variables (such as the TSP) rests
on the use of probability theory to judge the probability that an optimal
solution lies within a population N chromosomes. These studies indicate
that the performance of even large TSPs is not critically dependent on the
population size if the population is reasonably large (144 chromosomes
for a 30-city problem, and 409 chromosomes for a 75-city problem).
A good rule of thumb connects the initial population size to the number
of variables in the algorithm (the number of loci in the chromosome) and
the number of possible states in the solution space. An initial population,
as shown in Expression 9.14, should be at least as large as ﬁve times the
number of variables or about half the maxim number of possible states,
whichever is smaller.
p = min

(5 × v),
1
2 × s

(9.14)

384
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Here,
p
is the population estimate
v
is the total number of variables or chromosome loci
s
is the number of possible states in the solution space
In a TSP, the maximum number of routes is n! (1 × 2 × 3 × 4 . . .× n),
meaning that the solution space has grown very large very fast. The ﬁve-
city TSP, however, has only 120 possible routes. With this rule of thumb,
the population size estimate is 25 individuals: the minimum of 25 (ﬁve
times the ﬁve chromosome loci) and 60 (one-half the possible 120 states).
Although the maximum number of states is small enough to include all
possible solutions in the initial population, this would actually impede
the genetic search. The genetic process of breeding and mutating new
solutions in a population that as an initial condition already contains all
possible solutions can easily result in a protracted search through the
population, as the crossover process will simply produce duplicates that
must be discarded.
➌Evaluate the Chromosome
This is the core functional component of the genetic algorithm: evaluat-
ing a chromosome associates a goodness-of-ﬁt or performance measure
to each chromosome in the population. But assigning this goodness
of ﬁt is not necessarily a part of the genetic algorithm itself. In many
large-scale business applications (such as those in inventory optimization,
manufacturing assembly line balancing, and crew and project scheduling)
the solution represents a conﬁguration or a set of parameters that must
be processed by the connected system. As illustrated schematically in
Figure 9.23, the system generates an actual outcome whose degree of
performance (or “goodness”) is measured by the ﬁtness function.
Figure 9.23 outlines the basics of the genetic algorithm’s evaluation
process. The iterative analysis of each chromosome creates a popula-
tion of solutions ranked by their “goodness” as measured by the ﬁtness
function. The process is fairly straightforward.
①The evaluation process is applied to each chromosome (poten-
tial solution) in the current population. Much of the mechanism
in a genetic algorithm is concerned with the production of this
population of chromosomes.
②The next chromosome is selected from the population.
③Use the solution conﬁguration in the chromosome to execute the
underlying system. In some cases, the system itself is embedded
in the ﬁtness function (as is the case with the TSP). In many other

9.3
The Architecture of a Genetic Algorithm
■
385
Get chromosome
Chromosome
population
Run
system
Produce solution
Apply fitness
Repeat
5
4
3
2
1
6
Evaluate potential solution
Figure 9.23
Evaluating a chromosome.
cases, the chromosome’s values are passed to a larger system. For
example, consider a genetic algorithm that performs trend ﬁtting.
The chromosome might contain the coefﬁcients and powers of the
equation terms. These values are passed to a regression engine that
forms the equation, reads a ﬁle of data points, and computes the
standard error from the differences between actuals and estimates.
The regression engine is the “system” connected to the genetic
algorithm.
④From the chromosome conﬁguration a solution is produced by the
underlying system. In some cases, of course, the chromosome and
the system are the same (as in the TSP model). In other cases, such
as the trend-ﬁtting model, the solution is the standard error of esti-
mate produced from the equation described by the chromosome’s
coefﬁcients and exponent values.
⑤Apply a ﬁtness function to the solution in order to assign a per-
formance rank to the current chromosome. The ﬁtness analysis
actually changes the content of the chromosome by storing a
goodness-of-ﬁt measure.

386
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
⑥Go back and get the next chromosome in the population. The
evaluation process is repeated for each chromosome (unless some
special application-speciﬁc control strategy interrupts this cycle).
Thus, in summary, a chromosome is a potential solution. The “system”
is the process that creates an outcome state (a solution) based on the
content of a chromosome. It is this outcome space that is evaluated by the
genetic algorithm based on the ﬁtness function. Because the evaluation
process is the core of the genetic algorithm and because it is dependent
completely on the ﬁtness function, the genetic algorithm as a whole is
critically dependent on a proper formulation of its ﬁtness function.
➍Terminate
Like biological evolution that goes on and on, a genetic algorithm con-
tinues to run, evolving solutions until explicitly terminated. After each
population is evaluated, the algorithm checks a set of termination condi-
tions. There are several common termination conditions used in genetic
algorithms.
■
The maximum number of generations has been reached.
■
A maximum elapse (wall clock) time has been reached.
■
A maximum amount of computer resources has been used.
■
A chromosome with a particular ﬁtness value emerges.
■
The average (or best) ﬁtness function value reaches a steady state over
successive generations. Normally this is a tolerance comparison (for
example, stop when the change in the ﬁtness function is less than
.001 over 100 generations).
■
The average (or best) ﬁtness function value oscillates over the
generations.
■
The average ﬁtness reaches a very early steady state or begins to decay.
A sudden or gradual lack of average ﬁtness indicates a problem in
the way high-performing chromosomes are bred and their offspring
propagated into the next generation. This is sometimes caused by
mutation rates that are too high.
These termination conditions are often used in combination. A genetic
algorithm can be terminated after a certain number of generations, when a
particular average ﬁtness has been achieved, or when the ﬁtness function
does not change after a speciﬁc number of generations.

9.3
The Architecture of a Genetic Algorithm
■
387
A genetic algorithm’s termination conditions can also include a set of
policy rules associated with the underlying application. These if-then-else
rules are often invoked to check the way the search behavior is evolving,
as well as to apply additional post-evaluation logic to the population.
➎Create a New Population
Next to the evaluation of each chromosome’s ﬁtness, the techniques for
breeding each succeeding generation of chromosomes are the most cru-
cial component of a genetic algorithm. It is this phase that is intended
to balance the dual and often conﬂicting objectives of increasing the
average population ﬁtness and increasing genetic diversity. Figure 9.24
outlines the step-by-step process of breeding a new generation from an
old generation.
①The top N best (high-performance) chromosomes are retained.
Some percentage of the chromosomes with the best ﬁtness values
is retained and moved to the next generation. These chromosomes
will also form the core (but not complete) collection of chromo-
somes for the breeding (crossover) process that generates new
child chromosomes.
Cross-over
c% best
chromosomes
Save top N
best
chromosomes
Mutate
m% of
chromosomes
Insert n%
new
chromosomes
4
3
2
1
New (outgoing)
population
Current (incoming)
population
Figure 9.24
Breeding a new population of chromosomes.

388
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
②Of the ﬁttest chromosomes, a percentage (c%) of these will
become parents and generate one or more children in the new pop-
ulation. There are several stochastic-based techniques for selecting
which pair of chromosomes will be mated, the most common
being a form of roulette wheel that assigns a chromosome a chance
of being selected based on its degree of ﬁtness. These techniques
are discussed in more detail later in this chapter.
③After selecting the top-performing individuals and breeding new
child chromosomes from the randomly chosen parents, a small
number of chromosomes (m%) in the new population is subjected
to mutation. Mutation increases genetic diversity by a process of
selecting a random locus in the chromosome and changing its value
to a random (but allowable) value.
④Not a usual part of traditional genetic algorithms, inserting a
small number (n%) of new chromosomes with randomly valued
genomes can, along with mutation, increase genetic diversity in
the population. Inserting new chromosomes often provides an
effective form of simulated annealing when the mutation rate alone
appears to be insufﬁcient.
The idea behind breeding a new population is analogous to the evolution
of ﬁtter organisms in a biological population. Crossover (the equivalent of
sexual reproduction) exchanges the genetic material of two parents and
produces one or more offspring. If the genetics of the parents have a high
degree of ﬁtness, the crossover (or breeding) process is designed to pro-
duce children that also have genetics with an elevated degree of ﬁtness,
but due to the scrambling of the genomes their chromosomes will lie in
a slightly different area of the solution terrain. Mutation, the equivalent
of a fault in chromosome transcription during biological reproduction, is
designed to introduce a small amount of genetic diversity in the popula-
tion. Like biological mutation, some are advantageous and others are fatal
(in terms of improving the ﬁtness of the population). Although there is no
direct biological counterpart to the insertion of new individuals into the
population (aside from some ancient beliefs in spontaneous generation),
one way of looking at the creation of new chromosomes is a localized
form of complete genome mutation.
Strategies for Chromosome Selection
Which chromosomes in the current population do we select to become
members of the next generation, to breed with another chromosome,
or to have their existing genome changed through mutation? This is the

9.3
The Architecture of a Genetic Algorithm
■
389
process of selection. In many cases a different selection process is used for
selecting the set of ﬁttest individuals, for selecting parents for crossover,
or for selecting chromosomes for mutation. Different selection techniques
have differing access probabilities depending on how they are used in
the genetic algorithm. We now turn to a discussion of several common
techniques.
The Elitist Strategy
Using an elitist strategy, a percentage of the chromosomes with the best
ﬁtness function values are selected. Elitism is not only used to pick chro-
mosomes for breeding but to directly copy chromosomes into the next
generation. In many cases an elitist technique is used in combination
with other selection methods to ensure that some of the strongest chro-
mosomes always make it into successive generations. The elitist selection
technique is implicit in the ﬁrst step of the genetic algorithm mechanism
shown in Figure 9.13.
Proportional Fitness
Using a proportional (or roulette wheel) ﬁtness strategy, a wide spectrum
of chromosomes with varying degrees of ﬁtness are selected. The selec-
tion is biased toward chromosomes with best ﬁtness values. Proportional
ﬁtness works in two steps: it ﬁrst creates a conceptual roulette wheel
weighted according to the best ﬁtness function values, and then essen-
tially spins the wheel and ﬁnds the chromosome that ﬁts in the currently
weighted slot. In a conventional approach to proportional selection, the
ratio to of each unique ﬁtness value and the sum of the total ﬁtness values
in the population is used to create the roulette wheel. For populations
that contain repeating ﬁtness functions, using a weighted frequency count
of the ﬁtness function provides a way of maintaining the same type of
roulette wheel approach. Expression 9.15 shows how the wheel slice is
calculated.
w =
fi
N
k=1 fk
(9.15)
Here,
fi
is the ﬁtness of the i-th chromosome
fk
is the ﬁtness of the k-th chromosome
N
is the number of chromosomes in the population
This expression works well for maximization functions because the larger
the individual ﬁtness value the larger its fraction of the sum of all ﬁtness

390
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
values and the larger its proportion of the wheel. For minimization func-
tions, the magnitude of the numbers must be reversed. Expressions 9.16
and 9.17 show one possible way of reversing the magnitude of the ﬁtness
functions.
f R = (fmax −fi) + 1
(9.16)
w =
f R
N
k=1 f R
k
(9.17)
Here,
f R
is the reversed magnitude ﬁtness function
fi
is the ﬁtness of the i-th chromosome
fmax
is the maximum ﬁtness among all chromosomes in the
population
fk
is the ﬁtness of the k-th chromosome
N
is the number of chromosomes in the population
Examining a small portion of the TSP population illustrates how the
weighted roulette wheel is created from the ﬁtness value. Because we
are attempting to minimize the route through the ﬁve cities, the wedge
expression from Expression 9.17 is used. Table 9.10 shows the ﬁtness
(path length) values for a set of city routes, their adjusted ﬁtness values,
and their proportion of the ﬁtness roulette.
The ratio (w) derived from the total sum of adjusted ﬁtness values in the
population speciﬁes the weighted slot in the underlying roulette wheel.
Figure 9.25 shows this roulette wheel (not exactly to scale) proportioned
according to the magnitude of the adjusted ﬁtness values.
The actual proportional ﬁtness algorithm is rather easy to understand
and implement. Listing 9.1 shows the basic logic. This code computes a
random ﬁtness level (max_ ﬁtness) from the sum of the population’s indi-
vidual ﬁtness values. The algorithm then loops through the population,
TABLE 9.10
The TSP ﬁtness and ﬁtness ratios
Raw
Magnitude
Tour
Fitness
Adjusted
w
1
Able, Baker, Echo, Delta, Charlie
90
1
.007
2
Able, Echo, Charlie, Baker, Delta
34
57
.387
3
Able, Echo, Delta, Baker, Charlie
57
34
.232
4
Able, Baker, Charlie, Echo, Delta
59
32
.217
5
Able, Charlie, Echo, Delta, Baker
68
23
.156
Sum
308
147
1.000

9.3
The Architecture of a Genetic Algorithm
■
391
57
38.77%
34
23.13%
32
23.81%
23
14.96%
1
>1%
Figure 9.25
The proportional ﬁtness roulette wheel.
Proportional Selection:
For each chromosome (i)
total_fitness = total_fitness + chromosome(i).fitness
End for each
max_fitness = total_fitness * random()
cumm_fitness=0
for each chromosome(i)
cumm_fitness = cum_fitness + chromosome(i).fitness
If chromosome(i).fitness > max_fitness
Then select(chromosome(i))
end for each
Listing 9.1
The proportional ﬁtness approach.
summing each chromosome’s ﬁtness (into cumm_ ﬁtness). When the
algorithm encounters a chromosome with a ﬁtness greater than or equal
to the max_ ﬁtness value, that chromosome is selected. Note that it is
important that the population not be sorted by ﬁtness. Otherwise, the
selection process is signiﬁcantly biased (and will simply function as a
slightly more complicated form of elitism).

392
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Proportional ﬁtness selection suffers from two general problems. This
ﬁrst has already been directly encountered: it is difﬁcult to use on min-
imization problems. Often, the ﬁtness function for minimization must
be converted to a maximization function. Although to some degree this
solves the selection problem, it introduces semantic confusion into the
problem (the best chromosome in the TSP problem, for instance, will
continually be assigned a ﬁtness value that is the maximum of all other ﬁt-
ness functions, and thus we are seeking the minimum tour but the ﬁtness
maximizes the ﬁtness value). The second problem is directly related to
how the roulette wheel is constructed. The proportional ﬁtness selection
will fail (it will drive the population to early convergence) if some of the
chromosomes have ﬁtness values that are very much larger than all other
ﬁtness values.
Ranking
Using a linear ranking strategy, an ordered population of chromosomes
is assigned ﬁtness values based on their position or rank within the pop-
ulation. There are many mathematical means of converting a rank into a
relative or subjective ﬁtness.
fi = (N −r) ×

f u
max −f u
min

N −1
+ f u
min
(9.18)
Here,
N
is the number of chromosomes in the population (the
population count, obviously, must be greater than 1).
r
is the current rank of the chromosome. In most cases (but not
necessarily all) for the i-th chromosome r = i.
fi
is the ﬁtness assigned to the i-th chromosome.
f u
max
is the maximum user ﬁtness that should be assigned to the
best-performing individual.
f u
min
is the minimum user ﬁtness that should be assigned to the
worst-performing individual.
The ranking strategy prevents extremely ﬁt individuals from dominating
the population during the early generations of the search process. Thus,
the ranking technique can be very effective in maintaining diversity and
inhibiting convergence. The outcome of ranking can be used with an
elitist selection or with other selection approaches (such as proportional
ﬁtness). One constraint on the use of ranking is the requirement that the
population be reordered and reranked for each generation.

9.3
The Architecture of a Genetic Algorithm
■
393
Tournament
Generally used for large diverse populations, this approach is somewhat
akin to the divide-and-conquer strategy in search and sorting. In a tour-
nament selection, a set of k chromosomes is selected randomly from the
population (where k = 2 is the usual size). From the set of k chromo-
somes the individual with the best ﬁtness is selected. For example, using
the TSP chromosomes in Table 9.10, the tournament strategy could select
genomes 2 (Able, Echo, Charlie, Baker, Delta) and 4 (Able, Baker, Charlie,
Echo, Delta). Chromosome 2 would be selected over 4 because 2 has a
better raw ﬁtness function (34 miles instead of 59 miles). This process of
selecting a group and picking the best (ﬁttest) chromosome is continued
as long as a selection process is needed.
Tournament differs from proportional ﬁtness because it is indifferent
to the relative frequency or range of ﬁtness values. It is only sensitive to
the ﬁtness ranking within the population. Because it is essentially prob-
ing the population in blocks of k chromosomes (with replacement, so
that chromosomes may participate in multiple tournaments), the tourna-
ment approach can select a wider spectrum of chromosomes with varying
degrees of ﬁtness.
Random
Using a random strategy, a random chromosome in the population is
selected. Random selection is used to pick chromosomes for breeding
and to directly copy chromosomes into the next generation. In hybrid
algorithms, a random strategy is used in combination with other selection
methods to ensure that a mix of chromosomes with a wide spectrum of
ﬁtness values always makes it into successive generations.
Selection strategies are malleable; that is, a genetic algorithm can
switch from one strategy to the next during the search. An example is
using an elitist strategy for a few generations to quickly increase the aver-
age ﬁtness and then switching to proportional ﬁtness to ensure some
continued measure of ﬁtness diversity in the population. In the next sec-
tion we will discuss the actual process of breeding new chromosomes.
The breeding mechanism is tightly coupled to the selection strategy.
Conventional Crossover (Breeding) Techniques
A genetic algorithm derives its name from the way in which potential solu-
tions are developed. They are bred from existing solutions using a process
with analogies to sexual reproduction in biological organisms. The pro-
cess of mating and reproduction in a genetic algorithm is called crossover.

394
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Cross-over
method
Parent
chromosome 1
Parent
chromosome 2
Child
chromosome 1
Child
chromosome 2
Figure 9.26
Mating and breeding of chromosomes.
This name follows from the way in which genetic material is exchanged.
Pieces of the genetic material from two parents are exchanged by extract-
ing a set of loci from one parent and a set of loci from another parent
and appending or inserting them in the corresponding position in each
other’s genome. Thus, they move in an X pattern as the genome segments
cross over each other. Figure 9.26 illustrates the way two chromosomes
mate and breed two offspring.
Like biological reproduction, reproduction in genetic algorithms is
intended to create better chromosomes from already highly performing
parents while at the same time adding genetic diversity to the gene pool
(the population of chromosomes in the current generation). Philosophi-
cally, a genetic algorithm is a directed eugenics program designed to breed
better and better individuals. This section examines the principal methods
of crossover and explains how they work in the search algorithm’s breed-
ing program. As discussed in the section on genome design (see “Genome
Structural Design”), conventional crossover cannot be used for the TSP.
Breeding and mutation techniques for the TSP and similar connectionist
problems are discussed in the section “Advanced Crossover and Mutation
Techniques.”
A Crew Scheduling Cost Model
To illustrate crossover (and mutation, in the next section), we will explore
a small and quite simple cost modeling system for crew-to-job assignments.

9.3
The Architecture of a Genetic Algorithm
■
395
TABLE 9.11
Job ID and Duration and Crew ID and Rate
Job ID
Duration
Crew ID
Rate
J1
6
C1
10
J2
5
C2
15
J3
9
C3
20
J4
8
J5
3
The model contains two tables: an N × M matrix of jobs and their duration
times and an N × M matrix of crews and their daily charge rates. Table 9.11
shows the job and crew schedules.
This is a cost minimization problem. We want to assign the crews to
the jobs in order to ﬁnd the minimum cost schedule. There are fewer
crews than jobs, and thus some crews will be assigned to multiple jobs.
The total cost for a schedule is shown in Expression 9.19 (which is also
the ﬁtness function).
f =
K

i=1
d(i) × r(c(i))
(9.19)
Here,
K
is the number of jobs. In this case, there are ﬁve jobs.
f
is the total cost of the job assignments and is the ﬁtness
assigned to the i-th job-to-crew chromosome.
d( )
is duration of the i-th job (chromosome locus).
r( )
is the rate of the crew assigned to the i-th job.
c( )
is the crew found in the i-th genome locus.
Because the number of jobs is static, the genome representation can
be very straightforward: ﬁve locus sites. Each locus contains the crew
assigned to the job (that is, it contains the index to Table 9.11, the crew
table). Figure 9.27 is a schematic of the job-to-crew costing chromosome.
The genetic search mechanism generates a population of candidate
cost plans by assigning crews randomly to the ﬁve jobs. Our only con-
straint is that every crew must be used at least once. Table 9.12 shows a
small part of the initial population.
This small job-cost planning model now provides the background
needed to explore the various types of conventional crossover techniques.
These techniques are used to breed new cost models based on the genetic

396
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Crew Assignments
J1
J2
J3
J4
J5
Figure 9.27
The job-to-crew assignment genome.
TABLE 9.12
Initial Population of Job Cost Genomes
Jobs with Crew Assignment
J1
J2
J3
J4
J5
1
C1
C3
C3
C2
C3
2
C2
C3
C1
C1
C1
3
C2
C1
C2
C3
C3
4
C3
C1
C2
C2
C1
5
C2
C2
C2
C1
C3
6
C3
C2
C2
C1
C3
material in parents (ultimately chosen through one of the selection strate-
gies). The next section explores the single-point crossover in some detail.
The remaining crossover patterns are extensions of this basic concept.
Single-point Crossover
In single-point crossover a single point along the genome is selected.
Two parent chromosomes are selected from the population. A crossover
point is chosen at random. The genome segments to the right (or left) of
the point are swapped, creating two new chromosomes (the children).
Figure 9.28 schematically illustrates the crossover process that produces
new children from the genetic material of the parents. The single-point
crossover is immediately after the second locus in the chromosome.
In this example, the children inherit the ﬁrst two loci from the parents.
Child 1 inherits the right-hand genetic material from parent 2, whereas
child 2 inherits the right-hand genetic material from parent 1. The children
are usually inserted into the new population, displacing either the par-
ents or two chromosomes whose ﬁtness values are less than the parents’
ﬁtness value. Table 9.13 shows the small job assignment population with
each chromosome’s ﬁtness (cost) and the average population ﬁtness.

9.3
The Architecture of a Genetic Algorithm
■
397
Single point
cross-over
Parent
chromosome 1
Parent
chromosome 2
Child
chromosome 1
Child
chromosome 2
AB   CDEF
TU      VWXYZ
ABVWXYZ
TUCDEFG
Figure 9.28
The single-point crossover process.
TABLE 9.13
Job Cost Genomes with Fitness Values (Costs)
Jobs with Crew Assignments
J1
J2
J3
J4
J5
Cost
1
C1
C3
C3
C2
C3
520.00
2
C2
C3
C1
C1
C1
318.00
3
C2
C1
C2
C3
C3
495.00
4
C3
C1
C2
C2
C1
455.00
5
C2
C2
C2
C1
C3
440.00
6
C3
C2
C2
C1
C3
470.00
Average Fitness
449.66
Through some selection process, individuals 2 and 4 are chosen as
parents. The single-point crossover is at the second locus (at job J2). As
Table 9.14 shows, the crossover generates two new chromosomes (c1
and c2). Both have ﬁtness values less than the maximum ﬁtness of the two
parents. Even without removing two lower-performing chromosomes,
this average population is improved (a change of 16.91).
If the genetic algorithm maintains a steady population size (as is con-
ventional in most but not all situations), the two new children replace
poorer-performing genomes. In this case, chromosomes 1 and 4 are

398
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
TABLE 9.14
Job Cost Genomes with Fitness Values (Costs)
Jobs with Crew Assignments
J1
J2
J3
J4
J5
Cost
1
C1
C3
C3
C2
C3
520.00
2
C2
C3
C1
C1
C1
318.00
3
C2
C1
C2
C3
C3
495.00
4
C3
C1
C2
C2
C1
455.00
c1
C2
C3
C2
C2
C1
370.00
c2
C3
C1
C1
C1
C1
394.00
5
C2
C2
C2
C1
C3
440.00
6
C3
C2
C2
C1
C3
470.00
Average Fitness
432.75
TABLE 9.15
Job Cost Genomes with Fitness Values (Costs)
Jobs with Crew Assignments
J1
J2
J3
J4
J5
Cost
1
C2
C3
C2
C2
C1
370.00
2
C2
C3
C1
C1
C1
318.00
3
C3
C1
C1
C1
C1
394.00
4
C3
C1
C2
C2
C1
455.00
5
C2
C2
C2
C1
C3
440.00
6
C3
C2
C2
C1
C3
470.00
Average Fitness
407.83
removed and the new children take their place. Table 9.15 shows both the
new population with the children of 2 and 4 and the signiﬁcant increase
in the average population ﬁtness.
Not every crossover will improve the ﬁtness of the population. In fact,
in addition to the goal of ﬁnding better and better chromosomes, breeding
has the goal of increasing genetic diversity in the population. Only through
genetic diversity can a genetic algorithm economically and effectively
explore a sufﬁcient portion of the solution terrain. The other forms of
crossover provide different approaches to breeding new offspring. Each
technique has its own place in attempts to gain better (ﬁtter) individuals.

9.3
The Architecture of a Genetic Algorithm
■
399
Double-point Crossover
In double-point crossover two points along the genome are selected at
random. The genome segment to the left of the rightmost point is swapped
with the genome to the right of the leftmost point, creating two new
children. Figure 9.29 schematically illustrates the crossover process that
produces new children from the genetic material of the parents. The
crossover points are immediately after the second locus and immediately
before the last locus in the chromosome.
In terms of exchanging genetic material and how it is used, the double-
point process is almost exactly like the single-point: the children inherit
the loci to the right of the rightmost point and to the left of the leftmost
point. The genetic material to be swapped is bounded by the two random
point values. Child 1 inherits bounded genetic material from parent 2,
whereas child 2 inherits the bounded genetic material from parent 1. The
advantage of the double-point crossover is its inherent ability to introduce
a higher degree of variability (randomness) into the selection of genetic
material.
Uniform Crossover
Uniform crossover works at the individual locus level rather than with
segments of the genome. Loci positions are picked at random from
the genomes and exchanged. Figure 9.30 schematically illustrates the
Double point
cross-over
Parent
chromosome 1
Parent
chromosome 2
Child
chromosome 1
Child
chromosome 2
AB     CDEF     G  
TU      VWXY      Z
ABVWXYZ
TUCDEFZ
Figure 9.29
The double-point crossover process.

400
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Uniform
cross-over
Parent
chromosome 1
Parent
chromosome 2
Child
chromosome 1
Child
chromosome 2
A
TBCDXFZ
AUVWEYG
BCD
E
F
G
T
UVW
X
Y
Z
Figure 9.30
The uniform crossover process.
crossover process that produces new children from the genetic material
of parents.
The probability of selecting a locus for exchange, called the mixing
rate, can be very low or very high. A mixing rate of .5, for example,
means that any locus in the genome has a 50% chance of being selected
for exchange. The mixing rate acts like a variable rheostat, increasing or
decreasing the probability that the n-th locus on the genome in parent 1
will exchange its value with the n-th locus on the genome in parent 2.
Although uniform crossover has the advantage of precisely controlling the
amount of genetic variability in the population, it also has two signiﬁcant
disadvantages. First, because the crossover is done at the individual gene
(locus) level rather with sets or patterns of genes, behavior that evolves as
patterns in the population will not normally be preserved. Second, if the
mix rate is too high, too much genetic diversity (that is, noise) emerges
in each successive population and the search mechanism cannot ﬁnd an
accurate solution. On the other hand, if the mix rate is too low not enough
genetic diversity will emerge and the search mechanism cannot efﬁciently
spread over the solution terrain.
Weighted (Arithmetic) Crossover
Weighted (or arithmetic) crossover is unlike the other crossover oper-
ations. Weighted crossover modiﬁes rather than exchanges genetic
material and works at the complete genome level rather than with

9.3
The Architecture of a Genetic Algorithm
■
401
segments of the genome. The crossover appears in the way a weight-
ing factor is used. A weighting factor (w) in the range [0,1] is selected
before each crossover. Loci are picked at random from the genomes and
exchanged. Expression 9.20 is the crossover process that produces new
children from the genetic material of the parents.
c1 = (w × p1) + ((1 −w) × p2)
c2 = ((1 −w) × p1) + (w × p2)
(9.20)
Here,
c1
is child 1 from the crossover operation
c2
is child 2 from the crossover operation
p1
is the ﬁrst selected parent
p2
is the second selected parent
w
is the weighing factor in the range [0,1]
The weight values act like scaling factors over the range of the chro-
mosome. The values are rescaled (from 0 . . . n, where n is the value of
the genome at that locus). Table 9.16 illustrates the creation of child c1
from two parents when the scaling weight is .4 (see Table 9.13 for the
underlying chromosomes).
Arithmetic scaling mathematically distorts the genome values in a pre-
dictable manner, but it does not rely on the mixing of genetic materials.
From the author’s experience in production models, weighted crossover
(unless the weight is carefully adjusted) tends to produce lower average
population ﬁtness than the single- or double-point crossover. Table 9.17
(based on the data in Table 9.13) shows the result of applying weighted
crossover to individuals (2,4).
In this instance, the average ﬁtness of the population is reduced by
the crossover. Naturally, the same decrease in ﬁtness can result from any
of the other crossover techniques. This example simply illustrates that
different crossover methods yield different ﬁtness values (as you would
TABLE 9.16
Creating Child c1 from Parents (2,4), where w = (.4)
J1
J2
J3
J4
J5
P1
90.00
100.00
90.00
80.00
30.00
P2
120.00
50.00
135.00
120.00
30.00
P1*w
36.00
40.00
36.00
48.00
12.00
P2*(1-w)
72.00
30.00
81.00
72.00
18.00
Total
108.00
70.00
117.00
120.00
30.00

402
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
TABLE 9.17
Job Cost Genomes with Fitness Values (Costs)
Jobs with Crew Assignments
J1
J2
J3
J4
J5
Cost
1
C1
C3
C3
C2
C3
520.00
2
90.00
100.00
90.00
80.00
30.00
318.00
3
C2
C1
C2
C3
C3
495.00
4
120.00
50.00
135.00
120.00
30.00
455.00
c1
108.00
70.00
117.00
120.00
30.00
445.00
c2
173.60
80.00
75.60
76.60
30.00
435.80
5
C2
C2
C2
C1
C3
440.00
6
C3
C2
C2
C1
C3
470.00
Average Fitness
596.46
expect) and, in this case, the single-point crossover for these two parents
results in a better set of offspring.
Analytical Crossover
The analytical crossover method is a ﬁnal crossover technique that like
the weighted approach works at the complete chromosome level. This
technique is somewhat iterative, and like tournament selection considers
the best and worst ﬁtness of two selected parents.
c1 = pb + s × (pb −pw)
c2 = pb
(9.21)
Here,
c1
is child 1 from the crossover operation
c2
is child 2 from the crossover operation
pb
is the parent with the best ﬁtness
pw
is the parent with the worst ﬁtness
s
is a scaling factor in the range [0,1]
The scaling factor is a random number that changes the range of the dif-
ference between each of the parent genome sites. This can sometimes lead
to infeasible solutions because the scaled value is unallowable. As a con-
sequence, analytical crossover generally has a search parameter, k, that
attempts to ﬁnd a new value of s that will produce a feasible solution. After
the number of searches exceeds k, s = 0, so that the best-ﬁt parent is also
returned as the ﬁrst child. Other modiﬁcations to this technique are also

9.3
The Architecture of a Genetic Algorithm
■
403
implemented, such as searching through the population for acceptable
parents instead of changing the scaling factor.
Breeding techniques create new chromosomes in the population. By
breeding individuals with a high ﬁtness ranking the genetic search process
hopes to introduce new individuals that are slightly superior to some
other chromosomes in the population. These ﬁtter chromosomes, over
generations, place selective pressure on the population and slowly replace
less ﬁt individuals. Breeding alone, however, is insufﬁcient in many cases.
genetic algorithms can become locked in a local region of the solution
space due to a general lack of diversity in the current gene pool. The next
section takes up issues and techniques related to one of the conventional
methods for recovering genetic diversity.
Conventional Mutation (Diversity) Techniques
Genetic algorithms are very sensitive to genetic diversity. In some cases,
the diversity introduced by crossover breeding is insufﬁcient to explore
the underlying solution space. The population becomes conﬁned to a
small region of the solution space. Breeding simply moves the search
around and around this region. Figure 9.31 illustrates this problem. Each
point represents the average ﬁtness of the entire population.
z
30
10
20
40
50
x
60
70
= Average population fitness
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
Figure 9.31
A population without sufﬁcient genetic diversity.

404
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
In this ﬁgure, the genetic algorithm is wandering around a small
region of the potential solution space. Any combination of the current
genetic material is insufﬁcient to produce individuals that move outside
this region. To solve this problem, genetic algorithms use the concept of
mutation. Mutation, as the name implies, randomly changes the value of
a genome locus. Through mutation, genetic diversity can be maintained
or reintroduced into the population. Figure 9.32 illustrates the locked
population in Figure 9.31 with a few mutations in one of the generations.
Mutation operators must be applied carefully and sparingly to the pop-
ulation. Too much mutation and the genome loses its ability to retain any
pattern, and although the population may be scattered over a wide region
of the solution terrain the search mechanism has no way of improving its
performance. Only a very small number of individuals should be subject to
mutation during a generation. In many genetic algorithms, a parameter of
the search system itself determines whether or not a mutation is applied
to any chromosomes during the current generation. In this section, the
various types of conventional mutation operators are discussed. With the
exception of the binary inversion technique, these mutation operators are
designed to work on integer and real-number genomes. A knowledge of
z
30
10
20
40
50
x
60
70
= Average population fitness
80
90
100
120
140
y
50
40
30
20
10
0
–10
–20
–30
–40
–50
160
180
200220
= Average population fitness (without mutation)
Figure 9.32
Genetic diversity through mutation.

9.3
The Architecture of a Genetic Algorithm
■
405
the loci allowed range of values is necessary for each operator that works
on numbers instead of bit strings.
Binary (Bit) Inversion
For genomes represented by binary (bit) strings, the inversion operator
simply ﬂips the value of a randomly chosen bit. A 1-bit becomes zero,
and a 0-bit becomes one. Due to the nature of binary chromosomes, this
is the primary and often principal mutation operator used in classical
(binary-represented) genetic algorithms.
Uniform Replacement
Uniform mutation replaces a randomly selected gene (locus) with a value
chosen from a uniform random distribution between the upper and lower
domain bounds for the gene. This is the most frequently used mutation
operator because it requires only the range of allowed values for the gene.
Distribution-based Replacement
Instead of a value uniformly drawn from the domain of the gene, this oper-
ator updates the gene position with a statistical value drawn from some
probability distribution. Normally, a Gaussian distribution is used (and
the value is truncated or regenerated if it lies outside the allowable range
for the gene). However, a binomial (Poisson or other type) of distribution
can also be used.
Central-and-Limits Replacement
The central-and-limits mutation replaces the gene value with one of three
randomly chosen values: the upper boundary value from the gene’s
domain, the lower boundary value from the gene’s domain, or the value
from the center of the domain (upper-lower)/2). Generally, the mutation
operator has differing probabilities for each assignment: a high probability
for boundary values and a smaller probability for the center of distribution
value. The central-and-limits mutation operator often provides a way of
introducing a signiﬁcant amount of genetic diversity into the population
and is useful in the early stages of evolution. The sharp three-step process
of assigning the minimum, maximum, or middle domain values “shakes”
the genome in a way similar to simulated annealing.

406
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Nonuniform Decay
The nonuniform decay operator is designed to slowly reduce genetic
diversity caused by mutation as the number of generations increases. The
operation begins to drive the probability of a mutation toward zero as the
number of generations increases. Expression 9.22 shows one possible
representation for the decay mutation operator.
pm = pm × min
 l
gc , 1

(9.22)
Here,
pm
is the current probability that a gene will mutate.
l
is the switchover limit. When l = 1, the decay begins right after
the ﬁrst generation. When l > 1, the mutation probability stays
at its initial value until gc = l, after which time it begins to fall.
gc
is the current generation count (1,2,3, . . . , n).
The nonuniform decay operator keeps genetic diversity relatively high
during the early generations of the search but slowly eliminates mutation
as the search mechanism begins to evolve toward the target solution.
In many cases, a form of decay mutation is switched on by the search
mechanism if it determines that convergence is being inhibited (possibly
by too much mutation).
Advanced Crossover and Mutation Techniques
Breeding approaches that use the conventional crossover and mutation
methods discussed in the previous section will not work for a large family
of problems involving structural, time, and ﬂow dependencies between
genes. Typical examples include the following.
■
Resource-constrained project,
crew,
and machine (job shop)
scheduling
■
Packing and containerization problems in logistics
■
Transportation route scheduling (such as the TSP family of problems)
■
Conﬁguration planning
These problems are encountered frequently in the real worlds of busi-
ness, industry, and government. They all share a common property: the
sequence of genome values represents a collection of discrete objects that

9.3
The Architecture of a Genetic Algorithm
■
407
TABLE 9.18
Invalid Conventional Crossover for the TSP
City Tour
p1
Charlie
Baker
Charlie
Able
Echo
p2
Baker
Able
Echo
Delta
Charlie
c1
Charlie
Baker
Echo
Delta
Charlie
c2
Baker
Able
Charlie
Able
Echo
are being arranged in a particular order. It is the attributes of the objects
that determine the goodness of ﬁt, not the value of the object itself.
For example, returning to the TSP, consider the single-point crossover
operation (at the second locus) shown in Table 9.18.
The crossover operators generate tours with duplicate cities (Charlie
in child 1 and Able in child 2). Conventional mutation operators also gen-
erate duplicate tours (because a tour includes all cities, mutating any of the
ﬁve cities to another city will automatically create a duplicate city in the
tour). To address these problems, a large number of alternate crossover
and mutation techniques have been developed. This section discusses
a few of the more common and easy-to-implement methods, addressing
crossover issues ﬁrst and then mutation techniques.
Greedy Crossover
The greedy crossover approach (also called the nearest-neighbor
crossover) was ﬁrst formalized by Grefenstette in 1985 (see “Further Read-
ing”). The algorithm assembles offspring tours in a small but effective
number of steps.
Let t be the current tour
Let ci be the current city
Select one parent as the base
The ﬁrst city in the parent is chosen as the starting node.
This is the current city (ci)
t = ci
Repeat:
Examine the connecting edges leaving ci in both parents.
Make a list of all cities on the connecting edges
Remove any cities already in the tour

408
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
If the list is empty exit repeat
The edge with the shorter duration is used as the next city in the tour.
Set this next city as ci
t = append(ci)
End repeat
For each unused city (cu)
t = append(cu)
End for each
child = t
The idea of crossover occurs as the algorithm searches for the next seg-
ment in a tour by comparing the length of the next edge in the tour. The
algorithm is repeated for each parent to produce two children. In this
section we examine, for compactness, the generation of one child from
the two candidate parents. Figure 9.33 shows the two tours deﬁned by
the distance measurements in Table 9.4. The numbers on the edges are
the distances between the connected cities.
Using these two tours, the greedy crossover works in the following
steps to produce one of two possible children.
1. Select a parent as the base. In this example, we choose the sec-
ond parent. This starts the template for constructing a child. The
template now appears as follows.
Baker
?
?
?
?
2. Find the edges in both parents from Baker to the next city. These
are (Baker,Able) in parent 2 and (Baker,Charlie) in parent 1.
Choose the edge that has the shortest distance. This edge becomes
the next segment in the tour. The shortest edge is (Baker,Charlie)
in parent 2, with a length of 8 miles. The template now appears as
follows.
Baker
Charlie
?
?
?
3. Find the edges in both parents from Charlie to the next city. These
are (Charlie,Echo) in parent 2 and (Charlie,Able) in parent 1. The
shortest edge is (Charlie,Echo) in parent 2, with a length of 13
miles. This becomes the next edge and the template now appears
as follows.
Baker
Charlie
Echo
?
?

9.3
The Architecture of a Genetic Algorithm
■
409
Charlie
Delta
Delta
Able
Able
Echo
Echo
Baker
Baker
Charlie
12
8
15
9
28
15
10
13
Parent 2
Parent 1
Figure 9.33
Two ﬁve-city tours.
4. Find the edges in both parents from Echo out to the next city.
These are (Echo,Delta ) and (Echo,<terminate>). Thus, Delta is
selected as the next city node, as follows.
Baker
Charlie
Echo
Delta
?
5. Find the edges in both parents from Delta out to the next city.
These are (Delta,Baker) in parent 1 and (Delta,<terminate>) in
parent 2. Baker has already been used in the tour and is removed
from the candidate list. Delta is a terminal city in the parent 2 tour

410
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Able
Baker
Echo
Charlie
Delta
5
8
13
28
Figure 9.34
The child tour from greedy crossover.
and is removed. We now complete the tour by adding Able, the
only unused city, as follows.
Baker
Charlie
Echo
Delta
Able
Figure 9.34 shows the tour created by the greedy crossover, using parent
2 as the starting point (the template basis).
The greedy algorithm has created a valid tour with a length of 54 miles
(8 + 13 + 28 + 5). This is about midway between the lengths of the
incoming parents, which have tour length of 44 and 66, respectively. A
second child is created by selecting the remaining parent (parent 1) as
the base and reapplying the algorithm.
The greedy (or nearest-neighbor) crossover approach produces fea-
sible offspring tours with a minimum of exception processing. This is
a byproduct of its reliance on graph theory to drive the generation of a
tour based on edges. To complete this section on advanced crossover
techniques, we examine two methods that are modeled after the conven-
tional single-point crossover. Both of these generate infeasible solutions
and must employ exception handling to compensate for duplicate and
missing cities.
City Pivot Crossover
The city pivot approach selects a city in the tour at random. This
city becomes the crossover point in the same manner as conventional

9.3
The Architecture of a Genetic Algorithm
■
411
TABLE 9.19
City Pivot Crossover on Baker
City Tour
p1
Delta
Baker
Charlie
Echo
Able
p2
Able
Echo
Delta
Baker
Charlie
c1
Delta
Baker
Able
Echo
Charlie
single-point crossover except that (1) the crossover position is not the
same for both chromosomes but is relative to the location of the city in
each tour and (2) a compression technique must be used to ensure that
duplicate cities do not appear in the offspring chromosomes. Table 9.19
illustrates how a child chromosome is produced from the city pivot
crossover when Baker is selected as the crossover city.
In the crossover at Baker, we have two tour segments sliced by Baker:
(Delta, Baker) in parent 1 and (Able,Echo,Delta) in parent 2. Delta is a
duplicate and is removed from the second segment. When they are spliced
together, the offspring (Delta,Baker, Able,Echo,Charlie) form a valid tour.
Position Pivot Crossover
The position pivot crossover approach, shown in Table 9.20, is very simi-
lar to the conventional single-point crossover discussed previously. In this
method, a crossover position is selected at random along the length of the
tour. The child chromosome consists of the tour in parent 1 on the left
and parent 2 on the right. However, this crossover approach must also
ensure that (1) any duplicate cities (those that appear, for instance, to the
left of the crossover point) are not included in the ﬁnal tour and (2) any
missing cities (those that for example appear to the left of the crossover
but are not included in the left-hand chromosome) are included in the
ﬁnal chromosome.
TABLE 9.20
Position Pivot Crossover on Locus 2
City Tour
p1
Delta
Baker
Charlie
Echo
Able
p2
Able
Echo
Delta
Baker
Charlie
c1
Delta
Baker
Able
Baker
Charlie

412
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
In the crossover at locus 2, we have two tour segments: (Char-
lie,Echo, Able) in parent 1 and (Delta,Baker,Charlie) in parent 2. Delta is
a duplicate and is removed from the second segment. Able is missing from
the offspring when the left-hand set of p1 (Delta,Baker) is crossed with
the right-hand side of p2 (Delta,Baker,Charlie). We now delete Delta and
push Able onto the tour. When they are spliced together, the offspring
(Delta,Baker, Able,Baker,Charlie) form a valid tour.
In the next section the various types of genome mutations are dis-
cussed. For chromosomes in which relationships are deﬁned as connected
edges, these mutation operators change the chromosome structure
instead of changing the value of a single gene.
Random Swap Mutation
In random swap, two loci (chosen at random) have their values swapped.
As illustrated in Table 9.21, this results in a valid tour.
Move-and-Insert Gene Mutation
Using move-and-insert, a genome locus (chosen at random) is moved
before or after another randomly chosen locus in the genome. Table 9.22
shows the offspring when Baker is selected as an insert before a point
and Echo is selected as the city node to move.
TABLE 9.21
The Random Swap Mutation
City Tour
p1 (before)
Delta
Baker
Charlie
Echo
Able
p1 (after)
Delta
Able
Charlie
Echo
Baker
TABLE 9.22
The Move-and-Insert Gene Mutation
City Tour
p1 (before)
Delta
Baker
Charlie
Echo
Able
p1 (after)
Delta
Echo
Baker
Charlie
Able

9.4
Practical Issues in Using a Genetic Algorithm
■
413
TABLE 9.23
The Move-and-insert Gene Mutation
City Tour
p1 (before)
Delta
Baker
Charlie
Echo
Able
p1 (after)
Delta
Charlie
Echo
Able
Baker
TABLE 9.24
The Order Reversal Mutation
City Tour
p1 (before)
Delta
Baker
Charlie
Echo
Able
p1 (after)
Delta
Echo
Charlie
Baker
Able
Move-and-Insert Sequence Mutation
Sequence mutation is very similar to the gene move-and-insert but instead
of a single locus a sequence of loci are moved and inserted. Table 9.23
shows the offspring when Baker is selected as an insert before a point
and the gene sequence (Charlie,Echo, Able) is selected as the set of nodes
to move.
Order Reversal Mutation
With order reversal, a series of the genome loci (chosen at random) have
their values reversed. As illustrated in Table 9.24, this results in a valid
tour. Although treated here as important mechanisms for modifying the
organization of a schedule, conﬁguration, or route, they can also be used
with conventional genetic algorithms (with varying degrees of effect on
the population diversity).
9.4
Practical Issues in Using a Genetic Algorithm
To use a genetic algorithm you must generally have the ability to perform
the following.
■
Generate possible solutions to a problem
■
Set the properties of the genetic algorithm so that it can converge on
a solution

414
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
■
Measure the goodness of those solutions in terms of the outcome from
the underlying model
■
Change the system if the solutions are not very good
The practical issues confronted with analysts in using a genetic algorithm
usually fall within two areas: properly setting the population, breeding,
and mutation properties of the genetic algorithm and ﬁnding ways to
model large, highly complex systems without executing the real-world
model itself. In the next section we address the last two of these issues.
Execution Times for Real-world Systems
In many cases, the execution time is fairly straightforward; that is, the ﬁt-
ness function and the system that processes the potential solution are
essentially the same. Finding the maximum value for a function, the
shortest circuit through a set of cities, the least cost to assemble a piece
of equipment, or the maximum carrying capacity of a road system are
straightforward applications of mathematical models. That is, a genetic
algorithm does not need to depend on dispatching a ﬂeet of vehicles to
follow a potential tour and then actually clocking their time between
cities spread over several states to ﬁnd the shortest route. A road atlas
and a table of inter-city distances from the atlas are sufﬁcient to build
a credible and accurate TSP model. On the other hand, this disconnect
between physical reality and the genetic model does not always exist. In
such cases, a genetic algorithm may be difﬁcult or impossible to use. This
brings us to a set of issues associated with applying genetic algorithms to
complex, large-scale, real-world models.
This disconnect is not universal. Some genetic algorithms are attached
to complex business, industrial, and government policy models. In these
cases, running one generation of 50 chromosomes could take anywhere
from several hours to several days. For example, suppose we want a
genetic algorithm to optimize (that is, minimize) point-of-sales transac-
tion throughput time in a commercial relational database. Unless we have
a reliable mathematical model of how the database performs under spe-
ciﬁc loads with a speciﬁc set of conﬁguration parameters, the genetic
algorithm must run its evaluation against a working relational database.
Our genetic algorithm can generate conﬁguration parameters for the rela-
tional database, feed in a very large collection of transactions, and measure
the throughput time. The same set of transactions is used over and over
to measure the change in processing and total throughput times. Some
combination of virtual-memory-swap area size, disk space, table page size,

9.4
Practical Issues in Using a Genetic Algorithm
■
415
transaction (or job) queue length, number of page buffers, and column
indexing will produce the best database conﬁguration. However, the time
to reconﬁgure a dedicated relational database with a new page size, buffer
count, and different column indexing could take anywhere from four to
ﬁve minutes. Processing all transactions (say a mix of 150,000 query and
update transactions, with a reasonable amount of noise such as bad SKU
numbers, invalid quantities, out of stock responses, and so on) might
take an additional three minutes. Thus, Expression 9.23 is the time per
chromosome.
ct = tt + tr + tp
(9.23)
Here,
ct
is the current chromosome evaluation elapse time
tt
is the tear-down and setup time for each evaluation
tr
is the runtime to evaluate the chromosome
tp
is the inter-generation processing time
As a result, the total time to optimize the database, shown in Expres-
sion 9.24, is the sum of the individual chromosome evaluation times the
number of chromosomes in the population times the total number of
generations used to evolve the solution.
Et ≈N ×
P

i=1
(ct)i
(9.24)
Here,
Et
is the total evolution elapse time
N
is the total number of generations
P
is the number of chromosomes in the population
gt
is generation elapse time (see previous expression)
tr
is the runtime
tp
is the inter-generation processing time
When ct is eight (8) minutes, then for 50 chromosomes a database-tuning
generation will take 400 minutes or 6.6 hours (a bit less than a full work day
of processing). For 20 generations, the total optimization time is 132 hours
(or 5.5 full days). This time frame is not the norm, but it can be typical for
many real-world applications that involve large-scale complex systems.
Setting Process Parameters
Translating the concepts of a genetic algorithm into a working engine
involves not only designing ways to represent the basic data structures

416
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
but ways of setting the principal properties or parameters of the genetic
algorithm. Most commercial or off-the-shelf genetic algorithms provide
default values for these parameters, but a brief review of the typical val-
ues for each major parameter also provides a check list for the control
properties necessary in a genetic algorithm.
Population Size
The initial number of chromosomes in the population depends on the
number of variables in the search. For a “typical” problem of moderate
complexity, a population of 50 chromosomes is often a good starting
point. In many genetic algorithms the population size remains the same
from generation to generation. In others, the population size can expand
or contract depending on the degree of genetic diversity, the rate of
convergence, or other factors in the search process.
Population Generation
There are generally two types of population management techniques in
a genetic algorithm. In the steady-state model, a single population is
constantly updated. In the dynamic-state model, a new population is cre-
ated form the old population of each generation. As a general guideline,
dynamic populations are often easier to maintain and usually provide a
higher intrinsic degree of diversity.
Maximum Number of Generations
One of the primary termination conditions is a limitation on the maximum
number of generations. The default value for this parameter is difﬁcult to
set independently of the number of variables and the number of objective
functions. However, a default of 2.5 times the population size is often a
good maximum generation count estimate.
Type of Crossover
The type of crossover used in breeding depends on the nature of the chro-
mosome; that is, whether it is a binary or real number representation, the
length of the chromosome, the possible number of states that can exist,
and the amount of genetic diversity needed in the search model. A rela-
tively good choice is double-point crossover during the early generations
of the model, converting to single-point crossover in later generations. For
scheduling, conﬁguration, and other dependency problems, the greedy
(or nearest-neighbor) algorithm is almost always the best choice.

9.4
Practical Issues in Using a Genetic Algorithm
■
417
Type of Mutation
The type of permissible mutation depends on the genome representation.
The bit inversion technique is used for binary chromosomes, whereas a
wider range of mutation options is available for real-number representa-
tions. For real numbers the uniform replacement is an excellent default
mutation type (and for scheduling and network or dependency problems,
random swap is a good default mutation technique).
Retention Rate
The retention rate is a percentage of the population and determines how
many of the top-performing chromosomes in a ranked (sorted) population
will be selected and copied into the next generation (or for steady-state
modes which will remain in the existing population). A default value of
10 to 15% is a good estimate.
Breeding Rate
Whether or not a chromosome is selected for breeding (crossover) is
often determined by a probability tied to its ﬁtness relative to all other
ﬁt chromosomes in the population (this is the case with proportional
ﬁtness). In many cases, however, the algorithm selects the ﬁrst 2n + 1
chromosomes in the population and breeds these (subject to the crossover
rate) in order to create the next generation of offspring. The quantity n
is tied to the breeding rate, which is expressed as a percentage of the
population.
Crossover Rate
The crossover rate is a probability that a chromosome will be selected for
breeding. This is used when the search algorithm uses a breeding rate to
pick chromosomes for crossover. A default value between [.5] and [.9]
(say .66) is a good default estimate for the crossover rate. In some genetic
searches, the crossover rate can begin low and increase if the average
ﬁtness of the population does not signiﬁcantly improve over a speciﬁed
number of generations.
Mutation Rate
The mutation rate determines the probability that a chromosome will
have one of its genes changed through a mutation technique. In general,
mutation rates should be very low, in order to sustain genetic diversity

418
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
but not overwhelm the population with too much noise. Expression 9.25
shows a good default mutation probability rate.
mr = max

.01, 1
N

(9.25)
Here,
mr
is the current probability of mutation
N
is the population size
The mutation rate is inversely proportional to the population size, but
not less than .001 is a good default value. For a population of 125
chromosomes, this is max(.01,.008), or [.01].
New Individual Rate
In some genetic algorithms new individuals are introduced into the next
generation. These individuals have randomly valued genes (created in the
same way as the genetic algorithm’s initial population). New individuals
can signiﬁcantly increase genetic diversity but can also have an adverse
effect on convergence if overused. As a rule of thumb, if new individu-
als are introduced into the population the probability should be half the
mutation rate (.5*mr).
9.5
Review
Genetic algorithms form a family of directed optimization and search tech-
niques that can solve highly complex and often highly nonlinear problems.
They can be used to explore very large problem spaces and ﬁnd the best
solution based on multiobjective functions under a collection of multi-
ple constraints. In this chapter we examined the fundamental nature of
genetic algorithms, how they work, and how they evolve or breed solu-
tions to problems. You should now understand the principal nature of
the genetic algorithm and be familiar with such concepts and ideas as the
following.
■
The types of problems solved by genetic algorithms
■
The organization and ﬂow of control in a genetic algorithm
■
How a problem solution is encoded in a chromosome
■
The design and use of a ﬁtness function

Further Reading
■
419
■
How to introduce and limit diversity in a population
■
How to measure and control convergence in a population
■
How to select the correct crossover and mutation types and rates
■
How to set the process parameters for a genetic algorithm
■
The strengths and weaknesses of genetic algorithms
Genetic algorithms play an important role in tuning, optimizing, and
measuring the performance of adaptive models. They can be instrumental
in evolving parameters that keep models responsive to many external
stresses. In this part of the book we continued our exploration of genetic
algorithms and evolutionary strategies. In the next chapter we examine
a simpliﬁed (but still rather complex) crew-scheduling system. This is
followed by a chapter on the genetic tuning of fuzzy models and the
genetic discovery of linear and nonlinear regression equations.
Further Reading
■
Goldberg, D. E. “A Note on Boltzmann Tournament Selection for Genetic Algo-
rithms and Population-Oriented Simulated Annealing,” in Complex Systems,
Volume 4, pp. 445–460, Complex Systems Publications, 1990.
■
Goldberg, D. E. Genetic Algorithms in Search, Optimization, and Machine
Learning. Reading, MA: Addison-Wesley, 1989.
■
Goldberg, D. E. “Real-coded Genetic Algorithms, Virtual Alphabets, and Block-
ing,” in Complex Systems, Volume 5, pp. 139–167, Complex Systems
Publications, 1991.
■
Goldberg, D. E., and K. Deb. “A Comparative Analysis of Selection Schemes
Used in Genetic Algorithms,” in Foundations of Genetic Algorithms, G. J. E.
Rawlins (ed.), pp. 69–93, San Mateo, CA: Morgan Kaufmann Publishers,
1991.
■
Goldberg, D. E., and J. Richardson. “Genetic Algorithms with Sharing for
Multimodal Function Optimization,” in Genetic Algorithms and Their Appli-
cations: Proceedings of the Second International Conference on Genetic
Algorithms, pp. 41–49, 1987.
■
Goldberg, D. E., K. Deb, and J. H. Clark. “Genetic Algorithms, Noise, and the Siz-
ing of Populations,” in Complex Systems, Volume 6, pp. 333–362, Complex
Systems Publications, 1992.
■
Grefenstette, J., R. Gopal, R. Rosmaita, and D. Gucht. “Genetic Algorithms for the
Traveling Salesman Problem,” in Proceedings of the Second International

420
■
Chapter 9
Fundamental Concepts of Genetic Algorithms
Conference on Genetic Algorithms,
Mahwah,
NJ: Lawrence Erlbaum
Associates, 1985.
■
Holland, J. H. Adaptation in Natural and Artiﬁcial Systems: An Introductory
Analysis with Applications to Biology, Control, and Artiﬁcial Intelligence,
1975.
■
Krishnakumar, K. “Micro-Genetic Algorithms for Stationary and Non-Stationary
Function Optimization,” SPIE: Intelligent Control and Adaptive Systems,
vol. 1196, Philadelphia, PA, 1989.
■
Syswerda, G. “Uniform Crossover in Genetic Algorithms,” in Proceedings of the
Third International Conference on Genetic Algorithms, J. Schaffer (ed.),
pp. 2–9, Los Altos, CA: Morgan Kaufmann Publishers, 1989.

■■■
Chapter 10
Genetic Resource
Scheduling Optimization
In Chapter 9 we explored the nature, organization, and function of GAs.
These search and optimization mechanisms not only play an important
part in effectively deriving fuzzy models but provide advanced optimiza-
tion, exploration, and analysis features that play an important role in many
applications in such varied ﬁelds as logistics, transportation, ﬁnancial ser-
vices, retailing, manufacturing, and project management. In this chapter
we focus on solidifying our understanding of how GAs work. We do this
by examining a small but nevertheless complex crew-scheduling system.
Crew scheduling brings together the multiple objective and multiple con-
straint optimization capabilities of GAs in a way that illustrates how a
GA connects to and works with the underlying application (in this case, a
resource-constrained scheduling system). We begin our study of the genetic
scheduler by examining the architecture of a fully robust scheduler and
then explore a real scheduler with a small number of constraints.
10.1
The Vocabulary of Resource-constrained
Scheduling
Most of the underlying terminology associated with genetic scheduling
should already be familiar from the previous chapter on GAs. A review
of some vocabulary terms that are used in ordinary resource-constrained
scheduling, however, will make reading and understanding the material
in this chapter somewhat easier. It is not always possible, while maintain-
ing an uncluttered and coherent discussion of the clustering process, to
ensure that every term is introduced before it is used. This preview of
vocabulary is primarily concerned with the crew scheduler discussed in
this chapter. For some exceptions to or elaboration on these deﬁnitions
see “Some Terminology Issues” following the deﬁnitions.
421

422
■
Chapter 10
Genetic Resource Scheduling Optimization
Availability
Availability speciﬁes the amount of resource supply available per unit
time. For example, an individual could have available eight hours per day
for work. A crew, regardless of the number of individuals in the crew,
also has for the purpose of scheduling eight hours per day. Availability is
a hard constraint on producing a feasible schedule.
Calendar
A calendar is a component of the scheduler that indicates the distribution
of a resource’s availability. A calendar of a crew or an individual indicates
which days of the year are working days and which hours of the day are
working hours. Each individual resource has its own calendar, allowing
speciﬁcation of different work days, holidays, working cycles, and so
forth.
Crew
A crew is a composite resource used in scheduling. A crew is the unit of
resource availability that is assigned to a job. A crew normally has its own
availability, but can also have an availability that is the composite of its
underlying members (although we do not consider this advanced type of
resource constraint in this chapter).
Duration
The duration is the elapse time of a task (or job) speciﬁed in the units of
the associated resource (or expressed in some multiple of the resource).
Hence, jobs are speciﬁed in duration times of minutes, whereas crews
are speciﬁed with an availability in hours per day. The combination
of duration and availability created the underlying supply-and-demand
model, which is the basic mechanism that drives all resource-constrained
schedulers.
Early Start
An early start is the earliest possible start for a job. This time is assigned
by the crew dispatcher and is the time the crew is scheduled to be on
site. In real-world schedules this is the time the service department has

10.1
The Vocabulary of Resource-constrained Scheduling
■
423
promised the customer that the crews (or customer service engineer) will
be at the customer’s site.
Execution Window
Each job has an execution window. This is the time between the early
start and the latest ﬁnish date. The execution window implicitly indicates
the amount of elasticity in the job’s start.
Feasible Schedule
For each job, a feasible schedule (or solution) has (1) a crew with the
required skills, (2) a time frame that corresponds to a time when the crew
is available, (3) all the necessary supporting resources, and (4) a start time
that is after all its predecessor jobs have been completed. Feasibility is the
highest integrity constraint on a schedule. Every potential schedule must
be a feasible solution.
Float
The job ﬂoat is the amount of surplus time in the execution window.
This is the length of the execution window minus the length of the
job. Thus, ﬂoat is the number of days the job can be delayed without
exceeding the latest ﬁnish date.
Job
A demand on crew time is a job (or a service order in the argot of crew
scheduling). In crew scheduling, jobs have several important properties:
an initially ﬁxed window in time, an estimated duration, a set of required
skills, a priority, and a planned or expected start and ﬁnish time. The core
idea behind crew scheduling is to assign, from a pool of many possible
crews, a crew to each job that produces a feasible schedule with the
minimum possible span of time.
Latest Finish
Latest ﬁnish is the latest possible completion time for a job. This can
be either a hard or a soft constraint. When the latest ﬁnish is a soft
constraint, the date is relaxed and the job is scheduled to complete

424
■
Chapter 10
Genetic Resource Scheduling Optimization
regardless of the planned ﬁnish date. When the latest ﬁnish is a hard
constraint and a job is scheduled fully or partially outside its execution
window, the job must remain unscheduled. Although we do not deal
with unscheduled jobs in this chapter (in that [in that?]the latest ﬁnish is
a soft constraint), the ﬁtness function assigns a very large penalty value
to unscheduled jobs.
Network
The complete collection of precedence-connected jobs is called the net-
work. This is a term drawn from graph theory that speciﬁes a collection
of nodes connected by edges.
Precedence Relationship
The time dependency relationship between two nodes in a network (such
as two or more jobs) is called a precedence relationship. If job B cannot
start until job A completes, a precedence relationship exists between the
two jobs. In this case, job A is the predecessor to job B and job B is the
successor to job A.
Precedence Strategy
The way in which precedence relationships connect two nodes (such a
jobs) is the dependency or precedence strategy. The normal strategy is
ﬁnish to start (job B cannot start until job A is ﬁnished). But there are
many other strategies, such as ﬁnish to ﬁnish, start to start, and so forth.
Precedence strategies also involve lead and lag weights that specify lead
or lag time delays between jobs.
Renewable and Nonrenewable Resources
A resource that, once consumed, is no longer available is called a nonre-
newable resource. An allocated budget is nonrenewable. On a daily basis,
crews are nonrenewable (one a crew’s time is consumed for the day, no
more crew time is available), although on a day-by-day basis they repre-
sent a renewable resource (the next day the full crew time is available for
assignment).

10.2
Some Terminology Issues
■
425
Resource
A source of supply is called a resource. In general, when we want
to construct a feasible schedule that can be implemented in the real
world each task in the schedule must be associated with some type of
resource.
Span Time
The total elapsed time for a schedule from the start of the earliest task to
the completion of the latest task is known as the span time. The objective
function of the crew schedule is designed to minimize span time.
Supply-and-Demand Model
A model that is constrained by resource availability is, in general terms,
known as a supply-and-demand model. Supply-and-demand models have
long been known as a regular feature in economics (where they are
subject to many forms of dynamic equilibrium analysis). All scheduling
systems (crew schedulers, project management systems, machine shop
schedulers) are forms of supply-and-demand models.
10.2
Some Terminology Issues
Crew scheduling and general project management are related disciplines
but with different meanings associated with some of their terms. These dif-
ferences arise primarily in crew-scheduling plans (such as that discussed
in this chapter) that do not involve precedence relationships. In the anal-
ysis of a project schedule, the project is given a start date and each task
is assigned a duration. From these two values (resource availability and
precedence relationships) the network analysis computes the early and
late start and ﬁnish dates, as well as the amount of ﬂoat for each task. In
the crew schedule, each task (or job) is assigned an early start date and
latest allowed ﬁnish date. The difference between these two dates less
the actual duration of the job is the amount of ﬂoat in the job. Many real-
world crew-scheduling plans are constructed in this fashion (most major
electric and water utilities and cable television companies schedule their
repair and service crews in this fashion).

426
■
Chapter 10
Genetic Resource Scheduling Optimization
10.3
Fundamentals
The genetic-based crew scheduler ﬁnds an optimal solution to the prob-
lem of assigning crews to outstanding jobs (service orders) consistent
with crew availability and the required completion dates of each job. The
task is complicated by both the constraints imposed on a feasible solution
and the combinatorial complexity of even a simple crew-scheduling prob-
lem. We start our exploration of crew scheduling with the very simple
example illustrated in Figure 10.1. We have a problem of three crews and
ﬁve jobs.
If a job can be handled by any crew, there are 35 or 243 possible
schedules (combinations), and each of these schedules, lacking any addi-
tional constraints, would be just as good as any other. This perspective
constitutes the assignment problem in crew scheduling.
Job 1
Job 2
Job 3
Job 4
Job 5
Crew 1
Crew 2
Crew 3
Figure 10.1
A 3 × 5 crew-scheduling problem.

10.3
Fundamentals
■
427
The Crew-scheduling Problem
So, if all choices of crew assignments are equally good, why is there a prob-
lem? There is a problem because real-world scheduling involves more than
parceling out work to a set of waiting crews. Even if the crews are inter-
changeable in terms of skills, some crews may have properties that make
them a better choice for a particular assignment to a particular job at a
particular time. If, for example, crews have varying availabilities or crews
must contend for limited and shared equipment assets (or if we take the
locality of a crew’s preceding job into account, or take the size or experi-
ence of a crew’s members into account, thus accounting for risk factors),
the complexity of the assignment problem is increased signiﬁcantly.
Feasible Solutions: An Example
Let’s examine the way this crew-scheduling problem can be resolved. Our
objective is to assign a job to a crew and then schedule each job so that it
is completed within as few days as possible. First, we need to know the
time required by each job. Table 10.1 shows the duration, in hours, for
each of the jobs.
The simplest way of solving this problem is to start with the ﬁrst crew
and begin making assignments, starting with the ﬁrst job. If the daily
availability of a crew is eight hours, Figure 10.2 shows a solution to this
problem.
This is a feasible schedule (that is, a workable schedule that does
not violate any constraints and meets our basic objective of completing
all work on the ﬁrst day). If the schedule is satisfactory, our work is
complete and we can stop. On the other hand, we might have imposed
a few other objectives for our schedule: that each job is completed as
soon as possible within the day, that travel time between jobs is mini-
mized, and that whenever possible all available crews are used. A simple
rearrangement of jobs can help us meet our ﬁrst objective (minimizing
TABLE 10.1
Job Duration Times
Job
Duration (Hours)
JOB1
3.0
JOB2
4.0
JOB3
2.0
JOB4
3.5
JOB5
2.0
Total
14.5

428
■
Chapter 10
Genetic Resource Scheduling Optimization
Job 3
Job 4
Job 5
Job 1
Job 2
2 Hrs
3.5 Hrs
2 Hrs
3 Hrs
4 Hrs
Day 1
(8 hours)
Crew 3
Crew 2
Crew 1
Figure 10.2
A feasible crew schedule.
job completion times). Figure 10.3 shows a schedule in which completion
times are minimized relative to the initial schedule.
A particular crew-scheduling problem often has several equally feasible
(and occasionally equally optimal) solutions. For example, Figure 10.4
illustrates another feasible solution to the assignment of these ﬁve jobs
across three crews.
This is the nature of crew scheduling: assigning jobs to crews so that
each job is completed consistent with any constraints (see the section
“Objective Functions and Constraints”). As the number of jobs increases
and the number of crews increases, the assignment process becomes
increasingly difﬁcult. Finding a solution that meets the minimum require-
ments (all work is done within its allowable time frame) as well as respects
all constraints on the schedule (such as minimizing the completion time or
assigning crews to minimize travel time) can be a computationally inten-
sive task. This computational burden evolves not just from the number
of jobs and crews in the schedule but from the objectives and constraints
placed on the schedule.
10.4
Objective Functions and Constraints
Objective functions (or simply, objectives) and constraints are developed
as part of the problem speciﬁcation. Constraints deﬁne the feasibility

10.4
Objective Functions and Constraints
■
429
Job 4
3.5 Hrs
Job 2
4 Hrs
Job 3
2 Hrs
Job 5
2 Hrs
Job 1
3 Hrs
Day 1
(8 hours)
Crew 3
Crew 2
Crew 1
Figure 10.3
A schedule with minimized completion times.
Job 1
3 Hrs
Job 4
3.5 Hrs
Job 2
4 Hrs
Job 5
2 Hrs
Job 3
2 Hrs
Day 1
(8 hours)
Crew 3
Crew 2
Crew 1
Figure 10.4
An alternative feasible solution.

430
■
Chapter 10
Genetic Resource Scheduling Optimization
of a schedule. Objectives deﬁne the optimality of a schedule. Although
objectives should be satisﬁed, constraints must be satisﬁed. We now turn
to a discussion of these issues.
Constraints
Returning to the simple crew schedule shown in Figure 10.1, because
there are a few hundred possible schedules and our only criterion is that
each job must be paired with an available crew, we can select any of these
as our working schedule. In the real world of crew scheduling, however,
the choice among feasible, acceptable, and optimal schedules1 is nar-
rowed considerably. This narrowing process is driven by the constraints
imposed by both the schedule user and the schedule developer (human
or machine). These constraints are in the form of objective functions
and solution constraints. Constraints impose restrictions on the overall
nature of the schedule and may be in conﬂict with one or more objective
functions.
Thus, in real-world crew scheduling we have restricted degrees of
freedom in constructing a workable or feasible schedule. Although these
objective functions and constraints may reduce the total possible number
of schedules, ﬁnding a feasible (not to mention optimal) schedule in the
restricted space can be a very difﬁcult and time-consuming process. To
list just a few constraints, a feasible solution must match a job’s
■
Skill and asset requirements
■
Degree of difﬁculty
■
Work site (customer identiﬁcation)
■
Geographic location relative to related jobs
■
Predecessor relationship with other jobs
■
Customer priority
■
Estimated duration (time demand)
■
Earliest possible start time
■
Latest permissible ﬁnish time
1 A feasible schedule can be actually implemented. It does not violate start date constraints
(the start must be after or on today’s date), resource availability, job precedence relationships,
or crew-to-job skill requirements. It may violate objective functions or the latest permissible
ﬁnish date of a job. An acceptable schedule is both feasible and close to optimal (a “very good”
schedule). An optimal schedule is one in which no other schedule is objectively (but sometimes
subjectively) better. There may be more than one optimal schedule.

10.4
Objective Functions and Constraints
■
431
and a crew’s
■
Skill proﬁciencies
■
Job performance history
■
Available (remaining) time
■
Charge rate
■
Renewable and nonrenewable asset rates
■
Access to limited asset resources
■
Inter-job travel time
■
Geographic work restrictions
These factors establish constraints on the form of a ﬁnal schedule. They
are an intrinsic part of the relationships among crews, assets, and jobs.
As such, they are implicit in the nature of any schedule and form the
kernel constraint criteria for any feasible solution. A few of the constraints
implied by even this relatively small list of job and crew properties include
the following.
■
The matching of a job’s required skills and the skill proﬁciencies of a
crew means that jobs cannot be randomly assigned to crews but must
be assigned to crews that have the proper skills.
■
Some jobs are inherently more difﬁcult than others, and some crews
have greater or lesser proﬁciencies in particular skills. This implies
a relationship between crew assignments and risk of meeting a com-
pletion date. Thus, high-priority, high-visibility, or rescheduled jobs
should be assigned to crews with the highest possible proﬁciency in
the required skills.
■
A crew’s daily availability coupled with a job’s estimated duration
(earliest start date and its latest permissible ﬁnish date) means that a
crew can only work on a certain number of jobs each day (that is, the
total estimated time for all jobs cannot exceed the daily availability of
the crew).
■
A crew’s geographic work restriction. In a statewide crew schedule,
for example, crews might be restricted to particular counties. In any
case, some crews have restrictions on their general work areas. These
restrictions are on the same constraint level as the crew’s skill and
overall availability.
■
The travel time between jobs means that the total daily availability of
a crew is not actually available to work on a set of candidate jobs. This

432
■
Chapter 10
Genetic Resource Scheduling Optimization
“dead time” must be factored into the schedule. And, of course, some
crews must share critically limited assets, and thus the availability of
the asset must be considered when scheduling the crew.
■
The fact that some jobs are dependent on the start or completion of
other jobs means that we must consider the precedence relationships
between jobs in devising a schedule. It also means that if a scheduled
completion date of a job exceeds its latest permissible date the start
or allowed ﬁnish dates of successor jobs must be adjusted.
Not all constraints implicit in the properties of crews, jobs, and assets
are implemented in a scheduling system, but the constraints we chose
will impose a limitation on the number of feasible solutions that can be
constructed. Thus, as previously mentioned, all constraints imposed on a
schedule must be obeyed.
As a matter of fundamental principles in scheduling there are, gen-
erally, only two types of constraints that must always be considered.
The ﬁrst is the predecessor and successor relationships between jobs.
Satisfying this constraint produces a time-feasible schedule. The sec-
ond is the availability of resources to actually perform the work (crews,
money, tools, machines, and so on). Satisfying this constraint produces
a resource-feasible schedule. Because jobs must be properly sequenced
in time in order to determine resource availability, a resource-feasible
schedule is always a time-feasible schedule. The purpose behind a
resource-constrained scheduler is to produce a set of resource-feasible
schedules.
Objectives
The set of feasible solutions satisﬁes all constraints imposed on a valid
schedule. An optimal schedule, often thought of as the best schedule,
not only satisﬁes all the constraints but is at least as good as any other
feasible solution. This quality of goodness is deﬁned by the objective func-
tions. Objective functions characterize the critical nature of the schedule
itself — an attempt to minimize or maximize a measure of goodness in the
schedule. Some of the objective functions include the following.
■
Minimize the scheduled completion time of each job
■
Maximize the number of jobs done in a particular time period
■
Minimize overall costs
■
Maximize resource (crew) utilization

10.4
Objective Functions and Constraints
■
433
■
Minimize risk
■
Minimize slippage
■
Maximize crew efﬁciency
■
Minimize travel time
Crew scheduling typically minimizes the scheduled completion times
for each job as its primary objective. A schedule, however, can have
multiple objective functions. These objective functions are often in con-
ﬂict with each other. We can, for example, minimize the cost of jobs
by assigning less experienced crews or scheduling jobs when less expen-
sive resources become available. However, this might conﬂict with the
completion date minimization or risk minimization objective functions.
Evaluating Objective Functions
In a multiobjective function schedule, the optimal solution often repre-
sents a saddle (or compromise) point in the solution space. Optimization
in the presence of multiobjective functions usually means ﬁnding a way to
rank the utility or importance of each objective and then ﬁnding a way
to judge the optimality of a feasible solution based on the goodness of ﬁt
or rank of their optimality measures. As a rather simple example, if each
objective function ( f ) returns a value between 0 and 100 indicating how
close the schedule is to the optimal (0 = best, 100 = worst), we can ﬁnd
an optimality ranking for a multiobjective function schedule by evaluating
franked =
N
i=1 fi × wi
N
i=1 wi
,
(10.1)
where
franked
is the ranked ﬁtness from all individual ﬁtness functions
fi
is the ﬁtness of the i-th objective function
N
is the number of objective functions in an individual
chromosome
wi
is the contribution weight for the i-th objective function
By selecting schedules with the smallest weighted average objective func-
tion, the feasible schedules with the closest optimality ﬁt will continually
percolate to the top. This form of evaluating a collection of objective func-
tions makes it easy to combine both minimizing and maximizing objective
functions in the same analysis (maximizing functions simply return the
inverse of the ﬁtness value).

434
■
Chapter 10
Genetic Resource Scheduling Optimization
10.5
Bringing It All Together: Constraint
Scheduling
There are a wide variety of possible and actual constraint factors in a
real crew schedule. These constraints, as we saw earlier, affect the solu-
tion space of a feasible schedule. There are, however, three principal
constraints that must be satisﬁed before any schedule is considered for
other constraints. These involve matching jobs to allowable crews and
then accounting for the availability of crews and other related resources.
In this section we will look at these two issues and see how they affect
the schedule. Allowable crew determination involves two restrictions: a
crew’s geographic range and a crew’s set of skills.
Operating Area-constrained Crew Assignments
If we consider the crew-scheduling process to encompass a set of widely
dispersed crews that operate in restricted geographic regions,2 the naive
schedules in Figures 10.2 through 10.4 fail to account for this feature of
a feasible schedule. We can plan for the matching of crews to jobs with
a site restriction by visualizing customer sites placed on a very ﬁne grid.
Figure 10.5 illustrates this concept.
Our crews then have restricted operating limits. These limits are often
placed on political as well as economic considerations (some examples
include: road crews are restricted to a particular county, crews can-
not travel more than X miles from their home base, some crews can
only service residential customers and others only business customers).
As Figure 10.6 illustrates, we have a set of crews that operate within a
particular physical or logical territory.
In many cases the grid contains the degree to which a crew can operate
within the territory or adjacent territories. If the partitioning into territo-
ries is hard, the degree will be zero or one. If the partitioning is relatively
soft, the degree along the edges of territories will be a number in the
interval [0,1]. This acts as a form of topology or hurdle function, ensuring
that a crew from one territory is only scheduled into another territory
when no other satisfactory solution is available.
2 We use geographic constraints as a common example. However, this type of operating con-
straint could just as easily be based on similar restrictions, such as sales territory (customer
assignments), seasonal or shift constraints, department or organizational function, and so on.
The territory grid (or grids), of course, need not be rectangular. Any arbitrarily nonuniform shape
can be used.

10.5
Bringing It All Together: Constraint Scheduling
■
435
0
1
2
3
4
5
6
7
8
9
20
20
.    .    .
.    .    .
0
1
2
3
4
5
6
7
8
9
Figure 10.5
Customer sites on the location grid.
Skill-constrained Crew Assignments
The schedules illustrated in Figures 10.2 through 10.4 also omit a critical
constraint on the feasibility of a schedule: the matching of jobs with the
correct crew. This correctness is a function of the skill set required by
the job and the skill set of one or more crews. This means that the ﬁrst
constraint in producing a schedule, as illustrated in Figure 10.7, is a match
between a job and the crews that have the skills to perform that job.
Thus, the ﬁrst step in formulating a feasible solution is the matching
of jobs to the crews with the correct skills. This matching process is
complicated by several considerations.
■
Crews can have multiple skills.
They can also have varying
degrees of proﬁciencies in each of their skills. For example, a
telecommunications-placing crew might have skills in both copper
and optical ﬁber land lines. However, due to the composition of the
crew it has a high degree of proﬁciency (or experience) in copper
lines and a low degree of proﬁciency in optical ﬁber lines.

436
■
Chapter 10
Genetic Resource Scheduling Optimization
0
1
2
3
4
5
6
7
8
9 . . .
20
0
1
2
3
4
5
6
7
8
9...
20
Crew 2
Crew 1
Crew 4
Crew 3
Figure 10.6
Territory crew restrictions.
■
Jobs can also have multiple skill requirements. A single job might
require, for example, expertise in electrical work, in construction,
and in ground excavation work.
This multiple-skills-to-multiple-skills relationship means that we have
two distinct options in satisfying the crew-to-job constraint. To illustrate
these options, consider a job that requires welding and electrical skills.
A schedule generator can perform the following.
■
Find a single crew that has both Welding and Electrical in its skill set.
In this case, the assignment problem is reduced to the ordinary crew
assignment.
■
Find a collection of crews that has Welding and Electrical in their skill
sets. In this case, each crew is assigned the same job. The sched-
uler must then ensure that both crews are scheduled together so
that they are at the job site at the same time to complete the task

10.5
Bringing It All Together: Constraint Scheduling
■
437
Job 1
(welding)
Job 2
(electrical)
Job 3
(electrical)
Job 4
(paving)
Job 5
(welding)
Job 6
(electrical)
Job 7
(electrical)
Job 8
(welding)
Crew 1
Paving
Crew 2
Welding
Crew 3
Electrical
Figure 10.7
Aligning crews and jobs based on skills.
(or are scheduled according to the permissible predecessor relation-
ships associated with the underlying work).
The process of merging crews and jobs with multiple skills is further
complicated by a need to consider the proﬁciency or experience of a
crew relative to the complexity, priority, duration, location, or customer
satisfaction criteria associated with the job. This type of extended match-
ing between jobs and crews is critical under certain objectives (such as
minimizing risk or minimizing slippage).

438
■
Chapter 10
Genetic Resource Scheduling Optimization
Resource-constrained Scheduling
A pervasive and persistent constraint in real-world scheduling is a respect
for resource availability. The basic resource in a crew-scheduling prob-
lem is the crew. However, crews can be composed of individuals (with
their own available times, rates, skills, and proﬁciencies). A crew can
also be dependent on other scarce or costly nonrenewable and renew-
able resources (collectively known as assets). In matters of generating a
feasible solution, the complications arising from limited resource avail-
ability combined with the cost, quantity on hand, and the renewable or
nonrenewable properties of a resource must be taken into consideration.
Nonrenewable and ﬁxed-quantity resources are depleted as they are used
or distributed. Ordinary nonrenewable resources include the following.
■
Supplies
■
Raw materials
■
Capital
Fixed-quantity resources include assets such as shared machinery. For
example, a long-lines placing or splicing crew may contend with other
crews for a limited number of cherry pickers. A feasible schedule depends
not only on the availability of the crew but on the availability of the
cherry pickers. Such nonrenewable resources often have a rate as well
as a penalty for use. Renewable resources are generally refreshed in each
time period. Crew availability or equipment availability is an example of
a renewable resource.3 When we combine resource constraints with the
job pools, as illustrated in Figure 10.8, the result is a resource-feasible
schedule that respects crew assignments (based on skills).
Let’s see how this works in practice. At its simplest level, crew
scheduling must respect the daily availability of each crew. If we assume
that each crew is available eight hours a day (ignoring break and lunch
times), Figure 10.9 illustrates a feasible schedule that considers resource
constraints.
Crew 1 (paving) has a single job during day 1. Crew 2 (welding) is
scheduled to work on jobs 1, 3, and 5 on day 1. Although two hours time
3 A renewable resource may have a renewable pattern that is (1) not aligned with the period
availability of the crews and (2) not regular. For example, there are N special band saws available
to X carpenter crews. Crew Xi checks out a band saw and keeps it for M days (knowing that
they will need it for three upcoming job assignments). Thus, crew Xi has exclusive use of this
resource (it has a periodic renewable behavior as perceived by crew Xi) while it is unavailable
to all the other crews during this period.

10.5
Bringing It All Together: Constraint Scheduling
■
439
Time
supply
Calendar
Resource availability
QOH
Time
supply
Calendar
Resource availability
Time
supply
Calendar
Resource availability
QOH
Time
supply
Calendar
Resource availability
QOH
Raw material inventories
Budgets
Crew
Time
supply
Calendar
Resource availability
Crew
Time
supply
Calendar
Resource availability
Crew
Job 2
Job 6
Job 10
Job 9
Job 11
Job 1
Job 7
Job 5
Job 4
Job 3
Job 8
Crews
Time periods
Renewable and nonrenewable resources (assets)
Figure 10.8
Factors in resource-constrained scheduling.
Job 1
3 Hrs
Job 8
4 Hrs
Job 3
2 Hrs
Job 5
2 Hrs
Job 2
4 Hrs
Job 6
4 Hrs
Job 1
3 Hrs
Job 4
3.5 Hrs
Day 1
(8 hours)
Crew 3
Crew 2
Crew 1
Day 2
(8 hours)
Figure 10.9
A resource-feasible schedule.
remains on day 1, job 8 must be moved to the next day because it requires
eight hours of crew time. Crew 3 (electrical) is scheduled to work on job
2 during day 1. But the travel time to both job 6 and job 3 is two hours,
and thus neither can be scheduled on day 1 and must be moved to day 2.
Unlike our simple examples in the overview section, we are unable to
move jobs delayed by resource constraints to other crews because they
are matched to crews by their skill requirements.

440
■
Chapter 10
Genetic Resource Scheduling Optimization
10.6
A Genetic Crew Scheduler Architecture
The resource-constrained crew scheduler fuses a master plan of jobs,
crews, skills, and other assets into an optimal schedule. The schedule gen-
erator combines a GA (which searches for the optimum schedule) with
a precedence manager to manage the ordering of jobs and a ﬁne-detail
resource allocation manager to apply availability constraints to crews, as
well as renewable and nonrenewable resources. Figure 10.10 illustrates
the organization of the scheduler.
Creating a schedule using the crew scheduler begins with the deﬁni-
tion of a master plan: the statement of the problem in terms of its elements.
Closely related and tightly coupled, the planning and scheduling phases
Calendar
Crews
Resource-constrained
scheduler
Genetic optimizer
Crew scheduler
Schedule of jobs
with crews
Audit file
Job sequencer
(optional)
Precedence analyzer
Jobs
Skills
Assets
Topology
2
3
4
5
6
1
Schedule of jobs
with crews
7
8
Figure 10.10
The crew scheduler schematic organization.

10.6
A Genetic Crew Scheduler Architecture
■
441
of generating a crew schedule are distinctly different but complementary
activities. Planning involves the creation of a task/crew process model as
well as the deﬁnition of constraints and global objective functions. This
process model (or master plan) is either entered into the system from a
client’s application framework or is produced through the master plan
compiler. Scheduling, on the other hand, is the ordering of tasks accord-
ing to their precedence relationships and the assignment of resources to
activities at speciﬁc time frames.
➊The Master Plan
The master plan consists of many different elements, which together
deﬁne the extent as well as the content of the schedule. These elements
consist of the following objects in the plan.
■
Crews: An entity that is responsible for performing work. A crew can
also be decomposed into individuals who have the same properties
as the crew. Crew properties include a period of availability, a set of
skills with proﬁciencies and experiences, and regular and exceptional
charge rates.
■
Calendars: A scheduling entity (created by the calendar compiler)
that deﬁnes a working period (such as a day), working slots (such as a
range of hours in a day), and a set of patterns or exclusions that deﬁnes
dates and times that are unavailable. Each crew (or individual within
a crew) as well as certain types of limited resources has an assigned
calendar. The availability of a crew, individual, or asset is mapped to
the period of its associated calendar.
■
Skills: A category of work or type of service that is either performed
by a crew or is required by a job.
■
Assets: Resources that are used by crews to perform their general work
or that are needed for speciﬁc jobs (and may be speciﬁed by the job).
An asset is either limited or unlimited and can be renewable or nonre-
newable. Limited asset resources are subject to resource constraints
and must be considered in scheduling. Limited assets also have their
own calendar to deﬁne their available supply over time.
■
Job: An entity that deﬁnes a piece of work that must be performed.
Jobs are also called service orders. Jobs have properties that include
a time demand (duration), an earliest possible start, latest or target
ﬁnish, a set of possible predecessor jobs, a relative priority, a set of
required skills, and any special tolls or other special assets.

442
■
Chapter 10
Genetic Resource Scheduling Optimization
■
Topology:
Deﬁnes the network of predecessor and successor
jobs in the master schedule.
The topology network is actually
constructed from the predecessor relationships deﬁned in the master
plan jobs.
➋The Job Sequencer
The ﬁrst step in sense preparing a crew schedule is the job sequencer.
This facility is used to order jobs that have predecessor relationships.
The sequencer works in a fashion similar to a critical path analysis in
traditional project management: by using the earliest permissible start
date in a chain of jobs and by applying the duration of each job, it orders
the jobs across time and thus provides an initial time-feasible schedule
(but not yet resource feasible).
➌The Crew Scheduler
The crew scheduler is the top-level controller for the scheduling frame-
work. The crew scheduler manages the various elements in the schedule,
monitors the convergence of the GA toward an optimal solution, and noti-
ﬁes the application framework when the ﬁnal schedule is available. The
crew scheduler also manages all constraint conditions and objective func-
tions for the current schedule process. The crew scheduler is responsible
for decomposing large multidimensional schedules into linearly separable
pieces for stepwise resolution.
➍The Genetic Optimizer
The genetic optimizer solves the underlying multiconstraint, multiob-
jective schedule through the parallel exploration of and convergence
through a large n-dimensional space of candidate schedules. The genetic
optimizer explores this space by creating a random set of crew-feasible and
time-feasible schedules. These are then sent to the resource-constrained
scheduler, where they are turned into working, resource-feasible sched-
ules. Each feasible schedule is measured against the objective functions
and ranked according to their particular goodness of ﬁt. The optimizer
continues to breed better and better schedules from the ﬁttest schedules
found in each analysis cycle.

10.6
A Genetic Crew Scheduler Architecture
■
443
➎The Resource-constrained Scheduler
The resource-constrained scheduler generates a complete feasible sched-
ule. This component takes a candidate-time and crew-feasible schedule
and turns it into a working schedule by attempting to start each job at
its earliest possible time. If the associated crew does not have sufﬁcient
availability, the job is moved forward until a day and time of day are
found where it can be scheduled. The resource-constrained scheduler
also uses the precedence analyzer to adjust any successor jobs when due
to resource availability constraints the current job slips past its target date.
The resource-constrained scheduler also couples the current job schedule
to the availability of any limited asset requirements (thus, a job must be
scheduled into a time frame consistent with both the crew availability and
the availability of all other limited assets).
➏The Precedence Analyzer
A precedence analyzer manages the topological relationships among the
various jobs. It is used by the job sequencer to initially order jobs that do
not have a speciﬁed start and ﬁnish date. The resource-constrained sched-
uler uses the precedence analyzer to adjust the start and likely completion
times for successor jobs when a predecessor job slips beyond its target
completion time.
➐The Auditing Facility
Integrated into the scheduling framework is a comprehensive, multilevel
auditing facility that allows the analyst or developer to expose a con-
siderable amount of the ﬁne-grain details in all scheduling components.
The auditing levels range from OFF, through LOW, MEDIUM, HIGH, and
MAXIMUM. Each level exposes more detail than the one below it.
➑The Crew Schedules
The crew scheduler automatically produces two schedules: one ordered
by job and one by crew. However, the GA sometimes ﬁnds and ranks sev-
eral optimal schedules. These are available to the application framework
as alternative schedules.

444
■
Chapter 10
Genetic Resource Scheduling Optimization
10.7
Implementing and Executing the Crew
Scheduler
In this section we examine the structure and processing of the resource-
constrained crew scheduler. This Java implementation is less robust and
extensive than the architecture discussed in the previous chapter. It is
concerned only with a single objective function (minimizing completion
time) and a single resource constraint (crew availability).
Objective Functions and Constraints
The objective function for the resource-constrained crew scheduler is sim-
ple: minimize job completion time. Each job has an estimated duration
(measured in hours) and an allowed window of execution (see the discus-
sion of master ﬁle content). The scheduler attempts to complete each job
as early as possible within this window of execution consistent with the
availability of the assigned crew. The ﬁtness of a schedule is determined
by Expression 10.2.
f =
N

i=1
(sa
i −se
i ) + ((ca
i −cl
i)2 × (ca
i > cl
i))
(10.2)
Here,
f
is the ﬁtness of the current schedule
N
is the number of jobs in the schedule
Each of the following expression variables represents the i-th job in the
schedule deﬁned by the current chromosome.
sa
i
is the actual start date of the job
se
i
is the earliest permitted start in the execution window
ca
i
is the actual completion date of the job
cl
i
is the latest permitted ﬁnish in the execution window
The ﬁtness function computes the degree of early start ﬁtness for the
current schedule. The goodness of ﬁt sums the difference between the
earliest possible start for a job and its actual (scheduled) start. The more
jobs scheduled close to or on their earliest possible time the smaller the
delta. If all jobs ﬁnish as soon as possible, the total ﬁtness function will
be zero. The ﬁtness function also includes a penalty for completing a
job after its latest permitted ﬁnish. We also sum, for all jobs that were
scheduled late, the square of the delta between the scheduled date and
latest allowable date. This is added to the ﬁtness function (and thus biases

10.7
Implementing and Executing the Crew Scheduler
■
445
the ﬁtness toward solutions in which all jobs complete at the earliest
possible time).
Constraints establish the boundaries for a feasible solution. Hard con-
straints are imposed by the actual scheduling system, whereas soft con-
straints are imposed as penalties in the ﬁtness function. There are several
hard and soft constraints embedded in the scheduling system.
■
(Hard)
A job can only be assigned to a crew that has the necessary
skills
■
(Hard)
A job cannot start before its earliest start time
■
(Soft)
A job should complete before its latest completion time
■
(Hard)
A job must be scheduled in a contiguous time frame (that is,
for this scheduler a job cannot be interrupted)
■
(Hard)
A job cannot be scheduled unless the assigned crew has
sufﬁcient available time
The constraints fall generally into two classes: resource availability restric-
tions and time-to-schedule restrictions. The resource constraints ensure
that the proper crew resource is assigned to a job and that the job is per-
formed when the crew has available time. The time-to-schedule constraint
ensures that a job does not start early, but allows a job to be scheduled
anytime after its earliest start date (although late ﬁnishes are penalized by
the ﬁtness function; see Expression 10.2).
The Genome Structure
A genome in the scheduler represents a complete schedule. The struc-
ture is relatively simple, allowing very rapid access and evaluation of each
scheduled job. As Figure 10.11 illustrates, in addition to the ﬁtness func-
tion value the chromosome consists of an array of N cells or objects, where
N is the number of jobs.
The Schedule
Job1
Job2
Jobn
Fitness
Crew
es
ef
Crew
es
ef
. . . . 
Figure 10.11
The scheduling genome structure.

446
■
Chapter 10
Genetic Resource Scheduling Optimization
In this chromosome, the i-th array (or vector) position corresponds
to the i-th job in the table of jobs for this schedule (see Listing 10.3). At
this location is the assigned crew (also an index into the table of available
crews for this schedule; see Listing 10.3) and the early start (es) and
the early ﬁnish (ef ) calculated by the underlying resource-constrained
scheduler. Thus, the chromosome consists of an array of three values
(C, s, f; crew, start, and ﬁnish).
In this exploration of resource-constrained crew scheduling the jobs
are located in a ﬁxed order, left to right across the chromosome. The
scheduler also supports a priority attribute on a job (see Listing 10.1).
When prioritization is used, the jobs are sorted in descending order by
priority (the lower the number the higher the priority) and a different
chromosome structure is used: the genome is an ordered array of four
values ( J, C, s, f; job, crew, start, and ﬁnish). When all priorities are
the same, this is essentially equivalent to the chromosome structure we
use in this chapter. Note also that this chromosome organization is also
necessary when precedence relationship constraints are used.
The Genetic Scheduler Process
The object design and control structures in the crew scheduler gener-
ally follow the overall architecture of the general scheduler discussed in
the previous section (see Figure 10.10). The chief difference is in the
restricted set of scheduling objects and the lack of precedence or topol-
ogy management in the current scheduler. Figure 10.12 is a schematic
overview of the scheduler with the principal object classes.
➊Assign Log and Audit Files
Audit and error logging ﬁles are attached to the system as one of the
ﬁrst activities. Any number of different audit ﬁles (both formatted and
unformatted) can be attached. The listings in this section are all derived
from an audit ﬁle (somewhat modiﬁed to remove date and time stamps,
class name signatures, and other forms of clutter).
➋Compile Schedule Master Plan
Although numerous methods exist for fetching the schedule components
from diverse sources (such as corporate databases), a basic master plan
compiler provides a simple and straightforward way of stating a crew-
scheduling problem. Listing 10.1 is the master plan for the example in
this chapter.

10.7
Implementing and Executing the Crew Scheduler
■
447
Schedule of jobs
with crews
siGenericScheduler
Resource-constrained
crew scheduler
5
siCrewScheduleGerenerator
Schedule jobs based
on crew availability
6
Audit file
Actual job
competition
statistics
Master plan
Skills (a,b,c,d)
Crews (c1, c2, c3)
Jobs (j1, j2, j3, j4, j5)
siGeneticControls
Set genetic algorithm
parameters
4
siCrewScheduleCompiler
Compile
schedule master plan 2
siLogMgr
Assign log
and audit files
1
siFeasibleAssignments
Returns feasible
job-to-crew
assignments
3
Figure 10.12
The Java implementation of the crew scheduler.
The master plan layout speciﬁes a collection of skills (indicated by the
skill keyword). One or more of these skills is associated with a crew in the
section that lists all crews with their daily availability (a crew is speciﬁed
with the crew keyword). The jobs to be scheduled are speciﬁed after the
crews. Each job (indicated by the job keyword) has a name, a priority
(unused in this example), a time requirement in minutes, a required early
start, a late ﬁnish, and a list of skills required to perform this job. In this
example, to reduce complexity each job has a single required skill.
Note that the required start and ﬁnish dates in this example are
expressed as offsets from the current date (in that a job cannot, in all
likelihood, start on the date that the schedule is actually produced, the
earliest start date is tomorrow, which has an offset of 1). In a real-world
implementation of the crew scheduler the actual calendar dates supplied
for the start and ﬁnish dates are converted to Java date values so that we
can do date arithmetic in the same way as the these integer offsets.

448
■
Chapter 10
Genetic Resource Scheduling Optimization
schedule;
skill
welding;
skill
welding;
skill
electrician;
skill
splicer;
skill
placer;
skill
builder;
#
#_____name__supply___skills
crew
C1
8
welding;
crew
C2
9
splicer;
crew
C3
8
welding electrician;
crew
C4
8
placer builder;
crew
C5
8
welding;
crew
C6
8
electrician;
#
#------name--priority-time-------es---ef--skill
job
J01
5
50
1
20 welding;
job
J02
5
75
1
10 electrician;
job
J03
5
58
1
35 placer;
job
J04
5
96
1
40 welding;
job
J05
5
209
1
16 electrician;
job
J06
5
50
1
20 welding;
job
J07
5
95
1
10 electrician;
job
J08
5
158
1
35 placer;
job
J09
5
96
1
40 welding;
job
J10
5
209
1
16 electrician;
job
J11
5
50
1
20 welding;
job
J12
5
375
1
10 electrician;
job
J13
5
158
1
35 placer;
job
J14
5
296
1
40 welding;
job
J15
5
279
1
16 electrician;
#
end;
Listing 10.1
The crew schedule master plan.
➌Return Feasible Job to Crew Assignments
The crew schedule compiler builds a number of important tables (the
list of jobs and properties and the list of crews and their properties). The
compiler also creates an important M × N matrix of all jobs, their required
skills, and the list of crews that can be assigned to this job. This matrix,
shown in Listing 10.2, shows all feasible assignments.

10.7
Implementing and Executing the Crew Scheduler
■
449
Feasible Assignments (Jobs-->Crews)
Required
Service Order
Skill
Feasible Crew Assignments
-------------
----------
--------------------------
J01
welding
C1
C3
C5
J02
electrician
C3
C6
J03
placer
C4
J04
welding
C1
C3
C5
J05
electrician
C3
C6
J06
welding
C1
C3
C5
J07
electrician
C3
C6
J08
placer
C4
J09
welding
C1
C3
C5
J10
electrician
C3
C6
J11
welding
C1
C3
C5
J12
electrician
C3
C6
J13
placer
C4
J14
welding
C1
C3
C5
J15
electrician
C3
C6
Compile Time: 31 milliseconds.
Listing 10.2
The feasible assignments.
The Feasible Assignments class fuses the active crews and the active
service orders (jobs) based on their skill requirements to generate a matrix
of jobs and the set of crews that could feasibly be assigned to the job. The
feasible assignments matrix is the core data control mechanism in the
genetic scheduler. It speciﬁes the elements that can constitute a valid
(that is, feasible) schedule. The GA works by creating an initial random
schedule from this set of feasible assignments. The algorithm randomly
selects crews from the feasible set and then generates a schedule (con-
sistent with crew resource availability). Crossover and mutations act on
the index values into the crew array by breeding good schedules. Muta-
tion occurs by randomly picking one of the crews in the feasible set and
assigning it randomly to one of the other legal assignments.
➍Set Genetic Algorithm Parameters
A relatively large number of parameters control the operation of a GA.
These are collected in a single class. Within the controls object you can set

450
■
Chapter 10
Genetic Resource Scheduling Optimization
such parameters and attributes as the maximum number of generations,
the type and frequency of crossover, the type and frequency of mutations,
the termination conditions, convergence tolerance, the size of the initial
population, and the population breeder type of the GA (steady state or
dynamic; see Chapter 9 for details on population management).
➎Execute the Resource-constrained Scheduler
The scheduler consists of two main class components: the GA that creates
and breeds possible schedules and the resource-constrained scheduler
itself, which implements a schedule, develops the start and ﬁnish times,
and measures its goodness of ﬁt. The constructor4 for the schedule takes
two parameters: the feasible assignments and the GA parameters. Listing
10.3 is the Java application interface that creates and runs the sample
crew schedule.
A schedule is actually initiated by the solve( ) method in the aiGe-
neticScheduler class (see Listing 10.3, where the solve method starts the
scheduler for an initial population of 135 chromosomes). Listing 10.4
shows the scheduler front end with the principal GA properties and the
underlying crew and job tables.
The default calendar for each crew allocates six hours of work time
(one hour for lunch and one hour for combined travel to and from jobs).
Because the job times are speciﬁed in minutes, the scheduler also con-
verts the crew availability to minutes (as shown in the crew table). From
the feasible assignments matrix, the GA creates a population of potential
schedules. Each chromosome in the population represents a schedule
(see Figure 10.11) with a crew, selected from the job’s feasible assign-
ments, randomly selected for each job. Listing 10.5 shows part of an
initial schedule (truncated both vertically and horizontally).
There are 135 chromosomes in this population, numbered at the left-
hand side of the initial population listing. As Listing 10.4 illustrates, an
initial schedule is simply a random assignment of crews to jobs. There is
no ﬁtness measure for any of the chromosomes.
➏Schedule Jobs Based on Crew Availability
To convert the assignments into an actual schedule and measure its
ﬁtness, we need to pass each chromosome, in turn, to the actual
4 A constructor in an object-oriented system is a special class method that actually allocates
memory and initializes an instance of the object associated with the class. In this example, an
siGeneticScheduler object is created from the siFeasibleAssignments and siGeneticProperties
objects.

10.7
Implementing and Executing the Crew Scheduler
■
451
package com.scianta.schedulers;
import java.io.IOException;
import java.util.Vector;
import com.scianta.tools.*;
import com.scianta.acm.*;
import com.scianta.common.*;
import com.scianta.audit.aiAuditMgr;
import com.scianta.audit.aiLogMgr;
import com.scianta.crewscheduler.objects.*;
import com.scianta.crewscheduler.utilities.*;
import com.scianta.crewscheduler.compiler.*;
import com.scianta.crewscheduler.scheduler.*;
public class SampleCrewScheduler {
public static void main(String args[]) {
// Configure the Log File System
siLogMgr.initLog("scianta", "c:/scianta/", "sciantalog.fil");
siAuditMgr.initAudit(siCrewScheduleStrategy.CLASSNAME,
true,false,aiAuditMgr.MAXIMUM,"schedules",
"c:/sciantalog/","audit.fil");
siCSLogger.putHeader();
siCSLogger.msg(" ");
System.out.println(" 1--compile the schedule definition.");
siCrewScheduleCompiler compiler = new siCrewScheduleCompiler();
System.out.println(" 2--run the scheduler.");
try
{
siCrewScheduleStrategy strategy=
compiler.compile("c:/schedules/example01.mplan");
siFeasibleAssignments fa = strategy.getAssignments();
siGeneticControls gc = new siGeneticControls();
gc.setMaximumGenerations(50);
gc.setRemoveDuplicates(true);
siGeneticScheduler gs = new siGeneticScheduler(gc,fa);
gs.solve(135);
}
catch(Exception e) {
System.out.println(" ERROR----> exception thrown.");
}
}
}
Listing 10.3
Running the crew scheduler (main class).

452
■
Chapter 10
Genetic Resource Scheduling Optimization
Crew Scheduling Generator
Maximum Duration (days): 40
=========================================================
G E N E T I C
C R E W
S C H E D U L I N G
=========================================================
GENETIC ALGORITHM PARAMETERS
Genome Length
: 15
Population Size
: 135
Maximum Cycles
: 50
Cross Over Rate
: 0.6
Mutation
Rate
: 0.05
Selection Type
: Strong Tournament
Cross-Over Type
: One Point Cross-Over
Breeder Type
: Steady-State Breeder
Crew Name
Supply
Associated Skill Capabilities
---------
------
------------------------------
C1
480.00
welding
C2
540.00
splicer
C3
480.00
electrician welding
C4
480.00
builder placer
C5
480.00
welding
C6
480.00
electrician
Earliest Latest
Job Name
Pri
Time
Start
Finish
Required Skills
--------
---
----
-----
------
--------------
J01
5
50
1
20
welding
J02
5
75
1
10
electrician
J03
5
58
1
35
placer
J04
5
96
1
40
welding
J05
5
209
1
16
electrician
J06
5
50
1
20
welding
J07
5
95
1
10
electrician
J08
5
158
1
35
placer
J09
5
96
1
40
welding
J10
5
209
1
16
electrician
J11
5
50
1
20
welding
J12
5
375
1
10
electrician
J13
5
158
1
35
placer
J14
5
296
1
40
welding
J15
5
279
1
16
electrician
Listing 10.4
Initial scheduling parameters.

10.7
Implementing and Executing the Crew Scheduler
■
453
Initial (Random) Schedules
Population size: 135
Fitness
(Job,Crew)
-------- ---------------------------------------------------------
1. (none) (J01,C5) (J02,C6) (J03,C4) (J04,C1) (J05,C6) ... (J15,C3)
2. (none) (J01,C3) (J02,C3) (J03,C4) (J04,C1) (J05,C3) ... (J15,C6)
3. (none) (J01,C3) (J02,C6) (J03,C4) (J04,C1) (J05,C3) ... (J15,C3)
4. (none) (J01,C5) (J02,C3) (J03,C4) (J04,C3) (J05,C3) ... (J15,C3)
5. (none) (J01,C1) (J02,C6) (J03,C4) (J04,C3) (J05,C3) ... (J15,C3)
6. (none) (J01,C1) (J02,C3) (J03,C4) (J04,C3) (J05,C6) ... (J15,C6)
7. (none) (J01,C1) (J02,C6) (J03,C4) (J04,C3) (J05,C3) ... (J15,C3)
8. (none) (J01,C1) (J02,C3) (J03,C4) (J04,C1) (J05,C3) ... (J15,C3)
9. (none) (J01,C1) (J02,C3) (J03,C4) (J04,C1) (J05,C6) ... (J15,C6)
10. (none) (J01,C3) (J02,C3) (J03,C4) (J04,C5) (J05,C6) ... (J15,C3)
:
:
130. (none) (J01,C3) (J02,C6) (J03,C4) (J04,C5) (J05,C6) ... (J15,C6)
131. (none) (J01,C5) (J02,C6) (J03,C4) (J04,C1) (J05,C6) ... (J15,C6)
132. (none) (J01,C5) (J02,C3) (J03,C4) (J04,C1) (J05,C6) ... (J15,C3)
133. (none) (J01,C1) (J02,C6) (J03,C4) (J04,C5) (J05,C3) ... (J15,C6)
134. (none) (J01,C3) (J02,C6) (J03,C4) (J04,C1) (J05,C3) ... (J15,C6)
135. (none) (J01,C3) (J02,C6) (J03,C4) (J04,C5) (J05,C6) ... (J15,C3)
Listing 10.5
The initial job-to-crew population (extract).
resource-constrained scheduler. Jobs within a chromosome, however,
are not processed in left-to-right order. If there are n jobs in the chromo-
some, they are selected randomly until all n jobs have been scheduled.
Listing 10.6 illustrates this with a small section of Java code from the
scheduler.
The method creates a bit (Boolean) array the same size as the number
of jobs in the genome (say, n). When the scheduler selects the next job to
schedule, it generates a random number between [0,n −1], picks that job,
and sets the bit array position to “true,” indicating that that job has already
been scheduled. This is repeated for each of the jobs in the chromosome.
The scheduler takes a selected job, ﬁnds its early start date, and
attempts to schedule the job on that day. If insufﬁcient time exists to
schedule the entire job on that day (one of the hard constraints requires
each job to be contiguously scheduled), it is moved to the next day and an
attempt is again made to schedule the job. This is repeated until the job
is eventually scheduled. Listing 10.7 is the result of the result-constrained
scheduler applied to the ﬁrst generation of chromosomes.

454
■
Chapter 10
Genetic Resource Scheduling Optimization
boolean[] numberUsed = new boolean[genomes.length];
for(int k=0;k<genomes.length;k++) numberUsed[k]=false;
for(int g=0;g<genomes.length;g++)
{
int i=0;
getNewJob:
while(true)
{
i=randomNumbers.nextInt(genomes.length);
if(!numberUsed[i])
{
numberUsed[i]=true;
break getNewJob;
}
}
}
Listing 10.6
Selecting random jobs from the schedule chromosome.
Upon returning from the resource-constrained scheduler, each job has
a start day and a time of day (expressed as the start minute in the day).
Each job is scheduled in the order it was randomly selected. Table 10.2
shows, in job order, the complete schedule for the ﬁrst chromosome in
the schedule.
Ordering Table 10.2 by start time within day by crew reveals the actual
schedule. For example, crew 4 begins with job J08 at time zero (at start
of day), and then 158 minutes later begins job J13. At this point, as shown
in Table 10.3, 316 minutes (158 + 158) of its 480-minute daily availability
have been used. Job J03 is now scheduled at time offset 316.
After each schedule is generated, a ﬁtness function value is produced
(see Expression 10.2). The ﬁtness function measures the goodness of
the schedule relative to the objective function (which in this case is the
minimization of the completion times for all jobs). In this initial schedule,
the ﬁtness value has a wide spectrum of values, from a minimum of 2
through a maximum of 13, and an average of 4.5.
When each schedule has been evaluated, every chromosome in the
population will have a ﬁtness value. The steady-state breeder operation
now randomly breeds, saves the top few best-performing chromosomes,
and then breeds (applies a crossover operator) and applies the mutation
operator to the remainder. Listing 10.8 shows a small extract of the new
population.

10.7
Implementing and Executing the Crew Scheduler
■
455
Generation: 1: Schedule Populations
Fitness (Job,Crew,StartDate,StartTime)
------- -------------------------------------------------------------------------------
1.
4.0000 (J01,C5,1,50)
(J02,C6,1,375) (J03,C4,1,316) (J04,C1,1,146) ... (J15,C3,1,0)
2.
6.0000 (J01,C3,1,100) (J02,C3,2,209) (J03,C4,1,316) (J04,C1,1,0)
... (J15,C6,2,0)
3.
3.0000 (J01,C3,1,375) (J02,C6,1,0) (J03,C4,1,316) (J04,C1,1,346) ... (J15,C3,3,0)
4.
5.0000 (J01,C5,1,146) (J02,C3,2,209) (J03,C4,1,316)(J04,C3,1,375) ... (J15,C3,3,0)
5.
7.0000 (J01,C1,1,50) (J02,C6,1,0) (J03,C4,1,316) (J04,C3,2,0) ...(J15,C3,3,0)
6.
4.0000 (J01,C1,1,96) (J02,C3,2,0) (J03,C4,1,316) (J04,C3,1,375)... (J15,C6,2,0)
7.
4.0000 (J01,C1,1,96) (J02,C6,1,0) (J03,C4,1,316) (J04,C3,2,0) ...(J15,C3,3,0)
8.
4.0000 (J01,C1,1,146) (J02,C3,1,259) (J03,C4,1,316)(J04,C1,1,196) ... (J15,C3,2,0)
9.
5.0000 (J01,C1,1,50) (J02,C3,1,96) (J03,C4,1,316) (J04,C1,2,0) ... (J15,C6,3,0)
10.
3.0000 (J01,C3,1,146) (J02,C3,2,0) (J03,C4,1,316) (J04,C5,1,296) ... (J15,C3,1,196)
11.
3.0000 (J01,C1,1,0) (J02,C3,1,279) (J03,C4,1,0) (J04,C1,1,346) ... (J15,C3,1,0)
12.
5.0000 (J01,C3,1,209) (J02,C3,1,259) (J03,C4,1,0) (J04,C3,1,334) ... (J15,C6,2,0)
13.
3.0000 (J01,C3,1,209) (J02,C3,1,309) (J03,C4,1,0) (J04,C5,1,50) ... (J15,C6,2,0)
14.
3.0000 (J01,C1,1,0) (J02,C6,1,375) (J03,C4,1,0) (J04,C1,1,346) ... (J15,C3,1,50)
15.
5.0000 (J01,C3,1,259) (J02,C6,1,375) (J03,C4,1,0) (J04,C5,1,0) ... (J15,C6,2,0)
:
:
130.
4.0000 (J01,C3,1,0) (J02,C6,1,279) (J03,C4,1,0) (J04,C5,1,296) ... (J15,C6,1,0)
131.
2.0000 (J01,C5,1,0) (J02,C6,1,374) (J03,C4,1,0) (J04,C1,1,296) ... (J15,C6,1,0)
132.
4.0000 (J01,C5,1,0) (J02,C3,1,374) (J03,C4,1,0) (J04,C1,1,96) ... (J15,C3,1,0)
133.
4.0000 (J01,C1,1,0) (J02,C6,1,374) (J03,C4,1,0) (J04,C5,1,0) ... (J15,C6,1,0)
134.
4.0000 (J01,C3,1,0) (J02,C6,1,279) (J03,C4,1,0) (J04,C1,2,0) ... (J15,C6,1,0)
135.
6.0000 (J01,C3,1,279) (J02,C6,2,95) (J03,C4,1,0) (J04,C5,1,0) ... (J15,C3,1,0)
Generation: 1
Current Convergence Statistics
Minimum Fitness (Best)
: 2.0000
Maximum Fitness (Worse) : 13.000
Average Fitness
: 4.5185
Listing 10.7
Generation 1 of the genetic scheduler (extract).
Newly created chromosomes and those with mutated genomes have
their ﬁtness values removed (as shown at the bottom of Listing 10.6).
This initial population forms the population for the next generation of
schedules. The process of creating potential schedules, scheduling each
potential schedule based on resource availability and time constraints,
and evaluating each schedule for its goodness of ﬁt continues generation
after generation. Listing 10.9 shows the population after 48 generations.
By the end of the genetic search (48 out of 50 generations), the popula-
tion has converged toward a large number of high-performance schedules.

456
■
Chapter 10
Genetic Resource Scheduling Optimization
TABLE 10.2
A Job-crew Schedule
Scheduled Times
Job
Crew
Start Day
Start Minute
J01
C5
1
50
J02
C6
1
375
J03
C4
1
316
J04
C1
1
146
J05
C6
2
0
J06
C5
1
0
J07
C6
3
0
J08
C4
1
0
J09
C1
1
0
J10
C6
2
209
J11
C1
1
96
J12
C6
1
0
J13
C4
1
158
J14
C5
1
100
J15
C3
1
0
TABLE 10.3
A Job-crew Schedule (in Crew Order)
Scheduled Times
Job
Time
Crew
Start Day
Start Minute
J09
96
C1
1
0
J11
50
C1
1
96
J04
96
C1
1
146
J15
279
C3
1
0
J08
158
C4
1
0
J13
158
C4
1
158
J03
58
C4
1
316
J06
50
C5
1
0
J01
50
C5
1
50
J14
296
C5
1
100
J12
375
C6
1
0
J02
75
C6
1
375
J05
209
C6
2
0
J10
209
C6
2
209
J07
95
C6
3
0

10.7
Implementing and Executing the Crew Scheduler
■
457
Generation: 1: Schedule Populations
Fitness (Job,Crew,StartDate,StartTime)
------- -------------------------------------------------------------------------------
1.
2.0000 (J01,C1,1,0) (J02,C6,1,375) (J03,C4,1,0) (J04,C5,1,346) ... (J15,C6,2,0)
2.
2.0000 (J01,C1,1,96) (J02,C6,1,304) (J03,C4,1,158) (J04,C3,1,375) ... (J15,C6,2,0)
3.
2.0000 (J01,C1,1,96) (J02,C3,1,405) (J03,C4,1,158) (J04,C3,1,259) ... (J15,C3,2,0)
4.
2.0000 (J01,C1,1,392) (J02,C6,1,375) (J03,C4,1,316) (J04,C5,1,50) ... (J15,C3,1,0)
5.
2.0000 (J01,C5,1,0) (J02,C3,1,146) (J03,C4,1,316) (J04,C5,1,50) ... (J15,C3,2,0)
6.
2.0000 (J01,C5,1,50) (J02,C3,1,0) (J03,C4,1,316) (J04,C5,1,100) ... (J15,C6,1,0)
7.
3.0000 (J01,C1,1,0) (J02,C3,1,96) (J03,C4,1,316) (J04,C1,1,50) ... (J15,C6,1,0)
8.
3.0000 (J01,C3,1,0) (J02,C6,1,374) (J03,C4,1,0) (J04,C1,1,296) ... (J15,C6,1,0)
9.
3.0000 (J01,C5,1,0) (J02,C3,1,374) (J03,C4,1,0) (J04,C5,1,146) ... (J15,C3,1,0)
10.
3.0000 (J01,C5,1,0) (J02,C6,1,374) (J03,C4,1,0) (J04,C1,1,296) ... (J15,C6,1,0)
11.
3.0000 (J01,C3,1,375) (J02,C6,1,0) (J03,C4,1,316) (J04,C1,1,346) ... (J15,C3,3,0)
12.
4.0000 (J01,C3,1,146) (J02,C3,2,0) (J03,C4,1,316) (J04,C5,1,296) ... (J15,C3,1,196)
13.
4.0000 (J01,C1,1,0) (J02,C3,1,279) (J03,C4,1,0) (J04,C1,1,346) ... (J15,C3,1,0)
:
:
133.
(none) (J01,C5,1,0) (J02,C3,1,279) (J03,C4,1,316) (J04,C5,1,50) ... (J15,C3,1,0)
134.
(none) (J01,C5,1,0) (J02,C3,3,296) (J03,C4,1,0) (J04,C3,3,371) ... (J15,C6,1,0)
135.
(none) (J01,C5,1,0) (J02,C3,3,296) (J03,C4,1,0) (J04,C3,3,371) ... (J15,C6,1,0)
Listing 10.8
Generation 1 after retention, breeding, and mutation (extract).
Thus, the average ﬁtness function value of the population begins to fall
toward the minimum value. Figure 10.13 graphs the minimum, maximum,
and average ﬁtness function values of the crew scheduler over the initial
48 generations.
In the steady-state GA used in this scheduler, we keep genetic diversity
(mutation and crossover) relatively high to avoid premature convergence
and to explore as much of the solution terrain as possible. This gener-
ally means that even late in the genetic search process a small number
of poor-performing chromosomes will exist in the population. However,
the minimum ﬁtness (the measure of high-performance schedules) arises
midway through the search and remains throughout the rest of the pro-
cessing. Listing 10.10 shows the best schedule after 50 generations. The
schedule is displayed in two forms: by job name and by crew identiﬁer.
In the example master plan for this schedule, all jobs started on day 1.
Because it is impossible to schedule all jobs on their earliest start date
(at least one job must be moved to another day), the ﬁtness function can
never converge to zero for this problem.

458
■
Chapter 10
Genetic Resource Scheduling Optimization
Generation: 48: Schedule Populations
Fitness (Job,Crew,StartDate,StartTime)
------- -------------------------------------------------------------------------------
1.
1.0000 (J01,C5,1,0) (J02,C6,1,374) (J03,C4,1,158) (J04,C1,1,96) ... (J15,C6,1,0)
2.
1.0000 (J01,C5,1,96) (J02,C6,1,95) (J03,C4,1,316) (J04,C5,1,0) ... (J15,C6,1,170)
3.
1.0000 (J01,C3,1,0) (J02,C6,1,374) (J03,C4,1,158) (J04,C5,1,50) ... (J15,C6,1,95)
4.
1.0000 (J01,C1,1,0) (J02,C6,1,375) (J03,C4,1,0) (J04,C5,1,346) ... (J15,C6,2,0)
5.
1.0000 (J01,C1,1,96) (J02,C6,1,304) (J03,C4,1,158) (J04,C3,1,375) ... (J15,C6,2,0)
6.
1.0000 (J01,C1,1,96) (J02,C3,1,405) (J03,C4,1,158) (J04,C3,1,259 ... (J15,C3,2,0)
7.
2.0000 (J01,C1,1,392) (J02,C6,1,375) (J03,C4,1,316) (J04,C5,1,50) ... (J15,C3,1,0)
8.
2.0000 (J01,C5,1,0) (J02,C3,1,146) (J03,C4,1,316) (J04,C5,1,50) ... (J15,C3,2,0)
9.
2.0000 (J01,C5,1,50) (J02,C3,1,0) (J03,C4,1,316) (J04,C5,1,100) ... (J15,C6,1,0)
10.
2.0000 (J01,C1,1,0) (J02,C3,1,96) (J03,C4,1,316) (J04,C1,1,50) ... (J15,C6,1,0)
11.
2.0000 (J01,C3,1,0) (J02,C6,1,374) (J03,C4,1,0) (J04,C1,1,296) ... (J15,C6,1,0)
12.
2.0000 (J01,C5,1,0) (J02,C3,1,374) (J03,C4,1,0) (J04,C5,1,146) ... (J15,C3,1,0)
:
:
133.
3.0000 (J01,C5,1,146) (J02,C6,1,0) (J03,C4,1,158) (J04,C5,1,0) ... (J15,C6,1,75)
134.
4.0000 (J01,C3,1,424) (J02,C6,1,0) (J03,C4,1,158) (J04,C5,1,0) ... (J15,C3,1,50)
135.
4.0000 (J01,C3,1,424) (J02,C6,1,0) (J03,C4,1,158) (J04,C5,1,0) ... (J15,C3,1,50)
Generation: 48
Current Convergence Statistics
Minimum Fitness (Best)
: 1.0000
Maximum Fitness (Worse) : 4.0000
Average Fitness
: 2.4592
Listing 10.9
Generation 48 of crew scheduler (extract).
1
1
1
1 1 1 1 1 1 1 1 1 1 1 1 1 1
2 2 2 2 2 2 2 2
2
2
2 2
3
3
3
3
3
3
3
3
4
4
3
4
3
3
3 3
4
4
4
4
4
4
4
4
4
4
4
4
4
4
6
5
5
5
6
6 6
6
7
7
8
8
9
10
11
10
101214161820222426272931333537394142444648
7 9
1317151312
9
20
15
10
5
0
Schedule goodness of fit convergence
Minimum Fitness
Maximum Fitness
Average Fitness
Figure 10.13
The goodness-of-ﬁt convergence.

10.7
Implementing and Executing the Crew Scheduler
■
459
BEST SCHEDULE CONFIGURATION
Order: Service Order (Job) Name
Service Order
Duration Crew
Day Time
-------------
-------------
--------
1. J01
50 C5
1 0800
2. J02
75 C6
1 1414
3. J03
58 C4
1 1038
4. J04
96 C1
1 0936
5. J05
209 C3
1 0850
6. J06
50 C3
1 0800
7. J07
95 C6
1 1239
8. J08
158 C4
1 0800
9. J09
96 C1
1 0800
10. J10
209 C3
1 1219
11. J11
50 C1
1 1112
12. J12
375 C3
2 0800
13. J13
158 C4
1 1136
14. J14
296 C5
1 0850
15. J15
279 C6
1 0800
--------------------------------------------------------------
BEST SCHEDULE CONFIGURATION
Order: Crew Assignment/Date and Time of Day
Service Order
Duration Crew
Day Time
-------------
-------------
--------
1. J09
96 C1
1 0800
2. J04
96 C1
1 0936
3. J11
50 C1
1 1112
4. J06
50 C3
1 0800
5. J05
209 C3
1 0850
6. J10
209 C3
1 1219
7. J12
375 C3
2 0800
8. J08
158 C4
1 0800
9. J03
58 C4
1 1038
10. J13
158 C4
1 1136
11. J01
50 C5
1 0800
12. J14
296 C5
1 0850
13. J15
279 C6
1 0800
14. J07
95 C6
1 1239
15. J02
75 C6
1 1414
Listing 10.10
The best resource-constrained crew schedule.

460
■
Chapter 10
Genetic Resource Scheduling Optimization
10.8
Topology Constraint Algorithms and
Techniques
The crew scheduler is representative of a broad class of problems that are
strictly (or mostly) concerned with placing objects in time subject only to
resource availability. None of the jobs has a time dependency on any of the
other jobs in the schedule. That is, any job can move backward or forward
in time without directly affecting5 the scheduled start or completion time
of any other job. When two (or more) jobs are dependent on each other,
they have a precedence relationship with each other. These dependencies
form a graph. Before taking up the issues associated with predecessor-
successor constraints in a genetic scheduling system, we review some
basic principles in graph theory.
A Brief Introduction to Graph Theory
Scheduling is predicated on a branch of mathematics known as graph
theory. Graphs are collections of “things” connected in some fashion.
Generally speaking, a graph and a network are equivalent and we use the
terms somewhat interchangeably. This introduction is intended to be just
that: a high-altitude and rather superﬁcial discussion of the way graphs
are assembled, what they can represent, and how they become concrete
representations instead of arbitrary abstractions.
Nodes and Edges: Basic Terminology
The basic principles and terminology of a graph are fairly easy to under-
stand. Essentially, a graph consists of two components: nodes and edges.
Nodes are connected by edges. Figure 10.14 shows the organization of a
simple graph.6
This graph has four nodes (A, B, C, and D) and ﬁve edges (e1, e2, e3,
e4, and e5). Note that a node is an object or thing in the network and
the edge is the relationship that establishes the connection between two
5 Even without explicit precedence relationships, the slippage of job A can affect the start or
completion of job B if they share the same crew or if they both need, for example, a piece of
equipment that has limited availability.
6 Although graphs and networks share a common topology (shape), they differ in a technical
way. Graphs are homogeneous: all of their nodes are the same. Networks, on the other hand, are
heterogeneous. That is, their nodes can be different types of things. For example, a network of
computers can have nodes that are application and web servers, mainframes, desktop computers,
load balancers, and other devices.

10.8
Topology Constraint Algorithms and Techniques
■
461
A
D
C
B
e4
e1
e2
e5
e3
Figure 10.14
The organization of a simple graph.
nodes. It is common practice in graph theory to refer to a unique edge
by the two nodes it connects (thus, edge e1 is A,D, or simply AD). Both
nodes and edges can have a wide assortment of properties. Some of these
properties are associated with the nature of the node, whereas others
are associated with the organization, semantics, and characteristics of the
graph itself.
Connectivity
If you can navigate from any node in a graph to any other node in the graph,
the graph has the important property of being fully connected. The way
you get from node X to node Y is called a path (which represents a list of
all edges in the order that they must be traveled). In most graphs, the path
from one node to another is not unique. Thus, for the network shown in
Figure 10.14 the path from B to D could be any of the following.
Path
1
(B, C)(C, D)
2
(B, D)
3
(B, C)(C, A)(A, D)
Because a graph is an abstraction, the number of edges in a path does not
necessarily reﬂect the length of the route. The length depends, usually, on
some value of the edge (the value being measured). For example, suppose
Figure 10.15 represents the highway distances (or traveling times) on the
roads that connect four neighboring cities in the Midwest.

462
■
Chapter 10
Genetic Resource Scheduling Optimization
City
A
City
D
City
C
City
B
8
4
5
22
10
Figure 10.15
Highway travel distances between cities.
In this case, the direct path from city B to city D is 22 miles, but the
shortest path is through city C and then through city A and on to city D
(for a total of 17 miles). In the travel graph shown in Figure 10.15, the
edges contain the distances between cities and the nodes are the cities
themselves. Nodes and edges can have rather complex properties. As an
uncomplicated example, the city node might also contain the average
travel time through the city. The edge could contain the distance in miles
as well as the average speed limit,7 any toll charges, the amount of high-
way construction arrayed by month and time of day, the average number
of speeding tickets given to in-state and out-of-state drivers over the past
12 months (by month), and the driving conditions (snowfall, rain, and so
on) by time of year. With these properties a network analysis could arrive
at many driving solutions between these cities, such as the least cost, least
time (independent of risk), least time (weighted by risk and best time to
travel), worse time to travel, and so forth.
Direction and Cycles
Graphs come in a variety of ﬂavors: cyclical and acyclical, as well as
directed and undirected. These two concepts are very closely related.
A directed graph means that node-to-node movement is restricted to a
speciﬁc direction (such as left to right or top to bottom). A cyclical
graph allows loops and generally means that the graph is undirected. In
Figure 10.15, for example, a driver can go from city B to city D and then
7 Hence, a short path that has several edges at a low speed limit might take longer to travel than
a longer path with most or all edges having a high speed limit.

10.8
Topology Constraint Algorithms and Techniques
■
463
drive back from city D to city B. The graph does not specify a direction
of process ﬂow. A driver can also drive from city C to city D to city A and
back to city C (thus making a loop within the graph).
Although graphs in a large number of applications are loop tolerant,
most scheduling applications (such as project management and crew
scheduling) require a directed, acyclical graph. A crew schedule is
directed in time. It cannot contain any loops (which are considered errors
in the network graph, because following a loop would mean going back in
time and would introduce dependency errors in the network). Each node
represents a job and speciﬁes the associated resource and time require-
ments. The edges not only connect the nodes in precedence order but
allow for overlaps in the way successor nodes can start.
Scheduling Network Dependency Issues
The precedence relationship enforces a time constraint. If job A must
occur before job B, a slip in the completion of job A might mean a slip
in the start of job B. These types of dependencies are common in most
scheduling and conﬁguration problems. For example, Figure 10.16 is a
small critical path precedence network in a project-scheduling system.
In a more practical and real-world crew-scheduling system, this
independence from precedence constraints seldom exists. If a crew is
scheduled to work on job A and then work on job B, job B cannot be
wholly independent from the completion of job A. The network of inter-
connected jobs forms the topology of the schedule. This topology places
a hard constraint on the feasibility of a schedule. How a network places a
constraint, however, depends on the nature of the precedence relation-
ship. Figure 10.17 illustrates a small portion of a crew schedule with a set
of jobs, their assigned crews, and their precedence relationships.
Design
Budget
approval
Build
system
Test
Buy
servers
Deploy
Write
manuals
Figure 10.16
A project critical path network.

464
■
Chapter 10
Genetic Resource Scheduling Optimization
Job B
Crew 2
Job C
Crew 1
Job D
Crew 3
Job E
Crew 2
Job F
Crew 3
Job G
Crew 1
Job A
Crew 1
Figure 10.17
A crew schedule with precedence-connected jobs.
Job B
Crew 2
Job C
Crew 1
Job A
Crew 1
Execution window
Figure 10.18
The execution windows for jobs A, B, and C.
The nature of the precedence relationships determines how tightly
a schedule is constrained. For example, from Figure 10.15 suppose job
A slips. What impact will this have on job B? This depends to a large
extent on the size of job A’s permissible execution window. Figure 10.18
examines the schedule in ﬁner detail, showing the execution windows
for jobs A, B, and C.
If job A’s actual completion time falls within the execution window,
job B can still start on its scheduled (earliest) start time. Job B, on the
other hand, has a very narrow execution window, and thus even a small
amount of slippage will have an effect on the scheduled start time of job
C. In crew schedules that use execution windows, the overall effect is a
network that is very similar to a critical path project network with built-in
ﬂoat.

10.8
Topology Constraint Algorithms and Techniques
■
465
Types of Dependency Relationships
The network constraints discussed so far have been ﬁnish-to-start (FS)
relationships. These are the most common, but a complex crew, project,
conﬁguration, or loading schedule can be constrained by many other
types of relationships. Each network relationship is a dependency strat-
egy. The strategy can also have an associated weight, the meaning of
which depends on the associated strategy. A scheduling (or any other
type of general network) can support the followings types of precedence
relationships.
■
Finish-to-ﬁnish (FF). An activity cannot ﬁnish until its predecessor
event is also complete. The weight value entered with the strat-
egy indicates a lag time between the completion of the predecessor
and the completion of the successor. Figure 10.19 illustrates a
ﬁnish-to-ﬁnish relationship.
■
Finish-to-start (FS). An activity cannot start until its predecessor event
is complete. This is generally the default precedence relationship in
a network. The weight is entered with the strategy, indicating a lag
time between ﬁnish of the predecessor activity and the start of the
successor activity. Figure 10.20 illustrates a ﬁnish-to-start relationship.
■
Start-to-ﬁnish (SF). An activity cannot complete until its predecessor
event starts. The weight entered with the strategy indicates an overlap
Job B
Job A
weight
Figure 10.19
The ﬁnish-to-ﬁnish strategy.
Job B
Job A
weight
Figure 10.20
The ﬁnish-to-start strategy.

466
■
Chapter 10
Genetic Resource Scheduling Optimization
of lead times between the predecessor and successor events. Figure
10.21 illustrates a start-to-ﬁnish relationship.
■
Start-to-start (SS). An activity cannot start until its predecessor activity
also starts. The weight entered with the strategy indicates an overlap
of the starting times for the predecessor and successor activity. Figure
10.22 illustrate a start-to-start relationship.
In crew scheduling, the lag value on the ﬁnish-to-start relationship (the
most common strategy) can be used to specify the travel time between
jobs. For example, in Figure 10.23 consider the ﬁnish-to-start precedence
relationship.
Job A
Job B
weight
Figure 10.21
The start-to-ﬁnish strategy.
Job A
Job B
weight
Figure 10.22
The start-to-start strategy.
Job A
Job B
60
Figure 10.23
A delay relationship between activities.

10.8
Topology Constraint Algorithms and Techniques
■
467
When job A is complete, 60 minutes must elapse before job B can
start. This reﬂects the travel time from the site of job A to the site of job B.
An important enhancement to crew-scheduling systems involves learning
the optimal travel time weight for a job that occurs in the neighborhood
of site X and is followed by a job occurring in the neighborhood of site Y.
Using the actual values form the completed work order forms, a real-
time, long-range GA can, for example, explore the effects of generating
travel times for future schedules that slightly extend or slightly contract
the travel times — depending on whether the jobs in the (SiteX,Site Y)
relation in the previous schedule experienced a slippage or completed
on time (or completed early).
Representing Dependency Graphs
There are several compact ways to represent a precedence graph. Perhaps
the best and the easiest to use is the dependency matrix. This approach not
only provides simple methods for traversing the graph but incorporates
robust and ﬂexible methods of specifying and managing the properties
of both nodes and edges. To see how the dependency (sometimes called
an adjacency) matrix is used consider the simple scheduling network
shown in Figure 10.17. The dependency graph is an N × N square matrix.
For nodes N1, N2, N3, and Nk, rows Ri and column Ci correspond to
node Ni. The rows (vertical nodes) are the FROM nodes, and the columns
(horizontal nodes) are the TO nodes. In its simplest form, the dependency
matrix is Boolean. We place a “1” (or true) value in any (row,column)
intersection when there is an edge between the FROM node and the
TO node. To see how this works, consider the crew schedule network
shown in Figure 10.15. The corresponding dependency matrix8 is shown
in Figure 10.24.
This matrix representation lacks ambiguity and is essentially direction
free as long we are consistent in the way we specify the topology of the
network. The choice of representation is, of course, somewhat arbitrary.
In Figure 10.22 we listed the nodes in left-to-right order (the way we
visualize them in a time-ordered crew schedule). We could also enter the
nodes in top-to-bottom order or any order that records each unique edge
only once in the network. In this way, dependency matrices can represent
any arbitrarily complex graph (such as a tree of class inheritances).
8 The zero (Boolean false) values have been omitted for clarity.

468
■
Chapter 10
Genetic Resource Scheduling Optimization
TO - Node
FROM - Node
A
B
C
D
E
F
G
A
B
C
D
E
F
G
1
1
1
1
1
1
1
1
Figure 10.24
The job schedule dependency matrix.
Implementing Edge Properties
How do we incorporate node and edge properties into the dependency
matrix? One of the signiﬁcant advantages of the matrix approach is the
ease with which we can specify edge and node properties. We simply
replace the Boolean value in the row and column intersection with a
reference to an edge property object. When the intersection is NULL (or
empty), we know that no edge exists (equivalent to the Boolean false).
Otherwise, we examine the edge object to determine the properties of
this particular edge. Because nodes are only implied in the matrix, we
create an ordered array of node identiﬁers and an associated node property
object (these need not, of course, be two distinct objects). Figure 10.25
shows a schematic of how this might appear.
The node array provides a clean and rapid correlation between any
row or column in the matrix and its corresponding node (thus, the node
for Rowi is simply Nodei in the array). This data structure allows us to
robustly and efﬁciently handle node and edge properties. It also provides
both the key mechanism for traversing the graph and connecting multiple
graphs into a heterogeneous network.
Sparse Matrix Representations
A signiﬁcant drawback to the dependency matrix representation is its
storage requirements. The computer memory needed to store the matrix
grows exponentially as the square of the number of nodes. Furthermore,

10.8
Topology Constraint Algorithms and Techniques
■
469
TO - Node
FROM - Node
A
B
C
D
E
F
G
A
B
C
D
E
F
G
Node
Properties
A
B
C
D
E
F
G
ep
1
ep
2
ep
4
ep
5
ep
6
ep
7
ep
8
ep
3
→
(p1,p2,...,pn)
(p1,p2,...,pn)
(p1,p2,...,pn)
(p1,p2,...,pn)
(p1,p2,...,pn)
(p1,p2,...,pn)
(p1,p2,...,pn)
Figure 10.25
Adding node and edge properties to the matrix.
A
A
B
C
D
E
F
G
B
C
D
E
F
G
1
1
1
1
1
1
1
1
Columns
Rows
Figure 10.26
A sparse matrix representation.
the actual network itself is almost always represented by just a small num-
ber of non-zero cells (relative to the very large size of the matrix). This
means that the matrix is very sparse. Fortunately, efﬁcient and very robust
space matrix algorithms exist that can represent extremely large matrices
in a small amount of actual storage. As Figure 10.26 illustrates, a sparse
matrix only stores the non-zero (or nonempty) cells.

470
■
Chapter 10
Genetic Resource Scheduling Optimization
In this example (using the graph in Figure 10.22), only nine out of a
possible 81 cells are stored, meaning that only 11% of the matrix actually
exists in computer memory. Sparse matrix representations allow schedul-
ing, conﬁguration, and routing models to form very large multinetwork
graphs and perform high-speed data access and analysis without loss of
generality or extensive modiﬁcations to existing analysis algorithms.
Graph Traversal Algorithms
To move forward in the matrix (from left to right for the graph in
Figure 10.22) along the ﬁrst available path, we follow the chains implicit
in the matrix rows. This is expressed in the following algorithm.
Let D(n,n) be the dependency matrix of n nodes
Let N(n) be the ordered set of node identiﬁers
Let P(n) be the ordered path found by this algorithm
Let S be the current node identiﬁer
Traversal:
P()
empty
S
a Node Identifier
P(1)
S
Walk_Forward:
k = location of S in N()
for j=1 to n
if D(k,j) not NULL then
insert N(j) into P()
S
N(j)
Goto Walk_Forward
End if
Exit Traversal
End for
Note that this algorithm returns the path associated with the ﬁrst node in
each row. Thus, starting with node A the algorithm ﬁnds B as the ﬁrst node
identiﬁer. It then stores (A,B), sets the current node to B, and searches
across the row associated with node B. It will then ﬁnd node C, and store
(B,C). This continues until it ﬁnds an empty row or column. Any number
of recursive or stack-based extensions to this algorithm would allow it to
search through all paths from the starting node identiﬁer to the end of the
graph.

10.8
Topology Constraint Algorithms and Techniques
■
471
To move backward in the matrix (from right to left for the graph in
Figure 10.22) along the ﬁrst available path, we follow the chains implicit
in the matrix columns. This is really a trivial modiﬁcation to the algo-
rithm (essentially reversing the subscripts in testing for an empty D( )
matrix position), which for completeness is expressed in the following
algorithm.
Let
D(n,n) be the dependency matrix of n nodes
Let
N(n) be the ordered set of node identiﬁers
Let
P(n) be the ordered path found by this algorithm
Let
S be the current node identiﬁer
Traversal:
P()
empty
S
a Node Identifier
P(1)
S
Walk_Backward:
k = location of S in N()
for j=1 to n
if D(j,k) not NULL then
insert N(j) into P()
S
N(j)
Goto Walk_backward
End if
Exit Traversal
End for
These simple graph-traversal algorithms provide the core mechanism
for incorporating all node and edge property capabilities (although this
description has excluded, as overly detailed, the necessary table lookup
routines, the associated node identiﬁer tables, and the exact nature of
the node and edge objects). This extensibility and robustness in design
allow the easy development of much more advanced families of graph
exploration algorithms.
Topological Sorts
A topological sort places the nodes of a network in their dependency
order. The algorithm is used to create a working list of nodes for subse-
quent processing and to detect loops in the network. The algorithm is
relatively straightforward.

472
■
Chapter 10
Genetic Resource Scheduling Optimization
while not done
if the graph is empty, then we’re done
exit the loop
pick a node with no predecessors
if no such node exists
send message "the graph has a loop"
exit the sort
output that node (number that node)
delete that node from the graph
end while
For example, a topological sort applied to the crew-scheduling schedule
shown in Figure 10.17 creates a linear, ordered array of the job nodes.
Listing 10.11 shows a small Java program that creates the job schedule net-
work, prints out the topology in backward and forward order, performs
a topological sort, and displays the results.
This small Java driver for the topology display facilities creates a net-
work (an instance of the topology manager) and populates it with the
job network (shown in Figure 10.17). It then creates an instance of the
topological sort for this network and displays the results. Listing 10.12 is
the audit log from this program, with the topological sort output shown
at the bottom.
A topological sort guarantees that the identiﬁers will be in precedence
order. This means that when we select Jobn for processing we are certain
that all predecessors for Jobn are in the list from Job1 to Jobn−1. The
topological sort does not necessarily place them in the best possible order.
In the topological sort order in Listing 10.11, perhaps a better organization
would have placed Job-D immediately before Job-F.
The topological sort is also an important network validation tool
because it can check for loops in an acyclical directed graph — exactly
the type of graph we use in crew scheduling. If we change the network
construction to connect job F back to job A (thus forming a loop in the
network), the outcome of the topology driver (Listing 10.13) now appears
with a loop error.
Understanding the nature of precedence networks and how they are
constructed and used is crucial in designing advanced genetic scheduling,
conﬁguration, planning, and transportation systems. Network topology
constraints must be considered in the design of the genome, in the meth-
ods of creating an initial population, in techniques for crossover breeding,
and in the allowable forms of genome mutation. The idea of a feasi-
ble schedule in complex genetic scheduling systems must consistently
involve a topological analysis of the network.

10.8
Topology Constraint Algorithms and Techniques
■
473
public class JobTopoSort{
public static void main(String args[]) {
//--Configure the Log File System
aiLogMgr.initLog("crews", "c:/jobs/", "joblog.fil");
aiAuditMgr.initAudit(aiModelManager.CLASSNAME,
true,false,aiAuditMgr.MAXIMUM,"crews","c:/jobs/","audit.fil");
System.out.println(" 1--creating an empty network.");
aiTopologyManager network = new aiTopologyManager("mynetwork");
try
{
System.out.println(" 2--adding the job nodes");
network.connect("Job-A","Job-B");
network.connect("Job-B","Job-C");
network.connect("Job-B","Job-D");
network.connect("Job-C","Job-E");
network.connect("Job-C","Job-F");
network.connect("Job-E","Job-G");
network.connect("Job-F","Job-G");
System.out.println(" 3--nodes have been added.");
System.out.println(" 4--network structure reports.");
aiModelLogger.msg("Unique Network Node Identifiers:");
String[] keys = network.getIdentifiers();
for(int i=0;i<keys.length;i++)
aiModelLogger.msg(keys[i]);
aiModelLogger.msg(" ");
network.setTraverseDirection(aiModelConstants.FORWARD);
network.printNetwork("The forward network topology");
network.setTraverseDirection(aiModelConstants.BACKWARD);
network.printNetwork("The backward network topology");
System.out.println(" 5--calling topological sort.");
aiModelLogger.msg("Performing topological sort");
aiTopologicalSort tsort = new aiTopologicalSort(network);
Vector orderedKeys = tsort.order();
System.out.println(" 6--topological sort is complete.");
aiModelLogger.msg("Topological Order:");
for(int i=0;i<orderedKeys.size();i++)
aiModelLogger.msg((String)orderedKeys.get(i));
return;
}
catch(Exception e) {
System.out.println("ERROR. " + e.getMessage());
}
}
}
Listing 10.11
The Java topological sort program.

474
■
Chapter 10
Genetic Resource Scheduling Optimization
Audit Level
: MAXIMUM
Debug
: On
Unique Network Node Identifiers:
Job-A
Job-B
Job-C
Job-D
Job-E
Job-F
Job-G
The forward network topology
The Network nodes
From-Node
Edges
To-Node
---------
-----
-------
1.
Job-F
1 Job-G
2.
Job-E
1 Job-G
3.
Job-C
2 Job-E Job-F
4.
Job-B
2 Job-C Job-D
5.
Job-A
1 Job-B
The backward network topology
The Network nodes
To-Node
Edges
From-Node
-------
-----
---------
1.
Job-G
2 Job-E Job-F
2.
Job-F
1 Job-C
3.
Job-E
1 Job-C
4.
Job-D
1 Job-B
5.
Job-C
1 Job-B
6.
Job-B
1 Job-A
Performing topological sort
Topological Order:
Job-A
Job-B
Job-D
Job-C
Job-F
Job-E
Job-G
Listing 10.12
Topology operations and sort on job schedule.

10.9
Adaptive Parameter Optimization
■
475
1--creating an empty network.
2--adding the job nodes
3--nodes have been added.
4--network structure reports.
5--calling topological sort.
ERROR. [ID: com.scianta.model.topology.aiTopologicalSort-0003]
[Severity: 05]=> Unable to place network into topological order.
::[ID: com.scianta.model.topology.aiTopologicalSort-0005]
[Severity: 01]=> Unable to order. Loop exists in topology.
Probable Loop: Job-A to Job-F
Listing 10.13
Topological sort with a loop error.
10.9
Adaptive Parameter Optimization
A schedule relies on the accuracy of its constraints and its parameters.
Many of the parameters in a schedule are often difﬁcult to predict and
establish with a high degree of certainty. Yet, the relative precision of
parameter values affects not only the outcome of the schedule but the
way in which optimization is achieved. There are several critical factors
(or parameters) that are especially important in evolving a workable and
effective crew schedule. These include the following.
■
Job duration times: The actual time necessary to complete a job
given the type, location, intrinsic difﬁculty, site work constraints,
crew assignment, and other variables associated with the job itself.
This is often quite different from the “standard” time associated with
a particular type of job. Job duration times are often time-of-day
dependent.
■
Crew efﬁciencies: The actual productivity or effective skill efﬁciency
of one crew versus another crew with the same skills in the same
geographic or demographic region. Crew effectiveness is a critical
measure in managing the assignment of crews to high-priority, high-
cost, or high-risk jobs.
■
Travel times between sites: The actual elapse time necessary for a
crew to travel from site X to site Y. Learning the time-of-day variations
or the travel constraint properties of certain sites allows the sched-
uler to ﬁne tune the actual travel times between customer sites, thus
providing a more robust and realistic schedule.

476
■
Chapter 10
Genetic Resource Scheduling Optimization
■
Available crew times: Each crew has a calendar that speciﬁes its
total daily availability and its time-of-day intervals that are open for
work. Actual available times are often different from the theoretical
(calendar-based) times. Learning to estimate the effective available
supply of a crew as well as its effective available time intervals
signiﬁcantly improves scheduling performance.
■
Crew size or crew composition: The organization or size of a crew
often has a direct bearing on its effectiveness, adaptability, and travel
time. Crew size is also a factor in setup and tear-down times for jobs.
■
Job prioritization: How to effectively prioritize jobs based on past
customer experiences, risk assessment, travel time, and other fac-
tors provides an important ordering element that can be critical in
satisfying one class of jobs over another.
The ﬁrst three of these factors (duration times, crew efﬁciency, and travel
times) are part of the standard machine learning facilities in the crew
scheduler. This machine learning approach uses an adaptive feedback
mechanism to learn the actual properties of these parameters. This is
accomplished by comparing the estimated or standard values for param-
eters (such as the normal or standard duration time for a particular type
of job) with the actual value for the parameter. In this way, the system
evolves a set of crew-to-job performance metrics. Figure 10.27 illustrates
schematically how the best values for a schedule are developed by an
adaptive learning technique.
The optimization of scheduling parameters involves two principal
components: a mechanism that learns the behavior of crews “in the ﬁeld”
and compares this behavior to the parameters that generated the current
schedule, and an adaptive feedback loop that adjusts the set of parameters
for the next schedule.
➊The Current Schedule
The current schedule is generated from the mixture of crews and jobs
based on the set of constraint parameters, such as estimated job duration
times, travel times, crew availability, and crew skill effectiveness. This
schedule is the currently ﬁelded and operational plan.
➋The Actual Completion Statistics
A learning mechanism needs to know how the scheduled jobs were actu-
ally completed. This is a measure of how well the schedule matched

10.9
Adaptive Parameter Optimization
■
477
Resource-constrained
scheduler
Genetic optimizer
Crew scheduler
Schedule of jobs
with crews
Audit file
Job sequencer
(optional)
Precedence analyzer
2
6
1
Performance rule
discovery
Statistical learning
4
Actual job
completion
statistics
Parameter
estimates
5
Job
history
3
Feed-back loop
Figure 10.27
The adaptive schedule parameter optimization.
reality. In most cases the acquisition of job completion statistics is fairly
straightforward and is derived form the ﬁeld service report associated
with the job. The fundamental properties the scheduler needs to extract
from the actual completion data are the differences between the times
speciﬁed in the current schedule and the times actually recorded by the
crew.
➌The Job History Repository
The scheduler maintains a comprehensive repository of past job comple-
tion statistics. The history provides the foundation for learning about the

478
■
Chapter 10
Genetic Resource Scheduling Optimization
past and predicting the future. Each current schedule is transformed into
a set of descriptive statistics that is used by the learning mechanism.
➍The Machine Learning System
The machine learning facilities are the core of parameter optimization.
The ﬁrst component of this system is based on statistical learning the-
ory and extracts, from the historical database and from the differences
between the planned and actual schedules (the value for each parame-
ter over time). As Figure 10.28 illustrates, these values are discovered at
various levels of granularity. The granularities constitute various types of
periodic cycles across time.
The second component of the machine learning system is a very
advanced and very deep pattern-recognition processor. Using a form of
knowledge discovery (also known as data mining), this process extracts
behavior rules in the form of fuzzy logic from the historical database.
These rules are stored in a knowledge base and are used to make ﬁne-grain
classiﬁcation and categorization decisions. As Figure 10.29 illustrates,
the combination leads to better and better values for critical scheduling
parameters.
As the learning progresses, the parameters converge on a small optimal
range of values consistent with the properties of the associated feature.
Learning is a continuous process that provides constant reinforcement so
that values change as the scheduling environment changes.
Monthly
time series
Weekly
time series
Daily
time series
Time
Parameter value
Figure 10.28
The periodic behavior of parameters.

10.10
Review
■
479
Manual
estimate
Optimized
value
Time
Parameter value
Figure 10.29
Estimated versus optimized parameters.
➎Parameter Estimates
This is the result of the statistical learning and rule extraction — a collec-
tion of estimates for each of the underlying parameters associated with
the schedule. The parameter estimates are designed to coincide with the
actual schedule parameters. This means that duration time estimates, for
example, are computed for individual job classes (such as jobs of the same
type, at the same site location, and falling within the same periodicity).
➏The Feedback Loop
The current generation of parameter estimates is fed back into the genetic
scheduler and the resource constraint scheduler to produce the next-
generation schedule. The actual completion statistics for this schedule
are then used to compute or reﬁne the parameters, thus completing the
feedback loop.
Adaptive optimization of the underlying scheduling parameters allows
the scheduler to respond dynamically and continuously to the actual
performance of crews, as well as to the actual estimates for job time
duration. This dual approach to optimality provides unequaled power
and ﬂexibility.
10.10
Review
Multiobjective and multiconstraint scheduling problems are members of
a larger class of computationally intensive and computationally difﬁcult

480
■
Chapter 10
Genetic Resource Scheduling Optimization
problems. These problems take many forms and appear in various guises
in a broad spectrum of industries, as well as in policy formulation and
military strategic decision making. Examples include route planning,
production and materials requirement scheduling, integrated circuit fab-
rication design, network topology optimization, and containerization
and packing. The resource-constrained crew scheduling application in
this chapter illustrates many of the important features and capabilities of
evolutionary systems — features that when combined with fuzzy system
modeling allow system architects and designs to build robust, ﬂexible,
adaptive components to corporate and agency applications. From this
chapter you should now be familiar with the following.
■
The concepts behind resource-constrained scheduling
■
How to represent schedules in a varying-length genome
■
How to organize and implement objective functions as well as hard
and soft constraints
■
The way a genetic system is partitioned into two major object com-
ponents: the GA and the underlying scheduler (or other complex
allocation, scheduling, or conﬁguration system)
■
How to measure ﬁtness, convergence, and the effect of constraints
on the problem as the genetic search engine works toward a solution
■
Techniques and algorithms for implementing precedence and other
topology constraints
■
How to apply the basic principles of the crew scheduler to a broad
family of similar problems to solve similar objectives
The core technologies and methods used in the crew scheduler are found
throughout the modern enterprise, government agencies, and the mili-
tary. This chapter has provided a way of examining the application of
GAs in a structured problem domain in much the same way that the fuzzy
SQL application provides a way of examining the use of fuzzy logic in a
well-focused, structured domain (see Chapter 6).
Further Reading
■
Goldberg, D. E. “A Note on Boltzmann Tournament Selection for Genetic Algo-
rithms and Population-Oriented Simulated Annealing,” in Complex Systems,
Volume 4, pp. 445–460, Complex Systems Publications, 1990.

Further Reading
■
481
■
Goldberg, D. E. Genetic Algorithms in Search, Optimization, and Machine
Learning. Reading, MA: Addison-Wesley, 1989.
■
Goldberg, D. E. “Real-coded Genetic Algorithms, Virtual Alphabets, and Block-
ing,” in Complex Systems, Volume 5, pp. 139–167, Complex Systems
Publications, 1991.
■
Goldberg, D. E., and K. Deb. “A Comparative Analysis of Selection Schemes
Used in Genetic Algorithms,” in Foundations of Genetic Algorithms, G. J. E.
Rawlins (ed.), pp. 69–93, San Mateo, CA: Morgan Kaufmann Publishers,
1991.
■
Goldberg, D. E., and J. Richardson. Genetic Algorithms with Sharing for
Multimodal Function Optimization,” in Genetic Algorithms and their Appli-
cations: Proceedings of the Second International Conference on Genetic
Algorithms, pp. 41–49, 1987.
■
Goldberg, D. E., K. Deb, and J. H. Clark. “Genetic Algorithms, Noise, and the
Sizing of Populations,” in: Complex Systems, Vol. 6, pp. 333–362, Complex
Systems Publications, Inc., 1992.
■
Krishnakumar, K., “Micro-Genetic Algorithms for Stationary and Non-Stationary
Function Optimization,” SPIE: Intelligent Control and Adaptive Systems,
Vol. 1196, Philadelphia, PA, 1989.
■
Syswerda, G., “Uniform Crossover in Genetic Algorithms,” in: Proceedings of the
Third International Conference on Genetic Algorithms, J. Schaffer, (Ed.),
pp. 2–9, Los Altos, CA: Morgan Kaufmann Publishers, Los Altos, CA, 1989.

THIS PAGE INTENTIONALLY LEFT BLANK

■■■
Chapter 11
Genetic Tuning of Fuzzy
Models
Fuzzy models come in two broad ﬂavors: those in which the underlying
fuzzy sets represent the semantics of the model and those in which the
underlying fuzzy sets represent a somewhat arbitrary partitioning of the
data space in each variable. The pricing model (see Chapter 5) is an exam-
ple of a semantic-based fuzzy model. The fuzzy models generated by the rule
induction process (see Chapter 8) represent the latter class of fuzzy mod-
els. Fuzzy control systems represent models that straddle the two extremes.
Because fuzzy models are highly dependent on the term sets underlying
each variable and on the method of aggregation and defuzziﬁcation, they
are often difﬁcult to design and tune. In this chapter we examine the ways
in which a genetic tuner can optimize the architecture of a fuzzy model
created through the rule induction process.
11.1
The Genetic Tuner Process
The genetic tuner optimizes the architecture of a fuzzy model by creating
a population of new parameter speciﬁcations for the rule induction pro-
cess. The parameters are encoded in the chromosome as real numbers
representing the controls for the rule induction mechanism. Each chro-
mosome represents the deﬁnition of a complete fuzzy system. Figure 11.1
schematically illustrates the overall ﬂow and control mechanism for the
genetic tuner.
➊Generate and Initialize the Genetic Algorithm
The initial phase of the genetic tuner involves the design and imple-
mentation of a chromosome population to represent a set of parameters
for inducing, executing, and evaluating a collection of rules (the fuzzy
KB). For example, a fuzzy model that predicts investment risk from a
483

484
■
Chapter 11
Genetic Tuning of Fuzzy Models
Standard
Error
Run
Fuzzy Model
Parameters
1.0
0
x
µ
Outcome
Genetic
Optimizer
Training Data
Goodness
of Fit
continue?
Breed
New
Population
Stop
Rule
Induction
Run Rules
on Validation Data
Validation Data
Measured
Errors
 
Fuzzy
Knowledge Base
 
Yes
Rule 1
Rule 2
Rule 3
Rule 4
Rule n
No
1
2
3
4
5
6
7
Figure 11.1
The fuzzy model genetic tuner.
Rule Induction Parameters Chromosome
Client_Age 
Client_Income
Investment_Risk
#Sets
Shape
%Overlap
#Sets
Shape
%Overlap
#Sets
Shape
%Overlap
Figure 11.2
A simple rule induction chromosome.
client’s age and income has three variables (Client_Age, Client_Income,
and Investment_Risk). Figure 11.2 shows how a chromosome in the
parameter population might appear.
This chromosome speciﬁes for each variable the number of fuzzy sets
(#Sets), the shape of the fuzzy set (triangular, trapezoidal, bell-shaped, and
so forth, encoded as an enumerated value), and the percentage of overlap
on each fuzzy set.1 From this chromosome structure a random popula-
tion of possible values is created, producing the initial population of fuzzy
system models. At the same time, of course, a set of constraints must be
developed. General constraints include not only restrictions on the fuzzy
set shape values (the random number must be in the interval [1 . . . n],
1 Naturally, a more complex genome structure is possible (see the following section on Objective
Function).

11.1
The Genetic Tuner Process
■
485
where n is the number of possible fuzzy set shapes), but constraints
(for example) on the number of fuzzy sets and the amount of overlap. The
minimum number of fuzzy sets might be held to three, and the maximum
perhaps to some number that reﬂects the magnitude of the underlying
domain. The overlap percentage should be constrained to the interval
[10,80].
In the case of a simple performance optimization model, the objective
function is easy to specify. The model must minimize the standard error
of estimate for the model’s outcome variable (see “Evaluate Goodness of
Fit”). Although accuracy is the primary objective of any tuning process,
other contingent objective functions are possible (such as minimizing the
number of fuzzy sets or minimizing the number of variables).2
➋Rule Induction
Each chromosome deﬁnes a way of inducing rules from the data. Rule
induction reads the associated training data ﬁle and, using the parame-
ters in the current chromosome, creates the fuzzy KB. In the simplest
case, the parameters deﬁne the number and shape of fuzzy sets in each
variable’s term set. This KB contains the if-then rules derived from the
underlying data.
➌Run the Fuzzy Model
Once the fuzzy KB has been created through rule induction, the rules are
executed against the validation data ﬁle. The validation ﬁle compares the
outcome generated by the rules against the actual value for each observa-
tion (record) in the validation ﬁle. The difference is the degree of error in
the estimate.
➍Accumulate Measured Errors for Each Instance
In this step the tuner measures the degree of error between the estimated
and the actual values for each record or instance in the validation ﬁle. The
standard error of estimate is used to ﬁnd the accuracy of the current model.
2 A genetic tuner can also provide variable selection or principal component analysis for an
outcome variable by randomly selecting a collection of independent variables from the complete
population of possible independent variables. The combination of variable with the minimum
standard error of estimate is the best model.

486
■
Chapter 11
Genetic Tuning of Fuzzy Models
Expression 11.1 deﬁnes this measure.
eest =
!N
i=1 (xa
i −xe
i )2
N
(11.1)
Here,
e
is the error of estimate for the current model
N
is the total number of observations in the validation ﬁle
xa
is actual value of outcome for the i-th observation
xe
is estimated value of outcome for the i-th observation
When the estimate is close to the actual, the variance will be small. Thus,
a small standard error in the model indicates a fuzzy system conﬁguration
with good predictive power.
➎Evaluate Goodness of Fit
The standard error is a measure of how well a particular fuzzy model per-
forms in predicting an outcome value. The smaller the standard error the
better the model. In most instances this is sufﬁcient to assign a goodness-
of-ﬁt value to a chromosome. After a fuzzy model, generated from a
chromosome’s parameters, has computed the standard error from the
validation data ﬁle, this standard error is stored in the chromosome as the
genome’s goodness of ﬁt.
➏Decide Whether to Evolve More Models
When the current population of chromosomes has been evaluated, each
chromosome will have a goodness-of-ﬁt value indicating how well the
model performed. Now some exit condition must be evaluated. This can
be a maximum number of generations, but is more often a measure of
the improvement in the overall population. This improvement, shown in
Expression 11.2, is based on the average ﬁtness in the population.
fa = 1
N
N

i=1
fi
(11.2)
Here,
fa
is the average population ﬁtness
N
is the total number of chromosomes in the population
fi
is the ﬁtness of the i-th chromosome

11.1
The Genetic Tuner Process
■
487
The average ﬁtness is a measure of convergence in the population. If the
genetic tuner is working correctly, the average should improve over time
(the average standard error should approach zero). We can then decide
to terminate the genetic tuner when there is no more improvement in
the average chromosome ﬁtness. That is, we can terminate when the
GA has reached a steady state. Expression 11.3 shows how a termination
condition can be established.
fd = 1
N
"N−1

i=1
( f a
i −f a
i+1
#
fd ≈M
(11.3)
Here,
fd
is the average change in the population ﬁtness
fa
is the average ﬁtness of a population for the speciﬁed generation
N
is the number of generations over which to compute the change
in the average ﬁtness
M
is the change tolerance (a value, such as .005, indicating that if
the difference in average population ﬁtness over the previous
N generations is not greater than M then the population has
reached a steady state)
For a reasonably well-designed and reasonably performing GA, a measure
of convergence toward a single value is preferable to terminating the
algorithm after a speciﬁc number of generations. A maximum generation
threshold, however, should almost always be a fail-safe condition on the
genetic tuner.
➐Breed a New Population of Fuzzy Model
Conﬁgurations
If a termination condition is not satisﬁed, a new population of fuzzy
model conﬁguration parameters is created from the current population.
Some portion of the best chromosomes is retained, some of the chromo-
somes are bred through crossover to produce new chromosomes, and a
few chromosome genomes are randomly mutated to produce new vari-
eties of models in the population. This is the core of the genetic tuning
process.

488
■
Chapter 11
Genetic Tuning of Fuzzy Models
11.2
Conﬁguration Parameters
A conﬁguration parameter tells the rule induction mechanism how to
decompose variables into fuzzy sets, how to evolve the rules, and how to
execute the rules. Tuning is the process of ﬁnding the set of conﬁguration
parameters that will create the best fuzzy model (one that has the highest
degree of predictive accuracy). Tuning is, however, interpreted differ-
ently for different types of fuzzy systems. For semantic models, the tuning
options are somewhat restricted and at a basic level include the following.
■
Adjustments to the shape of fuzzy sets
■
The degree of overlap in adjoining fuzzy sets
■
The method of correlation and aggregation
■
The defuzziﬁcation technique
The options are restricted to a large extent because the underlying fuzzy
sets in semantic models represent the knowledge of SMEs and have been
created by knowledge engineers to reﬂect this knowledge. Thus, we can-
not arbitrarily add or remove fuzzy sets, change the overall shape of a
fuzzy set, or change the degree of overlap between semantically related
fuzzy concepts without altering the semantics and therefore the operation
of the system.
The tuning aspects are particularly important and more varied in fuzzy
models that are used to discover patterns in data and convert these into
rules. This is due to the number of principal architecture components
controlling the system’s performance. These components include the
following.
■
The number of fuzzy sets
■
The shape of the fuzzy sets
■
The overlap of fuzzy sets
■
The skew (asymmetry) of fuzzy sets
■
The compactness (or range) of the fuzzy sets
■
The method of correlation and aggregation
■
The defuzziﬁcation technique
Each of these components has a signiﬁcant effect on the performance of
the fuzzy system. By changing the organization of the fuzzy sets underlying

11.2
Conﬁguration Parameters
■
489
the independent and dependent (solution) variables, we can ﬁnd a combi-
nation that provides the correct amount of decomposition or granularity
and the correct shape for the component fuzzy sets.
Fuzzy Set Granularity
Figure 11.3 shows one possible representation of the variable Client_Age
in a portfolio risk model automatically broken down into a number of
equally spaced fuzzy sets.
This type of fuzzy set representation corresponds to the default decom-
position method used in the Wang-Mendel fuzzy rule induction algorithm.
The number of fuzzy sets speciﬁed for each variable determines the ﬁne-
grain resolution of the rules. Rules are induced from the intersection of the
fuzzy sets underlying each variable. How much of the decision terrain is
effectively mapped by each rule depends on how precisely each rule maps
the relations between variables. For example, a rule induced from invest-
ment portfolios that gained or lost value might be represented as follows.
If Client_Age is A03
And Client_Income is I03
Then Investment_Risk is R15
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
75
0
1.0
A01
A02
A03
A05
A04
Client_age
Figure 11.3
The decomposition of Client_Age.

490
■
Chapter 11
Genetic Tuning of Fuzzy Models
Then, the degree to which a client age data value belongs to the A03 fuzzy
set and the degree to which a client income data value belongs to the I03
fuzzy set indicates the degree to which Investment_Risk is a member
of the R15 fuzzy set. These form a patch of fuzzy regions representing
the actual fuzzy rules. Figure 11.4 illustrates the fuzzy relationships for
this rule.
As Figure 11.4 illustrates, the region over the variable’s domain sub-
tended by the fuzzy set directly affects the speciﬁcity of the rule. If fuzzy
sets span large portions of the domain, the rules are less responsive to
patterns that fall within these regions. We can see this by examining the
same relationships with a less ﬁne-grain set of fuzzy sets. As Figure 11.5
illustrates, the region covered by the intersection of A03 and I03 is much
larger.
The size of the fuzzy region is the resolving power of the rules. Large
regions are more likely to obscure patterns because they will be lost in
the intrinsic noise associated with the span of data points underlying
each fuzzy set. As we increase the number of fuzzy sets, the resolving
power increases, but only to a certain limit. If the granularity is too ﬁne,
as illustrated in Figure 11.6, the rule induction process will miss larger
patterns and ﬁnd only random noise.
Client_Age
Client_Income
I01
I02
103
I04
I05
106
0
1
A05
A04
A03
A02
m(x)
0
1
m(x)
A01
R15
Investment_Risk
Figure 11.4
A rule as fuzzy relationships.

11.2
Conﬁguration Parameters
■
491
Client_Age
Client_Income
I01
I02
I03
I04
0
1
A04
A03
A02
m(x)
0
1
m(x)
A01
R09
Investment_Risk
Figure 11.5
A rule with insufﬁcient fuzzy relationships.
Client_Age
Client_Income
I01I02 I03
I14
0
1
A03
A02
m(x)
0
1
m(x)
A01
R05
Investment_Risk
A13
Figure 11.6
A rule with too many fuzzy relationships.

492
■
Chapter 11
Genetic Tuning of Fuzzy Models
Finding the proper level of granularity or decomposition is one of
the principal objective functions in tuning a fuzzy model. The genetic
tuner creates a random number of fuzzy sets placed equidistantly across
the variable’s UoD. In fact, this will be the objective of the fuzzy system
tuning example in this chapter.
Fuzzy Set Compactness and Skew
Compactness and skew are connected properties and are related to the
distribution of fuzzy sets across the domain of the variable. In the ordinary
way of decomposing a variable into fuzzy sets, a uniform partitioning of
the variable’s range is used to create N symmetric fuzzy sets, arbitrarily
named and spaced from left to right. Yet in many models some parts of the
range of values require a ﬁner granularity than other parts (for example, a
fuzzy model that evaluates health care requirements might need to have a
higher distribution of fuzzy sets around the middle-aged and senior range
of values in order to discriminate among various types of risks or services).
Figure 11.7 illustrates an example of a patient’s age variable with a change
in fuzzy set compactness.
Grade of membership m(x)
Patient age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
75
0
1.0
A01
A02
A03
A07
A06
A05
A04
Patient_Age
Figure 11.7
A variable with varying fuzzy set compactness.

11.2
Conﬁguration Parameters
■
493
Related to compactness is the concept of fuzzy set skew or asymmetry.
One or more fuzzy sets must be asymmetric in order to create a variable
term set with differing degrees of compactness (as you can see with fuzzy
sets A01 and A02 in Figure 11.7). In some models, an even higher degree
of skew is necessary to properly represent the patterns in the underlying
data. Figure 11.8 illustrates the patient age variable decomposed into
asymmetric fuzzy sets with a general focus around middle-aged values.
Changing the distribution of fuzzy sets across the domain of a variable
is often a powerful pattern-discovery mechanism and should be one of the
secondary parameter controls in the rule induction engine. Combining the
number of fuzzy sets with the idea of compactness and skew, however,
requires a signiﬁcant and somewhat complex change in the way automatic
fuzzy set partitioning is accomplished.
Fuzzy Set Shape
With the exception of the number of fuzzy sets, no other parameter
has such a direct impact on model performance as the shape of the
underlying fuzzy sets. Traditional control theory applications generally
use triangular fuzzy sets due to their ease of construction and evaluation.
Grade of membership m(x)
Patient age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
75
0
1.0
A01
A02
A03
A05
A04
Patient_Age
Figure 11.8
A variable with highly skewed fuzzy sets.

494
■
Chapter 11
Genetic Tuning of Fuzzy Models
Grade of membership m(x)
Policy holder age (in years)
10
15
20
25
30
35
40
45
50
55
60
65
70
0
1.0
A01
A02
A03
A04
A05
Client_Age
Figure 11.9
The trapezoid decomposition of Client_Age.
Business, industry, and policy fuzzy systems generally use bell-shaped or
trapezoidal fuzzy sets. But the nature of how a variable is decomposed into
fuzzy partitions depends on a fuller understanding of how the data is being
used and the types of fuzzy relationships (or rules) we hope to induce
from the data. Figure 11.9 shows the Client-Age variable decomposed
into a collection of trapezoid fuzzy sets.
Although bell-shaped fuzzy sets provide a smoother and more robust
partitioning than triangular sets, trapezoid fuzzy sets are often used when
the model has (or the knowledge engineer expects that it will have) classes
of values rather than continuous values across the domain. The use of
trapezoids can force a subset of values in the domain to become clustered
in plateaus and thus act as classes (so that, for example, all ages close to
40 years old will have the same membership value, which forms a class
and causes rules to treat all of these values as essentially the same value).
11.3
Implementing and Running the Genetic
Tuner
A genetic tuner must be connected to the both rule discovery mecha-
nism and the fuzzy model execution engine in such a way that it can

11.3
Implementing and Running the Genetic Tuner
■
495
repetitively generate new systems and determine their level of perfor-
mance. In this chapter we brieﬂy examine a tuner that is connected to a
fuzzy data mining and modeling tool. A tuning mechanism can be used to
adjust the performance of any fuzzy model, including those based solely
on semantic fuzzy sets derived by SMEs. A genetic tuner connected to a
rule discovery facility provides a powerful mechanism for evolving fuzzy
models with a diverse range of objective functions. In this section we
examine a single objective: to ﬁnd the best decomposition of variables
into fuzzy sets.
The Temperature Prediction Model
The genetic tuner will optimize the temperature prediction model dis-
cussed in Chapter 8. As a review, the model learns the seasonal variations
in annual temperatures in the Midwest of the United States from several
years of data. The model then predicts the average temperature for a given
month. Expression 11.4 shows this as a functional relationship.
tm = f (mi)
(11.4)
Here,
tm
is the average temperature for the speciﬁed month
mi
is a month index in the range 1 . . . 12
f( )
is a function that maps the month index to a temperature
The idea behind rule induction is to evolve or discover this functional rela-
tionship. The relationship in the fuzzy temperature prediction model is
expressed through a series of rules that deﬁnes regions on the underlying
solution surface. Each region is deﬁned by a fuzzy if-then rule. Figure 11.10
illustrates how the temperature and month fuzzy sets decompose the vari-
ables so that the seasonal periodicity of the temperature trend line can be
deﬁned (and later discovered).
As Figure 11.10 illustrates, as the temperature and time axes are decom-
posed into fuzzy sets a collection of “patches” emerges that deﬁnes the
underlying function. In this case, the fuzzy relation (T05,M05) represents
a region on the function in the area bounded by the domain of the tem-
perature fuzzy set T05 and the domain of the month fuzzy set M05. The
number of fuzzy sets along the two axes determines the ﬁne-grain detail of
the function. Figure 11.11 illustrates this schematically by overlaying the
implicit month-to-temperature function with a small set of overlapping
fuzzy regions.
Each patch on the function line represents a potential rule. Whether a
rule appears for the patch depends on whether a data point exists in the

496
■
Chapter 11
Genetic Tuning of Fuzzy Models
Temperature
Time (month)
M05
0
1
T05
m(x)
0
1
m(x)
Rule?
Temperature Prediction
Figure 11.10
Fuzzy representation of the t = f(m) function.
training data for the patch (fuzzy relationship). Whether a data point gen-
erates a fuzzy relationship depends on the number of observations in the
training data corresponding to the ﬁne-grain granularity of the variables.
Unlike the rectangular rule space of a conventional FAM, which produces
an implicit rule for each combination of fuzzy sets in the variables, the rule
induction process generates rules from the training data, thus producing
highly effective rules only in those regions of the problem space that have
representative data. Figure 11.12 illustrates the connection between rule
induction and the target fuzzy model.
In Figure 11.6, the data points aligned with the month-to-temperature
function provide the exemplars for each generated rule. Note that the
region speciﬁed by the relation (T06,M07) lacks any data points. No rule
is generated for this point. If we increase the width of the time fuzzy sets,
relations (T06,M06) and (T06,M07) could be merged and would have
data points to generate a rule. However, the speciﬁcity in this case would
be reduced.
The objective of a fuzzy tuning mechanism, then, is to ﬁnd the gran-
ularity of fuzzy sets that corresponds to the data and at the same time to
provide the highest degree of generality and speciﬁcity (often conﬂicting

11.3
Implementing and Running the Genetic Tuner
■
497
Rule?
Rule?
Rule?
Rule?
M03
M05 M06 M07
Temperature
Time (month)
Temperature Prediction
T03
T05 T06
m(x)
m(x)
1
1
0
0
r(T05,M05)
r(T03,M03)
r(T06,M06)
r(T06,M07)
1
2
3
4
Figure 11.11
Fuzzy patches overlaying the month-to-temperature function.
objectives). In this genetic tuning example, we are concerned with only
the second objective: reducing the standard error or estimate by ﬁnding
the rules with highest degree of speciﬁcity.
Because the temperature prediction model itself is relatively simple, it
makes it rather easy to explore the ways in which random fuzzy systems
are created, processed, and evaluated to ﬁnd this functional relationship.
This section builds on three previous ideas: fuzzy systems, rule induction,
and GAs.
The Parameter Chromosome
The genome structure for the genetic tuner consists of a single value
for each variable: the number of fuzzy sets. Because this is a simple GA
with an equally simple objective function (minimize the standard error
of prediction), the tuner uses a binary (bit) encoding for the number.
Figure 11.13 shows the structure of the chromosome.

498
■
Chapter 11
Genetic Tuning of Fuzzy Models
Temperature Prediction
1
0
Rule?
Rule?
Rule?
Rule?
M03
M05 M06 M07
Temperature
Time (month)
T03
T05 T06
m(x)
m(x)
1
0
r(T05,M05)
r(T03,M03)
r(T06,M06)
r(T06,M07)
3
4
Figure 11.12
Data points on the function surface.
Chromosome 
Fitness
Temperature
Month
Figure 11.13
The genetic tuner chromosome.
The genome locus for each fuzzy set count consists of 5 bits, giving
a possible value of (2 5) −1 or 31 fuzzy sets per variable. The ﬁtness is
stored as a real number. Each chromosome represents the term set size for
the associated variables. The rule induction system will decompose the
variables into this speciﬁed number of 50% overlapping triangular fuzzy
sets. A collection of rules is derived from this conﬁguration. Figure 11.14
is an example of the initial genome population for the tuner.

11.3
Implementing and Running the Genetic Tuner
■
499
Chromosome
Fitness
Fuzzy Set Counts
1
0011100010
2
1010000001
3
1000100110
4
0111110010
5
0101011101
6
0101111010
7
0010111110
8
0011111110
9
1001101110
10
1011111101
Figure 11.14
An initial population of genes.
Each bit string genome encodes the count of fuzzy sets for the model
variables. In the ﬁrst chromosomes, for example, [11100] (or 7) is the
number of fuzzy sets in the temperature’s term set, whereas [01000]
(or 2) is the number of fuzzy sets in the month’s term set. Each value
is decoded using a simple power-of-2 accumulator to retrieve the deci-
mal value. Listing 11.1 is the algorithm for making this binary to decimal
conversion.
Let bits = incoming chromosome bit string
Let value = Returned decimal value
set value
= 0
set N
= length of(bits)
set powerOf2
= 1
set i
= 0
repeat while(i < N)
if bits[i] = "1"
set value = value + powerOf2
powerOf2 = powerOf2 * 2
i= i + i
End repeat
Return(value)
Listing 11.1
Decoding a chromosome bit string.

500
■
Chapter 11
Genetic Tuning of Fuzzy Models
The bit length of the chromosome automatically provides a constraint
on the maximum number of fuzzy sets we can assign to any variable, thus
maintaining an upward limit on the degree of granularity. This model does
not ensure that the minimum number of chromosomes is greater than 1
(or some other small number), but this would be a reasonable constraint
on the tuning algorithm.
Setting Up Initial Genetic Parameters
We start the tuning or optimization of the temperature model by spec-
ifying a population size (the number of initial chromosomes) and the
maximum number of generations. A general control panel, shown in
Figure 11.15, handles all high-level parameters for the tuner.
The control mechanism for genetic tuning also establishes several
operating characteristics for the GA. These include a measure of the dif-
ference between one degree of ﬁtness and the next (the sensitivity to
changes in the standard error of estimate), the crossover probability rate,
Figure 11.15
The genetic tuning parameters screen.

11.3
Implementing and Running the Genetic Tuner
■
501
and the mutation probability rate. The type of selection strategy and the
type of population scaling are also speciﬁed on this control screen.
Understanding the Objective Function
The GA now attempts to minimize the standard error of estimate by gener-
ating a series of fuzzy systems with the month and temperature variables
partitioned into differing numbers of fuzzy sets. The number of fuzzy sets
associated with a variable determines the number of possible rules. The
number of rules, as discussed earlier, determines the level of granularity,
which in turn determines broadly the number of rules that can be dis-
covered. The GA uses the standard error of estimate as its goodness-of-ﬁt
measure and attempts to ﬁnd the balance of fuzzy sets in the variables
that minimizes the error. Examining the outcome of the genetic tuner for
a few genomes in one generation illustrates this point. Listing 11.2 is part
of the audit log from a tuning run.
The standard error for this model is relatively low due to the coverage
of fuzzy sets over the problem space. Figure 11.16 takes the conﬁgu-
ration represented by this model and illustrates its features in terms of
fuzzy relationships. Each connection between a month and a temperature
represents a possible rule.
At the same time, a less dense representation of the problem space
(in terms of fuzzy set coverage) means that fewer rules can be discov-
ered. Each rule covers a broader region of the problem space and has
less speciﬁcity. Listing 11.3 is another chromosome from a tuning run
with a smaller, randomly selected, number of fuzzy sets for each variable.
GENERATION: 1
GENOME: 1
GA--Gen: 1 RUN FUZZY SYSTEM...
GA--[01] Clear Fuzzy System
GA--[02] Create Fuzzy System (Genome=1101110110 )
Temperature 11101---Term Set: 23
Month
10110---Term Set: 13
GA--[02-1] END--Create Fuzzy System
GA--[03] Evolve IF-THEN Rules
GA--[03-1] END--Evolve IF-THEN Rules. Rule Count= 293
GA--[04] Execute Fuzzy System
GA--[04-1] END--Execute Fuzzy System. Std Error= 0.128
Listing 11.2
An extract from a genetic tuning audit log.

502
■
Chapter 11
Genetic Tuning of Fuzzy Models
Temperature Prediction
1
0
Temperature
Month
Rule?
m(x)
m(x)
1
0
Figure 11.16
A dense fuzzy representation of the problem space.
GENOME: 2
GA--Gen: 1 RUN FUZZY SYSTEM...
GA--[01] Clear Fuzzy System
GA--[02] Create Fuzzy System (Genome=1111010100 )
Temperature 11110---Term Set: 15
Month
10100---Term Set:
5
GA--[02-1] END--Create Fuzzy System
GA--[03] Evolve IF-THEN Rules
GA--[03-1] END--Evolve IF-THEN Rules. Rule Count= 38
GA--[04] Execute Fuzzy System
GA--[04-1] END--Execute Fuzzy System. Std Error=4.405
Listing 11.3
An extract from a genetic tuning audit log.

11.3
Implementing and Running the Genetic Tuner
■
503
Temperature Prediction
1
0
Temperature
Month
Rule?
m(x)
m(x)
1
0
Figure 11.17
A sparse fuzzy representation of the problem space.
The standard error for this model is large due to the sparse coverage
of fuzzy sets over the problem space. Figure 11.17 takes the conﬁguration
represented by this model and illustrates its features in terms of fuzzy rela-
tionships. Again, each connection between a month and a temperature
represents a possible rule.
The GA generates many possible conﬁgurations, with many degrees
of granularity in the underlying fuzzy sets. It is guided toward the cor-
rect balance by a simple objective function: minimize the standard error
of estimate (the difference between the actual outcome and predicted
outcome in the validation data ﬁle).
Executing the Genetic Tuner
Figure 11.18 shows the steady decrease in the average standard error
as the algorithm converges on a proper number of fuzzy sets for each
variable.
Without increasing genetic diversity in a small number of chro-
mosomes over a relatively small number of generations, the optimal

504
■
Chapter 11
Genetic Tuning of Fuzzy Models
Figure 11.18
The standard error (performance) trace.
conﬁguration can be missed. This means that the amount of variability
in the genome drives the average standard error close to a single value
too quickly over a small number of generations. Figure 11.19 shows an
example of this situation.
In Figure 11.19, the standard error of estimate began at a larger value
(the “luck of the draw” in a GA, presumably a fuzzy system conﬁgura-
tion with fewer fuzzy sets than required for a higher degree of precision).
In a small number of generations, the average ﬁtness drops quickly to
roughly 4.5, and remains steady at this value for the remainder of the
generations. Without increasing genetic diversity in a small number of
chromosomes over a relatively small number of generations, the conver-
gence can be accelerated but the optimal conﬁguration can be missed.
This means, as reﬂected in Figure 11.19, that the amount of variability in
the genome keeps the average standard error close to a single value over
a small number of generations.
A genetic tuner provides a fast and ﬂexible method of partitioning vari-
ables into a collection of fuzzy sets so that a balance is achieved between
generality and speciﬁcity. By seeking to minimize the standard error of
estimate, a genetic tuner can quickly ﬁnd the best partitioning for fuzzy

11.4
Advanced Genetic Tuning Issues
■
505
Figure 11.19
Early convergence with higher genetic variability.
models with even a large number of variables. In this example, we have
explored the partitioning of the temperature prediction model because it
provides a tight coupling between the issues covered in the rule induc-
tion chapter and the ideas covered in this chapter on the optimization
of fuzzy systems. A GA, however, can provide a search and optimization
mechanism to ﬁnd the best architecture for systems under a variety of
complex constraints.
11.4
Advanced Genetic Tuning Issues
Tuning or optimization provides a sophisticated tool for exploring the
architecture, processing, and adaptive response of a model. Although
ﬁnding the optimal partitioning of variables in a rule induction engine is an
important capability, the same type of genetic optimization can also easily
ﬁnd a broad spectrum of conﬁguration parameters (see “Conﬁguration
Parameters” at the start of this chapter). These conﬁguration parameters
are focused primarily on the nature of the fuzzy sets associated with a

506
■
Chapter 11
Genetic Tuning of Fuzzy Models
variable. That is, they address the conﬁguration of the variable’s term set.
But more advanced optimization and search techniques can be used to
tune a larger spectrum of model properties.
Principal Component Analysis and Variable
Selection
In many large knowledge discovery (or data mining) projects the choice of
independent variables (and sometimes the choice of a dependent variable)
must be made from a very large collection of possible variables. A rule
induction engine connected to a variable selection GA can search through
a large number of variables attempting to ﬁnd those that minimize the
standard error of estimate. Figure 11.20 illustrates a possible architecture
for a variable selector.
➊Generate and Initialize the Genetic Algorithm
The initial phase of the genetic variable selector involves the design
and implementation of a chromosome population to represent the set
Best models
No
Continue?
Goodness
of  t
Run
fuzzy model
Outcome
1.0
0
µ(x)
x
Measured
errors
Standard
error
Yes
Training data
Rule
induction
Variable selection
Genetic
variable
optimizer
Fuzzy
knowledge base
Run rules
on validation data
Validation data
Rule n
Rule 4
Rule 3
Rule 2
Rule 1
Breed
new
population
1
Genetic
tuner
7
2
3
5
6
4
8
Figure 11.20
A variable selection genetic optimizer.

11.4
Advanced Genetic Tuning Issues
■
507
Chromosome
Fitness 
Variables
1 
3 
8 
17 
2 
2 
23 
3 
7 
12 
17 
21 
34 
4 
5 
8 
13 
19 
: 
n 
2 
3 
5 
11 
35 
48 
Figure 11.21
Chromosome design for variable selection.
of possible independent variables. Perhaps the easiest complement is a
varying-length vector or linked list containing index values into an array
of all possible names. Figure 11.21 illustrates one possible design for the
chromosome.
This has the advantage of simplicity as well as extensibility, allowing
the variable selector to more or less easily select modiﬁcation operators
for the variables. This provides a selection capability somewhat similar to
evolutionary programming, providing the ability to pick log(x), sqr(x),
power(x,y), and similar transformational operators.
➋Run the Rule Induction Engine
Each genome in the population represents a potential model. Assum-
ing y is the dependent (outcome) variable, the ﬁrst chromosome in the
population (see Figure 11.21) represents the possible model shown in
Expression 11.5.
y = f (v3, v8, v17)
(11.5)
Here,
y
is the dependent or outcome variable
vi
is an independent variable in the range 1. . .n, where n is the
maximum number of candidate independent variables
f( )
is a function that maps y to the set of independent variables
From these independent and dependent variables the rule induction
process generates a set of rules in the form
if v3=xi and v8=yi and v17=zi then y,

508
■
Chapter 11
Genetic Tuning of Fuzzy Models
where xi, yi, and zi are fuzzy sets drawn from the variable’s corresponding
term set. The term set is created automatically for each variable from the
default parameters of the rule induction engine (which are based on a
statistical analysis of a randomly selected subset of the data). At this phase
of the process the engine is concerned with deriving a set of operationally
effective rules, not with ensuring that the fuzzy set partitioning is optimal.
➌Run the Fuzzy Model
Once the fuzzy KB has been created through rule induction, the rules are
executed against the validation data ﬁle. The validation ﬁle compares the
outcome generated by the rules against the actual value for each observa-
tion (record) in the validation ﬁle. The difference is the degree of error in
the estimate.
➍Accumulate Measured Errors for Each Instance
In this step the variable selection mechanism measures the degree of error
between the estimated and the actual values for each record or instance
in the validation ﬁle. The standard error of estimate is used to ﬁnd the
accuracy of the current model (see Expression 11.1 for a deﬁnition of this
measure). We should expect that a rule with insufﬁcient variable support
will have a very high standard error of estimate. The objective function
for variable selection, however, involves two conditions.
■
Minimize the standard error of estimate
■
Minimize the number of selected variables
That is, the GA searches for the rule set that has the smallest standard
error with the minimum number of variables. Expression 11.6 is a simple
weighted average objective function to drive the algorithm.
fi =
1
w1 + w2

w1 × eest(i)

+
$
w2 × vi
N
%
(11.6)
Here,
fi
is ﬁtness for the i-th chromosome
w1
is the weight associated with the standard error of estimate
value
w2
is the weight associated with the number of variables used

11.4
Advanced Genetic Tuning Issues
■
509
eest(i)
is the standard error of estimate for the i-th chromosome (see
Expression 11.1 for a deﬁnition of the standard error used
in this context)
vi
is a count of the number of variables used in this model
N
is the total number of variables in the selection pool
With w1 = 1 and w2 = 1, this is the average of the standard error and
the ratio of the variables used to the total possible used. Changing the
weight can bias the ﬁtness function toward the standard error or toward
the number of variables. In any case, for models with the same standard
error of estimate the one with the smaller number of variables will be
preferred.
➎Evaluate Goodness of Fit
The multiobjective ﬁtness function is a measure of how well a particular
fuzzy model conﬁguration performs in predicting the outcome values in
the validation ﬁle. The smaller the ﬁtness function the better the overall
model conﬁguration. After a fuzzy model, generated from a chromosome’s
variable selection, has computed the average standard error from the
validation data ﬁle, this value is stored in the chromosome as the genome’s
goodness of ﬁt.
➏Decide Whether to Evolve More Models
When the current population of chromosomes has been evaluated, each
will have a goodness-of-ﬁt value indicating how well the model per-
formed with a particular number of variables. Now some exit condition
must be evaluated. This can be a maximum number of generations,
but is more often a measure of the improvement in the overall popu-
lation. This improvement (see Expression 11.2 and surrounding text for
a more detailed discussion) is usually based on the average ﬁtness in the
population.
➐Breed a New Population of Fuzzy Model
Conﬁgurations
If a termination condition is not satisﬁed, a new population of variable
conﬁgurations is created from the current population. Some portion of
the best chromosomes is retained, some of the chromosomes are bred
through crossover to produce new chromosomes, and a few chromosome
genomes are randomly mutated to produce new varieties of models in the
population.

510
■
Chapter 11
Genetic Tuning of Fuzzy Models
➑Optimize Fuzzy Set Partitioning for Set of Best Models
When the variable conﬁguration GA is terminated, the population con-
tains the models with the smallest possible number of variables and
the best (minimum) standard error of estimate. These models were
found because they had the best predictive ability based on the default
(statistical-based) fuzzy set partitioning. In order to select the model with
the best functional or predictive generality, the top N% of the models are
selected for fuzzy set partitioning optimization.
Searching for an Optimal Choice of Model
Parameters
Even in semantic models in which the fuzzy sets are derived from expert
knowledge, the parameters associated with the type and size of fuzzy sets
as well as measures of performance are often estimates. These estimates
can, in many instances, be optimized through a genetic search process
that measures the overall performance of the model against one or more
calibration or global performance metrics. In Chapter 5, we discussed the
competitive pricing model. This model, shown in Listing 11.4, has four
principal rules.
Unlike most of the models discussed in previous chapters, the pricing
model was not derived from data and as a result does not have a training
and validation set. Its price prediction must “make sense” based on the
expert pricing knowledge of the experts who built the model.
Search the Problem Landscape
The goal of this model is to ﬁnd a price that satisﬁes a set of default bound-
ary constraints (speciﬁed by a set of unconditional rules) and to balance
the price subject to its relationship to the competition price. That is, as
long as the competition price is not considered very high our price should
be near the competition. Otherwise, our price should be constrained in
our price must be High
our price must be Low
our price must be around 2*MfgCosts
if the competition_price is not very High
then our price should be near the competition_price
Listing 11.4
The fuzzy pricing model.

11.4
Advanced Genetic Tuning Issues
■
511
the space bounded by Low, High, and twice the manufacturing costs.
Figure 11.22 illustrates the movement of price relative to the increase in
competition price and subject to manufacturing costs.
The outcome price essentially follows the competition price until the
competition price begins to be very High, at which point the price falls
away and is constrained by the manufacturing costs. The central dashed
area represents the region over the solution surface at which the land-
scape falls off as the competition price increases relative to the very High
concept.
The Expectancy of Fuzzy Numbers
In this type of model we cannot arbitrarily generate fuzzy sets for High
and Low (in the context of price and competition_price) because they
have shapes that reﬂect a speciﬁc meaning in the model. We can, how-
ever, attempt to ﬁnd the optimal expectancy (e) for the fuzzy numbers
2*MfgCosts and near competition_price. Expectancy is the amount of
tolerance or elasticity that exists in a fuzzy number and is generally mea-
sured as the distance from the center of the fuzzy number out to the edge
of the number (the point where the membership value falls to either zero
or below the alpha cut threshold). Figure 11.23 illustrates several different
expectancies for twice the manufacturing costs.
As a percentage of the value, an expectancy has a broad range of
values, from e = 0 (in which case the fuzzy number is equivalent to an
Price
(outcome)
30
10
20
40
50
Competition price
60
70
80
90
1.00
1.20
1.40 Manufacturing
costs
70
60
50
40
30
20
10
1.60
1.80
2.00
2.20
Figure 11.22
The movement of price in the fuzzy pricing model.

512
■
Chapter 11
Genetic Tuning of Fuzzy Models
1.0
Around 2*MfgCosts
1.2
2.4
3.6
Expectancy
0
Grade of membership m(x)
Figure 11.23
Different expectancy values for 2*MfgCosts.
ordinary crisp number) to e >> 100, in which case the fuzzy number is
spread out over a very large space. Normal expectancies are between 10
and 50% of the number’s value.
The Performance Fitness System
The pricing model parameter optimization system is a form of adaptive
feedback in that the outcome price is dependent on the expectancy mea-
sures of the manufacturing costs and the competition price, which in
turn are modiﬁed by the value of the outcome price. Figure 11.24 is a
schematic representation of the genetic system designed to discover the
best expectancy values for the pricing model.
➊Generate and Initialize the Genetic Algorithm
The initial phase of the genetic parameter optimizer involves the design
and implementation of a chromosome population to represent the set of
possible expectancy values for the around and near hedges (expressed
as a percentage of the current value of the target variable). Given a static

11.4
Advanced Genetic Tuning Issues
■
513
No
Continue?
Goodness
of fit
Run
fuzzy pricing
model
Outcome
1.0
0
µ(x)
x
Product
price
Yes
Change hedge
expectancies
Genetic
fuzzy number
optimizer
Fuzzy
knowledge base
Production data
Rule n
Rule 4
Rule 3
Rule 2
Rule 1
Breed
new
population
1
Stop
6
2
4
5
3
Figure 11.24
The fuzzy number (expectancy) optimization system.
Chromosome
Expectancy (Percentages)
Fitness
Around
Near
1 
5
9
2 
11
3
3 
8
21
: 
n 
13
7
Figure 11.25
Chromosome design for fuzzy numbers.
competition price and a manufacturing cost,3 the chromosome for the
problem (illustrated in Figure 11.25) involves the expectancy values for
the around and the near hedges that convert the manufacturing costs
and the competition costs into bell-shaped fuzzy numbers.
We generate a random population of expectancies between 5 and 50%,
which is generally a reasonable range.
3 This constraint is introduced to simplify the discussion. Production pricing models often involve
price optimization against varying competition and manufacturing costs.

514
■
Chapter 11
Genetic Tuning of Fuzzy Models
➋Run the Fuzzy Pricing Model
Each chromosome in the population represents a pricing model with a dif-
ferent expectancy for the manufacturing costs and the competition_price
variables.
➌Find the Product Price Recommendation
The fuzzy pricing model fuses the conditional and unconditional rules to
generate a recommended price. The outcome price is not only dependent
in the model’s concepts of High and Low pricing but the tolerances for
being around twice the manufacturing costs and near the competition
price. Changes in these tolerances move the recommended cost over the
solution space.
➍Compute and Evaluate Goodness of Fit
The GA minimizes the difference between the competition price and the
recommended price consistent with the constraint that they cannot be
the same. Model optimality is based on minimizing the simple objective
function shown in Expression 11.7.
fi = ∥pi −c∥> 0
(11.7)
Here,
fi
is ﬁtness for the i-th chromosome
pi
is the outcome price for the model, for the i-th chromosome
c
is the competition price
Thus, we are minimizing the difference between the predicted product
price and the value of the competition price subject to the constraint that
the difference must be greater than zero (our price cannot be the same
as the competition). The recommended price must lie just above or just
below the competition (we could preserve the sign of the difference and
thus ensure that the recommended price moves toward but always stays
just below the competition price).
➎Decide Whether to Evolve More Models
In an analysis of the hedge expectancy parameters, we can terminate the
GA based on the usual conditions (number of generations, steady state
of hedge expectancy values, and so forth), but we can also impose a
convergence tolerance on the recommended price. The tolerance stops

11.5
Review
■
515
the search when the price approaches the competition price and meets
a speciﬁc price point value.
➏Breed a New Population of Hedge Expectancy
Conﬁgurations
If a termination condition is not satisﬁed, a new population of hedge
expectancy conﬁgurations is created from the current population. Some
portion of the best chromosomes is retained, some of the chromosomes
are bred through crossover to produce new chromosomes, and a few
chromosome genomes are randomly mutated to produce new varieties of
models in the population.
11.5
Review
This chapter extends the evolutionary strategy concepts introduced in the
previous chapter by focusing on the optimal tuning or reconﬁguration of
a fuzzy system created through the rule induction algorithm. The tech-
nique highlights one of the important optimization capabilities of GAs: the
ability to search through a high number of complex conﬁgurations to ﬁnd
an optimal or near-optimal solution. The chapter has also explored the
general idea of optimizing the performance of any fuzzy system through a
multiobjective and multiconstraint GA. You should now be familiar with
the following.
■
The way fuzzy systems are conﬁgured for optimization
■
The possible objective functions used in tuning and reconﬁguration
■
How the number, shapes, and overlap of fuzzy sets inﬂuence the
performance of a fuzzy system
■
How tuning is balanced between generality and speciﬁcity
■
The ways in which semantic fuzzy models can be tuned or reconﬁg-
ured using GAs
■
How evolutionary strategies can be used to perform variable-selection-
related forms of principal component analysis
The use of GAs to create, tune, and reconﬁgure fuzzy models pro-
vides a powerful and ﬂexible mechanism for building and deploying

516
■
Chapter 11
Genetic Tuning of Fuzzy Models
self-analyzing and self-modifying systems. The adaptive feedback capa-
bilities supported by genetic programming allow models to adjust their
internal controls and operating parameters in response to changes in data.
Further Reading
■
Cherkassky, V., and F. Mulier. Learning from Data: Concepts, Theory and
Methods. New York: John Wiley and Sons, 1998.

Index
about approximation hedge,
85, 86, 260
above restriction hedge, 86,
109, 203
acceptable schedule, 430n
accuracy, precision and,
161–162
ACD, 47–48
acyclical graphs, 462, 472
adaptive feedback
business models and, 21–22
genetic tuning and, 512
rule-based systems and, 56–57
adaptive fuzzy clustering. See
fuzzy adaptive clustering
aggregation
CQIX, 180
deﬁned, 96
fuzzy model and, 271, 282
inference engine, 121–122
Mamdani method, 123
allele, 344
almost contrast hedge, 85
alpha cut thresholds
adaptive fuzzy algorithm and,
254
deﬁned, 68, 96
fuzzy controllers, 111
for fuzzy sets, 88
SQL process and, 190
zero membership predicates
and, 182
ambiguity
Aristotle on, 75
clustering and, 223–224
detecting, 301–303, 306
fuzzy numbers, 82
semantic, 67
analog models, 32–33
analysis models, 4–8
analysis of variance, 38, 39
analytical crossover, 402–403
And (intersection) operator
about, 89–90
adjustable parameters and, 93
control systems, 130
fuzzy associative memory, 129
fuzzy propositions and,
120–121
fuzzy rules and, 103, 281
predicates and, 181–182
premise and, 102
sets and, 74
SQL process and, 190, 198
Standard Additive Model, 136
annealing
mutation as, 360
simulated, 344, 358
anomaly detection, 17, 18
antecedent. See premise
Application Server Pages, 5
approximate reasoning, 97
approximation hedges
about, 85, 86, 260
around, 85, 86, 512–513
close to, 85
conditional fuzzy rules and,
103
fuzzy numbers, 153
near, 85
scalars and, 71, 88
shape limitations, 87
in vicinity of, 85
architecture
control blocks and, 326
crew scheduling, 440–443
expert systems, 111
of genetic algorithms,
365–368
for genetic scheduling,
440–443
for knowledge bases, 113–116
Aristotle, 75, 92
arithmetic crossover, 400–402
around approximation hedge,
85, 86, 512–513
artiﬁcial intelligence, 14, 22
ASCII text strings, 306
ASP, 5
assessment, 120–121
asymmetry. See skew
attributes
cluster, 212–213
models and, 285
auditing, 443, 446
automatic cluster detection, 17,
47–48
availability
business models and, 19–20
deﬁned, 422
resource-constrained
scheduler and, 443,
450–454
of resources, 438–439
B2B activities, 4–5
B2C activities, 4
Bayes’ Theorem, 31
behavior patterns. See patterns
belief function, 266, 279
bell curve
ambiguity detection and, 303
approximation hedges and, 87
fuzzy numbers, 79, 81, 141,
153
fuzzy systems and, 494
new product pricing model,
145
plateaus and, 81
standard deviation and, 38
517

518
■
Index
below restriction hedge, 86
Bezdek, Jim, 228
bifurcation point, 20
binary inversion, 404, 405
binomial distribution, 405
bivalence, law of, 72–76
Boole, George, 69, 74, 75, 170
Boolean logic
crew scheduler and, 453
deﬁned, 68–69
dependency matrix and,
468–469
fuzzy logic and, 67, 76, 88
law of bivalence, 72–76
law of noncontradiction,
75–76
sets and, 170–172
Boolean sets. See crisp sets
boundaries
crisp sets and, 171
fuzzy sets and, 76–77, 92,
171–173
overlapping, 224
BPM, 23–24, 33
breeding
about, 393–403
crew scheduler and, 454
deﬁned, 344
fuzzy models, 487
genetic tuning and, 487, 509,
515
new solutions with, 361
selection and, 351
breeding rate, 417
brittleness
of conventional SQL, 187
fuzziness and, 80
fuzzy models and, 69
where statement, 161
business models
data mining and, 18–23
distributed systems, 5–6
feedback loop and, 6
fuzzy systems for, 23–24
line-of-business, 6–7
prediction and, 33
rule induction in, 25
business-to-business activities,
4–5
business-to-consumer
activities, 4
C4.5 classiﬁer, 49
calendars
adaptive parameter
optimization, 476
deﬁned, 422
master plan and, 441
candidate keys, 155
candidate rules
compression process, 280,
305–306
generating, 275–278
rule induction and, 270, 273,
299, 303–305
CART, 33, 35, 49
CBR system, 26–29
center of gravity
clustering and, 223, 236–238
deﬁned, 208
defuzziﬁcation and, 98, 122
new product pricing model,
147
selecting values, 130–132
stability and, 210
Standard Additive Model, 124
central-and-limits replacement,
405
centroid. See center of gravity;
cluster centers
CGI programs, 5n
CHAID models, 33, 49
chaos, 20–21
chi-squared automatic induction
models, 33, 49
chromosomes
annealing and, 360
deﬁned, 345–347
evaluating, 384–386
genetic tuning and, 487, 515
jobs within, 453
locus of, 370
population and, 350–351, 369
ranking, 361, 364
selection strategies, 388–393
city pivot crossover, 410–411
CIX. See compatibility index
classiﬁcation
decision trees and, 32, 48–51
neural networks as, 35
regression trees and, 33,
35, 49
close to approximation hedge,
85
closure process, 158
cluster analysis
of membership, 224–228
models and, 35, 47–49
principles of detection,
210–211
unsupervised data mining, 60
cluster centers
adaptive fuzzy algorithm and,
255
cluster analysis and, 224
deﬁned, 208
estimating cluster count,
241–242
fuzzy c-means algorithm, 230,
235, 239
as fuzzy numbers, 259–260
iteration and, 209
k-means algorithm and, 218,
220–222
membership and, 259
seeding, 242–244
updating, 232–234
cluster seeds, 208, 218, 241
clustering. See also fuzzy
adaptive clustering; fuzzy
c-means clustering
center of gravity, 223,
236–238
concepts in, 223–228
crisp, 218–223, 224
decomposing variables and,
111
deﬁned, 208
dynamic rule-based models
and, 57
general concepts, 211–218
genetic algorithms, 60–62
measuring distances, 48

Index
■
519
membership and, 233–236,
238
outcomes, 239
vocabulary, 208–210
clusters
attributes of, 212–213
converting into rules, 259–262
deﬁned, 208
detecting, 17–18, 47–48
initial, 241
membership in, 209, 213–216
Codd, Edgar, 159
coefﬁcient estimating, 21
collections of sets, 83–85, 170
columns
composite keys, 151
data elements and, 156
deﬁned, 150
domains and, 151
indexes and, 153
normalization and, 154
primary identiﬁer, 155
rows and, 155
where statements and, 194
common gateway interface
programs, 5n
compactness, 47, 492–493
compatibility index. See also
query compatibility index
assessing robustness, 315–318
detecting robustness, 318–323
evidence and, 314–317
predicate and, 180–184
zero membership and, 184
complement (negation)
operator. See negation
operator
composite key, 150–151
compression tournament.
See tournament
conditional rules
about, 134–136
control systems and, 129
deﬁned, 97
fuzzy systems and, 102–104
knowledge base and, 99, 114
new product pricing model,
142
order of execution, 99, 136
conﬁdence in relationships,
14–16
conﬁguration parameters,
488–494, 503–506
conﬂict set, 278
connectivity, graph theory and,
461–462
consequent fuzzy set, 97,
102–103, 266
constraints
crew scheduling and,
428–439, 444–445
deﬁned, 347
on membership, 248–249
operating area as, 434–435
predecessor relationships and,
432
skills as, 435–437
time, 463–464
constructor, 450n
continuous variables, 34, 49, 67
contrast hedges, 85
control blocks, 325–328, 334
control decomposition, 105
control systems, 109–111,
125–133, 274
control variables, 110, 112
convergence, 347, 487
correlation
deﬁned, 97
degree of, 16
inference engine, 121
Mamdani method, 123
models and, 145, 282
prediction and, 40–43
Standard Additive Model, 124
statistical, 39
crew scheduling
adaptive parameter
optimization, 475–479
architecture, 440–443
constraints in, 428–439,
444–445
deﬁned, 422
fundamentals, 426–428
genome structure, 445–446
network dependency issues,
463–464
process, 446–460
terminology issues, 425
topology constraint algorithms
and techniques, 460–475
crews. See also
resource-constrained
scheduler
adaptive parameter
optimization, 475–476
deﬁned, 422
master plan and, 441
crisp logic. See Boolean logic
crisp numbers (scalars)
deﬁned, 151
defuzziﬁcation and, 122
FSDB and, 329
fuzziﬁcation, 99
hedges and, 71, 85
crisp sets
Boolean logic and, 68,
170–172
deﬁned, 69
fuzzy sets and, 70, 77
law of bivalence, 72–76
crossover
deﬁned, 347–348, 360
dependence on, 365
genetic algorithms and, 355,
393–403, 406–413
genetic tuning and, 487
selection and, 351
setting parameters, 416
crossover rate, 417
customer proﬁling, 26–29
customer proﬁtability analysis,
12, 13
cycles, graph theory and,
462–463
data marts, 8, 12, 24
data mining. See also
knowledge discovery
about, 3
adaptive fuzzy clustering, 258
ambiguity detection, 302
business examples, 13–14,
17–18

520
■
Index
data mining (continued )
business models and, 18–23
decomposing variables and,
111
deﬁned, 12
if-then rules and, 56
mathematical models and, 33
nonparametric statistics and,
38–39
rule induction and, 268
supervised, 16–17, 47, 57–60
target marketing and, 26–29
unsupervised, 17–18, 48,
60–62
data warehouses
distributed systems and, 8, 24
movement toward, 12
rise of, 9–11
databases. See relational
databases
Date, Chris, 159
decay curve, 79
decision models, 6–8, 23–24,
133
decision support systems, 33
decision trees
about, 32
classiﬁers and, 48–51
fuzzy logic and, 95
indexes, 153
as mathematical models, 33
decomposition
genetic tuning and, 489, 492
rules for, 97
variables and, 56, 60,
104–113, 270
deﬁnitely contrast hedge, 85
defuzziﬁcation
centroid and, 232
control systems, 129
deﬁned, 97–98
inference engine, 122
knowledge bases and, 119,
337
models and, 124, 147, 271,
282
Standard Additive Model, 124
Delphi method, 33
dependencies
on crossover, 365
in data, 14
decision trees and, 49
on mutation, 365
in networks, 463–464
representing, 467–468
statistical analysis of, 38
topological sorts and, 471–475
types of, 465–467
dependency graphs
representing, 17, 467–468
storage requirements,
468–470
traversal algorithms, 470–471
dependent variables. See also
outcome variables
adaptive feedback model
and, 21
control systems, 128
data mining and, 16, 17, 57
deﬁned, 267
fuzzy associative memory, 98
fuzzy sets and, 107, 108
genetic tuning and, 507
knowledge bases and, 114
models and, 35, 124–125, 285
as output values, 19
partitioning into fuzzy sets,
273–275
redeﬁnition of, 288
temperature prediction
example, 297–299
dilation, 249
dilution hedges
centroids and, 261
deﬁned, 85–86
distribution and, 254–255
power functions and, 87
rules and, 262
dimensionality
of clustering, 211–212,
216–218
of models, 330, 332
rules and, 260–262
directed graphs, 462, 472
directed knowledge discovery,
16–17, 47, 57–60
discrete variables, 34, 67
dispersion, 261
distribution
binomial, 405
changing for fuzzy sets, 493
dilution hedges and, 254–255
frequency, 50
Gaussian, 405
kurtosis of, 38
normal, 39, 405
partitioning and, 113
Poisson, 405
rules for, 261
skew and, 39
distribution-based replacement,
405
diversity
assessing, 380–382
conventional techniques,
403–406
crew scheduling and, 457
DLL, 325
domain. See also universe of
discourse
columns and, 150
data space partitioning and,
105
deﬁned, 69, 71, 151
dilution and, 86
FSDB and, 328
fuzzy sets and, 77
integrity checks, 9
negation operator and, 92
psychometric scaling of,
82–83
simpliﬁcation of, 17
temperature prediction
example, 297
double-point crossover, 399
do. . .while loop, 188
duration, 422
dynamic link library, 325
e-business, 3–5
early start, 422–423
Easingwood-Mahajan-Muller
diffusion model, 46
edges
deﬁned, 460–461

Index
■
521
dependency graphs and,
468–469, 471
effectiveness, degree of
ambiguities and, 302, 306
computing, 278–279
elasticity
execution window and, 423
of fuzzy numbers, 69
numeric, 67
elitist strategy, 389
entropy, 208–209
equi-joins, 154
error measurement, 485–486,
508–509
estimating. See also standard
error of estimate
cluster count, 241–242
coefﬁcient, 21
feedback loop and, 479
ETL process, 15
Euclidean distance
adaptive fuzzy algorithm,
250–252
cluster membership and,
214–215
deﬁned, 209
fuzzy c-means algorithm, 230,
235
k-means algorithm and, 219,
222
as straight line, 368
Euler diagrams, 73
evaluation process,
chromosomes and, 384–386
event state categorization,
34–35
evident separation, degree of,
83
evolutionary programs, 35,
353–354
excluded middle, law of, 72,
75–76, 92
Exclusive-Or operator, 74
execution window, 423
exemplars, 299–301
expectancy
deﬁned, 69
fuzzy numbers and, 69–70, 80,
261, 511–512
hedges and, 85, 514–515
expert systems
about, 31
control system as, 109–111
data mining and, 14
fuzzy logic and, 95
models and, 33, 35
extract, transform, load, 15
extremely intensiﬁcation
hedge, 86
FAM. See fuzzy associative
memory
FDB, 328–329
feasible schedules
constraints and, 428–429
in crew scheduling, 427–428
deﬁned, 423
resources and, 425, 432, 443
topological analysis and, 472
feasible solutions
constraints and, 445
in crew scheduling, 427–428
deﬁned, 348
predecessor relationships and,
430
feedback
adaptive, 21–22, 56–57, 512
models and, 6
parameter estimates in, 479
Fibonacci numbers, 73
ﬁelds, columns as, 150
ﬁltering
candidate rules and, 277
data mining tools and, 16
QCIX ranking, 204
where statement and, 160
ﬁnish-to-ﬁnish relationship,
465
ﬁnish-to-start relationship,
465–466
ﬁrst normal form (1NF), 154
Fisher-Pry semi-log growth
curve, 46
ﬁtness functions
chromosomes and, 361, 385
crew scheduling and, 454, 457
deﬁned, 348–349
genetic algorithms and, 364,
373–379
late jobs and, 444–445
ﬂoat, 423
foreign key, 152–153, 157
frequency distribution, 50
FSDB, 328–329
fuzziﬁcation, 99, 282. See also
defuzziﬁcation
fuzziﬁcation parameter,
228–229, 230, 234
fuzzy adaptive clustering
algorithm, 248–252
example, 255–258
membership and, 48
normalization and, 252–254
fuzzy associative memory
about, 98–99
code declaration for, 330
control systems, 125–126
deﬁned, 267
example, 118
fuzzy rule base and,
280–281
RDR structure and, 332
rule induction and, 269
fuzzy c-means clustering
about, 228–230
algorithm, 230–238
behavior of, 238–241
cluster count estimate,
241–242
cluster seeds, 241
example, 244–247
fuzzy adaptive and, 249
initial clusters, 241
membership and, 48, 225, 231
Fuzzy Data Explorer, 188, 193,
296, 328
fuzzy logic
about, 67–68
data mining and, 14
deﬁned, 69
fuzzy sets, 76–93
law of bivalence, 72–76
truth values and, 69, 89–90
vocabulary of, 68–72

522
■
Index
fuzzy numbers
cluster centers as, 259–260
deﬁned, 70, 153
expectancy of, 69–70, 80,
261, 511–512
fuzzy sets and, 79–83
genetic algorithms and, 95
new product pricing model,
133, 141, 145
fuzzy proposition, 102, 120, 267
fuzzy quantiﬁers, 70, 77–79
fuzzy rules. See rules
fuzzy set descriptor block,
328–329
fuzzy sets. See also outcome
fuzzy sets; under-generation
fuzzy set
alpha cut thresholds, 88
ambiguity detection, 303
Boolean sets comparison,
170–172
cluster centers, 259–260
collections of, 83–85
combining, 178–180
compactness, 492–493
consequent, 97, 102–103, 266
control blocks and, 326
deﬁned, 22n, 70, 170–171,
194–196
fuzzy logic and, 69
fuzzy numbers, 79–83
genetic tuning and, 488–489
granularity of, 489–492
hedges, 85–88, 144n
indexes and, 333
membership functions, 70,
76–77, 176–177
models and, 56, 57, 59, 126
operators, 67, 88–92
partitioning variables into,
273–275
patterns and, 25, 493
quantiﬁers, 77–79
in queries, 54
redeﬁnition of, 288
representation of, 328–329
rule induction and, 269
skew, 492–493
variable decomposition,
104–113
fuzzy systems
about, 95
fuzzy knowledge base,
113–119
genetic tuning and, 488
inference engine, 119–122
models and, 23–24, 35,
124–147
rule-based, 100–104
variable decomposition,
104–113
vocabulary of, 96–100
Gaussian distribution, 39, 405
generally contrast hedge, 85
generate and test, 354–355
generations
breeding and, 361
crew scheduler and, 455,
457–458
deﬁned, 349
setting parameters, 416
genetic algorithms
about, 353–356
architecture of, 365–368
chromosome selection,
388–393
clustering, 60–62
creating new populations,
387–388
crossover techniques, 355,
393–403, 406–413
evaluating chromosomes,
384–386
execution times and, 408–415
ﬁtness functions, 364,
373–379
fuzzy numbers and, 95
generate and initialize,
483–485, 506–507,
512–513
generate and test, 354–355
generating initial population,
379–384
genome representations,
369–373
models and, 35, 57
mutation techniques, 355,
403–413
Pearl curve and, 44
practical issues, 413–418
setting parameters, 415–418,
449–450
strengths and limitations,
362–365
terminating, 386–387
vocabulary, 344–353
workings of, 356–362
genetic optimizer, 442
genetic scheduling
adaptive parameter
optimization, 475–479
architecture for, 440–443
constraint scheduling,
434–439
fundamentals, 426–428
implementing, 444–446
objective functions and
constraints, 428–433
process, 446–460
topology constraint algorithms
and techniques, 460–475
vocabulary, 421–425
genetic tuning
advanced issues, 505–515
conﬁguration parameters,
488–494
executing, 503–505
implementing, 494–503
process, 483–487
genomes
in crew scheduler, 445–446
deﬁned, 349
encoding, 365
ﬁtness functions and, 377–379
genetic tuning, 497–500
phenotype and, 350
representations of, 372–373
structural design for, 369–372
genotype, 349
Gompertz, Benjamin, 45
Gompertz curve, 45–46, 79
goodness of ﬁt
deﬁned, 349
fuzzy logic and, 95

Index
■
523
genetic tuning and, 486, 509,
514
objective function and, 355
performance and, 350
ranking chromosomes, 361,
364
resource-constrained
scheduler and, 450
survival of the ﬁttest and, 351
granularity
control systems and, 110, 126
execution speed and, 116
FSDB and, 329
of fuzzy sets, 489–492
index values and, 298
of models, 83
rule induction and, 273
of time-series data, 295
graph theory, 460–463
greedy crossover, 407–410
group by statement, 161
growth curves, 44–46, 79
Hartigan, J. A., 49
having statement, 154, 161
hedges. See also speciﬁc hedges
deﬁned, 70–71
expectancy and, 85, 514–515
fuzzy controllers, 111
fuzzy reasoning and, 97
fuzzy sets, 85–88, 144n
linguistic variables and,
108–109
new product pricing model,
133
queries and, 54
rules and, 262
heuristic models, 32–33, 88
hill climbing, 356–359
Holland, John H., 343
Hypertext Transfer Protocol,
5–6
IBM, 160, 162
ID3 algorithm, 49, 50
if-then rules
conditional rules as, 103
control systems, 125, 130
decision trees and, 32
dependent variables and, 267
domains and, 151
expert systems and, 31
fuzzy systems and, 100
independent variables and,
267
knowledge base and, 306
rule indication and, 25
rule induction and, 56–57,
285–286
syllogisms and, 75
in vicinity of approximation
hedge, 85
independent variables
adaptive feedback model
and, 21
CART and, 49
control block for, 330
control systems, 128
deﬁned, 267
fuzzy associative memory, 98
fuzzy sets and, 107, 108
genetic tuning and, 485n,
506–507
hill climbing and, 356
as input values, 19
knowledge bases and, 114,
333
models and, 34–35, 124–125,
285
partitioning into fuzzy sets,
273–275
redeﬁnition of, 288
supervised data mining and,
16, 57
unsupervised data mining
and, 17
index
deﬁned, 153
example, 157–158
rules and, 330
time-series data and, 290–296
variable properties, 326
induction. See rule induction
inference engines
about, 119–122
approaches, 122–123
deﬁned, 119
dynamic systems and, 22–23
knowledge bases and, 287
ordering rules, 114
reasoning process and,
100–101
inner joins, 154
input variables. See
independent variables
intensiﬁcation hedges, 86–87,
262
intensity, 82–83
Internet
advertising on, 26
businesses and, 4
knowledge and, 11
intersection operator. See And
operator
interval displacement, 39
intranets, 4, 11
inverse correlation, 41–42
inversion, binary, 404, 405
inverted pendulum model,
125–133
iteration
adaptive fuzzy algorithm and,
255–258
deﬁned, 209
fuzzy c-means clustering, 238
Java Messaging System, 5
Java Server Pages, 5
JMS, 5
job sequencer, 442
jobs
actual completion statistics,
476–478
adaptive parameter
optimization, 475–476
within chromosomes, 453
deﬁned, 423
ﬁtness functions and, 444–445
master plan and, 441, 447
joins
deﬁned, 153–154
tables and, 157–158
where statement and, 160
JSP, 5

524
■
Index
k-means clustering algorithm,
218–223
k-nearest-neighbor approach, 48
keys. See also primary identiﬁer
candidate, 155
composite, 150–151
deﬁned, 156
foreign, 152–153, 157
KNN approach, 48
knowledge base
about, 113–119
ambiguous rules and,
301–302, 306
creating, 279–281, 286–287
deﬁned, 99
fuzzy associative memory,
98–99
if-then rules and, 306
organization of, 333–337
as repository, 100–101
rule induction and, 269, 270,
272
rule storage and, 332
knowledge discovery. See also
data mining
directed, 16–17, 47, 57–60
evolutionary programs as,
353–354
if-then rules and, 56
information and, 12–14
models and, 25, 33
target marketing and, 26–29
techniques for, 14–16
undirected, 17–18, 48, 60–62
Kosko, Bart, 122, 123, 124
kurtosis, 38, 82, 113
latest ﬁnish, 423–424
Lee, Young-Jun, 248
less than restriction hedge, 86
line-of-business models, 6–7
linear regression
correlation and prediction,
40–43
deﬁned, 38
logistic curve and, 44
linguistic variables
deﬁned, 71
fuzzy sets and, 105–106
hedges and, 85, 108–109
term sets and, 72, 83–85,
107–108
LOB models, 6–7
locus, 349
logistic (Pearl) curve, 44–45, 78
looping, 472
machine learning system, 476,
478–479
Mamdani, Ebrahim, 122
Mamdani inference method,
122–123, 136
Markov chains, 50
master plan, crew scheduling,
441–442, 446–448
mathematical models, 32–33,
35, 354
maximization
aggregation and, 121–122
Mamdani method, 123
new product pricing model,
145
proportional ﬁtness and,
389–390
mean, population data and, 38
mean time between failure, 14
mean time to repair, 14
measurement
of compatibility, 180–187
of distances, 48
error, 485–486, 508–509
of model robustness, 312–323
median, population data and, 38
membership
adaptive fuzzy algorithm,
248–252
candidate rule induction and,
303–305
cluster centers and, 259
clustering and, 224–228,
233–236, 238
in clusters, 209, 213–216
confusion over, 80n
deﬁned, 209
examples, 127, 130
FSDB and, 329
fuzzy c-means clustering, 48,
225, 231
fuzzy propositions and,
120–121
fuzzy sets and, 70, 73–74, 76,
92, 107, 275–277
new product pricing model,
144
QCIX and, 175–176
weight of, 130
zero, 182, 184
membership functions
aggregation and, 96
alpha cut threshold, 68, 96
ambiguity detection and, 303
clustering, 236–238
combining, 180
crisp sets, 69
FSDB and, 329
fuzzy sets and, 70, 76–77,
106–107, 174–177
hedges and, 70–71, 85–87
weighted average value of, 98
Mendel, Jerry M., 269
microprocessor chips, 131
min-max inference technique,
123
minimization, 390, 392
minimum-of-memberships
compatibility, 181–182
minimum QCIX threshold. See
alpha cut thresholds
mixing rate, 400
mode, population data and, 38
models. See also business
models
about, 31–36, 58
analysis, 4–8
based on rules, 265–266
CHAID, 33, 49
classiﬁcation of, 34–35
decision, 6–8, 23–24
deﬁned, 18–19, 32
dimensionality of, 330
evolving, 26–29, 486–487,
509, 514–515
granularity of, 83
heuristic, 88
line-of-business, 6–7
mathematical, 32–33, 35, 354

Index
■
525
measuring robustness,
312–323
rule-based, 56–57, 85, 354
statistical, 33, 40–43
sustainability, 79
temperature prediction,
296–299
validating, 285, 287–288,
306–312, 323
vs. reports, 22–23
modern scientiﬁc method, 207
more than restriction hedge, 86
motion, law of, 50
move-and-insert gene mutation,
412
move-and-insert sequence
mutation, 413
MTBF, 14
MTTR, 14
mutation
as annealing, 360
crew scheduler and, 455
deﬁned, 350
dependence on, 365
genetic algorithms and, 355,
403–413
genetic tuning and, 487
setting parameters, 417
uniform replacement, 405
mutation rate, 417–418
narrative models, 32–33
near approximation hedge, 85,
512–513
nearest-neighbor crossover,
407–410
negation hedges, 86, 143
negation (Not) operator
Boolean logic and, 74, 75
fuzzy logic and, 89, 92–93
network
deﬁned, 424
dependency issues, 463–464
dependency matrix and,
469–470
graphs and, 460
topological sorts and, 472
neural networks
about, 31
as classiﬁcation, 35
data mining and, 14
fuzzy logic and, 95
models and, 33, 51–53
new product pricing model,
114–115, 133–147
nodes
deﬁned, 460–461
dependency graphs and,
468–469, 471
noise
belief function and, 266
business models and, 19–20
deﬁned, 208–209
genetic algorithms and, 362
noncontradiction, law of, 72,
75–76, 92
nonlinear correlation, 42–43
nonlinear growth curves, 44–46
nonlinear time-series model,
289–294
nonlinearity, 20–21, 362–363
nonparametric analysis, 14, 18
nonparametric statistics, 38–39
nonrenewable resources, 424
nonuniform decay, 406
normal distribution, 39, 405
normal form, 154
normalization
adaptive fuzzy algorithm, 250,
252–254
databases and, 156
deﬁned, 154
Standard Additive Model, 124
Not hedge, 86
Not (negation) operator
Boolean logic and, 74, 75
fuzzy logic and, 89, 90–93
objective functions
in crew scheduling, 428–439,
444–445
decomposition and, 492
deﬁned, 350
genetic tuning and, 501–503
goodness of ﬁt and, 355
supervised data mining
and, 16
unsupervised data mining
and, 17
on-line analytical processing, 12
on-line data store, 12
operators. See also speciﬁc
operators
Boolean sets, 73–74
fuzzy reasoning and, 97
fuzzy sets, 67, 88–92
optimal schedule, 430n
optimization techniques
adaptive parameter
optimization, 475–479
genetic algorithms and, 60,
355, 362, 363–364
genetic tuning and, 495,
503–515
Or (union) operator
about, 89, 90–91
adjustable parameters and, 93
conditional fuzzy rules and,
103
fuzzy propositions and,
120–121
Mamdani model and, 136
premise and, 102
sets and, 74
tournament and, 281
order reversal mutation, 413
ordinary statistics, 37–38
outcome fuzzy sets
aggregation and, 96, 121–122
compatibility index and,
314–317
control blocks and, 330
control systems, 129
deﬁned, 99
defuzzifying, 97–98, 122
Mamdani method, 123
new product pricing model,
146, 147
Standard Additive Model, 124
outcome variables. See also
dependent variables
adaptive feedback model
and, 21
consequent fuzzy set, 97
control systems, 129

526
■
Index
outcome variables (continued )
deﬁned, 99
defuzzifying, 98
dependent variables as, 19
fuzzy relations and, 275
genetic tuners and, 485n
models and, 35, 124–125,
285, 316
rules and, 100, 114, 118
tournament and, 268
unconditional rules and, 104
under-generation fuzzy set
and, 99, 102, 103
outcomes
clustering, 239
conditional rules and,
103–104, 136
dependent variables and, 267
evaluating in SQL, 200–204
unconditional rules and, 136
outer joins, 154
outliers, 223, 224, 238–239
output variables. See outcome
variables
parameters
adaptive optimization of,
475–479
adjustable, 93
conﬁguration, 488–494,
503–506
crossover, 416
estimates for, 479
fuzziﬁcation, 228–229, 230,
234
genetic algorithms and,
415–418, 449–450
genetic tuning of, 500–501,
510–515
mutation, 417
partitioning
arbitrary, 112
cluster centers and, 218
clustering and, 225
data space, 105, 111–113
fuzzy logic and, 95
genetic tuning and, 510
ID3 algorithms and, 50
k-means algorithm and, 219
linguistic variables and, 71
temperature prediction
example, 297–298
variables, 112, 273–275, 504
path, 461
patterns
anomaly detection of, 17
cluster analysis and, 47, 210
conﬂict set and, 278
conﬂicting, 302
critical role of, 3
data mining and, 17
discovering, 493
examples of, 13
fuzzy associative memory
and, 269
fuzzy c-means clustering
and, 241
fuzzy sets and, 25, 493
models and, 284, 322
neural networks and, 31,
51–52
noise and, 208–209
overlooking, 8
rule induction and, 272
separation of recurring,
294–295
SQL and, 149
Pearl, Raymond, 44
Pearl curve, 44–45, 78
performance, 350, 495
periodicity, time-series data,
288, 290, 292, 294–295
phenotype, 350
physical models, 32–33
plateaus, 81
Poisson distribution, 405
polynomial regression, 43,
318–323
population
chromosomes and, 350–351,
369
creating new, 387–388
crossover and, 398
deﬁned, 350–351
generating initial, 379–384
nonparametric statistics and,
38–39
ordinary statistics and, 38
sensitivity to, 364–365
population diversity, 380–382
population size, 382–384, 416
position pivot crossover,
411–412
positively contrast hedge, 85
precedence analyzer, 443
precedence relationships
constraints and, 432
crew assignments and, 437
deﬁned, 424
feasible schedule and, 423
feasible solutions, 430
as job property, 441
representing, 467–468
time constraints and, 463–464
topological sorts and, 472
types of, 465–467
precedence strategy, 424
precision
accuracy and, 161–162
control systems and, 110
expectancy and, 69
fuzzy systems and, 122
term set detail and, 115–116
predecessor relationships. See
precedence relationships
predicate
alpha cut thresholds, 68, 96
compatibility index and,
180–184
conditional rules and, 97
deﬁned, 154–155
generating rules, 262
in queries, 170–178
where statement, 154–155,
180, 197–200
prediction
correlation and, 40–43
genetic algorithms and, 60
growth, 7
models and, 33, 34–35,
287–288
robustness and, 312–314
temperature prediction
model, 289–299, 495–497
predictive models, 31–36, 58

Index
■
527
premise. See also predicate
conditional rules and, 103,
136
correlation and, 121
deﬁned, 267
fuzziﬁcation and, 99
fuzzy propositions and,
120–121, 267
And operator and, 281
operators and, 102
truth and, 103
unconditional rules and, 104,
136
primary identiﬁer (key)
columns and, 150
deﬁned, 155
foreign keys and, 152
joins and, 157
normalization and, 154
SQL process and, 190
private virtual networks, 10
proportional ﬁtness, 389–392
psychometric fuzzy sets, 82–83
public variables, 114–115
PVNs, 10
Pythagorean theorem, 209
QCIX. See query compatibility
index
quantiﬁers, 70, 77–79
queries
about, 169–170
combining fuzzy sets,
178–180
expanding scope of, 165–169
measuring compatibility,
180–187
predicates in, 170–178
SQL and, 52–55
vocabulary for, 150–156
query compatibility index
compatibility threshold
management, 187–188
deﬁned, 155
degree of membership as,
175–176
queries and, 54–55, 180–187
SQL process and, 188–193,
200–204
Quinlan, J. Ross, 49
quite dilution hedge, 86
random numbers, 243, 492
random swap mutation, 412
ranking, 392
rapid application development,
328
rather dilution hedge, 86
RDR structure, 331–333
realization, degree of, 82–83
reasoning process
CBR system, 26–29
deﬁned, 97
inference engines, 100–101
recombination. See crossover
records
ﬁltering, 160
rows as, 150
redeﬁnition, 287–288
reduction. See defuzziﬁcation
redundancies, normalization
and, 154
regression. See linear regression
relation, 155
relational databases. See
also SQL
basic concepts, 156–159
information decision models
and, 124
normalization and, 156
searching, 162–165
System R database project,
160
vocabulary, 150–156
relationships. See also
precedence relationships
cluster analysis and, 47
conﬁdence in, 14–16
dependency, 465–467
examples of, 20
expressing, 108
membership functions and, 70
models and, 20–23
random, 40–43
in relational databases,
156–158
rule induction and, 270,
285–286
rules and, 57, 99, 105
SQL and, 149
statistical analysis of, 38
supervised data mining
and, 57
tables and, 150–151
target marketing and, 26–29
unsupervised data mining
and, 60
renewable resources, 424, 438n
reports, models and, 22–23,
55–56
repositories, 8–11, 100–101
resource-constrained scheduler
crew availability and, 450–454
function of, 443
objective function for,
444–445
resources
as constraints, 438–439
crew as, 422
deﬁned, 425
master plan and, 441
nonrenewable, 424
renewable, 424, 438n
restriction hedges, 86, 109, 203
retention rate, 417
risk assessment, 12, 79
RMS error, 260–261
robustness
crossover and, 360
fuzzy numbers and, 70, 80
genetic algorithms and, 365
measuring for models,
312–323
root mean squared error,
260–261
roulette wheel ﬁtness strategy,
389–392
rows (tables)
composite keys, 151
data elements and, 156
deﬁned, 155
normalization and, 154
predicates and, 154
primary identiﬁer, 155
as records, 150, 155
rule, degree of the, 266, 268

528
■
Index
rule descriptor structure,
331–333
rule discovery. See rule
induction
rule induction
conﬁguration parameters and,
488
deﬁned, 25, 271
example, 288–313
external controls, 325–333
genetic tuning and, 485,
494–495, 507
knowledge base organization,
333–337
methodology, 283–288
models and, 55–62, 269–272,
285–286
robustness and, 312–323
rule induction algorithm,
273–282
technical implementation,
323–325
vocabulary, 266–268
rules. See also candidate rules;
conditional rules; if-then
rules; unconditional rules
adaptive feedback model
and, 21
case-based reasoning and,
26–29
cases and, 299–301
control systems, 126, 129–133
deﬁned, 57–58, 99
defuzziﬁcation and, 97–98
degree of effectiveness,
278–279
dynamic systems as, 22–23
fuzzy sets and, 24, 489
generating prototypes,
259–262
indexes and, 333
knowledge bases and, 99,
113–114, 116–119
linguistic variables and, 71
models and, 126, 321–322
with multiple time index
values, 295–296
relationships and, 57, 99, 105
term sets and, 72
from training data, 275–278
truth thresholds and, 103n
S-curve. See sigmoid curves
SAM, 122, 123–124, 136
saturation, 46
scalars. See crisp numbers
scaling
control systems, 129
correlation and, 97, 121
models and, 116
rule degree, 266
Standard Additive Model, 124
schedules. See crew scheduling;
genetic scheduling
schema, 351
second normal form (2NF), 154
secondary identiﬁer, 156
seeding cluster centers,
242–244
selection
deﬁned, 351
extending scope of criteria
for, 165–169
strategies for, 388–393
self-organizing maps, 35
semantic, 177
semantic decomposition, 105
SEQUEL language, 160
server pages, 5
service orders. See jobs
sets. See fuzzy sets
sigmoid curves
fuzzy sets and, 273
Pearl curve as, 44
quantiﬁer sets and, 77–79
of trapezoids, 196
simile models, 32–33
simulated annealing, 344, 358
single-point crossover, 396–398
singleton, 80, 282
skew
curves and, 79
deﬁned, 38
fuzzy numbers, 82
fuzzy sets and, 492–493
partitioning and, 113
population distributions
and, 39
skills
as constraint, 434–435
master plan and, 441, 447
SKU, 28
slightly dilution hedge, 86, 87
SMEs. See subject matter
experts
solution variables, 19, 99, 102
somewhat dilution hedge
adaptive fuzzy algorithm and,
254–255
centroids and, 261
deﬁned, 86
example, 86, 87
sorts, topological, 471–475
span time, 425
speciﬁcity, degree of, 116
spread, degree of, 79–80
spreadsheets, 33, 124
SQL
CQIX, 180
data patterns, 149
evaluating outcomes, 200–204
example, 193–200
fundamentals, 159–161
joins, 153–154
model building, 52–55
process ﬂow, 188–193
where predicates, 197–200
stability, 20, 210
Standard Additive Model, 122,
123–124, 136
standard deviation, 38
standard error of estimate
computing, 309
deﬁned, 268
genetic tuning and, 485n,
501–504
model correction and, 323
start-to-ﬁnish relationship,
465–466
start-to-start relationship, 466
statistical learning theory, 31,
35, 258
statistical moments, 38
Stock Keeping Unit, 28

Index
■
529
storage, dependency matrix
and, 468–470
strategies
decision-making and, 6–8
elitist, 389
precedence, 424
random, 393
selection, 388–393
strength, degree of, 88
strong alpha cuts, 68, 96
Structured Query Language.
See SQL
subject matter experts
data quality and, 279
disagreement and, 133
expert systems and, 31
genetic tuning and, 488
supervised data mining
(knowledge discovery),
16–17, 47, 57–60
supply-and-demand model, 425
supply chain management, 4
support set. See domain
survival of the ﬁttest, 351
syllogisms, 75
system, 351–352
T-norm, 90n
tables
deﬁned, 155
foreign keys, 152–153
joining, 153–154, 157–158
in relational databases, 156
relationships in, 150–151
where statement and, 160
target marketing, 26–29
taxonomy, 35, 77
temperature prediction model,
289–299, 495–497
templates, queries and, 54
term sets. See fuzzy sets
termination
deﬁned, 352–353
of genetic algorithms,
386–387
genetic tuning and, 487, 515
tests
distribution-free, 39
generate and test approach,
354–355
third normal form (3NF), 154
time-series data
line regression equations and,
43
model building example,
288–296
neural networks and, 52
time-series model, 289–294
time-to-schedule constraint, 445
topological sorts, 471–475
tournament
deﬁned, 268
functional rule set and,
305–306
Or operator, 281
rule induction and, 270, 299,
321
as selection strategy, 393
training data
deﬁned, 268
fuzzy rules from, 275–278
models and, 285, 323n, 326
rule induction and, 269, 299
transaction message handlers, 5
transformations, hedges and,
85–88
trapezoids
ambiguity detection and, 303
explicit class intervals and,
273
fuzzy numbers, 79, 81, 153
fuzzy sets and, 493–494
sigmoid curves of, 196
traveling salesman problem
(TSP), 345–347, 366–367,
369
traversal algorithms, 470–471
Treasury Department, 13
trend analysis
about, 32
detecting lack of robustness,
318–323
robustness and, 316–318
time-series data, 288
triangles
approximation hedges and, 87
fuzzy controllers, 111
fuzzy numbers, 79, 80, 153
fuzzy sets and, 273, 493–494
negation operator and, 92
plateaus and, 81
truncation
alpha cut threshold and, 96
control systems, 129
correlation and, 97, 121
Mamdani method, 123
new product pricing model,
145
truth
alpha cut threshold, 68, 88, 96
Boolean logic, 68, 74
chain of custody, 75
conditional rules and, 136
consequent fuzzy sets and, 97
correlation and, 121
fuzziﬁcation and, 99
fuzzy logic and, 69, 89–90
models and, 282
Not hedge and, 86
premise and, 103
support set and, 71
unconditional rules and,
104, 136
unconditional rules
about, 134–136
deﬁned, 100
fuzzy systems and, 104
new product pricing model,
133, 139–140, 142
order of execution, 99, 136
under-generation fuzzy set
aggregation and, 96
deﬁned, 100
defuzzifying, 97–98, 119
outcome variables and, 99,
102, 103
undirected graphs, 462
undirected knowledge
discovery, 17–18, 48, 60–62
uniform crossover, 399–400
uniform replacement, 405
union (Or) operator. See Or
operator

530
■
Index
universe of discourse. See also
domain
alpha cuts and, 88
crisp sets and, 72–73
deﬁned, 72
domain and, 69, 72n
fuzzy sets and, 77
genetic tuning and, 492
overlap and, 71
temperature prediction
example, 297
unsupervised data mining
(knowledge discovery),
17–18, 48, 60–62
UoD. See universe of discourse
usually contrast hedge, 85
validation data
deﬁned, 268
models and, 285, 287–288,
306–312, 323
variables. See also dependent
variables; independent
variables; linguistic
variables
continuous, 34, 49, 67
control blocks and, 326–328
decomposing, 56, 60,
104–113, 270
discrete, 34, 67
fuzzy numbers, 153
fuzzy sets and, 54, 70, 105,
489
genetic tuning and, 506–510
granularity of, 110
inference engines and, 22
initial population size and, 383
knowledge base and, 113, 287
model building and, 285
partitioning, 112, 273–275,
504
public, 114–115
RDR structure and, 333
rule induction and, 269
rules and, 101
solution, 19, 99, 102
statistical analysis of, 38
symbolic, 326
term sets, 72
where statement and, 188
Venn diagrams, 73
very intensiﬁcation hedge,
86, 87
centroids and, 261
Wang, Li-Xin, 269
Wang-Mendel rule induction
algorithm, 60, 269, 489
Watkins, Fred, 124
weak alpha cuts, 68, 96
weighted-average-of-
memberships compatibility,
182–183
weighted average value
defuzziﬁcation and, 282
fuzzy c-means algorithm,
231–232
k-means algorithm and, 220
of membership functions,
98
of outcome fuzzy sets, 122
weighted crossover, 400–402
weighted-product-of-
membership compatibility,
183–184
where statement
in fuzzy query, 54
fuzzy sets and, 194
predicate in, 154–155, 180,
197–200
role of, 160
variables in, 188
World Wide Web. See Internet
Zaheh, Lotﬁ, 69, 76
zero membership, 182, 184

