Learning and Instruction 82 (2022) 101667
Available online 30 August 2022
0959-4752/© 2022 Elsevier Ltd. All rights reserved.
Impact of programming on primary mathematics learning 
Manon Laurent a,*, Rosamaria Crisci b, Pascal Bressoux a,**, Hamid Chaachoua b, C´ecile Nurra a, 
Erica de Vries a, Pierre Tchounikine b 
a Univ. Grenoble Alpes, LaRAC, 38000, Grenoble, France 
b Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000, Grenoble, France   
A R T I C L E  I N F O   
Keywords: 
Computational thinking 
Programming activity 
Mathematics learning 
Primary school 
Randomized trial 
Learning transfer 
A B S T R A C T   
The aim of this study is to investigate whether a programming activity might serve as a learning vehicle for 
mathematics acquisition in grades four and five. For this purpose, the effects of a programming activity, an 
essential component of computational thinking, were evaluated on learning outcomes of three mathematical 
notions: Euclidean division (N = 1,880), additive decomposition (N = 1,763) and fractions (N = 644). Classes 
were randomly assigned to the programming (with Scratch) and control conditions. Multilevel analyses indicate 
negative effects (effect size range −0.16 to −0.21) of the programming condition for the three mathematical 
notions. A potential explanation of these results is the difficulties in the transfer of learning from programming to 
mathematics.   
1. Introduction 
Over the past decade, computational thinking, popularised since 
Wing (2006), has been incorporated into the curricula of a large and 
growing number of countries to meet the societal and economic needs of 
an expanding digital world (Bocconi et al., 2016; Tang et al., 2019; Wing 
& Stanzione, 2016). In primary school, computational thinking is often 
introduced through programming within mathematics classes (Bocconi 
et al., 2016; Paniagua & Istance, 2018). Shute et al. (2017) define 
computational thinking, a generic term, as “the conceptual foundation 
required to solve problems effectively and efficiently (i.e., algorithmi­
cally, with or without the assistance of computers) with solutions that 
are reusable in different contexts” (p.151). Although, according to Shute 
et al. (2017), computational thinking should not be reduced to pro­
gramming, programming is an important way to teach, assess and 
expose students to computational thinking (Scherer, 2016; Scherer et al., 
2019; Weintrop et al., 2016). The spreading of computational thinking 
has been concomitant with the emergence of free visual programming 
languages. They allow overcoming the syntax-related difficulties of 
languages such as Logo. Programming takes place by nesting blocks 
encapsulating pre-programmed lines of code in a logical manner like 
puzzles (Cheng, 2019; Resnick et al., 2009). 
In primary school, computational thinking is often introduced within 
mathematics, because decision makers believe that the close links be­
tween mathematics and computational thinking will positively affect 
learning (Bocconi et al., 2016; Paniagua & Istance, 2018). The general 
skill of programming, as a way of solving problems, is thought to be 
beneficial for mathematical learning (Wing, 2006, 2011). However, 
research on the effects of programming on mathematics learning is 
inconclusive. This study, through a robust methodology based on a 
random controlled trial and a large sample of students (Grades 4 and 5), 
aims at evaluating whether a programming activity is an effective 
learning vehicle for the acquisition of mathematical concepts. 
1.1. Effects of computational thinking on learning 
Two consecutive lines of research studied the effects of computa­
tional thinking on learning. Initial studies in the 1970s focused on 
mathematics learning in the context of Papert’s constructionism and the 
development of the programming language Logo (1980). These studies 
followed Papert’s idea that programming promotes mathematics 
learning by providing an environment that makes mathematical con­
cepts concrete (Clements, 2000; Hoyles & Noss, 1987). Lee (1990) 
showed small but positive effects of programming with Logo on problem 
solving skills and mathematics learning. These studies gradually stopped 
when the teaching of these languages was abandoned due to the 
* Corresponding author. Laboratoire de Recherche sur les Apprentissages en Contexte, Universit´e Grenoble Alpes, 1251 Avenue centrale, BP 47, Cedex 9., 38040, 
Grenoble, France. 
** Corresponding author. 
E-mail addresses: Manon.Laurent@ac-grenoble.fr (M. Laurent), Pascal.Bressoux@univ.grenoble.alpes.fr (P. Bressoux).  
Contents lists available at ScienceDirect 
Learning and Instruction 
journal homepage: www.elsevier.com/locate/learninstruc 
https://doi.org/10.1016/j.learninstruc.2022.101667 
Received 22 July 2021; Received in revised form 30 July 2022; Accepted 9 August 2022   

Learning and Instruction 82 (2022) 101667
2
complexity of their syntax (Benton et al., 2018; Moreno-Le´on & Robles, 
2016). Another series of studies started with the introduction of new 
visual programming languages. These languages may facilitate problem 
solving by allowing students to rely on intuitive and visual approaches 
to access a deeper level of analysis (Papert & Harel, 1991). Furthermore, 
they provide immediate feedback, which has been shown repeatedly to 
promote learning (Hattie & Timperley, 2007). Scratch (Resnick et al., 
2009) is the most widely used visual programming language throughout 
the educational community (Moreno-Le´on & Robles, 2016; Tang et al., 
2019). 
Studies conducted by computer scientists focussed primarily on the 
effects of programming on the development of computational thinking 
skills, without addressing learning effects in the disciplines in which 
computational thinking is embedded (Cheng, 2019; Hickmott et al., 
2017; Moreno-Le´on & Robles, 2016; Shute et al., 2017). These studies 
respond to the demand of international policy makers for the develop­
ment of computational thinking skills for itself. 
Relatively few studies focussed on the effects of programming on 
learning in the main discipline taught. For example, of the 107 studies 
initially collected in their literature review, Moreno-Le´on and Robles 
(2016) found only eight examining the effects of computational thinking 
on learning the disciplines in which it was embedded (of which only 
three were on mathematics). In the review by Hickmott et al. (2017), 
focussing on mathematics, only 27% of the 393 relevant studies were 
explicitly related to mathematics. Among these, only three conducted an 
experimental study to test the effect of computational thinking on 
mathematics learning. These three studies had methodological limita­
tions. None of them proceeded by randomisation of participants and two 
of them were based on a very small sample (e.g. Babbitt et al., 2015; 
Boyce, Campbell, Pickford, Culler, & Barnes, 2011, June). The latter 
study did not involve a comparison between a computational thinking 
condition and a control condition. The third study, by Calao, 
Moreno-Le´on, Correa, and Robles (2015, September) does not give 
enough information on the nature of the post-tests. This led Hickmott 
et al. (2017) to say that “there is a lack of quasi-experimental research 
linking mathematics learning (as evidenced by students’ academic 
performance) with computational thinking.” (p.16). 
To our knowledge, only the ScratchMaths study (Benton et al., 2018; 
Boylan et al., 2018), a two-year intervention with 6,232 pupils aged 
9–11 in England, has a robust design. The study is a randomised 
controlled trial (RCT) that investigates the effects of programming both 
on computational thinking and on mathematical skills. Computational 
thinking skills were assessed once, during the intervention. The pre- and 
post-tests in the ScratchMaths were national assessments (KS1 and KS2 
maths attainment) collected from the National Pupil Database (NPD) in 
2013 for KS1, two years before randomisation, and 2017 for KS2. The 
ScratchMaths intervention report (Boylan et al., 2018) shows little effect 
on computational thinking (0.10 SD) and no effect of programming on 
mathematics learning (0.03 SD). Although this design is interesting for 
understanding the effect of programming in ecological conditions, it 
does not precisely assess whether computational thinking skills benefit 
mathematics learning. First, mathematics learning was assessed via 
general pre- and post-tests which did not constitute a close match with 
the specific content taught during Scratch vs non-Scratch sequences. 
Second, since pre- and post-tests encompassed a two-year period, stu­
dents in the programming condition received both traditional and 
Scratch mathematics lessons. The ScratchMaths study therefore 
measured a global effect and does not inform on the effect of isolated 
programming sequences for teaching specific content. 
1.2. Programming and mathematics: A matter of transfer of learning? 
Computational thinking is considered to be a general intellectual 
skill which, according to the originators, should be easily transferable to 
other domains. For example, Wing states “The educational benefits of 
being able to think computationally–starting with the use of 
abstractions–enhance and reinforce intellectual skills, and thus can be 
transferred to any domain” (Wing, 2011, p.7). Transfer of learning, as a 
crucial phenomenon, has been studied in many domains. Broadly 
speaking, transfer of learning “refers to knowledge applied in new ways, 
in new situations, or in familiar situations with different content” 
(Schunk, 2012, p.24). Transfer is most likely to be successful between 
closely related domains and when it is carried out consciously. Indeed, 
Perkins and Salomon describe near transfer, as opposed to far transfer, 
as a situation in which there is a large overlap between the starting and 
final situations (Perkins & Salomon, 1992; Schunk, 2012). Furthermore, 
they describe mindful or high road transfer (as opposed to low-road 
transfer) involving deliberate identification of a common structure be­
tween the current situation and a past learning situation (Perkins & 
Salomon, 1992; Salomon & Perkins, 1989). But the recognition of the 
common elements between two situations is not self-evident. For 
knowledge to be transferred, students need to move away from the 
surface structure, i.e. the way the activity is presented, to acquire the 
deep structure of the concept being studied (Willingham, 2009). 
High-road transfer has to be facilitated by helping students to recognise 
the similarities between situations (Bransford et al., 2000). 
In many cases, early studies into the transfer of computer program­
ming to other domains have shown negative results (Perkins & Salomon, 
1992). Yet, the argument highlighting the benefits of computational 
thinking in terms of transfer of learning to other disciplines is particu­
larly advocated in relation to mathematics learning, as programming 
and mathematics are considered to be closely related, particularly in the 
area of problem solving. Many researchers argue that this overlap makes 
programming an effective vehicle for learning mathematics (Calao et al., 
2015, September; Grover & Pea, 2013; Lye & Koh, 2014) and suggest a 
strong potential for transfer of learning (Scherer, 2016; Shute et al., 
2017). 
A recent meta-analysis showed some evidence of positive transfer of 
programming to situations involving mathematical skills (Scherer et al., 
2019). However, the programming activities are very diverse (e.g., Lego 
Robotics), some are obsolete (e.g. Logo, Basic) and the mathematical 
skills aggregate problem solving, modelling, achievement in general (e. 
g., measured by course grades), and conceptual knowledge. Thus, there 
is a lack of recent empirical research on transfer of learning from pro­
gramming to specific concepts in mathematics effects (Drot-Delange 
et al., 2019; Scherer, 2016; Shute et al., 2017). 
In mathematics, children often do not recognise the similarities be­
tween specific mathematical problems when presented in different 
contexts (Fuchs et al., 2003). The abstract nature of mathematics re­
quires the use of multiple representational registers, e.g. symbolic 
algebra, graphs, proportionality tables, etc. (Duval, 1999). A mathe­
matical object, or concept, can be represented in different registers, each 
allowing specific operations. One of the aims of teaching mathematics is 
to enable students to choose the appropriate register to solve the prob­
lem, and be able to transition from one register to another when 
necessary. Duval (2000) coined the term “conversion” and asserted that 
it is an essential step in achieving in-depth understanding of mathe­
matical concepts (Duval, 2000, 2006). 
Computer language is a system of representation (Bråting & Kil­
hamn, 2021). Thus, the conversion of an algorithm into mathematical 
writing for solving a given problem should promote the acquisition of 
the deep structure of the mathematical concept. Such a change in reg­
ister, from algorithm to mathematical writing, meets the definition of 
transfer of learning, specifically near transfer, as both situations concern 
the same mathematical concept. Nevertheless, conversion is not obvious 
for many students as many of them are unable to recognise the same 
concept represented in different registers. They almost have to relearn 
for the new register what they have learned in the original register in 
order to use their knowledge. Difficulty of recognition hinders conver­
sion and is at the root of learning difficulties in mathematics (Duval, 
2000). In coherence with the findings on high-road transfer (Salomon & 
Perkins, 1989; Sweller et al., 1998), conversion has to be enhanced by 
M. Laurent et al.                                                                                                                                                                                                                                

Learning and Instruction 82 (2022) 101667
3
the teacher through explicit guidance (Duval, 2006) by highlighting 
similarities between algorithms and mathematical notations (Bransford 
et al., 2000). 
Moreover, results on the effect of programming to learn mathematics 
should be viewed with caution because among the majority of studies 
interested in the effects of the use of programming through mathe­
matics, lots are targeting the effects on computational thinking skills and 
not on mathematical learning. Among the few that are targeting math­
ematical learning, few are randomized controlled trials based on 
empirical measures of learning outcomes in mathematics (Hickmott 
et al., 2017). 
1.3. The current study 
Despite these mixed results, programming might be a good vehicle 
for learning mathematics (Clements, 2000; Hoyles & Noss, 1987; Shute 
et al., 2017; Voogt, Fisser, Good, Mishra, & Yadav, 2015). Multiple 
registers are essential for learning mathematics (Duval, 1999, 2000). 
Thus, we expect a positive effect of programming on learning, in 
particular when a mathematical concept can be easily expressed in both 
an algebraic and an algorithmic register. This algorithmic register (vi­
sual programming language) would allow students get immediate 
feedback (Hattie & Timperley, 2007). Moreover, it corresponds to the 
theoretical supposition according to which programming allows 
exploring mathematical concepts, abstract in essence, through their 
concrete manipulation (Papert, 1980; Papert & Harel, 1991). 
In the current study, three mathematical concepts were selected for 
their potential to create an algorithmic programming (Scratch) variant 
of the traditional mathematic algebraic register. Two groups, a pro­
gramming and a control group worked on these same three mathemat­
ical concepts. In the programming group, students start by writing an 
algorithm expressing the mathematical concept. They subsequently 
manipulate the elements of the problem in Scratch. Finally, with the 
help of the teacher, the constructed algorithm is converted into the 
algebraic register. In order to favour high-road transfer, teachers were 
instructed to stress the relations between algorithmic and algebraic 
registers. In the control group, students express the mathematical 
concept in the traditional algebraic notation. 
To assess the effect of the instructional sequence on mathematics 
learning, this study uses a rigorous methodology designing a large-scale 
RCT. The effect of programming on learning each of the three concepts 
was evaluated through pretests and post-tests just before and just after 
the relevant activities. The tests targeted assessment of the specific skills 
taught during the interventions for both the programming and the 
control group. 
2. Method 
2.1. Participants and random assignment method 
The current study is based on fourth and fifth grade students whose 
teachers volunteered to a call for teachers from the Grenoble district, 
France (all were accepted) to participate in 2017–2018. An a priori 
analysis showed a minimum of 117 classes required for reaching a 
Minimum Detectable Effect of 0.20. A total of 109 classes participated. 
Post hoc analysis shows a Minimum Detectable Effect of 0.17 for 
Euclidean Division, 0.24 for Additive Decomposition, and 0.28 for 
Fractions (see Supplementary Materials A). Schools were used as a unit 
for random assignment to the two groups: programming (28 schools, 68 
classes) and control (18 schools, 41 classes). 
The programming group initially intended to experiment an out-of- 
class activity that was not implemented. Thus, a slightly larger num­
ber of schools took part in the programming condition. Two teachers 
dropped out for medical reasons (one in each condition). Signed 
parental consent was obtained for the students. Students without 
parental consent were excluded from the analyses. The third sequence 
concerned fifth grade only. This reduced the final sample to 2,472 stu­
dents (1,235 girls; 1,172 Grade 4; 1,296 Grade 5; theoretical age of 9–11 
years), 107 classes from 46 schools (see Table 1). Among them, 36 
classes were in high priority education areas (i.e. low social index and 
low achievement areas in which schools obtain additional resources). 
2.2. Design of the instructional activities 
The content in terms of mastery of mathematical operations was the 
same in both groups and varied only in instructional approach: pro­
gramming in Scratch versus traditional activities. Comparable instruc­
tional sequences were designed for learning to construct and master 
operations specific for each of three mathematical concepts: Euclidean 
division, additive decomposition and fractions. For example, Euclidean 
division requires constructing an equation in mathematical writing such 
as: a = b*q + r. A sequence consisted of three 50-min sessions about the 
same mathematical concept.  
• In the control group, a traditional activity involved cutting a ribbon 
into several parts. For example, cutting a 23 cm ribbon into several 5 
cm parts leaving one remaining 3 cm part. Students then have to 
write it in an algebraic formula: 23 = (4*5) + 3. 
• In the programming group, the activity involved building an algo­
rithm to move a cursor on a number strip to reach a target number, 
using several jumps of specified length and one shorter wildcard 
jump. For example, reaching the number 23 by using several jumps 
of 5 and a single wildcard jump of 3. The expected algorithm was 
“repeat 4 times [advance 5 steps]; advance 3 steps” as shown in 
Fig. 1. Teachers were instructed to implement an explanation phase 
in which they had to explicitly show how to convert the algorithm 
into the same algebraic formula: 23 = (4*5) + 3. 
Both groups spent the same amount of time on the mathematical 
concepts through either programming or traditional activities. Tradi­
tional activities were paper-and-pencil based involving additional ma­
terial for Euclidean division (ribbon and scissors) and fractions (unit 
ribbon with fractions). At the end of the Euclidean division and 
decomposition sequences, both groups performed a paper-and-pencil 
problem-solving task in the traditional way. 
2.3. Measures 
2.3.1. Pre and post tests for mathematics learning (T1, T2, T3, T4, T5, T6, 
T7) 
Students completed mathematical tests at seven measurement times: 
a general test before the study (T1), a pre- and a post-test for each 
concept: Euclidean division (T2, T3), additive decomposition (T4, T5) 
and fractions (T6, T7). All tests were based on the Ministry of National 
Table 1 
Characteristics of the groups.   
Programming 
Control 
(N Classes = 67, N 
Students = 1,519) 
(N Classes = 40, N 
Students = 953) 
M 
SD 
% 
M 
SD 
% 
Teachers 
Women 
– 
– 
79 
– 
– 
92 
Age 
42.0 
8.8 
– 
45.4 
8.8 
– 
Seniority in teaching (years) 
16.2 
8.4 
– 
16.4 
8.7 
– 
Highest education level (years) 
3.7 
1.3 
– 
4.2 
1.3 
– 
Classes 
Priority area 
– 
– 
30 
– 
– 
38 
Students 
Girls 
– 
– 
50 
– 
– 
49 
Older than appropriate age 
– 
– 
5.5 
– 
– 
6,1 
Younger than appropriate age 
– 
– 
3.5 
– 
– 
1,5 
K4 
– 
– 
45 
– 
– 
51  
M. Laurent et al.                                                                                                                                                                                                                                

Learning and Instruction 82 (2022) 101667
4
Education’s assessments. The general test measured three key skills 
acquired at the end of Grade 3: number knowledge, calculation and 
problem solving. 
For each of the three concepts, the items of the pre- and post-tests 
were designed to measure the same skill. For example, items for 
Euclidean division involved distributing entities over groups, e.g. 448 
stamps in groups of 32 stamps per page (pretest item), or 468 pupils in 
groups of 36 pupils per bus (post-test item). The tests for Euclidean di­
vision contained 7 items, those for additive decomposition contained 8 
items and those for fractions 10 items. Each test produced a global score, 
which was centred and reduced in order to facilitate comparisons of the 
effect size across concepts. 
2.3.2. Data collection 
From October 2017, the student questionnaires and the general 
performance test (T1) were administered to the whole class, under the 
supervision of their teacher. Teachers completed online questionnaires 
during the same period (T1). Then the mathematics sequences began 
according to the progression prescribed. Each of the three concepts had a 
pretest (T2, T4, T6) and a post-test (T3, T5, T7), administered by the 
teachers in their classrooms as a whole class. Affective and motivational 
measures not relevant to the current research question were also taken 
at T1 and at the end of the experiment. 
2.3.3. Student and teacher questionnaires (T1) 
Questionnaires were submitted to collect socio-demographic infor­
mation about students and teachers. Concerning students, their gender 
was collected (a dichotomous variable coded 1 if girl, 0 if boy), year of 
birth, grade level (a dichotomous variable named "Grade 5", coded 1 
when student’s grade is 5, 0 when student’s grade is 4), and if they were 
in the appropriate level for their age (coded in three modalities: −1 if 
older than appropriate age, 0 if appropriate, 1 if younger than appro­
priate age). Concerning teachers, their age was collected, gender (a 
dichotomous variable coded 1 if a woman, 0 if a man), seniority in 
teaching, city where the school is located, highest level of education 
after the baccalaureate (six modalities: 0, 2, 3, 4, 5 or 8 indicating the 
number of years of study after the baccalaureate), whether they worked 
in a priority education area (a dichotomous variable coded 1 if priority 
education area, 0 otherwise), their scientific background, a dichotomous 
variable (coded 1 if the teacher had studied in scientific tracks such as 
computer science, mathematics, science and 0 otherwise), and the group 
their class belongs to (coded 0 if control group and 1 if programming 
group). 
2.4. Procedure 
The intervention took place from October 2017 to February 2018 
(approximately four months), in schools of the Grenoble district 
(France). Teachers in both groups received a 3-h training by members of 
the research team from computer science and mathematics didactics. 
The training consisted in a presentation of the mathematical concepts 
and instructional sequences. Training both groups also helps to prevent 
the Hawthorne effect as much as possible. The teachers of the pro­
gramming group received an additional 6 h of training to become 
familiar with Scratch. 
Teachers were given links to download the documentation specific to 
their group sequence by sequence. For each sequence, it provided about 
10 pages of pedagogical details for the teacher and 6 pages of exercises 
for the students. The documentation detailed the expected progression 
for each of the three concepts: order of the learning sequences (first 
Euclidean division, then additive decomposition and finally fractions), 
number of sessions per concept (3), duration of the sessions (50 min), 
relevant vocabulary to be used, instructions for the students, expected 
procedures, students’ sheets for the exercises requiring a written trace, 
students’ organisation by dyads in both groups (usual format under 
regular conditions, suitable for working in a computer environment), 
and the teachers’ role. 
The teachers of the programming group received computers equip­
ped with Scratch for their class, one computer for every two students. 
While the researchers were not present during the sessions, the pro­
cedure itself was carried out under their control. For example, pre- and 
post-tests were systematically collected before and after the sequences. 
3. Results 
3.1. Data analysis 
The three dependent variables studied are the centred and reduced 
scores of the post-test of each of the 3 concepts (T3, T5 and T7). The 
variable of interest is the learning condition with programming. The 
multilevel statistical model approach (using IBM SPSS Statistics 25.0 
software) was used because of the nesting of students within classes. 
These analyses make it possible to attribute the explanation of depen­
dent variables to the correct level (Bressoux, 2010). The schools were 
not included as a level of analysis because the intraclass correlation 
coefficients were low (0.050 for Euclidean division, 0.040 for additive 
decomposition and 0.028 for fractions) and the between-school variance 
was not significant. 
Fig. 1. Screenshot of an exercise in programming condition 
Note. Only the programming area and the display area are used for problem solving. The other areas (the strip on the left not presented here and the frame on the 
bottom right) are part of the environment, but are not used. 
M. Laurent et al.                                                                                                                                                                                                                                

Learning and Instruction 82 (2022) 101667
5
The model specification procedure implied four steps: 1) Estimation 
for each concept of an empty model, in order to estimate the within-class 
variance and the between-class variance, 2) Addition of level 1 variables 
(student characteristics), 3) Addition of level 2 variables (class charac­
teristics), 4) Addition of the experimental variable (programming versus 
control). At each stage, non-significant variables were removed from the 
model in order to fit parsimonious models explaining the final perfor­
mance of each concept. 
3.1.1. Control variables at level 1 
Prior knowledge was controlled for by the two initial mathematics 
scores for each model (the general entry test and the pretest for each 
concept) as well as by the student’s grade and class level relative to their 
age. The model incorporated gender since being a girl tended to nega­
tively affect performance in mathematics. 
3.1.2. Control variables at level 2 
Class composition was controlled using both means of class on the 
entrance and pretest of the concept analysed, and class heterogeneity, 
measured by the standard deviations of the classes on these same tests. 
Belonging to a priority education area was controlled as well as variables 
relating to the characteristics of the teachers: gender, seniority in the 
profession, highest level of education after the baccalaureate, and sci­
entific background. 
3.1.3. Missing data 
The percentage of missing data for the variables included in the 
analyses ranged from 0.48% to 12.58% for the Euclidean division, 
0.04%–16.04% for the additive decomposition and 0.23%–33.59% for 
the fractions (Grade 5 students only). For each concept, Little’s test was 
nonsignificant indicating that data were missing completely at random 
(MCAR). As a result, no imputation procedure was performed and the 
missing data were removed from the samples. 
Thus, the sample used for the Euclidean division, includes 91 classes 
(56 from the programming group), representing 1,880 students. The 
complete sample used for the additive decomposition includes 84 classes 
(49 from the programming group), representing 1,763 students. The 
complete sample used for the fractions includes 38 classes (25 classes 
from the programming group), representing 644 students. 
3.2. Descriptive and correlational analyses 
Descriptive statistics of the variables involved in the models are 
presented in Table 2 for the Euclidean Division, Table 3 for the Additive 
Decomposition and Table 4 for the Fractions. The Cronbach’s alphas of 
the general entry, pretest and post-test were calculated and report good 
reliability (0.72 < α < 0.83). 
The correlations between the variables measuring performance at 
the seven measurement times shows, as expected, positive, strong and 
very significant relationships between them (see Table 5). The score of 
the general entry test is strongly related to each of the pretests of the 
three mathematics concepts (r = 0.41 to r = 0.56). This relationship is 
slightly stronger with each of the post-tests of the three concepts (r = .49 
to r = 0.58). Finally, the correlations between the pretests and post-tests 
of each of the three concepts are strong (r = 0.58 to r = 0.69). 
3.3. Multilevel model analyses 
Whatever the models studied (Euclidean division, additive decom­
position or fraction), presented in Table 6, the greatest part of variance is 
between students. Thus, 81% of the differences can be explained by 
within-class differences for Euclidean division, 78% for additive 
decomposition and 76% for fractions. 
3.3.1. Explained variance 
The pseudo-R2 calculated from the final models compared to the 
empty models, tell for each concept, the part of variance explained by 
the models at each level of variance. For Euclidean division, the final 
model explains 44% of the within-class variance and 74% of the 
between-class variance. For additive decomposition, the model explains 
41% of the within-class variance and 48% of the between-class variance. 
For fractions, the model explains 48% of the within-class variance and 
79% of the between-class variance. The proportion of within-class 
variance explained is globally identical for all models. On the other 
hand, if the between-class variances are quite similar between Euclidean 
division and Fractions, it is less important for additive decomposition. 
Variables that were not significant were removed from the models. 
At level 1, this was the case for the gender of the students, for all three 
concepts. Grade was removed from the additive decomposition model 
(and not relevant for the fraction model with Grade 5 only). Most level 2 
variables were removed from the models: all class composition variables 
(class average and heterogeneity) and most of the variables concerning 
teachers’ characteristics (gender, scientific background, highest level of 
education, age and seniority as a teacher). 
3.3.2. Level 1 control variable effects 
Whatever the concept studied, the pretest score of the concept ana­
lysed affects positively and significantly the final performance. One 
additional standard deviation from the pretest score of the concept 
analysed is accompanied on average, by an increase of 0.44 SD in the 
Table 2 
Descriptive Statistics for Euclidean Division.  
Variable 
Total 
Programming 
Control 
Alla 
Alla 
(N = 1,880) 
(n = 1,120) 
(n = 760) 
M 
SD 
M 
SD 
M 
SD 
Min 
Max 
1.General Entry score(T1) 
0.00 
1.00 
0.02 
0.99 
−0.03 
1.01 
−4.32 
1.19 
2.Pretest (T2) 
0.00 
1.00 
0.05 
1.03 
−0.07 
0.95 
−1.12 
2.10 
3.Post-test (T3) 
0.00 
1.00 
−0.03 
1.00 
0.04 
1.00 
−1.29 
1.79 
4.Grade 5 
.52 
.50 
.54 
.50 
.48 
.50 
0 
1 
5.Level-appropriate age 
.02 
.28 
.01 
.29 
.04 
.27 
−1.00 
1.00 
5.Girl 
.51 
.50 
.51 
.50 
.51 
.50 
0 
1 
7.Class average entry test(T1) 
0.00 
1.00 
0.05 
0.93 
−0.07 
1.10 
−2.77 
1.73 
8.Class average pretest(T2) 
0.00 
1.00 
0.09 
1.06 
−0.13 
0.88 
−1.69 
3.39 
9.SD entry test (T1) 
2.68 
0.69 
2.72 
0.73 
2.63 
0.63 
1.04 
4.84 
10.SD pretest (T2) 
1.86 
0.45 
1.91 
0.40 
1.80 
0.51 
0.68 
2.81 
11.Priority area 
.29 
.45 
.27 
.45 
.32 
.47 
0 
1 
12.Woman Teacher 
.89 
.32 
.87 
.33 
.90 
.30 
0 
1 
13.Teacher seniority 
17.47 
8.17 
17.23 
8.24 
17.81 
8.06 
1 
35 
14.Teacher highest level of study 
3.80 
1.21 
3.65 
1.23 
4.02 
1.14 
0 
8 
15.Teachers’ scientific background 
.32 
.47 
.32 
.47 
.31 
.46 
0 
1  
a Same value for Min and Max for both groups. 
M. Laurent et al.                                                                                                                                                                                                                                

Learning and Instruction 82 (2022) 101667
6
final score for Euclidean division, 0.46 SD for the additive decomposi­
tion and 0.53 SD for fractions. Similarly, whatever the concept studied, 
the general entry score positively and significantly affects the final 
performance. This is a smaller effect than that of the pretest of the 
concept analysed (ranging from 0.20 SD to 0.29 SD). 
The grade the students belong to, has a significant impact in 
Euclidean division model: Fifth grade students have higher post-test 
scores than fourth grade students (0.18 SD). Students who repeated 
grades are −0.20 SD in the final performance for the Euclidean division 
and −0.18 SD in the final score of the additive decomposition, and made 
no contribution to the analysis of fractions. While this loss is very sig­
nificant for Euclidean division and additive decomposition, descriptive 
analyses however show that more than 97% of students are on time. 
3.3.3. Level 2 control variable effects 
Belonging to a priority education area causes a significant loss of 
0.15 SD from the final score on average for Euclidean division and a 
significant loss of 0.40 SD for fractions. 
3.3.4. Effect of the experiment 
The effect of the programming group is systematically significant and 
negative for all final models. Belonging to the programming group leads 
to a significant decrease in final performance for each concept in com­
parison to the control group. This decrease is 0.16 SD for Euclidean 
division, 0.19 SD for additive decomposition and 0.21 SD for fractions, 
all other things been equals. It explains 10% of the between-class vari­
ance of the final score for Euclidean division, 6.3% of this same variance 
for additive decomposition and 17% of this same variance for fractions. 
Table 3 
Descriptive Statistics for Additive Decomposition.  
Variable 
Total 
Programming 
Control 
All a 
All a 
(N = 1,763) 
(n = 1,008) 
(n = 755) 
M 
SD 
M 
SD 
M 
SD 
Min 
Max 
1.General Entry score(T1) 
0.00 
1.00 
0.02 
0.99 
−0.03 
1.02 
−4.30 
1.18 
2.Pretest (T4) 
0.00 
1.00 
0.03 
1.01 
−0.04 
0.99 
−1.20 
3.21 
3.Post-test (T5) 
0.00 
1.00 
−0.06 
0.99 
0.078 
1.00 
−1.51 
1.93 
4.Grade 5 
.52 
.50 
.55 
.50 
.48 
.50 
0 
1 
5.Level-appropriate age 
−.02 
.28 
.00 
.29 
−.04 
.27 
−1.00 
1.00 
6.Girl 
.51 
.50 
.52 
.50 
.50 
.50 
0 
1 
7.Class average entry test(T1) 
0.00 
1.00 
0.04 
0.91 
−0.05 
1.11 
−2.77 
1.66 
8.Class average pretest (T4) 
0.00 
1.00 
0.06 
0.90 
−0.09 
1.12 
−1.41 
2.01 
9.SD entry test (T1) 
2.64 
0.68 
2.68 
0.71 
2.60 
0.63 
1.04 
4.74 
10.SD pretest (T4) 
1.56 
0.44 
1.65 
0.40 
1.44 
0.47 
0.80 
2.35 
11. Priority area 
.29 
.45 
.27 
.44 
.31 
.46 
0 
1 
12.Woman teacher 
.89 
.32 
.88 
.33 
.90 
.30 
0 
1 
13.Teacher seniority 
17.10 
8.56 
17.32 
8.43 
16.79 
8.79 
2 
35 
14.Teacher highest level of study 
3.76 
1.15 
3.62 
1.26 
3.96 
0.95 
0 
8 
15.Teachers’ scientific background 
.31 
.46 
.34 
.48 
.26 
.41 
0 
1  
a Same value for Min and Max for both groups. 
Table 4 
Descriptive statistics for Fractions.  
Variable 
Total 
Programming 
Control 
All a 
All a 
(N = 644) 
(n = 409) 
(n = 235) 
M 
SD 
M 
SD 
M 
SD 
Min 
Max 
1.General entry score(T1) 
0.00 
1.00 
0.01 
0.98 
−0.01 
1.03 
−4.66 
1.04 
2.Pretest (T6) 
0.00 
1.00 
−0.02 
1.02 
0.04 
0.98 
−1.38 
2.78 
3.Post-test (T7) 
0.00 
1.00 
−0.09 
0.95 
0.15 
1.07 
−1.56 
1.97 
4.Level-appropriate age 
−.02 
.28 
−.01 
.29 
−.05 
.27 
−1 
1 
5.Girl 
.49 
.50 
.49 
.50 
.50 
.50 
0 
1 
6.Class average entry test(T1) 
0.00 
1.00 
−0.06 
1.00 
0.10 
0.99 
−2.20 
1.50 
7.Class average pretest (T6) 
0.06 
0.97 
0.01 
1.06 
0.15 
0.80 
−2.16 
2.13 
8.SD entry test (T1) 
2.36 
0.64 
2.39 
0.68 
2.31 
0.56 
1.04 
3.82 
9.SD pretest (T6) 
2.11 
0.46 
2.09 
0.50 
2.14 
0.38 
.87 
3.31 
10. Priority area 
.18 
.39 
.17 
.37 
.22 
.41 
0 
1 
11.Woman teacher 
.78 
.42 
.81 
.39 
.71 
.45 
0 
1 
12.Teacher seniority 
18.91 
8.65 
19.29 
8.11 
18.24 
9.49 
1 
35 
13.Teacher highest level of study 
3.77 
1.27 
3.38 
0.94 
4.46 
1.47 
2 
8 
14.Teachers’ scientific background 
.34 
.48 
.43 
.50 
.19 
.39 
0 
1  
a Same value for Min and Max for both groups. 
Table 5 
Correlation between scores variables of each models.  
Variables 
Pretest 
(T2) 
Post- 
test 
(T3) 
Pretest 
(T4) 
Post- 
test 
(T5) 
Pretest 
(T6) 
Post- 
test 
(T7) 
Euclidean Division 
Additive 
Decomposition 
Fractions 
(N = 1,880) 
(N = 1,763) 
(N = 644) 
1.Entry 
score 
(T1) 
.56** 
.58** 
.41** 
.49** 
.49** 
.51** 
2. Pretest 
(T2)  
.64**     
3. Pretest 
(T4)    
.58**   
4. Pretest 
(T6)      
.69** 
*p < .05, **p < .01. 
M. Laurent et al.                                                                                                                                                                                                                                

Learning and Instruction 82 (2022) 101667
7
4. Discussion 
The use of computational thinking to develop mathematical learning 
is at work in schools (Bocconi et al., 2016). However, results on the 
effects of computational thinking on mathematical learning are still few, 
although encouraging (Hickmott et al., 2017). The aim of this study was 
to investigate the effectiveness of a programming activity with Scratch 
on mathematics learning. Three concepts were tested on fourth and fifth 
grade students during four months with a robust methodology. Our 
study utilized a large randomized sample, pre- and post-tests for each of 
the three concepts, based on academic performance measures targeting 
these concepts, in order to fill the lack of results on the effects of 
computational thinking on mathematical learning (Hickmott et al., 
2017). 
Our results show that students in the programming group make less 
progress than students in the control group. The average effects on final 
performance of the programming group are −0.16 SD (Euclidean divi­
sion), −0.19 SD (additive decomposition) and −0.21 SD (fractions). 
These effect sizes, although in the expected range for large RCTs (Lor­
tie-Forgues & Inglis, 2019), are clearly in the lower tail of the distri­
bution of RCTs on mathematics learning as found by Kraft (2020). In 
addition, the negative effect differs from the ScratchMaths study, which 
showed no effect of programming on general mathematical learning 
(Boylan et al., 2018). These elements show that transfer of learning is 
not automatic, even when carried out in the presumed best conditions: 
near and high-road transfer with high level of guidance from the 
teachers, highlighting the common elements between the two learning 
situations. Rather than no effect as in the ScratchMaths study, we may 
have captured a significant negative effect because our assessments were 
closer in time and in content to the Scratch sequences. This design 
allowed us to detect the effect of programming activities more 
accurately. 
The present RCT study excludes the possibility that its results could 
be due to selection bias. Moreover, the effects are consistent across 
mathematical concepts, which gives clear assurance that the effects did 
not appear at random, nor are restricted in scope. Furthermore, it can be 
excluded that these negative results would be due to students being non- 
familiar with the use of Scratch because the effects tend to increase 
slightly over time, and therefore they increase concomitantly with 
higher familiarity with Scratch use. Indeed, the effects range from −0.16 
for the first concept to −0.21 for the last concept even if, in the latter 
case, this was restricted to fifth graders. This gives good assurance that 
the present results have a general validity, at least for the acquisition of 
mathematics in grades 4 and 5. Moreover, none of the characteristics of 
students (pretest, gender), teachers (gender, highest level of education, 
age or seniority) or classes (composition variables) interact with the 
effect, which shows the robustness: it applies to all students regardless of 
the context. 
The many overlaps between computer and mathematical thinking 
suggest that it can be a good vehicle for learning mathematics (Calao 
et al., 2015, September; Grover & Pea, 2013; Lye & Koh, 2014). Duval, 
(1999; 2000; 2006) reinforces this idea because in mathematics, the 
conversion from one register to another is conducive to a deep under­
standing of mathematical concepts. However, in the current study, 
conversion between the two registers, algorithmic to algebraic, did not 
occur as well as expected. 
A first explanation of the results may reside in implementation is­
sues. Although teachers were instructed to explicitly point to the cor­
respondences between registers, teachers may have varied in the 
execution of this explanation phase. Moreover, the use of the computer 
and Scratch may require some help. Teachers in the programming group 
may have devoted some of their guidance to the management of com­
puter equipment and/or the use of Scratch, and thus reduced their 
guidance directly targeting mathematics learning. 
Another explanation relates to time on task, which is known to be a 
strong predictor of learning (Muijs et al., 2014). Students in the pro­
gramming group may have spent some time exploring the many func­
tionalities of Scratch. Indeed, the palette of options available via the 
Scratch interface may have distracted or even diverted students from 
their task, as suggested in Mason and Cooper’s (2013, January) study. 
Despite the same amount of instructional time as the control group, 
students in the programming group may have spent less time actively 
engaged in the mathematical activities. 
A last potential explanation lies in the limits of working memory 
(Baddeley & Hitch, 1974; Sweller et al., 2019). Cognitive load theory 
posits that learning can be hindered by instructional material itself. 
Dealing with the two registers, algorithmic and algebraic, rather than 
just the algebraic register might be cognitively demanding. In cognitive 
load terms, this could increase element interactivity which in turn ex­
ceeds working memory limits (Sweller et al., 2019). It is therefore 
possible that the conversion is too demanding unless each register has 
already been mastered and integrated in long-term memory. 
Table 6 
Multilevel models explaining performance for the tree concepts.  
Variables 
Post-test (T3) 
Post-test (T5) 
Post-test (T7) 
Euclidean Division 
Additive decomposition 
Fractions 
EM 
FM 
EM 
FM 
EM 
FM 
Fixed effects 
Intercept 
−0.02ns 
−0.09ns 
−0.03ns 
−0.10ns 
−0.02ns 
−0.15ns 
Level 1 variables 
Entry score(T1)  
0.29***  
0.28***  
0.21*** 
Pretest (T 2)  
0.44***     
Pretest (T4)    
0.46***   
Pretest (T6)      
0.53*** 
Grade 5  
0.18***     
Level-appropriate age  
0.20***  
0.18**   
Level 2 variables 
Priority area  
−0.15*    
−0.37** 
Programming  
−0.16*  
−0.19*  
−0.21* 
Random effects 
Level1 rand effect 
.81*** 
.45*** 
.78*** 
.46*** 
.76*** 
.41*** 
Level2 rand effect 
.19*** 
.05*** 
.22*** 
.12*** 
.25*** 
.05** 
−2 log V 
5,102.4 
3,982.5 
4,733.2 
3,819.4 
1,722.6 
1308.80  
n = 1,880 
n = 1,763 
n = 644 
Note. EM is empty model, FM is final model. 
*p < .05. **p < .01. 
M. Laurent et al.                                                                                                                                                                                                                                

Learning and Instruction 82 (2022) 101667
8
4.1. Limits and future research 
The present study has several limitations and provides a new agenda 
for research. First, the current study did not include implementation 
measures for the explanation phase. In particular, how teachers made 
the algorithmic-algebraic correspondence explicit was not assessed. In 
order to capture potential variations in implementation and corre­
sponding variations in students’ capacity to transfer their knowledge, 
future research should include such measures. This might help identify 
elements that favour or undermine the quality of transfer. 
Second, regarding limits of working memory, it would be interesting 
to introduce worked examples. In a review about learning to program, 
Berssanette and de Francisco (2021) showed that worked examples are 
an effective way for novices who have to perform complex tasks. In a 
future instructional sequence, problem solving in the programming 
register and in the algebraic register each could be taught in isolation, 
before addressing the conversion between the two registers. Empirical 
study of such a sequence would benefit of measures of cognitive load in 
order to monitor the overall level in both programming and control 
groups. 
Finally, future research could test near and high-road transfer of 
learning mathematics from programming to mathematics with different 
age groups. Moreover, the inclusion of delayed post-tests would allow to 
assess long-term effects. 
4.2. Conclusion 
In summary, the present RCT study shows that the use of computa­
tional thinking via programming as a vehicle for learning mathematics 
in grades 4 and 5 is not as efficient as regular lessons where maths is 
taught for itself. The transfer of learning, although critical, is difficult to 
achieve, even when the presumed best conditions are met to facilitate it 
by relying on a near (mathematics in both situations) and high-road 
transfer, optimised by an explicit guidance of the teacher. Thus, visual 
programming languages should be introduced with caution if they are 
intended to replace regular mathematics teaching. The presented results 
are a strong indicator that this might be deleterious to mathematics 
acquisition. 
Visual programming software such as Scratch can be useful for 
learning computational thinking. This is not evaluated in this article; 
however, it is what the ScratchMaths study indicates with a significant 
effect on computational thinking test scores (0.10 SD) for the inter­
vention group. Thus, it seems more appropriate for computational 
thinking to be learned for its own sake, with a dedicated place in the 
curriculum, at least for grades 4 and 5. In addition to this, the use of 
programming could be exploited for complementary mathematics ac­
tivities, which would not shorten the time devoted to regular lessons, but 
could be placed at other times in different forms (complementary work 
to manage differentiated work groups for example). 
Author contributions 
Manon Laurent: Conceptualization, Methodology, Formal analysis, 
Investigation, Writing - Original Draft, Visualization, Review & Editing. 
Maria-Rosa Crisci: Conceptualization, Investigation, Resources, 
Writing - Review & Editing. Pascal Bressoux: Conceptualization, 
Methodology, Formal analysis, Investigation, Writing - Review & Edit­
ing, Supervision. Hamid Chaachoua: Conceptualization, Investigation, 
Resources, Writing - Review & Editing. C´ecile Nurra: Conceptualiza­
tion, Methodology, Investigation, Writing - Review & Editing. Erica de 
Vries: Review & Editing. Pierre Tchounikine: Conceptualization, 
Software, Investigation, Resources, Writing - Review & Editing, Project 
administration, Funding acquisition. 
Role of the funding source 
This research has been conducted as part of "Expire", an e-FRAN 
project founded by the "Investissement d’Avenir" program handled by 
the French "Caisse des d´epˆots". 
Declaration of competing interest 
None. 
Appendix A. Supplementary data 
Supplementary data to this article can be found online at https://doi. 
org/10.1016/j.learninstruc.2022.101667. 
References 
Babbitt, W., Lachney, M., Bulley, E., & Eglash, R. (2015). Adinkra mathematics: A study 
of ethnocomputing in Ghana. Multidisciplinary Journal of Educational Research, 5(2), 
110–135. 
Baddeley, A. D., & Hitch, G. (1974). Working memory. In G. A. Bower (Ed.), Recent 
advances in learning and motivation, 8 pp. 647–667). New York: Academic Press.  
Benton, L., Saunders, P., Kalas, I., Hoyles, C., & Noss, R. (2018). Designing for learning 
mathematics through programming: A case study of pupils engaging with place 
value. International Journal of Child-Computer Interaction, 16, 68–76. https://doi:10 
.1016/j.ijcci.2017.12.004. 
Berssanette, J. H., & de Francisco, A. C. (2021). Cognitive load theory in the context of 
teaching and learning computer programming: A systematic literature review. IEEE 
Transactions on Education. 
Bocconi, S., Chioccariello, A., Dettori, G., Ferrari, A., Engelhardt, K., Kampylis, P., & 
Punie, Y. (2016). Developing computational thinking in compulsory education. European 
commission, JRC science for policy report. https://komenskypost.nl/wp-content 
/uploads/2017/01/jrc104188_computhinkreport.pdf. 
Boyce, A. K., Campbell, A., Pickford, S., Culler, D., & Barnes, T. (2011, June). 
Experimental evaluation of BeadLoom game: How adding game elements to an 
educational tool improves motivation and learning. In Proceedings of the 16th annual 
joint conference on Innovation and technology in computer science education (pp. 
243–247). 
Boylan, M., Demack, S., Wolstenholme, C., Reidy, J., & Reaney, S. (2018). ScratchMaths: 
Evaluation report and executive summary. Project Report. Education Endownment 
Foundation. https://educationendowmentfoundation.org.uk/public/files/Project 
s/Evaluation_Reports/ScratchMaths.pdf.  
Bransford, J. D., Brown, L., & Cocking, R. R. (Eds.). (2000). How people learn. Brain, mind, 
experience and school. Washington D.C.: National Academy Press.  
Bråting, K., & Kilhamn, C. (2021). Exploring the intersection of algebraic and 
computational thinking. Mathematical Thinking and Learning, 23(2), 170–185. 
Bressoux, P. (2010). Mod´elisation statistique appliqu´ee aux sciences sociales. Louvain-la- 
Neuve, De Boeck Sup´erieur. https://doi:10.7202/039868ar.  
Calao, L. A., Moreno-Le´on, J., Correa, H. E., & Robles, G. (2015, September). Developing 
mathematical thinking with scratch. In European conference on technology enhanced 
learning (pp. 17–27). Cham: Springer.  
Cheng, G. (2019). Exploring factors influencing the acceptance of visual programming 
environment among boys and girls in primary schools. Computers in Human Behavior, 
92, 361–372. https://doi:10.1016/j.chb.2018.11.043. 
Clements, D. H. (2000). From exercises and tasks to problems and projects: Unique 
contributions of computers to innovative mathematics education. The Journal of 
Mathematical Behavior, 19(1), 9–47. https://doi:10.1016/S0732-3123(00)00036-5. 
Drot-Delange, B., Pellet, J. P., Delmas-Rigoutsos, Y., & Bruillard, ´E. (2019). Pens´ee 
informatique : Points de vue contrast´es. STICEF – Sciences et Technologies de 
l’Information et de la Communication pour l’´Education et la Formation, 26(1), 1–24. 
Duval, R. (1999). Representation, vision and visualization: Cognitive functions in 
mathematical thinking. Basic issues for learning (plenary address). In F. Hitt, & 
M. Santos (Eds.), Proceedings of the 21st PME-NA conference, 1 pp. 3–26). Mexico: 
Cuernavaca, Morelos.  
Duval, R. (2000). Basic issues for research in mathematics education. In Proceedings of the 
24th PME-NA conference, 1 pp. 55–69) (Hiroshima, Japan). 
Duval, R. (2006). A cognitive analysis of problems of comprehension in a learning of 
mathematics. Educational Studies in Mathematics, 61(1–2), 103–131. https://doi.org/ 
10.1007/s10649-006-0400-z 
Fuchs, L. S., Fuchs, D., Prentice, K., Burch, M., Hamlett, C. L., Owen, R., Hosp, M., & 
Jancek, D. (2003). Explicitly teaching for transfer: Effects on third-grade students’ 
mathematical problem solving. Journal of Educational Psychology, 95(2), 293–305. 
https://doi.org/10.1037/0022-0663.95.2.293 
Grover, S., & Pea, R. (2013). Computational thinking in K–12: A review of the state of the 
field. Educational Researcher, 42(1), 38–43. https://doi:10.3102/0013189x1 
2463051. 
Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational 
Research, 77(1), 81–112. 
Hickmott, D., Prieto-Rodriguez, E., & Holmes, K. (2017). A scoping review of studies on 
computational thinking in K–12 mathematics classrooms. Digital Experiences in 
Mathematics Education, 4(3), 48–69. https://doi:10.1007/s40751-017-0038-8. 
M. Laurent et al.                                                                                                                                                                                                                                

Learning and Instruction 82 (2022) 101667
9
Hoyles, C., & Noss, R. (1987). Synthesizing mathematical conceptions and their 
formalization through the construction of a Logo-based school mathematics 
curriculum. International Journal of Mathematical Education in Science & Technology, 
18(4), 581–595. https://doi.org/10.1080/0020739870180411 
Kraft, M. A. (2020). Interpreting effect sizes of education interventions. Educational 
Researcher, 49(4), 241–253. https://doi.org/10.3102/0013189X20912798 
Lee, W. C. (1990). The effectiveness of computer-assisted instruction and computer 
programming in elementary and secondary mathematics: A meta-analysis. Doctoral 
dissertation, University of Massachusetts Amherst. https://scholarworks.umass.edu 
/cgi/viewcontent.cgi?article=5569&context=dissertations_1.  
Lortie-Forgues, H., & Inglis, M. (2019). Rigorous large-scale educational RCTs are often 
uninformative: Should we be concerned? Educational Researcher, 48(3), 158–166. 
https://doi.org/10.3102/0013189X19832850 
Lye, S. Y., & Koh, J. H. L. (2014). Review on teaching and learning of computational 
thinking through programming: What is next for K-12? Computers in Human Behavior, 
41, 51–61. 
Mason, R., & Cooper, G. (2013, January). Distractions in programming environments. In 
Proceedings of the fifteenth australasian computing education conference, 136 pp. 
23–30). 
Moreno-Le´on, J., & Robles, G. (2016). Code to learn with scratch? A systematic literature 
review. In IEEE global engineering education conference (EDUCON) (pp. 150–156). Abu 
Dhabi: United Arab Emirates [Paper presentation]. 2016. 
Muijs, D., Kyriakides, L., van der Werf, G., Creemers, B., Timperley, H., & Earl, L. (2014). 
State of the art – teacher effectiveness and professional learning. School Effectiveness 
and School Improvement, 25(2), 231–256. https://doi.org/10.1080/ 
09243453.2014.885451 
Paniagua, A., & Istance, D. (2018). Teachers as designers of learning environments: The 
importance of innovative pedagogies, educational research and innovation. Paris: OECD 
Publishing. https://doi.org/10.1787/9789264085374-en 
Papert, S. (1980). Mindstorms: Children, computers, and powerful ideas. Basic Book.  
Perkins, D. N., & Salomon, G. (1992). Transfer of learning. International encyclopedia of 
education, 2, 6452–6457. 
Resnick, M., Maloney, J., Monroy-Hern´andez, A., Rusk, N., Eastmond, E., Brennan, K., 
Millner, A., Rosenbaum, E., Silver, J., Silverman, B., & Kafai, Y. (2009). Scratch: 
Programming for all. Communications of the ACM, 52, 60–67. https://doi:10.1145/1 
592761.1592779. 
Salomon, G., & Perkins, D. N. (1989). Rocky roads to transfer: Rethinking mechanism of 
a neglected phenomenon. Educational Psychologist, 24(2), 113–142. 
Scherer, R. (2016). Learning from the past–the need for empirical evidence on the 
transfer effects of computer programming skills. Frontiers in Psychology, 7. https:// 
doi:10.3389/fpsyg.2016.01390. 
Scherer, R., Siddiq, F., & S´anchez Viveros, B. (2019). The cognitive benefits of learning 
computer programming: A meta-analysis of transfer effects. Journal of Educational 
Psychology, 111, 764–792. https://doi:10.1037/edu0000314. 
Schunk, D. H. (2012). Learning theories an educational perspective (6th ed.). Pearson.  
Shute, V. J., Sun, C., & Asbell-Clarke, J. (2017). Demystifying computational thinking. 
Educational Research Review, 22, 142–158. https://doi:10.1016/j.edurev.2017.09.00 
3. 
Sweller, J., Van Merrienboer, J. J., & Paas, F. G. (1998). Cognitive architecture and 
instructional design. Educational Psychology Review, 10(3), 251–296. 
Sweller, J., van Merri¨enboer, J. J. G., & Paas, F. (2019). Cognitive architecture and 
instructional design: 20 Years later. Educational Psychology Review, 31(2), 261–292. 
https://doi.org/10.1007/s10648-019-09465-5 
Tang, K.-Y., Chou, T.-L., & Tsai, C.-C. (2019). A content analysis of computational 
thinking research: An international publication trends and research typology. The 
Asia-Pacific Education Researcher, 29, 9–19. https://doi:10.1007/s40299-01 
9-00442-8. 
Voogt, J., Fisser, P., Good, J., Mishra, P., & Yadav, A. (2015). Computational thinking in 
compulsory education: Towards an agenda for research and practice. Education and 
Information Technologies, 20(4), 715–728. https://doi.org/10.1007/s10639-015- 
9412-6 
Weintrop, D., Beheshti, E., Horn, M., Orton, K., Jona, K., Trouille, L., & Wilensky, U. 
(2016). Defining computational thinking for mathematics and science classrooms. 
Journal of Science Education and Technology, 25(1), 127–147. 
Willingham, D. T. (2009). Why don’t students like school? San Francisco: John Wiley & 
Sons.  
Wing, J. (2006). Computational thinking. Communications of the ACM, 49, 33–36. 
Wing, J. (2011). Research notebook: Computational thinking—what and why? The link 
magazine. Pittsburgh: Carnegie Mellon University. Retrieved from http://link.cs.cmu 
.edu/article.php?a=600. 
Wing, J. M., & Stanzione, D. (2016). Progress in computational thinking, and expanding 
the HPC community. Communications of the ACM, 59(7), 10–11. https://doi:10.114 
5/2933410. 
M. Laurent et al.                                                                                                                                                                                                                                

