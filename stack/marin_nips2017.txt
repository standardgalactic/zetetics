Some recent advances on Approximate
Bayesian Computation techniques
Jean-Michel Marin
University of Montpellier, CNRS
Alexander Grothendieck Montpellier Institute
9 December 2017
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
1 / 41

Thanks
Numerous colleagues participated to parts of this work
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
2 / 41

Thanks
Numerous colleagues participated to parts of this work
▶Pierre Pudlo (Marseille)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
2 / 41

Thanks
Numerous colleagues participated to parts of this work
▶Pierre Pudlo (Marseille)
▶Louis Raynal (PhD student Montpellier)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
2 / 41

Thanks
Numerous colleagues participated to parts of this work
▶Pierre Pudlo (Marseille)
▶Louis Raynal (PhD student Montpellier)
▶Arnaud Estoup (molecular ecologist, Montpellier)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
2 / 41

Thanks
Numerous colleagues participated to parts of this work
▶Pierre Pudlo (Marseille)
▶Louis Raynal (PhD student Montpellier)
▶Arnaud Estoup (molecular ecologist, Montpellier)
▶Christian Robert (Paris and Warwick)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
2 / 41

Thanks
Numerous colleagues participated to parts of this work
▶Pierre Pudlo (Marseille)
▶Louis Raynal (PhD student Montpellier)
▶Arnaud Estoup (molecular ecologist, Montpellier)
▶Christian Robert (Paris and Warwick)
▶Judith, Natesh, ...
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
2 / 41

Introduction
Bayesian parametric paradigm
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
3 / 41

Introduction
Bayesian parametric paradigm
Likelihood function f(y|θ) expensive or impossible to calculate
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
3 / 41

Introduction
Bayesian parametric paradigm
Likelihood function f(y|θ) expensive or impossible to calculate
Extremely difﬁcult to sample from the posterior distribution
π(θ|y) ∝π(θ)f(y|θ)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
3 / 41

Introduction
Two typical situations
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
4 / 41

Introduction
Two typical situations
▶f(y|θ) =
Z
f(y, u|θ)µ(du) intractable
population genetics models, coalescent process
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
4 / 41

Introduction
Two typical situations
▶f(y|θ) =
Z
f(y, u|θ)µ(du) intractable
population genetics models, coalescent process
EM algorithms, Gibbs sampling, pseudo-marginal
MCMC methods, variational approximations
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
4 / 41

Introduction
Two typical situations
▶f(y|θ) =
Z
f(y, u|θ)µ(du) intractable
population genetics models, coalescent process
EM algorithms, Gibbs sampling, pseudo-marginal
MCMC methods, variational approximations
▶f(y|θ) = g(y, θ)/Z(θ) and Z(θ) intractable
Markov random ﬁeld
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
4 / 41

Introduction
Two typical situations
▶f(y|θ) =
Z
f(y, u|θ)µ(du) intractable
population genetics models, coalescent process
EM algorithms, Gibbs sampling, pseudo-marginal
MCMC methods, variational approximations
▶f(y|θ) = g(y, θ)/Z(θ) and Z(θ) intractable
Markov random ﬁeld
pseudo-marginal MCMC methods, variational
approximations
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
4 / 41

Introduction
ABC is a technique that only requires being able to sample
from the likelihood f(·|θ)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
5 / 41

Introduction
ABC is a technique that only requires being able to sample
from the likelihood f(·|θ)
This technique stemmed from population genetics models,
about 15 years ago, and population geneticists still signiﬁcantly
contribute to methodological developments of ABC
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
5 / 41

Introduction
ABC is a technique that only requires being able to sample
from the likelihood f(·|θ)
This technique stemmed from population genetics models,
about 15 years ago, and population geneticists still signiﬁcantly
contribute to methodological developments of ABC
If, with Christian, we work on ABC methods, we can be very
grateful to our biologist colleagues!
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
5 / 41

Introduction
▶some methodological aspects of ABC
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
6 / 41

Introduction
▶some methodological aspects of ABC
▶our ABC random forests proposal
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
6 / 41

Introduction
▶some methodological aspects of ABC
▶our ABC random forests proposal
▶ABC and PAC-Bayes
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
6 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
7 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
Rubin (1984) The Annals of Statistics
Tavar´e et al. (1997) Genetics
Pritchard et al. (1999) Mol. Biol. Evol.
1) Set i = 1
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
7 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
Rubin (1984) The Annals of Statistics
Tavar´e et al. (1997) Genetics
Pritchard et al. (1999) Mol. Biol. Evol.
1) Set i = 1
2) Generate θ′ from the prior distribution π(·)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
7 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
Rubin (1984) The Annals of Statistics
Tavar´e et al. (1997) Genetics
Pritchard et al. (1999) Mol. Biol. Evol.
1) Set i = 1
2) Generate θ′ from the prior distribution π(·)
3) Generate z from the likelihood f(·|θ′)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
7 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
Rubin (1984) The Annals of Statistics
Tavar´e et al. (1997) Genetics
Pritchard et al. (1999) Mol. Biol. Evol.
1) Set i = 1
2) Generate θ′ from the prior distribution π(·)
3) Generate z from the likelihood f(·|θ′)
4) If d(η(z), η(y)) ⩽ϵ, set θi = θ′ and i = i + 1
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
7 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
Rubin (1984) The Annals of Statistics
Tavar´e et al. (1997) Genetics
Pritchard et al. (1999) Mol. Biol. Evol.
1) Set i = 1
2) Generate θ′ from the prior distribution π(·)
3) Generate z from the likelihood f(·|θ′)
4) If d(η(z), η(y)) ⩽ϵ, set θi = θ′ and i = i + 1
5) If i ⩽N, return to 2)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
7 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
ϵ reﬂects the tension between computability and accuracy
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
8 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
ϵ reﬂects the tension between computability and accuracy
▶if ϵ →∞, we get simulations from the prior
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
8 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
ϵ reﬂects the tension between computability and accuracy
▶if ϵ →∞, we get simulations from the prior
▶if ϵ →0, we get simulations from the posterior
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
8 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
ϵ reﬂects the tension between computability and accuracy
▶if ϵ →∞, we get simulations from the prior
▶if ϵ →0, we get simulations from the posterior
ABC target
πϵ(θ|y) =
R
π(θ)f(z|θ)I(z ∈Aϵ,y)dz
R
Aϵ,y×Θ π(θ)f(z|θ)dzdθ
Aϵ,y = {z|d(η(z), η(y)) ⩽ϵ} the acceptance set
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
8 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
A toy example from Richard Wilkinson (Tutorial on ABC,
NIPS 2013)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
9 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
A toy example from Richard Wilkinson (Tutorial on ABC,
NIPS 2013)
y|θ ∼N1
 2(θ + 2)θ(θ −2), 0.1 + θ2
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
9 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
A toy example from Richard Wilkinson (Tutorial on ABC,
NIPS 2013)
y|θ ∼N1
 2(θ + 2)θ(θ −2), 0.1 + θ2
θ ∼U[−10,10]
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
9 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
A toy example from Richard Wilkinson (Tutorial on ABC,
NIPS 2013)
y|θ ∼N1
 2(θ + 2)θ(θ −2), 0.1 + θ2
θ ∼U[−10,10]
y = 2
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
9 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
A toy example from Richard Wilkinson (Tutorial on ABC,
NIPS 2013)
y|θ ∼N1
 2(θ + 2)θ(θ −2), 0.1 + θ2
θ ∼U[−10,10]
y = 2
d(z, y) = |z −y|
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
9 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−3
−2
−1
0
1
2
3
−10
0
10
20
theta
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−ε
+ε
D
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
theta
Density
ABC
True
ϵ = 7.5
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
10 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−3
−2
−1
0
1
2
3
−10
0
10
20
theta
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−ε
+ε
D
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
theta
Density
ABC
True
ϵ = 7.5
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−3
−2
−1
0
1
2
3
−10
0
10
20
theta
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−ε
+ε
D
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
theta
Density
ABC
True
ϵ = 5
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
10 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−3
−2
−1
0
1
2
3
−10
0
10
20
theta
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−ε
+ε
D
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
theta
Density
ABC
True
ϵ = 2.5
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
11 / 41

Methodological aspects of ABC
Likelihood-free rejection sampler
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−3
−2
−1
0
1
2
3
−10
0
10
20
theta
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−ε
+ε
D
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
theta
Density
ABC
True
ϵ = 2.5
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−3
−2
−1
0
1
2
3
−10
0
10
20
theta
●●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−ε
+ε
−3
−2
−1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
theta
Density
ABC
True
ϵ = 1
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
11 / 41

Methodological aspects of ABC
A k-NN approximation
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
a) Generate θi from the prior π(·)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate z from the model f(·|θi)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate z from the model f(·|θi)
c) Calculate di = d(η(z), η(y))
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate z from the model f(·|θi)
c) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate z from the model f(·|θi)
c) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
3) Return the θi’s that correspond to the N-smallest distances
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate z from the model f(·|θi)
c) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
3) Return the θi’s that correspond to the N-smallest distances
N = ⌊αM⌋
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Practitioners really use
1) For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate z from the model f(·|θi)
c) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
3) Return the θi’s that correspond to the N-smallest distances
N = ⌊αM⌋
ϵ corresponds to a quantile of the distances
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
12 / 41

Methodological aspects of ABC
A k-NN approximation
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
13 / 41

Methodological aspects of ABC
A k-NN approximation
New insights into Approximate Bayesian Computation
Biau, C´erou, Guyader (2015) Annales de l’IHP
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
13 / 41

Methodological aspects of ABC
A k-NN approximation
New insights into Approximate Bayesian Computation
Biau, C´erou, Guyader (2015) Annales de l’IHP
▶intuitive
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
13 / 41

Methodological aspects of ABC
A k-NN approximation
New insights into Approximate Bayesian Computation
Biau, C´erou, Guyader (2015) Annales de l’IHP
▶intuitive
▶simple to implement
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
13 / 41

Methodological aspects of ABC
A k-NN approximation
New insights into Approximate Bayesian Computation
Biau, C´erou, Guyader (2015) Annales de l’IHP
▶intuitive
▶simple to implement
▶embarrassingly parallelisable
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
13 / 41

Methodological aspects of ABC
A k-NN approximation
New insights into Approximate Bayesian Computation
Biau, C´erou, Guyader (2015) Annales de l’IHP
▶intuitive
▶simple to implement
▶embarrassingly parallelisable
▶BUT curse of dimensionality: most of the simulations are at
the boundary of the space as the number of summary
statistics increases
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
13 / 41

Methodological aspects of ABC
Two views of the ABC approximation
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
14 / 41

Methodological aspects of ABC
Two views of the ABC approximation
=⇒Wilkinson (2013) SAGMB shows that ABC is exact but for
a different model to that intended
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
14 / 41

Methodological aspects of ABC
Two views of the ABC approximation
=⇒Wilkinson (2013) SAGMB shows that ABC is exact but for
a different model to that intended
=⇒Blum (2010) JASA emphasizes that ABC is a kernel
smoothing approximation of the likelihood function
πϵ(θ|y) =
R
π(θ)f(z|θ)I(z ∈Aϵ,y)dz
R
Aϵ,y×Θ π(θ)f(z|θ)dzdθ
=
π(θ)
R
f(z|θ)K(d(η(z), η(y)))dz
R
π(θ)f(z|θ)K(d(η(z), η(y)))dzdθ
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
14 / 41

Methodological aspects of ABC
More efﬁcient algorithms
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
15 / 41

Methodological aspects of ABC
More efﬁcient algorithms
Simulate all the θ’s particles using the prior distribution
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
15 / 41

Methodological aspects of ABC
More efﬁcient algorithms
Simulate all the θ’s particles using the prior distribution
=⇒very inefﬁcient
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
15 / 41

Methodological aspects of ABC
More efﬁcient algorithms
Simulate all the θ’s particles using the prior distribution
=⇒very inefﬁcient
various sequential Monte Carlo algorithms have been con-
structed as an alternative
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
15 / 41

Methodological aspects of ABC
More efﬁcient algorithms
Simulate all the θ’s particles using the prior distribution
=⇒very inefﬁcient
various sequential Monte Carlo algorithms have been con-
structed as an alternative
Sisson et al. (2007) PNAS
Beaumont, Cornuet, Marin and Robert (2009) Biometrika
Del Moral et al. (2012) Statistics and Computing
Marin, Pudlo and Sedki (2012) IEEE Proceedings of WSC
Filippi et al. (2013) SAGMB
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
15 / 41

Methodological aspects of ABC
More efﬁcient algorithms
The key idea is to decompose the difﬁcult problem of sam-
pling from πϵ(θ, z|y) into a series of simpler subproblems
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
16 / 41

Methodological aspects of ABC
More efﬁcient algorithms
The key idea is to decompose the difﬁcult problem of sam-
pling from πϵ(θ, z|y) into a series of simpler subproblems
Time 0 sampling from πϵ0(θ, z|y) with large ϵ0
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
16 / 41

Methodological aspects of ABC
More efﬁcient algorithms
The key idea is to decompose the difﬁcult problem of sam-
pling from πϵ(θ, z|y) into a series of simpler subproblems
Time 0 sampling from πϵ0(θ, z|y) with large ϵ0
Then simulating from an increasing difﬁcult sequence of target
distribution πϵt(θ, z|y) that is ϵt < ϵt−1
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
16 / 41

Methodological aspects of ABC
More efﬁcient algorithms
The key idea is to decompose the difﬁcult problem of sam-
pling from πϵ(θ, z|y) into a series of simpler subproblems
Time 0 sampling from πϵ0(θ, z|y) with large ϵ0
Then simulating from an increasing difﬁcult sequence of target
distribution πϵt(θ, z|y) that is ϵt < ϵt−1
Likelihood free MCMC sampler Majoram et al. (2003) PNAS
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
16 / 41

Methodological aspects of ABC
Regression adjustments
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
17 / 41

Methodological aspects of ABC
Regression adjustments
Beaumont et al. (2002) Genetics
local linear regression adjustment of the parameter values
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
17 / 41

Methodological aspects of ABC
Regression adjustments
Beaumont et al. (2002) Genetics
local linear regression adjustment of the parameter values
Blum and Francois (2010) Statistics and Computing
heteroscedastic models, feed-forward neural networks
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
17 / 41

Methodological aspects of ABC
Summary statistics
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
18 / 41

Methodological aspects of ABC
Summary statistics
Best subset selection
▶Joyce and Marjoram (2008) SAGMB, τ-sufﬁciency
▶Nunes and Balding (2010) SAGMB, entropy
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
18 / 41

Methodological aspects of ABC
Summary statistics
Best subset selection
▶Joyce and Marjoram (2008) SAGMB, τ-sufﬁciency
▶Nunes and Balding (2010) SAGMB, entropy
Projection
▶Fearnhead and Prangle (2012) JRSS B introduce
semi-automatic ABC
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
18 / 41

Methodological aspects of ABC
Summary statistics
Best subset selection
▶Joyce and Marjoram (2008) SAGMB, τ-sufﬁciency
▶Nunes and Balding (2010) SAGMB, entropy
Projection
▶Fearnhead and Prangle (2012) JRSS B introduce
semi-automatic ABC
Regularization techniques
▶Blum, Nunes, Prangle and Fearnhead (2013) Statistical
Science use ridge regression
▶Saulnier, Gascuel, Alizon (2017) Plos Computational
Biology use LASSO
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
18 / 41

Methodological aspects of ABC
ABC model choice procedure
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
d) Calculate di = d(η(z), η(y))
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
d) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
d) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
3) Return the mi’s that correspond to the N-smallest
distances
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
d) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
3) Return the mi’s that correspond to the N-smallest
distances
N = ⌊αM⌋
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
1) For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
d) Calculate di = d(η(z), η(y))
2) Order the distances d(1), . . . , d(M)
3) Return the mi’s that correspond to the N-smallest
distances
N = ⌊αM⌋
A k-NN approximation of the posterior probabilities
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
19 / 41

Methodological aspects of ABC
ABC model choice procedure
If η(y) is a sufﬁcient statistics for the model choice problem, this
can work pretty well
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
20 / 41

Methodological aspects of ABC
ABC model choice procedure
If η(y) is a sufﬁcient statistics for the model choice problem, this
can work pretty well
ABC likelihood-free methods for model choice in Gibbs
random ﬁelds Grelaud, Robert, Marin, Rodolphe and Taly
(2009) Bayesian Analysis
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
20 / 41

Methodological aspects of ABC
ABC model choice procedure
If η(y) is a sufﬁcient statistics for the model choice problem, this
can work pretty well
ABC likelihood-free methods for model choice in Gibbs
random ﬁelds Grelaud, Robert, Marin, Rodolphe and Taly
(2009) Bayesian Analysis
If not...
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
20 / 41

Methodological aspects of ABC
ABC model choice procedure
If η(y) is a sufﬁcient statistics for the model choice problem, this
can work pretty well
ABC likelihood-free methods for model choice in Gibbs
random ﬁelds Grelaud, Robert, Marin, Rodolphe and Taly
(2009) Bayesian Analysis
If not...
Lack of conﬁdence in approximate Bayesian computation
model choice Robert, Cornuet, Marin, Pillai (2011) PNAS
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
20 / 41

Methodological aspects of ABC
ABC model choice procedure
If η(y) is a sufﬁcient statistics for the model choice problem, this
can work pretty well
ABC likelihood-free methods for model choice in Gibbs
random ﬁelds Grelaud, Robert, Marin, Rodolphe and Taly
(2009) Bayesian Analysis
If not...
Lack of conﬁdence in approximate Bayesian computation
model choice Robert, Cornuet, Marin, Pillai (2011) PNAS
Relevant statistics for Bayesian model choice Marin, Pillai,
Robert, Rousseau (2014) JRSS B
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
20 / 41

Methodological aspects of ABC
ABC model choice procedure
We investigate some ABC model choice techniques that use
others machine learning procedures
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
21 / 41

Methodological aspects of ABC
ABC model choice procedure
We investigate some ABC model choice techniques that use
others machine learning procedures
Estimation of demo-genetic model probabilities with Ap-
proximate Bayesian Computation using linear discriminant
analysis on summary statistics Estoup, Lombaert, Marin,
Guillemaud, Pudlo, Robert, Cornuet (2012) Molecular Ecol-
ogy
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
21 / 41

Methodological aspects of ABC
Sofwares
abc R package several ABC algorithms for performing parame-
ter estimation and model selection
abctools R package tuning ABC analyses
https://journal.r-project.org/archive/2015-2/nunes-prangle.pdf
abcrf R package ABC via random forests
EasyABC R package several algorithms for performing efﬁ-
cient ABC sampling schemes, including 4 sequential sampling
schemes and 3 MCMC schemes
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
22 / 41

Methodological aspects of ABC
Sofwares
DIY-ABC software performs parameter estimation and model
selection for population genetics models
ABC-SysBio python package parameter inference and model
selection for dynamical systems
ABCtoolbox programs various ABC algorithms including rejec-
tion sampling, MCMC without likelihood, a particle-based sam-
pler, and ABC-GLM
PopABC software package for inference of the pattern of de-
mographic divergence, coalescent simulation, bayesian model
choice
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
23 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
DIYABC v2.0: a software to make Approximate Bayesian Computation
inferences about population history using Single Nucleotide Polymor-
phism, DNA sequence and microsatellite data Cornuet, Pudlo, Veyssier,
Dehne-Garcia, Gautier, Leblois, Marin, Estoup (2014) Bioinformatics
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
DIYABC v2.0: a software to make Approximate Bayesian Computation
inferences about population history using Single Nucleotide Polymor-
phism, DNA sequence and microsatellite data Cornuet, Pudlo, Veyssier,
Dehne-Garcia, Gautier, Leblois, Marin, Estoup (2014) Bioinformatics
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
DIYABC v2.0: a software to make Approximate Bayesian Computation
inferences about population history using Single Nucleotide Polymor-
phism, DNA sequence and microsatellite data Cornuet, Pudlo, Veyssier,
Dehne-Garcia, Gautier, Leblois, Marin, Estoup (2014) Bioinformatics
Asian ladybug
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
DIYABC v2.0: a software to make Approximate Bayesian Computation
inferences about population history using Single Nucleotide Polymor-
phism, DNA sequence and microsatellite data Cornuet, Pudlo, Veyssier,
Dehne-Garcia, Gautier, Leblois, Marin, Estoup (2014) Bioinformatics
Asian ladybug
European honey bee
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
DIYABC v2.0: a software to make Approximate Bayesian Computation
inferences about population history using Single Nucleotide Polymor-
phism, DNA sequence and microsatellite data Cornuet, Pudlo, Veyssier,
Dehne-Garcia, Gautier, Leblois, Marin, Estoup (2014) Bioinformatics
Asian ladybug
European honey bee
drosophila suzukii
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
DIYABC v2.0: a software to make Approximate Bayesian Computation
inferences about population history using Single Nucleotide Polymor-
phism, DNA sequence and microsatellite data Cornuet, Pudlo, Veyssier,
Dehne-Garcia, Gautier, Leblois, Marin, Estoup (2014) Bioinformatics
Asian ladybug
European honey bee
drosophila suzukii
Pigmies populations
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Sofwares
Infering population history with DIY ABC: a user-friedly approach Ap-
proximate Bayesian Computation Cornuet, Santos, Beaumont, Robert,
Marin, Balding, Guillemaud, Estoup (2008) Bioinformatics
DIYABC v2.0: a software to make Approximate Bayesian Computation
inferences about population history using Single Nucleotide Polymor-
phism, DNA sequence and microsatellite data Cornuet, Pudlo, Veyssier,
Dehne-Garcia, Gautier, Leblois, Marin, Estoup (2014) Bioinformatics
Asian ladybug
European honey bee
drosophila suzukii
Pigmies populations
Four human populations, to study
the out-of-Africa colonization
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
24 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
25 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
DIYABC (2014) paper has now around 300 citations
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
25 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
DIYABC (2014) paper has now around 300 citations
▶simulate from the model can be very computationally
intensive, parallelizable algorithms are necessary
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
25 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
DIYABC (2014) paper has now around 300 citations
▶simulate from the model can be very computationally
intensive, parallelizable algorithms are necessary
▶likelihoods are intractable due to the strong and complex
dependence structure of the model
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
25 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
DIYABC (2014) paper has now around 300 citations
▶simulate from the model can be very computationally
intensive, parallelizable algorithms are necessary
▶likelihoods are intractable due to the strong and complex
dependence structure of the model
▶sequential methods are difﬁcult to calibrate and do not give
reproducible results
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
25 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
DIYABC (2014) paper has now around 300 citations
▶simulate from the model can be very computationally
intensive, parallelizable algorithms are necessary
▶likelihoods are intractable due to the strong and complex
dependence structure of the model
▶sequential methods are difﬁcult to calibrate and do not give
reproducible results
▶post hoc adjustments are crucial but they underestimate
the amount of uncertainty
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
25 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
DIYABC (2014) paper has now around 300 citations
▶simulate from the model can be very computationally
intensive, parallelizable algorithms are necessary
▶likelihoods are intractable due to the strong and complex
dependence structure of the model
▶sequential methods are difﬁcult to calibrate and do not give
reproducible results
▶post hoc adjustments are crucial but they underestimate
the amount of uncertainty
▶available techniques to select the summary statistics do
not give reproducible results
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
25 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
Despite all these works, two major difﬁculties
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
26 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
Despite all these works, two major difﬁculties
▶to ensure reliability of the method, the number of
simulations should be large
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
26 / 41

Methodological aspects of ABC
Frontline news from population geneticists country
Despite all these works, two major difﬁculties
▶to ensure reliability of the method, the number of
simulations should be large
▶choice of the summaries statistics is still a problem
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
26 / 41

Methodological aspects of ABC
Use modern machine learning tools
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
27 / 41

Methodological aspects of ABC
Use modern machine learning tools
Exploiting a large number of summary statistics is not an issue
for some machine learning methods
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
27 / 41

Methodological aspects of ABC
Use modern machine learning tools
Exploiting a large number of summary statistics is not an issue
for some machine learning methods
Idea: learn on a huge reference table using random forests
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
27 / 41

Methodological aspects of ABC
Use modern machine learning tools
Exploiting a large number of summary statistics is not an issue
for some machine learning methods
Idea: learn on a huge reference table using random forests
Some theoretical guarantees for sparse problems
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
27 / 41

Methodological aspects of ABC
Use modern machine learning tools
Exploiting a large number of summary statistics is not an issue
for some machine learning methods
Idea: learn on a huge reference table using random forests
Some theoretical guarantees for sparse problems
Analysis of a random forest model
Biau (2012) JMLR
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
27 / 41

Methodological aspects of ABC
Use modern machine learning tools
Exploiting a large number of summary statistics is not an issue
for some machine learning methods
Idea: learn on a huge reference table using random forests
Some theoretical guarantees for sparse problems
Analysis of a random forest model
Biau (2012) JMLR
Consistency of random forests
Scornet, Biau, Vert (2015) The Annals of Statistics
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
27 / 41

Methodological aspects of ABC
Use modern machine learning tools
This work stands at the interface between Bayesian inference
and machine learning techniques
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
28 / 41

Methodological aspects of ABC
Use modern machine learning tools
This work stands at the interface between Bayesian inference
and machine learning techniques
As an alternative, Papamakarios and Murray (2016) propose to
approximate the whole posterior distribution by using Mixture
Density Networks (MDN, Bishop, 1994)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
28 / 41

Methodological aspects of ABC
Use modern machine learning tools
This work stands at the interface between Bayesian inference
and machine learning techniques
As an alternative, Papamakarios and Murray (2016) propose to
approximate the whole posterior distribution by using Mixture
Density Networks (MDN, Bishop, 1994)
Fast e-free Inference of Simulation Models with Bayesian
Conditional Density Estimation
Papamakarios and Murray (2016) NIPS
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
28 / 41

Methodological aspects of ABC
Use modern machine learning tools
The MDN strategy consists in using Gaussian mixture models
with parameters calibrated thanks to neural networks
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
29 / 41

Methodological aspects of ABC
Use modern machine learning tools
The MDN strategy consists in using Gaussian mixture models
with parameters calibrated thanks to neural networks
Idea: iteratively learn an efﬁcient proposal prior (approximating
the posterior distribution), then to use this proposal to train the
posterior, both steps making use of MDN
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
29 / 41

Methodological aspects of ABC
Use modern machine learning tools
The MDN strategy consists in using Gaussian mixture models
with parameters calibrated thanks to neural networks
Idea: iteratively learn an efﬁcient proposal prior (approximating
the posterior distribution), then to use this proposal to train the
posterior, both steps making use of MDN
The number of mixture components and the number of hid-
den layers of the networks require calibration
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
29 / 41

Methodological aspects of ABC
Use modern machine learning tools
Deep Learning for Population Genetic Inference
Sheehan and Song (2016) PLOS Computational Biology
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
30 / 41

Methodological aspects of ABC
Use modern machine learning tools
Deep Learning for Population Genetic Inference
Sheehan and Song (2016) PLOS Computational Biology
Deep learning makes use of multilayer neural networks to learn
a feature-based function from the input (hundreds of correlated
summary statistics) to the output (population genetic parameters
of interest).
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
30 / 41

Methodological aspects of ABC
Use modern machine learning tools
Deep Learning for Population Genetic Inference
Sheehan and Song (2016) PLOS Computational Biology
Deep learning makes use of multilayer neural networks to learn
a feature-based function from the input (hundreds of correlated
summary statistics) to the output (population genetic parameters
of interest).
Unsupervised pretraining using autoencoders very inter-
esting, but requires a lot of calibration
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
30 / 41

ABC random forests
Model choice
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
31 / 41

ABC random forests
Model choice
Reliable ABC model choice via random forests Pudlo, Marin, Estoup,
Cornuet, Gauthier and Robert (2016) Bioinformatics
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
31 / 41

ABC random forests
Model choice
Reliable ABC model choice via random forests Pudlo, Marin, Estoup,
Cornuet, Gauthier and Robert (2016) Bioinformatics
Input ABC reference table involving model index and summary
statistics, table used as learning set
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
31 / 41

ABC random forests
Model choice
Reliable ABC model choice via random forests Pudlo, Marin, Estoup,
Cornuet, Gauthier and Robert (2016) Bioinformatics
Input ABC reference table involving model index and summary
statistics, table used as learning set
possibly large collection of summary statistics: from scien-
tiﬁc theory input to machine-learning alternatives
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
31 / 41

ABC random forests
Model choice
Reliable ABC model choice via random forests Pudlo, Marin, Estoup,
Cornuet, Gauthier and Robert (2016) Bioinformatics
Input ABC reference table involving model index and summary
statistics, table used as learning set
possibly large collection of summary statistics: from scien-
tiﬁc theory input to machine-learning alternatives
For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
d) Calculate xi = η(zi)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
31 / 41

ABC random forests
Model choice
Reliable ABC model choice via random forests Pudlo, Marin, Estoup,
Cornuet, Gauthier and Robert (2016) Bioinformatics
Input ABC reference table involving model index and summary
statistics, table used as learning set
possibly large collection of summary statistics: from scien-
tiﬁc theory input to machine-learning alternatives
For i = 1, . . . , M
a) Generate mi from the prior π(M = m)
b) Generate θ′
mi from the prior πmi(·)
c) Generate z from the model fmi(·|θ′
mi)
d) Calculate xi = η(zi)
Output a random forest classiﬁer to infer model indexes
[
m(η(y))
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
31 / 41

ABC random forests
Model choice
Random forest predicts a MAP model index, from the observed
dataset
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
32 / 41

ABC random forests
Model choice
Random forest predicts a MAP model index, from the observed
dataset
the predictor provided by the forest is good enough to select the
most likely model
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
32 / 41

ABC random forests
Model choice
Random forest predicts a MAP model index, from the observed
dataset
the predictor provided by the forest is good enough to select the
most likely model
but not to derive directly the associated posterior probabil-
ities
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
32 / 41

ABC random forests
Model choice
Random forest predicts a MAP model index, from the observed
dataset
the predictor provided by the forest is good enough to select the
most likely model
but not to derive directly the associated posterior probabil-
ities
frequency of trees associated with majority model is no
proper substitute to the true posterior probability
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
32 / 41

ABC random forests
Model choice
Estimate of the posterior probability of the selected model
P[M =
[
m(η(y))|η(y)]
random comes from M (bayesian)!
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
33 / 41

ABC random forests
Model choice
Estimate of the posterior probability of the selected model
P[M =
[
m(η(y))|η(y)]
random comes from M (bayesian)!
P[M =
[
m(η(y))|η(y)] = 1 −E
h
I(M ,
[
m(η(y)))|η(y)
i
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
33 / 41

ABC random forests
Model choice
A second random forest in regression
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
34 / 41

ABC random forests
Model choice
A second random forest in regression
1) compute the value of I(M ,
[
m(η(z)) for the trained
random forest ˆm and for all terms in the ABC reference
table using the out-of-bag classiﬁers
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
34 / 41

ABC random forests
Model choice
A second random forest in regression
1) compute the value of I(M ,
[
m(η(z)) for the trained
random forest ˆm and for all terms in the ABC reference
table using the out-of-bag classiﬁers
2) train a RF regression and get bE
h
I(M ,
[
m(η(z)))|η(z)]
i
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
34 / 41

ABC random forests
Model choice
A second random forest in regression
1) compute the value of I(M ,
[
m(η(z)) for the trained
random forest ˆm and for all terms in the ABC reference
table using the out-of-bag classiﬁers
2) train a RF regression and get bE
h
I(M ,
[
m(η(z)))|η(z)]
i
3) return
bP[M =
[
m(η(y))|η(y)] = 1 −bE
h
I(M ,
[
m(η(z)))|η(z)]
i
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
34 / 41

ABC random forests
Model choice
A second random forest in regression
1) compute the value of I(M ,
[
m(η(z)) for the trained
random forest ˆm and for all terms in the ABC reference
table using the out-of-bag classiﬁers
2) train a RF regression and get bE
h
I(M ,
[
m(η(z)))|η(z)]
i
3) return
bP[M =
[
m(η(y))|η(y)] = 1 −bE
h
I(M ,
[
m(η(z)))|η(z)]
i
on same reference table out-of-bag magic trick avoid over-
ﬁtting!
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
34 / 41

ABC random forests
Parameter inference
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
35 / 41

ABC random forests
Parameter inference
ABC random forests for Bayesian parameter inference Raynal, Marin,
Pudlo, Ribatet, Robert and Estoup (2017) Preprint reviewed and recom-
mended by Peer Community In Evolutionary Biology
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
35 / 41

ABC random forests
Parameter inference
ABC random forests for Bayesian parameter inference Raynal, Marin,
Pudlo, Ribatet, Robert and Estoup (2017) Preprint reviewed and recom-
mended by Peer Community In Evolutionary Biology
Input ABC reference table involving parameters values and
summary statistics, table used as learning set
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
35 / 41

ABC random forests
Parameter inference
ABC random forests for Bayesian parameter inference Raynal, Marin,
Pudlo, Ribatet, Robert and Estoup (2017) Preprint reviewed and recom-
mended by Peer Community In Evolutionary Biology
Input ABC reference table involving parameters values and
summary statistics, table used as learning set
For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate zi from the model f(·|θi)
c) Calculate xi = η(zi)
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
35 / 41

ABC random forests
Parameter inference
ABC random forests for Bayesian parameter inference Raynal, Marin,
Pudlo, Ribatet, Robert and Estoup (2017) Preprint reviewed and recom-
mended by Peer Community In Evolutionary Biology
Input ABC reference table involving parameters values and
summary statistics, table used as learning set
For i = 1, . . . , M
a) Generate θi from the prior π(·)
b) Generate zi from the model f(·|θi)
c) Calculate xi = η(zi)
Output some regression RF predictors to infer posterior expec-
tations, quantiles, variances and covariances
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
35 / 41

ABC random forests
Parameter inference
Expectations Construct d regression RF, one per dimension
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
36 / 41

ABC random forests
Parameter inference
Expectations Construct d regression RF, one per dimension
Quantiles very nice trick to estimate the cdf, no new forest
Quantile Regression Forests Meinshausen (2006) JMLR
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
36 / 41

ABC random forests
Parameter inference
Expectations Construct d regression RF, one per dimension
Quantiles very nice trick to estimate the cdf, no new forest
Quantile Regression Forests Meinshausen (2006) JMLR
Variances use of a out-of-bag trick, no new forest
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
36 / 41

ABC random forests
Parameter inference
Expectations Construct d regression RF, one per dimension
Quantiles very nice trick to estimate the cdf, no new forest
Quantile Regression Forests Meinshausen (2006) JMLR
Variances use of a out-of-bag trick, no new forest
Covariances new forests for which the responses variables are
the products of out-of-bag errors
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
36 / 41

ABC random forests
Parameter inference
We constructed forests able to estimate everywhere in the space
of summary statistics but we are interested only in one point, the
observed dataset
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
37 / 41

ABC random forests
Parameter inference
We constructed forests able to estimate everywhere in the space
of summary statistics but we are interested only in one point, the
observed dataset
construct local random forest, thesis of Louis Raynal
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
37 / 41

ABC and PAC-Bayes
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
38 / 41

ABC and PAC-Bayes
Bayesian and PAC-Bayesian frameworks have learned from
each other
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
38 / 41

ABC and PAC-Bayes
Bayesian and PAC-Bayesian frameworks have learned from
each other
PAC-Bayesian Theory Meets Bayesian Inference
Germain, Bach, Lacoste, Lacoste-Julien (2016) NIPS
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
38 / 41

ABC and PAC-Bayes
Bayesian and PAC-Bayesian frameworks have learned from
each other
PAC-Bayesian Theory Meets Bayesian Inference
Germain, Bach, Lacoste, Lacoste-Julien (2016) NIPS
choosing the negative log-likelihood loss function: minimizing
the PAC-Bayes bound is equivalent to maximizing the marginal
likelihood
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
38 / 41

ABC and PAC-Bayes
Bayesian and PAC-Bayesian frameworks have learned from
each other
PAC-Bayesian Theory Meets Bayesian Inference
Germain, Bach, Lacoste, Lacoste-Julien (2016) NIPS
choosing the negative log-likelihood loss function: minimizing
the PAC-Bayes bound is equivalent to maximizing the marginal
likelihood
A general framework for updating belief distributions Bis-
siri, Holmes and Walker (2016) JRSS B
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
38 / 41

ABC and PAC-Bayes
Bayesian and PAC-Bayesian frameworks have learned from
each other
PAC-Bayesian Theory Meets Bayesian Inference
Germain, Bach, Lacoste, Lacoste-Julien (2016) NIPS
choosing the negative log-likelihood loss function: minimizing
the PAC-Bayes bound is equivalent to maximizing the marginal
likelihood
A general framework for updating belief distributions Bis-
siri, Holmes and Walker (2016) JRSS B
The Safe Bayesian: Learning the Learning Rate via the Mix-
ability Gap Gr¨unwald (2012) ALT
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
38 / 41

ABC and PAC-Bayes
ABC approximations are based on the existence and the use of
a generative model
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
39 / 41

ABC and PAC-Bayes
ABC approximations are based on the existence and the use of
a generative model
antithetical with the PAC-Bayes paradigm
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
39 / 41

ABC and PAC-Bayes
ABC approximations are based on the existence and the use of
a generative model
antithetical with the PAC-Bayes paradigm
ABC approximations are not useful when the calculation of the
likelihood is tractable
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
39 / 41

ABC and PAC-Bayes
ABC approximations are based on the existence and the use of
a generative model
antithetical with the PAC-Bayes paradigm
ABC approximations are not useful when the calculation of the
likelihood is tractable
not a good idea to use ABC as an inferential tools for the
PAC-Bayes pseudo-posterior
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
39 / 41

ABC and PAC-Bayes
ABC approximations are based on the existence and the use of
a generative model
antithetical with the PAC-Bayes paradigm
ABC approximations are not useful when the calculation of the
likelihood is tractable
not a good idea to use ABC as an inferential tools for the
PAC-Bayes pseudo-posterior
The ABC toolbox seems unable to bring anything to the
PAC-Bayesian framework
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
39 / 41

ABC and PAC-Bayes
On the other hand
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

ABC and PAC-Bayes
On the other hand
ABC concerns are machine learning concerns
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

ABC and PAC-Bayes
On the other hand
ABC concerns are machine learning concerns
Use PAC-Bayes learning on the ABC reference table
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

ABC and PAC-Bayes
On the other hand
ABC concerns are machine learning concerns
Use PAC-Bayes learning on the ABC reference table
Contrary to the standard context
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

ABC and PAC-Bayes
On the other hand
ABC concerns are machine learning concerns
Use PAC-Bayes learning on the ABC reference table
Contrary to the standard context
▶we generate the learning set
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

ABC and PAC-Bayes
On the other hand
ABC concerns are machine learning concerns
Use PAC-Bayes learning on the ABC reference table
Contrary to the standard context
▶we generate the learning set
▶we are only interested in one point, the observed dataset!
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

ABC and PAC-Bayes
On the other hand
ABC concerns are machine learning concerns
Use PAC-Bayes learning on the ABC reference table
Contrary to the standard context
▶we generate the learning set
▶we are only interested in one point, the observed dataset!
Probably approximate Bayesian computation: nonasymp-
totic convergence of ABC under misspeciﬁcation Ridgway
(2017) Preprint
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

ABC and PAC-Bayes
On the other hand
ABC concerns are machine learning concerns
Use PAC-Bayes learning on the ABC reference table
Contrary to the standard context
▶we generate the learning set
▶we are only interested in one point, the observed dataset!
Probably approximate Bayesian computation: nonasymp-
totic convergence of ABC under misspeciﬁcation Ridgway
(2017) Preprint
Convergence of the ABC posterior under model misspeciﬁcation
Use of concentration inequalities, PAC-Bayesian analysis
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
40 / 41

End
Yesterday with Benjamin and Pascal in a Chinese restaurant, the
bill arrives... with a cake and a hidden message:
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
41 / 41

End
Yesterday with Benjamin and Pascal in a Chinese restaurant, the
bill arrives... with a cake and a hidden message:
This year, take comfort in your rituals, but be open to new expe-
riences
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
41 / 41

End
Yesterday with Benjamin and Pascal in a Chinese restaurant, the
bill arrives... with a cake and a hidden message:
This year, take comfort in your rituals, but be open to new expe-
riences
A clear sign, I should try to use some PAC-Bayesian results
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
41 / 41

End
Yesterday with Benjamin and Pascal in a Chinese restaurant, the
bill arrives... with a cake and a hidden message:
This year, take comfort in your rituals, but be open to new expe-
riences
A clear sign, I should try to use some PAC-Bayesian results
Thank you very much for your attention
Jean-Michel Marin (UM, CNRS & IMAG)
NIPS 17 PAC-Bayes workshop
9 December 2017
41 / 41

