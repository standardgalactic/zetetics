A UNIFYING FRAMEWORK FOR FORMAL THEORIES OF
NOVELTY:FRAMEWORK, EXAMPLES AND DISCUSSION
A PREPRINT
T. E. Boult1, P. A. Grabowicz5, D. S. Prijatelj2, R. Stern6, L. Holder4, J. Alspector3,
M. Jafarzadeh1, T. Ahmad1, A. R. Dhamija1, A. Shrivastava7, C. Vondrick8, W. J. Scheirer2
1 U. Col. Col. Springs, 2 U. Notre Dame, 3 IDA/ITSD, 4 Wash. State U.
, 5 U. Mass., 6 PARC, BGU, 7 U. Maryland, 8 Columbia U.
{tboult | mjafarzadeh | tahmad | adhamija}@vast.uccs.edu
grabowicz@cs.umass.edu derek.prijatelj@nd.edu rstern@parc.com jalspect@ida.org holder@wsu.edu
abhinav@cs.umd.edu vondrick@cs.columbia.edu walter.scheirer@nd.edu
December 9, 2021
ABSTRACT
Managing inputs that are novel, unknown, or out-of-distribution is critical as an agent moves from
the lab to the open world. Novelty-related problems include being tolerant to novel perturbations
of the normal input, detecting when the input includes novel items, and adapting to novel inputs.
While signiﬁcant research has been undertaken in these areas, a noticeable gap exists in the lack of a
formalized deﬁnition of novelty that transcends problem domains. As a team of researchers spanning
multiple research groups and different domains, we have seen, ﬁrst hand, the difﬁculties that arise
from ill-speciﬁed novelty problems, as well as inconsistent deﬁnitions and terminology. Therefore, we
present the ﬁrst uniﬁed framework for formal theories of novelty and use the framework to formally
deﬁne a family of novelty types. Our framework can be applied across a wide range of domains,
from symbolic AI to reinforcement learning, and beyond to open world image recognition. Thus, it
can be used to help kick-start new research efforts and accelerate ongoing work on these important
novelty-related problems.
1
Introduction
“What is novel?” is an important AI research question that informs the design of agents tolerant to novel inputs. Is a
noticeable change in the world that does not impact an agent’s task performance a novelty? How about a change that
impacts performance but is not directly perceptible? If the world has not changed but the agent senses a random error
that produces an input that leads to an unexpected state, is that novel?
With decades of work and thousands of papers covering novelty detection and related research in anomaly detection,
out-of-distribution detection, open set recognition, and open world recognition, one would think that a consistent uniﬁed
deﬁnition of novelty would have been developed. Unfortunately, that is not the case. Instead, we ﬁnd a plethora of
variations on this theme, as well as ad hoc use and inconsistent reuse of terminology, all of which injects confusion as
researchers discuss these topics.
This paper introduces a unifying formal framework of novelty. The framework seeks to formalize what it means for
an input to be a novelty in the context of agents in artiﬁcial intelligence or in other learning-based systems. Using
the proposed framework, we formally deﬁne multiple types of novelty an agent can encounter. The goal of these
deﬁnitions is to be broad enough to encompass and unify the full range of novelty models that have been proposed in
the literature Pimentel et al. (2014); Markou and Singh (2003a,b); Scheirer et al. (2013); Bendale and Boult (2015);
Langley (2020). An important generalization beyond prior work is that we consider novelty in the world, observed
space, and agent space (see Fig. 1), with dissimilarity and regret operators critical to our deﬁnitions. The overarching
goal is a framework such that researchers have clear deﬁnitions for the development of agents that must handle novelty,
arXiv:2012.04226v1  [cs.AI]  8 Dec 2020

A PREPRINT - DECEMBER 9, 2021
World State in World Space 
Observed Space
Perceptual Operator
Agent Action
World Dissimarlity Operator
and World regret function  
Observed  Dissimarlity Operator
 and Observed regret function 
 
with State recognition function
 
Experience Tensors
 
 (World experience only
accessible to oracle)
Agent     Input
Task
Figure 1: Main elements of the implicit theories of novelty. The agent can only access world information indirectly
through a perceptual operator P. It can then update its internal state and act on the world state. Items with dashed
outlines are outside of the task or agent but are critical to deﬁning novelty. In the framework a theory of novelty is
obtained by specifying: world W and the world dimensionality d′, observation space O accessible to the agent and its
dimensionality d, and agent state space S family of perceptual operators P with associated Wt processed world regions,
task-dependent world dissimilarity functions Dw,T ;Et with associated threshold δw, task-dependent observation-space
dissimilarity functions Do,T ;Et with threshold δo, agent α in state st ∈S at time t, using state recognition function
ft(x, s) to determine the action at ∈A to be taken, world regret function Rw,T , observation-space regret function
Ro,T . and agent-space regret function Ra,T . Every set of these operators / functions / values deﬁnes a different theory
of novelty for its associated task.
including support for agents / algorithms that incrementally learn from novel inputs. A longer version of this theory
with example applications to three different domains can be found at Boult et al. (2020).
Our framework supports implicit theories of novelty, meaning the deﬁnitions use functions to implicitly specify if
something is novel. The framework does not require a way to generate novelties, but rather it provides functions that
can be used to evaluate if a given input is novel. This is similar to how any 2D shape can be implicitly deﬁned by a
function f(x, y) = 0, whether or not there is a procedure for generating the shape. We contend any constructive or
generative theory of novelty Langley (2020) must be incomplete because the construction or generation of deﬁned
worlds, states, and any enumerable set of transformations between them form, by deﬁnition, a closed world. We note,
however, that a constructive model can be consistent with our deﬁnition, but we do not require a constructive model.
2
Formalizing Novelty
We present frameworks for formalizing novelty for static or learning-based agents, operating in a setting where handling
unknown items is required. Fig. 1 shows the main elements of a novelty problem for task T . The formulation can support
a wide range of novelty problems including being robust to novelties, detecting novelties, learning from novelties or
generating novelties. The paper’s formalization is about theories, rather than “a theory,” because when the deﬁnition’s
set of items and associated functions are provided, a different theory of novelty is deﬁned. There are inﬁnitely many
such theories of novelty for any given task.
2

A PREPRINT - DECEMBER 9, 2021
For simplicity of presentation, our world and observation space are d′ and d dimensional spaces of real numbers. Let a
world state be wt ∈W, and let W ⊂Rd′, d′ ≥d, be the representation of the world state at time t obtained from a
subset of W, the allowed world states. Note that W need not be the entire world. It can be only that region of space
and time which is sampled during operation or that is relevant for the task. It could even be a ﬁnite set of items or
relationships to be considered by the system.
Let our observation at time t be xt ∈O in observation-space O ⊂Rd. Let P : Rd′ 7→Rd be the perceptual operator at
time t, which maps world spaces to observation spaces, i.e., xt = P(wt). The agent never has direct access to the world
and can only access it via the perceptual operator. This operator is generally a combination of real-time sensing plus
external processing on that sensed data. It can also include pre-processing on stored sensory data. But everything in
this operator is to be considered external to the agent. Since system hardware can change over time, this mapping is a
function of time. If the perceptual operator processes only a subregion of any world state, we let Wt be the subregion of
the world that has been processed up to time t. Accordingly, any world states that differ only outside the processed
subregions are indistinguishable.
Let agent αT solving task T at time t have an internal state representation st ∈S, where S is the space of possible
states. An agent reacts to the environment, but a common architecture in the design of autonomous agents is for
the agent to have an internal state st that inﬂuences how the agent will act at time t. To capture this common agent
architecture, we deﬁne a state recognition function that maps an observation-space input xt plus the current agent state
into its new internal state. Formally, let ft(·, ·) : Rd × S 7→S × A be a state recognition function at time t mapping an
observation-space input xt to its internal state space estimation y of the world state wt yielding the action at ∈A to be
taken to reach new state st, where A is the space of actions.
Based on the recognition function output, the agent takes an action. Then, the state transition function, T(wt, at) :
W × A 7→W, maps the current world state and agent’s selected action at time t to a new world state. This mapping
can be stochastic, e.g., a Markov decision process. Note for many real problems the world may change outside of any
assumptions, including oracle’s assumptions, in which case we cannot assume wt+1 = T(wt, at). For problems which
traditionally do not have state or specify an action, such as machine learning or open set classiﬁcation, the agent “action”
is reporting the results of the recognition function. Actions in learning systems may also update the parameters in the
state or may change state recognition functions, which is why ft is a function of time t.
Let N ⊆S be the possible empty set of internal states which are associated with the agent determining the world is
novel. The world state wt is identiﬁed by the agent as novel if st ∈N. When dealing with novelty, the agent may obtain
unexpected observation states, which could map to potentially unexpected internal states (e.g., a “crash” state).
To represent history, let Et = {(w1) . . . (wt)} be our experience tensor of states. It is important to note that the
experience tensor is not about what the agent “remembers.‘’ It is external to the agent and is about what world states the
agent has experienced up to and including time t. The experience tensor is integral to deﬁning novelty, which depends
on dissimilarity between the world and experience.
3
Dissimilarity and Regret Measures
For a given task T , data generally do not need to match exactly to be considered “the same” with respect to the task’s
objectives. Note that while we call it “a task,” it could include a set of objectives (e.g., a multi-task problem). In any
task, multiple dimensions can impact the task performance and determine when a state is effectively the same or how
different a state is from prior experience. We formulate this in terms of a measure of dissimilarity, which depends
strongly on the task. It is important to note that one should be careful in a priori deﬁnitions of what matters to a task, as
novel world states may have an unpredictable impact on it.
Essential to our framework is a set of task-dependent dissimilarity functions Dw,T ;Et : W × W 7→R+ and Do,T ;Et :
W × W 7→R+, which measure dissimilarity between items in the world space and the observation space respectively.
The perceptual dissimilarity may access the world but must map to observed space with the perceptual operator before
mapping to R+. Dissimilarity measures produce non-negative values that are a generalization of distance metrics. It
is possible to have zero dissimilarity for items that are identical in terms of the task, even if they are distinct items in
the world / observation space. Novelty deﬁnitions will generally include a similarity threshold of δ on the maximum
allowed dissimilarity for items treated the same by the task, which depends on the task / user requirements for matching.
Dissimilarity may be an actual distance metric, a statistical measure, an information-theoretic measure, or such measures
combined with ontological or hierarchical information. If the world modeling is probabilistic, then the dissimilarity
function may, but need not, be based on probabilistic computations. We use dissimilarity rather than distance since it is
well known that human perception / recognition is non-metric Tversky (1977); Scheirer et al. (2014).
3

A PREPRINT - DECEMBER 9, 2021
Not all novel items are of interest or present a risk to the agent. While we cannot know the risk of an unknown item until
it becomes known, we can, after the fact, assign a regret score associated with new world / observed / agent state after
the action at(wt, st). We let Rf,T : (O × A) 7→R, Ro,T : (O × A) 7→R and Rw,T : (W × A) 7→R be the regret
operators for task T in agent, observation, and world space respectively. Agent and observation regret must process the
world state via the perceptual operator. A suboptimal agent may have regret higher than observational regret, which
should be deﬁned with respect to an oracle or the best agent on observed data.
We separate these three because it allows one to reason about regret in terms of speciﬁc models. Only an oracle that has
access to ground-truth data has the ability to actually compute regret in world space. However, an agent can approximate
regret in observation space, especially given the ground-truth answers. It is important to note that agent-computed regret
can only be an approximation, even given the ground-truth action / outcome, because the optimal decision with limited
data can still lead to bad outcomes. Hence, an agent might estimate regret even if there should be none.
4
Example: CartPole Domain
As an example of using the constructs deﬁned above, consider the CartPole in the OpenAI Gym.1 In this domain, a
cart has a pole connected to it, and the task T is to push the cart left or right so as to prevent the attached pole from
falling. The world state w in the CartPole domain comprises the following real values (with default / initial values in
parentheses): gravity G (9.8), mass of cart Mc (1.0), mass of pole per unit length Mp (0.1), length of pole L (1.0),
force of push Fp (10.0), horizontal force acting on the cart Fh (0), min / max cart position zmin(−2.4), zmax(+2.4),
min / max pole angle φmin (−12◦), φmax (+12◦), time between state updates τ (0.02 seconds), start time t (0). The
initial cart position z0, cart velocity ˙x0, pole angle φ0, and pole angular velocity ˙φ0 are all i.i.d. random samples from
[−0.05...0.05]. The perceptual operator P in this domain is a projection of the world state that returns only the cart
position z, cart velocity ˙z, pole angle φ, and pole angular velocity ˙φ as the 4D observed state vector x = (z, ˙z, φ, ˙φ).
Based on these world state features, the task is more precisely deﬁned as: given x, select an action from the space of
A = {Left, Right} to maintain the cart position within the min / max cart position and maintain the pole angle within
the min / max pole angle. Note that the last four features (cart position, cart velocity, pole angle, pole angular velocity)
are determined via a deterministic physics model based on the full world state combined with agent actions.
4.1
Dissimilarity and Regret in CartPole
State transitions in the CartPole domain are determined by the equations of motion and can be simulated in discrete
time with numerical integration. In this example we assume transitions between observed states are Markovian, which
simpliﬁes presentation; other theories for this domain could consider dissimilarity and regret in more general settings.
The dissimilarity measure for CartPole might take a simple form (e.g., the Euclidean distance in the world or observed
space). However, Euclidean distance between world states is affected by factors other than novelties, including the
choice of units. It is also insensitive to the variation in the impact of different variables on the state evolution or task
outcome. Proper conditioning would reduce dependency on units and account for states that correspond to different
samples from the same world (e.g., the same CartPole world with a different initial position of the pole is not considered
novel).
To avoid these issues, we compare two worlds, w and ˇw, the states that proceed from a common observed state and
action. We consider an action of an optimal agent, a∗, in the ﬁrst world, w, and choose as the common observed states
the states that the agent encounters, ˇxt, in the second world, ˇw. Then, we average over all these states, including the
initial observed state:
Do,T (w, ˇw) = (P(T(M(w, ˇxt), a∗
t ))−
P(T(M( ˇw, ˇxt), a∗
t ))2,
where M(w, x) : W 7→W is a function that returns a modiﬁed w whose observed components are replaced with the
values from x, such that P(M(w, x)) = x, while all other components remain unchanged. Overall, the dissimilarity
measures the average distance between observed states in two different worlds that proceed from a common observed
state and action, ˇxt and a∗. The agent is optimal in the ﬁrst world, while the trajectory is from the second world, so this
dissimilarity measure can be seen as an expected state prediction error of the optimal agent trained in the ﬁrst world and
tested in the second world. The world dissimilarity is deﬁned analogously, except it does not use the perceptual operator.
Note this is an asymmetric dissimilarity measure as the selected agent is optimal for the ﬁrst world and need not be
optimal for the second, and then marginalizes over the initial conditions and time in the second world. Due to the
1https://github.com/openai/gym/blob/master/gym/envs/classic control/cartpole.py
4

A PREPRINT - DECEMBER 9, 2021
Figure 2: Average dissimilarity, Ex′
0,tDo,T ( ˇwt, wt), between the future states expected and observed by agents that
were trained in a world with incorrect value of the magnitude of pushing force, Fp (left panel), or a horizontal force
acting on the cart, Fh (right panel). The expectation is computed over 20 samples of the initial world state w0.
conditioning, any pair of states from the same world will have zero dissimilarity. Furthermore, since these depend on
the choice of “optimal action” a∗
t from the ﬁrst world, it implicitly normalizes for how different dimensions (variables)
impact the evolution of the world state. One can consider actions of an optimal reference agent under given conditions
(e.g., a non-adaptive agent that performs optimally w.r.t. the main task T in a non-novel CartPole world. If it is infeasible
to obtain an optimal agent in practice, then one may use an arbitrary reference agent and its expectation, with the caveat
the dissimilarity measure will depend on the oracle’s reference agent.
Regret for the agent’s action at state xt w.r.t. T is
Ro,T (xt, at) = Rw,T (wt, at) = ℓT (wt, at) −ℓT (wt, a∗
t ),
where ℓT (wt, at) is the loss incurred in the world wt by a given agent that observes state xt and performs action at,
while ℓT (wt, a∗
t ) is the loss of an agent that performs the optimal action a∗
t in the world. In CartPole, the loss at the
given time step is 1 if the pole angle or cart’s position in the next time steps is beyond the threshold, |φ| > φmax or
|z| > zmax, otherwise the loss is 0. In this CartPole domain, the world regret is the same as observation regret, because
there are no hidden dynamic elements interacting with pole or cart, such as an invisible pendulum that hangs above the
pole and sometimes hits it. Once such elements are introduced, the two regrets may differ.
4.2
Agent State and Optimal Non-Adaptive Agents
In simple environments, like CartPole, it is possible to obtain optimal or near-optimal non-adaptive agents. A particularly
simple version of CartPole is the one where the agent’s action space is binary, i.e., the agent can choose to push the cart
left or right, and its reward is the time that the pole is up. In such a CartPole environment, an agent can ﬁnd optimal
actions by performing what-if simulations of the world and searching for actions that result in the best performance, i.e.,
the agent can simulate what would happen if it pushed the cart left or right and then choose the next action that results
in better performance. In this paper, for simplicity, we present the results for a near-optimal single-look-ahead agent.
The action is chosen based on the distance, ||β⊺(xt −xs)||, between the expected state resulting from the action, xt,
and the desired state, xs = (0, 0, 0, 0), where the weight vector β = (0, 0, 1, 0.005) weighs discrepancy in φ the most
and ignores the discrepancies in z and ˙z. The state of this agent is described by the parameters used to simulate system
dynamics and the weight vector, s = (G, Mc, Mp, L, Fp, Fh, τ, β). In non-adaptive agents, these parameters are ﬁxed
to the values that either correspond to the environment (no novelty) or do not (a novelty).
4.3
Measurements and Observations
As expected, the dissimilarities in observed state prediction capture the novelties in the magnitude of pushing force and
in a horizontal force acting on the cart (Figure 2). The larger the distance between the value of a given parameter in the
environment and its value assumed by the agent, the larger the dissimilarity in state prediction.
5

A PREPRINT - DECEMBER 9, 2021
Figure 3: Average reward, 1-Ew0,tℓT (wt, a∗
t ), of non-adaptive agents that are trained to act optimally in a certain world
but are tested in another world.
Surprisingly, in CartPole, reasonable changes to many of the aforementioned latent parameters, like the gravity or the
magnitude of pushing force, do not impact an optimal non-adaptive agent’s performance (left Figure 3). The columns
on the verge of the heat map correspond to the achievable limits in performance, i.e., the leftmost column corresponds
to such a small force magnitude that the push is insufﬁcient to counter the gravity, whereas the rightmost column
corresponds to such a large force magnitude that the push instantly rotates the pole beyond the allowed region. We
conclude that the optimal agent performs just as well in the novel world where gravity or pole length have new values,
despite making simulations that assume incorrect values of gravity or pole length, i.e., the values from the non-novel
world. Note that this agent is non-adaptive, so it does not change its internal model to adapt to the new environment, i.e.,
it has exactly the same internal model as in the non-novel environment. Again, the columns at the verge of the heat map
mark inherent performance limits, i.e., if the horizontal force is larger than the magnitude of the pushing force, Fp = 10,
then the pushes are insufﬁcient to counteract the horizontal force and the pole is destined to fall, despite taking optimal
actions.
Less surprisingly, some of the latent parameters impact the agent’s performance in an intuitive way (e.g., a horizontal
force applied to the cart) (right Figure 3). This happens because the horizontal force directly impacts the action that the
agent should choose: if the horizontal force pushes the cart right, then the agent should probably push it left, and vice
versa. The larger the difference between the horizontal force in the environment and assumed by the agent, the higher
the drop in the agent’s performance.
To distinguish between novelties that affect or do not affect the task performance, we use the regret of an optimal agent
trained in the non-novel world and tested in the novel world, i.e., Ro,T (xt, a∗
t ). Novelties with Ro,T (xt, a∗
t ) = 0 do
not affect task performance and can be ignored by agents that are near-optimal without a performance drop. By contrast,
Ro,T (xt, a∗
t ) > 0 tells us that the novelty impacts the performance of an optimal non-adaptive agent and that the agent
can update its state to improve its performance, i.e., learn the novelty. Naturally, one can develop an adaptive version of
this agent by learning an estimate of world state parameters from observations and using them to perform more accurate
simulations and taking better actions.
5
Types of Novelty
For the deﬁnitions, we introduce the primary types of novelty, with the subtypes deﬁned by combining primary types
and regret (see Table 1 and Boult et al. (2020)). Using our deﬁnitions of world, observation space, internal states, and
the corresponding dissimilarity functions, we can formally deﬁne the following primary types of novelty.
World novelty. A world state
ˇw
∈
W is considered a world novelty for an agent α at time t if
minw∈Ew,t Dw,T (w, ˇw; Et) > δw. That is, any world state ˇw sufﬁciently dissimilar from every world state in the
experience tensor is a world-level novelty. In general, only an oracle with access to Ew,t and Dw,T can determine
that a world state is truly novel. If the world representation is viewed as including distributional information (e.g.,
6

A PREPRINT - DECEMBER 9, 2021
World
Novelty
Observation
Novelty
Agent
Novelty
World Regret Rw,T > ϵw
No World Regret Rw,T ≤ϵw
Perceptual Regret
Ro,T > ϵo
No Perceptual Regret
Ro,T ≤ϵo
Perceptual Regret
Ro,T > ϵo
No Perceptual Regret
Ro,T ≤ϵo
Yes
Yes
Yes
Unanimous w/ Regret
Unanimous Nuisance
Unanimous Nuisance
Unanimous Managed
No
Ignored
Ignored Nuisance
Ignored Nuisance
Ignored Managed
No
Yes
Imperceptible
Imperceptible Nuis.
Imperceptible Nuis.
Managed Imperceptible
No
Imperceptible Ignored
Imper. Ignored Nuis.
Imper. Ignored Nuis.
Managed Imperceptible
No
Yes
Yes
Faux
Faux Nuis.
Faux Nuis.
Managed Faux
No
Faux Ignored
Faux Ignored Nuis.
Faux Ignored Nuis.
Managed Faux
No
Yes
Faux
Faux Nuis.
Faux Nuis.
Managed Faux
No
No novelty
No novelty Nuis.
No novelty Nuis.
No Novelty
Table 1: Subtypes of novelty deﬁned by interaction of primary novelty types and regret. Some combinations of states get
multiple labels (e.g., Unanimous Nuisance is both Unanimous (all types of novelty present) and Nuisance (inconsistent
regret values)).
probabilities of various items occurring) then a change in distributional parameters can be a world-level novelty even
if no new “objects” occur in the world. Thus world-level novelty can produce problems of domain adaption, not just
domain transfer.
Observation novelty. A world state ˇw ∈W is considered an observation novelty for an agent α at time t iff
minw∈Ew,t Do,T (w, ˇw; Et) > δo. That is, an observation novelty is the observation-space state obtained for any
world state ˇw that, when projected through a perceptual operator, is sufﬁciently dissimilar from every observation-space
state in the agent’s experience tensor. Note that in this deﬁnition, the observed world state, ˇw, is subject to the current
perceptual operator P at time t and is compared to the observation-space states in the experience tensor, which may
have used a potentially different perceptual operator P. It is not surprising that the same world state may be novel
at one point in time but not novel at another. However, it may be surprising that if the perceptual operator changes
over time, then something can be perceptually novel at time t even if it was not perceptually novel at time t −1. For
example, consider a transmission glitch creating errors in a static scene that has been viewed previously. It is important
to note that observation novelty is deﬁned considering all experience, which permits observation novelty that includes
distributional shifts or reasoning between states to detect novelty that impact dynamics. If the agent had access to the
true dissimilarity Do,T , it could use that to deﬁne its state recognition function f. However, in practice, an agent will
not have access to Do,T , since it is trying to learn such a function from the data or was programmed with static rules to
approximate it. Furthermore, agents rarely store all inputs.
Agent novelty. An observation-space state x = P( ˇw ∈W) is considered an agent novelty for an agent α at time t iff
ft(x) = N. That is, x is an agent novelty iff the agent at time t cannot map x to any of its internal states or maps to a
special state for when it detects novel inputs. We note that this deﬁnition does not consider something novel if the state
recognition functions ft associate x with an incorrect state.
These novelty types are not mutually exclusive, and their combinations deﬁne the following notable novelty sub-types:
• Unanimous novelty is any world novelty w for which the perceptual operator produces an observation-space
state that is both an observation novelty and an agent novelty. Unanimous novelty is correctly detected by the
agent.
• Imperceptible novelty is any world novelty w for which the perceptual operator produces an observation
space state x that is not an observation novelty. Accordingly, the agent cannot directly react to such novelties.
• Faux novelty is a world state w that is not a world novelty but its corresponding observation state x is an
observation novelty or an agent novelty.
• Ignored novelty is any world state w such that its corresponding observation state x is not an agent novelty
while either w is a world novelty or x is an observation novelty. Ignored novelty does not have to result in poor
performance (e.g., a non-adaptive agent may ignore all novelties while still performing well in the presence of
them).
Combining these novelty types and sub-types with the regret functions (Rf,T , Ro,T , and Rw,T ) allows us to formally
deﬁne additional useful novelty sub-types including:
• Managed novelty is a world novelty w such that its implication on regret (performance) is minimal, i.e.,
Rf,T (w) < ϵ.
7

A PREPRINT - DECEMBER 9, 2021
• Nuisance novelty is a novelty for which the world regret and the observation regret signiﬁcantly disagree.
These are important for evaluations deﬁning novelty ground-truth and associated world-regret, these sub-types need to
be avoided or at least accounted for in evaluation metrics.
5.1
Novelty Types in CartPole
In the previous section, we have deﬁned and measured dissimilarity and regret in the observation space. It is straightfor-
ward to develop the corresponding deﬁnitions in the world space, i.e., world dissimilarity shall include the unobserved
parts of the world, in addition to observed parts, while world regret shall depend on the unobserved parts of the world.
These notions of dissimilarity and regret can be used to categorize novelty types. The novelties discussed above in the
CartPole section are both world and observation novelties, since both world and perceptual dissimilarities are larger
than zero, Do,T (ˇxt, xt) > 0 and Dw,T ( ˇwt, wt) > 0.
If the CartPole environment had an additional unobserved cart that does not inﬂuence the main cart and pole, then
any novelty in that unobserved cart would be an imperceptible world novelty since it would not inﬂuence transitions
between the observed states, Do,T (ˇxt, xt) = 0 and Dw,T ( ˇwt, wt) > 0. If task performance (world regret) in either of
these last two also required detection of novelty, then they would be an imperceptible nuisance novelty.
6
Conclusion
We see three primary contributions of this formalization of novelty that will spur further research. First, formalization
forces one to specify (or intentionally disregard) the required items in the theory. This can lead to insights about
the problem and ﬁll in knowledge gaps. For example, when applying the theory to the CartPole problem, numerous
unanticipated issues were highlighted, new predictions made, and new experiments validated the new insights.
Second, formalization provides a common language to deﬁne and compare models of novelty across problems. The
precision of terms reduces confusion, while the ﬂexibility allows it to be applied to a wide range of problems.
Third, the formalization allows one to make predictions about where or why experiments incorporating some form of
novelty might run into difﬁculties. For example, when the world-level and perceptual-level dissimilarity assessments
disagree, we predict novelty problems will be more difﬁcult. One example of difﬁculty is world-disparity using variables
not represented in perceptual space. Another is when there are many possible world labels, but the input is only assigned
one label that is used for assessing world-level dissimilarity. In this case, the theory predicts a greater difﬁculty with
such novelty, especially if the assigned label is associated with a physically smaller aspect of the observation.
Biological intelligence has a remarkable capacity to generalize novel inputs with ease, yet artiﬁcial agents continue
to struggle with this behavior. It is our hope that the adoption and use of the framework proposed here leads to the
development of more effective solutions for novelty management and to make agents more robust to novel changes in
their world.
By formalizing CartPole using our novelty framework, we gained insights into what are meaningful “novelty” problems
for this task. We showed how to develop better measures to predict when novelty would be easy or hard to manage or
to detect. In line with this, our team of researchers has been reﬁning this theory and applying it to multiple problem
domains. More details can be found in the longer arXiv version Boult et al. (2020).
Acknowledgment
This research was sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Army
Research Ofﬁce (ARO) under multiple contracts/agreements including HR001120C0055, W911NF-20-2-0005,W911NF-20-2-
0004,HQ0034-19-D-0001, W911NF2020009. The views contained in this document are those of the authors and should not be
interpreted as representing the ofﬁcial policies, either expressed or implied, of the DARPA or ARO, or the U.S. Government.
References
Bendale, A.; and Boult, T. E. 2015. Towards Open World Recognition. In IEEE CVPR, 1893–1902.
Boult, T. E.; Jafarzadeh, M.; Ahmad, T.; Dhamija, A.; Scheirer, W. J.; Prijatelj, D.; Alspector, J.; Holder, L.; Grabowicz, P.; Stern, R.;
Shrivastava, A.; and Vondrick, C. 2020. Implicit theories of Novelty: theory and applicaitons. arXiv preprint arXiv:?????? .
Langley, P. 2020. Open-World Learning for Radically Autonomous Agents. In AAAI, 13539–13543. JSTOR.
Markou, M.; and Singh, S. 2003a. Novelty detection: a review part 1: statistical approaches. Signal processing 83(12): 2481–2497.
Markou, M.; and Singh, S. 2003b. Novelty detection: a review part 2: neural network based approaches. Signal processing 83(12):
2499–2521.
8

A PREPRINT - DECEMBER 9, 2021
Pimentel, M. A.; Clifton, D. A.; Clifton, L.; and Tarassenko, L. 2014. A review of novelty detection. Signal Processing 99: 215–249.
Scheirer, W. J.; de Rezende Rocha, A.; Sapkota, A.; and Boult, T. E. 2013. Toward open set recognition. IEEE TPAMI 35(7):
1757–1772.
Scheirer, W. J.; Wilber, M. J.; Eckmann, M.; and Boult, T. E. 2014. Good recognition is non-metric. Pattern Recognition 47(8):
2721–2731.
Tversky, A. 1977. Features of similarity. Psychological review 84(4): 327.
9

