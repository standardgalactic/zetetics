Theoretically Principled Trade-off between Robustness and Accuracy
Hongyang Zhang‚àó
CMU & TTIC
hongyanz@cs.cmu.edu
Yaodong Yu‚Ä†
University of Virginia
yy8ms@virginia.edu
Jiantao Jiao
UC Berkeley
jiantao@eecs.berkeley.edu
Eric P. Xing
CMU & Petuum Inc.
epxing@cs.cmu.edu
Laurent El Ghaoui
UC Berkeley
elghaoui@berkeley.edu
Michael I. Jordan
UC Berkeley
jordan@cs.berkeley.edu
Abstract
We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design
of defenses against adversarial examples. Although this problem has been widely studied empirically,
much remains unknown concerning the theory underlying this trade-off. In this work, we decompose
the prediction error for adversarial examples (robust error) as the sum of the natural (classiÔ¨Åcation) error
and boundary error, and provide a differentiable upper bound using the theory of classiÔ¨Åcation-calibrated
loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and
measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES,
to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in
real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision
Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by
11.41% in terms of mean ‚Ñì2 perturbation distance.
1
Introduction
In response to the vulnerability of deep neural networks to small perturbations around input data [SZS+13],
adversarial defenses have been an imperative object of study in machine learning [HPG+17], computer
vision [SKN+18, XWZ+17, MC17], natural language processing [JL17], and many other domains. In machine
learning, study of adversarial defenses has led to signiÔ¨Åcant advances in understanding and defending against
adversarial threat [HWC+17]. In computer vision and natural language processing, adversarial defenses
serve as indispensable building blocks for a range of security-critical systems and applications, such as
autonomous cars and speech recognition authorization. The problem of adversarial defenses can be stated as
that of learning a classiÔ¨Åer with high test accuracy on both natural and adversarial examples. The adversarial
example for a given labeled data (x, y) is a data point x‚Ä≤ that causes a classiÔ¨Åer c to output a different label
on x‚Ä≤ than y, but is ‚Äúimperceptibly similar‚Äù to x. Given the difÔ¨Åculty of providing an operational deÔ¨Ånition
of ‚Äúimperceptible similarity,‚Äù adversarial examples typically come in the form of restricted attacks such as
œµ-bounded perturbations [SZS+13], or unrestricted attacks such as adversarial rotations, translations, and
deformations [BCZ+18, ETT+17, GAG+18, XZL+18, AAG19, ZCS+19]. The focus of this work is the
former setting, though our framework can be generalized to the latter.
Despite a large literature devoted to improving the robustness of deep-learning models, many fundamental
questions remain unresolved. One of the most important questions is how to trade off adversarial robustness
‚àóPart of this work was done while H. Z. was visiting Simons Institute for the Theory of Computing.
‚Ä†Part of this work was done while Y. Y. was an intern at Petuum Inc.
1
arXiv:1901.08573v3  [cs.LG]  24 Jun 2019

Figure 1: Left Ô¨Ågure: decision boundary learned by natural training method. Right Ô¨Ågure: decision boundary
learned by our adversarial training method, where the orange dotted line represents the decision boundary in
the left Ô¨Ågure. It shows that both methods achieve zero natural training error, while our adversarial training
method achieves better robust training error than the natural training method.
against natural accuracy. Statistically, robustness can be be at odds with accuracy [TSE+19]. This has led to an
empirical line of work on adversarial defense that incorporates various kinds of assumptions [SZC+18, KGB17].
On the theoretical front, methods such as relaxation based defenses [KW18, RSL18a] provide provable
guarantees for adversarial robustness. They, however, ignore the performance of classiÔ¨Åer on the non-adversarial
examples, and thus leave open the theoretical treatment of the putative robustness/accuracy trade-off.
The problem of adversarial defense becomes more challenging when computational issues are considered.
For example, the straightforward empirical risk minimization (ERM) formulation of robust classiÔ¨Åcation
involves minimizing the robust 0-1 loss maxx‚Ä≤:‚à•x‚Ä≤‚àíx‚à•‚â§œµ 1{c(x‚Ä≤) Ã∏= y}, a loss which is NP-hard to optimize
even if œµ = 0 in general. Hence, it is natural to expect that some prior work on adversarial defense replaced
the 0-1 loss 1(¬∑) with a surrogate loss [MMS+18, KGB17, UOKvdO18]. However, there is little theoretical
guarantee on the tightness of this approximation.
1.1
Our methodology and results
We begin with an example that illustrates the trade-off between accuracy and adversarial robustness in Sec-
tion 2.4. This phenomenon was Ô¨Årst theoretically demonstrated by [TSE+19]. We construct another toy
example where the Bayes optimal classiÔ¨Åer achieves natural error 0% and robust error 100%, while the trivial
all-one classiÔ¨Åer achieves both natural error and robust error 50% (Table 1). While a large literature on the
analysis of robust error in terms of generalization [SST+18, CBM18, YRB18] and computational complex-
ity [BPR18, BLPR18], in this work we focus on how to address the trade-off between the natural error and the
robust error.
We show that the robust error can in general be bounded tightly using two terms: one corresponds to the
natural error measured by a surrogate loss function, and the other corresponds to how likely the input features
are close to the œµ-extension of the decision boundary, termed as the boundary error. We then minimize the
differentiable upper bound. Our theoretical analysis naturally leads to a new formulation of adversarial defense
which has several appealing properties; in particular, it inherits the beneÔ¨Åts of scalability to large datasets
exhibited by Tiny ImageNet, and the algorithm achieves state-of-the-art performance on a range of benchmarks
while providing theoretical guarantees. For example, while the defenses overviewed in [ACW18] achieve
robust accuracy no higher than ~47% under white-box attacks, our method achieves robust accuracy as high as
~57% in the same setting. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial
Vision Challenge where we won Ô¨Årst place out of ~2,000 submissions, surpassing the runner-up approach by
2

11.41% in terms of mean ‚Ñì2 perturbation distance.
1.2
Summary of contributions
Our work tackles the problem of trading accuracy off against robustness and advances the state-of-the-art in
multiple ways.
‚Ä¢ Theoretically, we characterize the trade-off between accuracy and robustness for classiÔ¨Åcation problems
via decomposing the robust error as the sum of the natural error and the boundary error. We provide
differentiable upper bounds on both terms using the theory of classiÔ¨Åcation-calibrated loss, which
are shown to be the tightest upper bounds uniform over all probability distributions and measurable
predictors.
‚Ä¢ Algorithmically, inspired by our theoretical analysis, we propose a new formulation of adversarial
defense, TRADES, as optimizing a regularized surrogate loss. The loss consists of two terms: the term
of empirical risk minimization encourages the algorithm to maximize the natural accuracy, while the
regularization term encourages the algorithm to push the decision boundary away from the data, so as to
improve adversarial robustness (see Figure 1).
‚Ä¢ Experimentally, we show that our proposed algorithm outperforms state-of-the-art methods under both
black-box and white-box threat models. In particular, the methodology won the Ô¨Ånal round of the
NeurIPS 2018 Adversarial Vision Challenge.
2
Preliminaries
We illustrate our methodology using the framework of binary classiÔ¨Åcation, but it can be generalized to other
settings as well.
2.1
Notations
We will use bold capital letters such as X and Y to represent random vector, bold lower-case letters such as x
and y to represent realization of random vector, capital letters such as X and Y to represent random variable,
and lower-case letters such as x and y to represent realization of random variable. SpeciÔ¨Åcally, we denote
by x ‚ààX the sample instance, and by y ‚àà{‚àí1, +1} the label, where X ‚äÜRd indicates the instance space.
sign(x) represents the sign of scalar x with sign(0) = +1. Denote by f : X ‚ÜíR the score function which
maps an instance to a conÔ¨Ådence value associated with being positive. It can be parametrized, e.g., by deep
neural networks. The associated binary classiÔ¨Åer is sign(f(¬∑)). We will frequently use 1{event}, the 0-1 loss,
to represent an indicator function that is 1 if an event happens and 0 otherwise. For norms, we denote by ‚à•x‚à•a
generic norm. Examples of norms include ‚à•x‚à•‚àû, the inÔ¨Ånity norm of vector x, and ‚à•x‚à•2, the ‚Ñì2 norm of vector
x. We use B(x, œµ) to represent a neighborhood of x: {x‚Ä≤ ‚ààX : ‚à•x‚Ä≤ ‚àíx‚à•‚â§œµ}. For a given score function f,
we denote by DB(f) the decision boundary of f; that is, the set {x ‚ààX : f(x) = 0}. The set B(DB(f), œµ)
denotes the neighborhood of the decision boundary of f: {x ‚ààX : ‚àÉx‚Ä≤ ‚ààB(x, œµ) s.t. f(x)f(x‚Ä≤) ‚â§0}. For
a given function œà(u), we denote by œà‚àó(v) := supu{uT v ‚àíœà(u)} the conjugate function of œà, by œà‚àó‚àóthe
bi-conjugate, and by œà‚àí1 the inverse function. We will frequently use œÜ(¬∑) to indicate the surrogate of 0-1 loss.
2.2
Robust (classiÔ¨Åcation) error
In the setting of adversarial learning, we are given a set of instances x1, ..., xn ‚ààX and labels y1, ..., yn ‚àà
{‚àí1, +1}. We assume that the data are sampled from an unknown distribution (X, Y ) ‚àºD. To characterize
3

 
!(#) 
0 
1 
% 
% 
1/2 
# 
1 
Figure 2: Counterexample given by Eqn. (2).
the robustness of a score function f : X ‚ÜíR, [SST+18, CBM18, BPR18] deÔ¨Åned robust (classiÔ¨Åcation) error
under the threat model of bounded œµ perturbation: Rrob(f) := E(X,Y )‚àºD1{‚àÉX‚Ä≤‚ààB(X, œµ) s.t. f(X‚Ä≤)Y ‚â§0}.
This is in sharp contrast to the standard measure of classiÔ¨Åer performance‚Äîthe natural (classiÔ¨Åcation) error
Rnat(f) := E(X,Y )‚àºD1{f(X)Y ‚â§0}. We note that the two errors satisfy Rrob(f) ‚â•Rnat(f) for all f; the
robust error is equal to the natural error when œµ = 0.
2.3
Boundary error
We introduce the boundary error deÔ¨Åned as Rbdy(f) := E(X,Y )‚àºD1{X ‚ààB(DB(f), œµ), f(X)Y > 0}. We
have the following decomposition of Rrob(f):
Rrob(f) = Rnat(f) + Rbdy(f).
(1)
2.4
Trade-off between natural and robust errors
Our study is motivated by the trade-off between natural and robust errors. [TSE+19] theoretically showed that
training models to be robust may lead to a reduction of standard accuracy by constructing a toy example. To
illustrate the phenomenon, we provide another toy example here.
Example. Consider the case (X, Y ) ‚àºD, where the marginal distribution over the instance space is a uniform
distribution over [0, 1], and for k = 0, 1, ..., ‚åà1
2œµ ‚àí1‚åâ,
Œ∑(x) := Pr(Y = 1|X = x)
=
(
0,
x ‚àà[2kœµ, (2k + 1)œµ),
1,
x ‚àà((2k + 1)œµ, (2k + 2)œµ].
(2)
See Figure 2 for the visualization of Œ∑(x). We consider two classiÔ¨Åers: a) the Bayes optimal classiÔ¨Åer
sign(2Œ∑(x) ‚àí1); b) the all-one classiÔ¨Åer which always outputs ‚Äúpositive.‚Äù Table 1 displays the trade-off
between natural and robust errors: the minimal natural error is achieved by the Bayes optimal classiÔ¨Åer with
large robust error, while the optimal robust error is achieved by the all-one classiÔ¨Åer with large natural error.
Our goal. In practice, one may prefer to trade-off between robustness and accuracy by introducing weights
in (1) to bias more towards the natural error or the boundary error. Noting that both the natural error and the
boundary error involve 0-1 loss functions, our goal is to devise tight differentiable upper bounds on both of
these terms. Towards this goal, we utilize the theory of classiÔ¨Åcation-calibrated loss.
4

Table 1: Comparisons of natural and robust errors of Bayes optimal classiÔ¨Åer and all-one classiÔ¨Åer in example
(2). The Bayes optimal classiÔ¨Åer has the optimal natural error while the all-one classiÔ¨Åer has the optimal robust
error.
Bayes Optimal ClassiÔ¨Åer
All-One ClassiÔ¨Åer
Rnat
0 (optimal)
1/2
Rbdy
1
0
Rrob
1
1/2 (optimal)
Table 2: Examples of classiÔ¨Åcation-calibrated loss œÜ and associated œà-transform. Here œàlog(Œ∏) = 1
2(1 ‚àí
Œ∏) log2(1 ‚àíŒ∏) + 1
2(1 + Œ∏) log2(1 + Œ∏).
Loss
œÜ(Œ±)
œà(Œ∏)
Hinge
max{1 ‚àíŒ±, 0}
Œ∏
Sigmoid
1 ‚àítanh(Œ±)
Œ∏
Exponential
exp(‚àíŒ±)
1 ‚àí
‚àö
1 ‚àíŒ∏2
Logistic
log2(1 + exp(‚àíŒ±))
œàlog(Œ∏)
2.5
ClassiÔ¨Åcation-calibrated surrogate loss
DeÔ¨Ånition. Minimization of the 0-1 loss in the natural and robust errors is computationally intractable
and the demands of computational efÔ¨Åciency have led researchers to focus on minimization of a tractable
surrogate loss, RœÜ(f) := E(X,Y )‚àºDœÜ(f(X)Y ). We then need to Ô¨Ånd quantitative relationships between the
excess errors associated with œÜ and those associated with 0‚Äì1 loss. We make a weak assumption on œÜ: it is
classiÔ¨Åcation-calibrated [BJM06]. Formally, for Œ∑ ‚àà[0, 1], deÔ¨Åne the conditional œÜ-risk by
H(Œ∑) := inf
Œ±‚ààR CŒ∑(Œ±) := inf
Œ±‚ààR (Œ∑œÜ(Œ±) + (1 ‚àíŒ∑)œÜ(‚àíŒ±)) ,
and deÔ¨Åne H‚àí(Œ∑) := infŒ±(2Œ∑‚àí1)‚â§0 CŒ∑(Œ±). The classiÔ¨Åcation-calibrated condition requires that imposing the
constraint that Œ± has an inconsistent sign with the Bayes decision rule sign(2Œ∑ ‚àí1) leads to a strictly larger
œÜ-risk:
Assumption 1 (ClassiÔ¨Åcation-Calibrated Loss). We assume that the surrogate loss œÜ is classiÔ¨Åcation-calibrated,
meaning that for any Œ∑ Ã∏= 1/2, H‚àí(Œ∑) > H(Œ∑).
We argue that Assumption 1 is indispensable for classiÔ¨Åcation problems, since without it the Bayes optimal
classiÔ¨Åer cannot be the minimizer of the œÜ-risk. Examples of classiÔ¨Åcation-calibrated loss include hinge loss,
sigmoid loss, exponential loss, logistic loss, and many others (see Table 2).
Properties. ClassiÔ¨Åcation-calibrated loss has many structural properties that one can exploit. We begin by
introducing a functional transform of classiÔ¨Åcation-calibrated loss œÜ which was proposed by [BJM06]. DeÔ¨Åne
the function œà : [0, 1] ‚Üí[0, ‚àû) by œà = eœà‚àó‚àó, where eœà(Œ∏) := H‚àí  1+Œ∏
2

‚àíH
  1+Œ∏
2

. Indeed, the function œà(Œ∏)
is the largest convex lower bound on H‚àí  1+Œ∏
2

‚àíH
  1+Œ∏
2

. The value H‚àí  1+Œ∏
2

‚àíH
  1+Œ∏
2

characterizes
how close the surrogate loss œÜ is to the class of non-classiÔ¨Åcation-calibrated losses.
Below we state useful properties of the œà-transform. We will frequently use the function œà to bound
Rrob(f) ‚àíR‚àó
nat.
Lemma 2.1 ([BJM06]). Under Assumption 1, the function œà has the following properties: œà is non-decreasing,
continuous, convex on [0, 1] and œà(0) = 0.
5

3
Relating 0-1 loss to Surrogate Loss
In this section, we present our main theoretical contributions for binary classiÔ¨Åcation and compare our results
with prior literature. Binary classiÔ¨Åcation problems have received signiÔ¨Åcant attention in recent years as many
competitions evaluate the performance of robust models on binary classiÔ¨Åcation problems [BCZ+18]. We defer
the discussion of multi-class problems to Section 4.
3.1
Upper bound
Our analysis leads to a guarantee on the performance of surrogate loss minimization. Intuitively, by Eqn. (1),
Rrob(f) ‚àíR‚àó
nat = Rnat(f) ‚àíR‚àó
nat + Rbdy(f) ‚â§œà‚àí1(RœÜ(f) ‚àíR‚àó
œÜ) + Rbdy(f), where the last inequality
holds because we choose œÜ as a classiÔ¨Åcation-calibrated loss [BJM06]. This leads to the following result.
Theorem 3.1. Let RœÜ(f) := EœÜ(f(X)Y ) and R‚àó
œÜ := minf RœÜ(f). Under Assumption 1, for any non-
negative loss function œÜ such that œÜ(0) ‚â•1, any measurable f : X ‚ÜíR, any probability distribution on
X √ó {¬±1}, and any Œª > 0, we have1
Rrob(f) ‚àíR‚àó
nat ‚â§œà‚àí1(RœÜ(f)‚àíR‚àó
œÜ)+Pr[X‚ààB(DB(f), œµ), f(X)Y > 0]
‚â§œà‚àí1(RœÜ(f)‚àíR‚àó
œÜ) + E
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª).
Quantity governing model robustness. Our result provides a formal justiÔ¨Åcation for the existence of adver-
sarial examples: learning models are vulnerable to small adversarial attacks because the probability that data
lie around the decision boundary of the model, Pr[X ‚ààB(DB(f), œµ), f(X)Y > 0], is large. As a result, small
perturbations may move the data point to the wrong side of the decision boundary, leading to weak robustness
of classiÔ¨Åcation models.
3.2
Lower bound
We now establish a lower bound on Rrob(f) ‚àíR‚àó
nat. Our lower bound matches our analysis of the upper bound
in Section 3.1 up to an arbitrarily small constant.
Theorem 3.2. Suppose that |X| ‚â•2. Under Assumption 1, for any non-negative loss function œÜ such that
œÜ(x) ‚Üí0 as x ‚Üí+‚àû, any Œæ > 0, and any Œ∏ ‚àà[0, 1], there exists a probability distribution on X √ó {¬±1}, a
function f : Rd ‚ÜíR, and a regularization parameter Œª > 0 such that Rrob(f) ‚àíR‚àó
nat = Œ∏ and
œà

Œ∏ ‚àíE
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª)

‚â§RœÜ(f) ‚àíR‚àó
œÜ ‚â§œà

Œ∏ ‚àíE
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª)

+ Œæ.
Theorem 3.2 demonstrates that in the presence of extra conditions on the loss function, i.e., limx‚Üí+‚àûœÜ(x) =
0, the upper bound in Section 3.1 is tight. The condition holds for all the losses in Table 2.
4
Algorithmic Design for Defenses
Optimization. Theorems 3.1 and 3.2 shed light on algorithmic designs of adversarial defenses. In order to
minimize Rrob(f) ‚àíR‚àó
nat, the theorems suggest minimizing2
min
f
E
n
œÜ(f(X)Y )
|
{z
}
for accuracy
+
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X)f(X‚Ä≤)/Œª)
|
{z
}
regularization for robustness
o
.
(3)
1We study the population form of the risk functions, and mention that by incorporating the generalization theory for classiÔ¨Åcation-
calibrated losses [BJM06] one can extend the analysis to Ô¨Ånite samples. We leave this analysis for future research.
2For simplicity of implementation, we do not use the function œà‚àí1 and rely on Œª to approximately reÔ¨Çect the effect of œà‚àí1, the
trade-off between the natural error and the boundary error, and the tight approximation of the boundary error using the corresponding
surrogate loss function.
6

We name our method TRADES (TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization).
Intuition behind the optimization. Problem (3) captures the trade-off between the natural and robust errors:
the Ô¨Årst term in (3) encourages the natural error to be optimized by minimizing the ‚Äúdifference‚Äù between f(X)
and Y , while the second regularization term encourages the output to be smooth, that is, it pushes the decision
boundary of classiÔ¨Åer away from the sample instances via minimizing the ‚Äúdifference‚Äù between the prediction
of natural example f(X) and that of adversarial example f(X‚Ä≤). This is conceptually consistent with the
argument that smoothness is an indispensable property of robust models [CBG+17]. The tuning parameter
Œª plays a critical role on balancing the importance of natural and robust errors. To see how the Œª affects the
solution in the example of Section 2.4, problem (3) tends to the Bayes optimal classiÔ¨Åer when Œª ‚Üí+‚àû, and
tends to the all-one classiÔ¨Åer when Œª ‚Üí0.
Comparisons with prior work. We compare our approach with several related lines of research in the prior
literature. One of the best known algorithms for adversarial defense is based on robust optimization [MMS+18,
KW18, WSMK18, RSL18a, RSL18b]. Most results in this direction involve algorithms that approximately
minimize
min
f
E

max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)Y )

,
(4)
where the objective function in problem (4) serves as an upper bound of the robust error Rrob(f). In complex
problem domains, however, this objective function might not be tight as an upper bound of the robust error, and
may not capture the trade-off between natural and robust errors.
A related line of research is adversarial training by regularization [MMIK18, KGB17, RDV17, ZSLG16].
There are several key differences between the results in this paper and those of [KGB17, RDV17, ZSLG16].
Firstly, the optimization formulations are different. In the previous works, the regularization term either mea-
sures the ‚Äúdifference‚Äù between f(X‚Ä≤) and Y [KGB17], or its gradient [RDV17]. In contrast, our regularization
term measures the ‚Äúdifference‚Äù between f(X) and f(X‚Ä≤). While [ZSLG16] generated the adversarial example
X‚Ä≤ by adding random Gaussian noise to X, our method simulates the adversarial example by solving the inner
maximization problem in Eqn. (3). Secondly, we note that the losses in [MMIK18, KGB17, RDV17, ZSLG16]
lack of theoretical guarantees. Our loss, with the presence of the second term in problem (3), makes our
theoretical analysis signiÔ¨Åcantly more subtle. Moreover, our algorithm takes the same computational resources
as [KGB17], which makes our method scalable to large-scale datasets. We defer the experimental comparisons
of various regularization based methods to Table 5.
Differences with Adversarial Logit Pairing. We also compare TRADES with Adversarial Logit Pairing
(ALP) [KKG18, EIA18]. The algorithm of ALP works as follows: given a Ô¨Åxed network f in each round, the
algorithm Ô¨Årstly generates an adversarial example X‚Ä≤ by solving argmaxX‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)Y ); ALP then
updates the network parameter by solving a minimization problem
min
f
E

Œ±œÜ(f(X‚Ä≤)Y ) + (1 ‚àíŒ±)œÜ(f(X)Y ) + ‚à•f(X) ‚àíf(X‚Ä≤)‚à•2/Œª
	
,
where 0 ‚â§Œ± ‚â§1 is a regularization parameter; the algorithm Ô¨Ånally repeats the above-mentioned pro-
cedure until it converges. We note that there are fundamental differences between TRADES and ALP.
While ALP simulates adversarial example X‚Ä≤ by the FGSMk attack, TRADES simulates X‚Ä≤ by solving
argmaxX‚Ä≤‚ààB(X,œµ) œÜ(f(X)f(X‚Ä≤)/Œª). Moreover, while ALP uses the ‚Ñì2 loss between f(X) and f(X‚Ä≤) to
regularize the training procedure without theoretical guarantees, TRADES uses the classiÔ¨Åcation-calibrated
loss according to Theorems 3.1 and 3.2.
Heuristic algorithm. In response to the optimization formulation (3), we use two heuristics to achieve more
general defenses: a) extending to multi-class problems by involving multi-class calibrated loss; b) approximately
solving the minimax problem via alternating gradient descent. For multi-class problems, a surrogate loss
7

Algorithm 1 Adversarial training by TRADES
1: Input: Step sizes Œ∑1 and Œ∑2, batch size m, number of iterations K in inner optimization, network
architecture parametrized by Œ∏
2: Output: Robust network fŒ∏
3: Randomly initialize network fŒ∏, or initialize network with pre-trained conÔ¨Åguration
4: repeat
5:
Read mini-batch B = {x1, ..., xm} from training set
6:
for i = 1, ..., m (in parallel) do
7:
x‚Ä≤
i ‚Üêxi + 0.001 ¬∑ N(0, I), where N(0, I) is the Gaussian distribution with zero mean and identity
variance
8:
for k = 1, ..., K do
9:
x‚Ä≤
i ‚ÜêŒ†B(xi,œµ)(Œ∑1sign(‚àáx‚Ä≤
iL(fŒ∏(xi), fŒ∏(x‚Ä≤
i))) + x‚Ä≤
i), where Œ† is the projection operator
10:
end for
11:
end for
12:
Œ∏ ‚ÜêŒ∏ ‚àíŒ∑2
Pm
i=1 ‚àáŒ∏[L(fŒ∏(xi), yi) + L(fŒ∏(xi), fŒ∏(x‚Ä≤
i))/Œª]/m
13: until training converged
is calibrated if minimizers of the surrogate risk are also minimizers of the 0-1 risk [PS16]. Examples of
multi-class calibrated loss include cross-entropy loss. Algorithmically, we extend problem (3) to the case of
multi-class classiÔ¨Åcations by replacing œÜ with a multi-class calibrated loss L(¬∑, ¬∑):
min
f
E

L(f(X), Y ) + max
X‚Ä≤‚ààB(X,œµ) L(f(X), f(X‚Ä≤))/Œª

,
(5)
where f(X) is the output vector of learning model (with softmax operator in the top layer for the cross-entropy
loss L(¬∑, ¬∑)), Y is the label-indicator vector, and Œª > 0 is the regularization parameter. One can also exchange
f(X) and f(X‚Ä≤) in the second term of (5). The pseudocode of adversarial training procedure, which aims at
minimizing the empirical form of problem (5), is displayed in Algorithm 1.
The key ingredient of the algorithm is to approximately solve the linearization of inner maximization in
problem (5) by the projected gradient descent (see Step 7). We note that xi is a global minimizer with zero
gradient to the objective function g(x‚Ä≤) := L(f(xi), f(x‚Ä≤)) in the inner problem. Therefore, we initialize
x‚Ä≤
i by adding a small, random perturbation around xi in Step 5 to start the inner optimizer. More exhaustive
approximations of the inner maximization problem in terms of either optimization formulations or solvers
would lead to better defense performance.
Semi-supervised learning. We note that TRADES problem (5) can be straightforwardly applied to the semi-
supervised learning framework, as the second term in problem (5) does not depend on the label Y . Therefore,
with more unlabeled data points, one can approximate the second term (in the expectation form) better by the
empirical loss minimization. There are many interesting recent works which explore the beneÔ¨Åts of invloving
unlabeled data [CRS+19, SFK+19, ZCH+19].
Acceleration. Adversarial training is typically more than 10x slower than natural training. To resolve this issue
for TRADES, [SNG+19, ZZL+19] proposed new algorithms to solve problem (5) at negligible additional cost
compared to natural training.
5
Experimental Results
In this section, we verify the effectiveness of TRADES by numerical experiments. We denote by Arob(f) =
1 ‚àíRrob(f) the robust accuracy, and by Anat(f) = 1 ‚àíRnat(f) the natural accuracy on test dataset. We
8

Table 3: Theoretical veriÔ¨Åcation on the optimality of Theorem 3.1.
Œª
Arob(f) (%)
RœÜ(f)
‚àÜ= ‚àÜRHS ‚àí‚àÜLHS
2.0
99.43
0.0006728
0.006708
3.0
99.41
0.0004067
0.005914
4.0
99.37
0.0003746
0.006757
5.0
99.34
0.0003430
0.005860
release our code and trained models at https://github.com/yaodongyu/TRADES.
5.1
Optimality of Theorem 3.1
We verify the tightness of the established upper bound in Theorem 3.1 for binary classiÔ¨Åcation problem on
MNIST dataset. The negative examples are ‚Äò1‚Äô and the positive examples are ‚Äò3‚Äô. Here we use a Convolutional
Neural Network (CNN) with two convolutional layers, followed by two fully-connected layers. The output size
of the last layer is 1. To learn the robust classiÔ¨Åer, we minimize the regularized surrogate loss in Eqn. (3), and
use the hinge loss in Table 2 as the surrogate loss œÜ, where the associated œà-transform is œà(Œ∏) = Œ∏.
To verify the tightness of our upper bound, we calculate the left hand side in Theorem 3.1, i.e.,
‚àÜLHS = Rrob(f) ‚àíR‚àó
nat,
and the right hand side, i.e.,
‚àÜRHS = (RœÜ(f) ‚àíR‚àó
œÜ) + E
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª).
As we cannot have access to the unknown distribution D, we approximate the above expectation terms by test
dataset. We Ô¨Årst use natural training method to train a classiÔ¨Åer so as to approximately estimate R‚àó
nat and
R‚àó
œÜ, where we Ô¨Ånd that the naturally trained classiÔ¨Åer can achieve natural error R‚àó
nat = 0%, and loss value
R‚àó
œÜ = 0.0 for the binary classiÔ¨Åcation problem. Next, we optimize problem (3) to train a robust classiÔ¨Åer f.
We take perturbation œµ = 0.1, number of iterations K = 20 and run 30 epochs on the training dataset. Finally,
to approximate the second term in ‚àÜRHS, we use FGSMk (white-box) attack (a.k.a. PGD attack) [KGB17] with
20 iterations to approximately calculate the worst-case perturbed data X‚Ä≤.
The results in Table 3 show the tightness of our upper bound in Theorem 3.1. It shows that the differences
between ‚àÜRHS and ‚àÜLHS under various Œª‚Äôs are very small.
5.2
Sensitivity of regularization hyperparameter Œª
The regularization parameter Œª is an important hyperparameter in our proposed method. We show how the
regularization parameter affects the performance of our robust classiÔ¨Åers by numerical experiments on two
datasets, MNIST and CIFAR10. For both datasets, we minimize the loss in Eqn. (5) to learn robust classiÔ¨Åers
for multi-class problems, where we choose L as the cross-entropy loss.
MNIST setup. We use the CNN which has two convolutional layers, followed by two fully-connected layers.
The output size of the last layer is 10. We set perturbation œµ = 0.1, perturbation step size Œ∑1 = 0.01, number of
iterations K = 20, learning rate Œ∑2 = 0.01, batch size m = 128, and run 50 epochs on the training dataset.
To evaluate the robust error, we apply FGSMk (white-box) attack with 40 iterations and 0.005 step size. The
results are in Table 4.
CIFAR10 setup. We apply ResNet-18 [HZRS16] for classiÔ¨Åcation. The output size of the last layer is 10. We
set perturbation œµ = 0.031, perturbation step size Œ∑1 = 0.007, number of iterations K = 10, learning rate
Œ∑2 = 0.1, batch size m = 128, and run 100 epochs on the training dataset. To evaluate the robust error, we
apply FGSMk (white-box) attack with 20 iterations and the step size is 0.003. The results are in Table 4.
9

Table 4: Sensitivity of regularization hyperparameter Œª on MNIST and CIFAR10 datasets.
MNIST
CIFAR10
1/Œª
Arob(f) (%)
Anat(f) (%)
Arob(f) (%)
Anat(f) (%)
0.1
91.09 ¬± 0.0385
99.41 ¬± 0.0235
26.53 ¬± 1.1698
91.31 ¬± 0.0579
0.2
92.18 ¬± 0.0450
99.38 ¬± 0.0094
37.71 ¬± 0.6743
89.56 ¬± 0.2154
0.4
93.21 ¬± 0.0660
99.35 ¬± 0.0082
41.50 ¬± 0.3376
87.91 ¬± 0.2944
0.6
93.87 ¬± 0.0464
99.33 ¬± 0.0141
43.37 ¬± 0.2706
87.50 ¬± 0.1621
0.8
94.32 ¬± 0.0492
99.31 ¬± 0.0205
44.17 ¬± 0.2834
87.11 ¬± 0.2123
1.0
94.75 ¬± 0.0712
99.28 ¬± 0.0125
44.68 ¬± 0.3088
87.01 ¬± 0.2819
2.0
95.45 ¬± 0.0883
99.29 ¬± 0.0262
48.22 ¬± 0.0740
85.22 ¬± 0.0543
3.0
95.57 ¬± 0.0262
99.24 ¬± 0.0216
49.67 ¬± 0.3179
83.82 ¬± 0.4050
4.0
95.65 ¬± 0.0340
99.16 ¬± 0.0205
50.25 ¬± 0.1883
82.90 ¬± 0.2217
5.0
95.65 ¬± 0.1851
99.16 ¬± 0.0403
50.64 ¬± 0.3336
81.72 ¬± 0.0286
We observe that as the regularization parameter 1/Œª increases, the natural accuracy Anat(f) decreases
while the robust accuracy Arob(f) increases, which veriÔ¨Åes our theory on the trade-off between robustness and
accuracy. Note that for MNIST dataset, the natural accuracy does not decrease too much as the regularization
term 1/Œª increases, which is different from the results of CIFAR10. This is probably because the classiÔ¨Åcation
task for MNIST is easier. Meanwhile, our proposed method is not very sensitive to the choice of Œª. Empirically,
when we set the hyperparameter 1/Œª in [1, 10], our method is able to learn classiÔ¨Åers with both high robustness
and high accuracy. We will set 1/Œª as either 1 or 6 in the following experiments.
5.3
Adversarial defenses under various attacks
Previously, [ACW18] showed that 7 defenses in ICLR 2018 which relied on obfuscated gradients may easily
break down. In this section, we verify the effectiveness of our method with the same experimental setup under
both white-box and black-box threat models.
MNIST setup. We use the CNN architecture in [CW17] with four convolutional layers, followed by three
fully-connected layers. We set perturbation œµ = 0.3, perturbation step size Œ∑1 = 0.01, number of iterations
K = 40, learning rate Œ∑2 = 0.01, batch size m = 128, and run 100 epochs on the training dataset.
CIFAR10 setup. We use the same neural network architecture as [MMS+18], i.e., the wide residual network
WRN-34-10 [ZK16]. We set perturbation œµ = 0.031, perturbation step size Œ∑1 = 0.007, number of iterations
K = 10, learning rate Œ∑2 = 0.1, batch size m = 128, and run 100 epochs on the training dataset.
5.3.1
White-box attacks
We summarize our results in Table 5 together with the results from [ACW18]. We also implement methods
in [ZSLG16, KGB17, RDV17] on the CIFAR10 dataset as they are also regularization based methods. For
MNIST dataset, we apply FGSMk (white-box) attack with 40 iterations and the step size is 0.01. For CIFAR10
dataset, we apply FGSMk (white-box) attack with 20 iterations and the step size is 0.003, under which the
defense model in [MMS+18] achieves 47.04% robust accuracy. Table 5 shows that our proposed defense
method can signiÔ¨Åcantly improve the robust accuracy of models, which is able to achieve robust accuracy
as high as 56.61%. We also evaluate our robust model on MNIST dataset under the same threat model as
in [SKC18] (C&W white-box attack [CW17]), and the robust accuracy is 99.46%. See appendix for detailed
information of models in Table 5.
10

Table 5: Comparisons of TRADES with prior defense models under white-box attacks.
Defense
Defense type
Under which attack
Dataset
Distance
Anat(f)
Arob(f)
[BRRG18]
gradient mask
[ACW18]
CIFAR10
0.031 (‚Ñì‚àû)
-
0%
[MLW+18]
gradient mask
[ACW18]
CIFAR10
0.031 (‚Ñì‚àû)
-
5%
[DAL+18]
gradient mask
[ACW18]
CIFAR10
0.031 (‚Ñì‚àû)
-
0%
[SKN+18]
gradient mask
[ACW18]
CIFAR10
0.031 (‚Ñì‚àû)
-
9%
[NKM17]
gradient mask
[ACW18]
CIFAR10
0.015 (‚Ñì‚àû)
-
15%
[WSMK18]
robust opt.
FGSM20 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
27.07%
23.54%
[MMS+18]
robust opt.
FGSM20 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
87.30%
47.04%
[ZSLG16]
regularization
FGSM20 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
94.64%
0.15%
[KGB17]
regularization
FGSM20 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
85.25%
45.89%
[RDV17]
regularization
FGSM20 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
95.34%
0%
TRADES (1/Œª = 1)
regularization
FGSM1,000 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
48.90%
TRADES (1/Œª = 6)
regularization
FGSM1,000 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
56.43%
TRADES (1/Œª = 1)
regularization
FGSM20 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
49.14%
TRADES (1/Œª = 6)
regularization
FGSM20 (PGD)
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
56.61%
TRADES (1/Œª = 1)
regularization
DeepFool (‚Ñì‚àû)
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
59.10%
TRADES (1/Œª = 6)
regularization
DeepFool (‚Ñì‚àû)
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
61.38%
TRADES (1/Œª = 1)
regularization
LBFGSAttack
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
84.41%
TRADES (1/Œª = 6)
regularization
LBFGSAttack
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
81.58%
TRADES (1/Œª = 1)
regularization
MI-FGSM
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
51.26%
TRADES (1/Œª = 6)
regularization
MI-FGSM
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
57.95%
TRADES (1/Œª = 1)
regularization
C&W
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
84.03%
TRADES (1/Œª = 6)
regularization
C&W
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
81.24%
[SKC18]
gradient mask
[ACW18]
MNIST
0.005 (‚Ñì2)
-
55%
[MMS+18]
robust opt.
FGSM40 (PGD)
MNIST
0.3 (‚Ñì‚àû)
99.36%
96.01%
TRADES (1/Œª = 6)
regularization
FGSM1,000 (PGD)
MNIST
0.3 (‚Ñì‚àû)
99.48%
95.60%
TRADES (1/Œª = 6)
regularization
FGSM40 (PGD)
MNIST
0.3 (‚Ñì‚àû)
99.48%
96.07%
TRADES (1/Œª = 6)
regularization
C&W
MNIST
0.005 (‚Ñì2)
99.48%
99.46%
11

Table 6: Comparisons of TRADES with prior defenses under black-box FGSM40 attack on the MNIST dataset.
The models inside parentheses are source models which provide gradients to adversarial attackers. We provide
the average cross-entropy loss value L(f(X), Y ) of each defense model in the bracket. The defense model
‚ÄòMadry‚Äô is the same model as in the antepenultimate line of Table 5. The defense model ‚ÄòTRADES‚Äô is the same
model as in the penultimate line of Table 5.
Defense Model
Robust Accuracy Arob(f)
Madry
97.43% [0.0078484]
(Natural)
TRADES
97.63% [0.0075324]
(Natural)
Madry
97.38% [0.0084962]
(Ours)
TRADES
97.66% [0.0073532]
(Madry)
Table 7: Comparisons of TRADES with prior defenses under black-box FGSM20 attack on the CIFAR10
dataset. The models inside parentheses are source models which provide gradients to adversarial attackers. We
provide the average cross-entropy loss value of each defense model in the bracket. The defense model ‚ÄòMadry‚Äô
is implemented based on [MMS+18], and the defense model ‚ÄòTRADES‚Äô is the same model as in the 11th line
of Table 5.
Defense Model
Robust Accuracy Arob(f)
Madry
84.39% [0.0519784]
(Natural)
TRADES
87.60% [0.0380258]
(Natural)
Madry
66.00% [0.1252672]
(Ours)
TRADES
70.14% [0.0885364]
(Madry)
5.3.2
Black-box attacks
We verify the robustness of our models under black-box attacks. We Ô¨Årst train models without using adversarial
training on the MNIST and CIFAR10 datasets. We use the same network architectures that are speciÔ¨Åed in the
beginning of this section, i.e., the CNN architecture in [CW17] and the WRN-34-10 architecture in [ZK16]. We
denote these models by naturally trained models (Natural). The accuracy of the naturally trained CNN model
is 99.50% on the MNIST dataset. The accuracy of the naturally trained WRN-34-10 model is 95.29% on the
CIFAR10 dataset. We also implement the method proposed in [MMS+18] on both datasets. We denote these
models by Madry‚Äôs models (Madry). The accuracy of [MMS+18]‚Äôs CNN model is 99.36% on the MNIST
dataset. The accuracy of [MMS+18]‚Äôs WRN-34-10 model is 85.49% on the CIFAR10 dataset.
For both datasets, we use FGSMk (black-box) method to attack various defense models. For MNIST
dataset, we set perturbation œµ = 0.3 and apply FGSMk (black-box) attack with 40 iterations and the step size is
0.01. For CIFAR10 dataset, we set œµ = 0.031 and apply FGSMk (black-box) attack with 20 iterations and the
step size is 0.003. Note that the setup is the same as the setup speciÔ¨Åed in Section 5.3.1. We summarize our
results in Table 6 and Table 7. In both tables, we use two source models (noted in the parentheses) to generate
adversarial perturbations: we compute the perturbation directions according to the gradients of the source
models on the input images. It shows that our models are more robust against black-box attacks transfered from
naturally trained models and [MMS+18]‚Äôs models. Moreover, our models can generate stronger adversarial
examples for black-box attacks compared with naturally trained models and [MMS+18]‚Äôs models.
5.4
Case study: NeurIPS 2018 Adversarial Vision Challenge
Competition settings. In the adversarial competition, the adversarial attacks and defenses are under the
black-box setting. The dataset in this competition is Tiny ImageNet, which consists of 550,000 data (with
12

2.256
2.025
1.637
1.585
1.476
1.401
0
0.5
1
1.5
2
2.5
1st (TRADES)
2nd
3rd
4th
5th
6th
Figure 3: Top-6 results (out of ~2,000 submissions) in the NeurIPS 2018 Adversarial Vision Challenge. The
vertical axis represents the mean ‚Ñì2 perturbation distance that makes robust models fail to output correct labels.
our data augmentation) and 200 classes. The robust models only return label predictions instead of explicit
gradients and conÔ¨Ådence scores. The task for robust models is to defend against adversarial examples that
are generated by the top-5 submissions in the un-targeted attack track. The score for each defense model is
evaluated by the smallest perturbation distance that makes the defense model fail to output correct labels.
Competition results. The methodology in this paper was applied to the competition, where our entry ranked
the 1st place. We implemented our method to train ResNet models. We report the mean ‚Ñì2 perturbation distance
of the top-6 entries in Figure 3. It shows that our method outperforms other approaches with a large margin. In
particular, we surpass the runner-up submission by 11.41% in terms of mean ‚Ñì2 perturbation distance.
6
Conclusions
In this paper, we study the problem of adversarial defenses against structural perturbations around input data.
We focus on the trade-off between robustness and accuracy, and show an upper bound on the gap between
robust error and optimal natural error. Our result advances the state-of-the-art work and matches the lower
bound in the worst-case scenario. The bounds motivate us to minimize a new form of regularized surrogate loss,
TRADES, for adversarial training. Experiments on real datasets and adversarial competition demonstrate the
effectiveness of our proposed algorithms. It would be interesting to combine our methods with other related line
of research on adversarial defenses, e.g., feature denoising technique [XWvdM+18] and network architecture
design [CBG+17], to achieve more robust learning systems.
Acknowledgements. We thank Maria-Florina Balcan, Avrim Blum, Zico Kolter, and Aleksander M Àõadry for
valuable comments and discussions.
References
[AAG19]
Rima Alaifari, Giovanni S Alberti, and Tandri Gauksson. ADef: an iterative algorithm to
construct adversarial deformations. In International Conference on Learning Representations,
2019.
[ACW18]
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense
of security: Circumventing defenses to adversarial examples. In International Conference on
Machine Learning, 2018.
13

[Bar01]
Franck Barthe. Extremal properties of central half-spaces for product measures. Journal of
Functional Analysis, 182(1):81‚Äì107, 2001.
[BCZ+18]
Tom B Brown, Nicholas Carlini, Chiyuan Zhang, Catherine Olsson, Paul Christiano, and Ian
Goodfellow. Unrestricted adversarial examples. arXiv preprint arXiv:1809.08352, 2018.
[BJM06]
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classiÔ¨Åcation, and risk
bounds. Journal of the American Statistical Association, 101(473):138‚Äì156, 2006.
[BLPR18]
S√©bastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from
cryptographic pseudo-random generators. arXiv preprint arXiv:1811.06418, 2018.
[BPR18]
S√©bastien Bubeck, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational
constraints. arXiv preprint arXiv:1805.10204, 2018.
[BRB18]
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks:
Reliable attacks against black-box machine learning models. In International Conference on
Learning Representations, 2018.
[BRRG18]
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One
hot way to resist adversarial examples. In International Conference on Learning Representa-
tions, 2018.
[CBG+17]
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier.
Parseval networks: Improving robustness to adversarial examples. In International Conference
on Machine Learning, 2017.
[CBM18]
Daniel Cullina, Arjun Nitin Bhagoji, and Prateek Mittal. PAC-learning in the presence of
adversaries. In Advances in Neural Information Processing Systems, pages 228‚Äì239, 2018.
[CRS+19]
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled
data improves adversarial robustness. arXiv preprint arXiv:1905.13736, 2019.
[CW17]
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In
IEEE Symposium on Security and Privacy, pages 39‚Äì57, 2017.
[DAL+18]
Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean KossaiÔ¨Å,
Aran Khanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial
defense. arXiv preprint arXiv:1803.01442, 2018.
[DLP+18]
Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li.
Boosting adversarial attacks with momentum. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 9185‚Äì9193, 2018.
[EIA18]
Logan Engstrom, Andrew Ilyas, and Anish Athalye. Evaluating and understanding the robust-
ness of adversarial logit pairing. arXiv preprint arXiv:1807.10272, 2018.
[ETT+17]
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation sufÔ¨Åce: Fooling CNNs with simple transformations. arXiv preprint
arXiv:1712.02779, 2017.
[FFF18]
Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classiÔ¨Åer.
In Advances in Neural Information Processing Systems, pages 1186‚Äì1195, 2018.
14

[GAG+18]
Justin Gilmer, Ryan P Adams, Ian Goodfellow, David Andersen, and George E Dahl. Motivating
the rules of the game for adversarial example research. arXiv preprint arXiv:1807.06732, 2018.
[GSS15]
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adver-
sarial examples. In International Conference on Learning Representations, 2015.
[HPG+17]
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial
attacks on neural network policies. arXiv preprint arXiv:1702.02284, 2017.
[HWC+17]
Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example
defenses: Ensembles of weak defenses are not strong. arXiv preprint arXiv:1706.04701, 2017.
[HXSS15]
Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesv√°ri. Learning with a strong
adversary. arXiv preprint arXiv:1511.03034, 2015.
[HZRS16]
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE conference on computer vision and pattern recognition, pages 770‚Äì778,
2016.
[JL17]
Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension
systems. In Empirical Methods in Natural Language Processing, 2017.
[KGB17]
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
International Conference on Learning Representations, 2017.
[KKG18]
Harini Kannan, Alexey Kurakin, and Ian Goodfellow. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373, 2018.
[KW18]
J Zico Kolter and Eric Wong. Provable defenses against adversarial examples via the convex
outer adversarial polytope. In International Conference on Machine Learning, 2018.
[MC17]
Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. In
ACM SIGSAC Conference on Computer and Communications Security, pages 135‚Äì147, 2017.
[MDFF16]
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple
and accurate method to fool deep neural networks. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 2574‚Äì2582, 2016.
[MLW+18]
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Michael E Houle,
Grant Schoenebeck, Dawn Song, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.
[MMIK18]
Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training:
a regularization method for supervised and semi-supervised learning. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 2018.
[MMS+18]
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
Learning Representations, 2018.
[NKM17]
Taesik Na, Jong Hwan Ko, and Saibal Mukhopadhyay. Cascade adversarial machine learning
regularized with a uniÔ¨Åed embedding. arXiv preprint arXiv:1708.02582, 2017.
15

[PS16]
Bernardo √Åvila Pires and Csaba Szepesv√°ri. Multiclass classiÔ¨Åcation calibration functions.
arXiv preprint arXiv:1609.06385, 2016.
[RBB17]
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox v0. 8.0: A python toolbox
to benchmark the robustness of machine learning models. arXiv preprint arXiv:1707.04131,
2017.
[RDV17]
Andrew Slavin Ross and Finale Doshi-Velez.
Improving the adversarial robustness and
interpretability of deep neural networks by regularizing their input gradients. arXiv preprint
arXiv:1711.09404, 2017.
[RSL18a]
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. CertiÔ¨Åed defenses against adversarial
examples. In International Conference on Learning Representations, 2018.
[RSL18b]
Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. SemideÔ¨Ånite relaxations for certifying
robustness to adversarial examples. In Advances in Neural Information Processing Systems,
pages 10899‚Äì10909, 2018.
[SFK+19]
Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et al. Are labels required for improving
adversarial robustness? arXiv preprint arXiv:1905.13725, 2019.
[SKC18]
Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classiÔ¨Åers
against adversarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018.
[SKN+18]
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend:
Leveraging generative models to understand and defend against adversarial examples. In
International Conference on Learning Representations, 2018.
[SND18]
Aman Sinha, Hongseok Namkoong, and John Duchi. CertiÔ¨Åable distributional robustness
with principled adversarial training. In International Conference on Learning Representations,
2018.
[SNG+19]
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer,
Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! arXiv preprint
arXiv:1904.12843, 2019.
[SST+18]
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander M Àõadry.
Adversarially robust generalization requires more data. In Advances in Neural Information
Processing Systems 31, pages 5019‚Äì5031, 2018.
[SYN15]
Uri Shaham, Yutaro Yamada, and Sahand Negahban.
Understanding adversarial train-
ing: Increasing local stability of neural nets through robust optimization. arXiv preprint
arXiv:1511.05432, 2015.
[SZC+18]
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robust-
ness the cost of accuracy? ‚Äî a comprehensive study on the robustness of 18 deep image
classiÔ¨Åcation models. In European Conference on Computer Vision, 2018.
[SZS+13]
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199,
2013.
16

[TKP+18]
Florian Tram√®r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick
McDaniel. Ensemble adversarial training: Attacks and defenses. In International Conference
on Learning Representations, 2018.
[TSE+19]
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander
Madry. Robustness may be at odds with accuracy. In International Conference on Learning
Representations, 2019.
[TV16]
Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial images. In International
Joint Conference on Neural Networks, pages 426‚Äì433, 2016.
[UOKvdO18]
Jonathan Uesato, Brendan O‚ÄôDonoghue, Pushmeet Kohli, and Aaron van den Oord. Adversarial
risk and the dangers of evaluating against weak attacks. In International Conference on Machine
Learning, pages 5025‚Äì5034, 2018.
[VNS+18]
Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio
Savarese. Generalizing to unseen domains via adversarial data augmentation. In Advances in
Neural Information Processing Systems, pages 5339‚Äì5349, 2018.
[WSMK18]
E Wong, F Schmidt, JH Metzen, and JZ Kolter. Scaling provable adversarial defenses. In
Advances in Neural Information Processing Systems, 2018.
[XWvdM+18] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature
denoising for improving adversarial robustness. arXiv preprint arXiv:1812.03411, 2018.
[XWZ+17]
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adver-
sarial examples for semantic segmentation and object detection. In International Conference
on Computer Vision, 2017.
[XZL+18]
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially
transformed adversarial examples. In International Conference on Learning Representations,
2018.
[YRB18]
Dong Yin, Kannan Ramchandran, and Peter Bartlett. Rademacher complexity for adversarially
robust generalization. arXiv preprint arXiv:1810.11914, 2018.
[ZCH+19]
Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John Hopcroft, and Liwei Wang. Adversar-
ially robust generalization just requires more unlabeled data. arXiv preprint arXiv:1906.00555,
2019.
[ZCS+19]
Huan Zhang, Hongge Chen, Zhao Song, Duane Boning, Inderjit S Dhillon, and Cho-Jui Hsieh.
The limitations of adversarial training and the blind-spot attack. In International Conference
on Learning Representations, 2019.
[Zha02]
Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal
of Machine Learning Research, 2:527‚Äì550, 2002.
[ZK16]
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
Conference, 2016.
[ZSLG16]
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness
of deep neural networks via stability training. In IEEE Conference on Computer Vision and
Pattern Recognition, pages 4480‚Äì4488, 2016.
17

[ZSS19]
Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural networks with multi-
branch architectures are intrinsically less non-convex. In International Conference on ArtiÔ¨Åcial
Intelligence and Statistics, pages 1099‚Äì1109, 2019.
[ZXJ+18]
Hongyang Zhang, Susu Xu, Jiantao Jiao, Pengtao Xie, Ruslan Salakhutdinov, and Eric P Xing.
Stackelberg GAN: Towards provable minimax equilibrium via multi-generator architectures.
arXiv preprint arXiv:1811.08010, 2018.
[ZZL+19]
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only
propagate once: Accelerating adversarial training via maximal principle. arXiv preprint
arXiv:1905.00877, 2019.
18

A
Other Related Works
Attack methods. Although deep neural networks have achieved great progress in various areas [ZSS19,
ZXJ+18], they are brittle to adversarial attacks. Adversarial attacks have been extensively studied in the recent
years. One of the baseline attacks to deep nerual networks is the Fast Gradient Sign Method (FGSM) [GSS15].
FGSM computes an adversarial example as
x‚Ä≤ := x + œµsign(‚àáxœÜ(f(x)y)),
where x is the input instance, y is the label, f : X ‚ÜíR is the score function (parametrized by deep nerual
network for example) which maps an instance to its conÔ¨Ådence value of being positive, and œÜ(¬∑) is a surrogate
of 0-1 loss. A more powerful yet natural extension of FGSM is the multi-step variant FGSMk (also known as
PGD attack) [KGB17]. FGSMk applies projected gradient descent by k times:
x‚Ä≤
t+1 := Œ†B(x,œµ)(x‚Ä≤
t + œµsign(‚àáxœÜ(f(x‚Ä≤
t)y))),
where x‚Ä≤
t is the t-th iteration of the algorithm with x‚Ä≤
0 := x and Œ†B(x,œµ) is the projection operator onto the
ball B(x, œµ). Both FGSM and FGSMk are approximately solving (the linear approximation of) maximization
problem:
max
x‚Ä≤‚ààB(x,œµ) œÜ(f(x‚Ä≤)y).
They can be adapted to the purpose of black-box attacks by running the algorithms on another similar network
which is white-box to the algorithms [TKP+18]. Though defenses that cause obfuscated gradients defeat
iterative optimization based attacks, [ACW18] showed that defenses relying on this effect can be circumvented.
Other attack methods include MI-FGSM [DLP+18] and LBFGS attacks [TV16].
Robust optimization based defenses. Compared with attack methods, adversarial defense methods are
relatively fewer. Robust optimization based defenses are inspired by the above-mentioned attacks. Intuitively,
the methods train a network by Ô¨Åtting its parameters to the adversarial examples:
min
f
E

max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)Y )

.
(6)
Following this framework, [HXSS15, SYN15] considered one-step adversaries, while [MMS+18] worked with
multi-step methods for the inner maximization problem. There are, however, two critical differences between
the robust optimization based defenses and the present paper. Firstly, robust optimization based defenses lack of
theoretical guarantees. Secondly, such methods do not consider the trade-off between accuracy and robustness.
Relaxation based defenses. We mention another related line of research in adversarial defenses‚Äîrelaxation
based defenses. Given that the inner maximization in problem (6) might be hard to solve due to the non-
convexity nature of deep neural networks, [KW18] and [RSL18a] considered a convex outer approximation of
the set of activations reachable through a norm-bounded perturbation for one-hidden-layer neural networks.
[WSMK18] later scaled the methods to larger models, and [RSL18b] proposed a tighter convex approximation.
[SND18, VNS+18] considered a Lagrangian penalty formulation of perturbing the underlying data distribution
in a Wasserstein ball. These approaches, however, do not apply when the activation function is ReLU.
Theoretical progress. Despite a large amount of empirical works on adversarial defenses, many fundamental
questions remain open in theory. There are a few preliminary explorations in recent years. [FFF18] derived
upper bounds on the robustness to perturbations of any classiÔ¨Åcation function, under the assumption that the
data is generated with a smooth generative model. From computational aspects, [BPR18, BLPR18] showed that
adversarial examples in machine learning are likely not due to information-theoretic limitations, but rather it
19

could be due to computational hardness. From statistical aspects, [SST+18] showed that the sample complexity
of robust training can be signiÔ¨Åcantly larger than that of standard training. This gap holds irrespective of the
training algorithm or the model family. [CBM18] and [YRB18] studied the uniform convergence of robust
error Rrob(f) by extending the classic VC and Rademacher arguments to the case of adversarial learning,
respectively. A recent work demonstrates the existence of trade-off between accuracy and robustness [TSE+19],
without providing a practical algorithm to address it.
B
Proofs of Main Results
In this section, we provide the proofs of our main results.
B.1
Proof of Theorem 3.1
Theorem 3.1 (restated). Let RœÜ(f) := EœÜ(f(X)Y ) and R‚àó
œÜ := minf RœÜ(f). Under Assumption 1, for any
non-negative loss function œÜ such that œÜ(0) ‚â•1, any measurable f : X ‚ÜíR, any probability distribution on
X √ó {¬±1}, and any Œª > 0, we have
Rrob(f) ‚àíR‚àó
nat ‚â§œà‚àí1(RœÜ(f) ‚àíR‚àó
œÜ) + Pr[X ‚ààB(DB(f), œµ), f(X)Y > 0]
‚â§œà‚àí1(RœÜ(f) ‚àíR‚àó
œÜ) + E
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª).
Proof. By Eqn. (1), Rrob(f) ‚àíR‚àó
nat = Rnat(f) ‚àíR‚àó
nat + Rbdy(f) ‚â§œà‚àí1(RœÜ(f) ‚àíR‚àó
œÜ) + Rbdy(f), where
the last inequality holds because we choose œÜ as a classiÔ¨Åcation-calibrated loss [BJM06]. This leads to the Ô¨Årst
inequality.
Also, notice that
Pr[X ‚ààB(DB(f), œµ), f(X)Y > 0] ‚â§Pr[X ‚ààB(DB(f), œµ)]
= E
max
X‚Ä≤‚ààB(X,œµ) 1{f(X‚Ä≤) Ã∏= f(X)}
= E
max
X‚Ä≤‚ààB(X,œµ) 1{f(X‚Ä≤)f(X)/Œª < 0}
‚â§E
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª).
This leads to the second inequality.
B.2
Proof of Theorem 3.2
Theorem 3.2 (restated). Suppose that |X| ‚â•2. Under Assumption 1, for any non-negative loss function œÜ
such that œÜ(x) ‚Üí0 as x ‚Üí+‚àû, any Œæ > 0, and any Œ∏ ‚àà[0, 1], there exists a probability distribution on
X √ó {¬±1}, a function f : Rd ‚ÜíR, and a regularization parameter Œª > 0 such that Rrob(f) ‚àíR‚àó
nat = Œ∏ and
œà

Œ∏ ‚àíE
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª)

‚â§RœÜ(f) ‚àíR‚àó
œÜ ‚â§œà

Œ∏ ‚àíE
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª)

+ Œæ.
20

Proof. The Ô¨Årst inequality follows from Theorem 3.1. Thus it sufÔ¨Åces to prove the second inequality.
Fix œµ > 0 and Œ∏ ‚àà[0, 1]. By the deÔ¨Ånition of œà and its continuity, we can choose Œ≥, Œ±1, Œ±2 ‚àà[0, 1]
such that Œ∏ = Œ≥Œ±1 + (1 ‚àíŒ≥)Œ±2 and œà(Œ∏) ‚â•Œ≥ Àúœà(Œ±1) + (1 ‚àíŒ≥) Àúœà(Œ±2) ‚àíœµ/3. For two distinct points
x1, x2 ‚ààX, we set PX such that Pr[X = x1] = Œ≥, Pr[X = x2] = 1 ‚àíŒ≥, Œ∑(x1) = (1 + Œ±1)/2, and
Œ∑(x2) = (1 + Œ±2)/2. By the deÔ¨Ånition of H‚àí, we choose function f : Rd ‚ÜíR such that f(x) < 0 for all
x ‚ààX, CŒ∑(x1)(f(x1)) ‚â§H‚àí(Œ∑(x1)) + œµ/3, and CŒ∑(x2)(f(x2)) ‚â§H‚àí(Œ∑(x2)) + œµ/3. By the continuity of
œà, there is an œµ‚Ä≤ > 0 such that œà(Œ∏) ‚â§œà(Œ∏ ‚àíœµ0) + œµ/3 for all 0 ‚â§œµ0 < œµ‚Ä≤. We also note that there exists an
Œª0 > 0 such that for any 0 < Œª < Œª0, we have
0 ‚â§E
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª) < œµ‚Ä≤.
Thus, we have
RœÜ(f) ‚àíR‚àó
œÜ = EœÜ(Y f(X)) ‚àíinf
f EœÜ(Y f(X))
= Œ≥[CŒ∑(x1)(f(x1)) ‚àíH(Œ∑(x1))] + (1 ‚àíŒ≥)[CŒ∑(x2)(f(x2)) ‚àíH(Œ∑(x2))]
‚â§Œ≥[H‚àí(Œ∑(x1)) ‚àíH(Œ∑(x1))] + (1 ‚àíŒ≥)[H‚àí(Œ∑(x2)) ‚àíH(Œ∑(x2))] + œµ/3
= Œ≥ Àúœà(Œ±1) + (1 ‚àíŒ≥) Àúœà(Œ±2) + œµ/3
‚â§œà(Œ∏) + 2œµ/3
‚â§œà

Œ∏ ‚àíE
max
X‚Ä≤‚ààB(X,œµ) œÜ(f(X‚Ä≤)f(X)/Œª)

+ œµ.
Furthermore, by Lemma C.6,
Rrob(f) ‚àíR‚àó
nat = E[1{sign(f(X)) Ã∏= sign(f‚àó(X)), X ‚ààB(DB(f), œµ)‚ä•}|2Œ∑(X) ‚àí1|]
+ Pr[X ‚ààB(DB(f), œµ), sign(f‚àó(X)) = Y ]
= E|2Œ∑(X) ‚àí1|
= Œ≥(2Œ∑(x1) ‚àí1) + (1 ‚àíŒ≥)(2Œ∑(x2) ‚àí1)
= Œ∏,
where f‚àóis the Bayes optimal classiÔ¨Åer which outputs ‚Äúpositive‚Äù for all data points.
C
Extra Theoretical Results
In this section, we provide extra theoretical results for adversarial defenses.
C.1
Adversarial vulnerability under log-concave distributions
Theorem 3.1 states that for any classiÔ¨Åer f, the value Pr[X ‚ààB(DB(f), œµ)] characterizes the robustness of the
classiÔ¨Åer. In this section, we show that among all classiÔ¨Åers such that Pr[sign(f(X)) = +1] = 1/2, linear
classiÔ¨Åer minimizes
lim inf
œµ‚Üí+0
Pr[X ‚ààB(DB(f), œµ)]
œµ
,
(7)
provided that the marginal distribution over X is products of log-concave measures. A measure is log-concave if
the logarithm of its density is a concave function. The class of log-concave measures contains many well-known
(classes of) distributions as special cases, such as Gaussian and uniform measure over ball.
Our results are inspired by the isoperimetric inequality of log-concave distributions by the work of [Bar01].
Intuitively, the isoperimetric problem consists in Ô¨Ånding subsets of prescribed measure, such that its measure
increases the less under enlargement. Our analysis leads to the following guarantee on the quantity (7).
21

classifier 
!"#$%&' 
2) 
Red area of 
!"#"$%&"'() 
classifier 
2+ 
<   Red area of 
Figure 4: Left Ô¨Ågure: boundary neighborhood of linear classiÔ¨Åer. Right Ô¨Ågure: boundary neighborhood of
non-linear classiÔ¨Åer. Theorem C.1 shows that the mass of Slinear is smaller than the mass of Snon-linear, provided
that the underlying distribution over the instance space is the products of log-concave distribution on the real
line.
Theorem C.1. Let ¬µ be an absolutely continuous log-concave probability measure on R with even density
function and let ¬µ‚äód be the products of ¬µ with dimension d. Denote by d¬µ = e‚àíM(x), where M : R ‚Üí[0, ‚àû]
is convex. Assume that M(0) = 0. If
p
M(x) is a convex function, then for every integer d and any classiÔ¨Åer
f with Pr[sign(f(X)) = +1] = 1/2, we have
lim inf
œµ‚Üí+0
PrX‚àº¬µ‚äód[X ‚ààB(DB(f), œµ)]
œµ
‚â•c
for an absolute constant c > 0. Furthermore, among all such probability measures and classiÔ¨Åers, the linear
classiÔ¨Åer over products of Gaussian measure with mean 0 and variance 1/(2œÄ) achieves the lower bound.
Theorem C.1 claims that under the products of log-concave distributions, the quantity Pr[X ‚ààB(DB(f), œµ)]
increases with rate at least ‚Ñ¶(œµ) for all classiÔ¨Åer f, among which the linear classiÔ¨Åer achieves the minimal
value.
C.1.1
Proofs of Theorem C.1
For a Borel set A and for œµ > 0, denote by Aœµ = {x : d(x, A) ‚â§œµ}. The boundary measure of A is then
deÔ¨Åned as
¬µ+(A) = lim inf
œµ‚Üí+0
¬µ(Aœµ) ‚àí¬µ(A)
œµ
.
The isoperimetric function is
I¬µ = inf{¬µ+(A) : ¬µ(A) = 1/2}.
(8)
Before proceeding, we cite the following results from [Bar01].
Lemma C.2 (Theorem 9, [Bar01]). Let ¬µ be an absolutely continuous log-concave probability measure on R
with even density function. Denote by d¬µ = e‚àíM(x), where M : R ‚Üí[0, ‚àû] is convex. Assume that M(0) = 0.
If
p
M(x) is a convex function, then for every integer d, we have I¬µ‚äód ‚â•IŒ≥‚äód, where Œ≥ is the Gaussian
measure with mean 0 and variance 1/(2œÄ). In particular, among sets of measure 1/2 for ¬µ‚äód, the halfspace
[0, ‚àû) √ó Rd‚àí1 is solution to the isoperimetric problem (8).
22

Now we are ready to prove Theorem C.1.
Proof. We note that
Pr[X ‚ààB(DB(f), œµ)]
= Pr[X ‚ààB(DB(f), œµ), sign(f(X)) = +1] + Pr[X ‚ààB(DB(f), œµ), sign(f(X)) = ‚àí1].
To apply Lemma C.2, we set the A in Lemma C.2 as the event {sign(f(X)) = +1}. Therefore, the set
Aœµ = {X ‚ààB(DB(f), œµ), sign(f(X)) = ‚àí1}.
By Lemma C.2, we know that for linear classiÔ¨Åer f0 which represents the halfspace [0, ‚àû) √ó Rd‚àí1, and any
classiÔ¨Åer f,
lim inf
œµ‚Üí+0
PrX‚àº¬µ‚äód[X ‚ààB(DB(f), œµ), sign(f(X)) = ‚àí1] ‚àíPr[sign(f(X)) = +1]
œµ
‚â•lim inf
œµ‚Üí+0
PrX‚àºŒ≥‚äód[X ‚ààB(DB(f0), œµ), sign(f0(X)) = ‚àí1] ‚àíPr[sign(f0(X)) = +1]
œµ
.
(9)
Similarly, we have
lim inf
œµ‚Üí+0
PrX‚àº¬µ‚äód[X ‚ààB(DB(f), œµ), sign(f(X)) = +1] ‚àíPr[sign(f(X)) = ‚àí1]
œµ
‚â•lim inf
œµ‚Üí+0
PrX‚àºŒ≥‚äód[X ‚ààB(DB(f0), œµ), sign(f0(X)) = +1] ‚àíPr[sign(f0(X)) = ‚àí1]
œµ
.
(10)
Adding both sides of Eqns. (9) and (10), we have
lim inf
œµ‚Üí+0
PrX‚àº¬µ‚äód[X ‚ààB(DB(f), œµ)]
œµ
‚â•lim inf
œµ‚Üí+0
PrX‚àºŒ≥‚äód[X ‚ààB(DB(f0), œµ)]
œµ
‚â•c.
C.2
Margin based generalization bounds
Before proceeding, we Ô¨Årst cite a useful lemma. We say that function f1 : R ‚ÜíR and f2 : R ‚ÜíR have a Œ≥
separator if there exists a function f3 : R ‚ÜíR such that |h1 ‚àíh2| ‚â§Œ≥ implies f1(h1) ‚â§f3(h2) ‚â§f2(h1).
For any given function f1 and Œ≥ > 0, one can always construct f2 and f3 such that f1 and f2 have a Œ≥-separator
f3 by setting f2(h) = sup|h‚àíh‚Ä≤|‚â§2Œ≥ f1(h‚Ä≤) and f3(h) = sup|h‚àíh‚Ä≤|‚â§Œ≥ f1(h‚Ä≤).
Lemma C.3 (Corollary 1, [Zha02]). Let f1 be a function R ‚ÜíR. Consider a family of functions fŒ≥
2 : R ‚ÜíR,
parametrized by Œ≥, such that 0 ‚â§f1 ‚â§fŒ≥
2 ‚â§1. Assume that for all Œ≥, f1 and fŒ≥
2 has a Œ≥ separator. Assume
also that fŒ≥
2 (z) ‚â•fŒ≥‚Ä≤
2 (z) when Œ≥ ‚â•Œ≥‚Ä≤. Let Œ≥1 > Œ≥2 > ... be a decreasing sequence of parameters, and pi be a
sequence of positive numbers such that P‚àû
i=1 pi = 1, then for all Œ∑ > 0, with probability of at least 1 ‚àíŒ¥ over
data:
E(X,Y )‚àºDf1(L(w, X, Y )) ‚â§1
n
n
X
i=1
fŒ≥
2 (L(w, xi, yi)) +
s
32
n

ln 4N‚àû(L, Œ≥i, x1:n) + ln 1
piŒ¥

for all w and Œ≥, where for each Ô¨Åxed Œ≥, we use i to denote the smallest index such that Œ≥i ‚â§Œ≥.
23

Lemma C.4 (Theorem 4, [Zha02]). If ‚à•x‚à•p ‚â§b and ‚à•w‚à•q ‚â§a, where 2 ‚â§p < ‚àûand 1/p + 1/q = 1, then
‚àÄŒ≥ > 0,
log2 N‚àû(L, Œ≥, n) ‚â§36(p ‚àí1)a2b2
Œ≥2 log2[2‚åà4ab/Œ≥ + 2‚åâ+ 1].
Theorem C.5. Suppose that the data is 2-norm bounded by ‚à•x‚à•2 ‚â§b. Consider the family Œì of linear classiÔ¨Åer
w with ‚à•w‚à•2 = 1. Let Rrob(w) := E(X,Y )‚àºD1[‚àÉXrob ‚ààB2(X, œµ) such that Y wT Xrob ‚â§0]. Then with
probability at least 1‚àíŒ¥ over n random samples (xi, yi) ‚àºD, for all margin width Œ≥ > 0 and w ‚ààŒì, we have
Rrob(w) ‚â§1
n
n
X
i=1
1(‚àÉxrob
i
‚ààB(xi, œµ) s.t. yiwT xrob
i
‚â§2Œ≥) +
s
C
n
 b2
Œ≥2 ln n + ln 1
Œ¥

.
Proof. The theorem is a straightforward result of Lemmas C.3 and C.4 with
L(w, x, y) =
min
xrob‚ààB(x,œµ) ywT xrob,
f1(g) = 1(g ‚â§0)
and
fŒ≥
2 (h) =
sup
|g‚àíh|<2Œ≥
f1(g) = f1(g ‚àí2Œ≥) = 1(g ‚â§2Œ≥),
and Œ≥i = b/2i and pi = 1/2i.
We note that for the ‚Ñì2 ball B2(x, œµ) = {x‚Ä≤ : ‚à•x ‚àíx‚Ä≤‚à•2 ‚â§œµ}, we have
1(‚àÉxrob
i
‚ààB(xi, œµ) s.t. yiwT xrob
i
‚â§2Œ≥) =
max
xrob
i
‚ààB(xi,œµ) 1(yiwT xrob
i
‚â§2Œ≥) = 1(yiwT xi ‚â§2Œ≥ + œµ).
Therefore, we can design the following algorithm‚ÄîAlgorithm 2.
Algorithm 2 Adversarial Training of Linear Separator via Structural Risk Minimization
Input: Samples (x1:n, y1:n) ‚àºD, a bunch of margin parameters Œ≥1, ..., Œ≥T .
1: For k = 1, 2, ..., T
2:
Solve the minimax optimization problem:
Lk(w‚àó
k, x1:n, y1:n) =
min
w‚ààS(0,1)
1
n
n
X
i=1
max
xrob
i
‚ààB(xi,œµ) 1(yiwT xrob
i
‚â§2Œ≥k)
=
min
w‚ààS(0,1)
1
n
n
X
i=1
1(yiwT xi ‚â§2Œ≥k + œµ).
3: End For
4: k‚àó= argmink Lk(w‚àó
k, x1:n, y1:n) +
r
C
n

b2
Œ≥2
k ln n + ln 1
Œ¥

.
Output: Hypothesis wk‚àó.
C.3
A lemma
We denote by f‚àó(¬∑) := 2Œ∑(¬∑) ‚àí1 the Bayes decision rule throughout the proofs.
Lemma C.6. For any classiÔ¨Åer f, we have
Rrob(f) ‚àíR‚àó
nat =E[1{sign(f(X)) Ã∏= sign(f‚àó(X)), X ‚ààB(DB(f), œµ)‚ä•}|2Œ∑(X) ‚àí1|]
+ Pr[X ‚ààB(DB(f), œµ), sign(f‚àó(X)) = Y ].
24

Proof. For any classiÔ¨Åer f, we have
Pr(‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) Ã∏= Y |X = x)
= Pr(Y = 1, ‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) = ‚àí1|X = x)
+ Pr(Y = ‚àí1, ‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) = 1|X = x)
= E[1{Y = 1}1{‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) = ‚àí1}|X = x]
+ E[1{Y = ‚àí1}1{‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) = 1}|X = x]
= 1{‚àÉx‚Ä≤ ‚ààB(x, œµ) s.t. sign(f(x‚Ä≤)) = ‚àí1}E1{Y = 1|X = x}
+ 1{‚àÉx‚Ä≤ ‚ààB(x, œµ) s.t. sign(f(x‚Ä≤)) = 1}E1{Y = ‚àí1|X = x}
= 1{‚àÉx‚Ä≤ ‚ààB(x, œµ) s.t. sign(f(x‚Ä≤)) = ‚àí1}Œ∑(x) + 1{‚àÉx‚Ä≤ ‚ààB(x, œµ) s.t. sign(f(x‚Ä≤)) = 1}(1 ‚àíŒ∑(x))
=
(
1,
x ‚ààB(DB(f), œµ),
1{sign(f(x)) = ‚àí1}(2Œ∑(x) ‚àí1) + (1 ‚àíŒ∑(x)),
otherwise.
Therefore,
Rrob(f)
=
Z
X
Pr[‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) Ã∏= Y |X = x]d PrX(x)
=
Z
B(DB(f),œµ)
Pr[‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) Ã∏= Y |X = x]d PrX(x)
+
Z
B(DB(f),œµ)‚ä•Pr[‚àÉX‚Ä≤ ‚ààB(X, œµ) s.t. sign(f(X‚Ä≤)) Ã∏= Y |X = x]d PrX(x)
= Pr(X ‚ààB(DB(f), œµ))
+
Z
B(DB(f),œµ)‚ä•[1{sign(f(x)) = ‚àí1}(2Œ∑(x) ‚àí1) + (1 ‚àíŒ∑(x))]d PrX(x).
We have
Rrob(f) ‚àíRnat(f‚àó)
= Pr(X ‚ààB(DB(f), œµ)) +
Z
B(DB(f),œµ)‚ä•[1{sign(f(x)) = ‚àí1}(2Œ∑(x) ‚àí1) + (1 ‚àíŒ∑(x))]d PrX(x)
‚àí
Z
B(DB(f),œµ)‚ä•[1{sign(f‚àó(x)) = ‚àí1}(2Œ∑(x) ‚àí1) + (1 ‚àíŒ∑(x))]d PrX(x)
‚àí
Z
B(DB(f),œµ)
[1{sign(f‚àó(x)) = ‚àí1}(2Œ∑(x) ‚àí1) + (1 ‚àíŒ∑(x))]d PrX(x)
= Pr(X ‚ààB(DB(f), œµ)) ‚àí
Z
B(DB(f),œµ)
[1{sign(f‚àó(x)) = ‚àí1}(2Œ∑(x) ‚àí1) + (1 ‚àíŒ∑(x))]d PrX(x)
+ E[1{sign(f(X)) Ã∏= sign(Œ∑(X) ‚àí1/2), X ‚ààB(DB(f), œµ)‚ä•}|2Œ∑(X) ‚àí1|]
= Pr(X ‚ààB(DB(f), œµ)) ‚àíE[1{X ‚ààB(DB(f), œµ)} min{Œ∑(X), 1 ‚àíŒ∑(X)}]
+ E[1{sign(f(X)) Ã∏= sign(Œ∑(X) ‚àí1/2), X ‚ààB(DB(f), œµ)‚ä•}|2Œ∑(X) ‚àí1|]
= E[1{X ‚ààB(DB(f), œµ)} max{Œ∑(X), 1 ‚àíŒ∑(X)}]
+ E[1{sign(f(X)) Ã∏= sign(Œ∑(X) ‚àí1/2), X ‚ààB(DB(f), œµ)‚ä•}|2Œ∑(X) ‚àí1|]
= Pr[X ‚ààB(DB(f), œµ), sign(f‚àó(X)) = Y ]
+ E[1{sign(f(X)) Ã∏= sign(f‚àó(X)), X ‚ààB(DB(f), œµ)‚ä•}|2Œ∑(X) ‚àí1|].
25

D
Extra Experimental Results
In this section, we provide extra experimental results to verify the effectiveness of our proposed method
TRADES.
D.1
Experimental setup in Section 5.3.1
We use the same model, i.e., the WRN-34-10 architecture in [ZK16], to implement the methods in [ZSLG16],
[KGB17] and [RDV17]. The experimental setup is the same as TRADES, which is speciÔ¨Åed in the beginning
of Section 5. For example, we use the same batch size and learning rate for all the methods. More speciÔ¨Åcally,
we Ô¨Ånd that using one-step adversarial perturbation method like FGSM in the regularization term, deÔ¨Åned in
[KGB17], cannot defend against FGSMk (white-box) attack. Therefore, we use FGSMk with the cross-entropy
loss to calculate the adversarial example X‚Ä≤ in the regularization term, and the perturbation step size Œ∑1 and
number of iterations K are the same as in the beginning of Section 5.
As for defense models in Table 5, we implement the ‚ÄòTRADES‚Äô models, the models trained by using other
regularization losses in [KGB17, RDV17, ZSLG16], and the defense model ‚ÄòMadry‚Äô in the antepenultimate
line of Table 5. We evaluate [WSMK18]‚Äôs model based on the checkpoint provided by the authors. The rest of
the models in Table 5 are reported in [ACW18].
D.2
Extra attack results in Section 5.3.1
Extra white-box attack results are provided in Table 8.
Table 8: Results of our method TRADES under different white-box attacks.
Defense
Under which attack
Dataset
Distance
Anat(f)
Arob(f)
TRADES (1/Œª = 1.0)
FGSM
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
56.38%
TRADES (1/Œª = 1.0)
DeepFool (‚Ñì2)
CIFAR10
0.031 (‚Ñì‚àû)
88.64%
84.49%
TRADES (1/Œª = 6.0)
FGSM
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
61.06%
TRADES (1/Œª = 6.0)
DeepFool (‚Ñì2)
CIFAR10
0.031 (‚Ñì‚àû)
84.92%
81.55%
The attacks in Table 5 and Table 8 include FGSMk [KGB17], DeepFool (‚Ñì‚àû) [MDFF16], LBFGSAt-
tack [TV16], MI-FGSM [DLP+18], C&W [CW17], FGSM [KGB17], and DeepFool (‚Ñì2) [MDFF16].
D.3
Extra attack results in Section 5.3.2
Extra black-box attack results are provided in Table 9 and Table 10. We apply black-box FGSM attack on the
MNIST dataset and the CIFAR10 dataset.
Table 9: Comparisons of TRADES with prior defense models under black-box FGSM attack on the MNIST
dataset. The models inside parentheses are source models which provide gradients to adversarial attackers.
Defense Model
Robust Accuracy Arob(f)
Madry
97.68% (Natural)
98.11% (Ours)
TRADES
97.75% (Natural)
98.44% (Madry)
26

Table 10: Comparisons of TRADES with prior defense models under black-box FGSM attack on the CIFAR10
dataset. The models inside parentheses are source models which provide gradients to adversarial attackers.
Defense Model
Robust Accuracy Arob(f)
Madry
84.02% (Natural)
67.66% (Ours)
TRADES
86.84% (Natural)
71.52% (Madry)
D.4
Experimental setup in Section 5.3.2
The robust accuracy of [MMS+18]‚Äôs CNN model is 96.01% on the MNIST dataset. The robust accuracy
of [MMS+18]‚Äôs WRN-34-10 model is 47.66% on the CIFAR10 dataset. Note that we use the same white-box
attack method introduced in the Section 5.3.1, i.e., FGSM20, to evaluate the robust accuracies of Madry‚Äôs
models.
D.5
Interpretability of the robust models trained by TRADES
D.5.1
Adversarial examples on MNIST and CIFAR10 datasets
In this section, we provide adversarial examples on MNIST and CIFAR10. We apply foolbox3 [RBB17] to
generate adversarial examples, which is able to return the smallest adversarial perturbations under the ‚Ñì‚àûnorm
distance. The adversarial examples are generated by using FGSMk (white-box) attack on the models described
in Section 5, including Natural models, Madry‚Äôs models and TRADES models. Note that the FGSMk attack is
foolbox.attacks.LinfinityBasicIterativeAttack in foolbox. See Figure 5 and Figure 6
for the adversarial examples of different models on MNIST and CIFAR10 datasets.
D.5.2
Adversarial examples on Bird-or-Bicycle dataset
We Ô¨Ånd that the robust models trained by TRADES have strong interpretability. To see this, we apply a
(spatial-tranformation-invariant) version of TRADES to train ResNet-50 models in response to the unrestricted
adversarial examples of the bird-or-bicycle dataset [BCZ+18]. The dataset is bird-or-bicycle, which consists of
30,000 pixel-224 √ó 224 images with label either ‚Äòbird‚Äô or ‚Äòbicycle‚Äô. The unrestricted threat models include
structural perturbations, rotations, translations, resizing, 17+ common corruptions, etc.
We show in Figures 7 and 8 the adversarial examples by the boundary attack with random spatial transforma-
tion on our robust model trained by the variant of TRADES. The boundary attack [BRB18] is a black-box attack
method which searches for data points near the decision boundary and attack robust models by these data points.
Therefore, the adversarial images obtained by boundary attack characterize the images around the decision
boundary of robust models. We attack our model by boundary attack with random spatial transformations, a
baseline in the competition. The classiÔ¨Åcation accuracy on the adversarial test data is as high as 95% (at 80%
coverage), even though the adversarial corruptions are perceptible to human. We observe that the robust model
trained by TRADES has strong interpretability: in Figure 7 all of adversarial images have obvious feature of
‚Äòbird‚Äô, while in Figure 8 all of adversarial images have obvious feature of ‚Äòbicycle‚Äô. This shows that images
around the decision boundary of truly robust model have features of both classes.
3Link: https://foolbox.readthedocs.io/en/latest/index.html
27

-norm=0.4208
Perturbed Image
Perturbation
label: 9
label: 5
label: 5
Original Image
‚Ñì‚àû
label: 3
-norm=0.4003
‚Ñì‚àû
-norm=0.1420
‚Ñì‚àû
TRADES
Madry
Natural
(a) adversarial examples of class ‚Äò3‚Äô
-norm=0.4054
Perturbed Image
Perturbation
label: 9
label: 9
label: 9
Original Image
‚Ñì‚àû
label: 4
-norm=0.3428
‚Ñì‚àû
-norm=0.1066
‚Ñì‚àû
TRADES
Madry
Natural
(b) adversarial examples of class ‚Äò4‚Äô
-norm=0.4151
Perturbed Image
Perturbation
label: 7
label: 9
label: 9
Original Image
‚Ñì‚àû
label: 5
-norm=0.3383
‚Ñì‚àû
-norm=0.0978
‚Ñì‚àû
TRADES
Madry
Natural
(c) adversarial examples of class ‚Äò5‚Äô
-norm=0.3694
Perturbed Image
Perturbation
label: 2
label: 2
label: 2
Original Image
‚Ñì‚àû
label: 7
-norm=0.3226
‚Ñì‚àû
-norm=0.0971
‚Ñì‚àû
TRADES
Madry
Natural
(d) adversarial examples of class ‚Äò7‚Äô
Figure 5: Adversarial examples on MNIST dataset. In each subÔ¨Ågure, the image in the Ô¨Årst row is the original
image and we list the corresponding correct label beneath the image. We show the perturbed images in the
second row. The differences between the perturbed images and the original images, i.e., the perturbations, are
shown in the third row. In each column, the perturbed image and the perturbation are generated by FGSMk
(white-box) attack on the model listed below. The labels beneath the perturbed images are the predictions of
the corresponding models, which are different from the correct labels. We record the smallest perturbations in
terms of ‚Ñì‚àûnorm that make the models predict a wrong label.
28

-norm=0.1076
Perturbed Image
Perturbation
label: truck
label: bird
label: frog
Original Image
‚Ñì‚àû
label: automobile
-norm=0.0988
‚Ñì‚àû
-norm=0.0088
‚Ñì‚àû
TRADES
Madry
Natural
(a) adversarial examples of class ‚Äòautomobile‚Äô
-norm=0.0832
Perturbed Image
Perturbation
label: ship
label: truck
label: bird
Original Image
‚Ñì‚àû
label: deer
-norm=0.0680
‚Ñì‚àû
-norm=0.0077
‚Ñì‚àû
TRADES
Madry
Natural
(b) adversarial examples of class ‚Äòdeer‚Äô
-norm=0.0809
Perturbed Image
Perturbation
label: truck
label: truck
label: dog
Original Image
‚Ñì‚àû
label: ship
-norm=0.0695
‚Ñì‚àû
-norm=0.0047
‚Ñì‚àû
TRADES
Madry
Natural
(c) adversarial examples of class ‚Äòship‚Äô
-norm=0.0783
Perturbed Image
Perturbation
label: airplane
label: automobile
label: automobile
Original Image
‚Ñì‚àû
label: truck
-norm=0.0665
‚Ñì‚àû
-norm=0.0061
‚Ñì‚àû
TRADES
Madry
Natural
(d) adversarial examples of class ‚Äòtruck‚Äô
Figure 6: Adversarial examples on CIFAR10 dataset. In each subÔ¨Ågure, the image in the Ô¨Årst row is the original
image and we list the corresponding correct label beneath the image. We show the perturbed images in the
second row. The differences between the perturbed images and the original images, i.e., the perturbations, are
shown in the third row. In each column, the perturbed image and the perturbation are generated by FGSMk
(white-box) attack on the model listed below. The labels beneath the perturbed images are the predictions of
the corresponding models, which are different from the correct labels. We record the smallest perturbations in
terms of ‚Ñì‚àûnorm that make the models predict a wrong label (best viewed in color).
29

(a) clean example
(b) adversarial example by boundary at-
tack with random spatial transformation
(c) clean example
(d) adversarial example by boundary at-
tack with random spatial transformation
(e) clean example
(f) adversarial example by boundary at-
tack with random spatial transformation
Figure 7: Adversarial examples by boundary attack with random spatial transformation on the ResNet-50
model trained by a variant of TRADES. The ground-truth label is ‚Äòbicycle‚Äô, and our robust model recognizes
the adversarial examples correctly as ‚Äòbicycle‚Äô. It shows in the second column that all of adversarial images
have obvious feature of ‚Äòbird‚Äô (best viewed in color).
30

(a) clean example
(b) adversarial example by boundary at-
tack with random spatial transformation
(c) clean example
(d) adversarial example by boundary at-
tack with random spatial transformation
(e) clean example
(f) adversarial example by boundary at-
tack with random spatial transformation
Figure 8: Adversarial examples by boundary attack with random spatial transformation on the ResNet-50
model trained by a variant of TRADES. The ground-truth label is ‚Äòbird‚Äô, and our robust model recognizes the
adversarial examples correctly as ‚Äòbird‚Äô. It shows in the second column that all of adversarial images have
obvious feature of ‚Äòbicycle‚Äô (best viewed in color).
31

