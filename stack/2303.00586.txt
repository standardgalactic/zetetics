FAIR-Ensemble: When Fairness Naturally Emerges
From Deep Ensembling
Wei-Yin Ko*
Cohere For AI Community
Daniel D’souza*
Cohere For AI Community
Karina Nguyen
UC Berkeley, Cohere For AI
Community
Randall Balestriero
Meta AI, FAIR
Sara Hooker
Cohere For AI
Abstract
Ensembling independent deep neural networks (DNNs) is a simple and eﬀective way to improve top-line metrics and
to outperform larger single models. In this work, we go beyond top-line metrics and instead explore the impact of
ensembling on subgroup performances. Surprisingly, even with a simple homogenous ensemble – all the individual
models share the same training set, architecture, and design choices – we ﬁnd compelling and powerful gains in
worst-k and minority group performance, i.e. fairness naturally emerges from ensembling. We show that the gains
in performance from ensembling for the minority group continue for far longer than for the majority group as more
models are added. Our work establishes that simple DNN ensembles can be a powerful tool for alleviating disparate
impact from DNN classiﬁers, thus curbing algorithmic harm. We also explore why this is the case. We ﬁnd that
even in homogeneous ensembles, varying the sources of stochasticity through parameter initialization, mini-batch
sampling, and the data-augmentation realizations, results in diﬀerent fairness outcomes.
1
Introduction
Deep Neural Networks (DNNs) are powerful function
approximators that outperform other alternatives on
a variety of tasks (Vaswani et al., 2017; Arulkumaran
et al., 2017; Hinton et al., 2012; He et al., 2016b). To
further boost performance, a simple and popular recipe
is to average the predictions of multiple DNNs, each
trained independently from the others to solve the given
task, this is known as model ensembling (Breiman, 2001;
Dietterich, 2000).
By averaging independently trained models, one avoids
single model symptomatic mistakes by relying on the
wisdom of the crowd to improve generalization perfor-
mance, regardless of the type of model being employed.
While existing work has focused on improvements to
aggregate performance (Fort et al., 2019; Gupta et al.,
2022; Opitz & Maclin, 1999) or gains in eﬃciency over a
single larger model (Wang et al., 2020; Wortsman et al.,
2022), there has been limited consideration of how sen-
sitive ensembling performance is on certain subsets of
the data distribution.
Understanding performance on subgroups is frequently
a concern from a fairness perspective. A common fair-
ness objective is mitigating disparate impact (Kleinberg
et al., 2016; Zafar et al., 2015) where a class or sub-
group of the dataset presents far higher error rates than
other subsets of the distribution. Speciﬁcally, we eval-
uate how ensembling impacts subgroup performance in
both balanced and imbalanced settings.
We evaluate
the impact of ensembling on both top-k and bottom-k
consisting of the K classes that the model or ensemble
predicts the most and least accurately respectively, and
ensemble performance on minority subgroups.
Our results are surprising: we ﬁnd that while average
performance quickly plateaus after a few models have
been aggregated, bottom-k and minority group per-
*Equal Contribution. Released as a preprint on March 2, 20231
arXiv:2303.00586v1  [stat.ML]  1 Mar 2023

CIFAR100
0
2
4
6
8
10
12
14
16
18
20
Ensemble Size
0
2
4
6
8
10
12
14
16
18
20
Acc. Ensemble/Base (%)
Resnet-9
Resnet-18
Resnet-34
Resnet-50
Top-10
Bottom-10
TinyImagenet
0
2
4
6
8
10
12
14
16
18
20
Ensemble Size
0
10
20
30
40
50
60
Acc. Ensemble/Base (%)
Resnet-9
Resnet-18
Resnet-34
Resnet-50
Top-10
Bottom-10
Figure 1: Relative Accuracy for Top-K/Bottom-K. Plot of the ratio of the ensemble accuracy over a single base model
(y-axis) illustrates strong beneﬁts for the minority group of ensembling (bottom-k) while the majority group (top-k) only
marginally beneﬁts.
formance disproportionately beneﬁts from ensembling.
Hence, a highly beneﬁcial and prized property of deep
ensembling with non-independent models emerges in
the regime where dozens of DNNs are aggregated: im-
provements to fairness outcomes naturally emerge from
the resulting ensemble. Furthermore, this fairness bene-
ﬁt appears even if all the individual models share the ex-
act same architecture, training objective, optimization
schedule and training set, i.e. the ensemble self-corrects
its bias only from aggregation.
We demonstrate consistent results across thousands of
experiments run on diﬀerent datasets and architectures.
Beyond fairness of deep ensembles, our empirical study
also oﬀers a rich variety of new observations e.g. tying
the severity of image corruption to the relative beneﬁts
that emerges from deep ensembles.
Our contributions can be enumerated as follows:
1. We demonstrate that simple homogeneous deep en-
sembles trained with the same objective, architec-
ture and optimization settings minimize worst-case
error. This holds in both balanced and imbalanced
datasets.
2. We further perform controlled sensitivity experi-
ments where constructed class imbalance and data
perturbation is applied (section 4.2). We observe
that homogeneous ensembles continue to improve
fairness and in particular, the minority group bene-
ﬁts more and more as the severity of the corruption
increases, while the corruption has very little im-
pact into the majority group’s beneﬁt from model
ensembling.
3. We further dive into possible causes for this emer-
gence of fairness in homogeneous deep ensembles
by measuring model disagreement (section 5.1) and
by ablating for the diﬀerent sources of random-
ness e.g. weight-initialization (section 5.2). We ob-
tain interesting results that suggest certain sources
of stochasticty disproportionately beneﬁt bottom-k
performance.
The codebase to reproduce our results and ﬁgures is
available here
2
Preliminaries
Throughout our study, we will consider a DNN to be
a mapping fθ : X 7→Y with trainable weights θ ∈Θ.
The training dataset D consists of N data points D =
2

{xn, yn}N
n=1. Given the training dataset D, the train-
able weights are optimized by minimizing an objective
function L.
We denote an ensemble of m classiﬁcation models by
{fθ1, . . . , fθm}, where fθi is the ith model. Each model
is trained with an empirical risk minimization objec-
tive (ERM) where the goal is to minimize the average
training loss.
Given n training points {(x1, y1) , . . . (xn, yn)},
JERM (fθ) = 1
n
n
X
i=1
ℓ(xi, yi; fθ) ,
where ℓ(x, y; fθ) : X × Y × fθ →R+is a loss function.
In our setting, each individual model is a probabilistic
classiﬁer which ouputs a conditional distribution over
the class labels Y given the sample x, i.e.
fθ(x) =
p(· | x).
Thus, for an speciﬁc data-point (x, y), the
loss of an individual predictor is deﬁned as ℓ(x, y; fθ) =
−log p(y | x, fθ). The loss of this ensemble is deﬁned as
ℓce (ρ, x, y) = −log Eρ[p(y | x, fθ)].
In this work, we consider the impact of ensembling on
both balanced and imbalanced subgroups. Fairness con-
siderations emerge for both groups.
Real world data
tends to be imbalanced, where infrequent events and
minority groups are less well represented in data collec-
tion processes. This leads to representational disparity
(Hashimoto et al., 2018) where the under-represented
group consequently experiences higher error rates. Even
when training sets are balanced, with an equivalent
number of training data points, certain features may
be imbalanced leading to a long-tail within a balanced
class.
Both settings can result in disparate impact,
where error rates for either a class or a subgroup are
far higher (Chatterjee, 2020; Feldman & Zhang, 2020).
This notion of unfairness is widely documented in ma-
chine learning systems: (Buolamwini & Gebru, 2018)
ﬁnd that facial analysis datasets reﬂect a preponderance
of lighter-skinned subjects, with far higher model error
rates for dark skinned women. (Shankar et al., 2017)
show that models trained on datasets with limited geo-
diversity show sharp degradation on data drawn from
other locales.
Word frequency co-occurrences within
text datasets frequently reﬂect social biases relating to
gender, race and disability (Garg et al., 2017; Zhao
et al., 2018; Bolukbasi et al., 2016; Basta et al., 2019).
Here, our goal is to understand whether design choices
such as ensembling exacerbate or mitigate this disparity.
3
Experimental Set-up
Experimental set-up. We evaluate our methodology
on CIFAR100 (Krizhevsky et al., 2009) and TinyIma-
genet (Russakovsky et al., 2015) datasets across vari-
ous architectures: Resnet9/18/34/50 (He et al., 2016a),
VGG16 (Simonyan & Zisserman, 2014) and MLP-Mixer
(Tolstikhin et al., 2021), ViT (Dosovitskiy et al., 2020).
We include additional training and implementation de-
tails in appendix A.
Whenever we report results on the ensemble, unless the
number of models is explicitly stated, it will comprise
of 20 models that were trained on the same training
dataset.
All the models are trained completely independent from
each other as described in (Breiman, 2001; Lee et al.,
2015), i.e.
we do not control for any source of ran-
domness as this will be explored exclusively within sec-
tion 5.2. With homogeneous training settings, the only
diﬀerence between the weak learners is the inherent
stochasticity introduced by DNN training.
We com-
bine the predictions from each model by averaging the
predicted probabilities.
Balanced Dataset Sub-Groups.
For top-k and
bottom-k, we calculate the class accuracy on the base
model and ﬁnd the best and worst K (K=10) performing
classes and track the associated images. We then pro-
ceed to measure how performance on these subgroups
change as a function of the ensemble size. We highlight
that although we leverage K = 10 to deﬁne the top and
bottom groups, the precise choice of K does not impact
our ﬁndings, as demonstrated in ﬁg. 9.
Imbalanced Dataset Sub-Groups We construct a
minority group by modifying the CIFAR10 training set
to have a randomly sampled class which is only 15%,
10% of the original value. We preserve the remaining
classes as balanced classes and refer to these classes as
3

CIFAR100
ensemble/base
Resnet9
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
Resnet18
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
Resnet34
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
Resnet50
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
VGG16
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
MLP-Mixer/ViT
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
TinyImagenet
ensemble/base
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
Figure 2: Depiction of the accuracy gain as a ratio of ensemble accuracy % over the singular base model (y-axis) by
per-group (top-k and bottom-k) test set accuracies. top row: CIFAR100, bottom row: TinyImagenet. For each dataset,
we plot the accuracy gain for diﬀerent DNN architectures (columns) as the number of models within the ensemble grows
(x-axis). We clearly observe that as the number of models within the ensemble grows, the bottom-k group beneﬁts, i.e.
its corresponding classiﬁcation accuracy, outgrows the top-k group. This occurs despite the fact that the models within the
ensemble are all employing the same hyperparameters–i.e. they inherently share the same functional biases. The absolute
accuracies are provided in table 1 below.
majority.
4
Results and Discussion.
In (section 4.1) we establish that an ensemble of DNNs
that have the same architecture and hyperparameters
beneﬁt the minority or bottom-k group. In (section 4.2)
we perform a precise ablation study when controlling
for class-imbalance and corruption levels in the training
data, to again observe that fairness naturally emerges
in deep ensembles.
4.1
Ensembling provides disproportionate
gains to bottom-k and minority classes
Impact on bottom-k In ﬁg. 1 and ﬁg. 2, we plot
the relative gain in accuracy i.e.
the ratio be-
tween the ensemble and base model performance on top-
k/bottom-k for each model architecture and dataset.
This amounts to asking what is the relative improve-
ment in performance of using an ensemble over a single
model?
Our results are consistent across models and datasets.
Ensembling disproportionately beneﬁts bottom-k per-
formance. For CIFAR100, this beneﬁt ranges from 14%-
29% for bottom-k across diﬀerent architectures com-
pared to 1%-4% for top-k. For TinyImagenet the ben-
eﬁts are even more pronounced with a maximum gain
of 55% for bottom-k compared to 5% for top-k across
diﬀerent architectures.
We also provide in table 1 the absolute per-group ac-
curacy and average performances for the corresponding
models and datasets, further reinforcing that observa-
tion. For example, we observe that a 20-model ensemble
produces a maximum gain of 12.92% absolute accuracy
gain for bottom-k over all architectures vs. a maximum
gain of 3.86% in accuracy for top-k.
This is a crucial ﬁnding, as it demonstrates how even
when ensembling DNNs that share all the same hy-
perparameters and trained on the same data, fairness
emerges out of ensembling to improve disparate impact
outcomes for bottom-k. While it is clear that bottom-
k beneﬁts the most and cannabilizes performance from
top-k in some settings, the overall gains in mean accu-
racy indicate that often both groups beneﬁt from en-
sembling with bottom-k just disproportionately so.
Impact on minority subgroups We independently
train a 50 model Resnet9 ensemble on a modiﬁed ci-
far10 dataset with a minority and majority subgroup.
4

Table 1: Depiction of the average and per-group (top-k and bottom-k) absolute test set accuracies corresponding to the
models and datasets depicted in ﬁg. 2 above, again the ensemble consists of 20 models. We clearly observe that fairness
naturally emerges through ensembling i.e. the bottom group substantially beneﬁts from ensembling compared to the top
group.
CIFAR100
TinyImagenet
Ensemble
Single
Ensemble
Single
Arch.
mean
top-k bottom-k mean
top-k bottom-k mean
top-k bottom-k mean
top-k bottom-k
Resnet9
77.01
92.18
58.43
72.21
90.80
51.30
58.29
86.66
23.60
50.71
82.80
15.20
Resnet18
78.15
94.19
59.13
73.57
92.00
49.70
56.50
86.64
24.82
49.29
84.20
16.60
Resnet34
78.68
93.84
58.89
74.26
92.10
50.30
58.89
87.44
27.25
52.18
84.60
20.60
Resnet50
77.94
93.53
58.34
74.88
92.40
50.70
60.35
87.38
28.09
55.00
86.20
22.00
VGG16
76.95
92.88
57.32
71.24
91.50
44.40
67.04
90.27
38.71
60.36
89.20
26.20
MLPMixer/ViT
66.69
87.95
40.93
60.25
84.50
33.00
56.97
85.60
22.42
51.23
84.20
17.20
In table 2, we report average performance, abso-
lute change, relative change of the 50 model ensem-
ble relative to a single base model.
We observe that
the minority group at both 10% and 15% beneﬁts more
than the majority from ensembling, where it receives
a maximum relative gain in test-set accuracy of 9.61%
vs the majority at 1.49%. We conclude that minority
groups beneﬁt the most from the average of indepen-
dently trained models.
Beneﬁts of even larger ensembles. We are inter-
ested in understanding what is an optimal number of
ensembles to optimize for bottom-k gain.
To do so,
in ﬁg. 2 we plot the trend lines in performance as we
increase the size of the ensemble.
As the number of
models within the ensemble grows, accuracy gains dis-
proportionately accrue to the bottom group versus the
top group.
We observe that top-k gains plateau far
faster than bottom-k – indeed even at 20 model ensem-
bles we still see positive gains for bottom-k for certain
architectures such as VGG16 and Resnet9. In the ap-
pendix, we observe the limits of this trend by extend-
ing the ensemble size past 20 models; upto 50 model
ensembles for some conﬁgurations in ﬁg. 14. In both
TinyImagenet and CIFAR100 datasets, the mean ac-
curacy improvements of architectures such as Resnet9,
Resnet50, and VGG16 all slowly plateaued as the en-
semble reaches the size of 50. Moreover, the top-k and
bottom-k relative test set accuracies of ensemble versus
single model between the 20 to 50 ensemble sizes also
plateaued. Continued improvements are slight to negli-
gible as the ensemble size extends beyond 20 across all
variants (as shown in ﬁgs. 15 and 16).
Figure 3:
For each row we depict three of the diﬀer-
ent types of corruptions from the CIFAR100-C dataset
(elastic_transform, snow, and impulse_noise respec-
tively), and for each column we depict the corruption sever-
ity levels (1 7→5). The image belongs to the lion class.
4.2
FAIR Ensemble: Improved Robustness
Data Corruption. In this section, we aim to uncover
the relationship between ensembling and performance
in a corrupted inputs setting. We note that prior work
has found that deep neural network ensembles improve
uncertainty estimates (Lakshminarayanan et al., 2016)
and can be beneﬁcial for OOD (Xia & Bouganis, 2022).
Here our goal is to instead understand how these ben-
eﬁts accrue to sub-groups when the data in corrupted
and how this relates to the severity of the input corrup-
tion.
We benchmark our ensembles on all severity levels[1-
5

5] in CIFAR100-C (Hendrycks & Dietterich, 2018).
CIFAR100-C is an artiﬁcially constructed dataset of 19
individual corruptions on the CIFAR100 Test Dataset.
ﬁg. 3 shows an example of the corruptions that are
present in the dataset.
For completeness, we bench-
mark and average performance across all corruptions
for each severity level.
In ﬁg. 4, we plot the gain in
test-set accuracy achieved by the top-k and bottom-k
(K=10) classes as the ensemble size increases relative
to a single model. We see that, consistent with earlier
results, gains on top-k plateau earlier as the size of the
ensemble increases. However, the beneﬁts of ensembles
are outsized when the data is noisy with corruptions.
We observe in ﬁg. 17 that the largest gains occur where
there is maximum severity, with a maximum relative
gain of 40.17% for severity 5 vs 20.18% for severity 1.
0
2
4
6
8
10
12
14
16
18
20
0%
10%
20%
30%
40%
50%
Relative Accuracy %
Top-10
Bottom-10
Figure 4: Depiction of the per-group (top-k and bottom-k)
test performances (y-axis) of the ensemble as the number of
models being aggregated increases (x-axis) and for varying
severity of corruption levels (recall ﬁg. 3) from light to dark
color shading. A striking observation is that not only DNN
ensembling improves fairness by increasing the performance
on the bottom group more drastically than on the top group,
but also that this eﬀect is even more prominent for higher
severity.
5
Why do ensembles
disproportionately beneﬁt
bottom-k and minority group
performance?
5.1
Diﬀerence in Churn Between Models
Explains Ensemble Fairness
It might not be clear a priori how to explain the dis-
parate impact of deep ensembling in minority groups
compared to majority groups, as we observed in the
previous sections 4.1 and 4.2. One popular metric of
model disagreement, known as the churn will provide
us with an obvious yet quantiﬁable answer which we
propose to measure in this section.
Experiment set-up. To understand the beneﬁt of
model ensembling one has to recall that if all the models
within the ensemble agree, then there will not be any
beneﬁt to aggregating the individual predictions.
In
fact, it is quite trivial to see that to aggregate the same
prediction is futile. Hence, model disagreement is a key
metric that will explain the stark change in performance
that our DNN ensembles have shown on the minority
group. We consider diﬀerences in churn between top-k
and bottom-k. We also recall that the predictive churn
is a measure of predictive divergence between two mod-
els. There are several diﬀerent proposed deﬁnitions of
predictive churn (Chen et al., 2020; Shamir & Coviello,
2020; Snapp & Shamir, 2021); we will employ the one
that is deﬁned on two models f1 and f2 as done by (Mi-
lani Fard et al., 2016) as the fraction of test examples
where the predictions of two models disagree:
C(f1, f2) = EX

1{ ˆYx;f1̸= ˆYx;f2}

,
(1)
where 1 is an indicator function for whether the pre-
dictions by each model match. For an ensemble with
more than two models, we will always report the average
churn computed across 100 randomly sampled (without
replacement) pairwise combination of models.
Observations.
In ﬁg. 5 we report churn for each ar-
chitecture we benchmark for both datasets. We observe
that architectures diﬀer in the overall level of churn,
but a consistent observation across architectures is that
there are large gaps in the level of churn between top-k
and bottom-k. For example, on ResNet18 for TinyIm-
6

Table 2: Depiction of the controlled imbalanced CIFAR10 experiment for which one class (Minority) is subsampled to only
maintain 10% (left) and 15% (right) of its original training set size, all other classes (Majority) are untouched. We observe
again that ensembling (All Sources row) improves the minority group (here the sub-sampled class) much more than the
majority group (the other nine classes), echoing our results from ﬁg. 2 and table 1. We further observe that when controlling
for the sources of randomness (recall section 5.3) the fairness of the ensemble can be further improved.
10%
15%
Ensemble Acc.
Abs. Change
Rel. Change
Ensemble Acc.
Abs. Change
Rel. Change
variant
Majority Minority Majority Minority Majority Minority Majority Minority Majority Minority Majority Minority
All Sources
95.13
52.95
1.38
3.45
1.47
6.97
95.12
62.72
1.35
3.02
1.44
5.06
BatchOrder
95.08
53.44
1.32
3.94
1.41
7.96
95.09
63.19
1.32
3.49
1.41
5.84
Init
95.03
52.64
1.27
3.14
1.36
6.35
94.97
62.74
1.21
3.04
1.29
5.10
DA
95.06
54.26
1.30
4.76
1.39
9.61
95.03
63.46
1.27
3.77
1.35
6.31
Init & BatchOrder 95.15
53.68
1.40
4.18
1.49
8.45
95.13
63.29
1.36
3.59
1.45
6.02
CIFAR100
Resnet9
Resnet18
Resnet34
Resnet50
VGG16
MLP-Mixer
0%
10%
20%
30%
40%
50%
60%
70%
Churn %
Top-10
Bottom-10
TinyImagenet
Resnet9
Resnet18
Resnet34
Resnet50
VGG16
ViT
0%
10%
20%
30%
40%
Churn %
Top-10
Bottom-10
Figure 5: Depiction of churn results across models and datasets. The results demonstrate that churn is signiﬁcantly higher
for the bottom-k group compared to the top-k group, indicating that ensembling these models disproportionately impacts the
bottom-k group (as deﬁned in eq. (1)). The diﬀerence in churn between bottom-k and top-k groups varies based on model
architecture, suggesting that some ensembles achieve more fairness than others.
ageNet the diﬀerence is churn of 9.22% and 33.21% for
top-k and bottom-k respectively, while it is 7.78% and
39.89% for top-k and bottom-k for CIFAR100. In short,
the models disagree much more when looking at samples
belonging to the bottom-k groups than when looking at
samples belonging to the top-k group. The models not
only perform poorly on the bottom-k groups, but also
vary in which samples are incorrectly classiﬁed (for def-
inition of churn, please see eq. (1)). As a result, that
group beneﬁts much more from ensembling.
From our experience we conclude that there is not a
systematic failure of a given architecture on the in-
correctly classiﬁed samples (bottom-k). Furthermore,
those models tend to agree on well-classiﬁed samples
(top-k), where we recall that agreement is measured in
term of the churn. From those we naturally reach the
observation that ensembling models is particularly use-
ful for improving accuracy on the bottom-k samples.
5.2
Characterizing Stochasticity In Deep
Neural Networks Training
What is particularly striking about our results so far is
that we are observing these gains in a setting of homo-
geneous ensembles, where all models are trained with
the same optimization choices. This means the diver-
sity in the ensemble arises from conventional training
choices that introduce stochasticity into the deep neural
network optimization. Stochasticity is a crucial ingre-
dient to produce meaningful ensembles – no stochastic-
ity directly implies that a single model or an ensemble
would be identical. In this section, we ask why does the
7

stochasticity introduced in homogenous ensembles ben-
eﬁt per-group performance? We study how stochastic-
ity emerges across training from diﬀerent sources (sec-
tion 5.3), and also ﬁnd diﬀerence in what sources of
stochasticity appear to disproportionately beneﬁt top-k
vs bottom-k performance.
5.3
Controlling for the Sources of
Stochasticity in Ensembles
To understand more what introduces the most signiﬁ-
cant levels of stochasticity, we ﬁrst explore how diﬀerent
sources of randomness impact the training trajectories
of DNNs. Deep Neural Network training includes model
design choices which are inherently stochastic, including
implementation choices such as Random Initialization
(Glorot & Bengio, 2010; He et al., 2016b), Data aug-
mentation (Kukačka et al., 2017; Hernández-García &
König, 2018), Data shuﬄing and ordering (Smith et al.,
2018; Shumailov et al., 2021). In this section we ask
What source dominates stochasticity in DNN training?
Can we perform ablations to understand whether there
are more beneﬁcial distributions of stochasticity for fair-
ness outcomes?
Experiment set-up. To isolate the impact of the dif-
ferent sources of stochasticity, we propose a thorough
ablation study. In particular, we consider the following
sources:
• Change
Model
Initialization
(Init):
for
this ablation, we change the model initialization
weights by changing the torch seed for each model
before the model is instantiated.
• Change Batch Ordering (BatchOrder): for this
ablation, we change the ordering of image data in
each minibatch by changing the seed for the dat-
aloader for each model training.
• Change Model Initialization and Batch Or-
dering (Init & BatchOrder): for this ablation,
both the model initialization and batch ordering
are changed for each model training.
• Change Data Augmentation (DA): for this ab-
lation, only the randomness in the data augmen-
tation (e.g. probability of random ﬂips, probabil-
ity of CutMix(Yun et al., 2019), etc.) is changed.
The relevant torch and numpy seeds are changed
right before instantiating the data augmentation
pipeline. Custom ﬁxed-seed data augmentations is
also used.
• Change Model Initialization, Batch Order-
ing and Data Augmentation (All Sources):
for this ablation, the model initialization, batch or-
dering and data augmentation seeds are changed
for each model training. This ablation is used as
the baseline to compare with the other ablations.
A last source of randomness can emerge from hardware
or software choices and round-oﬀerrors (Zhuang et al.,
2022; Shallue et al., 2019) which we found to be neg-
ligible compared to the others, and as we use the ex-
act same hardware and software choices across experi-
ments, we already control for it and thus omit it from
our study. In order to measure the amount of stochas-
ticity that each of the above sources accounts for we will
not only visually depict the per-group performances be-
tween individual training episodes, but we also propose
the use of two quantitative metrics. First, we will lever-
age the L1-Distance which is calculated for every epoch
by averaging the absolute distance in accuracy among
the ensemble members. The ﬁnal metric is obtained by
averaging these values across the training epochs. We
will also leverage the Variance which is calculated on
the accuracy among the ensemble members at a given
epoch. The ﬁnal metric is obtained by averaging these
values across the training epochs.
Observations. In ﬁg. 6, we plot these measures of
stochasticity for both CIFAR100 and TinyImagenet on
diﬀerent networks. We observe that the single sources
of noise dominate, such that the ablations themselves
equate to the level of noise in the network with all
sources of noise present.
In particular, we observe
one striking phenomenon:
the variation of the data
ordering within each epoch between training trajecto-
ries BatchOrder is the main source of randomness. It
is equivalent to the level of noise we observe for the
network with all sources of noise All Sources, and
the network with the ablation Init & BatchOrder. As
seen in ﬁg. 6 when the batch-ordering is kept the same
across training episodes, varying the data-augmentation
and/or the model initialization has very little impact.
8

CIFAR100 Resnet9
TinyImagenet Resnet50
test accuracy %
(a) Batch-Order
0
5
10
15
20
0
20
40
60
80
100
Top-10
Overall
Bottom-10
epochs
(b) Data-Augmentation
0
5
10
15
20
0
20
40
60
80
100
Top-10
Overall
Bottom-10
epochs
(c) Batch-order
0
20
40
60
80
0
20
40
60
80
100
Top-10
Overall
Bottom-10
epochs
(d) Model-initialization
0
20
40
60
80
0
20
40
60
80
100
Top-10
Overall
Bottom-10
epochs
All Sources
BatchOrder
Init
DA
Init & BatchOrder
0
1
2
3
4
5
6
L1 Distance
Top-10
Bottom-10
All Sources
BatchOrder
Init
DA
Init & BatchOrder
0
5
10
15
20
Variance
Top-10
Bottom-10
All Sources
BatchOrder
Init
DA
Init & BatchOrder
0
1
2
3
4
5
6
L1 Distance
Top-10
Bottom-10
All Sources
BatchOrder
Init
DA
Init & BatchOrder
0
5
10
15
20
Variance
Top-10
Bottom-10
Figure 6: Depiction of multiple individual training episodes of a Resnet9 model on CIFAR100 (top row, left) and Resnet50
model on TinyImagenet (top row, right). We clearly observe that varying one factor of stochasticity at a time highlights
which ones provide the most randomness between training episodes, in particular in this setting we see that batch-ordering
is the main source. On the other hand model-init and data-augmentation have little eﬀect and we even observe very similar
trends at diﬀerent epochs between the individual runs.
5.4
Can Diﬀerent Sources of Stochasticity
Improve Deep Ensemble Fairness?
We already observed that standard ensembles, i.e. with
no source of stochasticity being controlled, improve fair-
ness over individual models (recall sections 4.1 and 4.2).
Yet, we also observed in the previous section 5.3 that
diﬀerent sources of stochasticity have a clear impact into
individual training episodes of a DNN. In this section,
we seek to understand how these diﬀerent sources of
noise aggregate in an ensemble and the diﬀerences in
fairness outcomes these sources of noise produce. We
also seek to understand if certain distributions of noise
beneﬁt top-k versus bottom-k performance.
Dominant sources of stochasticity. In ﬁg. 7, we plot
the accuracy diﬀerence between average top-k and
bottom-k. At 0 this measure indicates that the model
performs uniformly on both top-k and bottom-k. We
observe that for the majority of dataset/architecture
combinations, batch-ordering minimizes the gap be-
tween top and bottom-k class accuracy. There is one ex-
ception to this, as we see that data-augmentation vari-
ation for Resnet18 on TinyImagenet creates the largest
decrease.
Distributions of Stochasticity that beneﬁt ma-
jority vs minority. While accuracy diﬀerence gives
a sense of what bridges the gap between sub-group per-
formance, it does not tell us if certain sources of noise
beneﬁt minority subgroups more. To understand this,
we compare our noise ablations of our minority group
experiment in section 4.2.
In table 2, we see a con-
sistent trend, where stochasticity introduced by data
augmentation DA consistently favors minority group per-
formance, and stochasticity introduced by initialization
and batch ordering Init & BatchOrder consistently fa-
vors majority group performance.
9

Resnet family on CIFAR100
% accuracy diﬀ.
1
3
5
7
9
11
13
15
17
19
33
34
35
36
37
38
39
Init. & BatchOrder
Init.
BatchOrder
DA
All Sources
models in ensemble
(a) Resnet18, 20 models
1
3
5
7
9
11
13
15
17
19
33
34
35
36
37
38
39
Init. & BatchOrder
Init.
BatchOrder
DA
All Sources
models in ensemble
(b) Resnet34, 20 models
1
3
5
7
9
11
13
15
17
19
33
34
35
36
37
38
39
Init. & BatchOrder
Init.
BatchOrder
DA
All Sources
models in ensemble
(c) Resnet50, 20 models
Resnet family on TinyImagenet
% accuracy diﬀ.
1
3
5
7
9
11
13
15
17
19
58
59
60
61
62
63
64
65
Init. & BatchOrder
Init.
BatchOrder
DA
All Sources
models in ensemble
(d) Resnet18, 20 models
1
3
5
7
9
11
13
15
17
19
58
59
60
61
62
63
64
65
Init. & BatchOrder
Init.
BatchOrder
DA
All Sources
models in ensemble
(e) Resnet34, 20 models
1
3
5
7
9
11
13
15
17
19
58
59
60
61
62
63
64
65
Init. & BatchOrder
Init.
BatchOrder
DA
All Sources
models in ensemble
(f) Resnet50, 20 models
Figure 7: Accuracy % diﬀerence between top and bottom 10 classes, for Resnet18, 34, and 50 for CIFAR100 and TinyIma-
genet. We clearly observe that once we control for the diﬀerent sources of stochasticities, it is possible to skew the ensemble
to favor the bottom group, in which case fairness is further ampliﬁed compared to the baseline ensemble, but also to favor
the top group, in which case fairness is worse than the baseline ensemble. Although the trends seem mostly consistent across
architectures of the same family (Resnets) and datasets, it is not necessarily the case between architecture families: see ﬁg. 10
for additional runs on CIFAR100 with MLP-Mixer and VGG16, and on TinyImagenet with VGG16 and ViT.
6
Related Work
Deep ensembling of neural networks has been shown to
be a simple and popular formula for improving top-line
metrics (Lakshminarayanan et al., 2016). Several works
have sought to further improve aggregate performance
by amplifying diﬀerences between models in the ensem-
ble – ranging from varying the data augmentation used
for each model (Stickland & Murray, 2020), architec-
ture (Zaidi et al., 2021), and hyperparameters (Worts-
man et al., 2022) to objectives (Jain et al., 2020). In
contrast to prior work, we focus on simple ensembles
where all design choices are shared between each model
in the ensemble. We are also focused on understanding
the implications of ensembling on fairness objectives,
rather than on top-line metrics.
Beyond Top-line metrics Discussions of algorithmic
bias often focus on the way in which our datasets are
collected and curated (Barocas et al., 2019; Zhao et al.,
2017; Shankar et al., 2017), with limited work to-date
understanding the role of model design or optimization
choices on amplifying or curbing bias (Ogueji et al.,
2022; Hooker et al., 2019). Consistent with this, there
has been limited work to-date on understanding the
implications of ensembling on subgroup error. (Grgić-
Hlača et al., 2017) point out the theoretical possibility
of using an ensemble to improve model fairness on a
simple binary model.
However, they do not provide
empirical evidence and limit consideration to an ensem-
ble formed by randomly selecting the individual models
from a set of possible classiﬁers. In contrast, our work
establishes that ensembles mitigate harm across 1000s of
10

model and dataset combinations.Another relevant work
is (Bhaskaruni et al., 2019) which considers AdaBoost
(Freund & Schapire, 1995) ensembles and shows that
upweighting unfairly predicted examples reaches higher
fairness when learning base models. In contrast to our
simple averaging of predictions of homogeneous DNNs
trained in parallel, the authors explicitly optimize for
improved fairness by using boosting techniques with a
focus on specialized ensembles.
Understanding why ensembling beneﬁts sub-
group performance.
Several works to date have
sought to understand why weight averaging performs
well and improves top-line metrics (Gupta et al.,
2022).
However, few to our knowledge have sought
to understand why ensembles disproportionately ben-
eﬁt bottom-k and minority group performance. (Rame
et al., 2022) explore why weight averaging performs well
on out-of-distribution data, relating variance to diver-
sity shift. In this work, we instead explore how individ-
ual sources of inherent stochasticity in uniform ensem-
bles impact subgroup performance.
Impact of stochasticity on fairness objectives Sev-
eral works to-date have considered how stochasticity
can impact top-line metrics (Nagarajan et al., 2018).
Most relevant to our work is (Qian et al., 2021; Zhuang
et al., 2022; Madhyastha & Jain, 2019; Summers & Din-
neen, 2021) that evaluates how stochasticity in training
impacts fairness in DNN Systems. However, all these
works restrict their treatment to a single model setting,
and do not evaluate the impact of ensembling.
7
Conclusion and Future Work
In this work, we establish that while ensembling is often
seen as a method of improving average performance,
it can also provide signiﬁcant fairness gains.
This
suggests ensembles are a powerful tool to improve
fairness outcomes in sensitive domains where human
welfare is at risk.
We explore why ensembles are so
eﬀective at improving fairness outcomes,
and ﬁnd
that certain distributions of stochasticity appear to
disproportionately favor top-k and bottom-k.
This
is an interesting avenue of future work, and suggests
designing ensembles which amplify certain sources of
stochasticity may further amplify the gains for fairness
outcomes.
Future work. Our work ablating sources of stochas-
ticity and ﬁnding that certain sources consistently fa-
vor minority and majority groups suggests that future
work should explicitly optimize to amplify these sources.
Another avenue would be to a priori identify, given an
architecture, the source of stochasticity that will fa-
vor fairness. We obtained preliminary results that such
sources might vary based on the architecture.
11

References
Kai
Arulkumaran,
Marc
Peter
Deisenroth,
Miles
Brundage, and Anil Anthony Bharath. A brief sur-
vey of deep reinforcement learning.
arXiv preprint
arXiv:1708.05866, 2017.
Solon Barocas, Moritz Hardt, and Arvind Narayanan.
Fairness and Machine Learning: Limitations and Op-
portunities. fairmlbook.org, 2019. http://www.fair
mlbook.org.
Christine Basta, Marta R. Costa-jussà, and Noe Casas.
Evaluating the underlying gender bias in contextu-
alized word embeddings. In Proceedings of the First
Workshop on Gender Bias in Natural Language Pro-
cessing, pp. 33–39, Florence, Italy, August 2019. As-
sociation for Computational Linguistics. doi: 10.186
53/v1/W19-3805. URL https://aclanthology.org
/W19-3805.
Dheeraj Bhaskaruni, Hui Hu, and Chao Lan. Improving
prediction fairness via model ensemble. In 2019 IEEE
31st International Conference on Tools with Artiﬁcial
Intelligence (ICTAI), pp. 1810–1814, 2019. doi: 10.1
109/ICTAI.2019.00273.
Tolga Bolukbasi,
Kai-Wei Chang,
James Y Zou,
Venkatesh Saligrama, and Adam T Kalai. Man is to
computer programmer as woman is to homemaker?
debiasing word embeddings. Advances in neural in-
formation processing systems, 29, 2016.
Leo Breiman. Random forests. Machine learning, 45
(1):5–32, 2001.
Joy Buolamwini and Timnit Gebru. Gender shades: In-
tersectional accuracy disparities in commercial gender
classiﬁcation. In Conference on fairness, accountabil-
ity and transparency, pp. 77–91, 2018.
Satrajit Chatterjee. Coherent gradients: An approach
to understanding generalization in gradient descent-
based optimization. arXiv preprint arXiv:2002.10657,
2020.
Zhe Chen, Yuyan Wang, Dong Lin, Derek Zhiyuan
Cheng, Lichan Hong, Ed H. Chi, and Claire Cui. Be-
yond point estimate: Inferring ensemble prediction
variation from neuron activation strength in recom-
mender systems, 2020.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay
Vasudevan, and Quoc V Le. Autoaugment: Learn-
ing augmentation policies from data. arXiv preprint
arXiv:1805.09501, 2018.
Thomas G Dietterich. Ensemble methods in machine
learning. In International workshop on multiple clas-
siﬁer systems, pp. 1–15. Springer, 2000.
Alexey
Dosovitskiy,
Lucas
Beyer,
Alexander
Kolesnikov,
Dirk
Weissenborn,
Xiaohua
Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer,
Georg Heigold,
Sylvain Gelly,
et al.
An image is worth 16x16 words:
Transformers
for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020.
Vitaly Feldman and Chiyuan Zhang. What neural net-
works memorize and why: Discovering the long tail
via inﬂuence estimation. In H. Larochelle, M. Ran-
zato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
Advances in Neural Information Processing Systems,
volume 33, pp. 2881–2891. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/pap
er/2020/file/1e14bfe2714193e7af5abc64ecbd6b4
6-Paper.pdf.
Stanislav
Fort,
Huiyi
Hu,
and
Balaji
Lakshmi-
narayanan. Deep ensembles: A loss landscape per-
spective. arXiv preprint arXiv:1912.02757, 2019.
Yoav Freund and Robert E. Schapire.
A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. In EuroCOLT, 1995.
Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and
James Zou.
Word embeddings quantify 100 years
of gender and ethnic stereotypes. Proceedings of the
National Academy of Sciences, 115, 11 2017.
doi:
10.1073/pnas.1720347115.
Xavier Glorot and Yoshua Bengio. Understanding the
diﬃculty of training deep feedforward neural net-
works.
In Yee Whye Teh and Mike Titterington
(eds.), Proceedings of the Thirteenth International
Conference on Artiﬁcial Intelligence and Statistics,
volume 9 of Proceedings of Machine Learning Re-
search, pp. 249–256, Chia Laguna Resort, Sardinia,
Italy, 13–15 May 2010. PMLR. URL http://procee
dings.mlr.press/v9/glorot10a.html.
12

Nina Grgić-Hlača, Muhammad Bilal Zafar, Krishna P.
Gummadi, and Adrian Weller.
On fairness, diver-
sity and randomness in algorithmic decision making,
2017. URL https://arxiv.org/abs/1706.10208.
Neha Gupta, Jamie Smith, Ben Adlam, and Zelda Ma-
riet. Ensembling over classiﬁers: a bias-variance per-
spective. arXiv preprint arXiv:2206.10566, 2022.
Tatsunori Hashimoto,
Megha Srivastava,
Hongseok
Namkoong, and Percy Liang. Fairness without de-
mographics in repeated loss minimization.
In Jen-
nifer Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learn-
ing, volume 80 of Proceedings of Machine Learning
Research, pp. 1929–1938, Stockholmsmässan, Stock-
holm Sweden, 10–15 Jul 2018. PMLR. URL http://
proceedings.mlr.press/v80/hashimoto18a.html.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition. In
CVPR, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun.
Deep residual learning for image recognition.
In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR, 2016b.
Dan Hendrycks and Thomas G Dietterich.
Bench-
marking neural network robustness to common cor-
ruptions and surface variations.
arXiv preprint
arXiv:1807.01697, 2018.
Alex Hernández-García and Peter König. Further ad-
vantages of data augmentation on convolutional neu-
ral networks. Lecture Notes in Computer Science, pp.
95–103, 2018. ISSN 1611-3349. doi: 10.1007/978-3-
030-01418-6_10. URL http://dx.doi.org/10.10
07/978-3-030-01418-6_10.
Geoﬀrey Hinton, Li Deng, Dong Yu, George E Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N
Sainath, et al.
Deep neural networks for acoustic
modeling in speech recognition: The shared views of
four research groups. IEEE Signal processing maga-
zine, 29(6):82–97, 2012.
Sara Hooker, Aaron Courville, Gregory Clark, Yann
Dauphin, and Andrea Frome. What Do Compressed
Deep Neural Networks Forget?
arXiv e-prints, art.
arXiv:1911.05248, November 2019.
Jeremy Howard and Sebastian Ruder. Universal lan-
guage model ﬁne-tuning for text classiﬁcation. arXiv
preprint arXiv:1801.06146, 2018.
Siddhartha Jain, Ge Liu, Jonas Mueller, and David Gif-
ford. Maximizing overall diversity for improved uncer-
tainty estimates in deep ensembles. In Proceedings of
the AAAI Conference on Artiﬁcial Intelligence, vol-
ume 34, pp. 4264–4271, 2020.
Diederik P Kingma and Jimmy Ba.
Adam:
A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Jon M. Kleinberg, Sendhil Mullainathan, and Manish
Raghavan. Inherent trade-oﬀs in the fair determina-
tion of risk scores. CoRR, abs/1609.05807, 2016. URL
http://arxiv.org/abs/1609.05807.
Alex Krizhevsky, Geoﬀrey Hinton, et al. Learning mul-
tiple layers of features from tiny images. 2009.
Jan Kukačka, Vladimir Golkov, and Daniel Cremers.
Regularization for deep learning: A taxonomy, 2017.
Balaji
Lakshminarayanan,
Alexander
Pritzel,
and
Charles Blundell. Simple and Scalable Predictive Un-
certainty Estimation using Deep Ensembles.
arXiv
e-prints, art. arXiv:1612.01474, Dec 2016.
Stefan Lee, Senthil Purushwalkam, Michael Cogswell,
David J. Crandall, and Dhruv Batra. Why M heads
are better than one: Training a diverse ensemble of
deep networks. CoRR, abs/1511.06314, 2015. URL
http://arxiv.org/abs/1511.06314.
Ilya Loshchilov and Frank Hutter.
Sgdr: Stochastic
gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983, 2016.
Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. arXiv preprint arXiv:1711.05101,
2017.
Pranava Madhyastha and Rishabh Jain. On model sta-
bility as a function of random seed. arXiv preprint
arXiv:1909.10447, 2019.
13

Mahdi Milani Fard, Quentin Cormier, Kevin Canini,
and Maya Gupta. Launch and iterate: Reducing pre-
diction churn. In D. Lee, M. Sugiyama, U. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural
Information Processing Systems, volume 29. Curran
Associates, Inc., 2016. URL https://proceedings.
neurips.cc/paper/2016/file/dc5c768b5dc76a084
531934b34601977-Paper.pdf.
Prabhat Nagarajan, Garrett Warnell, and Peter Stone.
The impact of nondeterminism on reproducibility in
deep reinforcement learning.
In Reproducibility in
ML Workshop at the 35th International Conference
on Machine Learning, ICML, 2018.
Kelechi Ogueji, Orevaoghene Ahia, Gbemileke Onilude,
Sebastian
Gehrmann,
Sara
Hooker,
and
Julia
Kreutzer.
Intriguing properties of compression on
multilingual models, 2022.
URL https://arxiv.
org/abs/2211.02738.
D. Opitz and R. Maclin. Popular ensemble methods:
An empirical study. Journal of Artiﬁcial Intelligence
Research, 11:169–198, aug 1999. doi: 10.1613/jair.6
14. URL https://doi.org/10.1613%2Fjair.614.
Shangshu Qian, Viet Hung Pham, Thibaud Lutellier,
Zeou Hu, Jungwon Kim, Lin Tan, Yaoliang Yu, Jia-
hao Chen, and Sameena Shah. Are my deep learning
systems fair? an empirical study of ﬁxed-seed train-
ing. Advances in Neural Information Processing Sys-
tems, 34:30211–30227, 2021.
Alexandre
Rame,
Matthieu
Kirchmeyer,
Thibaud
Rahier, Alain Rakotomamonjy, Patrick Gallinari,
and Matthieu Cord.
Diverse weight averaging for
out-of-distribution generalization.
arXiv preprint
arXiv:2205.09739, 2022.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al.
Imagenet large scale visual recognition challenge. In
IJCV, 2015.
Christopher J. Shallue, Jaehoon Lee, Joseph Antognini,
Jascha Sohl-Dickstein, Roy Frostig, and George E.
Dahl. Measuring the eﬀects of data parallelism on
neural network training, 2019.
Gil I. Shamir and Lorenzo Coviello. Anti-distillation:
Improving reproducibility of deep networks. CoRR,
abs/2010.09923, 2020. URL https://arxiv.org/ab
s/2010.09923.
Shreya Shankar, Yoni Halpern, Eric Breck, James At-
wood, Jimbo Wilson, and D Sculley. No classiﬁcation
without representation: Assessing geodiversity issues
in open data sets for the developing world.
arXiv
preprint arXiv:1711.08536, 2017.
Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan,
Yiren Zhao, Nicolas Papernot, Murat A. Erdogdu,
and Ross Anderson. Manipulating sgd with data or-
dering attacks, 2021.
Karen Simonyan and Andrew Zisserman.
Very deep
convolutional networks for large-scale image recogni-
tion. arXiv preprint arXiv:1409.1556, 2014.
Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying,
and Quoc V. Le. Don’t decay the learning rate, in-
crease the batch size, 2018.
Robert R. Snapp and Gil I. Shamir.
Synthesiz-
ing irreproducibility in deep networks.
CoRR,
abs/2102.10696, 2021. URL https://arxiv.org/
abs/2102.10696.
Asa Cooper Stickland and Iain Murray.
Diverse en-
sembles improve calibration. CoRR, abs/2007.04206,
2020. URL https://arxiv.org/abs/2007.04206.
Cecilia Summers and Michael J Dinneen. Nondetermin-
ism and instability in neural network optimization. In
International Conference on Machine Learning, pp.
9913–9922. PMLR, 2021.
Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov,
Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner,
Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob
Uszkoreit, et al. Mlp-mixer: An all-mlp architecture
for vision. Advances in Neural Information Processing
Systems, 34:24261–24272, 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin.
Attention is All you
Need.
In Advances in Neural Information Process-
ing Systems 30: Annual Conference on Neural Infor-
mation Processing Systems 2017, 4-9 December 2017,
Long Beach, CA, USA, pp. 6000–6010, 2017.
14

Xiaofang Wang, Dan Kondratyuk, Kris M. Kitani, Yair
Movshovitz-Attias, and Elad Eban.
Multiple net-
works are more eﬃcient than one: Fast and accu-
rate models via ensembles and cascades.
CoRR,
abs/2012.01988, 2020. URL https://arxiv.org/
abs/2012.01988.
Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,
Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-
cos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,
Simon Kornblith, et al.
Model soups:
averaging
weights of multiple ﬁne-tuned models improves ac-
curacy without increasing inference time. In Interna-
tional Conference on Machine Learning, pp. 23965–
23998. PMLR, 2022.
Guoxuan Xia and Christos-Savvas Bouganis.
On
the
usefulness
of
deep
ensemble
diversity
for
out-of-distribution
detection.
arXiv
preprint
arXiv:2207.07517, 2022.
Sangdoo
Yun,
Dongyoon
Han,
Seong
Joon
Oh,
Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong clas-
siﬁers with localizable features. In Proceedings of the
IEEE/CVF international conference on computer vi-
sion, pp. 6023–6032, 2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez
Rodriguez, and Krishna P. Gummadi. Fairness con-
straints:
Mechanisms for fair classiﬁcation, 2015.
URL https://arxiv.org/abs/1507.05259.
Sheheryar Zaidi, Arber Zela, Thomas Elsken, Chris C
Holmes, Frank Hutter, and Yee Teh. Neural ensemble
search for uncertainty estimation and dataset shift.
Advances in Neural Information Processing Systems,
34:7898–7911, 2021.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk min-
imization. arXiv preprint arXiv:1710.09412, 2017.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang. Men also like shopping:
Reducing gender bias ampliﬁcation using corpus-level
constraints. In Proceedings of the 2017 Conference on
Empirical Methods in Natural Language Processing,
September 2017.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-
donez, and Kai-Wei Chang.
Gender bias in coref-
erence resolution: Evaluation and debiasing meth-
ods.
In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 2 (Short Papers), pp. 15–20, New Or-
leans, Louisiana, June 2018. Association for Compu-
tational Linguistics.
doi: 10.18653/v1/N18-2003.
URL https://aclanthology.org/N18-2003.
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li,
and Yi Yang. Random erasing data augmentation.
In Proceedings of the AAAI conference on artiﬁcial
intelligence, volume 34, pp. 13001–13008, 2020.
Donglin Zhuang, Xingyao Zhang, Shuaiwen Song, and
Sara Hooker. Randomness in neural network train-
ing: Characterizing the impact of tooling. In D. Mar-
culescu, Y. Chi, and C. Wu (eds.), Proceedings of Ma-
chine Learning and Systems, volume 4, pp. 316–336,
2022. URL https://proceedings.mlsys.org/pape
r/2022/file/757b505cfd34c64c85ca5b5690ee5293
-Paper.pdf.
15

A
Experimental Setup
A.1
Sampling
Given a pool of M models, for each ensemble size S we sample 100 times with replacement. We then average the
accuracy across the 100 samples plus one base model that is shared across all variants. The results at each S is
reported until an ensemble of M models is reached.
A.2
CIFAR-100 Training
We use the following architectures: Resnet9 (He et al., 2016a), VGG16 (Simonyan & Zisserman, 2014) and MLP-
Mixer (Tolstikhin et al., 2021). We train them as follows :
Resnet-9 We train the model for 24 steps using Stochastic Gradient Descent (SGD). We implemented standard
data augmentation by applying Random Horizontal Flip, Random Translate, and Cutout.
We use a Slanted
Triangular Learning Rate (SLTR) (Howard & Ruder, 2018). The top-1 test set accuracy is 72.24%
Resnet18/34/50 For these 3 Resnet architectures, we train the model for 50 epochs using Stochastic Gradient
Descent (SGD), batch size of 512, momentum=0.9, and weight decay=0.0005. We implemented standard data
augmentation by applying Random Horizontal Flip, Random Crop, Random Aﬃne, and Cutout. We use a combi-
nation of warmup for ﬁrst 5 epoch and cosine annealing for scheduler. The top-1 test set accuracy for Resnet-18 is
73.56%, Resnet-34 is 74.24%, and Resnet-50 is 74.89%
VGG16 We train the model for 130 epochs using Stochastic Gradient Descent (SGD). We implemented standard
data augmentation by applying Random Horizontal Flip, Random Crop, and Random Rotation. We use a combi-
nation of warmup for 1 epoch and a multi-step scheduler with milestones at steps 60 and 120. The top-1 test set
accuracy is 71.23%
MLP-Mixer We train the model for 300 steps using Adaptive Moment Estimation (Adam) (Kingma & Ba, 2014).
We implemented standard data augmentation by applying Random Crop, AutoAugment (CIFAR10 Policy) (Cubuk
et al., 2018), and CutMix (Yun et al., 2019). We use a combination of warmup for ﬁrst 5 epoch and cosine annealing
for scheduler. The top-1 test set accuracy is 60.28%
A.3
TinyImageNet Training
We use the following architectures: Resnets (He et al., 2016a), VGG-16 (Simonyan & Zisserman, 2014) and ViT
(Dosovitskiy et al., 2020). We train them as follows :
Resnets We train 3 diﬀerent architectures from the Resnet family (Resnet18, 34, 50) for 100 steps using Stochastic
Gradient Descent (SGD). We implemented standard data augmentation by applying Random Resized Crop and
Random Horizontal Flip. We use a Slanted Triangular Learning Rate (SLTR) (Howard & Ruder, 2018). The top-1
test set accuracy for Resnet-18 is 49.27%, Resnet-34 is 52.18%, and Resnet-50 is 54.99%
VGG16 We train the model for 100 steps using Stochastic Gradient Descent (SGD). We implemented standard
data augmentation by applying Random Resized Crop and Random Horizontal Flip. We use a Slanted Triangular
Learning Rate (SLTR) (Howard & Ruder, 2018). The top-1 test set accuracy is 60.37%
16

ViT We train the model for 100 steps using Adaptive Moment Estimation with decoupled weight decay (AdamW)
(Loshchilov & Hutter, 2017). We implemented standard data augmentation by applying Random Horizontal Flip,
Random Resized Crop, AutoAugment (Cubuk et al., 2018), Random Erasing (Zhong et al., 2020), Cutmix (Yun
et al., 2019), and Mixup(Zhang et al., 2017). We use a combination of warmup for ﬁrst 10 epoch and cosine
annealing (Loshchilov & Hutter, 2016) for scheduler. The top-1 test set accuracy is 51.21%
17

B
Why do ensembles disproportiontely beneﬁt bottom-k and minority group
performance?
B.1
Can Diﬀerent Sources of Stochasticity Improve Deep Ensemble Fairness?
CIFAR100 Top-K
test accuracy %
Resnet9
0
2
4
6
8
10
12
14
16
18
20
90
91
92
93
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
VGG16
0
2
4
6
8
10
12
14
16
18
20
91
92
93
94
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
MLP-Mixer
0
2
4
6
8
10
12
14
16
18
20
84
85
86
87
88
89
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
CIFAR100 Bottom-K
test accuracy %
Resnet9
0
2
4
6
8
10
12
14
16
18
20
51
52
53
54
55
56
57
58
59
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
VGG16
0
2
4
6
8
10
12
14
16
18
20
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
MLP-Mixer
0
2
4
6
8
10
12
14
16
18
20
33
34
35
36
37
38
39
40
41
42
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
18

TinyImagenet Top-K
test accuracy %
Resnet50
0
2
4
6
8
10
12
14
16
18
20
85
86
87
88
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
Subset TrainData
VGG16
0
2
4
6
8
10
12
14
16
18
20
89
90
91
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
ViT
0
2
4
6
8
10
12
14
16
18
20
84
85
86
87
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
TinyImagenet Bottom-K
test accuracy %
Resnet50
0
2
4
6
8
10
12
14
16
18
20
22
23
24
25
26
27
28
29
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
Subset TrainData
models in ensemble
VGG16
0
2
4
6
8
10
12
14
16
18
20
26
27
28
29
30
31
32
33
34
35
36
37
38
39
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
ViT
0
2
4
6
8
10
12
14
16
18
20
17
18
19
20
21
22
23
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Figure 8: Average Test Accuracy on CIFAR100 and TinyImagenet for Top-K and Bottom-K (K=10) Performing
Classes
C
Experimental Set-up
C.1
Balanced Dataset Sub-Groups
test accuracy %
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
86
88
90
92
94
96
Init. & BatchOrder
BatchOrder
Init.
DA
All Sources
Subset TrainData
K size
(a) Resnet-50 Top-K classes
test accuracy %
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
15
20
25
30
Init. & BatchOrder
BatchOrder
Init.
DA
All Sources
Subset TrainData
K size
(b) Resnet-50 Bottom-K classes
Figure 9: Average Top and Bottom-K accuracy as number of K-classes increase. We observe that DA and Init.
outperforms All Sources baseline performance in Top-K classes, whereas BatchOrder and Init. outperfroms All
Sources on Bottom-K classes. In both top and bottom groups, only the TrainSubsetData variant underperforms
All Sources.
19

D
Can Diﬀerent Sources of Stochasticity Improve Deep Ensemble Fairness?
D.1
Dominant sources of stochasticity
CIFAR100 Accuracy % diﬀerence
% accuracy diﬀ.
Resnet9
1
3
5
7
9
11
13
15
17
19
33.0
33.5
34.0
34.5
35.0
35.5
36.0
36.5
Init. & BatchOrder
Init.
BatchOrder
DA
All Sources
models in ensemble
VGG16
1
3
5
7
9
11
13
15
17
19
34
35
36
37
38
39
40
41
42
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
MLP-Mixer
1
3
5
7
9
11
13
15
17
19
46.5
47.0
47.5
48.0
48.5
49.0
49.5
50.0
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
TinyImagenet Accuracy % diﬀerence
% accuracy diﬀ.
Resnet9
1
3
5
7
9
11
13
15
17
19
63.0
63.5
64.0
64.5
65.0
65.5
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
VGG16
1
3
5
7
9
11
13
15
17
19
52
53
54
55
56
57
58
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
ViT
1
3
5
7
9
11
13
15
17
19
62.75
63.00
63.25
63.50
63.75
64.00
64.25
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Figure 10: Accuracy % diﬀerence between top and bottom 10 classes for Resnet-9, VGG16, and MLPMixer trained
on CIFAR100 and TinyImagenet
20

Batch Sizes
test accuracy %
All-K classes (K=10)
128
512
1024
74
75
76
77
78
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
batch size
Top-K classes (K=10)
128
512
1024
90
91
92
93
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
batch size
Bottom-K classes (K=10)
128
512
1024
52
54
56
58
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
batch size
Resnet Architectures
test accuracy %
All-K classes (K=10)
Resnet9
Resnet18 Resnet34 Resnet50
76
77
78
79
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
network architecture
Top-K classes (K=10)
Resnet9
Resnet18 Resnet34 Resnet50
92
93
94
95
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
network architecture
Bottom-K classes (K=10)
Resnet9
Resnet18 Resnet34 Resnet50
57
58
59
60
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
network architecture
Figure 11: Average Test Accuracy on CIFAR100 as batch and architecture size increases. Batch 512 is default.
Learning Rate
test accuracy %
All-K classes (K=10)
0.05
0.1
0.2
55
56
57
58
59
60
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
learning rate
Top-K classes (K=10)
0.05
0.1
0.2
85
86
87
88
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
learning rate
Bottom-K classes (K=10)
0.05
0.1
0.2
24
26
28
30
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
learning rate
Weight Decay
test accuracy %
All-K classes (K=10)
0.00025
0.0005
0.001
55
56
57
58
59
60
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
weight decay
Top-K classes (K=10)
0.00025
0.0005
0.001
86
87
88
89
90
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
weight decay
Bottom-K classes (K=10)
0.00025
0.0005
0.001
22
24
26
28
30
BatchOrder
Init. & BatchOrder
Init.
DA
All Sources
weight decay
Figure 12: Average Eval Accuracy on TinyImagenet as learning rate and weight decay increases. 0.1 is default
learning rate, and 0.0005 is default weight decay.
21

E
Results and Discussion
E.1
Ensembling provides disproportionate gains to bottom-k and minority classes
Top-K
test accuracy %
Resnet9
0
2
4
6
8
10
12
14
16
18
20
90
91
92
93
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Resnet18
0
2
4
6
8
10
12
14
16
18
20
92
93
94
95
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Resnet34
0
2
4
6
8
10
12
14
16
18
20
92
93
94
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Resnet50
0
2
4
6
8
10
12
14
16
18
20
92
93
94
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Bottom-K
test accuracy %
Resnet9
0
2
4
6
8
10
12
14
16
18
20
51
52
53
54
55
56
57
58
59
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Resnet18
0
2
4
6
8
10
12
14
16
18
20
49
50
51
52
53
54
55
56
57
58
59
60
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Resnet34
0
2
4
6
8
10
12
14
16
18
20
50
51
52
53
54
55
56
57
58
59
60
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Resnet50
0
2
4
6
8
10
12
14
16
18
20
50
51
52
53
54
55
56
57
58
59
60
BatchOrder
Init.
Init. & BatchOrder
DA
All Sources
models in ensemble
Figure 13: Average Test Accuracy on CIFAR100 for Top-K and Bottom-K classes across diﬀerent sizes of Resnets.
F
Ensembling provides disproportionate gains to bottom-k and minority
classes
F.1
Beneﬁts of even larger ensembles
test accuracy %
0
10
20
30
40
50
72
73
74
75
76
77
Models
models in ensemble
(a) Resnet9 on CIFAR100
test accuracy %
0
10
20
30
40
50
71
72
73
74
75
76
77
Models
models in ensemble
(b) VGG16 on CIFAR100
test accuracy %
0
10
20
30
40
50
55
56
57
58
59
60
61
Models
models in ensemble
(c) Resnet50 on TinyImageNet
Figure 14: Average Accuracy per size of ensemble. The average accuracy for each model added is calculated by
averaging 100 random samples from a population of 50 models. We can see that the average accuracy starts to
slowly plateau as the ensemble grows to 50 models.
22

F.2
CIFAR-100
Resent9 20 model ensemble
ensemble/base
Init
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
DA
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
Init & BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
All Sources
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
Resent9 50 model ensemble
ensemble/base
Init
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
BatchOrder
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
DA
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
Init & BatchOrder
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
All Sources
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
VGG16 20 model ensemble
ensemble/base
Init
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
DA
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
Init & BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
All Sources
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
VGG16 50 model ensemble
ensemble/base
Init
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
BatchOrder
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
DA
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
Init & BatchOrder
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
All Sources
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
MLP-Mixer 20 model ensemble
ensemble/base
Init
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
models in ensemble
BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
models in ensemble
DA
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
models in ensemble
Init & BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
models in ensemble
All Sources
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
models in ensemble
Figure 15: Ratio of Top & Bottom K ensemble accuracy for diﬀerent model architectures and ensemble sizes on
CIFAR100
23

F.3
TinyImageNet
Resnet50 20 model ensemble
ensemble/base
Init
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
DA
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Init & BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
All Sources
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Resnet50 30 model ensemble
ensemble/base
Init
0
5
10 15 20 25 30
1.0
1.1
1.2
1.3
1.4
1.5
1.6
BatchOrder
0
5
10 15 20 25 30
1.0
1.1
1.2
1.3
1.4
1.5
1.6
DA
0
5
10 15 20 25 30
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Init & BatchOrder
0
5
10 15 20 25 30
1.0
1.1
1.2
1.3
1.4
1.5
1.6
All Sources
0
5
10 15 20 25 30
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Resnet50 50 model ensemble
ensemble/base
Init
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
1.5
1.6
BatchOrder
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
1.5
1.6
DA
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Init & BatchOrder
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
1.5
1.6
All Sources
0
10
20
30
40
50
1.0
1.1
1.2
1.3
1.4
1.5
1.6
24

Resnet34 20 model ensemble
ensemble/base
Init
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
DA
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Init & BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
All Sources
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
VGG16 20 model ensemble
ensemble/base
Init
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
DA
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
Init & BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
All Sources
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
ViT 20 model ensemble
ensemble/base
Init
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
DA
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
Init & BatchOrder
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
All Sources
0
5
10
15
20
1.0
1.1
1.2
1.3
1.4
1.5
1.6
models in ensemble
Figure 16: Ratio of Top & Bottom K ensemble accuracy for diﬀerent model architectures and ensemble sizes on
TinyImageNet
G
FAIR Ensemble: Improved Robustness
G.1
Data Corruption
0
2
4
6
8
10
12
14
16
18
20
0%
10%
20%
30%
40%
50%
Relative Accuracy %
Top-10
Bottom-10
models in ensemble
(a) Resnet-9
0
2
4
6
8
10
12
14
16
18
20
0%
10%
20%
30%
40%
50%
Relative Accuracy %
Top-10
Bottom-10
models in ensemble
(b) VGG-16
0
2
4
6
8
10
12
14
16
18
20
0%
10%
20%
30%
40%
50%
Relative Accuracy %
Top-10
Bottom-10
models in ensemble
(c) MLP-Mixer
Figure 17: Performance on CIFAR-100 Corrupt based on Severity Levels.
25

H
Results and Discussion
H.1
CIFAR100
Table 3: Top-10 and Bottom-10 class names for CIFAR100. The classes are from the averaged test accuracies from
the 20-model ensembles.
Resnet9
Resnet18
Resnet34
Resnet50
VGG16
MLP-Mixer
Top-10
wardrobe
skunk
skunk
orange
road
wardrobe
motorcycle
orange
road
wardrobe
wardrobe
motorcycle
orange
motorcycle
orange
motorcycle
sunﬂower
orange
skunk
road
sunﬂower
skunk
motorcycle
sunﬂower
road
wardrobe
motorcycle
road
skyscraper
road
chimpanzee
palm_tree
wardrobe
sunﬂower
skunk
skyscraper
sunﬂower
chimpanzee
palm_tree
chimpanzee
palm_tree
keyboard
orchid
sunﬂower
pickup_truck
palm_tree
orange
palm_tree
mountain
tractor
aquarium_ﬁsh
aquarium_ﬁsh
chair
plain
apple
skyscraper
skyscraper
lawn_mower
chimpanzee
skunk
Bottom-10
man
mouse
shark
girl
possum
mouse
shark
bear
possum
lizard
crocodile
bowl
lizard
shark
crocodile
possum
girl
woman
bowl
girl
lizard
maple_tree
shark
girl
possum
lizard
girl
bear
bear
squirrel
shrew
man
man
otter
lizard
possum
seal
otter
bowl
bowl
seal
lizard
girl
seal
otter
man
boy
boy
otter
bowl
seal
boy
otter
otter
boy
boy
boy
seal
man
seal
26

H.2
TinyImageNet
Table 4: Top-10 and Bottom-10 wnid names for TinyImagenet. The names are from the averaged test accuracies
from the 20-model ensembles.
Resnet9
Resnet18
Resnet34
Resnet50
VGG16
ViT
Top-10
n02791270
n02791270
n02791270
n02791270
n02791270
n07875152
n02509815
n02509815
n02509815
n02509815
n03042490
n03814639
n03976657
n02906734
n02906734
n02906734
n02509815
n03983396
n02124075
n03042490
n03814639
n03042490
n03814639
n03042490
n03814639
n03814639
n01950731
n01950731
n02906734
n02823428
n03089624
n03976657
n03599486
n04067472
n01950731
n03599486
n03983396
n01950731
n03042490
n03599486
n04398044
n02509815
n02002724
n04560804
n03976657
n03976657
n02124075
n02791270
n03126707
n03599486
n04067472
n07579787
n03089624
n03126707
n03447447
n02002724
n03126707
n03126707
n04067472
n02906734
Bottom-10
n02437312
n04532670
n03160309
n03544143
n02085620
n02927161
n04070727
n03544143
n01945685
n03617480
n04417672
n03544143
n02268443
n04486054
n04417672
n04070727
n02268443
n04070727
n01945685
n02268443
n04532670
n03804744
n04486054
n01641577
n02226429
n03160309
n03617480
n03160309
n01945685
n02094433
n02233338
n03617480
n01855672
n01945685
n02094433
n02480495
n02480495
n01855672
n03804744
n02268443
n04070727
n02410509
n02410509
n02480495
n02480495
n02480495
n02480495
n04532670
n03617480
n02123394
n02123394
n02123394
n02410509
n02950826
n02123394
n02410509
n02410509
n02410509
n02123394
n02123394
27

