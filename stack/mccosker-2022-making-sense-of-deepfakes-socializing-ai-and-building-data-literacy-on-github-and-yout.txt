https://doi.org/10.1177/14614448221093943
new media & society
﻿1­–18
© The Author(s) 2022
Article reuse guidelines:  
sagepub.com/journals-permissions
DOI: 10.1177/14614448221093943
journals.sagepub.com/home/nms
Making sense of deepfakes: 
Socializing AI and building  
data literacy on GitHub  
and YouTube
Anthony McCosker
Swinburne University of Technology, Australia
Abstract
As a form of synthetic media built on the Internet’s extensive visual datasets with 
evolving machine learning techniques, deepfakes raise the specter of new types of 
informational harms and possibilities for image-based abuse. There are calls for three 
types of defensive response: regulation, technical controls, and improved digital or media 
literacy. Each is problematic by itself. This article asks what kind of literacy can address 
deepfake harms, proposing an artificial intelligence (AI) and data literacy framework to 
explore the potential for social learning with deepfakes and identify sites and methods 
for intervening in their cultures of production. The article applies contextual qualitative 
content analysis to explore the most popular GitHub repositories and YouTube 
accounts teaching “how to deepfake.” The analysis shows that these sites contribute 
to socializing AI and establishing cultures of social learning, offering potential sites of 
intervention and pointing to new methods for addressing AI and data harms.
Keywords
AI, automated media, data literacy, deepfakes, GitHub, social learning, YouTube
Introduction
Deepfakes are a form of synthetic media built on the Internet’s extensive visual datasets with 
ever-evolving machine learning techniques. They raise the specter of new types of informa-
tional harms and possibilities for image-based abuse, especially in their historical origins in 
porn production cultures (Winter and Salter, 2020). As a result, there are calls for three types 
Corresponding author:
Anthony McCosker, Swinburne University of Technology, Melbourne, VIC 3122, Australia. 
Email: amccosker@swin.edu.au
1093943 NMS0010.1177/14614448221093943new media & societyMcCosker
research-article2022
Article

2	
new media & society 00(0)
of defensive response: regulation, technical controls, and improved digital or media literacy. 
Each is problematic by itself and reflects gaps in understanding the situated socio-technical 
and everyday contexts of data production, use, and manipulation. As with other harms asso-
ciated with datafication and artificial intelligence (AI), the question of “what is to be done?” 
is not so easy to answer (Burgess et al., 2022: 146). This article focuses on educational and 
social learning responses, asking what kind of AI and data literacy might make a difference 
in addressing deepfake harms. Rather than applying individual or behavior change models 
of literacy, I explore the new forms of social learning that deepfakes are implicated in to 
identify sites and methods for intervening in their cultures of production. This shows the 
work of deepfakes in socializing AI and establishing new cultures and touch points for pub-
lic understanding of machine learning, offering potential sites for intervention and new lit-
eracy-based methods for addressing AI and data harms.
After engaging in existing debates and research examining deepfakes, I use contex-
tual qualitative content analysis to analyze the most popular GitHub repositories and 
YouTube accounts teaching “how to deepfake.” Those developing software code and 
interfaces, as well as those constructing detection and disruption systems and the “devel-
oper ed” influencers (or educational YouTube personalities and Channels) act to social-
ize AI through README files, training datasets, video explainers, and public education. 
The analysis contributes to critical scholarship calling for cultural and computational 
approaches to scrutinizing AI and algorithmic systems (Bucher, 2018; Carah and Angus, 
2018), and new forms of educational or critical literacy-based action. I argue that much 
can be achieved by drawing lessons from those generating deepfakes to better skill up 
those who will encounter them. Among other responses, deepfake repositories and 
explainers can be shaped to act as “objects-to-interpret-with” (Lewis and Stoyanovich, 
2021) in the informal social learning of AI models and datasets. To this end, the article 
develops an approach to AI and data literacies that addresses the “cultures of learning” 
surrounding deepfakes, drawing on approaches to critical data literacies (D’Ignazio and 
Bhargava, 2016; McCosker, 2017; Carmi et al., 2020; Pangrazio and Selwyn, 2019).
Deepfakes have become part of contemporary “everyday data cultures” (Burgess 
et al., 2022) that implicate platforms, code, and AI models in cultures of data production, 
cultivation, and use. As sites of struggle, they offer the chance to understand how AI 
systems and the visual data that drive them are “encountered, experienced, exploited and 
resisted” by ordinary Internet users (Albury et al., 2017), alongside developers, data 
scientists, and aspiring machine learning engineers. In this way, participation in deepfak-
ery can also, potentially, be used to model and improve inclusive and accountable AI 
practices. Approaches to improving AI and data literacies can target and seek to address 
the root issue defining the emergence of deepfakes: the proliferation of digital video as 
visual data and their associated methods of data mining and manipulation or transforma-
tion in the form of automated media (McCosker and Wilken, 2020).
While deepfakes can create spectacular social media events, they also exemplify the 
everyday activities and knowledge-building that shape the way AI is beginning to be 
circulated, encountered, and built in ordinary rather than just corporate technology con-
texts. There is an opportunity to learn from and with deepfakes (and to intervene) through 
the social learning (Stilgoe, 2018) that takes place on code-sharing and video-sharing 
platforms like GitHub and YouTube. These are contemporary sites of struggle in the 

McCosker	
3
enactment of digital and data citizenship (Carmi et al., 2020; McCosker et al., 2016). 
Despite their problematic uses and clear harms, deepfakes can become a site for scruti-
nizing and socializing generative models of synthetic media and building knowledge 
about the manipulability of visual data. Highlighting literacy-enabling practices among 
those who are “learning to deepfake,” I show that a literacies approach can be effective 
if it capitalizes on and extends the capacity to learn informally and socially with deep-
fakes and similar synthetic and automated media, embedding interpretability, scrutiny, 
and responsibility in the process.
Technocultural trajectories: image-based abuse, 
disinformation, and memetics
It is clear from existing research that deepfakes have contributed to a crisis in media 
cultures of trust (Chesney and Citron, 2019). Emerging scholarship and popular responses 
push for a combination of technical, regulatory, and educational forms of redress to 
reduce their risk and quell their potential harms (Albahar and Almalki, 2019; Delfino, 
2019; Karnouskos, 2020; Pishori et  al., 2020; Westerlund, 2019; Yeh et  al., 2020). 
However, there is little to guide regulation or suggest how education and literacy can 
help. Umbrella terms like media or digital literacy are often simply equated with aware-
ness and understanding, or more disparagingly with skepticism and doubt (boyd, 2018). 
So far, these approaches (technical, regulatory, and educational) miss the way deepfakes 
operate as a site for the contestation and socialization of new forms of automated cultural 
production. Reaching back to the early work of Richard Hoggart (1957), cultural studies 
approaches have shone a spotlight on the multi-directional and situated character of lit-
eracies in the development of new media forms and modes of cultural self-awareness and 
agency. There is urgent, ongoing work to do in this vein as AI and automated cultural 
production become embedded in popular culture and social media.
Technically, deepfakes describe various forms of synthetic media but primarily refer 
to the convincing replacement of faces and voice in digital videos. This is made possible 
by developments in deep learning systems and the availability of extensive video data-
sets used to “train” generative learning models and to produce synthesized outputs. The 
problems of accountability with deepfakes stem from their underlying technology. They 
apply convolutional neural networks (CNNs) and generative adversarial networks 
(GANs) and similar techniques for automating image classification and transformation. 
Introduced in 2014, GANs are deep neural net architectures in which two deep learning 
models are pitted against one another (Goodfellow et al., 2014). A generator network 
creates new data instances, usually based on set parameters and labeled or known train-
ing data, while another, the discriminator, evaluates them, attempting to identify anoma-
lies, in turn pushing the generator to create more accurate outputs to “fool” the 
discriminator. As they generate realistic fakes, synthesizing elements of moving images 
sliced up into thousands of still frames, GAN systems exceed their own and other deep 
neural net systems’ ability to detect their inauthenticity (Mirsky and Lee, 2020; Yeh 
et al., 2020). The visual effects are model- and data-driven (Nguyen et al., 2019). The 
logic of automation at play here is underpinned by easy access to extensive video data 
flows combined with established training datasets.

4	
new media & society 00(0)
The term deepfake entered popular discourse in 2017 through the Reddit user /u/
deepfakes, creator of the subsequently banned subreddit /r/deepfakes devoted to synthe-
sized porn videos, and the widely replicated and forked open-source GitHub code reposi-
tory for the deepfake system. Their origins in non-consensual porn (Winter and Salter, 
2020) have meant that deepfakes emerged as acts of harmful digital citizenship, posing 
another challenge to the general willingness to “lean in” to AI technologies. In this 
“macro context of gender inequality” (Öhman, 2019; Winter and Salter, 2020), toxic 
“geek masculinity” (Newton and Stanfill, 2019) has been prevalent, resulting in contest 
over control and consent in personal image use and associated image-based abuse. While 
fake porn is not new, the extensive use of deepfakes for these purposes exposes the 
misogynistic technocultural foundations of new AI developments (Burkell and Gosse, 
2019). They can be thought of as provocations like other disruptions to public communi-
cation and representational practices, aligning them with mis- and mal-information, and 
in some ways trolling and hate speech (McCosker and Wilken, 2014; Isin and Ruppert, 
2020). And in this way, deepfakes have a trajectory that is decidedly “technocultural” 
(Penley and Ross, 1991).
The implications for gender inequality and abuse have driven calls for legal redress 
(e.g. Delfino, 2019), but it is their perceived threat to democracy through political decep-
tion and their impact on trust in media institutions more broadly that have dominated 
(Chesney and Citron, 2019; Gosse and Burkell, 2020). Deepfake videos join other politi-
cal interventions and abuses of trust such as the use of social media bots and other forms 
of automated and invasive intervention into digital media environments for political or 
commercial gain (Bimber and Gil de Zúñiga, 2020; Diakopoulos and Johnson, 2020; 
Vaccari and Chadwick, 2020). But as Paris and Donovan (2019) have argued, “the ‘truth’ 
of audiovisual content has never been stable—truth is socially, politically, and culturally 
determined.” Moreover, the dominant focus on political misinformation in the public eye 
overshadows “the harm caused by sexual deepfakes, and thus the harm to the women in 
such videos,” rendering sexual deepfakes as secondary, subordinating “misogynistic 
harms to political harms” (Gosse and Burkell, 2020: 500).
Despite these potential harms and increasing calls to ban deepfakes from social media 
platforms (Bimber and Gil de Zúñiga, 2020), it is unlikely that they will disappear. 
Celebrity deepfakes shared through social media platforms are driven by the distinctive 
logic of memes and remix (Popova, 2019). Many are circulated on Reddit via the /r/
SFWdeepfakes subreddit (“safe for work,” to distinguish it from the original and subse-
quently banned /r/deepfakes porn-focused subreddit), along with YouTube, TikTok, and 
Twitter. In these contexts, deepfakes take on what Milner (2016) identifies as the five 
fundamental logics of memetic media: multimodality (mix of video, audio, and text), 
reappropriation (cultural genre swapping enacted through face or speech swapping), 
resonance (their visceral impactful meaning created through juxtaposition, dissonance, 
and recognition), collectivism (the social learning that comes from producing and shar-
ing in content communities), and their spread through those communities and platforms. 
The cultural logic of memes helps to contextualize both the model of cultural production 
that deepfakes are embedded within and the way they enact forms of “social learning.”
While literacy and education are seen as mitigating responses (Delfino, 2019; 
Karnouskos, 2020), there is little work detailing the sites and methods for interventions 

McCosker	
5
that are able to shift harmful use toward production cultures of care, accountability, 
and responsible learning with AI. In major reports on the imminent threat of deepfakes, 
literacy is delegated to humans who confront or consume them, in a “behavior change” 
model that is suggestive but says nothing about how it is to be achieved (e.g. Smith and 
Mansted, 2020). Emerging literature exploring new forms of social learning from a 
literacy perspective point toward mechanisms and sites for intervention.
Building AI and data literacies
Every development in media format and communication technology affects vernacular 
and expert literacies. In Hoggart’s (1957) early cultural studies work, as in Street’s 
(1984) new literacy and Freire’s (1968) critical pedagogy, literacies are at the coalface 
of individual empowerment and collective political struggle. One of Hoggart’s lessons 
in his Uses of Literacy was that despite the assumed effect of the new mass media 
publicists, working-class life proceeded in its rich traditions of language and speech. 
The ecosystem of the publicists, the pulp novels, magazines, and popular music and 
film transformed what it meant to be literate. “Reading” popular culture was not just a 
matter of taste and education but related to new media technologies, social structures, 
and media practices. Media literacy emerged out of the subsequent push in education 
to equip students and publics to respond critically and creatively to the mass broadcast 
media environment (Buckingham, 2003; Sefton-Green, 1998), as did information lit-
eracy in the context of increasingly complex library, information search systems, and 
the Internet (Koltay, 2011).
These approaches have positioned forms of digital literacy as a toolkit for navigating 
new media environments, a set of individual competencies social practices and concep-
tions of reading and writing (Barton et al., 2000; Pangrazio and Selwyn, 2019; Street, 
1984). But rather than static competencies that an individual develops or possesses, lit-
eracies are dynamic and collective. Literacy practices are situated historically, politi-
cally, and culturally (Barton et al., 2000). They are relational and social, and fundamental 
to driving change in new cultural and technological milieus (Knobel and Lankshear, 
2008; Pangrazio, 2016).
This tradition can illuminate the technocultural shifts associated with new media 
forms. However, many proposed strategies for addressing deepfakes through education 
or “more literacy” place an untenable emphasis on human vision and perception, encour-
aging techniques for identifying small visual cues that reveal media synthesis (eSafety 
Commissioner, 2020; Walker, 2019), or they take a more global approach to verifying the 
source of deepfakes as visual content within platform contexts, along with responsible 
sharing practices. It is easy to forget that visual literacy, as Brumberger (2017) has 
shown, is not given or natural, but rather situated and learned, including through social 
interaction and media practices. As visual discrepancies in synthetic media become more 
difficult to “see,” these approaches need to be extended to account for the data inputs, 
machine learning models, and outputs.
Recent work aiming to increase awareness and agency in response to algorithms, 
automated systems and the big datasets that underpin them, helps to further evolve con-
cepts of digital and data literacy (Gran et al., 2020; Hargittai et al., 2020). Cultures of 

6	
new media & society 00(0)
deepfake production involve some degree of “coding literacy” (Vee, 2017), built upon 
the application of machine learning models toward the extraction, treatment, and manip-
ulation of visual data. The notion of AI and data literacy I apply in the following analysis 
draws on this trajectory of media, digital, data, and algorithmic literacy to establish a 
way of assessing and ultimately intervening in the cultures of social learning that are 
driving deepfake production. To investigate how these practices are emerging, I apply 
this framework to GitHub repositories and YouTube tutorials, asking: to what extent do 
the environments for sharing and developing deepfake code and datasets support new AI 
and data literacies?
Approach and methods: assessing AI and data literacies
I have followed open-source software development communities and code reposito-
ries since the emergence of deepfakes on Reddit and GitHub in 2017, observing them 
for their role in iterating machine vision technology and their potential to build, sus-
tain, and widen AI and data literacies. This approach aligns with Winter and Salter’s 
(2020) analysis of code repositories on GitHub and Reddit discussions, which exam-
ines the early deepfake developer communities as they emerged in 2017 and 2018. I 
draw case studies from 791 GitHub repositories available through a search of “deep-
fake” as of October 2020. Considering the known gender disparity, bias, and discrimi-
nation on GitHub (Winter and Salter, 2020), the analysis examines code purpose, the 
critical or ethical “stance” taken (Zappavigna, 2012: 51) in README.md instruc-
tional content, and the number of stars as an indicator of attention within the 
community.
Alongside cases of select GitHub repositories, I examine a selection of 14 popular 
YouTube developer and AI education channels and their tutorial videos on deepfakes, 
drawn from the top 50 deepfake videos (or videos about deepfakes) by number of views 
during October 2020. While there are many YouTube videos presenting deepfake 
explainers and news, and deepfake video outputs (celebrity faceswaps in movies, televi-
sion, music, and so on) with tens of millions of views, tutorial videos and education-
oriented YouTubers work alongside the GitHub community in socializing AI. Together 
they offer significant insights into the cultures of learning AI with and through deepfakes 
and test the scope of open-source AI and data literacy development.
Table 1 sets out a framework for identifying and understanding the core competen-
cies, attributes, and the broader dimensions of AI and data literacies. The table synthe-
sizes and extends approaches to critical and personal data literacies to also account for AI 
applications (D’Ignazio and Bhargava, 2016; McCosker, 2017; Carmi et  al., 2020; 
Fotopoulou, 2020; Pangrazio and Selwyn, 2019), algorithm skills (Gran et al., 2020; 
Hargittai et  al., 2020), data visualization (Kennedy and Moss, 2015; McCosker and 
Wilken, 2014), civic data citizenship (Carmi et al., 2020), and “data infrastructure liter-
acy” (Gray et al., 2018). At stake here is the accessibility of AI techniques and the treat-
ment of big datasets and personal data in a rapidly changing digital media landscape. 
New AI and data literacies describe the asymmetrical relationships affecting who can 
identify, appraise, create, and control automated and AI-based synthetic media produc-
tion and the datasets that underpin them.

McCosker	
7
Table 1 presents a set of common competencies associated with a more dynamic 
range of attributes that can be used as indicators of the qualities of AI and data liter-
acy. The dimensions refer to how the competencies apply to context and indicate the 
scope and social effects of literacy practices. For instance, are the competencies ori-
ented toward individual skillsets or collective practices? Are they inclusive and open 
or exclusive? Are they purely operational or critical in their orientation and effects? 
Ethics and civic responsibility underlie each of the competencies and dimensions to 
some extent, and these dimensions overlap. This framework can be used as a heuristic 
to assess and understand literacy practices in situ and consider the digital-cultural 
environments that sustain and might extend them beyond the narrow technical com-
munities I focus on.
With reference to this framework, the analysis below addresses the research ques-
tions: (1) To what extent are the competencies and attributes of AI and data literacies 
manifest among the loosely connected social learning and coding communities 
involved in making, interrogating, and circulating deepfakes? (2) How are the dimen-
sions of AI and data literacy shaped by these forms of individual and collective 
engagement through deepfakes? Qualitative content analysis (Schreier, 2020) was 
used to address these research questions, offering a close reading of (a) the tools, 
technologies, and spaces for developing deepfake and counter-deepfake code and 
outputs, and (b) the network of key influential actors and code and data sources driv-
ing developments in deepfakes.
Table 1.  Core competencies, exemplary attributes, and dimensions of critical AI and data literacy.
Competencies
Attributes
Dimensions
Read
•  Awareness, understanding, and ability to makes 
sense of data, algorithms, and AI models
Operational/Critical
 ↓↑
Individual/Collective
 ↓↑
Generative/Disruptive
 ↓↑
Exclusive/Inclusive
Write
•  Practical ability to use data processing tools
•  Management of and active intervention in data and 
AI applications for operational purposes
Appraise
•  Civic, critical, and ethical attention to the use of 
data, algorithms, and AI models
•  Ability to evaluate and respond to metrics, 
analytics, and algorithmic systems
•  Reflexivity in data and AI engagement and use
Communicate
•  Ability to make meaning about and with data and 
AI outputs, or produce data visualization and data 
storytelling
Participate
•  Involvement in data and AI governance
•  Openness and inclusion through communities of 
practice
•  Tactical and creative data and AI use with a civic 
disposition
•  Countering misuse, distortions, and rights 
impingement
AI: artificial intelligence.

8	
new media & society 00(0)
Socializing AI through GitHub README files and YouTube 
tutorials
At the time of analysis, there were 791 deepfake GitHub repositories, many of them 
forks or derivatives of a handful of major code repositories. Each establishes building 
blocks for new work in an open collaboration or social coding format (Dabbish et al., 
2012; Mergel, 2015). Just under one-third of the repositories (248) are deepfake detec-
tion software or resources, and 65 of these related to detection challenges, the most 
prominent being the Deepfake Detection Challenge (DFDC) created by Amazon Web 
Services (AWS), Facebook, Microsoft, the Partnership on AI, and academics, which was 
hosted on kaggle.com with US$1m in prize money.
Top active GitHub deepfake repositories by stars include deepfakes/faceswap (32.6K), 
iperov/DeepFaceLab (20.1), alievk/avatarify (9K), AliaksandrSiarohin/first-order-model 
(7.9K), and ondyari/FaceForensics (1.4K). For comparison, pytorch/pytorch, one of the 
top machine learning repositories, has 42.9K stars. GitHub users star repositories for 
three main reasons: to show appreciation for a project, to bookmark a project, and 
because they are using a project (Borges and Valente, 2018). Many repositories connect 
directly with their own YouTube demonstration and tutorial videos, or they underpin the 
multitude of YouTube tutorial videos created by others.
The GitHub repositories selected in Table 2 are some of the most stared repositories 
but are also exemplars of a range of typical repository types. The stance taken through 
README information and contextual materials and interactions through links to exter-
nal sites and forums, along with the functionality of the code or resources, points to vari-
ation in their attention to building AI and data competencies. Some are applications 
intended for wide use (deepfakes/faceswap and iperov/DeepFaceLab), and others are 
educational in focus or offer resources for deeper understanding (llSourcell/deepfakes, 
aerophile/awesome-deepfakes). Repositories focusing on detection and disruption are 
predominantly aligned with university-based research or code competitions (selimsef/
dfdc_deepfake_challenge, desso-oss/DeepFake-Detection, natanielruiz/disrupting-deep-
fakes), but these do not implicitly convey a full range of attributes of ethical judgment 
and responsible practice.
GitHub officially encourages egalitarian values and ethical practice and removed 
what it considered the most harmful deepfake repositories in early 2018, imposing plat-
form or structural controls (Winter and Salter, 2020). However, Winter and Salter (2020) 
show that code depositories and those who use them “do not always uphold these values” 
(p. 389). They point to the explicit attempt to mitigate potential harms through appeals to 
ethical use in the README “manifesto” for the most popular deepfake repository on 
GitHub (deepfake/faceswap). However, they also illustrate the gender imbalance broadly, 
and specifically the ongoing use of deepfakes for image-based abuse, as well as mem-
bers’ connections to misogynistic discussion on Reddit (Winter and Salter, 2020: 389). 
Bias of this sort on GitHub has been shown to inhibit female participation, with signifi-
cant flow on effects for the types of collaboration the platform engenders (Wang et al., 
2018). These findings highlight the contested and varied ethical stances underpinning 
social coding, which also flows through to their inconstant capacity to build certain com-
petencies in AI and data literacy. In other words, many deepfake repositories are open to 

McCosker	
9
misuse or skew use toward harms, even while others work to counter misuse or to build 
ethical awareness and competency. In addition, often missing is a reflexive account of 
training datasets, or accountability for the source and target video datasets used to gener-
ate synthetic media.
As noted, the deepfakes/faceswap repository contains an extensive README “mani-
festo” that establishes guides and resources for understanding and running the code for 
“people interested in generative models,” for “devs” (software developers), for “non-dev 
advanced users,” and for “end users.” It takes an inclusive stance, opening access to AI 
techniques beyond specialists, as “the first AI code that anyone could download, run and 
learn by experimentation.” The README goes on to note that “to us, developers, the 
release of this code opened up a fantastic learning opportunity” and provides a statement 
of ethical use, noting the software is not for “creating inappropriate content,” “changing 
faces without consent or with the intent of hiding its use,” and “any illicit, unethical, or 
questionable purposes.” While these statements are easily ignored, they contribute to a 
form of appraisal and ethical or civic disposition at the point of production. Faceswap 
includes a judicious array of educational resources that do not just target specialist devel-
opers, aiding broader awareness and production competencies along with some degree of 
civic appraisal. These include explainer videos, links to guides and academic papers, and 
a strong community support component through a separate website (https://faceswap.
dev//) with a blog, discord server, downloadable guides, video, and other educational 
content, all supported through a Patreon page.
Like other code repository applications, faceswap enables experimentation and 
engagement with AI techniques and practices and has built a community of practice 
around the tools it makes available. It is, however, “hands off” in supporting or specify-
ing ethical data practices, leaving the process of gathering video and constructing outputs 
to end users with no accountability for where that data come from and ultimately how 
they are used. In the case of the other widely popular repository, DeepFaceLab, a far less 
holistic type of AI and data literacy is proffered. DeepFaceLab is used widely among 
YouTube’s most-viewed deepfake tutorial videos (e.g. “DEEPFAKE Tutorial: A 
Beginners Guide [using DeepFace Lab],” Cinecom.net, 10 December 2019, 765K 
views). Its focus is on simple application and use for image synthesis. Participation in AI 
practice is enabled, but it is an unreflexive participation with little attempt to establish 
ethical boundaries and civic mindedness.
Table 2.  Selection of top GitHub repositories, code purpose, and key competencies enabled.
Profile/Repository
Stars
Code purpose
Key competencies enabled
deepfakes/faceswap
32.2K
Application
Read, write, appraise, participate
iperov/DeepFaceLab
20.2K
Application
Write
llSourcell/deepfakes
805
Education
Read, communicate
aerophile/awesome-deepfakes
779
Resources
Read, communicate
ondyari/FaceForensics
1.4K
Detection
Write, appraise, participate
selimsef/dfdc_deepfake_challenge
250
Detection
Write, appraise, participate
natanielruiz/disrupting-deepfakes
118
Disruption
Read, write, appraise, 
communicate, participate

10	
new media & society 00(0)
Through these relatively accessible tools, deepfake memes have spread with the aid 
of associated YouTube tutorial or how-to videos like face swap memes created with the 
song “Baka Mitai Dame Da Ne” made popular through its inclusion in the karaoke sec-
tions of the video games Yakuza 0 and 5 (see, for example, YouTube video: “How to 
Make the Baka Mitai Dame Da Ne Meme [Complete Tutorial with Templates],” Kapwing 
App, 4 August 2020, 586K views). Establishing and working with image datasets (source 
and target images) is a key component of these tutorials, so they build awareness about 
visual data and skills through frame-by-frame preparation and manipulation. But the 
stance taken in DeepFaceLab’s GitHub README.md supports little more than basic 
production competencies. While an arXiv paper details the construction of the code and 
algorithms and techniques used (Petrov et al., 2020), the repository courts misuse, noting 
its ability to “manipulate politicians’ speech.” As well as providing a range of celebrity 
face datasets and trained models, the README.md file requests users to gather and 
contribute face datasets, without ethical consideration of their origins or consent.
Technical work on detection and disruption should be a main source of efforts to build 
AI and data literacies, but this is often not the case. Developing detection tools can be 
competitive and hence exclusionary, or they are derived from complicated mathematical 
modeling difficult to understand for non-experts. The three repositories and detection or 
disruption projects noted in Table 2—ondyari/FaceForensics, selimsef/dfdc_deepfake_
challenge, natanielruiz/disrupting-deepfakes—present different facets of the knowledge 
and competencies needed to make sense of and respond to potential harms in the produc-
tion of deepfakes. Because they are university-funded or competition-oriented, they 
make their datasets and processes public, and are civic-minded in their stance. In their 
focus on the treatment of datasets, they can build greater awareness of the core elements 
of image manipulation. For instance, the model produced by Ruiz et al. (2020) at Boston 
University (disrupting-deepfakes) uses the public CelebA face dataset consisting of 
200K celebrity images each with 40 attribute annotations. Their method targets the 
manipulation of attributes—smile, hair color or type, eyes, skin tone, and so on that 
allow it to identify and corrupt the outputs of popular deepfake models, aiding model 
interpretability. In doing so, they enable read and appraisal competencies and open the 
potential for wider participation.
There is a lag between detection and disruption techniques and the models used to 
generate deepfakes and synthetic media (Mirsky and Lee, 2020; Yeh et al., 2020), so it 
can seem counter-intuitive to make disruption tools and their code, models, and resources 
open access. Nonetheless, the stance taken in these repositories is collaborative. The 
three listed in Table 2 are examples of projects and published work that enable some 
level of “appraisal” with a clear civic-mindedness in relation to AI code development 
and application. Because they speak more directly to specialist communities in academic 
research or software development, they are not inclusive in enabling “write” competen-
cies but do draw attention to the treatment of datasets and algorithms used for deepfakes 
(“read” and “appraisal” competencies). Their reflexive use and ethical development of 
datasets through models of consent and managed access can enhance data practices 
(Dufour and Gully, 2019; Rössler et al., 2018).
Civic resources are more common in dedicated educational repositories. The aero-
phile/awesome-deepfake-resources repository takes an explicit ethical stance in its 

McCosker	
11
README, aiming “to enhance and promote efforts into research and development and 
not to promote or aid in the creation of nefarious content.” Like others, it links to code, 
explainers, and academic papers. These resources aid public learning around AI, like 
other informal learning sources including the “Towards Data Science” collection on 
Medium.com and free online courses through open learning platforms. Where these 
resources seem to gain traction and attention is through the work of developer influenc-
ers on YouTube where there has always been learning communities.
Technology influencers and YouTube’s how-to-AI tutorials
Video explainers and tutorials by AI and software developer influencers offer an inter-
esting middle ground between developer and data science communities and a broader 
interested public. The llSourcell/deepfakes repository, for example, connects directly 
with YouTube tutorials through the author’s popular channel (Siraj Ravel, 715K sub-
scribers). A self-described “developer and AI literacies influencer” and entrepreneur 
Siraj Ravel exemplifies a new wave of communicators with technical knowledge 
engaging publicly through platforms like YouTube, GitHub, and Medium. The llSour-
cell/deepfakes repository reproduces the instructional material provided by deepfake/
faceswap. In fact, Ravel has been heavily criticized for reproducing others’ code often 
without attribution, sometimes claiming it as his own (see his apology video: “My 
Apology,” 22 December 2019, 218K views). However, his sustained effort in building 
a community around deepfakes and many other AI applications and coding projects is 
literacy-enabling in line with the whole genre of YouTube “how-to” and tutorials 
(Lindgren, 2012; Utz and Wolfers, 2020).
Many GitHub popular repos link to their own or others’ tutorial videos on YouTube. 
Along with discussion forums (including Reddit), blogs, discord servers, and other 
collaborative tools, YouTube features strongly as a space for informal or “folk learn-
ing” practices (Pangrazio and Sefton-Green, 2020) and holds potential for establish-
ing new public pedagogies. These videos show how to prepare video datasets and run 
them through AI code against training data. Once stills are extracted from any source 
video, preparation involves often substantial data cleaning to ensure only faces are 
framed, unobscured, and not incorrectly positioned. These processes highlight the 
data components of machine learning systems and suggest potential methods for 
building better visual data literacies. But this work needs effective intermediaries, 
those who can translate and help cultivate responsible, interpretable AI applications 
and outputs.
YouTube stars and emerging channel presenters offer a mechanism for filling some of 
the AI and data literacy gaps noted earlier. While the many news explainers and technol-
ogy briefs can give some initial insights into what AI techniques are and their potential 
social impact (see, for example, Bloomberg’s “QuickTake” series), channels like those of 
Siraj Ravel, Dev Ed, or MIT researcher Jordan Harrod aim to take people further along 
the competency pathway beyond basic “reading” to understand context, models, and 
mechanics. While tutorial videos (like Kwaping App) encourage and build AI applica-
tion “write” skills, they do so in an operational sense. Nonetheless, they do more for 
learning AI “with” deepfakes than many “explainer” or “developer ed” videos. Where 

12	
new media & society 00(0)
GitHub code sharing can be highly technical and exclusive, YouTube tends toward “edu-
tainment” and step-by-step instruction.
“tl;dr: training data + trial and error”: toward a data 
literacy social learning agenda
The title quote above is from the Deepfake/FaceSwap repository’s README in a sec-
tion “About machine learning.” The section contains two explainer videos to illustrate: 
“How does a computer know how to recognize/shape faces? How does machine learning 
work? What is a neural network?” Alongside these instructional and educational pieces, 
the ironic humor of the simplified application of data to deepfake models is cute but also 
points to some of the potential and limitations of developing AI and data literacy through 
these sites of informal learning. Three insights can be drawn from the analysis presented 
above, illustrating the limitations and troubles embedded in these learning communities 
and sites, as well as their potential for intervening in the socialization of AI.
First, code-sharing and developer communities help shape the dimensions through 
which AI tech and the data that underpin it are accessed and operationalized. The cases 
discussed cover both simple applications and projects seeking to cultivate critical data AI 
mindset (but rarely about the datasets on which the systems are trained or the outputs are 
constructed). They are sometimes uncritically generative of synthesized outputs, and 
sometimes disruptive, protective, or security-oriented. The supporting materials, aca-
demic papers, code-sets, and video explainers can be inclusive or exclusive but not 
always as expected. Those working to produce detectors or disruptors often use highly 
specialized knowledge and language. Developer ed influencers are driven by a desire for 
large audiences, but do not always help to make AI interpretable or generate collective 
knowledge and abilities.
Second, while the GitHub repositories and YouTube tutorials set purely to apply deep-
fake code and algorithms take an ethical stance of sorts, this is rarely in relation to the con-
sensual and ethical use of datasets and the treatment or circulation of outputs. Some try to 
establish ethical or critical practice, but most stop short of interrogating the (potential) out-
puts as part of their technical work. The ability to appraise an AI system involves knowledge 
of the context of its deployment and who is using it, the data inputs, and the model it uses 
according to the Organisation for Economic Co-operation and Development (OECD) 
Framework (OECD.AI, 2022). Civic responsibility does not automatically follow appraisal 
or an operational ability to feed in datasets and implement code to produce AI outputs.
Third, despite the limitations in this shared AI and data literacy work, these contexts 
and sites are shaping public engagement with both the datasets and models that generate 
AI outputs. While the practices and competencies discussed are dynamic and shifting 
alongside other measures of regulation or platform controls (from GitHub, YouTube, and 
Reddit, for instance), most importantly they are changeable. They constitute potential 
sites of intervention in the social learning of responsible AI.
These sites can help in socializing AI and widening AI and data literacy beyond nar-
row technical communities. This is possible where such activity moves beyond literacy 
as “awareness” or behavior change toward crucial practice. Practices that involve “learn-
ing with” deepfakes and the visual datasets that power AI models can open those models 

McCosker	
13
and their inputs and outputs to public scrutiny. Social learning occurs when there is “a 
change in understanding that goes beyond the individual to become situated within wider 
social units or communities of practice through social interactions between actors within 
social networks” (Reed et al., 2010). In the processes of building and applying AI to new 
media production examined above, there is an opportunity to embed these social learning 
sites and practices with greater reflexivity and awareness of the implications of visual 
data manipulation.
Responding to deepfakes
The question of how to detect, ban or regulate, or educate to mitigate the harms of deep-
fakes moves in the wrong direction if it does not address the multiple dimensions of AI 
and data literacies, and the contexts of their development and deployment. Of course, as 
Redden et al. (2020) note, efforts (such as theirs) to define data harms are only a starting 
point. Attention to new data practices and uses through AI models is needed to deepen 
public understandings of harms such as the harmful manipulation, identity theft, and 
trauma associated particularly with non-consensual sexual deepfakes. In addition to reg-
ulatory controls and technical redress, formal and informal educational responses also 
need to evolve. The analysis in this article does not point to easy solutions or suggest that 
the technocultures building and circulating deepfake code, datasets, and outputs on 
GitHub and YouTube are the answer. But rather, these are important sites for widening 
critical, situated literacies, breaking down the key elements of AI and visual data manip-
ulation toward the appraisal and generation of responsible AI and data use. Collective 
effort in characterizing and addressing data and AI harms also needs the support of plat-
forms (such as GitHub and YouTube) and the input of key developer influencers along-
side communities of informal AI and data science learners.
Lewis and Stoyanovich (2021) offer a related exemplary pedagogical approach to 
achieving similar goals of interpretability in AI models through the notion of “objects-to-
interpret-with.” They target formal learning, working with computer and data science 
students, through an approach that can be adapted to address wider informal learning 
practices and contexts. Drawing on meta-cognitive pedagogical traditions, they use the 
notion of “objects-to-interpret-with” to help make sense of “ill-defined information and 
indefinite meanings to achieve deeper learning” (Lewis and Stoyanovich, 2021: 8). To 
improve interpretability, they explore the use of model cards, which communicate bench-
marking and information on intended usage and ethical considerations, along with data 
and fact sheets and the use of labeling in the style of “nutritional labels.” These peda-
gogical tools could be adopted as methods of intervention into the communities experi-
menting with deepfakes and other AI applications on GitHub and YouTube. If datasets, 
models, and outputs are approached or overlaid with these or similar pedagogical strate-
gies, repository video platforms like GitHub and YouTube can feature prominently in 
extending AI and data literacies toward ethical and accountable practice.
In combination with technical and regulatory approaches, literacy can play a more 
prominent role in widening understanding of AI systems and models potentially improv-
ing collective accountability for outputs (see Dan et al., 2021, for an excellent account of 
combined socio-technical, design, and legal responses to deepfakes). However, as 

14	
new media & society 00(0)
Pangrazio and Sefton-Green (2020) argue regarding the utility of data literacy as a 
response to datafication, there is a need for “far more investment and commitment in 
both formal and informal education” (p. 209). The notion of AI and data literacy applied 
in this article is not just about educating people about deepfakes or setting out a critical 
account of their uses, sources, and impact—encouraging and deepening skepticism in 
digital media ecosystems. In addition to knowledge and weariness, it is also about 
expanding the civic stance around digital and visual data capabilities and AI model 
appraisal and use, alongside technical and regulatory methods of assigning and enforcing 
data and AI responsibilities. This is not, ultimately, something that can be imposed on 
developer communities, but can be modeled, for instance, by those participating in those 
spaces or elsewhere in forums that can enable public engagement with machine learning 
and AI models and their data practices.
Conclusion
In this article, I aimed to show the need to multiply the spaces for learning AI with and 
through deepfakes, and what this might look like in the context of code repositories and 
the platforms circulating deepfake outputs alongside how-to tutorials and explainers. 
Deepfakes exemplify a prominent provocation for cultivating and building new digital, 
AI, and data literacies. The framework for AI and data literacies set out in this article 
emphasizes a set of competencies, attributes, and dimensions that together establish 
emerging cultures of data use and AI practice that result in, for instance, both harmful 
and generative media outputs. Observing these practices in situ, through GitHub and 
YouTube sharing cultures, can not only help to both make sense of deepfakes and how 
they come about, but also suggest ways to foster the kind of environments (and actors) 
needed to counter harm and misuse in visual data and AI. The response constructed here 
leans into AI and data literacy as an approach to co-learning with deepfakes, to unpick 
the seams of automated culture through the platforms on which they are socially con-
structed and in relation to the spaces and domains in which they appear.
Acknowledgements
I would like to thank Taylor Hardwick for her research assistance in the scoping phase of this project.
Declaration of conflicting interests
The author(s) declared no potential conflicts of interest with respect to the research, authorship, 
and/or publication of this article.
Funding
The author(s) disclosed receipt of the following financial support for the research, authorship, and/or 
publication of this article: This research has been supported by funding from the Australian Research 
Council Centre of Excellence for Automated Decision Making and Society CE200100005.
ORCID iD
Anthony McCosker 
https://orcid.org/0000-0003-0666-3262

McCosker	
15
References
Albahar M and Almalki J (2019) Deepfakes: threats and countermeasures systematic review. 
Journal of Theoretical and Applied Information Technology 97(22): 3242–3250.
Albury K, Burgess J, Light B, et al. (2017) Data cultures of mobile dating and hook-up apps: 
emerging issues for critical social science research. Big Data and Society 4(2): 1–11.
Barton D, Hamilton M, Ivaniúc R, et  al. (2000) Situated Literacies: Reading and Writing in 
Context. New York: Psychology Press.
Bimber B and Gil de Zúñiga H (2020) The unedited public sphere. New Media & Society 22(4): 
700–715.
Borges H and Valente MT (2018) What’s in a GitHub star? Understanding repository starring 
practices in a social coding platform. Journal of Systems and Software 146: 112–129.
boyd d (2018) You think you want media literacy. . . do you? Medium, 10 March. Available at: 
https://points.datasociety.net/you-think-you-want-media-literacy-do-you-7cad6af18ec2
Brumberger E (2017) The myth of visual literacy and digital natives. In: Messaris P and Humphreys 
L (eds) Digital Media: Transformations in Human Communication. New York: Peter Lang, 
pp. 40–47.
Bucher T (2018) If. . . Then: Algorithmic Power and Politics. Oxford: Oxford University Press.
Buckingham D (2003) Media Education: Literacy, Learning and Contemporary Culture. New 
York: John Wiley & Sons.
Burgess J, Albury K, McCosker A, et al. (2022) Everyday Data Cultures. London: Polity Press.
Burkell J and Gosse C (2019) Nothing new here: emphasizing the social and cultural context of 
deepfakes. First Monday 24(12): 10287.
Carah N and Angus D (2018) Algorithmic brand culture: participatory labour, machine learning 
and branding on social media. Media, Culture & Society 40(2): 178–194.
Carmi E, Yates SJ, Lockley E, et al. (2020) Data citizenship: rethinking data literacy in the 
age of disinformation, misinformation, and malinformation. Internet Policy Review 9(2): 
1–22.
Chesney R and Citron D (2019) Deepfakes and the new disinformation war: the coming age of 
post-truth geopolitics. Foreign Affairs 98: 147.
D’Ignazio C and Bhargava R (2016) DataBasic: design principles, tools and activities for data 
literacy learners. The Journal of Community Informatics 12(3): 83–107.
Dabbish L, Stuart C, Tsay J, et  al. (2012) Social coding in GitHub: transparency and col-
laboration in an open software repository. In: Proceedings of the ACM 2012 conference 
on computer supported cooperative work, Seattle, WA, 11–15 February, pp. 1277–1286. 
New York: ACM.
Dan V, Paris B, Donovan J, et al. (2021) Visual mis-and disinformation, social media, and democ-
racy. Journalism & Mass Communication Quarterly 98(3): 641–664.
Delfino RA (2019) Pornographic deepfakes: the case for federal criminalization of revenge porn’s 
next tragic act. Fordham Law Review 88(3): 887–938.
Diakopoulos N and Johnson D (2020) Anticipating and addressing the ethical implications of 
deepfakes in the context of elections. New Media & Society 23: 2072–2098.
Dufour N and Gully A (2019) Contributing data to deepfake detection research. Google AI Blog, 
24 September. Available at: https://ai.googleblog.com/2019/09/contributing-data-to-deep-
fake-detection.html
eSafety Commissioner (2020) Deepfake Trends and Challenges—Position Statement. Available 
at: https://www.esafety.gov.au/about-us/tech-trends-and-challenges/deepfakes-position-
statement

16	
new media & society 00(0)
Fotopoulou A (2020) Conceptualising critical data literacies for civil society organisations: agency, 
care, and social responsibility. Information, Communication & Society 24: 1640–1657.
Freire P (1968) Pedagogy of the Oppressed. New York: Continuum.
Goodfellow I, Pouget-Abadie J, Mirza M, et al. (2014) Generative adversarial nets. Proceedings in 
Advances in Neural Information Processing Systems 27: 2672–2680.
Gosse C and Burkell J (2020) Politics and porn: how news media characterizes problems presented 
by deepfakes. Critical Studies in Media Communication 37(5): 497–511.
Gran AB, Booth P and Bucher T (2020) To be or not to be algorithm aware: a question of a new 
digital divide? Information, Communication & Society 24: 1779–1796.
Gray J, Gerlitz C and Bounegru L (2018) Data infrastructure literacy. Big Data & Society 5(2): 
1–13.
Hargittai E, Gruber J, Djukaric T, et al. (2020) Black box measures? How to study people’s algo-
rithm skills. Information, Communication & Society 23: 764–775.
Hoggart R (1957) The Uses of Literacy. London: Routledge.
Isin EF and Ruppert ES (2020) Being Digital Citizens. London: Rowman & Littlefield.
Karnouskos S (2020) Artificial intelligence in digital media: the era of deepfakes. IEEE 
Transactions on Technology and Society 1: 138–147.
Kennedy H and Moss G (2015) Known or knowing publics? Social media data mining and the 
question of public agency. Big Data & Society 2(2): 1–11.
Knobel M and Lankshear CJ (eds) (2008) Digital Literacies: Concepts, Policies and Practices. 
New York: Peter Lang.
Koltay T (2011) The media and the literacies: media literacy, information literacy, digital literacy. 
Media, Culture & Society 33(2): 211–221.
Lewis A and Stoyanovich J (2021) Teaching responsible data science: charting new pedagogical 
territory. International Journal of Artificial Intelligence in Education. Epub ahead of print 5 
April. DOI: 10.1007/s40593-021-00241-7.
Lindgren S (2012) “It took me about half an hour, but I did it!” Media circuits and affinity spaces 
around how-to videos on YouTube. European Journal of Communication 27(2): 152–170.
McCosker A (2017) Data literacies for the postdemographic social media self. First Monday, 
22(10). DOI: 10.5210/fm.v22i10.7307.
McCosker A, Vivienne S and Johns A (eds) (2016) Negotiating Digital Citizenship: Control, 
Contest and Culture. London: Rowman and Littlefield International. 
McCosker A and Wilken R (2014) Rethinking ‘big data’ as visual knowledge: the sublime and the 
diagrammatic in data visualisation. Visual Studies 29(2): 155–164.
McCosker A and Wilken R (2020) Automating Vision: The Social Impact of the New Camera 
Consciousness. New York: Routledge. 
Mergel I (2015) Open collaboration in the public sector: the case of social coding on GitHub. 
Government Information Quarterly 32(4): 464–472.
Milner RM (2016) The World Made Meme: Public Conversations and Participatory Media. 
Cambridge, MA: MIT Press.
Mirsky Y and Lee W (2020) The creation and detection of deepfakes: a survey. Available at: 
https://arxiv.org/abs/2004.11138
Newton OB and Stanfill M (2019) My NSFW video has partial occlusion: deepfakes and the tech-
nological production of non-consensual pornography. Porn Studies 7: 398–414.
Nguyen TT, Nguyen CM, Nguyen D, et al. (2019) Deep learning for deepfakes creation and detec-
tion. Available at: https://arxiv.org/pdf/1909.11573.pdf
OECD.AI (2022) OECD framework for classifying AI Systems: a tool for effective AI policies. 
OECD.AI Policy Observatory. Available at: https://oecd.ai/en/classification

McCosker	
17
Öhman C (2019) Introducing the pervert’s dilemma: a contribution to the critique of Deepfake 
Pornography. Ethics and Information Technology 22: 133–140.
Pangrazio L (2016) Reconceptualising critical digital literacy. Discourse: Studies in the Cultural 
Politics of Education 37(2): 163–174.
Pangrazio L and Sefton-Green J (2020) The social utility of ‘data literacy’. Learning, Media and 
Technology 45(2): 208–220.
Pangrazio L and Selwyn N (2019) Personal data literacies: a critical literacies approach to enhanc-
ing understandings of personal digital data. New Media & Society 21(2): 419–437.
Paris B and Donovan J (2019) Deepfakes and Cheap Fakes: The Manipulation of Visual 
Evidence. New York: Data and Society. Available at: https://datasociety.net/wp-content/
uploads/2019/09/DS_Deepfakes_Cheap_FakesFinal-1.pdf
Penley C and Ross A (eds) (1991) Technoculture. Minneapolis, MN: University of Minnesota 
Press.
Petrov I, Gao D, Chervoniy N, et al. (2020) DeepFaceLab: a simple, flexible and extensible face 
swapping framework. Available at: https://arxiv.org/abs/2005.05535
Pishori A, Rollins B, van Houten N, et al. (2020) Detecting deepfake videos: an analysis of three 
techniques. Available at: https://arxiv.org/abs/2007.08517
Popova M (2019) Reading out of context: pornographic deepfakes, celebrity and intimacy. Porn 
Studies 7: 367–381.
Redden J, Brand J and Terzieva V (2020) Data harm record (updated). Data Justice Lab, August. 
Available at: https://datajusticelab.org/data-harm-record/
Reed MS, Evely AC, Cundill G, et al. (2010) What is social learning? Ecology and Society 15(4): 
1–10.
Rössler A, Cozzolino D, Verdoliva L, et al. (2018) Faceforensics: a large-scale video dataset for 
forgery detection in human faces. Available at: https://niessnerlab.org/papers/2018/z0facefo-
rensics/faceforensics-large-scale.pdf
Ruiz N, Bargal SA and Sclaroff S (2020) Disrupting deepfakes: adversarial attacks against con-
ditional image translation networks and facial manipulation systems. In: Computer vision—
ECCV 2020 workshops, Glasgow, 23–28 August.
Schreier M (2020) Content Analysis, Qualitative. Thousand Oaks, CA: SAGE.
Sefton-Green J (1998) Digital Diversions. London: UCL Press.
Smith H and Mansted K (2020) Weaponised deep fakes: national security and democracy. 
Australian Strategic Policy Institute. Available at: https://www.aspi.org.au/report/weapon-
ised-deep-fakes
Stilgoe J (2018) Machine learning, social learning and the governance of self-driving cars. Social 
Studies of Science 48(1): 25–56.
Street BV (1984) Social Literacies: Critical Approaches to Literacy in Development, Ethnography 
and Education. New York: Routledge.
Utz S and Wolfers LN (2020) How-to videos on YouTube: the role of the instructor. Information, 
Communication & Society. Epub ahead of print 23 July. DOI: 10.1080/1369118X.2020.180 
4984.
Vaccari C and Chadwick A (2020) Deepfakes and disinformation: exploring the impact of syn-
thetic political video on deception, uncertainty, and trust in news. Social Media + Society 
6(1): 1–13.
Vee A (2017) Coding Literacy: How Computer Programming Is Changing Writing. Cambridge, 
MA: MIT Press.
Walker AS (2019) Preparing students for the fight against false information with visual verifi-
cation and open source reporting. Journalism and Mass Communication Educator 74(2): 
227–239.

18	
new media & society 00(0)
Wang Z, Wang Y and Redmiles D (2018) Competence-confidence gap: a threat to female develop-
ers’ contribution on GitHub. In: 2018 IEEE/ACM 40th international conference on software 
engineering: software engineering in society (ICSE-SEIS), Gothenburg, 27 May–3 June, pp. 
81–90. New York: IEEE.
Westerlund M (2019) The emergence of deepfake technology: a review. Technology Innovation 
Management Review 9(11): 40–53.
Winter R and Salter A (2020) DeepFakes: uncovering hardcore open source on GitHub. Porn 
Studies 7(4): 382–397.
Yeh CY, Chen HW, Tsai SL, et al. (2020) Disrupting image-translation-based deepfake algorithms 
with adversarial attacks. In: Proceedings of the IEEE winter conference on applications of 
computer vision workshops, Snowmass, CO, 1–5 March.
Zappavigna M (2012) Discourse of Twitter and Social Media: How We Use Language to Create 
Affiliation on the Web. London: Bloomsbury.
Author biography
Anthony McCosker is Professor of Media and Communication at Swinburne University of 
Technology, Australia, where he is Deputy Director of the Social Innovation Research Institute 
and Node Leader for the ARC Centre of Excellence for Automated Decision Making and Society. 
His latest co-authored books are Everyday Data Cultures (2022, Polity Press) and Automating 
Vision: The Social Impact of the New Camera Consciousness (2020, Routledge).

