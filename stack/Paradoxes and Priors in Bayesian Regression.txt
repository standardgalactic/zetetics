Paradoxes and Priors in Bayesian Regression
Dissertation
Presented in Partial Fulﬁllment of the Requirements for the Degree
Doctor of Philosophy in the Graduate School of The Ohio State
University
By
Agniva Som, B. Stat., M. Stat.
Graduate Program in Statistics
The Ohio State University
2014
Dissertation Committee:
Dr. Christopher M. Hans, Advisor
Dr. Steven N. MacEachern, Co-advisor
Dr. Mario Peruggia

c⃝Copyright by
Agniva Som
2014

Abstract
The linear model has been by far the most popular and most attractive choice of
a statistical model over the past century, ubiquitous in both frequentist and Bayesian
literature. The basic model has been gradually improved over the years to deal with
stronger features in the data like multicollinearity, non-linear or functional data pat-
terns, violation of underlying model assumptions etc. One valuable direction pursued
in the enrichment of the linear model is the use of Bayesian methods, which blend
information from the data likelihood and suitable prior distributions placed on the
unknown model parameters to carry out inference.
This dissertation studies the modeling implications of many common prior distri-
butions in linear regression, including the popular g prior and its recent ameliorations.
Formalization of desirable characteristics for model comparison and parameter esti-
mation has led to the growth of appropriate mixtures of g priors that conform to
the seven standard model selection criteria laid out by Bayarri et al. (2012). The
existence of some of these properties (or lack thereof) is demonstrated by examining
the behavior of the prior under suitable limits on the likelihood or on the prior itself.
The ﬁrst part of the dissertation introduces a new form of an asymptotic limit, the
conditional information asymptotic, driven by a situation arising in many practical
problems when one or more groups of regression coeﬃcients are much larger than the
rest. Under this asymptotic, many prominent “g-type” priors are shown to suﬀer from
two new unsatisfactory behaviors, the Conditional Lindley’s Paradox and Essentially
ii

Least Squares estimation. The cause behind these unwanted behaviors is the existence
of a single, common mixing parameter in these priors that induces mono-shrinkage.
The novel block g priors are proposed as a collection of independent g priors on distinct
groups of predictor variables and improved further through mixing distributions on
the multiple scale parameters. The block hyper-g and block hyper-g/n priors are
shown to overcome the deﬁciencies of mono-shrinkage, and simultaneously display
promising performance on other important prior selection criteria. The second part
of the dissertation proposes a variation of the basic block g prior, deﬁned through a
reparameterized design, which has added computational beneﬁts and also preserves
the desirable properties of the original formulation.
While construction of prior distributions for linear models usually focuses on the
regression parameters themselves, it is often the case that functions of the parameters
carry more meaning to a researcher than do the individual regression coeﬃcients. If
prior probability is not apportioned to the parameter space in a sensible manner, the
implied priors on these summaries may clash greatly with reasonable prior knowl-
edge. The third part of the dissertation examines the modeling implications of many
traditional priors on an important model summary, the population correlation coef-
ﬁcient that measures the strength of a linear regression. After detailing deﬁciencies
of standard priors, a new, science driven prior is introduced that directly models the
strength of the linear regression. The resulting prior on the regression coeﬃcients
belongs to the class of normal scale mixture distributions in particular situations.
Utilizing a ﬁxed-dimensional reparameterization of the model, an eﬃcient MCMC
strategy that scales well with model size and requires little storage space is developed
for posterior inference.
iii

Dedicated to my father, Mr. Arup Kumar Som, my late mother, Mrs. Sanchaita
Som and my grandmother, Mrs. Lila Som.
iv

Acknowledgments
I would like to express my gratitude to everyone who has made it possible for me
to arrive at this momentous occasion in my life. First and foremost, I wish to thank
my advisors Dr. Chris Hans and Dr. Steve MacEachern for their endless support
and amazing mentorship during my four years of doctoral studies.
I am greatly
indebted to them for their unwavering patience in me, which helped me immensely to
overcome my limitations whenever I faltered. I consider myself to be very fortunate
that I got the opportunity to learn about the discipline from such brilliant minds
and to enhance my knowledge and understanding of statistics under their watchful
guidance. My advisors are greatly responsible for instilling in me the tenacity and
passion toward research work that is needed to be successful at this high level. Fond
memories of our meetings, including the serious discussions that frequently opened
my mind to new perspectives on approaching unresolved questions as well as the light
hearted chats on non-academic topics, will remain with me forever.
I am grateful to Dr. Mario Peruggia for serving on my doctoral and candidacy
exam committees and and to Dr. Xinyi Xu for serving on my candidacy exam commit-
tee. Their comments and suggestions helped to broaden the scope of my dissertation
research. A word of thanks should go to Dr. Radu Herbei as well for introducing
me to new areas of statistics in my ﬁrst year of graduate school and for helping me
v

to develop a fascination for research in this ﬁeld. I am thankful to NSF for par-
tially supporting my research under grant numbers DMS-1007682, DMS-1209194 and
DMS-1310294.
My family has always been a great source of love and support and without their
immense inﬂuence this would never have been possible. Maa, Baba, Dida and Dada
have always been there for me whenever I needed them, especially during the tough
times that I experienced in my ﬁrst year living so far away from home for the ﬁrst
time in my life. It was my parents’ dream that I get a doctorate degree, but now
that I am so close to receiving one, I feel heartbroken that my mother is no longer
with us to enjoy this moment with the rest of the family. I dedicate this dissertation
to my dad, who has been a constant source of my inspiration, and my mother and
my grandmother, both of whom sacriﬁced so much for me to achieve this cherished
dream.
I would also like to thank all my wonderful friends who made life in graduate school
and in Columbus enjoyable and worth cherishing all my life. There is a long list of
friends without whom my life in the last four years would not have been as much fun.
Thank you Abrez, Aritra, Arjun, Arka, Casey, Dani, Grant, Indrajit, Jingjing, John,
Matt, Parichoy, Prasenjit, Puja, Steve and the many others who I did not mention
here (but have been equally important to me in recent years), for helping me get
through the thick and thin of life as a graduate student. And most importantly, thank
you Sarah, the most special person in my life, for our wonderful times together. I am
highly appreciative of Sarah’s support, who stood by me throughout, in happiness
and in sorrow, and put up with my craziness the past few years.
vi

Vita
August 4, 1987 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Born - Kolkata, India.
2005-2008 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Stat. (Hons.), Indian Statistical
Institute, Kolkata, India.
2008-2010 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M. Stat., Indian Statistical Institute,
Kolkata, India.
2010-2014 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Graduate Teaching Associate,
The Ohio State University.
Publications
Research Publications
Som, A., Hans, C. M., MacEachern, S. N. “Block Hyper-g Priors in Bayesian Regres-
sion”. Technical Report No. 879, Department of Statistics, The Ohio State University,
2014.
Fields of Study
Major Field: Statistics.
vii

Table of Contents
Page
Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ii
Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iv
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
Vita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xii
List of Figures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xiv
Chapters
Page
1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Prior Speciﬁcation in the Linear Model . . . . . . . . . . . . . . . .
3
1.2
Problems with Mono-Shrinkage . . . . . . . . . . . . . . . . . . . .
6
1.3
Implications of the Prior on Other Regression Summaries
. . . . .
9
2.
Review of Popular Priors in Bayesian Regression Problems . . . . . . . .
12
2.1
Variable Selection Priors . . . . . . . . . . . . . . . . . . . . . . . .
13
2.1.1
“Spike and Slab”-type Priors
. . . . . . . . . . . . . . . . .
14
2.1.2
Stochastic Search Algorithms . . . . . . . . . . . . . . . . .
16
2.1.3
Priors on the Model Space . . . . . . . . . . . . . . . . . . .
17
2.2
Shrinkage Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
2.2.1
Scale Mixture Representation . . . . . . . . . . . . . . . . .
19
2.3
Classiﬁcation Based on Prior Knowledge . . . . . . . . . . . . . . .
24
2.3.1
Subjective Priors . . . . . . . . . . . . . . . . . . . . . . . .
24
2.3.2
Objective Priors
. . . . . . . . . . . . . . . . . . . . . . . .
25
2.4
Other Discussions on Bayesian Regression . . . . . . . . . . . . . .
26
viii

3.
Block g Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.1
Review of g Priors . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.1.1
Zellner’s g Prior
. . . . . . . . . . . . . . . . . . . . . . . .
28
3.1.2
Mixtures of g Priors . . . . . . . . . . . . . . . . . . . . . .
29
3.2
Asymptotic Evaluations: Paradoxes, Old and New
. . . . . . . . .
31
3.2.1
A Conditional Information Asymptotic . . . . . . . . . . . .
33
3.2.2
A Conditional Lindley’s Paradox . . . . . . . . . . . . . . .
34
3.2.3
The CLP in Other Mixtures of g Priors
. . . . . . . . . . .
36
3.2.4
Avoiding ELS and CLP Behaviors
. . . . . . . . . . . . . .
39
3.3
Block g Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.3.1
The Block Hyper-g Prior
. . . . . . . . . . . . . . . . . . .
42
3.3.2
Sampling from the Block Hyper-g Prior
. . . . . . . . . . .
42
3.3.3
Expansion with respect to Least Squares Estimates under the
Block g prior . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.4
Asymptotic Evaluations of the Block Hyper-g Prior . . . . . . . . .
52
3.5
Consistency of the Block Hyper-g Prior
. . . . . . . . . . . . . . .
57
3.5.1
Information Consistency . . . . . . . . . . . . . . . . . . . .
58
3.5.2
Conditions and Assumptions
. . . . . . . . . . . . . . . . .
59
3.5.3
Model Selection Consistency . . . . . . . . . . . . . . . . . .
60
3.5.4
Prediction Consistency . . . . . . . . . . . . . . . . . . . . .
61
3.6
The Block Hyper-g/n Prior . . . . . . . . . . . . . . . . . . . . . .
64
3.6.1
The Conditional Information Asymptotic in the Block Hyper-
g/n Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.6.2
Consistency of the Block Hyper-g/n Prior . . . . . . . . . .
69
3.7
Application to Real Data
. . . . . . . . . . . . . . . . . . . . . . .
71
3.7.1
US Crime Data . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.7.2
Ozone Data . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
4.
Block g Priors in Non-Orthogonal Designs . . . . . . . . . . . . . . . . .
89
4.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
4.2
Block g Prior on a Transformed Design
. . . . . . . . . . . . . . .
90
4.2.1
Transformation to a Block Orthogonal Design . . . . . . . .
91
4.2.2
The Block Orthogonalized Block g Prior . . . . . . . . . . .
93
4.3
Properties of the BOB Hyper-g Prior . . . . . . . . . . . . . . . . .
95
4.3.1
Eﬀect of the CLP and ELS
. . . . . . . . . . . . . . . . . .
96
4.3.2
Signiﬁcance of the Order of Orthogonalization . . . . . . . .
105
4.4
Consistency of the BOB Hyper-g and BOB Hyper-g/n Priors . . .
111
4.4.1
Information Consistency . . . . . . . . . . . . . . . . . . . .
112
ix

4.4.2
Model Selection Consistency . . . . . . . . . . . . . . . . . .
113
4.4.3
Prediction Consistency . . . . . . . . . . . . . . . . . . . . .
118
4.5
BOB Hyper-g Prior over all Block Permutations
. . . . . . . . . .
121
4.5.1
Prior with Equal Weights on Permutations . . . . . . . . . .
121
4.5.2
Prior on Model Space Encompassing all Permutations
. . .
123
4.5.3
Inference with BOPB Hyper-g Priors . . . . . . . . . . . . .
124
4.6
Application to Boston Housing Data . . . . . . . . . . . . . . . . .
131
4.6.1
Speciﬁcation of the BOPB Hyper-g/n Prior . . . . . . . . .
132
4.6.2
Cross-validated Predictive Performance
. . . . . . . . . . .
134
4.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
5.
Science Driven Priors on the Strength of Linear Regression . . . . . . . .
139
5.1
Implications of Standard Priors . . . . . . . . . . . . . . . . . . . .
139
5.2
Priors on ρ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
5.3
Priors on β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
5.4
Modeling with a Science Driven Prior on ρ2 . . . . . . . . . . . . .
154
5.4.1
Model Framework
. . . . . . . . . . . . . . . . . . . . . . .
154
5.4.2
Transformation to Hyperspherical Coordinates
. . . . . . .
155
5.4.3
Rotation of the Coordinate Axes . . . . . . . . . . . . . . .
157
5.5
Sampling from the R-Posterior . . . . . . . . . . . . . . . . . . . .
159
5.5.1
Gibbs Sampling in the Reparameterized Design . . . . . . .
159
5.5.2
The Data Augmentation Gibbs Sampler . . . . . . . . . . .
164
5.6
Model Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
5.7
Application to US Crime Data
. . . . . . . . . . . . . . . . . . . .
172
5.7.1
US Crime Data . . . . . . . . . . . . . . . . . . . . . . . . .
172
5.7.2
Eﬀect of the Prior on ρ2 . . . . . . . . . . . . . . . . . . . .
175
5.7.3
Cross-validated Predictive Performance
. . . . . . . . . . .
177
5.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
6.
Discussion and Future Work . . . . . . . . . . . . . . . . . . . . . . . . .
182
Appendices
Page
A.
Appendix for Chapter 3
. . . . . . . . . . . . . . . . . . . . . . . . . . .
186
A.1 Proof of Theorem 3.2.1 . . . . . . . . . . . . . . . . . . . . . . . . .
186
A.2 Proof of Theorem 3.2.2 . . . . . . . . . . . . . . . . . . . . . . . . .
190
A.3 Proof of Corollary 3.2.1
. . . . . . . . . . . . . . . . . . . . . . . .
193
A.4 Finding the Limit of (3.7) in Theorem 3.2.4 . . . . . . . . . . . . .
194
x

A.5 Proof of Invariance of Block g Priors to Reparameterizations by
Blockwise Aﬃne Transformations . . . . . . . . . . . . . . . . . . .
196
A.6 Proof of Lemma 3.4.2
. . . . . . . . . . . . . . . . . . . . . . . . .
197
A.7
Proof of Theorem 3.4.1 . . . . . . . . . . . . . . . . . . . . . . . .
199
A.8 Proof of Corollary 3.4.1
. . . . . . . . . . . . . . . . . . . . . . . .
201
A.9 Proof of Corollary 3.4.2
. . . . . . . . . . . . . . . . . . . . . . . .
203
A.10 Proof of Theorem 3.5.1 . . . . . . . . . . . . . . . . . . . . . . . . .
204
A.11 Proof of Theorem 3.5.2 . . . . . . . . . . . . . . . . . . . . . . . .
206
A.12 Proof of Lemma 3.5.1 . . . . . . . . . . . . . . . . . . . . . . . . .
211
A.13 Proof of Theorem 3.6.3 . . . . . . . . . . . . . . . . . . . . . . . . .
213
A.14 Proof of Theorem 3.6.4 . . . . . . . . . . . . . . . . . . . . . . . . .
214
A.15 Proof of Theorem 3.6.5 . . . . . . . . . . . . . . . . . . . . . . . . .
217
A.16 Proof of Theorem 3.5.2 when 2 < a ≤3 . . . . . . . . . . . . . . .
218
A.17 Description of the Ozone Data
. . . . . . . . . . . . . . . . . . . .
220
B.
Appendix for Chapter 4
. . . . . . . . . . . . . . . . . . . . . . . . . . .
222
B.1 Proof of Lemma 4.3.2
. . . . . . . . . . . . . . . . . . . . . . . . .
222
B.2 Proof of Lemma 4.3.3
. . . . . . . . . . . . . . . . . . . . . . . . .
224
B.3 Proof of Proposition B.2.1 . . . . . . . . . . . . . . . . . . . . . . .
227
B.4 Proof of Theorem 4.3.6 . . . . . . . . . . . . . . . . . . . . . . . . .
229
B.5 Proof of Lemma 4.5.3
. . . . . . . . . . . . . . . . . . . . . . . . .
232
B.6 Proof of Lemma 4.5.4
. . . . . . . . . . . . . . . . . . . . . . . . .
233
B.7 Description of the Boston Housing Data . . . . . . . . . . . . . . .
235
C.
Appendix for Chapter 5
. . . . . . . . . . . . . . . . . . . . . . . . . . .
236
C.1 Derivation of π(β | σ2
ϵ) in (5.4) . . . . . . . . . . . . . . . . . . . .
236
C.2 Derivation of the Scale Mixture Representation of β
. . . . . . . .
239
C.3 Log-concavity of the Full Conditional of 1/σ2
ϵ . . . . . . . . . . . .
240
C.4 MH Step for the joint update of (M, φ−1) . . . . . . . . . . . . . .
241
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
xi

List of Tables
Table
Page
3.1
Table showing entropy values for the model posterior under diﬀerent
priors. Hg and Hg/n stand for hyper-g and hyper-g/n respectively,
BHg/n denotes the block hyper-g/n prior.
. . . . . . . . . . . . . . .
75
3.2
Table showing (A) entropy values for posterior distribution of model
size and (B) average entropy values for posterior distributions of marginal
inclusion under diﬀerent priors. Hg, Hg/n and BHg/n stand for hyper-
g, hyper-g/n and block hyper-g/n respectively. . . . . . . . . . . . . .
75
3.3
Table showing marginal inclusion probabilities of variables under dif-
ferent posteriors. Predictors marked with asterisks belong to the cor-
responding median probability model. . . . . . . . . . . . . . . . . . .
78
3.4
Table displaying the cross-validated predictive MSE (under BMA) for
diﬀerent procedures when w = 1
2.
. . . . . . . . . . . . . . . . . . . .
79
3.5
Table displaying the cross-validated predictive MSE (under BMA) for
diﬀerent procedures when w = 1
3.
. . . . . . . . . . . . . . . . . . . .
79
3.6
Table from Yu et al. (2011) displaying the cross-validated predictive
SSE under diﬀerent methods.
. . . . . . . . . . . . . . . . . . . . . .
83
3.7
Table displaying the cross-validated predictive SSE (under BMA) for
diﬀerent procedures using the entire training data. . . . . . . . . . . .
84
3.8
Table displaying the cross-validated predictive SSE (under BMA) for
diﬀerent procedures after discarding the inﬂuential observation from
the training data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.9
Marginal inclusion probabilities under the block hyper-g/n (Form 1)
and the hyper-g posteriors. . . . . . . . . . . . . . . . . . . . . . . . .
86
xii

4.1
Table showing cross-validated predictive SSE under diﬀerent methods.
The SSE values for the Bayesian models are obtained by model aver-
aging. All ﬁve forms of the BOPB hyper-g/n prior have the hyperpa-
rameter value of a = 3.
. . . . . . . . . . . . . . . . . . . . . . . . .
135
4.2
Table showing cross-validated predictive SSE (under BMA) for the
hyper-g, hyper-g/n and BOPB hyper-g/n priors. The model averaging
is done over all possible permutations and combinations of the set of
design blocks appearing in the speciﬁc prior form. . . . . . . . . . . .
136
5.1
Prior mean and standard deviation of ρ2 as a function of model size
when β ∼N(0, σ2
ϵIp). . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
5.2
Prior mean and standard deviation of ρ2 as a function of model size
when (A) β ∼N(0, Ip) and σ2
ϵ ∼IG(3, 1
2), and (B) β ∼N(0, Ip) and
σ2
ϵ ∼Gamma(1, 1). . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
5.3
Prior mean and standard deviation of ρ2 as a function of model size
when n = 20 and (A) β ∼N(0, σ2
ϵ(XTX)−1), and (B) β ∼N(0, p2σ2
ϵ(XTX)−1).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
5.4
Cross-validated predictive MSE (under BMA) for diﬀerent procedures
when w = 1
2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
5.5
Cross-validated predictive MSE (under BMA) for diﬀerent procedures
when w = 1
3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
5.6
Cross-validated predictive MSE (under BMA) for diﬀerent procedures
when w = 1
4. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
xiii

List of Figures
Figure
Page
1.1
The implied prior on ρ2 as a function of model size p. . . . . . . . . .
10
3.1
Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs
in areas shaded with vertical lines and expansion of ˆβ2 occurs in areas
shaded with horizontal lines. . . . . . . . . . . . . . . . . . . . . . . .
48
3.2
Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs
in areas shaded with vertical lines and expansion of ˆβ2 occurs in areas
shaded with horizontal lines. . . . . . . . . . . . . . . . . . . . . . . .
49
3.3
Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs
in areas shaded with vertical lines and expansion of ˆβ2 occurs in areas
shaded with horizontal lines. . . . . . . . . . . . . . . . . . . . . . . .
50
3.4
Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs
in areas shaded with vertical lines. ˆβ2 does not expand since the region
of expansion is negligible. . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.5
Comparison of log(P(Mγ | y)) under the two priors.
The ﬁgures
on the upper panel correspond to prior probability of inclusion of a
predictor equal to 1
2 and the ones on the lower panel correspond to
prior probability of inclusion equal to
1
3.
The red line denotes the
y = x line. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
3.6
Distance d = 1 −
|| bβ−bβLS||
|| bβLS||
plotted against log posterior probability.
Prior inclusion probability w = 1
2 in the upper panel and w = 1
3 in the
lower panel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
3.7
Comparison of log(P(Mγ | y)) under the hyper-g and block hyper-g/n
(Form 1) priors. The red line denotes the y = x line.
. . . . . . . . .
87
xiv

5.1
Kernel density estimates of the posteriors of δ, σ2
ϵ and ρ2 from the full
model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
5.2
Kernel density estimates of the posterior of ρ2 from the full model when
the prior mean is ﬁxed. . . . . . . . . . . . . . . . . . . . . . . . . . .
176
5.3
Kernel density estimates of the posterior of ρ2 from the full model when
a + b is ﬁxed.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
xv

Chapter 1: Introduction
The linear model has been a mainstay of statistical practice for over 200 years,
dating to Gauss and Legendre’s work on least squares estimation and Laplace’s work
on least absolute deviation regression. Early use of the model was propelled by its
clear focus on the relationship between a set of covariates and the mean or median of
the response variable which lends a natural interpretation to the regression coeﬃcients
and connects the ﬁtted model with prediction of future responses. The model’s scope
can be extended considerably through creation of new covariates, commonly in the
form of a polynomial basis expansion of an original set of covariates, allowing one to
approximate a wide variety of mean and median functions and leading to the linear
model’s prominence in many areas of science. The computational convenience and
analytical tractability of least squares estimation gave the edge to the mean-based
formulation of the linear model.
In today’s world, the relevance of the linear model remains undiminished, and
much of the practice of statistics involves developing, ﬁtting, choosing among and
averaging forecasts from linear models. Modern themes include automated screening
of enormous numbers of models and adjustment to least squares through penalization
of the likelihood and use of Bayesian methods.
The latter are the focus of this
dissertation.
1

Bayesian methods have made substantial contributions to linear model.
The
contributions of Bayesian methods include the coherence of the analysis which fol-
lows from the internal consistency of Bayesian methods, the seamless integration of
model ﬁtting and inference which follows from a decision theoretic speciﬁcation of the
problem, the ability to combine pre-experimental and post-experimental information
which follows from Bayes theorem, and the ability to account for both model uncer-
tainty and parameter uncertainty when making predictions which also follows from
Bayes theorem.
In contrast to classical work on the linear model, a Bayesian analysis requires
speciﬁcation of the prior distribution. The prior distribution may be subjectively
speciﬁed, as is commonly done when one has a speciﬁc model in mind, or a rule
may be used to specify it, as is often the case when one wishes to screen hundreds,
thousands or millions of models. The analysis then ﬂows from the combination of
prior distribution and likelihood into the posterior distribution, and from the posterior
distribution into inference. The prior distribution is thus a key determinant of the
success or failure of the analysis, and an essential part of the Bayesian analysis is
an understanding of the consequences of selecting a prior distribution of one form or
another.
The ﬁrst focus of this dissertation is the investigation of properties of certain
types of prior distributions for the linear model. These prior distributions include
commonly used priors such as the g prior and a variety of mixtures of g priors. This
investigation makes use of novel limiting arguments which lead to the description of
the Essentially Least Squares phenomenon and uncover a new paradox which we call
the Conditional Lindley’s Paradox. Current prior distributions perform poorly under
2

the phenomenon and paradox. This leads to the development of a new class of prior
distributions, the block hyper-g priors, which perform well on traditional criteria such
as model selection consistency and predictive consistency, and equally well under our
new evaluations.
The second focus of this dissertation is the practical application of the Bayesian
linear model. Investigation into use of the model is twofold. First, an implementation
is developed for the block hyper-g prior distributions, with attention given to com-
putational and modeling issues. Second, a set of science driven prior distributions
are developed. These prior distributions make use of a small amount of subjective in-
put, focusing on the implied distribution of an important summary of an experiment.
The science driven prior distributions keep this implied distribution approximately
constant across the set of models under consideration, in sharp contrast to currently
popular prior distributions. These prior distributions are developed, a computational
implementation is devised, and their beneﬁts for data analysis are illustrated.
1.1
Prior Speciﬁcation in the Linear Model
The basic regression problem can be described as explaining the behavior of the
response vector y = (y1, y2, . . . , yn)T using a known set of p potential predictor vari-
ables x1, x2, . . . , xp. If X = (x1, x2, . . . , xp) denotes the n × p design matrix and β
denotes the vector of regression coeﬃcients, then the linear model can be represented
as:
y = 1α + Xβ + ϵ
where 1 is a n × 1 vector of 1’s and α is the intercept. The vector of errors ϵ =
(ϵ1, . . . , ϵn)T is generally assumed to be distributed as a Gaussian random vector with
3

independent and identically distributed components, ϵ ∼N(0, σ2In). The assumption
of mean zero errors leads to the expression E(y) = α1+Xβ. The method adopted by
Bayesians involves placing a prior distribution on the vector of unknown parameters
(α, β, σ2) and the choice of prior distributions on these parameters can be based on
a variety of theoretical, computational and practical considerations.
Choice of the prior distribution is generally considered to be more important for
model selection than for estimation. A classic example which advocates the signiﬁ-
cance of the prior distribution in model selection is the Lindley Paradox (also known
as Lindley’s Paradox). It demonstrates how a diﬀuse prior distribution can sometimes
lead to a major conﬂict between Bayesian and frequentist approaches to inference.
Selection of meaningful prior distributions for model speciﬁc parameters is crucial
as well, since model selection procedures are highly sensitive to prior speciﬁcation
(Berger and Pericchi, 2001). The prior distribution plays a strong role in determining
the model-averaged predictions since these involve the relative weighting of multiple
models. Predictions from Bayes procedures are generally averaged over a collection
of models to account for model uncertainty, and the contribution of each model to
the weighted sum is directly inﬂuenced by prior choice.
When working with the linear model, one fundamental task is the determination
of which covariates are active (i.e., have a non-zero regression coeﬃcient) and which
are inactive (i.e., have a zero regression coeﬃcient). In the Bayesian setting, this is
accomplished by specifying a mixed distribution on the regresssion coeﬃcients. For
a linear model with a speciﬁed set of active variables, a probability distribution is
placed directly on the active regression coeﬃcients. Some probability 1 −w ∈(0, 1)
is assigned to each regression coeﬃcient being inactive, so that the coeﬃcient is
4

active with probability w and the size of the non-zero active coeﬃcient is distributed
according to some absolutely continuous (w.r.t. Lebesgue measure) density function.
When the continuous distribution is uniform, these prior distributions are called the
“spike and slab” priors (Mitchell and Beauchamp, 1988). A uniform “slab” might
not always be ideal, and often times it is appropriate to impose more ﬂexible (and
complex) prior distributions on the non-zero regression coeﬃcients. The foremost
choice among non-uniform “slabs” is the normal slab which generates the following
prior on a single coeﬃcient βi
βi ∼(1 −w) δ0 + w N(β0, σ2
βi)
where δ0 represents a point mass at 0 and σ2
βi is a variance parameter.
The choice of σβi also plays a major role in inference and over the years researchers
have suggested distinctive forms of the variance parameter. Often times for conve-
nience, the priors on βi, i = 1, . . . , p, are taken to be independent. But researchers
have also come up with distributions to model structural dependence among the re-
gression coeﬃcients and a leading example of such a prior is the g prior due to Zellner
(Zellner, 1986):
β | g, σ2 ∼N
 0, gσ2(XTX)−1
.
The g prior can also be speciﬁed as a mixed distribution when there is uncertainty
regarding the “correct” model to be used. As in the case of the “spike and slab”-type
priors, the g prior may be imposed only on the active coeﬃcients in a particular model
and prior speciﬁcation is then repeated over a big collection (generally all possible
combinations) of sets of active variables. Each set of active variables determines a
unique model.
5

An alternative is to design prior distributions without the complexity of determin-
ing the sets of active and inactive covariates and the prior weights associated with
each combination. This variation of priors has gained importance in statistical liter-
ature as the class of shrinkage priors, where a prior distribution is speciﬁed focusing
on the shrinkage properties of the estimators of regression coeﬃcients. The idea is to
impart a continuous prior to each regression coeﬃcient with the prior density having
a spike at zero and, preferably, heavy tails. This can lead to substantial compu-
tational beneﬁts in large problems, but this approach precludes any predictor from
being exactly zero. Unimportant covariates in the model admit near zero coeﬃcient
estimates, but those small values can never exactly equal zero.
It is a familiar tactic to use shrinkage priors that can be expressed as scale mixtures
of normal distributions (West, 1987); currently there is an enormous literature and a
profusion of priors that can be represented as scale mixtures of normals. While most of
the normal scale mixture priors assume the regression coeﬃcients to be independent,
priors inducing dependence between the coeﬃcients through the empirical design
matrix (or some other covariance matrix) are also not uncommon. A prior distribution
with such dependence structure is usually expressed as a scale mixture of g priors (Cui
and George, 2008; Liang et al., 2008; Maruyama and George, 2011; Bayarri et al.,
2012), which is a special case of a scale mixture of normal distributions.
1.2
Problems with Mono-Shrinkage
Section 1.1 outlines the broad classiﬁcation of popular variations of priors assigned
to coeﬃcients in the linear model. The repeated reference to the Gaussian prior and
its many variants in the preceding discussion can be attributed to these probably
6

being the most regularly used prior distributions in the statistics literature. A general
class of prior distribution on regression coeﬃcients with widespread use in regression
modeling is described as:
β | c, σ2 ∼N
 0, cσ2Σ

(1.1)
for some scale parameter c and prior covariance matrix Σ. Notice that the popular g
prior belongs to this class of priors if we consider c = g and Σ = (XTX)−1. The scale
parameter c greatly impacts the amount of shrinkage experienced by the regression
coeﬃcient estimates. Large values of c are associated with little or no shrinkage in the
estimated coeﬃcients while small values of c lead to strong shrinkage in the estimates.
In many practical regression problems with a sizable number of predictor variables,
it is not unreasonable to expect that only a few of them will really explain the variation
in the response while the other variables will not be useful. In such a situation one
should expect a sensible Bayesian procedure to generate estimates that get shrunk
toward zero for the small or moderate coeﬃcients, and to leave the large coeﬃcients
unaltered at the same time.
So it is often necessary to accommodate distinctive
degrees of shrinkage in separate groups of predictor coeﬃcients. Expecting such an
ideal poly-shrinkage behavior is unrealistic when one implements a prior of the form
(1.1), since it is only possible to have identical shrinkage across all coeﬃcients (mono-
shrinkage) with a common scale c.
In recent literature, priors like (1.1) have been extended further by placing a prior
distribution on the unknown parameter c.
No ﬁxed choice of c can work well in
all regression problems, and a distribution on c helps to generate a data adaptive
robust solution that usually leads to better estimation and prediction. The prior
distribution on c is suitably chosen to generate a thick-tailed marginal prior on β,
7

since thick tails have been linked to some essential consistency properties in the
linear model. However, despite the data adaptiveness with a hierarchical prior, the
original complication still persists since the prior causes adaptive shrinkage, but mono-
shrinkage nonetheless.
Further pitfalls of mono-shrinkage are evident when one closely examines the prop-
erties of many standard priors. The motivation behind the development of most sin-
gle scale priors mainly stems from model comparison and consistency properties of
Bayesian procedures, and are occasionally driven by computational considerations.
We demonstrate in Chapter 3 that priors associated with mono-shrinkage display un-
desirable inferential performance when there is an appreciable diﬀerence in coeﬃcient
sizes for predictor variables. The structure of shrinkage arising from speciﬁc priors
of the form (1.1) acts as a catalyst for some paradoxical and perplexing behaviors,
described in Chapter 3 as the Conditional Lindley’s Paradox (CLP) and Essentially
Least Squares (ELS) estimation. The reason behind these aberrations is that the
single scale parameter adapts with the data, and the presence of even a few huge co-
eﬃcients forces most of the posterior probability of c toward big values. Dissemination
of the probability mass of c away from small values makes the prior distribution (1.1)
on the regression coeﬃcients very diﬀuse. As a consequence, ELS and CLP behav-
iors emerge in the priors connected to mono-shrinkage, and deteriorate the quality of
inference.
Due to all of these considerations, the need for multiple scale parameters in a
prior, capable of inducing poly-shrinkage in the coeﬃcients, becomes indispensable
in many practical problems. The block hyper-g prior, which is an adaptation of the
traditional g prior with a single scale parameter, is introduced in Chapter 3 as a
8

remedy to the aforementioned problems. The properties of the new multiscale prior
are studied in detail and the beneﬁts of poly-shrinkage resulting from the block g
priors are highlighted in Chapters 3 and 4.
1.3
Implications of the Prior on Other Regression Summaries
The most commonly used prior distributions for Bayesian regression models, some
of which are brieﬂy described in Section 1.1, typically assume that coeﬃcients are a
priori independent or induce dependence via the empirical design matrix.
While
these standard priors (and recently reﬁned versions of them) may exhibit desirable
behavior with respect to targeted inferential goals, they do not distribute probability
throughout the entire parameter space in a way that is consistent with all prior
beliefs. For example, most standard priors lead to a peculiar modeling implication
on the strength of regression measured by the population correlation coeﬃcient ρ. A
serious aberration arises due to these common priors allocating unreasonably high
probability to values of ρ2 near 1 when the number of predictor variables increases.
To illustrate this eﬀect, consider a regression model with a very basic prior distri-
bution on the regression coeﬃcients
β ∼N(0, gΣ)
where g is a scale parameter and Σ is a covariance matrix. In the simple case when
the covariance matrix Σ = Ip and g = 1, the left panel of Figure 1.1 shows how the
prior on ρ2 focuses most of its mass around 1 as p grows. It is implausible to believe
that merely incorporating more covariates, irrespective of their signiﬁcance in the
particular model, would explain nearly all of the variation in the response.
9

One possible way to control this peculiar behavior of the prior distribution on ρ2
is to ﬁx the scale parameter g to be a function of the number of predictors p, but
even in this case some problems persist as illustrated in the right panel of Figure 1.1.
With a scale of g = 1
p, one is able to force the unrealistic amount of prior mass away
from 1, but now the prior concentrates around a neighborhood of 1
2. In the general
case when g is chosen to be an arbitrary decreasing function of p, the prior on ρ2
again moves toward a degenerate distribution and concentrates either around one or
around some arbitrary value within the unit interval (depending on the particular
choice of g), as the number of predictors grows.
Figure 1.1: The implied prior on ρ2 as a function of model size p.
The correlation coeﬃcient is not the only measure that suﬀers from strange mod-
eling implications.
Oftentimes direct prior speciﬁcation on regression coeﬃcients
10

severely impacts inference related to other regression parameters and summaries.
Strong knowledege about a function of the model parameters can be appropriately
modeled with a meaningful prior distribution on the function itself. As an exam-
ple, in some designed experiments it would be completely impractical to expect the
individual eﬀect sizes as well as the sizes of some speciﬁc contrasts (or other linear
combinations) to exceed certain threshold values. In situations like these, it makes
better sense to elicit carefully designed priors jointly on the contrasts and particular
eﬀects, since common variable selection priors may concentrate excessive prior mass
outside of the tolerable bounds. It is perfectly logical to impart information indirectly
to the regression model in order to be compatible with prior beliefs that are more
signiﬁcant to the researcher.
This motivates the origin of a new class of priors, called science driven priors, to
overcome shortcomings of traditional priors and to develop sensible Bayesian models
with favorable inferential properties. A science driven prior can be developed based on
a variety of model summaries, but our formulation in Chapter 5 is aimed speciﬁcally
to deal with the undesirable implications on ρ2. Following this new approach, prior
information is incorporated into the model by directly eliciting a prior on the strength
of linear regression relationship. Chapter 5 presents the success of one version of
science driven prior called the R-prior, and elaborates on on its statistical properties.
11

Chapter 2: Review of Popular Priors in Bayesian Regression
Problems
There have been two main streams of thought in analyzing the Bayesian estimation
and variable selection problem for the linear model. The ﬁrst stream casts uncertainty
about predictor variables being active or not in the traditional framework of variable
selection. A regression coeﬃcient is zero if the corresponding predictor is inactive
and non-zero if the predictor is active. Hence, the prior speciﬁcation is carried out
over a collection of diﬀerent models, with each model determined by its set of active
predictor variables. Given a set of p explanatory variables, the collection of models
arising from including or excluding speciﬁc predictors in the model is often taken to
be the entire set of 2p possible models. This approach is described in Section 2.1. The
second stream of thought retains all candidate variables in the model. Unimportant
predictor variables have regression coeﬃcients that are shrunk toward zero.
The
relative importance of the predictors is driven by the amount of shrinkage, and the
focus of research is on developing prior distributions that lead to an appropriate
shrinkage pattern. This approach is described in Section 2.2.
12

2.1
Variable Selection Priors
The development of prior distributions in Bayesian regression problems is often
motivated by theoretical and practical considerations related to model selection and
estimation. Sometimes the primary goal of research is to discover the model (or col-
lection of models) that best explains the data among the set of available models.
When the model space consists of all possible combinations of predictors (any covari-
ate can be in or out of the model), variable selection can be cast as a special case
of model selection. Each group of active variables selected from the entire collection
of candidate predictors determines a particular model. The set of prior distributions
speciﬁed (by a rule) on the regression parameters from all such models forms the
variable selection prior.
The variable selection prior is suitable to deal with model uncertainty when
equipped with a prior on the model space. This generates a posterior distribution on
the set of models which helps to quantify the amount of uncertainty associated with
each model as well as each individual predictor. The marginal inclusion probability
of a predictor variable manifests its importance to the regression model, while a high
posterior weight for a model indicates that predictors in the given model are useful
collectively. Even while implementing a variable selection prior, it is rare to use a
single model like the highest posterior probability model or the median probability
model (Barbieri and Berger, 2004) for estimation purposes. No model is believed to
be true with certainty and the usual path adopted to account for the unpredictabil-
ity in model selection is Bayesian model averaging. Estimation and prediction with
these priors are carried out by averaging estimates over all possible models, where
each estimate is weighted according to the relative importance of the model.
13

2.1.1
“Spike and Slab”-type Priors
The most basic version of a variable selection prior is the “spike and slab” prior
proposed by Mitchell and Beauchamp (1988). In this prior, the inactive covariates are
chosen with a prespeciﬁed prior probability and their coeﬃcients are set equal to zero
in the model. This constitutes the “spike” in the prior while the “slab” part comes
from a uniform distribution on the set of active predictor variables, which includes all
candidate variables excluding the selected inactive ones. The uniform slab is deﬁned
over a very wide interval on the real line, and so there is eﬀectively no shrinkage in
the estimates of the active coeﬃcients with this prior.
Over the years there have been many extensions of the basic prior by Mitchell and
Beauchamp (1988), including non-uniform “slabs”, continuous “spikes” and empirical
Bayes as well as fully Bayesian versions of the model. George and McCulloch (1993)
and Ishwaran and Rao (2005) modiﬁed the “spike and slab” idea to implement con-
tinuous “spikes” in place of the degenerate spike. They proposed an independent two
component mixture of normals prior (centered at zero) for the regression coeﬃcients,
assigning a very small variance to one of the normal components in order to closely
mimic a degenerate distribution at zero. Ishwaran and Rao (2005) improved on the
prior by George and McCulloch (1993) with a “rescaled spike and slab” prior and
added another level of distributions in the heirarchical prior speciﬁcation to reduce
reliance on prior hyperparameters. There is no clear distinction between active and
inactive covariates in these priors since none of the coeﬃcients can be exactly zero,
but the normal component in the prior with near-zero variance forces the unimpor-
tant predictors to have very small coeﬃcient estimates. Other notable extensions of
Mitchell and Beauchamp (1988)’s prior include the work of Foster and George (1994),
14

Kass and Wasserman (1995) and Fernandez et al. (2001). They implemented the g
prior as a particular form of a normal (non-uniform) “slab,” and suggested diﬀerent
choices of the hyperparameter g based on various theoretical motivations.
Some researchers also pursued an empirical Bayes (EB) approach to the linear
regression problem. George and Foster (2000) worked with the familiar g prior form
and used EB estimates of g to resolve the issue of specifying a predetermined value
for the hyperparameter. Johnstone and Silverman (2004) analyzed the asymptotic
properties of the familiar “spike and slab” procedure, but replaced the known mixing
probability w with a data driven EB choice of w.
The fully Bayesian approach to prior speciﬁcation is popular for its robust behav-
ior, as it enables the prior distribution on regression coeﬃcients to adapt according
to the data. In a fully Bayesian formulation, the distributions on all the parameters
adjust with the sparsity pattern in the data, which ultimately guide the amount of
shrinkage in the coeﬃcient estimates. Liang et al. (2008), Maruyama and George
(2011), Bayarri et al. (2012) and Maruyama and Strawderman (2010) resorted to a
fully Bayesian analysis of “g-type” priors by placing a prior distribution on the hy-
perparameter g. The main beneﬁt of this approach is that it obviates selection of
hyperparameters for each individual problem.
Scott and Berger (2010) showed that there is an inherent diﬀerence between the
empirical Bayes and the fully Bayes procedures and that inference from the two
approaches might be quite diﬀerent, and drastically so in certain situations. Most
of these “variable selection” priors were developed subject to the design matrix X
having full column rank, but they have been extended to the large p, small n setting
as well (West, 2003; Maruyama and George, 2011).
15

2.1.2
Stochastic Search Algorithms
The choice of the diﬀerent “spike” and “slab” components over the years has been
guided by both theoretical and computational considerations and it is impossible to
unearth one absolute winner that is superior in performance to all others in any given
problem. The main drawback for this kind of priors, however, is that the complexity
of computation grows exponentially fast with the number of explanatory variables
and it becomes impractical to perform a complete analysis involving the entire model
space even in moderately large problems. The speciﬁcation of the two component
normal mixture prior was initially intended to simplify calculations and to obtain
eﬃcient MCMC routines to explore the posterior. An important consideration in
selecting the prior on regression parameters in the earlier days had been to select
conjugate priors that lead to simple analytic expressions of the marginal likelihood.
The normal–inverse-gamma family (George and McCulloch, 1993, 1997; Raftery et al.,
1997) gradually gained importance because the conjugate normal prior on the coeﬃ-
cients along with an inverse-gamma prior on the error variance σ2 promoted wonderful
computationally tractable algorithms aiding posterior inference.
Stochastic search routines have been devised in recent years as a solution to the
computational limitation in large problems, and various algorithms with distinctive
attributes have been suggested (George and McCulloch, 1993; Raftery et al., 1997;
Berger and Molina, 2005; Nott and Kohn, 2005; Hans et al., 2007; Scott and Car-
valho, 2008; Bottolo and Richardson, 2010; Clyde et al., 2011). Instead of exploring
the whole model space, these eﬃcient routines aim to detect models with higher pos-
terior probabilities and provide approximate values for many posterior summaries of
interest, which would have otherwise necessitated a full enumeration of the model
16

space.
The Shotgun Stochastic Search (SSS) algorithm developed by Hans et al.
(2007) is useful to rapidly discover regions of high posterior probability in the model
space by deﬁning appropriate neighborhoods of models which can be explored in par-
allel. Online estimation of marginal posterior inclusion probabilities drive the model
search strategies of Berger and Molina (2005), Scott and Carvalho (2008) and Clyde
et al. (2011), with a goal of identifying highly likely models containing the most rel-
evant predictors. The collection of models can then be used to furnish reasonably
accurate posterior summaries of the entire model space.
2.1.3
Priors on the Model Space
This approach to Bayesian linear regression entails speciﬁcation of a prior dis-
tribution on the model space since there is uncertainty about which predictors are
active. The prior distribution on the model space plays a direct role in determining
the posterior weights of individual models and one has to be careful in assigning prior
probabilities to the models. When the size of the model space is small, it is possible
to assign a subjective probability to each model based on prior beliefs. However,
subjective speciﬁcation of these prior probabilities is unrealistic for even a moderate
number of candidate predictors. With p predictors, if all possible combinations of
predictors are considered to be within the model space, there are 2p models. For
p = 30, there are over 1 billion models. For large problems, subjective speciﬁcation
of the prior distribution cannot be done, and so the prior distribution is speciﬁed by
a rule. The most common rule leads to the Bernoulli variable inclusion prior. Under
this prior distribution, a single parameter, w, is speciﬁed. The predictor variables
17

are independently declared active (with probability w) or inactive (with probability
1 −w).
The value of w determines whether larger or smaller models receive greater or
lesser prior probability. A larger value of w favors bigger models while a smaller
value of w favors smaller models. The “indiﬀerence prior” takes w = 1
2 under which all
models receive the same prior probability. This prior, however, is strongly informative
about the number of variables to be included in the model, and heavily favors half of
the total number of predictors to receive the largest probability of inclusion a priori.
Unreasonable values of the hyperparameter w can have a negative impact on inference
and so it is imperative to choose the Bernoulli variable inclusion prior judiciously.
Use of a ﬁxed choice of w in the Bernoulli prior, independent of the total number
of predictors in the model, is prone to problems.
Scott and Berger (2010) show
that such priors (with ﬁxed w) do not account for multiplicity correction in model
selection. Multiplicity correction is essential in a Bayesian procedure, and more so in
high-dimensional problems where the prior can wash out the eﬀect of the likelihood
without the necessary multiplicity adjustments. For example, in a regression problem
with p predictors, the full model (with all p predictors) is
 w
1−w
p times more likely in
the prior than the null model (with no predictors). When p is large and w = 1
3 (say),
 w
1−w
p = 2−p which makes the null model favored over the full model so strongly by
the prior that it becomes diﬃcult for the data to override the prior eﬀect. The full
model is unlikely to be preferred over the null model in the posterior distribution,
even when it is clearly a much better ﬁt.
Placing a prior distribution on w resolves this issue, as this fully Bayesian method
has the desirable property of automatically adjusting for multiplicity.
The usual
18

path adopted in the fully Bayesian approach is to assign a suitable Beta prior to
w, where the prior hyperparameters are chosen to match the prior belief regarding
the expected number of predictor variables in the data generating model. The EB
(Type-II maximum likelihood) estimate of w is sometimes used as a plug-in value for
w in the analysis. This EB model prior can also successfully handle the multiplicity
problem (Scott and Berger, 2010). Both the EB and fully Bayesian priors on the
model space display admirable properties because of their ability to adapt with data.
2.2
Shrinkage Priors
The second stream of thought adopted in Bayesian regression problems is to work
with the full model and specify a shrinkage prior on the regression coeﬃcients. This is
a computationally eﬃcient alternative to the “spike and slab” priors. Most shrinkage
priors can be expressed as diﬀerent scale mixtures of normals. The Bayesian Lasso
prior (Park and Casella, 2008; Hans, 2009), the orthant normal prior (Hans, 2011),
the generalized double Pareto prior (Armagan et al., 2013), the Horseshoe prior (Car-
valho et al., 2010) and the normal-exponential-gamma/normal-gamma prior (Griﬃn
and Brown, 2005, 2010, 2011, 2012) all conform to this well-studied mixture repre-
sentation. The properties of the mixture determine the shrinkage properties of the
prior.
2.2.1
Scale Mixture Representation
Polson and Scott (2012a) represent common shrinkage priors as part of the class
of global-local scale mixture of normals:
βi | τ 2, λ2
i
∼
N(0, τ 2λ2
i )
19

λ2
i
∼
π(λ2
i )
(τ 2, σ2)
∼
π(τ 2, σ2), i = 1, 2, . . . , p
where λ2
i denotes a local variance component and τ 2 denotes the global variance
component. It has been argued by many researchers (using diﬀerent techniques to
justify their claim) that the prior on τ 2 should focus a lot of mass around zero while
the prior on λ2
i should have fat tails. The reasoning behind such thought comes from
the fact that the global scale τ with its substantial mass around zero would help
to shrink noise vigorously while the heavy tails of the local scales λi would nullify
the shrinkage eﬀect on large signals. Marginalizing over the local scale parameter
λi results in a symmetric distribution for βi with a mode at zero and both (left and
right) tails decaying to zero away from the origin. The mode at zero helps to shrink
noise toward zero while presence of suﬃcient prior mass in the tails is necessary
to explain large signals. Tails that are heavier than exponential (e.g. Cauchy tails)
are recommended for the marginal distribution of βi, otherwise the prior will always
shrink coeﬃcient estimates toward zero, even when they correspond to real signals
(Carvalho et al., 2010). The degree of shrinkage in observations that are noise is
controlled by the “spike” of the marginal prior density at the origin and a bigger
spike is more eﬀective in squelching noise to zero.
Following Polson and Scott (2011), some popular shrinkage priors that can be
encompassed within the framework of global-local normal scale mixtures are:
• Double-exponential: λ2
i has an exponential density resulting in the Bayesian
lasso prior (Park and Casella, 2008; Hans, 2009). The Bayesian lasso due to
20

Hans (2009), however, has been characterized by an orthant normal prior (mix-
ture of normal distributions restricted to diﬀerent orthants of the Euclidean
space) rather than by normal scale mixtures.
• Student-t: λ2
i has an inverse-gamma density resulting in a Student-t distri-
bution for βi with ν (say) degrees of freedom. The relevance vector machine
(Tipping, 2001) arises from the t distribution prior with the degrees of freedom
parameter ν →0.
• Normal/Jeﬀreys: λ2
i has an improper prior (Jeﬀreys’ prior) given by π(λ2
i ) ∝
1
λ2
i that results in π(βi) ∝
1
|βi| (Figueiredo, 2003; Bae and Mallick, 2004).
• Horseshoe: λ2
i has an inverted-beta IB(a, b) distribution with parameters a =
b =
1
2 (Carvalho et al., 2010). The horseshoe prior can be envisioned to be
part of the broader class of hypergeometric-beta mixture priors for regression
coeﬃcients by choosing arbitrary values of a and b.
• Strawderman-Berger: This prior is represented as βi | κi ∼N(0, κ−1
i
−1)
and κi ∼Beta
  1
2, 1

(Strawderman, 1971; Berger, 1980).
• Elastic net: The mixing parameter is represented in a slightly diﬀerent form
and the mixing density is inverse-gamma restricted to the (0,1) interval resulting
in the Bayesian elastic net prior (Li and Lin, 2010; Hans, 2011). The Bayesian
elastic net characterization in Hans (2011) is also based on the orthant normal
prior.
21

• Normal-gamma (NG)/Normal-exponential-gamma (NEG): For the NG
prior, λ2
i has a gamma distribution while for the NEG prior, λ2
i has an exponen-
tial distribution with a further gamma mixing distribution for the exponential
rate parameter (Griﬃn and Brown, 2005, 2010, 2011). Another prior used in
sparse regression modeling comes from a linear combination of independent NG
priors inducing prior correlation between the regression coeﬃcients and is called
the correlated normal-gamma (CNG) prior (Griﬃn and Brown, 2012).
• Generalized double Pareto: This prior also originates from an exponential
prior on λ2
i , but instead of assigning a gamma prior on the exponential rate
parameter as in the NEG case, the generalized double Pareto (Armagan et al.,
2013) has the gamma prior on the square root of the rate parameter.
• Dirichlet-Laplace (DL): The DL prior (Bhattacharya et al., 2014) is of a
slightly more general form than the global-local normal scale mixture represen-
tation described earlier. The induced prior on βi is implied by the hierarchical
structure:
βi | τ 2, λ2
i , φ2
i
∼
N(0, τ 2λ2
i φ2
i )
λ2
i
∼
Exp
1
2

(with mean 2)
φ
∼
Dirichlet (a, a, . . . , a).
These prior distributions on the local variance components have been speciﬁcally
developed to produce heavy tails for βi and a spike around zero. The prior speci-
ﬁcation in the shrinkage prior setup is usually completed by assigning an improper
(Jeﬀreys’) prior to the error variance σ2 and in most situations, an inverse-gamma
22

prior to the global scale parameter τ 2. Gelman (2006) and Polson and Scott (2012b)
argue against the routine use of the inverse-gamma prior and instead advise using
a half-Cauchy distribution as the default prior for τ 2.
They show that posterior
inference based on the inverse-gamma prior is highly unstable and sensitive to hyper-
parameter choices, and that even “non-informative” or “vague” inverse-gamma prior
choices, in truth, impart a lot of information to the prior distribution. The half-
Cauchy prior leads to more stable inference without adding to the computational
burden as it allows conditionally conjugate MCMC steps.
Other variations of shrinkage priors and connections to nonparametric procedures
can be found in Polson and Scott (2011), which includes an elaborate discussion on
a more general class of shrinkage priors. A common observation in this regard is the
equivalence between the maximum a posteriori (MAP) estimates from these shrinkage
priors and frequentist solutions to many penalized regression problems. The LASSO
(Tibshirani, 1996), bridge regression (Fu, 1998) and the elastic net (Zou and Hastie,
2005) are classic examples of the penalized regression problem where (penalized) coef-
ﬁcient estimates can be viewed as the mode of the posterior distribution of regression
coeﬃcients given the global variance parameter. The global variance τ 2 is closely
related to the penalty parameter in a penalized likelihood. MAP coeﬃcient estimates
from the conditional posteriors generated by these shrinkage priors lead to sparse
solutions when certain required conditions are satisﬁed by the penalized likelihood
function (Fan and Li, 2001).
23

2.3
Classiﬁcation Based on Prior Knowledge
2.3.1
Subjective Priors
Some researchers also classify prior speciﬁcation into two broad categories: sub-
jective prior elicitation and objective prior speciﬁcation. Subjective prior elicitation
quantiﬁes expert opinion about the regression problem in the form of a prior dis-
tribution on model parameters. The shape of the prior distribution and its hyper-
parameters are chosen to build on the expert’s knowledge. Garthwaite and Dickey
(1988, 1992) and Kadane and Wolfson (1998) stressed the importance of careful prior
speciﬁcation, focusing mainly on the predictive distribution of the response variable.
Garthwaite and Dickey (1988, 1992) elicit a prior distribution by assessing quartiles
of the response variable under an optimal choice of design points. These “optimal”
points are determined by minimizing the interquartile range of the prior predictive
distribution subject to some design constraints.
Kadane and Wolfson (1998) de-
veloped diﬀerent general and application-speciﬁc prior elicitation methods, also by
assessing certain quantiles of the response at “optimal” design points, but used a
diﬀerent criterion for deﬁning optimality.
Another example of subjective prior elicitation is that of a suitable prior being
imposed on the strength of linear regression relationship, with the distribution chosen
according to an expert’s beliefs. Chapter 5 of this dissertation shows how such a prior
indirectly induces a distribution with nice properties on the regression coeﬃcients and
distributes probabilities consistently in the parameter space in accordance with the
expert’s prior knowledge. Bedrick et al. (1996) suggested a new form of an informative
prior distribution, called the conditional means prior, in the generalized linear model
problem. The conditional means prior is built by assigning a prior distribution to
24

each of p (p is the number of covariates) chosen points on the regression surface. The
subjective priors at these design locations collectively induce a prior on the regression
coeﬃcients which has close connections to a data augmentation prior.
2.3.2
Objective Priors
Objective prior speciﬁcation has been much more popular in statistical literature
than subjective prior elicitation primarily because of the discomfort many feel in
specifying a subjective prior distribution, and also because objective prior distribu-
tions are more easily treated from a mathematical perspective. The recent interest in
higher dimensional models has, of necessity, tilted prior speciﬁcation in the direction
of rule-based methods.
The prior distributions recommended for use in model selection diﬀer from those
used in estimation. The conventional choices of objective priors in estimation prob-
lems are typically improper, with the impropriety tied to the dimension of the model.
This creates a new obstacle in model selection as the arbitrary multiplicative con-
stants arising from these improper priors lead to indeterminate Bayes factors. This
obstacle can be overcome by using an improper prior on parameters common to all
of the candidate models, provided the corresponding parameters are orthogonal to
the remaining parameters in the full model (Berger et al., 1998). The non-common
parameters are given proper prior distributions to avoid the arbitrariness of the Bayes
factor.
Bayarri et al. (2012) lay out seven important criteria that any objective prior
should satisfy in order to conform to a coherent and rational statistical analysis. With
25

a view to construct an “uninformative” objective prior so that the likelihood domi-
nates posterior inference, the reference prior (Bernardo, 1979; Berger and Bernardo;
Berger et al., 2009) was developed from an information theoretic view by maximizing
an appropriate measure of distance between the prior and the likelihood.
In some other situations, researchers start with a noninformative improper prior
(generally a reference prior) and then deﬁne a new kind of default Bayes factor, ex-
pressed as a combination of two distinct Bayes factors, one calculated with the entire
sample and the other with a sample of a properly deﬁned “minimal” size. The vari-
ous intrinsic Bayes factors Berger and Pericchi (1996b,a) and fractional Bayes factors
(O’Hagan, 1995) are examples of this approach. Posterior probabilities for all the
models can be directly calculated via the default Bayes factors and the prior distri-
bution on the model space. Although these default Bayes factors are not generated
like real Bayes factors with respect to some inherent prior distribution on the re-
gression parameters, many intrinsic Bayes factors still asymptotically correspond to
procedures developed from proper objective priors known as intrinsic priors (Berger
and Pericchi, 2001).
2.4
Other Discussions on Bayesian Regression
This review of recent (and some not so recent) advances in Bayesian linear re-
gression outlines many attractive and popular methodologies widely practiced by
statisticians all over the world, but the list is by no means close to being exhaustive.
Clyde and George (2004) and Heaton and Scott (2010) assess the major innovations
in Bayesian regression from a diﬀerent perpective. Immense volumes of commendable
research on the regression problem have already been completed within the Bayesian
paradigm and there are many more new techniques emerging with time.
26

Chapter 3: Block g Priors
Mono-shrinkage originating from a single latent scale parameter often has a detri-
mental eﬀect on inference in many common priors (see Section 1.2). Use of a common
scale parameter in the prior covariance matrix of the regression coeﬃcients, as is the
case in the current crop of “g-type” priors, is often not only questionable from a mod-
eling perspective, but, as we show in this chapter, also produces peculiar paradoxical
behavior in the resulting posterior distributions. The block g priors and their mix-
tures are capable of ﬁxing these troubling anomalies and they generate procedures
that correspond to a sound Bayesian analysis.
3.1
Review of g Priors
Suppose we have a set of responses y = (y1, y2, . . . , yn)T and a corresponding set
of p candidate predictors x1, x2, . . . , xp. We consider the traditional setting where
n > p. Let γ ∈Γ = {0, 1}p denote the index set of the subsets of the predictor
variables to be included/excluded in a model so that under a particular model Mγ,
the ith element of the vector γ signiﬁes inclusion of predictor i (in model Mγ) if
γi = 1 and exclusion if γi = 0. Thus, Γ describes the collection of all the 2p models
possible with the p predictor variables and each element γ represents a unique model
in Γ. Let Xγ denote the n × pγ design matrix and βγ denote the pγ length vector of
27

regression coeﬃcients corresponding to model Mγ. Then the linear model Mγ can
be represented as:
y = 1α + Xγβγ + ϵ
with ϵ ∼N(0, σ2In). Assume that a g prior is assigned to the vector of the unknown
parameters (α, βγ, σ2) corresponding to each model Mγ along with an additional
prior on the model space Γ. To keep the implication of the intercept α the same
across all models, without loss of generality, we center the columns of Xγ and the
response y so that 1Ty = 0 and 1TXγ = 0. This transformation allows speciﬁcation
of a ﬂat prior on the common regression parameters (Berger et al., 1998).
3.1.1
Zellner’s g Prior
Zellner’s g prior (Zellner, 1986) is speciﬁed as
π(α, σ2 | Mγ)
∝
1
σ2
βγ | g, σ2, Mγ
∼
N
 0, gσ2(XTγXγ)−1
(3.1)
which results in simple closed form expressions for the marginal likelihoods and Bayes
factors. It is straightforward to show that the Bayes factor for any model Mγ com-
pared to the null model M0 is
BF(Mγ : M0) = p(y | Mγ)
p(y | M0) =
(1 + g)(n−pγ−1)/2
[1 + g(1 −R2γ)](n−1)/2
(3.2)
where R2γ is the coeﬃcient of determination for the model Mγ. Under the standard
squared error loss, the Bayes estimator of βγ is
bβγ
=
g
1 + g
bβγ,LS,
(3.3)
where bβγ,LS is the least squares estimator of βγ.
28

The deﬁnition of the g prior requires speciﬁcation of the unknown parameter g.
A variety of choices have been suggested based on diﬀerent considerations. Some of
the well known (ﬁxed) g priors are:
(i) Unit information prior (Kass and Wasserman, 1995): g = n.
(ii) Risk inﬂation criterion (Foster and George, 1994): g = p2.
(iii) Benchmark prior (Fernandez et al., 2001): g = max(n, p2).
(iv) Local empirical Bayes (Hansen and Yu, 2001):
ˆgEBL
γ
(for model Mγ) = max

R2γ/pγ
(1−R2γ)/(n−pγ−1) −1 , 0

.
(v) Global empirical Bayes (George and Foster, 2000):
ˆgEBG = max
g>0
P
γ
p(Mγ)
(1+g)(n−pγ−1)/2
[1+g(1−R2γ)](n−1)/2.
Liang et al. (2008) review the ﬁxed g priors and summarize the justiﬁcations behind
the use of these speciﬁc values of g.
3.1.2
Mixtures of g Priors
There are a variety of motivations for considering a “fully Bayes” approach where
g is modeled with a prior distribution, leading to a marginal prior for βγ which can
be represented as a “mixture of g priors”
π(βγ | α, σ2, Mγ) =
Z ∞
0
N
 βγ | 0, gσ2(XTγXγ)−1
π(g)dg.
Careful choice of the mixing distribution can result in thick-tailed priors for βγ after
marginalization of g. An early example is the Zellner–Siow prior (Zellner and Siow,
1980), which can be expressed as a mixture of g priors with an inverse-gamma (1
2, n
2)
29

mixing density for g. Mixing over g also endows the Bayes estimator of βγ with
data-adaptive shrinkage of the least squares estimator:
bβγ = E(β | y, Mγ) = E

g
1 + g | y, Mγ

bβγ,LS.
(3.4)
The quantity E(
g
1+g | y, Mγ) is often called the shrinkage factor.
A variety of prior distributions for g have been considered in the literature. Zellner
and Siow (1980), West (2003), Maruyama and Strawderman (2010), Maruyama and
George (2011) and Bayarri et al. (2012) are notable examples.
Hyper-g Priors
The hyper-g prior was proposed by Liang et al. (2008) to overcome deﬁciencies
of the ﬁxed g priors and to lead to a robust estimator that is not overly reliant on a
particular value of g. The main idea is to place a proper prior on g and let the data
dictate the probable values that g can take. The mixture g prior suggested in the
paper imposes the following prior on g:
π(g) = a −2
2
(1 + g)−a/2 , g > 0.
a > 2 is needed for the prior to be proper and the authors suggested anything in
the range 2 < a ≤4 to be quite reasonable, with a = 3 being a default choice. The
hyper-g prior oﬀers a solution to the Lindley and Information paradoxes and serves as
a robust fully Bayesian solution, but it does not suppress the computational beneﬁts
of the original g prior since this particular prior proposal preserves the closed form
representations of the marginal likelihood and the Bayes factors. The Bayes factor
under the hyper-g prior can be expressed as
30

BF(Mγ : M0)
=
Z ∞
0
(1 + g)(n−pγ−1)/2[1 + g(1 −R2
γ)]−(n−1)/2π(g)dg
=
a −2
pγ + a −2
2F1(n −1
2
, 1; a + pγ
2
; R2
γ)
where 2F1(·) is the Gaussian hypergeometric function.
Moreover, the Bayes estimator (posterior mean) for the hyper-g prior can be
expressed as
bβγ = E(β | y, Mγ) = E

g
1 + g | y, Mγ

bβγ,LS
where bβγ,LS is the least squares estimate of the parameter β under model Mγ.
The Bayes estimate is always a shrunken version of the least squares estimate. The
quantity E(
g
1+g | y, Mγ) is called the shrinkage factor since this is the fraction of
the least squares estimate that the Bayes estimate shrinks to.
There is a simple
expression for the posterior expectation of the shrinkage factor for the hyper-g prior.
The hyper-g prior has a nice interpretation in terms of the shrinkage factor since a
change of variables from g leads to
g
1+g ∼Beta(1, a
2 −1).
3.2
Asymptotic Evaluations: Paradoxes, Old and New
While the g priors described in Section 3.1.1 oﬀer many conveniences, they are
known to have several undesirable properties commonly referred to as “paradoxes.”
In their purest form, the paradoxes associated with g priors are revealed when taking a
limit. Liang et al. (2008) describe two such paradoxes which arise from diﬀerent limits.
The ﬁrst, Lindley’s paradox, relies on a limit which weakens the prior distribution.
The second, the Information Paradox, relies on a limit where the signal in the data
becomes stronger. Both limits hold the design X (and hence sample size) ﬁxed. These
two “old” paradoxes can be summarized as follows.
31

Bartlett’s Paradox/Lindley’s Paradox: Lindley’s Paradox is an anomaly associ-
ated with a ﬁxed g prior when the scale factor g is intentionally chosen to be large
in an attempt to make the prior weakly informative. Holding the data (Xγ, y) ﬁxed,
the Bayes factor (3.2) comparing any arbitrary non-null model Mγ to the null model
M0 approaches zero in the limit when g →∞, irrespective of the data. The full
description of the paradox contrasts this undesirable behavior with the results of a
classical hypothesis test (Lindley, 1957; Bartlett, 1957; Jeﬀreys, 1961; Liang et al.,
2008).
Information Paradox: The Information paradox is associated with a strong signal
in the data, as manifested by a high value of R2γ. Holding (Xγ, ϵ) ﬁxed, let ||βγ|| →
∞, so that ||bβγ,LS|| →∞and R2γ →1. It follows from (3.2) that BF(Mγ : M0) →
(1 + g)(n−pγ−1)/2, a ﬁnite constant. Thus the Bayes factor for Mγ relative to M0 is
bounded even though the likelihood evidence in favor of Mγ grows without bound
(Zellner, 1986; Berger and Pericchi, 2001; Liang et al., 2008).
These undesirable properties can be avoided by using mixtures of g priors with a
careful choice of mixing distribution. Liang et al. (2008) provide suﬃcient conditions
under which a prior π(g) resolves the Information Paradox, and prove that the hyper-
g prior avoids both of the above paradoxes (as does the robust prior of Bayarri et al.,
2012). While these “old” paradoxes have been studied extensively, the limits taken
to produce them have further, less well known implications.
The ﬁrst is initially
seen with the limit in Lindley’s Paradox. The second, a new paradox, follows from a
modiﬁcation to Liang et al. (2008)’s limit. Qualitative descriptions of these behaviors
are as follows, with formal results provided in Section 3.2.2.
32

Essentially Least Squares Estimation (ELS): It is well-known that under the g
prior, the Bayes estimator of βγ in (3.3) tends to bβγ,LS as g →∞. Formally, we
identify ELS behavior as ||bβγ −bβγ,LS||/||bβγ,LS|| →0 under some appropriate limit.
In Sections 3.2.1 and 3.2.2, we consider limits which are driven by changes to the
data rather than changes to the prior and show that several common mixtures of g
priors exhibit ELS behavior.
Conditional Lindley’s Paradox (CLP): The Conditional Lindley’s Paradox arises
when comparing two models Mγ1 and Mγ2 with Mγ1 ⊂Mγ2, where Mγ2 is
the “correct” model. Speciﬁc asymptotics for the data (described explicitly in Sec-
tion 3.2.1) yield BF(Mγ2 : Mγ1) →0, compelling one to accept the smaller (incor-
rect) model.
Before connecting these behaviors to existing mixtures of g priors, we describe the
limits driving the phenomena.
3.2.1
A Conditional Information Asymptotic
We consider an asymptotic analysis of a sequence of problems, where each element
in the sequence is related to the linear regression model y = α1 + Xβ + ϵ. The
design matrix X is an n × p matrix with full column rank, and the columns of X
and the response y are centered.
Speciﬁcally, we write the linear model as y =
α1 + X1β1 + X2β2 + ϵ, where X = (X1, X2), X1 is an n × p1 matrix, X2 is an n × p2
matrix and β = (βT
1 , βT
2 )T. We construct a sequence {ΨN}∞
N=1 where each element
ΨN represents the linear model with
ΨN
=
(X1(N), X2(N), αN, β1(N), β2(N), ϵN)
=
(X1, X2, α, β1(N), β2, ϵ),
(3.5)
33

and ||β1(N)|| →∞as N →∞while X1, X2, α, β2 and ϵ are held ﬁxed. This is a
ﬁxed-n, ﬁxed-pγ asymptotic, and represents a strengthening of the likelihood that is
driven by one particular set of predictor variables.
We refer to this as a conditional information asymptotic, as it can be viewed as
the limit that drives the Information Paradox of Liang et al. (2008) (i.e., ||β|| →∞)
with the additional condition that a portion of β remains ﬁxed in the analysis. The
consequences of the information limit considered in Liang et al. (2008) were driven
by R2 →1. The following lemma notes that the conditional information asymptotic
produces the same behavior.
Lemma 3.2.1. Let R2(N) denote the coeﬃcient of determination for element N in
the sequence {ΨN} as deﬁned in (3.5). Then R2(N) →1 as N →∞.
The lemma follows immediately by noting that the error vector ϵ is ﬁxed, and
hence bσ2 = ||y−bαLS1−X bβLS||2
n−p−1
also remains unchanged. R2(N) = ||XbβLS||2/[||XbβLS||2+
(n −p −1)bσ2] which tends to one as ||β1|| →∞.
3.2.2
A Conditional Lindley’s Paradox
In this section we investigate the behavior of several mixtures of g priors under the
conditional information asymptotic deﬁned by (3.5). To streamline notation, we drop
the subscript γ from Mγ and refer to R2(N) simply as R2. The following results apply
to an arbitrary model Mγ unless otherwise mentioned. The ﬁrst theorem reveals a
behavior of the Bayes estimator (bβ) under the hyper-g prior.
Theorem 3.2.1. (ELS) Under the hyper-g prior, for the sequence {ΨN} deﬁned in
(3.5),||bβ−bβLS||
||bβLS||
→0 as N →∞, provided n ≥a + p −1.
34

The proof of the theorem is given in Appendix A.1. The theorem indicates that
when at least one of the coeﬃcients in the model grows really large in size, the esti-
mates from the hyper-g procedure are essentially identical to those from the ordinary
least squares procedure. The failure of the hyper-g in such a situation is worrisome,
and it runs counter to the conventional wisdom that, for a low-information prior, the
small (near zero) coeﬃcients should be shrunk substantially while larger coeﬃcients
should be left unchanged (Berger, 1985). For a single model, we often “know” that
one or some of the regression coeﬃcients are large, and the use of the complex proce-
dure to generate a more sound and robust analysis than simple least squares would
thus appear futile.
Theorem 3.2.2. (CLP) Consider the two models M1 and M2 such that
M1
:
y = α1 + X1β1 + ϵ
M2
:
y = α1 + X1β1 + X2β2 + ϵ
(3.6)
where βi is a vector of length pi, (pi > 0) for i = 1, 2 and p1 + p2 = p. Under the
hyper-g prior, when ||β1|| →∞(i.e, N →∞) in the sequence {ΨN} deﬁned in (3.5)
and n ≥a + p1 −1, the Bayes factor BF(M2 : M1) comparing model M2 to model
M1 goes to zero, irrespective of the data.
The proof of this theorem can be found in Appendix A.2. The import of the
theorem is that when comparing a pair of nested models, if at least one of the regres-
sion coeﬃcients common to both models is quite large compared to the additional
coeﬃcients in the bigger model, then the Bayes factor due to the hyper-g shows un-
warranted bias toward choosing the smaller model. The theorem refers to the limiting
case with the size of the common coeﬃcients growing inﬁnitely large, resulting in the
35

Bayes factor inappropriately choosing the smaller model with probability 1. This
absurd behavior of the Bayes factors which we described as the CLP in Section 3.2
is particularly unsettling since no matter how signiﬁcantly diﬀerent from zero the
additional coeﬃcients in the bigger model are, the more important predictor(s) com-
mon to both the models will bias the Bayes factor towards inferring those additional
coeﬃcients are zero.
The behavior of the posterior of σ2 in the sequence of problems we consider here is
not the reason for the strange behavior of the Bayes factor, as some might envisage.
The posterior distribution of σ2 does not get botched up as N grows, causing the
likelihood to become diﬀuse. Rather, the posterior of σ2 is well-behaved and converges
to a proper distribution with ﬁnite center and spread.
Corollary 3.2.1. Under the hyper-g prior, the posterior distribution of σ2 in the
sequence of problems {ΨN} deﬁned in (3.5) converges to an IG

n+1−a−p
2
,
2
(n−p−1)bσ2

distribution when n > a + p −1.
The proof of this corollary is in Appendix A.3. The CLP coincides with Lindley’s
Paradox for any ﬁxed g prior, and as such it is easily shown that ﬁxed g priors are
also adversely aﬀected by the CLP.
3.2.3
The CLP in Other Mixtures of g Priors
The generalized g prior of Maruyama and George (2011) was developed as a more
general “g-type” prior which can be extended easily to the p > n −1 case. They
specify a prior on β through a prior on the rotated vector W Tβ, where W is deﬁned
through the singular value decomposition X = UDW T. We show that in the simple
36

situation where X is a block orthogonal matrix, the generalized g prior suﬀers from
the CLP under the asymptotics described earlier.
Theorem 3.2.3. Consider the models in (3.6) with the assumption that X1 ⊥⊥X2.
Under the generalized g prior, BF(M2 : M1) →0 as N →∞in the sequence {ΨN}
deﬁned in (3.5), irrespective of the data.
Proof. Using the expressions derived in Maruyama and George (2011) and deﬁning
the two models M1 and M2 as in Theorem 3.2.2, it follows that
BF(M2 : M1)
=
C (1 −R2
1)(n−p1−2)/2−a
(1 −R2
2)(n−p−2)/2−a
(1 −Q2
2)−p/2−a−1
(1 −Q2
1)−p1/2−a−1
where C is a constant, R2
1 = λ2
1+. . .+λ2
p1, R2
2 = τ 2
1 +. . .+τ 2
p , Q2
1 = Pp1
i=1(1−1
νi1)λ2
i and
Q2
2 = Pp
i=1(1 −
1
νi2)τ 2
i . Here λi = corr(ui, y), the correlation between the response y
and ui, the ith principal component of X1 in model M1 and τi is the corresponding
entity for model M2 with X = (X1, X2). νi1 and νi2 are arbitrary constants appearing
in the prior covariance matrix with the restriction that νij ≥νi(j+1) ≥1 for all i, j.
For the asymptotic considered here and because of the block orthogonality as-
sumption (X1 ⊥⊥X2), R2
1 →1 and R2
2 −R2
1 →0 as in Theorem 3.2.2. Since νij ≥1
∀i, j, as N →∞, Q2
1 →η1 and Q2
2 →η2 for some η1, η2 satisfying 1 > η1, η2 ≥0.
lim
||β1||→∞
BF(M2 : M1)
=
lim
z→1
q→0
C
(1 −z)(n−p1−2)/2−a
(1 −z −q)(n−p−2)/2−a
(1 −η2)−p/2−a−1
(1 −η1)−p1/2−a−1
=
C∗× lim
z→1
q→0
(1 −z)(n−p1−2)/2−a
(1 −z −q)(n−p−2)/2−a
=
C∗× lim
z→1(1 −z)(p−p1)/2 = 0.
37

The robust prior of Bayarri et al. (2012) also suﬀers from the CLP.
Theorem 3.2.4. Consider the models in (3.6) with n > p1 + 2. Under the robust
prior of Bayarri et al. (2012) with the recommended hyperparameters (a = 1/2, b = 1
and ρ =
1
p+1), as N →∞in the sequence {ΨN} deﬁned in (3.5), BF(M2 : M1) →0
irrespective of the data.
Proof. As before, consider the setup as in Theorem 3.2.2 and assume that X1 ⊥⊥X2
without any loss of generality. If the design matrix is not block orthogonal, we can
use the technique in the proof of Theorem 3.2.2 since the “robust” prior is also a
speciﬁc mixture of a g prior and transform the design to be block orthognal. Using
expressions derived in Bayarri et al. (2012)
BF(M2 : M1) = C
Q10
Q20
(n−1)/2
2F1
h
(p + 1)/2; (n −1)/2; (p + 3)/2; (1−Q−1
20 )(p+1)
1+n
i
2F1
h
(p1 + 1)/2; (n −1)/2; (p1 + 3)/2; (1−Q−1
10 )(p1+1)
1+n
i
where Q10 = 1 −R2
1 and Q20 = 1 −R2
2.
Since 2F1(a; b; c; z) =
2F1(b; a; c; z),
BF(M2 : M1)
=
C
Q10
Q20
−(n−1)/2
2F1
h
(n −1)/2; (p + 1)/2; (p + 3)/2; (1−Q−1
20 )(p+1)
1+n
i
2F1
h
(n −1)/2; (p1 + 1)/2; (p1 + 3)/2; (1−Q−1
10 )(p1+1)
1+n
i
=
C

1 −z
1 −z −q
(n−1)/2 2F1
h
(n −1)/2; (p + 1)/2; (p + 3)/2; −
(z+q)(p+1)
(1+n)(1−z−q)
i
2F1
h
(n −1)/2; (p1 + 1)/2; (p1 + 3)/2; −
z(p1+1)
(1+n)(1−z)
i
=
C∗

1 −z
1 −z −q
(n−1)/2 R 1
0 t(p−1)/2 
1 +
t(z+q)(p+1)
(n+1)(1−z−q)
−(n−1)/2
dt
R 1
0 t(p1−1)/2

1 +
tz(p1+1)
(n+1)(1−z)
−(n−1)/2
dt
where z = R2
1, q = R2
2 −R2
1, and C and C∗are constants. Here N →∞implies
z ↑1 and q ↓0. This is because R2
2 = R2
1 + (X2 bβ2)T (X2 bβ2)
yTy
due to the block orthogonal
design.
38

Hence
lim
||β1||→∞
BF(M2 : M1)
=
lim
z→1
q→0
C∗

1 −z
1 −z −q
(n−1)/2
R 1
0 t(p−1)/2 
1 +
t(z+q)(p+1)
(n+1)(1−z−q)
−(n−1)/2
dt
R 1
0 t(p1−1)/2

1 +
tz(p1+1)
(n+1)(1−z)
−(n−1)/2
dt
=
lim
z→1
q→0
C∗
R 1
0 t(p−1)/2 
1 −z −q + t(z+q)(p+1)
n+1
−(n−1)/2
dt
R 1
0 t(p1−1)/2

1 −z + tz(p1+1)
n+1
−(n−1)/2
dt
=
lim
z→1 C∗
R 1
0 t(p−1)/2 
1 −z + tz(p+1)
n+1
−(n−1)/2
dt
R 1
0 t(p1−1)/2

1 −z + tz(p1+1)
n+1
−(n−1)/2
dt
, (Monotone Convergence Theorem)
=
lim
z→1 K
R 1
0 t(p−1)/2 (B + t)−(n−1)/2 dt
R 1
0 t(p1−1)/2 (B∗+ t)−(n−1)/2 dt
= 0.
(3.7)
where K is a constant, B = B(z) = (n+1)(1−z)
(p+1)z
and B∗= B∗(z) = (n+1)(1−z)
(p1+1)z . Clearly
both B and B∗go to zero as z →1 and
B
B∗= p1+1
p+1 for all z.
The limit in (3.7) is zero when n > p1 + 2 (see Appendix A.4 for a proof).
The performance of these popular and widely used priors can thus be criticized
on the basis of the CLP. The introduction of the block hyper-g priors in Section 3.3.1
is meant to rectify their ﬂaws, hopefully leading to an improved regression analysis
and to a better quantiﬁcation of uncertainty for the parameters in the model.
3.2.4
Avoiding ELS and CLP Behaviors
The ELS and CLP behaviors described in Section 3.2.2 for mixtures of g priors
arise as a result of the use of a single, latent scale parameter g that is common to
each predictor variable. In order for the model to ﬁt the data in the presence of one
(or more) large coeﬃcients, g must be large (with high probability). Because g aﬀects
estimation of all coeﬃcients (3.4), this has the side-eﬀect that small coeﬃcients are
not shrunk, producing ELS behavior. The CLP can be explained by an argument
39

similar to the one that explains Lindley’s Paradox: as the common parameter g is
driven to be larger and larger (by a portion of the data, in our case) the diminishing
corresponding prior mass in the neighborhood near zero containing any small, nonzero
coeﬃcients eﬀectively rules out these predictors.
As we show in the next section, these behaviors can be avoided through the use
of multiple latent mixing parameters in place of a single, common g. This approach
has a connection to the concept of “local shrinkage,” which has a rich history in the
study of the related normal means problem, e.g. Strawderman (1971) and Berger
(1980). Recent research in this area includes Scott and Berger (2006), Carvalho et al.
(2010), Scott and Berger (2010), Polson and Scott (2012b) and Bhattacharya et al.
(2014). The use of multiple latent scale parameters in regression settings has typically
focused on ridge-regression like settings where regression coeﬃcients are conditionally
independent a priori (e.g., Polson and Scott, 2011; Armagan et al., 2013). Polson
and Scott (2012a) consider local shrinkage in regression where the local shrinkage
parameters are attached to linear combinations of the regression coeﬃcients. A similar
setting is considered by West (2003).
3.3
Block g Priors
Our approach is to endow collections of regression coeﬃcients with their own,
independent, mixture of g priors. Having latent scale parameters gi that are local to
collections of coeﬃcients result in models that avoid ELS and CLP behavior under the
conditions described in Section 3.4. The extreme case where each predictor variable
is associated with its own gi was described, but not pursued, by Liang et al. (2008)
as representing “scale mixtures of independent g priors.” The approach we propose
40

emerges as a more general version of this idea with added theoretical underpinning
related to ELS and the CLP.
Assume that the design matrix X of dimension n × p is of full column rank.
We build a block g prior distribution by partitioning the predictors into k blocks,
X = (X1, X2, . . . , Xk), where Xi is a submatrix of dimension n × pi, i = 1, 2, · · · , k.
The subscript γ is suppressed here to simplify notation. The regression setup for the
block g prior is:
y | α, β, σ2
∼
N(α1 + Xβ, σ2I)
β | g, σ2
∼
N(0, Aσ2)
(3.8)
and π(α, σ2)
∝
1
σ2
where A is a block diagonal matrix deﬁned as
A =





g1(XT
1 X1)−1
0
. . .
0
0
g2(XT
2 X2)−1
. . .
0
...
...
...
...
0
0
. . .
gk(XT
k Xk)−1




.
The k distinct groups of predictors are chosen to be independent a priori and the
separate scales gi are meant to allow diﬀerential shrinkage on distinct blocks with the
amount of shrinkage on a block governed almost entirely by the characteristics of the
concerned group. When the design matrix X is block orthogonal with k orthogonal
blocks X1, X2, . . . , Xk and g = (g1, g2, . . . , gk)T = g1k, the block g prior reduces to
the ordinary g prior.
The block g prior is invariant to a linear reparameterization of the predictors
within a particular block (see Appendix A.5 for a proof of the invariance property).
41

This ensures that coding the predictors diﬀerently within an arbitrary block does not
result in a new prior, as one would expect in any coherent prior speciﬁcation.
3.3.1
The Block Hyper-g Prior
The block hyper-g prior arises as a speciﬁc mixture of the block g priors with
the mixing distribution on each component of the vector g resembling the mixing
distribution of g in an ordinary hyper-g prior. The mixing distributions on the gi’s
are also chosen to be a priori independent so that the data adaptive robustness of gi
has minimal eﬀect on any block j ̸= i. The prior on g is
g
∼
k
Y
i=1
πi(gi)
and πi(gi)
=
a −2
2
(1 + gi)−a/2 , gi > 0.
(3.9)
We follow the recommendations in Liang et al. (2008) regarding the choice of
the hyperparameter a and prescribe a to be bounded by 2 and 4 with a = 3 the
default choice in absence of any prior information. It is possible to allow for diﬀerent
hyperparameters ai, i = 1, 2, . . . , k for each of the individual blocks, but this would
only complicate the analysis and choosing a default value of a1 = a2 = . . . = ak = a
with a = 3 or a = 4 seems to achieve good results in applications.
3.3.2
Sampling from the Block Hyper-g Prior
An important factor working in the favor of the ordinary hyper-g prior is the
availability of closed form expressions for marginal likelihoods and Bayes factors.
Our formulation of the block hyper-g prior does not allow such simple analytic solu-
tions for the marginal likelihood under a general design, but we suggest three eﬃcient
42

ways of drawing samples from the posterior to aid inference. First, a Gibbs sam-
pler for the block hyper-g is simple since most of the full conditionals are standard
distributions admitting eﬃcient sampling techniques. The second approach relies on
marginalization of all parameters other than g. Conditional on g, the model (3.8) is
of completely conjugate form. This allows one to draw samples from the posterior
distribution of g using a Metropolis–Hastings algorithm. Third, when k is small, di-
rect Monte Carlo sampling of g is eﬀective. Estimation of features of the model such
as the marginal distribution of β is improved with use of distributions conditional on
g, whether viewed as Rao–Blackwellization or mere use of conditional distributions.
Gibbs Sampling Steps
The full conditionals in the Gibbs sampling steps are
I. Full conditional for β, deﬁned on Rp:
π(β | α, σ2, g, y) = N

β | (XTX + A−1)−1XT(y −α1) , (XTX + A−1)−1σ2
.
II. Full conditional for σ2, deﬁned on (0, ∞):
π(σ2 | α, β, g, y)
=
IG

σ2 | n + p
2
,
2
(y −α1 −Xβ)T(y −α1 −Xβ) + βTA−1β

.
III. Full conditional for α, deﬁned on R:
π(α | β, σ2, g, y)
=
N

α | 1
n1T(y −Xβ) , σ2
n

.
IV. Full conditional for g, deﬁned on (0, ∞)k:
π(g | α, β, σ2, y) ∝
k
Y
i=1

exp

−
1
2giσ2βT
i (XT
i Xi)βi

g−pi/2
i
(1 + gi)−a/2

.
The full conditional distribution for g is not a standard distribution, unlike the other
full conditionals, and one has to resort to Metropolis–Hastings methods to sample
43

from it. A random-walk Metropolis algorithm with a Gaussian proposal distribution
performs well and allows good mixing in the Gibbs sampling steps.
Instead of generating α and β samples separately, one can perform a joint update
from the full conditional of (α, β). In that case steps (I) and (III) will be replaced by
a single step (V).
V. Full conditional for (α, β), deﬁned on Rp+1:
π(α, β | σ2, g, y) = N
"
(α, β)T


n
1TX
XT1
XTX + A−1
−1  1T
XT

y,

n
1TX
XT1
XTX + A−1
−1
σ2
#
.
We recommend using steps (II), (IV) and (V) in the Gibbs sampler since blocking
of parameters usually leads to better and faster convergence. Sampling from the full
conditionals (I), (II), (III) and (V) is quite simple and eﬃcient, only (IV) demands a
more involved sampling technique. However, this can be handled with little eﬀort by
a random-walk Metropolis method.
Marginalized Posterior Distributions
It is not feasible with a block hyper-g prior to obtain a simple analytical expression
for the marginal likelihood given a particular model. But partial marginalization is
possible and given the value of g, we can integrate out all other parameters in the
model. The joint density involving all the parameters in the model is given by:
π(α, β, σ2, g | y)
∝
Qk
i=1 πi(gi)
σn+p+2
exp
h
−
1
2σ2(y −α1 −Xβ)T(y −α1 −Xβ)
−1
2σ2βTA−1β
i
.
The posterior for g and the posteriors of the other parameters conditional on g are
obtained as:
44

I. Partially Marginalized Posterior Density of β
π(β | g, y) ∝
1

1 +
1
n−1(β −µβ)TR−1(β −µβ)
(n+p−1)/2 , β ∈Rp.
This is the density for a p-variate t-distribution with degrees of freedom (ν) = n −1,
location parameter vector (µ) = µβ = (XT(I −1
nJn)X +A−1)−1XTy and scale matrix
(Σ) = R = yT (I−F)y
n−1
Σβ. Here Σβ = (XT(I −1
nJn)X + A−1)−1 and F = XΣβXT.
II. Partially Marginalized Posterior Density of α
π(α | g, y) ∝
1
h
1 +
1
n−1{(n−1)A2
0
D2
}(α −τα)2
in/2 , α ∈R.
This is the density for a one dimensional t-distribution with degrees of freedom (ν) =
n −1, location parameter (µ) = τα = 1T (I−Fα)y
1T (I−Fα)1 and scale parameter (σ) =
D2
(n−1)A2
0.
Here Fα = X(XTX + A−1)−1XT , D2 = yT(I −Fα)y −[1T (I−Fα)y]2
1T (I−Fα)1 and A2
0 = 1T(I −
Fα)y.
III. Partially Marginalized Posterior Density of σ2
π(σ2 | g, y) ∝
1
σn+1 exp

−1
2σ2yT(I −F)y

, σ2 > 0.
This is the density of an inverse-gamma distribution with shape parameter (α) = n−1
2
and scale parameter (β) =
2
yT (I−F)y. F = X(XT(I −1
nJn)X + A−1)−1XT as deﬁned
in (I).
IV. Posterior Density of g
π(g | y) ∝
hQk
i=1 g−pi/2
i
(1 + gi)−a/2i
|Σβ|1/2
[yT(I −F)y](n−1)/2
, g ∈(0, ∞)k.
As before, Σβ = (XT(I −1
nJn)X + A−1)−1 and F = XΣβXT.
The usefulness of these partly marginalized posterior distributions is evident by
examining the structure of the conditional densities (I), (II) and (III), all of which
are familiar distributions with known moment expressions.
45

3.3.3
Expansion with respect to Least Squares Estimates un-
der the Block g prior
The Bayes estimate of β under the ordinary g prior is E(β | y) =
g
1+g bβLS, which
can never be greater than the least squares estimate bβLS in magnitude. For the block
g prior, the Bayes estimate of β is given by
bβ = E(β | y) = (XTX + A−1)−1XTy
with A deﬁned as in (3.8).
The posterior mean can be rewritten in terms of the least squares estimate using
the Sherman–Morrison–Woodbury matrix identity (Golub and Van Loan, 1996)
bβ
=

(XTX)−1 −(XTX)−1[A + (XTX)−1]−1(XTX)−1	
XTy
=
bβLS −[A(XTX) + I]−1bβLS
=
bβLS −




(g1 + 1)Ip1
g1T12
· · ·
g1T1k
g2T21
(g2 + 1)Ip2
· · ·
g2T2k
· · ·
· · ·
· · ·
· · ·
gkTk1
gkTk2
· · ·
(gk + 1)Ipk




−1
bβLS
where Tij = (XT
i Xi)−1XT
i Xj.
While the g prior always leads to contraction of the Bayes estimator, the block
g prior can result in either contraction or expansion depending on the design matrix
X, the response y, and the vector g = (g1, g2, · · · , gk)T.
For a block orthogonal
design where each Xi ⊥⊥Xj, however, the Bayes estimator bβ will always be shrunk
with respect to the least squares estimate. In this case Tij = 0, ∀i, j and hence
bβ
T = (
g1
1+g1 bβ
T
1,LS, . . . ,
gk
1+gk bβ
T
k,LS), so that each block of the Bayes estimate contracts
relative to the least squares estimate. The amount of shrinkage varies from block to
block, in contrast to the behavior under the g prior where all coeﬃcients experience
46

the same degree of contraction. An illustration of expansion of the estimate for a
simple case is provided below.
Expansion in the Two Groups (Blocks) case
Consider the simplest two blocks case with the regression model deﬁned as
y = α1 + x1β1 + x2β2 + ϵ
where y, x1, and x2 are n × 1 vectors. Further assume that the predictors have been
standardized so that xT
i xi = 1, i = 1, 2 and deﬁne xT
1 x2 = a. Here, a is the correlation
between the two predictors and hence 0 ≤|a| ≤1. The regression model is given
in (3.8). The prior on β = (β1, β2)T is β ∼N(0, Aσ2) with A =
 g1
0
0
g2

since
xT
i xi = 1.
In this situation, the Bayes estimator of the regression coeﬃcients is
bβ = bβLS −
1
g1g2 + g1 + g2 + 1 −g1g2a2
 g2 + 1
−g1a
−g2a
g1 + 1

bβLS.
The conditions for expansion of the individual coordinates (ˆβ1 and ˆβ2) of the Bayes
estimate under squared error loss can be derived explicitly for this simple situation
in terms of the coordinates (ˆβ1,LS and ˆβ2,LS) of the least squares estimate.
Conditions leading to expansion in ˆβ1:
1. aˆβ2,LS < 0 and g1aˆβ2,LS
g2 + 1
< ˆβ1,LS < −
g1aˆβ2,LS
2g1g2(1 −a2) + 2g1 + g2 + 1
2. aˆβ2,LS > 0 and g1aˆβ2,LS
g2 + 1
> ˆβ1,LS > −
g1aˆβ2,LS
2g1g2(1 −a2) + 2g1 + g2 + 1
Conditions leading to expansion in ˆβ2:
1. aˆβ1,LS < 0 and g2aˆβ1,LS
g1 + 1
< ˆβ2,LS < −
g2aˆβ1,LS
2g1g2(1 −a2) + 2g2 + g1 + 1
2. aˆβ1,LS > 0 and g2aˆβ1,LS
g1 + 1
> ˆβ2,LS > −
g2aˆβ1,LS
2g1g2(1 −a2) + 2g2 + g1 + 1
47

The regions of expansion for the ﬁrst and second coordinates of bβ are disjoint,
implying that there can never be expansion in both coordinates simultaneously. This
restriction generalizes in the k blocks scenario to guarantee that not all k components
can simultaneously expand relative to the least squares estimator.
Note that as
a becomes smaller, the regions of expansion begin to shrink and for a = 0 which
corresponds to the case of blockwise orthogonality, the regions of expansion cease to
exist. To understand the eﬀects of g and a, we consider speciﬁc values of a, g1 and g2
and observe how the regions of coeﬃcient expansion change with these parameters.
Figure 3.1: Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs
in areas shaded with vertical lines and expansion of ˆβ2 occurs in areas shaded with
horizontal lines.
• Eﬀect of g
As is evident from Figures 3.1 and 3.2, the regions of coeﬃcient expansion are
determined mainly by the ratio of the gi values, and not so much by the individual
values of g1 and g2. For a constant value of a, the regions of expansion grow larger as
48

the discrepancy between g1 and g2 grows larger, and the wider region corresponds to
the coordinate of g with the larger value. When g1 and g2 are of similar magnitude,
the regions of expansion become narrow but the size of each region is quite strongly
dependent on a.
Figure 3.2: Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs
in areas shaded with vertical lines and expansion of ˆβ2 occurs in areas shaded with
horizontal lines.
• Eﬀect of a
Figures 3.3 and 3.4 show that for any ﬁxed set of values of g1 and g2, the regions of
expansion grow larger as |a| increases. When there is a big diﬀerence in the g values
(or equivalently their ratio is quite high or quite low), the region of expansion for the
larger coordinate of g dominates the other region, and this region further grows in
size with an increment in |a|. When the magnitudes of g1 and g2 are comparable,
increasing the strength of the correlation has the eﬀect of widening the regions of
expansion for both coordinates. The sign of a does not inﬂuence the size of a region
49

Figure 3.3: Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs
in areas shaded with vertical lines and expansion of ˆβ2 occurs in areas shaded with
horizontal lines.
of coeﬃcient expansion, but has an eﬀect on the relative positions of these regions
on the R2 plane. Changing the sign of a while keeping its magnitude ﬁxed ﬂips the
position of each region from one quadrant to its adjacent one and the resulting plot
looks like a mirror image of the one with the opposite sign on a.
• Eﬀect of one gi becoming unbounded
Suppose that one of the components of g, say g1 →∞while the other component
g2 is ﬁxed at a ﬁnite value. In this case the formula for the Bayes estimate becomes
ˆβ1 = ˆβ1,LS +
aˆβ2,LS
g2(1−a2)+1 and ˆβ2 = ˆβ2,LS

g2(1−a2)
g2(1−a2)+1

. The second coordinate of the
Bayes estimate can never be larger than the corresponding least squares estimate in
absolute value. The ﬁrst coordinate on the other hand expands over a huge region
covering the entire ﬁrst and third quadrants and portions of the other two quadrants
depending on the g2 and a values.
50

Figure 3.4: Regions of expansion of the Bayes estimator: Expansion of ˆβ1 occurs in
areas shaded with vertical lines. ˆβ2 does not expand since the region of expansion is
negligible.
Conditions leading to expansion in ˆβ1:
1. aˆβ2,LS < 0 and −∞< ˆβ1,LS < −
aˆβ2,LS
2g2(1 −a2) + 2
2. aˆβ2,LS > 0
and ∞> ˆβ1,LS > −
aˆβ2,LS
2g2(1 −a2) + 2
As mentioned above, ˆβ2 cannot expand.
Signiﬁcance of Coeﬃcient Expansion
The g prior is closely associated with the notion of shrinkage and the hyper-g
prior mixes over a collection of such g priors. The posterior mean of β lies between
its prior mean and the maximum likelihood estimate bβLS.
For prediction of the
response y given a covariate vector x, the posterior predictive mean lies between
the prior predictive mean and the predictive mean based on the plug-in maximum
likelihood estimate xT bβLS. Whether we consider β or prediction of y, each estimate
would shrink toward some point, and so the hyper-g shows shrinkage behavior.
51

The results in this subsection show that the block g and block hyper-g priors
show shrinkage when coupled with a block orthogonal design. However, the block
g prior (and as a consequence any related mixture prior) does not necessarily lead
to shrinkage of coeﬃcient estimates or predictions in any general design. This is, of
course, not a bad property of the priors. Rather, it illustrates the richness of Bayesian
analysis which goes far and beyond stylized descriptions of eﬀects such as shrinkage.
The previous example shows that the block g prior does not cause expansion in the
small or moderate coeﬃcient estimates when the scale parameters gi are carefully
assigned, which is a desirable feature of any reasonable prior.
3.4
Asymptotic Evaluations of the Block Hyper-g Prior
In this section an asymptotic analysis of the block hyper-g prior is carried out,
under a situation identical to the conditional information asymptotic described in
Section 3.2.1, to present the advantages of using a prior with multiple latent scale
parameters. It shows that careful selection of blocks in the block hyper-g prior can
resolve the ELS and CLP behaviors described in Section 3.2 associated with ordinary
mixtures of g priors.
Consider the same model as in (3.8) and (3.9), with y = α1+X1β1+X2β2+· · ·+
Xkβk +ϵ. As in Section 3.2.1, deﬁne the sequence of problems ΨN = (X1(N), .., Xk(N),
αN, β1(N), .., βk(N), ϵN) so that the only quantity that changes in the sequence is the
group of regression pararmeters β1(N). Thus,
ΨN = (X1, . . . , Xk, α, β1(N), . . . , βk, ϵ)
(3.10)
with ||β1(N)|| →∞as N →∞. We make use of the following assumption in many
of the forthcoming theoretical results.
52

Condition 3.4.1. The predictors and the response are centered and the design matrix
is block orthogonal:
1 ⊥⊥y, X1, X2, . . . , Xk and Xi ⊥⊥Xj, where i ̸= j.
The assumption of block orthogonality greatly facilitates the asymptotic analysis
by providing simpler expressions for many posterior summaries. The non-block or-
thogonal design case is considered in Chapter 4. The Bayes estimator (under squared
error loss) for the regression coeﬃcient β given g under the model assumption Con-
dition (3.4.1) becomes:
E(β | g, y)
=
(XTX + A−1)−1XTy
=

g1
1 + g1
bβ
T
1,LS, . . . ,
gk
1 + gk
bβ
T
k,LS
T
(3.11)
where bβi,LS denotes the ith component of the least squares estimator bβLS.
The posterior density of g for the general regression model deﬁned by (3.8), (3.9)
simpliﬁes under Condition (3.4.1) to
π(g | y)
∝
Qk
i=1(1 + gi)−a+pi
2
||y||n−1
h
1 −Pk
i=1
gi
1+giR2
i
i(n−1)/2
where R2
i = yT PXiy
yTy , i = 1, 2, . . . , k and PXi is the projection matrix for the column
space of Xi. Deﬁne ti =
gi
1+gi for i = 1, . . . , k. In the block orthogonal design, each ti
represents the shrinkage factor for the ith block under a block ﬁxed g prior. Then
π(t | y) ∝
k
Y
i=1
(1 −ti)
a+pi
2
−2(1 −
k
X
i=1
tiR2
i )−n−1
2 .
These simpliﬁed forms for the density expressions make the asymptotic analysis
for the block hyper-g prior more tractable. The following lemma, similar to Lemma
3.2.1, is the building block for the main results in this section.
53

Lemma 3.4.1. Assume that the model in (3.8) and (3.9) holds and Condition (3.4.1)
is true. Then R2
1 →1 and R2
i →0, ∀i ̸= 1 as N →∞.
The proof of this lemma is identical to that of Lemma 3.2.1. The next lemma acts
as a crucial precursor to the proofs of theorems in this section and the next one.
Lemma 3.4.2. If f1(tm) and f2(tm) denote properly normalized pdfs on (0, 1) with
f1(tm)
∝
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2
#
(1 −
k
X
i=1
tiR2
i )−n−1
2 dt−m
and f2(tm)
∝
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2
#
(1 −tjR2
j)−n−1
2 dt−m
for some m, j ∈{1, 2, ..., k}, where j may or may not equal m and t−m = {ti : i ̸= m},
then Ef1(tm) ≥Ef2(tm). Strict inequality holds when R2
m > 0 and R2
i > 0, for at least
one i ̸= m.
The proof of the lemma is in Appendix A.6.
The next result describes the shrinkage behavior of the posterior mean of the
regression coeﬃcients under the block hyper-g prior.
Theorem 3.4.1. For the regression model described by (3.8) and (3.9) and satisfying
Condition (3.4.1),
E(β | y) =





E

g1
1+g1 | y

bβ1,LS
...
E

gk
1+gk | y

bβk,LS




.
Further assume that n ≥a + p1 −1. Then, as N →∞in the sequence {ΨN} deﬁned
in (3.10), E

g1
1+g1 | y

→1 and, for i ̸= 1, E

gi
1+gi | y

→∆i with
2
a+pi ≤∆i < 1.
The proof of this theorem can be found in Appendix A.7. The result shows that
the amount of shrinkage is speciﬁc to the block. Under the block hyper-g prior, the
54

mere presence of one or a few very large regression coeﬃcients does not prevent one
from shrinking moderate and small coeﬃcients. The amount of shrinkage is driven
largely by the ratio of yTPXiy and bσ2. The lower bound
2
a+pi occurs when R2
i = 0 in
which case bβi,LS = 0.
The next result shows that the block hyper-g prior with a block orthogonal design
does not suﬀer from the CLP.
Theorem 3.4.2. Consider the two models M1 and M2 such that
M1
:
y = α1 + X1β1 + ϵ
M2
:
y = α1 + X1β1 + X2β2 + ϵ
where βi is a vector of length pi, (pi > 0) for i = 1, 2 and p1 + p2 = p. Assume a block
hyper-g prior on β as in (3.8) and (3.9) (with k = 2) and that blocks X1 and X2
satisfy Condition (3.4.1) on the design. When ||β1|| →∞(N →∞) in the sequence
{ΨN} deﬁned in (3.10), the Bayes factor BF(M2 : M1) comparing model M2 to model
M1 is bounded away from zero.
Proof. For the block hyper-g prior, the expressions for the Bayes Factors BF(Mi :
M0) comparing the models Mi, i = 1, 2 to the null (intercept only) model are
BF(M2 : M0)
=
a −2
2
2 Z 1
0
Z 1
0
2
Y
i=1
(1 −ti)
a+pi
2
−2(1 −
2
X
i=1
tiR2
i )−n−1
2 dt1dt2,
and BF(M1 : M0)
=
a −2
2
Z 1
0
(1 −t1)
a+p1
2
−2(1 −t1R2
1)−n−1
2 dt1.
Thus,
BF(M2 : M1)
=
BF(M2 : M0)
BF(M1 : M0)
=
a −2
2
R 1
0
R 1
0
Q2
i=1(1 −ti)
a+pi
2
−2(1 −P2
i=1 tiR2
i )−n−1
2 dt1dt2
R 1
0 (1 −t1)
a+p1
2
−2(1 −t1R2
1)−n−1
2 dt1
55

≥
a −2
2
R 1
0
R 1
0
Q2
i=1
h
(1 −ti)
a+pi
2
−2(1 −tiR2
i )−n−1
2
i
dt1dt2
R 1
0 (1 −t1)
a+p1
2
−2(1 −t1R2
1)−n−1
2 dt1
=
a −2
2
Z 1
0
(1 −t2)
a+p2
2
−2(1 −t2R2
2)−n−1
2 dt2.
The above inequality follows from the following result which holds for any k ∈N
1 −
k
X
i=1
xi ≤
k
Y
i=1
(1 −xi) , when 0 ≤xi ≤1 ∀i.
(3.12)
As ||β1|| →∞, R2
1 →1 and R2
2 →0, and so
lim
||β1||→∞BF(M2 : M1) ≥a −2
2
Z 1
0
(1 −t2)
a+p2
2
−2dt2 =
a −2
a + p2 −2.
Thus the limiting Bayes Factor is bounded away from zero.
The lower bound is
decreasing in p2, the number of additional predictors in the superset model.
The posterior distribution of σ2 in the sequence {ΨN} under the block hyper-g
prior is also well-behaved.
The posterior for σ2 does not converge to a standard
distribution as N →∞, but Corollaries 3.4.1 and 3.4.2 show that the center of the
limit distribution is ﬁnite and has an upper bound that can be easily calculated.
Corollary 3.4.1. Consider a regression model of the form (3.8) and (3.9) which
satisﬁes Condition (3.4.1) and let p = Pk
i=1 pi. When n > k(a −2) + p + 1, as
N →∞in the sequence {ΨN} deﬁned in (3.10), the sequence of posteriors of σ2
converges to the distribution F(·) with density
f(σ2) ∝
1
(σ2)
n+1
2 −ka+p
2
+k exp
h
−(n −p −1)bσ2
2σ2
i
k
Y
i=2
γ
a + pi
2
−1, (Xibβi)T(Xibβi)
2σ2

where γ(s, x) =
R x
0 ts−1e−tdt is the lower incomplete gamma function.
The proof can be found in Appendix A.8.
56

Corollary 3.4.2. Consider a regression model of the form (3.8) and (3.9) which
satisﬁes Condition (3.4.1). Then in the sequence {ΨN} deﬁned in (3.10), lim
N→∞E(σ2 |
y) ≤
1
n−1−a−p1
h
(n −p −1)bσ2 + Pk
i=2(Xi bβi)T(Xi bβi)
i
when n > a + p1 + 1.
The proof of the corollary is in Appendix A.9. We show that the posterior ex-
pectation of σ2 as N →∞can be bounded from above by a constant involving least
squares estimates of the ﬁxed regression parameters in the sequence. The bound on
the expectation is
1
n−1−a−p1
h
(n −p −1)bσ2 + Pk
i=2(Xi bβi)T(Xi bβi)
i
. This bound is ﬁ-
nite and as a result, E(σ2 | y) is ﬁnite for all the problems in the sequence {ΨN}.
The upper bound for the expectation is achieved by the ordinary hyper-g prior (a
block hyper-g prior with k = 1). In such a case
lim
N→∞E(σ2 | y) = (n −p −1)bσ2
n −p −a −1
which is consistent with Corollary 3.2.1.
3.5
Consistency of the Block Hyper-g Prior
In this section, we analyze the block hyper-g prior with respect to three exist-
ing notions of consistency: information consistency, model selection consistency and
prediction consistency. All three are considered by Liang et al. (2008) with respect
to the hyper-g prior, and the ﬁrst two are two of the seven “criteria for Bayesian
model choice” posited by Bayarri et al. (2012). The hyper-g prior was shown to be
consistent, performing favorably on all three criteria, barring one trivial situation.
The authors proposed the hyper-g/n prior to ﬁx the procedure under that special
situation.
57

3.5.1
Information Consistency
This form of consistency is directly related to the Information Paradox described
in Section 3.2. Liang et al. (2008) deﬁne a Bayesian normal linear regression model
under a particular prior to be information consistent if, under an appropriate limit
on the data vector y for a ﬁxed sample size n, R2γ →1 for model Mγ implies
BF(Mγ : M0) →∞, where M0 is the null model. Bayarri et al. (2012) provide
a formal deﬁnition of information consistency that applies to models other than the
normal linear model. The ordinary hyper-g prior is information consistent when the
condition n ≥a + pγ −1 is satisﬁed. The following theorem conﬁrms that the block
hyper-g prior is as good as the ordinary hyper-g in regard to this form of consistency.
Theorem 3.5.1. Consider a regression model of the form (3.8) and (3.9) which
satisﬁes Condition (3.4.1). The block hyper-g prior is “information consistent” when
either of two suﬃcient conditions hold:
(1) R2γ →1 and n > kγ(a −2) + pγ + 1, where kγ is the total number of blocks,
pγ = Pkγ
j=1 pj,γ and pj,γ is the size of block Xj,γ in model Mγ, or
(2) For some i = 1, . . . , kγ, R2
i,γ →1 and n ≥a+pi,γ−1, where R2
i,γ is the component
of R2γ corresponding to the ith orthogonal block.
The proof of the theorem is in Appendix A.10. Note that Condition (1) provides
a general form of the theorem where R2γ approaches 1 with explosion in size of any
arbitrary set of coeﬃcients.
Condition (2) is a special case when the coeﬃcients
blowing up in size all belong to a single block. Condition (2) is restrictive in the
sense that one needs to have all expanding predictors in a single group and this
forces additonal constraints on the selection of blocks, but Condition (1) states that
58

the block hyper-g prior is information consistent irrespective of the choice of groups.
Condition (1) requires a larger sample size than does Condition (2).
3.5.2
Conditions and Assumptions
For the remaining two consistency results we revert to the traditional asymptotic
setting where parameters are held ﬁxed and the sample size increases. Before pro-
ceeding, we ﬁrst need to ﬁx the notion of a “true” model from which the data y
are assumed to have been generated. Assume that BT ⊆{1, 2, . . . , k} denotes the
indices of the blocks included in the true model MT, where each block has at least
one non-zero coeﬃcient. Then MT : y = αT1 + P
i∈BT
Xiβi,T + ϵ denotes the true data
generating process. Under the model MT there are |BT| = kT diﬀerent blocks with
separate and independent hyper-g priors on each block. The following basic model
assumptions and conditions are used in the results below.
Condition 3.5.1. The n × p design matrix X grows in size with the restriction that
lim
n→∞
1
nXTX = D,
for some p × p positive deﬁnite matrix D.
The following condition is a direct consequence of Condition 3.5.1 (Maruyama and
George, 2011).
Condition 3.5.2. The design allows the true model MT and any arbitrary model
Mγ ̸⊇MT to be asymptotically distinguishable:
lim
n→∞
1
nβT
TXT
T (I −PXγ)XTβT = Vγ > 0
where PXγ = Xγ(XTγXγ)−1XTγ.
59

These conditions are standard assumptions used to establish consistency of Bayesian
procedures. Fernandez et al. (2001), Liang et al. (2008), Maruyama and George (2011)
and Bayarri et al. (2012) also use these conditions (or slight variations) to demonstrate
posterior model selection consistency and prediction consistency of their priors.
3.5.3
Model Selection Consistency
The second form of consistency we are interested in is posterior model selection
consistency which describes the ability of a Bayesian procedure to recover the true
model in the presence of a large number of data points. The ordinary hyper-g prior
has been shown to be model selection consistent when the true model is any model
but the null model. According to Fernandez et al. (2001), the deﬁnition of posterior
consistency for model selection can be expressed as
π(MT | y)
P→1 as n →∞, assuming MT is the true model.
It follows naturally from the relation between posterior probabilities and Bayes factors
that the above consistency criterion is equivalent to:
BF(Mγ : MT)
P→0 as n →∞, for any model Mγ ̸= MT.
(3.13)
In both the relations above, convergence in probability is with respect to the
sample size growing unbounded, i.e., when n →∞. It is simpler to check condition
(3.13) in showing model selection consistency. The following theorem checks whether
or not (3.13) holds for diﬀerent choices of the “true” model under the block hyper-g
prior.
Theorem 3.5.2. Consider a regression model of the form (3.8) and (3.9) which
satisﬁes Conditions 3.4.1, 3.5.1 and 3.5.2. For any model Mγ such that Mγ ⊃MT
60

and all predictors in Mγ that are not in MT are in blocks not in MT, BF(Mγ :
MT)
d→Wγ as n →∞for some non-degenerate random variable Wγ. For all other
models Mγ, BF(Mγ : MT)
P→0.
The block hyper-g prior is not model selection consistent, as the Bayes factor in
(3.13) does not converge to zero in all situations. We refer the readers to Appendices
A.11 and A.16 for a complete proof. Convergence of the Bayes factor to zero depends
on the grouping of predictors in the true model as well as the arbitrary model Mγ.
This is also the case for the hyper-g prior of Liang et al. (2008). The defect in these
priors is that, as n →∞, the priors do not stabilize. This issue for the block hyper-g
prior is addressed in Section 3.6.
3.5.4
Prediction Consistency
Prediction consistency is an important property for any satisfactory Bayesian pro-
cedure. Given a set of n observed data points and predictor values corresponding to
them, the task is to predict the unknown response y∗for a new vector of predictors
x∗∈Rp. When the true model is unknown (as is usually the case in practice), the
optimal prediction with respect to squared-error loss is the Bayesian model averaged
prediction deﬁned as
by∗
n = E(α | y) +
X
γ
π(Mγ | y)x∗TE(β | y, Mγ)
and consistency is achieved when by∗
n
P→E(y∗) = αT +x∗TβT as n →∞. The following
lemma and its extension are used in the main result on prediction consistency of the
block hyper-g prior.
61

Lemma 3.5.1. Consider a regression model of the form (3.8) and (3.9) which satisﬁes
Condition 3.4.1. When MT is the true model, for any i ∈BT
lim
n→∞
Z
(0,1)kT
gi
1 + gi
π(g | MT, y)dg = 1.
The proof of the lemma is in Appendix A.12. Following the proof of the lemma,
it is quite easy to derive the following result:
Corollary 3.5.1. Consider a regression model of the form (3.8) and (3.9) which
satisﬁes Condition 3.4.1. For any model Mγ containing the true model, i.e., for any
Mγ ⊇MT,
lim
n→∞E

gi
1 + gi
| Mγ, y

= 1, if i ∈BT.
The generalization of Lemma 3.5.1 in the form of the preceding corollary is possible
because the only required condition for this extension is 0 < lim
n→∞R2
i,γ < 1, which
is true for any superset model Mγ ⊇MT as long as i ∈BT (see Lemma A.11.1 in
Appendix A.11). The rest of the proof is identical to that of Lemma 3.5.1.
The next theorem states the main conclusion of prediction consistency for the
block hyper-g. Even though the block hyper-g posterior is not consistent in model
selection, the procedure is consistent in prediction under Bayesian model averaging
(BMA).
Theorem 3.5.3. Consider a regression model of the form (3.8) and (3.9) which
satisﬁes Conditions 3.4.1, 3.5.1 and 3.5.2. The predictions under BMA are consistent
for this model.
62

Proof. Using the same notation as in Theorem 3.5.2, we observe that block orthogo-
nality of the design gives
E(β | y, Mγ)
=
E

E(β | y, g, Mγ)

=
Z




gi1
1+gi1
bβi1,γ,LS
...
gikγ
1+gikγ
bβikγ ,γ,LS



π(g | y, Mγ)dg
and E(α | y)
=
bαLS
assuming Bγ = {i1, i2, . . . , ikγ}.
When MT = M0, bβγ,LS
P→0 and bαLS
P→αT for every Mγ since least squares es-
timators are consistent. Thus the model averaged prediction by∗
n converges to E(y∗) =
αT.
Denote the set of all models belonging to Case 2C of Theorem 3.5.2 together with
MT by Ω. We have shown that, as n →∞, π(Mγ | y) →0 for any model Mγ ̸∈Ω.
Thus, lim
n→∞
P
γ:Mγ∈Ω
π(Mγ | y) = 1.
When MT ̸= M0, the least squares estimates are consistent for Mγ ∈Ω, so that
bβi,γ,LS
P→βi,T for i ∈BT and bβi,γ,LS
P→0 for i ̸∈BT. Hence,
lim
n→∞by∗
n
=
αT +
X
γ:Mγ∈Ω
lim
n→∞π(Mγ | y)x∗T ×




βi1,T lim
n→∞
R
gi1
1+gi1 π(g | y, Mγ)dg
...
...
βikγ ,T lim
n→∞
R
gikγ
1+gikγ π(g | y, Mγ)dg



.
Use Lemma 3.5.1 and Corollary 3.5.1 to get lim
n→∞E

gi
1+gi | Mγ, y

= 1 ∀i ∈BT
when Mγ ∈Ω, while 0 ≤E

gi
1+gi | Mγ, y

≤1 for any other i. So,
lim
n→∞by∗
n
=
αT + x∗TβT lim
n→∞
X
γ:Mγ∈Ω
π(Mγ | y) = E(y∗)
indicating that the block hyper-g prior is prediction consistent under BMA.
63

3.6
The Block Hyper-g/n Prior
The block hyper-g prior, despite its desirable properties, can still be criticized
on the basis of its asymptotic performance in model selection.
While prediction
consistency and information consistency are critical and desirable aspects of a sensible
Bayesian procedure, it is troubling that for some choices of the “true” model the
block hyper-g prior cannot guarantee correct identiﬁcation of the model. A similar
problem of posterior model inconsistency plagues the ordinary hyper-g prior when
the null model is true. The hyper-g/n prior was suggested by Liang et al. (2008)
to eliminate the irregularity. In this section we introduce the block hyper-g/n prior
which is similar to the block hyper-g prior, but the prior on g is now scaled by the
sample size.
The good properties of the block hyper-g are retained by the block
hyper-g/n prior. In addition, the block hyper-g/n prior stabilizes as n grows and is
asymptotically consistent (in all three aspects described in Section 3.5) for any choice
of the true model. All of these properties point toward the block hyper-g/n prior
being the preferred mixture of block g priors.
Suppose we have the same design as in (3.8) with the only diﬀerence in the regres-
sion setup appearing in the form of the prior on g. Instead of the usual prior (3.9)
on the individual components of g, a similar prior is placed on each component after
scaling it by the sample size n. The prior on gi is
πi(gi) = a −2
2n

1 + gi
n
−a/2
, i = 1, 2, ..., k.
(3.14)
The gi are independent in the prior as before. We refer to the design setup (3.8) along
with the prior (3.14) as the block hyper-g/n prior. Under the additional Condition
(3.4.1), the posterior distribution on g can be expressed as
64

π(g | y) ∝
k
Y
i=1

(1 + gi)−pi/2 
1 + gi
n
−a/2 "
1 −
k
X
i=1
gi
1 + gi
R2
i
#−(n−1)/2
.
Deﬁne ti =
gi
1+gi , i = 1, 2, .., k. The posterior distribution on t is then
π(t | y) ∝
k
Y
i=1
(
(1 −ti)
a+pi
2
−2

1 −n −1
n
ti
−a
2 )
(1 −
k
X
i=1
tiR2
i )−n−1
2 .
3.6.1
The Conditional Information Asymptotic in the Block
Hyper-g/n Prior
Consider the sequence of problems {ΨN} deﬁned in (3.10) in Section 3.4. The
results in this section display that in the setting of a conditional information asymp-
totic, the desirable properties of the block hyper-g prior are also shared by the block
hyper-g/n prior.
Lemma 3.6.1. If f1(tm) and f2(tm) denote properly normalized pdfs on (0, 1) with
f1(tm)
∝
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2

1 −n −1
n
ti
−a
2 #
(1 −
k
X
i=1
tiR2
i )−n−1
2 dt−m
f2(tm)
∝
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2

1 −n −1
n
ti
−a
2 #
(1 −tjR2
j)−n−1
2 dt−m
for some m, j ∈{1, 2, . . . , k}, where j may or may not equal m and t−m = {ti : i ̸=
m}. Then Ef1(tm) ≥Ef2(tm) and strict inequality holds when R2
m > 0 and at least
one R2
i > 0, i ̸= m.
Proof. The proof is identical to the proof of Lemma 3.4.2.
Lemma 3.6.1 is needed in the proof of the following theorem which demonstrates
that the block hyper-g/n prior does not result in the Bayes estimator emulating the
least squares estimator in presence of a few large predictor coeﬃcients.
65

Theorem 3.6.1. For the regression model described by (3.8) and (3.14) and satisfying
Condition (3.4.1),
E(β | y) =





E

g1
1+g1 | y

bβ1,LS
...
E

gk
1+gk | y

bβk,LS





Further assume that n ≥a + p1 −1. Then, as N →∞in the sequence {ΨN} deﬁned
in (3.10), E

g1
1+g1 | y

→1 and, for i ̸= 1, E

gi
1+gi | y

→∆i with
2
a+pi ≤∆i < 1.
Proof. After establishing the result below, the proof should be identical to that of
Theorem 3.4.1.
P
=
R
(0,1)k
Qk
j=1
h
(1 −tj)
a+pj
2
−2  1 −n−1
n tj
−a
2 i
t1(1 −t1R2
1)−n−1
2 dt
R
(0,1)k
Qk
j=1
h
(1 −tj)
a+pj
2
−2  1 −n−1
n tj
−a
2 i
(1 −t1R2
1)−n−1
2 dt
=
R 1
0 t1(1 −t1)
a+p1
2
−2  1 −n−1
n t1
−a
2 (1 −t1R2
1)−n−1
2 dt1
R 1
0 (1 −t1)
a+p1
2
−2  1 −n−1
n t1
−a
2 (1 −t1R2
1)−n−1
2 dt1
→1
when N →∞or equivalently when R2
1 →1.
Deﬁne h1(t1) =
1
C1(1−t1)
a+p1
2
−2  1 −n−1
n t1
−a
2 (1−t1R2
1)−n−1
2
and h2(t1) =
1
C2(1−
t1)
a+p1
2
−2(1 −t1R2
1)−n−1
2
where C1 and C2 are the respective normalizing constants
for the pdfs h1(·) and h2(·). For any n ≥1, it is clear that the ratio h1(t1)
h2(t1) is non-
decreasing in t1 which according to the Useful Result (UR) in Appendix A.6 implies
that h1 is stochastically larger than h2 and hence Eh1(t1) ≥Eh2(t1).
=⇒
lim
N→∞P
=
lim
N→∞
R 1
0 t1(1 −t1)
a+p1
2
−2  1 −n−1
n t1
−a
2 (1 −t1R2
1)−n−1
2 dt1
R 1
0 (1 −t1)
a+p1
2
−2  1 −n−1
n t1
−a
2 (1 −t1R2
1)−n−1
2 dt1
≥
lim
N→∞
R 1
0 t1(1 −t1)
a+p1
2
−2(1 −t1R2
1)−n−1
2 dt1
R 1
0 (1 −t1)
a+p1
2
−2(1 −t1R2
1)−n−1
2 dt1
= 1.
(3.15)
Imitating the proof of Theorem 3.4.1 and using Lemma 3.6.1, it is easy to show
that E(
g1
1+g1 | y) ≥P so that lim
N→∞E(
g1
1+g1 | y) = 1 as stated in the theorem. The
66

proof for the second part of the theorem is similar to the proof of the corresponding
statement in Theorem 3.4.1.
Similar to the block hyper-g prior, the block hyper-g/n prior also causes block-
speciﬁc shrinkage in regression coeﬃcients. In particular, a block of huge coeﬃcients
will not force inference to be aﬀected by ELS behavior. The lower bound on the
shrinkage factor
2
a+pi is not very tight and our investigations indicate that there are
speciﬁc situations when the bound cannot be achieved at all.
Remark. For the regression model deﬁned by (3.8) and (3.14) with the assumption
of Condition (3.4.1), the limiting posterior distribution of σ2 in the sequence {ΨN}
deﬁned by (3.10) does not have a closed form representation as it does for the ordinary
hyper-g and block hyper-g priors. Nevertheless, it can be shown that lim
N→∞E(σ2 | y)
is ﬁnite, since the bound in Corollary 3.4.2 acts as a conservative upper bound for the
limit of the expectation. To prove this, we use Lemma 3.6.1 and relation (3.15), and
then follow the steps in the proof of Corollary 3.4.2.
The next theorem shows that the block hyper-g/n prior does not suﬀer from the CLP.
Theorem 3.6.2. Consider the two models M1 and M2 such that
M1
:
y = α1 + X1β1 + ϵ
M2
:
y = α1 + X1β1 + X2β2 + ϵ
where βi is a vector of length pi, (pi > 0) for i = 1, 2 and p1 + p2 = p. Assume a block
hyper-g prior on β as in (3.8) and (3.14) (with k = 2) and that blocks X1 and X2
satisfy Condition (3.4.1) on the design. When ||β1|| →∞(N →∞) in the sequence
{ΨN} deﬁned in (3.10), the Bayes factor BF(M2 : M1) comparing model M2 to model
M1 is bounded away from zero.
67

Proof. For the block hyper-g/n prior, the expressions for the Bayes factors BF(Mi :
M0) comparing the models Mi, i = 1, 2 to the null (intercept only) model are given
by
BF(M2 : M0)
=
a −2
2n
2 Z 1
0
Z 1
0
2
Y
i=1
"
(1 −ti)
a+pi
2
−2

1 −n −1
n
ti
−a
2 #
× (1 −
2
X
i=1
tiR2
i )−n−1
2 dt1dt2
and
BF(M1 : M0)
=
a −2
2n
Z 1
0
(1 −t1)
a+p1
2
−2

1 −n −1
n
t1
−a
2
(1 −t1R2
1)−n−1
2 dt1.
Thus,
BF(M2 : M1)
=
a −2
2n
R 1
0
R 1
0
Q2
i=1
h
(1 −ti)
a+pi
2
−2  1 −n−1
n ti
−a
2 i
(1 −P2
i=1 tiR2
i )−n−1
2 dt1dt2
R 1
0 (1 −t1)
a+p1
2
−2  1 −n−1
n t1
−a
2 (1 −t1R2
1)−n−1
2 dt1
≥
a −2
2n
R 1
0
R 1
0
Q2
i=1
h
(1 −ti)
a+pi
2
−2  1 −n−1
n ti
−a
2 (1 −tiR2
i )−n−1
2
i
dt1dt2
R 1
0 (1 −t1)
a+p1
2
−2  1 −n−1
n t1
−a
2 (1 −t1R2
1)−n−1
2 dt1
=
a −2
2n
Z 1
0
(1 −t2)
a+p2
2
−2

1 −n −1
n
t2
−a
2
(1 −t2R2
2)−n−1
2 dt2.
The above inequality follows from the fact that when 0 ≤xi ≤1 ∀i,
1 −
m
X
i=1
xi ≤
m
Y
i=1
(1 −xi) for any m ∈N.
As ||β1|| →∞, R2
1 →1 and R2
2 →0, due to which
lim
||β1||→∞BF(M2 : M1) ≥a −2
2n
Z 1
0
(1 −t2)
a+p2
2
−2

1 −n −1
n
t2
−a
2
dt2.
This lower bound keeps the Bayes factor away from zero in the limit.
68

The lower bound for the limit of the Bayes factor is decreasing in both p2 and
n. As the sample size n →∞, the bound decreases to 0 as one would hope, for the
bound must tend to 0 if we are to establish model selection consistency. A closed
form, conservative lower bound for the above limit is
lim
||β1||→∞BF(M2 : M1)
≥
a −2
2n
Z 1
0
(1 −t2)
a+p2
2
−2

1 −n −1
n
t2
−a
2
dt2
>
a −2
2n
Z 1
0
(1 −t2)
a+p2
2
−2dt2 =
a −2
n(a + p2 −2).
3.6.2
Consistency of the Block Hyper-g/n Prior
We suggest the block hyper-g/n prior as a remedy to the posterior model selection
inconsistency of the block hyper-g prior. However, we do not wish to sacriﬁce the
other consistency properties. Brieﬂy, this scaled version of the block hyper-g prior
inherits the information consistent and prediction consistent behavior from the block
hyper-g while the dependence on n in the prior provides posterior model selection
consistency. We need two extra assumptions, namely Conditions 3.5.1 and 3.5.2 from
Section 3.5.2, to validate asymptotic consistency of the block hyper-g/n prior.
Information Consistency
The suﬃcient conditions needed for information consistency of the block hyper-g
prior are adequate to achieve information consistency in the block hyper-g/n prior.
Theorem 3.6.3. Consider a regression model of the form (3.8) and (3.14) which
satisﬁes Condition (3.4.1). The block hyper-g/n prior is “information consistent”
when either of two suﬃcient conditions hold:
(1) R2γ →1 and n > kγ(a −2) + pγ + 1, where kγ is the total number of blocks,
pγ = Pkγ
j=1 pj,γ and pj,γ is the size of block Xj,γ in model Mγ, or
69

(2) For some i = 1, . . . , kγ, R2
i,γ →1 and n ≥a+pi,γ−1, where R2
i,γ is the component
of R2γ corresponding to the ith orthogonal block.
The proof is similar to the proof of Theorem 3.5.1 and is provided in Appendix
A.13.
Model Selection Consistency
Model selection consistency holds for the block hyper-g/n prior for any choice of
the true model. This means that as n →∞, BF(Mγ : MT) →0 for any arbitrary
model Mγ (̸= MT) and under any assumed true model MT.
Theorem 3.6.4. Consider a regression model of the form (3.8) and (3.14) which
satisﬁes Conditions 3.4.1, 3.5.1 and 3.5.2. The posterior distribution is consistent in
model selection.
The proof of this theorem can be found in Appendix A.14. This result highlights
the success of the block hyper-g/n prior in the only consistency criterion where the
block hyper-g prior fails.
Prediction Consistency
Predictions from the block hyper-g prior have been shown to be consistent under
BMA. Bayesian model averaged predictions are consistent in the sense outlined in
Section 3.5.4 under the block hyper-g/n prior as well.
Theorem 3.6.5. Consider a regression model of the form (3.8) and (3.14) which
satisﬁes Conditions 3.4.1, 3.5.1 and 3.5.2. The predictions under BMA are consistent
for this model.
70

The proof is given in Appendix A.15. The asymptotic consistency of the block
hyper-g/n posterior in prediction further encourages selection of the block hyper-g/n
prior as a default prior choice in many Bayesian variable selection problems.
3.7
Application to Real Data
3.7.1
US Crime Data
In this subsection, the performance of the block hyper-g/n prior is examined on
Ehrlich’s US crime data from 1960 which has 15 predictor variables and 47 cases
resulting in a total of 215 = 32, 768 possible models in the model space. The imple-
mentation of the block hyper-g/n prior hinges on the choice of the group or block
structure in the design matrix. In the presence of subjective information, we recom-
mend grouping similar predictors together since all predictors in a single block share
the same scale (say gi) and the common scale causes their shrinkage pattern to be
identical. In this application a speciﬁc group structure based on theoretical constructs
speciﬁed in Ehrlich (1973) has been used to deﬁne the block g prior. According to
Ehrlich’s economic theory, the collection of potential predictors can be broadly clas-
siﬁed into 3 groups (blocks 1, 2 and 3), and each of the remaining two (possibly
irrelevant) predictors is assigned a separate group. The ﬁve groups according to the
theoretical constructs and the predictors included in each block are:
• Block 1: Pool of potential criminals
Number of young males, education level, labor force participation rate, unem-
ployment rate in age group 14-24, unemployment rate in age group 35-39, sex ratio,
percentage of non-whites.
71

• Block 2: Rewards of crime
Income inequality, wealth of family.
• Block 3: Consequences of crime
Probability of imprisonment, average time served in prisons, police expenditure
in 1959, police expenditure in 1960.
• Block 4: Indicator variable
Indicator for southern states.
• Block 5: Population size
1960 state population.
Following Liang et al. (2008) and earlier analyses of the US crime data, the re-
sponse and the predictor variables (excluding the one binary predictor) are ﬁrst log
transformed and the block hyper-g/n prior is then imparted on the transformed data
as described in (3.8) and (3.14) with the number of blocks k = 5 and a is set to either
of the standard choices a = 3 or a = 4. We add a further Bernoulli variable inclusion
prior on the model space so that each predictor enters a model with prior probability
w. Two diﬀerent choices of w are considered in this example: w = 1
2 resulting in
a uniform prior on the model space and w = 1
3 leading to smaller models being fa-
vored in the prior. All the posterior summaries are calculated using the marginalized
posterior distributions presented in Section 3.4.2, modiﬁed accordingly for the block
hyper-g/n prior.
Figure 3.5 shows how the two procedures—the ordinary hyper-g and the block
hyper-g/n—allocate posterior probabilities diﬀerently to the model space. It is ev-
ident that models having low posterior probabilities under the hyper-g prior get
severely weighted down by the block hyper-g/n prior. This results in a more peaked
72

Figure 3.5: Comparison of log(P(Mγ | y)) under the two priors. The ﬁgures on the
upper panel correspond to prior probability of inclusion of a predictor equal to 1
2 and
the ones on the lower panel correspond to prior probability of inclusion equal to 1
3.
The red line denotes the y = x line.
posterior on the model space under the block hyper-g/n. Zooming in on the models
with high posterior probabilities under both priors, one can see that a majority of
these models fall on the region where the block hyper-g/n weights are larger than
the ordinary hyper-g weights. The magnitude of these model probabilities on the log
scale indicate that these are the models accounting for most of the posterior mass.
73

Investigations show that the posterior distribution over models under the hyper-g
prior is much ﬂatter than it is under the block hyper-g/n prior.
In other words,
the block hyper-g/n prior escalates the posterior weights of the set of models com-
prising the important predictors from each separate block. Numerical evidence of
this unﬂattening phenomenon can be acquired through entropy calculations.
For
a discrete distribution P = {pi; i = 1, . . . , m}, the entropy is calculated as E =
−Pm
i=1 pi log(pi) while entropy of distribution P = {pi; i = 1, . . . , m} relative to
distribution Q = {qi; i = 1, . . . , m} is described by the Kullback-Leibler divergence
KL = Pm
i=1 pi log(pi
qi ). Table 3.1 below shows (1) the entropy of the posterior distri-
bution over the model space and (2) the entropy of the model posterior relative to the
model prior distribution for diﬀerent procedures. The deﬁnition of relative entropy
suggests that small values imply less disparity in the distributions P and Q while
small values of entropy imply less diﬀuseness (uniformity) in the distribution values.
Looking at Table 3.1, it is clear that the entropy for the block hyper-g/n is lower
implying that the posterior distribution under the new prior is more peaked. The en-
tropy of the model posterior relative to the model prior distribution is also larger for
the block hyper-g/n signifying that the block hyper-g/n speciﬁcation imparts more
information to the model space from the likelihood.
Further entropy calculations (shown in Table 3.2) for the posterior distributions
of individual variable inclusion and model size reveal the same picture. There are 15
entropy values for posterior inclusion probabilities of the p = 15 distinct predictor
variables and Table 3.2(B) summarizes the information by presenting the average en-
tropy/relative entropy value across all the 15 variables. The relative entropy (or aver-
age relative entropy) values for the posterior distributions of model size and marginal
74

Criteria
Hg
Hg/n
BHg/n
Hg
BHg/n
(w = 1
2)
(w = 1
2)
(w = 1
2)
(w = 1
3)
(w = 1
3)
Entropy
7.274
7.155
6.879
6.794
6.516
Relative Entropy
3.123
3.243
3.518
3.873
4.216
Table 3.1: Table showing entropy values for the model posterior under diﬀerent priors.
Hg and Hg/n stand for hyper-g and hyper-g/n respectively, BHg/n denotes the block
hyper-g/n prior.
inclusion relative to their corresponding priors are higher for the block hyper-g/n,
again indicating that information from the data is more eﬀectively extracted by the
block hyper-g/n model. The simple entropy values under the block hyper-g/n are not
always the lowest; the block hyper-g/n posteriors for certain summaries are sometimes
more diﬀuse than the ordinary hyper-g ones.
(A)
Hg
Hg/n
BHg/n
Hg
BHg/n
(w = 1
2)
(w = 1
2)
(w = 1
2)
(w = 1
3)
(w = 1
3)
Entropy
1.868
1.863
1.871
1.846
1.883
Relative Entropy
0.121
0.110
0.155
0.422
0.454
(B)
Hg
Hg/n
BHg/n
Hg
BHg/n
(w = 1
2)
(w = 1
2)
(w = 1
2)
(w = 1
3)
(w = 1
3)
Entropy
0.523
0.516
0.502
0.506
0.492
Relative Entropy
0.170
0.177
0.191
0.592
0.607
Table 3.2: Table showing (A) entropy values for posterior distribution of model size
and (B) average entropy values for posterior distributions of marginal inclusion under
diﬀerent priors. Hg, Hg/n and BHg/n stand for hyper-g, hyper-g/n and block hyper-
g/n respectively.
The Bayes estimator of the regression coeﬃcients under squared error loss, E(β |
y, Mγ), for a particular model Mγ has a quite contrasting nature under the diﬀerent
75

Figure 3.6: Distance d = 1−
|| bβ−bβLS||
|| bβLS||
plotted against log posterior probability. Prior
inclusion probability w = 1
2 in the upper panel and w = 1
3 in the lower panel.
Bayesian models. Deﬁning a measure of distance d = 1 −
|| bβ−bβLS||
|| bβLS|| , where bβLS and
bβ denote the least squares and the Bayes estimator (block hyper-g/n or hyper-g)
respectively, the behavior of the two diﬀerent Bayes estimators can be compared
relative to the least squares estimator. The distance measure d simply reduces to the
shrinkage factor E(
g
1+g | y, Mγ) in the hyper-g case. For a general non-orthogonal
design matrix X, the block hyper-g/n prior does not accomodate a simple analytical
expression for d. Figure 3.6 illustrates an inherent diﬀerence in the two models when
76

d is plotted against the logarithm of model posterior probabilities.
The hyper-g
distance shows an explicit pattern for both choices of w, the distinctive curves visible
in the plot indicate identical model size, meaning any two models of the same size
will always fall on the same curve. It is diﬃcult to comprehend any pattern in the
block hyper-g/n plots other than the fact that the high posterior probability models
have d values close to 1, which is true for both models. This is reasonable, because
there should be little diﬀerence between any of the Bayes estimators and the simple
least squares estimator in the cluster of highly likely models. The haphazard clouds
of points in each of the block hyper-g/n plots indicate greater disparity between the
block hyper-g/n and the least squares estimates, caused by the diﬀerent levels of
shrinkage occurring within separate blocks.
The entropy calculations earlier demonstrate how the block hyper-g/n prior in-
duces a more peaked posterior distribution than the ordinary hyper-g. It is expected
that a less diﬀuse posterior would diminish the eﬀect of irrelevant variables and in-
tensify the eﬀect of essential ones. Table 3.3 illustrates how the block hyper-g/n
generates posterior inclusion probabilities with characteristics distinct from the mix-
tures of ordinary g priors. The median probability model (MPM) proposed by Barbieri
and Berger (2004) is deﬁned as the model consisting of predictors, each of which has
posterior inclusion probability greater than 1
2. The predictors belonging to the MPM
under a speciﬁc prior have been marked in the table, representing a set of highly
relevant explanatory variables for predicting crime rate. The MPM is identical under
the hyper-g and hyper-g/n priors, but for each value of w, the MPM under the block
hyper-g/n is of a bigger size. Notice that some of the insigniﬁcant predictors seem
to be weighted down compared to the ordinary hyper-g hinting at the proﬁciency
77

Block
Predictors
Hg
Hg/n
BHg/n
Hg
BHg/n
Number
(w = 1
2)
(w = 1
2)
(w = 1
2)
(w = 1
3)
(w = 1
3)
1
log(M)
0.8430∗
0.8468∗
0.9298∗
0.7086∗
0.8207∗
1
log(Ed)
0.9670∗
0.9704∗
0.9789∗
0.9066∗
0.9179∗
1
log(LF)
0.2261
0.2134
0.2534
0.1296
0.1443
1
log(M.F)
0.2279
0.2157
0.2558
0.1416
0.1488
1
log(NW)
0.6862∗
0.6879∗
0.8331∗
0.4823
0.6327∗
1
log(U1)
0.2725
0.2615
0.3083
0.1533
0.1747
1
log(U2)
0.6075∗
0.6089∗
0.6920∗
0.4133
0.4931
2
log(GDP)
0.3770
0.3670
0.5099∗
0.2265
0.3242
2
log(Ineq)
0.9946∗
0.9956∗
0.9939∗
0.9873∗
0.9811∗
3
log(Po1)
0.6625∗
0.6634∗
0.6477∗
0.6382∗
0.6501∗
3
log(Po2)
0.4655
0.4572
0.4182
0.4277
0.3810
3
log(Prob)
0.8889∗
0.8926∗
0.7847∗
0.7416∗
0.5615∗
3
log(Time)
0.3815
0.3747
0.2967
0.2093
0.1366
4
So
0.2953
0.2838
0.2260
0.1914
0.1249
5
log(Pop)
0.3848
0.3761
0.3008
0.2590
0.1688
Table 3.3: Table showing marginal inclusion probabilities of variables under diﬀerent
posteriors.
Predictors marked with asterisks belong to the corresponding median
probability model.
of the block hyper-g/n prior to single out extraneous variables. Actually the esca-
lation/diminishment of the marginal inclusion probabilities of predictors is greatly
inﬂuenced by the choice of blocks in the design. Groups of signiﬁcant (insigniﬁcant)
predictors identiﬁed and placed in separate blocks have the most conspicuous incre-
ment (decline) in inclusion probabilities.
The predictive performance under the block hyper-g/n prior speciﬁcation on re-
gression coeﬃcients also looks promising. The 47 (= n) cases are randomly split into
seven groups of size 6 and one of size 5 and an eight-fold cross-validation is performed
to compare the cross-validated predictive mean squared errors (MSE) under diﬀerent
priors. Tables 3.4 and 3.5 display the MSE values under Bayesian model averaging
78

Procedure
MSE
Procedure
MSE
g-prior (g = n)
0.4396
Hyper-g (a = 3)
0.4212
g-prior (g = p2)
0.4810
Hyper-g (a = 4)
0.4179
EB-Local
0.4227
Hyper-g/n(a = 3)
0.4308
EB-Global
0.4223
Block Hyper-g/n (a = 3)
0.4152
ZS-Null
0.4271
Block Hyper-g/n (a = 4)
0.4152
ZS-Full
0.4095
Table 3.4: Table displaying the cross-validated predictive MSE (under BMA) for
diﬀerent procedures when w = 1
2.
(BMA) with two diﬀerent Bernoulli variable inclusion prior choices on the model
space. Both the tables show that the block hyper-g/n prior (with a = 3 or a = 4)
does better than the ordinary hyper-g and hyper-g/n priors, and a lot better than
the popular varieties of g priors apart from the full based Zellner–Siow prior. In both
situations w = 1
2 and w = 1
3, the full based Zellner–Siow prior has the minimum MSE
and the block hyper-g/n priors with a = 4 and a = 3 follow right after.
Procedure
MSE
Procedure
MSE
g-prior (g = n)
0.4703
ZS-Full
0.4310
g-prior (g = p2)
0.5118
Hyper-g (a = 3)
0.4537
EB-Local
0.4555
Hyper-g (a = 4)
0.4488
EB-Global
0.4549
Block Hyper-g/n (a = 3)
0.4463
ZS-Null
0.4604
Block Hyper-g/n (a = 4)
0.4407
Table 3.5: Table displaying the cross-validated predictive MSE (under BMA) for
diﬀerent procedures when w = 1
3.
While the block hyper-g/n prior does not seem to do better than ZS-full in predic-
tion, one should note that the block hyper-g/n prior has been formulated according to
79

the null-based Bayes factor approach and the ZS-full prior according to the full-based
approach (see Liang et al. (2008) for a discussion of the null-based and full-based
Bayes factor approaches), which is internally incoherent with respect to overall prior
speciﬁcation. Among all the coherent null-based priors, the block hyper-g/n clearly
has the best out of sample predictive performance. The choice of a seems to have an
eﬀect on the ordinary hyper-g predictions with a = 4 outperforming the standard and
default choice of a = 3. The choice of a = 4 also lowers the MSE in both situations for
the block hyper-g/n predictions, but the amount of improvement is not particularly
eye-catching.
3.7.2
Ozone Data
The ozone data (Breiman and Friedman, 1985) is a data set which has been used
by multiple researchers over the years to test performances of diﬀerent model selec-
tion techniques, both in Bayesian and frequentist frameworks. The data set consists
of daily measurements in 1976 of ozone concentration and eight meteorological vari-
ables over a span of 330 days. Thus, the ozone data comprises 330 observations on
10 variables (including day as a predictor variable) collected with the speciﬁc goal of
predicting future ozone concentrations. A brief description of the data set is provided
in Appendix A.17. Yu et al. (2011) analyzed this data and compared the predictive
performances of a host of automated methods against those arising from human in-
tervened procedures and their novel Bayesian Synthesis method. Many analyses of
the ozone data in earlier research papers have been carried out with log(ozone) as the
response variable. Scatterplots of the response variable against the predictors also
point towards a log transformation being ideal and we pursue the same strategy in
80

this application. First, from a classical regression analysis of the data, we unearth
a set of fairly relevant and signiﬁcant explanatory variables useful to predict log of
ozone concentration. Then we compare the predictive performances of a variety of
Bayesian methods, including the block hyper-g priors, using the discovered set of
essential predictor variables. Our ﬁndings are contrasted with the predictive sum of
squared error (SSE) values from the table appearing in Yu et al. (2011). Following
the strategy undertaken by Yu et al. (2011), the initial analysis to ﬁt the regression
model is executed with a random split of two-thirds (220 observations) of the data
and the predictive performance of each method is tested on the remaining one-third
part (110 observations).
Classical Regression Analysis
The scatterplot matrices of the log(ozone) response variable plotted against the
other nine predictor variables show strong patterns, indicating high importance of
many of the predictors.
The plots suggest a sinusoidal relation between the day
variable and the response. So a linear relation between log(ozone) and sine of day
scaled appropriately to ensure a period of π over the 366 days of the year seems
appropriate.
The variable ibht appears to be truncated at the value of 5000, so
an indicator variable ind.ibht is included to capture any disparate behavior of the
response at the truncation point. Following the addition of the indicator variable and
the sine transformation of the day variable (named sin.day), a backward stepwise
regression results in exclusion of the less signiﬁcant variables like ibtp, hmdt, wdsp and
vdht. Scatterplots and best subsets regression with the adjusted R2 and Mallow’s Cp
criteria indicate that the same set of variables are important. Indicators for dgpg and
vsty corresponding to observations at the truncated value of ibht also seem to be useful
81

predictors. We name the two indicator variables as ind.ibht.dgpg (obtained as the
interaction between ind.ibht and dgpg) and ind.ibht.vsty (interaction between ind.ibht
and vsty). When we allow for higher level terms, variables ibht, vsty, dgpg and sin.day
are found to admit some squared terms and interactions as signiﬁcant predictors.
Among all possible combinations of squared (and subsequently cubic) terms of the
main eﬀects and interactions between variables, none appeared to improve on the
earlier model apart from the interaction term between sin.day and sbtp.
The Durbin–Watson test statistic is calculated to check for autocorrelation in the
log(ozone) residuals, but the value of the statistic is never beyond the universally
accepted critical values in any of the few ﬁnal model choices. Classical regression and
ANOVA summaries for the ﬁnal model (listed below) further conﬁrm the importance
of each of the selected predictors, all of which turn out to be highly signiﬁcant in the
corresponding t and F tests. Since the ind.ibht.dgpg and ind.ibht.vsty variables are
generated like interaction terms, one might be inclined to add the main eﬀect ind.ibht
ﬁrst, but it always failed to be meaningful in presence of the other important ibht
terms. Any Bayesian analysis performed with the model including the ind.ibht term
also invariably led to uniformly worse prediction errors, displaying the deterrent eﬀect
of this variable. However, the two indicator variables ind.ibht.vsty and ind.ibht.dgpg
can be also thought of as independent predictors intended to capture unexplained
variability at the truncation point of ibht (ibht=5000). From this perspective there
is no apparent need to include the ind.ibht term in the model if the other two are
also present. The ﬁnal regression model from the classical analysis then has the fol-
lowing 11 predictors: sbtp, ibht, ibht2, vsty, vsty2, dgpg, dgpg2, sin.day, ind.ibht.dgpg,
ind.ibht.vsty and sin.day*sbtp.
82

Particular tests for inﬂuential observations reveal that some observations may be
inﬂuential, but none of the observations display signiﬁcantly large Cook’s distance
values. However, the 69th observation still appears to be moderately inﬂuential and
we include additional results later on removing that observation while ﬁtting the
model.
Comparison of Diﬀerent Methods in Prediction
Yu et al. (2011) ﬁt diﬀerent Bayesian and frequentist regression models, arising
from human intervention as well as from automated procedures, with a randomly
chosen two-thirds of the data set and used the ﬁtted models to predict the log(ozone)
concentration at speciﬁc covariate values from the remaining one-third of the data.
The predictive sum of squared errors (SSE) for the two analysts who tested predictions
on Data Set 2 (the unused one-third of the data) along with those from the novel
Bayesian Synthesis method and the top six automated procedures for this random
split of the data have been listed in Table 3.6. The result for the Convex Synthesis
prediction error has not been included in Table 3.6 since it is identical to the Bayesian
Synthesis error when implementing the “once and for all” method of Yu et al. (2011).
Procedure
SSE
Procedure
SSE
Analyst 1
12.31
Bagged CART
14.91
Analyst 3
14.21
LASSO
16.76
Mean Human Prediction Error
13.26
AIC
16.91
Bayesian Synthesis
11.93
Forward Stagewise
17.20
BART
11.40
Smoothing Splines
17.21
Table 3.6: Table from Yu et al. (2011) displaying the cross-validated predictive SSE
under diﬀerent methods.
83

We used the same split of the ozone data (Data Set 2 being the test data) and
the set of 11 candidate predictors selected in the classical regression analysis as the
full model for some traditional Bayesian and block hyper-g/n prior implementations.
For the Bayesian analyses with 11 predictor variables, there is a total of 211 = 2, 048
diﬀerent models under consideration, all of which are assumed to be equally likely a
priori. Table 3.7 presents the observed prediction errors from a few popular Bayesian
methods when the entire training data of 220 observations is used to ﬁt the model.
The result for the simple least squares method is also included, displaying how the
more complex procedures improve the log(ozone) predictions from the simplest pos-
sible linear model ﬁt. But speciﬁcation of a block hyper-g/n prior is not complete
without mentioning a group structure among the predictors. We present three subjec-
tively chosen block structures for the block hyper-g/n prior based on slightly diﬀerent
rationale.
Procedure
SSE
Procedure
SSE
g-prior (g = n)
13.32
Hyper-g (a = 3)
13.46
g-prior (g = p2)
13.40
Hyper-g (a = 4)
13.48
EB-Local
13.44
BHg/n (a = 3) Form 1
13.14
EB-Global
13.44
BHg/n (a = 3) Form 2
13.23
ZS-Null
13.41
BHg/n (a = 3) Form 3
13.14
ZS-Full
13.33
Classical linear model
13.79
Table 3.7: Table displaying the cross-validated predictive SSE (under BMA) for dif-
ferent procedures using the entire training data.
• Group structure in the block g prior:
Form 1: Block 1 - sbtp, ibht, vsty, dgpg, sin.day; Block 2 - ibht2, vsty2, dgpg2;
Block 3 - ind.ibht.dgpg, ind.ibht.vsty, sin.day*sbtp.
84

Form 2: Block 1 - sbtp, ibht, vsty, dgpg; Block 2 - ibht2, vsty2, dgpg2;
Block 3 - ind.ibht.dgpg, ind.ibht.vsty; Block 4 - sin.day, sin.day*sbtp.
Form 3: Block 1 - sbtp, ibht, vsty, dgpg, sin.day; Block 2 - ibht2, vsty2, dgpg2;
Block 3 - ind.ibht.dgpg, ind.ibht.vsty; Block 4 - sin.day*sbtp.
In Form 1 the predictors have been grouped with all the linear terms (main ef-
fects) falling in one block, the quadratic terms in a second block and the predictors
resembling interaction terms in another separate block. Form 2 separates the sine
terms and places both of them in a single block due to their similar behavior, while
the indicator terms belong to a distinct group of their own. As before, there are two
more blocks for the remaining linear and quadratic terms. Form 3 is almost identical
to Form 2, the only diﬀerence comes from separating the sine terms and including the
variable sin.day in the ﬁrst block of main eﬀects. Table 3.7 displays the predictive
SSEs from these three forms of block hyper-g/n prior together with the SSEs from
other standard Bayesian priors.
All the models exhibit an enhanced predictive performance when the apparently
inﬂuential 69th observation is removed from consideration while determining the initial
model ﬁt. The predictive SSEs for the same methods in the absence of the inﬂuential
observation are listed in Table 3.8 and the numbers clearly show some improvement
over the corresponding values from Table 3.7.
The SSE values corresponding to all three forms of the block hyper-g/n prior are
the lowest in either situation (with and without the inﬂuential case) among all the
standard Bayesian methods. But the block hyper-g/n priors fail to do better than
Analyst 1, Bayesian Synthesis and BART in this example. High marginal posterior
85

Procedure
SSE
Procedure
SSE
g-prior (g = n)
13.10
Hyper-g (a = 3)
13.23
g-prior (g = p2)
13.17
Hyper-g (a = 4)
13.25
EB-Local
13.22
BHg/n (a = 3) Form 1
12.93
EB-Global
13.22
BHg/n (a = 3) Form 2
13.03
ZS-Null
13.19
BHg/n (a = 3) Form 3
12.94
ZS-Full
13.10
Classical linear model
13.53
Table 3.8: Table displaying the cross-validated predictive SSE (under BMA) for dif-
ferent procedures after discarding the inﬂuential observation from the training data.
inclusion probabilities of predictors under the hyper-g prior as well as the block hyper-
g/n (Form 1) prior imply that all of them are quite important and for a given model,
the Bayes estimates are not much diﬀerent from the least squares estimate. As a
result, diﬀerences between model posterior probabilities for the two Bayesian models
are not that contrasting. Figure 3.7 illustrates how similar the two sets of posterior
probabilities are, but some marginal inclusion probabilities still tend to be upweighted
or downweighted because of the group structure.
sbtp
ibht
ibht2
vsty
vsty2
dgpg
Hyper-g
0.996
0.623
0.986
0.971
0.352
0.497
BHg/n
0.988
0.682
0.994
0.980
0.496
0.367
dgpg2
sin.day
ind.ibht.dgpg
ind.ibht.vsty
sin.day*sbtp
Hyper-g
1.000
0.998
0.937
0.835
0.421
BHg/n
1.000
0.997
0.927
0.800
0.412
Table 3.9: Marginal inclusion probabilities under the block hyper-g/n (Form 1) and
the hyper-g posteriors.
86

Figure 3.7: Comparison of log(P(Mγ | y)) under the hyper-g and block hyper-g/n
(Form 1) priors. The red line denotes the y = x line.
Most of the low probability models seem to have diminished importance under the
block hyper-g/n prior, but the diﬀerence is not very noticeable. Observe that highly
relevant explanatory variables increase the posterior weights of other predictors when
they appear in the same group of the block hyper-g/n prior. The marginal inclusion
probabilities in Table 3.9 verify the occurrence of this phenomenon in the blockwise
prior implementation, which is particularly evident when looking at the strikingly
ﬂuctuating inclusion probabilities of variables vsty2 and dgpg under the two priors.
3.8
Summary
In this chapter, we identify two novel behaviors for Bayesian regression models,
Essentially Least Squares estimation and the Conditional Lindley’s Paradox, that
are exhibited by many common mixtures of g priors in Bayesian regression. Both
87

behaviors stem from the use of a single latent scale parameter that is common to all
regression coeﬃcients. We argue that ELS behavior is, in general, undesirable, as it
precludes shrinkage of small coeﬃcients in the presence of large ones. Similarly, we
argue that priors exhibiting the CLP should be avoided as, asymptotically, they can
provide inﬁnite evidence in support of a false hypothesis. Our analyses are driven by a
new, conditional information asympotic that sheds light on a Bayesian linear model’s
behavior under a strengthening of the likelihood due to one component of the model.
This style of asymptotic is important in practice, as it is often the case that models
are comprised of covariates with coeﬃcients of diﬀering magnitude. We develop the
block hyper-g prior and provide conditions under which the prior does not suﬀer
from either ELS or the CLP. The block hyper-g/n prior follows suit, maintaining the
desirable properties of the block hyper-g prior along with the philosophical advantage
of converging to a ﬁxed limit (for the prior) as n →∞. The block hyper-g/n is our
recommended prior as it yields consistent model selection, an essential criterion for
sensible prior choice, which is lacking in the block hyper-g prior.
88

Chapter 4: Block g Priors in Non-Orthogonal Designs
4.1
Introduction
In Chapter 3, we described new paradoxes which highlight undesirable behaviors
of many popular “g-type” priors. The principal cause for these troubling behaviors is
the mono-shrinkage generated from the single latent scale parameter appearing in the
prior. The block g priors and their mixtures, proposed in Chapter 3, allow for poly-
shrinkage and are capable of avoiding these new paradoxes. However, the theoretical
beneﬁts of the block g priors are mitigated by a few practical concerns.
The primary concerns leading to the development of the block g priors are theoret-
ical, with the desire to avoid Essentially Least Squares behavior and the Conditional
Lindley’s Paradox.
These issues are most easily investigated when the regression
design is block orthogonal. However, in a typical applied problem, apart from exper-
iments speciﬁcally designed to ensure block orthgonality, the regression design is not
block orthogonal. This suggests a need to extend the theory for the block hyper-g
and block hyper-g/n priors.
The secondary concern with a non-orthogonal design is computational. As men-
tioned in Section 3.3.2, the block hyper-g or block hyper-g/n prior does not lead to
a simple closed form expression for the marginal likelihood unlike some well-known
89

mixtures of ordinary g priors. This results in increased computational time for pos-
terior inference. It has been observed throughout Chapter 3 that a block g prior has
simpler expressions for posterior summaries when the groups or blocks of regression
parameters deﬁned in the prior have orthogonal components in the design matrix. It
is advantageous to work with the simpliﬁed posterior from a block orthogonal design
since it allows for faster and simpler computational algorithms for inference.
In this chapter, we introduce a modiﬁed version of the block g prior where the
blockwise prior is imposed on the transformed regression coeﬃcients after reparam-
eterizing the model to admit orthogonal blocks. This approach is similar to the one
adopted by Maruyama and George (2011) in their speciﬁcation of the generalized g
prior. The generalized g prior of Maruyama and George (2011) is a particular mixture
of a “g-type” prior applied to regression coeﬃcients in a rotated orthogonal design.
The new variation of the block g prior discussed in this chapter is the usual prior (de-
scribed in (3.8)) applied to the regression coeﬃcients after transforming the design
by a block Gram–Schmidt orthogonalization process. The dependence of the prior on
the regression design via the covariance matrix is realized through the transformed
design matrix, and not directly through the original design. This allows us to tap
into the beneﬁts of orthogonality for both theory and computation. Permutations of
the blocks leading to diﬀerent orthogonalizations are also considered.
4.2
Block g Prior on a Transformed Design
Consider the problem of modeling the response y as a function of p predictor
variables x1, x2, . . . , xp using a linear regression model. Henceforth, we suppress the
90

subscript γ when working with model Mγ whenever there is no scope for confu-
sion. The following display captures the relationship between the response and the
predictors in the model:
y
=
α1 + Xβ + ϵ
=
α1 + X1β1 + X2β2 + . . . + Xkβk + ϵ
(4.1)
where βp×1 is a vector of regression coeﬃcients, α is an intercept and ϵn×1 is a vector
of mutually independent errors, each of which is assumed to have a Gaussian distri-
bution. Here Xn×p = (X1, X2, . . . , Xk) is an arbitrary design matrix of full column
rank, i.e., rank(X) = p and each Xi is a n × pi submatrix formed by grouping the
p (= Pk
i=1 pi) predictors in k distinct blocks. For the time being, take the predictors
as grouped, and assume that the groups are ordered based on relative importance (the
motivation behind this ordering is discussed later), so that the collection of predictors
in X1 are believed to be most useful while those in Xk are believed to be least useful
in explaining y. Without loss of generality, we assume that the columns of the design
matrix X and response y have been centered.
4.2.1
Transformation to a Block Orthogonal Design
We implement a block Gram–Schmidt process (with submatrices as individual
elements instead of the usual vector elements) to orthogonalize the design matrix
X and generate a block orthogonal matrix Q with k orthogonal blocks. The block
Gram–Schimdt process results in the following matrix Q :
Q1
=
X1
Q2
=
(I −PQ1)X2
91

...
...
Qk
=
(I −PQ1 −PQ2 −. . . −PQk−1)Xk
(4.2)
where PQi = Qi(QT
i Qi)−1QT
i for all i. Clearly Qi ⊥⊥Qj ∀i ̸= j.
It is not diﬃcult to show that X can now be represented as X = QU where
U =







I
T12
T13
· · ·
T1k
0
I
T23
· · ·
T2k
...
...
...
...
...
0
0
0
I
Tk−1,k
0
0
0
0
I







is a block upper triangular matrix with Tij = (QT
i Qi)−1QT
i Xj.
The regression model (4.1) can be expressed as
y
=
α1 + Xβ + ϵ
=
α1 + QUβ + ϵ = α1 + Qτ + ϵ
such that τ = Uβ and the assumptions on ϵ are unchanged. This implies that
τ i = βi +
k
X
j=i+1
Tijβj = βi +
k
X
j=i+1
(QT
i Qi)−1QT
i Xjβj.
(4.3)
A direct consequence of the relation X = QU is that the block matrices {Xi}k
i=1 can
also be expressed in terms of the orthogonal blocks {Qi}k
i=1 as follows:
X1
=
Q1
X2
=
Q2 + Q1T12
...
...
...
Xk
=
Qk + Qk−1Tk−1,k + . . . + Q1T1k.
92

The order in which the blocks Xi are orthogonalized in the block Gram–Schmidt
process plays an important role in determining Q and hence τ. This cascades into
inference, which is demonstrated in Section 4.3.2. As stated earlier, we will only
consider the situtation where the blocks are arranged according to importance and are
successively orthogonalized in the same order with the most important block coming
ﬁrst. For our purposes, the importance of a block is equated to its contribution to
the coeﬃcient of determination (R2), given previous blocks in the model.
4.2.2
The Block Orthogonalized Block g Prior
The block orthogonalized block g prior (BOB g prior) is a block g prior on the
reparameterized regression model y = α1 + Qτ + ϵ and is deﬁned by:
y | α, τ, σ2
∼
N(α1 + Qτ, σ2I)
τ | g, σ2
∼
N(0, Aτ σ2)
(4.4)
and π(α, σ2)
∝
1
σ2
where the matrix Aτ is deﬁned as:
Aτ =





g1(QT
1 Q1)−1
0
· · ·
0
0
g2(QT
2 Q2)−1
· · ·
0
...
...
...
...
0
0
· · ·
gk(QT
k Qk)−1




.
The block orthogonalized block hyper-g (BOB hyper-g) and the block orthogo-
nalized block hyper-g/n (BOB hyper-g/n) priors are deﬁned in the natural way by
assigning prior distributions in the form of (3.9) and (3.14) respectively to the vector
of scale parameters g. The BOB g prior does not directly specify a distribution on
the original regression parameters β, but the prior on τ induces a prior on β = U −1τ.
The generalized g prior of Maruyama and George (2011) is deﬁned in a similar way
93

with the prior on β deﬁned through a distribution on W Tβ instead.
W Tβ | σ2, g ∼N(0, σ2Ψ(g, ν))
for a particular covariance matrix Ψ(g, ν), and W is an orthogonal matrix that diag-
onalizes XTX, i.e., W T(XTX)W = D2 for some diagonal matrix D. The generalized
g prior is also not a direct prior on β, but it is straightforward to derive the induced
prior on the original regression coeﬃcients from the prior on W Tβ.
The beneﬁt of the prior form (4.4) is that the reparameterized regression model
admits orthogonal blocks and this facilitates posterior calculations immensely. After
integrating out all the remaining parameters, the posterior density of g under the
BOB hyper-g prior is simply
π(g | y)
∝
k
Y
i=1
(1 + gi)−a+pi
2
"
1 −
k
X
i=1
gi
1 + gi
R2
i
#−(n−1)/2
where R2
i = yT PQiy
yTy , i = 1, 2, . . . , k. It should be pointed out here that the coeﬃcient
of determination (R2) for the model is split into k additive components based on the
transformed block orthogonal design Q, and not the original non-orthogonal design
X. Hence, R2 = yT PXy
yTy
= yT PQy
yTy
= Pk
i=1
yT PQiy
yTy
= Pk
i=1 R2
i , where R2
i denotes the
ith component of R2 due to the covariates in the ith orthogonal block Qi.
As before, deﬁning ti =
gi
1+gi for i = 1, . . . , k, we get the posterior
π(t | y) ∝
k
Y
i=1
(1 −ti)
a+pi
2
−2(1 −
k
X
i=1
tiR2
i )−n−1
2 .
The posterior for t, rather than the posterior for g, is used in all the posterior com-
putations, because it is more convenient to work with the parameter t. Similarly, the
posterior density of t under the BOB hyper-g/n prior is
π(t | y) ∝
k
Y
i=1

(1 −ti)
a+pi
2
−2
1 −n −1
n
ti
−a
2 
(1 −
k
X
i=1
tiR2
i )−n−1
2 .
94

This leads to the following expressions for Bayes factors:
BF(Mγ : M0) =
a −2
2
k Z
(0,1)k
k
Y
i=1
(1 −ti)
a+pi
2
−2(1 −
k
X
i=1
tiR2
i )−n−1
2 dt
for the BOB hyper-g prior and
BF(Mγ : M0)
=
a −2
2
k Z
(0,1)k
k
Y
i=1
h
(1 −ti)
a+pi
2
−2
1 −n −1
n
ti
−a
2 i
× (1 −
k
X
i=1
tiR2
i )−n−1
2 dt
for the BOB hyper-g/n prior, where M0 denotes the null (intercept only) model.
An important observation here is that a BOB g prior or a mixture of BOB g
priors is not invariant to the order of orthogonalization in the block Gram–Schmidt
process. As a result, in a general non-orthogonal design, the posterior distribution
for regression parameters will vary if the positions of two or more blocks Xi from the
original design are switched in the orthogonalization procedure. However, a BOB g
prior is invariant to model reparameterizations by aﬃne transformation within blocks,
a characteristic which is shared by the block g prior as well. This follows immediately
upon noting that the column space of Qi is unchanged by this type of transformation.
4.3
Properties of the BOB Hyper-g Prior
The motivation behind the development of the block hyper-g and block hyper-
g/n priors is to ﬁx the unappealing behavior of the single scale g priors in presence
of regression coeﬃcients of greatly diﬀering sizes. The two new paradoxes presented
in Section 3.2, the CLP and ELS, reveal the shortcomings of the usual “g-type”
priors. The block hyper-g/n is our recommended prior, as it enjoys a variety of sound
properties and conforms to the prior selection criteria suggested by Bayarri et al.
95

(2012) under the assumption of a block orthogonal design. We introduce the BOB
hyper-g and BOB hyper-g/n priors in this chapter to alleviate the computational
burden of using mixtures of block g priors and to relinquish the restriction of a block
orthogonal design for producing a few crucial theoretical results. But it would be
a cause for concern if the beneﬁts of the block hyper-g/n prior are lost in exchange
for the computational advantage of the BOB g priors. In this section we investigate
the eﬀectiveness of the BOB hyper-g and BOB hyper-g/n priors, with a particular
emphasis on their ability to cope with ELS and the CLP, the two main defects that
inspired the development of the block g priors.
4.3.1
Eﬀect of the CLP and ELS
Consider a sequence of regression problems {ΨN} = {X1(N), . . . , Xk(N), αN, β1(N),
. . . , βk(N), ϵN} deﬁned in an identical way as the sequences in Sections 3.2.1 and
3.4. As before, the only quantity that changes in the sequence {ΨN} is βs for some
s ∈{1, 2, .., k} with ||βs(N)|| →∞as N →∞. Hence,
ΨN = {X1, . . . , Xk, α, β1, . . . , βs(N), . . . , βk, ϵ}
(4.5)
with lim
N→∞||βs(N)|| = ∞.
We make the following assumption to ensure convergence of all the additive com-
ponents of R2 (the R2
i terms) in the sequence {ΨN} in (4.5) to ﬁxed limits.
Condition 4.3.1. For every i = 1, 2, . . . , k, ||τ i||2 = O(M 2
i ), where Mi = max(||βi||,
||βi+1||, . . . , ||βk||).
The relationship between τ and β displayed in (4.3) indicates that the above
condition might not hold if Xi ⊥⊥Xj for some i ̸= j.
Condition 4.3.1 calls for
96

a stronger form of blockwise non-orthogonality that requires columns of the block
matrices Xi not to be orthogonal to particular (sometimes all) columns of other blocks
Xj, j ̸= i. This assumption is automatically satisﬁed by most general non-orthogonal
designs (without any kind of design structure) observed in practical problems, but
does not hold for all designs. For example, when the block matrices are all orthogonal
to each other, i.e., Xi ⊥⊥Xj ∀i ̸= j, Condition 4.3.1 does not hold. But in such a case,
the BOB g prior reduces to the simple block g prior and blockwise orthogonalization
of X becomes unnecessary. We make the above assumption to explore the behavior of
mixtures of BOB g priors in the most general designs without any sort of orthogonality
pattern.
The following lemma furnishes a useful result which is applied in the proofs of
some of the theoretical results from this chapter.
Lemma 4.3.1. If each problem from the sequence {ΨN} deﬁned in (4.5) is or-
thogonalized according to the order mentioned in (4.2), then under Condition 4.3.1,
Ps
j=1 R2
j →1 and R2
i →0 ∀i = s + 1, . . . , k as N →∞.
Proof. For any i ∈{1, 2, . . . , k}, R2
i = yT PQiy
yTy
= (Qi bτ i,LS)T (Qi bτ i,LS)
yTy
. When ||βs|| →
∞, (4.3) and Condition 4.3.1 implies that ||τ j|| →∞∀j = 1, 2, . . . , s and for
j = s + 1, . . . , k, τ j (and hence ||τ j||) is unaﬀected in the sequence of problems. On
the other hand, ||y|| →∞in the sequence.
Since the design matrix Q is block orthogonal, this means that ||bτ j,LS|| →∞, j =
1, . . . , s and all other bτ j,LS are left unaltered as N →∞. But bσ2 =
||y||2
n−p−1(1 −R2)
remains ﬁxed in the sequence of problems implying that R2 →1. The result follows
immediately from the relation R2 = Pk
i=1 R2
i = Pk
i=1
(Qi bτ i,LS)T (Qi bτ i,LS)
yTy
, since y grows
unbounded and only the ﬁrst s components of R2 grow in size simultaneously.
97

The next theorem, a modiﬁed version of Theorem 3.2.2 from Chapter 3, displays
how the ordinary hyper-g prior suﬀers from the CLP.
Theorem 4.3.1. Consider two models M1 and M2
M1
:
y = α1 + Xi1βi1 + . . . + Xilβil + ϵ
M2
:
y = α1 + Xi1βi1 + . . . + Xilβil + Xil+1βil+1 + ϵ
where each βij is a vector of length pij and ij is a unique index from the set {1, 2, . . . , k}
such that the index il′ = s appears in both models. Under the hyper-g prior, when
||βs|| →∞(i.e., N →∞) in the sequence {ΨN} deﬁned in (4.5) and n ≥a +
Pl
j=1 pij −1, the Bayes factor BF(M2 : M1) goes to zero, irrespective of the data.
Proof. This is essentially the same result as in Theorem 3.2.2 for a more general form
of the design matrix X. Since the hyper-g prior is invariant to aﬃne transformations,
this general design can be readily transformed to the setting appearing in Theorem
3.2.2 and the result follows.
Remark. The robust prior of Bayarri et al. (2012) also falls prey to the CLP under
any arbitrary regression design (which may or may not be orthogonal) as established
in Theorem 3.2.4.
We will show that the BOB hyper-g prior deﬁned through the transformed design
also avoids the CLP and ELS when the block orthogonalization of X is done in the
“correct” order.
Recall that the block Gram–Schmidt process is carried out in a
speciﬁc order with the blocks successively orthgonalized according to importance.
For the sequence of regression problems {ΨN} in (4.5), the most important block is
obviously the group of predictors in block Xs, since Xsβs accounts for almost all of
98

the variation in the response. So the block orthogonalization of the design matrix
X is carried out in the same manner as in (4.2), but with Xs recognized as the
most signiﬁcant block, i.e., Q1 = Xs. Theorem 4.3.2 conﬁrms the potential of the
BOB hyper-g prior to handle the CLP when the most important block is correctly
identiﬁed.
The proofs of many of the results from this section will depend on the following two
lemmas. We shall use the notation C or Ci, i ∈N to denote generic ﬁnite constants
which are ﬁxed throughout the sequences unless mentioned otherwise.
Lemma 4.3.2. Let η1 + η2 + . . . + ηk = M such that
lim
N→∞ηi > 0 for each i ∈
{1, 2, . . . , k (≥2)}. Consider the ratio of integrals:
I =
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −t1η1 −t2η2 −. . . −tkηk)−mdt
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −tjL)−mdt
where each pi ∈Z+, m > a+pj
2
+ k −2, a ≥3 and j ∈{1, 2, . . . , k}. Assume that as
N →∞, M →1, L →1 and
1−L
1−M →D, a ﬁnite non-zero constant. If a, m and all
pi’s are ﬁxed for all N, then lim
N→∞I = 0.
The proof for the lemma is in Appendix B.1.
Lemma 4.3.3. Consider the sequence of problems {ΨN} from (4.5) with the block
Gram–Schmidt orthogonalization of X carried out in a prespeciﬁed order. Suppose
that Xs is identiﬁed as the lth most important block and orthogonalized with respect to
the ﬁrst l −1 blocks, where l ∈{1, 2, .., k (≥2)}. If a BOB hyper-g prior is imposed
on the model as in (4.4), then under the assumption of Condition 4.3.1,
(i)
lim
N→∞R2
i = 0 ∀i = l + 1, l + 2, . . . , k.
(ii)
lim
N→∞R2
i > 0 ∀i = 1, 2, ..., l.
99

(iii)
lim
N→∞
lP
i=1
R2
i = 1.
(iv) When n > 2k + 1 and a ≥3,
lim
N→∞
R
kQ
i=1
(1 −ti)
a+pi
2
−2

1 −
kP
i=1
tiR2
i
−n−1
2
dt
= C × lim
N→∞
R
kQ
i=1
(1 −ti)
a+pi
2
−2

1 −
lP
i=1
tiR2
i
−n−1
2
dt, for some ﬁnite C ≥1.
The proof of this result can be found in Appendix B.2.
With the help of these two valuable lemmas we can now establish a theoretical
result on the ability of the BOB hyper-g prior to avoid the CLP.
Theorem 4.3.2. Consider two models M1 and M2
M1
:
y = α1 + Xi1βi1 + . . . + Xilβil + ϵ
M2
:
y = α1 + Xi1βi1 + . . . + Xilβil + Xil+1βil+1 + ϵ
where each βij is a vector of length pij and ij is a unique index from the set {1, 2, . . . , k}
such that the index il′ = s appears in both models. Assume that a BOB hyper-g prior
is imposed on the reparameterized model with Q1 = Xs in both models. Then for the
sequence {ΨN} in (4.5), when n > 2l + 3 and a ≥3, the Bayes factor BF(M2 : M1)
is bounded away from zero as N →∞.
Proof. We can rewrite the two models as
M1
:
y = α1 + Q1τ 1 + Q2τ 2 + . . . + Qlτ l + ϵ
M2
:
y = α1 + Q∗
1τ ∗
1 + Q∗
2τ ∗
2 + . . . + Q∗
l+1τ ∗
l+1 + ϵ
where the ﬁrst block in the block Gram–Schmidt process is Xs for both M1 and M2
(Q1 = Q∗
1 = Xs). The successive blocks might be diﬀerent in M1 and M2 depending
on the placement of the block Xil+1 in the order of orthogonalization.
100

It is not diﬃcult to show that
BF(M2 : M1)
=
a −2
2
R
(0,1)l+1
Ql+1
j=1(1 −tj)
a+pij
2
−2

(1 −Pl+1
j=1 tj(R(2)
j )2)−n−1
2 dt
R
(0,1)l
Ql
j=1(1 −tj)
a+pij
2
−2

(1 −Pl
j=1 tj(R(1)
j )2)−n−1
2 dt
where (R(c)
j )2 is the component of R2 due to the jth orthogonal block Qj or Q∗
j under
model Mc, c = 1, 2.
Due to Lemma 4.3.1,
h
R(2)
1
i2
→1 and
h
R(1)
1
i2
→1 while all other
h
R(c)
j
i2
→0 as
N →∞. Hence,
lim
N→∞BF(M2 : M1)
=
lim
N→∞
a −2
2
R
(0,1)l+1
Ql+1
j=1(1 −tj)
a+pij
2
−2

(1 −Pl+1
j=1 tj(R(2)
j )2)−n−1
2 dt
R
(0,1)l
Ql
j=1(1 −tj)
a+pij
2
−2

(1 −Pl
j=1 tj(R(1)
j )2)−n−1
2 dt
=
C1 lim
N→∞
R
(0,1)l+1
Ql+1
j=1(1 −tj)
a+pij
2
−2

(1 −t1[R(2)
1 ]2)−n−1
2 dt
R
(0,1)l
Ql
j=1(1 −tj)
a+pij
2
−2

(1 −t1[R(1)
1 ]2)−n−1
2 dt
(4.6)
=
C2 lim
N→∞
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[R(2)
1 ]2)−n−1
2 dt1
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[R(1)
1 ]2)−n−1
2 dt1
=
C2
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[ lim
N→∞R(2)
1 ]2)−n−1
2 dt1
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[ lim
N→∞R(1)
1 ]2)−n−1
2 dt1
(4.7)
=
C2
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[ lim
N→∞R(1)
1 ]2[ lim
N→∞
R(2)
1
R(1)
1 ]2)−n−1
2 dt1
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[ lim
N→∞R(1)
1 ]2)−n−1
2 dt1
=
C2
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[ lim
N→∞R(1)
1 ]2)−n−1
2 dt1
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[ lim
N→∞R(1)
1 ]2)−n−1
2 dt1
(4.8)
=
C2 lim
N→∞
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[R(1)
1 ]2)−n−1
2 dt1
R 1
0
h
(1 −t1)
a+pi1
2
−2i
(1 −t1[R(1)
1 ]2)−n−1
2 dt1
(4.9)
=
C2 > 0.
101

The equality (4.6) follows from Lemma 4.3.3 since n > 2l +3. (4.8) is true due to the
relation

R(2)
1
R(1)
1
2
=
yT PQ∗
1y/yTy
yT PQ1y/yTy = yT PXsy/yTy
yT PXsy/yTy = 1 for all N. (4.7) and (4.9) can be
justiﬁed by the Monotone Convergence Theorem since the function (1−t1)
a+p1
2
−2(1−
t1r2)−n−1
2
is increasing in r2.
The theorem shows that the BOB hyper-g prior avoids the CLP if the block Gram–
Schmidt orthogonalization is carried out in the right order. An identical result can
be shown to be true for the BOB hyper-g/n prior as well.
Theorem 4.3.3. Consider two models M1 and M2
M1
:
y = α1 + Xi1βi1 + . . . + Xilβil + ϵ
M2
:
y = α1 + Xi1βi1 + . . . + Xilβil + Xil+1βil+1 + ϵ
where each βij is a vector of length pij and ij is a unique index from the set {1, 2, . . . , k}
such that the index il′ = s appears in both models. Assume that a BOB hyper-g/n
prior is imposed on the reparameterized model with Q1 = Xs in both models. Then for
the sequence {ΨN} in (4.5), when n > 2l+3 and a ≥3, the Bayes factor BF(M2 : M1)
is bounded away from zero as N →∞.
Proof. The proof is very similar to the one for Theorem 4.3.2, we just need one new
result to bound the Bayes factor for the block hyper-g/n prior
BF(M2 : M1)
=
a −2
2
R
(0,1)l+1
Ql+1
j=1(1 −tj)
a+pij
2
−2(1 −n−1
n ti)−a
2

(1 −Pl+1
j=1 tj(R(2)
j )2)−n−1
2 dt
R
(0,1)l
Ql
j=1(1 −tj)
a+pij
2
−2(1 −n−1
n ti)−a
2

(1 −Pl
j=1 tj(R(1)
j )2)−n−1
2 dt
.
If we are able to prove a modiﬁcation of Lemma 4.3.3 (iv) for the BOB hyper-g/n
prior, then the proof of the theorem can be completed with the same steps used in
102

proving Theorem 4.3.2. We will show that under the same setup as in Lemma 4.3.3,
the following result holds:
When n > 2k + 1 and a ≥3,
lim
N→∞
Z
k
Y
i=1
"
(1 −ti)
a+pi
2
−2

1 −n −1
n
ti
−a
2 #  
1 −
k
X
i=1
tiR2
i
!−n−1
2
dt
=
C × lim
N→∞
Z
k
Y
i=1
"
(1 −ti)
a+pi
2
−2

1 −n −1
n
ti
−a
2 #  
1 −
l
X
i=1
tiR2
i
!−n−1
2
dt
(4.10)
for some ﬁnite C ≥1.
Using the same logic as before, we know that the constant C ≥1, but we still
have to prove that C < ∞. Since 1 ≤
 1 −n−1
n ti
−a
2 ≤n
a
2 , we have
R
kQ
i=1
h
(1 −ti)
a+pi
2
−2  1 −n−1
n ti
−a
2 i 
1 −
kP
i=1
tiR2
i
−n−1
2
dt
R
kQ
i=1
h
(1 −ti)
a+pi
2
−2  1 −n−1
n ti
−a
2 i 
1 −
lP
i=1
tiR2
i
−n−1
2
dt
≤
R
kQ
i=1
h
(1 −ti)
a+pi
2
−2 n
a
2
i 
1 −
kP
i=1
tiR2
i
−n−1
2
dt
R
kQ
i=1
h
(1 −ti)
a+pi
2
−2i 
1 −
lP
i=1
tiR2
i
−n−1
2
dt
=
n
ak
2
R
kQ
i=1
h
(1 −ti)
a+pi
2
−2i 
1 −
kP
i=1
tiR2
i
−n−1
2
dt
R
kQ
i=1
h
(1 −ti)
a+pi
2
−2i 
1 −
lP
i=1
tiR2
i
−n−1
2
dt
.
It has been shown in Appendix B.2 that the ratio of integrals in the last display is
bounded from above as N →∞. This is a suﬃcient condition to show that (4.10) is
true.
Both the BOB hyper-g and the BOB hyper-g/n priors are unaﬀected by the ELS
behavior when the most important block Xs is correctly identiﬁed as the ﬁrst block in
103

the transformed model, i.e., when Q1 = Xs. We demonstrate how the BOB hyper-g
prior avoids ELS in the following theorem.
Theorem 4.3.4. For the regression model described by (4.4), (3.9) and (4.2) with
Q1 = Xs,
E(τ | y) =





E

g1
1+g1 | y

bτ 1,LS
...
E

gk
1+gk | y

bτ k,LS




.
Further assume that n ≥a + p1 −1. As N →∞in the sequence {ΨN} deﬁned in
(4.5), E

g1
1+g1 | y

→1 and for i ̸= 1, E

gi
1+gi | y

→∆i with
2
a+pi ≤∆i < 1.
Proof. The result follows directly from the proof of Theorem 3.4.1 in Appendix A.7
once we establish that lim
N→∞R2
1 = 1 and lim
N→∞R2
i = 0, ∀i ̸= 1, in the block orthogonal
design Q. This is obviously true by Lemma 4.3.1 because Q1 = Xs.
Since τ = Uβ for a block upper triangular matrix U, clearly bτ LS = U bβLS due to
the invariance of least squares estimates to linear transformations. Also the relation
E(β | y) = U −1E(τ | y) implies that
||bβB −bβLS||
||bβLS||
= ||U −1bτ B −U −1bτ LS||
||U −1bτ LS||
= ||U −1(bτ B −bτ LS)||
||U −1bτ LS||
where the notation bθB stands for the BOB hyper-g (Bayes) estimate and bθLS stands
for the least squares estimate of the parameter θ. This means that
λp
λ1
||bτ B −bτ LS||
||bτ LS||
≤||bβB −bβLS||
||bβLS||
≤λ1
λp
||bτ B −bτ LS||
||bτ LS||
where λ1 ≥λ2 ≥. . . ≥λp(> 0) are the eigenvalues of (U −1)TU −1.
According to Theorem 4.3.4, ||bτ B−bτ LS||
||bτ LS||
is bounded away from zero as N →∞, so
lim
N→∞
|| bβB−bβLS||
|| bβLS||
must also be bounded away from zero avoiding the ELS behavior.
104

Remark. The BOB hyper-g/n prior also does not suﬀer from ELS when the same
suﬃcient conditions hold. This is a result that can be veriﬁed easily using a modiﬁ-
cation of Theorem 3.6.2 along the lines of Theorem 4.3.4.
4.3.2
Signiﬁcance of the Order of Orthogonalization
We assumed in the last subsection that the group of predictors corresponding to
the exploding coeﬃcient vector βs from the sequence of problems {ΨN} in (4.5) has
been predetermined to be the most important block. Under such an assumption,
many nice properties of the block hyper-g and block hyper-g/n priors carry over to
the BOB hyper-g and BOB hyper-g/n priors. It is an intriguing fact that the same
conclusions do not persist if the order of orthogonalization is switched. Consider a
simple situation with a pair of nested models M1 and M2 with M1 ⊂M2 such that Xs
is identiﬁed as the most important block among all the groups of predictors in M1,
but the extra block of predictors in M2 is incorrectly assigned more importance than
Xs. So Xs is the ﬁrst block in the block Gram–Schmidt orthogonalization process
for model M1 (i.e., Xs = Q1), but it is the second block in case of model M2. (4.11)
displays the exact order in which the orthogonalization is implemented in the two
models.
Model M1
Model M2
Q1 = Xs
Q∗
1 = Xil+1
Q2 = (I −PQ1)Xi1
Q∗
2 = (I −PQ∗
1)Xs
...
...
...
...
Ql = (I −PQ1 −... −PQl−1)Xil
Q∗
l = (I −PQ∗
1 −... −PQ∗
l−1)Xil−1
Q∗
l+1 = (I −PQ∗
1 −... −PQ∗
l )Xil
(4.11)
105

Theorem 4.3.5. Consider two models M1 and M2
M1
:
y = α1 + Xi1βi1 + . . . + Xilβil + ϵ
M2
:
y = α1 + Xi1βi1 + . . . + Xilβil + Xil+1βil+1 + ϵ
where each βij is a vector of length pij and ij is a unique index from the set {1, 2, . . . , k}
such that the index il′ = s appears in both models. Assume that a BOB hyper-g prior
is imposed on the reparameterized versions of models M1 and M2 described in (4.11).
Then for the sequence {ΨN} deﬁned in (4.5), when n > (a + ps + 1) ∨(2l + 3), a ≥3
and Condition 4.3.1 holds, the Bayes factor BF(M2 : M1) goes to zero as N →∞.
Proof. As before, we can rewrite the two models as
M1
:
y = α1 + Q1γ1 + Q2γ2 + . . . + Qlγl + ϵ
M2
:
y = α1 + Q∗
1γ∗
1 + Q∗
2γ∗
2 + . . . + Q∗
l+1γ∗
l+1 + ϵ
where the orthogonal blocks Qi and Q∗
i are deﬁned by (4.11).
Deﬁne [R(c)
j ]2 in the same way as in Theorem 4.3.2 and notice that (R(1)
1 )2 →1
and (R(2)
1 )2 + (R(2)
2 )2 →1 as N →∞due to Lemma 4.3.1 and Condition 4.3.1.
Hence,
lim
N→∞BF(M2 : M1)
=
lim
N→∞
a −2
2
R
(0,1)l+1
Ql+1
j=1(1 −tj)
a+pij
2
−2

(1 −Pl+1
j=1 tj(R(2)
j )2)−n−1
2 dt
R
(0,1)l
Ql
j=1(1 −tj)
a+pij
2
−2

(1 −Pl
j=1 tj(R(1)
j )2)−n−1
2 dt
=
C1 lim
N→∞
R
(0,1)l+1
Ql+1
j=1(1 −tj)
a+pij
2
−2

(1 −t1[R(2)
1 ]2 −t2[R(2)
2 ]2)−n−1
2 dt
R
(0,1)l
Ql
j=1(1 −tj)
a+pij
2
−2

(1 −t1[R(1)
1 ]2)−n−1
2 dt
106

=
C2 lim
N→∞
R
(0,1)2
Q2
j=1(1 −tj)
a+pij
2
−2

(1 −t1[R(2)
1 ]2 −t2[R(2)
2 ]2)−n−1
2 dt1dt2
R
(0,1)2
Q2
j=1(1 −tj)
a+pij
2
−2

(1 −t1[R(1)
1 ]2)−n−1
2 dt1dt2
.
The last two steps follow from Lemma 4.3.3 along with the additional assumption
of Condition 4.3.1. To apply Lemma 4.3.3, we need n > 2l + 3.
If we deﬁne M = (R(2)
1 )2 + (R(2)
2 )2, L = (R(1)
1 )2, then as N →∞we have M →1,
L →1 and
1−L
1−M =
1−[R(1)
1 ]2
1−[R(2)
1 ]2+[R(2)
2 ]2 =
O(1)/O(||βs||2)
O(1)/O(||βs||2) = O(1) = D, a ﬁnite non-zero
constant. Furthermore, when n > a+ps +1, all the prerequisites for applying Lemma
4.3.2 are satisﬁed and we conclude that lim
N→∞BF(M2 : M1) = 0.
The Bayes factor BF(M2 : M1) equals zero in the limit which indicates that the
peculiar behavior associated with the CLP does not disappear with all conceivable
versions of the BOB hyper-g prior.
A generalization of Theorem 4.3.5 shows that if any number of extra blocks of
predictors from the larger model are considered more important than the group of
predictors in block Xs and Xs is orthogonalized with respect to the new blocks, then
CLP persists in the BOB hyper-g model when the sample is suﬃciently large. Before
stating this result, ﬁrst consider the following pair of nested models M1 and M2,
where Xs is considered to be the dth most important block in the larger model M2.
Assume that r, l ≥1, d ≥2 and j1, j2, . . . , jd−1 ∈{l + 1, l + 2, . . . , l + r}.
Model M1
Model M2
Q1 = Xs
Q∗
1 = Xij1
Q2 = (I −PQ1)Xi1
Q∗
2 = (I −PQ∗
1)Xij2
(4.12)
...
...
...
...
107

...
...
Q∗
d = (I −PQ∗
1 −... −PQ∗
d−1)Xs
Ql = (I −PQ1 −... −PQl−1)Xil
...
...
Q∗
l+r = (I −PQ∗
1 −... −PQ∗
l+r−1)Xil+r
Corollary 4.3.1. Consider two models M1 and M2
M1
:
y = α1 + Xi1βi1 + . . . + Xilβil + ϵ
M2
:
y = α1 + Xi1βi1 + . . . + Xilβil + . . . + Xil+rβil+r + ϵ
where each βij is a vector of length pij and ij is a unique index from the set {1, 2, . . . , k}
such that the index il′ = s appears in both models. Assume that a BOB hyper-g prior
is imposed on the reparameterized models M1 and M2 described in (4.12). Then for
the sequence {ΨN} deﬁned in (4.5), when n > (a + ps + 2d −3) ∨(2l + 2r + 1), a ≥3
and Condition 4.3.1 holds, the Bayes factor BF(M2 : M1) goes to zero as N →∞.
It is straightforward to come up with a version of Corollary 4.3.1 for the BOB
hyper-g/n prior, which reveals that it suﬀers from the CLP under identical conditions.
Corollary 4.3.2. Consider two models M1 and M2
M1
:
y = α1 + Xi1βi1 + . . . + Xilβil + ϵ
M2
:
y = α1 + Xi1βi1 + . . . + Xilβil + . . . + Xil+rβil+r + ϵ
where each βij is a vector of length pij and ij is a unique index from the set {1, 2, . . . , k}
such that the index il′ = s appears in both models. Assume that a BOB hyper-g/n
prior is imposed on the reparameterized models M1 and M2 described in (4.12). Then
for the sequence {ΨN} deﬁned in (4.5), when n > (a+ps+2d−3)∨(2l+2r+1), a ≥3
and Condition 4.3.1 holds, the Bayes factor BF(M2 : M1) goes to zero as N →∞.
108

The proofs for Corollaries 4.3.1 and 4.3.2 rely on a minor modiﬁcation of the steps
in the proof of Theorem 4.3.5.
Theorem 4.3.6. For the regression model described by (4.4), (3.9) and (4.2),
E(τ | y) =





E

g1
1+g1 | y

bτ 1,LS
...
E

gk
1+gk | y

bτ k,LS




.
Suppose that Xs is considered as the lth (l ≥2) most important block of predictors in
the sequence {ΨN} deﬁned in (4.5) so that Ql = Xs. When n > 1+Pl
j=1 pj +l(a−2),
a ≥3 and Condition 4.3.1 holds, E

gi
1+gi | y

→1 for i = 1, 2, . . . , l and for all other
i, E

gi
1+gi | y

→∆i with
2
a+pi ≤∆i < 1, as N →∞.
The proof of this theorem is in Appendix B.4. The conclusion about shrinkage
factors for the transformed regression coeﬃcients τ does not directly relate to the
ELS behavior for estimates of β. But the following corollary shows that a modiﬁed
form of ELS, which we call the Componentwise Essentially Least Squares estimation
or CELS, aﬀects inference in the sequence {ΨN}.
In this situation, we have the
Bayes estimates (under squared error loss) coinciding with least squares estimates for
some components of β, while the remaining components experience shrinkage in their
coeﬃcient estimates depending on the quality of the predictor variables. Ideally, we
would want only the Bayes estimate of βs to be close to its least squares counterpart,
but the next corollary demonstrates that CELS aﬀects coeﬃcient estimates in all
blocks orthogonalized prior to Xs.
Corollary 4.3.3. (CELS) Under the same setup as in Theorem 4.3.6 for the sequence
{ΨN}, we have lim
N→∞
|| bβi,B−bβi,LS||
|| bβi,LS||
= 0, for i = 1, 2, .., l (≥2) and the limit is non-zero
for all other i.
109

Proof. The relation β = U −1τ implies that
||bβi,B −bβi,LS||
||bβi,LS||
=
|| Pk
j=i U ij(bτ j,B −bτ j,LS)||
|| Pk
j=i U ijbτ j,LS||
=
|| Pk
j=i E(
1
1+gj | y)U ijbτ j,LS||
|| Pk
j=i U ijbτ j,LS||
≤
Pk
j=i ||E(
1
1+gj | y)U ijbτ j,LS||
|| Pk
j=i U ijbτ j,LS||
where U ij is the (i, j)th block matrix in U −1. U −1 is block upper triangular since U
is block upper triangular and it can be shown that U ii = I ∀i.
Since ||βs|| →∞and Xs is the lth orthogonalized block, O(||bτ j||2) = O(||βs||2)
for all i ∈{1, 2, ..., l}. Hence, when i ∈{1, 2, ..., l},
lim
N→∞
||bβi,B −bβi,LS||
||bβi,LS||
≤
lim
N→∞
Pk
j=i E(
1
1+gj | y)||U ijbτ j,LS||
|| Pk
j=i U ijbτ j,LS||
=
lim
N→∞
l
X
j=i
E

1
1 + gj
| y
 O(||βs||2)
O(||βs||2)
+ lim
N→∞
k
X
j=l+1
E

1
1 + gj
| y

O(1)
O(||βs||2).
The ﬁrst part of the last expression equals zero because E

1
1+gj | y

= 1 −
E

gj
1+gj | y

tends to zero for each j = 1, 2, . . . , l when n is suﬃciently large (by
Theorem 4.3.6). The second part also equals zero as lim
N→∞||βs|| = ∞. This proves
that lim
N→∞
|| bβi,B−bβi,LS||
|| bβi,LS||
= 0, for i = 1, 2, .., l.
When i = l + 1, l + 2, . . . , k,
||bβi,B −bβi,LS||
||bβi,LS||
=
|| Pk
j=i E(
1
1+gj | y)U ijbτ j,LS||
|| Pk
j=i U ijbτ j,LS||
,
which does not involve βs in any way and the ratio of the ﬁnite, non-zero numerator
and denominator is always non-zero, even in the limit as N →∞.
110

The same result regarding CELS in the BOB hyper-g prior is also true in the BOB
hyper-g/n prior given the aforementioned set of suﬃcient conditions.
4.4
Consistency of the BOB Hyper-g and BOB Hyper-g/n
Priors
In this section, we study the properties of the BOB hyper-g and BOB hyper-g/n
priors with respect to the three important aspects of asymptotic consistency discussed
in Liang et al. (2008): information consistency, model selection consistency and pre-
diction consistency. The analysis of these consistency criteria is almost identical to
the asymptotic analyses in Sections 3.5 and 3.6.2, with only a slight diﬀerence in the
set of all possible models due to distinctive model spaces.
Consider a regression model with predetermined blocks of predictor variables and
assume that the total number of blocks in the full model with all p predictors is
K(∈N). The model space under consideration is of a smaller size than the usual
model space containing all possible combinations of all p covariates. The set of models
only includes all combinations of the K blocks from the full model and, thus, has 2K
elements. Let z ∈ΓB = {0, 1}K be a vector that denotes the presence (if zi = 1) or
absence (if zi = 0) of the ith block of predictors in an arbitrary model Mz. Let kz
denote the number of blocks in the model Mz.
As in Section 3.5, in order to show model selection consistency and prediction
consistency in Sections 4.4.2 and 4.4.3, we have to ﬁrst deﬁne a “true” model which
generates the data. Let MT : y = αT1+ P
i∈BT
Xi,Tβi,T +ϵ denote the true model, where
BT ⊆{1, 2, . . . , K} is the set of block indices in the true model and |BT| = kT. The
“true” reparameterized model, generated by the block Gram–Schmidt process, can
111

be represented as MT : y = αT1 + P
i∈BT
Qi,Tτ i,T + ϵ, and (4.2) and (4.3) connect the
two alternative forms of the same model. The standard assumption for consistency
of Bayesian methods in linear models (Fernandez et al., 2001; Liang et al., 2008;
Maruyama and George, 2011; Bayarri et al., 2012) is considered to hold in the form
of Condition 3.5.1.
4.4.1
Information Consistency
The notion of information consistency arises from preventing the Information
Paradox described in Section 3.2. A Bayesian normal linear regression model is in-
formation consistent, if for a ﬁxed sample size n and a ﬁxed design, an appropriate
limit on the likelihood causing R2z →1 for an arbitratry model Mz implies that
BF(Mz : M0) →∞. Theorems 4.4.1 and 4.4.2 verify that the BOB hyper-g and
hyper-g/n priors are information consistent.
Theorem 4.4.1. Consider a regression model with a BOB hyper-g prior as described
in (4.4), (4.2) and (3.9). The BOB hyper-g is “information consistent” when n >
kz(a −2) + pz + 1, where kz is the total number of blocks, pz = Pkz
j=1 pj,z and pj,z is
the size of block Xj,z in model Mz.
Theorem 4.4.2. Consider a regression model with a BOB hyper-g/n prior as de-
scribed in (4.4), (4.2) and (3.14). The BOB hyper-g/n is “information consistent”
when n > kz(a −2) + pz + 1, where kz is the total number of blocks, pz = Pkz
j=1 pj,z
and pj,z is the size of block Xj,z in model Mz.
The proofs of the above theorems follow immediately from the proofs of Theorems
3.5.1 and 3.6.3 in Appendices A.10 and A.13 once we note that R2z = yT PXzy
yTy
=
yT PQzy
yTy
= Pkz
i=1
yT PQi,zy
yTy
= Pkz
i=1 R2
i,z.
112

4.4.2
Model Selection Consistency
Model selection consistency and prediction consistency are derived under the usual
asymptotic setting related to increasing sample size and ﬁxed parameters. Condition
3.5.1 needs to be true for both of these consistency properties to hold in the hyper-
mixtures of BOB g priors. Because the original and the reparameterized design ma-
trices are related as X = QU, Condition 3.5.1 on the design X implies an equivalent
condition on the block orthogonal n × p design matrix Q:
lim
n→∞
1
nQTQ = D∗
(4.13)
for some p × p positive deﬁnite matrix D∗.
This follows directly from
1
nQTQ =
1
n(U −1)T(XTX)U −1 = (U −1)T  1
n(XTX)

U −1.
Maruyama and George (2011) show that (4.13) is a precursor to the following
condition:
lim
n→∞
1
nτ T
TQT
T(I −PQz)QTτ T = Vz > 0.
(4.14)
Both (4.13) and (4.14) are essential to derive the subsequent results on model
selection consistency. According to Fernandez et al. (2001), posterior consistency in
model selection is attained when
π(MT | y)
P→1 as n →∞, assuming MT is the true model,
or equivalently, if
BF(Mz : MT)
P→0 as n →∞, for any model Mz ̸= MT.
(4.15)
Mixtures of BOB g priors behave identical to the mixtures of block g priors in
regards to posterior model selection. Theorems 4.4.3 and 4.4.4 show that the BOB
113

hyper-g prior is not consistent in model selection in some situations, but the BOB
hyper-g/n is always consistent. Before presenting the main results from this section,
a modiﬁcation of Lemma A.11.1 from Appendix A.11 is ﬁrst stated, which will be
used in the proofs of both the main results.
Lemma 4.4.1. Let R2
i,z and R2
i,T denote the ith component of R2 under an arbitrary
model Mz and the true model MT.
(i) For i ∈BT, R2
i,T
P→Fi,T > 0.
(ii) For i ∈BT, R2
i,z
P→F ′
i,z > 0.
(iii) There exists a set Hz ⊂Bz\BT ⊂{1, 2, . . . , k} depending on the order of block
orthogonalization such that for i ∈Hz, R2
i,z
P→0. For all such i, nR2
i,z
d→cχ2
pi,z =
O(1) for a constant c.
(iii) For i ∈(Bz\BT) ∩Hcz, R2
i,z
P→F ′′
i,z > 0.
(iv) For any model Mz ⊃MT,

1−R2
T
1−R2z
n
is bounded from above in probability.
The proof of this lemma is similar to the proof of Lemmas B.2 and B.3 from
Maruyama and George (2011) and the proof of Lemma A.11.1. The lemma signiﬁes
that based on how the blocks in a model Mz are orthogonalized, the components
of R2 in Mz which belong to blocks not included in the “true” model might have a
non-zero limit with increasing sample size. The other parts of the lemma are identical
to Lemma A.11.1.
Theorem 4.4.3. Consider a regression model with a BOB hyper-g prior as described
in (4.4), (4.2) and (3.9) which satisﬁes 4.13 and 4.14. This model does not have
posterior model selection consistency for all choices of the true model.
114

Proof. Let R2
i,T and pi,T represent the component of R2 and the number of predic-
tors in the ith block Qi,T from the true model MT while R2
i,z and pi,z denote the
corresponding entities for block Qi,z from an arbitrary model Mz (̸= MT). Further
assume that Bz denotes the set of indices of the blocks within Mz, and let kz = |Bz|.
We only prove the theorem for the case a > 3, as in Appendix A.11, and argue that
extension to the case 2 < a ≤3 is analogous to the proof in Appendix A.16.
First consider the case when MT ̸= M0. Following the notation and the steps
of the proof of Theorem 3.5.2 and utilizing Lemmas A.11.1 and A.11.2, we can show
that for large m = n−1
2
(same as having large n),
BF(Mz : MT)
≈
O(mkT −qz) exp
"
−m log
m(1 −R2z)
m −bz

+ m log
m(1 −R2
T)
m −bT

+
X
i∈Bz
bi,z log
 bi,z(1 −R2z)
R2
i,z(m −bz)

−
X
i∈BT
bi,T log
 
bi,T(1 −R2
T)
R2
i,T(m −bT)
! #
=
O(mkT −qz) exp
"
m log
1 −R2
T
1 −R2z

+ (bT −bz) log m −
X
i∈Bz
bi,z log(R2
i,z) + O(1)
#
due to a Laplace approximation.
Here qz = kz −Lz, and Lz is the number of
components R2
i,z going to zero, while R2
j = P
i∈Bj R2
i,j, bi,j =
a+pi,j
2
−2 and bj =
P
i∈Bj bi,j for j ∈{z, T}.
Case 1: Mz ̸⊃MT
In this case, R2z < R2
T (see Lemmas B.1 and B.2 in Maruyama and George (2011))
indicating that
  1−R2
T
1−R2z

< 1. For i ∈Hz, R2
i,z →0 and mR2
i,z = O(1) (from Lemma
4.4.1), which leads to
lim
m→∞BF(Mz : MT) = lim
m→∞O(ms)O(f m).
Here 0 < f < 1, and so regardless of the choice of s ∈R, lim
m→∞BF(Mz : MT) = 0.
115

Case 2: Mz ⊃MT
This situation arises when Mz contains all the design blocks in MT and has at
least one more block not in MT. By Lemma 4.4.1 (iii) and (v),

1−R2
T
1−R2z
m
= O(1)
and mR2
i,z = O(1) for all i ∈Hz ⊂Bz\BT. Then the logarithm of the Bayes factor
is approximated as
m log
1 −R2
T
1 −R2z

+ (bT −bz) log m −
X
i∈Bz
bi,z log(R2
i,z) + (kT −qz) log m
=
O(1) +
h
(kT −qz) + 1
2
X
i∈BT
(pi,T −pi,z) + 1
2
X
i∈Bz\BT
(0 −pi,z)
+(a −4)kT −kz
2
+
X
i∈Hz
 a + pi,z
2
−2
i
log m

since mR2
i,z = O(1)

=
O(1) +
h
kT −kz + Lz −1
2
X
i∈Bz\BT
pi,z + (a −4)kT −kz
2
+1
2
X
i∈Hz
pi,z + a −4
2
Lz
i
log m
 pi,z = pi,T ∀i ∈BT

=
O(1) +
h
(a/2 −1)(kT −kz + Lz) −1
2
X
i∈Bz\(BT ∪Hz)
pi,z
i
log m
Now, a > 2 and qz = kz −Lz ≥kT for any model Mz ⊃MT, which implies that
the expression
s = (a/2 −1)(kT −kz + Lz) −1
2
X
i∈Bz\(BT ∪Hz)
pi,z ≤0.
When the above expression is strictly negative, lim
m→∞BF(Mz : MT) = lim
m→∞O(ms) =
0, otherwise BF(Mz : MT)
d→Wz for a non-degenerate random variable Wz. It is
possible for s to equal zero and this happens, for example, when Hz = Bz\BT so that
qz = kT and Bz\(BT ∪Hz) = φ, the null set. In such a situation, model selection
consistency does not hold.
116

When MT = M0, BF(Mz : MT) = BF(Mz : M0)
d→Wz for any model Mz,
and P( lim
n→∞BF(Mz : M0) = 0) = 0 so that the BOB hyper-g prior is again model
selection inconsistent.
Theorem 4.4.4. Consider a regression model with a BOB hyper-g/n prior as de-
scribed in (4.4), (4.2) and (3.14) which satisﬁes 4.13 and 4.14. This posterior distri-
bution is consistent in model selection.
Proof. We imitate the proof of Theorem 3.6.4 appearing in Appendix A.14 to show
that (4.15) holds.
When MT ̸= M0, using a Laplace approximation as in Appendix A.14, we get
lim
m→∞BFBOBHg/n(Mz : MT)
=
lim
m→∞O(mkT −kz+ a
2 (qz−kT )) BFBOBHg(Mz : MT),
where the subscripts denote the corresponding Bayes factors for the BOB hyper-g
and BOB hyper-g/n priors.
Case 1: Mz ̸⊃MT
It has been shown in the proof of Theorem 4.4.3 that BFBOBHg(Mz : MT) goes
to zero as m →∞(n →∞) at an exponential rate and hence BFBOBHg/n(Mz :
MT) →0 for all such models Mz, regardless of the values of kT, kz and qz.
Case 2: Mz ⊃MT
The log Bayes factor in this case equals
O(1) +
h
(a/2 −1)(kT −kz + Lz) −1
2
X
i∈Bz\(BT ∪Hz)
pi,z
i
log m
+
ha
2(qz −kT) + kT −kz
i
log m
=
O(1) +
h
−1
2
X
i∈Bz\(BT ∪Hz)
pi,z −(kT −kz + Lz) + kT −kz
i
log m
=
O(1) −
h
Lz + 1
2
X
i∈Bz\(BT ∪Hz)
pi,z
i
log m
117

Observe that Lz and P
i∈Bz\(BT ∪Hz) pi,z can never be zero simultaneously.
If
Lz = 0, then Hz = φ and there is at least one i ∈Bz\BT such that pi,z > 0 as
Mz ⊃MT. The other term can equal zero only if Hz = Bz\BT and clearly Lz > 0
in such case. This ensures that lim
n→∞BFBOBHg/n(Mz : MT) = 0 for any model Mz
proving model selection consistency when MT ̸= M0.
The case MT = M0 is exactly similar to Case 2, but with the extra restriction
that BT = φ. The posterior distribution under the BOB hyper-g/n prior is thus
model selection consistent for any choice of the true model.
The BOB hyper-g/n prior is model selection consistent while the BOB hyper-g
prior is not, because of the same argument in favor of the hyper-g/n mixing distri-
bution in the ordinary g and block g priors. The hyper-g/n prior converges to a
stable limit, whereas the hyper-g prior degenerates to a point mass at 0 as n →∞
under Condition 3.5.2 or (4.13). Notice that the conditions in (4.13) and (4.14) are
important and have been used in the previous two theorems through the application
of Lemma 4.4.1.
4.4.3
Prediction Consistency
The Bayes-optimal prediction by∗
n of the unknown, true response y∗corresponding
to a new vector of predictors x∗∈Rp is the model averaged prediction
by∗
n = E(α | y) +
X
z∈ΓB
π(Mz | y)x∗TE(β | y, Mz),
where y is the vector of n data points observed simultaneously with the n × p matrix
of covariate values X. Prediction consistency is achieved when by∗
n
P→E(y∗) = αT +
x∗TβT as n →∞. The BOB hyper-g and BOB hyper-g/n priors are both prediction
118

consistent under Bayesian model averaging (BMA), as displayed by the next set of
results.
Theorem 4.4.5. Consider a regression model with a BOB hyper-g prior as described
in (4.4), (4.2) and (3.9) which satisﬁes 4.13 and 4.14. The predictions under BMA
are consistent for this model.
Proof. The proof is similar to the proof of Theorem 3.5.3. First, observe that block
orthogonality of the reparameterized design gives
E(τ | y, Mz)
=
E [E(τ | y, g, Mz)]
=
Z





gi1
1+gi1 bτ i1,z,LS
· · ·
· · ·
gikz
1+gikz bτ ikz,z,LS




π(g | y, Mγ)dg
and E(α | y)
=
bαLS
assuming Bz = {i1, i2, . . . , ikz}.
When MT = M0, bτ z,LS
P→0 and bαLS
P→αT for every Mz since least squares es-
timators are consistent. Thus the model averaged prediction by∗
n converges to E(y∗) =
αT.
Let Ωdenote the set of all models Mz (including MT) such that lim
n→∞π(Mz |
y) > 0. Theorem 4.4.3 shows that the set Ωcan only include some models Mz of
the form Mz ⊇MT. Thus, lim
n→∞
P
z:Mz∈Ω
π(Mz | y) = 1. When MT ̸= M0, the least
squares estimates for the reparameterized coeﬃcients in Mz ∈Ωare consistent, so
that bτ i,z,LS
P→τ i,T for i ∈BT and bτ i,z,LS
P→0 for i ̸∈BT.
We simply need to show that Corollary 3.5.1 is true under this setup, i.e.,
lim
n→∞E

gi
1+gi | Mz, y

= 1 ∀i ∈BT when Mz ∈Ω, while 0 ≤E

gi
1+gi | Mz, y

≤1
119

for all other i. But β and τ are related as βz = U −1τ z so that bβz,LS = U −1bτ z,LS
and E(β | y, g, Mz) = U −1E(τ | y, g, Mz). Hence,
lim
n→∞by∗
n
=
E(α | y) + lim
n→∞
X
z∈ΓB
π(Mz | y)x∗TU −1E(τ | y, Mz)
=
αT + x∗TU −1τ T lim
n→∞
X
z:Mz∈Ω
π(Mz | y)
=
αT + x∗TβT = E(y∗)
which makes the BOB hyper-g prior consistent in prediction under BMA.
The proof of the theorem will be complete after verifying that Corollary 3.5.1
holds. The only requirement for Corollary 3.5.1 to be true is that the relevant com-
ponent R2
i,z is strictly greater than zero in the limit as n →∞(see proof of Lemma
3.5.1 in Appendix A.12). Lemma 4.4.1 (i) and (ii) conﬁrm that lim
n→∞R2
i,z > 0 for all
i ∈BT and all z such that Mz ⊇MT.
Theorem 4.4.6. Consider a regression model with a BOB hyper-g/n prior as de-
scribed in (4.4), (4.2) and (3.14) which satisﬁes 4.13 and 4.14. The predictions under
BMA are consistent for this model.
Proof. The derivation follows directly from the proof of Theorem 3.6.5 in Appendix
A.15. As the BOB hyper-g/n prior is model selection consistent, lim
n→∞
P
z∈ΓB
π(Mz |
y) = lim
n→∞π(MT | y) = 1. We only need to show that
lim
n→∞E

gi
1 + gi
| MT, y

= 1, ∀i ∈BT
(4.16)
in the BOB hyper-g/n prior and then use the steps in the proof of Theorem 4.4.5 to
demonstrate prediction consistency. The derivation of (4.16) has already been done
in Appendix A.15 and in the proof of Theorem 4.4.5 earlier.
120

4.5
BOB Hyper-g Prior over all Block Permutations
The deﬁnition of the BOB g prior hinges on two distinct features, both of which
strongly determine the performance of the associated model. In order to achieve the
best performance, ﬁrstly, an appropriate choice of blocks (groups) of predictor vari-
ables is necessary, and secondly, the groups need to be ordered according to their
relative importance in the regression model. In many regression problems it is pos-
sible to identify latent theoretical constructs that connect sets of covariates. The
latent constructs justify the suitability of a common scale parameter for each of these
blocks of predictors and motivate a natural block structure in the design. However,
even when the proper group structure is known, it is a challenging task to order the
covariate blocks accurately in the block Gram–Schmidt process. As demonstrated
in Section 4.3.2, inference with the BOB hyper-g or hyper-g/n prior suﬀers if the
blocks are poorly arranged in the implemention of the block Gram–Schmidt orthog-
onalization. This weakness of the mixtures of BOB g priors can be ameliorated if
we deﬁne the prior to be invariant to the order of orthogonalization. In this section
we suggest two new, closely related formulations of the BOB g prior based on all
possible arrangements of the blocks of predictors, which obviate predetermining the
“correct” ordering of blocks for sensible inference. We call each such prior the block
orthogonalized permuted block g prior or the BOPB g prior.
4.5.1
Prior with Equal Weights on Permutations
Consider a regression model with K blocks for the total p predictor variables in
the full model. We consider the same model space ΓB = {0, 1}K as in Section 4.4,
that consists of 2K models resulting from the inclusion or exclusion of entire blocks of
121

covariates. The subscript z (∈ΓB) in a model Mz is used to denote the set of blocks
present in the particular model. Any model Mz is thus deﬁned by its kz blocks of
active predictors, where kz =
K
P
i=1
I(zi = 1).
The ﬁrst form of the new order-invariant BOPB g prior assigns the same prior
weight to each arrangement of covariate blocks in the block Gram–Schimdt process.
For a model with k blocks, a block g type prior is assigned to the vector of regression
coeﬃcients over all k! permutations of the k distinct blocks. The BOPB g prior on a
model Mz consisting of kz blocks is deﬁned as:
y | α, β, σ2
∼
N(α1 + Xzβz, σ2I)
π(βz | Mz, σ2)
=
1
kz!N(U (1)
z βz | 0, A(1)
z σ2) + 1
kz!N(U (2)
z βz | 0, A(2)
z σ2) + · · ·
· · · · · · + 1
kz!N(U (kz!)
z
βz | 0, A(kz!)
z
σ2)
(4.17)
π(α, σ2)
∝
1
σ2
where N(x | µ, Σ) denotes a Gaussian distribution with mean µ and covariance
matrix Σ on the random variable x. Here U (j)
z , j = 1, 2, . . . , kz!, represents the upper
triangular matrix resulting from the block Gram–Schimdt process with respect to the
jth permutation of the blocks from the original design Xz. For the BOPB g prior, A(j)
z
is the usual prior covariance matrix for the BOB g prior when the reparameterized
design Qz is derived from the jth permutation of the blocks. The BOPB hyper-g and
BOPB hyper-g/n priors are obtained by adding prior distributions on the latent scale
vector g as in (3.9) and (3.14).
The Bayes factor comparing model Mz to the null model M0
BF(Mz : M0) = p(y | Mz)
p(y | M0)
=
1
p(y | M0)
Z
p(y | Mz, βz, α, σ2)π(βz | Mz, σ2)π(α, σ2)dβzdαdσ2
122

is a weighted average of the individual Bayes factors arising from each unique permu-
tation. Observe that
BF(Mz : M0)
=
p(y | Mz)
p(y | M0) =
kz
P
j=1
p(y | Mz, j)
kz!p(y | M0)
=
1
kz!
kz!
X
j=1
BF (j)(Mz : M0),
where p(y | Mz, j) is the component of the marginal likelihood due to the jth per-
mutation of blocks,
BF (j)(Mz : M0)
=
1
p(y | M0)
Z
p(y | Mz, βz, α, σ2)N(U (j)
z βz; 0, A(j)
z σ2)π(α, σ2)dβzdαdσ2
=
a −2
2
kz Z
(0,1)kz
" kz
Y
i=1
(1 −ti)
a+pj
i,z
2
−2
#  
1 −
kz
X
i=1
ti[R(z,j)
i
]2
!−n−1
2
dt,
and [R(z,j)
i
]2 =
yT P
Q(j)
i,z
y
yTy
is the ith component of R2 from model Mz. The orthogonal
blocks Q(j)
i,z can be derived as in (4.2) when the blocks Xi,z from model Mz are
arranged according to the jth permutation, j = 1, . . . , kz!.
4.5.2
Prior on Model Space Encompassing all Permutations
We can take a second view of the model space, identifying the model by its blocks
of active predictors and the permutations of these blocks.
The model space con-
tains all k! permutations for each model having k blocks, k = 0, 1, . . . , K, and the
permutations decide the order in which the k blocks are orthogonalized. Thus, the
size of this larger model space is |M| =
K
P
k=0
 K
k

× k! =
K
P
k=0
KPk = eΓ(K + 1, 1),
where Γ(s, x) =
R ∞
x ts−1e−tdt is the upper incomplete gamma function. The expres-
sion eΓ(K + 1, 1) is always guaranteed to be an integer for any K ∈N ∪{0} due
123

to the extra exponential factor. In this setup, we index a model by a subscript z to
denote the set of included blocks (as in Section 4.5.1) and a superscript j to indicate
a speciﬁc permutation of those blocks.
The second form of the BOPB g prior is deﬁned as:
y | α, β, σ2
∼
N(α1 + Xzβz, σ2I)
π(βz | Mjz, σ2)
=
N(U (j)
z βz; 0, A(j)
z σ2) , j = 1, 2, . . . , kz!
(4.18)
π(α, σ2)
∝
1
σ2
where kz is the number of blocks in the unordered model Mz and Mjz refers to the
model with the jth permutation of the blocks in Mz. U (j)
z
and A(j)
z
are deﬁned in
(4.17). The BOB hyper-g and BOB hyper-g/n priors result from the prior distribu-
tions (3.9) and (3.14) respectively on the parameter g. As before, the Bayes factor
for the model Mjz is given by:
BF(Mjz : M0) = BF (j)(Mz : M0)
=
a −2
2
kz Z
(0,1)kz
" kz
Y
i=1
(1 −ti)
a+pj
i,z
2
−2
#  
1 −
kz
X
i=1
ti[R(z,j)
i
]2
!−n−1
2
dt.
This variation of the BOB g prior is not order-invariant in the typical sense, since
the prior on each model Mjz depends on the order of block orthogonalization within
the model. Nevertheless, since all feasible arrangements of the blocks of predictors
are included in the model space as separate models, the “correct” arrangement is also
guaranteed to contribute to inference.
4.5.3
Inference with BOPB Hyper-g Priors
The beneﬁt of using a BOPB hyper-g or hyper-g/n prior distribution with prior
component (4.17) or (4.18) on the regression coeﬃcients is that the choice of the order
124

in which the blocks are orthogonalized does not have any adverse eﬀect. The pitfalls
of incorrect judgment in sorting the blocks, exhibited in the form of CLP and CELS
in the BOB hyper-g prior over a single arrangement, are avoided. The permutations
are the key to avoiding these problems. For a given set of k design blocks, (k −1)!
permutations will have the “largest” coeﬃcient block in the ﬁrst position, placing us
in the ambit of Theorem 4.3.2 (avoiding the CLP) and 4.3.4 (avoiding the ELS). We
turn to an adjustment of our earlier theory to formally handle the permutations.
Another prominent characteristic of these priors is that prior speciﬁcation is now
invariant to the order of block orthogonalization, in addition to the usual invariance
to blockwise linear reparameterizations.
Prior (4.17) is invariant to the order of
orthogonalization since it averages over all possible arrangements while prior (4.18)
treats distinct block arrangements as separate models in the model space.
Although the prior distributions of the form (4.17) and (4.18) make the BOPB
hyper-g prior invariant to the order of orthogonalization, it is not immediate that the
procedure does not suﬀer from the CLP and CELS. We show in the next two lemmas
that in the situation when one set of regression coeﬃcients is huge compared to the
rest and should be identiﬁed as the most important block, the BOPB hyper-g prior
concentrates posterior probability only on those models where the important block
has been correctly identiﬁed. We prove the results only for prior form (4.18), the
implications on prior form (4.17) are the same. Under appropriate limits, the entire
posterior probability is placed on the components corresponding to the “correct”
permutations which contain the most important block in the ﬁrst position.
Consider the sequence of regression problems {ΨN} described in (4.5) with a prior
distribution of the form (4.18) and (3.9) assigned to each model in the sequence. The
125

model space M consists of eΓ(K + 1, 1) models arising from entire blocks of predic-
tors being included or excluded from consideration together with all permutations of
the set of included blocks. Let Iz
s represent the set of block permutations from an
unordered model Mz which includes all covariates in Xs, where Xs is the ﬁrst and
the most important block in the block Gram–Schmidt process. If kz is the number
of design blocks in Mz, then |Iz
s | = (kz −1)!.
Lemma 4.5.1. Consider the sequence of problems {ΨN} deﬁned in (4.5) with a
BOPB hyper-g prior as in (4.18) and (3.9) on each model in the sequence.
Let
l1 and l2 denote permutation indices for an unordered model Mz containing block
Xs, such that l1 ∈Iz
s
and l2 ̸∈Iz
s .
When Condition 4.3.1 holds, a ≥3 and
n > (2kz + 1) ∨(a + ps + 2q −3), where q (≥2) is the position of block Xs in
model Ml2z, lim
N→∞BF(Ml2
z : Ml1
z ) = 0.
Proof. The Bayes factor BF(Ml2z : Ml1z) comparing model Ml2z to model Ml1z has
the limit
lim
N→∞
BF(Ml2z : M0)
BF(Ml1z : M0)
=
lim
N→∞
R
(0,1)kz
Qkz
i=1(1 −ti)
a+pj
i,z
2
−2
 
1 −Pkz
i=1 ti[R(z,l2)
i
]2−n−1
2 dt
R
(0,1)kz
Qkz
i=1(1 −ti)
a+pj
i,z
2
−2
 
1 −Pkz
i=1 ti[R(z,l1)
i
]2
−n−1
2 dt
=
C1 lim
N→∞
R
(0,1)kz
Qkz
i=1(1 −ti)
a+pj
i,z
2
−2
 
1 −Pq
i=1 ti[R(z,l2)
i
]2−n−1
2 dt
R
(0,1)kz
Qkz
i=1(1 −ti)
a+pj
i,z
2
−2
 
1 −t1[R(z,l1)
1
]2
−n−1
2 dt
(by Lemma 4.3.3 as n > 2kz + 1)
=
C2 lim
N→∞
R
(0,1)q
Qq
i=1(1 −ti)
a+pj
i,z
2
−2
 
1 −Pq
i=1 ti[R(z,l2)
i
]2−n−1
2 dt
R
(0,1)q
Qq
i=1(1 −ti)
a+pj
i,z
2
−2
 
1 −t1[R(z,l1)
1
]2
−n−1
2 dt
126

where q denotes the maximum index i with lim
N→∞[R(z,l2)
i
]2 > 0. It is easy to see that
q ≥2 since l2 ̸∈Iz. As N →∞, L = [R(z,l1)
1
]2 →1, M =
qP
i=1
[R(z,l2)
i
]2 →1 and
1−L
1−M =
1−[R(z,l1)
1
]2
1−
qP
i=1
[R(z,l2)
i
]2 = O(1)/O(||βs||2)
O(1)/O(||βs||2) = O(1). We can show with a little eﬀort that the
O(1) term equals a ﬁnite non-zero constant in the limit.
Lemma 4.3.2 can be applied whenever n > a + ps + 2q −3, and thus the Bayes
factor tends to zero as N →∞.
Lemma 4.5.2. Consider the sequence of problems {ΨN} deﬁned in (4.5) with a
BOPB hyper-g prior as in (4.18) and (3.9) on each model in the sequence. Let M ∗
denote a model with Xs as the ﬁrst block in the block Gram–Schmidt process and M ′
be any other model which does not have Xs as the ﬁrst block. For suﬃciently large n,
when Condition 4.3.1 holds and a ≥3, lim
N→∞BF(M ′ : M ∗) = 0.
Proof. Let k∗and k′ denote the number of blocks, p∗
i and p′
i the size of the ith block,
and [R∗
i ]2 and [R′
i]2 the component of R2 due to the ith orthogonal block in models
M ∗and M ′ respectively. It is easy to see that p∗
1 = ps, and since the sequence {ΨN}
has k blocks in total, k∗∈{1, 2, . . . , k} and k′ ∈{0, 1, . . . , k}.
Case 1: M ′ does not contain the block Xs.
If M ′ ̸= M0, then
lim
N→∞BF(M ′ : M ∗) = lim
N→∞
BF(M ′ : M0)
BF(M ∗: M0)
=
lim
N→∞
a −2
2
k′−k∗R
(0,1)k′
Qk′
i=1(1 −ti)
a+p′
i
2
−2
 
1 −Pk′
i=1 ti[R′
i]2−n−1
2 dt
R
(0,1)k∗
hQk∗
i=1(1 −ti)
a+p∗
i
2
−2i 
1 −Pk∗
i=1 ti[R∗
i ]2
−n−1
2 dt
=
0
127

when n > k∗(a −2) + Pk∗
i=1 p∗
i + 1 (see proof of Theorem 3.5.1 in Appendix A.10).
Since Pk′
i=1[R′
i]2 →0 and Pk∗
i=1[R∗
i ]2 →1 as N →∞, the numerator is ﬁnite for all
N, but the denominator diverges to ∞with N.
If M ′ = M0, BF(M0 : M ∗) →0 as N →∞like before, when n > k∗(a −2) +
Pk∗
i=1 p∗
i + 1.
Case 2: M ′ contains the block Xs.
Case 2A: M ′ and M ∗are distinct permutations of the same set set of blocks.
This is exactly the same situation as in Lemma 4.5.1 and so lim
N→∞BF(M ′ : M ∗) =
0 when a ≥3, n > a+ps +2k −3 and Condition 4.3.1 holds. Note that the condition
on n is suﬃcient to ensure that the conclusion of Lemma 4.5.1 holds for any position
of the block Xs within model M ′.
Case 2B: M ′ and M ∗have a diﬀerent collection of blocks.
lim
N→∞BF(M ′ : M ∗) = lim
N→∞
BF(M ′ : M0)
BF(M ∗: M0)
=
lim
N→∞
a −2
2
k′−k∗R
(0,1)k′
Qk′
i=1(1 −ti)
a+p′
i
2
−2
 
1 −Pk′
i=1 ti[R′
i]2−n−1
2 dt
R
(0,1)k∗
hQk∗
i=1(1 −ti)
a+p∗
i
2
−2i 
1 −Pk∗
i=1 ti[R∗
i ]2
−n−1
2 dt
.
Since M ′ contains block Xs in a position other than the ﬁrst one, lim
N→∞[R′
i]2 > 0 for at
least two i, while only lim
N→∞[R∗
1]2 > 0 among all [R∗
i ]2, i = 1, . . . , k∗. Using Lemmas
4.3.2 and 4.3.3 as in the proof for Case 2A, lim
N→∞BF(M ′ : M ∗) = 0 when a ≥3,
n > a + ps + 2k −3 and Condition 4.3.1 holds.
Hence, when n is suﬃciently large such that both sets of restrictions on n (from
Cases 1 and 2) are satisﬁed, the Bayes factor BF(M ′ : M ∗) under the BOPB hyper-g
prior converges to zero in the limit.
128

Lemmas 4.5.1 and 4.5.2 indicate that when ||βs|| →∞, the BOPB hyper-g prior
allocates posterior probability (given a suitable prior on the model space) entirely
to the set of permutations where Xs is the ﬁrst block. The models with “incorrect”
order of block orthogonalization, which suﬀer from the CLP and CELS, do not get
any posterior weight in the limit. Hence, when N →∞in the sequence {ΨN} in (4.5),
these models cannot aﬀect model-averaged coeﬃcient estimates or play a detrimental
role in model selection.
The limit that drives the CLP and ELS can be extended to allow diﬀerent blocks
of coeﬃcients to grow at diﬀerent rates. In the upcoming Lemmas 4.5.3 and 4.5.4, we
show that the posterior concentrates on a unique set of models/permutations when
the design blocks can be ordered according to importance.
Consider a new sequence of regression problems {ΨN} = {X1(N), . . . , Xk(N), αN,
β1(N), . . . , βk(N), ϵN} similar to the one in (4.5).
In this sequence, the groups of
regression coeﬃcients grow in size simultaneously, but at diﬀerent rates while the
design is unaltered. β1 grows in size at the fastest rate, β2 at the next highest rate
and so on. βk−1 is the slowest growing set of coeﬃcients and βk is ﬁxed throughout
the sequence. Thus
ΨN = {X1, . . . , Xk, α, β1(N), . . . , βk−1(N), βk, ϵ}
(4.19)
with lim
N→∞||βi|| = ∞and lim
N→∞
||βi+1||
||βi|| = 0 for all i = 1, 2, . . . , k −1.
Among the k! models containing all k active design blocks, the BOPB hyper-g
posterior concentrates on the “true” arrangement of covariate blocks. Lemma 4.5.3
establishes this result for the case when a = 3 and each block βi is of size one. In
this situation, there are k = p blocks and pi = 1, ∀i = 1, . . . , p. Extension of the
129

result to the general case with arbitrary a ∈(2, 4] and arbitrary pi ≥1 for all k, is
currently being investigated.
Lemma 4.5.3. Consider the sequence of problems {ΨN} deﬁned in (4.19) with a
BOPB hyper-g prior as in (4.18) and (3.9) on each model in the sequence. Assume
that a = 3 and pi = 1 ∀i so that there are exactly p blocks in the design. Let M1 de-
note the model with the design blocks orthogonalized in the correct order of importance
{X1, X2, . . . , Xp} and Ml denote any other model with a diﬀerent permutation of the
same p blocks. When Condition 4.3.1 holds and n > 2p+1, lim
N→∞BF(Ml : M1) = 0.
The proof of this lemma is given in Appendix B.5.
Assume the same setup as in Lemma 4.5.3. Let S denote the set containing only
two models, one with blocks {X1, X2, . . . , Xp} and the other with blocks {X1, X2, . . . ,
Xp−1}, both models having design blocks orthogonalized in that speciﬁc order.
Lemma 4.5.4. Consider the sequence of problems {ΨN} deﬁned in (4.19) with a
BOPB hyper-g prior as in (4.18) and (3.9) on each model in the sequence. Assume
that a = 3 and pi = 1 ∀i so that there are exactly p blocks in the design. Let M ∗∈S
and M ′ ̸∈S be any model of an arbitrary size. When Condition 4.3.1 holds and n is
suﬃciently large, lim
N→∞BF(M ′ : M ∗) = 0.
The proof of this lemma is provided in Appendix B.6. The posterior distribution
under a BOPB hyper-g prior deﬁned by (4.18) and (3.9) focuses only on the models
that include the important blocks, and among such models, only on the arrangements
that correctly recognize the relative importance of the blocks. In a practical regression
problem, coeﬃcients will diﬀer in size, but will be of ﬁxed magnitude, and the BOB
hyper-g posterior will allocate a lot of probability to the model with the important
130

blocks of covariates with the “correct” order of block orthogonalization. The same
behavior is also observed under the BOPB hyper-g prior deﬁned by (4.17) and (3.9),
which substantiates our faith in models where such prior distributions are imposed.
Since the models with the most important blocks in the “correct” permutation get
most of the posterior weight (they get all the posterior weight in the limit), these
models contribute the most to model-averaged inferences.
Identical conclusions hold for the BOPB hyper-g/n prior deﬁned by (4.17) and
(3.14) or by (4.18) and (3.14) and versions of all the lemmas from this section related
to the BOB hyper-g/n prior follow directly from previous developments. Once again,
the same set of suﬃcient conditions is adequate for the new theoretical results. We
recommend using the BOPB hyper-g/n prior in applications instead of the BOPB
hyper-g prior because (under mild and standard conditions) the hyper-g/n prior con-
verges to a limit as n →∞while the hyper-g prior does not, and because of the
consistency arguments in favor of the prior scaled by sample size (see Section 4.4).
The BOPB g prior is simply another form of the BOB g prior, and a mixture of
BOPB g priors has the same behavior in regards to the three main kinds of asymp-
totic consistency properties discussed in Section 4.4 as the correponding mixture of
BOB g priors.
4.6
Application to Boston Housing Data
This section studies the behavior of the BOPB hyper-g/n prior in an applied
example. The Boston housing data set appearing in Harrison and Rubinfeld (1978)
consists of 506 observations on 14 variables concerning housing values in the suburbs
of Boston in 1970. A brief description of the data set is provided in Appendix B.7. The
131

median value of owner-occupied homes is the dependent variable which is regressed
on the thirteen other covariates. Following the analysis in Harrison and Rubinfeld
(1978), the predictor variables dis, rad, lstat and the response variable medv are log
transformed, while variables nox2 and rm2 are included as predictors instead of the
corresponding linear terms. All of the predictors and the response are centered and
scaled to have zero mean and unit variance.
4.6.1
Speciﬁcation of the BOPB Hyper-g/n Prior
Implementation of the BOPB hyper-g/n prior requires grouping the predictor
variables into blocks, and ﬁve diﬀerent block structures are contrasted in this problem.
The ﬁve block structures are listed below.
Form 1: Block 1- log(lstat), crim; Block 2- chas, nox2, rm2, log(dis), log(rad), tax,
ptratio, b; Block 3- zn, indus, age.
Form 2: Block 1- zn, indus, age; Block 2- crim, chas, nox2, rm2, log(dis), log(rad),
tax, ptratio, b, log(lstat).
Form 3: Block 1- log(lstat), crim; Block 2- zn, indus, chas, nox2, rm2, age, log(dis),
log(rad), tax, ptratio, b.
Form 4: Block 1- log(lstat); Block 2- crim, zn, indus, chas, nox2, rm2, age, log(dis),
log(rad), tax, ptratio, b.
Form 5: Block 1- nox2; Block 2- crim, zn, indus, chas, rm2, age, log(dis), log(rad),
tax, ptratio, b, log(lstat).
The motivation behind most of these choices comes from an examination of the t
statistics (used in the t test) associated with each of the predictor variables. A classical
regression analysis with the data set reveals that most of the predictor variables in
132

the model are signiﬁcant while only a few of them are not so important.
The t
statistics from the covariates are huge for a couple of predictors and relatively big
for a lot of the other ones.
The blocks in Form 1 are constructed based on this
feature. Covariates in block 1 are believed to be highly important since they have
large absolute values of the t statistic. The covariates in block 3 are considered to
be the least important due to low t statistic values, while the rest in block 2 with
medium to big absolute t values are moderately important. The block structure is
chosen so that the estimated coeﬃcients from block 1 will be left unchanged from least
squares estimates, whereas coeﬃcients from block 3 will be shrunk strongly toward
zero, highlighting the ﬂexibility in the shrinkage pattern of the blockwise prior.
The other group structures are chosen with the same rationale. The structure in
Form 2 separates only the unimportant predictor variables while Form 3 places the
very important covariates in a block of their own. The last two block structures,
Form 4 and Form 5, isolate exactly one highly signiﬁcant predictor from the rest to
form a block by itself in either case. The structure in Form 1 seems to take advantage
of the blocking strategy in the most eﬀective way by placing the highly signiﬁcant as
well as the insigniﬁcant predictors in distinct blocks.
The BOPB hyper-g/n prior imposed on the model with any of the block structures
(Forms 1-5) is the same prior as described in (4.18):
π(y | α, βz, σ2)
=
N(α1 + Xzβz, σ2I)
π(βz | Mjz, σ2)
=
N(U (j)
z βz; 0, A(j)
z σ2)
π(α, σ2)
∝
1
σ2
π(g)
=
a −2
2n
kz kz
Y
i=1

1 + gi
n
−a/2
133

where j ∈{1, 2 . . . , kz!} denotes a block permutation index and kz, A(j)
z , U (j)
z
have
the usual deﬁnition as in (4.17) and (4.18).
The model space under consideration in the BOPB g prior formulation is the set
of all models with entire blocks in and out of the model and all possible permutations
for orthogonalization given each set of blocks. Thus the size of the model space is
eΓ(K+1, 1) as mentioned in Section 4.5.2, with K being the total number of candidate
blocks in the design. We assume a Bernoulli(1/2) block inclusion prior on the model
space so that all combinations of blocks have the same probability a priori. But in this
extended model space, each unique block combination has multiple representations
through diﬀerent permutations for block orthogonalization. The prior probability for
any speciﬁc block combination is divided equally among all the distinct permutations.
4.6.2
Cross-validated Predictive Performance
This subsection compares the predictive performance of the new BOPB hyper-
g/n prior with the performances of existing g priors and mixtures of g priors. The
data set containing 506 observations is ﬁrst randomly split into 11 equal parts of 46
observations each in order to carry out an eleven-fold cross-validation. The cross-
validated predictive sum of squared errors (SSE) are calculated for the ﬁve diﬀerent
forms of the BOPB hyper-g/n prior and contrasted with the SSE values of existing
g priors and their mixtures. All ﬁve forms of the BOPB hyper-g/n prior have the
hyperparameter choice of a = 3 while the robust prior of Bayarri et al. (2012) is
speciﬁed with their recommended hyperparameter values of a = 1
2, b = 1 and ρ =
1
p+1.
The prior distribution on the 213 = 8, 192 models in the model space for the usual
Bayesian methods mentioned in the left column of Table 4.1 is taken to be uniform.
134

Table 4.1 shows the improvement in predictive SSE when any of the diﬀerent
forms of the new prior is assigned to the linear model. The predictive SSE from a
simple least squares ﬁt of the full model with all 13 predictors is also mentioned in
the table. Among the standard Bayesian methods, the Zellner–Siow full-based prior
does the best job in prediction closely followed by the RIC and the hyper-g priors.
Surprisingly, the least squares ﬁt of the full linear model gives a lower cross-validation
error than the more complex traditional Bayesian methods. The block structure in
prior Form 1 yields the lowest SSE among all methods. Prior Form 2, which moves
the insigniﬁcant predictors into a separate block, also does a commendable job in
prediction.
Prior on β
SSE
Prior on β
SSE
g-prior (g = n)
109.0293
g-prior (g = p2)
108.5533
EB-Local
108.6240
BOPB Form 1
106.7322
EB-Global
108.6164
BOPB Form 2
106.8235
ZS-Full
108.4286
BOPB Form 3
107.9949
ZS-Null
108.7108
BOPB Form 4
107.9304
Hyper-g (a = 3)
108.6254
BOPB Form 5
108.1448
Hyper-g (a = 4)
108.5802
Robust
108.7328
Least Squares
108.2285
Table 4.1: Table showing cross-validated predictive SSE under diﬀerent methods.
The SSE values for the Bayesian models are obtained by model averaging. All ﬁve
forms of the BOPB hyper-g/n prior have the hyperparameter value of a = 3.
An important point to note here is that the model space for the existing methods
and that for the mixtures of the BOPB g prior are not the same. Traditional priors
are associated with a model space consisting of 2p models, where any of the total p
135

covariates can be included or excluded in a model. The BOPB hyper-g/n prior has
a model space of size eΓ(K + 1, 1), where any of the total K blocks can be included
or excluded and given a particular combination of k blocks, all k! block permutations
deﬁne distinct models. Uniform priors on these model spaces do not allocate prior
probabilities to the models in the same way and so the eﬀect of the model prior on
prediction errors is worth investigating. Moreover, there is a big discrepancy in the
sizes of the model spaces; the model space for the common priors has 8,192 models
while the BOB hyper-g/n over block permutations has only eΓ(3, 1) = 5 models for
prior Forms 2, 3, 4 and 5 and eΓ(4, 1) = 16 models for prior Form 1.
BOPB Hyper-g/n
Hyper-g
Hyper-g/n
(a = 3)
(a = 3)
(a = 3)
Form 1
106.7322
106.8158
106.8229
Form 2
106.8235
106.8158
106.8229
Form 3
107.9949
108.1508
108.1598
Form 4
107.9304
108.1508
108.1598
Form 5
108.1448
108.1523
108.1614
Table 4.2: Table showing cross-validated predictive SSE (under BMA) for the hyper-
g, hyper-g/n and BOPB hyper-g/n priors.
The model averaging is done over all
possible permutations and combinations of the set of design blocks appearing in the
speciﬁc prior form.
Although diﬀerent permutations of the same set of blocks lead to distinct priors in
the BOPB hyper-g/n prior, the ordinary hyper-g and hyper-g/n priors are unaﬀected
by block permutations and all permutations of a given combination of blocks result in
an identical prior. Table 4.2 reveals the role of the model space in lowering predictive
errors in the BOPB hyper-g/n, hyper-g and hyper-g/n priors. The cross-validated
prediction errors for the mixtures of ordinary g priors decrease slightly in the case of
136

the smaller model space and the prediction errors get closer to values corresponding
to the new prior. However, with the exception of prior Form 2, the BOPB hyper-g/n
still furnishes lower predictive SSE values than the ordinary mixtures of g priors when
model averaging is carried out over the same curtailed model space for each prior.
The BOPB hyper-g/n prior in Form 1 dominates all other priors in terms of
predictive accuracy in the reduced as well as the complete model space. The degree of
improvement associated with a BOPB hyper-g/n prior is diminished when estimates
are averaged over the smaller model space, hinting at a positive eﬀect of the reduced
set of models.
But in the usual situation with a complete model space, as in a
general implementation of a standard prior, the predictive errors are signiﬁcantly
higher compared to the new prior.
Appropriate choice of group structure in the
design is essential for good behavior of the BOPB hyper-g/n prior, and an intelligent
choice, like prior Form 1 in this example, enhances the performance of the model
remarkably.
4.7
Summary
In this chapter, we modiﬁed the setting of the basic block g prior and proposed a
new version of the block g prior on a reparameterized design with orthogonal design
blocks. There are at least two motivating reasons to justify the introduction of the
new block orthogonalized block (BOB) g prior. The ﬁrst is that the block orthogonal
design simpliﬁes expressions for posterior summaries and expedites posterior infer-
ence. The second reason is that some formal results related to the basic block g
priors in Chapter 3 depend on the assumption of a blockwise orthogonal regression
design. These same results hold in the BOB g prior without the extra assumption.
137

The novel BOB g prior is imposed on a transformed block orthogonal design to begin
with and so the extra assumption of block orthogonality needed for amenable poste-
rior expressions becomes irrelevant. The BOB hyper-g and hyper-g/n priors possess
desirable asymptotic consistency properties and are shown to avoid the CLP and ELS
when the orthogonalization of blocks is carried out in the “correct” order. In practice,
we recommend orthogonalizing the blocks of covariates according to their relative or-
der of importance in the regression model, to avert the CLP and a new, unwanted
phenomenon called CELS. The complication of ranking blocks accurately is avoided
by a BOB hyper-g (or hyper-g/n) prior over the set of all block permutations, called
the BOPB hyper-g (or hyper-g/n) prior. This prior considers every possible arrange-
ment for block orthogonalization, including the “correct” arrangement and is formally
demonstrated to focus its probability on the model with the “correct” arrangement
of blocks. Models with properly ordered design blocks have the main contribution
to the calculations for posterior summaries and consequently, the adverse eﬀects of
erroneous ordering only have a negligible impact on posterior inference.
138

Chapter 5: Science Driven Priors on the Strength of Linear
Regression
In Chapter 1, we brieﬂy discussed how speciﬁcation of traditional priors on the
regression coeﬃcients β in a linear model might not be meaningful in situations
where they impart unnecessarily strong (and often inplausible) information about
certain regions of the parameter space. This chapter demonstrates how some common,
popular priors have unwanted implications on the strength of linear relationship in
regression. Use of these “ﬂawed” priors for model comparison and inference is often
unwarranted and alternative prior distributions need to be developed to address their
drawbacks. We propose a new class of science driven priors in this chapter as a viable
solution to the deﬁciencies of the usual priors.
5.1
Implications of Standard Priors
Consider the regression model described by
y = βTx + ϵ
where β and x are both p × 1 vectors denoting the unknown regression parameters
and the covariate vector respectively. The observed error ϵ is assumed to be normally
distributed with variance σ2
ϵ. Further assume that x ∼N(0, Ip) is independent of ϵ
139

which makes this model consistent with the joint distribution
(y, x)T | Σyx ∼N

0, Σyx =
 σ2
y
βT
β
Ip

where σ2
y = σ2
ϵ + βTβ.
In this particular setup we work with E
 (y, xT)T
= 0
and V ar(x) = Ip, with the understanding that any regression model can be easily
transformed into this setting. The prior on the error variance σ2
ϵ is inverse-gamma,
i.e., σ2
ϵ ∼IG(a1, b1), for some suitably chosen hyperparameters a1 and b1 such that
E(σ2
ϵ) =
1
b1(a1−1) if a1 > 1.
Let ρ2, the proportion of variation in y explained by the regression component
βTx in the model, be the population analogue of the coeﬃcient of determination R2.
Due to the independence assumption in the model, the total variation in y can be
decomposed as V ar(y) = V ar(βTx) + V ar(ϵ), also represented as σ2
y = σ2x + σ2
ϵ,
where σ2x = βTβ. This leads to the following expression for ρ2 :
ρ2 = σ2x
σ2
y
=
σ2x
σ2x + σ2
ϵ
=
βTβ
βTβ + σ2
ϵ
.
As discussed in Chapter 1, many commonly used priors for β resemble
β | c, Σ, σ2
ϵ ∼N(0, cσ2
ϵΣ).
(5.1)
The matrix Σ may be Ip, (XTX)−1 or have some other form determined by the user.
A closely-related prior is speciﬁed to be conditionally independent of the variance of
the error term:
β | c, Σ, σ2
ϵ ∼N(0, cΣ).
As mentioned in Chapter 1, c may be ﬁxed a priori or it may be given a prior
distribution that results in a thick-tailed marginal prior for β.
140

It is instructive to study the behavior of the implied prior on ρ2 under various
default priors for β. Under prior (5.1) that conditions on σ2
ϵ, the common choice of
Σ = Ip results in
βTβ | σ2
ϵ
∼
Gamma
p
2, 2cσ2
ϵ

(with mean cpσ2
ϵ)
and
ρ2
1 −ρ2 | σ2
ϵ
∼
Gamma
p
2, 2c

,
the latter of which is independent of σ2
ϵ. Then the implied prior density for ρ2 is
π(ρ2) = (2c)−p/2
Γ(p/2)

ρ2
1 −ρ2
p/2+1
ρ−4 exp
 
−
ρ2
2c(1 −ρ2)
!
, 0 < ρ2 < 1
(5.2)
for all choices of priors for σ2
ϵ.
The left panel of Figure 1.1 illustrates the behavior of this prior when c = 1 as the
number of predictors (p) increases. Table 5.1 illustrates the behavior of the prior’s
ﬁrst two central moments (computed numerically). When p = 1, the prior is fairly
ﬂat over most of the range of ρ2 and has an inﬁnite spike at ρ2 = 0. This behavior
may be reasonable in a situation where there is no prior information available about
the strength of the linear relationship between the predictor and the response and
shrinkage of this parameter toward zero is desired. When p = 2, the prior density for
ρ2 is ﬁnite at zero and ﬂat over most of its range, with a slight accumulation of prior
mass near large values of ρ2. As the number of predictors increases, the behavior
changes dramatically: prior mass concentrates around one, eﬀectively declaring that
we are nearly certain a priori that all variation in the response can be explained
by the predictor variables. While we typically collect predictor variables because we
think they will explain variation in the response, we rarely expect them to explain
141

almost all of the variation, even with a sizable number of “signiﬁcant” predictors. (A
notable exception is a carefully designed and controlled physical experiment where
inputs to a system have a known relationship with the response and σ2
ϵ is thought to
predominantly represent measurement error.)
Model Size
1
2
3
5
7
E(ρ2)
0.34
0.54
0.66
0.78
0.84
SD(ρ2)
0.26
0.24
0.19
0.12
0.08
Model Size
10
12
15
18
20
E(ρ2)
0.89
0.91
0.93
0.94
0.95
SD(ρ2)
0.05
0.04
0.03
0.02
0.02
Table 5.1: Prior mean and standard deviation of ρ2 as a function of model size when
β ∼N(0, σ2
ϵIp).
The following lemma provides a formal description of the absurd behavior of the
prior density (5.2) on ρ2 when the number of predictors (p) increases.
Lemma 5.1.1. Consider a prior distribution on ρ2 with density of the form (5.2).
The distribution on ρ2 converges to a degenerate distribution with all of its mass at 1
as p →∞.
Proof. Recall that the prior in 5.2 is generated from the distribution
ρ2
1 −ρ2 ∼Gamma
p
2, 2c

,
and hence ρ2/(1 −ρ2)
d= Z, where Z is a Gamma(p/2, 2c) random variable. This
means that ρ2 d= Z/(1 + Z) because the map g : t 7→
t
1+t is continuous.
Since a gamma random variable exhibits the property of inﬁnite divisibility, we
can express Z as Z = Pp
i=1 Zi, where the Zi’s are i.i.d. Gamma(1/2, 2c) random
variables.
142

So,
ρ2 = ρ2(p)
d=
Pp
i=1 Zi
1 + Pp
i=1 Zi
=
1
p
Pp
i=1 Zi
1
p + 1
p
Pp
i=1 Zi
.
By the Strong Law of Large Numbers,
1
p
Pp
i=1 Zi →E(Z1) = c almost surely as
p →∞. This implies that the RHS of the last expression converges almost surely
to c/c = 1 as p →∞due to the continuous mapping theorem. As a result, the
sequence of random variables ρ2(p) converges in distribution to 1, or equivalently,
to a degenerate distribution with mass at 1 when p →∞. Note that almost sure
convergence guarantees convergence in law for the RHS in the above display.
The behavior of the prior on ρ2 as p grows large is driven by the behavior of βTβ
in the same limit. In particular, the expected squared L2 norm of the regression
coeﬃcients grows linearly in p: E(βTβ | σ2
ϵ) = pcσ2
ϵ. It is tempting to ameliorate
this eﬀect by choosing c to scale with p. One quick attempt at a ﬁx is to assign c = 1
p
so that E(βTβ | σ2
ϵ) is constant across models of diﬀerent sizes. Unfortunately, this
speciﬁcation forces the implied prior on ρ2 to concentrate around 1
2 as p →∞as
demonstated in Figure 1.1. This follows from the fact that V ar(βTβ | σ2
ϵ) = 2σ4
ϵ/p
causing the prior on ρ2 = βT β/(βT β + σ2
ϵ) to converge to a degenerate distribution at
1
2 in the limit as p →∞. We see that it is not possible to adjust the scale of the prior
to maintain both a constant mean and a constant variance for βTβ as p increases,
leading to the passing of unreasonably strong prior information to ρ2 in the limit.
The same complication appears in other standard priors, like priors of the form
β ∼N(0, cIp) that do not condition on σ2
ϵ. In this case, βTβ ∼Gamma(p/2, 2c)
and the implied prior on ρ2 now depends on the prior on σ2
ϵ because
ρ2
1 −ρ2 | σ2
ϵ
∼
Gamma
p
2, 2c
σ2
ϵ

.
143

The traditional choice of a prior on σ2
ϵ is the inverse-gamma prior, but it does not
lead to a closed form expression for the implied marginal density function of ρ2. The
non-standard choice of a gamma prior on σ2
ϵ induces a generalized three-parameter
Beta distribution on ρ2 which has a density available in closed form. When σ2
ϵ ∼
Gamma(a1, b1), the induced prior on ρ2 is G3B(p/2, a1, b1/2c) with density
π(ρ2) = Γ(p/2 + a1)
Γ(p/2)Γ(a1)
(2c)a1ρp−1/2(1 −ρ2)a1−1
ba1
1
2c
b1(1 −ρ2) + ρ2p/2+a1 , 0 < ρ2 < 1.
Table 5.2 illustrates the eﬀect of increasing p on the marginal moments of ρ2
under both the inverse-gamma and gamma priors on σ2
ϵ. For the illustration, we
choose an inverse-gamma prior with parameters 3 and 1
2 and a gamma prior with
both parameters equal to 1, so that E(σ2
ϵ) = 1 and V ar(σ2
ϵ) = 1 for both prior
choices.
As before, we use numerical integration to calculate the moments.
The
inﬂation in βTβ as p grows pushes the prior mass toward one as before. Rescaling
the covariance matrix has the same eﬀect again of failing to remedy the problem
(results not shown).
(A)
Model Size
1
5
10
15
20
E(ρ2)
0.38
0.80
0.90
0.93
0.95
SD(ρ2)
0.29
0.15
0.08
0.06
0.04
(B)
Model Size
1
5
10
15
20
E(ρ2)
0.43
0.81
0.90
0.93
0.95
SD(ρ2)
0.32
0.17
0.09
0.07
0.05
Table 5.2: Prior mean and standard deviation of ρ2 as a function of model size when
(A) β ∼N(0, Ip) and σ2
ϵ ∼IG(3, 1
2), and (B) β ∼N(0, Ip) and σ2
ϵ ∼Gamma(1, 1).
144

Zellner’s g prior (Zellner, 1986), which has a distribution of the form (5.1) with
Σ = (XTX)−1 where X is the n×p matrix of covariates, demonstrates similar behavior
in the implied prior on ρ2. The prior on β as well as the implied prior on ρ2 now
depend on both n and p. Several choices for the unknown parameter c have been
considered in recent literature, and Section 3.1.1 brieﬂy reviews some of the popular
methods of choosing c. Under our model assumptions, each row of X is independently
distributed as N(0, Ip). The unit information prior (Kass and Wasserman, 1995) sets
c = n, making this speciﬁcation asymptotically equivalent to our earlier example
where β ∼N(0, σ2
ϵIp), since n(XTX)−1 →Ip as n →∞. The choice of c = p2
(Foster and George, 1994) is used to calibrate the prior based on the risk inﬂation
criterion.
(A)
Model Size
1
5
10
15
20
E(ρ2)
0.05
0.24
0.48
0.71
0.95
SD(ρ2)
0.06
0.13
0.15
0.13
0.06
(B)
Model Size
1
5
10
15
20
E(ρ2)
0.05
0.85
0.99
> 0.99
> 0.99
SD(ρ2)
0.06
0.11
0.01
< 0.01
< 0.01
Table 5.3: Prior mean and standard deviation of ρ2 as a function of model size when
n = 20 and (A) β ∼N(0, σ2
ϵ(XTX)−1), and (B) β ∼N(0, p2σ2
ϵ(XTX)−1).
Table 5.3 displays the marginal moments of ρ2 for the two cases c = 1 and c = p2
when n = 20, which again indicate the strange behavior of the induced prior on ρ2.
As in the earlier example with prior form (5.1), scaling the variance in the g prior by
σ2
ϵ causes the distribution of ρ2 to be invariant to the prior on σ2
ϵ. All the moments
have been calculated by Monte Carlo integration.
145

It is clear that the most commonly used standard priors on β induce highly in-
formative priors on ρ2 for models with even a modest number of predictor variables.
Disturbingly, the prior information concentrates in a region to which, in almost all
cases, we would want to assign relatively little prior mass. While it may be tempting
to not worry about this phenomenon—with the view that, with enough data, the prior
will be dominated by the likelihood—we argue that the use of priors that behave in
this way should be avoided for (at least) two reasons.
First, the purest descriptions of Bayesian methods emphasize their ability to co-
herently incorporate prior information into an analysis. It seems at odds with best
modeling practices to knowingly include prior information that is orthogonal to actual
belief, especially when alternative priors exist.
Second, while the likelihood may dominate the prior asymptotically with respect
to targeted intra-model inferences, it is well known that prior-data conﬂict can have a
substantial impact on the marginal likelihood. In turn, such conﬂict can strongly and
adversely aﬀect the comparison of models via the Bayes factor. These eﬀects cascade
to the posterior distribution on the model space, from which the weights for Bayesian
model averaged prediction are derived. As there is typically uncertainty about which
predictors belong in the model, the use of a prior that behaves sensibly with respect
to ρ2 is important.
The rest of this chapter is devoted to the development of prior distributions for
β for which the implied prior on ρ2 is reasonable. Our approach ﬁrst posits a prior
for ρ2 itself. This allows prior information about likely values of ρ2 (with the sample
R2 statistic in mind) to be included in the model when it is available. It will also
allow us to be non-informative about ρ2 when appropriate. Given the prior on ρ2,
146

we then construct a prior for the “rest” of β, with the goal of being non-informative
about the aspects of β for which we have little prior information. Computational
considerations are addressed, and we investigate the performance of the prior on a
substantive example.
5.2
Priors on ρ2
An alternative strategy is to place a prior distribution directly on ρ2 and then
pass this information to the regression coeﬃcients. Naturally the strongest form of
prior information about ρ2 that we could inject into the model is to place all of the
prior mass on one particular value of ρ2. Recall that β and ρ2 are related as βTβ =
σ2
ϵρ2/(1 −ρ2), which is equivalent to an equation of a p-dimensional hypersphere
in β when σ2
ϵ and ρ2 are ﬁxed and known. Conditional on σ2
ϵ and for a ﬁxed ρ,
the above relation restricts all of the prior mass for the regression vector β on a p-
dimensional hypersphere with radius σϵρ/
p
1 −ρ2. In practical situations one cannot
expect to have such strong prior information, but this nice structure can be used
as a building block for constructing meaningful prior distributions. When we have
problem speciﬁc prior information to pin down a tight range of values for ρ2, a possible
formulation of prior distribution would place most of its mass in a “ring” surrounding
a circular region or hypersphere. Weaker prior information would correspond to a
greater diﬀusion of probability mass away from the mode at the circular part.
Our preferred approach is to model ρ2 directly with a parametric prior distribution.
A natural choice is ρ2 ∼Beta(a, b) where the hyperparameters a and b are chosen to
reﬂect prior information about ρ2 when available and to be non-informative about ρ2
otherwise. Deﬁne δ2 = βTβ , so that
147

ρ2
=
δ2
δ2 + σ2
ϵ
and
δ2
=
σ2
ϵ
ρ2
1 −ρ2.
It follows that
ρ2
1−ρ2 ∼IB(a, b), an inverted-beta (or beta-prime) distribution, which
leads to
δ2 | σ2
ϵ
∼
GIB(a, b, σ2
ϵ)
with
π(δ2 | σ2
ϵ)
=
Γ(a + b)
Γ(a)Γ(b)σ2b
ϵ (δ2)a−1(σ2
ϵ + δ2)−(a+b), δ2 ≥0
(5.3)
where GIB(·) stands for a generalized inverted-beta distribution. The generalized
inverted-beta distribution on δ2 induces the following prior on δ =
p
βTβ when
conditioned on σ2
ϵ:
π(δ | σ2
ϵ) =
2σ2b
ϵ
Be(a, b)δ2a−1(σ2
ϵ + δ2)−(a+b), δ ≥0.
where Be(a, b) =
R 1
0 ta−1(1 −t)b−1dt = Γ(a)Γ(b)/Γ(a + b). The parameter δ can be
interpreted as the radius of the p-dimensional hypersphere or simply the distance of
β from the origin.
It is interesting that speciﬁc choices of a and b in the prior for ρ2 result in well-
known distributions on δ when σ2
ϵ is known. For example, the particular choice of
a = 1
2 and b = ν
2 induces the half t-distribution with the density
π(δ | σ2
ϵ) =
2
√πσϵ
Γ(ν+1
2 )
Γ(ν
2)

1 + δ2
σ2
ϵ
−( ν+1
2 )
, δ ≥0.
It is also possible to induce a half-Cauchy distribution (Gelman, 2006; Polson and
Scott, 2012b) on δ
π(δ | σ2
ϵ) =
2σϵ
π(σ2
ϵ + δ2), δ ≥0,
148

with a Beta(1
2, 1
2) prior on ρ2. Such a prior choice for ρ2 is a bit unrealistic as it
allocates substantial probability to the regression model being either really good or
really bad, but it has close connections to the prior on the shrinkage factor in the
horseshoe prior (Carvalho et al., 2010).
Another special case is when the prior on ρ2 is uniform over (0, 1) which can be
thought of as a non-informative prior on ρ2. When ρ2 ∼Beta(1, 1),
π(δ2 | σ2
ϵ)
=
σ2
ϵ
(σ2
ϵ + δ2)2, δ2 ≥0,
and π(δ | σ2
ϵ)
=
2σ2
ϵδ
(σ2
ϵ + δ2)2, δ ≥0.
The distribution on δ2 is sometimes called the “log-logistic” distribution (because
log δ2 follows a logistic distribution), which is popular in the survival analysis litera-
ture. The implied prior distribution on δ is known as the Dagum distribution or the
Burr Type III distribution. A Dagum distribution with parameters (a, b, p) has a pdf
of the form
f(x; a, b, p) = ap
x
(x
b)ap

(x
b)a + 1
p+1, x > 0.
The induced prior on δ in this case corresponds to a Dagum (2, σϵ, 1) distribution.
5.3
Priors on β
It has been established in Section 5.2 that a Beta(a, b) prior on ρ2 implies a
generalized inverted-beta prior on the squared norm of the regression coeﬃcients
(δ2 = βTβ) and the following distribution on δ when σ2
ϵ is known
π(δ | σ2
ϵ) =
2σ2b
ϵ
Be(a, b)δ2a−1(σ2
ϵ + δ2)−(a+b), δ ≥0.
In order to obtain a prior on β, we need to make assumptions on how the total size δ
is distributed among the individual coeﬃcients βi. One sensible assumption is that of
149

isotropy for the distribution of β, so that given a p-dimensional hypersphere of radius
δ, the β coeﬃcients are uniformly spread out over the (p −1)-dimensional surface of
the sphere. This is reasonable when a priori no single coeﬃcient is expected to be
more important than any of the other coeﬃcients. The conditional prior is therefore,
π(β | δ) =
Γ(p/2)
2πp/2δp−1, β ∈{z ∈Rp : zT z = δ2},
which leads to a prior π(β | σ2
ϵ) where the elements of β are exchangeable:
π(β | σ2
ϵ)
=
π(β, δ | σ2
ϵ)

since δ =
q
βTβ is ﬁxed given β

=
π(β | δ)π(δ | σ2
ϵ)
=
Γ(p/2)
Be(a, b)
σ2b
ϵ
πp/2(βTβ)a−p/2(σ2
ϵ + βTβ)−(a+b), β ∈Rp.
(5.4)
A detailed derivation of the density π(β | σ2
ϵ) in (5.4) using the hyperspherical
representation of the vector of regression coeﬃcients β is given in Appendix C.1.
The prior density (5.4) on β given σ2
ϵ corresponds to a spherical distribution whose
behavior depends primarily on the value of the hyperparameter a relative to the
number of predictor variables p. The three cases a < p/2, a = p/2 and a > p/2 are
described separately below. Each case corresponds to a diﬀerent behavior of the prior
at the origin. In two cases, the prior can be represented as a scale mixture of normals
(West, 1987). Such a representation of the distribution of regression coeﬃcients in
a normal linear model is well known to aid posterior computation. In this case, a
data augmentation Gibbs sampler with closed-form (conditionally normal) updates
for the regression coeﬃcients can be constructed by including updates of the latent
scale parameters. We describe such samplers in Section 5.5.2.
150

Representations and Properties of the Prior: a < p/2
When a < p/2 that the prior density (5.4) can be represented as a normal scale
mixture of the form
β | v, σ2
ϵ
∼
N(0, vσ2
ϵIp)
v
∼
Gv(·)
(5.5)
where Gv(·) has the density
gv(v) = Γ(p/2)2−bv−(b+1)
Be(a, b)Γ(b + p/2)
1F1(a + b; b + p/2; −1/2v), v > 0
and 1F1(A, B, z) is the Kummer conﬂuent hypergeometric function with B > A >
0. The derivation of the scale mixture representation is provided in Appendix C.2
and utilizes the multiplicative rule for Laplace transforms of function convolutions
(suggested by Dr. Jim Berger, personal communication, October 20, 2012).
Prior (5.5) on the scale factor v can be further broken down by adding another
step of hierarchy in the Bayesian model with the introduction of a new latent variable
λ
β | v, σ2
ϵ
∼
N(0, vσ2
ϵIp)
v | λ
∼
IG

b, 2
λ

λ
∼
Beta

a, p
2 −a

.
(5.6)
This follows directly from the derivation of the scale mixture form (5.5) in Appendix
C.2 where we show that v ∼Gv(·) with density
gv(v)
=
Γ(p/2)2−b
Γ(a)Γ(b)Γ(p/2 −a)v−(b+1)
Z 1
0
(1 −y)p/2−a−1e−y/2vya+b−1dy
=
Z 1
0
"
λb
Γ(b)2bv−(b+1)e−λ/2v
#
λa−1(1 −λ)p/2−a−1dλ.
151

The ﬁrst part of the last display is the density of an inverse-gamma distribution
and the second part is the density of a Beta distribution with the aforementioned
parameters.
The hierarchical model structure in (5.6) can be partially collapsed by marginaliz-
ing over one of the latent variables and obtaining π(β | λ, σ2
ϵ) as
R ∞
0 π(β | v, σ2
ϵ) π(v |
λ)dv. Then
β | λ, σ2
ϵ
∼
t2b

0, λσ2
ϵ
2b Ip

λ
∼
Beta

a, p
2 −a

,
(5.7)
where tν(µ, Σ) denotes a multivariate t-distribution with ν degrees of freedom, loca-
tion parameter µ and scale matrix Σ. A data augmentation Gibbs sampler may be
applied for sampling from the posterior distribution with one of the mixture repre-
sentations described by (5.5), (5.6) and (5.7).
When a < p/2, the joint density function is unbounded at the origin because
lim
||β||→0 π(β | σ2
ϵ) = ∞. But the marginal density functions of the individual coeﬃcients
βi have a ﬁnite, non-zero value at the origin. Though it is complicated to derive closed
form expressions for the marginal densities of βi, i = 1, . . . , p, evaluating any of these
densities at 0 is not cumbersome when a > 1
2 since
πβi(0 | σ2
ϵ)
=
Z
Rp−1
Γ(p/2)
Be(a, b)
σ2b
ϵ
πp/2(βT
−iβ−i)a−p/2(σ2
ϵ + βT
−iβ−i)−(a+b)dβ−i
=
Γ(p/2)σ2b
ϵ
Be(a, b)πp/2 × Be(a∗, b∗)π(p−1)/2
Γ((p −1)/2)σ2b∗
ϵ
×
Z
Rp−1
Γ((p −1)/2)σ2b∗
ϵ
Be(a∗, b∗)π(p−1)/2(βT
−iβ−i)a∗−(p−1)/2(σ2
ϵ + βT
−iβ−i)−(a∗+b∗)dβ−i
=
Γ(p/2)
Γ((p −1)/2)
Γ(a −1/2)Γ(b + 1/2)
Γ(a)Γ(b)
p
πσ2
ϵ
> 0 (if a > 1
2)
152

where β−i = {βk : k ̸= i}, a∗= a −1
2 and b∗= b + 1
2. The joint density π(β | σ2
ϵ) is
unimodal with the mode located at β = 0 because of the inﬁnite spike.
Representations and Properties of the Prior: a = p/2
When a = p/2, β | σ2
ϵ ∼t2b(0, (σ2
ϵ/2b)Ip), a multivariate t distribution with 2b
degrees of freedom with density
π(β | σ2
ϵ) = Γ(b + p/2)
Γ(b)σp
ϵπp/2

1 + βTβ
σ2
ϵ
−(b+p/2)
, β ∈Rp.
The mode of this distribution is at β = 0 and the joint density function is ﬁnite at
the mode, with the value
π(0 | σ2
ϵ) = Γ(b + p/2)
Γ(b)σp
ϵπp/2 > 0.
Each of the marginal distributions for βi, i = 1, 2, . . . , p is a univariate t-distribution
with ﬁnite density at the origin.
The multivariate t distribution has a well-known representation as a scale mix-
ture of normal distributions where the mixing distribution for the scale parameter is
inverse-gamma:
β | υ, σ2
ϵ
∼
N(0, υσ2
ϵIp),
υ
∼
IG(b, 2).
Representations and Properties of the Prior: a > p/2
When a > p/2, the joint density function is zero at the origin and the mode of
the distribution occurs along a hypersphere with radius δ =
p
βTβ = σϵ
q
a−p/2
b+p/2. As
before, we can show that the marginal density of each βi conditional on σ2
ϵ is ﬁnite
and non-zero at the origin provided a > 1
2. But this is not necessarily a restriction
153

on the value of a as a > p/2 ≥1/2 in any regression model considered. The prior has
no scale mixture of normals representation when a > p/2.
5.4
Modeling with a Science Driven Prior on ρ2
5.4.1
Model Framework
Suppose we describe the basic linear regression model as
y = Xβ + ϵ
where X = (x1, . . . , xp) is a n × p design matrix, and the n × 1 vector ϵ is a vector
of independent normal errors. This model is simply the matrix version of the regres-
sion setting that we considered in Sections 5.1 and 5.2, without the restriction that
V ar(xi) = 1 for all i = 1, 2, . . . , p. Without loss of generality, we ﬁrst center and
scale all variables so that every column of the observed matrix X and the response
y have mean zero and variance 1. Due to this centering we remove the intercept
term from the model. Further assume that the matrix X is of full column rank, i.e.,
rank(Xn×p) = p.
We work with a reparameterization of the original design X so that all likelihood
contours in the modiﬁed design W (say) are spherical. The reparameterization can
be done easily by the tranformation W = X(XTX)−1
2 which converts the original re-
gression parameters β to the new set of parameters η = (XTX)
1
2β and the regression
model to
y = Wη + ϵ
which has spherical likelihood contours in η because W TW = Ip. Recall that the
relationship between ρ2 and δ2 = βTβ from the original model is ρ2 = δ2/(δ2 + σ2
ϵ)
154

(see Section 5.2), but it depends on the assumption of unit variance for all predic-
tor variables. Following the transformation to the new regression design, we have
V ar(W) =
1
n−1Ip and ηTη = βT(XTX)β which implies that η and δ2 are connected
through the expression
ρ2 =
ηTη
ηTη + (n −1)σ2
ϵ
.
Henceforth, for notational convenience, we rename ηTη as δ2 so that now δ represents
the L2 norm of the parameter η. When ρ2 ∼Beta(a, b), we have
δ2 | σ2
ϵ
d= (n −1)σ2
ϵ
ρ2
1 −ρ2 | σ2
ϵ ∼GIB

a, b, (n −1)σ2
ϵ

.
As mentioned in Section 5.1, the model speciﬁcation is completed with an inverse-
gamma prior on the error variance σ2
ϵ, so that σ2
ϵ ∼IG(a1, b1) for suitable hyperpa-
rameters a1 and b1. We call this version of science driven prior the R-prior.
5.4.2
Transformation to Hyperspherical Coordinates
The structure of the R-priors deﬁned in Section 5.4.1 allows for the construction
of eﬃcient computational strategies for model ﬁtting. To implement the sampling
strategy we work with the spherical representation (for p dimensions) of the regression
parameter β instead of the usual Euclidean form. Any vector x = (x1, x2, . . . , xm) ∈
Rm has a representation in terms of its Euclidean norm r = ∥x∥and the (m −1)
angles θ = (θ1, . . . , θm−1)T that uniquely deﬁnes the vector.
The hyperspherical
representation of x is given by:
x1
=
r cos(θ1)
x2
=
r sin(θ1) cos(θ2)
x3
=
r sin(θ1) sin(θ2) cos(θ3)
155

...
...
xm−1
=
r sin(θ1) sin(θ2) · · · cos(θm−1)
and,
xm
=
r sin(θ1) sin(θ2) · · · sin(θm−1)
where the angles θi ∈[0, π], for i = 1, 2, . . . , m −2 and θm−1 ∈[0, 2π). We call
the m-tuple (r, θ) the representation of the m-dimensional vector x in the spherical
coordinate system.
The p-dimensional parameter η can thus be characterized in terms of spherical
coordinates with (p −1) angular components φ1, φ2, . . . , φp−1 and a size component
||η|| =
p
ηTη = δ as described above. Conditional on δ and σ2
ϵ, the entire prior
mass of η lies on the surface of a p-dimensional hypersphere of radius δ. We specify
that the transformed regression coeﬃcient η is uniformly distributed on the (p −1)-
dimensional surface. The surface area of a p-dimensional hypersphere of radius δ is
1
Γ(p/2)2πp/2δp−1, so that the density of η given δ is
π(η | δ) =
Γ(p/2)
2πp/2δp−1, η ∈{z ∈Rp : zT z = δ2}.
(5.8)
The last assumption implies that the regression coeﬃcients ηi, i = 1, 2, . . . , p, in
the reparameterized model are identically distributed for a given δ, but they are not
independent of each other because of the constraint on their total size. This generates
a form of spherical symmetry in the η coeﬃcients lying uniformly on the surface of the
hypersphere that makes the joint distribution of η exchangeable. Prior (5.8) translates
to the following induced prior on the angular components φ = (φ1, . . . , φp−1)T in the
hyperspherical characterization of η (see relation (C.1) in Appendix C.1)
156

π(φ | δ2) = Γ(p/2)
2πp/2
p−2
Y
j=1
sinj(φp−1−j)
where 0 ≤φi ≤π for i = 1, 2, . . . , p −2 and 0 ≤φp−1 < 2π. The form of the above
prior indicates that all the (p −1) angles are independent of each other as well as
independent of the parameter δ2.
5.4.3
Rotation of the Coordinate Axes
The eﬃcient sampling algorithm that we introduce later hinges on a further repa-
rameterization of the model and the associated parameters. The goal of the transfor-
mation is to ensure that the least squares estimate of the parameter η, bηLS = W Ty,
has only one non-zero Euclidean coordinate (the ﬁrst component) while the rest are
all zero. If we denote the hyperspherical representation of the vector bηLS by (r, θ),
this is equivalent to rotating the coordinate axes appropriately to make all angles θi
equal to zero in the new system. The idea of rotation is crucial to streamline sam-
pling from the posterior distribution limited to the surface of a hypersphere. The
transformation is implemented by rotating each axis with the objective of making the
least squares estimate bηLS point directly toward the top or the “north pole” of the
p-dimensional hypersphere. In the rotated design, any spherical likelihood contour
(a rotation preserves the spherical likelihood contours) and the surface of the hyper-
sphere intersect at a ﬁxed angle θ∗
1, the ﬁrst angle in the spherical representation
of the rotated regression parameter. Hence the (p −1)-dimensional sphere arising
from the intersection of any spherical contour and the p-dimensional hypersphere of
a speciﬁed radius is uniquely determined and characterized by the angle θ∗
1 only, and
we denote this sphere as Cθ∗
1.
157

The steps undertaken to implement the rotation are detailed below.
(i) First we determine the least squares estimate bηLS = W Ty in the spherical
system. This entails knowledge of (p −1) angular components θ and a size
component r =
q
bηT
LSbηLS for the vector bηLS.
(ii) Construct rotation matrices Hi, i = 1, 2, ..., p −1, where each matrix Hi rotates
the coordinate axes in one direction at a time, to align along the direction with
bηLS at the “north pole”. Speciﬁcally, for each i ∈{1, 2, ..., p −1}, we create the
ith rotation matrix as
Hi =











1
0
. . .
. . .
. . .
0
0
0
1
. . .
. . .
. . .
0
0
...
...
...
...
...
0
. . .
cos(θi)
sin(θi)
. . .
0
0
. . .
−sin(θi)
cos(θi)
. . .
0
...
...
...
...
...
...
0
0
. . .
. . .
. . .
0
1











(iii) Calculate the ﬁnal rotation matrix R as a combination of the individual rotation
matrices Hi as R =
p−1
Q
i=1
Hi. Since any rotation matrix is orthogonal, we have
R−1 = RT.
(iv) After determining the complete rotation matrix R, we reparameterize the re-
gression model as
y = Wη + ϵ = WRTRη + ϵ = V τ + ϵ
where τ = Rη and V = WRT.
Observe that in the new regression setting, bτ LS = (V TV )−1V Ty = RW Ty = RbηLS,
a simple rotation of bηLS that constrains bτ LS to be of the form (||bτ LS||, 0, . . . , 0)T.
158

More signiﬁcantly, τ Tτ = ηTRTRη = ηTη = δ2, displaying that spherical symmetry
is preserved in the new regression parameter τ. Let (δ, φ) denote the parameter τ
in the spherical coordinate system from here on so that φ1, φ2, . . . , φp−1 now relate to
the parameter τ (and not η).
5.5
Sampling from the R-Posterior
Posterior inference with the R-prior speciﬁcation on a linear model is not straight-
forward and might be computationally intensive without the transformation, as the
posterior distributions for the regression parameters are not available in closed form.
Direct application of MCMC methods—for example, coordinate-wise Metropolis–
Hastings updates—without the model reparameterization is possible, but the resulting
chains may converge slowly. We suggest two methods for carrying out inference in the
R-posterior. The ﬁrst one involves executing a Gibbs sampler in a reparameterized
design after rotating the coordinate axes. The motivation behind reparameterizing
the model is to develop a sampling algorithm that is eﬃcacious in terms of conver-
gence, speed and storage. The second method is the application of the standard Gibbs
sampling algorithm in an augmented model using the scale mixture representations
derived in Section 5.3.
5.5.1
Gibbs Sampling in the Reparameterized Design
For the linear model y = V τ + ϵ described in Section 5.4.3, the least squares
estimator of τ is of the form bτ LS = (δ0, 0, . . . , 0)T for some δ0 = ||bτ LS|| and hence
the log-likelihood (given σ2
ϵ) can be simpliﬁed as
−1
2σ2
ϵ
(y −V τ)T(y −V τ) = −1
2σ2
ϵ
h
(y −V bτ LS)T(y −V bτ LS) + (τ −bτ LS)T(τ −bτ LS)
i
159

since V TV = Ip. Recall that (δ, φ) denotes τ in the spherical coordinate system and
so τ =
 δ cos(φ1), δ sin(φ1) cos(φ2), . . . , δ sin(φ1) sin(φ2) · · · sin(φp−1)
T. Thus,
(τ −bτ LS)T(τ −bτ LS)
=
p
X
i=1
(τi −bτi,LS)2
=
(τ1 −δ0)2 +
p
X
i=2
(τi −0)2
=
[δ cos(φ1) −δ0]2 + δ2 sin2(φ1)
=
δ2 + δ2
0 −2δδ0 cos(φ1)
which implies that the likelihood in the transformed model depends only on one
component (φ1) of φ when conditioned on δ and σ2
ϵ. After the rotation of the axes,
only the full conditional of φ1 in the Gibbs sampler gets updated using information
from the data, while the rest of the angles have full conditionals that are independent
of the likelihood. This speeds up sampling because at each iteration of the Gibbs
sampler, we need to draw from distributions for the three parameters σ2
ϵ, δ2 and φ1
only, irrespective of the number of predictors (p) in the model. The ﬁxed dimensional
parameterization not only helps to learn about the posterior distribution rapidly by
discarding (p −2) parameters while performing the MCMC steps, but also promotes
eﬃcient storage of posterior samples in high dimensional problems with large values
of p.
The transformed regression coeﬃcients τi, i = 1, . . . , p, are functions of the hyper-
spherical parameters δ2 and φ so that τ = τ(φ, δ). All the angles in φ range from
0 to π apart from the last one which varies between 0 and 2π. For computational
simplicity, we take the range of all the angles to be the same, i.e., from 0 to π and
add a new random variable γ ∈{−1, 1} to the model that decides which half of the
real axis φp−1 lies in. Renaming the restricted version of the parameter φp−1 as φ∗
p−1,
160

we follow the convention that γ = −1 actually corresponds to φp−1 = 2π −φ∗
p−1
while γ = 1 means φp−1 = φ∗
p−1. Since the prior on φp−1 is uniform on (0, 2π), the
implied prior on γ is discrete uniform on {−1, 1}. Let φ∗= (φ1, . . . , φp−2, φ∗
p−1)T and
it follows that τ = τ(φ∗, δ, γ).
We sample from the posterior distributions of the unknown regression parameters
using the Gibbs sampler:
Full conditional of δ2
Standard Metropolis–Hastings (MH) algorithms are suitable to draw samples from
the full conditional of δ2.
π(δ2 | y, σ2
ϵ, γ, φ∗) ∝(δ2)a−1h
(n −1)σ2
ϵ + δ2i−(a+b)
× exp

−1
2σ2
ϵ
(y −V τ)T(y −V τ)

, δ2 ≥0.
A random-walk MH algorithm is used to generate samples from the posterior of δ2.
At the tth iteration, a proposed value of δ2 is drawn from a distribution which is
uniform on

(δ2)(t−1) −B, (δ2)(t−1) + B

, where B is a predetermined step size. The
proposed positive value of δ2 is either accepted or rejected according to the usual MH
acceptance ratio. Negative proposed values of δ2 are rejected, as the MH acceptance
ratio is zero.
Full conditional of σ2
ϵ
Posterior samples of σ2
ϵ can be drawn conveniently using the Adaptive Rejection
Sampling (ARS) method on a transformed variable. The full conditional of σ2
ϵ is:
π(σ2
ϵ | y, δ2, γ, φ∗)
∝
h
(n −1)σ2
ϵ + δ2i−(a+b)
(σ2
ϵ)b−(a1+1)
× 1
σn
ϵ
exp

−1
b1σ2
ϵ
−
1
2σ2
ϵ
(y −V τ)T(y −V τ)

161

∝
h
(n −1)σ2
ϵ + δ2i−(a+b)
σ2b−2a1−n−2
ϵ
× exp

−1
σ2
ϵ
n 1
b1
+ 1
2(y −V τ)T(y −V τ)
o
, σ2
ϵ > 0.
It is shown in Appendix C.3 that the full conditional of 1/σ2
ϵ is log-concave when
n > 2(b + 1) −2a1. So for a suﬃciently large sample size, we can use ARS to get a
sample from the full conditional of 1/σ2
ϵ, and invert this value to get a sample from
the full conditional of σ2
ϵ.
Full conditional of φ2, φ3, . . . , φ∗
p−1
This posterior is most convenient to sample from because of the nice structure
induced in the density by the axis rotations. When conditioned on φ1, the full con-
ditional is independent of the data as the spherical likelihood contours intersect the
hypersphere along a circular region characterized by a ﬁxed angle φ1, over which the
likelihood is constant throughout. If Cφ1 denotes the (p −1)-dimensional circle gen-
erated at the intersection of a spherical contour and the p-dimensional hypersphere
of radius δ, varying any of the angles φ2, φ3, . . . , φ∗
p−1 has no eﬀect on the likelihood
over the region Cφ1. So sampling from the posterior is equivalent to sampling from
the independent prior distributions on each of the (p −2) angles:
[φ2, φ3, . . . , φ∗
p−1 | y, σ2
ϵ, δ2, γ, φ1]
d=
[ φ2, φ3, . . . , φ∗
p−1 | Cφ1, γ]
d=
[ φ2, φ3, . . . , φ∗
p−1 | γ]
where
π(φ2, φ3, . . . , φ∗
p−1 | γ) =
Γ(p−1
2 )
π(p−1)/2 sinp−3(φ2) sinp−4(φ3) · · · sin(φp−2)
162

over the parameter space (0, π)p−2. Since φ∗
p−1 does not explicitly appear in the joint
prior density, the prior distribution on φp−1 is uniform over (0, π) when γ = 1 and
uniform over (π, 2π) when γ = −1.
Full conditional of φ1
Again the MH algorithm is used to draw from the full conditional of φ1:
π(φ1 | y, σ2
ϵ, δ2, γ, φ2, φ3, . . . , φ∗
p−1)
∝
sinp−2(φ1) exp

−1
2σ2
ϵ
(y −V τ)T(y −V τ)

,
for 0 ≤φ1 ≤π.
We implement the Metropolis–Hastings algorithm with one of two diﬀerent choices
of proposal distributions for φ1. But ﬁrst φ1 is scaled so that the range of the new
parameter ω = φ1/π is between 0 and 1.
(a) The ﬁrst proposal distribution is based on the random-walk MH algorithm. We
draw ω(t) from Uniform
 ω(t−1)−Ω, ω(t−1)+Ω

, where Ω(< 1) is a predetermined
step size and ω(t−1) is the sample from the last (current) iteration.
To avoid
direct rejection of the ω(t) samples proposed outside the interval (0,1), we use a
wrapping-around strategy that ensures that all proposed ω values are utilized in
the Markov chain. For any 0 < B < Ω, if ω(t) = −B is proposed, we immediately
change the proposal to ω(t) = 1 −B, which obviously lies within the proper
parameter space (0, 1). Likewise, a proposal ω(t) = 1 + B is ﬁrst converted to
ω(t) = B before calculating the acceptance ratio. Proposals ω(t) are transformed
back to φ(t)
1
= πω(t) which are then accepted or rejected depending on the MH
acceptance probability.
(b) The second method involves generating a proposal ω(t) from a Beta
 ω(t−1)λ, (1−
ω(t−1))λ

distribution which has mean ω(t−1) and variance
ω(t−1)(1−ω(t−1))
1+λ
. The
163

value of λ is chosen to be large so that proposals are close to the current iteration
when ω(t−1) is small (very close to zero) or large (very close to one). The large
value of λ stabilizes the acceptance rate for proposals when an iteration ω(t) is
close to either 0 or 1.
Full conditional of γ
The parameter γ is used to determine the quadrant in which φp−1 belongs. Con-
ditional on all the angles φ, the full conditional of γ is the same as the prior on γ
which is discrete uniform over two values:
[γ | y, σ2
ϵ, δ2, φ∗]
d=
Uniform {−1, 1}.
An important observation here is that when sampling from the full conditionals of
δ2, φ1, σ2
ϵ and γ, the current value of the vector φ∗
−1 = (φ2, . . . , φ∗
p−1)T does not aﬀect
the Markov chain in any way. So we ﬁx φ∗
−1 at the beginning and skip sampling
from the full conditional of φ∗
−1 in the Gibbs sampling steps.
Once the Markov
chain is deemed to have reached its stationary distribution and posterior samples
of a suﬃciently large size have been collected, it is straightforward to draw from
the individual priors of φi, i = 1, 2, . . . , p −1, and get samples of τ by converting
 (δ2)(t), φ(t)
to the Euclidean form for each iteration t .
5.5.2
The Data Augmentation Gibbs Sampler
The innovative reparameterization of the model allows us to develop a useful
and economical MCMC method. Nevertheless, when a < p/2, it is also possible to
work with the original model and the prior (5.4) on β using the scale mixture repre-
sentations outlined in (5.5), (5.6) and (5.7). The case when a = p/2 can be handled
164

similarly using the normal scale mixture form of a multivariate t-distribution. Though
it is possible to perform Gibbs sampling with the mixture representations (5.5) and
(5.7), the full conditionals for some of the parameters in both represenetations are
complicated distributions which are diﬃcult to sample from. A Metropolis within
Gibbs sampling algorithm can be implemented to sample from these complex distri-
butions, but tuning proposal distributions (to achieve reasonable acceptance rates for
the proposed samples) for the Metropolis step requires some eﬀort. However, it is
easy to sample from the remaining full conditionals which are standard distributions.
We suggest using the data augmentation Gibbs sampler on the regression model
described by the scale mixture form in (5.6) when a < p/2, since the full conditionals
for the latent variables in this form are simpler to manage. The full model is
y | β, σ2
ϵ
∼
N(Xβ, σ2
ϵIn)
β | v, σ2
ϵ
∼
N(0, vσ2
ϵIp)
v | λ
∼
IG

b, 2
λ

λ
∼
Beta

a, p
2 −a

and,
σ2
ϵ
∼
IG(a1, b1).
The full conditional distributions for the Gibbs method are as follows:
(i) Full Conditional of β, deﬁned on Rp:
π(β | y, v, λ, σ2
ϵ) = N

β | (XTX + 1
vIp)−1XTy, (XTX + 1
vIp)−1σ2
ϵ

(ii) Full Conditional of v, deﬁned on (0, ∞):
π(v | y, β, λ, σ2
ϵ) = IG

v | b + p
2, 2
 λ + βTβ
σ2
ϵ
−1
165

(iii) Full Conditional of σ2
ϵ, deﬁned on (0, ∞):
π(σ2
ϵ | y, β, λ, v) = IG

σ2
ϵ | a1 + n
2,
 1
b1
+ 1
2(y −Xβ)T(y −Xβ)
−1
(iv) Full Conditional of λ:
π(λ | y, β, v, σ2
ϵ) ∝λa+b−1(1 −λ)p/2−a−1e−λ/2v , 0 < λ < 1
This full conditional corresponds to a conﬂuent hypergeometric (CH) distribu-
tion (Gordy, 1998), which is a special case embedded within the class of com-
pound conﬂuent hypergeometric (CCH) distributions (Gordy, 1998; Armagan
et al., 2011). Polson and Scott (2009) refer to the class of CCH distributions as
the hypergeometric-beta (HB) distributions. The full conditional correponds to
a CH(a + b, p/2 −a, 1/2v) distribution with the normalized density
π(λ | y, β, v, σ2
ϵ) = λa+b−1(1 −λ)p/2−a−1 exp(−λ/2v) I(0 < λ < 1)
Be(a + b, p/2 −a) 1F1(a + b, b + p/2, −1/2v) .
(5.9)
Polson and Scott (2009) suggest utilizing a rejection sampling algorithm to draw
samples from the HB distribution. Since the CH distribution is a special case
of the HB/CCH distribution, the same sampling technique will work to obtain
posterior samples of λ.
They show that (5.9) is bounded by Mλa+b−1(1 −
λ)p/2−a−1/Be(a+b, p/2−a), for an appropriately chosen ﬁnite constant M > 0.
So we can generate proposals of λ from a Beta(a + b, p/2 −a) distribution and
use M ×Beta(λ | a+b, p/2−a) as the envelope for (5.9) in the rejection sampler.
When a = p/2, the prior on β is a multivariate t-distribution which can be
expressed as a normal scale mixture with an inverse-gamma mixing distribution. The
full conditionals in this situation will either be normal or inverse-gamma and a fast,
eﬃcient Gibbs chain can be run eﬀortlessly.
166

5.6
Model Uncertainty
The MCMC techniques discussed in the last two sections are based on sampling
from a ﬁxed design corresponding to a particular model. But the true data generating
model is never known to the researcher beforehand in any real world problem and
there is always some uncertainty in choosing the best model to explain the variation in
the data. For estimation and prediction purposes, it is common in a Bayesian setting
to average over posterior summaries from a range of models. The summary from each
model is weighted appropriately to reﬂect the belief of the researcher and the ability
of the given model to explain the observed data pattern. Bayesian model averaging
(BMA) requires a prior distribution on the space of all possible models. Subjective
elicitation of prior probabilities for all 2p probable models becomes diﬃcult for large or
even moderately large problems and default priors are set on the model space in such
cases. Let M denote the set of all 2p models that can be constructed from all viable
combinations of the p candidate predictors. A popular default prior on M is the
Bernoulli variable inclusion prior where a predictor variable is included or excluded
from a model with a ﬁxed probability w (say) ∈(0, 1), independent of the other
predictors. We follow this strategy in the remainder so that the prior probability of
any model M of size k is π(M) = wk(1−w)p−k, k ∈{0, 1, . . . , p}. We note that other
priors on model space can be easily accommodated (e.g., Scott and Berger (2010)).
The Gibbs sampler for the reparameterized model, discussed in the previous sec-
tion, can be suitably modiﬁed in this situation by adding the model space to the
existing parameter space and treating each model as a random entity.
A similar
strategy is adopted in the MC3 algorithm of Madigan and York (1995) and the SSVS
algorithm of George and McCulloch (1993, 1997). The Markov chain now traverses
167

the set of models as well, visiting models with high posterior probabilities more often
than the ones with low posterior probabilities. However, inclusion of the model space
within the parameter space comes with additional implications as the Markov chain
now has a state space with varying dimensions. This happens because the number
of components in φ in the transformed design is a function of the size of the model.
Depending on whether a proposed model in the Gibbs sampler is of a higher or lower
dimension, the dimension of φ also increases or decreases simultaneously. To accom-
modate for this restriction, we design the sampling steps to allow a joint update of
the model and some of the angles together in the following manner.
Full Conditional of δ2
The full conditional of δ2 given a particular model is similar to the one described
in Section 5.5, the only diﬀerence now being that some of the parameters are model
speciﬁc:
π(δ2 | y, σ2
ϵ, φ, M, γ)
∝
(δ2)a−1
(n −1)σ2
ϵ + δ2−(a+b)
× exp

−1
2σ2
ϵ
(y −VMτ)T(y −VMτ)

, δ2 ≥0.
where VM describes the rotated design matrix originating from a transformation of
the original design XM corresponding to predictors from the model M. The random-
walk Metropolis algorithm with proposals from a uniform distribution is again used
to draw samples from this density.
Full Conditional of σ2
ϵ
This full conditional is also identical to the one outlined in the preceding section,
but the density again depends on the given model M:
168

π(σ2
ϵ | y, δ2, φ, M, γ)
∝

(n −1)σ2
ϵ + δ2−(a+b)
σ2b−2a1−n−2
ϵ
× exp

−1
σ2
ϵ
n 1
b1
+ 1
2(y −VMτ)T(y −VMτ)
o
, σ2
ϵ > 0.
Full Conditional of φ1
We can use one of the two proposal distributions laid out in Section 5.5 within a
random-walk Metropolis sampler to draw samples from the density
π(φ1 | y, σ2
ϵ, δ2, φ−1, M, γ) ∝sin|M|−2(φ1) exp

−1
2σ2
ϵ
(y −VMτ)T(y −VMτ)

deﬁned on 0 ≤φ1 ≤π, where |M| denotes the size of the given model M.
Full Conditional of (M, φ−1)
This is the only full conditional which is inherently diﬀerent from all the full
conditionals appearing in Section 5.5. The joint update of M and φ−1 is necessary
due to the complication of diﬀering dimensions. We factor the density as
π(M, φ−1 | y, δ2, φ1, σ2
ϵ, γ) ∝π(φ−1 | M, y, δ2, φ1, σ2
ϵ, γ) π(M | y, δ2, φ1, σ2
ϵ, γ)
from which we sample with a Metropolis–Hastings algorithm described in Appendix
C.4.
The proposal distribution q((M, φ−1) →(M ∗, φ∗
−1)) to update the current
iteration (M, φ−1) to the new values (M ∗, φ∗
−1) is also chosen so that it factors as
q((M, φ−1) →(M ∗, φ∗
−1)) = p(M ∗| M) p(φ∗
−1 | M ∗).
We choose a simple and
symmetric proposal distribution p(M ∗| M) = p(M | M ∗) where a future model M ∗
is suggested by randomly adding or deleting a single explanatory variable from the
current model M, so the newly proposed model always diﬀers from the current model
by exactly one predictor. The proposal density p(φ∗
−1 | M ∗) is chosen so that the
169

angles in φ∗
−1 are independent and each angle has a uniform proposal distribution on
(0, π).
Full Conditional of γ
The full conditional of γ (whenever the parameter exists in the model) for any
arbitrary model M is discrete uniform over two possible values
[ γ | y, σ2
ϵ, δ2, φ, M ]
d= Unif {−1, 1}.
However, the combination of sampling steps mentioned above work only for models
of size strictly greater than 2 (p > 2) when the parameter φ−1 is actually present in
the model. For models of size p = 2, φ−1 ceases to exist and we have to modify the
algorithm slightly. We update from the full conditionals of δ2, σ2
ϵ, φ1 and γ as usual,
but the last step of the Gibbs sampler deals only with the full conditional of a model
(since φ−1 does not appear in the model). So we have
π(M | y, δ2, φ1, σ2
ϵ, γ) ∝p(y | M, δ2, φ1, σ2
ϵ, γ) π(M)
and the MH acceptance probability for a proposed model M ∗is
α = min

1, p(y | M ∗, δ2, φ1, σ2
ϵ, γ)
p(y | M, δ2, φ1, σ2
ϵ, γ)
π(M ∗)
π(M)

when we use the same symmetric proposal density p(M ∗| M) as before.
In any model of size p = 1, the entire parameter vector φ is missing. We instead
consider a new parameter I(τ > 0) in the model because given a value of δ2, τ can
take either of two values: δ or −δ. The parameter γ is no longer required and is
also not included in the model. So the Gibbs sampler is set up to sample from the
following full conditionals:
170

(i) π(δ2 | y, σ2
ϵ, M, I(τ > 0)).
(ii) π(σ2
ϵ | y, δ2, M, I(τ > 0)).
(iii) π(I(τ > 0) | y, σ2
ϵ, M, δ2).
(iv) π(M | y, σ2
ϵ, δ2, I(τ > 0)).
If we wish to persist with the original parameter φ (= φ1), the new parameter I(τ >
0) can be replaced by a discrete distribution on φ1, with equal prior probability of
0.5 on the choices of φ1 = 0 and φ1 = π. The MH acceptance probability for (iv) is
calculated in the same way as in the case when p = 2.
For the unique model of size zero (p = 0) with no predictors, neither φ nor γ
shows up in the model and furthermore, the distribution of δ becomes degenerate
at 0. So the Gibbs sampler works with only two steps: (a) sampling from the full
conditional of σ2
ϵ keeping the value of δ2 ﬁxed at 0 and (b) proposing a new model M
and a new δ2 jointly which is accepted according to the MH acceptance ratio. Note
that the proposal for M is always a model with exactly one predictor.
A unique characteristic of the Gibbs sampling algorithm discussed here is that
migration from one non-null model to any other non-null model in the sampler takes
place holding ρ2 ﬁxed in both models. Since ρ2 is completely determined with the
knowledge of σ2
ϵ and δ2, the proposed model M ∗from this full conditional will share
the identical ρ2 value as the current model M. This is indeed a very sensible feature
which is not common in prevalent MCMC methods. A model of higher (lower) dimen-
sion is usually favored (disfavored) by a goodness of ﬁt criterion due to the presence
(lack) of additional predictors that further improve (deteriorate) the model ﬁt. This
sampler cuts across models of diﬀerent sizes in a much more meaningful manner as it
171

ﬁxes the strength of linear model ﬁt and allows the data to choose the more suitable
covariates.
The scale mixture representation of β in the case when a ≤p/2 can be extended
similarly to develop a data augmentation Gibbs sampler that deals with model un-
certainty. The usual Gibbs sampling steps for model-speciﬁc regression parameters
need to be supplemented by a simultaneous update of the model in each iteration.
Due to the recurring issue of uneven parameter dimensions across diﬀerent models,
the updates for β and M need to be carried out simultaneously.
5.7
Application to US Crime Data
In this section we analyze the performance of the R-prior on the US crime data
set from 1960 and investigate the practical beneﬁts of using this new form of science
driven prior. We assess how the procedure functions under a variety of Beta prior
speciﬁcations (by suitably adjusting the hyperparameters) on the strength of linear
regression, with diﬀerent priors concentrating most of the mass of ρ2 on diﬀerent
regions of the unit interval.
5.7.1
US Crime Data
Originally the US crime data was analyzed by Isaac Ehrlich (Ehrlich, 1973), where
he scientiﬁcally analyzed the rates for crimes committed in 47 of the 50 U.S. states in
the year 1960. The study stressed on the deterrent eﬀects of punishment on seven FBI
index crimes: murder, rape, robbery, assault, burglary, larceny and theft. The crime
data set has 15 candidate predictor variables which were used by Ehrlich to examine
the relationship between aggregate levels of punishment and crime rates. Most of the
15 predictor variables reported in the study were socio-economic variables like family
172

income, number of families earning below half of the median income, unemployment
rates for urban males (per 1000 males) in the age groups 14-24 and 35-39, education
level, labor force participation rate, number of young males and non-whites (per 1000)
in the population, sex ratio, indicator for southern states and the state population
size. The other variables included were per capita police expenditure for 1959 and
1960, average time served in state prisons and probability of imprisonment of the
oﬀenders. These variables were used to predict crime rates in diﬀerent states, deﬁned
as the number of oﬀenses reported to the police per hundred thousand population.
Since 47 states were included in the study (excluded states are Hawaii, New Jersey
and Alaska), the dataset has 47 cases and 15 predictor variables.
We ﬁrst log transform all the predictors (apart from the single indicator variable)
and then center and scale the variables to have mean zero and variance one. The
classical linear model ﬁt using all the 15 predictor variables, but without the intercept
term (the Bayesian model in this analysis also does not include an intercept), leads to
only 6 predictors: education level, number of young males (per 1000), unemployment
rates for urban males in age group 35-39, number of non-whites (per 1000), probability
of imprisonment and number of families below half the median income (per 1000)
being signiﬁcant at 5% level of signiﬁcance. The sample R2 value obtained from the
full model equals 0.8688, and the adjusted R2 equals 0.8073, so the data suggest that
the true population correlation coeﬃcient is on the higher side. The prior distribution
on σ2
ϵ is inverse-gamma, as mentioned in the model speciﬁcation for the R-prior, with
parameters a1 and b1 chosen meaningfully. The mean and variance of the inverse-
gamma distribution are selected to be 0.6 and 0.4 respectively. Specifying the ﬁrst
two moments of this distribution uniquely determines the hyperparameters a1 and b1.
173

The model space is deﬁned by the inclusion or exclusion of each predictor variable
and contains a total of 215 models. We assume a Bernoulli(w) variable inclusion prior
on the model space so that each predictor variable is included in a model with prior
probability w. We consider three choices of w in this analysis: (i) w = 1
2, (ii) w = 1
3
and (iii) w = 1
4.
Figure 5.1: Kernel density estimates of the posteriors of δ, σ2
ϵ and ρ2 from the full
model.
174

Model prior (i) corresponds to a uniform prior on the model space where all
models are equally likely a priori. Priors (ii) and (iii) place more mass on smaller
models, and any predictor is more likely to be excluded than being included in a
particular model. Big models are favored a posteriori only when the data strongly
favors their inclusion. We carry out posterior inference on the US crime data set
using the novel and computationally eﬃcient MCMC strategy on the reparameterized
design described in Sections 5.4 and 5.6.
5.7.2
Eﬀect of the Prior on ρ2
The prior information on ρ2 has a considerable eﬀect on the posterior distributions
of ρ2, δ2 and σ2
ϵ. Figure 5.1 shows how the posterior distributions from the full model
(model containing all 15 predictors) under R-priors vary with diﬀerent choices of
the hyperparameters a and b in the Beta prior for ρ2. It is clear from Figure 5.1
that as the mean
  a
a+b

of the prior distribution increases (i.e., high values of ρ2 are
allocated more prior mass), the posterior density estimate of ρ2 concentrates near
the high sample R2 value obtained from ordinary regression. When the strength of
regression is assumed to be very weak in the prior, as in the case with a = 2 and
b = 6, the posterior density estimate of ρ2 becomes ﬂatter and the mode shifts toward
a lower value. The same eﬀect is also noticed in the density estimate for δ which also
becomes more diﬀuse when the prior on ρ2 focuses around smaller values. Moderate
strength of ρ2 in the prior (when a = 4, b = 4) causes the posterior density of ρ2
to shift midway between the other two cases. The posterior densities of σ2
ϵ and δ
are inversely related: concentration of posterior mass of δ around a higher value is
accompanied by concentration of posterior mass of σ2
ϵ near a smaller value. When
175

the prior on ρ2 focuses more on large values, the σ2
ϵ posterior gets pulled back a bit
toward zero and the distribution becomes tighter and more peaked.
Figure 5.2: Kernel density estimates of the posterior of ρ2 from the full model when
the prior mean is ﬁxed.
A researcher might choose to include diﬀerent levels of prior information in the
model through the Beta prior on ρ2. Figures 5.2 and 5.3 illustrate how the posterior
density of ρ2 from the full model adapts to changes in the center and spread of the
prior distribution. Observe that the variance of a Beta(a, b) distribution is given by
ab
(a+b)2(a+b+1) = µ(1−µ)
a+b+1 , where µ =
a
a+b is the mean of the same distribution. Hence for
a given mean, increasing (a + b) has the eﬀect of lowering the variance of the Beta
distribution. Figure 5.2 displays that when the prior mean is high, the spread of the
prior distribution has very little eﬀect on the posterior. But strong prior information
176

(corresponding to a large (a+b) value) with a small mean drags the posterior toward
the lower values and makes the density more diﬀuse. Figure 5.3 illustrates that the
prior mean again pulls the center of the posterior toward itself, but the pull is much
stronger in the case when (a + b) is ﬁxed at a large value. This is reasonable, since
a higher (a + b) value with the identical mean results in a tighter prior distribution
with smaller variance.
Figure 5.3: Kernel density estimates of the posterior of ρ2 from the full model when
a + b is ﬁxed.
5.7.3
Cross-validated Predictive Performance
In this subsection we compare the predictive performances of existing Bayesian
methods to that of the novel R-prior proposed in this chapter. The BAS package in R
177

(Clyde et al., 2012) has been used to obtain the prediction results for the standard pri-
ors popular in current statistical literature. We perform a twelve-fold cross-validation,
dividing the 47 cases into eleven random groups of size 4 and one group of size 3. A
variety of priors on the squared population correlation coeﬃcient ρ2 are considered,
and the diﬀerent Beta prior densities focus on distinct regions of the parameter space
in diﬀerent magnitudes. The Beta prior hyperparameters are selected to manifest
many possible contrasting levels of belief on the suitability of the predictor variables
in the regression model.
Prior on β
MSE
Beta(a, b) Prior on ρ2
MSE
g-prior (g = n)
0.3623
a = 1, b = 7
0.3305
g-prior (g = p2)
0.4055
a = 2, b = 6
0.3208
EB-Local
0.3516
a = 4, b = 4
0.3170
EB-Global
0.3513
a = 6, b = 2
0.3102
ZS-Null
0.3540
a = 2, b = 14
0.3997
ZS-Full
0.3401
a = 4, b = 12
0.3590
Hyper-g (a = 3)
0.3522
a = 8, b = 8
0.3296
Hyper-g (a = 4)
0.3511
a = 12, b = 4
0.3108
Table 5.4: Cross-validated predictive MSE (under BMA) for diﬀerent procedures
when w = 1
2.
The distributions on ρ2 reﬂect prior faith ranging from extreme pessimism to utter
optimism regarding the strength of the linear regression model in explaining the data
variation. The centers (means) for the various prior choices are ﬁxed at four distinct
values representing diﬀerent levels of conﬁdence in the linear model: 0.125 (extreme
pessimism), 0.25 (pessimism), 0.5 (moderate faith) and 0.75 (optimism). The same
prior mean is assigned to a pair of priors in each case, but the variability within
178

Prior on β
MSE
Beta(a, b) Prior on ρ2
MSE
g-prior (g = n)
0.4019
a = 1, b = 7
0.3530
g-prior (g = p2)
0.4452
a = 2, b = 6
0.3412
EB-Local
0.3882
a = 4, b = 4
0.3344
EB- Global
0.3875
a = 6, b = 2
0.3289
ZS-Null
0.3924
a = 2, b = 14
0.4023
ZS-Full
0.3642
a = 4, b = 12
0.3673
Hyper-g (a = 3)
0.3883
a = 8, b = 8
0.3368
Hyper-g (a = 4)
0.3853
a = 12, b = 4
0.3259
Table 5.5: Cross-validated predictive MSE (under BMA) for diﬀerent procedures
when w = 1
3.
a pair are disparate since the hyperparameter choices are diﬀerent. Recall that a
higher value of (a + b) is associated with a lower prior variance and implies stronger
prior information. Regardless of the prior distribution on the model space (w = 1
2
or 1
3 or 1
4), an identical pattern is visible in each of the three tables. In Tables 5.4,
5.5 and 5.6, among all the g-priors and their mixtures, the full-based Zellner–Siow
prior performs the best with respect to cross-validated predictive mean squared errors
(MSE). Clearly the R-priors are doing a much better job at prediction in this example
than all of the g priors when the prior information imparted on ρ2 is sensible. As
long as the prior on ρ2 is not too strongly pessimistic (low center with little spread),
the R-prior is superior in prediction under any choice of w. Observe that even if
the prior mean is low but the prior information is weak (large prior variance), the
posterior can adapt suﬃciently well to deliver satisfactory predictions. The failure
of the R-posterior predictions in the case of pessimistic priors with small spread is
not surprising, since a prior plays a signiﬁcant role in determining the behavior of
the posterior. Strong and inaccurate prior information invariably deteriorates the
179

quality of any Bayes procedure. Nonetheless, an immense improvement in prediction
is noticeable when the subjective prior on ρ2 is not terribly inconsistent with the
truth. The “optimistic” priors on ρ2, and even the “moderately optimistic” priors,
display tremendous enhancement in performance compared to the traditional priors
in all three tables, which substantiates the potential of rational science driven prior
elicitation.
Prior on β
MSE
Beta(a, b) Prior on ρ2
MSE
g-prior (g = n)
0.4235
a = 1, b = 7
0.3604
g-prior (g = p2)
0.4608
a = 2, b = 6
0.3556
EB-Local
0.4105
a = 4, b = 4
0.3411
EB- Global
0.4099
a = 6, b = 2
0.3386
ZS-Null
0.4149
a = 2, b = 14
0.4117
ZS-Full
0.3808
a = 4, b = 12
0.3760
Hyper-g (a = 3)
0.4101
a = 8, b = 8
0.3427
Hyper-g (a = 4)
0.4063
a = 12, b = 4
0.3377
Table 5.6: Cross-validated predictive MSE (under BMA) for diﬀerent procedures
when w = 1
4.
5.8
Summary
In this chapter, we identify an undesirable modeling implication on the distribu-
tion of the population correlation coeﬃcient ρ when common priors are imposed on a
linear model. We propose a version of a science driven prior called the R-prior, which
rectiﬁes the unwanted implication by directly modeling ρ2 and apportions probabili-
ties to the parameter space compatible with sensible prior belief. We suggest a novel
MCMC method for inference based on a reparameterization of the model that scales
well in high dimensional problems. The R-prior can also be represented as a normal
180

scale mixture in certain situations, which opens up the possibility of working with a
data augmentation Gibbs sampler as another alternative for posterior inference. The
new procedure demonstrates exemplary performance even when mild subjective prior
information is passed on to the model through a reasonable distribution on ρ2.
181

Chapter 6: Discussion and Future Work
This dissertation has focused on the Bayesian linear model. In it, we have exam-
ined currently popular prior distributions and found them lacking from a theoretical
perspective. This has led to the development of new classes of prior distributions that
have preferable theoretical properties. These new priors have been shown to have bet-
ter empirical performance as well. In addition, a class of science driven priors with
appealing performance has been developed.
In Chapter 3 we introduced the block g prior as a remedy to the undesirable
behaviors displayed by many standard priors, including the popular hyper-g prior
of Liang et al. (2008). A major factor behind the success of the ordinary hyper-
g prior is the availability of closed form marginal likelihoods for models and the
resultant computational ease in exploring the posterior over the model space. The
computational advantages of the standard mixtures of g priors are lost with the
block g priors and we have to resort to Gibbs sampling, Monte Carlo integration or
numerical integration of the partially-marginalized posterior distributions to make
posterior inference. The number of blocks is highly inﬂuential in determining the
computational complexity of the problem and the computing burden increases rapidly
with the number of blocks. A blockwise orthogonal design simpliﬁes the block hyper-
g posterior and allows faster computations, but the procedure still remains more
complex than the hyper-g.
182

A practical question is how to best select the groups or blocks of predictors in
the design. Our analysis for the block g priors so far have been based on identifying
predictor variables related to one another through a latent or theoretical construct.
Predictors measuring the same construct are placed in the same block, as they are
more likely to have comparable coeﬃcient sizes once the predictors are normalized.
An identical scale parameter for these related explanatory variables would thus appear
reasonable. In some situations, grouping the predictor variables based on the absolute
values of the t statistics in the least squares regression table (corresponding to t tests)
generates a data-dependent prior that performs satisfactorily in inference, as is evident
in the data analysis example in Section 4.6. In the absence of any such knowledge,
our preliminary empirical research suggests that placing correlated predictors in the
same block leads to a better performance. There is scope for theoretical investigation
as to why this choice works or if some other choice can be established as “optimal”
under speciﬁc settings.
The conditional information asymptotic deﬁned in Chapter 3 through the se-
quences {ΨN} in (3.5) and (3.10) is not the only form of such an asymptotic for
which the relevant results in Chapter 3 hold. These sequences allow only one group
of coeﬃcients to grow in size, lending insight into a situation commonly found in
practical data analysis where one set of coeﬃcients is huge compared to the rest.
The asymptotic can be easily modiﬁed to ﬁt the situation where diﬀerent groups of
coeﬃcients grow in size at diﬀerent rates, similar to the situation in (4.19), and the
theoretical results for traditional priors and for the block hyper-g and block hyper-g/n
priors regarding the CLP and ELS remain valid. This modiﬁed form of the conditional
information asymptotic applies to practical problems where more than one group of
183

predictors are believed to have big coeﬃcients, but the sizes of the big coeﬃcients are
not comparable across groups.
Throughout Chapter 3 we have assumed that the design matrix Xγ under model
Mγ is of full column rank, similar to the setup in Liang et al. (2008). Maruyama
and George (2011) extended the deﬁnition for the ordinary g prior to the p > n −1
setting calling it the generalized g prior. It would be interesting to see whether their
approach can be suitably modiﬁed to formulate a generalized block g prior. In a
similar vein, while this dissertation focuses on block hyper-g and block hyper-g/n
priors, it is clear that the beneﬁts of these priors extend to other mixtures of g priors.
The BOB g prior introduced in Chapter 4 is a variation of the basic block g
prior, speciﬁed on a reparameterized design with orthogonal blocks. We recommend
using the BOPB hyper-g/n prior deﬁned over all possible block permutations to
avoid problems in modeling like the CLP and CELS, that arise as a consequence of
orthogonalizing design blocks in an unfortunate order. But prior speciﬁcation again
depends on the choice of a block structure, and the same issue regarding the selection
of “optimal” blocks of predictor variables poses a valuable question for future research.
Chapter 5 demonstrates how a science driven prior elicited on the strength of
linear regression generates a sound Bayesian modeling procedure. A distribution on
ρ2 indirectly passes on information about the regression coeﬃcients to the model and
the subjective prior concentrates probabilities on important regions of the parameter
space in a coherent manner. The R-prior described in Chapter 5 is one version of a
science driven prior which models ρ2; it is possible to come up with other versions
of science driven priors that focus on diﬀerent summaries of a regression model. It
is worth exploring how other science driven priors perform in estimation and model
184

selection when the prior distributions are judiciously chosen to reﬂect a strong degree
of awareness about the relevant parameters.
Empirical studies reveal the promising performance of the R-prior in ﬁnite sam-
ples, but a concrete theoretical justiﬁcation of the asymptotic properties of the prior
is still lacking. We believe that an inspection of the posterior concentration rate will
shed further light on the eﬀectiveness of the R-posterior. In an earlier version of
their paper, Bhattacharya et al. (2014) discuss how many normal global-local scale
mixture priors, including the Bayesian lasso prior, have suboptimal posterior contrac-
tion rates. A knowledge of the posterior contraction rate for the R-prior should be
able to give a clear indication of the relative strength or weakness of the R-prior in
comparison to other existing priors.
185

Appendix A: Appendix for Chapter 3
A.1
Proof of Theorem 3.2.1
The posterior mean of the regression coeﬃcients is
bβ = E

g
1 + g | y

bβLS
where bβ denotes the posterior mean of the regression coeﬃcient β and bβLS denotes
the estimate of β under least squares.
For the hyper-g prior, the posterior expectation of the shrinkage factor can be
expressed in terms of R2 (see Liang et al. (2008))
E

g
1 + g | y

=
2
p + a
2F1(n−1
2 , 2; p+a
2 + 1; R2)
2F1(n−1
2 , 1; p+a
2 ; R2)
,
where 2F1 is the Gaussian Hypergeometric Function. 2F1(a, b; c; z) is ﬁnite for |z| <
1 whenever c > b > 0. Here, c −b = (p + a)/2 −1 > 0 since 2 < a ≤4 and p > 0.
Thus, for all values of R2 < 1, both numerator and denominator are ﬁnite. We use
an integral representation of the 2F1 function.
E

g
1 + g | y

=
2
p + a
2F1(n−1
2 , 2; p+a
2 + 1; R2)
2F1(n−1
2 , 1; p+a
2 ; R2)
=
R 1
0 t(1 −t)
p+a
2 −2(1 −tR2)−n−1
2 dt
R 1
0 (1 −t)
p+a
2 −2(1 −tR2)−n−1
2 dt
.
186

Deﬁne m = n−1
2 , b = p+a
2 −2 and z = R2(≤1) so that we have
E

g
1 + g | y

=
R 1
0 t(1 −t)b(1 −tz)−mdt
R 1
0 (1 −t)b(1 −tz)−mdt
.
In our problem we have m > b > −1
2 since a > 2 and p ≥1 which satisﬁes the
requirement of b > −1 required later in the proof.
Numerator
=
Z 1
0
t(1 −t)b
" ∞
X
k=0
m + k −1
k

(tz)k
#
dt
=
Z 1
0
∞
X
k=0
m + k −1
k

zktk+1(1 −t)bdt
=
∞
X
k=0
m + k −1
k

zk
Z 1
0
tk+1(1 −t)bdt
(the inﬁnite sum converges for all |z| < 1)
=
∞
X
k=0
(k + 1)Γ(m + k)Γ(b + 1)
Γ(m)Γ(b + k + 3)
zk.
(need b > −1)
Similarly we can show by interchanging the positions of the inﬁnite sum and the
integral that
Denominator =
Z 1
0
(1 −t)b
" ∞
X
k=0
m + k −1
k

(tz)k
#
dt
=
∞
X
k=0
m + k −1
k

zk
Z 1
0
tk(1 −t)bdt =
∞
X
k=0
Γ(m + k)Γ(b + 1)
Γ(m)Γ(b + k + 2)zk.
Thus,
E

g
1 + g | y

=
P∞
k=0
(k+1)Γ(m+k)Γ(b+1)
Γ(m)Γ(b+k+3)
zk
P∞
k=0
Γ(m+k)Γ(b+1)
Γ(m)Γ(b+k+2)zk
=
P∞
k=0
Γ(m+k)
Γ(b+k+2)
1
1+ b+1
k+1 zk
P∞
k=0
Γ(m+k)
Γ(b+k+2)zk
.
When m = n−1
2
> b + 2 = p+a
2
, we show that
Γ(m+k)
Γ(b+k+2) is increasing in k. Consider
the function D(k) = log Γ(m + k) −log Γ(b + k + 2) which has the derivative, where
187

Ψ(·) is the digamma function
D′(k)
=
Ψ(m + k) −Ψ(b + k + 2)
=
Z ∞
0
e−t
t −e−(m+k)t
1 −e−t

dt −
Z ∞
0
e−t
t −e−(b+k+2)t
1 −e−t

dt
=
Z ∞
0
e−(b+k+2)t −e−(m+k)t
1 −e−t

dt > 0
∀k ∈N, whenever m > b + 2.
This implies that D(k) is increasing in k and so is exp(D(k)). The algebra above
makes use of the following standard integral representation of the digamma function
Ψ(x) =
Z ∞
0
e−t
t −
e−tx
1 −e−t

dt , when x > 0.
Lemma 3.2.1 states that R2 →1 as N →∞so that lim
N→∞
bβ = lim
R2→1 E

g
1+g | y

bβLS.
The proof will be complete if we can show that lim
R2→1 E

g
1+g | y

= 1. Throughout,
we make use of the expression
lim
R2→1 E

g
1 + g | y

=
lim
z↑1
P∞
k=0
Γ(m+k)
Γ(b+k+2)
1
1+ b+1
k+1 zk
P∞
k=0
Γ(m+k)
Γ(b+k+2)zk
.
Case 1: n > p + a + 1
First note that
1
1+ b+1
k+1 is increasing in k and ↑1 as k →∞. So for any η > 0, ∃N0
such that ∀k > N0,
1
1+ b+1
k+1 > 1 −η.
Hence, lim
z↑1
P∞
k=0
Γ(m+k)
Γ(b+k+2)
1
1+ b+1
k+1
zk
P∞
k=0
Γ(m+k)
Γ(b+k+2)zk
=
lim
z↑1
PN0
k=0
Γ(m+k)
Γ(b+k+2)
1
1+ b+1
k+1 zk + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)
1
1+ b+1
k+1 zk
PN0
k=0
Γ(m+k)
Γ(b+k+2)zk + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
>
lim
z↑1
q1 + (1 −η) P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
q2 + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
188

=
(1 −η) + lim
z↑1
q1 −(1 −η)q2
q2 + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
≥
(1 −η) + 0 = (1 −η)
=⇒
lim
R2→1 E

g
1 + g | y

= lim
z↑1
P∞
k=0
Γ(m+k)
Γ(b+k+2)
1
1+ b+1
k+1 zk
P∞
k=0
Γ(m+k)
Γ(b+k+2)zk
= 1.
Note that q1 and q2 are ﬁnite numbers corresponding to the ﬁnite sums of the ﬁrst
N0 terms. Also
Γ(m+k)
Γ(b+k+2) →∞as k →∞due to which P∞
k=0
Γ(m+k)
Γ(b+k+2) and hence the
denominator goes to inﬁnity causing the second term above to vanish in the limit.
Case 2: p + a −1 ≤n ≤p + a + 1
Let n = p + a −1 + 2ξ, where 0 ≤ξ ≤1 and deﬁne N0 as in Case 1,
E

g
1 + g | y

=
P∞
k=0
Γ(b+k+1+ξ)
Γ(b+k+2)
1
1+ b+1
k+1 zk
P∞
k=0
Γ(b+k+1+ξ)
Γ(b+k+2) zk
lim
R2→1 E

g
1 + g | y

=
lim
z↑1
P∞
k=0
Γ(b+k+1+ξ)
Γ(b+k+1)(b+k+1)
1
1+ b+1
k+1 zk
P∞
k=0
Γ(b+k+1+ξ)
Γ(b+k+1)(b+k+1)zk
.
Proceeding as in Case 1, we can show that
lim
R2→1 E

g
1 + g | y

>
(1 −η) + lim
z↑1
q1 −(1 −η)q2
q2 + P∞
k=N0+1
Γ(b+k+1+ξ)
Γ(b+k+1)(b+k+1)zk
≥
(1 −η) , for any η > 0.
As z ↑1, the denominator of the second term becomes
q1−(1−η)q2
q2+P∞
k=N0+1
Γ(b+k+1+ξ)
Γ(b+k+1)(b+k+1) which
tends to zero if the inﬁnite sum P∞
k=N0+1
Γ(b+k+1+ξ)
Γ(b+k+1)(b+k+1) diverges.
It does, since
Γ(b+k+1+ξ)
Γ(b+k+1)(b+k+1) = O(k−λ), where 0 ≤λ ≤1 and P∞
k=N0+1 O(k−λ) = ∞for any
0 ≤λ ≤1. Thus,
189

lim
R2→1 E

g
1 + g | y

= 1.
Case 3: n < p + a −1 (proving necessity of the constraint)
lim
R2→1 E

g
1 + g | y

=
lim
R2→1
R 1
0 t(1 −t)
p+a
2 −2(1 −tR2)−n−1
2 dt
R 1
0 (1 −t)
p+a
2 −2(1 −tR2)−n−1
2 dt
=
R 1
0 t(1 −t)
p+a
2 −2−n−1
2 dt
R 1
0 (1 −t)
p+a
2 −2−n−1
2 dt
=
Beta(2, p+a−n−1
2
)
Beta(1, p+a−n−1
2
) =
2
p + a −n + 1
which is strictly less than 1 ∀n < p + a −1 and has a minimum value of
2
p+a when
n = 1.
A.2
Proof of Theorem 3.2.2
Liang et al. (2008) show that
BF(M2 : M0)
=
2F1(n −1
2
, 1; a + p
2
; R2
M2) ×
a −2
a + p −2
BF(M1 : M0)
=
2F1(n −1
2
, 1; a + p1
2
; R2
M1) ×
a −2
a + p1 −2
where R2
Mi is the coeﬃcient of determination for model Mi, i = 1, 2.
The g prior is invariant to linear transformation of X, and so we can work with
an orthogonalized version of the design without loss of generality. Speciﬁcally, we
consider Q1 = X1 and Q2 = (I −PQ1)X2, where PQ1 is the projection matrix for
the column space of Q1.
Then X can be represented as X = QT for a suitable
upper triangular matrix T and Xβ = Qκ, where κ = Tβ also has a hyper-g prior.
Since T is upper triangular, ||β1|| →∞is equivalent to ||κ1|| →∞while κ2 stays
190

ﬁxed in the sequence. Under the block orthogonal setup, R2
M1 = (Q1 bκ1)T (Q1 bκ1)
yTy
and
R2
M2 = R2
M1 + (Q2 bκ2)T (Q2 bκ2)
yTy
. The term (Q2bκ2)T(Q2bκ2) is constant throughout the
sequence {ΨN} and so (Q2 bκ2)T (Q2 bκ2)
yTy
→0 as N →∞.
BF(M2 : M1)
=
BF(M2 : M0)
BF(M1 : M0)
=
a + p1 −2
a + p −2 .
2F1(n−1
2 , 1; a+p
2 ; R2
M2)
2F1(n−1
2 , 1; a+p1
2 ; R2
M1)
=
R 1
0 (1 −t)
a+p
2 −2(1 −tR2
M2)−n−1
2 dt
R 1
0 (1 −t)
a+p1
2
−2(1 −tR2
M1)−n−1
2 dt
.
Deﬁne b = a+p1
2
−2, m = n−1
2 , R2
M1 = z and R2
M2 = z + q. When ||β1|| →∞, both
R2
M2 and R2
M1 go to 1 which results in z ↑1 and q ↓0. Hence,
BF(M2 : M1) =
R 1
0 (1 −t)b+ p2
2 [1 −t(z + q)]−m dt
R 1
0 (1 −t)b [1 −tz]−m dt
.
Proceeding as in Theorem 3.2.1,
Numerator =
∞
X
k=0
Γ(m + k)Γ(b + 1 + p2
2 )
Γ(m)Γ(b + k + 2 + p2
2 )(z + q)k
and
Denominator =
∞
X
k=0
Γ(m + k)Γ(b + 1)
Γ(m)Γ(b + k + 2)zk.
Thus,
BF(M2 : M1) = Γ(b + 1 + p2
2 )
Γ(b + 1)
P∞
k=0
Γ(m+k)
Γ(b+k+2)
n
Γ(b+k+2)
Γ(b+k+2+ p2
2 )
o
(z + q)k
P∞
k=0
Γ(m+k)
Γ(b+k+2)zk
.
Hence
lim
||β1||→∞BF(M2 : M1)
=
lim
z→1
q→0
BF(M2 : M1)
=
lim
z→1

lim
q→0 BF(M2 : M1)

191

The last step is justiﬁed when lim
q→0 BF(M2 : M1) exists for all 0 ≤z < 1. This holds
since lim
q→0 BF(M2 : M1) =
a+p1−2
a+p−2
2F1(m,1;b+2+ p2
2 ;z)
2F1(m,1;b+2;z)
which exists and is ﬁnite for all
0 ≤z < 1.
lim
||β1||→∞BF(M2 : M1)
=
lim
z↑1
Γ(b + 1 + p2
2 )
Γ(b + 1)
P∞
k=0
Γ(m+k)
Γ(b+k+2)
n
Γ(b+k+2)
Γ(b+k+2+ p2
2 )
o
zk
P∞
k=0
Γ(m+k)
Γ(b+k+2)zk
.
But
Γ(b+k+2)
Γ(b+k+2+ p2
2 ) decreases to 0 as k →∞(see Appendix A.1 for a proof). Hence given
an arbitrary η > 0, we can ﬁnd a number N0 such that ∀k > N0,
Γ(b+k+2)
Γ(b+k+2+ p2
2 ) < η.
∴
BF(M2 : M1) = Γ(b + 1 + p2
2 )
Γ(b + 1)
×
PN0
k=0
Γ(m+k)
Γ(b+k+2)
n
Γ(b+k+2)
Γ(b+k+2+ p2
2 )
o
zk + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)
n
Γ(b+k+2)
Γ(b+k+2+ p2
2 )
o
zk
PN0
k=0
Γ(m+k)
Γ(b+k+2)zk + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
<
q1 + η P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
q2 + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk × Γ(b + 1 + p2
2 )
Γ(b + 1)
=
Γ(b + 1 + p2
2 )
Γ(b + 1)
.q1 + ηT
q2 + T , with T =
∞
X
k=N0+1
Γ(m + k)
Γ(b + k + 2)zk
=
Γ(b + 1 + p2
2 )
Γ(b + 1)
η(q1 + T)
q2 + T
+ (1 −η)q1
q2 + T

.
We later show that ||β1|| →∞(or z ↑1) implies that T →∞when n ≥a + p1 −1.
=⇒
lim
||β1||→∞BF(M2 : M1)
≤
lim
T→∞
Γ(b + 1 + p2
2 )
Γ(b + 1)
η(q1 + T)
q2 + T
+ (1 −η)q1
q2 + T

=
η Γ(b + 1 + p2
2 )
Γ(b + 1)
.
Hence
lim
||β1||→∞BF(M2 : M1)
≤
η Γ(b + 1 + p2
2 )
Γ(b + 1)
, for any arbitrary choice of η > 0,
and
lim
||β1||→∞BF(M2 : M1)
=
0.
We now prove that, for n ≥a + p1 −1, T →∞when ||β1|| →∞.
192

Case 1: n > a + p1 + 1
Then m > b + 2 and so
Γ(m+k)
Γ(b+k+2) ↑∞as k →∞.
lim
z↑1 T =
∞
X
k=N0+1
Γ(m + k)
Γ(b + k + 2).
So T →∞for such values of n.
Case 2: a + p1 −1 ≤n ≤a + p1 + 1
Let n = a + p1 −1 + 2ξ where 0 ≤ξ ≤1. Then m = a+p1
2
−1 + ξ and
Γ(m+k)
Γ(b+k+2) =
Γ( a+p1
2
−1+ξ+k)
Γ( a+p1
2
+k)
. For 0 ≤ξ ≤1,
Γ( a+p1
2
−1+ξ+k)
Γ( a+p1
2
+k)
=
Γ( a+p1
2
−1+k+ξ)
Γ( a+p1
2
−1+k+1) = O(k−λ) for some
0 ≤λ ≤1. But P∞
k=N0+1 O(k−λ) = ∞for such values of λ implying that T →∞as
z ↑1.
Case 3: n < a + p1 −1 (proving necessity of the constraint)
lim
||β1→∞|| BF(M2 : M1)
=
lim
R2
M1→1
R2
M2→1
R 1
0 (1 −t)
a+p
2 −2(1 −tR2
M2)−n−1
2 dt
R 1
0 (1 −t)
a+p1
2
−2(1 −tR2
M1)−n−1
2 dt
=
R 1
0 (1 −t)
a+p
2 −2−n−1
2 dt
R 1
0 (1 −t)
a+p1
2
−2−n−1
2 dt
=
a + p1 −n −1
a + p −n −1 < 1.
When n < a + p1 −1, the Bayes Factor BF(M2 : M1) is strictly less than 1 and still
favors the smaller model M1 in the limit, but not with overwhelming evidence as in
the other two cases.
A.3
Proof of Corollary 3.2.1
It can be veriﬁed that
π(σ2, g | y)
∝
(1 + g)−a+p
2
1
σn+1 exp

−1
2σ2yT(I −
g
1 + gPX)y

.
So π(σ2 | y)
∝
Z ∞
0
(1 + g)−a+p
2
1
σn+1 exp

−||y||2
2σ2 (1 −
g
1 + gR2)

dg
193

∝
1
σn+1
Z 1
0
(1 −t)
a+p
2 −2 exp

−||y||2
2σ2 {(1 −R2) + R2(1 −t)}

dt
∝
1
σn+1 exp

−||y||2(1 −R2)
2σ2
 Z 1
0
(1 −t)
a+p
2 −2 exp

−||y||2R2
2σ2
(1 −t)

dt.
Now ||y||2(1 −R2) = (n −p −1)bσ2, which is ﬁxed for all N, so that
π(σ2 | y)
∝
1
σn+1 exp

−(n −p −1)bσ2
2σ2
 Z
||y||2R2
2σ2
0
x
a+p
2 −2e−xdx × (σ2)
a+p
2 −1
∝
1
(σ2)(n+1)/2−(a+p)/2+1 exp

−(n −p −1)bσ2
2σ2
 Z
||y||2R2
2σ2
0
x
a+p
2 −2e−xdx.
As N →∞, ||y|| →∞and R2 →1 so that ||y||2R2
2σ2
→∞. (Lemma 3.2.1)
=⇒
lim
N→∞π(σ2 | y) ∝
1
(σ2)(n+1)/2−(a+p)/2+1 exp

−(n −p −1)bσ2
2σ2
 Z ∞
0
x
a+p
2 −2e−xdx
∝
1
(σ2)(n+1)/2−(a+p)/2+1 exp

−(n −p −1)bσ2
2σ2

.
Thus, the distribution of lim
N→∞π(σ2 | y) is inverse-gamma with shape parameter=
n+1−a−p
2
and scale parameter =
2
(n−p−1)bσ2. The mean of this distribution is (n−p−1)bσ2
(n−p−a−1).
A.4
Finding the Limit of (3.7) in Theorem 3.2.4
Let P = I1
I2
=
R 1
0 t(p−1)/2 (B + t)−(n−1)/2 dt
R 1
0 t(p1−1)/2 (B∗+ t)−(n−1)/2 dt
.
For any ﬁxed B∗> 0 (z < 1), I2 is ﬁnite and so fB∗(t) =
1
CB∗t(p1−1)/2(B∗+ t)−(n−1)/2
is a proper density function with CB∗= I2 being the normalizing constant.
P
=
1
CB∗
Z 1
0
t(p−1)/2
p1 + 1
p + 1 B∗+ t
−(n−1)/2
dt
=
EfB∗

t(p−p1)/2
"
B∗+ t
p1+1
p+1 B∗+ t
#(n−1)/2

=
PfB∗(t < ϵ)EfB∗

t(p−p1)/2
"
B∗+ t
p1+1
p+1 B∗+ t
#(n−1)/2
| t < ϵ


194

+PfB∗(t ≥ϵ)EfB∗

t(p−p1)/2
"
B∗+ t
p1+1
p+1 B∗+ t
#(n−1)/2
| t ≥ϵ

(for a small ϵ > 0)
<
PfB∗(t < ϵ)
"
ϵ(p−p1)/2
 p + 1
p1 + 1
(n−1)/2#
+ PfB∗(t ≥ϵ)


 
B∗+ ϵ
p1+1
p+1 B∗+ ϵ
!(n−1)/2
.
The last inequality follows from the fact that t(p−p1)/2 is increasing in t while
B∗+t
p1+1
p+1 B∗+t
is decreasing in t on the interval (0, 1). It can also be shown that for any integer k so
that k >
p
p1,
B∗+ϵ
p1+1
p+1 B∗+ϵ < k and hence,
P < PfB∗(t < ϵ)
"
ϵ(p−p1)/2
 p + 1
p1 + 1
(n−1)/2#
+ PfB∗(t ≥ϵ)k(n−1)/2.
We will show that PfB∗(t < ϵ) →1 as B∗→0 (N →∞) if n > p1 + 2. For any
arbitrary 0 < ϵ < 1,
lim
N→∞P ≤ϵ(p−p1)/2
 p + 1
p1 + 1
(n−1)/2
+ 0 = ϵ(p−p1)/2
 p + 1
p1 + 1
(n−1)/2
.
It is possible to choose ϵ arbitrarily small, making the upper bound for the limit of
P arbitrarily small, and so P →0 as claimed in (3.7).
To show that lim
N→∞PfB∗(t < ϵ) = 1 for any 0 < ϵ < 1, consider
PfB∗(t < ϵ)
PfB∗(t ≥ϵ)
=
1
CB∗
R ϵ
0 fB∗(t)dt
1
CB∗
R 1
ϵ fB∗(t)dt
≥
R ϵj
ϵj+1 t(p1−1)/2(B∗+ t)−(n−1)/2dt
R 1
ϵ t(p1−1)/2(B∗+ t)−(n−1)/2dt
, for some j ≥2
≥
(ϵj −ϵj+1)
inf
ϵj+1<t<ϵj t(p1−1)/2(B∗+ t)−(n−1)/2
(1 −ϵ) sup
ϵ<t<1 t(p1−1)/2(B∗+ t)−(n−1)/2
.
For B∗< ϵj+1(n−p1)
p1−1
, the above term equals ϵj (ϵj)
p1−1
2
(B∗+ϵj)−(n−1)/2
ϵ
p1−1
2
(B∗+ϵ)−(n−1)/2
, since the function
t(p1−1)/2(B∗+ t)−(n−1)/2 is decreasing on (ϵj+1, 1).
195

So,
lim
N→∞
PfB∗(t < ϵ)
1 −PfB∗(t < ϵ)
≥
ϵj ϵ(j−1) p1−1
2 ϵ−(j−1)(n−1)/2
(since B∗↓0)
=
ϵ
1
2 [2j+(j−1)(p1−n)].
This relation holds for any j ≥2 and hence also for the limit as j →∞. Since
0 < ϵ < 1, lim
j→∞ϵ
1
2 [2j+(j−1)(p1−n)] = ϵ(n−p1)/2 lim
j→∞ϵ
j
2 [2+p1−n] = ∞when n > p1 + 2. This
shows that lim
N→∞PfB∗(t < ϵ) = 1, for any 0 < ϵ < 1 if n > p1 + 2, completing the
argument for the convergence of (3.7) to zero.
A.5
Proof of Invariance of Block g Priors to Reparameteri-
zations by Blockwise Aﬃne Transformations
Suppose we have two separate regression problems:
Problem 1:
y | α, β, σ2
∼
N(α1 + X1β1 + . . . + Xkβk, σ2I)
β | g, σ2
∼
N(0, Aσ2)
π(α, σ2)
∝
1
σ2
with A =





g1(XT
1 X1)−1
0
· · ·
0
0
g2(XT
2 X2)−1
· · ·
0
...
...
...
...
0
0
· · ·
gk(XT
k Xk)−1




,
and Problem 2:
y | α, γ, σ2
∼
N(α1 + Z1γ1 + . . . + Zkγk, σ2I)
γ | g, σ2
∼
N(0, Bσ2)
π(α, σ2)
∝
1
σ2
196

with B =





g1(ZT
1 Z1)−1
0
· · ·
0
0
g2(ZT
2 Z2)−1
· · ·
0
...
...
...
...
0
0
· · ·
gk(ZT
k Zk)−1




.
We wish to show that if Problem 2 is a reparameterization of Problem 1 by a linear
map acting within blocks, then inference is the same for both problems.
If the predictors in Problem 2 are a within-block linear transformation of the
predictors in Problem 1, then there exist non-singular matrices Pi such that Zi =
XiPi for each i = 1, 2, ..., k.
If we deﬁne P =





P1
0
· · ·
0
0
P2
· · ·
0
...
...
...
...
0
0
· · ·
Pk




, it is clear
that Z = (Z1, Z2, ..., Zk) and X = (X1, X2, ..., Xk) are related by Z = XP.
So
y = α1 + Zγ + ϵ can be rewritten as y = α1 + XP γ + ϵ. For each i,
(ZT
i Zi)−1 = [(XiPi)T(XiPi)]−1 = [P T
i XT
i XiPi]−1 = P −1
i
(XT
i Xi)−1(P T
i )−1.
It follows now that B = P −1A(P T)−1 so that
γ | g, σ2
∼
N(0, Bσ2) = N(0, P −1A(P T)−1σ2)
=⇒P γ | g, σ2
∼
N(0, P

P −1A(P T)−1σ2
P T) = N(0, Aσ2)
d=
β | g, σ2.
The equivalence of these two priors signiﬁes that inferences from Problem 1 and from
Problem 2 will be exactly the same. Hence the block g prior is invariant under a
blockwise aﬃne transformation of the problem in any general design.
A.6
Proof of Lemma 3.4.2
The proof relies on a result in, for example, Lehmann and Romano (2005) on
stochastic ordering of random variables.
197

UR: A Useful Result ( Stochastic ordering of densities/random vari-
ables )
Let pθ(x) be a family of densities on the real line with monotone likelihood ratio
in x. Then
1. For any θ < θ′, the cumulative distribution functions of X under θ and θ′ satisfy
Fθ′(x) ≤Fθ(x) for all x.
2. If ψ is a non-decreasing function of x, then Eθ(ψ(X)) is a non-decreasing func-
tion of θ.
If C1 and C2 denote the normalizing constants for the two densities then
f1(tm)
=
1
C1
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2
#
(1 −
k
X
i=1
tiR2
i )−n−1
2 dt−m
f2(tm)
=
1
C2
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2
#
(1 −tjR2
j)−n−1
2 dt−m
So, f1(tm)
f2(tm)
=
C2
C1
R Qk
i=1(1 −ti)
a+pi
2
−2(1 −Pk
i=1 tiR2
i )−n−1
2 dt−m
R Qk
i=1(1 −ti)
a+pi
2
−2(1 −tjR2
j)−n−1
2 dt−m
.
Case 1: m ̸= j
f1(tm)
f2(tm) = C2
C1
(1 −tm)
a+pm
2
−2
(1 −tm)
a+pm
2
−2
R Q
i̸=m(1 −ti)
a+pi
2
−2(1 −Pk
i=1 tiR2
i )−n−1
2 dt−m
R Q
i̸=m(1 −ti)
a+pi
2
−2(1 −tjR2
j)−n−1
2 dt−m
= C2
C1
R Q
i̸=m(1 −ti)
a+pi
2
−2(1 −Pk
i̸=m tiR2
i −tmR2
m)−n−1
2 dt−m
R Q
i̸=m(1 −ti)
a+pi
2
−2(1 −tjR2
j)−n−1
2 dt−m
.
Note that (1 −Pk
i=1 tiR2
i )−(n−1)/2 is a non-decreasing function of tm, and so f1(tm)
f2(tm)
is a non-decreasing function of tm.
Applying the Useful Result (UR) stated earlier, f1 is stochastically larger than f2
and, using part (2) of the UR with the strictly increasing function ψ(tm) = tm, we
198

have Ef1(tm) ≥Ef2(tm). In fact, the ratio f1(tm)
f2(tm) is strictly increasing and we have
strict inequality in the result when R2
m > 0.
Case 2: m = j
f1(tj)
f2(tj) = C2
C1
(1 −tj)
a+pj
2
−2
(1 −tj)
a+pj
2
−2
R Q
i̸=j(1 −ti)
a+pi
2
−2(1 −Pk
i=1 tiR2
i )−n−1
2 dt−j
R Q
i̸=j(1 −ti)
a+pi
2
−2(1 −tjR2
j)−n−1
2 dt−j
= C2
C1
1
(1 −tjR2
j)−n−1
2
R Q
i̸=j(1 −ti)
a+pi
2
−2(1 −Pk
i=1 tiR2
i )−n−1
2 dt−j
R Q
i̸=j(1 −ti)
a+pi
2
−2dt−j
= C2
C1
R Q
i̸=j(1 −ti)
a+pi
2
−2(1 −P
i̸=j ti
R2
i
1−tjR2
j )−n−1
2 dt−j
R Q
i̸=j(1 −ti)
a+pi
2
−2dt−j
.
The function (1 −P
i̸=j ti
R2
i
1−tjR2
j )−n−1
2
is non-decreasing in tj and so
f1(tj)
f2(tj) is a
non-decreasing function of tj. Using UR, we conclude that Ef1(tj) ≥Ef2(tj). Strict
inequality holds in this case when the ratio f1(tm)
f2(tm) is strictly increasing and this happens
when R2
m > 0 and R2
i > 0 for at least one i ̸= m.
A.7
Proof of Theorem 3.4.1
The ﬁrst part of the proof is trivial and follows directly from (3.11).
In the block orthogonal setup
π(g | y) ∝
Qk
j=1(1 + gj)−
a+pj
2
h
1 −Pk
j=1
gj
gj+1R2
j
i(n−1)/2.
So for any i = 1, 2, .., k,
E

gi
1 + gi
| y

=
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
≥
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −tiR2
i )−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −tiR2
i )−n−1
2 dt
(by Lemma 3.4.2)
199

=
2
a + pi
2F1(n−1
2 , 2; a+pi
2
+ 1; R2
i )
2F1(n−1
2 , 1; a+pi
2 ; R2
i )
Hence
lim
N→∞E

gi
1 + gi
| y

≥
lim
N→∞
2
a + pi
2F1(n−1
2 , 2; a+pi
2
+ 1; R2
i )
2F1(n−1
2 , 1; a+pi
2 ; R2
i )
.
As N →∞, R2
1 →1 so that for i = 1,
lim
N→∞E

g1
1 + g1
| y

≥
lim
z→1
2
a + p1
2F1(n−1
2 , 2; a+p1
2
+ 1; z)
2F1(n−1
2 , 1; a+p1
2 ; z)
=
1
when n ≥a + p1 −1 (see Theorem 3.2.1). But E

g1
1+g1 | y

≤1, implying that
E

g1
1+g1 | y

→1 in the limit.
For i > 1 and m ̸= i, E

gi
1+gi | y

=
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
≥
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −tmR2
m)−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −tmR2
m)−n−1
2 dt
(by Lemma 3.4.2)
=
Beta(2, a+pi
2
−1)
Beta(1, a+pi
2
−1) ×
2F1(n−1
2 , 1; a+pm
2
; R2
m)
2F1(n−1
2 , 1; a+pm
2
; R2
m) =
2
a + pi
.
Thus,
lim
N→∞E

gi
1+gi | y

≥
2
a+pi.
Equality in the relation above is attained when
R2
i = 0 or R2
i →0 and
lim
N→∞
P
j̸=i
R2
j < 1, i.e, when no linear combination of the
predictors explains all of the variation in the response.
Again,
E

gi
1 + gi
| y

=
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
≤
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −P
j̸=i R2
j −tiR2
i )−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −P
j̸=i R2
j −tiR2
i )−n−1
2 dt
(using a variation of Lemma 3.4.2)
200

=
R 1
0 ti(1 −ti)
a+pj
2
−2(1 −P
j̸=i R2
j −tiR2
i )−n−1
2 dti
R 1
0 (1 −ti)
a+pj
2
−2(1 −P
j̸=i R2
j −tiR2
i )−n−1
2 dti
=
(1 −P
j̸=i R2
j)−(n−1)/2
(1 −P
j̸=i R2
j)−(n−1)/2
R 1
0 ti(1 −ti)
a+pj
2
−2(1 −ti
R2
i
1−P
j̸=i R2
j )−n−1
2 dti
R 1
0 (1 −ti)
a+pj
2
−2(1 −ti
R2
i
1−P
j̸=i R2
j )−n−1
2 dti
=
2
a + pi
2F1(n−1
2 , 2; a+pi
2
+ 1; κi)
2F1(n−1
2 , 1; a+pi
2 ; κi)
< 1
where κi =
R2
i
1−P
j̸=i R2
j .
For this sequence of problems, 0 < κi < 1 is ﬁxed for all i ̸= 1, because
κi =
R2
i
1 −P
j̸=i R2
j
=
yTPXiy
(n −p −1)ˆσ2 + yTPXiy.
If yTPXiy is quite large compared to ˆσ2, then κi ≈1 and the shrinkage factor is near
1 while very small values of yTPXiy relative to ˆσ2 implies κi ≈0 and the shrinkage
factor is near the lower bound
2
a+pi. For all values of 0 < κi < 1, we have
lim
N→∞E

gi
1 + gi
| y

< 1, for i ̸= 1.
A.8
Proof of Corollary 3.4.1
It is easy to show that
π(σ2, g | y) ∝
k
Y
i=1
(1 + gi)−a+pi
2
1
σn+1 exp
"
−1
2σ2yT(I −
k
X
i=1
gi
1 + gi
PXi)y
#
.
So, π(σ2 | y)
∝
1
σn+1
Z
(0,∞)k
k
Y
i=1
(1 + gi)−a+pi
2
exp
"
−||y||2
2σ2 (1 −
k
X
i=1
gi
1 + gi
R2
i )
#
dg
∝
1
σn+1
Z
(0,1)k
k
Y
i=1
(1 −ti)
a+pi
2
−2 exp
"
−||y||2
2σ2 (1 −
k
X
i=1
tiR2
i )
#
dt
∝
1
σn+1
Z
(0,1)k
k
Y
i=1
(1 −ti)
a+pi
2
−2 exp
"
−||y||2
2σ2 (1 −
k
X
i=1
R2
i +
k
X
i=1
(1 −ti)R2
i )
#
dt
201

∝
1
σn+1 exp
"
−||y||2(1 −Pk
i=1 R2
i )
2σ2
#
×
k
Y
i=1
Z 1
0
(1 −ti)
a+pi
2
−2 exp

−||y||2
2σ2 R2
i (1 −ti)

dti

∝
1
σn+1 exp

−(n −p −1)bσ2
2σ2

k
Y
i=1


Z
||y||2R2
i
2σ2
0
x
a+pi
2
−2
i
e−xidxi


k
Y
i=1
(σ2)
a+pi
2
−1.
Now as N →∞, ||y|| →∞and R2
1 →1 while R2
i →0 ∀i ̸= 1 (by Lemma 3.4.1).
The expression ||y||2R2
i
2σ2
= yT PXiy
2σ2
= (Xi bβi)T (Xi bβi)
2σ2
is constant for i ̸= 1 and goes to ∞
for i = 1. So only ||y||2R2
1
2σ2
→∞, while the other integrals are over a ﬁnite unchanging
domain.
lim
N→∞π(σ2 | y)
∝
1
(σ2)
n+1
2 +k−
ka+Pk
i=1 pi
2
exp

−(n −p −1)bσ2
2σ2

× lim
N→∞
k
Y
i=2


Z
||y||2R2
i
2σ2
0
x
a+pi
2
−2
i
e−xidxi


∝
1
(σ2)
n+1
2 +k−ka+p
2
exp

−(n −p −1)bσ2
2σ2

k
Y
i=2
γ
a + pi
2
−1, (Xibβi)T(Xibβi)
2σ2

where γ(·, ·) is the lower incomplete gamma function.
The normalizing constant in the density π(σ2 | y) exists for any N since
1
(σ2)[n−1−k(a−2)−p]/2 +1 exp

−(n −p −1)bσ2
2σ2

k
Y
i=2


Z
||y||2R2
i
2σ2
0
x
a+pi
2
−2
i
e−xidxi


≤
1
(σ2)[n−1−k(a−2)−p]/2 +1 exp

−(n −p −1)bσ2
2σ2

k
Y
i=2
Z ∞
0
x
a+pi
2
−2
i
e−xidxi

which is integrable as a function of σ2 over (0, ∞) if n > k(a −2) + p + 1. The
integral (over σ2) for the expression above is also clearly bounded away from zero as
long as ||y||2R2
i does not equal zero or converge to zero for any i = 2, . . . , k. But for
202

any such i, ||y||2R2
i = yTPXiy is strictly greater than zero with probability one in
every element of the sequence {ΨN} (and also in the limit). This guarantees that the
normalizing constant in π(σ2 | y) is ﬁnite and non-zero for all N and validates the
existence of a proper limiting distribution for the sequence of posteriors of σ2.
A.9
Proof of Corollary 3.4.2
Proceeding as in Corollary 3.4.1,
π(σ2 | y)
∝
1
σn+1
Z
(0,∞)k
k
Y
i=1
(1 + gi)−a+pi
2
exp
"
−||y||2
2σ2 (1 −
k
X
i=1
gi
1 + gi
R2
i )
#
dg.
So, E(σ2 | y)
=
R
(0,1)k
R ∞
0
1
σn−1 exp
h
−||y||2
2σ2 (1 −Pk
i=1 tiR2
i )
i Qk
i=1(1 −ti)
a+pi
2
−2dσ2dt
R
(0,1)k
R ∞
0
1
σn+1 exp
h
−||y||2
2σ2 (1 −Pk
i=1 tiR2
i )
i Qk
i=1(1 −ti)
a+pi
2
−2dσ2dt
=
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2 R ∞
0
1
(σ2)
n−3
2
+1 exp
h
−||y||2
2σ2 (1 −Pk
i=1 tiR2
i )
i
dσ2dt
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2 R ∞
0
1
(σ2)
n−1
2
+1 exp
h
−||y||2
2σ2 (1 −Pk
i=1 tiR2
i )
i
dσ2dt
=
||y||2
n −3
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −Pk
i=1 tiR2
i )−n−3
2 dt
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −Pk
i=1 tiR2
i )−n−1
2 dt
.
This leads to
E(σ2 | y)
≤
||y||2
n −3
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −t1R2
1)−n−3
2 dt
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −t1R2
1)−n−1
2 dt
(A.1)
=
||y||2
n −3
R 1
0 (1 −t1)
a+p1
2
−2(1 −t1R2
1)−n−3
2 dt1
R 1
0 (1 −t1)
a+p1
2
−2(1 −t1R2
1)−n−1
2 dt1
=
||y||2
n −3
2F1
  n−3
2 , 1; a+p1
2 ; R2
1

2F1
  n−1
2 , 1; a+p1
2 ; R2
1
.
To show that (A.1) holds, deﬁne the pdfs f1 and f2 as
f1(tm)
∝
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2
#
(1 −
k
X
i=1
tiR2
i )−n−1
2 dt−m
and f2(tm)
∝
Z
(0,1)k−1
" k
Y
i=1
(1 −ti)
a+pi
2
−2
#
(1 −t1R2
1)−n−1
2 dt−m
203

and use Lemma 3.4.2 to obtain the result
Ef1(1−
k
X
i=1
tiR2
i ) = 1−
k
X
i=1
R2
i Ef1(ti) ≤1−
k
X
i=1
R2
i Ef2(ti) ≤1−R2
1Ef2(t1) = Ef2(1−t1R2
1)
Note that for the ordinary hyper-g prior, the inequality is replaced by the equality
E(σ2 | y) = ||y||2
n −3
2F1
  n−3
2 , 1; a+p
2 ; R2
2F1
  n−1
2 , 1; a+p
2 ; R2.
We shall use the following identity for Gaussian hypergeometric functions to simplify
the RHS of the inequality (equality under the ordinary hyper-g):
lim
z→1(1 −z)a+b−c
2F1 (a, b; c; z) = Γ(a + b −c)Γ(c)
Γ(a)Γ(b)
when a + b −c > 0.
As N →∞, ||y|| →∞and R2
1 →1. Thus,
lim
N→∞
||y||2
n −3
2F1
  n−3
2 , 1; a+p1
2 ; R2
1

2F1
  n−1
2 , 1; a+p1
2 ; R2
1

=
lim
N→∞
||y||2(1 −R2
1)
n −3
(1 −R2
1)
n−3
2 +1−a+p1
2
2F1
  n−3
2 , 1; a+p1
2 ; R2
1

(1 −R2
1)
n−1
2 +1−a+p1
2
2F1
  n−1
2 , 1; a+p1
2 ; R2
1

=
(n −3)/2
(n −1 −a −p1)/2 × lim
N→∞
||y||2(1 −R2
1)
n −3
, provided n > a + p1 + 1.
Thus,
lim
N→∞E(σ2 | y)
≤
lim
N→∞
||y||2(1 −R2
1)
n −1 −a −p1
=
1
n −1 −a −p1
"
(n −p −1)bσ2 +
k
X
i=2
(Xi bβi)T(Xi bβi)
#
.
A.10
Proof of Theorem 3.5.1
Information consistency under model Mγ is equivalent to BF(Mγ : M0) →∞as
R2γ →1. We ﬁrst establish the suﬃciency of Condition (2). We know R2γ = Pkγ
j=1 R2
j,γ
and Condition (2) of the theorem enforces R2
i,γ →1 for a given block i, meaning all
other R2
j,γ →0, j ̸= i.
204

Then, BF(Mγ : M0)
=
a −2
2
kγ Z
(0,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2
#  
1 −
kγ
X
j=1
tjR2
j,γ
!−n−1
2
dt
≥
a −2
2
kγ
kγ
Y
j=1
Z 1
0
(1 −tj)
a+pj,γ
2
−2  1 −tjR2
j,γ
−n−1
2 dtj

(by (3.12))
=⇒
lim
R2
i,γ→1 BF(Mγ : M0)
≥
lim
R2
i,γ→1
a −2
2
kγ Z 1
0
(1 −ti)
a+pi,γ
2
−2  1 −tiR2
i,γ
−n−1
2 dti
×
Y
j̸=i
Z 1
0
(1 −tj)
a+pj,γ
2
−2dtj

.
The ﬁrst term on the RHS goes to ∞when n ≥a+pi,γ −1 while the rest of the terms
converge to nonzero constants. Hence the Bayes factor also diverges in the limit as
required to show information consistency.
The more general situation is represented through Condition (1) where the block
structure does not play any role in driving consistency. For an arbitrary 0 < η < 1,
BF(Mγ : M0)
=
a −2
2
kγ Z
(0,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2
#  
1 −
kγ
X
j=1
tjR2
j,γ
!−n−1
2
dt
>
a −2
2
kγ Z
(1−η,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2
#  
1 −
kγ
X
j=1
tjR2
j,γ
!−n−1
2
dt
>
a −2
2
kγ Z
(1−η,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2
#
(1 −(1 −η)R2
γ)−n−1
2 dt.
Therefore,
lim
R2γ→1 BF(Mγ : M0)
≥
η−n−1
2
a −2
2
kγ
kγ
Y
j=1
Z 1
1−η
(1 −tj)
a+pj,γ
2
−2dtj

(by Monotone Convergence Theorem)
=
η−n−1
2
a −2
2
kγ
kγ
Y
j=1

2
a + pj,γ −2η
a+pj,γ
2
−1

=
η−n−1
2 −kγ+ akγ
2 + pγ
2
kγ
Y
j=1

a −2
a + pj,γ −2

205

where pγ = Pkγ
j=1 pj,γ and the above inequality holds for any choice of η ∈(0, 1).
When n > kγ(a−2)+pγ +1, the exponent of η is negative and since the inequality is
true for any such η, it holds for the limit η →0 as well, indicating that lim
R2γ→1 BF(Mγ :
M0) = ∞.
A.11
Proof of Theorem 3.5.2
Let R2
i,T and pi,T represent the component of R2 and the number of predictors
in the ith block of the true model MT while R2
i,γ and pi,γ denote the corresponding
entities for model Mγ. Further assume Bγ denotes the set of indices of the blocks
within Mγ, and let kγ = |Bγ|. Recall that BT and kT are the block indices and the
number of blocks respectively in model MT.
We shall use the following two lemmas in the proof of this theorem. The results
from Lemma A.11.1 are slightly generalized versions of Lemmas B.2 and B.3 from
Maruyama and George (2011) and can be proved in a similar way. Conditions 3.5.1
and 3.5.2 are used to prove Lemma A.11.1.
Lemma A.11.1. Let R2
i,γ and R2
i,T denote the ith component of R2 under an arbitrary
model Mγ and the true model MT.
(i) Then for i ∈BT,
R2
i,γ
P→
βT
i,TDi,Tβi,T −Vi,γ
σ2 + P
j∈BT βT
j,TDj,Tβj,T + α2
T
R2
i,T
P→
βT
i,TDi,Tβi,T
σ2 + P
j∈BT βT
j,TDj,Tβj,T + α2
T
where Di,γ = lim
n→∞
1
nXT
i,γXi,γ is positive deﬁnite for all models Mγ (and all blocks)
and
206

Vi,γ = lim
n→∞
1
nβT
i,TXT
i,T(I −PXi,γ)Xi,Tβi,T =
(
0
Xi,γ ⊇Xi,T
> 0
Xi,γ ̸⊃Xi,T .
(ii) When i ̸∈BT, R2
i,γ
P→0.
(iii) For i ∈Bγ\BT, nR2
i,γ
d→cχ2
pi,γ = Op(1). (c is a constant)
(iv) For any model Mγ ⊃MT,

1−P
j∈BT R2
j,T
1−P
j∈Bγ R2
j,γ
n
=

1−R2
T
1−R2γ
n
is bounded from above
in probability.
Lemma A.11.2. Consider the function h(t) deﬁned on (0, 1)k as h(t) = P
i∈I
bi log(1−
ti) −m log(1 −P
i∈I
tiri) where each bi > 0, ri ≥0, m > P
i∈I
bi and P
i∈I
ri < 1. Then the
(unique) maximum of h(t) is attained at the point t = t∗in the interior of the set
(0, 1)k with t∗
i = 1 −bi(1−r)
ri(m−b) for all i ∈I, where b = P
i∈I
bi and r = P
i∈I
ri.
If we denote the Hessian matrix as H(t) = ((Hij(t))) = ((∂2h(t)
∂titj )), then
Hij(t∗)
=
(m −b)2rirj
m(1 −r)2
, i ̸= j
and Hii(t∗)
=
−(m −b)2r2
i
(1 −r)2
 1
bi
−1
m

.
The proof of Lemma A.11.2 is skipped for brevity. It is not diﬃcult to check that
the partial derivatives of h(t) attain a value of zero at t∗. The Hessian matrix is
non-positive deﬁnite at t = t∗.
Returning to the proof of the theorem, ﬁrst consider the case when the true model
diﬀers from the null (intercept only) model, i.e., MT ̸= M0.
The Bayes factor comparing model Mγ to the true model MT can be written as
BF(Mγ : MT)
=
BF(Mγ : M0)
BF(MT : M0)
=
a −2
2
kγ−kT
R Q
i∈Bγ
(1 −ti)
a+pi,γ
2
−2(1 −P
i∈Bγ
tiR2
i,γ)−n−1
2 dt
R Q
i∈BT
(1 −ti)
a+pi,T
2
−2(1 −P
i∈BT
tiR2
i,T)−n−1
2 dt
207

=
a −2
2
kγ−kT R
exp(hγ(t))dt
R
exp(hT(t))dt.
(A.2)
We deﬁne hγ(t) = P
i∈Bγ
(a+pi
2
−2) log(1 −ti) −n−1
2 log(1 −P
i∈Bγ
tiR2
i,γ) and hT(t) is
deﬁned similarly. We wish to use Lemma A.11.2 on these two functions, and that
requires the following conditions to hold:
(A) a+pi,γ
2
> 2 for all i ∈Bγ and a+pi,T
2
> 2 for all i ∈BT.
(B) n > p + 1 + (a −4) × min(kγ, kT).
It is easy to see that (B) is satisﬁed since n ≥p + 2 and 2 < a ≤4. For (A) to
hold, we must have a + pi > 4 for all blocks in both models. If a > 3, this condition
is automatically satisﬁed and we proceed with the proof assuming that a > 3 so that
(A) is true. The proof for 2 < a ≤3 is provided in Appendix A.16.
Using the multivariate generalization of the Laplace approximation, we can approxi-
mate (A.2) upto an O( 1
n) term as BF(Mγ : MT)
≈
a −2
2
kγ−kT |HT(ˆtT)|1/2
|Hγ(ˆtγ)|1/2
exp[ P
i∈Bγ
bi,γ log(1 −ˆti,γ) −m log(1 −P
i∈Bγ
ˆti,γR2
i,γ)]
exp[ P
i∈BT
bi,T log(1 −ˆti,T) −m log(1 −P
i∈BT
ˆti,TR2
i,T)]
where bi,j = a+pi,j
2
−2, m = n−1
2
and Hj(ˆtj) is the Hessian matrix of hj(t) evaluated
at the maximizer ˆtj of hj(t); j ∈{γ, T}.
Using Lemmas A.11.1 and A.11.2, we can show that |HT(ˆtT)| = O(m2kT ) and
also |Hγ(ˆtγ)| = O(m2qγ) for some 0 ≤qγ ≤kT. This follows from the fact that
(Hγ(tγ))ii = O(m2R2
i,γ) and so qγ = kγ −Lγ, where Lγ is the number of components
R2
i,γ going to zero in probability.
208

Then for large m = n−1
2
(or equivalently for large n), BF(Mγ : MT)
≈
O(mkT −qγ) exp
"
−m log
m(1 −R2
γ)
m −bγ

+ m log
m(1 −R2
T)
m −bT

+
X
i∈Bγ
bi,γ log
 bi,γ(1 −R2
γ)
R2
i,γ(m −bγ)

−
X
i∈BT
bi,T log
 
bi,T(1 −R2
T)
R2
i,T(m −bT)
! # a −2
2
kγ−kT
=
O(mkT −qγ) exp
"
m log
1 −R2
T
1 −R2
γ

+ (bT −bγ) log m −
X
i∈Bγ
bi,γ log(R2
i,γ) + O(1)
#
where R2
j = P
i∈Bj R2
i,j and bj = P
i∈Bj bi,j for j ∈{γ, T}.
Case 1: Mγ ̸⊃MT
From Lemma A.11.1 (i) and (ii), R2
γ < R2
T in this case and hence log

1−R2
T
1−R2γ

is negative. Again P
i∈Bγ bi,γ log(R2
i,γ) = P
i∈Jc bi,γ log(R2
i,γ) + P
i∈J bi,γ log(R2
i,γ) =
C + P
i∈J bi,γ log(mR2
i,γ) −P
i∈J bi,γ log m, where J ⊆Bγ is the set of indices such
that R2
i,γ →0 for i ∈J. Then
lim
m→∞BF(Mγ : MT) = lim
m→∞O(ms) . O(f m) = 0
where 0 < f < 1 and s is some real number which might be positive or negative
depending on the block structures of models Mγ and MT. The limit is always zero
regardless of the value of s since the second term goes to zero at an exponential rate
and the ﬁrst term is either bounded in probability (s = 0) or goes to zero (s < 0) or
to inﬁnity (s > 0) at a polynomial rate.
Case 2: Mγ ⊃MT
Case 2A: Mγ has the same block structure as MT, i.e., Bγ = BT, but has more
predictors in at least one of the blocks.
In this case R2
γ ≥R2
T but due to Lemma A.11.1 (iv),

1−R2
T
1−R2γ
m
is bounded in
probability. Also Lemma A.11.1 (i) conﬁrms that none of the R2
i,γ converge to zero
in the limit and so qγ = kγ = kT.
209

Note that bγ = P
i∈Bγ bi,γ = P
i∈Bγ[a+pi,γ
2
−2] =
pγ
2 −(4 −a)kγ
2 and similarly
bT = pT
2 −(4 −a)kT
2 . For the model Mγ, bT −bγ = pT −pγ
2
< 0 and kT = kγ. Hence,
lim
m→∞BF(Mγ : MT) = lim
m→∞O(1) exp

O(1) +
pT −pγ
2

log m

= 0
Case 2B: Mγ has more blocks than MT, i.e., Bγ ⊃BT and in addition has more
predictors in at least one of the blocks common to both models.
As before

1−R2
T
1−R2γ
m
is bounded in probability and all R2
i,γ →0 for i ∈Bγ\BT
which implies that qγ = kγ −(kγ −kT) = kT. Since bT −bγ = P
i∈BT
pi,T
2 −(4−a)kT
2 −
P
i∈Bγ
pi,γ
2 + (4 −a)kγ
2 , we have
m log
1 −R2
T
1 −R2
γ

+ (bT −bγ) log m −
X
i∈Bγ
bi,γ log(R2
i,γ)
=
O(1) +

1
2
X
i∈BT
(pi,T −pi,γ) + 1
2
X
i∈Bγ\BT
(0 −pi,γ) −(4 −a)kT −kγ
2

log m
−
X
i∈Bγ\BT
bi,γ log(mR2
i,γ) +
X
i∈Bγ\BT
bi,γ log m

since mR2
i,γ = O(1) by Lemma A.11.1 (iii)

=
O(1) + log m
"
1
2
X
i∈BT
(pi,T −pi,γ) −1
2
X
i∈Bγ\BT
pi,γ + (a −4)kT −kγ
2
+1
2
X
i∈Bγ\BT
pi,γ + (a −4)kγ −kT
2
#
=
O(1) + log m
"
1
2
X
i∈BT
(pi,T −pi,γ)
#
.
=⇒
lim
m→∞BF(Mγ : MT) = lim
m→∞O(1) exp

O(1) +
P
i∈BT (pi,T −pi,γ)
2
log m

= 0,
since pi,γ ≥pi,T ∀i ∈BT with strict inequality for at least one i ensures that the
above sum is strictly negative.
210

Case 2C: Mγ has more blocks than MT, but has the exact same set of predictors in
all the blocks common to both.
The Bayes factor in this case equals
BF(Mγ : MT)
≈
O(1) exp

O(1) +
P
i∈BT (pi,T −pi,γ)
2
log m

=
O(1) exp[O(1) + 0] = O(1) , for all m
since pi,γ = pi,T ∀i ∈BT. The O(1) term here is a combination of ﬁnite constants and
random variables. As a consequence, the Bayes factor cannot equal 0 with probability
1. This is the only case where the Bayes factor of any arbitrary model Mγ compared
to the true model MT does not go to zero with increasing sample size, violating the
principle of model selection consistency.
The case when MT = M0 is identical to Case 2C with BT = φ, and in this case
too, the Bayes factor BF(Mγ : M0) will be not converge to zero in probability.
Model selection consistency does not hold in this case.
A.12
Proof of Lemma 3.5.1
We follow the same notation as in Theorem 3.5.2. For any i ∈BT,
Z
(0,1)kT
gi
1 + gi
π(g | MT, y)dg
=
R
(0,1)kT ti
QkT
j=1(1 −tj)
a+pj,T
2
−2(1 −PkT
j=1 tjR2
j,T)−n−1
2 dt
R
(0,1)kT
QkT
j=1(1 −tj)
a+pj,T
2
−2(1 −PkT
j=1 tjR2
j,T)−n−1
2 dt
≥
R
(0,1)kT ti
QkT
j=1(1 −tj)
a+pj,T
2
−2(1 −tiR2
i,T)−n−1
2 dt
R
(0,1)kT
QkT
j=1(1 −tj)
a+pj,T
2
−2(1 −tiR2
i,T)−n−1
2 dt
(by Lemma 3.4.2)
=
R 1
0 ti(1 −ti)
a+pi,T
2
−2(1 −tiR2
i,T)−n−1
2 dti
R 1
0 (1 −ti)
a+pi,T
2
−2(1 −tiR2
i,T)−n−1
2 dti
.
211

Given a speciﬁc index i, deﬁne m = n−1
2 , b = a+pi,T
2
−2 and z = R2
i,T, where 0 < z < 1
for all n (since the predictor Xi,T is part of the true model). Then,
E

gi
1 + gi
| MT, y

≥
R 1
0 t(1 −t)b(1 −tz)−mdt
R 1
0 (1 −t)b(1 −tz)−mdt
=
P∞
k=0
Γ(m+k)
Γ(b+k+2)
1
1+(b+1)/(k+1)zk
P∞
k=0
Γ(m+k)
Γ(b+k+2)zk
. (from Theorem 3.2.1)
For an arbitrary η > 0 , ∃N0 (not depending on m) such that ∀k > N0,
1
1+ b+1
k+1 > 1−η.
So, E

gi
1 + gi
| MT, y

>
PN0
k=0
Γ(m+k)(k+1)
Γ(b+k+3) zk + (1 −η) P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
PN0
k=0
Γ(m+k)
Γ(b+k+2)zk + P∞
k=N0+1
Γ(m+k)
Γ(b+k+2)zk
=
q1 + (1 −η)T
q2 + T
= (1 −η) + q1 −(1 −η)q2
q2 + T
.
To prove the lemma we have to show that for any i ∈BT, lim
m→∞E

gi
1+gi | MT, y

=
1, which is equivalent to proving that lim
m→∞
q1−(1−η)q2
q2+T
= 0.
q1 −(1 −η)q2
q2 + T
=
[q1 −(1 −η)q2]/Γ(m + N0 + 1)
q2
Γ(m+N0+1) +
zN0+1
Γ(b+N0+3) + P∞
k=N0+2
Γ(m+k)zk
Γ(b+k+2)Γ(m+N0+1)
.
Now for any 0 < z < 1, P∞
k=N0+2
Γ(m+k)zk
Γ(b+k+2)Γ(m+N0+1) < ∞for any ﬁnite m. With a large
enough m it is possible to make
q1−(1−η)q2
Γ(m+N0+1) arbitrarily small while the denominator is
a ﬁnite number exceeding (ρ−Φ)N0+1
Γ(b+N0+3), where Φ is some small positive number (smaller
than ρ). Here ρ can be deﬁned as ρ = lim
n→∞R2
i,T which must satisfy 0 < ρ < 1 since the
collection of predictors Xi,T is part of the true model (see Lemma A.11.1 in Appendix
A.11).
Thus, we can ﬁnd a large enough number M so that given any arbitrary η > 0
and δ > 0, whenever m ≥M
E

gi
1 + gi
| MT, y

>
1 −η −δ
=⇒lim
n→∞E

gi
1 + gi
| MT, y

=
1, for any i ∈BT.
212

A.13
Proof of Theorem 3.6.3
As in the proof of Theorem 3.5.1, we start by proving the suﬃciency of Condition
(2). As before, R2γ = Pkγ
j=1 R2
j,γ together with Condition (2) implies R2
i,γ →1 for a
given block i and all other R2
j,γ →0, j ̸= i.
Then BF(Mγ : M0) =
a −2
2n
kγ
Z
(0,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2

1 −n −1
n
tj
−a
2 #
×
 
1 −
kγ
X
j=1
tjR2
j,γ
!−n−1
2
dt
≥
a −2
2n
kγ
kγ
Y
j=1
"Z 1
0
(1 −tj)
a+pj,γ
2
−2

1 −n −1
n
tj
−a
2  1 −tjR2
j,γ
−n−1
2 dtj
#
by applying (3.12).
So,
lim
R2
i,γ→1 BF(Mγ : M0)
≥
lim
R2
i,γ→1
a −2
2n
kγ Z 1
0
(1 −ti)
a+pi,γ
2
−2

1 −n −1
n
ti
−a
2  1 −tiR2
i,γ
−n−1
2 dti
×
Y
j̸=i
"Z 1
0
(1 −tj)
a+pj,γ
2
−2

1 −n −1
n
tj
−a
2
dtj
#
≥
lim
R2
i,γ→1
a −2
2n
kγ Z 1
0
(1 −ti)
a+pi,γ
2
−2  1 −tiR2
i,γ
−n−1
2 dti
×
Y
j̸=i
Z 1
0
(1 −tj)
a+pj,γ
2
−2dtj

, since (1 −n −1
n
tj) ≤1 ∀j.
The ﬁrst term on the RHS goes to ∞when n ≥a+pi,γ −1 while the rest of the terms
converge to nonzero constants. Hence the Bayes factor also diverges in the limit as
required to show information consistency.
213

Directing our attention to Condition (1), notice that for any arbitrary 0 < η < 1,
BF(Mγ : M0)
=
a −2
2n
kγ
Z
(0,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2

1 −n −1
n
tj
−a
2 #  
1 −
kγ
X
j=1
tjR2
j,γ
!−n−1
2
dt
>
a −2
2n
kγ Z
(1−η,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2

1 −n −1
n
tj
−a
2 #  
1 −
kγ
X
j=1
tjR2
j,γ
!−n−1
2
dt
>
a −2
2n
kγ Z
(1−η,1)kγ
" kγ
Y
j=1
(1 −tj)
a+pj,γ
2
−2

1 −n −1
n
tj
−a
2 #
 1 −(1 −η)R2
γ
−n−1
2 dt.
Hence, lim
R2γ→1 BF(Mγ : M0)
≥
η−n−1
2
a −2
2n
kγ
kγ
Y
j=1
"Z 1
1−η
(1 −tj)
a+pj,γ
2
−2

1 −n −1
n
tj
−a
2
dtj
#
(by Monotone Convergence Theorem)
≥
η−n−1
2
a −2
2n
kγ
kγ
Y
j=1
Z 1
1−η
(1 −tj)
a+pj,γ
2
−2dtj

,

as (1 −n −1
n
tj) ≤1

=
η−n−1
2
a −2
2n
kγ
kγ
Y
j=1

2
a + pj,γ −2η
a+pj,γ
2
−1

=
η−n−1
2 −kγ+ akγ
2 + pγ
2
kγ
Y
j=1

a −2
n(a + pj,γ −2)

where pγ = Pkγ
j=1 pj,γ and the above inequality holds for any choice of η ∈(0, 1).
When n > kγ(a−2)+pγ +1, the exponent of η is negative and since the inequality is
true for any such η, it holds for the limit η →0 as well indicating that lim
R2γ→1 BF(Mγ :
M0) = ∞.
A.14
Proof of Theorem 3.6.4
We prove that condition (3.13) holds. That is, we show that for the true model
MT and any model Mγ ̸= MT, BF(Mγ : MT)
P→0.
214

First assume that MT ̸= M0, the intercept only model. Then
BF(Mγ : MT)
=
a −2
2n
kγ−kT
×
R Q
i∈Bγ
h
(1 −ti)
a+pi,γ
2
−2(1 −n−1
n ti)−a/2i
(1 −P
i∈Bγ
tiR2
i,γ)−n−1
2 dt
R Q
i∈BT
h
(1 −ti)
a+pi,T
2
−2(1 −n−1
n ti)−a/2
i
(1 −P
i∈BT
tiR2
i,T)−n−1
2 dt
.
(A.3)
A variation of the Laplace approximation can be used to approximate (A.3) upto
an O( 1
n) term which involves evaluation of the integrands at the maximizers of hγ(t) =
Q
i∈Bγ
h
(1 −ti)
a+pi,γ
2
−2i
(1 −P
i∈Bγ
tiR2
i,γ)−n−1
2
and hT(t) =
Q
i∈BT
h
(1 −ti)
a+pi,T
2
−2i
(1 −
P
i∈BT
tiR2
i,T)−n−1
2 . Without loss of generality, we assume that a+pi,γ > 4 and a+pi,T >
4 ∀i; if that is not the case we can modify the proof as in Appendix A.16 and still
obtain the result.
Thus, just as in Theorem 3.5.2, we have for large n
BF(Mγ : MT)
≈
a −2
2n
kγ−kT exp[ P
i∈Bγ
bi,γ log(1 −ˆti,γ) −m log(1 −P
i∈Bγ
ˆti,γR2
i,γ)]
exp[ P
i∈BT
bi,T log(1 −ˆti,T) −m log(1 −P
i∈BT
ˆti,TR2
i,T)]
× |HT(ˆtT)|1/2
|Hγ(ˆtγ)|1/2
Q
i∈Bγ
(1 −n−1
n ˆti,γ)−a/2
Q
i∈BT
(1 −n−1
n ˆti,T)−a/2
where bi,j = a+pi,j
2
−2, m = n−1
2
and Hj(ˆtj) is the Hessian matrix of hj(t) evaluated
at the maximizer ˆtj of hj(t); j ∈{γ, T}.
This means that lim
m→∞BFBHg/n(Mγ : MT)
=
lim
m→∞(2m + 1)kT −kγBFBHg(Mγ : MT)
Q
i∈Bγ
(1 −n−1
n ˆti,γ)−a/2
Q
i∈BT
(1 −n−1
n ˆti,T)−a/2
where the subscripts denote the corresponding Bayes factors for the blockwise hyper-g
and blockwise hyper-g/n priors.
215

Since (1 −n−1
n ˆti,γ) =
1
2m+1 −
2m
2m+1
bi,γ(1−R2
γ)
R2
i,γ(m−bγ), note that both (1 −n−1
n ˆti,T)−a/2 and
(1−n−1
n ˆti,γ)−a/2 are O(ma/2) terms for i ∈BT and whenever R2
i,γ →0, they correspond
to O(1) terms.
So for large m,
BFBHg/n(Mγ : MT)
=
(2m + 1)kT −kγBFBHg(Mγ : MT) O(m
aqγ
2 )
O(m
akT
2 )
=
O(mkT −kγ+ a
2 (qγ−kT )) BFBHg(Mγ : MT)
where qγ is deﬁned as in Theorem 3.5.2.
Case 1: Mγ ̸⊃MT
Following the steps of the proof of Theorem 3.5.2, we know that BFBHg(Mγ :
MT) goes to zero as m →∞(n →∞) at an exponential rate and hence BFBHg/n(Mγ :
MT) →0 for all such models Mγ, regardless of the values of kT, kγ and qγ.
Case 2: Mγ ⊃MT
For Cases 2A and 2B in Theorem 3.5.2, BFBHg(Mγ : MT) →0 at a polynomial
rate. Since qγ = kT in both cases and kγ ≥kT, BFBHg/n(Mγ : MT) must go to zero
in probability as well.
For Case 2C of Theorem 3.5.2, BFBHg(Mγ : MT) = O(1), but as in Case 2A and
2B, qγ = kT along with kγ > kT so that
lim
m→∞BFBHg/n(Mγ : MT)
=
lim
m→∞O(mkT −kγ) BFBHg(Mγ : MT)
=
lim
m→∞O(mkT −kγ)O(1) = 0.
This establishes relation (3.13) for the block hyper-g/n prior implying that this prior
is model selection consistent for MT ̸= M0.
216

Notice that when MT = M0, all models Mγ belong to Case 2C from Theorem
3.5.2 with the minor diﬀerence that BT = φ in this case. Keeping this in mind, the
rest of the proof should be the same as the one above.
Both Conditions 3.5.1 and 3.5.2 are essential in this proof since we borrowed
results from Lemma A.11.1 that depend on these assumptions.
A.15
Proof of Theorem 3.6.5
Since the block hyper-g/n prior leads to consistent model selection, π(MT | y)
P→
1 as n →∞.
Following the proof of Theorem 3.5.3, it is suﬃcient to prove the
condition lim
n→∞E

gi
1+gi | MT, y

= 1 ∀i ∈BT, to claim prediction consistency. As
the least squares estimates for the coeﬃcients in the true model MT converge to the
true regression coeﬃcients in probability and the posterior probability for the true
model tends to 1, prediction consistency is immediate once we prove the preceding
condition.
Given any i ∈BT, E

gi
1+gi | MT, y

=
R
(0,1)kT ti
QkT
j=1
h
(1 −tj)
a+pj,T
2
−2(1 −n−1
n tj)−a/2i
(1 −PkT
j=1 tjR2
j,T)−n−1
2 dt
R
(0,1)kT
QkT
j=1
h
(1 −tj)
a+pj,T
2
−2(1 −n−1
n tj)−a/2
i
(1 −PkT
j=1 tjR2
j,T)−n−1
2 dt
≥
R
(0,1)kT ti
QkT
j=1
h
(1 −tj)
a+pj,T
2
−2(1 −n−1
n tj)−a/2i
(1 −tiR2
i,T)−n−1
2 dt
R
(0,1)kT
QkT
j=1
h
(1 −tj)
a+pj,T
2
−2(1 −n−1
n tj)−a/2
i
(1 −tiR2
i,T)−n−1
2 dt
(by Lemma 3.6.1)
=
R 1
0 ti
h
(1 −ti)
a+pi,T
2
−2(1 −n−1
n ti)−a/2i
(1 −tiR2
i,T)−n−1
2 dt
R 1
0
h
(1 −ti)
a+pi,T
2
−2(1 −n−1
n ti)−a/2
i
(1 −tiR2
i,T)−n−1
2 dt
≥
R 1
0 ti(1 −ti)
a+pi,T
2
−2(1 −tiR2
i,T)−n−1
2 dt
R 1
0 (1 −ti)
a+pi,T
2
−2(1 −tiR2
i,T)−n−1
2 dt
.
( due to (3.15) )
217

Hence, lim
n→∞E

gi
1+gi | MT, y

≥lim
n→∞
R 1
0 ti(1 −ti)
a+pi,T
2
−2(1 −tiR2
i,T)−n−1
2 dt
R 1
0 (1 −ti)
a+pi,T
2
−2(1 −tiR2
i,T)−n−1
2 dt
= 1. (by Lemma 3.5.1)
So lim
n→∞E

gi
1+gi | MT, y

= 1 ∀i ∈BT and consistency for the block hyper-g/n
predictions follows.
A.16
Proof of Theorem 3.5.2 when 2 < a ≤3
We rely on Conditions 3.5.1 and 3.5.2 in this proof through the use of Lemma
A.11.1. When Mγ ̸= M0, as in the proof of Theorem 3.5.2 for 3 < a ≤4,
BF(Mγ : MT)
=
a −2
2
kγ−kT
R Q
i∈Bγ
(1 −ti)
a+pi,γ
2
−2(1 −P
i∈Bγ
tiR2
i,γ)−n−1
2 dt
R Q
i∈BT
(1 −ti)
a+pi,T
2
−2(1 −P
i∈BT
tiR2
i,T)−n−1
2 dt
=
a −2
2
kγ−kT
R Q
i∈Jγ
(1 −ti)−1
2hγ(t)dt
R Q
i∈JT
(1 −ti)−1
2hT(t)dt
where hj(t) = Q
i∈Jj
h
(1 −ti)
a+pi,j+1
2
−2i
Q
i∈Bj\Jj
h
(1 −ti)
a+pi,j
2
−2i
(1 −P
i∈Bj
tiR2
i,j)−n−1
2
=
Q
i∈Bj

(1 −ti)
a+p∗
i,j
2
−2

(1−P
i∈Bj
tiR2
i,j)−n−1
2 , with p∗
i,j = pi,j+1 when i ∈Jj and p∗
i,j = pi,j
otherwise. Here both Jγ ⊆Bγ and JT ⊆BT are the block indices corresponding to
a + pi,j ≤4 which happens only when 2 < a ≤3 and the related pi,j = 1; j ∈{γ, T}.
It is clear that b∗
i,j =
a+p∗
i,j
2
−2 > 0 ∀i, as in the situation described in Appendix
A.11. Using a variation of the Laplace approximation,
lim
m→∞BF(Mγ : MT)
=
a −2
2
kγ−kT
lim
m→∞
|HT(ˆtT)|1/2
|Hγ(ˆtγ)|1/2
Q
i∈Jγ
(1 −ˆti,γ)−1/2
Q
i∈JT
(1 −ˆti,T)−1/2
×
exp[ P
i∈Bγ
b∗
i,γ log(1 −ˆti,γ) −m log(1 −P
i∈Bγ
ˆti,γR2
i,γ)]
exp[ P
i∈BT
b∗
i,T log(1 −ˆti,T) −m log(1 −P
i∈BT
ˆti,TR2
i,T)]
(A.4)
218

with the same deﬁnitions for Hj(tj), ˆti,j and m as in Appendix A.11.
If we can show that (A.4) behaves exactly like the Laplace approximation to (A.2)
for any model Mγ ̸= MT, the proof is complete. Observe that the diﬀerence between
the approximations to (A.2) and (A.4) comes just from the extra term
Q
i∈Jγ
(1−ˆti,γ)−1/2
Q
i∈JT
(1−ˆti,T )−1/2
which equals
Q
i∈Jγ
[
bi,γ(1−R2
γ)
R2
i,γ(m−bγ)]−1/2
Q
i∈JT
[
bi,T (1−R2
T )
R2
i,T (m−bT )]−1/2 =


Q
i∈Jγ
O(mR2
i,γ)
Q
i∈JT
O(mR2
i,T)


1
2
.
(A.5)
Case 1: Mγ ̸⊃MT
In this case, the remaining term in the Laplace approximation to the integral goes
to zero in probability at an exponential rate (see Appendix A.11) while the extra part
will either be bounded, go to inﬁnity or go to zero in probability at a polynomial rate.
For all models Mγ belonging to Case 1, (A.4) equals zero.
Case 2: Mγ ⊃MT
Case 2A: For Case 2A of Theorem 3.5.2, Bγ = BT and MT ⊂Mγ which means that
whenever pi,γ = 1, we must have pi,T = 1 whereas it is possible to have pi,T = 1
along with pi,γ > 1. This indicates that |JT| ≥|Jγ| for this group of models. Also
note that all R2
i,j
P→C for some non-zero constant C (by Lemma A.11.1 (i) from
Appendix A.11). Hence (A.5) reduces to O(m
|Jγ|−|JT |
2
) while the other part of the
integral is of the order O(m
p∗
T −p∗γ
2
) = O(m
pT −pγ+|JT |−|Jγ|
2
). This implies that (A.4)
equals lim
m→∞O(mpT −pγ) = 0, as in Case 2A from Appendix A.11.
Case 2B: For Case 2B of Theorem 3.5.2, Bγ ⊃BT and MT ⊂Mγ.
As before
for i ∈BT
T Bγ = BT, whenever pi,γ = 1, we must have pi,T = 1 whereas it is
possible to have pi,T = 1 along with pi,γ > 1.
This translates to the inequality
219

|Jγ
T BT| ≤|JT|.
Whereas for i ∈Bγ\BT, it might happen that pi,γ = 1, but
the corresponding pi,T = 0 since the block does not exist. However, R2
i,γ →0 and
nR2
i,γ
d→cχ2
pi,γ for i ∈Bγ\BT (by Lemma A.11.1 (ii) and (iii)) so that mR2
i,γ = O(1)
for all i ∈Jγ\BT. Thus (A.5) becomes O(m
|Jγ
T BT |−|JT |
2
) and the other part of the
integral is O

m
1
2
P
i∈BT
[p∗
i,T −p∗
i,γ]
= O

m
1
2
P
i∈BT
[pi,T −pi,γ]+ 1
2 [ |JT |−|Jγ
T BT | ]
. (A.4) is zero
since it reduces to lim
m→∞O

m
1
2
P
i∈BT
[pi,T −pi,γ]
= 0 and we have the same result as in
Case 2B from Appendix A.11.
Case 2C: For Case 2C of Theorem 3.5.2, we know that there is a one to one equivalence
between pi,T = 1 and pi,γ = 1 for i ∈BT since the predictors are exactly the same in all
blocks common to Mγ and MT. This means that |Jγ
T BT| = |JT|, while as before,
mR2
i,γ = O(1) for all i ∈Jγ\BT. Now (A.5) is of the order O(m
|Jγ
T BT |−|JT |
2
) = O(1)
and the other term in the Laplace approximation is also O(1), leading to the same
conclusion as in Case 2C from Appendix A.11 that the limit of the Bayes factor does
not equal zero with probability 1.
The situation when Mγ = M0 is similar to Case 2C here and in Theorem 3.5.2
with BT = φ. By the same reasoning, the Bayes factor does not converge to zero in
probability.
A.17
Description of the Ozone Data
The ozone data set comprises of the 10 variables described below:
upo3: Daily ozone concentration (maximum 1-hour average, parts per million) at
Upland, CA.
vdht: Vanderburg 500-millibar-pressure height (m).
wdsp: Wind speed (mph) at Los Angeles International Airport (LAX).
220

hmdt: Humidity (%) at LAX.
sbtp: Sandburg Air Force Base temperature (◦F).
ibht: Inversion base height at LAX.
dgpg: Daggett pressure gradient (mmHg) from LAX to Daggett, CA.
ibtp: Inversion base temperature at LAX (◦F).
vsty: Visibility (miles) at LAX.
day: Calender day, an integer between 1 and 365.
221

Appendix B: Appendix for Chapter 4
B.1
Proof of Lemma 4.3.2
We will prove the lemma for the simplest case when k = 2. Generalization of this
result to k > 2 is immediate and has been discussed at the end.
The following identity is an essential step of the proof:
lim
z→1(1 −z)a+b−c
2F1(a, b, c; z) = Γ(a + b −c)Γ(c)
Γ(a)Γ(b)
, when Re(c −a −b) < 0.
(B.1)
We want to show that if η1 + η2 = M and lim
N→∞ηi > 0, lim
N→∞L = 1, lim
N→∞M = 1
and lim
N→∞
1−L
1−M = D, then as N →∞
I =
R
(0,1)2
Q2
i=1(1 −ti)
a+pi
2
−2(1 −t1η1 −t2η2)−mdt
R
(0,1)2
Q2
i=1(1 −ti)
a+pi
2
−2(1 −t2L)−mdt
→0.
The denominator of I,
Denom
=
2
a + p1 −2
Z 1
0
(1 −t2)
a+p2
2
−2(1 −t2L)−mdt2
=
4
(a + p1 −2)(a + p2 −2)
2F1

m, 1, a + p2
2
; L

.
Numerator of I,
Num
≤
Z 1
0
Z 1
0
(1 −t2)
a+p2
2
−2(1 −t1η1 −t2η2)−mdt1dt2
(since (1 −t1)
a+p1
2
−2 ≤1 when a ≥3)
222

=
1
(m −1)η1
" Z 1
0
(1 −t2)
a+p2
2
−2(1 −t2η2 −η1)1−mdt2
−
Z 1
0
(1 −t2)
a+p2
2
−2(1 −t2η2)1−mdt2
#
=
(1 −η1)1−m
(m −1)η1
Z 1
0
(1 −t2)
a+p2
2
−2(1 −t2
η2
1 −η1
)1−mdt2
−
2
(a + p2 −2)(m −1)η1
2F1(m −1, 1, a + p2
2
; η2).
So, Num ≤C1 × 2F1(m −1, 1, a+p2
2 ;
η2
1−η1) −C2, where C1 and C2 are ﬁnite non-zero
constants with the property that lim
N→∞C1 and lim
N→∞C2 are also ﬁnite and non-zero..
Then, I
=
Num
Denom ≤
C1 2F1(m −1, 1, a+p2
2 ;
η2
1−η1) −C2
C3 2F1(m, 1, a+p2
2 ; L)
.
Applying (B.1), the following relations hold under additional restrictions on m:
lim
L→1(1 −L)m+1−a+p2
2
2F1(m, 1, a + p2
2
; L)
=
Γ(a+p2
2 )Γ(m + 1 −a+p2
2 )
Γ(m)Γ(1)
,
lim
η2
1−η1 →1(1 −
η2
1 −η1
)m−a+p2
2
2F1(m −1, 1, a + p2
2
;
η2
1 −η1
)
=
Γ(a+p2
2 )Γ(m −a+p2
2 )
Γ(m −1)Γ(1)
.
The second relation can be further simpliﬁed as
lim
N→∞(1 −η1)−m+ a+p2
2 (1 −η1 −η2)m−a+p2
2
2F1(m −1, 1, a + p2
2
;
η2
1 −η1
)
=
lim
N→∞(1 −η1)−m+ a+p2
2 (1 −M)m−a+p2
2
2F1(m −1, 1, a + p2
2
;
η2
1 −η1
)
=
Γ(a+p2
2 )Γ(m −a+p2
2 )
Γ(m −1)Γ(1)
since M →1 and 0 < lim
N→∞η1 < 1.
Hence,
lim
N→∞I
≤
lim
N→∞
C1 2F1(m −1, 1, a+p2
2 ;
η2
1−η1) −C2
C3 2F1(m, 1, a+p2
2 ; L)
223

=
lim
N→∞
C∗
1(1 −M)
a+p2
2
−m −C2
C∗
3(1 −L)
a+p2
2
−m−1
=
lim
N→∞
C∗
1(1 −M)
a+p2
2
−m(1 −L)m+1−a+p2
2
−C2(1 −L)m+1−a+p2
2
C∗
3
=
−C2 lim
N→∞(1 −L)m+1−a+p2
2
+ C∗
1 lim
N→∞(1 −L)

1−L
1−M
m−a+p2
2
C∗
3
=
0 + C∗
1 × Dm−a+p2
2
× 0
C∗
3
= 0
where C∗
1, C∗
3 are ﬁnite non-zero constants provided that m > a+p2
2 . When k = 2, the
requirement regarding the minimum value of m is precisely the condition stated in
the lemma which ensures m > a+p2
2
+k −2 = a+p2
2 . So lim
N→∞I ≤0 and I being a ratio
of non-negative integrals, we must have lim
N→∞I = 0.
This completes the proof of the lemma for k = 2. The proof for k > 2 can be
done similarly applying (B.1) and the following inequality
I
=
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −t1η1 −t2η2 −. . . −tkηk)−mdt
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −tjL)−mdt
≤
R
(0,1)k(1 −tj)
a+pk
2
−2(1 −Pk
i=1 tiηi)−mdt
R
(0,1)k
Qk
i=1(1 −ti)
a+pi
2
−2(1 −tjL)−mdt
since (1 −ti)
a+pi
2
≤1 for any i when a ≥3. The only diﬀerence in the extension to
k > 2 is that the required condition in the general case is m > a+pj
2
+ k −2.
B.2
Proof of Lemma 4.3.3
In the sequence {ΨN}, yTy = Pk
i=1 yTPQiy+(n−p−1)ˆσ2 = Pk
i=1(Qibτ i)T(Qibτ i)+
(n −p −1)bσ2, and bσ2 is ﬁxed for all N. Because of blockwise orthogonality of the
design Q and invariance of least squares estimates to linear transformations, given an
index i, O

(Qibτ i)T(Qibτ i)

= O(||τ i||2) = O(||βj||2) for some index j.
224

When Condition 4.3.1 holds, it is evident that for i = l + 1, l + 2, .., k, R2
i =
O(1)
O(||βs||2) →0, since ||βs||2 →∞. This proves (i).
When i = 1, 2, .., l, we can show that R2
i = O(||βs||2)
O(||βs||2) →ηi for constants 0 < ηi < 1
for all i. This proves (ii).
It is also easy to prove that 1 −
lP
i=1
R2
i =
O(1)
O(||βs||2) →0, implying that
lP
i=1
R2
i →1
as stated in (iii).
To prove (iv) when a BOB hyper-g prior is speciﬁed on the sequence {ΨN}, one
needs to show that for some ﬁnite C ≥1,
lim
N→∞
R
kQ
i=1
(1 −ti)bi

1 −
kP
i=1
tiR2
i
−m
dt
R
kQ
i=1
(1 −ti)bi

1 −
lP
i=1
tiR2
i
−m
dt
= C
where bi = a+pi
2
−2 ≥0 (since a ≥3) and m = n−1
2 .
Since R2
i ≥0 for each i,
Z
k
Y
i=1
(1 −ti)bi
 
1 −
k
X
i=1
tiR2
i
!−m
dt
≥
Z
k
Y
i=1
(1 −ti)bi
 
1 −
l
X
i=1
tiR2
i
!−m
dt
=⇒
lim
N→∞
Z
k
Y
i=1
(1 −ti)bi
 
1 −
k
X
i=1
tiR2
i
!−m
dt
≥
lim
N→∞
Z
k
Y
i=1
(1 −ti)bi
 
1 −
l
X
i=1
tiR2
i
!−m
dt
which indicates that C ≥1, but it still needs to be shown that C is ﬁnite. For the
remaining part of the proof we need the following proposition which has been derived
in Appendix B.3.
225

Proposition B.2.1. Let f1(t) and f2(t) denote the properly normalized densities on
(0, 1)k with m > 0 and bi ≥0 ∀i:
f1(t)
∝
k
Y
i=1
(1 −ti)bi(1 −
X
i∈I
tiR2
i )−m
f2(t)
∝
(1 −
X
i∈I
tiR2
i )−m
where I ⊆{1, 2, .., k} and R2
i > 0 ∀i ∈I.
Assume that Pk
i=1 R2
i < 1 so that both the densities deﬁned above are proper.
If
h(t) =

1−P
i∈I tiR2
i
1−Pk
i=1 tiR2
i
m
, then Ef2(h(t)) ≥Ef1(h(t)), i.e.,
R Qk
i=1(1 −ti)bi

1 −Pk
i=1 tiR2
i
−m
dt
R Qk
i=1(1 −ti)bi  1 −P
i∈I tiR2
i
−m dt
≤
R 
1 −Pk
i=1 tiR2
i
−m
dt
R  1 −P
i∈I tiR2
i
−m dt
.
Applying the result from Proposition B.2.1 in this problem, we have
R
kQ
i=1
(1 −ti)
a+pi
2
−2

1 −
kP
i=1
tiR2
i
−n−1
2
dt
R
kQ
i=1
(1 −ti)
a+pi
2
−2

1 −
lP
i=1
tiR2
i
−n−1
2
dt
≤
R 
1 −
kP
i=1
tiR2
i
−n−1
2
dt
R 
1 −
lP
i=1
tiR2
i
−n−1
2
dt
,
and so
lim
N→∞
R
kQ
i=1
(1 −ti)
a+pi
2
−2

1 −
kP
i=1
tiR2
i
−n−1
2
dt
R
kQ
i=1
(1 −ti)
a+pi
2
−2

1 −
lP
i=1
tiR2
i
−n−1
2
dt
≤lim
N→∞
R 
1 −
kP
i=1
tiR2
i
−n−1
2
dt
R 
1 −
lP
i=1
tiR2
i
−n−1
2
dt
.
Observe that the conditions essential for using the proposition are satisﬁed since
n−1
2
= m > 0 and bi = a+pi
2
−2 ≥0 (because a ≥3).
We simply need to prove that lim
N→∞
R
 
1−
k
P
i=1
tiR2
i
!−n−1
2
dt
R
 
1−
lP
i=1
tiR2
i
!−n−1
2
dt
= C, for some ﬁnite con-
stant C which must be bigger than or equal to 1 as reasoned earlier. Notice that
226

when m > k or equivalently n > 2k + 1,
Z
(0,1)k
 
1 −
k
X
i=1
tizi
!−m
dt
=
[(m −1)(m −2)...(m −k)]−1
z1z2 . . . zk
"
(1 −
k
X
i=1
zi)k−m −
X
i1,...,ik−1
(1 −
k−1
X
j=1
zij)k−m +
X
i1,...,ik−2
(1 −
k−2
X
j=1
zij)k−m −. . . + (−1)k1
#
.
(B.2)
So,
lim
N→∞
R 
1 −
kP
i=1
tiR2
i
−n−1
2
dt
R 
1 −
lP
i=1
tiR2
i
−n−1
2
dt
= lim
N→∞
1
kQ
i=l+1
h
(m −i)R2
i
i × EN
ED
where EN and ED contain the remaining set of terms when (B.2) is applied to the
numerator and denominator respectively.
For each i = l +1, . . . , k, R2
i =
O(1)
O(||βs||2), and EN consists of terms which are either
of the form

O(1)
O(||βs||2)
k−m
or

O(||βs||2)
O(||βs||2)
k−m
or simply O(1). Similarly ED consists
of terms which are either

O(1)
O(||βs||2)
l−m
or

O(||βs||2)
O(||βs||2)
l−m
or O(1). So,
lim
N→∞
R 
1 −
kP
i=1
tiR2
i
−n−1
2
dt
R 
1 −
lP
i=1
tiR2
i
−n−1
2
dt
=

O(||βs||2)
k−l [O(||βs||2)]m−k
[O(||βs||2)]m−l = O(1).
Thus the limit equals a ﬁnite constant C ≥1 as stated in the lemma.
B.3
Proof of Proposition B.2.1
First note that h(t) =

1−P
i∈I tiR2
i
1−Pk
i=1 tiR2
i
m
=

1−P
i∈I tiR2
i
1−P
i∈I tiR2
i −P
i̸∈I tiR2
i
m
is non-decreasing
(strictly increasing if all R2
i > 0) in each of the individual coordinates ti, i = 1, 2, . . . , k,
when we keep the remaining coordinates (t−i) ﬁxed. A popular result on multivariate
likelihood ratio order (MLRO) that appears in, for example Karlin and Rinott (1980),
suggests that a random variable U with density f1 is stochastically less than or equal
227

to another random variable V with density f2 if the following suﬃcient condition
holds
f1(x)f2(y) ≤f1(x ∧y)f2(x ∨y) ∀x, y.
Here we want to show that for any x, y
1
C1C2
k
Y
i=1
(1 −xi)bi(1 −
X
i∈I
xiR2
i )−m(1 −
X
i∈I
yiR2
i )−m
≤
1
C1C2
k
Y
i=1
(1 −(xi ∧yi))bi(1 −
X
i∈I
(xi ∧yi)R2
i )−m(1 −
X
i∈I
(xi ∨yi)R2
i )−m (B.3)
where C1, C2 are normalizing constants. Then f2(t) >st f1(t) and hence Ef2(h(t)) ≥
Ef1(h(t)) for any coordinate-wise non-decreasing function h(t).
To show (B.3), note that (1−(xi ∧yi))bi ≥(1−xi)bi ∀x whenever bi ≥0 and it is
suﬃcient to show that (1−P
i∈I
xiR2
i )(1−P
i∈I
yiR2
i ) ≥(1−P
i∈I
(xi∧yi)R2
i )(1−P
i∈I
(xi∨yi)R2
i )
∀x, y. If we denote xi ∧yi by mi and xi ∨yi by Mi ∀i, it suﬃces to prove that for
all l ∈N
(1 −
l
X
i=1
xi)(1 −
l
X
i=1
yi) ≥(1 −
l
X
i=1
mi)(1 −
l
X
i=1
Mi).
The above relation is trivially true for l = 2. Assume that it is true for all l = 2, 3, ..., q.
If we can prove that the relation is true for l = q + 1, then the proof is done (by
induction). Without loss of generality, assume that xq+1 < yq+1. Therefore,
(1 −
q+1
X
i=1
xi)(1 −
q+1
X
i=1
yi)
=
(1 −
q
X
i=1
xi −xq+1)(1 −
q
X
i=1
yi −yq+1)
=
(1 −
q
X
i=1
xi)(1 −
q
X
i=1
yi) −xq+1(1 −
q
X
i=1
yi) −yq+1(1 −
q
X
i=1
xi) + xq+1yq+1
≥
(1 −
q
X
i=1
mi)(1 −
q
X
i=1
Mi) + mq+1Mq+1 −mq+1(1 −
q
X
i=1
yi) −Mq+1(1 −
q
X
i=1
xi).
228

Showing that the last display is greater than (1 −Pq+1
i=1 mi)(1 −Pq+1
i=1 Mi) =
(1 −Pq
i=1 mi)(1 −Pq
i=1 Mi) + mq+1Mq+1 −mq+1(1 −Pq
i=1 Mi) −Mq+1(1 −Pq
i=1 mi)
is equivalent to proving that
Mq+1
q
X
i=1
xi + mq+1
q
X
i=1
yi
≥
Mq+1
q
X
i=1
mi + mq+1
q
X
i=1
Mi
⇔Mq+1
"
q
X
i=1
(xi −mi)
#
≥
mq+1
"
q
X
i=1
(Mi −yi)
#
which is true since Pq
i=1(xi −mi) = Pq
i=1(Mi −yi) = P
i:xi>yi(xi −yi).
This completes the proof of the Proposition.
B.4
Proof of Theorem 4.3.6
For any i ∈{1, 2, . . . , l},
E

gi
1 + gi
| y

=
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pk
j=1 tjR2
j)−n−1
2 dt
≥
R
(0,1)k ti
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt
R
(0,1)k
Qk
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt
=
R
(0,1)l ti
Ql
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt
R
(0,1)l
Ql
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt
using a minor variation of Lemma 3.4.2.
Due to Condition 4.3.1 and Lemma 4.3.1, Pl
j=1 R2
j →1 as N →∞.
Deﬁne
t1:l = (t1, . . . , tl) and let h(t1:l) ∝Ql
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 .
Then
E

gi
1+gi | y

≥Eh(ti) and if we show that lim
N→∞Eh(ti) = 1 for i = 1, 2..., l, then
lim
N→∞E

gi
1+gi | y

= 1 as well for any such i.
We argue that for any 0 < η < 1, lim
N→∞P(t1:l > (1 −η)1) = 1. Then
Eh(ti)
=
Eh(ti | t1:l > (1 −η)1)P(t1:l > (1 −η)1)
+Eh

ti | [t1:l > (1 −η)1]c
P

[t1:l > (1 −η)1]c
229

>
Eh(ti | t1:l > (1 −η)1)P(t1:l > (1 −η)1)
>
(1 −η)P(t1:l > (1 −η)1)
=⇒
lim
N→∞Eh(ti)
≥
(1 −η) lim
N→∞P(t1:l > (1 −η)1) = 1 −η
for any arbitrary 0 < η < 1. This can only happen if lim
N→∞Eh(ti) = 1 and our ﬁrst
claim follows.
We still need to prove that lim
N→∞P(t1:l > (1 −η)1) = 1 for any 0 < η < 1. First
deﬁne Sd = {t1:l : ti > 1 −η, for d indices i and ti ≤1 −η, for (l −d) indices i} for
every d = 0, 1, . . . , l −1. If the notation P
d is used to denote the sum over the
 l
d

unique elements contained in set Sd, then for any d ∈{0, 1, . . . , l −1},
P(t1:l > (1 −η)1)
P(t1:l ∈Sd)
=
R
(1−η,1)l
Ql
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt1:l
P
d
R
(0,1−η)l−d
R
(1−η,1)d
Ql
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt1:l
>
R
(1−ηc,1−ηc+1)l
Ql
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt1:l
P
d
R
(0,1−η)l−d
R
(1−η,1)d
Ql
j=1(1 −tj)
a+pj
2
−2(1 −Pl
j=1 tjR2
j)−n−1
2 dt1:l
(for any c ≥2)
>
R
(1−ηc,1−ηc+1)l
Ql
j=1 (ηc+1)
a+pj
2
−2 (1 −(1 −ηc) Pl
j=1 R2
j)−n−1
2 dt1:l
(l
d)
P
w=1
R
(0,1−η)l−d
R
(1−η,1)d
Q
j∈Id,w η
a+pj
2
−2(1 −(1 −η) P
j̸∈Id,w R2
j −P
j∈Id,w R2
j)−n−1
2 dt1:l
where each Id,w = {i : ti > 1 −η} and |Id,w| = d. Obviously, there are
 l
d

many
choices of Id,w.
The last step holds because (1 −tj)
a+pj
2
−2 is non-increasing in tj
whenever a ≥3 and (1 −Pl
j=1 tjR2
j)−(n−1)/2 is non-decreasing in each tj.
But we know that as N →∞,
lP
j=1
R2
j →1 and
P
j∈Id,w
R2
j →Md,w, for some
0 ≤Md,w < 1. Note that Md,w = 0 is feasible only when d = 0, in all other cases the
230

inequalities are strict. This means that as N →∞,
P
j̸∈Id,w
R2
j =
lP
i=1
R2
j −P
j∈Id,w
R2
j →
1 −Md,w.
Hence, given any 0 < η < 1 and any d = 0, 1, ..., l −1
lim
N→∞
P(t1:l > (1 −η)1)
P(t1:l ∈Sd)
≥
η(c+1) Pl
j=1(
a+pj
2
−2)η−c n−1
2 (ηc −ηc+1)l
P(l
d)
w=1
h
η
P
j∈Id,w(
a+pj
2
−2)[(1 −Md,w)η]−n−1
2 (1 −η)l−dηd
i
=
(1 −η)dη
n−1
2 −dηc[ la
2 + 1
2
Pl
j=1 pj−2l+l−n−1
2 ]
P(l
d)
w=1
h
(1 −Md,w)−n−1
2 η
−P
j̸∈Id,w(
a+pj
2
−2)i
=
η
n−1
2 −d+ (l−d)a
2
−2(l−d)(1 −η)d
P(l
d)
w=1
h
(1 −Md,w)
n−1
2 η
−P
j̸∈Id,w
pj
2 iη
c
2[l(a−2)+Pl
j=1 pj−n+1].
The last relation is true for any c ≥2 and hence, also for the limit as c →∞. Since
0 < η < 1, lim
c→∞η
c
2[l(a−2)+Pl
j=1 pj−n+1] = ∞whenever n > l(a −2) + Pl
j=1 pj + 1. This
proves that lim
N→∞
P(t1:l>(1−η)1)
P(t1:l∈Sd)
= ∞for any η and all possible values of d. Notice that
lim
N→∞
1 −P(t1:l > (1 −η)1)
P(t1:l > (1 −η)1)
=
lim
N→∞
P(t1:l ∈Sl−1
d=0 Sd)
P(t1:l > (1 −η)1)
=
lim
N→∞
Pl−1
d=0 P(t1:l ∈Sd)
P(t1:l > (1 −η)1)
=
l−1
X
d=0
lim
N→∞
P(t1:l ∈Sd)
P(t1:l > (1 −η)1) = 0,
which is possible if and only if lim
N→∞P(t1:l > (1 −η)1) = 1.
The next part of the proof is borrowed from the proof of Theorem 3.4.1 in Ap-
pendix A.7. Using the same steps we can show that for any i = l + 1, l + 2, . . . , k,
2
a + pi
≤E

gi
1 + gi
| y

≤
2
a + pi
2F1(n−1
2 , 2; a+pi
2
+ 1; κi)
2F1(n−1
2 , 1; a+pi
2 ; κi)
231

where κi =
R2
i
1−P
j̸=i R2
j =
yT PQiy
(n−p−1)ˆσ2+yT PQiy. Observe that for all i > l, κi is ﬁxed and
bounded away from 1 in the sequence {ΨN}. This signiﬁes that
2
a + pi
≤lim
N→∞E

gi
1 + gi
| y

< 1 ∀i > l.
B.5
Proof of Lemma 4.5.3
As before, the Bayes factor for a model Ml can be written as
BF(Ml : M0) =
a −2
2
k Z
(0,1)k
" k
Y
i=1
(1 −ti)
a+pi
2
−2
#  
1 −
k
X
i=1
tiR2
i,l
!−n−1
2
dt
which simpliﬁes under the conditions a = 3 and pi = 1 ∀i in the lemma to
BF(Ml : M0) =
1
2
p Z
(0,1)p
 
1 −
p
X
i=1
tiR2
i,l
!−n−1
2
dt.
We want to show that
lim
N→∞
R
(0,1)p
 1 −Pp
i=1 tiR2
i,l
−n−1
2 dt
R
(0,1)p
 1 −Pp
i=1 tiR2
i,1
−n−1
2 dt
= 0
(B.4)
where R2
i,1 and R2
i,l denote the components of R2 in models M1 and Ml respectively.
Assume that in model Ml, Xl1 and Xl2 are two blocks with arbitrary indices 1 ≤
l1 < l2 ≤p that have interchanged positions in the order of block orthogonalization.
This is the simplest case where only two blocks are not in the ‘correct’ order and we
will prove that (B.4) is true for such a model Ml. The result for more complicated
cases where many blocks are wrongly ordered follows similarly.
Observe that R2
i,1 = O(|βi|2)
O(|β1|2) for all i = 1, 2, . . . , p. On the other hand R2
i,l =
O(|βl1|2)
O(|β1|2)
for i ∈T = {l1 + 1, . . . , l2} and R2
i,l = O(|βi|2)
O(|β1|2) for all other i ̸∈T . Hence by applying
(B.2) (which requires n > 2p + 1), the LHS of (B.4) reduces to
lim
N→∞
R
(0,1)p
 1 −Pp
i=1 tiR2
i,l
−n−1
2 dt
R
(0,1)p
 1 −Pp
i=1 tiR2
i,1
−n−1
2 dt
=
lim
N→∞
Qp
i=1 R2
i,1
Qp
i=1 R2
i,l
× lim
N→∞
El
E1
232

where each term in El and E1 is of the form
h
O(|βi|2)
O(|β1|2)
ip−(n−1)/2
or
h
O(|β1|2)
O(|β1|2)
ip−(n−1)/2
or
O(1). The terms of the order
h
O(|β1|2)
O(|β1|2)
ip−(n−1)/2
will converge to a ﬁnite constant, while
terms of the order
h
O(|βi|2)
O(|β1|2)
ip−(n−1)/2
for i > 1. will diverge to inﬁnity as N →∞.
However, the rate of divergence is the same in both El and E1 due to which lim
N→∞El/E1
is a ﬁnite non-zero constant.
Hence
lim
N→∞
R
(0,1)p
 1 −Pp
i=1 tiR2
i,l
−n−1
2 dt
R
(0,1)p
 1 −Pp
i=1 tiR2
i,1
−n−1
2 dt
=
lim
N→∞
Qp
i=1
O(|βi|2)
O(|β1|2)
Q
i̸∈T
O(|βi|2)
O(|β1|2)
Q
i∈T
O(|βl1|2)
O(|β1|2)
× C
=
C × lim
N→∞
Q
i̸∈T
O(|βi|2)
O(|β1|2)
Q
i̸∈T
O(|βi|2)
O(|β1|2)
× lim
N→∞
Q
i∈T
O(|βi|2)
O(|β1|2)
Q
i∈T
O(|βl1|2)
O(|β1|2)
=
C∗lim
N→∞
Ql2
i=l1+1 O(|βi|2)
Ql2
i=l1+1 O(|βl1|2)
= 0
where C and C∗are ﬁnite and non-zero constants. The last expression goes to zero
since
lim
N→∞
|βi|2
|βl1|2 = 0, ∀i > l1, in the sequence {ΨN} described in (4.19) when
pi = 1 ∀i. This proves (B.4).
B.6
Proof of Lemma 4.5.4
The proof is closely related to the proof of Lemma 4.5.3 in Appendix B.5. Since all
the blocks are of size one, the number of blocks is equal to the number of predictors
in both models M ′ and M ∗. Let p∗and p′ be the number of blocks (predictors) in
the models M ∗and M ′ respectively. Since M ∗∈S, we must have p∗= p or p −1.
Following the proof of Lemma 4.5.3, if n > 2p + 1,
lim
N→∞BF(M ′ : M ∗)
=
lim
N→∞
R
(0,1)p′

1 −Pp′
i=1 ti[R′
i]2−n−1
2 dt
R
(0,1)p∗
 1 −Pp∗
i=1 ti[R∗
i ]2−n−1
2 dt
= lim
N→∞
Qp′
i=1[R∗
i ]2
Qp∗
i=1[R′
i]2 × E′
E∗
(B.5)
233

where E′ and E∗are derived using (B.2).
The combination of all the terms in E∗lead to E∗= (O(|β1|2))(n−1)/2−p∗. The
order of E′ is decided by the blocks present in the model M ′.
If X1 appears in
M ′, then E′ =

O(|β1|2)
O(|βl|2)
(n−1)/2−p′
for the smallest index l missing from M ′ (if all p
predictors are present in model M ′, take l = p), otherwise E′ = O(1). Recall that for
each i, [R∗
i ]2 = O(|βi|2)/O(|β1|2) and [R′
i]2 = O(|βj|2)/O(|β1|2), for some j = 1, . . . , p,
which depends on the index i and the permutation of the blocks in M ′. This implies
that E∗/ Qp∗
i=1[R∗
i ]2 = O(|β1|2)(n−1)/2
Qp−1
j=1 O(|βj|2) .
Case 1: X1 is included in model M ′
In this case, E′/ Qp′
i=1[R′
i]2 =

O(|β1|2)
O(|βl|2)
(n−1)/2−p′
(O(|β1|2))p′
Q
j∈T O(|βj|2), where T is an index
set with p′ elements determined by the model M ′ and the order of the covariate blocks
within M ′. Some of the elements in T might be repeated, and l ≥2, l ̸∈T because
Xl does not belong to model M ′. Again, from the deﬁnition of l = min{j : Xj ̸∈M ′},
one of the following two scenarios must be true:
(i) all of the indices {1 : l −1} = {1, 2, . . . , l −1} belong to the set T.
(ii) some (not all) of the indices {1 : l −1} belong to the set T, and there are at least
l −1 indices in T that are smaller than l. Index 1 appears at least once among these
l −1 elements.
Hence, (B.5) reduces to the limit of the product of three parts
lim
N→∞
O(|β1|2)
O(|β1|2)
(n−1)/2
Qp∗
j=l O(|βj|2)
O(|βl|2)(n−1)/2−p′ Q
j∈T∩{1:l−1}c O(|βj|2)
Ql−1
j=1 O(|βj|2)
Q
j∈T∩{1:l−1} O(|βj|2).
The ﬁrst part is an O(1) term, while the third part is either O(1) or goes to zero as
N →∞depending on the elements in T. (B.5) equals zero due to the second and
third parts of the product when n is large enough, since βl diverges faster than all βj,
j = l + 1, . . . , p∗, in the sequence {ΨN}. The choice n > 2(p + p′) −1 always works,
234

but in general a smaller value of n is suﬃcient for the result to be true, depending on
l and the permutation of the blocks in M ′.
Case 2: X1 is not included in model M ′
Then E′/ Qp′
i=1[R′
i]2 = O(1)O(|β1|2)p′
Q
j∈T O(|βj|2) for an index set T of size p′ which does not
include the element 1 and (B.5) equals
lim
N→∞
O(1)O(|β1|2)p′
Q
j∈T O(|βj|2)
Qp−1
j=1 O(|βj|2)
O(|β1|2)(n−1)/2 = 0
when n is suﬃciently large using an argument similar to the one in Case 1.
B.7
Description of the Boston Housing Data
The Boston housing data set consists of the following 14 variables:
crim: per capita crime rate by town.
zn: proportion of residential land zoned for lots over 25,000 sq.ft.
indus: proportion of non-retail business acres per town.
chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox: nitric oxides concentration (parts per 10 million).
rm: average number of rooms per dwelling.
age: proportion of owner-occupied units built prior to 1940.
dis: weighted distances to ﬁve Boston employment centres.
rad: index of accessibility to radial highways.
tax: full-value property-tax rate per $10,000.
ptratio: pupil-teacher ratio by town.
b: 1000(Bk −0.63)2 where Bk is the proportion of blacks by town.
lstat: % lower status of the population.
medv: Median value of owner-occupied homes in $1000’s.
235

Appendix C: Appendix for Chapter 5
C.1
Derivation of π(β | σ2
ϵ) in (5.4)
We ﬁrst write β in terms of its hyperspherical coordinates as in Section 5.4.2:
β1 = δ cos(φ1)
β2 = δ sin(φ1) cos(φ2)
...
...
βp−1 = δ sin(φ1) · · · sin(φp−2) cos(φp−1)
βp = δ sin(φ1) · · · sin(φp−2) sin(φp−1),
where 0 ≤φj ≤π, for j = 1, . . . , p −2, 0 ≤φp−1 < 2π and δ2 = Pp
j=1 β2
j ≥0. For a
given δ2, the regression coeﬃcients β are constrained to lie on the (p−1)-dimensional
surface of a p-dimensional hypersphere of radius δ. The assumption that, given δ,
probability is uniformly distributed on this surface results in the following Jacobian
for the transformation:
δp−1 sinp−2(φ1) sinp−3(φ2) · · · sin(φp−2) dδ dφ1 · · · dφp−1.
Therefore, for a ﬁxed δ, to obtain a uniform distribution on the surface of the
hypersphere we must have
π(φ1, . . . , φp−1) ∝sinp−2(φ1) sinp−3(φ2) · · · sin(φp−2).
236

Now,
Z π
0
sinn(x) dx =
√πΓ
  n+1
2

Γ
  n
2 + 1

for n > −1, so the marginal density function for φj, j = 1, . . . , p −2 is
π(φj) = Γ
  p−j−1
2
+ 1

√πΓ
  p−j
2

sinp−j−1(φj),
0 ≤φj ≤π,
and π(φp−1) = (2π)−1, 0 ≤φp−1 < 2π.
The properly normalized joint density function for all the φj is therefore
π(φ1, . . . , φp−1)
=
1
2π
p−2
Y
n=1
sinn(φp−1−n)
√π
Γ
  n
2 + 1

Γ
  n+1
2

=
Γ(p/2)
2πp/2
p−2
Y
n=1
sinn(φp−1−n).
(C.1)
Recalling that the density function for δ given σ2
ϵ is
π(δ | σ2
ϵ) =
2σ2b
ϵ
Be(a, b)δ2a−1(σ2
ϵ + δ2)−(a+b),
the joint density function for δ and φ = (φ1, . . . , φp−1)T is
π(φ, δ | σ2
ϵ)
=
pδ(δ)pφ(φ1, . . . , φp−1)
=
Γ(p/2)
πp/2
σ2b
ϵ
Be(a, b)δ2a−1(σ2
ϵ + δ2)−(a+b)
p−2
Y
n=1
sinn(φp−1−n),
for δ ≥0, 0 ≤φj ≤π for j = 1, . . . , p −2 and 0 ≤φp−1 < 2π.
Using the density π(φ, δ | σ2
ϵ), we can transform (φ1, ..., φp−1, δ) →(β1, ..., βp−1, δ)
to obtain a prior density function for β1, . . . , βp−1 given δ. This is a many-to-one
mapping: both φp−1 and |2π −φp−1| correspond to the same value of βp−1 because
φp−1 range between 0 and 2π. We will adjust for this by multiplying the resulting
density function by 2. The Jacobian matrix for this transformation is
237








−δ sin(φ1)
0
· · ·
cos(φ1)
δ cos(φ1) cos(φ2)
−δ sin(φ1) sin(φ2)
· · ·
sin(φ1) cos(φ2)
...
...
...
...
δ cos(φ1) · · · sin(φp−2) cos(φp−1)
...
· · ·
sin(φ1) · · · cos(φp−1)
0
0
· · ·
1







,
the absolute value of the determinant of which is
δp−1 sinp−1(φ1) sinp−2(φ2) · · · | sin(φp−1)|.
The joint density function of β−p = (β1, . . . , βp−1)T and δ given σ2
ϵ is therefore
π(β−p, δ | σ2
ϵ)
=
2 Γ(p/2)
πp/2
σ2b
ϵ
Be(a, b)δ2a−1(σ2
ϵ + δ2)−(a+b)
Qp−2
n=1 sinn(φp−1−n)
δp−1| sin(φp−1)| Qp−2
j=1 sinp−j(φj)
=
Γ(p/2)
πp/2
2σ2b
ϵ
Be(a, b)δ2a−p(σ2
ϵ + δ2)−(a+b)

p−1
Y
j=1
{sin(φj)}−1

=
Γ(p/2)
πp/2
2σ2b
ϵ
Be(a, b)δ2a−p(σ2
ϵ + δ2)−(a+b)

δ/
v
u
u
tδ2 −
p−1
X
j=1
β2
j

,
since |βp| =
q
δ2 −Pp−1
j=1 β2
j .
Now consider the transformation from (β1, . . . , βp−1, δ) →(β1, . . . , βp−1, βp), where
βp = ±
q
δ2 −Pp−1
j=1 β2
j . This is a one-to-many mapping where βp can be positive or
negative for a ﬁxed δ.
We adjust for this in the density function calculation by
dividing by two. The Jacobian matrix is
 Ip−1
0(p−1)×1
uT
1
2(Pp
j=1 β2
j )−1/22βp

,
for some (p-1)-dimensional vector u. The absolute value of the determinant of the
Jacobian matrix is
q
β2
p/βTβ.
238

The joint density function for β given σ2
ϵ is therefore
π(β | σ2
ϵ)
=
1
2π(β−p, δ | σ2
ϵ)
q
β2
p/βTβ
=
Γ(p/2)
πp/2
σ2b
ϵ
Be(a, b)δ2a−p(σ2
ϵ + δ2)−(a+b)

δ/
v
u
u
tδ2 −
p−1
X
j=1
β2
j


q
β2
p/βTβ
=
Γ(p/2)
Be(a, b)
σ2b
ϵ
πp/2(βTβ)a−p/2(σ2
ϵ + βTβ)−(a+b) , β ∈Rp.
C.2
Derivation of the Scale Mixture Representation of β
We know that
π(β | σ2
ϵ) = Γ(p/2)
Be(a, b)
σ2b
ϵ
πp/2(βTβ)a−p/2(σ2
ϵ + βTβ)−(a+b), β ∈Rp
which we wish to express in the form
β | v, σ2
ϵ
∼
N(0, vσ2
ϵIp)
v
∼
Gv(·)
for some distribution Gv with density gv such that
π(β | σ2
ϵ)
=
Z ∞
0
1
(2πvσ2
ϵ)p/2 exp

−
1
2vσ2
ϵ
βTβ

gv(v)dv.
(C.2)
The following method for deriving the mixing density using inverse Laplace trans-
forms for function convolutions was suggested by Jim Berger (personal communica-
tion, October 20, 2012). Consider the function H(s) = F(s)G(s) =
1
sc(s+σ2ϵ )d, where
F(s) = 1
sc and G(s) =
1
(s+σ2ϵ )d for some c, d > 0. The inverse Laplace transforms for F
and G are respectively f(t) = tc−1
Γ(c) and g(t) = td−1
Γ(d)e−tσ2
ϵ so that F(s) =
R ∞
0 f(t)e−stdt
and G(s) =
R ∞
0 g(t)e−stdt. Due to the convolution theorem for Laplace transforms,
H(s) = F(s)G(s)
=
Z ∞
0
 Z t
0
f(t −x)g(x)dx

e−stdt
=⇒
1
sc(s + σ2
ϵ)d
=
Z ∞
0
 Z t
0
(t −x)c−1
Γ(c)
xd−1
Γ(d)e−xσ2
ϵ

e−stdt.
239

When a < p/2 so that the exponent a −p/2 < 0, selecting the appropriate
constants in the last result allows us to express (5.4) as
π(β | σ2
ϵ)
=
Γ(p/2)σ2b
ϵ π−p/2
Γ(a)Γ(b)Γ(p/2 −a)
Z ∞
0
h Z t
0
(t −x)p/2−a−1xa+b−1e−xσ2
ϵ dx
i
exp(−(βTβ)t)dt
=
Γ(p/2)σ2b
ϵ π−p/2
Γ(a)Γ(b)Γ(p/2 −a)
Z ∞
0
h Z
1
2vσ2ϵ
0
(
1
2vσ2
ϵ
−x)p/2−a−1xa+b−1e−xσ2
ϵ dx
i
×
1
2v2σ2
ϵ
exp
 −βTβ
2vσ2
ϵ

dv
=
Γ(p/2)σ2b+p−2
ϵ
2p/2−1
Γ(a)Γ(b)Γ(p/2 −a)
Z ∞
0
vp/2−2h Z
1
2vσ2ϵ
0
(
1
2vσ2
ϵ
−x)p/2−a−1xa+b−1e−xσ2
ϵ dx
i
×
1
(2πvσ2
ϵ)p/2 exp
 −βTβ
2vσ2
ϵ

dv.
Comparing the last expression with (C.2), it is clear that
gv(v)
=
Γ(p/2)σ2b+p−2
ϵ
2p/2−1
Γ(a)Γ(b)Γ(p/2 −a) vp/2−2
Z
1
2vσ2ϵ
0
(
1
2vσ2
ϵ
−x)p/2−a−1xa+b−1e−xσ2
ϵ dx
=
Γ(p/2)2−b
Γ(a)Γ(b)Γ(p/2 −a)v−(b+1)
Z 1
0
(1 −y)p/2−a−1e−y/2vya+b−1dy
(substituting y = 2vσ2
ϵx)
=
Γ(p/2)2−b
Be(a, b)Γ(b + p/2) v−(b+1)
1F1(a + b; b + p/2; −1/2v)
as 1F1(A; B; z) =
R 1
0 ezttA−1(1 −t)B−A−1dt when B > A > 0.
C.3
Log-concavity of the Full Conditional of 1/σ2
ϵ
To simplify notation, we refer to σϵ simply as σ in the proof. We know that
π(σ2 | δ2, y, φ)
∝
((n −1)σ2 + δ2)−(a+b)σ2b−2a1−n−2
× exp

−1
σ2
h 1
b1
+ 1
2(y −V τ)T(y −V τ)
i
.
Letting v =
1
σ2, the Jacobian for the transformation is −1
v2.
If we deﬁne SS =
(y −V τ)T(y −V τ), then
240

π(v | δ2, y, φ)
∝
(n −1
v
+ δ2)−(a+b)va1+ n
2 +1−b exp

−v
h 1
b1
+ SS
2
i −1
v2

∝
(n −1
v
+ δ2)−(a+b)va1+ n
2 −b−1 exp

−v
h 1
b1
+ SS
2
i
.
We want to show that P = log π(v | rest) is concave, where
P = −A log(n −1
v
+ δ2) + B log v −Cv −log D,
A = (a + b), B = n
2 + a1 −b −1, C = 1
b1 + SS
2 , and D is the normalizing constant for
the density π(v | δ2, y, φ). When n > 2(b + 1) −2a1, all of the constants A, B, C and
D are positive since a, b, a1, b1 > 0 and SS ≥0. Now,
dP
dv
=
B
v −C −
A
n−1
v
+ δ2
h
−(n −1) 1
v2
i
= B
v −C +
(n −1)A
(n −1)v + δ2v2
=⇒d2P
dv2
=
−B
v2 −0 −
A(n −1)

(n −1)v + δ2v22(n −1 + 2vδ2)
=
−B
v2 −(n −1)A(n −1 + 2vδ2)
[(n −1)v + δ2v2]2
.
Because A, B, v > 0 , d2P
dv2 is strictly less than zero, implying that the full conditional
of 1/σ2
ϵ is log-concave.
C.4
MH Step for the joint update of (M, φ−1)
The dimension matching problem can be resolved with a joint proposal for M and
φ−1. Factor the joint density as
π(M, φ−1 | y, δ2, φ1, σ2
ϵ, γ) ∝π(φ−1 | M, y, δ2, φ1, σ2
ϵ, γ) π(M | y, δ2, φ1, σ2
ϵ, γ).
Then the acceptance probability for a proposed update (M ∗, φ∗
−1) from the current
iteration (M, φ−1) is given by α = min{1, αr}, where
241

αr
=
π(M ∗, φ∗
−1 | y, δ2, φ1, σ2
ϵ, γ)
π(M, φ−1 | y, δ2, φ1, σ2
ϵ, γ)
q
 (M ∗, φ∗
−1) →(M, φ−1)

q
 (M, φ−1) →(M ∗, φ∗
−1)

=
π(φ∗
−1 | M ∗, y, δ2, φ1, σ2
ϵ, γ) π(M ∗| y, δ2, φ1, σ2
ϵ, γ)
π(φ−1 | M, y, δ2, φ1, σ2
ϵ, γ) π(M | y, δ2, φ1, σ2
ϵ, γ)
p(M | M ∗)p(φ−1 | M)
p(M ∗| M)p(φ∗
−1 | M ∗).
The proposal q(·) can be decomposed as q((M, φ−1) →(M ∗, φ∗
−1)) = p(M ∗| M)
p(φ∗
−1 | M ∗). p(M ∗| M) is chosen to be symmetric and the proposed model M ∗has
one fewer predictor or one more predictor compared to the current model M. Given
the proposed model and hence its dimension, each of the new angles in φ∗
−1 is drawn
independently from a Uniform(0, π) distribution. Furthermore, the ratio p(M|M∗)
p(M∗|M) = 1
due to the symmetric proposal distribution. Thus,
αr
=
π(φ∗
−1 | M ∗, y, δ2, φ1, σ2
ϵ, γ)
p(φ∗
−1 | M ∗)
p(φ−1 | M)
π(φ−1 | M, y, δ2, φ1, σ2
ϵ, γ)
π(M ∗| y, δ2, φ1, σ2
ϵ, γ)
π(M | y, δ2, φ1, σ2
ϵ, γ)
=
p(y | M ∗, δ2, φ1, σ2
ϵ, γ)
p(y | M, δ2, φ1, σ2
ϵ, γ)
π(M ∗)
π(M)
π(φ∗
−1 | M ∗, y, δ2, φ1, σ2
ϵ, γ)
p(φ∗
−1 | M ∗)
×
p(φ−1 | M)
π(φ−1 | M, y, δ2, φ1, σ2
ϵ, γ).
Due to the reparameterization and the structure of the spherical prior when con-
ditioned on φ1, the density π(φ−1 | M, y, δ2, φ1, σ2
ϵ, γ) is the same as the prior density
for φ−1 given a particular model M. It can be shown easily that the prior density of
φ−1 is
π(φ−1 | M) = Γ(|M|−1
2
)
π
|M|−1
2
|M|−3
Y
n=1
sinn(φ|M|−1−n)
where |M| denotes the size of a model M.
242

Bibliography
Armagan, A., Clyde, M., and Dunson, D. (2011). Generalized beta mixtures of Gaus-
sians. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., and Weinberger,
K., editors, Advances in Neural Information Processing Systems 24, pp. 523–531.
Curran Associates, Inc.
Armagan, A., Dunson, D., and Lee, J. (2013). Generalized double Pareto shrinkage.
Statistica Sinica, 23:119–143.
Bae, K. and Mallick, B. (2004). Gene selection using a two-level hierarchical Bayesian
model. Bioinformatics, 20:3423–3430.
Barbieri, M. and Berger, J. (2004). Optimal predictive model selection. The Annals
of Statistics, 32:870–897.
Bartlett, M. (1957). A comment on D. V. Lindley’s statistical paradox. Biometrika,
44:533–534.
Bayarri, M., Berger, J., Forte, A., and Garc´ıa-Donato, G. (2012). Criteria for Bayesian
model choice with application to variable selection.
The Annals of Statistics,
40:1550–1577.
243

Bedrick, E., Christensen, R., and Johnson, W. (1996). A new perspective on priors
for generalized linear models.
Journal of the American Statistical Association,
91:1450–1460.
Berger, J. (1980). A robust generalized Bayes estimator and conﬁdence region for a
multivariate normal mean. The Annals of Statistics, 8:716–761.
Berger, J. (1985). Statistical decision theory and Bayesian analysis. Springer.
Berger, J. and Bernardo, J. On the development of reference priors. In Bernardo,
J., Berger, J., Dawid, A., and Smith, A., editors, Bayesian Statistics 4, pp. 35–60.
Oxford University Press, Oxford.
Berger, J., Bernardo, J., and Sun, D. (2009). The formal deﬁnition of reference priors.
The Annals of Statistics, 37:905–938.
Berger, J. and Molina, G. (2005). Posterior model probabilities via path-based pair-
wise priors. Statistica Neerlandica, 59:3–15.
Berger, J. and Pericchi, L. (1996a). The intrinsic Bayes factor for linear models. In
Bernardo, J., Berger, J., Dawid, A., and Smith, A., editors, Bayesian Statistics 5,
pp. 25–44. Oxford University Press, Oxford.
Berger, J. and Pericchi, L. (1996b). The intrinsic Bayes factor for model selection
and prediction. Journal of the American Statistical Association, 91:109–122.
Berger, J. and Pericchi, L. (2001). Objective bayesian methods for model selection:
Introduction and comparison.
In Model Selection, volume 38 of Lecture Notes-
Monograph Series, pp. 135–207. Institute of Mathematical Statistics, Beachwood,
OH.
244

Berger, J., Pericchi, L., and Varshavsky, J. (1998). Bayes factors and marginal distri-
butions in invariant situations. Sankhy¯a: The Indian Journal of Statistics, Series
A, 60:307–321.
Bernardo, J. (1979). Reference posterior distributions for Bayesian inference. Journal
of the Royal Statistical Society. Series B (Statistical Methodology), 41:113–147.
Bhattacharya, A., Pati, D., Pillai, N., and Dunson, D. (2014).
Dirichlet–Laplace
priors for optimal shrinkage. arXiv preprint arXiv:1401.5398.
Bottolo, L. and Richardson, S. (2010). Evolutionary stochastic search for Bayesian
model exploration. Bayesian Analysis, 5:583–618.
Breiman, L. and Friedman, J. (1985). Estimating optimal transformations for multiple
regression and correlation. Journal of the American Statistical Association, 80:580–
598.
Carvalho, C., Polson, N., and Scott, J. (2010). The horseshoe estimator for sparse
signals. Biometrika, 97:465–480.
Clyde, M. and George, E. (2004). Model uncertainty. Statistical science, 19:81–94.
Clyde, M., Ghosh, J., and Littman, M. (2011).
Bayesian adaptive sampling for
variable selection and model averaging. Journal of Computational and Graphical
Statistics, 20:80–101.
Clyde, M., Ghosh, J., and Littman, M. (2012). BAS: Bayesian model averaging using
Bayesian adaptive sampling. R package version 1.0.
245

Cui, W. and George, E. (2008). Empirical Bayes vs. fully Bayes variable selection.
Journal of Statistical Planning and Inference, 138:888–900.
Ehrlich, I. (1973). Participation in illegitimate activities: a theoretical and empirical
investigation. The Journal of Political Economy, 81:521–565.
Fan, J. and Li, R. (2001). Variable selection via nonconcave penalized likelihood and
its oracle properties. Journal of the American Statistical Association, 96:1348–1360.
Fernandez, C., Ley, E., and Steel, M. (2001). Benchmark priors for Bayesian model
averaging. Journal of Econometrics, 100:381–427.
Figueiredo, M. (2003). Adaptive sparseness for supervised learning. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 25:1150–1159.
Foster, D. and George, E. (1994). The risk inﬂation criterion for multiple regression.
The Annals of Statistics, 22:1947–1975.
Fu, W. (1998). Penalized regressions: the bridge versus the lasso. Journal of Com-
putational and Graphical Statistics, 7:397–416.
Garthwaite, P. and Dickey, J. (1988). Quantifying expert opinion in linear regression
problems. Journal of the Royal Statistical Society. Series B (Statistical Methodol-
ogy), 50:462–474.
Garthwaite, P. and Dickey, J. (1992). Elicitation of prior distributions for variable-
selection problems in regression. The Annals of Statistics, 20:1697–1719.
Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models.
Bayesian Analysis, 1:515–533.
246

George, E. and Foster, D. (2000). Calibration and empirical Bayes variable selection.
Biometrika, 87:731–747.
George, E. and McCulloch, R. (1993). Variable selection via Gibbs sampling. Journal
of the American Statistical Association, 88:881–889.
George, E. and McCulloch, R. (1997). Approaches for Bayesian variable selection.
Statistica Sinica, 7:339–373.
Golub, G. and Van Loan, C. (1996). Matrix Computations (3rd Ed.). Johns Hopkins
University Press, Baltimore, MD, USA.
Gordy, M. (1998). A generalization of generalized beta distributions. In Finance and
Economics Discussion Series. Board of Governors of the Federal Reserve System
(U.S.).
Griﬃn, J. and Brown, P. (2005). Alternative prior distributions for variable selection
with very many more variables than observations. Technical Report.
Griﬃn, J. and Brown, P. (2010). Inference with normal-gamma prior distributions in
regression problems. Bayesian Analysis, 5:171–188.
Griﬃn, J. and Brown, P. (2011). Bayesian hyper-lassos with non-convex penalization.
Australian & New Zealand Journal of Statistics, 53:423–442.
Griﬃn, J. and Brown, P. (2012). Structuring shrinkage: some correlated priors for
regression. Biometrika, 99:481–487.
Hans, C. (2009). Bayesian lasso regression. Biometrika, 96:835–845.
247

Hans, C. (2011).
Elastic net regression modeling with the orthant normal prior.
Journal of the American Statistical Association, 106:1383–1393.
Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p
regression. Journal of the American Statistical Association, 102:507–516.
Hansen, M. and Yu, B. (2001). Model selection and the principle of minimum de-
scription length. Journal of the American Statistical Association, 96:746–774.
Harrison, D. and Rubinfeld, D. (1978). Hedonic housing prices and the demand for
clean air. Journal of Environmental Economics and Management, 5:81–102.
Heaton, M. and Scott, J. (2010). Bayesian computation and the linear model. In Fron-
tiers of Statistical Decision Making and Bayesian Analysis, pp. 527–545. Springer,
New York.
Ishwaran, H. and Rao, J. (2005). Spike and slab variable selection: frequentist and
Bayesian strategies. Annals of Statistics, 33:730–773.
Jeﬀreys, H. (1961). Theory of Probability. Oxford University Press, New York.
Johnstone, I. and Silverman, B. (2004). Needles and straw in haystacks: empirical
Bayes estimates of possibly sparse sequences. The Annals of Statistics, 32:1594–
1649.
Kadane, J. and Wolfson, L. (1998). Experiences in elicitation. Journal of the Royal
Statistical Society. Series D (The Statistician), 47:3–19.
248

Karlin, S. and Rinott, Y. (1980). Classes of orderings of measures and related corre-
lation inequalities. I. Multivariate totally positive distributions. Journal of Multi-
variate Analysis, 10:467–498.
Kass, R. and Wasserman, L. (1995). A reference Bayesian test for nested hypotheses
and its relationship to the Schwarz criterion. Journal of the American Statistical
Association, 90:928–934.
Lehmann, E. and Romano, J. (2005). Testing Statistical Hypotheses. Springer.
Li, Q. and Lin, N. (2010). The Bayesian elastic net. Bayesian Analysis, 5:151–170.
Liang, F., Paulo, R., Molina, G., Clyde, M., and Berger, J. (2008). Mixtures of g priors
for Bayesian variable selection. Journal of the American Statistical Association,
103:410–423.
Lindley, D. (1957). A statistical paradox. Biometrika, 44:187–192.
Madigan, D. and York, J. (1995). Bayesian graphical models for discrete data. Inter-
national Statistical Review, 63:215–232.
Maruyama, Y. and George, E. (2011). Fully Bayes factors with a generalized g-prior.
The Annals of Statistics, 39:2740–2765.
Maruyama, Y. and Strawderman, W. (2010). Robust Bayesian variable selection with
sub-harmonic priors. arXiv preprint arXiv:1009.1926.
Mitchell, T. and Beauchamp, J. (1988). Bayesian variable selection in linear regres-
sion. Journal of the American Statistical Association, 83:1023–1032.
249

Nott, D. and Kohn, R. (2005). Adaptive sampling for Bayesian variable selection.
Biometrika, 92:747–763.
O’Hagan, A. (1995). Fractional Bayes factors for model comparison. Journal of the
Royal Statistical Society. Series B (Statistical Methodology), 57:99–138.
Park, T. and Casella, G. (2008).
The Bayesian lasso.
Journal of the American
Statistical Association, 103:681–686.
Polson, N. and Scott, J. (2009).
Alternative global-local shrinkage priors using
hypergeometric-beta mixtures. Technical Report.
Polson, N. and Scott, J. (2011). Shrink globally, act locally: sparse Bayesian reg-
ularization and prediction. In Bernardo, J., Bayarri, M., Berger, J., Dawid, A.,
Heckerman, D., Smith, A., and West, M., editors, Bayesian Statistics 9, pp. 501–
538. Oxford University Press, Oxford.
Polson, N. and Scott, J. (2012a). Local shrinkage rules, L´evy processes and regu-
larized regression. Journal of the Royal Statistical Society. Series B (Statistical
Methodology), 74:287–311.
Polson, N. and Scott, J. (2012b). On the half-Cauchy prior for a global scale param-
eter. Bayesian Analysis, 7:887–902.
Raftery, A., Madigan, D., and Hoeting, J. (1997). Bayesian model averaging for linear
regression models. Journal of the American Statistical Association, 92:179–191.
Scott, J. and Berger, J. (2006). An exploration of aspects of Bayesian multiple testing.
Journal of Statistical Planning and Inference, 136:2144–2162.
250

Scott, J. and Berger, J. (2010). Bayes and empirical-Bayes multiplicity adjustment
in the variable-selection problem. The Annals of Statistics, 38:2587–2619.
Scott, J. and Carvalho, C. (2008). Feature-inclusion stochastic search for Gaussian
graphical models. Journal of Computational and Graphical Statistics, 17:790–808.
Strawderman, W. (1971). Proper Bayes minimax estimators of the multivariate nor-
mal mean. The Annals of Mathematical Statistics, 42:385–388.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of
the Royal Statistical Society. Series B (Statistical Methodology), 58:267–288.
Tipping, M. (2001). Sparse Bayesian learning and the relevance vector machine. The
Journal of Machine Learning Research, 1:211–244.
West, M. (1987). On scale mixtures of normal distributions. Biometrika, 74:646–648.
West, M. (2003). Bayesian factor regression models in the “large p, small n” paradigm.
In Bernardo, J., Bayarri, M., Berger, J., Dawid, A., Heckerman, D., Smith, A., and
West, M., editors, Bayesian Statistics 7, pp. 733–742. Oxford University Press,
Oxford.
Yu, Q., MacEachern, S., and Peruggia, M. (2011). Bayesian synthesis: combining
subjective analyses, with an application to ozone data.
The Annals of Applied
Statistics, 5:1678–1698.
Zellner, A. (1986). On assessing prior distributions and Bayesian regression analysis
with g-prior distributions. In Goel, P. and Zellner, A., editors, Bayesian inference
and decision techniques: Essays in Honor of Bruno De Finetti, pp. 233–243. North-
Holland, Amsterdam.
251

Zellner, A. and Siow, A. (1980). Posterior odds ratios for selected regression hypothe-
ses. In Bernardo, J., DeGroot, M., Lindley, D., and Smith, A., editors, Bayesian
Statistics 1, pp. 585–603. University Press, Valencia.
Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic
net. Journal of the Royal Statistical Society. Series B (Statistical Methodology),
67:301–320.
252

