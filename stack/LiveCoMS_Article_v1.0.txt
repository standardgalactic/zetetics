Enhanced Sampling Methods for
Molecular Dynamics Simulations
[Article v1.0]
Jérôme Hénin1*, Tony Lelièvre2*, Michael R. Shirts3*, Omar Valsson4,5*, Lucie
Delemotte6*
1Université Paris Cité, Laboratoire de Biochimie Théorique, CNRS UPR 9080, Paris,
France; 2CERMICS, Ecole des Ponts ParisTech, INRIA, Marne-la-Vallée, France;
3Department of Chemical and Biological Engineering, University of Colorado Boulder,
Boulder, CO, USA, 80309; 4University of North Texas, Department of Chemistry, Denton,
TX, USA; 5Max Planck Institute for Polymer Research, Mainz, Germany; 6KTH Royal
Institute of Technology, Science for Life Laboratory, Stockholm, Sweden
This LiveCoMS document is
maintained online on
GitHub at https://github.
com/jhenin/Methods-for-
enhanced-sampling-and-
free-energy-calculations; to
provide feedback,
suggestions, or help
improve it, please visit the
GitHub repository and
participate via the issue
tracker.
This version dated
December 14, 2022
Abstract Enhanced sampling algorithms have emerged as powerful methods to extend the utility
of molecular dynamics simulations and allow the sampling of larger portions of the conﬁguration
space of complex systems in a given amount of simulation time. This review aims to present the
unifying principles of and differences between many of the computational methods currently used
for enhanced sampling in molecular simulations of biomolecules, soft matter and molecular crys-
tals. In fact, despite the apparent abundance and divergence of such methods, the principles at
their core can be boiled down to a relatively limited number of statistical and physical concepts. To
enable comparisons, the various methods are introduced using similar terminology and notation.
We then illustrate in which ways many different methods combine features of a relatively small
number of the same enhanced sampling concepts. This review is intended for scientists with an
understanding of the basics of molecular dynamics simulations and statistical physics who want
a deeper understanding of the ideas that underlie various enhanced sampling methods and the
relationships between them. This living review is intended to be updated to continue to reﬂect the
wealth of sampling methods as they continue to emerge in the literature.
*For correspondence:
jerome.henin@cnrs.fr (JH); tony.lelievre@enpc.fr (TL); michael.shirts@colorado.edu (MRS); omar.valsson@unt.
edu (OV); lucied@kth.se (LD)
Received: 10 February 2022
Accepted: 26 November 2022
1 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Contents
1
Introduction
3
1.1
Scope . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
An attempt at classiﬁcation . . . . . . . . . . . .
5
1.3
Can enhanced sampling methods be com-
pared critically?
. . . . . . . . . . . . . . . . . .
6
2
Useful notions and notations
6
2.1
Basic notations . . . . . . . . . . . . . . . . . . .
6
2.2
Glossary of essential notions . . . . . . . . . . .
6
Conﬁguration or microstate . . . . . . .
6
Molecular dynamics simulation . . . . .
7
Statistical ensembles from molecular
dynamics . . . . . . . . . . . . .
7
Distribution
. . . . . . . . . . . . . . . .
8
Macrostate . . . . . . . . . . . . . . . . .
8
Density of states . . . . . . . . . . . . . .
9
Free energy
. . . . . . . . . . . . . . . .
9
Free energy estimator
. . . . . . . . . .
9
Reduced quantities for homogeneous
treatment of different ensem-
bles . . . . . . . . . . . . . . . .
9
Collective variable (CV) . . . . . . . . . .
9
Dimensionality reduction . . . . . . . . .
9
Alchemical transformation . . . . . . . .
9
Free energy proﬁle / landscape / surface
(FES)
. . . . . . . . . . . . . . .
10
Potential of mean force (PMF) . . . . . .
10
Multimodal distribution
. . . . . . . . .
11
Ensemble average . . . . . . . . . . . . .
11
Ergodic dynamics . . . . . . . . . . . . .
11
Metastability and metastable states
. .
11
Biasing and biased energy . . . . . . . .
11
Biased conﬁgurational distribution . . .
12
Importance sampling . . . . . . . . . . .
12
Reweighting
. . . . . . . . . . . . . . . .
12
Biased CV distribution
. . . . . . . . . .
13
Target distribution
. . . . . . . . . . . .
13
Replicas . . . . . . . . . . . . . . . . . . .
13
Generalized ensemble . . . . . . . . . .
13
Expanded ensemble
. . . . . . . . . . .
13
Extended
Lagrangian
dynamics,
λ
dynamics . . . . . . . . . . . . .
13
Metropolis Monte Carlo
. . . . . . . . .
13
Gibbs sampler . . . . . . . . . . . . . . .
14
Detailed balance and balance . . . . . .
14
Temperature-based sampling . . . . . .
14
Replica exchange
. . . . . . . . . . . . .
15
Adiabatic dynamics . . . . . . . . . . . .
15
Driven simulations
. . . . . . . . . . . .
15
Out-of-equilibrium system . . . . . . . .
15
Out-of-equilibrium method
. . . . . . .
15
Seeding . . . . . . . . . . . . . . . . . . .
15
Adaptive method
. . . . . . . . . . . . .
15
Free energy perturbation (FEP)
. . . . .
15
3
Free energy estimators
15
3.1
Directly measured ratios . . . . . . . . . . . . .
15
3.2
Estimating free energies from the transition
count matrix . . . . . . . . . . . . . . . . . . . .
16
3.3
Thermodynamic integration (TI) . . . . . . . . .
16
3.3.1
TI along an alchemical parameter . . . .
17
3.3.2
TI along a collective variable . . . . . . .
17
3.3.3
Signiﬁcance of the Jacobian term . . . .
17
3.3.4
Comparison between alchemical and
conﬁgurational TI . . . . . . . . . . . . .
17
3.4
Exponential averaging
. . . . . . . . . . . . . .
17
3.5
Bennett’s acceptance ratio (BAR)
. . . . . . . .
18
3.6
Multistate Bennett acceptance ratio (MBAR) . .
18
3.7
Weighted histogram analysis method (WHAM)
19
4
Out-of-equilibrium / driven methods
19
5
Localization methods
20
6
Non-adaptive biasing potential methods
20
7
Adaptive bias simulations
20
7.1
Adaptation and the adaptation rate . . . . . . .
21
7.2
Adaptive biasing potential (ABP) methods . . .
21
7.2.1
Metadynamics . . . . . . . . . . . . . . .
21
Well-tempered metadynamics . . . . . .
22
Obtaining the free energy surface
. . .
22
Conventional metadynamics . . . . . . .
24
Multiple walkers metadynamics . . . . .
24
Well-tempered ensemble
. . . . . . . .
24
Parallel-bias metadynamics . . . . . . .
25
Infrequent metadynamics . . . . . . . .
25
Adaptive Gaussian metadynamics
. . .
25
Other variants of metadynamics
. . . .
25
Public implementations of metadynamics 25
7.2.2
Variationally enhanced sampling . . . .
25
7.2.3
Wang-Landau and other adaptive bias-
ing potential methods using the poten-
tial energy as a collective variable . . . .
27
7.3
Adaptive biasing force (ABF) . . . . . . . . . . .
27
7.3.1
Standard ABF
. . . . . . . . . . . . . . .
28
7.3.2
Multiple-walker ABF . . . . . . . . . . . .
28
7.3.3
Extended-system ABF . . . . . . . . . . .
28
7.3.4
Other ways to calculate the biasing force
29
7.3.5
Estimating convergence and diagnosing
issues in ABF simulations . . . . . . . . .
29
2 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

7.3.6
Public implementations of ABF
. . . . .
29
8
Generalized ensemble and replica exchange meth-
ods
29
8.1
Replica exchange
. . . . . . . . . . . . . . . . .
30
8.1.1
Estimating convergence and diagnosing
issues in replica exchange simulations .
32
8.1.2
Public
implementations
of
replica
exchange methods . . . . . . . . . . . .
32
8.2
Expanded Ensemble
. . . . . . . . . . . . . . .
32
8.2.1
Methods updating biases one state at a
time . . . . . . . . . . . . . . . . . . . . .
34
Wang-Landau updating . . . . . . . . . .
34
1/t modiﬁcations to Wang-Landau
. . .
35
8.2.2
Multiple state reweighting methods
. .
35
Transition matrix approaches . . . . . .
35
Asymptotically optimal weights . . . . .
35
8.2.3
Estimating the biasing weights near the
beginning of the simulation . . . . . . .
36
8.2.4
Convergence and diagnosing problems
in expanded ensemble simulations . . .
36
8.2.5
Public implementations of expanded
ensemble methods . . . . . . . . . . . .
36
9
Adaptive seeding methods
36
9.1
Adaptive sampling . . . . . . . . . . . . . . . . .
36
9.2
Weighted
ensemble
simulations
-
split-
ting/replication strategies
. . . . . . . . . . . .
38
10 Selective acceleration methods
38
11 Hybrid methods
39
11.1 Combination of replica exchange and external
biasing potential methods . . . . . . . . . . . .
40
11.1.1 Replica exchange umbrella sampling . .
40
11.1.2 Parallel-tempering metadynamics
. . .
40
11.1.3 Bias-exchange metadynamics . . . . . .
40
11.1.4 Parallel-tempering in the well-tempered
ensemble . . . . . . . . . . . . . . . . . .
40
11.1.5 Replica exchange with collective vari-
able tempering
. . . . . . . . . . . . . .
41
11.2 Combinations of metadynamics and other en-
hanced sampling methods . . . . . . . . . . . .
41
11.3 Combinations of metadynamics and structural
ensemble determination methods
. . . . . . .
42
11.4 Combinations of ABF and other enhanced sam-
pling methods . . . . . . . . . . . . . . . . . . .
42
12 Software implementations
42
A Abbreviations and acronyms
44
1
Introduction
Molecular dynamics (MD) simulations are nowadays rou-
tinely employed to gain insights into the atomistic-level
behavior of molecular systems. They are often used in com-
bination with experiments, usually to provide the atomistic
counterpart to a more macroscopic description afforded by
other techniques. MD simulations rely on the numerical and
iterative solution of the equations of motion, using small
timesteps for integration, on the order of femtoseconds.
While they are useful to monitor the time evolution of a
system, for instance, in response to a perturbation, they are
also very often used as an eﬃcient sampling tool to recover
statistical ensembles, much in the same way Monte Carlo
(MC) based methods of conﬁgurational sampling are.
In this review, we assume that we have a MD simulation
algorithm that samples a single speciﬁed ensemble (constant
number of particles, constant volume or pressure, constant
temperature—NVT or NPT, respectively) 1. A large number of
algorithms have been proposed to achieve this type of con-
ﬁgurational sampling and the algorithm choice does not im-
pact what is covered in this review. Two important criteria
must be satisﬁed, however: that this algorithm samples the
distribution of choice, and that the sampling is ergodic, i.e. it
will eventually cover the entire conﬁguration space. However,
the samples are allowed to be, and almost always are, corre-
lated, and the time needed to approach this ergodic behav-
ior could effectively be inﬁnite, or at least beyond the time
scale of any reasonable computer simulation. We note that
many methods described here can also be applied if Monte
Carlo algorithms are used instead of MD simulations to sam-
ple conformation space, but focus on the use of molecular
dynamics in this review.
MD simulations are generally considered to suffer from
three main limitations:
• the accuracy of the interaction model or force ﬁeld (MM,
QM/MM, semi-empirical, ab initio...) may not enable the
desired insights.
• the
simulation
output
(the
trajectory)
is
high-
dimensional, noisy and can be diﬃcult to interpret
and describe using a meaningful and relevant lower-
dimension level of description.
• given the limitation on the timestep, which needs to be
small enough for integration to be stable and accurate,
the timescales that can be sampled are often shorter
than the process of interest to the researcher.
This review focuses on describing the numerous meth-
1Many of the methods work for constant chemical potential, but as such
simulations cannot be carried out in standard MD simulations because of
changing particle numbers, and dedicated simulations are challenging and
rare, we will not explicitly address the application of the methods to these
systems.
3 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

ods that have been put forth to address the third issue,
broadly referred to as “enhanced sampling MD simulation”
techniques.
We note that of course other relatively com-
prehensive reviews have been written on this subject; see
also [1–3] for earlier reviews surveying the ﬁeld.
We do note that the second issue, of ﬁnding a low-
dimensional projection for the interpretation of MD sim-
ulations, is directly related to several of the algorithms
presented in this review, grouped under the umbrella term
“collective variable (CV)-based methods.” Indeed, the conﬁg-
urational ensemble of systems of interest is generally very
peaked, featuring several metastable and well-deﬁned states
with high probability, while the regions between these states
have probabilities close to zero in the full high-dimensional
Cartesian space. This explains why schemes that reduce the
dimensionality of the space sampled by projecting it onto
a lower-dimensional surface can be successful if they ac-
celerate sampling along a useful CV connecting metastable
states. If the process of interest is a transition between two
states, then the ideal CV accelerating transitions between
these states is the committor function, which describes the
progress of the transition between the two states, and the
transition state between the two endpoints is the region
where the committor value is around 1
2 [4–8]. In such cases,
CV is synonymous with reaction coordinate. However, ﬁnd-
ing CVs that approximate the committor function is a very
challenging task and rarely done in practice, though several
recent methods show promise in achieving this [9–14]. In-
stead, CVs are generally chosen using chemical and physical
intuition, or by using methods that aim to automatically
extract CVs from simulations.
While ﬁnding these “good”
CVs is crucial for the success of CV-based methods, we will
only brieﬂy touch on the various methods that exist for
identifying such CVs. The reader is referred to [15–17] for a
more extensive discussion of this issue.
1.1
Scope
The concept of accelerated sampling is so broad that we
must make some decisions as to what scope of approaches
to cover in a coherent review.
Some enhanced sampling
schemes are purely exploratory, i.e. their aim is to discover
uncharted regions of the conﬁguration space eﬃciently;
however, they only produce semi-quantitative estimates
of probability distributions.
In contrast, many schemes
not only accelerate sampling through conﬁgurations space,
but enable the estimation of probability distributions and
free energies from the sampled space. Methods generally
fall onto an exploration / free energy estimation trade-off
continuum. Some methods, such as metadynamics, were
initially proposed as simply exploration schemes for conﬁgu-
ration space, but were later reﬁned and shown to be useful
to calculate probability densities as well. Others started off
from their derivation being grounded in the estimation of
free energies. An entire class of methods remains exclusively
useful to broadly survey the conﬁguration space, and learn
to do that in optimal ways using adaptive schemes [18]. In
this review, we focus on algorithms that recover original
statistical ensembles and free energy landscapes. Note that
exploratory methods can nevertheless be useful to some
of these schemes, as they offer initial conﬁgurations from
which to start simulations.
Another type of information that can be valuable to the
scientist is the evolution of the system over time, which
gives access to the rates of transition between states. It is
therefore important to note that many enhanced sampling
schemes do not preserve the kinetics of the system, and are
therefore primarily useful to recover equilibrium probability
distributions. In fact, the most eﬃcient sampling methods
arguably alter the dynamics as much as possible while
preserving thermodynamics.
However, some methods of
these methods still do preserve kinetic information, or at
least allow kinetic information to be recovered, and we will
still generally note which methods have this property.
New algorithms are constantly proposed to increase ex-
ploration, and/or reduce the variance of the estimates on the
conformational landscape. These usually combine several of
the original strategies in advantageous ways. However, it is
often diﬃcult to compare these schemes, or even to simply
understand their similarities and differences, due to the pro-
liferation of acronyms, and the use of inhomogeneous and
diverse notation schemes. One of the main aims of this re-
view is to thus list and describe the basic methods that are
based on leveraging a single statistical or physical principle,
using a uniﬁed framework we can then use to orient the MD
simulation practitioner in the forest of available schemes.
With these facts in mind, we can better summarize the
scope of this review.
• We focus on methods of interest to chemical and biolog-
ical systems, soft matter systems, and other molecular
systems amenable to molecular dynamics simulations.
• We describe methods for accelerated sampling of a
given probability distribution at equilibrium.
We do
not review purely exploratory methods that cannot be
used to recover equilibrium statistics.
• We do not describe methods that are mainly used to
characterize kinetic rates. See instead Refs [19–24] for
reviews of methods to estimate kinetics from enhanced
sampling schemes.
• We do not exhaustively review path sampling algo-
rithms, though some of the pathway-based methods
that allow to recover statistical ensembles are men-
4 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

tioned.
See instead Refs [25–28] for reviews of
methods for ﬁnding and sampling pathways linking
states.
• We do not review methods that aim to extract collec-
tive variables from simulations. See instead Refs [15–
17] for recent reviews of the topic. When relevant, we
assume that any necessary collective variables are al-
ready known and speciﬁed.
Given the large scope of this project, the current ver-
sion of this review is necessarily incomplete.
Because of
LiveCoMS’ unique updating process, we look forward to
including reader suggestions and contributions in later
versions of this perpetually updated review. Such updates
will include major methods that were unintentionally missed,
better ways to explain methods, combinations of methods
that escaped our notice, or additional ways or organizing and
structuring the classiﬁcation of theoretical ideas presented
here. We encourage contributors to post their suggestions
for improvements as issues on the GitHub repository at
https://github.com/jhenin/Methods-for-enhanced-sampling-
and-free-energy-calculations.
1.2
An attempt at classiﬁcation
Several mathematical and physical principles have been
recognized as useful to enhance the sampling and converge
the probability distribution with a low variance.
From
probability theory, several strategies have been borrowed:
importance sampling (carrying out a biased simulation to
reduce the variance of the estimated property), localization
(restricting the system to the sampling of a speciﬁc region
of space), and conditioning (using conditional probabilities),
for example. Originating more from the physics community,
several levers have been proposed: sampling at higher tem-
peratures, adding external forces or potentials, driving an
adiabatically decoupled degree of freedom or expanding the
ensemble considered, with or without exchanges between
systems sampling a different but related conﬁguration
space.
Crucially, most methods hinge on a relatively small num-
ber of statistical or physical ideas or principles. Here, we of-
fer one possible way to organize these methods. We start by
explicitly listing these methodological ingredients, and clas-
sify the methods according to dichotomies, asking which of
the different ingredients is leveraged in each approach (dia-
monds in Figure 1).
The decision tree in the ﬁgure presents one of the
possible alternatives by answering these different questions
in a semi-arbitrary order, but attempting to list the ques-
tions from the most fundamental to the most specialized
ones. The ordering of the questions inherently contains a
level of subjectivity and we recognize that ordering these
dichotomies in different ways can be equally reasonable,
and that other representations may be useful (See Figure 2
for an early attempt at a Venn Diagram).
There are certainly other binary methods of classiﬁcation
that could be used which are not immediately obvious in the
decisions tree in Figure 1. For example, there are two main
ways to impart additional structure to a system in a way that
can that can aid in enhanced sampling:
• One of these ways is to create partitions of conﬁg-
uration space that are completely non-overlapping;
this can be done by creating ”level sets” of a collective
variable, which all have the same value of some func-
tion of the coordinates, such as all the conﬁgurations
with the same energy, or same distance between two
speciﬁed particles, or same dihedral torsion between
four particles. Collective variables generally should be
deﬁned on a contiguous region so that all values of
interest can be visited.
• The second main way to add structure to a system is
to create new ensembles that all share the same con-
ﬁgurations, but for which their probabilities in each en-
semble are different. For example, each separate en-
semble could have a different temperature, meaning
that conﬁgurations of different energies will have dif-
ferent probabilities in each ensemble. Alternatively, dif-
ferent ensembles could have a harmonic bias centered
around a different point in CV space for each ensemble,
creating ensembles that are centered around different
values of the CV. Generally, to be useful, the different
ensembles must be interconnected by sharing at least
some conﬁgurations that have non-negligible probabil-
ities in multiple ensembles. Each ensemble need not
overlap with all other ensembles, but there must be a
interconnected network such that one can move step-
wise between all the ensembles.
This division into non-overlapping (partitioning) and overlap-
ping structure is a fundamental one, because the algorithms
used to calculate free energies and perform sampling,
are different in the two cases.
This is because different
algorithms are needed depending on whether or not mi-
crostates have deﬁned probabilities in multiple ensembles
or each microstate only belongs to one ensemble [29, 30].
For example, when moving between ensemble members
in the overlapping case, one generally uses Monte Carlo
methods to generate moves to neighboring states. In the
non-overlapping case, one usually uses standard dynamics
to change between values of collective variables, but with
the effect of any biases as a function of collective variables
back-calculated to determine the resulting forces on the
5 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

system. This particular division of methods is one that is not
directly used in our classiﬁcation, but shows up repeatedly
in the descriptions of the algorithm.
We have thus indi-
cated this division using a green (partitioning) and purple
(overlapping) coloring scheme in Figure 1.
Another complementary classiﬁcation might to be con-
sider the probability distribution sampled at convergence by
the enhanced sampling methods (sometimes called target
distribution), instead of the speciﬁcs of how the enhanced
sampling methods achieve this in practice [31].
The next sections provide the background information
needed to understand the scheme presented in Figure 1,
starting with the notation used throughout the paper and
a glossary in Section 2, a description of the free energy
estimators that are used in various enhanced sampling
schemes and will be referred to later in the text in Section 3,
and followed by the more detailed description of the various
families of enhanced sampling methods that emerge from
our classiﬁcation. We then list a selection of hybrid schemes
that combine different principles. Finally, we summarize the
software packages (MD simulation codes and libraries) that
are publicly and openly available to run these different types
of enhanced sampling simulations.
1.3
Can enhanced sampling methods be
compared critically?
The eﬃciency of an enhanced sampling method depends on:
• the application;
• the choice of parameters—and the optimal parameters
are themselves application dependent;
• the expertise of the user.
Comparing the eﬃciency of these methods requires
deﬁning a benchmark for which the results are known to a
high accuracy and precision, and then attempting to study
this benchmark by using each method “fairly,” that is, either
using the optimal parameters for the particular application,
or the best parameters that a typical user will be able to set
in practice. Such studies [32, 33] are challenging to carry
about, and usually can only cover a few methods at a time.
A fundamental issue is that the ground truth is seldom
known beyond simple systems such as alanine dipeptide or
a few fast-folding miniproteins, and there is no guarantee
that methods that work well for those systems will work
well on more complex systems.
In fact, the features of
the conformational landscape might be very different and
make methods that work well on model systems perform
particularly badly on “real” systems. In addition, sometimes
a method may be less eﬃcient than another when used
optimally, but more robust to non-optimal circumstances. A
method may lead quickly to an approximate or qualitative
result and go no further, whereas another may guarantee a
precise result but require much more resources or take an
uncertain time to converge.
For the current time, we have found that it would not
be feasible to identify optimal methods for all applications
within this review, as optimality indeed depends on the type
of problem. Instead, we hope that presenting methods in a
uniﬁed way can help guide the practitioner in their choice of
enhanced sampling scheme when tackling their problem of
interest.
2
Useful notions and notations
One of the aims of this review is to use consistent notations
to enable the reader to compare different methods and
ﬁnd similarities and differences across enhanced sampling
schemes.
We introduce here the notation we will use
throughout the paper.
Note that given the existence of
different notations in the literature, we have chosen the
following one, while recognizing the validity of others. When
especially crucial to understand the cited literature, we
sometimes explicitly mention an alternative notation in
following sections.
2.1
Basic notations
• Cartesian coordinates of atoms (or coarse-grain parti-
cles) x ∈R3N and momenta p ∈R3N, where N is the
number of particles.
• Hamiltonian H(x, p) = U(x) + K(p), which is a sum of the
potential energy function denoted by U(x), deﬁned by a
classical molecular mechanics force ﬁeld, various levels
of electronic structure theory in an ab initio dynamics
framework, or a QM/MM hybrid and the kinetic energy
K(p).
• Force on particles F(x) = –∇xU(x), where ∇x is the gra-
dient with respect to x.
• Absolute temperature T and inverse temperature β =
(kBT)–1.
• Extended (auxiliary) variable λ. This auxiliary variable
can be a thermodynamic parameter, such as the tem-
perature T or the pressure P, or a parameter of the en-
ergy function Uλ(x). The auxiliary variable can have a
ﬁxed value per simulation, can follow a pre-determined
schedule, or can obey some dynamical equation of mo-
tion. It can have multiple dimensions, which we repre-
sent by the bold-faced vector λ.
2.2
Glossary of essential notions
Conﬁguration or microstate
A single spatial arrangement of particles, represented by co-
ordinates x. The set of possible conﬁgurations deﬁnes the
6 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Is the original 
conﬁgurational 
distribution 
preserved?
yes
Does one 
use transitions 
to other 
ensembles?
yes
yes
Are the 
systems 
simulated 
in parallel?
Replica 
exchange
no
Expanded 
ensemble
no
Selective 
acceleration 
methods
no
Does 
the simulation
converge to an 
equilibrium 
ensemble?
yes
Out-of-
equilibrium/
Driven 
methods
no
yes
Is sampling 
enhanced by 
specifying 
starting/ending 
coordinates?
no
yes
Are the
simulations
localized?
Localization 
methods
partitioning (non-overlapping)
overlapping
no
Adaptive 
seeding
Importance 
sampling
yes
Is the bias 
learned along the 
simulation?
no
Non-adaptive
biasing potential
methods
Biased
simulations
Generalized
ensemble
section 10
section 8.2
section 4
section 9
section 5
section 7
section 6
section 8.1
Adaptive
bias simulation
methods
Figure 1. An attempt at classifying enhanced sampling schemes, answering yes/no questions that delineate various strategies based on phys-
ical or statistical principles (black diamonds). The sorting algorithm results in eight different classes of methods (boxes). These methods can
further be sorted according to other classiﬁcation schemes. An example is given by the division of methods into overlapping and partitioning
schemes, highlighted by the coloring of the boxes. The section of the review describing the family of methods is shown in blue below the
corresponding box. Labels in orange refer to families of methods that can be grouped under an umbrella term.
conﬁguration space Γ. Conﬁguration space augmented with
the momenta variables is called the phase space, and there-
fore has twice the dimensionality. In many methods and cal-
culations, we can take advantage of the fact that the momen-
tum distribution obeys the analytical Maxwell-Boltzmann dis-
tribution at equilibrium to compute this quantity analytical
rather than using sampling methods. In many cases this con-
tribution will be the same at both endpoints, and thus cancels
out of the overall calculation. In some cases, the deﬁnition of
conﬁguration x also includes the cell vectors of a periodic sys-
tem (for example, in an isobaric ensemble); this deﬁnition of
x is used where applicable.
We are using the original statistical mechanical deﬁnition
of a microstate; we note that in the Markov State Modeling
(MSM) literature, a microstate can also refer to an ensemble
of conﬁgurations grouped together according to one or a set
of order parameters, which is not intended here.
Molecular dynamics simulation
A process that generates a trajectory, or sequence of conﬁgu-
rations xt. The best-known classical dynamics is Hamiltonian
dynamics:
(
dx = M–1p dt
dp = –∇xU(x) dt,
(1)
where M is the diagonal mass matrix. A simplistic, discrete-
time version of the above with time step δt is:
(
δx = M–1p δt
δp = –∇xU(x) δt.
(2)
In practice, however, trajectories are often generated numer-
ically using Verlet-style integrators.
Statistical ensembles from molecular dynamics
Hamiltonian dynamics conserves mechanical energy, and
can be used to sample from the microcanonical (constant
7 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Figure 2. Early attempt at listing and classifying existing enhanced
sampling schemes.
number of particles N, constant volume V and constant
energy E, NVE) ensemble under an ergodicity assumption.
Isothermal dynamics includes modiﬁcations that function
as a thermostat, simulating the equilibrium with an external
temperature bath at a target temperature T. As a result, as
long as the simulation is at or near equilibrium, its tempera-
ture ﬂuctuates around T, and it samples the canonical ensem-
ble (NVT). An example of isothermal dynamics is Langevin dy-
namics:
(
dx = M–1p dt
dp =
 –∇xU(x) – γp

dt +
q
2γM
β
dWt,
(3)
where γ is a friction coeﬃcient and Wt is a Brownian motion
in dimension 3N.
To make the difference between Hamiltonian dynamics
and Langevin dynamics more intuitive, consider this discrete-
time approximation of Langevin dynamics:



δx = M–1p δt
δp =

–∇xU(x) – γp +
q
2γM
βδt G

δt,
(4)
where G is a Gaussian-distributed stochastic 3N-vector of
zero mean and variance 1.
Note that in practice, more
sophisticated discrete Langevin integration schemes are
used, which bring much better accuracy, stability, and
performance [34, 35]. Still, comparing the relatively simple
Equations 2 and 4 shows that Langevin dynamics can be
interpreted intuitively as similar to Hamiltonian dynamics,
but including a modiﬁed force with added friction and
stochastic collision terms. When γ is zero, it reduces exactly
to Hamiltonian dynamics. Langevin dynamics will be used
as a basic example in later sections of this review.
Distribution
The Boltzmann distribution (which characterizes the canon-
ical ensemble) in phase space has the following probability
density:
µ(x, p) = 1
Qe–β(U(x)+K(p)),
(5)
where Q =
Z
e–β(U(x)+K(p))dxdp is the normalization factor,
known as partition function.
The physical meaning of µ is a probability per unit volume
of (x, p) space. The probability of a region of phase space Σ
is:
P(Σ) =
Z
Σ
µ(x, p) dxdp.
(6)
In most cases, the energy is a sum of a potential term that
depends only on positions and a kinetic term that depends
only on momenta, as written in Equation 5. Then the mo-
menta are statistically independent from the system conﬁg-
uration, hence their distribution is that of the ideal gas and
does not bear signiﬁcant information on any speciﬁc system.
This leads to a simple expression for the conﬁgurational dis-
tribution, where the momenta and kinetic energy do not ap-
pear:
ν(x) =
Z
µ(x, p) dp = 1
Z e–βU(x),
(7)
where Z is the conﬁgurational partition function, Z
=
R
e–βU(x) dx . Sometimes, it may also be useful to deﬁne an
unnormalized version of the conﬁgurational distribution,
q(x), such that ν(x) =
1
Z q(x). There exist equivalent deﬁni-
tions of distributions for the isothermal-isobaric ensemble
(NPT), which can be found in most statistical mechanics
books [8, 36].
Note that there are other notation conventions: in some
texts and papers, Q denotes the conﬁgurational partition
function and Z denotes the conﬁgurational and momenta
partition function.
Macrostate
Macrostates are experimentally distinguishable or measur-
able states of a system.
They can be described formally
either in terms of the thermodynamic state variables (E, T,
P, V, or parameters of the Hamiltonian) or by specifying
speciﬁc regions of conﬁguration space (that is, disjoint sets
of microstates). A macrostate, besides being just a collection
of microstates, also speciﬁes a probability associated with
each microstate that is contained in the microstate.
The
term “thermodynamic state” is often used synonymously
with macrostate, as the macrostates that we are most gen-
erally interested in studying with molecular simulation are
8 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

macrostates that are completely deﬁned by the speciﬁcation
of the macroscopic thermodynamic variables.
Density of states
The density of states Ω(E) is deﬁned as the number of states
in a system that have a speciﬁc total energy E. It can be math-
ematically expressed as
Ω(E) =
Z
δ(E – H(x, p)) dxdp,
(8)
where δ is a Dirac delta function that is zero everywhere ex-
cept where E = H(x, p). The entropy S as a function of E is
then simply S(E) = kB ln Ω(E). Additionally, the conﬁgurational
density of states is deﬁned as the number of states that have
a speciﬁc potential energy U and is computed as
Ω(U) =
Z
δ(U – U(x)) dx,
(9)
and the conﬁgurational entropy S as a function of U is S(U) =
kB ln Ω(U).
Free energy
In the canonical ensemble, the Helmholtz free energy F is a
property of a macrostate of a system, and is proportional to
the logarithm of its partition function, which measures its sta-
tistical weight compared to other macrostates:
F ∝–β–1 ln ZΣ = –β–1 ln
Z
Σ
e–βU(x) dx,
(10)
where the integration is done over a subset Σ of conﬁgu-
ration space corresponding to the macrostate. F thus de-
pends on Σ, U(x), and β, although this dependency is often
not stated explicitly, but implied by the context.
When using a classical energy function, F is only deﬁned
up to an arbitrary additive constant. In practice, this is not
a limitation, as quantities of measurable physical interest in-
volve only free energy differences between two macrostates,
rather than absolute free energies. If two macrostates A and
B can be distinguished experimentally, the ratio of the time
they spend in each system (PA = ZA/Z and PB = ZB/Z) is an
experimental observable, and the free energy difference be-
tween the two states is:
∆FA,B = FA – FB
= –β–1 ln ZA
ZB
= –β–1 ln PA
PB
.
(11)
The Helmholtz free energy is sometimes notated A in the
literature. In this review, we will use F for Helmholtz free en-
ergy, and the symbol A will be used for the free energy sur-
face. Gibbs free energy G is the equivalent quantity in the
isothermal-isobaric ensemble.
Free energy estimator
An expression or algorithm that takes simulation data and
estimates a numerical value for free energies or their differ-
ences. See Section 3 for a list and description of useful free
energy estimators.
Reduced quantities for homogeneous treatment of
different ensembles
We deﬁne the reduced energy function ui(x) for state i to be
ui(x) = βi(Ui(x) + piV(x)),
(12)
where the pressure-volume term piV(x) is only included in the
case of a constant pressure ensemble. Other terms, such as
chemical potentials, may be added to generalize to other en-
sembles. For each state i, βi is the inverse temperature, Ui(x)
the potential energy function (which may include external bi-
asing potentials), pi the external pressure. This formalism
allows a very large number of different situations to be de-
scribed by the same mathematics.
The reduced free energy f is deﬁned as f = βF for the
canonical ensemble, or f = βG for the isothermal-isobaric
ensemble. Then all Boltzmann-like distributions for the ther-
modynamic ensembles mentioned above are given in their
un-normalized form as q(x) = e–u(x) and normalized form as
ν(x) = ef–u(x).
Collective variable (CV)
A function ξ mapping the full 3N-dimension conﬁgurations
x to a lower-dimensional representation z (sometimes
denoted as s in the literature):
z = ξ(x).
(13)
In the literature, the letter ξ is sometimes used for both the
function and the variable. The same goes for z (and s).
The multi-dimensional case ξ = (ξ1, ξ2, . . . , ξd) can be de-
scribed either as a single vector CV or a family of scalar CVs:
z = (z1, z2, . . . , zd) = ξ(x),
(14)
where d is the number of scalar collective variables (i.e., the
dimension of the CV space), with d ≪3N.
Dimensionality reduction
The process of ﬁnding functions ξ mapping high-dimensional
x to low-dimensional z, such that z = ξ(x).
Alchemical transformation
Non-physical transition between Hamiltonians representing
different molecular systems are characterized as “alchemi-
cal.” Typically the change consists in either transforming a
molecule into another one, or decoupling a molecule from its
environment. Alchemical transformations are often used to
estimate free energies of binding or solvation, as they create
9 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

a continuous pathway of intermediate ensembles between
two end states of physical interest.
Free energy proﬁle / landscape / surface (FES)
While free energy can be expressed as the logarithm of a par-
tition function, a free energy surface (FES) is the logarithm of
a partially integrated partition function. Given a chosen set of
collective variables z = ξ(x), this partially integrated partition
function is, up to a normalization factor, the marginal prob-
ability density ρ(z), and is obtained by integrating the Boltz-
mann density over all variables except z (at constant z):
A(z) = –β–1 ln
Z
δ (z – ξ(x)) ν(x) dx = –β–1 ln ρ(z),
(15)
where δ indicates the multivariate Dirac delta distribution de-
ﬁned by δ (z – ξ(x)) = Qd
i=1 δ (zi – ξi(x)) with the single variable
Dirac delta distribution deﬁned by
R
f(x) δ(x – y) dx = f(y), for
any function f. Intuitively, δ behaves like a function that is
zero everywhere but at 0, and whose integral is 1.
In practice, most free energy surface calculations concern
a Helmholtz free energy, but Gibbs free energy surfaces
could be computed as well. The difference is small, unless
the transformation of interest entails a measurable change
in overall density.
If two macrostates A and B correspond to domains of col-
lective variable space, then the probability ratio can be ob-
tained from a free energy surface A(z) by integrating its expo-
nential (the associated density) over the corresponding do-
mains:
∆FA,B = –β–1 ln ZA
ZB
= –β–1 ln
R
A e–βA(z) dz
R
B e–βA(z) dz.
(16)
Similarly to other reduced quantities, one may deﬁne the
reduced free energy surface
a(z) = βA(z).
(17)
Note that in expanded ensemble approaches, a free en-
ergy as a function of the extended variable λ can be deﬁned
as:
A(λ) = –β–1 ln Zλ
= –β–1 ln
Z
ν(x, λ) dx.
(18)
Note the difference between this deﬁnition and that of a free
energy surface as a function of a collective variable (Equa-
tion 15). Here there is no Dirac distribution δ because x and
λ are separate variables, so we obtain the marginal distribu-
tion by integrating over x, which preserves the dependence
on λ. In other words, a slice of conﬁguration space at con-
stant λ is the complete space of x coordinates, whereas a
probabilistic quantity
thermodynamic quantity
•
–β–1 ln(•)
probability density ν(x)
potential energy U(x)
↓
integrate x at constant z
↓
marginal probability density ρ(z)
free energy surface A(z)
↓
integrate z over Σ
↓
probability (measure) P(Σ)
free energy F(Σ)
Table 1. Relations between key statistical and thermodynamic quan-
tities. The right column is –β–1 times the logarithm of the left col-
umn. Probabilistic quantities are related by successive integration
over larger slices of conﬁguration space.
slice of conﬁguration space at constant z = ξ(x) is only a slice
or subset of this coordinate space.
There is a very general relation between thermodynamic
quantities (with the dimension of an energy) and probabilis-
tic quantities. The latter can be expressed as minus thermal
energy (–kBT = –β–1) times the natural logarithm of the for-
mer: this process is called Boltzmann inversion. The main
quantities discussed above are summarized in Table 1.
The free energy surface can be interpreted as an effective
potential energy surface deﬁned on collective variables. The
free energy surface is related to a probability density in the
same way a free energy is related to the probability of a state
(Table 1). Beware, however, that a probability density is not
a probability measure: a value of the density is not the prob-
ability of any particular event. Probability values are unitless
and between 0 and 1; in contrast a probability density has
units of inverse volume, and can take values greater than 1.
The probability of a state is obtained from a probability den-
sity by integrating over the relevant region of conﬁguration
space. In a continuous conﬁguration space, the probability of
each individual conﬁguration is zero. A density is normalized:
its integral over the whole space is 1, which is the probabil-
ity of the whole space. Similarly, a free energy surface is not
directly interpretable as a macroscopic free energy. Unlike a
free energy, it is not an experimental observable. Very impor-
tantly, free energy surfaces do not generally have a simple in-
terpretation in terms of dynamics (e.g. free energy maxima
may not be kinetic barriers), because of distortions due to
the nonlinear geometry of the variables [37, 38].
Potential of mean force (PMF)
Beware that this phrase is used in the literature in two differ-
ent, incompatible meanings. The current colloquial meaning
is the free energy surface as deﬁned above. However, this
can sometimes be misleading. The historic notion of PMF is
used to describe the structure of simple liquids. The poten-
10 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

tial of mean force W(r) between two particles was deﬁned
based on its radial distribution function g(r). The RDF is cal-
culated from the probability density for the interparticle dis-
tance r, ρ(r), divided by a normalization term (proportional to
r2) that makes it constant, conventionally equal to 1, at large
distances in a homogeneous ﬂuid.
W(r) = –β–1 ln g(r)
= –β–1 ln ρ(r)
r2 + C
= A(r) + 2β–1 ln(r) + C,
(19)
where A(r) is the FES along the interparticle distance and C is
an arbitrary constant. The historic PMF and FES are therefore
related, but distinct quantities.
The phrase “potential of mean force” describes very lit-
erally that W(r) is a potential arising from an average force
that would act on a particle at that location. Crucially, the
potential of mean force is zero in non-interacting systems: if
U(x) = 0 for all x then W(r) = 0 for all r, which is not gener-
ally the case for free energy surfaces of nonlinear CVs, due
to a Jacobian term describing a purely geometric entropy. For
details, refer to Section 3.3.
Here again, one may deﬁne the reduced version of W(r):
w(r) = βW(r).
(20)
Multimodal distribution
A probability distribution featuring many disconnected
regions of high probability (each local maximum is a mode)
separated by regions of low probability.
Typically, if the
dimension is high, most of the volume of conﬁguration
space has a very small probability, but there are often a
large number of signiﬁcantly separated high probability
regions.
Ensemble average
The ensemble average of an observable O(x, p), which is a
function of the phase space, is deﬁned as:
⟨O(x, p)⟩=
Z
O(x, p)µ(x, p) dxdp,
(21)
or if O(x) is only a function of conﬁgurations, then
⟨O(x)⟩=
Z
O(x)ν(x) dx.
(22)
All measurable thermodynamic quantities of interest are
equal to ensemble averages of some observable. For exam-
ple, the total energy E of a system is the expectation value of
the Hamiltonian ⟨H⟩. Furthermore, the marginal probability
density ρ(z) is an ensemble average of a Dirac δ “function,”
ρ(z) = ⟨δ (z – ξ(x))⟩(see Equation 15). With ﬁnite sampling, ex-
pectation values have a measurable uncertainty. However,
in the thermodynamic limit (when the number of samples
approaches Avogadro’s number), these can be assumed to
be exact, as any uncertainties are on the order of 10–10 or
smaller.
Ergodic dynamics
The dynamics of a system is said to be ergodic if samples
taken from any single, inﬁnitely long trajectory describe the
complete statistical properties of the dynamics. This allows
the estimation of ensemble averages by the time average or
ergodic average. If we want to compute the average of ob-
servable O(x, p) according to distribution µ(x, p), and we can
generate a discrete dynamics (xt, pt) that is ergodic with re-
spect to distribution µ, then the ensemble average ⟨O⟩can
be estimated as a time average:
⟨O⟩=
Z
O(x, p)µ(x, p) dxdp
≈1
M
M
X
t=1
O(xt, pt) for suﬃciently large M,
(23)
where M is the number of samples. More precisely:
⟨O⟩= lim
M→∞
1
M
M
X
t=1
O(xt, pt).
(24)
Solution-phase molecular dynamics of small molecules is
nearly always ergodic in practice (i.e in simulations of more
than 10 ns), and many biological and soft materials problems
are ergodic in the limit of suﬃcient samples: from here on,
we generally assume the existence of ergodic dynamics in the
systems to which the accelerated methods are applied, as
long as enough sampling is performed. However, ergodicity
is a theoretical notion that characterizes asymptotic behavior
over inﬁnitely long times (the limit in Equation 24). In practice,
molecular dynamics simulations are often very short com-
pared to the longest relaxations times, so that trajectories do
not explore the full conﬁguration space. This situation is de-
scribed as “quasi-nonergodicity.” Enhanced sampling meth-
ods target precisely this case, and aim to recover the statis-
tical properties of ergodic dynamics from trajectories of lim-
ited duration.
Metastability and metastable states
When a system resides in some regions of conﬁguration
space for long times, with rare transitions between those
regions, those regions are called metastable regions or
metastable states. Metastable regions are typically modes
of a multimodal distributions.
Biasing and biased energy
A biasing energy, also called bias energy, is an extra energetic
term Ubias added to obtain a potential energy ˜U(x) biased to
behave a certain way:
˜U(x) = U(x) + Ubias(x).
(25)
11 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

In molecular dynamics, a bias energy Ubias(x) gives rise to a
bias force Fbias(x) = –∇xUbias(x), so that the total biased force
˜F(x) is
˜F(x) = –∇x˜U(x) = F(x) + Fbias(x).
(26)
The bias Ubias(x) is often a function of low-dimension collec-
tive variables z = ξ(x), and can be written Ubias(ξ(x)) so that in
this case, the biased potential energy function is:
˜U(x) = U(x) + Ubias(ξ(x)).
(27)
The resulting biasing force on atoms is calculated using the
chain rule:
˜F(x) = –∇x˜U(x)
= F(x) – ∇x[Ubias(ξ(x))]
= F(x) – dUbias
dz

z=ξ(x)
∇xξ(x).
(28)
This requires the computation of the gradient ∇xξ(x) of the
collective variable with respect to atomic Cartesian coordi-
nates.
Biased conﬁgurational distribution
Under the inﬂuence of a biased potential energy ˜U(x) = U(x)+
Ubias(x), the simulations will sample a biased conﬁgurational
distribution ˜ν(x) given by
˜ν(x) = 1
˜Z
e–β˜U(x),
(29)
where ˜Z =
R
e–β˜U(x) dx =
R
e–β
h
U(x)+Ubias(x)
i
dx is the biased
partition function. This can be re-written as
˜ν(x) = 1
˜Z
e–β
h
U(x)+Ubias(x)
i
= 1
Z e–βU(x) Z
˜Z
e–βUbias(x)
= ν(x) Z
˜Z
e–βUbias(x),
(30)
where
Z
˜Z
=
R
e–βU(x) dx
R
e–β˜U(x) dx
=
R
eβUbias(x) e–β˜U(x) dx
R
e–β˜U(x) dx
=
Z
eβUbias(x) ˜ν(x) dx
= ⟨eβUbias(x)⟩˜U,
(31)
where ⟨· · · ⟩˜U indicates an ensemble average under the bi-
ased distribution ˜ν(x).
Importance sampling
A family of methods where a separate, distinct probability
distribution ˜ν(x) from the target one is sampled, but in such
a way that the ratio of the two distributions is known or es-
timated numerically. Therefore, the target probability and
averages using the target probability can be calculated. Usu-
ally this is done to focus sampling on conﬁgurations that con-
tribute more to any averages of interest.
The name refers to the idea of favoring sampling of the
regions of importance, or if they are unknown, to ﬂatten sam-
pling towards a more uniform distribution. Frequently, the
difference in probability is expressed in terms of some sort
of biasing potential ˜U(x). Importance sampling methods in-
clude the biasing potential and biasing force methods (adap-
tive or not), localization methods and adaptive seeding meth-
ods described in Sections 5, 6, 7 and 9.
As explained below, unbiased properties of the original
distribution may be obtained by reweighting from the modi-
ﬁed, sampled distribution to the desired distribution. In prac-
tice, the sampled distribution is chosen to emphasize sam-
ples that contribute strongly to the averages of interest, mak-
ing the reweighted average over samples from the modiﬁed
distribution a low-variance estimator.
Reweighting
Reweighting involves calculating averages and free energies
of one distribution using samples from another one, as oc-
curs in importance sampling, though it can be used in other
situations as well. The distribution that one samples from
may be one explicitly generated by a biased simulation or
performed at a different temperature, or one that is a mix-
ture of several sampled distributions [39].
If the sampled distribution is ˜ν(x) and the original, unmod-
iﬁed distribution is ν(x), then the average of some observable
O(x) is calculated as
⟨O(x)⟩=
Z
O(x)ν(x)dx
=
Z
O(x)ν(x)
˜ν(x) ˜ν(x)dx
=

O(x)ν(x)
˜ν(x)

˜ν
,
(32)
where the ﬁnal average is taken from samples obtained from
the modiﬁed distribution ˜ν, but the expectation is in the orig-
inal distribution ν. This reweighting can be done effectively
whenever the ratio ν(x)
˜ν(x) doesn’t vary much over the x sam-
pled.
In the speciﬁc example of where ˜ν(x) corresponds to
a simulation with an added bias potential Ubias (see Equa-
tion 30), then the average of an observable O(x) over the
12 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

distribution ν(x) can be estimated by:
⟨O(x)⟩=
*
O(x)
˜Z
Z eβUbias(x)
+
˜U
=
⟨O(x) eβUbias(x)⟩˜U
⟨eβUbias(x)⟩˜U
,
(33)
where the ensemble averages are taken according to the bi-
ased distribution arising from the biased potential energy
˜U(x). The term in the denominator is the exponential averag-
ing estimate of Z
˜Z (see Equation 31 and Section 3.4 for more
detail).
Biased CV distribution
Under the inﬂuence of a bias potential acting in CV space
Ubias(z) = Ubias(ξ(x)), the CV will follow a biased CV distribu-
tion given by
˜ρ(z) = 1
˜Z
e–β
h
A(z)+Ubias(z)
i
,
(34)
where ˜Z =
R
e–β
h
U(x)+Ubias(ξ(x))
i
dx =
R
e–β
h
A(z)+Ubias(z)
i
dz is the
biased partition function.
Target distribution
Common in some enhanced sampling methods is the con-
cept of a target distribution. This represents a desired prob-
ability distribution that an enhanced sampling simulation is
trying to achieve. The target distribution can be in the space
of some collective variables (normally, the ones being biased)
or another space such as the state space in T or along an aux-
iliary variable λ. In some methods, the target distribution is
set by the user. In others, the target distribution can be in-
ferred from experimental observations. A common choice is
to set the target distribution to be uniform over the variables
or states of interest, with all values sampled equally. How-
ever, most methods theoretically support the usage of any
given non-uniform target distributions if the user desires.
Replicas
A replica is a copy of a molecular system. Replicas might
simply be independent copies started from different random
number seeds for velocities or different initial conﬁgurations,
or they might each have a different value of some thermody-
namic parameter like temperature or λ, or have a different bi-
asing distribution. In many types of accelerated simulations,
replicas can exchange information with each other, and this
exchange is key to the success of the method. In most meth-
ods, however, they do have the same chemical composition
and number of atoms.
Generalized ensemble
Generalized ensemble methods encompass expanded en-
semble methods and replica exchange methods (Section 8).
Expanded ensemble
While a usual statistical-mechanical ensemble describes one
set of macroscopic conditions, an expanded (or extended) en-
semble allows for additional degrees of freedom (physical or
non-physical) to vary. An expanded ensemble may be sam-
pled mainly in two ways. The ﬁrst option is to run a collection
of simulations (replicas, i) among which an auxiliary parame-
ter λi takes a discrete set of values. In replica i, the dynamics
of coordinates xi(t) are then propagated under a potential
energy Uλi(xi) or at inverse temperature βi. The second op-
tion is to propagate λi as an additional dynamic variable (see
Extended Lagrangian).
Extended Lagrangian dynamics, λ dynamics
A special case of expanded ensemble simulation, whose
equations of motion include “ﬁctitious” (auxiliary) dynamical
degrees of freedom λ that are not the spatial coordinates
of physical objects or the associated momenta. To sample
the canonical distribution of (x, λ), they can be propagated
following e.g. Langevin dynamics:













dx = M–1p dt
dp =

–∇xUext(x, λ) – γp

dt +
q
2γM
β
dWt
dλ = m–1
λ pλ dt
dpλ =

– ∂Uext(x,λ)
∂λ
– γλpλ

dt +
q
2γλmλ
β
dWt,
(35)
where mλ is a ﬁctitious mass associated to λ. While the two
phrases are essentially synonymous, the term λ-dynamics
is mostly used when a continuous dynamic variable λ con-
nects physically meaningful, and sometimes discrete, states,
such as in alchemical transformations. On the other hand,
the term extended Lagrangian (or extended Hamiltonian) is
more often used when the ﬁctitious coordinate λ follows a
collective variable ξ(x), typically coupled through a harmonic
potential: Uext(x, λ) = U(x)+ kext
2 |ξ(x)–λ|2. These methods are
also referred to as “extended-system dynamics.” Note that
much of this additional baggage can be avoided by making
moves in λ space using Monte Carlo approaches.
Metropolis Monte Carlo
A Metropolis Monte Carlo step makes some change in the
system in a way that preserves the underlying probability
distribution (usually the Boltzmann distribution). Taking the
canonical ensemble for speciﬁcity, the simplest rule is to pro-
pose some new coordinate x′, and change from x to x′ with
probability
P(x →x′) = min
n
1, e–[βU(x′)–βU(x)]o
.
(36)
This means that the move always occurs if the energy is low-
ered (probability is increased), and sometimes occurs if the
energy is higher, with the probability given in Equation 36.
13 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

In order for this to preserve the underlying Boltzmann
distribution, proposals must be made in a symmetric way,
such that the probability of being at x′ and proposing x is
the same as being in x and proposing x′. This is satisﬁed by
simple rules like translating or rotating particles a symmetric
amount from the current position, but care must be taken
for more complex coordinate transformations like torsional
displacement volume changes, or polymer chain moves [40].
It is in fact possible to choose new states with asymmetric
or biased probabilities if those biases and asymmetries
are properly accounted for; this generalization is called a
Metropolis-Hastings step [41].
In addition, the Metropolis (or Metropolis-Hastings algo-
rithms), can also be used to propose new parameters, such
as a change in the temperature of the system, or a change in
the potential energy of the system governed by an auxiliary
parameter λ:
P(i →j) = min
n
1, e–[βiU(λi,x)–βjU(λj,x)]o
.
(37)
Gibbs sampler
Metropolis Monte Carlo steps are ways to move from one
state of the system to another, considering one trial step at
a time. The Gibbs sampler [42] is a way to move to another
state considering many trial states simultaneously. It is gen-
erally used when there are two (or more) different types of
variables deﬁned in the system that could be changed, x and
y, so the probability of the system is deﬁned by ν(x, y). The
Gibbs sampler is deﬁned by taking steps in x ﬁrst and then y,
by the following algorithm:
1. Start at xi,yi.
2. Pick a variable to change randomly with some fre-
quency f, such that f(x) + f(y) = 1.
3. If you choose x, pick a new x from the conditional distri-
bution ν(x|yi), i.e. the probability of x given yi.
4. If you choose y, pick a new y from the conditional distri-
bution ν(y|xi), i.e. the probability of x given yi.
5. Repeat.
Any method to pick x and y from the distribution can be used,
including Metropolis Monte Carlo. The algorithm could be
generalized to more than two variables, by randomly choos-
ing each of the variables. Interestingly, one can alternate
between variables deterministically (ﬁrst pick x, then pick y,
then pick x again, or pick x 100 times, then y, then x 100 times
again) and the results will not satisfy detailed balance, but
they will still satisfy balance, which means the distribution
ν(x, y) will still be preserved. To be concrete, let us assume
one variable is the coordinate x, and the other is the temper-
ature T. Then one implementation of the Gibbs sampler, one
would carry out simulations in x using molecular dynamics in
the NVT or NPT ensemble for a certain number of steps, then
perform a move in T space using an algorithm that generates
a new T from ν(T|x), while keeping the coordinates constant.
Detailed balance and balance
Detailed balance is a constraint on the way moves from a
given state of a system to another state of the system one
are performed. If i and j are two states, then detailed balance
requires that
P(i)
P(j) =
Pi→j
Pj→i
,
(38)
where P(i) and P(j) are the desired probabilities of i and j, and
Pi→j and Pj→i are the probabilities of transitioning from state
i to j and state j to i during some process. These states could
be sets of conﬁgurations, for physical dynamics, or between
different thermodynamic ensembles, as occurs in many
nonphysical dynamical systems. Generally, physical systems
obey detailed balance, and most common simulation meth-
ods, such as molecular dynamics or Metropolis Monte Carlo
are designed to obey detailed balance, and thus preserve
the overall probability distribution of the system.
Balance is a weaker requirement [43], and is merely the
requirement that the physical desired probability distribu-
tion is preserved by a set of dynamical moves. This can be
done without forcing the ratio of the ﬂuxes between states
being equal to the ratio of the probabilities of two states,
as is the case in detailed balance. For example, you could
have a cycle of ﬂuxes between three states i,j, and k, and
still preserve the probability distribution. Proving a given set
of ways to perform moves between states obeys balance is
usually signiﬁcantly harder than proving that a set of moves
preserves detailed balance. However, studies have shown
that it is often possible to obtain better sampling through
states with algorithms that only obey balance rather than
detailed balance [43, 44].
Temperature-based sampling
Refers to enhanced-sampling methods relying on an in-
creased effective temperature ˜T to reduce metastability.
This can be done on only some replicas within a set of repli-
cas, on some fraction of the time within a given trajectory, or
on some fraction of the system by selective scaling of poten-
tial energy terms. In the latter case, if U = Uunscaled + Uscaled,
scaling Uscaled to reach effective inverse temperature ˜T
means using the modiﬁed potential ˜U:
˜U = Uunscaled + T
˜T
Uscaled.
(39)
Then the Boltzmann factor is
e
Uunscaled
kBT
+ Uscaled
kB˜T ,
(40)
and the scaled degrees of freedom have probabilities consis-
tent with the different ˜T.
14 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Replica exchange
Generalized ensemble methods that allow for exchanging
conﬁgurations between replicas, usually according to criteria
that guarantee that each replica samples from a well-deﬁned
distribution.
Adiabatic dynamics
A dynamics where selected degrees of freedom are assumed
to not exchange energy with the rest of the system. This can
be achieved if these degrees of freedom are decoupled from
the rest of the system, and evolve effectively independently.
Driven simulations
Refers to simulations with a time-dependent bias potential
Ubias
t
(x) or force Fbias
t
(x) following a pre-determined schedule.
Out-of-equilibrium system
A system that has not reached its steady state, either because
it is considered on a time-scale smaller than its relaxation
time-scales, or because it is driven by time-dependent forces
which maintain out-of-equilibrium behavior.
Out-of-equilibrium method
A simulation protocol that does not generate trajectories
that sample from the canonical ensemble associated with
a given potential energy function.
Instead, initial and
boundary conditions are given, as well as a possibly time-
and history-dependent potential energy function or force
schedule.
The statistics of the generated trajectories and
conﬁgurations are determined by these conditions. Unlike
the canonical distribution, however, there is no general
closed-form expression for the resulting out-of-equilibrium
distributions of trajectories or conﬁgurations.
Seeding
A strategy where the conﬁguration space covered by a set
of (relatively short) simulations is governed by the choice of
their starting conditions. This strategy is typically used to in-
crease the diversity of the samples produced, despite the in-
ternal correlation of each trajectory.
Adaptive method
All enhanced sampling methods have parameters, whose
value can often be improved based on information from
a simulation. This process can be repeated, leading to an
iterative form of the algorithm. In some cases, this iteration
can be built into the dynamics itself, so that parameters
are updated (adapted) on the ﬂy during the simulation.
It is then called adaptive.
This can take the form of a
time-dependent bias potential Ubias
t
(x) or force Fbias
t
(x), or
branching decisions to stop or launch simulation instances.
Many methods can be used with ﬁxed parameters, but
also have iterative and adaptive variants. Conversely, the pa-
rameters of adaptive methods can be frozen when they are
deemed suﬃciently close to convergence, to continue sam-
pling in a theoretically simpler setting.
Free energy perturbation (FEP)
Free energy perturbation is the process of calculating the
free energy differences due to small changes in the potential
energy using reweighting or exponential averaging, hence
the term “pertubation.” It is performed using the exponential
averaging formula. Alternatively, and somewhat confusingly,
it can also refer to calculating a free energy difference using
any sort of pathway, using any sort of free energy estimator.
3
Free energy estimators
Recovering the original statistical ensemble often requires es-
timating a free energy. For some enhanced sampling meth-
ods, the free energy estimator is central to the enhanced
sampling scheme, while for others, it is a post-processing
tool. This section provides a concise presentation, but exten-
sive reviews can be found elsewhere [45–49].
Free energy estimators are expressions that are used nu-
merically to compute free energy differences or free energy
surfaces from quantities that are available in simulations
(conﬁgurations, energies, and forces). The important prop-
erties of an estimator are its accuracy or bias, how far off the
real value it is given inﬁnite, ideal sampling, and precision
or variance, how much the result ﬂuctuates given a ﬁnite
amount of noisy data. Note that the bias of the estimator is
a different concept than the bias that is intentionally added
to a system via a potential.
In this section we consider estimators for:
• free energy as a function of a Hamiltonian parameter
(auxiliary variable) λ, which the energy uλ depends on,
or
• free energy surfaces as a function of a collective vari-
able (or a vector of variables) z = ξ(x).
For generality and cleanness of presentation, reduced
units are used, which can converted back to united formulas
using the deﬁnitions of reduced units.
3.1
Directly measured ratios
Given a set of sampled conﬁgurations that visits two states
i and j, their reduced free energy difference fij can be esti-
mated by estimating free energies between states by taking
the ratio of the time spent in each state, a process called
Boltzmann inversion, by approximating Equation (5):
∆fij = – ln
Nj
Ni
,
(41)
where fij is the reduced free energy, and Ni and Nj are
the numbers of observed conﬁgurations in states i and j.
15 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

This equation holds for whatever division into states one
uses. However, this is only true if the simulation samples
the equilibrium between states i and j, or equivalently,
if it is long enough such that many transitions between
those states have been observed [50].
If a list of several
states is deﬁned, for example 1,2, ...,K, then estimates
from neighboring states can be chained to compute relative
free energies for the entire list of states, by estimating
f12 + f23 = – ln N2
N1 + – ln N3
N2 = – ln N3
N1 = f13, and so forth. If
the states are deﬁned as bins along collective variables,
this yields a (discretized) free energy proﬁle along those
variables. This idea extends to the calculation of continuous
probability distributions and free energy surfaces. In that
case, the probability distribution can be estimated using a
kernel density estimator (KDE) or a Gaussian Mixture Model
(GMM) [51].
Convergence of this ratio can be accelerated by impor-
tance sampling: adding biasing potentials ubias
i
and ubias
j
,
with ∆ubias
ij
= ubias
j
– ubias
i
. If the bias ubias
i
depends only on
the state i, and is constant over x within a state, the free
energy can be estimated by simply subtracting the bias from
the free energy estimate of the histogram in Equation 41:
∆fij = – ln
Nj
Ni
– ∆ubias
ij
.
(42)
3.2
Estimating free energies from the
transition count matrix
The transition probabilities between states, which could
be along one or more discretized CVs or between different
states in an expanded ensemble, can be used to build a tran-
sition probability matrix. This matrix containing equivalently
either the number of transitions from a state i to a state j, or
the probabilities Pi→j of performing the transition.
If one is considering a simulation that allows transitions
between two states i and j, then this implies the average of
the transition probabilities between states is equal to the ra-
tio of the partition functions between the states of interest
[52–54]. Speciﬁcally:
efj–fi = Zi
Zj
=

Pi→j(x)


Pj→i(x)
,
(43)
where Pi→j(x) is the probability of accepting a move from
state i to state j proposed from conﬁguration x. For sim-
plicity we assume that the probability of moves proposed
from i to j is equal to the probability of transition from j to
i; more general transitions can also be included [54]. We
can construct a matrix of all possible transitions between
any pair of states; hence we can call this a transition count
matrix or transition matrix.
Typically, this averaging is carried out over the transitions
that were actually observed. However, this average can be
calculations over transitions that were not actually performed.
If one counts only whether a move is made or not, one
averages a number of either 0’s or 1’s.
However, in any
method that makes transitions between states, we need
to calculate the probability that the transition would occur
to decide whether we move or not, as in the Metropolis
Monte Carlo algorithm (Equation 36). This probability can be
calculated and used in averaging even if the moves are not
made.
Interestingly, one can even compute this average of tran-
sition probability distributions using a different transition
probability formula than the one actually used to carry out
jumps between states. For example, one can show that the
Metropolis Monte Carlo criterion (min{1, e–uj+ui)}) is more
eﬃcient to use to decide whether or not to move between
two states, but the Barker criterion

e–uj
euj +eui

is more eﬃcient
to average in order to calculate free energies [55].
Note that free energy estimated directly from population
ratios will of course be inaccurate if too few transitions have
been observed between states [50]. More generally, the dis-
cretized master equation expresses the probability distribu-
tion νi of each state i over time as the sum of all of the prob-
abilities of states moving into and out of that state, as:
νi(t + δt) = νi(t) +
X
j̸=i
(νj(t)Pj→i – νi(t)Pi→j).
(44)
This is a time-dependent equation, but can be shown to
have an equilibrium stationary probability distribution νi(x).
Convergence of probability distributions is slow because the
samples extracted from molecular dynamics simulations are
correlated. However, by taking into account the conditional
probability distribution (probability of an event happening
given previous history), the statistical dependency of the
MD trajectory is taken into account [56–59]. An even more
accurate estimation of the free energies can therefore be
achieved by building a Markov State Model from the tran-
sition count matrix, whereby the probability of each state
is computed as the leading eigenvector of the transition
matrix.
In this case, it may not be necessary for single
simulations to sample the entire state space, as long as each
of the individual transitions is estimated accurately. This is
the basis for such methods as DHAM [58], dTRAM [56, 57],
and TRAM [59], which are somewhat beyond the scope of
this review, as they are advanced analysis methods, but not
technically by themselves accelerated sampling methods.
3.3
Thermodynamic integration (TI)
Thermodynamic integration is a family of free energy estima-
tors that express the derivative of free energy with respect
to a continuous parameter as an ensemble average of the
16 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

derivative of the energy with respect to the same parameter,
possibly with additional terms as discussed below.
3.3.1
TI along an alchemical parameter
When the reduced potential energy is a function uλ of a
smooth coupling parameter λ that connects all states of
interest (e.g. in alchemical perturbations), a continuous free
energy f(λ) is deﬁned, the derivative of which is the “mean
force” at a ﬁxed value of λ:
df
dλ =
∂uλ
∂λ

λ
,
(45)
where the averaging is over samples obtained with the given
value of λ.
If states i and j correspond to values λi and λj of the con-
tinuous coupling parameter, then the free energy difference
between states i and j, ∆fij, is
∆fij =
Z λj
λi
∂uλ
∂λ

λ
dλ.
(46)
If i and j are neighboring states along λ with a suﬃciently
small difference ∆λij, the integral can be approximated by
the trapezoidal rule:
∆fij =
∆λij
2
"∂uλ
∂λ

λi
+
∂uλ
∂λ

λj
#
.
(47)
Other numerical integration formulas can of course be
used, although usually there has not been found to be much
advantage over the straightforward trapezoidal rule [47].
3.3.2
TI along a collective variable
A generalization of Equation 45 holds for the gradient of the
reduced FES a(z) over a single collective variable z = ξ(x):
da(z)
dz
= –

Fξ(x)

ξ(x)=z ,
(48)
where the average is over all conﬁguration with ξ(x) = z. Fξ is
a generalized force that includes two terms:
• the energy gradient with respect to collective variables;
• a geometric, Jacobian term due to the curvature of the
isosurfaces of a nonlinear CV ξ (see Refs [60–62] for de-
tails).
More precisely, the “free energy gradient with respect to
the CVs” is a partial derivative, which is not well-deﬁned un-
less one speciﬁes a complete set of generalized coordinates
including the CVs [63]. This complicated or even intractable
process can be circumvented [64, 65] by noting that the force
may be projected onto a CV ξi along an arbitrary vector ﬁeld
bi, verifying for all i, j:
bi · ∇xξj = δi,j,
(49)
that is, each bi has a scalar product of 1 with the gradient of
ξi and is orthogonal to the gradients of all other CVs. In the
case of a single CV, one possible choice of b that satisﬁes this
condition is to make it proportional to the gradient of ξ with
an appropriate normalization factor: b(x) = 1/(|∇ξ|2)∇ξ.
Then the following expression for the generalized force
along coordinate ξi holds [65]:
Fξi(x) = –bi(x) · ∇xu(x) + ∇x · bi(x),
(50)
where the ﬁrst term is the projection of atomic forces –∇xu
onto the CV, and the second is the Jacobian term described
above, computed as the divergence of bi.
Integrating the gradient to obtain the free energy surface
is easy when using a scalar collective variable. In dimension
greater than one, however, special integration methods are
required [66].
3.3.3
Signiﬁcance of the Jacobian term
Take the example of a collective variable measuring the dis-
tance r between two particles. Here, the gradient of the free
energy surface depends on the geometry of the CV. Given a
complete coordinate transform from Cartesian to general-
ized coordinates including r, which is necessary to deﬁne par-
tial derivatives with respect to r, one may write [61]:
da
dr =
∂u
∂r – 2
r

ξ(x)=r
.
(51)
As mentioned in Section 2.2, the historic deﬁnition of the “po-
tential of mean force” (PMF) for r is simply the potential aris-
ing from the mean force and corresponds to the ﬁrst term in
Equation 51:
dw
dr =
∂u
∂r

ξ(x)=r
.
(52)
3.3.4
Comparison between alchemical and
conﬁgurational TI
Let us consider extended conﬁgurations (x, λ) that include
the alchemical parameter, and deﬁne an alchemical CV as a
simple projection of those onto λ:
ξ(x, λ) = λ.
(53)
Then Equation 45 can be seen as a special case of Equa-
tion 50, in which the second term is zero because no
nonlinear coordinate transform is involved.
3.4
Exponential averaging
The following sections apply to the free energy difference be-
tween two discrete states i and j, when they can be charac-
terized by different reduced energies ui and uj, with ∆uij =
uj – ui. These estimators (exponential averaging, BAR, MBAR
and WHAM) rely on the overlap between the thermodynamic
17 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

states i and j, i.e. that conﬁgurations have a signiﬁcant proba-
bility under both states. They cannot be used when the states
do not have overlap, i.e. the states are deﬁned by each hav-
ing a different value of some collective variable. In contrast,
free energy calculated by visitation ratios, transition matrices,
and thermodynamic integration can still be used when states
do not have overlap.
The total free energy difference between two such states
is, by deﬁnition:
e–∆fij =
R
e–uj dx
R
e–ui dx
=
R
e–uie–(uj–ui) dx
R
e–ui dx
.
(54)
On the right-hand side, one can recognize the expression for
an ensemble average in state i. Thus the free energy differ-
ence can be written:
∆fij = – ln⟨e–∆uij⟩i.
(55)
This average can be estimated numerically as:
∆fij = – ln 1
Ni
Ni
X
n=1
e–∆uij(xn),
(56)
where the Ni samples are from the ith state. While this ex-
pression is formally exact, convergence of the exponential
average is critically dependent on the tail of the distribution
of ∆uij, and as a result, on the overlap between states i and
j. This can be alleviated by collecting energy differences go-
ing both ways (∆uij sampled in state i, and ∆uji sampled in
state j), and combine them using the BAR estimator. This gen-
eral strategy is sometimes also referred to as “Overlap Sam-
pling” [67].
3.5
Bennett’s acceptance ratio (BAR)
The Bennett acceptance ratio method is the lowest variance
method to estimate the free energy difference between two
states using the energies sampled at those states. Speciﬁ-
cally, we obtain an optimal value of the free energy difference
∆fij given Ni samples performed with reduced energy ui and
Nj samples performed with reduced energies uj. There are
many ways to write the method [48, 68, 69]; see these refer-
ences for the detailed derivations.
One standard approach is to derive the thermodynamic
identity:
∆fij = ln
D
1
1+exp(–∆uij+c))
E
j
D
1
1+exp(∆uij–c)
E
i
+ c – ln
Nj
Ni
,
(57)
where c is an arbitrary constant. This expression is an asymp-
totically unbiased estimator (meaning, if enough data is col-
lected, it will converge to the correct answer) for any choice of
c. However, one can prove that the lowest variance estimate
of free energy is obtained when c = ∆fij [68]. To ﬁnd ∆fij, this
equation needs to be solved self-consistently, resulting in the
equation one can solve for c, and therefore fij as well:
Nj
X
i=1
1
1 + exp(–∆uij + c)) –
Ni
X
j=1
1
1 + exp(∆uij) – c = 0.
(58)
This can be solved by a number of numerical techniques
as implemented in many codes, such as pymbar [70] and
gmx bar [71].
Several variants, which can be useful in speciﬁc cases, use
ﬁxed c [47, 69], but for almost all standard uses, the opti-
mized, lowest variance version is best.
3.6
Multistate Bennett acceptance ratio
(MBAR)
If we carry out simulations at K different thermodynamic
states, then the free energies of all of the states can be
estimated as:
fi = – ln


N
X
n=1
e–ui(xn)
P
k Nkefk–uk(xn)

,
(59)
where N is a sum over all of the samples collected at any of
the K states. Since there is one equation for each of the free
energies fi from each of the K states, this leads to a series of
K that must be solved for the set of fi
2. This is a set of implicit
equations since the fk appear on both sides of the equations.
Similarly to BAR, there are a standard ways to solve this set of
equations implemented in a number of different codes [70,
72, 73].
MBAR also allows the computation of high precision un-
certainties in the ∆fij’s, as it can take into account the corre-
lations in fi and fj thanks to their simultaneous estimation. A
key fact is that the MBAR system of equations reduces pre-
cisely to the equation for BAR for estimating the free energy
difference between two states [70].
MBAR can also be seen as exponential averaging to target
distributions described by the reduced potential ui(x) from
the mixture distribution νmix(x) [39]. Speciﬁcally, the mixture
distribution is formed by combining all the samples from all
simulations that are put into the mixture, proportional to the
number of samples Nk from each simulation. Using Equa-
tion 56, we can then rewrite Equation 59 as:
fi = – ln

1
N
N
X
n=1
e–ui(xn)
νmix(xn)

,
(60)
2Effectively, there are only K – 1 independent free energies since the set of
free energies has an arbitrary reference zero.
18 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

with
νmix(x) =
X
k
Nk
N efk–uk(x),
=
X
k
Nk
N νk(x).
(61)
3.7
Weighted histogram analysis method
(WHAM)
In its original formulation, WHAM consists in applying
Boltzmann inversion (i.e. calculating free energies from
probabilities) to histograms collected under a localizing po-
tential (Section 5), and joining the bias-corrected histograms
in an iterative and statistically optimal way to reconstruct
the global histogram, from which the global free energy
proﬁle is calculated [74]. However, the same idea can apply
to histograms collected at different temperatures or other
sets of histograms, as long as the histograms overlap.
One can start from the MBAR equations to derive WHAM
as well. If one can histogram the data by energy into i bins,
then the sum over n samples in MBAR becomes a sum over
M energy bins instead. The iterative equations of WHAM can
be written as:
Pi,k =
PK
k=1 nk,ie–uk,i
PK
j=1 nkefj–uj,i ,
(62)
with
e–fk =
M
X
i=1
Pi,k,
(63)
where nk,i are the counts in each bin from the kth state, nk =
PM
i=1 nk,i is the sum of all the samples in the kth state, over
all of the M bins, Pi,k is the probability distribution of bin i in
the kth state, and uk,i is the energy of the system in the ith
bin and kth state [74]. These are equivalent; in the WHAM
equations, instead of directly summing over all n samples,
we ﬁrst sum over all of the samples in each bin nk,i from each
state to get Pi,k, and then sum over all bins to get the free en-
ergy. When the bins in WHAM are shrunk to have zero width
(i.e. down to δ functions), then the WHAM equations become
equivalent to MBAR, as the sum again becomes a sum over
samples. If there are many samples, WHAM can be signiﬁ-
cantly faster than MBAR. However, WHAM only gives similar
results to MBAR if the bins are narrow enough for the energy
u and probability Pi to both be approximately constant within
a bin.
4
Out-of-equilibrium / driven methods
The idea of out-of-equilibrium driven methods is to force
the system to follow a given schedule of a collective variable,
or of an alchemical parameter λ, in order to explore con-
ﬁguration space. This class of methods yields simulations
in which the original distribution is modiﬁed, and which do
not converge to an equilibrium ensemble (Figure 1).
The
equilibrium distribution may be retrieved under speciﬁc
circumstance, as detailed below.
The schedule followed
may be fast, so that even if the system starts at equilibrium
(a usual assumption), it does not remain at equilibrium.
An early version was targeted MD [75], which consists in a
moving constraint on the RMSD between current Cartesian
coordinates and a target. Steered molecular dynamics, intro-
duced shortly thereafter to mimic Atomic Force Microscopy
experiments, introduces a ﬁctitious 3D particle moving at
constant velocity, and connected to a molecule by a har-
monic spring [76]. These two methods can be considered
to have converged, as moving harmonic restraints can be
applied to arbitrary collective variables z using software
tools such as Colvars [77] or PLUMED [78].
Out-of-equilibrium pulling behaves differently depending
on the rate of the transformation. In the limit of inﬁnitely
slow (quasistatic) switching, all orthogonal degrees of free-
dom are fully relaxed at all times, so that equilibrium proper-
ties are recovered. This is the case of the “slow growth” ap-
proach [79], which uses very slow switching from energy UA
to UB. The work W performed along the way is an approxima-
tion of the reversible work, that is, the free energy difference
from A to B.
At the other end of the spectrum, inﬁnitely fast switching
amounts to comparing the energy of a given conﬁguration
(in absence of any relaxation) for two different Hamiltonians,
using a FEP approach [80, 81].
In intermediate cases, the free energy difference can be
estimated [82] by weighting the non-equilibrium trajectories.
Calling Wλ the total non-equilibrium work exerted by the bias
over a trajectory up to a value λ, the so-called Jarzynski iden-
tity states:
e–β(Fλ–F0) =
D
e–βWλE
,
(64)
where the average is taken over the equilibrium ensemble of
initial conditions at λ = 0.
Out-of-equilibrium methods are not frequently used to
estimate free energy differences because the exponential
averaging free energy estimator is in general plagued by
a large variance.
Nevertheless, variants of the Jarzynski
identity have been successfully applied to rare examples
of suﬃciently fast-relaxing systems [83]. The high variance
of the Jarzynski estimator is due to the fact that the work
values that contribute the most to the average have small
probability (as discussed in Section 3.4). Improved estima-
tors [84] and modiﬁed algorithms [85–87] can thus take
advantage of running simulations in the forward and back-
ward direction [88]. Although less common, there have been
important applications of these principles for applications
19 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

such as ligand binding free energies [89].
As a result, most simulations that aim to recover free en-
ergies resort to equilibrium or near-equilibrium sampling (as
is often the case with adaptive biasing algorithms).
5
Localization methods
In the broad sense, localization refers to sampling only in a
small, well-deﬁned volume of conﬁguration space. In this
class of methods, the original conﬁgurational distribution
is modiﬁed, the simulation converges to an equilibrium en-
semble, and sampling is enhanced by specifying the starting
and ending coordinates of the simulation, and localizing
them to a well-deﬁned region of space (Figure 1). Localized
sampling within region i can be achieved by either imposing
constraints around zi (constraining strategy) or a conﬁning
potential Ubias
i
(x) (restraining strategy), which can result in
sampling from overlapping regions.
Note that this strategy is also sometimes referred to as
“stratiﬁcation.” In statistics, stratiﬁcation strictly means par-
titioning conﬁguration space along one or more collective
variable, and collecting samples separately in each discrete
section (stratum). This approach can be used in conjunction
with ABF (11.4). In contrast, the most common restraining
strategies mentioned in this section yield overlapping sam-
ples, which is why we prefer the term “localization.”
Restraining or constraining a simulation is equivalent to
convolving the original probability density with a localizing
function: a Dirac distribution δ(z) (constrained case), a Gaus-
sian kernel, or a (possibly smoothed) indicator function of
an interval (exponential of a ﬂat-bottom potential) in the re-
strained case. The most used restraining strategy involves
biasing the distribution by imposing harmonic potential re-
straints Ubias
i
(x) = k
2|ξ(x) – zi|2, and is equivalent to convolv-
ing the original probability distribution with a Gaussian ker-
nel. Nowadays this is generally referred to as “umbrella sam-
pling,” although it is quite different from the historic umbrella
sampling method [90] which was not localized (see Section 6
for a further description of this approach and how it relates
to other methods described in this review). Based on um-
brella sampling trajectories, the free energy landscape can
be estimated using the WHAM, BAR, or MBAR estimators (see
Section 3). The constraining strategy was introduced as “Blue
Moon” sampling [91]. In that case, the free energy can be re-
constructed by thermodynamic integration (Section 3.3). Us-
ing ﬂat-bottom potentials, equivalent to convolving the prob-
ability distribution with an indicator function, is referred to
as, for example, “boxed MD” [92], and is frequently used in
combination with ABF (Section 7.3).
These methods have been improved upon by slightly
more complicated variants that include transitions between
the restrained or constrained windows can improve sam-
pling by avoiding kinetic traps. Successful (and thus popular)
approaches involve introducing transitions between the
restraining potentials, in which case the methods fall under
expanded ensemble or replica exchange (see Section 8).
6
Non-adaptive biasing potential
methods
A range of methods are designed to ﬂatten the energy land-
scape in a static way.
This class of methods modiﬁes the original conﬁgura-
tional distribution , lets the simulations converge to an
equilibrium ensemble, but does not do so by localizing
the simulations to speciﬁc regions of conﬁguration space.
Instead, sampling is biased by employing an external bias po-
tential. Contrary to the adaptive methods presented in the
next section, in this case the bias potential is pre-determined
and static (Figure 1).
This is the case of the original Accelerated MD [93] and
Gaussian-accelerated MD [94, 95] methods. In these meth-
ods, the form of the modiﬁed potential energy is determined
by user-controlled parameters, lowering the energy barriers
either for speciﬁc transitions (e.g. along dihedral angles), or
within a given energy range. Note that these methods can
also be run in an adaptive manner (Section 7.2.3).
Following the principle of importance sampling, unbiased
statistics can be recovered by reweighting. Success of this,
however, depends crucially on good overlap between the bi-
ased and unbiased distributions of conﬁgurations.
Custom-designed static biasing potentials combined with
the idea of localization (Section 5) are the principle behind
modern Umbrella Sampling. In the seminal “Umbrella Sam-
pling” paper from 1977 by Torrie and Valleau [90], the au-
thors incorporate an external bias (called a weighting func-
tion in their language) that is designed to lead to ﬂat sam-
pling. Note that in this original “Umbrella Sampling” paper, a
single simulation was used. The idea of multiple windows (lo-
calized, “stratiﬁed”) umbrella sampling with ﬁxed harmonic
bias potential, as described in Section 5, was introduced later
on. The authors constructed the bias potential by hand using
trial and error but suggest that the computer could be pro-
grammed to perform this task, as is now done routinely in
adaptive methods (see Section 7).
7
Adaptive bias simulations
This class of the method is related to the previous one in that
sampling is biased, but contrary to the methods presented
above, the bias is learned during the simulation and adapted
on-the-ﬂy (Figure 1).
20 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

7.1
Adaptation and the adaptation rate
In adaptive bias simulations, the external forces needed to
enhance sampling are learned based on information from
the trajectory itself, and updated as the simulation pro-
gresses. The bias always depends on statistical properties of
the trajectory, so that it cannot be adapted instantaneously,
but more or less progressively as information becomes
available.
This is controlled by some external parameter,
generically called adaptation rate, or learning rate in the ma-
chine learning community. For example, in well-tempered
metadynamics (7.2.1), the adaptation rate results from the
deposition rate NG, the height H0 of the Gaussian kernels,
and the bias factor γ, whereas in ABF (7.3), it depends on
the number of samples collected locally before applying the
estimated biasing force.
The choice of adaptation rate is essential to the success
of an adaptive bias simulation: if it is too slow, the process
will not learn eﬃciently, but setting it too fast will also neg-
atively affect convergence. Typically, adaptation should be
slow enough that some orthogonal degrees of freedom have
time to relax to changes in biased degrees of freedom, or at
least, remain within a short relaxation time of their equilib-
rium distribution under the current state of the bias. Details
depend on the speciﬁc method, and the optimal value of the
adaptation rate is system-dependent. Choosing a high adap-
tation rate may favor rapid initial exploration, while slower
adaptation optimizes long-term convergence [96].
7.2
Adaptive biasing potential (ABP)
methods
In adaptive biasing potential (ABP) methods, an external bias
potential in a space of some chosen collective variables (CVs)
is added to bias the dynamics of the system. The purpose
of the bias potential is to counteract the free energy surface
and lead to more uniform sampling in CV space. In other
words, the bias potential leads to sampling of a biased CV-
distribution that is easier to sample. As the FES is naturally a
priori unknown, the bias potential is generally constructed in
an adaptive manner through some kind of iterative scheme.
ABP methods differ in how the bias potential is constructed
and which kind of sampling is obtained at convergence.
Note that most ABP methods can also be used in conjunc-
tion with Monte Carlo simulations. In this case, the bias po-
tential needs to be taken into account in the Monte Carlo ac-
ceptance probability (Equation 36).
The performance of ABP methods depends critically on
the choice of the CVs, the CVs need to be chosen carefully and
they should properly separate the relevant metastable states
and correspond to essential slow degrees of freedom. Fur-
thermore, most ABP methods are limited in the number of
CVs that they can handle and generally we can use no more
than three to four CVs, though there are ABP methods that
can handle a larger number of CVs [97, 98].
A wide range of ABP methods have been introduced
throughout the years. In the following two Sections, we dis-
cuss two of these methods in detail, metadynamics [99–101]
and variationally enhanced sampling [102, 103].
Metady-
namics is the most widely used ABP method and it has
spurred the development of a great number of variants.
Variationally enhanced sampling is a more recent ABP
method that is based on the variational principle. For other
ABP methods, it would be beyond the scope of this review
to discuss all of them in detail, so we limit ourselves to give
an non-exhaustive list of ABP methods. We refer the reader
to the original references and review papers [104–106] on
the subject for further details regarding these methods.
Among methods that we can categorise as ABP methods
are local elevation [107], energy landscape paving [108], self-
healing umbrella sampling [109, 110], adaptive biasing MD
(ABMD) [111], Gaussian-mixture umbrella sampling [112],
the adaptive biasing potential method [113], basis function
sampling [114], Green’s function sampling [115], ﬂying Gaus-
sian method [116], artiﬁcial neural network sampling [117],
on-the-ﬂy probability-enhanced sampling (OPES) [31, 118],
reweighted autoencoded variational Bayes for enhanced
sampling (RAVE) [119], targeted adversarial learning op-
timized sampling (TALOS) [120], Gaussian mixture-based
enhanced sampling (GAMBES) [121], adaptive topography
of landscapes for accelerated sampling (ATLAS) [122], and
reweighted Jarzynski sampling [123].
For mathematical analysis of the convergence and eﬃ-
ciency of ABP methods, we refer to the series of works avail-
able in the literature [124–127].
7.2.1
Metadynamics
One of the most widely used ABP method is metadynam-
ics [99–101], and a large number of metadynamics variants
have been developed and introduced through the years. In
metadynamics methods, we enhance the sampling along
a few selected collective variables (CVs) that correspond
to slow degrees of freedom.
Metadynamics methods are
based on adding a time-dependent external bias potential
that counteracts the free energy surface.
This external
adaptive biasing potential is composed as a sum of repulsive
Gaussian kernels that are periodically deposited at the
current location in the CV space, see Figure 3. From the bias
potential, one can directly estimate the free energy surface
as a function of the selected CVs. Furthermore, it is possible
to obtain the FES, both for the biased CVs and for any other
set of CVs, via reweighting. Under certain conditions, one
can also rescale simulation time to obtain rare-event kinetics
21 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

from biased metadynamics simulations.
There are two main variants of metadynamics that
are in common usage nowadays,
well-tempered meta-
dynamics
[100]
and
conventional
(non-well-tempered)
metadynamics [99], that we will discuss below. Metadynam-
ics methods and their applications have been discussed in
various reviews [101, 128–133].
Well-tempered metadynamics
In well-tempered metadynamics [100], the bias potential
Ubias(z) is updated through the following stochastic iteration
scheme where every NG simulation steps (i.e., MD steps) we
add a Gaussian biasing kernel G(z, zn) at the current CV value
zn
Ubias
n
(z) = Ubias
n–1 (z) + exp

–
1
γ – 1βUbias
n–1 (zn)

G(z, zn).
(65)
Here n is the current step number in the recursive updat-
ing of the bias potential (i.e., number of added Gaussian
kernels;
note that this is not the same as MD steps),
Ubias
0
(z) = 0, and the Gaussian kernels are scaled by the
factor exp
h
– 1
γ–1βUbias
k–1 (zn)
i
, with γ a parameter called the
bias factor. The Gaussian kernels are given by
G(z, zn) = H0 exp

–1
2 (z – zn)T Σ–1 (z – zn)

,
(66)
where H0 is the height and Σ is the variance matrix of the
kernel. Generally the variance matrix is taken as diagonal,
Σij = δijσ2
i where δij is the Kronecker delta function (δij = 1 if
i = j and 0 otherwise) and σ = [σ1, . . . , σd] is a vector of the
standard deviations (i.e., widths) corresponding to the CVs.
In this case, the Gaussian kernels can be written as
G(z, zn) = H0 exp

–1
2
d
X
i=1
(zi – zn,i)2
σ2
i

,
(67)
where d is the number of CVs
In between bias potentials updates, after depositing n
Gaussian kernels, the bias potential is given by
Ubias
t
(z) =
n
X
k=1
exp

–
1
γ – 1βUbias
k–1 (zk)

G(z, zk)
=
n
X
k=1
Hk exp

–1
2
d
X
i=1
(zi – zk,i)2
σ2
i

,
(68)
where we write Hk = H0 exp
h
– 1
γ–1βUbias
k–1 (zk)
i
as the height of
the k-th added Gaussian. In the long time limit, the height
Hk goes to zero as the scaling factor exp
h
– 1
γ–1βUbias
k–1 (zk)
i
decreases as
1
k [100, 134] (see Equation 3 in Ref [100]).
Therefore, as the metadynamics simulations progresses,
the change of the bias potential is smaller and it becomes
quasi-stationary.
It has been proven [134] that updating
the bias potential according to Equation 65 leads to an
asymptotic solution given by
Ubias
t
(z) = –

1 – 1
γ

A(z) + K(t),
(69)
where K(t) is a time-dependent constant. Note that in the
metadynamics literature, the FES is generally denoted as F(z)
(or F(s)) instead of A(z).
The bias potential in Equation 69 only partially cancels out
the FES and the CVs are sampled according to a so-called well-
tempered distribution
˜ρ(z) =
[ρ(z)]1/γ
R
[ρ(z)]1/γ dz,
(70)
which can be viewed as a distribution where CV ﬂuctuations
are enhanced as compared to the equilibrium distribution ,
see Figure 3. The bias factor determines the magnitude of
the ﬂuctuation enhancements and also how fast the height
Hk of the deposited Gaussian kernels goes to zero.
By taking the logarithm on both sides of Equation 70, we
can view the well-tempered distribution as sampling an effec-
tive FES, Aγ(z) = 1
γ A(z) (up to a constant), where the barriers
have been reduced by a value corresponding to the selected
bias factor, as we can see in Figure 3. This gives a rule of
thumb for selecting the bias factor: it should be chosen such
that the barriers become on the order of the thermal energy
so that the system can easily migrate between metastable
states on the simulation timescale.
In the metadynamics literature, a temperature parameter
∆T is often used instead of the bias factor γ. Their relation is
given by γ = (T + ∆T)/T. We can re-write the well-tempered
distribution in Equation 70 as
˜ρ(z) =
exp
h
–
1
kB(T+∆T)A(z)
i
R
exp
h
–
1
kB(T+∆T)A(z)
i
dz
=
e– ¯β A(z)
R
e– ¯β A(z) dz
,
(71)
where ¯β =

kB(T + ∆T)
–1. This introduces another interpreta-
tion of well-tempered metadynamics. We can view the well-
tempered distribution as sampling the CVs at a higher tem-
perature T + ∆T (but with the FES A(z) ﬁxed).
Like most CV-based enhanced sampling methods, meta-
dynamics cannot work with too many CVs. In practical appli-
cations, we are generally limited to biasing three to four CVs.
However, there are variants of metadynamics that allow us to
employ a large number of CVs, see Sections 7.2.1 and 11.1.3.
Obtaining the free energy surface
The FES can be estimated directly from the bias potential at
time t through Equation 69 by summing up the deposited
22 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Figure 3. Prototypical behaviour of a well-tempered metadynamics simulations. Shown are results for a model energy landscape given by
A(z) = 2z4 – 8z2 + 0.1918z where the metadynamics simulation is performed using β = 0.5 and γ = 4. (Left top panel) Time series of the added
Gaussian kernels (Left bottom panel). The height Hk of the added Gaussians goes down as the simulations proceeds (see equations 65
and 68). (Right top panel) The FES A(z) and FES with added bias potential A(z)+Ubias(z) shown for different number of added Gaussians (colors
to correspond to the time series on the left side). The bias potential only partially cancel out the FES at convergence, leading to sampling on
an effective FES Aγ(z) = A(z) + Ubias(z) = 1
γ A(z) where the barriers have been reduced by a factor of γ. (Right bottom panel) At convergence,
the CV are sampled according to the well-tempered distribution where ﬂuctuations are enhanced as compared to the unbiased distribution
(see Equation 70). The bias factor γ determines how much we enhance the ﬂuctuations. The ﬁgure is inspired by Figure 1 in Ref [132]
Gaussians
A(z) = –

γ
γ – 1

Ubias
t
(z) + 1
β ln
Z
exp

γ
γ – 1βUbias
t
(z)

dz
= –

γ
γ – 1

n
X
k=1
Hk exp

–1
2
d
X
i=1
(zi – zk,i)2
σ2
i


+ 1
β ln
Z
exp

γ
γ – 1βUbias
t
(z)

dz,
(72)
where the second term is a time-dependent constant and al-
lows us to assess the FES’s convergence and measure error
bars [101, 135]. Here, the two terms will increase in mag-
nitude with time, the ﬁrst term with the sum over the Gaus-
sians will become more negative, while the second term with
the integral will become more positive. However, their sum
will converge. Alternately, a common procedure to assess
convergence is to compare FESs obtained at different times
by aligning their minimum to zero.
We can also obtain the FES via various post-processing
reweighting procedures [135–141]. The most common way
to achieve this task is the so-called c(t) reweighting proce-
dure [135, 136] that takes the time-dependence of the bias
potential into account.
Starting from Equation 30, we can re-write the biased con-
ﬁgurational distribution at time t as
˜νt(x) = ν(x) exp

–β
h
Ubias
t
(ξ(x)) – c(t)
i
,
(73)
where the time-dependent constant c(t) is the logarithm of
the ratio of the unbiased and biased partition functions (see
Equation 31),
c(t) = 1
β ln Z
˜Z(t)
.
(74)
Here the biased partition function ˜Z(t) is explicitly time-
dependent due to dependence on the adaptive metady-
namics bias potential.
The unbiased and biased partition
functions can be written as integrals over either the coordi-
nates x or the CVs z, that is,
Z =
Z
e–βU(x) dx =
Z
e–βA(z) dz,
(75)
and
˜Z(t) =
Z
e–β
h
U(x)+Ubias
t
(ξ(x))
i
dx =
Z
e–β
h
A(z)+Ubias
t
(z)
i
dz.
(76)
We can thus write the time-dependent constant c(t) as
c(t) = 1
β ln Z
˜Z(t)
= 1
β ln
R
e–βA(z) dz
R
e–β

A(z)+Ubias
t
(z)

dz
.
(77)
23 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

In practice, c(t) can be estimated using
c(t) = 1
β ln
R
exp
h
γ
γ–1βUbias
t
(z)
i
dz
R
exp
h
1
γ–1βUbias
t
(z)
i
dz
,
(78)
which is obtained by inserting Equation 72 into Equa-
tion 77 [101, 135].
The biased conﬁgurational distribution (Equation 73) and
the c(t) (Equation 78) allows us to reweight and calculate any
average of an observable as
⟨O(x)⟩=
⟨O(x) eβ
h
Ubias
t
(ξ(x))–c(t)
i
⟩˜U
D
eβ

Ubias
t
(ξ(x))–c(t)
E
˜U
,
(79)
where ⟨· · · ⟩˜U indicates an ensemble average in the biased
metadynamics simulation (see Equation 33). To perform the
c(t) reweighting in practice, we weight each conﬁguration x,
taken at time t, with weighting factor eβ
h
Ubias
t
(ξ(x))–c(t)
i
acting
at time t. We can think of the sum Ubias
t
(ξ(x)) – c(t) as a rela-
tive (or re-normalized) bias potential (the relative bias poten-
tial will converge while the two terms will still increase). This
reweighting procedure assumes that the relative bias poten-
tial is quasi-stationary. Therefore, in practice, we ignore a
short initial transient part of the simulation where the rela-
tive bias potential is not converged and still changing consid-
erably.
Another reweighting procedure that avoids the calcula-
tion of the time-dependent constant c(t) is the so-called last
bias reweighting [137]. In this case, we use weights obtained
using the bias potential at the end of the simulation, in other
words, the last bias potential. An average of an observable is
then calculated as
⟨O(x)⟩=
⟨O(x) eβUbias
t=tF (ξ(x))⟩˜U
⟨eβUbias
t=tF (ξ(x))⟩˜U
,
(80)
where tF is the time at the end of the simulation.
Using O(x) = δ(z – ξ(x)) in Equation 79 or 80 (see Equa-
tion 15), allows us to use reweighting to estimate the FES as
a function of the biased CVs, or for any other set of CVs. It is
a good practice to compare the FES estimated from the bias
potential via Equation 72 to the FES estimated using reweight-
ing. They should give the same results and a major disagree-
ment would indicate that the simulation is not converged or
that the CVs might not be good enough. Furthermore, we
can estimate error bars for reweighted FESs using block av-
eraging (see PLUMED tutorials [142]).
Other post-processing methods for obtaining the FES
include mean force integration [140] and a weighted his-
togram analysis method adapted to metadynamics [141],
which both allow for estimating the FES by combining differ-
ent independent metadynamics simulations. Metadynamics
has also been combined with Gaussian Process Regression
for obtaining the FES [143]. See the references for further
details on these post-processing methods.
Conventional metadynamics
In conventional (non-well-tempered) metadynamics [99],
which is the original formulation of metadynamics, the
height of Gaussian kernels is kept ﬁxed throughout the
simulations.
We can view this as the limit γ
→
∞for
well-tempered metadynamics. At convergence, this leads to
a bias potential that oscillates around the negative of the
FES. Thus, on average the bias potentials cancels out the FES
and a uniform CV sampling is obtained. In this case, the FES
should be estimated as the negative of a time-average of the
bias potentials [133]
A(z) = –
1
n – n0
n
X
k=n0
Ubias
k
(z).
(81)
The accuracy of this estimate will depend on how well the bi-
ased CVs are adiabatically separated from the dynamics of
the other variables [144, 145], see Ref [133] for further dis-
cussion regarding this point.
In conventional metadynamics simulations, it is a com-
mon practice to introduce restraining walls (i.e., bias poten-
tial) to avoid exploring unimportant free energy regions. This
is generally not needed in well-tempered metadynamics sim-
ulations as the bias factor and the well-tempered distribution
naturally limits the extent of CV space exploration.
Multiple walkers metadynamics
A way to reduce the wall-clock time for convergence and
make a better usage of modern parallel HPC resources is
to employ multiple walkers or replicas [146]. The walkers
collaboratively sample the free energy landscape and share
a bias potential that is constructed by considering the
Gaussians deposited by all the walkers.
Well-tempered ensemble
The well-tempered ensemble [147] is obtained by using the
potential energy U as CV within well-tempered metadynam-
ics. It is an example of a category of ABP methods that use
the potential energy as a CV that we will discuss in more de-
tail below in Section 7.2.3. In the well-tempered ensemble,
we obtain at convergence a statistical ensemble deﬁned by
˜ρ(U) ∝[ρ(U)]1/γ where the potential energy U has the same
average as the canonical one but mean square ﬂuctuations
ampliﬁed by a factor of γ (assuming that ρ(U) is roughly Gaus-
sian). By tuning γ, we can interpolate between the canoni-
cal ensemble (γ = 1) and the multicanonical [148] ensemble
(γ →∞). In Section 11.1.4, we show how this property of the
well-tempered ensemble can be used to our beneﬁt when
24 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

combined with parallel-tempering (see Section 8). The well-
tempered ensemble can also be used to obtain thermody-
namic properties like the density of states [149]. It has been
shown [150] that the well-tempered ensemble can be mathe-
matically related to the Wang-Landau method [151] and sta-
tistical temperature molecular dynamics [152].
Parallel-bias metadynamics
Parallel-bias metadynamics [98] allows for biasing a large
number of CVs simultaneously.
This is done by consid-
ering
many
low-dimensional
bias
potentials
(generally
one-dimensional), each biasing a separate CVs, all acting
within a single simulation. The metadynamics bias potential
update step is modiﬁed to account for the effect of the
bias potentials on each other. Thus, the method yields the
correct low-dimensional free energy proﬁle for each CV. The
method has been extended to bias CV sets where the CVs
can be considered identical or indistinguishable [153].
Infrequent metadynamics
Well-tempered metadynamics has been extended for obtain-
ing kinetics of rare events in a so-called infrequent metady-
namics method [154]. The idea is to reduce the deposition
rate, in other words increase NG, the number of simulation
steps between depositing Gaussians. In this way, we can
avoid adding bias to the transition state region. If there is no
bias acting on the transition state, we can rescale the time
using the ideas of hyperdynamics [155] and conformational
ﬂooding [156]. The physical time for a rare event to happen
tb is obtained by summing up the MD steps and scaling the
MD time step dt by the bias acting at each step
tb =
X
i
dt eβUbias
t=ti (z(ti)),
(82)
where ti = i dt.
By performing multiple simulations, one
can obtain a distribution of escape times from a long-lived
metastable state. It is possible to assess the reliability of the
kinetics by performing a statistical analysis to test how well
the obtained escape time distribution follows the expected
time-homogeneous Poisson distribution [157].
An exten-
sion on the idea of infrequent metadynamics is frequency
adaptive metadynamics [158] where the deposition rate
is adjusted on the ﬂy during the simulation.
In addition,
various other approaches and strategies for obtaining
rare-event kinetics from metadynamics simulations have
been introduced in the literature [22, 159–166].
Adaptive Gaussian metadynamics
In the adaptive Gaussian variant [137], the shape of the de-
posited Gaussian biasing kernel is not kept ﬁxed but changes
with time and adapts to the features of the underlying FES,
by employing an off-diagonal variance matrix that is dynami-
cally adjusted according to the estimated shape of the under-
lying free energy landscape. Employing adaptive Gaussian
can improve the performance in some cases, for example, if
the FES’s metastable states differ considerably in their shape,
or if the CVs are highly coupled.
Other variants of metadynamics
An non-exhaustive list of other extensions and variants
of metadynamics that have been introduced throughout
the years includes reconnaissance metadynamics [167],
λ-metadynamics
[168],
ﬂux-tempered
metadynam-
ics
[169,
170],
path-metadynamics
[171,
172],
funnel
metadynamics [173, 174], algorithms for boundary cor-
rections [175], transition-tempered metadynamics [176],
metabasin metadynamics [177], experiment directed meta-
dynamics
[178]
ensemble-biased
metadynamics
[179],
target
metadynamics
[180],
µ-tempered
metadynam-
ics
[181],
adaptive-numerical-bias
metadynamics
[182],
altruistic metadynamics [183, 184], metadynamics for au-
tomatic sampling of quantum property manifolds [185],
and metadynamics with scaled hypersphere search for
high-dimensional FES [186].
We refer the reader to the
references for further details. Metadynamics has also been
used in various types of hybrid methods as discussed in
Section 11.
Public implementations of metadynamics
Various variants of metadynamics are implemented in
the PLUMED 2 enhanced sampling plug-in [78, 187, 188].
PLUMED also includes numerous practical tutorials show-
ing how to perform and analyze metadynamics simula-
tions [142]. Furthermore, a large number of example input
ﬁles are available in the PLUMED-NEST [188, 189]
Metadynamics are also available in the external libraries
SSAGES and the Colvars module (see Table 3)., and also is
natively implemented in some MD codes such as CP2K and
DESMOND, see Table 2 and also Table 2 in Ref [133].
7.2.2
Variationally enhanced sampling
A more recently introduced ABP method is variationally en-
hanced sampling (VES) that is based on a variational princi-
ple [102, 103]. In variationally enhanced sampling, a bias po-
tential Ubias(z) is constructed by minimizing a convex func-
tional given by
Ω[Ubias] = 1
β ln
R
e–β
h
A(z)+Ubias(z)
i
dz
R
e–βA(z) dz
+
Z
ptg(z) Ubias(z) dz, (83)
where ptg(z) is a so-called target distribution that is chosen
by the user. The Ω[Ubias] functional has a global minimum
given by
Ubias(z) = –A(z) – 1
β ln ptg(z) + C,
(84)
25 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

where C is an unimportant constant. This bias potential re-
sults in a biased CV distribution that is equal to the target
distribution, ˜ρ(z) = ptg(z). Therefore, the target distribution
determines the CV sampling that is obtained when minimiz-
ing Ω[Ubias]. By choosing a target distribution that is easier
to sample than the equilibrium distribution , we enhance the
sampling of the CVs. Furthermore, we can directly obtain the
FES from the bias potential through Equation 84.
To obtain a better understanding of the functional in
Equation 83, we can rewrite it as [103, 190]
βΩ[Ubias] = DKL(ptg||˜ρ) – DKL(ptg||ρ),
(85)
where DKL(p||q) =
R
p(x) ln p(x)
q(x) dx is the Kullback-Leibler
divergence between a probability distribution p(x) and a
probability distribution q(x) (or more correctly, from q(x)
to p(x) as the Kullback-Leibler divergence is not symmet-
ric with respect to its arguments).
Therefore, minimizing
Ω[Ubias] is equivalent to minimizing the Kullback-Leibler di-
vergence (or the relative entropy or cross entropy) between
the target distribution ptg(z) and the biased distribution
˜ρ(z) ∝e–β
h
A(z)+Ubias(z)
i
[191–194]. Note that the second term,
DKL(ptg||ρ), is independent of Ubias(z) and is a constant for a
given target distribution ptg(z).
In practice, Ω[Ubias] is minimized by introducing a func-
tional form for the bias potential Ubias
α
(z) that depends on a
set of variational parameters α. We then go from an abstract
functional minimization to a minimization of the multidimen-
sional function Ω(α) = Ω[Ubias
α
]. The elements of the gradient
∇Ω(α) are deﬁned as
∂Ω(α)
∂αi
= –
*
∂Ubias
α
(z)
∂αi
+
˜Uα
+
*
∂Ubias
α
(z)
∂αi
+
ptg
.
(86)
The ﬁrst term is an ensemble average obtained in the biased
ensemble given by the biased potential energy ˜Uα(x) = U(x) +
Ubias
α
(ξ(x)), and the second term is an average over the tar-
get distribution ptg(z). The gradient is noisy due to the need
of estimating the ﬁrst term from a biased simulation. There-
fore, it is better to employ stochastic optimization methods
to minimize Ω(α) by iteratively updating the bias potential.
The most general bias representation is to use a linear
expansion in some set of basis functions
Ubias
α
(z) =
X
k
αk · fk(z).
(87)
This could for example be a tensor product of one-
dimensional basis functions like Chebyshev or Legendre
polynomials, or wavelets.
In particular, localized wavelet
basis functions have been shown to perform the best [195].
A neural network bias potential has also been used [196].
Furthermore, one can use bespoke bias potentials like a
model of the free energy proﬁle [197, 198].
The target distribution ptg(z) can be chosen freely by the
user. However, one needs to keep in mind that it should be
chosen such that the sampling is easier than in the equilib-
rium distribution . The most straightforward choice would
be a uniform target distribution, such that the aim to com-
pletely cancel out the FES and ﬂatten the sampling in the
CV space. However, that is generally not optimal. Instead
it is better to only enhance CV ﬂuctuations to a certain de-
gree and thus only partly cancel out the FES [199]. We can
achieve this by employing a well-tempered distribution as in
Equation 70 (see Figure 3). The bias factor γ determines how
much we enhance CV ﬂuctuations. The well-tempered distri-
bution is unknown a priori so it needs be determined itera-
tively [199].
Note that in variationally enhanced sampling, we gener-
ally do not need to account for time-dependent constants
as in metadynamics when performing reweighting. It is suf-
ﬁcient to include the bias potential in the weights (see Equa-
tion 33). In other words, we can obtain the unbiased equilib-
rium average of an observable as
⟨O(x)⟩=
⟨O(x) eβUbias
α (ξ(x))⟩˜Uα
D
eβUbias
α (ξ(x))E
˜Uα
,
(88)
where ⟨· · · ⟩˜Uα indicates an ensemble average in the biased
simulation This reweighting procedure assumes a quasi-
stationary bias potential and thus is valid after a short initial
transient, i.e., once the variational parameters α are no
longer changing substantially.
The FES can be estimated directly from the bias poten-
tial though Equation 84. Furthermore, the FES, both for the
biased CVs and for any other CVs, can be obtained through
reweighting by using O(x) = δ(z–ξ(x)) in Equation 88 (see Equa-
tion 15). For the reweighted FES, we can also estimate error
bars. As for metadynamics, it is a good practice to estimate
the FES both from the bias potential and through reweight-
ing and compare the results. Furthermore, in cases where
the bias potential might not have suﬃcient variational ﬂexi-
bility to represent the underlying FES, the reweighted results
can be more accurate.
Variationally enhanced sampling has been extended to
obtaining kinetics of rare events [200], using the same prin-
ciples of hyperdynamics [155] as used in infrequent metady-
namics. The idea is to use variationally enhanced sampling
to construct a bias potential that only ﬂoods the FES up to
some given cutoff value. If the ﬂooding bias potential does
not touch the transition state, we can rescale the biased sim-
ulation times using Equation 82 as in infrequent metadynam-
ics. It is convenient and useful to combine the ﬂooding bias
potential with infrequent metadynamics as done in Ref [201].
Furthermore, variationally enhanced sampling has been
26 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

extended in various ways, for example: to obtain parameters
for phenomenological coarse-grained model [190]; to per-
form Monte Carlo renormalization group simulations [202–
204]; to perform multithermal-multibaric simulations [205];
for enhanced sampling targeting transition states [206]; and
multiscale simulations for sub-optimal CVs [207].
Variationally enhanced sampling is implemented in the
VES Code module of PLUMED 2. The VES Code also includes
practical tutorials showing how to employ the method [142].
7.2.3
Wang-Landau and other adaptive biasing
potential methods using the potential energy
as a collective variable
A certain category of enhanced sampling methods can
be loosely deﬁned as adaptive biasing potential methods
using the potential energy U as a CV, even though they
might not be explicitly formulated in terms of a bias poten-
tial. The most well-known example in this category is the
Wang-Landau method 3 [151].
What unites many of these methods is that they aim to
sample a potential energy distribution ptg(U) (i.e., a target
potential energy distribution) that is broadened or expanded
as compared to the unbiased distribution corresponding to
the simulation temperature. Thus, they can be considered
as sampling from a ensemble where the sampling should be
easier 4. A well known example of this is the multicanonical
(or multithermal) ensemble where the aim to sample the po-
tential energy near uniformly in a given range [148]. The idea
can also be extended to multithermal-multibaric simulations
by incorporating the volume as a variable whose dynamics is
biased or modiﬁed [208, 209]. Furthermore, sometime only
a subset of the potential energy is used to better focus the
sampling enhancement [210]. Other methods in this cate-
gory include for example the multicanonical ensemble [148],
statistical temperature MD [152], metadynamics using the
energy as CV [147, 211] (which is the well-tempered ensem-
ble discussed in Section 7.2.1 when using well-tempered
metadynamics, see also Ref [149]), integrated tempering
sampling [212, 213].
Furthermore, variationally enhanced
sampling and on-the-ﬂy probability-enhanced sampling have
been extended to perform simulations in the multithermal
and the multithermal-multibaric ensembles as described in
Refs [205, 214] and [31], respectively.
To achieve the given target potential energy distribution
ptg(U), these methods employ some kind of adaptive or
iterative scheme to estimate a priori unknown quantities.
For example, these unknown quantities can be the conﬁgura-
3The Wang-Landau procedure for converging weights can also be used with
other algorithms with discrete states, such as the expanded ensemble algo-
rithms; see Section 8.2.1 for further details.
4sometimes called a generalized ensemble in the literature, see Section 8.
tional density of states, which is the case in the Wang-Landau
method [151, 152, 215], factors or weights in sum over Boltz-
mann factors at different temperatures [31, 212], or a free
energy as in the case of well-tempered metadynamics. As
noted above, these methods are not necessarily explicitly
formulated in terms of a bias potential. Nevertheless, we
can still associate an effective bias potential as function of
the potential energy to them
Ubias(U) = –A(U)– 1
β ln ptg(U) = –U+ 1
β ln Ω(U)– 1
β ln ptg(U), (89)
where Ω(U) =
R
δ(U – U(x)) dx is the conﬁgurational density
of states that is related to the free energy as function of U
through A(U) = U – 1
β ln Ω(U), and we ignore unimportant con-
stants. Therefore, we can loosely deﬁne these methods as
adaptive biasing potential methods using the potential en-
ergy U as a CV. From this equation, we can also see a certain
theoretical equivalence between methods that work with the
density of states, such as Wang-Landau sampling and statis-
tical temperature MD, and methods that work with the free
energy, such as metadynamics, see Ref [150] for further dis-
cussion on this point.
Furthermore, closely related to this category are acceler-
ated MD [93] and Gaussian-accelerated MD [94, 95]. These
methods employ a bias potential acting on the potential en-
ergy (or generally a subset of it, e.g., only the dihedral angle
terms of the potential energy) to lower the energy barriers
of speciﬁc transitions. These methods do not directly aim to
sample a given target distribution, but rather employ a bias
potential (e.g., a harmonic potential in the case of Gaussian-
accelerated MD) that is designed to lower energy barriers of
speciﬁc transitions. The bias potential depends on a few so-
called boost parameters that are generally adaptively deter-
mined through a series of equilibration runs.
7.3
Adaptive biasing force (ABF)
The adaptive biasing force method (ABF) [62, 216] belongs
to the same category of methods as the adaptive biasing po-
tential methods 1. Contrary to those, however, it is rooted in
free energy estimation by thermodynamic integration (Sec-
tion 3.3). In ABF, the gradient of the free energy with respect
to selected CVs z = ξ(x) is estimated, and that estimate is
used to apply a time-dependent external force FABF
t
(z) that
counteracts the estimated free energy gradient, enhancing
sampling along those coordinates.
The principle is the following (Figure 4):
1. A small number of collective variables slow degrees of
freedom z = ξ(x) are chosen.
2. In the simulation, the gradient of the free energy sur-
face A(z) is estimated as:
∇zA(z) = –

Fξ(x)

ξ(x)=z .
(90)
27 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

timescale
µs
ns
ms
z
z
A(z) + bias
A(z)
rare event
apply counter-force
erase barriers
well-sampled
measure mean force
Figure 4. Principle of the ABF method. The native free energy sur-
face (solid black line, right) shows high barriers, resulting in long tran-
sition time scales. In ABF the mean force (red arrow) is measured and
countered by the biasing force (blue arrow). As a result, the biased
free energy surface (dashed black line, left) approaches a uniform
one, and the transition time scales become short enough to be well-
sampled in simulations.
That is, as the conditional ensemble average of a collec-
tive force Fξ(x) at a given value of ξ(x).
3. When suﬃcient sampling (determined by the adapta-
tion rate) is collected to have a reliable estimate of the
average force at the current value of z, a biasing force
FABF
t
(z) equal to the opposite of this average is applied.
4. This force cancels out, on average, the forces act-
ing along z, leveling the free energy barriers and
accelerating diffusion in collective variable space.
5. At convergence, the biasing force is the negative of the
gradient of the free energy, and z experiences a ﬂat ef-
fective free energy landscape. The gradient can be in-
tegrated numerically to estimate the free energy land-
scape itself.
Thus the core of ABF is an adaptation method to calcu-
late a time-dependent biasing force FABF
t
that, at long times,
converges towards the free energy gradient ∇zA, or an ap-
proximation of it.
The following optional components of an ABF method can
be used to obtain reliable free energies:
1. a different a posteriori estimator of the free energy gra-
dient (especially if the one used for biasing is approxi-
mate, see Sections 7.3.3 and 7.3.4);
2. a method to integrate the free energy gradients and ob-
tain the free energy surface [66].
In what we will call standard ABF (Section 7.3.1), a single
simulation is run at a time and the exact free energy deriva-
tive with respect to the coordinate of interest is estimated
directly as an ensemble average. Variants may involve multi-
ple interacting replicas of the simulation, ﬁctitious proxy co-
ordinates, approximate gradient estimators, and additional
biasing forces or potentials. ABF methods can be shown to
converge quickly to equilibrium for well chosen reaction coor-
dinates, see for example the following mathematical analysis
for rigorous formulations [217, 218].
7.3.1
Standard ABF
In the original formulation of ABF [216, 219, 220], the
collective force is calculated based on a constraint force
calculation. That is, a constraint solver algorithm such as
SHAKE or RATTLE [221, 222] is executed as if to keep the
coordinate of choice constant.
However, the calculated
constraint force is not applied and the coordinate remains
unconstrained; instead, this force is used to determine the
collective force Fξ, which is averaged over time to estimate
the free energy derivative [216]. Whereas a usual constraint
algorithm would cancel the instantaneous collective force
acting on the coordinate, the ABF algorithm cancels the
ensemble average of this force, so that the coordinate “sees”
no force on average, but simply zero-mean ﬂuctuations.
This is equivalent to evolving on a locally ﬂat free energy
surface.
An alternate formulation that does not need an iterative
constraint solver was proposed [63], using a projection of
Cartesian forces [64]. This was the basis of the ﬁrst public im-
plementation of ABF in NAMD. These were completed with
vector formulations that allow for estimation of free energy
gradients along several collective variables, either using time
derivatives [223], or projected forces [61]. The latter is imple-
mented in the Colvars Module [77]. In this multidimensional
projected force version [61], the free energy gradient is esti-
mated using Equation 50.
In dimension greater than 1, obtaining the free energy
surface knowing an estimate of its gradients is not trivial. In
the Colvars implementation, this is done by solving a Poisson
equation [46, 66, 224].
For a detailed review of ABF, see Ref [62].
ABF dynamics has also been successfully combined with
other enhanced sampling methods to accelerate the relax-
ation of orthogonal degrees of freedom. See Section 11.4 for
details.
7.3.2
Multiple-walker ABF
In shared or multiple-walker ABF (mwABF) [225–227], several
ABF simulations of the same system using the same collec-
tive variable run concurrently, sharing their ABF data at pe-
riodic intervals, thus beneﬁtting from the exploration of all
other walkers. In the selection variant [226, 227], walkers are
replicated or killed using a selection criterion that promotes
undersampled regions of collective variable space.
7.3.3
Extended-system ABF
Extended-system ABF (eABF) is an extended-Lagrangian for-
mulation of ABF. In eABF ([46] p. 368, [228]) the coordinate is
28 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

not a collective variable z = ξ(x), but an additional variable λ,
separate from Cartesian coordinates, so that the dynamics is
now propagated in the extended space (x, λ). λ is coupled to
the collective variable by a harmonic restraint:
Uext(x, λ) = 1
2k(λ – ξ(x))2.
(91)
The main beneﬁt of eABF is that the biasing coordinate λ
is not a function of Cartesian coordinates, so all geometric
considerations raised by the projected force formalism (Sec-
tion 3.3) become moot. This makes eABF easy to implement
for any combination of collective variables, as long as their
gradients can be calculated. One limitation is that the free
energy associated to λ is close to, but not identical to that
associated to z, which is the quantity of interest. The im-
plementation of eABF in the Colvars Module [229] is com-
plemented by two free energy estimators, the corrected z-
averaged restraint (CZAR) [229], and an Umbrella Integration
estimator [228, 230, 231]. For pointers on choosing parame-
ters for eABF, refer to [229, 231].
7.3.4
Other ways to calculate the biasing force
Besides the classic ways mentioned above (constraint force,
force projection, time derivatives, eABF), other estimators
have been proposed:
1. by projection of the (noisy) gradient estimate on the
space of “true gradients” to reduce their variance: pro-
jected ABF (pABF) [60, 224], which proved to lead to
slower exploration in practice [66];
2. by ABF with Gaussian Process Regression [143] to re-
construct the free energy in a smoother, less local way
(currently no accessible, high-performance implemen-
tation);
3. by a neural network (ABF-FUNN) [232], which is imple-
mented only in SSAGES (12);
4. using an implicit, adiabatic extended coordinate: ABF
with adiabatic reweighting (ABF-AR) [233] to bypass con-
straints of the extended dynamics and minimize vari-
ance (no public high-performance implementation yet);
5. approximated as the sum of independent biasing
forces on individual collective variables: generalized
ABF (gABF) [234, 235] to handle higher-dimensional CV
spaces (this is most effective for loosely-coupled CVs).
7.3.5
Estimating convergence and diagnosing issues
in ABF simulations
In ABF simulations, convergence of the calculated free
energy gradients is reﬂected in convergence of the sam-
pling histogram towards uniformity.
A more demanding
criterion is the occurrence of numerous transition events
between metastable basins.
This indicates not only that
the free energy gradients have converged, but that the
chosen CVs capture the major slow degrees of freedom at
play. Conversely, the presence of slow-relaxing orthogonal
degrees of freedom will cause a slow drift of the estimated
gradients, and will manifest itself by the trajectory getting
stuck in a region of CV space for a long time.
Remedies
include improving the set of CVs, or complementing ABF
dynamics with other methods that help overcome orthog-
onal barriers, such as multiple walkers, metadynamics, or
Gaussian-accelerated MD (Section 11.4).
7.3.6
Public implementations of ABF
The ﬁrst public implementation of ABF [63] was a scripted
extension to NAMD [236]. This was superseded by a more
ﬂexible, multi-dimensional implementation [61] which is part
of the Collective Variables (Colvars) Module [77]. The Colvars
Module is interfaced with NAMD [236], LAMMPS [237], GRO-
MACS [238], and VMD [239] for colvar analysis [240].
An
interface of the Colvars Module with Tinker-HP [241] is
in preparation.
ABF is implemented in PMFlib [242] for
use in the sander version of AMBER. Dynamic Reference
Restraining [228] (equivalent to eABF) is implemented in
PLUMED [78]. ABF-FUNN is part of SSAGES [243].
8
Generalized ensemble and replica
exchange methods
A broad category of simulation methodologies known as
generalized ensemble [244] (also sometimes referred to
extended ensemble [245]) algorithms have become popular
over the last two decades. These methods follow a strategy
that is orthogonal to the methods presented so far: in this
class of methods, the original conﬁgurational distribution is
preserved, and the sampling is enhanced by exploiting tran-
sitions to other ensembles (Figure 1). The main algorithmic
classes in this category are replica exchange, [246] which
includes parallel tempering [247–249] and Hamiltonian ex-
change [250–253], among others, and the serial equivalent,
the method of expanded ensembles [254], which includes
simulated tempering [255, 256] and simulated scaling [257].
In both replica exchange and expanded ensemble al-
gorithms, a mixture of thermodynamic states are sampled
within the same simulation framework.
Simulations are
able to access all of the thermodynamic states through a
stochastic hopping process between these thermodynamic
states. In the rest of the discussion of both replica exchange
and expanded ensembles, we will often use “states” as
shorthand
for
thermodynamically
deﬁned
macrostates,
which all share the same conﬁguration space Σ, but each of
which have different probabilities due to the differences in
T, P, or Hamiltonian parameters.
29 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

In expanded ensemble simulations, the states are ex-
plored in a single simulation via a biased random walk in
state space; in replica exchange simulations, multiple cou-
pled simulations are carried out in parallel, and periodically
the simulations exchange thermodynamic states with each
other, keeping the same number of simulations at each
state. Both methods, if implemented correctly, and poten-
tially after an initial equilibration stage, allow estimation of
equilibrium expectations at each state as well as free energy
differences between states.
The primary reason for introducing switching between
thermodynamic states is that the transitions between these
different thermodynamic states can reduce correlation
times in conﬁgurational sampling at any given thermody-
namic state and increase sampling eﬃciency relative to
straightforward sampling of a single state. This acceleration
is because the simulations can “go around” kinetic barriers
within any of these single states. This is done by escaping
to a neighboring state where, by chance or better yet, by
design, the free energy barriers are lower. For this switching
between states to work, each thermodynamic state must
have neighbors which have a moderate overlap (perhaps
5-20%, depending on the method) in the conﬁguration
space each samples. Overall state space must be connected,
which means that there must exist pathways between all the
states.
This class of methods therefore only works when the
states are designed to have overlap, such as alchemical
intermediates, temperatures, or harmonic biasing poten-
tials, and not when the states are deﬁned by values of
a collective variable.
One can however approximate the
computation of a collective variable while still using an
overlapping states method. One can use series of harmonic
biasing functions with that are closely spaced enough to still
overlap in sampled conﬁgurations. For example, if one was
interested in the free energy as a function of the center of
mass of two molecules, one could put a series of harmonic
biases on these distance, with spring constants that allowed
the distances to ﬂuctuate by enough that neighboring
simulations would share some visited distances with each
other. Unlike the methods use partitioning of the collective
variable, one is not guaranteed to get good sampling at each
value of the collective variable if the harmonic potentials
are too spread apart, or the spring constants are too strong.
But because the states have overlap, all of the methods
of analysis and simulation used for such simulations are
the ones used for overlapping state methods. This parallel
between methods demonstrates again how many ways the
different “ingredients“ in free energy calculations can be
combined.
Because of their popularity, these algorithms for simu-
lating multiple simulations with different states and their
properties have been the subject of intense study over
recent years.
For example, given optimal weights, ex-
panded ensemble simulations have been shown to have
provably higher exchange acceptance rates than replica
exchange simulations using the same set of thermody-
namic states [258]. Higher exchange attempt frequencies
have been demonstrated to improve mixing for replica
exchange simulations [259, 260]. Alternative velocity rescal-
ing schemes have been suggested to improve exchange
probabilities [261].
Other work has examined the degree
to which replica exchange simulations enhance sampling
relative to straightforward molecular dynamics simula-
tions [262–268].
Numerous studies have examined the
issue of how to optimally choose thermodynamic states to
enhance sampling in systems with second-order phase tran-
sitions [269–275], though systems with strong ﬁrst-order-like
phase transitions (such as two-state protein systems) remain
challenging [276, 277]. A number of combinations [69, 278]
and elaborations [262, 279–282] of these algorithms have
also been explored. A few publications have examined the
mixing and convergence properties of replica exchange
and expanded ensemble algorithms with mathematical
rigor [283–286], but there remain many unanswered ques-
tions about these sampling algorithms, both in terms of
theoretical bounds and practical guidelines for how much
these methods accelerate sampling for complex molecular
systems.
In these methods, we label the different K thermody-
namic states either using an auxiliary variable λ or by an
index k. We will assume that the simulation can visit the
same conﬁgurations for each choice of λ 5. Each choice of λ
results in a sub-ensemble within this expanded ensemble, in
which we can carry out a perfectly reasonable simulation in
absence of any switching. Finally, note that in this section, we
use reduced units, as it makes it possible to use a common
framework and a coherent notation for the different replica
exchange and expanded ensemble methods, easing the
comparison between the schemes.
8.1
Replica exchange
In a replica exchange simulation, we run K separate simula-
tions, with one simulation in each of the K thermodynamic
states. In many cases, the data gathered in all of the states
is important to calculate observables, such when calculating
properties as a function of temperature, or to calculate a free
5Note however that the Boltzmann weights of any given sample with coordi-
nates x can vary signiﬁcantly between choices of λ. For example, if the K ther-
modynamic states are deﬁned by different temperatures, then the simulation
will visit all of the same sets of conﬁgurations, but low-energy conﬁgurations
will have much higher probability at lower temperatures.
30 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

energy difference. However, in other cases, only one simula-
tion actually samples a state of interest, and the other sim-
ulations are added to aid the sampling as they have faster
kinetics than the state of interest.
The current state of the replica exchange simulation at
any time can be given by specifying (X, S), where X is a vector
of the conﬁgurations of all of the replicas, X ≡{x1, x2, . . . , xK},
and S ≡{s1, . . . , sK} ∈SK is set of the state labels {1, . . . , K}
currently associated with each of the replica conﬁgurations
{x1, . . . , xK}. Then the joint probability density of the entire
set of all simulations Q is given by
Q(X, S) ∝
K
Y
i=1
νsi(xi) ∝exp

–
K
X
i=1
usi(xi)

,
(92)
where µsi and usi are the normalized probability distributions
and the reduced energies of the si state, respectively. The
conditional densities, upon specifying a particular order of
the replicas S, is given by:
Q(X|S) =
K
Y
i=1
"
e–usi (xi)
R
Ωdx e–usi (xi)
#
,
(93)
and
Q(S|X) =
exp
"
–
KP
i=1
usi(xi)
#
P
S′∈SK
exp
"
–
KP
i=1
us′
i (xi)
#.
(94)
Note that this is more complicated than the equations for a
single simulation: we need an equation that describes the
state of all of the replicas, since they all are coupled together.
With the conditional densities, we can then describe how to
make jumps in coordinate space (which just ends up being in-
dependent, standard dynamics in each replica) and jumps in
permutations of the replica (described in more detail below).
In the most standard replica exchange simulation algo-
rithms, a proposed new permutation S of the state of the
set of all systems (X, S) only considers exchanges between
states that are currently neighboring each other [247–253].
For example, one such scheme involves attempting to
exchange either the set of state index pairs {(1, 2), (3, 4), . . .}
or {(2, 3), (4, 5), . . .}, chosen with equal probability. Each state
index pair (i, j) exchange attempt is carried out indepen-
dently, with the exchange of states i and j associated with
conﬁgurations xi and xj, respectively, accepted with the
standard Metropolis probability
Paccept(xi, i, xj, j) = min
(
1, e–[ui(xj)+uj(xi)]
e–[ui(xi)+uj(xj)]
)
.
(95)
However, it is also possible to sample from the space of all
possible permutations, which can in some cases improve
sampling [42].
Because replica exchange is so general, only requiring
that the K states have the same coordinates but any reason-
able reduced potentials ui that have overlap with each other,
then there are many different variants that attempt to solve
different sampling problems by deﬁning different ui.
The most straightforward versions are parallel implemen-
tations of algorithms that already use K simulations. For ex-
ample, if one is computing some property as a function of
temperature, then one can run a number of simulations at
different temperatures, with the simulations at higher tem-
perature providing faster kinetics to all of the simulations. If
one is performing an alchemical protein-ligand binding sim-
ulation, then one typically has simulations at K values of λ,
which describe the degree of interaction of the ligand with
the rest of the system, which can be run in replica exchange.
In this case, the fully interacting ligand can escape by mov-
ing to the fully uncoupled state. If one is performing um-
brella sampling with K umbrellas, they can be run in replica
exchange, allowing simulations to move between umbrellas
as long as they are placed suﬃciently closely with suﬃciently
weak restraints to allow overlap between states.
However, even if one is only interested in a single state,
one can add higher T replicas just to cross free energy bar-
riers, or add alchemical states to allow parts of the system
to move more freely. The possibilities are virtually endless
to create different states where conﬁguration sampling hap-
pens faster.
For example, in the Replica Exchange Solute Temper-
ing [287] (REST) and REST2 [288] variants, the “temperature”
is adjusted for only a part of the system.
This is a un-
fortunately confusing misnomer, as temperature is only
rigorously deﬁned for an entire system. What this means
in practice is that the potential energy of a select part of
the system is scaled by Tm/T0 in replica m, where T0 is the
temperature of the system. In the case of REST2, the terms
of the energy function corresponding interactions between
this designated part of the system and the rest of the system
is additionally scaled by the Tm/T0, which can be shown to
sample better than the original REST [288] by keeping the
subsystem m better coupled to the rest of the simulation.
A combination of different approaches can be used in the
same set of simulations; one of the popular protocols that
the computational chemistry software company Schrödinger
has implemented for relative free energy binding is to si-
multaneously perform alchemical ligand transformations,
apply REST2 to the area around the ligand, and reduce the
torsional potentials of side chains in the binding site [289].
Various replica exchange techniques have a long history
of investigation over the last 20 years. For other analyses of
the issues, subtleties, and variants of replica exchange, see a
number of reviews such as [260, 261, 269, 275, 290–294].
31 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

8.1.1
Estimating convergence and diagnosing issues
in replica exchange simulations
One of the main limitations of replica exchange is the need to
have simulations with some overlap with each other that are
arranged to exchange. Otherwise, the swaps will occur with
too low of a probability, resulting in an ineﬃcient exchange
scheme (i.e. Paccept between some pairs of replicas in Equa-
tion 95 will tend to 0). If the replica spacing in the auxiliary
variable is chosen poorly, it is very common for many of the
replicas to remain in the same few states for the entire simu-
lation, a clear indication of poor global overlap. A necessary
(though not suﬃcient) check on the global overlap of the sim-
ulation is to make sure that each individual simulation can
travel between all of the different states, preferably multiple
times in the same simulation (see Figure 5).
If one is using temperature replica simulations, then the
width of the energy distribution of each state will scale as
roughly N–1/2, where N is the number of particles. Because
the overlap in energy decreases as systems get larger, a
tighter spacing is required for large systems and replica ex-
change becomes increasingly less eﬃcient. Indeed, not only
does each individual simulation become more expensive,
but more simulations are needed to span the same range of
T’s. For the other types of replica exchange described here,
such as those in Hamiltonian variables, the changes in ui
affect a smaller portion of the simulation or smaller number
of atoms, and thus this scaling problem is not as severe, but
the limitation of requiring overlapping states must be kept
in mind when deciding how different the differences in ui
are between systems.
A common problem, even when the average overlap
between simulations is reasonable, is to have a “bottleneck”
where the set of simulations separates into two essentially
independent set of simulations that only exchange between
themselves. This usually defeats the purpose of the replica
exchange simulation, since the simulations cannot move
between all of the states, especially since the system of
interest is often at one end of the chain of replicas, and the
“fastest” system at the other. Plotting the state index of each
of the replicas versus time can help reveal these sorts of
issues, as seen in Figure 5.
A related diagnostic quantity to look at is the matrix of
transitions between states, where each entry correspond to
the transitions from state i to state j (which ones are the rows
and which ones are columns is a matter of convention). One
generally wants each replica to transition to another state at
least 30% of the time that exchanges are proposed; if replicas
are transitioning at a slower rate, then sampling can be im-
proved by increasing the spacing; or if some transitions are
occurring more frequently than that, reallocating the spacing.
However, if the spacing is too small, then it will take too long
for each replica visit all of the states, so there is a delicate
balance.
One important note is that in the development of many
replica exchange methods, there is frequently an assump-
tion that the states contain some natural ordering, so that
one can deﬁnitively say the conﬁgurations that result from
simulations at i are more similar to the conﬁgurations gener-
ated in state i – 1 and i + 1 than they are similar to any other
states. A number of methods of choosing how to exchange
therefore assume this ordering, but a natural ordering may
not exist in the general case. Such an ordering is straightfor-
ward when states are selected points along a single alchem-
ical variable λ or temperature T, but when instead thermo-
dynamic states are deﬁned in a multidimensional space, say
both T and λ, no such ordering may exist, and some of the
schemes for exchange in replica exchange may not apply.
8.1.2
Public implementations of replica exchange
methods
Replica exchange is perhaps one of the most common gen-
eralized ensemble methods, and is implemented natively in
GROMACS, AMBER, OpenMM, CHARMM, LAMMPS, and other
MD packages. PLUMED provides some additional tools to
run and analyze replica exchange simulations as well, but it
builds on top of the native replica exchange packages.
8.2
Expanded Ensemble
Expanded ensemble simulations [254] use many of the same
concepts as replica exchange simulations, but uses only a sin-
gle simulation moving between K states. Speciﬁcally, a single
replica or “walker” samples pairs (x, k) from a joint distribu-
tion of conﬁgurations x ∈Γ and state indices k ∈{1, . . . , K}
given by,
ν(x, k) ∝egk–uk(x),
(96)
where gk is an optional (but usually necessary for effective
sampling) state-dependent weighting factor that adjusts the
relative probability of the simulation visiting each of the k
states.
The conditional distribution of the state index k given x is
speciﬁcally:
ν(k|x) =
egk–uk(x)
KP
k′=1
egk′–uk′(x)
.
(97)
If we were to sample independently in the joint (x, k) space
with all of the gk = 0 we would ﬁnd that we would be spend-
ing more time in the states with the lowest free energies fk.
If we wish to visit lower probability (i.e. higher free energy)
states, we need to adjust the compensating biases gk to in-
crease the time spent at these states. It turns out that the
32 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Figure 5. Three examples of state indices plotted as a function of time for replica exchange simulations, plotted with a color gradient on the
state index. On the left, all replicas travel between all states multiple times, suggesting good mixing of states on this timescale. In the center,
exchanges are occurring slowly and not making the trip from top to bottom, suggesting that simulation might be more eﬃcient if additional
replicas were added, reducing the spacing between replicas. On the right, we have a bottleneck with poor overlap in the middle of the state
space (speciﬁcally between states 4 and 5), preventing mixing of all the states, even though states 0–4 and 5–9 can mix readily. For the similar
expanded ensemble cases, see Figure 7.
states will all be visited equally in the long time limit if the
gk are equal to the reduced free energy fk (so-called “perfect
weights” [274]). Of course, the goal of our simulations them-
selves is often to calculate fk, so it all becomes somewhat
circular; we need to know fk to calculate fk. Thus we need
some sort of adaptive method to gradually learn fk as we
go. The choice of the appropriate iterative procedure is usu-
ally the main topic of research in expanded ensemble simu-
lations [151, 254, 255, 257, 274, 295, 296], and is discussed
below.
Generally,
such algorithms are classiﬁed as “visited
states” algorithms, because we collect statistics about the
states as we visit them, and then update our information
(Figure 6). Consider the free energies of the physical states
as holes/wells of some initially unknown depth. A ﬁctitious
random walker visits the different states labeled by k as
the simulation proceeds, dropping “dirt” into the wells, thus
gradually building up the importance weights. At the end
of the simulation, when all states are visited equally, one
counts how much “dirt” the walker has added to each state’s
weight to achieve equal sampling. The negative logarithm of
this weight is simply the free energy of the state. The basic
principle is similar to metadynamics, so expanded ensemble,
unlike replica exchange, can be considered in some ways
an “adaptive bias” method, though rather than bias being
calculated as a function of some collective coordinate, the
bias is calculated as a function of the parameter that k labels
(such as λ or temperature) that controls the thermodynamic
state.
Transitions between the subensembles labeled by indices
k or equivalently discrete values of a vector λ can most sim-
ply be performed by Monte Carlo 6. We can use any pro-
6Molecular dynamics is also possible, but we will restrict our discussion of
posal/acceptance scheme that ensures this conditional dis-
tribution is sampled in the long run for any ﬁxed x. At each
step, we can choose to sample in either k or x according to
some ﬁxed probability p. We can also alternate Nk and Nx
steps of k and x sampling, respectively. Although this Gibbs
sampling algorithm does not satisfy detailed balance, it does
satisfy the weaker condition of balance [43] which is suﬃ-
cient to preserve sampling from the joint stationary probabil-
ity distribution ν(x, k). When proposal probabilities are based
on past history, however, the algorithm will not preserve the
equilibrium distribution [297], though in some cases the de-
viations caused by the history dependence can be mitigated
with proper choices of parameters (as shown in the parallel
case of metadynamics) [298].
One possibility is to make steps to neighboring states,
much like in replica exchange; if the states have a natural
ordering, this is perfectly reasonable. However, they may
not be a natural ordering, for example when there are
multiple neighboring states in dimension higher than one,
or if temperature and external biases are combined.
If we think of sampling this joint space in conﬁguration
and state in the context of Gibbs sampling, an expanded en-
semble simulation can proceed by alternating between sam-
pling from the two conditional distributions,
ν(x|k) =
qk(x)
R
Γ qk(x) dx =
e–uk(x)
R
Γ e–uk(x) dx,
(98)
and
ν(k|x) =
egkqk(x)
KP
k′=1
egk′ qk′(x)
=
egk–uk(x)
KP
k′=1
egk′–uk′(x)
.
(99)
transitions between states to Monte Carlo for now, as dynamics in a continu-
ous λ has a number of additional subtleties and is much less common.
33 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

(a) Beginning of Simulation
(b) During Simulation
(c) End of Simulation
Figure 6. The expanded ensemble hole analogy. (6a) At the beginning the biasing weights as a function of thermodynamic state are
unknown. (6b) As the simulation proceeds, a random walker samples the states and deposits “dirt” (probability of visitation) in each location.
The short arrow over the random walker signiﬁes uneven sampling of the states as the biases are built adaptively. (6c) At the end of the
simulation, the weights, given by the height of “dirt,” in each well are equal to the free energy each point. The random walker now samples
all states equally, as illustrated by the long bar over the walker which now extends over all of state space.
Sampling from the conditional probability distribution
ν(x|k) is just standard molecular dynamics sampling that
generates time-correlated samples.
Fortunately, the sam-
pling in ν(k|x) is often much simpler. If we have discrete
states which can be enumerated, we can simply calculate
uk(x) for each state and select randomly the state k to move
to according to ν(k|x).
The probabilities of transition to
state k only depend on the reduced energy differences
∆uik(x)
=
ui(x) – uk(x), which is often much cheaper to
calculate than the entire uk(x) is.
Sampling in this way is
often signiﬁcantly easier with expanded ensemble than
with replica exchange.
In replica exchange, often each
state is only stored on a single replica, and communicating
the energy between states is complicated. With expanded
ensemble sampling, all of the states are being kept track
of in the same simulation, so calculating the conditional
probabilities of the other states requires no additional
communication.
8.2.1
Methods updating biases one state at a time
Given ways to sample between the states, we now need
methods that adaptively change the biases gk until sampling
of the different states reaches the desired ratio. The simplest
way to update biases is to do it one state at a time, where
the state that is currently visited is the one that is changed.
Other schemes will introduce updating the bias of several
schemes at a time.
Wang-Landau updating
As noted above, if subensemble weighting factors gk are
equal to reduced free energies then one will eventually
obtain even sampling of all the states [254]. One procedure
for the iterative calculation of these biasing weights is the
Wang-Landau algorithm 7 [151]. While this algorithm was
originally proposed to calculate biasing weights where
the different values of the total energy are the different
thermodynamic states, in which case these biasing weights
gk are equal to the density of states Ω(U) (see Section 7.2.3),
it has also been used as a general approach to calculate
biasing weights associated with other ensembles.
The approach is as follows. We keep track of a histogram
h(i) of visits to each thermodynamic state during the ex-
panded ensemble simulation. When a state is visited, the
corresponding histogram is updated by 1. The weight gi of
that state is updated by some user-chosen increment –δ. δ
itself is reduced by a monotonically decreasing function as
the simulation proceeds until the changes in the gi weights
go to zero. The choice of how δ decreases is discussed later.
The weight of the reference state is subtracted from all
states after each step, as only the weight differences matter
physically. As an algorithm, this is written as:
hnew(i) = hold(i) + 1,
(100)
and
gi,new = gi,old – δ.
(101)
The Wang-Landau algorithm is self-correcting; if a state is vis-
ited more frequently than it should, its weight decreases, re-
sulting in fewer visits to that state. Eventually, the weight
falls back to the correct range. In the original Wang-Landau
scheme, the δ increment is decreased during the simulation
when the histogram h(i) reaches a speciﬁed ﬂatness criteria,
meaning none of the histograms is lower than the average
occupancy, often set at 80% [151]. When a suﬃciently ﬂat
histogram is reached, δ is reduced by multiplying by a scaling
7Not to be confused with the Wang-Landau method of enhanced sampling,
see Section 7.2.3
34 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

factor 0 < s < 1, typically s = 1/2 as proposed in the original
algorithm [151], and the histograms are set to zero again.
1/t modiﬁcations to Wang-Landau
The Wang-Landau updating scheme can lead to saturation in
the error, meaning that frequently, simulations weights will
converge too quickly, and the system will get stuck in only a
subset of the possible states. Even if it does visit all states,
the updating scheme could be too slow, and thus the simu-
lation will still never reach the correct answer in the alloted
amount of time [299, 300]. Taking s closer to one will delay
this saturation at the cost of slower convergence.
To avoid this saturation of error,
Belardinelli and
Pereyra proposed a power law update to δ, independent
of histogram ﬂatness at long time scales.
This update to
Wang-Landau, called the 1/t method scales δ as 1/t, where t
is the Monte Carlo time, e.g. the number of attempted state
transitions. Belardinelli and Pereyra suggest starting with
a standard Wang-Landau algorithm, and then switching to
using weights of 1/t when δ ≤1/t [299, 300]. This need to
have the increments decrease by 1/t has also been noted by
statisticians [301], and most improved versions of updating
schemes also have this feature, even if it is not clear in the
formulation.
8.2.2
Multiple state reweighting methods
The Wang-Landau approach and its 1/t variant only update
the free energy of one state at one time, i.e. when it is visited
during the random walk in the expanded ensemble. How-
ever, in theory, it should be possible to update the weights
of multiple states at the same time, based on information
obtained at any given state.
A number of closely related methods have been devel-
oped that update multiple states simultaneously. The key to
these approaches is recognizing that the same information
needed to correctly calculate the probability of transitions be-
tween states (i.e. the transition matrix) is the same informa-
tion that is needed to calculate the free energy differences
between the same states [53, 54] (see Section 3.2).
Transition matrix approaches
The simplest way to use this concept is to directly compute
the transition matrix and calculate free energies, and thus
the weights gk to use from this matrix [302].
Assuming
one uses Metropolis Monte Carlo, rather than collecting
histograms, we collect the transitions from each state i to j
in a matrix C. This matrix can be updated with any of the
following:
1. Transitions actually performed (adding 1 if the transi-
tion occurs, 0 if it does not)
2. The probability of acceptance of a proposed move
whether or not it was accepted.
3. The probability of acceptance of any move transition
that could have been proposed, independent of the al-
gorithm actually used to perform the move.
The free energy difference between any two states i and
j can then be estimated, either at each step, or at some inter-
val, as
fij = – ln⟨Cij/Cji⟩,
(102)
and the weights to apply can simply be set from f.
Asymptotically optimal weights
We can also update all weights simultaneously by reweight-
ing the information gathered at the current state. Several
variants of this approach have been proposed. These include
the accelerated weight histogram (AWH) method [303] and
the independently developed self-adjusted mixture sampling
(SAMS) [304].
The basic idea of these variants is to use the Gibbs sam-
pler to transition between states. After Nx conﬁgurational up-
dates with ﬁxed state i, a new state j is proposed using the
Gibbs sampler probabilities α(j|x, i):
α(j|x, i) = ν(i|x),
(103)
where ν(i|x) is given in Equation 99.
In the self-adjusted mixture sampling variant, the weights
are updated at each step. Using the transition rule deﬁned
in Equation 98, the rule for updating is:
gi,new = gi,old – t–1ν(x|k),
(104)
where t is a timescale that is ideally the number of uncorre-
lated steps in (k, x) space that have been taken in the algo-
rithm so far 8.
In the accelerated weight histogram approach, a his-
togram of the weights for each state is maintained during
the simulation. The weight histogram is then updated using
the computed ν(i|x) weights:
hnew(i) = hold(i) + ν(i|x).
(105)
Every NI iterations, the simulation weights are updated
according to:
gi,new = gi,old – ln hnew(i)K
N
,
(106)
where N is the total number of samples collected up to that
point and K is the number of states.
8t is not necessarily the number of timesteps taken so far in the simulation:
if the correlation time in conﬁguration space is slow, then updating the weights
at every step, or updating them every step without a scaling factor to make
their contribution smaller might lead to premature convergence of the biases
on a subset of the conformational states.
35 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Although we do not prove it here, AWH is actually an close
approximation to SAMS. The difference is that in AWH, the
free energies are only updated every NI steps instead of ev-
ery step. The 1/t dependence in SAMS shows up as a 1/N
dependence in AWH.
8.2.3
Estimating the biasing weights near the
beginning of the simulation
One common problem with all of these near-optimal meth-
ods is that they are only optimal in the asymptotic limit, when
a large number of uncorrelated samples have been collected.
If only a small number of samples have been collected, they
may not converge quickly, and indeed may tend to keep sam-
ples in the same thermodynamic state for quite a while.
It is still not clear what the best approach is to initialize
the biasing weights until the number of samples reach the
asymptotic limit. The current general approach is to run the
original Wang-Landau approach with a large initial increment
size without reducing the weights until some predetermined
point. This predetermined point varies between approaches
and implementations. The simulation is kept in this initial
phase until the histograms are roughly equal using weights
somewhat near kBT, and then switching to a variant of the
asymptotically optimal methods.
However, it is not really
known if this is optimal, and there are still choices to make,
such as the initial bias added to each step.
8.2.4
Convergence and diagnosing problems in
expanded ensemble simulations
Unsurprisingly, measuring convergence and diagnostics for
expanded ensemble are very similar to those of replica ex-
change. A necessary (but not suﬃcient) requirement for ex-
panded ensemble simulations to have converged is if all ther-
modynamic states are being visited evenly (or, if the target
distribution is some other uneven distribution, according to
that distribution). This equal visitation does not have to be ex-
act; if the most visited states are visited twice as much as the
least visited states, the free energies will still be accurately
estimated. However, if not all states are visited, or some are
visited signiﬁcantly less (say, an order of magnitude) than oth-
ers, the free energies of the unvisited states will most likely
not be accurately estimated. Note that there is an additional
problem that can occur with expanded ensemble compared
to replica exchange that can lead to poor transitions between
states; not only can overlap between states be low, but the bi-
ases on each state can be poorly estimated, so both of these
criteria must be checked.
As with replica exchange, another important quantity to
measure is the movement of the auxiliary state variable over
the total possible values; in one dimension, this means ex-
ploring the range from the lowest to the highest values (see
Figure 7) If there is not a substantial number of transitions
across the entire range, perhaps 20–40, then it is likely that
the weights along the variable are not well-estimated. It is not
uncommon to see good mixing of states in both the upper
and lower range, but very few transitions in between these,
which usually means that the free energy differences within
the upper and lower ranges might be much better estimated
than the overall free energy difference between these two
groups of states.
Similarly to replica exchange, one can also check the
transition matrix generated from the counts or probabilities
of going from state i to state j, and one should decrease the
spacing, or reallocate the spacing from better transitioning
intervals to worse transitioning intervals, so that transitions
occur approximately 30% of the time.
As with replica ex-
change, tight spacing does increase the amount of time it
takes for each replica to move back and forth between all
states, so the proper balance must be struck.
Note that equal visitation of all states does not of course
guarantee sampling in the coordinate space. The hope is
that some expanded ensemble states can travel through the
slow degrees of freedom faster than others, but this depends
highly on how useful the deﬁnition of the auxiliary states is.
One must thus explicitly check the slow degrees of freedom
of the system to see if enhanced sampling actually occurred.
8.2.5
Public implementations of expanded
ensemble methods
Of these different variants of expanded ensemble, the
SAMS version is implemented in OpenMM [305], and GRO-
MACS [71] implements both the AWH and a version of SAMS
as “weighted Wang-Landau“ option in expanded ensemble
sampling.
9
Adaptive seeding methods
These methods are related to localization techniques in that
the sampling is enhanced by specifying starting coordinates,
in order to focus sampling on productive or undersampled
regions of conﬁguration space. Contrary to localization meth-
ods, however, the simulations are not restricted to a region
with restraints or constraints, but instead are merely instan-
tiated – and sometimes terminated – strategically (Figure 1).
9.1
Adaptive sampling
Adaptive sampling seeks to sample the conﬁguration space
by focusing simulation efforts in regions that will lead to a
more accurate description of the ensemble. It generally takes
advantage of analysis methods such as Markov State Mod-
eling (MSM) to then stitch together trajectories and build a
coherent model of, not only the equilibrium population of
36 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Figure 7. Three examples of the state index trajectory in an expanded ensemble simulation in one dimension. On the left, the simulation
travels from top to bottom state multiple times, suggesting good mixing of states on this timescale. In the center, jumps between states are
occurring slowly, and the simulation is not spanning the entire state range, suggesting the spacing between states may need to be decreased
(i.e. overlap between states increased), or weights may need to be improved. On the right, we have a bottleneck in the middle of the states,
preventing the overall simulation from mixing easily through the entire range of states. Although states and 0–4 and 5–9 can mix readily, the
overall traversal of state space is slow, and the overlap and weights of that transition should be examined to improve sampling. For similar
replica exchange cases, see Figure 5.
metastable states, but also the rates of conversion between
these states [306, 307].
Adaptive seeding methodologies were mostly developed
with the problem of protein folding in mind, a process that is
characterized by rare transitions along a pathway to a single
folded state. In such an application, observing a rare event
for which a free energy barrier needs to be crossed only de-
pends on the aggregated simulation time, not the length of
the simulation so far, because the probability of overcom-
ing a free energy barrier depends on the total number of
attempts made at crossing it [308, 309]. Adaptive seeding
thus seeks to seed simulations from regions of space that
are likely to lead to free energy barrier crossing in order to
sample the entire conﬁguration space, using the information
acquired through the sampling so far. In other words, sam-
pling over and over regions of space that have already been
explored does not add valuable information, whereas cross-
ing into regions of space not previously sampled adds valu-
able information to reconstruct probability distributions.
MSM building involves discretizing the conﬁguration
space into states (usually called microstates in the MSM
literature, at odds with the deﬁnition used in this review)
and estimating their probability distribution as well as the
probability of transitions between these states for a given
time lag τ. Discretization of the space can in principle be
done based on geometric criteria. However, since the focus
of MSM building has been to extract kinetic properties,
discretization is often performed based on kinetic proximity,
using time-lagged independent component analysis (TICA).
The transition probabilities are gathered in a transition
matrix Tij which records the probabilities of transitions from
state i to j given a time lag τ.
Estimating the probability
density directly from counting the number of conﬁgurations
falling in a state would be incorrect for a strategically seeded
ensemble. However, the fact that the transition probabilities
are taken into account enables to reconstruct the probability
density, see Section 3. These microstates are generally then
clustered into macrostates using a distance metric.
Different adaptive seeding strategies have been put
forward. They all follow the same basic algorithm: a single
or a set of relatively short MD simulations are launched.
Conﬁgurations from these simulations are grouped into
“states.”
New simulations are then started from selected
states. The different strategies then differ according to the
principle they follow to select which states to seed new
simulations from.
Arguably the ﬁrst proposed strategy
relied on selecting randomly a ﬁxed number of structures
from each macrostate [310–312]. This method was coined
adaptive seeding. In contrast, all the more reﬁned methods
derived therefrom and listed below are referred to as adap-
tive sampling. Another natural proposal has been to seed
simulation from states that contribute the most to the sta-
tistical uncertainty of MSMs built after each iteration [313].
Several variants have proposed to seed from low-population
microstates, and have been called “counts” methods. This
strategy is well-suited to enhance the exploration of space,
but not necessarily to accurately estimate the probability dis-
tribution of low free energy states [314–318]. Seeding from
low-population macrostates has also been suggested. This
is better suited to converge the free energy calculation but
will not lead to as an extensive space exploration [319, 320].
There are also methods that do not rely on MSM analysis.
iMapD relies on clustering in a low-dimensional manifold
inferred by dimensionality reduction and selecting states to
seed from the boundaries of a diffusion map in diffusion
37 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

coordinates [18] 9.
Following a similar approach, it has
been suggested to pick conﬁgurations to reseed from using
dimensionality reduction algorithms such as sketch-map
[321]. PIGS, for Progress Index Guided Sampling, uses an
unsupervised heuristic to avoid re-sampling the same region
of space by organizing simulation frames along a progress
index that connects conﬁguration to existing conﬁguration
by ﬁnding the one to which a chosen distance is minimal.
Weakly connected snapshots that have a large distance
to other conﬁgurations are used to start seeding new
trajectories [322].
Methods introducing directionality into
the sampling have also been suggested and are particulary
interesting when the target state (or set of states) is known.
Broadly speaking, those suggest seeding from states that
are close to the end state in terms of a target property:
1. AdaptiveBandit expands on the methods described
above by proposing to formulate the adaptive sam-
pling problem in terms of reinforcement learning. This
offers the promise and the computational platform
needed to increased performance and ﬂexibility of the
algorithm across different systems [323].
2. Reinforcement Learning Based Adaptive Sampling
(REAP) proposes to eﬃciently explore conﬁguration
space by using reinforcement learning to choose new
states. It does so by learning the relative importance
of candidate collective variables as it makes progress
along the landscape, in a framework that rewards ac-
tions that lead to further exploration of the landscape.
Here too, states to learn from are selected as the least
sample microstates. In that sense, it is a derivative of
the counts method [324].
3. The Fluctuation Ampliﬁcation of Speciﬁc Traits (FAST)
method proposes to choose new states based on an ob-
jective function that balances tradeoffs between explor-
ing novel regions of space (exploration) and focusing on
regions that are important to lead to a converged esti-
mate of the target properties (exploitation) [309, 325]
The parameter that balances these two aspects needs
to be tuned, a non-trivial aspect of using this method.
This method has been recognized to be a speciﬁc case
of a multi-armed bandit problem.
4. Speciﬁcally for protein folding,
or conformational
changes in biological molecules, a common target
property is inter-residue contacts, or its opposite,
minimizing non-desirable contacts [320].
5. In the same vein, it has been suggested to use
evolutionary information by deriving inter-residue
contacts from a multiple-sequence alignment, in a
method called evolutionary couplings-guided adaptive
9iMapD does not enable the direct estimation of probability distributions.
sampling [317].
Attempts at a quantitative comparison of several of these
adaptive sampling methods have been published [309, 320],
but a systematic comparison is still missing. Preliminary work
indicates that accelerating rare event is better achieved with
macrostate count or directed methods, while exploring the
space is most eﬃcient using microstate count [320]. Meth-
ods explicitly taking into account the tradeoff between ex-
ploitation and exploration can be more versatile in their us-
age but their success (measured as the convergence of the
conﬁgurational distribution, or of the MSM) will depend on
hyperparameter choice for a speciﬁc application.
9.2
Weighted ensemble simulations -
splitting/replication strategies
The weighted ensemble (WE) method, also referred to as a
splitting/replication approach, is particularly well-suited to
exhaustively ﬁnd pathways between macrostates and eval-
uate transition rates between states [326]. The approach
relies on running relatively short MD simulation and termi-
nating simulations that are not making progress towards the
target state while replicating simulations that are instead
progressing towards the end state [327]. By keeping track of
the total number of trajectories, the approach is statistically
rigorous. Clustering into pathway ensembles can provide a
rigorous estimate of the relative importance of the different
pathways.
Because of their non-equilibrium nature, this variety of
enhanced sampling methods is not particularly well-suited
to calculate equilibrium population distributions. However,
even if weighted ensemble simulations do not achieve steady
state, frameworks have been proposed to recover equilib-
rium properties [328–330].
We note that weighted ensemble methodologies, while
falling under the umbrella of adaptive seeding strategies, can
be categorized under transition path-ﬁnding methods, along
with the string method with swarms of trajectories [331],
milestoning
[332],
transition
interface
sampling
[333],
forward
ﬂux
sampling
[334],
adaptive
multilevel
split-
ting [335, 336] and supervised unbiased MD [337, 338]. We
choose here to not review these methods in detail given the
focus of this review on estimating conﬁgurational averages
(see instead reviews [4, 339, 340]). The Weighted Ensemble
method is implemented in the WESTPA software. Given the
versatility of the framework, strategies and schedules can
be easily explored [341].
10
Selective acceleration methods
In selective acceleration methods, the dynamics of slow
degrees of freedom is directly modiﬁed to accelerate tran-
38 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

sitions.
Contrary to adaptive biasing methods, this is not
achieved through a modiﬁcation of the statistical distribu-
tion sampled by the dynamics – indeed, these methods are
designed so that the sampled distribution is as close as
possible (identical, in recent variants) to the unbiased target
distribution. Such methods differ from expanded ensemble
methods in that they modify the local dynamics instead of
going through transitions to other ensembles to enhance
the sampling (Figure 1).
Timescale separation between the dynamics along the
collective variable (or the external parameter in an alchem-
ical setting) is assumed, or artiﬁcially enforced, in order to
obtain (almost) Markovian dynamics along the collective
variable (respectively the auxiliary variable). This was ﬁrst
introduced as the adiabatic free energy dynamics (AFED)
method [342].
For example, the temperature accelerated molecular dy-
namics (TAMD) [343] consists in adding an extended degree
of freedom λ, with mass mλ, and a harmonic coupling poten-
tial kext (ξ(x) – λ). The extended Langevin dynamics reads:















dx = M–1p dt
dp =

–∇xU(x) + kext(λ – ξ(x)) – γp

dt +
q
2γM
β
dWt
dλ = m–1
λ pλ dt
dpλ =

kext(ξ(x) – λ) – γpλ

dt +
r
2¯γmλ
β
dWt.
(107)
Note that the Langevin equation on λ is based on a separate
temperature factor β and friction coeﬃcient γ.
TAMD relies on the regime where γ ≫γ (the extended
dynamics is slower than the original one) and β ≫
1
kext (tight
coupling). Then the dynamics on λ becomes uncoupled from
the dynamics of x and converges [343] to an effective dynam-
ics of the form (in the overdamped case γ ≫1):
dλ = –
1
mλγ ∇A(λ) dt +
s
2mλ
β γ
dWt,
(108)
so that the probability distribution sampled by λ is propor-
tional to exp(–βA(λ)).
The artiﬁcial inverse temperature β
is then chosen so that the effective dynamics (108) is less
metastable. The practical diﬃculty of such a method lies in
choosing the numerical parameters (γ, γ, kext, β) for the algo-
rithm to be eﬃcient while keeping the adiabatic separation
between the extended coordinate and the physical system –
failure to do so introduces a bias in the simulation.
In the “single sweep” method, TAMD is used to estimate
local free energy gradients, followed by estimation of the
free energy surface [344]. Driven-AFED (d-AFED) [345], uni-
ﬁed free energy dynamics (UFED) [346], canonical adiabatic
free energy sampling (CAFES) [347] and on-the-ﬂy free
energy parameterization (OTFP) [348] are all related to this
scheme.
The family of methods known as self-guided Molec-
ular/Langevin Dynamics (SGMD [349] and SGLD [350])
shares with these “adiabatic” methods the idea of selectively
enhancing the dynamics while preserving (at least approxi-
mately) the statistical distribution sampled by the trajectory.
In SGLD, Langevin Dynamics is augmented with a biasing
force that accelerates slow degrees of freedom, computed
as a running time average of the momenta, which acts as
a low-pass ﬁlter.
This process pinpoints slow degrees of
freedom without having to deﬁne them a priori.
This accelerates sampling, but creates a bias in conﬁg-
urational statistics that increases with the strength of the
acceleration applied, and has to be corrected a posteriori
to recover accurate averages. However, SGLD with a gen-
eralized Langevin equation (where stochastic forces are
not white noise but obey a given memory kernel) [351]
recovers the detailed balance property of standard Langevin
dynamics, and as a result, is able to sample the NVT and NPT
ensembles in an unbiased way. Recently, variants of SGMD
and SGLD have been proposed to combine optimally biases
based on momenta and on forces [352]. SGMD and SGLD
are implemented in the CHARMM and AMBER simulation
packages.
Note also that conventional (non well-tempered) metady-
namics (see Section 7.2.1) implicitly makes an adiabatic hy-
pothesis to get an unbiased free energy estimator [144, 145].
11
Hybrid methods
Enhanced sampling methods leveraging different principles
can be combined together leading to hybrid schemes. For
example, a common theme is to combine an enhanced sam-
pling method that focuses on biasing speciﬁc degrees of free-
dom or CVs (e.g., ABP methods such as metadynamics) with
a enhanced sampling method that more generally enhances
the sampling of a large number of, or even all, degrees of
freedom (e.g., replica exchange methods). In this way, one
can better sample slow orthogonal degrees of freedom that
are missing in the biased CV set. Another common combi-
nation is to complement a sampling method with an exter-
nal free energy estimator. In addition, path ﬁnding methods
like the string-of-swarms method can be combined with, e.g.
umbrella sampling along the discretized minimum path ob-
tained by the path ﬁnding method [331]. This is followed by
free energy estimation along path coordinates.
There are so many possible combinations of enhanced
sampling methods that it is diﬃcult to offer a comprehensive
discussion of all hybrid methods. Thus, we limit ourselves to
discuss a few notable combinations below.
39 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

11.1
Combination of replica exchange and
external biasing potential methods
Many hybrid enhanced sampling methods are a combination
of Hamiltonian replica exchange and methods incorporating
an external bias potential, which can be static or adaptively
updated (e.g., umbrella sampling and metadynamics). Each
replica i includes its own bias potential Ubias
i
(z), which
depends on a collective variable set zi = ξi(x) that can differ
between replicas.
Within each replica, the bias potential
is kept static or evolved according to the adaptive biasing
potential method.
When calculating the acceptance prob-
ability Paccept(xi, i, xj, j) for an exchange of conﬁgurations xi
and xj between two replicas i and j given in Equation 95,
we need to take the bias potentials into account. We limit
ourselves to the, still rather general, case that all replicas
have same potential energy function U(x) but can have
different temperatures.
We start by rewriting the exchange acceptance probabil-
ity given in Equation 95 as 10
Paccept(xi, i, xj, j) = min
(
1,
exp(–[βiU(xj) + βjU(xi)])
exp(–[βiU(xi) + βjU(xj)])
)
= min

1, exp
  βi – βj
 
U(xi) – U(xj)
	
= min

1, exp ∆i,j
	
,
(109)
where we have deﬁned ∆i,j =
 βi – βj
 
U(xi) – U(xj)

as the
exponential term for conventional replica-exchange.
Incorporating the effect of the bias potentials,
the
exchange acceptance probability is calculated using the
exponential term
∆i,j =
 βi – βj
 
U(xi) – U(xj)

+ βi
h
Ubias
i
(ξi(xi)) – Ubias
i
(ξi(xj))
i
+ βj
h
Ubias
j
(ξj(xj)) – Ubias
j
(ξj(xi))
i
,
(110)
where the two last terms originate from the effect of the bias
potentials acting on the two replicas [353]. In the following
we discuss a few speciﬁc cases.
11.1.1
Replica exchange umbrella sampling
In replica exchange umbrella sampling [354], all the replicas
are simulated at the same temperature but differ in the fact
that the different replicas have their umbrella potential cen-
tered at different locations. Thus, by allowing exchanges be-
tween neighboring umbrella windows, the convergence is im-
proved. The exchange also helps with sampling orthogonal
10Note that we use here full quantities rather than reduced quantities as in
Section 8. However, the discussion is also valid for canonical and isothermal-
isobaric ensembles.
slow degrees of freedom not included in the CV set. The ex-
change probability is obtained using
∆i,j =β
h
Ubias
i
(ξ(xi)) – Ubias
i
(ξ(xj)) + Ubias
j
(ξ(xj)) – Ubias
j
(ξ(xi))
i
,
(111)
where the bias potential correspond to different umbrella
sampling windows (generally neighbouring windows) that
are centered at different locations in CV space.
11.1.2
Parallel-tempering metadynamics
Metadynamics can be combined with parallel-tempering to
help with sampling missing slow orthogonal degrees of free-
dom not included in the biased CV set [353]. The exchange
probability is obtained using
∆i,j =
 βi – βj
 
U(xi) – U(xj)

+ βi
h
Ubias
i
(ξ(xi)) – Ubias
i
(ξ(xj))
i
+ βj
h
Ubias
j
(ξ(xj)) – Ubias
j
(ξ(xi))
i
.
(112)
The same idea can also be used for other ABP methods (e.g.,
variationally enhanced sampling). In a similar way, metady-
namics (and other ABP methods) can be combined with other
replica exchange methods such as replica exchange solute
tempering [288, 355], that have a better scaling in term num-
ber of replicas needed. Then the ﬁrst term in Equation 112
would be adjusted while the last two terms would remain the
same.
11.1.3
Bias-exchange metadynamics
Bias-exchange metadynamics [97] allows for biasing a large
set of CVs simultaneously by considering multiple replicas
running at the same simulation temperature, but each bias-
ing a different set of CVs using metadynamics. Generally, one
considers one CV per replica so the outcome are several one-
dimensional free energy proﬁles. By allowing for exchange of
conﬁgurations between replicas, we can avoid the problem
of missing slow orthogonal CVs in each replica. The exchange
probability is obtained using
∆i,j =β
h
Ubias
i
(ξi(xi)) – Ubias
i
(ξi(xj)) + Ubias
j
(ξi(xj)) – Ubias
j
(ξi(xi))
i
.
(113)
Traditionally, bias-exchange is used with conventional (non-
well-tempered) metadynamics but it can also be used with
well-tempered metadynamics. We can even imagine using
bias-exchange with other ABP methods such as variationally
enhanced sampling.
11.1.4
Parallel-tempering in the well-tempered
ensemble
Combining parallel-tempering with the well-tempered en-
semble [147] (i.e., well-tempered metadynamics biasing
40 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

the potential energy) allows to greatly reduce the number
of replicas required for parallel-tempering simulations
of solvated systems [356].
This comes from two effects.
First, within each replica potential, energy ﬂuctuations are
enhanced by a factor of γ while averages stay more or
less the same, leading to a better potential energy overlap
between replicas. Second, the ∆ij factor used to calculate
the exchange acceptance probability in Equation 109 is given
as
∆i,j =γ–1  βi – βj
 
U(xi) – U(xj)

,
(114)
and thus reduced by a factor of γ as compered to conven-
tional replica exchange (Equation 109), which leads to higher
exchange probability (if ∆i,j < 0, otherwise if ∆i,j > 0 the
exchange acceptance probability is unity).
Due to these
two effect, one can use a larger temperature difference be-
tween replicas and thus require fewer replicas overall [356].
Parallel-tempering in the well-tempered ensemble can also
be combined with metadynamics where other CVs are
biased separately [357].
11.1.5
Replica exchange with collective variable
tempering
The idea behind replica exchange with collective variable
tempering [358] is to reduce the number of replicas needed
for replica-exchange simulations by focusing only on se-
lected degrees of freedom. One considers M replicas with
the same temperature and within each replica, one performs
concurrent well-tempered metadynamics simulations where
one considers the same CV set within replica and biases
each CV by a separate one-dimensional metadynamics
potential. One can then bias a large number of degrees of
freedom (e.g., all dihedral angles). The replicas are arranged
in a ladder of increasing bias factor values where the lowest
replica corresponds to the canonical ensemble (γ = 1). Thus,
by going up the replica ladder, the ﬂuctuations of the biased
degrees of freedom are enhanced. Though one considers
a large number of degrees of freedom, this is considerably
smaller than the total number of the system’s degrees of
freedom. Therefore, the number of replicas needed is con-
siderably less than parallel-tempering, where ﬂuctuations of
all degrees of freedom are enhanced by heating the system.
11.2
Combinations of metadynamics and
other enhanced sampling methods
Apart from the replica exchange-based hybrid methods dis-
cussed in the previous section, there are various other hy-
brid methods where metadynamics has been combined with
other types of enhanced sampling methods. Furthermore,
some of the variants of metadynamics listed in Section 7.2.1
could be considered as hybrid methods, and vice versa, as
the distinction between a variant and a hybrid method is not
always so clear.
Metadynamics has been combined with methods that
enhance the potential energy sampling (see Section 7.2.3)
such as the multicanonical ensemble [359] and integrated
tempering sampling [360, 361]. Here the idea is similar as
when metadynamics is combined with parallel tempering,
this should help with sampling missing slow orthogonal
degrees of freedom.
In a similar spirit, variationally en-
hanced sampling has been combined with sampling in the
multithermal-multibaric ensemble [205, 214].
In driven metadynamics [362], metadynamics is com-
bined with steered MD. In orthogonal space random
sampling (OSRW) [363–365], metadynamics is combined
with a procedure based on thermodynamic integration
to facilitate sampling of orthogonal degrees of freedom.
Metadynamics has been combined with umbrella sampling
in various ways as we discuss in the following.
In Refs [366, 367], metadynamics is used to generate a
bias potential that leads to effective sampling and diffusion
in the CV space. The bias potential is then used as a static bias
potential in another simulations where the FES is calculated
using umbrella sampling corrections (i.e., reweighting with a
static bias potential).
In Ref [368], metadynamics is used to identify a pathway
and then the free energy proﬁle along the pathway is calcu-
lated using multiple window (localized) umbrella sampling.
In Refs [369, 370], multiple windows (localized) umbrella
sampling is used to bias some chosen CV while within each
umbrella window, metadynamics is used to bias another
set of CVs. The main idea behind this strategy is that um-
brella sampling is more suited than metadynamics to bias
CV whose free energy proﬁle is broad.
Furthermore, the
metadynamics bias potential within each umbrella window
helps to sample degrees of freedom that are orthogonal
to the CV biased in the umbrella sampling simulations and
thus improve the convergence.
This combined umbrella
sampling and metadynamics strategy has been extended
to incorporate temperature accelerated MD [371, 372] or
replica exchange with solute tempering (REST2) [373] to
further improve the sampling of orthogonal degrees of
freedom.
The basic framework of metadynamics can also be
used to update the weights in expanded ensemble simu-
lations [374]. Since the gk weights can be updated by any
methods the user might choose, one can simply use various
metadynamics techniques to update them, but as a discrete
variable rather than a continuous one.
The GROMACS
expanded ensemble implementation has been adjusted to
allow the biasing functionality to be built using PLUMED
(starting from version 2.8).
This is not particularly better
41 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

than existing expanded ensemble techniques for a single
dimension, but becomes particularly useful in multiple
dimensions, where one has one alchemical dimension, and
one or more collective variable dimensions. This allows one
to perform binding free energy calculations using metady-
namics and simultaneously accelerating transitions along
slow degrees of freedom, such as accelerating crossing of a
high intramolecular torsional barrier or overcoming a free
energy barrier in the hydration of a host pocket by ﬂattening
the distribution of the number of water molecules in that
pocket [374].
11.3
Combinations of metadynamics and
structural ensemble determination
methods
Structural ensemble determination methods [375–377], for
example based on maximum entropy principle [378, 379],
are used to integrate experimental observations into molec-
ular simulations and yield structural ensembles that are com-
patible with experiments. While such structural ensemble
determination methods are not strictly enhanced sampling
methods, they are somewhat related as they generally intro-
duce external restrains in the form of bias potentials that can
be ﬁxed or adaptively updated. To accelerate the conﬁgu-
rational sampling, structural ensemble determination meth-
ods are often combined with enhanced sampling methods
such as metadynamics.
In Ref [380], parallel-bias metadynamics is combined
with metainference [381] that is a structural ensemble de-
termination methods that incorporates experimental errors
via a Bayesian inference framework. In Ref [382], parallel-
tempering in the well-tempered ensemble (Section 11.1.4)
is combined with experiment directed simulations [383].
An older work along this line is replica-average metady-
namics [384, 385], where metadynamics or bias-exchange
metadynamics is combined with replica-averaging.
Related to the idea of structural ensemble determination
methods are experiment directed metadynamics [178],
ensemble-biased metadynamics [179], and target meta-
dynamics [180].
In these variants of metadynamics, the
bias updating procedure is modiﬁed such that the biased
CV distribution that the simulation converges to is some
predeﬁned target probability distribution. Thus, by taking
this target distribution from experimental measurements, it
is possible to obtain a structural ensemble that is compatible
with the experimental results. In a similar spirit, the target
distribution in variationally enhanced sampling [102, 103]
(Section 7.2.2) can be taken from experimental measure-
ments.
11.4
Combinations of ABF and other
enhanced sampling methods
The ABF method is applied within a deﬁned region of collec-
tive variable space, where its application leads to improved
sampling and yields an estimate of the free energy gradient.
To reduce the time needed for diffusive sampling of a large
volume of CV space, we can combine ABF with a stratiﬁcation
approach (Section 5). In this case, ABF can be applied inde-
pendently on several smaller, non-overlapping regions of the
collective variable [386] or several variables [61]. Thanks to
the local character of this gradient, the estimated gradient
in all regions can be merged by simple concatenation, and
then integrated in one piece [66]. Unlike energy-based meth-
ods like Umbrella Sampling, no particular precaution is nec-
essary to match the data from different strata (windows).
Metadynamics
has
been
combined
with
extended-
system ABF [387, 388] to improve the exploration properties
of ABF. For a review of these hybrid eABF methods, see [389].
This combination has been further extended by incor-
porating Gaussian-accelerated MD [390] to help sample
orthogonal degrees of freedom not included in the biased
CV set.
Orthogonal Space Tempering (OST) is a variant of OSRW
based on an extended-system ABF method, using a ﬁnite
sampling temperature in the orthogonal space [228]. In this
method, the ABF-like sampling of an alchemical parameter
λ is completed by enhanced sampling of the force along λ,
which correlates with slow orthogonal degrees of freedom,
thereby accelerating orthogonal relaxation.
12
Software implementations
The codes to run the various simulations outlined in the previ-
ous sections range from in-house scripts, to methods imple-
mented natively in the largely used MD simulation packages
GROMACS, AMBER, NAMD or CP2K, and via the open-access
libraries or plugins Colvars [77], PLUMED [78, 187, 188], PM-
Flib [242], and SSAGES [243]. The most popular software op-
tions for each method have been mentioned in the dedicated
sections and are summarized in Tables 2 and 3.
Several free and open source codes are gathered under
a GitHub topic: https://github.com/topics/enhanced-sampling
Contributions
J.H., O.V., T.L., M.R.S., and L.D. conceptualized the paper. All
authors wrote initial drafts, in particular with L.D. and J.H.
drafting Section 2, J.H. and T.L. drafting Sections 4 and 10, J.H.
drafting Sections 5 and 7.3, O.V. drafting Sections 7.2 and 11,
M.R.S. drafting Sections 3 and 8, L.D. drafting Section 9, and
other sections jointly drafted by all authors. J.H., O.V., M.R.S.,
42 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Table 2. Built-in capabilities of widely used molecular dynamics simulations codes. Methods not discussed in the text may be included, see
user guides for more discussion. Not an exhaustive list; enhanced sampling methods maybe be available in other MD codes.
MD engine
Main features
Reference
CHARMM
Adaptively biased path optimization (ABPO), adaptive umbrella sampling,
constraints, distributed CSA (conformational space annealing), dynamic importance
sampling (DIMS), enveloping distribution sampling method, replica exchange, free
energy perturbation, self-guided Langevin dynamics, string method, targeted
molecular dynamics, transition path sampling
[391]
NAMD
Simulated annealing, steered MD, (unconstrained variant of) targeted MD, replica
exchange, accelerated and Gaussian-accelerated MD, custom algorithms via Tcl
scripting, grid forces.
[236]
GROMACS
Restraints (various potentials including harmonic potentials for umbrella sampling),
simulated annealing, replica exchange, expanded ensemble (both as AWH and a
separate implementation), non-equilibrium pulling (steered MD), applying forces
from three-dimensional densities.
[71]
OpenMM
Simulated annealing, replica exchange (with OpenMMtools), applying external
forces, versatile Python framework to implement any scheme, expanded ensemble
(as self-adjusted mixture sampling).
[305]
AMBER
Replica exchange, targeted MD, steered MD, accelerated and Gaussian-accelerated
MD, self-guided Langevin dynamics, external forces, umbrella sampling,
string-of-swarms.
[392]
CP2K
Constraints, harmonic restraints, targeted MD, steered MD, metadynamics.
[393]
DESMOND
Umbrella sampling, metadynamics, replica exchange
[394]
LAMMPS
Harmonic restraints, applying external forces, replica exchange, parallel replica
dynamics, temperature-accelerated dynamics, original hyperdynamics, local
hyperdynamics.
[395]
HOOMD-blue
Restraints, applying external forces, versatile Python framework to implement any
scheme.
[396]
Tinker-HP
Steered MD, Gaussian-accelerated MD, umbrella sampling
[397, 398]
GROMOS
Replica exchange, umbrella sampling, thermodynamic integration, enveloping
distribution sampling.
[399]
GENESIS
Replica exchange, umbrella sampling, Gaussian-accelerated MD, restraints, targeted
MD, steered MD
[400]
SPONGE
Integrated tempering sampling, selective integrated tempering sampling,
metadynamics
[401]
and L.D. signiﬁcantly revised all sections of the paper. L.D.
and J.H. provided additional management of the process.
Acknowledgments
L.D. would like to thank the Science for Life Laboratory, the
Göran Gustafsson Foundation, and the Swedish Research
Council (Grant No. VR-2018-04905) for support. J.H. acknowl-
edges support from the French National Research Agency
under grant LABEX DYNAMO (ANR-11-LABX-0011). M.R.S. ac-
knowledges support from the National Science Foundation
under grant numbers OAC-1835720 and OAC-2118174, and
help from Arjan Kool for Figure 6 and preliminary drafts of
expanded ensemble analysis.
O.V. acknowledges support
from the Deutsche Forschungsgemeinschaft (DFG, German
Research Foundation) - Project number 233630050 - TRR
146 “Multiscale Simulation Methods for Soft Matter Sys-
tems”. O.V. thanks Benjamin Pampel for help with preparing
Figure 3.
We would also like to thank the people who have com-
mented on GitHub to help make this document better,
namely Soumendranath Bhakat, Giovanni Bussi, Ramon
Crehuet, Michele Invernizzi, Yinglong Miao, and Adrian
Roitberg.
ORCID:
Jérôme Hénin: 0000-0003-2540-4098
Tony Lelièvre: 0000-0002-3412-113X
43 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

Table 3. Libraries and Modules for enhanced sampling
Library name
Main features
Reference
Collective Variables Module (Colvars)
Deﬁnition and biasing of various CVs. Multiple variants of
Adaptive Biasing Force (ABF). and metadynamics.
Support for multiple walker simulations. Scripted
variables and biasing forces. Collective variables as
custom functions. Built into in VMD for preparation and
analysis of CVs.
[77, 240]
PLUMED
Deﬁnition of various CVs that can be analyzed and biased.
Various biasing methods (e.g., umbrella sampling,
steered MD, metadynamics, parallel-bias metadynamics,
bias-exchange metadynamics, extended-system adaptive
biasing force, variationally enhanced sampling). Support
for multiple walker simulations and replica exchange
simulations. Methods for integrating experimental
results (e.g., maximum entropy principle, metainference,
experiment directed simulation). Modular design making
it easy to add new features. Can be interfaced with a
wide range of MD codes. Large number of tutorials are
available [142]. Large number of example input ﬁles are
available in the PLUMED-NEST [189].
[78, 187, 188]
PMFlib
ABF, constrained dynamics, metadynamics, restraints,
string method.
[242]
SSAGES
Deﬁnition of various CVs that can be analyzed and biased.
Various biasing methods (e.g., umbrella sampling,
steered MD, metadynamics, adaptive biasing force, basis
function sampling, artiﬁcial neural network sampling,
combined force-frequency). Support for multiple walker
simulations and replica exchange simulations. Other
path-based methods such as nudged elastic band, ﬁnite
temperature string, swarm of trajectories, forward ﬂux
sampling.
[243]
WESTPA
Weighted ensemble
[341, 402]
Wepy
Weighted ensemble
[403]
Michael R. Shirts: 0000-0003-3249-1097
Omar Valsson: 0000-0001-7971-4767
Lucie Delemotte: 0000-0002-0828-3899
Appendix A
Abbreviations and acronyms
• ABF - Adaptive Biasing Force
• ABF-AR
-
Adaptive
Biasing
Force
with
Adiabatic
Reweighting
• ABF-FUNN - ABF-Force Biasing using Neural Networks
• ABMD - Adaptive Biasing MD
• ABP - Adaptive Biasing Potential
• ABPO - Adaptively Biased Path Optimization
• AFED - Adiabatic Free Energy Dynamics
• ATLAS - Adaptive Topography of Landscapes for Accel-
erated Sampling
• BAR - Bennett’s Acceptance Ratio
• CAFES - Canonical Adiabatic Free Energy Sampling
• CSA - Conformational Space Annealing
• CVs - Collective Variables
• CZAR - Corrected Z-Averaged Restraint
• d-AFED - Driven Adiabatic Free Energy Dynamics
• DIMS - Dynamic Importance Sampling
• eABF - extended-system Adaptive Biasing Force
• FAST - Fluctuation Ampliﬁcation of Speciﬁc Traits
• FEP - Free Energy Perturbation
• FES - Free Energy Surface
• GAMBES - Gaussian Mixture-Based Enhanced Samplin
• MBAR - Multistate Bennett’s Acceptance Ratio
44 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

• MC - Monte Carlo
• MD - Molecular Dynamics
• MetaD - Metadynamics
• MM - Molecular Mechanics
• MSM - Markov State Model
• mwABF - multiple-walker Adaptive Biasing Force
• NPT - Isothermal-Isobaric Ensemble
• NVE - Microcanonical Ensemble
• NVT - Canonical Ensemble
• OPES - On-the-ﬂy Probability-Enhanced Sampling
• OSRW - Orthogonal Space Random Sampling
• OST - Orthogonal Space Tempering
• OTFP - On-the-ﬂy Free Energy Parameterization
• PIGS - Progress Index Guided Sampling
• PMF - Potential of Mean Force
• QM - Quantum Mechanics
• QM/MM - Quantum Mechanics/Molecular Mechanics
• RAVE - Reweighted Autoencoded Variational Bayes for
Enhanced Sampling
• REAP - Reinforcement Learning Based Adaptive Sam-
pling
• REST - Replica Exchange Solute Tempering
• SAMS - Self-Adjusted Mixture Sampling
• SGMD - Self-Guided Molecular Dynamics
• SGLD - Self-Guided Langevin Dynamics
• TALOS - Targeted Adversarial Learning Optimized Sam-
pling
• TAMD - Temperature Accelerated Molecular dynamics
• TI - Thermodynamics Integration
• TICA - Time-Lagged Independent Component Analysis
• UFED - Uniﬁed Free Energy Dynamics
• VES - Variationally Enhanced Sampling
• WE - Weighted Ensemble
• WHAM - Weighted Histogram Analysis Method
References
[1] Bernardi RC, Melo MCR, Schulten K.
Enhanced sampling
techniques in molecular dynamics simulations of biologi-
cal systems.
Biochim Biophys Acta. 2015; 1850(5):872–877.
https://doi.org/10.1016/j.bbagen.2014.10.019.
[2] Yang YI, Shao Q, Zhang J, Yang L, Gao YQ. Enhanced sampling
in molecular dynamics.
J Chem Phys. 2019; 151(7):070902.
https://doi.org/10.1063/1.5109531.
[3] Kamenik
AS,
Linker
SM,
Riniker
S.
Enhanced
sam-
pling without borders:
on global biasing functions and
how to reweight them.
Phys Chem Chem Phys. 2021;
https://doi.org/10.1039/d1cp04809k.
[4] Bolhuis PG, Chandler D, Dellago C, Geissler PL. TRANSITION
PATH SAMPLING: Throwing Ropes Over Rough Mountain
Passes, in the Dark. Annu Rev Phys Chem. 2002; 53(1):291–318.
https://doi.org/10.1146/annurev.physchem.53.082301.113146.
[5] Li W, Ma A.
Recent developments in methods for identify-
ing reaction coordinates. Mol Sim. 2014; 40(10-11):784–793.
https://doi.org/10.1080/08927022.2014.907898.
[6] Peters B.
Reaction Coordinates and Mechanistic Hy-
pothesis
Tests.
Annu
Rev
Phys
Chem.
2015;
67(1).
https://doi.org/10.1146/annurev-physchem-040215-112215.
[7] Banushkina PV, Krivov SV.
Optimal reaction coordinates.
Wiley Interdiscip Rev Comput Mol Sci. 2016; 6(6):748–763.
https://doi.org/10.1002/wcms.1276.
[8] Tuckerman ME. Statistical mechanics: theory and molecular
simulation. Oxford University Press; 2010.
[9] Li Q, Lin B, Ren W. Computing committor functions for the
study of rare events using deep learning. J Chem Phys. 2019;
151(5):054112. https://doi.org/10.1063/1.5110439.
[10] Mori Y, Okazaki Ki, Mori T, Kim K, Matubayasi N.
Learning
reaction coordinates via cross-entropy minimization: Applica-
tion to alanine dipeptide. J Chem Phys. 2020; 153(5):054115.
https://doi.org/10.1063/5.0009066.
[11] Palacio-Rodriguez K, Pietrucci F.
Free Energy Land-
scapes,
Diffusion
Coeﬃcients,
and
Kinetic
Rates
from
Transition
Paths.
J
Chem
Theory
Comput.
2022;
https://doi.org/10.1021/acs.jctc.2c00324.
[12] Jung H, Covino R, Arjun A, Bolhuis PG, Hummer G.
Au-
tonomous artiﬁcial intelligence discovers mechanisms of
molecular self-organization in virtual experiments.
arXiv.
2021; .
[13] Frassek M, Arjun A, Bolhuis PG. An extended autoencoder
model for reaction coordinate discovery in rare event molec-
ular dynamics datasets.
J Chem Phys. 2021; 155(6):064103.
https://doi.org/10.1063/5.0058639.
[14] Wu S, Li H, Ma A.
A Rigorous Method for Identify-
ing a One-Dimensional Reaction Coordinate in Complex
Molecules.
J Chem Theory Comp. 2022; 18(5):2836–2844.
https://doi.org/10.1021/acs.jctc.2c00132.
[15] Wang Y, Lamim Ribeiro JM, Tiwary P.
Machine learning
approaches for analyzing and enhancing molecular dynam-
ics simulations.
Curr Opin Struct Biol. 2020; 61:139–145.
https://doi.org/10.1016/j.sbi.2019.12.016.
[16] Sidky H, Chen W, Ferguson AL.
Machine learning for
collective variable discovery and enhanced sampling in
biomolecular simulation.
Mol Phys. 2020; 118(5):e1737742.
https://doi.org/10.1080/00268976.2020.1737742.
[17] Gkeka P, Stoltz G, Barati Farimani A, Belkacemi Z, Ceri-
otti M, Chodera JD, Dinner AR, Ferguson AL, Maillet JB,
Minoux H, Peter C, Pietrucci F, Silveira A, Tkatchenko A,
Trstanova Z, Wiewiora R, Lelièvre T.
Machine Learn-
ing Force Fields and Coarse-Grained Variables in Molecu-
lar Dynamics:
Application to Materials and Biological Sys-
tems.
J Chem Theory Comput. 2020;
16(8):4757–4775.
https://doi.org/10.1021/acs.jctc.0c00355.
45 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[18] Chiavazzo E, Covino R, Coifman RR, Gear CW, Georgiou
AS, Hummer G, Kevrekidis IG.
Intrinsic map dynam-
ics exploration for uncharted effective free-energy land-
scapes.
Proc Natl Acad Sci. 2017; 114(28):E5494–E5503.
https://doi.org/10.1073/pnas.1621481114.
[19] Bruce NJ, Ganotra GK, Kokh DB, Sadiq SK, Wade RC.
New
approaches
for
computing
ligand–receptor
bind-
ing
kinetics.
Curr
Opin
Struct
Biol.
2018;
49:1–10.
https://doi.org/10.1016/j.sbi.2017.10.001,
theory
and
sim-
ulation • Macromolecular assemblies.
[20] Bernetti M, Masetti M, Rocchia W, Cavalli A. Kinetics of Drug
Binding and Residence Time.
Annu Rev Phys Chem. 2019;
70(1):143–171.
https://doi.org/10.1146/annurev-physchem-
042018-052340.
[21] Dickson A, Tiwary P, Vashisth H.
Kinetics of Ligand
Binding Through Advanced Computational Approaches:
A
Review.
Curr Top Med Chem. 2017;
17(23):2626–2641.
https://doi.org/10.2174/1568026617666170414142908.
[22] Ribeiro JML, Tsai ST, Pramanik D, Wang Y, Tiwary P.
Ki-
netics of Ligand–Protein Dissociation from All-Atom Simula-
tions: Are We There Yet? Biochemistry. 2018; 58(3):156–165.
https://doi.org/10.1021/acs.biochem.8b00977.
[23] Limongelli V.
Ligand binding free energy and kinetics cal-
culation in 2020. WIREs Comput Mol Sci. 2020; 10(4):e1455.
https://doi.org/10.1002/wcms.1455.
[24] Kieninger S, Donati L, Keller BG. Dynamical reweighting meth-
ods for Markov models. Curr Opin Struct Biol. 2020; 61:124–
131. https://doi.org/10.1016/j.sbi.2019.12.018.
[25] Dellago C, Bolhuis PG. In: Holm C, Kremer K, editors. Tran-
sition Path Sampling and Other Advanced Simulation Tech-
niques for Rare Events Berlin, Heidelberg: Springer Berlin Hei-
delberg; 2009. p. 167–233. https://doi.org/10.1007/978-3-540-
87706-6_3.
[26] Chong LT, Saglam AS, Zuckerman DM.
Path-sampling
strategies
for
simulating
rare
events
in
biomolecu-
lar systems.
Curr Opin Struct Biol. 2017;
43:88–94.
https://doi.org/10.1016/j.sbi.2016.11.019.
[27] Peters B.
Reaction Rate Theory and Rare Events.
Elsevier;
2017.
https://www.elsevier.com/books/reaction-rate-theory-
and-rare-events/peters/978-0-444-56349-1.
[28] Elber
R,
Makarov
DE,
Orland
H.
Molecular
Kinetics
in
Condensed
Phases.
Wiley;
2020.
https://doi.org/10.1002/9781119176800.
[29] Escobedo FA.
A uniﬁed methodological framework for the
simulation of nonisothermal ensembles. J Chem Phys. 2005;
123(4):044110. https://doi.org/10.1063/1.1938190.
[30] Abreu CRA, Escobedo FA.
A general framework for non-
Boltzmann Monte Carlo sampling.
J Chem Phys. 2006;
124(5):054116. https://doi.org/10.1063/1.2165188.
[31] Invernizzi M, Piaggi PM, Parrinello M.
Uniﬁed approach
to enhanced sampling.
Phys Rev X. 2020;
10:041034.
https://doi.org/10.1103/PhysRevX.10.041034.
[32] Rizzi A, Jensen T, Slochower DR, Aldeghi M, Gapsys V, Ntek-
oumes D, Bosisio S, Papadourakis M, Henriksen NM, de Groot
BL, Cournia Z, Dickson A, Michel J, Gilson MK, Shirts MR, Mob-
ley DL, Chodera JD.
The SAMPL6 SAMPLing Challenge: As-
sessing the Reliability and Eﬃciency of Binding Free Energy
Calculations. J Comput Aided Mol Des. 2020; 34(5):601–633.
https://doi.org/10.1007/s10822-020-00290-5.
[33] Hruska E, Abella JR, Nüske F, Kavraki LE, Clementi C.
Quantitative comparison of adaptive sampling methods for
protein dynamics.
J Chem Phys. 2018;
149(24):244119.
https://doi.org/10.1063/1.5053582.
[34] Skeel
RD,
Izaguirre
JA.
An
impulse
integrator
for
Langevin dynamics.
Mol Phys. 2002; 100(24):3885–3891.
https://doi.org/10.1080/0026897021000018321.
[35] Leimkuhler B, Matthews C. Rational Construction of Stochas-
tic Numerical Methods for Molecular Sampling. Appl Math Res
Express. 2012; https://doi.org/10.1093/amrx/abs010.
[36] Zuckerman DM. Statistical Physics of Biomolecules: An Intro-
duction. CRC Press; 2010. https://doi.org/10.1201/b18849.
[37] Bal KM, Fukuhara S, Shibuta Y, Neyts EC. Free energy barriers
from biased molecular dynamics simulations. J Chem Phys.
2020; 153(11):114118. https://doi.org/10.1063/5.0020240.
[38] Dietschreit JCB, Diestler DJ, Ochsenfeld C. How to obtain re-
action free energies from free-energy proﬁles. J Chem Phys.
2022; 156(11):114105. https://doi.org/10.1063/5.0083423.
[39] Shirts MR. Reweighting from the Mixture Distribution as a Bet-
ter Way to Describe the Multistate Bennett Acceptance Ratio.
ArXiv170400891 Cond-Mat. 2017; .
[40] Siepmann JI, Frenkel D. Conﬁgurational bias Monte Carlo: a
new sampling scheme for ﬂexible chains. Mol Physics. 1992;
75(1):59–70. https://doi.org/10.1080/00268979200100061.
[41] Hastings WK. Monte Carlo sampling methods using Markov
chains and their applications. Biometrika. 1970; 57(1):97–109.
https://doi.org/10.1093/biomet/57.1.97.
[42] Chodera JD, Shirts MR. Replica exchange and expanded en-
semble simulations as Gibbs sampling: simple improvements
for enhanced mixing.
J Chem Phys. 2011; 135(19):194110.
https://doi.org/10.1063/1.3660669.
[43] Manousiouthakis VI, Deem MW.
Strict detailed balance is
unnecessary in Monte Carlo simulation. J Chem Phys. 1999;
110:2753. https://doi.org/10.1063/1.477973.
[44] Faizi F, Deligiannidis G, Rosta E. Eﬃcient Irreversible Monte
Carlo Samplers.
J Chem Theory Comput. 2020; 16(4):2124–
2138. https://doi.org/10.1021/acs.jctc.9b01135.
[45] Chipot Ch, Pohorille A, editors.
Free Energy Calculations:
Theory and Applications in Chemistry and Biology, vol. 86 of
Springer Series in Chemical Physics. Berlin: Springer-Verlag;
2007. https://doi.org/10.1007/978-3-540-38448-9.
[46] Lelièvre T, Rousset M, Stoltz G. Free Energy Computations:
A Mathematical Perspective.
Imperial College Press; 2010.
https://doi.org/10.1142/p579.
46 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[47] Paliwal H, Shirts MR.
A benchmark test set for alchemical
free energy transformations and its use to quantify error in
common free energy methods. J Chem Theory Comput. 2011;
7(12):4115–4134. https://doi.org/10.1021/ct2003995.
[48] Shirts MR, Pande VS. Comparison of eﬃciency and bias of free
energies computed by exponential averaging, the Bennett ac-
ceptance ratio, and thermodynamic integration. J Chem Phys.
2005; 122(14):144107. https://doi.org/10.1063/1.1873592.
[49] Klimovich PV, Shirts MR, Mobley DL. Guidelines for the analy-
sis of free energy calculations. J Comput Aided Mol Des. 2015;
29(5):397–411. https://doi.org/10.1007/s10822-015-9840-9.
[50] Noé
F,
Schütte
C,
Vanden-Eijnden
E,
Reich
L,
Weikl
TR.
Constructing
the
equilibrium
ensemble
of
fold-
ing
pathways
from
short
off-equilibrium
simula-
tions.
Proc Natl Acad Sci. 2009;
106(45):19011–19016.
https://doi.org/10.1073/pnas.0905466106.
[51] Westerlund AM, Harpole TJ, Blau C, Delemotte L. Inference
of Calmodulin’s Ca2+-Dependent Free Energy Landscapes via
Gaussian Mixture Model Validation. J Chem Theory Comput.
2017; 14(1):63–71. https://doi.org/10.1021/acs.jctc.7b00346.
[52] de Oliveira PMC, Penna TJP, Herrmann HJ.
Broad His-
togram Monte Carlo.
Eur Phys J B. 1998; 1(2):205–208.
https://doi.org/10.1007/s100510050172.
[53] Wang
JS,
Swendsen
RH.
Transition
Matrix
Monte
Carlo
Method.
J
Stat
Phys.
2002;
106(1):245–285.
https://doi.org/10.1023/A:1013180330892.
[54] Escobedo FA, Abreu CRA.
On the use of transition ma-
trix methods with extended ensembles. J Chem Phys. 2006;
124(10):104110. https://doi.org/10.1063/1.2174010.
[55] Liu JS.
Peskun’s Theorem and a Modiﬁed Discrete-
State Gibbs Sampler.
Biometrika. 1996;
83(3):681–682.
https://doi.org/10.1093/biomet/83.3.681.
[56] Wu H, Mey ASJS, Rosta E, Noé F.
Statistically optimal anal-
ysis of state-discretized trajectory data from multiple ther-
modynamic states.
J Chem Phys. 2014; 141(21):214106.
https://doi.org/10.1063/1.4902240.
[57] Wu H, Noé F.
Optimal Estimation of Free Energies
and
Stationary
Densities
from
Multiple
Biased
Simu-
lations.
Multiscale
Model
Simul.
2014;
12(1):25–54.
https://doi.org/10.1137/120895883.
[58] Rosta
E,
Hummer
G.
Free
Energies
from
Dynamic
Weighted Histogram Analysis Using Unbiased Markov State
Model.
J Chem Theory Comput. 2014;
11(1):276–285.
https://doi.org/10.1021/ct500719p.
[59] Wu H, Paul F, Wehmeyer C, Noé F.
Multiensemble
Markov models of molecular thermodynamics and kinet-
ics.
Proc Natl Acad Sci USA. 2016; 113(23):E3221–E3230.
https://doi.org/10.1073/pnas.1525092113.
[60] Lelièvre T, Rousset M, Stoltz G. Computation of free energy
differences through nonequilibrium stochastic dynamics: The
reaction coordinate case. J Comput Phys. 2007; 222(2):624–
643. https://doi.org/10.1016/j.jcp.2006.08.003.
[61] Hénin J, Fiorin G, Chipot C, Klein ML. Exploring multidimen-
sional free energy landscapes using time-dependent biases on
collective variables. J Chem Theory Comput. 2010; 6(1):35–47.
https://doi.org/10.1021/ct9004432.
[62] Comer J, Gumbart JC, Hénin J, Lelièvre T, Pohorille A, Chipot
C. The adaptive biasing force method: everything you always
wanted to know but were afraid to ask. J Phys Chem B. 2015;
119(3):1129–1151. https://doi.org/10.1021/jp506633n.
[63] Hénin J, Chipot C. Overcoming free energy barriers using un-
constrained molecular dynamics simulations.
J Chem Phys.
2004; 121:2904–2914. https://doi.org/10.1063/1.1773132.
[64] den
Otter
WK.
Thermodynamic
integration
of
the
free
energy
along
a
reaction
coordinate
in
Carte-
sian coordinates.
J Chem Phys. 2000;
112:7283–7292.
https://doi.org/10.1063/1.481329.
[65] Ciccotti G, Kapral R, Vanden-Eijnden E.
Blue moon sam-
pling,
vectorial reaction coordinates,
and unbiased con-
strained dynamics.
ChemPhysChem. 2005; 6(9):1809–1814.
https://doi.org/10.1002/cphc.200400669.
[66] Hénin J. Fast and Accurate Multidimensional Free Energy In-
tegration.
J Chem Theory Comput. 2021; 17(11):6789–6798.
https://doi.org/10.1021/acs.jctc.1c00593.
[67] Lu N, Kofke DA, Woolf TB. Improving the eﬃciency and re-
liability of free energy perturbation calculations using over-
lap sampling methods.
J Comput Chem. 2003; 25(1):28–40.
https://doi.org/10.1002/jcc.10369.
[68] Bennett CH. Eﬃcient estimation of free energy differences
from Monte Carlo data. J Comput Phys. 1976; 22(2):245–268.
https://doi.org/10.1016/0021-9991(76)90078-4.
[69] Fenwick
MK,
Escobedo
FA.
Expanded
ensemble
and
replica
exchange
methods
for
simulation
of
protein-like systems.
J Chem Phys. 2003;
119:11998.
https://doi.org/10.1063/1.1624822.
[70] Shirts MR, Chodera JD. Statistically optimal analysis of sam-
ples from multiple equilibrium states.
J Chem Phys. 2008;
129(12):124105. https://doi.org/10.1063/1.2978177.
[71] Lindahl,
Abraham,
Hess,
van
der
Spoel,
GRO-
MACS
2021.2
Source
code.
Zenodo;
2021.
https://doi.org/10.5281/zenodo.4723562.
[72] Tan Z, Gallicchio E, Lapelosa M, Levy RM.
Theory of bin-
less multi-state free energy estimation with applications to
protein-ligand binding.
J Chem Phys. 2012; 136(14):144102.
https://doi.org/10.1063/1.3701175.
[73] Zhang
BW,
Xia
J,
Tan
Z,
Levy
RM.
A
Stochas-
tic
Solution
to
the
Unbinned
WHAM
Equa-
tions.
J
Phys
Chem
Lett.
2015;
6(19):3834–3840.
https://doi.org/10.1021/acs.jpclett.5b01771.
[74] Kumar S, Bouzida D, Swendsen RH, Kollman PA, Rosen-
berg
JM.
The
weighted
histogram
analysis
method
for
free-energy
calculations
on
biomolecules
.1.
the
method.
J Comput Chem. 1992;
13(8):1011 – 1021.
https://doi.org/10.1002/jcc.540130812,.
47 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[75] Schlitter J, Engels M, Krüger P.
Targeted molecular dy-
namics:
a new approach for searching pathways of con-
formational transitions.
J Mol Graph. 1994; 12(2):84–89.
https://doi.org/10.1016/0263-7855(94)80072-3.
[76] Grubmüller
H,
Heymann
B,
Tavan
P.
Ligand
Bind-
ing:
Molecular Mechanics Calculation of the Streptavidin-
Biotin Rupture Force.
Science. 1996; 271(5251):997–999.
https://doi.org/10.1126/science.271.5251.997.
[77] Fiorin
G,
Klein
ML,
Hénin
J.
Using
collective
variables
to
drive
molecular
dynamics
simula-
tions.
Mol
Phys.
2013;
111(22-23):3345–3362.
https://doi.org/10.1080/00268976.2013.813594.
[78] Tribello
GA,
Bonomi
M,
Branduardi
D,
Camilloni
C, Bussi G.
PLUMED 2:
New feathers for an old
bird.
Comput
Phys
Commun.
2014;
185(2):604–613.
https://doi.org/10.1016/j.cpc.2013.09.018.
[79] Postma JPM, Berendsen HJC, Haak JR.
Thermodynamics
of cavity formation in water. A molecular dynamics study.
Faraday Symposia of the Chemical Society. 1982;
17:55.
https://doi.org/10.1039/fs9821700055.
[80] Kirkwood JG. Statistical mechanics of ﬂuid mixtures. J Chem
Phys. 1935; 3:300–313. https://doi.org/10.1063/1.1749657.
[81] Zwanzig RW. High-temperature equation of state by a per-
turbation method. I. Nonpolar gases.
J Chem Phys. 1954;
22:1420–1426. https://doi.org/10.1063/1.1740409.
[82] Jarzynski
C.
Equilibrium
free
energy
differences
from
nonequilibrium
measurements:
A
master
equa-
tion
approach.
Phys
Rev
E.
1997;
56(5):5018–5035.
https://doi.org/10.1103/PhysRevE.56.5018.
[83] Park S, Khalili-Araghi F, Tajkhorshid E, Schulten K. Free energy
calculation from steered molecular dynamics simulations us-
ing Jarzynski’s equality. J Chem Phys. 2003; 119(6):3559–3566.
https://doi.org/10.1063/1.1590311.
[84] Minh
DDL,
Adib
AB.
Optimized
Free
Energies
from
Bidirectional
Single-Molecule
Force
Spec-
troscopy.
Phys
Rev
Lett.
2008;
100:180602.
https://doi.org/10.1103/physrevlett.100.180602.
[85] Vaikuntanathan
S,
Jarzynski
C.
Escorted
Free
En-
ergy
Simulations:
Improving
Convergence
by
Reduc-
ing
Dissipation.
Phys
Rev
Lett.
2008;
100:190601.
https://doi.org/10.1103/physrevlett.100.190601.
[86] Hartmann C, Schütte C, Zhang W. Jarzynski equality, ﬂuctua-
tion theorems, and variance reduction: Mathematical analysis
and numerical algorithms. J Stat Phys. 2019; 175(6):1214–1261.
https://doi.org/10.1007/s10955-019-02286-4.
[87] Rousset M, Stoltz G.
Equilibrium sampling from nonequi-
librium dynamics.
J Stat Phys. 2006;
123(6):1251–1272.
https://doi.org/10.1007/s10955-006-9090-2.
[88] Hummer G.
Nonequilibrium methods for equilibrium
free-energy calculations.
In:
Chipot C, Pohorille A, ed-
itors. Free energy calculations Springer;
2007.p. 171–198.
https://doi.org/10.1007/978-3-540-38448-9_5.
[89] Gapsys V, Pérez-Benito L, Aldeghi M, Seeliger D, van Vlijmen H,
Tresadern G, de Groot BL. Large Scale Relative Protein Ligand
Binding Aﬃnities Using Non-Equilibrium Alchemy. Chem Sci.
2020; 11(4):1140–1152. https://doi.org/10.1039/C9SC03754C.
[90] Torrie
GM,
Valleau
JP.
Nonphysical
sampling
distri-
butions
in
Monte
Carlo
free-energy
estimation:
Um-
brella sampling.
J Comput Phys. 1977;
23(2):187–199.
https://doi.org/10.1016/0021-9991(77)90121-8.
[91] Ciccotti
G,
Ferrario
M.
Blue
Moon
Approach
to
Rare
Events.
Mol
Simul.
2004;
30(11-12):787–793.
https://doi.org/10.1080/0892702042000270214.
[92] Glowacki DR, Paci E, Shalashilin DV.
Boxed Molecular Dy-
namics: A Simple and General Technique for Accelerating
Rare Event Kinetics and Mapping Free Energy in Large Molec-
ular Systems.
J Phys Chem B. 2009; 113(52):16603–16611.
https://doi.org/10.1021/jp9074898.
[93] Hamelberg D, Mongan J, McCammon JA. Accelerated molec-
ular dynamics: A promising and eﬃcient simulation method
for biomolecules.
J Chem Phy. 2004; 120(24):11919–11929.
https://doi.org/10.1063/1.1755656.
[94] Miao Y, McCammon JA. Gaussian Accelerated Molecular Dy-
namics: Theory, Implementation, and Applications. In: Annual
Reports in Computational Chemistry Elsevier; 2017.p. 231–278.
https://doi.org/10.1016/bs.arcc.2017.06.005.
[95] Wang J, Arantes PR, Bhattarai A, Hsu RV, Pawnikar S, ming
M Huang Y, Palermo G, Miao Y. Gaussian accelerated molecu-
lar dynamics: Principles and applications. WIREs Comput Mol
Sci. 2021; https://doi.org/10.1002/wcms.1521.
[96] Invernizzi
M,
Parrinello
M.
Exploration
vs
Con-
vergence
Speed
in
Adaptive-Bias
Enhanced
Sam-
pling.
J Chem Theory Comput. 2022;
18(6):3988–3996.
https://doi.org/10.1021/acs.jctc.2c00152.
[97] Piana S, Laio A.
A Bias-Exchange Approach to Pro-
tein Folding.
J Phys Chem B. 2007; 111(17):4553–4559.
https://doi.org/10.1021/jp067873l.
[98] Pfaendtner J, Bonomi M.
Eﬃcient Sampling of High-
Dimensional Free-Energy Landscapes with Parallel Bias Meta-
dynamics. J Chem Theory Comput. 2015; 11(11):5062–5067.
https://doi.org/10.1021/acs.jctc.5b00846.
[99] Laio
A,
Parrinello
M.
Escaping
free-energy
minima.
Proc
Natl
Acad
Sci
USA.
2002;
99(20):12562–12566.
https://doi.org/10.1073/pnas.202427399.
[100] Barducci A, Bussi G, Parrinello M.
Well-Tempered Meta-
dynamics:
A
Smoothly
Converging
and
Tunable
Free-
Energy Method.
Phys Rev Lett. 2008;
100(2):020603.
https://doi.org/10.1103/physrevlett.100.020603.
[101] Valsson O, Tiwary P, Parrinello M.
Enhancing Important
Fluctuations: Rare Events and Metadynamics from a Concep-
tual Viewpoint.
Annu Rev Phys Chem. 2016; 67(1):159–184.
https://doi.org/10.1146/annurev-physchem-040215-112229.
48 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[102] Valsson
O,
Parrinello
M.
Variational
Approach
to
Enhanced
Sampling
and
Free
Energy
Calcu-
lations.
Phys
Rev
Lett.
2014;
113(9):090601.
https://doi.org/10.1103/physrevlett.113.090601.
[103] Valsson O, Parrinello M.
Variationally Enhanced Sam-
pling. In: Andreoni W, Yip S, editors. Handbook of Materials
Modeling Springer International Publishing; 2020.p. 621–634.
https://doi.org/10.1007/978-3-319-44677-6_50.
[104] Dickson BM.
Survey of adaptive biasing potentials: com-
parisons and outlook. Curr Opin Struct Biol. 2017; 43:63–67.
https://doi.org/10.1016/j.sbi.2016.11.007.
[105] Awasthi S, Nair NN. Exploring high-dimensional free energy
landscapes of chemical reactions.
WIREs Comput Mol Sci.
2019; 9(3):e1398. https://doi.org/10.1002/wcms.1398.
[106] Allison JR.
Computational methods for exploring protein
conformations.
Biochem Soc Trans. 2020; 48(4):1707–1724.
https://doi.org/10.1042/bst20200193.
[107] Huber T, Torda AE, van Gunsteren WF.
Local elevation: A
method for improving the searching properties of molecular
dynamics simulation. J Comput Aided Mol Des. 1994; 8:695–
708. https://doi.org/10.1007/BF00124016.
[108] Hansmann U, Wille L.
Global optimization by energy
landscape paving.
Phys Rev Lett. 2002;
88(6):068105.
https://doi.org/10.1103/physrevlett.88.068105.
[109] Marsili S, Barducci A, Chelli R, Procacci P, Schettino V. Self-
healing umbrella sampling: a non-equilibrium approach for
quantitative free energy calculations. J Phys Chem B. 2006;
110(29):14011–14013. https://doi.org/10.1021/jp062755j.
[110] Fort G, Jourdain B, Lelièvre T, Stoltz G. Self-healing umbrella
sampling: convergence and eﬃciency.
Stat Comput. 2017;
27(1):147–168. https://doi.org/10.1007/s11222-015-9613-2.
[111] Babin V, Roland C, Sagui C.
Adaptively biased molecular
dynamics for free energy calculations.
J Chem Phys. 2008;
128(13):134101. https://doi.org/10.1063/1.2844595.
[112] Maragakis P, van der Vaart A, Karplus M. Gaussian-mixture
umbrella sampling. J Phys Chem B. 2009; 113(14):4664–4673.
https://doi.org/10.1021/jp808381s.
[113] Dickson BM, Legoll F, Lelièvre T, Stoltz G, Fleurat-Lessard
P.
Free energy calculations: an eﬃcient adaptive biasing
potential method.
J Phys Chem B. 2010; 114:5823–5830.
https://doi.org/10.1021/jp100926h.
[114] Whitmer JK, Chiu Cc, Joshi AA, Pablo JJd.
Basis Func-
tion Sampling:
A New Paradigm for Material Property
Computation.
Phys
Rev
Lett.
2014;
113(19):190602.
https://doi.org/10.1103/physrevlett.113.190602.
[115] Whitmer JK, Fluitt AM, Antony L, Qin J, McGovern M, Pablo
JJd.
Sculpting bespoke mountains: Determining free ener-
gies with basis expansions. J Chem Phys. 2015; 143(4):044101.
https://doi.org/10.1063/1.4927147.
[116] Šućur Z, Spiwok V.
Sampling Enhancement and Free
Energy
Prediction
by
the
Flying
Gaussian
Method.
J
Chem
Theory
Comput.
2016;
12(9):4644–4650.
https://doi.org/10.1021/acs.jctc.6b00551.
[117] Sidky H, Whitmer JK. Learning free energy landscapes using
artiﬁcial neural networks. J Chem Phys. 2018; 148(10):104111.
https://doi.org/10.1063/1.5018708.
[118] Invernizzi
M,
Parrinello
M.
Rethinking
metady-
namics:
From
bias
potentials
to
probability
distri-
butions.
J
Phys
Chem
Lett.
2020;
11(7):2731–2736.
https://doi.org/10.1021/acs.jpclett.0c00497.
[119] Ribeiro JML, Bravo P, Wang Y, Tiwary P.
Reweighted
autoencoded
variational
Bayes
for
enhanced
sam-
pling
(RAVE).
J
Chem
Phys.
2018;
149(7):072301.
https://doi.org/10.1063/1.5025487.
[120] Zhang J, Yang YI, Noé F. Targeted Adversarial Learning Opti-
mized Sampling. J Phys Chem Lett. 2019; 10(19):5791–5797.
https://doi.org/10.1021/acs.jpclett.9b02173.
[121] Debnath
J,
Parrinello
M.
Gaussian
Mixture-
Based
Enhanced
Sampling
for
Statics
and
Dynam-
ics.
J
Phys
Chem
Lett.
2020;
11(13):5076–5080.
https://doi.org/10.1021/acs.jpclett.0c01125.
[122] Giberti
F,
Tribello
GA,
Ceriotti
M.
Global
Free-
Energy
Landscapes
as
a
Smoothly
Joined
Collec-
tion of Local Maps.
J Chem Theory Comput. 2021;
https://doi.org/10.1021/acs.jctc.0c01177.
[123] Bal KM.
Reweighted Jarzynski Sampling:
Acceleration of
Rare Events and Free Energy Calculation with a Bias Potential
Learned from Nonequilibrium Work. J Chem Theory Comput.
2021; https://doi.org/10.1021/acs.jctc.1c00574.
[124] Fort G, Jourdain B, Kuhn E, Lelièvre T, Stoltz G.
Conver-
gence and Eﬃciency of Adaptive Importance Sampling Tech-
niques with Partial Biasing.
J Stat Phys. 2018; 171:220–268.
https://doi.org/10.1007/s10955-018-1992-2.
[125] Fort G, Jourdain B, Kuhn E, Lelièvre T, Stoltz G.
Con-
vergence of the Wang-Landau algorithm.
Math Comput.
2015;
84(295):2297–2327.
https://doi.org/10.1090/s0025-
5718-2015-02952-4.
[126] Fort G, Jourdain B, Kuhn E, Lelièvre T, Stoltz G.
Ef-
ﬁciency
of
the
Wang-Landau
algorithm:
a
simple
test case.
Appl Math Res Express. 2014;
2:275–311.
https://doi.org/10.1093/amrx/abu003.
[127] Fort G, Jourdain B, Lelièvre T, Stoltz G. Self-Healing Umbrella
Sampling: Convergence and eﬃciency.
Stat Comput. 2017;
27(1):147–168. https://doi.org/10.1007/s11222-015-9613-2.
[128] Barducci
A,
Bonomi
M,
Parrinello
M.
Metadynam-
ics.
WIREs
Comput
Mol
Sci.
2011;
1(5):826–843.
https://doi.org/10.1002/wcms.31.
[129] Zheng
S,
Pfaendtner
J.
Enhanced
sampling
of
chemical
and
biochemical
reactions
with
meta-
dynamics.
Mol
Simul.
2014;
41(1-3):55–72.
https://doi.org/10.1080/08927022.2014.923574.
[130] Giberti F, Salvalaglio M, Parrinello M.
Metadynamics
studies of crystal nucleation.
IUCrJ. 2015; 2(2):256–266.
https://doi.org/10.1107/s2052252514027626.
49 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[131] Pfaendtner
J.
Biomolecular
Simulations,
Methods
and Protocols.
Methods Mol Biol. 2019;
2022:179–200.
https://doi.org/10.1007/978-1-4939-9608-7_8.
[132] Bussi G, Laio A, Tiwary P. Metadynamics: A Uniﬁed Frame-
work for Accelerating Rare Events and Sampling Thermo-
dynamics and Kinetics.
In:
Handbook of Materials Mod-
eling Springer International Publishing;
2020.p. 565–595.
https://doi.org/10.1007/978-3-319-44677-6_49.
[133] Bussi G, Laio A.
Using metadynamics to explore complex
free-energy landscapes.
Nat Rev Phys. 2020; 2(4):200–212.
https://doi.org/10.1038/s42254-020-0153-0.
[134] Dama
JF,
Parrinello
M,
Voth
GA.
Well-
Tempered
Metadynamics
Converges
Asymptot-
ically.
Phys
Rev
Lett.
2014;
112(24):240602.
https://doi.org/10.1103/physrevlett.112.240602.
[135] Tiwary P, Parrinello M. A Time-Independent Free Energy Es-
timator for Metadynamics. J Phys Chem B. 2015; 119(3):736–
742. https://doi.org/10.1021/jp504920s.
[136] Bonomi M, Barducci A, Parrinello M.
Reconstructing
the equilibrium Boltzmann distribution from well-tempered
metadynamics.
J Comp Chem. 2009;
30(11):1615–1621.
https://doi.org/10.1002/jcc.21305.
[137] Branduardi D, Bussi G, Parrinello M.
Metadynamics with
Adaptive Gaussians. J Chem Theory Comput. 2012; 8(7):2247–
2254. https://doi.org/10.1021/ct3002464.
[138] Schäfer TM, Settanni G. Data Reweighting in Metadynamics
Simulations. J Chem Theory Comput. 2020; 16(4):2042–2052.
https://doi.org/10.1021/acs.jctc.9b00867.
[139] Giberti F, Cheng B, Tribello GA, Ceriotti M. Iterative Unbiasing
of Quasi-Equilibrium Sampling. J Chem Theory Comput. 2019;
16(1):100–107. https://doi.org/10.1021/acs.jctc.9b00907.
[140] Marinova V, Salvalaglio M. Time-independent free energies
from metadynamics via mean force integration. J Chem Phys.
2019; 151(16):164115. https://doi.org/10.1063/1.5123498.
[141] Ono
J,
Nakai
H.
Weighted
histogram
analysis
method
for
multiple
short-time
metadynamics
sim-
ulations.
Chem
Phys
Lett.
2020;
751:137384.
https://doi.org/10.1016/j.cplett.2020.137384.
[142] PLUMED Masterclass Tutorials;. Accessed: May 26, 2022. https:
//www.plumed.org/masterclass.
[143] Mones L, Bernstein N, Csányi G. Exploration, Sampling, And
Reconstruction of Free Energy Surfaces with Gaussian Process
Regression. J Chem Theory Comput. 2016; 12(10):5100–5110.
https://doi.org/10.1021/acs.jctc.6b00553.
[144] Laio A, Gervasio FL. Metadynamics: a method to simulate rare
events and reconstruct the free energy in biophysics, chem-
istry and material science. Rep Prog Phys. 2008; 71(12):126601.
https://doi.org/10.1088/0034-4885/71/12/126601.
[145] Jourdain B, Lelièvre T, Zitt PA. Convergence of metadynam-
ics: discussion of the adiabatic hypothesis. Ann Appl Probab.
2021; 31(5):2441 – 2477. https://doi.org/10.1214/20-AAP1652.
[146] Raiteri P, Laio A, Gervasio FL, Micheletti C, Parrinello M. Ef-
ﬁcient Reconstruction of Complex Free Energy Landscapes
by Multiple Walkers Metadynamics †. J Phys Chem B. 2006;
110(8):3533–3539. https://doi.org/10.1021/jp054359r.
[147] Bonomi M, Parrinello M.
Enhanced Sampling in the Well-
Tempered Ensemble.
Phys Rev Lett. 2010; 104(19):190601.
https://doi.org/10.1103/physrevlett.104.190601.
[148] Berg BA, Neuhaus T.
Multicanonical ensemble: A new ap-
proach to simulate ﬁrst-order phase transitions. Phys Rev Lett.
1992; 68(1):9–12. https://doi.org/10.1103/physrevlett.68.9.
[149] Valsson O, Parrinello M. Thermodynamical Description of a
Quasi-First-Order Phase Transition from the Well-Tempered
Ensemble.
J Chem Theory Comput. 2013; 9(12):5267–5276.
https://doi.org/10.1021/ct400859f.
[150] Junghans C, Perez D, Vogel T.
Molecular Dynamics in the
Multicanonical Ensemble: Equivalence of Wang–Landau Sam-
pling, Statistical Temperature Molecular Dynamics, and Meta-
dynamics.
J Chem Theory Comput. 2014; 10(5):1843–1847.
https://doi.org/10.1021/ct500077d.
[151] Wang F, Landau DP.
Eﬃcient, multiple-range random walk
algorithm to calculate density of states. Phys Rev Lett. 2001;
86:2050–2053. https://doi.org/10.1103/physrevlett.86.2050.
[152] Kim J, Straub JE, Keyes T. Statistical-Temperature Monte Carlo
and Molecular Dynamics Algorithms.
Phys Rev Lett. 2006;
97(5). https://doi.org/10.1103/physrevlett.97.050601.
[153] Prakash A, Fu CD, Bonomi M, Pfaendtner J. Biasing Smarter,
Not Harder, by Partitioning Collective Variables into Families
in Parallel Bias Metadynamics. J Chem Theory Comput. 2018;
14(10):4985–4990. https://doi.org/10.1021/acs.jctc.8b00448.
[154] Tiwary
P,
Parrinello
M.
From
Metadynamics
to
Dynamics.
Phys
Rev
Lett.
2013;
111(23):230602.
https://doi.org/10.1103/physrevlett.111.230602.
[155] Voter AF. Hyperdynamics: Accelerated Molecular Dynamics
of Infrequent Events. Phys Rev Lett. 1997; 78(20):3908–3911.
https://doi.org/10.1103/physrevlett.78.3908.
[156] Grubmüller
H.
Predicting
slow
structural
tran-
sitions
in
macromolecular
systems:
Conforma-
tional
ﬂooding.
Phys
Rev
E.
1995;
52(3):2893–2906.
https://doi.org/10.1103/physreve.52.2893.
[157] Salvalaglio M, Tiwary P, Parrinello M.
Assessing the
Reliability of the Dynamics Reconstructed from Metady-
namics.
J Chem Theory Comput. 2014; 10(4):1420–1425.
https://doi.org/10.1021/ct500040r.
[158] Wang Y, Valsson O, Tiwary P, Parrinello M, Lindorff-Larsen
K.
Frequency adaptive metadynamics for the calculation
of rare-event kinetics.
J Chem Phys. 2018; 149(7):072309.
https://doi.org/10.1063/1.5024679.
[159] Cavalli A, Spitaleri A, Saladino G, Gervasio FL.
Investigat-
ing Drug–Target Association and Dissociation Mechanisms Us-
ing Metadynamics-Based Algorithms.
Acc Chem Res. 2015;
48(2):277–285. https://doi.org/10.1021/ar500356n.
50 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[160] Marinelli F, Pietrucci F, Laio A, Piana S.
A Kinetic Model
of Trp-Cage Folding from Multiple Biased Molecular Dynam-
ics Simulations.
PLoS Comput Biol. 2009; 5(8):e1000452.
https://doi.org/10.1371/journal.pcbi.1000452.
[161] Pietrucci F, Marinelli F, Carloni P, Laio A.
Substrate Bind-
ing Mechanism of HIV-1 Protease from Explicit-Solvent Atom-
istic Simulations. J Am Chem Soc. 2009; 131(33):11811–11818.
https://doi.org/10.1021/ja903045y.
[162] Juraszek
J,
Saladino
G,
Erp
TSv,
Gervasio
FL.
Ef-
ﬁcient
Numerical
Reconstruction
of
Protein
Fold-
ing
Kinetics
with
Partial
Path
Sampling
and
Path-
like
Variables.
Phys
Rev
Lett.
2013;
110(10):108106.
https://doi.org/10.1103/physrevlett.110.108106.
[163] Donati L, Keller BG.
Girsanov reweighting for metady-
namics simulations.
J Chem Phys. 2018; 149(7):072335.
https://doi.org/10.1063/1.5027728.
[164] Sicard
F.
Computing
transition
rates
for
rare
events:
When
Kramers
theory
meets
the
free-
energy
landscape.
Phys
Rev
E.
2018;
98(5):052408.
https://doi.org/10.1103/physreve.98.052408.
[165] Ribeiro JML, Provasi D, Filizola M.
A combination of
machine learning and infrequent metadynamics to eﬃ-
ciently predict kinetic rates, transition states, and molec-
ular determinants of drug dissociation from G protein-
coupled receptors.
J Chem Phys. 2020; 153(12):124105.
https://doi.org/10.1063/5.0019100.
[166] Palacio-Rodriguez K, Vroylandt H, Stelzl LS, Pietrucci F,
Hummer G, Cossio P.
Transition Rates and Eﬃciency
of
Collective
Variables
from
Time-Dependent
Biased
Simulations.
J Phys Chem Lett. 2022;
p. 7490–7496.
https://doi.org/10.1021/acs.jpclett.2c01807.
[167] Tribello
GA,
Ceriotti
M,
Parrinello
M.
A
self-
learning
algorithm
for
biased
molecular
dynam-
ics.
Proc Natl
Acad
Sci. 2010;
107(41):17509–17514.
https://doi.org/10.1073/pnas.1011511107.
[168] Wu P, Hu X, Yang W.
λ-Metadynamics Approach To Com-
pute Absolute Solvation Free Energy. J Phys Chem Lett. 2011;
2(17):2099–2103. https://doi.org/10.1021/jz200808x.
[169] Singh
S,
Chiu
Cc,
de
Pablo
JJ.
Flux
Tempered
Metadynamics.
J
Stat
Phys.
2011;
145(4):932–945.
https://doi.org/10.1007/s10955-011-0301-0.
[170] Singh S, Chiu Cc, de Pablo JJ.
Eﬃcient Free Energy Calcu-
lation of Biomolecules from Diffusion-Biased Molecular Dy-
namics.
J Chem Theory Comput. 2012; 8(11):4657–4662.
https://doi.org/10.1021/ct3003755.
[171] Leines GD, Ensing B.
Path Finding on High-Dimensional
Free Energy Landscapes. Phys Rev Lett. 2012; 109(2):020601.
https://doi.org/10.1103/physrevlett.109.020601.
[172] Ortíz APdA, Tiwari A, Puthenkalathil RC, Ensing B.
Ad-
vances in enhanced sampling along adaptive paths of col-
lective variables.
J Chem Phys. 2018;
149(7):072320.
https://doi.org/10.1063/1.5027392.
[173] Limongelli
V,
Bonomi
M,
Parrinello
M.
Funnel
metadynamics
as
accurate
binding
free-energy
method.
Proc Natl Acad Sci. 2013;
110(16):6358–6363.
https://doi.org/10.1073/pnas.1303186110.
[174] Raniolo S, Limongelli V.
Ligand binding free-energy cal-
culations with funnel metadynamics.
Nat Protoc. 2020;
15(9):2837–2866. https://doi.org/10.1038/s41596-020-0342-4.
[175] McGovern M, de Pablo J. A boundary correction algorithm
for metadynamics in multiple dimensions. J Chem Phys. 2013;
139(8):084102. https://doi.org/10.1063/1.4818153.
[176] Dama JF, Rotskoff G, Parrinello M, Voth GA.
Transition-
Tempered
Metadynamics:
Robust,
Convergent
Meta-
dynamics
via
On-the-Fly
Transition
Barrier
Estima-
tion.
J Chem Theory Comput. 2014;
10(9):3626–3633.
https://doi.org/10.1021/ct500441q.
[177] Dama JF, Hocky GM, Sun R, Voth GA.
Exploring Valleys
without Climbing Every Peak: More Eﬃcient and Forgiving
Metabasin Metadynamics via Robust On-the-Fly Bias Domain
Restriction. J Chem Theory Comput. 2015; 11(12):5638–5650.
https://doi.org/10.1021/acs.jctc.5b00907.
[178] White AD, Dama JF, Voth GA.
Designing Free Energy
Surfaces That Match Experimental Data with Metadynam-
ics.
J Chem Theory Comput. 2015;
11(6):2451–2460.
https://doi.org/10.1021/acs.jctc.5b00178.
[179] Marinelli F, Faraldo-Gómez J.
Ensemble-Biased Metady-
namics: A Molecular Simulation Method to Sample Experi-
mental Distributions.
Biophys J. 2015; 108(12):2779–2782.
https://doi.org/10.1016/j.bpj.2015.05.024.
[180] Gil-Ley A, Bottaro S, Bussi G.
Empirical Corrections
to the Amber RNA Force Field with Target Metadynam-
ics.
J Chem Theory Comput. 2016;
12(6):2790–2798.
https://doi.org/10.1021/acs.jctc.6b00299.
[181] Dickson BM.
µ-tempered metadynamics: Artifact indepen-
dent convergence times for wide hills.
J Chem Phys. 2015;
143(23):234109. https://doi.org/10.1063/1.4937939.
[182] Khanjari N, Eslami H, Müller-Plathe F.
Adaptive-numerical-
bias metadynamics. J Comput Chem. 2017; 38(31):2721–2729.
https://doi.org/10.1002/jcc.25066.
[183] Hošek P, Toulcová D, Bortolato A, Spiwok V. Altruistic Metady-
namics: Multisystem Biased Simulation. J Phys Chem B. 2016;
120(9):2209–2215. https://doi.org/10.1021/acs.jpcb.6b00087.
[184] Hošek P, Kříž P, Toulcová D, Spiwok V. Multisystem altruistic
metadynamics—Well-tempered variant. J Chem Phys. 2017;
146(12):125103. https://doi.org/10.1063/1.4978939.
[185] Lindner JO, Röhr MIS.
Metadynamics for automatic sam-
pling of quantum property manifolds: exploration of molec-
ular biradicality landscapes.
Phys Chem Chem Phys. 2019;
21(44):24716–24722. https://doi.org/10.1039/c9cp05182a.
[186] Mitsuta Y, Shigeta Y. Analytical Method Using a Scaled Hy-
persphere Search for High-Dimensional Metadynamics Sim-
ulations.
J Chem Theory Comput. 2020; 16(6):3869–3878.
https://doi.org/10.1021/acs.jctc.0c00010.
51 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[187] Bonomi
M,
Branduardi
D,
Bussi
G,
Camilloni
C,
Provasi
D,
Raiteri
P,
Donadio
D,
Marinelli
F,
Pietrucci
F,
Broglia
RA,
et
al.
PLUMED:
A
portable
plugin
for
free-energy
calculations
with
molecular
dynam-
ics.
Comput Phys Commun. 2009;
180(10):1961–1972.
https://doi.org/10.1016/j.cpc.2009.05.011.
[188] The PLUMED Consortium. Promoting transparency and re-
producibility in enhanced molecular simulations. Nat Meth-
ods. 2019; 16:670–673.
https://doi.org/10.1038/s41592-019-
0506-8, For the full list of researches from the PLUMED Con-
sortium, see https://www.plumed-nest.org/consortium.html.
[189] PLUMED-NEST;. Accessed: June 30, 2022. https://www.plumed-
nest.org.
[190] Invernizzi M, Valsson O, Parrinello M. Coarse graining from
variationally enhanced sampling applied to the Ginzburg–
Landau model. Proc Natl Acad Sci. 2017; 114(13):3370–3374.
https://doi.org/10.1073/pnas.1618455114.
[191] Rubinstein
R.
The
Cross-Entropy
Method
for
Combinatorial
and
Continuous
Optimization.
Methodol
Comput
Appl
Probab.
1999;
1(2):127–190.
https://doi.org/10.1023/a:1010091220143.
[192] Shell MS. The relative entropy is fundamental to multiscale
and inverse thermodynamic problems.
J Chem Phys. 2008;
129(14):144108. https://doi.org/10.1063/1.2992060.
[193] Bilionis I, Koutsourelakis PS.
Free energy computations
by minimization of Kullback–Leibler divergence:
An eﬃ-
cient adaptive biasing potential method for sparse rep-
resentations.
J Comput Phys. 2012;
231(9):3849–3870.
https://doi.org/10.1016/j.jcp.2012.01.033.
[194] Zhang W, Wang H, Hartmann C, Weber M, Schütte C. Appli-
cations of the Cross-Entropy Method to Importance Sampling
and Optimal Control of Diffusions. SIAM J Sci Comput. 2014;
36(6):A2654–A2672. https://doi.org/10.1137/14096493x.
[195] Pampel B, Valsson O.
Improving the Eﬃciency of Vari-
ationally Enhanced Sampling with Wavelet-Based Bias Po-
tentials.
J Chem Theory Comput. 2022; 18(7):4127–4141.
https://doi.org/10.1021/acs.jctc.2c00197.
[196] Bonati
L,
Zhang
YY,
Parrinello
M.
Neural
networks-based
variationally
enhanced
sampling.
Proc
Natl
Acad
Sci.
2019;
116(36):17641–17647.
https://doi.org/10.1073/pnas.1907975116.
[197] McCarty J, Valsson O, Parrinello M.
Bespoke Bias for Ob-
taining Free Energy Differences within Variationally Enhanced
Sampling.
J Chem Theory Comput. 2016; 12(5):2162–2169.
https://doi.org/10.1021/acs.jctc.6b00125.
[198] Piaggi PM, Valsson O, Parrinello M. A variational approach to
nucleation simulation. Faraday Discuss. 2016; 195:557–568.
https://doi.org/10.1039/c6fd00127k.
[199] Valsson O, Parrinello M.
Well-Tempered Variational Ap-
proach to Enhanced Sampling. J Chem Theory Comput. 2015;
11(5):1996–2002. https://doi.org/10.1021/acs.jctc.5b00076.
[200] McCarty J, Valsson O, Tiwary P, Parrinello M.
Vari-
ationally
Optimized
Free-Energy
Flooding
for
Rate
Calculation.
Phys
Rev
Lett.
2015;
115(7):070601.
https://doi.org/10.1103/physrevlett.115.070601.
[201] Palazzesi
F,
Valsson
O,
Parrinello
M.
Confor-
mational
Entropy
as
Collective
Variable
for
Pro-
teins.
J
Phys
Chem
Lett.
2017;
8(19):4752–4756.
https://doi.org/10.1021/acs.jpclett.7b01770.
[202] Wu Y, Car R.
Variational Approach to Monte Carlo Renor-
malization Group.
Phys Rev Lett. 2017; 119(22):220602.
https://doi.org/10.1103/physrevlett.119.220602.
[203] Wu Y, Car R.
Determination of the critical manifold
tangent
space
and
curvature
with
Monte
Carlo
renor-
malization
group.
Phys
Rev
E.
2019;
100(2):022138.
https://doi.org/10.1103/physreve.100.022138.
[204] Wu
Y,
Car
R.
Monte
Carlo
Renormalization
Group
for
Classical
Lattice
Models
with
Quenched
Dis-
order.
Phys
Rev
Lett.
2020;
125(19):190601.
https://doi.org/10.1103/physrevlett.125.190601.
[205] Piaggi
PM,
Parrinello
M.
Multithermal-Multibaric
Molecular
Simulations
from
a
Variational
Prin-
ciple.
Phys
Rev
Lett.
2019;
122(5):050601.
https://doi.org/10.1103/physrevlett.122.050601.
[206] Debnath J, Invernizzi M, Parrinello M. Enhanced Sampling of
Transition States. J Chem Theory Comput. 2019; 15(4):2454–
2459. https://doi.org/10.1021/acs.jctc.8b01283.
[207] Invernizzi M, Parrinello M.
Making the Best of a Bad
Situation:
A Multiscale Approach to Free Energy Calcu-
lation.
J Chem Theory Comput. 2019;
15(4):2187–2194.
https://doi.org/10.1021/acs.jctc.9b00032.
[208] Okumura H, Okamoto Y. Monte Carlo simulations in multi-
baric–multithermal ensemble. Chem Phys Lett. 2004; 383(3-
4):391–396. https://doi.org/10.1016/j.cplett.2003.10.152.
[209] Shell MS, Debenedetti PG, Panagiotopoulos AZ.
Gen-
eralization
of
the
Wang-Landau
method
for
off-
lattice
simulations.
Phys
Rev
E.
2002;
66(5):056703.
https://doi.org/10.1103/physreve.66.056703.
[210] Yang
L,
Gao
YQ.
A
selective
integrated
temper-
ing
method.
J
Chem
Phys.
2009;
131(21):214109.
https://doi.org/10.1063/1.3266563.
[211] Micheletti
C,
Laio
A,
Parrinello
M.
Reconstructing
the
Density
of
States
by
History-Dependent
Meta-
dynamics.
Phys
Rev
Lett.
2003;
92(17):170601.
https://doi.org/10.1103/physrevlett.92.170601.
[212] Gao YQ.
An integrate-over-temperature approach for en-
hanced sampling.
J Chem Phys. 2008;
128(6):064105.
https://doi.org/10.1063/1.2825614.
[213] Yang L, Liu CW, Shao Q, Zhang J, Gao YQ. From Thermodynam-
ics to Kinetics: Enhanced Sampling of Rare Events. Acc Chem
Res. 2015; 48(4):947–955. https://doi.org/10.1021/ar500267n.
52 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[214] Piaggi PM, Parrinello M.
Calculation of phase diagrams in
the multithermal-multibaric ensemble.
J Chem Phys. 2019;
150(24):244119. https://doi.org/10.1063/1.5102104.
[215] Singh S, Chopra M, Pablo JJd. Density of States–Based Molec-
ular Simulations.
Annu Rev Chem Biomol. 2012; 3(1):369–
394.
https://doi.org/10.1146/annurev-chembioeng-062011-
081032.
[216] Darve E, Pohorille A.
Calculating free energies using
average
force.
J
Chem
Phys.
2001;
115:9169–9183.
https://doi.org/10.1063/1.1410978.
[217] Lelièvre T, Rousset M, Stoltz G. Long-time convergence of an
Adaptive Biasing Force method. Nonlinearity. 2008; 21:1155–
1181. https://doi.org/10.1088/0951-7715/21/6/001.
[218] Benaïm M, Bréhier CE, Monmarché P. Analysis of an Adap-
tive Biasing Force method based on self-interacting dynamics.
Electron J Probab. 2020; 25:1–28. https://doi.org/10.1214/20-
ejp490.
[219] Darve E, Pohorille A.
Calculating free energies using
scaled-force
molecular
dynamics
algorithm.
Center
for Turbulence Research Annual Research Briefs;
2000.
https://doi.org/10.1080/08927020211975.
[220] Darve E, Wilson M, Pohorille A. Calculating Free Energies Us-
ing a Scaled-Force Molecular Dynamics Algorithm. Mol Sim.
2002; 28:113–144. https://doi.org/10.1080/08927020211975.
[221] Ryckaert J, Ciccotti G, Berendsen HJC.
Numerical inte-
gration of the cartesian equations of motion of a system
with constraints: Molecular dynamics of n-alkanes.
J Com-
put Phys. 1977; 23(3):327–341. https://doi.org/10.1016/0021-
9991(77)90098-5.
[222] Andersen HC.
Rattle:
A “velocity” version of the Shake
algorithm for molecular dynamics calculations.
J Com-
put Phys. 1983; 52(1):24–34.
https://doi.org/10.1016/0021-
9991(83)90014-1.
[223] Darve E, Rodríguez-Gómez D, Pohorille A.
Adaptive
biasing
force
method
for
scalar
and
vector
free
en-
ergy calculations.
J Chem Phys. 2008;
128(14):144120.
https://doi.org/10.1063/1.2829861.
[224] Alrachid H, Lelièvre T. Long-time convergence of an adaptive
biasing force method: Variance reduction by Helmholtz pro-
jection.
SMAI-Journal of Computational Mathematics. 2015;
1:55–82. https://doi.org/10.5802/smai-jcm.4.
[225] Lelièvre T, Rousset M, Stoltz G. Computation of free energy
proﬁles with parallel adaptive dynamics. J Chem Phys. 2007;
126(13):134111. https://doi.org/10.1063/1.2711185.
[226] Minoukadeh K, Chipot C, Lelièvre T.
Potential of Mean
Force Calculations: A Multiple-Walker Adaptive Biasing Force
Approach.
J Chem Theory Comput. 2010; 6(4):1008–1017.
https://doi.org/10.1021/ct900524t.
[227] Comer J, Phillips JC, Schulten K, Chipot C.
Multiple-
replica strategies for free-energy calculations in NAMD:
multiple-walker adaptive biasing force and walker selection
rules.
J Chem Theory Comput. 2014; 10(12):5276–5285.
https://doi.org/10.1021/ct500874p.
[228] Zheng L, Yang W.
Practically eﬃcient and robust free
energy calculations:
Double-integration orthogonal space
tempering.
J Chem Theory Comput. 2012; 8(3):810–823.
https://doi.org/10.1021/ct200726v.
[229] Lesage
A,
Lelièvre
T,
Stoltz
G,
Hénin
J.
Smoothed
Biasing
Forces
Yield
Unbiased
Free
Energies
with
the
Extended-System
Adaptive
Biasing
Force
Method.
J
Phys
Chem
B.
2017;
121(15):3676–3685.
https://doi.org/10.1021/acs.jpcb.6b10055.
[230] Kastner J, Thiel W.
Bridging the gap between thermody-
namic integration and umbrella sampling provides a novel
analysis method: “Umbrella integration”. J Chem Phys. 2005;
123(14):144104. https://doi.org/10.1063/1.2052648.
[231] Fu H, Shao X, Chipot C, Cai W. Extended Adaptive Biasing Force
algorithm. An on-the-ﬂy implementation for accurate free-
energy calculations. J Chem Theory Comput. 2016; 12:3506–
3513. https://doi.org/10.1021/acs.jctc.6b00447.
[232] Guo AZ, Sevgen E, Sidky H, Whitmer JK, Hubbell JA, de Pablo
JJ.
Adaptive enhanced sampling by force-biasing using
neural networks.
J Chem Phys. 2018;
148(13):134108.
https://doi.org/10.1063/1.5020733.
[233] Cao L, Stoltz G, Lelièvre T, Marinica MC, Athènes M. Free en-
ergy calculations from adaptive molecular dynamics simula-
tions with adiabatic reweighting. J Chem Phys. 2014; 140(10).
https://doi.org/10.1063/1.4866811.
[234] Chipot C, Lelièvre T.
Enhanced Sampling of Multidimen-
sional Free-Energy Landscapes Using Adaptive Biasing Forces.
SIAM Journal on Applied Mathematics. 2011; 71(5):1673–1695.
https://doi.org/10.1137/10080600x.
[235] Zhao T, Fu H, Lelièvre T, Shao X, Chipot C, Cai W. The Extended
Generalized Adaptive Biasing Force Algorithm for Multidimen-
sional Free-Energy Calculations. J Chem Theory Comput. 2017;
13(4):1566–1576. https://doi.org/10.1021/acs.jctc.7b00032.
[236] Phillips J, Hardy D, Maia J, Stone J, Ribeiro J, Bernardi R, Buch
R, Fiorin G, Hénin J, Jiang W, McGreevy R, Melo MCdR, Radak B,
Skeel R, Singharoy A, Wang Y, Roux B, Aksimentiev A, Luthey-
Schulten Z, Kale L, et al.
Scalable molecular dynamics on
CPU and GPU architectures with NAMD. J Chem Phys. 2020;
153(4):044130. https://doi.org/10.1063/5.0014475.
[237] Plimpton S.
Fast parallel algorithms for short-range
molecular-dynamics.
J Comput Phys. 1995; 117(1):1–19.
https://doi.org/10.1006/jcph.1995.1039.
[238] Abraham MJ, Murtola T, Schulz R, Páll S, Smith JC, Hess
B,
Lindahl
E.
GROMACS:
High
performance
molecu-
lar simulations through multi-level parallelism from lap-
tops to supercomputers.
SoftwareX. 2015;
1-2:19–25.
https://doi.org/10.1016/j.softx.2015.06.001.
[239] Humphrey W, Dalke A, Schulten K.
VMD: visual molec-
ular dynamics.
J Mol Graph. 1996;
14(1):33–8,
27–8.
https://doi.org/10.1016/0263-7855(96)00018-5.
[240] Hénin J, Lopes LJS, Fiorin G. Human learning for molecular
simulations: the Collective Variables Dashboard in VMD.
J
Chem Theory Comput. 2022; https://pubs.acs.org/doi/10.1021/
acs.jctc.1c01081.
53 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[241] Lagardère L, Jolly LH, Lipparini F, Aviat F, Stamm B, Jing ZF,
Harger M, Torabifard H, Cisneros GA, Schnieders MJ, Gresh
N, Maday Y, Ren PY, Ponder JW, Piquemal JP.
Tinker-HP: a
massively parallel molecular dynamics package for multiscale
simulations of large complex systems with advanced point
dipole polarizable force ﬁelds. Chem Sci. 2018; 9(4):956–972.
https://doi.org/10.1039/c7sc04531j.
[242] Kulhánek P, Mones L, Střelcová Z, Simon I, Fuxreiter M, Koča
J, et al., PMFLib–A Toolkit for Free Energy Calculations; 2011.
https://pmflib.ncbr.muni.cz.
[243] Sidky H, Colón YJ, Helfferich J, Sikora BJ, Bezik C, Chu W, Giberti
F, Guo AZ, Jiang X, Lequieu J, Li J, Moller J, Quevillon MJ, Rahimi
M, Ramezani-Dakhel H, Rathee VS, Reid DR, Sevgen E, Thapar
V, Webb MA, et al. SSAGES: Software Suite for Advanced Gen-
eral Ensemble Simulations. J Chem Phys. 2018; 148(4):044104.
https://doi.org/10.1063/1.5008853.
[244] Mitsutake A, Sugita Y, Okamoto Y.
Generalized-ensemble
algorithms for molecular simulations of biopolymers. Biopoly-
mers.
2001;
60:96–123.
https://doi.org/10.1002/1097-
0282(2001)60:2<96::AID-BIP1007>3.0.CO;2-F.
[245] Iba Y. Extended ensemble Monte Carlo. Intl J Mod Phys C.
2001; 12:623. https://doi.org/10.1142/S0129183101001912.
[246] Geyer CJ. Markov chain Monte Carlo maximum likelihood. In:
Computing Science and Statistics: The 23rd symposium on the in-
terface Fairfax: Interface Foundation; 1991. p. 156–163.
[247] Hukushima K, Nemoto K. Exchange Monte Carlo and applica-
tion to spin glass simulations. J Phys Soc Jpn. 1996; 65:1604–
1608. https://doi.org/10.1143/JPSJ.65.1604.
[248] Hansmann UHE. Parallel tempering algorithm for conforma-
tional studies of biological molecules. Chem Phys Lett. 1997;
281:140–150.
https://doi.org/10.1016/S0009-2614(97)01198-
6.
[249] Sugita Y, Okamoto Y. Replica-exchange molecular dynamics
method for protein folding. Chem Phys Lett. 1999; 314:141–
151. https://doi.org/10.1016/s0009-2614(99)01123-9.
[250] Sugita Y, Kitao A, Okamoto Y.
Multidimensional replica-
exchange method for free-energy calculations. J Chem Phys.
2000; 113:6042. https://doi.org/10.1063/1.1308516.
[251] Fukunishi H, Watanabe O, Takada S.
On the Hamil-
tonian
replica
exchange
method
for
eﬃcient
sam-
pling
of
biomolecular
systems:
Application
to
protein
structure
prediction.
J
Chem
Phys.
2002;
116:9058.
https://doi.org/10.1063/1.1472510.
[252] Jang S, Shin S, Pak Y.
Replica-exchange method using the
generalized effective potential. Phys Rev Lett. 2003; 91:58305.
https://doi.org/10.1103/physrevlett.91.058305.
[253] Kwak W, Hansmann UHE. Eﬃcient sampling of protein struc-
tures by model hopping.
Phys Rev Lett. 2005; 95:138102.
https://doi.org/10.1103/PhysRevLett.95.138102.
[254] Lyubartsev AP, Martsinovski AA, Shevkunov SV, Vorontsov-
Velyaminov PN. New approach to Monte Carlo calculation of
the free energy: Method of expanded ensembles. J Chem Phys.
1992; 96:1776–1783. https://doi.org/10.1063/1.462133.
[255] Marinari E, Parisi G.
Simulated tempering:
a new
Monte Carlo scheme.
Europhys Lett. 1992; 19(6):451–458.
https://doi.org/10.1209/0295-5075/19/6/002.
[256] Geyer CJ, Thompson EA. Annealing Markov chain Monte Carlo
with applications to ancestral inference. J Am Stat Assoc. 1995;
90:909–920. https://doi.org/10.2307/2291325.
[257] Li H, Fajer M, Yang W. Simulated scaling method for localized
enhanced sampling and simultaneous “alchemical” free en-
ergy simulations: A general method for molecular mechanical,
quantum mechanical, and quantum mechanical/molecular
mechanical simulations.
J Chem Phys. 2007; 126:024106.
https://doi.org/10.1063/1.2424700.
[258] Park
S.
Comparison
of
the
serial
and
parallel
al-
gorithms
of
generalized
ensemble
simulations:
An
analytical
approach.
Phys
Rev
E.
2008;
77:16709.
https://doi.org/10.1103/physreve.77.016709.
[259] Sindhikara D, Meng Y, Roitberg AE. Exchange frequency in
replica exchange molecular dynamics.
J Chem Phys. 2008;
128:24103. https://doi.org/10.1063/1.2816560.
[260] Sindhikara D, Emerson DJ, Roitberg AE.
Exchange of-
ten
and
properly
in
replica
exchange
molecular
dy-
namics.
J Chem Theory Comput. 2010;
6:2804–2808.
https://doi.org/10.1021/ct100281c.
[261] Nadler W, Hansmann UHE.
Optimized replica exchange
moves for molecular dynamics. Phys Rev E. 2007; 76:057102.
https://doi.org/10.1103/physreve.76.057102.
[262] Rhee YM, Pande VS.
Multiplexed-replica exchange molec-
ular dynamics method for protein folding simulation.
Bio-
phys J. 2003; 84:775–786.
https://doi.org/10.1016/s0006-
3495(03)74897-8.
[263] Zuckerman DM, Lyman E.
A second look at canoni-
cal sampling of biomolecules using replica exchange sim-
ulation.
J Chem Theory Comput. 2006;
2:1200–1202.
https://doi.org/10.1021/ct0600464.
[264] Zheng W, Andrec M, Gallicchio E, Levy RM. Simulating replica
exchange simulations of protein folding with a kinetic net-
work model. Proc Natl Acad Sci USA. 2007; 104:15340–15345.
https://doi.org/10.1073/pnas.0704418104.
[265] Nymeyer H. How eﬃcient is replica exchange molecular dy-
namics? An analytic approach. J Chem Theory Comput. 2008;
4:626. https://doi.org/10.1021/ct7003337.
[266] Denschlag R, Lingenheil M, Tavan P. Eﬃciency reduction and
pseudo-convergence in replica exchange sampling of peptide
folding-unfolding equilibria. Chem Phys Lett. 2008; 458:244–
248. https://doi.org/10.1016/j.cplett.2008.04.114.
[267] Rosta E, Hummer G.
Error and eﬃciency of replica ex-
change molecular dynamics simulations. J Chem Phys. 2009;
131:165102. https://doi.org/10.1063/1.3249608.
[268] Rosta E, Hummer G.
Error and eﬃciency of simulated
tempering simulations.
J Chem Phys. 2010;
132:34102.
https://doi.org/10.1063/1.3290767.
54 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[269] Kofke DA.
On the acceptance probability of replica-
exchange Monte Carlo trials.
J Chem Phys. 2002; 117:6911.
https://doi.org/10.1063/1.1507776.
[270] Katzgraber HG, Trebst S, Huse DA, Troyer M.
Feedback-
optimized
parallel
tempering
Monte
Carlo.
J
Stat
Mech. 2006;
2006:03018.
https://doi.org/10.1088/1742-
5468/2006/03/p03018.
[271] Trebst S, Troyer M, Hansmann UHE.
Optimized parallel
tempering simulations of proteins.
J Chem Phys. 2006;
124:174903. https://doi.org/10.1063/1.2186639.
[272] Nadler W, Hansmann UHE.
Generalized ensemble and
tempering simulations: A uniﬁed view.
Phys Rev E. 2007;
75:026109. https://doi.org/10.1103/physreve.75.026109.
[273] Gront D, Kolinski A.
Eﬃcient scheme for optimization of
parallel tempering {Monte Carlo} method.
J Phys:
Con-
dens Matter. 2007; 19:36225.
https://doi.org/10.1088/0953-
8984/19/3/036225.
[274] Park
S,
Pande
VS.
Choosing
weights
for
simu-
lated
tempering.
Phys
Rev
E.
2007;
76:016703.
https://doi.org/10.1103/physreve.76.016703.
[275] Shenfeld DK, Xu H, Eastwood MP, Dror RO, Shaw DE.
Minimizing
thermodynamic
length
to
select
interme-
diate
states
for
free
energy
calculations
and
replica-
exchange simulations.
Phys Rev E. 2009; p. To appear.
https://doi.org/10.1103/physreve.80.046705.
[276] Neuhaus T, Magiera MP, Hansmann UHE. Eﬃcient parallel
tempering for ﬁrst-order phase transitions. Phys Rev E. 2007;
76:045701. https://doi.org/10.1103/physreve.76.045701.
[277] Kim
J,
Keyes
T,
Straub
JE.
Generalized
replica
ex-
change
method.
J
Chem
Phys.
2010;
132:224107.
https://doi.org/10.1063/1.3432176.
[278] Mitsutake A, Okamoto Y.
Replica-exchange extensions of
simulated tempering method. J Chem Phys. 2004; 121:2491.
https://doi.org/10.1063/1.1766015.
[279] Mitsutake A, Sugita Y, Okamoto Y.
Replica-exchange
multicanonical
and
multicanonical
replica-exchange
Monte
Carlo
simulations
of
peptides.
I.
Formulation
and benchmark test.
J Chem Phys. 2003;
118:6664.
https://doi.org/10.1063/1.1555847.
[280] Okur A, Roe DR, Cui G, Hornak V, Simmerling C. Improving con-
vergence of replica-exchange simulations through coupling to
a high-temperature structure reservoir. J Chem Theory Com-
put. 2007; 3:557. https://doi.org/10.1021/ct600263e.
[281] Gallicchio E, Levy RM, Parashar M.
Asynchronous replica
exchange for molecular simulations. J Comput Chem. 2008;
29:288–794. https://doi.org/10.1002/jcc.20839.
[282] Hansmann UHE.
Temperature random walk sampling
of
protein
conﬁgurations.
Phys
A.
2010;
389:1400.
https://doi.org/10.1016/j.physa.2009.12.027.
[283] Madras N, Randall D. Markov Chain decomposition for con-
vergence rate analysis. Annals Appl Prob. 2002; 12(2):581–606.
https://doi.org/10.1214/aoap/1026915617.
[284] Bhatnagar N, Randall D. Torpid mixing of simulated temper-
ing on the Potts model. In: SODA ’04: Proceedings of the ﬁfteenth
annual ACM-SIAM symposium on Discrete algorithms Philadel-
phia, PA, USA: Society for Industrial and Applied Mathematics;
2004. p. 478–487.
[285] Woodard DB. Conditions for rapid mixing of parallel and sim-
ulated tempering on multimodal distributions. Ann Appl Prob.
2009; 19(2):617–640. https://doi.org/10.1214/08-aap555.
[286] Woodard DB, Schmidler SC, Huber M. Suﬃcient conditions
for torpid mixing of parallel and simulated tempering. Elect J
Prob. 2009; 14:780–804. https://doi.org/10.1214/ejp.v14-638.
[287] Liu P, Kim B, Friesner RA, Berne BJ.
Replica exchange with
solute tempering: A method for sampling biological systems in
explicit water. Proc Natl Acad Sci. 2005; 102(39):13749–13754.
https://doi.org/10.1073/pnas.0506346102.
[288] Wang L, Friesner RA, Berne BJ. Replica Exchange with Solute
Scaling: A More Eﬃcient Version of Replica Exchange with So-
lute Tempering (REST2). J Phys Chem B. 2011; 115(30):9431–
9438. https://doi.org/10.1021/jp204407d.
[289] Wang L, Deng Y, Knight JL, Wu Y, Kim B, Sherman W, Shelley JC,
Lin T, Abel R. Modeling Local Structural Rearrangements Using
FEP/REST: Application to Relative Binding Aﬃnity Predictions
of CDK2 Inhibitors. J Chem Theory Comput. 2013; 9(2):1282–
1293. https://doi.org/10.1021/ct300911a.
[290] Abrams C, Bussi G.
Enhanced Sampling in Molecu-
lar Dynamics Using Metadynamics, Replica-Exchange, and
Temperature-Acceleration.
Entropy. 2014; 16(1):163–199.
https://doi.org/10.3390/e16010163.
[291] Gallicchio
E,
Xia
J,
Flynn
WF,
Zhang
B,
Samlalsingh
S,
Mentes
A,
Levy
RM.
Asynchronous
Replica
Ex-
change
Software
for
Grid
and
Heterogeneous
Com-
puting.
Comput
Phys
Commun.
2015;
196:236–246.
https://doi.org/10.1016/j.cpc.2015.06.010.
[292] Itoh SG, Okumura H.
Replica-Permutation Method with
the Suwa–Todo Algorithm beyond the Replica-Exchange
Method.
J Chem Theory Comput. 2013;
9(1):570–581.
https://doi.org/10.1021/ct3007919.
[293] Liu
Y,
Li
W,
Mu
Y.
Optimization
of
Replica
Ex-
change
Temperature
Ladder
under
the
Well-Tempered
Ensemble.
Chem
Phys
Lett.
2018;
711:66–72.
https://doi.org/10.1016/j.cplett.2018.09.036.
[294] Qi R, Wei G, Ma B, Nussinov R.
Replica Exchange Molec-
ular Dynamics:
A Practical Application Protocol with Solu-
tions to Common Problems and a Peptide Aggregation and
Self-Assembly Example.
In:
Nilsson BL, Doran TM, edi-
tors. Peptide Self-Assembly: Methods and Protocols Methods in
Molecular Biology, New York, NY: Springer; 2018.p. 101–119.
https://doi.org/10.1007/978-1-4939-7811-3_5.
[295] Park S, Ensign DL, Pande VS.
Bayesian update method for
adaptive weighted sampling.
Phys Rev E. 2006; 74:066703.
https://doi.org/10.1103/physreve.74.066703.
[296] Chelli R.
Optimal weights in serial generalized-ensemble
simulations.
J Chem Theory Comput. 2010; 6:1935–1950.
https://doi.org/10.1021/ct100105z.
55 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[297] Miller MA, Amon LM, Reinhardt WP.
Should one ad-
just
the
maximum
step
size
in
a
Metropolis
Monte
Carlo simulation?
Chem Phys Lett. 2000;
331:278.
https://doi.org/10.1016/s0009-2614(00)01217-3.
[298] Bussi G, Laio A, Parrinello M.
Equilibrium Free Energies
from Nonequilibrium Metadynamics.
Phys Rev Lett. 2006;
96:090601. https://doi.org/10.1103/physrevlett.96.090601.
[299] Belardinelli RE, Pereyra VD. Wang-Landau algorithm: a the-
oretical analysis of the saturation of the error. J Chem Phys.
2007; 127(18):184105. https://doi.org/10.1063/1.2803061.
[300] Belardinelli R, Manzi S, Pereyra V.
Analysis of the conver-
gence of the 1/t and Wang-Landau algorithms in the calcula-
tion of multidimensional integrals.
Phys Rev E. 2008; 78(6).
https://doi.org/10.1103/PhysRevE.78.067701.
[301] Atchadé YF, Liu JS. The Wang-Landau Algorithm in General
State Spaces: Applications and Convergence Analysis.
Stat
Sinica. 2010; 20(11):209–233.
[302] Siderius DW, Shen VK. Use of the Grand Canonical Transition-
Matrix Monte Carlo Method to Model Gas Adsorption in
Porous Materials. J Phys Chem C. 2013; 117(11):5861–5872.
https://doi.org/10.1021/jp400480q.
[303] Lidmar
J.
Improving
the
eﬃciency
of
extended
ensemble
simulations:
The
accelerated
weight
his-
togram
method.
Phys
Rev
E.
2012;
85(5):56708.
https://doi.org/10.1103/PhysRevE.85.056708.
[304] Tan
Z.
Optimally
Adjusted
Mixture
Sam-
pling
and
Locally
Weighted
Histogram
Analy-
sis.
J
Comput
Graph
Stat.
2017;
26(1):54–65.
https://doi.org/10.1080/10618600.2015.1113975.
[305] Eastman P, Swails J, Chodera JD, McGibbon RT, Zhao Y,
Beauchamp KA, Wang LP, Simmonett AC, Harrigan MP,
Stern CD, Wiewiora RP, Brooks BR, Pande VS.
OpenMM
7: Rapid development of high performance algorithms for
molecular dynamics.
PLOS Comput Biol. 2017; 13(7):1–17.
https://doi.org/10.1371/journal.pcbi.1005659.
[306] An Introduction to Markov State Models and Their Ap-
plication to Long Timescale Molecular Simulation.
Ad-
vances in Experimental Medicine and Biology, Springer; 2014.
https://doi.org/10.1007/978-94-007-7606-7.
[307] Husic BE, Pande VS.
Markov State Models: From an Art
to a Science.
J Am Chem Soc. 2018; 140(7):2386–2396.
https://doi.org/10.1021/jacs.7b12191.
[308] Shirts MR, Pande VS.
Mathematical Analysis of Coupled
Parallel Simulations.
Phys Rev Lett. 2001; 86:4983–4987.
https://doi.org/10.1103/PhysRevLett.86.4983.
[309] Zimmerman MI, Porter JR, Sun X, Silva RR, Bowman GR.
Choice of Adaptive Sampling Strategy Impacts State Discov-
ery, Transition Probabilities, and the Apparent Mechanism
of Conformational Changes.
J Chem Theory Comput. 2018;
14(11):5459–5475. https://doi.org/10.1021/acs.jctc.8b00500.
[310] Hinrichs NS, Pande VS.
Calculation of the distribution of
eigenvalues and eigenvectors in Markovian state models for
molecular dynamics.
J Chem Phys. 2007; 126(24):244101.
https://doi.org/10.1063/1.2740261.
[311] Huang X, Bowman GR, Bacallado S, Pande VS.
Rapid
equilibrium sampling initiated from nonequilibrium data.
Proc
Natl
Acad
Sci
USA.
2009;
106(47):19765–19769.
https://doi.org/10.1073/pnas.0909088106.
[312] Bowman GR, Ensign DL, Pande VS.
Enhanced Model-
ing via Network Theory:
Adaptive Sampling of Markov
State Models.
J Chem Theory Comput. 2010; 6(3):787–794.
https://doi.org/10.1021/ct900620b.
[313] Voelz VA, Elman B, Razavi AM, Zhou G. Surprisal Metrics for
Quantifying Perturbed Conformational Dynamics in Markov
State Models.
J Chem Theory Comput. 2014; 10(12):5716–
5728. https://doi.org/10.1021/ct500827g.
[314] Weber JK, Pande VS.
Characterization and Rapid Sam-
pling
of
Protein
Folding
Markov
State
Model
Topolo-
gies.
J Chem Theory Comput. 2011;
7(10):3405–3411.
https://doi.org/10.1021/ct2004484.
[315] Doerr S, De Fabritiis G.
On-the-Fly Learning and Sam-
pling of Ligand Binding by High-Throughput Molecular Sim-
ulations.
J Chem Theory Comput. 2014; 10(5):2064–2069.
https://doi.org/10.1021/ct400919u.
[316] Lecina D, Gilabert JF, Guallar V. Adaptive simulations, towards
interactive protein-ligand modeling. Sci Rep. 2017; 7(1):8466.
https://doi.org/10.1038/s41598-017-08445-5, number: 1 Pub-
lisher: Nature Publishing Group.
[317] Shamsi Z, Moffett AS, Shukla D. Enhanced unbiased sampling
of protein dynamics using evolutionary coupling information.
Sci Rep. 2017; 7(1):12700.
https://doi.org/10.1038/s41598-
017-12874-7, number: 1 Publisher: Nature Publishing Group.
[318] Pronk S, Bowman GR, Hess B, Larsson P, Haque IS, Pande VS,
Pouya I, Beauchamp K, Kasson PM, Lindahl E. Copernicus: A
new paradigm for parallel adaptive molecular dynamics. In:
SC ’11: Proceedings of 2011 International Conference for High Per-
formance Computing, Networking, Storage and Analysis; 2011. p.
1–10. https://doi.org/10.1145/2063384.2063465.
[319] Noé F, Banisch R, Clementi C.
Commute Maps: Separat-
ing Slowly Mixing Molecular Conﬁgurations for Kinetic Mod-
eling.
J Chem Theory Comput. 2016; 12(11):5620–5630.
https://doi.org/10.1021/acs.jctc.6b00762.
[320] Hruska E, Abella JR, Nüske F, Kavraki LE, Clementi C.
Quantitative comparison of adaptive sampling methods for
protein dynamics.
J Chem Phys. 2018;
149(24):244119.
https://doi.org/10.1063/1.5053582.
[321] Kukharenko O, Sawade K, Steuer J, Peter C.
Using Di-
mensionality
Reduction
to
Systematically
Expand
Con-
formational
Sampling
of
Intrinsically
Disordered
Pep-
tides.
J Chem Theory Comput. 2016; 12(10):4726–4734.
https://doi.org/10.1021/acs.jctc.6b00503.
56 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[322] Bacci
M,
Vitalis
A,
Caﬂisch
A.
A
molecular
simula-
tion
protocol
to
avoid
sampling
redundancy
and
dis-
cover
new
states.
Biochimica
et
Biophysica
Acta
(BBA)
-
General
Subjects.
2015;
1850(5):889–902.
https://doi.org/10.1016/j.bbagen.2014.08.013,
recent
de-
velopments of molecular dynamics.
[323] Pérez A, Herrera-Nieto P, Doerr S, De Fabritiis G. Adaptive-
Bandit: A Multi-armed Bandit Framework for Adaptive Sam-
pling in Molecular Simulations. J Chem Theory Comput. 2020;
16(7):4685–4693. https://doi.org/10.1021/acs.jctc.0c00205.
[324] Shamsi
Z,
Cheng
KJ,
Shukla
D.
Reinforcement
Learning
Based
Adaptive
Sampling:
REAPing
Re-
wards
by
Exploring
Protein
Conformational
Land-
scapes.
J
Phys
Chem
B.
2018;
122(35):8386–8395.
https://doi.org/10.1021/acs.jpcb.8b06521.
[325] Zimmerman MI, Bowman GR.
FAST Conformational
Searches
by
Balancing
Exploration/Exploitation
Trade-
Offs.
J Chem Theory Comput. 2015;
11(12):5747–5757.
https://doi.org/10.1021/acs.jctc.5b00737.
[326] Huber GA, Kim S.
Weighted-ensemble Brownian dynamics
simulations for protein association reactions. Biophys J. 1996;
70(1):97–110.
https://doi.org/10.1016/S0006-3495(96)79552-
8.
[327] Zuckerman DM, Chong LT.
Weighted Ensemble Sim-
ulation:
Review
of
Methodology,
Applications,
and
Software.
Annu Rev Bioph Biom. 2017;
46(1):43–57.
https://doi.org/10.1146/annurev-biophys-070816-033834.
[328] Bhatt D, Zhang BW, Zuckerman DM. Steady-state simulations
using weighted ensemble path sampling. J Chem Phys. 2010;
133(1):014110. https://doi.org/10.1063/1.3456985.
[329] Suárez E, Lettieri S, Zwier MC, Stringer CA, Subramanian SR,
Chong LT, Zuckerman DM.
Simultaneous Computation of
Dynamical and Equilibrium Information Using a Weighted
Ensemble of Trajectories.
J Chem Theory Comput. 2014;
10(7):2658–2667. https://doi.org/10.1021/ct401065r.
[330] Adhikari
U,
Mostoﬁan
B,
Copperman
J,
Subramanian
SR, Petersen AA, Zuckerman DM.
Computational Es-
timation
of
Microsecond
to
Second
Atomistic
Fold-
ing Times.
J Am Chem Soc. 2019;
141(16):6519–6526.
https://doi.org/10.1021/jacs.8b10735.
[331] Pan AC, Sezer D, Roux B. Finding Transition Pathways Using
the String Method with Swarms of Trajectories. J Phys Chem B.
2008; 112(11):3432–3440. https://doi.org/10.1021/jp0777059.
[332] Faradjian AK, Elber R.
Computing time scales from re-
action coordinates by milestoning.
J Chem Phys. 2004;
120(23):10880–10889. https://doi.org/10.1063/1.1738640.
[333] van Erp TS, Moroni D, Bolhuis PG.
A novel path sampling
method for the calculation of rate constants.
J Chem Phys.
2003; 118(17):7762–7774. https://doi.org/10.1063/1.1562614.
[334] Allen RJ, Warren PB, ten Wolde PR.
Sampling Rare Switch-
ing Events in Biochemical Networks.
Phys Rev Lett. 2005;
94:018104. https://doi.org/10.1103/PhysRevLett.94.018104.
[335] Cérou F, Guyader A, Lelièvre T, Pommier D. A multiple replica
approach to simulate reactive trajectories. J Chem Phys. 2011;
134:054108. https://doi.org/10.1063/1.3518708.
[336] Teo
I,
Mayne
CG,
Schulten
K,
Lelièvre
T.
Adaptive
multilevel
splitting
method
for
molecular
dynam-
ics
calculation
of
benzamidine-trypsin
dissociation
time.
J Chem Theory Comput. 2016;
12(6):2983–2989.
https://doi.org/10.1021/acs.jctc.6b00277.
[337] Deganutti G, Moro S, Reynolds CA.
A Supervised Molec-
ular
Dynamics
Approach
to
Unbiased
Ligand–Protein
Unbinding.
J Chem Inf Model. 2020;
60(3):1804–1817.
https://doi.org/10.1021/acs.jcim.9b01094.
[338] Cuzzolin A, Sturlese M, Deganutti G, Salmaso V, Sabbadin
D, Ciancetta A, Moro S.
Deciphering the Complexity of Lig-
and–Protein Recognition Pathways Using Supervised Molec-
ular Dynamics (SuMD) Simulations. J Chem Inf Model. 2016;
56(4):687–705. https://doi.org/10.1021/acs.jcim.5b00702.
[339] E
W,
Vanden-Eijnden
E.
Transition-Path
Theory
and
Path-Finding
Algorithms
for
the
Study
of
Rare
Events.
Annu
Rev
Phys
Chem.
2010;
61(1):391–420.
https://doi.org/10.1146/annurev.physchem.040808.090412.
[340] Hussain
S,
Haji-Akbari
A.
Studying
rare
events
us-
ing
forward-ﬂux
sampling:
Recent
breakthroughs
and
future
outlook.
J
Chem
Phys.
2020;
152(6):060901.
https://doi.org/10.1063/1.5127780.
[341] Bogetti AT, Mostoﬁan B, Dickson A, Pratt AJ, Saglam
AS, Harrison PO, Adelman JL, Dudek M, Torrillo PA, De-
Grave AJ, Adhikari U, Zwier MC, Zuckerman DM, Chong
LT.
A Suite of Tutorials for the WESTPA Rare-Events Sam-
pling Software [Article v1.0].
LiveCoMS. 2019; 1(2):10607.
https://doi.org/10.33011/livecoms.1.2.10607.
[342] Rosso L, Mináry P, Zhu Z, Tuckerman ME. On the use of the
adiabatic molecular dynamics technique in the calculation of
free energy proﬁles. J Chem Phys. 2002; 116(11):4389–4402.
https://doi.org/10.1063/1.1448491.
[343] Maragliano L, Vanden-Eijnden E.
A temperature ac-
celerated
method
for
sampling
free
energy
and
de-
termining
reaction
pathways
in
rare
events
simula-
tions.
Chem
Phys
Lett.
2006;
426(1-3):168
–
175.
https://doi.org/10.1016/j.cplett.2006.05.062.
[344] Maragliano L, Vanden-Eijnden E. Single-sweep methods for
free energy calculations. J Chem Phys. 2008; 128(18):184110.
https://doi.org/10.1063/1.2907241.
[345] Abrams JB, Tuckerman ME. Eﬃcient and Direct Generation of
Multidimensional Free Energy Surfaces via Adiabatic Dynam-
ics without Coordinate Transformations. J Phys Chem B. 2008;
112(49):15742–15757. https://doi.org/10.1021/jp805039u.
[346] Chen M, Cuendet MA, Tuckerman ME.
Heating and
ﬂooding: A uniﬁed approach for rapid generation of free
energy surfaces.
J Chem Phys. 2012;
137(2):024102.
https://doi.org/10.1063/1.4733389.
57 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[347] VandeVondele J, Rothlisberger U. Canonical Adiabatic Free
Energy Sampling (CAFES): A Novel Method for the Exploration
of Free Energy Surfaces. J Phys Chem B. 2002; 106(1):203–208.
https://doi.org/10.1021/jp013346k.
[348] Abrams CF, Vanden-Eijnden E.
On-the-ﬂy free energy
parameterization
via
temperature
accelerated
molec-
ular dynamics.
Chem Phys Lett. 2012;
547:114–119.
https://doi.org/10.1016/j.cplett.2012.07.064.
[349] Wu X, Wang S.
Enhancing systematic motion in molecular
dynamics simulation. J Chem Phys. 1999; 110(19):9401–9410.
https://doi.org/10.1063/1.478948.
[350] Wu X, Brooks BR.
Self-guided Langevin dynamics sim-
ulation method.
Chem Phys Lett. 2003; 381(3):512–518.
https://doi.org/https://doi.org/10.1016/j.cplett.2003.10.013.
[351] Wu X, Brooks BR, Vanden-Eijnden E.
Self-guided Langevin
dynamics via generalized Langevin equation. J Comp Chem.
2015; 37(6):595–601. https://doi.org/10.1002/jcc.24015.
[352] Wu X, Brooks BR.
Reformulation of the self-guided molec-
ular simulation method. J Chem Phys. 2020; 153(9):094112.
https://doi.org/10.1063/5.0019086.
[353] Bussi G, Gervasio FL, Laio A, Parrinello M. Free-Energy Land-
scape for β Hairpin Folding from Combined Parallel Temper-
ing and Metadynamics. J Am Chem Soc. 2006; 128(41):13435–
13441. https://doi.org/10.1021/ja062463w.
[354] Sugita Y, Kitao A, Okamoto Y.
Multidimensional replica-
exchange method for free-energy calculations. J Chem Phys.
2000; 113(15):6042–6051. https://doi.org/10.1063/1.1308516.
[355] Bussi G.
Hamiltonian replica exchange in GROMACS: a
ﬂexible implementation.
Mol Phys. 2013; 112(3-4):379–384.
https://doi.org/10.1080/00268976.2013.824126.
[356] Deighan M, Bonomi M, Pfaendtner J.
Eﬃcient Simula-
tion of Explicitly Solvated Proteins in the Well-Tempered En-
semble.
J Chem Theory Comput. 2012; 8(7):2189–2192.
https://doi.org/10.1021/ct300297t.
[357] Barducci A, Bonomi M, Prakash MK, Parrinello M. Free-energy
landscape of protein oligomerization from atomistic simula-
tions.
Proc Natl Acad Sci USA. 2013; 110(49):E4708–E4713.
https://doi.org/10.1073/pnas.1320077110.
[358] Gil-Ley A, Bussi G.
Enhanced Conformational Sampling
Using Replica Exchange with Collective-Variable Temper-
ing.
J Chem Theory Comput. 2015;
11(3):1077–1085.
https://doi.org/10.1021/ct5009087.
[359] Yonezawa Y, Shimoyama H, Nakamura H.
Multicanonical
molecular dynamics simulations combined with Metadynam-
ics for the free energy landscape of a biomolecular system
with high energy barriers. Chem Phys Lett. 2011; 501(4-6):598–
602. https://doi.org/10.1016/j.cplett.2010.11.061.
[360] Yang YI, Zhang J, Che X, Yang L, Gao YQ.
Eﬃcient
sampling over rough energy landscapes with high barri-
ers: A combination of metadynamics with integrated tem-
pering sampling.
J Chem Phys. 2016;
144(9):094105.
https://doi.org/10.1063/1.4943004.
[361] Yang
YI,
Niu
H,
Parrinello
M.
Combining
Metadynamics
and
Integrated
Tempering
Sam-
pling.
J
Phys
Chem
Lett.
2018;
9(22):6426–6430.
https://doi.org/10.1021/acs.jpclett.8b03005.
[362] Moradi M, Tajkhorshid E.
Driven Metadynamics:
Recon-
structing Equilibrium Free Energies from Driven Adaptive-
Bias Simulations.
J Phys Chem Lett. 2013; 4(11):1882–1887.
https://doi.org/10.1021/jz400816x.
[363] Zheng L, Chen M, Yang W.
Random walk in orthogonal
space to achieve eﬃcient free-energy simulation of complex
systems.
Proc Natl Acad Sci. 2008; 105(51):20227–20232.
https://doi.org/10.1073/pnas.0810631106.
[364] Zheng L, Chen M, Yang W. Simultaneous escaping of explicit
and hidden free energy barriers: Application of the orthogonal
space random walk strategy in generalized ensemble based
conformational sampling. J Chem Phys. 2009; 130(23):234105.
https://doi.org/10.1063/1.3153841.
[365] Min D, Zheng L, Harris W, Chen M, Lv C, Yang W.
Prac-
tically
Eﬃcient
QM/MM
Alchemical
Free
Energy
Sim-
ulations:
The
Orthogonal
Space
Random
Walk
Strat-
egy.
J
Chem
Theory
Comput.
2010;
6(8):2253–2266.
https://doi.org/10.1021/ct100033s.
[366] Babin V, Roland C, Darden TA, Sagui C.
The free energy
landscape of small peptides as obtained from metadynam-
ics with umbrella sampling corrections. J Chem Phys. 2006;
125(20):204909. https://doi.org/10.1063/1.2393236.
[367] Autieri E, Sega M, Pederiva F, Guella G.
Puckering free
energy of pyranoses: A NMR and metadynamics-umbrella
sampling investigation.
J Chem Phys. 2010; 133(9):095104.
https://doi.org/10.1063/1.3476466.
[368] Zhang Y, Voth GA. Combined Metadynamics and Umbrella
Sampling Method for the Calculation of Ion Permeation Free
Energy Proﬁles. J Chem Theory Comput. 2011; 7(7):2277–2283.
https://doi.org/10.1021/ct200100e.
[369] Johnston JM, Wang H, Provasi D, Filizola M.
Assess-
ing the Relative Stability of Dimer Interfaces in G Protein-
Coupled Receptors. PLoS Comput Biol. 2012; 8(8):e1002649.
https://doi.org/10.1371/journal.pcbi.1002649.
[370] Awasthi S, Kapil V, Nair NN.
Sampling free energy
surfaces as slices by combining umbrella sampling and
metadynamics.
J Comput Chem. 2016; 37(16):1413–1424.
https://doi.org/10.1002/jcc.24349.
[371] Awasthi
S,
Nair
NN.
Exploring
high
dimensional
free
energy
landscapes:
Temperature
accelerated
sliced
sampling.
J
Chem
Phys.
2017;
146(9):094108.
https://doi.org/10.1063/1.4977704.
[372] Pal A, Pal S, Verma S, Shiga M, Nair NN. Mean force based tem-
perature accelerated sliced sampling: Eﬃcient reconstruction
of high dimensional free energy landscapes. J Comput Chem.
2021; https://doi.org/10.1002/jcc.26727.
[373] Kapakayala AB, Nair NN. Boosting the conformational sam-
pling by combining replica exchange with solute temper-
ing and well-sliced metadynamics.
J Comput Chem. 2021;
https://doi.org/10.1002/jcc.26752.
58 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[374] Hsu WT, Merz PT, Bussi G, Shirts MR.
Adding alchem-
ical variables to metadynamics to enhance sampling in
free energy calculations.
arXiv. 2022;
p. 2206.01329.
https://doi.org/10.48550/arXiv.2206.01329.
[375] Bonomi
M,
Heller
GT,
Camilloni
C,
Vendruscolo
M.
Principles
of
protein
structural
ensemble
determi-
nation.
Curr
Opin
Struct
Biol.
2017;
42:106–116.
https://doi.org/10.1016/j.sbi.2016.12.004.
[376] Cesari
A,
Reißer
S,
Bussi
G.
Using
the
Maximum
Entropy
Principle
to
Combine
Simulations
and
So-
lution
Experiments.
Computation.
2018;
6(1):15.
https://doi.org/10.3390/computation6010015.
[377] Bottaro S, Lindorff-Larsen K.
Biophysical experiments and
biomolecular simulations: A perfect match?
Science. 2018;
361(6400):355–360. https://doi.org/10.1126/science.aat4010.
[378] Pitera JW, Chodera JD. On the Use of Experimental Observa-
tions to Bias Simulated Ensembles. J Chem Theory Comput.
2012; 8(10):3445–3451. https://doi.org/10.1021/ct300112v.
[379] Roux B, Weare J.
On the statistical equivalence of
restrained-ensemble
simulations
with
the
maximum
entropy method.
J Chem Phys. 2013;
138(8):084107.
https://doi.org/10.1063/1.4792208.
[380] Bonomi M, Camilloni C, Vendruscolo M.
Metadynamic
metainference:
Enhanced sampling of the metainference
ensemble using metadynamics.
Sci Rep. 2016; 6(1):31232.
https://doi.org/10.1038/srep31232.
[381] Bonomi
M,
Camilloni
C,
Cavalli
A,
Vendruscolo
M.
Metainference:
A
Bayesian
inference
method
for
het-
erogeneous
systems.
Sci
Adv.
2016;
2(1):e1501177.
https://doi.org/10.1126/sciadv.1501177.
[382] Amirkulova DB, Chakraborty M, White AD.
Experimen-
tally Consistent Simulation of Aβ21–30 Peptides with a Min-
imal NMR Bias.
J Phys Chem B. 2020; 124(38):8266–8277.
https://doi.org/10.1021/acs.jpcb.0c07129.
[383] White AD, Voth GA.
Eﬃcient and Minimal Method
to
Bias
Molecular
Simulations
with
Experimental
Data.
J Chem Theory Comput. 2014;
10(8):3023–3030.
https://doi.org/10.1021/ct500320c.
[384] Camilloni C, Cavalli A, Vendruscolo M. Replica-Averaged Meta-
dynamics.
J Chem Theory Comput. 2013; 9(12):5610–5617.
https://doi.org/10.1021/ct4006272.
[385] Camilloni C, Vendruscolo M.
Statistical Mechanics of
the Denatured State of a Protein Using Replica-Averaged
Metadynamics.
J Am Chem Soc. 2014; 136(25):8982–8991.
https://doi.org/10.1021/ja5027584.
[386] Chipot C, Hénin J.
Exploring the free energy landscape of
a short peptide using an average force. J Chem Phys. 2005;
123:244906. https://doi.org/10.1063/1.2138694.
[387] Fu H, Zhang H, Chen H, Shao X, Chipot C, Cai W.
Zoom-
ing across the Free-Energy Landscape: Shaving Barriers, and
Flooding Valleys.
J Phys Chem Lett. 2018; 9(16):4738–4745.
https://doi.org/10.1021/acs.jpclett.8b01994.
[388] Fu H, Chen H, Wang X, Chai H, Shao X, Cai W, Chipot
C.
Finding
an
Optimal
Pathway
on
a
Multidimen-
sional Free-Energy Landscape.
J Chem Inf Model. 2020;
https://doi.org/10.1021/acs.jcim.0c00279.
[389] Fu
H,
Shao
X,
Cai
W,
Chipot
C.
Taming
Rugged
Free
Energy
Landscapes
Using
an
Average
Force.
Acc
Chem
Res.
2019;
52(11):3254–3264.
https://doi.org/10.1021/acs.accounts.9b00473.
[390] Chen H, Fu H, Chipot C, Shao X, Cai W.
Overcoming
Free-Energy
Barriers
with
a
Seamless
Combination
of
a
Biasing
Force
and
a
Collective
Variable-Independent
Boost
Potential.
J
Chem
Theory
Comput.
2021;
https://doi.org/10.1021/acs.jctc.1c00103.
[391] Brooks BR, Brooks III CL, Mackerell Jr AD, Nilsson L, Petrella
RJ, Roux B, Won Y, Archontis G, Bartels C, Boresch S, Caﬂisch A,
Caves L, Cui Q, Dinner AR, Feig M, Fischer S, Gao J, Hodoscek
M, Im W, Kuczera K, et al. CHARMM: The biomolecular sim-
ulation program.
J Comput Chem. 2009; 30(10):1545–1614.
https://doi.org/10.1002/jcc.21287.
[392] Case DA, Aktulga HM, Belfon K, Ben-Shalom IY, Brozell SR,
Cerutti DS, Cheatham TE, III, Cruzeiro VWD, Darden TA, Duke
RE, Giambasu G, Gilson MK, Gohlke H, Goetz AW, Harris R, Izadi
S, Izmailov SA, Jin C, Kasavajhala K, et al., Amber 2021; 2021.
[393] Kühne TD, Iannuzzi M, Ben MD, Rybkin VV, Seewald P, Stein
F, Laino T, Khaliullin RZ, Schütt O, Schiffmann F, Golze D, Wil-
helm J, Chulkov S, Bani-Hashemian MH, Weber V, Borštnik
U, Taillefumier M, Jakobovits AS, Lazzaro A, Pabst H, et al.
CP2K: An electronic structure and molecular dynamics soft-
ware package - Quickstep: Eﬃcient and accurate electronic
structure calculations.
J Chem Phys. 2020; 152(19):194103.
https://doi.org/10.1063/5.0007045.
[394] Bowers KJ, Chow E, Xu H, Dror RO, Eastwood MP, Gregersen
BA, Klepeis JL, Kolossvary I, Moraes MA, Sacerdoti FD,
Salmon JK, Shan Y, Shaw DE.
Scalable Algorithms for
Molecular Dynamics Simulations on Commodity Clusters.
ACM/IEEE SC 2006 Conference (SC’06). 2006;
p. 43–43.
https://doi.org/10.1109/sc.2006.54.
[395] Thompson AP, Aktulga HM, Berger R, Bolintineanu DS,
Brown WM, Crozier PS, Veld PJit, Kohlmeyer A, Moore SG,
Nguyen TD, Shan R, Stevens MJ, Tranchida J, Trott C, Plimp-
ton SJ.
LAMMPS - a ﬂexible simulation tool for particle-
based materials modeling at the atomic, meso, and con-
tinuum scales.
Comput Phys Commun. 2022; 271:108171.
https://doi.org/10.1016/j.cpc.2021.108171.
[396] Anderson
JA,
Glaser
J,
Glotzer
SC.
HOOMD-blue:
A
Python
package
for
high-performance
molecu-
lar
dynamics
and
hard
particle
Monte
Carlo
sim-
ulations.
Comput
Mater
Sci.
2020;
173:109363.
https://doi.org/10.1016/j.commatsci.2019.109363.
[397] Célerse F, Lagardère L, Derat E, Piquemal JP. Massively Parallel
Implementation of Steered Molecular Dynamics in Tinker-HP:
Comparisons of Polarizable and Non-Polarizable Simulations
of Realistic Systems. J Chem Theory Comput. 2019; 15(6):3694–
3709. https://doi.org/10.1021/acs.jctc.9b00199.
59 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

[398] Célerse F, Jaffrelot-Inizan T, Lagardère L, Adjoua O, Mon-
marché P, Miao Y, Derat E, Piquemal JP. An Eﬃcient GaMD
Multi-Level Enhanced Sampling Strategy: Application to Po-
larizable Force Fields Simulations of Large Biological Sys-
tems.
ChemRxiv. 2021; https://doi.org/10.33774/chemrxiv-
2021-ggjfx-v2.
[399] Schmid N, Christ CD, Christen M, Eichenberger AP, Gun-
steren WFv.
Architecture, implementation and paralleli-
sation of the GROMOS software for biomolecular simu-
lation.
Comput Phys Commun. 2012;
183(4):890–903.
https://doi.org/10.1016/j.cpc.2011.12.014.
[400] Kobayashi C, Jung J, Matsunaga Y, Mori T, Ando T, Tamura K,
Kamiya M, Sugita Y. GENESIS 1.1: A hybrid-parallel molecu-
lar dynamics simulator with enhanced sampling algorithms
on multiple computational platforms.
J Comp Chem. 2017;
38(25):2193–2206. https://doi.org/10.1002/jcc.24874.
[401] Huang Y, Xia Y, Yang L, Wei J, Yang YI, Gao YQ. SPONGE: A GPU-
Accelerated Molecular Dynamics Package with Enhanced Sam-
pling and AI-Driven Algorithms. Chin J Chem. 2022; 40(1):160–
168. https://doi.org/10.1002/cjoc.202100456.
[402] Russo JD, Zhang S, Leung JMG, Bogetti AT, Thompson JP, De-
Grave AJ, Torrillo PA, Pratt AJ, Wong KF, Xia J, Copperman J,
Adelman JL, Zwier MC, LeBard DN, Zuckerman DM, Chong
LT. WESTPA 2.0: High-Performance Upgrades for Weighted
Ensemble Simulations and Analysis of Longer-Timescale Ap-
plications.
J Chem Theory Comput. 2022; 18(2):638–649.
https://doi.org/10.1021/acs.jctc.1c01154.
[403] Lotz SD, Donyapour N, Dickson A, Dixon T, Roussey N, Hall
R, ADicksonLab/wepy: 1.0.0 Major version release. Zenodo;
2020. https://doi.org/10.5281/zenodo.4270219.
60 of 60
https://doi.org/10.33011/livecoms.4.1.1583
Living J. Comp. Mol. Sci. 2022, 4(1), 1583

