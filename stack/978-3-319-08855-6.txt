123
Anne Laurent
Olivier Strauss
Bernadette Bouchon-Meunier
Ronald R. Yager (Eds.)
15th International Conference, IPMU 2014
Montpellier, France, July 15-19, 2014
Proceedings, Part II
Information Processing
and Management
of Uncertainty in
Knowledge-Based Systems
Communications in Computer and Information Science 
443
Part 2

Communications
in Computer and Information Science
443
Editorial Board
Simone Diniz Junqueira Barbosa
Pontiﬁcal Catholic University of Rio de Janeiro (PUC-Rio),
Rio de Janeiro, Brazil
Phoebe Chen
La Trobe University, Melbourne, Australia
Alfredo Cuzzocrea
ICAR-CNR and University of Calabria, Italy
Xiaoyong Du
Renmin University of China, Beijing, China
Joaquim Filipe
Polytechnic Institute of Setúbal, Portugal
Orhun Kara
TÜB˙ITAK B˙ILGEM and Middle East Technical University, Turkey
Igor Kotenko
St. Petersburg Institute for Informatics and Automation
of the Russian Academy of Sciences, Russia
Krishna M. Sivalingam
Indian Institute of Technology Madras, India
Dominik ´Sle˛zak
University of Warsaw and Infobright, Poland
Takashi Washio
Osaka University, Japan
Xiaokang Yang
Shanghai Jiao Tong University, China

Anne Laurent Olivier Strauss
Bernadette Bouchon-Meunier
Ronald R. Yager (Eds.)
Information Processing
and Management
of Uncertainty in
Knowledge-Based Systems
15th International Conference, IPMU 2014
Montpellier, France, July 15-19, 2014
Proceedings, Part II
1 3

Volume Editors
Anne Laurent
Université Montpellier 2, LIRMM, France
E-mail: laurent@lirmm.fr
Olivier Strauss
Université Montpellier 2, LIRMM, France
E-mail: olivier.strauss@lirmm.fr
Bernadette Bouchon-Meunier
Sorbonne Universités, UPMC Paris 6, France
E-mail: bernadette.bouchon-meunier@lip6.fr
Ronald R. Yager
Iona College, New Rochelle, NY, USA
E-mail: ryager@iona.edu
ISSN 1865-0929
e-ISSN 1865-0937
ISBN 978-3-319-08854-9
e-ISBN 978-3-319-08855-6
DOI 10.1007/978-3-319-08855-6
Springer Cham Heidelberg New York Dordrecht London
Library of Congress Control Number: 2014942459
© Springer International Publishing Switzerland 2014
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied speciﬁcally for the purpose of being entered and
executedonacomputersystem,forexclusiveusebythepurchaserofthework.Duplicationofthispublication
or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher’s location,
in ist current version, and permission for use must always be obtained from Springer. Permissions for use
may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution
under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of publication,
neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or
omissions that may be made. The publisher makes no warranty, express or implied, with respect to the
material contained herein.
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
Here we provide the proceedings of the 15th International Conference on In-
formation Processing and Management of Uncertainty in Knowledge-based Sys-
tems, IPMU 2014, held in Montpellier, France, during July 15–19, 2014. The
IPMU conference is organized every two years with the focus of bringing to-
gether scientists working on methods for the management of uncertainty and
aggregation of information in intelligent systems.
This conference provides a medium for the exchange of ideas between theo-
reticians and practitioners working on the latest developments in these and other
related areas. This was the 15th edition of the IPMU conference, which started
in 1986 and has been held every two years in the following locations in Eu-
rope: Paris (1986), Urbino (1988), Paris (1990), Palma de Mallorca (1992), Paris
(1994), Granada (1996), Paris (1998), Madrid (2000), Annecy (2002), Perugia
(2004), Malaga (2008), Dortmund (2010) and Catania (2012).
Among the plenary speakers at past IPMU conferences, there have been three
Nobel Prize winners: Kenneth Arrow, Daniel Kahneman, and Ilya Prigogine. An
important feature of the IPMU Conference is the presentation of the Kamp´e de
F´eriet Award for outstanding contributions to the ﬁeld of uncertainty. This year,
the recipient was Vladimir N. Vapnik. Past winners of this prestigious award were
LotﬁA. Zadeh (1992), Ilya Prigogine (1994), Toshiro Terano (1996), Kenneth
Arrow (1998), Richard Jeﬀrey (2000), Arthur Dempster (2002), Janos Aczel
(2004), Daniel Kahneman (2006), Enric Trillas (2008), James Bezdek (2010),
Michio Sugeno (2012).
The program of the IPMU 2014 conference consisted of 5 invited academic
talks together with 180 contributed papers, authored by researchers from 46
countries, including the regular track and 19 special sessions. The invited aca-
demic talks were given by the following distinguished researchers: Vladimir
N. Vapnik (NEC Laboratories, USA), Stuart Russell (University of California,
Berkeley, USA and University Pierre et Marie Curie, Paris, France), In´es Couso
(University of Oviedo, Spain), Nadia Berthouze (University College London,
United Kingdom) and Marcin Detyniecki (University Pierre and Marie Curie,
Paris, France).
Industrial talks were given in complement of academic talks and highlighted
the necessary collaboration we all have to foster in order to deal with current
challenges from the real world such as Big Data for dealing with massive and
complex data.
The success of IPMU 2014 was due to the hard work and dedication of a
large number of people, and the collaboration of several institutions. We want to
acknowledge the industrial sponsors, the help of the members of the International
Program Committee, the reviewers of papers, the organizers of special sessions,
the Local Organizing Committee, and the volunteer students. Most of all, we

VI
Preface
appreciate the work and eﬀort of those who contributed papers to the conference.
All of them deserve many thanks for having helped to attain the goal of providing
a high quality conference in a pleasant environment.
May 2014
Bernadette Bouchon-Meunier
Anne Laurent
Olivier Strauss
Ronald R. Yager

Conference Committee
General Chairs
Anne Laurent
Universit´e Montpellier 2, France
Olivier Strauss
Universit´e Montpellier 2, France
Executive Directors
Bernadette Bouchon-Meunier
CNRS-UPMC, France
Ronald R. Yager
Iona College, USA
Web Chair
Yuan Lin
INRA, SupAgro, France
Proceedings Chair
J´erˆome Fortin
Universit´e Montpellier 2, France
International Advisory Board
Giulianella Coletti (Italy)
Miguel Delgado (Spain)
Mario Fedrizzi (Italy)
Laurent Foulloy (France)
Salvatore Greco (Italy)
Julio Gutierrez-Rios (Spain)
Eyke H¨ullermeier(Germany)
Luis Magdalena (Spain)
Christophe Marsala (France)
Benedetto Matarazzo (Italy)
Manuel Ojeda-Aciego (Spain)
Maria Rifqi (France)
Lorenza Saitta (Italy)
Enric Trillas (Spain)
Lloren¸c Valverde (Spain)
Jos´e Luis Verdegay (Spain)
Maria-Amparo Vila (Spain)
LotﬁA. Zadeh (USA)
Special Session Organizers
Jose M. Alonso
European Centre for Soft Computing, Spain
Michal Baczynski
University of Silesia, Poland
Edurne Barrenechea
Universidad P´ublica de Navarra, Spain
Zohra Bellahsene
University Montpellier 2, France
Patrice Buche
INRA, France
Thomas Burger
CEA, France
Tomasa Calvo
Universidad de Alcal´a, Spain

VIII
Conference Committee
Brigitte Charnomordic
INRA, France
Didier Coquin
University of Savoie, France
C´ecile Coulon-Leroy
´Ecole Sup´erieure d’Agriculture d’Angers,
France
S´ebastien Destercke
CNRS, Heudiasyc Lab., France
Susana Irene Diaz
Rodriguez
University of Oviedo, Spain
Luka Eciolaza
European Centre for Soft Computing, Spain
Francesc Esteva
IIIA - CSIC, Spain
Tommaso Flaminio
University of Insubria, Italy
Brunella Gerla
University of Insubria, Italy
Manuel Gonz´alez Hidalgo
University of the Balearic Islands, Spain
Michel Grabisch
University of Paris Sorbonne, France
Serge Guillaume
Irstea, France
Anne Laurent
University Montpellier 2, France
Christophe Labreuche
Thales Research and Technology, France
Kevin Loquin
University Montpellier 2, France
Nicol´as Mar´ın
University of Granada, Spain
Trevor Martin
University of Bristol, UK
Sebastia Massanet
University of the Balearic Islands, Spain
Juan Miguel Medina
University of Granada, Spain
Enrique Miranda
University of Oviedo, Spain
Javier Montero
Complutense University of Madrid, Spain
Jes´us Medina Moreno
University of C´adiz, Spain
Manuel Ojeda Aciego
University of M´alaga, Spain
Martin Pereira Farina
Universidad de Santiago de Compostela, Spain
Olga Pons
University of Granada, Spain
Dragan Radojevic
Serbia
Anca L. Ralescu
University of Cincinnati, USA
Fran¸cois Scharﬀe
University Montpellier 2, France
Rudolf Seising
European Centre for Soft Computing, Spain
Marco Elio Tabacchi
Universit`a degli Studi di Palermo, Italy
Tadanari Taniguchi
Tokai University, Japan
Charles Tijus
LUTIN-CHArt, Universit´e Paris 8, France
Konstantin Todorov
University Montpellier 2, France
Lionel Valet
University of Savoie, France
Program Committee
Michal Baczynski
Gleb Beliakov
Radim Belohlavek
Salem Benferhat
H. Berenji
Isabelle Bloch
Ulrich Bodenhofer
P. Bonissone
Bernadette
Bouchon-Meunier
Patrice Buche
Humberto Bustince
Rita Casadio
Yurilev Chalco-Cano
Brigitte Charnomordic
Guoqing Chen
Carlos A. Coello Coello
Giulianella Coletti

Conference Committee
IX
Oscar Cord´on
In´es Couso
Keeley Crockett
Bernard De Baets
S´ebastien Destercke
Marcin Detyniecki
Antonio Di Nola
Gerard Dray
Dimiter Driankov
Didier Dubois
Francesc Esteva
Mario Fedrizzi
Janos Fodor
David Fogel
J´erˆome Fortin
Laurent Foulloy
Sylvie Galichet
Patrick Gallinari
Maria Angeles Gil
Siegfried Gottwald
Salvatore Greco
Steve Grossberg
Serge Guillaume
Lawrence Hall
Francisco Herrera
Enrique Herrera-Viedma
Kaoru Hirota
Janusz Kacprzyk
A. Kandel
James M. Keller
E.P. Klement
Laszlo T. Koczy
Vladik Kreinovich
Christophe Labreuche
J´erˆome Lang
Henrik L. Larsen
Anne Laurent
Marie-Jeanne Lesot
Churn-Jung Liau
Yuan Lin
Honghai Liu
Weldon Lodwick
Kevin Loquin
Thomas Lukasiewicz
Luis Magdalena
Jean-Luc Marichal
Nicol´as Mar´ın
Christophe Marsala
Trevor Martin
Myl`ene Masson
Silvia Massruha
Gilles Mauris
Gaspar Mayor
Juan Miguel Medina
Jerry Mendel
Enrique Miranda
Pedro Miranda
Paul-Andr´e Monney
Franco Montagna
Javier Montero
Jacky Montmain
Seraf´ın Moral
Yusuke Nojima
Vilem Novak
Hannu Nurmi
S. Ovchinnikov
Nikhil R. Pal
Endre Pap
Simon Parsons
Gabriella Pasi
W. Pedrycz
Fred Petry
Vincenzo Piuri
Olivier Pivert
Henri Prade
Anca Ralescu
Dan Ralescu
Mohammed Ramdani
Marek Reformat
Agnes Rico
Maria Rifqi
Enrique H. Ruspini
Glen Shafer
J. Shawe-Taylor
P. Shenoy
P. Sobrevilla
Umberto Straccia
Olivier Strauss
M. Sugeno
Kay-Chen Tan
S. Termini
Konstantin Todorov
Vicenc Torra
I. Burhan Turksen
Barbara Vantaggi
Jos´e Luis Verdegay
Thomas Vetterlein
Marco Zaﬀalon
Hans-J¨urgen
Zimmermann
Jacek Zurada
Additional Members of the Reviewing Committee
Iris Ada
Jes´us Alcal´a-Fernandez
Cristina Alcalde
Jos´e Mar´ıa Alonso
Alberto Alvarez-Alvarez
Leila Amgoud
Massih-Reza Amini
Derek Anderson
Plamen Angelov
Violaine Antoine
Alessandro Antonucci
Alain Appriou
Arnaud Martin
Jamal Atif
Rosangela Ballini
Carlos Barranco
Edurne Barrenechea
Gleb Beliakov
Zohra Bellahsene
Alessio Benavoli
Eric Benoit

X
Conference Committee
Moulin Bernard
Christophe Billiet
Fernando Bobillo
Gloria Bordogna
Christian Borgelt
Stefan Borgwardt
Carole Bouchard
Antoon Bronselaer
Thomas Burger
Ana Burusco
Manuela Busaniche
Silvia Calegari
Tomasa Calvo
Jesus Campa˜na
Andr´es Cano
Philippe Capet
Arnaud Castelltort
La¨etitia Chapel
Yi-Ting Chiang
Francisco Chiclana
Laurence Cholvy
Davide Ciucci
Chlo´e Clavel
Ana Colubi
Arianna Consiglio
Didier Coquin
Pablo Cordero
Mar´ıa Eugenia Cornejo
Chris Cornelis
C´ecile Coulon-Leroy
Fabio Cozman
Madalina Croitoru
Jc Cubero
Fabio Cuzzolin
Inmaculada de Las Pe˜nas
Cabrera
Gert Decooman
Afef Denguir
Thierry Denœux
Lynn D’Eer
Guy De Tr´e
Irene Diaz Rodriguez
Juliette
Dibie-Barth´el´emy
Pawel Dryga´s
Jozo Dujmovic
Fabrizio Durante
Luka Eciolaza
Nicola Fanizzi
Christian Ferm¨uller
Javier Fernandez
Tommaso Flaminio
Carl Fr´elicot
Ram´on
Fuentes-Gonz´alez
Louis Gacogne
Jose Galindo
Jos´e Luis
Garc´ıa-Lapresta
Brunella Gerla
Maria Angeles Gil
Lluis Godo
Manuel Gonzalez
Michel Grabisch
Salvatore Greco
Przemyslaw
Grzegorzewski
Kevin Guelton
Thierry Marie Guerra
Robert Hable
Allel Hadjali
Pascal Held
Sascha Henzgen
Hykel Hosni
Celine Hudelot
Julien Hu´e
Eyke H¨ullermeier
Atsushi Inoue
Uzay Kaymak
Anna Kolesarova
Jan Konecny
Stanislav Krajci
Ondrej Kridlo
Rudolf Kruse
Naoyuki Kubota
Maria Teresa Lamata
Cosmin Lazar
Florence Le Ber
Fabien Lehuede
Ludovic Lietard
Berrahou Lilia
Chin-Teng Lin
Xinwang Liu
Nicolas Madrid
Ricardo A. Marques
Pereira
Luis Martinez
Carmen Martinez-Cruz
Sebasti`a Massanet
Brice Mayag
Gaspar Mayor
Jes´us Medina Moreno
Corrado Mencar
David Mercier
Radko Mesiar
Andrea Mesiarova
Arnau Mir
Franco Montagna
Susana Montes
Tommaso Moraschini
Mar´ıa Moreno Garc´ıa
Olivier Naud
Manuel Ojeda-Aciego
Daniel Paternain
Andrea Pedrini
Carmen Pel´aez
Mart´ın Pereira-Farina
David Perez
Irina Perﬁlieva
Nathalie Perrot
Valerio Perticone
Davide Petturiti
Pascal Poncelet
Olga Pons
Thierry Pun
Erik Quaeghebeur
Benjamin Quost
Dragan Radojevic
Eloisa Ram´ırez
Jordi Recasens
Juan Vicente Riera
Antoine Rolland
Daniel Ruiz
M. Dolores Ruiz
Nobusumi Sagara

Conference Committee
XI
Daniel Sanchez
Sandra Sandri
Francois Scharﬀe
Johan Schubert
Rudolf Seising
Carlo Sempi
Robin Senge
Ammar Shaker
Prakash Shenoy
Beatriz Sinova
Damjan Skulj
Dominik Slezak
Roman Slowinski
Alejandro Sobrino
H´el`ene Soubaras
Jaume Suner
Nicolas Sutton-Charani
Ronan Symoneaux
Eulalia Szmidt
Marcoelio Tabacchi
Eiichiro Takahagi
Nouredine Tamani
Tadanari Taniguchi
Elizabeth Tapia
Joao Manuel R.S.
Tavares
Maguelonne Teisseire
Matthias Thimm
Rallou Thomopoulos
Charles Tijus
Joan Torrens
Carmen Torres
Gwladys Toulemonde
Nicolas Tremblay
Graci´an Trivino
Matthias Troﬀaes
Luigi Troiano
Wolfgang Trutschnig
Esko Turunen
Lionel Valet
Francisco Valverde
Arthur Vancamp
Barbara Vantaggi
Jirina Vejnarova
Paolo Vicig
Amparo Vila
Pablo Villacorta
Serena Villata
Sof´ıa Vis
Paul Weng
Dongrui Wu
Slawomir Zadrozny

Table of Contents – Part II
Fuzzy Logic in Boolean Framework
Supplier Selection Using Interpolative Boolean Algebra and Logic
Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Ksenija Mandic and Boris Delibasic
Finitely Additive Probability Measures in Automated Medical
Diagnostics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
Milica Kneˇzevi´c, Zoran Ognjanovi´c, and Aleksandar Perovi´c
Demonstrative Implecations of a New Logical Aggregation Paradigm
on a Classical Fuzzy Evalution Modal of «Green» Buildings . . . . . . . . . . .
20
Milan Mrkalj
Structural Functionality as a Fundamental Property of Boolean Algebra
and Base for Its Real-Valued Realizations . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
Dragan G. Radojevi´c
Management of Uncertainty in Social Networks
Fuzzy Concepts in Small Worlds and the Identiﬁcation of Leaders in
Social Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
Trinidad Casas´us-Estell´es and Ronald R. Yager
Generating Events for Dynamic Social Network Simulations . . . . . . . . . . .
46
Pascal Held, Alexander Dockhorn, and Rudolf Kruse
A Model for Preserving Privacy in Recommendation Systems . . . . . . . . . .
56
Luigi Troiano and Irene D´ıaz
Classiﬁcation of Message Spreading in a Heterogeneous Social
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
Siwar Jendoubi, Arnaud Martin, Ludovic Li´etard, and
Boutheina Ben Yaghlane
Measures of Semantic Similarity of Nodes in a Social Network . . . . . . . . .
76
Ahmad Rawashdeh, Mohammad Rawashdeh, Irene D´ıaz, and
Anca Ralescu
From Diﬀerent to Same, from Imitation to Analogy
Imitation and the Generative Mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
Jacqueline Nadel

XIV
Table of Contents – Part II
Conditions for Cognitive Plausibility of Computational Models of
Category Induction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Daniel Devatman Hromada
3D-Posture Recognition Using Joint Angle Representation . . . . . . . . . . . .
106
Adnan Al Alwani, Youssef Chahir, Djamal E. Goumidi,
Mich`ele Molina, and Fran¸cois Jouen
Gesture Trajectories Modeling Using Quasipseudometrics and
Pre-topology for Its Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
Marc Bui, Souﬁan Ben Amor, Michel Lamure, and Cynthia Basileu
Analogy and Metaphors in Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
Charles Candau, Geoﬀrey Ventalon, Javier Barcenilla, and
Charles Tijus
Soft Computing and Sensory Analysis
Fuzzy Transform Theory in the View of Image Registration
Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
Petr Hurt´ık, Irina Perﬁlieva, and Petra Hod´akov´a
Improved F-transform Based Image Fusion . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Marek Vajgl and Irina Perﬁlieva
Visual Taxometric Approach to Image Segmentation Using
Fuzzy-Spatial Taxon Cut Yields Contextually Relevant Regions . . . . . . . .
163
Lauren Barghout
Multi-valued Fuzzy Spaces for Color Representation . . . . . . . . . . . . . . . . . .
174
Vasile Patrascu
A New Edge Detector Based on Uninorms . . . . . . . . . . . . . . . . . . . . . . . . . .
184
Manuel Gonz´alez-Hidalgo, Sebastia Massanet, Arnau Mir, and
Daniel Ruiz-Aguilera
Database Systems
Context-Aware Distance Semantics for Inconsistent Database
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
Anna Zamansky, Ofer Arieli, and Kostas Stefanidis
An Analysis of the SUDOC Bibliographic Knowledge Base from a Link
Validity Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
L´ea Guizol, Olivier Rousseaux, Madalina Croitoru,
Yann Nicolas, and Aline Le Provost
A Fuzzy Extension of Data Exchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
Jes´us Medina and Reinhard Pichler

Table of Contents – Part II
XV
Fuzzy Set Theory
Fuzzy Relational Compositions Based on Generalized Quantiﬁers. . . . . . .
224
Martin ˇStˇepniˇcka and Michal Holˇcapek
A Functional Approach to Cardinality of Finite Fuzzy Sets . . . . . . . . . . . .
234
Michal Holˇcapek
Piecewise Linear Approximation of Fuzzy Numbers Preserving the
Support and Core . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244
Lucian Coroianu, Marek Gagolewski, Przemyslaw Grzegorzewski,
M. Adabitabar Firozja, and Tahereh Houlari
Characterization of the Ranking Indices of Triangular Fuzzy
Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
254
Adrian I. Ban and Lucian Coroianu
New Pareto Approach for Ranking Triangular Fuzzy Numbers . . . . . . . . .
264
Oumayma Bahri, Nahla Ben Amor, and Talbi El-Ghazali
MI-groups: New Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Martin Bacovsk´y
Measurement and Sensory Information
On Combining Regression Analysis and Constraint Programming . . . . . .
284
Carmen Gervet and Sylvie Galichet
Graph-Based Transfer Learning for Managing Brain Signals Variability
in NIRS-Based BCIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
294
Sami Dalhoumi, G´erard Derosiere, G´erard Dray,
Jacky Montmain, and St´ephane Perrey
Design of a Fuzzy Aﬀective Agent Based on Typicality Degrees of
Physiological Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
304
Joseph Onderi Orero and Maria Rifqi
Aggregation
Multilevel Aggregation of Arguments in a Model Driven Approach to
Assess an Argumentation Relevance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
314
Olivier Poitou and Claire Saurel
Analogical Proportions and Square of Oppositions. . . . . . . . . . . . . . . . . . . .
324
Laurent Miclet and Henri Prade
Towards a Transparent Deliberation Protocol Inspired from Supply
Chain Collaborative Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
Florence Bannay and Romain Guillaume

XVI
Table of Contents – Part II
Encoding Argument Graphs in Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
Philippe Besnard, Sylvie Doutre, and Andreas Herzig
Formal Methods for Vagueness and Uncertainty
in a Many-Valued Realm
On General Properties of Intermediate Quantiﬁers . . . . . . . . . . . . . . . . . . .
355
Vil´em Nov´ak and Petra Murinov´a
A Note on Drastic Product Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
365
Stefano Aguzzoli, Matteo Bianchi, and Diego Valota
Fuzzy State Machine-Based Refurbishment Protocol for Urban
Residential Buildings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
Gergely I. Moln´arka and L´aszl´o T. K´oczy
Exploring Inﬁnitesimal Events through MV-algebras and non-
Archimedean States . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
385
Denisa Diaconescu, Anna Rita Ferraioli, Tommaso Flaminio, and
Brunella Gerla
Graduality
Accelerating Eﬀect of Attribute Variations: Accelerated Gradual
Itemsets Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
395
Amal Oudni, Marie-Jeanne Lesot, and Maria Rifqi
Gradual Linguistic Summaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
405
Anna Wilbik and Uzay Kaymak
Mining Epidemiological Dengue Fever Data from Brazil: A Gradual
Pattern Based Geographical Information System . . . . . . . . . . . . . . . . . . . . .
414
Yogi Satrya Aryadinata, Yuan Lin, C. Barcellos, Anne Laurent, and
Therese Libourel
Preferences
A New Model of Eﬃciency-Oriented Group Decision and Consensus
Reaching Support in a Fuzzy Environment . . . . . . . . . . . . . . . . . . . . . . . . . .
424
Dominika Golu´nska, Janusz Kacprzyk, and Slawomir Zadro˙zny
Aggregation of Uncertain Qualitative Preferences for a Group
of Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
434
Paolo Viappiani
Choquet Expected Utility Representation of Preferences on Generalized
Lotteries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
444
Giulianella Coletti, Davide Petturiti, and Barbara Vantaggi

Table of Contents – Part II
XVII
Utility-Based Approach to Represent Agents’ Conversational
Preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
454
Kaouther Bouzouita, Wided Lejouad Chaari, and Moncef Tagina
Alternative Decomposition Techniques for Label Ranking . . . . . . . . . . . . .
464
Massimo Gurrieri, Philippe Fortemps, and Xavier Siebert
Uncertainty Management in Machine Learning
Clustering Based on a Mixture of Fuzzy Models Approach . . . . . . . . . . . .
475
Miguel Pagola, Edurne Barrenechea, Ar´anzazu Jur´ıo,
Daniel Paternain, and Humberto Bustince
Analogical Classiﬁcation: A Rule-Based View . . . . . . . . . . . . . . . . . . . . . . . .
485
Myriam Bounhas, Henri Prade, and Gilles Richard
Multilabel Prediction with Probability Sets: The Hamming Loss
Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
496
S´ebastien Destercke
Cooperative Multi-knowledge Learning Control System for Obstacle
Consideration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
506
Syaﬁq Fauzi Kamarulzaman and Seiji Yasunobu
Building Hybrid Fuzzy Classiﬁer Trees by Additive/Subtractive
Composition of Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
516
Arne-Jens Hempel, Holger H¨ahnel, and Gernot Herbst
Philosophy and History of Soft Computing
Applying CHC Models to Reasoning in Fictions. . . . . . . . . . . . . . . . . . . . . .
526
Luis A. Urtubey and Alba Massolo
Probabilistic Solution of Zadeh’s Test Problems . . . . . . . . . . . . . . . . . . . . . .
536
Boris Kovalerchuk
Some Reﬂections on Fuzzy Set Theory as an Experimental Science . . . . .
546
Marco Elio Tabacchi and Settimo Termini
Fuzziness and Fuzzy Concepts and Jean Piaget’s Genetic
Epistemology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
556
Rudolf Seising
Paired Structures in Logical and Semiotic Models of Natural
Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
566
J. Tinguaro Rodr´ıguez, Camilo Franco De Los R´ıos,
Javier Montero, and Jie Lu

XVIII
Table of Contents – Part II
Soft Computing and Sensory Analysis
A Fuzzy Rule-Based Haptic Perception Model for Automotive
Vibrotactile Display . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
576
Liviu-Cristian Du¸tu, Gilles Mauris, Philippe Bolon,
St´ephanie Dabic, and Jean-Marc Tissot
A Linguistic Approach to Multi-criteria and Multi-expert Sensory
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
586
Jos´e Luis Garc´ıa-Lapresta, Cristina Aldavero, and
Santiago de Castro
Using Fuzzy Logic to Enhance Classiﬁcation of Human Motion
Primitives. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
596
Barbara Bruno, Fulvio Mastrogiovanni, Alessandro Saﬃotti, and
Antonio Sgorbissa
Optimization of Human Perception on Virtual Garments by Modeling
the Relation between Fabric Properties and Sensory Descriptors Using
Intelligent Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
606
Xiaon Chen, Xianyi Zeng, Ludovic Koehl, Xuyuan Tao, and
Julie Boulenguez-Phippen
Customization of Products Assisted by Kansei Engineering, Sensory
Analysis and Soft Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
616
Jose M. Alonso, David P. Pancho, and Luis Magdalena
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
627

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 1–9, 2014. 
© Springer International Publishing Switzerland 2014  
Supplier Selection Using Interpolative Boolean Algebra 
and Logic Aggregation 
Ksenija Mandic* and Boris Delibasic 
University of Belgrade, Faculty of Organizational Sciences, Jove Ilica 154, Belgrade, Serbia 
ksenija.mandic@crony.rs, boris.delibasic@fon.bg.ac.rs 
Abstract. The interest of the decision makers in the selection process of suppli-
ers is constantly growing as a reliable supplier reduces costs and improves the 
quality of products/services. This process is essentially reducible to the problem 
of multi-attribute decision-making. Namely, the large number of quantitative 
and qualitative attributes is considered. This paper presents a model of supplier 
selection. Weighted approach for solving this model was used combined with 
logical interactions between attributes. Setting logical conditions between 
attributes was carried out by using the Boolean Interpolative Algebra. Then the 
logical conditions are transformed into generalized Boolean polynomial that is 
through logical aggregation translated into a single value. In this way, the rank-
ing of the suppliers is provided. Using this model managers will be able to 
clearly express their demands through logical conditions, i.e. will be able to 
conduct a comprehensive analysis of the problem and to make an informed  
decision. 
Keywords: Fuzzy logic, Interpolative Boolean algebra, Generalized Boolean 
polynomial, Logical aggregation, Supplier selection problem. 
1 
Introduction 
Selection of the most favorable supplier is a strategic decision that ensures profitabili-
ty and long-term survival of the company. The company's goal is to carefully choose 
the right suppliers who will provide the requested product at the right time. In most 
cases, the strengths and weaknesses of suppliers vary over time, so that managers are 
in a position to have to make complex decisions in the selection. 
In real situations, managers often want to set up mutual relationships between the 
attributes in order to bring the best possible decision. As conventional fuzzy methods 
of multi-attribute decision-making do not allow setting of logical interactions between 
attributes, i.e. they are not in the Boolean frame, the consistent fuzzy logic is intro-
duced. The basis of this approach is interpolative realization of Boolean algebra that 
transforms logical conditions between attributes into a generalized Boolean poly-
nomial, then the set logical conditions merge into a single value by using the logic 
                                                           
* Corresponding author. 

2 
K. Mandic and B. Delibasic 
 
aggregation. In this way, a suitable tool is developed for mapping linguistic require-
ments of decision-makers in the appropriate Boolean polynomial. 
The paper is organized as follows: Section 2 provides an introduction to Boolean 
consistent fuzzy logic and transformation of logic functions in generalized Boolean 
polynomial and applying Boolean aggregation. Section 3 analyzes the problem of 
selecting suppliers and using Boolean consistent fuzzy logic is presented. The paper 
concludes with Section 4 where the conclusive considerations are presented. 
2 
Boolean Consistent Fuzzy Logic 
Classical Boolean algebra [1] is based on the statements that are true/false, yes/no, 
white/black. However, there are situations in which classical two-valued realization of 
Boolean algebra is not adequate. Often it is impossible to express in the absolutely 
precise way, but we are forced to use vague constellations. In this regard, the necessi-
ty of gradation in relations is recognized, so that fuzzy logic is introduced [2], which 
in its implementation uses the principle of Many-valued logic [3]. The main advan-
tage of fuzzy logic is that it is very close to human perception and does not require 
completely exact data. Indicating that it is not precisely defined by an element belong-
ing to a certain set, but elements can take values from the interval [0,1]. However, the 
main disadvantage of fuzzy logic is that it is not in the Boolean frame. 
Extension of fuzzy logic by introducing logical interactions is enabled by using In-
terpolative Boolean Algebra - IBA [4,5], which is a consistent generalization of fuzzy 
logic. IBA is a real valued, and/or, [0,1] value realization of Boolean algebra 
[6]. Under the IBA all Boolean axioms and theorems apply. 
IBA consists of two levels: a) symbolic or qualitative - at this level the elements 
structure is defined, and is the matter of final Boolean framework, b) semantic or 
value - at this level the values are introduced in this way to preserve all the laws set 
symbolically, in the general case it is a matter of interpolation [7,8]. 
In fact, the IBA represents an atomic algebra (as a result of a finite number of ele-
ments) and is substantially a different gradation approach in comparison to fuzzy 
approach. Atoms as the simplest elements of algebra play a fundamental role. One of 
the basic concepts of symbolic levels is the structure of IBA elements. The structure 
of any IBA element and/or the principle of structural functionality [9] is a bridge be-
tween these two levels and basis of generalization, as long as the elements are value-
independent. The structure of the analyzed elements determines which atom is (of the 
final set of elements IBA) included and/or not included. The principle of structural 
functionality indicates that the structure of any element of IBA may be directly calcu-
lated based on the structure of its components. The structure is an independent value 
and that is the key to preserving Boolean laws both at the symbolic and at the level of 
values [10]. This principle requires that the IBA transformations are performed at the 
symbolic level before the introduction of value. Indicating that the negation is treated 
in a different way, at a structural level, rather than negated variable immediately 
transforms into value. Thus the observation of negation allows preservation of all 
Boolean laws. Also, within the IBA applies the law of excluded middle, the axiom of 

 
Supplier Selection Using Interpolative Boolean Algebra and Logic Aggregation 
3 
 
Boolean logic, where ܽש ൓ܽൌ1, which is not respected in the conventional fuzzy 
logic [11]. Based on all the foregoing, we conclude that fuzzy logic is not in the Boo-
lean frame. 
2.1 
Generalized Boolean Polynomial and Logical Aggregation 
IBA is an algebraic structure with elements which is represented by the Eq. (1) [12]: 
                                                               ۃܤܣ,ר,ש, ൓ۄ                                                                  ሺ1ሻ 
where ܤܣ is the set of finite elements, binary operators of conjunction ר and disjunc-
tion ש and unary negation operator ൓, for which all Boolean axioms and theorems are 
valid. 
Under the IBA every Boolean function can be uniquely transformed into the cor-
responding generalized Boolean polynomial (GBP) [8]. Technically, if any element of 
Boolean algebra can be represented in a canonical disjunction way, it can be 
represented also by appropriate GBP. And thus, it allows for the processing of the 
corresponding element of Boolean algebra into the value on the real interval [0, 1] 
using operators such as classical (+), classical (-) and generalized product 
(ٔ) [13]. Generalized product (GP) can be any function (ٔ): ሾ0,1ሿ ൈ ሾ0,1ሿ ՜
 ሾ0,1ሿ which meets all the requirements that one function be a t-norm (commutativity, 
associativity, monotonicity and limitation Eq. (2,3,4,5)), as well as additional non 
negativity condition, which is defined as Eq. (6) [7]: 
1.  
ܣ௜ ٔ ܣ௝ൌܣ௝ ۪ ܣ௜                                                                                                 ሺ2ሻ 
2.            ܣ௜ ٔ ൫ܣ௝ ٔ ܣ௞൯ൌ൫ܣ௜ ٔ ܣ௝൯ٔ ܣ௞                                                               ሺ3ሻ 
3.  
ܣ௜൑ ܣ௝ ฺ ܣ௜ ٔ ܣ௞ ൑ ܣ௝ ٔ ܣ௞                                                                 ሺ4ሻ 
4.  
ܣ௜ ٔ 1 ൌ ܣ௝                                                                                                             ሺ5ሻ 
5.  
∑
ሺെ1ሻ|௄|
௄א௉ሺΩௌ
⁄ ሻ
ٔ ܣ௜ ሺݔሻ൒0, ܵא ܲሺΩሻ,  ܣ௜ ሺݔሻא ሾ0,1ሿ,  ܣ௜ א Ω        (6) 
Within the IBA, the method enabling unification of factors is referred to as Logical 
Aggregation (LA). The main task of LA is a merger of the primary attributes Ωൌ
 ሼܽଵ, … , ܽ௡ሽ into a single value, which represents a given set, by using the logi-
cal/pseudological function. If we consider the problem of multi-attribute decision 
making, which is the subject of this paper, LA can be realized in two steps [14]: 
1. The normalization of attributes values, which is represented by the Eq. (7): 
                                                     ห|·|ห: Ω ՜ ሾ0,1ሿ                                                                 ሺ7ሻ 
2. Aggregation of normalized attributes values into one, by using a logical aggrega-
tion or pseudological functions as LA operator, defined by Eq. (8) [15]: 
                                              ܣ݃݃ݎ ሾ0,1ሿ ௡՜ ሾ0,1ሿ                                                             ሺ8ሻ 
Boolean function enables the aggregation of factors, i.e. it is an expression that 
transforms into GBP. Pseudological function is a convex combination of GBP. LA is 

4 
K. Mandic and B. Delibasic 
 
a technique that gives the user the most options in modeling and treating negation in 
the right way. 
3 
The Method of Solving the Problem of Supplier Selection by 
Using IBA 
The problem which is analyzed in the paper is the selection of the best suppliers with-
in a telecommunications company. The company specializes in the manufacture of the 
equipment necessary for building, monitoring and maintenance of telecommunication 
systems and wants to choose the best company for the supply of repeater transmission 
frequencies that allow coverage area without GSM signal or a very weak sig-
nal. Three suppliers' companies were considered that are ranked based on four basic 
attributes and nine sub-attributes (Table 1). 
 
Table 1. Presentation of attributes and sub-attributes 
Attributes 
Sub-attributes 
Attribute type 
Unit 
Max/Min 
Production  
characteristics (K1) 
Technical perfor-
mances (k11) 
Quantitative 
Excellent, Very good, 
Good, Satisfactory, 
Unsatisfactory 
Max 
Product quality (k12) 
Qualitative 
Excellent, Very good, 
Good, Satisfactory, 
Unsatisfactory 
Max 
Delivery time (k13) 
Quantitative 
Day 
Min 
Supplier profile 
(K2) 
Reference (k21) 
Qualitative 
Excellent, Very good, 
Good, Satisfactory, 
Unsatisfactory 
Max 
Brand position (k22) 
Qualitative 
Excellent, Very good, 
Good, Satisfactory, 
Unsatisfactory 
Max 
Financial 
aspect (K3) 
Product price (k31) 
Quantitative 
Eur 
Min 
Product costs (k32) 
Quantitative 
Eur 
Min 
Support and 
services (K4) 
Service (k41) 
Qualitative 
Excellent, Very good, 
Good, Satisfactory, 
Unsatisfactory 
Max 
Technical support 
(k42) 
Qualitative 
Excellent, Very good, 
Good, Satisfactory, 
Unsatisfactory 
Max 
 
 
 
Within this paper it will be displayed how IBA can help managers include their 
preferences in a more sophisticated way compared to weights approach. In many 
techniques of decision making (conventional fuzzy) a weighted approach is used 
which allows exclusively linear relationship between the attributes. However, when 
solving problems with multi-attribute decision-making method, such as the problem 
of selection of suppliers, often attributes are interdependent and it is needed to estab-
lish between them the logical interactions. Logical interactions are based on the intro-
duction of Boolean algebra operators ר,ש, ൓, by which managers can more clearly 

 
Supplier Selection Using Interpolative Boolean Algebra and Logic Aggregation 
5 
 
show dependence and comparisons between attributes. In this way, a large number of 
real problems can be expressed by the Boolean algebra. 
As part of Table 2 the quantitative and qualitative values of the sub-attributes are 
presented. 
Table 2. The values of sub-attributes 
  
Production characteristics 
Supplier profile 
Financial 
aspect 
Support and services 
  
Tech 
perform. 
Quality 
Delivery 
Time 
References
Brand 
Price 
Costs 
Service 
Tech. 
support 
S1 
good 
very 
good 
30 
good 
satisfactory
250 
120 
excellent 
excellent 
S2 
very good 
very 
good 
45 
excellent 
very good 
345 
85 
excellent 
very good 
S3 
satisfactory
good 
30 
good 
good 
275 
110 
good 
excellent 
 
 
 
 
 
 
 
 
 
 
The problem was analyzed in the initial interval ሾ1,5ሿ, the values of Table 2 are 
converted into the quantitative values presented in Table 3. 
Table 3. Quantitative values of the sub-attributes 
  
Production characteristics 
Supplier profile 
Financial aspect 
Support and services 
  
Technical 
perform. 
Quality 
Delivery 
Time 
References 
Brand 
Price 
Costs 
Service 
Technical 
support 
S1 
3 
4 
3 
3 
2 
3 
4 
5 
5 
S2 
4 
4 
2 
5 
4 
2 
5 
5 
4 
S3 
2 
3 
3 
3 
3 
3 
4 
3 
5 
 
As mentioned above, fuzzy logic takes values from the interval ሾ0,1ሿ, it indicates 
that it is necessary to convert the value of the sub-attributes from the initial 
val ሾ1,5ሿ to interval ሾ0,1ሿ, i.e. it is necessary to perform a normalization. After the 
normalization, the values of the sub-attributes are presented in Table 4. 
Table 4. Normalized values of sub-attributes 
  
Production characteristics 
Supplier profile 
Financial aspect 
Support and services 
  
Technical 
perform. 
Quality 
Delivery 
Time 
References 
Brand 
Price 
Costs 
Service 
Technical 
support 
S1 
0,6 
0,8 
0,6 
0,6 
0,4 
0,6 
0,8 
1 
1 
S2 
0,8 
0,8 
0,4 
1 
0,8 
0,4 
1 
1 
0,8 
S3 
0,4 
0,6 
0,6 
0,6 
0,6 
0,6 
0,8 
0,6 
1 
 

6 
K. Mandic and B. Delibasic 
 
In order to select the best supplier it is necessary to introduce a weighted sum of 
the attributes/sub-attributes, which is represented by Eq. (9):   
                                                   ݓଵכ ݇ଵ൅ ݓଶכ ݇ଶൌ݌ ,                                                  ሺ9ሻ 
where ݓଵ and ݓଶ represent the weight in this model, ݇ଵ and ݇ଶ are the values of 
attributes/sub-attributes and ݌ represents supplier's total point in interval ሾ0,1ሿ.  
Managers believe that for the selection of suppliers, in this case, it is important to 
take into consideration sub-attributes Technical performances and Quality, as an sub-
attribute does not exclude other. Hence, the logical relation was established by using 
the Boolean operator ר and the sub-attribute function have the following form Eq. 
(10): 
               0,7 כ ሺ݇ଵଵר ݇ଵଶሻ൅0,3 כ ݇ଵଷ ൌ0,7 כ ሺ݇ଵଵ ٔ ݇ଵଶሻ൅0,3 כ ݇ଵଷൌ݌,     ሺ10ሻ 
where weights ݓଵ and ݓଶ have following values 0.7 and 0.3. Also, within the equation 
was used standard product as appropriate operator of GP. 
Weight sum for sub-attributes Reference and Brand position have values 0.6 and 
0.4 respectively, shown in Eq. (11): 
                                                   0,6 כ ݇ଶଵ൅0,4 כ ݇ଶଶൌ݌,                                              ሺ11ሻ 
for sub-attributes Price and Costs weight sum are 0.7 and 0.3 Eq. (12): 
                                                   0,7 כ ݇ଷଵ൅0,3 כ ݇ଷଶൌ݌ ,                                              ሺ12ሻ 
and for sub-attributes Service and Technical support are 0.6 and 0.4 Eq. (13): 
                                                   0,6 כ ݇ସଵ൅0,4 כ ݇ସଶൌ݌ ,                                             ሺ13ሻ 
By the inclusion of normalized ݇-values from Table 4 sub-attributes functions were 
set and by the application of LA we obtain the values of alternatives (suppliers) for 
the four basic attributes as shown in the Table 5. 
Table 5. Values of suppliers for the four basic attributes 
  
Production characte-
ristics (K1) 
Supplier Profile (K2) 
Financial aspect (K3) 
Support and  
Services (K4) 
S1 
0,516 
0,52 
0,66 
1 
S2 
0,568 
0,92 
0,58 
0,92 
S3 
0,348 
0,6 
0,66 
0,76 
 
However, in real situations, managers often want to set the mutual relationships be-
tween the attributes in order to bring the best possible decision. This was enabled by 
using the logical conditions, presented hereinafter: 

 
Supplier Selection Using Interpolative Boolean Algebra and Logic Aggregation 
7 
 
Condition 1: "If the production characteristics are at a high level, then the product is 
acceptable, if it is not at high level pay attention to the profile of the supplier, the 
financial aspect and the support and services." (Eq. (14)) 
 
                                                  ݇ଵ ש ሺ൓ ݇ଵר ݇ଶר ݇ଷר ݇ସሻ                                        ሺ14ሻ 
Condition 2: "If a supplier profile is satisfying he should also have good production 
characteristics, if the profile of the supplier is not satisfactory attention should be paid 
to the financial aspect and the support and services." (Eq. (15)) 
 
                                            ሺ݇ଶ ר ݇ଵሻש ሺ൓ ݇ଶר ݇ଷר ݇ସሻ                                         ሺ15ሻ 
Condition 3: "If the financial aspect is high, attention should be paid to the manufac-
turing characteristics, if not high, attention should be paid to profile of supplier." (Eq. 
(16)) 
                                              ሺ݇ଷ ר ݇ଵሻ ש ሺ൓ ݇ଷר ݇ଶሻ                                               ሺ16ሻ 
Each of these logical conditions is transformed to the GBP, by using standard 
product as appropriate operator of GP. Transformation is given in the following steps 
Eq. (17): 
 
݇ଵ ש ሺ൓ ݇ଵר ݇ଶר ݇ଷר ݇ସሻ                   
ൌ  ݇ଵ൅ሺ൓ ݇ଵר ݇ଶר ݇ଷר ݇ସሻെ݇ଵٔ ሺ൓ ݇ଵר ݇ଶר ݇ଷר ݇ସሻ
ൌ݇ଵ൅൫ሺ1 െ݇ଵሻٔ ݇ଶٔ ݇ଷ ٔ ݇ସ൯െ݇ଵ
ٔ ൫ሺ1 െ݇ଵሻٔ ݇ଶٔ ݇ଷ ٔ ݇ସ൯
ൌ݇ଵ൅݇ଶٔ ݇ଷ ٔ ݇ସെ݇ଵٔ ݇ଶٔ ݇ଷ 
ٔ ݇ସ                                                                                                         ሺ17ሻ 
 
In the same way the remaining two logical conditions are transformed, which is 
represented by the Eq. (18,19): 
ሺ݇ଶ ר ݇ଵሻש ሺ൓ ݇ଶר ݇ଷר ݇ସሻൌ ݇ଶ ٔ ݇ଵ൅ ݇ଷ ٔ ݇ସെ ݇ଶٔ ݇ଷٔ ݇ସ  ሺ18ሻ 
ሺ݇ଷ ר ݇ଵሻ ש ሺ൓ ݇ଷר ݇ଶሻൌ ݇ଶെ݇ଶٔ ݇ଷ൅݇ଷٔ ݇ଵ                                    ሺ19ሻ 
In the presented GBP equations we will introduce the attributes values from Table 
5 based on which by using LA we obtain the values in Table 6. 
Table 6. The values of logical conditions for three suppliers (S1, S2, S3) 
  
Condition 1
Condition 2
Condition 3 
S1 
0,682 
0,585 
0,517 
S2 
0,78 
0,565 
0,715 
S3 
0,544 
0,409 
0,433  

8 
K. Mandic and B. Delibasic 
 
The final ranking of suppliers is obtained by placing another of the weighted func-
tion, where instead of individual values of sub-attributes we introduce previously 
mentioned logical conditions, shown in Eq. (20): 
 
0,5 כ ሺ݇ଵ ש ሺ൓ ݇ଵר ݇ଶר ݇ଷר ݇ସሻሻ൅0,2 כ ሺሺ݇ଶ ר ݇ଵሻש ሺ൓ ݇ଶר ݇ଷר ݇ସሻ൅0,3
כ ሺ݇ଷ ר ݇ଵሻ ש ሺ൓ ݇ଷר ݇ଶሻൌ݌                                                       ሺ20ሻ 
     Weights for conditions are 0.5, 0.2 and 0.3 respectively, and ݌ represents the final 
supplier rank. 
By entering the obtained values of logical conditions of Table 6 in the expression 
and by application of pseudo logical aggregation we obtain values of Table 7. 
Table 7. The final ranking of suppliers 
S1 
0,613 
S2 
0,717 
S3 
0,484 
 
From Table 7 we can see that the order of suppliers is as follows: ܵ2>ܵ1>ܵ3. 
4 
Conclusion 
The reason of analysis of the presented model is primarily to provide practical support 
to decision-makers i.e. managers when choosing suppliers in the telecommunications 
sector. Also, with the use of logical conditions managers are able to present more 
clearly their requirements. In this way, they can make a more comprehensive and 
better decision than would be the case with conventional fuzzy methods which are not 
in the Boolean framework and that in a different way treat negation. 
In addition to solving the observed problems in this paper is used the weighted ap-
proach combined with the Boolean consistent fuzzy logic. IBA logic enabled the 
transformation of logic functions to a generalized Boolean polynomial. While by the 
use of Logical/pseudological aggregation GBP is reduced to values.  In this way we 
achieved the ranking of suppliers (ܵ2>ܵ1>ܵ3). What makes this logic more suitable 
way to solve these types of problems compared to conventional fuzzy logic is that the 
structural transformations are performed before the introduction of values. 
Further research will be directed towards the inclusion of logical conditions into 
the multi-attribute decision-making method AHP, TOPSIS, ELECTRE and compari-
son of the obtained results. 
 

 
Supplier Selection Using Interpolative Boolean Algebra and Logic Aggregation 
9 
 
References 
1. Boole, G.: The calculus of logic. The Cambridge and Dublin Mathematical Journal 3, 183–
198 (1848) 
2. Zadeh, A.L.: Fuzzy sets. Information and Control 8(3), 338–353 (1965) 
3. Lukasiewicz, J.: Selected Works. In: Brokowski, L. (ed.). North-Holland Publ. Comp., 
Amsterdam and PWN (1970) 
4. Radojevic, D.: Logical measure of continual logical function. In: 8th International Confe-
rence IPMU – Information Processing and Management of Uncertainty in Knowledge-
Based Systems, Madrid, pp. 574–578 (2000) 
5. Radojevic, D.: New [0, 1] – valued logic: A natural generalization of Boolean logic.  
Yugoslav Journal of Operational Research – YUJOR 10(2), 185–216 (2000) 
6. Dragovic, I., Turajlic, N., Radojevic, D., Petrovic, B.: Combining Boolean consistent 
fuzzy logic and AHP illustrated on the web service selection problem. International Jour-
nal of Computational Intelligence Systems 7(1), 84–93 (2013) 
7. Radojevic, D.: Fuzzy Set Theory in Boolean Frame. International Journal of Computers, 
Communications & Control 3, 121–131 (2008) 
8. Radojevic, D.: Interpolative Realization of Boolean Algebra as a Consistent Frame for 
Gradation and/or Fuzziness. In: Nikravesh, M., Kacprzyk, J., Zadeh, L.A. (eds.) Forging 
New Frontiers: Fuzzy Pioneers II. STUDFUZZ, vol. 218, pp. 295–317. Springer,  
Heidelberg (2008) 
9. Radojevic, D.: Logical measure – structure of logical formula. In: Bouchon-Meunier, B., 
Gutiérrez-Ríos, J., Magdalena, L., Yager, R.R. (eds.) Technologies for Constructing Intel-
ligent System 2: Tools. STUDFUZZ, vol. 90, pp. 417–430. Springer, Heidelberg (2002) 
10. Radojevic, D.: Interpolative realization of Boolean algebra. In: Proceedings of the 
NEUREL 2006, The 8th Neural Network Applications in Electrical Engineering, pp. 201–
206 (2006) 
11. Radojevic, D.: Interpolative Relations and Interpolative Preference Structures. Yugoslav 
Journal of Operations Research 15(2), 171–189 (2005) 
12. Radojevic, D.: Real probability (R-probability): fuzzy probability idea in Boolean frame. 
In: 28th Linz Seminar on Fuzzy Set Theory (2007) 
13. Milošević, P., Nešić, I., Poledica, A., Radojević, D.G., Petrović, B.: Models for Ranking 
Students: Selecting Applicants for a Master of Science Studies. In: Balas, V.E., Fodor, J., 
Várkonyi-Kóczy, A.R., Dombi, J., Jain, L.C. (eds.) Soft Computing Applications. AISC, 
vol. 195, pp. 93–103. Springer, Heidelberg (2012) 
14. Mirkovic, M., Hodolic, J., Radojevic, D.: Aggregation for Quality Management. Yugoslav 
Journal for Operational Research 16(2), 177–188 (2006) 
15. Radojevic, D.: Logical Aggregation Based on Interpolative Boolean Algebra. Mathware & 
Soft Computing 15, 125–141 (2008) 
 

Finitely Additive Probability Measures
in Automated Medical Diagnostics
Milica Kneˇzevi´c1, Zoran Ognjanovi´c1, and Aleksandar Perovi´c2
1 Mathematical Institute of the Serbian Academy of Sciences and Arts,
Kneza Mihaila 36, 11000 Belgrade, Serbia
knezevic.milica@gmail.com, zorano@mi.sanu.ac.rs
2 University of Belgrade – Faculty of Transport and Traﬃc Engineering,
Vojvode Stepe 305, 11000 Belgrade, Serbia
pera@sf.bg.ac.rs
Abstract. We describe one probabilistic approach to classiﬁcation of
a set of objects when a classiﬁcation criterion can be represented as a
propositional formula. It is well known that probability measures are not
truth functional. However, if μ is any probability measure and α is any
propositional formula, μ(α) is uniquely determined by the μ-values of
conjunctions of pairwise distinct propositional letters appearing in α. In
order to infuse truth functionality in the generation of ﬁnitely additive
probability measures, we need to ﬁnd adequate binary operations on [0, 1]
that will be truth functions for ﬁnite conjunctions of pairwise distinct
propositional letters. The natural candidates for such truth functions are
t-norms. However, not all t-norms will generate a ﬁnitely additive prob-
ability measure. We show that G¨odel’s t-norm and product t-norm, as
well as their linear convex combinations, can be used for the extension
of any evaluation of propositional letters to ﬁnitely additive probabil-
ity measure on formulas. We also present a software for classiﬁcation
of patients with suspected systemic erythematosus lupus (SLE), which
implements the proposed probabilistic approach.
Keywords: probability measures, classiﬁcation, fuzzy logic, soft com-
puting, medical diagnosis.
1
Introduction
Reasoning under uncertainty is one of the most prominent research themes in
theoretical computer science for more than four decades. Many-valued logics, i.e.
logics with more than 2 truth values (usually the set of truth values is the real or
the hyper-real unit interval [0,1]) have been extensively studied and developed in
order to formalize various phenomena related to artiﬁcial intelligence. The most
signiﬁcant many-valued logics are fuzzy logics [4, 6, 7, 9, 10, 13], possibilistic
logics [1–3] and probability logics [5, 8, 11, 12, 14–21, 27–31].
Since the late sixties, probability theory has found application in develop-
ment of various medical expert systems. Bayesian analysis, which is essentially
an optimal path ﬁnding through a graph called Bayesian network, has been (and
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 10–19, 2014.
c
⃝Springer International Publishing Switzerland 2014

Finitely Additive Probability Measures in Automated Medical Diagnostics
11
still is) successfully applied in so called sequential diagnostics, when the large
amount of reliable relevant data is available. The graph (network) represents
our knowledge about connections between studied medical entities (symptoms,
signs, diseases); the Bayes formula is applied in order to ﬁnd the path (connec-
tion) with maximal conditional probability. Moreover, a priori and conditional
probabilities were used to deﬁne a number of measures designed speciﬁcally to
handle uncertainty, vague notions and imprecise knowledge. Some of those mea-
sures were implemented in MYCIN in the early seventies. The success of MYCIN
has initiated construction of rule based expert systems in various ﬁelds.
However, designing expert systems with the large number of rules (some of
them like CADIAG-2 have more than 10000) has an unfortunate consequence,
i.e., some of them are turned to be inconsistent. On the other hand, the emer-
gence of theoretical computer science as a new scientiﬁc discipline has lead to
discovery that the completeness techniques from mathematical logic are well
found (and sometimes the only ones) methods for proving correctness of hard-
ware and software. Consequently, mathematical logic has become a theoretical
foundation of artiﬁcial intelligence.
In this paper we will present the mathematical background of the medical
decision support system for the systemic erythematosus lupus (SLE) that we
have been lately working on with our colleagues from the Institute of Allergology
and Immunology in Belgrade. The methodology is based on the work of Dragan
Radojevi´c [22–25] and our joint work [21].
2
Probabilities and Truth Functionality
It is well known that probability measures are not truth functional. However, if
μ is any probability measure and α is any propositional formula, then μ(α) is
uniquely determined by the μ-values of conjunctions of pairwise distinct propo-
sitional variables appearing in α.
Example 1. Let α = (p0 →p1) ∧p2, μ(α) is calculated as:
μ((p0 →p1) ∧p2) = μ((¬p0 ∨p1) ∧p2)
= μ((¬p0 ∧p2) ∨(p1 ∧p2))
= μ(¬p0 ∧p2) + μ(p1 ∧p2) −μ(¬p0 ∧p1 ∧p2)
= μ(p2) −μ(p0 ∧p2) + μ(p1 ∧p2) −μ(p1 ∧p2)
+μ(p0 ∧p1 ∧p2)
= μ(p2) −μ(p0 ∧p2) + μ(p0 ∧p1 ∧p2).
It turns out that the previous example is not a singularity but rather a pattern
that indicates how to fuse truth functionality and ﬁnitely additive probability
measures. Namely, if μ is a ﬁnitely additive probability measure on the set of
propositional formulas, using the complete disjunctive normal form and ﬁnite
additivity, it is easy to see that μ(α) is a ﬁnite sum of terms of the form
μ(±p1 ∧· · · ∧±pn),

12
M. Kneˇzevi´c, Z. Ognjanovi´c, and A. Perovi´c
where p1, . . . , pn are all propositional letters appearing in α, and +pi and −pi
denote pi and ¬pi, respectively. Since
μ(¬ε) = 1 −μ(ε)
(1)
and
μ(ε ∧¬γ) = μ(ε) −μ(ε ∧γ),
(2)
negation can be eliminated from each μ(±p1∧· · ·∧±pn), so μ(α) depends only on
the logical structure of α and μ-values of ﬁnite conjunctions of pairwise distinct
propositional letters.
Hence, in order to infuse truth functionality in the generation of ﬁnitely ad-
ditive probability measures, we need to ﬁnd adequate binary operations on [0, 1]
that will be truth functions for ﬁnite conjunctions of pairwise distinct proposi-
tional letters.
The natural candidates for such truth functions are t-norms. However, not
all t-norms will generate a ﬁnitely additive probability measure, which will be
explicitly shown in this section. We will show that G¨odel’s t-norm TG(x, y) =
min(x, y) and product t-norm TΠ(x, y) = xy can always be used for the extension
of any evaluation e : V ar −→[0, 1] to ﬁnitely additive probability measures eG
and eΠ on formulas. In particular, for pairwise distinct propositional letters
p1, . . . , pn,
eΠ(p1 ∧· · · ∧pn) = e(p1) · · · e(pn)
(3)
and
eG(p1 ∧· · · ∧pn) = min(p1, . . . , pn).
(4)
Product measures eΠ correspond to one extreme situation: stochastic or prob-
ability independence of propositional letters. G¨odel’s measures eG correspond to
another kind of extreme situation: logical dependence of propositional letters.
While stochastic independence is a measure-theoretic property and cannot be
forced by some nontrivial logical conditions (see [8]), logical dependence is ex-
pressible in classical propositional calculus. For instance, logical condition
p →q
clearly entails that
¯e(p ∧q) = min(e(p), e(q)) ,
where ¯e is a ﬁnitely additive probability measure on propositional formulas that
extends e.
Linear convex combinations of ﬁnitely additive probability measures are
ﬁnitely additive probability measures as well, so using eG and eΠ we can con-
struct an inﬁnite scale of probability measures
e(s) = seΠ + (1 −s)eG,
s ∈(0, 1) ∩Q .
(5)
From the uncertainty point of view, the measures e(s) correspond to various
degrees of dependence between propositional letters. From the fuzzyness point

Finitely Additive Probability Measures in Automated Medical Diagnostics
13
of view, the measures e(s) provide countably many ways to extend any initial
evaluation e of propositional letters, which enables probability evaluations of
fuzzy quantities. As before, we will try to illustrate our intended meaning with
the following simple example:
Example 2. Suppose that we have to classify compounds C1, C2, and C3 of
the substances p and q according to the criteria of minimal harmfulness of a
compound. It is known that both p and q are harmful, but they neutralize each
other. Concentrations of substances p and q in compounds C1 , C2, and C3 are
given in Table 1.
Table 1. Concentrations of substances in compounds
Compound Concentration of p Concentration of q
C1
0.95
0.05
C2
0.15
0.85
C3
0.65
0.35
Syntactically, we consider p and q as propositional letters. Since substances
p and q neutralize each other, minimal harmfulness criteria is adequately repre-
sented by the formula p ↔q. If e is any [0, 1]-evaluation of p and q, then
eΠ(p ↔q) = eΠ((p ∧q) ∨(¬p ∧¬q))
= eΠ(p ∧q) + eΠ(¬p ∧¬q)
= e(p) · e(q) + eΠ(¬p) −eΠ(¬p ∧q)
= e(p) · e(q) + 1 −e(p) −e(q) + e(p) · e(q)
= 1 −e(p) −e(q) + 2e(p) · e(q) .
Similarly,
eG(p ↔q) = 1 −e(p) −e(q) + 2 min(e(p), e(q))
and
e(0.25)(p ↔q) = 0.25 eΠ(p ↔q) + 0.75 eG(p ↔q) .
If we interpret C1, C2, and C3 as [0, 1]-evaluations of p and q, then we can
easily evaluate p ↔q in eG, eΠ, and e(0.25). The results are displayed in Table
2.
All three columns eG(p ↔q), eΠ(p ↔q), and e(0.25)(p ↔q) induce the
obviously correct classiﬁcation: the least harmful compound is C3, then follows
C2, and the most harmful compound is C1.
Arguably, a mathematical form of classiﬁcation is an ordering of some ﬁnite
set of evaluations according to certain criteria. More precisely, classiﬁcation of
the ﬁnite set of evaluations {e0, . . . , ek}, ei : V ar −→[0, 1] with respect to the

14
M. Kneˇzevi´c, Z. Ognjanovi´c, and A. Perovi´c
Table 2. Evaluation of formula p ↔q in eG, eΠ, and e(0.25)
Evaluation e(p) e(q) eG(p ↔q) eΠ(p ↔q) e(0.25)(p ↔q)
C1
0.95 0.05
0.1
0.095
0.099
C2
0.15 0.85
0.3
0.255
0.289
C3
0.65 0.35
0.7
0.455
0.639
set of attributes (propositional letters) {p1, . . . , pn} and the criterium function
f : [0, 1]n −→[0, 1] is the partial ordering < deﬁned by
ei < ej iﬀf(ei(p1), . . . , ei(pn)) < f(ej(p1), . . . , ej(pn)) .
In practice, we can apply the measures e(s)
i
in any classiﬁcation problem where at
least one part of the computation of the criterion function f involves computation
of the truth value of certain formula α(p1, . . . , pn).
For example, suppose that we want to develop a fuzzy relational database
for automated trade of furniture, where database entries are evaluations of
predeﬁned quality attributes. User’s queries should be stated in the form of
propositional formulas over the quality attributes. The resolution process we will
illustrate on the example of the query “ﬁnd me a sturdy but light wooden chair
that is not too expensive”:
– Prompt the user to chose a rational s ∈[0, 1]. Here s represents user’s esti-
mation of dependence between quality attributes in the query. 1 represents
stochastic independence, while 0 represents logical dependence;
– Compute
e(s)(sturdy ∧light ∧wooden ∧¬(too expenssive))
for all relevant database entries e;
– Return to the user all relevant database entries e with maximal e(s)-values.
Of course, one might ask the adequacy of such approach, especially in the
evaluation of the queries such as “ﬁnd me a sturdy but light wooden chair that is
not too expensive”, which, at least at the ﬁrst sight, have no natural probabilistic
interpretation.
We can oﬀer several justiﬁcation arguments. The most obvious is the capri-
cious one “why not”, which is rooted in the fact that any classiﬁcation criterion
expressible as a classical propositional formula can be eﬀectively evaluated from
the initial evaluation e of primary attributes (propositional letters) using eG,
eΠ and e(s) measures. Substantiality (with respect to the question of adequacy),
this is not too much diﬀerent from any other evaluation method involving truth
functionality principle. The only diﬀerence is in the computation of the value of
the underlying formula.

Finitely Additive Probability Measures in Automated Medical Diagnostics
15
Deeper arguments are connected with the choice of the adequate parameter
s, as well as with the construction of the initial fuzzy sets that represent un-
derlying fuzzy attributes. For instance, attributes such as ”expensive” are more
psychological than anything else, so it is natural to construct their mathematical
representation - fuzzy set using relevant statistical data.
As we have mentioned before, the product measure corresponds to the one
extreme - stochastic or probability independence of attributes (propositional
letters), while G¨odel’s measure corresponds to the other extreme - logical de-
pendence of attributes. The standard statistical techniques, such as linear or
nonlinear regression, can be applied to measure stochastic independence of fuzzy
attributes.
Intermediate measures e(s) are particularly useful in the cases where both
eG and eΠ do not classify observed objects. Namely, it is easy to construct an
example with the measurement results given in Table 3.
Table 3. Ordering of objects according to classiﬁcation criteria α
object eΠ(α) eG(α) e(0.5)
A
0.3
0.4
0.35
B
0.3
0.6
0.45
C
0.5
0.6
0.55
As a consequence, neither the product nor G¨odel’s measure provide classiﬁ-
cation of objects A, B and C according to the classiﬁcation criteria α, while the
arithmetic mean e(0.5) provides a classiﬁcation - linear ordering A < B < C that
is sound with both partial orderings induced by eΠ and eG. In this example, eΠ
induces partial ordering A < C and B < C, while eG induces partial ordering
A < C and A < C.
What we want to say is that, in cases where we disregard independence is-
sues and only evaluate formulas with both eΠ and eG, intermediate measures
e(s) might give additional information that is sound with the partial orderings
induced by eΠ and eG, and provide a ﬁner classiﬁcation.
3
Implementation
Based on the previously presented theoretical framework, a software component
was developed as a utility to measure similarity among patients, with suspected
SLE, based on patients’ medical test results and symptoms. Classifying patients
in this way and comparison of new patients with those with already given di-
agnosis would help and support medical decision making process. The software
component is implemented as a desktop application written in Java and Pro-
log. The graphical user interface (GUI) is written in Java, while Prolog was a
more natural choice for implementation of the logic component that lies in the
background of the algorithm.

16
M. Kneˇzevi´c, Z. Ognjanovi´c, and A. Perovi´c
Patients’ data are stored in JavaScript Object Notation (JSON) format[32]. It
is an open-standard format for representing data as human readable text. JSON
format is language independent and parsers for JSON data are readily available
in nearly all modern programming languages. Patients’ data are organized in
two JSON ﬁles. First ﬁle is parameterList and it contains descriptions of 80
medical symptoms and tests related and relevant for diagnosing SLE. We will
use the term parameter to denote both medical symptoms and tests. Second ﬁle
is patientList and it contains actual patients’ results for parameters deﬁned in
parameterList. Description of a parameter in the ﬁle parameterList is given as
a JSON data object of the following structure:
parameterCode:{
”name”:name,
”possibleValues”:{
value : valueName,
...
value : valueName
}
}
Parameter code is an integer that is used for easy identiﬁcation of the parameter
itself. Name is a string that represents medical term that describes the parame-
ter. Possible values is a JSON object that represents a discrete set of values that
a given parameter can have. Value name is a human readable description of the
associated value. For better understanding of the described object structure we
give examples that show representation of Antinuclear Antibodies (ANAs) and
Coombs test as JSON objects:
”0”:{
”name”:”ANA”,
”possibleValues”:{
”0”:”negative”,
”0.5”:”positive ≤1:80”,
”1”:”positive > 1:80”
}
},
”1”:{
”name”:”Coombs test”,
”possibleValues”:{
”0”:”negative”,
”1”:”positive”
}
}

Finitely Additive Probability Measures in Automated Medical Diagnostics
17
A user enters a formula ϕ with names of the symptoms and medical tests
from the ﬁle parameterList as propositional letters. We give an example of
such formula, which can be used to express level of dependency of leukopenia,
lymphopenia, malar rash and ANA:
ϕ = ((leukopenia ∨lymphopenia) ∧malar rash) →ANA
Similarity of the patients is then computed with regard to ϕ. In order to compute
μ(ϕ) for each patient, formula ϕ is transformed into a logically equivalent formula
ϕ′ for which it is simpler to compute μ(ϕ′) by applying rules from Sect. 2.
Disjunctive normal form is selected as a suitable logical equivalent of the entered
formula ϕ. ϕdnf will denote the disjunctive normal form of ϕ. Formula ϕ is passed
to the Prolog module of the software component and this module is responsible
for the transformation ϕ ⇝ϕdnf. Formula ϕdnf is in the form Clause1 ∨... ∨
Clausen, where each Clausei is in the form ±pi1 ∧...∧±pik. Internally, in Prolog,
ϕdnf is represented as a list of lists: [[±p11, ..., ±p1k], ..., [±pn1, ..., ±pnk]]. This
representation enables computation of μ(ϕ) using dynamic programming and
applying inclusion-exclusion principle.
μ(ϕ) = μ(ϕdnf) =
n

l=1
(−1)l

1≤i1<...<il≤n
μ(Clausei1 ∧... ∧Clauseil)
where each μ(Clausei1 ∧...∧Clauseil) is of the form μ(±pj1 ∧...∧±pjm) and thus
can be computed easily following the rules from Sect. 2. For each patient, μ(ϕ) is
then computed by assigning actual values to propositional letters in accordance
with data stored in patientList ﬁle. Value of μ(ϕ) for a given patient, expresses
the level of accuracy of the formula ϕ in accordance with patient’s data.
Acknowledgments. This work is supported by the Ministry of Science, Tech-
nology and Development, Republic of Serbia (project III 044006).
References
1. Dubois, D., Lang, J., Prade, H.: Possibilistic logic. In: Gabbay, D.M., Hogger, C.J.,
Robinson, J.A., Seikmann, J.H. (eds.) Handbook of Logic in Artiﬁcial Intelligence
and Logic Programming, vol. 3, pp. 439–513. Oxford University Press, Inc., New
York (1994)
2. Dubois, D., Prade, H.: Possiblistic logic: a retrospective and prospective view.
Fuzzy Sets and Systems 144, 3–23 (2004)
3. Dubois, D., Godo, L., Prade, H.: Weighted logic for artiﬁcial intelligence: an intro-
ductory discussion. In: Godo, L., Prade, H. (eds.) ECAI 2012 Workshop, Weighted
Logic for Artiﬁcial Intelligence, pp. 1–7 (2012)
4. Esteva, F., Godo, L., Montagna, F.: The LΠ and LΠ 1
2 logics: two complete fuzzy
logics joining Lukasiewicz and product logic. Archive for Mathematical Logic 40,
39–67 (2001)
5. Fagin, R., Halpern, J., Megiddo, N.: A logic for reasoning about probabilities.
Information and Computation 87(1-2), 78–128 (1990)

18
M. Kneˇzevi´c, Z. Ognjanovi´c, and A. Perovi´c
6. Flaminio, T.: Strong non-standard completeness for fuzzy logic. Soft Comput-
ing 12(4), 321–333 (2008)
7. Godo, L., Marchioni, E.: Coherent conditional probability in a fuzzy logic setting.
Logic Journal of the IGPL 14(3), 457–481 (2006)
8. Hailperin, T.: Sentential Probability Logic. Associated University Presses, Inc.,
London (1996)
9. Hajek, P., Esteva, F., Godo, L.: Fuzzy logic and probability. In: Proceedings of the
11th Conference on Uncertainty in Artiﬁcial Inteligence, Montreal, Canada, pp.
237–244 (1995)
10. H´ajek, P.: Methemathematics of Fuzzy Logic. Kluwer Academic Publishers,
Dordrecht (1998)
11. Halpern, J.: Reasoning about Uncertainty. The MIT Press, Cambridge (2003)
12. Lehmann, D.: Generalized qualitative probability: savage revisited. In: Proceedings
of the 12th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 1996), pp.
381–388 (1996)
13. Marchioni, E., Montagna, F.: On triangular norms and uninorms deﬁnable in LΠ 1
2.
International Journal of Approximate Reasoning 47(2), 179–201 (2008)
14. Narens, L.: On qualitative axiomatizations for probability theory. Journal of Philo-
sophical Logic 9(2), 143–151 (1980)
15. Nilsson, N.: Probabilistic logic. Artiﬁcial Intelligence 28(1), 71–87 (1986)
16. Ognjanovi´c, Z., Raˇskovi´c, M.: A logic with higher order probabilities. Publications
de l’Institut Mathematique, Nouvelle s´erie 60(74), 1–4 (1996)
17. Ognjanovi´c, Z., Raˇskovi´c, M.: Some probability logics with new types of probability
operators. Journal of Logic and Computation 9(2), 181–195 (1999)
18. Ognjanovi´c, Z., Raˇskovi´c, M.: Some ﬁrst-order probability logics. Theoretical Com-
puter Science 247(1-2), 191–212 (2000)
19. Ognjanovi´c, Z., Perovi´c, A., Raˇskovi´c, M.: Logic with the qualitative probability
operator. Logic Journal of IGPL 16(2), 105–120 (2008)
20. Ognjanovi´c, Z., Raˇskovi´c, M., Markovi´c, Z.: Probability logics. In: Ognjanovi´c, Z.
(ed.) Logic in Computer Science, vol. 12(20), pp. 35–111. Mathematical Institute
of Serbian Academy of Sciences and Arts (2009)
21. Ognjanovi´c, Z., Perovi´c, A., Raˇskovi´c, M., Radojevi´c, D.: Finitely additive prob-
ability measures on classical propositional formulas deﬁnable by G¨odel’s t-norm
and product t-norm. Fuzzy Sets and Systems 169(1), 65–90 (2011)
22. Radojevi´c, D.: [0, 1]-valued logic: a natural generalization of Boolean logic. Yu-
goslav Journal on Operations Research 10(2), 185–216 (2000)
23. Radojevi´c, D.: Interpolative realization of Boolean algebra. In: 8th Seminar on
Neural Network Applications in Electrical Engineering (NEUREL 2006), pp. 201–
206 (2006)
24. Radojevi´c, D.: Interpolative realization of Boolean algebra as a consistent frame
for gradation and/or fuzziness. In: Nikravesh, M., Kacprzyk, J., Zadeh, L.A. (eds.)
Forging New Frontiers: Fuzzy Pioneers II. STUDFUZZ, vol. 218, pp. 295–317.
Springer, Heidelberg (2008)
25. Radojevi´c, D.: Logical aggregation based on interpolative realization of Boolean
algebra. Mathware and Soft Computing 15(1), 125–141 (2008)
26. Radojevi´c, D., Perovi´c, A., Ognjanovi´c, Z., Raˇskovi´c, M.: Interpolative Boolean
logic. In: Dochev, D., Pistore, M., Traverso, P. (eds.) AIMSA 2008. LNCS (LNAI),
vol. 5253, pp. 209–219. Springer, Heidelberg (2008)
27. Raˇskovi´c, M.: Classical logic with some probability operators. Publications de
l’Institut Mathematique, Nouvelle S´erie 53(67), 1–3 (1993)

Finitely Additive Probability Measures in Automated Medical Diagnostics
19
28. Raskovi´c, M., Ognjanovi´c, Z.: A ﬁrst order probability logic LPQ. Publications de
l’Institut Mathematique, Nouvelle S´erie 65(79), 1–7 (1999)
29. Raˇskovi´c, M., Markovi´c, Z., Ognjanovi´c, Z.: A logic with approximate conditional
probabilities that can model default reasoning. International Journal of Approxi-
mate Reasoning 49(1), 52–66 (2008)
30. van der Hoek, W.: Some considerations on the logic PF D: a logic combining modal-
ity and probability. Journal of Applied Non-Classical Logics 7(3), 287–307 (1997)
31. Wellman, M.P.: Some varieties of qualitative probability. In: Bouchon-Meunier, B.,
Yager, R.R., Zadeh, L.A. (eds.) IPMU 1994. LNCS, vol. 945, pp. 171–179. Springer,
Heidelberg (1995)
32. JSON (JavaScript Object Notation), http://json.org

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 20–27, 2014. 
© Springer International Publishing Switzerland 2014  
Demonstrative Implications of a New Logical 
Aggregation Paradigm on a Classical Fuzzy Evaluation 
Model of «Green» Buildings  
Milan Mrkalj 
Faculty of Organizational Sciences, Belgrade, Serbia  
mrki2000@mail.bg 
Abstract. This paper demonstrates the application possibilities of the genera-
lised model for Logical Aggregation (LA) in performance and quality evalua-
tion of Green buildings through establishing a linear order of them by a (scalar) 
general aggregated quality parameter and sorting them into categories. An exist-
ing classical fuzzy model is experimentally modified in order to demonstrate 
the advanced capabilities of adequate articulation of the partial demands for 
quality via the new generalised model for logical aggregation. 
Keywords: Logical Aggregation, Quality management, Green buildings. 
1 
Introduction 
The advanced abilities of generalised model for Logical Aggregation presented in 
[1,4,5,6,7] expand the boundaries of existing fuzzy model presented in [2]. The exist-
ing model is in essence an aggregation model since the inputs are multidimensional 
values and the output is a scalar quantity which provides base for simple linear sort-
ing. In existing fuzzy model only trivial quality attributes are used, without interaction 
between them through logical functions which should improve the articulation of 
partial demands for quality. Also, the aggregation operator (explained in [3,6]) is only 
dot product. 
The model for Logical Aggregation (LA) introduces the use of logical functions  
between trivial attributes which emerge from the finite set of possible functions – 
Boolean algebra generated over the set of attributes. The new complex aggregation 
operators are introduced with the use of different generalised products which belong 
to a subclass of T-norms with additional axiomatic term of nonnegativity, as shown in 
[1,4,6]. 
Implementation of the model for Logical Aggregation [1] is possible in both phases 
of aggregation in the existing model [2]. The LA paradigm emanates an algebraic 
structure which provides the basis for arithmetic calculation of the output results. 
 

 
Demonstrative Implications of a New Logical Aggregation Paradigm 
21 
 
2 
Implementaton of Logical Aggregation in the First Phase of 
Aggregation of the Existing Fuzzy Model 
Implementation of LA on the first phase of aggregation of existing model is demon-
strated, and the values of the output are compared. The aim is to show the ability to 
create complex attributes which have more power of expressing the partial demands 
for quality in quality management. The term attributes from [1,3,6] are named as 
“factors” in [2] and are organized as two “factor sets”. 
Vector of weights of secondary attributes of the primary attribute energy efficient 
and energy utilization from [2] is: A2 =  [
]
0,35
0,35
0,2
0,1 . 
Weights of secondary attributes are respectively wa = 0,35  wb = 0,35  wc = 0,2  
and  wd = 0,1. In this case, generalised pseudo-Boolean polynomial is trivial (con-
tains only single basic attributes). 
Now we shall unify attributes architecture design (a) and envelope structure (b) in-
to one single new complex attribute on the basis of the fact that these two single 
attributes are significantly positive correlated in qualitative sense (architecture design 
and envelope structure which assures the rational heat transfer coefficient of external 
surfaces) [2]. 
Single attributes a and b will be dismissed (their respective weights become zero 
instead of 0,35), and new complex attribute a∧b (1) will be introduced, with weight 
wa∧b = wa + wb = 0.7. 
 
ϕ(a,b) = (a ∩ b) 
(1) 
Generalised Boolean polynomial of this logical expression is given in (2). 
 
ϕ⊗ (a,b) = ((a ∩ b)) ⊗ = a ⊗ b 
(2) 
New Logical Aggregation operator based on [3] is given in (3). 
 
Agg⊗ (a,b,c,d) = 7
10
ϕ⊗ (a,b) + 1
5
c + 1
10
d = 7
10
(a ⊗ b) + 1
5
c + 1
10
d 
(3) 
Corresponding aggregation measure [3] is given in (4). 
 
μ = 7
10
 (σa ∧ σb) + 1
5
σc + 1
10
σd 
(4) 
Aggregation measures (4) are shown in Table 1: 
Table 1.  
S 
a 
b 
0,2 c 
0,1 d 
0,7 a∧b 
μ(S) 
{0} 
0 
0 
0 
0 
0 
0 
{a} 
1 
0 
0 
0 
0 
0 
{b} 
0 
1 
0 
0 
0 
0 

22 
M. Mrkalj 
 
Table 1. (continued) 
{c} 
0 
0 
1 
0 
0 
0,2 
{d} 
0 
0 
0 
1 
0 
0,1 
{a,b} 
1 
1 
0 
0 
1 
0,7 
{a,c} 
1 
0 
1 
0 
0 
0,2 
{a,d} 
1 
0 
0 
1 
0 
0,1 
{b,c} 
0 
1 
1 
0 
0 
0,2 
{b,d} 
0 
1 
0 
1 
0 
0,1 
{c,d} 
0 
0 
1 
1 
0 
0,3 
{a,b,c} 
1 
1 
1 
0 
1 
0,9 
{a,b,d} 
1 
1 
0 
1 
1 
0,8 
{a,c,d} 
1 
0 
1 
1 
0 
0,3 
{b,c,d} 
0 
1 
1 
1 
0 
0,3 
{a,b,c,d} 
1 
1 
1 
1 
1 
1 
 
 
On value level input parameters from R2 matrix from [2] are processed through op-
erator given in (3). 
Table 2.  
«Energy efficient and 
energy utilization» -
values from R2 
CATEGORIES 
one star 
* 
two stars 
** 
three stars 
*** 
Architecture design a 
0,1 
0,2 
0,7 
Envelope structure b 
0,1 
0,2 
0,7 
HVAC c 
0,15 
0,25 
0,6 
Lighting system d 
0,1 
0,25 
0,65 
 
In qualitative sense, attributes given in Table 2 are highly positive correlated, so 
the most suitable generalised product [3,4,6] should be the min function (⊗ := min). 
Aggregation operator (3) processes one by one column of R2 matrix and outputs into 
vector B2'. 
 
Agg⊗ (a,b,c,d) :  R2 → B2' 
 
B2'=[0,11   0,215   0,675] 
This result of Logical Aggregation coincides with the result of first phase aggrega-
tion of the existing fuzzy model developed in [2], so B2' = B2. 
Now we shall use dot product as generalised product for LA (⊗: = *). 
 
B2'=[0,047   0,115   0,675] 
Different choice of generalised product obviously induces changes on the level of 
values,  B2' ≠ B2. After first phase of aggregation, the resulting values of quality are 
different from the values given by existing model. 

 
Demonstrative Implications of a New Logical Aggregation Paradigm 
23 
 
3 
Simulation of the Use of Aggregation Operator of Existing 
Fuzzy Model through Logical Aggregation Operator 
New model of LA [1,4,6] will be applied again on the first phase of aggregation of the 
existing model [2], and the results will be compared. The aim is to show the possibili-
ties of manipulation on the level of internal structure of the attributes which is frag-
mented via the new LA paradigm and to achieve the same results as with usage of the 
existing model. 
Vector of weights of the secondary attributes of the primary attribute operation 
management [2] is: A6  = [0,3   0,3   0,4]. Weights of secondary attributes a (gar-
bage classification and biologic treatment),b (intellectualized system) and c (all kinds 
of management system) are respectively wa = 0,3  wb = 0,3  and wc = 0,4. 
Generalised pseudo-Boolean polynomial [1] of the single attributes is equal to the 
attributes themselves: 
 
ϕ⊗ (a) = av      ϕ⊗ (b) = bv      ϕ⊗ (c) = cv 
From the aspect of immanent structure, single attributes are complex elements 
(contain more than one atomic attribute [1]). In this particular case, every attribute a, 
b, or c includes 4 atomic attributes. Some of the atomic attributes (which present in-
ternal algebraic structure) are shared among the attributes. 
Now aggregation measures μ(S) are assigned to the atomic attributes (S) according 
to weights given in vector A6. 
Table 3.  
 
 
 
Atoms 
(S) 
w σϕ = 0.3 
 
 
w σϕ = 0.1 
 
 
w σϕ = 0.6 
 
 
 
 
 
μ(S) 
{0} 
0 
0 
0 
0 
{a} 
1 
0 
0 
0.3 
{b} 
1 
0 
0 
0.3 
{c} 
1 
1 
0 
0.4 
{a,b} 
0 
0 
1 
0.6 
{a,c} 
0 
1 
1 
0,7 
{b,c} 
0 
1 
1 
0,7 
{a,b,c} 
1 
1 
1 
1 
 
In Table 3 one of the possible weights assignment and structures is presented, and 
it defines the immanent structure σϕ (as explained in [1,4,6,7]) of the new attributes 
which are used to simulate the aggregation operator of existing fuzzy model [2]. 

24 
M. Mrkalj 
 
The corresponding generalised pseudo-Boolean polynomial is generated by summa-
tion of atomic pseudo-Boolean polynomials included into structure defined by struc-
ture vectors σϕ . This procedure gives the aggregation operator shown in equation (5): 
Agg⊗ (a,b,c) = 3
10
[ a - 2(a ⊗ c) - 2(a ⊗ b) + 4(a ⊗ b ⊗ c) + b - 2(b ⊗ c) + c ] + 1
10
c + 3
5
[ a ⊗ b - 2(a ⊗ b ⊗ c) + a ⊗ c + b ⊗ c ] 
(5) 
On the level of values (arithmetic level), one by one column of matrix of input pa-
rameters R6 from [2] is processed through the operator given in (5) and put into vector 
B6'. For generalised product, dot product is used. 
 
B6' = [0,17   0,285   0,545],   B6' = B6 
If the min function is used as the generalised product, the yield is: 
 
B6' = [0,17   0,285   0,545],   B6' = B6 
As shown above, using the Logical Aggregation operator (5) and generalised prod-
uct (⊗ := *) and (⊗ := min) respectively, the yield is the same as the one in the first 
phase of the existing model presented in [2], and the aim of simulating the model 
from [2] is successfully achieved. 
4 
Implementation of Logical Aggregation in the Second Phase 
of Aggregation of the Existing Fuzzy Model 
The second phase of aggregation in [2] will be modified for the purpose of demon-
stration of two interesting abilities of the LA model.  
• If there is a need to satisfy only one of the two or more quality demands and not all 
of them at the same time 
• If there is a quality demand which should “swap” the relative importance of other 
quality demands 
 
Table 4.  
PRIMARY ATTRIBUTES (A) 
WEIGHT VECTOR OF PRIMARY 
ATTRIBUTES A=[w(ai)] 
Land-saving and outdoor environment (a) 
0,1 
Energy efficient and energy utilization (b) 
0,5 
Water-saving and water utilization (c) 
0,2 
Material-saving and material utilization (d) 
0,1 
Indoor environment quality (e) 
0,05 
Operation management (f) 
0,05 
 
w a
(
)
i

= 1  

 
Demonstrative Implications of a New Logical Aggregation Paradigm 
25 
 
In Table 4 the primary attributes with their respective weights are shown. The 
weight vector is A = [0,1   0,5   0,2   0,1   0,05   0,05].  
If we wish to have satisfied only one of the two quality demands material-saving 
and material utilization and indoor environment quality, and not both of them at the 
same time, this can be achieved by removing single attributes (their respective 
weights become zero), and introducing new complex attribute d∨e with weight wd∨e = 
wd + we = 0,15. 
 
ϕ(d,e) = (d ∪ e) 
(6) 
Generalised Boolean polynomial [1] of this logical expression is given in (7). 
 
ϕ⊗ (d,e) = ((d ∪ e)) ⊗ = d + e - d ⊗ e 
(7) 
If we wish to have a quality demand which can “swap” the relative importance of 
other quality demands, it can be done through a logical function of these attributes. 
For example, if Energy efficient and energy utilization is not highly satisfied, then 
a complex attribute can be created in a way that it gives less importance to it (overall 
quality will be less affected by this unsatisfaction), and more importance to Water-
saving and water utilization, Material-saving and material utilization and Land-
saving and outdoor environment.  
But if Energy efficient and energy utilization is highly satisfied, then the impor-
tance will be brought back to itself (so the overall quality will be more affected by 
Energy efficient and energy utilization attribute), and importance of other attributes is 
reduced. Logical expression of such complex attribute is given in (8). 
 
ϕ (a,b,c,d) = b ∪ (Cb ∩ a ∩ c ∩ d) 
(8) 
Generalised pseudo-Boolean polynomial [1] of this logical expression is given in 
(9). 
 
ϕ⊗ (a,b,c,d) = b + [(1 - b) ⊗ a ⊗ c ⊗ d] 
 
ϕ⊗ (a,b,c,d) = b + a ⊗ c ⊗ d - a ⊗ b ⊗ c ⊗ d 
(9) 
Weight of this complex attribute will be taken from the single attribute Energy effi-
cient and energy utilization. 
 
wb∨ (¬b ∧ a ∧ c ∧ d) = 0,25        wb = 0,5 - 0.25 = 0.25 
After both modifications of the model shown in chapter 4, instead of second phase 
operator from [2], the new operator of Logical Aggregation [3] is given in (10). 
Agg⊗ (a,b,c,d) = a + 1
4
b + 1
5
c + 1
20
f  + 3
20
(d + e - d ⊗ e) + (b + a ⊗ c ⊗ d - a ⊗ 
b ⊗ c ⊗ d) 
 
(10) 
On the arithmetic level, the input parameters matrix R from [2] will be processed 
through operator given in (10). 

26 
M. Mrkalj 
 
 
Agg⊗ (a,b,c,d) :  R → B*  or  Bmin  
R =
L
N
MMMMMMM
O
Q
PPPPPPP
0,16875
0,285
0,54625
0,11
0,215
0,675
0,2025
0,2925
0,505
0,22
0,285
0,495
0,265
0,265
0,47
0,17
0,285
0,545
 
If generalised product is dot product (⊗ := *): 
 
B*  = [0,186553   0,284584   0,641322] 
If generalised product is min function (⊗ := min): 
 
Bmin  = [0,175312   0,269   0,594625] 
Final performance index T from [2] is calculated as a product of vector [1   2   3] 
with transposed vector B, and it is also valid for our modified model. It is shown in 
(11). 
 
T = [1   2   3] * BT (transposed) 
(11) 
 
T
B
=
=
=
L
N
MMM
O
Q
PPP
1
2
3
1
2
3
0,186553
0,284584
0,641322
2
*
*
,6796
*
T
 
T
Bmin
=
=
=
L
N
MMM
O
Q
PPP
1
2
3
1
2
3
0,175312
0,269
0,594625
2
*
*
,4971
T
 
 
Depending on performance index, the building belongs to one of the three categories 
(one, two or three stars) [2]. 
 
T ∈ [1,0  1,7] 
- one star building 
T ∈ [1,7  2,4] 
- two stars building 
T ∈ [2,4  3,0] 
- three stars building. 
 
When modified model presented in chapter 4 of this paper is applied, the building 
from the example given in [2] stays in the interval T ∈ [2,4  3,0], which implies that 
it belongs to the category of three stars building. 

 
Demonstrative Implications of a New Logical Aggregation Paradigm 
27 
 
5 
Conclusion 
Models for performance evaluation of Green buildings enable good quality and objec-
tive evaluation, minimizing the subjectivity bias. Experts in the subject area contri-
buted as the weight coefficients providers, while the final result (performance index) 
is achieved through processing on arithmetic level, through the fixed mathematic 
model. 
Conventional tools for aggregation are often inadequate due to limitations in the 
sense of disability of using logical interactions between quality attributes. Improve-
ments of the model’s “articulation abilities” by using advanced techniques of Logical 
Aggregation significantly expand the possibilities of customization of the model to 
more specific needs. This is especially noticeable in the ability to include complex 
logical functions in which nontrivial attributes emerge. New paradigm treats logical 
functions – partial aggregation demands as a generalised Boolean polynomial, which 
processes values from the unitary real interval [0, 1]. Aggregation in general is a ge-
neralised pseudo-logical function. 
Comparative review of the results of modified improved model presented in this 
paper, and existing fuzzy model [2] is shown, and the advanced abilities of the new 
approach to aggregation of quality parameters and performance of Green buildings 
are demonstrated. 
References 
1. Mirković, M., Hodolič, J., Radojević, D.: Aggregation for Quality Management. Yugoslav 
Journal of Operations Research 16(2), 177–188 (2006) 
2. Sun, J., Wu, Y., Hao, Y., Dai, Z.: Fuzzy Comprehensive Evaluation Model and Influence 
Factors Analysis on Comprehensive Performance of Green Buildings. In: ICEBO 2006, 
Shenzhen, vol. VIII-4-2. China Renewable Energy Resources and a Greener Future (2006) 
3. Radojević, D.: Logical Aggregation Based on Interpolative Realization of Boolean Algebra. 
In: Eusflat Conf., vol. (1), pp. 119–126 (2007) 
4. Radojevic, D.: Fuzzy Set Theory in Boolean Frame. Int. J. of Computers, Communications 
& Control III (2008) 
5. Radojevic, D.: New [0,1]-valued logic: A natural generalization of Boolean logic. Yugoslav 
Journal of Operational Research - YUJOR 10(2) (2000) 
6. Radojevic, D.: Logical Aggregation Based on Interpolative Boolean Algebra. Mathware & 
Soft Computing 15 (2008) 
7. Radojević, D.: Interpolative relations and interpolative preference structures. Yugo-slav 
Journal of Operational Research – YUJOR 15(2) (2005) 
 
 

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 28–36, 2014. 
© Springer International Publishing Switzerland 2014  
Structural Functionality as a Fundamental Property  
of Boolean Algebra and Base for Its Real-Valued 
Realizations 
Dragan G. Radojević 
University of Belgrade, Institute Mihajlo Pupin, Belgrade  
dragan.radojevic@pupin.rs 
Abstract. The value of the complex Boolean function can be calculated directly 
on the basis of its components value. It is a principle known as the truth 
functionality. Properties of the Boolean algebra have indifferent values. The 
truth functional principle is taken as a valid principle in general case in the 
conventional generalization: multi-valued and/or real-valued realizations (fuzzy 
logic in the broad sense). This paper presents that truth functionality is not 
valued indifferent property of the Boolean algebra and it is valid only in two-
valued realization, and thus it cannot be the basic of the value generalization. 
The value generalization (real-valued realizations) enables incomparably more 
descriptiveness than the two-valued classical Boolean algebra, so that the finite 
Boolean algebra is enough for any real application. Each finite Boolean algebra 
is atomic. Every Boolean function (the element of the analyzed finite Boolean 
algebra) can be presented uniquely as disjunction of the relevant atoms – 
disjunctive canonical form. Which atoms are and which are not included in the 
analyzed Boolean function is defined by its structure: 0-1 vector which 
dimension matches the number of atoms (in the case of n independent variables, 
the number of atoms is
n2 ). Atom corresponds uniquely to each vector structure 
position and value 0 means that the adequate atom is not included in the 
analyzed function, and 1 means that it is included. The principle of the 
structural functionality is: the structure of the complex Boolean function is 
defined directly on the basis of its components structure. The truth functionality 
is a value image of the structural functionality only in the case of two-valued 
realization. Each insisting on the truth functionality, such as in the case of 
conventional multi-valued logic and fuzzy logic in general sense,  is unjustified 
from the point of the Boolean consistency. 
Keywords: Boolean algebra, atomic Boolean functions, disjunctive canonical 
form, Boolean function structure, structural functionality, truth functionality, 
generalized value realization of the Boolean functions. 
1 
Introduction 
The truth functionality is a property of the two-valued realization of Boolean algebra. 
The value of complex Boolean function can be calculated directly based on its 

 
Structural Functionality as a Fundamental Property of Boolean Algebra 
29 
 
components values. The principle of the truth functionality is taken as a basis of 
generalization in regular (conventional) value generalization of the theories based on 
the Boolean algebra: multi-valued and/or real-valued logic (fuzzy logic in general 
sense). Practically, there is no explanation why the truth functionality is taken as a 
basis of the valued generalization except ``because it is usual and technically useful`` 
(obviously not mathematical motivation!).  
This paper presents that truth functionality is valid only in two-valued realization 
of the Boolean algebra. Every finite Boolean algebra is atomic. The Boolean algebra 
generated with n independent variables contains 2n atoms. Atoms are the simplest 
elements from the point of the valued realization (in the classical case for the free set 
of 0-1 independent variables values , only one atom has value 1 and all the others 0). 
The important atom property of the analyzed Boolean algebra is the fact that they do 
not have anything in common and/or that conjunction of two different atoms is 
identical to 0 element of the Boolean algebra i.e. it has value realization identical to 0. 
Each Boolean function (element of the analyzed Boolean algebra) can uniquely be 
presented as a union of relevant atoms – disjunctive canonical form. Which atoms are 
and which are not included is defined with 0-1 vector dimension 2n – by the structural  
function. Atom uniquely corresponds to each vector position and value 0 means that 
the appropriate atom is not included in the analyzed function, and 1 means that it is 
included.  
The structure of the complex Boolean function is defined directly on the basis of 
the structure of its components – the principle of the structural functionality. This 
principle is value indifferent – algebra and it must be saved in all Boolean consistent 
value realizations. The truth functionality is a value image of the structural 
functionality and only in the case of two-valued realization – known truth table, for 
example. Each insisting on the truth functionality in general case (multi-valued and/or 
real-valued realization) has as a result impossibility of simultaneous keeping all 
properties of Boolean algebra. So, the answer on the question stated in the paper title 
is that the structural functionality is the basic of the Boolean consistent generalization 
of the finite Boolean algebra valued realization in general case.  
The Boolean consistent generalization enables direct generalization of all theories 
based on the classical finite Boolean algebra. From the point of the Boolean 
consistency,  it is completely unjustified to insist on the truth functionality such as in 
the case of the conventional multi-valued logic and fuzzy logic in general sense.  
2 
Fuzzy Logic in Boolean Frame 
Fuzzy logic, realized in the Boolean frame, means that all Boolean axioms and theo-
rems are valid in the most general case i.e. in the real-valued case. Since the appropri-
ate classical techniques are based on the Boolean algebra, precisely, on its two-valued 
realization, the consistent generalization should be based on the real-valued realiza-
tion of the Boolean algebra. The real-valued realization of the finite or atomic Boo-
lean algebra [4, 5 and 6] is described here in detail.  
The main problem of the conventional approaches (the main stream of the usual 
realization) is fact that they are based on the principle of the truth functionality, which 

30 
D.G. Radojević 
 
is taken from the classical logic based on the two-valued realization. From the point 
of the Boolean algebra, this principle is adequate or correct only in two-valued case. 
The reason is simple: the Boolean function has the vector nature in general case, but 
in the classical case the attention is drawn only to one component (which is defined 
with 0-1 values of the independent variables). In general case, when the values of the 
independent variables are not only 0 or 1 but also include everything in between, it is 
necessary to include more components in calculation and in the most general case all 
components of the vector immanent to the analyzed Boolean function in the analyzed 
Boolean algebra. In order to illustrate the main idea, we will use the Boolean function 
of two independent variables x and y, from the famous Boolean paper from 1848 [2]: 
 
(
)
(
)
(
) (
)
(
)(
)
(
)(
)(
)
,
1,1
1,0
1
0,1 1
0,0 1
1
x y
xy
x
y
x y
x
y
φ
= φ
+ φ
−
+ φ
−
+ φ
−
−
. (1) 
 
 
 
   
    Actually, this equation can be treated also as a special case of the Boolean poly-
nomial [4]: 
(
)
(
)
(
)(
)
(
)(
)
(
)(
)
,
1,1
1,0
0,1
0,0 1
.
x y
x
y
x
x
y
y
x
y
x
y
x
y
⊗
φ
= φ
⊗
+ φ
−
⊗
+
φ
−
⊗
+ φ
−
−
+
⊗
 (2) 
    The independent variables in general case take the value from the unit real inter-
val
[
]
,
0,1
x y ∈
. 
⊗ is generalized product [4] or   t-norm with the following property: 
 
   
(
)
(
)
max
1,0
min
,
.
x
y
x
y
x y
+
−
≤
⊗
≤
  
2.1 
Boolean Polynomial 
The real-valued realization of the finite (atomic) Boolean algebra is based on the Boo-
lean polynomials. The free Boolean function can be uniquely transformed into the 
appropriate Boolean [6].   
 
Example 1: Using the equation (2) of the equivalence relation, exclusive disjunction 
and implication, respectively 
 
(
)
(
)
(
)
(
)
(
)
11
1
1 0
0
0 1
0
0 0
1
1
2
def
a.
x,y
x
y
,
;
,
;
,
;
,
;
x
y
x
y
x
y .
φ
=
⇔
φ
=
φ
=
φ
=
φ
=
⇔
= −
−
+
⊗
  

 
Structural Functionality as a Fundamental Property of Boolean Algebra 
31 
 
(
)
(
)
(
)
(
)
(
)
11
0
1 0
1
0 1
1
0 0
0
2
def
b.
x,y
x
y
,
;
,
;
,
;
,
;
x
y
x
y
x
y .
φ
=
∨
φ
=
φ
=
φ
=
φ
=
∨
=
+
−
⊗
 
 
(
)
(
)
(
)
(
)
(
)
11
1
1 0
0
0 1
1
0 0
1
1
def
c.
x,y
x
y
,
;
,
;
,
;
,
;
x
y
x
x
y .
φ
=

φ
=
φ
=
φ
=
φ
=

= −
+
⊗
 
 
The finite (atomic) Boolean algebra generated by the set of independent va-
riables
{
}
1,...,
,
Ω =
n
x
x
 is
( )
( )
(
),
ΒΑ Ω = Ρ Ρ Ω
 where:  
(
)
Ρ Ω  is a set of all sub-
sets Ω . The atomic elements of the analyzed Boolean algebra 
(
)
ΒΑ Ω are [6]: 
 
( )(
)
( )
1
\
,...,
,
.
i
j
n
i
j
x
S
x
S
S
x
x
x
x
S
∈
∈Ω
α
=
∈Ρ Ω
∧
∧
                   (3) 
                                    
The atomic Boolean polynomial 
( )(
)
1,...,
⊗
α
n
S
x
x
 uniquely corresponds to the 
atomic element  
( )(
)
1,...,
,
α
n
S
x
x
 and it is defined by the following equation [6]: 
 
( )(
)
(
)
(
)
( )
1
\
,...,
1
;
.
i
C
n
i
x
C
S
C
S
S
x
x
x
S
⊗
∈∪
∈Ρ Ω
α
=
−
∈Ρ Ω

⊗
           (4) 
                               
Example 2:  The atomic Boolean polynomials for the Boolean algebra generated with 
{
}
Ω = x, y are: 
 
    
{
}
(
)
{ }
(
)
{ }
(
)
{ }
(
)
1
x,y
x
y;
x
x
x
y;
y
y
x
y;
x
y
x
y.
⊗
⊗
⊗
⊗
α
=
⊗
α
=
−
⊗
α
=
−
⊗
α
= −
−
+
⊗
 
 
The values of the atomic polynomials in the real-valued case are nega-
tive
( )(
) [
]
1
0 1
n
S
x ,...,x
,
⊗
α
∈
 
( )
S ∈Ρ Ω , and their sum is identically equal to 1. In 
the case of the example described by the equation (2) for
[
]
0 1
∈
x, y
,
:  

32 
D.G. Radojević 
 
      
(
) (
) (
)
1
1
x
y
x
x
y
y
x
y
x
y
x
y
⊗
+
−
⊗
+
−
⊗
+
−
−
+
⊗
≡. 
 
The classical two-valued case is just a special case which satisfies this fundamental 
equity, since the value of only one atom is equal to 1 and all others are identically 
equal to 0.   
The free Boolean function, the appropriate element of the analyzed Boolean alge-
bra (
)
( )
1,...,
,
n
x
x
φ
∈ΒΑ Ω
 can uniquely be presented in disjunctive canonical form 
as a disjunction of relative atomic elements:  
 
(
)
( )
( ) ( )(
)
1
1
,...,
,...,
n
n
S
x
x
S
S
x
x
φ
∈Ρ Ω
φ
=
σ
α
∨
.                         (5) 
                                          
Where:
( )
( )
(
)
,
φ
σ
∈Ρ Ω
S
S
 the relation of inclusion of corresponding 
atom ( )(
)
1,...,
n
S
x
x
α
 in the analyzed Boolean function (
)
1,...,
,
n
x
x
φ
 is defined in 
the following way:  
 
( )
( )
(
)
( )
( )
(
)
1,...,
,
1,
,
.
0,
def
S
i
i
S
i
def
i
S
x
i
n
x
S
x
S
x
S
φ
σ
=
φ χ
=


∈

χ
=
∈Ρ Ω



∉



                       (6) 
 
The relation of inclusion determines which atoms 
( )
(
)
∈Ρ Ω
S
 are included in the 
analyzed Boolean function: 
  
( )
( )(
)
(
)
( )(
)
(
)
1
1
1
1
1,
,...,
,...,
,
0,
,...,
,...,
n
n
n
n
S
x
x
x
x
S
S
x
x
x
x
φ

α
⊂φ

σ
= 
α
⊄φ

                   (6.1) 
                        
(The values of the corresponding relation of inclusion are equal to 1) and which are 
not included (the values of the relation of inclusion are equal to 0). 
The Boolean polynomial uniquely corresponds to the analyzed Boolean function as 
Figure (5):  
 
(
)
( )
( )(
)
( )
1
1
,...,
,...,
.
n
n
S
x
x
S
S
x
x
⊗
⊗
φ
∈Ρ Ω
φ
=
σ
α

                       (7) 
                  
The Boolean polynomial (7) can be presented as a scalar product for two vectors:  
 
(
)
(
)
[
]
1
1
1
,...,
,...,
,
,...,
0,1
n
n
n
x
x
x
x
x
x
⊗
⊗
φ
φ
= σ α
∈


                 (8) 

 
Structural Functionality as a Fundamental Property of Boolean Algebra 
33 
 
( )
( )
S S
φ
φ


σ = σ
∈Ρ Ω



 is a structure of the analyzed Boolean func-
tion (
)
(
)
1,...,
φ
∈ΒΑ Ω
n
x
x
, i.e. a vector of the relation of inclusion of atomic func-
tions in the analyzed function. 
 
 
(
)
( )(
)
( )
1
1
,...,
,...,
T
n
n
x
x
S
x
x
S
⊗


α
= α
∈Ρ Ω



is a vector of atomic polynomials 
of the analyzed finite (atomic) Boolean algebra 
(
).
ΒΑ Ω
 
 
Example 2: Structures of the analyzed Boolean functions from the example 1:   
 
   
[
]
[
]
[
]
1 0 0 1 ,
0 1 1 0 ,
1 0 1 1 .
x
y
x
y
x
y
⇔
∨

σ
=
σ
=
σ
=



 
 
and atomic polynomials vectors for two independent variables:  
 
   
(
)
1
2
1
1
2
1
2
2
1
2
1
2
1
2
,
1
x
x
x
x
x
x x
x
x
x
x
x
x
x
⊗
⊗




−
⊗


α
= 

−
⊗


−
−
+
⊗



. 
3 
The Structural Functionality  
The principle of the structural functionality: The structure of the free complex Boo-
lean function can be calculated directly on the basis of its components using the fol-
lowing identities:  
 
   
φ∧ψ
φ
ψ
φ∨ψ
φ
ψ
σ
= σ ∧σ
σ
= σ ∨σ






 
  
1
.
¬φ
φ
φ
σ
= ¬σ =
−σ




 
 
The known principle of the truth functionality is just an image of the structural 
functionality at the value level and thus only in the case of two-valued realization. In 
general case (multi-valued and/or real-valued realizations), the principle of the truth 
functionality is not able to keep all Boolean algebra properties. That is the reason why 
fuzzy approaches, based on the principle of the truth functionality (conventional fuzzy 
logic in wider sense), cannot be in the Boolean frame and/or they are not the Boolean 
consistent generalizations. 
    Structures as algebra properties of the Boolean functions keep all Boolean laws [6]:  
 

34 
D.G. Radojević 
 
3.1 
Laws of Monotonicity 
Associativity 
  
(
) (
)
(
) (
)
,
.
φ
ψ
ξ
φ
ψ
ξ
φ
ψ
ξ
φ
ψ
ξ
σ ∨σ ∨σ
= σ ∨σ
∨σ
σ ∧σ ∧σ
= σ ∧σ
∧σ












                                (9) 
Commutatively 
 
,
;
,
.
φ∨ψ
ψ∨φ
φ
ψ
ψ
φ
φ∧ψ
ψ∧φ
φ
ψ
ψ
φ
σ
= σ
σ ∨σ
= σ ∨σ
σ
= σ
σ ∧σ
= σ ∧σ












                       (10) 
Distributive 
   
(
) (
) (
)
(
) (
) (
)
,
.
φ
ψ
ξ
φ
ψ
φ
ξ
φ
ψ
ξ
φ
ψ
φ
ξ
σ ∧σ ∨σ
= σ ∧σ
∨σ ∧σ
σ ∨σ ∧σ
= σ ∨σ
∧σ ∨σ














                    (11) 
Identity 
 
0
,
1
1;
0
0,
1
.
φ
φ
φ
φ
φ
φ
σ ∨
= σ
σ ∨
=
σ ∧
=
σ ∧
= σ












                                 (12) 
Idempotent 
 
,
.
φ
φ
φ
φ
φ
φ
σ ∨σ = σ
σ ∧σ = σ






                             (13) 
   
Absorption 
 
(
)
(
)
,
.
φ
φ
ξ
φ
φ
φ
ξ
φ
σ ∧σ ∨σ
= σ
σ ∨σ ∧σ
= σ








             (14) 
3.2 
No Monotonicity Laws 
Complementarity 
 
0,
1.
φ
¬φ
φ
¬φ
σ ∧σ
=
σ ∨σ
=






                                  (15) 
De Morgan Laws 
 
(
)
(
)
,
.
φ
ψ
¬φ
¬ψ
φ
ψ
¬φ
¬ψ
¬ σ ∧σ
= σ
∨σ
¬ σ ∨σ
= σ
∧σ








           (16) 

 
Structural Functionality as a Fundamental Property of Boolean Algebra 
35 
 
4 
Conclusion 
This approach based on the application of the structural functionality keeps all Boo-
lean algebra laws in all possible value realizations (from the classical two-valued to 
the most general real-valued realization) independently from the selected operators of 
the generalized product. 
If you interpret the general case concretely (real-valued realization), then even the 
special case (two-valued realization) is treated in a different way. 
Excluded middle and non-contradiction are defined for two-valued realization case 
(in logic, for example, the statement is either truth or untruth and it cannot be both 
truth and untruth) and thus they are not adequate for general case. It seems that in the 
case of the conventional approaches to the generalization, the excluded middle may 
even not be valid. In general case one statement can be partially truth but then it is 
untruth with complementary intensity. From the point of the structure, the comple-
mentary function corresponds to each Boolean function so that it contains all atoms 
which the analyzed function does not contain and whereat they do not have a single 
common atom. The consequence is that the disjunction of the free Boolean function 
and its complementary functions contain all atoms and /or it is identical to the Boo-
lean constant 1, i.e. it has a value identically equal to 1 – the excluded middle. Also, 
its conjunction does not include a single atom and/or it is identical to the Boolean 
constant 0, i.e. it has a value identically equal to 0 – non-contradiction. However, 
these fundamental laws are valid in all consistent value realizations and known defini-
tions are valid only in classical two-valued case.  
Actually, excluded middle and non-contradiction uniquely define the complemen-
tary property of the analyzed property and thus these two laws are fundamental and 
important for cognition in general.  
This can be illustrated in a simple example, such as a glass filled with water. The 
classical two-valued case treats only full or empty glass. Empty is complement of full 
and vice versa. In general or real case, the glass can be partially full and, at the same 
time, with the rest it is empty with complementary intensity, so that the sum of the 
intensity “full” and intensity “empty” is identically equal to 1.  
 It is obvious that, besides the fact that properties “full” and “empty” do not have 
anything in common, they are both simultaneously in the same glass and thus  the 
union intensity is equal to 1 and intersection intensity to 0.  
 The free classical theory, based on the finite Boolean algebra using the real-valued 
realization of the Boolean algebra, can be generalized directly [6]. This is very impor-
tant for many interesting examples which are logically complex such as: artificial 
intelligence, mathematical cognition, prototype theory in psychology, concept theory, 
etc. 
The structural functionality sheds a new light on the relationship between syntax 
and semantics in the classical logic which will be presented in the next paper. 
 

36 
D.G. Radojević 
 
References 
1. Zadeh, L.A.: From Circuit Theory to System Theory. In: Proc. of Institute of Ratio Engi-
neering, vol. 50, pp. 856–865 (1962) 
2. Boole, G.: The Calculus of Logic. The Cambridge and Dublin Mathematical Journal 3, 
183–198 (1848) 
3. Radojevic, D.: New [0,1]-valued logic: A natural generalization of Boolean logic. Yugoslav 
Journal of Operational Research - YUJOR 10(2), 185–216 (2000) 
4. Radojevic, D.: There is Enough Room for Zadeh’s Ideas, Besides Aristotle’s in a Boolean 
Frame. In: 2nd International Workshop on Soft Computing Applications, SOFA 2007 
(2007) 
5. Radojevic, D.: Interpolative Realization of Boolean algebra as a Consistent Frame for Gra-
dation and/or Fuzziness. In: Nikravesh, M., Kacprzyk, J., Zadeh, L.A. (eds.) Forging New 
Frontiers: Fuzzy Pioneers II. STUDFUZZ, vol. 218, pp. 295–317. Springer, Heidelberg 
(2008) 
6. Radojevic, D.: Real-valued realization of Boolean algebra is natural frame for consistent 
fuzzy logic. In: Seising, R., Trillas, E., Moraga, C., Termini, S. (eds.) On Fuzziness, A Ho-
mage to Lotfi Zadeh. STUDFUZZ, vol. 2, pp. 559–566. Springer, Heidelberg (2013) 
 

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 37–45, 2014. 
© Springer International Publishing Switzerland 2014  
Fuzzy Concepts in Small Worlds and the Identification  
of Leaders in Social Networks* 
Trinidad Casasús-Estellés1 and Ronald R. Yager2 
1 Faculty of Economics-IEI, University of Valencia, Spain 
casasus@uv.es 
2 Machine Intelligence Institute, Iona College, New Rochelle, NY 10801 USA 
yager@panix.com 
Abstract. In the study of the Social Networks, the Small World phenomenon 
appears frequently. We apply some techniques of graph theory and fuzzy sets to 
characterize the Small World features as well as the existence of the figure of 
leader in Social Networks. These techniques help to the conceptual formalization 
in relational networks analysis, by transforming linguistic and human-focused 
manner concepts related to social networks in some formal representation. These 
techniques are also applied when the similarity among nodes wants to be meas-
ured in order to study the current homophily present in a Network.   
Keywords: Fuzzy Sets, Small Worlds, Social Network, Leader. 
1 
Introduction 
Social Networks spread through almost all the aspects of our economic and social 
lives and they have been widely studied from several approaches given their presence 
in a huge range of fields [1]. The study of Random Graphs has proved that these 
graphs can exhibit some of the features of observed social networks but they lack 
some others [1]. Some of these social networks present a feature of “high local clus-
tering and short global separation” [2], known in the literature as the small world 
phenomenon. In this sense Watts & Strogatz [3] developed a variation of a random 
network in order to generate networks that simultaneously exhibit high clustering and 
low diameter, a combination which they called a small-world network referring to the 
Stanley Milgram’s “Small Worlds” experiment [4]: Many social networks tend to 
have small diameter and small average path length, where small is on the order of the 
log of the number of nodes or less.  
In [5] the natural connection between graph theory and granular computing, partic-
ularly fuzzy set theory is pointed out, and fuzzy sets are used to formalize some con-
cepts associated with social networks. In [6] the idea of fuzzy relationships and their 
role in modeling weighted social relational networks as well as the idea of vector-
valued nodes are discussed.  
                                                           
* This work has been partially supported by the project TIN2008-06872-C04-02. 

38 
T. Casasús-Estellés and R.R. Yager 
 
Following this reasoning, we are going to use fuzzy set methodology to define 
Small Worlds as a part of the structure of Social Networks. Let’s start by introducing 
some notation we will use in the following chapters. (See [5] and [7] for more nota-
tion and definitions).  
1.1 
Some Definitions and Notation 
Let G = V, E be a graph, where V is the set of vertices and E is the collection of 
edges.  We consider associated with G a relationship R: VxV →{0,1}, such that R(x, 
y) = 1 = R(y, x) when (x, y) is an edge of G, and R(x, y) = 0 = R(y, x) if (x, y) is not 
an edge of G. By the term graph we will understand undirected graph, this is, the 
graph with the associated relationship R being symmetric R(x, y) = R(y, x). 
 
Definition 1. Let define the neighbor of a node x, Neighx(y) = R(x, y), the Neighbor-
hood of x as Nx = {y / Neighx (y) = 1}, and the size of Neighborhood of x,  
dx = Card(Nx). 
Given two vertices x and y of V, we will understand by an x-y path in G a se-
quence of distinct vertices {x1, x2,…, xn} beginning with x, x1 = x, and ending with y, 
xn = y, so that there is an edge between any two adjacent vertices in the sequence. We 
will say that two nodes are connected if it is possible to find a path from one node to 
the other. A subset S ⊆ V is completely connected if each pair of nodes belonging to S 
are connected. 
The length of a path is defined as the number of edges the path contains. A cycle is 
an x-y path in which x = y and it must contain at least three distinct vertices. We refer 
to a shortest path as a geodesic and denote the shortest path between x and y as Geo 
(x, y). If Len (p) indicates the length of a path then the distance (x, y) = Len (Geo (x, 
y)). Given two relationships R1 and R2, over the same vertex set V, their composition, 
over the set V, is defined as 
R1* R2(x, z) = Maxy∈V[Min(R1(x, y), R2(y, z))].                 (1) 
It will be denote by Rk = R*R*…*R, k-times, and the distance (x, y) can be unders-
tood as the smallest k for which Rk(x, y) = 1. 
Given G = V, E an undirected graph representing a social network, with R the as-
sociated relationship, a subset S ⊆ V is called a cluster of order k ([5]) if the follow-
ing conditions hold: 
a. for all x, y ∈ S,  Len(Geo(x, y)) ≤ k 
b. for all z ∉ S, Len(Geo(x, z)) > k, for some x ∈S. 
S is a clique in the special case when S is a cluster of order k =1, that is, a maximal 
completely connected sub-network of a given network. 

 
Fuzzy Concepts in Small Worlds and the Identification of Leaders 
39 
 
2 
Small Worlds 
To define the Small-World behavior in a Network G = V, E, V={xi}, following [3], 
two concepts are considered, the Characteristic Path Length L, a global property of 
the graph, and the Clustering Coefficient C, a local property that quantifies the neigh-
borhood ‘cliquishness’. In a small world the coincidence of high local clustering and 
short global separation are given. Thus, the Small World (SW from now on) behavior 
is characterized by [8]:  
A. Low Characteristic Path Length, where the characteristic path length L (average 
connectivity of the network), as defined by [3], is the average distance between all 
pair of vertices, given by the formula 
ܮൌ
ଵ
௡ሺ௡ିଵሻ∑
݀௜௝
௜ஷ௝
                                  (2) 
being dij the distance between the nodes xi and xj. It is a global property of the graph 
that describes the average number of the shortest path inside the network. In case of 
not-fully connected network, there exist another ways to give a measure of the aver-
age path length of the network (See for instance [9])  
B. High Clustering Coefficient, it is the proportion of pairs of neighbors of a node 
that are also neighbors of each other; it quantifies how close neighbors are to being a 
clique. Given the node xi, the clustering coefficient of xi, Ci, denotes the fraction of 
the links actually present among its neighbors 
ܥ௜ൌ
݌ܽ݅ݎݏ ݋݂ ݄ܾ݊݁݅݃݋ݎݏ ݋݂ ݔ௜ ܿ݋݊݊݁ܿݐ݁݀
݌݋ݏݏܾ݈݅݁ ܿ݋݊݊݁ܿݐ݅݋݊ݏ ܾ݁ݐݓ݁݁݊ ݄ܾ݊݁݅݃݋ݎݏ 
ܥ௜ൌ 
ଶ
ௗ೔ሺௗ೔ିଵሻ∑
∑
݄ܰ݁݅݃௜ሺݔ௞ሻ݄ܰ݁݅݃௜ሺݔ௝ሻ݄ܰ݁݅݃௝ሺݔ௞ሻ
௡
௝ୀଵ,௝ஷ௜
௡
௞ୀ௝ାଵ
,            (3) 
where di = Card(Ni) was defined in Paragraph 1.1., the Neighborhood size of node xi. 
The Clustering Coefficient of whole network is given by  
ܥҧ ൌ
ଵ
௡∑
ܥ௜
௡
௜ୀଵ
                                   (4) 
Roughly speaking, a small-world graph with n vertex is a large, sparsely con-
nected, decentralized graph, which exhibits a characteristic path length close to that of 
an equivalent random graph, yet with a clustering coefficient much greater.  
In the spirit of human perception, we could say that “small” in the environment of 
Social Networks means that almost every element of the network is somehow “close” 
to almost every other element, even those that are perceived as likely to be far away. 
The Small World implies the claim that even when two people do not have a friend in 
common, only a short chain of intermediaries separates them [5] & [6]). In the next 
paragraph we are giving some soft definitions by using fuzzy tools, following some 
techniques introduced by those authors. This method will help us to approach some 
ideas, as for instance those of “small” and “close”.  

40 
T. Casasús-Estellés and R.R. Yager 
 
2.1 
Soft Clustering Coefficient  
We can give the softer fuzzy definition of the Clustering Coefficient as the proportion 
of pairs of close neighbors of a node that are close neighbors of each other. 
 
 
 
Fig. 1. Prototypical definition of Close 
Let Q(m) be the degree m links satisfies the idea of “close”, Q: Z+ → [0,1] like in 
Figure 1. One way to define the degree to which node xj is close neighbor of xi can be 
given by 
݄ܳܰ݁݅݃௜ሺݔ௝ሻൌܯܽݔ௠ሾܳሺ݉ሻר ܴ௠ሺݔ௜, ݔ௝ሻሿ 
Then, the Soft Clustering Coefficient of Node xi can be defined as  
ܵܥ௜ൌ
∑
∑
ொே௘௜௚௛೔ሺ௫ೖሻொே௘௜௚௛೔ሺ௫ೕሻொே௘௜௚௛ೕሺ௫ೖሻ
೙
ೕసభ,೔ಯೕ
೙
ೖసೕశభ
∑
∑
ொே௘௜௚௛೔ሺ௫ೖሻொே௘௜௚௛೔ሺ௫ೕሻ
೙
ೕసభ,೔ಯೕ
೙
ೖసೕశభ
                 (5) 
2.2 
Soft Characteristic Path Length 
In a similar way to the previous Paragraph, it is possible to give a definition of Soft 
Path Length SL. In communal vocabulary a short path is expressed as a fuzzy subset 
S defined on the set of positive integers. Here S must be such that S(1) = 1 and S(l1) ≥ 
S(l2), when l1 < l2, where S(k) denotes the degree k links is short distance, similar to 
that of close defined in Paragraph 2.1. 
 
 
Fig. 2. Prototypical definition of Short Path 
The satisfaction of the criteria “there is a Short Distance from xi to xj” is given by  
ܵܲ௜ሺݔ௝ሻൌܯܽݔ௞ሾܵሺ݇ሻר ܴ௞ሺݔ௜, ݔ௝ሻሿ 
 
&/26(


P
6KRUWSDWK6
OO

 
Fuzzy Concepts in Small Worlds and the Identification of Leaders 
41 
 
Then, we could define the Soft Characteristic Path Length as 
ܵܮൌ
ଵ
௡ሺ௡ିଵሻ∑
ܵܲ௜ሺݔ௝
௜ஷ௝
ሻ ∈ [0, 1]                     (6) 
And SL will give us a value that reflects how “short” is the average length of our 
network according to our criteria of Short Path S. 
Note that given S(k) is decreasing with respect to k, we see SPi(xj) will be equal to 
S(k) for the first k for which there is a path from xi to xj. If we denote this kj then 
SPi(xj) = Si(kj) ∈ [0,1] 
3 
Leadership Inside the Network 
To study the position of a given node inside the network is an interesting question that 
can be formulated as the analysis of the connections of that node in the overall net-
work. [1] writes about many different measures of centrality that tend to capture  
different aspects of the position that a node has, been one of them the Betweenness 
introduced by [10], together with the Degree – how connected a node is, the Close-
ness – how easily a node can reach other nodes, and the Neighbors’ Characteristics  
C- how important, central, or influential a node’s neighbors are. Newman [11] , refers 
to the Betweenness of a node x, as the total number of shortest paths between pairs of 
nodes that pass through x. This quantity would be an indicator of the most influential 
people in a network, the one who control the flow of information between the others. 
Following with our proposal, let us define in a human-meaningful way what we 
will understand by leader of a SW and provide a procedure for evaluating when a 
node satisfies our definition. To do this, we will take into account the number of 
edges arriving to a node (idea of congestion and centrality, [5]) as well as the  
betweenness of it. 
 
Definition 2. A node x of G = V, E will be considered a leader inside a small world 
of a social network if the following conditions are given: 
1.  There is a high congestion on this node. 
2.  Most of the elements, in the SW x belongs to, are connected with it. (Most of the 
nodes close to x are connected directly to it) 
3.  High betweenness of the node x. 
4. The node x is connected with most of other nodes that verify the same conditions 
1, 2 and 3. 
Let’s go in depth in each point of the above criteria: 
1. We shall say that a node is a congested node if it has many incident nodes, that 
is, it has a high density. Given the set of nodes V, it is possible to define a fuzzy set C 
over the set V of nodes in the network such that C(x) ∈ [0,1] is going to indicate the 
degree to which x is a congested node. 
Following [5], we consider MANY as a fuzzy set over the set N = {1,2,…, n}, with 
n the number of nodes in the network G, such that for each y ∈ N the value 

42 
T. Casasús-Estellés and R.R. Yager 
 
MANY(y) ∈ [0,1], MANY: N → [0,1]  a monotonically increasing function of y that 
indicates the degree to which the quantity y satisfies the concept many.  
The degree to which a node xi is a congested node is defined by 
ܥሺݔ௜ሻൌܯܣܻܰሺ∑
ܴሺݔ௜, ݔ௝ሻ
௡
௝ୀଵ,௜ஷ௝
) 
 
 
Fig. 3. Prototypical definition of MANY 
Other definitions of MANY (see [5]) may be considered, as for example, in terms 
of proportion: 
ܥܩሺݔ௜ሻൌܯܣܻܰ௣ሺ
∑
ோ൫௫೔,௫ೕ൯
೙
ೕసభ,೔ಯೕ
ே
ሻ                       (7) 
where by MANYp(y) ∈ [0,1] indicates the degree to what y (as proportion) satisfies 
the concept many. 
2. For most nodes of the small world x belongs to, the path from x to each one of 
them is short and highly centered (relatively inside the small world): 
The idea of Most is given, as in point 1, by a fuzzy subset Q: [0,1] → [0,1], where 
for any proportion p the value Q(p) indicates the degree to it satisfies the concept 
Most, Q(p) ∈ [0,1] is a monotonically increasing function of p, such that Q(0) = 0, 
Q(1) = 1.  
We have seen in Paragraph 2.2. the definition of Soft Characteristic Path between 
xi and xj as 
ܵܲ௜ሺݔ௝ሻൌܯܽݔ௞ሾܵሺ݇ሻר ܴ௞ሺݔ௜, ݔ௝ሻሿ = Si(kj) 
To check that x satisfies Condition 2, one way would be (see [5] for other alterna-
tive procedures, using OWA operators): 
We first order the set of values of SPx(xj), the degree to which there is a short path 
from x to xj, x≠xj, and let dk be the value of the kth largest of the SPx(xj). Using this 
and the fuzzy subset Q representing our concept of Most, we calculate 
ܥܶሺxሻൌܯܽݔ௞ୀଵ…௡ିଵሾܳሺ
௞
௡ିଵሻר ݀௞ሿ                      (8) 
which will give us the degree of centrality of the considered node x. 
3. High betweenness of the node x would mean that the leader x lies on most of the 
Soft Short paths between pairs of nodes being in the small world considered. 
Given two nodes x and xj, if we consider the soft short path between them defined 
previously, SPx(xj), the degree to the node x satisfies the Soft Betweenness can be 
calculated as follows: 
 
0$1<

 
Fuzzy Concepts in Small Worlds and the Identification of Leaders 
43 
 
Let’s denote by B the set of paths z-y such that x is one of the nodes included in the 
path, and let D = Card (B) be. If we consider the corresponding set of values Soft 
Short Path {SPz(y)}z-y∈B we can assume, as before, these paths ordered being dk the 
value of the kth largest of the set. Then, the Soft Betweenness of x can be defined as: 
ܵܤሺxሻൌܯܽݔ௞ୀଵ…௡ିଵሾܳሺ
௞
஽ሻר ݀௞ሿ                    (9) 
and this value will give us a criteria about the betweenness of node x. 
The last condition we consider for leaders has to be with the relationship among 
nodes with similar characteristics. Are there any correlation patterns in the degrees of 
connected nodes? (see [1] and [9]). For instance, do relatively high degree nodes have 
a higher tendency to be connected to other high degree nodes? This is termed as Posi-
tive Assortativity. While there is little systematic study of assortativity, there is a 
hypothesis that positive assortativity is a property of many socially generated net-
works, and it contrasts with the opposite relationship that is more prevalent in tech-
nological and biological networks. In our concept of leader we can include one more 
criteria related to the connectivity among the leaders of SW in a given network: 
4. Most of the leaders are connected each other (there exists positive assortativity 
among them). Most of x ∈ V that verify simultaneously conditions 1, 2 and 3 are 
connected each other. Let L = {yi}r
i=1 be the set of nodes that verifies conditions 1, 2, 
and 3. As we have seen in the Condition 2, given that the Soft Short Path between yi 
and yj is obtained from 
ܵܲ௜ሺݕ௝ሻൌܯܽݔ௞ሾܵሺ݇ሻר ܴ௞ሺݕ௜, ݕ௝ሻሿ 
we proceed in a similar way for a given yi, and we order the set of SPi(yj), j ≠ i, such 
that let dk be the value of the kth largest of the SPi(yj). Using this and the fuzzy subset 
Q representing our concept of Most, we calculate 
ܲܣሺݕ௜ሻൌܯܽݔ௞ୀଵ…௡ିଵሾܳሺ
݇
݊െ1ሻר ݀௞ሿ 
(also in this point it is possible to consider other alternative definitions, see [5]). This 
value will give us the degree of positive assortatitivity corresponding to the given 
node yi with the others nodes considered as leaders of small worlds. 
Once these criteria have been valuated for some nodes, one way to analyze the degree 
of leadership of a given node x would be, for instance, to calculate the minimum  
value  
SL(x) = min[CG(x), CT(x), SB(x), PA(x)] 
4 
About Homophily 
A feature observed frequently in many social networks, that we have considered in 
Condition 4 of Paragraph 3, is that many leaders use to be connected each other. The 
tendency of individuals to associate and bond with ‘similar’ others is known as Ho-
mophily [12]. In our study, the homophily may be understood as the tendency of 

44 
T. Casasús-Estellés and R.R. Yager 
 
nodes to be attached to other nodes with similar characteristics. But, if we speak about 
similarity among nodes, this is because some features do exist on each node suscepti-
ble to be compared.  
We keep on working with the fuzzy set based technologies to introduce and study 
the similarity among the nodes of a social network and the presence of homophily:  
Let’s consider a Network G = V,G and let’s assume attached to each node vi ∈ V 
a vector of attributes that take their values in the space (see [6]) U = U1xU2x…xUq,     
such that vi(u1,u2,… uq) := (u1i,u2i,… uqi) ∈ U where each uri ∈Ur is the value of the 
rth attribute for the node vi. If we are able to compare the attributes of the nodes, it is 
because some kind of ‘metric’ is defined on the set of these attributes. We are going 
to consider each attribute belonging to the Subset Ur, uri ∈ Ur, a set where we have 
defined a metric (Ur,||.||r)1 (to be defined in each case). 
 
Definition 3. We define the Soft Similarity on G = V,G by means of the following 
criteria: 
Given two nodes, vi , vj  ∈ V, with attributes vi(u1,u2,… uq) := (u1i,u2i,… uqi) ∈ U and 
vj(u1,u2,… uq) := (u1j,u2j,… uqj) ∈ U, the similarity of vi to vj  is calculated as 
SSIM୧ሺv୨ሻൌSIMሺሺuଵ୧, uଶ୧, …  u୯୧ሻ, ሺuଵ୨, uଶ୨, …  u୯୨ሻሻൌܵܫܯ൬
∑
צ௨ೝ೔ି௨ೝೕצೝ
೜
ೝసభ
௤ெ஺௑೔,ೕ,ೝצ௨ೝ೔ି௨ೝೕצೝ ൰  (9) 
SSIMi(vj) ∈ [0,1], where by SIM we understand a decreasing function defined (as 
seen in previous concepts) 
SIM: [0,1] →[0,1]  such that     SIM(0)=1      and SIM(1)=0.   
 
 
Fig. 4. Prototypical definition of Similar 
In the study of the small worlds, it would be interesting to analyze the similarity 
among the members of a given small world S⊆V. 
We can speak about Soft Homophily in a given subset of a network when most of 
the elements of the subset are similar in the sense given above, this is, we can calcu-
late the degree to which vi is similar to most of the nodes vj in the subset. We are 
going to follow a similar technique to that seen previously, but we may also follow a 
way based on OWA operators (see [5] and [6]). 
                                                           
1 It would be also possible to consider a soft similarity defined on each Ur and proceed later in 
a similar way to define the Global Soft Similarity among the nodes. 
6LPLODULW\
l1 l2

 
Fuzzy Concepts in Small Worlds and the Identification of Leaders 
45 
 
We order the values SIMi (vj) and let dk be the value of the kth largest of the SI-
Mi(vj). Using this and the fuzzy subset Q representing our concept of Most, we  
calculate 
ܪ௜ൌܪሺݒ௜ሻൌܯܽݔ௞ୀଵ…௡ିଵሾܳሺ
௞
௡ିଵሻר ݀௞ሿ, for each node vi 
where the idea of Most is given by a fuzzy subset Q: [0,1] → [0,1] as previously.  
A measure of the Soft Homophily of the world S will be given by  
ܪሺܵሻൌܯܽݔ௜ୀଵ…௡൤ܳሺ݅
݊ሻר ܪ௜൨ 
We must stress that we have considered undirected edges. Obviously, if we con-
sider the above metrics verifying the symmetric property, it will happen that SSIMi 
(vj) = SSIMj(vi). 
5 
Conclusions 
We follow the ideas started in [6] and [5] to formalize concepts related to the Small 
Worlds in Social Networks, by using Fuzzy techniques and the theory of graphs. We 
give also a proposal for the identification of a node as a leader in the Small World and 
to identify some common features among the elements belonging to a given Social 
Network. The fuzzy techniques are useful to represent linguistic concepts related to 
the theory of Social Networks. 
References 
1. Jackson, M.: Social and Economic Networks. Princeton University Press (2008) 
2. Watts, D.: Networks, Dynamics, and the Small-World Phenomenon. American Journal of 
Sociology 105(2), 493–527 (1999) 
3. Watts, D., Strogatz, S.: Collective Dynamics of ´Small World´Networks. Nature 393, 440–
442 (1998) 
4. Milgram, S.: The Small-World Problem. Psychology Today 2, 60–67 (1967) 
5. Yager, R.R.: Intelligent Social Network Analysis using Granular Computing 23, 1197–
1220 (2008) 
6. Yager, R.R.: Concept Representation and Data base Structures in Fuzzy Social Relational 
Networks. IEEE Transactions on Systems. Man & Cib. 40(2), 413–419 (2010) 
7. Jungnickel, D.: Graphs, networks and algorithms, 4th edn. Springer (2013) 
8. Mathias, N., Gopal, V.: Small - worlds: How and why.arXiv:cond-mat/0002076v1, 1-20 
(2000) 
9. Newman, M.: The structure and function of complex networks. SIAM Review 45,  
167–256 (2003) 
10. Freeman, L.C.: A set of measures of centrality based on betweenness. Sociometry 40,  
35–41 (1977) 
11. Newman, M.: Scientific Collaboration Networks II. Shortest paths, weighted networks, 
and centrality. Phys. Rev. E 64, 016132 (2001) 
12. Lazarsfeld, P., Merton, R.: Friendship as a social process: a sustantive and methodological 
analysis. In: Freedom and Control in Modern Society. Van Nostrad, NewYork (1954) 
 

Generating Events for Dynamic Social Network
Simulations
Pascal Held, Alexander Dockhorn, and Rudolf Kruse
Department of Knowledge Processing and Language Engineering
Faculty of Computer Science
Otto von Guericke University of Magdeburg, Magdeburg, Germany
{pascal.held,rudolf.kruse}@ovgu.de, alexander.dockhorn@st.ovgu.de
http://fuzzy.cs.uni-magdeburg.de
Abstract. Social Network Analysis in the last decade has gained re-
markable attention. The current analysis focuses more and more on the
dynamic behavior of them. The underlying structure from Social Net-
works, like facebook, or twitter, can change over time. Groups can be
merged or single nodes can move from one group to another. But these
phenomenas do not only occur in social networks but also in human
brains. The research in neural spike trains also focuses on ﬁnding func-
tional communities. These communities can change over time by switch-
ing the stimuli presented to the subject. In this paper we introduce a
data generator to create such dynamic behavior, with eﬀects in the in-
teractions between nodes. We generate time stamps for events for one-to-
one, one-to-many, and many-to-all relations. This data could be used to
demonstrate the functionality of algorithms on such data, e.g. clustering
or visualization algorithms. We demonstrated that the generated data
fulﬁlls common properties of social networks.
1
Introduction
Social Network Analysis is a hot topic since some years. The ﬁrst investigations
where done in a static analysis of the networks. Examples for this are networks
representing friendship, or co-author relationships, between people. But social
networks, like facebook, or Twitter, are very dynamic. So a static view is too
simple. In [8] we described some algorithms which are based on events between
nodes. This could be e.g. an interaction between users of a social network, or
interactions between neurons in human brains.
The human brain consists of diﬀerent regions. In each region there are a
lot of neurons which are connected. These connections cause that stimuli from
outside are passed through this network of neurons by ﬁre events of neurons.
A group of neurons which handle the same stimuli are called functional groups
or communities. The discovery of such functional communities is very similar
to social networks analysis. [14] The history of ﬁre events of a single neuron is
called spike train.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 46–55, 2014.
c
⃝Springer International Publishing Switzerland 2014

Generating Events for Dynamic Social Network Simulations
47
There are some data sets available, e.g. the Enron Dataset, extracts from
twitter, or recordings from human brains. These are all real data sets, where we
know nothing about the base structure of the underlying data.
In this paper we present a data generator to generate events for social net-
works or ﬁre events in spike trains. This data represents interaction between
diﬀerent nodes in the network. We do not only represent static behavior, but
also dynamics in the underlying structure. So we can conﬁgure the generator in
a way, that clusters of nodes are changing, like in real social structures.
The generated data is based on clusters of nodes in a graph. Nodes within
the same cluster have a higher communication frequency than nodes in diﬀerent
clusters. During the simulation it is possible to generate new clusters, or modify
them by adding nodes, moving nodes from one to another cluster, merging with
other clusters, split up the clusters.
The rest of the paper is organized as follows. In the second section we present
some fundamentals in the ﬁeld of social networks and human brain spike trains.
The third section describes the generator in detail, followed by experiments with
the generated data in Section 4. In the last section we discuss our results.
2
Fundamentals
The generation of graphs, especially in social network analysis is nothing new.
There are a lot of generators, like the Kronecker graph generators [10] or the
generator from McGlohon et al. [12]. The most algorithms generate graphs which
hold some typical properties or have problems with a growing number of nodes.
Akoglu and Faloutsos developed the realistic graph generator [1] which holds
most typical properties and also enables the evolving of graphs. A good survey
of diﬀerent generators is given in [3].
The main drawback of these algorithms is that evolution of the graph in most
cases is a static growing, where new nodes and edges are created. In some cases
there are also changes in the connections, but the main structure is constant.
Our focus is to create a graph, where we know the main structure of the
underlying communities, so we have a ground truth to check cluster algorithms
on dynamic graphs. Also we want to be able to change this ground structure.
Another point is that we are primary not interested in the graph itself, but
on the events between the nodes, which could be used to create such a graph.
In the following we present same requirements for spike train simulation as
well as for social network generation.
2.1
Simulating Spike Trains
A neuron in the human brain can be simulated as a list of events. An event
occurs when the potential of the concerning neuron increases. This means that
the neuron ﬁres. Such an event list is commonly named spike train.
The easiest way to simulate such spike trains is a point process simulation
based on a Poisson process model [15]. This model is based on the assumption

48
P. Held, A. Dockhorn, and R. Kruse
that the probability of a neuron ﬁring in a given time frame [t, t + δt] is simply
given by Rδt where R is the ﬁring rate, for suﬃcient small δt. The probability
is absolutely independent from the position of the last ﬁring.
One interpretation of interacting neurons are ensembles in parallel spike trains
[11]. The main idea is, that neurons that ﬁre together are wired together. Borgelt
et al. generate data parallel spike trains with multiple Poisson processes. Every
spike train has one generator process. Additionally one process for every en-
semble is present. The events from the ensemble process will be copied into the
individual spike trains with a given probability. These probabilities describes
how close the neurons are interacting with each other. The individual point pro-
cesses of neurons within an ensemble have a lower ﬁre rate, so the combined ﬁre
rate together with the common ﬁre events is the same as from other neurons.
2.2
Social Networks
One aspect of our data generator is the simulation of interactions in social net-
works. For this simulation the generated data should have similar characteristics
as real social networks. In this section we would like to present some of these
characteristics and how to show them. These are the small-world property, scale-
free characteristics, and the structure of communities and clusters.
The Small-World Property was introduced by Watts and Strogatz [16] for
graphs with social network characteristics in 1998. It is based on the six de-
gree of separation from Guare [5]. They focused on the average shortest path
between two nodes (global connectivity measure) L and the average clustering
coeﬃcient (local neighborhood connectivity measure) C. In their experiments
they started with a regular graph where the nodes are placed in a ring. Every
node is connected to the k following and previous nodes. With a probability of
p they replaced a given edge by a new random one. For p = 0.0 this yield to
the original graph and p = 1.0 yield to a total random graph, with the same
amount of nodes and edges. They compared three graphs with social network
character with random graphs of the same complexity (same number of nodes
and edges) and proofed that they all follow the small-world model, which means
that L ≳Lrandom and C ≫Crandom.
The Scale-Free Characteristic is a another property of social networks. Typ-
ically is that not every node has the same connectivity degree. There are some
nodes with a strong connectivity to others and other nodes with much fewer
connections. This is based on the fact, that such networks grow over time. New
nodes connect themselves more probable with nodes with a strong connectivity.
For example, a new person in a friendship network will connect ﬁrst to high con-
nected friends then to other new nodes, or a new website will link to a known
common website then to another new one. Barabasi and Albert investigate in
1999 [2] this phenomenon. They found out that the degree distribution of the
nodes from social networks follow a power-law distribution.

Generating Events for Dynamic Social Network Simulations
49
To proof this property in the generated data, we will run the same experiments
and compare the L and C values with equivalent random graphs.
Communities and Clusters are the core phenomenon in social network. Peo-
ple organize in groups and these groups could be recognized in communities in
social networks. Such a group could be a group of people from the same univer-
sity or a clique of friends.
The main concept of this groups is that the connectivity within a group is
much denser then the connectivity to elements outside of the group. In the
ﬁeld of data mining this is called the intra-cluster-density and the inter-cluster-
sparseness. In our work we get this property by construction. The data generator
itself supports the modeling of such clusters.
Social Networks are Dynamic and not static. They evolve over time. New
elements join the network, clusters are emerging, splitting, growing, or shrinking.
Nodes change from one cluster to another. In [7] we describe how to analyze this
dynamics in clusters.
With our data generator we can generate communication events for such dy-
namics in social networks.
3
Data Generator
We already introduced a broad area of applications for graphs. However, all
variants require the graph structure or the type of output to comply special
constraints. For that reason the generator needs to be easily adjustable for a
multitude of network characteristics. The implemented data generator is able to
produce diﬀerent types of static and dynamic graphs. It consists of methods for
deﬁning the structure of the start graph and allows the import of scripts, which
describe changes to speciﬁed points in time. The basic behavior of the generator
and script functionalities will be outlined in the following sections.
The graph is divided into several clusters. Each network structure of a cluster
is generated using the model proposed by Barab´asi in [2]. This model proposes a
growing network starting with m0 nodes. Further nodes are sequentially added
to the graph and connected using m(≤m0) edges. The probability of a node
having k edges follows a power-law with an exponent of ymodel = 2.9 ± 0.1.
By deﬁnition the network structure of each cluster will organize itself into a
scale-free stationary state. We use the same model to decide which clusters are
connected. Connecting nodes will be drawn at random.
The ﬁring rates and intervals in a cluster can be conﬁgured by entering con-
stant values or a distribution where values are drawn of. The generator therefore
supports the creation of poisson processes by using gamma distributions to de-
termine time intervals between two events as it was modeled in [15]. Nodes and
clusters store corresponding parameters and can be manipulated through script
functions as it will be explained later.

50
P. Held, A. Dockhorn, and R. Kruse
Generated events will be stored and exported in a .csv-ﬁle. The respective
format can be set in the command line and be of one of the following types:
Communication with one source and single/multiple targets: Every en-
try will contain the time of the event, the id of the source node and targets of
the communication. ”time; idsource; idtarget(s)”
Groups of nodes active at the same time: This format does not include
information about the direction of any communication and stores simul-
taneously active nodes per time frame. ”time; idsource(s)”
Activity of single nodes: The third format records undirected potentials per
node and can be used to record spike trains. ”time; idnode”
The main focus of our designed program lies on enabling the user to precon-
ﬁgure changes of the network in a script ﬁle. This way analysis techniques can be
brought to the test for already known features of the dynamic graph. Currently
provided script-functions and their possible applications are listed below:
Delete/add nodes/clusters: Nodes and clusters can be added/deleted at
speciﬁed time points. E.g. new people are joining a network, a neuron dies
Change behavior of individual nodes: Adjust ﬁring rates, activity or clus-
ter assignments per node. E.g. a person starts to communicate more fre-
quently, communication partners change because of a new job, a neuron
ﬁres more often because a stimulus changed
Change behavior of clusters: Adjust activity of the whole cluster. E.g. a
group of people starts to communicate more often to organize an event,
a brain region changes activity level while sleeping, the visual part of the
brain adapts to changes in visual stimuli
Divide or merge clusters: Multiple clusters are merged to form a new one,
a cluster will be divided in parts. E.g. circle of friends splits after ﬁnishing
school, departments are joined to minimize communication costs, special
stimuli only activate speciﬁc parts of the brain
4
Experiments
The generator was tested on the simulation of spike trains and social networks.
The former was compared to another generator used by Borgelt el al. [11] which
is based on a model from Nawrot et al. [13]. We evaluated diﬀerent measures
relating to spike trains and social networks. Our test cases will be presented in
the following sections. Our evaluation platform was an HP Z400 with 6GB RAM
and a 3.45 GHz 6-core Intel Xeon processor. However the program only used one
core for the generation process. Event generation for our biggest test-case (1000
Cluster, 100 000 Nodes and 25 000 000 Events) took about 7 minutes.
4.1
Spike Trains
In spike trains the activity potentials per node are recorded. Therefore we
chose the third output type for our generator. Both tools were used to generate

Generating Events for Dynamic Social Network Simulations
51
50 nodes, 10 forming a single ensemble and 40 nodes for noise. Events were
recorded over ten seconds. The nodes ﬁring rates were set to an average of 20
events per second. The ensemble had a copy rate of 50 percent, therefore an av-
erage of half of the ensemble nodes participated on events of the whole ensemble.
First we checked for similarity of individual spike trains of both tools by
using a Kolmogorow-Smirnow test for the distributions of time intervals between
consecutive events per node. The average of recorded p-values was ∼0.55. For
this reason we can accept the hypothesis that both distributions were drawn
from the same continuous distribution.
dCorrelation = 1
2 −
n11n00 −n01n10
2

(n10 + n11)(n01 + n00)(n11 + n01)(n00 + n10)
(1)
dY ule =
n01 + n10
n11n00 −n01n10
(2)
dJaccard =
n10 + n01
n11 + n10 + n01
(3)
dHamming = n01 + n10
n∗∗
(4)
We continued the analysis of our generator by binning resulting spike trains
into 1000 bins of size 0.01 second to transform them into a binary matrix. This
was used to calculated distance matrices for the four distance measures Corre-
lation (1), Yule (2) [17], Jaccard (3) [9], and Hamming (4) [6]. The results are
shown in Figure 1. The distance matrices for hamming and yule distance show
clearly the similarity of the ﬁrst ten nodes forming an ensemble. Distances of
noise-noise and noise-ensemble combinations were much higher.
4.2
Social Network
Our second evaluation involves the generation of social networks. We created
four graphs with the following conﬁgurations.
– Graph 1 = 5 clusters with 20 nodes each
– Graph 2 = 3 clusters with 50 nodes each
– Graph 3 = 5 clusters with 10, 20, 30, 40 and 50 nodes
– Graph 4 = 25 clusters with 25 nodes each
Noise was excluded in all three generation processes. The activity and commu-
nication intervals per node were gamma-distributed with α = 1 and ε = 0.35.
We compared the average path length and the average cluster coeﬃcient of the
generated graphs to random graphs with same number of total edges. Recorded
values are shown in Table 1.
It has been shown that for all three generated graphs the small-world model
(L ≳Lrandom and C ≫Crandom) is applicable. We used the same set of graphs
to test for scale-free characteristics. We used the procedure, described in [4].
It uses a bootstrapping hypothesis test to maximize the Kolmogorow-Smirnow

52
P. Held, A. Dockhorn, and R. Kruse
Fig. 1. Distance matrices for jaccard, correlation, yule and hamming distance
statistic to test for a goodness-of-ﬁt between the data and the power law. The
resulting p-values need to be ≥0.1 to accept the power-law distribution as a
plausible hypothesis. Responding p-values per graph are shown in Table 1. The
results were all signiﬁcant (≥0.1) and therefore a power-law distribution can be
accepted as a hypothesis.
Forming communities and clusters is directly inferred by the generation pro-
cess. Each cluster will be generated as a separate graph using the model proposed
by Barab´asi in [2]. Noise can be added by deﬁning a base probability for inter-
cluster-events for a whole cluster or by adding speciﬁed noise nodes. Dependent
on the chosen cluster deﬁnition it is possible to design clusters of nodes which
are active at the same time, in equal distributed time intervals or are more likely
to talk to each other than to nodes from diﬀerent clusters.
To show the dynamic capabilities of our generator we created graphs with two
clusters of 20 nodes each and used a simple script, which included the following
commands:

Generating Events for Dynamic Social Network Simulations
53
– t1, move ﬁve nodes from cluster one to cluster two
– t3, add ﬁve new nodes to cluster one
– t5, remove ﬁve nodes from cluster two
– t7, lower the cluster activity of cluster two to 50%
– t9, set cluster activity of cluster two back to 100%
Table 1. Measured average shortest path length and clustering coeﬃcient per graph
compared to a random graph with same number of edges, p-values for accepting a
power-law distribution as plausible hypothesis
Graph 1 Graph 2 Graph 3 Graph 4
Lgenerator
4.30
4.73
4.38
6.45
Lrandom
2.83
2.97
2.96
3.28
Cgenerator
0.40
0.27
0.30
0.42
Crandom
0.07
0.05
0.35
0.14
p
0.5
0.5
0.5
0.9
We recorded the number of inter- and intra-cluster events per second and aver-
aged those over ten generation processes. Recorded values are binned per sepa-
rate graph conﬁguration and presented in Table 2 and Figure 2.
Table 2. Evolution of event frequency for dynamic graph, values represent average
count of events per second, time-intervals are based on diﬀerent graph conﬁgurations
t0−1 t1−3 t3−5 t5−7 t7−9 t9−10
Intra-cluster events one 133
67
139 137 134
133
Intra-cluster events two 138 200 197 161
75
164
Inter-cluster events
13
12
16
15
11
12
The inﬂuence of the script can be seen in the changes of events. Both clusters
start with nearly the same number of events per second. The intra-cluster com-
munication of cluster one is reduced in time frame two, because the number of
nodes changed from 20 to 15. Simultaneously an increase for intra-cluster com-
munication of cluster two was recorded. After reinserting ﬁve nodes to cluster
one, the number of events went back to the initial value. No further signiﬁcant
changes of cluster ones communication level were observed. In contrast records
for cluster two include a decrease of events in time frame four, which is correlated
with the decrease of nodes. In the following time frames the communication level
was ﬁrst set to 50% and in time frame six set back to 100%, which is observable
in the number of events of cluster two, as well. The number of inter-cluster events
was constant on a lower level than intra-cluster communication. This can be ex-
plained by the set base communication probability of pbase = 0.01. However, the
number of expected events can be changed by adjusting this parameter.

54
P. Held, A. Dockhorn, and R. Kruse
t0−1
t1−3
t3−5
t5−7
t7−9
t9−10
Time Step
0
50
100
150
200
250
Events
133
67
139
137
134
133
138
200
197
161
75
164
13
12
16
15
11
12
Events per Cluster
Intra-Cluster 1
Intra-Cluster 2
Inter Cluster
Fig. 2. Evolution of event frequency for dynamic graph, values represent average count
of events per second, time-intervals are based on diﬀerent graph conﬁgurations
5
Results and Outlook
We presented a generator for the creation of static and dynamic graphs. The
generator in its current version was able to produce reasonable data as a base
for multiple graph related problems. The comparison with a previous available
spike train generator showed that spike trains with similar distributions of event-
intervals can be created. Furthermore predeﬁned ensembles could be detected
in calculated distance matrices. Social network experiments demonstrated the
ability to create graphs with small world properties and scale-free characteristics.
All these processes can be combined with a script for planning changes in the
graph structure. Therefore researchers will be able to test analysis techniques
for dynamic graphs on event data containing predeﬁned features.
Until the tool gets released we will concentrate on refurbishing the current gen-
eration process. Conﬁgurations with a high number of events per second (> 100)
can still be too much aﬄicted with noise in the resulting distributions. Addition-
ally further script functions will be added to insert more dynamic characteristics
to the graph.
The implementation of our generator could be downloaded from
http://iws.cs.uni-magdeburg.de/~pheld/publications/IPMU2014/.
References
1. Akoglu, L., Faloutsos, C.: Rtg: a recursive realistic graph generator using random
typing. Data Mining and Knowledge Discovery 19(2), 194–209 (2009)
2. Barab´asi, A.L., Albert, R.: Emergence of scaling in random networks. Sci-
ence 286(5439), 509–512 (1999)

Generating Events for Dynamic Social Network Simulations
55
3. Chakrabarti, D., Faloutsos, C.: Graph mining: Laws, generators, and algorithms.
ACM Comput. Surv. 38(1) (June 2006)
4. Clauset, A., Shalizi, C.R., Newman, M.E.: Power-law distributions in empirical
data. SIAM Review 51(4), 661–703 (2009)
5. Guare, J.: Six Degrees of Separation: A Play. Vintage Books, New York (1990)
6. Hamming, R.W.: Error detecting and error correcting codes. Bell System Technical
Journal 29(2), 147–160 (1950)
7. Held, P., Kruse, R.: Analysis and visualization of dynamic clusterings. In: 2013
46th Hawaii International Conference on System Sciences (HICSS), pp. 1385–1393.
IEEE (2013)
8. Held, P., Moewes, C., Braune, C., Kruse, R., Sabel, B.A.: Advanced analysis
of dynamic graphs in social and neural networks. In: Borgelt, C., Gil, M.´A.,
Sousa, J.M.C., Verleysen, M. (eds.) Towards Advanced Data Analysis. STUD-
FUZZ, vol. 285, pp. 205–222. Springer, Heidelberg (2012)
9. Jaccard, P.: ´Etude comparative de la distribution ﬂorale dans une portion des alpes
et des jura. Bulletin del la Soci´et´e Vaudoise des Sciences Naturelles 37, 547–579
(1901)
10. Leskovec, J., Chakrabarti, D., Kleinberg, J., Faloutsos, C.: Realistic, mathemati-
cally tractable graph generation and evolution, using kronecker multiplication. In:
Jorge, A.M., Torgo, L., Brazdil, P.B., Camacho, R., Gama, J. (eds.) PKDD 2005.
LNCS (LNAI), vol. 3721, pp. 133–145. Springer, Heidelberg (2005)
11. Louis, S., Borgelt, C., Gr¨un, S.: Generation and selection of surrogate methods for
correlation analysis. In: Gr¨un, S., Rotter, S. (eds.) Analysis of Parallel Spike Trains.
Springer Series in Computational Neuroscience, vol. 7, pp. 359–382. Springer US
(2010)
12. McGlohon, M., Akoglu, L., Faloutsos, C.: Weighted graphs and disconnected com-
ponents: Patterns and a generator. In: Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD 2008,
pp. 524–532. ACM, New York (2008)
13. Nawrot, M., Aertsen, A., Rotter, S.: Single-trial estimation of neuronal ﬁring rates:
from single-neuron spike trains to population activity. Journal of Neuroscience
Methods 94(1), 81–92 (1999)
14. Shalizi, C., Camperi, M., Klinkner, K.: Discovering functional communities in dy-
namical networks. In: Airoldi, E.M., Blei, D.M., Fienberg, S.E., Goldenberg, A.,
Xing, E.P., Zheng, A.X. (eds.) ICML 2006. LNCS, vol. 4503, pp. 140–157. Springer,
Heidelberg (2007)
15. Vreeswijk, C.: Stochastic models of spike trains. In: Gr¨un, S., Rotter, S. (eds.)
Analysis of Parallel Spike Trains. Springer Series in Computational Neuroscience,
vol. 7, pp. 3–20. Springer US (2010)
16. Watts, D.J., Strogatz, S.H.: Collective dynamics of ’small-world’ networks. Na-
ture 393(6684), 440–442 (1998)
17. Yule, G.U.: On the association of attributes in statistics: with illustrations from
the material of the childhood society, &c. Philosophical Transactions of the Royal
Society of London. Series A, Containing Papers of a Mathematical or Physical
Character 194, 257–319 (1900)

A Model for Preserving Privacy
in Recommendation Systems⋆
Luigi Troiano1 and Irene D´ıaz2
1 University of Sannio, Italy
2 University of Oviedo, Spain
Abstract. The problem of preserving privacy in recommendation sys-
tems is faced in this work. The approach presented reduces the study
of privacy threats to the study of frequent property set obtained from
the characteristics of the objects the recommendation system provides
to a target user. This study is made by deﬁning a prominence index for
each item and by using eﬃcient methods to explore the lattice of item
characteristics.
Keywords: Recommendation
System,
Privacy,
Prominence
Index,
Frequent Item Sets.
1
Introduction
Research in Recommendation Systems (RS) has been growing with the devel-
opment of e-commerce mainly because many web sites and e-shops produce
recommendations to users in order to retain them. RS provides a rating or a
preference for a user using certain information about his/her preferences. This
information can be acquired either explicitly from the rates provided by users
or implicitly from monitoring users’ behavior (booked hotels or heard songs). In
addition to that, RS can beneﬁt of other kinds of information such as location
or demographic features.
The research related to RS has been focused on movie, music and book rec-
ommendation [3], with music recommendation being the most studied topic.
Recently it has been applied to other domains such as e-commerce. Many of
the works about recommender systems are focused on the understanding a users
browsing and purchase history via implicit and explicit actions and how to rec-
ommend them products that they have a higher probability of being preferred
[18].
Privacy issues have been largely studied for data bases [20] and also for Social
Networks. Brief reviews to this topic are presented in [7,25]). There are other
works presenting diﬀerent approaches related to the study of privacy concerns
in Social Networks [5,8,9,19,24]).
⋆Author acknowledges ﬁnancial support by Grant TEC2012-38142-C04-04 from Min-
istry of Education and Science, Government of Spain and by Grant UNOV-13-
EMERG-GIJON-10 from University of Oviedo, Spain.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 56–65, 2014.
c
⃝Springer International Publishing Switzerland 2014

A Model for Preserving Privacy in Recommendation Systems
57
However, privacy issues involved in recommender systems have been studied
only in few works. For example in [13] the problem of producing recommenda-
tions from collective user behavior while simultaneously providing guarantees of
privacy for these users is studied.
In [16] it is presented a privacy preserving recommendation framework based
on groups with the aim of protecting users from unreliable service providers.
The starting point of this work is to ﬁnd a way out of the opposition between
anonymity and personalization, that is, how a certain level of anonymitycan
be maintained without sacriﬁcing useful and accurate recommendations. Other
works have been focused on some concern related to both privacy issues or
recommendation systems [19,9].
In this paper we face the problem of privacy concerns in a recommendation
system based on frequent item sets. The basic approach is to consider all the
recommendations provided by a given web site or e-shop and to study the set of
frequent properties of these recommended objects. The remainder of this contri-
bution is organized as follows: Section 2 provides some preliminaries regarding
recommender systems and frequent itemsets theory, Section 3 outlines the model
we adopted, Section 4 describes the model application by means of an example,
Section 5 draws conclusions and future directions.
2
Preliminaries
2.1
Recommendation Systems
Recommendation systems were created out of the user needs to handle the in-
creasing volume of information that is available in the world wide web [3]. The
deﬁnition of a RS depends on several issues such as the type of data available
to provide the recommendation. For example, it is possible to use the ratings or
the information provided by the user when registration or social relationships.
The data can be provided both explicitily and implicitily. Explicit data is given
by a customer (for example a rate) while implicit data is obtained from the
user’s behavior (for example if he looks at the description of an object, marks
the object or refers to it). On the other hand, a recommendation algorithm is
essentially a ﬁltering algorithm. Therefore, according to the ﬁltering approach,
the RS engine can be classiﬁed as Content-based recommenders if recommenda-
tions are based on the past user preferences ([15]), Collaborative recommenders if
recommendations to each user are based on the information provided by similar
users ([6]), Demographic recommenders if they categorize the user based on per-
sonal attributes assuming that common personal attributes such as age or sex
derive on some common preferences ([14]) and Hybrid methods which combine
the aforementioned methods ([4]).
Many data mining techniques have been used to provide recommendations
[11]. In addition, other issues such as scarcity, scalability or privacy should be
addressed when working with RS (see for example [17,12]). This last property,
privacy, is faced in this work.

58
L. Troiano and I. D´ıaz
2.2
Frequent Item Set Theory
Frequent item sets theory represents one of the major families of techniques for
characterizing data. This problem is often viewed as the discovery of association
rules. The reason is that they are strongly related because the discovery of
frequent item sets is a prior step to discover association rules. Frequent Item Set
mining is based on the Market-Basket Model of data. Therefore it looks for sets
that frequently co-occur together.
Let I = {i1, . . . , im} be a set of items and let D = {d1, . . . , dn} be the set
of transactions. Each dk, with 1 ≤k ≤n is represented by a vector dk =
(iσ(1), . . . , iσ(r)) with iσ(j) ⊂I and 1 ≤r ≤m.
A transaction d ∈D covers an item set J if and only if J ⊂d. The support
of an item set J is deﬁned as the number of transactions covering it. In other
terms, the support provides a measure of how often a combination of values is
presented.
The problem of mining frequent item set is deﬁned as searching all item
sets that have support greater than the user-speciﬁed minimum support (called
minsup). The most well-known algorithm to produce frequent item sets is Apri-
ori. This algorithm in addition computes association rules from the obtained
frequent item sets by selecting those association rules according to a minimum
conﬁdence (called minconf). Conﬁdence measures how often the association
between values occur. For a detailed description, authors refer to [2].
3
The Model
Let I ≡{I1, I2, . . . , In} be the collection of items considered by the recom-
mender system as suggestion to user. Each item Ih is scored by ρ(Ih), that is
the relevance assigned by the recommender system to the item with respect to
the user preferences. This score is assigned over a ratio scale, and we can assume
ρ(Ih) ∈[0, 1] without any loss of generality. In addition we assume that each
itemset Ik is tagged by a set of keywords, K(Ih) = {kh,1, . . . , kh,m} ⊆K. Table
1 outlines an example of result set obtained by the recommender system. In this
case, the system replied with 10 suggested items, each with a relevance scoring
between 0.9 (the highest) and 0.1 (the lowest). Each itemset is tagged with a
subset of the keywords K ≡{A, B, C, D}.
We assume keywords are the sensitive information the user would like to
protect against association a recommender systems could infer for his/her proﬁle.
So the question we aim to answer is the following: what are the keyword subsets
that are most prominently linked to the user proﬁle? To answer this question it
is considered the inclusion lattice, as that depicted in Figure 1.
An index of prominence is a function
π : 2K →[0, 1]
(1)
It is a function of the relevence assigned to each item. In particular
π(K) = pK(ρ(I1), . . . , ρ(Iq)) K ⊆K(Ih), h = 1 . . . q
(2)

A Model for Preserving Privacy in Recommendation Systems
59
Table 1. An example of result set
Item (I) Relevance (ρ) Keywords (K)
1
0.9
B, C
2
0.9
C, D
3
0.8
A, B, C
4
0.8
B, C
5
0.7
A, B, C
6
0.7
C, D
7
0.6
C, D
8
0.6
A
9
0.2
A, D
10
0.1
A, D
Fig. 1. The keyword powerset lattice
where
pK : [0, 1]q →[0, 1] K ⊆K
(3)
is a function of relevance assigned to each item, thus to the associated keyword
set.
We expect π to be anti-monotone w.r.t. inclusion and monotone w.r.t. rele-
vance, that is
π(K1) = pK1(ρ1, . . . , ρq1) ≥pK2(ρ1, . . . , ρq2) = π(K2) ∀K1 ⊂K2 ⊆K
(4)
and
pK(ρ1, . . . , ρq) ≤pK(ρ′
1, . . . , ρ′
q) ∀ρi ≤ρ′
i, i = 1..n, ∀K ⊆K
(5)
Condition expressed by Eq.(4) states that the index associated to a combina-
tion of keywords is lower than the index associated to all its parts, while Eq.(5)
states that a result set with higher relevance will provide a higher prominence
index.
Other properties are related to the symmetry of information, that is
π(K1) = π(K2) ∀K1, K2 ⊆K, |K1| = |K2|
(6)

60
L. Troiano and I. D´ıaz
and
p(ρ1, . . . , ρq) = p(ρ(1), . . . , ρ(q))
(7)
where (·) is a generic permutation of arguments.
Finally, we expect that considering 0-relevant supersets should not aﬀect the
result, that is
pK(ρ1, . . . , ρq) = pK(ρ1, . . . , ρq−1) ρq = 0, ∀K ⊆K
(8)
An index of prominence satisfying all the previous properties is the relevance
average over the whole result set.
π(K) = 1
n

Ih∈I,K⊆K(Ih)
ρ(Ih)
(9)
Once the index is determined, it is established a threshold τ in order to decide
which subset is prominent or not. The threshold τ can be ﬁxed depending on
problem characteristics and solution conservativeness. Higher threshold values
will reduce the number of risky keyword subsets, thus relaxing the solution con-
servativeness. Diﬀerently, lower values will enlarge the number of risky keyword
subset, thus enforcing the solution conservativeness.
The method proposed to set the threshold consists in computing the expected
index E[¯π] assuming for each subset K the average relevance value ¯ρ = 0.5.
Formally,
τ = E[π] = 1
2m

K⊆K
¯π(K)
(10)
where
¯π(K) = pK(
qn
m −times
  
¯ρ, . . . , ¯ρ), |K| = q
(11)
It could be objected that computing τ is computationally prohibitive, as it
requires to move over the whole power set. However, as the average value ¯ρ is
used, this is not necessary, since
¯π(K1) = ¯π(K2) = pK(
qn
m −times
  
¯ρ, . . . , ¯ρ) = ¯πq
∀K1, K2, K ⊆K, |K1| = |K2| = |K| = q
(12)
Therefore,
τ = E[π] = 1
2m

K⊆K
¯π(K) = 1
2m
m

q=0
m
q
	
¯πq
(13)
In the case of relevance average we have
τ = 1
2m
m

q=0
m
q
	
¯πq = 1
2m
m

q=0
m
q
	 q
m ¯ρ = 1
2 ¯ρ = 0.25
(14)

A Model for Preserving Privacy in Recommendation Systems
61
In this case, the lattice is splitted in two parts as depicted in Fig. 2. The upper
part of the lattice is made of non-prominent subsets, while the bottom is ﬁlled
by prominent subsets. We can observe the following:
– if a keyword subset is prominent, all subsets will be prominent as well (e.g.,
subset BC)
– a subset can be prominent due the joint contribution of its supersets (e.g.,
subset D)
– a subset, although having relevance above ¯ρ, can be not prominent (e.g.,
subset A)
– the empty set is subset of any keyword set, thus π(∅) provides the prominence
index for the whole lattice and the index upper limit (e.g., subset None)
(a) r = 2
(b) r = 3
Fig. 2. Prominent (left) and Generator (right) subsets
The prominence index is computed from recommendations given by the result
set. They represent the source of information. For this reason, we call generators
the keyword subsets that are associated to items suggested by the recommender
system. Their mapping to lattice is given by Fig. 2. Generators are circled by
a bold line; inside the relvance of related recommendations is given. Obviously,
when a generator is prominent, all its subsets are prominent as well (e.g., subset
BC). Diﬀerently, non-prominent generators might contain prominent keyword
subsets (e.g., subsets AD and CD), or not (e.g. subset ABC).
Since prominence of keyword subset depends on contribution provided by
supersets, it might be necessary to extend the analysis to the whole keyword
powerset lattice. This requires eﬃcient methods to explore the lattice. Anti-
monotonicity w.r.t. inclusion opens to the possibility of adopting data mining
algorithm developed for the search of itemsets [1,10,23,22,21].
The ﬁrst and most noticeable algorithm for mining frequent itemsets is known
as Apriori [1]. Apriori is a levelwise, breadth-ﬁrst, bottom-up algorithm, as out-
lined by Algorithm 1.

62
L. Troiano and I. D´ıaz
Algorithm 1. Apriori Prominence
1: Lq: prominent keyword subsets of size q
2: Gq: generated keyword subset of size q
3: ml: size of the largest keyword subset
4: τ: the prominence threshold
5:
6: ﬁnd L1
7: q = 1
8: while Lq ̸= ∅and q < ml do
9:
Gq+1 = Lq join Lq
10:
Lq+1 = {K ∈Gq+1|π(K) ≥τ}
11:
q = q + 1
12: end while
13: return 
k
Lk
After selecting prominent keywords, pairs of keywords are generated by join-
ing prominent keywords and ﬁltered according to the threshold τ. So, keyword
triplets are generated from prominent keywords and ﬁltered. At each step, (q+1)-
keyword subsets (candidates) are generated from q-keyword prominent subsets
and ﬁltered. The process is iterated ultil no candidate is available or the largest
keyword subset is reached.
The main limitation of Apriori is that the generation of candidates produces
subsets which will not be cosidered at the following step as too long. This might
lead the algorithm to consider the same subset at diﬀerent iterations. FP-growth
[10] provides an eﬃcient search based on FP-tree structure, an extended preﬁx-
tree structure able to store minimal information regarding freuent itemsets. In-
deed FP-trees allow to pack the representation of dataset transactions, avoiding
the generation of candidate itemsets and employing a partitioning-based method
to decompose the mining task into a set of smaller tasks.
Connections to data mining are not only limited to the discovery of frequent
itemset. For instance, it is possible to transpose the concept of closed keyword
sets to
So far, we considered the average relevance as prominence index. Other pos-
sibilities are feasible. For instance we might consider the maximum relevance,
deﬁned as
π(K) =
max
Ih∈I,K⊆K(Ih) ρ(Ih)
(15)
In this case, according to Eq.(13), the prominence threshold can be computed
as
τ = 1
2m
m

q=0
m
q
	
¯ρ = ¯ρ = 0.5
(16)
and the prominence area is depicted if Fig. 3.
We can observe how the prominence region enlarged. Indeed, if on one side
we increased the threshold, we generally provide higher prominence indexes to

A Model for Preserving Privacy in Recommendation Systems
63
(a) r = 2
(b) r = 3
Fig. 3. Closed (left) and Prominence-max (right) subsets
keyword subsets: a keyword subset is prominent if there is at least one recom-
mendation with a relevance over the threshold. We can generalize this criterion
and look for keyword subsets with at least r recoomendations with a relevance
over the threshold. This is can be obtained by considering
π(K) = rth −ordstat
Ih∈I,K⊆K(Ih) ρ(Ih)
(17)
Threshold can be computed considering that τ for the maximum, assuming
i = qn, can be written as
τ = 1
2m
m

q=0
m
q
	
¯ρ =
¯ρ
2m

1 + 1
n
nm−1

i=0
 m
 i
n

	
(18)
This equation can be generalized to statistic order r as
τ =
¯ρ
2m
⎛
⎝1 + 1
n
nm−1

i=(r−1)m
 m
 i
n

	⎞
⎠
(19)
(a) r = 2
(b) r = 3
Fig. 4. Prominence regions (order statistic)

64
L. Troiano and I. D´ıaz
In our example, if r = 2 we have τ = 156
160 ¯ρ = 0.4875 and if r = 3 we have
τ = 152
160 ¯ρ = 0.475. Prominence regions for both cases are depicted in Fig. 4.
4
Conclusions and Future Work
In this paper we proposed a model to identify threats from the recommendations
made by a RS. It is assumed that the sensitive information are the keywords
deﬁning each recommended object. So the question is which keyword subsets are
most prominently linked to the user proﬁle and thus, could be more risky. This
problem is solved by deﬁning a prominence index which is a function to measure
the relevance assigned to each item. To study the prominence of all the subsets
we propose the use of eﬃcient methods to explore the lattice of keywords. In the
future we plan to test these approach using standard data bases. In addition, it
is neccessary a deep study about prominence indexes that could be used as well
as aggregation procedures.
References
1. Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., Verkamo, A.I.: Fast discovery
of association rules. In: Advances in Knowledge Discovery and Data Mining (1996)
2. Agrawal, R., Srikant, R.: Fast algorithms for mining association rules in large
databases. In: Proceedings of the 20th International Conference on Very Large
Data Bases, VLDB 1994, pp. 487–499. Morgan Kaufmann Publishers Inc., San
Francisco (1994)
3. Bobadilla, J., Ortega, F., Hernando, A., Guti´errez, A.: Recommender systems sur-
vey. Knowledge-Based Systems 46, 109–132 (2013)
4. Burke, R.: Knowledge-based recommender systems (2000)
5. Campan, A., Truta, T.M.: Data and structural k-anonymity in social networks. In:
Bonchi, F., Ferrari, E., Jiang, W., Malin, B. (eds.) PinKDD 2008. LNCS, vol. 5456,
pp. 33–54. Springer, Heidelberg (2009)
6. Candillier, L., Meyer, F., Boull´e, M.: Comparing state-of-the-art collaborative ﬁl-
tering systems
7. D´ıaz, I., Ralescu, A.: Privacy issues in social networks: A brief survey. In: Greco, S.,
Bouchon-Meunier, B., Coletti, G., Fedrizzi, M., Matarazzo, B., Yager, R.R. (eds.)
IPMU 2012, Part IV. CCIS, vol. 300, pp. 509–518. Springer, Heidelberg (2012)
8. D´ıaz, I., Rodr´ıguez-Mu˜niz, L.J., Troiano, L.: Fuzzy sets in data protection: strate-
gies and cardinalities. Logic Journal of IGPL 20(4), 657–666 (2012)
9. D´ıaz, I., Rodr´ıguez-M˜uniz, L.J., Troiano, L.: On mining sensitive rules to iden-
tify privacy threats. In: Pan, J.-S., Polycarpou, M.M., Wo´zniak, M., de Carvalho,
A.C.P.L.F., Quinti´an, H., Corchado, E. (eds.) HAIS 2013. LNCS, vol. 8073, pp.
232–241. Springer, Heidelberg (2013)
10. Han, J., Pei, J., Yin, Y., Mao, R.: Mining frequent patterns without candidate
generation: A frequent-pattern tree approach. In: Mannila, H. (ed.) Data Mining
and Knowledge Discovery, pp. 53–87. Kluwer, New York (2004)
11. Lee, M., Choi, P., Woo, Y.: A hybrid recommender system combining collaborative
ﬁltering with neural network. In: De Bra, P., Brusilovsky, P., Conejo, R. (eds.) AH
2002. LNCS, vol. 2347, pp. 531–534. Springer, Heidelberg (2002)

A Model for Preserving Privacy in Recommendation Systems
65
12. Luo, X., Xia, Y., Zhu, Q.: Incremental collaborative ﬁltering recommender based
on regularized matrix factorization. Know.-Based Syst. 27, 271–280 (2012)
13. McSherry, F., Mironov, I.: Diﬀerentially private recommender systems: Building
privacy into the net. In: Proceedings of the 15th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD 2009, pp. 627–636.
ACM, New York (2009)
14. Pazzani, M.J.: A framework for collaborative, content-based and demographic ﬁl-
tering. Artif. Intell. Rev. 13(5-6), 393–408 (1999)
15. Salter, J., Antonopoulos, N.: Cinemascreen recommender agent: Combining collab-
orative and content-based ﬁltering. IEEE Intelligent Systems 21(1), 35–41 (2006)
16. Shang, S., Hui, Y., Hui, P., Cuﬀ, P.W., Kulkarni, S.R.: Privacy preserving recom-
mendation system based on groups. CoRR, abs/1305.0540 (2013)
17. Tak´acs, G., Pil´aszy, I., N´emeth, B., Tikk, D.: Scalable collaborative ﬁltering ap-
proaches for large recommender systems. J. Mach. Learn. Res. 10, 623–656 (2009)
18. Troiano, L., D´ıaz, I., Kriplani, A.: A recommender system based on dempster-shafer
theory. In: Eurofuse, pp. 232–241 (2013)
19. Troiano, L., D´ıaz, I., Rodr´ıguez-Mu˜niz, L.J.: A model for assessing the risk of
revealing shared secrets in social networks. In: Greco, S., Bouchon-Meunier, B.,
Coletti, G., Fedrizzi, M., Matarazzo, B., Yager, R.R. (eds.) IPMU 2012, Part IV.
CCIS, vol. 300, pp. 499–508. Springer, Heidelberg (2012)
20. Troiano, L., Rodr´ıguez-Mu˜niz, L.J., Ranilla, J., D´ıaz, I.: Interpretability of fuzzy
association rules as means of discovering threats to privacy. Int. J. Comput.
Math. 89(3), 325–333 (2012)
21. Troiano, L., Scibelli, G.: Mining frequent itemsets in data streams within a time
horizon. Data & Knowledge Engineering 89, 21–37 (2014)
22. Troiano, L., Scibelli, G.: A time-eﬃcient breadth-ﬁrst level-wise lattice-traversal
algorithm to discover rare itemsets. Data Mining and Knowledge Discovery 28(3),
773–807 (2014)
23. Troiano, L., Scibelli, G., Birtolo, C.: A fast algorithm for mining rare itemsets. In:
ISDA 2009, pp. 1149–1155 (2009)
24. Zhou, B., Pei, J.: The k-anonymity and l-diversity approaches for privacy preserva-
tion in social networks against neighborhood attacks. Knowledge and Information
Systems 28(1), 1–38 (2010)
25. Zhou, B., Pei, J., Luk, W.: A brief survey on anonymization techniques for privacy
preserving publishing of social network data. SIGKDD Explor. Newsl. 10, 12–22
(2008)

Classiﬁcation of Message Spreading in a
Heterogeneous Social Network⋆
Siwar Jendoubi1,2, Arnaud Martin2,
Ludovic Liétard2, and Boutheina Ben Yaghlane1
1 LARODEC, University of Tunis, Avenue de la liberté, 2000 Le Bardo, Tunisie
2 IRISA, University of Rennes 1, Rue E. Branly, 22300 Lannion, France
jendoubi.siwar@yahoo.fr, {Arnaud.Martin,ludovic.lietard}@univ-rennes1.fr,
boutheina.yaghlane@ihec.rnu.tn
Abstract. Nowadays, social networks such as Twitter, Facebook and
LinkedIn become increasingly popular. In fact, they introduced new
habits, new ways of communication and they collect every day several
information that have diﬀerent sources. Most existing research works fo-
cus on the analysis of homogeneous social networks, i.e. we have a single
type of node and link in the network. However, in the real world, social
networks oﬀer several types of nodes and links. Hence, with a view to
preserve as much information as possible, it is important to consider so-
cial networks as heterogeneous and uncertain. The goal of our paper is to
classify the social message based on its spreading in the network and the
theory of belief functions. The proposed classiﬁer interprets the spread
of messages on the network, crossed paths and types of links. We tested
our classiﬁer on a real word network that we collected from Twitter, and
our experiments show the performance of our belief classiﬁer.
Keywords: Information propagation, heterogeneous social network,
classiﬁcation, evidence theory.
1
Introduction
Nowadays, social networks such as Twitter, Facebook and LinkedIn become in-
creasingly popular. In fact, they introduced new habits and new ways of com-
munication. Besides, one of the distinguishing features of on-line social networks
is the information spreading through social links. This is due to the “word-
of mouth” exchanges, i.e. user-to-user exchanges, which makes the information
more accessible and it spreads and reaches a large scale in few minutes. The
volume and the dynamic of the exchange has attracted the attention of re-
search communities. This research is motivated by the fact that the study of the
diﬀusion of information is useful for understanding the dynamic behind social
networks and the evolution of human relationships. Thus, they have focused on
⋆These research works and innovation are carried out within the framework of the
device MOBIDOC ﬁnanced by the European Union under the PASRI program and
administrated by the ANPR.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 66–75, 2014.
c
⃝Springer International Publishing Switzerland 2014

Classiﬁcation of Message Spreading in a Heterogeneous Social Network
67
the processing of such data to extract high quality information, this informa-
tion may be an important event, it can also be useful for optimizing business
performance, or even for preventing terrorist attacks, etc.
The processing of a social network, always, starts by studying its structural
properties, in fact the simple visualization of the network cannot give us a clear
analysis about it. In the literature, we found a lot of structural properties mea-
sures like the degree, the betweenness, the closeness, the eigenvector centrality,
etc. Quantifying structural properties and interpreting them will be essential to
characterize the behavior of social actors, their position in the network, their
interactions and how do they diﬀuse the information. Hence, the analysis of
the network structural properties is an essential step when we study and model
information propagation.
In our work we are interested in the classiﬁcation of the spreading of the
information in a heterogeneous social network. We assume that each type of
content has some speciﬁc behavior when it propagates in the network. Hence,
we propose a new algorithm of information propagation in a heterogeneous social
network that takes into account the behavior of the content to be propagated.
Therefore, we introduce an evidential algorithm to classify the propagation of
the information through the network.
In the next section, we outline the literature review of the information prop-
agation in social networks, the social message classiﬁcation and the theory of
belief functions. In section three, we introduce our algorithm of information
propagation in a heterogeneous social network. In section four, we present our
classiﬁcation algorithm. Finally, we present our experiments in the ﬁfth section.
2
Literature Review
2.1
Information Propagation in Social Networks
Information dissemination is a wide research domain that attracted the at-
tention of researchers from various ﬁeld such as physics and biology. We ﬁnd
the family of epidemiological models that are used to understand how diseases
spread through populations. The simplest version is SI (Suspected-Infected), in
this model, an individual is suspected if he has not the disease yet but he can
catch it and become infected. This model was extended and many other version
appeared to model speciﬁc diseases. Hence, we ﬁnd SIS model (Suspected-
Infected-Suspected), SIR model (Suspected-Infected-Recovered), SIRS model
(Suspected-Infected-Recovered-Suspected), etc. The reader can refer to [1,17] for
further details.
Computer scientists are generally interested in studying information propa-
gation in on-line social networks. Mainly, their goal is to develop a model that
simulates the diﬀusion process. Basic models are Linear Threshold Model (LTM)
[7] and Independent Cascade Model
(ICM) [6]. They assume the existence of a
structure of a directed graph where each node can be activated or not knowing
that you can not inactivate already activated nodes. The ICM model requires
a probability distribution which must be associated with each link and LTM

68
S. Jendoubi et al.
requires a degree of inﬂuence that must be set on each link and a threshold of
inﬂuence for each node [12]. These two models were reused and improved in a
lot of works like [5,18].
In this paper, we focus on information propagation in a heterogeneous social
network, i.e. on which we ﬁnd several types of links and/or nodes. In fact, in real
word social networks we ﬁnd many types of objects (users, groups, applications,
etc) that are connected via many types of social links (friendship, membership,
colleague, etc). Information dissemination in homogeneous social networks has
been widely studied and the reader can refer to [8] for a recent survey. Now, re-
search works start focusing on the processing of heterogeneous social networks.
We ﬁnd the work of [19] that simulates the propagation of the information in het-
erogeneous social networks based on the conﬁguration model approach. In [13],
authors propose to consider the behavior of individuals to model the inﬂuence
propagation, their model is based on a heterogeneous social network.
2.2
Social Message Classiﬁcation
Social message classiﬁcation approaches, presented in the literature, are generally
based on the content of the information and text mining techniques. They search
to classify the user generated content to positive or negative about a some speciﬁc
product. This task is so called sentiment classiﬁcation and it is used to mine
opinions. It starts by an item and/or feature extraction step, then it compares
the extracted items and/or features to an existing corpus, ﬁnally comes the
sentiment classiﬁcation that can be based on items, features or both of them
[14]. We ﬁnd the work of [15] in which the author used a random sample of 3516
tweets to classify the feelings of consumers with respect to well-known brands.
He classiﬁed the opinions (tweets) into positive and negative to see what is the
most dominant opinion. In [10], a detailed case study that applies text mining
to analyze unstructured textual content published on Twitter and Facebook and
that talks about three chains of pizza. The reader can refer to the work of [16]
for a recent study of the state of the art of social networks data mining.
2.3
Theory of Belief Functions
Upper and Lower probabilities [4] was the ﬁrst ancestor of the theory of belief
functions. Then comes the Mathematical theory of evidence [20] which deﬁnes
the basic framework of information management and processing in the evidence
theory, often called Shafer model. The main purpose of the theory of belief
functions is to achieve more reliable, precise and coherent information. Here
we present a short introduction of this theory, for more details the reader can
refer to [20].
Let Ω = {ω1, ω2, . . . , ωn} be a set of all possible decisions that can be made
in a particular problem, it is called frame of discernment. The basic belief as-
signment (BBA), mΩ, represents the agent belief on Ω, and it must respect

A⊆Ω mΩ (A) = 1. In the case where we have mΩ(A) > 0, A is called focal set
of mΩ. The basic belief assignment can be converted into other functions deﬁned

Classiﬁcation of Message Spreading in a Heterogeneous Social Network
69
Algorithm 1. Information propagation algorithm
Inputs:
– N: number of iteration
– S: source of the message
– Str: propagation strategy
– Network: the heterogeneous social network
Output:
– PrNet: propagation network
Algorithm:
1. ReadyNodes.add(S);
2. For i = 1 to N do
(a) for j = 1 to ReadyNodes.size() do
i. Node ←ReadyNodes.get(j);
ii. if(Node.propagate()=True)
foreach LinkType do
x ←Node.outdegree() ∗Node.propagationTendancy()
∗Str.LinkTypeProportion();
R←(Node.randomSelection(x, LinkT ype));
(b) Pr.reﬁne(R);
(c) R1.addAll(R);
(d) ReadyNodes.addAll(R1);
(e) R1.clear;
from 2Ω to [0, 1]. This theory presents a rich framework for information fusion
and combining pieces of information (evidence). We ﬁnd the Dempster’s rule [4],
the conjunctive and disjunctive combination rule [21], etc.
3
Propagation Algorithm in a Heterogeneous Social
Network
In this section we introduce an algorithm of information propagation in a het-
erogeneous social network. This new algorithm takes four diﬀerent inputs which
are the number of iterations (stopping condition), the source of the message, the
propagation strategy and the heterogeneous social network. As output, we have
the propagation network that preserves the traversed paths. Algorithm 1 shows
outlines of our propagation process. It starts by the source node. First, we verify
if the current node is ready (wants) to propagate the message. Then, for each
type of link in the network we compute the number of neighbors that will receive
the message.
We assume that each type of message has some special characteristics of prop-
agation in the network that is related to the types of links, so we deﬁne a prop-
agation strategy for each type of message. Moreover, we consider the tendency
of a particular node to propagate the message as a propagation parameter. In-
deed, this parameter models the fact that a node can choose to distribute the
message to a subset of its contacts (that he selects) or to retain it. The novelty
of this algorithm is that we consider the type of the message while propagating

70
S. Jendoubi et al.
it. Moreover our algorithm works with heterogeneous social networks where we
have diﬀerent types of links.
4
Classiﬁcation of Information Propagation
The main purpose of this paper is to classify the spreading of the information
through the network in order to characterize its content. In this section, we
introduce our classiﬁcation process that is composed of two steps; parameter
learning step and the classiﬁcation step. As mentioned in the algorithm 2, to
learn the parameters of the model we need a set of propagation networks. First
of all, we compute the number of nodes that have received the message via
each type of link. We do this computation for each propagation level, i.e. we
call propagation level the number of links between the source of the message
and the target node. Second, we calculate the accrued eﬀective by summing
the eﬀective of each level with the eﬀective of the one before, this computation
is done in order to preserve the propagation history at each propagation level.
After that we transform the eﬀective set of each level to a probability distribution
deﬁned on types of links, this transformation is done for two reasons; the ﬁrst
one, we need a probability distribution for the probabilistic classiﬁer and the
second one, it is an essential step to get the basic belief assignment distribution.
Finally we transform each probability distribution to a BBA distribution using
the consonant transformation [2,3].
Algorithm 2. Parameter learning algorithm
Input:
– PrNetSet: a set of propagation networks
Output:
– ProbaSet: a set of probabilities distributions (a probability distribution by prop-
agation level).
– BbaSet: a set of BBA distributions (a BBA distribution by propagation level).
Algorithm:
//eﬀective computation
Foreach PrNet in PrNetSet do
1. Foreach Level in PrNet do
(a) Foreach TypeLink do
N (TypeLink, Level) ←N (TypeLink, Level)
+ComputeNodes(T ypeLink);
//Accrued eﬀective calculation
For Level= 2 to NbrLevels do
1. Foreach TypeLink do
(a) N (TypeLink, Level) ←N (TypeLink, Level)
+N (TypeLink, Level −1);
//ProbaSet and BbaSet computation
ProbaSet←ProbabilitiesCalculation(N);
BbaSet←ConsonantTransformation(ProbaSet);

Classiﬁcation of Message Spreading in a Heterogeneous Social Network
71
Once model’s parameters are learned, we can use it to classify new coming
message (propagation network of the message) as shown in algorithm 3. Our
classiﬁcation algorithm starts by applying the same parameter learning process
(algorithm 2) on the propagation network to be classiﬁed. Then for each level in
the network we compute the distance between its probability distribution and
the probability distribution of each propagation strategy, then we choose the
class of the nearest propagation strategy (with the shortest distance) to be the
class of the message in the current level. The same process is done with BBA
distributions as mentioned in the algorithm.
Algorithm 3. Classiﬁcation algorithm
Input:
– ProbaSets: a set of probabilities distributions for each strategy of propagation.
– BbaSets: a set of BBA distributions for each strategy of propagation.
– PrNet: The propagation network to be classiﬁed
Output:
– In order to see the impact of the level of propagation on the classiﬁcation results,
in our output we have a class by level.
Algorithm:
1. (ProbaPr, BbaPr) ←ParameterLearning (PrNet);
2. For i = 1 to NbrStrategies do
(a) Foreach Level do
i. ProbaDist(i, Level) ←Distance (ProbaPr, ProbaSets (i));
ii. BbaDist(i, Level) ←Distance (BbaPr, BbaSets (i));
3. Foreach Level do
(a) ProbaClasses(Level) ←StrategyMinDistance (ProbaDist (:, Level));
(b) BbaClasses(Level) ←StrategyMinDistance (BbaDist (:, Level));
5
Experiments and Results
In this section, we present some experiments to show the power of the proposed
evidential classiﬁcation algorithm.
5.1
Data Description
We used NodeXL V 1.0.1.245 [9] to collect social network data from Twitter. We
collected the network shown in ﬁgure 1. It is a directed network in which nodes
are Twitter users. Table 1 shows the characteristics of our network data.
Table 1. Data characteristics
Vertices Edges
Geodesics
distance
Betweenness Closeness Eigenvector
97
350
6
184.99
0.004
0.01

72
S. Jendoubi et al.
Fig. 1. Network visualization
As mentioned above, we need a heterogeneous social network to test proposed
algorithms. Therefore, we used the structure of the network collected from Twit-
ter and we generated, randomly, the types of links. We assume four types of link
in the network which are “Professional”, “Familial”, “Friendly” and “Undeﬁned”.
Then we obtained a heterogeneous social network that is used as input for our
propagation algorithm.
5.2
Experiment Conﬁguration
In the following experiments, we deﬁned three diﬀerent propagation strategies
for three types of messages which are: “Spam”, “Professional” and “Familial”.
Each strategy is deﬁned as the proportion of the nodes that will receive the
message from each type of links. Hence, we have to deﬁne four proportions for
each propagation strategy. To be as near as possible to the reality, we added a
noise rate to the strategy. We note that the noise value can be added or removed
from the proportions of kind of messages. We used the euclidean distance for the
probabilistic classiﬁer:
dE (Pr1, Pr2) =




card

i=1
(Pr1 (i) −Pr2 (i))2
(1)
and the Jousselme distance [11] for the evidential one:
dJ (m1, m2) =

1
2 (m1 −m2)T D
= (m1 −m2)
(2)
such that D
= is an 2n × 2n matrix and D (A, B) = |A∩B|
A∪B . We ﬁxed the number
of levels in the network to three (three iterations in the propagation algorithm).

Classiﬁcation of Message Spreading in a Heterogeneous Social Network
73
Then we run the proposed propagation algorithm to create a training set for
each propagation strategy, we ﬁxed the size of the strategy training set to 100
propagation networks. Also, we created a testing set of size 100.
5.3
Results and Discussion
In this section we present our results and a comparison between the proba-
bilistic and the evidential classiﬁer. To obtain accurate results we turned the
experimental process ten times and we take the mean of the percentage of cor-
rectly classiﬁed (PCC) propagation networks. Figure 2 shows the impact of the
propagation level on the PCC of the probabilistic results (ﬁgure 2a) and the ev-
idential results (ﬁgure 2b). Figures 2a and 2b illustrate that the PCC increases
when the propagation level increases and we observe this fact starting from the
noise level 20%. In ﬁgure 2a we observe that the curve of the second level coin-
cides with the curve of third level and practically there is no improvement in the
PCC. However, in ﬁgure 2b (evidential results), we note that the PCC increases
with the propagation level, this fact is observed starting from the noise rate 20%.
Hence, we have the PCC of the third level greater than the PCC of the ﬁrst and
the second levels, and the PCC of the second level is higher than the PCC of
the ﬁrst one. Therefore, more the message propagates in the network, more we
can characterize it.
(a) Probabilistic results
(b) Evidential results
Fig. 2. The impact of the propagation level on the PCC
In ﬁgure 3, we compare the probabilistic and the evidential results of the
third propagation level. We note that without noise (0%) the probabilistic PCC
is about 96% (with a 95% conﬁdence interval of ±1.27) and the evidential PCC
is equal to 93% (with a 95% conﬁdence interval of ±1.60), but in real world
social networks the absence of the noise is an ideal fact and cannot be realistic.
When the noise rate increases, the curve shows that the percentage of correctly
classiﬁed propagation networks (messages) decreases. However, we see that the
evidential (Belief) PCC starts to be greater than the probabilistic (Proba) one.

74
S. Jendoubi et al.
We observe this fact from the noise rate 20% where we have an evidential PCC
equals to 70.7% (±4.33) and a probabilistic PCC equals to 65.8% (±4.18). Thus,
we can conclude that the evidential classiﬁer is more robust against the noise
and gives better classiﬁcation rates than the probabilistic classiﬁer.
Fig. 3. Comparison between probabilistic results and evidential results (level three)
6
Conclusion
To conclude, we presented a state of the art of the information propagation, clas-
siﬁcation of social messages and the evidence theory. Then, we proposed an algo-
rithm of information propagation in a heterogeneous social network. Thereafter
we introduced a new evidential classiﬁcation approach that classiﬁes message
propagation in a heterogeneous social network. Finally, we presented some ex-
periments and we noticed the performance of the evidential classiﬁer against the
probabilistic one in noisy cases. Moreover, we observed that when the propaga-
tion level increases, the message class becomes more accurate and more realistic.
For future works, we will compare our propagation algorithm with previous
algorithms. Also, we will search to improve it by the management of the uncer-
tainty and the imprecision related to types of relationships between social actors.
Our next goal is therefore to deﬁne a message propagation algorithm that takes
into account the uncertainty of the types of relationships that is deﬁned on the
links, also we will search to consider the heterogeneity of nodes in the network.
Second, we will run our classiﬁcation algorithm with a more complex heteroge-
neous social network in order to prove its applicability.
References
1. Anderson, R.M., May, R.M.: Infectious Diseases of Humans. Oxford University
Press (1991)
2. Aregui, A., Denœux, T.: Consonant belief function induced by a conﬁdence set
of pignistic probabilities. In: Mellouli, K. (ed.) ECSQARU 2007. LNCS (LNAI),
vol. 4724, pp. 344–355. Springer, Heidelberg (2007)

Classiﬁcation of Message Spreading in a Heterogeneous Social Network
75
3. Aregui, A., Denoeux, T.: Constructing consonant belief functions from sample data
using conﬁdence sets of pignistic probabilities. Int. J. Approx. Reasoning 49(3),
575–594 (2008)
4. Dempster, A.P.: Upper and Lower probabilities induced by a multivalued mapping.
Annals of Mathematical Statistics 38, 325–339 (1967)
5. Galuba, W., Aberer, K., Chakraborty, D., Despotovic, Z., Kellerer, W.: Outtweet-
ing the twitterers - predicting information cascades in microblogs. In: WOSN 2010,
pp. 3–11 (2010)
6. Goldenberg, J., Libai, B., Muller, E.: Talk of the network: A complex systems
look at the underlying process of word-of-mouth. Marketing Letters 12(3), 211–
223 (2001)
7. Granovetter, M.: Threshold models of collective behavior. American Journal of
Sociology, 1420–1443 (1978)
8. Guille, A., Hacid, H., Favre, C., Zighed, D.A.: Information diﬀusion in online social
networks: a survey. SIGMOD Rec. 42(1), 17–28 (2013)
9. Hansen, D.L., Shneiderman, B., Smith, M.A.: Analysing social media network with
nodeXL insights from a connected world. Elsevier Inc. (2011)
10. He, W., Zhab, S., Li, L.: Social media competitive analysis and text mining: A case
study in the pizza industry. International Journal of Information Management 33,
464–472 (2013)
11. Jousselme, A.L., Grenier, D., Bossé, E.: A new distance between two bodies of
evidence. Information Fusion 2, 91–101 (2001)
12. Kempe, D., Kleinberg, J., Tardos, E.: Maximizing the spread of inﬂuence through
a social network. In: Proceedings of the Ninth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, KDD 2003, pp. 137–146. ACM
Press (2003)
13. Li, C.T., Lin, S.D., Shan, M.K.: Inﬂuence propagation and maximization for hetero-
geneous social networks. In: WWW 2012-Poster Presentation, pp. 559–560 (April
2012)
14. Lo, Y.W., Potdar, V.: A review of opinion mining and sentiment classiﬁcation
framework in social networks. In: 3rd IEEE International Conference on Digital
Ecosystems and Technologies, DEST 2009 (June 2009)
15. Mostafa, M.M.: More than words: Social networks text mining for consumer brand
sentiments. Expert Systems with Applications 40, 4241–4251 (2013)
16. Nettleton, D.F.: Survey data mining of social networks represented as graphs. Com-
puter Sciences Review 7, 1–34 (2013)
17. Newman, M.E.J.: Networks: An introduction. Oxford University Press (2010)
18. Saito, K., Ohara, K., Yamagishi, Y., Kimura, M., Motoda, H.: Learning diﬀu-
sion probability based on node attributes in social networks. In: Kryszkiewicz, M.,
Rybinski, H., Skowron, A., Raś, Z.W. (eds.) ISMIS 2011. LNCS, vol. 6804, pp.
153–162. Springer, Heidelberg (2011)
19. Sermpezis, P., Spyropoulos, T.: Information diﬀusion in heterogeneous networks:
The conﬁguration model approach. In: 2013 Proceedings IEEE INFOCOM, pp.
3261–3266 (April 2013)
20. Shafer, G.: A mathematical theory of evidence. Princeton University Press (1976)
21. Smets, P.: Belief Functions: the Disjunctive Rule of Combination and the General-
ized Bayesian Theorem. International Journal of Approximate Reasoning 9, 1–35
(1993)

Measures of Semantic Similarity of Nodes
in a Social Network
Ahmad Rawashdeh1, Mohammad Rawashdeh1, Irene D´ıaz2, and Anca Ralescu1
1 EECS Department, University of Cincinnati, Cincinnati, OH 45221-0030, USA
2 Computer Science Department, University of Oviedo, Spain
{rawashay,rawashmy}@mail.uc.edu, sirene@uniovi.es, Anca.Ralescu@uc.edu
Abstract. Assessing the similarity between node proﬁles in a social net-
work is an important tool in its analysis. Several approaches exist to
study proﬁle similarity, including semantic approaches and natural lan-
guage processing. However, to date there is no research combining these
aspects into a uniﬁed measure of proﬁle similarity. Traditionally, semantic
similarity is assessed using keywords, that is, formatted text information,
with no natural language processing component. This study proposes an
alternative approach, whereby the similarity assessment based on key-
words is applied to the output of natural language processing of proﬁles.
A uniﬁed similarity measure results from this approach. The approach
is illustrated on a real data set extracted from Facebook and compared
with other similarity measures for the same data.
1
Introduction
Social networks allow people to connect and share their personal details. Many
social networking websites have been created and they vary in the services which
they provide. Mainly, they allow users to comment and post pictures or video
and share.
Facebook is a social networking website that has over one billion users. It
allows the user to connect to friends, create personal proﬁles by specifying their
interest –TV, movies, sports, and books – and by posting images and videos of
their activities. The website also allows anyone to create pages for their business
or favorite personality. Users can even create pages for special interest groups
which are open on a restricted basis to group members.[6]
People tend to form relationships with people who are similar to them. Alter-
natively, it can be said that if a relationship is formed between two people, then
there must be some similarity between them. Indeed, it has been found that 80%
of social network users form relationships with the contact of their friends [3].
Analysis of similarity between Facebook proﬁles can be assessed from the
study of keyword similarity [3]. To ﬁnd the relationship between the keywords,
these are arranged in a hierarchical structure to form trees of diﬀerent heights. In
the forest model more than one tree is generated for each proﬁle. Related words
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 76–85, 2014.
c
⃝Springer International Publishing Switzerland 2014

Semantic Similarity of Nodes in a Social Network
77
are retrieved by search in these proﬁle trees, implemented as heuristic search.
Semantic relationships between the words can be assessed by using Wordnet.
[10]
This study proposes to ﬁnd the semantic relationship between attribute entries
in the social network, not only between keywords. Therefore the category of the
words which appear in these entries must be found. This can be accomplished
by using a tagger, a program which tags a word by its semantic category. These
categories are used to extract the words suitable to assess proﬁle similarity [1].
The (semantic) distance between proﬁles is very important to this process, as it
has been shown that the similarity between proﬁles deteriorates as the distance
between them increases [4].
2
Finding Similar Proﬁles
The approach to proﬁle similarity proposed here, semantic distance based simi-
larity, combines Wordnet [8] with the cosine similarity, which is a very common
device to assess document similarity [9]. The results obtained are compared with
other similarity measures, including, (i) occurrence frequency, (ii) set similarity
(Jaccard index), (iii) syntactic similarity, (iv) word frequency vector similarity.
Before deﬁning each of these measures of similarity, Wordnet and the semantic
tagger are brieﬂy described.
2.1
Wordnet
Wordnet is a free lexical database that organizes English words into concepts
and relations, well-known for assessing semantic similarity. English nouns, verbs,
adjectives, and adverbs form hierarchies of synset where relations exist that
connect them. The relations are Synonymy, Antonymy, Hypernymy, Meronymy,
Troponymy, Entailment.
Hypernym of a Word. Hypernym of a word conveys its place in a hierarchy
of concepts/words and can be retrieved using Wordnet. Consider for example,
the two senses of word ”comedy”:
– comedy as a ”humorous drama”
– comedy as ”comic incident”
Taking the ﬁrst sense, since comedy is kind of drama, drama is a hypernym
of comedy. Similarly, since drama is kind of literary work, literary work is a
hypernym of drama [8]. The hierarchy determined by the hypernym relationship
is a synset. Therefore, based on the above, the synset for comedy (with respect
to the ﬁrst meaning) is

78
A. Rawashdeh et al.
Synset 1: [entity] ←[abstract entity] ←[abstraction] ←[communication]
←[expressive style,style] ←[writing style,literary genre,genre]
←[drama] ←[comedy] -
light and humorous drama with a happy ending
(1)
while the Synset with respect to the second meaning is:
Synset 2: [entity] ←[abstract entity] ←[abstraction]
←[communication] ←[message,content,subject matter,substance]
←[wit, humor, humor, witticism, wittiness] ←[fun, play,sport]
←[drollery, clowning, comedy, funniness] -
a comic incident or series of incidents
(2)
2.2
Semantic Tagger
A semantic tagger takes as input text and outputs a collection of word-tags pairs
(w, tw) [7]. Tags denote semantic categories. The innovative aspect of the current
study is the use of tags instead of words to assess proﬁle similarity. The tags
used in this study are shown in Table 1.
Table 1. Word tags and their descriptions [1]
Tag
Description
NN
noun, proper, singular or mass
NNP
noun, proper, singular
NNS
noun, common, plural
NNPS
noun, proper, plural
3
Semantic Similarity Measures
The literature of intelligent data processing, including information retrieval, doc-
ument processing, etc. is rich in similarity measures. The similarity measures
considered here are all adapted to deal with word tags /semantic categories
rather than (key) words. For each of the measures described below Di, i = 1, 2
denote two documents or proﬁles.
3.1
Cosine Similarity for Vectors
Cosine similarity for vectors [9] has been successfully used as measure of simi-
larity between documents. A vocabulary of size N of words of interest is deﬁned
and each document is described by an N dimensional vector of word frequen-
cies. The similarity of two documents is then based on the cosine of the angle

Semantic Similarity of Nodes in a Social Network
79
made between their corresponding vectors. More precisely, given the documents
Di, i = 1, 2, with corresponding word frequency vectors v1 and v2, the cosine
similarity between D1 and D2 is deﬁned as
CS(D1, D2) =
v1 · v2
∥v1∥∥v2∥
(3)
where · is the dot product between two vectors, and ∥v∥denotes the norm of a
vector v. Since frequencies are positive quantities, it can be easily seen that this
similarity is consistent with the distance between vectors.
3.2
Wordnet Cosine Similarity
Adapting to the encoding using Wordnet and tags, the cosine similarity is applied
to the vectors of the frequencies of tags (rather than vectors of frequencies of
word) used to encode a proﬁle. This means that two proﬁles that contain diﬀerent
words can be evaluated as similar (even identical) if these words fall into the same
semantic categories.
3.3
Set-Based Similarity
A frequently used, set-based similarity is the well-known Jaccard Index. Given
two subsets A and B of the same universe of discourse, their similarity is J(A, B)
is deﬁned by
J(A, B) = |A ∩B|
|A ∪B|
(4)
where |A| denotes the size of the set A. When Jaccard index is used in con-
junction with Wordnet, given two documents, D1 and D2, their corresponding
Jaccard index is computed by equation (5).
J(D1, D2) = |P1 ∩P2|
|P1 ∪P2|
(5)
where Pi denotes the collection of semantic categories from the parent sets for
each word in the document Di, i = 1, 2.
3.4
The Occurrence Frequency Similarity(OF)
Let D and D′ denote two documents/proﬁles, each having the multiple valued
attribute i. Following the work in [2] the occurrence frequency similarity measure
between D and D′ is deﬁned by equation (6).
OF(iD, iD′) =

1
if iD.n = iD′.n
1
B
B
k=1(1 + A × B)−1 if iD.n ̸= iD′.n
(6)
where iD denotes the value of attribute i in the proﬁle D, iD.n denotes the value
of the nth subﬁeld for iD, N is the total number of item values, and f(·) is the
number of records; A = log(
N
1+f(iu.n)), and B = log(
N
f(ix.k)).

80
A. Rawashdeh et al.
4
A Uniﬁed Similarity Measure: Wordnet-Cosine
Similarity
A uniﬁed similarity measure takes into account the semantics of the input, en-
codes it into some numerical representation, and computes the similarity based
on this representation. The general algorithm to compute such a measure for
two documents/proﬁles is as shown below:
1. Extract the text in the feature ﬁeld (movies, title) if the data-set is not
formatted well.
2. Natural Language Processing: Parse the sentence to obtain its structure.
3. Get the ﬁrst synset of the word using Wordnet.
4. Encode the word
– Get all hypernym of the synset of the word.
– Find the distance from the word to the root of the synset.
5. Each feature ﬁeld of a proﬁle is encoded as a vector of such distances.
6. Apply cosine similarity between vectors of such distances.
Each proﬁle is represented as a collection of word-tag pairs (w, tw). Given a word-
tag pair, (w, tw), w is considered for inclusion in the similarity evaluation if and
only if tw ∈T ags of Table 1. Each selected word, w is input to Wordnet which
returns the list of hypernyms, in the hierarchical synset representation of w.
As already mentioned, in this study, only the ﬁrst sysnset is used for similarity
assessment. The encoding of w is the distance to it from the top hypernym
(’entity’) in the synset. For example, the encoding of the word ”comedy” based
on the ﬁrst synset 1 is equal to 7. If a word has no hypernym (e.g., it is not in
Wordnet) then its encoding is 0. Therefore, for each word wi, use Wordnet to
extract its ﬁrst synset and encode it as di = d(wi) where, for a given word w,
d(w) =

dist(w, [entity]) if w is in Wordnet
0
otherwise
(7)
where dist is the distance to [entity], the top hypernym of w in its ﬁrst synset,
output by Wordnet. The encoding of the proﬁle D is a mapping e : D →ℜk
+
such that
e(D) = (d1, . . . , dk)
Given two proﬁles, D, and D′ and their corresponding encoding e(D) =
(d1, . . . , dk) and e(D′) = (d′
1, . . . , d′
k) the similarity between D and D′ is de-
ﬁned as the cosine similarity of e(D) and e(D′), as shown in equation (8)
Sim(D, D′) = CS(e(D), e(D′))
(8)
where CS is deﬁned as in equation (3). The process described above converts
the problem of similarity assessment between unstructured data into a more
rigorously deﬁned problem of similarity between real valued vectors. In principle,
it is possible, for a given word w (and hence for a proﬁle), to obtain more than
one encoding, by using all the synsets to encode a line of text using several
synsets. However, this case is beyond the scope of the current study. Figure 1
illustrates the approach proposed in this study and described above.

Semantic Similarity of Nodes in a Social Network
81
Fig. 1. Diagram for computing the uniﬁed similarity measure using Wordnet and cosine
similarity
5
Experimental Results
The approach described in the preceding section is applied to a Facebook data
set as shown next.
5.1
Facebook Proﬁles Data-Set
The Facebook data-set considered in experiments contains 2013 proﬁle pages
from Facebook (raw data before the introduction of the Facebook time-line).
Skull security has a list of publicly available Facebook URLs which is used to
download this data-set that consists of 2013 proﬁles [5]. More speciﬁcally, Data-
set.txt (Facebook Data-set) contains all the movies interest for diﬀerent Facebook
proﬁle numbers. The format of the data-set is as follows: Proﬁle id followed by
the Movies interest entered by the user identiﬁed by the Proﬁle id. Furthermore,
various characteristics are extracted from the Facebook Data-set, as shown in
Table 2. Figure 2 shows the frequency of the top 20 movies in the Facebook
data-set.
Table 3 illustrates the encoding the Movie Attribute for three Facebook pro-
ﬁles.
Table 2. Characteristics of the Facebook proﬁle data
Number of Facebook proﬁles
2013
Average movies entries per proﬁle
2.9
Number of movies entries for all proﬁles
1744
Maximum movies entries
8
Most Common Genre type 1
which is the genre type ”unknown”
Minimum movies entries
0
Diﬀerent movies count
1089

82
A. Rawashdeh et al.
Fig. 2. Frequency of the top 20 movies from the Facebook data-set
Table 3. Illustration of Movie Attribute of Facebook proﬁles: their tags and Hypernyms
Proﬁle 1: Movie Attribute
Harry Potter, Transformers, Mr. & Mrs. Smith
Words
Harry
Potter Transformers
Mr.
&
Mrs. Smith
Tags
NNP
NNP
NNPS
NNP
CC
NNP
NNP
dist to root in synset
0
7
8
8
ignored
8
0
Proﬁle 2: Movie Attribute
Sherina’s Adventure
Words
Sherina
’s
Adventure
Tags
NNP
POS
NNP
dist to root in synset
0
ignored
8
Proﬁle 3: Movie Attribute
Love mein Gum, Maqsood Jutt Dog Fighter
Words
Love
mein
Gum
Maqsood
Jutt
Fog Fighter
Tags
NNP
NNP
NNP
NNP
NNP
NNP
NNP
dist to root in synset
7
0
7
0
0
6
4
5.2
Results
All the similarity measures described here, including the algorithm of [2], were
implemented in Java. In the ﬁrst set of experiments, the similarity was calculated
between each adjacent nodes’ row in the data-set using both the OF measure
and Wordnet-cosine approach. Table 4 illustrates these similarity results for two
proﬁles.
Figure 3 shows the result of applying the OF similarity and the Wordnet-
cosine similarity for all the node pairs connected by an edge in the data set.
Using OF, most of the data are similar, with similarity value equal to 1. By
contrast, using Wordnet, the similarity values are distributed over all the data
having a peak value at 0.2.
In the second experiment, for the same data, the similarities obtained by using
Wordnet cosine similarity, Jaccard index, semantic similarity and vector cosine

Semantic Similarity of Nodes in a Social Network
83
Table 4. OF and Wordnet Similarity of two Facebook proﬁles along their Movie At-
tribute. Proﬁle IDs are partially masked for privacy.
Data Set
Facebook
Proﬁle-1 ID
10000006XXXXXX.html
Movies Interests
Captain Jack Sparrow, Meet The Spartans, Ice Age Movie, Spider-Man
Proﬁle-2 ID
100000067XXXXXX.html
Movies Interests
Clash of the Titans, Ratatouille, Independence Day, Mr. Nice Guy,
The Lord of the Rings Trilogy (Oﬃcial Page)
OF Similarity
0.9472
Wordnet based similarity 0.1892
Fig. 3. OF and Wordnet similarity results for the Facebook data-set
Table 5. Results for the similarity of two proﬁles based on the four similarities measures
used in this study. Proﬁle IDs are partially masked.
Proﬁle ID
Proﬁle
132XXXXXXX.html
Comedy, Action ﬁlms, American, El El
774XXXXXXX.html
Haunted 3D, Saw, Transformers, Pirates of the Caribbean, Mind Hunter
SIMILARITY MEASURES
Wordnet cosine Set similarity Semantic similarity
Word frequency vector similarity
0.862795963
0.0659340066
0.877526909
0.74900588
similarity were compared. Table 5 shows the diﬀerence in the similarity results
for two proﬁles, using four similarity measures. The results for all four similarity
measures on the Facebook data set, are shown in Figure 4.

84
A. Rawashdeh et al.
Fig. 4. Similarity using four diﬀerent similarity measures
6
Conclusions
This study introduces a new approach towards a uniﬁed measure of similarity
between node proﬁles, and in general, between pieces of unstructured text. Natu-
ral language processing is used to extract speech parts from the texts of interest,
and to encode them into vectors with positive components using the distance
between the words extracted to the root of a hierarchy of concepts. Similarity is
then evaluated between the resultant encoding vectors. While the results seem
promising, several issues remain to be discussed and developed in subsequent
studies.
References
1. The university of pennsylvania (penn) treebank tag-set,
http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html
(accessed October 1, 2013)
2. Akcora, C.G., Carminati, B., Ferrari, E.: Network and proﬁle based measures for
user similarities on social networks. In: 2011 IEEE International Conference on
Information Reuse and Integration (IRI), pp. 292–298. IEEE (2011)
3. Bhattacharyya, P., Garg, A., Wu, S.F.: Analysis of user keyword similarity in online
social networks. Social Network Analysis and Mining 1(3), 143–158 (2011)
4. Boriah, S., Chandola, V., Kumar, V.: Similarity measures for categorical data:
A comparative evaluation. Red 30(2), 3 (2008)
5. Bowes, R.: Return of the Facebook Snatchers (2012),
http://www.skullsecurity.org/blog/2010/
return-of-the-facebook-snatchers (accessed July 19, 2012)
6. Ellison, N.B., et al.: Social network sites: Deﬁnition, history, and scholarship. Jour-
nal of Computer-Mediated Communication 13(1), 210–230 (2007)

Semantic Similarity of Nodes in a Social Network
85
7. The Stanford Natural Language Processing Group. Pos Tagger FAQ,
http://nlp.stanford.edu/software/pos-tagger-faq.shtml
(accessed July 19, 2012)
8. Miller, G.A.: Wordnet: a lexical database for english. Communications of the
ACM 38(11), 39–41 (1995)
9. Peat, H.J., Willett, P.: The limitations of term co-occurrence data for query ex-
pansion in document retrieval systems. JASIS 42(5), 378–383 (1991)
10. Spear, M., Lu, X., Matloﬀ, N.S., Wu, S.F.: Inter-proﬁle similarity (IPS): A method
for semantic analysis of online social networks. In: Zhou, J. (ed.) Complex 2009.
LNICST, vol. 4, pp. 320–333. Springer, Heidelberg (2009)

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 86–92, 2014. 
© Springer International Publishing Switzerland 2014  
Imitation and the Generative Mind 
Jacqueline Nadel 
CNRS Centre Emotion, Pierre & Marie Curie University, ICM,   
La Salpêtrière Hospital, Paris, France 
jacqueline.nadel@upmc.fr 
Abstract. In its perpetual capacity to imagine, create and revisit artifacts and 
representations, human mind is the perfect example of generativity. Yet if we 
agree with Epstein (1996)’s theory of generativity, new ideas result from inter-
connections among old ones. That is, cultural knowledge heavily influences our 
individual minds. In this line, our minds need meeting other minds to generate 
innovation. I will argue in this article that the basis of a generative meeting be-
tween minds is imitation. This proposal is developed against the well-
established reputation of imitation as an idiotic behaviour stifling creativity.  
Keywords: imitation, generativity, flexibility, development, brain dynamics. 
1 
Introduction 
An ancient tradition hinders the reputation of imitation. This tradition comes from the 
great philosopher Plato. Plato described imitation as dangerous because it stifles crea-
tivity, hampers the development of personal identity and disrupts the perception of 
other people as unique beings. Girard recalls in his book, ‘Things hidden since the 
foundation of the world’ (Girard, 1987), that in certain cultures, one child out of every 
set of twins would be killed, as would a son who looked too much like his father. 
Who exactly was at risk, in a world where such little importance was given to the 
concept of individuality? Surely the danger was not for the imitator but rather for the 
social group, where too close a physical resemblance might have caused confusion 
about roles in the community (Vernant, 1983). It remains that for centuries and centu-
ries, imitation has been an object of contempt. For instance, Piaget (1945) called “in-
telligent imitation,” a reproduction that is not stuck in the present (i.e., imitating an 
absent model), nor is it stuck with what the infant already knows how to do; thanks to 
representation, an action can be performed without requiring a direct perception of it. 
In sum, according to Piaget’s theory of intelligence, simply doing what the other does 
is not ‘generative’. The aim of this paper is to demonstrate that this view does not take 
into account the capacities required in order to imitate, and the generativity it allows 
to brain, behavior and mind. 

 
Imitation and the Generative Mind 
87 
 
2 
Generativity and the Meeting of Minds 
Generativity is described via Wikipedia as ‘a self-contained system from which its 
user draws an independent ability to create, generate, or produce new content unique 
to that system without additional help or input from the system's original creators’. In 
its perpetual capacity to imagine, create and revisit artifacts and representations, hu-
man mind is the perfect example of generativity. As a consequence, human mind 
inspires generative models in computer modelling. Linguistic theories such as the 
famous Chomsky’s theory have emphasized the unique role of language in the ex-
pression of our generative structure of mind (Chomsky, 1985). Language offers us the 
means to express our thoughts through unique pieces of linguistic creation.  These 
pieces are built thanks to a generative and transformational grammar that possesses 
compositionality (Dennett, 1971) and provides us the capacity to construct complex 
messages. Chomsky’s model of generative syntax contributes to the theory of mind’s 
perspective. It meets probabilistic generative models that aim to infer invisible va-
riables of the investigated phenomenon on the basis of visible variables.  Indeed it is 
what we do each time we infer the unobservable mental states of others on the basis 
of probabilistic computation on observed events  Now suppose we adopt Ziffrain 
(2008)’s definition of web generativity as “ a system's capacity to produce unantici-
pated change through unfiltered contributions from broad and varied audiences." 
Then, we have to broaden our definition of a system to the assembly of two or more 
persons. In this view, our mind possesses means to produce generativity proviso it 
works in concert with other minds.  
3 
Social Cognition and Social Interaction in Cognitive Sciences 
This way of thinking is in line with a burgeoning field in cognitive and neurocogni-
tive sciences.  After a long focus on mentalizing processes studied in subjects in isola-
tion, cognitive sciences are now turning to analyze the role of social cognition in  
online social interaction. Yet the need for a clear-cut distinction, at the theoretical and 
methodological levels, between the generic term of social cognition and the specific 
phenomenon described as social interaction is not shared by all specialists in the field. 
A traditional cognitive interpretation holds that the brain is simply entering another 
mode of functioning when immerged in a social interactive context. Of course social 
interaction mostly involves social cognition as an underlying process by which hu-
mans understand, anticipate, or infer the intentional behavior of others. Moreover, it is 
the place where social cognition most frequently occurs in everyday life. Yet social 
interaction is a specific online phenomenon which cannot be considered merely as a 
category of inputs to be processed by individual mechanisms (De Jaeger & Di Paolo, 
2012; Dumas, Martinerie, Soussignan & Nadel, 2012).  The reason is that social inte-
raction is a co-regulated coupling between at least two agents who are mutually  
influencing each other. This definition and the underlying dynamical theory are  
currently gaining ground in social neuroscience against a solipsistic view of the  
generative mind.  

88 
J. Nadel 
 
4 
Developmental Psychology, Imitation and the ‘Two-Person’ 
Stance   
Inspiration for an alternative approach to social interaction comes from developmen-
tal psychology, in which, beginning in the 1970s, attention turned to studying real-
time dynamic interactions involving two or more partners. The emphasis was then put 
on dyadic variables (Nadel & Camaioni, 1993) such as imitation, joint attention, turn-
taking and coregulation, various aspects of which have been referred to as co-
regulation (Fogel, 1993), synchrony (Trevarthen, 1977), or harmonization (Stern, 
1977). The concept of coregulation suggests a dynamically changing individual dur-
ing the process of transaction with others. In the same line, cognition is considered to 
be constantly evolving in dynamic interactions (Varela, Thompson & Rosch, 1991).  
Immediate imitation is often defined as a social behaviour leading to the individual 
benefit of learning. There is however another function of imitation which fits well the 
two-person perspective. Studying spontaneous imitation in an online meeting of peers 
aged 12, 15, 18, 24, 30, 36 and 42 months, we have shown that young children take 
advantage of the two facets of imitation to get two roles (imitator and model) that they 
switch to take turns (Nadel & Butterworth, 1999). Indeed the dynamics of imitation 
makes it a genuine communicative system which presents the three parameters of any 
interactive system: synchrony, joint attention and turn-taking. Like in conversation, 
roles are exchanged smoothly on the basis of a coregulation. Prepin and Revel (2007) 
have shown that two oscillators facing each other loose progressively thei specific 
tempo and adopt a common tempo different from their own. Similarly, young children 
imitating each other form a system which generates novel common actions differing 
from the repertory of action of each partner: it is literally a generative two-person 
system. Thus, it appears that imitation serves both the traditionally-recognized  
function of promoting skill acquisition and a previously unacknowledged interactive 
function (Andry, Gaussier, Moga, Banquet & Nadel, 2001).. Where does this  
double function of imitation emerge from? 
5 
Neonatal Imitation and the Foundation of Social Interaction 
Social interaction in its basic foundation is well represented by imitation from birth 
on. Literally from birth, typical neonates are able to imitate a tongue protrusion 
(Meltzoff & Moore, 1977). They are even able to imitate a tongue protrusion pre-
sented on a screen (Soussignan, Courtial,  Canet, Danon-Apter & Nadel, 2011). It is 
not a prowess: Protruding tongue is already in the motor repertory of foetuses of ges-
tational age 25 weeks (Piontelli, 2010). The prowess is that they are able to use their 
motor repertory according to their perception. So doing, they relate their motor pat-
terns to the others’ motor patterns. Moreover, the newborns match more and more  
exactly the perceived stimulus after repeated attempts, which shows that they are able 
to modulate their motor repertoire (Soussignan et al, 2011).  Thus neonatal imitation, 
though experience- dependent, adapts action to perception with great plasticity. For 
Lepage and Théoret (2007), this plasticity renders plausible the hypothesis of a  

 
Imitation and the Generative Mind 
89 
 
gradual development of the Mirror neuron System (MNS) through repeated motor 
activity and related sensory feed-backs of the foetus. Similarly, the adult MNS is ex-
perience-dependent and plastic: our mirror neurons resonate to the observation of  
actions that are not part of our motor repertoire only after repeated exposure  
(Calvo-Merino et al., 2005).This demonstration of a flexible repertoire is of para-
mount importance. Indeed the individual deprived of social encounters would not 
have the opportunity to enrich their repertory according to the observed actions of the 
others. It is the beginning of a perception-action coupling that will take many differ-
ent forms, from acts to thoughts, but will never stop.  A few months later, having 
enriched their motor repertoire thanks to their matching of others’ actions, toddlers 
will start being able to store representations of actions they have never done: How? 
The storage originates from somatotopic and proprioceptive recalls of past experience 
(Raos, Evangeliou &Savaki, 2007) involving elements of the observed actions. This 
mental recombination is a powerful multiplier of experiences as it prints in our memo-
ry of actions those actions performed by others that we have observed but never done. 
This is possible, proviso we have elements of the observed action in our repertory. 
Then we can build new possibilities with old ones -a basic illustration of Epstein 
(1996)’s theory which asserts that new ideas result from interconnections among old 
ones. Notice that though this novel repertory is built thanks to the actions of the oth-
ers, it is different from the others’ repertory just because it is issued from our own 
history of actions, and our own gestural procedures. From acts to thoughts, the 
process is similar, as shown by Fadiga’s team (Fadiga, Craighero & D’Ausilio, 2009). 
The benefit of innovation is individual here at first but through the process of interac-
tive imitation, it will be revisited as a common and innovative by-product of the  
interaction.   
6 
Imitation and Social Neuroscience 
Inspired by our developmental research, we have built an innovative fMRI platform 
which allows synchronizing behavioural and brain recordings during online imitative 
interaction.  Our results replicated previous findings demonstrating the existence of an 
imitative neural network (Iacoboni et al., 1999), and most importantly revealed the 
involvement of the dorsolateral prefrontal cortex and other regions involved in social 
anticipation and adjustment, thus verifying that reciprocal imitation is a prototype of 
two-person coregulation ( Guionnet, Nadel, Bertasi, Delaveau, Sperduti, & Fossati, 
2011). Our fMRI work thus supported the notion that imitation is a useful model for 
two-person neuroscience.  
Two-person neuroscience aimed at investigating the simultaneous activity of two 
brains recorded simultaneously during a dyadic encounter. The novel technique 
known as ‘hyperscanning’ allows for simultaneous recording (through fMRI or EEG) 
of brain activity in multiple participants, facilitating both within- and between-brain 
analyses.  We used hyperscanning in a dyadic context of free interaction and it was 
the first experiment of this kind, to our own knowledge. The dyads were composed of 
two unacquainted subjects seated in separate experimental cabins and viewing each 
other’s hand gestures. Dyads engaged in imitation (i.e., made hand gestures of similar 

90 
J. Nadel 
 
morphology) roughly about 65% of the time and synchronized hand movements (i.e., 
gestures began and ended at the same time, but did not necessarily share the same 
morphology) about 78% of the time. Within each dyad, we observed a spontaneous 
emergence of a balanced turn taking between the role of model and imitator; EEG 
data showed emergent synchronization of brainwaves in subjects who were engaged 
in spontaneous imitation with interactional synchrony (Dumas,  Nadel, Soussignan, 
Martinerie & Garnero,2010). This inter-brain relationship was strongly present in the 
alpha-mu frequency band where it symmetrically linked the right parietal regions of 
the two subjects (Figure 5B). Inter-brain synchronization of right parietal regions in 
this range of rhythmic activity suggests a link between inter-individual coordination 
and the intra-individual temporal estimation and anticipation necessary for an effec-
tive alternation of roles (Wilson and Wilson, 2005). Interbrain synchronization was 
also observed in higher frequency bands, though not between homologous brain re-
gions according to the role of imitator or model. (Dumas, Martinerie, Soussignan & 
Nadel,. 2012). 
Besides an understanding of the other’s action, turn-taking requires anticipation of 
other’s intention and active co-regulation of complementary action on the part of the 
two partners. Our PsychoPhysical Interaction results suggest that these sophisticated 
aspects of an ongoing social interaction involve both the mirror and the mentalizing 
systems (Sperduti, Guionnet, Delaveau, Fossati & Nadel, 2014). The mirror system 
allows understand and anticipate action schemes leading to synchronized actions  
and the mentalizing system accounts for the novelty emerging from the imitative  
interaction.   
7 
Conclusion  
Cognition involving others, or social cognition, is often conceptualized as the solitary 
third person computation of mental states. Relatively little attention has been paid to 
how individuals use their cognitive capacities at the behavioral and brain levels in 
social exchanges. We introduced imitation as a valuable model of dynamic social 
interactive phenomenon, and described laboratory procedures for studying it in behav-
ioral and neuroimaging contexts. From birth on, imitation allows us to continuously 
revisit our resources thanks to the observation of others. Interacting with others multi-
plies the effect of observation. Indeed it generates novelty emerging from the dynamic 
coregulation of two different repertories that couple perception and action and antici-
pate each other’s responses.  
We reviewed research that reveals behavioural and neural synchronization of indi-
viduals engaged in imitation. In the latter case, brain activity is correlated in imitative 
partners but the pattern expressed by an individual depends on the individual's role 
(i.e., model or imitator). We linked  these findings to theoretical notions about mirror-
ing and mentalizing brain systems, and then described how mirroring and mentalizing 
support the notion of generative cognition, even in basic forms of communication 
such as reciprocal imitation. And finally we showed that the traditional view of imita-
tion does not take into account the exceptional potential of generativity that it allows. 

 
Imitation and the Generative Mind 
91 
 
References 
Andry, P., Gaussier, P., Moga, S., Banquet, J.P., Nadel, J.: Learning and communication in 
imitation: an autonomous robot perspective. IEEE Transactions on Systems, Man and  
Cybernetics 31, 431–444 (2001) 
Calvo-Merino, B., Glaser, D.E., Grèzes, J., Passingham, R.E., Haggard, P.: Action observation 
and acquiredmotor skills: an fMRI study with expert dancers. Cerebral Cortex 15, 1243–
1249 (2005) 
Chomsky, N.: Aspects of the theory of syntax. MIT Press, Cambridge (1965) 
Dennett, D.C.: Intentional systems. Journal of Philosophy 68, 87–106 (1971) 
De Jaegher, H., Di Paolo, E.: Enactivism is not interactionism. Frontiers in Human Neuros-
cience 6 (2012), doi:10.3389/fnhum.00128 
Di Paolo, E., De Jaegher, H.: The interactive brain hypothesis. Frontiers in Human  
Neuroscience 6 (2012), doi:10.3389/fnhum.2012.00128 
Dumas, G., Nadel, J., Soussignan, R., Martinerie, J., Garnero, L.: Interbrain synchronization 
durting social interaction. PlosOne 5, e12166 (2010) 
Dumas, G., Martinerie, J., Soussignan, R., Nadel, J.: Does the brain know who is at the origin 
of what in an imitative interaction? Frontiers in Human Neuroscience 6 (2012), 
doi:10.3389/fnhum.2012.00128 
Epstein, R.: Cognition, Creativity, and Behavior: Selected Essays. Praeger, München (1996) 
Fadiga, L., Craighero, L., D’Audilio, A.: Broca’s area in language, action and music. Ann. N.Y. 
Acad; Sci. 119, 448–458 (2009) 
Fogel, A.: Two principles of communication: Co-regulation and framing. In: Nadel, J., Ca-
maioni, L. (eds.) New Perspectives in Communicative Development, pp. 9–22 
Girard, R.: Things hidden since the foundation of the world (1987). Stanford University Press, 
Stanford (2005) (French edition: 1978) 
Guionnet, S., Nadel, J., Bertasi, E., Sperduti, M., Delaveau, P., Fossati, P.: Reciprocal imita-
tion: toward a neural basis of social interaction. Cerebral Cortex 22(4), 971–978 (2012) 
Iacoboni, M., Woods, R.P., Brass, M., Bekkering, H., Mazziota, J., Rizzolatti, G.: Cortical 
mechanisms of human imitation. Science 286, 2526–2528 (1999) 
Lepage, J.F., Théoret, H.: The mirror neuron system: grasping others’ actions from birth? De-
velopmental Science 10, 513–523 (2007) 
Meltzoff, A.N., Moore, M.: Newborn infants imitate adult facial gestures. Child Develop-
ment 54, 702–709 (1983) 
Nadel, J., Butterworth, G.: Imitation in infancy. Cambridge University Press, Cambridge 
(1999) 
Nadel, J., Camaioni, L.: New perspectives in communicative development. Routledge, London 
(1993) 
Piaget, J.: Play, dreams and imitation in childhood. Norton (translated from: La formation du 
symbole chez l’enfant), New York (1945/1962) 
Piontelli, A.: Development of normal fetal movements. Springer, Milano (2010) 
Prepin, K., Revel, A.: Human-machine interaction as a model of machine-machine interaction: 
how to make machines interact as humans do. Advanced Robotics 21, 1709–1723 (2007) 
Raos, V., Evangeliou, M.N., Savaki, H.E.: Mental simulation of action in the service of action 
perception. Journal of Neuroscience 27, 12675–12683 (2007) 
Soussignan, R., Courtial, A., Canet, P., Danon-Apter, G., Nadel, J.: Human newborns match 
tongue protrusion of disembodied human and robotic mouths. Developmental Science 
(2010), doi:10.1111/j.1467-7687.2010.00984 

92 
J. Nadel 
 
Sperduti, M., Guionnet, S., Fossati, P., Nadel, J.: Mirror Neuron System and Mentalizing Sys-
tem connect during online social interaction. Cognitive Processing (2014) 
Stern, D.: The first relationship: infant and mother. Harvard University Press, Harvard (1977) 
Trevarthen, C.: Descriptive analyses of infant communicative behaviour. In: Schaffer, H.R. 
(ed.) Studies of Infant-Mother Interaction. The Loch Lomond Symposium, pp. 227–270. 
Academic Press, London (1977) 
Varela, F.J., Thompson, E.T., Rosch, E.: The embodied mind: Cognitive science and human 
experience. MIT Press, Cambridge (1991) 
Vernant, J.-P.: Myth and thought among the Greeks. Routledge & Kegan Paul, London (1983) 
Wilson, M., Wilson, T.: An oscillator model of the timing of turn-taking. Psychon. Bull. 
Rev. 12, 957–968 (2005) 
Zittrain, J.: The future of the internet and how to stop it. Yale University Press, Yale (2008) 
 

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 93–105, 2014. 
© Springer International Publishing Switzerland 2014  
Conditions for Cognitive Plausibility of Computational 
Models of Category Induction 
Daniel Devatman Hromada 
Slovenská Technická Univerzita, Fakulta Elektrotechniky a Informatiky 
Ilkovičova 3, 812 19 Bratislava 1 
hromi@giver.eu 
Abstract. We present two axiomatic and three conjectural conditions which a 
model inducing natural language categories should dispose of, if ever it aims to 
be considered as “cognitively plausible”. 1st axiomatic condition is that the 
model should involve a bootstrapping component. 2nd axiomatic condition is 
that it should be data-driven. 1st conjectural condition demands that the model 
integrates the surface features – related to prosody, phonology and morphology 
– somewhat more intensively than is the case in existing Markov-inspired mod-
els. 2nd conjectural condition demands that asides integrating symbolic and 
connectionist aspects, the model under question should exploit the global geo-
metric and topologic properties of vector-spaces upon which it operates. At last 
we shall argue that model should facilitate qualitative evaluation, for example 
in form of a POS-i oriented Turing Test. In order to support our claims, we shall 
present a POS-induction model based on trivial k-way clustering of vectors 
representing suffixal and co-occurrence information present in parts of Multext-
East corpus. Even in very initial stages of its development, the model succeeds 
to outperform some more complex probabilistic POS-induction models for less-
er computational cost. 
Keywords: categorization, part-of-speech induction, surface features, vector 
spaces, categorization-oriented Turing Test, clustering of formal syntactic anal-
ogies, cognitive plausibility. 
1 
Introduction 
The notion of “cognitive plausibility” and “part-of-speech induction” shall be defined 
in subsection 1.1. Subsection 1.2 shall clarify the position of syntactic category induc-
tion within the field of Natural Language Processing (NLP). The last subsection (1.3) 
shall offer a brief overview of the history of the problem, arguing that the current 
paradigm is probabilistic and English-centered one. 
1.1 
Cognitive Plausibility 
This article enumerates some basic conditions which should be fulfilled, we believe, 
by engineers aiming to transform their computational models into “cognitively  

94 
D.D. Hromada 
 
plausible” artificial agents. We label as “cognitively plausible” a model which tends 
to address some basic function of human cognitive system not only by simulating, in a 
sort of “black-box apparatus”, the mapping of inputs (stimuli, corpus data etc.) upon 
outputs (results), but also tends to faithfully represent the way how the respective 
function/skill is accomplished by a human mind and its material substrate – the brain.  
In other terms, we believe that a cognitively plausible model should not only aim 
to attain the most quantitatively accurate results, but also to do so by processing the 
information similarly to the way mind does it. 
The aim of this article is to elucidate the notion of “cognitive plausibility” (CP) by 
relating it to one particular problem, that of construction of grammatical categories 
present in natural languages. More concretely, we shall try to illustrate our point on 
the problem of construction of part-of-speech (POS) classes. We precise that the term 
POS-induction (POS-i) designates the process which endows the human or an artifi-
cial agent with the competence to attribute the POS-labels (like “verb”, “noun”, “ad-
jective”) to any token observable in agent’s linguistic environment. For the simplicity 
of the argument, only parts of textual corpora like Multext-East (Erjavec, 2012) shall 
be considered as such “linguistic environment” of the computational agent introduced 
below. 
1.2 
Part-of-Speech Induction in NLP and Language Acquisition Studies 
POS-i is often considered to be “one of the most popular tasks in research on unsu-
pervised NLP” (Christodoulopoulos et al., 2010). The problem of construction of 
grammatical categories is closely related to problem of “grammar induction” and 
language acquisition. Since “syntactic category information is part of the basic know-
ledge about language that children must learn before they can acquire more compli-
cated structures” (Schütze, 1993), it is hard to imagine any computational model of 
grammar induction - aiming to discover the set of rules of the grammar of the lan-
guage under study- without it being able to construct, in the first place, the equiva-
lence classes upon which the rules-to-discover shall be applied (Elman, 1989; Solan et 
al., 2005).  
Acquisition of formal grammatical categories, be it parts-of-speech or others, is 
thoroughly studied in psycholinguistic literature – for introductory overview c.f. Levy 
et al. (1988).  For few decades the main motivation in the field was the question 
“whether grammatical categories are innate, or induced through interaction with 
environment by means of imitation and analogy?”. Whole spectrum of answers 
were proposed during the deba, generating as a by-product vase amount of both em-
piric and theoretic knowledge. Such knowledge can be further exploited by engineers 
aiming to bring together disparate disciplines of artificial intelligence and develop-
mental psychology. 
1.3 
POS-i Paradigm(s) 
While already latent in worthy POS-i models, like that of (Elman, 1989) existed  
before, or were published more or less in parallel (Schütze, 1993), the paradigm  

 Conditions for Cognitive Plausibility of Computational Models of Category Induction 
95 
 
currently dominating the POS-i domain was fully born with article published by 
Brown et al. in 1992. Without going into detail, we precise that the model was suc-
cessful because of its ability to apply both Markovian probabilistic concepts and those 
coming from information theory (Shannon & Weaver, 1949) upon the information 
contained in the co-occurrences of the words in the sequences, thus becoming the 
flagship of what we label hereby as “co-occurrence distribution” or “contextual distri-
bution” (CD) paradigm. In decades to follow, the CD paradigm has clearly dominated 
the POS-i field. Be it hidden Markov Models tweaked with variational Bayes (John-
son, 2007) , Gibbs sampling (Goldwater & Griffiths, 2007), morphological features 
(Berg-Kirkpatrick, Bouchard-Côté, DeNero, & Klein, 2010; Clark, 2003) or graph-
oriented methods (Biemann, 2006) – all such approaches and many others consider 
contextual co-occurrence to be the primary source of POS-irelevant information. 
But as comparative study of (Christodoulopoulos et al., 2010) indicates when de-
monstrating that models integrating morphological features tend to better than those 
who do not, it seems plausible that the uncontested primary role of CD in POS should 
be revised. While it is evident that the CD indeed must furnish relevant information if 
ever distributional hypothesis is valid (Harris, 1954) and it is axiomatic that distribu-
tional hypothesis applies in case of any agent creating categories consistently with 
Hebb’s law (Hebb, 1964) we shall argue in subsection 3.1 that pertinent POS-I clues 
can be extracted not only from word’s “external” contextual properties but also from 
word’s very “internal” Mορφε. 
2 
Axiomatic Conditions of Cognitive Plausibility 
This section deals with what we believe are necessary (i.e. sine qua non) conditions  
of cognitive plausibility of a computational model. Subsection 2.1 deals with the 
“bootstrapping” condition stating that categories which are being built are based on 
categories which have already been built. Emergence of bootstrapping effect shall be 
illustrated on a trivial multi-iterative re-clustering of clusters pre-clustered according 
to CD features. Subsection 2.2 discusses the assumption that in order to be cognitively 
plausible, the model should be data and/or oracle-driven. 
2.1 
Bootstrapping the Bootstrapping 
From biochemistry to social sciences it is a well known fact that structuring structures 
are the structures structured. Computational Linguistics and NLP in particular is not 
an exception. The most general definition of the term bootstrapping (B) – i.e. that B is 
a multi-iterative process whereby outputs of the previous iteration modify the very 
execution of the next iteration – could be indeed apply upon so many computational 
“recurrent”, “self-feeding” (Riloff & Jones, 1999), “auto-organizing” (Nowak et al., 
1999) approaches that have been already applied in so many NLP studies, that to  
state about a NLP algorithm X that “X bootstraps” may sometimes seem to be plain 
tautology. 
 

96 
D.D. Hromada 
 
In certain sense almost any POS-i model based on CD paradigm are, ex vi termini, 
bootstrapping ones because even in the most simplistic models, the information about 
the membership of the target word WT in the candidate class C is inferred from the 
probabilities of membership of WL (WT’s left context) and WR (WT’s right context) 
to their respective candidate POS classes. Given the fact that the WT plays the role of 
right context for WL and the role of left context for WR, whole problem is circular 
and as such often calls for a bootstrapping solution. 
Solan et al. (2005) refer to a crucial 4th component of their automatic distillation of 
structure (ADIOS) algorithm as “generalized bootstrapping”. Differently from the 
“geometric approach” which shall be presented in our experiment below, ADIOS 
implements graph-like structures in order to attain its aim of construction of equiva-
lence classes useful in subsequent grammar induction. But in its very essence, the 
approach of Solan et al., i.e. that one should substitute the vertices “subsumed” by a 
“subsuming” non-terminal class-denoting vertex is analogical, mutatis mutandi, to the 
approach presented in the following paragraphs. 
1st Experiment: Bootstrapping k-way POS Clustering Seeded by Token  
Co-occurrence Features.  
 
Experiment was performed with data contained in English (en), Czech (cs) and Slo-
vak (sk), corpora contained in 4th version of Multext-East corpus (Erjavec, 2012). 
Table 1. Overall statistics of analyzed corpora 
Corpus 
Word Types 
Tokens 
TagsPOS 
Featcooc 
Cs 
19283 
100368 
13 
70426 
En 
10511 
134832 
12 
36774 
Sk 
20588 
103452 
13 
74912 
 
Table 1 presents summary statistics concerning the quantities of distinct word  
tokens, word types (i.e. tokens without context) and the most coarse-grained “gold 
standard” POS-tags is presented along with total number of distinct co-occurrence 
features which is equivalent to the number of columns (dimensions) in the resulting 
co-occurrence matrix. 
Every word WT type was characterized by a (row) vector of values [W1L, W2L 
...WNL, W1R, W2R ... WNR ], W1L referring to cases when the word W1 occurred 
to the left of WT, W2L to cases when W2L was to the left, W3R to cases when W3 
was to the right from the target word. What results is a simple co-occurrence matrix 
with N rows and maximum of FeatCOOC==2*N columns. Given that in the experi-
ment we were actually looking two words to the left and two words to the right from 
WT, the maximum possible number of columns was FeatCOOC =4*N. But since not 
all word couples do occur asides each other, the final number FeatCOOC was always 
below the theoretical limit. 
 

 Conditions for Cognitive Plausibility of Computational Models of Category Induction 
97 
 
The matrix has been clustered in C={2 … 50} clusters by the fast & frugal re-
peated bisection k-way clustering algorithm as implemented in the clustering tool 
CLUTO (Karypis, 2002). Columns were scaled according to IDF principle and the 
clustering was done according to cosine metrics. Once finished, comparison with 
“gold standard” yielded V-measure (Rosenberg & Hirschberg, 2007) values presented 
in Table 2 for cluster sizes {10, 20, 30, 40, 50}. which are also illustrated as NO 
curves on Figure 1. 
We have implemented the bootstrapping component in a following manner: After 
each clustering, the information about the proposed cluster is added as a new feature 
to target’s word vector description. Thus, if matrix with 20 columns entered the first 
iteration which clustered the vectors into 5 clusters, the matrix entering the second 
iteration shall have 20+5 columns. If second iteration yields 6 clusters, a matrix with 
25+6 columns will become the input for the third iteration etc. Figure 1 shows that in 
case of all 3 studied corpora, the bootstrapping BO method always attains higher 
scores than the static NO approach.1 
 
Fig. 1. Bootstrapping of contextual co-occurrence statistics 
2.2 
Data and Oracle-Driven Learning 
Computational models unable to analyze what they have previously synthesized  
and synthesize what they have previously analyzed could be hardly labeled as  
                                                           
1  Note that the V-measure of NO-bootstrap curves seem to be relatively stable in regards to 
increase of number of clusters. Contrary to many-to-one accuracy (purity) which increases 
with number of clusters, V-measure thus seems to be better evaluation measure for cases 
when solutions containing different numbers of clusters have to be compared. 

98 
D.D. Hromada 
 
“cognitively plausible”. But even the presence of such “dialectic” component cannot 
be the guarantee of absolute success, if ever the model’s initial prima materia – the 
data with which the whole bootstrapping is initiated – are not adapted to model’s 
prewired “innate” state. 
It is unfortunately often the case in computational linguistics that whenever the 
model does not attain the expected performance, huge amount of effort is invested 
into tuning the model by diverse ad hoc modifications. After hours of exhaustive 
search, both intellectual as well as automatic, diverse parameters, meta-parameters 
and hyper-parameters are finally discovered which allow the model to attain some-
what superior performances when confronted, for example, with Wall Street Journal 
(WSJ) corpus But human categorization faculties – POS-i included – do not develop 
in such a way. While it seems plausible that same sort of “tuning of parameters”  
indeed takes place during initial period of language acquisition, it seems to be so effi-
cient because the data itself is well adapted to ever-evolving state of baby’s neuro-
linguistic structures. Said more concretely, parents do not recite to its children the 
WSJ or Eulex corpora in order to adjust the synaptic weights in the brains of their 
children, they rather modify all their narrative intentions by pragmatic, prosodic, pho-
nological as well as semantic Babytalk (Ferguson, 1964) cognitive filters. In doing so 
– by pre-processing the stimuli before it even attains perceptual buffers of child 
agent’s ears – parents affirm themselves in the role of computational oracle (Turing, 
1939).  
Since it was already demonstrated by Clark (Clark, 2010) with sufficient analytical 
clarity that the “supervision” coming from external oracle machines can significantly 
reduce the complexity of the grammar induction and POS-i problems, we found it 
worthwhile to state that “fully unsupervised approaches are very rare because the 
engineer’s decision to confront the algorithm with corpus X and not Y, and to do so in 
the moment T1 and not T2, is already an act of supervision”. 
By saying so we do not want to underestimate the importance of using the same 
corpora for mutual comparison of scientific results. We simple want to indicate that, 
because it determines everything which follows, the question of corpus choice should 
not be neglected. More concretely, cognitively plausible models of POS-i should be 
firstly tuned and “raised” with corpora like CHILDes (MacWhinney, 2000) and only 
later should be their scope of validity extended by means of confrontation with corpo-
ra of adult and expert utterances. 
3 
Conjectural Conditions of  Model’s Cognitive Plausibility 
Subsection 3.1 discuss the role of non-distributional “surface” features for POS-
induction. Discussion is followed by results of an experiment suggesting that features 
like suffix can indeed offer quite strong clues for the creation of syntactic categories. 
Subsection 3.2 introduces a conjectural condition for model’s CP by proposing to 
base it principally on geometric grounds. It is followed by subsection 3.3 arguing that 
CP model should facilitate evaluation by means of qualitative inspection. In general, 
these sections deal with CP’s conjectural conditions, meaning that while they may 
seem less self-evident that the axiomatic ones, we nonetheless consider them as valid. 

 Conditions for Cognitive Plausibility of Computational Models of Category Induction 
99 
 
3.1 
Integration of Surface Features 
Natural languages are very redundant communication channels (de Saussure., 1922; 
Shannon & Weaver, 1949). Three facets of the word – its morpho-phonological signi-
fiant, its invisible signifiée and its its syntactic function – are not independent from 
one another and more often than not do they significantly overlap (Jackendoff, 2003; 
Lakoff, 1990). Thus it is not surprising that especially in morphologically rich lan-
guages, token’s very syntactic function is encoded by morphemes present in the sur-
face, i.e. objectively perceivable form, of the token itself. And results obtained by 
Clark (Clark, 2003) or (Berg-Kirkpatrick et al., 2010) indeed point in this direction – 
it may be no coincidence that approaches which exploit the morphological features 
turned out, in (Christodoulopoulos et al., 2010) comparative study, to perform better 
than models which do not. 
2nd Experiment: Assessing the Impact of Sufixal Features on Part-of-Speech 
Categorisation. 
 
We used the same three Multext-East corpora as in the first experiment. Ultimate 
character trigram was extracted from every word type and considered to be a feature. 
Word types are subsequently clustered in C clusters according these FeatSUFFIX 
orthogonal dimensions. The comparison with Mutext-East gold standard subsequently 
yields V-measures (V), entropies (H) and purities (P) presented in Table 2.  
Table 2. Performance of model’s inducing C categories solely according to suffixal features 
C=10
C=30
C=50
Cs
534 
V=0.178
H=0.487 
P=0.582 
V=0.24
H=0.392 
P=0.642 
V=0.26 
H=0.34 
P=0.69 
En
286 
V=0.248
H=0.428 
P=0.639 
V=0.215
H=0.4 
P=0.652 
V=0.2
H=0.39 
P=0.66 
Sk
523 
V=0.17
H=0.5 
P=0.504 
V=0.272
H=0.373 
P=0.685 
V=0.274 
H=0.339 
P=0.714 
 
Amount below the corpus name in the above table denotes the length of the  
FeatSUFFIX vector, i.e. the number of distinct suffixal trigrams observed in their 
respective corpora. 
FeatSUFFIX-driven model attains lesser V-measures as had obtained (Christodou-
lopoulos et al., 2010) when evaluating models of (Clark, 2003) or (Berg-Kirkpatrick 
et al., 2010) within their 2013 comparative study. The very same study however also 
indicates that even the simplistic FEATSUFFIX-driven model can be worth of certain 
interest since it seems to be quite fast – in comparison to models harnessing the power 
of more than dozen computational cores to attain comparable or even better  
 

100 
D.D. Hromada 
 
V-measures than FEATSUFFIX-driven method , we are glad to state that in order to 
attain results presented above, our dual-core Pentium needed in average TEN=1.8, 
TSK=3.2, TCS=3.6 seconds per simulation. 
3.2 
Knowledge Is Geometric 
After the Turing machine symbol-operating paradigm started to put more importance 
upon ever-still more & more fine-grained modular to probabilistic and connectionist 
models. But in recent years, a “geometric” paradigm starts to gain momentum in di-
verse fields of cognitive sciences including computational linguistics and NLP. In 
experiments described above such paradigm was harnessed in a sense that instead of 
modulating weights along different dimensions, geometers often modulate the number 
of dimensions itself. It could be possibly reproached to such a geometric approach 
that associating every plausible feature with a new dimension can induce some se-
rious matrix-sparsity problems and|or that such an approach would be, sooner or later, 
confronted with insurmountable computational and memory limits. It is true that me-
thods by means of which some older approaches deal with the problem of huge co-
occurrency matrices can be very costly, as is the case, for example, in singular value 
decomposition within LSA (Landauer & Dumais, 1997). But since very elegant, sim-
ple and concise representations of sparse matrices can be very easily generated  
(Karypis, 2002) and since lemma of Johnson-Lindenstrauss (W. B. Johnson &  
Lindenstrauss, 1984) indicates that sparse high-dimensional matrices can be easily 
projected into low-dimensional as is often done in random-indexing (Sahlgren, 2005), 
it seems to be plausible to state that construction of vector spaces which are 1) dense 
but 2) transformable for low computational cost 3) encode huge amount of features 
attributed to huge amount of objects is not so problematic as it used to be in time 
when HMM-mastered POS-i paradigm was born. 
Series of articles by Sahlgren (2002; 2005), Cohen (2010), Widdows (2004) and 
their colleagues offer valuable initiation into advantages of random-projection based 
semantic models. For more general discussion of “geometrization of thought” in di-
verse fields of cognitive sciences, see (Gärdenfors, 2004). Within all such geometric 
models, categories can be considered as local subspaces of a global space derived 
from the data. 
3.3 
Mix of Quantitative and Qualitative Evaluation 
Performance of early grammatical category induction models was evaluated manually 
by introspection into induced equivalence classes and articles published in the period 
of “golden age” of POS-i often used to enumerate members of at least one particularly 
pleasing class or presenting their dendograms. Such an approach was later critiqued 
by Clark (2003) as “inadequate” and attention of POS-I community turned towards 
more quantitative  measures like perplexity, conditional entropy, cross-validation  
 

 Conditions for Cognitive Plausibility of Computational Models of Category Induction 
101 
 
(Gao & Johnson, 2008), one-to-one (Haghighi & Klein, 2006) or many-to-1 accuracy 
(purity); variation of information (Meila, 2003) , substituable F-score (Frank et al., 
2009) etc.  
For the purposes of this article we had decided to present our simulations principal-
ly in terns of V-measure. Given its elegance, stability in regards to growing number of 
clusters but also certain “strictness” (note that even the best performing models 
present in comparative study (Christodoulopoulos et al., 2010)  rarely  surpass the 
V>0.6 limit), we consider the V-measure to be very valuable quantitative measure of 
performance of clustering POS-i algorithms. 
But we also believe that the “old school” many-to-1 purity measure can be of cer-
tain interest, especially for those aiming to create a “semi-supervised bridge” between 
POS-induction and POS-tagging models; or by those aiming not to evaluate the per-
formance of the model by rather to gain insights of correct annotations of analyzed 
corpora. In other terms, asides to “global” statistic measures informing the researcher 
about the overall performance of the model, more “local” measures can still offer 
interesting and useful information about individual induced classes themselves. Val-
ues presented in Table 3 represent the number C of clusters into which the corpus has 
to be partitioned in order to obtain at least Φ absolutely pure (i.e. Purity=1) classes. 
Table 3. Distillation of absolutely pure categories 
 
SFFX 
CD
CD+BO
SFFX+CD+BO
Φ=1 
72 
168
107
69
Φ=2 
92 
194
142
71
Φ=3 
105 
196
180
80
Φ=4 
126 
248
189
90
Φ=5 
131 
281
194
96
Φ=10 
160 
377
256
116
 
For example, in order to obtain an absolutely pure cluster on the basis of contextual 
distribution (CD) features, one would have to partition the English part of Multext-
East corpus into 168 clusters among which shall emerge following noun-only cluster: 
 
authority, character, frontispiece, judgements, levels, listlessness, popularity, sharp-
ness, stead, successors, translucency, virtuosity. 
 
Interesting insights can also be attained by inspection of some exact points of the 
clustering procedure. Let’s inspect, as an example, the case when one clusters the 
English corpus into 7 clusters according to features both internal to the word – i.e. 
suffixes – and external – i.e. co-occurrence with other words . Such an inspection 
indicates that the model somehow succeeds to distinguish verbs from nouns. As is 
shown on Table 4, whose columns represent the “gold standard” tags and rows denote 
the artificially induced clusters, our naïve computational model tends to put nouns in 
clusters 4 and 6 while putting verbs into clusters 2, 3 and 5.  
 

102 
D.D. Hromada 
 
Table 4. Formal origins of Noun-Verb distinction 
 
N 
V 
M 
D 
R
A
S
C
I
P
X 
G 
0 
10 
3 
0 
0 
413
30
0
0
0
0
1 
0 
1 
568
67 
0 
0 
1
0
1
2
0
1
0 
0 
2 
97 
668 
0 
0 
1
137
3
2
0
0
0 
0 
3 
13 
1011 
1 
0 
275
0
2
0
0
0
0 
0 
4 
117
3 
67 
4 
0 
6
133
0
0
0
4
3 
0 
5 
608
958 
72 
67
252
321
99
72
7
106
3 
12 
6 
197
7 
97 
22 
0 
42
109
1 
3
0
3
0
2 
0 
 
The objective of our ongoing work is to align as much as possible such “seeding” 
states like that presented on Table 4. with data consistent with psycholinguistic know-
ledge about diverse stages of language acquisition process.  
At last but not least, we believe that the temporal aspects of model’s performance, 
i.e. the answer to the question “How long does the model need to run in order to fur-
nish reasonable results?” should be always seriously considered. One way how to 
evaluate such temporal aspects of categorization could be a simplistic Turing-Test 
(TT) like POS-i oriented scenario where the evaluator asks the model (or an agent)  
to attribute the POS-label to word posed by evaluator, or at least to return a set of 
members of the same category. In such a real-life scenario, an absolute perfection of 
possible future answer could be possibly traded off for less perfect (yet still locally 
optimal) answer given in reasonable time. 
But because with this TTPOS proposal we already depart from the domain of un-
supervised induction towards semi-supervised “learning with oracle” or fully super-
vised POS-tagger, we conclude that we consider the condition “cognitively plausible 
model of part of speech induction should be evaluated by both quantitative and qualit-
ative means” to be the weakest among all proposals concerning the development of an 
agent inducing the categories of natural language in a “cognitively plausible” way. 
4 
Conclusion 
Model should be labeled as “cognitively plausible” model of certain human faculty if 
and only if it not only accurately emulates the input (problem) → output (solution) 
mapping executed by the faculty, but also emulates the basic “essential” characteris-
tics associated to such mapping operation in case of human cognitive systems, i.e. 
emulates not only WHAT but also HOW the problem → solution mapping is done. 
In relation to the problem of how part-of-speech induction is effectuated by human 
agents, two characteristic conditions have been defined as axiomatic (necessary). First 
postulates that POS-i should involve a “bootstrapping” multi-iterative process able to 
subsume terminals sharing common features under a new non-terminal and to subse-
quently exploit the information related to occurrence of the new non-terminal to ex-
tend the (vectorial) definition terminals represented in the memory. Ideally the 

 Conditions for Cognitive Plausibility of Computational Models of Category Induction 
103 
 
process should converge to partitions “optimally” corresponding to the gold standard. 
First experiment has shown for three distinct corpora that even a very simple model 
based on clustering of the most trivial co-occurrence information can attain higher 
accuracies if such a bootstrapping component is involved. The second necessary con-
dition of POS-i’s CP is that it should be data or oracle-driven. It should perform better 
when first confronted with simple corpora like CHILDes (MacWhinney, 2000) and 
only latter with more complex ones than if it would be first confronted with complex 
corpora.  
Another condition of POS-i’s CP proposed that morphological and surface features 
should not be neglected and instead of playing a secondary “performance increasing 
role”, they should possibly “seed” whole bootstrapping process which shall follow. 
This condition is considered to be conjectural (i.e. “weaker” ) just because it points to 
somewhat orthogonal direction than does a traditionally acclaimed distributional hy-
pothesis (Harris, 1954). It may be the case, however, that especially native speakers of 
some morphologically rich languages shall consider the “syntax-is-also-IN-the-word” 
paradigm not only as conjectural but also axiomatic.  
Another “weak” condition of cognitive plausibility postulates that many phenome-
na related to mental representations and thinking, POS-i included, can be not only 
described but also explained and represented in geometric and topologic terms. Ideal-
ly, the geometric paradigm (Gärdenfors, 2004) should not be contradictory but rather 
complementary to symbolic and connectionist paradigms. The last and weakest condi-
tion of CP proposed that computational models of part-of-speech induction should be 
not only easily quantitatively analyzed but should be also transparent for researcher’s 
or supervisor’s qualitative analyses. They should facilitate and not complicate posing 
of all sorts of “Why?” questions and the results should be easily interpretable. A sort 
of categorization-faculty Turing Test was proposed which could be potentially em-
bedded into the linguistic component of the hierarchy of Turing Tests which we pro-
pose elsewhere (Hromada, 2012). 
It may be the case that the list of conditions of cognitive plausibility presented  
in this article is not sufficient one and should be extended with other terms like  
“modularity”, “self-referentiality” or notions coming from complex systems and evo-
lutionary computing. Regarding the problem of elucidation of how could a machine 
induce, from the environment-representing corpus, the categories in a way analogical 
to that of a child learning by imitating its parents, we consider even the list of 2 strong 
precepts and 3 weak precepts hereby presented as quite useful and possibly necessary. 
References 
1. Berg-Kirkpatrick, T., Bouchard-Côté, A., DeNero, J., Klein, D.: Painless unsupervised 
learning with features. In: Human LanguageTechnologies: The 2010 Annual Conference 
of the North American Chapter of the Association for Computational Linguistics, pp. 582–
590 (2010) 
2. Biemann, C.: Unsupervised part-of-speech tagging employing efficient graph clustering. 
In: Proceedings of the 21st International Conference on Computational Linguistics and 
44th Annual Meeting of the Associationfor Computational Linguistics: Student Research 
Workshop, pp. 7–12 (2006) 

104 
D.D. Hromada 
 
3. Brown, P.F., Desouza, P.V., Mercer, R.L., Pietra, V.J.D., Lai, J.C.: Class-based ngram 
models of natural language. Computational Linguistics 18(4), 467–479 (1992) 
4. Christodoulopoulos, C., Goldwater, S., Steedman, M.: Two Decades of Unsupervised POS 
induction: How far have we come? In: Proceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pp. 575–584 (2010) 
5. Clark, A.: Combining distributional and morphological information for part of speech in-
duction. In: Proceedings of the Tenth Conference on European Chapter of the Association 
for Computational Linguistics, vol. 1, pp. 59–66 (2003) 
6. Clark, A., de Jong, J.: Towards general algorithms for grammatical inference. In: Hutter, 
M., Stephan, F., Vovk, V., Zeugmann, T. (eds.) ALT 2010. LNCS, vol. 6331, pp. 11–30. 
Springer, Heidelberg (2010) 
7. Cohen, T., Schvaneveldt, R., Widdows, D.: Reflective Random Indexing and indirect infe-
rence: A scalable method for discovery of implicit connections. Journal of Biomedical In-
formatics 43(2), 240–256 (2010) 
8. Elman, J.L.: Representation and structure in connectionist models. DTIC Document (1989) 
9. Erjavec, T.: MULTEXT-East: morphosyntactic resources for Central and Eastern Euro-
pean languages. Language Resources and Evaluation 46(1), 131–142 (2012) 
10. Ferguson, C.A.: Baby talk in six languages. American Anthropologist 66(6_PART2), 103–
114 (1964) 
11. Frank, S., Goldwater, S., Keller, F.: Evaluating models of syntactic category acquisition 
without using a gold standard. In: Proc. 31st Annual Conf. of the Cognitive Science Socie-
ty, pp. 2576–2581 (2009) 
12. Gao, J., Johnson, M.: A comparison of Bayesian estimators for unsupervised Hidden Mar-
kov Model POS taggers. In: Proceedings of the Conference on Empirical Methods in Natu-
ral Language Processing, pp. 344–352 (2008) 
13. Gärdenfors, P.: Conceptual spaces: The geometry of thought. MIT Press (2004) 
14. Goldwater, S., Griffiths, T.: A fully Bayesian approach to unsupervised part-of-speech 
tagging. In: Annual Meeting Association for Computational Linguistics, vol. 45, p. 744 
(2007) 
15. Haghighi, A., Klein, D.: Prototype-driven learning for sequence models. In: Proceedings of 
the Main Conference on Human LanguageTechnology Conference of the North American 
Chapter of the Association of Computational Linguistics, pp. 320–327 (2006) 
16. Harris, Z.S.: Distributional structure. Word (1954) 
17. Hebb, D.O.: The Organization of Behavior: A Neuropsychlogical Theory. John Wiley & 
Sons (1964) 
18. Hromada, D.D.: Taxonomy of Turing Test Scenarios. In: Proceedings of AISB/IACAP 
2012 Symposium, Birmingham, United Kingdom (2012) 
19. Jackendoff, R.: Foundations of language: Brain, meaning, grammar, evolution. OxfordU-
niversity Press, USA (2003) 
20. Johnson, M.: Why doesn’t EM find good HMM POS-taggers. In: Proceedings of the 2007 
Joint Conference on Empirical Methods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL), pp. 296–305 (2007) 
21. Johnson, W.B., Lindenstrauss, J.: Extensions of Lipschitz mappings into a Hilbert space. 
Contemporary Mathematics 26, 1 (1984) 
22. Karypis, G.: CLUTO-a clustering toolkit. DTIC Document (2002) 
23. Lakoff, G.: Women, fire, and dangerous things. Univ. of Chicago Press (1990) 
24. Landauer, T.K., Dumais, S.T.: A solution to Plato’s problem: The latent semantic analysis 
theory of acquisition, induction, and representation of knowledge. Psychological Re-
view 104(2), 211–240 (1997) 

 Conditions for Cognitive Plausibility of Computational Models of Category Induction 
105 
 
25. Levy, Y., Schlesinger, I.M., Braine, M.D.S.: Categories and Processes in Language Acqui-
sition. Lawrence Erlbaum (1988) 
26. MacWhinney, B.: The CHILDES Project: Tools for Analyzing Talk. Transcription, format 
and programs, vol. 1. Lawrence Erlbaum (2000) 
27. Meilă, M.: Comparing clusterings by the variation of information. In: Schölkopf, B., 
Warmuth, M.K. (eds.) COLT/Kernel 2003. LNCS (LNAI), vol. 2777, pp. 173–187. Sprin-
ger, Heidelberg (2003) 
28. Nowak, M.A., Plotkin, J.B., Krakauer, D.C.: The evolutionary language game. Journal of 
Theoretical Biology 200(2), 147–162 (1999) 
29. Riloff, E., Jones, R.: Learning dictionaries for information extraction by multi-level boot-
strapping. In: Proceedings of the National Conference on Artificial Intelligence, pp. 474–
479 (1999) 
30. Rosenberg, A., Hirschberg, J.: V-measure: A conditional entropy-based external cluster 
evaluation measure. In: Proceedings of the 2007Joint Conference on Empirical Methods in 
Natural Language Processing and Computational Natural Language Learning (EMNLP-
CoNLL), vol. 410, p. 420 (2007) 
31. Sahlgren, M.: An introduction to random indexing. In: Methods and Applications of Se-
mantic Indexing Workshop at the 7th International Conference on Terminologyand Know-
ledge Engineering, TKE, vol. 5 (2005) 
32. Sahlgren, M., Karlgren, J.: Vector-based semantic analysis using random indexing for 
cross-lingual query expansion. In: Peters, C., Braschler, M., Gonzalo, J., Kluck, M. (eds.) 
CLEF 2001. LNCS, vol. 2406, p. 169. Springer, Heidelberg (2002) 
33. De Saussure, F., Bally, C., Séchehaye, A., Riedlinger, A., Calvet, L.J., De Mauro, T.: 
Cours de linguistique générale. Payot, Paris (1922) 
34. Schütze, H.: Part-of-speech induction from scratch. In: Proceedings of the 31st Annual 
Meeting on Association for Computational Linguistics, pp. 251–258 (1993) 
35. Shannon, C.E., Weaver, W.: The mathematical theory of information, vol. 97. University 
of Illinois Press, Urbana (1949) 
36. Solan, Z., Horn, D., Ruppin, E., Edelman, S.: Unsupervised learning of natural languages. 
Proceedings of the National Academy of Sciences 102(33), 11629 (2005) 
37. Turing, A.M.: Systems of logic based on ordinals. Proceedings of the LondonMathemati-
cal Society 2(1), 161–228 (1939), Language and Speech 40(1), 47–62 
38. Vlachos, A., Korhonen, A., Ghahramani, Z.: Unsupervised and constrained Dirichlet 
process mixture models for verb clustering. In: Proceedings of the Workshop on Geome-
trical Models of Natural Language Semantics, pp. 74–82 (2009) 
39. Widdows, D., Kanerva, P.: Geometry and meaning. CSLI Publications Stanford (2004) 
 

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 106–115, 2014. 
© Springer International Publishing Switzerland 2014  
3D-Posture Recognition Using Joint Angle 
Representation   
Adnan Al Alwani1, Youssef Chahir1, Djamal E. Goumidi2,  
Michèle Molina2, and François Jouen3 
1 GREYC CNRS (UMR 6072) 
2 PALM, EA 4649 
Université de Caen, Basse-Normandie 
Caen, France 
3 CHArt EA 4004 
Ecole Pratique des Hauts Etudes 
{firstname.secondname}@unicaen.fr, 
gmdjml@yahoo.com, 
francois.jouen@ephe.sorbonne.fr 
Abstract.  This paper presents an approach for action recognition performed by 
human using the joint angles from skeleton information. Unlike classical ap-
proaches that focus on the body silhouette, our approach uses body joint angles 
estimated directly from time-series skeleton sequences captured by depth sen-
sor. In this context, 3D joint locations of skeletal data are initially processed. 
Furthermore, the 3D locations computed from the sequences of actions are de-
scribed as the angles features. In order to generate prototypes of actions poses, 
joint features are quantized into posture visual words. The temporal transitions 
of the visual words are encoded as symbols for a Hidden Markov Model 
(HMM). Each action is trained through the HMM using the visual words sym-
bols, following, all the trained HMM are used for action recognition.  
Keywords: 3D-joint locations, Action recognition, Hidden Markov Model, 
Skeleton angle.  
1 
Introduction  
Action recognition from video is considered as one of the most active research area in 
the field of computer vision, especially in the field of video analysis, surveillance 
system, and human-computer interaction. There is a rich literature in action recogni-
tion in a wide range of applications, including computer vision, machine learning, and 
pattern recognition [22,23]. In the past years, efforts have focused on recognizing 
actions from video sequences with single camera. Among the different approaches, 
spatial-temporal interest points (STIP) and 2D binary silhouettes are the most popular 
representations of the human activity and action [8,14,17,18]. In the past decade, sev-
eral silhouette-based methods for action recognition were mainly categorized into two 

 
3D-Posture Recognition Using Joint Angle Representation 
107 
 
subsets. One is designed to handle the sequences of silhouettes in order to extract 
action descriptors. Then conventional classification strategies are frequently used for 
recognition [1,2,3,4]. The other category models the dynamics of the action explicitly 
based on the features extracted from each silhouette [5,6,7,16]. 
However, particular challenges in the human action recognition can alter the per-
formance of actions descriptor from 2D image sequences: intra-class variation, inter-
class dependence of action, different contexts of the same action and occlusions are 
the major challenges in action recognition. The use of several cameras significantly 
alleviates the challenges such as, occlusion, cluttered background, and viewpoints 
changes, which are the major low-level difficulties that reduce the recognition per-
formance from traditional 2D imagery. Furthermore, using multiple cameras provided 
stable information of actions from certain viewpoints. For example, taking account a 
direction of the camera makes possible to distinguish object pointing from reaching 
from depth map rather than in RGB space. However, earlier range sensors were either 
difficult to use on human subjects, or may provide poor measurement. To overcome 
the limitations of range sensors, depth has to be inferred from stereoscopic using low-
cost visible light cameras. Furthermore, 3D body configurations can be captured by 
multiple cameras in a predefined environment [25]. 
Skeleton is an articulated system, which consists of limbs segments and the joints 
between segments. Joints connect rigid segments and articulated motion can be consi-
dered as a continuous evolution of local poses configuration [10]. Therefore, for a 
given sequences of 3D maps, if we can get the stream of the 3D joints location, then 
reliably action recognition can be achieved by using the tracked joints locations, 
which significantly improves human action recognition that is under-recognized by 
traditional techniques. 
Recently 3D information has been interpreted using special release of the Micro-
soft Kinect®, which provides both depth and RGB image streams. Although mainly 
targeted for commercial purpose, this device has brought considerable interest to the 
research in computer vision, and hand gesture control. 
In this article, we recommend a method for posture-based human action recogni-
tion. In the proposed work, 3D locations of joints from skeleton configurations are 
considered as inputs. Skeletal joints positions are extracted and simple relation be-
tween coordinates vector is used to describe the 3D human poses. We perform first 
the representation of human postures by selecting 7 primitive joints positions. The 
collection of joint-angle features is quantized through unsupervised clustering into k 
pose vocabularies. Then encoding temporal joint-angle features into discrete symbols 
is performed to generate Hidden Markov Model HMM (HMM) for each action. We 
recognize individual human action using generated HMM. The proposed method is 
evaluated with public 3D dataset.  
The Contribution Parts in This Work Consist of Two Parts: First, we use joint-
angle positions to describe posture representation as human action recognition system. 
Second, our method presents low computational cost since only 7 joints are adopted, 
and includes representation of poses that is view-invariant.  

108 
A. Al Alwani et al. 
 
The organization of this paper is as follows. We introduce the related works in sec-
tion 2. Section3 describes the method we used to elaborate the architecture of pro-
posed system from postures representation to features extraction. Section 4 addresses 
action recognition by an HMM. Section 5 explains experimental results. Section 6 
concludes the paper. 
2 
Related Work 
Efforts have been reported for the problem of human action recognition, by exploring 
different kind of visual information. Review on the categories of visual features can 
be found in [8,22,25]. However, only few attempts on action recognition using depth 
maps have been recently proposed. Therefore, we present a review of works based on 
3D poses action recognition since they are related to our work.  
The recent trends in the field of action recognition that use depth maps have  
induced further progress. Uddin et al. [13] reported a novel method of action recogni-
tion using body joint angles estimated from a pair of stereo images from stereo cam-
eras. Thang et al. [21] developed a method for estimating body joint angles from 
time-series of paired stereo images recreded with a single stereo camera. Yu and Ag-
garwal [11] adopted an approach for action recognition where body parts are consi-
dered as a semantic representation of postures. Weinland et al. [26] proposed a model 
action involving 3D sequences of prototypes, which are represented as visual struc-
tures captured by a system of 5 cameras. The work proposed by Li et al.[9]  suggested 
to map the dynamic actions as a graph, and sample a set of 3D points from the depth 
maps to describe a set of salient postures, that correspond to the nodes in the graph. 
However, the challenge in the sampling technique is view dependent. Xai et al. [12] 
presented a method of action recognition based on 3D skeleton joints location. They 
proposed a compact representation of postures by characterizing human poses as his-
togram of 3D joints locations sampled inside spherical coordinates system.  
3 
Body Parts Representation 
In this section we describe the human poses representation and joints position estima-
tion from skeleton model. This kind of representation involves 3D joints coordinates 
to describe a basic body structure reduced to 20 skeletal joints. Recent release of Ki-
nect® system offers better solution for the estimation of the 3D joint positions. Figure  
1 demonstrates the result of  the application of depth map and the 3D skeletal joints 
extraction according to algorithm of Shotton et al [24] who proposed to extract 3D 
body joint locations from a depth map.  
This algorithm is used to estimate pose locations of skeletal joints. Starting with a 
set of 20 joints coordinates in a 3D space, we compute a set of features to form the 
representation of postures. Among the 20 joints, 7 primitive joints coordinates are 
selected to describe geometrical relations between body parts. The category of primi-
tive joints offers redundancy reduction to the resulting representation. Most impor-
tantly, primitive joints achieve view invariance to the resulting pose representation, by 

 
3D-Posture Recognition Using Joint Angle Representation 
109 
 
aligning the cartesian coordinates with the reference direction of the person. Moreo-
ver, we propose an efficient and view-invariant representation of postures using 7 
skeletal joints, including L/R hand, L/R feet, L/R hip, and hip center.    
 
 
 
Fig. 1. (a) Depth map image. (b) Skeletal joints positions proposed by [15]. 
The hip center is considered as the center of coordinate system, and the horizontal 
direction is defined according to left hip and right hip junction. The remaining 4 ske-
letal locations are used for poses joint angles descriptor. 
3.1 
Action Coordinates Description for Skeletal Joints 
View invariance is a challenging problem in action recognition. With the use of 3D 
body skeleton, we can capture the 3D positions of the human body. We propose a 
viewpoint-invariant representation of body poses by using 3D joint angles from ske-
letal data. In our approach of poses features inference, we achieve the view-invariant 
by aligning the Kinect® cartesian system with the direction of human body as shown 
in the Fig 2. We consider the hip center joint as the center of the new orthogonal 
coordinates. We define the horizontal offset vector γ to represent the vector from left 
to right of the hip center, the reference vertical vector ρ as the vector that is perpendi-
cular to the horizontal reference vector computed by rotating the vector γ by 90°.  The 
depth reference vector β is obtained by cross product operation between γ and ρ. The 
next steps demonstrate the procedure of aligning the orthogonal coordinates with  
the specific reference direction of the body. 
Let the system Landmark be defined as Rs (Ο,i,j,k), and  the actions landmark as 
Ra( Ó, γ, ρ, β). If we define the hip center as origin of the action coordinates, then the 
action horizontal direction γ is written as: 
 
ߛԦ ൌ ቌ
݄݅݌ܿ݁݊ݐ݁ݎ௫
൅
ߣ௫
݄݅݌ܿ݁݊ݐ݁ݎ௬
൅
ߣ௬
݄݅݌ܿ݁݊ݐ݁ݎ௭
൅
ߣ௭
ቍൌ൭
ߛ௫
ߛ௬
ߛ௭
൱.                                         ሺ1ሻ 
Where    ߣൌ
௨ሬሬԦ
|௨|
ሬሬሬሬሬԦ , is the normal unit vector, and  u is defined as: 

110 
A. Al Alwani et al. 
 
ݑሬሬሬ
where  lh , rh are left hip and
By performing some vec
vertical vector that is perpe
lated from the cross produc
For the point in the 3D c
ΟM to ÓM is defined as: 
 
 
ൌ ܯ௫
  
where hc, is the hip cente
system. 
In order to express the Ó
system unit vectors i,j,k in t
                   
 Substituting eq. 4 into eq. 3
   
Fig. 2. Co
ݑ ሬԦ ൌቌ
݈݄௫
െ
ݎ݄௫
݈݄௬
െ
ݎ݄௬
݈݄௭
െ
ݎ݄௭
ቍൌ൭
ݑ௫
ݑ௬
ݑ௭
൱.                                            
d right hip. 
ctor manipulations, the reference vector ρ is defined as 
endicular to the horizontal plane, and the vector β is cal
t operation between γ and ρ vectors.  
coordinate system M(x,y,z), the unit vector translation fr
 ࣩܯ
ሖ
ሬሬሬሬሬሬሬԦ ൌ ࣩࣩሖሬሬሬሬሬԦ ൅ ࣩܯ
ሬሬሬሬሬሬԦ 
 ൌ ࣩܯ
ሬሬሬሬሬሬԦ ൅ࣩࣩሖሬሬሬሬሬԦ  
௫ଓԦ ൅ܯ௬ ଔԦ ൅ܯ௭݇ሬԦ െ݄ܿ௫ଓԦ െ݄ܿ௬ଔԦ െ݄ܿ௭݇ሬԦ.                             
er, and i,j,k are the unit direction vectors of coordina
ÓM as a function of skeletal landmarks, we first specify 
terms of the action system coordinates as: 
        ଓሬԦ ൌ݅ఊߛԦ ൅
݆ఘߩԦ ൅
݇ఉߚԦ 
ଔԦ ൌ݅ఊߛԦ ൅݆ఘߩԦ ൅݇ఉߚԦ  
       ݇ሬԦ ൌ ݅ఊߛԦ ൅݆ఘߩԦ ൅݇ఉߚԦ.                                                    
3, we get the  final formula for the vector ÓM as: 
ܱܯ
ሬሬሬሬሬሬԦ ൌܯఊߛԦ ൅ܯఘߩԦ ൅ܯఉߚ.
ሬሬሬԦ                                               
 
ordinates system description of skeletal joints  
ሺ2ሻ 
the 
lcu-
rom 
ሺ3ሻ 
ates  
the 
ሺ4ሻ 
ሺ5ሻ 

 
3D-Posture Recognition Using Joint Angle Representation 
111 
 
 
Fig. 3. Joints angles features.(a) XY plane. (b) ZY plane. 
3.2 
Features Description 
In our approach human poses are distinguished by the idea of angles groups estimated  
from the four junctions, which are mentioned above. The joints angles groups are 
sampled from two planes, XY plane and ZY plane. In the XY plane, four angles 
represent the angles between hand left-foot left, hand left-hand right, hand right-foot 
right, and foot right-foot left respectively. Same joints angles are also defined from 
the plane 
ZY. The final features 
vector includes eight joints angles 
Ft={θ1,θ2,……θ8}at each pose instant t.  Fig. 3 shows the joint angles of two planes, 
where each angle is defined according to the corresponding four junctions which were  
illustrated in section 3. 
4 
HMM for Action Recognition 
To apply HMMs to the problem of human action recognition, video frames 
V={I1,I2,…IT}are transformed into symbols sequences O. The transformation is done 
during the learning and recognition phases. From each video frames, a feature vector 
fi ϵ R,{ (i=1,2,….T),T is the number of the frames} is extracted, and fi is assigned to a 
symbol vj chosen from the set of symbols V. In order to specify observation symbols, 
we perform the clustering of features vector into k clusters using K-means. Then each 
posture is represented as a single number of a visual word. In this way, each action is 
a time series of visual words. The obtained symbol sequences are used to train HMMs 
to learn the correct model for each action. For the recognition of a test action, the 
obtained observation symbol sequence O = {O1,O2,….ON} is used to  determine 
across all trained  HMMs  which is the most accurate for  the tested human action.   
HMMs, which have been recently applied with particular success to speech recog-
nition, are a kind of stochastic state transit model [20]. HMMs use observation  
sequence to determine the hidden states. We suppose O = {O1,O2,….ON} as the obser-
vation of the stochastic sequence. HMM with N state is specified by three groups of 
parameters: β={A,B,π},where A={aij,=pr(qt=sj|qt-1=si)} denotes the state transition  
 

112 
A. Al Alwani et al. 
 
probability matrix, used to describe the state transition between probability, where, aij 
is the probability of transiting from state qi to state qj, and B={bj(k)=pr(vk|qt=sj)}, is 
the matrix of observation probabilities, used to describe the state j, the probability of 
the output corresponding to the observed values bj(k)  of output symbol vk at state qj, 
and π=π{πi=pr(q1=si )} the initial state probability used to describe the observed se-
quence of probability when t=1. 
Each state of the HMM stochastically outputs a symbol. In state si, symbol vk is 
output with a probability of bi(k). If there are M kinds of observation symbols, bj(k) 
becomes an N x M  matrix, where N is the number of states in the model. The HMM 
outputs the symbol sequence O = O1,O2,. . . , OY from time 1 to T. The initial state of 
the HMM is also stochastically determined by the initial state probability π. 
To recognize the observed symbol sequences, we create a single HMM for each ac-
tion. For a classifier of C actions, we choose the model which best matches the obser-
vations from C HMMs (βi={Ai, Bj, πi}), i = 1 . . . C. This means that when a sequence 
of unknown category is given, we calculate Pr(βi |O) for each HMM βi, and select 
βc˜. For instance, given the observation sequence O = O1,. . . OT and the HMM βi, 
according to the Bayes’s rule, the problem is how to evaluate Pr(O|βi), the probability 
that the sequence was generated by HMM βi, which can be solved using the forward 
algorithm.  Then we classify the action as the one that presents the largest posterior 
probability 
 
c˜=argmaxi(Pr(βi|O)).                                               (6) 
where i indicates the likelihood of test sequence for the ith HMM.  
5 
Experiments  
We evaluate the performance of our algorithm with the public G 3D dataset collected 
by Bloom et al.[19]. In addition, we evaluated the algorithm with the MSR Action 3D 
dataset collected by Li et al.[9] and we compared our results with results reported  
in [9].  
Table 1. The subsets of actions used with the MSR Action 3D dataset 
Action Set 1 (AS1) 
Action Set2 (AS2) 
Action set3 (AS3) 
Horizontal arm wave  
Hammer 
Forward punch  
High throw  
Hand clap 
Bend 
Tennis serve 
Pickup & throw 
High arm wave 
Hand catch 
Draw x 
Draw tick 
Draw circle 
Two hand wave 
Forward kick 
Side boxing 
High throw 
Forward kick  
Side kick 
Jogging  
Tennis swing  
Tennis serve 
Golf swing  
Pickup & throw 
 

 
3D-Posture Recognition Using Joint Angle Representation 
113 
 
Table 2. Recognition rates of our method on the G3D action dataset. Results are compared with 
Bloom et al. [19]. 
Action Category 
Bloom et al. 
Ours method 
Fighting  
70.46% 
79.84% 
Golf 
83.37% 
100% 
Tennis  
56.44% 
78.66% 
FPS 
53.57% 
54.10% 
Driving a car 
84.24 
81.34% 
Misc. 
78.21% 
89.40% 
Overall 
 71.04% 
80.55% 
Table 3. Recognition rates of our method on the MSR Action 3D dataset. Results are compared 
result with Li et al. [9].  
Action subset 
Li et al. 
Ours method 
AS1 
72.9% 
86.30% 
AS2 
71.9% 
65.40% 
AS3 
79.2% 
77.70% 
Overall 
74.7% 
76.46% 
5.1 
Experimental Results 
The results of our approach with the G3D dataset collected by Bloom et al.[19], con-
taining 22 types of human actions are summarized in table 2. Each action was per-
formed by 10 individuals for 3 times. Note that we only used the information from the 
skeleton for action recognition in our algorithm. The experiment was repeated 20 
times, and the averaged performance is reported in Table 2. The set of clusters was 
fixed to K=80, and the number of states to N=6. Half of the subjects were used for 
training and the rest of the subjects were used for testing.  Across experiments, the 
overall mean accuracy is 80.55% demonstrating that our method performs better rec-
ognition than Bloom et al [19]. 
We also tested our algorithm on the public MSR Action3D database that contains 
20 actions. We divided the actions into three subsets (similar to [9]), each comprising 
8 actions (see table 1). We used the same parameter settings as previously described. 
In this test, half of the subjects were used for training and the rest of the subjects were 
used for testing. Each test was repeated 20 times, and the averaged performance is 
given in Table3. We compared our performance with Li et al[9]: our algorithm 
achieves considerably better recognition rates than Li et al. 
6 
Conclusion  
This paper presents a framework to recognize human action from sequences of skele-
ton data. We use 3D joints positions inferred from skeleton data as input. We propose 

114 
A. Al Alwani et al. 
 
a method for postures representation that involves joint angles in xy and zy planes 
within a modified action coordinates system as description of postures. In order to 
classify action types, we model sequential postures with HMMs. Experimental results 
illustrate the performance of the proposed method, and also refer to a promising  
approach to perform recognition tasks using 3D points.   
References 
1. Bobick, A., Davis, J.: The recognition of human movement using temporal templates. 
IEEE Trans. PAMI 23(3), 257–267 (2001) 
2. Meng, H., Pears, N., Bailey, C.: A human action recognition system for embedded  
computer vision application. In: Proc. CVPR (2007) 
3. Davis, J.W., Tyagi, A.: Minimal-latency human action recognition using reliable-
inference. Image and Vision Computing 24(5), 455–473 (2006) 
4. Chen, D.–Y., Liao, H.–Y.M., Shih, S.–W.: Human action recognition using 2-D spatio-
temporal templates. In: Proc. ICME, pp. 667–670 (2007) 
5. Kellokumpu, V., Pietikainen, M., Heikkila, J.: Human activity recognition using sequences 
of postures. In: Proc. IAPR Conf. Machine Vision Applications, pp. 570–573 (2005) 
6. Sminchisescu, C., Kanaujia, A., Li, Z., Metaxas, D.: Conditional models for contextual 
human motion recognition. In: Proc. ICCV, vol. 2, pp. 808–815 (2005) 
7. Zhang, J., Gong, S.: Action categoration with modified hidden conditional random field. 
Pattern Recognitoin 43, 197–203 (2010) 
8. Li, W., Zhang, Z., Liu, Z.: Expandable data-driven graphical modeling of human actions 
based on salient postures. IEEE Transactions on Circuits and Systems for Video Technol-
ogy 18(11), 1499–1510 (2008) 
9. Li, W., Zhang, Z., Liu, Z.: Action recognition based on a bag of 3D points. In: CVPRW 
(2010) 
10. Zatisiorsky, V.M.: Kinematics of Human motion. Human Kinetics Publisher 
11. Yu, E., Aggarawal, J.K.: Human action recognition with extremities as semantic posture 
representation. In: Proc. CVPR (2009) 
12. Xai, L., Chen, C.C., Aggarwal, J.K.: View invariant human action recognition using histo-
gram of 3D Joints. In: 2nd International Workshop on Human Action Understanding from 
3D Data in Conjunction with IEEE CVPR, pp. 20–27 (2012) 
13. Uddin, M.Z., Thang, N.D., Kim, J.T., Kim, T.S.: Human Activity Recognition Using Body 
Joint-Angle Features and Hidden Markov Model. ETRI Journal 33(4), 569–579 (2011) 
14. Yamato, J., Ohya, J., Ishii, K.: Recognition Human action in time-sequential images using 
hidden markov model. In: IEEE Int. Conf. Computer Vision Pattern Recognition, pp. 379–
385 (1992) 
15. http://dipresec.king.ac.uk/G3D 
16. Niu, F., Abdel-Mottaleb, M.: View-invariant human activity recognition based on shape 
and motion features. In: IEEE 6th Int. Symp. Multimedia Software Eng., pp. 546–556 
(2004) 
17. Uddin, M.Z., et al.: Human activity recognition using independent components features 
from depth images. In: 5th Int. Conf. Ubiquitous Healthcare, pp. 181–183 (2008) 
18. Uddin, M.Z., Lee, J., Kim, T.-S.: independent shape component-based human activity rec-
ognition via hidden markov model. Appl. Intellig. 33(2), 193–206 (2009) 

 
3D-Posture Recognition Using Joint Angle Representation 
115 
 
19. Bloom, V., Makris, D., Argyriou, V.: G3D: a Gaming action dataset and real time action 
recognition evaluation framework. In: 3rd IEEE Inter. Workshop on Computer Vision for 
Computer Games, CVCG (2012) 
20. Rabiner, L.R.: A Tutorial on Hidden Markov Models and Selected Applications in Speech 
Recognition. Proc. of the IEEE 77(2), 257–285 (1989) 
21. Thang, N.D., et al.: Estimation of 3D Human Body Posture via Co-registration of 3D Hu-
man Model and sequential stereo information. Applied Intell (2010), doi:10.1007/s10489-
009-0209-4 
22. Turaga, P., Chellapa, R., Subrahmanian, V.S., Udrea, O.: Machine recognition of human 
activities: A survey. IEEETransactions on Circuits and Systems for Video Technolo-
gy 18(11), 1473–1488 (2008) 
23. Aggarwal, J.K., Ryoo, M.S.: Human activity analysis: A review. ACM Computing Sur-
veys (2011) 
24. Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A., 
Blake, A.: Real-Time Human Pose Recognition in Parts from a Single Depth Image. In: 
CVPR IEEE (June 2011) 
25. Moeslund, T.B., Granum, E.: A survey of computervision-based human motion capture. 
Computer Vision and Image Understanding 81, 231–268 (2001) 
26. Weinland, D., Boyer, E., Ronfard, R.: Action recognition from arbitrary views using 3D 
exemplars. In: ICCV 2007 (2007) 
 
 

Gesture Trajectories Modeling Using
Quasipseudometrics and Pre-topology for Its
Evaluation
Marc Bui1, Souﬁan Ben Amor2, Michel Lamure3, and Cynthia Basileu3
1 CHaRt-EA4004, Universit´e paris 8 & EPHE
marc.bui@univ-paris8.fr
2 Laboratoire PRISM, Universit´e de Versailles-Saint-Quentin-en-Yvelines, 45,
Versailles, France
soufian.ben-amor@uvsq.fr
3 Laboratoire SIS EA 4128, Universit´e Lyon 1-UFR d’odontologie-11 rue Guillaume
Paradin 69372
lamure@universite-lyon1.fr
Abstract. The main question addressed in this work deals with the
diﬃculty to compare diﬀerent data trajectories patterns in particular
due to the non symmetry properties. In order to tackle this fundamental
issue in its generality from a theoretical point of view, we introduce
the quasipseudo-metrics concepts, and with the induced pre-topological
space on the datasets we can identify proximity between trajectories. We
will illustrate the ideas by discussing the application of the theoretical
framework on gestual analysis.
1
Introduction
Complex systems present characteristic features linked to their capability to be
constituted from strongly interconnected heterogeneous components, to be char-
acterized by complex hierarchical network structure and by to be highly open
to their environment. This is the case with human centric-sensing systems such
as location-based services, ubiquitous computing facilities or smart wearable de-
vices. In order to understand, predict and manage such systems we need to
deﬁne an adequate mathematical framework to model their structural dynam-
ics. In fact, the main characteristics of complex systems are not totally handled
by graph theory. It is the case when a structure appears, based on a family of
intricate relationships between individuals. Complex systems understanding is
mainly based on the analysis of the associated complex data. Given the impor-
tance and speciﬁcity of the data associated with complex systems both in qual-
itative and quantitative terms, we looked into the adapted techniques, namely
quasipseudometrics, in analyzing and classiﬁcation of complex data in order to
extract valuable information concerning the systems behavior.
Knowledge extraction from massive amount of tracking data need news math-
ematical theoretical models and tool. Clustering techniques are based, as a rule,
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 116–134, 2014.
c
⃝Springer International Publishing Switzerland 2014

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
117
on the concept of distance that supposes a precise axiomatic. This axiomatic
sometimes proves to be very coercive, in particular in the applications in social
sciences. In various cases, a problem that arises is the property of symmetry
which expresses that the distance from x to y is equal to the distance from y to
x. The question is therefore to know what it happens when this hypothesis is
relaxed.
The concept of similarity between objects is very well modeled when the
working data can be immersed in a metric space where the notion of distance
can precisely quantify the similarity between two objects. It is quite diﬀerent
when the data, by their nature, can not be immersed in such a metric space
without being denatured. The purpose of this paper is to address this question of
measuring the similarity between objects in the latter case. For this, we propose
a model based on a generalization of the topology, called pretopology ([5], [6],
[7], [4],[3],[2],[1]), associated with the deﬁnition of quasipseudometric , extension
of the concept of distance or metric.
The paper is divided into ﬁve sections. The ﬁrst is a reminder of the basics on
pretopological spaces, the second presents the concept of quasipseudometric, the
third explores the link with a speciﬁc type of pretopological space, the fourth
sets an example and the ﬁfth concludes with future prospects .
2
Pretopological Spaces
In this section, we introduce the pretopology theory with its basic deﬁnitions.
We deﬁne pretopology by mean of its two fundamental set functions: the pseu-
doclosure and interior maps instead of introducing a deﬁnition based on the
family of open sets (or closed sets) as in topology. There is a theoretical reason
for this: in pretopological spaces, family of open subsets or closed subsets do not
characterize the pretopological structure. According to properties fulﬁlled by
pseudoclosure and interior maps, we get diﬀerent pretopological spaces, which
will be detailed in a subsequent section. We will then be able to present the
two diﬀerent concepts of pretopological subset and pretopological subspace of a
pretopological space.
2.1
Deﬁnitions
In this subsection, we deﬁne the pseudoclosure and interior maps and the pre-
topological space this deﬁned.
From now, we consider a non empty set E and we deﬁne two functions from
P(E) into P(E).
Deﬁnition 1. We call pseudoclosure deﬁned on E, any function a(.) from P(E)
into P(E) such as:
– a(∅) = ∅
– ∀A ⊂E, A ⊂a(A)

118
M. Bui et al.
We can note that pseudoclosure fulﬁlls two of properties of a topological
closure.
Deﬁnition 2. We call interior deﬁned on a set E, any function i(.) from P(E)
into P(E) such as:
– i(E) = E
– ∀A, A ⊂E, i(A) ⊂A
Very often, i(.) is deﬁned by c-duality. If we denote Ac the set E −A, then,
given a pseudoclosure a(.), we can deﬁne i(.) by putting:
∀A, A ⊂E, i(A) = (a(Ac))c. And conversely ∀A, A ⊂E, a(A) = (i(Ac))c.
Deﬁnition 3. Given, on E, a(.) and i(.), the couple s = ((a(.), i(.)) is called
pretopological structure on E an the 3-uple (E, a(.), i(.)) is called a pretopological
space.
Thus, a(.) and i(.) are two means to transform subsets of E in such a way
that we have ∀A ⊂E, i(A) ⊂A ⊂a(A). As in topology, we can deﬁne deﬁne
closed and open subsets of a pretopological space (E, a(.), i(.)).
Deﬁnition 4. Given a pretopological space (E, a(.), i(.)), any subset A of E is
said to be a closed subset of E if and only if A = a(A).
Deﬁnition 5. Given a pretopological space (E, a(.), i(.)), any subset A of E is
said to be an open subset of E if and only if A = i(A).
If a subset A of a pretopological space (E, a(.), i(.)) is both a closed and an
open subset, we call it is an oc of the pretopological space.
In the same way as in topology, we obviously obtain the following result.
Proposition 1. Given a pretopological space (E, a(.), i(.)) where a(.) and i(.)
are deﬁned by c-duality, then, for any A, A ⊂E, we have:
A closed subset of E ⇔Ac open subset of E.
Then, it is possible to generalize the two concepts of closure and opening of
any subset of a pretopological space.
Deﬁnition 6. Given a pretopological space (E, a(.), i(.)), we call closure of any
subset A of E, when exists, the smallest closed subset of (E, a(.), i(.)) which
contains A.
It is also the intersect of all closed subsets of (E, a(.), i(.)) which contains A.
The closure of A is denoted by F(A).
And, in a same way:
Deﬁnition 7. Given a pretopological space (E, a(.), i(.)), we call opening of any
subset A of E, when exists, the biggest open subset of (E, a(.), i(.)) which is
included in A.
It is also the union of all open subsets of (E, a(.), i(.)) which are included A. The
opening of A is denoted by O(A).

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
119
Here, we can note a fundamental diﬀerence between topology and pretopology:
as in topology, the family of closed subsets characterizes the topological struc-
ture, we do not get the same result in pretopology. Two diﬀerent pseudoclosure
a(.) and a′(.) can give the same family of closed subsets on a set E.
2.2
Diﬀerent Types of Pretopological Spaces
In this subsection, we give the various types of pretopological before restricting
to the most common one, and then, focus on how to elaborate the proximity
concept tools with the fundamentals set operations available.
Given a pretopological space (E, a(.), i(.)), we can deﬁne the following family
of subsets of E.
∀x ∈E, U(x) = {B ⊂E|x ∈i(B)}
Then:
(i) ∀B ∈U(x), x ∈B
(ii) ∀B ∈U(x), ∀B′ ∈U(x), B ∩B′ ̸= ∅
Conversely, let us suppose we consider for any x in E, a family U(x) which
satisﬁes (i) and (ii), and let us deﬁne i(.) from P(E) into itself such as:
∀A ⊂E, i(A) = {y ∈E|A ∈U(y)}
then, obviously i(∅) = ∅and ∀A ⊂E, i(A) ⊂A.
Thus, i(.) fulﬁlls the two properties of an interior. By duality, we can deﬁne the
related pseudoclosure a(.):
∀A ⊂E, a(A) = ({y ∈E|Ac ∈U(y)})c
So:
Deﬁnition 8. Given a pretopological space (E, a(.), i(.)), the family deﬁned by
∀x ∈E, U(x) = {B ⊂E|x ∈i(B)} is called the family of neighborhoods of x.
Thus, as in topology, we get a concept of neighborhoods. However, up to now,
pretopological neighborhoods do not verify the same properties than topological
neighborhoods. For example, it is very easy to see that if U is a neighborhood of
a given x in E and if U is included in a subset V of E, that does not mean that V
is a neighborhood of x. So, we were led to deﬁne diﬀerent types of pretopological
spaces which are less general than the basic ones but for which, good properties
are fulﬁlled by neighborhoods. In this section, we propose the diﬀerent types of
pretopological spaces which have been deﬁned:
1. V-type spaces: a(.) also veriﬁes A ⊂B ⇒a(A) ⊂a(B)
2. VD-type spaces: a(.) also veriﬁes a(A ∪B) = a(A) ∪a(B)
3. VS-type spaces: a(.) also veriﬁes a(A) = 
x∈A a({x})
4. and at last Topological spaces: a(.) also veriﬁes a(A ∪B) = a(A) ∪a(B) and
a(a(A))
Among these diﬀerent types of pretopological spaces, we further only consider
the ﬁrst one for for the interest it has.

120
M. Bui et al.
2.3
A Useful Type: The V-type Pretopological Space
As previously suggested, a V-type pretopological space is deﬁned as follows:
Deﬁnition 9. Let a pretopological space (E, a(.), i(.)), we say that it is a V type
space if and only if:
∀A ⊂E, ∀B ⊂E, (A ⊂B ⇒a(A) ⊂a(B))
Equivalently, we can put:
Deﬁnition 10. Let a pretopological space (E, a(.), i(.)), we say that it is a V
type space if and only if:
∀A ⊂E, ∀B ⊂E, (A ⊂B ⇒i(A) ⊂i(B))
In this case, we can use the concept of neighborhood to characterize a V-type
space (E, a(.), i(.)).
For that, given such a space, let us consider the following family:
∀x ∈E, V(x) = {V ⊂E|x ∈i(V )} which is the family of neighborhoods of x as
already deﬁned. Then:
Proposition 2. Given a V type space (E, a(.), i(.)), the family V(x) is a preﬁlter
of subsets of E.
Proof. Given a family B of subsets of a set E, we say that B is a preﬁlter of
subsets of E if and only if:
(i) ∅/∈B
(ii) ∀A ∈B, (A ⊂B ⇒B ∈B)
Let x ∈E,
V ∈V(x) ⇒x ∈V . So it is impossible that ∅belongs to V(x).
Let V ∈V(x) and V ⊂W.
V ∈V(x) ⇒x ∈i(V ). But (E, a(.), i(.)) is a V type space.So V ⊂W ⇒i(V ) ⊂
i(W) then x ∈i(W) which implies that W ∈V(x) by deﬁnition of V(x).
By duality, for any x ∈E, we can consider the family A(x) deﬁned by A(x) =
{A ⊂E|x /∈a(A)}. It is easy to prove that A(x) is a preideal of subsets of E,
i.e.
(i) E /∈A(x)
(ii) ∀A ∈A(x), (B ⊂A ⇒B ∈A(x))
A(x) is said the family of points of E which are ”far away” from x.
Proposition 3. Let V(x) a preﬁlter of subsets of E for any x in E.
Let i(.) and a(.) the functions from P(E) into itself deﬁned as:
(i) ∀A ⊂E, i(A) = {x ∈E|∃V ∈V(x), V ⊂A}
(ii) ∀A ⊂E, a(A) = {x ∈E|∀V ∈V(x), V ∩A ̸= ∅}
Then (E, a(.), i(.)) is a V type space. We say it is generated by the family
{V(x), x ∈E}

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
121
Proof. First, we can note that functions a(.) et i(.) are c-dual.
x ∈(a(Ac)c ⇔x /∈a(Ac)
⇔⌝(∀V ∈V(x), V ∩Ac ̸= ∅)
⇔∃V, V ∈V, ⌝(V ∩Ac ̸= ∅)
⇔∃V, V ∈V, V ⊂A
⇔x ∈i(A)
a(∅) = {x ∈E|∀V ∈V(x), V ∩∅̸= ∅} = ∅
Obviously, ∀A ⊂E, A ⊂a(A) and let A and B such as A ⊂B, then:
∀V ∈V(x), V ∩A ̸= ∅⇒V ∩B ̸= ∅
so a(A) ⊂a(B)
At this point, given a V-type space (E, a(.), i(.)), we are able to determine
the family of neighborhoods V(x) for any x ∈E. And if for any x ∈E, we have
a preﬁlter of subsets of E, we are able to determine a pseudoclosure a(.) (and
so an interior function i(.)) such as we get a V-type space (E, a(.), i(.)). The
problem then is to answer the following question: given the initial V-type space
(E, a(.), i(.)), from the family V(x), ∀x, x ∈E, we deﬁne a new pseudoclosure
function and a new interior function, are they the same as the functions of the
initial space?
The following proposition gives this answer.
Proposition 4. The V-type pretopological space (E, a(.), i(.)) is generated by
an unique family of preﬁlters V(x) and conversely any family of preﬁlters V(x)
generates an unique pretopological structure on E.
Proof. In a ﬁrst step, let us prove the family V(x), ∀x ∈E is the only one
family of preﬁlters which generates (E, a(.), i(.)). For that, we suppose there
exists another family of preﬁlters W(x) which contains x and which generates
(E, a(.), i(.)). Let us suppose we can ﬁnd an element V such as V ∈V(x)−W(x).
This implies that x ∈i(V ) as V is an element of V(x) and that x /∈i(V ) as V
is not an element of W(x), which leads to a contradiction. The same conclusion
holds if V ∈W(x) −V(x).
Now, second step, let us suppose the family V(x) generates another space
(E, a∗(.), i∗(.)).
In this case, i(.) ̸= i∗(.), so ∃A ⊂E such as i(A) ̸= i∗(A). Let y ∈i∗(A)−i(A),
then ∃V ∈V(y) such as V ⊂A, which leads to y ∈i(A).
An advantage of V type spaces is the fact that the family of neighbor-
hoods of elements is a preﬁlter which characterizes the space. This gives
a practical way to build spaces. However, it can be diﬃcult to specify this family
of neighborhoods.
A practical concept is the following, by introducing the basis of neighborhood.
Deﬁnition 11. Given a V type pretopological space (E, a(.), i(.)), for any x in
E, the family B(x) is called a basis of neighborhoods of x if and only if
∀x ∈E, ∀V ∈V(x), ∃B ∈B(x) such as B ⊂V .

122
M. Bui et al.
The question then is to know how B(x) works in the deﬁnition of the pseudo-
closure a(.) and the interior i(.).
Proposition 5. Given a V type pretopological space (E, a(.), i(.)) deﬁned by the
family V(x), ∀x ∈E and the basis B(x), then:
(i) ∀A ⊂E, a(A) = {x ∈E|∀B ∈B(x), B ∩A ̸= ∅}
(ii) ∀A ⊂E, i(A) = {x ∈E|∃B ∈B(x), B ⊂A}
Proof. (i) x ∈{x ∈E|∀B ∈B(x), B ∩A ̸= ∅}
⇒x ∈{x ∈E|∀V ∈V(x), V ∩A ̸= ∅}
Conversely, let us suppose x ∈{x ∈E|∀V ∈V(x), V ∩A ̸= ∅} and
x /∈{x ∈E|∀B ∈B(x), B ∩A ̸= ∅}
⇒∃B ∈B(x), B ∩A = ∅
⇒∃V ∈V(x), B ∩A = ∅
⇒x /∈a(A)
(ii) is obvious from (i) by c-duality.
We have deﬁned open and closed subsets in a previous paragraph, which
leads to closure and opening of a subset A. In the most general case, opening
and closure does not necessarily exist.
In the case of V-type pretopological spaces, we get the following results which
lead us to a speciﬁc result about opening and closure.
Proposition 6. Given a V-type pretopological space (E, a(.), i(.))
(i) A ⊂E is open if and only if A is a neighborhood for each of its elements
(ii) Let x ∈E and V ⊂E, if there exists an open subset A such as {x} ⊂A ⊂V ,
then V is a neighborhood of x. The converse generally is not true.
(iii) Any union of open sets is an open set
(iv) Any intersection of closed sets is a closed set.
Proof. (i) A is open so A = i(A). Then, for any x ∈A, x ∈i(A) which implies
A is a neighborhood of any x in A.
(ii) {x} ⊂A ⊂V , so as A is open, x ∈i(A) which implies x ∈i(V ). Therefore
V is a neighborhood of x.
(iii) Let Aj, j ∈J a family of open sets and let us consider 
j∈J Aj. x ∈
j∈J Aj,
then
∃j0 such as x ∈Aj0. Aj0 is an open set, then it is a neighborhood of x which is
included in 
j∈J Aj. Therefore 
j∈J Aj is a neighborhood of x.
(iv) is proven by c-duality
This last result leads to the following which establish existence of opening and
closure of any subset in a V-type space.
Proposition 7. In a V-type pretopological space (E, a(.), i(.)), opening and clo-
sure of any subset A of E always exist.
Proof. Given A ⊂E, let us consider FA the family of all closed subsets which
contain A. FA ̸= ∅because E ∈FA

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
123
Let us consider H(A) = 
G∈FA G. H(A) is a closed subset from property (iv)
of the previous proposition. Obviously, it is the closest regarding inclusion, so
H(A) = F(A).
We have settle down the theoretical part concerning pre topological spaces.
These spaces corresponds to datasets in which we have complex objects linked by
various relations, possibly valued, and for which we want to identify similarities
according to a proximity measure. This is why we turn now to the concept of
quasipseudo-metrics.
3
Concept of Quasipseudometrics
Clustering techniques are based, as a rule, on the concept of distance that sup-
poses a precise axiomatic. This axiomatic sometimes proves to be very coercive,
in particular in the applications in social sciences. In various cases, a problem
that arises is the property of symmetry which expresses that the distance from
x to y is equal to the distance from y to x. The question is therefore to know
what it happens when this hypothesis is relaxed.
Obviously, the ﬁrst questions, in this case, are relevant of theoretical points:
– Is it possible to deﬁne a concept of ”metric” without the symmetry axiom,
but with suﬃcient properties to build get theoretical results?
– What are the basic properties of such a ”metric” ?
– What kind of topological space can we endow the metric space with ?
In this work, we propose to give some answers these questions related to
such a measure of distance fulﬁlling neither the property of symmetry nor the
triangular inequality property. We derive from this deﬁnition the ﬁrst properties
relative to the structures which the set could be endowed with. In particular, we
show that it is not more possible to get a topology and that the structures that
we can drift of such a distance measure are only pretopological structures ([5]
[6] [7] [1]), however endowed with interesting properties ([4]).
3.1
Deﬁnitions and Basic Properties
In this section, we shall focus ourselves on deﬁning more general spaces than
metric ones and we shall study properties of those spaces. This extension of
metric spaces is obtained by way of keeping only the ﬁrst axiom of a metric. We
present all deﬁnitions and basic results about quasipseudometrics hereafter.
Deﬁnition 12. (Quasipseudometric) Let be E a non empty set, we call
quasipseudometric on E, any mapping from E × E into P(E) such as
∀(x, y), (x, y) ∈E × E, d(x, y) = 0 ⇔x = y.

124
M. Bui et al.
Example 1. Let E = R2.
For any x = (x1, x2) and y = (y1, y2) in E, we set:
d(x, y) = 2(x1 −y1) + 2(x2 −y2) if y1 ≤x1 and y2 ≤x2
d(x, y) = (y1 −x1) + (y2 −x2) if y1 ≥x1 and y2 ≥x2
d(x, y) = 2(x1 −y1) + (y2 −x2) if y1 < x1 and x2 < y2
d(x, y) = 2(x2 −y2) + (y1 −x1) if y1 > x1 and x2 < y2
It is obvious to see that d(x, y) = 0 ⇔x = y and furthermore, if we take
x = (0, 0) and y = (0, 1) we get d(x, y) = 1 and d(y, x) = 2. Thus, d is a
quasipseudometric on E.
Example 2. Let E be the set E = {x, y, z, t} and ℜa binary relationship on E
characterized by the following table:
x y z t
x 0 1 0 1
y 0 0 1 1
z 1 1 0 0
t 0 0 1 0
We deﬁne d as:
d : E × E →R+
∀a ∈E, d(a, a) = 0
∀a ∈E,∀b ∈E, d(a, b) = n(a, b) where n(a, b) is the length of the shortest
path from a to b. So, we get the following distance table:
d x y z t
x 0 1 2 1
y 2 0 1 1
z 1 1 0 2
t 2 2 1 0
It is not a symmetric table, which shows that d is a quasipseudometric, not a
metric in the usual sense.
Example 3. Let us consider E = {a, b, c, d, e} and a distance table on E given by:
d
a
b
c
d
e
a
0
1
√2 √17
4
b
1
0
1
4
√17
c √2
1
0
3
√10
d √17
4
3
0
1
e
4
√17 √10
1
0
d is a classical metric on E. Let us now consider δ deﬁned on E by: δ(x, y) = k ⇔
y is the kth nearest neighbor of x. Then, we get the following table for δ:

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
125
δ a b c d e
a 0 1 2 4 3
b 1.5 0 1.5 3 4
c 2 1 0 3 4
d 4 3 2 0 1
e 3 4 2 1 0
We can see that δ fulﬁlls axioms of a quasipseudometric, not of a metric.
Deﬁnition 13. (Quasipseudometric space) The couple (E, d) where E is a non
empty set and d is a quasipseudometric on E, is called quasipseudometric space
In clustering techniques, we are used to work with metrics which do not fulﬁll
the third axiom, the triangle axiom. We know it is without consequences on
existing methods of clustering. However, when the axiom of symmetry is not
fulﬁlled, we cannot use the classical methods of clustering and we have to design
new ones. We also can imagine an intermediate case between the case where d
is symmetric and the case where d is not symmetric.
Deﬁnition 14. Let (E, d) be a quasipseudometric space. If there exists a map-
ping t from E into itself such that ∀(x, y), (x, y) ∈E2, d(x, y) = d(t(y), t(x)),
then we say that (E, d) is t-pseudosymmetric
We can note that, if t is identity mapping, we get the symmetry property.
From now, we shall work with a quasipseudometric d which is not a symmetric
one.
Deﬁnition 15. Let (E, d) a quasipseudometric space, we call surface of sym-
metry the set S deﬁned by S = {(x, y ∈E2|d(x, y) = d(y, x)}
Then, it is obvious to note that:
Proposition 8. Let (E, d) a quasipseudometric space with a surface of symme-
try S
– S ̸= ∅
– ∀(x, y) ∈E2, (x, y) ∈S ⇔(y, x) ∈S
Moreover, we can note that S is a neighborhood of the diagonal Δ of E
(Δ = (x, x) | x ∈E). It is also obvious that d is a symmetric quasipseudometric
if and only if S = E × E. By analogy with metric spaces, we can study the
concept of open or closed ball, with center x and radius r, but in the case of a
quasipseudometric space, we have to distinguish between right and left balls.
Deﬁnition 16. Let (E, d) be a quasipseudometric space, r a positive real number
and x a point of E
– We call half open right ball with center x and radius r, the set, noted
˙Bd(x, r), deﬁned by
˙Bd = {y ∈E|d(x, y) < r}

126
M. Bui et al.
– We call half closed right ball with center x and radius r, the set, noted
¯Bd(x, r), deﬁned by
¯Bd = {y ∈E|d(x, y) ≤r}
– We call half open left ball with center x and radius r, the set, noted ˙Bg(x, r),
deﬁned by
˙Bg = {y ∈E|d(y, x) < r}
– We call half closed left ball with center x and radius r, the set, noted ¯Bg(x, r),
deﬁned by
¯Bg = {y ∈E|d(y, x) ≤r}
Example 4. If we consider the previous example 1 with x = (0, 0) and r = 1, we
get the following half right and left balls (see Figures 1, 2):
Fig. 1. Half right ball
Deﬁnition 17. Let us consider a quasipseudometric space (E, d). For any x in
E and any r positive real number:
– We call lower open ball (resp. lower closed ball), with center x and radius r,
the set, noted ˙Binf(x, r) (resp. ¯Binf(x, r)), deﬁned by ˙Binf(x, r) = ˙Bd(x, r)∩
˙Bg(x, r)
(resp. ¯Binf(x, r) = ¯Bd(x, r) ∩¯Bg(x, r)).
– We call upper open ball (resp. lower closed ball), with center x and radius r,
the set, noted ˙Bsup(x, r) (resp. ¯Bsup(x, r)), deﬁned by ˙Bsup(x, r) = ˙Bd(x, r)∪
˙Bg(x, r)
(resp. ¯Bsup(x, r) = ¯Bd(x, r) ∪¯Bg(x, r))

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
127
Fig. 2. Half left ball
We then get the obvious following result:
Proposition 9. Let (E, d) be a quasipseudometric space.
(i) ∀r, r > 0, ∀x, x ∈E, ∀y, y ∈E, y ∈˙Bd(x, r) ⇔x ∈˙Bg(y, r)
(ii) ∀r, r > 0, ∀x, x ∈E, ∀y, y ∈E, y ∈˙Binf(x, r) ⇔x ∈˙Binf(y, r)
(iii) ∀r, r > 0, ∀x, x ∈E, ∀y, y ∈E, y ∈˙Bsup(x, r) ⇔x ∈˙Bsup(y, r)
(iv) ∀r, r > 0, ∀x, x ∈E, x ∈˙Bd(x, r) ∩˙Bg(x, r)
The last proposition obviously holds for closed balls. Let us suppose that the
quasipseudometric d is t-pseudo symmetric, then
∀(x, y) ∈E2, d(x, y) = d(t(y), t(x)).
So
y ∈˙Bd(x, r) ⇔d(x, y) < r
⇔d(t(y), t(x)) < r
⇔t(y) ∈˙Bg(x, r).
If t−1(A) denotes the set deﬁned by
t−1(A) = {x, x ∈E|t(x) ∈A}, we can write:
Proposition 10. Let (E, d) be a quasipseudometric space, if d is t-pseudo sym-
metric, then :
(i) ˙Bd(x, r) = t−1( ˙Bg(t(x), r))
(ii) Bd(x, r) = t−1(Bg(t(x), r))

128
M. Bui et al.
4
Quasipseudometrics and Pretopological Spaces
In this section, we shall study how to build pretopological structures related to a
given quasipseudometric on a set E, then we will study what are their properties.
First, we shall consider a given threshold r, r > 0.
Let us consider Br(x) = { ˙Bd(x, r), ˙Bg(x, r)} and the preﬁlter Vr of subsets of E
generated by Br(x), i.e. Vr = {V, V ∈P(E) | V ⊃˙Bd(x, r) or V ⊃˙Bg(x, r)}.
Then, let us deﬁne the mapping ar from P(E) into itself by
ar(A) = {x, x ∈E|∀V, V ∈Vr, V ∩A ̸= ∅}
Deﬁnition 18. The pretopological structure Pr deﬁned onto E by families Vr
is called the r-pretopology associated to d.
This pretopology is a V-type one, but generally speaking it is not a VD-type
one (see [1]). If we consider the ﬁrst example of the previous section, in the case
where r = 1, B1(0) = { ˙Bd(0, 1), ˙Bg(0, 1)}. Then V ∈V1(0) ⇔V ⊃˙Bd(0, 1) or
V ⊃˙Bd(0, 1). Let us consider the case illustrated by the ﬁgure 3:
Fig. 3. Neighbourhood
V is a neighbourhood of 0, W is also a neighbourhood of 0, but V ∩W is not
a neighbourhood of 0. This implies that V1(0) is not a ﬁlter of subsets of E and
then the pretopology is not a VD one.
The pseudoclosure map can then be expressed as follows:
Proposition 11. ∀A, A ∈P(E)
ar(A) = {x ∈E | ˙Bd(x, r) ∩A ̸= ∅and ˙Bg(x, r) ∩A ̸= ∅}

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
129
Thus, we know the main properties of the pretopology Pr with a ﬁxed r.
Let us examine what it can be said when we consider two diﬀerent pretopolo-
gies generated by two distinct thresholds r1 and r2. Let us consider the case
where r1 ≤r2. Then, it is immediate to note that :
∀x, x ∈E, ˙Bd(x, r1) ⊂˙Bd(x, r2)
∀x, x ∈E, ˙Bg(x, r1) ⊂˙Bg(x, r2). Let us note by Br1(x) and Br2(x) the corre-
sponding neighborhoods basis.
∀V, V ∈Vr2(x), V ⊃˙Bd(x, r2) ∨V ⊃˙Bd(x, r2)
⇒∀V, V ∈Vr2(x), V ⊃˙Bd(x, r1) ∨V ⊃˙Bd(x, r1)
⇒∀V, V ∈Vr2(x) ⇒V ∈Vr1(x)
Then:
Proposition 12. If r1 ≤r2, the pretopology Pr1 is coarser than the pretopology
Pr2 (we denote Pr1 ≺Pr2)
These pretopologies, generated by a given threshold, rely upon the concepts of
half right balls and half left balls. But, we also have the concepts of sup and inf
balls.It is possible to generate two new pretopologies from them by setting:
Pr = {V(x), x ∈E}
where
V(x) = {V ⊂E/V ⊃˙Binf(x, r)}
Pr = {V(x), x ∈E}
where
V(x) = {V ⊂E/V ⊃˙Bsup(x, r)}
These two pretopologies are obviously VD pretopologies and by the deﬁnitions
of ˙Binf(x, r) and ˙Bsup(x, r), we get:
Proposition 13. The pretopologies Pr and Pr are VD pretopologies and we
have Pr ≺Pr ≺Pr
Up to now, we worked with pretopologies generated by giving a threshold r.
The selection of that threshold may introduce a bias, so it would be important
to be able to deﬁne associated pretopologies to a quasipseudometric without
considering such a threshold. For that, let us consider
∀x, x ∈E, B(x) = { ˙Bd(x, r), ˙Bg(x, r), r > 0}
and let denote by V(x) the preﬁlter generated by B(x), i.e. V ∈V(x)
⇔
(∃r, r > 0, V ⊃˙Bd(x, r)) ∨(∃r′, r′ > 0, V ⊃˙Bg(x, r′)) So we can put
∀A, A ⊂E, a(A) = {x ∈E/∀V, V ∈V(x), V ∩A ̸= ∅}

130
M. Bui et al.
It is obvious to see that :
a(∅) = ∅
∀A, A ∈E, A ⊂a(A)
It follows the deﬁnition of the induced pretopology on E by the quasipseudo-
metric:
Deﬁnition 19. (Induced pretopology) The pretopology which is deﬁned by the
family V(x)given above and the pseudo closure a of which is the function deﬁned
above, is called the pretopology induced on E by the quasipseudometric d
Proposition 14. ∀A, A ⊂E
a(A) = {x ∈E | ∀r, r > 0, ( ˙Bd(x, r) ∩A ̸= ∅) ∧( ˙Bg(x, r) ∩A ̸= ∅)}
Proof.
∀A, a(A) = {x ∈E/∀V, V ∈V(x), V ∩A ̸= ∅}
⇒
x ∈A ⇒∀r, r > 0, ( ˙Bd(x, r) ∩A ̸= ∅) ∧( ˙Bg(x, r) ∩A ̸= ∅)
Conversely, let us suppose that : ∀r, r > 0
˙Bd(x, r) ∩A ̸= ∅∧˙Bg(x, r) ∩A ̸= ∅) ∨(∃V 0, V 0 ∈V(x), V 0 ∩A = ∅)
As ∃r0 such that V 0 ⊃˙Bd(x, r0) or V 0 ⊃˙Bg(x, r0)
This leads us to ˙Bd(x, r0) ∩A = ∅or ˙Bg(x, r0) ∩A = ∅which is contradictory.
Proposition 15. The pretopology induced by a quasipseudometric d is a V one.
Proof. Let us suppose that A ⊂B
x ∈a(A) ⇔∀r, r > 0, ˙Bd(x, r) ∩A ̸= ∅∧˙Bg(x, r) ∩A ̸= ∅
⇒
x ∈a(A) ⇔∀r, r > 0, ˙Bd(x, r) ∩B ̸= ∅∧˙Bg(x, r) ∩B ̸= ∅
Then A ⊂B ⇒a(A) ⊂a(B)
Remark. This pretopology is not a VD-one because, generally speaking, V(x) is
not a ﬁlter of subsets of E.
As in the case of pretopologies using a given threshold for r, we can use
the concepts of upper balls and lower balls to deﬁne two other pretopologi-
cal structures on E. Let us consider B(x) = { ˙Binf(x, r), r > 0} and B(x) =
{ ˙Bsup(x, r), r > 0}. We can put:
a(A) = {x ∈E/ ˙Binf(x, r) ∩A ̸= ∅, ∀r, r > 0}
and
a(A) = {x ∈E/ ˙Bsup(x, r) ∩A ̸= ∅, ∀r, r > 0}
Obviously, we deﬁne two pseudoclosures of two pretopologies respectively noted
P and P. We straightforwardly get the following result:
Proposition 16. If P denotes the pretopology induced by the quasipseudometric
d, we have : P ≺P ≺P

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
131
Remark. The two pretopologies P and P are VD ones.
An interesting particular case is the case when d fulﬁlls the triangle axiom al-
though being non symmetric. In that case, what happens to the pretopology
induced by d ?
Proposition 17. If d fulﬁlls the triangle axiom, the pseudoclosure function a
of the pretopology P induced by d is an idempotent function.
Proof. We have to show that
∀A, A ⊂E, a(a(A)) = a(A).
In fact, it is suﬃcient to prove that
∀A, A ⊂E, a(a(A)) ⊂a(A).
Let us consider x ∈a(a(A)).
Then, ∀r, r > 0, ∃yr, yr ∈a(A),
d(x, yr) < r ∧∃y′
r, y′
r ∈a(A), d(y′
r, x) < r
But yr ∈a(A) ∧y′
r ∈a(A), then: ∀s, s > 0, ∃zs, zs ∈A, d(yr, zs) < s ∧
∃z′
s, z′
s ∈A, d(z′
s, yr) < s. By the triangle axiom, we can say: ∀r, ∀s, ∃zs, zs ∈A,
d(x, zs) < r + s ∧∃z′
s, z′
s ∈A, d(z′
s, x) < r + s It is suﬃcient to prove that
x ∈a(A)and then ∀A, A ⊂E, a(a(A)) ⊂a(A).
Now, we have deﬁned a family of pretopologies on E : the pretopologies as-
sociated to the quasipseudometric d for a given value of r. We also have the
pretopology induced on E by the quasipseudometric d.
What are the links between all these pretopologies?
The answer is given by the following result.
Proposition 18. ∀A, A ⊂E, a(A) = 
r>0 ar(A).
The pretopology P induced by the quasipseudometric d is the lower bound of the
pretopologies Pr.
Proof. Let us consider x, x ∈a(A)
⇒∀r, r > 0, ˙Bd(x, r) ∩A ̸= ∅and ˙Bg(x, r) ∩A ̸= ∅
⇒∀r, r > 0, x ∈ar(A)
The following deﬁnition and result allows to characterize elements of a(A).
Deﬁnition 20. Let (E, d) a quasipseudometric space, A a subset of E. For every
x in E, we put:
d(x, A) = inf{d(x, y), y ∈A}
d(A, x) = inf{d(y, x), y ∈A}
We can then write:
Proposition 19. Let A ∈P(E), the two following assertions are equivalents
(i) x ∈a(A)
(ii) d(x, A) = 0 and d(A, x) = 0

132
M. Bui et al.
Fig. 4. Expert gesture
Fig. 5. Expert gesture VS Newbie test 1
Proof. x ∈a(A)
⇔∀r, r > 0, ˙Bd(x, r) ∩A ̸= ∅and ˙Bg(x, r) ∩A ̸= ∅
⇔∀r, r > 0, ∃yr, yr ∈A, 0 ≤d(x, yr) < r and ∀r, r > 0, ∃y′
r, y′
r ∈A, 0 ≤
d(y′
r, x) < r
⇔inf{d(x, y), y ∈A} = 0 and inf{d(y, x), y ∈A} = 0
⇔d(x, A) = 0 and d(A, x) = 0

Gesture Trajectories Modeling Using Quasipseudometrics and Pre-topology
133
Fig. 6. Expert gesture VS Newbie test 2
A consequence of this result is that it is possible to characterize the neighbor-
hoods of x by means of the quasipseudometric d:
Proposition 20. Let V ∈P(E), a necessary and suﬃcient condition for V to
be a neighborhood of x is that d(x, V c) ̸= 0 or d(V c, x) ̸= 0
Proof. It is suﬃcient to note that if V is a neighborhood of x, that means that
x does not belong to a(V c).
5
Discussion
The results presented in this paper are the ﬁrst results of a complete analysis
on quasipseudometrics and on related pretopological spaces. Other theoretical
questions are still subject to further investigations, in particular the analysis of
conditions allowing a V-pretopological space to be associated to a quasipseudo-
metric space. Otherwise, concerning the applications in the ﬁeld of data analysis,
it remains to deﬁne clustering methods founded on that concept of quasipseu-
dometric.
In our illustrating example, we intend to use our modeling approach to iden-
tify the proximity between various trajectories corresponding to the gesture of
an expert serving a glass of wine versus the gesture of a newbie learning the
technique. The question is thus to decide, given a threshold, the trajectories
which look alike in order to identify when a newbie has been trained enough to
master the gesture — which should then be analog to the one of the expert .

134
M. Bui et al.
Coping with the symmetry (in the semi-circular trajectories in our case study)
is the particular characteristic of our modeling of the dataset with quasipseudo-
metric pre topological space. It allows to distinguish gesture patterns that would
be next to the expert one even if they are going around externally compared to
gesture border of the expert and consider as more diﬀerent gestures that are
too direct ... This is clearly addressed by the general half right/left balls of the
quasipseudometrics.
References
1. Belmandt, Z.: Manuel de pr´etopologie. Editions Herm´es (1993)
2. Bourbaki, N.: Topologie G´en´erale. 2 tomes, Hermann (1971)
3. Cech, E.: Topological Spaces. John Wiley and Sons (1966)
4. Lamure, M., Dalud-Vincent, M., Brissaud, M.: Pretopology as an extension of graph
theory: the case of strong connectivity. International Journal of Applied Mathemat-
ics 5, 455–472 (2001)
5. Duru, G.: Contribution `a l’analyse des espaces abstraits, le cas des images digitales.
Technical report, Th`ese d’Etat, Universit´e Claude Bernard - Lyon (November 1987)
6. Duru, G., Auray, J.P., Brissaud, M.: Connexit´e des espaces pr´e f´erenci´es. In: Col-
loque Math´ematiques Discr`etes: Codes et hypergraphes (1978)
7. Lamure, M.: Contribution `a l’analyse des espaces abstraits, le cas des images digi-
tales. Technical report, Th`ese d’Etat, Universit´e Claude Bernard - Lyon (November
1987)

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 135–142, 2014. 
© Springer International Publishing Switzerland 2014  
Analogy and Metaphors in Images 
Charles Candau, Geoffrey Ventalon, Javier Barcenilla, and Charles Tijus 
Laboratory Cognitions Humaine et Artificielle, University Paris 8, France 
Abstract. Museums have large databases of images. The librarians that are us-
ing these databases are doing two types of images search: either they know 
what they are looking for in the database  (a specific image or a specific set of 
well defined images such as kings of France), or they do not know precisely 
what they are looking for (e.g., when they are required to build images portfo-
lios about some concepts such as “decency”). As each image is having a num-
ber of metadata, searching for a well-defined image, or for set of images, is  
easily solved. On the contrary, this is a hard problem when the task is to  
illustrate a given concept such as “freedom”, “decency”,  “bread”, or “transpa-
rency” since these concepts are not metadata. How to find images that are 
somewhat analogs because they illustrate a given concept? 
We collected and analyzed the search results of librarians that were given 
themselves the task of finding images related to a given concept. Seven rela-
tions between the concept and the images were found as explanation of the se-
lection of images for any concept: conceptual property, causality, effectivity 
semantic, anti-logic, metaphorical-vehicle and metaphorical-topic. The inter-
rate agreement of independent judges that evaluated the relations was of .78. 
Finally, we designed an experiment to evaluate how much metaphor in im-
ages can be understandable. 
1 
Introduction: Prospective Ergonomics about the Design of 
Search Engine for Images  
Everywhere, homes or workplaces, in the streets or the public areas, where we go for 
business, service, shopping, leisure or travel, they are digital systems with which we 
interact. Because they have to be adapted to humans, these digital systems include a 
model of theirs users [1] and are more and more made of Cognitive Technologies that 
are technologies that process as inputs data provided by their users. These emerging 
Cognitive Technologies are flourishing areas of multifaceted scientific research and 
research development, including neuroscience (e.g., brain computing), psycho-
physiology (e.g., emotive computing), psychophysics (e.g., actimetry), cognitive  
psychology (e.g., digital reading and learning), computational linguistics (e.g.,  
texts processing) to be used in association with artificial intelligence, cognitive robot-
ics, distributed Human-Machine systems, cognitive ergonomics, and cognitive  
engineering.  
The framework of the content of this paper is the open innovation process of imag-
ing future things (prospective ergonomics) based on cognitive technologies [2], in the 

136 
C. Candau et al. 
 
context of Living Labs, more precisely in the context of LUTIN, which is a Living 
Lab located in Universcience - City of Science and of Industry in Paris, a member of 
LabEx Smart dedicated to “Smart Human/Machine/Human Interactions In The Digital 
Society”. For an user centered approach of conception of future services and products, 
these are made for and by the people and then industrially manufactured, and a step 
further, one considers as a citizen duty to participate in innovation process: “innova-
tion needs you, innovation needs your expertise” [3]. Thus, although we still need the 
best methods of prospective innovation, the problem at hand is not how to imagine, 
but what can be ergonomics of things that do not exist: their usability and learnability. 
In addition, with cognitive technologies, ergonomics studies for prospective ergonom-
ics are not solely how to facilitate interactions with a digital device, but also how to 
implement the system with a pertinent model of the users (data to collect, computing 
modes). 
The emerging field of prospective ergonomics has potential for many domains, 
such as everyday life technologies, conception of teaching and learning in the class-
room, e-learning, science and technology-related museology, e-government applica-
tions, heath, military and intelligence applications, and so on. However, among these 
fields, because of its contents, searching for images is a special challenge. Although, 
it appears easy to group pictures according to one object, it is a difficult topic about 
how to group things that are analogs. For instance, Museums have large databases of 
images (e.g., one of the database involved in the present project, - which is one of the 
Réunion des Musées Nationaux, RMN, is about of 700 000 images). The librarians 
that are using these databases are doing two types of images search: either they know 
what they are looking for in the database  (a specific image or a specific set of well 
defined images such as kings of France), or they do not know precisely what they are 
looking for (e.g., when they are required to build images portfolios about some con-
cepts such as “decency”). As each image is having a number of metadata, searching 
for a well-defined image, or for set of images, is easily solved. On the contrary, this is 
a hard problem when the task is to illustrate a given concept such as “freedom”, “de-
cency”,  “bread”, or “transparency” since these concepts are not metadata. How to 
find images that are somewhat analogs because they illustrate a given concept? 
2 
Searching for Images 
Searching for images is a challenging cognitive activity for several reasons. When the 
search is a well-defined search (we know what we want to find), a profitable search 
engine for images must be able to quickly find what we want by providing a small 
result set of images that contains what is sought. However, most of the times, the 
research is often poorly defined (we do not know precisely what we are looking for) 
and, even more, is also multi-criteria (about contents, but also definition, size...). 
These criteria are often approximate: we would like a particular criterion being met, 
but a compromise on this criterion can be done if another criteria are quite satisfied. 
When the search is poorly defined, the size of the group of images that correspond 
to what would satisfy the search is unknown and when you got the results, it is  

 
Analogy and Metaphors in Images 
137 
 
unclear if any others images who would be more satisfactory. It follows that judgment 
about the search criterion is problematic because we do not know when to stop 
searching. Another difficulty comes from results that were not been retained (because 
we sought better): they are difficult to find again because the capacity of working 
memory (how did I do?). Short-term human cognition capacities are limited. 
Finally, there is above all the discovery and the encounter during search of images 
that were not searched for (serendipity) and for which we would be eventually ready 
to question the selection criteria and to renew the search. 
For prospective ergonomics, we can therefore expect the user of an images search 
engine find quite useful and enjoyable a system that facilitates his research if it be-
comes so easy to find, not only what we want, but also to find what was not re-
searched. This, especially, if the interface is the most appropriate way and if users 
have a good understanding of its operations, in order to maximize them. 
Our work was to define specific needs of users in terms of image search, and new 
services in this area. Users in question are professionals who use the database of about 
700,000 images. Activity of librarians was searching for specific images, but also the 
realization of portfolios. Achieving portfolios carries a significant theoretical interest. 
To make a portfolio (i.e., a collection of images of works in a given subject), is to find 
a set of images that are analogs in the sense that they are exemplars of images of a 
given concept. 
Thus, the question at hand is how librarians are searching for images related to a 
concept, and how to help them finding images that are analogs. 
3 
Analogy in Images 
We have seen that the task is to find a set of images corresponding to a theme (e.g., 
bread) or to a concept (modesty) in order to get a set of images to be retained in a 
portfolio. 
This type of task is emblematic of a poorly defined search:  "How to illustrate ab-
stract concepts?" "How to find what we are unaware ... and we do not even know 
name?" An example is how to find the illustration of a work of Umberto Eco entitled 
"To say almost the same thing. " Thus, the task is to find a set of images constituting a 
portfolio of images corresponding to "almost the same" in order for the author or pub-
lisher to finally choose the one he likes the most. 
The theory we adopt, is given by Tijus et al. [4]. Searching for a specific image is a 
well-defined problem solving [5] that requires legal reasoning. In opposite, searching 
for images that could illustrate a concept is a kind of innovative problem solving that 
requires finding ideas of images content. The main theory is the analogy-based 
theory: innovation is the transportation of a solution process from a source domain to 
the problem domain (if to illustrate “freedom”, I was using a photo of the statue of 
liberty, to illustrate “decency”, I will search for a statue).  Thus, the making of analo-
gies, through Case-based or analogy-based systems to find analogous cases could be 
of help [6]. By contrast, we explore a cognitive mechanism that is based on the mak-
ing of substitutes as sources. 

138 
C. Candau et al. 
 
Tijus et al. [4] reported that for Sir Ernst Gombrich [7], a well-known art specialist, 
a ordinary hobbyhorse "Should we describe it as an 'image of a horse'?" Is it rather "a 
substitute for a horse", a "horse like" that can be ridden by a child. There is no analo-
gy between the horse and the stick. Gombrich’s conclusion was that “substitution may 
precede portrayal and creation communication". 
According to [8], the finding of possible sources for analogy is as follows. First, 
the goal being defined, a matching process starts by carrying out a large number of 
comparisons between the components (objects, objects attributes and relations) of the 
sources and of the goal. Second, source and structure are mapped for "global" identi-
fications. However, [9] discussed the order of these successive phases (encode target, 
find sources with local matches, match structure) in human analogical processes. With 
the additional problem of how components are selected for mapping [10], since anal-
ogy is based on similarity with already encoded data, it would be difficult to find a 
never used solution.  
We advocate that the solution is a substitute that is inferred while searching how 
the problem could be solved. Differently of analogy, the process does not involve 
matching sources to be modified and adapted to target, but supposes target adaptation 
to find sources. When searching for images related to a concept, the solution might be 
finding substitutes. 
4 
Experiment: Observing and Analyzing the Conceptual Search 
of Images 
This task was to analyze the results of research done by librarians, in order to induc-
tively find the reasons for the selection of images. If it is possible to determine the 
selection criteria, then an automatic search engine could use these criteria to facilitate 
the search. These criteria are of semantic order, conceptual. 
The librarians have an indexed images base (700,000 images). Besides a number of 
information about each image that might be used for the search (size, definition, au-
thor, date...): for example, to provide images about an exhibition on Egypt for the 
general public, descriptors (eras, collections...) are known and can be used. In oppo-
site, librarians are also required to conduct more conceptual research as to form an 
online portfolio on a particular theme (the naked, modesty, the taste). For this second 
type of research, conventional descriptors cannot be used: this is the kind of research 
during which we do not know what to look for.  
4.1 
Method 
The interviews were made with 15 Participants (8 librarians, 3 Iconographers, 2 illu-
strators and 2 Art Directors in advertising). The interview was about the use of search 
engines for finding images. The method was the Critical Incident Technique [11], 
which is about recollection of facts, which comprises the set of pictures that were 
found. Some of these groups of analog pictures can be found at  
 

 
Analogy and Metaphors in Images 
139 
 
http://www.photo.rmn.fr/cf/htm/ 
CDocT.aspx?V=CDocT&E=2C6NU0O0Z17S&DT=ALB 
http://www.photo.rmn.fr/cf/htm/ 
CDocT.aspx?V=CDocT&E=2C6NU0YDXTJU&DT=ALB 
 
We collected a number of search results for a number of topics such as "Shame", 
"transparency", “decency”, "bread", "hair" and "prejudices about women", with sub-
searches such as "women at work", "parity", "nudity", "parenthood". For bread, there 
were combinations such as "bread making", "crafts related to bread making", "manu-
facturing", "baker", "bakery", "consumption", "bread: a daily concern around the 
world", "the sacred bread", "the symbolic bread". 
Thus, our work for PCE was to analyze the results of research librarians in order to 
find inductively the reasons for the selection of images.  
If we could determine the selection criteria librarians were using, then we could 
use these criteria to help facilitate the search. These criteria, which are of semantic 
nature, might be added to the search engine in addition to other kinds of image com-
ponents. The principle is that users' needs stems from their embodiment of the task. 
Note that searching for a well-defined image is as a well-defined problem and that 
looking for images related to a given concept is as an ill-defined problem.  
Because we need a specific goal, we reasoned that a concept refers to objects that 
have this conceptual property (ugliness refers to objects that are ugly; freedom to ob-
jects which are free), but also because of causality (which makes it ugly, makes it free), 
of effectivity (what are the consequences to be ugly; to be free), or semantic: near to be 
(could be ugly, might be free), anti-logical (a great beauty to designate ugliness; un-
dressed to represent decency by the lack of decency; lack of freedom to designate free-
dom), metaphorical-vehicle to mean something else (ugliness to designate the absence 
of moral; freedom to designate democracy), metaphorical-topic (to be represented by 
another object: the mud to indicate ugliness,  bird to indicate freedom). 
4.2 
Results 
Figure 1 shows samples of pictures that were grouped as instances of decency and as 
instances of transparency, one image can be in both sets. Figure 2 shows the kind of 
relation that links the image to the concept. 
 
 
Fig. 1. Some pictures found as analogous for being examples of « decency» (top) and of 
“transparency” (bottom) 

140 
C. Candau et al. 
 
 
Fig. 2. Images for freedom, according to the relations between the image and the concept to be 
illustrated, from left to right: Name, Attribute, Causality, Effectivity, and Semantics 
We reasoned that, starting from a set of images corresponding to a concept, it is 
possible to determine what in each image refers to the concept using non-exclusive 
relations as criteria. 
Thus, we defined six types of relationships between the image and the concept. 
Then, we define some coding rules to attribute or not an image with each of the six 
relationships and three judges coded 170 photos of 3 portfolios ("bread", "modesty" 
and "transparency"), while two other judges coded 196 photos of 2 portfolios (‘free-
dom” and “decency”) attributing each of the six relations, assessed independently for 
each image. The inter-rate agreement was of .78 and of .84 respectively. 
Note that the judges as well as feedback from the librarians affected the final 6 
types of " concept-to-images" relationships that were identified: 
• Name: first relationship between the concept and the presence of his name in the 
image (e.g. concept of "freedom " with " Statue of Liberty "). 
• Cause: the object (or action) in the picture helps to explain the existence of the 
content of the concept. (e.g., the "revolt" contributes to the "freedom") . 
• Effect: the object represented occurs as a consequence of the concept application 
(e.g. "freedom" by excess may arise as "debauchery.") 
• Attribute: the object is represented as a single component or a concept (e.g., "free-
dom " offers the quality to be free, autonomous, independent, etc. 
 
 
Fig. 3. Galois Lattice describing how the 6 object-to-concept relations distributed over the 
categories of images (one single image is given to illustrate the category) 

 
Analogy and Metaphors in Images 
141 
 
• Typicality: the object represented is clearly characteristic and could serve as a 
model of the concept (e.g., the "bread" can be characterized by a loaf of round, 
brown, notched at the top, etc.) 
• Semantics: this is a relationship that is reported with a higher degree of cognitive 
flexibility (compared to the relationships mentioned above) on the linkage between 
the figurative object and the concept. 
5 
Discussion and Conclusion 
The making of portfolios could well be done with intuitive navigation through im-
ages, by linking automatic metadata associated with each image corresponding to the 
selected theme, as well as navigation data, leading to relevant images for this topic. 
Thus, if a librarian decided to create a portfolio with a search engine related with a 
thesaurus. The system processes all available information (navigation data, compari-
son of formal and textual metadata images...), and provides relevant findings taking 
into account the six types of relationship.  
These relationships therefore participate to guide the user in his search by a seman-
tic reformulation, thus contributing to the "serendipity" of images search in the data-
base. The librarians say that their search were very intuitive and did not follow the 
kind of rules we listed, they found this method impressive and agree that it could be 
useful to have a search engine that could use them.  
Thus, having a detailed description of the future semantic search engine, we started 
defining how the search could be simple (search in turn for each of the relationships 
according to the librarian preferences), familiar (using procedures of advanced 
search), feedbacks (an image should indicate which of the relationships it is exempli-
fied), transparency (display the chosen criteria), presence (good image definition if 
needed; low definition for first survey), safety (does not be losing preceding results) 
and affordances (have an image being the prototype of the relationship). 
Next step of this current research for future search engines is to test these relation-
ships as research criteria with naïve users. 
Acknowledgment. This work was supported by FUI EGONOMY. 
References 
1. Tijus, C., Cambon de Lavalette, B., Poitrenaud, S., Leproux, C.: L’interaction autorégula-
trice entre dispositif et utilisateur: une modélisation des inférences sur les durées du  
parcours routier. Le Travail Humain 66(1), 23–44 (2003) 
2. Tijus, C., Barcenilla, J., Jouen, F., Rougeaux, M.: Open innovation and prospective ergo-
nomics for smart clothes. In: 2nd International Conference on Ergonomics in Design 
(2014) 
3. Barcenilla, J., Tijus, C.: Ethical issues raised by the new orientations in ergonomics and 
living labs. Work 41, 5259–5265 (2012) 

142 
C. Candau et al. 
 
4. Tijus, C., Poitrenaud, S., Léger, L., Brézillon, P.: Counterfactual Based Innovation: A Ga-
lois Lattice Approach of Creative Thinking. In: International Conference on Computing 
and Communication Technologies, RIVF 2009, pp. 1–4. IEEE (2009) 
5. Newell, A., Simon, H.A.: Human problem solving. Prentice Hall, Englewood Cliffs (1972) 
6. Roth-Berghofer, T.R.: Explanations and Case-Based Reasoning. Foundational Issues. In: 
Funk, P., González Calero, P.A. (eds.) ECCBR 2004. LNCS (LNAI), vol. 3155, pp. 389–
403. Springer, Heidelberg (2004) 
7. Gombrich, E.H.: Mediations on a hobby horse. In: Meditations on a Hobby Horse and 
Other Essays on the Theory of Art, London (1963) 
8. Gentner, D., Toupin, C.: Systematicity and surface similarity in the development of analo-
gy. Cognitive Science 10(3), 277–300 (1986) 
9. Ripoll, T., Eynard, J.: A Critical Analysis of Current Models of Analogy. In: Proceedings 
of the 2002 Information Processing And Management of Uncertainty in Knowledge-Based 
Systems, IPMU 2002 (2002) 
10. Kwon, H., Im, I., Van de Walle, B.: Are you thinking what I am thinking – A comparison 
of decision makers’ cognitive map by means of a new similarity measure. In: Proceedings 
of the 35th Hawaii International Conference on System Sciences, vol. 78, p (2002) 
11. Flanagan, J.C.: The critical incident technique. Psychological Bulletin 5, 327–358 (1954) 
 
 

Fuzzy Transform Theory in the View of Image
Registration Application
Petr Hurt´ık, Irina Perﬁlieva, and Petra Hod´akov´a
University of Ostrava, Centre of Excellence IT4Innovations,
Institute for Research and Applications of Fuzzy Modeling,
30. dubna 22, 701 03 Ostrava 1, Czech Republic
{petr.hurtik,irina.perfilieva,petra.hodakova}@osu.cz
Abstract. In this paper, the application of the fuzzy transforms of the
zero degree (F 0-transform) and of the ﬁrst degree (F 1-transform) to the
image registration is demonstrated. The main idea is to use only one
technique (F-transform generally) to solve various tasks of the image
registration. The F 1-transform is used for an extraction of feature points
in edge detection step. The correspondence between the feature points
in two images is obtained by the image similarity algorithm based on
the F 0-transform. Then, the shift vector for corresponding corners is
computed, and by the image fusion algorithm, the ﬁnal image is created.
Keywords: image registration, feature detection, edge detection, image
similarity, image fusion.
1
Introduction
In computer graphics, interactions between the machine and the real worlds are
basically ensured by the image processing. One of the tasks is to represent data
for computer processing to be similar to the humen eye vision as much as possi-
ble. Therefore, this task is very popular in developing soft-computing methods. It
became a common practice that soft computing methods work with uncertain in-
formation and can achieve better result than methods based on crisps information.
One of the eﬀective soft computing methods is fuzzy transform (F-transform
for short) developed by Irina Perﬁlieva. The main theoretical preliminaries were
described in [1][2]. The F-transform is a technique that performs a transfor-
mation of an original universe of functions into a universe of their “skeleton
models”. Each component of the resulting skeleton model is a weighted local
mean of the original function over an area covered by a corresponding basic
function. The F-transform consists of two steps: direct and inverse transform.
This method proved to be very general and powerful in many applications. Par-
ticularly, image compression [3][4], where the user can control the strength and
the quality of compression by choosing the number of components used in F-
transform. Another application is image fusion [7][8], where several damaged
images are fused in one image which then has better quality than all the partic-
ular images. Image reduction and interpolation [5] is another application where
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 143–152, 2014.
c
⃝Springer International Publishing Switzerland 2014

144
P. Hurt´ık, I. Perﬁlieva, and P. Hod´akov´a
the direct F-transform can reduce (shrink) the original image and the inverse F-
transform can be used as an interpolation method. The F-transform of a higher
degree (F s-transform, s ≥1) [10] can approximate the original function even
better. Moreover, the F 1-transform can approximate the partial derivatives of
the original function and therefore, it can be used in edge detection to compute
the image gradient [9].
The task of the image registration is to match up two or more images. There
are several examples where the image registration is used - images taken by
diﬀerent sensors, in diﬀerent time, from diﬀerent positions, with diﬀerent size,
etc. One of the most natural applications is to match several images of land-
scape which are partially overlapped into one large image. There exists a lot of
methods how to register images [6], most of them consist of four basic steps: de-
tect important features in each image; match the features from all images; ﬁnd a
suitable mapping function which describes image shift, rotation, etc.; interpolate
images and fuse their overlaps.
This contribution, we demonstrate the use of the F-transform technique for
all those steps: the F 1-transform for the gradient detection and for the feature
points extraction; the F 0-transform for image similarity measures and for the
image fusion.
2
Fuzzy Transform
2.1
Generalized Fuzzy Partitions
A generalized fuzzy partition appeared in [10] in connection with the notion of
the higher-degree F-transform. Its even weaker version was implicitly introduced
in [3] for the purpose of meeting the requirements of image compression. We
summarize both these notions and propose the following deﬁnition.
Deﬁnition 1. Let [a, b] be an interval on the real line R, n > 2, and let x1, . . . , xn
be nodes such that a ≤x1 < . . . < xn ≤b. Let [a, b] be covered by the intervals
[xk −h′
k, xk + h′′
k] ⊆[a, b], k = 1, . . . , n, such that their left and right margins
h′
k, h′′
k ≥0 fulﬁll h′
k + h′′
k > 0.
We say that fuzzy sets A1, . . . , An : [a, b] →[0, 1] constitute a generalized fuzzy
partition of [a, b] (with nodes x1, . . . , xn and margins h′
k, h′′
k, k = 1, . . . , n), if for
every k = 1, . . . , n, the following three conditions are fulﬁlled:
1. (locality) — Ak(x) > 0 if x ∈(xk −h′
k, xk + h′′
k), and Ak(x) = 0 if x ∈
[a, b] \ (xk −h′
k, xk + h′′
k);
2. (continuity) — Ak is continuous on [xk −h′
k, xk + h′′
k];
3. (covering) — for x ∈[a, b], n
k=1 Ak(x) > 0.
4. (monotonicity) — Ak(x), for k = 2, . . . , n, strictly increases on [xk −h′
k, xk]
and Ak(x), for k = 1, . . . , n −1, strictly decreases on [xk, xk + h′′
k];
An (h, h′, h′′)-uniform generalized fuzzy partition of [a, b] is deﬁned for equidis-
tant nodes xk = a+h(k−1), k = 1, . . . , n, where h = (b−a)/(n−1); h′, h′′ > h/2
and two additional properties are satisﬁed:

Fuzzy Transform Theory in the View of Image Registration Application
145
4. Ak(x) = Ak−1(x −h) for all k = 2, . . . , n −1 and x ∈[xk, xk+1], and
Ak+1(x) = Ak(x −h) for all k = 2, . . . , n −1 and x ∈[xk, xk+1].
5. h′
1 = h′′
n = 0, h′′
1 = h′
2 = . . . = h′′
n−1 = h′
n = h′ and for all k = 2, . . . , n −1
and all x ∈[0, h′], Ak(xk −x) = Ak(xk + x).
An (h, h′)-uniform generalized fuzzy partition of [a, b] can also be deﬁned
using the generating function A0 : [−1, 1] →[0, 1], which is assumed to be even1,
continuous and positive everywhere except for on boundaries, where it vanishes.
Then, basic functions Ak of an (h, h′)-uniform generalized fuzzy partition are
shifted copies of A0 in the sense that
A1(x) =

A0
 x−x1
h′

,
x ∈[x1, x1 + h′],
0,
otherwise,
and for k = 2, . . . , n −1,
Ak(x) =

A0
 x−xk
h′

,
x ∈[xk −h′, xk + h′],
0,
otherwise.
,
(1)
An(x) =

A0
 x−xn
h′

,
x ∈[xn −h′, xn],
0,
otherwise,
2.2
F 0-transform
The direct and inverse F 0transform (originally just as F-transform) of a function
of two (and more) variables is a direct generalization of the case of one variable.
We introduce the discrete version only, because it is used in our applications
below. Let us refer to [2] for more details.
Suppose that the universe is a rectangle [a, b] × [c, d] ⊆R × R and that
x1 < . . . < xn are ﬁxed nodes of [a, b] and y1 < . . . < ym are ﬁxed nodes of
[c, d] such that x1 = a, xn = b, y1 = c, ym = d and n, m ≥2. Assume that
A1, . . . , An are basic functions that form a generalized fuzzy partition of [a, b]
and B1, . . . , Bm are basic functions that form a generalized fuzzy partition of
[c, d]. Then, the rectangle [a, b] × [c, d] is partitioned into fuzzy sets Ak × Bl
with the membership functions (Ak × Bl)(x, y) = Ak(x)Bl(y), k = 1, . . . , n,
l = 1, . . . , m.
In the discrete case, an original function f is assumed to be known only at
points (pi, qj) ∈[a, b] × [c, d], where i = 1, . . . , N and j = 1, . . . , M. In this case,
the (discrete) F 0-transform of f can be introduced in a manner analogous to
the case of a function of one variable.
Deﬁnition 2. Let a function f be given at points (pi, qj) ∈[a, b] × [c, d], for
which i = 1, . . . , N and j = 1, . . . , M, and A1, . . . , An and B1, . . . , Bm, where
n < N and m < M, be basic functions that form generalized fuzzy partitions
1 The function A0 : [−1, 1] →R is even if for all x ∈[0, 1], A0(−x) = A0(x).

146
P. Hurt´ık, I. Perﬁlieva, and P. Hod´akov´a
of [a, b] and [c, d] respectively. We say that the n × m-matrix of real numbers
F[f] = (Fkl)nm is the discrete F 0-transform of f with respect to A1, . . . , An and
B1, . . . , Bm if
Fkl =
M
j=1
N
i=1 f(pi, qj)Ak(pi)Bl(qj)
M
j=1
N
i=1 Ak(pi)Bl(qj)
(2)
holds for all k = 1, . . . , n, l = 1, . . . , m.
The inverse F 0-transform of a discrete function f of two variables is deﬁned
as follows.
Deﬁnition 3. Let A1, . . . , An and B1, . . . , Bm be basic functions that form gen-
eralized fuzzy partitions of [a, b] and [c, d], respectively. Let function f be de-
ﬁned on the set of points (pi, qj) ∈P × Q where P = {p1, . . . , pN} ⊆[a, b],
Q = {q1, . . . , qM} ⊆[c, d] and both sets P and Q are suﬃciently dense with
respect to corresponding partitions, i.e .∀k, l ∃i, j; Ak(pi)Bl(pj) > 0. Moreover,
let F[f] = (Fkl)nm be the discrete F 0-transform of f w.r.t. A1, . . . , An and
B1, . . . , Bm. Then, the function ˆf : P × Q →R represented by
ˆf(pi, qj) =
n
k=1
m
l=1 FklAk(pi)Bl(qj)
n
k=1
m
l=1 Ak(pi)Bl(qj)
(3)
is called the inverse F 0-transform of f.
2.3
F1-transform
We
can
generalize
the
F-transform
with
constant
components
to
the
F 1-transform with linear components. The latter are orthogonal projections of
an original function f onto a linear subspace of functions with the basis of poly-
nomials P 0
k = 1, P 1
k = (x −xk). We say that the n-tuple
F 1[f] = [F 1
1 , . . . , F 1
n]
(4)
is the F 1-transform of f w.r.t. A1, . . . , An where the k−th component F 1
k is
deﬁned by
F 1
k = ck,0P 0
k + ck,1P 1
k , k = 1, . . . , n.
(5)
For the h-uniform fuzzy partition and the triangular-shaped basic functions
we can compute the coeﬃcients ck,0, ck,1 for each k = 1, . . . , n as follows
ck,0 = 1
h
N

i=1
f(pi)Ak(pi),
(6)
ck,1 = 12
h3
N

i=1
f(pi)(pi −xk)Ak(pi).
(7)
It can be shown that the coeﬃcient ck,0 is equal to the F-transform component
Fk, k = 1, . . . , n. The next theorem shows the important property of the coef-
ﬁcient ck,1 which will be useful for the proposed edge detection technique. The
theorem is formulated for the continuous version of the F 1-transform.

Fuzzy Transform Theory in the View of Image Registration Application
147
Theorem 1. Let A1, . . . , An, be an h-uniform partition of [a, b], let functions
f and Ak, k = 1, . . . , n be four times continuously diﬀerentiable on [a, b], and
let F 1[f] = [F 1
1 , . . . , F 1
n]) be the F 1-transform of f with respect to A1, . . . , An,.
Then, for every k = 1, . . . , n, the following estimation holds true:
ck,1 = f ′(xk) + O(h).
(8)
We refer to [10] for a proof of Theorem 1 and for a detailed description of the
F 1-transform.
3
Image Registration
This section focuses on applications of the F-transforms theory into image reg-
istration. The developed method is divided into four steps: feature extraction;
feature matching; image mapping; image fusion.
3.1
Feature Extraction
Let us remark that there exist many algorithms for feature extraction, the most
used are FAST, ORB or SIFT [11]. In this contribution, we understand the
problem of feature extraction as a procedure that selects small corner areas
in the image. According to the accepted terminology, we call the latter point
features. Extracted point features in a reference and sensed images should be
detected on the similar places even if the sensed image is rotated, resized or has
diﬀerent intensity. We propose an original technique of point features detection
using the ﬁrst degree F-transform (F 1-transform) adopted from [9].
By Theorem 1, coeﬃcients ck,1 of the F 1-transform give us a vector whose
components approximate the ﬁrst derivative of the original function at certain
nodes. We use these coeﬃcients as components of the inverse F-transform and
we get the approximation of the ﬁrst derivative of the original image function in
each pixel.
Let triangular fuzzy sets A1, . . . , An establish a fuzzy partition of [1, N] and
triangular B1, . . . , Bm do the same for [1, M]. Let x1, . . . , xn ∈[1, N], hx =
xk+1 −xk, k = 1, . . . , n and y1, . . . , ym ∈[1, M], hy = yl+1 −yl, l = 1, . . . , m be
nodes on [1, N], [1, M] respectively. Then we can determine the approximation
of the ﬁrst derivative for each (pi, pj) ∈D in the horizontal direction
Gx(pi, pj) ≈
n

k=1
m

l=1
ck,1(yl)Ak(pi)Bl(pj)
(9)
and in the vertical direction
Gy(pi, pj) ≈
n

k=1
m

l=1
cl,1(xk)Ak(pi)Bl(pj)
(10)

148
P. Hurt´ık, I. Perﬁlieva, and P. Hod´akov´a
as the inverse F-transform of the image function u where the coeﬃcients ck,1(yl),
cl,1(xk), k = 1, . . . , n, l = 1, . . . , m are given by the F 1-transform
ck,1(yl) = 12
h3x
N

i=1
f(pi, yl)(pi −xk)Ak(pi),
(11)
cl,1(xk) = 12
h3y
M

j=1
f(xk, pj)(pj −yl)Bl(pj).
(12)
Then, the gradient magnitude G of an edge at point (pi, pj) is computed as
G(pi, pj) =

Gx(pi, pj)2 + Gy(pi, pj)2
(13)
and the gradient angle Θ is determined by
Θ(pi, pj) = arctan Gy(pi, pj)
Gx(pi, pj)
(14)
where for simplicity in according to [9] the gradient angle will be quantized by:
ΘQ : Θ →{0, 45, 90, 135}.
Deﬁnition 4. We say that a corner is a set of neighboring pixels (we call them
corner points) where at least three diﬀerent quantized angles show up. The center
of gravity of a corner is called a feature point.
Many corner points can be found in an image. It may happen that corner
points are close to each other, and in this case, we have to choose only one of
them. We modify computer graphic ﬂood ﬁll algorithm to detect clusters of close
corner points and then compute centers of gravity of each cluster. These centers
constitute the set of point features.
Image 1 simply demonstrate comparison of proposed algorithm with SIFT[11].
Two top images are original; two middle images were ﬁrstly vertically ﬂipped,
processed by both algorithms and then ﬂipped back for comparison. Two bottom
images were lighten and then processed by both algorithms. The result show
that both algorithms works correctly for this simplest image modiﬁcation. The
feature points detection should hold invariance for diﬃcult cases such as scale
transformation or rotation. The research of rotation and scale invariance of the
proposed algorithm deserve future work.
3.2
Feature Matching
In this step, a correspondence between the point features detected in the ref-
erence and sensed images is established. As a main technique (among various
similarity measures or spatial relationships) we propose to measure similarity by
a (inverse) distance between F 0-transform components of various levels.
In more details, the lowest (ﬁrst) level is comprised by the F 0-transform com-
ponents of image f and corresponds to the discretization given by the respective

Fuzzy Transform Theory in the View of Image Registration Application
149
Fig. 1. Left: inpainted circles by SIFT. Right: inpainted squares by the own algorithm.
fuzzy partition of the domain. This ﬁrst level F (1)[f] is given by the F 0-transform
of f so that
F (1)[f] = F[f] = (F11, ..., Fnm).
(15)
The vector of the F 0-transform components (F11, ..., Fnm) is a linear representa-
tion of a respective matrix of components. This ﬁrst level serves as a new image
for the F 0-transform components of the second level and so on. For a higher
level ξ we propose the following recursive formula:
F (ℓ)[f] = F[F (ℓ−1)] = (F (ℓ−1)
11
, ..., F (ℓ−1)
n(ℓ−1)m(ℓ−1)).
(16)
The top (last) level F (t)[f] consists of only one ﬁnal component F fin.
The F 0-transform based similarity S of two image functions f, g ∈I is pro-
posed to be as follows:
S(f, g) = 1 −|F fin −Gfin| ·
n
k=1
m
l=1 |F (1)
kl −G(1)
kl |
nm
(17)
where F fin, Gfin are the top F 0-transform components of f and g, and F (1)
kl , G(1)
kl ,
k = 1, . . . , n, l = 1, . . . , m are the ﬁrst level F 0-transform components of f and
g, respectively. The justiﬁcation that S is a similarity measure with respect to
the product t-norm was given in [4].

150
P. Hurt´ık, I. Perﬁlieva, and P. Hod´akov´a
We propose the following procedure in order to establishes a feature matching
between two point features:
– choose two point features P and Q from the reference and sensed images
respectively,
– create square areas SP and SQ of the same size around P and Q as center
points,
– compute similarity S(SP , SQ) according to (17),
– establish a feature matching between P and Q, if there is no point R in the
sensed image such that S(SP , SQ) < S(SP , SR).
Fig. 2. Green inpainted square illustrates an area around a point feature. Every couple
of image fragments illustrates detected matching between corresponding point features.
The Figure 2 demonstrates feature matching in diﬀerent images. Because of
images can be obtained in diﬀerent time, diﬀerent light conditions or some noise
can be in images, it is necessary to create similarity measure which will be robust
enough to these changes. The used F 0-transform based similarity approximate
image function with user deﬁned accuracy deﬁned by value of h parameter - see
approximation theorem 2 in [2]. This property allow to compare images even if
there are changes between them.
3.3
Image Mapping
The goal of the image mapping step is to ﬁnd a shift between two images in
x and y axis. The image mapping should determine a perspective distortion
and then transform images in such a way that they would ﬁt each other. For
simplicity of demonstration, we show on Fig. 3 the result of registration of seven
images where detected points features were shifted, but not interpolated. The
shift between every two images is computed as an average shift between all pairs
of matched points features. Figure 4 demonstrates result of existing software
called AutoStitch published in [12]. It is obvious that the proposed approach is
on the right way, but should be improved.

Fuzzy Transform Theory in the View of Image Registration Application
151
3.4
Image Fusion
Image fusion is the last step of our registration algorithm. It is applied to each
overlapping part of input images with the purpose to extract the best represen-
tative pixels. The detailed description of used image fusion that has been applied
in the proposed registration algorithm is in [8]. Figure 3 shows the ﬁnal result of
the proposed algorithm of image registration that uses seven input images. The
result image is not perfect, there are lot of artifacts inside. The reason why the
artifacts are there is because of mapping function without ﬂexible grid cannot
work with perspective distortion properly. Therefore some existing algorithm for
an landscape composition can achieve better result.
Fig. 3. Registered and fused image of seven images
Fig. 4. Same result obtained by AutoStitch [12]
4
Conclusion
In the paper we apply the techniques of the F0-transform and the F1-transform
to the problem of image registration. The technique of F0-transform is used in
computation of image similarity and that of the F1-transform is a part of the
gradient detection algorithm. The proposed theory is applied to image registra-
tion problem where F1-transform edge detection is a base of detecting important
areas (corners) in image. In order to ﬁnd a correspondence between corner areas
we used a newly proposed image similarity algorithm that helps in computation

152
P. Hurt´ık, I. Perﬁlieva, and P. Hod´akov´a
of shift vectors between images. Finally, overlapped part of images are inputs of
image fusion algorithm that is based on F0-transform. As a ﬁnal result panorama
image is created. Measuring of computation time show that the process of de-
tection of feature points and map them is lower than one second without any
perfect programming skill. The paper demonstrate how the unique technique of
F-transform can be successfully used in various algorithms that comprise the
multi-step problem of registration. The future work will be focused on the last
step: to ﬁnd image mapping function where the F-transform can be used (again)
for operation of an image interpolation. Without these missing part the result
is now worse than result obtained by existing approach called AutoStitch.
Acknowledgement.
This work was supported by the European Regional
Development
Fund
in
the
IT4Innovations
Centre
of
Excellence
project
(CZ.1.05/1.1.00/02.0070). This work was also supported by SGS14/PrF/2013
project and ”SGS/PrF/2014 – Vyzkum a aplikace technik soft-computingu ve
zpracovani obrazu” project.
References
1. Perﬁlieva, I.: Fuzzy transforms: Theory and applications. Fuzzy Sets and Sys-
tems 157, 993–1023 (2006)
2. Perﬁlieva, I., De Baets, B.: Fuzzy Transform of Monotonous Functions with Ap-
plications to Image Processing. Information Sciences 180, 3304–3315 (2010)
3. Hurtik, P., Perﬁlieva, I.: Image compression methodology based on fuzzy transform.
In: Herrero, ´A., Sn´aˇsel, V., Abraham, A., Zelinka, I., Baruque, B., Quinti´an, H.,
Calvo, J.L., Sedano, J., Corchado, E. (eds.) Int. Joint Conf. CISIS’12-ICEUTE’12-
SOCO’12. AISC, vol. 189, pp. 525–532. Springer, Heidelberg (2013)
4. Hurtik, P., Perﬁlieva, I.: Image compression methodology based on fuzzy transform
using block similarity. In: 8th Conference of the European Society for Fuzzy Logic
and Technology (EUSFLAT 2013). Atlantis Press (2013)
5. Hurtik, P., Perﬁlieva, I.: Image Reduction/Enlargement Methods Based on the
F-Transform, pp. 3–10. European Centre for Soft Computing, Asturias (2013)
6. Zitova, B., Flusser, J.: Image registration methods: a survey. Image and Vision
Computing 21(11), 977–1000 (2003)
7. Perﬁlieva, I., Daˇnkov´a, M.: Image fusion on the basis of fuzzy transforms. In: Proc.
8th Int. FLINS Conf., Madrid, pp. 471–476 (2008)
8. Vajgl, M., Perﬁlieva, I., Hod’kov, P.: Advanced F-Transform-Based Image Fusion.
Advances in Fuzzy Systems 2012 (2012)
9. Perﬁlieva, I., Hod´akov´a, P., Hurt´ık, P.: F1-transform edge detector inspired by
Canny’s algorithm. In: Greco, S., Bouchon-Meunier, B., Coletti, G., Fedrizzi, M.,
Matarazzo, B., Yager, R.R. (eds.) IPMU 2012, Part I. CCIS, vol. 297, pp. 230–239.
Springer, Heidelberg (2012)
10. Perﬁlieva, I., Daˇnkov´a, M., Bede, B.: Towards F-transform of a higher degree.
Fuzzy Sets and Systems 180, 3–19 (2011)
11. Lowe, D.G.: Distinctive Image Features from Scale-Invariant Keypoints. Interna-
tional Journal of Computer Vision 60 (2004)
12. Brown, M., Lowe, D.: Automatic Panoramic Image Stitching using Invariant Fea-
tures. International Journal of Computer Vision 74(1), 59–73 (2007)

Improved F-transform Based Image Fusion⋆
Marek Vajgl and Irina Perﬁlieva
Institute for Research and Applications of Fuzzy Modeling
Centre of Excellence IT4Innovations
University of Ostrava, Czech Republic
{marek.vajgl,irina.perfilieva}@osu.cz
Abstract. The article summarizes current approaches to image fusion
problem using fuzzy transformation (F-transform) with their weak points
and proposes improved version of the algorithm which suppress them.
The ﬁrst part of this contribution brings brief theoretical introduction
into problem domain. Next part analyses weak points of current imple-
mentations. Last part introduces improved algorithm and compares it
with the previous ones.
Keywords: Image processing, image fusion, F-transform.
1
Introduction
The contribution focus on the problem of image fusion, what is one of the im-
portant research areas in image processing ﬁeld. The main aim of image fusion is
the comparison and integration of the distorted scenes into result image, which
contains the best part extracted from the each input scene. The problem is how
to deﬁne, select and extract the ”best” pixel from all input scenes. The deﬁnition
of the ”best” pixel depends on the requested fusion result. In this contribution
we aim at the multi focus images, which diﬀer by focused area (e.g. see Fig.
1 (a),(b)). Moreover, if each pixel is ”best” in the some input image, we are
speaking about mosaic multi focus.
A measurement of the local focus is the main task, typically based on eval-
uation of the high frequencies in the image. The idea comes from the fact that
blurry parts (which are unfocused) suppress high frequencies, so focus measure
decreases with scene blur.
There already exist many approaches used to solve this problem. They diﬀers
by mathematical ﬁelds – statistical methods(e.g., usage of aggregation operators
([1]), estimation theory ([2]), fuzzy methods ([3], [4]), approaches based on the
optimization using genetic algorithms ([5]), etc.).
In this article approaches based on the fuzzy transform (called F-transform
for short) will be discussed. F-transform is integral transformation, for which
⋆This work was supported by the European Regional Development Fund in the
IT4Innovations Centre of Excellence project (CZ.1.05/1.1.00/02.0070). This work
was also supported by SGS14/PF/2013 project and ”SGS/PF/2014 – V´yzkum a
aplikace technik soft-computingu ve zpracov´an´ı obrazu” project.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 153–162, 2014.
c
⃝Springer International Publishing Switzerland 2014

154
M. Vajgl and I. Perﬁlieva
motivation came from fuzzy modeling ([6], [7]) and which was successfully ap-
plied in multiple areas of image processing, like image compression ([11]), edge
detection ([12]) or image reconstruction ([13]).
The contribution presents image fusion solutions using F-transform as the
main tool for image fusion([9], [10], [14]). After the experiments, further research
was done to obtain further improvements. Those improvements are presented in
this paper.
2
F-transform
As presented in the introduction, all methods explained in this contribution
refers to the F-transform technique, which will be explained only brieﬂy. For full
description see [6].
F-transform performs linear mapping from the set of ordinary continuous or
discrete functions over domain P into a set of discrete functions deﬁned on a
fuzzy partition of P using direct and inverse F-transform.
Let image function u is discrete function P →R of two variables, deﬁned over
the set of pixels P = {(i, j)|i = 1, ..., N; j = 1, ..., M}. Let fuzzy sets Ak × Bl,
k = 1, . . . , n, l = 1, . . . , m, (in the meaning deﬁned in [6]) establish a fuzzy
partition of [1, N] × [1, M]. The (direct) F-transform of u (with respect to the
chosen partition) is an image of the map F[u] : {A1, . . . , An}×{B1, . . . , Bm} →R
deﬁned by
F[u](Ak × Bl) =
N
i=1
M
j=1 u(i, j)Ak(i)Bl(j)
N
i=1
M
j=1 Ak(i)Bl(j)
,
(1)
where k = 1, . . . , n, l = 1, . . . , m. The value F[u](Ak × Bl) is called an F-
transform component of u and is denoted by F[u]kl. The components F[u]kl
can be arranged into the matrix Fnm[u].
The inverse F-transform of u is a function on P representing inversed im-
age unm obtained by the following inversion formula, where i = 1, . . . , N, j =
1, . . . , M:
unm(i, j) =
n

k=1
m

l=1
F[u]klAk(i)Bl(j).
(2)
It can be shown that the inverse F-transform unm approximates the original
function u on the domain P. The proof can be found in [6,7].
Processing of direct and inverse F-transform applied sequentially is called
ﬁlter F-transform. Values N, M are referred as number-of-components – noc,
minimal value is 2. Number of pixels covered by one basis function Ak or Bl is
called as points-per-base – ppb and minimal value is 3.
2.1
F-transform Application in Image Processing
The main approach used in almost all of the image processing application ar-
eas using F-transform is calculation of residuals, which can be obtained as a

Improved F-transform Based Image Fusion
155
subtraction between original input and input processed by direct and inverse
F-transform.
As the main property of the F-transform is removal of high frequencies from
the input image, the reconstructed image do not contain high frequency artifacts,
like noise, or sharp edges. The amount of removed frequencies depends on the
points–per–base/number-of-components values.
2.2
Image Fusion Using F-transform
The main aim of the image fusion is integration of multiple input images into
new one, where new image is in some way ”better” than original input ones.
In case of the multifocus area, ”better” image means the image containing the
most focused parts from the all input images.
Generally, for multifocus analysis, let u represents ideal image and c1, . . . , cK
are acquired (input) images. Then the relation between each ci and u can be
expressed by
ci(x, y) = di(u(x, y)) + ei(x, y), i = 1, . . . , K
where di is an unknown operator describing the image degradation, and ei is
some random noise. The aim of the fusion is to obtain fused image ˆu such that
it is closer to u (and therefore ”better”) than any of c1, . . . , cK.
The basic idea of usage of F-transform for image decomposition. We assume
that the image u is a discrete real function u = u(x, y) deﬁned on the N×M array
of pixels P = {(i, j) | i = 1, . . . , N, j = 1, . . . , M} so that u : P →R. Moreover,
let fuzzy sets Ak × Bl, k = 1, . . . , n, l = 1, . . . , m, where 2 ≤n ≤N, 2 ≤m ≤M
establish a fuzzy partition of [1, N] × [1, M].
We begin with the following representation of u on P:
u(x, y) = unm(x, y) + e(x, y),
(3)
e(x, y) = u(x, y) −unm(x, y),
(4)
where 0 < n ≤N, 0 < m ≤M, and unm is the inverse F-transform of u and
e is the respective ﬁrst diﬀerence. Value e represents residuals of the image u.
If we replace e in (3) by its inverse F-transform eNM with respect to the ﬁnest
partition of [1, N] × [1, M], the above representation can then be rewritten as
follows:
u(x, y) = unm(x, y) + eNM(x, y), ∀(x, y) ∈P.
(5)
We call (5) a one-level decomposition of u on P.
If function u is smooth, then the function eNM is small, and the one-level
decomposition (5) is suﬃcient for our fusion algorithm (see Section 3.2). How-
ever, for complex images there may be need to process next level decomposition
of ﬁrst diﬀerence e in (3) in following manner: we decompose e into its inverse
F-transform en′m′ (with respect to a ﬁner fuzzy partition of [1, N] × [1, M] with
n′ : n < n′ ≤N and m′ : m < m′ ≤M basic functions, respectively) and the
second diﬀerence e′. Thus, we obtain the second-level decomposition of u on P:

156
M. Vajgl and I. Perﬁlieva
u(x, y) = unm(x, y) + en′m′(x, y) + e′(x, y),
e′(x, y) = e(x, y) −en′m′(x, y).
In the same manner, we can obtain a higher-level decomposition of u on P
(see [14]).
In recursion, the whole iteration can be repeated to achieve such e decompo-
sition fulﬁlling algorithm requirements.
3
Previous Implementations
3.1
Original (CA) Algorithm
Complete algorithm (CA for short) was the ﬁrst approach researched in the im-
age fusion based on the F-transform. The algorithm processes iteratively the
input images where each iteration increases number-of-components value (and
takes into account higher frequencies) and processes residuals from the previ-
ous iteration until stopping condition (too high number-of-components value or
residuals are not signiﬁcant).
As CA algorithm was described precisely in [14] with its behavior, and is
replaced by ESA algorithm due to its memory and time consumption, it will not
be explained.
3.2
Simple (SA, ESA) Algorithm
The main idea of simple algorithm (SA) is to process input image via direct and
inverse F-transform to obtain residuals and then create ﬁnal image by taking
pixels from the image with the higher residual value (further referred as great-
est). This algorithm is based on the idea of the minimization time and memory
consumption.
Enhanced version (called ESA) of this algorithm (presented at [14]) did im-
prove behavior. The main improvement is that result pixel is calculated as
weighted sum of all input image pixel and weights represented by ﬁrst level
decomposition residuals (further referred as weighted) instead of picking one in-
put image (referred as greatest), what suppress disturbing eﬀect (compare Fig
2a vs. Fig. 2b).
The main advantage is lowered time and memory consumption. The main
disadvantage of this approach is requirement to set input variable points-per-
base by some expert, in wide range 3 – 50, where invalid selection has negative
eﬀect on the result. Another disadvantages are occurrences of artifacts as can be
seen in 2. Some of them may be avoided by correct ppb selection, but some will
remain.
3.3
Results Achieved by Presented Algorithms
Objective comparison of CA and ESA algorithm can be seen in Table 1 and
Table 2 (only best result combinations are presented).

Improved F-transform Based Image Fusion
157
(a)
Input
image,
back-
ground focus
(b)
Input
image,
fore-
ground focus
(c) Non-mosaic, ”greatest”
residuals
Fig. 1. ESA algorithm - fusion results, ppb=25
(a) ESA ”greatest” ppb=5 – low ppp
(b) ESA ”weighted” ppb=25 – ”lake” ar-
tifact
(c) ESA ”greatest” ppb=25 – ”lake” arti-
fact
(d) CA – ”ghost” artifact
Fig. 2. Fusion results – artifacts
From the subjective point of view (except enormously complex scenes) ESA
algorithm provides comparable results to CA algorithm in shorter time. However
invalid ppb selection will produces low quality results.
Moreover, both implementations still create artifacts called as a ”ghost/lakes”,
where blur edge is propagated from the blurred image into the ﬁnal result (see
Fig. 2).
4
Improved Implementation Based on ESA Algorithm -
IESA
The main aim of improved ESA algorithm is to produce better results than ESA
algorithm within acceptable time/memory consumption and to remove depen-
dency on expert decision about points-per-base.

158
M. Vajgl and I. Perﬁlieva
4.1
Ghost Issue Artifact Description
”Ghost” artifact occurs near the signiﬁcant edges in the result image. They are
caused by very blurry edges, which aﬀect the ”focus” detection in the image (see
Fig. 2), because:
– Exactly at the edge, the more ”focused” image has sharp intensity change
between both sides of the edge (e.g. between plane wing and sky). Less
focused image has only slight color change (due to blur over the edge) and
therefore more focused image seems to be more representative.
– Near the edge, the more ”focused” image may have no color change (e.g.
near the plane wing is only sky). However, less focused image contains ”fade”
between the edge colors (e.g. sky near plane contains fade between sky and
wing) and therefore the less focused image seems to be more representative.
As a result invalid image is preferred to obtain result. Therefore there is need
to suppress this behavior by spreading inﬂuence of the edge out.
4.2
Improved Fusion
Improved algorithm is built on the idea that edge is taken as signiﬁcant only if
there is no more signiﬁcant edge in the neighbor on diﬀerent source scene. There-
fore the ﬁrst step is to inﬂuence edge to its neighbor by blurring residuals using
F-transform. Second step is to fuse images together using one of the following
techniques:
– Greatest - the image with the highest diﬀerence at the processed pixel is the
winner and result image will be taken from the winning image only;
– Greatest soften - the image with the highest diﬀerence at the processed pixel
is the winner (see Fig. 3). However, winning indices (weights) are blurred
(using F-transform) to achieve slight fades between pixels from diﬀerent
images.
– Weighted - same as in ESA, but with threshold noise reduction.
– Square-weighted - same as previous, but with second power of the weights.
In implementation the ”greatest” and ”greatest-soften” approaches are calcu-
lated as weighted approaches with adequate weights set to zero.
4.3
Low Residual Values Reduction
Second part of the algorithm is the low residual values reduction - this occur when
residual values are to low, so there is no signiﬁcant edge. This is easily achieved
by setting threshold value for residuals - if residual value is lower than threshold,
it is set to zero. Otherwise there will be no change (see Fig. 3). Threshold value
will be referred as residual-threshold with typical value set to 0.9.

Improved F-transform Based Image Fusion
159
(a) ”Evolution” of weights during IESA
Fig. 3. Converting residuals to weights for ”greatest soften” option
4.4
Final Algorithm
Final algorithm combines updated SA algorithm together with new features
described above. It can be simply described as (numerical values are set according
to experimental results):
1. Calculate F-transform ﬁlter and residual for each input image (see section
2.2).
2. Blur residuals using F-transform ﬁlter (optimal ppb value to do this is 25).
3. Remove low residual values (typical threshold value should be 0.9).
4. For ”greatest”/”greatest-soften” approaches convert residuals (⟨0; 1⟩) into
weights ({0; 1}).
5. For ”greatest-soften” blur the weights using F-transform ﬁlter (that’s the
”soften”, for small ppb=5).
6. Calculate result image as combination of input images and weights.
(a) Non-mosaic, greatest
(b) Non-mosaic, greatest soften
(c) Non-mosaic, weighted
(d) Mosaic image
Fig. 4. IESA algorithm - cut-out fusion results, ppb=5, residual threshold=0.9

160
M. Vajgl and I. Perﬁlieva
Cut-out of fusion result examples are shown in Fig. 4 to demonstrate artifacts
elimination.
As shown in the following section, presented IESA algorithm achieves better
results than preceding CA and ESA algorithms with comparable time consump-
tion requirements.
IESA algorithm has two initial parameters - points-per-base and residual-
threshold. Experiments show that both parameters are in most cases independent
on input images and preferred values for ppb=6 and for residual-threshold=0.9.
5
Comparison to the Existing Algorithms
From the subjective point of view, results achieved by IESA algorithm are better
than results achieved by ESA algorithm and CA algorithm.
For objective evaluation, two characteristics describing image ”quality” are
used: mean square error – MSE and peak signal to noise ratio – PSNR.
For fused image uf and optimal (original) image u, MSE is deﬁned as:
MSEu,uf =
1
M × N

m∈M,n∈N
(u(m, n) −uf(m, n))2
(6)
The higher the MSE value is, the bigger is the diﬀerence between optimal image
u and the image created by fusion uf, so lower values are better. ”Error” of
MSE is evaluated against ”optimal” image for mosaic multifocus, or against the
best-available image created by expert in photo editor software for non-mosaic
images.
Peak signal-to-noise ratio is calculated from MSE using formula:
PSNRu,uf = 20 · log10
255
MSEu,uf
(7)
Higher value in PSNR means lower noise information, so higher values are better.
Results for the algorithms can be seen in Table 1. IESA algorithm gives ob-
jectively much better results than so far best results of ESA algorithm (CA
algorithm results were worse than SA (ppb=25) results and therefore were not
presented).
It is very important to notice that approaches based on the greatest-residual
(that are ”greatest” and ”greatest-soften”) give better results than approaches
based on weights. This supports idea that its better to pick pixel from speciﬁc
image than to calculate pixel as weight of input images.
Computation requirements can be described by measurable characteristics:
computation time and consumed memory, however they are very dependent on
hardware and software. Moreover number of direct or inverse F-transform evalu-
ations - NoFT characteristics is presented, describing number of direct of inverse
F-transforms evaluations. Results for the algorithms can be seen in table 2.
Computation time of IESA algorithm is higher than ESA algorithm due to
additional F-transform operations and more sophisticate residual processing.
However, for testing image (cca 11.2 MP) the time is still at level of seconds.

Improved F-transform Based Image Fusion
161
Table
1. Comparison by characteristics for selected image, two input images
4129 × 2726 pixels (for CA algorithm – PPB column deﬁnes how ppb value is increased
per iteration)
Algorithm PPB Other settings Non-mosaic image Mosaic image
MSE
PSNR
MSE PSNR
CA
2i
–
–
–
6.036 40.300
CA
i3
–
–
– 25.036 34.145
ESA
5
weighted
45.071
31.591 23.364 34.445
ESA
5
greatest
59.688
30.371 30.968 33.221
ESA
25
weighted
29.577
33.421
7.217 39.546
ESA
25
greatest
29.113
33.489
4.263 41.832
IESA
5
greatest
5.301
40.886
1.386 46.710
IESA
5
greatest-soften
4.507
41.591
0.921 48.485
IESA
5
weighted
5.243
40.934
2.504 44.143
IESA
5
squared-weights
9.408
38.395
1.600 46.089
Table 2. Comparison by resource consumption for selected image, two input images
4129 × 2726 pixels (for CA algorithm – PPB column deﬁnes how ppb value is increased
per iteration; for CA algorithm – images are half sized as full size images processing
did run out of memory)
Algorithm PPB Other settings NoFT Time (s) Memory (MB)
CA
2i
–
11
25.504
781
CA
i3
–
4
5.234
369
ESA
5
weighted
2 · n
3.202
193
ESA
5
greatest
2 · n
3.151
193
ESA
25
weighted
2 · n
2.873
128
ESA
25
greatest
2 · n
1.854
128
IESA
5
greatest
4 · n
3.939
286
IESA
5
greatest-soften
6 · n
4.447
214
IESA
5
weighted
4 · n
3.281
286
IESA
5
squared-weights 4 · n
3.099
279
6
Conclusion
In this paper we presented results about research of eﬀective fusion algorithms
based on previous research ([9] and [14]). Improved method presented in paper
brings better results within time and memory resources limits and suppression
of artifact occurring in result fused image; moreover dependency on input pa-
rameter is removed.
References
1. Blum, R.S.: Robust image fusion using a statistical signal processing approach.
Information Fusion 6(2), 119–128 (2005)

162
M. Vajgl and I. Perﬁlieva
2. Loza, A., Bull, D., Canagarajah, N., Achim, A.: Non-gaussian model-based fusion
of noisy images in the wavelet domain. Computer Vision and Image Understand-
ing 114(1), 54–65 (2010)
3. Singh, H., Raj, J., Kaur, G., Meitzler, T.: Image fusion using fuzzy logic and
applications. In: Proceedings of the 2004 IEEE International Conference on Fuzzy
Systems, vol. 1, pp. 337–340 (2004)
4. Ranjan, R., Singh, H., Meitzler, T., Gerhart, G.: Iterative image fusion technique
using fuzzy and neuro fuzzy logic and applications. In: Annual Meeting of the North
American Fuzzy Information Processing Society, NAFIPS 2005, pp. 706–710 (2005)
5. Mumtaz, A., Majid, A.: Genetic algorithms and its application to image fusion.
In: 4th International Conference on Emerging Technologies, ICET 2008, pp. 6–10
(2008)
6. Perﬁlieva, I.: Fuzzy transforms: Theory and applications. Fuzzy Sets and Sys-
tems 157, 993–1023 (2006)
7. Perﬁlieva, I.: Fuzzy transforms: A challenge to conventional transforms. In: Hawkes,
P.W. (ed.) Advances in Images and Electron Physics, vol. 147, pp. 137–196. Elsevier
Academic Press, San Diego (2007)
8. Perﬁlieva, I., Daˇnkov´a, M.: Image fusion on the basis of fuzzy transforms. In: Proc.
8th Int. FLINS Conf., Madrid, pp. 471–476 (2008)
9. Perﬁlieva, I., Daˇnkov´a, M., Hod´akov´a, P., Vajgl, M.: The Use of F-Transform for
Image Fusion Algorithms. In: Proc. Intern. Conf. of Soft Computing and Pattern
Recognition, SoCPaR 2010, pp. 472–477 (2010)
10. Hod´akov´a, P., Perﬁlieva, I., Daˇnkov´a, M., Vajgl, M.: F-transform based image
fusion. In: Ukimura, O. (ed.) Image Fusion, pp. 3–22. InTech (2011),
http://www.intechopen.com/articles/show/title/
f-transform-based-image-fusion
11. Perﬁlieva, I., Pavliska, V., Vajgl, M., De Baets, B.: Advanced image compression on
the basis of fuzzy transforms. In: Proc. Conf. IPMU 2008, Torremolinos, Malaga,
Spain, pp. 1167–1174 (2008)
12. Perﬁlieva, I., Hod´akov´a, P., Hurt´ık, P.: F 1-transform Edge Detector Inspired by
Canny’s Algorithm. In: Greco, S., Bouchon-Meunier, B., Coletti, G., Fedrizzi, M.,
Matarazzo, B., Yager, R.R. (eds.) IPMU 2012, Part I. CCIS, vol. 297, pp. 230–239.
Springer, Heidelberg (2012)
13. Perﬁljeva, I., Vlasanek, P., Wrublova, M.: Fuzzy transform for image reconstruc-
tion. In: Uncertainty Modeling in Knowledge Engineering and Decision Making,
pp. 615–620. World Scientiﬁc, Singapore (2012) ISBN 978-987-4417-73-0
14. Preﬁljeva, I., Vajgl, M.: Novel Image Fusion Based on F-transform. In: 2nd World
Conference on Soft Computing Proceedings, pp. 165–171. Letterpress Publishing
House (2012) ISBN 9789952452372

Visual Taxometric Approach to Image
Segmentation Using Fuzzy-Spatial Taxon Cut
Yields Contextually Relevant Regions
Lauren Barghout
Berkeley Institute for Soft Computing (BISC),
U.C. Berkeley, California, United States
lauren.barghout@gmail.com
http://www.laurenbarghout.org
Abstract. Images convey multiple meanings that depend on the context
in which the viewer perceptually organizes the scene. By assuming a stan-
dardized natural-scene-perception-taxonomy comprised of a hierarchy of
nested spatial-taxons [17] [6] [5], image segmentation is operationalized
into a series of two-class inferences. Each inference determines the opti-
mal spatial-taxon region, partitioning a scene into a foreground, subject
and salient objects and/or sub-objects. I demonstrate the results of a
fuzzy-logic-natural-vision-processing engine that implements this novel
approach. The engine uses fuzzy-logic inference to simulate low-level vi-
sual processes and a few rules of ﬁgure-ground perceptual organization.
Allowed spatial-taxons must conform to a set of ”meaningfulness” cues,
as speciﬁed by a generic scene-type. The engine was tested on 70 real
images composed of three ”generic scene-types”, each of which required
a diﬀerent combination of the perceptual organization rules built into
our model. Five human subjects rated image-segmentation quality on a
scale from 1 to 5 (5 being the best). The majority of generic-scene-type
image segmentations received a score of 4 or 5 (very good, perfect). ROC
plots show that this engine performs better than normalized-cut [9] on
generic-scene type images.
Keywords: visual taxometrics, natural vision processing, image seg-
mentation, spatial taxon cut, fuzzy ﬁlter, spatial taxons, scene archi-
tecture, scene perception, fuzzy perceptual inference, fuzzy logic, image
processing, graph partitioning.
1
Introduction
Segmenting images into meaningful regions is pre-requisite to solving most com-
puter vision interpretation problems. Yet region relevancy depends less on the
numeric information stored at each pixel, then on the computer vision task
and corresponding scene architecture required to perceptually organize the con-
stituent visual components necessary for the task. This presents a problem for
automated image segmentation, because it adds uncertainty to the process of
selecting which pixels to include or not include within a segment.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 163–173, 2014.
c
⃝Springer International Publishing Switzerland 2014

164
L. Barghout
An analogous problem exists for text document interpretation. Segmentation1
of the document into its relevant components, such as characters, words, sen-
tences or paragraphs, is pre-requisite to interpretation. However unlike images,
text-documents have a standardized architecture with components designated
by punctuation. Traditional punctuation and modern innovations such as hyper-
text2 mark-up language (html), minimize uncertainty in the process of selecting
which characters to include or not include within a segment.
Standardized architecture for written documents provide an example of a
complex system that has proven to be stable across history, culture and technical
innovation. As pointed out by Nobel Laureate Herbert Simon [11], ”hierarchy is
one of the central structural schemes that the architect of complexity uses”. He
further observes that ”hierarchic systems have some common properties that are
independent of their speciﬁc content” and he roughly deﬁnes a complex system
as a system in which ”the whole is more than the sum of the parts... in the
pragmatic sense that given the properties of the parts and the laws of their
interaction, it is not a trivial matter to infer the properties of the whole.”
Text-document architecture succeeds because its structure is independent of
content semantics. Letters, words, sentences and paragraphs follow the same
structure regardless of whether they belong to a document discussing fashion,
religion or nature.
The standardized natural-scene-perception-architecture described in this pa-
per mimics text-document architecture in several ways: it’s structured as a nested
taxonomy, scene segment structure is independent of scene content semantics,
standardized structure is used to minimize uncertainty as to which pixels belong
within a segment; and architecture enables interpretation by delivering visually
relevant components.
1.1
Visual Taxometrics and Spatial Taxons
Visual-taxometrics seeks to distinguish categorical visual percepts -such as ﬁg-
ure/ground perception, from continuous visual percepts - such as distance or size.
Spatial-taxons, categorical variables of ’whole things’ such as foreground, object
groups or objects (Barghout 2009), are ’building blocks’ of scenes. In essence
they serve as a proxy for the ﬁgural status of the region. When human subjects
are asked to mark the center of the subject of the image, they tend choose the
center of a spatial taxon with little variance and rarely choose locations deﬁned
solely by continuous visual percepts [17] [6]. Furthermore, evidence suggests that
the frequency at which people choose spatial-taxons at a particular abstraction
level, follow rank-frequency distribution similar to Zipf’s law – independent of
image content [6]. This is consistent with the law of least eﬀort found in other
cognitive systems and with Simon’s observations of complex systems.
1 Usually the literature uses the term ’parsing’ instead of ’segmentation’ to refer
to breaking language into constituent parts. I chose this phrase to illustrate the
information-normic (similarity) between image and language parsing.
2 My collaboration with Roger Gregory, who pioneered hypertext with its inventor
Ted Nelson, informed my understanding of this point.

Image Segmentation Using Fuzzy-Spatial Taxon Cut
165
Fig. 1.
(A) Natural-scene-perception-taxonomy comprised of a hierarchy of nested
spatial-taxons. By assuming the taxonomy prior to segmentation, segmentation be-
comes a series of two-class fuzzy inferences. The full scene, top row, is at the highest
level of abstraction. Each subsequent row is at lower level of abstraction within the
taxonomy. (B) An image of a butterﬂy on a daisy. (C) A 3-dimensional version of im-
age B where the third dimension (height) designates the abstraction level of the spatial
taxon as shown in C.
The spatial-taxon view of scene perception assumes that humans parse scenes
not between regions of similar features that vary continuously, but instead via
discrete spatial ’jumps’ biased toward taxometric scene conﬁgurations. Theo-
ries of visual attention make a similar distinction. The ”spotlight theory” [12]
assumes that attention regions vary continuously. Theories of ”object based”
attention assume that attended spatial regions vary in discrete location jumps
as it accommodates attended objects.
If humans are parsing scenes by inferring categories, then quantifying pixel-
region as to their aggregate ”trueness” relative to the category prototype is pre-
requisite to human inspired computerized image segmentation. Humans assign
meaning to visual percepts that they use to infer categories. Fuzzy-logic, which
provides tools for handling partial or relative truth of meaning [15], enables
inference based on visual percepts [4]. I’ve coined the phrase ”natural-vision-
processing” to refer to the parsing of images into psychological variables whose
relative truth (fuzzy membership) corresponds to human phenomenological in-
terpretation. Gestalt psychological variables such as similarity, good continua-
tion, symmetry and proximity as ﬁrst introduced by Wertheimer [13] provide
the basis for ﬁtting membership functions. A more detailed discussion on ﬁtting
Gestalt variables with fuzzy membership functions can be found in Barghout
(2003) [4]. This paper focuses on fuzzy methods for optimizing spatial-taxon
inference after a hypothetical set has been posited from Gestalt variables.

166
L. Barghout
Fig. 2.
Image segmentation algorithms, such as jseg and normalized-cut, produce
what I call ”jig-saw puzzle segments” of the original image (top left). In other words,
though they do a good job of delineating regions of similar percepts, they are not
meaningful to people. (A) Output of jseg algorithm [16]. From UCSB and downloaded
2010 ,version 6b. (B) Output of normalized-cut algorithm [18]. 2010 version. (C) The
natural image processing engine output for not-spatial taxon inference and (D) spatial
taxon inference.
1.2
Prior Work on Image segmentation
Most image segmentation algorithms stem from the school of thought that atten-
tion varies continuously over retinotopic location. Thus it makes sense to view
images as a graph, image segmentation as a graph partitioning problem and
precise high-dimension descriptive data at each graph node as pre-requisite to
solving computer vision problems. For these approaches the criterion for graph
partition is vital. They tend choose criterion of maximal contrast, where contrast
is deﬁned between summary statistics aggregated over candidate regions [9], [16].
Shi and Malik [9]) provide an excellent review of these methods. Though these
methods succeed in parsing dissimilar regions, the regions in and of themselves
are not meaningful. For example, ﬁgure 2 shows regions parsed to maximize
diﬀerences between regions. The segments look like jigsaw puzzle pieces. Each
jigsaw segment is not relevant to the visual understanding of the context and
content or scene organization.
Fuzzy logic, however, provides an alternative school of thought where it makes
sense to view images as spatially overlapping universe of discourses, image seg-
mentation as a fuzzy set classiﬁcation inference problem and the relative truth of
the meaning of underlying a segmentation query [14] as pre-requisite to solving
computer vision problems. In this way, image segmentation becomes a series of
fuzzy two-class inference problems.
2
Fuzzy Natural Vision Processing and Spatial-Taxon
Cut
By assuming the taxonomy prior to segmentation, parsing an image becomes
a series of two-class fuzzy inferences. In this section, I will describe a system

Image Segmentation Using Fuzzy-Spatial Taxon Cut
167
that implements image segmentation as a nested two-class fuzzy inference sys-
tem. Figure 3 provides an overview of the whole system. The sub-system (box
A), shown on the left is similar to other fuzzy systems. It contains a fuzzi-
ﬁcation phase where the crisp values contained in the original image are re-
parameterized into fuzzy cognitively relevant variables (CV). CVs are designed
to ﬁt human data or mimic human psychophysical and perceptual variables. A
discussion along with detailed examples of calculations of fuzzy CVs can be found
in Barghout (2003). Meaningfulness cues are composition styles with known CV
spatial-taxon conﬁgurations. Its inference system, uses CV premises and mean-
ingfulness rules to posit hypothetical spatial-taxons. Thus far, the fuzzy logic
system is pretty standard in its design.
The next process (box C on the right), decides on the hypothetical spatial-
taxon set and appropriate weighting. It is novel to this system. The system
iterates through various combinations of hypothetical spatial-taxons, to infer the
defuzzifed spatial-taxon that would result from each combination, and scoring
the output for each combination. This enables posits to ’abstain’3. The score is
a combination of spatial-taxon utility and the attentional resource requirement
of the hypothetical spatial-taxon combination. The optimal set is chosen such
that it maximizes utility and minimizes attentional resources.
Fig. 3. Fuzzy natural-scene-perception system
The utility function I use to score the posited spatial-taxon was inspired by a
seminal study of pictorial object naming [10] that found that objects were iden-
tiﬁed ﬁrst at an ”entry point” level of abstraction. Curious as to the whether
the scene-architecture had an ’entry level’ region, I undertook a multi-year study
3 The idea to allow psychological detectors to abstain from contributing information
to the system was suggested to me by LotﬁZadeh in 2006, personal communication.

168
L. Barghout
(2007-2011) surveying participants at the Burningman Arts Festival in NV, the
Macworld conference in CA and the department of motor vehicles in Raleigh,
N.C. The results suggest that images do indeed have an entry-level spatial taxon.
Furthermore the spatial-taxon rank-frequency distribution measured in these
studies suggest a law of least eﬀort similar to that found in other cognitive
processes [7]. Thus the utility function is inspired by the law of least eﬀort. I
deﬁne it operationally over an ordinal scale such that entry-level had the most
utility, super-ordinate the next highest utility and all sub-ordinate decrease util-
ity as a function of abstraction. This is a soft restriction, with granularity at
abstraction levels. Use of attentional resources was also deﬁned on an ordinal
scale with granularity at the number of hypothetical spatial-taxons possible in
the natural-vision processing engine. It’s constrained to be inversely related to
the number of signiﬁcant spatial-taxon combination sets above threshold, where
threshold was deﬁned in terms of sub-population variance verses variance of the
sub-population with the lowest within-group variance. This process is described
in ﬁgure 4. Figure 5 provides a pictorial illustration using the image marked as
”original” in ﬁgure 6.
Fig. 4. Process description of box C in Figure 3
Partitioning by spatial-taxon cut has two phases. In the ﬁrst phase we decide
on the optimal hypothetical spatial-taxon set & appropriate rule weighting.
For [k
2:Q] defuzziﬁcations calculate utility and attention-resources-requirement
where
Utility(Φ) =
 
Φ
hypothetical −spatial −taxon −utility(Φ)dΦ
(1)
Attenionalresources(Φ) =
 
Φ
Attentional −inference −load(Γ)dΦ
(2)
Let A be a fuzzy set deﬁned on a universe of Φ discrete meaningfulness cues
Φ = [Φ1, Φ2, ..., Φa] deﬁned on the universe of discourse of two discrete scene

Image Segmentation Using Fuzzy-Spatial Taxon Cut
169
architecture states S = [s1, s2] where s1 is a spatial taxon and s2 is the back-
ground4 Set Φ represents the hypothetical spatial-taxons organizing constraints.
In the second phase, we ”cut” the spatial taxon by defuzzifying the fuzzy
conclusion. The crisp conclusion is normalized between zero and one. Spatial-
taxon threshold is chosen according to use-case. In this system the threshold was
set to 0.5.
Fig. 5. Pictorial example of spatial-taxon inference as described in Figure 4. The mu
axis designates the fuzzy ﬁring power of each spatial taxon.
Figure 5, a woman wearing a hooded poncho in a ﬁeld of yellow ﬂowers, is used
as an example. A series of meaningfulness cues are used to posit hypothetical
spatial taxon. To make it easier to follow, I show cut-outs from the original image,
next to the meaningfulness cues. An original is shown if the meaningfulness
ﬁring power of that pixel exceeds threshold. Note that because the poncho is
orange, the intersection of yellow and red, it has membership in spatial-taxons
and complement. These conﬂicting cues abstain because including them in the
set drains attentional resources and provides little utility.
3
Performance Test Methods
70 real images composed of four ”generic scene types”, each of which required a
diﬀerent combination of the perceptual organization rules built into our model,
4 Though the human perceptual state of ”ground” extends beyond the subject and
thus has fuzzy borders, it’s digital image counterpart exists in a deﬁned pixel set
such that the ”ground” is the complement of the spatial taxon.

170
L. Barghout
Fig. 6. The Golden image was hand segmented by a human and is considered ”ground
truth”. The Crisp Output is the spatial taxon inferred by the system. The diﬀerence
between the Golden and system output is shown in Diﬀerence.
were collected. The natural-vision-processing system engine segmented them.
Golden segmentations (ground truths) were manually segmented for each image
using photoshop. A canny edge detector was used to produce the contours for
both ground truths and system outputs. Fuzzy correspondence was calculated
[3]. It was important to calculate fuzzy correspondence as opposed to crisp cor-
respondence so as to not create error artifacts from slight oﬀsets or registration
errors. Hit-rate, false-alarm, correct-rejection and misses were determined and
used to calculate ROC curves. The same procedure was used on a downloaded
version of Normalized Cut [18].
Five human subjects rated image-segmentation quality on a scale from 1 to
5 (5 being the best). D-prime (detectability) was determined from the hit-rates
and false alarm rates. Human subjects also rated the meaningfulness cues with
results shown in Table 1.
Table 1. shows the four K spatial-taxon sets. An ANOVA was used to extract the
relative proportion of meaningfulness cue for each corpus type - shown as linguistic
hedges - as scored by 5 human subjects.

Image Segmentation Using Fuzzy-Spatial Taxon Cut
171
Fig. 7. Example engine outputs organized by meaningfulness cue combination cluster.
In each example, the original image is on the left, the golden (hand segmented ground
truth) in the middle and spatial taxon segmentation on the right.

172
L. Barghout
4
Segmentation Results
ROC curves for all 70 images, ﬁgure 8a, show that the majority of images are
well segmented. This is conﬁrmed by humans scoring (5 subjects) that show that
the majority of generic-scene-type images segmented via spatial taxon method
received a score of 4 or 5 (very good, perfect). Figure 8b, ROC plots for 20
generic-scene-type images segmented using normalized cut and spatial taxon
cut.
Fig. 8. (Left) ROC plot for spatial-taxon cut (70 images). (Right) Comparison between
Spatial-taxon cut (circle) and normalized cut (square).
5
Conclusion
In conclusion, assuming taxonomy prior to segmentation enables quality parsing
contextually relevant regions. A novel methodology that ﬁnds the optimal set
and weight of premises performs well for optimizing spatial-taxon cut. Using
fuzzy inference provides signiﬁcant advantage for quantifying relative truth of a
category, enabling cognitively relevant image segmentation. Both human grading
and ROC plots show that this engine performs better than normalized-cut [9]
on generic-scene type images.
Acknowledgments.
Roger Gregory, Eyegorithm’s co-founder and I co-wrote
the code base on which the results were obtained. Special thanks to Dr. Christo-
pher Tyler, Dr. Steve Palmer, Dr. LotﬁZadeh and Dr. Lora Likova provide
valuable feedback, suggestions and advice. Other collaborators include Haley
Winter, Analucia DaSivla, Yurik Riegal, Colin Rhodes, Eric Rabinowitz, and
Shawn Silverman. BurningEyeDeas LLC, an organization that does research at
the Burningman art festival. Data posted at www.burningeyedeas.com.

Image Segmentation Using Fuzzy-Spatial Taxon Cut
173
References
1. Ruscio, J., Haslam, N., Ruscio, A.: Introduction To Taxometric Method. Lawrence
Eelbaum Associates (2006)
2. Barghout, L.: Linguistic Image Label Incorporating Decision Relevant Perceptual,
Semantic, and Relationships Data. USPTO. patent application 20080015843 (2007)
3. Barghout, L.: System and Method for Edge Detection in Image Processing and
Recognition. WIPO Patent Application. WO/2007/044828 (2006)
4. Barghout, L., Lee, L.: Perceptual information processing system. USPTO patent
application number: 20040059754 (2003)
5. Barghout, L., Sheynin, J.: Real-world scene perception and perceptual organiza-
tion: Lessons from Computer Vision. Journal of Vision 13(9) (July 24, 2013)
6. Barghout, Winter, Riegal: Empirical Data on the Conﬁgural Architecture of Hu-
man Scene Perception and Linguistic Labels using Natural Images and Ambiguous
ﬁgures. In: VSS 2011 (2011)
7. Cancho, Sole: Zipf’s law and random texts. Advances in Complex Systems 5(1),
1–6 (2002)
8. James, W.: Principles of psychology, p. 403. Holt, New York (1890)
9. Shi, J., Malik, J.: Normalized Cuts and Image Segmentation. IEEE TPAMI 22(8)
(2000)
10. Jolicoeur, Gluck, Kosslyn: Pictures and names: making the connection. Cognitive
Psychology 16, 243–275 (1984)
11. Simon, H.: The Architecture of Complexity. Proceedings of the American Philo-
sophical Society 106(6), 467–482 (1962)
12. Treisman, A.M.: Strategies and models of selective attention. Psychological Re-
view 76(3), 282–299 (1969)
13. Wertheimer, M.: Laws of Organization in Perceptual Forms (partial translation).
In: Ellis, W.B. (ed.) A Sourcebook of Gestalt Psychology, pp. 71–88. Harcourt
Brace (1938)
14. Zadeh, L.: Outline of a new approach to the analysis of complex systems and
decision processes. IEEE Trans. Syst. Man & Cybern. SMC-3 (1973)
15. Zadeh, L.: Toward a Restriction-centered Theory of Truth and Meaning (RCT).
Information Sciences 248 (2013)
16. Deng, Y., Manjunath, B., Shin, H.: Color image segmentation. In: IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, vol. 2 (1999)
17. Barghout, L.: Empirical Data on the Conﬁgural Architecture of Human Scene
Perception using Natural Image. J. Vis. 9(8), 964 (2009), doi:10.1167/9.8.964
18. Berkeley Segmentation Database,
http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds

A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 174–183, 2014. 
© Springer International Publishing Switzerland 2014  
Multi-valued Fuzzy Spaces for Color Representation 
Vasile Patrascu 
Tarom Information Technology, Bucharest, Romania 
patrascu.v@gmail.com 
Abstract. This paper proposes two complementary color systems: red-green-
blue-white-black and cyan-magenta-yellow-black-white. Both systems belong to 
the five-valued category and they represent some particular case of neutrosoph-
ic information representation. The proposed multi-valued fuzzy spaces are ob-
tained by constructing fuzzy partitions in the unit cube. In the structure of these 
five-valued representations, the negation, the union and the intersection opera-
tors were defined. Next, using the proposed multi-valued representation in the 
framework of fuzzy clustering algorithm, it results some color image clustering 
procedure. 
Keywords: fuzzy color space, five-valued representation, intuitionistic fuzzy 
sets, neutrosophic set. 
1 
Introduction 
A color image generally contains tens of thousands of colors. Therefore, most color 
image processing applications first need to apply a color reduction method before 
performing further sophisticated analysis operations such as segmentation. The use of 
color clustering algorithm could be a good alternative for color reduction method 
construction. In the framework of color clustering procedure, we are faced with two 
color comparison subject. We want to know how similar or how different two colors 
are. In order to do this comparison, we need to have a good coordinate system for 
color representation and also, we need to define an efficient color similarity measure 
in the considered system. The color space is a three-dimensional one and because of 
that for a unique description there are necessary only three parameters. Among of the 
most important color systems there are the following: RGB, HSV, HSI, HSL, Luv, Lab, 
I1I2I3.  This paper presents two systems for color representation called rgbwk re-
spectively cmykw and they belong to the multi-valued color representation [14]. The 
presented systems are obtained by constructing a five-valued fuzzy partition of the 
unit cube. The sum of the parameters r,g,b,w,k  and c,m,y,k,w verifies the condition 
of partition of unity and we can apply some similarities related to this property. Thus, 
one obtains new formulas for color similarity/dissimilarity. The paper has the follow-
ing structure: Section 2 presents the construction modality for obtaining of the five-
valued color representation rgbwk, the inverse transform from the rgbwk color system 
to RGB one, and the definition in the framework of the proposed color representation 
for the negation, the union and the intersection operators. Section 3 presents the using 

 
Multi-valued Fuzzy Spaces for Color Representation 
175 
of the proposed multi-valued representation in the framework of the k-means cluster-
ing algorithm. The presentation is accompanied with some experimental results.  
Finally, Section 4 outlines some conclusions. 
2 
The Construction of a Five-Valued Color Representation 
For representing colors, several color spaces can be defined. A color space is a defini-
tion of a coordinate system where each color is represented by a single vector. The 
most commonly used color space is RGB [6]. It is based on a Cartesian coordinate 
system, where each color consists of three components corresponding to the primary 
colors red, green and blue. Other color spaces are also used in the image processing 
area: linear combination of RGB (similar to I1I2I3 [9]), color spaces based on human 
color terms like hue, saturation and luminosity (similar to HIS [4], HSV [18], HSL 
[8]), or perceptually uniform color spaces (similar to Lab [5], Luv [3].  
2.1 
The Fuzzy Color Space rgbwk 
We will construct this new representation starting from the RGB (red, green, blue) 
color system. We will suppose that the three parameters take value in the interval 
[0,1]. We will define the maximum V, the minimum v, the hue H , the luminosity L  
[12] and the saturation  S [10]: 
)
,
,
max(
B
G
R
V =
,  
)
,
,
min(
B
G
R
v =
                    (1) 
 






+
−
−
=
2
,
2
3
)
(
G
B
R
B
G
atan2
H
 
(2) 
 
v
V
V
L
−
+
= 1
 
(3) 
 
|
5.0
|
|
5.0
|
1
)
(
2
−
+
−
+
−
=
V
v
v
V
S
 
(4) 
Firstly, we will define a fuzzy partition with two sets: the fuzzy set of chromatic col-
ors and the fuzzy set of achromatic colors. These two fuzzy sets will be defined by the 
following two membership functions: 
 
S
C =
ρ
 
      (5) 
 
   
S
A
−
=1
ρ
 
      (6) 
We obtained the first fuzzy partition for the color space: 
 
1
=
+
A
C
ρ
ρ
 
(7) 

176 
V. Patrascu 
The parameter 
C
ρ  is related to the color chromaticity while 
A
ρ  is related to the  
color achromaticity.   
Next in the framework of the chromatic colors, we will define the reddish, bluish    
and greenish color sets by the following formulae: 
 






+
−
=
2
)
,
min(
)
,
min(
B
R
G
R
R
S
r
γ
γ
γ
γ
γ
σ
 
(8) 
 
 






+
−
=
2
)
,
min(
)
,
min(
B
G
G
R
G
S
g
γ
γ
γ
γ
γ
σ
 
(9) 
 
 






+
−
=
2
)
,
min(
)
,
min(
B
G
B
R
B
S
b
γ
γ
γ
γ
γ
σ
 
(10) 
where  
)
cos(H
R =
γ
, 






−
=
3
2
cos
π
γ
H
G
, 






+
=
3
2
cos
π
γ
H
B
 and  
)
,
,
min(
)
,
,
max(
B
G
R
B
G
R
γ
γ
γ
γ
γ
γ
σ
−
=
 
There exists the following equality: 
 
C
b
g
r
ρ
=
+
+
 
(11) 
After that, in the framework of the achromatic colors, we define two subsets: one 
related to the white color and the other related to the black color: 
 
L
w
A ⋅
= ρ
 
(12) 
 
)
1(
L
k
A
−
⋅
= ρ
 
(13) 
There exists the following equality: 
A
k
w
ρ
=
+
                                  (14) 
From (7), (11) and (14) it results the subsequent formula: 
 
1
=
+
+
+
+
k
w
b
g
r
                               (15) 
We obtained a five-valued fuzzy partition of unity and in the same time we obtained a 
five-valued color representation having the following five components: r (red), g 
(green), b (blue), w (white) and k (black). We must observe that among the three 
chromatic components r, g, and b at least one of them is zero, explicitly 
0
)
,
,
min(
=
b
g
r
. 

 
Multi-valued Fuzzy Spaces for Color Representation 
177 
2.2 
The Inverse Transform from rgbwk to RGB 
In this section, we will present the computing formulas for the RGB components hav-
ing as primary information the rgbwk components. Firstly, we will compute the HSL 
components and then the RGB ones. Thus for the computing of luminosity L, we will 
use the achromatic components w,k. 
k
w
w
L
+
=
 
 
                         (16) 
For the computing of saturation  S  and hue H , we will use the chromatic compo-
nents r,g,b. 
 
b
g
r
S
+
+
=
 
(17) 






+
−
−
=
2
,
2
)
(
3
B
G
R
B
G
atan2
H
ω
ω
ω
ω
ω
 
 
    (18) 
where 
 
)
,
min(
g
b
r
r
R
+
+
=
ω
 
                  (19) 
 
)
,
min(
r
b
g
g
G
+
+
=
ω
 
                  (20) 
 
)
,
min(
g
r
b
b
B
+
+
=
ω
 
                  (21) 
For the RGB components, we have the following formulae: 
v
S
v
V
R
R +
−
=
ω
)
(
 
 
                     (22) 
v
S
v
V
G
G +
−
=
ω
)
(
 
                            (23) 
v
S
v
V
B
B +
−
=
ω
)
(
 
 
                   (24) 
The parameters V  and v can be determined solving the system  of equations  (3) 
and (4) and taking into account (16) and (17). 
2.3 
Negation, Union and Intersection for rgbwk Space 
In the following, we consider the negation of color 
)
,
,
(
B
G
R
Q =
, namely 
)
1,
1,
1(
)
,
,
(
B
G
R
Y
M
C
Q
−
−
−
=
=
 . Using (2), (8), (9), (10), (12) and (13) it results: 

178 
V. Patrascu 
 






−
+
=
=
R
B
R
G
R
S
c
r
γ
γ
γ
γ
γ
σ
2
)
,
max(
)
,
max(
 
(25) 
 






−
+
=
=
G
B
G
G
R
S
m
g
γ
γ
γ
γ
γ
σ
2
)
,
max(
)
,
max(
 
(26) 
 






−
+
=
=
B
B
R
G
B
S
y
b
γ
γ
γ
γ
γ
σ
2
)
,
max(
)
,
max(
 
    (27) 
 
)
1(
L
k
w
A
−
⋅
=
=
ρ
 
(28) 
 
L
w
k
A ⋅
=
=
ρ
 
(29) 
The five components defined by (25), (26), (27), (28) and (29) verify the condition of 
partition of unity, namely: 
1
=
+
+
+
+
w
k
y
m
c
 
 
              (30) 
and in addition: 
C
y
m
c
ρ
=
+
+
 
After the negation operation, we obtained a new five-valued fuzzy partition of unity 
and in the same time we obtained a five-valued color representation having the fol-
lowing components: c (cyan), m (magenta), y (yellow), k (black) and w (white). We 
must observe that among the three chromatic components c, m and y at least one of 
them is zero, explicitly, 
0
)
,
,
(
=
y
m
c
min
 
There exist the following equivalent relations between these two complementary sys-
tems: 
2
)
,
min(
)
,
min(
2
)
,
min(
y
c
m
c
y
m
m
y
r
+
−
+
+
=
 
 
    (31) 
2
)
,
min(
)
,
min(
2
)
,
min(
y
m
m
c
y
c
c
y
g
+
−
+
+
=
 
 
    (32) 
2
)
,
min(
)
,
min(
2
)
,
min(
y
m
y
c
c
m
m
c
b
+
−
+
+
=
 
 
    (33) 
2
)
,
min(
)
,
min(
2
)
,
min(
b
r
g
r
b
g
b
g
c
+
−
+
+
=
 
 
    (34) 

 
Multi-valued Fuzzy Spaces for Color Representation 
179 
2
)
,
min(
)
,
min(
2
)
,
min(
b
g
g
r
b
r
b
r
m
+
−
+
+
=
 
 
    (35) 
2
)
,
min(
)
,
min(
2
)
,
min(
g
b
r
b
g
r
g
r
y
+
−
+
+
=
 
           (36) 
 
We must highlight that the pair Red-Cyan 
)
,
(
C
R
 defines a fuzzy set [19] for the 
reddish color and it verifies the condition of fuzzy sets, namely 
1
=
+ C
R
. The pair 
red-cyan 
)
,
( c
r
 defines an Atanassov’s intuitionistic fuzzy set [1] for the reddish 
colors and it verifies the condition 
1
≤
+ c
r
. Similarly, the pair blue-yellow 
)
,
(
y
b
 
defines an Atanassov’s intuitionistic fuzzy set for bluish colors, the pair  green-
magenta
)
,
(
m
g
 defines an Atanassov’s intuitionistic fuzzy set for greenish colors 
while the pair  
)
,
(
k
w
 defines an Atanassov’s intuitionistic fuzzy set for the white 
color.  Thus for the color 
)
8.0,5.0,3.0
(
=
Q
, one obtains the fuzzy set  
3.0
=
R
 and 
7.0
=
C
, while for intuitionistic fuzzy description one obtains 
0
=
r
, 
53
.0
=
c
. The 
intuitionistic description is better than fuzzy description because the color Q is a 
bluish one and then reddish membership degree must be zero.  More than that for the 
white color 
)1,1,1(
=
Q
 one obtains for the fuzzy set description 
1
=
R
 and 
0
=
C
 
while for intuitionistic description one obtains 
0
=
r
, 
0
=
c
. Again, the intuitionistic 
description is better than the fuzzy one. Thus, the fuzzy description is identically with 
that of the red color while in the framework of intuitionist fuzzy description, the intui-
tionistic index is 1. This value is a correct value for an achromatic color like the white 
color.  
Taking into account the neutrosophic theory proposed by Smarandache [15], [16], 
[17] we can consider that the vector 
)
,
,
,
,
(
k
w
b
g
r
 provides a neutrosophic set for 
reddish colors. From this point of view, r  represents the membership function, 
g
b,
 
represent two non-membership functions while 
k
w,  represent two neutralities func-
tions. For this neutrosophic set, we define the union and intersection . 
 
The Union 
For any two colors 
)
,
,
,
,
(
p
p
p
p
p
k
w
b
g
r
p =
 and 
)
,
,
,
,
(
q
q
q
q
q
k
w
b
g
r
q =
 we define the 
union by formulae: 
)
,
max(
q
p
q
p
r
r
r
=

 
 
 
 
    (37) 
)
,
min(
q
p
q
p
g
g
g
=

 
 
 
 
    (38) 
)
,
min(
q
p
q
p
b
b
b
=

 
 
 
 
    (39) 
)
,
max(
)
,
max((
q
p
q
q
p
p
q
p
r
r
r
w
r
w
w
−
+
+
=

  
    (40) 
)
,
min(
)
,
min(
)
,
min(
q
p
q
p
q
q
q
p
p
p
q
p
b
b
g
g
b
g
k
b
g
k
k
−
−
+
+
+
+
=

 
    (41) 

180 
V. Patrascu 
The Intersection 
For any two colors 
)
,
,
,
,
(
p
p
p
p
p
k
w
b
g
r
p =
, 
)
,
,
,
,
(
q
q
q
q
q
k
w
b
g
r
q =
we compute the 
intersection using two steps. Firstly, we compute the intersection for the color  nega-
tions by formulae: 
)
,
max(
q
p
q
p
c
c
c
=

 
 
 
 
    (42) 
)
,
min(
q
p
q
p
m
m
m
=

 
 
 
 
    (43) 
)
,
min(
q
p
q
p
y
y
y
=

 
 
 
 
    (44) 
)
,
max(
)
,
max((
q
p
q
q
p
p
q
p
c
c
c
k
c
k
k
−
+
+
=

  
    (45) 
)
,
min(
)
,
min(
)
,
min(
q
p
q
p
q
q
q
p
p
p
q
p
y
y
m
m
y
m
w
y
m
w
w
−
−
+
+
+
+
=

    (46) 
 
Secondly, having 
q
p
c ,
q
p
m ,
q
p
y  and using  (31),(32) and (33) we compute 
q
pr ,
q
p
g  and 
q
p
b . 
The results of union and intersection verify the condition of partition of unity. 
Also, the union and intersection are associative, commutative and verify the De 
Morgan properties. Similarly, we can define these two operations for blue, green, 
yellow, magenta or cyan colors.  
More than that, we can construct five-valued neutrosophic set for any color hue but 
this construction will not be subject of this paper. 
3 
Color Clustering in the rgbwk Color Space 
For any two colors 
)
,
,
,
,
(
p
p
p
p
p
k
w
b
g
r
p =
, 
)
,
,
,
,
(
q
q
q
q
q
k
w
b
g
r
q =
 we compute the 
Bhattacharyya similarity [2]: 
q
p
q
p
q
p
q
p
q
p
k
k
w
w
b
b
g
g
r
r
q
p
F
+
+
+
+
=
)
,
(
 
       (47) 
and its dissimilarity: 
 
)
,
(
1
)
,
(
q
p
F
q
p
D
−
=
 
(48) 
Using the dissimilarity defined by (48) in the framework of k-means algorithm, one 
obtains a color clustering algorithm. The algorithm k-means [7], [11] is one of the 
simplest algorithms that solve the clustering problem. The procedure classifies a giv-
en data set through a certain number of clusters fixed a priori. The main idea is to 
define k  centroids, one for each cluster. The next step is to take each point belong-
ing to the data set and associate it to the nearest centroid. After that, the cluster cen-
troids are recalculated and k  new centroids are obtained. Then, a new binding has to 

 
Multi-valued Fuzzy Spaces for Color Representation 
181 
done between the same data set points and the nearest new centroid. A loop has been 
generated. This algorithm aims at minimizing an objective function, in this case a 
squared error function. The objective function is defined by:  
 


=
=
=
n
i
j
j
i
k
j
c
x
D
J
1
)
(
2
1
)
,
(
 
(49) 
where 
)
,
(
)
(
2
j
j
i
c
x
D
 is a chosen dissimilarity measure between a data point 
j
ix  and 
the cluster center 
jc . The function J  represents an indicator of the dissimilarity of 
the n data points from their respective cluster centers. Using in (49) the dissimilarity 
(48), we obtained the experimental results shown in figures 1 and 2.  
For the image “bird” shown in figure 1, only in the case (i) for the rgbwk system 
the orange color was separated. For the image “bird”, the uniform green background 
was well separated for the HIS, HSV, Lab, Luv and rgbwk color systems.  For the 
image “flower” shown in figure 2, the orange color was separated in the case (h) for 
the Lab system and in the case (i) for the rgbwk system. For the image “flower”, the 
uniform gray background was well separated using the Lab, Luv and rgbwk color 
systems. 
 
 
 
 
a) 
Original 
 
 
b) 
HSI 
 
 
c) 
HSL 
 
 
d) 
HSV 
 
 
e) 
I1I2I3 
 
 
f) 
Lab 
 
 
g) 
Luv 
 
 
h) 
RGB 
 
 
i) 
rgbwk 
Fig. 1. The image bird  
 

182 
V. Patrascu 
 
 
 
    
    
 
a) 
Original                 d) HSI                   g) HSL 
   
   
 
b) 
HSV                   e) I1I2I3                   h) Lab 
   
    
 
c) 
Luv                     f) RGB                    i) rgbwk   
Fig. 2. The image flower 
4 
Conclusions 
Two complementary fuzzy color spaces, rgbwk and cmykw which are useful in the 
color image analysis are introduced. The semantic of the five values defining a color 
in the rgbwk space is the amount of red, green, blue, white and black necessary to 
provide the color. The transformation from RGB to rgbwk or cmykw turns out to be 
very simple. 
The similarity/dissimilarity formula using the five parameters r,g,b,w,k is intro-
duced and also the negation, the union and intersection  operators  were defined. 
The hue and saturation can be retrieved from the chromatic components red, green 
and blue while the luminosity can be retrieved from the achromatic components white 
and black. Experimental results verify the efficiency of rgbwk fuzzy color space for 
color clustering. 
 
 

 
Multi-valued Fuzzy Spaces for Color Representation 
183 
References 
1. Atanassov, K.T.: Remark on a Property of the Intuitionistic Fuzzy Interpretation Triangle. 
Notes on Intuitionistic Fuzzy Sets 8, 34 (2002) 
2. Bhattacharyya, A.: On a measure of divergence between two statistical populations defined 
by their probability distributions. Bulletin of the Calcutta Mathematical Society 35, 99–
109 (1943) 
3. Fairchild, M.D.: Color Appearance Models. Addison-Wesley, Reading (1998) 
4. Gonzales, J.C., Woods, R.E.: Digital Image Processing, 1st edn. Addison-Wesley (1992) 
5. Hunter, R.S.: Accuracy, Precision, and Stability of New Photoelectric Color-Difference 
Meter. JOSA 38(12) (1948), Proceedings of the Thirty Third Annual Meeting of the Opti-
cal Society of America 
6. Jain, A.K.: Fundamentals of Digital Image Processing. Prentice Hall, New Jersey (1989) 
7. MacQueen, J.B.: Some Methods for classification and Analysis of Multivariate Observa-
tions. In: Proceedings of 5-th Berkeley Symposium on Mathematical Statistics and Proba-
bility, vol. 1, pp. 281–297. University of California Press, Berkeley (1967) 
8. Michener, J.C., van Dam, A.: A functional overview of the Core System with glossary. 
ACM Computing Surveys 10, 381–387 (1978) 
9. Ohta, Y., Kanade, T., Sakai, T.: Color information for region segmentation. Computer 
Graphics and Image Processing 13(3), 222–241 (1980) 
10. Patrascu, V.: New fuzzy color clustering algorithm based on hsl similarity. In: Proceedings 
of the Joint 2009 International Fuzzy Systems Association World Congress (IFSA 2009), 
Lisbon, Portugal, pp. 48–52 (2009) 
11. Patrascu, V.: Fuzzy Image Segmentation Based on Triangular Function and Its n-
dimensional Extension. In: Nachtegael, M., Van der Weken, D., Kerre, E.E., Philips, W. 
(eds.) Soft Computing in Image Processing. STUDFUZZ, vol. 210, pp. 187–207. Springer, 
Heidelberg (2007) 
12. Patrascu, V.: Fuzzy Membership Function Construction Based on Multi-Valued Evalua-
tion. In: Proceedings of the 10th International FLINS Conference. Uncertainty Modeling in 
Knowledge Engineering and Decision Making, pp. 756–761. World Scientific Press (2012) 
13. Pătraşcu, V.: Cardinality and Entropy for Bifuzzy Sets. In: Hüllermeier, E., Kruse, R., 
Hoffmann, F. (eds.) IPMU 2010. CCIS, vol. 80, pp. 656–665. Springer, Heidelberg (2010) 
14. Patrascu, V.: Multi-valued Color Representation Based on Frank t-norm Properties. In: 
Proceedings of the 12th Conference on Information Processing and Management of Uncer-
tainty in Knowledge-Based Systems (IPMU 2008), Malaga, Spain, pp. 1215–1222 (2008) 
15. Smarandache, F.: Neutrosophy. / Neutrosophic Probability, Set, and Logic. American Re-
search Press, Rehoboth (1998) 
16. Smarandache, F.: Definiton of neutrosophic logic - a generalization of the intuitionistic 
fuzzy logic. In: Proceedings of the Third Conference of the European Society for Fuzzy 
Logic and Technology, EUSFLAT 2003, Zittau, Germany, pp. 141–146 (2003) 
17. Smarandache, F.: Generalization of the Intuitionistic Fuzzy Logic to the Neutrosophic 
Fuzzy Set. International Journal of Pure and applied Mathematics 24(3), 287–297 (2005) 
18. Smith, A.R.: Color Gamut transform pairs. Computer Graphics SIGGRAPH 1978 Pro-
ceedings 12(3), 12–19 (1978) 
19. Zadeh, L.A.: Fuzy sets. Inf. Control 8, 338–353 (1965) 
 
 

A New Edge Detector Based on Uninorms
Manuel Gonz´alez-Hidalgo, Sebastia Massanet,
Arnau Mir, and Daniel Ruiz-Aguilera
Department of Mathematics and Computer Science
University of the Balearic Islands, E-07122, Palma, Spain
{manuel.gonzalez,s.massanet,arnau.mir,daniel.ruiz}@uib.es
Abstract. A new fuzzy edge detector based on uninorms is proposed
and deeply studied. The behaviour of diﬀerent classes of uninorms is dis-
cussed. The obtained results suggest that the best uninorm in order to
improve the edge detection process is the uninorm Umin, with underly-
ing Lukasiewicz operators. This algorithm gets statistically substantial
better results than the others obtained by well known edge detectors, as
Sobel, Roberts and Prewitt approaches and comparable to the results
obtained by Canny.
Keywords: edge detection, uninorm, Canny, Sobel, Roberts, Prewitt.
1
Introduction
One of the main operations in image processing is edge detection. Determining
the borders of an image allows to use more complex analysis such as segmenta-
tion and it can be used in computer vision and recognition. Its performance is
crucial for the ﬁnal results of the image processing technique. In recent decades,
a large number of edge detection algorithms have been developed. These diﬀer-
ent approaches vary from classical algorithms [18] based on the use of a set of
convolution masks, to new techniques based on fuzzy sets and their extensions
[3]. From all classical algorithms, the Canny edge detector [4] is one of the most
used due to its performance.
A diﬀerent approach to edge detection is based in mathematical morphology
[20]. The classical setting has been generalized using concepts and techniques of
fuzzy sets and their extensions [5,6,8,9]. In particular, uninorms as a particular
case of conjunction and aggregation operator have been used in image processing
and analysis [1,6,8,10,16]. All these applications of uninorms are due to their
structure: like a t-norm in the region [0, e]2, like a t-conorm in the region [e, 1]2,
and like a compensatory operator on the remaining regions. This structure may
be useful in order to magnify the output value whenever all the input values are
greater than the neutral element, to reduce the output value whenever all the
input values are smaller than the neutral element, and ﬁnally to compensate the
output value when the input values are some of them greater than the neutral
element and some of them are smaller than this element.
Having this in mind, and taking into account the Canny edge detector, we
propose a new algorithm based on uninorms. So, after a brief revision about
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 184–193, 2014.
c
⃝Springer International Publishing Switzerland 2014

A New Edge Detector Based on Uninorms
185
uninorms (Section 2) we describe our edge detection algorithm (Section 3). In
Section 4 the experimental environment is speciﬁed, followed by the obtained
results. We ﬁnish the communication with some conclusions and future work to
expand our study.
2
Preliminaries
In these preliminaries we introduce the basic deﬁnitions and notations of the
operators that will be used in the paper. More details can be found in [12].
Deﬁnition 1. A t-norm (t-conorm) is a two-place function T : [0, 1]2 →[0, 1]
(S : [0, 1]2 →[0, 1]) which is associative, commutative, non-decreasing in each
place and such that that T (1, x) = x (S(0, x) = x) for all x ∈[0, 1].
Well known t-norms are the product t-norm TP(x, y) = x · y, the ΣLukasiewicz
t-norm TL(x, y) = max(x + y −1, 0), and the nilpotent minimum t-norm
TnM(x, y) =

0
if x + y ≤1,
min(x, y)
otherwise.
A way to construct t-conorms from t-norms is the duality, by using the formula
S(x, y) = 1−T (1−x, 1−y), for all x, y ∈[0, 1], where T is any t-norm and S, its
dual t-conorm. Therefore, the dual t-conorms of the t-norms presented before
are the probabilistic sum SP(x, y) = x + y −x · y, the ΣLukasiewicz t-conorm
SL(x, y) = min(x + y, 1), and the nilpotent maximum t-conorm
SnM(x, y) =

1
if x + y ≥1,
max(x, y)
otherwise.
Deﬁnition 2 ([21]). A uninorm is a two-place function U : [0, 1] × [0, 1] →[0, 1]
which is associative, commutative, non-decreasing in each place and such that
there exists some element e ∈[0, 1], called neutral element, such that U(e, x) = x
for all x ∈[0, 1].
It is clear that U becomes a t-norm when e = 1 and a t-conorm when e = 0.
Any uninorm satisﬁes U(0, 1) ∈{0, 1} and so U is called conjunctive when
U(1, 0) = 0 and disjunctive when U(1, 0) = 1.
Given any uninorm U, a t-norm T and a t-conorm S can be deﬁned from
its values in [0, e]2 and [e, 1]2, respectively. The values of U in [0, 1]2 \ ([0, e]2 ∪
[e, 1]2) are between minimum and maximum. The general structure of a uninorm
is depicted in Figure 1. In general, a uninorm U with neutral element e and
underlying t-norm T and t-conorm S will be denoted by U ≡⟨T, e, S⟩.
In the last years some classes of uninorms have been characterized (idempo-
tent, representable, continuous in ]0, 1[2) but here we will only focus on uninorms
in Umin and Umax, the classes that will be used in the experimental results.

186
M. Gonz´alez-Hidalgo et al.
t-norm T
min≤U ≤max
min≤U ≤max
t-conorm S
0
e
1
e
1
Fig. 1. General structure of a uninorm U with neutral element e
Theorem 1 ([7]). Let U be a uninorm with neutral element e ∈]0, 1[ having
functions x →U(x, 1) and x →U(x, 0) (x ∈[0, 1]) continuous except (perhaps)
at the point x = e. Then U is given by one of the following forms
(a) If U is conjunctive (U(0, 1) = 0), then U is given by
U(x, y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
eT
 x
e , y
e

if (x, y) ∈[0, e]2
e + (1 −e)S
#
x−e
1−e , y−e
1−e
$
if (x, y) ∈[e, 1]2
min(x, y)
otherwise.
(1)
(b) If U is disjunctive (U(0, 1) = 1), then U is given by
U(x, y) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
eT
 x
e , y
e

if (x, y) ∈[0, e]2
e + (1 −e)S
#
x−e
1−e , y−e
1−e
$
if (x, y) ∈[e, 1]2
max(x, y)
otherwise.
(2)
In both formulas T is a t-norm and S is a t-conorm.
The class of all uninorms with expression (1) will be denoted by Umin, and a
uninorm in Umin with associated t-norm T and t-conorm S will be denoted by
U ≡⟨T, e, S⟩min. Similarly, Umax will denote all uninorms with expression (2),
and a uninorm in this class will be referred as U ≡⟨T, e, S⟩max.
3
The Edge Detector Based on Uninorms: Algorithm
Before introducing the main algorithm, we need to explain what we understand
by a directional gradient based on uninorms. So, for each pixel location (i, j)
in the set of coordinates Ω of a gray level original image X, we set the eight
neighbours in a 3×3 windows of the pixel xi,j, see Fig. 2.a). Let us now consider
the two convolution masks shown in Fig. 2.b) then, the basic directional gra-
dients are deﬁned as the absolute value of the diﬀerence of the intensity values

A New Edge Detector Based on Uninorms
187
xi−1,j−1
xi,j−1
xi+1,j−1
xi−1,j
xi,j
xi+1,j
xi−1,j+1
xi,j+1
xi+1,j+1
1
1
1
−1 −1 −1
1
−1
1
−1
1
−1
(a)
(b)
Fig. 2. (a) Neighbourhood of a current pixel xi,j. (b) Convolution masks in order to
compute the basic directional gradients and the directional uninorm based gradients,
at left for y direction, at right for x direction.
between its horizontal and vertical neighbours respectively. The basic directional
gradients along x-direction are deﬁned as
yl
i−1,i+1 = |xi−1,l −xi+1,l|, l ∈{j −1, j, j + 1}
and, the basic directional gradients along y-direction are deﬁned as
xk
j−1,j+1 = |xk,j−1 −xk,j+1|, k ∈{i −1, i, i + 1}.
Let U ≡⟨T, e, S⟩be a uninorm, we deﬁne the y directional U based gradient
for the pixel xi,j, as the aggregation of the basic directional gradients along
y-direction using the uninorm U. This can be represented by
Uy(xi,j) = U(yj−1
i−1,i+1, yj
i−1,i+1, yj+1
i−1,i+1).
In a same way, we deﬁne the x directional U based gradient for the pixel xi,j,
as the aggregation of the basic directional gradients along x-direction using the
uninorm U. This can be represented by
Ux(xi,j) = U(xi−1
j−1,j+1, xi
j−1,j+1, xi+1
j−1,j+1).
Observe that if the values of the basic directional gradients are high (belong-
ing to the interval [e, 1]), the respective directional uninorm based gradient is
computed using the underlying t-conorm S, as an indicator of a presence of a re-
markable edge point. If these values are low (belonging to the interval [0, e]) the
respective directional uninorm based gradient is computed using the underlying
t-norm T , and we are in the presence of a point which can not be an edge point. In
other cases we compute the directional uninorm based gradient as a kind of aver-
age. Recall that for all (x, y) ∈[0, 1]2, S(x, y) ≥max(x, y) ≥min(x, y) ≥T (x, y)
and if (x, y) ∈[0, e] × [e, 1] ∪[e, 1] × [0, e], min(x, y) ≤U(x, y) ≤max(x, y).
Once we have deﬁned the bidirectional uninorm based gradients Ux(xi,j) and
Uy(xi,j), if we want to view the result at this point the two components must be
combined. The magnitude of the result is computed at each pixel location (i, j)
as
MU(i, j) =

Ux(xi,j)2 + Uy(xi,j)2.
(3)
As we can observe the magnitude is computed in the same manner as it was for
the gradient, which is in fact what is being computed. Note that these operators

188
M. Gonz´alez-Hidalgo et al.
have the desirable property of yielding zeros for uniform regions. Other ways to
calculate the magnitude and combine the bidirectional uninorm based gradients
are possible, for instance, taking the maximum norm or the 1-norm, or even
using another appropriate aggregation function.
After the magnitude given by Eq. 3 has been calculated, it should be nor-
malized in order to obtain the fuzzy edge map. So, our fuzzy edge image is the
normalized image of MU that, without confusion, we denote in the same way.
However, the fuzzy edge map generated by the magnitude is an image where
the value of a pixel represents its membership degree to the set of edges. This
idea contradicts the restrictions of Canny [4], forcing a representation of the edge
images as binary images with edges of one pixel width. Therefore the fuzzy edge
image must be thinned and binarized. The fuzzy edge map will contain large
values where there is a strong image gradient, but to identify edges the broad
regions present in areas where the slope is large must be thinned so that only
the magnitudes at those points which are local maxima remain. Non Maxima
Suppression (NMS) performs this by suppressing all values along the line of the
gradient that are not peak values [4]. NMS has been performed using P. Kovesis’
implementation in Matlab [13].




Original image
?
fuzzy gradient
Fuzzy edge image
?
NMS
Fuzzy thin edge image
?
Hysteresis




Binary thin edge image
Fig. 3. Block diagram of the proposed edge detector
Finally, to binarize the image, we have implemented an automatic non-
supervised hysteresis based on the determination of the instability zone of the
histogram to ﬁnd the threshold values [15]. Hysteresis allows to choose which
pixels are relevant in order to be selected as edges, using their membership val-
ues. Two threshold values T1, T2 with T1 ≤T2 are used. All the pixels with a
membership value greater than T2 are considered as edges, while those which
are lower to T1 are discarded. Those pixels whose membership value is between
the two values are selected if, and only if, they are connected with other pixels
above T2. The method needs some initial set of candidates for the threshold
values. In this case, {0.01, . . ., 0.25} has been introduced, the same set used in
[15]. In Figure 3, we display the block diagram of the algorithm describing the
edge detector proposed in this section.

A New Edge Detector Based on Uninorms
189
4
Experimental Environment
In this section, the diﬀerent conﬁgurations of the edge detector based on uni-
norms which are going to be analysed in Section 5 are introduced. Furthermore,
we settle the experimental environment based on an objective performance com-
parison of several edge detectors in order to determine which one of those con-
sidered in the analysis obtains the best results.
4.1
Conﬁgurations of the Edge Detector Based on Uninorms
In the previous section, the edge detector based on uninorms has been presented.
This edge detector depends on the uninorm chosen to aggregate the six basic
directional gradients obtained using the two masks, the vertical one and the
horizontal one. Therefore, an unavoidable step consists on determining which
uninorm has to be used to obtain the best results. With this aim in mind, we
will consider 24 diﬀerent uninorms of the classes of Umin and Umax with diﬀerent
underlying t-norms and t-conorms and with several neutral elements. Namely,
the uninorms U ≡⟨T, e, S⟩min and U ≡⟨T, e, S⟩max with the underlying t-
norms TnM, TP, TL, introduced in Section 2, and as underlying t-conorms their
dual operations, SnM, SP and SL, respectively, have been considered. For each of
the previous uninorms, four neutral elements e have been used: 0.02, 0.04, 0.06
and 0.08. Note that these values correspond to the values 5, 10, 15 and 20 if
we consider the usual chain {0, . . . , 255} with the possible gray levels of a pixel.
These values have been considered since if we consider a higher value than 20
there could be some basic gradients higher than 20 that were aggregated using
the underlying t-norm of the uninorm and thus, this potential edge would not
be stand out. Similarly, if we consider a lower value than 5, there could be some
basic gradients lower than 5 that were aggregated using the underlying t-conorm
of the uninorm and thus, this potential noise would not be weakened.
Remark 1. From their expressions, it is evident that the fuzzy edge image ob-
tained using the uninorm U ≡⟨T, e, S⟩max is going to contain more edges than
the obtained using the uninorm U ≡⟨T, e, S⟩min for some ﬁxed t-norm T , t-
conorm S and neutral element e ∈(0, 1). This does not imply that the con-
ﬁguration using the uninorm of the class of Umax obtains always better results
than the conﬁguration of the corresponding uninorm of Umin since, in addition
to the non-supervised hysteresis which does not satisfy always the monotonicity
because the thresholds can diﬀer in both cases, some pixels which are not edges
as texture or noise could be detected by the ﬁrst conﬁguration.
4.2
Objective Performance Comparison Method
Nowadays, it is well-established in the literature that the visual inspection of the
edge images obtained by several edge detectors can not be the unique criterion
with the aim of proving the superiority of one edge detector with respect to
the others. This is because each expert has diﬀerent criteria and preferences

190
M. Gonz´alez-Hidalgo et al.
and consequently, the reviews given by two experts can diﬀer substantially. For
this reason, the use of objective performance measures on edge detection is
growing in popularity to compare the results obtained by diﬀerent edge detection
algorithms. There are several measures of performance for edge detection in the
literature, see [14] and [17]. These measures require, in addition to the binary
edge image with edges of one pixel width (DE) obtained by the edge detector
we want to evaluate, a reference edge image or ground truth edge image (GT)
which is a binary edge image with edges of one pixel width containing the real
edges of the original image. In this work, we will use the following quantitative
performance measures
– The ρ-coeﬃcient ([11]), deﬁned as
ρ =
card(ET P )
card(E) + card(EF N) + card(EF P ),
where ET P is the set of well-detected edge pixels, EF N is the set of ground
truth edges missed by the edge detector and EF P is the set of edge pixels
detected but with no counterpart on the ground truth image.
– The F-measure ([19]), deﬁned as the harmonic mean between precision (PR)
and recall (RE) given by
PR =
card(ET P )
card(ET P ) + card(EF P ),
RE =
card(ET P )
card(ET P ) + card(EF N ).
Larger values of ρ and F (0 ≤ρ, F ≤1) are indicators of better capabilities for
edge detection.
Consequently, we need a dataset of images with their ground truth edge images
in order to compare the outputs obtained by the diﬀerent algorithms. So, the ﬁrst
15 images and their edge speciﬁcations from the public dataset of the University
of South Florida1 ([2]) have been used. In [2], the details about the ground truth
edge images and their use for the comparison of edge detectors are included.
5
Obtained Results
In Table 1, the mean and the standard deviation of the 15 values of the
ρ-coeﬃcient obtained by each conﬁguration, the number of best and worst im-
ages for each edge detector and the mean of the rankings of each conﬁgura-
tion in every image are collected. The best three uninorms are the following:
⟨TP, 0.04, SP⟩min, ⟨TP, 0.02, SP⟩min and ⟨TL, 0.02, SL⟩min. This last conﬁguration
obtains the higher mean value according to the ρ-coeﬃcient. We have performed
a Wilcoxon and a t-test to see if there are signiﬁcant diﬀerences between the
1 This image dataset can be downloaded from
ftp://figment.csee.usf.edu/pub/ROC/edge_comparison_dataset.tar.gz

A New Edge Detector Based on Uninorms
191
(a) Original
(b) GT
(c) U1
(d) Canny σ = 1.5
(e) Sobel
Fig. 4. Some original images, ground truth edge images and the results obtained by
some of the considered edge detectors, where U1 = ⟨TL, 0.02, SL⟩min
Table 1. Mean, standard deviation, number of best and worst images and mean of
rankings of the diﬀerent uninorm-based conﬁgurations according to the ρ-coeﬃcient
values
Method
x
σ
✓× xp
⟨TnM, 0.02, SnM⟩min 0.671 0.124 0 0 16.0
⟨TnM, 0.02, SnM⟩max 0.647 0.124 0 2 20.5
⟨TnM, 0.04, SnM⟩min 0.658 0.135 0 0 17.7
⟨TnM, 0.04, SnM⟩max 0.652 0.118 0 1 19.5
⟨TnM, 0.06, SnM⟩min 0.633 0.143 0 1 20.5
⟨TnM, 0.06, SnM⟩max 0.652 0.119 0 0 19.5
⟨TnM, 0.08, SnM⟩min 0.615 0.145 0 8 22.5
⟨TnM, 0.08, SnM⟩max 0.650 0.124 0 1 19.6
⟨TP, 0.02, SP⟩min
0.762 0.096 3 0 5.8
⟨TP, 0.02, SP⟩max
0.751 0.095 1 1 8.5
⟨TP, 0.04, SP⟩min
0.756 0.107 1 0 5.7
⟨TP, 0.04, SP⟩max
0.748 0.094 0 0 8.9
Method
x
σ
✓× xp
⟨TP, 0.06, SP⟩min 0.736 0.126 1 0 9.1
⟨TP, 0.06, SP⟩max 0.743 0.103 0 0 9.5
⟨TP, 0.08, SP⟩min 0.715 0.134 0 0 12.3
⟨TP, 0.08, SP⟩max 0.726 0.112 0 0 11.5
⟨TL, 0.02, SL⟩min 0.765 0.092 3 0 5.4
⟨TL, 0.02, SL⟩max 0.757 0.087 3 1 7.6
⟨TL, 0.04, SL⟩min 0.757 0.105 2 0 6.1
⟨TL, 0.04, SL⟩max 0.751 0.089 0 0 8.5
⟨TL, 0.06, SL⟩min 0.732 0.120 0 0 10.3
⟨TL, 0.06, SL⟩max 0.745 0.104 1 0 9.3
⟨TL, 0.08, SL⟩min 0.705 0.131 0 0 13.7
⟨TL, 0.08, SL⟩max 0.728 0.113 0 0 11.9
three previous uninorms and the uninorms ⟨TnM, e, SnM⟩min and their counter-
parts in Umax. The results show that the three best uninorms are statistically
better than these other ones.
Next, a comparison of this group of three uninorms and the classical methods
of Canny, Sobel, Roberts and Prewitt has been made. The results concerning
the ρ-coeﬃcient are shown in Table 2. If we perform a Wilcoxon test and a
t-test using the conﬁguration ⟨TL, 0.02, SL⟩min and the classical methods, the
results show that there are no signiﬁcant diﬀerences between this uninorm based
method and the methods of Canny with σ ∈{0.5, 1, 1.5} but the uninorm is
statistically better that the method of Canny with σ ∈{2, 2.5} and the methods
of Sobel, Robert and Prewitt. Nevertheless, the results indicate that the uninorm
based method is the best one according to the mean value and the mean of the
rankings. In Fig. 4 we can observe some of the results obtained by some of the
edge detectors considered in this paper.

192
M. Gonz´alez-Hidalgo et al.
If we choose the F-measure instead of the ρ-coeﬃcient, the results are the
same in the sense that the best and worst images and the means of rankings are
identical and the conclusions of the statistical tests are the same.
Table 2. Mean, standard deviation, number of best and worst images and mean of
rankings of the best uninorm-based conﬁgurations and some classical methods accord-
ing to the ρ-coeﬃcient values
Method
x
σ
✓×
xp
⟨TP, 0.04, SP⟩min 0.756 0.107 3 0
4.267
⟨TP, 0.02, SP⟩min 0.762 0.096 0 0
3.933
⟨TL, 0.02, SL⟩min 0.765 0.092 2 0
3.600
Sobel
0.604 0.122 0 0
7.867
Roberts
0.516 0.107 0 11 10.400
Prewitt
0.602 0.124 0 0
7.733
Method
x
σ
✓×
xp
Canny σ = 0.5 0.700 0.217 5 4 4.733
Canny σ = 1 0.762 0.149 3 0 3.800
Canny σ = 1.5 0.738 0.108 2 0 4.667
Canny σ = 2 0.666 0.096 0 0 6.400
Canny σ = 2.5 0.586 0.086 0 0 8.600
6
Conclusions and Future Work
In this work, we have introduced a new fuzzy edge detector based on uninorms
and the performance of several uninorms in the class of Umin has been studied.
From the results, we can conclude that the conﬁguration with the uninorm of
Umin, with ΣLukasiewicz underlying operations gets statistically better results
than the the ones obtained by well known edge detectors, as Sobel, Roberts and
Prewitt approaches and comparable to the results obtained by Canny.
As future work, it would be interesting to evaluate the proposed method
with respect to Canny’s algorithm using the same post-processing steps in order
to assess the quality of the gradient estimation exclusively, and to evaluate the
method with respect to the subsequent steps: object segmentation or recognition.
In addition, we want to study other classes of uninorms, such as idempotent
and representable uninorms. The use of other aggregation functions and other
directions of the basic directional gradients could further improve the results.
Acknowledgments. This paper has been partially supported by the Spanish
Grants MTM2009-10320 and TIN2013-42795-P with FEDER support.
References
1. Bede, B., Nobuhara, H., Rudas, I.J., Fodor, J.: Discrete cosine transform based
on uninorms and absorbing norms. In: IEEE International Conference on Fuzzy
Systems (FUZZ-IEEE), pp. 1982–1986 (2008)
2. Bowyer, K., Kranenburg, C., Dougherty, S.: Edge detector evaluation using empir-
ical ROC curves. Computer Vision and Pattern Recognition 1, 354–359 (1999)

A New Edge Detector Based on Uninorms
193
3. Bustince, H., Barrenechea, E., Pagola, M., Fernandez, J.: Interval-valued fuzzy
sets constructed from matrices: Application to edge detection. Fuzzy Sets and
Systems 160(13), 1819–1840 (2009)
4. Canny, J.: A computational approach to edge detection. IEEE Trans. Pattern Anal.
Mach. Intell. 8(6), 679–698 (1986)
5. De Baets, B., Kerre, E., Gupta, M.: The fundamentals of fuzzy mathematical
morfologies part I: basics concepts. International J. of General Systems 23, 155–
171 (1995)
6. De Baets, B., Kwasnikowska, N., Kerre, E.: Fuzzy morphology based on uninorms.
In: Proc. of the 7th IFSA World Congress, Prague, pp. 215–220 (1997)
7. Fodor, J.C., Yager, R.R., Rybalov, A.: Structure of uninorms. Int. J. Uncertainty,
Fuzziness, Knowledge-Based Systems 5, 411–427 (1997)
8. Gonz´alez, M., Ruiz-Aguilera, D., Torrens, J.: Algebraic properties of fuzzy mor-
phological operators based on uninorms. In: Artiﬁcial Intelligence Research and
Development. Frontiers in Artiﬁcial Intelligence and Applications, vol. 100, pp.
27–38. IOS Press, Amsterdam (2003)
9. Gonz´alez-Hidalgo, M., Massanet, S.: A fuzzy mathematical morphology based on
discrete t-norms: fundamentals and applications to image processing. Soft Com-
puting, 1–15 (December 2013)
10. Gonz´alez-Hidalgo, M., Mir-Torres, A., Ruiz-Aguilera, D., Torrens, J.: Edge-images
using a uninorm-based fuzzy mathematical morphology: Opening and closing. In:
Tavares, J., Jorge, N. (eds.) Advances in Computational Vision and Medical Image
Processing, ch. 8. Computational Methods in Applied Sciences, vol. 13, pp. 137–
157. Springer, Netherlands (2009)
11. Grigorescu, C., Petkov, N., Westenberg, M.A.: Contour detection based on non-
classical receptive ﬁeld inhibition. IEEE Transactions on Image Processing 12(7),
729–739 (2003)
12. Klement, E.P., Mesiar, R., Pap, E.: Triangular norms. Kluwer Academic Publishers,
London (2000)
13. Kovesi, P.D.: MATLAB and Octave functions for computer vision and image pro-
cessing. Centre for Exploration Targeting, School of Earth and Environment. The
University of Western Australia (2012)
14. Lopez-Molina, C., De Baets, B., Bustince, H.: Quantitative error measures for edge
detection. Pattern Recognition 46(4), 1125 (2013)
15. Medina-Carnicer, R., Mu˜noz-Salinas, R., Yeguas-Bolivar, E., Diaz-Mas, L.: A novel
method to look for the hysteresis thresholds for the Canny edge detector. Pattern
Recognition 44(6), 1201–1211 (2011)
16. Nagau, J., Regis, S., Henry, J.-L., Doncescu, A.: Study of aggregation operators for
scheduling clusters in digital images of plants. In: 26th International Conference on
Advanced Information Networking and Applications Workshops, 2012, pp. 1161–
1166 (2012)
17. Papari, G., Petkov, N.: Edge and line oriented contour detection: State of the art.
Image and Vision Computing 29(2-3), 79 (2011)
18. Pratt, W.K.: Digital Image Processing, 4th edn. Wiley Interscience (2007)
19. Rijsbergen, C.: Information retrieval. Butterworths (1979)
20. Serra, J.: Image analysis and mathematical morphology, vol. 1, 2. Academic Press,
London (1982)
21. Yager, R.R., Rybalov, A.: Uninorm aggregation operators. Fuzzy Sets and Sys-
tems 80, 111–120 (1996)

Context-Aware Distance Semantics
for Inconsistent Database Systems
Anna Zamansky1, Ofer Arieli2, and Kostas Stefanidis3
1 Department of Information Systems, University of Haifa, Israel
annazam@is.haifa.ac.il
2 School of Computer Science, The Academic College of Tel-Aviv, Israel
oarieli@mta.ac.il
3 Institute of Computer Science, Foundation for Research and Technology
Hellas (FORTH), Greece
kstef@ics.forth.gr
Abstract. Many approaches for consistency restoration in database sys-
tems have to deal with the problem of an exponential blowup in the
number of possible repairs. For this reason, recent approaches advocate
more ﬂexible and ﬁne grained policies based on the reasoner’s preference.
In this paper we take a further step towards more personalized inconsis-
tency management by incorporating ideas from context-aware systems.
The outcome is a general distance-based approach to inconsistency main-
tenance in database systems, controlled by context-aware considerations.
1
Introduction
Inconsistency handling in constrained databases is a primary issue in the context
of consistent query answering, data integration, and data exchange. The standard
approaches to this issue are usually based on the principle of minimal change,
aspiring to achieve consistency via a minimal amount of data modiﬁcations (see,
e.g., [2,7,10]). A key question in this respect is how to choose among the diﬀerent
possibilities of restoring the consistency of a database (i.e., ‘repairing’ it).
Earlier approaches to inconsistency management were based on the assump-
tion that there should be some ﬁxed, pre-determined way of repairing a database.
Recently, there has been a paradigm shift towards user-controlled inconsistency
management policies. Works taking this approach provide a possibility for the
user to express some preference over all possible database repairs, preferring
certain repairs to others (see [18] for a survey and further references). While
such approaches provide the user with ﬂexibility and control over inconsistency
management, in reality they entail a considerable technical burden on the user’s
shoulders of calibrating, updating and maintaining preferences or policies. More-
over, in many cases these preferences may be dynamic, changing quickly on the
go (e.g., depending on the user’s geographical location). In the era of ubiquitous
computing, users want easy – and sometimes even fully automatic – inconsis-
tency management solutions with little cognitive load, while still expecting them
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 194–203, 2014.
c
⃝Springer International Publishing Switzerland 2014

Context-Aware Distance Semantics for Inconsistent Database Systems
195
to be personalized to their particular needs. This leads to the idea of introducing
context-awareness into inconsistency management.
Context-awareness is deﬁned as the use of contexts to provide task-relevant
information and services to a user (see [1]). We believe that inconsistency man-
agement has natural relations to the concept of context. To capture this idea,
we incorporate notions and techniques that have been studied by the context-
aware computing community to consistency management for database systems,
by combining the following two ingredients:
– Distance-based semantics
for restoring the consistency of inconsistent
databases according to the principle of minimal change, and
– Context-awareness considerations for incorporating user preferences.
Example 1. Let us consider the following simple database instance:
empNum name
address
salary
1
John
Tower Street 3, London, UK
70K$
1
John
Herminengasse 8, Wien, AT
80K$
2
Mary
42 Street 15, New York, US
90K$
Two functional dependencies that may be violated here are empNum →address
and empNum →salary. Thus, a database with the above relation and integrity
constraints is not consistent. Minimal change considerations (which will be ex-
pressed in what follows by distance functions) imply that it is enough to delete
either the ﬁrst or the second tuple for restoring consistency. Now, the decision
which tuple to delete may be context-dependent. For instance, for tax assessments
tuples with higher salaries may be preferred, while tuples with lower salaries may
have higher priority when loans or grants are considered. The choice between
the ﬁrst two tuples may also be determined by more dynamic considerations,
such as geographic locations, etc.
2
Inconsistent Databases and Distance Semantics
For simplicity of presentation, in this paper we remain on the propositional level
and reduce ﬁrst-order databases to our framework by grounding them. In the
sequel, L denotes a propositional language with a ﬁnite set of atomic formulas
Atoms(L). An L-interpretation I is an assignment of a truth value in {T, F} to
every element in Atoms(L). Interpretations are extended to complex formulas
in L in the usual way, using the truth tables of the connectives in L. The set
of two-valued interpretations for L is denoted by ΛL. An interpretation I is a
model of an L-formula ψ if I(ψ) = T , denoted by I |= ψ, and it is a model of
a set Γ of L-formulas, denoted by I |= Γ, if it is a model of every L-formula in
Γ. The set of models of Γ is denoted by mod(Γ). We say that Γ is satisﬁable if
mod(Γ) is not empty.
Deﬁnition 1. A database DB in L is a pair ⟨D, IC⟩, where D (the database
instance) is a ﬁnite subset of Atoms(L), and IC (the integrity constraints) is a
ﬁnite and consistent set of L-formulas.

196
A. Zamansky, O. Arieli, and K. Stefanidis
The meaning of D is determined by the conjunction of its facts, augmented
with Reiter’s closed world assumption, stating that each atomic formula that
does not appear in D is false: CWA(D) = {¬p | p ̸∈D}. Henceforth, a database
DB = ⟨D, IC⟩will be associated with the theory ΓDB = IC ∪D ∪CWA(D).
Deﬁnition 2. A database DB is consistent iﬀΓDB is satisﬁable.
When a database is not consistent at least one integrity constraint is vio-
lated, and so it is usually required to look for “repairs” of the database, that is,
changes of the database instance so that its consistency will be restored. There
are numerous approaches for doing so (see, e.g., [2,7,10] for some surveys on this
subject). Here we follow the distance-based approach described in [3,5], which we
ﬁnd suitable for our purposes since it provides a modular and ﬂexible framework
for a variety of methods of repair and consistent query answering. In the context
of database systems this approach aims at addressing the problem that when
DB is inconsistent mod(ΓDB) is empty, so reasoning with DB is trivialized. This
may be handled by replacing mod(ΓDB) with the set Δ(DB) of interpretations
that, intuitively, are ‘as close as possible’ to (satisfying) DB, while still satis-
fying the integrity constraints. When DB is consistent, Δ(DB) and mod(ΓDB)
coincide (see Proposition 3 below), which assures that distance-based semantics
is a conservative generalization of standard semantics for consistent databases.
In what follows, we recall the relevant deﬁnitions for formalizing the intuition
above (see also [3,5]).
Deﬁnition 3. A pseudo-distance on a set U is a total function d : U ×U →R+,
which is symmetric (for all ν, μ ∈U, d(ν, μ) = d(μ, ν)) and preserves identity
(for all ν, μ ∈U, d(ν, μ) = 0 if and only if ν = μ). A pseudo-distance d is called a
distance (metric) on U, if it satisﬁes the triangular inequality: for all ν, μ, σ ∈U,
d(ν, σ) ≤d(ν, μ) + d(μ, σ).
Deﬁnition 4. A (numeric) aggregation function is a function f, whose domain
consists of multisets of real numbers and whose range is the real numbers, sat-
isfying the following properties:
1. f is non-decreasing when a multiset element is replaced by a larger element,
2. f({x1, . . . , xn}) = 0 if and only if x1 = x2 = . . . xn = 0, and
3. f({x}) = x for every x ∈R.
An aggregation function f is hereditary, if f({x1, . . . , xn}) < f({y1, . . . , yn})
entails that f({x1, . . . , xn, z1, . . . , zm}) < f({y1, . . . , yn, z1, . . . , zm}).
In what follows we shall aggregate distance values. Since distances are non-
negative numbers, aggregation functions in this case include the summation and
the maximum functions, the former is also hereditary.
Example 2. One may deﬁne the following distances on ΛL:
dU(I, I′) =

1
if I ̸= I′,
0
otherwise.
dH(I, I′) = | {p ∈Atoms(L) | I(p) ̸= I′(p)} |.
dU is sometimes called the uniform distance and dH is known as the Hamming
distance. More sophisticated distances are considered, e.g., in [5] and [12].

Context-Aware Distance Semantics for Inconsistent Database Systems
197
Deﬁnition 5. A distance setting (for a language L) is a pair DS = ⟨d, f⟩, where
d is a pseudo-distance on ΛL and f is an aggregation function.
The next deﬁnition is a common way of using distance functions for main-
taining inconsistent data (see, e.g, [15,16]).
Deﬁnition 6. For a ﬁnite set Γ = {ψ1, . . . , ψn} of formulas in L, an interpre-
tation I ∈ΛL, and a distance setting DS = ⟨d, f⟩for L, we denote: dDS(I, ψi) =
min{d(I, I′) | I′ |= ψi} and δDS(I, Γ) = f({dDS(I, ψ1), . . . , dDS(I, ψn)}).
Proposition 1. [3,16] For every interpretation I ∈ΛL and a distance setting
DS = ⟨d, f⟩, it holds that I |= ψ iﬀdDS(I, ψ) = 0 and I |= Γ iﬀδDS(I, Γ) = 0.
Deﬁnition 7. Given a database DB = ⟨D, IC⟩in L and a distance setting
DS = ⟨d, f⟩for L, the set of the most plausible interpretations of DB (with
respect to DS) is deﬁned as follows:
ΔDS(DB) =
%
I ∈mod(IC) | I′ ∈mod(IC) =⇒
δDS(I, D ∪CWA(D)) ≤δDS(I′, D ∪CWA(D))
&
.
Note 1. Since IC is satisﬁable, for every database DB = ⟨D, IC⟩and a distance
setting DS for its language, it holds that ΔDS(DB) ̸= ∅.
Deﬁnition 8. Let DB = ⟨D, IC⟩be a database and DS = ⟨d, f⟩a distance
setting. We say that R is a DS-repair of DB, if there is an I ∈ΔDS(DB) such that
R = {p ∈Atoms(L) | I(p) = T }. We shall sometimes denote this repair by R(I)
and say that it is induced by I (or that I is the characteristic model of R). The
set of all the DS-repairs is denoted by RepairsDS(DB) = {R(I) | I ∈ΔDS(DB)}.
An alternative characterization of the DS-repairs of DB is given next:
Proposition 2. Let DB = ⟨D, IC⟩be a database and DS = ⟨d, f⟩a distance
setting. Let IS be the characteristic function of S ⊆Atoms(L) (that is, IS(p) = T
if p ∈S and IS(p) = F otherwise). The DS-inconsistency value of S is:
IncDS(S) =

δDS(IS, D ∪CWA(D))
if IS ∈mod(IC),
∞
otherwise.
Then R ⊆Atoms(L) is a DS-repair of DB iﬀits DS-inconsistency value is
minimal among the DS-inconsistency values of the subsets of Atoms(L).
Proof. Let R ⊆Atoms(L) such that IncDS(R) ≤IncDS(S) for every S ⊆Atoms(L).
Since IC is satisﬁable, IncDS(R) < ∞, and so IR ∈mod(IC). Let now R′ be a
DS-repair of DB. Then there is an element I′ ∈ΔDS(DB) such that R′ = {p ∈
Atoms(L) | I′(p) = T }. But δDS(IR, D ∪CWA(D)) ≤δDS(I′, D ∪CWA(D)), and
so IR ∈ΔDS(DB) as well, which implies that R is a DS-repair of DB.
For the converse, let R be a DS-repair of DB and let S ⊆Atoms(L). We have to
show that IncDS(R) ≤IncDS(S). Indeed, if IS ̸∈mod(IC) then IncDS(S) = ∞and
so the claim is obtained. Otherwise, both IR and IS are models of IC, and since
R is a DS-repair of DB, IR ∈ΔDS(DB). It follows that δDS(IR, D ∪CWA(D)) ≤
δDS(IS, D ∪CWA(D)) and so IncDS(R) ≤IncDS(S).
□

198
A. Zamansky, O. Arieli, and K. Stefanidis
By Proposition 1 and Deﬁnition 8, we also have the following result:
Proposition 3. Let DB = ⟨D, IC⟩be a database and DS a distance setting.
The following conditions are equivalent: (1) DB is consistent, (2) ΔDS(DB) =
mod(ΓDB), (3) RepairsDS(DB) = {D}, (4) The DS-inconsistency value of every
DS-repair of DB is zero.
Example 3. Let us return to the database in Example 1. The projection of the
database table on the attributes id and salary is: {⟨1, 70K$⟩, ⟨1, 80K$)⟩, ⟨2, 90K$⟩}.
After grounding the database and representing the tuple ⟨empNum, salary⟩by a
propositional variable TempNum
salary
, we have:
D ∪CWA(D) =
'
T1
70K$, T1
80K$, ¬T1
90K$, ¬T2
70K$, ¬T2
80K$, T2
90K$
(
,
and the functional dependency empNum →salary is formulated as follows:
IC =
'
Tx
y →¬Tx
z | y ̸= z, y, z ∈{70K$, 80K$, 90K$}, x ∈{1, 2}
(
.
Using the distance dH from Example 2 and f = Σ, we compute:
I
dH(I, T1
70) dH(I, T1
80) dH(I, ¬T1
90) dH(I, ¬T2
70) dH(I, ¬T2
80) dH(I, T2
90) δdH,Σ(I, ΓDB)
∅
1
1
0
0
0
1
3
{T1
70}
0
1
0
0
0
1
2
{T1
80}
1
0
0
0
0
1
2
...
...
...
...
...
...
...
...
{T1
70, T2
90}
0
1
0
0
0
0
1
{T1
80, T2
90}
1
0
0
0
0
0
1
...
...
...
...
...
...
...
...
It follows that Δ⟨dH,Σ⟩(DB) = {I1, I2} and RepairsDS(DB) = {R(I1), R(I2)},
where R(I1) = {T1
70, T2
90} and R(I2) = {T1
80, T2
90}}. Thus, only T2
90 holds in all
the repairs of DB, that is, only the salary of employee 2 is certain.
3
Context-Aware Inconsistency Management
3.1
Context Modeling
As deﬁned in [1], “Context is any information that can be used to characterize
the situation of an entity. An entity is a person, place or object that is considered
relevant to the interaction between a user and an application, including the user
and application”. This notion has been found useful in several domains, such as
machine learning and knowledge acquisition (see, e.g., [8,9]). We shall consider as
a context any data that can be used to characterize database-related situations,
involving database entities, user contexts and preferences, etc. [11]. There is a
wide variety of methods for modeling contexts. Here we follow the data-centric
approach introduced in [20], and refer to contexts using a ﬁnite set of special
purpose variables (which may not be part of the database).
Deﬁnition 9. A context environment (or just a context) C is a ﬁnite tuple of
variables ⟨c1, . . . , cn⟩, where each variable ci (1 ≤i ≤n) has a corresponding
range Range(ci) of possible values. A context state for C (a C-state, for short)
is an assignment S such that S(ci) ∈Range(ci). The set of context states is
denoted by States(C).

Context-Aware Distance Semantics for Inconsistent Database Systems
199
Intuitively, a context environment C represents the parameters that may be
taken into consideration for the database inconsistency maintenance.
We are now ready to incorporate context-awareness into distance consider-
ations. We do so by making the ‘most plausible’ interpretations in DB, the
elements in ΔDS(DB), sensitive to context, in the sense that more ‘relevant’
formulas have higher impact on the distance computations than less ‘relevant’
formulas. Thus, while we still strive to minimize change, the latter will be mea-
sured in a more subtle, context-aware way.
Deﬁnition 10. A relevance ranking for a set Γ of formulas and a context en-
vironment C, is a total function R : Γ × States(C) →(0, 1].
Given a set Γ and a context environment C, a relevance ranking function for
Γ and C assigns to every formula ψ ∈Γ and every state S of C a (positive)
relevance factor R(ψ, S) indicating the relevance of ψ according to S. Intuitively,
higher values of these factors correspond to higher relevance of their formulas,
which makes changes to these formulas in computing database repairs less de-
sirable.1
Deﬁnition 11. A context setting for a set of formulas Γ is a triple CS(Γ) =
⟨C, S, R⟩, where C is a context environment, S ∈States(C) is a C-state, and R
is a relevance ranking function for Γ and C. In what follows we shall sometimes
denote by CS(L) a context setting CS(Γ) in which Γ is the set of all the well-
formed formulas of L.
Consistency restoration for databases can now be deﬁned as before (see Deﬁ-
nitions 6 and 7), except that the underlying distance setting DS = ⟨d, f⟩should
now be context-sensitive in the sense that dDS preserves the order induced by
ranking in the following way:
Deﬁnition 12. Let CS(L) = ⟨C, S, R⟩be a context setting for a language L.
A distance setting DS = ⟨d, f⟩is called CS-sensitive, if for every two atomic
formulas p1 and p2 such that R(p1, S) > R(p2, S), it holds that dDS(I2, p1) >
dDS(I1, p2) for every I1 ∈mod(p1) \ mod(p2) and I2 ∈mod(p2) \ mod(p1).
Clearly, Proposition 3 holds also for context-sensitive distance settings.
Next, we demonstrate the eﬀect of incorporating context sensitive distance
settings on inconsistency management.
Proposition 4. Let DB = ⟨D ⊔{p1, p2}, IC⟩be a database2, CS = ⟨C, S, R⟩a
context setting and DS = ⟨d, f⟩a CS-sensitive distance setting in which f is
hereditary. If R(p1, S) > R(p2, S), for every D′ ⊆Atoms(L) \ {p1, p2} such that
D′ ⊔{p1} |= IC, the DS-inconsistency value of D1 = D′ ⊔{p1} is smaller than
the DS-inconsistency value of D2 = D′ ⊔{p2}.
1 Relevance factors may be thought of as a context-dependent interpretation of weights
in prioritized theories (see, for example, [4]).
2 We denote by D ⊔{p1, p2} the disjoint union of D and {p1, p2}.

200
A. Zamansky, O. Arieli, and K. Stefanidis
Proof. Let D′ ⊆Atoms(L) \ {p1, p2} and D1 = D′ ∪{p1}. Since D1 |= IC, we
have that IncDS(D1) < ∞. Thus, IncDS(D1) < IncDS(D2) whenever D2 ̸|= IC.
Suppose then that D2 |= IC as well. In this case, in the notations of Propo-
sition 2, we have that ID1 and ID2 diﬀer only in the assignments for p1 and
p2 (I.e., ID1 satisﬁes p1 and falsiﬁes p2 while ID2 satisﬁes p2 and falsiﬁes p1.
Elsewhere, both interpretations are equal to ID′). Now, since DS is CS-sensitive,
by the facts that (i) R(p1, S) > R(p2, S), (ii) ID1 ∈mod(p1) \ mod(p2) and
(iii) ID2 ∈mod(p2) \ mod(p1), we have that dDS(ID1, p2) < dDS(ID2, p1). Let
D ∪CWA(D ⊔{p1, p2}) = {ψ1, . . . , ψn}. By the assumption that f is hereditary,
IncDS(D1) = δDS(ID1, DB) =
f({dDS(ID1, ψ1), . . . , dDS(ID1, ψn), dDS(ID1, p1), dDS(ID1, p2)}) =
f({dDS(ID1, ψ1), . . . , dDS(ID1, ψn), 0, dDS(ID1, p2)}) =
f({dDS(ID2, ψ1), . . . , dDS(ID2, ψn), 0, dDS(ID1, p2)}) <
f({dDS(ID2, ψ1), . . . , dDS(ID2, ψn), 0, dDS(ID2, p1)}) =
f({dDS(ID2, ψ1), . . . , dDS(ID2, ψn), dDS(ID2, p2), dDS(ID2, p1)}) =
δDS(ID2, DB) = IncDS(D2).
□
It follows that when context-sensitive distances are incorporated, “more rele-
vant” formulas are preferred in the repairs. This is shown next.
Corollary 1. Let DB = ⟨D ⊔{p1, p2}, IC⟩be a database, CS = ⟨C, S, R⟩a con-
text setting and DS = ⟨d, f⟩a CS-sensitive distance setting in which f is heredi-
tary. If DB1 = ⟨D ⊔{p1}, IC⟩is a consistent database, R(p1, S) > R(p2, S), and
IC ∪{p1, p2} is (classically) inconsistent, then no DS-repair of DB contains p2.
Corollary 1 can be generalized as follows:
Corollary 2. Let DB = ⟨D, IC⟩be a database, CS = ⟨C, S, R⟩a context setting
and DS = ⟨d, f⟩a CS-sensitive distance setting in which f is hereditary. Suppose
that D = D′ ⊔D′′ can be partitioned to two disjoint nonempty subsets D′ and D′′
such that (1): DB′ = ⟨D′, IC⟩is a consistent database, (2): ∀p′′ ∈D′′ ∃p′ ∈D′
such that IC ∪{p′, p′′} is not consistent, and (3): ∀p′ ∈D′ and ∀p′′ ∈D′′ it holds
that R(p′, S) > R(p′′, S). Then for every DS-repair R of DB, R ∩D′′ = ∅.
3.2
A Simple Construction of Context-Sensitive Distance Settings
Below we provide a concrete method for deﬁning context-sensitive distance set-
tings and exemplify some of the properties of the settings that are obtained.
Deﬁnition 13. Let CS(L) = ⟨C, S, R⟩be a context setting for L and let g be
an aggregation function. The (pseudo) distance dCS
g
on ΛL is deﬁned as follows:
d CS
g (I, I′) = g({R(p, S) · |I(p) −I′(p)| | p ∈Atoms(L)}).
It is easy to verify that for any CS and g, the function dCS
g
is a pseudo-
distance on ΛL. In particular, for any context setting CS(L) = ⟨C, S, R⟩where
R is uniformly 1, dCS
Σ coincides with the Hamming distance dH in Example 2.
The next proposition provides a general way of constructing context-sensitive
distance settings, based on the functions in Deﬁnition 13.

Context-Aware Distance Semantics for Inconsistent Database Systems
201
Proposition 5. Let CS = ⟨C, S, R⟩be a context setting and let DS = ⟨dCS
g , f⟩
be a distance setting, where g is a hereditary. Then DS is CS-sensitive.
Proof. Let p1 and p2 be atomic formulas such that R(p1, S) > R(p2, S), and
let I1 ∈mod(p1) \ mod(p2) and I2 ∈mod(p2) \ mod(p1). Below, we denote
g(0, x) = g({0, . . ., 0, x, 0, . . . , 0}). By Deﬁnition 6,
dDS(I1, p2) = min{d CS
g (I1, J) | J |= p2}
= min{g({R(p, S) · |I1(p) −J(p)| | p ∈Atoms(L)}) | J |= p2}.
Since g is hereditary, the minimum above must be obtained for a model J of p2
that coincides with I1 on every atom p ̸= p2. It follows, then, that dDS(I1, p2) =
g(0, R(p2, S)). By similar considerations, dDS(I2, p1) = g(0, R(p1, S)). Now, since
R(p1, S) > R(p2, S) and since g is hereditary, dDS(I2, p1) > dDS(I1, p2).
□
The next proposition demonstrates how CS-sensitive distance settings of the
form deﬁned above give precedence to “more relevant” facts.
Proposition 6. Let CS = ⟨C, S, R⟩be a context setting and let DS = ⟨dCS
g , f⟩
be a distance setting, where g and f are hereditary aggregation functions. Let
DB = ⟨D ⊔{p1, p2}, IC⟩be a database such that:
1. R(p1, S) > R(p2, S) (i.e., p1 is more relevant than p2), and
2. IC ∪{p1, p2} is not consistent but DB1 = ⟨D ⊔{p1}, IC⟩is consistent3.
Then ΔDS(DB) = {I1}, where I1 is the (unique) model of DB1.
Proof. Again, we denote: g(0, x) = g({0, . . ., 0, x, 0, . . . , 0}). Then, for all p, I,
dDS(I, p) =

0
if I |= p,
g(0, R(p, S))
otherwise.
Suppose that D ∪CWA(D ⊔{p1, p2}) = {ψ1, . . . , ψn}. Let I ∈ΔDS(DB) and I1 ∈
mod(ΓDB1) (Such a model exists, since DB1 is consistent). By Corollary 1, since
DS is CS-sensitive (Proposition 5), I ̸|= p2, and so dDS(I, p2) = g(0, R(p2, S)).
Now,
δDS(I, DB) = f({dDS(I, ψ1), . . . , dDS(I, ψn), dDS(I, p1), dDS(I, p2)}) =
f({dDS(I, ψ1), . . . , dDS(I, ψn), dDS(I, p1), g(0, R(p2, S))}).
and so, since f is non-decreasing,
δDS(I, DB) ≥f({0, . . ., 0, 0, g(0, R(p2, S))) =
f({dDS(I1, ψ1), . . . , dDS(I1, ψn), dDS(I1, p1), dDS(I1, p2)}) =
δDS(I1, DB).
Thus, I1 ∈ΔDS(DB). On the other hand, if there is some q ∈{ψ1, . . . , ψn, p1}
for which dDS(I, q) ̸= 0, then since f is hereditary the above inequality becomes
strict, which contradicts the assumption that I ∈ΔDS(DB). It follows that for
every q ∈{ψ1, . . . , ψn, p1} dDS(I, q) = dDS(I1, q) = 0, i.e., I |= q. One concludes,
then, that I is a model of DB1, that is, I = I1.
□
Proposition 6 may be extended in various ways. Below is one such extension.
3 DB2 = ⟨D ⊔{p2}, IC⟩may be consistent as well, but this is not a prerequisite.

202
A. Zamansky, O. Arieli, and K. Stefanidis
Proposition 7. Let DB = ⟨D, IC⟩CS = ⟨C, S, R⟩and DS = ⟨dCS
g , f⟩, where g
and f are hereditary aggregation functions. Suppose that D can be partitioned to
two nonempty subsets D′ and D′′, such that
1. DB′ = ⟨D′, IC⟩is a consistent database,
2. ∀p′′ ∈D′′ ∃p′ ∈D′ s.t. IC ∪{p′, p′′} is not consistent, and
3. ∀p′ ∈D′ and ∀p′′ ∈D′′, R(p′, S) > R(p′′, S).
Then ΔDS(DB) = {I′}, where I′ is the (unique) model of DB′.
Proof. The proof is similar to that of Proposition 6. We omit the details.
□
Example 4. Consider again the database in Example 1. By Example 3, the dis-
tance setting DS = ⟨dH, Σ⟩leads to the following two equally good repairs:
Repair 1 :
Repair 2 :
eNum name
address
salary
1
John
..., UK
70K$
2
Mary
..., US
90K$
eNum name
address
salary
1
John
..., AT
80K$
2
Mary
..., US
90K$
Sensitivity to context may diﬀerentiate between these repairs, preferring one to
another. Let us again denote by T1
UK, T1
AT and T2
US the tuple according to which
John lives in the UK and is payed 70K$, John lives in Austria and is payed
80K$, and the tuple with the information about Mary.
Now, consider the context setting CS(L) = ⟨C, S, R⟩and the distance set-
ting DS = ⟨dCS
Σ , Σ⟩, where C = {country}, Range(country) = {US, UK, AT},
S(country) = UK, and the relevance ranking is given by the following functions:
R(Ti
c, S) =

1,
if c = S(country),
0.5,
otherwise.
R(¬Ti
c, S) =

0.5,
if c = S(country),
1,
otherwise.
Computation of ΔDS is given below (where we abbreviate d(ψ, S) for dCS
Σ (ψ, S)).
I
d(I, T1
UK,S) d(I, T1
AT,S) d(I, ¬T1
US,S) d(I, ¬T2
UK,S) d(I, ¬T2
AT,S) d(I, T2
US,S) δDS(I, Γ, S)
∅
1
0.5
0
0
0
0.5
2
{T1
UK}
0
0.5
0
0
0
0.5
1
{T1
AT}
1
0
0
0
0
0.5
1.5
{T1
US}
1
0.5
1
0
0
0.5
3
...
...
...
...
...
...
...
...
{T1
UK, T2
US}
0
0.5
0
0
0
0
0.5
{T1
AT, T2
US}
1
0
0
0
0
0
1
...
...
...
...
...
...
...
...
According to CS, the single element in ΔDS(DB) satisﬁes {T1
UK, T2
US}, and so
Repair 1 is preferred. Dually, in a state S′ where S′(country) = AT, Repair 2 is
preferred. Thus, context-aware considerations lead us to choose diﬀerent repairs
according to the relevance ranking, as indeed guaranteed by Propositions 6 and 7.
4
Conclusion
As observed in [13], contexts have largely been ignored by the AI community. In
the database community context awareness has only recently been addressed in
relation to user preference in querying (consistent) databases [17,19]. To the best

Context-Aware Distance Semantics for Inconsistent Database Systems
203
of our knowledge, the approach presented here is the ﬁrst one to combine in-
consistency management with context-aware considerations. Combined with the
extensive work available on personalization and automatically determining user’s
context and preferences (see, e.g., [6,14]), it may open the door to new inconsis-
tency management solutions and novel database technologies. Implementation
and evaluation of the methods in this paper is currently a work in progress.
References
1. Abowd, G.D., Dey, A.K.: Towards a better understanding of context and context-
awareness. In: Gellersen, H.-W. (ed.) HUC 1999. LNCS, vol. 1707, pp. 304–307.
Springer, Heidelberg (1999)
2. Arenas, M., Bertossi, L., Chomicki, J.: Answer sets for consistent query answering
in inconsistent databases. TPLP 3(4-5), 393–424 (2003)
3. Arieli, O.: Distance-based paraconsistent logics. International Journal of Approxi-
mate Reasoning 48(3), 766–783 (2008)
4. Arieli, O.: Reasoning with prioritized information by iterative aggregation of dis-
tance functions. Journal of Applied Logic 6(4), 589–605 (2008)
5. Arieli, O., Denecker, M., Bruynooghe, M.: Distance semantics for database repair.
Annals of Mathematics and Artiﬁcial Intelligence 50(3-4), 389–415 (2007)
6. Baldauf, M., Dustdar, S., Rosenberg, F.: A survey on context-aware systems. In-
ternational Journal of Ad Hoc and Ubiquitous Computing 2(4), 263–277 (2007)
7. Bertossi, L.: Consistent query answering in databases. SIGMOD Record 35(2),
68–76 (2006)
8. Bolchini, C., Curino, C., Orsi, G., Quintarelli, E., Rossato, R., Schreiber, F.A.,
Tanca, L.: And what can context do for data? Comm. ACM 52(11), 136–140 (2009)
9. Br´ezillon, P.: Context in artiﬁcial intelligence: I. A survey of the literature. Com-
puters and Artiﬁcial Intelligence 18(4) (1999)
10. Chomicki, J.: Consistent query answering: Five easy pieces. In: Schwentick, T.,
Suciu, D. (eds.) ICDT 2007. LNCS, vol. 4353, pp. 1–17. Springer, Heidelberg (2006)
11. Dey, A.: Understanding and using context. Personal and Ubiquitous Comput-
ing 5(1), 4–7 (2001)
12. Eiter, T., Mannila, H.: Distance measure for point sets and their computation.
Acta Informatica 34, 109–133 (1997)
13. Ekbia, H.R., Maguitman, A.G.: Context and relevance: A pragmatic approach. In:
Akman, V., Bouquet, P., Thomason, R.H., Young, R.A. (eds.) CONTEXT 2001.
LNCS (LNAI), vol. 2116, pp. 156–169. Springer, Heidelberg (2001)
14. Henricksen, K., Indulska, J.: Developing context-aware pervasive computing appli-
cations: Models and approach. Pervasive and Mobile Comput. 2, 37–64 (2006)
15. Konieczny, S., Lang, J., Marquis, P.: DA2 merging operators. Artiﬁcial Intelli-
gence 157(1-2), 49–79 (2004)
16. Konieczny, S., Pino P´erez, R.: Merging information under constraints: A logical
framework. Logic and Computation 12(5), 773–808 (2002)
17. Pitoura, E., Stefanidis, K., Vassiliadis, P.: Contextual database preferences. IEEE
Data Engeneering Bulletin 34(2), 19–26 (2011)
18. Staworko, S., Chomicki, J., Marcinkowski, J.: Prioritized repairing and consistent
query answering in relational databases. Ann. Math. Artif. Intel. 64, 209–246 (2012)
19. Stefanidis, K., Pitoura, E.: Fast contextual preference scoring of database tuples.
In: Proceedings of EDBT 2008, pp. 344–355. ACM (2008)
20. Stefanidis, K., Pitoura, E., Vassiliadis, P.: Managing contextual preferences. Infor-
mation Systems 36(8), 1158–1180 (2011)

An Analysis of the SUDOC Bibliographic Knowledge
Base from a Link Validity Viewpoint
Léa Guizol1, Olivier Rousseaux2, Madalina Croitoru1,
Yann Nicolas2, and Aline Le Provost2
1 LIRMM (University of Montpellier II & CNRS), INRIA Sophia-Antipolis, France
2 ABES, France
Abstract. In the aim of evaluating and improving link quality in bibliographical
knowledge bases, we develop a decision support system based on partitioning se-
mantics. The novelty of our approach consists in using symbolic values criteria
for partitioning and suitable partitioning semantics. In this paper we evaluate and
compare the above mentioned semantics on a real qualitative sample. This sam-
ple is issued from the catalogue of French university libraries (SUDOC), a bibli-
ographical knowledge base maintained by the University Bibliographic Agency
(ABES).
1
Introduction
Real World Context. The SUDOC (catalogue du Système Universitaire de Documen-
tation) is a large bibliographical knowledge base managed by ABES (Agence Bibli-
ographique de l’Enseignement Supérieur). The SUDOC contains bibliographic notices
(document descriptions ≈10.000.000), and authorship notices (person descriptions
≈2.000.000). An authorship notice possesses some attributes (ppn1, appellation set,
date of birth...). A bibliographic notice also possesses some attributes (title, ppn1, lan-
guage, publication date...) and link(s) to authorship notices. A link is labeled by a role
(as author, illustrator or thesis advisor) and means that the person described by the
authorship notice has participated as the labeled role to the document described by the
bibliographic notice.
One of the most important tasks for ABES experts is to reference a new book in
SUDOC. To this end, the expert has to register the title, number of pages, types of pub-
lication domains, language, publication date, and so on, in a new bibliographic notice.
This new bibliographic notice represents the physical books in the librarian hands which
he/she is registering. He/she also has to register people which participated to the book’s
creation (namely the contributors). In order to do that, for each contributor, he/she se-
lects every authorship notice (named candidates) which has an appellation similar to
the book contributor. Unfortunately, there is not that much information in authorship
notices because the librarian politics is to give minimal information, solely in order to
distinguish two authorship notices which have the same appellation, and nothing more
(they reference books, not people!). So the librarian has to look at bibliographic notices
which are linked to authorship notices candidates (the bibliography of candidates) in
1 A ppn identiﬁes a notice.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 204–213, 2014.
c⃝Springer International Publishing Switzerland 2014

An Analysis of the SUDOC Bibliographic Knowledge Base
205
order to see whether the book in his/her hands seems to be a part of the bibliography
of a particular candidate. If it is the case, he/she links the new bibliographic notice to
this candidate and looks at the next unlinked contributor. If there is no good candidate,
he/she creates a new authorship notice to represent the contributor.
This task is fastidious because it is possible to have a lot of candidates for a single
contributor (as much as 27 for a contributor named “BERNARD, Alain”). This creates
errors, which in turn can create new errors since linking is an incremental process. In
order to help experts to repair erroneous links, we proposed two partitioning semantics
in [11] which enables us to detect erroneous links in bibliographic knowledge bases. A
partitioning semantics evaluates and compares partitions2.
Contribution. The contribution of this paper is to practically evaluate the results quality
of partitioning semantics [11] on a real SUDOC sample. We recall the semantics in
section 3, clearly explain on which objects and with which criteria the semantics have
been applied in section 2, and present qualitative results in section 4. We discuss the
results and conclude the paper in section 5.
2
Qualitative Experiments
In this section, we ﬁrst adapt the entity resolution problem3[4] to investigate link
quality in SUDOC in section 2.1. This problem is known in literature under very dif-
ferent names (as record linkage [16], data deduplication [2], reference reconciliation
[14]...). Then we deﬁne (section 2.3) and detail (section 2.4) criteria used in order to
detect erroneous links in SUDOC. Those criteria are used on SUDOC subsets deﬁned in
section 2.2.
2.1
Contextual entities: From Erroneous Links to Entity Resolution
In order to detect and repair erroneous links, we represent SUDOC links into contex-
tual entity (the i contextual entity is denoted Nci). A contextual entity represents a
bibliographic notice Nbj from the viewpoint of one of its contributor, named the C con-
tributor of Nci and denoted C(Nci). The contextual entities are compared together with
an entity resolution method, in order to see which ones have a contributor representing a
same real-word person. As explained in [8], traditional entity resolution methods cannot
be directly applied. This entity resolution method is supposed to group (in a same class
of the created partition) the contextual entities such as their C contributor represents a
same real-word person, and to separate the other ones (to put them in distinct partition
classes). A contextual entity Nci has several attributes. Most of them are Nb(Nci) at-
tributes (as title, publication date, publication language, publication domain codes list)
and others depend on the C contributor:
2 A partition P of an object set X is a set of classes (X subsets) such as each object of X is in
one and only one P class.
3 The entity resolution problem is the problem of identifying as equivalent two objects repre-
senting the same real-world entity.

206
L. Guizol et al.
– role of the C contributor (there is a set of typed roles as “thesis_advisor”),
– list of the possible appellations of the C contributor. An appellation is composed of
a name and a surname, sometimes abbreviated (as “J.” for surname),
– list of contributors which are not C. For each of them, we have the identiﬁer of the
authorship notice which represents it, and the role.
The publication language attribute is typed (for example, “eng” for English language,
“fre” for French language and so on). The publication date is most of the time the publi-
cation year (“1984”). Sometimes information is missing and it only gives the century or
decade (“19XX” means that the document has been published last century). A publica-
tion domain is not a describing string but a code with 3 digits which represent a domain
area.
Example 1 (Contextual entity attributes). The authorship notice of ppn 026788861,
which represents “CHRISTIE, Agatha” is linked as “author” to the bibliographic no-
tice of ppn 121495094, which represents “Evil under the sun” book. The contextual
entity which represents this links has the following attributes:
– title: “Evil under the sun”
– publication date: “2001”
– publication language: “eng”
– publication domain codes list: {} (they have not been given by a librarian)
– list of the possible appellations of the C contributor: {“CHRISTIE, Agatha”,“WEST-
MACOTT, Mary”,“MALLOWAN, Agatha”,“MILLER, Agathe Marie Clarissa”}
– role of the C contributor: “author”
– list of contributors which are not C: {} (there is no other contributors in this case)
Let Nci be the contextual entity identiﬁed by i. As any contextual entity, it has been
constructed because of a link between an authorship notice and a bibliographic notice,
which are respectively denoted Na(Nci) and Nb(Nci). We deﬁne two particular par-
titions: the initial one and the human one.
The initial partition (denoted Pi) of contextual entities is the one such as two con-
textual entities Nci, Ncj are in a same class if and only if Na(Nci) = Na(Ncj). This
represents the original organization of links in SUDOC.
The human partition (denoted Ph) of contextual entities is a partition based on an
human expert’s advice: two contextual entities Nci, Ncj are in a same class if and only
if the expert thinks that their C contributor corresponds to a same real word person.
The goal of this paper’s work is to distinguish SUDOC subsets constructed as in the
following section 2.2 with or without erroneous links. We make the hypothesis that the
human partition has to be a best one (because it is the good one according to expert) and
that the initial partition has to not be a best partition except if Pi = Ph. So, partitioning
semantics are approved if Ph is a best partition according to the semantics, but not Pi.
Let us determine what is a SUDOC contextual entities subset to partition.
2.2
Selecting contextual entities on Appellation
A SUDOC subset O selected for an appellation A contains all contextual entities which
represent a link between any SUDOC bibliographic notice and a SUDOC authorship

An Analysis of the SUDOC Bibliographic Knowledge Base
207
notice which has an appellation close to the appellation A. To select a SUDOC subset
for a given appellation (as “BERNARD, Alain”) is a way to separate SUDOC in sub-
sets which can be treated separately, This is also a simulation of how experts select a
SUDOC subset to work on it, as explained in part 1. In the following, we will only be
interested into partitioning SUDOC subsets selected for an appellation. Let us deﬁne
and describe criteria used in order to compare contextual entities together.
2.3
Symbolic Criteria
In the general case, a criterion is a function which compares two objects and returns a
comparison value. Let c be a criterion, and oi, oj are two objects. We denote c(oi, oj),
the comparison values according to c between oi and oj.
In this work case, we use symbolic criteria which can return always, never,
neutral, a closeness value or a farness value as comparison value. always (respec-
tively never) means that objects have to be in a same (respectively distinct) partition
class2. Closeness (respectively farness) values are more or less intense and far from the
neutral value, meaning that objects should be in a same (respectively distinct) partition
class. Closeness (respectively farness) values are strictly ordered between themselves,
speciﬁc to a criterion and less intense than always (respectively never). Those values
are denoted +,++ and so on (respectively −,−−) such as the more + (respectively −)
symbols they have, the more intense and the further from neutral the value is. For a
criterion, always is more intense than + + + + +, which is more intense than ++
which is more intense than +. + is only more intense than neutral. neutral means
that the criterion has no advice about whether to put objects in a same class or not.
2.4
Criteria for Detecting Link Issues in SUDOC
In order to simulate human expert behaviour, nine symbolic criteria have been develo-
ped. Some are closeness-criteria4 (title, otherContributors), farness-criteria4 (thesis,
thesisAdvisor, date, appellation, language) and others are both (role, domain).
Each of these criteria give the neutral comparison value when a required attribute of a
compared contextual entity is unknown and by default. Let Nci, Ncj be two contextual
entities.
– appellation criterion is a particular farness-criterion. Indeed, it compares appel-
lation lists to determine which contextual entities can not have a same contribu-
tor C. When it is certain (as when appellations are “CONAN DOYLE, Arthur” and
“CHRISTIE, Agatha”), it gives a never comparison value, which forbids other cri-
teria to compare the concerned authorship notices together. This is also used to
divide SUDOC in subsets which should be evaluated separately.
– title criterion is a closeness-criterion. This criterion can give an always value and
3 closeness comparison values. It is based on a Levenshtein comparison [13]. It is
useful to determine which contextual entities represent a same work, edited several
times. This is used by the thesis criterion.
4 A closeness-criterion (respectively a farness-criterion) c is a criterion which can give a close-
ness or always (respectively a farness or never) comparison value to two objects.

208
L. Guizol et al.
– otherContributors criterion is a closeness-criterion. It counts the others contribu-
tors in common, by comparing their authorship notices. One (respectively several)
other common contributor gives a + (respectively ++) comparison value.
– thesis criterion is a farness-criterion. thesis(Nci, Ncj) = −means that Nci, Ncj
are contextual entities which represent distinct thesis (recognized thanks to the title
criterion) from their “author” point of view. thesis(Nci, Ncj) = −−means that
Nci, Ncj have also been submitted simultaneously.
– thesisAdvisor criterion is a farness-criterion. thesisAdvisor(Nci, Ncj) = −−
(respectively −) means that Nci and Ncj have a same contributor C if and only
if this contributor has supervised a thesis before (respectively two years after) sub-
mitting his/her own thesis.
– date criterion is a farness-criterion. For 100 (respectively 60) years at least between
publication dates, it gives a −−(respectively −) comparison value.
– language criterion is a farness-criterion. When publication languages are distinct
and none of them is English, language returns a −value.
– role criterion returns + when contributor C roles are the same (except for current
roles as “author”, “publishing editor” or “collaborator”), or −when they are distinct
(except for some pairs of roles as “thesis advisor” and “author”).
– domain criterion compares list of domain codes. Domain codes are pair-wise com-
pared. domain(Nci, Ncj) gives closeness (respectively farness) comparison val-
ues if every Nci domain codes is close (respectively far) from a Ncj domain code
and the other way around.
Let us resume global and local semantics before to evaluate their relevance with
respect to the above mentioned criteria on real SUDOC subsets.
3
Partitioning Semantics
Let us summarize partitioning semantics detailed in [11]. A partitioning semantics eval-
uates and compares partitions on a same object set. The following partitioning seman-
tics (in sections 3.1 and 3.2) are based on symbolic criteria.
3.1
Global Semantics
In this section we deﬁne what is a a best partition on the object set O (with respect
to the C criteria set) according to global semantics. A partition has to be valid5[2] in
order to be a best one. A partition P has also an intra value and an inter value per
criterion of C. The intra value of a criterion c depends of the most intense (explained
in section 2.3) farness or never value of c such as it compares two objects in a same
class (should not be the case according to c). In the same way, the inter value of c
depends of the most intense closeness or always value of c such as it compares two
5 A partition P is valid if and only if there is no two objects oi,oj such as: (i) they are in a
same class of P and they never have to be together according to a criterion (expressed by
never comparison value), or (ii) they are in distinct P classes but always have to be together
according to at least a criterion.

An Analysis of the SUDOC Bibliographic Knowledge Base
209
objects in distinct P classes. The inter value measures proximity between classes and
the intra value measures distance between objects in a class [10]. We note that the
neutral comparison value does not inﬂuence partition values.
A partition P on an object set O is a best partition according to a criteria set C if
P is valid and P has a best value, meaning that it is impossible to improve an inter or
intra value of any criterion C ∈C without decreasing inter or intra value of a criterion
C′ ∈C (it is a Pareto equilibrium [15]).
Table 1. Example of objects set
id
title
date domains [...]
appellations
Nc1
“Letter to a Christian nation”
religion
“HARRIS, Sam”
Nc2 “Surat terbuka untuk bangsa kristen” 2008 religion
“HARRIS, Sam”
Nc3 “The philosophical basis of theism” 1883 religion
“HARRIS, Samuel”
Nc4
“Building pathology”
2001 building
“HARRIS, Samuel Y.”
Nc5
“Building pathology”
1936 building
“HARRIS, Samuel Y.”
Nc6
“Aluminium alloys 2002”
2002 physics
“HARRIS, Sam J.”
Example 2 (Global semantics evaluating a partition on an object set O).
Let us represent an object set O = {Nc1, Nc2, Nc3, Nc4, Nc5, Nc6} in table 1.
Each object is a contextual entity and represents a link between a bibliographic notice
and an authorship notice (here, an “author” of a book). Id is the object identity. For each
of them, title, date of publication, publication domain and appellation of the contributor
C are given as attributes.
Nc1 and Nc2 represent a same person, as Nc4, Nc5 does. The human partition on
O is: Ph = {{Nc1, Nc2}, {Nc3}, {Nc4, Nc5}, {Nc6}}. This partition, according to
global semantics and with respect to the criteria set C = {appellation, title, domain,
date} (criteria are detailed in section 2.4) is not coherent with some of C criteria. The
Ph value is such that:
– inter classes domain value is very bad (always) because Nc1 and Nc2 are in
distinct classes but are both about religion.
– intra classes date value is bad (−−) because Nc4 and Nc5 are in a same class, but
with publication dates distant of more than 60 years and less than 100 years.
Ph has a best partition value because increasing an inter or intra criterion value (as
inter domain value by merging {Nc1, Nc2} and {Nc3} classes) is not possible without
decreasing an other inter or intra criterion value (Nc2 and Nc3 have publication dates
distant more than 100 years, so put them in a same class will decrease date intra value).
3.2
Local Semantics
The local semantics, when evaluating a partition on an object set O with respect to
a criteria set C, gives a partition value per parts of O. Parts of O can be coherent or
incoherent. An incoherent part Oa is a subset of O such as:

210
L. Guizol et al.
– there is no c(oi, oj), an always or closeness value with Nci ∈O−Oa, Ncj ∈Oa,
and c ∈C;
– there is no subset of Oa for which the previous property is true;
– there is b(ok, ol), a farness or never value such as ok, ol ∈Oa, and b ∈C.
An incoherent part partition value is based on every comparison between objects which
are in it. The coherent part of an object set O is a O subset containing every O object
which is not in a incoherent part of O. The coherent part partition value of O is based
on every comparison between objects which are not in the same incoherent part of O.
Example 3 (Incoherent and coherent parts).
Let us identify incoherent parts of the object set O according to C given in example
2. Nc1, Nc2, Nc3 are close together due to domain criterion: they are about religion.
Nc1, Nc2, Nc3 are not close to Nc4, Nc5 or Nc6 according to any of C criteria and
Nc2, Nc3 are far according to date criterion (date(Nc2, Nc3) = −−) so {Nc1, Nc2,
Nc3} is an incoherent part of O. The same way, Nc4, Nc5 are close together according
to title and domain criteria, but not close to Nc6. Nc4, Nc5 are also far according to
date criterion (date(Nc4, Nc5) = −) so {Nc4, Nc5} is also an incoherent part.
So, there are 2 incoherent parts in O: {Nc1, Nc2, Nc3} and {Nc4, Nc5}. Nc6 is
not in an incoherent part so Nc6 is in the coherent part of O.
A partition on O is a best partition according to local semantics if it has best partition
values for each incoherent part of O and for the O coherent part.
Example 4 (Local semantics evaluating a partition on an object sets O).
In example 3, we identiﬁed the incoherent parts of the object set O = {Nc1, Nc2,
Nc3, Nc4, Nc5, Nc6} according to the criteria set C = {appellation, title, domain,
date}.
The partition on O given in example 2: is Ph = {{Nc1, Nc2}, {Nc3}, {Nc4, Nc5},
{Nc6}}. According to local semantics, Ph has 3 values, one for the coherent part and
2 for incoherent parts (1 per incoherent part):
– a perfect value for the coherent part of O;
– the incoherent part {Nc1, Nc2, Nc3} has a very bad inter value for the domain
criterion (always);
– the incoherent part {Nc4, Nc5} has an bad intra value for the date criterion (−−);
This semantics enables us to split an object set into several parts which can be eval-
uated separately. We explained local and global semantics in this part, which are a way
to solve the entity resolution problem. Let us evaluate them on a real SUDOC sample.
4
Results
ABES experts have selected 537 contextual entity divided into 7 SUDOC subsets se-
lected for an appellation. The table 2 shows for each SUDOC subset selected for an
appellation A (please see section 2.2):

An Analysis of the SUDOC Bibliographic Knowledge Base
211
1. |Nc| is the number of contextual entities which represent a link between a biblio-
graphic notice and an authorship notice which has a close appellation to A,
2. |Na| is the number of authorities notices according to human partitions (corre-
sponding to class number of human partition),
3. “Ph best” (respectively “Pi best”) shows whether the human partition Ph (respec-
tively initial partition Pi) has a best value according to global semantics and with
respect to all 9 criteria detailed in part 2.4,
4. Ph ≻Pi is true if and only if Ph has a better value than Pi.
Table 2. Human and initial partitions with respect to 9 criteria and global semantics
Appellation
|Nc| |Na| Ph best Pi best Ph ≻Pi Ph’ best Repairs
“BERNARD, Alain”
165
27
no
not valid
yes
yes
“DUBOIS, Olivier”
27
8
no
no
yes
no
1
“LEROUX, Alain”
59
6
no
not valid
yes
yes
“ROY, Michel”
52
9
yes
not valid
yes
yes
“NICOLAS, Maurice” 20
3
yes
no
yes
yes
“SIMON, Alain”
63
13
no
no
yes
no
1
“SIMON, Daniel”
151
16
no
not valid
yes
yes
Local semantics, has the same results than global semantics on this sample.
For global semantics, Pi is never a best partition. 5 times out of 7, Ph does not have
a best value (each time, it is due to the domain and language criteria, and two times
thesisAdvisor is also involved), but it is all the time valid and better than Pi, which
is encouraging for erroneous link detection. Erroneous links are particularly obvious
when Pi is not even valid (4 times out of 7). It is due to the title criterion detailed in
part 2.4. We regret that Ph is not all the time a best partition, but the global semantics
is able to distinguish Pi from Ph in 5 cases out of 7: when Pi is not valid, or when Ph
is a best partition but not Pi.
Because the domain and language criteria often considers that Ph is not a good
enough partition, Ph was also evaluated for global semantics according to all cri-
teria without domain and language (shown in table 2 in column “Ph’ best”) and
that increases the human partition which obtains a best value in 3 more cases (for
“BERNARD, Alain”, “SIMON, Daniel” and “LEROUX, Alain” appellations). This tells
us that domain and language criteria are not reasonably accurate.
In order to evaluate if Ph is far from having a best partition value, we enumerate
the number of repairs to transform Ph′ into a partition Ph′′ which has a best value
according to all criteria except domain and language. We show this repair number in
the “Repairs” column of table 2. An atomic repair could be:
– merging two partition classes (corresponds to merging two contextual entities which
represent a same real word person), or
– splitting a partition class in two classes (corresponds to separate books which are
attributed to a same real word person but belong to two distinct real word persons).

212
L. Guizol et al.
We can see that only a few repairs are needed compared to the number of classes
(corresponding to |Na| column in the table): 1 repair for “DUBOIS, Olivier” and for
“BERNARD, Alain” appellations.
Let us highlight that observing human partition values has permitted to detect and
correct an erroneous link (for “ROY, Michel” appellation) in the human reference set,
validated with experts. The global semantics does not always consider that the human
partition is a best partition, but in the worst case the human partition is very close
to be one according to repairs number, and global semantics allow us to detect that
initial partitions are much worse than human partitions. This last point is encouraging.
This means that the semantics can also be useful to help in criteria tuning, by showing
which criteria are bad according to human partitions, and for which authorship notices
comparison. For example, the fact that the human partition value is often bad according
to the domain criterion shows that this criterion is actually not an accurate criterion.
Let us talk about other entity resolution methods and conclude.
5
Discussion
The entity resolution problem [4][16][14][6] is the problem of identifying as equivalent
two objects representing the same real-world entity. The causes of such mismatch can
be due to homonyms (as in people with the same name), errors that occurred at data
entry (like “Léa Guizo” for “Léa Guizol”), missing attributes (e.g publication date =
XXXX), abbreviations (“L. Guizol”) or attributes having different values for two ob-
jects representing the same entity (change of address).
The entity resolution problem can be addressed as a rule based pairwise comparison
rule approach. Approaches have been proposed in literature [12] using a training pairs
set for learning such rules. Rules can be then be chained using different constraints:
transitivity [3], exclusivity [12] and functional dependencies [1] [9].
An alternative method for entity resolution problem is partitioning (hierarchical par-
titioning [5], closest neighbor-based method [7] or correlation clustering [3]). Our work
falls in this last category. Due to the nature of treating criteria values, the closest ap-
proach to our semantics are [3] and [2]. We distinguish ourself to [3] and [2] because
of (1) the lack of neutral values in these approaches, (2) the numerization of symbolic
values (numerically aggregated into −1 and +1 values), and (3) the use of numerical
aggregation methods on these values.
Conclusion. In this paper we proposed a practical evaluation of the global and local
semantics proposed in [11]. The conclusions of this evaluation are:
– For SUDOC subsets selected by appellation, both semantics are effective to dis-
tinguish a human partition from the initial partition; however it is not perfect with
respect to our set of criteria (if the human partition is not a best partition, it has a
close value).
– Both semantics could be useful to detect meaningless criteria.
As immediate next steps to complete this our work we mention using global or local
semantics to improve implemented criteria.

An Analysis of the SUDOC Bibliographic Knowledge Base
213
Acknowledgements. This work has been supported by the Agence Nationale de la
Recherche (grant ANR-12-CORD-0012). We are thankful to Mickaël Nguyen for his
support.
References
1. Ananthakrishna, R., Chaudhuri, S., Ganti, V.: Eliminating fuzzy duplicates in data ware-
houses. In: Proceedings of the 28th International Conference on Very Large Data Bases,
VLDB 2002, pp. 586–597. VLDB Endowment (2002)
2. Arasu, A., Ré, C., Suciu, D.: Large-scale deduplication with constraints using dedupalog. In:
Proceedings of the 25th International Conference on Data Engineering (ICDE), pp. 952–963
(2009)
3. Bansal, N., Blum, A., Chawla, S.: Correlation clustering 56, 89–113 (2004)
4. Bhattacharya, I., Getoor, L.: Entity Resolution in Graphs, pp. 311–344. John Wiley & Sons,
Inc. (2006)
5. Bilenko, M., Basil, S., Sahami, M.: Adaptive product normalization: Using online learning
for record linkage in comparison shopping. In: Fifth IEEE International Conference on Data
Mining, p. 8. IEEE (2005)
6. Bouquet, P., Stoermer, H., Bazzanella, B.: An entity name system (ens) for the semantic web.
In: Bechhofer, S., Hauswirth, M., Hoffmann, J., Koubarakis, M. (eds.) ESWC 2008. LNCS,
vol. 5021, pp. 258–272. Springer, Heidelberg (2008)
7. Chaudhuri, S., Ganti, V., Motwani, R.: Robust identiﬁcation of fuzzy duplicates. In: Proceed-
ings of the 21st International Conference on Data Engineering, ICDE 2005, pp. 865–876.
IEEE (2005)
8. Croitoru, M., Guizol, L., Leclère, M.: On Link Validity in Bibliographic Knowledge Bases.
In: Greco, S., Bouchon-Meunier, B., Coletti, G., Fedrizzi, M., Matarazzo, B., Yager, R.R.
(eds.) IPMU 2012, Part I. CCIS, vol. 297, pp. 380–389. Springer, Heidelberg (2012)
9. Fan, W.: Dependencies revisited for improving data quality. In: Proceedings of the Twenty-
Seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,
pp. 159–170. ACM (2008)
10. Guénoche, A.: Partitions optimisées selon différents critères: évaluation et comparaison.
Mathématiques et Sciences Humaines. Mathematics and Social Sciences (161) (2003)
11. Guizol, L., Croitoru, M., Leclere, M.: Aggregation semantics for link validity. In: AI-2013:
Thirty-third SGAI International Conference on Artiﬁcial Intelligence (page to appear, 2013)
12. Gupta, R., Sarawagi, S.: Answering table augmentation queries from unstructured lists on
the web. Proceedings of the VLDB Endowment 2(1), 289–300 (2009)
13. Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions and reversals. So-
viet Physics Doklady 10, 707 (1966)
14. Saïs, F., Pernelle, N., Rousset, M.-C.: Reconciliation de references: une approche logique
adaptee aux grands volumes de donnees. In: EGC, pp. 623–634 (2007)
15. Wang, S.: Existence of a pareto equilibrium. Journal of Optimization Theory and Applica-
tions 79(2), 373–384 (1993)
16. Winkler, W.E.: Overview of record linkage and current research directions. Technical report,
BUREAU OF THE CENSUS (2006)

A Fuzzy Extension of Data Exchange
Jes´us Medina1,⋆and Reinhard Pichler2,⋆⋆
1 Department of Mathematics, University of C´adiz, Spain
2 Faculty of Informatics, Vienna University of Technology, Austria
Abstract. Data exchange is concerned with the transformation of data
structured under one schema into a diﬀerent schema. In practice, this
task is usually accomplished in a procedural way. In a landmark pa-
per, Fagin et al. have proposed a declarative, purely logical approach to
this task. Since then, data exchange has been intensively studied in the
database research community. Recently, it has been extended to proba-
bilistic data exchange. In this paper, we propose an extension to fuzzy
data exchange.
1
Introduction
Data exchange is a classical problem in the database world. It is concerned with
the transformation of data structured under one schema into a diﬀerent schema.
In practice, this task is usually accomplished in a procedural way by so-called
ETL (extract-transform-load) scripts. In a landmark paper [9], Fagin et al. have
proposed a declarative, purely logical approach to this task. Since then, data
exchange has been intensively studied in the database research community.
The crucial concept in this approach are schema mappings. A schema mapping
is given by two schemas, called the source schema S and the target schema T,
as well as a set of dependencies describing the relationship between the two
schemas. The most fundamental kind of dependencies are source-to-target tuple
generating dependencies (s-t tgds): these are ﬁrst-order formulas of the form
∀x(φ(x) →∃yψ(x, y)), where the antecedent φ(x) is a conjunctive query (CQ)
over S and the conclusion ψ(x, y) is a CQ over T. Note that a CQ refers to
a (possibly existentially quantiﬁed) conjunction of atomic formulas. Intuitively,
such an s-t tgd deﬁnes a constraint that the presence of certain tuples in the
source database instance I (namely those in the image of some homomorphism
h from φ(x) to I) enforce the presence of certain tuples in the target database
instance J (s.t. h can be extended to a homomorphism from ψ(x, y) to J ).
Two further kinds of dependencies may be used to specify constraints on the
target instance J , namely again tuple-generating dependencies (target tgds) or
equality generating dependencies (target egds). The target tgds are formulas of
the same form as s-t tgds – but now both the antecedent and the conclusion are
⋆Partially supported by the Spanish Science Ministry project TIN2012-39353-C04-04.
⋆⋆Supported by the Austrian Science Fund (FWF):P25207-N23.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 214–223, 2014.
c
⃝Springer International Publishing Switzerland 2014

A Fuzzy Extension of Data Exchange
215
CQs over the target schema T. Target egds are formulas of the form ∀x(φ(x) →
(xi = xj)), where again φ(x) is a CQ over T. Intuitively, egds allow us to express
constraints that the presence of certain tuples in the target instance J enforces
the equality of certain values occurring in the instance.
In summary, a schema mapping is given by a triple M = (S, T, Σ) where S
is the source schema, T is the target schema, and Σ is a set of dependencies
expressing the relationship between S and T and possibly also local constraints
on T. The data exchange problem associated with M is the following: Given
a (ground) source instance I, ﬁnd a target instance J , s.t. I and J together
satisfy all dependencies in Σ, written as ⟨I, J ⟩|= Σ. Such a J is called a
solution for I. Another important computational problem in the area of data
exchange is query answering, where queries are posed against the target schema.
The answers should reﬂect the source instance and the mapping; they should
not depend on the concrete solution that was materialized. Hence, the generally
agreed semantics for query answering in data exchange are certain answers, i.e.,
we aim at those answers which are in the result of the query over every solution
for a given source instance under the given mapping. In case of (unions of)
conjunctive queries, eﬃcient query evaluation is possible via so-called universal
solutions [9], i.e., solutions which admit a homomorphism into any other solution.
Moreover, it was shown in [9] that the so-called chase procedure [6] can be used
to compute a universal solution – provided that any solution exists.
Over the past decade, many extensions of data exchange have been studied
such as more complex queries [1,3], data formats diﬀerent from relational data
[4,2,5], etc. Recently, also an extension to probabilistic data exchange has been
proposed by Fagin et al. [8]. In the latter approach, a probability space over
the source and target instances is considered rather than single (source and
target) instances. The authors extend all of the central concepts of classical
data exchange (such as solutions, universal solutions, the chase, certain answers,
etc.) to a probabilistic setting. Moreover, they provide a detailed complexity
analysis of the basic computational problems in data exchange. It turns out that
query answering in probabilistic data exchange is intractable (more precisely, it
is shown FP#P-complete) even in the most restricted cases.
The goal of this paper is to introduce fuzzy data exchange as another natural
extension of data exchange. Analogously to [8], we shall thus extend all of the
central concepts of classical data exchange to a fuzzy setting. It will turn out
that such an extension is indeed feasible and allows us to carry over the most
fundamental properties from the classical setting.
The paper is organized as follows. In Section 2, we brieﬂy recall some basic
notions. Due to space limitations, many concepts will be later directly presented
in the fuzzy extension without presenting their original classical form ﬁrst. In
Section 3, we introduce fuzzy data exchange and fuzzy solutions. Fuzzy universal
solutions and their computation are dealt with in Section 4. Query answering is
discussed in Section 5. An outlook to future work is given in Section 6.

216
J. Medina and R. Pichler
2
Preliminaries
A schema R = {R1, . . . , Rn} is a ﬁnite set of relation symbols Ri – each of a ﬁxed
arity ri. An instance I over a schema R consists of a relation for each relation
symbol in R, s.t. both have the same arity. For a relation symbol R, we write
RI (or simply R) to denote the relation of R in I. Tuples of the relations may
contain two types of terms: constants and variables. The latter are also called
labelled nulls. For every instance J , we write Dom(J ), Var(J ), and Const(J ) to
denote the set of terms (i.e., the domain), variables, and constants, respectively,
of J . Clearly, Dom(J ) = Var(J ) ∪Const(J ) and Var(J ) ∩Const(J ) = ∅.
We simply write Const and Var to denote the set of all possible constants and
variables, respectively.
Let S = {S1, . . . , Sn} and T = {T1, . . . , Tm} be two schemas with no rela-
tion symbols in common. Then we denote by ⟨S, T⟩the schema obtained by
concatenation of S and T. In the same way, given I and J , instances of S and
T, respectively, the sequence ⟨I, J ⟩is the instance K that satisﬁes SK
i
= SI
i
and T K
j
= T J
j , for all i ∈{1, . . ., n} and j ∈{1, . . ., m}, that is, ⟨I, J ⟩is the
(disjoint) union of I and J .
3
The Fuzzy Data Exchange Problem
Fuzzy Instances. For our extension of data exchange to a fuzzy setting, we
consider a complete lattice (L, ⪯). A fuzzy instance I (over schema R), f-instance
for short, is a set {RI
1 , . . . , RI
k}, such that each RI
i is a ﬁnite relation of arity
ri+1 with RI
i ⊆(Const∪Var)ri ×L for every i. To simplify the notation, we shall
write Ri to denote both, the relation symbol and the relation RI
i . We call I a
fuzzy ground instance if Var(I) = ∅. The set of all f-instances over R is denoted
by FInst(R) and the set of ground instances by FInstc(R). As in classical data
exchange, we shall always assume that source instances are ground.
A tuple (t1, . . . , tr, ϑ) in R is denoted as (R(t1, . . . , tr), ϑ) and it is called
fact. Hence, an f-instance can be identiﬁed by the set of its facts, i.e., I =
{(R1(t1), ϑ1), . . . , (Rk(tk), ϑk)}. The conﬁdence value associated with I is the
value ϑI = ϑ1 & . . . & ϑk.
Fuzzy Homomorphisms. Given two f-instances K1 and K2 over the same
schema, a fuzzy homomorphism h : K1 →K2 is a mapping from Dom(K1) to
Dom(K2), such that
– h(c) = c, for all constants c in Dom(K1),
– for all facts (R(t), ϑ) of K1, a fact (R(h(t)), ϑ′) exists in K2 with ϑ ⪯ϑ′,
where h(t) = (h(t1), . . . , h(tr)). In this case, we write K1 →K2.
Example 1. Consider the f-instances K1 = {(Q(x), 0.5), (R(y), 0.4)} and K2 =
{(Q(a), 0.6), (R(b), 0.4), (P(x), 0.7)}. The mapping h: Dom(K1) →Dom(K2)
with h(x) = a and h(y) = b can be easily veriﬁed to be an f-homomorphism. For
instance, the fact (Q(h(x)), 0.6) = (Q(a), 0.6) is in K2 with 0.5 ≤0.6.
3

A Fuzzy Extension of Data Exchange
217
As in the classical case, the composition of f-homomorphisms is also an f-
homomorphism. Moreover, the extension of an f-homomorphism is naturally
deﬁned: let h : K1 →K2 be an f-homomorphism between two f-instances K1 and
K2, and let K be an f-instance with K1 ⊆K. We call h′ an extension of h if
h′ : K →K2 is an f-homomorphism satisfying that h′ restricted to K1 is h.
The notion of (crisp) homomorphisms is naturally extended to f-instances,
even between an instance and an f-instance. For example, a mapping h from
K1 = {R1, . . . , Rk} to K2 = {(R∗
1, ϑ1), . . . , (R∗
l , ϑl)} is a homomorphism if it
satisﬁes the above conditions of an f-homomorphism ignoring the condition on
the conﬁdence values. Moreover, from a crisp instance K1 = {R1, . . . , Rk} and
a homomorphism h: K1 →K2 we can deﬁne an f-instance Kh
1. In this f-instance
Kh
1, each fact R(t) of K1 is augmented by the conﬁdence value ϑR ∈L such
that ϑR = sup{ϑ ∈L | (R(h(t)), ϑ) ∈K2}. Since h is a homomorphism, we can
be sure that for each R(t) in K1, there exists a fact (R(h(t)), ϑ) ∈K2 for some
ϑ ∈L. We shall write ˆh to denote the resulting f-homomorphism. Hence, we can
say that an f-homomorphism ˆh′ is an extension of a homomorphism h, if it is an
extension of the f-homomorphism ˆh associated with h.
For example, for the instance K1 = {Q(x)}, the f-instance K2 = {(Q(b), 0.5),
(Q(a), 0.6)} and the homomorphism h that maps x to a, we obtain the f-instance
Kh
1 = {(Q(x), 0.6)}.
Fuzzy Dependencies. Analogously to classical data exchange, we consider
schema mappings consisting of source-to-target tuple generating dependencies
(s-t tgds) and target dependencies in the form of tuple generating dependencies
(target tgds) and equality generating dependencies (target egds).
Recall that tgds are ﬁrst-order formulas of the form ∀x(φ(x) →∃yψ(x, y)),
where φ(x) and ψ(x, y) are CQs, and egds are ﬁrst-order formulas of the form
∀x(φ(x) →(xi = xj)), where φ(x) is a CQ. To simplify the notation, the uni-
versal quantiﬁers are usually omitted with the understanding that all variables
occurring in the antecedent are universally quantiﬁed over the entire formula. We
now extend the satisfaction of tgds and egds from crisp instances to f-instances.
Deﬁnition 1. Let R be a schema and K an instance over R. Consider a tgd d
and an egd e over R, i.e.,
d ≡(S1(x) ∧· · · ∧Sn(x)) →∃y(T1(x, y) ∧· · · ∧Tm(x, y)) and
e ≡(S1(x) ∧· · · ∧Sn(x)) →(x1 = x2)
Tgd d is satisﬁed by K (written as K |= d) if every homomorphism h: Id →K
can be extended to an f-homomorphism ˆh′ : Jd →K, where
Id = {S1(x), . . . , Sn(x)} is an instance over R,
ϑIh
d is the conﬁdence value associated with Ih
d , and
Jd = {(T1(x, y), ϑIh
d ), . . . , (Tm(x, y), ϑIh
d )} is an f-instance over R.
Likewise, egd e is satisﬁed by K (written as K |= e) if for every homomorphism
h: Id →K, we have h(xi) ̸= h(xj).
A set Σ of tgds and egds is satisﬁed by K (written as K |= Σ) if every dependency
of Σ is satisﬁed by K.
3

218
J. Medina and R. Pichler
In other words, for the satisfaction of a tgd, we ﬁrst compute the conﬁdence
value ϑIh
d of an f-homomorphism h from the antecedent of d into the f-instance
K and then check that h can be extended to an f-homomorphism ˆh′ from the
conclusion of d into K with at least this conﬁdence value. The satisfaction of an
egd is deﬁned as in the classical case by ignoring the conﬁdence values. As for
the tgd, the above deﬁnition clearly applies to both, s-t tgds and target tgds. In
case of s-t tgds over a source schema S and target schema T, we have to consider
instances K = ⟨I, J ⟩over the combined schema R = ⟨S, T⟩.
Example 2. Given the tgd d = Q(x) →∃yR(y) and
1. the instances I1 = {(P(a), 0.7), (Q(a), 0.5)} and J1 = {(R(b), 0.6)}. Con-
sider the homomorphism h from Id = {Q(x)} to I1 and Ih
d = {(Q(a), 0.5)}.
Clearly, the only extension ˆh′ of h from Jd = {(R(y), 0.5)} to J maps y to
b and 0.5 ≤0.6. Hence, ˆh′ is also an f-homomorphism. Hence, ⟨I1, J1⟩=
{(P(a), 0.7), (Q(a), 0.5), (R(b), 0.6)} satisﬁes the tgd d.
2. the instances I2 = {(Q(a), 0.6)} and J2 = {(R(b), 0.4)}. Given Id = {Q(x)},
the only homomorphism h from Id to I2 maps x to a. However there exists no
extension ˆh′ of h from Jd = {(R(y), 0.6)} to J2, since 0.6 ̸≤0.4. Therefore,
⟨I2, J2⟩= {(Q(a), 0.6), (R(b), 0.4)} does not satisfy the tgd d.
3
In summary, we get the following deﬁnition of fuzzy data exchange.
Deﬁnition 2. A fuzzy data exchange setting is a triple (S, T, Σ) consisting of
a source schema S, a target schema T, a ﬁnite set Σ of dependencies consisting
of s-t tgds, target tgds, and target egds.
The fuzzy data exchange problem associated with this setting is the following:
given a ﬁnite source f-instance I, ﬁnd a ﬁnite target f-instance J such that
⟨I, J ⟩satisﬁes Σ. In this case, J is called a fuzzy solution for I, f-solution for
short. The set of all solutions for I is denoted by FSol(I).
3
Example 3. Let S = {P, Q}, T = {R} and Σ = {d}, where d is an s-t tgd of the
form d = Q(x) →∃yR(y). The triple (S, T, Σ) is a fuzzy data exchange setting.
In Example 2, it is easy to verify that J1 is an f-solution for the source f-instance
I1 while J2 is not an f-solution for I2.
3
4
Universal Solutions
Fuzzy Universal Solutions. In classical data exchange, universal solutions
are crucial for the evaluation of conjunctive queries over the target schema. We
now deﬁne fuzzy universal solutions by making use of the extension from crisp
homomorphisms to f-homomorphisms introduced in the previous section.
Deﬁnition 3. Given a fuzzy data exchange setting (S, T, Σ) and a source f-
instance I, a fuzzy universal solution of I (universal f-solution, for short) is
an f-solution J of I such that, for every f-solution J ′ for I, there exists an
f-homomorphism h : J →J ′.
3

A Fuzzy Extension of Data Exchange
219
Example 4. Considering the fuzzy data exchange problem in Examples 2 and 3,
it can be easily veriﬁed that Ju = {R(Y ), 0.5}, where Y is a labelled null, is a
universal f-solution for I1.
3
Fuzzy Chase. In classical data exchange, the so-called chase procedure [6] is
used to decide if, for a data exchange setting and a given source instance I, a
solution J of I exists. We now show how the chase can be extended so as to be
applicable to fuzzy data exchange.
Deﬁnition 4. Consider an f-instance K together with the tgd d and egd e of the
following form.
d ≡(S1(x) ∧· · · ∧Sn(x)) →∃y(T1(x, y) ∧· · · ∧Tm(x, y)) and
e ≡(S1(x) ∧· · · ∧Sn(x)) →(x1 = x2)
Then the application of tgd d or of egd e to K is deﬁned as follows.
(tgd) Let h be a homomorphism h: Id →K which cannot be extended to an
f-homomorphism ˆh′ : Jd →K, where
Id = {S1(x), . . . , Sn(x)} is an instance over R,
ϑIh
d is the conﬁdence value associated with Ih
d , and
Jd = {(T1(x, y), ϑIh
d ), . . . , (Tm(x, y), ϑIh
d )} is an f-instance over R.
Then we say that d can be applied to K with homomorphism h. In this case,
we extend K to K′ as follows:
1. extending h to h∗over Var(Jd) such that each variable in y is assigned
a new labelled null, and
2. extending K to K′ by the image of the atoms of the conclusion of the
tgd d under h∗together with weight ϑIh
d , that is (Tj(h∗(x, y)), ϑIh
d ), with
j ∈{1, . . ., m}.
The result of applying d to K with h is K′ and will be denoted as K
d,h
→K′
(egd) Let h be a homomorphism h: Id →K with Id = {S1(x), . . . , Sn(x)},
such that h(xi) ̸= h(xj) holds. Then we say that e can be applied to K with
homomorphism h and we need to consider two possibilities:
1. If h(x1) and h(x2) are constants, then we obtain a “failure” denoted as
K
d,h
→⊥.
2. Otherwise, at least one of h(xi) and h(xj) is a labelled null, say h(xi).
Then we obtain the f-instance K′ by replacing every occurrence of h(xi)
in K by h(xj). In this case, the result of applying e to K with h is K′
and will be denoted as K
e,h
→K′.
The application of tgd d or egd e to K is called a fuzzy chase step.
3
Deﬁnition 5. Consider an f-instance K and a set Σ of tgds and egds. If we
apply several fuzzy chase steps with dependencies from Σ to K, we obtain a
fuzzy chase sequence. When, after a ﬁnite chase sequence K = K0
d1,h1
−→K1 →

220
J. Medina and R. Pichler
· · · →Km−1
dm,hm
−→
Km, no more fuzzy chase step can be applied to the last f-
instance Km or we get a “failure”, that is, Km = ⊥, then we say that Km is the
result of the ﬁnite chase. We distinguish between a successful ﬁnite chase in the
ﬁrst case and a failing ﬁnite chase in the second case.
3
In the presence of target tgds, there is no guarantee that the chase terminates.
Therefore, Fagin et al. [9] introduced a simple syntactic criterion (so-called “weak
acyclicity”) on the set of target tgds to ensure termination of the chase.
Example 5. Consider the fuzzy data exchange setting (S, T, Σ) with S = {P, Q},
T = {R} and Σ = {d1, d2}, such that d1 ≡P(x) →R(x, x), d2 ≡Q(x) →
∃yR(x, y). We compute a solution for I = {(P(a), 0.6), (Q(a), 0.5)} using the
fuzzy chase.
Initially, we set K = ⟨I, J ⟩with J = ∅. Suppose that we begin with
tgd d2. Analogously to the previous example we obtain the f-instance K′ =
{(P(a), 0.5), (Q(a), 0.5), (R(a, Y ), 0.5)}.
Next, we consider the other tgd. There is the homomorphism h from Kd1 =
{P(x)} to K′ which maps x to a. Hence, Kh
d1 = {(P(a), 0.6)} and ϑKh
d1 = 0.6.
There is no extension from {(R(x, x), 0.6)} to K′, since R(h(x), h(x)) = R(a, a)
is not a fact of K. Therefore, we extend K′ to K′′ = {(P(a), 0.5), (Q(a), 0.6),
(R(a, Y ), 0.5), (R(a, a), 0.6)}, which satisﬁes all dependencies in Σ. Hence, we
get the solution J = K′′ \ I = {(R(a, Y ), 0.5), (R(a, a), 0.6)} of I.
Alternatively, we could start the fuzzy chase by applying tgd d1, which
yields the instance K′ = {(P(a), 0.5), (Q(a), 0.6), (R(a, a), 0.6)}. Considering
now the tgd d2, the only homomorphism h from Kd2 = {Q(x)} to K′ maps
x to a. Now we need to check whether there exists an extension ˆh′ of h from
{(Q(a), 0.6), (R(a, y), 0.5)} to K′. Indeed, such an extension (mapping y to a
and observing 0.5 ≤0.6) exists. We therefore conclude that K′ = {(P(a), 0.5),
(Q(a), 0.6), (R(a, a), 0.6)} satisﬁes all dependencies in Σ and J = K′ \ I =
{(R(a, a), 0.6)} is a solution of I.
3
In the above example, the chase sequence starting with tgd d1 leads to a smaller
solution than the chase sequence starting with d2. In other words, the chase
sequence starting with d2 introduces a redundancy. In this simple example, the
redundancy could be avoided by the “right” choice of the chase sequence. How-
ever, Fagin et al. [10] showed that, in general, the introduction of redundancies
by the chase cannot be avoided – no matter in which order the tgds and egds
are applied. Therefore, Fagin et al. proposed the redundancy elimination as a
post-processing procedure by reducing the chase result J to the so-called core.
Formally, we thus search for a proper subinstance J ′ ⊂J with J →J ′ (i.e., J ′
is a proper subinstance of J and there exists a homomorphism from J to J ′).
The core J ∗of an instance J is a subinstance that cannot be further “shrunk” by
a proper endomorphism, i.e., there does not exist a proper subinstance ˆ
J ⊂J ∗
such that J →ˆ
J . The core is unique up to isomorphism. In [11] it was shown
that the core can be computed in polynomial time (data complexity) for schema
mappings consisting of s-t tgds, a weakly acyclic set of target tgds, and egds.

A Fuzzy Extension of Data Exchange
221
Extending this sophisticated core computation algorithm to our fuzzy setting is
left for future work.
Deciding the Data Exchange Problem. In the classical setting, it was shown
that the chase can be used to decide the data exchange problem [9], i.e., if the
chase fails, we may conclude that no solution at all exists; if the chase succeeds,
it ends with a universal solution. Below we carry this result over to the fuzzy
setting. As in the classical case, we start with the following crucial lemma:
Lemma 1. Given a fuzzy chase step K1
d,h
−→K2, where K2 ̸= ⊥and d is either
a tgd or an egd. If K is an f-instance such that:
– K satisﬁes d and
– there exists an f-homomorphism h1 : K1 →K,
then there exists an f-homomorphism h2 : K2 →K.
With the previous lemma, the main result of this section is obtained by an
easy induction argument.
Theorem 1. Given a fuzzy data exchange setting (S, T, Σ), where Σ consists
of s-t tgds, a weakly acyclic set of target tgds and target egds. Let I be a source
instance and let K be the result of a ﬁnite chase of ⟨I, ∅⟩with Σ. Then we have:
1. If K ̸= ⊥, then there exists an f-instance J , such that K = ⟨I, J ⟩, and J is
a fuzzy universal solution.
2. If K = ⊥, then there exists no fuzzy solution for I.
Proof: The proof is analogous to the one given in [9]. For instance, consider the
case K ̸= ⊥. The result of a successful ﬁnite chase of course is a fuzzy solution. It
remains to show that it is a fuzzy universal solution. Let J ′ be another solution.
We show that after every chase step we have an instance Ki = ⟨I, Ji⟩such that
there exists an f-homomorphism Ki →K′ with K′ = ⟨I, J ′⟩. Since source and
target schema are disjoint, it follows that there is an f-homomorphism from Ji
to J ′. Initially, we have K0 = ⟨I, J0⟩where J0 = ∅. Here, the identity is an
f-homomorphism J0 to J ′. We prove this property for every i by induction on
the length of the chase sequence, applying Lemma 1 for the induction step.
2
5
Query Answering
In data exchange, the semantics of query answering is given by the notion of
certain answers, i.e., for a given source instance I and query q, we are interested
in those tuples that are obtained by evaluating q over target instance J for every
solution J of I. In fuzzy data exchange, we consider weighted k-ary queries with
k ≥0, i.e., pairs (q(x), λ), where x is the k-ary vector of free variables in q and
λ ∈L. Below, we restrict ourselves to the most important class of queries, namely
conjunctive queries (CQs), i.e., q(x) is of the form ∃yR1(x, y) ∧· · · ∧Rm(x, y).

222
J. Medina and R. Pichler
For an f-instance J and a weighted k-ary CQ (q(x), λ), one important set is
q(J , λ). For k > 0 this set is formed by the k-tuples t, such that there exists
an f-homomorphism h from Kd = {R1(x, y) ∧· · · ∧Rm(x, y)} to J with the
properties that (1) the conﬁdence value associated with Kh
d is ϑKh
d; (2) we have
λ ⪯ϑKh
d; and (3) h(x) = t. For k = 0, the only possible answer tuple is the
empty tuple ⟨⟩and, therefore, q(J , λ) is either {⟨⟩} or ∅. In the former case,
we say q(J , λ) is true and in the latter case, q(J , λ) is false. Once the meaning
of a weighted query and q(J , λ) have been introduced, the deﬁnition of certain
answers in fuzzy data exchange can be given.
Deﬁnition 6. Given a fuzzy data exchange setting (S, T, Σ), a weighted k-ary
query q(x, λ) over the target schema T, a source f-instance I and a conﬁdence
value λ ∈L. The set of certain answers of q(x) with respect to I at level λ,
denoted by certain(q(x), I, λ), is deﬁned as follows
– If k > 0, certain(q(x), I, λ) is the set of all k-tuples t of constants from I
such that for every solution J of I, we have that t ∈q(J , λ).
– If k = 0, then certain(q(x), I, λ) = true if q(J , λ) = true for all solutions J
of I, and otherwise certain(q(x), I, λ) = false.
3
For k > 0, an interesting subset of q(J , λ) is formed by those k-tuples t which
consist of constants only (i.e., no labelled nulls). This subset of q(J , λ) will be
denoted as q(J , λ)↓. For Boolean queries q, we simply have q(J , λ)↓= q(J , λ)
(hence, true or false). In classical data exchange, universal solutions can be used
to compute the certain answers of conjunctive queries. This result naturally
extends to fuzzy data exchange. We thus get the following theorem.
Theorem 2. Given a fuzzy data exchange setting (S, T, Σ), a source instance
I, and a weighted CQ (q(x), λ) over the target schema T. Let J be an arbitrary
universal f-solution of I. Then certain(q(x), I, λ) = q(J , λ)↓.
Moreover, if J is a fuzzy solution of I such that, for every weighted CQ
(q(x), λ) over T the equality certain(q(x), I, λ) = q(J , λ)↓holds, then J is an
f-universal solution of I.
Proof: The proof is analogous to the proof of the corresponding theorem in clas-
sical data exchange [9].
2
6
Conclusion and Future Work
In this article, we have extended all of the basic concepts of data exchange to a
fuzzy setting. We have thus shown that data exchange can be nicely extended
to fuzzy data exchange. A lot of work remains to be done in this area.
First, further properties of fuzzy data exchange should be investigated. This
line of research includes, for instance, the study of the relationship with proba-
bilistic data exchange and a detailed analysis of the algorithmic aspects of fuzzy
data exchange: above all, the complexity of query answering. Second, our exten-
sion of classical data exchange to a fuzzy setting should include further concepts

A Fuzzy Extension of Data Exchange
223
and methods. We have already mentioned the computation of the so-called core
[10,11] to get a minimal solution as one interesting task for future work. In [8],
after extending classical data exchange to probabilistic data exchange by con-
sidering probability spaces of source and target instances, also the mappings
themselves are considered as probabilistic. Analogously, we should investigate
the extension of fuzzy exchange as presented here to fuzzy dependencies, i.e.,
also the tgds and egds have to be augmented with conﬁdence values [7,12].
References
1. Afrati, F.N., Kolaitis, P.G.: Answering aggregate queries in data exchange. In:
PODS, pp. 129–138. ACM (2008)
2. Arenas, M., Barcel´o, P., Libkin, L., Murlak, F.: Relational and XML Data Ex-
change. Morgan & Claypool Publishers (2010)
3. Arenas, M., Barcel´o, P., Reutter, J.L.: Query languages for data exchange: beyond
unions of conjunctive queries. In: ICDT, pp. 73–83. ACM (2009)
4. Arenas, M., Libkin, L.: Xml data exchange: Consistency and query answering. J.
ACM 55(2) (2008)
5. Barcel´o, P., P´erez, J., Reutter, J.L.: Schema mappings and data exchange for graph
databases. In: ICDT, pp. 189–200. ACM (2013)
6. Beeri, C., Vardi, M.Y.: A proof procedure for data dependencies. J. ACM 31(4),
718–741 (1984)
7. Viegas Dam´asio, C., Moniz Pereira, L.: Monotonic and residuated logic programs.
In: Benferhat, S., Besnard, P. (eds.) ECSQARU 2001. LNCS (LNAI), vol. 2143,
pp. 748–759. Springer, Heidelberg (2001)
8. Fagin, R., Kimelfeld, B., Kolaitis, P.G.: Probabilistic data exchange. J. ACM 58(4),
15 (2011)
9. Fagin, R., Kolaitis, P.G., Miller, R.J., Popa, L.: Data exchange: semantics and
query answering. Theor. Comput. Sci. 336(1), 89–124 (2005)
10. Fagin, R., Kolaitis, P.G., Popa, L.: Data exchange: getting to the core. ACM Trans.
Database Syst. 30(1), 174–210 (2005)
11. Gottlob, G., Nash, A.: Eﬃcient core computation in data exchange. J. ACM 55(2)
(2008)
12. Medina, J., Ojeda-Aciego, M., Vojt´aˇs, P.: Similarity-based uniﬁcation: a multi-
adjoint approach. Fuzzy Sets and Systems 146, 43–62 (2004)

Fuzzy Relational Compositions Based
on Generalized Quantiﬁers
Martin ˇStˇepniˇcka and Michal Holˇcapek
Institute for Research and Applications of Fuzzy Modeling
University of Ostrava
Centre of Excellence IT4Innovations
30. dubna 22, 701 03 Ostrava 1, Czech Republic
{Martin.Stepnicka,Michal.Holcapek}@osu.cz
http://irafm.osu.cz/
Abstract. Fuzzy relational compositions have been extensively studied
by many authors. Especially, we would like to highlight initial studies
of the fuzzy relational compositions motivated by their applications to
medical diagnosis by Willis Bandler and Ladislav Kohout. We revisit
these types of compositions and introduce new deﬁnitions that directly
employ generalized quantiﬁers. The motivation for this step is twofold:
ﬁrst, the application needs for ﬁlling a huge gap between the classi-
cal existential and universal quantiﬁers and second, the already existing
successful implementation of generalized quantiﬁers in so called divisions
of fuzzy relations, that constitute a database application counterpart of
the theory of fuzzy relational compositions. Recall that the latter topic is
studied within fuzzy relational databases and ﬂexible querying systems
for more than twenty years. This paper is an introductory study that
should demonstrate a unifying theoretical framework and introduce that
the properties typically valid for fuzzy relational compositions are valid
also for the generalized ones, yet sometimes in a weaken form.
1
Introduction
Fuzzy relational compositions are widely used in many areas of fuzzy mathemat-
ics, including the formal constructions of fuzzy inference systems [1, 2] , medical
diagnosis [3] or architectures for information processing and protection of IT
systems [4]. Since late 70’s and early 80’s when Willis Bandler and Ladislav Ko-
hout studied classical relational compositions and extended the concept in order
to deal with fuzzy relational compositions, these area became deeply elaborated
by numerous researchers. Let us recall mainly Radim Bˇelohl´avek’s book [5], an
article by Bernard De Baets and Etienne Kerre [6] and ﬁnally an exhaustive
investigation in the so called Fuzzy Class Theory [7] by Libor Bˇehounek and
Martina Daˇnkov´a [8].
As one may ﬁnd in the very early articles [3, 9], the fuzzy relational composi-
tions came to live as a very natural generalization of well motivated compositions
of classical relations. For example, the basic sup-T compositions is nothing else
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 224–233, 2014.
c
⃝Springer International Publishing Switzerland 2014

Fuzzy Relational Compositions
225
but a generalization of the basic composition of two classical binary relations.
It is suﬃcient to consider binary fuzzy relations and to deal with the operations
that serve as interpretations of fuzzy connectives, which involves the t-norm T
in the formula that gave rise to the notion “sup-T composition”. However, what
remained untouched and up to the best knowledge of the authors, never gen-
eralized so far, is the nature of the quantiﬁers that are used in the deﬁnitions
of the compositions. Particularly, sup-T compositions that use the operation of
supremum, implicitly employ the existential quantiﬁer while inf-R compositions
that use the operation of inﬁmum, implicitly employ the universal quantiﬁer.
The fact that there is nothing in between the option of the existential quantiﬁer
where just one element is enough to result the truth and the other option of the
universal quantiﬁer where all elements have to fulﬁll a given formula in order to
result the truth, may be very limiting in distinct applications. Therefore, the in-
troduction of fuzzy relational compositions based on generalized quantiﬁers, such
as ’Most’ or ’Many’, is a well motivated natural step. This has been noticed by
many authors who deal with so called ﬂexible query answering systems or simply
fuzzy relational databases where the use of so called soft quantiﬁers in evaluation
of quantiﬁed sentences or mainly in fuzzy relational divisions attracted a great
interest of many scholars, see e.g. [10–13].
2
Relational Compositions and Fuzzy Relational
Compositions
2.1
Relational Compositions
Let us consider three non-empty ﬁnite universes X, Y, Z of elements. Following
the work of Willis Bandler and Ladislav Kohout [3], for the sake of illustrative
nature, we can assume that X is a ﬁnite set of patients, Y is a ﬁnite set of
symptoms and Z is a ﬁnite set of diseases.
Let us be given two binary relations R ⊆X × Y and S ⊆Y × Z, i.e.,
if a pair (x, y) ∈X × Y belongs to relation R then it means that patient x
has symptom y and similarly, if a pair (y, z) ∈Y × Z belongs to relation S
then it means that symptom y belongs to disease z. Both relations are usually
at disposal since R can be easily obtained by asking patients or by measuring
symptoms (body temperature, cholesterol, blood pressure etc.) and S constitutes
an expert medical knowledge that is at disposal e.g. in literature. The usual
diagnosis task of a physician is from the mathematical point of view nothing else
but a composition of these two relation in order to obtain a relation between
patients and diseases. In order words, to state what are the potential diseases of
a given patient. Obviously, formally, a similar job may be done by a relational
composition @ which gives a binary relation R ◦S on X × Z:
R
⊆X × Y
S
⊆
Y × Z
R@S ⊆X
× Z.

226
M. ˇStˇepniˇcka and M. Holˇcapek
We will consider the four main compositions @ and denote them as [4] by
◦, ◁, ▷and □. The composed relations are given as follows
R ◦S = {(x, z) ∈X × Z | ∃y ∈Y : (x, y) ∈R & (y, z) ∈S},
(1)
R ◁S = {(x, z) ∈X × Z | ∀y ∈Y : (x, y) ∈R ⇒(y, z) ∈S},
(2)
R ▷S = {(x, z) ∈X × Z | ∀y ∈Y : (x, y) ∈R ⇐(y, z) ∈S},
(3)
R □S = {(x, z) ∈X × Z | ∀y ∈Y : (x, y) ∈R ⇔(y, z) ∈S}
(4)
and they are called basic (circlet) composition, Bandler-Kohout subproduct,
Bandler-Kohout superproduct and Bandler-Kohout square product, respectively.
The relation R◦S then expresses a sort of suspicion of a disease for a particular
patient. It is a basic relation – for each patient it is suﬃcient to have only a single
symptom related to a particular disease in order to detect the suspicion. Thus, a
patient having a very general symptom related to many diseases is immediately
suspicious of having all these diseases.
The “triangle” and square compositions (2)-(4) provide a sort of more accurate
speciﬁcation or a strengthening of the initial suspicion [3]. The Bandler-Kohout
(abb. BK) subproduct is deﬁned as a relation of patients and diseases such that,
for all symptoms that a given patient has, it holds, that they belong to the given
disease. The BK superproduct is deﬁned as a relation of patients and diseases
such that, for all symptoms that belong to a given disease, it holds, that the given
patient has them. Finally, the BK square product models an ideal example when
a given patient has all the symptoms of a given disease and all the symptoms of
the patient belong to the given disease.
Using the fact that the existential and universal quantiﬁers may be interpreted
by the operations of supremum and inﬁmum, respectively, formulas (1)-(4) may
be rewritten into the following functional form:
χ(R◦S)(x, z) =
)
y∈Y
(χR(x, y) ∧χS(y, z)) ,
(5)
χ(R◁S)(x, z) =
*
y∈Y
(χR(x, y) ⇒χS(y, z)) ,
(6)
χ(R▷S)(x, z) =
*
y∈Y
(χR(x, y) ⇐χS(y, z)) ,
(7)
χ(R □S)(x, z) =
*
y∈Y
(χR(x, y) ⇔χS(y, z)) ,
(8)
where χR, χS and χ(R@S) denote characteristic functions of relations R, S and
R@S, respectively, symbol ∧denotes the minimum, symbol ⇒expresses the
binary operation of the classical implication and ﬁnally, symbol ⇔denotes the
operation of the classical equivalence.
2.2
Compositions of Fuzzy Relations
Since usual symptoms such as high temperature, increased cholesterol or very
high blood pressure are basically vaguely speciﬁed and imprecisely measured

Fuzzy Relational Compositions
227
(all these values oscillate during a day) and very often some symptoms do not
clearly or necessarily belong to a given disease however, they might belong to
it under some assumptions or conditions, the extension of the compositions for
fuzzy relations R ⊂∼X × Y and S ⊂∼Y × Z was highly desirable. Obviously,
since such an extension causes that we deal with fuzzy relations which contain
pairs of elements up to some degrees from the interval [0, 1], we have to take
into account appropriate operations. Basically, it is appropriate to deal with a
residuated lattice as the underlying algebraic structure and the used operations
will be left-continuous t-norms and their residual (bi)implications [14].
In this article, we only brieﬂy recall the basic deﬁnitions of the fuzzy relational
compositions as introduced by Willis Bandler and Ladislav Kohout.
Deﬁnition 1. Let X, Y, Z be non-empty universes, let R ⊂∼X × Y , S ⊂∼Y × Z
and let →be a residual implication derived from a left-continuous t-norm ∗.
Then the compositions ◦, ◁, ▷, □of fuzzy relations R and S are fuzzy relations
on X × Z deﬁned as follows:
(R ◦S)(x, z) =
)
y∈Y
(R(x, y) ∗S(y, z)) ,
(R ◁S)(x, z) =
*
y∈Y
(R(x, y) →S(y, z)) ,
(R ▷S)(x, z) =
*
y∈Y
(R(x, y) ←S(y, z)) ,
(R □S)(x, z) =
*
y∈Y
(R(x, y) ↔S(y, z)) ,
for all x ∈X and z ∈Z.
Since ∗is a t-norm, often denoted by the capital T, the sup-∗composition
is also called the sup-T composition. Similarly, the Bandler-Kohout products,
since being constructed with help of the inﬁmum and the residual operation, are
called inf-R compositions or more speciﬁcally, inf-→, inf-←or inf-↔.
Remark 1. Note, that for x ∈X such that R(x, y) = 0 for all y ∈Y , the com-
posed relation (R ◁S)(x, z) = 1 for any z ∈Z. Similarly, for z ∈Z such that
S(y, z) = 0 for all y ∈Y , the composed relation (R▷S)(x, z) = 1 for any x ∈X.
Bernard De Baets and Etienne Kerre in [6] approached this problem by a redef-
inition of the original inf-R compositions where an existence of joining element
(symptom) y ∈Y is assumed. In this preliminary investigation we stay stuck to
the original deﬁnitions and we leave the investigation of the later modiﬁcation
for further studies.
As we may see from Deﬁnition 1, the generalizations focus on the use of fuzzy
relations and the internal operations only. However, the deﬁnitions still deal
with the supremum and the inﬁmum and thus, inherently use the existential
and universal quantiﬁers, respectively. It is obvious that this may be a potential

228
M. ˇStˇepniˇcka and M. Holˇcapek
drawback for applications as there is big gap between these quantiﬁers. Par-
ticularly, we can meet situations when all patients are suspicious of having all
diseases in a high degree when using ◦, but if we want to strengthen the suspi-
cion with help of □, no patients are suspicious of having any disease in a high
degree anymore because not for all but only for majority of symptoms, the inside
implication present in the deﬁnitions of inf-R compositions are valid.
3
Generalized Quantiﬁers
3.1
Generalized Quantiﬁers Based on Fuzzy Measures
In the above sections, we have recalled relational compositions and fuzzy rela-
tional compositions. Motivated by the theoretical drawback that has been ob-
served by many researchers and “solved” by the use of generalized quantiﬁers,
see e.g. [12, 13], we directly employ the so called monadic quantiﬁers of the
type ⟨1⟩determined by fuzzy measures [15]. First of all, let us recall some basic
deﬁnitions.
Deﬁnition 2. Let U = {u1, . . . , un} be a ﬁnite universe, P(U) denote the power
set of U and μ : P(U) →[0, 1] be a normalized fuzzy measure, i.e., a monotone
mapping with μ(∅) = 0 and μ(U) = 1. We say that the fuzzy measure μ is
invariant with respect to cardinality, if the following condition holds:
∀A, B ∈P(U) : |A| = |B| ⇒μ(A) = μ(B)
where | · | denotes the cardinality of a set.
Example 1. The measure called relative cardinality, given by
μrc(A) = |A|
|U| ,
(9)
is invariant w.r.t. cardinality. If f : [0, 1] →[0, 1] is a non-decreasing mapping
with f(0) = 0 and f(1) = 1 then μ deﬁned as μ(A) = f(μrc(A)) is again a fuzzy
measure that is invariant w.r.t. cardinality.
Note, that all the models of linguistic evaluative expressions [16] of the type
Big and modiﬁed by arbitrary linguistic hedge (e.g. More or less, Very,
Roughly, Extremely
etc.) are fuzzy sets on [0, 1] that fulﬁll the boundary
conditions and thus, may be used in order to modify the original relative
cardinality.
In the sequel, we will deal only with such fuzzy measures that are created by a
modiﬁcation of the relative cardinality by an appropriate fuzzy set (cf. Deﬁnition
3.7 in [15]).
Deﬁnition 3. Let U be non-empty ﬁnite universe and μ be a fuzzy measure on
U that is invariant w.r.t. cardinality. A mapping Q : F(U) →[0, 1] deﬁned by
Q(C) =
)
D∈P(U)\{∅}


 *
u∈D
C(u)

∗μ(D)

,
C ∈F(U)
(10)

Fuzzy Relational Compositions
229
where ∗is a left-continuous t-norm, is called fuzzy quantiﬁer determined by
fuzzy measure μ.
Example 2. Let us assume that the fuzzy measures μ deﬁned as follows
μ∀(D) =

1
D ≡U
0
otherwise,
μ∃(D) =

0
D ≡∅
1
otherwise.
(11)
Then the derived quantiﬁers are the classical universal and existential quantiﬁers.
One can immediately see, that formula (10) is not very appropriate from
the computational point of view as it requires calculation over all sets from
P(U) \ {∅}. However, we may use the property of fuzzy measure being invariant
w.r.t. cardinality and show that the fuzzy quantiﬁer may be eﬃciently calculated.
Theorem 1. Let Q be a fuzzy quantiﬁer on U determined by a fuzzy measure
μ that is invariant w.r.t. cardinality. Then
Q(C) =
n
)
i=1
C(uπ(i)) ∗μ({u1, . . . , ui}),
C ∈F(U)
(12)
where π is a permutation on U such that C(uπ(1)) ≥C(uπ(2)) ≥· · · ≥C(uπ(n)).
Proof. Let C be an arbitrary fuzzy set. It is easy to see that, for any i = 1, . . . , n
and D ∈P(U) with |D| = i, it holds
C(uπ(i)) =
*
u∈{uπ(1),...,uπ(i)}
C(u) ≥
*
u∈D
C(u).
The statement immediately follows from the invariance of μ w.r.t. cardinality. □
Theorem 1 shows that the fuzzy quantiﬁer deﬁned by Deﬁnition 3 can be
equivalently expressed by means of the Sugeno fuzzy integral, which is neither
surprising nor undesirable fact, cf. [17].
In other words, if we again apply the fuzzy measure μf that is constructed
from the relative cardinality by some modifying fuzzy set f, i.e. μf = f(μrc),
then formula (12) turns into the following equality:
Q(C) =
n)
i=1
C(uπ(i)) ∗f(i/n)
(13)
which is very easy to be calculated.
3.2
Fuzzy Relational Compositions Based on Generalized
Quantiﬁers
In this part of the text, we directly apply the above introduced theory of gener-
alized quantiﬁers to our problem of fuzzy relational compositions.

230
M. ˇStˇepniˇcka and M. Holˇcapek
Let us recall, e.g., the deﬁnition of the Bandler-Kohout subproduct of two
classical relations R and S given by formula (2). The universal quantiﬁer ∀may
be replaced by a generalized quantiﬁer, say, e.g., the quantiﬁer ’Most’. Then
the modiﬁed composition gives a set of pairs of patients and diseases such that
for most of the symptoms, that a given patient has, it holds, that they belong
to the given disease.
Deﬁnition 4. Let X, Y, Z be non-empty ﬁnite universes, let R ⊂∼X × Y , S ⊂∼
Y × Z, let ∗be a left-continuous t-norm and →be its residual implication. Let μ
be a fuzzy measure on Y that is invariant w.r.t. cardinality and let Q be a fuzzy
quantiﬁer on Y determined by the fuzzy measure μ. Then the ◦Q, ◁Q, ▷Q, □Q
compositions of fuzzy relations R and S are fuzzy relations on X × Z deﬁned as
follows:
(R ◦Q S)(x, z) =
)
D∈P(Y )\{∅}
⎛
⎝
⎛
⎝*
y∈D
R(x, y) ∗S(y, z)
⎞
⎠∗μ(D)
⎞
⎠,
(R ◁Q S)(x, z) =
)
D∈P(Y )\{∅}
⎛
⎝
⎛
⎝*
y∈D
R(x, y) →S(y, z)
⎞
⎠∗μ(D)
⎞
⎠,
(R ▷Q S)(x, z) =
)
D∈P(Y )\{∅}
⎛
⎝
⎛
⎝*
y∈D
R(x, y) ←S(y, z)
⎞
⎠∗μ(D)
⎞
⎠,
(R □Q S)(x, z) =
)
D∈P(Y )\{∅}
⎛
⎝
⎛
⎝*
y∈D
R(x, y) ↔S(y, z)
⎞
⎠∗μ(D)
⎞
⎠,
for all x ∈X and z ∈Z.
Though the deﬁnition is general and enables to use any quantiﬁer Q, obviously,
for application purposes, when dealing with the inf-R compositions ◁Q, ▷Q, □Q
quantiﬁers weakening the universal quantiﬁer such as ’Most’ or ’Many’ are ex-
pected to be applied. These quantiﬁers may be applied for example using the
fuzzy sets modeling the meaning of evaluative linguistic expression Very Big or
Roughly Big [16]. In the case of sup-T composition ◦Q, we should apply quan-
tiﬁers that slightly strengthen the expectations from the existential quantiﬁers,
such as ’A Few’ that may be modeled by a fuzzy sets representing the meaning
of the expression Not Very Small.
Corollary 1. Let μ be a fuzzy measure that is constructed from the relative
cardinality by the modiﬁcation using function f. Then for all x ∈X and z ∈Z:
(R ◦Q S)(x, z) =
n
)
i=1

R(x, yπ(i)) ∗S(yπ(i), z)

∗f(i/n)

,
(R ◁Q S)(x, z) =
n
)
i=1

R(x, yπ(i)) →S(yπ(i), z)

∗f(i/n)

,

Fuzzy Relational Compositions
231
(R ▷Q S)(x, z) =
n
)
i=1

R(x, yπ(i)) ←S(yπ(i), z)

∗f(i/n)

,
(R □Q S)(x, z) =
n
)
i=1

R(x, yπ(i)) ↔S(yπ(i), z)

∗f(i/n)

,
where π is a permutation such that (putting ⊛∈{∗, →, ←, ↔})
R(x, yπ(i)) ⊛S(yπ(i), z) ≥R(x, yπ(i+1)) ⊛S(yπ(i+1), z),
i = 1, . . . , n −1.
Indeed, the original compositions are special cases of the newly deﬁned ones.
Using the fuzzy measure μ∀given by (11), one may easily check that R ◁S ≡
R ◁∀S and similarly that R ▷S ≡R ▷∀S, R □S ≡R □∀S. Indeed, since
f(i/n) = 0 for all i < n and f(1) = 1 then
(R ◁∀S)(x, z) =

R(x, yπ(n)) →S(yπ(n), z)

∗f(n/n)
which due to the fact that
R(x, yπ(n)) →S(yπ(n), z) =
n
*
i=1
(R(x, yi) →S(yi, z))
proves R ◁S ≡R ◁∀S. The other equalities may be proved analogously.
4
Properties
Many appropriate properties were proved for the original classical as well as
fuzzy relational compositions, see e.g. [8]. In this section, we face the question
whether the same or similar properties may be valid also for the compositions
based on generalized quantiﬁers. As we will show, the answer is rather positive.
Theorem 2. Let X, Y, Z, U are ﬁnite universes and let R1, R2 ⊂∼X × Y ,
S1, S2 ⊂∼Y × Z and T ⊂∼Z × U. Furthermore, let ∪, ∩denote the G¨odel union
and intersection, respectively. Then
1. R ◦Q (S ◦Q T ) = (R ◦Q S) ◦Q T
2. R □Q S ≤(R ◁Q S) ∩(R ▷Q S)
3. R1 ≤R2 ⇒(R1 ◦Q S) ⊆(R2 ◦Q S) and S1 ≤S2 ⇒(R ◦Q S1) ⊆(R ◦Q S2)
4. R1 ≤R2 ⇒(R1 ◁Q S) ⊇(R2 ◁Q S) and (R1 ▷Q S) ⊆(R2 ▷Q S)
5. (R1 ∪R2) ◦Q S = (R1 ◦Q S) ∪(R2 ◦Q S)
6. (R1 ∩R2) ◁Q S = (R1 ◁Q S) ∪(R2 ◁Q S)
7. (R1 ∪R2) ▷Q S = (R1 ▷Q S) ∪(R2 ▷Q S)
8. (R1 ∩R2) ◦Q S ≤(R1 ◦Q S) ∩(R2 ◦Q S)
9. (R1 ∪R2) ◁Q S ≤(R1 ◁Q S) ∩(R2 ◁Q S)
10. (R1 ∩R2) ▷Q S ≤(R1 ▷Q S) ∩(R2 ▷Q S)

232
M. ˇStˇepniˇcka and M. Holˇcapek
Sketch of the proof: All the properties are proved based on the properties of left-
continuous t-norms and their residual implications on a linearly order set [0, 1],
i.e., using
(a ∧b) ∗c = (a ∗c) ∧(b ∗c),
(a ∨b) ∗c =(a ∗c) ∨(b ∗c),
(a ∧b) →c = (a →c) ∨(b →c),
(a ∨b) →c =(a →c) ∧(b →c),
a →(b ∧c) = (a →b) ∧(a →c),
a →(b ∨c) =(a →b) ∨(a →c),
(a ↔b) = (a →b) ∧(a ←b),
)
i
((ai ∗b) ∧(ai ∗c)) ≤
)
i
(ai ∗b) ∧
)
i
(ai ∗c)
and the antitonicity and the isotonicity of →in its ﬁrst and second argument,
respectively. Furthermore, the monotonicity properties 3.-4. are extensively used
in proving the latter properties.
□
Remark 2. Obviously, items 5.-10. may be also read as follows:
12. R ◦Q (S1 ∪S2) = (R ◦Q S1) ∪(R ◦Q S2)
13. R ▷Q (S1 ∩S2) = (R ▷Q S1) ∪(R ▷Q S2)
14. R ◁Q (S1 ∪S2) = (R ◁Q S1) ∪(R ◁Q S2)
15. R ◦Q (S1 ∩S2) ≤(R ◦Q S1) ∩(R ◦Q S2)
16. R ▷Q (S1 ∪S2) ≤(R ▷Q S1) ∩(R ▷Q S2)
17. R ◁Q (S1 ∩S2) ≤(R ◁Q S1) ∩(R ◁Q S2)
5
Conclusions
We have recalled classical and fuzzy relational composition. The big gap be-
tween the classical existential and universal quantiﬁers brings practical weakness
into the theory of fuzzy relational compositions. This can be elegantly solved
by the use of generalized quantiﬁers. Moreover, this fact has been observed by
the community of researchers dealing with ﬂexible querying systems, where the
compositions, investigated under the name fuzzy relational divisions, have been
“equipped” with generalized quantiﬁers a certain time ago. We stem from this
motivation and involve generalized quantiﬁers into the fuzzy relational composi-
tions as well. Particularly, they allow us to deﬁne fuzzy relational compositions
with help of linguistically very natural quantiﬁers such as ’A Few’, ’Many’,
’Majority’ or ’Most’. The whole topic is however, studied in the view of fuzzy
relational compositions and not in the view of fuzzy relational databases. The
reason lies in the older origin of compositions and in the goal to bring the nice
ideas from fuzzy relational databases back to the mathematical theory. Thus, we
follow the previous works on fuzzy relational compositions and investigate their
basic properties. It is shown that many of the appreciated properties of the stan-
dard fuzzy relational compositions are preserved at least in a weaken form. This
gives a promising potential to employ the fuzzy relational compositions based
on generalized quantiﬁers in many other areas of application such as inference
systems where the compositions stand for the main theoretical pilots.

Fuzzy Relational Compositions
233
Acknowledgments. This investigation was mainly supported by the European
Regional Development Fund in the IT4Innovations Centre of Excellence project
(CZ.1.05/1.1.00/02.0070).
References
1. Pedrycz, W.: Applications of fuzzy relational equations for methods of reasoning
in presence of fuzzy data. Fuzzy Sets and Systems 16, 163–175 (1985)
2. ˇStˇepniˇcka, M., De Baets, B., Noskov´a, L.: Arithmetic fuzzy models. IEEE Trans-
actions on Fuzzy Systems 18, 1058–1069 (2010)
3. Bandler, W., Kohout, L.J.: Fuzzy relational products and fuzzy implication opera-
tors. In: Proc. Int. Workshop on Fuzzy Reasoning Theory and Applications. Queen
Mary College, London (1978)
4. Bandler, W., Kohout, L.J.: Relational-product architectures for information pro-
cessing. Information Sciences 37, 25–37 (1985)
5. Bˇelohl´avek, R.: Fuzzy relational systems: Foundations and principles. Kluwer Aca-
demic, Plenum Press, Dordrecht, New York (2002)
6. De Baets, B., Kerre, E.: Fuzzy relational compositions. Fuzzy Sets and Systems 60,
109–120 (1993)
7. Bˇehounek, L., Cintula, P.: Fuzzy class theory 154(1), 34–55 (2005)
8. Bˇehounek, L., Daˇnkov´a, M.: Relational compositions in fuzzy class theory. Fuzzy
Sets and Systems 160(8), 1005–1036 (2009)
9. Bandler, W., Kohout, L.J.: Fuzzy relational products as a tool for analysis and
synthesis of the behaviour of complex natural and artiﬁcial systems. In: Wang,
S.K., Chang, P.P. (eds.) Fuzzy Sets: Theory and Application to Policy Analysis
and Information Systems, pp. 341–367. Plenum Press, New York (1980)
10. Pivert, O., Bosc, P.: Fuzzy preference queries to relational databases. Imperial
College Press, London (2012)
11. Dubois, D., Prade, H.: Semantics of quotient operators in fuzzy relational
databases. Fuzzy Sets and Systems 78, 89–93 (1996)
12. Delgado, M., Sanchez, D., Vila, M.A.: Fuzzy cardinality based evaluation of quan-
tiﬁed sentences. International Journal of Approximate Reasoning 23, 23–66 (2000)
13. Bosc, P., Li´etard, L., Pivert, O.: Flexible database querying and the division of
fuzzy relations. Scientia Iranica 2, 329–340 (1996)
14. Baczy´nski, M., Jayaram, B.: Fuzzy Implications. STUDFUZZ, vol. 231. Springer,
Heidelberg (2008)
15. Dvoˇr´ak, A., Holˇcapek, M.: L-fuzzy quantiﬁers of type ⟨1⟩determined by fuzzy
measures. Fuzzy Sets and Systems 160(23), 3425–3452 (2009)
16. Nov´ak, V.: A comprehensive theory of trichotomous evaluative linguistic expres-
sions. Fuzzy Sets and Systems 159(22), 2939–2969 (2008)
17. Bosc, P., Li´etard, L., Pivert, O.: Sugeno fuzzy integral as a basis for the interpre-
tation of ﬂexible queries involving monotonic aggregates. Information Processing
& Management 39(2), 287–306 (2003)

A Functional Approach to Cardinality
of Finite Fuzzy Sets
Michal Holˇcapek
University of Ostrava
Institute for Research and Applications of Fuzzy Modeling
Centre of Excellence IT4Innovations
30. dubna 22, 701 03 Ostrava, Czech Republic
michal.holcapek@osu.cz
http://irafm.osu.cz/
Abstract. In this contribution, we present a functional approach to the
cardinality of ﬁnite fuzzy sets, it means an approach based on one-to-one
correspondences between fuzzy sets. In contrast to one ﬁxed universe of
discourse used for all fuzzy sets, our theory is developed within a class of
fuzzy sets which universes of discourse are countable sets, and ﬁnite fuzzy
sets are introduced as fuzzy sets with ﬁnite supports. We propose some
basic operations with fuzzy sets as well as two constructions - fuzzy power
set and fuzzy exponentiation. To express the fact that two ﬁnite fuzzy
sets have approximately the same cardinality we propose the concept of
graded equipollence. Using this concept we provide graded versions of
several well-known statements, including the Cantor-Bernstein theorem
and the Cantor theorem.
Keywords: Fuzzy sets, fuzzy classes, graded equipollence, cardinal the-
ory of ﬁnite fuzzy sets.
1
Introduction
In the classical set theory, we can recognize two approaches to the cardinality of
sets. One of them is a functional approach that uses one-to-one correspondences
between sets to compare their sizes. More precisely, we say that two sets a and b
are equipollent (equipotent or have the same cardinality) and write a ∼b if there
exists a one-to-one mapping of a onto b. The relation “being equipollent” is an
equivalence on the class of all sets and is called equipollence. Note that special
objects expressing the power of sets are introduced in the second approach to
the cardinality of sets. These objects are called cardinal numbers and are deﬁned
as equivalence classes with respect to the relation of equipollence, or by initial
ordinal numbers of these classes, if it is possible (see, e.g., [7, 10]).
The equipollence of (ﬁnite) fuzzy sets has been investigated primarily by
S. Gottwald [2, 3] and M. Wygralak [13–16] (see also [8]). S. Gottwald proposed
a graded approach to the equipollence of fuzzy sets deﬁned using the uniqueness
of fuzzy mappings in his set theory for fuzzy sets of higher level. Additionally,
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 234–243, 2014.
c
⃝Springer International Publishing Switzerland 2014

A Functional Approach to Cardinality of Finite Fuzzy Sets
235
a graded generalization of equipollence suggesting that fuzzy sets have approxi-
mately the same number of elements has been noted by M. Wygralak in [14], but
a substantial development of cardinal theory based on this type of equipollence
has not been realized yet.
In [6] (see also [5]) we proposed a new approach to the equipollence of fuzzy
sets over a universe of sets (e.g., a universe of all ﬁnite or countable sets, all
sets, or a Grothendieck universe). Analogously to Gottwald’s approach, a graded
equipollence is considered, where the degrees of being equipollent are obtained
more simply than in Gottwald’s approach. More precisely, this approach is based
on a graded one-to-one correspondence between fuzzy sets, where the crisp map-
pings between universes are considered.
The aim of this short contribution is to present a functional approach to the
cardinality of ﬁnite fuzzy sets based on this type of equipollence. The deﬁnition
of graded equipollence for ﬁnite fuzzy sets generalizes the deﬁnition suggested in
[6] in such a way that we use the multiplication of algebra of truth values as an
alternative operation to the inﬁmum. We will show that the graded equipollence
with respect to a more general operation is a fuzzy similarity relation on the
class of all ﬁnite fuzzy sets, and well-known statements of the cardinal theory of
sets (including the Cantor-Bernstein theorem and the Cantor theorem stating
the diﬀerent cardinalities for sets and their power sets) can mostly be proved in
a graded design, where if-then formulas are replaced by the inequalities between
the degrees in which the antecedent and consequent are satisﬁed.
This contribution is structured as follows. In the next section, we recall the
deﬁnition of residuated lattice as an algebraical structure of membership de-
grees of fuzzy sets and fuzzy classes. In Section 3, we introduce basic concepts
concerning of the theory of (ﬁnite) fuzzy sets. The graded equipollence of ﬁnite
fuzzy sets is proposed, and some of their properties are analyzed in Section 4.
In Section 5, a functional approach to cardinal theory of ﬁnite fuzzy sets based
on the graded equipollence is elaborated. The last section concludes.
2
Algebras of Truth Values
In this contribution, we assume that the truth values are interpreted in a resid-
uated lattice, i.e., in an algebra L = ⟨L, ∧, ∨, ⊗, →, ⊥, ⊤⟩with four binary oper-
ations and two constants satisfying the following conditions:
(i) ⟨L, ∧, ∨, ⊥, ⊤⟩is a bounded lattice, where ⊥is the least element and ⊤is
the greatest element of L,
(ii) ⟨L, ⊗, ⊥⟩is a commutative monoid,
(iii) the pair ⟨⊗, →⟩forms an adjoint pair, i.e.,
α ≤ε →γ
if and only if
α ⊗ε ≤γ
hold for each α, ε, γ ∈L (≤denotes the corresponding lattice ordering).
The operations ⊗and →are called the multiplication and residuum, respec-
tively. We will say that a residuated lattice is complete (linearly ordered), if
⟨L, ∧, ∨, ⊥, ⊤⟩is a complete (linearly ordered) lattice.

236
M. Holˇcapek
Example 1. Let T be a left continuous t-norm; we deﬁne →T by
α →T ε =
)
{γ ∈[0, 1] | T (α, γ) ≤ε}.
Then, the algebra L = ⟨[0, 1], min, max, T, →T, 0, 1⟩is a complete residuated
lattice. If T is the ΣLukasiewicz conjunction, i.e., T (α, ε) = max(α + ε −1, 0)
(α →T ε = min(1 −α + ε, 1)), then we will use LL to denote the ΣLukasiewicz
algebra.
Let us deﬁne the following additional operations for any α, ε ∈L and set
{αi | i ∈I} of elements from L over a countable (possibly empty) index set I:
α ↔ε = (α →ε) ∧(ε →α),
(biresiduum)
¬α = α →⊥,
(negation)
+
i∈I
αi =
⊤,
I = ∅,
,
K∈Fin(I)
-
i∈K αi, otherwise,
(countable multiplication)
where Fin(I) denotes the set of all ﬁnite subsets of I.1 In order to integrate some
alternative constructions based on the operations of ∧(,) and ⊗(-), in the
sequel, we will use the common symbol ⊙(.).
Example 2. The residuum and negation in the ΣLukasiewicz algebra LL is deﬁned
as α ↔ε = 1 −|α −ε| and ¬α = 1 −α, respectively.
3
Fuzzy Sets in Count
Intuitively, ﬁnite fuzzy sets are fuzzy sets apparently deﬁned in ﬁnite universes
of discourse. Nevertheless, for an expression of their cardinalities using fuzzy
cardinals it is useful to consider fuzzy sets with the set of natural numbers
as their universe of discourse (see [4, 5, 15–17]). This motivates us to use the
proper class Count of all countable sets as a common framework for the functional
approach based on the relation of equipollence and the approach based on fuzzy
cardinals.
Let us suppose that a complete residuated lattice L is given. A fuzzy set in
Count is deﬁned as follows.
Deﬁnition 1. A mapping A : x →L is called a fuzzy set in Count if x ∈Count.
The class of all countable fuzzy sets will be denoted by FCount.
Let us denote by Dom(A) the domain or also universe of discourse of A,
Ran(A) the range of A and Supp(A) = {z ∈Dom(A) | A(z) > ⊥} the support
of fuzzy set A.
In the literature on fuzzy sets (see, e.g., [1, 9, 12]), a fuzzy set assigning ⊥to
each element of its universe of discourse is usually referred to the empty fuzzy
set, and a fuzzy set assigning α > ⊥to only one element of its universe to a
singleton (fuzzy set). In our theory, we will use a diﬀerent interpretation of the
empty fuzzy set and singleton in Count as follows.
1 Note that the countable multiplication in that form has been consider in [11].

A Functional Approach to Cardinality of Finite Fuzzy Sets
237
Deﬁnition 2. The empty mapping ∅: ∅→L is called the empty fuzzy set. A
fuzzy set A is called a singleton if Dom(A) contains only one element.
We say that two fuzzy sets are the same and write A = B if they coincide on
their domains, i.e., A(x) = B(x) for any x ∈Dom(A) = Dom(B). An essential
predicate in our theory is a binary relation that extends the concept of being
the same fuzzy sets and states that two fuzzy sets coincide on their supports.
Deﬁnition 3. We say that fuzzy sets A and B are equivalent (symbolically,
A ≡B) if Supp(A) = Supp(B) and A(x) = B(x) for any x ∈Supp(A). The
class of all equivalent fuzzy sets with A is denoted by cls(A).
Since we are dealing with fuzzy sets which have countable universes of dis-
course in general, we provide the following deﬁnition of ﬁnite fuzzy set.
Deﬁnition 4. We say that a fuzzy set A from FCount is ﬁnite if there exists
A′ ∈cls(A) such that Dom(A′) is a ﬁnite set. The class of all ﬁnite fuzzy sets
in Count is denoted by Ffin.
If a fuzzy set A has a ﬁnite universe, then we will use the following simple
notation
A = {α1/x1, . . . , αn/xn},
where Dom(A) = {x1, . . . , xn} and αi ∈L for any i = 1, . . . , n. The basic
operations with fuzzy sets are introduced as follows.
Deﬁnition 5. Let A, B ∈Fcount, x = Dom(A) ∪Dom(B) and A′ ≡A, B′ ≡B
such that Dom(A′) = Dom(B′) = x. Then,
• the union of A and B is the mapping A ∪B : x →L deﬁned by
(A ∪B)(a) = A′(a) ∨B′(a)
for any a ∈x,
• the intersection of A and B is the mapping A ∩B : x →L deﬁned by
(A ∩B)(a) = A′(a) ∧B′(a)
for any a ∈x,
• the diﬀerence of A and B is the mapping A \ B : x →L deﬁned by
(A \ B)(a) = A′(a) ⊗(B′(a) →⊥) = A′(a) ⊗¬B′(a)
for any a ∈x.
Deﬁnition 6. Let A, B ∈Fcount, x = Dom(A) × Dom(B) and y = Dom(A) ⊔
Dom(B) (the disjoint union). Then,
• the product of A, B is the mapping A × B : x →L deﬁned by
(A × B)(a, b) = A(a) ∧B(b)
for any (a, b) ∈x,

238
M. Holˇcapek
• the strong product of A, B is the mapping A ⊗B : x →L deﬁned by
(A ⊗B)(a, b) = A(a) ⊗B(b)
for any (a, b) ∈x,
• the disjoint union of A, B is the mapping A ⊔B : y →L deﬁned by
(A ⊔B)(a, i) =
A(a, i), if i = 1,
B(a, i), if i = 2,
for any (a, i) ∈y.
Deﬁnition 7. The fuzzy set A = Dom(A) \ A is called the complement of A.
It is easy to see that the relation “being equivalent” is a congruence for all
mentioned operations except the complement. Moreover, if the proposed opera-
tions are applied on ﬁnite fuzzy sets, the resulting fuzzy set is again ﬁnite.
Example 3. Let LL be the ΣLukasiewicz algebra and A = {1/a, 0.4/b} and B =
{0.6/a, 0.2/c}. Then, we have
A ∪B = {1/a, 0.4/b, 0.2/c},
A ∩B = {0.6/a, 0/b, 0/c},
A \ B = {0.4/a, 0.4/b, 0/c},
A × B = {0.6/(a, a), 0.2/(a, c), 0.4/(b, a), 0.2/(b, c)},
A ⊗B = {0.6/(a, a), 0.2/(a, c), 0/(b, a), 0/(b, c)},
A ⊔B = {1/(a, 1), 0.4/(b, 1), 0.6/(a, 2), 0.2/(c, 2)},
A = {0/a, 0.6/b}.
It is not easy to say what the power set means for fuzzy sets. We propose
the following simple deﬁnition that straightforwardly generalizes the classical
approach to the concept of power set. We will use χx to denote the characteristic
function of a set x, i.e., χx(y) = ⊤, if y ∈x, and χx(y) = ⊥, otherwise.
Deﬁnition 8. Let A ∈FCount and x = {y | y ⊆Dom(A)}. The fuzzy set
P(A) : x →L deﬁned by
P(A)(y) =
*
z∈Dom(A)
(χy(z) →A(z))
is called the fuzzy power set of A.
Example 4. Let LL be the ΣLukasiewicz algebra and A = {1/a, 0.4/b}. Then,
P(A) = {1/∅, 1/{a}, 0.4/{b}, 0.4/{a, b}}.
Further, we introduce the concept of exponentiation for fuzzy sets. Recall that
if x, y are sets, then the exponentiation xy is the set of all mappings of y to x.
We propose the following deﬁnition, which generalizes the classical one.

A Functional Approach to Cardinality of Finite Fuzzy Sets
239
Deﬁnition 9. Let A, B ∈FCount and put x = Dom(A) and y = Dom(B). The
fuzzy set BA : yx →L deﬁned by
BA(f) =
*
z∈x
(A(z) →B(f(z)))
is called the exponentiation of A to B.
Example 5. Let LL be the ΣLukasiewicz algebra and A = {1/a, 0.4/b}, B =
{0.6/a, 0.2/c}. Obviously, the domain of BA is the set of all mappings of {a, b}
to {a, c}. For example, if f ∈Dom(BA) is deﬁned as f(a) = f(b) = a, then
BA(f) = (1 →0.6) ∧(0.4 →0.6) = 0.6 ∧⊤= 0.6.
Finally, we introduce the concept of fuzzy class. Although our attention is
focused on fuzzy sets the concept of fuzzy class will help us to denote special
objects that are related to our theory but that are not fuzzy sets (because they
are too large).
Deﬁnition 10. A mapping A : x →L is called a fuzzy class in Count if x ⊆
Count.
4
Graded Equipollence of Finite Fuzzy Sets
In [6], we introduced the concept of graded equipollence using the degrees in
which a one-to-one mapping between sets is a one-to-one mapping between fuzzy
sets. This idea practically generalizes one of Cantor’s approaches to the cardi-
nality of sets, namely, that two sets have the same cardinality (are equipollent)
if there exists a one-to-one mapping of one set onto the second one. Thus, to
check the same cardinality of two sets, one needs to construct a one-to-one cor-
respondence between them. However, in the fuzzy case, the situation is more
complicated by the membership degrees, and intuitively, not all one-to-one cor-
respondences between universes of fuzzy sets are appropriate to assert that fuzzy
sets have the same cardinality. Moreover, if two fuzzy sets are very similar (con-
sider [A ≈B] = 0.999) but there is no one-to-one correspondence between them
(consider f →(A) = B), then it seems to be advantageous to say that these sets
have approximately the same cardinality. Thus, the graded equipollence gives
a degree in which two fuzzy sets have approximately the same cardinality, and
this degree is derived from degrees in which one may construct one-to-one cor-
respondences between fuzzy sets.
Let us start with the concept of one-to-one mapping between fuzzy sets in a
degree.
Deﬁnition 11. Let A, B ∈Ffin, x, y ∈Count, and let f : x →y be a one-to-
one mapping of x onto y in Count. We will say that f is a one-to-one mapping
of A onto B in the degree α with respect to ⊙if Supp(A) ⊆x ⊆Dom(A) and
Supp(B) ⊆y ⊆Dom(B) and
α =
/
z∈x
(A(z) ↔B(f(z))).

240
M. Holˇcapek
We write [A ∼
⊙
f B] = α if f is a one-to-one mapping of A onto B in the degree
α with respect to ⊙.
As could be seen above not all one-to-one mappings are considered to specify
the degree in which a mapping is a one-to-one mappings between fuzzy sets. The
following establishes the set of all important one-to-one mappings between fuzzy
sets.
Deﬁnition 12. Let A, B ∈Ffin. A mapping f : x →y belongs to the set
Bij(A, B) if f is a one-to-one mapping of x onto y, Supp(A) ⊆x ⊆Dom(A),
and Supp(B) ⊆y ⊆Dom(B).
Now we can proceed to the deﬁnition of graded equipollence.
Deﬁnition 13. Let A, B ∈Ffin. We will say that A is equipollent with B (or
A has the same cardinality as B) in the degree α with respect to ⊙if there exist
fuzzy sets C ∈cls(A) and D ∈cls(B) such that
α =
)
f∈Bij(C,D)
[C ∼
⊙
f D]
(1)
and, for each A′ ∈cls(A), B′ ∈cls(B) and f ∈Bij(A′, B′), there is [A′ ∼
⊙
f
B′] ≤α.
Let A, B ∈Ffin such that |Dom(A)| = |Dom(B)|. We will use Perm(A, B) to
denote the set of all f ∈Bij(A, B) such that Dom(f) = Dom(A) and Ran(f) =
Dom(B).2 The following theorem shows how to ﬁnd the degree of equipollence
α between A and B.
Theorem 1. Let A, B ∈Ffin. Then,
[A ∼
⊙B] =
)
f∈Perm(C,D)
[C ∼
⊙
f D].
(2)
for any C ∈cls(A) and D ∈cls(B) such that |Dom(C)| = |Dom(D)| = m.
Similarly to the equipollence of sets (or fuzzy sets), the graded equipollence
of fuzzy set is a ⊗-similarity relation (i.e., reﬂexive, symmetric and ⊗-transitive)
on the class of all ﬁnite fuzzy sets as the following theorem shows.
Theorem 2. The fuzzy class relation ∼⊙: Ffin × Ffin →L is a ⊗-similarity
relation on Ffin, i.e.,
[A ∼
⊙A] = ⊤,
[A ∼
⊙B] = [B ∼
⊙A],
[A ∼
⊙B] ⊗[B ∼
⊙C] ≤[A ∼
⊙C],
which holds for arbitrary fuzzy sets A, B, C ∈Ffin.
2 Although f is not a permutation on a universe in general, we use the denotation
Perm, because to each pair of universes of fuzzy sets A and B we can deﬁne a
common universe and fuzzy sets A′ and B′ equivalent to A and B, respectively, such
that each mapping from Perm(A′, B′) is a permutation on this common universe.

A Functional Approach to Cardinality of Finite Fuzzy Sets
241
5
Graded Versions of Fundamental Results of Set Theory
The most familiar theorem in set theory is the Cantor-Bernstein theorem (CBT).
One of its forms states that if a, b, c, d are sets such that b ⊆a and d ⊆c and
a ∼d and b ∼c, then a ∼c. Unfortunately, we cannot prove its graded form in
a full generality and have to restrict ourselves to the case of ⊙= ∧and assume
the linearity of residuated lattice.
Theorem 3 (Cantor-Bernstein theorem). Let L be a linearly ordered resid-
uated lattice and A, B, C, D ∈Ffin such that B ⊆A and D ⊆C. Then
[A ∼
∧D] ∧[C ∼
∧B] ≤[A ∼
∧C].
A consequence of Theorem 3 is the following form of the graded Cantor-Bernstein
theorem (cf. Corollary 4.8 in [16]), which generalizes a more commonly used
version of the Cantor-Bernstein theorem.
Corollary 1. Let L be a linearly ordered residuated lattice and A, B, C ∈Ffin
such that A ⊆B ⊆C. Then,
[A ∼
∧C] ≤[A ∼
∧B] ∧[B ∼
∧C].
Let a, b, c, d be sets such that a ∼c and b ∼d. Then, it is well-known that
a ∪b ∼c ∪d, whenever a ∩b = ∅and c ∩d = ∅, a × b ∼c × d, a ⊔b ∼
c ⊔d. Graded versions of these and two further statements are presented in the
following theorem.
Theorem 4. Let A, B, C, D ∈Ffin. Then,
[A ∼
⊙B] ≤[A ∼
⊙B],
[A ∼
⊙B] ⊗[C ∼
⊙D] ≤[A ⊗C ∼
⊙B ⊗D],
[A ∼
⊙B] ⊗[C ∼
⊙D] ≤[A × C ∼
⊙B × D],
[A ∼
⊙B] ⊗[C ∼
⊙D] ≤[A ⊔C ∼
⊙B ⊔D],
if Supp(A) ∩Supp(B) = Supp(C) ∩Supp(D) = ∅, then
[A ∼
⊙C] ⊗[B ∼
⊙D] ≤[A ∪B ∼
⊙C ∪D].
If a, b are sets and a ∼b, then P(a) ∼P(b), where P(a) and P(b) denote
the power sets of a and b, respectively. The following theorem provides a graded
version of this classical statement, where we have to restrict ourselves to the
operation ∧in the computation of the degree of graded equipollence between
fuzzy power sets.
Theorem 5. Let A, B ∈Ffin. Then, [A ∼⊙B] ≤[P(A) ∼∧P(B)].
One of the signiﬁcant Cantor’s theorems states that a is not equipollent with
its power, i.e., a ̸∼P(a). The following is a generalization of this statement
for ﬁnite fuzzy sets, stating that A and P(A) cannot have the same number of
elements.

242
M. Holˇcapek
Theorem 6 (Cantor’s theorem). Let A ∈Ffin. Then, [A ∼⊙P(A)] < ⊤.
Example 6. If A = {1/a, 0.4/b} and P(A) = {1/∅, 1/{a}, 0.4/{b}, 0.4/{a, b}}
are from Example 4, then with C = {1/a, 0.4/b, 0/c, 0/d}, one may simply check
that
[A ∼
∧P(A)] = [C ∼
∧P(A)] = (1 ↔1) ∧(1 ↔0.4) ∧(0.4 ↔0) ∧(0.4 ↔0) =
1 ∧0.4 ∧0.6 ∧0.6 = 0.4.
Hence, we have 0 < [A ∼∧P(A)] < 1. Moreover, we obtain [A ∼⊗P(A)] = 0.
If a, b, c, d are sets such that a ∼c and b ∼d, then ba ∼dc. The following
theorem is a graded version of this statement, where again we have to restrict
ourselves to the operation ∧in the computation of the degree of graded equipol-
lence between fuzzy exponentials.
Theorem 7. Let A, B, C, D ∈Ffin such that |Dom(A)| = |Dom(C)| = m and
|Dom(B)| = |Dom(D)| = n. Then,
[A ∼
⊙C] ⊗[B ∼
⊙D] ≤[BA ∼
∧CD].
If a, b, c are sets, then it holds ca×b ∼(cb)a. The following theorem generalizes
this relation for ﬁnite fuzzy sets.
Theorem 8. Let A, B, C ∈Ffin such that their universes are ﬁnite. Then,
[CA⊗B ∼
⊙(CB)A] = ⊤.
Remark 1. Note that an analogous relation to P(a) ∼2a cannot be proved for
fuzzy sets.
6
Conclusion
In this contribution, we presented a cardinal theory of ﬁnite fuzzy sets based on
the concept of graded equipollence. Fuzzy sets are propose to be inside the class
of all countable sets. In contrast to the standard approach to fuzzy sets, we do
not suppose a ﬁxed universe for all fuzzy sets. A basic theory of fuzzy sets in
the universe of all countable sets was introduced, including constructions such as
fuzzy power sets and exponentiation. Graded equipollence was deﬁned as a fuzzy
class relation assigning a degree to each pair of fuzzy sets, expressing the fact
that these fuzzy sets have approximately the same cardinality (are approximately
equipollent). The graded equipollence is derived from degrees in which one-to-
one mappings between sets may be considered to be one-to-one correspondences
between fuzzy sets. With this concept, a functional approach to cardinal theory of
ﬁnite fuzzy sets was developed, and several well-known statements, including the
Cantor-Bernstein theorem and the Cantor theorem stating diﬀerent cardinalities
for sets and their power sets, were generalized in a graded design.

A Functional Approach to Cardinality of Finite Fuzzy Sets
243
Acknowledgments.
This work was supported by the European Regional
Development
Fund
in
the
IT4Innovations
Centre
of
Excellence
project
(CZ.1.05/1.1.00/02.0070).
References
1. Dubois, D., Prade, H. (eds.): Fundamentals of fuzzy sets. Foreword by LotﬁA.
Zadeh. The Handbooks of Fuzzy Sets Series 7, vol. xxi, p. 647. Kluwer Academic
Publishers, Dordrecht (2000)
2. Gottwald, S.: Fuzzy uniqueness of fuzzy mappings. Fuzzy Sets and Systems 3,
49–74 (1980)
3. Gottwald, S.: A note on fuzzy cardinals. Kybernetika 16, 156–158 (1980)
4. Holˇcapek, M.: An axiomatic approach to fuzzy measures like set cardinality for
ﬁnite fuzzy sets. In: H¨ullermeier, E., Kruse, R., Hoﬀmann, F. (eds.) IPMU 2010,
Part I. CCIS, vol. 80, pp. 505–514. Springer, Heidelberg (2010)
5. Holˇcapek, M.: Graded equipollence and fuzzy c-measures of ﬁnite fuzzy sets. In:
Proc. of 2011 IEEE International Conference on Fuzzy Systems, pp. 2375–2382.
DnE, Taiwan (2011)
6. Holˇcapek, M., Turˇcan, M.: Graded equipollence of fuzzy sets. In: Carvalho, J.P.,
Dubois, D., Kaymak, D.U., Sousa, J.M.C. (eds.) Proceedings of IFSA/EUSFLAT
2009, pp. 1565–1570. Universidade T´ecnica de Lisboa, Lisbon (2009)
7. Jech, T.J.: Set Theory. Springer, Berlin (1997)
8. Klaua, D.: Zum kardinalzahlbegriﬀin der mehrwertigen mengenlehre. In: Asser,
G., Flashmayers, J., Rinow, W. (eds.) Theory of Sets and Topology, pp. 313–325.
Deutsher Verlag der Wissenshaften, Berlin (1972)
9. Klir, G.J., Yuan, B.: Fuzzy Sets and Fuzzy Logic: Theory and Applications. Pren-
tice Hall, New Jersey (1995)
10. Levy, A.: Basic set theory. Dover Books on Mathematics. Dover Publications (2002)
11. Mesiar, R., Thiele, H.: On T-quantiﬁers and S-quantiﬁers. In: Discovering the
World with Fuzzy Logic, pp. 310–326. Physica-Verlag, Heidelberg (2000)
12. Nov´ak, V.: Fuzzy Sets and Their Application. Adam-Hilger, Bristol (1989)
13. Wygralak, M.: Generalized cardinal numbers and operations on them. Fuzzy Sets
and Systems 53(1), 49–85 (1993)
14. Wygralak, M.: Vaguely deﬁned objects. Representations, fuzzy sets and nonclassi-
cal cardinality theory. Theory and Decision Library. Series B: Mathematical and
Statistical Methods, vol. 33. Kluwer Academic Publisher, Dordrecht (1996)
15. Wygralak, M.: Fuzzy sets with triangular norms and their cardinality theory. Fuzzy
Sets and Systems 124(1), 1–24 (2001)
16. Wygralak, M.: Cardinalities of Fuzzy Sets. Kluwer Academic Publisher, Berlin
(2003)
17. Zadeh, L.A.: A computational approach to fuzzy quantiﬁers in natural languages.
Comp. Math. with Applications 9, 149–184 (1983)

Piecewise Linear Approximation of Fuzzy
Numbers Preserving the Support and Core
Lucian Coroianu1, Marek Gagolewski2,3, PrzemysΣlaw Grzegorzewski2,3,
M. Adabitabar Firozja4, and Tahereh Houlari5
1 Department of Mathematics and Informatics, University of Oradea,
1 Universitatii Street, 410087 Oradea, Romania
2 Systems Research Institute, Polish Academy of Sciences,
Newelska 6, 01-447 Warsaw, Poland
3 Faculty of Mathematics and Information Science, Warsaw University of Technology,
Koszykowa 75, 00-662 Warsaw, Poland
4 Department of Mathematics, Qaemshahr Branch, Islamic Azad University, Iran
5 School of Mathematics and Computer Sciences, Damghan University, Iran
lcoroianu@uoradea.ro, {gagolews,pgrzeg}@ibspan.waw.pl,
mohamadsadega@yahoo.com
Abstract. A reasonable approximation of a fuzzy number should have
a simple membership function, be close to the input fuzzy number, and
should preserve some of its important characteristics. In this paper we
suggest to approximate a fuzzy number by a piecewise linear 1-knot fuzzy
number which is the closest one to the input fuzzy number among all
piecewise linear 1-knot fuzzy numbers having the same core and the same
support as the input. We discuss the existence of the approximation op-
erator, show algorithms ready for the practical use and illustrate the
considered concepts by examples. It turns out that such an approxima-
tion task may be problematic.
Keywords: Approximation of fuzzy numbers, core, fuzzy number, piece-
wise linear approximation, support.
1
Introduction
Complicated membership functions generate many problems in processing im-
precise information modeled by fuzzy numbers including problems with calcula-
tions, computer implementation, etc. Moreover, handling complex membership
functions entails diﬃculties in interpretation of the results too. This is the reason
why a suitable approximation of fuzzy numbers is so important. So we usually
try to substitute the original “input” membership functions by the “output”
which is simpler or more regular and hence more convenient for further tasks.
We expect that a desired approximation will reveal the following priorities:
(P.1) simplicity of a membership function,
(P.2) closeness to the input fuzzy number,
(P.3) preservation of some important characteristics.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 244–253, 2014.
c
⃝Springer International Publishing Switzerland 2014

Piecewise Linear Approximation of Fuzzy Numbers
245
The simplest possible shape of a membership function is acquired by linear
sides. Hence, (P.1) is fulﬁlled by the trapezoidal approximation. One may do it in
many ways but it seems that the desired approximation output should be as close
as possible to the input. Thus, (P.1) and (P.2) may lead to the approximation of
a fuzzy number by the closest trapezoidal one. However, such an approximation
does not guarantee automatically any other interesting properties. Therefore,
we often look for the approximation that has some additional properties like the
invariance of the expected interval (see, e.g. [1,6,5,8]). It seems that the core and
the support belong to the most important characteristics of fuzzy numbers. It is
quite obvious since these very sets are the only ones which are connected with
our “sure” knowledge. Actually, the core contains all the points which surely
belong to the fuzzy set under study. On the other hand, the complement of the
support consists of the points that surely do not belong to given fuzzy set. The
belongingness of all other points to the fuzzy set under discussion is just a matter
of degree described quantitatively by the membership function. Hence, one may
easily agree that both the support and core play a key role in fuzzy set analysis.
However, if we try to approximate a fuzzy number by a trapezoidal one that
preserves both the support and core of the input, the approximation problem
simpliﬁes too much since we obtain the unique solution just by joining the bor-
ders of the support and core by the straight lines. Unfortunately, the output of
such approximation may be signiﬁcantly distant from the input. The way out
from this dilemma is to consider the approximation by a trapezoidal fuzzy num-
ber which is as close as possible to the input and preserves either the core or the
support. This way we obtain a procedure which fulﬁlls all the desired conditions
(P.1)-(P.3). However, one may easily indicate examples where the output of the
approximation with ﬁxed core has the support signiﬁcantly diﬀerent than the
support of the input. And conversely, the output of the approximation with ﬁxed
support may have the core signiﬁcantly diﬀerent than the core of the input.
This discussion shows that usually we cannot obtain a satisfying trapezoidal
approximation of an arbitrary fuzzy number that fulﬁlls the nearness criterion
and preserves both the support and core. In this paper we propose to consider
the 1-knot piecewise linear fuzzy numbers (see [2]) as a reasonable solution of
the approximation problem satisfying requirements (P.1)-(P.3). More precisely,
we suggest to approximate a fuzzy number by the closest piecewise linear 1-knot
fuzzy number having the same core and the same support as the input.
2
Piecewise Linear 1-Knot Fuzzy Numbers
Fuzzy numbers are particular cases of fuzzy sets of the real line. The membership
function of a fuzzy number A is given by:
A(x) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
0
if
x < a1,
lA(x)
if
a1 ≤x < a2,
1
if
a2 ≤x ≤a3,
rA(x)
if
a3 < x ≤a4,
0
if
x > a4,
(1)

246
L. Coroianu et al.
where a1, a2, a3, a4 ∈R, lA : [a1, a2] −→[0, 1] is a nondecreasing upper semi-
continuous function, lA(a1) = 0, lA(a2) = 1, called the left side of the fuzzy
number, and rA : [a3, a4] −→[0, 1] is a nonincreasing upper semicontinuous
function, rA(a3) = 1, rA(a4) = 0, called the right side of the fuzzy number A.
The α-cut of A, α ∈(0, 1], is a crisp set deﬁned as: Aα = {x ∈R : A(x) ≥α}.
The support or 0-cut, A0, of a fuzzy number is deﬁned as
supp(A) = A0 = {x ∈R : A(x) > 0}.
It is easily seen that for each α ∈(0, 1] every α-cut of a fuzzy number is a closed
interval Aα = [AL(α), AU(α)], where AL(α) = inf{x ∈R : A(x) ≥α} and
AU(α) = sup{x ∈R : A(x) ≥α}.
Moreover, if the sides of the fuzzy number A are strictly monotone, then AL
and AU are inverse functions of lA and rA, respectively. The 1-cut of A will be
called the core of A and we use the notation
A1 = core(A) = {x ∈R : A(x) = 1}.
From now on, we denote by F(R) the set of all fuzzy numbers. However,
in practice, e.g. when calculations of arithmetic operations is performed, fuzzy
numbers with simple membership functions are often preferred. The most com-
monly used subclass of F(R) is formed by so-called trapezoidal fuzzy numbers,
i.e. fuzzy numbers with linear sides. The set of all trapezoidal fuzzy numbers is
denoted by FT (R). Trapezoidal fuzzy numbers are often used directly for model-
ing vague concepts or for approximating more complicated fuzzy numbers due to
their simplicity. Unfortunately, in some situations such simple description may
appear too limited. In some cases we are interested in specifying the membership
function in one (or more) additional α-cuts other than 0 or 1. Thus in [2] a gen-
eralization of the trapezoidal fuzzy numbers was proposed by considering fuzzy
numbers with piecewise linear side functions each consisting of two segments.
Deﬁnition 1. For any ﬁxed α0 ∈(0, 1) an α0-piecewise linear 1-knot fuzzy
number S is a fuzzy number with the following membership function
S(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
if
x < s1,
α0 x−s1
s2−s1
if
s1 ≤x < s2,
α0 + (1 −α0) x−s2
s3−s2
if
s2 ≤x < s3,
1
if
s3 ≤x ≤s4,
α0 + (1 −α0) s5−x
s5−s4
if
s4 < x ≤s5,
α0 s6−x
s6−s5
if
s5 < x ≤s6,
0
if
x > s6,
where s = (s1, . . . , s6) such that s1 ≤· · · ≤s6.
Since any α0-piecewise linear 1-knot fuzzy number is completely deﬁned by
its knot α0 and six real numbers s1 ≤· · · ≤s6, hence it will be denoted as
S = S(α0, s). An example of an α0-piecewise linear 1-knot fuzzy number is given
in Fig. 1.

Piecewise Linear Approximation of Fuzzy Numbers
247
0
1
2
3
4
5
0.0
0.2
0.4
0.6
0.8
1.0
x
μS(x)
s1
s2
s3
s4
s5
s6
α0
Fig. 1. The membership function of S = S(0.6, (0, 0.3, 1, 2, 4, 5))
Alternatively, an α0-piecewise linear 1-knot fuzzy number may be deﬁned
using its α-cut representation, i.e.
SL(α) =
s1 + (s2 −s1) α
α0
for
α ∈[0, α0),
s2 + (s3 −s2) α−α0
1−α0
for
α ∈[α0, 1]
(2)
and
SU(α) =
s5 + (s6 −s5) α0−α
α0
for
α ∈[0, α0),
s4 + (s5 −s4) 1−α
1−α0
for
α ∈[α0, 1].
(3)
Let us denote the set of all such fuzzy numbers by Fπ(α0)(R). By setting Fπ(0)(R)
= Fπ(1)(R) := FT (R) we also include the cases α0 ∈{0, 1}. Please note that the
inclusion FT (R) ⊆Fπ(α0)(R) holds for any α0 ∈[0, 1].
Moreover, to simplify notation, let Fπ[a,b](R) denote the set of all α-piecewise
linear 1-knot fuzzy numbers, where α ∈[a, b] for some 0 ≤a ≤b ≤1, i.e.
Fπ[a,b](R) :=
0
α∈[a,b]
Fπ(α)(R).
In many problems an adequate metric over the space of fuzzy numbers should
be considered. The ﬂexibility of the space of fuzzy numbers allows for the con-
struction of many types of metric structures over this space. In the area of fuzzy
number approximation the most suitable metric is an extension of the Euclidean
(L2) distance d deﬁned by (see, e.g., [4])
d2(A, B) =
 1
0
(AL(α) −BL(α))2dα +
 1
0
(AU(α) −BU(α))2dα.
(4)

248
L. Coroianu et al.
3
Approximation Problem
Let us consider any fuzzy number A ∈F(R). Suppose we want to approximate
A by an α0-piecewise linear 1-knot fuzzy number S. In [2] the piecewise linear 1-
knot fuzzy number approximation is broadly discussed. In this paper we suggest
another type of approximation. Keeping in mind postulates (P.1)–(P.3), our goal
now is to ﬁnd the approximation which fulﬁlls the following requirements:
1. Indicate the optimal knot α0 for the piecewise linear 1-knot fuzzy number
approximation of A, i.e. we are looking for the solution S(A) in Fπ[0,1](R).
2. The solution should fulﬁll the so-called nearness criterion (see [6]), i.e. for
any fuzzy number A the solution S(A) should be the α0-piecewise linear 1-
knot fuzzy number nearest to A with respect to some predetermined metric.
In our case we consider the distance d given by (4).
3. The solution should preserve the core and the support of A.
More formally, we are looking for such S∗= S∗(A) ∈Fπ[0,1](R) that
d(A, S∗) =
min
S∈Fπ[0,1](R) d(A, S),
(5)
which satisﬁes the following constraints:
core(S∗) = core(A),
(6)
supp(S∗) = supp(A).
(7)
At ﬁrst, let us investigate whether the above problem has at least one so-
lution for every A ∈F(R). For that we will use the property that the space
(F(R), d, +, ·) can be embedded in the Hilbert space
#
L2[0, 1] × L2[0, 1], 1d, ⊕, ⊙
$
(see e.g. [2]). Therefore, we have d(A, B) = 1d(A, B), A + B = A ⊕B and
λ · A = λ ⊙A, for all A, B ∈F(R) and λ ∈[0, ∞). By Proposition 4 in [2] it
is known that Fπ[0,1](R) is a closed subset of L2[0, 1] × L2[0, 1] in the topology
generated by 1d. Unfortunately, it may happen that the set
CS(A) =
'
S ∈Fπ[0,1](R) : core(S) = core(A), supp(S) = supp(A)
(
(8)
would not be closed in Fπ[0,1](R). Indeed, suppose that Aβ = [ε3, 1], ε ∈[0, 1].
Then let us consider a sequence (S(αn, sn))n≥1, sn = (sn,1, ..., sn,6) in CS(A),
where for each n ≥1 we have αn = (n −1)/n, sn,1 = sn,2 = 0 and sn,3 = ... =
sn,6 = 1. It is immediate that (1d) limn→∞S(αn, sn) = (d) limn→∞S(αn, sn) =
[0, 1] and since core ([0, 1]) ̸= core(A) it results that the set CS(A) is not closed
in L2[0, 1] × L2[0, 1], nor in F(R). Therefore, it is an open question whether
problem (5)-(7) has a solution for any A ∈F(R).
Interestingly, the solution always exists if we consider a local approximation
problem. Suppose that 0 < a < b < 1 and let us consider the set Fπ[a,b](R) =
{S(α, s) ∈Fπ[0,1](R) : a ≤α ≤b}. Now let us consider the following set
CSa,b(A) =
'
S ∈Fπ[a,b](R) : core(S) = core(A), supp(S) = supp(A)
(
.
(9)

Piecewise Linear Approximation of Fuzzy Numbers
249
We are looking for such S∗= S∗(A) ∈CSa,b(A) that
d(A, S∗) =
min
S∈CSa,b(A) d(A, S).
(10)
Obviously, there is a sequence (S(αn, sn))n≥1, in CSa,b(A), such that
lim
n→∞d(A, S(αn, sn)) =
inf
S∈CSa,b(A) d(A, S) := m.
(11)
Let n0 ∈N be such that d(A, S(αn, sn)) ≤m + 1 for all n ≥n0. This implies
that d(0, S(αn, sn)) ≤d(0, A) + d(A, S(αn, sn)) ≤d(0, A) + m + 1 for all n ≥n0.
Therefore, the sequence (S(αn, sn))n≥1 is bounded with respect to metric d and
hence with respect to 1d. By Lemma 2 (iii) in [2] it results that each sequence
(cn,i)n≥1 , i = 1, . . . , 8, is bounded, where
sn,1 = cn,1,
sn,2 = cn,2 · αn + cn,1,
sn,3 = cn,3 + cn,4,
sn,4 = cn,7 + cn,8,
sn,5 = cn,5 + cn,6 · αn,
sn,6 = cn,5.
Without loss of generality let us suppose that limn→∞αn = α0 (obviously we
have α0 ∈[a, b]) and limn→∞cn,i = ci, i = 1, . . . , 8. Letting n →∞in the above
equations and denoting s =(s1, ..., s6), where si = limn→∞sn,i, i = 1, . . . , 6, it
easily results that S(α0, s) ∈Fπ[a,b](R). Then, since S(αn, sn) ∈CSa,b(A) for all
n ≥1, it follows that sn,1 = AL(0), sn,3 = AL(1), sn,4 = AU(1) and sn,6 = AU(0)
and therefore we easily obtain that S(α0, s) preserves the core and support of A
and hence S(α0, s) ∈CSa,b(A). On the other hand, by Lemma 3 in [2] (making
some suitable substitutions) we also obtain that (1d) limn→∞S(αn, sn) = S(α0, s).
This property together with relation (11) and the continuity of d, implies that
d(A, S(α0, s)) = m. Hence we have just proved that problem (10) has at least
one solution. Note that one can easily prove that CSa,b(A) is not convex in
L2[0, 1] × L2[0, 1] which means that the solution of problem (10) may not be
unique. All these results are summarized in the following theorem.
Theorem 1. If A ∈F(R) and 0 < a < b < 1, then there exists at least one
element S∗= S∗(A) ∈Fπ[a,b](R) such that d(A, S∗) =
min
S∈CSa,b(A) d(A, S).
4
Algorithm
Let us show how to ﬁnd a solution to problem (10). We have to minimize the
function
f(α, x, y) =
 α
0

AL(ε) −

AL(0) + (x −AL(0)) · ε
α
		2
dε
+
 1
α

AL(ε) −

x + (AL(1) −x) · ε −α
1 −α
		2
dε
+
 α
0

AU(ε) −

y + (AU(0) −y) · α −ε
α
		2
dε
+
 1
α

AU(ε) −

AU(1) + (y −AU(1)) · 1 −ε
1 −α
		2
dε

250
L. Coroianu et al.
subject to AL(0) ≤x ≤AL(1) and AU(1) ≤y ≤AU(0).
This problem may have more than one solution and, in addition, it seems to be
diﬃcult to be solved analytically in this form since the equation f
′
α(α, x, y) = 0
cannot be solved in general as we are forced to work with functions where we
cannot separate α from the integral. Therefore, we will start by considering
the knot α = α0 being ﬁxed. For some α0 ∈(0, 1) we want to minimize the
function gα0(x, y) = f(α0, x, y) with the same restrictions as above. Obviously
we can split this problem into two independent sub-problems. Firstly, we have
to minimize the function
x →
 α0
0

AL(ε) −

AL(0) + (x −AL(0)) · ε
α0
		2
dε
+
 1
α0

AL(ε) −

x + (AL(1) −x) · ε −α0
1 −α0
		2
dε
on the interval [AL(0), AL(1)] and then we have to minimize the function
y →
 α0
0

AU(ε) −

y + (AU(0) −y) · α0 −ε
α0
		2
dε
+
 1
α0

AU(ε) −

AU(1) + (y −AU(1)) · 1 −ε
1 −α0
		2
dε
on the interval [AU(1), AU(0)]. Obviously, the above functions are quadratic
functions of one variable and after some simple calculations we obtain their
unique minimum points on R as
xm = 3
 α0
0

AL(ε) −AL(0) · α0 −ε
α0
	
· ε
α0
dε
+3
 1
α0

AL(ε) −AL(1) · ε −α0
1 −α0
	
· 1 −ε
1 −α0
dε
and
ym = 3
 α0
0

AU(ε) −AU(0) · α0 −ε
α0
	
· ε
α0
dε
+3
 1
α0

AU(ε) −AU(1) · ε −α0
1 −α0
	
· 1 −ε
1 −α0
dε.
From here we easily obtain the solutions of our two sub-problems as
x0 =
⎧
⎨
⎩
AL(0)
if
xm < AL(0),
AL(1)
if
xm > AL(1),
xm
if
AL(0) ≤xm ≤AL(1)
(12)
and
y0 =
⎧
⎨
⎩
AU(1)
if
ym < AU(1),
AU(0)
if
ym > AU(0),
ym
if
AU(1) ≤ym ≤AU(0).
(13)

Piecewise Linear Approximation of Fuzzy Numbers
251
When a computer implementation is needed, in most of the cases, xm and ym
may be easily calculated via numeric integration, cf. [3].
In conclusion, we have just proved for ﬁxed α the existence and uniqueness
of the piecewise linear 1-knot approximation which preserves the core and the
support. More exactly we have the following approximation result.
Theorem 2. Suppose that α0 ∈(0, 1) and for some fuzzy number A let us deﬁne
the set
CSα0(A) = {S ∈Fπ(α0)(R) : core(S) = core(A) and supp(S) = supp(A)}.
Then there exists a unique best approximation (with respect to metric d) of A
relatively to the set CSα0(A). This approximation is Sα0(A) = S(α0, s(A)),
s(A) = (s1(A), ..., s6(A)), where
s1(A) = AL(0),
s2(A) = x0,
s3(A) = AL(1),
s4(A) = AU(1),
s5(A) = y0,
s6(A) = AU(0),
and x0,y0 are given by (12) and (13) respectively.
We will use the previous theorem to approach a solution S∗(A) ∈CSa,b(A) of
problem (10). We will construct a sequence (Sαn(A))n≥1 in CSa,b(A) such that
(d) limn→∞Sαn(A) = S∗(A). Here, Sαn(A) is the unique best approximation of
A relatively to the set CSαn(A).
5
Some Numerical Examples
Example 1. Consider a fuzzy number A with supp = [0, 5], core = [3, 4] and
AL(α) = 3 qbetaA(α; 2, 1), AU(α) = 5 −α3, where qbeta(x; a, b) denotes the
quantile function of the Beta distribution B(a, b). Let α0 = 0.2. The best piece-
wise linear approximation A′
0.2 of A preserving the support and core is deﬁned
by s′ = (0, 1.53, 3, 4, 5, 5). We have d(A, A′
0.2) ≃0.212. On the other hand, for
the best piecewise linear approximation A′′
0.2 obtained using algorithm from [2]
and given by s′′ = (0.34, 1.46, 3.09, 4.26, 5.08, 5.08), we get d(A, A′′
0.2) ≃0.105.
The discussed fuzzy numbers are depicted in Fig. 2a.
Given a method to obtain S∗
α0(A), i.e. best α0-piecewise linear approximation
of A preserving its support and core, we may ﬁnd S∗(A) using some general
one-dimensional optimization technique, like the Brent algorithm implemented
in R’ optimize() function. This may be done for a ﬁxed fuzzy number A by
ﬁnding the argument minimizing the function DA(α) = d(A, S∗
α(A)), α ∈[0, 1].
Example 2. Let us go back to the fuzzy number A from Example 1. Fig. 2b
depicts the distance function DA. By applying the Brent algorithm we ﬁnd that
for α∗≃0.3 we get the solution to our problem, with d(A, S∗
α∗) ≃1.999.
Example 3. Consider a fuzzy number B with supp = [−1, 1] and core = {0},
BL(α) = qbeta(α; 2, 2), BU(α) = 1 −qbeta(α; 2, 2). Fig. 3a shows the distance
function DB having two minima near 0 and 1 that are hardly to ﬁnd numerically.

252
L. Coroianu et al.
0
1
2
3
4
5
0.0
0.2
0.4
0.6
0.8
1.0
x
α
A
A’
A’’
0.0
0.2
0.4
0.6
0.8
1.0
0.2
0.3
0.4
0.5
0.6
α
d(A, S*
α(A))
●
●
●
●
Fig. 2. (a) Fuzzy number A from Example 1 and its best approximation A′ preserving
its support and core as well as its best approximation A′′with no such constraints; (b)
distance function DA(α)
α
d(B, S*
α(B))
0.0
0.2
0.4
0.6
0.8
1.0
0.085644
0.086
0.088
0.098
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0995
0.1005
0.1015
α
d(C, S*
α(C))
●
●
Fig. 3. (a) Distance function DB(α) for a fuzzy number B from Example 3 (b) Distance
function DC(α) for a fuzzy number C from Example 4
Example 4. Consider a fuzzy number C with supp = [0, 1] and core = {1} with
CL(α) =
⎧
⎨
⎩
α
for
α ∈[0, 0.25),
0.5
for
α ∈[0.25, 0.75],
α
for
α ∈(0.75, 1].
The distance function DC is depicted in Fig. 3b. It has two minima at α = 0.25
and α = 0.75.
Moreover, an example may easily be constructed for which DX has a local
minimum that is not its global minimum. This information is important when
using numerical optimization techniques, as an algorithm may fall into a subop-
timal solution.

Piecewise Linear Approximation of Fuzzy Numbers
253
6
Conclusions
In this paper we have considered a fuzzy number approximation by a piecewise
linear 1-knot fuzzy number which is the closest one to the input fuzzy number
among all piecewise linear 1-knot fuzzy numbers having the same core and the
same support as the input. We have indicated and discussed problems related
to the existence and uniqueness of the solution.
One may easily notice that the approximation method suggested in this paper
works nicely if the input fuzzy number is more or less symmetrical. However, if
the left and the right side of a fuzzy number diﬀer a lot, e.g. one is convex while
the other is concave, our approximation method is not so convincing. It seems
that in such a case a much more natural approach is to treat both sides of a
fuzzy number separately.
Moreover, for particular fuzzy numbers (e.g. when the derivative in the neigh-
borhood of the borders of the support or core is close to 0) it might be reasonable
to consider weakened restrictions on the support and core than (6)–(7), just like
suggested in [7], i.e. supp(S(A)) ⊆supp(A) and core(A) ⊆core(S(A)).
Acknowledgments. The contribution of Lucian Coroianu was possible with
the support of a grant of the Romanian National Authority for Scientiﬁc Re-
search, CNCS-UEFISCDI, project number PN-II-ID-PCE-2011-3-0861. Marek
Gagolewski’s research was supported by the FNP START 2013 Scholarship.
References
1. Ban, A.I.: Approximation of fuzzy numbers by trapezoidal fuzzy numbers preserving
the expected interval. Fuzzy Sets and Systems 159, 1327–1344 (2008)
2. Coroianu, L., Gagolewski, M., Grzegorzewski, P.: Nearest piecewise linear approxi-
mation of fuzzy numbers. Fuzzy Sets and Systems 233, 26–51 (2013)
3. Gagolewski, M.: FuzzyNumbers Package: Tools to deal with fuzzy numbers in R
(2014), http://FuzzyNumbers.rexamine.com/
4. Grzegorzewski, P.: Metrics and orders in space of fuzzy numbers. Fuzzy Sets and
Systems 97, 83–94 (1998)
5. Grzegorzewski, P.: Trapezoidal approximations of fuzzy numbers preserving the ex-
pected interval - algorithms and properties. Fuzzy Sets and Systems 159, 1354–1364
(2008)
6. Grzegorzewski, P., Mr´owka, E.: Trapezoidal approximations of fuzzy numbers. Fuzzy
Sets and Systems 153, 115–135 (2005)
7. Grzegorzewski, P., Pasternak-Winiarska, K.: Natural trapezoidal approximations of
fuzzy numbers. Fuzzy Sets and Systems (2014),
http://dx.doi.org/10.1016/j.fss.2014.03.003
8. Yeh, C.T.: Trapezoidal and triangular approximations preserving the expected in-
terval. Fuzzy Sets and Systems 159, 1345–1353 (2008)

Characterization of the Ranking Indices
of Triangular Fuzzy Numbers
Adrian I. Ban and Lucian Coroianu
Department of Mathematics and Informatics, University of Oradea,
Universit˘at¸ii 1, 410087 Oradea, Romania
{aiban,lcoroianu}@uoradea.ro
Abstract. We ﬁnd necessary and suﬃcient conditions for a ranking in-
dex deﬁned on the set of triangular fuzzy numbers as a linear combi-
nation of its components to rank eﬀectively. Then, based on this result,
we characterize the class of ranking indices which generates orderings on
triangular fuzzy numbers satisfying the basic requirements by Wang and
Kerre in a slightly modiﬁed form.
Keywords: Fuzzy number, Triangular fuzzy number, Ranking.
1
Introduction
The main approach in the ordering of fuzzy numbers is based on so called rank-
ing indices. They are functions from fuzzy numbers to real values, the ordering
between fuzzy numbers being generated by a procedure based on the standard or-
dering of real numbers (see, e.g., [1], [2], [4]-[7], [10], [14], [15], [17]). A good choice
of the ranking is very important in many applications related with decision the-
ory, optimization, artiﬁcial intelligence, approximate reasoning, socioeconomic
systems, etc.
Starting from the reasonable properties proposed in [17], we characterize rank-
ings over triangular fuzzy numbers obtained from ranking indices. The main re-
sult of the paper is based on the ﬁnding of necessary and suﬀcient conditions
such that the requirements in [17] to be satisﬁed by a particular class which
include the most important ranking indices as expected value, value, ambiguity
and linear combinations of them. The present study continues our contribution
[3] dedicated to trapezoidal case.
2
Fuzzy Numbers and Ranking
We recall some deﬁnitions related to fuzzy numbers and basic requirements for
ordering fuzzy numbers.
Deﬁnition 1. (see [11]) A fuzzy number A is a fuzzy subset of the real line,
A : R →[0, 1], where A (x) denotes the value of the membership function of A
in x, satisfying the following properties:
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 254–263, 2014.
c
⃝Springer International Publishing Switzerland 2014

Characterization of the Ranking Indices of Triangular Fuzzy Numbers
255
(i) A is normal (i. e. there exists x0 ∈R such that A (x0) = 1);
(ii) A is fuzzy convex (i. e. A (αx1 + (1 −α) x2) ≥min (A (x1) , A (x2)) , for
every x1, x2 ∈R and α ∈[0, 1]);
(iii) A is upper semicontinuous in every x0 ∈R (i. e. ∀ε > 0, ∃γ > 0 such
that A (x) −A (x0) < ε, whenever |x −x0| < γ);
(iv) supp (A) is bounded, where supp (A) = cl {x ∈R : A (x) > 0} and cl (M)
denotes the closure of the set M.
The ϕ−cut, ϕ ∈(0, 1], of a fuzzy number A is a crisp set deﬁned as Aα =
{x ∈R : A(x) ≥ϕ}. Every ϕ−cut, ϕ ∈[0, 1], of a fuzzy number A is a closed
interval
Aα = [AL(ϕ), AU(ϕ)],
where
AL(ϕ) = inf{x ∈R : A(x) ≥ϕ},
(1)
AU(ϕ) = sup{x ∈R : A(x) ≥ϕ}
(2)
for any ϕ ∈(0, 1], with the convention A0 = [AL(0), AU(0)] := supp A.
Fuzzy numbers with simple membership functions are preferred in practice.
Often used are triangular fuzzy numbers, that is fuzzy numbers with ϕ−cuts
given by
δα = [x0 −ρ + ρϕ, x0 + π −πϕ]
(3)
where x0, ρ, π ∈R, ρ ≥0, π ≥0. We denote by δ = [x0, ρ, π] a such fuzzy
number. We observe that
δ0 = [x0 −ρ, x0 + π] := suppδ.
(4)
Another ϕ−cut representation of a triangular fuzzy number is given by
[t1 + (t2 −t1) ϕ, t3 −(t3 −t2) ϕ] .
(5)
In this case we denote δ = (t1, t2, t3). Comparing these representations we have
t1 = x0 −ρ
(6)
t2 = x0
(7)
t3 = x0 + π.
(8)
Throughout this paper we denote by F(R) the set of all fuzzy numbers and by
F Δ(R) the set of all triangular fuzzy numbers.
Let A, B ∈F (R) and α ∈R. The addition A+B and the scalar multiplication
α · A are deﬁned by
(A + B)α = Aα + Bα = [AL(ϕ) + BL(ϕ), AU(ϕ) + BU(ϕ)] ,
(α · A)α = α · Aα =
[αAL (ϕ) , αAU (ϕ)] , for α ≥0,
[αAU (ϕ) , αAL (ϕ)] , for α < 0.

256
A.I. Ban and L. Coroianu
The below basic requirements for the binary relation ⪰on the set S ⊆F(R)
were introduced in [17], under the assumption that one of A ≻B or B ≻A or
A ∼B is true for every (A, B) ∈S2 and with the notation A ⪰B when A ≻B
or A ∼B.
A1) A ⪰A for any A ∈S.
A2) For any (A, B) ∈S2, from A ⪰B and B ⪰A results A ∼B.
A3) For any (A, B, C) ∈S3, from A ⪰B and B ⪰C results A ⪰C.
A4) For any (A, B) ∈S2, from inf supp(A) ≥sup supp(B) results A ⪰B.
A′
4) For any (A, B) ∈S2, from inf supp(A) >sup supp(B) results A ≻B.
A′
4) For any (A, B) ∈S2, from inf supp(A) >sup supp(B) results A ≻B.
A5) Let S′⊆F(R) and A, B ∈S ∩S′. A ≻B on S if and only if A ≻B on S′.
A6) Let A, B, A+C and B+C be elements of S. If A ⪰B, then A+C ⪰B+C.
A′
6) Let A, B, A+C and B+C be elements of S. If A ≻B, then A+C ≻B+C.
A7) For any (A, B) ∈S2 and α ∈R such that α · A, α · B ∈S, from A ⪰B
results α · A ⪰α · B if α ≥0 and α · A ⪯α · B if α ≤0.
Remark 1. In [3] a justiﬁcation of the replacing of the corresponding property
from [17] introduced as
”2
A7) Let A, B, A · C and B · C be elements of S and C ≥0. If A ⪰B then
A · C ⪰B · C”
with A7) is given.
In most of the cases, ranking fuzzy numbers assumes to associate for each
fuzzy number a real number and so fuzzy numbers are ranked through these real
values. In this way, if S is a subset of F(R) then a ranking index P : S →R
generates on S a ranking in the following way:
(i) A ≻P B if and only if P(A) > P(B),
(ii) A ≺P B if and only if P(A) < P(B),
(iii) A ∼P B if and only if P(A) = P(B),
(iv) A ≻P B or A ∼P B if and only if P(A) ≥P(B),
(v) A ≺P B or A ∼P B if and only if P(A) ≤P(B).
When A ≻P B or A ∼P B we denote A ⪰P B and, when A ≺P B or A ∼P B,
we denote A ⪯P B.
Remark 2. In some recent papers (see e.g. [1], [2], [15] ) the following requirement
is considered as an important reasonable property for a ranking index P : S →R.
A′′
4) For any A ∈S, P(A) ∈supp(A).
In [3], Theorem 9, under the assumption R ⊂S, we prove that if P : S →R is a
ranking index on S such that ⪰P satisﬁes A′
4 (and simultaneously A4) and P is
continuous on R, then there exists a ranking index R : S →R which satisﬁes A′′
4
and ⪰R is equivalent of ⪰P , that is, for any A, B ∈S, from A ⪰P B it results
A ⪰R B and from A ≻P B it results A ≻R B. In the present paper, R ⊂S and
the continuity on R are assumed for every ranking index P : S →R.

Characterization of the Ranking Indices of Triangular Fuzzy Numbers
257
3
Characterization of Valuable Ranking Indices on
Triangular Fuzzy Numbers
We recall, the expected value EV (A), the ambiguity Amb(A) and the value
V al(A) of a fuzzy number A are given by (see [9], [12], [13])
EV (A) = 1
2
 1
0
(AL(ϕ) + AU(ϕ)) dϕ,
Amb(A) =
 1
0
ϕ(AU(ϕ) −AL(ϕ))dϕ,
V al(A) =
 1
0
ϕ(AU(ϕ) + AL(ϕ))dϕ.
After some simple calculations we get
EV (δ) = x0 −1
4ρ + 1
4π,
(9)
Amb(δ) = 1
6ρ + 1
6π,
(10)
V al(δ) = x0 −1
6ρ + 1
6π,
(11)
where δ = [x0, ρ, π] is a triangular fuzzy number. The expected value, value and
linear combinations of ambiguity and value are considered as ranking indices (see
[9], [16]), therefore it is justiﬁed to study the following set
τ = {R : F Δ(R) →R |R ([x0, ρ, π]) = ax0 + bρ + cπ }.
On the other hand, we introduce the sets
RanΔ =
%
R : F Δ(R) →R |R satisﬁes A′′
4
and ⪰R satisﬁes A1, A2, A3, A5, A6, A′
6, A7}
and

RanΔ =
%
R : F Δ(R) →R | ⪰R satisﬁes A1, A2, A3, A4, A′
4, A5, A6, A′
6, A7
&
.
We prove that not exist ranking indices in RanΔ and in 
RanΔ (making abstrac-
tion of equivalent orders over F Δ(R), see Remark 2), which do not belong to τ.
We mention here that the trapezoidal case was already treated in [3].
Therefore, it is very important to study in detail the set τ and we do that by
ﬁnding necessary and suﬀcient conditions for a, b, c such that R ∈τ can be used
to rank eﬃectively triangular fuzzy numbers. Because the order ⪰R, R ∈τ is
generated by a ranking index then A1, A2, A3 and A5 hold. Since one can easily
prove that R(δ + δ′) = R(δ) + R(δ′) for all R ∈τ and δ, δ′ ∈F Δ(R), it
results that A6 and A′
6 hold too. Therefore, it remains to ﬁnd necessary and
suﬀcient conditions such that properties A4, A′
4, A′′
4 and A7 would hold.

258
A.I. Ban and L. Coroianu
Theorem 1. Let R ∈τ, R ([x0, ρ, π]) = ax0 + bρ + cπ. The order ⪰R satisﬁes
A4 on F Δ(R) if and only if
a ≥c ≥0
(12)
and
a ≥−b ≥0.
(13)
Proof. (⇒) We consider particular cases for δ = [x0, ρ, π] and δ′ = [x′
0, ρ′, π′]
such that inf supp(δ) ≥sup supp(δ′) is satisﬁed, until we obtain that (12) and
(13) are satisﬁed. Note that since we have supposed that A4 holds it results that
δ ⪰R δ′ ⇔R(δ) ≥R(δ′). Also note that for all the choices of δ and δ′ we
obtain proper triangular fuzzy numbers.
Firstly, let us consider the particular case when x0 > x′
0 > 0 and ρ = π =
ρ′ = π′ = 0. Since R(δ) ≥R(δ′) implies ax0 ≥ax′
0 and since x0 > x′
0 > 0, it
is immediate that a ≥0.
Now, let us consider the particular case when x0 = ρ = π = x′
0 = π′ = 0.
Since R(δ) ≥R(δ′) implies bρ′ ≤0, it is immediate that b ≤0.
Consider now the case when x0 = ρ = x′
0 = ρ′ = π′ = 0 and π > 0. Since
R(δ) ≥R(δ′) implies cπ ≥0, we obtain c ≥0.
Now, we consider the case when x0 = π′ = 1 and ρ = x′
0 = ρ′ = π = 0. It is
immediate that from R(δ) ≥R(δ′) we get a −c ≥0.
Finally, we consider the case when x0 = ρ = 1, x′
0 = ρ′ = π′ = π = 0. It is
immediate that from R(δ) ≥R(δ′) we get a + b ≥0.
Collecting the inequalities obtained in the particular cases from above, we
obtain that (12) and (13) hold.
(⇐) Let a, b, c be real numbers satisfying (12) and (13) and let δ = [x0, ρ, π]
and δ′ = [x′
0, ρ′, π′] denote two arbitrary triangular fuzzy numbers such that inf
supp(δ) ≥sup supp(δ′). This immediately implies that x0 −x′
0 ≥ρ + π′ ≥0.
Noting that the hypothesis imply −bρ′ ≥0 and cπ ≥0, by direct calculations
we get
R(δ) −R(δ′)
= a(x0 −x′
0) + b(ρ −ρ′) + c(π −π′)
≥a(ρ + π′) + bρ −cπ′
= ρ(a + b) + π′(a −c) ≥0.
This implies δ ⪰R δ′ and the theorem is proved.
Theorem 2. Let R ∈τ, R([x0, ρ, π]) = ax0 + bρ + cπ. The order ⪰R satisﬁes
A′
4 if and only if
a ≥c ≥0
(14)
a ≥−b ≥0
(15)
and
a > 0.
(16)

Characterization of the Ranking Indices of Triangular Fuzzy Numbers
259
Proof. (⇒) In [3] we obtain that A′
4 is satisﬁed by ⪰R implies that A4 is satisﬁed
by ⪰R. Since A4 holds, by the previous theorem it results that for the direct
implication it suﬀces to prove that a > 0. For this purpose let us consider
δ = [x0, ρ, π] and δ′ = [x′
0, ρ′, π′] in the particular case when x0 = 1 and
ρ = π = x′
0 = ρ′ = π′ = 0. Clearly we have inf supp(δ) >sup supp(δ′),
therefore R(δ) > R(δ′), that is a > 0.
(⇐) Let a, b, c be real numbers such that (14)-(16) are satisﬁed and let δ =
[x0, ρ, π] , δ′ = [x′
0, ρ′, π′] denote two arbitrary triangular fuzzy numbers such
that inf supp(δ) >sup supp(δ′). Then it is easy to check that x0−x′
0 > ρ+π′ ≥
0. Then, from a > 0 we obtain
a(x0 −x′
0) > a(ρ + π′)
and this implies
R(δ) −R(δ′)
= a(x0 −x′
0) + b(ρ −ρ′) + c(π −π′)
> a(ρ + π′) + b(ρ −ρ′) + c(π −π′)
≥a(ρ + π′) + bρ −cπ′
= ρ(a + b) + π′(a −c) ≥0.
We obtain R(δ) > R(δ′), therefore δ ≻R δ′ and the proof is complete.
Theorem 3. The ranking index R ∈τ, R([x0, ρ, π]) = ax0 + bρ + cπ satisﬁes
A′′
4 if and only if
a = 1
(17)
b ∈[−1, 0]
(18)
and
c ∈[0, 1] .
(19)
Proof. (⇒) Let us notice that if A′′
4 holds then it is immediate that A′
4 holds
too. Therefore, comparing conditions (14)-(16) and (17)-(19), it follows that for
the direct implication of the present theorem it suﬀces to prove a = 1. Since
we have supposed that A′′
4 holds, it results that for any triangular fuzzy number
δ = [x0, ρ, π] , we have
x0 −ρ ≤ax0 + bρ + cπ ≤x0 + π.
(20)
Take x0 = 1 and ρ = π = 0. Replacing in (20) we get 1 ≤a ≤1 and thus we
obtain (17).
(⇐) Let a, b, c be real numbers such that (17)-(19) are satisﬁed and let δ =
[x0, ρ, π] denote a triangular fuzzy number. Conditions (17)-(19) imply
R(δ) = x0 + bρ + cπ
≤x0 + cπ ≤x0 + π

260
A.I. Ban and L. Coroianu
and
R(δ) = x0 + bρ + cπ
≥x0 + bρ ≥x0 −ρ.
From the two inequalities from above results that R(δ) ∈supp δ and the
theorem is proved.
Theorem 4. Let R ∈τ, R([x0, ρ, π]) = ax0 + bρ + cπ. The order ⪰R satisﬁes
A7 if and only if
b + c = 0.
(21)
Proof. (⇒) Let us choose δ = [0, 1, 1] and O = [0, 0, 0]. If R(δ) ≥R(O) then the
hypothesis imply R(−δ) ≤R(O). Since R(O) = 0 and −δ = δ, it immediately
follows that R(δ) = 0 that is b + c = 0. If R(δ) ≤R(O) then the reasoning is
similar therefore we omit the details.
(⇐) Let us consider the reals a, b, c such that b + c = 0. If δ = [x0, ρ, π]
then R(δ) = ax0 −cρ + cπ. To prove that A7 holds it suﬀces to prove that
the operator R is scale invariant. Now, if α ≥0 then one can easily prove that
R(α · δ) = αR(δ). Because
R (−δ) = −ax0 −cπ + cρ = −(ax0 −cρ + cπ) = −R (δ) ,
we immediately obtain R(α · δ) = αR(δ) for every δ ∈F Δ(R) and α ∈R.
Example 1. Delgado, Vila and Voxman [9] proposed the ranking index given by
ri(α, γ) (A) = αV al (A) + γAmb (A) ,
where α ∈[0, 1] and γ ∈[−1, 1] are given such that |γ| ≪α, γ representing the
decision-maker’s attitude against the uncertainty. If δ = [x0, ρ, π] is a triangular
fuzzy number then we get
ri(α, γ) (δ) = αx0 + γ −α
6
ρ + α + γ
6
π
and it is immediate that ri(α, γ) ∈τ for all choices of α and γ. From Theorems
1 and 2 we obtain ⪰ri(λ,δ) satisﬁes A4 and A′
4 for every α and γ. From Theorems
3 and 4 we obtain ⪰ri(λ,δ) satisﬁes A′′
4 if and only if α = 1 and A7 if and only if
γ = 0.
Example 2. A method of ranking fuzzy numbers with integral value as index
was proposed [14]. It becomes
Ik
Δ ([x0, ρ, π]) = x0 + k −1
2
ρ + k
2π,
where k ∈[0, 1] represents the degree of optimism. The order generated by Ik
Δ
satisﬁes A7 if and only if k = 1
2. If k ̸= 1
2 then the eﬃectiveness of the proposed
ranking method is at least debatable.

Characterization of the Ranking Indices of Triangular Fuzzy Numbers
261
The following important theorems, immediate consequences of the above the-
oretical results, characterize the element of τ which are in RanΔ or 
RanΔ.
Corollary 1. Let R ∈τ, R([x0, ρ, π]) = ax0 + bρ + cπ. Then R ∈RanΔ if and
only if
a = 1
(22)
b + c = 0
(23)
and
c ∈[0, 1] .
(24)
Proof. It is immediate taking into account that (17)-(19) and (21) have to be
satisﬁed simultaneously.
Corollary 2. Let R ∈τ, R([x0, ρ, π]) = ax0 + bρ + cπ. Then R ∈
RanΔ if and
only if a > 0, b = −c and a ≥c ≥0.
Proof. It is immediate taking into account that (14)-(16) and (21) have to be
satisﬁed simultaneously.
Example 3. ri(α, γ) ∈
RanΔ and hence ⪰ri(λ,δ) satisﬁes A1, A2, A3, A4, A′
4, A5,
A6, A′
6, A7 if and only if γ = 0. Moreover, if α < 1 then ri(α, γ) ∈
RanΔ \ RanΔ
and if α = 1 then ri(1, 0) = V al ∈RanΔ.
According to Corollary 1 it is easy to deduce that some already introduced
ranking indices are elements of RanΔ and implicitly they satisfy A1, A2, A3,
A4, A′
4, A5, A6, A′
6, A7.
Example 4. Let us consider the function EV : F Δ(R) →R which for any trian-
gular fuzzy number δ = [x0, ρ, π] associates its expected value that is EV (T ) =
x0 −1
4ρ + 1
4π. It is immediate that EV ∈RanΔ.
Example 5. In [10] a ranking procedure is proposed via so called valuation
functions. The authors consider a strictly monotonous function (valuation) f :
[0, 1] →[0, ∞) and a ranking index which on triangular fuzzy numbers becomes
R : F Δ(R) →R,
R([x0, ρ, π]) = (2 −Ω) x0 −1 −Ω
2
ρ + 1 −Ω
2
π,
where
Ω =
3 1
0 ϕf(ϕ)dϕ
3 1
0 f(ϕ)dϕ
.
Since 0 < Ω < 1 for any valuation f, by Theorems 1, 2 and 4 respectively, we
easily observe that R generates an order which satisﬁes A4, A′
4 and A7 and hence
R ∈
RanΔ for any valuation f. On the other hand by Theorem 3 we observe
that R /∈RanΔ. It is easily seen that R∗∈RanΔ where R∗=
1
2−ωR and ⪰R∗
is equivalent with ⪰R. We also observe that by taking f(ϕ) = ϕ, ϕ ∈[0, 1] we
obtain R∗= ri(1, 0) = V al.

262
A.I. Ban and L. Coroianu
Having in mind the results from above, it would be important to know whether
there exists any other ranking index R ∈RanΔ which does not belong to τ. The
answer to this question is negative and hence we can give now the main result
of this paper. The idea of the proof is the same as in the trapezoidal case [3].
Theorem 5. Let us consider a ranking index R : F Δ(R) →R. Then R ∈RanΔ
if and only if there exists c ∈[0, 1] such that for some δ ∈F Δ(R), δ = [x0, ρ, π],
we have
R(δ) = x0 −cρ + cπ.
(25)
Proof. Taking into account Corollary 1, it is easily seen that we can obtain the
desired conclusion by proving that R ∈RanΔ implies R ∈τ. Firstly, let us
observe that from Theorem 22 in [3] it results that R is linear on F Δ(R). Let
δ ∈F Δ(R), δ = (t1, t2, t3) where we used the form (5) because it is more
suitable for this proof. Now, let us consider the triangular fuzzy numbers
v1 = (0, 0, 1),
v2 = (0, 1, 1),
v3 = (1, 1, 1).
Having in mind the addition and the scalar multiplication of fuzzy numbers
we get that δ = t1v3 + (t2 −t1)v2 + (t3 −t2)v1. The linearity of R implies
R(δ) = t1R(v3) + (t2 −t1)R(v2) + (t3 −t2)R(v1). Returning now to the other
parametric representation of δ, that is δ = [x0, ρ, π] and taking into account
(6)-(8), we obtain R(δ) = x0R(v3) + ρ(R(v2) −R(v3)) + πR(v1). Clearly, this
last relation implies that R ∈τ and the proof is complete.
4
Conclusion
In applied sciences most often triangular or trapezoidal fuzzy numbers are used.
This is an important reason why we should investigate the theoretical properties
of these classes especially from application oriented purposes. In this paper,
we characterized the class of ranking indices over the set of triangular fuzzy
numbers which satisfy the reasonable criteria of Wang and Kerre [17]. This study
continues our interest on ranking of fuzzy numbers which started with paper
[3], where among others, the trapezoidal case was investigated. Recently, fuzzy
numbers of higher dimension have been proposed. In the paper [8] a special class
of piecewise linear fuzzy numbers was considered. These fuzzy numbers depend
on 6 parameters and recently a more general approach was proposed considering
piecewise linear fuzzy numbers depending on n parameters. Therefore it would
be interesting to extend the present study for such classes of fuzzy numbers.
Acknowledgments. The work of authors was supported by a grant of the Ro-
manian National Authority for Scientiﬁc Research, CNCS–UEFISCDI, project
number PN-II-ID-PCE-2011-3-0861.

Characterization of the Ranking Indices of Triangular Fuzzy Numbers
263
References
1. Abbasbandy, S., Hajjari, T.: A new approach for ranking of trapezoidal fuzzy
numbers. Computers and Mathematics with Applications 57, 413–419 (2009)
2. Asady, B., Zendehman, A.: Ranking fuzzy numbers by distance minimization.
Applied Mathematical Modelling 31, 2589–2598 (2007)
3. Ban, A.I., Coroianu, L.: Simplifying the search for eﬀective ranking of fuzzy num-
bers. IEEE Transactions on Fuzzy Systems (in press),
doi:10.1109/TFUZZ.2014.2312204
4. Chen, S.: Ranking fuzzy numbers with maximizing set and minimizing set. Fuzzy
Sets and Systems 17, 113–129 (1985)
5. Chen, L.H., Lu, H.W.: An approximate approach for ranking fuzzy numbers based
on left and right dominance. Computers and Mathematics with Applications 41,
1589–1602 (2001)
6. Choobineh, F., Li, H.: An index for ordering fuzzy numbers. Fuzzy Sets and Sys-
tems 54, 287–294 (1993)
7. Chu, T.-C., Tsao, C.-T.: Ranking fuzzy numbers with an area between the centroid
point and original point. Computers and Mathematics with Applications 43, 111–
117 (2002)
8. Coroianu, L., Gagolewski, M., Grzegorzewski, P.: Nearest piecewise linear approx-
imation of fuzzy numbers. Fuzzy Sets and Systems 233, 26–51 (2013)
9. Delgado, M., Vila, M.A., Voxman, W.: On a canonical representation of a fuzzy
number. Fuzzy Sets and Systems 93, 125–135 (1998)
10. Detynicki, M., Yagger, R.R.: Ranking fuzzy numbers using α−weighted valuations.
International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 8,
573–592 (2001)
11. Dubois, D., Prade, H.: Operations on fuzzy numbers. International Journal of Sys-
tems Science 9, 613–626 (1978)
12. Dubois, D., Prade, H.: The mean value of a fuzzy number. Fuzzy Sets and Sys-
tems 24, 279–300 (1987)
13. Heilpern, S.: The expected value of a fuzzy number. Fuzzy Sets and Systems 47,
81–86 (1992)
14. Liou, T.-S., Wang, M.-J.: Ranking fuzzy numbers with integral value. Fuzzy Sets
and Systems 50, 247–255 (1992)
15. Saeidifar, A.: Application of weighting functions to the ranking of fuzzy numbers.
Computers and Mathematics with Applications 62, 2246–2258 (2011)
16. Yager, R.R.: A procedure for ordering fuzzy subsets of the unit interval. Information
Sciences 24, 143–161 (1981)
17. Wang, X., Kerre, E.E.: Reasonable properties for the ordering of fuzzy quantities
(I). Fuzzy Sets and Systems 118, 375–385 (2001)

New Pareto Approach for Ranking Triangular
Fuzzy Numbers
Oumayma Bahri1,2, Nahla Ben Amor1, and Talbi El-Ghazali2
1 LARODEC Laboratory, ISG Tunis, Le Bardo, Tunisie
oumayma.b@gmail.com, nahla.benamor@gmx.fr
2 INRIA Laboratory, LIFL/CNRS, Villeneuve d’Ascq, Lille, France
el-ghazali.talbi@lifl.fr
Abstract. Ranking fuzzy numbers is an important aspect in dealing
with fuzzy optimization problems in many areas. Although so far, many
fuzzy ranking methods have been discussed. This paper proposes a new
Pareto approach over triangular fuzzy numbers. The approach is com-
posed of two dominance stages. In the ﬁrst stage, mono-objective dom-
inance relations are introduced and tested with some examples. In the
second stage, a Pareto dominance is deﬁned for multi-objective optimiza-
tion and then applied to solve a vehicle routing problem (VRP).
Keywords: Fuzzy ranking, Pareto approach, Triangular fuzzy numbers,
Mono-objective dominance, Multi-objective optimization, VRP.
1
Introduction
Real-world optimization problems are often subject to some uncertainty, which
must be taken into account. This uncertainty can takes diﬃerent forms of rep-
resentation, such as fuzzy numbers which are widely used in many applications,
since they oﬃer a suitable and natural way for expressing uncertain values. Nev-
ertheless, in practical applications, the comparison of fuzzy numbers becomes
an important and necessary procedure for decision makers. Therefore, several
methods for ranking fuzzy numbers have been suggested in literature. A review
and comparison of existing methods can be found in [8].
In addition, real optimization problems usually involve the simultaneous sat-
isfaction of more than one objective function and these objectives are generally
conﬂicting.
In such problems, commonly known as multi-objective, a set of
best solutions, called Pareto optimal, has to be determined. Over years, a sig-
niﬁcant number of resolution methods and techniques have been developed for
multi-objective optimization [16], while only few studies have been focused on ex-
tending multi-objective concepts to the uncertain optimization context [11]. This
paper addresses multi-objective optimization problems under uncertainty, in par-
ticularly, those containing fuzzy data represented by triangular fuzzy numbers.
To this end, a new Pareto approach for ranking the generated triangular fuzzy
solutions of such problems is proposed. The proposed approach is based ﬁrstly
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 264–273, 2014.
c
⃝Springer International Publishing Switzerland 2014

New Pareto Approach for Ranking Triangular Fuzzy Numbers
265
on the deﬁnition of new mono-objective dominance for ranking the triangular so-
lutions of each objective function and secondly on the use of this mono-objective
dominance for identifying the Pareto optimality in multi-objective case.
The remainder of this paper is organized as follows. Section 2 recalls the main
concepts on which the proposed approach is based. Section 3 presents the new
Pareto approach between triangular fuzzy numbers in both mono-objective and
multi-objective cases. The proposed approach is ﬁrst examined with numeri-
cal examples for mono-objective case and then illustrated on a multi-objective
vehicle routing problem with fuzzy demands, in Section 4.
2
Background
This section ﬁrst deﬁnes the fundamental notions of triangular fuzzy numbers
and then provides a brief background on multi-objective optimization.
2.1
Triangular Fuzzy Numbers
Among the diﬃerent shapes of fuzzy number, triangular fuzzy number (TFN) is
the most popular one, frequently used in various real world applications [21]. A
triangular fuzzy number A can be deﬁned as a normal fuzzy set represented by
a triplet [a, 4a, a], where [a, a] is the interval of possible values called its bounded
support and 4a denotes its kernel value (the most plausible value). This represen-
tation is characterized by a membership function μA(x) which assigns a value
within [0, 1] to each element in A. Its mathematical deﬁnition is given by:
μA(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x−a
a−a,
a ≤x ≤4a
1,
x = 4a
a−x
a−a,
4a ≤x ≤a
0,
otherwise.
(1)
However, in practical use of triangular fuzzy numbers, a ranking procedure
needs to be applied for decision-making. In other words, one triangular fuzzy
number needs to be evaluated and compared with the others in order to make
a choice among them.
Indeed, all possible topological relations between two
triangular fuzzy numbers A = [a, 4a, a] and B = [b,4b, b] may be covered by
only four diﬃerent situations, which are: Fuzzy disjoint, Fuzzy weak overlap-
ping, Fuzzy overlapping and Fuzzy inclusion [2]. These situations, illustrated
in Fig. 1 should be taken into account for ranking triangular fuzzy numbers.
In the literature, several diﬃerent methods of fuzzy rankings have been pro-
posed [1,3,4,5,7,9,10,19,20]. However, almost each ranking method may contain
some shortcomings, such as inconsistency with human intuition, requirement of
complicated calculations, diﬀculty of interpretation or indiscrimination in some
situations.

266
O. Bahri, N. Ben Amor, and T. El-Ghazali
Fuzzy Disjoint
Fuzzy Weak-Overlapping
Fuzzy Overlapping
Fuzzy Inclusion
Fig. 1. Possible topological situations for two TFNs
2.2
Multi-objective Optimization
Multi-objective optimization aims to optimize several conﬂicting objective func-
tions, simultaneously. Contrary to mono-objective optimization, multi-objective
optimization does not restrict to ﬁnd a unique single solution, but a set of best
solutions known as Pareto optimal set or Pareto front [16]. Therefore, obtaining
the Pareto front is the the main goal of solving a given multi-objective problem.
A classical multi-objective optimization problem (MOP), deﬁned in the sense
of minimization of all the objectives, is often formalized as follows:
Min F(x) = (f1(x), . . . , fn(x)) subject to x ∈S.
(2)
where x is a feasible solution from the decision space S and F(x) is the vector of
n independent objectives to be minimized. This vector can be deﬁned as a cost
function in the objective space by assigning an objective vector −→y = (y1, . . . , yn)
that represents the quality of each solution x.
An objective vector −→u = (u1, . . . , un) is said to Pareto dominate another
objective vector −→u ′ = (u′
1, . . . , u′
n) (denoted by −→u ≺p −→u ′) if and only if no
component of −→u ′ is smaller than the corresponding component of −→u and at
least one component of −→u is strictly smaller:
∀i ∈1, . . . , n : ui ≤u′
i ∧∃i ∈1, . . . , n : ui < u′
i.
(3)
A solution x∗∈S is said Pareto optimal (also known as non-dominated
or non-inferior) if for every x ∈S, F(x) does not dominate F(x∗), that is,
F(x) ⊀p F(x∗). Thus, the Pareto optimal set P ∗is deﬁned as:
P ∗= {x ∈X/∃x′ ∈X, F(x′) ⊀p F(x)}.
(4)
The image of this Pareto optimal set P ∗in the objective space is the Pareto
front PF ∗deﬁned as:
PF ∗= {F(x), x ∈P ∗}.
(5)
The above concepts are meant especially for deterministic multi-objective
optimization where the solutions are exact values.
Therefore, in the case of
uncertain multi-objective optimization, where the solutions become often uncer-
tain, these concepts will be addressed. Yet, most of proposed approaches for
handling multi-objective problems under uncertainty have been limited to treat
such problems in mono-objective context by considering the set of objectives as
if there is only one [13] or even in multi-objective context but with ignoration of

New Pareto Approach for Ranking Triangular Fuzzy Numbers
267
uncertainty propagation to the considered objectives using statistical properties
like expectation values [17]. Only few works have been focused on treating the
problem as-is without erasing any of its characteristics [11].
The issue we are interested in this paper is how handling multi-objective
problems with fuzzy data, consequently with triangular-valued objectives. To
this end, we introduce a new Pareto approach, reﬁned from our previous work
[14], in order to rank the generated triangular-valued solutions of such problems.
3
New Pareto Approach for Ranking Triangular Fuzzy
Numbers (TFNs)
In this section, we ﬁrst present new mono-objective dominance relations between
two TFNs. Then, based on these mono-objective dominance, we deﬁne a new
Pareto dominance between vectors of TFNs, for multi-objective case. Note that,
the minimization sense is considered in all our deﬁnitions.
3.1
Mono-objective Dominance Relations
In mono-objective case, three dominance relations over triangular fuzzy numbers
are deﬁned: Total dominance (≺t), Partial strong-dominance (≺s) and Partial
weak-dominance (≺w).
Deﬁnition 1 Total dominance
Let y = [y, 4y, y] ⊆R and y′ = [y′, 4y′, y′] ⊆R be two triangular fuzzy numbers.
y dominates y′ totally or certainly (denoted by y ≺t y′) if: y < y′.
This dominance relation represents the fuzzy disjoint situation between two tri-
angular fuzzy numbers and it imposes that the upper bound of y is strictly
inferior than the lower bound of y′ as shown by case (1) in Fig. 2.
1
μ
1
1
μ
2
Fig. 2. Total dominance and Partial strong-dominance
Deﬁnition 2 Partial strong-dominance
Let y = [y, 4y, y] ⊆R and y′ = [y′, 4y′, y′] ⊆R be two triangular fuzzy numbers.
y strong dominates y′ partially or uncertainly (denoted by y ≺s y′) if:
(y ≥y′) ∧(4y ≤y′) ∧(y ≤4y′).
This dominance relation appears when there is a fuzzy weak-overlapping between
both triangles and it imposes that ﬁrstly there is at most one intersection between
them and secondly this intersection should not exceed the interval of their kernel
values [4y, 4y′], as shown by case (2) in Fig.2.

268
O. Bahri, N. Ben Amor, and T. El-Ghazali
Deﬁnition 3 Partial weak-dominance
Let y = [y, 4y, y] ⊆R and y′ = [y′, 4y′, y′] ⊆R be two triangular fuzzy numbers.
y weak dominates y′ partially or uncertainly (denoted by y ≺w y′) if:
1. Fuzzy overlapping
[(y < y′) ∧(y < y′)]∧
[((4y ≤y′) ∧(y > 4y′)) ∨((4y > y′) ∧(y ≤4y′)) ∨((4y > y′) ∧(y > 4y′))].
2. Fuzzy Inclusion
(y < y′) ∧(y ≥y′).
In this dominance relation, the two situations of fuzzy overlapping and inclusion
may occur. Fig. 3 presents four examples of possible cases, where in (3) and (5)
y and y′ are overlapped, while, in (4) and (6) y′ is included in y.
1
μ
3
1
μ
4
1
μ
5
1
μ
6
Fig. 3. Partial weak-dominance
Yet, the partial weak-dominance relation cannot discriminate all possible cases
and leads often to some incomparable situations as for cases (5) and (6) in Fig.
3. These incomparable situations can be distinguished according to the kernel
value positions in fuzzy triangles. Thus, we propose to consider the kernel values
conﬁguration as condition to identify the cases of incomparability, as follows:
4y −4y′ =
< 0, y ≺w y′
≥0, y and y′ can be incomparable.
Subsequently, to handle the identiﬁed incomparable situations (with kernel con-
dition 4y −4y′ ≥0), we introduce another comparison criterion, which consists in
comparing the discard between both fuzzy triangles as follows:
y ≺w y′ ⇔(y′ −y) ≤(y′ −y)
Similarly, it is obvious that: y′ ≺w y ⇔(y′ −y) > (y′ −y).
It is easy to check that in the mono-objective case, we obtain a total pre-
order between two triangular fuzzy numbers, contrarily to the multi-objective
case, where the situation is more complex and it is common to have some cases
of indiﬃerence.

New Pareto Approach for Ranking Triangular Fuzzy Numbers
269
3.2
Pareto Dominance Relations
In multi-objective case, we propose to use the mono-objective dominance rela-
tions, deﬁned previously, in order to rank separately the triangular fuzzy solu-
tions of each objective function. Then, depending to the types of mono-objective
dominance founded for all the objectives, we deﬁne the Pareto dominance be-
tween the vectors of triangular fuzzy solutions. In this context, two Pareto dom-
inance relations: Strong Pareto dominance (≺SP ) and Weak Pareto dominance
(≺WP ) are introduced.
Deﬁnition 4 Strong Pareto dominance
Let −→y and −→y ′ be two vectors of triangular fuzzy numbers. −→y strong Pareto
dominates −→y ′ (denoted by −→y ≺SP −→y ′) if:
(a) ∀i ∈1, . . . , n : yi ≺t y′
i ∨yi ≺s y′
i.
(b) ∃i ∈1, . . . , n : yi ≺t y′
i ∧∀j ̸= i : yj ≺s y′
j
(c) ∃i ∈1, . . . , n : (yi ≺t y′
i ∨yj ≺s y′
j) ∧∀j ̸= i : yj ≺w y′
j.
f2
f1
(a)
f2
f1
(b)
f2
f1
(c)
f2
f1
(d)
Fig. 4. Strong Pareto dominance
The strong Pareto dominance holds if either yi total dominates or partial strong
dominates y′
i in all the objectives (Fig.4-(a): y1 ≺t y′
1 and y2 ≺t y′
2), either
yi total dominates y′
i in one objective and partial strong dominates it in an-
other (Fig.4-(b):y1 ≺s y′
1 and y2 ≺t y′
2 ), or at least yi total or partial strong
dominates y′
i in one objective and weak dominates it in another (Fig.4-(c),(d):
y1 ≺s y′
1 and y2 ≺w y′
2).
Deﬁnition 5 Weak Pareto dominance
Let −→y and −→y ′ be two vectors of triangular fuzzy numbers. −→y weak Pareto
dominates −→y ′ (denoted by −→y ≺WP −→y ′) if: ∀i ∈1, . . . , n : yi ≺w y′
i.
The weak Pareto dominance holds if yi weak dominates y′
i in all the objectives
(Fig.5-(e)). Yet, a case of indiﬃerence (deﬁned below) can occur if there is a
weak dominance with inclusion type in all the objectives (Fig.5-(f)).

270
O. Bahri, N. Ben Amor, and T. El-Ghazali
Deﬁnition 6 Case of indiﬀerence
Two vectors of triangular fuzzy numbers are indiﬀerent or incomparable (de-
noted by −→y ∥−→y ′) if: ∀i ∈1, . . . , n : yi ⊆y′
i.
f2
f1
(e)
f2
f1
(f)
Fig. 5. Weak Pareto dominance and Case of indiﬀerence
4
Results Analysis
4.1
Numerical Examples for Mono-objective Case
Here, we present some examples to illustrate the advantages of our mono-
objective dominance for ranking triangular fuzzy numbers and we compare our
results with the obtained results of some other ranking methods in order to
demonstrate its reasonability.
Example 1. Consider the two triangular fuzzy numbers A = [0.5, 3, 7] and B =
[1, 6, 10] in Fig.6-(1). The ranking order found by most of methods like Cheng’s
distance [4], Chu’s index [5], Wang’s centroid index [19] and kaufman’s left and
right scores [9], is A ≺B. By using our dominance method (Deﬁnition 3), it
is easy to check that A weak dominates B partially (A ≺w B). Therefore, the
ranking order in our case is the same as other tested methods (A ≺B).
Example 2. Consider the two triangular fuzzy numbers A = [0.1, 0.6, 0.8] and
B = [0.2, 0.5, 0.9] from [20] (see Fig.6-(2)). By using some ranking methods such
as Yao’s signed distance [20], Chu’s index [5] and Abbas’s sign distance [1], the
ranking order is A ≈B.
This is the shortcoming of previous methods that
rank two diﬀerent fuzzy numbers equally. However, by applying our dominance
method, we observe at the ﬁrst step, that the discrimination between A and B
is not possible using Deﬁnition 3, since the kernel condition gives 0.6 −0.5 ≥0.
At this level, we use the discard criterion (0.2 −0.1 = 0.9 −0.8) which leads to
conclude that A partial weak dominates B, and consequently A ≺B. The same
result is obtained by [10].
Example 3. Consider the two triangular fuzzy numbers A = [3, 6, 9] and B =
[5, 6, 7] from [1] (see Fig. 6-(3)). Almost the majority of ranking methods such as
[1,5,19,20] failed to discriminate two fuzzy numbers having the same symmetrical
spread, as for this example A ≈B, whereas Ezzati et al. [7] prefer the ranking
order B ≺A and consider this choice as reasonable result, since it agrees with

New Pareto Approach for Ranking Triangular Fuzzy Numbers
271
(1)
(2)
(3)
3
5
6
7
9
0.1
0.6
0.8
0.2
0.5
0.9
0.5
3
1
6
7
10
A
A
A
B
B
B
Fig. 6. Three examples of triangular fuzzy numbers A and B
human intuition. By using our dominance method, we conclude that A partial
weak dominates B, since the discard criterion gives: 5 −3 > 7 −9. Thus, we
obtain the same rational result B ≺A.
From these examples, we may conclude that our simple dominance method can
eﬃectively rank triangular fuzzy numbers and produces reasonable and intuitive
results to the well-deﬁned problems of indiscrimination, that have failed to be
ranked by some previous ranking methods.
4.2
Application to Multi-objective VRP with Fuzzy Demands
The Vehicle Routing Problem (VRP) is a diﬀcult and very well-known combi-
natorial optimization problem which has a large number of real-life applications
[18]. The classical VRP consists of ﬁnding the optimal routes used by a set of
identical vehicles, stationed at a central depot, to serve a given set of customers
geographically distributed, with known demands. In this paper, we are inter-
ested in a particular variant of VRP, the so-called Multi-objective VRP with
Time Windows and Uncertain Demands (MO-VRPTW-UD), in which a time
window is imposed on the visit of each customer and the customer’s demands
are supposed to be uncertain, expressed in our case by triangular fuzzy numbers
dm = [dm, 5
dm, dm]. The objective functions to be minimized are respectively,
the total traveled distance and total tardiness time. These two objectives will be
disrupted in our case by the used fuzzy form of uncertainty and so obtained as
triangular results. Therefore, in order to handle the generated triangular results,
we need to use our Pareto approach over triangular fuzzy numbers.
To solve the MO-VRPTW-UD, two well-known algorithms: the Strength
Pareto Evolutionary Algorithm SPEA2 [22] and the Non-dominated Sorting Ge-
netic Algorithm NSGAII [6], are adapted to our uncertain context by integrat-
ing the proposed Pareto approach. These algorithms are implemented with the
version 1.3-beta of ParadisEO framework under Linux [12]. Subsequently, to
validate our approach, we choose to use the Solomon’s benchmark, considered
as a basic reference for the evaluation of several VRP methods [15]. More pre-
cisely, six diﬃerent Solomon’s instances are used in our experimentation, namely,
C101, C201, R101, R201, RC101 and RC201. Yet, in these instances, all the in-
put values are exact and so the uncertainty of customer demands is not taken
into account. At this level, we propose to generate for each instance the tri-
angular fuzzy version of crisp demands in the following manner. Firstly, the
kernel value ( 5
dm) for each triangular fuzzy demand dm is kept the same as the
current crisp demand dmi of the instance. Then, the lower (dm) and upper

272
O. Bahri, N. Ben Amor, and T. El-Ghazali
(dm) bounds of this triangular fuzzy demand are uniformly sampled at random
in the intervals [50%dm, 95%dm] and [105%dm, 150%dm], respectively.
This
fuzzy generation manner ensures the quality and reliability of generated fuzzy
numbers. Finally, each of the six sampled fuzzy instances is tested on the both
algorithms, executed 30 times. Therefore, since 30 runs have been performed on
each algorithm, we obtained for each instance, 30 sets of optimal solutions that
represent the Pareto fronts of our problem. Each solution shows the minimal
traveled distance and tardiness time, which are represented by triangular fuzzy
numbers. Examples of Pareto fronts obtained for one execution of the instance
C101 using each algorithm, are shown in Fig. 7, where the illustrated Pareto
fronts are composed by a set of triangles, such that each triangle represents one
Pareto optimal solution. For instance, the bold triangular represents an optimal
solution with minimal distance (the green side) equal to [2413, 2515, 2623] and
tardiness time (the red side) equal to [284312, 295280, 315322].
SPEA2 Algorithm
NSGAII Algorithm
Fig. 7. Examples of Pareto fronts for C101 instance
5
Conclusion
In this paper, a new Pareto approach for handling multi-objective problems with
triangular fuzzy data is introduced. The approach is based on the deﬁnition of
new mono-objective dominance relations, that have been validated with some
examples. As practical application, a bi-objective vehicle routing problem with
uncertain demands is solved. For a future work, we intend to extend the proposed
approach for ranking diﬃerent fuzzy numbers (i.e. trapezoidal).
References
1. Abbasbandy, S., Asady, B.: Ranking of fuzzy numbers by sign distance. Informa-
tion Sciences 176(16), 2405–2416 (2006)
2. Boukezzoula, R., Galichet, S., Foulloy, L.: MIN and MAX operators for fuzzy
intervals and their potential use in aggregation operators. IEEE Transactions on
Fuzzy Systems 15(6), 1135–1144 (2007)
3. Chen, S.: Ranking fuzzy numbers with maximizing set and minimizing set. Fuzzy
Sets and Systems 17(3), 113–129 (1985)

New Pareto Approach for Ranking Triangular Fuzzy Numbers
273
4. Cheng, C.: A new approach for ranking fuzzy numbers by distance method. Fuzzy
Sets and Systems 95(3), 307–317 (1998)
5. Chu, T., Tsao, C.: Ranking fuzzy numbers with an area between the centroid point
and original point. Computers and Mathematics with Applications 43(1), 111–117
(2002)
6. Deb, K., et al.: A Fast Elitist Non-Dominated Sorting Genetic Algorithm for Multi-
Objective Optimization: NSGAII. In: Deb, K., Rudolph, G., Lutton, E., Merelo,
J.J., Schoenauer, M., Schwefel, H.-P., Yao, X. (eds.) PPSN 2000. LNCS, vol. 1917,
pp. 849–858. Springer, Heidelberg (2000)
7. Ezzati, R., Allahviranloo, T., Khezerloo, M.: An approach for ranking of fuzzy
numbers. Expert Systems with Applications 43(1), 690–695 (2012)
8. Bortolan, G., Degam, R.: A review of some methods for ranking fuzzy subsets.
Fuzzy Sets Systems 15, 1–19 (1985)
9. Kaufmann, A., Gupta, M.: Fuzzy Mathematical Models in Engineering and Man-
agement Science. Elsevier Science, New York (1988)
10. Boulmakoul, A., Laarabi, M.H., Sacile, R., Garbolino, E.: Ranking Triangular
Fuzzy Numbers Using Fuzzy Set Inclusion Index. In: Masulli, F. (ed.) WILF 2013.
LNCS (LNAI), vol. 8256, pp. 100–108. Springer, Heidelberg (2013)
11. Limbourg, P., Daniel, E.S.:
An Optimization Algorithm for Imprecise Multi-
Objective Problem Functions. Evolutionary Computation 1, 459–466 (2005)
12. Liefooghe, A., Basseur, M., Jourdan, L., Talbi, E.-G.:
ParadisEO-MOEO: A
Framework for Evolutionary Multi-objective Optimization. In: Obayashi, S., Deb,
K., Poloni, C., Hiroyasu, T., Murata, T., et al. (eds.) EMO 2007. LNCS, vol. 4403,
pp. 386–400. Springer, Heidelberg (2007)
13. Paquet, L.F.: Stochastic Local Search algorithms for Multi-objective Combina-
torial Optimization: A review. In: Handbook of Approximation Algorithms and
Metaheuristics, vol. 13 (2007)
14. Oumayma, B., Nahla, B.A., Talbi, E.G.: A Possibilistic Framework for Solving
Multi-objective Problems under Uncertainty. In: IPDPSW, pp. 405–414. IEEE
(2013)
15. Solomon, M.M.: Algorithms for the vehicle Routing and Scheduling Problem with
Time Window Constraints. Operations Research 35(2), 254–265 (1987)
16. Talbi, E.-G.: Metaheuristics: from design to implementation, vol. 74, pp. 309–373.
John Wiley & Sons (2009)
17. Teich, J.: Pareto Front Exploration with Uncertain Objectives. In: Zitzler, E.,
Deb, K., Thiele, L., Coello Coello, C.A., Corne, D.W. (eds.) EMO 2001. LNCS,
vol. 1993, pp. 314–328. Springer, Heidelberg (2001)
18. Toth, P., Vigo, D.: The vehicle routing problem, vol. 9. Siam (2002)
19. Wang, Y.: Centroid defuzziﬁcation and the maximizing set and minimizing set
ranking based on alpha level sets. Computers and Industrial Engineering 57(1),
228–236 (2009)
20. Yao, J., Wu, K.: Ranking fuzzy numbers based on decomposition principle and
signed distance. Fuzzy Sets and Systems 116(2), 275–288 (2000)
21. Zadeh, L.A.: Fuzzy Sets. Information and Control 8(3), 338–353 (1965)
22. Zitzler, E., Laumans, M., Thiele, L.: SPEA2: Improving the strength Pareto
evolutionary algorithm (2001)

MI-groups: New Approach⋆
Martin Bacovsk´y
University of Ostrava
Faculty of Science
Department of Mathematics
30. dubna 22, 701 03 Ostrava, Czech Republic
martin.bacovsky@osu.cz
http://www.osu.cz
Abstract. The notion of MI-group introduced in [1], [2] and later on
elaborated in [3] is redeﬁned and its structure analysed. In our ap-
proach, the role of the “Many Identities” set is replaced by an involutive
anti-automorphism. Every ﬁnite MI-group coincides with some classical
group, whilst inﬁnite MI-groups comprise two parts: a group part and a
semigroup part.
Keywords: many identities group, algebraic structures, fuzzy numbers,
involutive anti-automorphism.
1
Introduction
In the work [1] of Holˇcapek and ˇStˇepniˇcka, the MI-algebras (MI stands for “Many
Identities”) were introduced. Their axioms are generalization of arithmetics with
extensional numbers (for more details, again see [1]). The term MI refers to ex-
pressions a+(−a), which are not generally equal to zero element. These elements
are then grouped together to create the set E of pseudoidentities. Many results
similar to those known from the classical group theory were proven, so that one
might think about the real diﬃerence between MI-groups and groups. Our paper
aims to answer this question.
In doing so, it is convenient to redeﬁne the notion of MI-groups as introduced
in [3] to simplify the reasoning about its properties. After redeﬁnition we will
show that an MI-group can be viewed as a conglomeration of some group and
some semigroup. The number of MI-groups constructed over groups depends
solely on the number of involutive anti-automorphisms that can be constructed
over them. For the case of ﬁnite cyclic groups, these were described in [4] as
an answer to problems arising in the analysis of processor networks (see e.g.
[5]). Surprisingly, many of the techniques utilized in [4] are also very fruitful in
our approach. Next, we ﬁnd out that the only pseudoinversion applicable to the
most prominent monoid present in almost every fuzzy numbers model, namely
R+
0 , is the identity. In the last part, one example illustrates usefulness of such
approach.
⋆This work was supported by grant SGS13/PˇrF/2014 of the University of Ostrava.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 274–283, 2014.
c
⃝Springer International Publishing Switzerland 2014

MI-groups: New Approach
275
By N, Z and R we denote the set of positive integers, integers and real num-
bers, respectively. Subscript 0 means the neutral element is added, e.g. N0 is
the set of non-negative integers, and the superscript + denotes the restriction
to positive numbers, e.g. R+ denotes the set of positive reals. When integer m
divides integer n we write m|n, by a ≡b mod m is denoted that a is congruent
to b mod m, when k is the result of l modulo m we write k = l mod m. The
subtraction of sets A and B is denoted by A −B. For a non-empty set A, all
ﬁnite sequences over A are denoted by Af.
2
Redeﬁnition of MI-group
We start this section with the redeﬁnition of MI-group notion as introduced
in [3].
Deﬁnition 1. A quadruplet G = (G, ◦, ω, e) is said to be an MI-group if for all
x, y, z ∈G the following hold:
(M1) x ◦(y ◦z) = (x ◦y) ◦z,
(M2) x ◦e = e ◦x = x,
(I1) ω(x ◦y) = ω(y) ◦ω(x),
(I2) ω(ω(x)) = x,
(CL) if x ◦z = y ◦z or z ◦x = z ◦y then x = y,
i.e., if triplet (G, ◦, e) is a monoid (axioms (M1), (M2)), ω is an involutive
anti-automorphism (axioms (I1), (I2)) and the cancellation law is satisﬁed for
◦(axiom (CL)).
The axioms in the deﬁnition try to mimic the properties of the classical group
(see [6]) with inverse replaced by pseudoinverse, i.e., the ω(a) ◦a and a ◦ω(a)
may diﬃer from the identity e.
Proposed deﬁnition simpliﬁes the old one in the way that it allows more
objects to be considered as MI-groups (see ex. 4). It is also more convenient,
since when we try to construct an MI-group, we usually start ﬁrst with some
given carrier set G and operation ◦and we are looking for a ﬁtting ω. Then the
set E of pseudoidentities (see [3]) can be deﬁned appropriately with respect to ω.
In fact, E can be viewed just as a constraint restricting the set of all applicable
ω. The deﬁnition 1 is used throughout this paper as a deﬁnition of MI-group.
We will omit the symbol ◦in x ◦y and write the multiplication simply as xy
even for ◦restricted to some A ⊂G. ω will be often called a pseudoinversion.
The structure of MI-groups can be very close to a group one, thus we extract
the greatest group-like structure and study its properties with respect to the
whole MI-group. We will denote
ΔG = {x ∈G : (∃y ∈G)(x ◦y = e)}
and
ΘG = G −ΔG = {x ∈G : (∀y ∈G)(x ◦y ̸= e)}.

276
M. Bacovsk´y
Then we can enrich ΔG by the classical group inversion, while ΘG represents
the semigroup part of G.
Notice, that by associativity (M1) the result of ◦does not depend on the oper-
ations order, e.g. a(b(cd)) = (ab)(cd), so we simply write abcd without explicitly
stating the order. Next, e /∈ΘG, ω(e) = e (eω(x) = ω(x) = ω(xe) = ω(e)ω(x) and
apply the cancellation law) and when xy = e, then yx = e, i.e., both x, y ∈ΔG
(xy = e, (yx)2 = y(xy)x = yx and the proposition again follows from the can-
cellation law). When the MI-group G is not important, we will write only Δ
and Θ.
Lemma 1. Δ is closed under ◦and ω.
Proof. Let x, y ∈Δ and ˜x, ˜y ∈Δ be their group inversions, respectively. Then
(xy)(˜y˜x) = xy˜y˜x = xe˜x = x˜x = e,
hence xy ∈Δ. For ω it follows from ω(x)ω(˜x) = ω(˜xx) = ω(e) = e, thus ω(x) ∈Δ.
Lemma 2. Θ is closed under ◦, ω and multiplication by elements from Δ. In
symbols: ΘΘ ⊆Θ, ω(Θ) = Θ and ΘΔ ∪ΔΘ ⊆Θ.
Proof. Let x, y ∈Θ. To show that xy ∈Θ, let us consider the set Y = {yz : z ∈
G}. Note that e /∈Y . From x ∈Θ it follows that xw ̸= e for w = yz ∈Y and by
associativity xw = x(yz) = (xy)z ̸= e for all z ∈G.
For x ∈Δ, y ∈Θ and all z ∈G we have: (yx)z = y(xz) ̸= e and (xy)z =
x(yz) ̸= e, since by lemma 1 the group inversion of x lies in Δ, but yz from the
preceding step lies in Θ.
For x ∈Θ, y, z ∈G such that y = ω(z), we compute
ω(x)y = ω(x)ω(z) = ω(zx) ̸= e.
Hence Θ is closed under ω.
By the preceding lemmas, group and semigroup part of an MI-group are
self-contained algebraic structures, while their interactions ΘΔ always fall into
semigroup part. This property substantially separates MI-groups from groups
and semigroups and thus justiﬁes studying MI-groups.
Lemma 3. For x ∈Θ it holds that xn ̸= x and xn ̸= ω(x) for n ∈N, n > 1.
Proof. Let x ∈Θ and n ∈N such that xn = x. Then by the cancellation law
xn−1 = e and from x ∈Θ it follows that n = 1.
For the second part, suppose that ω(x) = xn. Then
x = ω(ω(x)) = ω(xn) = ω(x)n = xn2,
and by the ﬁrst part we get n = 1.
Corollary 1. When Θ is nonempty, it has inﬁnitely many elements.

MI-groups: New Approach
277
To study genuine MI-groups (those that can not be viewed as a group), one
shall restrict her studies to structures with group and semigroup parts non-
empty, i.e., by the corollary, necessarily to inﬁnite MI-groups. However, we will
in this paper also consider ﬁnite MI-groups, which can always be viewed as
groups, to ﬁnd out the diﬃerences between the two.
Following deﬁnition of homomorphisms diﬃers from that in [3] in the fact
there are no constraints regarding a set of pseudoidentities E.
Deﬁnition 2. A mapping f : G →H is said to be an MI-group homomorphism
of MI-groups G = (G, ◦G, μ, eG) and H = (H, ◦H, ω, eH) (shortly, a homomor-
phism) if it satisﬁes following conditions for all x, y ∈G:
(H1) f(x ◦G y) = f(x) ◦H f(y),
(H2) f(μ(x)) = ω(f(x)).
When f is an injective mapping, we will call it a monomorphism.
Lemma 4. A homomorphism f : G →H restricted to ΔG maps into ΔH and,
moreover, denoting x−1A a group inversion in ΔA for some MI-group A,
f(x−1G) = f(x)−1H.
Proof. Let x ∈ΔG. Then
f(eG) = f(xx−1G) = f(x)f(x−1G) = eH,
thus f(x) ∈ΔH and the last statement follows directly.
Lemma 5. f : ΔG →ΔH is a monomorphism if and only if f(x) = eH impli-
cates x = eG.
Proof. The only if direction is clear, we prove the if direction. If f(x) = f(y),
then also
eH = f(x)f(y)−1H = f(x)f(y−1G) = f(xy−1G),
thus xy−1G = eG and x = y.
In [3] a deﬁnition of a kernel homomorphism was proposed. Without going
into much detail, in usual cases it can be considered as preimages of elements
of the set {xμ(x) : x ∈H}. Nevertheless, considering G = (N0, +, id, 0) and
G2 = (2N0, +, id, 0) together with mapping f : N0 →2N0 given by prescription
f(x) = 4x, f maps onto {4x : x ∈N0} and is clearly a monomorphism, but
the kernel is the whole N0 and thus, we can not use kernels deﬁned in [3] to
determine whether a homomorphism is injective. The problem of introducing
similar notion to that of kernel is left open.
We end this section by the very basic examples of MI-groups.
Example 1. Gk = (kN0, +, id, 0), where k ∈N0 and kN0 = {kn | n ∈N0}. For
k > 0 the Gk is an MI-group, but can not be considered as a group. There
always exists a non-unique monomorphism between Gk and Gl for k, l ̸= 0,
namely f : kN0 →lN0, f(k) = nl for a ﬁxed n ∈N.
Every group is an MI-group, and every abelian group has at least two possible
pseudoinversions (when they do not coincide, e.g. Z2) the classical inversion and
the identity one. Other examples will be elicited in next sections.

278
M. Bacovsk´y
3
MI-groups over Finite Cyclic Groups
In this section, we consider MI-groups constructed over ﬁnite cyclic groups Zn,
i.e., with diﬃerent pseudoinversions. In this case, every homomorphism and con-
sequently every pseudoinversion is completely described when the image of one of
the group generators is known, w.l.o.g. we will take 1. Deﬁne ξ : N →{−1, 0, 1}
by
ξ(n) =
⎧
⎨
⎩
1
if 8|n,
−1 if 2|n and 4 ∤n,
0
otherwise.
In [4] the following theorem was proved.
Theorem 1. Let n = pm1
1 pm2
2
. . . pmr
r
be a prime decomposition of n with pi <
pi+1 for 1 ≤i ≤r −1, and mi > 0 for 1 ≤i ≤r. Then the number of involutive
automorphisms of Zn equals 2r+ξ(n).
For Z8 there are four pseudoinversions: ω1(1) = 1, ω2(1) = 3, ω3(1) = 5,
ω4(1) = 7. ω1 is the identity, ω4 the group inversion, and ω2 with ω3 are new
pseudoinversions. We will denote Zn,l an MI-group constructed over the group
Zn with pseudoinversion ω(1) = l. Note, that by the corollary 1 ﬁnite MI-groups
always allow such a pseudoinversion that is in fact an inversion, as is the case of
ω4 for Z8.
It is well known (see e.g. [6]), that two ﬁnite cyclic groups Zm and Zn for
m < n have a monomorphism f : Zm →Zn iﬃm|n, namely f(1) =
n
m. In the
case of MI-groups, this condition is necessary but not suﬀcient.
Lemma 6. A monomorphism f : Zm,k →Zn,l exists if and only if m|n and
k ≡l (mod m).
Proof. The monomorphism must map 1 →
n
m for the same reasons as in the
case of classical groups. Denote by ωk and ωl pseudoinversions of Zm,k and Zn,l,
respectively. We have to verify, that (H2) from the deﬁnition of homomorphism
is satisﬁed, i.e., that
f(ωk(1)) = f(k) = k n
m,
ωl(f(1)) = ωl
# n
m
$
= l n
m
are equal modulo n. The equivalence
n|(k −l) n
m ⇔k −l ≡0
(mod m)
completes the proof.
Lemma 7. For an MI-group Zn,l all injectively insertable MI-groups into Zn,l
constructed over ﬁnite cyclic group form a lattice (L, ∨, ∧, 0, 1), where

MI-groups: New Approach
279
– L is the set of all MI-groups Zm,k such that m|n and k ≡l mod m,
– Zm1,k1 ∨Zm2,k2 = Zm, l mod m, where m = lcm(m1, m2),
– Zm1,k1 ∧Zm2,k2 = Zm, l mod m, where m = gcd(m1, m2),
– 0 = Z1,0, the trivial group,
– 1 = Zn,l.
Proof. From the lemma 6 and the Chinese Remainder Theorem (see [6]), given
the image MI-group Zn,l and the domain group Zm, there is at most one possible
pseudoinversion ω(1) = k (k < m) such that a monomorphism f : Zm,k →Zn,l
exists, namely k = l mod n. Hence for every divisor m of n there is exactly one
k such that Zm,k is injectively insertable into Zn,l.
The statement then follows from the well known fact that the divisors of m
compose a lattice with respect to operations gcd and lcm.
Example 2. For the group Z3·5·7 there are eight possible pseudoinversions:
ω(1) = 1, 29, 34, 41, 64, 71, 76, 104.
In the Fig. 1, the lattice from the previous lemma is drawn for the choice ω(1) =
41. It is readily seen, that there is a one-to-one correspondence between elements
of this lattice and the lattice of all subgroups of the group Z3·5·7. For other
pseudoinversions the lattices are similar. From this point of view, the structure
of a ﬁnite MI-group is not essentially diﬃerent from its group structure.
Z3·5·7,41
Z5·7,6
u
u
u
u
u
u
u
u
u
Z3·7,20
Z3·5,11
JJJJJJJJJ
Z7,6
u
u
u
u
u
u
u
u
u
Z5,1
IIIIIIIII
u
u
u
u
u
u
u
u
u
u
Z3,2
IIIIIIIII
Z1,0
IIIIIIIII
u
u
u
u
u
u
u
u
u
u
Fig. 1. Lattice of all injectively insertable MI-groups into Z3·5·7,41 with groups under-
lined
4
MI-groups with Non-empty Semigroup Part
In this section, properties of monoids important from the MI-groups point of
view are presented.

280
M. Bacovsk´y
Lemma 8. For given MI-group G, if ΘG is nonempty, then ω has at least one
ﬁx point in ΘG.
Proof. Let x ∈ΘG. Then
ω(xω(x)) = ω(ω(x))ω(x) = xω(x).
Since ΘG is closed under ω and multiplication, xω(x) ∈ΘG.
Following lemma has left and right variant for non-commutative monoids. We
prove only the left variant, the right can be proved similarly.
Lemma 9. Let G = (G, ◦, e) be a monoid and ω a pseudoinversion such that
whenever ω(a) = a ◦c for some a, c ∈G, then a ̸= d ◦ω(a) for any d ∈G, d ̸= e.
Then c = e.
Proof. When ω(a) = a ◦c, then also ω(ω(a)) = a = ω(c) ◦ω(a) and by the
assumption must be ω(c) = e, which concludes that c must be in fact e.
Corollary 2. The only pseudoinversion on R+
0 is the identity.
Proof. Take a ∈R+
0 , then either ω(a) ≥a or a ≥ω(a), where ≥is a natural
ordering on R+
0 . In the ﬁrst case, there exists exactly one b ∈R+
0 so that ω(a) =
a + b holds. Then lemma 9 implies b = 0. For the second case put a′ = ω(a) and
apply the ﬁrst part for a′.
Lemma 9 may seem to be inappropriate for the preceding corollary. On R+
0
one could also deﬁne strict linear ordering < for a ̸= b by
a < b iﬃ∃c ∈R+ : b = a + c
and the statement would follow from (c is some suitable element from R+)
a < ω(a) ⇔ω(a) = a + c ⇔a = ω(a) + ω(c) ⇔a > ω(a), a contradiction.
However, following example shows that there are monoids on which such linear
ordering is not realizable. Based on this observation, we have chosen presented
form of lemma 9.
Example 3. Let Gα = (R+
0 ∪{e}, ⊕α, e) be a commutative monoid with
a ⊕α b = a + b + ϕ, e ⊕α a = a ⊕α e = a,
where a, b ∈R+
0 and ϕ ∈R+. Then take a, b ∈R+
0 such that |a −b| < ϕ.
Neither a = b ⊕α c = b + c + ϕ nor b = a ⊕α d = a + d + ϕ may hold for any
c, d ∈R+
0 . On the other hand, it is easily seen, that when ω(a) = a⊕α b for some
a, b ∈R+
0 ∪{e} it can not hold a = ω(a) ⊕α b for b ̸= e and, subsequently, b = e
and ω is the identity mapping on R+
0 . This together with satisﬁed cancellation
law (CL) shows that Gα can be extended to an MI-group with only one possible
pseudoinversion — the identity.

MI-groups: New Approach
281
Although R+
0 makes possible only identity pseudoinversion, on the product
R+
0 ×R+
0 with component-wise addition and neutral element (0, 0) one can besides
identity pseudoinversion deﬁne a swapping pseudoinversion ω((a, b)) = (b, a),
since e.g. (1, 2) and (2, 1) are not convertible to each other by addition on R+
0 ×
R+
0 .
Next example shows that there exist non-commutative MI-groups for which
need not be ∀a ∈G : a ◦ω(a) = ω(a) ◦a (axiom (G2) in [3] that is absent in our
deﬁnition).
Example 4. Consider ﬁnite sequences over a set {0, 1} with an operation ◦as
concatenation. Then ◦is associative, non-commutative and allows a cancellation.
The identity element can be added as an empty word Φ. To complete its struc-
ture as MI-group, we have to deﬁne involutive anti-automorphism. The identity
id(s) = s for s ∈{0, 1}f is not suitable, since
id(01) = 01 ̸= 10 = id(1) id(0).
On the other hand, it is easily seen that it suﬀces to deﬁne ω just on generators
0, 1 and extend it anti-homomorphically to other sequences. The only options are
”negation” ωn(0) = 1, ωn(1) = 0 and ”identity” ω(x) = x for x ∈{0, 1}. Thus,
we constructed MI-group W = ({0, 1}f ∪{Φ}, ◦, ωn, Φ) with a ◦ωn(a) ̸= ωn(a) ◦a,
e.g.
0 ◦ωn(0) = 01 ̸= ωn(0) ◦0 = 10.
By lemma 8, there is at least one ﬁx point in {0, 1}f for ωn. Indeed, sequences
01 and 10 are such ﬁx points.
One may wonder, if the statement of lemma 9 would be true in the case not
a ̸= d ◦ω(a), but a ̸= ω(a) ◦d for all d ̸= e. Following example disproves this
idea.
Example 5. Consider ﬁnite sequences with elements from {a, b, c, d}, the oper-
ation concatenation and neutral element as in the previous example with two
relations
dc = abcd, ab = badc.
We mean that one can instead of a sequence dc write abcd, i.e., a sequence twice
longer in the number of elements. Deﬁne ω as ω(a) = c and ω(b) = d and extend
it anti-automorphically to all ﬁnite sequences. Then ω(ab) = dc = abcd and
ab ̸= ω(ab) ◦ϕ for any ﬁnite sequence ϕ, since ω(ab) ◦ϕ = abcd ◦ϕ and there is
no way how to get rid of cd on the right-hand side. However, ab = π ◦ω(ab) has
a solution, since ab = badc, thus put π = ba and the equality follows.
5
Trapezoidal Fuzzy Numbers as MI-groups
As a main example we choose trapezoidal fuzzy numbers, since they generalize
interval fuzzy numbers together with triangular fuzzy numbers, the two most

282
M. Bacovsk´y
common fuzzy number models. Trapezoidal fuzzy numbers compose in fact an
MI-group with non-empty semigroup part (Θ ̸= ∅) and as it will be shown it is
this part, that plays the role of vagueness.
For a, b, c, d ∈R and a ≤b ≤c ≤d such a fuzzy number A = (a, b, c, d) is
deﬁned as
A(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0
if x < a or d ≤x,
(x −a)/(b −a) if a ≤x ≤b,
1
if b ≤x ≤c,
(d −x)/(d −c) if c ≤x < d.
Addition is deﬁned component-wise:
A +τ E = (a, b, c, d) +τ (e, f, g, h) = (a + e, b + f, c + g, d + h),
0τ = (0, 0, 0, 0) is the neutral element and pseudoinversion −τ(a, b, c, d) =
(−d, −c, −b, −a). Using the bijection (a, b, c, d) →( b+c
2 , c −b, b −a, d −c) which
maps {(a, b, c, d) | a ≤b ≤c ≤d, a, b, c, d ∈R} ⊂R4 onto T = R × (R+
0 )3 we
can view trapezoidal fuzzy numbers as MI-group T = (T, +T , −T, 0τ) with
A +T E = (a, b, c, d) +T (e, f, g, h) = (a + e, b + f, c + g, d + h),
−T A = −T (a, b, c, d) = (−a, d, c, b).
Notice that −T is now not constructable as a product of corresponding pseu-
doinversions acting only on R and R+
0 , since it also swaps components.
The R part is indeed a group, while the rest (R+
0 )3 is a monoidal part with
no chance to introduce a group inversion. The set
S = {(0, a, b, a) | a, b ∈R+
0 }
consists of symmetrical elements, i.e., for any x ∈S the −T x = x holds. In the
literature sometimes the equivalence ∼T for a, b ∈T
a ∼T b ⇔∃sa, sb ∈S : a +T sa = b +T sb
is proposed (see [7]). In this case of T and S the equivalence classes with respect
to ∼T are isomorphic to group R× R via the isomorphism (a, b, c, d) →(a, b−d)
(remember that we can take arbitrary representant of the equivalence class).
Thence, vagueness corresponding to b and d is inserted into the additive group
R by b −d and vagueness represented by c is completely forgotten.
6
Conclusion
We redeﬁned the notion of an MI-group to obtain more insight into its structure.
Theoretically, it may be described partly as a group and partly as a semigroup.
Practically, semigroup structure is often present in fuzzy numbers models and
as such it is a source of vagueness. Since MI-groups are generalization of groups,
some analogue theorems from groups are provable only under very restrictive
conditions. Instead of trying to make these MI-groups as similar as possible

MI-groups: New Approach
283
to groups, we tried to point out the main diﬃerences. For the ﬁnite MI-groups
constructed over cyclic groups these were not signiﬁcant. The situation is more
interesting for MI-groups with inﬁnite carrier set. Since there is just one pseu-
doinversion for the most prominent monoid utilized in fuzzy numbers, namely
R+
0 , our next research will concern other models of fuzzy numbers and pseudoin-
versions not constructable as direct products of elementary pseudoinversions over
the underlying carrier sets, as shown in the penultimate section.
References
1. Holˇcapek, M., ˇStˇepniˇcka, M.: Arithmetics of extensional fuzzy numbers – part I:
Introduction. In: Proc. IEEE Int. Conf. on Fuzzy Systems, Brisbane, pp. 1517–1524
(2012)
2. Holˇcapek, M., ˇStˇepniˇcka, M.: Arithmetics of extensional fuzzy numbers – part II:
Algebraic framework. In: Proc. IEEE Int. Conf. on Fuzzy Systems, Brisbane, pp.
1525–1532 (2012)
3. Holˇcapek, M., ˇStˇepniˇcka, M.: MI-algebras: A new framework for arithmetics of
(extensional) fuzzy numbers. Fuzzy Sets and Systems (2014),
http://dx.doi.org/10.1016/j.fss.2014.02.016
4. Hage, J., Harju, T.: On Involutive Anti-Automorphisms of Finite Abelian Groups.
Technical UU WINFI Informatica en Informatiekunde (2007)
5. Hage, J., Harju, T.: The size of switching classes with skew gains. Discrete Math. 215,
81–92 (2000)
6. Dummit, D.S., Foote, R.M.: Abstract Algebra. Wiley (2003)
7. Mareˇs, M.: Computation over Fuzzy Quantities. CRC Press, Boca Raton (1994)

On Combining Regression Analysis
and Constraint Programming⋆
Carmen Gervet and Sylvie Galichet
LISTIC, Laboratoire d’Informatique, Systems, Traitement de l’Information et de la
Connaissance - Universit de Savoie, BP 80439
74944 Annecy-Le-Vieux Cedex, France
{gervetec,sylvie.galichet}@univ-savoie.fr
Abstract. Uncertain data due to imprecise measurements is commonly
speciﬁed as bounded interval parameters in a constraint problem. For
tractability reasons, existing approaches assume independence of the pa-
rameters. This assumption is safe, but can lead to large solution spaces,
and a loss of the problem structure. In this paper we propose to com-
bine the strengths of two frameworks to tackle parameter dependency
eﬀectively, namely constraint programming and regression analysis. Our
methodology is an iterative process. The core intuitive idea is to ac-
count for data dependency by solving a set of constraint models such
that each model uses data parameter instances that satisfy the depen-
dency constraints. Then we apply a regression between the parameter
instances and the corresponding solutions found to yield a possible rela-
tionship function. Our ﬁndings show that this methodology exploits the
strengths of both paradigms eﬀectively, and provides valuable insights to
the decision maker by accounting for parameter dependencies.
Keywords: Data uncertainty, Constraint
Programming,
regression
analysis.
1
Introduction
Data uncertainty due to imprecise measurements or incomplete knowledge is
ubiquitous in many real world applications, network design, renewable energy
investment planning, and inventory management, to name a few. Regression
analysis is one of the most widely used statistical techniques to model and rep-
resent the relationship among variables to describe or predict phenomena for a
given context. Recently, models derived from fuzzy regression have been deﬁned
to represent incomplete and imprecise measurements in a contextual manner, us-
ing intervals [2]. Such models apply to problems in ﬁnance or complex systems
analysis in engineering whereby a relationship between crisp or fuzzy measure-
ments is sought.
Constraint Programming (CP) on the other hand, is a powerful paradigm
used to solve decision and optimization problems in areas as diverse as plan-
ning, scheduling, routing. The CP paradigm models a decision problem using
⋆This research was partly support by the Marie Curie CIG grant, FP7-332683.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 284–293, 2014.
c
⃝Springer International Publishing Switzerland 2014

On Combining Regression Analysis and Constraint Programming
285
constraints to express the relations between variables, and propagates any infor-
mation gained from a constraint onto other constraints. When data imprecision
is present, forms of uncertainty modeling have been embedded into constraint
models using bounded intervals to represent such parameters. For instance if we
model traﬀc in a network, the ﬂow over links between two routers cannot be
measured with precision as it depends on the collected traﬀc volume data at each
router which cannot be synchronized. As a result, the data information obtained
is erroneous. We use the sigcomm4 network given in Fig. 1 as a running example
taken from [8]. On link A →C, for example, the traﬀc volume might measure as
565 at A and as 637 at C, whereas the true value, equal at both nodes, is presum-
ably somewhere in between. Thus, the ﬂow between A and C will be speciﬁed by
565.0..637.0. The mean values are shown in the ﬁgure. Such models are used to
ﬁnd the ﬂow matrix between any pair of routers. Commonly ﬂow distribution is
considered between 30 ad 70 percent. Thus on the link A →C the ﬂow constraint
is speciﬁed by: 0.3..0.7 ∗FAC + 0.3..0.7 ∗FBC + 0.3..0.7 ∗FAB + 0.3..0.7 ∗FAD =
565.0..637.0. A variable FAD denotes the traﬀc (unknonwn) from router A to
router D.
Fig. 1. Sigcomm4 network topology and mean values for link traﬃc
Traditional models either omit any routing uncertainty for tractability rea-
sons, and consider solely the shortest path routing or embed the uncertain pa-
rameters but with no dependency relationships. Values for the ﬂow variables are
derived by computing bounded intervals, which are safe enclosing of all possible
solutions. Such intervals enclose the solution set without relating to the various
instances of the parameters. For instance, the traﬀc between A and C can also
pass through the link A →B. Thus the ﬂow constraint on this link also contains
0.3..0.7 ∗FAC. However, the parameter constraint stating that the sum of the
coeﬀcients of the traﬀc FAC in both constraints should be equal to 1 should also
be present. Assuming independence of the parameters for tractability reasons,
leads to safe computations, but at the potential cost of a very large solution set,
even if no solution actually holds. The problem structure is lost. Also, there is
not insight as to how the potential solutions evolve given instances of the data.
The question remains as to how can this information be embedded in a con-
straint model that would remain tractable. To our knowledge this issue has not
been addressed in the general case. It is the purpose of this work.

286
C. Gervet and S. Galichet
In this paper we propose a new methodology and eﬀcient process to account
for data dependency constraints in decision problems. We aim to more closely
model the actual problem structure, reﬁne the solutions produced and add ac-
curacy to the decision making process. We use regression analysis to show the
relationship among various instances of the uncertain data parameters and the
solutions produced. The basic process is to extract the parameter constraints
from the model, solve them to obtain tuple solutions over the parameters. Then
we run simulations on the constraint models using the parameter tuples as data
instances that embed the dependencies. In the example above this would imply
for the two constraints given, that if one parameter takes the value 0.3, the other
one would take the value 0.7. A set of constraint models can thus be solved ef-
ﬁciently, matching a tuple of consistent parameters to a solution, to determine
whether there are solutions once dependencies are taken into account or not.
Finally, we run a regression analysis between the parameter values and solutions
produced to determine the regression function, i.e. see how potential solutions
relate to parameter variations.
The paper is structured as follows. Section 2 summarizes the related work
and gives a small illustrative case study. Section 3 describes the approach and
algorithms, and Section 4 gives an application study. A conclusion is given in
Section 5.
2
Background and Case Study
The ﬁelds of regression analysis and constraint programming are both well es-
tablished in computer science. While we identiﬁed both ﬁelds as complementary,
there has been little attempt to integrate them together to the best of our knowl-
edge. The reason is, we believe, that technology experts tackle the challenges in
each research area separately. However, each ﬁeld has today reached a level of
maturity shown by the dissemination in academic and industrial works, and
their integration would bring new research insights and a novel angle in tack-
ling real-world optimization problems with measurement uncertainty. There has
been some research in Constraint Programming (CP) to account for data uncer-
tainty, and similarly there has been some research in regression modeling to use
optimization techniques. We give an account of the state of the art against our
main objective to integrate both paradigms.
CP is a paradigm within Artiﬁcial Intelligence that proved eﬃective and suc-
cessful to model and solve diﬀcult combinatorial search and optimization prob-
lems from planning and resource management domains [9]. Basically it models a
given problem as a Constraint Satisfaction Problem (CSP), which means: a set
of variables, the unknowns for which we seek a value, the range of values allowed
for each variable , and a set of constraints which deﬁne restrictions over the vari-
ables. Constraint solving techniques have been primarily drawn from Artiﬁcial
Intelligence (constraint propagation and search), and more recently Operations
Research (graph algorithms, Linear Programming). A solution to a constraint
model is a complete consistent assignment of a value to each decision variable.

On Combining Regression Analysis and Constraint Programming
287
In the past 15 years, the growing success of constraint programming tech-
nology to tackle real-world combinatorial search problems, has also raised the
question of its limitations to reason with and about uncertain data, due to in-
complete or imprecise measurements, (e.g. energy trading, oil platform supply,
scheduling). In the past 10 years, the generic CSP formalism has been extended
to account for forms of uncertainty: e.g. numerical, mixed, quantiﬁed, fuzzy, un-
certain CSP and CDF-interval CSPs [3]. The fuzzy and mixed CSP [7] coined
the concept of parameters, as uncontrollable variables, meaning they can take a
set of values, but their domain is not meant to be reduced to one value during
problem solving (unlike decision variables). Constraints over parameters, or un-
controllable variables, can be expressed and thus some form of data dependency
modeled. However, there is a strong focus on discrete data, and the consistency
techniques used are not always eﬃective to tackle large scale or optimization
problems.
Frameworks such as numerical, uncertain, or CDF-interval CSPs, extend the
classical CSP to approximate and reason with continuous uncertain data repre-
sented by intervals; see the real constant type in Numerica [11] or the bounded
real type in ECLiPSe [4]. Our previous work introduced the uncertain and CDF-
interval CSP [12,10]. The goal was then to derive eﬀcient techniques to compute
reliable solution sets that ensure that each possible solution corresponds to at
least one realization of the data. In this sense they compute an enclosure of the
set of solutions. Even though we identiﬁed the issue of having a large solution set,
the means to relate diﬃerent solutions to instances of the uncertain data param-
eters and their dependencies were not thought of. On the other hand, in the ﬁeld
of regression analysis, the main challenges have been in the deﬁnition of opti-
mization functions to build a relevant regression model, and the techniques to do
so eﬀciently. Regression analysis evaluates the functional relationship, often of a
linear form, between input and output parameters in a given environment. Here
we are interested in using regression to seek a possible relation between uncer-
tain constrained parameters in a constraint problem, e.g. distribution of traﬀc
among two routers on several routes and the solutions computed according to
the parameter instances. We note also that methods such as sensitivity analysis
in Operations Research allow to analyze how solutions evolve relative parameter
changes. However, such models assume independence of the parameters. In the
following case study we show how our approach can establish relationships with
the solutions and uncertain parameters while accounting for dependencies.
Case study. We present a small case study to give the intuition of our approach.
The core element is to go around the solving of a constraint optimization problem
with uncertain parameter constraints by ﬁrst solving the parameter constraints
alone. This way we handle uncertainty in an eﬀcient and tractable manner.
We then substitute solution tuples of these parameters to solve a set of con-
straint optimization problems (now without parameter constraints). Finally to
provide insight to the solutions to the uncertain constraint problem, we run a re-
gression between the solutions produced and the corresponding parameter tuple
instances.

288
C. Gervet and S. Galichet
Consider the following ﬁctitious constraint between two unknown positive
variables, X and Y ranging in 0.0..1000.0, with uncertain data parameters A, B
taking their values in the real interval [0.1..0.7]:
A ∗X + B ∗Y = 150
The objective is to compute values for X and Y in the presence of uncertain
parameters (A, B). Without any parameter dependency a constraint solver based
on interval propagation techniques with bounded coeﬀcients, derives the ranges
[0.0..1000.0] for both variables X and Y [4]. Let us add to the model a parameter
constraint over the uncertain parameters A and B: A = 2 ∗B. Without adding
this parameter constraint to the model, since it is not handled by the solver, we
can manually reﬁne the bounds of the uncertain parameters in the constraint
model such that the parameter constraint holds over the bounds, thus accounting
partially for the dependency. We obtain the constraint system:
[0.2..0.7] ∗X + [0.1..0.35] ∗Y = 150
The solution returned to the user is a solution space: X ∈[0.0..750.0], Y ∈
[0.0..1000.0]. The actual polyhedron describing the solution space is depicted in
Fig. 2.





	





	



	

	



	






	





	





Fig. 2. Left: Solution space. Tight and certain bounds for the decision variables: [0,
750] [0, 1000]. Right: Solutions vectors of problem instances with consistent parameter
solutions.
We now give the intuition of our approach. The idea is to ﬁrst solve the
parameter dependency constraints alone to obtain solution tuples, not intervals.
To do so we use a traditional branch and bound algorithm. We obtain a set of
tuples for A and B such that for each tuple the constraint A = 2 ∗B holds.
The idea is to have a well distributed sample of solutions for this parameter
constraint.
We obtain a set of tuples that satisfy the parameter constraint, in this case for
instance (0.2, 0.1), (0.3, 0.15), (0.4, 0.2), (0.5, 0.25), (0.6, 0.3), (0.7, 0.35). We then

On Combining Regression Analysis and Constraint Programming
289
substitute each tuple in the uncertain constraint model rendering it a standard
constraint problem, and solve each instance. We record the solution matching
each parameter instance. The issue now is that we have a set of solutions for each
tuple of paramers. There is no indication how the solutions are related and evolve.
The idea is to apply a regression analysis between both. The regression function
obtained includes the solution bounds obtained by the standard approach, but
mainly shows the trends between the data parameters and the solutions. In this
small example we can visualize how the solution evolves with the data, see Fig. 2
on the right. In the case of much larger data sets, a tool like Matlab can be used
to compute the regression function and display the outcome. The algorithm and
complexity analysis are given in the next section.
3
Process and Algorithms
Our methodology is a three-steps iterative process: 1) Extract the uncertain pa-
rameter constraints from the uncertain optimization problem and run branch
and bound to produce a set of tuple solutions, 2) solve a sequence of stan-
dard constraint optimization problems where the tuples are being substituted
to the uncertain parameters. This is a simulation process that produces, if it
exists, one solution per tuple instance. And ﬁnally, 3) run a regression analysis
on the parameter instances and their respective solution, to identify the rela-
tionship function showing how the solution evolves with consistent parameter
constraints. The overall algorithmic process is given in Fig. 3, where the out-
comes at each step are highlighted in italic bold. A constraint satisfaction and
optimisation problem, or CSOP, is a constraint satisfaction problem (CSP) that
seeks complete and consistent instantiations optimizing a cost function. We use
the notion of uncertain CSOP, or UCSOP ﬁrst introduced in [12]. It extends a
classical CSOP with uncertain parameters.
3.1
Uncertain CSOP and Uncertain Parameter Constraints
Recall that a CSOP is commonly speciﬁed as a tuple (X, D, C, f), where X is a
ﬁnite set of variables, D is the set of corresponding domains, C = {c1, . . . , cm} is
a ﬁnite set of constraints, and f is the objective function (min or max of a given
expression over a subset of the variables).
Deﬁnition 1 (UCSOP). An uncertain constraint satisfaction and optimisa-
tion problem is a classical CSOP in which some of the constraints may be uncer-
tain, and is speciﬁed by the tuple (X, D, CX , Γ, U, f). The ﬁnite set of parameters
is denoted by Γ, and the set of ranges for the parameters by U. A solution to a
UCSOP is a solution space enclosing safely the set of possible solutions.
Example 1. Let X1 ∈D1 and X2 ∈D2 both have domains D1 = D2 = [1.0..7.0].
Let α1 and α2 be parameters with uncertainty sets U1 = [2.0..4.0] and U2 =
[1.0..6.0] respectively. Consider three constraints:
C1 : X1 > α1, C2 : X1 = X2 + α2, C3 : X2 > 2, C4 : α2 = α1 + 3

290
C. Gervet and S. Galichet
UCSOP
Constraint 
simulation
Generator of
CSOP 
instances
Parameter 
constraints
Solution 
Solution 
set
set
Relationship function between 
Relationship function between 
uncertain data and solutions
uncertain data and solutions
Regression 
analysis
Problem 
constraints
Variables
Solution to each 
Solution to each 
CSOP
CSOP
 Parameter
instances
Parameter 
Parameter 
tuples
tuples
Objective 
function
Fig. 3. Process
and the objective function to maximize f(X1, X2) = X1 + X2. Writing X =
{X1, X2}, D = {D1, D2}, Γ = {α1, α2}, U = {U1, U2}, and CX = {C1, C2, C3},
then (X, D, CX , Γ, U, f) is a UCSOP. Note that C3 is a certain constraint; C1 and
C2 are both uncertain constraints because they contain uncertain parameters.
CΛ = {C4} is the set of parameter constraints.
3.2
Constraint Simulation
We now account for the parameters constraints by transforming the UCSOP into
a set of tractable CSOPs instances. More formally, consider a UCSOP (X, D, CX ∪
CΛ, Γ, U, f).
Deﬁnition 2 (instance of UCSOP). We denote n the number of variables,
m the number of uncertain parameters, p the number of parameter constraints,
and inst(Ui) a value within the range of an uncertainty set. An instance of an
UCSOP is a certain CSOP (X, D, CX ) such that for each uncertain constraint
Ci(X1..Xm, α1, ..αm), we have αj = inst(Uj), such that ∀k ∈{1, .., p}, the pa-
rameter constraint Ck(α1, ..αm) is satisﬁed.
Example 2. Continuing example 1, the UCSOP has two possible instances such
that the parameter constraint α2 = α1 + 3 holds, given that α1 ∈U1, α2 ∈U2.
The valid tuples (α1, α2) are (2, 5), and (3, 6). The CSOP instances are:
C1 : X1 > 2, C2 : X1 = X2 + 5, C3 : X2 > 2

On Combining Regression Analysis and Constraint Programming
291
and
C1 : X1 > 3, C2 : X1 = X2 + 6, C3 : X2 > 2
with the same objective function to maximize f = X1 + X2.
The generator of CSOP instances extracts the parameter constraints, poly-
nomial in the number of constraints in the worst case, then produces a set of
parameter tuples that satisfy the parameter constraints. We can use a branch and
bound search on the parameter constraints of the UCSOP. The constraint simu-
lation then substitutes the tuple solutions onto the original UCSOP to search for
a solution to each optimization problem, that is each CSOP. This is polynomial
in the complexity of the UCSOP. The process is depicted in Algorithm 1.
Algorithm 1. Generate and solve CSOPs from one UCSOP
Input: A UCSOP (X , D, CX ∪CΛ, Λ, U, f)
Output: Solutions to the CSOPs
1 SolsT uples ←∅
2 extract(CΛ)
3 T uples ←solveBB(Λ, U, CΛ)
4 for Ti ∈T uples do
5
substitute Λ with Ti in (X , D, CX, Λ, f)
6
Si ←solveOpt(X , D, CX, Ti, f)
7
SolsT uples ←SolsT uples ∪{(Si, Ti)}
8 return SampleSols
4
Application
We illustrate the beneﬁts of our approach by solving an uncertain constraint
optimization problem, the traﬀc matrix estimation for the sigcomm4 problem,
given in Fig. 1. The topology and data values can be found in [8,12]. Given traﬀc
measurements over each network link, and the traﬀc entering and leaving the
network at the routers, we search the actual ﬂow routed between every pair of
routers. To ﬁnd out how much traﬀc is exchanged between every pair of routers,
we model the problem as an uncertain optimization problem that seeks the min
and max ﬂow between routers such that the traﬀc link and traﬀc conservation
constraints hold. The traﬀc link constraints state that the sum of traﬀc using
the link is equal to the measured ﬂow. The traﬀc conservation constraints, two
per router, state that the traﬀc entering the network must equal the traﬀc
originating at the router, and the traﬀc leaving the router must equal the traﬀc
whose destination is the router.
We compare three models. The ﬁrst one does not consider any uncertain pa-
rameters and simpliﬁes the model to only the variables in bold with coeﬀcient
1. The traﬀc between routers takes a single ﬁxed path, as implemented in [8].
The second model extends the ﬁrst one with uncertain parameters but without

292
C. Gervet and S. Galichet
the parameter dependency constraints. The third one is our approach with the
parameter dependency constraints added. A parameter constraint, over the ﬂow
FAB, for instance, states that the coeﬀcients representing one given route of
traﬀc from A to B take the same value; and the sum of coeﬀcients correspond-
ing to diﬃerent routes equals to 1. Note that the uncertain parameter equality
constraints are already taken into account in the link traﬀc constraints. The un-
certain parameters relative to ﬂow distributions are commonly assumed between
30 and 70 % [12].
Link traﬀc constraints:
[λ1AB, λ1AC , λ1AD, λ2AB, λ2AC, λ2AD] ∈0.3..0.7
A →B
λ1AB ∗FAB + λ1AC ∗FAC + λ1AD ∗FAD
= 309.0..328.0
B →A
FBA + FCA + FDA + λ1BC ∗FBC
= 876.39..894.35
A →C
λ2AC ∗FAC + λ2AD ∗FAD + λ2AB ∗FAB + λ1BC ∗FBC = 591.93..612.34
B →C
λ2BC ∗FBC + FBD + λ1AC ∗FAC + λ1AD ∗FAD
= 543.30..562.61
C →B
λ2AB ∗FAB + FCB + FCA + FDA + FDB
= 1143.27..1161.06
C →D
FCD + FBD + FAD
= 896.11..913.98
D →C
FDC + FDB + FDA
= 842.09..861.35
Parameter constraints
λ1AB + λ2AB = 1, λ1AC + λ2AC = 1, λ1AD + λ2AD = 1, λ1BC + λ2BC = 1
Traﬀc conservation constraints
A origin
FAD + FAC + FAB
= 912.72..929.02
A destination
FDA + FCA + FBA
= 874.70..891.00
B origin
FBD + FBC + FBA
= 845.56..861.86
B destination
FDB + FCB + FAB
= 884.49..900.79
C origin
FCD + FCB + FCA
= 908.28..924.58
C destination
FDC + FBC + FAC
= 862.53..878.83
D origin
FDC + FDB + FDA
= 842.0..859.0
D destination
FCD + FBD + FAD
= 891.0..908.0
We ﬁrst run the initial model and reproduced the results of [12] in constant
time. By adding the uncertain prameters the solution bounds got much larger as
the space of potential solutions expanded. However when we run simulations us-
ing our approach and the linear EPLEX solver, and we were not able to ﬁnd any
solution to the model with dependency constraints. This shows the importance of
taking into account such dependencies, indicating that the data provided match
a single path routing algorithm for the sigcomm4 topology. After enlarging the
interval bounds of the input data we were able to ﬁnd a solution with a 50
% split of traﬀc, but none with 40 −60 or other combinations. Our approach
showed the eﬃectiveness and strong impact of taking into account dependency
constraints with simulations.
5
Conclusion
In this paper we introduced an approach to account for dependency constraints
among data parameters in an uncertain constraint problem. The approach fol-
lows an iterative process that ﬁrst satisﬁes the dependency constraints using a

On Combining Regression Analysis and Constraint Programming
293
branch and bound search. The solutions are then embedded to generate a set
of CSPs to be solved. However this does not indicate the relationship between
the dependent consistent parameters and possible solutions. We propose to use
regression analysis to do so. The current case study showed that by embedding
constraint dependencies only one instance had a solution. This was valuable
information on its own, but limited the use of regression analysis. Further ex-
perimental studies are underway with applications in inventory management,
problems clearly permeated with data uncertainty. Even though our approach
has been applied to traditional constraint problems in mind, its beneﬁts could
be stronger on data mining applications with constraints [6].
References
1. Benhamou, F., Goualard, F.: Universally quantiﬁed interval constraints. In:
Dechter, R. (ed.) CP 2000. LNCS, vol. 1894, pp. 67–82. Springer, Heidelberg (2000)
2. Boukezzoula, R., Galichet, S., Bisserier, A.: A Midpoint Radius approach to re-
gression with interval data. International Journal of Approximate Reasoning 52(9)
(2011)
3. Brown, K., Miguel, I.: Uncertainty and Change. In: Handbook of Constraint Pro-
gramming, ch. 21. Elsevier (2006)
4. Cheadle, A.M., Harvey, W., Sadler, A.J., Schimpf, J., Shen, K., Wallace, M.G.:
ECLiPSe: An Introduction. Tech. Rep. IC-Parc-03-1, Imperial College London,
London, UK
5. Chinneck, J.W., Ramadan, K.: Linear programming with interval coeﬃcients. J.
Operational Research Society 51(2), 209–220 (2000)
6. De Raedt, L., Mannila, H.: O’Sullivan and Van Hentenryck P. organizers. Con-
straint Programming meets Machine Learning and Data Mining. Dagstuhl seminar
(2011)
7. Fargier, H., Lang, J., Schiex, T.: Mixed constraint satisfaction: A framework for
decision problems under incomplete knowledge. In: Proc. of AAAI (1996)
8. Medina, A., Taft, N., Salamatian, K., Bhattacharyya, S., Diot, C.: Traﬃc Matrix
Estimation: Existing Techniques and New Directions. In: Proceedings of ACM
SIGCOMM 2002 (2002)
9. Rossi, F., van Beek, P., Walsh, T.: Handbook of Constraint Programming. Elsevier
(2006)
10. Saad, A., Gervet, C., Abdennadher, S.: Constraint Reasoning with Uncertain Data
using CDF-Intervals. In: Lodi, A., Milano, M., Toth, P. (eds.) CPAIOR 2010.
LNCS, vol. 6140, pp. 292–306. Springer, Heidelberg (2010)
11. Van Hentenryck, P., Michel, L., Deville, Y.: Numerica: a Modeling Language for
Global Optimization. The MIT Press, Cambridge (1997)
12. Yorke-Smith, N., Gervet, C.: Certainty Closure: Reliable Constraint Reasoning
with Uncertain Data. ACM Transactions on Computational Logic 10(1) (2009)

 
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 294–303, 2014. 
© Springer International Publishing Switzerland 2014  
Graph-Based Transfer Learning for Managing Brain 
Signals Variability in NIRS-Based BCIs 
Sami Dalhoumi1, Gérard Derosiere2, Gérard Dray1,  
Jacky Montmain1, and Stéphane Perrey2 
1 Laboratoire d’Informatique et d’Ingénierie de Production (LGI2P), Ecole des Mines d’Alès 
Parc Scientifique G. Besse, 30035 Nîmes, France 
{name.surname}@mines-ales.fr 
2 Movement to Health (M2H), Montpellier 1-University, Euromov, 
700 Avenue du Pic Saint-Loup, 34090 Montpellier, France 
{name.surname}@univ-montp1.fr 
Abstract. One of the major limitations to the use of brain-computer interfaces 
(BCIs) based on near-infrared spectroscopy (NIRS) in realistic interaction set-
tings is the long calibration time needed before every use in order to train a sub-
ject-specific classifier. One way to reduce this calibration time is to use data 
collected from other users or from previous recording sessions of the same user 
as a training set. However, brain signals are highly variable and using heteroge-
neous data to train a single classifier may dramatically deteriorate classification 
performance. This paper proposes a transfer learning framework in which we 
model brain signals variability in the feature space using a bipartite graph. The 
partitioning of this graph into sub-graphs allows creating homogeneous groups 
of NIRS data sharing similar spatial distributions of explanatory variables 
which will be used to train multiple prediction models that accurately transfer 
knowledge between data sets. 
Keywords: Brain-computer interface (BCI), near-infrared spectroscopy 
(NIRS), brain signals variability, transfer learning, bipartite graph partitioning. 
1 
Introduction 
A brain-computer interface (BCI) is a communication system that allows people suf-
fering from severe neuromuscular disorders to interact with their environment without 
using peripheral nervous and muscular system, by directly monitoring electrical or 
hemodynamic activity of the brain [1]. Recently, near-infrared spectroscopy (NIRS) 
has been investigated for use in BCI applications [2]. NIRS-based BCIs employ  
near-infrared light to characterize alterations in cerebral metabolism during neural 
activation. During neural activation in a specific region of the brain, hemodynamic 
concentration changes in oxyhemoglobin (oxy-Hb) increase while those in deoxyhe-
moglobin (deoxy-Hb) decrease slightly [3] (see figure 1).  

 
Graph-Based Transfer Learning for Managing Brain Signals Variability 
295 
 
 
Fig. 1. Prototypical brain activity pattern using NIRS technology. Measured values of concen-
trations are relative and not absolute. 
A BCI is considered as a pattern recognition system that classifies different brain 
activity patterns into different brain states according to their spatio-temporal characte-
ristics [4]. The relevant signals that decode brain states may be hidden in highly noisy 
data or overlapped by signals from other brain states. Extracting such information is a 
very challenging issue. To do so, a long calibration time is needed before every use of 
the BCI in order to extract enough data used for feature selection and classifier train-
ing [5]. Because calibration is time-consuming and boring even for healthy users, 
several works in BCIs based on electroencephalography [1] addressed this problem by 
developing new data processing and pattern classification methods. [6] proposed a 
semi-supervised support vector machine (SVM) classifier designed to accurately clas-
sify brain signals with small training set. [7] used data recorded from the same subject 
during past recording sessions in order to determine prototypical spatial filters which 
have better generalization properties than session-specific filters. Other authors [5, 8-
11] developed different subject-transfer frameworks to reduce calibration time before 
each use of a BCI. It consists of using data recorded from several users that performed 
the same experiment as a training set for a classifier that will be used to predict brain 
activity patterns of a new user. In NIRS-based BCIs, the problem of long calibration 
time has not been well addressed.  Recent studies highlighted the need of changing 
data processing and classifiers design strategies in order to conceive more robust and 
practical BCIs [12-14]. To our best knowledge, [15] is the only work that addressed 
this issue by designing an adaptive classifier based on multiple-kernel SVM.  
In this paper, we introduce a novel approach inspired from graph theory to address 
this problem. The novelty of our contribution compared to the works cited above lies 
in the fact of using a bipartite graph to have prior knowledge about variability in the 
feature space between brain signals recorded from different users. This prior know-
ledge allows designing a prediction model that adaptively chooses the best users set 
and features representation to accurately classify data of a new user and consequently 
reduce calibration time. The reminder of this paper is organized as follows. In section 
2, we formalize the problem in the context of transfer learning between heterogeneous 
data. In section 3, we briefly review the background of bipartite graph partitioning 

296 
S. Dalhoumi et al. 
 
and describe different steps of our approach. The effectiveness of our approach is 
demonstrated by an experimental evaluation on a real data set in section 4. Finally, 
section 5 concludes the paper and gives future directions of our work. 
2 
Problem Formulation 
Using traditional classification techniques for NIRS-based BCIs, the problem of clas-
sifying different brain activity patterns into different brain states can be stated as fol-
lows: given training data collected during the current recording session by performing 
several trials of different cognitive tasks, the objective is to find a hypothesis h that 
allows good prediction of class labels corresponding to each cognitive task for the 
reminder of trials performed during the same session. Because classification perfor-
mance of h increases with the size of training data, a long calibration time is neces-
sary before each use of the BCI.  In the context of transfer learning [16], the problem 
can be reformulated as follows. Given 
1. A small amount of labeled NIRS signals recorded during the current session. 
2. A large amount of labeled NIRS signals recorded during previous sessions of the 
same subject or from other subjects that performed the same experiment. 
We want to exploit relatedness between these data sets in order to find a hypothesis h 
that allows good prediction of the class labels for the reminder of trials for the current 
session. Item 1 is called training set and item 2 is called support set. A single hypo-
thesis h can achieve good classification performance when the training and support 
sets are assumed to be drawn from the same feature space and the same distribution. 
Such assumption may be too strong for our application because of high variability of 
NIRS signals collected from different subjects during different recording sessions 
which affects mostly the spatial distribution of explanatory variables. 
In this paper, we design a prediction model which learns multiple hypotheses  
instead of one in order to overcome the problem of NIRS signals variability and accu-
rately transfer knowledge between data of different individuals and different record-
ing sessions. To do so, we divide our support set into multiple subsets {S1, S2,..., SK} 
and we learn a hypothesis hi for each subset Si. The choice of the most appropriate 
hypothesis to predict class label y for a new trial x in the current session depends on 
the number of explanatory features shared between the training set T and each subset 
Si. The general architecture of our approach can be expressed in a probabilistic man-
ner as follows: 
                                        ܲሺݕ/ݔሻൌ∑
ܲሺ݄௜
௄
௜ୀଵ
/ܶሻܲሺݕ/ݔ, ݄௜ሻ                                 (1) 
In the next section, we illustrate how bipartite graph partitioning allows creation of 
multiple hypotheses which minimizes the influence of NIRS signals variability in the 
feature space on classification performance. 

 
Graph-Based Transfer Learning for Managing Brain Signals Variability 
297 
 
3 
Transfer Learning Framework Based on Bipartite Graph 
Partitioning  
In order to design a transfer learning framework for heterogeneous NIRS data, we 
model the spatial variability of explanatory features between different data sets. To do 
so, we propose to borrow the bipartite graph partitioning from graph theory [17-18]. 
This technique allows performing simultaneous grouping of instances and features 
and consequently mapping of data into richer space. This is important for reducing the 
effect spatial variability of brain signals on classification performance because each 
hypothesis hi is drawn from explanatory variables in subset Si. 
3.1 
Bipartite Graph Partitioning 
Before describing different steps of our approach, we start with relevant terminology 
related to bipartite graph partitioning. 
A bipartite graph G = (D, F, E) is defined by two sets of vertices D = {d1,d2,…,dN} 
and F = {f1,f2,…,fM}, and a set of edges E = {(di, fj) / di ߳ D and fj ߳ F}. In this  
paper, we assume that an edge Eij = (di, fj) exists if vertices di and fj are related  
(i.e., Eij א {0, 1}). 
Assume that the set of vertices F is grouped into disjoint clusters {F1, F2,…, FK}. 
The set D can be clustered as follows: a vertex di (i = 1…N) belongs to the cluster Dp 
(p = 1…K) if its association with the cluster Fp is greater than its association with any 
other cluster in the vertex set F. This can be written as: 
                                     ܦ௣ൌሼ݀݅ / ∑
ܧ௜௝
௝אி೛
൒∑
ܧ௜௝, ׊ ݈ൌ1, … , ܭሽ
௝אி೗
 
(2) 
Given disjoint clusters D1,…, DK, the set of vertices F can be clustered similarly. 
As illustrated in [18], the optimal clustering of the two sets of vertices can be 
achieved when: 
                               ܿݑݐሺܦଵ׫ ܨଵ, … , ܦ௄׫ ܨ௄ሻൌ݉݅݊௏భ,…,௏಼ܿݑݐሺܸଵ, … , ܸ௄ሻ 
(3) 
where V1, …, VK is a K-partition of the bipartite graph (Vk, k = 1, …, K are sub-graphs 
of the graph G) and  
                                     ܿݑݐሺܸଵ, … , ܸ௄ሻൌ∑
∑
ܧ௣௤
௣א஽ሺ௏೔ሻ,௤אிሺ௏ೕሻ
௜ழ௝
                             (4) 
It is well known that graph partitioning is a NP-complete problem. There are many 
heuristics that were introduced to give better global solutions and reduce complexity 
of bipartite graph partitioning. In this work, we use spectral clustering which is an 
effective heuristic that uses properties of graph Laplacian matrix to solve this prob-
lem. Because of space limitations, we will not show details of this heuristic. For more 
information, see [18]. 

298 
S. Dalhoumi et al. 
 
3.2 
Overview of Our Graph-Based Transfer Learning Framework 
In our support set, the first set of vertices D = {d1, d2, …, dN} corresponds to different 
data sets of  NIRS signals recorded from different subjects during different sessions 
and the second set of vertices F = {f1,f2,…,fM} corresponds to M measurement chan-
nels placed on the same locations of participants’ heads. A recording session consists 
of performing several trials of two cognitive tasks T1 and T2. Our approach is accom-
plished in three steps (figure 2). 
 
 
Fig. 2. Bipartite graph model for characterizing brain signals variability in the features space 
between different users 
Heterogeneous NIRS Signals Partitioning  
For each data set di, we perform features selection in order to find channels that allow 
detection of signals amplitude changes between different cognitive tasks. Then, we 
assign the number 1 to explanatory channels and 0 to the rest (i.e., ܧ௜௝א ሼ0,1ሽ). This 
channel selection procedure allows us to create an N by M co-occurrences matrix of 
data sets and channels and consequently create a representation of spatial variability 
of brain activity patterns in heterogeneous NIRS data. The creation of different groups 
of data sets {D1, …, DK} sharing similar spatial distributions of brain activity patterns 
{F1, …, FK} is performed by applying bipartite graph partitioning to the co-
occurrences matrix. 
Classifiers Training  
After creation of several groups of NIRS signals having local features representations, 
a single classifier is trained on each group. The training performance of each classifier 
is evaluated using leave-one-out cross validation. The global performance of our  

 
Graph-Based Transfer Learning for Managing Brain Signals Variability 
299 
 
prediction model is the average of all classifiers performance. If it is below the re-
quired performance, the bipartite graph partitioning step is repeated with different 
number of partitions.  
New NIRS Signals Classification Using the Multiple-Hypotheses Prediction 
Model  
Once prediction model training is finished, NIRS signals recorded during a new ses-
sion will be classified as follows: first, we find the group of data sets sharing the most 
similar spatial distribution of brain activity patterns and then use the hypothesis 
trained on that data to predict class labels of each trial in the new session. In real time 
conditions, assuming that spatial distribution of brain activity patterns do not vary 
significantly during the same session, only the first few trials (i.e., training set T) are 
used to find the closest co-cluster in our support set.  
In the probabilistic interpretation of our transfer learning framework given in (1), 
P(hi/T) is calculated using “the winner takes all” rule and consequently P(y/x) will be 
determined using only one hypothesis ܪ௜כ: 
                           ܲሺ݄௜/Tሻൌ൜ 1, ݂݅ ∑
ܧ்௝
௝אி೔
൒∑
ܧ்௝, ׊ ݈ൌ1, … , ܭ
௝אி೗
 0, ݋ݐ݄݁ݎݓ݅ݏ݁                                                     
(5) 
Then, ׌! ݅כ / ܲሺ݄௜כ/ܶሻൌ1 and 
                              ܲሺݕ/ݔሻൌ∑
ܲሺ݄௜
௄
௜ୀଵ
/ܶሻܲሺݕ/ݔ, ݄௜ሻൌܲሺݕ/ݔ, ݄௜כሻ 
(6) 
4 
Experimental Evaluation 
In this section, our approach is evaluated on a real NIRS-based BCI data set using a 
linear discriminant analysis (LDA) classifier, which is the most widely used classifier 
in BCI applications [4], as a base learner. Then, its performance is compared to a 
single LDA classifier. 
4.1 
Data Set Description 
To evaluate our approach, we used the publicly available data set described in [15]. It 
is composed of NIRS signals recorded from seven healthy subjects using 16 mea-
surement channels. The study consisted of two experiments, each one lasted four 
sessions. The aim of the first experiment was to discern brain activation patterns re-
lated to imagery movement of right forearm from the activation patterns related to 
relaxed state, denoted respectively t1 and b. While the aim of the second one was to 
discern brain activation patterns related to imagery movement of left forearm t2 from 
the activation patterns related to relaxed state b.  During each session, participants 
performed three trials of b and three trials of t1 for experiment 1 and three trials of b 
and three trials of t2 for experiment 2. Thus, in each experiment, the first set of vertic-
es D corresponds to 7 subjects ൈ 4 sessions (i.e., D = {d1, d2,… d28}), while the second 

300 
S. Dalhoumi et al. 
 
set of vertices corresponds to 16 channels (i.e., F = {f1, f2, …, f16}). Because the ma-
jority of NIRS-based BCIs studies reported that deoxy-Hb does not necessarily show 
significant changes in activated areas of the brain [3], we focused only on oxy-Hb 
changes. 
4.2 
Data Preprocessing 
NIRS signals going through human brain may be overlapped by many physiological 
(e.g., respiration, heart beat) and experimental (e.g., motion artifacts) sources of noise 
[3]. In order to minimize the effect of these sources of noise on classifiers perfor-
mance, we applied a 5th-oder Butterworth low-pass filter with cut-of frequency of 0.5 
Hz [15]. Another problem that we may encounter when we classify heterogeneous 
NIRS signals is the difference in amplitudes of hemodynamic brain activity between 
subjects, sessions or even trials performed during the same session [14]. To overcome 
this problem, we performed zero-mean and unit-variance normalization on time series 
data of each trial. 
4.3 
Results 
In this study, we used the Wilcoxon signed-rank test, which is a non-parametric statis-
tical hypothesis test, to compare mean amplitude oxy-Hb time series averaged over 
time windows of imagery movement and relaxed state. Among 28 data sets in each 
experiment, 15 data sets in experiment 1 illustrated a significant difference in oxy-Hb 
amplitudes between time windows of imagery movement and relaxed state and 22 
data sets in experiment 2. Data sets in which no channel discerned activity patterns 
related to each brain state were removed resulting in 15 by 16 co-occurrences matrix 
for the first experiment and 22 by 16 co-occurrences matrix for the second one. Inter-
subjects and inter-sessions variability in our data set are illustrated in figure 3. 
 
 
Fig. 3. Brain signals variability in NIRS-based BCIs. (a) Inter-sessions variability of explanato-
ry channels for subject 5 in experiment 1. (b) Inter-subjects variability of explanatory channels 
for subjects 3 and 4 in experiment 2. White dots represent explanatory channels and black dots 
represent non-explanatory channels. 

 
Graph-Based Transfer Learning for Managing Brain Signals Variability 
301 
 
Because the number of trials in each session is not enough to evaluate our approach 
in online fashion, we tested it in a batch mode and compared classification results to a 
single LDA. As performance measurement, we used sensitivity and specificity which 
are respectively the probability that the activity pattern is classified as movement 
imagination given that the participant effectively performed movement imagination 
and the probability that the activity pattern is classified as relaxed state given that the 
participant was resting. The comparison results are illustrated in table 1.  
Table 1. Comparison of classification performance of our approach and a single LDA 
classifier. The number of partitions is 2 in experiment 1 and 3 in experiment 2.  
 
Experiment 1 
Experiment 2 
Sensitivity 
Specificity 
Sensitivity 
Specificity 
LDA 
0.60 
0.71 
0.65 
0.68 
Our approach 
0.95 
0.94 
0.92 
0.87 
 
As expected, a single LDA classifier trained on heterogeneous data sets cannot find 
a hyper plane that separates well instances from different classes because each data-
set’s decision boundary lies in a different subspace. In contrast, projecting each group 
of data sets on the subspace spanned by their shared explanatory features and training 
different classifiers separately allows building a prediction model that significantly 
outperforms the former prediction model in classifying data recorded from a new 
user. 
5 
Conclusions and Directions for Future Work 
This paper proposed a graph-based transfer learning framework for accurately classi-
fying heterogeneous NIRS signals recorded from different subjects during different 
sessions. It consists of modeling brain signals variability in the feature space using a 
bipartite graph and adaptively choosing the best users set and features representation 
used to train a classifier that will predict class labels of brain signals recorded during 
new session. The experimental evaluation of our approach compared to a single LDA 
classifier showed that our approach accurately transfers knowledge between different 
data sets despite the high variability of spatial distributions of explanatory channels 
between different users and different sessions of the same user. 
Although first results are promising, many issues should be considered in future 
work. Boolean representation of the edges in our bipartite graph model may be not 

302 
S. Dalhoumi et al. 
 
suitable for distinguishing robust features from non-robust ones. In fact, in NIRS-
based BCIs there is a phenomenon of saturation which means that after performing 
several trials of different cognitive tasks the changes in oxy-hemoglobin and deoxy-
hemoglobin concentrations between different brain states become non-significant in 
some regions of the brain [14]. Thus, features weighting may be important for design-
ing robust classifiers which maintain good classification performance for long periods 
of time.  Another important issue related to our transfer learning framework is the 
classifiers aggregation method given in (5). Choosing only one hypothesis may be 
non-suitable when the data set of current user shares many significant features with 
more than one partition in the support set. Other classifiers aggregation methods like 
weighted sum [19] and fuzzy integrals [20] will be investigated in future work. 
Acknowledgement. We would like to thank Berdakh Abibullaev for providing the 
NIRS data set available on the website: http://kernelx.net/fnirs-bci-data/. 
References 
1. Nicolas-Alonso, L.F., Gomez-Gil, J.: Brain Computer Interfaces, a Review. Sensors 12, 
1211–1279 (2012) 
2. Coyle, S., Ward, T., Markham, C., McDarby, G.: On the suitability of near-infrared (NIR) 
systems for next-generation brain–computer interfaces. Physiological Measurement 25, 
815–822 (2004) 
3. Sitaram, R., Caria, A., Birbaumer, N.: Hemodynamic brain-computer interfaces for  
communication and rehabilitation. Neural Networks 22, 1320–1328 (2009) 
4. Lotte, F., Congedo, M., Lécuyer, A., Lamarche, F., Arnaldi, B.: A Review of Classifica-
tion Algorithms for EEG-based Brain-Computer Interfaces. Journal of Neural Engineer-
ing 4, R1–R13 (2007) 
5. Tu, W., Sun, S.: A subject transfer framework for EEG classification. Neurocomputing 82, 
109–116 (2011) 
6. Li, Y., Guan, C., Li, H., Chin, Z.: A self-training semi-supervised SVM algorithm and its 
application in an EEG-based brain computer interface speller system. Pattern Recognition 
Letters 29, 1285–1294 (2008) 
7. Krauledat, M., Tangermann, M., Blankertz, B., Muller, K.R.: Towards Zero Training for 
Brain-Computer Interfacing. Plos One 3(8), e2967 (2008) 
8. Lotte, F., Guan, C.: Learning from other subjects helps reducing brain-computer interface 
calibration time. In: International Conference on Audio Speech and Signal Processing 
(ICASSP), pp. 614–617 (2010) 
9. Falzi, S., Grozea, C., Danoczy, M., Popescu, F., Blankertz, B., Muller, K.R.: Subject inde-
pendent EEG-based BCI decoding. In: Neural Information Processing Systems Conference 
(NIPS), pp. 513–521 (2009) 
10. Samek, W., Meinecke, F.C., Muller, K.R.: Transferring Subspaces Between Subjects in 
Brain-Computer Interfacing. IEEE Transactions on Biomedical Engineering 60(8), 2289–
2298 (2013) 
11. Lu, S., Guan, C., Zhang, H.: Unsupervised Brain Computer Interface Based on Intersubject 
Information and Online Adaptation. IEEE Transactions on Neural Systems and Reabilita-
tion Engineering 17(2), 135–145 (2009) 

 
Graph-Based Transfer Learning for Managing Brain Signals Variability 
303 
 
12. Sato, H., Fushino, Y., Kiguchi, M., Katura, T., Maki, A., Yoro, T., Koizumi, H.: Intersub-
ject variability of near-infrared spectroscopy signals during sensorimotor cortex activation. 
Journal of Biomedical Optics 10(4), 44001 (2005) 
13. Power, S.D., Kushki, A., Chau, T.: Intersession Consistency of Single-Trial Classification 
of the Prefrontal Response to Mental Arithmetic and the No-Control State by NIRS. Plos 
One 7(7), e37791 (2012) 
14. Holper, L., Kobashi, N., Kiper, D., Scholkmann, F., Wolf, M., Eng, K.: Trial-to-trial va-
riability differentiates motor imagery during observation between low versus high res-
ponders : A functional near-infrared spectroscopy study. Bihavioural Brain Research 229, 
29–40 (2012) 
15. Abibullaev, B., An, J., Jin, S.H., Lee, S.H., Moon, J.I.: Minimizing Inter-Subject Variabili-
ty in fNIRS-based Brain-Computer Interfaces via Multiple-Kernel Support Vector Learn-
ing. Medical Engineering and Physics, S1350-4533(13)00183-5 (2013) 
16. Pan, S.J., Yang, Q.: A Survey on Transfer Learning. IEEE Transactions on Knowledge and 
Data Engineering 22(10), 1345–1359 (2010) 
17. Zha, H., He, X., Ding, C., Simon, H., Gu, M.: Bipartite Graph Partitioning and Data  
Clustering. In: CIKM 2001, Atlanta, Georgia, USA (2001) 
18. Dhillon, I.S.: Co-clustering documents and words using Bipartite Spectral Graph Partition-
ing. In: KDD, San Francisco, California, USA (2001) 
19. Kittler, J., Hatef, M., Duin, R.P.W., Matas, J.: On Combining Classifiers. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence 20(3), 226–239 (1998) 
20. Pizzi, N.J., Pedrycz, W.: Aggregating multiple classification results using fuzzy integration 
and stochastic feature selection. International Journal of Approximate Reasoning 51(8), 
883–894 (2010) 
 
 

Design of a Fuzzy Aﬀective Agent Based
on Typicality Degrees of Physiological Signals
Joseph Onderi Orero1 and Maria Rifqi2
1 Faculty of Information Technology, Strathmore University, Kenya
jorero@strathmore.edu
2 LEMMA, University Panth´eon-Assas, France
maria.rifqi@lip6.fr
Abstract. Physiology-based emotionally intelligent paradigms provide
an opportunity to enhance human computer interactions by continuously
evoking and adapting to the user experiences in real-time. However, there
are unresolved questions on how to model real-time emotionally intelli-
gent applications through mapping of physiological patterns to users’
aﬀective states.
In this study, we consider an approach for design of fuzzy aﬀective
agent based on the concept of typicality. We propose the use of typicality
degrees of physiological patterns to construct the fuzzy rules represent-
ing the continuous transitions of user’s aﬀective states. The approach
was tested on experimental data in which physiological measures were
recorded on players involved in an action game to characterize various
gaming experiences. We show that, in addition to exploitation of the
results to characterize users’ aﬀective states through typicality degrees,
this approach is a systematic way to automatically deﬁne fuzzy rules
from experimental data for an aﬀective agent to be used in real-time
continuous assessment of user’s aﬀective states.
Keywords: Machine learning, fuzzy logic, prototypes, typicality de-
grees, aﬀective computing, physiological signals.
1
Introduction
Emotions play a signiﬁcant role in normal human relations. Other than speech
and text channels, humans communicate very easily through implicit emotion
expressions. In many cases, humans attach importance on how something is
done or said and not only what was said or done. Communication is continually
modulated and enhanced through and by emotions. On the contrary, in human
computer interaction, computers rely mainly on the input and output, whose
eﬃectiveness is solely measured by the ability to execute and not how it was
executed. There is a need to develop methodologies for assessing user’s emotional
experience while interacting with computers. As a result, aﬃective computing [16]
has become a major research interest in Human Computer Interaction (HCI).
The main concern in this domain is how to enhance the quality of interaction
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 304–313, 2014.
c
⃝Springer International Publishing Switzerland 2014

Typicality Degrees Based Fuzzy Aﬀective Agent
305
between the user and the computer, making it more enjoyable by automatically
recognizing and adapting to the user’s emotional states.
In this context, among a vast range of possible ways to continuously assess
a user’s emotional responses such as facial gesture or voice recognition through
video and audio recording, physiological measures have a key advantage. In addi-
tion to their ability to be measured continuously in real-time, physiological data
are a record of involuntary autonomic nervous system processes and therefore
not culture speciﬁc like other modes.
In the recent past, scientiﬁc works have demonstrated the enormous prospects
in developing systems equipped with the ability to assess user emotional states
through the analysis of physiological data [8,2,14]. However, there are unresolved
questions on how to model real-time emotionally intelligent applications through
mapping of physiological patterns to users’ aﬃective states. In this study, we
consider a model that employs typicality degrees to construct fuzzy rules that
summarize cardinal physiological properties of users’ aﬃective states. We show
that, in addition to systematic exploitation of the data to characterize aﬃective
states through typicality degrees, the results can be extended to construct fuzzy
rules for an aﬃective agent to be used in real-time to assess user’s aﬃective states.
The rest of the paper is organized as follows: in Section 2, we give the state
of the art on physiological and aﬃective computing. In Section 3, we give details
of our approach. Then, in Section 4, we outline the details of the experimental
data for the design of aﬃective controller and give analysis of the test results.
Finally, we give conclusions and future perspectives in Section 5.
2
State of the Art
2.1
Modeling User’s Aﬀective States
In the ﬁeld of psychophysiology, many studies have been conducted to dis-
cover relationships between certain psychological states and physiological activ-
ity [3,8,2,14, gives a comparative study]. The experimental conceptualization has
heavily relied either on the emotional dimensional theory or the basic emotions
theory. On the one hand, dimensions theory considers emotions as dynamical,
boundaries-free states mainly represented in terms of arousal and valence [22,9].
On the other hand, in the basic emotions theory, emotions are conceptualized
as static, biologically-rooted states or classes such as anger, disgust, fear, joy,
sadness and surprise [7]. Other than variation in the methods of evoking emo-
tions and physiological measures used, there is a large volume of work to dis-
criminate emotions through physiological measures represented as the distinct
classes [17,18,3, among others]. For example, in [18] an attempt was made to
prove that the basic emotions of fear, anger, sadness and happiness are associ-
ated with distinct patterns of cardiorespiratory activity.
In relation to gameplay design, the user aﬃective states are modeled mainly
based on theory of ﬂow [5]. In relation to ﬂow, experiments are designed so as
to discriminate between three states based on game challenge [12,4]: easy game
(boredom), medium (engagement/ﬂow) and very diﬀcult (anxiety/frustration).

306
J.O. Orero and M. Rifqi
The agenda is to devise paradigms that enable real-time game adaptation accord-
ing to the player’s aﬃective states. For example, in the study by [4], participants
played Tetris and the player’s aﬃective states were hypothesized according to
three diﬀculty levels to give rise to: boredom, engagement/comfort and anxi-
ety/frustration states. In this work, we model the aﬃective states of an aﬃective
agent based on these three states of game challenge (low challenge, medium
challenge and very high challenge).
2.2
Assessment User’s Aﬀective States through Physiology
Various methods have been used in physiological modeling of basic emotions
such as k-nearest neighbors algorithm, discriminant analysis, support vector ma-
chines, bayesian networks and decision trees [17,22,19,9,3]. A compressive com-
parative study on these methods has been given in [14]. Speciﬁcally, in modeling
gameplay through physiology, similar methods have been proposed such artiﬁ-
cial neural networks [23], support vector machines [4], decision trees [12,15,11].
Although some of these models may have proved to successful in discriminating
aﬃective states, there has been less focus on how the extracted results should be
easily integrated in design of a real-life aﬃective systems.
In this context, [12]’s work is closer to our current study. In their study, a
decision trees based framework was used to map physiological measures to three
levels of anxiety based on diﬀculty: easy, moderately diﬀcult and very diﬀcult.
The extracted rules from the decision tree was then used to construct a controller
for an adaptive game. In our study, we consider a derivation of fuzzy rules from
the physiological data for the aﬃective controller.
2.3
Fuzzy Model in Physiological Computing
To begin with, modeling of aﬃective states through physiology involves aggre-
gation and fusion of various physiological measures and features. But the phys-
iological data from sensors is itself imperfect, such that it is diﬀcult to express
the results in crisp terms [1]. As such, it is necessary to express in fuzzy terms
the relationship between physiological data and aﬃective markers. Also, emo-
tions are conceptual quantities with indeterminate fuzzy boundaries [2]. As such
changes from one psychological state to the next can be gradual rather than
abrupt, owing to overlapping of class boundaries.
Models based on fuzzy logic are naturally most appropriate to represent the
continuous transitions, uncertainties and imperfections associated with physio-
logical data. In a fuzzy controller changes from one rule to another is gradual
with fuzzy values [0, 1] instead of crisp values {0, 1} in classical controller. There-
fore, in addition to robust decision making, fuzzy logic based models enable a
continuous assessment of these states to evoke the appropriate response.
In this domain, a fuzzy expert system has been proposed by [13]. In their study,
an expert knowledge derived from psychophysiological studies literature was used
to directly deﬁne fuzzy rules for each physiological signal. However, automatic
construction of fuzzy rules through induction of the experimental data has not

Typicality Degrees Based Fuzzy Aﬀective Agent
307
been explored before. Automatically deﬁning fuzzy rules based on induction from
experimental data has key advantages as it avoids prior deﬁnition of rules by an
expert, which are often subjective and diﬀcult to validate. Moreover, it enables
us to validate and compare the results with existing psychophysiological studies.
In this study, we propose typicality-based approach to automatically con-
struct fuzzy rules from the experimental data. The aim is to discover typical
psychophysiological patterns through induction of the collected physiological
data. We extract the most typical physiological patterns that best describe a
given user aﬃective state and use them as prototypes to construct fuzzy rules
relating to the aﬃective states.
3
Our Approach
3.1
Typicality and Prototypes from Cognitive Science Perspective
The concept of typicality and prototypes has been studied in the ﬁeld cognitive
science and psychology, initially by Rosch and Mervis [21]. According to their
study, typicality relies on the notion that some members of the same category are
more characteristic of the category they belong to than others. This is contrary
to the traditional thoughts that have treated category membership of items as
possessing a full and equal degree of membership. Some members are more char-
acteristic (typical) of the category they belong i.e have features that can be said
to be most descriptive of that category. Thus, subjects can belong to the same
category but diﬃering in their level of typicality. Typicality degree of an item
depends on two factors [21]; internal resemblance: an object’s resemblance to the
other members of the same category, and external dissimilarity; its dissimilarity
to the members of the other categories.
The concept of typicality can be used to deﬁne prototypes for a given group or
category as an object that summarizes the characteristics of the group. In this
case, a prototype of a given category is the object with the highest typicality in
that category.
3.2
Typicality Degrees
The aim is to discover pertinent psychophysiological characteristics based on
the concept of typicality. Since the typicality degree of an object indicates the
extent to which it resembles the members of the same group and diﬃers from
the members not in the same group, we can measure its power to characterize
i.e its ability to summarize the cardinal properties of a group. Speciﬁcally, we
consider Rifqi’s formalism [20] that computes the typicality degrees of objects
to automatically construct fuzzy prototypes.
Formally, let X be a data set composed of m instances in n dimensional space
and labeled to belong to a particular state or class, k ∈C and C = {1, . . . k, . . . c}
where c is the number of all possible states or classes.
It computes for each example, x ∈X, belonging to a given class, k, its internal
resemblance, R(x), the aggregate of similarity to the members in the same class

308
J.O. Orero and M. Rifqi
and its external dissimilarity, D(x), the aggregate of dissimilarity to members
not in the same class. The typicality degree, T (x), of x is then computed as the
aggregate of these two quantities given as:
R(x) = 1
|k|

y∈k
r(x, y)
(1)
D(x) =
1
|X\k|

z̸∈k
d(x, z)
(2)
T (x) = t(R(x), D(x))
(3)
Where r is a similarity measure for computing internal resemblance, d is a dis-
similarity measure for computing external dissimilarity, and t is an aggregation
operator for aggregating resemblance and dissimilarity. y is used to designate
examples belonging to the same class while z designates examples not belonging
to the same class as the given example x.
The choice of similarity measures, dissimilarity measures and aggregation op-
erators depends on the nature of the desired properties and have been studied in
detail [6,10]. In this study, we choose to use the normalized euclidian distance as
dissimilarity measure in Equation 2 and its complement as a similarity measure
in Equation 1. This ensures that both the internal resemblance and external
dissimilarity on a comparative scale. Then, to compute typicality degrees, as
an aggregation of internal resemblance and external dissimilarity in Equation 3,
we chose to use the symmetric sum. The symmetric sum has a reinforcement
property [6]. In such a case, if both the similarity and the dissimilarity are high,
the aggregated value becomes higher than any of the two and if both are low,
the aggregate becomes lower than any of the two values. This ensures that the
aggregation is high only if both the similarity and the dissimilarity are high and
vise versa.
We consider a prototype formulated by computing typicality degrees attribute
by attribute. In our context, we exploit this concept of typicality to determine
the characterization power of a given physiological feature from the typicality
degree of its prototypes. If an attribute typicality degree is high, then it follows
that the attribute is relevant in characterizing the given state. On the contrary,
if the typicality is low, then the attribute alone, can not be used as reference for
characterizing the given states.
3.3
Deﬁning Fuzzy Rules Based on Typicality Degrees
In this work, we wish to extend the concept of typicality degrees to construct
fuzzy rules for each state or class. We deﬁne fuzzy values based on typicality
of a particular attribute. We make the most typical example to have a full
membership of 1 and decreases until zero where the membership of the other
state is 1.
Formally, given v1, v2, . . . , vk . . . vs as the most typical values for states c1, c2,
. . . , ck, . . . , cs respectively and v1 < v2 < · · · < vk < · · · < vs. We use triangular

Typicality Degrees Based Fuzzy Aﬀective Agent
309
functions (for state ck) and trapezoidal functions (R-function for state c1 and
L-function for state cs). Therefore we have a membership function, μk, for a
given state, ck, in respect to values, σ of a given attribute given by:
μ1(σ) =
⎧
⎨
⎩
0
if σ > v2
v2−ϑ
v2−v1
if v1 ≤σ ≤v2
1
if σ < v1
(4)
μk(σ) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
0
if σ < vk−1
ϑ−vk−1
vk−vk−1
if vk−1 ≤σ < vk
vk+1−ϑ
vk+1−vk
if vk ≤σ ≤vk+1
0
if σ > vk+1
(5)
μc(σ) =
⎧
⎨
⎩
0
if σ < vs−1
ϑ−vs−1
vs−vs−1
if vs−1 ≤σ ≤vs
1
if σ > vs
(6)







	








Fig. 1. Representation of fuzzy memberships against signal values
In Figure 1, we show a representation of fuzzy memberships against signal
values. After constructing the fuzzy rules for each physiological feature, we use
the weighted mean based on the typicality degree of each attribute (feature) as
aggregation operator.

310
J.O. Orero and M. Rifqi
4
Experimental Data
4.1
Training Data Construction
To compute the typicality degrees for physiological features, we use data from
an experimental study in which physiological measures were recorded on players
involved in an action game as detailed in [11]. During this experiment, partici-
pants played successively four game sequences. Three of the four sequences that
were distinguished in terms of level of challenge and user’s aﬃective states we
classify the players’ experiences in relation to appraisal of challenge in three dis-
tinct categories: boredom (due to an insuﬀcient challenge), ﬂow/comfort (due
to comfortable level of challenge) and frustration (due to very high challenge).
We use three physiological measures recorded on thirty (30) participants dur-
ing the experiment: Electrodermal activity measure (EDA), Heart Rate (HR)
and Respiration Rate (RESP). To minimize the eﬃect of the transition periods,
we used only the physiological recordings of the last two minutes of each game
sequence.
For each participant, normalization was done for each signal using the mini-
mum and the maximum value of the signal for that participant. This is because
physiological signals are subject to signiﬁcant variations between individuals.
Also, to validate the homogeneity of the physiological signatures throughout a
given aﬃective state session, we subdivided these game sequences into 10 seconds
(2000 data points) segments, with a total of 12 segments for each game sequence.
Thus, we have a total of 1080 samples or segments.
4.2
Typicality Degrees and Fuzzy Rules
Typicality degrees for each segment/sample was computed for each physiological
signal, as detailed in Section 3: the average signal amplitude of electrodermal
activity, the heart rate and the respiration rate.
In Figure 2, we show a sample typicality degree curves for the average signal
amplitude of electrodermal activity.
As it can be seen from Figure 2, electrodermal activity was very relevant in
characterizing these three states: its typicality degree curves are clearly distinct
for the three states. The boredom state is easily characterized by low electroder-
mal activity (most typical value has typicality degree of 0.85). Similar behavior
is seen in the case of frustration state. The heart rate also had three distinct
patterns for the three states but typicality degrees were lower than those of elec-
trodermal activity. However, respiration rate’s characterization power is low for
all the three states i.e: the prototype typicality degree is less than 0.6 for all the
three states and their curves are almost horizontal.
As detailed in Section 3, the fuzzy aﬃective agent consists of a set of fuzzy
rules of these features that are used to determine the user’s aﬃective state from
experimental data. In this study, we focus on the aﬃective states in relation to
the Flow in the game. The aﬃective states are as presented before: Boredom,
Comfort (Flow) and Frustration.

Typicality Degrees Based Fuzzy Aﬀective Agent
311







	









	
	


	



Fig. 2. Flow states typical values for average electrodermal activity
In this case, we have v1, vk = v2, vs = v3 as the most typical values for states
c1, c2 and c3 respectively and v1 < v2 < v3. We use triangular functions (for state
c2) and trapezoidal functions (R-function for state c1 and L-function for state c3)
as given in Equation 5, Equation 4 and Equation 6. The values for Figure 2 above
were: v1 = 0.088, v2 = 0.420, v3 = 0.954. Based on Equation 4, Equation 5 and
Equation 6, respectively we have the fuzzy rules shown on Figure 3. For example,
if the EDA < 0.088 then Boredom = 1, Comfort = 0 and Frustration = 0,
. . . if the EDA > 0.95 then Boredom = 0, Comfort = 0 and Frustration = 1 .







	









	
	





Fig. 3. Fuzzy rules in ﬂow states for average electrodermal activity
As it can be seen, the constructed fuzzy rules are clearly realigned with the
typicality degrees of the signal in various aﬃective states. The constructed fuzzy
rules represent a more objective representation of psychophysiological relations
as they are induced from real experimental data.

312
J.O. Orero and M. Rifqi
5
Conclusions and Perspectives
In this study, we have presented an approach to design a fuzzy aﬃective agent
based on typicality degrees to assess aﬃective states through physiological signals.
We considered typicality as per cognitive and psychology principles of catego-
rization to deﬁne pertinent psychophysiological relations.
We demonstrated the viability of our framework by deﬁning fuzzy rules for
design of a fuzzy agent formulated from our characterization results to assess
player’s experiences in relation to various activities in the game. This demon-
strates both the viability of developed framework and how it the model can be
used in the design of emotionally intelligent HCI applications.
However, further studies need to be done to test the model on other real-
life scenarios to assess the user’s emotional processes. In particular, to consider
multi-modal fusion of measures such as audio-visual and various physiological
measures.
References
1. Bouchon-Meunier, B.: Aggregation and Fusion of Imperfect Information. Physica-
Verlag, Spring-Verlag Company (1998)
2. Calvo, R.A., D’Mello, S.: Aﬀect detection: an interdisciplinary review of models,
methods, and their applications. IEEE Transactions on Aﬀective Computing 1,
18–37 (2010)
3. Chanel, G., Kierkels, J.J.M., Soleymani, M., Pun, T.: Short-term emotion assess-
ment in a recall paradigm. International Journal of Human-Computer Studies 67,
607–627 (2009)
4. Chanel, G., Rebetez, C., Btrancourt, M., Pun, T.: Emotion assessment from phys-
iological signals for adaptation of game diﬃculty. IEEE Transactions on Systems,
Man, and Cybernetics 41(6), 1052–1063 (2011)
5. Csikszentmihalyi, M.: Harper and row ﬂow: the psychology of optimal experience.
Harper & Row, New York (1990)
6. Detyniecki, M.: Mathematical aggregation operators and their application to video
querying. PhD thesis, Universit´e Pierre et Marie Curie, France (2001)
7. Ekman, P., Levenson, R.W., Friesen, W.V.: Autonomic nervous system activity
distinguishes among emotions. Science 221, 1208–1210 (1983)
8. Fairclough, S.H.: Fundamentals of physiological computing. Interacting With Com-
puters 21, 133–145 (2009)
9. Kim, J., Andr´e, E.: Emotion recognition based on physiological changes in music
listening. IEEE Transactions on Pattern Analysis And Machine Intelligence 30(12),
2067–2083 (2008)
10. Lesot, M.-J., Rifqi, M., Bouchon-Meunier, B.: Fuzzy prototypes: from a cognitive
view to a machine learning principle. In: Bustince, H., Herrera, F., Montero, J.
(eds.) Fuzzy sets and Their Extensions: Representation, Aggregation and Models.
STUDFUZZ, vol. 220, pp. 431–452. Springer, Heidelberg (2008)
11. Levillain, F., Orero, J.O., Rifqi, M., Bouchon-Meunier, B.: Characterizing player’s
experience from physiological signals using fuzzy decision trees. In: IEEE Confer-
ence on Computational Intelligence and Games (2010)

Typicality Degrees Based Fuzzy Aﬀective Agent
313
12. Liu, C., Agrawal, P., Sarkar, N., Chen, S.: Dynamic diﬃculty adjustment in com-
puter games through real-time anxiety-based aﬀective feedback. International Jour-
nal of Human-Computer Interaction 25(6), 506–529 (2009)
13. Mandryk, R.L., Atkins, M.S.: A fuzzy physiological approach for continuously mod-
eling emotion during interaction with play technologies. International Journal of
Human-Computer Studies 65(4), 329–347 (2007)
14. Novak, D., Mihelj, M., Munih, M.: A survey of methods for data fusion and system
adaptation using autonomic nervous system responses in physiological computing.
Interacting with Computers 24, 154–172 (2012)
15. Orero, J.O., Levillain, F., Damez-Fontaine, M., Rifqi, M., Bouchon-Meunier, B.:
Assessing gameplay emotions from physiological signals: a fuzzy decision trees
based model. In: International Conference on Kansei Engineering and Emotion
Research (2010)
16. Picard, R.W.: Aﬀective computing. The MIT Press, Cambridge (1997)
17. Picard, R.W., Vyzas, E., Healey, J.: Toward machine emotional intelligence: Anal-
ysis of aﬀective physiological state. IEEE Transactions Pattern Analysis and Ma-
chine Intelligence 23, 1175–1191 (2001)
18. Rainville, P., Bechara, A., Naqvi, N., Damasio, A.R.: Basic emotions are associ-
ated with distinct patterns of cardiorespiratory activity. International Journal of
Psychophysiology 61, 5–18 (2006)
19. Rani, P., Sarkar, N., Adams, J.: Anxiety-based aﬀective communication for implicit
human machine interaction. Advanced Engineering Informatics 21, 323–334 (2007)
20. Rifqi, M.: Constructing prototypes from large databases. In: International Con-
frence on Information Processing and Management of Uncertainity in Knowledge-
Based System, IPMU (1996)
21. Rosch, E., Mervis, C.: Family resemblance: studies of the internal structure of
categories. Cognitive Psychology 7, 573–605 (1975)
22. Wagner, J., Kim, J., Andr´e, E.: From physiological signals to emotions: implement-
ing and comparing selected methods for feature extraction and classiﬁcation. In:
IEEE International Conference in Multimedia and Expo, pp. 940–943 (2005)
23. Yannakakis, G.N., Hallam, J.: Entertainment modeling through physiology in phys-
ical play. International Journal of Human-Computer Studies 66, 741–755 (2008)

Multilevel Aggregation of Arguments in a Model
Driven Approach to Assess an Argumentation
Relevance
Olivier Poitou and Claire Saurel
ONERA
Toulouse, France
{first name.last name}@onera.fr
Abstract. Figuring out which hypothesis best explain an observed on-
going situation can be a critical issue. This paper introduces a generic
model based approach to support users during this task. It then focuses
on an hypothesis relevance scoring function that helps users to eﬃcently
build a convincing argumentation towards hypothesis. This function uses
a multi-level extension of Yager’s aggregation algorithm, exploiting both
the strength of the components of an argumentation, and the conﬁdence
the user puts in them. The presented work was illustrated on a maritime
surveillance application.
Keywords: decision support, argumentation, model-based approach,
multilevel multicriteria aggregation.
1
Introduction
As far as sensor, vision, information, communication or intelligence technologies
have been widely developed and improved, a worthy challenge is to exploit these
capacities to eﬀciently support surveillance activities in operational situations.
In lots of critical domains surveillance is still performed by human operators
only equipped with paper and pencils. In those conditions, dealing with a huge
amount of uncertain and partial data, more or less reliable information sources,
and evolving and imperfectly deﬁned potential threats within a sometime very
short time period is almost unfeasible or at least error prone. Maritime domain
provided such an initial case study [1] for the theoretical work presented here1.
One aim is so to provide users with support in identifying ongoing abnormal
behaviours in order to ease their decision making to face up to them. Automatic
model-based approaches often fail because it is diﬀcult to capture all character-
istics of all abnormal behaviours in models : either new occurrences of a generic
behaviour have non modelled speciﬁcities, or not yet modelled behaviours ap-
pear. So human intervention seems necessary in a model-based situation recog-
nition process. However, we claim that a model-based assistance can help users
1 Our work has been funded by several projects since 2009: TaMaris (French Research
Agency , ANR), SisMaris (FUI): http://www.sismaris.org/ and now I2C (EU):
http://www.i2c.eu/, and also by ONERA fundings.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 314–323, 2014.
c
⃝Springer International Publishing Switzerland 2014

Multilevel Aggregation of Arguments in a Model Driven Approach
315
focusing their information gathering and analysis process, hence improving the
surveillance process eﬀciency. Moreover, as an assistance process keeps the user
in the loop it should ease users’ acceptance and promotion.
The generic approach we propose aims at : guiding users’ investigation, pro-
viding them with argumented hypothesis about ongoing situation identiﬁcation,
giving them a mean to compare argumentations. Users can then compare hypoth-
esis relevance to explain the observed situation, before choosing which hypothesis
appear to them as the most likely ones, then taking suited decision.
Our approach is based on models of behaviours which can explain most of
abnormal or suspicious situations that are to be interpreted and recognized.
Starting from alarms automatically raised about an observed situation2 it con-
sists ﬁrst in providing users with an initial set of abnormal behaviour hypothesis
which might explain the alarms, then in assisting them into further hypothesis
assessment, to get the hypothesis which best explain the ongoing situation. An
hypothesis assessment is deﬁned as the score of its argumentation – this argu-
mentation being continuously built from information collected by users on the
observed situation according to the contents of the associated model. A multi-
level aggregation algorithm is then used to assess hypothesis relevance to explain
the ongoing situation, by merging some characteristics of pro or cons arguments.
This paper focuses on information modeling and processing techniques under-
lying the behaviour hypothesis generation and assessment support.
Section 2 introduces the principles of the behaviour model-based approach.
Behaviour models and their formalization are described in section 3. In section
4 we explain how behaviour models generate behaviour hypothesis, and how an
argumentation is implicitly created for each hypothesis, based upon user’s beliefs
collected about the situation. Then in section 5 we make a focus on a multilevel
aggregation algorithm we have developed, in order to assess the behaviour hy-
pothesis matching to the ongoing situation. This algorithm here deals with two
ingredients of the quality of its argumentation : argument strength, and user’s
conﬁdence in its arguments. Section 6 concludes by positioning this work.
2
An Abnormal Behaviour Model-Driven Approach
Our approach relies on an abnormal behaviour models base which collects struc-
tured business knowledge, gathering some features which are often tied to oc-
currences of the described behaviour or, conversely, which generally disclaim the
described behaviour. When an alarm is thrown, it comes in with a set of infor-
mation about the ongoing situation. The interpretation support component then
goes through all the abnormal behaviour models, and if some alarm information
matches, even partially, a model contents, then this model is retained as an hy-
pothesis of ongoing situation identiﬁcation. From this initial set of hypothesis,
the aim is then to support users in further assessment : by using the model
contents to guide them in their collection of additional relevant information on
2 Meaning for instance that some physical parameters, as reported by sensors, have
abnormal values according to business rules.

316
O. Poitou and C. Saurel
Situation interpretation module
Detection systems
Smart alarm generator
Abnormal behaviour models base
User Beliefs
Hypothesis set manager
Hypothesis scoring
...
Information Sources
Decision
add/update belief
add hypothesis
Qualiﬁed hypothesis set
Fig. 1. A model-driven approach for inquiry assistance
the ongoing situation, and by evaluating how each hypothesis is likely to ex-
plain the observed situation at any state of the users’ knowledge acquisition (see
Figure 1).
The process ends when the user decides to stop the hypothesis investigation,
generally once he or she has got some hypothesis with a fairly high relevance
evaluation score.
3
Abnormal Behaviour Models: Concepts and Structure
Abnormal behaviour models describe speciﬁc features of situations in a given
application domain, according to users. These features are structured as sets
of conditions which roughly correspond to important clues of the ongoing oc-
curence or non occurence of the behaviour. Conditions are modelled as a user
understandable label and its pro or cons impact on the identiﬁcation of the be-
haviour. These conditions may be associated to a list of sources with a default
conﬁdence value.
The contents of a behaviour model is represented within the following formal
frame.
Let P be the set of all domain available propositions, supposed to be logically
independent.3
Let A be the set of argumentary forces {−μ, −3, −2, −1, 0, 1, 2, 3, μ}, where μ
and −μ are pure symbolic values.
Let M be the abnormal behaviour model set, with ∀m ∈M, m =< idm, Cm >
stating that :
– Pm ⊂P, the set of propositions considered by users to be relevant for the
model m deﬁnition
3 Proposition labels will all be expressed with aﬃrmative interrogative sentences,
since negative sentences are not user-friendly. For instance, in a context of maritime
surveillance : is the ship older than 15 years old ?

Multilevel Aggregation of Arguments in a Model Driven Approach
317
– Cm = {(p, f, cf) | p ∈Pm, f = fpropm(p), cf = cfpropm(p)}, being the clue
set associated to the model m
– fpropm : Pm →A , cfpropm : Pm →A , two functions respectively giving the
argumentary force (used if proposition is assessed to be true) and counter
force (used if the proposition is assessed to be false) of a proposition towards
the hypothesis that would derive from the model.
A represents the set of argumentary forces values going from −μ (the
proposition can be considered as a proof that the hypothesis is WRONG)
to μ (the proposition can be considered as a proof of the hypothesis being
RIGHT).
Note that the numerical values of the set {-3,... 3} are not signiﬁcative
from a numerical point of view : the aim is only to provide a preference order
between argumentary forces. What is important is to get a scale which seems
convenient for users (depending on applicative context) .
In the context of an hypothesis h based upon an abnormal behaviour model
m being assessed by an user, a proposition associated to its truth value (and
corresponding argumentary force ou counter force) will become an argument (or
counter argument) towards h.
4
From Abnormal Behaviour Models to Hypothesis
Abnormal behaviour models are used in order to be compared with the user’s
belief set, formally deﬁned hereafter.
let B denote {T rue, False}, a set of truth values.
let interpe : P →B the partial function that indicates which truth value is
associated to a proposition in an e user’s mind,
let Pe = {p ∈P, interpe(p) ∈B}. Pe denotes the subset of propositions on
which an user e has beliefs.
let confide : P →]0, 1] the real function that, given a proposition, returns the
conﬁdence a user e puts in the belief that the proposition has the truth value
returned by interpe (a value of 1 being reported as an absolute conﬁdence).
Given a function interpe, the belief set Be of the user e is then deﬁned as
Be = {(p, i, c) | p ∈Pe, i = interpe(p), c = confide(p)}
Given an ongoing observed situation, there are at least two ways of deﬁning
the contents of Be : alarms produced from sensors4, and beliefs acquisition about
the situation.
Alarms are supposed to be reliable. They are expressed as a set BA of beliefs,
each of them being an n-uplets < p, v, c > where p ∈P, v is the truth value of
p, and c = 1 represents the maximal conﬁdence for (p, v). We state : Be ⊃BA
(informally user fully accepts information coming from alarms).
Abnormal behaviour hypothesis are generated from BA and M (the behaviour
hypothesis models set) : given an behaviour hypothesis model m, if a belief from
4 This process is outside the scope of this paper: see [Brax, N. and al, 2012], or [Ray,
C. and al, 2013].

318
O. Poitou and C. Saurel
BA would become a positive argument for the hypothesis that would derive from
m, then m is actually derived into an hypothesis h potentially explaining the
situation. Formally, m is derived into h if and only if
∃< p, v, 1 >∈BA,
p ∈Pm ∧[(v = T rue ∧fpropm(p) > 0) ∨(v = False ∧cfpropm(p) > 0)]
Note that at any time the user may also declare a behaviour hypothesis model
as generating a candidate hypothesis, as far this model belongs to the model base.
Once a set H of hypothesis has been deﬁned, the mission of the user is to
furthermore assess hypothesis relevance to provide good explanations on the
observed situation. The contents of each hypothesis may be used as a guideline
to support user in his beliefs acquisition, by suggesting propositions on which it
is worth to make an opinion.
Thus we deﬁne the argument set of an hypothesis h as the set of its proposi-
tions that can ﬁnd a truth value in the beliefs of the user e.
Given a function interpe deﬁned on P,
Ah,e = {(p, c, f) | p ∈Ph ∩Pe, c = confide(p), f = fargh,e(p)}
with fargh,e(p) : P →A being

fproph(p) when interpe(p) = T rue
cfproph(p) when interpe(p) = False
For a given hypothesis h and an user e lets state that,
– fargh,e(p) is the argument force for p
– the sign of an argument fargh,e(p) gives the direction of the argument, p is
said to be positive (resp. negative) if
fargh,e(p) > 0 (resp. < 0).
– an argument p such that fargh,e(p) ∈{μ, −μ} is said to be a deﬁnitive
argument (or proof).
Given an argument set of an hypothesis and a user, the relevance of this
hypothesis to explain the ongoing situation is evaluated through the quality and
relevance score of the argumentation based on these arguments. This score is
computed with a multicriteria aggregation algorithm which is described in the
next section.
5
Hypothesis Evaluation
The aim is to evaluate the relevance of an abnormal behaviour hypothesis, that
means its ability to explain the observed situation (represented by user’s beliefs).
We assess an hypothesis relevance by assessing the argumentation produced to-
wards the hypothesis as described above, based upon its arguments’ features.
Arguments of hypothesis are characterized by two dimensions: argument force
(pro or cons the described behaviour) and conﬁdence, each of these dimensions
having several intensity levels. Thus we considered that deﬁning a global rele-
vance score of an hypothesis was a multicriteria aggregation process. Hence the
scoring function had to be written as a multicriteria aggregation function.

Multilevel Aggregation of Arguments in a Model Driven Approach
319
5.1
Quick Review of Algorithms
The problematic characteristics suggest that the chosen algorithm should have
the following properties.
1. Monotony against argument: if an argument a is added then the global
score evolves in the same direction as a
2. Monotony against argument metainformation: if argument metadata
improves (resp. downgrades) then the global score increases (resp. decreases)
3. Absorbance: if the belief set contains a deﬁnitive argument with a full
conﬁdence then the corresponding hypothesis must be evaluated as “sure”
(if the argument is positive) or “sure that not” (if the argument is negative).
4. Inconsistency detection: if the argument set of an hypothesis contains
both deﬁnitive positive and deﬁnitive negative arguments, both associated
to a full conﬁdence, then the state is inconsistent and no score should be
computed (formally the codomain of hsf is extended with the symbolic value
⊥that is returned in this situation)5.
5. Limited criteria compensability: The compensability is the ability to
obtain a high global evaluation even when having a low evaluation on a
criteria (intuitively: several good marks compensate a bad one).
6. No arbitrary weigth numerical values
Min and Max aggregation algorithm family does not meet the strict monotony
properties (1 and 2). Most usual and simple aggregation methods, such as aver-
ages and sums, are fully compensatory and then not suitable (property 5). Some
more complex weighted techniques try to overcome this diﬀculty: but, in this
family of functions, static numeric weights that are associated to each criteria
(or criteria combination) are diﬀcult to choose and control (property 6). Limits
to these weighted approaches are reached for example with Choquet integrals [2]
and its 2n weights for n criteria. Techniques to “guess” weight values then seem
to be necessary and they do not match the approach adopted here (no historical
data available and lower user acceptance).
In addition, none of above mentionned algorithms meets properties 3 and 4.
Prioritized aggregation functions [3] try to address these diﬀculties by relying
on a preference relation between criteria instead of on some arbitrary numeric
weights6. Criteria weights values are then no longer static and depend on each
alternative to evaluate (see ﬁgure 2). During evaluation of each alternative, the
weight that is to be associated to each criteria will be computed, based onto the
rank of the criteria class in the preference relation and the scores obtained in
user preferred criteria classes.
Practically a 1 value is given to the weight w1 associated with the most pre-
ferred criteria class, and all criteria scores v are such that 0 ≤v ≤1. Then the
next weight value is computed by the formula wi+1 = wi × mi.
5 Such a situation should never occur, when it does, either the user has done something
wrong or the model itself is wrong.
6 Given two criteria c1 and c2, c1 > c2 means that the user considers c1 more impor-
tant than c2, c1 ∼c2 means that c1 and c2 are in the same class.

320
O. Poitou and C. Saurel
Score(a) =
N

i=1
scorei(a) × wi(a)
where :
scorei(a) =

c∈cci
scorec(a)
∥cci∥
w1(a) = 1 and wi+1(a) = wi(a) × mi(a)
mi(a) = minc∈cci scorec(a)
cci being the ith criteria class
Fig. 2. An implementation of Yager’s algorithm
Thus, an alternative obtaining a low score for an important criteria, will result
in small weights for less important criteria, hence reducing the impact of less
preferred criteria scores on the overall evaluation score of the alternative.
This approach may be particularly eﬀcient from the compensability limitation
point of view [4] and may even oﬃer absorbing elements opportunity. Indeed if
the score corresponding to a given importance level criteria is null, then all lower
importance criteria will inherit a null weight : that means that they will not be
taken into account at all (this is what is previously referred to as absorbance
property). This is particularly powerful when applied to high importance level :
applying this at the highest level corresponds to consider the concerned criteria
as a necessary condition which must be ﬁlled for the alternative to be worthwhile
to be considered.
The ability of Yager’s algorithm to tend to overcome unwanted compensa-
tion eﬃects is not the only advantage it provides. Other advantages are that its
processing doesn’t require neither lots of experimental input data, nor some-
what arbitrary numerical weights; it relies mostly on symbolic notions, as user’s
preferences between criteria. These advantages are particularly relevant for ap-
plications where a suﬀcient reference data set is unavailable, for instance because
of conﬁdentiality constraints.
5.2
Algorithm Description
Our aggregation method is based upon the principles of prioritized aggregation
functions. However, we do not have a unique total preference relation on the
whole boolean criteria set. Instead we have distinct sets of criteria, so called
dimensions, on which such a relation exists. In our initial case study, dimensions
are conﬁdence in belief and argument force, in each of them a criteria corresponds
to an intensity degree. In order to keep compensation control property on every
dimensions, our approach was to keep dimensions separated, recursively consid-
ering each of them as a level of importance inside of which the next one also
represents a level of importance (i.e. deﬁning a preference relation between the
dimensions). We are so deﬁning a multi-level version of prioritized aggregation
functions, where each level is processed using a similar prioritized aggregation

Multilevel Aggregation of Arguments in a Model Driven Approach
321
algorithm : thus the criteria non compensability property is favoured at every
level (see ﬁgure 3).
As a ﬁrst introduction to the algorithm, let’s describe the two dimensions
version from the case study, and arbitrarily suppose that conﬁdence have been
preferred to argument force.
Following Yager, at the highest ﬁrst level (conﬁdence, in this case), the highest
conﬁdence degree is associated to a weight of w11 = 1. Suppose we have n1
degrees in this ﬁrst dimension, the global evaluation of hypothesis h will be:
Score1(Ah,e) =
n1

k1=1
w1k1 × Score2(Ah,e<k1>)
where Score2(Ah,e<k1>) is a computed score for the set of arguments having the
kth
1 degree according to the ﬁrst dimension.
At the lowest level, the highest force intensity degree is associated with a
maximal weight w21 = 1. Suppose we have n2 degrees in this second dimension.
Value Score2 of a given conﬁdence level k1 will then be computed with the
formula :
Score2(Ah,e<k1>) =
n2

k2=1
w2(k2) × localScore(Ah,e<k1,k2>)
where localScore(Ah,e<k1,k2>) is the score of the subset of arguments having a
k1 conﬁdence degree and a k2 force degree for the hypothesis h here deﬁned as
proportional to its cardinality. Each argument contributes to the score positively
or negatively according to its direction.
Note that numerical values of argument strength and argument conﬁdence are
only used through the preference order they induce. That means for instance,
that arguments having 3 as argument strength value is considered by experts
as more important than arguments having 2. So numerical values of argument
strength and argument conﬁdence have no direct impact in the ﬁnal score, as
far as their value scale is on accordance with the intended expert’s preference
order.
5.3
Algorithm Additional Tricks
As deﬁnitive arguments better correspond to a fully symbolic reasoning ap-
proach, their process is made outside of the previously described computation.
Thus maximal positive and negative scores as well as inconsistency are discov-
ered and dealt with before trying to estimate a numerical score.
Normalization of a positive score is realized by computing the maximal score
(based on a virtual belief set making all propositions produce positive arguments
for the hypothesis7), then dividing the score computed with the real belief set
7 Or null if the maximum between force and counter force is zero.

322
O. Poitou and C. Saurel
(∀d : 1..N)
scored(A<k1,..,kd−1>) = normd(
nd

kd=1
scored+1(A<k1,..,kd>) × wdkd )
with :
A<k1,..,kq> =
 A for q=0
{(p, k1, .., kq, vq+1, ..., vN) ∈A} elsewhere
wd1 = 1 and wdj+1 = impact(wdj, score(A<k1,..,kd−1>)) −naturalGradientdimi
scoreN(A<k1,..,kN >) = normlocal(localScore(A<k1,..,kN >))
and :
A = {(p, v1, ...vN), p ∈P} each vd being the set of evaluations of p against the
criteria from the criteria classes of the dimension d
kd = c meaning that k represents the criteria class c of the dimension d
Fig. 3. Proposed N-dimensional generic algorithm
by this maximum. Conversely negative score normalization is based on a virtual
belief set producing only negative arguments for the hypothesis.
We also introduce a minimum weight decrease between levels so that in no
case a lower level has the same weight as a higher one (naturalGradient).
6
Results and Perspectives
We have proposed a generic model-based approach to support users in any in-
vestigation process where potential decisions are modeled in a clue list way. We
then enable them to get argumented hypothesis as well as a mean to compare
hypothesis relevance based on their argumentation.
The multi-level version of the Yager prioritized aggregation algorithm we have
developed to score hypothesis argumentation has several advantages :
– it only relies on preferences between criteria and between criteria dimensions,
– it does not suﬃer of somewhat arbitrary numerical choices,
– it does not require a lot of historical data (no learning process),
– and its results satisfy usually wishable properties.
This algorithm is generic and would apply to any context where aggregating
several criteria dimensions is required. It is particularly relevant when suﬀcient
reference data set is unavailable to compute advanced weights (as with Choquet
or Sugeno integral algorithms[5]).
Some frameworks have been proposed in the literature to formalize argumen-
tation [6], and several types of interactions between component arguments have
been identiﬁed (for instance : support, aggregation, conﬂict). More recently some
works have exhibited the impact of some argument meta-information, such as
strength or validity in time, on the value of an argumentation [7] [8] [9]. Our
concepts of argument strength and user’s argument conﬁdence are such useful

Multilevel Aggregation of Arguments in a Model Driven Approach
323
meta-information (close to graded truth uncertainty sources from [10]). Our work
can be thus compared to this family of works [9]. However our argumentations are
only structured with aggregation and conﬂict interactions between arguments,
since we suppose all component arguments are logically independent. The multi-
level aggregation algorithm we have developed can be considered as a generic
operator which combines two (or more) kinds of argument meta-information (in
the illustration: strength and conﬁdence). As such it can be easily integrated in
many processes dealing, for instance, with imperfect information reasoning.
Our approach and algorithm are implemented [1] in a full maritime surveil-
lance and investigation chain (I2C project, http://www.i2c.eu/). Other projects
of the same domain have similar issues [11]. But as far as we know, they do not
include an assistance module as what we propose.
References
1. Poitou, O., Saurel, C.: Supporting situation assessment by threat modeling and
belief analysis. In: OCOSS Ocean and Coastal Observation: Sensors and observing
Systems, Numerical Models and Information. Nice (2013)
2. Choquet, G.: Theory of capacities. In: Annales de l’Institut Fourier – tome 5, pp.
131–295 (1953)
3. Yager, R.: Prioritized aggregation operators. International Journal of Approximate
Reasoning 48, 263–274 (2008)
4. da Costa Pereira, C., Dragoni, M., Pasi, G.: A prioritized “and” aggregation oper-
ator for multidimensional relevance assessment. In: Serra, R., Cucchiara, R. (eds.)
AI*IA 2009. LNCS (LNAI), vol. 5883, pp. 72–81. Springer, Heidelberg (2009)
5. Grabisch, M.: Fuzzy Measures and Integrals: Theory and Applications. Springer-
Verlag New York, Inc., Secaucus (2000)
6. Dung, P.: On the acceptability of arguments and its fundamental role in nonmono-
tonic reasoning, logic programming and n-person games. Artiﬁcial Intelligence 77,
321–357 (1995)
7. Bench-Capon, T.: Value-based argumentation frameworks, pp. 7–22. Taylor &
Francis (2002)
8. Pollock, J.: Defeasible reasoning and degrees of justiﬁcation. In: Argument and
Computation, vol. 1(1), pp. 7–22. Taylor & Francis (2010)
9. Maximiliano, C.D., Budan, M.J., Gomez Lucero, G.R.S.: Modeling reliability vary-
ing over time through a labeled argumentative framework. In: IJCAI 2013 workshop
on weighted logics for Artiﬁcial Intelligence, WL4AI 2013 (2013)
10. Demolombe, R.: Graded trust. AAMAS Trust, pp. 1–12 (2009)
11. Ray, C., Granger, A., Thibaud, R., Etienne, L.: Temporal rule-based analysis of
maritime traﬃc. In: OCOSS Ocean and Coastal Observation: Sensors and Observ-
ing Systems, Numerical Models and Information, pp. 171–178. Nice (2013)

Analogical Proportions and Square
of Oppositions
Laurent Miclet1 and Henri Prade2
1 University of Rennes 1 – Irisa, Lannion, France
2 CNRS/IRIT – University of Toulouse, Toulouse, France
laurent.miclet@univ-rennes1.fr, prade@irit.fr
Abstract. The paper discusses analogical proportions in relation with
the square of oppositions, a classical structure in Ancient logic which is
related to the diﬀerent forms of statements that may be involved in de-
ductive syllogisms. The paper starts with a short reminder on the logical
modeling of analogical proportions, viewed here as Boolean expressions
expressing similarities and possibly diﬀerences between four items, as in
the statement “a is to b as c is to d”. The square of oppositions and its
hexagon-based extension is then restated in a knowledge representation
perspective. It is observed that the four vertices of a square of oppositions
form a constrained type of analogical proportion that emphasizes diﬀer-
ences. In fact, the diﬀerent patterns making an analogical proportion true
can be covered by a square of oppositions or by a “square of agreement”,
leading to disjunctive expressions of the analogical proportion. Besides,
an “analogical octagon” is shown to capture the general construction of
an analogical proportion from two sets of properties. Since the square of
oppositions oﬀers a common setting relevant for syllogisms and analogi-
cal proportions, it also provides a basis for the discussion of the possible
interplay between deductive arguments and analogical arguments.
1
Introduction
Deductive reasoning and analogical reasoning are two ways of drawing inferences.
Both were already clearly identiﬁed in the Antiquity. Syllogistic reasoning was
then the basis for deduction. Related to syllogisms is the square of oppositions
which dates back to the same time. In its original form, the square of oppositions
was associated with universal and existential statements and their negations,
which corresponds to the diﬃerent statements encountered in syllogisms.
While deductive reasoning may involve both universal and existential state-
ments, analogical reasoning only considers instantiated statements. A basic ﬁgure
of analogical reasoning is analogical proportion, i.e. a comparative statement of
the form “a is to b as c is to d”. A logical view of analogical proportion has been
recently proposed [6]. Then the analogical proportion reads “a diﬃers from b as
c diﬃers from d and b diﬃers from a as d diﬃers from c”, which involves negation
in the logical writing. This fact, together with the quaternary nature of such
statements, leads us to establish a connection between the analogical proportion
and the square of oppositions, whose meaning is investigated in the following.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 324–334, 2014.
c
⃝Springer International Publishing Switzerland 2014

Analogical Proportions and Square of Oppositions
325
The paper ﬁrst brieﬂy recalls the logical view of the analogical proportion in
Section 2, and its relevance to analogical reasoning. Then the square of oppo-
sitions and its hexagonal extension to a triple of similar squares is restated in
Section 3, pointing out its interest in the analysis of the relations between cate-
gorical statements. Section 4 shows the relevance of the square of oppositions for
discussing analogical proportions as well, while Section 5 discusses the interplay
of arguments based on structures of opposition, before concluding in Section 6.
2
The Propositional View of the Analogical Proportion
An analogical proportion “a is to b as c is to d”, denoted a : b :: c : d, is
supposed to satisfy two characteristic properties: i) a : b :: c : d is equivalent
to c : d :: a : b (symmetry), and ii) a : b :: c : d is equivalent to a : c :: b : d
(central permutation). There are in fact 8 equivalent forms obtained by applying
symmetry and permutation. Moreover, a : b :: a : b always holds (reﬂexivity).
Viewing a, b, c and d as Boolean variables, and a : b :: c : d as a quaternary
connective forces the analogical proportion to be true for the following 6 patterns
0 : 1 :: 0 : 1, 1 : 0 :: 1 : 0, 1 : 1 :: 0 : 0, 0 : 0 :: 1 : 1, 1 : 1 :: 1 : 1, 0 : 0 :: 0 : 0.
A logical expression, which is true only for these 6 patterns (among 24 = 16
possible entries) has been proposed in [6]:
a : b :: c : d = ((a ∧¬b) ≡(c ∧¬d)) ∧((¬a ∧b) ≡(¬c ∧d)).
This remarkable expression makes clear that a diﬃers from b as c diﬃers from
d and, conversely, b diﬃers from a as d diﬃers from c. This clearly covers the
patterns 0 : 1 :: 0 : 1, 1 : 0 :: 1 : 0, as well as the 4 remaining patterns where
a and b are identical on the one hand and c and d are also identical on the
other hand. It can be easily checked that under this logical view the analogical
proportion indeed satisﬁes symmetry, central permutation, and reﬂexivity. It
is also transitive. Moreover a : b :: ¬b : ¬a also holds. Such a logical view
of an analogical proportion can be advocated as being the genuine symbolic
counterpart of the notion of numerical proportion [10,11].
This view easily extends to Boolean vectors a, b, c, d, whose components
can be thought as binary attribute values, each vector describing a particular
situation. Let xi be a component of a vector x. Then an analogical proportion
a : b :: c : d between such vectors can be deﬁned componentwise, i.e., for each
component i, the analogical proportion ai : bi :: ci : di holds. Then the diﬃerences
between a and b pertain to the same attribute(s) as the diﬃerence between c
and d and are oriented in the same way (e.g., if from ai to bi, one goes from 1
to 0, it is the same from ci to di). When there is no diﬃerence between ai and
bi, then ci and di are as well identical. This modeling of analogical proportions
has been successfully applied to learning [5] and to the solving of IQ tests [8].
This vector-based view can be easily related to a set-based view originally
proposed in [4], later proved to be equivalent (see [6] for details). In the set-
based view each situation is associated with the set of attributes that are true
in it. Let us denote A, B, C, and D the sets thus associated with a, b, c, and
d, respectively. Then we shall write A : B :: C : D if a : b :: c : d holds.

326
L. Miclet and H. Prade
Note that in particular, for any pair of sets A and B, A : B :: B : A holds (where
A denotes the set complement of A).
The case of attributes on discrete domains with more than 2 values can be
handled as easily as the binary case. Indeed, consider a ﬁnite attribute domain
{v1, · · · , vm}. This attribute can be binarized by means of the m properties
“having value vi, or not”. Consider the partial description of objects a, b, c,
and d with respect to this attribute. Assume, for instance, that objects a and
c have value v1, while objects b and d have value v2. Then it can be checked
that an analogical proportion holds true between the four objects for each of
the m binary property, and in the example, can be more compactly encoded as
an analogical proportion between the attribute values themselves, namely here:
v1 : v2 :: v1 : v2. More generally, x and y denoting possible values of a considered
attribute, the analogical proportion between objects a, b, c, and d holds for this
attribute iﬃthe 4-tuple of their values wrt this attribute is equal to a 4-tuple
having one of the three forms (s, s, s, s), (s, t, s, t), or (s, s, t, t).
3
The Square of Oppositions
Let us start with a refresher on the classical square of opposition [7]. This square
involves four logically related statements exhibiting universal or existential quan-
tiﬁcations: it has been noticed that a statement (A) of the form “every x is p” is
negated by the statement (O) “some x is not p”, while a statement like (E) “no
x is p” is clearly in even stronger opposition to the ﬁrst statement (A). These
three statements, together with the negation of the last one, namely (I) “some
x is p”, give birth to the Aristotelian square of opposition in terms of quantiﬁers
A : ∀x p(x), E : ∀x ¬p(x), I : ∃x p(x), O : ∃x ¬p(x), pictured in Figure 1 (where
it is assumed that there are some x in order that the square makes sense).
A
E
O
I
∀x p(x)
∃x ¬p(x)
∀x ¬p(x)
∃x p(x)
Subalterns
Subalterns
Contraries
Sub-contraries
Contradictories
Contradictories
Fig. 1. Square of opposition
Such a square is usually denoted by the letters A, I (aﬀrmative half) and E, O
(negative half). The names of the vertices come from a traditional Latin reading:
AﬃIrmo, nEgO). As can be seen, diﬃerent relations hold between the vertices:
- (a) A and O are the negation of each other, as well as E and I;
- (b) A entails I, and E entails O ;

Analogical Proportions and Square of Oppositions
327
- (c) A and E cannot be true together, but may be false together;
- (d) I and O cannot be false together, but may be true together.
Viewing the square in a Boolean way, where A, I, E, and O are now associated
with Boolean variables, i.e. A, I, E, and O are the truth values of statements of
the form ∀xS(x) →P(x), ∀xS(x) →¬P(x), ∃xS(x)∧P(x), and ∃xS(x)∧¬P(x)
respectively. Then, the following can be easily checked.
A
E
O
I
The link between A and E represents the symmetrical relation of contrariety,
whose truth table is given in Figure 2(a).
We recognize the mutual exclusion, i.e. ¬A ∨¬E holds.
A E A
E
0 0
1
0 1
1
1 0
1
1 1
0
(a) Contrariety
I O I
O
0 0
0
0 1
1
1 0
1
1 1
1
(b) Subcontrariety
A I A −→I
0 0
1
0 1
1
1 0
0
1 1
1
(c) Implication
A O A
O
0 0
0
0 1
1
1 0
1
1 1
0
(d) Contradiction
Fig. 2. The four relations involved in the square of oppositions
The link between I and O represents the symmetrical relation of subcontrari-
ety, whose truth table is is given in Figure 2(b).
We recognize the disjunction, i.e. I ∨O holds.
The vertical arrows represent implication relations A →I and E →O, whose
truth table is given in Figure 2(c).
The diagonal links represent the symmetrical relation of contradiction, whose
truth table is given in Figure 2(d).
We recognize the exclusive or, i.e. ¬(A ≡O) = (A ∧¬O) ∨(¬ A ∧O) =
(A ∨O) ∧(¬A ∨¬O) holds, or if we prefer we have A ≡¬O.
Thus, note that ¬A∨¬E and I∨O are consequences of A ≡¬O, A →I and
E →O in the square, but not E ≡¬I. Moreover I ∨O and E →O are not
enough for entailing E ≡¬I.
4
The Analogical Proportion and the Square of
Opposition
Let us ﬁrst continue to consider the Boolean square of oppositions where A, E,
I and O are binary variables. What are the joint assignments of values for these
variables that are compatible with the logical square of oppositions? It is easy
to prove that there are only 3 valid squares:

328
L. Miclet and H. Prade
0
0
1
1
A
I
E
O
0
1
1
0
1
0
0
1
One can immediately conclude that if A, E, I and O are the (Boolean-valued)
vertices of a square of oppositions, then they form an analogical proportion
when taken in this order, i.e. A : E :: I : O, since 0 : 0 :: 1 : 1, 0 : 1 :: 0 : 1 and
1 : 0 :: 1 : 0 are 3 of the 6 patterns that make an analogical proportion true.
This is fully consistent with an empirical reading of the traditional square
of oppositions. Indeed one can say that “∀xP(x) is to ∀x¬P(x) as ∃xP(x) is
to ∃x¬P(x)”. A less academic example of square of oppositions associated with
the analogical proportion “mice are to shrews as mammals except shrews are to
mammals except mice” is pictured below.
mice
shrews
mamals
except mice
mammals
except shrews
Remark 1. Consistency of analogical proportions with an empirical reading of
the square: Traditionally, the square is based on the empirical notions of quality
(aﬀrmative or negative) and quantity (universal or existential) which can be
used for describing the vertices of the square:
– A corresponds to a Universal Aﬀrmative statement,
– E corresponds to a Universal Negative statement,
– I corresponds to a Existential Aﬀrmative statement and
– O corresponds to a Existential Negative statement.
Thus in the square of oppositions each vertex can be described by means of a
2-component vector, the ﬁrst component being the quantity (value 0 if quantity
is existential, 1 if it is universal) the second component being the quality (value
1 if the quality is aﬀrmative, 0 if it is negative). The square thus obtained is the
superposition of the ﬁrst and the third previous squares, as can be seen.
0 1
0 0
1 0
1 1

Analogical Proportions and Square of Oppositions
329
4.1
The Analogical Proportion and Its Two Squares
As recalled in Section 2, the analogical proportion a
:
b
::
¬b
:
¬a
always holds. It clearly gives birth to 4 of the 6 patterns that makes an analogical
proportion true, namely 1 : 1 :: 0 : 0, 0 : 0 :: 1 : 1, 0 : 1 :: 0 : 1 and 1 : 0 :: 1 : 0.
However, the ﬁrst one (1 : 1 :: 0 : 0) is forbidden when an analogical proportion
is stated in terms of a square of oppositions, since in the following square a and b
form a square of oppositions together with their complements if and only if their
conjunction is false (indeed a and b cannot be true in the same time). Thus,
the square of oppositions appears to be a strict restriction of the analogical
proportion.
What about the 3 other patterns making true an analogical proportion that
are left aside, namely 1 : 1 :: 0 : 0, 1 : 1 :: 1 : 1 and 0 : 0 :: 0 : 0 ? Interestingly
enough, they can also be organized in another square that might be called square
of agreement. It is pictured below (the double arrow represents equivalence). In
this square, the following holds a ≡b, c ≡d, c →a, c →b, d →a, and d →b. It
corresponds to the pattern a : a :: b : b under the constraint b →a. Under these
constraints, the square has only 3 possible instantiations, namely 1 : 1 :: 0 : 0,
1 : 1 :: 1 : 1 and 0 : 0 :: 0 : 0.
a
b
¬a
¬b
a
b
d
c
Fig. 3. Square of opposition and square of agreement
Moreover it can be noticed that the three patterns involved in the square of
opposition (a : b :: c : d = 0 : 0 :: 1 : 1, 0 : 1 :: 0 : 1 or 1 : 0 :: 1 : 0) satisfy
constraints a →c and b →d, while the three patterns involved in the square of
agreement (a : b :: c : d = 1 : 1 :: 0 : 0, 1 : 1 :: 1 : 1 or 0 : 0 :: 0 : 0) satisfy the
constraints c →a and d →b.
Since analogical proportions are both a matter of dissimilarity and similarity
[11], it should come as no surprise that one “half” of it satisﬁes the square of
opposition, while the other “half” satisﬁes a square of agreement. Moreover, we
can see that that (a ̸≡d) ∧(b ̸≡c) is true only for the 4 patterns 1 : 1 :: 0 : 0,
0 : 0 :: 1 : 1, 0 : 1 :: 0 : 1 and 1 : 0 :: 1 : 0, and thus ((a ̸≡d)∧(b ̸≡c)∧(¬a∨¬b)) is
true only for the last 3 patterns, and thus corresponds to the square of opposition.
Similarly, ((a ≡b)∧(c ≡d) is true only for the 4 patterns 0 : 0 :: 1 : 1, 1 : 1 :: 0 : 0,
1 : 1 :: 1 : 1 and 0 : 0 :: 0 : 0, while ((a ≡b) ∧(c ≡d) ∧(c →b) ∧(d →a)) is true
only for the last 3 and corresponds to the square of agreement (∧(d →a) can

330
L. Miclet and H. Prade
be deleted and is put here only for emphasizing symmetry). This leads to three
noticeable, equivalent, disjunctive expressions of the analogical proportion:
a : b :: c : d = ((a ̸≡d) ∧(b ̸≡c)) ∨((a ≡b) ∧(c ≡d))
a : b :: c : d = ((a ≡b) ∧(c ≡d)) ∨((a ≡c) ∧(b ≡d))
a : b :: c : d = (((a ≡c ∧b ≡d)) ∨((a ̸≡d) ∧(b ̸≡c))
(since (a ≡c) ∧(b ≡d) is true only for the 4 patterns 1 : 0 :: 1 : 0, 0 : 1 :: 0 : 1,
1 : 1 :: 1 : 1 and 0 : 0 :: 0 : 0). A counterpart of the second expression can be
found in [13] in their factorization-based view of analogical proportion.
4.2
Analogical Proportion and the Hexagon of Oppositions
As proposed and advocated by Blanch´e [2], it is always possible to complete a
classical square of opposition into a hexagon by adding the vertices Y =def I∧O,
and U =def A ∨E. It fully exhibits the logical relations inside a structure of
oppositions generated by the three mutually exclusive situations A, E, and Y,
where two vertices linked by a diagonal are contradictories, A and E entail U,
while Y entails both I and O. Moreover I = A ∨Y and O = E ∨Y. Con-
versely, three mutually exclusive situations playing the roles of A, E, and Y
always give birth to a hexagon [3], which is made of three squares of opposition:
AEOI, YAUO, and EYIU, as in the ﬁgure below. The interest of this hexag-
onal construct has been rediscovered and advocated again by B´eziau [1] in the
recent years in particular for solving delicate questions in paraconsistent logic
modeling.
When the six vertices are Boolean variables, there is a unique instantiation
that satisﬁes all the expected relations in the hexagon:
U
A
E
O
I
Y
0
0
0
1
1
1
Thus the hexagon of oppositions is made of three squares of oppositions.
Each of these squares exactly corresponds to one of the three possible analogical
patterns. Indeed, the square AEOI corresponds to the analogical proportion
A : E :: I : O, i.e., 0 : 0 :: 1 : 1, the square YAUO to Y : A :: O : U, i.e.,
1 : 0 :: 1 : 0, and the square EYIU to E : Y :: U : I, i.e., 0 : 1 :: 0 : 1.
Since a structure of oppositions is generated by any three mutually exclusive
situations A, E, and Y, taking any pair of subsets R and S such that R ⊂S (in
order to insure that one cannot be in R and in ¬S in the same time), one gets the

Analogical Proportions and Square of Oppositions
331
hexagon below where the following analogical proportions hold R : ¬S :: S : ¬R,
¬R ∩S : R :: ¬R : R ∪¬S, and ¬S : ¬R ∩S :: R ∪¬S : S.
It is possible as well to build a more general hexagon from a pair of uncon-
strained subsets R and S, with proportions R : R ∩S :: R ∪S : R, R ∩S : R ::
R : R ∪S, and R ∩S : R ∩S :: R ∪S : R ∪S. An illustration in terms of Boolean
variables may be obtained by taking R as “speak English” and S as “speak
Spanish”, where the hexagon structures the logical relations between 6 possible
epistemic states regarding the competence of an agent wrt these languages.
R ∪S
R
S
R
S
R ∩S
(a)
R ∪S
R
R ∩S
R
R ∪S
R ∩S
(b)
Fig. 4. Two hexagons constructed from a square of oppositions
4.3
From an Hexagon to an Octagon
A way to display the construction of a general analogical proportion from un-
constrained subsets R and S is to start from the hexagon of Figure 4(a) and to
R
R ∩¯S
¯S
¯R ∩¯S
¯R
¯R ∩S
S
R ∩S
Analogical Proportion
Subalterns
Contraries
Contradictories
Fig. 5. The analogical octagon constructed from R and S

332
L. Miclet and H. Prade
add one node R ∩S between R and S, as well as one node ¯R ∩¯S between nodes
¯R and ¯S, and ﬁnally to turn the node R ∪¯S into the node R ∩¯S (see Figure 5).
The following square in this octagon is a complete analogical proportion:
R ∩S
:
R ∩¯S
::
¯R ∩S
:
¯R ∩¯S
Note that the nodes R ∩S and ¯R ∩¯S, from one side, and ¯R ∩S and R ∩¯S, from
the other side, are not contradictories. This new ﬁgure is not a full octagon of
oppositions, but could rather be called an analogical octagon, since it captures
the general construction of an analogical proportion from two sets of properties
R and S. Taking R (resp. ¯R) as “aerial” (resp. “aquatic”), and S (resp. ¯S) as
“move on ground” (resp. “move above ground”), it leads to state that “ants are
to birds as crabs are to ﬁshes” for instance.
5
Mixing Analogical Arguments and Deductive
Arguments
Stated in the setting of ﬁrst order logic, a basic pattern for analogical reasoning
(see, e.g., e.g. [12]) is then to consider 2 terms s and t, to observe that they share
a property P, and knowing that another property Q also holds for s, to infer
that it holds for t as well. This is known as the “analogical jump” and can be
described with the following inference pattern:
P(s) P(t) Q(s)
Q(t)
A typical instance of this kind of inference would be:
isBird(Coco) isBird(T weety) canFly(Coco)
canFly(T weety)
leading (possibly) to a wrong conclusion about Tweety. Making such an infer-
ence pattern valid would require the implicit hypothesis that P determines Q
inasmuch as ̸∃u P(u)∧¬Q(u). This may be ensured if there exists an underlying
functional dependency.
The above pattern, may be directly related to the idea of analogical propor-
tion. Taking advantage that “P(s) is to P(t) as Q(s) is to Q(t)” (indeed they are
similar changing s into t), the above pattern may be restated as
P(s) : P(t) :: Q(s) : Q(t)
P(s), P(t), Q(s)
Q(t)
which is a logically valid pattern of inference, from an analogical proportion
point of view (since the proposition P(s) : P(t) :: Q(s) : Q(t) holds) [9,11].
Analogical patterns, as well as deductive patterns, may be the basis of argu-
ments. Consider the following illustrative sequence of arguments:
“P’s are Q’s”
“s is a P”

Analogical Proportions and Square of Oppositions
333
then “s is a Q”
This is a deductive (syllogistic) argument in favor of Q(s). Assume the input of
the following information
“t is also a Q”
then “t should be a P”
by virtue of the analogical pattern, as t is a Q, s is a P and a Q. This an
analogical argument in favor of P(t). Now assume the following claim is made
“s is to t as u is to v”
based on the following facts: s, t are P’s and Q’s, while u, v are P’s and ¬Q’s
(then, for the 4-tuple (s, t, u, v), we have the following patterns 1 : 1 :: 1 : 1 for
P and 1 : 1 :: 0 : 0 for Q).
Then, an opposition takes place, and
“it is wrong that P’s are Q’s”.
Indeed we have a new argument that questions the ﬁrst premise (“P’s are
Q’s’). This little sketch intends to point out the fact analogy as deduction may
be the basis of arguments as well as counterarguments.
An analogical proportion-based statement may also support a deductive ar-
gument: “P’s are Q’s”, indeed “s is a P and a Q as t is a P and a Q”, then
knowing that “r is a P”, we conclude “r is a Q”, which is a form of syllogism
called epicherem (i.e. the basic syllogism is enriched by a supportive argument).
6
Concluding Remarks
This discussion paper has shown that squares of opposition may play a role
in the analysis of analogical proportions as much as they are encountered in
various other forms of reasoning. Squares of opposition can indeed be viewed as
a constrained form of analogical proportions (since analogical proportions both
encompass the ideas of disagreement and agreement). The paper has also pointed
out the relevance of other structures of oppositions (hexagon, octagon) in the
analysis of analogical proportions. This may also help classifying various forms
of arguments.
References
1. B´eziau, J.-Y.: New light on the square of oppositions and its nameless corner.
Logical Investigations 10, 218–233 (2003)
2. Blanch´e, R.: Structures Intellectuelles. Essai sur l’Organisation Syst´ematique des
Concepts. Vrin, Paris (1966)
3. Dubois, D., Prade, H.: From Blanch´e’s hexagonal organization of concepts to formal
concept analysis and possibility theory. Logica Univers. 6, 149–169 (2012)
4. Lepage, Y.: De l’analogie rendant compte de la commutation en linguistique. Ha-
bilit. `a Diriger des Recher. Univ. J. Fourier, Grenoble (2003)
5. Miclet, L., Bayoudh, S., Delhay, A.: Analogical dissimilarity: deﬁnition, algorithms
and two experiments in machine learning. JAIR 32, 793–824 (2008)
6. Miclet, L., Prade, H.: Handling analogical proportions in classical logic and fuzzy
logics settings. In: Sossai, C., Chemello, G. (eds.) ECSQARU 2009. LNCS (LNAI),
vol. 5590, pp. 638–650. Springer, Heidelberg (2009)

334
L. Miclet and H. Prade
7. Parsons, T.: The traditional square of opposition. In: Zalta, E.N. (ed.) The Stanford
Encyclopedia of Philosophy (Fall 2008 Edition) (2008)
8. Prade, H., Richard, G.: Analogy-making for solving IQ tests: A logical view. In:
Ram, A., Wiratunga, N. (eds.) ICCBR 2011. LNCS (LNAI), vol. 6880, pp. 241–257.
Springer, Heidelberg (2011)
9. Prade, H., Richard, G.: Cataloguing/analogizing: A non monotonic view. Int. J.
Intell. Syst. 26(12), 1176–1195 (2011)
10. Prade, H., Richard, G.: Homogeneous logical proportions: Their uniqueness and
their role in similarity-based prediction. In: Brewka, G., Eiter, T., McIlraith, S.A.
(eds.) Proc. 13th Int. Conf. on Principles of Knowledge Representation and Rea-
soning (KR 2012), Roma, June 10-14, pp. 402–412. AAAI Press (2012)
11. Prade, H., Richard, G.: From analogical proportion to logical proportions. Logica
Universalis 7(4), 441–505 (2013)
12. Russell, S.J.: The use of Knowledge in Analogy and Induction. Pitman, UK (1989)
13. Stroppa, N., Yvon, F.: Analogical learning and formal proportions: Deﬁnitions and
methodological issues. Technical report (June 2005)

Towards a Transparent Deliberation Protocol
Inspired from Supply Chain Collaborative Planning
Florence Bannay and Romain Guillaume
IRIT, Toulouse University, France
Abstract. In this paper we propose a new deliberation process based on argu-
mentation and bipolar decision making in a context of agreed common knowl-
edge and priorities together with private preferences. This work is inspired from
the supply chain management domain and more precisely by the “Collaborative
Planning, Forecasting and Replenishment” model which aims at selecting a pro-
curement plan in collaborative supply chains.
Keywords: Decision process, Argumentation, Supply Chain Management.
1
Introduction
In the supply chain management (SCM) domain, collaborative planning is the process
of ﬁnding a production plan which is suitable for every agent involved in the supply
chain (SC). In this domain, a useful requirement is that the agents have agreed about
some general conventions concerning the eligible criteria wrt production plans (avoid-
ing more discussions about criteria). From Artiﬁcial Intelligence point of view, the SCM
collaborative plan problem is a particular case of a collaborative decision given a back-
ground consensual knowledge and the particular preferences of each agent. This de-
cision is the selection of a candidate according to a global convention and according
to the precise arguments uttered by them about each candidate. In the classiﬁcation of
dialogs given by Walton and Krabbe [17], we are facing a deliberation process, since
the initial situation is a dilemma (the agents need to ﬁnd a good plan among several
options), each agent aims at coordinating its goals and actions with the others in order
to decide the best course of action that will be beneﬁcial for all the participants.
In existing process, supply chain participants are compelled to provide numerical
data (which may require some expensive computation, or may be based on some debat-
able numerical evaluations). Our main goal is to design a formal framework in which
(qualitative) arguments in favor or against a plan maybe expressed and may interact
with each-other. This framework will enable to conceive systems that could facilitate
and guide the agents to converge rationally towards a set of consensual plans. Moreover,
the production plan that will be chosen should have a guaranteed quality: if the selec-
tion leads to a set of plans that are under an admissibility threshold then the process is
aborted and an exception is raised. Otherwise, if several plans are admissible then the
customer (or the most important actor playing a role similar to a customer wrt a sup-
plier ([13]) has usually the last choice. Thus, our aim is twofold, enlarge the expressive
power and readability of the decision protocols concerning collaborative planning in
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 335–344, 2014.
c⃝Springer International Publishing Switzerland 2014

336
F. Bannay and R. Guillaume
SCM domains, import these ideas to AI ﬁeld in order to propose a new rational model
of collaborative decision making (or deliberation) under clear admissibility criteria.
More precisely we propose a new deliberation process based on bipolar argumenta-
tion where the arguments, their incompatibilities (called attacks) and their importance
levels are commonly agreed before the deliberation. We directly apply this process for
collaborative planning in supply chain using CPFR R⃝(see Section 2.3). In AI litera-
ture some models have already been proposed based on a bipolar view of alternatives.
Indeed, it is often the case that human people evaluate the possible alternatives consider-
ing positive and negative aspects separately [16]. Moreover, argumentation has already
been proposed to govern decision making in a negotiation context (see for instance [2]
and [14] for a survey). But, as far as we know, the use of bipolar argumentation in order
to govern a deliberation process under instantiated arguments had never been studied.
Moreover, in context of supply chain the agents only require to obtain admissible pro-
duction plan and do not necessarily need to class them (in the worst case the best plan
can be inadmissible, in other cases every plan may be admissible). Hence, we propose
and study the rationality and the calculability of the admissible set. Furthermore, the
notion of efﬁciency and simplicity that are central in SCM domain can bring a new
perspective for modeling deliberation process in AI.
We ﬁrst recall the basis of qualitative bipolar decision, classical argumentation and
supply chain planning. In a second step, we deﬁne a common decision making structure
gathering the concepts of bipolarity and argumentation, then we propose admissibility
thresholds. We apply this deliberation process to an example of collaborative planning.
2
Background
2.1
Qualitative Bipolar Decision
In this paper we focus on the problem of qualitative bipolar decision, this problem can
be formalized as follows. Let C be a ﬁnite set of potential choices and A be a set of
arguments (or criteria) viewed as decision attributes ranging on a bipolar scale {−,+}.
More precisely if a ∈A and c ∈C then a(c) is either an argument in favor of the choice
c (when a(c) = +) or against it (a(c) = −). Let us consider a totally ordered scale level
expressing the relative importance of arguments: ∀a1,a2 ∈A, if level(a1) > level(a2)
then a2 is more important than a1 (the best level is 1). In this paper, a set A ⊆A of
arguments is called a bipolar leveled set of arguments abbreviated bla, and A+ = {a |
a(c) = +} and A−= {a | a(c) = −} denote respectively the set of arguments in favor
and against the choice c. The problem of rank-ordering the possible choices has been
well studied in the literature. For this purpose, decision rules which build preference
relations between decisions were proposed [4].
Most approaches in the literature focus on preferences but do not discus the problem
of admissibility threshold. In this paper we propose such kind of thresholds.
Note that classical models do not take into account the conﬂicts that may occur
between arguments, indeed the presence of some arguments may decrease the validity
of other arguments. In this paper we propose a model that takes into account both the
importance of arguments and the relations between them.

Towards a Transparent Deliberation Protocol
337
2.2
Classical Argumentation
In the abstract argumentation theory, Dung [8] has deﬁned several ways, called “se-
mantics”, to select admissible arguments. Given a graph (X,R), called argumentation
system where X is a set of arguments and R is a binary relation on X called attack, the
selected sets of arguments S ∈X, called extensions, should be conﬂict free (i.e. should
not contain internal attacks). Most of the extensions proposed by Dung are based on a
defense notion1 that is not very intuitive in our context. Including defended arguments
would amount to accept every positive argument as soon as one positive argument is
not attacked (and similarly for negative arguments). Moreover, as already noticed by
Amgoud and Vesic [2] “Dung’s framework cannot be used for decision making since it
simply selects groups of arguments containing at last, one of the strongest (positive or
negative) arguments”. Hence, Dung’s framework is not useful in our context: it will give
the same results with a more complex computation, as the results obtained by simply
selecting the unattacked arguments.
2.3
Supply Chain Planning
A supply chain is “a network of connected and interdependent organizations mutually
and cooperatively working together to control, manage and improve the ﬂow of materi-
als and information from suppliers to end users” [5]. Handling a supply chain is called
supply chain management and can be deﬁned as “the management of upstream and
downstream relationships with suppliers and customers to deliver superior customer
value at less cost to the supply chain as a whole” [5]. Two types of supply chains can
be distinguished: decentralized supply chains (SC where actors are independents) and
centralized supply chains (SC where decision authority and information is hold by one
single party) [1]. In this paper we focus on decentralized supply chains. The key of
successful SCM for decentralized supply chains is the collaborative process.
The collaborative processes are usually characterized by a set of point-to-point cus-
tomer / supplier relationships with partial information sharing [7]: one or several pro-
curement plans are built and propagated through the supply chain using negotiation
processes. One of the most popular collaborative processes has been standardized un-
der the name of “Collaborative Planning, Forecasting and Replenishment” (CPFR R⃝)
[10]. It is also a registered trademark of Voluntary Inter-industry Commerce Standards
(VICS). The CPFR R⃝is a hierarchical process (strategic, tactical and operational) and is
decomposed into four main tasks: Strategy and Planning, Demand and Supply Manage-
ment, Execution and Analysis. In this paper, we are interested by the collaborative task
of Demand and Supply Management, and more precisely by Order planning/forecasting
and replenishment planning which are tasks that aim to determine future product order-
ing and some delivery requirements based upon the sales forecast, inventory positions,
transit lead times, and other factors. In the literature of production planning, the col-
laborative order planning and replenishment planning are based on linear programming
and/or mixed integer programming where the actors negotiate about a cost function
1 A set of arguments Y defends an argument x iff for any argument y s.t. (y,x) ∈R there exists
an argument z ∈Y such that (z,y) ∈R.

338
F. Bannay and R. Guillaume
which aggregates the inventory, the capacity, the production consequences ... (see [7]
and [1] for a survey). Since some coefﬁcients of the cost function are difﬁcult to esti-
mate for these actors, the analyses of resulting plans become difﬁcult.
Another method, is to propose a set of plans to the supplier. Then the supplier evalu-
ates the plans in terms of risk of back-ordering [9] and chooses the less risky plan. How-
ever, the risk of back-ordering is still poor information regarding the multi-dimension
nature of evaluation of a production planning process [12].
Another part of SCM literature focuses on the actor planning process and on how to
take into account the multi-objective nature of the problem and the qualitative evalu-
ation of a production plan. For these purposes, the fuzzy goal programming approach
[11] and the fuzzy multi-objective approach [3] have been proposed. The fuzzy goal
programming approach consists in deﬁning goals in terms of fuzzy intervals, the more
the result belongs to the goal the more pleased is the decision maker. The fuzzy multi-
objective approach takes into account the fuzziness of the importance of the criteria. For
the problem of production distribution planning, a collaborative fuzzy programming ap-
proach has been developed in [15] which proposes a hierarchy of priority levels for the
objective functions of the model, taking into account the dominance of each ﬁrm.
Those models do not take into account the bipolarity of decision. Our proposal aims
to generalize the approach of [9] by taking into account the multi-objectives and quali-
tative nature of the problem using a bipolar argumentative framework.
3
A Formal Framework for Bipolar Argumentative Deliberation
A collaborative deliberation process involves several agents that want to select a candi-
date (or an option) in a “rational” manner. In order to do this selection, the agents are
going to present arguments in favor or against some candidates. Then the system will
aggregate their arguments and return the winner(s). The idea, borrowed from collabo-
rative planning, is that the agents should ﬁrst agree on general criteria in terms of argu-
ments that may be used in favor or against a decision, and in terms of dominance/defeat
relations between those arguments. After this agreement stage which is done once and
for all, they can proceed to the selection of the candidates by giving arguments in favor
or against some of them according to their private knowledge and preferences.
3.1
Bipolar Leveled Features Set
Let C = {c1,...cn} be a set of candidates (choices) and V = {v1,...vn} a set of voters,
we are going to consider the arguments in favor or against the different candidates from
the point of view of each voter. Arguments are entities representing a particular feature
that may characterize some candidates, and inﬂuence their selection.
Example 1. In supply chain planning the problem is to ﬁnd a production plan which
satisﬁes both customers and suppliers. Thus, the set of candidates is the set of possible
production plans and the customers and the suppliers are the voters. In this example, we
consider three plans c1, c2 and c3 and two voters v1 (the supplier) and v2 (the customer).

Towards a Transparent Deliberation Protocol
339
Deﬁnition 1 (instantiated argument). An instantiated argument av(c) is a predicate
based on an argument a, given a voter v and a candidate c. Let I be the set of all
instantiated arguments2, namely, I = {av(c) | v ∈V,c ∈C,a ∈A}. Moreover for any
c ∈C, I(c) = {av(c) | a ∈A,v ∈V} denotes the set of instantiated arguments about c.
Given a set of arguments the universe is the structure that contains those arguments
together with their interactions.
Deﬁnition 2 ((instantiated) universe). A universe (respectively instantiated universe)
(AU,RU) is a graph whose vertices are arguments AU ⊆A (respectively instantiated
arguments AU ⊆I) and arcs are conﬂict relations between arguments such that RU ⊆
AU × AU.
If R is an attack relation then (a,b) ∈R means that the argument a defeats the argu-
ment b. In our context a is a possible feature, thus (a,b) ∈R means that if a characterizes
a candidate then the fact that b also holds for this candidate has no interest nor inﬂuence
on the ﬁnal result (b is invalidated by a).
Note that a universe is a Dung argumentation system [8]. Now, let us consider that
the protagonists have agreed about a consensual universe. This consensual agreement
concerns the features that will be used in arguments, it concerns also the way they
are divided into two sets (features “in favor of” and features “against” the candidates).
Finally the agreement between the participants concerns also the importance accorded
to each feature, it is translated by a level, the higher level corresponds to the features
that play the smaller role in the decision.
Note that the different features in A+ are arguments in favor of a candidate hence they
are, by nature, opposed to the arguments against this candidate i.e. features in A−(and
conversely). This is why a bla will be associated with a graph in which each argument
in A+ is attacking any less important argument in A−(i.e. belonging to a greater level).
For arguments situated at the same level λ, the relation is not so systematic. Let us
denote by Aλ the set of arguments at level λ, we consider that a relation Rλ should
be deﬁned in order to express the attacks between A+
λ and A−
λ . These attacks are not
necessarily symmetric. We can be confronted to three relationships between a positive
and a negative feature of the same level:
– they may have a completely independent impact on the accuracy to select a candidate
having these features: no attack (e.g. in the supply chain management domain, the
carbon impact and the stability of the production plan)
– they may have an opposite impact: hence when they appear together the attack is
symmetric (e.g., a plan that may be considered both satisfactory and not satisfactory
for personal reason)
– they may be related in a way that the presence of the ﬁrst one outperforms the other
(although they have the same level of importance) for instance a plan that may imply
a high inventory level for an actor of the supply chain is bad even if this plan makes a
quick ﬂow for an other actor.
This is why the relation Rλ should be given for each level λ.
2 In the literature, the instantiated arguments could be called practical arguments while generic
arguments are more related to epistemic arguments.

340
F. Bannay and R. Guillaume
Deﬁnition 3 (universe associated to a bla). Let A be a bla with l levels. The uni-
verse associated to A given a family of l binary relations on A, (Rλ)λ≤l such that
∀λ ∈1,l,Rλ ⊆(A+
λ × A−
λ )∪(A−
λ × A+
λ ), is the graph (A,R) with
R = {(x,y) ∈(A+ × A−)∪(A−× A+) and level(x) < level(y)} ∪
λ∈1,l Rλ
The instantiated universe associated to a bla A given R and given a set of instantiated
arguments I included in IA,V,C is a graph (I,RI) where RI is s.t. (a1v1(c1),a2v2(c2)) ∈RI
iff c1 = c2 and a1,a2 ∈A and (a1,a2) ∈R.
In other words, an instantiated universe is a graph of instantiated arguments. Each
argument is a feature relative to a given candidate according to the opinion of a voter.
Attack relations are induced between two arguments that concern the same candidate
when the features described in those arguments were consensually said incompatible.
Example 2. This example is inspired from the literature on multi-criteria production
planning. The supply chain characteristics and objectives give us concrete features that
are used in industrial domain. Here, each argument corresponds to a given industrial
objective, then, an argument is enabled for a given production plan if the plan achieves
this objective. The deﬁnition of arguments is done at the same time as the deﬁnition of
the bla. We consider that the customer and the supplier deliberate on the basis of the
following table A (which is a bla) containing the features that may characterize a plan3:
A+
A−
St: stable load
Rb: risk of back-ordering
Ruf : robust under failure
Hi: high inventory level
Rl: regrouping lot
Me: maintenance expensive
Qf : quick prod. ﬂow
Hci: high capacity not used
Pd: periodic delivery date
Sc: requires subcontract
Qd: constant quantity
Tw: necessity of temporary
delivered
workers
Sp: stable plan
Ot: necessity of overtime
Ic: important impact carbon
Pr: satisf. for pers. reason
NPr: non satisfactory
Lic: low impact carbon
for personal reason
St
Rb
Ru f
Hi
Rl
Me
Q f
Hci
Pd
Sc
Qd
Tw
Sp
Ot
Ic
Pr
NPr
Lic
R1
R2
R3
In this bla A, the set of arguments is A = {St, Rb, Ru f, Hi, Rl, Me, Qf, Hci, Pd, Sc,
Qd, Ot, Sp, Tw, Ic, Pr, NPr, Lic}. This bla has three levels of importance. The family
of attack relations (Rλ)λ≤3 between arguments of the same level is s.t. R1 = {(St,Hi),
(Rb,St), (Rb,Rl), (Rb,Qf), (Ru f,Hi), (Hi,Rl), (Hi,Qf), (Me,Ru f), (Qf,Hci), (Hci,
St)}, R2 = {(Pd,Ic), (Sc,Pd), (Sc,Qd), (Qd,Ic), (Tw,Sp), (Ot,Sp)} and R3 = {(Pr,
NPr), (NPr,Pr), (Lic,NPr)}. For sake of clarity, the other attack relations from any ar-
gument towards each opposite argument of a greater level are not shown on the picture.
The three leveled bla proposed in this example is consistent with the standard ideas
used in the supply chain management domain. This is why we consider at the most
important level that the supply chain has to satisfy the ﬁnal customer (hence avoid the
3 In real life, the establishment of this table is more complex and depends on the particular
objectives and priorities of the suppliers and the customers.

Towards a Transparent Deliberation Protocol
341
risk of back-ordering Rb). The supply chain should also have a quick ﬂow of produc-
tion (Qf) (hence the inventory level should not be high: Hi). A good production plan
should ensure a proper management of the storage (i.e a stable load St and no waste
in the storage capacities Hci). Moreover, the supply chain is supposed to have a low
risk of failure (Ru f) and to be cost effective (hence avoid expensive maintenance (Me)
and allow for regrouping lots Rl). At the second level, it is interesting (but not crucial)
to have stability on production and delivery (a stable plan Sp, a constant quantity de-
livered Qd and periodic delivery dates Pd) in order to satisfy the workers unless this
could increase the cost or induce some errors. Besides, here, the actors are considering
that subcontracting (Sc) increases the risk of back-ordering, hence they prefer not to
use overtime (Ot) because it is expensive. Temporary workers (Tw) decrease the pro-
ductivity because they need some learning time. Since the environmental impact has to
be integrated in the decision process (according to the idea of “Green SCM”), the actors
should consider carbon impact (Ic). The third level takes into account the good impact
on environment (Lic) and the actors personal preferences (a plan may be satisfactory -
or not- for personal reason Pr - NPr) witch are not mandatory.
Let us justify the attack relation R1 of the highest level of this bla, Rb attacks St, Rl
and Qf since the primary objective of the supply chain is to fulﬁll the demand. Rb does
not attack Ru f because this feature limits the risk to increase the level of back-ordering.
Hi attacks Rl because the actors want to enforce regrouping the lots unless the inventory
level becomes to high. The attack between Hi and Qf is different because for a voter
the two arguments may coexist (or may be exclusive). So this attack can be described
as follow “if the plan makes quick ﬂow for one of the voter then it is acceptable only
if this plan does not deal with a high inventory level for an other voter”. Hi is attacked
by St and Ru f because St or Ru f may justify Hi. Me attacks Ru f since a robust plan
is a beneﬁt only if this robustness is not too expensive to guarantee. Hci is attacked by
Qf since it is better to have a quick ﬂow of production than to under-exploit the storage
capacity. An example of set of instantiated arguments IB for c1, c2 and c3 is given below
together with a picture of the part of the instantiated bla obtained for candidate c3:
I+(c1)
I−(c1)
I+(c2)
I−(c2)
I+(c3)
I−(c3)
1
Qfv2
Rufv1, Qfv1, Hciv2
Stv1
2
Spv1
Pdv1, Qdv1
Icv2
Qdv2, Spv2 Twv1, Icv1
3 Prv1, Prv2,
Prv1, Licv1
NPrv2
Prv2
NPrv1
Licv2
Part of the bla relative to c3:
Qdv2
Twv1
Spv2
Icv1
Prv2
NPrv1
Proposition 1 (compactness of inputs). When the bla contains at least two levels, the
framework used to express arguments concerning a set of candidates from the point of
view of a set of voters with all the corresponding attacks is more compact4 in the bla
framework than in a classical argumentation system.
3.2
Decision Protocol
Given a bla A, the set of possible candidates C is given to all voters then each voter v:
- divides privately C into two sets C+
v and C−
v ,
4 In terms of number of symbols used to represent the same information in the worst case.

342
F. Bannay and R. Guillaume
- instantiates the features concerning each candidate5 c, collected in Iv(c),
- and ﬁnally chooses for each candidate the arguments that he wants to present.
For instance, in our example, v1’s private preferences are C+
v1 = {c1,c2}, C−
v1 = {c3}
and v2’s ones are C+
v2 = {c1,c3}, C−
v2 = {c2}. In order to select admissible candidates,
we can focus on the existence of non attacked arguments that are in favor and against
them. Given a candidate c, the set of non-attacked arguments concerning c, are denoted
S(c) (for “sources”), they are the vertices in I(c) that have no predecessors. We may
consider six main cases:
Deﬁnition 4 (admissibility status). Given a bla A with an associated instantiated uni-
verse (I,RI), and a candidate c, let S(c) = {x ∈I(c) | ∀y,(y,x) /∈R}, the status of c is
- necessary admissible (Nad) if ∅⊂S(c) ⊆I+(c)
- possibly admissible (Πad) if S(c)∩I+(c) ̸= ∅
- indifferent (Idad) if S(c) = ∅
- controversial (Ctad) if S(c)∩I+(c) ̸= ∅and S(c)∩I−(c) ̸= ∅
- possibly inadmissible (Π¬ad) if S(c)∩I−(c) ̸= ∅
- necessary inadmissible (N¬ad) if ∅⊂S(c) ⊆I−(c)
In other words, a necessary admissible candidate has an argument in its favor that is
unattacked and no unattacked argument against it; a possibly admissible candidate has
at least one unattacked argument in its favor. An indifferent candidate has no unattacked
arguments in its favor or against it, while a controversial candidate is both supported
and criticized by unattacked arguments. A candidate is possibly inadmissible if there is
an unattacked argument against it and necessary inadmissible if there is an unattacked
argument against it and no unattacked argument in favor of it.
Example 3. The arguments given by v1 and v2 about c1 belong only to I+, thus c1 ∈Nad,
they seem to agree to select candidate c1. c2 is also in Nad, c3 is only possibly admissible
(c3 ∈Πad) because Qdv2 and Twv1 are not attacked.
The above deﬁnition is related to possibility theory [18,6], where necessary (resp.
possibly) admissible could be understood as it is certain (resp. possible) that the candi-
date is admissible. The indifference case is linked to an impossibility to have unattacked
arguments in favor and against a candidate, thus an impossibility to decide. However
it is not related to a standard deﬁnition of possibilistic ignorance about the admissibil-
ity of a candidate, which rather corresponds to a controversial candidate that is both
possibly admissible and possibly inadmissible. With these deﬁnitions we get:
Proposition 2. Inclusion and Duality:
- Nad ⊆Πad and N¬ad ⊆Π¬ad
- Ctad = Πad ∩Π¬ad and Idad = (C \ Πad)∩(C \ Π¬ad)
- Nad = C \ (Π¬ad ∪Idad) and N¬ad = C \ (Πad ∪Idad).
The following property reveals that if a candidate is acceptable for all the voters then
it is necessary admissible, similarly, if a candidate is unacceptable for all the voters then
it is necessary inadmissible.
5 In this paper we suppose that a voter v uses the following basic strategy: if c ∈C+
v then he only
gives arguments in I +(c), if c ∈C−
v then he only gives arguments in I −(c).

Towards a Transparent Deliberation Protocol
343
Proposition 3 (unanimity). Considering a set of voters V using the basic strategy, for
any candidate c ∈C
– If ∀v ∈V,c ∈C+(v) and I+(c) ̸= ∅then c ∈Nad.
– If ∀v ∈V,c ∈C−(v) and I−(c) ̸= ∅then c ∈N¬ad.
With these admissibility status, we can propose 3 thresholds of admissibility (from the
strongest to the weakest): the ﬁrst threshold contains the candidates such that all the
unattacked arguments about them are positive, the second threshold can be divided into
two sets: 2a is the set of candidates under the ﬁrst threshold together with the candidates
for which no unattacked argument concerning them is available (neither positive nor
negative),the set 2b tolerates candidates that are concerned by negative unattacked argu-
ment provided that they are also concerned at least by one positive unattacked argument.
The third threshold is the union of the sets 1, 2a and 2b as shown in the ﬁgure below:
– threshold 1: c ∈Nad
– threshold 2a: c ∈Nad ∪Idad (or in C \ Π¬ad)
– threshold 2b: c ∈Πad (or in Nad ∪Ctad)
– threshold 3: c ∈Πad ∪Idad (or in C \ N¬ad)
Nad
Idad
Ctad
N¬ad
1
2a
2b
3
Note that qualitative explanations of the selection can be drawn on a graph, they are
simple and clear compared to quantitative explanations (that may result from complex
and obscure numeric computation and that are not necessarily based on a consensual
agreement).
How to compute the admissible sets? Given a candidate c, we ﬁrst consider the set
of unattacked arguments concerning c (i.e. S(c) = {x ∈I(c) | ∀y,(y,x) /∈R) and then
according to this set, we may assign one of the six status (necessary/possibly admissi-
ble/inadmissible, indifferent/controversial) to c by using Def. 4. Building the sets Nad,
Πad, Idad, N¬ad and Π¬ad can be done in polynomial time wrt |I|.
4
Conclusion
This paper introduces a new approach for SCM collaborative planning which is the basis
of a new framework for collaborative group decision making. The proposed process is
a kind of bipolar argumentative vote with more expressive power than standard vote
procedures but also a more compact representation of an argumentation system which
focus on admissibility of candidate and not on a ranking.
There are numerous beneﬁts of this new decision process. First, our framework fol-
lows the standard principles deﬁned by CPFR R⃝that are commonly used by industri-
alists. Moreover our proposal handles multi-criteria expression which is often not the
case for classical approaches or done roughly by using arbitrary aggregation functions.
Furthermore, it guarantees a succinct expression. And ﬁnally, the qualitative aspect of
this approach enables to justify the decision in an understandable and clear way to the
actors. However there are some drawbacks, for instance the need to deﬁne a protocol for
reaching a bla and a common deﬁnition of conﬂicts and priorities which maybe a difﬁ-
cult task (but it has only to be done once). This difﬁculty may be increased in domains

344
F. Bannay and R. Guillaume
with more open criteria than in the SCM ﬁeld where standard criteria are already well
established. In order to complete the deﬁnition of this bipolar deliberation protocol, a
study is still needed to establish that the admissible sets are rational regarding classical
decision rules such as Pareto, BiPoss and BiLexi [4].
References
1. Albrecht, M.: Supply Chain Coordination Mechanisms: New Approaches for Collaborative
Planning. Lecture Notes in Eco. and Math. Systems, vol. 628. Springer, Heidelberg (2010)
2. Amgoud, L., Vesic, S.: A formal analysis of the role of argumentation in negotiation dia-
logues. Journal of Logic and Computation 22, 957–978 (2012)
3. Baykasoglu, A., Gocken, T.: Multi-objective aggregate production planning with fuzzy pa-
rameters. Advances in Engineering Software 41(9), 1124–1131 (2010)
4. Bonnefon, J., Dubois, D., Fargier, H.: An overview of bipolar qualitative decision rules. In:
Della Riccia, G., Dubois, D., Kruse, R., Lenz, H.-J. (eds.) Preferences and Similarities. CISM
Courses and Lectures, vol. 504, pp. 47–73. Springer (2008)
5. Christopher, M.: Logistics And Supply Chain Management: Creating Value-Adding Net-
works. Pearson Education (2005)
6. Dubois, D., Prade, H.: Possibility theory: qualitative and quantitative aspects. In: Quanti-
ﬁed Representation of Uncertainty and Imprecision. Handbook of Defeasible Reasoning and
Uncertainty Management Systems, vol. 1, pp. 169–226. Kluwer Academic (1998)
7. Dudek, G.: Collaborative Planning in Supply Chains: A Negotiation-based Approach.
Springer, Heidelberg (2009)
8. Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic rea-
soning, logic programming and n-person games. Artiﬁcial Intelligence 77, 321–357 (1995)
9. Guillaume, R., Grabot, B., Thierry, C.: Management of the risk of backorders in a MTO-
ATO/MTS context under imperfect requirements. In: Applied Mathematical Modelling
(2013)
10. Ireland, R.K., Crum, C.: Supply Chain Collaboration: How To Implement CPFR And Other
Best Collaborative Practices. Integrated Business Management Series. J. Ross Publishing,
Incorporated (2005)
11. Jamalnia, A., Soukhakian, M.: A hybrid fuzzy goal programming approach with different
goal priorities to aggregate production planning. Comp. Ind. Eng. 56(4), 1474–1486 (2009)
12. Lu, T.-P., Trappey, A.J.C., Chen, Y.-K., Chang, Y.-D.: Collaborative design and analysis of
supply chain network management key processes model. Journal of Network and Computer
Applications (2013)
13. Marcotte, F., Grabot, B., Affonso, R.: Cooperation models for supply chain management.
International Journal of Logistics Systems and Management 5(1), 123–153 (2009)
14. Rahwan, I., Ramchurn, S.D., Jennings, N.R., McBurney, P., Parsons, S., Sonenberg, L.:
Argumentation-based negotiation. Knowledge Engineering Review 18(4), 343–375 (2003)
15. Selim, H., Araz, C., Ozkarahan, I.: Collaborative production–distribution planning in supply
chain: A fuzzy goal programming approach. Transportation Research Part E: Logistics and
Transportation Review 44(3), 396–419 (2008)
16. Slovic, P., Finucane, M., Peters, E., MacGregor, D.: Rational actors or rational fools? Impli-
cations of the affect heuristic for behavioral economics. The Journal of Socio-Economics 31,
329–342 (2002)
17. Walton, D.N., Krabbe, E.C.W.: Commitment in Dialogue: Basic Concepts of Interpersonal
Reasoning. State University of New York Press, Albany (1995)
18. Zadeh, L.A.: Fuzzy Sets as a Basis for a Theory of Possibility. Memorandum: Electronics
Research Laboratory. College of Eng., University of California (1977)

Encoding Argument Graphs in Logic
Philippe Besnard, Sylvie Doutre, and Andreas Herzig
IRIT-CNRS, University of Toulouse, France
{besnard,doutre,herzig}@irit.fr
Abstract. Argument graphs are a common way to model argumenta-
tive reasoning. For reasoning or computational purposes, such graphs
may have to be encoded in a given logic. This paper aims at providing
a systematic approach for this encoding. This approach relies upon a
general, principle-based characterization of argumentation semantics.
1
Introduction
In order to provide a method to reason about argument graphs [1], Besnard and
Doutre ﬁrst proposed encodings of such graphs and semantics in propositional
logic [2]. Further work by diﬃerent authors following the same idea was published
later, e.g. [3–7]. However, all these approaches were devoted to speciﬁc cases
in the sense that for each semantics, a dedicated encoding was proposed from
scratch. We aim here at a generalization, by deﬁning a systematic approach to
encoding argument graphs (which are digraphs) and their semantics in a logic
⊢. Said diﬃerently, our objective is to capture the extensions under a given
semantics of an argument graph in a given logic (be it propositional logic or
any other logic). We hence generalize the approach originally introduced in [2]
by parametrizing the encoding in various ways, including principles deﬁning a
given semantics.
We consider abstract arguments ﬁrst, and then provide guidelines to extend
the approach to structured arguments (made up of a support that infers a
conclusion).
2
Argument Graph and Semantics
2.1
Reminder
The notion of an argument graph has been introduced by Dung in [1]1.
Deﬁnition 1. An argument graph is a couple G = (A, R) such that A is a
ﬁnite set and R ⊆A × A is a binary relation over A.
The elements of the set of vertices A are viewed as a set of abstract arguments,
the origin and the structure of which are unspeciﬁed. The edges R represent
attacks: (a, b) ∈R, also written aRb, means that a attacks b. A set of arguments
S attacks an argument a if a is attacked by some element of S.
1 Dung uses the term argumentation framework instead of argument graph.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 345–354, 2014.
c
⃝Springer International Publishing Switzerland 2014

346
P. Besnard, S. Doutre, and A. Herzig
Dung introduced several semantics to deﬁne which sets of arguments can be
considered as collectively acceptable: the admissible, stable, grounded, preferred
and complete semantics. The application of a semantics to a given argument
graph results in a set of acceptable sets, called extensions. As an example of a
semantics, one may consider the stable semantics [1].
Deﬁnition 2. Given an argument graph G = (A, R), a stable extension S ⊆A
is a set that satisﬁes the following two conditions:
1. it does not exist two arguments a and b in S such that aRb;
2. for each argument b ̸∈S, there exists a ∈S such that aRb (any argument
outside the extension is attacked by the extension).
More generally, a semantics gives a formal deﬁnition of a method ruling the
argument evaluation process. Extensions under a given semantics ρ are called
ρ-extensions. Eσ(G) denotes the set of the ρ-extensions of an argument graph
G. Following Dung, a huge range of semantics have been deﬁned (see [8] for a
comprehensive overview). For these semantics, the following notions are essential
(where an argument graph G = (A, R) is assumed).
A set S ⊆A is conﬂict-free iﬃ̸ ∃a, b ∈S such that aRb.
An argument a ∈A is defended by S ⊆A iﬃ∀b such that bRa, ∃c ∈S such
that cRb.
A set of extensions E ⊆Eσ(G) is inclusive-maximal iﬃ∀E1, E2 ∈E, if E1 ⊆E2
then E1 = E2.
An admissible set is a conﬂict-free set that defends all its elements.
A stable extension is an admissible set, but not all admissible sets are stable
extensions.
The set of preferred extensions of an argument graph is the inclusive-maximal
set of its admissible sets.
A number of complexity results have been established for decision problems
in abstract argument graphs [9]. Two such problems are:
Veriﬁcation VERσ. Given a semantics ρ, an argument graph G = (A, R) and
a set S ⊆A, is S a ρ-extension of G?
Existence EXσ. Given a semantics ρ and an argument graph G = (A, R),
does G have at least one ρ-extension?
For instance, as regards the veriﬁcation problem [9]: VERstable is in P, but
VERpreferred is coNP-complete. As regards the existence problem, the question
of the existence of a stable extension, EXstable, is NP-complete.
2.2
Encoding
Given any semantics ρ, our objective is to capture the ρ-extensions of an argu-
ment graph (A, R) in a logic ⊢. The only requirements for this logic are that it
should contain all the Boolean connectives (in order to capture “not”, “and”,
and “or”).

Encoding Argument Graphs in Logic
347
There are two ways to achieve our objective:
(ϕ) By providing a formula Λσ whose models characterize the set Eσ(G) of ρ-
extensions of G = (A, R). So the set Mod(Λσ) of the models of Λσ is iso-
morphic to the set of ρ-extensions of (A, R): every model of Λσ determines
a ρ-extension of (A, R) and vice-versa. 2
(π) By providing a formula Λσ,S, depending on a subset S of A, that is satisﬁable
if and only if S is a ρ-extension of (A, R).
Adopting terminology from [2], we call (ϕ) “the model checking approach” and
(π) “the satisﬁability approach” (answering the veriﬁcation problem VERσ).
In (ϕ), we must provide a means to identify extensions in the encoding. (There
might for instance be non-eﬃective ways for a model to coincide with an exten-
sion.) In the rest of the paper, we will focus on the (π) approach.
An additional issue (ψ) may be to ﬁnd a formula (in the logic ⊢) that is sat-
isﬁable iﬃthere exists a ρ-extension for the argument graph (existence problem
EXσ). This issue is of interest for the stable semantics for instance, but not
for other admissibility-based semantics (preferred, complete, grounded. . .), the
empty set being always an admissible set.
3
Encoding Methodology
Now, we provide a methodology for encoding the ρ-extensions of an argument
graph in a given logic, following the satisﬁability approach previously introduced.
We are going to illustrate it by a case study in Section 5.
3.1
Encoding Extensions
At the abstract level, given a set of abstract arguments A = {a1, a2, . . .} and an
argument graph G = (A, R), in order to construct Λσ,S, the following questions
should be answered:
1. How to represent a subset S of the set of arguments A? For instance, it could
be:
νS =
*
ai∈S
ai ∧
*
aj̸∈S
¬aj
2. How to deﬁne that S is a ρ-extension of G? For instance, if ρ is the stable
semantics then we might have:
S is a stable extension of G iﬃνS |=
*
a∈A
(a ↔
*
b∈A:bRa
¬b)
In [2], it was shown that ,
a∈A(a ↔,
b∈A:bRa ¬b) = Λstable. More gener-
ally, we are aiming at constructing Λσ,S enjoying the following equivalence:
S is a ρ-extension of G iﬃνS |= Λσ, i.e.,
Λσ,S is satisﬁable iﬃνS |= Λσ
2 In the case that Mod(θσ) is isomorphic to Eσ(G) then the following consequence
holds: θσ ⊢ϕ iﬀϕ encodes a ⊢-deﬁnable property of G.

348
P. Besnard, S. Doutre, and A. Herzig
3. When a semantics involves a notion of maximality or minimality, how to
capture the corresponding sets?
3.2
Encoding Set-Theoretic Relations
Our methodology for systematic encodings Λσ,S where S is the subset to be tested
for being an extension relies on several building bricks and a rule as follows.
– Rule
An encoding is of the form
Λσ,S = ΣS ∧ΣS ∧φS
where ΣS encodes the necessary conditions for membership in S, ΣS encodes
the suﬀcient conditions, and φS is a Boolean combination over basic building
bricks (intuitively, φS expresses that S enjoys ρ).
– Basic Building Bricks3
• Membership in a subset of the arguments: a is an argument in X ⊆A is
encoded as
Σ(a∈X) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
Σ(a∈S)
if X = S
)
a=x∈X
⊤if X ̸= S
where for each a ∈A, we assume a generic formula4 Σ(a∈S) expressing
that “a is in the set of arguments S”.
• Subset X of the arguments:
ΣX =
*
a∈X
Σ(a∈X)
• Complement of X in the set of arguments:
ΣX =
*
a̸∈X
¬Σ(a∈X)
Intuitively, ΣS expresses that S contains all the elements of S whereas
ΣS expresses that S contains only elements of S.
3 Remember that an empty conjunction, i.e., a conjunction 
C(x) γ[x] whose condition
C(x) holds for no x, amounts to ⊤. An empty disjunction, i.e., a disjunction 
C(x) γ[x]
whose condition C(x) holds for no x, amounts to ⊥.
4 By generic formula, we mean a formula that is constructed in a systematic way,
by contrast to ad-hoc formulas with no common form. For example, ϕ(a∈S) can be
a (provided that, for all arguments a, there is an atom a in the language). This is
generic because all such formulas have the same form: an atom naming an argument.

Encoding Argument Graphs in Logic
349
– Intermediate Building Bricks
1. X ⊆Y
This is captured as
*
a∈A

Σ(a∈X) →Σ(a∈Y )

2. X is maximal such that φX holds
This, which amounts to φX & ∀Y ⊇X (φY →Y ⊆X), is captured as
φX ∧
*
X⊆Y ∈2A

φY →
*
a∈A
(Σ(a∈Y ) →Σ(a∈X))

3. X is minimal such that φX holds
This, which amounts to φX & ∀Y ⊆X (φY →X ⊆Y ), is captured as
φX ∧
*
X⊇Y ∈2A

φY →
*
a∈A
(Σ(a∈X) →Σ(a∈Y ))

Please bear in mind that, as a consequence of the ﬁrst building brick, the
following holds: In all of the above clauses, whenever X ̸= S (and similarly for
Y and Z), Σ(a∈X) must be encoded as 6
a=x∈X ⊤else it is encoded as Σ(a∈S).
As an illustration, here are the details for the case of maximality:
max: Let us assume that E satisﬁes φ ′ (hence φ ′
E holds). Then, for all S ⊂E,
Λσ,S
def
= ΣS ∧ΣS ∧φ ′
S ∧
*
S⊆Y ∈2A

φ ′
Y →
*
a∈A
(Σ(a∈Y ) →Σ(a∈S))

is not satisﬁable because the conjunct ΣS is contradicted by means of
φ ′
Y →ΣY (for the case Y = E); for a ∈E \ S, it happens that ΣS entails
¬Σ(a∈S) whereas φ ′
E →ΣE yields Σ(a∈S) (remember, φ ′
E holds).
4
Encoding Semantic Principles
Baroni and Giacomin have shown in [10] that the existing argumentation seman-
tics satisfy a number of principles. They have provided a comprehensive list of
such principles. From some subsets of these principles, it is possible to character-
ize existing semantics. Based on such a general characterization of a semantics,
the objective in this section is to encode into formulas the principles P1 . . . Pn
that deﬁne the semantics.
This requires two things. One is that we must prepare, from the building
bricks listed in Section 3.2, encodings of statements (and their denials) such as:
– “a is in E”
– “a attacks b” (in symbols, aRb) and set versions thereof

350
P. Besnard, S. Doutre, and A. Herzig
– “S is maximal such that. . . ”
– . . .
The other thing is to provide a concrete list of such principles P. In Section 2
we have already mentioned conﬂict-freeness, inclusion-maximality, and admissi-
bility. We recall here some of the list in [10]:
Conﬂict-Free Principle. A semantics ρ satisﬁes the C-F principle iﬃ∀G, ∀E ∈
Eσ(G), E is conﬂict-free.
From the building bricks in Section 3.2, conﬂict-freeness can be encoded as5
*
bRa
¬(Σ(a∈S) ∧Σ(b∈S))
Inclusion-Maximality Criterion. A semantics ρ satisﬁes the I-M criterion
iﬃ∀G, Eσ(G) is inclusive-maximal.
Here, an encoding has been already explicitly given in Section 3.2.
Encoding defence (for all b such that bRa, there exists c ∈S such that cRb) is
achieved by
*
bRa
)
cRb
Σ(c∈S)
which can be used in the encoding of the next three principles that are based on
defence, as follows.
Admissibility Criterion. A semantics ρ satisﬁes the admissibility criterion iﬃ
∀G, ∀E ∈Eσ(G), if a ∈E then a is defended by E.
The admissibility criterion can be captured through
Σ(a∈S) →(
*
bRa
)
cRb
Σ(c∈S))
Reinstatement Criterion. A semantics ρ satisﬁes the reinstatement criterion
iﬃ∀G, ∀E ∈Eσ(G), if a is defended by E then a ∈E.
The reinstatement criterion can be captured by means of
(
*
bRa
)
cRb
Σ(c∈S)) →Σ(a∈S)
Conﬂict-Free Reinstatement Criterion. A semantics ρ satisﬁes the CFR
criterion iﬃ∀G, ∀E ∈Eσ(G), if a is defended by E and E ∪{a} is conﬂict-
free then a ∈E.
Now, the CFR criterion can be captured just as the reinstatement criterion,
only adding (in the antecedent) a conjunct for conﬂict-freeness (see above
the conﬂict-free principle).
5 Please observe that S instead of E occurs in the speciﬁcation of the formula because
we construct a formula (parameterized by S) which is satisﬁable iﬀS is an extension.

Encoding Argument Graphs in Logic
351
In addition to the criteria listed in [10], we propose the following one:
Complement Attack Criterion. A semantics ρ satisﬁes this criterion iﬃ∀G,
∀E ∈Eσ(G) it holds that ∀b ∈A if b ̸∈E then ∃a ∈E such that aRb. The
complement attack criterion can be captured by means of
*
a∈A

¬Σ(a∈S) →

 )
bRa
Σ(b∈S)

Another encoding is to be found in the example detailed in Section 5.
The stable semantics is characterized by the conﬂict-free principle and the
complement attack criterion. The stable semantics satisﬁes the admissibility cri-
terion as well.
5
A Case Study
There are two approaches, (a) and (f). One approach (a) introduces dedicated
atoms in the object language to represent the attack relation. This means that
there is a list of fresh Attxy atoms, one for each ordered pair of nodes (x, y) in
the graph. The other approach (f) dispenses from such atoms, and the properties
to be encoded must be expressed in such a way that reference to attacks can be
captured as a range over conjunction or disjunction.
Example
Let us ﬁnd Λσ,S for ρ = stable. The deﬁnition for a set of arguments S being
conﬂict-free is
∀a ∈S ̸ ∃b ∈S
bRa
According to the (f)-approach, i.e., no dedicated atom is used, we must refor-
mulate the condition as follows: for all a in S, it is not the case that there exists
some b attacking a such that b is in S. The (f)-encoding for being conﬂict-free is
*
a∈S
¬
)
bRa
Σ(b∈S)
or, equivalently,
*
a∈S
*
bRa
¬Σ(b∈S)
In the case of stable extensions, we need to additionally encode the property of
S attacking its complement:
∀a ̸∈S∃b ∈S
bRa
According to the (f)-approach, we must reformulate the condition in the form:
for all a not in S, there exists some b attacking a such that b is in S. Hence the
(f)-encoding for S attacking its complement is
*
a̸∈S
)
bRa
Σ(b∈S)

352
P. Besnard, S. Doutre, and A. Herzig
Combining both conditions, we obtain the building brick φS (introduced above:
the general case) for stable extensions:
φS
=
(
*
a∈S
*
bRa
¬Σ(b∈S))
∧
(
*
a̸∈S
)
bRa
Σ(b∈S))
Conjoining with ΣS and ΣS, we obtain:
ΣS ∧ΣS ∧φS
=
*
a∈S
(Σ(a∈S) ∧
*
bRa
¬Σ(b∈S))
∧
*
a̸∈S
(¬Σ(a∈S) ∧
)
bRa
Σ(b∈S))
which is exactly the formula encoding stable extensions in Proposition 10 of [2]
where Σ(x∈S) is the atom x.
6
Towards Encoding Graphs of Structured Arguments
The aim of this section is to indicate how the approach, designed for abstract
argument graphs, may be extended to graphs with structured arguments. Our
exposition follows [11].
The very ﬁrst issue is to choose how to represent nodes of an argumentation
graph in this case. Then, arises the question of the representation of edges,
and next, of how to deﬁne the extensions. Moreover, the properties of the logic
underlying the arguments play a role now.
We assume that nodes in the graph (i.e., arguments) enjoy a minimal amount
of structure as pairs (Ai, ci) where Ai is a set of formulas, the premises of the
argument, and ci is a formula, the claim of the argument. Importantly,
Ai ⊩ci
In any case, our approach involves the following steps.
1. Representing nodes, with two options:
– Either every two arguments of the form (Ai, ci) and (Ai, c′
i) are treated
as two distinct entities,
– or they are treated as equals.
The former might be justiﬁed on the grounds that we have some (granted,
rudimentary) structure within an argument (it is less abstract) and there
must be the possibility to detail the content of the attack relation. We choose
to leave these two options open.
2. Representing edges, with the question:
– Is it necessary that the attack relation be captured at the object level,
i.e., by a formula of the logic?
Whether representing or not (i.e., simply capturing constraints and condi-
tions to be satisﬁed by the attack relation), we would need the language to
include something capturing consistency (and presumably, inference, hence
the need for the deduction theorem). E.g., there could be a modal possibility
operator ⋄and a naming device ⌈·⌉. All these would express consistency of
support of two arguments in an extension, as for example in
⋄(Ai ∧Aj) →¬⌈iRj⌉.

Encoding Argument Graphs in Logic
353
Another option would be to use QBF, as done in [12] to characterize the
complexity, or to use Dynamic Logic of Propositional Assignments, as done
in [13] to provide a dynamic account of the construction of extensions.
3. Representing extensions.
As a start, we must decide whether ¬Ai is to mean that (Ai, ci) fails to be
in the extensions being tested. The answer determines whether or not an
argument with the same support as an argument in the extension at hand
has to be in the extension.
Another point is that extensions are deﬁned as conditions using the attack
relation. This may mean building bricks, other than those we have examined,
from which various notions of extensions can be deﬁned.
4. Lastly, attention must be paid to properties of the logic that can play a role.
For example, contraposition and transitivity make the inference constraint
to turn the conﬂict between arguments i and j into Ai ⊩¬Aj as follows:
Ai ⊩¬cj
(assumption)
Aj ⊩cj
(assumption)
¬cj ⊩¬Aj (contraposition)
Ai ⊩¬Aj
(transitivity)
7
Conclusion
We have proposed in this paper a methodology to encode an argument graph
and a semantics in a given logic. Few constraints are imposed on the logic. We
have considered abstract arguments, and we have given guidelines to extend the
approach to structured, non-abstract arguments.
In this methodology, we have made the assumption that a semantics is deﬁned
by a set of principles. Even though evaluation principles have been put forward
by [8, 10], the characterization of a semantics by a set of principles is an issue
that has not been addressed yet. The example of the complement attack cri-
terion (mandatory to characterize the stable semantics), shows that in current
approaches, criteria are missing to obtain such a characterization.
In line with such a general characterization, it can be noticed that [14] con-
tains a general recursive schema that captures all of Dung’s semantics and that is
able to capture other admissibility-based, or non-admissibility-based, semantics.
However, the schema embeds a “base function”, that basically characterizes the
extensions of an argument graph made of only one strongly connected compo-
nent; that is, the semantics is not described by the principles it is based on.
As regards computational issues, [15] surveys implementations of abstract
argument graphs. Many abstract semantics have been implemented, with various
techniques, but none of these implementations is built from a principle-based
characterization of the semantics.
We have encoded in this paper a number of semantic principles, but others re-
main to be deﬁned (and encoded) in order to characterize the existing semantics,
and possibly, new semantics as well.
Moreover, the guidelines given to extend the approach to structured, non-
abstract arguments, will be further explored.

354
P. Besnard, S. Doutre, and A. Herzig
Acknowledgements. This work beneﬁted from the support of the AMANDE
project (ANR-13-BS02-0004) of the French National Research Agency (ANR).
References
1. Dung, P.M.: On the acceptability of arguments and its fundamental role in non-
monotonic reasoning, logic programming and n-person games. Artiﬁcial Intelli-
gence 77(2), 321–357 (1995)
2. Besnard, P., Doutre, S.: Checking the acceptability of a set of arguments. In:
Delgrande, J.P., Schaub, T. (eds.) Proc. NMR 2004, pp. 59–64 (2004)
3. Egly, U., Gaggl, S.A., Woltran, S.: Answer-set programming encodings for argu-
mentation frameworks. Argument and Computation 1(2), 147–177 (2010)
4. Amgoud, L., Devred, C.: Argumentation frameworks as constraint satisfaction
problems. In: Benferhat, S., Grant, J. (eds.) SUM 2011. LNCS, vol. 6929, pp.
110–122. Springer, Heidelberg (2011)
5. Walicki, M., Dyrkolbotn, S.: Finding kernels or solving SAT. Discrete Algo-
rithms 10, 146–164 (2012)
6. Dyrkolbotn, S.K.: The same, similar, or just completely diﬀerent? Equivalence for
argumentation in light of logic. In: Libkin, L., Kohlenbach, U., de Queiroz, R.
(eds.) WoLLIC 2013. LNCS, vol. 8071, pp. 96–110. Springer, Heidelberg (2013)
7. Nofal, S., Atkinson, K., Dunne, P.E.: Algorithms for decision problems in argument
systems under preferred semantics. Artiﬁcial Intelligence 207, 23–51 (2014)
8. Baroni, P., Giacomin, M.: Semantics of abstract argument systems. In: Simari,
G., Rahwan, I. (eds.) Argumentation in Artiﬁcial Intelligence, pp. 25–44. Springer
(2009)
9. Dunne, P.E., Wooldridge, M.: Complexity of abstract argumentation. In: Simari,
G., Rahwan, I. (eds.) Argumentation in Artiﬁcial Intelligence, pp. 85–104. Springer
(2009)
10. Baroni, P., Giacomin, M.: On principle-based evaluation of extension-based argu-
mentation semantics. Artiﬁcial Intelligence 171(10), 675–700 (2007)
11. Besnard, P., Garcia, A., Hunter, A., Modgil, S., Prakken, H., Simari, G., Toni,
F.: Special issue: Tutorials on structured argumentation. Argument and Compu-
tation 5(1) (2014)
12. Arieli, O., Caminada, M.W.: A QBF-based formalization of abstract argumentation
semantics. Journal of Applied Logic 11(2), 229–252 (2013)
13. Doutre, S., Herzig, A., Perrussel, L.: A dynamic logic framework for abstract ar-
gumentation. In: Baral, C., De Giacomo, G. (eds.) Proc. KR 2014. AAAI Press
(2014)
14. Baroni, P., Giacomin, M., Guida, G.: SCC-recursiveness: A general schema for
argumentation semantics. Artiﬁcial Intelligence 168(1), 162–210 (2005)
15. Charwat, G., Dvoˇr´ak, W., Gaggl, S.A., Wallner, J.P., Woltran, S.: Implementing
abstract argumentation — A survey. Technical Report DBAI-TR-2013-82, Tech-
nische Universit¨at Wien, Fakult¨at f¨ur Informatik, Vienna, Austria (2013)

On General Properties of Intermediate
Quantiﬁers
Vil´em Nov´ak and Petra Murinov´a
University of Ostrava, Institute for Research and Applications of Fuzzy Modeling
Centre of Excellence IT4Innovations
30 dubna 22, 701 03 Ostrava 1, Czech Republic
Vilem.Novak@osu.cz
Abstract. In this paper, we will ﬁrst discuss fuzzy generalized quan-
tiﬁers and their formalization in the higher-order fuzzy logic (the fuzzy
type theory). Then we will brieﬂy introduce a special model of inter-
mediate quantiﬁers, classify them as generalized fuzzy ones and prove
that they have the general properties of isomorphism invariance, exten-
sionality and conservativity. These properties are characteristic for the
quantiﬁers of natural language.
1
Introduction
Intermediate quantiﬁers form a class of linguistic quantiﬁers that characterize
size of a class of elements fulﬁlling a given property and whose meaning lays
between the classical general (∀) and existential (∃) ones. Typical examples of
intermediate quantiﬁers are “most, many, almost all, few”, and many other ones.
These quantiﬁers were studied from the point of view of their semantics and
general logical properties in the book [15]. A reasonable formalization of them
within a special formal theory of higher-order fuzzy logic (using the syntax of
fuzzy type theory (FTT)) was ﬁrst given in [11]. The intermediate quantiﬁers are
there deﬁned as classical ones ∀or ∃whose universe of quantiﬁcation is modiﬁed
by means of special evaluative linguistic expressions.
It is clear that intermediate quantiﬁers form a special subclass of generalized
quantiﬁers in the sense of their theory presented, e.g., in [5,14]. Therefore, it is
interesting to know whether our formalization fulﬁlls some of the basic prop-
erties, namely conservativity, extensionality, isomorphism invariance because as
stated, e.g., in [16], linguistic quantiﬁers are supposed to have just these proper-
ties. Recall that the fuzzy quantiﬁers were included in the theory of generalized
quantiﬁers in [3,8] and more precisely especially in [2,4].
In this paper, we will work in the ﬄLukasiewicz fuzzy type theory. On many
places, however, our results hold in an arbitrary one. Therefore, we will use
the short FTT to denote arbitrary fuzzy type theory and ﬄL-FTT to denote the
ﬄLukasiewicz one. Note that the ﬄLukasiewicz logic preserves most of the impor-
tant properties of the classical logic but still is highly non-trivial many-valued
one. Thus, the ﬄL-FTT proved to be an eﬃective tool for modeling of fuzzy quan-
tiﬁers. We proved that our model of the intermediate quantiﬁers, besides other
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 355–364, 2014.
c
⃝Springer International Publishing Switzerland 2014

356
V. Nov´ak and P. Murinov´a
properties, makes it possible to construct the generalized square of opposition
(cf. [7]) based on them.
2
Preliminaries
The formal theory of intermediate quantiﬁers is developed within special higher-
order fuzzy logic — the ﬄLukasiewicz fuzzy type theory ﬄL-FTT because this theory
is strong enough to ﬁt our requirements. The algebra of truth values is a linearly
ordered MVΔ-algebra
EΔ = ⟨E, ∨, ∧, ⊗, →, 0, 1, δ⟩
Because of the lack of space, we refer the reader to [9] for the details. Let us only
mention, that basic syntactical objects of ﬄL-FTT are classical (cf. [1]), namely
the concepts of type and formula. The atomic types are Φ (elements) and o (truth
values). General types are denoted by Greek letters ϕ, π, . . .. The set of all types
is denoted by Types. The language of ﬄL-FTT denoted by J, consists of variables
xα, . . ., special constants cα, . . . (ϕ ∈Types), the symbol α, and brackets.
Deﬁnition 1. Let J be a language of FTT and (Mα)α∈Types be a system of
sets called basic frame such that Mo, Mϵ are sets and for each ϕ, π ∈Types,
Mβα ⊆M Mα
β
, i.e. it is a set of weakly extensional functions1 from Mα to Mβ.
The frame is a tuple
M = ⟨(Mα, ⊜α)α∈Types , EΔ⟩
(1)
such that the following holds:
(i) The EΔ is a structure of truth values (a linearly ordered MVΔ-algebra).
We put Mo = E and assume that each set Moo ∪M(oo)o contains all the
operations from E. The fuzzy equality on truth values is the biresiduation
⊜o := ↔.
(ii) The set Mϵ is the set of individuals. The fuzzy equality ⊜ϵ on individuals
is a separated2 fuzzy equality on Mϵ.
(iii) If ϕ ̸= o, Φ then ⊜α is a fuzzy equality on Mα. We assume that ⊜α∈M(oα)α
for every ϕ ∈Types.
We often put Mϵ = R (set of real numbers).
Recall that interpretation of a formula Aα of type ϕ in M is determined with
respect to an assignment p ∈Asg(M) of elements from the sets Mα to variables
where Asg(M) is a set of all assignments. The assignment p assures that all free
variables occurring in the formula Aα are assigned speciﬁc elements so that the
whole formula Aα can be evaluated in M. The value of a formula Aα in a model
M is denoted by M(Aα). Clearly, M(Aα) ∈Mα for all ϕ ∈Types.
By a fuzzy set in Mα we understand a function A : Mα −→E, i.e., it is
obtained as interpretation of a formula Aoα in the model M. We will use the
1 By currying, we may conﬁne only to unary functions.
2 For all m, m′ ∈Mϵ, ⊜ϵ (m, m′) = 1 implies m = m′.

On General Properties of Intermediate Quantiﬁers
357
symbol A ⊂∼Mα to stress that A is a fuzzy set in Mα. If xα is a variable occurring
free in Aα and p(xα) = m ∈Mα then Mp(Aoαxα) ∈Mo is a membership degree
of m in the fuzzy set Mp(Aoα) (recall that Mo = E).
The subsethood between fuzzy sets is on the level of syntax deﬁned as a
formula
Aoα ⊆Boα := (∀uα)(Aoαuα ⇒
⇒
⇒Boαuα)
(2)
where Aoα, Boα are formulas whose interpretation are fuzzy sets (the symbol
“:= ” means “is deﬁned as”).
We say that a model M1 = ⟨

M 1
α, ⊜1
α

α∈Types , LΔ⟩is a submodel of M2 =
⟨

M 2
α, ⊜2
α

α∈Types , LΔ⟩, in symbols M1 ⊂M2, if M 1
α ⊆M 2
α, ⊜1
α is a restriction
of ⊜2
α to the set M 1
α for all ϕ ∈Types and
m1
oα(m1
α) = m2
oα(m1
α)
holds true for all m1
oα ∈M 1
oα, m2
oα ∈M 2
oα and m1
α ∈M 1
α, ϕ ∈Types. Note
that M 1 ⊆M 2 can be expressed using an identity function f : M 1 −→M 2. If
M1 ⊂M2, Aα is a quantiﬁer-free formula and p ∈Asg(M1) an assignment to
variables then
M1
p(Aα) = M2
p◦f(Aα)
(3)
where p ◦f is an assignment deﬁned by p ◦f(xα) = f(p(xα)).
We say that two models M1, M2 are isomorphic, in symbols M1 ∼= M2, if
there is a set of bijections
f = {f α : M 1
α −→M 2
α | ϕ ∈Types}
(4)
such that for each ϕ, π ∈Types the following diagram commutes:
M 1
α
f α
−−−−→M 2
α
m1
βα
⏐⏐8
⏐⏐8m2
βα=f βα(m1
βα)
M 1
β
f β
−−−−→M 2
β
where mi
βα ∈M i
βα, i = 1, 2 are functions and for each constant cα ∈J, ϕ ∈
Types, it holds that f α(M1(cα)) = M2(cα). The following theorem shows that
interpretation of a formula in two isomorphic models is the same.
Theorem 1. Let M1 ∼= M2 be an isomorphism and Aα a formula of type ϕ.
Then
f α(M1
p(Aα)) = M2
p◦f(Aα)
for any assignment p ∈Asg(M1) and the corresponding assignment p ◦f ∈
Asg(M2).
More details on model theory in FTT can be found in [12].
Note that the tautology
⊢(A ⇒
⇒
⇒B) ≡((A ∧∧∧B) ≡A).
(5)

358
V. Nov´ak and P. Murinov´a
is provable in (arbitrary) FTT.
The theory of intermediate quantiﬁers (denoted by T IQ) is an extension of a
special formal theory T Ev of ﬄL-FTT that is a theory of the meaning of evaluative
linguistic expressions. These are expressions of natural language such as small,
medium, big, about fourteen, very short, more or less deep, quite roughly strong,
etc. To develop the theory of their meaning, we must ﬁrst introduce the concept
of context3.
The concept of the context for evaluative expressions can be reduced to a
triple of numbers ⟨vL, vS, vR⟩where vL is the leftmost and vR the rightmost
value that has sense to consider. For example, for heights of people, vL =
40 cm and vR = 250 cm. The vS a typical medium value; in our example
vS = 170 cm.
The meaning of the evaluative expression
0
1
1
0.5
0.5
0.91
0.67
0.79
VeBi
0.97
0.75
0.86
ExBi
0.1
0.36
0.24
Sm
¬Sm
Fig. 1. Model of extensions of se-
lected evaluative expressions
is construed as a special formula representing
intension whose interpretation in a model is
a function from the set of contexts into a set
of fuzzy sets. Given a context, the intension
determines a corresponding extension that
is a fuzzy set in a universe determined by
the context. In this paper, we will deal with
the context ⟨0, 0.5, 1⟩only. Extensions of the
considered evaluative expressions are fuzzy sets of the shape depicted in Fig. 1.
All the details, justiﬁcation of the formal theory T Ev including its special axioms
and motivation can be found in [10].
In this paper, we do not need to introduce this theory in detail; it is suﬀcient
to consider special formulas Ev ∈Formoo that represent intensions of selected
evaluative linguistic expressions.
3
Generalized Fuzzy Quantiﬁers
3.1
Generalized Fuzzy Quantiﬁers in FTT
In the classical theory of generalized quantiﬁers, the type ⟨k1, . . . , kn⟩denotes a
quantiﬁer which acts on n formulas, each having ki free variables, i = 1, . . . , n
(cf. [5,14]). This corresponds to semantic interpretation of a generalized quan-
tiﬁer as a relation among subsets of certain sets M k1
1 , . . . , M kn
n . Generalization
of this concept to fuzzy generalized quantiﬁers was introduced in [8]. In fact, it
replaces sets and relations by the fuzzy ones. Both classical as well as general-
ized deﬁnitions relate to ﬁrst-order (fuzzy) logics. This means that all variables
occurring in formulas have the same type — namely, they represent objects in
a given universe.
In fuzzy type theory, however, this situation is more complicated because
formulas may contain variables of various types. At the same time, types are
included in the syntax of FTT and so, we can use them when characterizing a
generalized quantiﬁer.
3 A more general concept used in logic is possible world.

On General Properties of Intermediate Quantiﬁers
359
To simplify slightly the notation, let us use boldface Greek letters to denote
complex types, i.e. we write α instead of ϕ1 . . . ϕk where k depends on the
formula in consideration. Similarly, the symbol xα means a sequence of variables
xα1 · · · xαk. Then we can introduce the following deﬁnition.
Deﬁnition 2. Let M be a model of FTT. A generalized fuzzy quantiﬁer is a
special formula Q of type
o(oα1) . . . (oαn),
(6)
together with a symbol Q, deﬁned as follows:
Qo(oα1)...(oαn) := αuoα1 . . . αuoαn (Qxα1 · · · xαn)(uoα1xα1, . . . , uoαnxαn) (7)
where (Qxα1 · · · xαn)(uoα1xα1, . . . , uoαnxαn) is a formula of type o. Interpreta-
tion of the quantiﬁer (7) in the model M is
M(Q) ∈M
Moα1×···×Moαn
o
.
(8)
Hence, a type of quantiﬁer is just FTT-type of a special formula (7). We can
introduce a simpliﬁcation related to the classical theory by saying that Q is a
quantiﬁer of type ⟨α1, . . . , αn⟩understanding that it is a formula of type (6).
Then a quantiﬁer Q of type ⟨ϕ1, . . . , ϕn⟩is called monadic. This means that in
formula (7), single variables occur instead of sequences of them. If all formulas
in the quantiﬁer contain the same (known) variable xα, then we can also write
the type classically as ⟨1, . . . , 1⟩.
The interpretation M(Q) in (8) is obtained using a special function QM
assigning to each n-tuple of fuzzy relations from the sets Moα1, . . . , Moαn a
truth value. If the quantiﬁer is monadic then QM assigns a truth value to an n-
tuple of fuzzy sets. A global fuzzy generalized quantiﬁer is a functional assigning
to each model M a function QM.
If Aoα1, . . . , Aoαn
are formulas of the given types then the result of
α-conversion of Qo(oα1)...(oαn) will be written classically as
(Qxα1 · · · xαn)(Aoα1xα1, . . . , Aoαnxαn).
(9)
3.2
Intermediate Quantiﬁers
A special case of the generalized fuzzy quantiﬁers in the sense of Deﬁnition 2 are
intermediate quantiﬁers. They are deﬁned within a special theory denoted by
T IQ[S] where S is a set of distinguished types that must be considered to avoid
possible diﬀculties with interpretation of the formula μ representing a measure
(see below). The set S is supposed not to include too complex types ϕ that
would correspond to sets of very large, possibly non-measurable cardinalities.
The theory T IQ[S] is a special theory of ﬄL-FTT obtained as a certain extension
of the theory of evaluative linguistic expressions T Ev (for the precise deﬁnitions
— see [6,11]).
A special role in T IQ is played by a formula μ ∈Formo(oα)(oα) of the type
o(oϕ)(oϕ)4 representing measure of fuzzy sets that has the following properties:
4 Eeach type requires one speciﬁc formula μ.

360
V. Nov´ak and P. Murinov´a
(M1) T IQ ⊢δ
δ
δ(xoα ≡zoα) ≡((μzoα)xoα ≡⊤),
(M2) T IQ ⊢δ
δ
δ(xoα ⊆zoα)&&&δ
δ
δ(yoα ⊆zoα)&&&δ
δ
δ(xoα ⊆yoα) ⇒
⇒
⇒((μzoα)xoα ⇒
⇒
⇒
(μzoα)yoα),
(M3) T IQ ⊢δ
δ
δ(zoα ̸≡∅oα)&&&δ
δ
δ(xoα ⊆zoα) ⇒
⇒
⇒((μzoα)(zoα−xoα) ≡¬¬¬(μzoα)xoα),
(M4) T IQ ⊢δ
δ
δ(xoα ⊆yoα)&&&δ
δ
δ(xoα ⊆zoα)&&&δ
δ
δ(yoα ⊆zoα) ⇒
⇒
⇒((μzoα)xoα ⇒
⇒
⇒
(μyoα)xoα).
Interpretation of μ in a model M is a function Mp(μ) : Moα × Moα −→E.
In our case, we consider measures deﬁned for fuzzy subsets of a ﬁxed fuzzy set
B. Therefore, we will denote the measure Mp(μ) by μB. The properties (M1)–
(M4) are then the following: μB(B) = 1, μB(Z1) ≤μB(Z2) for all Z1 ⊆Z2 ⊆B,
μB(B −Z) = ¬μB(Z) and μB1(Z) ≥μB2(Z) for all Z ⊆B1 ⊆B2.
Deﬁnition 3. Let T IQ[S] be a theory of intermediate quantiﬁers and Ev ∈
Formoo be an intension of some evaluative expression. Furthermore, let z ∈
Formoα, x ∈Formα be variables and A, B ∈Formoα be formulas representing
measurable fuzzy sets. An intermediate quantiﬁer interpreting the sentence
“⟨Quantiﬁer⟩B are A”
is one of the following formulas:
(Q∀
Ev x)(Bx, Ax) := (∃z)((δ
δ
δ(z ⊆B)&&&(∀x)(zx ⇒
⇒
⇒Ax)) ∧∧∧Ev((μB)z)),
(10)
(Q∃
Ev x)(Bx, Ax) := (∃z)((δ
δ
δ(z ⊆B)&&&(∃x)(zx ∧∧∧Ax)) ∧∧∧Ev((μB)z)).
(11)
Both quantiﬁers are, with respect to the deﬁnition above, of type ⟨ϕ, ϕ⟩. In-
deed, the function Q obtained after interpretation formulas (10) and (11) is the
following:
Q∀
Ev,M(B, A)=
)  *
m∈M
(Z(m) →A(m)) ∧Ev(μB(Z))
99999 Z ⊂∼M, Z ⊆B
:
(12)
Q∀
Ev,M(B, A) =
)  )
m∈M
(Z(m) ∧A(m)) ∧Ev(μB(Z))
99999 Z ⊂∼M, Z ⊆B
:
(13)
where A, B ⊂∼M are given fuzzy sets, μB(Z) is a measure of the fuzzy set Z
w.r.t. the fuzzy set B and Ev is interpretation of the corresponding evaluative
expression in the context ⟨0, 0.5, 1⟩(cf. [6,10,11] and Fig. 1). Let us remark that
when referring to intermediate quantiﬁers we will in the sequel mean their model
in the sense of Deﬁnition 3.
The following is an example of two concrete intermediate quantiﬁers:
T: Most B are A := Q∀
Bi Ve(B, A) ≡
(∃z)((δ
δ
δ(z ⊆B)&&&(∀x)(zx ⇒
⇒
⇒Ax)) ∧∧∧(Bi Ve)((μB)z))
(14)
P: Almost all B are A := Q∀
Bi Ex(B, A) ≡
(∃z)((δ
δ
δ(z ⊆B)&&&(∀x)(zx ⇒
⇒
⇒Ax)) ∧∧∧(Bi Ex)((μB)z)), (15)

On General Properties of Intermediate Quantiﬁers
361
The formulas Bi Ve and Bi Ex represent the evaluative expressions “very big”
and “extremely big”, respectively. Interpretation of (14) and (15) in a model is
as follows: “all Bs have As” holds in a “very big” or an “extremely big” part (in
the sense of the measure μ) of the universe, respectively.
4
General Properties of Intermediate Quantiﬁers
In this section we will discuss three general properties of the fuzzy general quan-
tiﬁers, namely extensionality, isomorphism invariance, and conservativity and
show that the intermediate quantiﬁers have all of them.
4.1
Extensionality
Extensionality of the classical generalized quantiﬁers of type ⟨1, 1⟩is deﬁned as
follows: If M ⊆M ′ then
QM(B, A)
iﬃ
QM′(B, A)
(16)
for all A, B ⊆M.
Extensionality of the intermediate quantiﬁers in ﬄL-FTT must be deﬁned w.r.t.
models.
Theorem 2. Let M1 ⊂M2 be models of T IQ. Let Aoα, Boα be quantiﬁer-free
formulas. Then the following holds:
M1
p((Q∀
Ev x)(Bx, Ax)) = M2
p◦f((Q∀
Ev x)(Bx, Ax)),
(17)
M1
p((Q∃
Ev x)(Bx, Ax)) = M2
p◦f((Q∃
Ev x)(Bx, Ax))
(18)
which means that intermediate quantiﬁers (in the sense of Deﬁnition 3) are
extensional.
Proof. To simplify the notation, we denote A = M1
p(Aoα), B = M1
p(Boα),
Z = M1
p(zoα) and Ev(μB(Z)) = M1
p(Ev((μB)z)).
First, note that from A ⊂∼M 1
α ⊆M 2
α we have that A(m) = 0 for all
m ∈M 2
α −M 1
α and similarly for B and Z. Furthermore, M1
p(Ev((μB)z)) =
M2
p◦f(Ev((μB)z)) because Z ⊆B ⊂∼M 1
α ⊆M 2
α and so, the measure of the
fuzzy set Z is taken w.r.t. the fuzzy set B that is the same both in M 1
α as well
as in M 2
α. Hence,
M2
p◦f((∀x)(zx ⇒
⇒
⇒Ax)) =
*
m∈M1
α
(Z(m) →A(m))∧
*
m∈M2
α−M1
α
(Z(m) →A(m)) = M1
p((∀x)(zx ⇒
⇒
⇒Ax))
because Z(m) →A(m) = 1 for all M 2
α −M 1
α. From it follows using (12) that

362
V. Nov´ak and P. Murinov´a
M2
p◦f((Q∀
Ev x)(Bx, Ax)) =
)
⎧
⎨
⎩
*
m∈M2
α
(Z(m) →A(m))∧Ev(μB(Z))
999999
Z ⊂∼M 2
α and Z ⊆B ⊂∼M 1
α ⊆M 2
α
⎫
⎬
⎭=
= M1
p((Q∀
Ev x)(Bx, Ax)).
Analogously we proceed for (Q∃
Ev x)(Bx, Ax).
4.2
Invariance w.r.t. Isomorphism
Isomorphism of the classical generalized quantiﬁers of type ⟨1, 1⟩is deﬁned as
follows: If f : M −→M ′ is a bijection then
QM(B, A)
iﬃ
QM′(B, A)
(19)
for all A, B ⊆M. In FTT, however, the condition that the functions (4) are bijec-
tions is not suﬀcient. The theorem on isomorhpism invariance is thus stronger.
Theorem 3. Let M1 ∼= M2 be models of T IQ and A, B ∈Formoα be formulas.
Then the following holds:
M1
p((Q∀
Ev x)(Bx, Ax)) = M2
p◦f((Q∀
Ev x)(Bx, Ax)),
(20)
M1
p((Q∃
Ev x)(Bx, Ax)) = M2
p◦f((Q∃
Ev x)(Bx, Ax))
(21)
which means that intermediate quantiﬁers are isomorphism invariant.
Proof. This is an immediate consequence of Theorem 1.
4.3
Conservativity
Conservativity of the classical generalized quantiﬁers of type ⟨1, 1⟩is deﬁned as
follows:
QM(B, A)
iﬃ
QM(B, A ∩B)
(22)
for all A, B ⊆M. In case of fuzzy generalized quantiﬁers, we require equality of
truth values in (22) instead of the equivalence.
Theorem 4. The following is provable in T IQ:
(a) T IQ ⊢(Q∀
Ev x)(Bx, Ax) ≡(Q∀
Ev x)(Bx, Ax ∧∧∧Bx).
(b) T IQ ⊢(Q∃
Ev x)(Bx, Ax) ≡(Q∃
Ev x)(Bx, Ax ∧∧∧Bx).
This means that intermediate quantiﬁers are conservative.
Proof. The reﬂexivity
T IQ ⊢(Q∀
Ev x)(Bx, Ax) ≡(Q∀
Ev x)(Bx, Ax)
(23)

On General Properties of Intermediate Quantiﬁers
363
is provable. Hence, this formula is true in an arbitrary model M |= T IQ. Let us
check formula δ
δ
δ(z ⊆B) being a subformula of (10). Since the algebra of truth
values is linear, either M(δ
δ
δ(z ⊆B)) = 0 or M(δ
δ
δ(z ⊆B)) = 1. In the former
case, interpretation of both sides of (23) in the model M is equal to 0 so that
interpretation of (23) in M is equal to 1. Otherwise, by (2) and (5) we have
Mp(zx ≡(zx ∧∧∧Bx)) = 1
(24)
for arbitrary assignment p which gives Mp(zx) = Mp(zx ∧∧∧Bx). From this,
taking into account again deﬁnition (10) and tautology (5), we conclude that
Mp((∀x)(zx ⇒
⇒
⇒Ax)) = Mp((∀x)(z x ⇒
⇒
⇒Bx ∧∧∧Ax))
which implies T IQ ⊢(∀x)(zx ⇒
⇒
⇒Ax) ≡(∀x)(z x ⇒
⇒
⇒Bx ∧∧∧Ax). From this, using
rule (R) we derive (a) from (23).
Analogously we prove also (b).
5
Conclusion
In this paper, we focused on three basic general properties of the intermediate
quantiﬁers: conservativity, extensionality and isomorphism invariance. According
to [14,16], quantiﬁers of natural language should fulﬁll all these three proper-
ties. Using formal properties of ﬄL-FTT and model theory of higher-order fuzzy
logics we proved that our model of intermediate quantiﬁers fulﬁlls them which
supports the proclaim that (10) and (11) provide a reasonable mathematical
model of the meaning of the intermediate quantiﬁers used in natural language.
Other properties of them supporting further this claim can be found in the cited
literature (cf. [6,7,13]).
Acknowledgments. The research was supported by the European Regional
Development Fund in the IT4Innovations Centre of Excellence project (CZ.1.05/
1.1.00/02.0070).
References
1. Andrews, P.: An Introduction to Mathematical Logic and Type Theory: To Truth
Through Proof. Kluwer, Dordrecht (2002)
2. Dvoˇr´ak, A., Holˇcapek, M.: L-fuzzy quantiﬁers of the type ⟨1⟩determined by mea-
sures. Fuzzy Sets and Systems 160, 3425–3452 (2009)
3. Gl¨ockner, I.: Fuzzy Quantiﬁers: A Computational Theory. STUDFUZZ, vol. 193.
Springer, Berlin (2006)
4. Holˇcapek, M.: Monadic L-fuzzy quantiﬁers of the type ⟨1n, 1⟩. Fuzzy Sets and
Systems 159, 1811–1835 (2008)
5. Lindstr¨om, P.: First order predicate logic with generalized quantiﬁers. Theoria 32,
186–195 (1966)
6. Murinov´a, P., Nov´ak, V.: A formal theory of generalized intermediate syllogisms.
Fuzzy Sets and Systems 186, 47–80 (2012)

364
V. Nov´ak and P. Murinov´a
7. Murinov´a, P., Nov´ak, V.: Analysis of generalized square of opposition with inter-
mediate quantiﬁers. Fuzzy Sets and Systems 242, 89–113 (2014)
8. Nov´ak, V.: Antonyms and linguistic quantiﬁers in fuzzy logic. Fuzzy Sets and
Systems 124, 335–351 (2001)
9. Nov´ak, V.: On fuzzy type theory. Fuzzy Sets and Systems 149, 235–273 (2005)
10. Nov´ak, V.: A comprehensive theory of trichotomous evaluative linguistic expres-
sions. Fuzzy Sets and Systems 159(22), 2939–2969 (2008)
11. Nov´ak, V.: A formal theory of intermediate quantiﬁers. Fuzzy Sets and Sys-
tems 159(10), 1229–1246 (2008)
12. Nov´ak, V.: Elements of model theory in higher order fuzzy logic. Fuzzy Sets and
Systems 205, 101–115 (2012)
13. Nov´ak, V., Murinov´a, P.: Intermediate quantiﬁers, natural language and human
reasoning. In: Wang, G., Zhao, B., Li, Y. (eds.) Quantitative Logic and Soft Com-
puting, pp. 684–692. World Scientiﬁc, New Jersey (2012)
14. Peters, S., Westerst˚ahl, D.: Quantiﬁers in Language and Logic. Claredon Press,
Oxford (2006)
15. Peterson, P.: Intermediate Quantiﬁers. Logic, linguistics, and Aristotelian seman-
tics. Ashgate, Aldershot (2000)
16. Westerst˚ahl, D.: Quantiﬁers in formal and natural languages. In: Gabbay, D., Guen-
thner, F. (eds.) Handbook of Philosophical Logic, vol. IV, pp. 1–131. D. Reidel,
Dordrecht (1989)

A Note on Drastic Product Logic
Stefano Aguzzoli1, Matteo Bianchi1, and Diego Valota2
1 Department of Computer Science, Universit`a degli Studi di Milano, via Comelico
39/41, 20135, Milano, Italy
aguzzoli@di.unimi.it, matteo.bianchi@unimi.it
2 Dipartimento di Scienze Teoriche e Applicate, Universit`a degli Studi dell’Insubria,
via Mazzini 5, 21100, Varese, Italy
diego.valota@gmail.com
Abstract. The drastic product ∗D is known to be the smallest t-norm,
since x ∗D y = 0 whenever x, y < 1. This t-norm is not left-continuous,
and hence it does not admit a residuum. So, there are no drastic product
t-norm based many-valued logics, in the sense of [7]. However, if we re-
nounce standard completeness, we can study the logic whose semantics
is provided by those MTL chains whose monoidal operation is the dras-
tic product. This logic is called S3MTL in [17]. In this note we justify
the study of this logic, which we rechristen DP (for drastic product),
by means of some interesting properties relating DP and its algebraic
semantics to a weakened law of excluded middle, to the Δ projection
operator and to discriminator varieties. We shall show that the category
of ﬁnite DP-algebras is dually equivalent to a category whose objects are
multisets of ﬁnite chains. This duality allows us to classify all axiomatic
extensions of DP, and to compute the free ﬁnitely generated DP-algebras.
1
Introduction and Motivations
The drastic product t-norm ∗D : [0, 1]2 →[0, 1] is deﬁned as follows: x ∗D y = 0 if
x, y < 1, x∗D y = min{x, y} otherwise (see Fig. 1). It is clear from the deﬁnition
that ∗D is the smallest t-norm, in the sense that for any t-norm ∗and for each
x, y ∈[0, 1] it holds that x∗D y ≤x∗y. For this reason it is considered one of the
fundamental t-norms (see, e.g. [13]). This notwithstanding, there is no drastic
product t-norm-based logic, in the sense of [7], since ∗D is not left-continuous,
and hence it has no associated residuum.
In [18] Schweizer and Sklar introduce a class of t-norms which arise as mod-
iﬁcations of the drastic product t-norm in such a way to render them border
continuous. In this paper the authors explicitly state “The result is a t-norm
which coincides with [the drastic product] over most of the unit square”. In [11],
Jenei introduced left-continuous versions of the above mentioned t-norms, which
he called revised drastic product t-norms, as an example of an ordinal sum of tri-
angular subnorms, namely the ordinal sum of the subnorm which is constantly
0 with the t-norm min{x, y}. The logic RDP based on these t-norms has been
studied by Wang in [19], where, by way of motivation, the author recalls the ar-
gument of [18] about RDP t-norms as good approximators of the drastic product.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 365–374, 2014.
c
⃝Springer International Publishing Switzerland 2014

366
S. Aguzzoli, M. Bianchi, and D. Valota
As RDP is a prominent extension of the logic of weak nilpotent minimum WNM,
in [4] Bova and Valota introduce a categorical duality for ﬁnite RDP-algebras,
as a step towards a duality for the case of WNM-algebras.
As it has already been pointed out for [18] and [19], one justiﬁcation held
for the study of RDP is that revised drastic product t-norms make good ap-
proximations of ∗D, in the sense that the graph of such a t-norm can be chosen
to coincide with ∗D up to a subset of [0, 1]2 of euclidean measure as small as
desired.
A simple observation will show that RDP t-norms are as good approxima-
tors of ∗D as t-norms isomorphic (as ordered commutative semigroups) with
ﬄLukasiewicz t-norm. Consider, for instance, the parameterised family of t-norms
introduced by the same Schweizer and Sklar in [18], deﬁned as follows (here we
consider only positive real values for the parameter α): x ∗λ
SS y := max{0, xλ +
yλ −1}1/λ. These t-norms, being continuous and nilpotent, are all isomorphic
to ﬄLukasiewicz t-norm, which is obtained by choosing α = 1. It is easy to verify
that, for each c ∈(0, 1), the unique RDP t-norm ∗c having c as negation ﬁxpoint
(that is, ∼c = c), has its zeroset {(x, y) ∈[0, 1]2 | x ∗c y = 0} properly included
in the zeroset of ∗λ
SS for each α ≥log1/c 2. See Fig. 1 for an example.
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
Fig. 1. The DP t-norm, the RDP t-norm ∗2/3 and the Schweizer-Sklar t-norm ∗
log3/2 2
SS
Moreover, t-norms isomorphic with the ﬄLukasiewicz one are continuous func-
tions over [0, 1]2, while RDP t-norms are only left-continuous.
On the other hand, if we do not require a MTL logic to be standard com-
plete, that is, complete with respect to a set of standard algebras (algebras
([0, 1], ∗, ⇒, min, max, 0, 1), where ∗is a t-norm, and ⇒its associated residuum),
we can naturally study the logic of residuated drastic product chains, whose class,
clearly, does not contain any standard algebra. Needless to say, the drastic prod-
uct chains deﬁned on subsets of [0, 1] coincide with the drastic product t-norm
over their whole universes. Further, the logic of these chains is nicely axiomatised
by a slightly weakened version of the law of the excluded middle.
It turns out that the logic of all residuated drastic product chains is the logic
called S3MTL in [17, 10], where some of its properties are stated and proved.
In this note we shall justify the study of S3MTL, that we rename DP for
Drastic Product logic, in the light of several interesting logico-algebraic prop-
erties. Further, we introduce a category dually equivalent to ﬁnite DP-algebras

A Note on Drastic Product Logic
367
and utilise it to classify all schematic extensions of DP, and to characterise the
ﬁnitely generated free DP-algebras.
2
Preliminaries
We assume that the reader is acquainted with many-valued logics in H´ajek’s
sense, and with their algebraic semantics. We refer to [9, 6] for any unexplained
notion. We recall that MTL is the logic, on the language {&, ∧, →, ⊥}, of all left-
continuous t-norms and their residua, and that its associated algebraic semantics
in the sense of Blok-Pigozzi [3] is the variety MTL of MTL-algebras (A, ∗, ⇒
, ⊓, ⊔, 0, 1), that is, prelinear, commutative, bounded, integral, residuated lattices
[6]. Derived connectives are negation ¬Σ := Σ →⊥, top element ⊤:= ¬⊥, lattice
disjunction Σ ∨ϑ := ((Σ →ϑ) →ϑ) ∧((ϑ →Σ) →Σ). The connectives &,∧,∨
are modeled by the monoidal and lattices operations ∗,⊓,⊔, while →by the
residuum ⇒and ⊥, ⊤by the elements 0,1. On the algebraic side: ∼x := x ⇒0.
Every axiomatic extension L of MTL has its associate algebraic semantics: a
subvariety L of MTL such that a formula Σ is a theorem of L iﬃthe equation
Σ = ⊤holds in any algebra of L. BL is axiomatized as MTL plus Σ ∧ϑ =
Σ&(Σ →ϑ), MV as BL plus ¬¬Σ = Σ, and MV3 as MV plus Σ&Σ = (Σ&Σ)&Σ.
Drastic product chains are MTL-chains (A, ∗, ⇒, ⊓, ⊔, 0, 1) s.t., for all x, y ∈A,
x ∗y :=

0
if x, y < 1,
min{x, y}
otherwise.
(1)
We denote by DP the subvariety of MTL generated by all drastic product chains.
The members of DP are called drastic product algebras.
Noguera points out in [17, Page 108] that DP coincides with the variety named
S3MTL. Each logic in the hierarchy SkMTL (for 2 ≤k ∈Z) is axiomatised by
a generalised form of the excluded middle law: Σ ∨¬(Σk−1) (where Σ0 := ⊤and
Σn := Σn−1&Σ), whence the logic of drastic product DP is axiomatised by
Σ ∨¬(Σ2).
(DP)
Clearly, the logic S2MTL, axiomatised as MTL plus Σ ∨¬Σ is just classical
Boolean logic, as the latter axiom is the excluded middle law.
The basic example of a drastic product chain is, for any real c, with 0 < c < 1,
the algebra [0, 1]c := ([0, c]∪{1}, ∗, ⇒, ⊓, ⊔, 0, 1) where ∗is deﬁned as in (1), while
x ⇒y :=
⎧
⎪
⎨
⎪
⎩
1
if x ≤y,
c
if 1 > x > y,
y
if x = 1.
(2)
Equations (1) and (2) express the operations of any DP-chain. Indeed:
Lemma 1. A non-trivial MTL-chain A = (A, ∗, ⇒, ⊓, ⊔, 0, 1) is a DP-chain iﬀ
it has a coatom c, and x ∗x = 0 for all 1 > x ∈A. If A is a DP-chain then ∗
and ⇒are deﬁned as in (1) and (2). Moreover, if c > 0 then c =∼c is its only
negation ﬁxpoint.

368
S. Aguzzoli, M. Bianchi, and D. Valota
Proof. Assume A is a DP-chain: if A ∼= {0, 1} the claim trivially holds. Assume
then |A| > 2. Clearly, x ∗x = 0 for all 1 > x ∈A. Since ∗is non-decreasing,
x ∗y = 0 for all x, y < 1. Take y < x < 1 ∈A, and let c = x ⇒y. By the
properties of residuum, c < 1. Take now any z < 1 in A. Since x ∗z = 0 we have
z ≤c. Hence c is the coatom of A. It is now easy to check that (1) and (2) deﬁne
∗and ⇒. For the other direction notice that if A is an MTL-chain satisfying the
two assumptions, then x ∗x ∈{0, 1} for all x ∈A, and hence A satisﬁes (DP).
The lemma follows noting that if c > 0 then c∗1 = c and c∗c = 0, hence c =∼c
(an MTL-chain may have at most one negation ﬁxpoint).
Remark 1. Lemma 1 shows that DP does not contain any standard algebra, that
is a chain whose lattice reduct is ([0, 1], ≤R) (where ≤R denotes the restriction
of the standard order of real numbers). It must be stressed, however, that for
any 0 < c < 1, the operation ∗of the algebra [0, 1]c coincides with the drastic
product t-norm ∗D wherever deﬁned.
3
DP, RDP and WNM
We recall that WNM and RDP are the subvarieties of MTL respectively satisfying
the identities (wnm), and both (wnm) and (rdp), given below.
¬(Σ&ϑ) ∨((Σ ∧ϑ) →(Σ&ϑ)) = ⊤.
(wnm)
(Σ →¬Σ) ∨¬¬Σ = ⊤.
(rdp)
Proposition 1. DP ⊂RDP ⊂WNM.
Proof. Lemma 1 shows immediately that DP-chains satisfy both identities.
We recall that a variety V is locally ﬁnite whenever every ﬁnitely generated
subalgebra of an algebra in V is ﬁnite. Equivalently, free ﬁnitely generated alge-
bras are ﬁnite. In a locally ﬁnite variety the three classes of ﬁnitely generated,
ﬁnitely presented, and ﬁnite algebras coincide. Now, since WNM is locally ﬁnite
(see [17, Proposition 9.15]), from Proposition 1 we obtain:
Corollary 1. DP is locally ﬁnite and is generated by the class of all ﬁnite DP-
chains.
Proposition 2. DP ∩BL = NM ∩BL = MV3. RDP ∩BL = WNM ∩BL =
MV3⊕G, where NM (nilpotent minimum) is WNM plus ¬¬Σ = Σ, and MV3⊕G
is the variety generated by the ordinal sum of the 3-element MV-chain with the
standard G¨odel algebra.
Proof. The ﬁrst two equalities are shown in [17, 10]. For the other two equalities,
a direct inspection shows that a BL-chain satisﬁes (wnm) iﬃit is isomorphic to
an ordinal sum whose ﬁrst component is an MV-chain with no more than three
elements, and the others (if present) are isomorphic to {0, 1}. Finally, note that
(rdp) holds in a BL-chain iﬃit is isomorphic to an ordinal sum whose ﬁrst
component is an MV-chain with no more than three elements. So, if a BL-chain
models (wnm), then it satisﬁes also (rdp).

A Note on Drastic Product Logic
369
4
Canonical Completeness
We have seen that DP is axiomatised from MTL by a weakened form of excluded
middle law, and that algebras [0, 1]c are, defensibly, good approximators of the
drastic product t-norm, in the sense of [18, 19]. In this section we show that each
algebra [0, 1]c is a canonical model of the logic DP.
Recall that an extension L of MTL is standard complete if L is generated by
a set of standard algebras (see Remark 1).
Notice that DP is not standard complete since Lemma 1 shows that each
DP-chain must have a coatom. However, it must be noticed that the form of
completeness DP enjoys is precisely the same that is enjoyed by classical propo-
sitional logic, which technically is not a standard complete logic either. To stress
this fact we propose here the following notion of completeness, which strengthens
the notion of single-chain completeness (an extension L of MTL is single-chain
complete whenever its associated variety L is generated by a chain, [16]).
Deﬁnition 1. A schematic extension L of MTL is canonically complete if it is
complete with respect to a single algebra A, called canonical model of L, such
that:
– The lattice reduct of A is a sublattice of ⟨[0, 1], ≤R⟩.
– For every L-chain B whose lattice reduct is a sublattice of ⟨[0, 1], ≤R⟩, there
is A′ ∼= A such that ⟨B, ≤B⟩is a sublattice of ⟨A′, ≤A′⟩.
In other terms, A generates L and is (up to isomorphism of MTL-algebras)
lattice-inclusion-maximal among the algebras in L whose lattice reduct is a sub-
lattice of ⟨[0, 1], ≤R⟩. With this deﬁnition in place, we list some examples:
– Classical propositional logic is canonically complete even though not stan-
dard complete;
– G¨odel, product and ﬄLukasiewicz logic are both canonically and standard
complete, as it is BL (w.r.t. the ordinal sum of Ω copies of the standard
MV-algebra, for instance) [6, 9, 15].
– MTL is standard complete, but it is not known whether it is canonically
complete, nor single chain complete ([16]). The same applies to IMTL.
– The logic WCBL ([17, Ch. 7.2,7.3]) obtained extending BL with the weak
cancellativity axiom ¬(Σ&ϑ) ∨((Σ →(Σ&ϑ)) →ϑ) is standard complete,
being complete w.r.t. the set formed by the standard MV-algebra and the
standard product algebra, but it is not canonically complete. Indeed, an
MTL-chain belongs to this variety iﬃit is a product or an MV-chain: hence
WCBL is not single chain complete.
– Each logic BLn (with n ≥2), axiomatised as BL plus the n-contraction
axiom Σn →Σn+1 (see [2]), is neither standard nor canonically complete.
In the associated variety the only standard algebra is the standard G¨odel
algebra, but there are no generic chains at all.
Lemma 2. All the algebras of the form [0, 1]c are isomorphic.

370
S. Aguzzoli, M. Bianchi, and D. Valota
Proof. By Lemma 1.
Theorem 1. DP is generated by any inﬁnite DP-chain.
Proof. By Corollary 1, we have that if an equation fails in some DP-algebra,
then it fails in some ﬁnite DP-chain. Take now an inﬁnite DP-chain A. Denote
by c its coatom. By Lemma 1, we have that every ﬁnite DP-chain B embeds into
A: Trivially, {0, 1} λ→A. If |B| > 2, call b its coatom. Then every injective order
preserving mapping χ from B to A such that χ(0) = 0, χ(1) = 1, and χ(b) = c
(note that such a map always exists, since B is ﬁnite) is such that χ: B λ→A.
Hence every equation that fails in some ﬁnite DP-chain also fails in A.
Theorem 1 together with Corollary 1 proves the following result:
Theorem 2. DP is generated by any set of DP-chains of unbounded cardinality.
Theorem 3. The logic DP enjoys the strong completeness w.r.t. [0, 1]c, with
c ∈(0, 1). Moreover, DP is not standard complete but it is canonically complete.
Proof. By [5, Theorem 3.5] it is enough to show that every countable DP-chain
embeds into [0, 1]c. Let B be a countable DP-chain. Reasoning as in the proof
of Theorem 1 we ﬁnd the desired embedding χ: B λ→[0, 1]c (preserving 0, 1 and
the coatom). Such χ exists because [0, c] is an uncountable dense linear order.
The latter statement follows from Remark 1 and Lemma 2.
5
SnMTL and the Deﬁnability of the Δ Operator
DP coincides with S3MTL. In this section we shall recall some interesting prop-
erties of SnMTL-algebras from [17, 10, 14], and relate them to the deﬁnability
of the δ projection operator [1].
We recall that a variety is semisimple if all its subdirectly irreducible alge-
bras are simple, and it is a discriminator variety if the ternary discriminator t
(t(x, x, z) = z, while t(x, y, z) = x if x ̸= y), is deﬁnable on every subdirectly
irreducible algebra.
Theorem 4 ([17, 10, 14]). Let L be a variety of MTL-algebras. Then the
following are equivalent:
– L is semisimple.
– L is a discriminator variety.
– L is a subvariety of SnMTL for some n ≥2.
– Every chain in L is simple and n-contractive (i.e. it satisﬁes xn = xn−1),
for some n ≥2.
Given a schematic extension L of MTL we write LΔ for the extension/expansion
of L with the δ unary projection connective, axiomatised as follows:
δΣ ∨¬δΣ,
δΣ →Σ,
δΣ →δδΣ,
δ(Σ →ϑ) →(δΣ →δϑ),
δ(Σ ∨ϑ) →(δΣ ∨δϑ).
Recall that on every MTL-chain A and every x ∈A, the identities associated
with these axioms model the operation δx = 1 if x = 1, while δx = 0 if x < 1.

A Note on Drastic Product Logic
371
Proposition 3. Let L be a variety of MTL-algebras. Then LΔ is semisimple.
Proof. We prove that each chain in LΔ is simple.
Take a chain A ∈LΔ. For every non-trivial congruence Λ on A, it holds that
⟨a, b⟩∈Λ, for some a ̸= b. Then exactly one of a ⇒b and b ⇒a is 1, say a ⇒b.
Hence ⟨b ⇒a, 1⟩∈Λ, and ⟨δ(b ⇒a), δ(1)⟩∈Λ. That is, ⟨0, 1⟩∈Λ. Since Λ is a
congruence of the lattice reduct of A, all elements between 0 and 1 are in the
same class, which means that A is simple.
Theorem 5. Let L be a variety of MTL-algebras. Then LΔ = L iﬀL is a
subvariety of SkMTL for some integer k > 1.
Proof. It is immediate to check that the δ operator is deﬁnable in each variety
SkMTL as δx = xk−1.
For the other direction, assume δ is deﬁnable in a variety L of MTL-algebras.
Then L is semisimple by Proposition 3. By Theorem 4, L is a subvariety of
SkMTL for some integer k > 1.
Remark 2. Theorem 5 shows that if δ is deﬁnable in some subvariety L of MTL-
algebras, then it is always deﬁnable as δx = xk for some k ≥1. Further, L is a
discriminator variety iﬃit deﬁnes δ and, in this case, (δ(x ↔y)∧z)∨(¬δ(x ↔
y) ∧x) is the discriminator term.
Notice that the assumptions of Theorem 4 cannot be generalised to exten-
sions/expansions of MTL. For instance, consider the logic G∽introduced in [8],
which is G¨odel logic extended/expanded with an independent involutive nega-
tion ∽. It is an exercise to check that δ is deﬁnable in G∽as δx = ¬ ∽x, but
not as xk for any integer k. Hence G∽is a discriminator variety, but G∽is not
an extension/expansion of any SkMTL.
Corollary 2. A variety L of BL-algebras coincides with LΔ iﬀit is a variety of
MV-algebras generated by a ﬁnite set of ﬁnite MV-chains.
Proof. By [17, Corollary 8.16], SnMTL ∩BL is generated by the set of all MV-
chains with at most n elements, for every integer n > 1. The result follows from
Theorem 5.
The following result is needed in the next section.
Lemma 3. The classes of simple WNM-chains and of simple RDP-chains both
coincide with the class of DP-chains.
Proof. The result follows from Proposition 1 and from Theorem 4, since every
WNM-chain satisﬁes x3 = x2.
6
A Dual Equivalence
In [4] a dual equivalence between the category of ﬁnite RDP-algebras and ho-
momorphisms and the category HF of ﬁnite hall forests is proven. We recall

372
S. Aguzzoli, M. Bianchi, and D. Valota
here that a ﬁnite hall forest is a ﬁnite multiset whose elements are pairs (T, J),
where T is a ﬁnite tree (that is, a poset with minimum such that the downset
of each element is a chain) and J is a (possibily empty) ﬁnite chain, while a
morphism h: {(Ti, Ji)i∈I} →{(Tk, Jk)k∈K} is a family of pairs {(fi, gi)}i∈I such
that for each i ∈I there is k ∈K such that fi : Ti →Tk, and gi : Ji →Jk are
order-preserving downset preserving maps, with the additional constraint that
gi(max Ji) = max Jk. In case Jk is empty it is stipulated that gi is the partial,
nowhere deﬁned, map.
For each integer k > 0 let k denote the k-element chain.
Deﬁnition 2. Let MC be the category whose objects are ﬁnite multisets of
(nonempty) ﬁnite chains, and whose morphisms h: C →D, are deﬁned as fol-
lows. Display C as {C1, . . . , Cm} and D as {D1, . . . , Dn}. Then h = {hi}m
i=1,
where each hi is an order preserving surjection hi : Ci ↠Dj for some j =
1, 2, . . ., n. Let MC⊤be the nonfull subcategory of MC whose morphisms h: C →
D satisfy the following additional constraint: for each i = 1, 2, . . ., m, if the target
Dj of hi is not isomorphic with 1, then h−1
i (max Dj) = {max Ci}.
Theorem 6. The category MC⊤is equivalent to the full subcategory of HF whose
objects have the form {(1, Ji)}i∈I. Whence, MC⊤is dually equivalent to the
category DPfin of ﬁnite DP-algebras and their homomorphisms.
Proof. Note that the only map from 1 to itself is the identity id1. Then di-
rect inspection shows that the functor T r: MC⊤→HF, deﬁned on objects as
T r({C1, . . . , Cm}) = {(1, C1 \{max C1}), . . . , (1, Cm \{maxCm})}, and on mor-
phisms as T r({hi}m
i=1) = {(id1, hi ↾Ci \ {max Ci})}m
i=1 (we agree that hi ↾∅is
the nowhere deﬁned map), implements the equivalence stated in the ﬁrst state-
ment. Observe that the dual of a simple RDP-algebra in the category HF is a
hall forest of the form {(1, J)}. The last statement then follows from Lemma 3.
Clearly, the multiset {1} is the terminal object of MC⊤. Applying the ﬁrst
equivalence of Theorem 6 one can verify the following constructions, as they are
carried over to MC⊤from HF. Given two objects C, D ∈MC⊤, the coproduct
object C ⊎D of C and D is just the disjoint union of the multisets C and D;
the product object C × D is computed using the following MC⊤isomorphisms.
First, products distribute over coproducts: C × (D ⊎E) ∼= (C × D) ⊎(C × E).
Given C ∈MC⊤, let C⊤denote the object obtained adding to each chain in C
a fresh maximum. Then {i} × {1} ∼= {i} and {i + 1} × {2} ∼= {i + 1}. Moreover,
{i + 2}×{j + 2} ∼= (({i + 2}×{j + 1})⊎({i + 1}×{j + 1})⊎({i + 1}×{j + 2}))⊤.
Denote by MC : DPfin →MC⊤the functor implementing the dual equiva-
lence. The following lemma is straightforward.
Lemma 4. For any integer i > 0, MC−1 {i} is the DP-chain with i+1 elements.
Let Fk denote the free DP-algebra over a set of k many free generators. In
the following we write nC for the nth copower of C ∈MC⊤.

A Note on Drastic Product Logic
373
Theorem 7. MC F1 is the multiset of chains {1, 3, 2, 1}. Hence, F1 has exactly
22 · 3 · 4 = 48 elements. More generally,
MC Fk ∼= 2k{1}⊎(3k −2k){2}⊎
k+2
>
h=3

h−2

i=0
(−1)i
h −2
i
	
(h + 1 −i)k

{h} . (3)
Whence, the cardinality of Fk is given by
22k · 33k−2k ·
k+2
?
h=3
(h + 1)
h−2
i=0 (−1)i(
h−2
i )(h+1−i)k .
(4)
Proof. For what regards F1 the proof follows at once by Theorem 6, Lemma 4
and [4, Proposition 6]. For the general case, recall that Fk is the kth copower
of F1, hence MC Fk is given by the kth power of {1, 3, 2, 1}. Proceeding by
induction on k, we denote by a(k)
i
the coeﬀcient multiplier of {i} in Eq. (3).
Using the fact that {k} × {3} ∼= (k −1){k + 1} ⊎(k −2){k}, the computation of
MC Fk × {1, 3, 2, 1} gives the recurrences a(k+1)
1
= 2a(k)
1 , a(k+1)
2
= a(k)
1
+ 3a(k)
2 ,
a(k+1)
3
= a(k)
1
+ a(k)
2
+ 4a(k)
3 , and a(k+1)
h
= (h −2)a(k)
h−1 + (h + 1)a(k)
h
for h > 3,
whose solutions ﬁnally yield Eq. (3). By Lemma 4, Eq. (4) yields the cardinality
of Fk.
Notice that by replacing in Eq. (4) each base number with the DP-chain with the
same cardinality one gets the decomposition of Fk as direct product of chains.
By Theorem 2, any proper subvariety of DP is generated by a necessarily ﬁnite
family of ﬁnite chains. We can then classify all subvarieties of DP.
Theorem 8. Each proper subvariety of DP is generated by a ﬁnite chain. More-
over, two ﬁnite chains of diﬀerent cardinality generate distinct subvarieties.
Proof. First note that each non-trivial ﬁnite DP-chain embeds into any DP-
chain of greater cardinality. Whence, each proper subvariety V of DP is gener-
ated by a ﬁnite chain, which is the chain with maximum cardinality among any
given set of chains generating V. For each object C ∈MC⊤let its height H(C)
be the maximum cardinality of its chains. It is easy to see that H(C ⊎D) =
max{H(C), H(D)} and that given maps C1 λ→D1 and C2 ↠D2, it holds that
H(C1) ≤H(D1) and H(D2) ≤H(C2). It follows by Thm. 6, Lemma 4 and the
HSP theorem that all ﬁnite algebras in the variety generated by the k-element
DP-chain must have dual of height ≤k −1. Let Vh be the subvariety generated
by the h-element chain. Then, if h < k, (Vh)fin is properly included in (Vk)fin.
By [12, Cor. VI.2.2] and Cor. 1, Vh is properly included in Vk.
One may wonder whether there are classes of residuated lattices dually equiv-
alent to MC. The answer is in the positive, for it is easy to prove that the class
of ﬁnite algebras in GΔ, the variety of G¨odel algebras plus δ, is the category
sought for. It turns out that DP is equivalent to a non-full subcategory of GΔ,
with the same objects, but with fewer morphisms.

374
S. Aguzzoli, M. Bianchi, and D. Valota
Even if we do not have analyzed the ﬁrst-order case, there is a result that may
have some interest. The logic DP∀enjoys strong completeness w.r.t. [0, 1]c, with
c ∈(0, 1). This can be proved via a modiﬁcation of a construction described in
[5], that allows to embed every countable DP-chain into a chain isomorphic to
[0, 1]c (for some c ∈(0, 1)), by preserving all inf’s and sup’s. By [5, Theorems
5.9, 5.10] this suﬀces to prove DP∀is strongly complete w.r.t. [0, 1]c.
References
[1] Baaz, M.: Inﬁnite-valued G¨odel logics with 0-1-projections and relativizations. In:
G¨odel 1996: Logical Foundations of Mathematics, Computer Science and Physics
– Kurt G¨odel’s Legacy, pp. 23–33. Springer, Berlin (1996)
[2] Bianchi, M., Montagna, F.: n-contractive BL-logics. Arch. Math. Log. 50(3-4),
257–285 (2011)
[3] Blok, W., Pigozzi, D.: Algebraizable logics. Memoirs of The American Mathemat-
ical Society, vol. 77(396). American Mathematical Society (1989)
[4] Bova, S., Valota, D.: Finite RDP-algebras: duality, coproducts and logic. J. Log.
Comp. 22(3), 417–450 (2012)
[5] Cintula, P., Esteva, F., Gispert, J., Godo, L., Montagna, F., Noguera, C.: Distin-
guished algebraic semantics for t-norm based fuzzy logics: methods and algebraic
equivalencies. Ann. Pure Appl. Log. 160(1), 53–81 (2009)
[6] Cintula, P., H´ajek, P., Noguera, C.: Handbook of Mathematical Fuzzy Logic, vol. 1
and 2. College Publications (2011)
[7] Esteva, F., Godo, L.: Monoidal t-norm based logic: Towards a logic for left-
continuous t-norms. Fuzzy Sets Syst. 124(3), 271–288 (2001)
[8] Esteva, F., Godo, L., H´ajek, P., Navara, M.: Residuated fuzzy logics with an
involutive negation. Arch. Math. Log. 39(2), 103–124 (2000)
[9] H´ajek, P.: Metamathematics of fuzzy logic. Trends in Logic, vol. 4. Kluwer Aca-
demic Publishers (1998)
[10] Horˇc´ık, R., Noguera, C., Petr´ık, M.: On n-contractive fuzzy logics. Math. Log.
Q. 53(3), 268–288 (2007)
[11] Jenei, S.: A note on the ordinal sum theorem and its consequence for the con-
struction of triangular norms. Fuzzy Sets Syst. 126(2), 199–205 (2002)
[12] Johnstone, P.T.: Stone spaces. Cambridge Studies in Advanced Mathematics.
Cambridge University Press (1982)
[13] Klement, E.P., Mesiar, R., Pap, E.: Triangular norms. Trends in Logic, vol. 8.
Kluwer Academic Publishers (2000)
[14] Kowalski, T.: Semisimplicity, EDPC and Discriminator Varieties of Residuated
Lattices. Stud. Log. 77(2), 255–265 (2004)
[15] Montagna, F.: Generating the variety of BL-algebras. Soft Comput. 9(12), 869–874
(2005)
[16] Montagna, F.: Completeness with respect to a chain and universal models in fuzzy
logic. Arch. Math. Log. 50(1-2), 161–183 (2011)
[17] Noguera, C.: Algebraic study of axiomatic extensions of triangular norm based
fuzzy logics. Ph.D. thesis, IIIA-CSIC (2006), http://ow.ly/rV2sL
[18] Schweizer, B., Sklar, A.: Associative functions and abstract semigroups. Publ.
Math. Debrecen 10, 69–81 (1963)
[19] Wang, S.: A fuzzy logic for the revised drastic product t-norm. Soft Comput. 11(6),
585–590 (2007)

Fuzzy State Machine-Based Refurbishment
Protocol for Urban Residential Buildings
Gergely I. Moln´arka1 and L´aszl´o T. K´oczy2
1 Department of Building Constructions and Architecture, Sz´echenyi Istv´an
University, Egyetem t´er 1, Gy˝or, Hungary
2 Department of Automation, Sz´echenyi Istv´an University,
Egyetem t´er 1, Gy˝or, Hungary
Abstract. The urban-type residential houses built before World War
Two represent a large part of the built environment in Hungary. Due
to their physical condition and low energy-eﬃciency the retroﬁt of these
buildings is very much advisable nowadays. In this paper we propose
an approach based on fuzzy signatures and state machines, that helps
decision support for determining the renovation scenario concerning ne-
cessity, cost eﬃciency and quality. Using the knowledge obtained from
diagnostic surveys done during the previous decades by architect ex-
perts, and technical guides and the available database of contractors
billing, a protocol for the preparation for optimized refurbishment is pro-
posed, based on the concept of an extended fuzzy state machine model.
In this combined model the theoretical concepts of ﬁnite-state machine
and fuzzy state machine, and also the principles of fuzzy signatures are
applied.
Keywords: urban-type residential houses, building refurbishment pro-
tocol, fuzzy signature state machine.
1
Introduction
1.1
The EPBD Recast Requirements
Due to the new version of the European Energy Performance of Buildings Direc-
tive (EPBD), the EPBD Recast (Directive 2010/31/EU) newly erected buildings
in Europe may consume ”nearly zero” energy from 2021. This regulation also
inﬂuences the future of the existing building stock. In Hungary, the adaptabil-
ity of recast requirements in connection with several types of existing buildings
was examined recently. Among other statements, the ﬁnal report [1] veriﬁes that
even though due to settlement and other architectural attributes urban-type
residential houses (former tenement houses) in Budapest are not able to meet
these requirements; however, the improvement of their energy eﬀciency is still
strongly advisable.
In addition to this statement, a large-scale retroﬁt process of existing residen-
tial houses may be an eﬃective response to climate change and for reducing CO2
emission [2].
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 375–384, 2014.
c
⃝Springer International Publishing Switzerland 2014

376
G.I. Moln´arka and L.T. K´oczy
1.2
The Actual Physical State of Urban-Type Residential Houses
The above mentioned former tenement houses represent a signiﬁcant part of
existing Hungarian apartment buildings: 11% of all apartments are located in
these urban-type residential houses (in Budapest this ratio is 27%) built before
WW2 [3]. Due to several causes (age of buildings, highly fragmented ownership
structure (the former tenants became the owners), lack of capital, missing reg-
ular maintenance processes during the past, incompetence in the maintenance
at present, etc.) the average physical condition of this kind of residential build-
ings is below standard. In fact, they are unsuitable for direct energy eﬀciency
development. The symptoms of overall physical obsolescence and deterioration
are clearly observable even in representative urban areas. The types of building
failures that resulted in the current state are determined by the types of the
subject residential houses.
1.3
The Subgroups of Budapest Urban-Type Residential Houses
From building construction and architectural aspects in Budapest two main
types of urban-type residential houses exist. The major part of these buildings
was built between 1880 and 1920 in Academic Style; the remaining part between
1928 and 1944. The actual research focuses on the Academic Style residential
houses.
The criteria of this building type are the existence of courtyard and air-shafts;
the presence of apartments diﬃerentiated by size and orientation in the same
building; the traditional masonry of the load bearing structures, the varying
and innovative slab systems and the ventilated wooden structure pitched roofs.
A typical classic urban-type residential house is presented in Fig.1.
Fig. 1. Floor plan, section and elevation of an urban-type residential house with inner
court in Academic Style (copy of blueprints, property of Budapest Archives)
As a general overview of the deterioration of Academic Style residential houses
the following are ascertainable:

Fuzzy State Machine-Based Refurbishment Protocol for Urban-Type Houses
377
– Due to the lack of wall and plinth damp proof course (DPC), the soil moisture
oozes up into the brick walls, resulting in mold (damp air) in ground level
apartments and peeling the plaster layer oﬃ;
– Slab systems have an uncertain load-bearing performance (that also relates
to the outside corridors);
– The common mechanical (drainage systems, pipelines, elevators, etc.) and
electrical systems (including the lighting and the feeble current systems) are
out-dated, and may cause accidental bursts;
– The building envelope, ﬂoor covering and other additional elements of the
staircases and other common areas (entrance hall, court, outside corridors,
etc.) are broken and crumbling;
– The roof tiling and ﬂashing are deteriorated (cracked, punctured), and as
a result leaking occurs, especially in roof valleys and around the external
supplementary element fastenings;
– Low energetic performance of air-shaft partitioned wall;
– The ﬁnishing of the fa¸cade is detached or missing in greater areas, especially
on the surface of courtyard fa¸cades and ﬁrewalls.
Although in speciﬁc situations several other failures may be observed, the above
mentioned defects determine the actual state of the examined type of building
stock as it is shown in the series of [4], [5] and other building renovation guides.
Because of this these areas of deterioration are generally considered as the main
targets of the maintenance procedures (the refurbishment protocol). It should
be noted that the proposed method focuses on the building elements owned
jointly by the ﬂat owners and on constructions, which are determined by the
Householders’ Act (Hungarian Act of Residential Houses. 2003/CXXXIII).
1.4
Uncertainties in Decision-Making in the Refurbishment
Schedule
At present, the present owners (former tenants), as stakeholders, are responsible
for the stability of their property only. However, this role can be interpreted
widely: in case of developing the physical condition of the building construc-
tions a thorough refurbishment schedule has to be planned and accepted by the
owners.
In practice, several unprofessional factors inﬂuence the joint decision-making.
Among others, the community of the owners cannot interpret and handle the in-
formation in the standard building diagnostic surveys that report on the failures
and give general recommendations for a refurbishment. The more so as these
surveys date from diﬃerent period between the 1950s and the 2000s, and their
depth, precision, detail, etc. are very heterogeneous. Since the statements of sur-
veys do not give an explicit refurbishment protocol, the owners may have the
feeling that decisions about the steps of maintenance may be determined arbi-
trarily. However, although in some cases the accidentally determined individual
repairs may help develop the overall physical condition of the whole structure,
in general, only a consequent and well organized maintenance procedure might

378
G.I. Moln´arka and L.T. K´oczy
help determine the necessary decisions about the refurbishment steps, oﬃering
answers to questions on both ﬁnancing and scheduling.
1.5
Factors Inﬂuencing the Refurbishment Schedule
For designing such a decision support system, ﬁrst the steps of maintenance have
to be examined, taking into consideration their respective importance, interre-
lations and other non-constructional aspects. In our approach, the main goal of
the system is to examine each maintenance step that is necessary for renovating
the given building from its initial (present) state to the acceptable (ﬁnal) state.
There are several factors inﬂuencing the schedule of the refurbishment pro-
cedure. Without any details, the most important such factors are: the grade of
danger caused by the given failure, the interrelations among deteriorated con-
structions and decays, and the presence of a protocol for the repair, the ﬁnancial
schedulability, some complex logistics aspects, etc.
2
A Model Proposed for the Refurbishment Protocol
The proposed approach to be introduced here for tackling the problem described
above is a model and an attempt for solution based on the principles of fuzzy sig-
natures (or simply vector valued fuzzy sets) and fuzzy state machines combined
into what we will call Fuzzy Signature State Machines.
The most important reason for using fuzzy signatures here as the starting
point is the fact that the structure of the surveys follows the architectural and
civil engineering common sense, where the sub-structures and components of
each building are arranged in hierarchical tree-like structures, where the whole
building might be represented by the root of the tree and each major sub-
component is a ﬁrst level branch, with further sub-branches describing sub-sub-
components, etc. as it will be shown in the next sections.
At this point we revisit the deﬁnition of fuzzy signature. Fuzzy sets of a uni-
verse of discourse X are deﬁned by
A = {X, μA}, where μA : X →[0, 1].
Vector Valued Fuzzy Sets (VVFS) [6] are a simple extension that may be con-
sidered as a special case of L-fuzzy sets [7]:
An = {X, μAn}, where μAn : X →[0, 1]n. Thus a membership degree is a
multi-component value here, e.g. [μ1, μ2, · · · , μn]T .
Fuzzy Signatures represent a further extension of VVFS as here any compo-
nent might be a further nested vector, and so on [10], [8]:
Afs = {X, μAfs}, where μAfs : X →M1 × M2 × · · · × Mn, where Mi = [0, 1] or
[Mi1 × Mi2 × · · · × Min]T .

Fuzzy State Machine-Based Refurbishment Protocol for Urban-Type Houses
379
The next is a very simple example:
μAf s = [μ1, μ2, [μ31, μ32, [μ331 , μ332 , μ333 ]], μ4, [μ51, μ52], μ6]T
The ﬁrst advantage of using fuzzy signatures rather than VVFS is that here
any closer grouping and sub-grouping of fuzzy features may be given. Fuzzy
signatures are associated with an aggregation system. Each sub-component set
may be aggregated by its respective aggregation operation, thus reducing the
sub-component to one level higher. The above example has the following asso-
ciated aggregation structure:
{a0{a3{a33}{a5}}, where each a◦denotes an aggregation, particularly the one
associated with the child node x◦associated with μ◦, thus the following example
signature might be reduced upwards as follows:
μAfs ⇒[μ1, μ2, [μ31, μ32, μ33 = a33(μ331 , μ332 , μ333 ), μ4, μ5 = a5(μ51, μ52), μ6]T
⇒[μ1, μ2, μ3 = a3(μ31, μ32, μ33), μ4, μ5, μ6]T ⇒μ0 = a0(μ1, μ2, μ3, μ4, μ5, μ6)
This kind of membership degree reductions is necessary when the data are
partially of diﬃerent structure, e.g. some of the sub-components are missing.
Then operations among fuzzy signatures with partially diﬃerent structure may
be carried out, by ﬁnding the largest common sub-structure and reducing all sig-
natures up to that substructure. This might be necessary if the surveys referred
to in this paper are considered as often their depth and detail are diﬃerent. As
an example, maybe in survey one the roof is considered as a single component
of the house and is evaluated by a single linguistic quality label, while in sur-
vey two this is done in detail and tiles, beams, tinwork, chimneys are described
separately.
In our previous work we applied vector-valued fuzzy sets and fuzzy signa-
tures [10] for describing sets of objects with uncertain features, especially when
an internal theoretical structure of these features could be established. In [11]
we presented an approach where the fuzzy signatures could be deployed for de-
scribing existing (typically old) residential houses in order to support decisions
concerning when and how these buildings should be renovated (or, if necessary
demolished). In that research a series of theoretically arrangeable features were
taken into consideration and eventually a single aggregated fuzzy membership
value could be calculated on the basis of available detailed expert evaluation
sheets. In that model, however, the available information does not support any
decision strategy concerning actual sequence of the measures leading to complete
renovation; and it is also insuﬀcient to optimize the sequence from the aspect
of local or global cost eﬀciency.
In the following section, the mathematical model of the proposed refurbish-
ment protocol will be introduced.

380
G.I. Moln´arka and L.T. K´oczy
2.1
Application of Fuzzy Finite State Machines in the Modelling
Finite State Machines are determined by the sets of input states X, internal
states Q, and the transition function f. The latter determines the transition
that will occur when a certain input state change triggers state transition. There
are several alternative (but mathematically equivalent) models known from the
literature. For simplicity the following is assumed as the starting point of our
new model:
A = ⟨X, Q, f⟩
(1)
f : X × Q →Q
(2)
where X = {xi} and Q = {qi}.
Thus, a new internal state is determined by the transition function as follows:
qi+1 = f(xi, qi)
(3)
In matrix form:
F =
⎡
⎢⎢⎢⎣
f(x1, q1) f(x2, q1) . . . f(xn, q1)
f(x1, q2)
f(xn, q2)
...
...
f(x1, qm) f(x2, qm) . . . f(xn, qm)
⎤
⎥⎥⎥⎦
(4)
The transition function/matrix maybe interpreted with help of a relation R
on X × Q × Q,where
R(xi, qj, qk) = 1
(5)
if
f(xi, qj) = qk
(6)
and
R(xi, qj, qk) = 0
(7)
if
f(xi, qj) ̸= qk
(8)
The states of the ﬁnite state machine are elements of Q. In the present applica-
tion an extension to fuzzy states is considered in the following sense. Every aspect
of the phenomenon to model is represented by a state universe of sub-states Qi.
The states themselves are (fuzzy) subsets of the universe of discourse state sets,
so that within Qi a frame of cognition is determined (its ﬁneness depending on
the application context and on the requirements toward the optimisation algo-
rithm), so that typical states like ”Totally intact”, ”Slightly damaged”, ”Medium

Fuzzy State Machine-Based Refurbishment Protocol for Urban-Type Houses
381
condition”, etc., up to ”Dangerous for life” are considered. Any transition from
one state to the other (improvement of the condition, refurbishment or reno-
vation) involves a certain cost c. In the case of a transition from qi to qj it is
expressed by a membership value μij = c(qi, qj). In our model the added cost
Θμij along a path qi1 →qi2 →· · · →qin is not usually equivalent with the cost
of the transition μin along the edge qil →qin. This is in accordance with the
non-additivity property of the fuzzy (possibility) measure and is very convenient
in our application, as it is also not additive in the case of serial renovations.
As a simple example the Fig. 2 depicts the possible transitions among the
states of a speciﬁc sub-state: Q1
0 represents the initial (present) state, while Q1
9
represents the acceptable (ﬁnal) state.
Fig. 2. Diagram representation of a sub-state space and the possible transitions among
the states
In the case of fuzzy signature machines each of the leaves contains a sub-
automaton with the above property. The parent leave of a certain sub-graph is
constructed from the child leaves, so that the sub-automaton
Ai = Ai1 × Ai2 × · · · × Aim, and thus the states of Ai are
Qi = Qi1 × Qi2 × · · · × Qin, so that the transition Qj1 →Qj2 in this case
means the parallel (or subsequent) transitions
qj11 →qj12 × qj21 →qj22 × · · · × qjn1 →qjn2
A special aggregation is associated with each leaf; similarly as it is in the fuzzy
singatures, however, in this case the aggregation calculates the resulting cost
μj12 of the transition qj1 →qj2, so that
μj12 = c(qj1, qj2) = aj(c(qj11 , qj12 ), c(qj21 , qj22 ), · · · , c(qjn1 , qjn2 ))
where a stands for the respective aggregation. (Note that these aggregations
sometimes do not satisfy the symmetricity property of the general axiom struc-
ture of aggregations, thus it may be referred as a ”non-symmetric aggregation”.)
2.2
Modelling the Repair Procedure
As an initial state an overall visual diagnostic survey of the given residential
building is supposed. This evaluation sheet gives detailed determination and

382
G.I. Moln´arka and L.T. K´oczy
state description of each building component, disclosing the relation between
causes and consequences. The obtained information helps determining the main-
tenance steps; observing their inﬂuencing attributes additional data can be given
to each step. With the knowledge of professional rules as they are clearly de-
scribed in [12], [13], the general maintenance procedure can be decomposed into
distinct sequences that correspond to the previously mentioned supposition. In
Table 1, the sequences as sub-automaton of this procedure are represented by
A1 to A8.
Table 1. Distinct sequences of general refurbishment of Academic Style residential
houses
Sequence Maintenance procedure
A1
Roof structure refurbishment
A2
Basement and plinth renovation
A3
Side corridor renovation
A4
Entrance hall renovation
A5
Courtyard renovation
A6
Staircase renovation
A7
Mechanical system refurbishment
A8
Electrical system refurbishment
In further analyses, these sub-automata represent parent nodes of building
component groups that constitute independent or quasi-independent mainte-
nance processes. As an example, the Table 2 represents the sub-sub automata
of A1 process.
Table 2. Maintenance subsequences in A1 process (Roof structure refurbishment)
Subsequences Maintenance sequence
A11
Chimney shafts repair
A12
Removing unnecessary supplementary elements (e.g. aerials)
A13
Replacement of dangerous catwalks
A14
Replacement of damaged ﬂashings
A15
Timber roof structure repair
A16
Firewall renderings
A17
Mounting subsidiary sheeting and its components
A18
Roof tiling and necessary tinsmith work
The complexity of the renovation procedure can be demonstrated with the
example of chimney shaft repair maintenance step (subsequence A11). In case of
Academic Style urban-type residential buildings there are numerous chimneys
(or groups of chimneys) that have to be handled separately: depending on its
physical condition and necessity each chimney shaft can be repaired, rebuilt or

Fuzzy State Machine-Based Refurbishment Protocol for Urban-Type Houses
383
demolished. In addition to this, the stakeholders’ ﬁnancial abilities may also de-
termine that this maintenance process is executed in one step or in individual
steps considering the importance of intervention. In the proposed model these
facts can be considered with the implementation of signature structure: in this
case the Q1
1 vector as parent node can be determined as nested vectors of child
nodes (Q1
11, Q1
11, . . ., Q1
1n vectors). In the evaluation process, the importance
of several factors (e.g. necessity, ﬁnancial circumstances, etc.) can be taken into
consideration in the aggregation procedure with the application of weighted rel-
evance aggregation operator (WRAO) [14]. It is clearly visible that in major
cases the decomposition of vectors can help modelling the maintenance proce-
dure; in some cases this operation is omissible. Following the instructions of
major technical literatures of building renovation the state space of every de-
ﬁned component machine can be easily determined. With the support of oﬀcial
billing contractors’ database and with the basis of collected information about
stakeholders’ intention in renovation scenario, the input states that inﬂuence
the operation of component machines can be determined in each state. In major
cases, several alternative solutions for repair steps can be deﬁned: these solutions
can be diﬃerent in price (with the linguistic categories of cheap, reasonable, ex-
pensive) quality (good, acceptable and poor) and life span (short, average and
long). Among these the optimal solution can be chosen with the support of fuzzy
evaluation. The transition degrees that determine the next internal states in the
state space depend on the mentioned ﬁnancial factors and technical adaptability.
The state space of A11 sub-automata is illustrated in Fig. 2, where Q1
0 to Q1
9
represent the initial, the internal and ﬁnal states; the μ1
1 to μ1
16 represent the
transition degrees. As a result of the evaluation of the transitions among the
states of A11 the optimum solution for the most eﬃective renovation process of
chimney shaft can be obtained, concerning the renovation costs.
3
Conclusion and Future Work
The presented determination of the physical condition and the refurbishment
sequences of Academic Style residential houses give a general state description
and provide the necessary maintenance steps for the mentioned building type
only. Another seriously aﬃected building type that has similar diﬀculties in re-
furbishment procedure is the residential house that was built in the interwar era.
The experienced defects are quite diﬃerent, therefore the renovation sequences
of Modern Style residential buildings have to be determined separately.
In general case the extended structure of a vectorial fuzzy state machine
(with theoretically) unbounded number of components results in diﬀculties in
optimization. In practice, the optimization of the refurbishment procedure of any
sort of residential buildings always has a limited number of sequences; however
this number might be rather high. Thus, our intention for the future is to ﬁnd a
proper heuristics at the basic approach to this optimization task, which is able
to provide a quasi-optimal solution for every concrete problem, or a very lightly
optimal solution for every problem in a manageable time. These methods seem

384
G.I. Moln´arka and L.T. K´oczy
to be various population-based evolution algorithms, which may be combined
with local search cycles (evolutionary memetics), e.g. bacterial or particle swarm
algorithms.
Acknowledgments. The authors wish to record their gratitude to Szilveszter
Kov´acs for his advices and professional assistance during each phase of the
project.
The research was supported by T´AMOP-4.2.2.A-11/1/KONV-2012-0012 and
Hungarian Scientiﬁc Research Fund (OTKA) K105529, K108405.
References
1. Csoknyai, T., Kalmar, F., Szalai, Z., Talamon, A., Z¨old, A.: Basic standards of
nearly zero energy consuming buildings with renewable energy sources. Technical
report, University of Debrecen, Hungary (2012) (in Hungarian)
2. Hrabovszky-Horv´ath, S., P´alv¨olgyi, T., Csoknyai, T., Talamon, A.: Generalized
residential building typology for urban climate change mitigation and adaptation
strategies: The case of Hungary. Energy and Buildings 62, 475–485 (2013)
3. Csizmady, A., Heged¨us, J., Kravalik, Z., Teller, N.: Long-term housing conception
and mid-term housing program of Budapest, Hungary. Technical report, Local
Government of Budapest, Hungary (2005) (in Hungarian)
4. Schild, Oswald, Roger, Schweikert: Weak Points 1-4. Bauverlag GmbH (1980) (in
German)
5. Arat´o, A.: Directives and techincal guide to evaluate load-bearing constuctions of
old buildings 1-5. Technical report, TTI, Budapest (1987) (in Hungarian)
6. K´oczy, L.T.: Vector Valued Fuzzy Sets. J. BUSEFAL 4, 41–57 (1980)
7. Goguen, J.: L-fuzzy sets. Journal of Mathematical Analysis and Applications 18,
145–174 (1967)
8. Pozna, C., Minculete, N., Precup, R.-E., K´oczy, L.T., Ballagi, ´A.: Signatures: Deﬁ-
nitions, operators and applications to fuzzy modelling. Fuzzy Sets and Systems 201,
86–104 (2012)
9. Ruspini, E.H.: A new approach to clustering. Information and Control 15, 22–32
(1969)
10. K´oczy, L.T., V´amos, T., Bir´o, G.: Fuzzy Signatures. In: EUROFUSE-SIC 1999,
pp. 210–217 (1999)
11. Moln´arka, G.I., K´oczy, L.T.: Decision Support System for Evaluating Existing
Apartment Buildings Based on Fuzzy Signatures. IJCCC 6, 442–457 (2011)
12. de Freitas, V.P. (ed.): A State-of-the-Art Report on Building Pathology (CIB
W086). Technical report, Porto University-Faculty of Engineering, Porto, Portugal
(2013)
13. Harris, S.Y.: Building pathology: deterioration, diagnostics, and intervention. Wi-
ley, New York (2001)
14. Mendis, B.S.U., Gedeon, T.D., Botzheim, J., K´oczy, L.T.: Generalised Weighted
Relevance Aggregation Operators for Hierarchical Fuzzy Signatures. In: Interna-
tional Conference on Computational Intelligence for Modelling, Control and Au-
tomation (CIMCA 2006) (2006)

Exploring Inﬁnitesimal Events through
MV-algebras and non-Archimedean States
Denisa Diaconescu1, Anna Rita Ferraioli2,
Tommaso Flaminio2, and Brunella Gerla2
1
Department of Computer Science, Faculty of Mathematics and Computer Science,
University of Bucharest, Romania
ddiaconescu@fmi.unibuc.ro
2 DiSTA - Department of Theoretical and Applied Science,
University of Insubria, Italy
{annarita.ferraioli,tommaso.flaminio,brunella.gerla}@uninsubria.it
Abstract. In this paper we use tools from the theory of MV-algebras
and MV-algebraic states to study inﬁnitesimal perturbations of classical
(i.e. Boolean) events and their non-Archimedean probability. In particu-
lar we deal with a class of MV-algebras which can be roughly deﬁned by
attaching a cloud of inﬁnitesimals to every element of a ﬁnite Boolean al-
gebra and for them we introduce the class of Chang-states. These are non-
Archimedean mappings which we prove to be representable in terms of
a usual (i.e. Archimedean) probability measure and a positive group ho-
momorphism capable to handle the inﬁnitesimal side of the MV-algebras
we are dealing with. We also study in which relation Chang-states are
with MV-homomorphisms taking value in a suitable perfect MV-algebra.
Keywords: MV-algebras, non-Archimedean states, probability measures.
1
Motivation
Consider a Boolean algebra B. In classical probability theory every element b ∈B
determines an uncertain, but precisely deﬁned, statement about the world for
which, through a probability measure p on B, we want to estimate how likely it
is to happen, i.e. b is an event. Imagine now a situation in which the statements
for which we want to measure their uncertain value are imprecise, vague, and
hence such that they are loosely modeled in the realm of Boolean algebras. In
this paper we are interested in providing a model to deal and probabilistically
estimate those peculiar imprecise events which arise from classical, precisely
determined, ones by perturbing each of them by inﬁnitesimal values. In particular
we shall deal with algebraic structures which are built starting from any Boolean
algebra B and by attaching, to each element of B, a cloud of inﬁnitesimals. In
this generalized realm we shall introduce a notion of measure which we prove to
be a variant of classical probability measures which can further cope with the
inﬁnitesimal information brought by such models.
In order to introduce the algebraic models we want to deal with, we are going
to use the following ingredients: a Boolean algebra and an abelian ℓ-group G,
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 385–394, 2014.
c
⃝Springer International Publishing Switzerland 2014

386
D. Diaconescu et al.
the former being the domain of precise events and the latter being a suitable
environment for inﬁnitesimals. Although there might be several ways to combine
B and G to obtain a model for imprecise events, we shall deﬁne a construction
which allows to show that the resulting structure actually is a peculiar kind
of MV-algebra [8]. Those are algebraic structures which are intimately related
to the inﬁnite valued ﬄLukasiewicz calculus and for which, in the last years, a
generalization of probability theory (i.e. state theory [7,8]) have been developed.
For these reasons MV-algebras are an appropriate setting for our investigation.
It is also worth to remark the following:
(1) If, instead of an arbitrary Boolean algebra B, we restrict to the Boolean chain
2, the construction we are going to introduce, and which consists in combining
B with an abelian ℓ-group G, produces perfect MV-algebras [1]. Those form
a well-known class of MV-algebras which contains, in particular, Chang MV-
algebra C. This structure, although it is usually deﬁned in terms of the group-
theoretical construction of lexicographic product [6], can also be described in
our framework. C can intuitively be regarded as a perturbation of the Boolean
chain 2 by inﬁnitesimals from the abelian ℓ-group Z of integers.
(2) MV-algebraic states, being [0, 1]-valued functions, do not preserve positive
inﬁnitesimals. For this reason, and since the algebras we are going to introduce
do have positive inﬁnitesimal values, we shall need to slightly change the notion
of state by considering, rather than [0, 1]-valued functions, mappings ranging on
a suited deﬁned MV-subalgebra of a non-trivial ultrapower ∗[0, 1] of [0, 1].
The paper is structured as follows: in the next section we introduce all the
necessary background on MV-algebras with a particular focus on the class BG
of structures arising by adding a cloud of inﬁnitesimals around Boolean elements.
Also the notion of state of an MV-algebra is given together with its relation with
states of ℓ-groups. In the third section the main deﬁnitions and results are stated:
the notion of Chang-state over algebras in BG is given and a characterization
of Chang-states in terms of Boolean probabilities and ℓ-states is proved.
2
Preliminaries
2.1
MV-algebras and BG-algebras
Deﬁnition 1. An MV-algebra is a structure (A, ⊕,∗, 0), where ⊕is a binary
operation, ∗is a unary operation and 0 is a constant such that the following
conditions are satisﬁed for any a, b ∈A:
(MV1) (A, ⊕, 0) is an abelian monoid, (MV2) (a∗)∗= a,
(MV3) 0∗⊕a = 0∗
(MV4) (a∗⊕b)∗⊕b = (b∗⊕a)∗⊕a.
We can deﬁne a new constant 1 = 0∗and an auxiliary operation ⊙as x ⊙y =
(x∗⊕y∗)∗.
The class of MV-algebras forms a variety that we shall denote by MV.

Inﬁnitesimal Events, MV-algebras and non-Archimedean States
387
In any MV-algebra a lattice order is deﬁned by setting x ∨y = (x∗⊕y)∗⊕y.
The most important example of MV-algebra is the unit interval [0, 1] equipped
with the operations x∗= 1 −x and x ⊕y = min(x + y, 1). This algebra, denoted
[0, 1]MV , is called the standard MV-algebra and it generates MV as a variety and
as quasi-variety [2].
The largest Boolean subalgebra of an MV-algebra A, called the Boolean skele-
ton of A, is based on {x ∈A | x ⊕x = x} and it is denoted B(A).
A non-empty subset I of an MV-algebra A is said to be an ideal if x ∈I and
y ≤x, then y ∈I, and if x, y ∈I, then x⊕y ∈I. An ideal I is said maximal if it
is proper and it is not contained in any proper ideal of A. For a given MV-algebra
A, Rad(A) (the radical of A) denotes the intersection of all maximal ideals of A.
The radical of an MV-algebra A contains all the inﬁnitesimal elements of A. In
fact, for every x ∈A, x ∈Rad(A) iﬃ, for every n ∈N, nx ≤x∗, where nx is an
abbreviation for x ⊕. . . ⊕x (n-times). Dually, the co-radical Rad(A)∗= {x∗|
x ∈Rad(A)} of an MV-algebra A is the set of all co-inﬁnitesimal elements of A
and is denoted by Rad(A)∗.
An abelian ℓ-group is a structure G = (G, +, −, 0, ∨, ∧) such that (G, ∨, ∧) is
a lattice, (G, +, −, 0) is an abelian group and + is order preserving. Let G be
an ℓ-group and let u ∈G, u > 0. We call u a strong unit if for each x ∈G,
there exists n ∈Ω such that x ≤nu. Throughout this paper ℓ-groups will
always be abelian. Mundici proved the existence of an equivalence functor Δ
between the category of MV-algebras and the category of ℓ-groups with strong
unit [2]. For every ℓ-group with strong unit (G, +, 0, u), we obtain the MV-
algebra Δ(G, u) = [0, u] = {x ∈G | 0 ≤x ≤u} by equipping the unit interval
[0, u] with the following operations: x ⊕y = u ∧(x + y), x∗= u −x.
A relevant subclass of MV-algebras is that of perfect MV-algebras, i.e. MV-
algebras that are generated by their radical. Indeed, every perfect MV-algebra P
can be displayed as Rad(P)∪Rad(P)∗, all its elements being either inﬁnitesimals
or co-inﬁnitesimals. Although a complete treatment of these structures is beyond
the scope of this paper, it is worth mentioning that in [6] Di Nola and Lettieri
established a categorical equivalence between perfect MV-algebras and ℓ-groups
which can be roughly summarized as follows:
1. Given an ℓ-group G, the Di Nola-Lettieri functor δ assigns to G, the perfect
MV-algebra P ∼= Δ(Z ×lex G, (1, 0)) where ×lex is the lexicographic product
between ℓ-groups.
2. For every perfect MV-algebra P, Rad(P) uniquely generates an ℓ-group G
(denoted δ−1(⟨Rad(P)⟩), where ⟨Rad(P)⟩denotes the perfect MV-algebra
Rad(P) ∪Rad(P)∗generated by Rad(P)) and such that Rad(P) coincided
with the positive cone G+ of G.
The well known Chang MV-algebra C is hence the perfect MV-algebra δ(Z) =
Δ(Z ×lex Z, (1, 0)).
Let B = (B, ∨, ∧,∗, 0, 1) be a Boolean algebra. We recall that an ideal of B is
a subset I ⊆B such that for every x ∈I and y ≤x also y ∈I and if x, y ∈I
then x ∨y ∈I. I is maximal if it is proper and it is not contained in any proper
ideal of B. Note that M is a maximal ideal of B if and only if it is an ideal and

388
D. Diaconescu et al.
for every x ∈B either x ∈M or x∗∈M. We denote by Max(B) the set of
maximal ideals of B.
Let M be a maximal ideal of B, let G = (G, +, 0, ) be an ℓ-group and let A
be a subset of the cartesian product B × G deﬁned as follows:
A = {M × G+} ∪{M × G−}
where G+ = {g ∈G | g ≥0} is the positive cone of G, G−= {g ∈G | g ≤0} the
negative cone and M is the set-complement of M. We stress that 0 ∈G+ ∩G−.
Let us deﬁne over A the following two operations, ⊕and ¬:
(b1, g1) ⊕(b2, g2) =
⎧
⎨
⎩
(b1 ∨b2, g1 + g2)
if b1, b2 ∈M
(b1 ∨b2, 0)
if b1, b2 ∈M
(b1 ∨b2, (g1 + g2) ∧0) otherwise
¬(b, g) = (b∗, −g).
We hence have
(b1, g1) ⊙(b2, g2) =
⎧
⎨
⎩
(b1 ∧b2, g1 + g2)
if b1, b2 ∈M
(b1 ∧b2, 0)
if b1, b2 ∈M
(b1 ∧b2, (g1 + g2) ∨0) otherwise
(1)
We denote by B ×M G the structure (A, ⊕, ¬, (0, 0)). Then we have:
Proposition 1 ([4]). For each Boolean algebra B, maximal ideal M of B and
ℓ-group G, the structure A = B×MG is an MV-algebra such that B(A) = B×{0}
and Rad(A) = {0} × G+.
Let us denote by BG the class of all MV-algebras isomorphic to B ×M G, for
some ﬁnite1 Boolean algebra B, M ∈Max(B) and ℓ-group G.
Example 1.
1. Each ﬁnite Boolean algebra B is in BG. Indeed, take G = {0} and M be
any maximal ideal of B, then B ∼= B ×M {0}.
2. Each perfect MV-algebra P is in BG. Indeed, let B = 2, M = {0} and
G be the ℓ-group associated with P in the Di Nola-Lettieri functor, i.e.
G = δ−1(⟨Rad(A)⟩) then P ∼= 2 ×{0} G. In particular, the perfect MV-
algebra δ(R) = Δ(Z×lexR, (1, 0)) is the MV-algebra in BG given by B = 2,
M = {0} and G = R.
3. Consider the Boolean algebra B4 = 22 and take M = {(0, 0), (1, 0)} as
a maximal ideal of B4. Then B4 ×M Z = (M × Z+) ∪(M × Z−) that is
isomorphic to B2 × C.
Proposition 2 ([4]). For every A ∈BG and (b, g) ∈A, we have
(b, g) =

(b, 0) ⊕(0, g) if b ∈M
(b, 0) ⊙(1, g) if b /∈M.
1 This class of algebras can be deﬁned in general, getting rid of the restriction on B
to be ﬁnite. For the purpose of this paper, we shall just focus on algebras in BG
whose Boolean skeleton is ﬁnite.

Inﬁnitesimal Events, MV-algebras and non-Archimedean States
389
2.2
States of MV-algebras and States of ℓ-groups
The notion of state of an MV-algebra was introduced by Mundici [7]:
Deﬁnition 2. If A is an MV-algebra, a state of A is a function s : A →[0, 1]
satisfying:
– s(1) = 1,
– for all a, b ∈A such that a ⊙b = 0, s(a ⊕b) = s(a) + s(b).
A state s is faithful if s(a) = 0 implies a = 0, for every a ∈A.
Every MV-homomorphism of an arbitrary MV-algebra A into [0, 1]MV is a
state and, moreover, every state is a limit, in the product topology of [0, 1]A, of
convex combinations of homomorphisms [7].
It is well know (cf. [7]) that states do not preserve inﬁnitesimals of the MV-
algebra they are deﬁned on. This behavior is particularly drastic in the case of
perfect MV-algebra: each perfect MV-algebra admits only one trivial state which
sends the inﬁnitesimal elements to 0 and the co-inﬁnitesimal elements to 1. As
an example, it is worth noticing that Chang MV-algebra has only one trivial
state. i.e. a map s : C →[0, 1] such that s(x) = 0, for all x ∈Rad(C), and
s(x) = 1, for all x ∈Rad(A)∗.
We recall that a state of an ℓ-group G is a function m : G →R which is
additive and positive. If (G, u) is an ℓu-group, then a state m of G is normalized
if m(u) = 1. In the sequel, we will call a state of an ℓ-group simply by an ℓ-state
and we will call an ℓu-state any normalized ℓ-state.
The following result establishes the relation between ℓu-states and states of
MV-algebras:
Theorem 1 ([7]). Let (G, u) be an ℓu-group and let A = Δ(G, u). Then there
is a one-one correspondence between ℓu-states of G and states of A.
3
Chang-States
In this section we are going to introduce a notion of state for the algebras in
BG that we call Chang-state.2 As every MV-algebra A ∈BG has a non-trivial
radical, states of A make all the inﬁnitesimal elements collapse so that basically
they coincide with Boolean probabilities over the Boolean skeleton. Therefore,
we now present a particular MV-algebra, denoted L(Rε), which provides, in our
opinion, a natural codomain for Chang-states.
Let ∗[0, 1] be a non-trivial ultrapower of the standard MV-algebra [0, 1]MV ,
and let ε ∈∗[0, 1] be any positive inﬁnitesimal. Then we denote by L(Rε) that
2 The name Chang-state comes from the following observation: the MV-algebras in BG
are particular cases of structures which can be framed in the algebraic variety V(C)
generated by Chang MV-algebra C. Therefore, although we are not working in the
whole V(C), we are conﬁdent that the intuition behind our deﬁnition of Chang-states
also applies in generalizing this notion to the whole V(C).

390
D. Diaconescu et al.
MV-algebra which is generated, in ∗[0, 1], by the reals in [0, 1] plus all elements
εr for r ∈R. In symbols
L(Rε) = ⟨[0, 1] ∪{rε | r ∈R}⟩∗[0,1].
Remark 1. L(Rε) can be equivalently presented as the MV-algebra that Mundici’s
functor Δ associates to the ℓ-group with strong unit (R×lexR, (1, 0)). This kind of
representation somehow justiﬁes the symbol L(Rε), and the name “lexicographic
R”. Let us also notice that the radical R of L(Rε) coincides with {rε | r ∈R+}.
Deﬁnition 3. Let A ∈BG. A map s : A →L(Rε) is a Chang-state of A if
1. s(1) = 1,
2. if a ⊙b = 0, then s(a ⊕b) = s(a) + s(b),
3. s ↾B(A) is a faithful probability measure.
Lemma 1. For every A ∈BG and for every Chang-state s of A, the following
properties hold:
1. s(a∗) = 1 −s(a) = s(a)∗,
2. s(0) = 0,
3. if a ≤b then s(a) ≤s(b),
4. if y ∈Rad(A) then s(y) ∈R.
Proof. (1)-(3) The proof can be easily obtained by adapting the analogous result
[8, Proposition 10.2] proved for states of MV-algebras.
(4) Since y ∈Rad(A) we get ny ≤y∗, for any n ∈N, so, from (3) s(ny) ≤
s(y∗), and hence, from (1), s(ny) ≤s(y)∗for any n ∈N. From (ny) ⊙y = 0, for
any n ∈N, it follows that s(ny) = ns(y) for any n ≥1. Hence ns(y) ≤s(y)∗, for
any n ∈N, which implies that s(y) ∈R.
□
Theorem 2. Let A ∈BG and a map s : A →L(R). The following are equiva-
lent:
1. s is a Chang-state of A,
2. there exists a faithful probability measure p on B(A) and an ℓ-state ψ of
δ−1(⟨Rad(A)⟩) such that
s(b, g) = p(b) + εψ(g).
Proof. We can uniquely display A as A = (M × G+) ∪(M × G−), where M
is a maximal ideal of a Boolean algebra B and G is an ℓ-group. Note that
B(A) = B × {0} and Rad(A) = {0} × G+.
(1) ⇒(2). Let s be a Chang-state of A and denote by p the faithful probability
measure obtained as restriction of s to B(A). Further, for every (0, g) ∈Rad(A)
(where g ≥0), set
ψ1(0, g) = s(0, g)
ε
∈R+ .

Inﬁnitesimal Events, MV-algebras and non-Archimedean States
391
Then ψ1 can be extended to a local state of the perfect MV-algebra P(A) =
⟨Rad(A)⟩and, by Lemma 13 of [5], the map ψ : G →R such that for every
g ≥0
ψ(g) = ψ1(0, g) = s(0, g)
ε
is a state of the ℓ-group G.
If (b, g) ∈A with b ∈M and g ≥0, by Proposition 2 we have
s(b, g) = s((b, 0) ⊕(0, g)) = s(b, 0) + s(0, g) = p(b) + εψ(g)
since (b, 0) ⊙(0, g) = (0, 0).
On the other hand, if (b, g) is such that b /∈M and g ≤0, then
(b, g) = (b, 0) ⊙(1, g) = ¬(¬(b, 0) ⊕(1, g)) = ¬((b∗, 0) ⊕(0, −g))
where (b∗, 0) ⊙(0, −g) = (0, 0) and
s(b, g) = 1 −(s(b∗, 0) + s(0, −g)) = 1 −(1 −p(b) + εψ(−g)) = p(b) + εψ(g).
(2) ⇒(1). Let p be a faithful probability measure on B(A) and let ψ be an
ℓ-state on G = δ−1(⟨Rad(A)⟩). Let hence, for each (b, g) ∈A, s(b, g) be deﬁned
as
s(b, g) = p(b) + εψ(g).
Since p is faithful, for every b ∈B(A)\ 2, we have 0 < p(b) < 1 and hence, being
ε a positive inﬁnitesimal, 0 < s(b, g) < 1 too. Moreover, it is easy to see that
s(1, 0) = 1 and s(0, 0) = 0, whence s(b, g) ∈L(Rε) for each (b, g) ∈A.
Let (b, g), (b′, g′) ∈A such that (b, g) ⊙(b′, g′) = (0, 0) = 0. This means, from
(1), that, either b ∧b′ = 0 and g + g′ = 0 if b, b′ ∈M, or b ∧b′ = 0 if b, b′ ∈M,
or b ∧b′ = 0 and g + g′ ≤0 otherwise. Let hence enter a case distinction:
1. If b, b′ ∈M, then s((b, g) ⊕(b′, g′)) = s(b ∨b′, 0) = p(b ∨b′) + εψ(0) =
p(b) + p(b′) + εψ(g + g′) = p(b) + p(b′) + εψ(g) + εψ(g′) = s(b, g) + s(b′, g′).
2. If b, b′ ∈M, then s((b, g)⊕(b′, g′)) = s(b∨b′, g +g′) = p(b∨b′)+εψ(g +g′) =
s(b, g) + s(b, g′).
3. If ﬁnally b ∈M and b′ ∈M, then g ∈G+ and g′ ∈G−, hence either
g + g′ ≥0, or g + g′ ≤0. On the other hand, since (b, g) ⊙(b′, g′) = (b ∧
b′, (g + g′) ∨0) = (0, 0), the former is never the case and hence g + g′ ≤0.
Then s((b, g) ⊕(b′, g′)) = s(b ∨b′, g + g′) = p(b ∨b′) + εψ(g + g′) = p(b) +
p(b′) + εψ(g) + εψ(g′) = s(b, g) + s(b, g′).
Hence s is a Chang-state and the proof is complete.
□
3.1
On the Space of Chang-States
It is known [7] that, given any MV-algebra A, each homomorphism of A into the
standard MV-algebra [0, 1]MV is a state.

392
D. Diaconescu et al.
As we set L(Rε) as a natural codomain for Chang-states, we are interested
now in studying the relation between Chang-states and MV-homomorphisms
of any BG-algebra A into L(Rε). Notice that, since every MV-homomorphism
h : A →L(Rε) actually maps B(A) into B(L(Rε)) = 2, this is the same as con-
sidering homomorphisms of A into the BG-algebra δ(R) we brieﬂy introduced
in Example 1 (2).
It is easy to see that, if A ∈BG and h is any MV-homomorphism of A into
δ(R), then h is not a Chang-state. In fact, if h was a Chang-state of A, its
restriction h ↾B(A) on the Boolean skeleton B(A) of A, would be faithful (as
Boolean homomorphism). But h ↾B(A) is a homomorphism of B(A) into 2 and
hence, unless B(A) = 2, there are elements b, b∗∈B(A) \ 2 for which either
h ↾B(A) (b) = 1 or h ↾B(A) (b∗) = 1.
Faithful probability measures can be characterized as follows:
Proposition 3. Let B be a ﬁnite Boolean algebra. Then p : B →[0, 1] is a
faithful probability measure iﬀ
p ∈int(conv(H(B, 2))).
That is, p is a faithful probability measure iﬀp belongs to the interior of the
convex polytope generated by the (ﬁnitely many) homomorphisms of B into 2.
Proof. Let B be a ﬁnite Boolean algebra and let ϕ1, . . . , ϕt be its atoms. Then,
every probability measure on B, when restricted to the ϕi’s, deﬁnes a probability
distribution, and every probability measure on B arises in this way. Moreover
it is well known (see [9]) that the class of all probability measures on B coin-
cides with the convex polytope conv(H(B, 2)). In fact, since atoms of B and
homomorphisms of B into 2 are in bijection, every p is uniquely expressible as
t
j=1 p(ϕj)hj (where hj is the unique Boolean homomorphism corresponding
to ϕj). Moreover, the interior of the convex polytope generated by h1, . . . , ht
is characterized by those convex combinations 
j αjhj where the parameters
satisfy αj > 0 for every j. Then it is left to show that p is faithful iﬃ, for every
j = 1, . . . , t, p(ϕj) > 0.
(⇒) Assume, by way of contradiction that there exists an ϕj for which p(ϕj) = 0,
then, being ϕj ̸= 0, p cannot be faithful.
(⇐) Conversely, assume p(ϕj) > 0 for every ϕj. Then, if p(b) = 
h p(ϕj)h(b) =
0, necessarily h(b) = 0 for every h ∈H(B, 2), that is, b = 0 and p is faithful. □
Now, let A ∈BG, let h be a Boolean homomorphism of B(A) into 2, and let ψ
be an ℓ-state on G = δ−1(⟨Rad(A)⟩). Then, if we deﬁne k on A as
k(b, g) = h(b) + εψ(g) for every (b, g) ∈A,
then k is not an MV-homomorphism since, for some (b, g) ∈A it can be the
case that k(b, g) > 1. Indeed, k maps A into Z ×lex R. On the other hand,
these mappings provides a peculiar characterization for Chang-states in terms
of convex combinations.

Inﬁnitesimal Events, MV-algebras and non-Archimedean States
393
Proposition 4. Let A ∈BG and let s : A →L(Rε). The following are equiva-
lent:
1. s is a Chang-state,
2. for every Boolean homomorphism h : B(A) →2 there exists an ℓ-state
ψh : δ−1(⟨Rad(A)⟩) →R and a real number αh such that 
h αh = 1,
αh > 0 and for every (b, g) ∈A
s(b, g) =

h∈H(B(A),2)
αh(h(b) + εψh(g)).
Proof. (⇒). Let s : A →L(Rε) be a Chang-state. Then from Theorem 2,
there exists a faithful probability measure p : B(A) →[0, 1] and an ℓ-state
ψ : δ−1(⟨Rad(A)⟩) →R such that, for all (b, g) ∈A one has s(b, g) = p(b)+εψ(g).
Now, Proposition 3 ensures that, being p faithful and B(A) being ﬁnite, the ex-
istence, for every h ∈H(B(A), 2), of a real number αh > 0 such that 
h αh = 1
and, for all (b, g) ∈A,
s(b, g) = p(b) + εψ(g)
=
#
h∈H(B(A),2) αhh(b)
$
+ εψ(g)
=
#
h∈H(B(A),2) αhh(b) + ε γ(g)
λh
$
.
Then the claim follows setting ψh = ψ/αh and observing that, for each positive
αh, ψ/αh is an ℓ-state of δ−1(⟨Rad(A)⟩).
(⇐). Conversely, for each h ∈H(B(A), 2), let αh > 0 such that 
h αh = 1
and ψh a ℓ-state of δ−1(⟨Rad(A)⟩). Then, setting for every (b, g) ∈A, s(b, g) =

h∈H(B(A),2) αh(h(b) + εψh(g)) we get
s(b, g) =
#
h∈H(B(A),2) αhh(b)
$
+ ε
#
h∈H(B(A),2) αhψh(g)
$
= p(b) + εψ(g)
where p is the faithful probability measure whose existence is endured by Propo-
sition 3, and ψ is deﬁned through ψ = 
h∈H(B(A),2) αhψh. Obviously ψ is a ℓ-state
of δ−1(⟨Rad(A)⟩) since ℓ-states are closed under convex combinations.
□
In order to understand the general relationship among Chang-states and MV-
homomorphisms between two BG-algebras, we show the following proposition:
Proposition 5. Let A, A1 ∈BG with A = B ×M G and A1 = B1 ×M1 G1 and
let f : A →A1 be an MV-algebra homomorphism. Then either f maps M to M1
or all the elements of A are mapped into Boolean elements of A1.
Proof. We know that f maps the Boolean skeleton of A into the Boolean skeleton
of A1. Suppose there is b ∈M with f(b, 0) = (b1, 0) and b1 /∈M1, and let for
every g ∈G+, f(b, g) = (b1, h(g)) (it must be h(g) ≤0). Then:
f(b, g) ⊙f(b
∗, 0) = (b1, h(g)) ⊙(f(b, 0)∗) = (b1, h(g)) ⊙(b
∗
1, 0) = (0, 0)

394
D. Diaconescu et al.
while on the other side:
f(b, g) ⊙f(b
∗, 0) = f((b, g) ⊙(b
∗, 0)) = f(0, g)
hence, for every g ∈G+, f(0, g) = (0, 0). Since Rad(A) = {(0, g) | g ∈G+}
we have that f maps Rad(A) into the element (0, 0), and Rad(A)∗into (1, 0).
Now let (b, g) ∈A with b ∈M. By Proposition 2, (b, g) = (b, 0) ⊕(0, g) hence
f(b, g) = f(b, 0) ⊕f(0, g) = f(b, 0) ∈B1. If (b, g) ∈A with b /∈M then (b, g) =
(b, 0) ⊙(1, g) and f(b, g) = f(b, 0) ⊙(1, 0) = f(b, 0) ∈B1.
□
Let A ∈BG with A = (M × G+) ∪(M × G−) and consider an MV-
homomorphisms f of A into δ(R). Then, apart from the case in which f(A) = 2,
it must be f(M) = 0 and f(M) = 1 and hence the MV-homomorphism is com-
pletely determined on its Boolean elements. This means that, in Proposition 3,
the Boolean homomorphisms h are not restrictions of Chang-states.
4
Conclusion and Future Research
We generalized the notion of state given in [7] in order to deal with inﬁnitesimal
events, continuing the work done in [5] and in [3]. We started our investigation
with a class BG of MV-algebras that can be roughly described as obtained
by adding inﬁnitesimals around Boolean elements [4], and we deﬁned states
taking values in an ultrapower of [0, 1]. We gave a characterization in terms of
homomorphisms that is not a translation of the classical case, mainly due to the
behaviour of homomorphisms of MV-algebras in BG. This is a ﬁrst step towards
the formulation of a notion of state for all non-semisimple MV-algebras, keeping
the nature of the inﬁnitesimal elements.
References
1. Belluce, P., Di Nola, A., Lettieri, A.: Local MV-algebras. Rend. Circ. Mat.
Palermo 42, 347–361 (1993)
2. Cignoli, R., D’Ottaviano, I.M.L., Mundici, D.: Algebraic Foundations of Many-
valued Reasoning. Kluwer, Dordrecht (2000)
3. Diaconescu, D., Flaminio, T., Leu¸stean, I.: Lexicographic MV-algebras and lexico-
graphic states (submitted)
4. Di Nola, A., Ferraioli, A.R., Gerla, B.: Combining Boolean algebras and ℓ-groups in
the variety generated by Chang’s MV-algebra. Mathematica Slovaca (accepted)
5. Di Nola, A., Georgescu, G., Leustean, I.: States on Perfect MV-algebras. In: No-
vak, V., Perﬁlieva, I. (eds.) Discovering the World With Fuzzy Logic. STUDFUZZ,
vol. 57, pp. 105–125. Physica, Heidelberg (2000)
6. Di Nola, A., Lettieri, A.: Perfect MV-algebras are categorically equivalent to abelian
ℓ-groups. Studia Logica 53, 417–432 (1994)
7. Mundici, D.: Averaging the truth-value in Lukasiewicz logic. Studia Logica 55(1),
113–127 (1995)
8. Mundici, D.: Advanced Lukasiewicz calculus and MV-algebras. Trends in Logic,
vol. 35. Springer (2011)
9. Paris, J.B.: The uncertain reasoner’s companion: A mathematical perspective. Cam-
bridge University Press (1994)

Accelerating Eﬀect of Attribute Variations:
Accelerated Gradual Itemsets Extraction
Amal Oudni1,2, Marie-Jeanne Lesot1,2, and Maria Rifqi3
1 Sorbonne Universit´es, UPMC Univ Paris 06, UMR 7606
LIP6, F-75005, Paris, France
2 CNRS, UMR 7606, LIP6, F-75005, Paris, France
{amal.oudni,marie-jeanne.lesot}@lip6.fr
3 Universit´e Panth´eon-Assas - Paris 02, LEMMA, F-75005, Paris, France
maria.rifqi@u-paris2.fr
Abstract. Gradual itemsets of the form “the more/less A, the more/less
B” summarize data through the description of their internal tendencies,
identiﬁed as correlation between attribute values. This paper proposes
to enrich such gradual itemsets by taking into account an acceleration
eﬀect, leading to a new type of gradual itemset of the form “the more/less
A increases, the more quickly B increases”. It proposes an interpretation
as convexity constraint imposed on the relation between A and B and a
formalization of these accelerated gradual itemsets, as well as evaluation
criteria. It illustrates the relevance of the proposed approach on real data.
Keywords: Gradual Itemset, Acceleration, Enrichment, Convexity.
1
Introduction
Information extraction can take many forms, leading to various types of knowl-
edge which are then made available to experts. This paper focuses on gradual
itemsets which can be illustrated by the example “the closer the wall, the harder
the brakes are applied”. Initially introduced in the fuzzy implication formalism
[1–3], gradual itemsets have then been interpreted as expressing constraints on
the attribute covariations. Several interpretations of the constraints have been
proposed, as regression [4], correlation of induced order [5, 6] or identiﬁcation of
compatible object subsets [7, 8]. Each interpretation is associated with the deﬁ-
nition of a support to quantify the validity of gradual itemsets and to methods
for the identiﬁcation of the itemsets that are frequent according to these support
deﬁnitions.
Furthermore, several types of enrichments have been proposed: in the case of
categorical or fuzzy data clauses, clauses linguistically introduced by the expres-
sion “all the more” lead to so-called strengthened gradual itemsets [9]. They can
be illustrated by an example such as “the closer the wall, the harder the brakes
are applied, all the more the higher the speed”. For numerical data, an enrich-
ment by characterization clauses [10] adds a clause linguistically introduced by
the expression “especially if”: characterized gradual itemsets can be illustrated
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 395–404, 2014.
© Springer International Publishing Switzerland 2014

396
A. Oudni, M.-J. Lesot, and M. Rifqi
Fig. 1. Two data sets, leading to “the more A, the more B” where an acceleration
eﬀect is observed for the right data set and not for the left one
by a sentence as “the closer the wall, the harder the brakes are applied, especially
if the distance to the wall ∈[0, 50]m”.
In this paper, we consider a new type of enrichment in the case of numerical
data, to capture a new type of information: the aim is to express how fast
the values of some attributes vary as compared to others, as illustrated by the
two data sets represented in Figure 1. In both cases, a covariation constraint is
satisﬁed, which justiﬁes the extraction of the gradual itemset “the more A, the
more B”. However, on the right-hand example, the speed of B augmentation
appears to increase, making it possible to enrich the gradual itemset to “the
more A increases, the more quickly B increases”.
This paper addresses the task of extracting such accelerated gradual itemsets.
The principle of acceleration is naturally understood as speed variation increase,
which can be translated as a convexity constraint on the underlying function
associating the considered attributes. This constraint can be modelled as an
additional covariation constraint, leading to the deﬁnition of a criterion called
accelerated support to assess the validity of such accelerated gradual itemsets.
The paper is organized as follows: Section 2 recalls the formalism of gradual
itemsets and details the existing types of enrichment. Section 3 discusses the pro-
posed interpretation of accelerated gradual itemsets and its formalization. Sec-
tion 4 deﬁnes the criteria proposed for the evaluation of this new type of itemsets.
Section 5 illustrates and analyses the experimental results obtained on real data.
2
Typology of Gradual Itemset Enrichments
This section ﬁrst recalls the notations and deﬁnitions of gradual items and item-
sets [9, 8] as well as the support deﬁnition based on compatible data subsets [8].
It then describes the existing enrichments of gradual itemsets.
2.1
Gradual Itemset Deﬁnitions
Let D denote the data set. A gradual item A∗is made of an attribute A and a
variation ∗∈{≥, ≤}, which represents a comparison operator. A gradual itemset

Accelerated Gradual Itemsets
397
is then deﬁned as a set of gradual items M = {(Aj, ∗j), j = 1..k}, interpreted
as their conjunction. It induces a pre-order, ⪯M, deﬁned as o ⪯M o′ iﬃ∀j ∈
[1, k] Aj(o)∗jAj(o′) where Aj(o) represents the value of attribute Aj for object o.
As brieﬂy recalled in the introduction, there exists several interpretations
of gradual itemsets [1–8]. In this paper, we consider the interpretation of co-
variation constraint by identiﬁcation of compatible subsets [7, 8]: it consists in
identifying subsets D of D, called paths, that can be ordered so that all data pairs
of D satisfy the pre-order induced by the considered itemset. More formally, for
an itemset M = {(Aj, ∗j), j = 1..k}, D = {o1, ..., om} ⊆D is a path if and only
if there exists a permutation π such that ∀l ∈[1, m −1], oπl ⪯M oπl+1. Gradual
itemsets thus depend on the order induced by the attribute values, not on the
values themselves.
Such a path is called complete if no object can be added to it without violating
the order constraint imposed by M. L(M) denotes the set of complete paths
associated to M. The set of maximal complete paths, i.e. complete paths of
maximal length, is denoted L∗(M) = {D ∈L(M)/∀D′ ∈L(M) |D| ≥|D′|}.
The gradual support of M, GSD(M), is then deﬁned as the length of its
maximal complete paths divided by the total number of objects [7]:
GSD(M) =
1
|D|
max
D∈L(M) |D|
(1)
2.2
Existing Enrichments
Two enrichment types for gradual itemsets have been proposed, namely through
characterization [10] and strengthening [9]. Both are based on a principle of
increased validity when the data are restricted to a subset: the gradual support
of the considered itemset must increase when it is computed on the data subset
only.
More precisely, in the case of characterization [10], the restriction is deﬁned
as a set of intervals: characterized gradual itemsets are linguistically of the form
“the more/less A, the more/less B, especially if
J ∈R”, where J is a set of
attributes belonging to A∪B and R is a set of intervals deﬁned for each attribute
in J. R deﬁnes the data subset, it applies only in the case of numerical data.
In the strengthening case [9], the restriction is deﬁned by a presence, pos-
sibly in a fuzzy weighted way, of values required by the strengthening clause:
the (fuzzy) data subset only contains objects possessing the required values.
Strengthened gradual itemsets are linguistically of the form “the more/less A,
the more/less B, all the more C”, where C is the strengthening clause that con-
sists of values of categorical attributes or fuzzy modalities of fuzzy attributes.
As opposed to the existing enrichments, the peculiarities of the proposed
enrichment are mentioned in the following section.
2.3
Characteristics of the Proposed Acceleration Enrichment
The main diﬃerence between accelerated gradual itemsets and the previous grad-
ual itemset enrichments comes from the nature of the additional clause: both for

398
A. Oudni, M.-J. Lesot, and M. Rifqi
characterization and strengthening the enriching clause has a presence seman-
tics, insofar as the additional constraint leads to a data restriction deﬁned by
the presence of speciﬁc values (in the interval R or in the clause C) on which
the itemset validity must increase. On the contrary, as detailed in the next sec-
tions, the semantics of the acceleration clause is gradual, depending not on the
attribute values but on the order they induce. It thus has the same nature as
the considered itemset.
It must be underlined that the accelerated gradual itemsets apply to numerical
data, excluding the categorical case.
3
Formalization of Accelerated Gradual Itemsets
This section presents the interpretation and the principle of gradual itemset
acceleration, as well as the proposed formalization.
3.1
Principle of Accelerated Gradual Itemsets
As already mentioned in the introduction, Figure 1 represents two data sets with
the same cardinality described by two attributes, A (x-axis) and B (y-axis). In
both cases, the data sets lead to the same gradual itemset M = A≥B≥supported
by all data points: the gradual support is 100% in both cases. Now it can be
noticed that the covariation between A and B is diﬃerent: an acceleration eﬃect
of B values with respect A values can be observed for the right data set, whereas
it does not hold for the left-hand data set.
Accelerated gradual itemsets aim at capturing this diﬃerence. It must be un-
derlined that it breaks the symmetry property, distinguishing the cases “the
more A, the more quickly B” and “the more B, the more quickly A”, whereas
the gradual itemset is “the more A, the more B” in both cases.
Mathematically, the acceleration eﬃect corresponds to a convexity property
of the function that associates B values to A values, imposing that its graph
is “turned up” as illustrated on the right part of Figure 1, meaning that the
line segment between any two points on the graph of the function lies above the
graph. Convex growth means “increasing at an increasing rate (but not necessar-
ily proportionally to current value)” which is equivalent to desired acceleration
eﬃect. Diﬃerentiable functions are convex if and only if their derivative is mono-
tonically non-decreasing.
Now, data sets from which accelerated gradual itemsets must be extracted
do not give access to the mathematical function relating A and B values, hence
its derivative cannot be computed. Therefore we propose to consider a rough
discretization, deﬁned as the quotient of the successive diﬃerences ΔB
ΔA
when
data are ordered with respect to their A values.
We thus propose to interpret the acceleration eﬃect as an increase of
 ΔB
ΔA

. It
must be noticed that this interpretation does not take into account the shape of
the convex function: for instance no diﬃerence is made whether the underlying
function is quadratic or exponential.

Accelerated Gradual Itemsets
399
Fig. 2. A≥B≥ ΔB
ΔA
≤with deceleration eﬀect
3.2
Formalization
To address the principle presented in the previous section, we propose to formal-
ize an accelerated gradual itemset as a triplet: A∗1B∗2  ΔB
ΔA
∗3, where A∗1B∗2
represents a gradual itemset, and
 ΔB
ΔA
∗3 represents the acceleration clause that
compares the variations of B with that of A. ∗1 determines whether “the more
A increases” (∗1 =≥) or “the more A decreases” (∗1 =≤). ∗2 plays the same role
for B. ∗3 determines whether acceleration or deceleration is considered: ∗3 =≥
leads to “the more quickly” and ∗3 =≤leads to “the less quickly” or equivalently
“the more slowly” .
This paper focuses on the acceleration eﬃect, i.e. attributes for which values
increase “quickly”, i.e. the case ∗3 =≥. It corresponds to the convex curve case.
The case where ∗3 =
≤corresponds to a deceleration eﬃect, as illustrated on
Figure 2, which can be described as “the more A, the more slowly B increases”.
It can be noticed that this is equivalent to “the more B increases, the more
quickly A increases”, i.e. A≥B≥ ΔA
ΔB
≥. Thus considering only ∗3 =≥is not a
limitation.
3.3
Generalization
The previous deﬁnition focuses on the case of itemsets containing two attributes.
In the general case, the itemset to enrich may be composed of several attributes,
as well as the acceleration clause.
Now the notion of convex function is also mathematically deﬁned for func-
tions depending on several variables, based on properties of their Hessian ma-
trices. Similarly, a discretization based on the available data may be computed
for a given data set, leading to accelerated gradual itemsets made on several
attributes, which may be written schematically M1M2
δM2
δM1
.

400
A. Oudni, M.-J. Lesot, and M. Rifqi
4
Evaluation Criterion of the Acceleration Eﬀect
An accelerated gradual itemset contains two components, the classical gradual
itemset M = A∗1B∗2 and the acceleration clause Ma =
 ΔB
ΔA
∗3. It must there-
fore be evaluated according to these two components. Its quality is measured
both by the classical gradual support as recalled in Equation (1) and an accel-
erated gradual support that measures the quality of the acceleration, as deﬁned
below.
4.1
Order Induced by the Acceleration Clause
The itemset M induces a pre-order on objects as deﬁned in Section 2; the accel-
eration clause
 ΔB
ΔA
∗3 induces a pre-order on pairs of objects denoted ⪯Ma: for
any o1, o2, o3 and o4
(o1, o2) ⪯Ma (o3, o4) ⇔B(o2) −B(o1)
A(o2) −A(o1) ∗3
B(o4) −B(o3)
A(o4) −A(o3) .
(2)
where A(o) and B(o) respectively represent the value of attributes A and B
for object o.
4.2
Deﬁnition of the Accelerated Support
The quality of the candidate accelerated gradual itemset MMa is high if there
exists a subset of data that simultaneously satisﬁes the order induced by M and
that induced by Ma. Therefore the acceleration quality ﬁrst requires to identify a
data subset that satisﬁes ⪯M. To that aim, the GRITE algorithm [7] can be used
to identify candidate gradual itemsets as well as their set of maximal complete
support paths L∗(M).
For any D ∈L∗(M), the computation of the accelerated support then con-
sists ﬁrst in identifying subsets of D so that the constraint
 ΔB
ΔA
∗3 is veriﬁed
simultaneously.
We denote Σ the function that identiﬁes a maximal subset of objects from D
such that
∀o1, o2, o3 ∈Σ(D), (o1 ⪯M o2 ⪯M o3 ⇒(o1, o2) ⪯Ma (o2, o3))
The accelerated gradual support of MMa is then computed as:
GSa =
1
|D| −1
max
D∈L∗(M)|Σ(D)|
(3)
where |D| denotes the size of any maximal complete path in L∗(M), as, by
deﬁnition of L∗(M), they all have the same size. |D| −1 is then the maximal
possible value of Σ(D) and thus the normalizing factor. Indeed,
 ΔB
ΔA
∗3 does not
have the same deﬁnition set as classical gradual itemsets: it applies to pairs of
successive objects.

Accelerated Gradual Itemsets
401
(a) A≥B≥ ΔB
ΔA
≥with high GS.
(b) A≥B≥ ΔB
ΔA
≥with low GS.
Fig. 3. Two data sets for which A≥B≥ ΔB
ΔA
≥holds with diﬀerent GS and the same
GSa = 100%
Combination of the Quality Criterion. The classical validity deﬁnition is
then extended to integrate the condition on GSa: an accelerated gradual itemset
MMa is valid if GS ≥s and GSa(MMa) ≥sa where sa is a threshold for the
accelerated gradual support and s the threshold of classical gradual support. It
is worth noticing that both GS and GSa are necessary to assess the quality of an
accelerated gradual itemset. Figure 3 illustrates the case of two datasets leading
to the same GSa = 100% but with diﬃerent GS: GS equals 45% for the data set
on the left and 22% on the right. Indeed GSa is computed relatively to the path
size whereas GS takes into account the total number of points. When combining
the two components, a priority is given to GS: for a given GS level, accelerated
gradual itemsets are compared in terms of GSa.
Computational Cost. The computational time of the extraction of accelerated
gradual itemsets depends on the number of objects and attributes of the data set,
as well as on the gradual support threshold. The experiments described in the
next section show that the most expensive step corresponds to the extraction of
the basic gradual itemsets and that the step of acceleration clause identiﬁcation
only adds a much smaller computational cost. More precisely, 85% of the total
time necessary for the extraction is used in the step of the basic gradual itemset
extraction and only 15% is used in the step of the acceleration clause extraction.
5
Experimental Study
This section describes the experiments carried out using the proposed method
of accelerated gradual itemset extraction on a real data set. The analysis of the
results is based on the number of extracted gradual itemsets and their quality.
5.1
Considered Data
We
use
a
real
data
set
called
weather
downloaded
from
the
site
http://www.meteo-paris.com/ile-de-france/station-meteo-paris/pro:

402
A. Oudni, M.-J. Lesot, and M. Rifqi
these data come from the Parisian weather station of St-Germain-des-Pr´es.
They contain 2164 meteorological observations realized during eight days
(December 20th to 27th 2013), described by 8 numerical attributes: temperature
(℃), wind chill (℃), wind run (km), rain (mm), outside humidity (%), pressure
(hPa), wind speed (km/hr) and wind gusts measured as high speed (km/hr).
5.2
Results: Extracted Itemsets
Setting a gradual support threshold s = 20%, 153 gradual itemsets are extracted,
two of them with 100%. Figure 4 represents the accelerated gradual support of
all identiﬁed gradual itemsets. It can be observed that itemsets with GSa below
20% are not numerous and almost 30% have GSa above 50%. When setting
the accelerated support threshold sa = 20%, represented by the horizontal line
on Figure 4, 130 itemsets are considered as enriched by an acceleration clause,
which corresponds to more than 85%.
According to the criteria combination with priority discussed in the previous
section, the most interesting accelerated gradual itemset is then the one cor-
responding to point A on the graph. It represents the itemset “the more the
wind speed increases, the more quickly the wind run increases: GS = 100%
and GSa = 90%”. This corresponds to an expected result from the proposed
deﬁnition of accelerated itemsets: the underlying linear relation between these
two attributes corresponds to the limit case of acceleration and thus gets a high
accelerated support.
The next most interesting itemsets are then the two points in region B in the
graph, that respectively correspond to the itemsets
– the more the temperature decreases, the more quickly the rain accumulation
increases: GS = 100% and GSa = 32%.
– the more the humidity decreases, the more quickly the temperature increases
GS = 94.73%, GSa = 34%.
The middle points in region C in the graph show a trade-oﬃbetween GS and
GSa. The two ones with highest GSa correspond to
– the more the wind gusts increase, the more quickly the wind run increases:
GS = 54.9% and GSa = 51%.
– the more the wind gusts increase, the more quickly the wind speed increases:
GS = 57.3% and GSa = 48%.
Finally, it can be observed that the majority of extracted gradual itemsets
have a gradual support slightly above the threshold 20%, many of them reaching
a high accelerated support. Examples with highest GSa in region D in the graph,
include
– the more the humidity increases, the more quickly the wind run increases:
GS = 22.69% and GSa = 81%.
– the more the pressure decreases, the more quickly the humidity increases:
GS = 20.93% and GSa = 88%.

Accelerated Gradual Itemsets
403
Fig. 4. Gradual support and accelerated gradual support, for each of the 153 extracted
gradual itemsets
– the more the wind chill increases, the more quickly the temperature increases:
GS = 22.88% and GSa = 86%.
It is also interesting to look at an example without accelerating eﬃect: the
gradual itemset represented by point E corresponds to
– the more the rain accumulation decreases, the more quickly the wind chill
increases: GS = 94.72% and GSa = 10%.
Accelerated gradual itemsets thus make it possible to extract rich meteoro-
logical knowledge from the individual weather station observations.
6
Conclusion and Future Work
In this paper we proposed an approach to enrich gradual itemsets, using an
acceleration clause linguistically expressed by the expression “quickly”, so as
to extract more information summarizing data sets. The extraction of these
accelerated gradual itemsets relies on the identiﬁcation of attributes occurring in
the considered gradual itemset for which the speed increase augments compared
with other attributes values. The constraint is interpreted in terms of convexity
and leads to the deﬁnition of a quality criterion to evaluate the acceleration
eﬃect.
Ongoing works include complementary experimentations taking into account
both computation eﬀciency (time and memory) and use of other real data where
expert advice can be given on the understanding and interest of extracted ac-
celerated gradual itemsets.
Future works also include the combination of the acceleration eﬃect with other
enrichment principles, applied to the acceleration clauses: it would be interesting
to identify restriction of the data sets on which the acceleration eﬃect particularly
holds. In particular, in the case of meteorological data, restriction induced by

404
A. Oudni, M.-J. Lesot, and M. Rifqi
a temporal attribute, or by categorical attributes derived from the date, could
make it possible to identify accelerated gradual itemsets of the form “the more
the temperature increases, the more quickly the rain accumulation decreases, in
the summer”. Besides, characterization could also allow to remove the ambiguity
that may exist when an itemset is extracted with an acceleration and deceleration
eﬃect at the same time: a characterization clause would make it possible to
identify the subsets of the data where they respectively hold.
References
1. Galichet, S., Dubois, D., Prade, H.: Imprecise speciﬁcation of illknown functions
using gradual rules. Int. Journal of Approximate Reasoning 35, 205–222 (2004)
2. H¨ullermeier, E.: Implication-based fuzzy association rules. In: Siebes, A., De Raedt,
L. (eds.) PKDD 2001. LNCS (LNAI), vol. 2168, pp. 241–252. Springer, Heidelberg
(2001)
3. Dubois, D., Prade, H.: Gradual inference rules in approximate reasoning. In: Proc.
of the Int. Conf. on Fuzzy Systems, vol. 61, pp. 103–122 (1992)
4. H¨ullermeier, E.: Association rules for expressing gradual dependencies. In: Elomaa,
T., Mannila, H., Toivonen, H. (eds.) PKDD 2002. LNCS (LNAI), vol. 2431, pp.
200–211. Springer, Heidelberg (2002)
5. Berzal, F., Cubero, J.C., Sanchez, D., Vila, M.A., Serrano, J.M.: An alternative
approach to discover gradual dependencies. Int. Journal of Uncertainty, Fuzziness
and Knowledge-Based Systems 15, 559–570 (2007)
6. Laurent, A., Lesot, M.-J., Rifqi, M.: GRAANK: Exploiting rank correlations for ex-
tracting gradual itemsets. In: Andreasen, T., Yager, R.R., Bulskov, H., Christiansen,
H., Larsen, H.L. (eds.) FQAS 2009. LNCS, vol. 5822, pp. 382–393. Springer, Hei-
delberg (2009)
7. Di Jorio, L., Laurent, A., Teisseire, M.: Fast extraction of gradual association rules:
a heuristic based method. In: Proc. of the 5th Int. Conf. on Soft Computing as
Transdisciplinary Science and Technology, pp. 205–210 (2008)
8. Di-Jorio, L., Laurent, A., Teisseire, M.: Mining frequent gradual itemsets from large
databases. In: Adams, N.M., Robardet, C., Siebes, A., Boulicaut, J.-F. (eds.) IDA
2009. LNCS, vol. 5772, pp. 297–308. Springer, Heidelberg (2009)
9. Bouchon-Meunier, B., Laurent, A., Lesot, M.-J., Rifqi, M.: Strengthening fuzzy
gradual rules through “all the more” clauses. In: Proc. of the Int. Conf. on Fuzzy
Systems, pp. 1–7 (2010)
10. Oudni, A., Lesot, M.-J., Rifqi, M.: Characterisation of gradual itemsets through
“especially if” clauses based on mathematical morphology tools. In: EUSFLAT, pp.
826–833 (2013)

Gradual Linguistic Summaries
Anna Wilbik and Uzay Kaymak
Information Systems
School of Industrial Engineering
Eindhoven University of Technology
Eindhoven, The Netherlands
{A.M.Wilbik,U.Kaymak}@tue.nl
Abstract. In this paper we propose a new type of protoform-based lin-
guistic summary – the gradual summary. This new type of summaries
aims in capturing the change over some time span. Such summaries can
be useful in many domains, for instance in economics, e.g., “prices of
X are getting smaller”, in eldercare, e.g., “resident Y is getting less ac-
tive”, in managing production, e.g. “production is dropping” or “delays
in deliveries are getting smaller”.
Keywords: linguistic summaries, fuzzy logic, computing with words,
protoforms.
1
Introduction
Recent increasing attention in the ﬁeld of big data, means that more and more
data are created and stored and need to be analyzed. However very often the
amount of available data is beyond human cognitive capabilities and comprehen-
sion skills. Acknowledging this problem, creating summaries of data has been
goal of the artiﬁcial intelligence and computational intelligence community for
many years. This was especially visible in the context of written texts. However
also other data like images and sensor data were summarized.
In this paper we are dealing with linguistic summaries of numerical data.
This topic has been widely investigated, cf. [4,16,17,24]. For instance, Dubois
and Prade [4] proposed representation and reasoning for gradual inference rules
for linguistic summarization in the form “the more X is F, the more/ the less
Y is G” that could summarize various relationships. Such rules expressed a
progressive change of the degree to which the entity Y satisﬁes the gradual
property G when the degree to which the entity X satisﬁes the gradual property
F is modiﬁed. Rasmussen and Yager [17] discussed the beneﬁt of using fuzzy sets
in data summaries based on generalized association rules. Gradual functional
dependences were investigated, and a query language called SummarySQL was
proposed. Bosc et al. [1] discussed the use of fuzzy cardinalities for linguistic
summarization. The SAINTETIQ model [16] provides the user synthetic views of
groups of tuples over the database. In [18], the authors proposed a summarization
procedure to describe long-term trends of change in human behavior, e.g., “the
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 405–413, 2014.
c
⃝Springer International Publishing Switzerland 2014

406
A. Wilbik and U. Kaymak
quality of the ‘wake up’ behavior has been decreasing in the last month” or “the
quality of the ‘morning routine’ is constant but has been highly unstable in the
last month.”
In this paper we follow the approach of Yager [24], in the form “Q objects
in Y are S,”. This approach was considerably advanced and implemented by
Kacprzyk [5], Kacprzyk and Yager [12], Kacprzyk et al. [13,15,14]. Those sum-
maries have been applied in diﬃerent areas, e.g. in ﬁnancial data [10,11,2,3],
eldercare [23,22,21]. However we notice that they describe more static situation,
and even if dealing with time series, they do not focus on change in time.
In the paper, inspired by gradual inference rules by Dubois and Prade [4], we
propose a new type of protoform-based linguistic summary, that we call gradual
linguistic summary. This new type of summaries aims in capturing the change in
time. Such summaries can be useful in many domains, for instance in economics,
e.g., “prices of X are getting smaller”, in eldercare, e.g., “resident Y is getting
less active”, in managing production, e.g. “production is dropping” or “problems
are solved faster”.
2
Gradual Linguistic Summaries
Linguistic summaries are protoform [26] (template-) based quasi-natural lan-
guage sentences that capture the information hidden in data. Yager [24] proposed
two protoforms, simple and extended one, which can be written, respectively as
Qy’s are P
QRy’s are P.
(1)
Q is a linguistic quantiﬁer, P is the summarizer, R is the qualiﬁer and y’s are
the objects that are to be summarized. Every sentence may be evaluated using
diﬃerent criteria, however the most important and basic one is the degree of
truth (T ) which is the degree to which the summary is valid. The truth value,
indication of how compatible it is with the database, is calculated using Zadeh’s
calculus of quantiﬁed propositions [25] and hence with the following formulas:
T (Qy’s are P) = μQ

1
n
n

i=1
μP (yi)

(2)
T (QRy’s are P) = μQ
n
i=1 μP (yi) ∧μR(yi)
n
i=1 μR(yi)
	
(3)
This approach turned out to be useful e.g. in small retail [9,8] or elder-
care [23,22,21]. Moreover the protoforms were modiﬁed to ﬁt other type of data,
e.g. time series [10,11,2], texts [19].
The drawback of the above mentioned solution is that they describe somewhat
static situation, and do not capture the change very well. To overcome this
drawback, we introduce a new type of linguistic summaries, namely gradual
summaries.

Gradual Linguistic Summaries
407
Gradual linguistic summaries are deﬁned with the following protoform
y’s are getting PG
(4)
where y’s are the objects that are summarized and PG is gradual summarizer,
e.g. faster. Examples of such summaries are “resident Y is getting less active”,
“problems are solved faster”, “the price is getting smaller”.
Naturally it is possible to extend the above summary by adding a qualiﬁer,
e.g. “problems of high impact are solved faster”, “the prices of luxiorious goods
are getting higher”. However we won’t consider this case in this paper.
With the gradual summary we wish to capture the decreasing (or increasing)
trend within the data points, assuming that we consider a single variable at a
time. One option could be to use some kind of linear approximation, however we
would like to have a high truth value also in case of nonlinear trends. Therefore
we assume for our purpose that the decreasing trend is present when in most
cases points are smaller than the previous ones.
We evaluate the degree to which most points fulﬁll the property that in most
cases data point has a smaller value than any previous point and bigger value
than any following point from some speciﬁed neighborhood of the point. The
neighborhood we deﬁne with two parameters k1 and k2, k1 > k2. For a point yi
generally are the points that have at most k1 and no less than k2 points between
with point yi, i.e. {yi−k1, yi−k1−1, . . . , yi−k2, yi+k2, yi+k2+1, . . . , yi+k1}. To avoid
a situation when point has no known neighborhood we add a condition, that all
points except the ﬁrst and last point have at least one neighbor preceding and
one following, and if there are no points in the neighborhood we consider then
ﬁrst or last data point as the neighborhood. Then, the truth value is calculated
as
T = μmost

1
n
F n

i=1
μmost(di)
G:
(5)
di =
max(i−k2,0)
j=max(i−k1,0) μd (yj −yi) + min(i+k1,n)
j=min(i+k2,n) μd (yi −yj)
min(i −k2, k1 −k2) + min(n −k2, k1 −k2)
(6)
where μmost is the membership function of the quantiﬁer most. In this formula
two diﬃerent quantiﬁers may be used, however we decided to keep it simple and
use the same quantiﬁer. n is the number of points in the data set. yi is the value
of i-th data point, and we assume that the points are ordered with respect to
the time. μd (yj −yi) is the degree to which value of yj is bigger than value of
yi.
For every point we calculate ﬁrst the number of preceding points from the
neighborhood that are bigger and following that point that are smaller. For this
purpose we use the Σ-count. Next we divide this number by the number of
considered neighbors (min(i −k2, k1 −k2) + min(n −k2, k1 −k2)) to normalize
it, and this value is denoted as di. Then we compute the degree to which this
proportion is “most”. This value can be understood as the degree to which the

408
A. Wilbik and U. Kaymak
point is “in trend” within a given neighborhood. We compute those degrees of
being “in trend” for each point, and then we aggregate them using quantiﬁer
based aggregation, since we wish to have most points “in trend”. Note some
similarity with the soft degree of consensus [6,7] that is a degree to which most
important experts agree on most important issues.
3
Examples
The method was tested on several artiﬁcial data sets as well as on real data. In
this section we will present the results shortly.
3.1
Artiﬁcial Examples
We generated 6 sets of data, each containing 100 points. We introduced also the
noise by adding a randomly chosen value from normal distribution N(0,1).
The ﬁrst data set is a set of uniformly decreasing points from 100 to 1, They
can be described by a formula y = 100 −x. Second data set was generated
with the equation y = 100 ∗e( −x
20 ) for x=1:100. The third data set contains
constant trend. In the fourth case, half of the points are decreasing uniformly,
and the next half is increasing to create a “V” pattern. The ﬁfth data set contains
points with a decreasing pattern sawtooth, if x is odd y(x) = y(x −1) −2 else
y(x) = y(x −1) + 0.5. The last, sixth data set can be described as short quickly
decreasing trend, that is followed by long slowly increasing trend, and ﬁnished
again by short quickly increasing trend. All the data sets with added noise are
shown in Figure 1.
For those data set we compute the truth value of the sentence “the values
are getting smaller” with the two methods described above. Those methods
require deﬁning the linguistic quantiﬁer most, which is in our case always deﬁned
with trapezoidal membership function as μmost = T rap [0.3, 0.8, 1, 1]. We deﬁne
notions “value a is bigger than b” and “value b is smaller than a” as a fuzzy set
of value a −b with the following membership value μd = T rap [−0.5, 0.5, ∞, ∞].
We varied also the size of the neighborhood and used following values of
parameters:
– k1 = 100 and k2=1, 30 or 99
– k1 = 30 and k2=1, 10 or 29
– k1 = 10 and k2=1, 5 or 9
– k1 = 2 and k2 = 1
For the data set 1 without and with noise and in all cases the truth value
was equal to 1. Same results were also obtained for the data set 2 without noise.
When noise was added the truth value was equal to 1, except for the case when
k1 = 2 and k2 = 1, in which the truth value was equal to 0.86, so also a high
value. In case of data set 3 without noise all values of the truth were equal to
0.2. In case with the noise, all truth values were from range 0.17 – 0.35, except
for one extreme case for k1 = 100 and k2 = 99, in which only ﬁrst and last point

Gradual Linguistic Summaries
409
0
10
20
30
40
50
60
70
80
90
100
−20
0
20
40
60
80
100
(a) Data set 1
0
10
20
30
40
50
60
70
80
90
100
−20
0
20
40
60
80
100
(b) Data set 2
0
10
20
30
40
50
60
70
80
90
100
−20
0
20
40
60
80
100
(c) Data set 3
0
10
20
30
40
50
60
70
80
90
100
−20
0
20
40
60
80
100
(d) Data set 4
0
10
20
30
40
50
60
70
80
90
100
−20
0
20
40
60
80
100
(e) Data set 5
0
10
20
30
40
50
60
70
80
90
100
−20
0
20
40
60
80
100
(f) Data set 6
Fig. 1. Artiﬁcial data sets used in the example

410
A. Wilbik and U. Kaymak
were taken into consideration. In this case the truth value is equal to 1, because
in this dataset it turned out that ﬁrst value is higher than the last one. In case
of data set 4, all the truth values were low from range 0.17 – 0.44, no matter if
noise or no noise was present. For the data set 5 without and with noise in all
cases the truth value was equal to 1.
Set 6 is an interesting, tricky case where the results vary a lot depending on
the size of the neighborhood. We show the exact values in Tab 1.
Table 1. Truth values for the data set 6 with and without noise for diﬀerent sizes of
neighborhood
neighborhood
no noise with noise
k1 = 100 and k2 = 1
0.33
0.48
k1 = 100 and k2 = 30
0.61
0.66
k1 = 100 and k2 = 99
1
1
k1 = 30 and k2 = 1
0.29
0.43
k1 = 30 and k2 = 10
0.28
0.41
k1 = 30 and k2 = 29
0.4
0.48
k1 = 10 and k2 = 1
0.3
0.48
k1 = 10 and k2 = 5
0.31
0.56
k1 = 10 and k2 = 9
0.32
0.49
k1 = 2 and k2 = 1
0.52
0.60
In this case we obtain very big value of truth value equal 1 just in one case
for k1 = 100 and k2 = 99, i.e if we consider just the ﬁrst and last data point.
We obtain moderate values for 2 neighborhoods when k1 = 100 and k2 = 30,
so distant quite big neighborhood, and k1 = 2 and k2 = 1, so very small close
neighborhood. For the other neighborhoods the truth values are small.
Generally, for the data sets 1, 2 and 5 we have received very high truth values,
therefore the summary “the values are getting smaller” is valid. For the data set
3 and 4 we have received small values of the truth value, and hence the above
summary is not valid. In case of set 6 situation is not clear, although in many
cases human perception would towards assuming that “the values are getting
smaller”. Also the truth value for diﬃerent neighborhoods were not decisive, in
many case close to 0.5. All those results agree with human intuition.
3.2
Real Example
The real example comes from Volvo IT Belgium, published in [20]. The data is
an event log, that contains events from an incident and problem management
system called VINST. In this example we analyze only the sets that contain
closed problems. There are 6660 events in the log that describe 1487 cases. Each
event is described by several attributes such as problem number, its status and
sub-status, time stamp and many others.
In our simple case we are only interested how long the problems were solved.
Especially we would like to compute the truth of the sentence “the problems are
solved faster”.

Gradual Linguistic Summaries
411
2006
2007
2008
2009
2010
2011
2012
2013
0
500
1000
1500
2000
2500
date of registering the problem
time how long the problem was solved (in days)
Fig. 2. Volvo data set – time of solving the problem (in days) vs the ﬁrst entry in the
event log of that problem
In the Figure 2 we plot the time of solving the problem vs the ﬁrst entry in
the event log of that problem. From this ﬁgure it is clearly that the analyzed
summary is true.
Here we deﬁned “value a is solved faster than b” as a fuzzy set of value a −b
with the following membership value μd = T rap [−5, 5, ∞, ∞]. We assume that
if the diﬃerence in time needed to solve a problem is bigger than a week (5 days)
then the problem is solved deﬁnitelly faster (with membership degree equal 1).
This deﬁnition was chosen arbitrary. Most was deﬁned in the same way as in
previous case.
Table 2. Truth values for the Volvo IT data set for diﬀerent sizes of neighborhood
neighborhood
truth value
k1 = 100 and k2 = 1
0.77
k1 = 100 and k2 = 50
0.86
k1 = 100 and k2 = 70
0.89
k1 = 50 and k2 = 1
0.62
k1 = 50 and k2 = 10
0.65
k1 = 50 and k2 = 30
0.71
k1 = 10 and k2 = 1
0.37
k1 = 10 and k2 = 5
0.41
k1 = 5 and k2 = 1
0.32
k1 = 2 and k2 = 1
0.30

412
A. Wilbik and U. Kaymak
In this case we were also changing the size of the neighborhood. For k1 equal
2000, 1000, 700, 500, 300 for any value of k2 we always received the truth value
of 1. The results for k1 equal 100, 50, 10, 5 and 2 are shown in Tab. 2 and are
smaller than 1.
Here we may notice that as we decrease neighborhood by decreasing the value
of k1 the truth values are getting smaller. However if we increase k2 we limit
the inﬂuence of noise of close observations and in this case the truth value is
increasing.
4
Concluding Remarks
We considered a new type of protoform-based linguistic summary, namely the
gradual summary. They may be exempliﬁed by ‘prices of X are getting smaller”,
“resident Y is getting more active”, “sale is dropping” or “share in the market
is getting smaller”. Such summaries focus on the change over the time and try
to describe the trend within the data. We proposed a way how to evaluate the
truth value of such sentences. Further work will focus on investigating more
complex gradual summaries, like e.g. “sale of expensive products is dropping”,
where expensive may be modeled as a fuzzy set.
References
1. Bosc, P., Dubois, D., Pivert, O., Prade, H., Calmes, M.D.: Fuzzy summarization
of data using fuzzy cardinalities. In: Proceedings of the IPMU 2002 Conference,
pp. 1553–1559 (2002)
2. Castillo-Ortega, R., Mar´ın, N., S´anchez, D.: Time series comparison using linguistic
fuzzy techniques. In: H¨ullermeier, E., Kruse, R., Hoﬀmann, F. (eds.) IPMU 2010.
LNCS, vol. 6178, pp. 330–339. Springer, Heidelberg (2010)
3. Castillo-Ortega, R., Mar´ın, N., S´anchez, D.: Linguistic local change comparison of
time series. In: 2011 IEEE International Conference on Fuzzy Systems (FUZZ),
pp. 2909–2915 (June 2011)
4. Dubois, D., Prade, H.: Gradual rules in approximate reasoning. Information Sci-
ences 61, 103–122 (1992)
5. Kacprzyk, J.: Intelligent data analysis via linguistic data summaries: a fuzzy logic
approach. In: Decker, R., Gaul, W. (eds.) Classiﬁcation and Information Processing
at the Turn of Millennium, pp. 153–161. Springer, Heidelberg (2000)
6. Kacprzyk, J., Fedrizzi, M.: “Soft” consensus measures for monitoring real consensus
reaching processes under fuzzy preferences. Control and Cybernetics 15, 309–323
(1986)
7. Kacprzyk, J., Fedrizzi, M.: A ’human-consistent‘ degree of consensus based on
fuzzy logic with linguistic quantiﬁers. Mathematical Social Sciences 18, 275–290
(1989)
8. Kacprzyk, J., Strykowski, P.: Linguistic data summaries for intelligent decision
support. In: Felix, R. (ed.) Proceedings of EFDAN 1999-4th European Workshop
on Fuzzy Decision Analysis and Recognition Technology for Management, pp. 3–12
(1999)

Gradual Linguistic Summaries
413
9. Kacprzyk, J., Strykowski, P.: Linguistic summaries of sales data at a computer
retailer: a case study. In: Proceedings of IFSA 1999, vol. 1, pp. 29–33 (1999)
10. Kacprzyk, J., Wilbik, A., Zadro˙zny, S.: Linguistic summarization of time series
using a fuzzy quantiﬁer driven aggregation. Fuzzy Sets and Systems 159(12), 1485–
1499 (2008)
11. Kacprzyk, J., Wilbik, A., Zadro˙zny, S.: An approach to the linguistic summariza-
tion of time series using a fuzzy quantiﬁer driven aggregation. International Journal
of Intelligent Systems 25(5), 411–439 (2010)
12. Kacprzyk, J., Yager, R.R.: Linguistic summaries of data using fuzzy logic. Inter-
national Journal of General Systems 30, 33–154 (2001)
13. Kacprzyk, J., Yager, R.R., Zadro˙zny, S.: A fuzzy logic based approach to linguis-
tic summaries of databases. International Journal of Applied Mathematics and
Computer Science 10, 813–834 (2000)
14. Kacprzyk, J., Zadro˙zny, S.: Fuzzy linguistic data summaries as a human consistent,
user adaptable solution to data mining. In: Gabrys, B., Leiviska, K., Strackeljan,
J. (eds.) Do Smart Adaptive Systems Exist? STUDFUZZ, vol. 173, pp. 321–340.
Springer, New York (2005)
15. Kacprzyk, J., Zadro˙zny, S.: Linguistic database summaries and their proto-
forms: toward natural language based knowledge discovery tools. Information Sci-
ences 173, 281–304 (2005)
16. Raschia, G., Mouaddib, N.: SAINTETIQ: a fuzzy set-based approach to database
summarization. Fuzzy Sets and Systems 129, 137–162 (2002)
17. Rasmussen, D., Yager, R.R.: Finding fuzzy and gradual functional dependencies
with SummarySQL. Fuzzy Sets and Systems 106, 131–142 (1999)
18. Ros, M., Pegalajar, M., Delgado, M., Vila, A., Anderson, D.T., Keller, J.M.,
Popescu, M.: Linguistic summarization of long-term trends for understanding
change in human behavior. In: Proceedings of the IEEE International Conference
on Fuzzy Systems, FUZZ-IEEE 2011, pp. 2080–2087 (2011)
19. Szczepaniak, P., Ochelska, J.: Linguistic summaries of standardized documents. In:
Last, M., Szczepaniak, P.S., Volkovich, Z., Kandel, A. (eds.) Advances in Web In-
telligence and Data Mining. SCI, vol. 23, pp. 221–232. Springer, Heidelberg (2006)
20. doi:10.4121/c2c3b154-ab26-4b31-a0e8-8f2350ddac11
21. Wilbik, A., Keller, J.M., Bezdek, J.C.: Linguistic prototypes for data from eldercare
residents. IEEE Transactions on Fuzzy Systems (2013) (in press)
22. Wilbik, A., Keller, J.M.: A distance metric for a space of linguistic summaries.
Fuzzy Sets and Systems 208, 79–94 (2012)
23. Wilbik, A., Keller, J.M., Alexander, G.L.: Linguistic summarization of sensor data
for eldercare. In: Proceedings of the IEEE International Conference on Systems,
Man, and Cybernetics (SMC 2011), pp. 2595–2599 (2011)
24. Yager, R.R.: A new approach to the summarization of data. Information Sci-
ences 28, 69–86 (1982)
25. Zadeh, L.A.: Toward a theory of fuzzy information granulation and its centrality
in human reasoning and fuzzy logic. Fuzzy Sets and Systems 9(2), 111–127 (1983)
26. Zadeh, L.A.: A prototype-centered approach to adding deduction capabilities to
search engines – the concept of a protoform. In: Proceedings of the Annual Meeting
of the North American Fuzzy Information Processing Society (NAFIPS 2002), pp.
523–525 (2002)

Mining Epidemiological Dengue Fever Data from
Brazil: A Gradual Pattern Based Geographical
Information System
Yogi Satrya Aryadinata1, Yuan Lin2, C. Barcellos3,
Anne Laurent1, and Therese Libourel2
1 LIRMM, Montpellier, France
{aryadinata,laurent}@lirmm.fr
2 UMR ESPACE-DEV (IRD-UM2), Montpellier, France
therese.libourel@univ-montp2.fr, yuan.lin@ird.fr
3 Funda´c˜o Oswaldo Cruz, Rio de Janeiro, Brazil
xris@fiocruz.br
Abstract. Dengue fever is the world’s fastest growing vector-borne disease.
Studying such data aims at better understanding the behaviour of this disease to
prevent the dengue propagation. For instance, it may be the case that the number
of cases of dengue fever in cities depends on many factors, such as climate con-
ditions, density, sanitary conditions. Experts are interested in using geographical
information systems in order to visualize knowledge on maps. For this purpose,
we propose to build maps based on gradual patterns. Such maps provide a solu-
tion for visualizing for instance the cities that follow or not gradual patterns.
Keywords: Epidemiological Data, Data Mining, Geographic Information
Systems, Gradual Patterns.
1
Introduction
There are approximately 50 millions new dengue cases each year, and approximately
2.5 billion people live in endemic countries [1] located in the tropical zone between
the latitudes of 35◦N and 35◦[2]. The vector for dengue infection is Aedes aegypti,
which has a strictly synanthropic lifestyle [3]. The proliferation of these mosquitoes is
supported by both weather patterns and the contemporary style of human life in large
cities, where large amounts of water are deposited in the environment and become po-
tential breeding grounds for mosquito reproduction [4]. These factors have been shown
to inﬂuence the occurrence and spread of dengue infection over the last 50 years.
The incidence of dengue infection has a characteristic seasonal movement in al-
most all regions of the world, where periods of high transmissibility are experienced
during certain months of the year. This phenomenon has been explained by the close
relationship between the density variation in winged forms of the vector and climatic
conditions [5,6], such as rainfall, temperature and relative humidity.
Since 1986, Brazil has been affected by dengue epidemics that have reached dramatic
proportions. This country of continental dimensions has a wide territorial range of tropi-
cal and subtropical climates (hot and humid), with an average annual temperature above
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 414–423, 2014.
c⃝Springer International Publishing Switzerland 2014

Mining Epidemiological Dengue Fever Data from Brazil
415
20◦C and rainfall exceeding 1,000 mm per year. These characteristics provide suitable
abiotic conditions for the survival of Aedes aegypti [7], the main dengue vector present
in the Americas.
From 1990 to 2010, 5.98 million cases of dengue were reported, and autochthonous
cases have been recorded in 80% of the 5,565 Brazilian municipalities. The period of
greatest risk for dengue occurrence has been shown to be during or immediately follow-
ing the rainy season [5,6], and there is a reduced incidence during the remaining months
of the year. However, epidemiological studies on the relationship between dengue in-
fection and climate variables in Brazil are scarce.
Studies in wide (national) geographical scales and considering the interactions be-
tween spatial and time are still scarce in dengue literature. [3] revealed rapid travelling
waves of DHF crossing Thailand emanating from Bangkok every 3 years. Inversely,
in Cambodia seasonal propagation waves are originated in poor rural areas being their
propagation conditioned by road trafﬁc [8]. In Peru, dengue spatial and temporal dy-
namics was inﬂuenced by the different sociodemographic and environmental among
eco-regions [9]. The recent spread of dengue in Brazil is equally related to human mo-
bility across cities network and leaving remote country regions relatively protected [10].
However, unlike contagious diseases, dengue transmission is constrained the environ-
mental substrate on which vector must reproduce and infect people. Thus, the pres-
ence and abundance of vector are necessary but not sufﬁcient condition to dengue
transmission.
Climate and environmental changes may exacerbate the present distribution of vector
borne diseases as well as extend transmission to new niches and populations [11]. Both
trends underline the role of health surveillance systems in order to detect and conduct
preventive actions in unusual transmission contexts. Climate changes affect populations
in different ways and intensities according to the vulnerability of social groups, which
is associated to their insertion in place and society. Spatial analysis offers important
tools to describe measure and monitor health impact on vulnerable populations under
possible scenarios. Brazilian territory presents a wide variety of temperature ranges and
rainfall regimes. In addition to climatic variations, unequal urban infrastructure among
cities and differential territory occupation patterns increases the complexity of dengue
nationwide dynamics.
The important questions arising from experts debate and studies are :
1. What factors are determinants to explain dengue distribution? Which are the most
important? Climate? Sanitary conditions? Human mobility?
2. Which years are typical and regular (the same patterns appear along all years)? And
which are abnormal (patterns are different for one atypical year)? For example,
extreme climate events, el Ni˜no ?
3. Mapping the patterns. Are patterns concentrated (spatial clusters)? Where are lo-
cated the different (and contradictory) patterns?
4. Is this spatial pattern related to other geographical features? (relief, ecosystems,
roads, rivers, urban regions, etc.).
In this paper, we propose a method using a gradual pattern mining to analyze and
to discover the patterns of behavior in dengue fever cases in Brazil. This method also

416
Y.S. Aryadinata et al.
allows us to produce a gradual map that can directly be used to see the behavior of the
cases of dengue fever from the geographic approach.
In Section 2, we introduce the method to ﬁnd the gradual patterns, which can be used
to create binary and gradual map that described in Section 3. Section 4 presents the data
and indicators in terms of analysis and how to produce the binary map (Section 4.2) and
the gradual map (Section 4.3). Section 5 concludes the paper and gives the perspectives
of our research.
2
Gradual Patterns
Gradual pattern mining has been recently introduced as the topic addressing the auto-
matic discovery of gradual patterns from large databases. Such databases are structured
over several attributes which domains are totally ordered, considering a relation ≤. For
instance Table 1 reports an example of such a database.
We consider a toy database containing the information about a disease taken on ﬁve
cities. Each tuple from the table 1 corresponds to a city, and reports the number of cases
for this disease (last column) studied by respect with the number of inhabitants from
the city (in thousands), the average humidity (in percentage), and the average income
(in K euros).
Table 1. Database D describing a toy example for a disease in 5 cities
Id Nb Inhabitants (×1000) Humidity Income Nb Cases (×1000)
C1
110
53
30
10
C2
202
71
61
28
C3
192
64
62
43
C4
233
83
81
41
C5
225
75
73
39
An example of gradual patterns is The higher the number of inhabitants, the higher
the degree of Humidity, the higher the number of cases of the disease.
We remind below some concepts of the literature on gradual patterns.
Deﬁnition 1. Gradual item. A gradual item is a pair (i, v) where i is an item and
v is variation v ∈{↑, ↓}. ↑stands for an increasing variation while ↓stands for a
decreasing variation.
Example 1. (NbInhabitants, ↑) is a graduel item.
Deﬁnition 2. Gradual Pattern (also known as Gradual Itemset). A gradual pattern is a
set of gradual items, denoted by GP = {(i1, v1), . . . , (in, vn)}. The set of all gradual
patterns that can be deﬁned is denoted by GP.
Example 2. {(NbInhabitants, ↑), (Humidity, ↑), (NbCases, ↑)} is a gradual item-
set.

Mining Epidemiological Dengue Fever Data from Brazil
417
Gradual pattern mining aims at extracting the frequent patterns, as in the classical
data mining framework for itemsets and association rules.
Deﬁnition 3. Given a threshold of a minimum support ρ, a gradual pattern GP is said
to be frequent if supp(GP) ≥ρ.
For describing what frequent means in the context of gradual patterns, several sup-
ports have been proposed in the literature. All these materials are based on the idea
that it takes the number / proportion of transactions in the database (e.g., the number /
proportion of cities in our example) that respect the pattern. For being counted, a trans-
action must behave adequately with respect to other cities. For example, in Table 2, we
see that the number of cases and the number of inhabitants increase together for cities
1 and 2, as 110 < 202 and 10 < 28. If the variation is decreasing (↓) then the num-
bers must follow it. For instance for cities 3 and 4, the number of inhabitants increases
((Inhabitants, ↑)) and the number of cases ((cases, ↓)) decreases as 192 < 233 and
43 > 41.
One of the support proposed in the literature [12] is based on the length of the longest
path of cities that can be built using this idea.
Deﬁnition 4. The support of gradual pattern GP is given by the following formula
: support(GP) =
maxL∈l(|L|)
(|R|)
, where L is the set of rows the support of a gradual
pattern GP and R is the set of rows of the database D.
For determining the longest path, we build the precedence graph for the pattern be-
ing considered (Fig. 1). It can be the case that several paths can be built for a pat-
tern, as shown below when trying to order the cities with respect with the pattern
P {(NbInhabitants, ↑), (Humidity, ↑), (NbCases, ↑)}. Two orderings are possible:
L1 and L2.
Table 2. Two list obtained of {(NbInhabitants, ↑), (Humidity, ↑), (NbCases, ↑)} : L1 and
L2 (from left to right)
Id Nb Inhabitants Humidity Nb Cases
C1
110
53
10
C2
202
71
28
C4
233
83
41
Id Nb Inhabitants Humidity Nb Cases
C1
110
53
10
C2
202
71
28
C5
225
75
39
Precedence graphs can also be represented in the form of binary matrices as shown
in Table 3 for the pattern {(NbInhabitants, ↑), (Humidity, ↑), (NbCases, ↑)}. C2
precedes C4 and C5 (value 1 of the matrix), but not C1 and C3 (value 0 of the matrix).
We have here: support ({(NbInhabitants, ↑), (Humidity, ↑), (NbCases, ↑)}) = 3
5 as
pattern P is taken by the maximum list of cities < C1, C2, C4 > and < C1, C2, C5 > .
For representing a pattern on the whole database, we consider precedence graphs,
as shown by Fig. for the pattern {(NbInhabitants, ↑), (Humidity, ↑), (NbCases, ↑
)}. Such a graph can be represented in a binary matrix, which allows to optimize the

418
Y.S. Aryadinata et al.
C1
C2
C4
C3
C5
Fig. 1. Precedence graph associated to the pattern {(NbInhabitants, ↑), (Humidity, ↑
), (NbCases, ↑)}
Table 3. Binary matrix associated to the pattern {(NbInhabitants, ↑), (Humidity, ↑
), (NbCases, ↑)}
City C1 C2 C3 C4 C5
C1
1
1
1
1
1
C2
0
1
0
1
1
C3
0
0
1
0
0
C4
0
0
0
1
0
C5
0
0
0
0
1
computations. For instance, there are two longest paths in this example: < C1, C2, C4 >
and < C1, C2, C5 >. The support is thus equal to 3
5.
In our work, we aim at displaying such gradual patterns computing from geograph-
ical data on maps. For this purpose, we propose to display cities in a form (e.g., shade
intensity or size) that depends on whether it contributes or not to some gradual pattern.
For instance, if the city 1 does not behave as all the other cities for pattern (Humidity, ↑
), (NbCases, ↑) then it will be displayed as squares, while the other cities will be rep-
resented as a triangle.
3
Building Binary and Gradual Maps
In this section, we present novel methods for visualizing gradual patterns on maps. We
propose two methods of visualization. Both these methods rely on the calculation of the
support with the longest path in the precedence graph (Fig. 4).
Our idea is to produce maps starting from the extraction of gradual patterns. We will
explain this part with a simple example of a region that contains ﬁve cities (C1, C2, C3,-
C4, C5). Based on data from these cities, we believe we have an interesting pattern that
we can apply in a map. We then want to know, for each city, how it contributes or not
to identify spatial pattern. Then we can represent this information to the user on the
maps which he is accustomed. The principle is that each pattern corresponds to a layer

Mining Epidemiological Dengue Fever Data from Brazil
419
Table 4. The list of longest paths associated to Fig. 1
List
Length
C1, C3
2
C1, C2, C4
3
C1, C2, C5
3
in the GIS. Cities are then represented differently depending on their contribution to the
cause, i.e., a dichotomous variable.
We consider two approaches. The ﬁrst approach, called ”binary map” is to repre-
sent by a form (e.g., triangle) the cities that contribute to maximum path length on the
pattern, and with other form (e.g. square) the other cities. The second approach is to
represent cities by the intensity of the shade that are in the longest path and less in-
tense shade for cities that are in the shortest path. This is called ”gradual map”, i.e., a
continuous variable.
3.1
Binary Map
In this binary map, we identify items through its participation in the gradual pattern
(between 0 and 1) we use the following steps:
1. Calculating each item of the support in line with the binary matrix
2. Extract lines that have the maximum support
3. Identiﬁcation (triangle (1)) cities that are included in the set of itemset length of the
maximum path.
4. Identiﬁcation (square (0)) of the cities that are not in the set of itemset length of the
maximum path.
By looking at Figure 1, we know the cities which respect the gradual interesting
pattern. For example, Figure 2 shows that the city C3 as a square, do not fully respect
the gradual pattern. In contrast, other cities are triangles because they are in the longest
path.
Fig. 2. Binary Map
Fig. 3. Gradual Map

420
Y.S. Aryadinata et al.
3.2
Gradual Map
In order to realize a more detailed maps of the binary map. To do this, we propose
a gradual map to visualize the value of the support of each item which corresponds
to a proportion of belonging to a support item. To realize this map, we consider the
following steps:
1. Calculate the support of each item per line in the binary matrix
2. Take the maximal support size
3. Calculate the intensity of each item in order to identify the importance of which
item depends on its intensity value
Intensity(v) = T he length of support of v
T he maximum path length
4. From the intensity value make the classiﬁcation using the shade intensity as indi-
cator on the map.
With this map, we can identify the cities that are more important than others con-
cerning the classiﬁcation deﬁned in the ﬁgure. Thus, the city be more or less illumi-
nated depending on the length of the maximum path in which it appears. For example,
in Figure 3, the cities C1, C4 and C5 are the most illuminated because they belong to
the maximum path length.
4
Experimentation
Therefore, in our experimentation we use our methods with the dengue cases in Brazil.
Firstly, we analyse the data sources and indicators that described in Section 4.1 and
Section 4.2 and 4.3 to build the binary and gradual maps.
4.1
Data Sources and Indicators
Dengue fever (DF) notiﬁcations from 2001 to 2012 were summarized by year of
symptoms upset and municipality of residence. Data were obtained from the Notiﬁ-
able Diseases Information System (SINAN acronym in Portuguese), organized by the
Brazilian Ministry of Health and freely available in Health Information Department
(Datasus). Cases are deﬁned as conﬁrmed Dengue Fever (DF) or Dengue Hemorrhagic
Fever (DHF). Cases are conﬁrmed by clinical and laboratory according to standard
procedures and submitted to epidemiological investigation by local health surveillance
teams. Approximately 30% of the cases of dengue are also laboratory-conﬁrmed.
The socio-demographic data were obtained from the website of the Brazilian Insti-
tute of Geography and Statistics (Instituto Brasileiro de Geograﬁa e Estatstica IBGE;
http://mapas.ibge.gov.br). Cities were categorized according to the climate classiﬁca-
tion map of climate obtained from the Brazilian Institute of Geography and Statistics
(IBGE). There are four types of variables on this data: Temperature regime was classi-
ﬁed in three categories, Rainfall regime was categorized into four classes, Humidity was
categorized into four classes, Sanitary conditions were summarized by the combination
of three variables, Mobility was evaluated by means of two variables.

Mining Epidemiological Dengue Fever Data from Brazil
421
Individual variables were ranked and the result of summing the ranks was used to
categorize into four classes: Very high, High, Medium and Low. All indicators were
geocoded and mapped using the coordinates of the city as the center of gravity on
the common 5506 existing in 2010. This position was used to create maps and assign
information from other layers, such as climate, in a geographic information system
(GIS).
During the extraction process of gradual patterns, we cleaned the data so that they do
not contain any missing values, false values, etc. In Section 2, we introduced methods
for research on dengue epidemic. Then, we will apply these methods in order to obtain
better results.
For simplicity, we choose 25 cities in the state of Rio de Janeiro, Brazil. The State of
Rio de Janeiro (RJ) is located east of the southeast region. The capital is Rio de Janeiro.
This state has an area of 43 909 km2 with about 14,367,000 mainly concentrated along
the coast. We choose the state of Rio de Janeiro because of the high frequency of dengue
outbreaks in the region. In addition, this region presents a wide morphological diversity
(mountains, beaches, dunes, lagoons, etc..). In general, it is divided into three major
geographical subregions: the metropolitan lowlands (often called Baixada Fluminense),
coastal elevations and northern lowlands. The climate is tropical and the average annual
temperature is 23 ◦C. With this data, we extract the gradual patterns with the method of
extraction of conventional gradual patterns on the Section 2.
Table 5 displays some of the interesting gradual patterns in the case of epidemic
dengue. After looking at the Table 5 and patterns found, we can infer that the climate
(the drought, temperature, humidity) plays the most important role in the case of the
dengue epidemic, followed by the level of mobility and sanitation state level.
Table 5. Example of extracted gradual patterns (Support = 0.25)
Motifs
Support Longest Path
[T emp ↑NbCases ↑]
19
(330450 330550 330320 330270 330555
330510 330250 330370 330430 330455
330190 330414 330490 330227 330600
330200 330220 330330)
[Drought ↑Humid ↓NbCases ↓]
15
(330330 330200 330227 330030 330490
330190 330430 330250 330510 330180
330610 330280 330320 330550 330450)
[Drought ↑T emp ↓Humid ↓NbCases ↓]
15
(330330 330200 330227 330414 330190
330430 330250)
[Mobility ↑NbCases ↓]
9
(330360 330220 330600 330490 330455
330250 330510 330555 330270)
[T emp ↑Mobility ↓NbCases ↓]
7
(330330 330190 330600 330227 330370
330320 330550)
Finally, we can make a binary map and a gradual map that take into account the
gradual patterns on the dengue epidemic in Brazil in Fig. 4 and Fig. 5 for the pattern
[T emp ↑Mobility ↑NbCases ↓].

422
Y.S. Aryadinata et al.
4.2
Binary Map
Fig. 4 show that the most of the cities have high dengue fever cases, followed to the
pattern (T emperature, ↑), (Mobility, ↓), (NbCases, ↓), displayed as triangles.
Fig. 4. Binary Map (T emperature,↑), (Mobility, ↓), (NbCases, ↓)
4.3
Gradual Map
In Fig. 5, we present the support level (0−1) of the cities using the intensity color. This
map shows more detail information than binary map,Which cities are more related to the
pattern (T emperature, ↑), (Mobility, ↓), (NbCases, ↓). We can see that important
emerging epidemic dengue fever mostly appeared in cities located around the coast
(shown with the green intensity).
Fig. 5. Gradual Map (T emperature,↑), (Mobility, ↓), (NbCases, ↓)

Mining Epidemiological Dengue Fever Data from Brazil
423
The difﬁculty of this step is the production of maps from found gradual patterns, but
it is important to retrieve all the items in each pattern corresponding to the length of
maximum path.
5
Conclusion
In this article, we study how gradual patterns can help to produce maps in geographical
information systems. We apply our method in the context of dengue epidemics in Brazil.
Our main perspectives are to merge more criteria for building such maps and to study
how a large volume of gradual maps can be displayed to end-users in a user-friendly
way by using the so-called layers in geographical information system tools.
References
1. Organization, W.H.: The World Health Report 2006: Working together for health (2006)
2. Gubler, D., Ooi, E., Vasudevan, S., Farrar, J.: Dengue and Dengue Hemorrhagic Fever. CABI
(2013)
3. Gubler, D.J., et al.: Dengue, urbanization and globalization: The unholy trinity of the 21st
century. Tropical Medicine and Health 39(4 suppl.), 3–11 (2011)
4. Kovats, R., Campbell-Lendrum, D., McMichael, A., Woodward, A., Cox, J.: Early effects
of climate change: do they include changes in vector-borne disease? Philos. Trans. R. Soc.
Lond. B. Biol. Sci. 356(1411), 1057–1068 (2001)
5. Souza-Santos, R.: The factors associated with the occurrence of immature forms of aedes
aegypti in Ilha do Governador, Rio de Janeiro, Brazil. Rev. Soc. Bras. Med. Trop. 32(4),
373–382 (1999)
6. Souza-Santos, R., Carvalho, M.: Spatial analysis of aedes aegypti larval distribution in the
Ilha do Governador neighborhood of Rio de Janeiro, Brazil. Cad. Saude Publica 16(1) (2000)
7. Yang, H.M., Macoris, M.L., Galvani, K.C., Andrighetti, M.T., Wanderley, D.M.: Dinˆamica
da transmissao da dengue com dados entomol´ogicos temperatura-dependentes. Tema–Tend.
Mat. Apl. Comput. 8(1), 159 (2007)
8. Teurlai, M., Huy, R., Cazelles, B., Duboz, R., Baehr, C., Vong, S.: Can human movements ex-
plain heterogeneous propagation of dengue fever in Cambodia? PLoS Negl. Trop. Dis. 6(12),
e1957 (2012)
9. Chowell, G., Cazelles, B., Broutin, H., Munayco, C.V.: The inﬂuence of geographic and
climate factors on the timing of dengue epidemics in Peru, 1994-2008. BMC Infectious Dis-
eases 11, 164 (2011)
10. Cat˜ao, R.D.C., Guimar˜aes, R.B.: Mapeamento da reemergˆencia do dengue no Brasil–
1981/82-2008. Hygeia 7(13) (2011)
11. McMichael, A., Lindgren, E.: Climate change: present and future risks to health, and neces-
sary responses. J. Intern. Med. 270(5), 401–413 (2011)
12. Di-Jorio, L., Laurent, A., Teisseire, M.: Mining frequent gradual itemsets from large
databases. In: Adams, N.M., Robardet, C., Siebes, A., Boulicaut, J.-F. (eds.) IDA 2009.
LNCS, vol. 5772, pp. 297–308. Springer, Heidelberg (2009)

A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 424–433, 2014. 
© Springer International Publishing Switzerland 2014 
  
A New Model of Efficiency-Oriented Group Decision  
and Consensus Reaching Support in a Fuzzy Environment 
Dominika Gołuńska1,2,*, Janusz Kacprzyk2, and Sławomir Zadrożny2 
1 Department of Automatic Control and Information Technology,  
Cracow University of Technology, ul. Warszawska 24, 31-155 Cracow, Poland 
2 Systems Research Institute, Polish Academy of Sciences  
ul. Newelska 6, 01-447 Warsaw, Poland  
dominika.golunska@pk.edu.pl 
kacprzyk@ibspan.waw.pl 
Abstract. We present a novel comprehensive model of a consensus reaching 
support system in the fuzzy context. We assume the individual fuzzy preferences, 
a fuzzy majority in group decision making, as proposed by Kacprzyk [9], some 
fuzzy majority based solution concepts in group decision making, notably fuzzy 
cores (cf. Kacprzyk [9]) and their choice function based representations by 
Kacprzyk and Zadrożny [15],[16], a soft degree of consensus by Kacprzyk and 
Fedrizzi [10],[11]. Using as a point of departure Kacprzyk and Zadrożny’s [18] 
approach of the use of linguistic data summaries to support the running of a 
consensus reaching process, we develop and implement a novel approach that 
synergistically combines the tools and techniques mentioned above. We assume 
that moderated consensus reaching process which is run in the group of agents by 
a special agent called a moderator, is the most effective and efficient solution. 
We attempt to facilitate the work of a moderator, by some useful guidelines and 
additional indicators. We extend this idea and finally, we present a new imple-
mentation followed by a numerical evaluation of the new model proposed. 
Keywords: consensus reaching, group decision support systems, fuzzy prefe-
rence relations, soft degree of consensus, linguistic quantifier, fuzzy cores. 
1 
Introduction 
Decision making processes usually lead to better decisions if are run in a goal-directed 
way [6], and in a collective decision making setting to help reach a decision better 
reflecting the opinion of the entire group, which is normally easier and more effective 
and efficient if the agents involved arrive first at a consensus. 
The problem considered in this article concerns a consensus reaching process, and 
its related group decision making process, in a (small) group of autonomous agents 
                                                           
 
* Dominika Gołunska’s work was supported by the Foundation for Polish Science under Inter na-
tional PhD Projects in Intelligent Computing financed from The European Union within the  In-
novative Economy Operational Programme 2007-2013 and European Regional Development 
Fund. 

A New Model of Efficiency-Oriented Group Decision and Consensus Reaching Support 
425 
(individuals, decision makers, experts, …) who present their testimonies as fuzzy 
(graded) preference relation defined on a set of options that usually significantly differ 
in the beginning. The consensus reaching process boils down to step-by-step changes 
of the testimonies of the particular agents until they become close enough, i.e., until the 
group arrives at a sufficient agreement to expressed through a degree of consensus 
assumed here due to Kacprzyk and Fedrizzi [10,11]. The willingness to change the 
testimonies by agents is clearly a prerequisite for consensus reaching. Recent devel-
opments in IT/ICT tools and techniques, notably in the area of human computer in-
terfaces (HCI), have made possible to use for this the decision support systems (DSSs) 
[25]. 
We are concerned with consensus reaching in a group of agents and our purpose is to 
develop and implement a novel model using modern tools for the manipulation of 
testimonies. We assume a moderated consensus reaching process run by a “superagent” 
called a moderator. This process may be substantially enhanced and accelerated by 
employing modern concepts and techniques (Kacprzyk and Zadrożny [20],[21]]) based 
on the use of additional information and insight into which agents and their pairs, 
options, etc. are critical by having a considerable influence on a degree of consensus, 
and should be dealt with by the moderator, e.g., by trying to persuade to change the 
testimonies of specified agents with respect to pairs of specified options, etc. 
This general approach boils down to a clearly efficiency-oriented scenario in which 
usually only a few of the most promising agents and options are taken into account as 
this may imply the best increase of the degree of consensus. 
2 
A General Framework for a Moderated Consensus Reaching 
We briefly present a general framework for the consensus reaching support proposed 
by Fedrizzi, Kacprzyk and Zadrożny [5] in which the process is moderator-run/assisted, 
with a special “superagent”, a moderator, being explicitly involved, as shown in Fig. 1. 
Basically, this is an interactive and iterative process, cf. [2], with a special role of a 
moderator who constantly measures distances (dissimilarities) between agents and 
checks whether (a proper degree of) consensus is reached or not, and supports the 
discussion by stimulating the exchange of information, suggesting arguments (pros and 
cons), convincing appropriate changes of preferences, focusing the discussion on issues 
which may resolve conflicts of opinions within the group, etc. This is repeated until the 
group arrives at a state of sufficient agreement or until we reach some time limit [11].  
3 
A Model of Consensus Reaching in a Fuzzy Environment 
The first basic element of the proposed approach is the use of some fuzzy logic-based 
representations of preferences. We assume a finite set of 
2
≥
m
 agents (individuals, 
experts), 
{
}
m
e
e
e
E
,...,
, 2
1
=
, and a set of 
2
≥
n
 options (alternatives, issues), 
{
}
ns
s
s
S
,...,
, 2
1
=
. Each agent 
E
ek ∈
 expresses his or her testimonies as to the par-
ticular pairs of options from S  which are assumed to be individual fuzzy preference 
relation, 
k
R , defined over the set of options S  (that is, in 
S
S×
) [9].   

426    D. Gołuńska, J. Kacprzyk, and S. Zadrożny 
An individual  fuzzy preference relation of agent 
ke , 
k
R , is given by its membership 
function 
]1,0
[
:
→
× S
S
k
R
μ
. Namely, 
5.0
)
,
(
>
j
i
R
s
s
k
μ
 indicates the preference degree 
of option 
is  over option 
js . 
5.0
)
,
(
<
j
i
R
s
s
k
μ
denotes the preference degree of  
js  
over 
is . Finally, 
5.0
)
,
(
=
j
i
R
s
s
k
μ
 determines the indifference between 
is  and 
js . 
 
Fig. 1. A general scheme of supporting consensus reaching run by the moderator [14] 
The relevant element of our consensus reaching model is the use of linguistic quan-
tifiers (using Zadeh’s calculus [26]) which has been proposed by Kacprzyk [9] as a model 
of a fuzzy majority in either a soft measure of consensus or a group decision making 
solution (cf. Kacprzyk [9], Kacprzyk and Fedrizzi [10]. Kacprzyk and Zadrożny [15-16]). 
Linguistic quantifiers are represented by terms like most, much more than a half, almost 
all, etc. and represented by fuzzy sets in [0, 1]. Basically, we will use such relative fuzzy 
quantifiers in order to represent the concept of a fuzzy majority. 
A linguistically quantified statement, exemplified by “most individuals are satis-
fied”, can be written as 
 
F
are
s
Qy'
 
(1) 
where Q  is a linguistic quantifier (e.g., most), 
{ }
y
Y =
 is a set of objects (e.g., 
agents) and F  is a property (e.g., satisfied).  
The degree of truth of the linguistically quantified statement (1) is determined using 
Zadeh’s calculus, with a fuzzy linguistic quantifier Q  assumed to be a fuzzy set in 
[0,1]. For instance, 
"
"most
Q =
may be defined as 
 



<
≤
≤
>
−
=
.3.0
8.0
3.0
8.0
0
6.0
2
1
)
(
"
"
x
x
x
for
for
x
for
x
most
μ
. 
(2) 
Property F  is defined as a fuzzy set in the set of objects Y, and if 
{
}
p
y
y
Y
,...,
1
=
, 
then the truth value of 
F
is
yi
is identified with 
)
(
i
F y
μ
[10]. Now, we have: 

A New Model of Efficiency-Oriented Group Decision and Consensus Reaching Support 
427 
 

=
=
p
i
i
F y
p
r
1
)
(
1
μ
 
(3) 
 
truth(Qy’s are F)
)
(r
Q
μ
=
. 
(4) 
We adopt the concept of a soft measure (degree) of consensus (Kacprzyk and Fe-
drizzi [10],[11]), that is, the degree to which: “most of agents agree with their prefe-
rences to most of the options”. A classic concept of consensus is when “all the agents 
agree with their preferences to all of the options”. However, this “full and unanimous 
agreement” is unrealistic in practice, because agents usually reveal significant differ-
ences in their viewpoints, flexibility, aversion to change opinions, etc. [5]. 
The soft degree of consensus is now derived in three steps [6]:  
1) for each pair of agents we calculate a degree of agreement as to their preferences 
between all the pairs of options,  
2) we aggregate these degrees to derive a degree of agreement of each pair of indi-
viduals as to their preferences between 
1
Q  (e.g., “most”) pairs of options,  
3) we combine these degrees to obtain a degree of agreement of 
2
Q  (a linguistic 
quantifier similar to 
1
Q ) pairs of individuals as to their preferences between 
1
Q  pairs 
of options and this is meant to be the degree of consensus. 
We start with the degree of a strict agreement between agents 
1
ke
 and 
2
ke
 as to 
their preferences between options 
is  and 
j
s : 
 



=
=
otherwise
r
r
if
k
k
v
k
ij
k
ij
ij
0
1
)
,
(
2
1
2
1
 
(5) 
where, 
,1
,...,
1
1
−
=
m
k
 
,
,...,
1
1
2
m
k
k
+
=
 
,1
,...,
1
−
=
n
i
 
n
i
j
,...,
1
+
=
. 
The relevance of options is given by a fuzzy set over the set of options, B , such that 
]1,0
[
)
(
∈
i
B s
μ
 is a degree of relevance of option 
is , from 0 for fully irrelevant to 1 for 
fully relevant, through all intermediate values. The relevance 
ij
b  of each pair of op-
tions 
S
S
s
s
j
i
×
∈
)
,
(
, may be defined, as 
 
)]
(
)
(
[
2
1
j
B
i
B
B
ij
s
s
b
μ
μ
+
=
 
(6) 
for each 
j
i,
, where 
j
i ≠
. Evidently 
B
ji
B
ij
b
b
=
, for each 
j
i,
; for simplicity, the 
relevance of options is not accounted for, ie.e.
1
)
(
=
i
B s
μ
 in (6), for each 
S
si ∈
. 
Then, the degree of agreement between agents 
1k  and 
2
k  as to their preferences 
between all the pairs of options is (∗ stands for a t-norm): 

428    D. Gołuńska, J. Kacprzyk, and S. Zadrożny 
 


−
=
=
=
−
=
+
=
∗
=
1
1
1
1
1
1
2
1
2
1
)
,
(
)
,
,
(
n
i
n
i
j
B
ij
n
i
n
i
j
B
ij
ij
b
b
k
k
v
S
k
k
v
 
(7) 
Next, the degree of agreement between agents 
1k  and 
2
k  as to their preferences 
between 
1
Q  pairs of options is: 
 
))
,
,
(
(
)
,
,
(
2
1
1
2
1
1
S
k
k
v
S
k
k
v
Q
Q
μ
=
. 
(8) 
The degree of agreement of all pairs of agents as to their preferences between 
1
Q  
pairs of options is: 
 

−
=
+
=
−
=
1
1
1
2
1
1
1
2
1
1
)
,
(
)1
(
2
)
,
(
m
k
m
k
k
Q
Q
k
k
v
m
m
S
E
v
. 
(9) 
Finally, the degree of agreement of 
2
Q pairs of agents as to their preferences be-
tween 
1
Q  pairs of options, called the degree of consensus, is: 
 
))
,
(
(
)
,
(
1
2
2
1,
S
E
v
S
E
con
Q
Q
Q
Q
μ
=
. 
(10) 
3.1 
Discussion Guidance Using Additional Indicators 
An important component of the consensus reaching support system is a set of indicators 
measuring the current state of agreement between agents and with respect to specific 
pairs of options. We are concerned how to determine those agents and pairs of options 
that are the main obstacles to reaching consensus, which preference matrix may be a 
candidate for a consensual one, etc. [22].  Those indicators may be both given as 
traditional numeric values or linguistic values (summaries), for instance expressed by 
using natural language generation techniques (cf. Kacprzyk and Zadrożny [18][21]). 
In addition to many indicators proposed by, e.g. Kacprzyk and Zadrożny [14,18], 
here we propose the following new effective and efficient indicators:  
1) Response to omission of agent 
E
ek ∈
, 
]1,1
[
)
(
−
∈
k
RTO
, is defined as a differ-
ence between the consensus degree for the whole group (10) and the consensus degree 
for the group without taking into account agent 
ke : 
 
)
},
{
(
)
,
(
)
(
2
1
2
1
,
,
S
e
E
con
S
E
con
k
RTO
k
Q
Q
Q
Q
−
−
=
. 
(11) 
This measure makes it possible to estimate the influence of a given agent on the 
agreement in the whole group. The range of values is from -1, for an absolutely negative 
impact, through 0 for a lack of effect, to +1 for a definitely positive influence. Hence, this 
indicator conveys how important is the participation of agent k  in the group for con-
sensus reaching. Its positive value indicates a “consensus-constructive” position of a given 
agent while its negative value indicates a “consensus-destructive” position. 

A New Model of Efficiency-Oriented Group Decision and Consensus Reaching Support 
429 
2) Personal consensus degree of agent 
E
ek ∈
, 
]1,0
[
)
(
∈
k
PCD
, is the degree of 
truth of the following proposition: “Preferences of an agent 
ke  as to the most pairs of 
options are in agreement with the preference of most agents” what is: 
 

≠
−
=
m
k
k
Q
Q
S
k
k
v
m
k
PCD
1
2
1
2
)]
,
,
(
)1
(
1
[
)
(
2
1
1
μ
. 
(12) 
PCD takes values from 0 for an agent who is the most isolated in his or her opinions 
as compared to the rest of the group; to 1 for an agent whose preferences are shared by 
most agents; through intermediate values for agents with opinions of a varying degree 
of agreement with the opinions of other agents. This indicator points out how “typical” 
the opinion of an agent 
ke  is and helps identify agents who may be called “outsiders”. 
3) Option consensus degree for option 
,is  
]1,0
[
)
(
∈
is
OCD
, is the degree of truth 
value of the statement: “Most pair of agents agree with their preferences in respect to 
option 
is ” which can be expressed as: 
 
])
,
(
1
1
[
)
,
(
2
1
2
1
,
1

≠
−
=
n
i
j
ij
Q
Q
i
k
k
v
n
k
k
s
μ
 
(13) 
 


+
=
−
=
−
=
m
k
k
Q
i
m
k
Q
i
k
k
s
m
m
s
OCD
1
2
1
,
1
1
1
2
1
2
)]
,
(
)1
(
2
[
)
(
μ
. 
(14) 
The OCD may take values from 0, that the preferences of most agents differ sub-
stantially for a given option; to 1, that there is a substantial agreement among the agents 
as to this option. This indicator may help us omit some options from a further discus-
sion and thus better focus the consensus reaching.  
4) Response to exclusion of option 
,
S
si ∈
 
]1,1
[
)
(
−
∈
is
RTE
, is the difference be-
tween the consensus degree for the whole set of options (10) and without option 
is . 
 
})
{
,
(
)
,
(
)
(
1
,
,
2
1
2
1
i
Q
Q
Q
Q
i
s
S
E
con
S
E
con
s
RTE
−
−
=
. 
(15) 
This measure makes it possible to estimate the influence of a given option on the 
consensus degree, and may be interpreted similarly as RTO.  
3.2 
Group Decision Making Solution 
Essentially, two lines of calculation may be followed while considering solution con-
cepts of group decision making (cf. Nurmi and Kacprzyk [24]. Kacprzyk, Zadrożny, 
Fedrizzi, and Nurmi [22]): 
1) A direct approach: 
solution
R
R
m →
}
,...,
{ 1
, that is, a solution is obtained directly 
just from the set of individual fuzzy preference relations. 
2) An indirect approach: 
,
}
,...,
{ 1
solution
R
R
R
m
→
→
 that is, from the set of indi-
vidual fuzzy preference relations we calculate first a group (social) fuzzy preference 
relation, R, which is then employed to find a final solution. 

430    D. Gołuńska, J. Kacprzyk, and S. Zadrożny 
In our model we use the direct approach, i.e. by using the individual fuzzy preference 
relations and the concept of a fuzzy core C defined as a set of non-ndominated options, i.e. 
those not defeated in pairwise comparisons by a required majority 
m
≤
θ
, i.e. 
 
S
s
S
s
C
i
j
∈
¬∃
∈
=
:
{
5.0
>
k
ijr
for at least θ agents ek}. 
(16) 
To employ a fuzzy majority to extend (fuzzify) the core, we start by determining [22] 
 



<
=
otherwise
r
if
h
k
ij
k
ij
0
5.0
1
 
(17) 
where 
n
j
i
,...,
1
,
=
 and 
m
k
,...,
1
=
. So, 
k
ij
h  reveals if 
j
s
 dominates (in pairwise 
comparison) 
is  ( 
1
=
k
ij
h
) or not (
0
=
k
ij
h
). 
Next, we compute 
 

≠
=
−
=
n
j
i
i
k
ij
k
j
h
n
h
,1
1
1
 
(18) 
which is the degree, from 0 to 1, to which agent 
ke  is not against option 
j
s , where 0 
denotes definitely against and 1 definitely not against, through all intermediate values.  
Then we calculate 
 

=
=
m
k
k
j
j
h
m
h
1
1
 
(19) 
which defines to which degree all the agents are not against option 
j
s . 
Finally, we compute 
 
)
(
j
Q
j
Q
h
v
μ
=
 
(20) 
which is the extent to which Q  (e.g., most) agents are not against option 
j
s . 
The fuzzy Q - core is now defined as a fuzzy set in S with a membership function, 
i
Q
i
C
v
s
Q
=
)
(
μ
, i.e., as a fuzzy set of options that are not defeated by Q  (most) agents. 
4 
A Numerical Example 
Consider Example 3.1 from Carlson, Fedrizzi, and Fuller [1], with the number of 
options, n =4, and agents, m=4, and the fuzzy preference relations: 

A New Model of Efficiency-Oriented Group Decision and Consensus Reaching Support 
431 










=
7.0
2.0
8.0
1.0
7.0
4.0
1
R
  










=
7.0
2.0
8.0
0.0
5.0
4.0
2
R
 
 










=
7.0
2.0
8.0
3.0
4.0
4.0
3
R
 










=
7.0
1.0
7.0
1.0
7.0
4.0
4
R
 
Initially, the degree of consensus in the group of four agents is
42
.0
)
,
(
,
=
S
E
con
most
most
, and suppose that this is not satisfactory so that we use indi-
cators defined in the paper: 
1) The value of the indicator 
)
(k
PCD
 points out that agent 4 is the most isolated 
with his opinion with the rest of the group.  
2) Moreover, value of the indicator 
)
(k
RTO
for agent 4 confirms that he or she has a 
negative influence on the agreement in the group. 
3) On the contrary, the values of indicators 
)
(k
PCD
and 
)
(k
RTO
point out that the 
preferences of agent 1 are shared by most of agents (his or her fuzzy preference relation 
matrix may be a candidate for a consensual one). 
4) The value of the indicator 
)
( is
OCD
points out that option 2 is the most promising 
direction for a further discussion. 
Table 1. Values of PCD and RTO, for each agent, and OCD, for each option 
PCD(k) 
Value 
RTO(k) 
Value 
OCD(si) 
Value 
PCD(1) 
0.86 
RTO(1) 
+0.42 
OCD(2) 
0.47 
PCD(2) 
0.42 
RTO(2) 
-0.09 
OCD(3) 
0.42 
PCD(3) 
0.42 
RTO(3) 
-0.09 
OCD(4) 
0.42 
PCD(4) 
0 
 RTO(4) 
-0.44 
OCD(1) 
0 
In conclusion, a change of the preferences as to option 2 may be suggested to agent 4 
towards the preferences expressed by agent 1, and if accepted, his or her new fuzzy 
preference relation matrix may be as follows: 

432    D. Gołuńska, J. Kacprzyk, and S. Zadrożny 
 










=
7.0
2.0
8.0
1.0
7.0
4.0
4
R
. 
Hence, the changes in the preference matrix are: 
8.0
7.0
23
→
=
r
, 
2.0
1.0
24
→
=
r
. 
These small changes in two values cause the new value of group consensus degree to 
be now:
96
.0
)
,
(
,
=
S
E
con
most
most
. This degree of consensus is satisfactory and the ses-
sion is finished. 
So, the fuzzy Q - core (21) is:
73
.0
)
4
(
;
23
.0
)
3
(
;
73
.0
)
2
(
;
0
)
1
(
=
=
=
=
s
Q
C
s
Q
C
s
Q
C
s
Q
C
μ
μ
μ
μ
. No-
tice that two options are now pointed out as the best group choice: options 2 and 4. 
5 
Concluding Remarks 
We proposed a new solution to the support of a group consensus reaching process under 
fuzzy preferences and a fuzzy majority. We assumed the moderated consensus reaching 
process which is clearly an efficiency oriented strategy. The main task here was to 
obtain the best increase either in the degree of consensus or in the speed of its reaching.  
We emphasized the role of a moderator who facilitates obtaining a (satisfactory 
degree of) consensus by stimulating the exchange of information, suggesting argu-
ments, convincing appropriate decision makers to change their preferences, focusing 
the discussion on the most promising directions, and the like. We developed some 
useful additional indicators to facilitate the process. 
Our further research is to extend the model using some fairness oriented indicators, 
and by explicitly employing linguistic summaries, in particular by using tools of natural 
language generation (cf. Kacprzyk and Zadrożny [21]). 
References 
1. Carlsson, C., Fedrizzi, M., Fuller, R.: Group Decision Support Systems. In: Carlsson, C., 
Fedrizzi, M., Fuller, R. (eds.) Fuzzy Logic in Management, vol. 66, ch. 3, Springer (2004) 
2. Consensus Decision Making, 
http://www.uhc.org.uk/projects/toolbox/ 
meetings_and_organisation/consensus_short.htm 
3. Fedrizzi, M., Kacprzyk, J., Nurmi, H.: Consensus degrees under fuzzy majorities and fuzzy 
preferences using OWA (ordered weighted average) operators. Control and Cybernetics 22, 
71–80 (1993) 
4. Fedrizzi, M., Kacprzyk, J., Owsiński, J.W., Zadrożny, S.: Consensus reaching via a GDSS 
with fuzzy majority and clustering of preference profiles. Annals of Operations Re-
search 51, 127–139 (1994) 
5. Fedrizzi, M., Kacprzyk, J., Zadrożny, S.: An interactive multi-user decision support system 
for consensus reaching process using fuzzy logic with linguistic quantifiers. Decision 
Support Systems 4(3), 313–327 (1988) 
6. Gołuńska, D., Kacprzyk, J.: The Conceptual Framework of Fairness in Consensus Reaching 
Process Under Fuzziness. In: Proceedings of the 2013 Joint IFSA World Congress NAFIPS 
Annual Meeting, Edmonton, Canada, June 24-28, pp. 1285–1290 (2013) 

A New Model of Efficiency-Oriented Group Decision and Consensus Reaching Support 
433 
7. Herrera-Viedma, E., García-Lapresta, J.L., Kacprzyk, J., Fedrizzi, M., Nurmi, H., Zadrożny, 
S. (eds.): Consensual Processes. STUDFUZZ, vol. 267. Springer, Heidelberg (2011) 
8. Herrera-Viedma, E., Cabrerizo, F.J., Kacprzyk, J., Pedrycz, W.: A review of soft consensus 
models in a fuzzy environment. Information Fusion 17, 4–13 (2014) 
9. Kacprzyk, J.: Group decision making with a fuzzy linguistic majority. Fuzzy Sets and 
Systems 18, 105–118 (1986) 
10. Kacprzyk, J., Fedrizzi, M.: A ‘soft’ measure of consensus in the setting of partial (fuzzy) 
preferences. European Journal of Operational Research 34, 315–325 (1988) 
11. Kacprzyk, J., Fedrizzi, M.: A ‘human-consistent’ degree of consensus based on fuzzy logic 
with linguistic quantifiers. Mathematical Social Sciences 18, 275–290 (1989) 
12. Kacprzyk, J., Fedrizzi, M., Nurmi, H.: Group decision making and consensus under fuzzy 
preferences and fuzzy majority. Fuzzy Sets and Systems 49, 21–31 (1992) 
13. Kacprzyk, J., Nurmi, H., Fedrizzi, I.M. (eds.): Consensus under Fuzziness, pp. 55–83. 
Kluwer Academic Publishers, Boston (1996) 
14. Kacprzyk, J., Zadrożny, S.: On the use of fuzzy majority for supporting consensus reaching 
under fuzziness. In: Proceedings of FUZZ-IEEE 1997 - Sixth IEEE International Confe-
rence on Fuzzy Systems, Barcelona, Spain, vol. 3, pp. 1683–1988 (1997) 
15. Kacprzyk, J.J., Zadrożny, S.: Computing with words in decision making through individual 
and collective linguistic choice rules. International Journal of Uncertainty, Fuzziness and 
Knowledge – Based Systems 9, 89–102 (2001) 
16. Kacprzyk, J., Zadrożny, S.: Collective choice rules in group decision making under fuzzy 
preferences and fuzzy majority: a unified OWA operator based approach. Control and Cy-
bernetics 31(4), 937–948 (2002) 
17. Kacprzyk, J., Zadrożny, S.: An Internet-based group decision support system. Manage-
ment VII(28), 4–10 (2003) 
18. Kacprzyk, J., Zadrożny, S.: Supporting consensus reaching in a group via fuzzy linguistic 
data summaries. In: IFSA 2005 World Congress, Beijing, pp. 1746–1751. Tsinghua Uni-
versity Press/Springer (2005) 
19. Kacprzyk, J., Zadrożny, S.: Towards a general and unified characterization of individual and 
collective choice functions under fuzzy and nonfuzzy preferences and majority via the Or-
dered Weighted Average Operators. International Journal of Intelligent Systems 24(1), 4–26 
(2009) 
20. Kacprzyk, J., Zadrożny, S.: Soft computing and Web intelligence for supporting consensus 
reaching. Soft Computing 14(8), 833–846 (2010) 
21. Kacprzyk, J., Zadrożny, S.: Computing with words is an implementable paradigm: fuzzy 
queries, linguistic data summaries and natural language generation. IEEE Transactions on 
Fuzzy Systems 18(3), 461–472 (2010) 
22. Kacprzyk, J., Zadrożny, S., Fedrizzi, M., Nurmi, H.: On Group Decision Making, Con-
sensus Reaching, Voting and Voting Paradoxes under Fuzzy Preferences and a Fuzzy Ma-
jority: A Survey and some Perspectives. In: Bustince, H., et al. (eds.) Fuzzy Sets and Their 
Extensions: Representations, Aggregation and Models. STUDFUZZ, vol. 220, pp. 263–295. 
Springer, Heidelberg (2008) 
23. Kacprzyk, J., Zadrożny, S., Raś, Z.W.: How to support consensus reaching using action 
rules: a novel approach. International Journal of Uncertainty, Fuzziness and Know-
ledge-Based Systems 18(4), 451–470 (2010) 
24. Nurmi, H., Kacprzyk, J.: On fuzzy tournaments and their solution concepts in group deci-
sion making. European Journal of Operational Research 51, 223–232 (1991) 
25. Turban, E., Aronson, J.E., Liang, T.P.: Decision Support Systems and Intelligent Systems, 
6th edn., pp. 11–19, 94-101.C. Prentice Hall (2005) 
26. Zadeh, L.A.: A computational approach to fuzzy quantifiers in natural languages. Com-
puters and Mathematics with Applications 9, 149–184 (1983) 
 

Aggregation of Uncertain Qualitative
Preferences for a Group of Agents
Paolo Viappiani1,2
1 Sorbonne Universit´es, UPMC Univ Paris 06, UMR 7606, LIP6
2 CNRS, UMR 7606, LIP6, F-75005, Paris, France
paolo.viappiani@lip6.fr
Abstract. We consider aggregation of partially known qualitative pref-
erences for a group of agents, considering necessary and potentially op-
timal choices with respect to diﬀerent notions of optimality (consensus,
extreme choices, Pareto optimality) and provide a theoretical character-
ization. We report statistics (obtained with simulations with synthetic
data) about the cardinality of the sets of possible and necessarily optimal
choices for the diﬀerent cases. Finally we introduce preliminary ideas on
a qualitative notion of fairness and on interactive elicitation.
1
Introduction
In this paper we consider aggregation of partially known qualitative preferences
of diﬃerent agents. By qualitative we mean that preferences are explicitly and
directly encoded by binary relations [6,1]. We are in a setting of strict uncertainty,
meaning that no probabilistic assumption is made. Preference information is
incomplete, meaning that we only know a fraction of the binary preferences. We
assume that any consistent extension of the currently known preference relations
is considered possible. In particular, we consider possible and necessary Pareto
optimality and study the relations with other notions of optimality.
This work is an eﬃort towards the direction of eﬃective, practical methods for
preference assessment (preference elicitation) with purely qualitative statements.
Our work is similar to [4]; however our setting diﬃers since we assume qualitative
preferences expressed as orders (while they assume numeric utility functions).
2
Model
General Assumptions. We assume a set C of m items or elements (choices) and
a set G = {1, ..., n} of n agents; preferences are explicitly modeled by binary
relations. For each agent the preference relation ⪰i (for each agent i) models the
fact that, if o⪰i o′ holds, option o is at least as preferred as option o′ for agent i.
As usual [5] a preference relation ⪰i induces a preference structure (≻i, ≈i, ∼i)
for each agent, where ≻i is the strict preference relation, ≈i is the indiﬀerence
relation and ∼i is the incomparability relation: o≻i o′ if o⪰i o′ and o′ ̸⪰i o, o≈i o′
if o⪰i o′ and o′ ⪰i o and o∼i o′ if o̸⪰i o′ and o′ ̸⪰i o.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 434–443, 2014.
c
⃝Springer International Publishing Switzerland 2014

Aggregation of Uncertain Qualitative Preferences for a Group of Agents
435
For a given relation ⪰, we use the operator top to denote the maximum
element (if it exists):
top(⪰) = {o ∈C|∀o′ ∈C, o′ ̸=o : o ≻o′}.
(2.1)
Notice that the cardinality of top(⪰) is at most 1. The operator max denotes
maximal elements:
max(⪰) = {o ∈C| ̸ ∃o′ ∈C : o′ ≻o}.
(2.2)
Obviously it holds top(⪰) ⊆max(⪰). If the preferences of an agent over the
available items constitute a chain (i.e. ⪰i is a total order), then his optimal
option is top(⪰i) = max(⪰i).
As usual in multi-agent systems and multi-objective decision analysis, an op-
tion o dominates another option o′ if it holds o ⪰i o′ for all i, and for at least one
j it holds o ≻j o′ (strict preference); the Pareto optimal choices for the group
of agents are the those choices that are not dominated by any other choice. We
can conveniently express Pareto Optimality using the notion of aggregate group
dominance relation ⪰G, deﬁned as o⪰G o′ iﬃ∀i∈G, o⪰i o′. A consensus choice,
if it exists, it is an option that is the best preferred item for all agents.
Cons(⪰1, ..., ⪰n) = top(⪰G) = top(
H
i∈G
⪰i).
(2.3)
By representing relations in their extensive form as subset of the Cartesian
product C × C, the group dominance relation can be conveniently written as
⪰G= 
i ⪰i. Then, Pareto Optimal choices with respect to ⪰G are the maximal
elements of the aggregated group preference order:
ParetoOpt(⪰1, ..., ⪰n) = max(⪰G) = max(
H
i∈G
⪰i).
(2.4)
A particular kind of solutions (that are also Pareto Optimal) are those that
are extreme in the sense that they are the best choice for one (or more) agents
but might not be good choices for the other agents. The set of these extreme
choices is
ExtG(⪰1, ..., ⪰n) =
0
i∈G
top(⪰i∈G).
(2.5)
Partial Knowledge of Preference Orders. We suppose that only partial informa-
tion about the agents’ preferences (the order relation) is known. We consider
that only a subset of each preference order is known, meaning that we are aware
of some pairwise preferences but not others. Since we assume a context where
preferences are transitive, we assume that also the known preference relation
is transitive, i.e. it is a partial order1. Let ⪰∗
i be the true preference order for
1 If preferences are given in terms of pairwise comparison statements, we consider the
transitive closure of the binary relation containing all such pairwise comparisons.

436
P. Viappiani
agent i (unknown to the system); the knowledge about the agent’s preferences
is encoded by a partial order ⪰i⊆⪰∗
i for each agent i. While the true maximal
elements of each agent i are max(⪰∗
i ), we can only deduce a set max(⪰i) of “cur-
rent” maximal items. Of course, current maximal items might not be maximal
in the “true” complete prefence model. It holds max(⪰∗
i ) ⊆max(⪰i), since a
maximal element according to the ⪰∗
i must be also maximal for the “sparser”
relation ⪰i.
From these partial orders, we consider the “known” aggregate relation of
dominance for group G, ⪰G= 
i∈G ⪰i. Notice that ⪰G is also a subset of ⪰∗
G,
as it is the intersection of partial orders that are included in the underlying true
orders: ⪰G⊆⪰∗
i as ⪰G= 
i∈G ⪰i ⊆
i∈G ⪰∗
i = ⪰∗
G.
The true Pareto optimal choices are ParetoOpt(⪰∗
1, ..., ⪰∗
n) and the extrema
ExtG(⪰∗
1, ..., ⪰∗
n) (one simply substitute ⪰i with ⪰∗
i in Equation 2.4 and in
Equation 2.5); however we only know the “current” Pareto Optimal choices are
ParetoOpt(⪰1, ..., ⪰n) and the current extrema choices are ExtG(⪰1, ..., ⪰n),.
When considering aggregation, the ideal case (but usually rare in practice) is
that all agents agree on the best option (consensus); the agents share a common
maximum element. If we have incomplete knowledge about the individual pref-
erences, we might wonder if, given the current information, a consensus might
exist. Hence, we deﬁne the notion of possible and necessary consensus for a group
of agents, and similarly for Pareto Optimal and Extreme choice.
3
Consensus, Extreme and Pareto Optimal Items: The
Possible and the Necessary
Deﬁnition 1. Given (⪰1, ..., ⪰n), the possible consensus choices PossConsG for
group G are those for which there exists a set of total orders (⪰′
1, ..., ⪰′
n), with
each ⪰′
i extending ⪰i, such that they are a maximum element of the derived
aggregate dominance relation ⪰′
G= 
i ⪰
′
i.
PossConsG(⪰1, ..., ⪰n) = (3.1)
{o ∈C | ∃(⪰′
1, ..., ⪰′
n) : (⪰′
1⊇⪰1), ..., (⪰′
n⊇⪰n) ∧o∈Cons(⪰′
1, ..., ⪰′
n)} = (3.2)
=
0
⪰′
i⊇⪰i∀i∈G
top(
H
⪰′
i) =
0
⪰′
i⊇⪰i∀i∈G
top(⪰′
G). (3.3)
The necessary consensus choices NecConsG for group G are those that are a
maximum element for all complete orders extending ⪰G.
NecConsG(⪰1, ..., ⪰n) =
(3.4)
= {o ∈C | ∀(⪰′
1, ..., ⪰′
n) : (⪰′
1⊇⪰1), ..., (⪰′
n⊇⪰n) ∧o∈top(⪰′
G)} =
(3.5)
=
H
⪰′
i⊇⪰i∀i∈G
top(⪰′
G)
(3.6)

Aggregation of Uncertain Qualitative Preferences for a Group of Agents
437
Proposition 1. Necessary and possible consensus for G can be formulated as:
1. NecConsG = 
i∈G top(⪰i)
2. PossConsG = 
i∈G max(⪰i)
Proof. 1) NecConsG: If a choice belongs to 
i top(⪰i) then (by deﬁnition) it
means that it dominates all other choices for each agent i; it will still be the
maximum in any extension of the preference relations. Moreover, if an option is
dominated in a preference relation ⪰i, it will still dominated in any extension.
Therefore NecConsG is exactly the intersection of all top(⪰i).
2) PossConsG: If an option o1 is dominated wrt agent i (e.g. there is a choice o2
such that o2 ≻i o1), it will also be dominated in any extension of ≻i, and cannot
be a possible consensus. Therefore only options that are maximal elements for all
agents can potentially be a consensus. Consider an option o ∈
i∈G max(⪰i); we
construct an extension of the preference order by breaking all incomparabilities
in favor of o. Since o is a consensus in the extension, the argument follows.
While theoretically interesting, the concept of possible/necessary consensus can
be of limited interest, as frequently agents will have conﬂicting preferences. We
therefore consider weaker notions of optimality.
Deﬁnition 2. The possible extreme choices PossExtG for group G are those for
which there is a set of total orders (⪰′
1, ..., ⪰′
n), with each ⪰′
i extending ⪰i, for
which they are an extreme choice.
PossExtG = {o | ∃(⪰′
1, ..., ⪰′
n) : (⪰′
1⊇⪰1), ..., (⪰′
n⊇⪰n) ∧o∈Ext(⪰′
1, ..., ⪰′
n)}
(3.7)
The necessary extreme choices NecExtG for group G are those that are extreme
for all total orders (⪰′
1, ..., ⪰′
n), with each ⪰′
i extending ⪰i.
NecExtG = {o | ∀(⪰′
1, ..., ⪰′
n) : (⪰′
1⊇⪰1), ..., (⪰′
n⊇⪰n), o∈Ext(⪰′
1, ..., ⪰′
n)}
(3.8)
Proposition 2. Necessary extreme and possible extreme choices for a group G
can be rewritten as follows:
– NecExtG = 
i∈G top(⪰i)
– PossExtG = 
i∈G max(⪰i)
We now consider potential and necessary Pareto optimal choices.
Deﬁnition 3. A choice is a possible Pareto optimal for group G if there exists
a set of total orders (⪰′
1, ..., ⪰′
n), with each ⪰′
i extending ⪰i, for which they are
a Pareto optimal choice.
PossParetoOptG ={o | ∃(⪰′
1, ..., ⪰′
n):(⪰′
1⊇⪰1), ..., (⪰′
n⊇⪰n) ∧o∈max(∩i∈G ⪰′
i)}
A choice is a necessary Pareto optimal for group G if it is a maximal element
of the aggregate preference relation with respect to all extensions of the current
preference orders.
NecParetoOptG ={o | ∀(⪰′
1, ..., ⪰′
n):(⪰′
1⊇⪰1), ..., (⪰′
n⊇⪰n), o∈max(∩i∈G ⪰′
i)}

438
P. Viappiani
An equivalent statement is the following: a choice is necessary Pareto Opti-
mal if there is no option that possibly dominates it. An option o1 is a possible
dominator for o2 if there is a consistent extension of the known preference orders
that make o1 dominate o2. Possible dominance can be formalized accordingly:
Deﬁnition 4. The relation ⪰P D
G
of possible dominance for a group G is such
that
o1 ⪰P D
G
o2 iﬀ∃(⪰′
1, ..., ⪰′
n) : o1 ≻′
G o2
(3.9)
with each ⪰′
i extending ⪰i and ⪰′
G= 
i ⪰′
i being the associated aggregate rela-
tion.
Given this deﬁnition, an item o is a necessary Pareto Optimal if o is a maximal
element of ≻P D (the induced strict relation). The relation ⪰P D can be char-
acterized in terms of the currently known preferences in the following way: an
option o1 is a potential dominator of o2 if, for every agent i, o2 is not strictly
preferred to o1, and o1 and o2 are not equally preferred for all agents.
Proposition 3. The relation of possible dominance (⪰P D
G
) can be written as:
⪰P D
G
=
I 	
i∈G
̸≺i
J
−
	
i∈G
≈i=
I 	
i∈G
⪰i ∪∼i
J
−
	
i∈G
≈i
(3.10)
In the case of underlying linear orders, two options are never equally preferred,
and the expression simpliﬁes to
⪰P D
G
=
I 	
i∈G
⪰i ∪∼i
J
(3.11)
One could make a similar reasoning and deﬁne a relation of necessary dominance
≻ND
G
. Notice that, if o1 ⪰G o2 then o1 ⪰ND
G
o2, therefore ⪰G⊆≻ND
G
. Moreover
it can be shown that it holds exactly that ⪰ND
G
=⪰G.
We can now characterize the sets PossParetoOpt and NecParetoOpt of pos-
sible and necessary Pareto Optimal choices in the following way.
Proposition 4. The set of Possible Pareto Optimal choices coincides with the
set of the current undominated (Pareto Optimal) options given the known pref-
erence orders ⪰i.
PossParetoOpt(⪰1, ..., ⪰n) = max(⪰G) = max
 H
i∈G
⪰i

(3.12)
The set of Necessary Pareto Optimal choices coincides with the maximal choices
with respect to the strict relation of possible dominance.
NecParetoOpt(⪰1, ..., ⪰n) = max(≻P D
G
)
(3.13)
Proof. 1) PossParetoOptG: We show the argument by constructions. Let choice
o1 belong to the Pareto set of the currently known preference orders; o1 ∈
max(⪰G). Then for all preference orders ⪰i construct a complete (linear) or-
der ⪰′
i extending ⪰i such that, for any item o2, if the preference between o1

Aggregation of Uncertain Qualitative Preferences for a Group of Agents
439
and o2 is not known (they are incomparable in ⪰i) the pair o1 ⪰i o2 is added
into ⪰′
i. For pairs not involving o1, pick any assignment consistent with transi-
tivity. o1 is a Pareto Optimal choice in ⪰′
i by construction, hence it belongs to
PossParetoOptG. This shows that max(⪰G)⊆PossParetoOpt. To show that only
Pareto optimal choices wrt ⪰i belong to PossParetoOptG, it is enough to realize
that adding new pairwise preferences cannot break domination ; hence domi-
nated options will remain such in any extensions, and cannot be possible Pareto
optimal choices. Therefore if o1 ̸∈max(⪰G) →o1 ̸∈PossParetoOpt, meaning
that PossParetoOpt⊆max(⪰G). Hence, PossParetoOpt = max(⪰G).
2) NecParetoOptG: straightforward from the previous considerations on ⪰P D
G
and Proposition 3.
We are able now able to characterize the relationship between NecCons,
PossCons, NecExt, PossExt, NecParetoOpt and PossParetoOpt.
Proposition 5. We derive the following taxonomy:
1. NecConsG ⊆NecExtG ⊆NecParetoOptG
2. PossConsG ⊆PossExtG ⊆PossParetoOptG
3. NecConsG ⊆PossConsG, NecExtG ⊆PossExtG and
NecParetoOptG ⊆PossParetoOptG.
Proof.
1. NecConsG = 
i∈G top(⪰i)
⊆

i∈G top(⪰i) = NecExtG ⊆max(≻P D
G
) =
NecParetoOptG; the last inclusion inequality holding as an element o of NecExtG
belongs to top(⪰j) for a given j ∈G (by deﬁnition); o cannot be possibly dominated
(wrt ≻P D= ∩i∈G ⪰i ∪∼i) since it is the maximum in ⪰j, therefore it has to be a
maximal element of ≻P D
G
.
2. PossConsG = 
i∈G max(⪰i) ⊆
i∈G max(⪰i) = PossExtG ⊆max(⪰G) =
= PossParetoOptG
3. Straightforward from deﬁnition of possible and necessary: if an element is, respec-
tively, a consensus, extreme, or Pareto optimal for all extensions of the preference
orders, then in particular there exists an extension for which it is, respectively, a
consensus, extreme or Pareto optimal.
Example 1. Consider the following case with C = {o1, o2, o3}. It is known that
agent 1 prefers option o1 to o2, and option o2 to o3 (that is a linear order, or
chain, o1 ≻1 o2 ≻1 o3). We also know that agent 2 prefers o1 to o3 and also o2 to
o3 (o1 ≻2 o3 and o3 ≻2 o3), but nothing is known about his preference between
o1 and o2. For agent 3 we only know o1 ≻o3, meaning that he prefers o1 to o3.
There is only one maximal element for agent 1 (o1, that is also the maximum
element for this agent), while o1 and o2 are maximal for agents 2 and 3. The
intersection is {o1} and therefore o1 is the only possible consensus choice. There
is no necessary optimal choice as only the preference order of agent 1 has a
maximum. There is one necessary extreme, o1, and the possible extreme options
are o1, o2 (the union of maximal elements). Then the group relation ≻G is such
that o1 ≻G o3 (the pair o1 and o2 is incomparable with respect to ≻G, as well
as the pair o2 and o3). Then o1 and o2 are the maximal elements for ≻G and
the possible Pareto optimal choices for the agents. The relation of potential
dominance is ⪰P D that in this case is a linear order o1 ≻P D o2 ≻P D o3; option
o1 is the only necessary Pareto optimal choice.

440
P. Viappiani
The taxonomy that we obtained (Figure 1) is perhaps not very surprising: the
stricter the notion of optimality (consensus, extreme, Pareto) the smaller the
sets. Sets of possible optimal items are included in the sets of necessary optimal
items. However, our theoretical characterisation is useful as it allows to compute
the possible and necessary optimal items reasoning only with respect to the cur-
rent available preference information (the ⪰i) without the need to individually
consider all possible extensions of the currently available preference information.
In particular, the set of possible Pareto optimal choices PossParetoOpt coincides
with the current Pareto optimal set, the set of maximal items with respect to
the group dominance relation computed with the currently available preferences.
The set of necessary Pareto optimal choices are those items that are non domi-
nated with respect to the strict relation of possible dominance (≻P D), that can
be expressed in a convenient way thanks to Equation 3.13. Furthermore the in-
tersection of the maximal elements of the ⪰i coincides PossCons and the union
of the maximal elements of the ⪰i coincides with PossExt.
PossParetoOpt
PossExt
PossCons
NecParetoOpt
NecExt
NecCons

@
@
@
@
@
@
@
@
I

@
@
@
@
@
@
@
@
I

@
@
@
@
@
@
@
@
I

Fig. 1. Inclusion membership between the diﬀerent classes
4
Cardinality
The previous section provided a theoretical characterization of diﬃerent kinds
of “optimality” (consensus, extreme choices and Pareto Optimal choices) when
dealing with partial binary preference information, providing a mathematical for-
mulation of possible and necessary optimal choices under the diﬃerent semantics.
Here we perform a number of simulations in order to assess the cardinality of the
sets NecCons, PossCons, NecExt, PossExt, NecParetoOpt and PossParetoOpt
in practical circumstances.
– We randomly generate (uniformly) a permutation of the n elements for each
agent of the m agents, this is assumed to be their true preference ranking.
– For each agent, from the incidence matrix representing the preference rela-
tion, we randomly cancel a fraction f of pairs in relation. We compute the
transitive closure of the relation.
– From the obtained partial orders, we compute the sets NecCons, PossCons,
NecExt, PossExt, NecParetoOpt and PossParetoOpt.

Aggregation of Uncertain Qualitative Preferences for a Group of Agents
441
In the table below we report the cardinalty of these sets averaged over 100 runs,
for some values of n, m and f. Further studies might consider cardinality under
diﬃerent ranking probability models (for instance considering the models in [3]).
n m
f
|NecCons| |PossCons| |NecExt| |PossExt| |NecParetoOpt| |PossParetoOpt|
2 3 0.20
0.40
0.40
1.60
1.60
1.78
1.78
2 3 0.40
0.19
0.63
1.16
2.02
1.34
2.13
2 3 0.60
0.15
0.56
1.21
2.08
1.36
2.25
2 3 0.80
0.00
1.33
0.00
2.67
0.20
2.86
2 3 0.90
0.00
1.40
0.00
2.60
0.16
2.78
2 3 0.95
0.00
1.42
0.00
2.58
0.15
2.83
2 4 0.20
0.24
0.43
1.37
1.96
1.75
2.31
2 4 0.40
0.12
0.52
1.12
2.24
1.47
2.60
2 4 0.60
0.02
0.80
0.51
2.71
0.70
3.20
2 4 0.80
0.00
1.26
0.00
3.29
0.10
3.64
2 4 0.90
0.00
2.18
0.00
3.82
0.00
3.94
2 4 0.95
0.00
2.29
0.00
3.71
0.00
3.89
2 5 0.20
0.10
0.25
1.53
2.12
1.97
2.62
2 5 0.40
0.02
0.47
0.92
2.64
1.34
3.29
2 5 0.60
0.00
0.81
0.08
3.55
0.32
4.09
2 5 0.80
0.00
2.08
0.00
4.42
0.02
4.75
2 5 0.90
0.00
3.25
0.00
4.75
0.00
4.93
2 5 0.95
0.00
3.19
0.00
4.81
0.00
4.94
n m
f
|NecCons| |PossCons| |NecExt| |PossExt| |NecParetoOpt| |PossParetoOpt|
3 3 0.20
0.09
0.09
2.12
2.12
2.40
2.40
3 3 0.40
0.02
0.25
1.68
2.48
1.89
2.69
3 3 0.60
0.01
0.20
1.58
2.55
1.97
2.78
3 3 0.80
0.00
0.83
0.00
2.91
0.43
2.97
3 3 0.90
0.00
0.87
0.00
2.88
0.38
2.98
3 3 0.95
0.00
0.94
0.00
2.87
0.34
2.94
3 4 0.20
0.06
0.19
1.86
2.46
2.39
2.94
3 4 0.40
0.04
0.26
1.39
2.83
2.06
3.25
3 4 0.60
0.00
0.38
0.61
3.25
1.32
3.65
3 4 0.80
0.00
0.76
0.00
3.60
0.44
3.90
3 4 0.90
0.00
1.76
0.00
3.90
0.01
3.95
3 4 0.95
0.00
1.61
0.00
3.91
0.01
3.99
3 5 0.20
0.03
0.07
2.04
2.72
2.89
3.56
3 5 0.40
0.00
0.14
1.10
3.42
2.02
4.07
3 5 0.60
0.00
0.39
0.24
4.11
0.90
4.72
3 5 0.80
0.00
1.37
0.00
4.66
0.03
4.95
3 5 0.90
0.00
2.57
0.00
4.95
0.00
5.00
3 5 0.95
0.00
2.64
0.00
4.92
0.00
4.99
n m
f
|NecCons| |PossCons| |NecExt| |PossExt| |NecParetoOpt| |PossParetoOpt|
4 3 0.20
0.07
0.07
2.28
2.28
2.57
2.57
4 3 0.40
0.00
0.17
1.82
2.63
2.14
2.81
4 3 0.60
0.01
0.17
1.82
2.62
2.15
2.82
4 3 0.80
0.00
0.63
0.00
2.98
0.69
3.00
4 3 0.90
0.00
0.64
0.00
2.93
0.66
2.99
4 3 0.95
0.00
0.69
0.00
2.95
0.72
3.00
4 4 0.20
0.00
0.04
2.37
2.95
3.14
3.57
4 4 0.40
0.00
0.06
1.79
3.29
2.63
3.67
4 4 0.60
0.00
0.17
0.72
3.61
1.86
3.88
4 4 0.80
0.00
0.31
0.00
3.85
0.83
3.98
4 4 0.90
0.00
1.31
0.00
3.96
0.08
4.00
4 4 0.95
0.00
1.29
0.00
3.98
0.04
4.00
4 5 0.20
0.00
0.03
2.56
3.18
3.73
4.27
4 5 0.40
0.01
0.04
1.60
3.85
2.93
4.61
4 5 0.60
0.00
0.14
0.30
4.48
1.35
4.88
4 5 0.80
0.00
0.89
0.00
4.92
0.07
5.00
4 5 0.90
0.00
2.02
0.00
4.99
0.00
5.00
4 5 0.95
0.00
2.06
0.00
4.98
0.00
5.00
5
Current Works
This section discusses current work dealing with elicitation of qualitative pref-
erences and with the identiﬁcation of “fair” choices.
Elicitation. We are interested in interactive settings, where the agents provide
new information to the system (statements of the type o1 ⪰o2) at each step; this
results in an update of the preference order ⪰i:=⪰i ∪(o1 ⪰o2) for the agent i
who entered new information. The challenge is to deﬁne eﬃective strategies for
elicitation, that pick the items to compare in some smart way. Of course, we want

442
P. Viappiani
to ask agents to compare two items that are currently incomparable given the
available information. One heuristic strategy would be to consider queries about
maximal items in the currently known preference relation. When choosing to
which agent ask queries, one heuristic could consist in targeting the agent whose
current preference relation is sparser (the lowest number of pairwise comparison
is known).
We believe that the development of eﬀcient query strategies, as well as practi-
cal evaluation and comparison of diﬃerent techniques, is an important next step
involving a substantial research eﬃort. One challenge would be the deﬁnition of
suitable measures of the value of information [7] of a candidate query in this
intrinsically qualititative setting.
Fairness. We aim to provide a characterisation of fairness in a qualitative way
(in a way similar to [2]). Intuitively, a choice is more fair than another if it is less
“extreme” in the satisfaction of the agents. Fairness is a well developed concept
in numerical approaches, where one can consider the least satisﬁed agent, or
more reﬁned aggregators such as OWA. However, typical fairness measures are
not meaningful in this context, as we deal with qualitative preferences. One could
of course map choices according to their position in each agent’s ranking, and
then consider fairness using numerical methods, but then the advantages of a
qualitative approach would disappear.
We want to work on partial preference orders without the need of a distance-
from-equality measure. In order to deﬁne what equity means in this qualitative
setting, we propose to use the notion of reference point. The reference point e
expresses an intermediate level of preference satisfaction, encoding a somewhat
intermediate level of satisfaction. Note that the option at which the level e is
assessed, can be diﬃerent for each agent (for instance, the e might corresponds
to the second best choice for agent 1, while to the third choice for agent 2).
This point is such that, if each agent could achieve e, this would correspond to
a maximally fair situation, as all agents will be equality satisﬁed.
Deﬁnition 5. A choice o1 is more fair than choice o2 with respect to agent i
and j, written as o1 ⪰F
(i,j) o2, if o1 ⪰i o2 ⪰i e and if e ⪰j o2 ⪰j o2
From ⪰F
(i,j) a single fairness relation ⪰F for all agents can be obtained by inter-
section: ⪰F = 
(i,j)∈G ⪰F
(i,j). In order to obtain a practical way to rank items
accounting for both preferences and fairness, we now combine the two preference
relations ⪰G and ⪰F into a single preference order. A choice o1 is preferred to
another choice o2 if it is either that o1 dominates o2 or if o1 is more fair than
o2.
Deﬁnition 6. The combined dominance-fairness preference relation ⪰C is de-
ﬁned as ⪰C=⪰G ∪⪰F .
We propose to consider the Pareto optimal choices wrt the obtained order ⪰C:
these solutions might be considered candidate choices for the groupwise deci-
sion making. These solutions can be considered the qualitative counterpart of

Aggregation of Uncertain Qualitative Preferences for a Group of Agents
443
the concept of well balanced solutions in multi-objective optimization (Lorentz
dominance). Future works consist in an investigation of the mathematical prop-
erties of the relation ⪰C and in practical evaluation (including simulations).
6
Conclusions
In this paper we considered the case of partially known preferences of diﬃerent
agents, considering purely qualitative preferences (partial preference relations).
We derived a mathematical characterisation and a taxonomy of the possible and
necessary optimal choices, according to three diﬃerent notions of optimality:
shared optimality, extrema and Pareto optimality. In simulations we reported
the cardinality measures of these sets in a number of diﬃerent settings.
Notice that we have considered a setting with multiple agents, but this work
can be very easily adapted to a multi-criteria setting: in this other setting each
i would refer to a diﬃerent criteria and G to the group of criteria. This work is
related to works in computational social choice theory, interested in establishing
how hard is to compute necessary and possible winners given common election
rules [8]. Researchers in Operations Research [4] have also considered necessary
and possibly optimal items with several feasible utility functions.
Acknowledgments. The author is supported by the projects BR4CP (con-
tract n.ANR-11-BS02-008) and LARDONS (contract n. ANR-10-BLAN-0215)
ﬁnanced by the Agence Nationale de la Recherche (ANR).
References
1. Aleskerov, F., Bouyssou, D., Monjardet, B.: Utility Maximization, Choice and Pref-
erence, 2nd edn. Studies in economic theory. Springer (April 2007) (Anglais)
2. Cowell, F., Flachaire, E.: Inequality with ordinal data (2012),
http://www.hec.ca/iea/seminaires/130226_emmanuel_flachaire.pdf
(manuscript available online)
3. Critchlow, D.E., Fligner, M.A., Verducci, J.S.: Probability models on rankings. Jour-
nal of Mathematical Psychology 35(3), 294–318 (1991)
4. Greco, S., Mousseau, V., Slowinski, R.: Ordinal regression revisited: Multiple criteria
ranking using a set of additive value functions. European Journal of Operational
Research 191(2), 416–436 (2008)
5. Kaci, S.: Working with preferences: Less is more. Cognitive Technologies. Springer
(2011)
6. Roubens, M., Vincke, P.: Preference modelling. Lecture Notes in Economics and
Mathematical Systems (250) (1985)
7. Viappiani, P., Boutilier, C.: Optimal Bayesian recommendation sets and myopically
optimal choice query sets. In: Advances in Neural Information Processing Systems
23 (NIPS), pp. 2352–2360. MIT Press (2010)
8. Xia, L., Conitzer, V.: Determining possible and necessary winners under common
voting rules given partial orders. In: Proceedings of the Twenty-Third AAAI Con-
ference on Artiﬁcial Intelligence (AAAI), pp. 196–201 (2008)

Choquet Expected Utility Representation
of Preferences on Generalized Lotteries
Giulianella Coletti1, Davide Petturiti2, and Barbara Vantaggi2
1 Dip. Matematica e Informatica, Universit`a di Perugia, Italy
coletti@dmi.unipg.it
2 Dip. S.B.A.I., Universit`a di Roma “La Sapienza”, Italy
{barbara.vantaggi,davide.petturiti}@sbai.uniroma1.it
Abstract. The classical von Neumann–Morgenstern’s notion of lottery
is generalized by replacing a probability distribution on a ﬁnite support
with a belief function on the power set of the support. Given a partial
preference relation on a ﬁnite set of generalized lotteries, a necessary and
suﬃcient condition (weak rationality) is provided for its representation
as a Choquet expected utility of a strictly increasing utility function.
Keywords: Generalized lottery, preference relation, belief function,
probability envelope, Choquet expected utility.
1
Introduction
In decision problems as well as in automated reasoning, especially in situations
of incomplete and revisable information, both in the classical expected utility
(EU) [20,9,15] and in the Choquet expected utility (CEU) [11,17,18,6] (see also
[1,14,22,21]) frameworks it can be diﬀcult to construct the utility function u
and even to test if the preferences agree with an EU (or a CEU). In fact, to
ﬁnd the utility u the classical methods ask for comparisons between “lotteries”
and “certainty equivalent” or, in any case, comparisons among particular large
classes of lotteries (for a discussion in the EU framework see [12]). For that, the
decision maker is often forced to make comparisons which have little or nothing
to do with the given problem, having to choose between risky prospects and
certainty.
In [2], referring to the EU model, a diﬃerent approach (based on a “rationality
principle”) is proposed: it does not need all these non-natural comparisons but,
instead, it can work by considering only the (few) lotteries and comparisons
of interest. Moreover, when new information is introduced, the same principle
assures that the preference relation can be extended maintaining rationality, and,
even more, the principle suggests how to extend it. The “rationality principle”
can be summarized as follows: it is not possible to obtain the same lottery by
combining in the same way two groups of lotteries, if the ﬁrst ones are strictly
preferred to the second ones.
The aim of this paper is to propose a similar approach for the CEU model
by generalizing the usual deﬁnition of lottery. In detail, a generalized lottery L
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 444–453, 2014.
c
⃝Springer International Publishing Switzerland 2014

CEU Representation of Preferences on Generalized Lotteries
445
(or g-lottery for short) is a random quantity with a ﬁnite support XL endowed
with a Dempster-Shafer belief function BelL [5,16,19] (or, equivalently, a basic
assignment mL) deﬁned on the power set ℘(XL). We recall that, in particular
probabilistic inferential problems, the belief function can be obtained either as a
lower envelope of a family of probabilities (see next Example 1, generalizing the
famous Ellsberg “paradox” [7]) or as a coherent lower extension of a probability
related to a set of lotteries with diﬃerent support (see for instance [5,3,8,13,4]).
The “weak rationality principle” proposed here is not the direct generalization
of the “rationality principle” given for probability, simply obtained by changing
“lotteries” with “generalized lotteries”: actually, the condition derived in this
way is only necessary for the representability of the preference by a CEU. Such
a principle is based on the following property: if the elements of the set X =
{x1, . . . , xn} resulting by the union of the supports of the considered g-lotteries is
totally ordered as x1 < . . . < xn, then for every g-lottery L the Choquet integral
of any strictly increasing utility function u : X →R, not only is a weighted
average (as observed in [10]), but the weights have a clear meaning. In fact, for
the least preferred prize x1 the weight is the sum of the values of mL on the events
implied by the event {L = x1}, for x2 it is the sum of the values of mL on the
events implied by the event {L = x2} but not by {L = x1}, and so on. This allows
to map every g-lottery L to a “standard” lottery whose probability distribution
is constructed (following a pessimistic approach) through the aggregated basic
assignment ML.
The “weak rationality principle” turns out to be a necessary and suﬀcient
condition for the existence of a strictly increasing u : X →R whose CEU repre-
sents our preferences on a ﬁnite set L of g-lotteries, under a natural assumption
of agreement of the preference relation with the order of X.
2
A Motivating Example
To motivate the topic dealt with in this paper we introduced the following ex-
ample, which is inspired to the well-known Ellsberg’s paradox [7].
Example 1. Consider the following hypothetical experiment. Let us take two
urns, say U1 and U2, from which we are asked to draw a ball each. U1 contains
1
3 of white (w) balls and the remaining balls are black (b) and red (r), but in
a ratio entirely unknown to us, analogously, U2 contains
1
4 of green (g) balls
and the remaining balls are yellow (y) and orange (o), but in a ratio entirely
unknown to us.
In light of the given information, the composition of U1 singles out a class of
probability measures P1 = {P θ} on the power set ℘(S1) of S1 = {w, b, r} s.t.
P θ({w}) = 1
3, P θ({b}) = Λ, P θ({r}) = 2
3 −Λ, with Λ ∈
I
0, 2
3
J
. Analogously, for
the composition of U2 we have the class P2 = {P λ} on ℘(S2) with S2 = {g, y, o}
s.t. P λ({g}) = 1
4, P λ({y}) = α, P λ({o}) = 3
4 −α, with α ∈
I
0, 3
4
J
.
Concerning the ball drawn from U1 and the one drawn from U2, the following
gambles are considered:

446
G. Coletti, D. Petturiti, and B. Vantaggi
w
b
r
L1 100e 0e
0e
L2
0e
0e
100e
L3
0e
100e 100e
L4 100e 100e 0e
g
y
o
G1 100e 10e 10e
G2 10e
10e 100e
G3 10e 100e 100e
G4 100e 100e 10e
If we express the strict preferences L2 ≺L1, L4 ≺L3, then for no value of Λ
there exists a function u : {0, 100} →R s.t. its expected value on the Li’s w.r.t.
P θ represents our preferences on the Li’s. Indeed, putting w1 = u(0) and w2 =
u(100), both the following inequalities must hold 1
3w1+Λw1+
 2
3 −Λ

w2 < 1
3w2+
Λw1 +
 2
3 −Λ

w1 and 1
3w2 + Λw2 +
 2
3 −Λ

w1 < 1
3w1 + Λw2 +
 2
3 −Λ

w2, from
which, summing memberwise, we get w1 + w2 < w1 + w2, i.e., a contradiction.
The same can be proven if we express the strict preferences G2 ≺G1, G4 ≺G3.
Now take P 1 = min P1 and P 2 = min P2, where the minimum is intended
pointwise on the elements of ℘(S1) and ℘(S2), obtaining:
℘(S1) ∅{w} {b} {r} {w, b} {w, r} {b, r} S1
P 1
0
1
3
0
0
1
3
1
3
2
3
1
℘(S2) ∅{g} {y} {o} {g, y} {g, o} {y, o} S2
P 2
0
1
4
0
0
1
4
1
4
3
4
1
It is easily veriﬁed that both P 1 and P 2 are belief functions, i.e., n-monotone
Choquet capacities, for every n ≥2 (see next equation 1).
In this case, for any strictly increasing function u : {0, 100} →R we have
that the Choquet integral of u on the Li’s w.r.t. P 1 represents our preferences
L2 ≺L1, L4 ≺L3 and coincides with the minimum of the expected utilities
of u on the Li’s w.r.t. the class P1. Indeed, denoting w1 = u(0) and w2 =
u(100) and CEU(Li) =
3
Ch u(Li)dP 1 we get CEU(L1) = CEU(L4) = 2
3w1 +
1
3w2, CEU(L2) = w1, CEU(L3) = 1
3w1 + 2
3w2, thus CEU(L2) < CEU(L1) and
CEU(L4) < CEU(L3) hold whenever w1 < w2. The same can be proven for the
preferences G2 ≺G1, G4 ≺G3.
3
Numerical Model of Reference
Let X be a ﬁnite set of states of nature and denote by ℘(X) the power set of X.
We recall that a belief function Bel [5,16,19] on an algebra of events A ⊆℘(X) is
a function such that Bel(∅) = 0, Bel(X) = 1 and satisfying the n-monotonicity
property for every n ≥2, i.e., for every A1, . . . , An ∈A,
Bel

 n
0
i=1
Ai

≥

∅̸=I⊆{1,...,n}
(−1)|I|+1Bel

H
i∈I
Ai

.
(1)
A belief function Bel on A is completely singled out by its M¨obius inverse,
deﬁned for every A ∈A as
m(A) =

B⊆A
(−1)|A\B|Bel(B).

CEU Representation of Preferences on Generalized Lotteries
447
Such a function, usually called basic (probability) assignment, is a function m :
A →[0, 1] satisfying m(∅) = 0 and 
A∈A m(A) = 1, and is such that for every
A ∈A
Bel(A) =

B⊆A
m(B).
(2)
A set A in A is a focal element for m (and so also for the corresponding Bel)
whenever m(A) > 0.
In the classical von Neumann–Morgenstern theory [20] a lottery L consists of
a probability distribution on a ﬁnite support XL, which is an arbitrary ﬁnite set
of prizes or consequences.
In this paper we adopt a generalized notion of lottery L, by assuming that a
belief function BelL is assigned on the power set ℘(XL) of XL.
Deﬁnition 1. A generalized lottery, or g-lottery for short, on a ﬁnite set
XL is a pair L = (℘(XL), BelL) where BelL is a belief function on ℘(XL).
Let us notice that, a g-lottery L = (℘(XL), BelL) could be equivalently deﬁned
as L = (℘(XL), mL), where mL is the basic assignment associated to BelL.
We stress that this deﬁnition of g-lottery generalizes the classical one in which
mL(A) = 0 for any A ∈℘(XL) with card A > 1.
For example, a g-lottery L on XL = {x1, x2, x3} can be expressed as
L =
{x1} {x2} {x3} {x1, x2} {x1, x3} {x2, x3} {x1, x2, x3}
b1
b2
b3
b12
b13
b23
b123
	
where the belief function BelL on ℘(XL) is such that bI = BelL({xi : i ∈I})
for every I ⊆{1, 2, 3}. Notice that as one always has BelL(∅) = mL(∅) = 0,
the empty set is not reported in the tabular expression of L. An equivalent
representation of previous g-lottery is obtained through the basic assignment
mL associated to BelL (where mI = mL({xi : i ∈I}) for every I ⊆{1, 2, 3})
L =

{x1} {x2} {x3} {x1, x2} {x1, x3} {x2, x3} {x1, x2, x3}
m1
m2
m3
m12
m13
m23
m123
	
.
Given a ﬁnite set L of g-lotteries, let X = {XL : L ∈L}. Then, any g-
lottery L on XL with belief function BelL can be rewritten as a g-lottery on X
by deﬁning a suitable extension Bel′
L of BelL.
Proposition 1. Let L = (℘(XL), BelL) be a g-lottery on XL. Then for any
ﬁnite X ⊇XL there exists a unique belief function Bel′
L on ℘(X) with the same
focal elements of BelL and such that Bel′
L|℘(XL) = BelL.
Proof. The extension Bel′
L is deﬁned through the corresponding m′
L. For every
A ∈℘(X) we put m′
L(A) = mL(A) if A ∈℘(XL) and m′
L(A) = 0 otherwise.
The function m′
L is easily seen to be a basic assignment on ℘(X), moreover, the
corresponding belief function Bel′
L on ℘(X) is an extension of BelL and has the
same focal elements.
□

448
G. Coletti, D. Petturiti, and B. Vantaggi
Given L1, . . . , Lt ∈L, all rewritten on X, and a real vector k = (k1, . . . , kt)
with ki ≥0 (i = 1, . . . , t) and t
i=1 ki = 1, the convex combination of L1, . . . , Lt
according to k is deﬁned as
k(L1, . . . , Lt) =

A
t
i=1 kimLi(A)
	
for every A ∈℘(X) \ {∅}.
(3)
Since the convex combination of belief functions (basic assignments) on ℘(X) is
a belief function (basic assignment) on ℘(X), k(L1, . . . , Lt) is a g-lottery on X.
For every A ∈℘(X)\{∅}, there exists a degenerate g-lottery γA on X such that
mδA(A) = 1, and, moreover, every g-lottery L with focal elements A1, . . . , Ak
can be expressed as k(γA1, . . . , γAk) with k = (mL(A1), . . . , mL(Ak)).
4
Preferences over a Set of Generalized Lotteries
Consider a ﬁnite set L of g-lotteries with X = {XL : L ∈L} and assume X
is totally ordered by the relation <, which is a quite natural condition thinking
at elements of X as money payoﬃs.
Let ≾be a preference/indiﬀerence relation on L . For every L, L′ ∈L the
assertion that “L is indiﬃerent to L′”, denoted by L ∼L′, summarizes the two
assertions L ≾L′ and L′ ≾L. Observe that not all the pairs of g-lotteries are
necessarily compared. An additional strict preference relation can be elicitated
by assertions such as “L is strictly preferred to L′”, denoted by L ≺L′. Let
≺∗be the asymmetric relation formally deduced from ≾, namely ≺∗=≾\ ∼. If
the pair of relations (≾, ≺) represents the opinion of the decision maker, then
it is natural to have ≺⊂≺∗: in fact, it is possible that, at an initial stage of
judgement, the decision maker has not decided yet if L ≺L′ or L ∼L′ and he
expresses his opinion only by L ≾L′. Obviously if ≾is complete then ≺=≺∗
and so for every pair (L, L′) either L ≺L′ or L ∼L′.
We call the pair (≾, ≺) strengthened preference relation if ≺is not empty.
We say that a function U : L →R represents (or agrees with) (≾, ≺) if, for
every L, L′ ∈L
L ≾L′ ⇒U(L) ≤U(L′) and L ≺L′ ⇒U(L) < U(L′).
(4)
In analogy with [2], given (≾, ≺) on L, our aim is to ﬁnd a necessary and
suﬀcient condition for the existence of a utility function u : X →R such that
the Choquet expected utility of g-lotteries in L represents (≾, ≺). In particular,
with the money payoﬃs interpretation in mind we search for a strictly increasing
u. To reach our goal we need ﬁrst to deﬁne the Choquet expected utility of a
g-lottery.
Deﬁnition 2. Let u : X →R be a utility function, then the Choquet ex-
pected utility, or CEU for short, of a g-lottery L = (℘(X), BelL) is
CEU(L) =
 0
−∞
(BelL({x : u(x) ≥t}) −1)dt +
 +∞
0
BelL({x : u(x) ≥t})dt.
(5)

CEU Representation of Preferences on Generalized Lotteries
449
Since X = {x1, . . . , xn} is totally ordered as x1 < . . . < xn, we can deﬁne the
aggregated basic assigment of a g-lottery L, for every xi ∈X,
ML(xi) =

xi∈B⊆Ei
mL(B),
(6)
where Ei = {xi, . . . , xn} for i = 1, . . . , n. Note that ML(xi) ≥0 for every xi ∈X
and n
i=1 ML(xi) = 1, thus ML determines a probability distribution on X.
The next axiom requires that it is not possible to obtain two g-lotteries hav-
ing the same aggregated basic assignment, by combining in the same way two
groups of g-lotteries, if each g-lottery in the ﬁrst group is not preferred to the
corresponding one in the second group, and at least a preference is strict.
Deﬁnition 3. A strengthened preference relation (≾, ≺) on a set L of g-lotteries
is said to be weakly rational if it satisﬁes the following condition:
(WR) For all h ∈N and Li, L′
i ∈L with Li ≾L′
i (i = 1, . . . , h), if
k(ML1, . . . , MLh) = k(ML′
1, . . . , ML′
h)
with k = (k1, . . . , kh), ki > 0 (i = 1, . . . , h) and h
i=1 ki = 1, then it can
be Li ≺L′
i for no i = 1, . . . , h. In particular, if ≾is complete, it must be
Li ∼L′
i for every i = 1, . . . , h.
Note that the convex combination referred to in condition (WR) is the
usual one involving probability distributions on X. Moreover, it is easily proven
that if k(L1, . . . , Lh) = k(L′
1, . . . , L′
h), then it also holds k(ML1, . . . , MLh) =
k(ML′
1, . . . , ML′
h) but the converse is generally not true as shown in next exam-
ple.
Example 2. Let X = {x1, x2} with x1 < x2 and consider the g-lotteries
L1 =
 {x1} {x2} {x1, x2}
1
4
3
4
0

, L′
1 =
 {x1} {x2} {x1, x2}
1
3
2
3
0

,
L2 =

{x1} {x2} {x1, x2}
0
2
3
1
3

, L′
2 =

{x1} {x2} {x1, x2}
0
3
4
1
4

,
with the preferences L1 ≾L′
1 and L2 ≾L′
2. There is no k ∈[0, 1] such that
kL1 + (1 −k)L2 = kL′
1 + (1 −k)L′
2, indeed, the following system
⎧
⎨
⎩
k 1
4 = k 1
3
k 3
4 + (1 −k) 2
3 = k 2
3 + (1 −k) 3
4
(1 −k) 1
3 = (1 −k) 1
4
has not solution. Nevertheless, considering the aggregated basic assignments of
L1, L′
1, L2, L′
2 we have
ML1 =
 x1 x2
1
4
3
4

, ML′
1 =
 x1 x2
1
3
2
3

, ML2 =
 x1 x2
1
3
2
3

, ML′
2 =
 x1 x2
1
4
3
4

,
for which we have 1
2ML1 + 1
2ML2 = 1
2ML′
1 + 1
2ML′
2.

450
G. Coletti, D. Petturiti, and B. Vantaggi
In order to get a strictly increasing u : X →R we have to require that L
contains the set of degenerate g-lotteries on singletons L0 = {γ{x} : x ∈X}
and that γ{x} ≺γ{x′} when x < x′, for x, x′ ∈X. Actually, the decision maker
is not asked to provide such a set of preferences, but in this case the initial
partial preference (≾, ≺) on L must be extended in order to reach this technical
condition and, of course, the decision maker is asked to accept such an extension.
The following theorem proves that (WR) is a necessary and suﬀcient condi-
tion for the existence of a strictly increasing utility function u whose Choquet
expected value on g-lotteries represents (≾, ≺), moreover its proof provides a
procedure to compute such a u.
Theorem 1. Let L be a ﬁnite set of g-lotteries, X = {XL : L ∈L} with
X totally ordered by <, and (≾, ≺) a strengthened preference relation on L.
Assume L0 ⊆L and for every x, x′ ∈X, x < x′ implies γ{x} ≺γ{x′}. The
following statements are equivalent:
(i) (≾, ≺) is weakly rational (i.e., it satisﬁes (WR));
(ii) (≾, ≺) is representable by a CEU of a strictly increasing function u : X →R
(unique up to a positive linear transformation).
Proof. Let X = {x1, . . . , xn} with x1 < . . . < xn and assume all g-lotteries in L
are rewritten on X. Introduce the collections S = {(Lj, L′
j) : Lj ≺L′
j, Lj, L′
j ∈
L} and R = {(Gh, G′
h) : Gh ≾G′
h, Gh, G′
h ∈L} with s = card S and r = card R.
(ii) ⇒(i). Condition (ii) holds if and only if there are n real numbers wi =
u(xi), with w1 < . . . < wn, s.t. for all (Lj, L′
j) ∈S we have CEU(Lj) < CEU(L′
j),
and for all (Gh, G′
h) ∈R we have CEU(Gh) ≤CEU(G′
h).
Setting Ei = {xi, . . . , xn} (i = 1, . . . , n) and En+1 = ∅, for every g-lottery
L ∈L it holds (see [6])
CEU(L) =
n

i=1
wi [BelL(Ei) −BelL(Ei+1)]
=
n

i=1
wi
⎡
⎣
B⊆Ei
mL(B) −

B⊆Ei+1
mL(B)
⎤
⎦=
n

i=1
wiML(xi).
Hence, condition (ii) is equivalent to the existence of a (n × 1) column vector w
which is solution of the following system
S :

Aw > 0,
Bw ≥0,
where A = (aj) and B = (bh) are, respectively, (s × n) and (r × n) real matrices
with rows aj = ML′
j−MLj for j = 1, . . . , s, and bh = MG′
h−MGh for h = 1, . . . , r.
Due to the homogeneity of S we can restrict to w ≥0, so by a known al-
ternative theorem (see, e.g., [9]) the existence of a non-negative solution of S is
equivalent to the non-solvability of the following system
S′ :
⎧
⎨
⎩
yA + zB ≤0,
y, z ≥0,
y ̸= 0,

CEU Representation of Preferences on Generalized Lotteries
451
where y and z are, respectively, (1 × s) and (1 × r) unknown row vectors. If y, z
is a solution of system S′ then summing memberwise the inequalities related
to yA + zB ≤0 we get that the resulting inequality is veriﬁed as 0 = 0, thus
in system S′ we can write yA + zB = 0. Now, let k = s
i=1 yi + r
i=1 zi and
k the (1 × s + r) row vector with ki = yi
k , for i = 1, . . . , s, and ks+i = zi
k for
i = 1, . . . , r. The solution y, z of system S′ implies
k(ML1, . . . , MLs, MG1, . . . , MGr) = k(ML′
1, . . . , ML′s, MG′
1, . . . , MG′r),
which can be restricted to the positive ki’s, and since ki > 0 for at least one
index i ∈{1, . . ., s}, this implies condition (WR) is violated.
(i) ⇒(ii). Suppose (WR) holds. In this case, considering a convex combina-
tion k(ML1, . . . , MLs, MG1, . . . , MGr) = k(ML′
1, . . . , ML′s, MG′
1, . . . , MG′r) where
some of the ki’s can be 0, it must be that ki = 0 for i = 1, . . . , s, thus system
S′ cannot have solution, while S has solution w. The hypothesis L0 ⊆L and
for every x, x′ ∈X, x < x′ implies γ{x} ≺γ{x′}, assures that w1 < . . . < wn.
Hence, u(xi) = wi, i = 1, . . . , n, is a strictly increasing utility function on X
whose CEU represents (≾, ≺). Moreover, as any positive linear transformation
of w produces another solution of system S it follows the unicity of u up to a
positive linear transformation.
□
Example 3. Consider again the situation described in Example 1 and suppose to
toss a fair coin and to choose among L1 and G1 depending on the face shown by
the coin. In analogy, suppose to choose among L2 and G1 with a totally similar
experiment. Let us denote with F1 and F2 the results of the two experiments.
We can transport the belief functions deﬁned in Example 1 to the sets of
prizes of each gamble Li’s and Gi’, thus we obtain
L1 =
{0} {100} {0, 100}
2
3
1
3
1
	
, L2 =
{0} {100} {0, 100}
1
3
0
1
	
,
G1 =
{10} {100} {10, 100}
3
4
1
4
1
	
.
In order to express F1 and F2 we need to properly combine L1, L2 and G1 and
for this we have to rewrite them on the same set of prizes {0, 10, 100}
L1 =
 {0} {10} {100} {0, 10} {0, 100} {10, 100} {0, 10, 100}
2
3
0
1
3
2
3
1
1
3
1

,
L2 =

{0} {10} {100} {0, 10} {0, 100} {10, 100} {0, 10, 100}
1
3
0
0
1
3
1
0
1

,
G1 =
 {0} {10} {100} {0, 10} {0, 100} {10, 100} {0, 10, 100}
0
3
4
1
4
3
4
1
4
1
1

.
L1, L2 and G1 can be simply regarded as belief functions on the same ﬁeld,
thus F1 and F2 can be deﬁned as the convex combinations F1 = 1
2L1 + 1
2G1 and
F2 = 1
2L2 + 1
2G1, obtaining
F1 =
 {0} {10} {100} {0, 10} {0, 100} {10, 100} {0, 10, 100}
8
24
9
24
7
24
17
24
15
24
16
24
1

,

452
G. Coletti, D. Petturiti, and B. Vantaggi
F2 =
 {0} {10} {100} {0, 10} {0, 100} {10, 100} {0, 10, 100}
4
24
9
24
3
24
13
24
15
24
12
24
1

.
It is easily proven that for every strictly increasing u : {0, 10, 100} →R
the strict preferences L2 ≺L1, L4 ≺L3, G2 ≺G1, G4 ≺G3 are represented
by their Choquet expected utility. Nevertheless, if we consider the further strict
preference F1 ≺F2 then there is no strictly increasing u : {0, 10, 100} →R whose
Choquet expected utility represents our preference. Indeed, denoting w1 = u(0),
w2 = u(10), w3 = u(100), one would have CEU(F1) = 1
3w1 + 3
8w2 +
7
24w3 and
CEU(F2) = 1
2w1 + 3
8w2 + 1
8w3, thus CEU(F1) < CEU(F2) would imply w3 < w1
and so a contradiction with the constraint w1 < w2 < w3.
In particular, assuming the natural preferences γ{0} ≺γ{10} and γ{10} ≺
γ{100}, and introducing the aggregated basic assignments
MF1 =
 0 10 100
1
3
3
8
7
24

, MF2 =
 0 10 100
1
2
3
8
1
8

, Mδ{0} =
 0 10 100
1 0
0

,
Mδ{10} =
 0 10 100
0 1
0

, Mδ{100} =
 0 10 100
0 0
1

,
we get 3
4MF1 + 1
8Mδ{0} + 1
8Mδ{10} = 3
4MF2 + 1
8Mδ{10} + 1
8Mδ{100}, which implies
that (WR) is violated.
5
Conclusions
We introduced a rationality principle (WR) for preference relations among ran-
dom quantities equipped with a belief function (g-lotteries), inspired to a ra-
tionality principle introduced in [2]. We proved that (WR) is a necessary and
suﬀcient condition for the existence of a strictly increasing utility function u
on the prizes whose Choquet integral on g-lotteries represents the preferences.
In the probabilistic framework the rationality principle assures and rules the
extendibility of the relation to new lotteries, which is a very useful property for
an actual use of the model. The extendibility of weakly rational preferences on
generalized lotteries is one of our future aims.
References
1. Chateauneuf, A., Cohen, M.: Choquet expected utility model: a new approach to
individual behavior under uncertainty and social choice welfare. In: Fuzzy Meas.
and Int.: Th. and App., pp. 289–314. Physica, Heidelberg (2000)
2. Coletti, G., Regoli, G.: How can an expert system help in choosing the optimal
decision? Th. and Dec. 33(3), 253–264 (1992)
3. Coletti, G., Scozzafava, R.: Toward a General Theory of Conditional Beliefs. Int.
J. of Int. Sys. 21, 229–259 (2006)
4. Coletti, G., Scozzafava, R., Vantaggi, B.: Inferential processes leading to possibility
and necessity. Inf. Sci. 245, 132–145 (2013)

CEU Representation of Preferences on Generalized Lotteries
453
5. Dempster, A.P.: Upper and Lower Probabilities Induced by a Multivalued Map-
ping. Ann. of Math. Stat. 38(2), 325–339 (1967)
6. Denneberg, D.: Non-additive Measure and Integral. Theory and Decision Library:
Series B, vol. 27. Kluwer Academic, Dordrecht (1994)
7. Ellsberg, D.: Risk, Ambiguity and the Savage Axioms. Quart. Jour. of Econ. 75,
643–669 (1961)
8. Fagin, R., Halpern, J.Y.: Uncertainty, belief and probability. Comput. Int. 7(3),
160–173 (1991)
9. Gale, D.: The Theory of Linear Economic Models. McGraw Hill (1960)
10. Gilboa, I., Schmeidler, D.: Additive representations of non-additive measures and
the Choquet integral. Ann. of Op. Res. 52, 43–65 (1994)
11. Jaﬀray, J.Y.: Linear utility theory for belief functions. Op. Res. Let. 8(2), 107–112
(1989)
12. Mc Cord, M., de Neufville, R.: Lottery Equivalents: Reduction of the Certainty
Eﬀect Problem in Utility Assessment. Man. Sci. 23(1), 56–60 (1986)
13. Miranda, E., de Cooman, G., Couso, I.: Lower previsions induced by multi-valued
mappings. J. of Stat. Plan. and Inf. 133, 173–197 (2005)
14. Quiggin, J.: A Theory of Anticipated Utility. J. of Ec. Beh. and Org. 3, 323–343
(1982)
15. Savage, L.: The foundations of statistics. Wiley, New York (1954)
16. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press (1976)
17. Schmeidler, D.: Subjective probability and expected utility without additivity.
Econometrica 57(3), 571–587 (1989)
18. Schmeidler, D.: Integral representation without additivity. Proc. of the Am. Math.
Soc. 97(2), 255–261 (1986)
19. Smets, P.: Decision making in the tbm: the necessity of the pignistic transformation.
Int. J. Approx. Reasoning 38(2), 133–147 (2005)
20. von Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior.
Princeton University Press (1944)
21. Walley, P.: Statistical reasoning with imprecise probabilities. Chapman & Hall,
London (1991)
22. Wakker, P.: Under stochastic dominance Choquet-expected utility and anticipated
utility are identical. Th. and Dec. 29(2), 119–132 (1990)

Utility-Based Approach
to Represent Agents’ Conversational Preferences
Kaouther Bouzouita1, Wided Lejouad Chaari1, and Moncef Tagina2
1 SOIE Research Laboratory,
2 RIADI Research Laboratory,
National School of Computer Studies, University of Manouba, Tunisia
{kaouther.bouzouita,wided.chaari,moncef.tagina}@ensi.rnu.tn
Abstract. With the growing interest in Multi-Agent Systems (MAS)
based solutions, one can ﬁnd multiple MAS conceptions and implemen-
tations dedicated to the same goal. Those systems with their complex
behaviors are rarely predictable. They may provide diﬀerent results ac-
cording to agents’ interactions sequences. Consequently, evaluation of the
quality of MAS returned results became an urgent need. Our approach is
interested in evaluating high level data by considering agent’s preferences
regarding performatives. By analogy with the economic ﬁeld, agents may
ask for services, so they are consumers and may receive diﬀerent possi-
ble answers to their requests from other agents which are producers. We
will then focus on the analysis of messages exchanged within standard
interaction protocols and compute the utility value associated to every
conversation. Then we conclude utility measures for each agent and for
the whole MAS regarding some execution results.
Keywords: Multi-agent systems, rational agents, evaluation, utility, au-
tomaton, Mealy machine, interaction protocol, preferences, performa-
tives.
1
Introduction
In this paper we intend to present an evaluation methodology of Multi-Agent
Systems (MAS) based on the use of utility function and studying agents’ inter-
actions. Our approach aims at estimating the satisfaction of an agent about all
undertaken conversations where she is asking for services and thereafter conclude
the global agents utility in a given MAS.
Related Work. Many existing works present approaches for system low-level
performances measurement like [1] where a queuing model based approach was
deﬁned to predict systems performances regarding response time. In [2] authors
used the number of cycles necessary for agents to achieve a result, for time mea-
surement. In [3], three aspects of evaluation were studied: structural, typological
and statistical aspects. Nevertheless, emphasis was placed on the structural as-
pect of the evaluation. Such works do not consider the evaluation of the quality
of the result returned by agents.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 454–463, 2014.
c
⃝Springer International Publishing Switzerland 2014

Utility-Based Approach to Represent Agents’ Conversational Preferences
455
The work described in [4] used a utility function to generate agents’ activity
plans. Such an approach is limited to agents executing daily activities, such as
MATSim [10]. In addition, every agent’s plan was evaluated independently from
other agents’ plans. Agents’ preferences is an interesting ﬁeld of study, since it is
essential for the estimation of agents’ satisfaction through exchanged messages.
Thus some works have been concerned with representing agents’ preferences by
considering alternatives they can have during inference processes. For instance,
agents interaction in automated negotiation processes were used in [5] for repre-
senting and learning preferences. Qualitative representations of preferences and
reasoning were used.
Coste-Marquis et al. present in [6] a comparison between languages for rep-
resenting preferences relation using propositional logic. Other works such as [7]
studied numerical languages for describing utility functions using UCP-networks
which consist in a directed graphical representation of utility functions combining
two models: generalized additive models and CP-networks [8] which are graphi-
cal models for representing preference ordering. However, they just give a partial
ordering.
Our Work Positioning. We are considering agents’ interactions for the study
of consumer agents’ satisfaction regarding the conversations aimed at getting
services or information. Our approach focuses on high level data analysis. We
are considering performatives as preferences. In other words, we are trying to
attribute utility values to performatives depending on their judged “usefulness”
to the agent. Conversational preferences of consumer agents will be represented
using ﬁnite-state automaton. By studying consumer agents’ satisfaction, we eval-
uate the eﬃectiveness of the multi-agent system. Indeed, we estimate that satis-
ﬁed agents correspond to a system that could reach positive results, intermediate
ones at least, which are a necessary requirement for system’s performance vali-
dation. In this paper, we are not evaluating protocols since we work on standard
ones which are widely adopted by many systems. Standards of the Foundation
for Intelligent Physical Agents (FIPA) [11] will be used.
Overview of This Paper. The remainder of this paper is structured as follows:
Section 2 presents the syntax of conversational preference representation and an
example of open multi-agent system used in this paper. Section 3 is dedicated for
the presentation of our representation approach. Section 4 details the evaluation
methodology. Results are discussed in section 5. Sections 6 and 7 are respectively
dedicated for future work and conclusion.
2
Preliminaries
We will describe the syntax of the utility function and preferences representation
and we will ﬁnish by presenting the MAS used for the experimentation phase.

456
K. Bouzouita, W. Lejouad Chaari, and M. Tagina
2.1
Utility Function and Preferences Representation Syntax
A conversation is a list of messages. It is started by an agent in order to commu-
nicate with other agents for diﬃerent kinds of purposes, like asking for service or
information. The agent asking for service is called a consumer agent. The one
who has been asked is a producer agent. We use terminology of the economic
ﬁeld since an analogy can be detected between multi-agent model and economic
model regarding services request and supply. We design by ci
a the ith conversation
started by the consumer agent a. C is the space of all possible conversations.
In the Mealy machine conceived for our evaluation approach, a conversation is
shortened to a list of performatives leading to some states of the system. C is then
the set of regular expressions recognized by the automaton. U is a utility function
from C to R. It can be deduced from the output function α and attributes a real
value to a conversation after considering the following parameters:
• nb performi
a: Number of messages having the performative perform in the
considered conversation,
• wperform: weight associated to the messages of performative perform.
Let ⪰be a total order relation. For two conversations c1
a and c2
a, c1
a ⪰c2
a means
that c1
a is at least as good as c2
a.
2.2
MAS Used for the Experimentation
An open multi-agent system of the Java Agent DEvelopment Framework (JADE)
[12] which is the book-trading MAS will be used throughout this paper to il-
lustrate our approach. That system allows ﬁnal users to buy books. It uses a
FIPA standard interaction protocol (IP), called ﬁpa-request. It is composed of
a buyer agent (consumer) and seller agent(s) (producer(s)). Buyer agent will
requests a book from producers by sending a CALL-FOR-PROPOSAL (CFP)
messages. These agents return the price if they ﬁnd the book in their catalogs
(PROPOSE message), otherwise a REFUSE message will be returned. Buyer
agent can accept one of the proposals. Thus, the correspondent seller will re-
trieve the book from her catalog and inform the buyer agent of the success of
purchase (INFORM message). If the seller does not ﬁnd it anymore in her cata-
log , a FAILURE message will then be sent. The deﬁnition of automaton states
and transitions will be based on the performatives mentioned above.
3
Conversational Preferences Representation
We will explain steps of our approach and present its properties.
3.1
Mealy Machine for Describing Conversations Evolution
Our Mealy machine will represent all possible states of a conversation. These
states will characterize its main transformations. By “main transformations” we

Utility-Based Approach to Represent Agents’ Conversational Preferences
457
mean all changes that are signiﬁcant in the generation of a response to the initial
request started by the consumer agent. Every transition in the automaton will
be adding a value to the utility measure of the conversation. That value can be
positive, negative or null, depending on the corresponding performative. Let’s
ﬁrst deﬁne regular expressions recognized by the automaton: Performatives are
extracted from received messages. By considering all types of communicative
acts, we will not only deﬁne positive and negative responses, but we will also
consider intermediate kinds of responses so as to get a utility-based comparison
between all possible conversations.
In the example of book-trading system, regular expressions of conversations
may be represented as follows:
(CFP{1, N}REF{N −i}PROP{i}ACC PROP?(INF|FAIL)) +
(1)
where N is the number of seller agents in the system and i is the number of sellers
having the requested book in their catalogs and proposing to sell it (0 ≤i ≤
N). Here we abbreviate performatives designations as follows: CFP for CALL-
FOR-PROPOSAL, REF for REFUSE , PROP for PROPOSE, ACC PROP for
ACCEPT-PROPOSAL, INF for INFORM and ﬁnally FAIL for FAILURE.
Mealy machine, consisting in a tuple (Q, Θ, τ, γ, α, q0, F), is deﬁned in our
example as follows:
• Q: All possible states of a given conversation. In the book-trading example,
states are the following:
S0: Initial state of the system. No CFP1 is emitted yet, or CFP is emitted
and as many refusals as the number of sellers are received. One iteration
may also ﬁnish with the reception of a FAILURE message which sends
the system back to the state S0.
{S1
1, . . . , SN
1 }: Service request process has been started (again) by emitting
as CFP messages as the number of sellers. Every CFP moves the system
from one Si
1 to the next state Si+1
1
(1 ≤i < N). Reception of a REFUSE
message moves the system back to the previous state.
S2: Consumer agent receives one proposal from a seller agent. Eventually
other requested agents may send REFUSE or PROPOSE messages (the
system stays then in the same state S2).
S3: The buyer agent has received all responses (that include at least one
PROPOSAL message) and will choose the best received proposal (e.g.
the lowest price). Then, she will send an ACCEPT-PROPOSAL message
and wait for a conﬁrmation from the seller (or a FAILURE message).
S4: A conﬁrmation is sent to the buyer agent (INFORM message).
1 Buyer agent sends a CFP message at the beginning of every iteration, with the ﬁled
receivers
ﬁlled with seller agents’ identiﬁers. We consider in our approach that a
CFP message is sent to each seller agent, since each one of them receives a copy of
that message.

458
K. Bouzouita, W. Lejouad Chaari, and M. Tagina
• Θ: A ﬁnite (non-empty) input alphabet composed of abbreviations of all
possible performatives that can be contained in ACL messages:
Θ = {ACC PROP, CFP, FAIL, INF, PROP, REF}.
• τ: is the output alphabet. In our case, it is the real space R. Choice of output
values will be presented and justiﬁed in the next paragraph.
• γ : Q × Θ →Q: is the state-transition function, mapping a pair of a state of
a conversation and a performative to the corresponding next state.
• α : Q × Θ →τ: is the output function where τ ≡R. This function will be
used to generate the utility function.
Table 1 shows all values of state-transition and output function associ-
ated to a given pair of state and input symbol. Negative utility values were
attributed to refusals and failures. Proposals add positive values since they
correspond to initial positive responses. We assume that the more proposals
the consumer agent gets, the better it is. Indeed, the agent will be more satis-
ﬁed when receiving many proposals which, most likely, include the best one.
This heuristic allows us to assess the quality of the result without necessar-
ily going through the analysis of proposals’ content. An INFORM message
will add positive value too since it corresponds to a ﬁnal positive response.
CALL-FOR-PROPOSALs and ACCEPT-PROPOSAL messages are neutral
(they are emitted by consumer agent and add no supplementary service to
her).
Table 1. State-transition/Output function values (1 < i < N)
CFP
REF
PROP ACC PROP INF FAIL
S0
S1
1/0
-
-
-
-
-
Si
1 Si+1
1
/0 Si−1
1
/ −1
S2/1
-
-
-
SN
1
-
Si−1
1
/ −1
S2/1
-
-
-
S2
-
S2/ −1
S2/1
S3/0
-
-
S3
-
-
-
-
S4/1 S0/ −1
S4
-
-
-
-
-
-
• q0: Initial state: S0.
• F: the state ending with the book purchased is the ﬁnal state. At this state,
the buyer agent will be terminated. We have: F = {S4}.
In the state diagram of Mealy machine, edges are labeled with an input symbol
and an output symbol. Figure 1 shows the Mealy machine used for conversations’
graphical representation.
3.2
Utility per Conversation
We have seen so far a graphical representation using a state diagram of one
conversation of the ﬁpa-request protocol. We will now deduce the utility value

Utility-Based Approach to Represent Agents’ Conversational Preferences
459
Fig. 1. Mealy machine related to a conversation in the Book-Trading System
associated to one conversation. This value will be simply deﬁned by summing all
values read on the diagram when identifying diﬃerent states of that conversation.
In other words, conversations will be considered as regular expressions accepted
by that automaton. Utility function is:
U(ci
a) =
nbi
a

j=1
α(Sj−1, pj)
(2)
where nbi
a is the number of messages in the conversation ci
a, Sj−1 the state
preceding the current state and pj is the performative of the current message.
Another expression of the utility function in terms of number of performatives
can be deduced from the previous expression:
U(ci
a) =

perform∈Σ
wperform.nb performi
a
(3)
where: wREF = wF AIL = −1, wP ROP = wINF = 1 and wCF P = wACC P ROP =
0. Utility function’s expression is then:
U(ci
a) = wP ROP.nb PROP i
a + wREF .nb REF i
a + wF AIL.nb FAILi
a + wINF .nb INF i
a
(4)
3.3
Characteristics of the Representation
Let c1
a and c2
a be two conversations of an agent a.
• Completeness: Our function gives a complete preorder relation, since any
two conversations c1
a and c2
a may be trivially compared using utility values:
U(c1
a) ≥U(c2
a) ⇔c1
a ⪰c2
a

460
K. Bouzouita, W. Lejouad Chaari, and M. Tagina
• Transitivity: Let c3
a be a conversation of an agent a. If c1
a ⪰c2
a and c2
a ⪰c3
a
then c1
a ⪰c3
a. In fact, this property can be simply demonstrated using to
the utility function U: c1
a ⪰c2
a means that U(c1
a) ≥U(c2
a), same thing for
c1
a and c3
a : U(c1
a) ≥U(c3
a). We then have U(c1
a) ≥U(c3
a) and consequently:
c1
a ⪰c3
a.
• Indiﬀerence: Indiﬃerence too can be represented by our preference rep-
resentation method. In fact, conversations having same utility values are
considered equal regarding satisﬁability for the consumer agent.
• Eﬃciency: Our representation oﬃers a trade-oﬃbetween simplicity and ex-
pressiveness by simply representing all possible conversations.
4
Evaluation Methodology
In this section, we will describe all steps in our evaluation methodology that
aims at estimating consumer agents’ satisfaction by all conversations.
4.1
Messages Interception
First step in our methodology consists in intercepting emitted messages during
the execution of the MAS. Aspect-Oriented programming [13] was used for the
implementation of two aspects: Interception of message successful emission and
detection of the end of the MAS execution. After detecting one Agent Commu-
nication Language (ACL) message emission, that message will be stored. The
aspect dedicated to the termination detection is deﬁned according to the multi-
agent system. In the example of the book-trading system, termination of the
system corresponds to the ending of buyer agents.
4.2
Messages Semantic Analysis
Intercepted messages will be analyzed semantically to get the necessary infor-
mation for the evaluation step. Every ﬁeld of the message that is necessary to
the evaluation such as: performative, sender or receiver, and conversation iden-
tiﬁer will be extracted. Then consumers will be identiﬁed and added to a list
of consumers. For this, performatives should be considered to decide whether to
extract the sender ﬁeld or the receiver(s) one. Performatives whose sender is the
consumer: Inform-If / Inform-Ref, Propagate / Proxy, Request / Request-When
/ Request-Whenever, Query-If / Query-Ref, Call-For-Proposal, Accept-Proposal,
Reject-Proposal, Subscribe, Cancel. Performatives whose sender is a producer:
Inform, Refuse, Agree, Failure, Conﬁrm / Disconﬁrm, Propose. Then we deﬁne,
for each consumer, a list of conversations including all related messages (having
same conversation ID).

Utility-Based Approach to Represent Agents’ Conversational Preferences
461
4.3
Use of Utility Function
• Conversation’s Utility: We apply the utility function deﬁned above for
calculating every conversation utility value. Best scenario can be deﬁned on
the automaton and by using utility function as follows: it has to include the
ﬁnal state (i.e. a positive response has been received) and a maximum utility.
• Average Agent’s/MAS Conversations Utility: For every consumer
agent a, an average value of all conversations’ utilities will be estimated.
The following function will then be used:
Ua =
nbconversations

i=1
U(ci
a)/nbconversations
(5)
where nbconversations is the number of conversations started by a consumer
agent a. After estimating every agent’s satisfaction, all utility values will
be considered for estimating the average utility relative to the MAS after a
given execution:
UMAS =
nbconsumers

i=1
Ui/nbconsumers
(6)
where nbconsumers is the number of consumer agents.
5
Case Study
In this section, we will consider the book-trading system with multiple conﬁg-
uration settings. We will present two sets of results. In the ﬁrst one, we used
a MAS with one buyer and two sellers, and checked utility values for diﬃerent
numbers of iterations (see Table 2).
Table 2. Utility variation for diﬀerent numbers of iterations
Number of it-
erations
Number of ex-
changed mes-
sages
Number
of
PROPOSE
messages
Number
of
REFUSE
messages
Number
of
INFORM
messages
MAS Utility
1
5
1
1
1
1
2
8
1
3
1
−1
3
11
1
5
1
−3
4
14
1
7
1
−5
10
32
1
19
1
−17
In the second set of experiments, we considered a single buyer agent and varied
the number of sellers to see the subsequent variation of the utility value. Results
of Table 3 were obtained. (P.S: All sellers have the sought book in their catalogs,
this is why we have one successful iteration in every execution). In both sets of
experiments, number of FAILURE messages was equal to zero.

462
K. Bouzouita, W. Lejouad Chaari, and M. Tagina
Table 3. Utility variation for diﬀerent numbers of sellers
Number
of
Sellers
Number of ex-
changed mes-
sages
Number
of
PROPOSE
messages
Number
of
REFUSE
messages
Number
of
INFORM
messages
MAS Utility
1
4
1
0
1
2
2
5
2
0
1
3
3
6
3
0
1
4
4
7
4
0
1
5
10
13
10
0
1
11
6
Results Interpretation and Discussion
In this section we will discuss the results found in the experimental section and
try to interpret them in order to deduce, in the considered multi-agent system,
the eﬃect of the variation of both the number of iterations and the number of
agents on the utility values.
• First Set of Experiments Analysis: When the number of iterations in-
creases, this means that buyer agent did not ﬁnd the requested book yet and
thus, the number of refusals has increased which automatically decreases the
utility of the system.
• Second Set of Experiments Analysis: We may clearly notice the increase
of the utility value when the number of producer agents increases. In fact,
in our method, the more proposals the consumer agent receives the better
it is. Indeed, we estimate that it is better for the consumers to have a large
set of alternatives to compare. This will lead to making a better choice.
In conclusion, best scenario corresponds to an execution with positive response,
several seller agents and a minimum number of iterations.
7
Future Work
We intend to deepen our study in the ﬁeld of MAS evaluation, by considering
the content of positive response messages and evaluating ﬁnal user’s satisfaction.
Weighted preferences of agents and users will then be used.
8
Conclusion
In this paper we presented an approach for the evaluation of consumer agents’
satisfaction regarding services they ask from producers. Our approach uses a
utilitarian preference representation that oﬃers both a qualitative and quantita-
tive evaluation. In fact cardinal utility was used to have an ordinal utility and
give a complete preorder relation among all possible conversations. To represent
those conversations we used a Mealy machine. In conclusion, we used as main

Utility-Based Approach to Represent Agents’ Conversational Preferences
463
criterion of evaluation the performatives of exchanged messages. Thanks to this
approach, a best execution scenario can be deﬁned as a conversation with a pos-
itive response and a maximum utility value using the automaton deﬁned for the
system’s conversations recognition.
References
1. Gnanasambandam,
N., Lee,
S., Kumara,
S.R.T.,
Gautam,
N., Peng,
W.,
Manikonda, V., Brinn, M., Greaves, M.: An Autonomous Performance Control
Framework for Distributed Multi-Agent Systems: A Queueing Theory Based Ap-
proach. In: Proceedings of the AAMAS (2005)
2. Kaddoum, E., Gleizes, M.P., George, J.P., Glize, P., Picard, G.: Analyse des crit`eres
d’´evaluation de syst`emes multi-agents adaptatifs (regular paper). In: Journ´ees
Francophones Sur Les Syst`emes Multi-Agents (JFSMA 2009), Lyon, C´epadu`es,
pp. 123–132 (2009)
3. Ben Hmida, F., Lejouad Chaari, W., Tagina, M.: Graph theory to evaluate com-
munication in industrial multiagent systems. International Journal of Intelligent
Information and Database Systems (IJIIDS) 5(4), 361–388 (2011)
4. Charypar, D., Nagel, K.: Generating complete all-day activity plans with genetic
algorithms. Transportation 32(4), 369–397 (2005)
5. Aydo˘gan, R.: Preferences and Learning in Multi-agent Negotiation. In: AAAI-DC,
Atlanta, USA, pp. 1972–1973 (2010)
6. Coste-Marquis, S., Lang, J., Liberatore, P., Marquis, P.: Expressive power and suc-
cinctness of propositional languages for preference representation. In: Proceedings
of the Ninth International Conference (KR 2004), Whistler, Canada, June 2-5, pp.
203–212. AAAI Press (2004)
7. Boutilier, C., Bacchus, F., Brafman, R.: UCPnetworks: a directed graphical repre-
sentation of conditional utilities. In: Proc. of UAI 2001, pp. 56–64 (2001)
8. Boutilier, C., Brafman, R.I., Domshlak, C., Hoos, H.H., Poole, D.: Cp-nets: A tool
for representing and reasoning with conditional ceteris paribus preference state-
ments. J. Artif. Intell. Res (JAIR) 21, 135–191 (2004)
9. Amato, C., Bonet, B., Zilberstein, S.: Finite-State Controllers Based on Mealy Ma-
chines for Centralized and Decentralized POMDPs. Paper presented at the Meeting
of the AAAI (2010)
10. MATSim web site, http://matsim.org/ (last accessed on December 1, 2013)
11. FIPA web site, http://www.fipa.org (last accessed on October 10, 2013)
12. JADE web site, http://jade.tilab.com (last accessed on October 5, 2013)
13. AspectJ, http://eclipse.org/aspectj/ (last accessed on October 19, 2013)

Alternative Decomposition Techniques
for Label Ranking
Massimo Gurrieri⋆, Philippe Fortemps, and Xavier Siebert
UMons, Rue du Houdain 9, 7000 Mons, Belgium
massimo.gurrieri@umons.ac.be
Abstract. This work focuses on label ranking, a particular task of pref-
erence learning, wherein the problem is to learn a mapping from instances
to rankings over a ﬁnite set of labels. This paper discusses and proposes
alternative reduction techniques that decompose the original problem
into binary classiﬁcation related to pairs of labels and that can take into
account label correlation during the learning process.
Keywords: Preference Learning, Label Ranking, Reduction Techniques,
Machine Learning, Binary Classiﬁcation.
1
Introduction
Preference learning [1] is gaining increasing attention in data mining and related
ﬁelds. Preferences can be considered as instruments to support or identify liking
or disliking of an object over others in a declarative and explicit way. In particu-
lar, the learning and modelling of preferences are being recently investigated in
several ﬁelds such as knowledge discovery, machine learning, multi-criteria deci-
sion making, information retrieval, social choice theory and so on. In a general
meaning, preference learning is a non trivial task consisting in inducing pre-
dictive preference models from collected empirical data. The most challenging
aspect is the possibility of predicting weak or partial orderings of classes (la-
bels), rather than single values (as in supervised classiﬁcation). For this reason,
preference learning can be considered as an extension of conventional supervised
learning tasks, wherein the input space can be interpreted as the set of preference
contexts (e.g. queries, users) while the output space consists in the preference
predictions provided in the form of partial orders, linear orders, top-k lists, etc.
Preference learning problems are typically distinguished in three topics [1]: ob-
ject ranking, instance ranking and label ranking. Object ranking consists in
ﬁnding a ranking function F whose input is a set X of instances characterized by
attributes and whose output is a ranking of this set of instances, in the form of a
weak order. Such a ranking is typically obtained by giving a score to each x ∈X
and by ordering instances with respect to these scores. The training process
takes as input either partial rankings or pairwise preferences between instances
of X. In the context of instance ranking, the goal is to ﬁnd a ranking function
⋆Corresponding author.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 464–474, 2014.
c
⃝Springer International Publishing Switzerland 2014

Alternative Decomposition Techniques for Label Ranking
465
F whose input is a set X of instances characterized by attributes and whose
output is a ranking of this set (again a weak order on X). However, in con-
trast with object ranking, each instance x is associated with a class among a set
of ordered classes. The output of a such a kind of problem consists in rankings
wherein instances labeled with higher classes are preferred (or precede) instances
labeled with lower classes. The third learning scenario concerns a set of training
instances which are associated with rankings over a ﬁnite set of labels, i.e. label
ranking [2, 3, 4, 5, 6, 8]. The main goal in label ranking is to predict weak or
partial orderings of labels. This paper is organized as follows. In section 2, we in-
troduce label ranking and existing approaches. In particular, we discuss learning
reduction techniques that transform label ranking into binary classiﬁcation. In
section 3, we describe some novel reduction techniques to reduce label ranking to
binary classiﬁcation that are capable of taking into account correlations among
labels during the learning process. Finally, in section 4 and 5, we present some
experimental results, conclusions and future work, respectively.
2
Label Ranking
In label ranking, the main goal is to predict for any instance x, from an instance
space X, a preference relation ≻x: X →L, where L= {α1; α2; ...; αk} is a set of
labels or alternatives, such that αi ≻x αj means that instance x prefers label αi
to label αj or, equivalently, αi is ranked higher than αj. More speciﬁcally, we
are interested to the case where ≻x is a total strict order over L, or equivalently,
a ranking of the entire set L. This ranking can therefore be identiﬁed with a
permutation πx ∈τ (the permutation space of the index set of L), such that
πx(i) < πx(j) means that label αi is preferred to label αj (πx(i) represents the
position of label αi in the ranking). As in classiﬁcation, it is possible to associate
x to an unknown probability distribution P(.|x) over the set τ so that P(τ|x)
is the probability to observe the ranking τ given the instance x. Typically, the
prediction quality of a label ranker M is measured by means of its expected loss
on rankings:
E(D(τx, τ′
x)) = E(D(τ, τ′)|x)
(2.1)
where D(., .) is a distance function (between permutations), τx is the true value
(ground truth) and τ′
x is the prediction made by the model M. Given such a
distance metric, the best prediction is: τ∗= arg min
τ ′∈Ω

τ∈Ω
P(τ|x)D(τ ′, τ). Spear-
man’s footrule, Kendall’s tau and the sum of squared distances are well-known
distances between rankings [14, 15, 17]. There are two main groups of approaches
to label ranking. On the one hand, decomposition (or learning reduction) meth-
ods transform label ranking problem into binary classiﬁcation [2, 3, 4, 5]. On
the other hand, direct methods adapt existing classiﬁcation algorithms in order
to deal with label ranking [6, 9, 10, 16]. This work focuses on decomposition
methods because they directly learn binary preferences, i.e. simple statements
like x ≻y and allow to build meta-learners, i.e. rankers where any binary clas-
siﬁer can be used as base classiﬁer. For example, a rule-based label ranker has

466
M. Gurrieri, P. Fortemps, and X. Siebert
been recently proposed [2, 3]. In the context of multi-label classiﬁcation, it has
been recently reported [7, 13] that it is crucial to take into account correlations
between labels. As a consequence, it seems natural to put this issue into per-
spective also in label ranking. However, the standard pairwise learning reduction
[5] does not take such correlations into account: a separate binary classiﬁer is
trained for each pair of labels so that each pair of labels is treated independently
from the remaining pairs. In view of this, we propose in this paper alternative
decomposition techniques, other than [4, 5], to take into account correlations
among labels, while limiting computational complexity.
3
Reduction Framework
In the context of label ranking, the training set is T = {(x, πx)}, where x =
(q1, q2, ..., ql) is a vector of l attributes (the feature vector) and πx is the corre-
sponding target label ranking associated with the instance x. In sections 3.1, 3.2,
3.3 we present three novel pairwise reductions techniques: Nominal decomposi-
tion (similar to the one presented in [2, 3]), Dummy Coding decomposition and
Classiﬁer Chains (based on the Classiﬁer Chains for Multi-label Classiﬁcation
[7]). In section 3.4 we also discuss a method for the ranking aggregation problem.
A probabilistic interpretation of the Classiﬁer Chains for label ranking is ﬁnally
presented in section 3.5.
3.1
Pairwise Decomposition: Nominal Coding
In this decomposition, each learning instance (x, πx) = (q1, q2, ..., ql, πx) is trans-
formed into a set of simpler and easier-to-learn instances {x1,2, x1,3, ..., xi,j, ...},
where the generic instance xi,j is responsible to convey not only the feature
vector (q1, q2, ..., ql) but also information about a speciﬁc pair of labels (αi,αj),
according to a given decomposition of πx into pairwise preferences. The number
of pairs is at most (in case of full rankings) k(k −1)/2, where k = |L|. The
learning process associated with this reduction is:
xi,j = (q1, q2, ..., ql, ri,j, d)
(3.1)
with i, j ∈{1, 2, ..., k}, i < j, d ∈{−1, +1} and ri,j is a nominal attribute
which uniquely identiﬁes the pair (αi,αj). In this manner, each xi,j is a learn-
ing instance responsible only for the speciﬁc pair (αi,αj). The binary attribute
d ∈{−1; +1} takes into account the preference relation between the two labels
(αi,αj), according to the original ranking πx. That is, d = +1 when αi is pre-
ferred to αj (or ranked higher), otherwise d = −1, according to the provided
input (x, πx). For example, the instance x = (−1.5, 2.4, 1.6, α2 ≻α1 ≻α3) gen-
erates the following learning instances: x1,2 = (−1.5, 2.4, 1.6, r1,2, −1), x1,3 =
(−1.5, 2.4, 1.6, r1,3, +1), x2,3 = (−1.5, 2.4, 1.6, r2,3, +1). By using this reduction,
it is possible to treat the overall pairwise preference information in a single
learning set, instead of creating independent learning sets as in [5]. This allows

Alternative Decomposition Techniques for Label Ranking
467
to process the overall preference information at once in a unique learning set and
therefore to learn a model M that takes into account correlations between labels,
if any. The classiﬁcation problem derived from the original label ranking problem
can be solved by any binary classiﬁer (e.g. Multilayer Perceptron). Assuming a
given base learner whose complexity is Φ(l, |X|), the complexity of this reduction
is Φ(l + 1, |X|× p), where p = k(k −1)/2, since the number of instances is multi-
plied by p (i.e. a copy of the original instance for every pair of labels). To classify
a new instance x′, for each pair of labels (αi, αj), with i, j ∈{1, 2, ..., k}, i < j,
the feature vector of the testing instance x′ is augmented by adding a variable
ri,j one at a time. This allows the model M to predict either +1 or −1 for the
speciﬁc query pair (αi, αj).
3.2
Pairwise Decomposition: Dummy Coding
To avoid the use of nominal attributes, another reduction is based on the dummy
coding so that ones and zeros are added to the feature space in order to con-
vey the preference information about pairs of labels. Similarly as in (3.1), each
learning instance (x, πx) = (q1, q2, ..., ql, πx) is transformed into a set of instances
{x1,2, x1,3, ..., xi,j, ...}. While in the previous reduction the feature space was
augmented by one (a nominal attribute identifying a pair of labels), in this re-
duction scheme the feature space is augmented by exactly p binary attributes,
where only one attribute is set to 1, the others being set to 0. The learning
process associated with this reduction is:
xi,j = (q1, q2, ..., ql, r1,2, ..., ri,j, ...rk−1,k, d)
(3.2)
with i, j ∈{1, 2, ..., k}, i < j, d ∈{−1, +1} and rv,z = 1 if v = i ∧z = j, 0
otherwise. Since for every pair (i, j) only one variable ri,j is set to 1, the corre-
sponding learning instance xi,j is responsible for that speciﬁc pair. For example,
the instance: x = (−1.5, 2.4, 1.6, α2 ≻α1 ≻α3) generates the following learn-
ing instances: x1,2 = (−1.5, 2.4, 1.6, 1, 0, 0, −1), x1,3 = (−1.5, 2.4, 1.6, 0, 1, 0, +1),
x2,3 = (−1.5, 2.4, 1.6, 0, 0, 1, +1). Assuming a given base learner whose com-
plexity is Φ(l, |X|), the complexity of this reduction is Φ(l + p, |X| × p), where
p = k(k −1)/2, since p binary attributes are added to each instance xi,j, while
the number of instances is multiplied by p (i.e. a copy of the original instance
for each pair of labels). The classiﬁcation of a new instance x′ is similar to the
nominal decomposition.
3.3
Pairwise Decomposition: Classiﬁer Chains
In this section we present another learning reduction technique which is based
on Classiﬁer Chains for multi-label classiﬁcation [7, 13]. The proposed reduc-
tion scheme involves p = k(k −1)/2 binary classiﬁers, each binary classiﬁer
being responsible for learning and predicting the preference for a speciﬁc pair,
given preference relations on previous pairs of labels, in a chaining scheme. In
this way all previous pairs of labels are treated as additional attributes to model

468
M. Gurrieri, P. Fortemps, and X. Siebert
conditional dependence between a given pair of labels and all preceding pairs.
The most interesting aspect of this reduction scheme is that it is possible to
propagate preference information about pairs of labels between all classiﬁers
throughout the chain, enabling thus to take correlations among labels into ac-
count. Moreover, there is a gain in complexity w.r.t. the previous reductions
because the size of the learning set does not change at each iteration. The set of
binary classiﬁers (the chain) h = (h1, h2, ..., hp) is used to model a global label
ranker where each classiﬁer hj is trained with
x = (q1, q2, ..., ql, r1, r2, ..., rj−1, d)
(3.3)
as a learning instance, r1, r2, ..., rj−1 being the values (either +1 meaning ≻, or
−1 meaning ≺) on the j −1 previous pairs of labels provided by the ranking πx
and according to the chosen order of decomposition. The attribute d ∈{−1; +1}
is the preference information about the jth pair of labels also according to πx
and to the order of decomposition. It should be noticed that a default or a
random order of labels can be considered in the decomposition of the label set
L. Assuming a given base learner whose complexity is Φ(l, |X|), the complexity
of each single classiﬁer hi is Φ(l + ci, |X|), where 1 ≤ci ≤p, since ci attributes
(binary variables) are added (at most p, in the last classiﬁer) to each instance. For
example, if |L| = 3 and the order decomposition is {(2, 1), (2, 3), (3, 1)}, the chain
consists in (h1, h2, h3) and the input instance x = (−1.5, 2.4, 1.6, α2 ≻α1 ≻α3) is
used as a learning instance in the following way: h1 ←(−1.5, 2.4, 1.6, +1), h2 ←
(−1.5, 2.4, 1.6, +1, +1), h3 ←(−1.5, 2.4, 1.6, +1, +1, −1). The classiﬁcation of a
new instance x′ is performed in the following way. The classiﬁer h1 predicts
the value (either +1 or −1) for the ﬁrst pair of labels, according to the given
decomposition order. Afterwards, the feature vector of x′ is augmented with
the prediction on the ﬁrst pair of label and the classiﬁer h2 predicts the value
of the second pair of labels (by testing the feature vector of x′ augmented by
the previous prediction). In an iterative way the classiﬁer hj predicts the value
of the jth pair using the feature vector augmented by all previous predictions
provided by (h1, h2, ..., hj−1). Since the order of labels could have an impact
on the prediction accuracy, we also consider the ensemble scheme proposed in
[7, 13]. In this manner, it is possible to avoid not only the bias due to a single
(default or random) order of labels but also the eﬀect of error propagation along
the chain in case the ﬁrst classiﬁers perform poorly. The main idea is to train T
classiﬁer chains (typically T = 10) where each classiﬁer is given a random label
order and moreover, each classiﬁer is trained on a random selection of learning
instances sampled with replacement (typically 75% of the learning set) in order
to reduce time complexity without loss in prediction quality. It should be noticed
that to avoid the use of an ensemble of classiﬁer chains, some heuristics could be
used to select the most appropriate order. Such heuristics are currently under
study.

Alternative Decomposition Techniques for Label Ranking
469
3.4
Ranking Generation Process
The reduction techniques (3.1), (3.2) and (3.3) require an additional step to
provide a ﬁnal ranking for a testing instance x′. The ﬁnal ranking should be
as much as possible consistent with the preference relations ≻x′ on each pair
of labels learned during the classiﬁcation process. However, this is not trivial
[2, 3, 4, 11, 14, 15] since the resulting preference relation is total, asymmetric,
irreﬂexive but not transitive, in general. The underlying problem is how to ﬁnd
a consensus between the pairwise predictions in order to obtain a linear order?
This is related to the well-known NP-hard Kemeny optimal rank aggregation
problem [14, 15]. A natural choice, at least in this context, for solving the ranking
aggregation problem is the Net Flow Score procedure [2, 3, 11] whose complexity
is O(k2), where k is the number of labels. This procedure allows to obtain a
ranking by ordering labels according to their net ﬂow scores. These scores can
be computed by using estimations of conditional probabilities on pairs of labels
(good estimations can be provided for example by neural networks [18]) and are
deﬁned as follows. Let us deﬁne:
Δ +
(i,j) = P(αi ≻x′ αj) = P(d = +1|x′)
(3.4)
as the probability that for the instance x′ label αi is ranked higher (preferred
to) than αj. Each label αi is then evaluated by means of the following score:
S(i) =

j̸=i
(Δ +
(i,j) −Δ +
(j,i)),
(3.5)
where Δ +
(i,j) is given by (3.4). The ﬁnal ranking is obtained by ordering labels
according to decreasing values of (3.5), so that the higher the score, the higher
the preference in the ranking: S(i) > S(j) ⇔τi < τj. It is possible to prove, in
a similar way as proved in [5], that the Net Flow Score procedure, as deﬁned in
(3.5), minimizes the expected loss (2.1), according to the sum of squared rank
distance. This means that, if correct posterior probabilities can be obtained
(or at least good estimations thereof), it is possible to ﬁnd an optimal ranking
by simply ordering labels according to scores (3.5). Even though the net ﬂow
score procedure does not provide in general optimal rankings w.r.t. the Kendall
distance, empirically it provides good performances (see section 4).
3.5
Probabilistic Classiﬁer Chains
In the context of multi-label classiﬁcation, a Bayes-optimal probabilistic classiﬁer
chains has been recently discussed [13]. In this section, we discuss a probabilis-
tic classiﬁer chains for Label Ranking which relies on the same idea. Given a
decomposition order of the label set into pairs, a permutation π can be iden-
tiﬁed in a unique way with a binary vector (yπ
1 , ..., yπ
p ) ∈{−1, +1}p so that

470
M. Gurrieri, P. Fortemps, and X. Siebert
yπ
i = +1 ⇔αj ≻αv while yπ
i = −1 otherwise. In this manner, the probabil-
ity of a permutation π is equivalent to the probability of its associated vector
(yπ
1 , ..., yπ
p ) and by means of the chain rule as in a Bayesian network:
P(π|x′) = P(yπ
1 , ..., yπ
p |x′) = P(yπ
1 |x′) ∗
p
?
i=2
P(yπ
i |x′, yπ
1 , ..., yπ
i−1).
(3.6)
The chaining procedure (3.3) allows to learn a probabilistic classiﬁer fi, i =
1, ..., p for each pair of labels, where p = k ∗(k −1)/2. This classiﬁer predicts, for
the ith pair of labels yi = (αj, αv), either +1 meaning αj ≻αv or −1 meaning
that αv ≻αj, according to the probability distribution learnt by the classiﬁer
fi. By knowing for each pair of labels its (conditional) probability, it is possible
to compute the (conditional) probability of π. By means of (3.6), it is therefore
possible to rank all possible permutations for x′ w.r.t. their probabilities so that
an optimal prediction is given by:
π∗= arg max
π∈S [P(yπ
1 |x′) ∗
p
?
i=2
P(yπ
i |x′, yπ
1 , ..., yπ
i−1)]
(3.7)
As a result, this probabilistic formulation is well-tailored for the subset 0/1 loss
function [7, 13]:
L(π, π′) = 1π̸=π′.
(3.8)
Interestengly, it can easily be proved that the optimal prediction for the associ-
ated risk minimization problem is given by: π∗= arg max
π∈S P(π|x′). Moreover,
the classiﬁer chains presented in section 3.3 can be considered as a deterministic
approximation of (3.6), as similarly pointed out in [13]. While the probabilis-
tic approach evaluates all possible permutations, the classiﬁer chain provides,
in general, a suboptimal prediction gradually obtained at each iteration of the
chaining scheme by using:
hj(x′) = arg
max
rj∈{−1,+1} P(rj|x′, r1, ..., rj−1).
(3.9)
As a consequence, the chaining scheme (3.9) does not provide in general neither
an optimal solution w.r.t the subset 0/1 loss function nor a linear order. Inter-
estingly, the probabilistic approach (3.6) does not require any ranking aggrega-
tion algorithm since it directly evaluates permutations. However, a label ranker
well-tailored for the subset 0/1 loss is probably not reasonable in this context
given that it is a quite severe loss (even a slightly diﬀerent prediction gets the
highest penalty). Nevertheless, a method well-tailored for the subset 0/1 loss
function should exibit good performances w.r.t. other loss functions (Kendall’s
tau distance, Spearman’s footrule, etc.). On the other hand, the cost in terms of
computational complexity is very high: in case of k labels, k! permutations have
to be evaluated, which imposes an upper limit of k ≈6 labels. Nevertheless, it
should be noticed that a stop criterion could be applied in order to reduce time

Alternative Decomposition Techniques for Label Ranking
471
complexity. If p∗is the probability associated with an initial permutation π0,
the evaluation of another permutation πk can be stopped at the jth iteration as
soon as:
P(yπk
1 |x′) ∗
j?
i=2
P(yπk
i |x′, yπk
1 , ..., yπk
i−1) < p∗
(3.10)
This stop criterion allows to discard not only the permutation πk but also all
permutations wherein the ﬁrst j pairs share the same values as πk. This crite-
rion could considerably reduce the complexity during the testing process. As in
section 3.3, an ensemble of probabilistic classiﬁer chains can be considered.
4
Experimental Setup and Results
This section is devoted to experimentations that we conducted to evaluate the
performances of the proposed methods in terms of predictive accuracy. The data
sets used in this paper were taken from the KEBI Data Repository1. The evalu-
ation measures used in this study are the Kendall’s tau and the Spearman Rank
Correlation coeﬃcient [3, 4, 16, 17]. Performance of the methods was estimated
by using a cross-validation study (10-fold). We compared the standard pairwise
comparison (SD) [5] (note that the Net Flow score is used for the rank aggrega-
tion issue) with the proposed reductions: the nominal decomposition (ND), the
dummy coding decomposition (DD), random classiﬁer chains (CD) and ensem-
bled classiﬁer chains (ECD) (the voting procedure for the ﬁnal ranking is also
based on the Net Flow Score procedure). In this experiment, we used Multilayer
Perceptron (MLP) and Radial Basis Function (RBF) as base classiﬁers, both
with default parameters, which generally provide good estimations of posterior
probabilities [18]. All experiments were run on a 64-bit machine, allowing up to 4
GB RAM of heap memory size for larger datasets. Results w.r.t. the probabilistic
classiﬁers chains (PCD) and ensembled probabilistic classiﬁers chains (EPCD)
are not yet available. Tables 1 and 2 show the performances of the ﬁve classiﬁers
in terms of Kendall’s tau and Spearman’s Rank correlation with MLP and RBF
as base classiﬁers, respectively. Following the Friedman Test described in [12],
we found that in both cases the null-hypothesis is rejected at a signiﬁcance level
of 1%. According to the post-hoc Nemenyi test [12], the signiﬁcant diﬀerence
in average ranks of the classiﬁers is 1.760 at a signiﬁcant level of 5% and 1.587
at a signiﬁcant level of 10%. At a signiﬁcance level of 5%, ECD outperforms
SD and ND when using MLP as base classiﬁer, while the post-hoc test is not
powerful enough to establish any other statistical diﬀerence. At a signiﬁcance
level of 10%, ECD also outperforms CC. When using RBF as base classiﬁer, at
a signiﬁcant level of 5%, ECD outperforms ND and DD while SD outperforms
ND and DD. Moreover, CD outperforms ND.
1 See http://www.uni-marburg.de/fb12/kebi/research/repository

472
M. Gurrieri, P. Fortemps, and X. Siebert
Table 1. Comparison of reduction techniques with MLP as base classiﬁer
Kendall tau
SD
ND
DD
CD
ECD
IRIS
.973+-.045 (4)
.964+-.055 (5)
.991+-.017 (1)
.982+-.021 (2)
.977+-.029 (3)
GLASS
.880+-.064 (2)
.860+-.079 (5)
.865+-.062 (4)
.878+-.055 (3)
.888+-.056 (1)
WINE
.929+-.048 (4)
.925+-.040 (5)
.939+-.059 (1)
.936+-.036 (2.5)
.936+-.048 (2.5)
VEHICLE
.875+-.028 (4)
.877+-.023 (5)
.877+-.023 (3)
.892+-.026 (2)
.893+-.020 (1)
VOWEL
.910+-.014 (1.5)
.825+-.038 (5)
.861+-.022 (4)
.888+-.027 (3)
.910+-.014 (1.5)
STOCK
.830+-.013 (5)
.874+-.010 (3)
.868+-.009 (4)
.905+-.010 (2)
.914+-.017 (1)
CPU
.443+-.011 (4)
.472+-.015 (3)
.479+-.009 (2)
.431+-.024 (5)
.487+-.014 (1)
BODYFAT
.229+-.054 (4)
.241+-.065 (3)
.272+-.042 (1)
.150+-.081 (5)
.243+-.072 (2)
DDT
.062+-.040 (5)
.103+-.027 (3)
.120+-.022 (2)
.069+-.041 (4)
.123+-.022 (1)
HOUSING
.641+-.032 (5)
.712+-.040 (2)
.699+-.032 (3)
.667+-.061 (4)
.721+-.034 (1)
AUTORSHIP
.858+-.023 (5)
.929+-.016 (3)
.915+-.015 (4)
.937+-.010(2)
.941+-.015 (1)
WISCONSIN
.583+-.039 (1)
.108+-.111 (5)
.294+-.125 (4)
.451+-.014 (3)
.573+-.031 (2)
Av. Rate
3.70
3.91
2.75
3.12
1.50
Spearman rank correlation
SD
ND
DD
CD
ECD
IRIS
.980+-.033 (4)
.973+-.041 (5)
.993+-.013 (1)
.986+-.016 (2)
.983+-.022 (3)
GLASS
.908+-.059 (2)
.891+-.078 (5)
.891+-.072 (4)
.900+-.059 (2)
.918+-.057 (1)
WINE
.944+-.042 (4)
.943+-.030 (5)
.954+-.044 (1)
.952+-.027 (2)
.949+-.042 (3)
VEHICLE
.901+-.026 (4)
.880+-.031 (5)
.902+-.021 (3)
.913+-.025 (2)
.916+-.018 (1)
VOWEL
.956+-.009 (1.5)
.900+-.031 (5)
.928+-.014 (4)
.930+-.022 (3)
.956+-.008 (1.5)
STOCK
.902+-.008 (5)
.931+-.005 (3)
.928+-.005 (4)
.947+-.007 (2)
.953+-.011 (1)
CPU
.520+-.011 (4)
.536+-.017 (3)
.543+-.013 (2)
.475+-.028 (5)
.547+-.017 (1)
BODYFAT
.297+-.062 (4)
.315+-.078 (3)
.347+-.047 (1)
.196+-.098 (5)
.317+-.090 (2)
DDT
.071+-.044 (5)
.119+-.029 (3)
.135+-.028 (2)
.076+-.050 (4)
.141+-.024 (1)
HOUSING
.751+-.028 (4)
.802+-.040 (2)
.791+-.029 (3)
.742+-.059 (5)
.807+-.035 (1)
AUTORSHIP
.895+-.018 (5)
.954+-.011 (3)
.942+-.015 (4)
.958+-.007 (2)
.963+-.010 (1)
WISCONSIN
.737+-.040 (1)
.158+-.155 (5)
.400+-.161 (4)
.589+-.021 (3)
.728+-.035 (2)
Av. Rate
3.66
3.91
2.75
3.12
1.54
Table 2. Comparison of reduction techniques with RBF as base classiﬁer
Kendall tau
SD
ND
DD
CD
ECD
IRIS
.968+-.044 (3)
.808+-.063 (5)
.844+-.057 (4)
.982+-.035 (2)
.986+-.020 (1)
GLASS
.876+-.049 (2)
.687+-.083 (5)
.707+-.053 (4)
.852+-.051 (3)
.885+-.040 (1)
WINE
.958+-.050 (3)
.749+-.011 (5)
.828+-.061 (4)
.977+-.033 (1)
.973+-.038 (2)
VEHICLE
.808+-.034 (3)
.544+-.041 (5)
.548+-.095 (4)
.816+-.017 (2)
.830+-.025 (1)
VOWEL
.815+-.015 (1)
.309+-.044 (5)
.313+-.040 (4)
.728+-.030 (3)
.786+-.019 (2)
STOCK
.861+-.013 (1)
.527+-.072 (5)
.609+-.098 (4)
.841+-.016 (3)
.853+-.010 (2)
CPU
.429+-.017 (1)
.245+-.034 (5)
.254+-.009 (4)
.400+-.012 (2)
.397+-.009 (3)
BODYFAT
.174+-.077 (2)
.127+-.046 (4)
.125+-.053 (5)
.143+-.038 (3)
.179+-.053 (1)
DDT
.126+-.030 (3)
.114+-.034 (4)
.130+-.045 (2)
.112+-.027 (5)
.144+-.018 (1)
HOUSING
.659+-.019 (2)
.441+-.111 (4)
.438+-.087 (5)
.642+-.049 (3)
.665+-.037 (1)
AUTORSHIP
.935+-.017 (3)
.517+-.073 (5)
.584+-.103 (4)
.936+-.013 (1.5)
.936+-.009 (1.5)
WISCONSIN
.459+-.030 (2)
.198+-.089 (4)
.177+-.071 (5)
.427+-.047 (3)
.465+-.037 (1)
Av. Rate
2.16
4.75
4.08
2.54
1.45
Spearman rank correlation
SD
ND
DD
CD
ECD
IRIS
.976+-.033 (3)
.856+-.047 (5)
.883+-.042 (4)
.986+-.026 (2)
.990+-.015 (1)
GLASS
.910+-.041 (2)
.739+-.084 (4)
.735+-.043 (5)
.886+-.048 (3)
.922+-.032 (1)
WINE
.965+-.044 (3)
.803+-.085 (5)
.865+-.051 (4)
.983+-.025 (1)
.980+-.029 (2)
VEHICLE
.842+-.029 (3)
.613+-.051 (5)
.621+-.098 (4)
.853+-.017 (2)
.868+-.023 (1)
VOWEL
.895+-.012 (1)
.347+-.055 (4)
.345+-.044 (5)
.808+-.030 (3)
.874+-.015 (2)
STOCK
.923+-.009 (1)
.631+-.079 (5)
.706+-.111 (4)
.908+-.012 (3)
.918+-.006 (2)
CPU
.480+-.019 (1)
.304+-.042 (5)
.309+-.009 (4)
.448+-.012 (2)
.442+-.012 (3)
BODYFAT
.212+-.095 (2)
.162+-.061 (5)
.166+-.074 (4)
.180+-.051 (3)
.227+-.076 (1)
DDT
.144+-.036 (3)
.127+-.039 (5)
.149+-.045 (2)
.128+-.036 (4)
.163+-.023 (1)
HOUSING
.760+-.022 (2)
.529+-.123 (5)
.531+-.095 (4)
.738+-.041 (3)
.761+-.031 (1)
AUTORSHIP
.958+-.012 (3)
.631+-.059 (5)
.696+-.078 (4)
.959+-.009 (2)
.960+-.006 (1)
WISCONSIN
.605+-.036 (2)
.281+-.126 (5)
.250+-.097 (4)
.569+-.051 (3)
.608+-.043 (1)
Av. Rate
2.16
4.83
4
2.58
1.41
5
Conclusions
In this paper, we introduced alternative decomposition techniques for Label
Ranking, closely related to the standard decomposition method [5], but that
can take correlations among labels into account. We mainly investigated three

Alternative Decomposition Techniques for Label Ranking
473
decompositions that transform label ranking into binary classiﬁcation and that
allow to create meta learners. In particular, we adapted the classiﬁer chains and
its ensembled version for multi-label classiﬁcation [7, 13] to label ranking and
showed that the ensemble of classiﬁer chains outperforms all others decompo-
sition methods in a statistically signiﬁcant way. In order to increase accuracy
of classiﬁer chains, some heuristics to determine the most appropriate label or-
der are currently under study. Furthermore, probabilistic interpretations of the
classiﬁer chains and the ensembled version have also been introduced, though
experimental results have not been provided due to their extremily high com-
putational complexity. In particular the probabilistic classiﬁer chains minimizes
the subset 0/1 loss function in expectation. Another important result concerns
the Net Flow Score procedure that provides a good approximation algorithm
to the ranking aggregation problem.
References
1. Fürnkranz, J., Hüllermeier, E. (eds.): Preference Learning. Springer (2010)
2. Gurrieri, M., Siebert, X., Fortemps, P., Greco, S., Słowiński, R.: Label Ranking:
A New Rule-Based Label Ranking Method. In: Greco, S., Bouchon-Meunier, B.,
Coletti, G., Fedrizzi, M., Matarazzo, B., Yager, R.R. (eds.) IPMU 2012, Part I.
CCIS, vol. 297, pp. 613–623. Springer, Heidelberg (2012)
3. Gurrieri, M., Siebert, X., Fortemps, P., Slowinski, R., Greco, S.: Reduction from
Label Ranking to Binary Classiﬁcation. In: DA2PL 2012 From Multiple Criteria
Decision Aid to Preference Learning, pp. 3–13. UMONS (Université de Mons),
Mons (2012)
4. Har-Peled, S., Roth, D., Zimak, D.: Constraint classiﬁcation for multiclass classi-
ﬁcatin and ranking. In: Advances in Neural Information Processing Systems, pp.
785–792 (2002)
5. Hüllermeier, E., Fürnkranz, J., Cheng, W., Brinker, K.: Label Ranking by learning
pairwise preference. Artif. Intell. 172(16-17), 1897–1916 (2008)
6. Cheng, W., Hühn, J., Hüllermeier, E.: Decision Tree and Instance-Based Learning
for Labele Ranking. In: Proc. ICML 2009, International Conference on Machine
Learning, Montreal, Canada (2009)
7. Read, J., Pfahringer, B., Holmes, G., Frank, E.: Classiﬁer chains for multi-label
classiﬁcation. Machine Learning 85(3), 333–359 (2011)
8. Gärtner, T., Vembu, S.: Label Ranking Algorithms: A Survey. In: Fürnkranz, J.,
Hüllermeier, E. (eds.) Preference Learning. Springer (2010)
9. Dekel, O., Manning, C.D., Singer, Y.: Log-linear models for label ranking. In:
Advances in Neural Information Processing Systems, vol. 16 (2003)
10. Elisseeﬀ, A., Weston, J.: A kernel method for multi-labelled classiﬁcation. In: Ad-
vances in Neural Information Processing Systems, vol. 14 (2001)
11. Bouyssou, D.: Ranking methods based on valued preference relations: A character-
ization of the net ﬂow method. European Journal of Operational Research 60(1),
61–67 (1992)
12. Demšar, J.: Statistical Comparisons of Classiﬁers over Multiple Data Sets. Journal
of Machine Learning Research 7, 1–30 (2006)
13. Cheng, W., Hüllermeier, E., Dembczynski, K.J.: Bayes optimal multilabel classiﬁ-
cation via probabilistic classiﬁer chains. In: Proceedings of the 27th International
Conference on Machine Learning (ICML 2010), pp. 279–286 (2010)

474
M. Gurrieri, P. Fortemps, and X. Siebert
14. Dwork, C., Kumar, R., Naor, M., Sivakumar, D.: Rank aggregation revisited (2001)
15. Schalekamp, F., van Zuylen, A.: Rank Aggregation: Together We’re Strong. In:
ALENEX, pp. 38–51 (2009)
16. de Sá, C.R., Soares, C., Jorge, A.M., Azevedo, P., Costa, J.: Mining Association
Rules for Label Ranking. In: Huang, J.Z., Cao, L., Srivastava, J. (eds.) PAKDD
2011, Part II. LNCS, vol. 6635, pp. 432–443. Springer, Heidelberg (2011)
17. Diaconis, P., Graham, R.L.: Spearman’s footrule as a measure of disarray. Journal
of the Royal Statistical Society, Series B (Methodological) 39, 262–268 (1977)
18. Hung, M.S., Hu, M.Y., Shanker, M.S., Patuwo, B.E.: Estimating posterior prob-
abilities in classiﬁcation problems with neural networks. International Journal of
Computational Intelligence and Organizations 1(1), 49–60 (1996)

Clustering Based on a Mixture
of Fuzzy Models Approach
Miguel Pagola⋆, Edurne Barrenechea, Ar´anzazu Jur´ıo,
Daniel Paternain, and Humberto Bustince
Universidad Publica de Navarra,
Campus Arrosadia s/n., 31006 Pamplona, Spain
miguel.pagola@unavarra.es
http://giara.unavarra.es/
Abstract. In this work we propose a clustering methodology model
named as Mixture of Fuzzy Models (MFMs). We adopt two assumptions:
the data points are generated by a membership function and the sum of
the memberships to all of the clusters must be greater or equal than zero.
The objective is to obtain a set of membership functions which represent
the data. It is formulated as a multiobjective optimization problem with
two objectives: to maximize the sum of memberships within each cluster
and to maximize the diﬀerences of memberships between clusters.
Keywords: Clustering, Membership functions, Multi-objetive optimiza-
tion, Outliers.
1
Introduction
Clustering an unlabeled data set X = {x(1), . . . , x(m)} is the partitioning of X
into 1 < C < m subgroups such that each subgroup represents natural sub-
structure in X [10]. In statistics, a mixture model corresponds to the mixture
distribution that represents the probability distribution of observations in the
overall population. Therefore a mixture model can be regarded as providing a
statistical framework for clustering, where each cluster is modelled by a distribu-
tion, and each data point is eventually assigned to a single cluster. When dealing
with real data, Gaussian distribution (normal distribution) is the used one [9]
and this formulation is named Gaussian Mixture Models (GMMs). However any
distribution of probability can be used (Binomial, Student, Poisson, Exponential
or Log-normal).
One of the most widely used fuzzy clustering models is Fuzzy C-Means (FCM)
[1]. The FCM algorithm assigns memberships to the elements which are inversely
related to the relative distance of them to the point prototypes that are the
cluster centers.
FCM type fuzzy clustering algorithms have been shown to be closely related
to GMMs. Gan et al. [6] showed that the GMMs can be translated to an additive
⋆Corresponding author.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 475–484, 2014.
c
⃝Springer International Publishing Switzerland 2014

476
M. Pagola et al.
fuzzy system. Ichihashi et al. [7] showed that the EM algorithm for the GMM
can be derived from the FCM fuzzy clustering, when considering a regularized
fuzzy objective function, for a proper selection of the distance metric.
Most of the existing clustering algorithms, including GMMs and FCM,impose
a probabilistic constraint on the utilized membership functions. That the ob-
tained cluster memberships of a data point must sum up to one over the derived
clusters. Such constrained membership functions are not capable of distinguish-
ing between data points which would be equally likely to belong to more than
one cluster, or data points that would be unlikely to belong to anyone of the
known clusters, usually referred as outliers [2].
A great deal of research work has been devoted to the attenuation of these
problems, both in the case of generative mixture models, and the fuzzy clustering
framework case [10].
In this work we propose a model named as Mixture of Fuzzy Models
(or Mixture of Fuzzy Membership functions) in which we take two assump-
tions: the data points are generated by a membership function and the
sum of the memberships to all of the clusters must be greater or equal to
zero. Such a way a memberships can be viewed as the probability for
one cluster to include one point.
The objective is to obtain a set of membership functions which represent
the data. These memberships can be selected as speciﬁc functions (triangular,
exponential, etc.). For example, when we create a fuzzy rule based system, we can
represent the labels with triangular membership functions. Due to the relaxation
of the constrain, the sum up to 1, the model obtained should not be inﬂuenced
by the presence of noise or outliers.
There exist some works that present a model called Fuzzy Gaussian Mixture
Models in which they used the Gaussian distribution into the calculation of
the dissimilarity in the FCM calculation [8]. But this model is diﬃerent from
the one presented here. In this work we propose a method to ﬁt diﬃerent fuzzy
membership functions to data points. We will call our model Mixture of Fuzzy
Models (MFMs).
The remainder of this work is organized as follows. In Section 2, we provide an
overview of the formulation of Gaussian mixture models, and a brief description
of fuzzy clustering algorithm. In Section 3, the proposed model of mixture fuzzy
models is formulated. In Section 4, the learning method of the MFMs is provided,
giving some examples. In section 5 we evaluate the proposed model in a clustering
applications with noisy data. Finally, in the concluding section, we summarize
and discuss our results.
2
Preliminaries
Let an unlabeled data set X = {x(1), . . . , x(m)} of m examples, and each example
x(i) = (x(i)
1 , . . . , x(i)
n ) is a vector of n dimensions.
Next we describe the basis of GMMs and the FCM algorithms and we recall
two common measures of cluster validity as compactness and separability.

Clustering Based on a Mixture of Fuzzy Models Approach
477
2.1
Gaussian Mixture Models
A generative mixture model to represent this dataset is a common approach in
the ﬁeld of statistical pattern recognition. A C-component generative mixture
model is, in essence, a superposition of weighted probability distributions of the
form:
P(x(i)) =
C

c=1
ϕcp(x(i)|Λc)
(1)
where Λc are the component distributions of the model, and ϕc are their mixing
weights (prior probabilities). The typical case of GMM is the Gaussian Mixture
Model in which the probability distributions are Gaussian distributions.
The GMMS can be formulated as a maximum likelihood problem. The objec-
tive is to estimate the parameters set Θ that maximizes L(Θ|X).
Θ∗= argmax
Θ
L(Θ|X)
(2)
being L(Θ|X) the likelihood function:
L(Θ|X) =
m
?
i=1

 C

c=1
ϕcpc(x(i)|ωc, Θc)

(3)
where the parameters of the gaussians are (ω1, . . . , ωC, Θ1, . . . , ΘC) and (ϕ1, . . . ,
ϕC) are the C mixing coeﬀcients of the C mixed components such that:
C
c=1 ϕc = 1. The usual choice to obtain the maximum likelihood estimate
of the GMMs parameters is the Expectation-Maximization algorithm [5].
2.2
Fuzzy Cluster Means
The aim of FCM is to ﬁnd the cluster centres (centroids) ωc that minimize a
dissimilarity function. The dissimilarity function measures de distance between
the point x(i) and the cluster prototype ωc:
d2
ic = ||x(i) −ωc||2
A
(4)
being ||.||A an induced norm on Rn with A a positive deﬁnite (n × n) weight
matrix. The objective function J(U, ω) is the weighted sum of dissimilarities
within each cluster:
J(U, ω) =
m

i=1
C

c=1
(μc(x(i)))bd2
ic
(5)
being U the fuzzy partition and b the weighting exponent called the degree of
fuzziness. The processing of minimizing object function J(U, ω) depends on how
centres ﬁnd their ways to the best positions, as the fuzzy memberships μc(x(i))
and norm distance d2
ic would change along with the new centres’ position.

478
M. Pagola et al.
2.3
Compactness and Separability
The objective of cluster validity is to ﬁnd the optimal C clusters. Some validity
indexes measure the degree of compactness and separation for the data structure
in all of c clusters and then ﬁnds an optimal C that each one of these optimal c
clusters is compact and separated from the other clusters [13].
Chen and Linkens [3] proposed the following expressions:
JC = 1
m
m

i=1
max(μc(x(i)))
(6)
JS = 1
K
C

c=1
C

g=c+1

1
m
m

i=0
min(μc(x(i)), μg(x(i)))

(7)
where K = C
i=1 i
3
Model Formulation
In our proposal, we suppose that each data point belongs to every fuzzy set, but
the summation of all membership degrees must be greater or equal than zero.
The result of the clustering process in the mixture of fuzzy models is going to
be a set of membership functions, and in order to obtain a cluster, we demand
two conditions:
1. The sum of all the memberships to all the sets should be maximum. The
membership functions obtained should cover the data points such a way
that the membership degrees of the points that are clearly located inside a
cluster should have large membership values to said cluster. Therefore, if the
clustering process creates membership functions that represent correctly the
clusters, the membership degrees of the points will be all 1 and therefore
the sum of these values will be maximum.
2. The diﬃerence between the membership degrees to each cluster of any data
point should be maximum. After the clustering process every data point will
have a membership degree to every cluster. A desirable property is that if a
point has a large membership to a cluster, then the memberships to the other
clusters should be low. Therefore the diﬃerences between the membership
degrees should be maximum.
Therefore we have two diﬃerent objectives, the ﬁrst one is to maximize the
memberships:
J1(Θ) = 1
m
C

c=1
m

i=1
μc(x(i))
(8)
and the second is to maximize the diﬃerences between membership degrees:
J2(Θ) = 1
m
C−1

c=1
C

g=c+1
m

i=1
|μc(x(i)) −μg(x(i))|
(9)

Clustering Based on a Mixture of Fuzzy Models Approach
479
Where Θ is the set of parameters that deﬁne the membership functions and ¯x(c)
is the center of the membership function (μc(¯x(c)) = 1). We impose the following
restrictions for all:
¯x(c)
j
≥min{x(1)
j , . . . , x(m)
j
} for all j ∈{1 . . .n} and for all c ∈{1 . . . C} (10)
¯x(c)
j
≤max{x(1)
j , . . . , x(m)
j
} for all j ∈{1 . . . n} and for all c ∈{1 . . . C}. (11)
These constrains mean that the center of the membership functions (clusters)
must be within the maximum value and the minimum value of the data points.
Depending on the membership function used we have additional constrains. For
example if we use an exponential membership function:
μc(x) = exp(−ϕ ∗

|x −¯x(c)|)2
(12)
then the value of ϕ should be greater than 0. In case we want to use a Maha-
lanobis distance based membership function:
μc(x) =
1
1 + (x −¯x(c))T A(x −¯x(c))
(13)
then the matrix A must be deﬁnite positive. Therefore our model is a multi-
objective optimization problem (MOOP) with restrictions.
The ﬁrst objective, J1, is similar to the concept of compactness and the second
objective, J2, is similar to the concept of separability used in diﬃerent cluster
validity measures. Therefore we can reformulate our problem with these objective
functions, i.e. maximize the compactness JC and maximize the separability JS.
3.1
Probability Condition or Ruspini Condition
In Mixture Models there exist a condition that all of the probabilities must sum
up to one. The same restriction was inherited in the fuzzy setting by Ruspini
[11] and others. As it was discussed previously, the FCM algorithm requires the
sum of all of the memberships of an element to all clusters must be equal to one.
In the case of a single-objective function, (where J(Θ) = J1(Θ) + J2(Θ)) we can
add an restriction to make the sum of the memberships of every element should
be one:
Jr(Θ) = J(Θ) −K 1
m
m

i=1
|1 −
C

c=1
μc(x(i))|
(14)
This term penalizes by a factor of K, the diﬃerence of the sum of the mem-
berships and one. Therefore, the larger is K, the solution is forced to obtain a
set of memberships in which the sum of the memberships in most of the training
points is close to one.
4
Learning of MFMs
The optimum solution to a multi-objective optimization problem is a set of trade-
oﬃsolutions known as Pareto optimal set. Each solution in Pareto optimal set is

480
M. Pagola et al.
referred as Pareto optimal solution. A solution is considered Pareto optimal if no
solution in entire design space is better in all objectives and constraints than it.
In this section we describe how to solve the MFMs problem by means of two dif-
ferent ways to solve a MOOP. First, we are going to convert the original problem
with multiple objectives into a single objective optimization problem. Next, by
means of evolutionary algorithms we will solve the multi-objective optimization
problem approximating a representative set of Pareto optimal solutions.
4.1
Weighted Sum Strategy
This is the most popular method to convert a MOOP into a single objective op-
timization problem (it is also known as linear scalarization). We specify weights
associated with each objective and optimize the weighted sum of objectives.
J(Θ) = w1J1(Θ) + w2J2(Θ)
(15)
Weights reﬂect the preference of each objective to be satisﬁed. The advantage of
this method is its simplicity and ease of use. This approach works the best for
problems with convex and continuous Pareto fronts.
Solving this optimization problem is a non-trivial task and may become com-
putationally expensive. The number of parameters and constrains increase lin-
early with the number of clusters to be identiﬁed. We have applied the solver
fmincon function implemented in the optimization toolbox of Matlab. This func-
tion provides a method for constrained non-linear optimization based on sequen-
tial quadratic programming.
We begin with a toy example on simulated data to demonstrate the perfor-
mance of the MFMs in a clustering scheme. The synthetic data is generated by
means of 100 samples of each of these bivariate Gaussian distribution functions:
fa = N
K
0
2
L
,
K
1 0.8
0.8 1
L	
, fb = N
K
2
0
L
,
K
1 0.8
0.8 1
L	
In Fig. 1 we show the MFMs obtained using the distance based membership
functions of equation (13) for both clusters. We can obtain a similar results
optimizing the following objective function based on the compactness and sepa-
rability:
JCS(Θ) = w1JC(Θ) + w2JS(Θ)
(16)
This methodology, the same as mixture models, have problems in the initial-
ization in sparse data sets due to ﬂat local minima. Therefore it is useful to
choose the starting points by means of the k-means algorithm or similar.
4.2
Multi-Objective Evolutionary Algorithms
Multi-objective evolutionary algorithms (MOEAs) have the potential of yielding
many Pareto optimal solutions in a single simulation. Since Evolutionary Algo-
rithms (EAs) work with a population of solutions, a simple EA can be extended
to maintain a diverse set of solutions. In our case we have used the matlab

Clustering Based on a Mixture of Fuzzy Models Approach
481
function gamultiobj which uses a controlled elitist genetic algorithm, which is
a modiﬁcation of NSGA-II [4]. This algorithm uses a non-domination criterion
based selection operator to handle multiple objectives. In Fig. 2(a) is depicted
the Pareto front obtained for the MOOP in the simulated data set (remark:
objectives have negative values due to gamultiobj only minimizes). The solver
was applied to the problem of maximization of J1 and J2. Also in Fig 2(b) are
shown the centers of the diﬃerent solutions of the Pareto optimal subset.
5
Experimental Results
We are going to use simulated data (similar to de the one used in [4]) to illustrate
the advantages of the introduced MFMs over conventional approaches.
The synthetic data is generated by means of 100 points of each of these
three bivariate Gaussian distribution functions: fa = N
K
0
3
L
,
K
2 0.5
0.5 0.5
L	
, fb =
N
K3
0
L
,
K1 0
0 0.1
L	
, fc = N
K−3
0
L
,
K 2
−0.5
−0.5 0.5
L	
The ﬁnal 100 points, outliers and noise, are generated from a bivariate Gaus-
sian distribution with each of its components in the interval [−10, 10] (see Fig.
3). We compare our result with the ones obtained by the algorithms FCM [1],
GMMs [9], and PFCM [10]. All of the algorithms start with the centres obtained
by the k-means algorithm. In Table 1, we provide the Rand index for every
method. The values depicted are the average mean after 10 executions of every
algorithm. The MFMs uses as the objective function eq. (14) with K = 1 and
eq. (16), both with w1 = w2 = 0.5 and the membership function of eq. (13).
Parameter b of FCM (see eq. 5) is equal to 2 and in PFCM a = 2 and b = 4 (see
[10]).
−4
−3
−2
−1
0
1
2
3
4
5
−3
−2
−1
0
1
2
3
4
5
−3
−2
−1
0
1
2
3
4
5
−3
−2
−1
0
1
2
3
4
5
6
(a)
(b)
Fig. 1. (a) Simulated data (b) MFMs obtained in the simulated data

482
M. Pagola et al.
Table 1. Precision rates for each cluster
Rand Index
FCM
0.9656
GMMs
0.7277
PFCM
0.9656
Jr
0.9610
JCS
0.9347
−4
−3
−2
−1
0
1
2
3
4
5
−3
−2
−1
0
1
2
3
4
5
6
(a)
(b)
Fig. 2. (a) Pareto front obtaind by the NSGA-II algorithm (matlab version gamultiobj
(b) Simulated data and the centers of every Pareto solution obtained by the NSGA-II
algorithm (matlab version gamultiobj)
−10
−5
0
5
10
−10
−8
−6
−4
−2
0
2
4
6
8
10
Fig. 3. Synthetic data set with outliers
As we can see although FCM have the restriction that the sum of all of
the memberships of an element to all of the clusters must be one, it generate
the clusters eﬀciently. However GMMs create a cluster that covers most of the
outliers, and then it fails in most of the elements of one cluster. PFCM and the
proposed methodology obtain similar results.

Clustering Based on a Mixture of Fuzzy Models Approach
483
−10
−5
0
5
10
−10
−8
−6
−4
−2
0
2
4
6
8
10
−10
−5
0
5
10
−10
−8
−6
−4
−2
0
2
4
6
8
10
(a)
(b)
Fig. 4. (a) MFMs obtained in the synthetic data with outliers (b) Outliers detected
with threshold 0.25
In Fig. 4 the MFMs obtained in the simulated data are shown. In the FCM and
GMMs is not possible to identify outliers, but PFCM, through the typicalities
and MFMs could detect outliers. In MFMs an outlier can be the element which
all the memberships to the clusters are less than a threshold. In the PFCM an
outlier can be a point such that its typicality is less than threshold to every
cluster. In Fig. 4(b) are shown the points detected as outliers with the threshold
equal to 0.25 in MFMs.
6
Conclusions and Future Research
In this work we have proposed a simple clustering methodology in which we ﬁt
the data to a set of membership functions relaxing the restriction of Ruspini. This
a preliminary work in which we just prove in simple data sets the performance
of this methodology. Future research will be to test and compare the eﬀciency
in larger datasets and more complex problems.
Analyzing these results we plan to apply this methodology to obtain the
membership functions of linguistic labels in the deﬁnition rule based systems.
Also it can be easily applied to histogram ﬁtting and thresholding. Our model
has the advantage that the user can choose the membership function (if it is
previously known) or to test diﬃerent membership functions and select the best
one. Moreover we plan to test diﬃerent objective functions so the clustering
structure obtained could satisfy diﬃerent objectives. If the property that deﬁnes
the data is the connectedness, then this approach is not useful. This type of
problems are correctly solved by means of agglomerative methods like Single-
link [12]. This methodology works only if the data is clustered in hyper spherical
shapes.
Another problem that we plan to tackle is semi-supervised learning. The pro-
posed model can be easily adapted to a semi-supervised learning problem adding
new restrictions to the MOOP problem.

484
M. Pagola et al.
References
1. Bezdek, J.C.: Pattern Recognition With Fuzzy Objective Function Algorithms.
Plenum, New York (1981)
2. Chatzis, S.P., Tsechpenakis, G.: A possibilistic clustering approach toward gener-
ative mixture models. Pattern Recognition 45, 1819–1825 (2012)
3. Chen, M.Y., Linkens, D.A.: Rule-base self-generation and simpliﬁcation for data-
driven fuzzy models. Fuzzy Sets and Systems 142, 243–265 (2004)
4. Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A Fast and Elitist Multiobjec-
tive Genetic Algorithm: NSGA-II. IEEE Transactions on Evolutionary Computa-
tion 6(2), 182–197 (2002)
5. Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the Royal Statistical Society. Series B
(Methodological) 39, 1–38 (1977)
6. Gan, M.T., Hanmandlu, M., Tan, A.H.: From a Gaussian mixture model to additive
fuzzy systems. IEEE Transactions on Fuzzy Systems 13(3), 303–316 (2005)
7. Ichihashi, H., Honda, K., Tani, N.: Gaussian mixture pdf approximation and fuzzy
c-means clustering with entropy regularization. In: Proceedings of the Fourth Asian
Fuzzy System Symposium, pp. 217–221 (2000)
8. Ju, Z., Liu, H.: Fuzzy Gaussian Mixture Models. Pattern Recognition 45, 1146–
1158 (2012)
9. McLachlan, G., Peel, D.: Finite Mixture Models. John Wiley & Sons, Inc. (2000)
10. Pal, N., Pal, K., Keller, J., Bezdek, J.: A possibilistic fuzzy c-means clustering
algorithm. IEEE Transactions on Fuzzy Systems 13(4), 517–530 (2005)
11. Ruspini, E.H.: Numerical Methods for Fuzzy Clustering. Information Sciences (2),
319–350 (1970)
12. Sibson, R.: SLINK: an optimally eﬃcient algorithm for the single-link cluster
method. The Computer Journal (British Computer Society) 16(1), 30–34 (1973)
13. Wu, K.L., Yang, M.S.: A cluster validity index for fuzzy clustering. Pattern Recog-
nition Letters 26, 1275–1291 (2005)

Analogical Classiﬁcation: A Rule-Based View
Myriam Bounhas1, Henri Prade2, and Gilles Richard2
1 LARODEC Laboratory, ISG de Tunis, 41 rue de la Libert´e, 2000 Le Bardo, Tunisia
& Emirates College of Technology, P.O. Box: 41009, Abu Dhabi, United Arab Emirates
2 IRIT – CNRS, 118, route de Narbonne, Toulouse, France
myriam bounhas@yahoo.fr, {prade,richard}@irit.fr
Abstract. Analogical proportion-based classiﬁcation methods have been intro-
duced a few years ago. They look in the training set for suitable triples of ex-
amples that are in an analogical proportion with the item to be classiﬁed, on a
maximal set of attributes. This can be viewed as a lazy classiﬁcation technique
since, like k-nn algorithms, there is no static model built from the set of examples.
The amazing results (at least in terms of accuracy) that have been obtained from
such techniques are not easy to justify from a theoretical viewpoint. In this paper,
we show that there exists an alternative method to build analogical proportion-
based learners by statically building a set of inference rules during a preliminary
training step. This gives birth to a new classiﬁcation algorithm that deals with
pairs rather than with triples of examples. Experiments on classical benchmarks
of the UC Irvine repository are reported, showing that we get comparable results.
Introduction
Comparing objects or situations and identifying in what respects they are identical (or
similar) and in what respects they are different, is a basic type of operations at the core
of many intelligent activities. A more elaborate operation is the comparison between
pairs of objects or situations, where a comparison has already been done inside the
pairs. This corresponds to the idea of analogical proportions, i.e. statements of the form
“A is to B as C is to D”, denoted A : B :: C : D, expressing the fact “A differs from
B as C differs from D”, as well as “B differs from A as D differs from C” [5].
Analogical reasoning has been recognized for a long time as a powerful heuristic tool
for solving problems. In fact, analogical proportions are not explicitly used in general.
Compound situations identiﬁed as analogous are rather put in parallel, leading to the
plausible conclusion that what holds in one case should also hold in the other case (up
to suitable transpositions). However, analogical reasoning can also be directly based
on analogical proportions. This requires a formalized view of these proportions. Such
a modeling has been only recently developed in algebraic or logical settings [2,8,5,6].
Then analogical proportions turn to be a powerful tool in classiﬁcation tasks [4].
We assume that the objects or situations A, B, C, D are represented by vectors of
attribute values, denoted a, b, c, d. The analogical proportion-based approach to classi-
ﬁcation relies on the idea that the unknown class x = cl(d) of a new instance d, may
be predicted as the solution x of an equation expressing that the analogical proportion
cl(a) : cl(b) :: cl(c) : x holds between the classes. This is done on the basis of triples
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 485–495, 2014.
c⃝Springer International Publishing Switzerland 2014

486
M. Bounhas, H. Prade, and G. Richard
of examples a, b and c of the training set that are such that the analogical proportion
a : b :: c : d holds on vector components for all, or at least on a large number of, the
attributes describing the items. This approach has been tested on benchmarks [4] where
results competitive with the ones of classical machine learning methods have been ob-
tained. These good results have remained largely unexplained since it looks unclear
why this analogical proportion-based approach may be so effective, beyond the general
merits of analogy. In this paper, we investigate a new type of algorithm based on the
induction of particular rules induced from pairs of examples, which can still be related
to analogical proportions, thus providing some light on the underlying learning process.
The paper is organized as follows. First a background on analogical proportions is
provided, emphasizing noticeable properties important for application to classiﬁcation,
before discussing how they can be applied to this task. Then the new rule-based ap-
proach is contrasted with the original triples-based approach. Algorithms are proposed
and their results on machine learning benchmarks are reported and discussed.
A Short Background on Analogical Proportions
Analogical proportions are statements of the form “A is to B as C is to D”, which have
been supposed to continue to hold when the pairs (A, B) and (C, D) are exchanged, or
when the terms B and C are permuted, just like numerical proportions, since Aristotle
time; see, e.g., [7]. Thus, A : B :: C : D is equivalent to C : D :: A : B (symmetry),
and A : B :: C : D is equivalent to A : C :: B : D (central permutation). By combining
symmetry and permutation, this leads to 8 equivalent forms.
In this paper, A, B, C, D are represented by Boolean vectors. Let a denote a com-
ponent of such a vector a. Then an analogical proportion between such vectors can
be expressed componentwise, in a logical manner under various equivalent forms [5].
One remarkable expression of the analogical proportion is given by the expression
a : b :: c : d = (a ∧¬b ≡c ∧¬d) ∧(¬a ∧b ≡¬c ∧d).
As can be seen, this expression of the analogical proportion uses only dissimilarities
and could be informally read as what is true for a and not for b is exactly what is true
for c and not for d, and vice versa. This logical expression makes clear in an analogical
proportion a : b :: c : d that a differs from b as c differs from d and, conversely, b
differs from a as d differs from c. The 6 cases (among 24 = 16 possible entries) where
the above Boolean expression is true are given in the truth Table 1.
Table 1. When an analogical proportion is true
a b c d a : b :: c : d
0 0 0 0|
1
1 1 1 1|
1
0 0 1 1|
1
1 1 0 0|
1
0 1 0 1|
1
1 0 1 0|
1

Analogical Classiﬁcation: A Rule-Based View
487
It can be easily checked on the above truth Table 1 that the logical expression of
the analogical proportion indeed satisﬁes symmetry and central permutation. Assuming
that an analogical proportion holds between four binary items, three of them being
known, then one may try to infer the value of the fourth one. The problem can be stated
as follows. Given a triple (a, b, c) of Boolean values, does there exist a Boolean value
x such that a : b :: c : x = 1, and in that case, is this value unique? It is easy to
see that there are cases where the equation has no solution since the triple a, b, c may
take 23 = 8 values, while A is true only for 6 distinct 4-tuples. Indeed, the equations
1 : 0 :: 0 : x = 1 and 0 : 1 :: 1 : x = 1 have no solution. It is easy to prove that the
analogical equation a : b :: c : x = 1 is solvable iff (a ≡b) ∨(a ≡c) holds true. In
that case, the unique solution is given by x = a ≡(b ≡c). Note that due to symmetry
and permutation properties, there is no need to consider the equations x : b :: c : d = 1,
a : x :: c : d = 1, and a : b :: x : d = 1 that can be handled in an equivalent way.
Analogical Proportions and Classiﬁcation
Numerical proportions are closely related to the ideas of extrapolation and of linear
regression, i.e., to the idea of predicting a new value on the ground of existing values,
and to the idea of inducing general laws from data. Analogical proportions may serve
similar purposes. The equation solving property recalled above is at the root of a brute
force method for classiﬁcation. It is based on a kind of proportional continuity principle:
if the binary-valued attributes of 4 objects are componentwise in analogical proportion,
then this should still be the case for their classes. More precisely, having a 2-class
classiﬁcation problem, and 4 Boolean objects a, b, c, d over Bn, 3 in the training set
with known classes cl(a), cl(b), cl(c), the 4th being the object to be classiﬁed in one of
the 2 classes, i.e. cl(d) ∈B is unknown, this principle can be stated as:
∀i ∈[1, n], ai : bi :: ci : di = 1
cl(a) : cl(b) :: cl(c) : cl(d) = 1
Then, if the equation cl(a) : cl(b) :: cl(c) : x = 1 is solvable, we can allocate its
solution to cl(d). This principle can lead to diverse implementations; see next section.
The case of attributes on discrete domains and of a number of classes larger than 2
can be handled as easily as the binary case. Indeed, consider a ﬁnite attribute domain
{v1, · · · , vm}. Note that the attribute may also be the class itself. This attribute (or
the class), say A, can be straightforwardly binarized by means of the m properties
“having value vi, or not”. Consider the partial description of objects a, b, c, and d
wrt A. Assume, for instance, that objects a and c have value v1, while objects b and
d have value v2. This situation is summarized in Table 2 where the respective truth-
values of the four objects wrt each binary property “having value vi” are indicated. As
can be seen on this table, an analogical proportion holds true between the four objects
for each binary property, and in the example, can be more compactly encoded as an
analogical proportion between the attribute values themselves, namely here: v1 : v2 ::
v1 : v2. More generally, x and y denoting possible values of a considered attribute
A, the analogical proportion between objects a, b, c, and d holds for A iff the 4-
tuple (A(a), A(b), A(c), A(d)) is equal to one 4-tuple having one of the three forms
(s, s, s, s), (s, t, s, t), or (s, s, t, t).

488
M. Bounhas, H. Prade, and G. Richard
Table 2. Handling non binary attributes
v1 v2 v3 · · · vm
a 1 0 0 · · ·
0 | v1
b 0 1 0 · · ·
0 | v2
c 1 0 0 · · ·
0 | v1
d 0 1 0 · · ·
0 | v2
A training set T S of examples xk = (xk1, ..., xki, ..., xkn) together with their class
cl(xk), with k = 1, t may also be read in an analogical proportion style: “x1 is to
cl(x1) as x2 is to cl(x2) as · · · as xt is to cl(xt)”. However note that xk and cl(xt) are
vectors of different dimensions. This may still be written (abusively) as x1 : cl(x1) ::
x2 : cl(x2) :: · · · :: xt : cl(xt). Note that this view exactly ﬁts with the idea that
in a classiﬁcation problem there exists a classiﬁcation function that associates a unique
class with each object, which is unknown, but exempliﬁed by the training set. Indeed
xk : cl(xk) :: xk : cl′(xk) with cl(xk) ̸= cl′(xk) is forbidden, since it cannot hold as
a generalized analogical proportion obeying to a pattern of the form (s, t, s, t) where s
and t belong to different spaces.
Postulating the central permutation property, the informal analogical proportion xi :
cl(xi) :: xj : cl(xj) linking examples xi and xj can also be rewritten as xi : xj ::
cl(xi) : cl(xj) (still informally as we deal with vectors of different dimensions). This
suggests a new reading of the training set, based on pairs. Namely, the ways vectors
xi and xj are similar / dissimilar should be related to the identity or the difference
of classes cl(xi) and cl(xj). Given a pair of vectors xi and xj, one can compute
the set of attributes A(xi, xj) where they agree (i.e. they are equal) and the set of
attributes D(xi, xj) where they disagree (i.e. they are not equal). Suppose, we have in
the training set T S, both the pair (xi, xj), and the example xk which once paired with
x0 has exactly the same disagreement set as D(xi, xj) and moreover with the changes
oriented in the same way. Note that although A(xi, xj) = A(xk, x0), the 4 vectors
are not everywhere equal on this subset of attributes. Then we have a perfect analogical
proportion componentwise, between the 4 vectors (of the form (s, s, s, s) or (s, s, t, t)
on the agreement part of the components, and of the form (s, t, s, t) on the disagreement
set). Indeed, the above view straightforwardly extends from binary-valued attributes to
attributes with ﬁnite domains. Thus, working with pairs, we can implicitly reconstitute
4-tuples of vectors that form an analogical proportion as in the triple-based brute force
approach to classiﬁcation. We now discuss the algorithmic aspects of this approach.
Analogical Classiﬁcation: The Standard View
Before introducing the analogical classiﬁers, let us restate the classiﬁcation problem.
Let T be a data set where each vector x = (x1, ..., xi, ..., xn) ∈T is a set of n feature
values representing a piece of data. Each vector x is assumed to belong to a unique
class cl(x) ∈C = {c1, ..., cl}, where C is ﬁnite and covered through the data set (in
the binary class case, l = 2). If we suppose that cl is known on a subset T S ⊂T ,

Analogical Classiﬁcation: A Rule-Based View
489
given a new vector y = (y1, ..., yi, ..., yn) /∈T S, the classiﬁcation problem amounts to
assign a plausible value cl(y) on the basis of the examples stored in T S.
Learning by analogy, as developed in [1], is a lazy learning technique which uses a
measure of analogical dissimilarity between 4 objects. It estimates how far 4 situations
are from being in analogical proportion. Roughly speaking, the analogical dissimilarity
ad between 4 Boolean values is the minimum number of bits that have to be switched to
get a proper analogy. Thus ad(1, 0, 1, 0) = 0, ad(1, 0, 1, 1) = 1 and ad(1, 0, 0, 1) = 2.
Thus, A(a, b, c, d) holds if and only if ad(a, b, c, d) = 0. Moreover ad differentiates
two types of cases where analogy does not hold, namely the 8 cases with an odd number
of 0 and an odd number of 1 among the 4 Boolean values, such as ad(0, 0, 0, 1) = 1
or ad(0, 1, 1, 1) = 1, and the two cases ad(0, 1, 1, 0) = ad(1, 0, 0, 1) = 2. When,
instead of having 4 Boolean values, we deal with 4 Boolean vectors in Bn, we add the
ad evaluations componentwise to get the analogical dissimilarity between the 4 vectors,
which leads to an integer belonging to the interval [0, 2n]. This number estimates how
far the 4 vectors are from building, componentwise, a complete analogy. It is used in
[1] in the implementation of a classiﬁcation algorithm where the input is a set S of
classiﬁed items, a new item d to be classiﬁed, and an integer k. It proceeds as follows:
Step 1: Compute the analogical dissimilarity ad between d and all the triples in S3
that produce a solution for the class of d.
Step 2: Sort these n triples by the increasing value of ad wrt with d.
Step 3: Let p be the value of ad for the k-th triple, then ﬁnd k′ as being the greatest
integer such that the k′-th triple has the value p.
Step 4: Solve the k′ analogical equations on the label of the class. Take the winner
of the k′ votes and allocate this winner as the class of d.
This approach provides remarkable results and, in several cases, outperforms the best
known algorithms [4]. Another equivalent approach [6] does not use a dissimilarity
measure but just applies the previous continuity principle, adding ﬂexibility by allow-
ing to have some components where analogy does not hold. A majority vote is still
applied among the candidate voters. Any triple a, b, c, such that the cardinal of the set
{i ∈[1, n]|A(ai, bi, ci, di) holds and A(cl(a), cl(b), cl(c), cl(d)) is solvable} is maxi-
mal, belongs to the candidate voters.
Analogical Classiﬁcation: A Rule-Based View
We claim here that analogical classiﬁers behave as if a set of rules was build inductively
during a pre-processing stage. To support intuition, we use an example inspired from the
Golf data set (UCI repository [3]). This data set involves 4 multiple-valued attributes:
1: Outlook: sunny or overcast or rainy. ; 2: Temperature: hot or mild or cool ;
3: Humidity: high or normal. ; 4: Windy: true or false.
Two labels are available: ‘Yes’ (play) or ‘No’ (don’t play).
Main Assumptions. Starting from a ﬁnite set of examples, 2 main assumptions are
made regarding the behavior of the function cl:

490
M. Bounhas, H. Prade, and G. Richard
– Since the target relation cl is assumed to be a function, when 2 distinct vectors x,
y have different labels (cl(x) ̸= cl(y)), the cause of the label switch is to be found
in the switches of the attributes that differ. Take x and y in the Golf data set, as:
x = (overcast, mild, high, false) and cl(x) = Y es
y = (overcast, cool, normal, false) and cl(y) = No
then the switch in attributes 2 and 3 is viewed as the cause of the ‘Yes’-‘No’ switch.
– When 2 distinct x and y are such that cl(x) = cl(y), this means that cl does not
preserve distinctness, i.e. cl is not injective. We may then consider that the label
stability is linked to the particular value arrangement of the attributes that differ.
Patterns. Let us now formalize these ideas. Given 2 distinct vectors x and y, they deﬁne
a partition of [1, n] as A(x, y) = {i ∈[1, n]|xi = yi} and D(x, y) = [1, n]\A(x, y) =
{i ∈[1, n]|xi ̸= yi}. Given J ⊆[1, n], let us denote x|J the subvector of x made of
the xj, j ∈J. Obviously, x|A(x,y) = y|A(x,y) and, in the binary case, when we know
x|D(x,y), we can compute y|D(x,y). In the binary case, the pair (x, y) allows us to
build up a disagreement pattern Dis(x, y) as a list of pairs (value, index) where the
2 vectors differ. with n = 6, x = (1, 0, 1, 1, 0, 0), y = (1, 1, 1, 0, 1, 0), Dis(x, y) =
(02, 14, 05). It is obvious that having a disagreement pattern Dis(x, y) and a vector x
(resp. y), we can get y (resp. x). In the same way, the disagreement pattern Dis(y, x)
is deducible from Dis(x, y). For the previous example, Dis(y, x) = (12, 04, 15).
In the categorical case, the disagreement pattern is a bit more sophisticated as we
have to store the changing values. Then the disagreement pattern Dis(x, y) becomes a
list of triple (value1, value2, index) where the 2 vectors differ, with value1 being the
attribute value for x and value2 being the attribute value for y. For instance, with the
previously described Golf dataset, for the pair of given examples x and y, Dis(x, y) is
{(mild, cool)2, (high, normal)3}. Then we have two situations:
1. x and y have different labels, i.e. cl(x) ̸= cl(y). Their disagreement pattern
Dis(x, y) is called a change pattern. Then Dis(y, x) is also a change pattern.
2. x and y have the same label cl(x) = cl(y). Their disagreement pattern Dis(x, y)
is called a no-change pattern. Then Dis(y, x) is also a no-change pattern.
To build up a change (resp. no-change) pattern, we have to consider all the pairs (x, y)
such that cl(x) ̸= cl(y) (resp. such that cl(x) = cl(y)). We then build 2 sets of patterns
Pch and Pnoch, each time keeping only one of the 2 patterns Dis(x, y) and Dis(y, x)
to avoid redundancy. As exempliﬁed below, these 2 sets are not disjoint in general. Take
n = 6, and assume we have the 4 binary vectors x, y, z, t in T S:
- x = (1, 0, 1, 1, 0, 0), y = (1, 1, 1, 0, 1, 0) with cl(x) = 1 and cl(y) = 0. Then, for
(x, y), the disagreement pattern is a change pattern, i.e., (02, 14, 05) ∈Pch.
- z =(0, 0, 1, 1, 0, 1), t=(0, 1, 1, 0, 1, 1) with cl(z)=cl(t). They have the same dis-
agreement pattern as x and y, which is now a no-change pattern (02, 14, 05) ∈Pnoch.
Now, given an element x in T S whose label is known, and a new element to be classiﬁed
y, if the disagreement pattern Dis(x, y) belongs to Pch ∩Pnoch, we do not get any hint
regarding the label of y. Then we remove the patterns in Pch ∩Pnoch: the remaining
patterns are the valid patterns (still keeping the same notations for the resulting sets).

Analogical Classiﬁcation: A Rule-Based View
491
Rules. Thanks to the concept of pattern, it is an easy game to provide a formal deﬁnition
of the 2 above principles. We get 2 general classiﬁcation rules, corresponding to dual
situations, for a new element y to be classiﬁed:
Change Rule: ∃x ∈T S, ∃D ∈Pch|(Dis(x, y) = D) ∨(Dis(y, x) = D)
cl(y) ̸= cl(x)
NoChange Rule: ∃x ∈T S, ∃D ∈Pnoch|(Dis(x, y) = D) ∨(Dis(y, x) = D)
cl(y) = cl(x)
NoChange rules tell us when a new item y to be classiﬁed should get the class of its
associated example x, and Change rules tell the opposite. Let us note that if there is no
valid pattern, then we cannot build up any rule, then we cannot predict anything! This
has never been the case for the considered benchmarks.
Implementation
It is straightforward to implement the previous ideas.
1. Construct from T S the sets Pch and Pnoch of all disagreement patterns.
2. Remove from Pch and from Pnoch the patterns belonging to Pch ∩Pnoch to get the
set of valid patterns.
The remaining change patterns in Pch and no-change patterns in Pnoch are used to
build up respectively the Change Rule Set Rch and No-Change Rule Set Rnoch. In
this context, we have implemented two different classiﬁers: the Change Rule based
Classiﬁer (CRC) and the No Change Rule based Classiﬁer (NCRC), which have the
same principles in all respect. The only difference is in the classiﬁcation phase where
the CRC only uses the set Pch of pattern and applies the Change rules, whereas the
second classiﬁer NCRC uses the no-change patterns Pnoch and applies the No-Change
rules to classify new items.
Classiﬁcation. The classiﬁcation process for CRC and NCRC are detailed in the
following algorithms 1 and 2, where the Boolean function Analogy(x, x′, y) is true
if and only if card({cl(x), cl(x′), cl(y)}) ≤2. For the NCRC, the Analogy(x, x′, y)
always has a solution since classes associated to any No-Change rule r in Rnoch are
homogeneous. In terms of complexity, the algorithms are still cubic in the size of T S
since the disagreement pattern sets have a maximum of n2 elements and we still have
to check every element of T S to build up a relevant pair with y.
With our approach, contrary to k-nn approaches, we always deal with pairs of ex-
amples: i) to build up the rules, ii) to classify a new item, we just associate to this item
another one to build a pair in order to trigger a rule. Moreover, the two pairs of items in-
volved in an analogical proportion are not necessarily much similar as pairs, beyond the
fact they should exhibit the same dissimilarity. An analogical view of the nearest neigh-
bor principle could be “close/far instances are likely to have the same/possibly different
class”, making an assumption that the similarity of the classes is related to the similar-
ity of the instances. This does not ﬁt, e.g., our No-Change rules where the similarity
of the classes is associated with dissimilarities of the instances. More generally, while

492
M. Bounhas, H. Prade, and G. Richard
Algorithm 1. Change Rule Classiﬁer
Given a new instance y′ /∈T S to be classiﬁed.
CandidateRules(cj) = 0, for each j ∈[1, l] (in the binary class case, l = 2).
for each y in T S do
Construct the disagreement patterns D(y, y′) and D(y′, y)
for each change rule r ∈Rch // r has a pattern D(x, x′) do
if Analogy(x,x′, y) AND (D(y, y′) = D(x, x′) OR D(y′, y) = D(x, x′) ) then
if (cl(x) = cl(y)) then c∗= cl(x′) else c∗= cl(x) end if
CandidateRules(c∗) + +.
end if
end for
end for
cl(y′) = arg maxcj CandidateRules(cj)
Algorithm 2. No Change Rule Classiﬁer
Given a new instance y′ /∈T S to be classiﬁed.
CandidateRules(cj) = 0, for each j ∈Dom(cj).
for each y in T S do
Construct the disagreement patterns D(y, y′) and D(y′, y)
for each no change rule r ∈Rnoch // r has a pattern D(x, x′) do
if Analogy(x,x′, y) AND (D(y, y′) = D(x, x′) OR D(y′, y) = D(x, x′)) then
c∗= cl(y)
CandidateRules(c∗) + +.
end if
end for
end for
cl(y′) = arg maxcj CandidateRules(cj)
k-nn-like classiﬁers focus on the neighborhood of the target item, analogical classiﬁers
“take inspiration” of information possibly far from the immediate neighborhood.
Example. Let’s continue with the previous Golf example to show the classiﬁcation
process in Algorithm1. Given three change rules r1, r2 and r3:
r1(Y es−No) = {(sunny, overcast)1, (false, true)4}
r2(No−Y es) = {(cool, mild)2, (high, normal)3}
r3(No−Y es) = {(rainy, overcast)1, (false, true)4},
and a new instance y′ to be classiﬁed: y′ : overcast, mild, normal, true, →?
Assume that there are three training examples y1, y2 and y3 in Ts:
y1 : sunny, mild, normal, false, →Y es
y2 : overcast, cool, high, true, →No
y3 : rainy, mild, normal, false, →No
We note that disagreement patterns p1, p2 and p3 corresponding respectively to the
pairs (y1, y′), (y2, y′) and (y3, y′) match respectively the change rules r1, r2 and r3.
Inferring the ﬁrst rule predict a ﬁrst candidate class “No” for y′. In the same manner

Analogical Classiﬁcation: A Rule-Based View
493
the second rule predict a class “Y es” and the third one also predict “Y es”. The rule-
based inference produces the following set of candidate classes for y′: Candidate =
{No, Y es, Y es}. So the most plausible class for y′ is “Y es”.
Experimental Results and Comparison
This section provides experimental results for the two analogical proportion-based clas-
siﬁers. The experimental study is based on several data sets selected from the U.C.I.
machine learning repository [3]. A brief description of these data sets is given in Ta-
ble 3. We note that for all classiﬁcation results given in the following, only half of the
Table 3. Description of datasets
Datasets
Instances Attributes Classes
Breast cancer
286
9
2
Balance
625
4
3
Tic tac toe
958
9
2
Car
743
7
4
Monk1
432
6
2
Monk 2
432
6
2
Monk3
432
6
2
training set is used to extract patterns. We ensured that all class labels are represented
in this data set. The classiﬁcation results for the CRC or NCRC are summarized in the
ﬁrst and second columns of Table 4. We also tested a hybrid version of these classiﬁers
called Hybrid Analogical Classiﬁer (HAC) based on the following process. Given an
instance y′ to classify,
1. Merge the two rule subsets Rch and Rnoch into a single rule set Rchnoch.
2. Assign to y′ the class label with the highest number of candidate rules in Rchnoch.
Classiﬁcation results for HAC are given in Table 4, where we also give the mean number
of Change (MeanCh) and No-Change rules (MeanNoCh) generated for each data set.
In order to compare analogical classiﬁers with other classiﬁcation approaches, Table
5 includes classiﬁcation results of some machine learning algorithms (the SVM, k-
nearest neighbors IBK with k=10 and the propositional rule learner JRip) obtained by
using the Weka software. By analyzing classiﬁcation performance in Table 4 we can
see that:
• Overall, the analogical classiﬁers show good performance to classify test examples
(at least for one of CRC and NCRC), especially NCRC.
• If we compare classiﬁcation results for the two analogical classiﬁers, CRC and NCRC,
we see that NCRC seems to be more efﬁcient than CRC for almost all data sets, except
the case of “Tic tac toe” where the two classiﬁers have the same accuracy.

494
M. Bounhas, H. Prade, and G. Richard
Table 4. Classiﬁcation accuracies: mean and standard deviation of 10 cross-validations
Datasets
CRC
NCRC
HAC
MeanCh MeanNoCh
Breast cancer 50.03 ± 8.03 74.03±7.48 73.39±8.44
6243.4
8738.5
Balance
82.82 ±5.8 91.02±4.44 90.51 ± 4.27 31736.2
20805.4
Tic tac toe
98.3±5.11
98.3±5.11
98.3±5.11
74391.9
86394.2
Car
79.54±4.23 95.02± 2.16 92.6 ±2.69
36526.6
20706.1
Monk1
90.52±6.16
100±0
99.54 ±1.4
9001.2
8644.6
Monk2
78.02 ±4.71
100±0
94.68 ± 4.38 7245.9
10607.8
Monk3
91.93±7.04
97.93±1.91 97.93±1.91
10588.0
10131.7
Table 5. Classiﬁcation results of some known machine learning algorithms
Datasets
SVM IBK(k=10) JRip
Breast cancer 69.58
73.07
70.97
Balance
90.24
83.84
71.68
Tic tac
98.32
98.64
97.80
Car
91.65
91.92
87.88
Monk1
75.0
95.60
94.44
Monk2
67.12
62.96
66.43
Monk3
100
98.37
98.61
• HAC shows good performance if compared to CRC and very close accuracies to
NCRC for “Balance, Tic tac toe, Monk1 and Monk3”. For the remaining datasets, the
lower classiﬁcation accuracy of Change rules may affect the efﬁciency of HAC.
• In general, analogical classiﬁers (especially NCRC) show very good performance
when compared to some of existing algorithms. NCRC signiﬁcantly outperforms all
other classiﬁers for all tested data sets (bold results in Table 5) except to some extent
for “Monk3” and SVM. We see that NCRC is largely better than other classiﬁers, in
particular for data sets “Monk1”, “Monk2” and “Car”.
• The classiﬁcation success of NCRC for “Monks” datasets with noisy data and “Bal-
ance” and “Car” (which have multiple classes) demonstrates its ability to deal with
noisy and multiple class data sets.
• The analogy-based classiﬁers seem to be very efﬁcient when classifying data sets with
a limited number of attribute values and seems to have more difﬁculties for classifying
data sets with a large number of attribute values. In order to evaluate analogical classi-
ﬁers such a dataset, we tested CRC and NCRC on “Cancer” (9 attributes, each of them
having 10 different labels). From this additional test, we note that analogical classiﬁers
are signiﬁcantly less efﬁcient on “Cancer” when compared to the state of the art algo-
rithms. By contrast, if we look at the 3 “Monks” and ”Balance” data sets, we note that
these data sets have a smaller number of attributes and more importantly all attributes
have a reduced number of possible values (the maximum number of possible attribute
values in “Balance” and “Monks” is 5, and most of attributes have only 3 possible la-
bels). This clearly departs from the “Cancer” situation. So we may say that this latter
dataset is closer to a data set with numerical rather than categorical data. The proposed

Analogical Classiﬁcation: A Rule-Based View
495
classiﬁers are basically designed for handling categorical attributes. We plan to extend
analogical rule-based classiﬁers in order to support numerical data in future.
• In Table 4 we see that a huge number of rules of the two kinds are generated. We
may wonder if a reduced subset of rules could lead to the same accuracy. This would
mean that there are some redundancy among each subset of rules, raising the question
of how to detect it. We might even wonder if all the rules have the same “relevance”,
which may also mean that some rules have little value in terms of prediction, and should
be identiﬁed and removed. This might also contribute to explain why CRC has results
poorer than NCRC in most cases.
• In the case of NCRC, we come apparently close to the principle of a k-nn classiﬁer,
since we use nearest neighbors for voting, but here some nearest neighbors are disqual-
iﬁed because there is no NoChange rule (having the same disagreement pattern) that
supports them.
Concluding Remarks
This paper has shown that analogical classiﬁcation can rely on a rule-based technique,
which contrasts with the existing implementations which are mainly lazy techniques.
In the proposed approach, the rules are built at compile time, ofﬂine with respect to the
classiﬁcation process itself, where this set of rules is applied to new unclassiﬁed items
in order to predict their class. This view brings new highlights in the understanding of
analogical classiﬁcation and may make this kind of learner more amenable to be mixed
with logical ones like the ones coming from Inductive Logic Programming.
References
1. Bayoudh, S., Miclet, L., Delhay, A.: Learning by analogy: A classiﬁcation rule for binary and
nominal data. In: Proc. Inter. Conf. on Artiﬁcial Intelligence, IJCAI 2007, pp. 678–683 (2007)
2. Lepage, Y.: Analogy and formal languages. Electr. Notes Theor. Comput. Sci. 53 (2001)
3. Mertz, J., Murphy, P.: Uci repository of machine learning databases,
ftp://ftp.ics.uci.edu/pub/machine-learning-databases
4. Miclet, L., Bayoudh, S., Delhay, A.: Analogical dissimilarity: deﬁnition, algorithms and two
experiments in machine learning. JAIR 32, 793–824 (2008)
5. Miclet, L., Prade, H.: Handling analogical proportions in classical logic and fuzzy logics set-
tings. In: Sossai, C., Chemello, G. (eds.) ECSQARU 2009. LNCS, vol. 5590, pp. 638–650.
Springer, Heidelberg (2009)
6. Prade, H., Richard, G.: Reasoning with logical proportions. In: Lin, F.Z., Sattler, U.,
Truszczynski, M. (eds.) Proc. 12th Int. Conf. on Principles of Knowledge Representation and
Reasoning, KR 2010, Toronto, May 9-13, pp. 545–555. AAAI Press (2010)
7. Prade, H., Richard, G.: From analogical proportion to logical proportions. Logica Univer-
salis 7(4), 441–505 (2013)
8. Stroppa, N., Yvon, F.: Du quatri`eme de proportion comme principe inductif: une propo-
sition et son application `a l’apprentissage de la morphologie. Traitement Automatique des
Langues 47(2), 1–27 (2006)

Multilabel Prediction with Probability Sets:
The Hamming Loss Case
Sebastien Destercke
Heudiasyc, UMR 7253, Centre de recherche de royallieu, 60203 Compiegne, France
sebastien.destercke@hds.utc.fr
Abstract. In this paper, we study how multilabel predictions can be obtained
when our uncertainty is described by a convex set of probabilities. Such predic-
tions, typically consisting of a set of potentially optimal decisions, are hard to
make in large decision spaces such as the one considered in multilabel problems.
However, we show that when considering the Hamming loss, an approximate pre-
diction can be efﬁciently computed from label-wise information, as in the precise
case. We also perform some ﬁrst experiments showing the interest of performing
partial predictions in the multilabel case.
Keywords: Credal sets, multilabel, indeterminate classiﬁcation, k-nn, binary rel-
evance.
1
Introduction
The problem of multi-label classiﬁcation, which generalizes the traditional (single la-
bel) classiﬁcation setting by allowing multiple labels to belong simultaneously to an in-
stance, has recently attracted a lot of attention. Such a setting indeed appears in a lot of
cases: a ﬁlm can belong to multiple categories, a music can stir multiple emotions [12],
proteins can possess multiple functions [14], etc. In such problems, obtaining a com-
plete ground truth (sets of relevant labels) for the training data and making accurate
predictions is more complex than in traditional classiﬁcation, in which the aim is to
predict a single label.
In such a setting the appearance of incomplete observations, i.e., instances for which
we do not know whether some labels are relevant, is much more likely. For example,
a user may be able to tag a movie as a comedy and not as a science-ﬁction movie, but
may hesitate about whether it should be tagged as a drama. Other examples include
cases with high number of labels and where an expert cannot be expected to provide all
relevant ones. Such partial labels are commonly called weak labels [9] and are common
in problems such as image annotation [10] or protein function prediction [14].
Even when considering weak labels, all multilabel methods we are aware of still pro-
duce complete predictions as outputs. However, given the complexity of the predictions
to make and the likely presence of missing data, it may be sensible to look for means to
do cautious yet more trustful predictions. That is it may be interesting for the learner to
abstain to make a prediction about a label whose relevance is too uncertain, so that the
ﬁnal prediction is partial but more accurate. Such an approach can be seen as an exten-
sion of the reject option implemented in learning problems [1] or of the fact of making
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 496–505, 2014.
c⃝Springer International Publishing Switzerland 2014

Multilabel Prediction with Probability Sets: The Hamming Loss Case
497
partial predictions [4], and has been recently investigated for the related problem of
label ranking [3,2].
In this paper, we consider the problem of making partial predictions in the multilabel
setting using convex sets of probabilities, or credal sets [8], as our predictive model.
Indeed, making partial predictions is one central feature of approaches using credal
sets [4], and these approaches are also well-designed to cope with the problem of miss-
ing or incomplete data [15]. However, making partial predictions with credal sets in
large decision space such as the one considered in mutlilabel is usually difﬁcult.
In Section 3, we nevertheless demonstrate that when focusing on the Hamming loss,
obtaining approximate partial predictions can be done in a quite efﬁcient way by fo-
cusing on label-wise information. We then perform (Section 4) some experiments to
demonstrate the interest of making partial predictions in the multilabel setting. Sec-
tion 2 presents necessary background material.
2
Preliminary Material
In this section, we introduce the multilabel setting as well as basic notions needed to
deal with sets of probabilities.
2.1
Multilabel Problem Setting
The usual goal of classiﬁcation problems is to associate an instance x coming from an
instance space X to a single (preferred) label of the space Λ = {λ1,...,λm} of possible
classes. In a multilabel setting, an instance x is associated to a subset Lx ⊂Λ of labels,
often called the subset of relevant labels while its complement Λ \ Lx is considered
as irrelevant. We denote by Y = {0,1}m the set of m-dimensional binary vectors, and
identify a set L of relevant labels with a binary vector y = (y1,...,ym) ∈Y such that
yi = 1 if and only if λi ∈L. As there is a one-to-one mapping between subsets L of Λ
and Y , we will indifferently work with one or the other.
The task in a multilabel problem is the same as in usual classiﬁcation: to use the
training instances (x j,yj), j = 1,...,n to estimate the theoretical conditional probabil-
ity measure Px : 2Y →[0,1] associated to an instance x ∈X . Ideally, observed outputs
yj should be completely speciﬁed vectors , however it may be the case that the value
for some component yj
i is unknown, which will be denoted by yj
i = ∗. We will denote
incomplete vectors by capital Y. Alternatively, an incomplete vector Y can be charac-
terized by two sets L ⊆L ⊆Λ of necessarily and possible relevant labels, deﬁned as
L := {λi|yi = 1} and L := {λi|yi = 1 ∨yi = ∗} respectively. An incomplete vector Y de-
scribes a corresponding set of complete vectors, obtained by replacing each yi = ∗either
by 1 or 0, or equivalently by considering any subset L such that L ⊆L ⊆L. To simplify
notations, in the sequel we will use the same notation for an incomplete vector and its
associated set of complete vectors.
Example 1. Table 1 provides an example of a multilabel data set with Λ = {λ1,λ2,λ3}.
Y 3 = [∗1 0] is an incomplete observed instance with L3 = {λ2} and L3 = {λ1,λ2}. Its
corresponding set of complete vectors is {[0 1 0],[1 1 0]}

498
S. Destercke
Table 1. Multilabel data set example
X1
X2
X3
X4 y1 y2 y3
107.1 25 Blue 60 1
0
0
−50 10 Red
40 1
0
1
200.6 30 Blue 58 ∗
1
0
107.1 5 Green 33 0
1
∗
...
...
...
... ... ... ...
In multilabel problems the size of the prediction space increases exponentially with
m (|Y | = 32768 for m = 15), meaning that estimating directly Px will be intractable
even for limited sizes of Λ. As a means to solve this issue, different authors have
proposed so-called transformation techniques [13] that reduce the initial problem (in
which 2m parameters have to be estimated) into a set of simpler problems. For instance
Binary Relevance (BR) consists in predicting relevance label-wise, solving an indepen-
dent binary problem for each label. It therefore comes down to estimate m parameters
Px(yi), i = 1,...,m and to predict ˆyi = 1 if Px(yi = 1) ≥1/2. A common critic of the
BR approach is that it does not take account of label dependencies, however it has been
shown that this approach is theoretically optimal for the Hamming loss, on which this
paper focuses [5]. Other reduction approaches include, for instance, Calibrated Ranking
(CR) [7] that focuses on pairwise comparisons.
Another issue is that making a precise and accurate estimation of Px is an extremely
difﬁcult problem given the number 2m of alternatives and the possible presence of miss-
ing data. This problem is even more severe if little data are available, and this is why
making cautious inferences (i.e., partial predictions) using as model a (convex) set Px
of probability distributions may be interesting in the multilabel setting.
2.2
Notions about Probability Sets
We assume that our uncertainty is described by a convex set of probabilities Px deﬁned
over Y rather than by a precise probability measure Px. Such a set is usually deﬁned
either by a collection of linear constraints on the probability masses or by a set of
extreme probabilities. Given such a set, we can deﬁne for any event A ⊆Y the notions
of lower and upper probabilities Px(A) and Px(A), respectively deﬁned as
Px(A) =
inf
Px∈Px Px(A) and Px(A) = sup
Px∈Px
Px(A).
Lower and upper probabilities are dual, in the sense that P(A) = 1 −P(Ac). Similarly,
if we consider a real-valued bounded function f : Y →R, the lower and upper expec-
tations Ex(f) and Ex(f) are deﬁned as
Ex(f) =
inf
Px∈Px Ex(f) and Ex(f) = sup
Px∈Px
Ex(f),
where Ex(f) is the expectation of f w.r.t. P. Lower and upper expectations are also
dual, in the sense that E(f) = −E(−f). They are also scale and translation invariant in
the sense that given two numbers α ∈R+,β ∈R, we have E(α f + β) = αE(f)+ β.

Multilabel Prediction with Probability Sets: The Hamming Loss Case
499
In the next sections, we explore how the multilabel problem can be solved with such
credal estimates. We discuss the problem, usually computationally intensive, of making
partial decision and show that it can be simpliﬁed when considering the Hamming loss
as our loss function. Using these results, we then perform some experiment based on
label-wise decomposition and k-nn algorithms to assess the interest of making partial
predictions based on credal sets.
3
Credal Multilabel Predictions with Hamming Loss
In this section, we ﬁrst recall the principle of credal predictions, before proceeding to
show that in the case of Hamming loss, such predictions can be efﬁciently approximated
by an outer-approximation.
3.1
Credal Predictions
Once a space Y of possible observations is deﬁned, selecting a prediction, or equiva-
lently making a decision, requires to deﬁne:
– a space A = {a1,...,ad} of possible actions (sometimes equal to Y , but not nec-
essarily);
– a loss function ℓ: A × Y →R such that ℓ(a,y) is the loss associated to action a
when y is the ground-truth.
Given an instance x and a precise estimate ˆPx, a decision a will be preferred to a
decision a′ under loss function ℓ, denote a ≻ℓa′, if
Ex

ℓ(a′,·)−ℓ(a,·)

= ∑
y∈Y
ˆPx(y)

ℓ(a′,y)−ℓ(a,y)

> 0,
(1)
where Ex is the expectation w.r.t. ˆPx. This equation means that exchanging a′ for a
would incur a positive expected loss, therefore a should be preferred to a′. In the case
of a precise estimate ˆPx, ≻ℓis a complete pre-order and the prediction comes down to
take the maximal element of this pre-order, i.e.,
ˆaℓ= argmin
a∈A Ex (ℓ(a,·)) = argmin
a∈A ∑
y∈Y
ˆPx(y)ℓ(a,y)
(2)
that is to minimize the expected loss (ties can be broken arbitrarily, as they will lead
to the same expected loss). This means that ﬁnding the best action (or prediction) will
therefore requires d computations of expectations.
When considering a set Px as cautious estimate, there are many ways [11] to extend
Eq. (1), but the most well-founded is the maximality criterion, which states that a ≻ℓa′,
if
Ex

ℓ(a′,·)−ℓ(a,·)

> 0,
(3)
that is if exchanging a′ for a is guaranteed to give a positive expected loss. In such a
case, the relation ≻ℓwill be a partial order, and the maximal set ˆAℓof alternatives will
be chosen as a prediction, that is
ˆAℓ= {a ∈A | ̸ ∃a′ ∈A s.t. a′ ≻ℓa}.
(4)

500
S. Destercke
Computing ˆAℓrequires at worst d(d −1) computations, a quadratic number of com-
parisons with respect to the number of alternatives. Also notes that evaluating Eq. (3)
usually requires solving a linear programming problem, a computationally more inten-
sive task than evaluating Eq. (1). ˆAℓis a cautious prediction, since it considers a set of
potential optimal solutions.
Multilabel loss functions usually considers the set A = Y as possible actions, or
even bigger sets (for example the ranking loss considers as actions the set of complete
orders over Λ). This means that getting ˆaℓis already quite hard in the general case,
hence computing ˆAℓwill be intractable in most cases, as the worst number of computa-
tion will then be 22m (m = 15 labels means at worst ∼109 comparisons).
In the next subsection, we show that for the Hamming loss ℓH, we can get an outer
approximation of ˆAℓat an affordable computational cost. Offering such efﬁcient way
to make cautious predictions based on Px is essential to be able to use such kind of
models in complex problems.
3.2
The Hamming Loss
Let the set of alternatives be A = Y . Given an observation y and a prediction ˆy, Ham-
ming loss ℓH reads
ℓH(ˆy,y) = 1
m
m
∑
i=1
1(ˆyi̸=yi) .
(5)
It counts the number of labels for which our prediction is wrong, and normalizes it.
When the estimate ˆPx is precise, it is known [5] that the optimal decision is the vector
ˆy such that ˆyj = 1 if ˆPx(yj = 1) ≥1/2 and ˆyj = 0 else. In particular, this means that the
optimal decision can be derived from the sole knowledge of the marginals ˆPx(yj = 1),
j = 1,...,n, provided they are good estimates of Px.
Given a probability set Px, let ˆYℓH be the maximal set of vectors that would be
obtained using Eq. (4). The next proposition shows that ˆYℓH can be outer-approximated
using the marginals of the cautious estimate Px, in contrast with the precise case.
Proposition 1. Let Px be our estimate, then the imprecise vector ˆY ∗such that
ˆY ∗
j =
⎧
⎪
⎨
⎪
⎩
1
if P(yj = 1) > 1/2
0
if P(yj = 0) > 1/2
∗
else, i.e. P(yj = 1) ≤1/2 ≤P(yj = 1)
for j = 1,...,m
is an outer approximation of ˆYℓH, in the sense that ˆYℓH ⊆ˆY ∗.
Proof. Consider a given j ∈{1,...,m} and two alternatives ˆy and ˆy′ such that ˆyj = 1 ̸=
ˆy′
j and ˆyi = ˆy′
i for any i ̸= j. Then for any y such that yj = 1 we have
ℓH(ˆy′,y)−ℓH(ˆy,y) =

∑
k̸= j
1(ˆy′
k̸=yk) + 1(ˆy′
j̸=yj)

−

∑
k̸= j
1(ˆyk̸=yk) + 1(ˆyj̸=yj)

= 1(ˆy′
j=0) −1(ˆyj=0) = 1,

Multilabel Prediction with Probability Sets: The Hamming Loss Case
501
and for any y such that yj = 0 we have
ℓH(ˆy′,y)−ℓH(ˆy,y) =

∑
k̸= j
1(ˆy′
k̸=yk) + 1(ˆy′
j̸=yj)

−

∑
k̸= j
1(ˆyk̸=yk) + 1(ˆyj̸=yj)

= 1(ˆy′
j=1) −1(ˆyj=0) = −1.
We therefore have (ℓH(ˆy′,·)−ℓH(ˆy,·)+1)/2 = 1(yj=1) , hence
P(yj = 1) = E
ℓH(ˆy′,·)−ℓH(ˆy,·)+ 1
2
	
= 1
2E

ℓH(ˆy′,·)−ℓH(ˆy,·)

+ 1
2
the last equality coming from scale and translation invariance. Hence E(ℓH(ˆy′,·) −
ℓH(ˆy,·)) > 0 if and only if P(yj = 1) > 1/2. This means that, if P(yj = 1) > 1/2, any
vector ˆy′ with ˆy′ j = 0 is dominated (in the sense of Eq. (3)) by the vector ˆy where only
the j-th element is modiﬁed, hence no vector with ˆy′ j = 0 is in the maximal set ˆYℓH. The
proof showing that if P(yj = 0) > 1/2, then no vector with ˆy′ j = 1 is in the maximal set
is similar.
■
We now provide an example showing that the inclusion can be strict in general.
Example 2. Consider the 2 label case Λ = {λ1,λ2} with the following constraints:
0.4 ≤P(y1 = 1) = P({[1 0]})+ P({[1 1]}) ≤0.6
0.9(P({[1 0]})+ P({[1 1]})) = P({[1 0]})
0.84(P({[0 1]})+ P({[0 0]})) = P({[0 1]})
These constraints describe a convex set P, whose extreme points (obtained by sat-
urating the ﬁrst inequality one way or another) are summarized in Table 2. The ﬁrst
constraints induces that P(y1 = 1) = 0.4 and P(y1 = 0) = 0.6, while the bounds P(y2 =
1) = 0.396,P(y2 = 1) = 0.544, are reached by the extreme distributions P([1 1]) =
0.06, P([0 1]) = 0.336 and P([1 1]) = 0.04, P([0 1]) = 0.504, respectively. Given these
bounds, we have that ˆY ∗= [∗∗] corresponds to the complete space Y (i.e., the empty
prediction). Yet we have that
E(ℓH([1 1],·)−ℓH([0 0],·)) = 0.0008 ≥0
also obtained with the distribution P([1 1]) = 0.06, P([0 0]) = 0.064. This means that
the vector [0 0] is not in the maximal set ˆYℓH, while it is included in ˆY ∗.
Proposition 1 shows that we can rely on marginal information to provide an outer-
approximation of ˆYℓH that is efﬁcient to compute, as it requires to compute 2m values,
which are to be compared to the 22m usually required to assess ˆYℓH. It also indicates
that extensions of the binary relevance approach are well adapted to provide partial
predictions from credal sets when considering the Hamming loss, and that in this case
global models integrating label dependencies are not necessary, thus saving a lot of
heavy computations.

502
S. Destercke
Table 2. Extreme points of P of Example 2
P({[0 0]}) P({[1 0]}) P({[0 1]}) P({[1 1]})
0.096
0.36
0.504
0.04
0.064
0.54
0.336
0.06
4
First Experimentations
In this section, we provide ﬁrst experimentations illustrating the effect of making partial
predictions with a decreasing amount of information. These experiments illustrate that
such partial predictions may indeed improve the correctness of predictions.
4.1
Evaluation
Usual loss functions such as Eq. (5) are based on complete predictions. When mak-
ing partial predictions, such loss functions need to be adapted. This can be done, for
instance, by decomposing it into two components [3], one measuring the accuracy or
correctness of the made prediction, the other measuring its completeness.
If the partial prediction is an incomplete vector such as ˆY ∗, then Hamming loss can
be easily split into these two components. Given the prediction ˆY ∗characterized by
subsets L,L, let us denote Q = Λ \ (L ∩L) the set of predicted labels (i.e., labels such
that ˆY ∗
j = 1 or ˆY ∗
j = 0). Then, if the observed set is y, we deﬁne correctness (CR) and
completeness (CP) as
CR(ˆY ∗,y) = 1
|Q| ∑
λi∈Q
1(ˆyi̸=yi) ;
(6)
CP(ˆY ∗,y) = |Q|
m .
(7)
when predicting complete vectors, then CP = 1 and CR equals the Hamming loss (5).
When predicting the empty vector, then CP = 0 and by convention CR = 1.
4.2
Method
The method we used was to apply, label-wise, the k-nn method using lower probabilities
introduced in [6] (in which details can be found). This means that from an initial training
data set D, m data sets Dj corresponding to binary classiﬁcation problems are built, this
decomposition being illustrated in Figure 1. Given an instance x, the result of the k-nn
method on data set Dj provides an estimate of [P(yj = 1),P(yj = 1)] and by duality an
estimate of P(yj = 0) = 1 −P(yj = 1) and P(yj = 0) = 1 −P(yj = 1).
The method also automatically takes account of missing label information, and treat
such missing data in a conservative way, considering them as completely vacuous in-
formation (that is, we treat them as non-MAR variables [16]).

Multilabel Prediction with Probability Sets: The Hamming Loss Case
503
X1
X2
X3
X4 y1 y2 y3
107.1 25 Blue 60 1
0
0
−50 10 Red 40 1
0
1
200.6 30 Blue 58 ∗
1
0
...
... ... ... ... ... ...
data set D1
X1
... X4 y1
107.1
60 1
−50
40 1
200.6
58 ∗
...
... ...
data set D2
X1
... X4 y2
107.1
60 0
−50
40 0
200.6
58 1
...
... ...
data set D3
X1
... X4 y3
107.1
60 0
−50
40 1
200.6
58 0
...
... ...
Fig. 1. Label-wise decomposition of data set D
4.3
Results
In the experiments, the parameters of the k-nn algorithm were set to β = 0.75 and
ε0 = 0.99, so that results obtained when ﬁxing the number k of neighbors to 1 display
a sufﬁcient completeness. ε0 settles the initial imprecision, while β determines how
much imprecision increases with distance (details about the role of these parameters
can be found in [6]). We ran experiments on well-known multilabel data sets having
real-valued features. Their characteristics are summarized in Table 3. For each of them,
we ran a 10-fold cross validation with the number k of neighbors varying from 1 to 5,
and with various percentages of missing labels in the training data set (0%, 20% and
40%). Varying k in the algorithm allows us to control the completeness of the prediction:
the higher k is, the more imprecise become the estimations.
Table 3. Multilabel data sets summary
Name
# Features
# Labels
# Instances
emotion
72
6
593
scene
294
6
2407
yeast
103
14
2417
CAL500
68
174
502
The results of the experiment are displayed in Figure 2. From this ﬁgure, two main
conclusions can be drawn: on the used data sets, allowing for partial predictions (here,
by increasing the number k of neighbours) systematically improve the correctness, and
missing labels only inﬂuence the completeness of the predictions, not the correctness
of the results. This latter fact, however, may be due to the learning method. How fast
completeness decreases with the number of neighbors, however, clearly depends on the
data set.

504
S. Destercke
Emotions
Scene
yeast
CAL500
Fig. 2. Experimental results
5
Conclusions
Producing sets of optimal predictions in the multilabel setting when uncertainty is mod-
eled by convex probability sets is computationally hard. The main contribution of this
paper was to show that when using the Hamming loss, such sets can be easily outer-
approximated by focusing only on the marginal probability bounds of each label being
relevant. This makes both computation and learning issues easier, as one can focus on
estimating such marginals (instead of the whole joint model). We can consider that as
an important result, as it shows that imprecise probabilistic approaches are computa-
tionally affordable (at least under some conditions).
We also made some ﬁrst preliminary experiments indicating the interest of producing
such partial predictions, showing that making more cautious predictions lead to more
correct predictions. In the future, we intend to make similar studies for other well-
known loss functions, such as the ranking loss. We also intend to make further the
experiments, i.e., to compare this approach with other methods, or to empirically assess
(for small values of m) the quality of the made approximation.
Acknowledgements. Work carried out in the framework of the Labex MS2T, funded by
the French Government, through the National Agency for Research (Reference ANR-
11-IDEX-0004-02).

Multilabel Prediction with Probability Sets: The Hamming Loss Case
505
References
1. Bartlett, P., Wegkamp, M.: Classiﬁcation with a reject option using a hinge loss. The Journal
of Machine Learning Research 9, 1823–1840 (2008)
2. Cheng, W., H¨ullermeier, E., Waegeman, W., Welker, V.: Label ranking with partial abstention
based on thresholded probabilistic models. In: Advances in Neural Information Processing
Systems 25 (NIPS 2012), pp. 2510–2518 (2012)
3. Cheng, W., Rademaker, M., De Baets, B., H¨ullermeier, E.: Predicting partial orders: ranking
with abstention. In: Balc´azar, J.L., Bonchi, F., Gionis, A., Sebag, M. (eds.) ECML PKDD
2010, Part I. LNCS (LNAI), vol. 6321, pp. 215–230. Springer, Heidelberg (2010)
4. Corani, G., Antonucci, A., Zaffalon, M.: Bayesian networks with imprecise probabilities:
Theory and application to classiﬁcation. In: Holmes, D.E., Jain, L.C. (eds.) Data Min-
ing: Foundations and Intelligent Paradigms. ISRL, vol. 23, pp. 49–93. Springer, Heidelberg
(2012)
5. Dembczynski, K., Waegeman, W., Cheng, W., H¨ullermeier, E.: On label dependence and loss
minimization in multi-label classiﬁcation. Machine Learning 88(1-2), 5–45 (2012)
6. Destercke, S.: A k-nearest neighbours method based on imprecise probabilities. Soft Com-
put. 16(5), 833–844 (2012)
7. F¨urnkranz, J., H¨ullermeier, E., Loza Menc´ıa, E., Brinker, K.: Multilabel classiﬁcation via
calibrated label ranking. Machine Learning 73(2), 133–153 (2008)
8. Levi, I.: The Enterprise of Knowledge. MIT Press, London (1980)
9. Sun, Y.-Y., Zhang, Y., Zhou, Z.-H.: Multi-label learning with weak label. In: Twenty-Fourth
AAAI Conference on Artiﬁcial Intelligence (2010)
10. Tian, F., Shen, X.: Image annotation with weak labels. In: Wang, J., Xiong, H., Ishikawa,
Y., Xu, J., Zhou, J. (eds.) WAIM 2013. LNCS, vol. 7923, pp. 375–380. Springer, Heidelberg
(2013)
11. Troffaes, M.: Decision making under uncertainty using imprecise probabilities. Int. J. of
Approximate Reasoning 45, 17–29 (2007)
12. Trohidis, K., Tsoumakas, G., Kalliris, G., Vlahavas, I.P.: Multi-label classiﬁcation of music
into emotions. ISMIR 8, 325–330 (2008)
13. Tsoumakas, G., Katakis, I.: Multi-label classiﬁcation: An overview. International Journal of
Data Warehousing and Mining (IJDWM) 3(3), 1–13 (2007)
14. Yu, G., Domeniconi, C., Rangwala, H., Zhang, G.: Protein function prediction using depen-
dence maximization. In: Blockeel, H., Kersting, K., Nijssen, S., ˇZelezn´y, F. (eds.) ECML
PKDD 2013, Part I. LNCS (LNAI), vol. 8188, pp. 574–589. Springer, Heidelberg (2013)
15. Zaffalon, M.: Exact credal treatment of missing data. Journal of Statistical Planning and
Inference 105(1), 105–122 (2002)
16. Zaffalon, M., Miranda, E.: Conservative inference rule for uncertain reasoning under incom-
pleteness. Journal of Artiﬁcial Intelligence Research 34(2), 757 (2009)

Cooperative Multi-knowledge Learning Control
System for Obstacle Consideration
Syaﬁq Fauzi Kamarulzaman1,2 and Seiji Yasunobu2
1 Faculty of Computer Systems and Software Engineering,
Universiti Malaysia Pahang, Malaysia
2 Dept. of Intelligent Interaction Technologies, University of Tsukuba, Japan
syafiq@fz.iit.tsukuba.ac.jp,
yasunobu@iit.tsukuba.ac.jp
Abstract. A safe and reliable control operation can be diﬃcult due to
limitations in operator’s skills. A self-developing control system could
help assist or even replaces the operators in providing the required con-
trol operations. However, the self-developing control system is lack of
ﬂexibility in determining the necessary control option in multiple con-
ditions where a human operator usually prevails by experiences in op-
timizing priority. Here, a cooperative multi-knowledge learning control
system is proposed in providing ﬂexibility for determining priority in con-
trol options, within multiple conditions by considering the required self-
developing control knowledge in fulﬁlling these conditions. The results
show that the system was able to provide consideration in prioritizing
the use of the required control knowledge of the condition assigned.
Keywords: Multi-knowledge, Learning Control, Reinforcement Learn-
ing.
1
Introduction
Human operators are prone to be ineﬀcient during any control operation due to
the lack of skills in unfamiliar operation’s environment and parameters. Learn-
ing Control System provides a self-developing Control Knowledge that changes
according to interaction with unfamiliar environment, thus reduces the possible
risk related to skills ineﬀciency [2]. The Control Knowledge will be continu-
ously updated through the control operations that later provides instructions
for controlling the related machine to perform at a high eﬀcient manner. The
Learning Control System reduces the dependency on human command since the
Control Knowledge provides most of the required control instruction, learned
from successes in previous operations [1].
However applying Learning Control operation on various conditions of state
parameters is diﬀcult and slow since the Control Knowledge is needed to be
constructed for each condition. Here, issues concerning application of multiple
knowledge in learning are brought to establish a way of learning an optimum
action from multiple sources of individual Control Knowledge [7][9]. This study
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 506–515, 2014.
c
⃝Springer International Publishing Switzerland 2014

Cooperative Multi-knowledge Learning Control System
507
Acon, A
Plant
Environment
Learning Control 
System
Human
Policy and 
Merger
Learning Agents
Cooperave agent
Goal
Informaon
Obstacle 
Informaon
(As1, q(s, As1))
Command Suggeson 2 
with preference value
(As2, q(s, As2))
Command Suggeson 1 
with preference value
state
state
Knowledge for 
Avoiding Obstacles
Knowledge for 
Aaining Goal
Fig. 1. The structure of cooperative multi-knowledge learning system
is driven by the need to combine multiple Control Knowledge into a single con-
trol output while putting the necessity of individual Control Knowledge into
consideration. In order to combine multiple knowledge protocol, a mechanism
that involves cooperation of multiple Learning Control System for producing an
optimum action is presented.
In this research, the cooperation mechanism is developed by evaluating the
output preference value from two sets of Control Knowledge with speciﬁc input
parameters as shown in Fig. 1. Focusing on two diﬃerent types of states; termed
as Goal Distance and Obstacle Distance, separated learning process was con-
ducted to obtain a Cooperative Knowledge that operates to satisfy both state
condition. The ﬁrst set consists of a Control Knowledge that controls the oper-
ation upon achieving a goal while the second set consists of a Control Knowl-
edge that controls the operation upon avoiding detected obstacles. Each Control
Knowledge provides the preference value of each output depending on current
state. Later, a policy and merger agent evaluates the preference value from both
knowledge and produce a control output.
Applying both Control Knowledge on the system provides an eﬀcient, safe
and successful operation. The success is achieved, due to the application of Co-
operative Knowledge for learning and establishing a preference between two
Control Knowledge specializing in two diﬃerent state parameters at the same
time. Learning and application of multiple Control Knowledge in a control sys-
tem are simpliﬁed using the proposed method. Simulations were conducted to
evaluate the proposed control technique. The results prove that the system was
able to cooperate the learning process between the two Control Knowledge that
results in a successful control operation.
2
Cooperative Multi-knowledge Learning Control
Multi-knowledge Learning Control is deﬁned by having a control system that
learns multiple Control Knowledge. The knowledge will then be used to perform

508
S.F. Kamarulzaman and S. Yasunobu
a certain control operation. These Control Knowledge are speciﬁed, each for dif-
ferent task with diﬃerent concerned parameters. Usually, Control Knowledge is
applied by control operators consecutively and manually depending on the re-
quirement and conditions. However, the Cooperative Multi-knowledge Learning
Control proposed in this research applies cooperation between multiple Control
Knowledge that is developed using preference values stored in each Control Knowl-
edge. The preference values represents the importance of each Control Knowledge
in a given situation. These preference values are obtained from Learning Control,
where the Control Knowledge is constructed in a form of value function through
trial and error.
As an example, the Cooperative Multi-Knowledge Learning Control will be
able to conﬁgure around an obstacle and achieved the desired goal by using two
types of Control Knowledge; Control Knowledge for goal attainment and Control
Knowledge for obstacle avoidance. Using ”Achieving Goal” as main command,
the system uses ”Avoiding Obstacle” for creating a cooperative command, head-
ing to the goal while avoiding closing obstacles. Here, Learning Control is applied
twice in the protocol. Firstly, Learning Control is applied to construct two spe-
ciﬁc Control Knowledge. Then, the knowledge gained from the ﬁrst iteration is
combined and relearn for cooperative control operation.
2.1
Learning Control
Learning Control is a method of obtaining Control Knowledge by repeatedly
construct and correct the Control Knowledge depending on the outcome of a
control operation in several trials [1]. In this research, this method applies rein-
forcement learning where Control Knowledge is constructed in a form of value
functions Q. The value will then be updated depending on the reward r that is
received after performing a control operation trial [3].
The Control Knowledge is divided into state S, which deﬁnes the current
situation of the control object and action A, which deﬁnes the next move of
the control object. State S and Action A is divided into a set of numbers as
State S = s1, s2, , sn and Action A = a1, a2, , an. During the update phase, the
preference value q of the combination between state s and action a is deﬁned
through the reward obtained after performing action a. In the case successful
operation, the preference value q increases, should the action contribute to a fail
operation, the value decreases. A set of preference value q can be deﬁned as value
function Q, where all preference value for the combination between state S and
action A is recorded. The value function Q is deﬁned as Control Knowledge.
In this research, this Control Knowledge is updated based on Q-learning al-
gorithm,
Q(S, A) = (1 −ϕ)Q(S, A) + ϕ[r + ψQmax],
(1)
Qmax = max
A Q(S, A)
(2)
where ϕ is the learning rate and ψ is update value discount rate [3]. The algorithm
is applied in updating all the Control Knowledge constructed.

Cooperative Multi-knowledge Learning Control System
509
Two Control Knowledge were constructed to conﬁrm the eﬃectiveness of the
system through a simulation. The ﬁrst Control Knowledge is for Goal Attain-
ment Control, while another is for Obstacle Avoidance Control. For each Con-
trol Knowledge, various parameters for state S were used, without imposing any
changes to action A.
Goal Attainment Control. Goal Attainment Control consists Learning Con-
trol method for operating the control object towards the goal [5]. Here, the
Goal Attainment Control applies goal distance δG = {δXgoal, δYgoal} as state
S while control output u and rotation Λ as action AGA. Therefore, the value
function Q for Goal Attainment Control can be deﬁned as Q(δG, AGA).
The update equation for Goal Attainment Control alone is,
QGoal(δG, AGA) = (1 −ϕ)QGoal(δG, AGA) + ϕ[r + ψQmax],
(3)
Qmax = max
AGA QGoal(δG, AGA)
(4)
Goal
Inial 
Posion
Goal Aainment Command
dt
dt+1
r = dt - dt+1
dt+1
Fig. 2. Method for applying reward r to the Control Knowledge for goal attainment
Obstacle Avoidance Control. Obstacle Avoidance Control consists Learning
Control method for operating the control object away from obstacles. Here, the
Obstacle Avoidance Control applies obstacle distance δO = {δXobs, δYobs} as
state S while control output u and rotation Λ as action AOA. Therefore, the
value function Q for Obstacle Avoidance Control is deﬁned as Q(δO, AOA).
The update equation for Obstacle Avoidance Control alone is,
Qobs(δO, AOA) = (1 −ϕ)Qobs(δO, AOA) + ϕ[r + ψQmax],
(5)
Qmax = max
AOA Qobs(δO, AOA)
(6)

510
S.F. Kamarulzaman and S. Yasunobu
Obstacle
Inial 
Posion
Obstacle 
Detecon 
Range 
Obstacle Avoidance Command
if ΔOt+1>ΔOt  , r=1
if ΔOt+1=0             , r=0
Fig. 3. Method for applying reward r to the Control Knowledge for obstacle avoidance
Main Command
Obstacle
Goal
Inial 
Posion
Obstacle 
Detecon 
Range 
cooperave Command
Fig. 4. The control method of an object using cooperative multi-knowledge learning
control around obstacles
2.2
Cooperative Multi-knowledge Learning
Applying Learning Control for multiple Control Knowledge requires the sys-
tem to analyze the value functions of both knowledge prior to the execution of
the control output. The preference value of outputs supplied from each control
method at a moment of state are needed to determine which Control Knowledge
is more preferred at the current state. Here, update method for both Control
Knowledge is modiﬁed so that the preference value is limited between 0(bad)
and 1(Good) [2]. Therefore, the updated value discount rate ψ in the update
equation for each knowledge is applied as,
ψGoal = 1 −QGoal(δG, AGA)
(7)

Cooperative Multi-knowledge Learning Control System
511
qt(ΔGt, AGA)
AGA
qt
A
qt(ΔOt, AOA)
AOA
At=A
1
0
1
0
Learned Knowledge for
Goal Aainment 
Learned Knowledge for
Obstacle Avoidance
Merger Agent
A
r
If  At=
If  At=
Q(ΔG, AGA)
Q(ΔO, AOA)
ΔG
ΔO
t
t
Fig. 5. The application and update process for all Control Knowledge
and
ψobs = 1 −Qobs(δO, AOA),
(8)
so that the preference value will be restricted between 0 and 1.
Fig. 5 described the method of cooperating both Control Knowledge in con-
structing an output. The preference values of a set of output A from both Control
Knowledge are used to construct a new set of preference value for output A with
the identity of the source knowledge attached; termed as Merger Output. Merger
Output is constructed by selecting the minimum preference value among the two
Control Knowledge for each element in the set of output A. The control output is
determined from Merger Output by a greedy policy where the action a with the
maximum preference value among the options in the set is chosen as the output
at current state. The result after the output been operated will determined the
rewards. Rewards will be given to the Control Knowledge based on the identity
of the source knowledge in Merger output that supplies the preference value of
the executed output.
The system structure involving cooperative multi-knowledge learning control
is later applied in a series of simulation as validation of its eﬃectiveness in pro-
ducing an eﬀcient, safe and reliable operation.

512
S.F. Kamarulzaman and S. Yasunobu
State
State
Reward for GA
Reward for OA
Goal Coordinator
Obstacle Coordinator
Plant
Control Knowledge for 
Goal Aainment
Control Knowledge for 
Obstacle Avoidance
Reward Funcon 
for Obstacle 
Avoidance
Reward 
Funcon for 
Goal Aainment
Reinforcement 
Learning for GA
Reinforcement 
Learning for OA
Agent for Goal Aainment
Agent for Obstacle Avoidance
Merger Agent
Goal 
Informaon
(human)
Obstacle 
informaon 
(environment)
Output 
A(u, Θ)
Command for 
GA & Preference 
value, (AGA,q)
Command for OA 
& Preference 
value, (AOA,q)
Output 
informaon
Output 
informaon
Policy
Control System
Fig. 6. The system structure for simulation experiment
3
Simulation Experiment
Simulation regarding the proposed method was constructed based on the system
structure shown in Fig. 6. The simulation was conducted based on a small robot
with the parameters shown in table 1 and Fig. 7b. The simulation was operated
in Matlab Simulink and the result was based on operation with and without
obstacles in a pre-constructed map.
The map was constructed as Fig. 7a, where 4 diﬃerent goals were prepared before
the simulation was done. The simulated operation results were divided into two
sections for easy comparison between an operation with and without obstacles.
The simulation starts by constructing the Control Knowledge of goal attain-
ment and obstacle avoidance separately using the Learning Control. Learning
Control was done in 350 episodes in separate simulation. The simulation was
continued with cooperative simulation where the constructed knowledge was in-
tegrated in the simulation. Here, the Cooperative Learning Control was done for
100 trials and the results were taken after.

Cooperative Multi-knowledge Learning Control System
513
Table 1. Speciﬁcations of the simulated control object
Parameters
Value
Weight
0.42 [kg]
Size:
Length
0.53 [m]
Width
0.52 [m]
Height
0.1 [m]
Turning Radius
−1 < θ < 1 [rad]
Torque Force
−10 < V < 10 [volt]
Table 2. Q-learning parameters
Parameters
Range
Intervals
State (Goal)
Goal Distance,ΔG[m]
−10 < ΔG(x, y) < 10
2
State (Obstacle)
Obstacle
Distance,ΔO[m]
−2 < ΔO(x, y) < 2
0.5
Action
Target Angle, θ[rad]
−1 < θ < 1
0.5
Torque Force, V [volt]
−10 < V < 10
2
Learning rate, α 0.5 Discount rate, γ 0.3
-10
-8
-6
-4
-2
0
2
4
6
8
10
-10
-8
-6
-4
-2
0
2
4
6
8
10
X-Axis
Y- Axis
Goal 4
Obstacles
Goal 3
Goal 2
Goal 1
Start
(a) Field map for simulation
x
y
Obstacle Sensor Range
Maximum turning angle
Maximum turning angle
1 [rad]
-1 [rad]
0[rad]
2[m]
(b) Robot speciﬁcations
Fig. 7. Simulation setup for ﬁeld and robot
4
Experiment Results
Here, the results for control operation without obstacles and control operation
with obstacles are presented. Control operation without obstacles was done to
conﬁrm the eﬃectiveness of the Learning Control process in constructing the most

514
S.F. Kamarulzaman and S. Yasunobu
-10
-8
-6
-4
-2
0
2
4
6
8
10
-10
-8
-6
-4
-2
0
2
4
6
8
10
Y- Axis
X-Axis
Goal 1
Goal 2
Goal 3
Goal 4
(a) without obstacle
-10
-8
-6
-4
-2
0
2
4
6
8
10
-10
-8
-6
-4
-2
0
2
4
6
8
10
X-Axis
Y- Axis
Goal 4
Obstacles
Goal 3
Goal 2
Goal 1
(b) with obstacle
Fig. 8. Results of Control Operation Simulation
eﬃective Control Knowledge for the system. Control operation with obstacles was
done to conﬁrm the eﬃectiveness of the whole system in utilizing the cooperative
multi-knowledge learning control by using two Control Knowledge constructed
prior to the simulation operation.
4.1
Control Operation without Obstacles
Fig. 8a shows the results of control operation without any obstacles. Here, The
system was needed to achieve the goal through Learning Control. Four goals
were assigned prior to the simulation. The simulation shows that the system
was able to learn and operate the control object to achieve the designated goals.
4.2
Control Operation with Obstacles
Fig. 8b shows the results from the simulation of control operation with obstacles.
In this simulation, obstacles were assigned prior to the simulation with four goals
assigned to be achieved. The robot was set to reach the assigned goals in the
domain. The results show that the system was able to operate the control object
towards the designated goals while avoiding existing obstacles.
5
Conclusion
This study presents a cooperative multi-knowledge learning control to overcome
operations with obstacles. The proposed method applies learning control for mul-
tiple Control Knowledge at the same time and cooperatively shares the Control
Knowledge by referring to the preference value sustained by each Control Knowl-
edge. Control Knowledge is continuously updated through the Learning Control

Cooperative Multi-knowledge Learning Control System
515
process. Rewards are given to the knowledge that provides the preference value
of the operated control action.
The proposed method was applied in a control system for simulation of a
small robot in a virtually constructed ﬁeld map. The control system simulation
was applied in a ﬁeld, both with and without the obstacles. Results show that
the system is able to utilize both Control Knowledge in performing a control
operation with obstacle consideration. Therefore, Obstacle Consideration was
achieved by calibrating two Control Knowledge in creating a safer cooperative
command during the control operation.
Acknowledgement. This research was supported by JSPS KAKENHI Grant
Number 24500272.
References
1. Schaal, S., Atkeson, C.G.: Learning Control in Robotics; Trajectory-Based Optimal
Control Techniques. IEEE Robotics and Automation Magazine 7(2), 20–29 (2010)
2. Matsubara, T., Yasunobu, S.: An Intelligent Control Based on Fuzzy Target and
Its Application to Car Like Vehicle. In: SICE Annual Conference (2004)
3. Xu, X., Zuo, L., Huang, Z.: Reinforcement learning algorithms with function
approximation: Recent advances and applications. Journal of Information Sci-
ences 261, 1–31 (2014)
4. Nakamura, Y., Ohnishi, S., Ohkura, K., Ueda, K.: Instance-Based Reinforcement
Learning for Robot Path Finding in Continuous Space. In: IEEE SMC, pp. 1229–
1234 (1997)
5. Kamarulzaman, S.F., Shibuya, T., Yasunobu, S.: A Learning-based Control Sys-
tem by Knowledge Acquisition within Constrained Environment. In: IFSA World
Congress, vol. FC-104, pp. 1–6 (2011)
6. Chang, D., Meng, J.E.: Real-Time Dynamic Fuzzy Q-Learning and Control of
Mobile Robots. In: 5th Asian Control Conference, vol. 3, pp. 1568–1576 (2004)
7. Busoniu, L., Babuska, R., Schutter, B.D.: A Comprehensive Survey of Multiagent
Reinforcement Learning. IEEE Trans. Systems, Man and Cybernetics 38(2), 156–
172 (2008)
8. Gullapalli, V.: Direct Associative Reinforcement Learning Methods for Dynamic
Systems Control. Neurocomputing 9, 271–292 (1995)
9. Sun, R., Sessions, C.: Multi-agent reinforcement learning with bidding for auto-
matic segmentation of action sequences. In: 4th International Conference on Multi
Agent Systems, pp. 445–446 (2000)
10. Yu, J.: An Adaptive Gain Parameters Algorithm for Path Planning Based on Re-
inforcement Learning. In: 4th International Conference on Machine Learning and
Cybernetics, pp. 3557–3562 (2005)

Building Hybrid Fuzzy Classiﬁer Trees
by Additive/Subtractive Composition of Sets
Arne-Jens Hempel1, Holger H¨ahnel1, and Gernot Herbst2
1 Technische Universit¨at Chemnitz, Chemnitz, Germany
2 Siemens AG, Chemnitz, Germany
Abstract. Especially for one-class classiﬁcation problems, an accurate
model of the class is necessary. Since the shape of a class might be arbi-
trarily complex, it is hard to choose an approach that is generic enough to
cope with the variety of shapes, while delivering an interpretable model
that remains as simple as possible and thus applicable in practice. In this
article, this problem is tackled by combining convex building blocks both
additively and subtractively in a tree-like structure. The convex build-
ing blocks are represented by multivariate membership functions that
aggregate the respective parts of the learning data. During the learning
process, proven methods from support vector machines and cluster anal-
ysis are employed in order to optimally ﬁnd the structure of the tree.
Several academic examples demonstrate the viability of the approach.
1
Introduction
Besides traditional classiﬁcation techniques that try to distinguish between two
or more classes, the demand for one-class classiﬁcation methods has recently
been growing [1]. In real world applications, this relevance of unary classiﬁca-
tion can be caused by two facts. First, for example, when modeling the class
of “interesting products” for the customer of an online store, the learning data
include no counterexamples, by which a “non-interesting” class could be deter-
mined. Secondly, the structure of counterexamples could be too extensive to be
modeled appropriately, which holds e. g. for the decision of an airbag deployment
in a vehicle. One approach here would be to decide on the basis of an accurately
described “accident” class in a one-class classiﬁcation task. This might be more
apposite than discriminating between the “accident” class and a “non-accident”
class, which may have a highly intricate structure making it diﬀcult to model.
Naturally, a classiﬁcation approach based on (crisp or fuzzy) sets that model
a phenomenon has a “local” character and is thus especially suitable for one-
class problems. This holds in contrast to discriminatory methods like linear dis-
criminant analysis or (standard) support vector machines (SVMs), which, by
construction, allow “global” decisions beneﬁcial for two- or multi-class tasks.
The amenities of a fuzzy classiﬁcation approach lie in its ability to cope with
practically occuring problems such as noisy data, transitional eﬃects, drifting
and evolving classes, or overlaps in the case of two- and multi-class problems
[2,3]. The high interpretability of fuzzy classiﬁcation has often been stressed.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 516–525, 2014.
c
⃝Springer International Publishing Switzerland 2014

Building Hybrid Fuzzy Classiﬁer Trees by Additive/Subtractive Composition
517
It provides the opportunity to incorporate expert knowledge into the classiﬁer
[4]. Picking up the above example of an airbag deployment, interpretability might
be very important when evaluating data of a car accident by an assessor.
The setup of an accurate and at the same time simple and applicable model
can be challenging if the shape of the class that is to be described is non-convex
or even arbitrarily complex. Such phenomena can be found in applications such
as banknote authentication and machine diagnosis [5,6]. The root idea addressing
this problem has been given by [7,8] and others. It revolves around the additive
combination of convex fuzzy sets, such as fuzzy partitions. Besides that, a sub-
tractive composition using so-called complementary classes has been proposed
[6]. We recommend a combination of these two approaches resulting in a tree-like
model, which we refer to as hybrid fuzzy classiﬁer tree (FCT).
(a)
C(1)
1
C(1)
2
C(2)
2
F
(b)
μF
F
x
x
μC(1)
1
C(1)
1
x
μC(1)
2
C(1)
2
μ
OR
x C(2)
2
AND
μC(2)
2
AND
(c)
Fig. 1. Additive/subtractive classiﬁer tree (c) built from the least possible number of
convex sets (b) for two-dimensional data forming a “two-hole” shape (a)
In Fig. 1, an example leading to such a classiﬁer tree is sketched. Aim of this
article is to provide an algorithm which, when given a set of learning data as in
Fig. 1a, builds up a tree as in Fig. 1c. As leaf nodes, it employs multidimensional
membership functions representing convex basic building blocks similar to the
sets given in Fig. 1b. This can be understood as a geometrical viewpoint for
setting up the classiﬁer. In contrast, the similar approach of fuzzy pattern trees
[9] provides a rather logical description by using one-dimensional fuzzy terms as
leaf nodes which form partitions of the corresponding attribute’s domains.
2
Towards Hybrid Fuzzy Classiﬁer Trees
Our aim is to build an FCT from a given set {x1, . . . , xN} ⊂RM of N learning
objects, each with M features. In the unary case, the classiﬁer shall return one
truth value μ(x) ∈[0, 1], such that the classiﬁcation of a test datum x ∈RM
appearing in a region not supported by learning data results in a low degree
of membership indicating that x does not belong to the class. Moreover, the
procedure can be used for two- or multi-class problems, simply by independently
building a tree for each class and comparing the truth values.
Let us assume that we possess a (fuzzy) model and learning method for sets of
objects forming a convex shape in their feature space. How could we use them to
cope with non-convex data? A straightforward approach would try to break the
data set apart into convex subsets, e. g. using partitioning or segmentation, and

518
A.-J. Hempel, H. H¨ahnel, and G. Herbst
(a)
F2
F3
F4
F5
F6
F7
F8
F1
(b)
x
μF8
F8
OR
μ
μF1
F1
x
(c)
F
C(1)
(d)
μF
F
x
x
μC(1)
C(1)
μ
AND
(e)
Fig. 2. Building a classiﬁer tree for two-dimensional data forming a non-convex “one-
hole” shape (a). Additive approach: (b), (c) and subtractive approach: (d), (e).
subsequently learn convex models Fi for each of the subsets. The overall classiﬁer
for this non-convex class would then be built from a disjunctive combination (“F1
OR F2 OR . . . ”) of these convex building blocks, cf. Fig. 2a to 2c.
In contrast, the approach from [6] starts with a convex model F covering the
whole data set. Afterwards, it subtracts the convex part C(1) in order to ﬁt the
model to the non-convex shape (“F AND NOT C(1)”), cf. Fig. 2d and 2e. If
a non-convex part had to be subtracted, it could be modeled recursively by a
subtraction of convex elements (“C(1) AND NOT C(2) . . . ”). C(l) are called com-
plementary classes with l indicating the level of complementation (cf. Sect. 5).
For arbitrarily complex shapes, one can expect to achieve more compact mod-
els (with a smaller number of building blocks) by combining both approaches.
A classiﬁer tree which is built up using the subtractive “AND NOT” approach
as well as the additive “OR” is shown in Fig. 3. The subscripts in C(1)
1
and C(1)
2
enumerate the additively combined complementary classes of level 1.
(a)
C(1)
1
C(1)
2
F
(b)
μF
F
x
x
μC(1)
1
C(1)
1
x
μC(1)
2
C(1)
2
μ
AND
OR
(c)
Fig. 3. Hybrid additive/subtractive classiﬁer tree (c) for a “two-hole” shape (a)
In this paper, we start with one model F as a convex “hull” of the data and re-
move the parts not supported by learning data, referred to as object-unsupported
class space C. C itself may be built up additively and/or subtractively from con-
vex elements in a tree-like structure. In this way, one always gets a rough result
in one step, i. e. an instance outside of F can never belong to the considered
class. Subsequently, this statement is reﬁned using the convex elements of C.
There are various options for both the structure of the tree and the type of
membership functions (MFs) modelling the convex sets that are to be combined.

Building Hybrid Fuzzy Classiﬁer Trees by Additive/Subtractive Composition
519
For the latter, we use a speciﬁc parametric MF of potential type (Sect. 3). In
order to learn these models also for parts of the object-unsupported class space C,
we propose to ﬁll up C by an artiﬁcially generated set of so-called complementary
objects (Sect. 4). Finally, the algorithm for setting up a model for C (Sect. 5)
employs a combination of learning of MFs, cluster analysis (to ﬁnd subsets for
additive “OR” combinations), and the recursive “AND NOT” approach from [6].
3
Convex Building Blocks: Fuzzy Pattern Classes
Since we might need several (but as few as possible) convex building blocks to
create FCTs, they should be well formalized and possess a comprehensible aggre-
gation process when dealing with high-dimensional feature spaces. It needs to be
emphasized that, in principle, any convex fuzzy set could serve as a component
for building FCTs. Nevertheless, we propose the usage of a speciﬁc multivariate
parametric fuzzy set. Besides the above-named properties, it features additional
advantages, such as the treatment of asymmetric data distributions. We refer
to this set as fuzzy pattern class (FPC), a concept which was introduced by
Bocklisch as a generalisation of Aizerman’s potential function [10]. It has al-
ready been applied for the modeling of traﬀc ﬂows [11], medical diagnosis [12],
condition monitoring [6], and time series analysis [13]. The structure of the FPC
approach even suits embedded implementations in industrial applications [14].
Purpose of this section is to give a brief review of the FPC concept together
with its main properties. A comprehensive description can be found in [10].
3.1
Deﬁnition and Learning of Fuzzy Pattern Classes
An FPC is a multivariate parametric fuzzy set with the membership function
μ(x) =
a
1 + 1
M
M

i=1

1
bi,l/r
−1
	
·
9999
xi −ui
ci,l/r
9999
di,l/r .
(1)
It derives from the intersection of M univariate FPC basis functions of the same
type, each deﬁned by a set of well interpretable parameters.1 These include the
location parameter ui ∈R, left and right class borders ci,l/r ∈R+ with their
corresponding border memberships bi,l/r ∈[0, 1], and the speciﬁers for the class
fuzziness di,l/r ∈[2, ∞). The parameters’ impact on a univariate basis function
can be understood by means of Fig. 4a. The quantity a can serve as a weight
parameter for prioritising certain blocks of the classiﬁer tree while decreasing
the inﬂuence of others. Due to their interpretability, all parameters enable the
incorporation of expert information in terms of a knowledge-based system.
An FPC is optimally ﬁt to its supporting data by means of a translation
relative to the class representative u = (u1, . . . , uM)⊤∈RM and a rotation
1 The intersection leading to (1) is conducted by a compensatory Hamacher operator,
preserving the function concept, parameters, and properties of the basis function [10].

520
A.-J. Hempel, H. H¨ahnel, and G. Herbst
ui−ci,l
ui
ui+ci,r
0
bi,l
bi,r
1
x
μi(x)
di,r = 2
di,r = 6
di,r = 20
(a)
(b)
Fig. 4. 1-D (a) and 2-D (b) versions of the MF in (1) including rotation
described by the matrix T ∈RM×M in the form x′ = T (x −u). Accordingly,
the membership value for a rotated and translated FPC is given by μ(x) =
μ′(x′) = μ′(T (x −u)), where μ′ has the same structure as μ in (1), though with
ui = 0, i = 1, . . . , M. Two examples of two-dimensional FPCs are illustrated in
Fig. 4b along with their univariate basis functions for each dimension.
The determination of an FPC’s parameters on the basis of given learning
data is described in detail in [10]. In recent works, the parameterisation has
been further developed, notably with regard to its robustness [15].
3.2
Fuzzy Pattern Class Properties
If a class of N learning objects is modeled by only one FPC, which is deﬁned by
8M parameters, the approach will provide a data compression ratio of N
8 . The
ratio scales down linearly with a growing number B of building blocks in the
tree. In our approach, the aim of a small value of B is achieved not only by
the chosen tree structure, but also by the fact that FPCs already can be tuned
quite ﬂexibly to the properties of a data set compared to simpler choices of MFs.
4
Exploration of the Object-Unsupported Class Space
The convexity of an FPC makes it a proper description for classes with a convex
data-inherent structure. In contrast, a convex set F would obviously not provide
a tight description for a non-convex class like in Fig. 1. But how can we decide in
practice whether a convex model is suﬀciently accurate—only on the basis of a
data set? An approach that has been proven to solve this problem eﬀciently for
low- and high-dimensional data sets incorporates one-class SVMs [6]. This graph-
based exploration scheme distributes complementary objects uniformly alongside
the edges between “border objects”, which are found in an optimal manner using
SVMs. The algorithm assures that only edges within the object-unsupported
class space are deployed. It also limits the number of complementary objects by
N −1 and thereby sets an upper limit for further computational costs.

Building Hybrid Fuzzy Classiﬁer Trees by Additive/Subtractive Composition
521
The result of this procedure is depicted in Fig. 5 where it has been applied
to an academic example featuring a shape of learning data similar to Fig. 1.
Subsequently, complementary classes can be learned from the complementary
objects. Since we employ the membership function proposed in Sect. 3 again,
they are referred to as complementary fuzzy pattern classes (CFPCs).
−1.5
−0.5
0.5
1.5
−1.5
−0.5
0.5
1.5
feature 1
feature 2
(a)
−1.5
−0.5
0.5
1.5
−1.5
−0.5
0.5
1.5
feature 1
feature 2
(b)
Fig. 5. (a) Learning/border objects and edges (b) learning/complementary objects
One may argue that the use of a one-class SVM sets a limit for the accuracy of
the approach. Yet, we employ the SVM solely for generating complementary ob-
jects and return to the model set up based on (C)FPCs. Thus, the interpretability
of the fuzzy classiﬁer and its low complexity are retained (cf. Sect. 6). However,
the accuracy can be controlled e. g. by tuning the SVM’s kernel parameters.
5
Composing the Blocks and Building a Hybrid Tree
Given a set of learning data, we want to set up a shape-preserving model of a
class via a fuzzy classiﬁer tree where FPCs and CFPCs serve as basic building
blocks. As mentioned, we set up an encircling fuzzy pattern class F based upon
the given learning objects preliminarily (cf. Sect. 2 and Fig. 1). Obviously, F
has to be combined with the “remainder” C of the classiﬁer tree by a fuzzy-
logical AND NOT. However, the locating of the CFPCs as well as their suitable
interconnection for setting up C in form of a subtree is more intricate. The
algorithm consists of ﬁve steps, which are processed as depicted in Fig. 6.
Step 1. Exploration of the object-unsupported class space
The ﬁrst step is conducted by generating complementary objects according to
Sect. 4. The level of complementation is set to the initial value l = 1.
Step 2. Cluster analysis of complementary objects
In order to ascertain whether there are separate groups of complementary ob-
jects, i. e. distinguishable object-unsupported partitions, we apply a density-
based clustering scheme because of its property to ﬁnd clusters independent of
their underlying shape [16]. The clustering is governed by a distance parameter
γ, whose value follows from the distribution of complementary objects in step 1.

522
A.-J. Hempel, H. H¨ahnel, and G. Herbst
exploration of object-
unsupported
class
space,
set
level
of
complementation to 1
step 1
cluster analysis
step 2
for
each
rele-
vant cluster setup
a
CFPC
of
the
current level
step 3
step 4
step 5
increment
level
of
complementation,
set
up
a
fuzzy-logical
AND NOT in the tree
for each non-convex
complementary class
combine all comple-
mentary
classiﬁer
subtrees
additively
(fuzzy-logical OR)
verify each
complementary
class on
convexity
determination of
complementary
ob-
jects of the current
level
Fig. 6. Algorithm for setting up C as a subtree of complementary fuzzy pattern classes
Step 3. Aggregation of complementary objects
Relevant clusters of complementary objects are aggregated to complementary
classes, see Sect. 3.1. Clusters are considered to be relevant if their cardinality
exceeds the value r · N, where r ∈[0, 1] is a task-speciﬁc parameter. For the
example of Fig. 1, this results in the CFPCs C(1)
1
and C(1)
2 .
Step 4. Veriﬁcation of convexity for complementary classes
Due to the fact that the clusters of complementary objects may also be char-
acterized by non-convex shapes, their convex CFPC models may be inadequate
as well. For l = 1, the suitability of each CFPC is conﬁrmed by a classiﬁcation
of the learning objects xi applying (1), where μ = μCFPC. If this classiﬁcation
yields low memberships μCFPC(xi) for at least (1 −r) · N instances, the consid-
ered CFPC is assumed to be suitable. The description of the respective cluster is
completed and the CFPC is branched oﬃvia a fuzzy-logical OR. This applies to
C(1)
1
in Fig. 1. On the contrary, if the classiﬁcation of xi results in high degrees
of membership for at least r · N instances, the considered CFPC is expected to
be inadequate (as C(1)
2
in Fig. 1). The CFPC description itself has to be reﬁned
with one or more complementary classes of the next level, see step 5. Hence,
the level of complementation is incremented. The respective CFPC is branched
oﬃvia an OR and an additional AND NOT connective for its further reﬁning.
In Fig. 1, this corresponds to the combination “C(1)
2
AND NOT C(2)
2 ”. How-
ever, a fuzzy description of C(2)
2
in terms of (1) is not known at this step of the
algorithm.
Generally, for l > 1, the veriﬁcation of convexity is performed with the com-
plementary objects of the preceding level (as deﬁned in step 5).
Step 5. Selection of higher-level complementary objects
Each CFPC with an indication of non-convexity is reﬁned separately but in the
same manner, thus forming new branches in the next layer of the tree. Due to the
fact that learning and complementary objects are mutually complementing each
other, it follows that we can refrain from a further generation of complementary
objects. For l even, the set of complementary objects is given by those learning
objects with high degrees of membership to the currently reﬁned CFPC of level
l −1 (cf. step 4). For l odd, this set is represented by the complementary ob-
jects, generated in step 1, exhibiting high memberships to the associated CFPC.

Building Hybrid Fuzzy Classiﬁer Trees by Additive/Subtractive Composition
523
The selected complementary objects are fed back into the algorithm (step 2).
After the cluster analysis, they form complementary classes of level l (step 3).
Regarding the example from Fig. 1, the algorithm determines the description of
C(2)
2
and terminates with the veriﬁcation of its convexity.
After the tree setup, all blocks (FPCs) are weighted via their parameter a in
order to obtain normalized membership values μ(xi). For the calculation of the
memberships, we apply the max operator as OR combination and the algebraic
product with the natural negation for the fuzzy-logical AND NOT.
The process parameter r governs the convergence and the detailedness of the
emerging classiﬁer tree. The larger r the coarser the model (i. e. smaller B) and
the faster the learning converges. The value r · N represents the least number
of objects to form a building block. Thus, one can always choose r suﬀciently
large in order to achieve highly understandable and interpretable trees.2 In the
special case r = 1, the algorithm stops immediately resulting in the trivial tree
deﬁned upon F. Usually, r is set based on task-speciﬁc knowledge, e. g. in terms
of the desired model complexity. The complexity of the algorithm itself scales
quadratically with N and linearly with the dimensionality of the feature space.
6
Examples
We will now demonstrate the viability of the approach with the help of several
academic examples being set up in the spirit of the introductory examples from
Sect. 1 and Sect. 2. To this end, we will start with a simple distribution of learning
data in a two-dimensional feature space, forming a convex shape. Subsequently,
convex and non-convex parts of the shape are being taken out successively in
order to resemble the shapes of Fig. 2a, Fig. 3a, and Fig. 1a.
First aim of this section is to visually conﬁrm that the algorithm from Sect. 5
delivers a classiﬁer tree with a minimal number B of convex sets. That is to
say the resulting tree should exhibit the structure of Fig. 2c, 3c, and 1c for the
examples from Fig. 2a, 3a, and 1a, respectively. It should not have any branches
for the purely convex case. The second aim is to provide ﬁrst ﬁndings for a
comparison of the proposed tree with other one-class learners.
The learning data, the generated complementary objects, and the resulting
MFs are shown in Fig. 7. The process parameter r has been set to 0.01. Obviously,
the shape of each data distribution is captured very well. The resulting tree
structures are not shown here since they are equivalent to Fig. 2c, 3c, and 1c.
For the example of Fig. 7d, a comparison with the purely subtractive, an
additive (segmentation) tree approach, and a one-class SVM is given in Table 1.
Considering only the tree approaches, the hybrid FCT is optimal w. r. t. B.
Additionally, the model complexity (number of parameters) is almost one order
of magnitude smaller than for a one-class SVM. In this setup, the SVM has been
2 The understandability of the tree originates from a small number of blocks and their
hierarchical arrangement using fuzzy-logical connectives. It is further fostered by
the interpretation of a single block as a rule-based (sub)system (due to the used
intersection operator) and the semantical parameters of its MF (cf. Sect. 3.1).

524
A.-J. Hempel, H. H¨ahnel, and G. Herbst
(a) convex
(b) one hole
(c) two holes
(d) convex/non-convex holes
Fig. 7. Learning data (black), complementary objects (white) and resulting member-
ship function for examples with increasing complexity. The classiﬁer tree structure for
(b), (c) and (d) can be found in Fig. 2c, 3c and 1c, respectively.
Table 1. Comparison of hybrid fuzzy classiﬁer trees with other one-class approaches
method
hybrid tree subtractive tree additive tree one-class SVM
#convex sets (B) / #SVs
4
5
6
133
#parameters
64
80
96
400
run time
2.4s
3.1s
1.8s
0.6s
tuned in such a way that it produces a similar accuracy, which is measured by
the rejection rate of the complementary objects.
7
Conclusions and Outlook
In this article, we presented the idea of a model and method for one-class learn-
ing described by a hybrid FCT. The classiﬁer tree consists of additive and/or
subtractive combinations of convex fuzzy sets learned from a given set of data
with an arbitrarily complex shape. It could be demonstrated that the algorithm
is able to learn and parameterize FCTs with the same minimal number and com-
bination of convex sets as an expert would construct manually. Our future work
includes a further comparison of the hybrid FCT to other unary classiﬁcation
methods such as Bayesian approaches, artiﬁcial neural networks, and boundary

Building Hybrid Fuzzy Classiﬁer Trees by Additive/Subtractive Composition
525
methods. This also involves an application to benchmark and real world data
sets, e. g. from the ﬁeld of condition monitoring.
References
1. Zhuang, L., Dai, H.: Parameter Optimization of Kernel-based One-class Classiﬁer
on Imbalance Text Learning. In: Yang, Q., Webb, G. (eds.) PRICAI 2006. LNCS
(LNAI), vol. 4099, pp. 434–443. Springer, Heidelberg (2006)
2. Saez, J., Luengo, J., Herrera, F.: On the Suitability of Fuzzy Rule-based Classi-
ﬁcation Systems with Noisy Data. IEEE Transactions on Fuzzy Systems PP(99)
(2012)
3. Szmidt, E., Kukier, M.: Classiﬁcation of Imbalanced and Overlapping Classes Using
Intuitionistic Fuzzy Sets. In: 2006 3rd International IEEE Conference on Intelligent
Systems, pp. 722–727. IEEE Press, New York (2006)
4. Li, J.D., Zhang, X.J., Chen, Y.S.: Applying Expert Experience to Interpretable
Fuzzy Classiﬁcation System Using Genetic Algorithms. In: 4th International Con-
ference on Fuzzy Systems and Knowledge Discovery, vol. 2, pp. 129–133 (2007)
5. Hempel, A.-J., H¨ahnel, H., M¨onks, U., Lohweg, V.: SVM-integrated Fuzzy Pat-
tern Classiﬁcation for Nonconvex Data-inherent Structures Applied to Banknote
Authentication. In: Bildverarbeitung in der Automation. inIT, Lemgo (2012)
6. Hempel, A.-J., H¨ahnel, H., Herbst, G.: Learning Non-convex Fuzzy Classiﬁers Us-
ing Single-class SVMs. In: IEEE International Conference on Fuzzy Systems, pp.
1–8. IEEE Press, New York (2013)
7. Kosko, B.: Fuzzy Systems as Universal Approximators. IEEE Transactions on Com-
puters 43(11), 1329–1333 (1994)
8. Devillez, A.: Four Fuzzy Supervised Classiﬁcation Methods for Discriminating
Classes of Non-convex Shape. Fuzzy Sets and Systems 141(2), 219–240 (2004)
9. Senge, R., H¨ullermeier, E.: Top-down Induction of Fuzzy Pattern Trees. IEEE
Transactions on Fuzzy Systems 19(2), 241–252 (2011)
10. Hempel, A.-J., Bocklisch, S.F.: Fuzzy Pattern Modelling of Data Inherent Struc-
tures Based on Aggregation of Data with Heterogeneous Fuzziness. In: Rey, G.R.,
Muneta, L.M. (eds.) Modelling Simulation and Optimization, pp. 637–655. InTech
(2010)
11. P¨aßler, M., Bocklisch, S.F.: Fuzzy Time Series Analysis. In: Hampel, R., Wa-
genknecht, M., Chaker, N. (eds.) Fuzzy Control: Theory and Practice, pp. 331–345.
Physica-Verlag HD, Heidelberg (2000)
12. Schmidt, B., Bocklisch, S.F., P¨aßler, M., Czonsnyka, M., Schwarze, J.J., Klin-
gelh¨ofer, J.: Fuzzy Pattern Classiﬁcation of Hemodynamic Data Can Be Used to
Determine Noninvasive Intracranial Pressure. Acta Neurochirurgica (suppl. 95),
345–349 (2006)
13. Herbst, G., Bocklisch, S.F.: Recognition of Fuzzy Time Series Patterns Using Evolv-
ing Classiﬁcation Results. Evolving Systems 1(2), 97–110 (2010)
14. M¨onks, U., Petker, D., Lohweg, V.: Fuzzy-Pattern-Classiﬁer Training with Small
Data Sets. In: H¨ullermeier, E., Kruse, R., Hoﬀmann, F. (eds.) IPMU 2010, Part I.
CCIS, vol. 80, pp. 426–435. Springer, Heidelberg (2010)
15. H¨ahnel, H., Hempel, A.-J., M¨onks, U., Lohweg, V.: Integration of Statistical Anal-
yses for Parameterisation of the Fuzzy Pattern Classiﬁcation. In: 22. Workshop
Computational Intelligence, pp. 115–131. KIT, Karlsruhe (2012)
16. Ester, M., Kriegel, H.P., Sander, J., Xu, X.: A Density-based Algorithm for Discov-
ering Clusters in Large Spatial Databases with Noise. In: Proc. of 2nd International
Conference on Knowledge Discovery and Data Mining, pp. 226–231 (1996)

Applying CHC Models to Reasoning in Fictions
Luis A. Urtubey⋆and Alba Massolo
Facultad de Filosof´ıa y Humanidades, Universidad Nacional de C´ordoba,
Ciudad Universitaria, 5000 C´ordoba, Argentina
urtubey@ffyh.unc.edu.ar,
albamassolo@gmail.com
http://www.ffyh.unc.edu.ar
Abstract. In ﬁguring out the complete content of a ﬁctional story, all
kinds of consequences are drawn from the explicitly given material. It
may seem natural to assume a closure deductive principle for those con-
sequences. Notwithstanding, the classical closure principle has notorious
problems because of the possibility of inconsistencies. This paper aims to
explore an alternative approach to reasoning with the content of ﬁctional
works, based on the application of a mathematical model for conjectures,
hypotheses and consequences (abbr. CHCs), extensively developed dur-
ing the last years by Enric Trillas and some collaborators, with which
deduction in this setting becomes more comprehensive.
Keywords: Soft-Computing, CHC-Models, Reasoning, Fiction, Philos-
ophy.
1
Introduction
Issues concerning ﬁction has increasingly attracted attention of logicians, philoso-
phers and computer scientists during the last years. Particularly, several formal
systems have been proposed and applied in order to represent the way in which
a cognitive agent reasons about a work of ﬁction, [13], [14], [16], [15].
Talking about ﬁctions is often restricted to conversations about literature,
movies or TV-shows. Nevertheless, appealing to ﬁction in many areas of formal
and empirical sciences has been also very fruitful. A clear example of this is the
great interest in relating ﬁction with scientiﬁc models [11], [12]. Fiction has been
applied not only to explain how a scientist builds a model but also to determine
what kind of ontological entities models are. In this way, models have been un-
derstood as ﬁctional entities; and the work scientists do while modelling diﬃerent
phenomena has been compared to the work of authors who create ﬁction. This
relationship between models and ﬁction can also work in the opposite direction.
If that were the case, it would be possible to deﬁne ﬁction, in its turn, as a sort
of model.
During the last decade, Enric Trillas with some collaborators have worked out
in [3], and more recently in [2], [7], [8], [9], a mathematical model for conjectures,
⋆Corresponding author.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 526–535, 2014.
c
⃝Springer International Publishing Switzerland 2014

Applying CHC Models to Reasoning in Fictions
527
hypotheses and consequences (abbr. CHCs), in order to execute with this model
certain mathematical and informal reasoning. These interesting mathematical
models for CHCs have been established algebraically, and the statements and
propositions of human thinking are represented as those elements in an ortho-
complemented lattice. Additionally, several meaningful operators are deﬁned,
which act on each given set of premises, intuitively standing for the conjectures
and hypotheses as well as the consequences of that set of premises. The election
of Orthocomplemented lattices is justiﬁed because these are quite general alge-
braic structures, in order to establish a suﬀciently extensive reasoning model
in which CHCs can be mathematically described. Alternatively, other algebraic
settings have been also studied [4], [5].
This paper will be concerned with formal reasoning applied to ordinary expe-
rience with works of ﬁctions. It aims to explore an approach to reasoning with the
content of ﬁctional works, which especially deals with deduction in this setting,
based on the application of CHC-Models. The article is organized as follows.
First a short reference about current philosophical work on ﬁction is given. In
second place, recent work on CHC-Models is addressed and it is shown how it
can bear on reasoning in ﬁctions. Finally, solutions to some problems concern-
ing deduction in this setting are considered. The conclusion will point out some
further research.
2
Philosophy and Logic on Fiction
Problems related to ﬁction has raised several troubles for classical conceptions
in the ﬁeld of philosophy of language and logic. Already starting from the work
of Frege [17], the semantic role of ﬁctional names, i.e., names of characters,
creatures and places that belong to ﬁctional works, has been far from clear.
Moreover, the semantic value of ﬁctional sentences turned out to be controversial.
Notably, it is hard to establish whether a sentence like “Sherlock Holmes is a
detective” is true, false or truth-valueless, but it is also embarrassing to accept
that ﬁctional sentences are never true.
From the standpoint of philosophy and logic, inference in ﬁction involves rea-
soning with incomplete information. This is due to the fact that ﬁctional stories
describe their characters, places, and events only in an incomplete way. It is not
possible, for instance, to determine if Sherlock Holmes is 1.80 meters tall. Addi-
tionally, inference in ﬁction also involves reasoning with inconsistent information
that can emerge from two sources. On the one hand, information belonging to
a ﬁction contradicts reality in many aspects. For example, while according to
Doyle’s stories Sherlock Holmes used to live in London in 221B Baker Street, in
the real London, there was no Sherlock Holmes who used to live there. On the
other hand, some stories are based on a contradiction or contain inconsistent
information. For instance, this would be the case of a story where it is said that
a character x has and does not have certain property P. Specially, cases of this
last type will be addressed later in this paper.

528
L.A. Urtubey and A. Massolo
Consequently, it has turned out to be quite diﬀcult to provide a formal ac-
count of reasoning in ﬁction based on classical semantics. Firstly, as it was shown,
the standard approach to interpret classical languages, is objectual. According to
this interpretation, the domain of discourse assumes the existence of a non-empty
set of real objects. Therefore, sentences like “Sherlock Holmes is a detective” or
“Sherlock Holmes is a ﬁctional character”, in this classical formal setting, must
be evaluated as false. Secondly, the notion of logical consequence is deﬁned in
terms of necessary truth preservation. Clearly, it can be seen that this deﬁnition
forces to take bare truth as the only semantic value acceptable in a consequence
relation. In this sense, a conclusion would follow from the premises of a ﬁctional
story just in case that conclusion is as barely true as the premises. Admittedly,
sentences that contain ﬁctional names never hold in a classical interpretation.
Hence, it is not possible to give a compelling formal account of reasoning in
ﬁction inside classical semantics.
Anyway, it could be argued that it is possible to give a classical formal account
of reasoning in ﬁction conﬁned to propositional classical logic. In this way, it is
possible to avoid speaking about objects and try to deal with ﬁctional discourse
at a propositional level. However, problems arise also in a classical propositional
formal setting. On the one hand, classical propositional semantics is bivalent.
The principle of bivalence states that every sentence expressing a proposition
has exactly one truth value: true or false (one or zero). As a consequence, the
proposition “p or not p” equals one. Nonetheless, as ﬁctional works are essen-
tially incomplete, it is impossible for some sentences to establish whether they
are true or false. On the other hand, classical propositional logic obeys the prin-
ciple Ex falso quodlibet. According to this principle, anything follows from an
inconsistent set of premises. It turns out that the consequences of the propo-
sition “p and not p’’ equals L –the entire language. But a work of ﬁction can
contain contradictions. Hence, in a classical formal framework, dealing with in-
consistent information will overgenerate propositions derived from a story. And
even worse, any proposition could be drawn from a ﬁctional work. Thence, clas-
sical propositional logic does not provide an adequate formal system for dealing
with reasoning in ﬁction. The following sections will reaﬀrm this diagnosis on
the basis of the relationship between classical propositional calculus and Boolean
algebra.
3
Introducing CHC Models
CHCs Models have been conceived as mathematical tools for studying common-
sense, everyday or ordinary reasoning. Clearly, a model of this kind is not co-
incidental with the reality that is modelled by it, but a simpliﬁcation of the
reality. Anyway, these mathematical models bring a good mean of applying for-
mal deductive reasoning for trying to understand reality and to do more accurate
and clearer philosophical reﬂections on it. Moreover mathematical models can
be viewed as useful devices to construct new realities through computational
methods.

Applying CHC Models to Reasoning in Fictions
529
From the perspective of CHC–Models, common-sense reasoning can be decom-
posed in the pair consisting of ’conjecturing+refuting’, two terms that embed
two diﬃerent types of deducing. On the one hand there is a type of deducing,
which corresponds with the informal type of deduction that people carry out in
Common-sense reasoning. On the other hand, there is a more restrictive kind,
which has to do with the formal or mathematical concept of deductive conse-
quence. Admittedly this last one is the concept that Alfred Tarski [10] formulated
into the well-known deﬁnition of a consequence operator.
Thus, in common-sense reasoning, deduction is an informal and weaker con-
cept than in formal, mathematical reasoning. To better characterize what people
do with common-sense deduction, a moderate dose of formalization will be in-
troduced in the next section.
4
A Model for Reasoning in Fiction
Assuming that reasoning impose the existence of some previous information
about the subject under consideration, in any reasoning task there exist, from the
beginning, a body of information already available on the subject. This is usually
expressed through a ﬁnite number of statements or premises in natural language,
which also can include other type of expressions like numbers or functions. Thus,
one can assume to start with that the information given by a ﬁction F is somehow
stored under this form of representing knowledge. Certain other constraints will
be imposed later.
As it was just said, this paper mainly aims to apply CHC Models to the formal
treatment of reasoning in ﬁction, hence it is convenient to introduce ﬁrstly some
concepts concerning CHC-Models from previous work of Enric Trillas and his
collaborators. Specially paying attention to [7], [9] and to [8], which seem to be
better suited to the present work.
In the setting of CHC Models knowledge is represented in an adequate alge-
braic structure in a set L, (L, ⩽, · , +, ′), containing a pre-order ⩽representing
if/then, a unary operation ′ representing not, and two binary operations repre-
senting the linguistic and (·) and the linguistic or (+). Clearly the set of premises
of any type of reasoning cannot be trivially inconsistent. If they were, reason-
ing would be absurd or utter nonsense. Thus it makes sense to assume that the
set of premises must satisfy at least a somehow minimal requirement of consis-
tency. Let be P = {p1, . . . , pn} the subset of L with these statements, which are
taken as premises. It will be assumed that the element (not necessarily in P)
p∧= p1 · . . . · pn is not self-contradictory, i.e., p∧≰p
′
∧. Let F be such family of
subsets in L. Thus, this lack of self-contradiction means that P does not contain
absurd premises.
The following example –loosely adapted from [7]– illustrate these concepts
and it will help to clarify their application when reasoning with information
retrieved from a ﬁction. The knowledge representation part revolves around the
following statements, which are true about the novel “Farenheit 451” by Ray
Bradbury:

530
L.A. Urtubey and A. Massolo
Guy Montag is a ﬁreman
A ﬁreman burns books
Guy Montag loves books
Moreover a very clear fact also is: “Neither does Guy Montag love nor burn
books”.
Example 1. Let L be an ortholattice with the elements f for Guy Montag is a
ﬁreman, b for A ﬁreman burns books, and l for Guy Montag loves books, and its
corresponding negations, conjunctions and disjunctions. It is also the case that
Guy Montag is a ﬁreman, Guy Montag is a ﬁreman and does not burn books, and
neither does Guy Montag love nor burn books. Representing and, or and not as
stated before, the set of premises is P = {f , f ·b′, (l · b)′}. Thus the core-value of
this information can be identiﬁed with p∧= f ·f ·b′ ·(l · b)′ = f ·f ·b′ ·(b′ + l ′) =
f · b′.
Some reasoning can be done in the lattice. Notably some inferences can be
drawn, on the basis of the information of a ﬁction F, once conceded that a ⩽b
means that b is a logical consequence of a. All these inferences conform the
class of “conjectures” in the setting of CHC-Models. Among conjectures, conse-
quences, hypothesis and speculations are distinguished. However to keep things
as simple as possible, these classiﬁcation between diﬃerent types of conjectures,
can be put aside and an overall distinction can be made only between “conjec-
tures” and ”consequences” for the sake of convenience. Thus, from the example
it follows,
– Guy Montag does not burn books, b’, is a consequence of P, since p∧= f ·b′ ⩽
b′.
– The statement “Guy Montag is a ﬁreman and He does not burn books and
He loves books”, f · b′ · l and “He loves books”, l, are conjectures of P
– The statements “he burns books” and “he is not a ﬁreman” are refutations
of P, since f · b′ ⩽b′, and f · b′ ⩽f = (f ′)′
Understandably for representing reasoning in natural language a richer and more
complex framework would be highly desirable. Specially, aspects of tense and
other subtleties of common language are most diﬀcult to interpret in these more
rigid algebraic structures. However, within this limited and closed framework,
this example may still count as a formalization of a piece of human reasoning.
5
The Problem of Consistency
At ﬁrst glance, in the previous example, a contradiction looms over the conclu-
sions. On the one hand it holds on the story that “Guy Montag is a ﬁreman”
and, also that “a ﬁreman burns books”. On the other hand one knows that “Guy
Montag does not burn books”. Accordingly, it can be inferred then that “Guy
Montag is not a ﬁreman‘”. This is a very simple inference supported by the infer-
ence rule known as “Modus Tollens”, from a →b and b′, it follows a′, where →is

Applying CHC Models to Reasoning in Fictions
531
the material conditional, interpreted as a →b = a′ + b. Nevertheless, in spite of
its obviousness, this inference presupposes some particular structural features.
As argued in [1] “deduction” and “inference”, must be clearly distinguished when
formalizing reasoning. And one has to be aware also that the validity of formulas
belonging to the language, depends on the particular deductive system at stage.
In the example, for the inference to be valid, L must be endowed with the alge-
braic structure of a Boolean algebra, and the consequence operator should be the
greatest one for such a framework. The veriﬁcation of the inequality expressing
Modus Tollens , i.e., b′ · (a →b) = b′ · (a′ + b) = a′ · b′ ⩽a′ in ortholattices as
much as in De Morgan algebras, will cause the validity of the laws of Boolean
algebras. Conversely, if the consequence operator is changed, the validity of the
inference scheme can no longer be guaranteed. Arguably, the problem in this
case does not have to do with the formalization of a conditional proposition as
material conditional. Actually, there is no conditional statement to be formalized
at all. In spite of this, according to certain na¨ıve reading of the story, it is right
to think that “either Guy Montag is not a ﬁreman or he does burn books”. A
logic-minded person who sticks to classical logic, would feel that things could
not be otherwise. But they are, because in the story, Guy Montag manages to
do both, he remains a ﬁreman and refrains from burning books. Unquestionably,
the problem can be ascribed to the consequence operator, which is the greatest
one for a Boolean algebra, and contributes to validate mentally this inference
scheme.
Thus, it seems impossible to conﬁne oneself to the premises and to ensure an
overall consistency, while keeping this kind of propositional reasoning in the set-
ting of classical logic or Boolean algebras. These are the kind of problems, which
are frequent when reasoning in ﬁctions. The closure of classical deductive con-
sequence, together with the meaning attributed to classical connectives, implies
that some undesirable conclusions must be accepted in spite of contradicting ex-
plicit information. Hence, the type of consistency concerning reasoning in ﬁction
has to comply somehow with standards of inference and consequence other than
those of classical logic. Additionally, the example about Guy Montag, helps to
see that “consequences” are not the only type of deductions gained from the
premises. There are also “conjectures”, which are in fact a lot more useful.
The following deﬁnition from [7] speciﬁes what is meant by a conjecture,
relative to a given problem on which some information conveyed by a set P =
{p1, p2, . . . , pn} of n premises is known.
Deﬁnition 1. q is a conjecture from P, provided q is not incompatible with the
information on the given problem once it is conveyed throughout all pi in P.
The most important things in this deﬁnition are, on the one side, how to state
that P is consistent; and on the other side, how to interpret the requirement that
q is not incompatible with the information given by P.
Accordingly, to apply CHC-Models to reasoning in ﬁctions, an appropriate no-
tion of incompatibility will be needed in each case. This notion of incompatibility
should be conveniently dissociated from the closure property of classical deduction.

532
L.A. Urtubey and A. Massolo
In the development of CHC-Models during the last decade, the construction of
conjecture’s operators has relied persistently on standard consequences’ opera-
tors. Recently, in [1] it has been shown that to keep the most typical properties
of the concept of conjecture, it suﬀces to only consider operators that are ex-
tensive and monotonic, but without enjoying the closure property. It is worth to
get a glimpse at this. Given a non empty set of sentences L let F be a family of
subsets in L. Then a standard consequence operator, is a mapping C: F →F,
such that,
– P ⊂C (P), C is extensive
– If P ⊂Q, then C (P) ⊂C (Q), C is monotonic,
– C (C (P)) = C (P), or C2 = C, C is a closure.
for all P, Q in F. In addition, consistent operators of consequence verify
– If q ∈C (P), then q′ /∈C (P)
In [7] several consequence operators lacking in closure are distinguished. Es-
pecially there are three operators of consequence, which are signiﬁcant to the
purpose of formalizing reasoning in ﬁctions,
– C1 (P) = {q ∈L : r (P) · q′ = 0}
– C2 (P) = {q ∈L : p∧· q′ ⩽(p∧· q′)′}
– C3 (P) = {q ∈L : p∧⩽q} = C∧(P).
where r (P) refers to the core-value of the information gathered in the premises,
which could verify for example, r (P) ⩽p∧.
Concerning these consequence operators, for i = {1, 2} it is P ⊂Ci (P), and
if P ⊂Q, then Ci (P) ⊂Ci (Q). Nevertheless, Ci cannot be always applicable to
Ci (P) since it easily can be rCi (P) = 0, due to the lack of consistency of Ci.
Furthermore, C∧(P) is also a consistent operator of consequence.
Hence, the corresponding operator of conjectures Conj i does not come from
an operator of consequences, but only from an extensive and monotonic one, for
which the closure property has no sense, since Ci (P) cannot be taken as a body
of information in the sense of [7], i.e., guaranteed free from incompatibility.
Remark 1. To have C (P) ⊂Conj C (P), it suﬀces for C to be a consistent
operator of consequences. Hence, the consistency of C is what characterizes
the inclusion of C (P) into Conj C (P), that consequences are a special type of
conjectures. Thus, when the premises harbour inconsistencies, which cannot be
taken away, C1 and C2 seem to be the only alternatives to take. Apparently, it
is the case while reasoning in ﬁctional stories.
6
Designing the Appropriate Framework
The problem of knowledge representation is one of the most important aspects
of any formalization process. Notably, for representing ordinary reasoning, which

Applying CHC Models to Reasoning in Fictions
533
usually involves natural language, more ﬂexible constructions are needed. Con-
cerning algebraic structure, it turns out that algebras of fuzzy sets enjoy such a
ﬂexibility. In [7] and [6] an abstract deﬁnition of a Basic Flexible Algebra is given.
Latices with negations and, in particular, ortholatices and De Morgan algebras
are instances of BFAs. Also the standard algebras of fuzzy sets
#
[0, 1]X , T, S, N
$
are particular BFAs.
Previous to any speciﬁc application of CHC-Models, some questions must be
addressed concerning the representational framework. First of all, where do the
objects (‘represented’ statements) belong to. That is, which is L, such that P ⊂L
and q ∈L? Secondly, with which algebraic structure is endowed L?. And lastly,
how to state that P is consistent, and how to translate that q is not incompatible?
To assume that P is free of incompatible elements is to concede that there are
not elements pi, pj in P, such that pi ⩽p′
j, or pi · pj = 0. To avoid the odd case
pi · . . . · pn = 0 it is convinient to assume that r (P) ̸= 0.
To keep things simpler in a ﬁrst attempt, it will be convenient to pay attention
only to the cases in which the BFA is an ortholatice. As a matter of fact, a De
Morgan algebra oﬃers a basic suitable structure for reasoning with the content
of a ﬁction. Moreover, some clariﬁcation is in order concerning the formulation
of the key principles of Non-contradiction and Excluded Middle. For that goal,
the following distinction between the incompatibility concept of contradictory
and self-contradictory elements in a BFA may be introduced.
– Two elements a, b in a BFA are said to be contradictory with respect to the
negation ′ , if a ⩽b′.
– An element a in BFA is said to be self-contradictory with respect to the
negation ′ , if a ⩽a′.
In dealing with De Morgan algebras, these principles, formulated in the way
that is typical of standard modern logic (a · a′ = 0; a + a′ = 1), do not hold.
Nevertheless, with an alternative formulation, De Morgan algebras also verify
those principles, that is,
– NC: a · a′ ⩽(a · a′)′
– EM: (a + a′)′ ⩽

(a + a′)′′
7
More on Consequence Operators and Conjectures
Arguably, it is worth to submit reasoning in ﬁctions to the supposition that L is
endowed with an ortholatice structure L = (L, ·, +,′ , 0, 1). Among the alterna-
tives to express the non incompatibility between the premises and a conjecture
q, there are two, which seem to be adequate in the present case: r (P) · q ̸= 0
and r (P) · q ≰(r (P) · q)′. It means that either the proposition added to the
core is admissible or that the set formed this way is not auto-contradictory or
impossible. The applications will make sense as much as r (P) = p∧̸= 0.
Manifestly, it was readily seen that there is a close connection between con-
jectures and consequence operators. Moreover it has been also shown that there

534
L.A. Urtubey and A. Massolo
are conjecture operators Conji, which do not come from an operator of conse-
quence. Anyway, even in the case of ﬁctions, it seems that logical consequences
are always counted as a particular case of conjectures. Then, it is natural that
one wonders whether there is also a consequence operator C (P) ⊂Conji. No-
tably, it turns out that C∧is such an operator and it is C∧⊂Conji (P). Hence,
C∧turns out to be in this case the safer conjectures. The set of conjectures,
which no longer include C∧, is the set
Conji (P)−C∧(P) = {q ∈Conji (P) : q < p∧}∪{q ∈Conji (P) : qNCp∧} (1)
where NC stands for non order comparable. Each set in (1) that remains when
C∧is taken away, has more risky deductions obtained by reasoning from the
premises.
Furthermore, any operator of conjectures verify some of the following proper-
ties [7] ,
1. Conj(P) ̸= ∅
2. 0 /∈Conj(P)
3. There exist an operator C such that Conj(P) = {q ∈L : q′ /∈C (P)}
4. Conj is expansive: P ⊂Conj (P)
5. Conj is anti-monotonic: If P ⊂Q, then Conj (Q) ⊂Conj (P)
Concerning this intended application to reasoning in ﬁctions, all these features
of conjecture operators are attractive. Especially, anti-monotonicity is very ap-
pealing. As noted also in [7], ConjC is anti-monotonic if and only if C is mono-
tonic. That is, conjectures and consequences are particularly linked with respect
to monotony. When reasoning in ﬁctions, it is also important to observe that a
more encompassing set of premises can make showing up conjectures that one
could not see before. This is an admissible interpretation of anti-monotonicity
in this setting.
8
Conclusion
By using CHC-Models, deductions are treated diﬃerently in connection with
reasoning. In particular, several types of deduction are distinguished. Moreover,
by separating consequence and inference, a new perspective on the use of logic
for reasoning is gained. It is possible now to apply more complex algebraic set-
tings to account for diﬃerent types of deductions, which taken together give a
more promissory approach to the variety of human reasoning. Speciﬁcally, the
application sketched in this paper, helps to show how a complex type of rea-
soning concerning ﬁctional content, can be approached from the methodological
perspective of CHC-Models. Deductions are no longer submitted to a classical
closure principle. An alternative model, based on consequences and conjectures,
can control reasoning instead. The advantages of this approach deliver promising
results. Arguably, more complex information, such as inexact or fuzzy knowledge,
coming from ﬁctions, can be also represented. Even in this case, by changing the

Applying CHC Models to Reasoning in Fictions
535
algebraic setting, this type of content shall be also accommodated. Future work
on the subject can be also addressed to consistently combine suitable contextual
principles into CHCs models, in order to search for conjectures and consequences
thereby generated. These models can be achieved by mimicking simple ﬁctional
scenarios, on which subjects can perform speciﬁc inference tasks, whose results
are somehow circumscribed by determinate constraints.
Acknowledgments. We’d like to thank the two anonymous referees for their
helpful suggestions regarding this paper. First author owe special thanks to Pro-
fessor Trillas for having introduce him to his appealing work on CHC models.
The work on this paper has received partial ﬁnancial support of SeCyT (UNC).
References
1. Trillas, E., Garc´ıa–Honrado, I.: Hacia un Replanteamiento del C´alculo Proposi-
cional Cl´asico. ´Agora–Papeles de Filosof´ıa 32, 7–25 (2013) (in Spanish)
2. Trillas, E., Garc´ıa–Honrado, I., Pradera, A.: Consequences and conjectures in pre-
ordered sets. Information Sciences 180, 3573–3588 (2010)
3. Trillas, E., Cubillo, S., Casti˜neira, E.: On conjectures in orthocomplemented lat-
tices. Artiﬁcial Intelligence 117, 255–275 (2000)
4. Ying, M.S., Wang, H.: Lattice-theoretic models of conjectures, hypotheses and
consequences. Artiﬁcial Intelligence 139, 253–267 (2002)
5. Qiu, D.: A note on Trillas CHC models. Artiﬁcial Intelligence 171, 239–254 (2007)
6. Trillas, E.: A Model for Crisp Reasoning with Fuzzy Sets. International Journal of
Intelligent Systems 27, 859–872 (2012)
7. Garc´ıa-Honrado, I., Trillas, E.: On an Attempt to Formalize Guessing. In: Seising,
R., Sanz Gonz´alez, V. (eds.) Soft Computing in Humanities and Social Sciences.
STUDFUZZ, vol. 273, pp. 237–255. Springer, Heidelberg (2012)
8. Trillas, E.: Reasoning: in Black and White? In: Annual Meeting of the North Ameri-
can Fuzzy Information Processing Society (NAFIPS), pp. 1–4. IEEE, San Francisco
(2012)
9. Trillas, E., S´anchez, D.: Conjectures in De Morgan Algebras. In: Annual Meeting
of the North American Fuzzy Information Processing Society (NAFIPS), pp. 1–6.
IEEE, San Francisco (2012)
10. Tarski, A.: Logic, Semantics, Metamathematics. John Corcoran (ed). Hackett, In-
dianapolis (1983)
11. Frigg, R., Hunter, M. (eds.): Beyond Mimesis and Convention. Representation in
Art and Science. Springer, London (2010)
12. Woods, J. (ed.): Fiction and Models. New Essays. Philosophia Verlag, Munich
(2010)
13. Parsons, T.: Nonexistent Objects. Yale University Press, New Haven (1980)
14. Priest, G.: Towards Non-Being: The Logic and Metaphysics of Intentionality. Ox-
ford University Press, New York (2005)
15. Rapaport, W., Shapiro, S.: Fiction and Cognition: an Introduction. In: Ram,
A., Moorman, K. (eds.) Understanding Language Understanding: Computational
Models of Reading, MIT Press, Cambridge (1999)
16. Woods, J.: The Logic of Fiction. Studies in Logic, vol. 23. College Publications,
London (2009)
17. Frege, G.: Posthumous Writings. Basil Blackwell, Oxford (1979)

A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 536–545, 2014. 
© Springer International Publishing Switzerland 2014 
Probabilistic Solution of Zadeh’s Test Problems 
Boris Kovalerchuk  
Dept. of Computer Science, Central Washington University,  
400 E. University Way, Ellensburg, WA 98926, USA  
borisk@cwu.edu 
Abstract. Zadeh posed several Computing with Words (CWW) test problems 
such as:  “What is the probability that John is short?” These problems assume a 
given piece of information in the form of membership functions for linguistic 
terms including tall, short, young, middle-aged, and the probability density 
functions of age and height. This paper proposes a solution that interprets 
Zadeh’s solution for these problems as a solution in terms of probability spaces 
as defined in the probability theory. This paper also discusses methodological 
issues of relations between concepts of probability and fuzzy sets. 
1 
Introduction  
Zadeh’s test problems include:  “What is the probability that Mary is middle-aged?”, 
“What is the probability that Mary is young?”, “What is the probability that John is 
short?”, and “What is the probability that John is tall?” [Zadeh, 2012, Belyakov et al, 
2012]. 
These problems assume given information I in the form of membership 
functions µtall, µshort, µyoung, µmiddle-aged, and the probability density functions of age PA 
and height PH  
Below we propose a solution that interprets Zadeh’s solution [2011] for these 
problems as a solution in terms of probability spaces as defined in the probability 
theory.   
First, we focus on one of the problems: “What is the probability that John is tall?” 
The given information I for this problem is: a specified membership function µtall and 
the probability density function of Height(John), pH.  Zadeh’s solutions for other 
listed problems are similar.  
In this notation, Zadeh defines the probability that John is tall as:  
 
                                    
du
u
p
u
H
tall
R
)
(
)
(
μ

                                                   (1) 
 
where R is the real line, u belongs to R and u is the height value.  Formula (1) is 
called the probability measure of the fuzzy set tall [Zadeh, 1968] and is considered as 
a translation of the given linguistic information I into a mathematical language with 
precisiation of meaning as Zadeh calls it.  Similarly a formula for the probability that 
Mary is middle-aged is 

 
Probabilistic Solution of Zadeh’s Test Problems 
537 
du
u
p
u
A
aged
middle
R
)
(
)
(
−
μ
. 
 
Here pA(u) is a probability that age of Mary is u. 
The solution (1) has difficulties to be interpreted as a probabilistic solution. First, 
the probability space associated with probability measures of the fuzzy sets (1) is 
unclear. Second, it is unclear if a probability measure of the fuzzy set (1) is an 
additive probability measures as required to be a probability measure in the 
probability theory. Third, the robustness/invariance of formula (1) is not clear.  
The robustness/invariance issue is related to the need to clarify the measurement 
scale of membership function μtall. If μtall is not in an absolute scale then the 
equivalent membership function μ`tall that is produced within its scale will give a 
different value of (1). Note that the probability is measured in the absolute scale that 
has no such difficultly.  
To illustrate the first two difficulties consider two fuzzy sets: F = “Probably John is 
tall” and G = “Probably John is not tall”. What is the probability measure of the 
disjunction of these fuzzy sets, F or G?  It depends on the definition of μF_or_G(u). The 
probabilistic additive measure requires  
 
                                     
du
u
p
u
H
tall
not
or
tall
R
)
(
)
(
μ

=1                                      (2) 
 
The use of a common in fuzzy logic and possibility theory max operation [Zadeh, 
1968],  
                                      μF_or_G (u) = max(μF(u),μG(u))  
 
does not give us (2), because  
 
               max(μtall(u),μnot tall(u))  < 1 when μtall(u) < 1 and μnot tall(u) < 1.  
2 
Mathematical Models 
2.1 
Computing Probability P(John is Tall) 
Let µtall(Height(X)) be a membership function, where X is a person (e.g., John) and a 
probability density function pH(Height(John)) be in the height interval U=[u1,un], e.g., 
u1 = 160 cm and un = 200 cm.  
The probability space for PH contains a set of events ei=“Height of John is ui”: 
e1= “Height of John is u1”, e2=“Height of John is u2”,…,en=“Height of John is un” 
with  
෍݌ுሺ݁௜ሻൌ1
௡
௜ୀଵ
 
 

538 
B. Kovalerchuk 
Next we build a set of probability spaces for the concept “tall” having a 
membership function µtall(u), where u=Height(X). Each of these probability spaces 
consists of two events gi1, gi2, where  
     gi1 = “John is tall if John’s height is ui”, 
     gi2 = “John is not tall if John’s height is ui” 
 
with probabilities defined as  
        P(gi1 | ei) = µtall(ui),  P(gi2 | ei) = 1- µtall(ui) 
 
when values of µtall(ui) are given. This requires that μtall is defined in the absolute 
scale by using a frequency or other approaches.   
Consider a statement (event)  
gi1&ei = “(John is tall if John’s height is ui) & (height of John is ui)”.   
 
We are interested to find the probability of this event, P(gi1&ei), in the probability 
space with 2 events  (gi1&ei) and (gi2&ei), where  
 
gi2&ei  = “(John is not tall if John’s height is ui) & (John’s height is ui )”. 
 
In accordance with the conditional probability rules we have  
 
P(gi1&ei) = P(gi1 | ei)P(ei). 
 
Above P(ei) was denoted pH(ei). Also above we have got P(g1i|ei) = µtall(ui), therefore  
 
P(gi1&ei) = P(gi1 | ei)P(ei) = µtall(ui)pH(ei) 
 
Next we are interested in getting the probability that John is tall that will include 
all heights. We denote this event for short as “John is tall”. This means that we need 
to compute the probability for the disjunction (union) of all events {gi1&ei} that 
constitute “John is tall”:  
 
    John is tall ؝ ሺ g11&e1) ∨ ( g21&e2) ∨…∨ ( gn1&en) = ڀ
ሺ݃௜ଵ&݁௜ሻ
௡
௜ୀଵ
.                
 
 Similarly we define the event “John is not tall”:  
 
John is not tall ؝ ሺ g12&e1) ∨ ( g22&e2) ∨…∨ ( gn2&en) = ڀ
ሺ݃௜ଶ&݁௜ሻ
௡
௜ୀଵ
. 
 
These events are defined in the probability space that consists of all events  
 
{(gi1&ei)}, {(gi2&ei)}, i=1:n. 
 

 
Probabilistic Solution of Zadeh’s Test Problems 
539 
Accordingly,  
ܲሺJohn is tallሻൌܲሺሧሺ݃௜ଵ&݁௜ሻሻ
௡
௜ୀଵ
ൌ 
෍ܲሺ݃௜ଵ&݁௜ሻ
௡
  ௜ୀଵ
ൌ෍ܲுሺ݁௜ሻܲሺ݃௜ଵ|݁௜ሻ
௡
௜ୀଵ
ൌ෍ܲுሺ݁௜ሻߤ௧௔௟௟ሺݑ௜ሻ                            ሺ3ሻ
௡
௜ୀଵ
 
   
For the continuous set of heights (in the interval [u1,un])  the last formula is 
transformed to   
න
ߤ௧௔௟௟ܲுሺݑ௜ሻ݀ݑ
௨೙
௨భ
 
 
This is formula (1) that has been proposed by Zadeh [2011]. Thus, we have 
reinterpreted Zadeh’ formula in probabilistic terms. 
2.2 
Computing Probability P(John is Tall or Not Tall) 
Next we show P(John is tall or not tall) = 1  in accordance with (2) and property 
 
ߤ ௧௔௟௟ሺݑ௜ሻ൅ߤ௡௢௧ ௧௔௟௟ሺݑ௜ሻൌ1: 
 
ܲሺJohn is tall or John is not tallሻൌ  
ܲሺሧሺ݃௜ଵ&݁௜ሻሧሺ݃௜ଶ&݁௜ሻ
௡
௜ୀଵ
ሻ
௡
௜ୀଵ
ൌ෍݌ுሺ݁௜ሻߤ௧௔௟௟ሺݑ௜ሻ
௡
௜ୀଵ
൅෍݌ுሺ݁௜ሻߤ௡௢௧ ௧௔௟௟ሺݑ௜ሻ
௡
௜ୀଵ
ൌ 
 
෍݌ுሺ݁௜ሻሺߤ ௧௔௟௟ሺݑ௜ሻ൅ߤ௡௢௧ ௧௔௟௟ሺݑ௜ሻ
௡
௜ୀଵ
ሻൌ෍݌ுሺ݁௜ሻൌ1
௡
௜ୀଵ
 
2.3 
Computing Probability P(John is of Middle Height or Tall)    
Now we show how to compute ܲሺJohn is of middle height or John is tallሻ.  This 
requires computing ߤ ௠௜ௗௗ௟௘ ௢௥ ௧௔௟௟ሺݑ௜ሻ,  which in turn requires creating a probability 
space where it can be done. 
Let the elementary events in this space be:  
 
gi1 = “John is short if John’s height is ui”,   
gi2 = “John is of middle height if John’s height is ui”  
gi3 = “John is tall if John’s height is ui” 
Then the negation of gi1 should be equal to the disjunction of two other elementary 
events,  ¬gi1 = gi2 ∨ gi3, and similarly  ¬gi2 = gi1 ∨ gi3, ¬gi3 = gi1 ∨ gi2 
We also keep the same probability space as above for PH with a set of events  
ei=“Height of John is ui”: 

540 
B. Kovalerchuk 
 
e1= “Height of John is u1”,e2=”Height of John is u2”,…,en=“Height of John is un”  
 
with  
෍݌ுሺ݁௜ሻൌ1
௡
௜ୀଵ
 
Therefore,   
 
                          P(gi2 ∨ gi3) = P(¬gi1) = 1- P(gi1) = 1- μshort(ui).                    
 
This property requires for any ui that  
                               μshort(ui) + μmedium(ui) + μtall(ui) =1.       
              (4) 
This property is commonly satisfied by triangular membership functions as shown 
in Figure 1. We call this set of fuzzy sets (linguistic variable) an exact complete 
context space [Kovalerchuk, 1996, 2013; Kovalerchuk, Vityaev, 2000].  
 
Fig. 1. Example of exact complete set of fuzzy sets (linguistic variable) 
2.4 
Computing Probability P(gi1&ei) 
Similarly to the consideration above we are interested to find the probability of event, 
P(gi1&ei), in the probability space with 3 events  
(gi1&ei), (gi2&ei), (gi3&ei), 
where   
 
gi1&ei  = “(John is short if John’s height is ui) & (John’s height is ui )”, 
gi2&ei  = “(John is of medium height if John’s height is ui) & (John’s height is ui )”, 
gi3&ei  = “(John is tall if John’s height is ui) & (John’s height is ui )”, 
 
In accordance with the conditional probability rules we have P(gi1&ei)=P(gi1 | ei)P(ei). 
Above P(ei) was denoted pH(ei). Also we define   
 
1 
0 
ui
μshort 
μtall
0.3
0.7
μmedium

 
Probabilistic Solution of Zadeh’s Test Problems 
541 
P(gi1 | ei) = µshort(ui), P(gi2 | ei) = µmedium(ui),    P(gi3 | ei) = µtall(ui). 
 
Therefore  
 
P(gi1&ei) = P(gi1 | ei)P(ei) = µshort(ui)pH(ei) 
P(gi2&ei) = P(gi2 | ei)P(ei) = µmedium(ui)pH(ei)                                                          (5)                   
P(gi3&ei) = P(gi3 | ei)P(ei) = µtall(ui)pH(ei) 
Here (4) is assumed in formulas (5).  
2.5 
Probability P(John is Short) in the Space with Three Membership Functions 
Next we build a probability space that consists of all events  
 
{(gi1&ei)}, {(gi2&ei)}, {(gi3&ei)}, i=1:n 
 
under assumption (4) 
The version of (3) for μshort(ui) is  
ܲሺJohn is shortሻൌܲሺሧሺ݃௜ଵ&݁௜ሻሻ
௡
௜ୀଵ
ൌ 
    ൌ෍ܲሺ݃௜ଵ&݁௜ሻ
௡
௜ୀଵ
ൌ෍ܲுሺ݁௜ሻܲሺ݃௜ଵ|݁௜ሻ
௡
௜ୀଵ
ൌ ෍ܲுሺ݁௜ሻߤ௦௛௢௥௧ሺݑ௜ሻ                              
௡
௜ୀଵ
 
 
Similar formulas can be written place for terms medium and tall.  In contrast if we 
use the max rule (that is common in fuzzy logic): 
 
                         ߤ ௧௔௟௟ ௢௥ ௠௜ௗௗ௟௘ሺݑ௜ሻൌmax ሺߤ௠௜ௗௗ௟௘൫ݑ௜ሻ, ߤ௧௔௟௟ሺݑ௜ሻ൯                            
we will not get an additive probability space for the case shown in Figure 1, where  
ߤ ௧௔௟௟ ௢௥ ௠௜ௗௗ௟௘ሺݑ௜ሻൌ1, 
 
but max ሺߤ௠௜ௗௗ௟௘൫ݑ௜ሻ, ߤ௧௔௟௟ሺݑ௜ሻ൯൏1 for many ui values of ui  
3 
Test Problem: What is the Probability that John is Short? 
Below we propose another solution for Zadeh’s task:  “It is quite probable that Robert 
is tall. What is the probability that Robert is short?” 
Model 1.   An assumption is that people who know Robert answer questions if 
statement/event e1 or statement/event e2 is happened: 
e1: Robert is tall, e2: Robert is short. 
 

542 
B. Kovalerchuk 
In other words we have a probability space with two events {e1,e2}. Let the 
probabilities in this space be P(e1) = 0.7 and  P(e2) = 0.3  and computed as frequencies 
of respective answers.  Next we construct the probability spaces to map numeric 
probabilities such as 0.7 and 0.3 to linguistic probabilities such as improbable, 
unlikely, quite probable, etc.   
Consider a probability space with two elementary events: {a1,a2},  where  
 
a1 = (linguistic term improbable means probability 0.3),  
a2 = (linguistic term unlikely means probability 0.3). 
 
In short we will write  
a1  = (improbable=0,3), a2=(unlikely=0.3). 
Let some people be asked to select a1 or a2 as a preferred one with results: P(a1)=0 
and P(a2)=1. This means that all those people prefer to associate 0.3 with unlikely not 
with improbable.  Thus,  
P(e2)=P(Robert is short)=0.3 
is mapped to unlikely, that is the answer is: It is unlikely that Robert is short.  
  
Model 2. We remove an assumption that people know Robert, and assume that his 
height is known, and it is 175 cm. Now people will judge 175 cm as tall or short.  The 
rest is the same as above. Alternatively we can estimate Robert's height from the 
statement that it is quite probable that Robert is tall without directly assuming 175 cm.  
  
Model 3. We change elementary events e1 and e2. Now 
 
e1 = (is it quite probable that Robert is tall), 
  e2 = (is it quite probable that Robert is short) 
 
asking n people which answer they prefer and getting frequencies P(e1)=0.7 and  
P(e2)=0.3. Also these can be subjective answers by a single person without computing 
frequencies.  The rest is the same as in models 1 and 2.  
For the possibility the process is the same with changing elementary events e1 and 
e2 to:  
      
 
e1 = (it is quite possible that Robert is tall), 
      
 
e2 = (it is quite possible that Robert is short) 
 
asking n people which answer they prefer and getting frequencies, e.g., P(e1) = 0.8, 
P(e2) = 0.2. 
We will call these numbers numeric possibility levels.  For P(e2) = 0.2 to map it to 
a linguistic possibility value we build a probability space with two elementary 
events: {a1,a2}, where  
 
a1 = (linguistic term impossible means numeric possibility level 0.2), and 
a2 = (linguistic term low possibility means numeric possibility level 0.2). 

 
Probabilistic Solution of Zadeh’s Test Problems 
543 
In short we can write  
 
a1 = (impossible = 0.2), a2 = (low possibility = 0.2). 
 
Again some people are asked to select a1 or a2 as a preferred answer with results 
P(a1) = 0.1 and P(a2) = 0.9, that is most of those people prefer associate 0.2 with low 
possibility not with impossible.  Thus,  
 
P(e2) = P(it is quite possible that Robert is short) = 0.2 
 
is mapped to low possibility with a high confidence (probability 0.9).   
4 
Probability, Possibility and Sufficiency 
Zadeh formulated several test problems such as we are solving in this paper. While 
these tasks are challenging and this paper proposes some solutions for them, it is not 
obvious that solutions of these tasks will resolve the general concern about 
sufficiency/insufficiency of the probability theory.  
There is an obvious semantic difference in the meaning of the natural language 
concepts of probability and possibility.  
Is this fact sufficient for requiring two separate formal theories for dealing with 
concepts of probability and possibility?    
We already have examples where the same theory can model both [Kovalerchuk, 
2013] and the solutions proposed in this paper add more examples. In general, it may 
take time to build and refine such models. The history of the probability theory had 
shown that some models have been developed very quickly in a matter of days and 
some took much more time.  
We want to emphasize that the axiomatic probability theory, and probabilistic 
models based on it, are not the same. The first one needs multiple additions to build 
rich probabilistic models with such concepts as Zadeh’s very productive concept of 
linguistic variables.   
The WCCI 2012 panel [Belyakov et al, 2012] and BISC discussion in 2013 on 
relations between fuzzy logic and probability theory turned to the question to clarify   
what is the probability theory.  
We view the probability theory as consisting of several parts:   (1) the Formal  
Mathematical Theory of the probability spaces that satisfy Kolmogorov’s axioms, (2) 
multiple Mathematical Models based on these spaces (e.g., Markov Chains, Markov 
Processes, Bayesian Networks), (3) Mathematical Statistics as a way to link (1) with 
real world via  its frequency interpretation, (4) Subjective Probability as a way to 
links (1) with real world via its subjective interpretation, (5) Other existing and future 
real world  interpretations of (1) (e.g., using concepts of games or linguistic variables 
that may overlap with (3) and (4) or derived from them).  
Zadeh [BISC, 12.05.2013] stated: “The problem is that computation with 
probability distributions is significantly more complex than computation with 
membership functions”.   

544 
B. Kovalerchuk 
The models and formulas that we deal with in this paper are consistent with this 
statement.  
The simplicity of fuzzy logic comes from removing a significant part of the context 
that is substituting context-dependent AND (intersection) and OR (union) operations 
by truth-functional operations (min/max and other T-norms/T-conorms).   
In particularly, in P(A&B) = P(A | B)P(B) the conditional probability P(A | B) 
expresses context. In contrast, μ(A&B) = min(μ(A), μ(B)) does not express such 
context, because it does not involve context-dependent μ(A | B) or μ(B | A).    
Zadeh responded to such type of criticism [BISC, 12.29.2013]: “With regard to 
truth-functionality, in fuzzy logic truth-functionality applies when there is non-
interactivity. In probability theory, joint probability is the product of marginal 
probabilities when there is independence. Thus, in regard to truth functionality, 
probability theory and possibility theory are on the same footing.” 
This is an important statement which means that today fuzzy logic is applicable 
only to the tasks with non-interactivity by its design and a theory to deal with 
interactivity needs to be developed. In contrast in the probability theory the technique 
of conditional probability addresses the issue of interactivity.    
Similarly Lassiter [BISC post, 01.01.2014] responded to Zadeh’s statement on 
simplicity with a comment: “This is true, but relevant only if we have some reason to 
think that human language understanding relies on a cognitive system which can cope 
with fuzzy logic but cannot cope with probability. I do not have any reason to believe 
this, and in fact there is much evidence that people are able to perform complex 
probabilistic calculations in many cognitive domains (see, e.g., http://www. 
sciencemag.org/content/331/6022/1279.short and references therein). So, I do not 
think that relative complexity of computation is decisive about which theory is better 
as a model of the meanings of vague terms.” 
5 
Conclusion  
The analysis and solutions of Zadeh’s test problems in this paper shows that: (1) 
membership functions provide a computationally efficient way to build a set of 
conditional probabilities, (2) the formula (1) proposed by Zadeh for the probability of 
a fuzzy set is interpretable in rigorous probability theory terms, and (3) the 
operations with probabilities of fuzzy sets should follow probability theory rules.  
These results have been obtained by introduction of a minimal context-dependence 
between fuzzy sets in the linguistic variable in the form of the additive formula (4). 
This allows us to get a rigorous probabilistic interpretation of membership functions 
of fuzzy sets in accordance with the concept of the linguistic context space 
[Kovalerchuk, 1996, 2013; Kovalerchuk, Vityaev, 2000], which is consistent with an 
empirical derivation of membership functions [Hall, Szabo, Kandel, 1986] and 
Hisdal’s probabilistic interpretation of membership functions [Hisdal, 1998].    

 
Probabilistic Solution of Zadeh’s Test Problems 
545 
References 
1. Hall, L., Szabo, S., Kandel, A.: On the Derivation of Memberships for Fuzzy Sets in 
Expert Systems. Information Sciences 40, 39–52 (1986) 
2. Hisdal, E.: Logical Structures for Representation of Knowledge and Uncertainty. 
STUDFUZZ, vol. 14. Springer, Heidelberg (1998) 
3. Kovalerchuk, B.: Quest for Rigorous Combining Probabilistic and Fuzzy Logic 
Approaches for Computing with Words. In: Seising, R., Trillas, E., Moraga, C., Termini, 
S. (eds.) On Fuzziness. STUDFUZZ, vol. 298, pp. 325–336. Springer, Heidelberg (2013) 
4. Kovalerchuk, B.: Context Spaces as Necessary Frames for Correct Approximate 
Reasoning. International Journal of General Systems 25(1), 61–80 (1996) 
5. Kovalerchuk, B., Vityaev, E.: Data Mining in Finance: Advances in Relational and Hybrid 
Methods (ch. 7 on fuzzy systems). Kluwer, Boston (2000) 
6. Lassiter, D.: BISC-group post (January 1, 2014), http://mybisc.blogspot.com/ 
7. Zadeh, L.: A problem in probability theory—a solution (2011); Posted in BISC in (2013) 
8. Zadeh, L.: Probability measures of fuzzy events. Journal of Mathematical Analysis and 
Applications 23(2), 421–427 (1968) 
9. Zadeh, L.: BISC-group posts (December 5, 2013) (December 29, 2013), 
http://mybisc.blogspot.com/ 
10. Zadeh, L.A.: Computing with Words. STUDFUZZ, vol. 277. Springer, Heidelberg (2012) 
11. Beliakov, G., Bouchon-Meunier, B., Kacprzyk, J., Kovalerchuk, B., Kreinovich, V., 
Mendel, J.: Computing With Words (CWW): role of fuzzy, probability and measurement 
concepts, and operations. Mathware& Soft Computing Magazine 19(2), 27–45 (2012) 
 
 

Some Reﬂections on Fuzzy Set Theory
as an Experimental Science
Marco Elio Tabacchi1,2 and Settimo Termini1,3
1 Dipartimento di Matematica ed Informatica,
Universit`a degli Studi di Palermo, Italy
2 Istituto Nazionale di Ricerche Demopolis, Italy
3 ECSC, Spain
{marcoelio.tabacchi,settimo.termini}@unipa.it
Abstract. The aim of this paper is to open a critical discussion on
the claim, recently presented in the community and especially heralded
by Enric Trillas, that fuzzy logic should be seen as an “experimental
science”. The ﬁrst interesting aspect of such remark is whether and in
which way such position has consequences on the real development of
the research, or if it is simply a (diﬀerent) way of looking at the same
phenomenon. As a consequence, we investigate the possible connection
to Zadeh’s distiction between Fuzzy logic in a restricted sense and in
a general sense. We shall argue that Trillas’s claim not only strongly
supports the necessity for such a distinction, but provides a path of
investigation which can preserve the conceptual innovativeness of the
notion of fuzziness.
Keywords: Fuzzy Sets, Soft Computing, Theoretical Computer Science.
1
Introduction
The development of fuzzy set theory has been always constellated by dichotomies
and contrapositions; something, perhaps, which can be considered very natural
in every young ﬁeld which propounds new ideas. It seems, however, that the
contrapositions in such ﬁeld have been particularly frequent and violent in style.
A measure – in our view – of the fact that the intrinsic innovativeness of the
basic ideas was so strong that a normal assimilation was not possible is given
by the emergence of a few epistemological problems, which should be carefully
studied and understood. We should, perhaps, summarize in a simpliﬁed way the
situation through the following remark: in the ﬁrst years of the development of
fuzzy set theory, it was not completely clear the range of the proposed innovation.
The fact is that, if taken seriously, the fuzzy revolution shakes at its root some
of the basic principles of the scientiﬁc revolution of XVII Century. This does not
mean that the proposal of modeling quantities with unsharp boundaries is un-
scientiﬁc, but that it shakes some of the basic pillars of modern science. It is
our job in the vein of our previous research [1–9] to try to preserve the innova-
tiveness of the notion of fuzziness combining it with the unavoidable features of
scientiﬁc method. Let us, just for a clear statement of the ideas involved, point
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 546–555, 2014.
c
⃝Springer International Publishing Switzerland 2014

FST as an Experimental Science
547
in a rough way to two items which could be considered typical and paradigmatic,
and postpone further details and comments:
1. Kalman’s attack to fuzzy set, often quoted by Zadeh itself [10, 11].
2. The paradigmatic use of the word perception in some crucial papers by Lofti
Zadeh (see [12]).
The classical, well known debate points are the probability vs. fuzziness conun-
drum, the interpretation of linguistic labels and all the cascading consequences
of the assumption of this notion in a world made of numbers (of real numbers,
measured and wrote down in their units). All considered the ﬁrst, basic and
founding concept – Fuzzy Sets – is the natural domain of application of fuzzy
ideas and techniques, and the relationship of fuzzy sets with many valued logics
brings the epistemological meaning and signiﬁcance of the development of the
“applications” of fuzzy techniques, especially in the setting of the so called “fuzzy
control”. To keep with the intended point, however, the heart of the matter is: in
which direction the notion of fuzziness has been – and still is – innovative? What
enduring changes does this nature have produced and in which ﬁelds and sectors
of investigation? Did it integrate with the diﬃerent domains of investigation with
whom it had the opportunity of interacting? Will it remain as a separate ﬁeld
or – despite the existence of speciﬁc problems and questions will it eventually
merge with all the domains in which it has shown to be useful?
Let us observe that another dichotomy exists, sometimes openly and more
often surreptitiously: the one between people that substantially think that the
ﬁeld of “fuzzy world and research” is deﬁnitely stabilized for what regards both
its aims and its relationship with other ﬁelds of research; and the alternative
position of a few scientists that opine that the present “acceptance” on the part
of other scientiﬁc disciplines does not correspond to a true acceptance of the
most innovative aspects of the idea of fuzziness. The ﬁrst position is accepted
only as soon as it “conforms” to the traditional “canons” of scientiﬁc rigor, while
acceptance of the other side is not so universally shared. Among the positions
held in this other half of the ﬁeld we shall examine and brieﬂy take into account
the ideas heralded by Enric Trillas in recent years; in particular the thesis that
Fuzzy Sets theory should be approached and studied as an experimental science.
Only this approach – according to Trillas – completely and fully recognizes the
real and deep innovativeness of the notion of fuzziness. In the rest of the paper
we will present this idea in a compact way, trying to relate it with a few of the
other “founding dichotomies” of the theory of Fuzzy Sets as well as with a chain
of new concepts introduced by Zadeh along the years such as linguistic labels and
manipulation of perceptions. We use the term “linguistic labels”, but – perhaps
– the turning point was the notion of linguistic variable, something which can
be represented or assumes the value of a Fuzzy Set a function assuming values
in [0,1] which is then suitably interpreted. The numerical assignment which we
shall ﬁnally do is strictly related to the meaning associated to the linguistic
parts of manipulations. If we had a way of associating numbers and numerical
evaluations without referring to the meaning of the words implied and used them
the same introduction of linguistic objects would be ﬁctitious and redundant.

548
M.E. Tabacchi and S. Termini
1.1
On the Attack on Fuzziness and All That
Now we can go back to the two questions as yet unanswered. Kalman’s criticism
and the implicit consequences of manipulating perceptions. Let us examine these
questions now from a general epistemological point of view. Kalman’s attack can
be seen – from this general point of view, ﬁltered or strained by all the personal
and sociological components – as the recognition that the innovativeness implicit
in taking the notion of fuzziness as a starting point for a new approach was so
atypical that one could tell from the start that it could not have been correctly
incorporated into the “canon” of scientiﬁc tradition that we know and accept
today. So, it should be violently attacked for it role in undermining the same
scientiﬁc method at its very core. In fact Zadeh [13] remembers that the attacks
became stronger after the introduction of the notion of linguistic variable.
The challenges of introducing such notions as fuzzy set, fuzzy control and
linguistic variable are big, but what is a bigger one is the global enterprise. This
is witnessed by one of its subsequent developments, namely the idea that one
should take as a starting point “the manipulation of perception” instead of “the
manipulation of measurements” [12]. Such stance – in an uncritical version of the
received view of scientiﬁc method - implies a violation of one of the basic pillars
of scientiﬁc revolution, namely that science can develop in a secure way only
when it can deal with primary qualities which we are able to aﬃord and study
with objective methods. Perceptions, instead, belong to the world of “secondary”
qualities – which according to a long tradition – are not easily approachable
with classically recognized scientiﬁc methodologies. Now, it is not strange that
such an approach that proposes to construct something starting just from such
debatable and subjective building blocks is not immediately palatable to the
scientiﬁc community in its majority.
1.2
FST as an Experimental Science
The thesis that Fuzzy Sets Theory can be seen and treated as an “Experimental
Science” poses some challenging and interesting questions from an epistemologi-
cal point of view. To further complicate the matter, a further question arises: are
all the possible declinations of Fuzzy Logic related with the distinction, periodi-
cally remarked by Zadeh, between FL in a wide sense and in a restricted sense?
This network of ideas seems to be very slippery and it seems that these kind of
considerations are not included in the daily routine of scientist. We hope that
the bird’s eye recognition we shall present in the following pages will show why
these three topics are connected with the development of the same ﬁeld and –
far from being some additional musing about the actual scientiﬁc work without
any real inﬂuence on his development, they can have a very strong inﬂuence
in determining the future lines of development, and the directions along which
this ﬁeld will evolve in the following years. The habit of doing things following
only the well established and secure paths, or duties implicitly imposed by what
is considered methodologically mandatory in nearby ﬁelds of investigation are
the obstacles we will ﬁnd. Such rules are often more of a limit in frontier ﬁelds,

FST as an Experimental Science
549
and in some cases the innovativeness of the ﬁeld lies exactly in rejecting them,
identifying new ones in the process.
2
A Detour among a Few Quotations
In a paper now almost ten years old [14], Enric Trillas put forward some general
reﬂections on fuzzy sets with the aim of extending “the current theories of fuzzy
sets to wider areas of both language and reasoning”. This path can be followed
– according to him – by “rethinking fuzzy sets from their roots”. This program-
matic statement introduced in the abstract is expanded in the introduction of
the same paper in a sort of manifesto. In fact, after a brief historical summary
of the steps leading from the seminal 1965 paper [15] to the actual stance, he
writes:
Fuzzy logic not only deals with problems at the technological side of
computational intelligence. Since what a fuzzy set does represent is a
concrete use of a predicate (or linguistic label), and as Wittgenstein as-
serted, “the meaning of a word is its use in the language”, fuzzy logic also
deals with what is known as the Gordian Knot of computational intel-
ligence, the problem of meaning. [...] Currently, in the thinking of some
people working in fuzzy logic from almost its beginning, it is sprouting
the idea that the time to rethink fuzzy sets and fuzzy logic is coming.
Such rethinking is not only viewed to push ahead the knowledge of (lin-
guistic) imprecision, and the corresponding uncertainty, but to give a
better support to the more complex applications that are foreseeable
in a not too long future. What this paper tries to oﬃer from a theo-
retical point of view, are just some hints in the direction of extending
the theories of fuzzy sets to face broader areas of language than those
they can currently deal with. [...] Although the paper will refer only
to mathematical models, before beginning with them it is important to
declare, like in the customs, that if mathematics are really important
for the understanding of the phenomena linked to imprecision, they are
nothing more and nothing less than a basic tool for such a goal. What
really matters is imprecision and, of course, mathematical models can
help both to clarify some of its aspects and to base applications in solid
grounds. But if there can be “maths for the Fuzzy” there are not “fuzzy
maths”. Mathematics are what they are and, as always in the history
of science and technology, they are important in that they can help us
is in the study of imprecision with as much precision as possible once
questions on the phenomena are well posed. Only in conjunction with
good questions and ﬁne observations on them, are mathematical models
interesting, and useful, for a deeper understanding of phenomena.
The introduction ends with the following passage, which can be considered a
small programmatic manifesto:

550
M.E. Tabacchi and S. Termini
Provided these ideas for rethinking fuzzy logic would be followed, what
can be guessed is a ramiﬁcation of current fuzzy logic in three branches:
An experimental science of fuzziness, mainly dealing with imprecision in
natural and speciﬁc languages; theoretical fuzzy logic, dealing with math-
ematical models and their linguistic counterparts as well as the necessary
computing tools for their computer implementation, and a broad ﬁeld
of new practical applications to a multiplicity of domains, like internet,
robotics, management, economy, linguistics, medicine, education, etc.
Let us now see how this manifesto is developed in a few subsequent papers. For
instance, in [16] one reads:
This paper [...] is a kind of essay that tries to shed some light on those
links by means of mathematical representations in algebraic frameworks
as simple as possible and, hence, suﬀciently general to allow the study
of a wide spectrum of dynamical systems and reasonings expressed in
Natural Language.
The author gives some speciﬁcations:
It should be noticed that predicates appearing in the language were
usually introduced by naming a property exhibited by some elements
in a ‘universe of discourse’. After this, it is frequently the case that the
considered predicate migrates to another universe of discourse, and that
its use results in some form distorted, but showing ‘family resemblance’
with its former use. Hence, the use we analyze of a predicate is with
reference to a given universe of discourse. The resemblance of uses is also
taken into account, yet an initial study of them can be found in. This
paper does not deal with the processes going from a collective towards
a predicate naming it, but from a predicate on a universe towards the
‘representation’ in mathematical terms of the collective it can originate.
This general Weltanschaung which constructively mixes – a la Wittgenstein –
meaning and use, family resemblances with predicates and usefully migrates from
a universe of discourse to another cannot but escape a sort of mechanical and
automatic application of a general theory, as well as the boundaries ﬁxed by the
crucial and central concepts imposed by the ortodoxy of mathematical logic as
formally structured in the last decades. In fact, two leading ideas are crucial in
this programme: the need for a careful design of fuzzy sets and the necessary
condition for doing that, namely, to consider fuzzy logic as an experimental
science. Let us see what is written in [17]:
The ﬂexible subjects fuzzy logic deals with (that are in contraposition to
the typically rigid of formal logic) force a diﬃerent methodology than the
one formal sciences use to approach the problem. This is like the case of
physics, whose methodology is not as strictly formal as the methodology
of mathematics, even though mathematical models play an important
role in physics. But these models are to be experimentally tested against

FST as an Experimental Science
551
the world. Like it happens with the mathematical models in fuzzy logic,
which are important in the amount that they allow to represent well the
linguistic description of systems and/or processes.
and also
Despite its name, fuzzy logic is, in the ﬁrst place, used for representing
some reasonings involved with imprecision and uncertainty. Fuzzy logic
is closer to an experimental science than to a formal one. In part, because
of the use of real numbers and continuity properties, as well as its use in
real systems.
According to LotﬁZadeh, there are two diﬃerent ways in which the deﬁnition of
fuzzy logic can be understood. Since this distinction is well known in the ﬁeld, we
shall recollect it in a very succinct way. From a recent (January 2013) message
by Zadeh to the components of the Berkeley Initiative in Soft Computing:
A major source of misunderstanding is rooted in the fact that fuzzy logic
has two diﬃerent meanings – fuzzy logic in a narrow sense, and fuzzy logic
in a wide sense. Informally, narrow-sense fuzzy logic is a logical system
which is a generalization of multivalued logic. An important example of
narrow-sense fuzzy logic is fuzzy modal logic. In multivalued logic, truth
is a matter of degree. A very important distinguishing feature of fuzzy
logic is that in fuzzy logic everything is, or is allowed to be, a matter
of degree. Furthermore, the degrees are allowed to be fuzzy. Wide-sense
fuzzy logic, call it FL, is much more than a logical system. Informally, FL
is a precise system of reasoning and computation in which the objects
of reasoning and computation are classes with unsharp (fuzzy) bound-
aries. The centerpiece of fuzzy logic is the concept of a fuzzy set. More
generally, FL may be a system of such systems.
We want to stress for the purposes of the present paper the statement aﬀrm-
ing that “Wide-sense fuzzy logic is much more than a logical system”; it is “a
system of reasoning and computation”. The universe in which we move is not a
logically-dominated world. And this notwithstanding the fact that Fuzzy Logic
has become a very developed and respected brand of mathematical logic, able to
develop systems in which the notion of truth (or better, the notion of truth-value)
is a matter of degree.
Zadeh observes that the research in these subﬁelds is a tiny fragment of the
total quantity of papers and of all the research published. However it is clear,
although not aﬀrmed in this explicit form by Zadeh (at least in the previously
mentioned article) that the main problem and question is not quantitative, but
a conceptual one. The term Fuzzy Logic is generally used to indicate all the
research done using the notion of Fuzzy Set and with the aim of treating in
a more precise way all the situations in which “the objects of reasoning and
computation are classes with unsharp (fuzzy) boundaries”. But it is also clear
that the approach is not proper of logic, and that the word logic is used in a
sense more similar to the one used before Frege’s revolution – and certainly not

552
M.E. Tabacchi and S. Termini
in a axiomatic sense. Maybe it could be interpreted in the sense in which Boole
called his books. And, in a sense, in Zadeh’s approach there is also the ambition
of founding a general theory applicable to all the cases in which we are reasoning
about something that has unsharp boundaries.
Let us now come back to the problem of precision: in Zadeh’s words
Fundamentally, fuzzy logic is aimed at precisiation of what is imprecise.
[...] But in many of its applications fuzzy logic is used, paradoxically to
imprecisiate what is precise. In such applications, there is a tolerance for
imprecision, which is exploited through the use of fuzzy logic. Precisia-
tion carries a cost. Imprecisiation reduces cost and enhances tractability.
This is what I call the Fuzzy Logic Gambit. What is important to note
is that precision has two diﬃerent meanings: precision in value and preci-
sion in meaning. In the Fuzzy Logic Gambit what is sacriﬁced is precision
in value, but not precision in meaning.
So, the right and correct use of FL in a speciﬁc domain is always aimed at
rendering more precise what one is researching, although in some cases the “pre-
cisiation of meaning” can be strictly and inavoidably linked to an “imprecisia-
tion in value”. This is what Zadeh calls “Fuzzy Logic Gambit”. In any case and
without too much details, Fuzzy Logic in Zadeh’s view is aimed at increasing
precision, although the general and global increase can in some cases correspond
to a decrease of precision in value, accepted in order to obtain a more relevant
increase in meaning. If we combine this observation with his claim that FL is
not an axiomatic logical system in the sense required by mathematical logic, we
can certainly aﬀrm that an enlarged setting like the one proposed by Trillas is
a solution that allows preserving the scientiﬁc rigor while looking for new ways
to be explored.
3
Conclusions
The aim of this paper was to present the outline of a brief analysis in order
to consider and classify the proposal of considering FST as an “experimental
science”. First it was recognized that discussing this topic would involve to both
look at the history of FS, and to indicate the most fruitful path to be followed
in the future. The reason for that resides in that if by classifying a theory of
a mathematical / logic and applied kind as an experimental science we are
obliged to reconsider all the previous developments of the ﬁeld. Not, of course,
in the sense of the validation or re-evaluation of all the results obtained in the
previous decades – as they remain valid for what they are – but in the sense
of looking at the various, crucial results in a diﬃerent way. Let us for instance
have a look at the problem of extended and propositional connectives. In the
previous Section we brieﬂy remembered how Zadeh has posed the question of
the diﬃerence between fuzzy logic in a strict sense and in a wide sense. Let us
now see how the logician Petr H´ajek [18] who has certainly worked hard toward
obtaining important and deep results, but has also tried to be a good translator

FST as an Experimental Science
553
and ambassador between the republic of mathematical logic and the kingdom of
fuzzy sets, sees the problem:
In a broad sense, the term ‘fuzzy logic’ has been used as synonymous
with ‘fuzzy set theory and its applications’; in the emerging narrow sense,
fuzzy logic is understood as a theory of approximate reasoning based on
many-valued logic. Zadeh stresses that the questions of fuzzy logic in
the narrow sense diﬃer from usual questions of many-valued logic and
concern more questions of approximate inferences than those of com-
pleteness, etc.; nevertheless, with full admiration to Zadeh’s pioneering
and extensive work a logician will ﬁrst study classical logical questions
on completeness, decidability, complexity, etc. of the symbolic calculi
in question and then try to reduce the question of Zadeh’s agenda to
questions of deduction as far as possible.
And he concludes his survey paper by aﬀrming that
Fuzzy logic in the narrow sense is a logic, a logic with a comparative
notion of truth. It is mathematically deep, inspiring and in quick devel-
opment; papers on it are appearing in respected logical journals. [...] The
bridge between fuzzy logic in the broad sense and pure symbolic logic is
being built and the results are promising.
We think that the situation is clear enough to allow the drawing of some conclu-
sions. H´ajek attitude, which is one of the most open in the ﬁeld of mathematical
logic, is very optimistic. A substantial piece of the questions asked and posed
by FS has been thoroughly introduced into the agenda of mathematical logic.
He also recognizes that the interaction between the old program of many-valued
logic and the informal questions posed by the fuzzy approach has been mas-
sively fruitful, also for mathematical many-valued logic: some new questions has
been posed and answered in the aﬀrmative way. However the agenda – as it is
natural – is ﬁxed by mathematical logic, by the questions asked and posed by
its research programme. Let us observe that the attitude outlined seems more
radical that the one emerging from the work done by people working in quantum
logic looking for bridges with fuzzy concepts and techniques. In this context, at
least following the attitude of researchers such as Pykacz [19] – who seems to
be very enthusiastic towards the innovative aspects of the fuzzy approaches –
an attempt can be found at considering fully classical points and requirements
of the quantum logic approach. But as well a similar care in preserving all the
nuances of the language of fuzzy sets theory is present. And, et pour cause –
as he wants to recover all the innovativeness at a conceptual level of the notion
of fuzziness. This can also be fully obtained by detailing a complete and total
reduction of quantum logic to the language of FST. We see and frankly admit
that the situation in quantum logic, and in general in mathematical logic, is
diﬃerent, but cannot refrain from observing that the epistemological attitude is
– let us say – totalitarian. It is not a problem of passing judgements. It is exactly
a question of looking for the approaches and attitudes that can be more suitable
for the expansion and development of a certain ﬁeld.

554
M.E. Tabacchi and S. Termini
In a not distant future it could also happen that all the work done in fuzzy logic
in an extended sense would be recognized as a province of mathematical logic
(possibly in an enlarged version which beside accepting “a comparative notion
of truth” as written by H´ajek for what regards “Fuzzy logic in the narrow sense”
will accept and assimilate many other new concepts and notions that today seem
to be outside the scope and very far from mathematical logic as is intended
today). It is not very likely – in our view – that such a new framework will be
able to include almost all the nuances of the concepts related to language that
are present in the (too) numerous quotations reported in the previous Section.
However this is not the crucial and central problem at the moment.
The problem at hand today is to recognize whether many crucial questions
can be better aﬃorded in a general scheme in which the agenda is ﬁxed by math-
ematical logic, or by other actors on the stage. It seems to us that due to the
richness of the concepts and problems involved a good pluralism is more useful
for the advancement of the ﬁeld(s). This is what, with his polite and diplomatic
approach, Zadeh propounds in his periodic comments on the diﬃerences between
the two ways of interpreting the term “fuzzy logic”. It seems that all the prob-
lems related to a treatment using approaches related or conceptually inspired
to the paradigm of natural language receive their best consideration inside a
general strategy like the one outlined by Trillas with his proposal inspired to
the epistemology and working of experimental sciences. It does not exclude that
more and more (small or big) fragments of the future technical developments of
FST (or fuzzy logic) obtained by working in this direction could be embedded
into the paradigm of mathematical logic. However, will this happen, the basic
reason will be connected to the fact that – a posteriori – the new results will be
naturally tuned with the crucial and central questions of mathematical logic, and
not that – a priori – the questions to be studied were selected by their adherence
to its paradigm. It is probable that if this (“experimental”) approach is taken
seriously, the results – as seen globally - could show that there exists a coherent
something which has many distinguishing and speciﬁc features which prevent its
total collapse into the other paradigm. So, we think that it is possible to con-
clude that the idea of considering fuzzy set theory as an experimental science
not only is perfectly tuned with the general ideas of its founder, but strongly
reinforces the possibility that the new developments preserve all the conceptual
innovativeness of the original idea.
References
1. Cardaci, M., Di Ges´u, V., Petrou, M., Tabacchi, M.E.: On the evaluation of images
complexity: A fuzzy approach. In: Bloch, I., Petrosino, A., Tettamanzi, A.G.B.
(eds.) WILF 2005. LNCS (LNAI), vol. 3849, pp. 305–311. Springer, Heidelberg
(2006)
2. Cardaci, M., Di Gesu, V., Petrou, M., Tabacchi, M.E.: A fuzzy approach to the
evaluation of image complexity. Fuzzy Sets Syst. 160(10), 1474–1484 (2009)
3. Petrou, M., Tabacchi, M.E., Piroddi, R.: Networks of Concepts and Ideas. The
Computer Journal 53(10), 1738–1751 (2010)

FST as an Experimental Science
555
4. Termini, S., Tabacchi, M.E.: Fuzzy set theory as a methodological bridge between
hard science and humanities. International Journal of Intelligent Systems 29(1),
104–117 (2014)
5. Tabacchi, M.E., Termini, S.: Measures of fuzziness and information: some chal-
lenges from reﬂections on aesthetic experience. In: Proceedings of WConSC 2011
(2011)
6. Tabacchi, M.E., Termini, S.: Fuzziness and social life: informal notions, formal
deﬁnitions. In: Proceedings of the Annual Meeting of the North American Fuzzy
Information Processing Society, NAFIPS (2012)
7. Tabacchi, M.E., Termini, S.: A few remarks on the roots of fuzziness measures. In:
Greco, S., Bouchon-Meunier, B., Coletti, G., Fedrizzi, M., Matarazzo, B., Yager,
R.R. (eds.) IPMU 2012, Part II. CCIS, vol. 298, pp. 62–67. Springer, Heidelberg
(2012)
8. Seising, R., Tabacchi, M.E.: A very brief history of soft computing. In: Pedrycz, W.,
Reformat, M. (eds.) 2013 Joint IFSA World Congress NAFIPS Annual Meeting.
IEEE SMC (2013)
9. D’Asaro, F., Perticone, V., Tabacchi, M.E., Termini, S.: Reﬂections on technology
and human sciences: rediscovering a common thread through the analysis of a few
epistemological features of fuzziness. Archives for Philosophy and History of Soft
Computing 1 (2013)
10. Zadeh, L.A.: The evolution of systems analysis and control: a personal perspective.
IEEE Control Systems 16(3), 95–98 (1996)
11. Zadeh, L.A.: Is there a need for fuzzy logic? Information Sciences 178(13), 2751–
2779 (2008)
12. Zadeh, L.A.: Toward a perception-based theory of probabilistic reasoning with
imprecise probabilities. Journal of Statistical Planning and Inference 105(1), 233–
264 (2002)
13. Zadeh, L.A.: Foreword. In: Trillas, E., Bonissone, P.P., Magdalena, L., Kacprzyk, J.
(eds.) Combining Experimentation and Theory. STUDFUZZ, vol. 271, pp. IX–XI.
Springer, Heidelberg (2011)
14. Trillas, E.: On the use of words and fuzzy sets. Information Sciences 176(11), 1463–
1487 (2006)
15. Zadeh, L.A.: Fuzzy sets. Information and Control 8, 338–353 (1965)
16. Garc´ıa-Honrado, I., Trillas, E.: An essay on the linguistic roots of fuzzy sets. In-
formation Sciences 181(19), 4061–4074 (2011)
17. Trillas, E., Guadarrama, S.: Fuzzy representations need a careful design. Interna-
tional Journal of General Systems 39(3), 329–346 (2010)
18. H´ajek, P.: Why fuzzy logic? In: Jacquette, D. (ed.) A Companion to Philosophical
Logic, pp. 595–605. Wiley (2008)
19. Pykacz, J.: Fuzzy sets in foundations of quantum mechanics. In: Seising, R., Trillas,
E., Moraga, C., Termini, S. (eds.) On Fuzziness: Volume 2. STUDFUZZ, vol. 299,
pp. 553–557. Springer, Heidelberg (2013)

A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 556–565, 2014. 
© Springer International Publishing Switzerland 2014 
Fuzziness and Fuzzy Concepts  
and Jean Piaget’s Genetic Epistemology 
Rudolf Seising 
European Centre for Soft Computing, Mieres, Spain 
Rudolf.seising@softcomputing.es 
Abstract. How do humans develop concepts? -- This paper presents a historical 
view on answers to this question. Psychologist Piaget was influenced by Philo-
sopher Kant when he founded his theory of cognitive child development named 
“Genetic Epistemology”. Biologist and historian of science Rheinberger empha-
sized that scientific concepts are “fluctuating objects” or “imprecise concepts” 
when he founded his “Historical Epistemology”. In this paper we combine these 
approaches with that of fuzzy concepts. We give some hints to establish a  
new approach to extend Piaget’s theory, to a so-called “Fuzzy Genetic Episte-
mology”. 
Keywords: Epistemology, Genetic Epistemology, Historical Epistemology, 
Fuzzy Sets, Fuzzy Concepts, Structuralism. 
1 
Introduction 
In this paper we consider the concept of concepts. The Stanford Encyclopedia of Phi-
losophy differentiates at least between three prevailing ways to understand what a 
concept is in contemporary philosophy [1]:  
• Concepts as mental representations, i.e. entities that exist in the brain; 
• Concepts as abilities, peculiar to cognitive agents; 
• Concepts as abstract objects, where these objects are the constituents of proposi-
tions that mediate between thought, language, and referents. 
In their book Concepts and Fuzzy Logic Belohlavek and Klir [2] use the concept of 
concepts from Cognitive science and they refer to Edouard Machery: “In cognitive 
science, concepts are the bodies of knowledge that are stored in long-term memory 
and are used by default in the higher cognitive processes (categorization, inductive 
and deductive reasoning, analogy making, language understanding, etc.).” [3] 
In this paper we will argue that those concepts are unsharp or fuzzy and we will 
show that this fuzzy concept is also fruitful in Historical and Genetic epistemology − 
that are two disciplines that have been founded in the 20th century by Hans-Jörg 
Rheinberger and Jean Piaget, respectively. 

 
Fuzziness and Fuzzy Concepts and Jean Piaget’s Genetic Epistemology 
557 
2 
What Is Epistemology 
2.1 
Knowledge and Cognition 
Epistemology is the branch of philosophy that is concerned with knowledge. The 
Stanford Encyclopedia of Philosophy defines “narrowly, epistemology is the study of 
knowledge and justified belief. As the study of knowledge, epistemology is concerned 
with the following questions: What are the necessary and sufficient conditions of 
knowledge? What are its sources? What is its structure, and what are its limits?” [4] 
To answer these questions philosopher Immanuel Kant (1724-1804) tried to unify 
the two main views in former philosophy of science, empiricism and rationalism. The 
rationalist approach came to fundamental, logical and theoretical investigations using 
logics and mathematics to formulate axioms and laws, however from the empiricist 
point of view the source of our knowledge is sense experience and we have to use 
experiments to find or prove or refute natural laws. In both directions – from experi-
mental results to theoretical laws or from theoretical laws to experimental proves or 
refutations – scientists have to bridge the gap that separates theory and practice in 
science. 
In his Critique of Pure Reason Kant came from the view that knowledge increases 
through, and that cognition starts from experience. He named “concepts” the “a priori 
forms of cognition” and he proposed that these concepts exist within the subject of 
cognition. On the other hand, he said, that the object of cognition is established when 
the sensory content coming from the object is put in order by the subject’s concepts. 
In Kant’s epistemology the human mind provides a structure that shapes all sensory 
experience and thought. Perceptions and thoughts must conform into this structure in 
order to be representations. Kant says that humans have an active mind that produces 
our conception of reality by acting as a filter, and also it is organizing and enhancing. 
Kant says that objective reality is made possible by the form of its representation. Our 
mind’s structure includes space, time, and causation, they are preconditions of our 
perceptions, and they are fundamental conditions for human experience. 
2.2 
Science, Nature and Concepts 
All our scientific theories are abstract and formal but the systems we are intended to 
reconstruct by our theories are real world systems. Therefore we now draw our atten-
tion to the philosophical topic of the gap between theoretical and real entities. Again 
we have to mention Kant. His construction of the system of science with a progres-
sion from the formal to the most empirical phases he named an “Architectonic of 
Science”: “By the term architectonic I mean the art of constructing a system. Without 
systematic unity, our knowledge cannot become science; it will be an aggregate, and 
not a system. Thus architectonic is the doctrine of the scientific in cognition, and 
therefore necessarily forms part of our methodology.” [5] 
In this Critic of Pure Reason he also developed a system of physical nature starting 
with “the most formal act of human cognition, called by him the transcendental unity 
of apperception, and its various aspects, called the logical functions of judgment. He 
then proceeds to the pure categories of the understanding, and then to the schematized 
categories, and finally to the transcendental principles of nature in general.” [6]  

558 
R. Seising 
It is this concept of a “schema” that Kant used to both the structure of human 
knowledge itself and the procedure by which the human mind produces and uses such 
structures, that Jean Piaget adopted to his epistemology, the Genetic epistemology. 
2.3 
Genetic Epistemology 
In the history of Developmental Psychology where some psychologists became inter-
ested in the field of child development in the early 20th-century, mainly two doctrines 
are significant to date: 
• In his Analytic Psychology (Psychoanalysis) Sigmund Freud (1856-1939) stressed 
the importance of childhood events and experiences. In his Three Essays on the 
Theory of Sexuality he described child development as a series of 'psychosexual 
stages' or phases: oral (ages 0–2), anal (2–4), phallic (3–6), latency (6-puberty), 
and mature genital (puberty-onward). Each phase involves the satisfaction of a li-
bidinal desire and can later play a role in adult personality. [7] 
• When he was a young scientist, Jean Piaget was engaged in Psychoanalysis, but 
later he established a very different stage theory of children’s cognitive develop-
ment: Genetic Epistemology. In his theory it is assumed that children think diffe-
rently than adults and they play an active role in gaining knowledge of the world. 
They actively construct their knowledge! 
2.4 
Historical Epistemology 
The Swiss historian of science and biologist Hans-Jörg Rheinberger proposed in the 
last decade of the 20th century the program of “historical epistemology” as a turna-
round in philosophy and history of science. He made allowance for the situation that 
historians and philosophers of science in the 1980s and 1990s concentrated their at-
tention to experiments in science. Historical epistemology deals with the concept of 
so-called “experimental systems”. What is an experimental system? − Rheinberger 
illustrated: “This notion is firmly entrenched in the everyday practice and vernacular 
of the twentieth-century life scientists, especially of biochemists and molecular biolo-
gists. Scientists use the term to characterize the scope, as well as the limits and the 
constraints, of their research activities. Ask a laboratory scientist what he is doing, 
and he will speak to you about his “system”. Experimental systems constitute integral, 
locally manageable, functional units of scientific research.” ([8], p. 246) 
In Rheinberger’s epistemology we find again two parts of scientific research, that 
he named epistemic and technical, respectively, but he emphasizes that there is no 
sharp boundary between, moreover this boundary is vague or fuzzy. Here is a brief 
sketch of his approach: “If there are concepts endowed with organizing power in a 
research field, they are embedded in experimental operations. The practices in which 
the sciences are grounded engender epistemic objects, epistemic things as I call them, 
as targets of research.” We follow Rheinberger’s arguments to show that theses “epis-
temic things” are vague or fuzzy and that they “move the world of science” ([9],  
p. 220). He considered these “fluctuating objects” and “imprecise concepts” – as he 
also called them - in detail in his historical work. 

 
Fuzziness and Fuzzy Concepts and Jean Piaget’s Genetic Epistemology 
559 
3 
Inexact or Fuzzy Concepts 
3.1 
Fuzzy Sets and Fuzzy Systems 
In the early 1960s Lotfi A. Zadeh, a professor of Electrical Engineering at the Univer-
sity of California at Berkeley “began to feel that complex systems cannot be dealt 
with effectively by the use of conventional approaches largely because the description 
languages based on classical mathematics are not sufficiently expressive to serve as a 
means of characterization of input-output relations in an environment of imprecision, 
uncertainty and incompleteness of information.” [10] In the year 1964 he discovered 
how he could describe real systems as they appeared to people. “I’m always sort of 
gravitated toward something that would be closer to the real world” [11]. In order to 
provide a mathematically exact expression of experimental research with real sys-
tems, it was necessary to employ meticulous case differentiations, differentiated ter-
minology and definitions that were extremely specific to the actual circumstances, a 
feat for which the language normally used in mathematics could not provide well. The 
circumstances observed in reality could no longer simply be described using the 
available mathematical means. These thoughts indicate the beginning of the genesis 
of Fuzzy Set Theory.” ([12], p. 7) In his first article “Fuzzy Sets” he launched new 
mathematical entities as classes or sets that “are not classes or sets in the usual sense 
of these terms, since they do not dichotomize all objects into those that belong to the 
class and those that do not.” He introduced “the concept of a fuzzy set, that is a class 
in which there may be a continuous infinity of grades of membership, with the grade 
of membership of an object x in a fuzzy set A represented by a number fA(x) in the 
interval [0,1].” [13, 14] 
Some years later Zadeh compared the strategies of problem solving by computers 
on the one hand and by humans on the other hand. He called it a paradox that the 
human brain is always solving problems by manipulating “fuzzy concepts” and “mul-
tidimensional fuzzy sensory inputs” whereas “the computing power of the most po-
werful, the most sophisticated digital computer in existence” is not able to do this. 
Therefore, he stated that “in many instances, the solution to a problem need not be 
exact”, so that a considerable measure of fuzziness in its formulation and results may 
be tolerable. The human brain is designed to take advantage of this tolerance for im-
precision whereas a digital computer, with its need for precise data and instructions, is 
not.” ([15], p. 132)  
3.2 
Fuzzy Concepts 
Zadeh had served as first reviewer and his Berkeley-colleague and mathematician 
Hans-Joachim Bremermann (1926–1996), as second for the Ph.D. thesis of mathema-
tician Joseph A. Goguen’s (1941-2006) entitled Categories of Fuzzy Sets. [16] Here, 
Goguen generalized the fuzzy sets to so-called “L-sets”. An L-set is a function that 
maps the fuzzy set carrier X into a partially ordered set L. The partially ordered set L 
Goguen called the “truth set” of A. The elements of L can thus be interpreted as “truth 
values”; in this respect, Goguen then also referred to a Logic of Inexact Concepts 
[17].  

560 
R. Seising 
Zadeh’s efforts to use his fuzzy sets in linguistics led to an interdisciplinary scien-
tific exchange between him and Goguen on the one hand and between Berkeley-
psychologist Eleanor Rosch and the Berkeley-linguist George Lakoff on the other. 
In her psychological experiments Rosch could show that concept categories are 
graded. Consequently she argued that concepts are not adequately represented by 
classical sets. Rosch developed her prototype theory on the basis of these empirical 
studies. This theory assumes that people perceive objects in the real world by compar-
ing them to prototypes and then ordering them accordingly. In this way, according to 
Rosch, word meanings are formed from prototypical details and scenes and then in-
corporated into lexical contexts depending on the context or situation. It could there-
fore be assumed that different societies process perceptions differently depending on 
how they go about solving problems. [18] 
Lakoff referred to the fact that also statements in natural language are graded, “that 
sentences of natural languages (at least declarative sentences) are either true or false 
or, at worst, lack a truth value, or have a third value.” He argued “that natural lan-
guage concepts have vague boundaries and fuzzy edges and that, consequently, natu-
ral language sentences will very often be neither true, nor false, nor nonsensical, but 
rather true to a certain extent and false to a certain extent, true in certain respects and 
false in other respects. (19, p. 458) In this paper Lakoff wrote that Zadeh’s fuzzy set 
theory is an appropriate tool of dealing with degrees of membership, and with (con-
cept) categories that have unsharp boundaries. Because he used the term “fuzzy logic” 
he deserves the credit for first introducing this expression in the scientific literature 
but based on his later research, however, he came to find that fuzzy logic is not an 
appropriate logic for linguistics [20]. Now, we move to fuzzy concepts in psychology! 
4 
Cognitive Development 
4.1 
A Brief Sketch of a Biography on Jean Piaget 
Jean Piaget (1896-1980) was born in Neuchâtel, in Switzerland. After his graduation 
in biology his interest turned to psychoanalysis and he moved to Paris where he be-
came a teacher in Alfred Binet’s (1857-1911) school for boys. Here he helped to mark 
intelligence and during that time he noticed that young children consistently gave 
wrong answers to certain questions. Piaget did not focus so much on the fact of the 
children's answers being wrong, but that young children consistently made types of 
mistakes that older children and adults did not. He established the theory that young 
children's cognitive processes are inherently different from those of adults and finally 
he proposed a global theory of cognitive developmental stages in which individuals 
exhibit certain common patterns of cognition in each period of development [21, 22]. 
In 1921, Piaget returned to Switzerland as director of the Rousseau Institute in Gene-
va. In 1923, Piaget married his co-worker Valentine Châtenay. The couple had three 
children, whom the parents studied from infancy. Piaget was professor of psychology, 
sociology, and philosophy of science at the University of Neuchatel (1925-1929). In 
1929 he became Director of the International Bureau of Education (IBE) and he  
remained the head of this international organization until 1968. Piaget died in 1980. 

 
Fuzziness and Fuzzy Concepts and Jean Piaget’s Genetic Epistemology 
561 
4.2 
Structuralism 
How is the making of ideas, concepts, and structures (or their “preforms”) in the de-
velopment of human beings’ minds, i.e. in the minds of children? An answer to this 
question was given by Piaget with his “Genetic epistemology” that he also named 
“mental embryology” or “embryology of intelligence”. In this approach the concept 
of structures is most significant. In his early years Piaget noticed a tendency to con-
sider structures in various academic disciplines, as in psychology (Gestalt psycholo-
gy, e.g. Kurt Koffka, Max Wertheimer, and Wolfgang Köhler), in sociology (Karl 
Marx) and particularly in linguistics (Roman O. Jakobson and Ferdinand de Saus-
sure), in physics (e.g. field theory of James C. Maxwel)l and in mathematics (Nicolas 
Bourbaki). Last mentioned “Nicolas Bourbaki” was a pseudonym under which a 
group of French mathematicians wrote a series of books. The group members’ view 
of modern advanced mathematics was to found all of mathematics on set theory. With 
this goal the group strove for rigour and generality in modern mathematics [23].  
This programme is the root of an approach in the 1950s to determine the mathe-
matical structure of a theory in a precise way by use of informal set theory without 
recourse to formal languages, created by the American mathematician and philoso-
pher Patrick Suppes (born 1922) [24-26]. In the 1970s, Suppes’ Ph D.-student and 
physicist Joseph D. Sneed (born 1938) developed informal semantics meant to  
include not only mathematical aspects, but also application subjects of scientific theo-
ries in the framework, based on this method. In The Logical Structure of Mathemati-
cal Physics [27] he presented the view that all empirical claims of physical theories 
have the form “x is an S”, where “is an S” is a set-theoretical predicate (e.g., “x is a 
classical particle mechanics”).  
In the last third of the 20th century this structuralist approach has been elaborated 
by Sneed, W. Stegmüller, C. U. Moulines, W. Balzer and others. Their developments 
act as a bridge between philosophy and history of science to describe logical struc-
tures and dynamics of scientific theories. These authors have published their results as 
An Architectonic for Science as [28].  
To adapt this structuralist metatheory to fuzzy set theory the physician and philoso-
pher Kazem Sadegh-Zadeh requires in his Handbook of Analytical Philosophy of Medi-
cine “to render the metatheory applicable to real world scientific theories, it needs to be 
fuzzified because like everything else in science, scientific theories are vague entities and 
implicitly or explicitly fuzzy. To explicitely fuzzify scientific theories he claimes the 
“Introduction of the theory’s set-theoretical predicate as a fuzzy predicate (“x is a fuzzy 
S” instead of “x is an S”)” and may be “also any other component of the theory appearing 
in the structure that defines the predicate may be fuzzified.” He concludes with the fol-
lowing outlook: “Fuzzifications of both types will impact the application and applicabili-
ty of theories as well as the nature of the knowledge produced by using them” [29,  
p. 439f]. For further analyses and assessments we refer to [30, 31] 
The early structuralist approaches encouraged Piaget to create his new epistemological 
approach with a genetic view and with the concept of abstract structures as central enti-
ties. In his view humans have adaptive mental structures. These mental structures assimi-
late external events; humans convert these external events to fit their mental structures. 
On the simplest level of these mental structures Piaget introduced so-called “schemata” 
as categories of knowledge to describe mental or physical actions. 

562 
R. Seising 
4.3 
Genetic Epistemology 
As a biologist by training Piaget was skilled to observe that organism adapted to their 
environment. He then applied this model to cognitive development and in his theory 
the mind organizes internalized regularities or operations into dynamic cognitive 
structures. Piaget named these structures “schema”. Schemata are structured clusters 
of concepts that can be used to represent objects, scenarios or sequences of events or 
relations between concepts. As we mentioned already, the original idea was proposed 
by philosopher Kant as innate structures used to help us perceive the world. Piaget’s 
concept of a schema covers category of knowledge and a process to receive that 
knowledge. When we obtain new knowledge then our schema may be modified or 
changed. In his view, the cognitive development (the adaption to the environment) is 
a process of four aspects: schema, assimilation, accommodation, and equilibrium. 
• Schema: Humans develop “cognitive structures” or “mental categories” that Piaget 
named schemata in order to name and organize, and to make sense of life and reali-
ty. [32, p. 10] 
• Assimilation: An individual uses its existing schemata to make sense of a new 
event. This process involves trying to understand something new by fitting it into 
what we already know. 
• Accommodation: Also existing schemata can change to respond to a new situation. 
If new information cannot be made to fit into existing schemata, a new, more ap-
propriate structure must be developed. There are also instances when an individual 
encounters new information that is too unfamiliar that neither assimilation nor  
accommodation will occur because the individual may choose to ignore it. Equili-
bration: This is the complex act of searching for the balance in organizing, assimi-
lating, and accommodating.  
On the other hand, it is the state of Disequilibrium that motivates us to search for a 
solution through assimilation or accommodation.  
In Psychogenesis and the History of Science – a coauthored and posthumous pub-
lished book of Piaget and physicist Rolando Garcia, Piaget referred to an evolutionary 
view of theory change in science as MacIsaac [33] quoted: “Our notion of epistemic 
framework ...”, which describes “...an explanatory schema for the interpretation of the 
evolution of knowledge, both at the level of the individual and that of social evolu-
tion”. Piaget further recognizes the role of social construction in schemata, stating “... 
when language becomes the dominating means of communication ... what we might 
call direct experience of objects becomes subordinated ... to the system of interpreta-
tions attributed to it by the social environment.” He claimed that there is a connection 
of the psychogenesis of logico-mathematical thinking and historical development, he 
distinguished between “prescientific” (other authors name it “immature”) and “scien-
tific” periods in each field, he mentions “analogies” between ontogenesis and histori-
cal development in both periods [34, p. 63].  
Quoting Wadsworth, “...the child’s active assimilation of objects and events results 
in the development of structures (schemata) that reflect the child’s concepts of the 
world or reality. As the child develops these structures, reality or his knowledge of the 
world changes.” [35] 

 
Fuzziness and Fuzzy Concepts and Jean Piaget’s Genetic Epistemology 
563 
4.4 
Unsharp Concepts in Piaget’s Genetic Epistemology 
Piaget defined four stages of cognitive development: 
• Sensorimotor stage: Birth through ages 18-24 months. −  “In this stage, infants 
construct an understanding of the world by coordinating experiences (such as see-
ing and hearing) with physical, motoric actions. Infants gain knowledge of the 
world from the physical actions they perform on it. An infant progresses from ref-
lexive, instinctual action at birth to the beginning of symbolic thought toward the 
end of the stage.”[36] 
• Preoperational stage: 18-24 months – 7 years. −  The children do not yet under-
stand concrete logic and cannot mentally manipulate information. They increase in 
playing but this is mainly categorized by symbolic play and manipulating symbols. 
They still have trouble seeing things from different points of view. The children 
lack basic logic. An example of transitive inference: when a child is presented with 
the information “ a is greater than b and b is greater than c.” This child may have 
difficulty here understanding that a is also greater than c. [37, ch. 3] [38, ch. 8] 
• Concrete operational stage: Ages 7 – 12 years. −  In this stage children use logic 
appropriate. They start solving problems in a more logical fashion but abstract, hy-
pothetical thinking has not yet developed. The children can only solve problems 
that apply to concrete events or objects. They can draw inferences from observa-
tions in order to make a generalization (inductive reasoning) but they struggle with 
deductive reasoning, which involves using a generalized principle in order to try to 
predict the outcome of an event. The Children cannot figure out logic, e.g., a child 
will understand A>B and B>C, however when asked is A>C, the child might not be 
able to logically figure the question out in their heads.” 
• Formal operational stage: Adolescence – adulthood. −  Now, the person has the 
ability to distinguish between their own thoughts and the thoughts of others. He or 
she recognizes that their thoughts and perceptions may be different from those 
around them. The children are now able to classify objects by their number, mass, 
and weight, they can think logically about objects and events and they have the 
ability to fluently perform mathematical problems in both addition and subtraction. 
However, it was quite plain to Piaget that not all children may pass through the stages 
at the exactly the same age. Also he acknowledged that children could show at a giv-
en time the characteristics of more than one of the stages above. Nevertheless he em-
phasized that cognitive development always follows the sequence of the enumeration 
above. No stage can be skipped, and every stage prepares the child with new intellec-
tual abilities and a more complex understanding of the world. Therefore, Piaget’s four 
stages of cognitive development are unsharp defined stages. Also the analysis of Pia-
get’s model by MacIsaac’ leads to the assumption that this model comprises non-crisp 
concepts, e.g. Piagetian assimilation “is a much more complex and fluid activity. 
There is a great deal of flexibility displayed within assimilation […].” [28] 

564 
R. Seising 
5 
Outlook on Fuzzy Concepts in Genetic Epistemology 
In our view the concepts that Piaget named “schema” are fuzzy concepts, Piaget’s 
processes of assimilation and accommodation are fuzzy relations, and the stages in 
Piaget’s constitution of the series of the cognitive development of children have no 
sharp borders but are fuzzy stages. A fuzzy structuralist approach to model Piaget’s 
theory, i.e. a “Fuzzy Genetic Epistemology” could be established by using fuzzy sets 
and fuzzy relations instead of usual sets and relations. With fuzzy structures we will 
create fuzzy models for unsharp concepts (“fuzzy schemata”) that children use to 
represent objects, scenarios or sequences of or relations between events. Fuzzy rela-
tions will model changes of fuzzy schemata like Piaget’s “assimilation” and “accom-
modation”. 
Acknowledgements. Work leading to this paper was partially supported by the 
Foundation for the Advancement of Soft Computing Mieres, Asturias (Spain). 
References 
1. Margolis, E., Lawrence, S.: Concept. Stanford Encyclopedia of Philosophy. Metaphysics 
Research Lab at Stanford University (retrieved November 6, 2012) 
2. Belohlavek, R., Klir, G.J. (eds.): Concepts and Fuzzy Logic. The MIT Press, Cambridge 
(2011) 
3. Machery, E.: Concepts are not a Natural Kind. Phil. of Science 72, 444–467 (2005) 
4. Steup, M.: Epistemology. The Stanford Encyclopedia of Philosophy (Winter 2013 Edition), 
http://plato.stanford.edu/archives/win2013/entries/epistemology 
5. Kant, I.: Critic of Pure Reason, ch. III. The Architectonic of Pure Reason, The Cambridge 
Edition of the Works of Immanuel Kant. Cambridge University Press (1998), 
http://www.philosophy-index.com/kant/critique_pure_reason/ 
ii_iii.php 
6. Ellington, J.W.: The Unity of Kant’s Thought in his Philosophy of Corporeal Nature. Phi-
losophy of Material Nature. Hackett Publishing Company, Indianapolis (1985) 
7. Freud, S.: Three Essays on the Theory of Sexuality, VII, 1905, 2nd edn. Hogarth Press 
(1955) 
8. Rheinberger, H.-J.: Experimental complexity in biology: Some Epistemological and histor-
ical remarks. Philosophy of Science, Proc. of the 1996 Biennial Meeting of the Philosophy 
of Science Association 64(suppl. 4), 245–254 (1997) 
9. Rheinberger, H.-J.: Gene Concepts. Fragments from the Perspective of Molecular Biology. 
In: Beurton, P., Falk, R., Rheinberger, H.-J. (eds.) The Concept of the Gene in Develop-
ment and Evolution, pp. 219–239. Cambridge University Press, Cambridge (2000) 
10. Zadeh, L.A.: Autobiographical Note, undated type-written manuscript (after 1978) 
11. Seising, R.: Interview with L. A. Zadeh, UC Berkeley, Soda Hall, see [14] (July 26, 2000) 
12. Zadeh, L.A.: My Life and Work – A Retrospective View. Applied and Computational Ma-
thematics 10(1), 4–9 (2011) 
13. Zadeh, L.A.: Fuzzy Sets. Information and Control 8, 338–353 (1965) 
14. Seising, R.: The Fuzzification of Systems. The Genesis of Fuzzy Set Theory and Its Initial 
Applications – Developments up to the 1970s. STUDFUZZ, vol. 216. Springer, Heidel-
berg (2007) 

 
Fuzziness and Fuzzy Concepts and Jean Piaget’s Genetic Epistemology 
565 
15. Zadeh, L.A.: Fuzzy Languages and their Relation to Human and Machine Intelligence. In: 
Marois, M. (ed.) Man and Computer. Proc. of the First International Conference on Man 
and Computer, Bordeaux, June 22-26, 1970, pp. 130–165. S. Karger, Basel (1972) 
16. Goguen, J.A.: Categories of Fuzzy Sets: Applications of a Non-Cantorian Set Theory. 
Ph.D. Thesis. University of California at Berkeley (June 1968) 
17. Goguen, J.A.: The Logic of Inexact Concepts. Synthese 19, 325–373 (1969) 
18. Rosch, E.: Natural Categories. Cognitive Psychology 4, 328–350 (1973) 
19. Lakoff, G.: Hedges: A study in meaning criteria and the logic of fuzzy concepts. Journal of 
Philosophical Logic 2, 458–508 (1972) 
20. Lakoff, G.: R. S. Interview, UC Berkeley, Dwinell Hall (August 6, 2002) 
21. Piaget, J.: The Child’s Conception of Causality, transl. by M. Gabain, London (1930) 
22. Piaget, J.: Les notions de mouvement et de vitesse chez l’enfant, Paris (1946) 
23. For more information to “Nicolas Bourbaki”, http://www.bourbaki.ens.fr/ 
24. Suppes, P.: A set of independent axioms for extensive quantities (1951), 
http://suppescorpus.stanford.edu/browse.html?c=mpm&d=1950 
25. Suppes, P.: Some remarks on problems and methods in the philosophy of science (1954), 
http://suppescorpus.stanford.edu/browse.html?c=mpm&d=1950 
26. Suppes, P.: Introduction to Logic. Van Nostrand, New York (1957) 
27. Sneed, J.D.: The Logical Structure of Mathematical Physics. Reidel, Dordrecht (1971) 
28. Balzer, W., Moulines, C.U., Sneed, J.D.: An Architectonic for Science. The Structuralist 
Program. Reidel, Dordrecht (1987) 
29. Sadegh-Zadeh, K.: Handbook of Analytical Philosophy of Medicine, Dordrecht (2012) 
30. Seising, R.: Fuzzy Sets and Systems and Philosophy of Science. In: Seising, R. (ed.) 
Views on Fuzzy Sets and Systems. STUDFUZZ, vol. 243, pp. 1–35. Springer, Heidelberg 
(2009) 
31. Seising, R.: A “Goodbye to the Aristotelian Weltanschauung” and a Handbook of Analyti-
cal Philosophy of Medicine. In: Seising, R., Tabacchi, M. (eds.) Fuzziness and Medicine. 
STUDFUZZ, vol. 302, pp. 19–76. Springer, Heidelberg (2013) 
32. Wadsworth, B.: Piaget’s Theory of Cognitive Development: An Introduction for Students 
of Psychology and Education. David McKay & Comp., NY (1971) 
33. MacIsaac, D.: The Pedagogical Implications of Parallels between Kuhn’s Philosophy of 
Science and Piagets’ Model of Cognitive Development, 
http://physicsed.buffalostate.edu/danowner/kuhnpiaget/KP1.html 
34. Piaget, J., Garcia, R.: Psychogenesis and the history of science (H. Fieder, Trans.). Colum-
bia University Press, New York (1988) (Original published in 1983) 
35. Wadsworth, B.: Piaget for the classroom teacher. Longman, New York (1978) 
36. Santrock, W.: A Topical Approach To Life-Span Development, NY, pp. 211–216 (2008) 
37. Loftus, G., et al.: Introduction to Psychology, 15th edn. Cengage, London (2009) 
38. Santrock, J.W.: Life-Span Development, 9th edn. McGraw-Hill College, Boston (2004) 
 
 

A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 566–575, 2014. 
© Springer International Publishing Switzerland 2014 
Paired Structures in Logical and Semiotic Models  
of Natural Language 
J. Tinguaro Rodríguez1, Camilo Franco De Los Ríos2, Javier Montero1, and Jie Lu3 
1 Faculty of Mathematics, Complutense University of Madrid, Madrid, Spain 
{jtrodrig,javier_montero}@mat.ucm.es 
2 Department of Food and Resource Economics, Faculty of Science,  
University of Copenhagen, Frederiksberg, Denmark 
cf@ifro.ku.dk 
3 Faculty of Engineering and Information Technology,  
University of Technology, Sydney, Australia 
jie.lu@uts.edu.au 
Abstract. The evidence coming from cognitive psychology and linguistics 
shows that pairs of reference concepts (as e.g. good/bad, tall/short, nice/ugly, 
etc.) play a crucial role in the way we everyday use and understand natural lan-
guages in order to analyze reality and make decisions. Different situations and 
problems require different pairs of landmark concepts, since they provide the 
referential semantics in which the available information is understood accor-
dingly to our goals in each context. In this way, a semantic valuation structure 
or system emerges from a pair of reference concepts and the way they oppose 
each other. Such structures allow representing the logic of new concepts ac-
cording to the semantics of the references. We will refer to these semantic valu-
ation structures as paired structures. Our point is that the semantic features of a 
paired structure could essentially depend on the semantic relationships holding 
between the pair of reference concepts from which the valuation structure 
emerges. Different relationships may enable the representation of different 
types of neutrality, understood here as an epistemic hesitation regarding the ref-
erences. However, the standard approach to natural languages through logical 
models usually assumes that reference concepts are just each other complement. 
In this paper, we informally discuss more deeply about these issues, claiming in 
a positional manner that an adequate logical study and representation of the fea-
tures and complexity of natural languages requires to consider more general 
semantic relationships between references.            
Keywords: knowledge representation, natural languages, logic, fuzzy sets.  
1 
Introduction 
In many situations of our everyday life, whenever we analyze the available informa-
tion in order to understand a certain reality and make decisions in view of our know-
ledge, we elect and use pairs of references to assess such information and the available 
options in adequate terms. In this way, for instance, the information we collect may be 

 
Paired Structures in Logical and Semiotic Models of Natural Language 
567 
relevant or irrelevant in terms of our knowledge; the possible decisions we can make 
are usually regarded in terms of their appropriateness or inappropriateness in order to 
achieve our goals; and the possible scenarios that can arise as a consequence of our 
decisions may be judged in terms of whether they are positive or negative for our 
interests. Or one can think about whether some previous actions have been good or 
bad in order to act in a better way next time. Or whether a person is being honest or 
fallacious in its words, or whether a situation is fair or unjust for a person or a com-
munity. Or whether some music, movies or news makes you feel happy or sad. And 
many different things, or even the same in different contexts, can be judged in terms 
of different pairs as cold/warm, night/day, dry/raining, fast/slow, black/white, 
cheap/expensive, big/small, sure/impossible victory/defeat, success/failure, entertain-
ing/boring, legal/illegal, safe/dangerous, quiet/loud, tall/short, rich/poor, nice/ugly, 
coward/temerarious, smart/dumb, weak/strong, young/old, love/hate, life/death and so 
on.  
Thus, different problems or situations, related to different contexts, objects of study 
and pieces of information, require different pairs of references providing an adequate 
contextual meaning (or semantics) to objects and information. That is, the concepts in 
a pair provide the semantic references in terms of which objects and information are 
assessed regarding a specific problem or context. In this way, for instance we can 
regard a day or a particular moment of the day as e.g. definitely cold, or slightly warm, 
or not-warm at all. Similarly, regarding its speed, a car could be e.g. very fast, or 
quite slow, while simultaneously, in relation to its price, it could be e.g. neither cheap 
nor expensive, or both not-cheap and not too expensive. And though we can judge a 
person in terms of tall/short, perhaps a specific problem, e.g. choosing a peer to climb 
a dangerous mountain, would rather require judging persons in terms of co-
ward/temerarious, or weak/strong. These instances show how a pair of concepts pro-
vides a referential context for assessing objects and information accordingly to the 
semantics of the concepts in the pair. Moreover, an object or piece of information can 
be simultaneously assessed through different pairs, each referring to a different cha-
racteristic or criterion, whenever an object possesses different features to be analyzed. 
In this sense, a specific problem or situation could require to elect a particular pair (or 
set of pairs) of references among all those pairs that may be applied to a certain ob-
ject, since some of these references may define the adequate semantic context for a 
given problem, while some others may not provide any relevant information for the 
current analysis.   
It is important to note that the two concepts making up any of the mentioned pairs 
are related in a very general sense: both refer to the same underlying characteristic or 
criterion, but at the same time they represent opposite references, acting on the same 
information while simultaneously defining somehow extreme and antagonistic se-
mantic landmarks for the evaluation of such criterion. That is, it is easy to devise a 
situation in which objects are assessed through e.g. the pairs young/old and smart/not-
smart (for instance, the evaluation of candidates for a post-graduate research grant), 
but it is rather weird (if not impossible) to analyze objects in terms of e.g. the pairs 
young/day and/or smart/raining. Somehow, these last pairs cannot serve for the pur-
pose of reference since they are made up of concepts that hardly refer to the same 
characteristic (though some poets can made surprising associations). Similarly, note 

568 
J.T. Rodríguez et al. 
that e.g. the pairs old/very old or smart/very smart does not provide adequate refer-
ences for assessing age or intelligence in a general way, and could only provide mea-
ningful landmarks once they are considered as extreme, somehow opposite references 
on a constrained subset of the range of these characteristics. 
Another important remark concerns the linguistic character of the references as 
well as of the assessments to be made in terms of the former. That is, these pairs are 
all made of terms or concepts of a natural language, providing a referential semantics 
in terms of which further linguistic evaluations (i.e. further terms or concepts) can be 
obtained. In other words, it is the semantics of the references and the relationships 
between them and the new concepts what provide the semantics (i.e. the meaning) of 
the new concepts. Thus, pairs of reference concepts provide the semantics for a lin-
guistic evaluation of reality [13]. Furthermore, note that even when we explicitly take 
only a single concept as reference, e.g. let us say tall, allowing us to express degrees 
of verification as certainly tall, quite tall or little tall, we are implicitly (even perhaps 
unconsciously) taking the lack of verification of such reference, i.e. not-tall, as a 
second reference.  
2 
Logical Models of Language 
On the other hand, different formal tools exist for the representation and the treatment 
of natural language and the semantics of its concepts (see for instance [14], [21], 
[30]). Let us focus on logical models of language. Logic, through set theory, provide a 
classical representation of the semantics of a concept (or predicate) P in terms of the 
collection or set of objects x of a universe of discourse X (i.e. the whole collection of 
objects under consideration) for which the assertion “the object x is (or verifies) P”, 
denoted as P(x), is true. In formal terms, using (classical) logic we can represent (by 
extension) the semantics of P as the set 
 
{
|
( )}
P
x
X P x
=
∈
. 
(1) 
This representation is equivalent to that provided (by assignation) through an indica-
tor function 1 :
{0,1}
P
X →
 such that for any object x
X
∈
, it is 1 ( )
P x  = 1 if and 
only if P(x) holds, i.e. is true. And note that in this classical model the semantics of 
the predicate not-P can be obtained from that of P, since it is 1
( )
not P x
−
 = 1 if and 
only if 1 ( )
P x  = 0 if and only if P(x) does not hold, i.e. is false. 
In this sense, classical logic can be regarded as a formal discourse built upon the 
semantics of the pair true/false, in fact a discourse based upon a particular semantics 
of this pair of notions, in which both of them are regarded as the only possible evalua-
tions, being these each other complements. And note that through this discourse we 
can model and describe the semantics of other concepts, in such a way that particular-
ly we can identify precise, crisp concepts to those concepts having its semantics de-
fined through classical logic (e.g. all the usual mathematical notions).  
In this way, fuzzy logic and fuzzy set theory [30], as a generalization of classical 
logic and set theory, provides a discourse based upon the same pair of references 
true/false, but now allowing these notions to be a matter of degree, i.e. notions with 

 
Paired Structures in Logical and Semiotic Models of Natural Language 
569 
an imprecise semantics, and therefore allowing more than just two opposite, extreme 
evaluations 0 and 1 in order to estimate the veracity of a predicate P. Let us recall 
that, in the context of fuzzy logic, the indicator function 1P  is replaced by a member-
ship function 
:
[0,1]
P
X
μ
→
, in such a way that 
( )
[0,1]
P x
μ
∈
 represents the degree 
(or the intensity) up to which the object x verifies P, i.e. up to which P(x) is true. Note 
also that fuzzy logic still considers true and false as complementary notions, since any 
degree of verification or truth 
( )
P x
μ
 is always associated with an inverse degree of 
lack of verification or falsehood 
( )
(
( ))
not P
P
x
n
x
μ
μ
−
=
, obtained from the former 
through a negation 
:[0,1]
[0,1]
n
→
, usually defined by 
( )
1
n v
v
= −
 for any 
[0,1]
v∈
. Thus, in consonance with its status of generalization of classical logic, note 
again that fuzzy logic provides a formal tool through which we can model and de-
scribe the semantics of imprecise predicates, as many of the concepts we use in our 
everyday life, i.e. the mathematically ill-defined concepts of a natural language. 
Then, our first observation is that, though the modeling of the semantics of con-
cepts of a natural language through classical and fuzzy logic is a quite extended prac-
tice (see for instance [31]), much less attention has been paid through these tools to 
the correlative semantics that a pair of concepts should maintain in order to be re-
garded as appropriate references for linguistic evaluation (see [24]). And even less 
attention (if actually any) has received the possibility that the features of the semantic 
evaluation structure enabled by a pair of references (i.e. emerging, see [3,4], from 
these references) could depend in a drastic manner on the semantic relationship hold-
ing between the references (and see [15]). That is, our point is that different semantic 
relationships between the concepts making up a pair of references could determine 
semantically-different structures for the evaluation and the interpretation of reality in 
terms of a natural language.  
To some extent, this previous observation should not be surprising, since classical 
logic has somehow introduced the bias of thinking in terms of complementary no-
tions, in order to analyze the world logically (see [6], [1], [28,29]). This bias has also 
profound philosophical roots in the Western cultural tradition, historically attracted by 
dualism, in the form of pairs of notions regarded as each other complements, as e.g. 
being/not-being, material/immaterial, body/mind (or body/soul), God/Devil, etc.   
Particularly, fuzzy representations suffer from this bias, since, as discussed above, 
we can only consider degrees of verification of a concept P when we use the implicit 
complementary reference not-P. In this sense, as also pointed above, a concept P and 
its complement not-P maintain a very specific semantic relationship in terms of being 
opposite and extreme notions, allowing us to construct a pair of references from 
which semantically analyze reality in terms of just the semantics of a single concept 
P. And observe that, emerging from such complementarity relationship between a 
concept P and its negation or complement not-P, enabling to form the reference pair 
P/not-P, we in fact obtain a semantic structure for evaluating a certain reality (e.g. the 
height of a person): particularly, the semantic structure of precise, binary notions 
allowed by classical logic (it is either 1 ≡ tall or 0 ≡ not-tall), or, in a more general 
framework, the semantic structure of imprecise, gradable concepts allowed by grada-
tion procedures and particularly fuzzy logic (it could then be e.g. 1 ≡ certainly tall, 

570 
J.T. Rodríguez et al. 
0.75 ≡ quite tall, 0.5 ≡ half tall, 0.25 ≡ not so tall and 0 ≡ not tall at all). In fact, we 
can regard fuzzy logic, in its usual formulation, as the study of the class of gradable 
and imprecise semantics representable through the assumption of implicit comple-
mentary references. 
3 
Paired Concepts and Structures 
Therefore, the semantic evaluation structure arising from a pair of references has been 
widely studied by means of logical tools, but mainly under the assumption that paired 
references are each other complements, due to the binary bias long time ago attached 
to our scientific conception of reality. For this reason, little attention has been paid to 
the possibility (indeed the key idea of this paper) that different semantic relationships 
between the two concepts in a reference pair could in fact define essentially different 
semantic structures for a linguistic evaluation of reality. 
Let us stress that two natural language concepts P, Q making up a reference pair 
P/Q could be far away from being regarded as each other complements, i.e. we could 
understand them in such a way that P ≠ not-Q and Q ≠ not-P. For instance, a 
person not being tall could be also not-short, and a not-fast car is not necessarily slow, 
in the same manner as it could also be simultaneously not-cheap and not-expensive. 
Or, e.g. a not-happy song could be far away from being sad (people can even sing sad 
songs and feel happy for that, or reciprocally). And a day could be forecasted as hav-
ing both cold and warm moments, making us hesitate about the adequate clothes to 
wear. Also, a certain thing could be not-good and at the same time not-bad, and even 
simultaneously good and bad: in fact, the evidence coming from neuroscience [5] 
points out that our brain has different channels to process negative emotions as pain, 
sadness or loneliness than to process positive feelings as happiness, joy or euphoria. 
And precisely for this reason, because they are evaluated separately, positive and 
negative emotions are not each other complement, but rather they harvest independent 
families of arguments, and thus both can appear together (conforming different 
mixed, conflictive or contradictory emotions as thrill or tornness) or not appear at all 
(producing neutral, flat emotions as indifference or serenity). In fact, every day we 
find that many things can have simultaneous good and bad sides, and in certain re-
gions of the world this is indeed a form of ancestral wisdom (e.g. Taoism and its 
yin/yang principle).  
These ideas suggest that the semantic structure arising from a pair of non-
complementary references may in fact enable the emergence of new linguistic con-
cepts as categories for valuation, exhibiting a somehow neutral or mixed character (or 
semantics) with respect to the landmark concepts, and which cannot be properly re-
garded as degrees of verification of a single reference concept correlative to its com-
plement. For instance, when somebody is neither coward nor temerarious we could 
rely on or define an intermediate category as courageous, with a proper semantics of 
its own, to refer to such neutral assessment in terms of the considered reference pair 
(and note that such neutrality could represent the best alternative for a given problem, 
as in the mountain-climbing example). Similarly, the hesitation we feel when we find 

 
Paired Structures in Logical and Semiotic Models of Natural Language 
571 
something being simultaneously good and bad (e.g. the pleasure of smoking even 
knowing it makes you ill), or both desirable and unattainable (e.g. a nice, fast car too 
expensive to afford it) is hardly interpretable as a degree expressed in terms of a refer-
ence and its complement (i.e. in terms of the semantics of a single concept). Rather, it 
provides a differentiated semantics, better fitting to the idea of a new concept availa-
ble for valuation, emerging as a consequence of considering non-complementary ref-
erences.  
Thus, the second key idea we propose is that different types of neutrality can arise 
in a semantic structure as a consequence of the different relationships holding be-
tween the pair of references. That is, non-complementary references could define new 
concepts or categories with a differentiated semantics with respect to the references, 
not showing the particular semantics of degrees either. Particularly, these neutral cat-
egories could not allow an ordering, or not refer to the same notion of order as de-
grees (and see [19] for an instance in paraconsistent logics). In other words, there 
could be many ways of being in between a pair of references (see [15]). 
Let us insist that apparently complementary notions could be in fact regarded as 
non-complementary and define new categories (rather than degrees) for valuation. For 
instance, the references true and false, even in a rigorous mathematical setting, can be 
understood as allowing a neutral category, let us call it undecidability, applicable to 
those statements, as e.g. the twin prime or Hardy–Littlewood conjectures, for which 
either a proof or a counterexample has been not yet found. In this sense, if mathemati-
cal truth is identified with provability from a set of axioms, the Second Gödel’s Theo-
rem [10] could be interpreted as stating that for certain propositions P (e.g. the logical 
consistency of arithmetic), both P and not-P are (and will forever be) undecidable 
once we choose a set of axioms to fundament our logical system. 
However, let us also note that choosing two antonym words (e.g. tall/short) as ref-
erences does not necessarily entail considering them as non-complementary concepts. 
For instance, in a binary setting we could understand that 1 ≡ tall ≡ not-short and  
0 ≡ short ≡ not-tall, and even allow a linear or fuzzy gradation between these refer-
ences, which are in this way regarded as each other complements. And conversely, in 
practice we could use the terms P and not-P to refer to non-complementary refer-
ences, e.g. when there is not an antonym word for P [23]. For instance, there is not an 
antonym word for yellow, so we could be forced to use the term not-yellow to refer to 
an opposite idea or concept to yellow, even when we understand that a color not being 
yellow, as e.g. white, light green or beige, could not fulfill such (artificially defined) 
opposite notion of not-yellow, instead verified by other colors as e.g. blue, black or 
violet. Thus, it is the semantics of the reference concepts, and not the labels or words 
that represent them, what determines the semantic features of the linguistic valuation 
structure arising from such a pair of references. And, in this sense, perhaps a more 
general notion of opposition than antonymy, let us call it duality or antagonism, 
should be devised to represent in a general way the semantic relationships holding 
between pairs of references. Therefore, in principle there could be duality relation-
ships different from antonymy and complementation, and again, our basic observation 
is that different duality relationships between reference concepts could determine 
different semantic structures for valuation.   

572 
J.T. Rodríguez et al. 
A final remark concerns the notions of decomposable and primitive references. 
Many (if not all) concepts can be explained in terms of other concepts, as we usually 
do to explain the meaning of a word of our mother language to a child or a non-native 
speaker, or as it is done in a dictionary. In this sense, many reference pairs could be 
decomposable, in the sense of being high-level references explainable in terms of, or 
obtainable from, a set of lower-level references, then in charge of defining the seman-
tic features of the high-level valuation structure (possibly through a hierarchy of con-
cepts, see [25]) But, as discussed for instance in [20], by starting from explaining a 
single concept in terms of other concepts and successively, recursively explaining 
these lower-level concepts in terms of further concepts, we would end obtaining the 
whole language in the process, in such a way that a circular reference (returning to the 
starting concept) is obtained sooner or later. That is, there are not lowest-level con-
cepts, i.e. absolute linguistic references (but note that some basic physiological emo-
tions could in fact provide absolute pre-linguistic semantic references, as suggested in 
[9], [17]).  
To some extent, at this respect language is similar to the Universe: it constitutes the 
whole system in which we (our minds) move, in which no absolute references can be 
laid down as it has not a center or starting point. For this reason, in any problem or 
situation we face, at some point we have to regard some concepts as primitive refer-
ences, i.e. not admitting decomposition in terms of further references and thus provid-
ing the basic, primitive semantics for building the meaning of other concepts. As 
mentioned before, that is in fact the essence of reference pairs, and the reason why we 
organize our thinking in such terms. Moreover, this is also related to the way we learn 
a language when we are children [9], [17]: from a small set of linguistic references 
(e.g. mummy/daddy, or even mummy/not-mummy) learned through both mimicry and 
the basic, non-verbal semantics of physiological emotions, we are able to successively 
add further levels of concepts into our vocabulary by relating the semantics of the 
new concepts to previous semantic references, in a dynamical, always-changing 
(learning) process. And to some extent, this is also the reason why mathematics and 
logic have to be built from primitive axioms, the truth of which is supposed in order 
to develop further theories (and see [22] for a discussion on how axioms can be re-
lated to the empirical knowledge the human kind has obtained throughout its cultural, 
linguistic evolution).   
Thus, in a few words, this work proposes, in a positional manner, to join together 
the three following ideas: 
1. Pairs of references, and the semantic structures arising from them, constitute a key 
aspect of the manner we use, organize and understand a natural language in order 
to evaluate reality and make decisions. 
2. The semantics of concepts, and even the semantic relationships between reference 
concepts, can be modeled through logical reasoning tools. 
3. The semantic features of the linguistic evaluation structure arising from a pair of 
references (particularly the types of neutrality being allowed) could depend on the 
semantic relationships holding between these references. 

 
Paired Structures in Logical and Semiotic Models of Natural Language 
573 
Then, departing from these ideas, our objective will be to study the semantic struc-
tures arising from pairs of reference concepts by means of logical and semantic tools, 
and particularly focusing on how the expressive (or representative) power of these 
structures could in turn depend on how the references are semantically related. For 
simplicity, let us refer to these semantic structures as paired structures. Particularly, 
let us remark that our position/proposal is not about formal logic or its interpretation, 
but rather it deals with knowledge and natural language representation by means of 
logical tools, under the assumption that the semantic relationships between references 
may determine the expressive power of our representations. 
Therefore, paired structures could constitute a relevant step towards a more general 
and useful model of language, allowing for several kinds of neutrality and linguistic 
uncertainty to be addressed and represented. In the long term, our vision aims a more 
general and ambitious objective: enabling us to speak in logical terms (but perhaps 
through a discourse not developed in terms of complementary references) of those 
things about which the first Wittgenstein [26] thought we should stay in silence, but 
about which the second Wittgenstein [27] referred to as being “the really important 
things” [7]: our everyday feelings, emotions and intentions. 
4 
Final Remarks 
As a preliminary and positional work, this paper has focused on presenting our argu-
ment and its potential relevance, rather than on providing a strictly formal approach 
addressing all possible technical difficulties. A more formal paper will follow, fo-
cused on addressing these aspects in more rigorous terms, although some first steps 
have been already presented in [18].  
Let us anticipate that in a general sense, besides containing the references them-
selves, paired structures have to be regarded as valuation structures providing a finite 
(possibly void) set of interrelated categories (with a differentiated semantics) as pri-
mary values, together with appropriate scales in which a secondary value, 
representing the correlative verification status of each primary category, is measured. 
In this sense, as each neutral category allowed in a paired structure can be associated 
with a different type of neutrality –in turn associated to a different kind of epistemic 
hesitation provoking difficulties to understand information, decide or act–, paired 
structures may provide a general framework in which several features of our everyday 
reasoning process (as e.g. different types of uncertainty and hesitation) can be 
represented through logical terms. 
Then, in practice, paired structures may be used and applied in two complementary 
ways: either a) the allowed categories emerging from the considered references and 
their verification degrees are estimated directly from the objects; or b) the relevant 
categories and their verification are obtained through aggregation from more primi-
tive information, as the verification degrees of the references or even from those of 
the concepts in lower-level structures. In either way, however, some semantic con-
straints should be imposed (on either the estimations or the aggregation operators) in 
order to relate the different categories and their verification accordingly to the seman-
tics they are intended to exhibit. Thus, different types of scales or valuation spaces 

574 
J.T. Rodríguez et al. 
(from univariate lattices to complex multidimensional polyhedrons) may be obtained 
depending on the semantic assumptions of the model. These two complementary me-
thodologies should then provide a manner of empirically testing whether these seman-
tic assumptions are appropriate or not in a given context and situation.   
Another issue that will be addressed in forthcoming works is the relationship be-
tween paired structures and the existing models for knowledge and language represen-
tation. We conjecture that many of the existing models and approaches (as e.g. type-n 
fuzzy sets [31], computing with words [32], [11], bipolarity [8], intuitionistic fuzzy 
sets [1], probability [12], etc.) can be understood and represented as special kinds of 
paired structures. In this way, paired structures would then provide a general, unifying 
notion allowing a deeper understanding of the relationships holding between many of 
the formalisms commonly used in knowledge representation.      
 
Acknowledgements. This research has been partially supported by the Government 
of Spain, grant TIN2012-32482. 
References 
1. Atanassov, K.T.: Intuitionistic Fuzzy-Sets. Fuzzy Sets and Systems 20(1), 87–96 (1986) 
2. Barnes, B.: On the conventional character of knowledge and cognition. Philosophy of the 
Social Sciences 11(3), 303–333 (1981) 
3. Bunge, M.: Emergence and the mind. Neuroscience 2(4), 501–509 (1977) 
4. Bunge, M.: Emergence and convergence: Qualitative novelty and the unity of knowledge. 
University of Toronto Press (2003) 
5. Cacioppo, J.T., Berntson, G.G.: The affect system, architecture and operating characteris-
tics - Current directions. Psycological Science 8, 133–137 (1999) 
6. Collins, R.: The sociology of philosophies: A global theory of intellectual change. Harvard 
University Press (1998) 
7. Doxiádis, A.K.: Logicomix. Bloomsbury Publishing (2009) 
8. Dubois, D., Prade, H.: An introduction to bipolar representations of information and prefe-
rence. International Journal of Intelligent Systems 23(8), 866–877 (2008) 
9. Edelman, G.M.: Bright air, brilliant fire: On the matter of the mind. The Penguin Press, 
London (1992) 
10. Gödel, K.: On Formally Undecidable Propositions Of Principia Mathematica And Related 
Systems. Monatshefte für Mathematik und Physik 38, 173–198 (1931) (in German) 
11. Herrera, F., Martínez, L.: A 2-tuple fuzzy linguistic representation model for computing 
with words. IEEE Transactions on Fuzzy Systems 8(6), 746–752 (2000) 
12. Kolmogorov, A.: Foundations of the Theory of Probability. Julius Springer, Berlin (1933) 
(in German) 
13. Lindsay, P.H., Norman, D.A.: Human information processing: An introduction to psychol-
ogy. Academic Press, New York (1972) 
14. Manning, C.D., Schütze, H.: Foundations of statistical natural language processing, 
vol. 999. MIT Press, Cambridge (1999) 
15. Montero, J., Gómez, D., Bustince, H.: On the relevance of some families of fuzzy sets. 
Fuzzy Sets and Systems 158(22), 2429–2442 (2007) 
16. Osgood, C.E., Suci, G.J., Tannenbaum, P.H.: The measurement of meaning. University of 
Illinois Press, Urbana (1957) 

 
Paired Structures in Logical and Semiotic Models of Natural Language 
575 
17. Piaget, J.: The language and thought of the child, vol. 5. Psychology Press (1959) 
18. Rodríguez, J.T., Franco, C.A., Montero, J.: On the semantics of bipolarity and fuzziness. 
In: Melo-Pinto, P., Couto, P., Serôdio, C., Fodor, J., De Baets, B., et al. (eds.) Eurofuse 
2011. AISC, vol. 107, pp. 193–205. Springer, Heidelberg (2011) 
19. Rodríguez, J.T., Turunen, E., Ruan, D., Montero, J.: Another paraconsistent algebraic se-
mantics for Lukasiewicz–Pavelka logic. Fuzzy Sets and Systems (June 25, 2013), 
http://dx.doi.org/10.1016/j.fss.2013.06.011 ISSN 0165-0114 
20. Rosch, E.: Linguistic relativity. In: Silverstein (ed.) Human Communication: Theoretical 
Perspectives (1974) 
21. Schank, R.C.: Conceptual dependency: A theory of natural language understanding. Cog-
nitive Psychology 3(4), 552–631 (1972) 
22. Searle, J.R.: Mind, language and society: Philosophy in the real world, vol. 157. Basic 
books, New York (1999) 
23. de Soto, A.R., Trillas, E.: On antonym and negate in fuzzy logic. International Journal of 
Intelligent Systems 14(3), 295–303 (1999) 
24. Trillas, E., et al.: Computing with antonyms. In: Nikravesh, M., Kacprzyk, J., Zadeh, L.A. 
(eds.) Forging New Frontiers: Fuzzy Pioneers I. STUDFUZZ, vol. 217, pp. 133–153. 
Springer, Heidelberg (2007) 
25. Willie, R.: Concept lattice and conceptual knowledge systems. Computers and Mathemat-
ics with Applications 23, 493–515 (1992) 
26. Wittgenstein, L.: Tractatus logico-philosophicus. Routledge & Kegan Paul, Rotterdam 
(1921) 
27. Wittgenstein, L.: Philosophical investigations (1953) 
28. Woolgar, S.: Configuring The User: the case of usability trials. In: Law, J. (ed.) A Sociol-
ogy of Monsters: Essays on Power, Technology and Domination. Routledge, London 
(1991) 
29. Woolgar, S., Hamilton, P.: Science, the very idea. Ellis Horwood, Chichester (1988) 
30. Zadeh, L.A.: Fuzzy sets. Information and control 8(3), 338–353 (1965) 
31. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate reason-
ing—I. Information sciences 8(3), 199–249 (1975) 
32. Zadeh, L.A.: Fuzzy logic=computing with words. IEEE Transactions on Fuzzy Sys-
tems 4(2), 103–111 (1996) 
 
 

A Fuzzy Rule-Based Haptic Perception Model
for Automotive Vibrotactile Display⋆
Liviu-Cristian Duţu1, Gilles Mauris1, Philippe Bolon1,
Stéphanie Dabic2, and Jean-Marc Tissot2
1 Univ. Savoie, LISTIC, F-74000 Annecy, France
{liviu-cristian.dutu,gilles.mauris,philippe.bolon}@univ-savoie.fr
2 Valeo Interior Controls, Annemasse, France
{stephanie.dabic,jean-marc.tissot}@valeo.com
Abstract. Currently, tactile surfaces implemented in automobiles are
passive, i.e., feedbackless, thus forcing the user to visually check the de-
vice. To improve drivers’ interaction, surface vibrations can be used to
deliver feedback to the ﬁnger when touched, and an associated percep-
tion model is required. Hence, this paper introduces a fuzzy model for
the comfort degree of vibrotactile signals. System input variables are
chosen from the physical characteristics of the signals, and are validated
on a dissimilarity judgment task. The system achieves an error of 9%
and correctly classiﬁes 17 out of 18 signals within a reasonable interval.
A graphical user interface to interact with the system is also presented.
Keywords: haptic interfaces, sensory evaluation, fuzzy system, wavelets.
1
Introduction
Mechanical interfaces installed in car cockpits are evolving and are currently
being replaced by tactile surfaces. Many of the functionalities of an automobile,
such as choosing a radio channel or controlling the CD player, have already
migrated on tactile surfaces, and it is expected that many others will soon be
migrated. This change forced car manufacturers to investigate the impact of this
new technology on passengers and on the security in a driving situation.
Its important deﬁciency is that it relies only on the visual sense for feedback,
forcing the driver to visually check the device after an interaction, thus distract-
ing the attention away from the road and increasing the risk of car accident [1].
To counterbalance this side eﬀect, feedback might be delivered directly to the
tactile sense using vibrations or vibrotactile signals. This is consistent with the
Multiple Resources Theory [2], stating that humans are able to process informa-
tion from diﬀerent senses without any signiﬁcant deterioration in the cognitive
performance (e.g., drive a car and listen to the radio). Thus, activating the tac-
tile sense through vibrotactile signals as a way to deliver the feedback of touch
interfaces, will allow the driver to focus his/her visual resources on the road.
⋆Developed under the FUI-MISAC project, approved by the French Government with
the contract number F-11-06-048-V.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 576–585, 2014.
c
⃝Springer International Publishing Switzerland 2014

A Fuzzy Rule-Based Haptic Perception Model
577
This calls for a deep understanding of the human tactile sense and its inter-
actions with the vibrotactile signal’s physical characteristics. In order to inves-
tigate these interactions, we propose to deﬁne a perception model associating
users’ subjective evaluations to measured properties of the vibrotactile signals.
Section 2 describes the device and the experiments conducted to collect users
subjective evaluations. Using these evaluations as ground truth, we conﬁrmed
the existence of a link between users’ perception and the measured physical
characteristics of the signals. Section 3 then proposes an intuitive 2-variable
fuzzy model associating the characteristics of the acceleration and velocity of
the vibrating tactile surface to the perceived comfort degree. The model is cus-
tomizable through a graphical user interface.
2
Signal Characteristics and Perceptual Distance
2.1
Apparatus and Psychophysical Procedures
Two experiments were ﬁrst conducted to collect users evaluations of vibrotactile
signals. The apparatus used to produce the signals was developed by Valeo An-
nemasse France. It includes a voice coil actuator encapsulated under a resistive
screen layer which the users can touch to perceive the vibrotactile signals. They
are produced by the moving actuator receiving electrical stimuli with varying fre-
quency, duration and waveform parameters, while the amplitude was constant.
To ﬁlter out the low frequency mechanical noise produced by the device while
vibrating, participants wear a pair of noise-canceling headphones.
The ﬁrst experiment consists in evaluating the perceived dissimilarity between
18 vibrotactile signals. Their corresponding electrical stimuli diﬀer in terms of
frequency, duration and waveform. Using the ﬁngertip, 24 subjects evaluated the
dissimilarity between each pair of signals on a 1 to 7 Likert scale, where 1 was
equivalent to the signals are very similar and 7 to the signals are very diﬀerent.
The order within a pair and the pairs occurrence order was counterbalanced
between subjects. A complete description of the experiment is available in [3].
An 18 × 18 perceptual dissimilarity matrix DP was created in [3] based on the
values collected for each pair of signals. Later, DP will be used as ground truth
to ﬁnd the variables accounting for signals dissimilarity.
The second experiment, conducted on the same set of signals, aimed at obtain-
ing an absolute appreciation degree (i.e., comfort) for each vibrotactile signal.
Ten subjects rated the appreciation of each signal, presented in a random order,
on a 1 to 7 Likert scale with 1 being the lowest appreciation degree and 7 the
highest. These values are used in Section 3 to deﬁne a fuzzy model of perception.
Due to the multiple layers between the actuator and the tactile surface, electri-
cal stimuli might be altered before they reach the tactile layer. Thus, we consider
that they can not fully account for human perception, and we have focused on
the acceleration and displacement data measured on the device output layer,
where users perceive the signals. To extract a pair of variables from these data,
we ﬁrst need to investigate the psychophysical properties of the tactile sense and
its relationship to the vibrational signals’ physical characteristics.

578
L.-C. Duţu et al.
2.2
Psychophysical Overview of the Tactile Sense
According to the four channels theory [4], the tactile sense is mediated by four
psychophysical channels: the Pacinian (P) channel and three Non-Pacinian chan-
nels (NP I, II, III). Each channel is activated by a duo of vibration frequency
and contact area, but with the current experimental setup, the NP II and NP
III channels are unreachable. Thus, we can assume that the tactile sensations
elicited by the signals used in section 2.1 mainly originate from the activation of
the P and NP I channels. Next, we will focus on these channels to ﬁnd the vari-
ables that simulate their neural activation, based on their inherent properties.
The Pacinian channel is the most sensitive among the four and operates in
range [40 Hz, 800 Hz] [4], with a sensitivity peak around 200Hz. It is the only
channel to exhibit temporal summation capabilities [6] and according to [5] it
shows "near perfect integration of stimulus energy" over time, which is compat-
ible with the neural-integration theory of temporal summation for the auditory
sense elaborated by Zwislocki in [7]. In [8], the authors found that stimuli in the
range of the P channel are discriminable mainly based on their diﬀerences in
power, and that their waveform diﬀerences are not perceived.
These suggest that the P channel is activated by a variable akin to the energy
gathered within its frequency range in the vibrating tactile surface acceleration.
The NP I channel is responsible for the perception of tactile stimuli in the
range [10 Hz, 100 Hz] [4]. It is optimally tuned at 30-50 Hz [6]. Unlike the P
channel, it does not exhibit temporal summation. Although for the NP I channel
there seems not to be a consensus in the literature on which variables better
simulate its activation, one of the early references [9] suggests that it might be
sensitive to the dynamic stimulus’ velocity. Therefore, in the next sections we
will use the mean positive velocity of the tactile surface to reﬂect its activation.
2.3
Pacinian Channel Activation and Wavelet Analysis
To obtain the energy delivered to the P channel, we inspected the measured
acceleration signal of the surface using the time-frequency technique of the con-
tinuous wavelet transform (CWT), since both the channel frequency selectivity
and its temporal summation capacity dictate its neural activity, as shown above.
Wavelets are ﬁnite-energy functions which can be shifted across the signal and
scaled by dilation or compression. In wavelet analysis, the notion of frequency is
replaced by that of scale. The CWT coeﬃcient of f(t) using wavelet ϑ at scale
ρ and time-shift b is deﬁned below, where ϑ∗is the complex conjugate of ϑ.
Cf(ρ, b) =
∞

−∞
f(t) 1
√ρ ϑ∗
t −b
ρ
	
dt .
(1)
The matrix Cf of CWT coeﬃcients is obtained by iterating (1) for a ﬁnite
number of scales ρi and time shifts bj. It can be seen as a 2D representation of
the similarity between f(t) and the wavelet’s shifted and scaled versions.

A Fuzzy Rule-Based Haptic Perception Model
579
The appropriate way to implement the CWT is to choose a wavelet which most
resembles the signal to be analyzed. In this paper we have chosen the real-valued
Morlet wavelet (ϑ(t) = e
−t2
2 cos(5t)), but exploratory studies realized in [10]
show that relatively similar results can be obtained with diﬀerent wavelets.
As wavelets are practically band-limited, |Cf(ρ, b)|2 is the energy of f(t) at
time b and in a frequency band around 1/ρ. Generalizing, we obtain the signal
energy distribution in the time-scale plane, called scalogram [11]: Es(ρ, b) ≡
|Cf(ρ, b)|2, ∀ρ, ∀b . Fig. 1 illustrates the scalograms, using the Morlet wavelet,
of two of the most perceptually dissimilar acceleration signals. We can clearly
see the diﬀerences between their energy patterns in the time-scale plane.
To switch from the intrinsic time-scale plane of the wavelet transform to a
time-frequency plane, the scale-frequency relationship is Fσ = Fc/ργ, where Fc
is the central frequency of the wavelet (e.g., Fc = 0.81 Hz for the Morlet wavelet),
Fσ is the equivalent frequency of scale ρ, and γ is the sampling period.
Linear interpolation of |Cf|2 coeﬃcients was employed to get the scalograms
to a common duration NA. Energy is preserved by multiplying the interpolated
coeﬃcients |CI
f|2 with the ratio between the original duration and NA.
Relying on section 2.2, we set the threshold between the P and NP I channel
at 70 Hz, i.e., scale ρ = 59 for the Morlet wavelet and a sampling frequency of
5 KHz. The P channel upper limit of 800 Hz is equivalent to scale ρ = 5.
The neural activity of the P channel is deﬁned as the sum of the squared
modulus of the acceleration scalogram coeﬃcients within ρ = [5, 59] normalized
by the P channel frequency sensitivity (T (ρ)), as shown in (2).
EPi =
59

σ=5
NA

b=0
|CI
i (ρ, b)|2T (ρ)2, ∀i = 1, 2, ..., 18
(2)
with T = 1 for frequencies above 175 Hz (ρ < 23), and decays linearly towards
T = 0.5 as we reach 70 Hz (ρ = 59). Fine tuning of T (ρ) needs to be investigated.
Fig. 1. Scalograms of two vibrating surface acceleration signals for scales σ ∈[1, 126].
The visual diﬀerence between the two is compatible with the high perceptual distance
of their corresponding tactile signals as evaluated by the users in section 2.1.

580
L.-C. Duţu et al.
Based on the EPi, a scalogram dissimilarity matrix, DEn, holding the pairwise
P channel energy-related dissimilarities between the signals was deﬁned with
the same structure as the DP matrix obtained in section 2.1. The details of the
method can be found in [10], where a constant T = 1, ∀ρ, was assumed. The
mean column Pearson’s correlation between the two matrices was around 88%.
This suggests that the P channel dominates the perception for the 18 tactile
signals and that the above measure is adequate to simulate its neural activation.
2.4
Non-Pacinian I Channel Activation
As mentioned in section 2.2, the debate in the literature on which variables better
reﬂect the activation of the NP I channel is still open. One of the variables used
for this purpose in [9], and which can be acquired, is the stimulus velocity.
The velocity data were obtained through numerical diﬀerentiation of the mea-
sured displacement data of the tactile surface. With the velocity data for all 18
signals obtained, we computed the signal mean positive velocity (V +), which was
then weighted by the percentage of energy delivered in the range of the NP I
channel [10 Hz, 70 Hz], to obtain the normalized mean positive velocity (V
+).
V +
i
=
NV
t=0 vi(t)
L
;
V
+
i = V +
i
·
70Hz
f=10Hz |Xi(f)|2
800Hz
f=10Hz |Xi(f)|2 ;
∀i = 1, 2, ..., 18;
(3)
∀t such that vi(t) > 0; where NV is the length of vi(t) , L is the number of points
for which vi(t) > 0, and Xi is the Fourier Transform of the ith acceleration signal.
With the V
+
i
deﬁned, we have constructed a velocity dissimilarity matrix
DV +, where DV +(i, j) = |V
+
i −V
+
j |/(V
+
i + V
+
j ). As the DEn matrix, DV +
too, has the same structure as the perceptual matrix DP . The mean column
correlation between the two matrices was 66%, implying that a certain link
between the normalized mean positive velocity of the signals and their perception
exists. It might also indicate that the NP I channel accounted for a small fraction
of perception in the signals, being dominated by the P channel, or that velocity
alone is not enough to fully characterize its activation.
Next, we will employ the two measures derived above (EP and V
+) as input
variables for a fuzzy system used to model perception of vibrotactile signals.
3
A Fuzzy Model of Tactile Perception
Since its early days, fuzzy logic was proven to assess the complexity and volatility
of subjective information properly [13]. Sensory information, a particular type
of subjective information, deals mainly with the quality evaluations and the
perceptions elicited by an object in a human being through one or more senses.
A pattern found in perceptual data modeling is the layering of the information
on two separate levels: the objective level, i.e., the physical characteristics of
an object, and the subjective level, i.e., the perceptions induced by the object.
According to [13] fuzzy logic is particularly adapted to ﬁnd relations between the
two levels and therefore it might be suited to deﬁne a tactile perception model.

A Fuzzy Rule-Based Haptic Perception Model
581
3.1
Fuzzy Inference System (FIS) Architecture
In order to model the perceived comfort of the tactile signals, we considered
a fuzzy rule-based symbolic system, endowed with signals physical character-
istics as input variables. Our proposed system uses two input fuzzy variables
(EP and V
+), whose universes are divided into three fuzzy sets labeled LowEP ,
MediumEP , HighEP and LowV , MediumV , HighV , respectively. Each set has a
corresponding membership function (MF) μ associating it a degree of belonging
for every numerical value. To maximize both system intuitiveness and results
intelligibility, the sum of the input degrees were set to one for both variables.
μ(xEP )(LowEP ) + μ(xEP )(MediumEP ) + μ(xEP )(HighEP ) = 1, ∀xEP
μ(xV )(LowV ) + μ(xV )(MediumV ) + μ(xV )(HighV ) = 1,
∀xV
(4)
Hence, a signal is initially characterized by its crisp numerical input {EP , V
+},
computed using (2) and (3). After fuzziﬁcation, they are converted into a fuzzy
linguistic description (e.g., 0.3/LowEP + 0.7/MediumEP + 0.0/HighEP ).
The system’s rule base is initially constructed by the a priori knowledge that
the neural activity of the P channel is inversely proportional to the perceived
comfort [10]. The actual rules used are shown in Table 1 where the × marks
denote areas where none of the 18 signals physical characteristics fall in.
The system output classes correspond to a degree of appreciation for the
signals belonging to them. Thus, for enhanced intuitiveness we can associate a
label to each class (e.g., Excellent, Good, Acceptable, Intolerable).
With the set of rules from Table 1, the inference from the input space EP ×V
+
described by trapezoidal MFs, to the output classes is made using Zadeh’s com-
positional rule of inference [12]. The choice of the T-norm combination operator
and of the T-conorm projection operator is subjugated to the limitation that the
sum of the output degrees is required to equal one. This is related to human rea-
soning, since a signal described as 0.8/Excellent + 0.5/Good + 0.8/Acceptable +
0.7/Intolerable lacks intuition. In [15] it is shown that if the sum of the input
degrees equals one (eq. (4)), then, choosing the arithmetic product as combi-
nation operator, ⊤(x, y) = x ∗y, and the bounded sum as projection operator,
⊥(x, y) = min(x+y, 1), satisﬁes this limitation even for a many-to-one mapping.
Table 1. Fuzzy Symbolic Rules of Haptic Perception
HHHHH
EP
V
+
LowV
MediumV
HighV
LowEP
Class 1 (Excellent) Class 3 (Acceptable)
Class 2 (Good)
MediumEP Class 3 (Acceptable) Class 3 (Acceptable) Class 4 (Intolerable)
HighEP
Class 4 (Intolerable)
×
×

582
L.-C. Duţu et al.
Further, we need to assign a modal value mk for each output class k, and
using the height method of defuzziﬁcation [16] we retrieve the numerical value
of the predicted preference for a signal i, whose input crisp values are (EPi, V
+
i ):
yi =

k[ μ(EPi,V
+
i )(Classk)mk ]

k μ(EPi ,V +
i )(Classk)
, ∀i, k = 1, 2, 3, 4
(5)
where μ(EPi,V +
i )(Classk) is the belonging degree of signal i to the output class
k, computed by the compositional rule of inference using ⊤and ⊥deﬁned above.
3.2
FIS Performance
Though the main goal of a fuzzy symbolic system is to produce a qualitative
intelligible model of human perception, we cannot ignore its quantitative aspects
given by the system performance indicators.
The 18 signals input values (EPi, V
+
i ) are normalized to [0, 1]. Their appre-
ciation degrees (section 2.1), are averaged across subjects and standardized by
the z-score procedure to obtain a sample of mean 0 and variance 1. After that,
their variation interval is IA = [−2.1, 2]. Three thresholds UX%, X ∈{5, 10, 20},
accounting for the evaluations uncertainty, were deﬁned to assess the quality of
the predictions. Their values correspond to 5%, 10% and 20% of the length of IA.
If the absolute diﬀerence between the actual and predicted degrees of a signal is
greater than UX%, it will be labeled as misclassiﬁed w.r.t. UX%. The global error
(δ) is deﬁned in (6), where M is the number of signals, A and P the arrays of
actual and predicted comfort degrees, MaxErri the maximal error the system
can make when estimating the degree of signal i, and k is the midpoint of IA.
δ =
M
i=1 |P(i) −A(i)|
M
i=1 MaxErri
, with MaxErri =
 A(i) −min(IA), if A(i) ≥k
max(IA) −A(i), otherwise
(6)
To ﬁnd the MFs for the input variables and the modal values for the output
classes (mk) we have used δ as a performance indicator and minimized it by
simulated annealing (SA) [17]. Using the optimized set of parameters, the system
error was δ = 9%. Fig. 2(a) shows the plot of the actual comfort degrees and of
those predicted by the fuzzy system after the SA optimization, for the set of 18
signals from section 2.1. The uncertainties UX% are represented at a 1 : 1 scale.
The thick vertical lines mark misclassiﬁed signals w.r.t. a certain UX%. There
were four misclassiﬁed signals when matched against the U5% threshold, three
for the U10% threshold and only one for the U20% threshold. Fig. 2(b) shows the
SA-optimized MFs for the fuzzy variables and also the ﬁnal rule base. Due to
the equation (4), the intersection between two adjacent symbols is at 0.5.
The optimized system was then validated on a new set of 10 vibrational signals
whose electrical stimuli are sine waves with diﬀerent frequencies and durations,
produced by a novel haptic bench developed within the FUI-MISAC project.
After a post factum analysis of the second experiment from section 2.1, we
have decided to measure the appreciation degrees for this new set of signals on

A Fuzzy Rule-Based Haptic Perception Model
583
Fig. 2. (a) Actual (green) and predicted (blue) comfort degrees for the set of 18 signals.
The thick vertical lines mark discrepancies greater than the diﬀerent values of UX%;
(b) SA-tuned MFs for the fuzzy variables and the ﬁnal rule base with modal values.
a bipolar scale with only 5 levels. Thus, thirteen subjects rated the appreciation
for the 10 signals, randomly presented to them, on a −2 to +2 scale, where −2
was the lowest degree of appreciation and +2 the highest. We sustain that this
new scale leads to a higher reliability of the responses, as the number of levels is
lowered by two, and that, thanks to its bipolarity and to its 0 neutral value, it
better represents human reasoning on hedonic evaluations. That is why all our
future experiments will be carried on this bipolar, 5 levels, −2 to +2 scale.
The 10 signals input values (EPi, V
+
i ) were normalized to [0, 1], and their
appreciation degrees standardized by the z-score procedure. As these degrees
were acquired on a diﬀerent scale than the one employed for the 18 signals
used to adjust the model, we limited the validation to the Pearson’s correlation
(ρ = 88%) and the Spearman’s rank correlation (ρS = 85%) between the vectors
of predicted and actual appreciation degrees of the set of 10 signals.
3.3
FIS Customization through Graphical User Interface (GUI)
Even though the optimized set (not to be confused with the optimal set) of pa-
rameters provided by the simulated annealing can boost system performances,
the rules derived might be somehow counterintuitive for the human user. There-
fore, we risks of losing the system structure comprehensibility. Comprehensibility
along with readability accounts for the fuzzy system interpretability [18].
This is why we created a customizable GUI, where the expert can see the
signals distribution in the space of the normalized input variables and the per-
formance indicators. At ﬁrst, the optimized set of parameters is loaded, but the
operator can alter the modal values mk with a set of sliders, and also the limits
of the MFs by changing with the mouse the positions of the lines on the 2D grid.

584
L.-C. Duţu et al.
Fig. 3. Qualitative illustration of the GUI, where the eﬀect of altering the lower limit
of HighEP can be seen on the system indicators and on the membership functions of
the EP variable. Same procedure goes for all fuzzy sets and variables of the system.
Fig. 3 reveals the two above described sets of signals dispatched in the 2D
space of the input variables (EP , V
+). The customizable limits of the trapezoidal
MFs are depicted by the series of vertical lines for EP and horizontal lines for V
+.
Their position and the four modal values of output classes, as illustrated above,
represent the optimized set of parameters found by SA to best minimize δ.
The use of trapezoidal membership functions, as presented in Fig. 3, justiﬁes
the choice of both the lower and upper limit of the sets as degrees of freedom for
the system. For triangular fuzzy partitions, the choice of the 0.5-intersection of
two partitions can be used as a degree of freedom [14]. As we can see in Fig. 3,
every change is reﬂected on the system’s performance indicators, Therefore, the
user can decide to trade-oﬀsome performance for a more comprehensible system.
4
Conclusions and Perspectives
An intuitive two variables fuzzy symbolic system was proposed to model the sen-
sory information of the tactile sense. Initially, a psychophysical study of the haptic
sense guided us towards the appropriate two measures that simulate the neural ac-
tivation of two psychophysical channels of the skin. We then analytically validated
their choice using a perceptual dissimilarity matrix as ground truth.
These two measures were later used as input variables for the fuzzy symbolic
system, which under the supervision of the simulated annealing managed to
obtain an error of 9% and to correctly classify 17 out of 18 signals. The correlation
between the predicted and actual values for the validation set was 88%.

A Fuzzy Rule-Based Haptic Perception Model
585
To increase the interpretability of the model, a GUI that easily customizes the
system parameters is available to the user. The intuitiveness of the GUI and the
optimization strength of the simulated annealing can be seen as a step toward a
semi-automatic fuzzy symbolic system of haptic perception. In the future, loop
interactions between the two might be implemented.
References
1. Van Erp, J.B., Van Veen, H.A.H.C.: Vibro-tactile Information Presentation in Au-
tomobiles. In: Proceedings of Eurohaptics, pp. 99–104 (2001)
2. Wickens, C.D.: Multiple Resources and Performance Prediction. Theoretical Issues
in Ergonomics Science 3, 159–177 (2002)
3. Dabic, S., Tissot, J.-M., Navarro, J., Versace, R.: User Perceptions and Evaluations
of Short Vibrotactile Feedback. J. of Cognitive Psychology 25, 299–308 (2013)
4. Bolanowski, S.J., Gescheider, G.A., Verrillo, R.T., Checkosky, C.M.: Four Channels
Mediate the Mechanical Aspects of Touch. J. Acoust. Soc. Am. 84, 1680–1694
(1988)
5. Gescheider, G.A., Berryhill, M.E., Verrillo, R.T., Bolanowski, S.J.: Vibrotactile
Temporal Summation: Probability Summation or Neural Integration? Somatosen-
sory and Motor Research 16, 229–242 (1999)
6. Gescheider, G.A., Bolanowski, S.J., Verrillo, R.T.: Some Characteristics of Tactile
Channels. Behavioural Brain Research 148, 35–40 (2004)
7. Zwislocki, J.: Theory of Temporal Auditory Summation. J. Acoust. Soc. Am. 32,
1046–1060 (1960)
8. Bensmaia, S.J., Hollins, M.: Complex Tactile Waveform Discrimination. J. Acoust.
Soc. Am. 108, 1236–1245 (2000)
9. Looft, F.J.: Response of Monkey Glabrous Skin Mechanoreceptors to Random
Noise Sequences: II. Dynamic Stimulus State Analysis. Somatosensory and Mo-
tor Research 13, 11–28 (1996)
10. Duţu, L.-C., Mauris, G., Bolon, P., Dabic, S., Tissot, J.-M.: A Fuzzy Model Re-
lating Vibrotactile Signal Characteristics to Haptic Sensory Evaluations. In: IEEE
International Conference on Computational Intelligence and Virtual Environments
for Measurement Systems and Applications (CIVEMSA), pp. 49–54 (2013)
11. Rioul, O., Flandrin, P.: Time-Scale Energy Distributions: A General Class Extend-
ing Wavelet Transforms. IEEE Trans. on Signal Processing 40, 1746–1757 (1992)
12. Zadeh, L.A.: The Concept of a Linguistic Variable and Its Application to Approx-
imate Reasoning - Part 1. Information Sciences 8, 199–249 (1975)
13. Bouchon-Meunier, B., Lesot, M.-J., Marsala, C.: Modeling and Management of
Subjective Information in a Fuzzy Setting. Int. J. Gen. Syst. 42, 3–19 (2013)
14. Valet, L., Mauris, G., Bolon, P., Keskes, N.: A Fuzzy Rule-Based Interactive Fusion
System for Seismic Data Analysis. Information Fusion 4, 123–133 (2003)
15. Mauris, G., Benoit, E., Foulloy, L.: The Aggregation of Complementary Informa-
tion via Fuzzy Sensors. Measurement 17, 235–249 (1996)
16. Bouchon-Meunier, B., Yager, R.R., Zadeh, L.A.: Fuzzy Logic and Soft Computing.
World Scientiﬁc (1995)
17. Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P.: Optimization by Simulated Annealing.
Science 220, 671–680 (1983)
18. Alonso, J.M., Magdalena, L.: Special Issue on Interpretable Fuzzy Systems. Infor-
mation Sciences 181, 4331–4339 (2011)

A Linguistic Approach to Multi-criteria
and Multi-expert Sensory Analysis
Jos´e Luis Garc´ıa-Lapresta1, Cristina Aldavero2, and Santiago de Castro2
1 PRESAD Research Group, IMUVA, Dept. de Econom´ıa Aplicada,
Universidad de Valladolid, Spain
2 Academia Castellana y Leonesa de Gastronom´ıa y Alimentaci´on, Valladolid, Spain
lapresta@eco.uva.es, {acaldavero,casalfsa12}@gmail.com
Abstract. In this paper, we introduce a multi-criteria and multi-expert
decision-making procedure for dealing with sensory analysis. Experts
evaluate each product, taking diﬀerent criteria into account. If they are
conﬁdent in their opinions, the evaluation is presented using speciﬁc lin-
guistic terms. If not, linguistic expressions generated from several con-
secutive linguistic terms are used. Products are ranked according to the
average distance between the obtained ratings and the highest possible
assessment. The procedure is applied to a ﬁeld experiment in which six
trained sensory panelists assessed a variety of wines and wild mushrooms.
Keywords: sensory analysis, imprecision, linguistic assessments, dis-
tances.
1
Introduction
According to ISO 5492 [6], the sensory analysis is deﬁned as “the examination
of the organoleptical properties of a product using the human senses”. Sensory
analysis includes a variety of tools and techniques for evaluating consumer prod-
ucts taking into account human sensations provided by trained experts and/or
potential consumers (see Stone and Sidel [15], Meilgaard et al. [11] and Lawless
and Heymann [7], among others).
In all evaluations, a panel of tasters use their senses to perceive qualities
such as color, size, shape, smell, ﬂavor, texture, malleability and sound. These
sensations are translated into graphical, verbal or numerical values of previously
deﬁned quality descriptors for each type of product (some analyses regarding
the use of diﬃerent kinds of hedonic scales in sensory analysis can be found in
Lim et al. [8]).
Clearly, judges who analyze sensorial aspects of food, beverages, cosmetics,
textile materials, etc. have to deal with imprecision. Although the use of exact
numerical values could be inappropriate for assessing vague attributes such as
appearance, smell and taste, in tasting procedures judges usually have to ﬁll in
some forms using ﬁnite numerical scales. Once the ratings have been provided
by the judges, an aggregation procedure (most often using the arithmetic mean)
generates a ranking of the products.
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 586–595, 2014.
c
⃝Springer International Publishing Switzerland 2014

A Linguistic Approach to Multi-criteria and Multi-expert Sensory Analysis
587
Due to the vagueness and imprecision involved in sensory evaluation, the use
of ﬁnite scales formed by linguistic terms is clearly more suitable than using
numerical scales.
In this paper, we propose a multi-criteria and multi-expert decision-making
procedure for dealing with sensory analysis from a linguistic approach1. To this
end, a ﬁnite linguistic scale is ﬁxed. When experts are conﬁdent in their opinions,
they assign a linguistic term to each alternative in each criterion.
It is important to note that there is empirical evidence showing that experts
may hesitate when they assess alternatives through linguistic terms2. For this
reason, we have considered linguistic expressions generated by consecutive lin-
guistic terms. Thus, experts are allowed to assign linguistic expressions to each
alternative in each criterion.
Once experts have provided their assessments, we calculate the distances be-
tween the linguistic ratings and the highest possible assessment for each alter-
native and each criterion. These distances are obtained by adding the geodesic
distances in the graph associated with the linguistic expressions set and the
penalization values which are dependent upon the increase of imprecision.
Taking into account a weighting vector that reﬂects the diﬃerent level of
importance attributed to each criterion, for each alternative we calculate the
weighted average distance between the obtained ratings and the highest possible
assessment. Then, the alternatives are ranked according to the above mentioned
weighted average distances (the less, the better), similarly to Falc´o [3, Chapter
4].
In order to show how the proposed decision-making process works, we have
considered the data obtained in a ﬁeld experiment in which six trained experts
gave their opinions on diﬃerent wines and wild mushrooms through linguistic
terms and linguistic expressions.
The paper is organized as follows. Section 2 includes the basic notation and
some concepts that are necessary to develop the proposal. Section 3 is devoted
to introduce the proposal of multi-criteria and multi-expert sensory analysis.
Section 4 contains the description of the ﬁeld experiment and the results. Finally,
Section 5 includes some concluding remarks.
2
Preliminaries
Let A = {1, . . . , m}, with m ≥2, be a set of agents and let X = {x1, . . . , xn},
with n ≥2, be the set of alternatives which have to be evaluated. Under total
certainty, each agent assigns a linguistic term to every alternative within a lin-
guistic ordered scale L = {l1, . . . , lg}, with l1 < l2 < · · · < lg. It is assumed that
the linguistic scale is balanced and consecutive terms are equispaced.
1 Some fuzzy linguistic approaches to sensory analysis can be found in Davidson and
Sun [2], Mart´ınez [9], Mart´ınez et al. [10] and Agell et al. [1], among others.
2 For instance, in the tasting described in Agell et al. [1], 40% of the assessments were
linguistic expressions with two or more linguistic terms.

588
J.L. Garc´ıa-Lapresta, C. Aldavero, and S. de Castro
Taking into account the absolute order of magnitude spaces introduced by
Trav´e-Massuy`es and Piera [16], the set of linguistic expressions is deﬁned as
L = {[lh, lk] | lh, lk ∈L , 1 ≤h ≤k ≤g},
where [lh, lk] = {lh, lh+1, . . . , lk}. Given that [lh, lh] = {lh}, this linguistic ex-
pression can be replaced by the linguistic term lh. In this way, L ⊂L.
Example 1. Consider the set of linguistic terms L = {l1, l2, l3, l4, l5} with the
meanings given in Table 1.
Table 1. Meaning of the linguistic terms
l1
l2
l3
l4
l5
very bad
bad
acceptable
good
very good
Since linguistic expressions are intervals of linguistic terms, their meanings
are straightforward. For instance, [l2, l3] means ‘between bad and acceptable’,
[l3, l5] means ‘between acceptable and very good’, or ‘at least acceptable’, etc.
Taking into account the approach introduced in Rosell´o et al. [14], the set of
linguistic expressions can be represented by a graph GL. In the graph, the lowest
layer represents the linguistic terms lh ∈L ⊂L, the second layer represents the
linguistic expressions created by two consecutive linguistic terms [lh, lh+1], the
third layer represents the linguistic expressions generated by three consecutive
linguistic terms [lh, lh+2], and so on up to last layer where we represent the
linguistic expression [l1, lg]. As a result, the higher an element is, the more
imprecise it becomes.
The vertices in GL are the elements of L and the edges E −F, where E =
[lh, lk] and F = [lh, lk+1], or E = [lh, lk] and F = [lh+1, lk]. Fig. 1 shows the
graph representation of Example 1. When an agent is conﬁdent about his opin-
ion on an alternative, he can assign a linguistic term lh ∈L to this alternative.
However, if he is unconﬁdent about his opinion, he might use a linguistic expres-
sion [lh, lk] ∈L, with h < k. For more details, see Rosell´o et al. [12], [13], [14]
and Falc´o et al. [5].
A binary relation ≽on a set Z ̸= ∅is a weak order if it is complete (x ≽y or
y ≽x, for all x, y ∈Z) and transitive (if x ≽y and y ≽z, then x ≽z, for all
x, y, z ∈Z). On the other hand, a linear order on Z ̸= ∅is an antisymmetric3
weak order on Z. Given a weak or linear order ≽on Z ̸= ∅, the asymmetric
part of ≽is denoted by ≻; in other words, x ≻y if not y ≽x.
3
The Multi-criteria and Multi-expert Decision Process
In order to introduce our proposal, we ﬁrst consider a family of parameterized
metrics in the set of linguistic expressions. They will be used for comparing the
alternatives and for generating a weak order over them.
3 ≽is antisymmetric if for all x, y ∈Z (x ≽y and y ≽x) implies x = y.

A Linguistic Approach to Multi-criteria and Multi-expert Sensory Analysis
589
[l1, l5]
[l2, l5]
[l1, l4]
[l3, l5]
[l2, l4]
[l1, l3]
[l4, l5]
[l3, l4]
[l2, l3]
[l1, l2]
l5
l4
l3
l2
l1
Fig. 1. Graph representation of the linguistic expressions for g = 5
3.1
Comparing Linguistic Expressions
The set of linguistic expressions can be ranked in diﬃerent ways. We now intro-
duce a linear order on this set that will be compatible with the ordering induced
by the distance with respect to the highest possible assessment lg.
Proposition 1. The binary relation ≽L on L, deﬁned as
[lh, lk] ≽L [lh′, lk′] ⇔
⎧
⎨
⎩
h + k > h′ + k′
or
h + k = h′ + k′ and k −h ≤k′ −h′,
is a linear order, and it is called the canonical order on L.
For g = 5, the elements of L are ordered as follows (see also Fig. 2):
l5 ≻L [l4, l5] ≻L l4 ≻L [l3, l5] ≻L [l3, l4] ≻L [l2, l5] ≻L l3 ≻L [l2, l4] ≻L
≻L [l1, l5] ≻L [l2, l3] ≻L [l1, l4] ≻L l2 ≻L [l1, l3] ≻L [l1, l2] ≻L l1.
The geodesic distance4 between two linguistic expressions E, F ∈L is deﬁned
as the distance in the graph GL between their associated vertices:
dG

[lh, lk], [lh′, lk′]

= |h −h′| + |k −k′|.
In Falc´o et al. [4] the geodesic distance is modiﬁed for penalizing the impre-
cision by means two parameters, ϕ and π. For simplicity, in this paper we only
consider the ﬁrst penalization through the parameter ϕ.
4 The geodesic distance between two vertices in a graph is the number of edges in one
of the shortest paths connecting them.

590
J.L. Garc´ıa-Lapresta, C. Aldavero, and S. de Castro
[l1, l5]
[l2, l5]
[l1, l4]
[l3, l5]
[l2, l4]
[l1, l3]
[l4, l5]
[l3, l4]
[l2, l3]
[l1, l2]
l5
l4
l3
l2
l1
Fig. 2. Canonical order in L for g = 5
Given E = [lh, lk] ∈L, with #E we denote the cardinality of E, i.e., the
number of linguistic terms in the interval [lh, lk]: #E = k + 1 −h.
Proposition 2. For every ϕ ≥0, the function d : L × L −→R, deﬁned as
d(E, F) = dG(E, F) + ϕ |#E −#F|,
is a metric, and it is called the metric associated with ϕ.
The following result is a direct consequence of Falc´o et al. [4, Prop. 3].
Proposition 3. Let dα : L2 −→R be the metric associated with ϕ ≥0. The
following statements are equivalent:
1. ∀E, F ∈L

E ≻F ⇔dα(E, lg) < dα(F, lg)

.
2. ϕ ∈Tg, where Tg =
#
0,
1
g−2
$
, if g is odd, and Tg =
#
0,
1
g−1
$
, if g is even.
Example 2. Following Example 1, where g = 5, Proposition 3 shows that ϕ < 1
3
should be satisﬁed for avoiding inconsistencies. Then,
d(l5, l5) = 0 < d([l4, l5], l5) = 1 + ϕ < d(l4, l5) = 2 < d([l3, l5], l5) = 2 + 2ϕ <
d([l3, l4], l5) = 3 + ϕ < d([l2, l5], l5) = 3 + 3ϕ < d(l3, l5) = 4 <
d([l2, l4], l5) = 4 + 2ϕ < d([l1, l5], l5) = 4 + 4ϕ < d([l2, l3], l5) = 5 + ϕ <
d([l1, l4], l5) = 5 + 3ϕ < d(l2, l5) = 6 < d([l1, l3], l5) = 6 + 2ϕ <
d([l1, l2], l5) = 7 + ϕ < d(l1, l5) = 8 .

A Linguistic Approach to Multi-criteria and Multi-expert Sensory Analysis
591
3.2
From the Individual Assessments to a Social Ranking
A proﬁle V is a matrix (va
i ) consisting of m rows and n columns of linguistic
expressions, where the element va
i ∈L represents the linguistic assessment given
by the agent a ∈A to the alternative xi ∈X. Then,
V =
⎛
⎜
⎜
⎜
⎜
⎝
v1
1 · · · v1
i · · · v1
n
· · · · · · · · · · · · · · ·
va
1 · · · va
i · · · va
n
· · · · · · · · · · · · · · ·
vm
1 · · · vm
i
· · · vm
n
⎞
⎟
⎟
⎟
⎟
⎠
= (va
i ) .
Given a proﬁle V = (va
i ),
D(xi) = 1
m
m

a=1
d (va
i , lg)
is the average distance between the ratings of xi and the highest possible assess-
ment lg.
Proposition 4. Given ϕ ≥0 satisfying the condition of Proposition 3, let d be
the metric associated with ϕ. The binary relation ≽on X deﬁned as
xi ≽xj ⇔D(xi) ≤D(xj)
is a weak order on X.
Consider now that agents have to assess each alternative from diﬃerent cri-
teria: c1, . . . , cr. Then we have r proﬁles V 1 =

va,1
i

, . . . , V r =

va,r
i

, where
va,k
i
∈L is the rating given by the agent a to the alternative xi with respect to
the criterion k.
Since each criterion may have diﬃerent importance in the decision, consider a
weighting vector w = (w1, . . . , wr) ∈[0, 1]r, with w1 + · · · + wr = 1.
Given a proﬁle V = (va
i ) and a weighting vector w = (w1, . . . , wr),
Dw(xi) =
r

k=1
wp
1
m
m

a=1
d
#
va,k
i
, lg
$
is the weighted average distance between the ratings of xi and the highest pos-
sible assessment lg.
Proposition 5. Given ϕ ≥0 satisfying the condition of Proposition 3, let d be
the metric associated with ϕ. The binary relation ≽w on X deﬁned as
xi ≽w xj ⇔Dw(xi) ≤Dw(xj)
is a weak order on X.

592
J.L. Garc´ıa-Lapresta, C. Aldavero, and S. de Castro
4
A Field Experiment
The proposed multi-criteria and multi-expert sensory analysis was demonstrated
in a ﬁeld experiment carried out in Trigo restaurant in Valladolid (November
30th, 2013), under appropriate conditions of temperature, light and service.
A total of six judges (one female and ﬁve males between the ages of 30 and
50) trained in the sensory analysis of wine and wild mushrooms were recruited
through the Gastronomy and Food Academy of Castilla y Le´on. When the test
was being carried out there was no communication between judges. The samples
were given without any identiﬁcation. The tasting was divided into two parts.
Fig. 3. Cooked Boletus pinophilus and the linguistic hedonic tasting sheet
Firstly, the experts gave their evaluations using a classical numerical hedonic
tasting sheet on a scale of 1 to 10. Secondly, they repeated the tasting –in a
diﬃerent order– assessing the same products through linguistic terms from a
linguistic hedonic scale and the corresponding linguistic expressions.
After the tasting, the six experts were asked if they had diﬀculties ﬁlling in
linguistic hedonic tasting sheet and whether they were more comfortable using
numerical or linguistic assessments. All of them declared that they did not have
diﬀculties ﬁlling in the linguistic hedonic tasting sheet. Four out of the six
experts preferred the linguistic hedonic tasting sheet over the numerical one;
one was indiﬃerent regarding the two styles; and another preferred the numerical
hedonic tasting sheet over the linguistic one.
We now describe the tasting in its linguistic part. The six experts assessed the
products included in Table 2 through the ﬁve linguistic terms of Table 3 (or the
corresponding linguistic expressions, when they hesitated) under three criteria:
appearance, smell and taste5.
Five of the six experts sometimes hesitate about their opinions and they
assigned linguistic expressions with two linguistic terms. This happened in 19 of
the 108 ratings, i.e., 17.59% of the cases.
5 We have presented the results all together. Clearly, wines, raw wild mushrooms and
cooked wild mushrooms have to be analyzed separately.

A Linguistic Approach to Multi-criteria and Multi-expert Sensory Analysis
593
Table 2. Alternatives
x1 White wine ‘Rueda Zascandil 2012’
x2 Red wine ‘Toro Valdelacasa 2007’
x3 Raw Boletus pinophilus
x4 Raw Tricholoma pertentosum
x5 Cooked Boletus pinophilus
x6 Cooked Tricholoma pertentosum
Table 3. Linguistic terms
l1
I don’t like it at all
l2
I don’t like it
l3
I like it
l4
I rather like it
l5
I like it so much
Taking into account the weights6 w1 = 0.2 for appearance, w2 = 0.3 for smell
and w3 = 0.5 for taste, i.e., w = (0.2, 0.3, 0.5), and the parameter ϕ = 0.3, we
obtain the following weighted average distances from the alternatives ratings to
the highest possible assessment l5:
Dw(x1) = 2.965
Dw(x2) = 2.081
Dw(x3) = 1.246
Dw(x4) = 3.515
Dw(x5) = 1.148
Dw(x6) = 2.193.
Then, x5 ≻w x3 ≻w x2 ≻w x6 ≻w x1 ≻w x4.
We note that the alternatives were similarly ordered when using the numerical
assessments with the same weights, after calculating the average of the individual
ratings in each phase: x5 ≻w x3 ≻w x6 ≻w x2 ≻w x1 ≻w x4, with x2 and
x6 being the only alternatives where the ranking is diﬃerent.
Although all six products are presented in the ranking, they can be divided
into three parts: wines (alternatives x1 and x2), raw wild mushrooms (alterna-
tives x3 and x4) and cooked wild mushrooms (alternatives x5 and x6).
Sensory Description. According to one of the judges who participated in the tast-
ing, the most aromatic and tasty wild mushroom was Boletus pinophilus, the low
temperature of the cooking process enhances volatilization of its essential oils, ﬂa-
vor development and improvement of texture, making it a favorite of the panel
of tasters in both the numerical rating and verbal evaluation methods. Raw Tri-
choloma portentosum had a medium ﬂavor intensity and did not have a very strong
ﬂavor, this mushroom improves with the cooking process, which enhances all of its
qualities outstandingly. The red wine presented medium-high primary aromas, a
good structure and a good balance of acidity, alcohol and tannins, while the white
wine had a lower aromatic intensity and a slight imbalance of acidity.
6 These weights are usual in this kind of tasting.

594
J.L. Garc´ıa-Lapresta, C. Aldavero, and S. de Castro
Table 4. Ratings
Judge 1
Judge 2
Judge 3
Judge 4
Judge 5
Judge 6
x1
Appearance
l3
l4
l4
l5
l3
l3
Smell
[l3, l4]
l3
l4
l4
l4
l4
Taste
l3
l3
l4
l4
l3
l3
x2
Appearance
l4
l4
l2
l5
l4
l4
Smell
[l4, l5]
l5
l2
l5
l3
l5
Taste
[l4, l5]
l5
l2
l5
l3
[l4, l5]
x3
Appearance
l4
l4
l5
l5
l3
l4
Smell
[l4, l5]
l3
[l4, l5]
l5
l3
l5
Taste
l5
l5
[l4, l5]
l5
l4
[l4, l5]
x4
Appearance
l4
l3
l4
l5
l3
[l2, l3]
Smell
l3
[l2, l3]
l4
l3
l2
[l2, l3]
Taste
l3
l3
[l4, l5]
l4
l2
l4
x5
Appearance
l5
l4
l4
l4
l3
l4
Smell
l5
l5
l5
l4
l3
[l4, l5]
Taste
[l4, l5]
l5
l5
l5
l4
[l4, l5]
x6
Appearance
l5
l3
[l4, l5]
l5
[l2, l3]
l4
Smell
l5
l3
l5
l3
[l3, l4]
l4
Taste
l4
l4
l5
l3
l4
[l3, l4]
5
Concluding Remarks
A generalization of our proposal consists of using appropriate aggregation func-
tions for deﬁning the aggregated distance between the ratings of an alternative
and the highest possible assessment. In this way, the arithmetic mean can be
replaced by an OWA operator (see Yager [17]).
Acknowledgments. The authors are grateful to the participants in the tasting,
as well as V´ıctor Mart´ın and Noem´ı Mart´ınez (Trigo restaurant in Valladolid).
The ﬁnancial support of the Spanish Ministerio de Econom´ıa y Competitividad
(project ECO2012-32178) and Consejer´ıa de Educaci´on de la Junta de Castilla
y Le´on (project VA066U13) are also acknowledged.
References
1. Agell, N., S´anchez, G., S´anchez, M., Ruiz, F.J.: Selecting the best taste: a group
decision-making application to chocolates design. In: Proceedings of the 2013 IFSA-
NAFIPS Joint Congress, Edmonton, pp. 939–943 (2013)
2. Davidson, V.J., Sun, W.: A linguistic method for sensory assessment. Journal of
Sensory Studies 13, 315–330 (1998)
3. Falc´o, E.: Voting Systems with Linguistic Assessments and their Application to
the Allocation of Tenders. PhD Dissertation, University of Valladolid (2013)

A Linguistic Approach to Multi-criteria and Multi-expert Sensory Analysis
595
4. Falc´o, E., Garc´ıa-Lapresta, J.L., Rosell´o, L.: Aggregating imprecise linguistic ex-
pressions. In: Guo, P., Pedrycz, W. (eds.) Human-Centric Decision-Making Models
for Social Sciences. SCI, vol. 502, pp. 97–113. Springer, Heidelberg (2014)
5. Falc´o, E., Garc´ıa-Lapresta, J.L., Rosell´o, L.: Allowing agents to be imprecise: A
proposal using multiple linguistic terms. Information Sciences 258, 249–265 (2014)
6. ISO 5492 (1992)
7. Lawless, H.T., Heymann, H.: Sensory Evaluation of Food: Principles and Practices.
Springer (2010)
8. Lim, J., Wood, A., Green, B.G.: Derivation and evaluation of a labeled hedonic
scale. Chemical Sense 34(9), 739–751 (2009)
9. Mart´ınez, L.: Sensory evaluation based on linguistic decision analysis. International
Journal of Approximate Reasoning 44, 148–164 (2007)
10. Mart´ınez, L., Espinilla, M., Liu, J., P´erez, L.G., S´anchez, P.J.: An evaluation model
with unbalanced linguistic information applied to olive oil sensory evaluation. Jour-
nal of Multiple-Valued Logic and Soft Computing 15, 229–251 (2009)
11. Meilgaard, M.C., Carr, T., Civille, G.V.: Sensory Evaluation Techniques, 4th edn.
CRC Press (2006)
12. Rosell´o, L., Prats, F., Agell, N., S´anchez, M.: Measuring consensus in group de-
cisions by means of qualitative reasoning. International Journal of Approximate
Reasoning 51, 441–452 (2010)
13. Rosell´o, L., Prats, F., Agell, N., S´anchez, M.: A qualitative reasoning approach to
measure consensus. In: Herrera-Viedma, E., Garc´ıa-Lapresta, J.L., Kacprzyk, J.,
Fedrizzi, M., Nurmi, H., Zadro˙zny, S. (eds.) Consensual Processes. STUDFUZZ,
vol. 267, pp. 235–261. Springer, Heidelberg (2011)
14. Rosell´o, L., S´anchez, M., Agell, N., Prats, F., Mazaira, F.A.: Using consensus
and distances between generalized multi-attribute linguistic assessments for group
decision-making. Information Fusion 17, 83–92 (2014)
15. Stone, H., Sidel, J.L.: Sensory Evaluation Practices. Academic Press (2004)
16. Trav´e-Massuy`es, L., Piera, N.: The orders of magnitude models as qualitative al-
gebras. In: Proceedings of the 11th International Joint Conference on Artiﬁcial
Intelligence, Detroit, pp. 1261–1266 (1989)
17. Yager, R.R.: Ordered weighted averaging operators in multicriteria decision mak-
ing. IEEE Transactions on Systems, Man and Cybernetics 8, 183–190 (1988)

Using Fuzzy Logic to Enhance Classiﬁcation
of Human Motion Primitives
Barbara Bruno1, Fulvio Mastrogiovanni1,
Alessandro Saﬀotti2, and Antonio Sgorbissa1
1 University of Genova, Dept. DIBRIS,
via Opera Pia 13, 16145 Genova, Italy
{barbara.bruno,fulvio.mastrogiovanni,antonio.sgorbissa}@unige.it
2 ¨Orebro University, AASS Cognitive Robotic Systems Lab.,
Fakultetsgatan 1, S-70182 ¨Orebro, Sweden
asaffio@aass.oru.se
Abstract. The design of automated systems for the recognition of spe-
ciﬁc human activities is among the most promising research activities in
Ambient Intelligence. The literature suggests the adoption of wearable
devices, relying on acceleration information to model the activities of
interest and distance metrics for the comparison of such models with the
run-time data. Most current solutions do not explicitly model the uncer-
tainty associated with the recognition, but rely on crisp thresholds and
comparisons which introduce brittleness and inaccuracy in the system.
We propose a framework for the recognition of simple activities in which
recognition uncertainty is modelled using possibility distributions. We
show that reasoning about this explicitly modelled uncertainty leads to
a system with enhanced recognition accuracy and precision.
Keywords: Activity recognition, Activities of Daily Living, wearable
sensors, possibility measures.
1
Introduction
Automatic recognition of human activities is a vivid area of research, whose
impact ranges from smart homes to future factory automation and to social
behavioural studies. One of the most timely application is the process of deter-
mining the level of autonomy of an elderly person. Ever since the publication of
the Index of Activities of Daily Living (ADL) by Katz and colleagues [1], this
process is usually accomplished by analysing the person’s ability to carry out a
set of daily activities, each one involving the use of diﬃerent motor and cogni-
tive capabilities. Unfortunately, the most commonly adopted indexes and sets
of ADL have been deﬁned assuming that a caregiver examines the person’s per-
formance on a qualitative basis; this makes the design of automated systems for
the monitoring, recognition and classiﬁcation of ADL particularly challenging.
Existing solutions take two diﬃerent approaches: smart environments rely on
heterogeneous sensors distributed in the environment [2–4]; wearable sensing
systems rely on sensors located on the person body [5, 6].
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 596–605, 2014.
c
⃝Springer International Publishing Switzerland 2014

Fuzzy Logic in Classiﬁcation of Human Motion Primitives
597
Wearable sensing is quickly becoming the preferred approach to monitor either
body gestures or bio signals. Most systems aim at engineering a single sensing
device, either based on the integration of diﬃerent sensors [7] or on the use of
a single sensing mode. Among the latter, systems based on a single tri-axial
accelerometer are the most common [8, 9]. As outlined in [5], most wearable
sensing systems have a similar architecture, whose main tasks are: (i) to extract
relevant features from the available sensory data; (ii) to create representations
of the target activities in terms of the features; and (iii) to classify the run-
time sensory data according to the representations. Features are chosen so that
they discriminate well diﬃerent activities while being invariant across diﬃerent
executions of the same activity: for wearable sensors, gravity and body accel-
eration are the most commonly adopted features [8],[10]. The representation of
the target activities, and the classiﬁcation of run-time sensor data, are typically
based on the deﬁnition of rules and the adoption of decision trees [10]; or on the
creation of models (e.g., Hidden Markov Models or Gaussian Mixture Models)
and the deﬁnition of adequate distance measures [8],[11, 12]. In both cases, the
system labels the run-time data as an occurrence of the activity that most closely
represents them.
The above approaches produce a crisp decision on the recognized activity,
and do not model the uncertainty associated with this decision. Suppose that a
person executes an activity A which is not modelled in the system; and suppose
that, although none of the existing models fully ﬁts the observed data, B is the
closest one. A crisp system would label this activity as B without providing any
warning of the unreliability of this decision. As another example, suppose that
the observed data ﬁt both the model of A and, to a less degree, the one of B.
A crisp system would label the activity as A without providing any warning of
the ambiguity of this decision. Failure to model the uncertainty associated with
a decision limits the possibility to integrate the decision with complementary
context-assessing systems [2],[13], and may reduce the applicability of the system
to real-life scenarios [14].
We propose a framework for the recognition of ADL that takes recognition
uncertainty into account. Building upon the work in [12],[15], we use information
from a single wrist-placed tri-axial accelerometer, and rely on Gaussian Mixture
Modelling and Gaussian Mixture Regression to create models of ADL. We extend
that work by building a possibility distribution over the space of ADL [16], where
degrees of possibilities are computed from the Mahalanobis distance between
the observed data and the model. The use of possibility measures is justiﬁed by
the semantics of our models, which is inherently similarity-based [17]. We then
show how to leverage possibility values to improve the accuracy and precision of
recognition, by reasoning about the evolution of these values in time.
This article is organized as follows. Section 2 outlines the architecture of the
system. Section 3 describes the classiﬁcation and analysis procedures that we
propose. Section 4 reports experimental results. Finally, conclusions follow.

598
B. Bruno et al.
2
System Architecture
In this paper, we reserve the term ADL for a whole complex activity, and deﬁne
as human motion primitives (HMP) those stereotyped motions that can uniquely
identify an ADL. For example, the ADL Feeding deﬁned in Katz Index comprises
the HMP pick up a glass and put down a glass, while the ADL Mobility comprises
the HMP walk and climb the stairs. Typically, HMP can be monitored with
wearable sensing systems while ADL may require a whole smart environment.
Fig. 1. System architecture for HMP classiﬁcation
The classical part of our system for the recognition of HMP builds upon the
techniques described in [12],[15], and it is composed of two distinct subsystems.
During the oﬃ-line phase (Figure1, left) one or more individuals are provided
with a wrist-mounted tri-axial accelerometer and asked to perform each HMP
multiple times. The acceleration data are recorded and each occurrence of any
HMP is tagged by a human observer. The result is a dataset which, for each
HMP, contains a large number of examples in the form of sequences of accelera-
tion data along the three axes x, y and z. Once the training set is available, the
model builder module performs a number of steps: raw data ﬁltering is aimed at
reducing high frequency noise through a median ﬁlter; feature extraction isolates
the gravitational components by applying a low-pass ﬁlter to the acceleration
signal and then obtains the body acceleration components by subtraction; GMM
and GMR modelling generate a probabilistic model of each HMP in terms of its
features. Since keeping the axes correlation into account allows for an increase
in the recognition accuracy [12], both the features and their models are deﬁned
in the 4D space of time and tri-axial acceleration. A description of the model
builder block can be found in [12]. During the on-line phase (Figure 1, right),
the monitored person wears a wrist-mounted device providing acceleration data.

Fuzzy Logic in Classiﬁcation of Human Motion Primitives
599
Analogously to the oﬃ-line phase, a number of steps are sequentially executed:
raw data ﬁltering and feature extraction execute the very same algorithms of the
oﬃ-line phase; then the comparison step performs the classiﬁcation by considering
a temporal window moving over the run-time data stream and comparing the
features extracted from acceleration data therein with all the stored models. A
description of the classiﬁer block can be found in [15].
It is important to notice that the presented system is independent from
the sensing device used for the acquisition of acceleration data. In particular,
it is compatible with already available smart watches, i.e., wrist-worn devices
equipped with various sensors.1 The positioning of the device, however, is crucial
for the reliability and the eﬃectiveness of the monitoring system, and it depends
on the target set of HMPs: e.g., hand gestures require a wrist placement, while
lower limbs movements are better monitored with waist-placed sensors.
3
Representing Uncertainty in the HMP Classiﬁer
The basic approach presented in the previous section is typically applied without
taking uncertainty into account: the best matching model is simply returned as
the classiﬁcation result. We now see how we can model classiﬁcation uncertainty
in a possibilistic way, and how we can reason about this uncertainty to enhance
the classiﬁcation performance.
3.1
Basic Concepts and Deﬁnitions
Let us denote with M the number of models available for the classiﬁcation,
created by the model builder in Figure 1, and with Km the number of data
points used to model the HMP m. Let us also denote with ξ the generic feature
of interest, i.e. ξ can either correspond to gravity g or body acceleration b. The
following deﬁnitions are in order.
– ˆξm,k is the data point k of feature ξ of model m, deﬁned as:
ˆξm,k = (tξ
m,k, ˆaξ
m,k, Θξ
m,k),
(1)
where tξ
m,k ∈R is the time information, ˆaξ
m,k ∈R3 contains the expected
x, y and z acceleration components, and Θξ
m,k ∈R3×3 is the conditional
covariance associated to ˆaξ
m,k.
– ˆΞξ
m is the generalized version of feature ξ for HMP m, deﬁned as:
ˆΞξ
m = {ˆξm,1, . . . , ˆξm,Km}.
(2)
– ˆΞm = ( ˆΞg
m, ˆΞb
m) is the model of HMP m.
1 Popular smart watches are Pebble SmartWatch (https://getpebble.com/) or Sony
SmartWatch (http://store.sony.com/smartwatch/cat-27-catid-Smart-Watch)

600
B. Bruno et al.
In a similar way, given a window Ξw = (Ξg
w, Ξb
w) of size N moving over the
run-time acceleration data stream, we deﬁne the feature ξ extracted from the
window as:
Ξξ
w = {ξw,1, . . . , ξw,N},
(3)
where ξw,n = (tξ
w,n, aξ
w,n) is the data point at position n inside the window. The
task of the classiﬁer is to compare Ξw with the M models { ˆΞ1, . . . , ˆΞM} in order
to identify the model ˆΞm that most likely represents the run-time data.
3.2
Modelling Uncertainty by Possibility Distributions
The “classical”, crisp approach to human activities recognition would select, at
each time instant, the activity, among those modelled by the system, that most
closely matches the acceleration data inside the window. In our case this is done
by ﬁrst computing Mahalanobis distance between the window of run-time data
Ξw and each of the models ˆΞm, then labelling the window as an occurrence
of the HMP with minimum distance. Figure 2 (a) reports the output of such
classiﬁer when the executed motion is the HMP standing up from a chair. It is
immediate to see that this classic, crisp classiﬁer: (i) does not allow for a correct
deﬁnition of the starting and ending moments of the recognized motion; and
(ii) has a high rate of false positive recognitions. In order to overcome these
limitations, we have designed a possibility based classiﬁer which, at each time
instant, returns the degree of possibility πw
m that the modelled HMP m is the
one representing the run-time data inside the current window w, computed as:
πw
m =

1 −Dw
m/τm
Dw
m < τm
0
Dw
m ≥τm
.
(4)
Dw
m is the distance between model m and the run-time data, deﬁned as:
Dw
m = ϕ · d( ˆΞg
m, Ξg
w) + π · d( ˆΞb
m, Ξb
w),
(5)
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
time [windows]
possibility
 
 
ground truth
climb
getup
pickUpDrink
putDownDrink
pickUpPour
putDownPour
sit
stand
walk
(a) Classic classiﬁer
0
100
200
300
400
500
0
0.2
0.4
0.6
0.8
1
time [windows]
possibility
 
 
ground truth
climb
getup
pickUpDrink
putDownDrink
pickUpPour
putDownPour
sit
stand
walk
(b) Possibility based classiﬁer
Fig. 2. Output of a (a) classic classiﬁer and (b) possibility based classiﬁer when tested
with an execution of the HMP stand up from a chair (within orange solid lines)

Fuzzy Logic in Classiﬁcation of Human Motion Primitives
601
where d is the Mahalanobis distance between the samples in the oﬃ-line model
ˆΞξ
m and the on-line window Ξξ
w, and ϕ and π are the relative weight of the
gravity and the body accelerations. The normalization factor τm is the distance
between model m and the farthest curve which is likely to be generated by the
model itself:
τm = ϕ · d( ˆΞg
m, ¯Ξg
m) + π · d( ˆΞb
m, ¯Ξb
m),
(6)
where the “farthest curve” ¯Ξξ
m generated by model m for feature ξ is deﬁned as:
¯Ξξ
m = {¯ξm,1, . . . , ¯ξm,Km},
¯ξm,k =
#
tξ
m,k, ˆaξ
m,k + ψ · diag(Θξ
m,k)
$
.
(7)
As it can be seen in Figure 2 (b), the possibilities computed by (4) allow an
immediate qualitative evaluation of the consistency of the classiﬁer output.
3.3
Reasoning about Uncertainty
Consider again Figure 2 (b), and assume that the monitored person starts to
execute the motion stand up from a chair. At the beginning no sample referring
to stand up is inside the window w of run-time data, and πw
stand = 0. While the
person executes the motion, the samples referring to stand up ﬁll the window
w and thus the value of πw
stand steadily increases up to its maximum value,
which corresponds to the ending moment of the motion, when all the samples
in w refer to stand up. As the person moves on to execute another activity, the
samples referring to stand up leave the window, thus making the value of πw
stand
steadily decrease. In case of a false positive, since there is no correlation between
the samples ﬁlling the window and the model, the possibility does not follow
the outlined rise-fall pattern. This argument suggests that a simple temporal
analysis of the possibility values can lead to: (i) the identiﬁcation of the starting
and ending moments of each execution of a modelled HMP and (ii) a signiﬁcant
reduction in the number of false positive recognitions.
On such basis, we deﬁne the interval Im of recognition of HMP m as:
Im = ⟨ts, te, open, πs, πmax⟩,
(8)
where:
– ts and te denote, respectively, the starting and ending time of the interval;
– open is a Boolean value which indicates if the interval is currently open;
– πs stores the possibility πw
m recorded at time ts;
– πmax stores the maximum possibility πw
m recorded until time te.
The recognition procedure based on the temporal analysis of the possibility
values πm is outlined in Algorithm 1 (we omit the w superscript for simplic-
ity). Parameter T is introduced to check the symmetry of the rise-fall tempo-
ral pattern of the possibility. Lines 4, 5 open a new potential interval, when
the possibility πm > 0. Procedure init initializes template interval Im as

602
B. Bruno et al.
Algorithm 1. Identify intervals Im of recognition
Require: the possibilities πm, πold
m computed at time instants t and t −1 respectively.
Require: the template intervals {I1, . . . , IM}.
1: for all m ∈{1, . . . , M} do
2:
if πm > 0 then
3:
if !open then
4:
init(t, πm, πold
m , Im)
5:
T ←1
6:
else if πm ≥πmax then
7:
update(t, πm, Im)
8:
T ←T + 1
9:
else
10:
T ←T −1
11:
if (πm > πold
m ) ∨(T = 0 ∧πm < πs) then
12:
open ←false
13:
else if T ≥0 ∧πm = πs ∧ts ̸= te then
14:
publish(Im)
15:
end if
16:
end if
17:
else if open = true ∧πs = 0 ∧ts ̸= te then
18:
publish(Im)
19:
end if
20: end for
Im = ⟨t, t, true, πm, πold
m ⟩. Lines 7, 8 correspond to the steady increase in the
possibility while the person executes the motion. Procedure update updates
template interval Im as Im = ⟨ts, t, true, πs, πm⟩. Lines 10 onward cover the sit-
uation in which the person has just ﬁnished executing the motion and check
whether the possibility follows the expected pattern. More speciﬁcally, line 12
resets the interval in case of a false positive recognition, while line 14 corresponds
to the recognition of a rise-fall pattern. Procedure publish makes the recogni-
tion information available to other procedures and resets interval Im by setting
open ←false. Line 18 is the analogous of Line 14 for the case πs = 0.
4
Experimental Results
To validate the system, we collected 243 executions of 9 HMP (listed in Fig. 2)
and split them into a modelling dataset (37%) and a validation dataset (63%).2
We chose the 9 HMP under the assumption of bounded rationality of the
user, to be representative of commonly considered ADL. We applied the model
builder described in Section 2 to the modelling dataset to build the 9 models.
Table 1 reports the results of the experiment we have designed to estimate, for
each motion primitive: the number of true positive recognitions corresponding to
2 We have collected a larger dataset, available at: http://archive.ics.uci.edu/ml/
datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer

Fuzzy Logic in Classiﬁcation of Human Motion Primitives
603
Table 1. Experimental validation of the rise-fall pattern of the possibility values
HMP
TRF TnRF FRF
FnRF
Get up from the bed
73.33% 26.67%
0%
100%
Pick up a glass (drink)
100%
0%
–
–
Put down a glass (drink)
100%
0%
12.5%
87.5%
Pick up a bottle (pour) 93.75% 6.25% 45.45% 54.55%
Put down a bottle (pour) 76.92% 23.08% 2.78% 97.22%
Sit down on a chair
76.92% 23.08%
0%
100%
Stand up from a chair
100%
0%
46.67% 53.33%
a rise-fall possibility pattern (TRF); the number of true positives corresponding
to a non rise-fall pattern (TnRF); the number of false positives corresponding
to a rise-fall pattern (FRF); and the number of false positives corresponding to
a non rise-fall pattern (FnRF). In all cases we have provided the classiﬁer with
the trials of the validation dataset and recorded the corresponding possibilities.
The motion pick up a glass (drink) was never misclassiﬁed as a false positive
and thus has no entries for the columns FRF and FnRF. The high percentages
in columns TRF and FnRF of Table 1 suggests that the rise-fall pattern can be
used to reduce the number of false positive recognitions of the classiﬁer.
Table 2. Accuracy, Precision and Recall of the possibility based classiﬁer
HMP
A
P
R
Get up from the bed
92.94%
90%
60%
Pick up a glass (drink)
96.47% 100% 82.35%
Put down a glass (drink) 96.47% 100% 82.35%
Pick up a bottle (pour) 91.76% 77.78% 82.35%
Put down a bottle (pour) 88.24% 76.92% 62.50%
Sit down on a chair
92.94% 100%
64.7%
Stand up from a chair
98.82% 100% 94.74%
AVERAGE
93.95% 92.1% 75.57%
Table 2 reports the accuracy (A), precision (P) and recall (R) of the recogni-
tion system implementing Algorithm 1. As expected, motions with higher values
of TRF in Table 1 also have higher recall values in Table 2, and vice versa. A
comparison with [15] shows that explicit reasoning about uncertainty results in
a signiﬁcant increase in both the recognition accuracy and precision.
Finally, Figure 3 reports the results of a real-time test in which a user performs
a full sequence of motion activities. The ﬁgure shows the possibility values for
each model computed by the possibility based classiﬁer at each time instant.
The results indicate that the correlation between the rise-fall pattern and true
positive recognitions holds valid also in the case of sequences of motion.
It should be noted that all the motions considered in our experiments are
one-shot. For cyclic motions, like climb the stairs and walk, the simple rise-fall
pattern would not apply. We expect that a similar analysis on the temporal

604
B. Bruno et al.
0
500
1000
1500
2000
2500
3000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
time [windows]
possibility
 
 
climb
pickUpDrink
putDownDrink
pickUpPour
putDownPour
sit
stand
walk
Fig. 3. Output of the possibility based classiﬁer during an execution of the sequence
of actions: pick up a glass (drink), put down a glass (drink), pick up a glass (drink),
put down a glass (drink), stand up from a chair, sit down on a chair, pick up a glass
(drink), put down a glass (drink)
evolution of the possibility values can still be made, though, based on their
periodic behavior. The validation of this hypothesis is part of our current work.
5
Conclusions
We have described a framework for the recognition of simple human activities
based on accelerometer data. Our framework extends classical approaches by
modelling uncertainty in recognition through possibility values, and by reasoning
about the temporal patterns of these values. Experimental results show that our
approach can lead to a signiﬁcant increase in recognition accuracy and precision.
One of the tenets of our approach is that we can add reasoning about un-
certainty on top of existing tools. Speciﬁcally, we did not replace the existing
classiﬁer, e.g., by designing a classical fuzzy classiﬁer. Rather, we modiﬁed it
in a minimal way in order to compute possibility values associated to its deci-
sions. Once we do that, we can then design a downstream reasoner module that
generates the ﬁnal decisions based on the analysis of these values and of their
temporal evolution. We expect that this modular approach may be proﬁtably
applied to other domains as well.
References
1. Katz, S., Chinn, A., Cordrey, L.: Multidisciplinary studies of illness in aged persons:
a new classiﬁcation of functional status in activities of daily living. J. Chron.
Dis. 9(1), 55–62 (1959)

Fuzzy Logic in Classiﬁcation of Human Motion Primitives
605
2. Mastrogiovanni, F., Scalmato, A., Sgorbissa, A., Zaccaria, R.: An integrated ap-
proach to context speciﬁcation and recognition in smart homes. In: Helal, S., Mitra,
S., Wong, J., Chang, C.K., Mokhtari, M. (eds.) ICOST 2008. LNCS, vol. 5120, pp.
26–33. Springer, Heidelberg (2008)
3. Aggarwal, J., Ryoo, M.: Human activity analysis: a review. ACM Comput.
Surv. 43(3), 16:1–16:43 (2011)
4. Cirillo, M., Pecora, F., Saﬃotti, A.: Proactive Assistance in Ecologies of Physi-
cally Embedded Intelligent Systems – A Constraint-Based Approach. In: Mastro-
giovanni, F., Chong, N.Y. (eds.) Handbook of Research on Ambient Intelligence
and Smart Environments. Information Science Reference, pp. 534–557. IGI Global,
Hershey (2011)
5. Lara, O.D., Labrador, M.A.: A survey on human activity recognition using wear-
able sensors. IEEE Communications Surveys & Tutorials 15(3), 1192–1209 (2012)
6. Bao, L., Intille, S.S.: Activity recognition from user-annotated acceleration data.
In: Ferscha, A., Mattern, F. (eds.) PERVASIVE 2004. LNCS, vol. 3001, pp. 1–17.
Springer, Heidelberg (2004)
7. Maurer, U., Rowe, A., Smailagic, A., Siewiorek, D.: Location and activity recogni-
tion using eWatch: A wearable sensor platform. In: Cai, Y., Abascal, J. (eds.) Am-
bient Intelligence in Everyday Life. LNCS (LNAI), vol. 3864, pp. 86–102. Springer,
Heidelberg (2006)
8. Allen, F., Ambikairajah, E., Lovell, N., Celler, B.: Classiﬁcation of a known se-
quence of motions and postures from accelerometry data using adapted gaussian
mixture models. Physiol. Meas. 27(10), 935–953 (2006)
9. Lee, M., Khan, A., Kim, T.: A single tri-axial accelerometer-based real-time per-
sonal life log system capable of human activity recognition and exercise information
generation. Personal and Ubiquitous Computing 15(8), 887–898 (2011)
10. Krassnig, G., Tantinger, D., Hofmann, C., Wittenberg, T., Struck, M.: User-
friendly system for recognition of activities with an accelerometer. In: Int. Conf.
on Pervasive Computing Technologies for Healthcare, pp. 1–8. IEEE Press, New
York (2010)
11. Minnen, D., Starner, T.: Recognizing and discovering human actions from on-body
sensor data. In: IEEE Int. Conf. on Multimedia and Expo, pp. 1545–1548. IEEE
Press, New York (2005)
12. Bruno, B., Mastrogiovanni, F., Sgorbissa, A., Vernazza, T., Zaccaria, R.: Human
motion modelling and recognition: A computational approach. In: IEEE Int. Conf.
on Automation Science and Engineering (CASE), pp. 156–161. IEEE Press, New
York (2012)
13. Pecora, F., Cirillo, M., Dell’Osa, F., Ullberg, J., Saﬃotti, A.: A constraint-based
approach for proactive, context-aware human support. Journal of Ambient Intelli-
gence and Smart Environments 4, 347–367 (2012)
14. Bloch, I., Hunter, A.: Fusion: general concepts and characteristics. Int. J. of Intel-
ligent Systems 16(10), 1107–1134 (2001)
15. Bruno, B., Mastrogiovanni, F., Sgorbissa, A., Vernazza, T., Zaccaria, R.: Analysis
of human behavior recognition algorithms based on acceleration data. In: IEEE
Int. Conf. on Robotics and Automation (ICRA), pp. 1602–1607. IEEE Press, New
York (2013)
16. Zadeh, L.A.: Fuzzy Sets as a Basis for a Theory of Possibility. Fuzzy Sets and
Systems 1, 3–28 (1978)
17. Ruspini, E.H.: On the semantics of fuzzy logic. Int. J. of Approximate Reasoning 5,
45–88 (1991)

A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 606–615, 2014. 
© Springer International Publishing Switzerland 2014 
Optimization of Human Perception on Virtual Garments 
by Modeling the Relation between Fabric Properties  
and Sensory Descriptors Using Intelligent Techniques 
Xiaon Chen1,2, Xianyi Zeng1,2, Ludovic Koehl1,2,  
Xuyuan Tao1,2, and Julie Boulenguez-Phippen1,2,3 
1 Université Lille 1 Sciences et Technologies, 59655 Lille, France 
2 GEMTEX, ENSAIT, 2 allée Louise et Victor Champier, 59056 Roubaix Cedex 1, France 
3 GEMTEX, HEI, 13 rue de Toul, 59046 Lille cedex, France 
Abstract. 3D virtual garment design using specific computer-aided-design 
software has attracted a great attention of textile/garment companies. However, 
there generally exists a perceptual gap between virtual and real products for 
both designers and consumers. This paper aims at quantitatively charactering 
human perception on virtual fabrics and its relation with the technical parame-
ters of real fabrics. For this purpose, two sensory experiments are carried out on 
a small number of fabric samples. By learning from the identified input (tech-
nical parameters of the software) and output (sensory descriptors) data, we set 
up a series of models using different techniques. The fuzzy ID3 decision tree 
model has shown better performance than the other ones.  
Keywords: virtual garment, human perception, CAD, sensory evaluation, fuzzy 
ID3. 
1 
Introduction 
Under the worldwide economic pressure, there is a strong need for industrial enter-
prises to quickly design new various products with short life cycles and low costs 
meeting personalized requirements of consumers in terms of functionalities, comfort 
and fashion style [1]. Currently, 3D virtual garment design using specific computer-
aided-design (CAD) software has attracted a great attention of textile/garment com-
panies. Virtual garment design can be considered as an optimal combination of  
designers, computer technology and animation technology, permitting to realize and 
validate design ideas and principles within a very short time [2]. New knowledge and 
design elements on garments can be obtained from human-machine interactions. The 
application of virtual technology in garment industry can effectively accelerate new 
product development and reduce design and production cost in order to enhance the 
competitiveness of garment companies in the worldwide market [3].  
Several popular garment CAD software systems supporting virtual garment colla-
borative design have been developed by Lectra Company in France (Modaris 3D Fit) , 
OptiTex, Clo3d and others. In garment industry, the most appreciated CAD systems 

 
Optimization of Human Perception on Virtual Garments by Modeling the Relation 
607 
are based on mechanical models, built according to the mechanical properties of real 
cloth measured on devices such as KES (Kawabata evaluation system) and FAST 
(Fabric Assurance by Simple Testing). These models can effectively simulate fabric 
deformable structures and be accurate enough to deal with nonlinearities and defor-
mations occurring in cloth, such as folds and wrinkles. However, as these models are 
generated from mechanical laws such as the finite element method and particle sys-
tem, there generally exists a perceptual gap between virtual and real products for both 
designers and consumers. In this context, the characterization of human perception on 
virtual garments with different styles and different fabric materials has become a very 
important element for the success of new garment design. In practice, the perception 
of virtual garments can be modified by adjusting the technical parameters of the cor-
responding garment CAD software so that their perceptual effects are as close as 
possible to those of real garments. In this way, designers can control some sensory 
criteria such as softness, smoothness and draping effects in virtual products according 
to their requirements. The perceptual quality of textile materials and garments is 
mainly related to two components, i.e. tactile and visual properties. Visual properties 
essentially include fabric appearance, color, fashion style and garment fitting effects 
while tactile properties (hand feeling) are generally related to the nature of materials. 
Researches have shown that a significant portion of tactile properties can be perceived 
by human’s eyes. Thus, creating a quantitative model characterizing the relation be-
tween technical parameters and human perception on virtual garments is a very signif-
icant approach for designing new user-oriented products.  
Many researches have studied the relation between technical parameters and sen-
sory descriptors on real fabrics. The classical analysis methods such as linear regres-
sion, multiple factor analysis, PCA (principal component analysis) are usually used 
tools for that. Also, intelligent techniques, including artificial neural network [4], 
genetic algorithm [5], fuzzy inference systems, decision tree (such as ID3 [6], CART) 
and their combinations are applied and found efficient. Recently, a fuzzy decision tree 
like Fuzzy-ID3 algorithm [7] as an extension of ID3  has been studied, which sup-
ports not only symbolic or discrete data but also linguistic data (e.g. small, warm, 
low) and numerical data represented by fuzzy sets.  
This paper aims at charactering human perception on virtual products and its rela-
tion with the technical parameters of real fabrics. For this purpose, two sensory expe-
riments are carried out on a small number of fabric samples displayed on a cylinder 
drapemeter. The target of the first experiment is to find the most appropriate combina-
tion of the parameters of the CAD software permitting to minimize the overall percep-
tual difference between real and virtual fabric draping. The second sensory experiment 
is aimed to extract normalized tactile and visual sensory descriptors characterizing 
human perception on the concerned fabric samples. In product design, sensory descrip-
tors can be considered as one part of fabric features and used for communications  
between designers and consumers. By learning from the identified inputs (technical 
parameters of the software) and outputs (sensory descriptors) data, a series of models 
using different techniques, including multi-linear regression, neural network and fuzzy 
ID3 decision tree are setup. A comparative analysis of the 3 methods is given at the 
end of this paper. 

608 
X. Chen et al. 
2 
Virtual Fabric and Sensory Tests 
2.1 
Software for Virtual Fabric Realization 
Of all the well-known commercialized 3D garment CAD software systems supporting 
virtual collaborative design, Clo3d, developed by South Korean company CLO Vir-
tual Fashion, is used in our study for creating virtual fabrics. This selection is not only 
related to its powerful simulation speed and high garment rendering quality, but also 
due to its capacity of supporting both static and dynamic effects of the 3D garment 
simulation. For virtual fabrics, the working scheme of Clo3d is described in Fig.1. 
Mechanical and optical parameters are required as inputs to generate the behavior of 
output virtual fabric. 
 
Fig. 1. A scheme of the Clo3D software for generating virtual fabrics 
In order to realize virtual fabrics using the Clo3D software, two categories of pa-
rameters, i.e. optical and mechanical parameters, are required as inputs. The optical 
parameters include texture picture, color, brilliance, and transparence while the me-
chanical ones are composed of fabric stretch resistance, shear resistance, basic mass 
and others. Those parameters are combined and integrated into the nonlinear models 
of the software in order to generate fabric deformations, folds and wrinkles.  
Different from some classical garment CAD software systems which require pre-
cise values of mechanical parameters as input variables, most of the parameters of 
Clo3D only require relative values from 0 to 99 with respect to a great number of 
standard references. In this context, the precise values measured on KES or FAST 
devices are not necessary and the software is then more adapted to industrial compa-
nies in terms of cost and time. For determining relative values of the optical and  
mechanical parameters of each fabric sample, we introduce a design of sensory expe-
riments in order to minimize static and dynamic perceptions between virtual fabrics 
generated by different values of the parameters and the real ones. 
2.2 
Virtual Fabric Realization (Sensory Experiment I) 
The proposed design of experiments is a qualitative sensory test equivalent to the 
comparison of a set of products with a standard reference in order to select the best 
one. The real fabric draping is considered as standard reference and the virtual ones, 
generated from an orthogonal table defining different combinations of parameters of 
software, are taken as products. In the qualitative sensory test, each evaluator only 
needs to compare the real fabric and each virtual product by assigning it a similarity 
degree (linguistic score). 

 
Optimization of Human Perception on Virtual Garments by Modeling the Relation 
609 
2.2.1   Fabric Real and Virtual Representation 
The static and dynamic behavior of a real fabric draping is realized on a drapemeter. 
The fabric is let dropped down freely over the drapemeter disc until it finally reaches 
a balance. The final shape of the fabric is characterized through several images taken 
by a camera from different angles. For showing optimized effects of the virtual fabric 
draping, we first extract numerical textures of the real fabric using a calibrated scan-
ner and then integrate them into the software in order to restore original fabric optical 
rendering.  
Of the 9 mechanical parameters required by the software, basic mass and thickness 
are 2 commonly used parameters in textile industry and they can be easily measured 
or provided by fabric suppliers. The other 7 parameters are determined from a series 
of human observations conducted by a design of sensory experiments. An orthogonal 
table of all the seven parameters with two levels (L16, 27) is built for setting two levels 
(minimal and maximal values of the range) of each mechanical parameter in order to 
keep the number of sensory experiments as small as possible. For simplicity, the 
processing of the optical parameters is not discussed in this paper. 
From the orthogonal table, we obtain 16 combinations of the mechanical parame-
ters for each fabric. Each combination permits to generate one virtual fabric draping 
(static and dynamic effects) using the garment CAD software. The experimental envi-
ronment (e.g. size of the drapemeter and fabric samples, gravity) is the same as that of 
the real fabric draping. As a result, for each fabric we obtain 16 virtual fabric drap-
ings. They are captured from different angles.  
2.2.2   Sensory Experiment I 
A sensory experiment is carried out in order to find the optimal combination of the 
mechanical inputs related to the real fabric draping. A panel of 10 members (4 males 
and 6 females) is recruited for evaluating the total 19 fabric samples. For each fabric, 
16 numerical slides representing 16 combinations of mechanical parameters are dif-
fused to the panelists independently, in which the photos of both real and virtual  
fabric drapings at the view of 0°, 45°, 90°, 135° and top are put together (Fig.2).  
 
Fig. 2. One example for comparing the real and virtual draping 
The panelists are first invited to have a quick overview of all the 16 slides in order 
to obtain a general impression of the fabrics. Then they are required to evaluate the 
differences between the real and virtual fabric draping for each combination of the 
parameters in the previous orthogonal table by selecting a score from 0 to 3. These 
scores are interpreted as follows: 0 - identical, 1 - close, 2 - different and 3 - very 
different. The above procedure is repeated for all the 19 fabrics. Finally, all the scores 

610 
X. Chen et al. 
are collected for each fabric. The combination of parameters corresponding to the 
minimal averaged evaluation score for all the panelists is considered as the optimized 
inputs to the software, leading the virtual fabric draping very close to the real one. 
2.3 
Tactile Perceptions Evaluation on Virtual Fabrics (Sensory Experiment II) 
In this section, the tactile perceptions are evaluated on the 19 virtual fabrics obtained 
from sensory experiment I. 6 pairs of fabric hand descriptors (Table 1) are chosen by 
experts for the evaluation experiment in brain-storm. They represent different features 
of the fabric in the field of bending, surface, stretch or assemblage of them. The selec-
tion of sensory descriptors is carried out according the following three principles. 
First, the descriptors representing the hand touch proprieties should be attractive for 
fabric developers, designers and consumers. Second, the tactile descriptors can really 
be perceived through eyes without direct touch on the real fabrics. Third, descriptors 
with ambiguity and uncertainty should be avoided. 
Table 1. 6 pairs of tactile descriptors 
Nm 
Descriptor pair 
Nm 
Descriptor pair 
Nm 
Descriptor pair 
D1 
Pliable – Stiff 
D2 
Draped – non draped 
D3 
Soft – harsh  
D4 
Smooth – rough  
D5 
Thin – thick 
D6 
Light – Heavy 
For each descriptor pair, a scale of 11 scores is used. For example, the scores of 
descriptor pair “pliable - stiff” is interpreted by: 0 - extremely pliable, 1 - very pliable, 
2 - quite pliable, 3 - fairly pliable, 4 - less than medium, 5 - medium, 6 - more than 
medium, 7 - fairly stiff, 8 - quite stiff, 9 - very stiff, 10 - extremely stiff. 
In this scenario, a panel of 12 members (7 males and 5 females) is recruited. About 
half of them are professionals in textile industry with more than 10 years experiences 
in the textile product design, development or inspection fields. The other half are re-
searchers including of professors, lecturers or PhD. students in textile universities who 
have mastered some knowledge on fabric hand properties. The training of the panelists 
before the evaluation is helpful for them to understand better the meaning of each de-
scriptor and strengthen their evaluation-related knowledge. In this stage, a number of 
real fabrics are shown to panelists, which are different from the final fabrics to be eva-
luated. Panelists are free to touch and feel the fabric hand independently. 
The panelists start by giving an overall perception of all 19 fabrics draping photos 
in the virtual environment with a view of 0°, 45°, 90°, 135° and top. Different from 
the first sensory experiment, photos of real fabric draping are no longer shown to 
them. This step is important because it can help panelists not only to have a general 
idea about the fabric features but also ‘pre-position’ each virtual fabric in the ‘scale’ 
for each descriptor. Next, during the evaluation, there is no time restriction for the 
panelists to evaluate the virtual fabric photos. Each person is free to compare, weigh 
and judge differences between any pair of the 19 fabrics. The scores of the evaluation 
result are filled on an answer sheet, and finally collected and treated using the statis-
tical method for each fabric. The averaged value given by all the panelists is the final 
result for each specific fabric and each descriptor. 

 
Optimization of Human Perception on Virtual Garments by Modeling the Relation 
611 
3 
Characterizing the Relation between Fabric Parameters and 
Sensory Descriptors 
The virtual garment software can provide a platform permitting to simulate fabric 
static and dynamic behaviors according to its real technique parameters. Perception 
on virtual fabrics as well as related virtual ambiance can be easily modified according 
to the preference of consumers by adjusting the previously identified optical and me-
chanical parameters of the 3D CAD software. In this context, a consumer-oriented 
new fabric product can be realized in this platform through interactions between fa-
shion designers, material developers and consumers. In this platform, a mathematical 
model characterizing the relation between technical parameters of fabrics and identi-
fied sensory descriptors will be helpful for optimizing these interactions in order to 
generate the most appropriate virtual fabric parameters. Their relation can be consi-
dered as a complex system in which the physically measured parameters and the  
sensory descriptors are taken as input and output variables respectively (See Fig.3). 
 
Fig. 3. The complex relation between the technical parameters and sensory descriptors 
After the previous sensory test (Experiment II), we finally obtain a (19×6)-
dimensional matrix, representing all the averaged evaluation data for the 6 pairs of 
descriptors on 19 fabric samples. They are considered as outputs of the model. The 
input data constitute a (19×9)-dimensional matrix, corresponding to the 9 identified 
technical parameters and 19 fabrics. 
For setting up an appropriate model for characterizing the relation between the 9 
technical parameters of fabrics and each sensory descriptor from the previous learning 
data, many methods can be available. Before selecting an appropriate model, a pre-
processing of the existing data is performed for checking the validity of the input 
variables, their internal correlation, and predicting the type of the modeling problem 
(linear or nonlinear) . In our study, linear regression method is first employed. 
3.1 
Linear Regression 
In our case, the 9 inputs (mechanical parameters) are denoted as A1, A2…A9 and 6 
outputs (sensory descriptor pairs) as D1, D2…D6 respectively. The method of Leave-
One-Out is used for detecting the performance of the model. Namely, of the 19  
available input-output data pairs, 18 pairs are taken as the training data to build a 
regression model. The remaining one is used for testing the performance of the model, 
expressed by the Root Mean Squared Error (RMSE). This procedure repeats 19 times 
by testing with each of the data pairs. Before the regression procedure, each of the 
input and output variables is normalized into the interval [0, 1] in order to remove the 
effects of different scales.  

612 
X. Chen et al. 
Fig. 4 shows the performance of the linear regression model characterizing the re-
lation between the physical parameters and the descriptor ‘pliable-stiff’. We can see 
that most RMSEs are around 0.2-0.4. The accuracy of the model is rather low because 
the normalized evaluation scores are included in [0, 1]. For T13 and T18, the values 
of RMSE even reach 0.8 and 0.5 respectively, meaning that there is a very large gap 
between the predicted output and the real output.  
 
Fig. 4. Performance of the linear regression model between physical parameters and descriptor 
‘pliable-stiff’ 
The previous result shows that the linear regression model cannot effectively 
process nonlinear relations existing between the technical parameters of fabrics and 
the sensory descriptors. 
3.2 
Artificial Neural Networks (ANN) 
ANN is considered as a dynamic, flexible and adaptive method with high precision 
for solving complex nonlinear prediction problems. However, it usually requires a 
great quantity of learning data for determining the appropriate parameters of an ANN 
network, including the number of layers and the number of neurons inside each layer. 
A too few number of learning data will cause “over fitting”, leading to a very instable 
performance of the model output. 
The characterization of the relation between the technical parameters of fabrics and 
one sensory descriptor is a problem of modeling with very few learning data. We need 
to set up the model of 9 input variables from the available 19 learning data. In this 
situation, we have to reduce the number of inputs in order to conform to the number 
of learning data. Principal Component Analysis (PCA) is an efficient method for re-
ducing a space of high dimensions into a low dimensional subspace while minimizing 
information loss of the original data. In our study, PCA is used to the original 9 di-
mensional input data and we find that the first 6 principle components (independent 
with each other) represent more than 95% of the original data. Next, a 3-2-1 network 
structure (3 inputs, 1 hidden layer with 2 neurons, 1 output) is built up (see Fig.5). We 
also use ‘Leave One Out’ method to test the performance of the ANN model. Its per-
formance is shown in Fig.6. 
The result shows that the combination of the ANN network and PCA cannot give 
efficient prediction results. Like the results of the linear regression, the values of 
RMSE are still around 0.2-0.4. The low accuracy is caused by 1) the information lost 
related to the application of PCA (the explanation rate of the three first principal 
components is only 60%), 2) there exist a number of local optima, preventing the 
ANN learning algorithm from converging to the global optimum.  

 
Optimization of Human Perception on Virtual Garments by Modeling the Relation 
613 
 
 
Fig. 5. ANN structure of 3-2-1 
Fig. 6. Performance of ANN model between physical 
parameters and descriptor ‘pliable-stiff’ 
In our experiments, the 3-2-1 structure of the ANN model is very stable because 
the variance of the testing results for 10 times is very small. However, when we in-
crease the number of neurons in the hidden layer, the results become more and more 
unstable due to the over fitting problem.  
3.3 
Fuzzy-ID3 Decision Tree 
Let S be a training set of examples dealing with 9 attributes or input variables (9 tech-
nical parameters) Ak’s (k=1…9), and D one pair of sensory descriptors (output varia-
ble). Each variable of Ak’s and D is normalized into the interval [0, 1] before its  
discretization and generation of the corresponding membership functions. After the 
discretization, we have A1, …, A7∈{0,1} because their corresponding values take two 
levels only. Also, we have A8, A9, D∈{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} 
because their values are continuously distributed in [0, 1]. The membership functions 
are defined in Fig.7(a) for A1, …, A7 while in Fig.7(b) for A8, A9 and D. 
 
             
 
                      (a)                                       (b) 
Fig. 7. Membership functions for different variables (a) A1…A7 (b) A8, A9and D 
Fig.8 shows an example of the fuzzy decision tree obtained by the FUZZY-ID3 al-
gorithm. It is built for the 9 input attributes and 1 output attribute (pliable-stiff). From 
this example, we can find that A8 (“basic mass”) is first selected. This selection ac-
cords with the knowledge of ‘stiffness’ because a heavy fabric with big basic mass is 
usually composed of the gross yarns or weaved by a high density structure. Next, for 
each branch of A8, the selection of further attributes is performed according to the 
computed values of Entropy E(A,S) and Gain G(A,S). Moreover, we define a thre-
shold for them, i.e. τE(A,S)=0.2 or τG(A,S)=0.1, in order to stop the generation of the 
decision tree. For example, the decision tree continues to be split when A8=0.2 and it 
selects A4 as its successive node. The further splitting node falls in A6 with 2 leaves 
when A4=0 and A8=0.2. Each final classification result (a leaf) is expressed in the 
form of membership function, which can be defuzzified for prediction.  

614 
X. Chen et al. 
 
Fig. 8. Fuzzy decision tree characterizing the relation between the physical parameters and the 
descriptor ‘pliable-stiff’ 
Considering the fact that the number of learning data is too limited, we still use the 
method of Leave-One-Out to test the effectiveness of the model. The values of RMSE 
are calculated for each experiment. As an example, the performance of the Fuzzy ID3 
model characterizing the relation between the physical properties of fabrics and the 
descriptor ‘pliable-stiff’ is shown in Fig.9. 
 
Fig. 9. The performance of Fuzzy ID3 model in the prediction of the descriptor ‘pliable-stiff’ 
We can see that for most of scenarios, the values of RMSE are less than 0.2 (the aver-
age value is 0.11), which means the predicted output is rather close to the real one. Dif-
ferent from the linear regression and the ANN method, no more ‘jump’ point can be 
found. The values of RMSE on T3 and T15 are slightly higher than 0.2. After the com-
parison with the original data, we find that the testing data with large values of RMSE are 
usually “marginal” related to the other data left in the learning database. In fact, these 
testing data represent some special properties quite different from the others. This prob-
lem can be improved by increasing the capacity and quality of the learning database. The 
more the training data is uniformly distributed, the more the model is efficient. 
The fuzzy decision tree with FUZZY-ID3 algorithm is finally proved to be an effi-
cient method for modeling with our dataset. It can model complex nonlinear relations 
of multiple inputs and single output with small quantity of learning data. Moreover, 
the fuzzy decision tree is composed of a set of IF…THEN rules, which is more inter-
pretable, flexible and robust than the other models. The problem of over-fitting can be 
effectively solved.  
4 
Conclusion 
This paper presents the method of realizing virtual fabric using a 3D garment CAD 
software. For obtaining perceptual effects of a virtual fabric very close to the corres-
ponding real fabric, we carry out a sensory experiment in order to select the most 

 
Optimization of Human Perception on Virtual Garments by Modeling the Relation 
615 
appropriate technical parameters, which constitute the input data to the software. 
Next, another sensory experiment is carried out to quantitatively characterize the hu-
man perception on the obtained virtual fabrics using a set of normalized sensory  
descriptors.   
Based on the learning data of the fabric technical parameters and the sensory data 
describing virtual fabrics, we try to find the most appropriate model in order to predict 
and control human perception on virtual fabrics by adjusting the technical parameters 
of the software. Compare with the other modeling methods, the fuzzy decision tree 
with Fuzzy-ID3 algorithm has shown its effectiveness. Robustness, flexibility and 
capacities of interpretation and processing very few data are the main advantages of 
this method. The proposed model can generate a decision tree for garment designers, 
which could effectively help them to identify technical criteria of required fabric ma-
terials satisfying a specific perceptual preference of consumers in terms of color, tex-
ture, style and even fabric hand feeling. In this context, garment consumers can be 
strongly involved in the product design process and quickly identify satisfying fabrics 
through a series of interactions with fashion designers and material developers. 
References 
1. Zeng, X., Li, Y., Ruan, D., Koehl, L.: Computational Textile. SCI, vol. 55. Springer, Hei-
delberg (2007) 
2. Volino, P., Thalmann, N.M.: Virtual clothing theory and practices. Springer (2000) 
3. Fontana, M., Rizzi, C., Cugini, U.: 3D virtual apparel design for industrial applications. 
Computer-Aided Design 37(6), 609–622 (2005) 
4. Haykin, S.O.: Neural Networks and Learning Machines, 3rd edn. Pearson (2008) 
5. Ruan, D.: Intelligent hybrid systems: fuzzy logic neural networks, and genetic algorithms. 
Kluwer, Boston (1997) 
6. Quinlan, J.R.: Induction on Decision Trees. Machine Learning 1, 81–106 (1986) 
7. Bartczuk, Ł., Rutkowska, D.: A new version of the fuzzy-ID3 Algorithm. In: Rutkowski, L., 
Tadeusiewicz, R., Zadeh, L.A., Żurada, J.M. (eds.) ICAISC 2006. LNCS (LNAI), 
vol. 4029, pp. 1060–1070. Springer, Heidelberg (2006) 
 
 

Customization of Products
Assisted by Kansei Engineering,
Sensory Analysis and Soft Computing
Jose M. Alonso, David P. Pancho, and Luis Magdalena
European Centre for Soft Computing, 33600 Mieres, Asturias, Spain
{jose.alonso,david.perez,luis.magdalena}@softcomputing.es
Abstract. This paper presents a new methodology aimed at making
simpler the product/market ﬁt process. We propose a user-centered ap-
proach inspired on the Oriental philosophy that is behind Kansei Engi-
neering. In essence, we advocate for customization of products guided by
users’ expectations. Our proposal combines Sensory Analysis and Soft
Computing techniques in order to uncover what users think but also
what they feel and desire when facing new products. That is elicitation
of the so-called kanseis or “psychological feelings”. Then, we can design
new prototypes that truly matter to people because they ﬁt the deepest
users’ demands. Thus, improving innovation and marketing success rate.
We have illustrated the details of our proposal in a case study related to
gin packaging.
Keywords: Fuzzy Logic, Sensory Analysis, Consumer Analysis, Kansei,
Iterative Design, Marketing, Gin Packaging.
1
Introduction
Nine out of ten startups fail [11]. The success of business model innovation
comes up with one of the following strategies [18]: (1) to satisfy existing but
unanswered market needs; (2) to bring new technologies, products, or services to
market; (3) to improve, disrupt, or transform an existing market with a better
business model; or (4) to create an entirely new market.
This paper tries to yield some light regarding the ﬁrst two strategies. Unfor-
tunately, many new products fail mainly because consumers’ desires and needs
are not clearly identiﬁed on advance [8]: Marketing research should be employed
as a tool to qualitatively and quantitatively specify the product’s role among con-
sumers.
Hence, market research becomes crucial in the product/market ﬁt process [20].
However, eliciting consumers’ expectations is not straightforward. According to
Maslow, human motivations generally move up through a hierarchy of needs that
can be represented by a pyramid with the most basic needs at the bottom [10].
They are physiological needs such as breathing, food, sleep, and so on. Once
these basic needs are satisﬁed then more and more complex needs turn up (safety,
belongingness and love, esteem, self-actualization and self-transcendence).
A. Laurent et al. (Eds.): IPMU 2014, Part II, CCIS 443, pp. 616–625, 2014.
c
⃝Springer International Publishing Switzerland 2014

Customization of Products
617
Consumers look for products satisfying their own needs. Therefore, consumers’
expectations about new products can be seen in a pyramid inspired on Maslow’s
hierarchy of needs. The basic expectations appear at the bottom (security,
healthy, etc.) but more and more demanding expectations (functionality, us-
ability, and so on) turn up while reaching the top (full aﬃective satisfaction).
This paper presents a user-centered methodology for increasing the success
rate of introducing new products into market. As a side eﬃect, the consumer rela-
tionship management is also improved because we oﬃer to consumers what they
actually need. The proposed methodology combines tools provided by Kansei
Engineering, Sensory Analysis, and Soft Computing.
The rest of the manuscript is organized as follows. Section 2 presents some
preliminaries. Section 3 introduces the proposed framework for customizing prod-
ucts. Section 4 describes a case study which illustrates some of the main beneﬁts
coming out from our proposal. Finally, some conclusions and future works are
pointed out in Section 5.
2
Preliminaries
2.1
Kansei Engineering
Kansei Engineering establishes a framework to formalize not only what con-
sumers verbalize but what they actually sense when exposed to new prod-
ucts [7,13,14]. The translation of Japanese word kansei is not straightforward.
It represents a kind of psychological thinking/feeling with deep roots in the
Japanese culture. Thus, kansei corresponds to aﬃection, feeling and/or emo-
tion [21]. Nagamachi, who is world-wide recognized as the father of Kansei En-
gineering, started with the so-called Jocho technology which focuses on dealing
with emotions [13]. However, he understood quickly that customized product
design is a multidisciplinary challenge in which considering only emotions is
not enough. That is why Nagamachi has actively promoted Kansei Engineering
since the 1970’s [14]. Kansei Engineering is aimed at designing products that
consumers will enjoy and be satisﬁed with because human feelings and emotions
(subjective consumer insights) are driving the design process [15]. Nowadays,
Kansei Engineering is still growing all around the world but the emphasis is on
the interaction with complementary methodologies and technologies [16].
2.2
Sensory and Consumer Analysis
Sensory analysis is the science of sensory measurements where diﬃerent tech-
niques have tackled for years with consumers’ expectations, likes and dislikes [6,
12], yielding powerful consumer analysis techniques [12]. Sensory sciences [24]
have deep roots into physiology, psychology and psychophysics. They apply prin-
ciples of experimental design and statistical analysis to evaluate consumer prod-
ucts [12]. Moreover, it is important to remark that consumer analysis studies are
not reproducible in nature contrary to sensory measurements.

618
J.M. Alonso, D.P. Pancho, and L. Magdalena
Sensory data coming from human senses (sight, smell, taste, touch and
hearing) can be translated into valuable information for companies in order to
reinforce/discard what they believe to know about consumer behavior [5]. There-
fore, most large companies have departments devoted to sensory analysis what
is becoming a key part of their business strategy. They are actually multidisci-
plinary teams which include sensory professionals, brand managers, marketing
researchers, engineers, psychologists, etc. The goal is to increase their knowledge
about consumers’ needs and expectations with the aim of predicting consumers’
responses. To do so, sensory evaluation is carried out by trained and/or un-
trained panels of human assessors who, for instance, are in charge of testing the
new products before they are introduced into market [3].
2.3
Soft Computing
The term Soft Computing was coined in the 1990’s by L. A. Zadeh [28] who was
already world-wide recognized as father of Fuzzy Logic [26]. It raised rapidly the
interest of many other researchers [1]. There are several deﬁnitions but prob-
ably the most popular ones describe Soft Computing in terms of its essential
properties, as a complement of hard computing, as a tool for coping with im-
precision and uncertainty, and as a family of complementary techniques (Fuzzy
Logic, Neuro-computing, Probabilistic Reasoning, Evolutionary Computation,
etc.) which are able to solve lots of complex real-world problems for which clas-
sical techniques cannot give accurate results [9]. The constituent technologies
are well-known because of their complementary and cooperative nature, i.e., be-
cause of their ability to work in a cooperative way, taking proﬁt from the main
advantages (and overcoming the main drawbacks) of each other.
3
Proposal
Qualitative and quantitative assessment of consumers’ insights into products
is inherently subjective. It becomes a key task for designing products ﬁtting
the market but it is not straightforward. Soft Computing techniques are well-
known because of their ability to tackle with imprecision and uncertainty in
system identiﬁcation [27] but also due to their suitability for computing with
perceptions [29,30]. Vague concepts (such as tasteful, elegant, exclusive, and so
on) can be formalized, in an approximate but even precise way, thanks to the
tools provided by Soft Computing. In consequence, Soft Computing techniques
can assist both Sensory Analysis and Kansei Engineering in the design and
customization of products.
This section presents a novel framework (Fig. 1) aimed at simplifying both
product/market ﬁt and customer relationship management processes.
First of all, a product speciﬁcation is given to the designer (that is, the per-
son in charge of designing the new product). The speciﬁcation normally includes
technical requirements (regarding security, usability, etc.) but also a set of de-
sired attributes or insights (usually deﬁned by vague concepts like originality,

Customization of Products
619
Product
Specification
Prototype
Design
Experimental
Setup
Sensory
Evaluation
Intelligent
Data Analysis
Fulfill 
Specification?
Final Product
Design
Commercialization
and Marketing
Yes
No
Iterative Design Process
Fig. 1. Proposed framework
exclusiveness, and so on). The product is expected to satisfy consumers’ desires
and needs identiﬁed on advance, in a careful market study. The selected insights
correspond to the so-called kanseis and are usually characterized by the well-
known Osgood’s semantic diﬃerential space [17,23]. Notice that, the selection of
the right kanseis (which are going to guide the synthesis of the new product) is
a key task deserving careful attention [16].
Then, the designer has to look for the best combination of design parameters
(that we call “prototype labels” and can be deﬁned either numerically or lin-
guistically depending on their nature) to satisfy the product speciﬁcation. The
right labels can be uncovered in an iterative design process (the core of Fig. 1)
which is made up of four main stages:
1. Prototype Design. The given set of labels (such as height, weight, color,
etc.) is translated into a set of prototypes to be validated. Each label is
deﬁned in a range of values, thus several alternative prototypes are built (all
of them deﬁned by selecting random values inside the range of interest of
each label, for instance height between 10 and 20 cm in case of a bottle).
2. Experimental Setup. This stage requires a strong statistical background
with the aim of setting up the right experiment. Firstly, we must establish the
set of kanseis to evaluate. Then, we have to select a panel of assessors. The
selection of a good panel becomes essential and it depends on the kind of test
to perform but also on the target market. In addition, a minimum number of
assessors are required in order to provide results with statistical signiﬁcance.
Finally, we should collect a set of samples that is representative enough.
Samples can include products provided by rival companies and/or alternative
prototypes. Notice that we have to set the right number of samples but also to
select their most suitable way of representation. Each sample can consist on
a physical object but also on an image/audio/video describing it. In the case
of opinion polls they are usually carried out by means of web questionnaires.
They can include a set of questions with diﬃerent kinds of possible answers:
yes/no, multiple options, checkbox, open text, ranking, rating scale, etc.
3. Sensory Evaluation. The type of evaluation depends on the product (along
with the set of kanseis) under consideration. An example of pure sensory
evaluation (involving human senses like smelling, tasting or touching) is the

620
J.M. Alonso, D.P. Pancho, and L. Magdalena
assessment of food products. However, eyesight and hearing are also im-
portant regarding other kind of products. We propose here a new kind of
question which is answered by means of fuzzy numbers deﬁned in a bipolar
rating scale (for instance Worthless-Valuable, Ordinary-Unique, and so on).
It can be seen as an extension of the popular Osgood’s semantic diﬃerential
scale. Nevertheless, the assessor is not asked for choosing one option from a
pre-deﬁned set of options (1 / 2 / 3 ; Very low / Low / Average / High /
Very high; etc.) but he/she is asked for a value in a continuous range along
with the uncertainty degree of such value.
(a) Low exclusiveness (doubtful assessor)
(b) High exclusiveness (self-conﬁdent assessor)
Fig. 2. Interpretation of fuzzy answers
Fig. 2 shows an illustrative example. Exclusiveness is assessed in a contin-
uous range from Ordinary to Unique. In the case of the picture at the top
(Fig. 2(a)) the assessor gives a small value to Exclusiveness. That is, the
upper pointer is closer to Ordinary. Moreover, the assessor’s answer is not
very trustful as it can be appreciated by the fact that the interval deﬁned
by the lower pointers (which express the uncertainty degree of the asses-
sors’ answer) is quite large. The larger the interval the higher imprecision
and uncertainty are attached to the given answer which is very likely to be
determined by the upper pointer but assuming that in practice it can take
whatever value in the whole interval. Notice that, the three pointers deﬁne
a fuzzy number. The picture at the bottom (Fig. 2(b)) shows the answer
provided by a self-conﬁdent assessor because the related uncertainty degree
is small. Furthermore, that assessor regards the sample under consideration
as somehow exclusive (the upper pointer is closer to Unique).
4. Intelligent Data Analysis. Here, the focus is at processing and analyzing
all the answers collected in the previous stage. To do so, we propose the use
of Soft Computing techniques, mainly fuzzy logic [26], in order to tackle with
the imprecision and uncertainty inherent to the assessors’ evaluations which
are clearly subjective. To start with, we have to check the goodness and relia-
bility of the available data. Outliers, rare answers very far from the rest, must
be identiﬁed and discarded, in the search for consensus. In addition, assessor

Customization of Products
621
proﬁles can be compared and grouped according to diﬃerent socio-economic
criteria. Afterwards, all collected answers for each sample should be properly
aggregated [2]. Finally, we can consider learning algorithms, mainly fuzzy mod-
eling [19,25] and fuzzy association rules [4], in order to match labels (deﬁning
the product) and kanseis (characterizing the customers’ expectations).
The iterative design process ends when at least one prototype fulﬁlls the
product speciﬁcation or no further improvement is achieved in comparison with
the previous iteration. The deﬁnition of labels is reﬁned, thus their range of
possible values is reduced, at the end of each iteration. Therefore, prototypes
are expected to be closer to consumers’ demands in each new iteration. Once we
have a prototype satisfying the given product speciﬁcation it is time to transform
it into a ﬁnal product ready for the market.
4
An Illustrative Case Study
This section presents a case study aimed at illustrating how the framework
previously introduced works in practice. For the sake of clarity, we concentrate
just in the last iteration of the iterative design process. Thus, we compare several
rival commercial products against the ﬁnal prototype. Moreover, notice that
product speciﬁcation is given on advance while details related to the ﬁnal product
design, commercialization and marketing remain secret1. The four stages in the
proposed iterative design process are detailed below:
1. Prototype Design. Our case study deals with gin packaging design. We
analyzed a representative sample of well-known gin bottles (Fig. 3). Nowa-
days, choosing the right packaging has a great impact for selling products:
With packaging being now a representation of the product, the development
of eﬀective and attractive packaging is critical to the ability to distribute and
deliver food that satisﬁes consumers [8].
2. Experimental Setup. We considered a panel made up of 50 untrained as-
sessors (62% men and 38% women). They were asked to assess 24 gin bottles
according to formal and aesthetic criteria. Notice that they had to give their
opinion about the bottle design, no matter their drinking preferences.
3. Sensory Evaluation. Each assessor had to evaluate ﬁve kanseis (Glamour,
British Personality, Gender, Exclusiveness, and Originality) for each bottle
through an opinion web poll. Fig. 4 shows an example of answers related to
the design prototype.
4. Intelligent Data Analysis. We ﬁrst checked all the collected answers look-
ing for outliers. In search of consensus, we discarded the most doubtful an-
swers, i.e. those above the third quartile regarding the related uncertainty
degree. Then, we grouped answers according to seven usual linguistic terms
(from VL=“Very Low” to VH=“Very High”, through A=“Average”) repre-
sented by a uniform strong fuzzy partition (Fig. 5(a)).
1 This study is part of a real project subject to preserve some conﬁdential information.
That is why we cannot reveal prototype design details.

622
J.M. Alonso, D.P. Pancho, and L. Magdalena
Fig. 3. Bottles of gin under consideration
Fig. 4. Screenshot of the webpoll
In order to provide a linguistic interpretation of the collected answers, we
computed the similarity [22] of each fuzzy answer (A) with respect to each
fuzzy set (B) in Fig. 5(a):
S(A, B) = 1 −|A ∩B|
|A ∪B|
(1)
where A and B are the two fuzzy sets to be compared, |·| denotes the car-
dinality of a set, and the ∩and ∪operators represent the intersection and
union, respectively. Notice that, the linguistic term related to the fuzzy set
B yielding the highest similarity to A is selected for each answer.
Fig. 5(b) shows a histogram with the percentage of answers assigned to
each linguistic term (regarding our prototype design in Fig. 4). Most assessors

Customization of Products
623
0.0
1.0
0.5
VL           L               AL              A              AH              H           VH
(a) Strong fuzzy partition with seven labels
(b) Collected answers after grouping by linguistic terms
Fig. 5. Linguistic interpretation of collected answers
gave high score (between AH and VH) to all kanseis except to Gender. Com-
paring all 24 bottles under consideration, the prototype was ranked as the sec-
ond more Glamorous, the ninth more British, the second more Female, the
sixth more Innovative, and the fourth more Unique. It is worthy to remark
that product speciﬁcation demanded a gin packaging expected to be Innova-
tive, Glamorous, and Unique, but also to be considered as quite British and
somehow Male. Each kansei is given by an expert panel and has attached a de-
gree of importance expressed in [0,1]: Glamour=0.9; British Personality=0.6;
Gender=0.3; Exclusiveness=0.7; and Originality=0.8. Our prototype design
was the ﬁrst ranked when considering all ﬁve kanseis together (in a weighted
linear combination). Hence, product speciﬁcation was successfully fulﬁlled.

624
J.M. Alonso, D.P. Pancho, and L. Magdalena
5
Conclusions and Future Work
We have introduced a new framework for human-centered design of products
that is supported by three well-known and established research ﬁelds: Kansei
Engineering, Sensory Analysis and Soft Computing.
It has already been applied in a real case study about gin packaging. We have
reported some preliminary results which only illustrates a small part of the entire
framework that is only the top of the iceberg. A lot of work is in progress.
In the near future we will extend this preliminary study by exploring diﬃerent
groups of users with clustering techniques, trying several aggregation operators,
exploring alternate graphical representations, looking for correlated answers,
learning rules (fuzzy association rules, fuzzy rule bases, and so on) support-
ing the design of new prototypes, statistical tests, etc. We also plan to apply our
framework in other related case studies.
Acknowledgments. This work has been funded by the Spanish Ministry of
Economy and Competitiveness under ABSYNTHE project (TIN2011-29824-C02-
01 and TIN2011-29824-C02-02) and by the Principality of Asturias Government
under the project with reference CT13-53. Regarding the case study, we would
like to thank “Rød brand consultans” and all the involved anonymous assessors.
With respect to the prototype design, we are grateful to Valent´ın Iglesias.
References
1. Bonissone, P.: Soft computing: The convergence of emerging reasoning technologies.
Soft Computing 1, 6–18 (1997)
2. Bouchon-Meunier, B.: Aggregation and fusion of imperfect information. Physica-
Verlag, Heidelberg (1997)
3. Carter, C., Riskey, D.: The roles of sensory research and marketing research in
bringing a product to market. Food Technology 44(11), 160–162 (1990)
4. Delgado, M., Mar´ın, N., S´anchez, D., Vila, M.A.: Fuzzy association rules: General
model and applications. IEEE Trans. on Fuzzy Systems 11(2), 214–225 (2003)
5. Douglas, S.P., Craig, C.S.: The changing dynamic of consumer behavior. Implica-
tions for cross-cultural research. International Journal of Research in Marketing 14,
379–395 (1997)
6. Galmarini, M.V., Symoneaux, R., Chollet, S., Zamora, M.C.: Understanding apple
consumers’ expectations in terms of likes and dislikes. Use of comment analysis in
a cross-cultural study. Appetite 62, 27–36 (2013)
7. Ikeda, G., Nagai, H., Sagara, Y.: Development of food kansei model and its ap-
plication for designing tastes and ﬂavors of green tea beverage. Food Science and
Technology Research 10(4), 396–404 (2004)
8. Lord, J.B.: New product failure and success. In: Developing New Food Products
for a Changing Marketplace, pp. 4.1–4.32. CRC Press (1999)
9. Magdalena, L.: What is soft computing? revisiting possible answers. International
Journal of Computational Intelligence Systems 3(2), 148–159 (2010)
10. Maslow, A.H.: A theory of human motivation. Psychological Review 50(4), 370–396
(1943)

Customization of Products
625
11. Maurya, A.: Running Lean: Iterate from plan A to a plan that works. O’Reilly Vlg.
Gmbh & Co. (2010)
12. Naes, T., Brockhoﬀ, P.B., Tomic, O.: Statistics for sensory and consumer science.
Wiley (2010)
13. Nagamachi, M.: A study of emotional technology. Japanese Journal of Er-
gonomics 10(2), 121–130 (1974)
14. Nagamachi, M.: Kansei Engineering. Kaibundou, Tokyo (1989)
15. Nagamachi, M.: Kansei engineering: The implication and applications to product
development. In: IEEE International Conference on Systems, Man, and Cybernet-
ics, pp. 273–278 (1999)
16. Nagamachi, M.: Perspectives and the new trend of kansei/aﬀective engineering.
TQM 20(4), 290–298 (2008)
17. Osgood, C.E., Suci, G., Tannenbaum, P.: The measurement of meaning. University
of Illinois Press (1957)
18. Osterwalder, A., Pigneur, Y.: Business model generation. John Wiley & Sons, Inc.
(2010)
19. Pedrycz, W., Gomide, F.: Fuzzy modeling: Principles and methodology. In: Fuzzy
Systems Engineering: Toward Human-Centric Computing (2007)
20. Schneider, J., Hall, J.: The new launch plan: 152 tips, tactics and trends from the
most memorable new products. BNP Publisher (2010)
21. Sch¨utte, S.: Engineering emotional
values in
product design. PhD thesis,
Link¨opings Universitet, Kansei Engineering Group (2005)
22. Setnes, M., Babuska, R., Kaymak, U., Van Nauta Lemke, H.R.: Similarity measures
in fuzzy rule base simpliﬁcation. IEEE Trans. on Systems, Man and Cybernetics,
Part B 28(3), 376–386 (1998)
23. Snider, J.G., Osgood, C.E.: Semantic diﬀerential technique: A sourcebook. Aldine,
Chicago (1969)
24. Stone, H., Bleibaum, R.N., Thomas, H.A.: Sensory evaluation practices. Food Sci-
ence and Technology, 4th edn. International Series. Academic Press (2012)
25. Yager, R.R., Filev, D.P.: Essentials of fuzzy modeling and control. John Wiley,
New York (1994)
26. Zadeh, L.A.: Fuzzy sets. Information and Control 8, 338–353 (1965)
27. Zadeh, L.A.: Outline of a new approach to the analysis of complex systems and
decision processes. IEEE Trans. on Systems, Man, and Cybernetics 3, 28–44 (1973)
28. Zadeh, L.A.: Soft computing and fuzzy logic. IEEE Software 11(6), 48–56 (1994)
29. Zadeh, L.A.: From computing with numbers to computing with words - from ma-
nipulation of measurements to manipulation of perceptions. IEEE Trans. on Cir-
cuits and Systems - I: Fundamental theory and applications 45(1), 105–119 (1999)
30. Zadeh, L.A.: A new direction in AI toward a computational theory of perceptions.
AI Magazine, American Association for Artiﬁcial Intelligence 1, 73–84 (2001)

Author Index
Aguzzoli, Stefano
II-365
Al Alwani, Adnan
II-106
Alcalde, Cristina
III-31
Aldavero, Cristina
II-586
Almeida, Rui Jorge
I-567
Alonso, Jose M.
II-616
Anderson, Derek T.
I-206
Antonucci, Alessandro
III-456
Arieli, Ofer
II-194
Aryadinata, Yogi Satrya
II-414
Asiain, Mar´ıa Jos´e
III-252
Asmuss, Svetlana
III-317
Aupetit, Micha¨el
I-588
Bacovsk´y, Martin
II-274
Baczy´nski, Michal
I-158
Bahri, Oumayma
II-264
Ban, Adrian I.
II-254
Banerjee, Mohua
I-335
Bannay, Florence
II-335
Barcellos, C.
II-414
Barcenilla, Javier
II-135
Barghout, Lauren
II-163
Barrenechea, Edurne
II-475, III-296
Bartl, Eduard
III-81
Bartoszuk, Maciej
III-21
Basileu, Cynthia
II-116
Bedregal, Benjam´ın
III-252
Beliakov, Gleb
III-364
Ben Amor, Nahla
II-264, III-269
Ben Amor, Souﬁan
II-116
Ben Hariz, Narjes
III-486
Ben Yaghlane, Boutheina
I-46, II-66,
III-486
Ben Yahia, Sadok
I-25
Besnard, Philippe
II-345
Bianchi, Matteo
II-365
Bisquert, Mar
I-67
Bj¨ork, Kaj-Mikael
I-406
Boatman, Nigel
III-476
Bolon, Philippe
II-576
Bolt, Janneke H.
I-496
Bordogna, Gloria
I-67, III-100
Borodin, Valeria
I-117
Boschetti, Mirco
I-67
Boukhris, Imen
III-223
Boulenguez-Phippen, Julie
II-606
Bounhas, Myriam
II-485
Bourtembourg, Jean
I-117
Boussarsar, Oumaima
III-223
Bouzouita, Kaouther
II-454
Bronselaer, Antoon
III-160
Bruno, Barbara
II-596
Bui, Marc
II-116
Burusco, Ana
III-31
Bustince, Humberto
II-475, III-252,
III-262, III-296
B¨uy¨uku˘gur, G¨ulseren
I-527
Caba˜nas, Rafael
I-506
Cabrera, Inma P.
III-91
Calvo, Tomasa
III-364
Camargo, Heloisa A.
I-87
Campi´on, Mar´ıa Jes´us
III-355
Candau, Charles
II-135
Candeal, Juan Carlos
III-355
Cano, Andr´es
I-506
Capotorti, Andrea
I-466
Casas´us-Estell´es, Trinidad
II-37
Castelltort, Arnaud
III-384
Cattaneo, Marco E.G.V.
I-226,
III-426
Ceolin, Davide
I-15
Chahir, Youssef
II-106
Charnomordic, Brigitte
I-127
Chen, Jesse Xi
III-396
Chen, Xiaon
II-606
Cholvy, Laurence
III-170
Cinicioglu, Esma Nur
I-527
Claeys, Peter
III-160
Clark, Patrick G.
I-386
Coletti, Giulianella
II-444, III-446
Coolen, Frank P.A.
III-498
Cordero, Pablo
III-91
Cornejo, Mar´ıa Eugenia
III-345
Coroianu, Lucian
II-244, II-254
Correa Beltran, William
III-110
Couceiro, Miguel
I-199

628
Author Index
Coulon-Leroy, C´ecile
I-127
Couso, In´es
I-1
Croitoru, Madalina
I-56, I-77,
II-204
Cunha, Luiz Manoel S.
I-87
Dabic, St´ephanie
II-576
Da Costa Sousa, Jo˜ao Miguel
I-567
Dalhoumi, Sami
II-294
d’Amato, Claudia
I-36
Daniel, Milan
III-212
Davis, Jonathan
I-355
de Aldama, Ricardo
I-588
de Castro, Santiago
II-586
Delibasic, Boris
II-1
Delmotte, Fran¸cois
III-190, III-200
De Miguel, Laura
III-355
Demirci, Mustafa
I-457
Denguir, Afef
I-416
Denœux, Thierry
I-107
Derosiere, G´erard
II-294
Destercke, S´ebastien
I-107, II-496,
III-416, III-498
De Tr´e, Guy
III-160
Diaconescu, Denisa
II-385
D´ıaz, Irene
II-56, II-76
D´ıaz-Moreno, Juan Carlos
III-81
Diaz-Valenzuela, Irene
I-179
Dimuro, Gra¸caliz Pereira
III-252
Divari, Maria
III-559
Doan, The-Vinh
I-345
Dockhorn, Alexander
II-46
Doutre, Sylvie
II-345
Dray, G´erard
II-294
Dubois, Didier
I-216, I-335
Durante, Fabrizio
III-243
Duthil, Benjamin
I-536
Dutra, Luciano
I-189
Du¸tu, Liviu-Cristian
II-576
Eciolaza, Luka
III-539, III-549
El-Ghazali, Talbi
II-264
Elkano, Mikel
III-296
El¸kins, Aleksandrs
III-41
Elouedi, Zied
III-223, III-233
Ennaceur, Amel
III-233
Esposito, Floriana
I-36
Essaid, Amira
I-46
Essghaier, Fatma
III-269
Fanizzi, Nicola
I-36
Fargier, H´el`ene
III-269
Farrokhi, Mohamad
III-528
Fernandez, Javier
III-262
Ferraioli, Anna Rita
II-385
Fig´a-Talamanca, Gianna
I-466
Fiorini, Nicolas
III-11
Firozja, M. Adabitabar
II-244
Flaminio, Tommaso
II-385
Fokkink, Wan
I-15
Fortemps, Philippe
II-464
Franco De Los R´ıos, Camilo
II-566
Fukuda, Ryoji
I-284
Gabrys, Bogdan
I-168
Gagolewski, Marek
II-244, III-21
Galar, Mikel
III-296
Galichet, Sylvie
II-284
Garc´ıa-Lapresta, Jos´e Luis
II-586
Garc´ıa-Pardo, Francisca
III-91
Gerla, Brunella
II-385
Gervet, Carmen
II-284
Godo, Lluis
I-335
Golu´nska, Dominika
II-424
G´omez-Olmedo, Manuel
I-506
Gonz´alez-Hidalgo, Manuel
II-184
Gonz´alez-Rodr´ıguez, In´es
I-447
Goumidi, Djamal E.
II-106
Greco, Salvatore
III-289
Grzegorzewski, Przemyslaw
I-158,
II-244
Grzymala-Busse, Jerzy W.
I-386
Guerra, Thierry Marie
III-528
Guillaume, Romain
II-335
Guillaume, Serge
I-127
Guizol, L´ea
II-204
Gurrieri, Massimo
II-464
Guttler, Fabio
I-97
Hachour, Samir
III-200
Hadjali, Allel
III-130
H¨ahnel, Holger
II-516
Han, Sang-Eon
III-41
Harispe, S´ebastien
III-1
Hart, Andy
III-476
Havens, Timothy C.
I-206
Held, Pascal
II-46
Heloulou, Nabila
I-427
Hempel, Arne-Jens
II-516
Herbst, Gernot
II-516

Author Index
629
Hermans, Jeroen
III-160
Herrera, Francisco
III-296
Herzig, Andreas
II-345
Hlinˇen´a, Dana
III-307
Hnaien, Faicel
I-117
Hod´akov´a, Petra
II-143, III-374
Holˇcapek, Michal
II-224, II-234
Honda, Aoi
I-284
Hosni, Hykel
III-436
Houlari, Tahereh
II-244
Hromada, Daniel Devatman
II-93
Hu, Lequn
I-206
Hurt´ık, Petr
III-374
Hurtik, Petr
II-143
Hussein, Mohamud
III-476
Ienco, Dino
I-97, III-100
Imoussaten, Abdelhak
I-536
Indur´ain, Esteban
III-355
Iwanaga, Saori
I-437
Janaqi, Stefan
III-1
Jaudoin, H´el´ene
III-120
Jaudoin, H´el`ene
III-110
Jendoubi, Siwar
II-66
Jirouˇsek, Radim
I-517
Jouen, Fran¸cois
II-106
Jur´ıo, Ar´anzazu
II-475
Kacprzyk, Janusz
II-424
Kalina, Martin
III-307
Kamarulzaman, Syaﬁq Fauzi
II-506
Karlsson, Alexander
III-456
Kaymak, Uzay
I-567, II-405
Keller, James M.
I-206
Kneˇzevi´c, Milica
II-10
K´oczy, L´aszl´o T.
II-375
Koehl, Ludovic
II-606
Koles´arov´a, Anna
III-262
Konecny, Jan
III-71
Kovalerchuk, Boris
II-536
Kr´al’, Pavol
III-307
Kruse, Rudolf
II-46
Kulacka, Agnieszka
I-325
Laˆamari, Wafa
III-486
Labadie, Nacima
I-117
Labreuche, Christophe
I-294
Lamure, Michel
II-116
Lauber, Jimmy
III-528
Laurent, Anne
II-414, III-384
Le, Mai
I-168
Lef`evre, Eric
I-25, III-233
Lejouad Chaari, Wided
II-454
Le Provost, Aline
II-204
Lesot, Marie-Jeanne
I-314, I-376,
II-395, III-140
Lewis, Daniel J.
III-406
Libourel, Therese
II-414
Li´etard, Ludovic
II-66, III-130
Lin, Yuan
II-414
Liu, Weiru
III-335
Lu, Jie
II-566
Lust, Thibaut
I-256
Ma, Jianbing
III-335
Ma, Wenjun
III-335
Maccatrozzo, Valentina
I-15
Madsen, Anders L.
I-506
Magdalena, Luis
II-616
Mandic, Ksenija
II-1
Marichal, Jean-Luc
I-199, III-327
Martin, Arnaud
I-46, I-557, II-66,
III-180
Martin, Trevor P.
III-406, I-168
Martin-Bautista, Maria J.
I-179
Martins-Bedˆe, Fl´avia
I-189
Massanet, Sebastia
I-138, I-148, II-184
Massolo, Alba
II-526
Mastrogiovanni, Fulvio
II-596
Matsuura, Yoshinori
I-437
Mauris, Gilles
I-396, II-576
Mayag, Brice
I-266
Medina, Jes´us
II-214, III-81, III-345
Mercier, David
III-190, III-200
Merig´o-Lindahl, Jos´e Maria
I-476
Mesiar, Radko
III-252, III-262, III-280,
III-289
Mezei, J´ozsef
I-406
Miclet, Laurent
II-324
Miller, Paul
III-335
Mir, Arnau
II-184
Molina, Mich`ele
II-106
Moln´arka, Gergely I.
II-375
Montagna, Franco
III-436
Montero, Javier
II-566
Montmain, Jacky
I-416, I-536, II-294,
III-1, III-11
Moodi, Hoda
III-528
Moreau, Luc
I-15

630
Author Index
Moyse, Gilles
I-376
Mrkalj, Milan
II-20
Mu˜noz-Hern´andez, Susana
III-51
Murinov´a, Petra
II-355
Nadel, Jacqueline
II-86
Nakata, Michinori
III-61
Narukawa, Yasuo
I-276
Nauck, Detlef
I-168
Nejjari, Fatiha
III-518
Nelsen, Roger B.
III-243
Nguyen, Thi-Minh-Tam
I-345
Nicolas, Yann
II-204
Nielandt, Joachim
III-160
Niemyska, Wanda
I-158
Nin, Jordi
I-97
Nov´ak, Vil´em
II-355
Ognjanovi´c, Zoran
II-10
O’Hara, Kieron
I-15
Ohnishi, Shin-ichi
III-508
Ojeda-Aciego, Manuel
III-91
Okamoto, Jun
I-284
Orero, Joseph Onderi
II-304
Orlovs, Pavels
III-317
Otsuki, Mika
III-508
Oudni, Amal
II-395
Pablos-Ceruelo, V´ıctor
III-51
Pagola, Miguel
II-475
Palacios, Juan Jos´e
I-447
Pan, Quan
I-557, III-180
Pancho, David P.
II-616
Paternain, Daniel
II-475, III-355
Paton, Lewis
III-476
Patrascu, Vasile
I-304, II-174
Perﬁlieva, Irina
II-143, II-153, III-374
Perovi´c, Aleksandar
II-10
Perrey, St´ephane
II-294
Petturiti, Davide
II-444, III-446
Pichler, Reinhard
II-214
Pichon, Fr´ed´eric
III-190
Pivert, Olivier
III-110, III-120, III-140,
III-150
Poitou, Olivier
II-314
Poncelet, Pascal
I-67, I-97
Prade, Henri
I-216, II-324, II-485,
III-150
Puente Peinador, Jorge
I-447
Puig, Vicen¸c
III-518
Quaeghebeur, Erik
III-466
Quesada-Molina, Jos´e Juan
III-243
Radojevi´c, Dragan G.
II-28
Ralescu, Anca
II-76
Ramdani, Messaoud
I-427
Ram´ırez-Poussa, Eloisa
III-345
Ranwez, Sylvie
III-1, III-11
Ranwez, Vincent
III-11
Rawashdeh, Ahmad
II-76
Rawashdeh, Mohammad
II-76
Reformat, Marek Z.
I-546, III-396
Revault d’Allonnes, Adrien
I-314
Ribeiro, Mariana V.
I-87
Richard, Gilles
II-485
Rico, Agn`es
I-216
Riera, Juan Vicente
I-138
Rifqi, Maria
II-304, II-395
Rindone, Fabio
III-289
Rizzo, Giuseppe
I-36
Rocacher, Daniel
III-120, III-130
Rodrigues, Luiz Henrique A.
I-87
Rodr´ıguez, Francisco J.
III-91
Rodr´ıguez, J. Tinguaro
II-566
Rolland, Antoine
I-256
Rotondo, Damiano
III-518
Rousseaux, Olivier
II-204
Ruiz-Aguilera, Daniel
I-138,
II-184
Russell, Stuart
I-10
Sackley, Alistair
I-15
Saﬃotti, Alessandro
II-596
Sagara, Nobusumi
I-236
Sakai, Hiroshi
III-61
Samet, Ahmed
I-25
Sandri, Sandra
I-189
Sanz, Jos´e
III-296
Saurel, Claire
II-314
Schreiber, Guus
I-15
Seising, Rudolf
II-556
Sgorbissa, Antonio
II-596
Shadbolt, Nigel
I-15
Shipley, Margaret F.
I-355
Siebert, Xavier
II-464
Smits, Gr´egory
I-46, III-140
ˇSostak, Alexander
III-41
ˇSpirkov´a, Jana
I-486
Stading, Gary L.
I-355
Stefanidis, Kostas
II-194

Author Index
631
ˇStˇepniˇcka, Martin
II-224
Strauss, Olivier
III-416
Stupˇnanov´a, Andrea
III-280
Sugeno, Michio
III-508, III-539,
III-549
Sundgren, David
III-456
Sutton-Charani, Nicolas
I-107
Tabacchi, Marco Elio
II-546
Tagina, Moncef
II-454
Takahagi, Eiichiro
I-246
Tamani, Nouredine
I-56, I-77
Taniguchi, Tadanari
III-539, III-549
Tao, Xuyuan
II-606
Teheux, Bruno
III-327
Teisseire, Maguelonne
I-67, I-97
Termini, Settimo
II-546
Thiollet-Scholtus, Marie
I-127
Thomopoulos, Rallou
I-56
Tijus, Charles
II-135
Tissot, Jean-Marc
II-576
Torra, Vicen¸c
I-276
Torrens, Joan
I-148
Toyoshima, Hisashi
III-508
Tran, Duc-Khanh
I-345
Troﬀaes, Matthias C.M.
III-476, III-498
Troiano, Luigi
II-56
Trousset, Fran¸cois
I-416, I-536
Turunen, Esko
III-81
´Ubeda-Flores, Manuel
III-243
Urtubey, Luis A.
II-526
Vajgl, Marek
II-153
Valota, Diego
II-365
Vandermeulen, Dirk
III-160
van Hage, Willem Robert
I-15
Vantaggi, Barbara
II-444, III-446
Vela, Camino R.
I-447
Ventalon, Geoﬀrey
II-135
Verbeek, Nick
I-567
Verstraete, J¨org
I-366
Viappiani, Paolo
II-434
Vila, Maria-Amparo
I-179
Vivona, Doretta
III-559
Vu, Viet-Trung
I-345
Wilbik, Anna
II-405
Wilkin, Tim
III-364
Yager, Ronald R.
I-546, I-577, II-37
Yamanoi, Takahiro
III-508
Yamazaki, Toshimasa
III-508
Yasunobu, Seiji
II-506
Yusoﬀ, Binyamin
I-476
Zadro˙zny, Slawomir
II-424
Zamansky, Anna
II-194
Zeng, Xianyi
II-606
Zhou, Kuang
I-557, III-180

