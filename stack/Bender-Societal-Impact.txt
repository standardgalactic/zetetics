Emily M. Bender 
University of Washington 
@emilymbender 
CLIP Colloquium 
University of Maryland 
October 7, 2020
A Typology of Ethical Risks in Language 
Technology with an Eye Towards Where 
Transparent Documentation Can Help

Goals
• Present a typology of the risks of adverse impacts of language technology 
• Present data statements: a positive step we can take to position ourselves to 
mitigate such risks 
• Reﬂect on which types of risks data statements help with 
• Describe some emerging best practices
Non-exhaustive, preliminary
One tool, not a panacea!
Some, not all
@emilymbender

Typology
• A systematic classiﬁcation of phenomena, along one or more dimensions 
• Helps to explore the space of possibilities 
• Helps to understand relationships across categories
@emilymbender

Hovy & Spruitt 2016  
“The Social Impact of Natural Language Processing”
• Survey of some types of issues 
• Importantly raised awareness of the discussion within English-language NLP 
circles 
• Introduced concepts of:  
• Exclusion, Overgeneralization, Bias conﬁrmation, Topic Overexposure, 
Dual use 
• Illustrated with NLP-speciﬁc examples of negative impacts 
• Not exhaustive, not a typology
@emilymbender

Another taxonomy of harms [Not mutually 
exclusive] (from Barocas et al, 2017; Crawford, 2017)
• allocational harms: ML systems unfairly allocating ﬁnite resources 
• representational harms: ML systems contribute to subordination of certain 
groups 
• quality of service (e.g. ASR working better for some groups than others; 
Tatman, 2017) 
• stereotyping (e.g. online ads suggesting that people with Black-sounding 
names had been arrested; Sweeney, 2013) 
• denigration (e.g. Tay, where the ML system actively participated in hate 
speech; Price, 2016) 
• under-representation (e.g. image search for “CEO” returning more images 
of white men than is reﬂected in the real world; Kay et al, 2015)
@emilymbender

Guiding principles: Value sensitive design
• Value sensitive design (Friedman et al 2006, Friedman & Hendry 2019): 
• Identify stakeholders 
• Identify stakeholders’ values 
• Design to support stakeholders’ values
@emilymbender

Guiding principles: Sociolinguistics 
(e.g. Labov 1966, Eckert & Rickford 2001)
• Variation is the natural state of language 
• Variation in pronunciation, word choice, grammatical structures 
• Status as ‘standard’ language is a question of power, not anything inherent to 
the language variety itself 
• Language varieties & features associated with marginalized groups tend to 
be stigmatized 
• Meaning, including social meaning, is negotiated in language use 
• Our social world is largely constructed through linguistic behavior
@emilymbender

Stakeholder-centered typology
Direct stakeholders
Indirect stakeholders
User, by choice
Harm to community
User, not by choice
Harm to individual
Annotator, crowdworker
Unwitting data contributor
@emilymbender
Tech use
Tech 
dev

Direct stakeholders: By choice
• I choose to use this voice assistant, dictation software, machine translation 
system… 
• … but it doesn’t work for my language or language variety 
• Suggests that my language/language variety is inadequate 
• Makes the product unusable for me 
• … but the system doesn’t indicate how reliable it is 
• Users reliant on machine translation/auto-captioning for important 
info left in the dark about what they might be missing
@emilymbender

Direct stakeholders: Not by choice
• My screening interview was conducted by a virtual agent 
• I can only access my account information via a virtual agent 
• Access to a 911 system requires interaction with a virtual agent ﬁrst 
• … but it doesn’t work or doesn’t work well for my language variety 
• I scored poorly on the interview, even though the content of my 
answers was good 
• I can’t access my account information or 911
@emilymbender

Direct stakeholders: Not by choice
• LM (language modeling) technology can now generate very real sounding text, 
in English at least (Radford et al 2019, Brown et al 2020) 
• … but which is not grounded in any actual relationship to facts 
• I mistake the text for statements made by a human publicly 
committing to them 
• I become more distrustful of all text I see online 
• Language models trained on ‘standard’ or ‘oﬃcial’ sounding documents 
will sound ‘standard’ or ‘oﬃcial’.
@emilymbender

Indirect stakeholders: Community harm
• Someone searched for me online 
• … but the search triggered display of negative ads including my name 
because of stereotypes about my ethnic identity (Sweeney 2013) 
•  Virtual assistants are gendered as female and bossed around
@emilymbender

Indirect stakeholders: Community harm
• Sentiment analysis systems don’t work well on my dialect 
• … my community’s input is not included when social media discussions 
are processed for public policy input 
• Language ID systems don’t identify my dialect 
• … Social-media based disease warning systems fail to work in my 
community (Jurgens et al 2017)
@emilymbender

Indirect stakeholders: Community harm
• Systems are built using general webtext as a proxy for word meaning or world 
knowledge 
• … but general web text reﬂects many types of bias (Bolukbasi et al 2016, 
Caliskan et al 2017, Gonen & Goldberg 2019) 
• autocompletion of search queries repeats & reinforces harmful 
stereotypes (Noble 2018)
@emilymbender

Indirect stakeholders: Individual harm
• Systems are built using general webtext as a proxy for word meaning or world 
knowledge 
• … but general web text reﬂects many types of bias (Bolukbasi et al 2016, 
Caliskan et al 2017, Gonen & Goldberg 2019) 
• My restaurant’s positive reviews are underrated because of the name 
of the cuisine (Speer 2017) 
• My resume is rejected because the screening system has learned 
that typically “masculine” hobbies correlate with getting hired 
• My image search reﬂects stereotypes back to me (Noble 2018)
@emilymbender

Indirect stakeholders: Individual harm
• LM (language modeling) technology can now generate very real sounding text, 
in English at least (Radford et al 2019) 
• … but which is not grounded in any actual relationship to facts 
• Such systems are then used by extremist groups to synthesize text 
to populate message boards used to radicalize people (McGuﬃe & 
Newhouse 2020)
@emilymbender

Indirect stakeholders: Individual harm
• Someone searched for critics of the government 
• … and found my blog post/tweet 
• Someone put my words into an MT system 
• … which got the translation wrong and led the police to arrest me          
(The Guardian, 24 Oct 2017; https://bit.ly/2zyEetp) 
• Someone built an identity characteristic classiﬁer 
• … and outed me based on characteristics of my language use
@emilymbender

Stakeholder-centered typology
Direct stakeholders
Indirect stakeholders
User, by choice
Harm to community
User, not by choice
Harm to individual
Annotator, crowdworker
Unwitting data contributor
@emilymbender

What does this mean for  
NLP researchers & developers?
• We have a responsibility to broaden our lens: 
• our jobs aren’t just about framing and solving technical problems 
• but also about understanding how the tech we build (or choose not to 
build) ﬁts into society 
• This requires a slower pace of “progress” 
• Being systematic about documentation can help
@emilymbender

Machine learning, in a nutshell
• “Each machine learning problem can be precisely deﬁned as the problem of 
improving some measure of performance P when executing some task T, 
through some type of training experience E. […] Once the three components 
⟨T,P,E⟩ have been speciﬁed fully, the learning problem is well 
deﬁned” (Mitchell 2017, p.2)
Task 
deﬁnition
Learning 
approach
Evaluation 
metric
Train/test 
data
@emilymbender

Machine learning, in context
Task Lear
Eval
Train
How does 
dataset model 
the task?
How 
does the task 
relate to the 
world?
Why do we 
care about this 
task?
-build something useful 
-learn about: computers, people, 
modeling domain 
How do we 
collect the data?
What 
happens when 
we deploy 
this?
@emilymbender

Data Statements for NLP: Transparent documentation 
(Bender & Friedman 2018)
• Foreground characteristics of our datasets (see also: AI Now Institute 2018, Gebru et 
al 2018, Mitchell et al 2019) 
• Make it clear which populations & linguistic styles are and are not represented 
• Support reasoning about what the possible eﬀects of mismatches may be 
• Recognize limitations of both training and test data: 
• Training data: eﬀects on how systems can be appropriately deployed 
• Test data: eﬀects on what we can measure & claim about system 
performance
@emilymbender

Proposed Schema: Long Form
• A. Curation Rationale 
• B. Language Variety 
• C. Speaker Demographic 
• D. Annotator Demographic 
• E. Speech Situation 
• F. Text Characteristics 
• G. Recording Quality 
• H. Other 
• I. Provenance Appendix
What data? Why?
Whose language?
What kind of  
language behavior?
@emilymbender

Proposed Schema: Short Form
• 60-100 word summary of the information in long form data statement, hitting 
most main points 
• Include pointer to where the long form can be found  
• Paper presenting the dataset originally 
• Project web page 
• System documentation
@emilymbender

Case: Direct stakeholders whose varieties aren’t 
well represented
• Speech/language tech researchers & developers: Map out 
underrepresented language varieties and direct eﬀort appropriately; test 
approaches more broadly 
• Procurers: Is this trained model likely to work for our clientele? 
• Consumers: Is this trained model likely to work for me?  
• Members of the public: Advocate for models trained on datasets that are 
responsive to the community of users 
• Policy makers: Require automated systems to be accessible to speakers of 
all language varieties in the community
@emilymbender

Case: Indirect stakeholders whose varieties aren’t 
well represented
• Speech/language tech researchers & developers: Map out 
underrepresented language varieties and direct eﬀort appropriately; test 
approaches more broadly 
• Procurers: What information is this system going to expose and what is it 
going to miss? 
• Consumers: Is this software being transparent about how well it can work 
and under what circumstances it works better/worse?  
• Members of the public: Advocate for transparency regarding system 
performance across representative samples 
• Policy makers: Require broad testing of systems and transparency regarding 
system conﬁdence/failure modes
@emilymbender

Data statements are not a panacea!
• Mitigation of the negative impacts of speech/language technology will require 
on-going work and engagement (and cost/beneﬁt analysis) 
• Data statements are intended as one practice among others that position us 
(in various roles) to anticipate & mitigate some negative impacts 
• Probably won’t help with e.g.: 
• impacts of gendering virtual agents 
• privacy concerns around classiﬁcation of identity characteristics 
• Can help with problems stemming from lack of representative data sets and 
possibly also ‘automation bias’ (Skitka et al 2000)
@emilymbender

Data statements workshop 2020 
(Joint work with Angelina McMillan-Major and Batya Friedman)
• “LREC” 2020 workshop — online May 2020 
• 38 participants, every continent represented 
• 3-day working meeting to 
• develop data statements for participants’ datasets 
• elicit feedback on data statement schema 
• distill best practices for data statement creation and use
@emilymbender
https://sites.google.com/uw.edu/data-statements-for-nlp/

Best practices for writing - preliminary
• Interview methodology 
• Collect information for the data statement while creating the dataset 
• Data statements don’t have to be exhaustive 
• If you can’t answer a schema element, it’s enough to say why not 
• Refrain from inferring information if it’s not available 
• Include pointers to other key documentation (license, annotation schema, 
copyright, etc)
@emilymbender

Best practices for using - preliminary
• Examine data statement to determine appropriateness of use for each use 
case 
• More clearly scope generalizability of results 
• Draft/plan the data statement before starting dataset creation, to help guide 
data collection 
• Communicate about NLP to allied ﬁelds
@emilymbender

Beyond data statements: 
What else can we do?
• Make time to consider, early & often, the following questions: 
• What are the use cases of the technology being developed? 
• How does the speciﬁc ML task (inputs, outputs) relate to the intended use 
case? 
• What are the failure modes and who might be harmed? 
• What kinds of bias are likely to be included in the training data? 
• Broaden our notion of ‘scaling up’: It’s not just about large numbers but also 
about diverse communities & experiences with the software
@emilymbender

Instructive case study: GermEval 2020
Subtask 1: Prediction of Intellectual Ability
The task is to predict measures of intellectual ability solemnly based on text. For this, 
z-standardized high school grades and IQ scores of college applicants are summed 
and globally ranked. The goal of this subtask is to reproduce their ranking, systems 
are evaluated by the Pearson correlation coefﬁcient between system and gold 
ranking. 
Subtask 2: Classiﬁcation of the Operant Motive Test (OMT)
Operant motives are unconscious intrinsic desires that can be measured by implicit or 
operant methods, such as the Operant Motive Test (OMT) (Kuhl and Scheffer, 1999). 
During the OMT, participants are asked to write freely associated texts to provided 
questions and images. An exemplary illustration can be found in the Data area. 
Trained psychologists label these textual answers with one of four motives. The 
identiﬁed motives allow psychologists to predict behavior, long-term development, and 
subsequent success. For this shared task, participants will be provided with an 
OMT_text and are asked to predict the motive and level of each instance. The 
success will be measured with the macro-averaged F1-score.
https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/germeval-2020-psychopred.html
@emilymbender

Instructive case study: GermEval 2020
• What could possibly go wrong?  
• (What could possibly go right??)
@emilymbender

Instructive case study: GermEval 2020
“In the United States, there is considerable evidence that IQ tests 
are racially biased. In the past, courts have excluded IQ tests from 
educational placement in California for precisely this reason. I 
wonder if there is research on this topic in the German context. 
“It is not difﬁcult to imagine that the outcome of this shared task 
would be a set of technologies that encode spurious correlations 
between estimates of intelligence and the linguistic features of 
speciﬁc racial groups. If such a system were trained on data that 
already contains biases, there is a risk that this bias would be not 
only entrenched but ampliﬁed. And even if the IQ test statistics are 
not themselves biased, an NLP system that predicts IQ from text 
could introduce bias, if there is an unmeasured confound that is 
statistically associated with both IQ and race.”
(Jacob Eisenstein, message to corpora mailing list, 12/4/2019)
@emilymbender

Questions that should have been asked
• Does the output of the ML task match the information it’s framed as 
predicting? (No.) 
• Does the input to the ML task actually contain enough information to predict 
the output? (No.) 
• What are the intended use cases for this technology? 
• If the technology is working as intended, who might be harmed and how? 
• If the technology is not working as intended, who might be harmed and how?
Asking is the ﬁrst step, but how to answer reliably?
@emilymbender

Too big to document = too big to deploy?
• GPT-3 and other ginormous language models pose a dilemma:  
• If their success rests on gathering datasets too large to feasibly thoroughly 
document, how could they be used safely? 
• McGuﬃe & Newhouse (2020) show that GPT-3 can be led, through few-shot 
learning, to produce text in the persona of a conspiracy theorist
@emilymbender

(McGufﬁe & Newhouse 2020:5)

Questions
• Could GPT-3 have produced this without having similar conspiracy theory 
texts in its training data? 
• If not, how much is required? 
• Worst case: only a little, because it’s much harder to design data collection 
and data cleaning processes that remove every last trace  
• See Gehman et al 2020
@emilymbender

Trade-offs
• How to get the beneﬁts from very large LMs for e.g. improvements in ASR 
without opening up the above-mentioned risks? 
• How do we know that very large LMs are the only way to get those beneﬁts? 
• Are there ways to prevent / reduce dispersal of synthetic texts (e.g. 
watermarking)?
@emilymbender

Suggested reading
• Blodgett et al 2020 (ACL) 
“Language (Technology) is Power: A 
Critical Survey of “Bias” in NLP”  
• Larson 2017 (EACL workshop) 
“Gender as a Variable in Natural-
Language Processing: Ethical 
Considerations” 
• Sweeney 2013 (CACM) 
“Discrimination in Online Ad 
Delivery” 
• Noble 2018 Algorithms of 
oppression: How search engines 
reinforce racism 
• Benjamin 2019 Race after 
technology: Abolitionist tools for the 
New Jim Code 
• Agüera y Arcas, Mitchell and 
Todorov 2017 (medium.com) 
“Physiognomy’s New Clothes”
+ Radical AI Podcast 

Summary
• The L in NLP means language and language means people (Schnoebelen 
2017) … and variation! 
• When we are working on tech that will be deployed in the world, we need to 
keep an eye on how it ﬁts into the world 
• It’s easy to get bogged down in “this is too terrible” or “this is too hard”, and 
then turn away (from NLP or its societal impacts), but we don’t have to get 
stuck there 
• Transparency is a good starting point: documentation of datasets & models, 
clear discussion of application—world relationship
Thank you!
@emilymbender

References
Ag¨uera y Arcas, Blaise, Mitchell, Margaret and Todorov, Alexander. 2017. Physiognomys New Clothes. Blog
post on Medium.com, https://medium.com/@blaisea/physiognomys-new-clothes-f2d4b59fdd6a.
AI Now Institute. 2018. Algorithmic Impact Assessments: Toward Accountable Automation in Public Agen-
cies. Medium.com.
Barocas, Solon, Crawford, Kate, Shapiro, Aaron and Wallach, Hanna. 2017. The Problem With Bias: Al-
locative Versus Representational Harms in Machine Learning. In SIGCIS Conference.
Bender, Emily M. and Friedman, Batya. 2018. Data Statements for Natural Language Processing: Toward
Mitigating System Bias and Enabling Better Science. Transactions of the Association for Computa-
tional Linguistics 6, 587–604.
Benjamin, Ruha. 2019. Race After Technology: Abolitionist Tools for the New Jim Code. Cambridge, UK:
Polity Press.
Blodgett, Su Lin, Barocas, Solon, Daum´e III, Hal and Wallach, Hanna. 2020. Language (Technology) is
Power: A Critical Survey of “Bias” in NLP. In Proceedings of the 58th Annual Meeting of the As-
sociation for Computational Linguistics, pages 5454–5476, Online: Association for Computational
Linguistics.
Bolukbasi, Tolga, Chang, Kai-Wei, Zou, James Y., Saligrama, Venkatesh and Kalai, Adam T. 2016. Man
is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. In D. D.
Lee, M. Sugiyama, U. V. Luxburg, I. Guyon and R. Garnett (eds.), Advances in Neural Information
Processing Systems 29, pages 4349–4357, Curran Associates, Inc.
Brown, Tom B., Mann, Benjamin, Ryder, Nick, Subbiah, Melanie, Kaplan, Jared, Dhariwal, Prafulla, Nee-
lakantan, Arvind, Shyam, Pranav, Sastry, Girish, Askell, Amanda, Agarwal, Sandhini, Herbert-Voss,
Ariel, Krueger, Gretchen, Henighan, Tom, Child, Rewon, Ramesh, Aditya, Ziegler, Daniel M., Wu,
Je↵rey, Winter, Clemens, Hesse, Christopher, Chen, Mark, Sigler, Eric, Litwin, Mateusz, Gray, Scott,
Chess, Benjamin, Clark, Jack, Berner, Christopher, McCandlish, Sam, Radford, Alec, Sutskever, Ilya
and Amodei, Dario. 2020. Language Models are Few-Shot Learners.
Caliskan, Aylin, Bryson, Joanna J and Narayanan, Arvind. 2017. Semantics Derived Automatically from
Language Corpora Contain Human-like Biases. Science 356(6334), 183–186.
Eckert, Penelope and Rickford, John R. (eds.). 2001. Style and Sociolinguistic Variation. Cambridge: Cam-
bridge University Press.
Friedman, Batya and Hendry, David G. 2019. Value Sensitive Design: Shaping Technology with Moral Imag-
ination. MIT Press.
Friedman, Batya, Kahn, Jr., Peter H and Borning, Alan. 2006. Value sensitive design and information
systems. In P Zhang and D Galletta (eds.), Human–Computer Interaction in Management Information
Systems: Foundations, pages 348–372, Armonk NY: M. E. Sharpe.
Gebru, Timnit, Morgenstern, Jamie, Vecchione, Briana, Wortman Vaughan, Jennifer, Wallach, Hanna,
Daum´e III, Hal and Crawford, Kate. 2018. Datasheets for Datasets, arXiv:1803.09010v1.
Gehman, Samuel, Gururangan, Suchin, Sap, Maarten, Choi, Yejin and Smith, Noah A. 2020. RealToxici-
tyPrompts: Evaluating Neural Toxic Degeneration in Language Models. Findings of EMNLP .
Gonen, Hila and Goldberg, Yoav. 2019. Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender
Biases in Word Embeddings But do not Remove Them, arXiv:1903.03862v1.
Hovy, Dirk, Spruit, Shannon, Mitchell, Margaret, Bender, Emily M. Strube, Michael and Wallach, Hanna
(eds.). 2017. Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. Va-
lencia, Spain: Association for Computational Linguistics.
Hovy, Dirk and Spruit, Shannon L. 2016. The Social Impact of Natural Language Processing. In Proceedings
of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),
pages 591–598, Berlin, Germany: Association for Computational Linguistics.
Jurgens, David, Tsvetkov, Yulia and Jurafsky, Dan. 2017. Incorporating Dialectal Variability for Socially
Equitable Language Identiﬁcation. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers), pages 51–57, Vancouver, Canada: Association
for Computational Linguistics.
1

Labov, William. 1966. The Social Stratiﬁcation of English in New York City. Washington, DC: Center for
Applied Linguistics.
Larson, Brian. 2017. Gender as a Variable in Natural-Language Processing: Ethical Considerations. In Pro-
ceedings of the First ACL Workshop on Ethics in Natural Language Processing, pages 1–11, Valencia,
Spain: Association for Computational Linguistics.
McGuﬃe, Kris and Newhouse, Alex. 2020. The Radicalization Risks of GPT-3 and Advanced Neural Lan-
guage Models. Technical Report, Center on Terrorism, Extremism, and Counterterrorism, Middlebury
Institute of International Studies at Monterrey, https://www.middlebury.edu/institute/sites/
www.middlebury.edu.institute/files/2020-09/gpt3-article.pdf.
Mitchell, Margaret, Wu, Simone, Zaldivar, Andrew, Barnes, Parker, Vasserman, Lucy, Hutchinson, Ben,
Spitzer, Elena, Raji, Inioluwa Deborah and Gebru, Timnit. 2019. Model Cards for Model Reporting.
In Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* ’19, pages
220–229, New York, NY, USA: ACM.
Mitchell, Tom. 2017. Machine Learning, Ch 14: Key Ideas in Machine Learning, http://www.cs.cmu.edu/
~tom/mlbook/keyIdeas.pdf.
Noble, Saﬁya Umoja. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press.
Radford, Alex, Wu, Je↵rey, Child, Rewon, Luan, David, Amodei, Dario and Sutskever, Ilya. 2019. Language
Models are Unsupervised Multitask Learners, unpublished MS, OpenAI San Francisco.
Schnoebelen, Tyler. 2017. The Carrots and Sticks of Ethical NLP, blog post, https://medium.com/
@TSchnoebelen/ethics-and-nlp-some-further-thoughts-53bd7cc3ff69, accessed 19 March 2019.
Skitka, Linda J., Mosier, Kathleen and Burdick, Mark D. 2000. Accountability and automation bias. Inter-
national Journal of Human-Computer Studies 52(4), 701 – 717.
Speer,
Robyn.
2017.
ConceptNet
Numberbatch
17.04:
better,
less-stereotyped
word
vectors,
blog
post,
https://blog.conceptnet.io/2017/04/24/conceptnet-numberbatch-17-04-better
-less-stereotyped-word-vectors/, accessed 6 July 2017.
Sweeney, Latanya. May 1, 2013. Discrimination in Online Ad Delivery. Communications of the ACM 56(5),
44–54.
2

