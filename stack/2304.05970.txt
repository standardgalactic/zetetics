Boosted Prompt Ensembles for Large Language Models
Silviu Pitis
Michael R. Zhang
Andrew Wang
Jimmy Ba
University of Toronto, Vector Institute
Abstract
Methods such as chain-of-thought prompting and
self-consistency have pushed the frontier of lan-
guage model reasoning performance with no addi-
tional training. To further improve performance,
we propose a prompt ensembling method for large
language models, which uses a small dataset to
construct a set of few shot prompts that together
comprise a “boosted prompt ensemble”. The few
shot examples for each prompt are chosen in a
stepwise fashion to be “hard” examples on which
the previous step’s ensemble is uncertain. We
show that this outperforms single-prompt output-
space ensembles and bagged prompt-space ensem-
bles on the GSM8k and AQuA datasets, among
others. We propose both train-time and test-time
versions of boosted prompting that use different
levels of available annotation and conduct a de-
tailed empirical study of our algorithm.
1. Introduction
When prompted with a few examples of a target behav-
ior, Large Language Models (LLMs) are able to solve an
impressive array of tasks, often with superhuman perfor-
mance (Brown et al., 2020; Srivastava et al., 2022). When
the examples include reasoning steps leading to the answer
(a “chain of thought”), e.g., for multi-step math problems,
LLMs demonstrate similar step-by-step reasoning during
inference, which greatly improves their accuracy (Nye et al.,
2021; Wei et al., 2022). Combined with output space en-
sembling, where several chains of thought are generated for
a single prompt, this achieves strong performance on a wide
array of tasks (Wang et al., 2022c).
The strong baseline performance of LLMs in the few shot
setting allows them to be used not only for immediate per-
formance on downstream tasks, but also to generate relevant,
high quality datasets on which the LLMs themselves can be
further trained or ﬁnetuned (Zelikman et al., 2022; Huang
Correspondence to: Silviu Pitis <spitis@cs.toronto.edu>.
et al., 2022; Bai et al., 2022). In both cases, we seek the high-
est possible few shot performance from the base foundation
model (Li et al., 2022b; Wang et al., 2022b). One way to do
this is to carefully select the initial few shot samples (and
chain of thought, where applicable) for each target task. This
“prompt engineering” can entail substantial manual effort
for each individual task, and there is uncertainty about how
choices impact performance (Zhou et al., 2022). For exam-
ple, one recent work recommends prompts with the “longest
questions” and most “complex reasoning” (Fu et al., 2022)
while another suggests “only considering shorter questions
with shorter rationales” (Zhang et al., 2022).
To improve baseline performance and reduce the manual ef-
fort involved in constructing few shot samples, we propose
a new technique, “boosted prompting,” which leverages a
small dataset to construct a set of few shot prompts that
progressively solve more of the problems. Boosted prompt-
ing, inspired by classical boosting algorithms (Freund et al.,
1999), is a stagewise ensemble method that iteratively adds
to a set of prompts so as to improve performance on prob-
lems just outside the frontier of what the model can currently
solve (Baranes & Oudeyer, 2013). See Figure 1 for a con-
ceptual illustration. The ﬁnal output of our algorithm is
an accumulated set of LLM prompts with representation
throughout the difﬁcult parts of the problem space. We pro-
pose both train-time (inductive) and test-time (transductive)
versions of our algorithm. We show that the former can
improve performance with as few as 100 labeled training
examples, and ﬁnd some evidence that the latter allows the
LLM to adapt to changes in the problem distribution. The
contributions of our work include:
1. A algorithm that constructs a boosted ensemble of
few shot prompts in a stagewise iterative process. It
is complementary to prior techniques for improving
reasoning performance.
2. Our proposed algorithm obtains strong results on
AQUA, GSM8K, and other challenging datasets, out-
performing the strong baseline performance of single
prompt output space ensembles (Wang et al., 2022c)
and bagged ensembles (Li et al., 2022b).
3. A detailed empirical study that investigates different
annotation settings and design choices.
arXiv:2304.05970v1  [cs.CL]  12 Apr 2023

Boosted Prompt Ensembles for Large Language Models
Complexity
Argument Style
1. Initial Prompt
prompt
correct
incorrect
Complexity
Argument Style
2. Select "Hard" Examples
prompt
hard
correct
incorrect
Complexity
Argument Style
3. Boosted Prompt Ensemble
prompt 1
prompt 2
correct
incorrect
Figure 1. Conceptual Diagram of Boosted Prompting. In this diagram (data crafted for illustrative purposes), we see the intuition of
boosted prompting in a simpliﬁed 2D space. The original prompt consists of example problems (⋆) and generalizes to parts of the target
problem space (·), but fails in others (×). Our algorithm selects the “Hard” examples (⃝)—overall incorrect examples for which the
model generated at least one correct solution (out of several generations combined via self-consistency (Wang et al., 2022c)). The Hard
examples are used to form a new few-shot prompt (◀), which is added to the ensemble, increasing overall coverage of the problem space.
When applied at train time, the Hard examples typically lie outside the model’s solution frontier, as depicted, since ground truth labels are
available. When no labels are available, the model uses its own generated labels and Hard examples are restricted to labels with sufﬁcient
conﬁdence.
2. Prior Work
Large Language Models
Large, transformer-based lan-
guage models (LLMs) have proven to be extremely capable
few shot learners in a wide variety of different contexts
(Vaswani et al., 2017; Brown et al., 2020). Their general
purpose nature has created something of a “paradigm shift”
in the AI landscape, whereby many downstream tasks re-
quiring language will make use of an LLM as a foundation
model, either directly or by ﬁnetuning (Bommasani et al.,
2021). Our work considers one approach to improving base-
line, untuned LLM performance, which builds on and is
complementary to a number of recent techniques.
Chain of Thought
Wei et al. (2022) show that prompt-
ing LLMs with intermediary reasoning steps, called chain
of thought (CoT) prompting, can signiﬁcantly increase the
ability of the LLM to perform complexity reasoning tasks.
Wang et al. (2022c) further improve reasoning performance
by introducing self-consistency (SC), which replaces the
standard greedy decoding of the LLM output with a stochas-
tic output space ensemble that marginalizes over multiple
reasoning paths by sampling with positive temperature (e.g.,
T = 0.7) and choosing the ﬁnal prediction p∗with highest
agreement:
p∗= arg maxp
X
iI(pi = p)
This exploits the fact that diverse reasoning paths that lead
to the same answer are more likely to be correct. Our work
builds on self-consistency by using the agreement among
reasoning paths to determine the set of “Hard” problems and,
for the test-time version of our algorithm, the set of LLM
generated answers that are likely to be correct. This latter
usage is similar to that of Huang et al. (2022), who show
that by ﬁnetuning LLMs on self generated answers with
“high agreement,” large language models can self improve.
Automatic Prompt Engineering
It has been observed
that language model performance can be sensitive to the
chosen prompt (Zhao et al., 2021), which has led to in-depth
studies of prompting methodology (Liu et al., 2023; Wang
et al., 2022a) and the development of several approaches to
automatic prompt generation (Shin et al., 2020; Gao et al.,
2020). While some of these approaches are gradient-based
(Li & Liang, 2021; Qin & Eisner, 2021), requiring access
to the model gradients, others are based on sampling (Zhou
et al., 2022) or elicited via a prompt-based algorithm (Li
et al., 2022a). For purposes of collecting chain of thought
annotations, a handful of past works have considered self-
generating the chain of thought (Zhang et al., 2022; Huang
et al., 2022) and possibly validating them using the ground
truth answers (Zelikman et al., 2022). We draw inspiration
from these works and use model generated chains of thought
when forming boosted prompts.
Example Selection
Few shot performance can be im-
proved by retrieving relevant examples from a large dataset
(Liu et al., 2021; Rubin et al., 2021), which may itself by
generated by the language model via a carefully guided pro-
cess (Li et al., 2022a). For chain of thought prompting, it
has been observed that relevance and coherence (ordering of
reasoning steps) are important for performance (Wang et al.,
2022c) and further, that choosing examples that require
more reasoning steps can improve performance (Fu et al.,
2022). In our work we use disagreement among ensemble
members both as a proxy for example informativeness and,
for the test-time version of our algorithm, as a measure of
conﬁdence in the correctness of the model’s prediction.
Ensemble Methods
Ensembles reliably improve perfor-
mance in a number of contexts (Lakshminarayanan et al.,
2017; Ganaie et al., 2022), including language modeling
(Wang et al., 2022b). Boosting (Freund et al., 1999) itera-

Boosted Prompt Ensembles for Large Language Models
Algorithm 1 Boosted Prompting. We assume access to a language model LLM that generates answers when prompted
with few-shot examples p and question q. Our algorithm progressively grows preds, the set of answers to each question,
and generates prompts consisting of questions of intermediate difﬁculty and their answers.
function BOOSTEDPROMPTING(model LLM, initial prompt p,
problem set T, iters n, num candidates m, [answers A]):
preds ←{q : [ ] for q in T}
prompts ←{p}
for
in 0 . . . n −1 do
for q in T do
preds[q].append([LLM(p, q) for
in range(m)])
p ←NEWPROMPT (T, preds, [answers A])
prompts.add(p)
return prompts, preds
function NEWPROMPT(T, preds, [answers A]):
if A then
▷train-time boosting
C ←[q for q in T if A[q] in preds[q]]
else
▷test-time boosting
C ←[q for q in T if there is “sufﬁcient agreement” for
the majority prediction in preds[q]]
C ←choose 8 of the questions for which the majority
prediction in preds[q] has minimal agreement
p ←for each question q in C choose one CoT from
preds[q] that led to the answer/prediction
return p
tively constructs an ensemble to optimize performance on
difﬁcult examples, and can be understood as a form of cur-
riculum learning (Bengio et al., 2009). We adapt boosting
to the prompt-based LLM setting by forming a new prompt
of “Hard” examples at each iteration.
In a concurrent work, (Hou et al., 2022) also adapt boosted
ensembles to LLM classiﬁers. Their work focuses on a
different setting than ours, in which classiﬁcation is done
via single tokens, as opposed to a solution following a chain
of thought. Rather than choose the prompt examples so as
to improve performance, Hou et al. (2022) choose random
examples for each prompt, but change the weighting of the
training examples as in classical boosting when optimizing
a “verbalizer” that maps model outputs to classes.
3. Boosted Prompt Ensembles
Our goal is to construct a set of few shot prompts for a
pretrained language model that work well together as an
ensemble, in the sense that their combined predictions do
better than the predictions of a single prompt output space
ensemble (Wang et al., 2022c) or a multi-prompt bagged
ensemble (Wang et al., 2022b; Li et al., 2022b).
To do this, we adopt a stagewise approach inspired by classi-
cal boosting algorithms that iteratively adds an informative
prompt to an existing set of prompts. At each iteration, a
new informative prompt is added, thereby expanding the
range of problems that the current ensemble solves.
As a proxy for informativeness, we propose to use the agree-
ment between several solutions sampled from the model. If
all solutions agree, the model already knows how to solve
the given sample, and it is not particularly informative. On
the other hand, if there is disagreement amongst the model’s
solutions, then we assume that the model is unsure about
the example, and that including a correct solution for the
example in a few shot prompt would be informative.
How we determine correctness depends on the setting, and
leads to two different instances of our algorithm: train-time
boosting, for the case where there is a small labeled training
set or some human-in-the-loop supervision available, and
test-time boosting, for the case where no supervision is avail-
able and the model must rely solely on its own predictions.
Both are summarized in Algorithm 1 (the difference being
whether the optional argument answers A is provided).
The output of our train-time algorithm is a set of prompts,
which are then applied to the test set. The output of our
test-time algorithm is a set of prompts together with a set of
test predictions.
Train-time Boosting
In this case, we assume access to a
small labeled dataset, DTrain = (T, A) = (qi, ai)N
i=1, where
ai are ﬁnal answer labels (we do not require chain of thought
annotations, as we generate the chain of thought using the
model). We also assume there is an initial prompt p0, which
may either contain manually annotated examples or be gen-
erated with zero shot chain of thought (Kojima et al., 2022;
Zhang et al., 2022).
Our algorithm is applied for n iterations, starting with the
initial prompt set {p0}. At the kth iteration we aim to
create a new prompt which generalizes to a region of the
target problem space to which our previous prompt set
{p0 . . . pk−1} performs poorly. To do this, we ﬁrst sam-
ple a set of m candidate reasoning paths and answers for
each problem in DTrain using the most recent prompt, and ap-
pend the m reasoning paths to the m(k −1) reasoning paths
that have already been sampled by the boosted ensemble.
Then, motivated by curriculum learning, we form a new
prompt by selecting correct reasoning paths from those prob-
lems of intermediate difﬁculty, where the current boosted
ensemble only sometimes gets the correct answer. Specif-
ically, we sort the problems where at least one reasoning
path led to the correct answer by the number of correct rea-
soning paths, and select (problem, correct reasoning path)

Boosted Prompt Ensembles for Large Language Models
pairs from amongst the hardest problems. Following Fu
et al. (2022)’s discovery that longer reasoning paths improve
in-context reasoning performance, for each hard problem
chosen, we choose from the reasoning paths that led to a
correct answer by using a complexity heuristic, measured by
the number of sentences in reasoning path (Fu et al., 2022).
Concatenating this set of (problem, correct reasoning path)
pairs forms a new prompt, which we use for the next iter-
ation of the algorithm, until we have a set of n prompts
comprising a boosted ensemble.
To perform inference at test time, we use our language
model to generate m chain of thought answers for each of
the n prompt in our boosted ensemble, and take a majority
vote over the nm predictions. Intuitively, each prompt in
{p0, . . . , pn−1} covers a part of the target problem space.
Those that do not cover the target problem space in which
the test question resides will lead the language model to
fail at answering correctly in likely different ways, while
prompts covering the target problem space in which the
test question resides will likely answer correctly. We study
the effect of varying n and m under a ﬁxed computational
budget in the experiments.
Test-time Boosting
In the absence of training labels, our
algorithm can be adapted to the transductive (where the
entire unlabeled test set is available; see Algorithm 1) and
online (where test problems come one at a time; see Algo-
rithm 2 in Appendix B) settings. We call this “test-time
boosting”. In this case, we substitute ground truth answer
labels with model predictions, using a similar motivation as
(Huang et al., 2022), whereby predictions with “sufﬁcient
agreement” are treated as correct. The deﬁnition of sufﬁ-
cient agreement is a hyperparameter. In our experiments, we
consider sufﬁcient agreement to be achieved for question q
with most common prediction p∗if Σp∈pred[q]I(p = p∗)/nm
is higher than some sufﬁcient agreement hyperparameter ∆.
The algorithm is otherwise the same as the train-time al-
gorithm. Note that since agreement is also used to deter-
mine problem difﬁculty for prompt generation, a natural
tension arises, and test-time boosting chooses easier sam-
ples than the train-time version. For this reason, one would
not generally expect test-time boosting to perform as well
as train-time boosting.
Test-time boosting has one notable advantage over train-
time boosting: in case of distribution shift between train and
test sets, test-time boosting has an opportunity to adapt to
out-of-distribution problems by including them in its prompt
set. In theory, this allows it to do a form of online “prompt
space exploration”, whereby the boosted prompt adapts
to the current problem distribution. Our experiments ﬁnd
some evidence of this possibility, but we leave a thorough
investigation to future work.
4. Experiments
We evaluate the supervised (“train time boosting”) and self-
supervised / transductive (“test time boosting”) versions
of our algorithm on a selection of more difﬁcult reasoning
benchmarks with varying amounts of annotation. Boosted
prompting outperforms baselines on all ﬁve datasets we
evaluated in our experiments.
Our experiments seek to answer the following questions:
• Do boosted prompt ensembles offer a performance
advantage over single prompt and bagged prompt en-
sembles?
• How does our method’s performance vary with the
amount of annotation available?
• How sensitive is boosting to the initial prompt?
• How does varying the number of ensemble members /
samples per ensemble member impact results?
• How does the level of “sufﬁcient agreement” for deter-
mining correctness impact test-time boosting?
• Can we further improve performance by applying
weights to the ensemble members?
• Does choosing from the most complex generated
chains of thought aid performance?
• Does the choice of LLM model impact the relative
performance of boosted prompting?
Model
Our primary experiments are carried out with
the code-davinci-002 (“Codex”) model via the Ope-
nAI API (Chen et al., 2021). As demonstrated by other
papers (Wang et al., 2022b; Fu et al., 2022), performance
trends between methods are consistent across models of
similar sizes, and Codex is the highest performing model on
our tested datasets, outperforming the larger PaLM-540B
(Chowdhery et al., 2022). We thank OpenAI for free access
to this model as part of their beta program but note that,
unfortunately, it has been discontinued. We also verify that
our results generalize to other models (text-davinci
and gpt-3.5-turbo). We describe our implementation
details and link to our code in the appendix.
Datasets
We consider the following datasets:
• AQUA (Algebra QA with Rationales), a dataset of
roughly 100,000 algebraic word problems and 254
test questions, which is sometimes referred to as the
MATHQA dataset due to a follow-up work (Ling et al.,
2017; Amini et al., 2019). We randomly sample 200
training problems for our labeled training set.
• GSM8K (Grade School Math 8k), a dataset of 1319
mathematical word problems curated by human prob-
lem writers (Cobbe et al., 2021). We randomly sample
200 training problems for our labeled training set.
• MMLU570, a stratiﬁed subsample of the Massive Mul-
titask Language Understanding (MMLU) dataset with

Boosted Prompt Ensembles for Large Language Models
Table 1. Results Table: Codex.
Summary: At each level of annotation, boosted prompting improves performance over the self-consistency baseline at that same level of
annotation. With a small training set, boosted prompting scores highest on all tested datasets.
Details: Results are grouped by level of annotation used, with rows corresponding to our method in light green. All baselines used
self-consistency. Since we noticed some variance in results due to self-consistency, we ran multiple seeds of certain rows (shown in the
second column). The second row shows the standard deviation of a 3 seed sample mean, assuming all methods have the same variance.
Cells marked with “-” have no data available. See main text and Appendix A for implementation details.
# Seeds
AQUA
GSM8K
MMLU570
CMATH420
SVAMP
±0.58
±0.27
±0.40
±0.34
±0.15
Large Dataset / Manual
LMSI (PALM 540B)
1
-
82.1
-
-
-
Minerva (PALM 540B)
1
-
78.5
-
-
-
Complexity + SC (reported)
1
58.6
82.6
-
-
-
Complexity + SC (our implementation)
3
60.7
85.1
70.6
40.4
-
Boosted Prompting w/ Complexity (ours)
3
61.2
85.6
70.1
40.0
-
Small Training Set
DiVeRSe Bagging (reported)
1
-
80
-
-
87
Bagging (our implementation)
3
61.3
83.2
70.4
36.5
-
Boosted Prompting (ours)
3
63.4
85.2
71.2
38.7
-
Few Shot Prompt
SC (reported)
1
52
78
-
-
86.8
SC (our implementation)
3
57.0
81.0
70.2
36.8
86.5
Boosted Prompting (ours)
3
61.7
83.3
68.1
39.0
88.6
No Annotation (zero shot)
Auto CoT + SC
1
55.1
78.9
67.7
30.2
86.2
Boosted Prompting w/ Auto CoT (ours)
1
57.1
82.3
68.1
29.8
86.6
Nonsense Annotation
Nonsense Prompt + SC
1
48
70.3
64.6
30
84.3
Boosted Prompting w/ Nonsense (ours)
1
54.7
79.7
66.8
35.7
86.8
570 multiple choice questions composed of 10 ques-
tions sampled from each of the 57 MMLU subjects
(Hendrycks et al., 2020). We use the 285 (5x57) sam-
ple dev set for our labeled training set.
• CMATH420, a stratiﬁed subsample of the challenging
Competition Math dataset with 12 test samples from
each of the 35 subject-level pairs (Hendrycks et al.,
2021). We use 71 Level 1 Prealgebra problems from
the training set (all such problems that meet our sub-
sampling criteria) for our labeled training set.
• SVAMP (Simple Variations on Arithmetic Math word
Problems), a set of 1000 algebraic word problems de-
signed to test different aspects of reasoning (Patel et al.,
2021). We do not have a labeled training set and do
not do train-time boosting for this dataset.
Baselines
Our main baseline is self-consistency (SC)
(Wang et al., 2022c), which uses a single prompt and creates
an ensemble in output space, by taking the plurality of n
positive temperature generations.
For alternative annotation settings (see next Section), we
combine self-consistency with various approaches to choos-
ing the few-shot prompt, including as baselines: (a) self-
consistency with auto CoT (Zhang et al., 2022), which boot-
straps the model’s own zero-shot CoT to form a few-shot
prompt, (b) self-consistency with complexity prompting (Fu
et al., 2022), which chooses the few shot examples to use a
maximal number of reasoning steps, and (c) self-consistency
with bagged prompts (Li et al., 2022b), which chooses sev-
eral few shot prompts at random. The self-consistency au-
thors also considered bagged and random prompt order
ensembles, but found that neither had a noticeable effect
on results relative to single-prompt output-space ensembles
(self-consistency) (Wang et al., 2022b).
To ensure a fair comparison, we re-implement each baseline

Boosted Prompt Ensembles for Large Language Models
in our codebase, which uses slightly different formatting and
answer and prediction extraction for GSM8K (see Appendix
A for details). In our implementation, we also extend all
baselines to use 100-path self-consistency. For the baselines
that reported results with self-consistency, we report both
the results from the original works as well as those from our
implementation.
4.1. Results
The main results are reported in Table 1, grouped by the
type of annotation used by the method. As self-consistency
introduces some stochasticity in small datasets, we aver-
age results over several seeds for both our method and our
implementations of the baselines (see table caption). All
of our Table 1 results consider 100 ensemble generations
sampled at temperature T = 0.7, which comes out to 100
single prompt samples for self-consistency, and for bagging
and boosting, 10 samples from each of 10 prompts.
Do boosted prompt ensembles offer a performance ad-
vantage over single prompt and bagged ensembles?
Yes, in all cases, when a small training dataset (50-300
samples) is available, we ﬁnd that boosting is superior to
randomly bagging few shot examples as well as to single-
prompt self-consistency. The difference to the latter can be
quite large if the initial prompt is suboptimal, as observed
in case of AQUA, where train time boosting obtains 63.5%
as compared to the 57% obtained by single prompt self-
consistency.
How does our method’s performance vary with the
amount of annotation available?
We consider four levels of annotation:
• Small training set (50-300 samples) with ground truth
labels. See “Datasets” above for details.
• A few shot CoT prompt of ≤8 examples. In this case,
we use the prompts from past work where available, or
make our own. See Appendix D for exact prompts.
• No relevant annotation (zero shot). Following Zhang
et al. (2022), we assume access to the entire test set
(transductive setting) when forming predictions, but
note that boosted prompting could also be applied on-
line. We use the Auto CoT method (Zhang et al., 2022)
to form a few shot prompt by applying zero shot CoT
(Kojima et al., 2022) to a sample from the test set. This
approach has been validated by Huang et al. (2022),
so we do not also consider a direct zero shot + SC
baseline.
• A pseudo-adversarial case, where the same nonsense
two shot prompt is provided for all datasets.
We also list four baselines that require a larger training set
or targeted manual annotation. The Minerva (Lewkowycz
et al., 2022) and LMSI (Huang et al., 2022) baselines ﬁne-
tune PALM 540B (Chowdhery et al., 2022). PALM has
worse baseline performance than Codex, which explains
why unﬁnetuned boosted prompting outperforms these ﬁne-
tuned baselines. Our method is compatible with ﬁnetun-
ing (see gpt-3.5-turbo results below), and could be
used together with LMSI to further improve their self-
annotations.
At each level of annotation considered, boosted prompt-
ing improves performance over the corresponding self-
consistency baseline. We see that boosted prompting is able
to take advantage of the small training set when it is avail-
able, with the train-time version generally outperforming
the test-time version. The one exception is on CMATH420,
where the training set is quite small (71) and there is signiﬁ-
cant distributional shift between train and test. This provides
some support for the hypothesis that test-time boosting can
do online “prompt space exploration”, although we leave a
thorough investigation of this to future work.
How sensitive is boosting to the initial prompt?
From Table 1, we notice that as the quality of the prompt
deteriorates, from the original, manually annotated few shot
setting, to the zero shot setting, to the nonsense setting, so
too does the performance of boosted prompting. This is un-
derstandable, because boosted prompting uses the model to
provide self supervised chains of thought for all subsequent
ensemble members and a worse initial prompt means worse
self supervision. The performance of the original prompt is
also a direct factor since we keep the original prompt as one
of the ten ensemble members.
How does varying the number of ensemble members /
samples per ensemble member impact results?
In Table 2, we report results on AQUA where we vary the
number of prompts generated and reasoning paths we take
from each prompt such that the total computational budget
is ﬁxed. We found the impact to be relatively small: all
settings outperform bagging and self-consistency baselines.
Table 2. Boosted Ensemble Composition. Varying the ensemble
composition has only minor performance impact. For a similar
ablation re: bagged ensemble composition, see Li et al. (2022b),
Table 4. We use 3 seeds for 10x10 and 1 seed for the other settings.
.
n x m
10x10
20x5
33x3
50x2
AQUA
63.4
62.6
63.0
62.6
How does the level of “sufﬁcient agreement” for deter-
mining correctness impact test-time boosting?
In the absence of training labels, our algorithm substitutes

Boosted Prompt Ensembles for Large Language Models
Table 3. Results Table: Curie / Davinci / GPT-3.5.
Summary: Boosted prompting improves upon baselines for the more capable Davinci and GPT-3.5 models. On the less capable Curie
model, boosted prompting does not appear to help.
Details: Each result in this table considers 40 ensemble generations sampled at temperature T = 0.7 (4 samples from each of 10 prompts
in the case of bagging and boosting). Each result is a single seed.
text-curie-001
text-davinci-003
gpt-3.5-turbo
AQUA
GSM8K
AQUA
GSM8K
AQUA
GSM8K
Small Training Set
Bagging (our implementation)
18.5
3.0
53.9
69.9
75.2
87.7
Boosted Prompting (ours)
14.2
3.2
55.9
71.7
76.4
88.2
Few Shot Prompt
SC (our implementation)
18.1
3.9
53.1
69.6
72.0
86.7
Boosted Prompting (ours)
13.0
3.1
55.1
71.6
72.8
87.1
ground truth answer labels with model predictions, using
a similar motivation as (Huang et al., 2022), whereby pre-
dictions with “sufﬁcient agreement” are treated as correct.
There is an inherent tradeoff: setting sufﬁcient agreement
higher means the predictions are more likely correct, but the
selected problems may be less useful as a prompt, since the
model already knows how to solve these problems. Figure 2
shows the AQUA performance and average prompt accuracy
as we vary the minimum agreement hyperparameter, which
indicates the threshold at which we consider model gener-
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Minimum Agreement Deemed "Suitable"
50
52
54
56
58
60
62
64
AQUA Performance
Suitable Agreement Ablation
AQUA Performance
Average Prompt Accuracy
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Average Prompt Accuracy
Figure 2. Suitable Agreement for Test-Time Boosting.
The
level of suitable agreement required for model generated answers
to be deemed “correct”, for purposes of new prompt formation,
can greatly reduce the performance of test-time boosting if it is too
low. In our main results we use a relatively high value of 0.8 for
multiple choice datasets (AQUA and MMLU570) and a value of
0.7 for open-ended datasets. We used one seed on AQUA to study
the effect of this hyperparameter.
ated answers to be correct. Setting minimum agreement
too low greatly reduces average prompt accuracy, since we
might form prompts by choosing questions and answers
pairs the model believes to be correct but are in fact not.
This can then lead to a decrease in test-time performance, as
the boosted prompts may contain more incorrect examples.
Can we further improve performance by applying
weights to the ensemble members?
Classical boosting applies weights to the ensemble members
(Freund et al., 1999). Though lacking in mathematical mo-
tivation, we consider applying weights to boosted prompt
ensembles via the K-class Adaboost formula from Hastie
et al. (2009):
wi = log [(1 −erri)/erri] + log(K −1)
except that we replace log(K −1) with a non-negative para-
metric offset that we optimize over using the training set.
The results are shown in Table 4. We ﬁnd the overall re-
sults unremarkable and reminiscent of the weighting results
of Wang et al. (2022c), who found that weighted voting
provided little advantage over a simple average.
Table 4. Weighted Boosting. 3 seeds each. Applying weights to
the boosted ensemble has only minor performance impact.
Unweighted
Weighted
GSM8K
85.2
85.5
AQUA
63.4
63.8
MMLU570
71.2
70.7
CMATH420
38.7
38.4
Does choosing from the most complex generated chains
of thought aid performance?
When forming boosted prompts, the CoT is chosen from the
several model samples that led to the correct (or majority)

Boosted Prompt Ensembles for Large Language Models
answer. Following Fu et al. (2022), we sample from amongst
the 5 most complex, which we determine by the combined
number of “\n” and “. ” substrings. Table 5 suggest that
this choice improves performance.
Table 5. Choosing Complex vs Random CoTs.
Sampling
randomly from model-generated CoTs (1 seed), instead of
sampling from amongst the most complex CoTs (3 seeds),
performs slightly worse, but still better than baselines.
.
Complex CoT
Random CoT
AQUA (train-time)
63.4
62.2
AQUA (test-time)
61.7
60.2
How does the choice of base LLM model impact relative
performance?
Table 3 contains the results of a subset of the experiments
from Table 1 on three other models: text-curie-001,
which
is
smaller
than
code-davinci-002;
text-davinci-003
model,
which
is
the
same
size; and gpt-3.5-turbo, which is an effective model
of unknown size that been ﬁnetuned on related data.
We see that boosted prompting is similarly effective for
stronger davinci and gpt-3.5 models, but not for the
weaker curie model. We hypothesize this is because a
minimal level of accuracy is needed to self-generate the
boosted prompts (see Figure 2, which shows a nearly 1:1
correlation between average prompt accuracy and AQUA
performance).
4.2. t-SNE visualization
In Figure 3, we show a visualization of the ﬁrst ﬁve
boosted prompts found by our train-time algorithm on
the 1319 question GSM8K test set. We used OpenAI’s
text-embedding-ada-002 model to generate 1536-
dimensional embeddings for (question, reasoning) tuples on
the test set and the prompts (from the training set) found
by our algorithm. After applying t-SNE, we see that the
coverage of boosted prompt set grows to cover the space.
5. Conclusion
In this paper, we adapted classical boosting to the language
model setting and proposed a few shot boosted prompt en-
sembling algorithm. On several reasoning benchmarks, we
showed that boosted prompt ensembles outperform single
prompt and bagged ensembles, especially when the initial
prompt is suboptimal.
We proposed two variants of the algorithm, a train-time al-
gorithm that uses training labels to select Hard problems
and a test-time version that substitutes labels with model
predictions. We hypothesized that our test-time boosting
algorithm can function as a form of self-guided curriculum
40
20
0
20
40
40
20
0
20
40
Evolution of First Five Boosted 
 Prompt Embeddings on GSM8K
Figure 3. t-SNE Visualization of embeddings on (question, reason-
ing) tuples on GSM8K dataset. The embeddings of the test set are
scattered as faded blue dots. The prompts are shown with stars.
The black stars are the initial prompt, and the next four prompts are
shown in red, orange, yellow, and white, respectively. Although
the initial prompt is biased toward the upper left side, later prompts
explore the space, so that the ensemble has good coverage overall.
and perform test-time adaptation via online prompt space
exploration. Though we provided some empirical evidence
of this, we leave a thorough investigation of this to future
work. We observed in our experiments that the performance
of test-time boosting is strongly correlated with prompt ac-
curacy. To further improve the effectiveness of test-time
boosting and allow for prompt space exploration, future
work may consider better options for verifying prompt ac-
curacy, such as the use of a veriﬁer (Cobbe et al., 2021; Li
et al., 2022b), or debate (Irving et al., 2018).
Acknowledgements
We thank Harris Chan, Keiran Paster, and Yongchao Zhou
for helpful comments and discussions. We thank OpenAI
for access to the Codex model as part of their beta program.
Resources used in preparing this research were provided, in
part, by the Province of Ontario, the Government of Canada,
and companies sponsoring the Vector Institute.
References
Amini, A., Gabriel, S., Lin, P., Koncel-Kedziorski, R., Choi,
Y., and Hajishirzi, H. Mathqa: Towards interpretable math
word problem solving with operation-based formalisms.
arXiv preprint arXiv:1905.13319, 2019.
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-
non, C., et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073, 2022.

Boosted Prompt Ensembles for Large Language Models
Baranes, A. and Oudeyer, P.-Y. Active learning of inverse
models with intrinsically motivated goal exploration in
robots. Robotics and Autonomous Systems, 61(1):49–73,
2013.
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
Curriculum learning. In Proceedings of the 26th annual
international conference on machine learning, pp. 41–48,
2009.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-
lut, A., Brunskill, E., et al. On the opportunities and risks
of foundation models. arXiv preprint arXiv:2108.07258,
2021.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,
G., et al. Evaluating large language models trained on
code. arXiv preprint arXiv:2107.03374, 2021.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311, 2022.
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., et al. Training veriﬁers to solve math word problems.
arXiv preprint arXiv:2110.14168, 2021.
Freund, Y., Schapire, R., and Abe, N. A short introduc-
tion to boosting. Journal-Japanese Society For Artiﬁcial
Intelligence, 14(771-780):1612, 1999.
Fu, Y., Peng, H., Sabharwal, A., Clark, P., and Khot, T.
Complexity-based prompting for multi-step reasoning.
arXiv preprint arXiv:2210.00720, 2022.
Ganaie, M. A., Hu, M., Malik, A., Tanveer, M., and Sugan-
than, P. Ensemble deep learning: A review. Engineering
Applications of Artiﬁcial Intelligence, 115:105151, 2022.
Gao, T., Fisch, A., and Chen, D. Making pre-trained lan-
guage models better few-shot learners. arXiv preprint
arXiv:2012.15723, 2020.
Hastie, T., Rosset, S., Zhu, J., and Zou, H. Multi-class
adaboost. Statistics and its Interface, 2(3):349–360, 2009.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
M., Song, D., and Steinhardt, J.
Measuring mas-
sive multitask language understanding. arXiv preprint
arXiv:2009.03300, 2020.
Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,
S., Tang, E., Song, D., and Steinhardt, J. Measuring math-
ematical problem solving with the math dataset. NeurIPS,
2021.
Hou, B., O’Connor, J., Andreas, J., Chang, S., and Zhang,
Y. Promptboosting: Black-box text classiﬁcation with ten
forward passes. arXiv preprint arXiv:2212.09257, 2022.
Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and
Han, J. Large language models can self-improve. arXiv
preprint arXiv:2210.11610, 2022.
Irving, G., Christiano, P., and Amodei, D. Ai safety via
debate. arXiv preprint arXiv:1805.00899, 2018.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
Y. Large language models are zero-shot reasoners. arXiv
preprint arXiv:2205.11916, 2022.
Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple
and scalable predictive uncertainty estimation using deep
ensembles. Advances in neural information processing
systems, 30, 2017.
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E.,
Michalewski, H., Ramasesh, V., Slone, A., Anil, C.,
Schlag, I., Gutman-Solo, T., et al. Solving quantitative
reasoning problems with language models. arXiv preprint
arXiv:2206.14858, 2022.
Li, J., Zhang, Z., and Zhao, H.
Self-prompting large
language models for open-domain qa. arXiv preprint
arXiv:2212.08635, 2022a.
Li, X. L. and Liang, P. Preﬁx-tuning: Optimizing continuous
prompts for generation. arXiv preprint arXiv:2101.00190,
2021.
Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G.,
and Chen, W. On the advance of making language mod-
els better reasoners. arXiv preprint arXiv:2206.02336,
2022b.
Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Pro-
gram induction by rationale generation: Learning to solve
and explain algebraic word problems. arXiv preprint
arXiv:1705.04146, 2017.
Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen,
W. What makes good in-context examples for gpt-3?
arXiv preprint arXiv:2101.06804, 2021.
Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,
G. Pre-train, prompt, and predict: A systematic survey of
prompting methods in natural language processing. ACM
Computing Surveys, 55(9):1–35, 2023.

Boosted Prompt Ensembles for Large Language Models
Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,
Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,
M., Luan, D., et al. Show your work: Scratchpads for
intermediate computation with language models. arXiv
preprint arXiv:2112.00114, 2021.
Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models
really able to solve simple math word problems? arXiv
preprint arXiv:2103.07191, 2021.
Qin, G. and Eisner, J.
Learning how to ask: Query-
ing lms with mixtures of soft prompts. arXiv preprint
arXiv:2104.06599, 2021.
Rubin, O., Herzig, J., and Berant, J.
Learning to re-
trieve prompts for in-context learning. arXiv preprint
arXiv:2112.08633, 2021.
Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and
Singh, S. Autoprompt: Eliciting knowledge from lan-
guage models with automatically generated prompts.
arXiv preprint arXiv:2010.15980, 2020.
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,
Garriga-Alonso, A., et al. Beyond the imitation game:
Quantifying and extrapolating the capabilities of language
models. arXiv preprint arXiv:2206.04615, 2022.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tention is all you need. Advances in neural information
processing systems, 30, 2017.
Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer,
L., and Sun, H. Towards understanding chain-of-thought
prompting: An empirical study of what matters. arXiv
preprint arXiv:2212.10001, 2022a.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and
Zhou, D. Rationale-augmented ensembles in language
models. arXiv preprint arXiv:2207.00747, 2022b.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,
and Zhou, D.
Self-consistency improves chain of
thought reasoning in language models. arXiv preprint
arXiv:2203.11171, 2022c.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.
Zelikman, E., Wu, Y., and Goodman, N. D. Star: Boot-
strapping reasoning with reasoning.
arXiv preprint
arXiv:2203.14465, 2022.
Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic
chain of thought prompting in large language models.
arXiv preprint arXiv:2210.03493, 2022.
Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.
Calibrate before use: Improving few-shot performance
of language models.
In International Conference on
Machine Learning, pp. 12697–12706. PMLR, 2021.
Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S.,
Chan, H., and Ba, J. Large language models are human-
level prompt engineers. arXiv preprint arXiv:2211.01910,
2022.

Boosted Prompt Ensembles for Large Language Models
A. Implementation Details
We use certain functions from the existing implementations of Kojima et al. (2022) (available at https://github.
com/kojima-takeshi188/zero_shot_cot) and Wang et al. (2022c) (available via their ICLR supplement), but
largely build out our own implementation, which we have made available at https://github.com/awwang10/
llmpromptboosting.
Our results on GSM8K differ slightly from past work due to a minor change in answer/prediction extraction and formatting:
• GSM8K: We apply the answer cleansing function to the answers, whereas past work applied it only to the prediction.
We did this after noticing that sometimes answer cleansing on the prediction would change what should have been a
correct answer (e.g., “200,000”) to a different format (e.g., “200000”) after which it was counted as incorrect. This
change improves results by 0.6-0.9%:
GSM8K results w/o cleaning answers
Bagging
82.4 (-0.6)
Train Time Boosting
84.3 (-0.9)
Test Time Boosting
82.5 (-0.8)
Self Consistency
80.2 (-0.8)
We use 100-shot self-consistency for all implementations. As we noticed some variance, especially in AQUA results (due to
a relatively small test set of 254 samples), we ran multiple seeds of the main results and baselines, as shown in Table 1.
To choose which questions to use for bagged prompts, we sample randomly with replacement from the training set.
To choose which questions to use for boosted prompts, we sort the “suitable” question/answer pairs in ascending order of
agreement with respect to the desired answer (the correct answer for the train-time algorithm, or the majority answer for the
test-time algorithm), and sample 8 questions from the 24 suitable question/answer pairs with lowest agreement (we speculate
that some randomness helps, so as to not repeat prompts if nothing changes between iterations, but we did not speciﬁcally
ablate our choice of “24”). “Suitable” is deﬁned as having at least 1 correct generation for the train-time algorithm, and
having at least “minimum agreement” agreement with respect to the predicted answer at test-time. Our test-time boosting
uses a minimum agreement of 0.7 for open-ended datasets (GSM8K, CMATH420, SVAMP), and a slightly higher minimum
agreement of 0.8 for multiple choice datasets (AQUA, MMLU570) because the answer distribution has lower entropy. The
initial value (and the algorithm generally) was developed on a 300 question subset of the GSM8K training set, but not
extensively tuned.
Having selected which questions and answers to use for our boosted (or bagged) prompts, we select the chain of thought
from the model-generated chains of thought by sampling it randomly from the 5 most complex chain of thoughts that led to
the desired answer (once again, we speciulate that some randomness helps, but we did not speciﬁcially ablate our choice of
“5”). For this purpose, complexity was measured as:
len(cot.replace(’\n’,’. ’).split(’. ’))
To reduce computational complexity at test-time, we move all predictions that have met the bar for “suitable agreement” to
a “solved set”, and consider their predictions ﬁnal. We only continue to generate predictions with newer prompts on the
remaining unsolved questions. When doing test-time boosting on GSM8K with a suitable agreement parameter of 0.7, the
unsolved set is only 280 samples of the original 1319. Thus, the total number of generations required is much smaller than
100*1319. The breakdown of performance on GSM8K at the 10th such iteration of the ﬁrst seed is:
Count
Agreement
Performance
Unsolved
280
27%
46%
“Solved”
1039
83%
92%
Overall
1319
71%
83%
When applying boosted ensembles learned at train time to the test dataset, we use a suitable agreement parameter of 0.9 for

Boosted Prompt Ensembles for Large Language Models
all datasets. This should not inﬂuence ﬁnal predictions much, while still saving non-negligible computation. The breakdown
of performance on GSM8K at the 10th such iteration of the ﬁrst seed is:
Count
Agreement
Performance
Unsolved
593
38%
72%
“Solved”
726
94%
96%
Overall
1319
69%
85%
The sample mean standard deviations shown in Table 1 are based on the 12 seeds from Bagging, Boosted Prompting (Small
Training Set), Self Consistency, and Boosted Prompting (Few Shot Prompt), by assuming that all methods have the same
variance. We ﬁrst compute the sample standard deviation using 12 samples of x −ˆx, and then divide by
√
3 to estimate the
standard deviation of the 3 seed sample mean.
B. Online Boosted Prompting
We can run Boosted Prompting online by adding to our set of prompts whenever a new diverse prompt becomes available.
For the ﬁrst few questions, there will only be one prompt, and Boosted Prompting will be identical to Self Consistency. Once
a few questions have been answered with sufﬁcient agreement, a new prompt can be added to the ensemble and applied
going forward. This modiﬁed algorithm is shown in Algorithm 2.
Algorithm 2 Boosted Prompting (Online). The BOOSTEDPROMPTING function is meant to be called each time a new
problem or batch of problems is added to T. Past predictions are cached via pastp, and m/n are adjusted automatically.
function BOOSTEDPROMPTING(model LLM, prompt set
prompts, past predictions pastp, cumulative problem set
T, total generations N):
preds ←{q : pastp[q] for q in T} # [ ] if q is new
n, m ←len(prompts), N/m
for
in range(n) do
for q in T do
for
in range(m) if len(preds[q]) < N do
preds[q].append([LLM(p, q)])
p ←NEWPROMPT (T, preds)
# evaluates to True if new prompt is generated
if p then
prompts.add(p)
n, m ←len(prompts), N/m
return prompts, preds
function NEWPROMPT(T, preds):
C ←[q for q in T if there is “sufﬁcient agreement” for
the majority prediction in preds[q]]
C ←choose 8 of the questions for which the majority
prediction in preds[q] has minimal agreement,
if cannot, return None
p ←for each question q in C choose one CoT from
preds[q] that led to the answer/prediction
▷future work: ensure new prompt is sufﬁciently different
from past prompts, and/or remove old prompts that aren’t
diverse enough
return p
C. Further results
C.1. Qualitative example difﬁculty
We investigate whether a measure of example difﬁculty can be identiﬁed via our ensemble of prompts and multiple
generations for each prompt on AQUA. At test-time, we compute the empirical distribution over answer choices from
running ten rounds of our algorithm (with ten generated responses at each iteration). Responses with no answers are
removed. We then sorted the problems by the probability assigned to the option the model is most conﬁdent and report the
5 most extreme probabilities and their corresponding questions in Table 6 and Table 7. Qualitatively, questions with low
conﬁdence appear to be more challenging, ambiguous, or contain spelling errors. The opposite is true for questions with
high conﬁdence.
C.2. t-SNE Visualization on AQUA
In Figure 4, we show a visualization of the prompts found by our algorithm on the 200 question AQUA training set. We
used OpenAI’s “text-embedding-ada-002,” to generate 1536-dimensional embeddings for (question, reasoning) tuples on

Boosted Prompt Ensembles for Large Language Models
Table 6. Test questions where the model is least conﬁdent. Questions tend to be more challenging, ambiguous, or contain spelling errors.
Max probability is computed by the empirical distribution from running ten rounds of our algorithm (with ten generated responses at each
iteration). We copy the questions verbatim here (including typos).
QUESTION: Of the three- digit integers greater than 700, how many have two digits T that are equal to each other and the
remaining digit different from the other two?
ANSWERS: [’A)90’, ’B)82’, ’C)80’, ’D)45’, ’E)36’]
MAX PROBABILITY: 0.255
QUESTION: What annual payment dischargea debit of Rs.12900, due in 4yrs.At 5% rate?
ANSWERS: [’A)2000’, ’B)1300’, ’C)3000’, ’D)4300’, ’E)4500’]
MAX PROBABILITY: 0.255
QUESTION: 81,162,49,98,25,50,55
ANSWERS: [’A)110’, ’B)127’, ’C)129’, ’D)128’, ’E)131’]
MAX PROBABILITY: 0.255
QUESTION: Two dice are thrown together .What is the probability that the sum of the number on the two faces is divided
by 4 or 6
ANSWERS: [’A)5/7’, ’B)3/4’, ’C)7/18’, ’D)5/17’, ’E)7/9’]
MAX PROBABILITY: 0.258
QUESTION: The Coen family consists of a father, a mother, two children and a dog. A photographer is about to take the
family’s picture. How many different arrangements (of standing in a row) does the photographer have, if it is known that the
father insists of standing by his woman, as the song says?
ANSWERS: [’A)12’, ’B)24’, ’C)48’, ’D)60’, ’E)120’]
MAX PROBABILITY: 0.260
the training set and the prompts found by our algorithm. After applying t-SNE, we see that the boosted prompts achieve
coverage over the space and are typically found near incorrect questions.

Boosted Prompt Ensembles for Large Language Models
Table 7. Test questions where the model is most conﬁdent. Max probability is computed by the empirical distribution from running ten
rounds of our algorithm (with ten generated responses at each iteration). We copy the questions verbatim here (including typos).
QUESTION: 200 + 8 × 4 = ?
ANSWERS: [’A)232’, ’B)202’, ’C)420’, ’D)209’, ’E)None of these’]
MAX PROBABILITY: 0.917
QUESTION: The price of a book is increased from 300to330. What is the % of increase in its price?
ANSWERS: [’A)10%’, ’B)20%’, ’C)40%’, ’D)50%’, ’E)60%’]
MAX PROBABILITY: 0.919
QUESTION: What is the product of all the prime factors of 13?
ANSWERS: [’A)12’, ’B)11’, ’C)13’, ’D)15’, ’E)18’]
MAX PROBABILITY: 0.930
QUESTION: Two brother X and Y appeared for an exam. The probability of selection of X is 1/5 and that of B is 2/3. Find
the probability that both of them are selected.
ANSWERS: [’A)2/69’, ’B)2/20’, ’C)2/18’, ’D)2/29’, ’E)2/15’]
MAX PROBABILITY: 0.969
QUESTION: A man buys an article for 100.andsellsitfor110. Find the gain percent?
ANSWERS: [’A)10%’, ’B)15%’, ’C)25%’, ’D)20%’, ’E)30%’]
MAX PROBABILITY: 0.970
20
10
0
10
20
20
10
0
10
20
30
correct
wrong
prompts
0
1
Figure 4. Visualization of prompt embeddings on (question, reasoning) tuples on AQUA. The darker shades of blue indicate prompts
generated at later iterations. Our boosting algorithm generates prompts which cover locations adjacent to incorrect answers.

Boosted Prompt Ensembles for Large Language Models
D. Prompts
D.1. Example Boosted Prompt
An example boosted prompt, on GSM8K, constructed during test time boosting is below. The answers are generated by the
model, and some are incorrect, as the overall accuracy of the prompt is only 75%.
Info:
New prompt accuracy: [ True
True
True False False
True
True
True] 0.75
Answers: [’144’ ’3000’ ’98’ ’10’ ’105’ ’10’ ’5’ ’1520’]
The prompt:
Q: A local town is expanding and wants to build several new homes across the
next three years. In the first year, they will build 12 homes. In the next
year, they will build three times this many homes. In the third year,
they will count how many homes they have built and double the amount. How
many homes will the town have built over the next three years?
A: In the first year, 12 homes will be built. In the second year, 3 times this
many homes will be built, so that is 3 * 12 = 36 homes. In the third year,
the town will count how many homes they have built so far and double the
amount, so that is 2 * (12 + 36) = 2 * 48 = 96 homes. In total, they will
build 12 + 36 + 96 = 144 homes. The answer is 144.
Q: Stanley was standing on the side of the mountain, at an elevation of 10,000
feet, when a gust air blew the comb out of his pocket, causing the comb
to fall 4,000 feet to a ledge below. Stanley’s brother, Oliver, was also
on the mountain, but he was at an elevation of 3,000 feet. Stanley called
Oliver on his cellphone and asked Oliver to find Stanley’s comb. How many
feet must Oliver climb to reach the elevation of Stanley’s comb?
A: Stanley was on the top of the mountain, at an elevation of 10,000 feet. The
comb fell 4,000 feet, so the comb is now at an elevation of 10,000 -
4,000 = 6,000 feet. Oliver is at an elevation of 3,000 feet. To get to the
comb, Oliver must climb 6,000 - 3,000 = 3,000 feet. The answer is 3,000.
Q: Janet had 22 green pens and 10 yellow pens. Then she bought 6 bags of blue
pens and 2 bags of red pens. There were 9 pens in each bag of blue and 6
pens in each bag of red. How many pens does Janet have now?
A: She initially had 22 green and 10 yellow pens, so in total she had 22 + 10
= 32 pens. She bought 6 bags of blue and 2 bags of red. There were 9 pens
in each bag of blue, so in total she got 6 * 9 = 54 blue pens. There were
6 pens in each bag of red, so in total she got 2 * 6 = 12 red pens. In
total she got 54 + 12 = 66 pens. So after buying, she has 32 + 66 = 98
pens in total. The answer is 98.
Q: Sara wants to buy herself a new jacket and 2 pairs of shoes. The jacket she
wants costs $30 and each pair of shoes cost $20. Sara babysits the
neighbor’s kids 4 times, earning $5 each time she babysits them. Her
parents pay her $4 each time she mows the lawn. If Sara already had $10
saved before she started babysitting, how many times must she mow the lawn
before she can afford the jacket and shoes?
A: The jacket costs $30 and the shoes cost $20 each. That means she needs $30
+ $20 + $20 = $70 to buy them all. She already has $10 saved. To buy the
jacket and shoes, she needs to save another $70 - $10 = $60. She earns $5
each time she babysits and $4 each time she mows the lawn. To earn $60,
she must mow the lawn 15 times: $60 / $4 = 15. The answer is 15.
Q: Elijah has one dog that is one-fourth the weight of Kory’s dog and another
dog that is half the weight of Kory’s dog. If Kory’s dog is 60 pounds, how
much do Elijah and Kory’s dogs weigh altogether, in pounds?
A: We are told that Kory’s dog weighs 60 pounds. We are also told that Elijah
has two dogs. One of his dogs weighs 1/4 of Kory’s dog and the other is
half as heavy as Kory’s dog. That means that Elijah has a dog that weighs
1/4 * 60 = 15 pounds and another dog that weighs 1/2 * 60 = 30 pounds. In

Boosted Prompt Ensembles for Large Language Models
total, he has a dog that weighs 15 + 30 = 45 pounds. The answer is 45.
Q: A clothing store has 40 white shirts and 50 floral shirts. Half of the
white shirts have collars, and 20 of the floral shirts have buttons. How
many more floral shirts with no buttons are there than white shirts with
no collars?
A: Half of the white shirts have collars, so the rest have no collars. That
means white shirts with no collars are 40 / 2 = 20. 20 of the floral
shirts have buttons. That means the rest have no buttons. The total number
of floral shirts is 50. So floral shirts with no buttons must be 50 - 20
= 30. The difference is 30 - 20 = 10. The answer is 10.
Q: Gary likes to walk around the edge of the local park, which is a rectangle
that measures 1.5 miles by 6 miles. If he walks at 3 miles/hour, how many
hours does he spend walking?
A: The perimeter of a rectangle is the sum of its sides. Here the perimeter
must be 1.5 + 6 + 1.5 + 6 = 15 miles. He walks at 3 miles/hour. So in 15 /
3 = 5 hours he will complete the walk. The answer is 5 hours.
Q: While working at the restaurant, each of the forty customers who came into
the restaurant gave Rafaela a $20 tip. Julieta received 10% less money in
tips than Rafaela. How much money did Julieta and Rafaela receive as tips
altogether?
A: There were 40 customers. They gave Rafaela $20 each, so in total she
received 40 * $20 = $800. Julieta received 10% less, so she received $800 -
10% of $800 = $720. In total, Rafaela and Julieta received $800 + $720 =
$1520. The answer is 1520.
D.2. Initial Prompts
Few Shot
The following prompts are used in the “few shot” annotation setting:
GSM8K
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees.
How many trees did the grove workers plant today?
A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have
planted 21 - 15 = 6 trees. The answer is 6.
Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.
Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74 chocolates. 35 have been
eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.
Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to
Denny?
A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of lollipops he has
given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.
Q: Shawn has ﬁve toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so in total he has 7 + 2
= 9 toys. The answer is 9.
Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday.
How many computers are now in the server room?
A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 = 20 computers
were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers. The answer is 29.
Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he
have at the end of wednesday?
A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On Wednesday he lost 2 more
so now he has 35 - 2 = 33 balls. The answer is 33.
Q: Olivia has $23. She bought ﬁve bagels for $3 each. How much money does she have left?
A: She bought 5 bagels for $3 each. This means she spent 5 * $3 = $15 on the bagels. She had $23 in beginning, so now she
has $23 - $15 = $8. The answer is 8.
AQUA

Boosted Prompt Ensembles for Large Language Models
Q: John found that the average of 15 numbers is 40. If 10 is added to each number then the mean of the numbers is? Answer
Choices: (a) 50 (b) 45 (c) 65 (d) 78 (e) 64
A: If 10 is added to each number, then the mean of the numbers also increases by 10. So the new mean would be 50. The
answer is (a).
Q: If a / b = 3/4 and 8a + 5b = 22,then ﬁnd the value of a. Answer Choices: (a) 1/2 (b) 3/2 (c) 5/2 (d) 4/2 (e) 7/2
A: If a / b = 3/4, then b = 4a / 3. So 8a + 5(4a / 3) = 22. This simpliﬁes to 8a + 20a / 3 = 22, which means 44a / 3 = 22. So a is
equal to 3/2. The answer is (b).
Q: A person is traveling at 20 km/hr and reached his destiny in 2.5 hr then ﬁnd the distance? Answer Choices: (a) 53 km (b)
55 km (c) 52 km (d) 60 km (e) 50 km
A: The distance that the person traveled would have been 20 km/hr * 2.5 hrs = 50 km. The answer is (e).
Q: How many keystrokes are needed to type the numbers from 1 to 500? Answer Choices: (a) 1156 (b) 1392 (c) 1480 (d) 1562
(e) 1788
A: There are 9 one-digit numbers from 1 to 9. There are 90 two-digit numbers from 10 to 99. There are 401 three-digit
numbers from 100 to 500. 9 + 90(2) + 401(3) = 1392. The answer is (b).
MMLU570
Q: Which of the following represents an accurate statement concerning arthropods? (a) They possess an exoskeleton composed
primarily of peptidoglycan. (b) They possess an open circulatory system with a dorsal heart. (c) They are members of a
biologically unsuccessful phylum incapable of exploiting diverse habitats and nutrition sources. (d) They lack paired, jointed
appendages.
A: Arthropods have an exoskeleton. This exoskeleton is not composed primarily of peptidoglycan. Arthropods have an open
circulatory system with a dorsal heart. Arthropods are very successful in exploiting diverse habitats and nutrition sources.
Arthropods have paired, jointed appendages. The answer is (b).
Q: The energy for all forms of muscle contraction is provided by: (a) ATP. (b) ADP. (c) phosphocreatine. (d) oxidative
phosphorylation.
A: The energy for all forms of muscle contraction is provided by ATP. The answer is (a).
Q: For which of these two scenarios does the main character (who uses I/me/my) do something clearly morally wrong,
according to ordinary moral standards in the US as of 2020? Scenario 1 — I asked for a cigarette from the guy outside the
store. Scenario 2 — When the security guard left his post, I pocketed a rare coin. (a) Wrong, Wrong (b) Wrong, Not wrong (c)
Not wrong, Wrong (d) Not wrong, Not wrong
A: The main character in Scenario 1 does not do something morally wrong, according to ordinary moral standards in the US as
of 2020. In Scenario 2, the main character does something clearly morally wrong, according to ordinary moral standards in the
US as of 2020. The answer is (c).
Q: Consider the following AR(1) model with the disturbances having zero mean and unit variance
yt = 0.2 + 0.4 yt-1 + ut
The (unconditional) mean of y will be given by (a) 0.2 (b) 0.4 (c) 0.5 (d) 0.33
A: We start with the unconditional mean of y, which is the average value of y across all possible values of time. Since the
disturbances have zero mean and unit variance, the mean of y is determined only by the coefﬁcients of the AR(1) model. We
have yt = 0.2 + 0.4 yt-1. So, the mean of y (i.e. the unconditional mean) is given by 0.2 + 0.4*(mean of y). This is a recursive
equation, so we can solve it by iteration. We start with the initial guess of 0.2, then we repeat the equation and set the mean of
y to be 0.2 + 0.4*(0.2) = 0.33. We repeat the equation again and set the mean of y to be 0.2 + 0.4*(0.33) = 0.33. We can see
that the mean of y is converging to 0.33, so the answer is (d).
CMATH420
Q: Compute $\left(\sqrt{625681}\right)ˆ2$.
A: For any nonnegative number $n$, the value of $\sqrt{n}$ is the
number whose square is $n$. So, when we square $\sqrt{n}$, we get
$n$. Therefore, $\left(\sqrt{625681}\right)ˆ2 = \boxed{625681}$.
The answer is 625681.
Q: Find $160\div \left(10+11\cdot 2\right)$.
A: Recall that we must perform the operations inside the parentheses
first. So, we must simplify $10+11\cdot 2$ first. Of these
operations, we must do the multiplication and then the addition
because multiplication and division must be done before addition
and subtraction. We get \begin{align*}10+11\cdot 2 &=10+22 \\
&=32.\end{align*}Now, we substitute back into the original
expression and do the division. Therefore, \begin{align*}160\div \
left(10+11\cdot 2\right)&=160\div 32 \\ &=\boxed{5}.\end{align*}
The answer is 5.

Boosted Prompt Ensembles for Large Language Models
Q: What number, when divided by 2, gives the same result as when 2 is
subtracted from it?
A: If we let $x =$ the number that we want to find, we know that $x/2$
must equal $x-2$. Multiplying both sides of the equation $x/2=x-2$
by $2$, we have that $x=2x-4$, so $x=\boxed{4}$. The answer is 4.
Q: What is the result when we compute $$1ˆ3 + 2ˆ3 + 3ˆ3 + 4ˆ3 + \dots +
99ˆ3 + 100ˆ3 $$and $$(-1)ˆ3 + (-2)ˆ3 + (-3)ˆ3 + (-4)ˆ3 + \dots +
(-99)ˆ3 + (-100)ˆ3,$$and then add the two results?
A: Recall that $(-a)ˆ3=-aˆ3$. Thus, our second sum can be rewritten as
$$ (-1ˆ3) + (-2ˆ3) + (-3ˆ3) + (-4ˆ3) + \dots + (-99ˆ3) + (-100ˆ3).
$$When we add this with $$1ˆ3 + 2ˆ3 + 3ˆ3 + 4ˆ3 + \dots + 99ˆ3 +
100ˆ3, $$we can pair the terms conveniently: \[1ˆ3 + (-1ˆ3) + 2ˆ3 +
(-2ˆ3)+ 3ˆ3 + (-3ˆ3) + \dots + 100ˆ3 + (-100ˆ3). \]Because any
number plus its negation is zero, each of these pairs of terms sum
to zero, and the sum of the entire sequence is $\boxed{0}$. The
answer is 0.
Q: A particular triangle has sides of length 14 cm, 8 cm and 9 cm. In
centimeters, what is the perimeter of the triangle?
A: The perimeter of a polygon is defined to be the sum of the measures
of the sides of the polygon. Therefore, the perimeter of a
triangle whose sides measure 14 cm, 8 cm, and 9 cm is $\boxed{31}$
centimeters. The answer is 31.
Q: A basketball player made the following number of free throws in 8
successive games: 6, 18, 15, 14, 19, 12, 19, and 15. What is the
median number of successful free throws?
A: To find the median, we first arrange the number of successful free
throws in increasing numerical order: $$6,12,14,15,15,18,19,19.$$
Since there is an even number of terms, the median can be found the
by averaging the middle two (the fourth and the fifth) terms. Both
the fourth and the fifth terms are $15$, so the median number of
successful free throws that the basketball player made is $\boxed
{15}$. The answer is 15.
Q: Find $1ˆ{234} + 4ˆ6 \div 4ˆ4$.
A: Since 1 raised to any power is 1, the expression equals $1 + 4ˆ6 \
div 4ˆ4$. Also recall that $\frac{aˆm}{aˆn}=aˆ{m-n}$, so we can
simplify the last term to get $1+4ˆ{6-4}=1+4ˆ2=1+16=\boxed{17}$.
The answer is 17.
Q: Marty wants to paint a box. He can choose to use either blue, green,
yellow, or black paint. Also, he can style the paint by painting
with a brush, a roller, or a sponge. How many different
combinations of color and painting method can Marty choose?
A: Marty can choose his paint in 4 ways and his style in 3 ways. Thus
there are a total of $4\cdot 3 = \boxed{12}$ different combinations
he can choose. The answer is 12.
SVAMP
SAME AS GSM8K
Complexity Prompting
The following prompts are used for our implementation of complexity-based prompting:
GSM8K
Q: You can buy 4 apples or 1 watermelon for the same price. You bought
36 fruits evenly split between oranges, apples and watermelons,
and the price of 1 orange is $0.50. How much does 1 apple cost if
your total bill was $66?
A: If 36 fruits were evenly split between 3 types of fruits, then I
bought 36/3 = <<36/3=12>>12 units of each fruit

Boosted Prompt Ensembles for Large Language Models
If 1 orange costs $0.50 then 12 oranges will cost $0.50 * 12 = $
<<0.5*12=6>>6
If my total bill was $66 and I spent $6 on oranges then I spent $66 -
$6 = $<<66-6=60>>60 on the other 2 fruit types.
Assuming the price of watermelon is W, and knowing that you can buy 4
apples for the same price and that the price of one apple is A,
then 1W=4A1
If we know we bought 12 watermelons and 12 apples for $60, then we
know that $60 = 12W + 12A
Knowing that 1W=4A, then we can convert the above to $60 = 12(4A) + 12
A
$60 = 48A + 12A
$60 = <<60=60>>60A
Then we know the price of one apple (A) is $60/60= $<<60/60=1>>1. The
answer is 1.
Q: Bella has two times as many marbles as frisbees. She also has 20
more frisbees than deck cards. If she buys 2/5 times more of each
item, what would be the total number of the items she will have if
she currently has 60 marbles?
A: When Bella buys 2/5 times more marbles, she’ll have increased the
number of marbles by 2/5*60 = <<2/5*60=24>>24
The total number of marbles she’ll have is 60+24 = <<60+24=84>>84
If Bella currently has 60 marbles, and she has two times as many
marbles as frisbees, she has 60/2 = <<60/2=30>>30 frisbees.
If Bella buys 2/5 times more frisbees, she’ll have 2/5*30 =
<<2/5*30=12>>12 more frisbees.
The total number of frisbees she’ll have will increase to 30+12 =
<<30+12=42>>42
Bella also has 20 more frisbees than deck cards, meaning she has 30-20
= <<20-10=10>>10 deck cards
If she buys 2/5 times more deck cards, she’ll have 2/5*10 =
<<2/5*10=4>>4 more deck cards.
The total number of deck cards she’ll have is 10+4 = <<10+4=14>>14
Together, Bella will have a total of 14+42+84 = <<14+42+84=140>>140
items. The answer is 140.
Q: Susy goes to a large school with 800 students, while Sarah goes to
a smaller school with only 300 students. At the start of the school
year, Susy had 100 social media followers. She gained 40 new
followers in the first week of the school year, half that in the
second week, and half of that in the third week. Sarah only had 50
social media followers at the start of the year, but she gained 90
new followers the first week, a third of that in the second week,
and a third of that in the third week. After three weeks, how many
social media followers did the girl with the most total followers
have?
A: After one week, Susy has 100+40 = <<100+40=140>>140 followers.
In the second week, Susy gains 40/2 = <<40/2=20>>20 new followers.
In the third week, Susy gains 20/2 = <<20/2=10>>10 new followers.
In total, Susy finishes the three weeks with 140+20+10 =
<<140+20+10=170>>170 total followers.
After one week, Sarah has 50+90 = <<50+90=140>>140 followers.
After the second week, Sarah gains 90/3 = <<90/3=30>>30 followers.
After the third week, Sarah gains 30/3 = <<30/3=10>>10 followers.
So, Sarah finishes the three weeks with 140+30+10 =
<<140+30+10=180>>180 total followers.
Thus, Sarah is the girl with the most total followers with a total of
180. The answer is 180.
Q: Mark’s basketball team scores 25 2 pointers, 8 3 pointers and 10
free throws. Their opponents score double the 2 pointers but half
the 3 pointers and free throws. What’s the total number of points
scored by both teams added together?
A: Mark’s team scores 25 2 pointers, meaning they scored 25*2=

Boosted Prompt Ensembles for Large Language Models
<<25*2=50>>50 points in 2 pointers.
His team also scores 6 3 pointers, meaning they scored 8*3= 24 points
in 3 pointers
They scored 10 free throws, and free throws count as one point so they
scored 10*1=<<10*1=10>>10 points in free throws.
All together his team scored 50+24+10= <<50+24+10=84>>84 points
Mark’s opponents scored double his team’s number of 2 pointers,
meaning they scored 50*2=<<50*2=100>>100 points in 2 pointers.
His opponents scored half his team’s number of 3 pointers, meaning
they scored 24/2= <<24/2=12>>12 points in 3 pointers.
They also scored half Mark’s team’s points in free throws, meaning
they scored 10/2=<<10/2=5>>5 points in free throws.
All together Mark’s opponents scored 100+12+5=<<100+12+5=117>>117
points
The total score for the game is both team’s scores added together, so
it is 84+117=<<84+117=201>>201 points. The answer is 201.
Q: The total average age of three friends is 40. Jared is ten years
older than Hakimi, and Molly’s age is 30. How old is Hakimi?
A: The total age for the three friends is 40*3 = <<40*3=120>>120
If Molly’s age is 30, then Jared and Hakimi have a total age of 120-30
= 90.
Let’s say the age of Hakimi is x.
Since Jared is 10 years older than Hakimi, Jared is x+10 years old.
Jared and Hakimi’s total age is x+(x+10) = 90
This translates to 2x=90-10
2x=80
Hakimi’s age is x=80/2
This gives us x=<<40=40>>40, which is Hamkimi’s age. The answer is 40.
Q: Together 3 friends watched 411 short videos. Kelsey watched 43 more
than Ekon. Ekon watched 17 less than Uma. How many videos did
Kelsey watch?
A: Let U = the number of videos Uma watched
Ekon = U - 17
Kelsey = (U - 17) + 43 = U + <<(-17)+43=26>>26
U + U - 17 + U + 26 = 411
3U + 9 = 411
3U = 402
U = <<134=134>>134
Kelsey = 134 + 26 = <<134+26=160>>160 videos
Kelsey watched 160 videos. The answer is 160.
Q: Sam bought a dozen boxes, each with 30 highlighter pens inside, for
$10 each box. He rearranged five of these boxes into packages of
six highlighters each and sold them for $3 per package. He sold the
rest of the highlighters separately at the rate of three pens for
$2. How much profit did he make in total, in dollars?
A: Sam bought 12 boxes x $10 = $<<12*10=120>>120 worth of highlighters.
He bought 12 * 30 = <<12*30=360>>360 highlighters in total.
Sam then took 5 boxes x 6 highlighters/box = <<5*6=30>>30 highlighters.
He sold these boxes for 5 * $3 = $<<5*3=15>>15
After selling these 5 boxes there were 360 - 30 = <<360-30=330>>330
highlighters remaining.
These form 330 / 3 = <<330/3=110>>110 groups of three pens.
He sold each of these groups for $2 each, so made 110 * 2 = $
<<110*2=220>>220 from them.
In total, then, he earned $220 + $15 = $<<220+15=235>>235.
Since his original cost was $120, he earned $235 - $120 = $
<<235-120=115>>115 in profit. The answer is 115.
Q: Megan pays $16 for a shirt that costs $22 before sales. What is the
amount of the discount?

Boosted Prompt Ensembles for Large Language Models
A: Let x be the amount of the discount.
We have, 22 - x = $16
We change the writing of the equation:
22 - x + x = 16 + x
So, 22 = 16 + x
We then Remove 16 from both sides:
22 - 16 = 16 + x - 16
So, 22 - 16 = x
So, the amount of the discount is x = $<<6=6>>6. The answer is 6.
AQUA
Q: The sum of the terms of an infinite G.P is 7 and the sum of the
cubes of the terms is 1,225. Find the first term of the series.
Answer Choices: (a) 35/3 (b) 35/2 (c) 15/2 (d) 9/4 (e) 7/4
A: S a = a/ ( 1 - r ) = 7 ..... (1)
Sum to infinity of the cubes = a 3 / 1 -r 3 = 1,225
From (1) a 3 / ( 1 - r ) 3 = 7 3 = 343
Therefore, ( 1 - r ) 3 / 1 - r 3 = 1225/343,
(1+ r 2 -2r)/ ( 1 + r 2 + r ) = 25/7
7 + 7 r 2 -14r = 25 + 25 r + 25 r 2
18 r 2 +39r + 18 = 0, on solving r = -3/2 or -2/3
for an infinite G.P | r | < 1, r = - 2/3
Therefore, a / [ 1 - ( - 2/3 ) ] = 7, a = 7 * 5/3 = 35/3 ANSWER: a.
The answer is (a).
Q: A factory has three types of machines, each of which works at its
own constant rate. If 7 Machine As and 11 Machine Bs can produce
261 widgets per hour, and if 8 Machine As and 22 Machine Cs can
produce 600 widgets per hour, how many widgets could one machine A,
one Machine B, and one Machine C produce in one 8-hour day? Answer
Choices: (a) 408 (b) 475 (c) 550 (d) 625 (e) 700
A: Let Machine A produce A widgets per hour. B produce B widgets per
hour and C produce C widgets per hour.
7A+11B=261 ---(1)
8A+22C=600 ---(2)
Dividing (2) by 2
4A+11C=300.....(3)
Adding (1)(3)
11A+11B+11C = 561
A+B+C=51 per hour
So for eight hrs = 51*8 = 408 = Answer = a. The answer is (a).
Q: Three boys are ages 4, 6 and 7 respectively. Three girls are ages 5,
8 and 9, respectively. If two of the boys and two of the girls are
randomly selected and the sum of the selected children’s ages is e
, what is the difference between the probability that e is even and
the probability that e is odd? Answer Choices: (a) 1/9 (b) 1/6 (c)
2/9 (d) 1/4 (e) 1/2
A: Age of Boys e:4, 6, 7
Sum of ages taken 2 at a time: 10,13,11
Ages of Girls:5, 8, 9
Sum of ages taken 2 at a time: 13,17,14
9 Combinations of sum between sets(10,12,11)(13,17,14)
=23,27,24- 16,30,17- 24,28,25
Prob(Even)= 5/9
Prob(Odd) =4/9
Answer=5/9 - 4/9 = 1/9. The answer is (a).
Q: The weights of one liter vegetable ghee packet of two brands ’A’
and ’B’ are 800 gm and 850 gm respectively. If they are mixed in
the ratio of 3 : 2 by volumes to form a mixture of 3 liters, what
is the weight (in kg) of the mixture? Answer Choices: (a) 3.84 (b)
1.75 (c) 3.52 (d) 2.46 (e) None of these

Boosted Prompt Ensembles for Large Language Models
A: Here’s how I did it. My notes from reading the problem were:
1L A = 800 gm
1L B = 850 gm
We are mixing five parts (3 parts A plus 2 parts B, 5 parts total) to
get 3L, so 5x = 3 ---> x = 3/5. Each part is 3/5 of a liter.
So if we have 3 parts A, we have 800*3*(3/5) = 1440
If we have 2 parts B, we have 850*2*(3/5) = 1020
1440 + 1020 = 2460
Solving for units gives us 2.46
So the answer is d. The answer is (d).
Q: What is the total number of X integers between 100 and 200 that are
divisible by 3? Answer Choices: (a) 33 (b) 32 (c) 31 (d) 30 (e) 29
A: yes there is a different way of arriving at that answer ....
u can also use airthmetic progression to get the answer
since the first term to be divisble by 3 is 102 ..take that as A ..
the starting no
and since 198 is the last digit to be divisible by 3 take that as N ...
since the difference is 3 take that as D
no u have to find what term is 198 take that as nth term
the formula for that is N = A + (n-1) * d
198 = 102 +(n-1) * 3
from this u get n =33. The answer is (a).
Q: An automobile parts supplier charges $25 per package of gaskets.
When a customer orders more than 10 packages of gaskets, the
supplier charges 4/5 the price for each package in excess of 10.
During a certain week, the supplier sold 60 packages of gaskets. If
30 percent of the gaskets went to Company X, 15 percent to Company
Y, and the rest to Company Z, what was the total amount, in
dollars, that the parts supplier received in payment for the
gaskets? Answer Choices: (a) 1305 (b) 1375 (c) 1345 (d) 1415 (e)
1455
A: $25 per packet of gasket in case a customer orders less than 10
in case a customer orders > 10 price per gasket = 25*4/5=20
a certain week the supplier sold 60 gasket
1.he sold 30 % of the gaskets to X = 18 gaskets = 25*10 + 20 *8 = 250 +
160 = 410
2.he sold 15 % of the gaskets to Y = 9 gaskets = 25*9= 225
3.he sold remaining 55% to z =33 gaskets = 25*10 =250 + 20*23=710
thus ,total money earned
410+225+710 = 1345
The answer is (c).
Q: A company spends an average of $500 for its 20 employees’s salaries
and an average of $100 for its 5 drivers’ salaries. What is the
average expenditure of the company? Answer Choices: (a) $360 (b)
$290 (c) $420 (d) $600 (e) $240
A: Number of employees is 20
Their average salary is $500
Number of drivers is 5
Their average salary is $100
Total salary of the employees is 20*500=$10,000
Total salary of the drivers is 5*100=$500
Total expenditure for company is 10,000+500=$10,500
Average expenditure is 10,500/20+5=$420
The answer is (c).
Q: The minute hand of a clock overtakes the hour hand at intervals of
65 minutes of the correct time. How much a day does the clock gain
or lose? Answer Choices: (a) (10 + 10/143 )min (b) (10 + 10/843 )
min (c) (10 + 19/143 )min (d) (90 + 10/143 )min (e) (10 + 10/193 )
min
A: In a correct clock, the minute hand gains 55 min. spaces over the

Boosted Prompt Ensembles for Large Language Models
hour hand in 60 minutes.
To be together again, the minute hand must gain 60 minutes over the
hour hand.
55 minutes are gained in 60 min.
60 min. are gained in [(60/55) * 60] min = min.
But they are together after 65 min.
Therefore, gain in 65 minutes = = min.
Gain in 24 hours = = 1440/143 min.
Therefore, the clock gains (10 + 10/143 )minutes in 24 hours.
Answer: (a) (10 + 10/143 )min. The answer is (a).
Auto CoT Prompting
The following Auto CoT prompts are used for the zero shot setting:
GSM8K
Q: Jack decides to visit a museum 150 miles from home. He drives 75 mph there and back. He spends 6 hours at the museum.
How long is he gone from home?
A: Let’s think step by step. Jack drives 75 mph there. This will take him 2 hours. He spends 6 hours at the museum. He drives
75 mph back. This will take him 2 hours. So Jack is gone for a total of 6 hours. The answer is 6.
Q: Robert had 3 pounds of candy, Cindy had 5 pounds of candy, and Aaron had 4 pounds of candy after Halloween. If they
decide to pool and share their candy equally with each other, how much candy would each of them have?
A: Let’s think step by step. Robert had 3 pounds of candy, Cindy had 5 pounds of candy, and Aaron had 4 pounds of candy
after Halloween. That means that there was a total of 3 + 5 + 4 = 12 pounds of candy. If they decide to pool and share their
candy equally with each other, that means that each person would get 12/3 = 4 pounds of candy. The answer is 4.
Q: Susan made 100 cookies for Christmas and was going to equally divide them between her 6 nephews. Before Susan could
package them, her husband snuck 4 cookies for himself. How many cookies will each of Susan’s nephews get?
A: Let’s think step by step. Susan made 100 cookies. Then her husband snuck 4 cookies. That means that now she only has 96
cookies. Since she wants to divide them equally between her 6 nephews, each nephew will get 16 cookies. The answer is 16.
Q: Tom buys a bedroom set for $3000. He sells his old bedroom for $1000 and uses that to pay for part of the bedroom set. He
then has to pay 10% a month for the bedroom set. How much does he have to pay per month?
A: Let’s think step by step. Tom buys a bedroom set for $3000. He sells his old bedroom for $1000 and uses that to pay for
part of the bedroom set. Tom has to pay $2000 for the new bedroom set. He then has to pay 10% a month for the bedroom set.
This means that every month, Tom has to pay $200 for his new bedroom set. The answer is 200.
Q: If a classroom has 3 times as many girls as they do boys, and 1/10 as many nongendered children as they do boys, and the
classroom has 30 boys. How many total children does it have?
A: Let’s think step by step. There are 3 times as many girls as boys, so if there are 30 boys, there are 3*30=90 girls. There are
1/10 as many nongendered children as there are boys, so if there are 30 boys, there are 30/10=3 nongendered children. Now we
can add these up: 90+30+3=123 total children. The answer is 123.
Q: Anna is trying to buy books for her local library. She had a budget of $16, and she has already spent $4. She also bought 2
books today, and both books cost her the same amount of money. If Anna has $2 left in her budget, how much did each of the
books cost?
A: Let’s think step by step. Anna has a budget of $16. She spends $4, so she has $12 left. She buys 2 books, so she has $2 left.
That means that each book cost her $5. The answer is 5.
Q: Jim spends 2 hours watching TV and then decides to go to bed and reads for half as long. He does this 3 times a week. How
many hours does he spend on TV and reading in 4 weeks?
A: Let’s think step by step. In one week, Jim spends 6 hours watching TV and 3 hours reading. In 4 weeks, he spends 24 hours
watching TV and 12 hours reading. The answer is 24.
Q: If Ann is 9 years old and her brother is twice her age, how old will her brother be in 3 years?
A: Let’s think step by step. Ann is 9 years old. Her brother is twice her age. That means her brother is 9 * 2 = 18 years old. In
3 years, her brother will be 18 + 3 = 21 years old. The answer is 21.
AQUA
Q: Julie’s yard is rectangular. One side of the yard is 100 feet wide. The total area of the yard is 3,000 square feet. What is the
length of the other side of the yard? Answer Choices: (a) 30 feet (b) 20 feet (c) 10 feet (d) 50 feet (e) 60 feet
A: Let’s think step by step. We know that the yard is rectangular, so it has four sides. We also know that one side of the yard is
100 feet wide. The total area of the yard is 3,000 square feet. We can set up the equation: 3,000 = 100x x = 30 So, the length of
the other side of the yard is 30 feet. The answer is (a).
Q: In a survey of students, each student selected from a list of 10 songs the 2 songs that the student liked best. If each song was
selected 5 times, how many students were surveyed? Answer Choices: (a) 96 (b) 48 (c) 32 (d) 25 (e) 18
A: Let’s think step by step. There are 10 songs, and each song is selected 5 times. That means that there are 50 selections in
total. Each student makes 2 selections, so there must be 25 students. The answer is (d).

Boosted Prompt Ensembles for Large Language Models
Q: Mark told John ”If you give me half your money I will have Rs.75”. John said, ”if you give me one third of your money, I
will have Rs.75”. How much money did John have ? Answer Choices: (a) 22 (b) 60 (c) 28 (d) 26 (e) 18
A: Let’s think step by step. If Mark has Rs.75, then John has Rs.75 as well. If Mark gives John half his money, then Mark will
have Rs.37.50 and John will have Rs.112.50. However, this isn’t one of the answer choices. If Mark gives John one third of
his money, then Mark will have Rs.50 and John will have Rs.100. This is one of the answer choices, so John must have had
Rs.100 to begin with. The answer is (d).
Q: A car ﬁnishes a journey in 20 hours at the speed of 60 km/hr. If the same distance is to be covered in 10 hours, how much
speed does the car gain? Answer Choices: (a) 80 kmph (b) 50 kmph (c) 120 kmph (d) 70 kmph (e) 80 kmph
A: Let’s think step by step. The car is currently travelling at 60 km/hr. In order to travel the same distance in 10 hours, the car
would need to travel at 6 times the speed, or 360 km/hr. Therefore, the car would need to gain 300 km/hr. The answer is (c).
MMLU570
SAME AS AQUA
CMATH420
SAME AS GSM8K
SVAMP
SAME AS GSM8K
Nonsense Prompting
The following two sample nonsense prompt, created by asking ChatGPT for nonsense questions
and answers (one multiple choice, one open-ended with integer solution), is used for ALL datasets:
Nonsense Prompt
Q: What is the capital of the country of Flibbertigibbet? Answer Choices: (a) Gigglemug (b) Snickerdoodle (c) Wibblewobble
(d) Flibbertigibbet City
A: We will pretend that Flibbertigibbet City is the capital of Flibbertigibbet. The answer is (d).
Q: How many lollipops are in a pile of ﬂuffernutter?
A: Let’s consider a pile of ﬂuffernutter is holding lollipops. It seems reasonable that a pile of ﬂuffernutter could hold 17
lollipops. The answer is 17.

