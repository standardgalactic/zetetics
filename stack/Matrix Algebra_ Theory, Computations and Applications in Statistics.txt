Springer Texts in Statistics
Matrix 
Algebra
James E. Gentle
Theory, Computations 
and Applications in Statistics
Second Edition

Springer Texts in Statistics
Series Editors
Richard DeVeaux
Stephen E. Fienberg
Ingram Olkin

More information about this series at http://www.springer.com/series/417

James E. Gentle
Matrix Algebra
Theory, Computations and Applications
in Statistics
Second Edition
123

James E. Gentle
Fairfax, VA, USA
ISSN 1431-875X
ISSN 2197-4136
(electronic)
Springer Texts in Statistics
ISBN 978-3-319-64866-8
ISBN 978-3-319-64867-5
(eBook)
DOI 10.1007/978-3-319-64867-5
Library of Congress Control Number: 2017952371
1st edition: © Springer Science+Business Media, LLC 2007
2nd edition: © Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole
or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical
way, and transmission or information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in
this publication does not imply, even in the absence of a speciﬁc statement, that such names
are exempt from the relevant protective laws and regulations and therefore free for general
use.
The publisher, the authors and the editors are safe to assume that the advice and informa-
tion in this book are believed to be true and accurate at the date of publication. Neither the
publisher nor the authors or the editors give a warranty, express or implied, with respect
to the material contained herein or for any errors or omissions that may have been made.
The publisher remains neutral with regard to jurisdictional claims in published maps and
institutional aﬃliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

To Mar´ıa

Preface to the Second Edition
In this second edition, I have corrected all known typos and other errors; I
have (it is hoped) clariﬁed certain passages; I have added some additional
material; and I have enhanced the Index.
I have added a few more comments about vectors and matrices with com-
plex elements, although, as before, unless stated otherwise, all vectors and
matrices in this book are assumed to have real elements. I have begun to
use “det(A)” rather than “|A|” to represent the determinant of A, except in
a few cases. I have also expressed some derivatives as the transposes of the
expressions I used formerly.
I have put more conscious emphasis on “user-friendliness” in this edition.
In a book, user-friendliness is primarily a function of references, both internal
and external, and of the index. As an old software designer, I’ve always thought
that user-friendliness is very important. To the extent that internal references
were present in the ﬁrst edition, the positive feedback I received from users of
that edition about the friendliness of those internal references (“I liked the fact
that you said ‘equation (x.xx) on page yy,’ instead of just ‘equation (x.xx)’ ”)
encouraged me to try to make the internal references even more useful. It’s
only when you’re “eating your own dog food,” that you become aware of where
details matter, and in using the ﬁrst edition, I realized that the choice of entries
in the Index was suboptimal. I have spent signiﬁcant time in organizing it,
and I hope that the user will ﬁnd the Index to this edition to be very useful.
I think that it has been vastly improved over the Index in the ﬁrst edition.
The overall organization of chapters has been preserved, but some sec-
tions have been changed. The two chapters that have been changed most are
Chaps. 3 and 12. Chapter 3, on the basics of matrices, got about 30 pages
longer. It is by far the longest chapter in the book, but I just didn’t see any
reasonable way to break it up. In Chap. 12 of the ﬁrst edition, “Software for
Numerical Linear Algebra,” I discussed four software systems or languages,
C/C++, Fortran, Matlab, and R, and did not express any preference for one
vii

viii
Preface to the Second Edition
over another. In this edition, although I occasionally mention various lan-
guages and systems, I now limit most of my discussion to Fortran and R.
There are many reasons for my preference for these two systems. R is ori-
ented toward statistical applications. It is open source and freely distributed.
As for Fortran versus C/C++, Python, or other programming languages, I
agree with the statement by Hanson and Hopkins (2013, page ix), “... For-
tran is currently the best computer language for numerical software.” Many
people, however, still think of Fortran as the language their elders (or they
themselves) used in the 1970s. (On a personal note, Richard Hanson, who
passed away recently, was a member of my team that designed the IMSL C
Libraries in the mid 1980s. Not only was C much cooler than Fortran at the
time, but the ANSI committee working on updating the Fortran language was
so fractured by competing interests that approval of the revision was repeat-
edly delayed. Many numerical analysts who were not concerned with coolness
turned to C because it provided dynamic storage allocation and it allowed
ﬂexible argument lists, and the Fortran constructs could not be agreed upon.)
Language preferences are personal, of course, and there is a strong “cool-
ness factor” in choice of a language. Python is currently one of the coolest
languages, but I personally don’t like the language for most of the stuﬀI do.
Although this book has separate parts on applications in statistics and
computational issues as before, statistical applications have informed the
choices I made throughout the book, and computational considerations have
given direction to most discussions.
I thank the readers of the ﬁrst edition who informed me of errors. Two
people in particular made several meaningful comments and suggestions. Clark
Fitzgerald not only identiﬁed several typos, he made several broad suggestions
about organization and coverage that resulted in an improved text (I think).
Andreas Eckner found, in addition to typos, some gaps in my logic and also
suggested better lines of reasoning at some places. (Although I don’t follow
an itemized “theorem-proof” format, I try to give reasons for any nonobvious
statements I make.) I thank Clark and Andreas especially for their comments.
Any remaining typos, omissions, gaps in logic, and so on are entirely my
responsibility.
Again, I thank my wife, Mar´ıa, to whom this book is dedicated, for everything.
I used TEX via LATEX2ε to write the book. I did all of the typing, program-
ming, etc., myself, so all misteaks (mistakes!) are mine. I would appreciate
receiving suggestions for improvement and notiﬁcation of errors. Notes on
this book, including errata, are available at
http://mason.gmu.edu/~jgentle/books/matbk/
Fairfax County, VA, USA
James E. Gentle
July 14, 2017

Preface to the First Edition
I began this book as an update of Numerical Linear Algebra for Applications
in Statistics, published by Springer in 1998. There was a modest amount of
new material to add, but I also wanted to supply more of the reasoning behind
the facts about vectors and matrices. I had used material from that text in
some courses, and I had spent a considerable amount of class time proving
assertions made but not proved in that book. As I embarked on this project,
the character of the book began to change markedly. In the previous book,
I apologized for spending 30 pages on the theory and basic facts of linear
algebra before getting on to the main interest: numerical linear algebra. In
this book, discussion of those basic facts takes up over half of the book.
The orientation and perspective of this book remains numerical linear al-
gebra for applications in statistics. Computational considerations inform the
narrative. There is an emphasis on the areas of matrix analysis that are im-
portant for statisticians, and the kinds of matrices encountered in statistical
applications receive special attention.
This book is divided into three parts plus a set of appendices. The three
parts correspond generally to the three areas of the book’s subtitle—theory,
computations, and applications—although the parts are in a diﬀerent order,
and there is no ﬁrm separation of the topics.
Part I, consisting of Chaps. 1 through 7, covers most of the material in
linear algebra needed by statisticians. (The word “matrix” in the title of this
book may suggest a somewhat more limited domain than “linear algebra”;
but I use the former term only because it seems to be more commonly used
by statisticians and is used more or less synonymously with the latter term.)
The ﬁrst four chapters cover the basics of vectors and matrices, concen-
trating on topics that are particularly relevant for statistical applications. In
Chap. 4, it is assumed that the reader is generally familiar with the basics of
partial diﬀerentiation of scalar functions. Chapters 5 through 7 begin to take
on more of an applications ﬂavor, as well as beginning to give more consid-
eration to computational methods. Although the details of the computations
ix

x
Preface to the First Edition
are not covered in those chapters, the topics addressed are oriented more to-
ward computational algorithms. Chapter 5 covers methods for decomposing
matrices into useful factors.
Chapter 6 addresses applications of matrices in setting up and solving
linear systems, including overdetermined systems. We should not confuse sta-
tistical inference with ﬁtting equations to data, although the latter task is a
component of the former activity. In Chap. 6, we address the more mechanical
aspects of the problem of ﬁtting equations to data. Applications in statistical
data analysis are discussed in Chap. 9. In those applications, we need to make
statements (i.e., assumptions) about relevant probability distributions.
Chapter 7 discusses methods for extracting eigenvalues and eigenvectors.
There are many important details of algorithms for eigenanalysis, but they are
beyond the scope of this book. As with other chapters in Part I, Chap. 7 makes
some reference to statistical applications, but it focuses on the mathematical
and mechanical aspects of the problem.
Although the ﬁrst part is on “theory,” the presentation is informal; neither
deﬁnitions nor facts are highlighted by such words as “deﬁnition,” “theorem,”
“lemma,” and so forth. It is assumed that the reader follows the natural
development. Most of the facts have simple proofs, and most proofs are given
naturally in the text. No “Proof” and “Q.E.D.” or “ ” appear to indicate
beginning and end; again, it is assumed that the reader is engaged in the
development. For example, on page 341:
If A is nonsingular and symmetric, then A−1 is also symmetric because
(A−1)T = (AT)−1 = A−1.
The ﬁrst part of that sentence could have been stated as a theorem and
given a number, and the last part of the sentence could have been introduced
as the proof, with reference to some previous theorem that the inverse and
transposition operations can be interchanged. (This had already been shown
before page 341—in an unnumbered theorem of course!)
None of the proofs are original (at least, I don’t think they are), but in most
cases, I do not know the original source or even the source where I ﬁrst saw
them. I would guess that many go back to C. F. Gauss. Most, whether they
are as old as Gauss or not, have appeared somewhere in the work of C. R. Rao.
Some lengthier proofs are only given in outline, but references are given for
the details. Very useful sources of details of the proofs are Harville (1997),
especially for facts relating to applications in linear models, and Horn and
Johnson (1991), for more general topics, especially those relating to stochastic
matrices. The older books by Gantmacher (1959) provide extensive coverage
and often rather novel proofs. These two volumes have been brought back into
print by the American Mathematical Society.
I also sometimes make simple assumptions without stating them explicitly.
For example, I may write “for all i” when i is used as an index to a vector.
I hope it is clear that “for all i” means only “for i that correspond to indices

Preface to the First Edition
xi
of the vector.” Also, my use of an expression generally implies existence. For
example, if “AB” is used to represent a matrix product, it implies that “A
and B are conformable for the multiplication AB.” Occasionally, I remind the
reader that I am taking such shortcuts.
The material in Part I, as in the entire book, was built up recursively. In
the ﬁrst pass, I began with some deﬁnitions and followed those with some
facts that are useful in applications. In the second pass, I went back and
added deﬁnitions and additional facts that led to the results stated in the ﬁrst
pass. The supporting material was added as close to the point where it was
needed as practical and as necessary to form a logical ﬂow. Facts motivated by
additional applications were also included in the second pass. In subsequent
passes, I continued to add supporting material as necessary and to address
the linear algebra for additional areas of application. I sought a bare-bones
presentation that gets across what I considered to be the theory necessary for
most applications in the data sciences. The material chosen for inclusion is
motivated by applications.
Throughout the book, some attention is given to numerical methods for
computing the various quantities discussed. This is in keeping with my be-
lief that statistical computing should be dispersed throughout the statistics
curriculum and statistical literature generally. Thus, unlike in other books
on matrix “theory,” I describe the “modiﬁed” Gram-Schmidt method, rather
than just the “classical” GS. (I put “modiﬁed” and “classical” in quotes be-
cause, to me, GS is MGS. History is interesting, but in computational matters,
I do not care to dwell on the methods of the past.) Also, condition numbers
of matrices are introduced in the “theory” part of the book, rather than just
in the “computational” part. Condition numbers also relate to fundamental
properties of the model and the data.
The diﬀerence between an expression and a computing method is em-
phasized. For example, often we may write the solution to the linear system
Ax = b as A−1b. Although this is the solution (so long as A is square and of
full rank), solving the linear system does not involve computing A−1. We may
write A−1b, but we know we can compute the solution without inverting the
matrix.
“This is an instance of a principle that we will encounter repeatedly:
the form of a mathematical expression and the way the expression
should be evaluated in actual practice may be quite diﬀerent.”
(The statement in quotes appears word for word in several places in the book.)
Standard textbooks on “matrices for statistical applications” emphasize
their uses in the analysis of traditional linear models. This is a large and im-
portant ﬁeld in which real matrices are of interest, and the important kinds of
real matrices include symmetric, positive deﬁnite, projection, and generalized
inverse matrices. This area of application also motivates much of the discussion
in this book. In other areas of statistics, however, there are diﬀerent matrices

xii
Preface to the First Edition
of interest, including similarity and dissimilarity matrices, stochastic matri-
ces, rotation matrices, and matrices arising from graph-theoretic approaches
to data analysis. These matrices have applications in clustering, data mining,
stochastic processes, and graphics; therefore, I describe these matrices and
their special properties. I also discuss the geometry of matrix algebra. This
provides a better intuition of the operations. Homogeneous coordinates and
special operations in IR3 are covered because of their geometrical applications
in statistical graphics.
Part II addresses selected applications in data analysis. Applications are
referred to frequently in Part I, and of course, the choice of topics for coverage
was motivated by applications. The diﬀerence in Part II is in its orientation.
Only “selected” applications in data analysis are addressed; there are ap-
plications of matrix algebra in almost all areas of statistics, including the
theory of estimation, which is touched upon in Chap. 4 of Part I. Certain
types of matrices are more common in statistics, and Chap. 8 discusses in
more detail some of the important types of matrices that arise in data anal-
ysis and statistical modeling. Chapter 9 addresses selected applications in
data analysis. The material of Chap. 9 has no obvious deﬁnition that could
be covered in a single chapter (or a single part or even a single book), so I
have chosen to discuss brieﬂy a wide range of areas. Most of the sections and
even subsections of Chap. 9 are on topics to which entire books are devoted;
however, I do not believe that any single book addresses all of them.
Part III covers some of the important details of numerical computations,
with an emphasis on those for linear algebra. I believe these topics constitute
the most important material for an introductory course in numerical analysis
for statisticians and should be covered in every such course.
Except for speciﬁc computational techniques for optimization, random
number generation, and perhaps symbolic computation, Part III provides the
basic material for a course in statistical computing. All statisticians should
have a passing familiarity with the principles.
Chapter 10 provides some basic information on how data are stored and
manipulated in a computer. Some of this material is rather tedious, but it
is important to have a general understanding of computer arithmetic before
considering computations for linear algebra. Some readers may skip or just
skim Chap. 10, but the reader should be aware that the way the computer
stores numbers and performs computations has far-reaching consequences.
Computer arithmetic diﬀers from ordinary arithmetic in many ways; for ex-
ample, computer arithmetic lacks associativity of addition and multiplication,
and series often converge even when they are not supposed to. (On the com-
puter, a straightforward evaluation of ∞
x=1 x converges!)
I emphasize the diﬀerences between the abstract number system IR, called
the reals, and the computer number system IF, the ﬂoating-point numbers
unfortunately also often called “real.” Table 10.4 on page 492 summarizes
some of these diﬀerences. All statisticians should be aware of the eﬀects of
these diﬀerences. I also discuss the diﬀerences between ZZ, the abstract number

Preface to the First Edition
xiii
system called the integers, and the computer number system II, the ﬁxed-point
numbers. (Appendix A provides deﬁnitions for this and other notation that I
use.)
Chapter 10 also covers some of the fundamentals of algorithms, such as
iterations, recursion, and convergence. It also discusses software development.
Software issues are revisited in Chap. 12.
While Chap. 10 deals with general issues in numerical analysis, Chap. 11
addresses speciﬁc issues in numerical methods for computations in linear al-
gebra.
Chapter 12 provides a brief introduction to software available for com-
putations with linear systems. Some speciﬁc systems mentioned include the
IMSLTM libraries for Fortran and C, Octave or MATLAB
R
⃝(or Matlab
R
⃝),
and R or S-PLUS
R
⃝(or S-Plus
R
⃝). All of these systems are easy to use, and
the best way to learn them is to begin using them for simple problems. I do
not use any particular software system in the book, but in some exercises, and
particularly in Part III, I do assume the ability to program in either Fortran
or C and the availability of either R or S-Plus, Octave or Matlab, and Maple
R
⃝
or Mathematica
R
⃝. My own preferences for software systems are Fortran and
R, and occasionally, these preferences manifest themselves in the text.
Appendix A collects the notation used in this book. It is generally “stan-
dard” notation, but one thing the reader must become accustomed to is the
lack of notational distinction between a vector and a scalar. All vectors are
“column” vectors, although I usually write them as horizontal lists of their
elements. (Whether vectors are “row” vectors or “column” vectors is generally
only relevant for how we write expressions involving vector/matrix multipli-
cation or partitions of matrices.)
I write algorithms in various ways, sometimes in a form that looks similar
to Fortran or C and sometimes as a list of numbered steps. I believe all of the
descriptions used are straightforward and unambiguous.
This book could serve as a basic reference either for courses in statistical
computing or for courses in linear models or multivariate analysis. When the
book is used as a reference, rather than looking for “deﬁnition” or “theo-
rem,” the user should look for items set oﬀwith bullets or look for numbered
equations or else should use the Index or Appendix A, beginning on page 589.
The prerequisites for this text are minimal. Obviously, some background in
mathematics is necessary. Some background in statistics or data analysis and
some level of scientiﬁc computer literacy are also required. References to rather
advanced mathematical topics are made in a number of places in the text. To
some extent, this is because many sections evolved from class notes that I
developed for various courses that I have taught. All of these courses were at
the graduate level in the computational and statistical sciences, but they have
had wide ranges in mathematical level. I have carefully reread the sections
that refer to groups, ﬁelds, measure theory, and so on and am convinced that
if the reader does not know much about these topics, the material is still
understandable but if the reader is familiar with these topics, the references

xiv
Preface to the First Edition
add to that reader’s appreciation of the material. In many places, I refer to
computer programming, and some of the exercises require some programming.
A careful coverage of Part III requires background in numerical programming.
In regard to the use of the book as a text, most of the book evolved in one
way or another for my own use in the classroom. I must quickly admit, how-
ever, that I have never used this whole book as a text for any single course. I
have used Part III in the form of printed notes as the primary text for a course
in the “foundations of computational science” taken by graduate students in
the natural sciences (including a few statistics students, but dominated by
physics students). I have provided several sections from Parts I and II in online
PDF ﬁles as supplementary material for a two-semester course in mathemati-
cal statistics at the “baby measure theory” level (using Shao, 2003). Likewise,
for my courses in computational statistics and statistical visualization, I have
provided many sections, either as supplementary material or as the primary
text, in online PDF ﬁles or printed notes. I have not taught a regular “applied
statistics” course in almost 30 years, but if I did, I am sure that I would draw
heavily from Parts I and II for courses in regression or multivariate analysis.
If I ever taught a course in “matrices for statistics” (I don’t even know if
such courses exist), this book would be my primary text because I think it
covers most of the things statisticians need to know about matrix theory and
computations.
Some exercises are Monte Carlo studies. I do not discuss Monte Carlo
methods in this text, so the reader lacking background in that area may need
to consult another reference in order to work those exercises. The exercises
should be considered an integral part of the book. For some exercises, the
required software can be obtained from either statlib or netlib (see the
bibliography). Exercises in any of the chapters, not just in Part III, may
require computations or computer programming.
Penultimately, I must make some statement about the relationship of this
book to some other books on similar topics. A much important statistical
theory and many methods make use of matrix theory, and many statisticians
have contributed to the advancement of matrix theory from its very early
days. Widely used books with derivatives of the words “statistics” and “ma-
trices/linearalgebra” in their titles include Basilevsky (1983), Graybill (1983),
Harville (1997), Schott (2004), and Searle (1982). All of these are useful books.
The computational orientation of this book is probably the main diﬀerence
between it and these other books. Also, some of these other books only ad-
dress topics of use in linear models, whereas this book also discusses matrices
useful in graph theory, stochastic processes, and other areas of application.
(If the applications are only in linear models, most matrices of interest are
symmetric and all eigenvalues can be considered to be real.) Other diﬀerences
among all of these books, of course, involve the authors’ choices of secondary
topics and the ordering of the presentation.
Fairfax County, VA, USA
James E. Gentle

Acknowledgments
I thank John Kimmel of Springer for his encouragement and advice on this
book and other books on which he has worked with me. I especially thank
Ken Berk for his extensive and insightful comments on a draft of this book.
I thank my student Li Li for reading through various drafts of some of the
chapters and pointing out typos or making helpful suggestions. I thank the
anonymous reviewers of this edition for their comments and suggestions. I also
thank the many readers of my previous book on numerical linear algebra who
informed me of errors and who otherwise provided comments or suggestions
for improving the exposition. Whatever strengths this book may have can be
attributed in large part to these people, named or otherwise. The weaknesses
can only be attributed to my own ignorance or hardheadedness.
I thank my wife, Mar´ıa, to whom this book is dedicated, for everything.
I used TEX via LATEX2ε to write the book. I did all of the typing, program-
ming, etc., myself, so all misteaks are mine. I would appreciate receiving sug-
gestions for improvement and notiﬁcation of errors.
Fairfax County, VA, USA
James E. Gentle
June 12, 2007
xv

Contents
Preface to the Second Edition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii
Preface to the First Edition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ix
Part I Linear Algebra
1
Basic Vector/Matrix Structure and Notation . . . . . . . . . . . . . .
3
1.1
Vectors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3.1
Subvectors and Submatrices . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4
Representation of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2
Vectors and Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1
Operations on Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.1.1
Linear Combinations and Linear Independence . . . . . . . . 12
2.1.2
Vector Spaces and Spaces of Vectors . . . . . . . . . . . . . . . . . 13
2.1.3
Basis Sets for Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . 21
2.1.4
Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.1.5
Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
2.1.6
Normalized Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.1.7
Metrics and Distances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
2.1.8
Orthogonal Vectors and Orthogonal Vector Spaces . . . . . 33
2.1.9
The “One Vector” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2.2
Cartesian Coordinates and Geometrical Properties of Vectors . 35
2.2.1
Cartesian Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.2.2
Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2.2.3
Angles Between Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
2.2.4
Orthogonalization Transformations: Gram-Schmidt . . . . 38
2.2.5
Orthonormal Basis Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
xvii

xviii
Contents
2.2.6
Approximation of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . 41
2.2.7
Flats, Aﬃne Spaces, and Hyperplanes . . . . . . . . . . . . . . . . 43
2.2.8
Cones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.2.9
Cross Products in IR3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.3
Centered Vectors and Variances and Covariances of Vectors . . . 48
2.3.1
The Mean and Centered Vectors . . . . . . . . . . . . . . . . . . . . 48
2.3.2
The Standard Deviation, the Variance, and
Scaled Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
2.3.3
Covariances and Correlations Between Vectors . . . . . . . . 50
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
3
Basic Properties of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.1
Basic Deﬁnitions and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
3.1.1
Multiplication of a Matrix by a Scalar . . . . . . . . . . . . . . . 56
3.1.2
Diagonal Elements: diag(·) and vecdiag(·) . . . . . . . . . . . . 56
3.1.3
Diagonal, Hollow, and Diagonally Dominant Matrices . . 57
3.1.4
Matrices with Special Patterns of Zeroes . . . . . . . . . . . . . 58
3.1.5
Matrix Shaping Operators . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3.1.6
Partitioned Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.1.7
Matrix Addition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.1.8
Scalar-Valued Operators on Square Matrices:
The Trace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
3.1.9
Scalar-Valued Operators on Square Matrices:
The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
3.2
Multiplication of Matrices and Multiplication of
Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.2.1
Matrix Multiplication (Cayley) . . . . . . . . . . . . . . . . . . . . . . 75
3.2.2
Multiplication of Matrices with Special Patterns. . . . . . . 78
3.2.3
Elementary Operations on Matrices . . . . . . . . . . . . . . . . . . 80
3.2.4
The Trace of a Cayley Product That Is Square . . . . . . . . 88
3.2.5
The Determinant of a Cayley Product of Square
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.2.6
Multiplication of Matrices and Vectors . . . . . . . . . . . . . . . 89
3.2.7
Outer Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
3.2.8
Bilinear and Quadratic Forms: Deﬁniteness . . . . . . . . . . . 91
3.2.9
Anisometric Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
3.2.10 Other Kinds of Matrix Multiplication . . . . . . . . . . . . . . . . 94
3.3
Matrix Rank and the Inverse of a Matrix . . . . . . . . . . . . . . . . . . . 99
3.3.1
Row Rank and Column Rank . . . . . . . . . . . . . . . . . . . . . . . 100
3.3.2
Full Rank Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
3.3.3
Rank of Elementary Operator Matrices and Matrix
Products Involving Them. . . . . . . . . . . . . . . . . . . . . . . . . . . 101
3.3.4
The Rank of Partitioned Matrices, Products of
Matrices, and Sums of Matrices . . . . . . . . . . . . . . . . . . . . . 102

Contents
xix
3.3.5
Full Rank Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
3.3.6
Full Rank Matrices and Matrix Inverses . . . . . . . . . . . . . . 105
3.3.7
Full Rank Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
3.3.8
Equivalent Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.3.9
Multiplication by Full Rank Matrices . . . . . . . . . . . . . . . . 112
3.3.10 Gramian Matrices: Products of the Form ATA . . . . . . . . 115
3.3.11 A Lower Bound on the Rank of a Matrix Product . . . . . 117
3.3.12 Determinants of Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
3.3.13 Inverses of Products and Sums of Nonsingular
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
3.3.14 Inverses of Matrices with Special Forms . . . . . . . . . . . . . . 120
3.3.15 Determining the Rank of a Matrix . . . . . . . . . . . . . . . . . . . 121
3.4
More on Partitioned Square Matrices:
The Schur Complement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
3.4.1
Inverses of Partitioned Matrices . . . . . . . . . . . . . . . . . . . . . 122
3.4.2
Determinants of Partitioned Matrices . . . . . . . . . . . . . . . . 122
3.5
Linear Systems of Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.5.1
Solutions of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . 123
3.5.2
Null Space: The Orthogonal Complement. . . . . . . . . . . . . 126
3.6
Generalized Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
3.6.1
Immediate Properties of Generalized Inverses . . . . . . . . . 127
3.6.2
Special Generalized Inverses: The Moore-Penrose
Inverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
3.6.3
Generalized Inverses of Products and Sums
of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.6.4
Generalized Inverses of Partitioned Matrices . . . . . . . . . . 131
3.7
Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
3.7.1
Orthogonal Matrices: Deﬁnition and
Simple Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
3.7.2
Orthogonal and Orthonormal Columns . . . . . . . . . . . . . . . 133
3.7.3
The Orthogonal Group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
3.7.4
Conjugacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
3.8
Eigenanalysis: Canonical Factorizations . . . . . . . . . . . . . . . . . . . . 134
3.8.1
Eigenvalues and Eigenvectors Are Remarkable . . . . . . . . 135
3.8.2
Left Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
3.8.3
Basic Properties of Eigenvalues and Eigenvectors . . . . . . 136
3.8.4
The Characteristic Polynomial . . . . . . . . . . . . . . . . . . . . . . 138
3.8.5
The Spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
3.8.6
Similarity Transformations . . . . . . . . . . . . . . . . . . . . . . . . . 146
3.8.7
Schur Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
3.8.8
Similar Canonical Factorization: Diagonalizable
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
3.8.9
Properties of Diagonalizable Matrices . . . . . . . . . . . . . . . . 152
3.8.10 Eigenanalysis of Symmetric Matrices . . . . . . . . . . . . . . . . . 153

xx
Contents
3.8.11 Positive Deﬁnite and Nonnegative Deﬁnite Matrices . . . 159
3.8.12 Generalized Eigenvalues and Eigenvectors . . . . . . . . . . . . 160
3.8.13 Singular Values and the Singular Value Decomposition
(SVD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
3.9
Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
3.9.1
Matrix Norms Induced from Vector Norms . . . . . . . . . . . 165
3.9.2
The Frobenius Norm—The “Usual” Norm . . . . . . . . . . . . 167
3.9.3
Other Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
3.9.4
Matrix Norm Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
3.9.5
The Spectral Radius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
3.9.6
Convergence of a Matrix Power Series . . . . . . . . . . . . . . . . 171
3.10 Approximation of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
3.10.1 Measures of the Diﬀerence Between Two Matrices . . . . . 175
3.10.2 Best Approximation with a Matrix of Given Rank . . . . . 176
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
4
Vector/Matrix Derivatives and Integrals . . . . . . . . . . . . . . . . . . . 185
4.1
Functions of Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 186
4.2
Basics of Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
4.2.1
Continuity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
4.2.2
Notation and Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
4.2.3
Diﬀerentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.3
Types of Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
4.3.1
Diﬀerentiation with Respect to a Scalar . . . . . . . . . . . . . . 190
4.3.2
Diﬀerentiation with Respect to a Vector . . . . . . . . . . . . . . 191
4.3.3
Diﬀerentiation with Respect to a Matrix . . . . . . . . . . . . . 196
4.4
Optimization of Scalar-Valued Functions . . . . . . . . . . . . . . . . . . . 198
4.4.1
Stationary Points of Functions . . . . . . . . . . . . . . . . . . . . . . 200
4.4.2
Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
4.4.3
Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
4.4.4
Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
4.4.5
Optimization of Functions with Constraints . . . . . . . . . . . 208
4.4.6
Optimization Without Diﬀerentiation . . . . . . . . . . . . . . . . 213
4.5
Integration and Expectation: Applications to Probability
Distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
4.5.1
Multidimensional Integrals and Integrals Involving
Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
4.5.2
Integration Combined with Other Operations . . . . . . . . . 216
4.5.3
Random Variables and Probability Distributions . . . . . . 217
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
5
Matrix Transformations and Factorizations . . . . . . . . . . . . . . . . 227
5.1
Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
5.2
Computational Methods: Direct and Iterative . . . . . . . . . . . . . . . 228

Contents
xxi
5.3
Linear Geometric Transformations . . . . . . . . . . . . . . . . . . . . . . . . . 229
5.3.1
Invariance Properties of Linear Transformations . . . . . . . 229
5.3.2
Transformations by Orthogonal Matrices . . . . . . . . . . . . . 230
5.3.3
Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
5.3.4
Reﬂections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
5.3.5
Translations: Homogeneous Coordinates . . . . . . . . . . . . . . 234
5.4
Householder Transformations (Reﬂections) . . . . . . . . . . . . . . . . . . 235
5.4.1
Zeroing All Elements But One in a Vector . . . . . . . . . . . . 236
5.4.2
Computational Considerations . . . . . . . . . . . . . . . . . . . . . . 237
5.5
Givens Transformations (Rotations) . . . . . . . . . . . . . . . . . . . . . . . 238
5.5.1
Zeroing One Element in a Vector . . . . . . . . . . . . . . . . . . . . 239
5.5.2
Givens Rotations That Preserve Symmetry . . . . . . . . . . . 240
5.5.3
Givens Rotations to Transform to Other Values . . . . . . . 240
5.5.4
Fast Givens Rotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
5.6
Factorization of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
5.7
LU and LDU Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
5.7.1
Properties: Existence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
5.7.2
Pivoting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
5.7.3
Use of Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
5.7.4
Properties: Uniqueness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
5.7.5
Properties of the LDU Factorization of a Square
Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
5.8
QR Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
5.8.1
Related Matrix Factorizations . . . . . . . . . . . . . . . . . . . . . . . 249
5.8.2
Matrices of Full Column Rank . . . . . . . . . . . . . . . . . . . . . . 249
5.8.3
Relation to the Moore-Penrose Inverse for Matrices of
Full Column Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
5.8.4
Nonfull Rank Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
5.8.5
Relation to the Moore-Penrose Inverse . . . . . . . . . . . . . . . 251
5.8.6
Determining the Rank of a Matrix . . . . . . . . . . . . . . . . . . . 252
5.8.7
Formation of the QR Factorization . . . . . . . . . . . . . . . . . . 252
5.8.8
Householder Reﬂections to Form the
QR Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
5.8.9
Givens Rotations to Form the QR Factorization . . . . . . . 253
5.8.10 Gram-Schmidt Transformations to Form the
QR Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.9
Factorizations of Nonnegative Deﬁnite Matrices . . . . . . . . . . . . . 254
5.9.1
Square Roots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
5.9.2
Cholesky Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
5.9.3
Factorizations of a Gramian Matrix . . . . . . . . . . . . . . . . . . 258
5.10 Approximate Matrix Factorization . . . . . . . . . . . . . . . . . . . . . . . . . 259
5.10.1 Nonnegative Matrix Factorization . . . . . . . . . . . . . . . . . . . 259
5.10.2 Incomplete Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . 260
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261

xxii
Contents
6
Solution of Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
6.1
Condition of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
6.1.1
Condition Number. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
6.1.2
Improving the Condition Number . . . . . . . . . . . . . . . . . . . 272
6.1.3
Numerical Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
6.2
Direct Methods for Consistent Systems . . . . . . . . . . . . . . . . . . . . . 274
6.2.1
Gaussian Elimination and Matrix Factorizations. . . . . . . 274
6.2.2
Choice of Direct Method . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
6.3
Iterative Methods for Consistent Systems . . . . . . . . . . . . . . . . . . . 279
6.3.1
The Gauss-Seidel Method with
Successive Overrelaxation . . . . . . . . . . . . . . . . . . . . . . . . . . 279
6.3.2
Conjugate Gradient Methods for Symmetric
Positive Deﬁnite Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
6.3.3
Multigrid Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
6.4
Iterative Reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
6.5
Updating a Solution to a Consistent System . . . . . . . . . . . . . . . . 287
6.6
Overdetermined Systems: Least Squares . . . . . . . . . . . . . . . . . . . . 289
6.6.1
Least Squares Solution of an Overdetermined
System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
6.6.2
Least Squares with a Full Rank Coeﬃcient Matrix . . . . . 292
6.6.3
Least Squares with a Coeﬃcient Matrix
Not of Full Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
6.6.4
Weighted Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
6.6.5
Updating a Least Squares Solution of an
Overdetermined System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
6.7
Other Solutions of Overdetermined Systems. . . . . . . . . . . . . . . . . 296
6.7.1
Solutions that Minimize Other Norms
of the Residuals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
6.7.2
Regularized Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
6.7.3
Minimizing Orthogonal Distances. . . . . . . . . . . . . . . . . . . . 301
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
7
Evaluation of Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . 307
7.1
General Computational Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 308
7.1.1
Numerical Condition of an Eigenvalue Problem . . . . . . . 308
7.1.2
Eigenvalues from Eigenvectors and Vice Versa. . . . . . . . . 310
7.1.3
Deﬂation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
7.1.4
Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
7.1.5
Shifting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
7.2
Power Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
7.2.1
Inverse Power Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
7.3
Jacobi Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
7.4
QR Method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318

Contents
xxiii
7.5
Krylov Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
7.6
Generalized Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
7.7
Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
Part II Applications in Data Analysis
8
Special Matrices and Operations Useful in Modeling and
Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
8.1
Data Matrices and Association Matrices . . . . . . . . . . . . . . . . . . . . 330
8.1.1
Flat Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
8.1.2
Graphs and Other Data Structures . . . . . . . . . . . . . . . . . . 331
8.1.3
Term-by-Document Matrices . . . . . . . . . . . . . . . . . . . . . . . . 338
8.1.4
Probability Distribution Models . . . . . . . . . . . . . . . . . . . . . 339
8.1.5
Derived Association Matrices . . . . . . . . . . . . . . . . . . . . . . . 340
8.2
Symmetric Matrices and Other Unitarily Diagonalizable
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
8.2.1
Some Important Properties of Symmetric Matrices . . . . 340
8.2.2
Approximation of Symmetric Matrices and an
Important Inequality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
8.2.3
Normal Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
8.3
Nonnegative Deﬁnite Matrices: Cholesky Factorization . . . . . . . 346
8.3.1
Eigenvalues of Nonnegative Deﬁnite Matrices . . . . . . . . . 347
8.3.2
The Square Root and the Cholesky Factorization . . . . . . 347
8.3.3
The Convex Cone of Nonnegative Deﬁnite Matrices . . . . 348
8.4
Positive Deﬁnite Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
8.4.1
Leading Principal Submatrices of Positive Deﬁnite
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
8.4.2
The Convex Cone of Positive Deﬁnite Matrices . . . . . . . . 351
8.4.3
Inequalities Involving Positive Deﬁnite Matrices . . . . . . . 351
8.5
Idempotent and Projection Matrices . . . . . . . . . . . . . . . . . . . . . . . 352
8.5.1
Idempotent Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
8.5.2
Projection Matrices: Symmetric Idempotent
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
8.6
Special Matrices Occurring in Data Analysis . . . . . . . . . . . . . . . . 359
8.6.1
Gramian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
8.6.2
Projection and Smoothing Matrices . . . . . . . . . . . . . . . . . . 362
8.6.3
Centered Matrices and Variance-Covariance
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
8.6.4
The Generalized Variance . . . . . . . . . . . . . . . . . . . . . . . . . . 368
8.6.5
Similarity Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
8.6.6
Dissimilarity Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371

xxiv
Contents
8.7
Nonnegative and Positive Matrices. . . . . . . . . . . . . . . . . . . . . . . . . 372
8.7.1
The Convex Cones of Nonnegative and Positive
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
8.7.2
Properties of Square Positive Matrices . . . . . . . . . . . . . . . 373
8.7.3
Irreducible Square Nonnegative Matrices . . . . . . . . . . . . . 375
8.7.4
Stochastic Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
8.7.5
Leslie Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
8.8
Other Matrices with Special Structures . . . . . . . . . . . . . . . . . . . . . 380
8.8.1
Helmert Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
8.8.2
Vandermonde Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
8.8.3
Hadamard Matrices and Orthogonal Arrays. . . . . . . . . . . 382
8.8.4
Toeplitz Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
8.8.5
Circulant Matrices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
8.8.6
Fourier Matrices and the Discrete Fourier
Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
8.8.7
Hankel Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
8.8.8
Cauchy Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391
8.8.9
Matrices Useful in Graph Theory . . . . . . . . . . . . . . . . . . . . 392
8.8.10 Z-Matrices and M-Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 396
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396
9
Selected Applications in Statistics . . . . . . . . . . . . . . . . . . . . . . . . . 399
9.1
Structure in Data and Statistical Data Analysis . . . . . . . . . . . . . 399
9.2
Multivariate Probability Distributions . . . . . . . . . . . . . . . . . . . . . . 400
9.2.1
Basic Deﬁnitions and Properties . . . . . . . . . . . . . . . . . . . . . 400
9.2.2
The Multivariate Normal Distribution. . . . . . . . . . . . . . . . 401
9.2.3
Derived Distributions and Cochran’s Theorem . . . . . . . . 401
9.3
Linear Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
9.3.1
Fitting the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
9.3.2
Linear Models and Least Squares . . . . . . . . . . . . . . . . . . . . 408
9.3.3
Statistical Inference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410
9.3.4
The Normal Equations and the Sweep Operator . . . . . . . 414
9.3.5
Linear Least Squares Subject to Linear
Equality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
9.3.6
Weighted Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
9.3.7
Updating Linear Regression Statistics . . . . . . . . . . . . . . . . 417
9.3.8
Linear Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
9.3.9
Multivariate Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . 420
9.4
Principal Components. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
9.4.1
Principal Components of a Random Vector . . . . . . . . . . . 424
9.4.2
Principal Components of Data . . . . . . . . . . . . . . . . . . . . . . 425
9.5
Condition of Models and Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
9.5.1
Ill-Conditioning in Statistical Applications . . . . . . . . . . . . 429
9.5.2
Variable Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
9.5.3
Principal Components Regression . . . . . . . . . . . . . . . . . . . 430

Contents
xxv
9.5.4
Shrinkage Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
9.5.5
Statistical Inference about the Rank of a Matrix . . . . . . 433
9.5.6
Incomplete Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
9.6
Optimal Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
9.6.1
D-Optimal Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
9.7
Multivariate Random Number Generation . . . . . . . . . . . . . . . . . . 443
9.7.1
The Multivariate Normal Distribution. . . . . . . . . . . . . . . . 443
9.7.2
Random Correlation Matrices . . . . . . . . . . . . . . . . . . . . . . . 444
9.8
Stochastic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
9.8.1
Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445
9.8.2
Markovian Population Models. . . . . . . . . . . . . . . . . . . . . . . 448
9.8.3
Autoregressive Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
Part III Numerical Methods and Software
10
Numerical Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
10.1 Digital Representation of Numeric Data . . . . . . . . . . . . . . . . . . . . 466
10.1.1 The Fixed-Point Number System . . . . . . . . . . . . . . . . . . . . 466
10.1.2 The Floating-Point Model for Real Numbers . . . . . . . . . . 468
10.1.3 Language Constructs for Representing Numeric
Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 476
10.1.4 Other Variations in the Representation of Data;
Portability of Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
10.2 Computer Operations on Numeric Data . . . . . . . . . . . . . . . . . . . . 483
10.2.1 Fixed-Point Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
10.2.2 Floating-Point Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 485
10.2.3 Language Constructs for Operations on
Numeric Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
10.2.4 Software Methods for Extending the Precision . . . . . . . . 493
10.2.5 Exact Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495
10.3 Numerical Algorithms and Analysis . . . . . . . . . . . . . . . . . . . . . . . . 496
10.3.1 Algorithms and Programs . . . . . . . . . . . . . . . . . . . . . . . . . . 496
10.3.2 Error in Numerical Computations . . . . . . . . . . . . . . . . . . . 496
10.3.3 Eﬃciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
10.3.4 Iterations and Convergence . . . . . . . . . . . . . . . . . . . . . . . . . 510
10.3.5 Other Computational Techniques . . . . . . . . . . . . . . . . . . . . 513
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
11
Numerical Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
11.1 Computer Storage of Vectors and Matrices. . . . . . . . . . . . . . . . . . 523
11.1.1 Storage Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524
11.1.2 Strides . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524
11.1.3 Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524

xxvi
Contents
11.2 General Computational Considerations for Vectors and
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
11.2.1 Relative Magnitudes of Operands. . . . . . . . . . . . . . . . . . . . 525
11.2.2 Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
11.2.3 Assessing Computational Errors . . . . . . . . . . . . . . . . . . . . . 528
11.3 Multiplication of Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . 529
11.3.1 Strassen’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531
11.3.2 Matrix Multiplication Using MapReduce . . . . . . . . . . . . . 533
11.4 Other Matrix Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533
11.4.1 Rank Determination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
11.4.2 Computing the Determinant . . . . . . . . . . . . . . . . . . . . . . . . 535
11.4.3 Computing the Condition Number . . . . . . . . . . . . . . . . . . . 535
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
12
Software for Numerical Linear Algebra . . . . . . . . . . . . . . . . . . . . 539
12.1 General Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 539
12.1.1 Software Development and Open Source Software . . . . . 540
12.1.2 Collaborative Research and Version Control . . . . . . . . . . 541
12.1.3 Finding Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
12.1.4 Software Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541
12.1.5 Software Development, Maintenance, and Testing. . . . . . 550
12.1.6 Reproducible Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
12.2 Software Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
12.2.1 BLAS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
12.2.2 Level 2 and Level 3 BLAS, LAPACK, and Related
Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
12.2.3 Libraries for High Performance Computing . . . . . . . . . . . 559
12.2.4 The IMSL Libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562
12.3 General Purpose Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
12.3.1 Programming Considerations . . . . . . . . . . . . . . . . . . . . . . . 566
12.3.2 Modern Fortran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
12.3.3 C and C++ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570
12.3.4 Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
12.4 Interactive Systems for Array Manipulation . . . . . . . . . . . . . . . . . 572
12.4.1 R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572
12.4.2 MATLAB and Octave. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 582
Appendices and Back Matter
Notation and Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
A.1 General Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
A.2 Computer Number Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
A.3 General Mathematical Functions and Operators . . . . . . . . . . . . . 592
A.3.1 Special Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594

Contents
xxvii
A.4 Linear Spaces and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595
A.4.1 Norms and Inner Products . . . . . . . . . . . . . . . . . . . . . . . . . 597
A.4.2 Matrix Shaping Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . 598
A.4.3 Notation for Rows or Columns of Matrices. . . . . . . . . . . . 600
A.4.4 Notation Relating to Matrix Determinants. . . . . . . . . . . . 600
A.4.5 Matrix-Vector Diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . 600
A.4.6 Special Vectors and Matrices. . . . . . . . . . . . . . . . . . . . . . . . 601
A.4.7 Elementary Operator Matrices . . . . . . . . . . . . . . . . . . . . . . 601
A.5 Models and Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602
Solutions and Hints for Selected Exercises . . . . . . . . . . . . . . . . . . . . . 603
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633

Author Biography
James E. Gentle, PhD, is University Professor of Computational Statistics
at George Mason University. He is a Fellow of the American Statistical Associ-
ation (ASA) and of the American Association for the Advancement of Science.
Professor Gentle has held national oﬃces in the ASA and has served as editor
and associate editor of journals of the ASA as well as for other journals in
statistics and computing. He is the author of “Random Number Generation
and Monte Carlo Methods” (Springer, 2003) and “Computational Statistics”
(Springer, 2009).
xxix

Part I
Linear Algebra

1
Basic Vector/Matrix Structure and Notation
Vectors and matrices are useful in representing multivariate numeric data,
and they occur naturally in working with linear equations or when expressing
linear relationships among objects. Numerical algorithms for a variety of tasks
involve matrix and vector arithmetic. An optimization algorithm to ﬁnd the
minimum of a function, for example, may use a vector of ﬁrst derivatives and
a matrix of second derivatives; and a method to solve a diﬀerential equation
may use a matrix with a few diagonals for computing diﬀerences.
There are various precise ways of deﬁning vectors and matrices, but we
will generally think of them merely as linear or rectangular arrays of numbers,
or scalars, on which an algebra is deﬁned. Unless otherwise stated, we will as-
sume the scalars are real numbers. We denote both the set of real numbers
and the ﬁeld of real numbers as IR. (The ﬁeld is the set together with the
two operators.) Occasionally we will take a geometrical perspective for vec-
tors and will consider matrices to deﬁne geometrical transformations. In all
contexts, however, the elements of vectors or matrices are real numbers (or,
more generally, members of a ﬁeld). When the elements are not members of
a ﬁeld (names or characters, for example) we will use more general phrases,
such as “ordered lists” or “arrays”.
Many of the operations covered in the ﬁrst few chapters, especially the
transformations and factorizations in Chap. 5, are important because of their
use in solving systems of linear equations, which will be discussed in Chap. 6;
in computing eigenvectors, eigenvalues, and singular values, which will be
discussed in Chap. 7; and in the applications in Chap. 9.
Throughout the ﬁrst few chapters, we emphasize the facts that are impor-
tant in statistical applications. We also occasionally refer to relevant compu-
tational issues, although computational details are addressed speciﬁcally in
Part III.
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 1
3

4
1 Basic Vector/Matrix Notation
It is very important to understand that the form of a mathematical expres-
sion and the way the expression should be evaluated in actual practice may
be quite diﬀerent. We remind the reader of this fact from time to time. That
there is a diﬀerence in mathematical expressions and computational methods
is one of the main messages of Chaps. 10 and 11. (An example of this, in
notation that we will introduce later, is the expression A−1b. If our goal is to
solve a linear system Ax = b, we probably should never compute the matrix
inverse A−1 and then multiply it times b. Nevertheless, it may be entirely
appropriate to write the expression A−1b.)
1.1 Vectors
For a positive integer n, a vector (or n-vector) is an n-tuple, ordered (multi)set,
or array of n numbers, called elements or scalars. The number of elements is
called the order, or sometimes the “length”, of the vector. An n-vector can be
thought of as representing a point in n-dimensional space. In this setting, the
“length” of the vector may also mean the Euclidean distance from the origin to
the point represented by the vector; that is, the square root of the sum of the
squares of the elements of the vector. This Euclidean distance will generally
be what we mean when we refer to the length of a vector (see page 27).
In general, “length” is measured by a norm; see Sect. 2.1.5, beginning on
page 25.
We usually use a lowercase letter to represent a vector, and we use the
same letter with a single subscript to represent an element of the vector.
The ﬁrst element of an n-vector is the ﬁrst (1st) element and the last is the
nth element. (This statement is not a tautology; in some computer systems,
the ﬁrst element of an object used to represent a vector is the 0th element
of the object. This sometimes makes it diﬃcult to preserve the relationship
between the computer entity and the object that is of interest.) Although we
are very concerned about computational issues, we will use paradigms and
notation that maintain the priority of the object of interest rather than the
computer entity representing it.
We may write the n-vector x as
x =
⎛
⎜
⎝
x1
...
xn
⎞
⎟
⎠
(1.1)
or
x = (x1, . . . , xn).
(1.2)
We make no distinction between these two notations, although in some con-
texts we think of a vector as a “column”, so the ﬁrst notation may be more
natural. The simplicity of the second notation recommends it for common use.

1.3 Matrices
5
(And this notation does not require the additional symbol for transposition
that some people use when they write the elements of a vector horizontally.)
Two vectors are equal if and only if they are of the same order and each
element of one vector is equal to the corresponding element of the other.
Our view of vectors essentially associates the elements of a vector with
the coordinates of a cartesian geometry. There are other, more abstract, ways
of developing a theory of vectors that are called “coordinate-free”, but we
will not pursue those approaches here. For most applications in statistics, the
approach based on coordinates is more useful.
Thinking of the coordinates simply as real numbers, we use the notation
IRn
(1.3)
to denote the set of n-vectors with real elements.
This notation reinforces the notion that the coordinates of a vector corre-
spond to the direct product of single coordinates. The direct product of two
sets is denoted as “⊗”. For sets A and B, it is the set of all ordered doubletons
{(a, b), s.t. a ∈A, b ∈B},
hence, IRn = IR ⊗· · · ⊗IR (n times).
1.2 Arrays
Arrays are structured collections of elements corresponding in shape to lines,
rectangles, or rectangular solids. The number of dimensions of an array is often
called the rank of the array. Thus, a vector is an array of rank 1, and a matrix
is an array of rank 2. A scalar, which can be thought of as a degenerate
array, has rank 0. When referring to computer software objects, “rank” is
generally used in this sense. (This term comes from its use in describing a
tensor. A rank 0 tensor is a scalar, a rank 1 tensor is a vector, a rank 2 tensor
is a square matrix, and so on. In our usage referring to arrays, we do not
require that the dimensions be equal, however.) When we refer to “rank of
an array”, we mean the number of dimensions. When we refer to “rank of
a matrix”, we mean something diﬀerent, as we discuss in Sect. 3.3. In linear
algebra, this latter usage is far more common than the former.
1.3 Matrices
A matrix is a rectangular or two-dimensional array. We speak of the rows and
columns of a matrix. The rows or columns can be considered to be vectors,
and we often use this equivalence. An n × m matrix is one with n rows and
m columns. The number of rows and the number of columns determine the

6
1 Basic Vector/Matrix Notation
shape of the matrix. Note that the shape is the doubleton (n, m), not just
a single number such as the ratio. If the number of rows is the same as the
number of columns, the matrix is said to be square.
All matrices are two-dimensional in the sense of “dimension” used above.
The word “dimension”, however, when applied to matrices, often means some-
thing diﬀerent, namely the number of columns. (This usage of “dimension” is
common both in geometry and in traditional statistical applications.)
We usually use an uppercase letter to represent a matrix. To represent an
element of the matrix, we usually use the corresponding lowercase letter with
a subscript to denote the row and a second subscript to represent the column.
If a nontrivial expression is used to denote the row or the column, we separate
the row and column subscripts with a comma.
Although vectors and matrices are fundamentally quite diﬀerent types of
objects, we can bring some unity to our discussion and notation by occasion-
ally considering a vector to be a “column vector” and in some ways to be the
same as an n × 1 matrix. (This has nothing to do with the way we may write
the elements of a vector. The notation in equation (1.2) is more convenient
than that in equation (1.1) and so will generally be used in this book, but its
use does not change the nature of the vector in any way. Likewise, this has
nothing to do with the way the elements of a vector or a matrix are stored
in the computer.) When we use vectors and matrices in the same expression,
however, we use the symbol “T” (for “transpose”) as a superscript to represent
a vector that is being treated as a 1 × n matrix.
The ﬁrst row is the 1st (ﬁrst) row, and the ﬁrst column is the 1st (ﬁrst)
column. (Again, we remark that computer entities used in some systems to
represent matrices and to store elements of matrices as computer data some-
times index the elements beginning with 0. Furthermore, some systems use
the ﬁrst index to represent the column and the second index to indicate the
row. We are not speaking here of the storage order—“row major” versus “col-
umn major”—we address that later, in Chap. 11. Rather, we are speaking of
the mechanism of referring to the abstract entities. In image processing, for
example, it is common practice to use the ﬁrst index to represent the col-
umn and the second index to represent the row. In the software packages IDL
and PV-Wave, for example, there are two diﬀerent kinds of two-dimensional
objects: “arrays”, in which the indexing is done as in image processing, and
“matrices”, in which the indexing is done as we have described.)
The n × m matrix A can be written
A =
⎡
⎢⎣
a11 . . . a1m
...
...
...
an1 . . . anm
⎤
⎥⎦.
(1.4)
We also write the matrix A above as
A = (aij),
(1.5)

1.3 Matrices
7
with the indices i and j ranging over {1, . . . , n} and {1, . . . , m}, respectively.
We use the notation An×m to refer to the matrix A and simultaneously to
indicate that it is n × m, and we use the notation
IRn×m
(1.6)
to refer to the set of all n × m matrices with real elements.
We use the notation (A)ij to refer to the element in the ith row and the
jth column of the matrix A; that is, in equation (1.4), (A)ij = aij.
Two matrices are equal if and only if they are of the same shape and each
element of one matrix is equal to the corresponding element of the other.
Although vectors are column vectors and the notation in equations (1.1)
and (1.2) represents the same entity, that would not be the same for matrices.
If x1, . . . , xn are scalars
X =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦
(1.7)
and
Y = [x1, . . . , xn],
(1.8)
then X is an n×1 matrix and Y is a 1×n matrix and X ̸= Y unless n = 1. (Y
is the transpose of X.) Although an n × 1 matrix is a diﬀerent type of object
from a vector, we may treat X in equation (1.7) or Y T in equation (1.8) as a
vector when it is convenient to do so. Furthermore, although a 1 × 1 matrix,
a 1-vector, and a scalar are all fundamentally diﬀerent types of objects, we
will treat a one by one matrix or a vector with only one element as a scalar
whenever it is convenient.
We sometimes use the notation a∗j to correspond to the jth column of the
matrix A and use ai∗to represent the (column) vector that corresponds to
the ith row. Using that noation, the n × m matrix A in equation (1.4) can be
written as
A =
⎡
⎢⎣
aT
1∗
...
aT
n∗
⎤
⎥⎦.
(1.9)
or as
A = [a∗1, . . . , a∗m] .
(1.10)
One of the most important uses of matrices is as a transformation of a vec-
tor by vector/matrix multiplication. Such transformations are linear (a term
that we deﬁne later). Although one can occasionally proﬁtably distinguish ma-
trices from linear transformations on vectors, for our present purposes there
is no advantage in doing so. We will often treat matrices and linear transfor-
mations as equivalent.

8
1 Basic Vector/Matrix Notation
Many of the properties of vectors and matrices we discuss hold for an
inﬁnite number of elements, but we will assume throughout this book that
the number is ﬁnite.
1.3.1 Subvectors and Submatrices
We sometimes ﬁnd it useful to work with only some of the elements of a vector
or matrix. We refer to the respective arrays as “subvectors” or “submatrices”.
We also allow the rearrangement of the elements by row or column permuta-
tions and still consider the resulting object as a subvector or submatrix. In
Chap. 3, we will consider special forms of submatrices formed by “partitions”
of given matrices.
The two expressions (1.9) and (1.10) represent special partitions of the
matrix A.
1.4 Representation of Data
Before we can do any serious analysis of data, the data must be represented
in some structure that is amenable to the operations of the analysis. In simple
cases, the data are represented by a list of scalar values. The ordering in the
list may be unimportant, and the analysis may just consist of computation of
simple summary statistics. In other cases, the list represents a time series of
observations, and the relationships of observations to each other as a function
of their order and distance apart in the list are of interest. Often, the data
can be represented meaningfully in two lists that are related to each other by
the positions in the lists. The generalization of this representation is a two-
dimensional array in which each column corresponds to a particular type of
data.
A major consideration, of course, is the nature of the individual items of
data. The observational data may be in various forms: quantitative measures,
colors, text strings, and so on. Prior to most analyses of data, they must be
represented as real numbers. In some cases, they can be represented easily
as real numbers, although there may be restrictions on the mapping into the
reals. (For example, do the data naturally assume only integral values, or
could any real number be mapped back to a possible observation?)
The most common way of representing data is by using a two-dimensional
array in which the rows correspond to observational units (“instances”) and
the columns correspond to particular types of observations (“variables” or
“features”). If the data correspond to real numbers, this representation is the
familiar X data matrix. Much of this book is devoted to the matrix theory
and computational methods for the analysis of data in this form. This type of
matrix, perhaps with an adjoined vector, is the basic structure used in many

1.4 Representation of Data
9
familiar statistical methods, such as regression analysis, principal components
analysis, analysis of variance, multidimensional scaling, and so on.
There are other types of structures based on graphs that are useful in
representing data. A graph is a structure consisting of two components: a
set of points, called vertices or nodes and a set of pairs of the points, called
edges. (Note that this usage of the word “graph” is distinctly diﬀerent from
the more common one that refers to lines, curves, bars, and so on to represent
data pictorially. The phrase “graph theory” is often used, or overused, to em-
phasize the present meaning of the word.) A graph G = (V, E) with vertices
V = {v1, . . . , vn} is distinguished primarily by the nature of the edge elements
(vi, vj) in E. Graphs are identiﬁed as complete graphs, directed graphs, trees,
and so on, depending on E and its relationship with V . A tree may be used
for data that are naturally aggregated in a hierarchy, such as political unit,
subunit, household, and individual. Trees are also useful for representing clus-
tering of data at diﬀerent levels of association. In this type of representation,
the individual data elements are the terminal nodes, or “leaves”, of the tree.
In another type of graphical representation that is often useful in “data
mining” or “learning”, where we seek to uncover relationships among objects,
the vertices are the objects, either observational units or features, and the
edges indicate some commonality between vertices. For example, the vertices
may be text documents, and an edge between two documents may indicate
that a certain number of speciﬁc words or phrases occur in both documents.
Despite the diﬀerences in the basic ways of representing data, in graphical
modeling of data, many of the standard matrix operations used in more tra-
ditional data analysis are applied to matrices that arise naturally from the
graph.
However the data are represented, whether in an array or a network, the
analysis of the data is often facilitated by using “association” matrices. The
most familiar type of association matrix is perhaps a correlation matrix. We
will encounter and use other types of association matrices in Chap. 8.
What You Compute and What You Don’t
The applied mathematician or statistician routinely performs many computa-
tions involving vectors and matrices. Many of those computations follow the
methods discussed in this text.
For a given matrix X, I will often refer to its inverse X−1, its determinant
det(X), its Gram XTX, a matrix formed by permuting its columns E(π)X, a
matrix formed by permuting its rows XE(π), and other transformations of the
given matrix X. These derived objects are very important and useful. Their
usefulness, however, is primarily conceptual.
When working with a real matrix X whose elements have actual known
values, it is not very often that we need or want the actual values of elements

10
1 Basic Vector/Matrix Notation
of these derived objects. Because of this, some authors try to avoid discussing
or referring directly to these objects.
I do not avoid discussing the objects, but, for example, when I write
(XTX)−1XTy, I do not mean that you should compute XTX and XTy, then
compute (XTX)−1, and then ﬁnally multiply (XTX)−1 and XTy. I assume
you know better than to do that. If you don’t know it yet, I hope after reading
this book, you will know why not to.

2
Vectors and Vector Spaces
In this chapter we discuss a wide range of basic topics related to vectors of real
numbers. Some of the properties carry over to vectors over other ﬁelds, such
as complex numbers, but the reader should not assume this. Occasionally, for
emphasis, we will refer to “real” vectors or “real” vector spaces, but unless it
is stated otherwise, we are assuming the vectors and vector spaces are real.
The topics and the properties of vectors and vector spaces that we emphasize
are motivated by applications in the data sciences.
2.1 Operations on Vectors
The elements of the vectors we will use in the following are real numbers, that
is, elements of IR. We call elements of IR scalars. Vector operations are deﬁned
in terms of operations on real numbers.
Two vectors can be added if they have the same number of elements.
The sum of two vectors is the vector whose elements are the sums of the
corresponding elements of the vectors being added. Vectors with the same
number of elements are said to be conformable for addition. A vector all of
whose elements are 0 is the additive identity for all conformable vectors.
We overload the usual symbols for the operations on the reals to signify
the corresponding operations on vectors or matrices when the operations are
deﬁned. Hence, “+” can mean addition of scalars, addition of conformable
vectors, or addition of a scalar to a vector. This last meaning of “+” may not
be used in many mathematical treatments of vectors, but it is consistent with
the semantics of modern computer languages such as Fortran, R, and Matlab.
By the addition of a scalar and a vector, we mean the addition of the scalar
to each element of the vector, resulting in a vector of the same number of
elements.
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 2
11

12
2 Vectors and Vector Spaces
A scalar multiple of a vector (that is, the product of a real number and
a vector) is the vector whose elements are the multiples of the corresponding
elements of the original vector. Juxtaposition of a symbol for a scalar and a
symbol for a vector indicates the multiplication of the scalar with each element
of the vector, resulting in a vector of the same number of elements.
The basic operation in working with vectors is the addition of a scalar
multiple of one vector to another vector,
z = ax + y,
(2.1)
where a is a scalar and x and y are vectors conformable for addition. Viewed
as a single operation with three operands, this is called an axpy operation
for obvious reasons. (Because the Fortran versions of BLAS to perform this
operation were called saxpy and daxpy, the operation is also sometimes called
“saxpy” or “daxpy”. See Sect. 12.2.1 on page 555, for a description of the
BLAS.)
The axpy operation is a linear combination. Such linear combinations of
vectors are the basic operations in most areas of linear algebra. The com-
position of axpy operations is also an axpy; that is, one linear combination
followed by another linear combination is a linear combination. Furthermore,
any linear combination can be decomposed into a sequence of axpy operations.
A special linear combination is called a convex combination. For vectors x
and y, it is the combination
ax + by,
(2.2)
where a, b ≥0 and a + b = 1. A set of vectors that is closed with respect to
convex combinations is said to be convex.
2.1.1 Linear Combinations and Linear Independence
If a given vector can be formed by a linear combination of one or more vectors,
the set of vectors (including the given one) is said to be linearly dependent;
conversely, if in a set of vectors no one vector can be represented as a linear
combination of any of the others, the set of vectors is said to be linearly
independent. In equation (2.1), for example, the vectors x, y, and z are not
linearly independent. It is possible, however, that any two of these vectors are
linearly independent.
Linear independence is one of the most important concepts in linear alge-
bra.
We can see that the deﬁnition of a linearly independent set of vectors
{v1, . . . , vk} is equivalent to stating that if
a1v1 + · · · akvk = 0,
(2.3)
then a1 = · · · = ak = 0. If the set of vectors {v1, . . . , vk} is not linearly inde-
pendent, then it is possible to select a maximal linearly independent subset;

2.1 Operations on Vectors
13
that is, a subset of {v1, . . . , vk} that is linearly independent and has maxi-
mum cardinality. We do this by selecting an arbitrary vector, vi1, and then
seeking a vector that is independent of vi1. If there are none in the set that
is linearly independent of vi1, then a maximum linearly independent subset
is just the singleton, because all of the vectors must be a linear combination
of just one vector (that is, a scalar multiple of that one vector). If there is a
vector that is linearly independent of vi1, say vi2, we next seek a vector in the
remaining set that is independent of vi1 and vi2. If one does not exist, then
{vi1, vi2} is a maximal subset because any other vector can be represented in
terms of these two and hence, within any subset of three vectors, one can be
represented in terms of the two others. Thus, we see how to form a maximal
linearly independent subset, and we see that the maximum cardinality of any
subset of linearly independent vectors is unique however they are formed.
It is easy to see that the maximum number of n-vectors that can form a
set that is linearly independent is n. (We can see this by assuming n linearly
independent vectors and then, for any (n + 1)th vector, showing that it is
a linear combination of the others by building it up one by one from linear
combinations of two of the given linearly independent vectors. In Exercise 2.1,
you are asked to write out these steps.)
Properties of a set of vectors are usually invariant to a permutation of the
elements of the vectors if the same permutation is applied to all vectors in the
set. In particular, if a set of vectors is linearly independent, the set remains
linearly independent if the elements of each vector are permuted in the same
way.
If the elements of each vector in a set of vectors are separated into sub-
vectors, linear independence of any set of corresponding subvectors implies
linear independence of the full vectors. To state this more precisely for a set
of three n-vectors, let x = (x1, . . . , xn), y = (y1, . . . , yn), and z = (z1, . . . , zn).
Now let {i1, . . . , ik} ⊆{1, . . . , n}, and form the k-vectors ˜x = (xi1, . . . , xik),
˜y = (yi1, . . . , yik), and ˜z = (zi1, . . . , zik). Then linear independence of ˜x, ˜y,
and ˜z implies linear independence of x, y, and z. (This can be shown directly
from the deﬁnition of linear independence. It is related to equation (2.19) on
page 20, which you are asked to prove in Exercise 2.5.)
2.1.2 Vector Spaces and Spaces of Vectors
Let V be a set of n-vectors such that any linear combination of the vectors
in V is also in V . Such a set together with the usual vector algebra is called
a vector space. A vector space is a linear space, and it necessarily includes
the additive identity (the zero vector). (To see this, in the axpy operation, let
a = −1 and y = x.) A vector space is necessarily convex.
The set consisting only of the additive identity, along with the axpy op-
eration, is a vector space. It is called the “null vector space”. Some people
deﬁne “vector space” in a way that excludes it, because its properties do not
conform to many general statements we can make about other vector spaces.

14
2 Vectors and Vector Spaces
The “usual algebra” is a linear algebra consisting of two operations: vector
addition and scalar times vector multiplication, which are the two operations
comprising an axpy. It has closure of the space under the combination of those
operations, commutativity and associativity of addition, an additive identity
and inverses, a multiplicative identity, distribution of multiplication over both
vector addition and scalar addition, and associativity of scalar multiplication
and scalar times vector multiplication.
A vector space can also be composed of other objects, such as matrices,
along with their appropriate operations. The key characteristic of a vector
space is a linear algebra.
We generally use a calligraphic font to denote a vector space; V or W, for
example. Often, however, we think of the vector space merely in terms of the
set of vectors on which it is built and denote it by an ordinary capital letter;
V or W, for example. A vector space is an algebraic structure consisting of
a set together with the axpy operation, with the restriction that the set is
closed under the operation. To indicate that it is a structure, rather than just
a set, we may write
V = (V, ◦),
where V is just the set and ◦denotes the axpy operation, or a similar linear
operation under which the set is closed.
2.1.2.1 Generating Sets
Given a set G of vectors of the same order, a vector space can be formed from
the set G together with all vectors that result from the axpy operation being
applied to all combinations of vectors in G and all values of the real number
a; that is, for all vi, vj ∈G and all real a,
{avi + vj}.
This set together with the axpy operation itself is a vector space. It is
called the space generated by G. We denote this space as
span(G).
We will discuss generating and spanning sets further in Sect. 2.1.3.
2.1.2.2 The Order and the Dimension of a Vector Space
The vector space consisting of all n-vectors with real elements is denoted IRn.
(As mentioned earlier, the notation IRn can also refer to just the set of n-
vectors with real elements; that is, to the set over which the vector space is
deﬁned.)
The dimension of a vector space is the maximum number of linearly inde-
pendent vectors in the vector space. We denote the dimension by

2.1 Operations on Vectors
15
dim(·),
which is a mapping IRn →ZZ+ (where ZZ+ denotes the positive integers).
The order of a vector space is the order of the vectors in the space. Because
the maximum number of n-vectors that can form a linearly independent set
is n, as we showed above, the order of a vector space is greater than or equal
to the dimension of the vector space.
Both the order and the dimension of IRn are n. A set of m linearly inde-
pendent n-vectors with real elements can generate a vector space within IRn
of order n and dimension m.
We also may use the phrase dimension of a vector to mean the dimension
of the vector space of which the vector is an element. This term is ambiguous,
but its meaning is clear in speciﬁc contexts, such as dimension reduction, that
we will discuss later.
2.1.2.3 Vector Spaces with an Inﬁnite Number of Dimensions
It is possible that no ﬁnite set of vectors span a given vector space. In that
case, the vector space is said to be of inﬁnite dimension.
Many of the properties of vector spaces that we discuss hold for those with
an inﬁnite number of dimensions; but not all do, such as the equivalence of
norms (see page 29).
Throughout this book, however, unless we state otherwise, we assume the
vector spaces have a ﬁnite number of dimensions.
2.1.2.4 Essentially Disjoint Vector Spaces
If the only element in common between two vector spaces V and W is the
additive identity, the spaces are said to be essentially disjoint. Essentially
disjoint vector spaces necessarily have the same order.
If the vector spaces V and W are essentially disjoint, it is clear that any
element in V (except the additive identity) is linearly independent of any set
of elements in W.
2.1.2.5 Some Special Vectors: Notation
We denote the additive identity in a vector space of order n by 0n or sometimes
by 0. This is the vector consisting of all zeros:
0n = (0, . . . , 0).
(2.4)
We call this the zero vector, or the null vector. (A vector x ̸= 0 is called a
“nonnull vector”.) This vector by itself is sometimes called the null vector
space. It is not a vector space in the usual sense; it would have dimension 0.
(All linear combinations are the same.)

16
2 Vectors and Vector Spaces
Likewise, we denote the vector consisting of all ones by 1n or sometimes
by 1:
1n = (1, . . . , 1).
(2.5)
We call this the one vector and also the “summing vector” (see page 34). This
vector and all scalar multiples of it are vector spaces with dimension 1. (This
is true of any single nonzero vector; all linear combinations are just scalar
multiples.) Whether 0 and 1 without a subscript represent vectors or scalars
is usually clear from the context.
The zero vector and the one vector are both instances of constant vectors;
that is, vectors all of whose elements are the same. In some cases we may abuse
the notation slightly, as we have done with “0” and “1” above, and use a single
symbol to denote both a scalar and a vector all of whose elements are that
constant; for example, if “c” denotes a scalar constant, we may refer to the
vector all of whose elements are c as “c” also. These notational conveniences
rarely result in any ambiguity. They also allow another interpretation of the
deﬁnition of addition of a scalar to a vector that we mentioned at the beginning
of the chapter.
The ith unit vector, denoted by ei, has a 1 in the ith position and 0s in all
other positions:
ei = (0, . . . , 0, 1, 0, . . ., 0).
(2.6)
Another useful vector is the sign vector, which is formed from signs of the
elements of a given vector. It is denoted by “sign(·)” and for x = (x1, . . . , xn)
is deﬁned by
sign(x)i =
1
if xi > 0,
=
0
if xi = 0,
= −1
if xi < 0.
(2.7)
2.1.2.6 Ordinal Relations Among Vectors
There are several possible ways to form a rank ordering of vectors of the same
order, but no complete ordering is entirely satisfactory. (Note the unfortunate
overloading of the words “order” and “ordering” here.) If x and y are vectors
of the same order and for corresponding elements xi > yi, we say x is greater
than y and write
x > y.
(2.8)
In particular, if all of the elements of x are positive, we write x > 0.
If x and y are vectors of the same order and for corresponding elements
xi ≥yi, we say x is greater than or equal to y and write
x ≥y.
(2.9)
This relationship is a partial ordering (see Exercise 8.2a on page 396 for the
deﬁnition of partial ordering).
The expression x ≥0 means that all of the elements of x are nonnegative.

2.1 Operations on Vectors
17
2.1.2.7 Set Operations on Vector Spaces
The ordinary operations of subsetting, intersection, union, direct sum, and
direct product for sets have analogs for vector spaces, and we use some of the
same notation to refer to vector spaces that we use to refer to sets. The set
operations themselves are performed on the individual sets to yield a set of
vectors, and the resulting vector space is the space generated by that set of
vectors.
Unfortunately, there are many inconsistencies in terminology used in the
literature regarding operations on vector spaces. When I use a term and/or
symbol, such as “union” or “∪”, for a structure such as a vector space, I use
it in reference to the structure. For example, if V = (V, ◦) and W = (W, ◦) are
vector spaces, then V ∪U is the ordinary union of the sets; however, V ∪W is
the union of the vector spaces, and is not necessarily the same as (U ∪W, ◦),
which may not even be a vector space. Occasionally in the following discussion,
I will try to point out common variants in usage.
The convention that I follow allows the wellknown relationships among
common set operations to hold for the corresponding operations on vector
spaces; for example, if V and W are vector spaces, V ⊆V ∪W, just as for sets
V and W.
The properties of vector spaces are proven the same way that properties
of sets are proven, after ﬁrst requiring that the axpy operation have the same
meaning in the diﬀerent vector spaces. For example, to prove that one vector
space is a subspace of another, we show that any given vector in the ﬁrst
vector space is necessarily in the second. To prove that two vector spaces
are equal, we show that each is a subspace of the other. Some properties of
vector spaces and subspaces can be shown more easily using “basis sets” for
the spaces, which we discuss in Sect. 2.1.3, beginning on page 21.
Note that if (V, ◦) and (W, ◦) are vector spaces of the same order and U is
some set formed by an operation on V and W, then (U, ◦) may not be a vector
space because it is not closed under the axpy operation, ◦. We sometimes refer
to a set of vectors of the same order together with the axpy operator (whether
or not the set is closed with respect to the operator) as a “space of vectors”
(instead of a “vector space”).
2.1.2.8 Subpaces
Given a vector space V = (V, ◦), if W is any subset of V , then the vector
space W generated by W, that is, span(W), is said to be a subspace of V, and
we denote this relationship by W ⊆V.
If W ⊆V and W ̸= V, then W is said to be a proper subspace of V. If
W = V, then W ⊆V and V ⊆W, and the converse is also true.
The maximum number of linearly independent vectors in the subspace
cannot be greater than the maximum number of linearly independent vectors
in the original space; that is, if W ⊆V, then

18
2 Vectors and Vector Spaces
dim(W) ≤dim(V)
(2.10)
(Exercise 2.2). If W is a proper subspace of V, then dim(W) < dim(V).
2.1.2.9 Intersections of Vector Spaces
For two vector spaces V and W of the same order with vectors formed from
the same ﬁeld, we deﬁne their intersection, denoted by V ∩W, to be the set
of vectors consisting of the intersection of the sets in the individual vector
spaces together with the axpy operation.
The intersection of two vector spaces of the same order that are not es-
sentially disjoint is a vector space, as we can see by letting x and y be any
vectors in the intersection U = V ∩W, and showing, for any real number a,
that ax + y ∈U. This is easy because both x and y must be in both V and
W.
Note that if V and W are essentially disjoint, then V ∩W = (0, ◦), which,
as we have said, is not a vector space in the usual sense.
Also note that
dim(V ∩W) ≤min(dim(V), dim(W))
(2.11)
(Exercise 2.2).
2.1.2.10 Unions and Direct Sums of Vector Spaces
Given two vector spaces V and W of the same order, we deﬁne their union,
denoted by V ∪W, to be the vector space generated by the union of the sets
in the individual vector spaces together with the axpy operation. If V = (V, ◦)
and W = (W, ◦), this is the vector space generated by the set of vectors V ∪W;
that is,
V ∪W = span(V ∪W).
(2.12)
The union of the sets of vectors in two vector spaces may not be closed
under the axpy operation (Exercise 2.3b), but the union of vector spaces is a
vector space by deﬁnition.
The vector space generated by the union of the sets in the individual vector
spaces is easy to form. Since (V, ◦) and (W, ◦) are vector spaces (so for any
vector x in either V or W, ax is in that set), all we need do is just include all
simple sums of the vectors from the individual sets, that is,
V ∪W = {v + w, s.t. v ∈V, w ∈W}.
(2.13)
It is easy to see that this is a vector space by showing that it is closed with
respect to axpy. (As above, we show that for any x and y in V ∪W and for
any real number a, ax + y is in V ∪W.)
(Because of the way the union of vector spaces can be formed from simple
addition of the individual elements, some authors call the vector space in

2.1 Operations on Vectors
19
equation (2.13) the “sum” of V and W, and write it as V +W. Other authors,
including myself, call this the direct sum, and denote it by V⊕W. Some authors
deﬁne “direct sum” only in the cases of vector spaces that are essentially
disjoint. Still other authors deﬁne “direct sum” to be what I will call a “direct
product” below.)
Despite the possible confusion with other uses of the notation, I often use
the notation V ⊕W because it points directly to the nice construction of
equation (2.13). To be clear: to the extent that I use “direct sum” and “⊕”
for vector spaces V and W, I will mean the direct sum
V ⊕W ≡V ∪W,
(2.14)
as deﬁned above.
Note that
dim(V ⊕W) = dim(V) + dim(W) −dim(V ∩W)
(2.15)
(Exercise 2.4). Therefore
dim(V ⊕W) ≥max(dim(V), dim(W))
and
dim(V ⊕W) ≤dim(V) + dim(W).
2.1.2.11 Direct Sum Decomposition of a Vector Space
In some applications, given a vector space V, it is of interest to ﬁnd essentially
disjoint vector spaces V1, . . . , Vn such that
V = V1 ⊕· · · ⊕Vn.
This is called a direct sum decomposition of V. (As I mentioned above, some
authors who do not use “direct sum” as I do would use the term in this context
because the individual matrices are essentially disjoint.)
It is clear that if V1, . . . , Vn is a direct sum decomposition of V, then
dim(V) =
n

i=1
dim(Vi)
(2.16)
(Exercise 2.4).
A collection of essentially disjoint vector spaces V1, . . . , Vn such that V =
V1 ⊕· · · ⊕Vn is said to be complementary with respect to V.
An important property of a direct sum decomposition is that it allows
a unique representation of a vector in the decomposed space in terms of a
sum of vectors from the individual essentially disjoint spaces; that is, if V =
V1 ⊕· · · ⊕Vn is a direct sum decomposition of V and v ∈V, then there exist
unique vectors vi ∈Vi such that

20
2 Vectors and Vector Spaces
v = v1 + · · · + vn.
(2.17)
We will prove this for the case n = 2. This is without loss, because additional
spaces in the decomposition add nothing diﬀerent.
Given the direct sum decomposition V = V1 ⊕V2, let v be any vector in
V. Because V1 ⊕V2 can be formed as in equation (2.13), there exist vectors
v1 ∈V1 and v2 ∈V2 such that v = v1 + v2. Now all we need to do is to show
that they are unique.
Let u1 ∈V1 and u2 ∈V2 be such that v = u1 +u2. Now we have (v −u1) ∈
V2 and (v −v1) ∈V2; hence (v1 −u1) ∈V2. However, since v1, u1 ∈V1,
(v1 −u1) ∈V1. Since V1 and V2 are essentially disjoint, and (v1 −u1) is in
both, it must be the case that (v1 −u1) = 0, or u1 = v1. In like manner, we
show that u2 = v2; hence, the representation v = v1 + v2 is unique.
An important fact is that for any vector space V with dimension 2 or
greater, a direct sum decomposition exists; that is, there exist essentially dis-
joint vector spaces V1 and V2 such that V = V1 ⊕V2.
This is easily shown by ﬁrst choosing a proper subspace V1 of V and then
constructing an essentially disjoint subspace V2 such that V = V1 ⊕V2. The
details of these steps are made simpler by use of basis sets which we will
discuss in Sect. 2.1.3, in particular the facts listed on page 22.
2.1.2.12 Direct Products of Vector Spaces and Dimension
Reduction
The set operations on vector spaces that we have mentioned so far require
that the vector spaces be of a ﬁxed order. Sometimes in applications, it is
useful to deal with vector spaces of diﬀerent orders.
The direct product of the vector space V of order n and the vector space
W of order m is the vector space of order n + m on the set of vectors
{(v1, . . . , vn, w1, . . . , wm), s.t. (v1, . . . , vn) ∈V, (w1, . . . , wm) ∈W},
(2.18)
together with the axpy operator deﬁned as the same operator in V and W
applied separately to the ﬁrst n and the last m elements. The direct product
of V and W is denoted by V ⊗W.
Notice that while the direct sum operation is commutative, the direct
product is not commutative in general.
The vectors in V and W are sometimes called “subvectors” of the vectors
in V ⊗W. These subvectors are related to projections, which we will discuss
in more detail in Sect. 2.2.2 (page 36) and Sect. 8.5.2 (page 358).
We can see that the direct product is a vector space using the same method
as above by showing that it is closed under the axpy operation.
Note that
dim(V ⊗W) = dim(V) + dim(W)
(2.19)
(Exercise 2.5).

2.1 Operations on Vectors
21
Note that for integers 0 < p < n,
IRn = IRp ⊗IRn−p,
(2.20)
where the operations in the space IRn are the same as in the component vector
spaces with the meaning adjusted to conform to the larger order of the vectors
in IRn. (Recall that IRn represents the algebraic structure consisting of the
set of n-tuples of real numbers plus the special axpy operator.)
In statistical applications, we often want to do “dimension reduction”.
This means to ﬁnd a smaller number of coordinates that cover the relevant
regions of a larger-dimensional space. In other words, we are interested in
ﬁnding a lower-dimensional vector space in which a given set of vectors in a
higher-dimensional vector space can be approximated by vectors in the lower-
dimensional space. For a given set of vectors of the form x = (x1, . . . , xn) we
seek a set of vectors of the form z = (z1, . . . , zp) that almost “cover the same
space”. (The transformation from x to z is called a projection.)
2.1.3 Basis Sets for Vector Spaces
If each vector in the vector space V can be expressed as a linear combination
of the vectors in some set G, then G is said to be a generating set or spanning
set of V. The number of vectors in a generating set is at least as great as the
dimension of the vector space.
If all linear combinations of the elements of G are in V, the vector space
is the space generated by G and is denoted by V(G) or by span(G), as we
mentioned on page 14. We will use either notation interchangeably:
V(G) ≡span(G).
(2.21)
Note that G is also a generating or spanning set for W where W ⊆span(G).
A basis for a vector space is a set of linearly independent vectors that
generate or span the space. For any vector space, a generating set consisting
of the minimum number of vectors of any generating set for that space is a
basis set for the space. A basis set is obviously not unique.
Note that the linear independence implies that a basis set cannot contain
the 0 vector.
An important fact is
•
The representation of a given vector in terms of a given basis set is unique.
To see this, let {v1, . . . , vk} be a basis for a vector space that includes the
vector x, and let
x = c1v1 + · · · ckvk.
Now suppose
x = b1v1 + · · · bkvk,

22
2 Vectors and Vector Spaces
so that we have
0 = (c1 −b1)v1 + · · · + (ck −bk)vk.
Since {v1, . . . , vk} are independent, the only way this is possible is if ci = bi
for each i.
A related fact is that if {v1, . . . , vk} is a basis for a vector space of order
n that includes the vector x and x = c1v1 + · · · ckvk, then x = 0n if and only
if ci = 0 for each i.
For any vector space, the order of the vectors in a basis set is the same as
the order of the vector space.
Because the vectors in a basis set are independent, the number of vectors
in a basis set is the same as the dimension of the vector space; that is, if B is
a basis set of the vector space V, then
dim(V) = #(B).
(2.22)
A simple basis set for the vector space IRn is the set of unit vectors
{e1, . . . , en}, deﬁned on page 16.
2.1.3.1 Properties of Basis Sets of Vector Subspaces
There are several interesting facts about basis sets for vector spaces and var-
ious combinations of the vector spaces. Veriﬁcations of these facts all follow
similar arguments, and most are left as exercises.
•
If B1 is a basis set for V1, B2 is a basis set for V2, and V1 and V2 are
essentially disjoint, then B1 ∩B2 = ∅.
This fact is easily seen by assuming the contrary; that is, assume that
b ∈B1 ∩B2. (Note that b cannot be the 0 vector.) This implies, however,
that b is in both V1 and V2, contradicting the hypothesis that they are
essentially disjoint.
•
If B is a basis set for V and V1 ⊆V, then there exists B1, with B1 ⊆B,
such that B1 is a basis set for V1.
•
If B1 is a basis set for V1 and B2 is a basis set for V2, then B1 ∪B2 is a
generating set for V1 ⊕V2.
(We see this easily from the deﬁnition of ⊕because any vector in V1 ⊕V2
can be represented as a linear combination of vectors in B1 plus a linear
combination of vectors in B2.)
•
If V1 and V2 are essentially disjoint, B1 is a basis set for V1, and B2 is a
basis set for V2, then B1 ∪B2 is a basis set for V = V1 ⊕V2.
This is the case that V1 ⊕V2 is a direct sum decomposition of V.
•
Suppose V1 is a real vector space of order n1 (that is, it is a subspace of
IRn1) and B1 is a basis set for V1. Now let V2 be a real vector space of
order n2 and B2 be a basis set for V2. For each vector b1 in B1 form the
vector
˜b1 = (b1|0, . . . , 0)
where there are n2 0s,

2.1 Operations on Vectors
23
and let B1 be the set of all such vectors. (The order of each ˜b1 ∈B1 is
n1 + n2.) Likewise, for each vector b2 in B2 form the vector
˜b2 = (0, . . . , 0|b2)
where there are n1 0s,
and let B2 be the set of all such vectors. Then B1∪B2 is a basis for V1⊗V2.
2.1.4 Inner Products
A useful operation on two vectors x and y of the same order is the inner
product, which we denote by ⟨x, y⟩and deﬁne as
⟨x, y⟩=

i
xi¯yi,
(2.23)
where ¯z represents the complex conjugate of z; that is, if z = a + bi, then
¯z = a −bi. In general, throughout this book unless stated otherwise, I assume
that we are working with real numbers, and hence, ¯z = z. Most statements
will hold whether the numbers are real or complex. When the statements only
hold for reals, I will generally include the exception in the statement. The main
diﬀerences have to do with inner products and an important property deﬁned
in terms of an inner product, called orthogonality.
In the case of vectors with real elements, we have
⟨x, y⟩=

i
xiyi.
(2.24)
In that case (which is what we generally assume throughout this book), the
inner product is a mapping
IRn × IRn →IR.
The inner product is also called the dot product or the scalar product.
The dot product is actually a special type of inner product, and there is some
ambiguity in the terminology. The dot product is the most commonly used
inner product in the applications we consider, and so we will use the terms
synonymously.
The inner product is also sometimes written as x · y, hence the name dot
product. Yet another notation for the inner product for real vectors is xTy,
and we will see later that this notation is natural in the context of matrix
multiplication. So for real vectors, we have the equivalent notations
⟨x, y⟩≡x · y ≡xTy.
(2.25)
(I will mention one more notation that is equivalent for real vectors. This is
the “bra·ket” notation originated by Paul Dirac, and is still used in certain

24
2 Vectors and Vector Spaces
areas of application. Dirac referred to xT as the “bra x”, and denoted it as
⟨x|. He referred to an ordinary vector y as the “ket y”, and denoted it as |y⟩.
He then denoted the inner product of the vectors as ⟨x||y⟩, or, omitting one
vertical bar, as ⟨x|y⟩.)
In general, the inner product is a mapping from a real vector space V to
IR that has the following properties:
1. Nonnegativity and mapping of the additive identity:
if x ̸= 0, then ⟨x, x⟩> 0 and ⟨0, x⟩= ⟨x, 0⟩= ⟨0, 0⟩= 0.
2. Commutativity:
⟨x, y⟩= ⟨y, x⟩.
3. Factoring of scalar multiplication in dot products:
⟨ax, y⟩= a⟨x, y⟩for real a.
4. Relation of vector addition to addition of dot products:
⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩.
These properties in fact deﬁne an inner product for mathematical objects for
which an addition, an additive identity, and a multiplication by a scalar are
deﬁned. Notice that the operation deﬁned in equation (2.24) is not an inner
product for vectors over the complex ﬁeld because, if x is complex, we can
have ⟨x, x⟩= 0 when x ̸= 0.
A vector space together with an inner product is called an inner product
space.
Inner products are also deﬁned for matrices, as we will discuss on page 97.
We should note in passing that there are two diﬀerent kinds of multiplication
used in property 3. The ﬁrst multiplication is scalar multiplication, that is,
an operation from IR × IRn to IRn, which we have deﬁned above, and the
second multiplication is ordinary multiplication in IR, that is, an operation
from IR × IR to IR. There are also two diﬀerent kinds of addition used in
property 4. The ﬁrst addition is vector addition, deﬁned above, and the second
addition is ordinary addition in IR. The dot product can reveal fundamental
relationships between the two vectors, as we will see later.
A useful property of inner products is the Cauchy-Schwarz inequality:
⟨x, y⟩≤⟨x, x⟩
1
2 ⟨y, y⟩
1
2 .
(2.26)
This relationship is also sometimes called the Cauchy-Bunyakovskii-Schwarz
inequality. (Augustin-Louis Cauchy gave the inequality for the kind of dis-
crete inner products we are considering here, and Viktor Bunyakovskii and
Hermann Schwarz independently extended it to more general inner products,
deﬁned on functions, for example.) The inequality is easy to see, by ﬁrst ob-
serving that for every real number t,
0 ≤⟨(tx + y), (tx + y)⟩
= ⟨x, x⟩t2 + 2⟨x, y⟩t + ⟨y, y⟩
= at2 + bt + c,

2.1 Operations on Vectors
25
where the constants a, b, and c correspond to the dot products in the preceding
equation. This quadratic in t cannot have two distinct real roots. Hence the
discriminant, b2 −4ac, must be less than or equal to zero; that is,
1
2b
2
≤ac.
By substituting and taking square roots, we get the Cauchy-Schwarz inequal-
ity. It is also clear from this proof that equality holds only if x = 0 or if y = rx,
for some scalar r.
Two vectors x and y such that ⟨x, y⟩= 0 are said to be orthogonal. This
term has such an intuitive meaning that we may use it prior to a careful
deﬁnition and study, so I only introduce it here. We will discuss orthogonality
more thoroughly in Sect. 2.1.8 beginning on page 33.
2.1.5 Norms
We consider a set of objects S that has an addition-type operator, +, a cor-
responding additive identity, 0, and a scalar multiplication; that is, a multi-
plication of the objects by a real (or complex) number. On such a set, a norm
is a function, ∥· ∥, from S to IR that satisﬁes the following three conditions:
1. Nonnegativity and mapping of the additive identity:
if x ̸= 0, then ∥x∥> 0, and ∥0∥= 0.
2. Relation of scalar multiplication to real multiplication:
∥ax∥= |a| ∥x∥for real a.
3. Triangle inequality:
∥x + y∥≤∥x∥+ ∥y∥.
(If property 1 is relaxed to require only ∥x∥≥0 for x ̸= 0, the function is
called a seminorm.) Because a norm is a function whose argument is a vector,
we also often use a functional notation such as ρ(x) to represent a norm of
the vector x.
Sets of various types of objects (functions, for example) can have norms,
but our interest in the present context is in norms for vectors and (later)
for matrices. (The three properties above in fact deﬁne a more general norm
for other kinds of mathematical objects for which an addition, an additive
identity, and multiplication by a scalar are deﬁned. Norms are deﬁned for
matrices, as we will discuss later. Note that there are two diﬀerent kinds of
multiplication used in property 2 and two diﬀerent kinds of addition used in
property 3.)
A vector space together with a norm is called a normed space.
For some types of objects, a norm of an object may be called its “length”
or its “size”. (Recall the ambiguity of “length” of a vector that we mentioned
at the beginning of this chapter.)

26
2 Vectors and Vector Spaces
2.1.5.1 Convexity
A function f(·) over a convex domain S into a range R, where both S and
R have an addition-type operator, +, corresponding additive identities, and
scalar multiplication, is said to be convex, if, for any x and y in S, and a such
that 0 ≤a ≤1,
f(ax + (1 −a)y) ≤af(x) + (1 −a)f(y).
(2.27)
If, for x ̸= y and a such that 0 < a < 1, the inequality in (2.27) is sharp, then
the function is said to be strictly convex.
It is clear from the triangle inequality that a norm is convex.
2.1.5.2 Norms Induced by Inner Products
There is a close relationship between a norm and an inner product. For any
inner product space with inner product ⟨·, ·⟩, a norm of an element of the
space can be deﬁned in terms of the square root of the inner product of the
element with itself:
∥x∥=

⟨x, x⟩.
(2.28)
Any function ∥· ∥deﬁned in this way satisﬁes the properties of a norm. It is
easy to see that ∥x∥satisﬁes the ﬁrst two properties of a norm, nonnegativity
and scalar equivariance. Now, consider the square of the right-hand side of
the triangle inequality, ∥x∥+ ∥y∥:
(∥x∥+ ∥y∥)2 = ⟨x, x⟩+ 2

⟨x, x⟩⟨y, y⟩+ ⟨y, y⟩
≥⟨x, x⟩+ 2⟨x, y⟩+ ⟨y, y⟩
= ⟨x + y, x + y⟩
= ∥x + y∥2;
(2.29)
hence, the triangle inequality holds. Therefore, given an inner product, ⟨x, y⟩,
then

⟨x, x⟩is a norm.
Equation (2.28) deﬁnes a norm given any inner product. It is called the
norm induced by the inner product.
Norms induced by inner products have some interesting properties. First of
all, they have the Cauchy-Schwarz relationship (inequality (2.26)) with their
associated inner product:
|⟨x, y⟩| ≤∥x∥∥y∥.
(2.30)
In the sequence of equations above for an induced norm of the sum of two
vectors, one equation (expressed diﬀerently) stands out as particularly useful
in later applications:
∥x + y∥2 = ∥x∥2 + ∥y∥2 + 2⟨x, y⟩.
(2.31)

2.1 Operations on Vectors
27
If ⟨x, y⟩= 0 (that is, the vectors are orthogonal), equation (2.31) becomes
the Pythagorean theorem:
∥x + y∥2 = ∥x∥2 + ∥y∥2.
Another useful property of a norm induced by an inner product is the
parallelogram equality:
2∥x∥2 + 2∥y∥2 = ∥x + y∥2 + ∥x −y∥2.
(2.32)
This is trivial to show, and you are asked to do so in Exercise 2.7. (It is also
the case that if the parallelogram equality holds for every pair of vectors in
the space, then the norm is necessarily induced by an inner product. This fact
is both harder to show and less useful than its converse; I state it only because
it is somewhat surprising.)
A vector space whose norm is induced by an inner product has an inter-
esting structure; for example, the geometric properties such as projections,
orthogonality, and angles between vectors that we discuss in Sect. 2.2 are
deﬁned in terms of inner products and the associated norm.
2.1.5.3 Lp Norms
There are many norms that could be deﬁned for vectors. One type of norm is
called an Lp norm, often denoted as ∥· ∥p. For p ≥1, it is deﬁned as
∥x∥p =

i
|xi|p
 1
p
.
(2.33)
This is also sometimes called the Minkowski norm and also the H¨older norm.
An Lp norm is also called a p-norm, or 1-norm, 2-norm, or ∞-norm (deﬁned
by a limit) in those special cases.
It is easy to see that the Lp norm satisﬁes the ﬁrst two conditions above. For
general p ≥1 it is somewhat more diﬃcult to prove the triangular inequality
(which for the Lp norms is also called the Minkowski inequality), but for some
special cases it is straightforward, as we will see below.
The most common Lp norms, and in fact the most commonly used vector
norms, are:
•
∥x∥1 = 
i |xi|, also called the Manhattan norm because it corresponds
to sums of distances along coordinate axes, as one would travel along the
rectangular street plan of Manhattan (except for Broadway and a few
other streets and avenues).
•
∥x∥2 =

i x2
i , also called the Euclidean norm, the Euclidean length,
or just the length of the vector.
The L2 norm is induced by an inner
product; it is the square root of the inner product of the vector with itself:
∥x∥2 =

⟨x, x⟩. It is the only Lp norm induced by an inner product. (See
Exercise 2.9.)

28
2 Vectors and Vector Spaces
•
∥x∥∞= maxi |xi|, also called the max norm or the Chebyshev norm. The
L∞norm is deﬁned by taking the limit in an Lp norm, and we see that it
is indeed maxi |xi| by expressing it as
∥x∥∞= lim
p→∞∥x∥p = lim
p→∞

i
|xi|p
 1
p
= m lim
p→∞

i
xi
m

p
 1
p
with m = maxi |xi|. Because the quantity of which we are taking the pth
root is bounded above by the number of elements in x and below by 1,
that factor goes to 1 as p goes to ∞.
It is easy to see that, for any n-vector x, the Lp norms have the relation-
ships
∥x∥∞≤∥x∥2 ≤∥x∥1.
(2.34)
More generally, for given x and for p ≥1, we see that ∥x∥p is a nonincreasing
function of p.
We also have bounds that involve the number of elements in the vector:
∥x∥∞≤∥x∥2 ≤√n∥x∥∞,
(2.35)
and
∥x∥2 ≤∥x∥1 ≤√n∥x∥2.
(2.36)
The triangle inequality obviously holds for the L1 and L∞norms. For the
L2 norm it can be seen by expanding (xi + yi)2 and then using the Cauchy-
Schwarz inequality (2.26) on page 24. Rather than approaching it that way,
however, we will show below that the L2 norm can be deﬁned in terms of an
inner product, and then we will establish the triangle inequality for any norm
deﬁned similarly by an inner product; see inequality (2.29). Showing that the
triangle inequality holds for other Lp norms is more diﬃcult; see Exercise 2.11.
A generalization of the Lp vector norm is the weighted Lp vector norm
deﬁned by
∥x∥wp =

i
wi|xi|p
 1
p
,
(2.37)
where wi ≥0 and 
i wi = 1.
In the following, if we use the unqualiﬁed symbol ∥· ∥for a vector norm
and do not state otherwise, we will mean the L2 norm; that is, the Euclidean
norm, the induced norm.
2.1.5.4 Basis Norms
If {v1, . . . , vk} is a basis for a vector space that includes a vector x with
x = c1v1 + · · · + ckvk, then

2.1 Operations on Vectors
29
ρ(x) =

i
c2
i
 1
2
(2.38)
is a norm. It is straightforward to see that ρ(x) is a norm by checking the
following three conditions:
•
ρ(x) ≥0 and ρ(x) = 0 if and only if x = 0 because x = 0 if and only if
ci = 0 for all i.
•
ρ(ax) =

i a2c2
i
 1
2 = |a|

i c2
i
 1
2 = |a|ρ(x).
•
If also y = b1v1 + · · · + bkvk, then
ρ(x + y) =

i
(ci + bi)2
 1
2
≤

i
c2
i
 1
2
+

i
b2
i
 1
2
= ρ(x) + ρ(y).
The last inequality is just the triangle inequality for the L2 norm for the
vectors (c1, · · · , ck) and (b1, · · · , bk).
In Sect. 2.2.5, we will consider special forms of basis sets in which the
norm in equation (2.38) is identically the L2 norm. (This is called Parseval’s
identity, equation (2.60) on page 41.)
2.1.5.5 Equivalence of Norms
There is an equivalence among any two norms over a normed ﬁnite-
dimensional linear space in the sense that if ∥· ∥a and ∥· ∥b are norms,
then there are positive numbers r and s such that for any x in the space,
r∥x∥b ≤∥x∥a ≤s∥x∥b.
(2.39)
Expressions (2.35) and (2.36) are examples of this general equivalence for
three Lp norms.
We can prove inequality (2.39) by using the norm deﬁned in equa-
tion (2.38). We need only consider the case x ̸= 0, because the inequality
is obviously true if x = 0. Let ∥· ∥a be any norm over a given normed linear
space and let {v1, . . . , vk} be a basis for the space. (Here’s where the assump-
tion of a vector space with ﬁnite dimensions comes in.) Any x in the space
has a representation in terms of the basis, x = c1v1 + · · · + ckvk. Then
∥x∥a =

k

i=1
civi

a
≤
k

i=1
|ci| ∥vi∥a.
Applying the Cauchy-Schwarz inequality to the two vectors (c1, · · · , ck) and
(∥v1∥a, · · · , ∥vk∥a), we have
k

i=1
|ci| ∥vi∥a ≤
 k

i=1
c2
i
 1
2  k

i=1
∥vi∥2
a
 1
2
.

30
2 Vectors and Vector Spaces
Hence, with ˜s = (
i ∥vi∥2
a)
1
2 , which must be positive, we have
∥x∥a ≤˜sρ(x).
Now, to establish a lower bound for ∥x∥a, let us deﬁne a subset C of the
linear space consisting of all vectors (u1, . . . , uk) such that  |ui|2 = 1. This
set is obviously closed. Next, we deﬁne a function f(·) over this closed subset
by
f(u) =

k

i=1
uivi

a
.
Because f is continuous, it attains a minimum in this closed subset, say for
the vector u∗; that is, f(u∗) ≤f(u) for any u such that  |ui|2 = 1. Let
˜r = f(u∗),
which must be positive, and again consider any x in the normed linear space
and express it in terms of the basis, x = c1v1 + · · · ckvk. If x ̸= 0, we have
∥x∥a =

k

i=1
civi

a
=
 k

i=1
c2
i
 1
2

k

i=1
⎛
⎜
⎝
ci
k
i=1 c2
i
 1
2
⎞
⎟
⎠vi

a
= ρ(x)f(˜c),
where ˜c = (c1, · · · , ck)/(k
i=1 c2
i )1/2. Because ˜c is in the set C, f(˜c) ≥r;
hence, combining this with the inequality above, we have
˜rρ(x) ≤∥x∥a ≤˜sρ(x).
This expression holds for any norm ∥·∥a and so, after obtaining similar bounds
for any other norm ∥·∥b and then combining the inequalities for ∥·∥a and ∥·∥b,
we have the bounds in the equivalence relation (2.39). (This is an equivalence
relation because it is reﬂexive, symmetric, and transitive. Its transitivity is
seen by the same argument that allowed us to go from the inequalities involv-
ing ρ(·) to ones involving ∥· ∥b.)
As we have mentioned, there are some diﬀerences in the properties of
vector spaces that have an inﬁnite number of dimensions and those with ﬁnite
dimensions. The equivalence of norms is one of those diﬀerences. The argument
above fails in the properties of the continuous function f. (Recall, however,
as we have mentioned, unless we state otherwise, we assume that the vector
spaces we discuss have ﬁnite dimensions.)

2.1 Operations on Vectors
31
2.1.6 Normalized Vectors
The Euclidean norm of a vector corresponds to the length of the vector x in a
natural way; that is, it agrees with our intuition regarding “length”. Although,
as we have seen, this is just one of many vector norms, in most applications
it is the most useful one. (I must warn you, however, that occasionally I will
carelessly but naturally use “length” to refer to the order of a vector; that is,
the number of elements. This usage is common in computer software packages
such as R and SAS IML, and software necessarily shapes our vocabulary.)
Dividing a given vector by its length normalizes the vector, and the re-
sulting vector with length 1 is said to be normalized; thus
˜x =
1
∥x∥x
(2.40)
is a normalized vector. Normalized vectors are sometimes referred to as “unit
vectors”, although we will generally reserve this term for a special kind of nor-
malized vector (see page 16). A normalized vector is also sometimes referred
to as a “normal vector”. I use “normalized vector” for a vector such as ˜x in
equation (2.40) and use “normal vector” to denote a vector that is orthogonal
to a subspace (as on page 34).
2.1.6.1 “Inverse” of a Vector
Because the mapping of an inner product takes the elements of one space into
a diﬀerent space (the inner product of vectors takes elements of IRn into IR),
the concept of an inverse for the inner product does not make sense in the
usual way. First of all, there is no identity with respect to the inner product.
Often in applications, however, inner products are combined with the usual
scalar-vector multiplication in the form ⟨x, y⟩z; therefore, for given x, it may
be of interest to determine y such that ⟨x, y⟩= 1, the multiplicative identity
in IR. For x in IRn such that x ̸= 0, the additive identity in IRn,
y =
1
∥x∥2 x =
1
∥x∥˜x
(2.41)
uniquely satisﬁes ⟨x, y⟩= 1. Such a y is called the Samelson inverse of the
vector x and is sometimes denoted as x−1 or as [x]−1. It is also sometimes
called the Moore-Penrose vector inverse because it satisﬁes the four properties
of the deﬁnition the Moore-Penrose inverse. (See page 128, where, for example,
the ﬁrst property is interpreted both as ⟨x, [x]−1⟩x and as x⟨[x]−1, x⟩.)
The norm in equation (2.41) is obviously the Euclidean norm (because of
the way we deﬁned ˜x), but the idea of the inverse could also be extended to
other norms associated with other inner products.
The Samelson inverse has a nice geometric interpretation: it is the inverse
point of x with respect to the unit sphere in IRn. This inverse arises in the
vector ϵ-algorithm used in accelerating convergence of vector sequences in
numerical computations (see Wynn 1962).

32
2 Vectors and Vector Spaces
2.1.7 Metrics and Distances
It is often useful to consider how far apart two objects are; that is, the “dis-
tance” between them. A reasonable distance measure would have to satisfy
certain requirements, such as being a nonnegative real number.
A function Δ that maps any two objects in a set S to IR is called a metric
on S if, for all x, y, and z in S, it satisﬁes the following three conditions:
1. Δ(x, y) > 0 if x ̸= y and Δ(x, y) = 0 if x = y;
2. Δ(x, y) = Δ(y, x);
3. Δ(x, y) ≤Δ(x, z) + Δ(z, y).
These conditions correspond in an intuitive manner to the properties we ex-
pect of a distance between objects.
A vector space together with a metric deﬁned on it is called a metric
space. A normed vector space is a metric space because the norm can induce
a metric. In the following, we may speak almost interchangeably of an inner
product space, a normed space, or a metric space, but we must recognize that
none is a special case of another. (Recall that a normed space whose norm is
the L1 norm is not equivalent to an inner product space, for example.)
2.1.7.1 Metrics Induced by Norms
If subtraction and a norm are deﬁned for the elements of S, the most common
way of forming a metric is by using the norm. If ∥· ∥is a norm, we can verify
that
Δ(x, y) = ∥x −y∥
(2.42)
is a metric by using the properties of a norm to establish the three properties
of a metric above (Exercise 2.12).
The norm in equation (2.42) may, of course, be induced by an inner prod-
uct.
The general inner products, norms, and metrics deﬁned above are relevant
in a wide range of applications. The sets on which they are deﬁned can consist
of various types of objects. In the context of real vectors, the most common
inner product is the dot product; the most common norm is the Euclidean
norm that arises from the dot product; and the most common metric is the
one deﬁned by the Euclidean norm, called the Euclidean distance.
2.1.7.2 Convergence of Sequences of Vectors
A sequence of real numbers a1, a2, . . . is said to converge to a ﬁnite number a
if for any given ϵ > 0 there is an integer M such that, for k > M, |ak −a| < ϵ,
and we write limk→∞ak = a, or we write ak →a as k →∞.
We deﬁne convergence of a sequence of vectors in a normed vector space in
terms of the convergence of a sequence of their norms, which is a sequence of

2.1 Operations on Vectors
33
real numbers. We say that a sequence of vectors x1, x2, . . . (of the same order)
converges to the vector x with respect to the norm ∥· ∥if the sequence of real
numbers ∥x1 −x∥, ∥x2 −x∥, . . . converges to 0. Because of the bounds (2.39),
the choice of the norm is irrelevant (for ﬁnite dimensional vector spaces), and
so convergence of a sequence of vectors is well-deﬁned without reference to a
speciﬁc norm. (This is one reason that equivalence of norms is an important
property.)
A sequence of vectors x1, x2, . . . in the metric space V that come arbitrarily
close to one another (as measured by the given metric) is called a Cauchy
sequence. (In a Cauchy sequence x1, x2, . . ., for any ϵ > 0 there is a number
N such that for i, j > N, Δ(xi, xj) < ϵ.) Intuitively, such a sequence should
converge to some ﬁxed vector in V, but this is not necessarily the case. A
metric space in which every Cauchy sequence converges to an element in the
space is said to be a complete metric space or just a complete space. The space
IRn (with any norm) is complete.
A complete normed space is called a Banach space, and a complete inner
product space is called a Hilbert space. It is clear that a Hilbert space is a
Banach space (because its inner product induces a norm). As we have indi-
cated, a space with a norm induced by an inner product, such as a Hilbert
space, has an interesting structure. Most of the vector spaces encountered in
statistical applications are Hilbert spaces. The space IRn with the L2 norm is
a Hilbert space.
2.1.8 Orthogonal Vectors and Orthogonal Vector Spaces
Two vectors v1 and v2 such that
⟨v1, v2⟩= 0
(2.43)
are said to be orthogonal, and this condition is denoted by v1 ⊥v2. (Sometimes
we exclude the zero vector from this deﬁnition, but it is not important to
do so.) Normalized vectors that are all orthogonal to each other are called
orthonormal vectors.
An Aside: Complex Vectors
If the elements of the vectors are from the ﬁeld of complex num-
bers, orthogonality and normality are also deﬁned as above; however,
the inner product in the deﬁnition (2.43) must be as deﬁned in equa-
tion (2.23), and the expression xTy in equation (2.25) is not equivalent
to the inner product. We will use a diﬀerent notation in this case: xHy.
The relationship between the two notations is
xHy = ¯xTy.
With this interpretation of the inner product, all of the statements
below about orthogonality hold for complex numbers as well as for
real numbers.

34
2 Vectors and Vector Spaces
A set of nonzero vectors that are mutually orthogonal are necessarily lin-
early independent. To see this, we show it for any two orthogonal vectors and
then indicate the pattern that extends to three or more vectors. First, sup-
pose v1 and v2 are nonzero and are orthogonal; that is, ⟨v1, v2⟩= 0. We see
immediately that if there is a scalar a such that v1 = av2, then a must be
nonzero and we have a contradiction because ⟨v1, v2⟩= a⟨v2, v2⟩̸= 0. Hence,
we conclude v1 and v2 are independent (there is no a such that v1 = av2). For
three mutually orthogonal vectors, v1, v2, and v3, we consider v1 = av2 + bv3
for a or b nonzero, and arrive at the same contradiction.
Two vector spaces V1 and V2 are said to be orthogonal, written V1 ⊥V2,
if each vector in one is orthogonal to every vector in the other. If V1 ⊥V2 and
V1 ⊕V2 = IRn, then V2 is called the orthogonal complement of V1, and this is
written as V2 = V⊥
1 . More generally, if V1 ⊥V2 and V1 ⊕V2 = V, then V2 is
called the orthogonal complement of V1 with respect to V. This is obviously
a symmetric relationship; if V2 is the orthogonal complement of V1, then V1
is the orthogonal complement of V2.
A vector that is orthogonal to all vectors in a given vector space is said to
be orthogonal to that space or normal to that space. Such a vector is called a
normal vector to that space.
If B1 is a basis set for V1, B2 is a basis set for V2, and V2 is the orthogonal
complement of V1 with respect to V, then B1 ∪B2 is a basis set for V. It is
a basis set because since V1 and V2 are orthogonal, it must be the case that
B1 ∩B2 = ∅. (See the properties listed on page 22.)
If V1 ⊂V, V2 ⊂V, V1 ⊥V2, and dim(V1) + dim(V2) = dim(V), then
V1 ⊕V2 = V;
(2.44)
that is, V2 is the orthogonal complement of V1. We see this by ﬁrst letting B1
and B2 be bases for V1 and V2. Now V1 ⊥V2 implies that B1 ∩B2 = ∅and
dim(V1) + dim(V2) = dim(V) implies #(B1) + #(B2) = #(B), for any basis
set B for V; hence, B1 ∪B2 is a basis set for V.
The intersection of two orthogonal vector spaces consists only of the zero
vector (Exercise 2.14).
A set of linearly independent vectors can be mapped to a set of mutu-
ally orthogonal (and orthonormal) vectors by means of the Gram-Schmidt
transformations (see equation (2.56) below).
2.1.9 The “One Vector”
The vector with all elements equal to 1 that we mentioned previously is useful
in various vector operations. We call this the “one vector” and denote it by 1
or by 1n. The one vector can be used in the representation of the sum of the
elements in a vector:
1Tx =

xi.
(2.45)
The one vector is also called the “summing vector”.

2.2 Cartesian Geometry
35
2.1.9.1 The Mean and the Mean Vector
Because the elements of x are real, they can be summed; however, in applica-
tions it may or may not make sense to add the elements in a vector, depending
on what is represented by those elements. If the elements have some kind of
essential commonality, it may make sense to compute their sum as well as
their arithmetic mean, which for the n-vector x is denoted by ¯x and deﬁned
by
¯x = 1T
nx/n.
(2.46)
We also refer to the arithmetic mean as just the “mean” because it is the most
commonly used mean.
It is often useful to think of the mean as an n-vector all of whose elements
are ¯x. The symbol ¯x is also used to denote this vector; hence, we have
¯x = ¯x1n,
(2.47)
in which ¯x on the left-hand side is a vector and ¯x on the right-hand side is a
scalar. We also have, for the two diﬀerent objects,
∥¯x∥2 = n¯x2.
(2.48)
The meaning, whether a scalar or a vector, is usually clear from the con-
text. In any event, an expression such as x −¯x is unambiguous; the addition
(subtraction) has the same meaning whether ¯x is interpreted as a vector or a
scalar. (In some mathematical treatments of vectors, addition of a scalar to
a vector is not deﬁned, but here we are following the conventions of modern
computer languages.)
2.2 Cartesian Coordinates and Geometrical
Properties of Vectors
Points in a Cartesian geometry can be identiﬁed with vectors, and several
deﬁnitions and properties of vectors can be motivated by this geometric in-
terpretation. In this interpretation, vectors are directed line segments with a
common origin. The geometrical properties can be seen most easily in terms
of a Cartesian coordinate system, but the properties of vectors deﬁned in
terms of a Cartesian geometry have analogues in Euclidean geometry without
a coordinate system. In such a system, only length and direction are deﬁned,
and two vectors are considered to be the same vector if they have the same
length and direction. Generally, we will not assume that there is a “location”
or “position” associated with a vector.

36
2 Vectors and Vector Spaces
2.2.1 Cartesian Geometry
A Cartesian coordinate system in d dimensions is deﬁned by d unit vectors,
ei in equation (2.6), each with d elements. A unit vector is also called a
principal axis of the coordinate system. The set of unit vectors is orthonormal.
(There is an implied number of elements of a unit vector that is inferred from
the context. Also parenthetically, we remark that the phrase “unit vector” is
sometimes used to refer to a vector the sum of whose squared elements is 1,
that is, whose length, in the Euclidean distance sense, is 1. As we mentioned
above, we refer to this latter type of vector as a “normalized vector”.)
The sum of all of the unit vectors is the one vector:
d

i=1
ei = 1d.
(2.49)
A point x with Cartesian coordinates (x1, . . . , xd) is associated with a
vector from the origin to the point, that is, the vector (x1, . . . , xd). The vector
can be written as the linear combination
x = x1e1 + . . . + xded
(2.50)
or, equivalently, as
x = ⟨x, e1⟩e1 + . . . + ⟨x, ed⟩ed.
(This is a Fourier expansion, equation (2.58) below.)
2.2.2 Projections
The projection of the vector y onto the nonnull vector x is the vector
ˆy = ⟨x, y⟩
∥x∥2 x.
(2.51)
This deﬁnition is consistent with a geometrical interpretation of vectors as
directed line segments with a common origin. The projection of y onto x is
the inner product of the normalized x and y times the normalized x; that is,
⟨˜x, y⟩˜x, where ˜x = x/∥x∥. Notice that the order of y and x is the same.
An important property of a projection is that when it is subtracted from
the vector that was projected, the resulting vector, called the “residual”, is
orthogonal to the projection; that is, if
r = y −⟨x, y⟩
∥x∥2 x
= y −ˆy
(2.52)
then r and ˆy are orthogonal, as we can easily see by taking their inner product
(see Fig. 2.2.2). Notice also that the Pythagorean relationship holds:

2.2 Cartesian Geometry
37
y
x
q
r
y^
y
x
q
r
y^
Figure 2.1. Projections and angles
∥y∥2 = ∥ˆy∥2 + ∥r∥2.
(2.53)
As we mentioned on page 35, the mean ¯y can be interpreted either as a
scalar or as a vector all of whose elements are ¯y. As a vector, it is the projection
of y onto the one vector 1n,
⟨1n, y⟩
∥1n∥2 1n = 1T
ny
n
1n
= ¯y 1n,
from equations (2.46) and (2.51).
We will consider more general projections (that is, projections onto planes
or other subspaces) on page 352, and on page 409 we will view linear regression
ﬁtting as a projection onto the space spanned by the independent variables.
2.2.3 Angles Between Vectors
The angle between the nonnull vectors x and y is determined by its cosine,
which we can compute from the length of the projection of one vector onto
the other. Hence, denoting the angle between the nonnull vectors x and y as
angle(x, y), we deﬁne
angle(x, y) = cos−1
 ⟨x, y⟩
∥x∥∥y∥

,
(2.54)
with cos−1(·) being taken in the interval [0, π]. The cosine is ±∥ˆy∥/∥y∥, with
the sign chosen appropriately; see Fig. 2.2.2. Because of this choice of cos−1(·),
we have that angle(y, x) = angle(x, y)—but see Exercise 2.19e on page 54.
The word “orthogonal” is appropriately deﬁned by equation (2.43) on
page 33 because orthogonality in that sense is equivalent to the corresponding
geometric property. (The cosine is 0.)

38
2 Vectors and Vector Spaces
Notice that the angle between two vectors is invariant to scaling of the
vectors; that is, for any positive scalar a, angle(ax, y) = angle(x, y).
A given vector can be deﬁned in terms of its length and the angles θi that
it makes with the unit vectors. The cosines of these angles are just the scaled
coordinates of the vector:
cos(θi) =
⟨x, ei⟩
∥x∥∥ei∥
=
1
∥x∥xi.
(2.55)
These quantities are called the direction cosines of the vector.
Although geometrical intuition often helps us in understanding properties
of vectors, sometimes it may lead us astray in high dimensions. Consider the
direction cosines of an arbitrary vector in a vector space with large dimensions.
If the elements of the arbitrary vector are nearly equal (that is, if the vector is
a diagonal through an orthant of the coordinate system), the direction cosine
goes to 0 as the dimension increases. In high dimensions, any two vectors are
“almost orthogonal” to each other; see Exercise 2.16.
The geometric property of the angle between vectors has important im-
plications for certain operations both because it may indicate that rounding
in computations will have deleterious eﬀects and because it may indicate a
deﬁciency in the understanding of the application.
We will consider more general projections and angles between vectors and
other subspaces on page 359. In Sect. 5.3.3, we will consider rotations of
vectors onto other vectors or subspaces. Rotations are similar to projections,
except that the length of the vector being rotated is preserved.
2.2.4 Orthogonalization Transformations: Gram-Schmidt
Given m nonnull, linearly independent vectors, x1, . . . , xm, it is easy to form
m orthonormal vectors, ˜x1, . . . , ˜xm, that span the same space. A simple way
to do this is sequentially. First normalize x1 and call this ˜x1. Next, project x2
onto ˜x1 and subtract this projection from x2. The result is orthogonal to ˜x1;
hence, normalize this and call it ˜x2. These ﬁrst two steps, which are illustrated
in Fig. 2.2.4, are
˜x1 =
1
∥x1∥x1,
˜x2 =
1
∥x2 −⟨˜x1, x2⟩˜x1∥(x2 −⟨˜x1, x2⟩˜x1).
(2.56)
These are called Gram-Schmidt transformations.
The Gram-Schmidt transformations have a close relationship to least
squares ﬁtting of overdetermined systems of linear equations and to least

2.2 Cartesian Geometry
39
squares ﬁtting of linear regression models. For example, if equation (6.33) on
page 290 is merely the system of equations in one unknown x1b = x2 −r, and
it is approximated by least squares, then the “residual vector” r is ˜x2 above,
and of course it has the orthogonality property shown in equation (6.37) for
that problem.
The Gram-Schmidt transformations can be continued with all of the vec-
tors in the linearly independent set. There are two straightforward ways equa-
tions (2.56) can be extended. One method generalizes the second equation in
an obvious way:
for k = 2, 3 . . .,
˜xk =

xk −
k−1

i=1
⟨˜xi, xk⟩˜xi
  xk −
k−1

i=1
⟨˜xi, xk⟩˜xi
 .
(2.57)
In this method, at the kth step, we orthogonalize the kth vector by comput-
ing its residual with respect to the plane formed by all the previous k −1
orthonormal vectors.
Another way of extending the transformation of equations (2.56) is, at the
kth step, to compute the residuals of all remaining vectors with respect just to
the kth normalized vector. If the initial set of vectors are linearly independent,
the residuals at any stage will be nonzero. (This is fairly obvious, but you are
asked to show it in Exercise 2.17.) We describe this method explicitly in
Algorithm 2.1.
Algorithm 2.1 Gram-Schmidt orthonormalization of a set of
linearly independent vectors, x1, . . . , xm
x2
x1
x~
1
p
projection onto
x~
1
x2 −p
x~
2
Figure 2.2. Orthogonalization of x1 and x2

40
2 Vectors and Vector Spaces
0. For k = 1, . . . , m,
{
set ˜xk = xk.
}
1. Ensure that ˜x1 ̸= 0;
set ˜x1 = ˜x1/∥˜x1∥.
2. If m > 1, for k = 2, . . . , m,
{
for j = k, . . . , m,
{
set ˜xj = ˜xj −⟨˜xk−1, ˜xj⟩˜xk−1.
}
ensure that ˜xk ̸= 0;
set ˜xk = ˜xk/∥˜xk∥.
}
Although the method indicated in equation (2.57) is mathematically equiv-
alent to this method, the use of Algorithm 2.1 is to be preferred for compu-
tations because it is less subject to rounding errors. (This may not be im-
mediately obvious, although a simple numerical example can illustrate the
fact—see Exercise 11.1c on page 537. We will not digress here to consider this
further, but the diﬀerence in the two methods has to do with the relative mag-
nitudes of the quantities in the subtraction. The method of Algorithm 2.1 is
sometimes called the “modiﬁed Gram-Schmidt method”, although I call it the
“Gram-Schmidt method”. I will discuss this method again in Sect. 11.2.1.3.)
This is an instance of a principle that we will encounter repeatedly: the form
of a mathematical expression and the way the expression should be evaluated
in actual practice may be quite diﬀerent.
These orthogonalizing transformations result in a set of orthogonal vectors
that span the same space as the original set. They are not unique; if the order
in which the vectors are processed is changed, a diﬀerent set of orthogonal
vectors will result.
Orthogonal vectors are useful for many reasons: perhaps to improve the
stability of computations; or in data analysis to capture the variability most
eﬃciently; or for dimension reduction as in principal components analysis
(see Sect. 9.4 beginning on page 424); or in order to form more meaningful
quantities as in a vegetative index in remote sensing. We will discuss various
speciﬁc orthogonalizing transformations later.
2.2.5 Orthonormal Basis Sets
A basis for a vector space is often chosen to be an orthonormal set because it
is easy to work with the vectors in such a set.
If u1, . . . , un is an orthonormal basis set for a space, then a vector x in
that space can be expressed as

2.2 Cartesian Geometry
41
x = c1u1 + · · · + cnun,
(2.58)
and because of orthonormality, we have
ci = ⟨x, ui⟩.
(2.59)
(We see this by taking the inner product of both sides with ui.) A represen-
tation of a vector as a linear combination of orthonormal basis vectors, as in
equation (2.58), is called a Fourier expansion, and the ci are called Fourier
coeﬃcients.
By taking the inner product of each side of equation (2.58) with itself, we
have Parseval’s identity:
∥x∥2 =

c2
i .
(2.60)
This shows that the L2 norm is the same as the norm in equation (2.38) (on
page 29) for the case of an orthogonal basis.
Although the Fourier expansion is not unique because a diﬀerent orthog-
onal basis set could be chosen, Parseval’s identity removes some of the arbi-
trariness in the choice; no matter what basis is used, the sum of the squares of
the Fourier coeﬃcients is equal to the square of the norm that arises from the
inner product. (“The” inner product means the inner product used in deﬁning
the orthogonality.)
Another useful expression of Parseval’s identity in the Fourier expansion
is
x −
k

i=1
ciui

2
= ⟨x, x⟩−
k

i=1
c2
i
(2.61)
(because the term on the left-hand side is 0).
The expansion (2.58) is a special case of a very useful expansion in an
orthogonal basis set. In the ﬁnite-dimensional vector spaces we consider here,
the series is ﬁnite. In function spaces, the series is generally inﬁnite, and so
issues of convergence are important. For diﬀerent types of functions, diﬀerent
orthogonal basis sets may be appropriate. Polynomials are often used, and
there are some standard sets of orthogonal polynomials, such as Jacobi, Her-
mite, and so on. For periodic functions especially, orthogonal trigonometric
functions are useful.
2.2.6 Approximation of Vectors
In high-dimensional vector spaces, it is often useful to approximate a given
vector in terms of vectors from a lower dimensional space. Suppose, for exam-
ple, that V ⊆IRn is a vector space of dimension k (necessarily, k ≤n) and x
is a given n-vector, not necessarily in V. We wish to determine a vector ˜x in
V that approximates x. Of course if V = IRn, then x ∈V, and so the problem
is not very interesting. The interesting case is when V ⊂IRn.

42
2 Vectors and Vector Spaces
2.2.6.1 Optimality of the Fourier Coeﬃcients
The ﬁrst question, of course, is what constitutes a “good” approximation. One
obvious criterion would be based on a norm of the diﬀerence of the given vector
and the approximating vector. So now, choosing the norm as the Euclidean
norm, we may pose the problem as one of ﬁnding ˜x ∈V such that
∥x −˜x∥≤∥x −v∥
∀v ∈V.
(2.62)
This diﬀerence is a truncation error.
Let u1, . . . , uk be an orthonormal basis set for V, and let
˜x = c1u1 + · · · + ckuk,
(2.63)
where the ci are the Fourier coeﬃcients of x, ⟨x, ui⟩.
Now let v = a1u1 + · · · + akuk be any other vector in V, and consider
∥x −v∥2 =
x −
k

i=1
aiui

2
=

x −
k

i=1
aiui, x −
k

i=1
aiui

= ⟨x, x⟩−2
k

i=1
ai⟨x, ui⟩+
k

i=1
a2
i
= ⟨x, x⟩−2
k

i=1
aici +
k

i=1
a2
i +
k

i=1
c2
i −
k

i=1
c2
i
= ⟨x, x⟩+
k

i=1
(ai −ci)2 −
k

i=1
c2
i
=
x −
k

i=1
ciui

2
+
k

i=1
(ai −ci)2
≥
x −
k

i=1
ciui

2
.
(2.64)
Therefore we have ∥x−˜x∥≤∥x−v∥, and so ˜x formed by the Fourier coeﬃcients
is the best approximation of x with respect to the Euclidean norm in the k-
dimensional vector space V. (For some other norm, this may not be the case.)
2.2.6.2 Choice of the Best Basis Subset
Now, posing the problem another way, we may seek the best k-dimensional
subspace of IRn from which to choose an approximating vector. This question

2.2 Cartesian Geometry
43
is not well-posed (because the one-dimensional vector space determined by x
is the solution), but we can pose a related interesting question: suppose we
have a Fourier expansion of x in terms of a set of n orthogonal basis vectors,
u1, . . . , un, and we want to choose the “best” k basis vectors from this set and
use them to form an approximation of x. (This restriction of the problem is
equivalent to choosing a coordinate system.) We see the solution immediately
from inequality (2.64): we choose the k uis corresponding to the k largest cis
in absolute value, and we take
˜x = ci1ui1 + · · · + cikuik,
(2.65)
where min({|cij| : j = 1, . . . , k}) ≥max({|cij| : j = k + 1, . . . , n}).
2.2.7 Flats, Aﬃne Spaces, and Hyperplanes
Given an n-dimensional vector space of order n, IRn for example, consider a
system of m linear equations in the n-vector variable x,
cT
1 x = b1
...
...
cT
mx = bm,
where c1, . . . , cm are linearly independent n-vectors (and hence m ≤n). The
set of points deﬁned by these linear equations is called a ﬂat. Although it is not
necessarily a vector space, a ﬂat is also called an aﬃne space. An intersection
of two ﬂats is a ﬂat.
If the equations are homogeneous (that is, if b1 = · · · = bm = 0), then the
point (0, . . . , 0) is included, and the ﬂat is an (n −m)-dimensional subspace
(also a vector space, of course). Stating this another way, a ﬂat through the
origin is a vector space, but other ﬂats are not vector spaces.
If m = 1, the ﬂat is called a hyperplane. A hyperplane through the origin
is an (n −1)-dimensional vector space.
If m = n−1, the ﬂat is a line. A line through the origin is a one-dimensional
vector space.
2.2.8 Cones
A set of vectors that contains all nonnegative scalar multiples of any vector
in the set is called a cone. A cone always contains the zero vector. (Some
authors deﬁne a cone as a set that contains all positive scalar multiples, and
in that case, the zero vector may not be included.) If a set of vectors contains
all scalar multiples of any vector in the set, it is called a double cone.
Geometrically, a cone is just a set, possibly a ﬁnite set, of lines or half-
lines. (A double cone is a set of lines.) In general, a cone may not be very
interesting, but certain special cones are of considerable interest.

44
2 Vectors and Vector Spaces
Given two (double) cones over the same vector space, both their union
and their intersection are (double) cones. A (double) cone is in general not a
vector space.
2.2.8.1 Convex Cones
A set of vectors C in a vector space V is a convex cone if, for all v1, v2 ∈C
and all nonnegative real numbers a, b ≥0, av1 + bv2 ∈C. Such a cone is
called a homogeneous convex cone by some authors. (An equivalent deﬁnition
requires that the set C be a cone, and then, more in keeping with the deﬁnition
of convexity, includes the requirement a + b = 1 along with a, b ≥0 in the
deﬁnition of a convex cone.)
If C is a convex cone and if v ∈C implies −v ∈C, then C is called a
double convex cone.
A (double) convex cone is in general not a vector space because, for ex-
ample, v1 + v2 may not be in C.
It is clear that a (double) convex cone is a (double) cone; in fact, a convex
cone is the most important type of cone. A convex cone corresponds to a solid
geometric object with a single ﬁnite vertex.
An important convex cone in an n-dimensional vector space with a Carte-
sian coordinate system is the positive orthant together with the zero vector.
This convex cone is not closed, in the sense that it does not contain some
limits. The closure of the positive orthant (that is, the nonnegative orthant)
is also a convex cone.
A generating set or spanning set of a cone C is a set of vectors S = {vi}
such that for any vector v in C there exists scalars ai ≥0 so that v =  aivi.
If, in addition, for any scalars bi ≥0 with  bivi = 0, it is necessary that
bi = 0 for all i, then S is a basis set for the cone. The concept of a generating
set is of course more interesting in the case of a convex cone.
If a generating set of a convex cone has a ﬁnite number of elements, the
cone is a polyhedron. For the common geometric object in three dimensions
with elliptical contours and which is the basis for “conic sections”, any gen-
erating set has an uncountable number of elements. Cones of this type are
sometimes called “Lorentz cones”.
It is easy to see from the deﬁnition that if C1 and C2 are convex cones
over the same vector space, then C1 ∩C2 is a convex cone. On the other hand,
C1 ∪C2 is not necessarily a convex cone. Of course the union of two cones, as
we have seen, is a cone.
2.2.8.2 Dual Cones
Given a set of vectors S in a given vector space (in cases of interest, S is
usually a cone, but not necessarily), the dual cone of S, denoted C∗(S), is
deﬁned as
C∗(S) = {v∗s.t. ⟨v∗, v⟩≥0 for all v ∈S}.

2.2 Cartesian Geometry
45
See Fig. 2.2.8.2 in which S = {v1, v2, v3}. Clearly, the dual cone is a cone, and
also S ⊆C∗(S).
If, as in the most common cases, the underlying set of vectors is a cone,
say C, we generally drop the reference to an underlying set of vectors, and
just denote the dual cone of C as C∗.
Geometrically, the dual cone C∗of S consists of all vectors that form
nonobtuse angles with the vectors in S.
Notice that for a given set of vectors S, if −S represents the set of vec-
tors v such that −v ∈S, then C∗(−S) = −(C∗(S)), or just −C∗(S), which
represents the set of vectors v∗such that −v∗∈C∗(S).
Further, from the deﬁnition, we note that if S1 and S2 are sets of vectors in
the same vector space such that S1 ⊆S2 then C∗(S1) ⊆C∗(S2), or C∗
1 ⊆C∗
2.
A dual cone C∗(S) is a closed convex cone. We see this by considering any
v∗
1, v∗
2 ∈C∗and real numbers a, b ≥0. For any v ∈S, it must be the case that
⟨v∗
1, v⟩≥0 and ⟨v∗
2, v⟩≥0; hence, ⟨(av∗
1 +bv∗
2), v⟩≥0, that is, av∗
1 +bv∗
2 ∈C∗,
so C∗is a convex cone. The closure property comes from the ≥condition in
the deﬁnition.
x2
x1
v1
v2
v3
C
C
C
C0
Figure 2.3. A set of vectors {v1, v2, v3}, and the corresponding convex cone C, the
dual cone C∗, and the polar cone C0
2.2.8.3 Polar Cones
Given a set of vectors S in a given vector space (in cases of interest, S is
usually a cone, but not necessarily), the polar cone of S, denoted C0(S), is
deﬁned as
C0(S) = {v0 s.t. ⟨v0, v⟩≤0 for all v ∈S}.
See Fig. 2.2.8.2.
We generally drop the reference to an underlying set of vectors, and just
denote the dual cone of the set C as C0.
From the deﬁnition, we note that if S1 and S2 are sets of vectors in the
same vector space such that S1 ⊆S2 then C0(S1) ⊆C0(S2), or C0
1 ⊆C0
2.
The polar cone and the dual cone of a double cone are clearly the same.

46
2 Vectors and Vector Spaces
From the deﬁnitions, it is clear in any case that the polar cone C0 can be
formed by multiplying all of the vectors in the corresponding dual cone C∗
by −1, and so C0 = −C∗.
The relationships of the polar cone to the dual cone and the properties we
have established for a dual cone immediately imply that a polar cone is also
a convex cone.
Another interesting property of polar cones is that for any set of vectors
S in a given vector space, S ⊆(C0)0. We generally write (C0)0 as just C00.
(The precise notation of course is C0(C0(S)).) We see this by ﬁrst taking
any v ∈S. Therefore, if v0 ∈C0 then ⟨v, v0⟩≤0, which implies v ∈(C0)0,
because
C00 = {v s.t. ⟨v, v0⟩≤0 for all v0 ∈C0}.
2.2.8.4 Additional Properties
As noted above, a cone is a very loose and general structure. In my deﬁnition,
the vectors in the set do not even need to be in the same vector space. A
convex cone, on the other hand is a useful structure, and the vectors in a
convex cone must be in the same vector space.
Most cones of interest, in particular, dual cones and polar cones are not
necessarily vector spaces.
Although the deﬁnitions of dual cones and polar cones can apply to any
set of vectors, they are of the most interest in the case in which the underlying
set of vectors is a cone in the nonnegative orthant of a Cartesian coordinate
system on IRn (the set of n-vectors all of whose elements are nonnegative).
In that case, the dual cone is just the full nonnegative orthant, and the polar
cone is just the nonpositive orthant (the set of all vectors all of whose elements
are nonpositive).
The whole nonnegative orthant itself is a convex cone, and as we have seen
for any convex cone within that orthant, the dual cone is the full nonnegative
orthant.
Because the nonnegative orthant is its own dual, and hence is said to be
“self-dual”. (There is an extension of the property of self-duality that we will
not discuss here.)
Convex cones occur in many optimization problems. The feasible region
in a linear programming problem is generally a convex polyhedral cone, for
example.
2.2.9 Cross Products in IR3
The vector space IR3 is especially interesting because it serves as a useful
model of the real world, and many physical processes can be represented as
vectors in it.
For the special case of the vector space IR3, another useful vector product is
the cross product, which is a mapping from IR3×IR3 to IR3. Before proceeding,

we note an overloading of the term “cross product” and of the symbol “×”
used to denote it. If A and B are sets, the set cross product or the set Cartesian
product of A and B is the set consisting of all doubletons (a, b) where a ranges
over all elements of A, and b ranges independently over all elements of B.
Thus, IR3 × IR3 is the set of all pairs of all real 3-vectors.
The vector cross product of the 3-vectors
x = (x1, x2, x3)
and
y = (y1, y2, y3),
written x × y, is deﬁned as
x × y = (x2y3 −x3y2, x3y1 −x1y3, x1y2 −x2y1).
(2.66)
(We also use the term “cross products” in a diﬀerent way to refer to another
type of product formed by several inner products; see page 359.) The cross
product has the following properties, which are immediately obvious from the
deﬁnition:
1. Self-nilpotency:
x × x = 0, for all x.
2. Anti-commutativity:
x × y = −y × x.
3. Factoring of scalar multiplication;
ax × y = a(x × y) for real a.
4. Relation of vector addition to addition of cross products:
(x + y) × z = (x × z) + (y × z).
The cross product has the important property (sometimes taken as the
deﬁnition),
x × y = ∥x∥∥y∥sin(angle(y, x))e,
(2.67)
where e is a vector such that ∥e∥= 1 and ⟨e, x⟩= ⟨e, y⟩= 0, and angle(y, x)
is interpreted as the “smallest angle through which y would be rotated to
become a nonnegative multiple of x”. (See Exercise 2.19e on page 54.)
In the deﬁnition of angles between vectors given on page 37, angle(y, x) =
angle(x, y). As we pointed out there, sometimes it is important to distinguish
the direction of the angle, and this is the case in equation (2.67), as in many
applications in IR3. The direction of angles in IR3 often is used to determine
the orientation of the principal axes in a coordinate system. The coordinate
system is often deﬁned to be “right-handed” (see Exercise 2.19f).
The cross product is useful in modeling phenomena in nature, which natu-
rally are often represented as vectors in IR3. The cross product is also useful in
“three-dimensional” computer graphics for determining whether a given sur-
face is visible from a given perspective and for simulating the eﬀect of lighting
on a surface.
2.2 Cartesian Geometry
47

48
2 Vectors and Vector Spaces
2.3 Centered Vectors and Variances and
Covariances of Vectors
In this section, we deﬁne some scalar-valued functions of vectors that are
analogous to functions of random variables averaged over their probabilities or
probability density. The functions of vectors discussed here are the same as the
ones that deﬁne sample statistics. This short section illustrates the properties
of norms, inner products, and angles in terms that should be familiar to the
reader.
These functions, and transformations using them, are useful for appli-
cations in the data sciences. It is important to know the eﬀects of various
transformations of data on data analysis.
2.3.1 The Mean and Centered Vectors
When the elements of a vector have some kind of common interpretation, the
sum of the elements or the mean (equation (2.46)) of the vector may have
meaning. In this case, it may make sense to center the vector; that is, to
subtract the mean from each element. For a given vector x, we denote its
centered counterpart as xc:
xc = x −¯x.
(2.68)
We refer to any vector whose sum of elements is 0 as a centered vector; note,
therefore, for any centered vector xc,
1Txc = 0;
or, indeed, for any constant vector a, aTxc = 0.
From the deﬁnitions, it is easy to see that
(x + y)c = xc + yc
(2.69)
(see Exercise 2.20). Interpreting ¯x as a vector, and recalling that it is the
projection of x onto the one vector, we see that xc is the residual in the
sense of equation (2.52). Hence, we see that xc and ¯x are orthogonal, and the
Pythagorean relationship holds:
∥x∥2 = ∥¯x∥2 + ∥xc∥2.
(2.70)
From this we see that the length of a centered vector is less than or equal to the
length of the original vector. (Notice that equation (2.70) is just the formula
familiar to data analysts, which with some rearrangement is (xi −¯x)2 =
 x2
i −n¯x2.)
For any scalar a and n-vector x, expanding the terms, we see that
∥x −a∥2 = ∥xc∥2 + n(a −¯x)2,
(2.71)

2.3 Variances and Covariances
49
where we interpret ¯x as a scalar here. An implication of this equation is that
for all values of a, ∥x −a∥is minimized if a = ¯x.
Notice that a nonzero vector when centered may be the zero vector. This
leads us to suspect that some properties that depend on a dot product are
not invariant to centering. This is indeed the case. The angle between two
vectors, for example, is not invariant to centering; that is, in general,
angle(xc, yc) ̸= angle(x, y)
(2.72)
(see Exercise 2.21).
2.3.2 The Standard Deviation, the Variance, and Scaled Vectors
We also sometimes ﬁnd it useful to scale a vector by both its length (normalize
the vector) and by a function of its number of elements. We denote this scaled
vector as xs and deﬁne it as
xs =
√
n −1
x
∥xc∥.
(2.73)
For comparing vectors, it is usually better to center the vectors prior to any
scaling. We denote this centered and scaled vector as xcs and deﬁne it as
xcs =
√
n −1
xc
∥xc∥.
(2.74)
Centering and scaling is also called standardizing. Note that the vector is
centered before being scaled. The angle between two vectors is not changed
by scaling (but, of course, it may be changed by centering).
The multiplicative inverse of the scaling factor,
sx = ∥xc∥/
√
n −1,
(2.75)
is called the standard deviation of the vector x. The standard deviation of xc
is the same as that of x; in fact, the standard deviation is invariant to the
addition of any constant. The standard deviation is a measure of how much
the elements of the vector vary. If all of the elements of the vector are the
same, the standard deviation is 0 because in that case xc = 0.
The square of the standard deviation is called the variance, denoted by V:
V(x) = s2
x
= ∥xc∥2
n −1 .
(2.76)
(In perhaps more familiar notation, equation (2.76) is just V(x) = (xi −
¯x)2/(n −1).) From equation (2.70), we see that
V(x) =
1
n −1

∥x∥2 −∥¯x∥2
.

50
2 Vectors and Vector Spaces
(The terms “mean”, “standard deviation”, “variance”, and other terms we will
mention below are also used in an analogous, but slightly diﬀerent, manner to
refer to properties of random variables. In that context, the terms to refer to
the quantities we are discussing here would be preceded by the word “sample”,
and often for clarity I will use the phrases “sample standard deviation” and
“sample variance” to refer to what is deﬁned above, especially if the elements
of x are interpreted as independent realizations of a random variable. Also,
recall the two possible meanings of “mean”, or ¯x; one is a vector, and one is
a scalar, as in equation (2.47).)
If a and b are scalars (or b is a vector with all elements the same), the
deﬁnition, together with equation (2.71), immediately gives
V(ax + b) = a2V(x).
This implies that for the scaled vector xs,
V(xs) = 1.
If a is a scalar and x and y are vectors with the same number of elements,
from the equation above, and using equation (2.31) on page 26, we see that
the variance following an axpy operation is given by
V(ax + y) = a2V(x) + V(y) + 2a⟨xc, yc⟩
n −1 .
(2.77)
While equation (2.76) appears to be relatively simple, evaluating the ex-
pression for a given x may not be straightforward. We discuss computational
issues for this expression on page 502. This is an instance of a principle that we
will encounter repeatedly: the form of a mathematical expression and the way
the expression should be evaluated in actual practice may be quite diﬀerent.
2.3.3 Covariances and Correlations Between Vectors
If x and y are n-vectors, the covariance between x and y is
Cov(x, y) = ⟨x −¯x, y −¯y⟩
n −1
.
(2.78)
By representing x −¯x as x −¯x1 and y −¯y similarly, and expanding, we see
that Cov(x, y) = (⟨x, y⟩−n¯x¯y)/(n −1). Also, we see from the deﬁnition of
covariance that Cov(x, x) is the variance of the vector x, as deﬁned above.
From the deﬁnition and the properties of an inner product given on
page 24, if x, y, and z are conformable vectors, we see immediately that
•
Cov(x, y) = 0
if V(x) = 0 or V(y) = 0;
•
Cov(ax, y) = aCov(x, y)
for any scalar a;

2.3 Variances and Covariances
51
•
Cov(y, x) = Cov(x, y);
•
Cov(y, y) = V(y); and
•
Cov(x + z, y) = Cov(x, y) + Cov(z, y),
in particular,
–
Cov(x + y, y) = Cov(x, y) + V(y), and
–
Cov(x + a, y) = Cov(x, y)
for any scalar a.
Using the deﬁnition of the covariance, we can rewrite equation (2.77) as
V(ax + y) = a2V(x) + V(y) + 2aCov(x, y).
(2.79)
The covariance is a measure of the extent to which the vectors point in
the same direction. A more meaningful measure of this is obtained by the
covariance of the centered and scaled vectors. This is the correlation between
the vectors, which if ∥xc∦= 0 and ∥yc∦= 0,
Cor(x, y) = Cov(xcs, ycs)
=
 xc
∥xc∥,
yc
∥yc∥
 
= ⟨xc, yc⟩
∥xc∥∥yc∥.
(2.80)
If ∥xc∥= 0 or ∥yc∥= 0, we deﬁne Cor(x, y) to be 0. We see immediately from
equation (2.54) that the correlation is the cosine of the angle between xc and
yc:
Cor(x, y) = cos(angle(xc, yc)).
(2.81)
(Recall that this is not the same as the angle between x and y.)
An equivalent expression for the correlation, so long as V(x) ̸= 0 and
V(y) ̸= 0, is
Cor(x, y) =
Cov(x, y)

V(x)V(y)
.
(2.82)
It is clear that the correlation is in the interval [−1, 1] (from the Cauchy-
Schwarz inequality). A correlation of −1 indicates that the vectors point in
opposite directions, a correlation of 1 indicates that the vectors point in the
same direction, and a correlation of 0 indicates that the vectors are orthogonal.
While the covariance is equivariant to scalar multiplication, the absolute
value of the correlation is invariant to it; that is, the correlation changes only
as the sign of the scalar multiplier,
Cor(ax, y) = sign(a)Cor(x, y),
(2.83)
for any scalar a.

52
2 Vectors and Vector Spaces
Exercises
2.1. Write out the step-by-step proof that the maximum number of n-vectors
that can form a set that is linearly independent is n.
2.2. Prove inequalities (2.10) and (2.11).
2.3. a) Give an example of a vector space and a subset of the set of vectors
in it such that that subset together with the axpy operation is not
a vector space.
b) Give an example of two vector spaces such that the union of the sets
of vectors in them together with the axpy operation is not a vector
space.
2.4. Prove the equalities (2.15) and (2.16).
Hint: Use of basis sets makes the details easier.
2.5. Prove (2.19).
2.6. Let {vi}n
i=1 be an orthonormal basis for the n-dimensional vector space
V. Let x ∈V have the representation
x =

bivi.
Show that the Fourier coeﬃcients bi can be computed as
bi = ⟨x, vi⟩.
2.7. Show that if the norm is induced by an inner product that the parallel-
ogram equality, equation (2.32), holds.
2.8. Let p = 1
2 in equation (2.33); that is, let ρ(x) be deﬁned for the n-vector
x as
ρ(x) =
 n

i=1
|xi|1/2
2
.
Show that ρ(·) is not a norm.
2.9. Show that the L1 norm is not induced by an inner product.
Hint: Find a counterexample that does not satisfy the parallelogram
equality (equation (2.32)).
2.10. Prove equation (2.34) and show that the bounds are sharp by exhibiting
instances of equality. (Use the fact that ∥x∥∞= maxi |xi|.)
2.11. Prove the following inequalities.
a) Prove H¨older’s inequality: for any p and q such that p ≥1 and
p + q = pq, and for vectors x and y of the same order,
⟨x, y⟩≤∥x∥p∥y∥q.
b) Prove the triangle inequality for any Lp norm. (This is sometimes
called Minkowski’s inequality.)
Hint: Use H¨older’s inequality.

Exercises
53
2.12. Show that the expression deﬁned in equation (2.42) on page 32 is a
metric.
2.13. Show that equation (2.53) on page 37 is correct.
2.14. Show that the intersection of two orthogonal vector spaces consists only
of the zero vector.
2.15. From the deﬁnition of direction cosines in equation (2.55), it is easy to
see that the sum of the squares of the direction cosines is 1. For the
special case of IR3, draw a sketch and use properties of right triangles
to show this geometrically.
2.16. In IR2 with a Cartesian coordinate system, the diagonal directed line
segment through the positive quadrant (orthant) makes a 45◦angle
with each of the positive axes. In 3 dimensions, what is the angle be-
tween the diagonal and each of the positive axes? In 10 dimensions? In
100 dimensions? In 1000 dimensions? We see that in higher dimensions
any two lines are almost orthogonal. (That is, the angle between them
approaches 90◦.) What are some of the implications of this for data
analysis?
2.17. Show that if the initial set of vectors are linearly independent, all resid-
uals in Algorithm 2.1 are nonzero. (For given k ≥2, all that is required
is to show that
˜xk −⟨˜xk−1, ˜xk⟩˜xk−1 ̸= 0
if ˜xk and ˜xk−1 are linearly independent. Why?)
2.18. Convex cones.
a) I deﬁned a convex cone as a set of vectors (not necessarily a cone)
such that for any two vectors v1, v2 in the set and for any nonneg-
ative real numbers a, b ≥0, av1 + bv2 is in the set. Then I stated
that an equivalent deﬁnition requires ﬁrst that the set be a cone,
and then includes the requirement a + b = 1 along with a, b ≥0 in
the deﬁnition of a convex cone. Show that the two deﬁnitions are
equivalent.
b) The restriction that a + b = 1 in the deﬁnition of a convex cone
is the kind of restriction that we usually encounter in deﬁnitions of
convex objects. Without this restriction, it may seem that the linear
combinations may get “outside” of the object. Show that this is not
the case for convex cones.
In particular in the two-dimensional case, show that if x = (x1, x2),
y = (y1, y2), with x1/x2 < y1/y2 and a, b ≥0, then
x1/x2 ≤(ax1 + by1)/(ax2 + by2) ≤y1/y2.
This should also help to give a geometrical perspective on convex
cones.
c) Show that if C1 and C2 are convex cones over the same vector space,
then C1 ∩C2 is a convex cone. Give a counterexample to show that
C1 ∪C2 is not necessarily a convex cone.

54
2 Vectors and Vector Spaces
2.19. IR3 and the cross product.
a) Is the cross product associative? Prove or disprove.
b) For x, y ∈IR3, show that the area of the triangle with vertices
(0, 0, 0), x, and y is ∥x × y∥/2.
c) For x, y, z ∈IR3, show that
⟨x, y × z⟩= ⟨x × y, z⟩.
This is called the “triple scalar product”.
d) For x, y, z ∈IR3, show that
x × (y × z) = ⟨x, z⟩y −⟨x, y⟩z.
This is called the “triple vector product”. It is in the plane deter-
mined by y and z.
e) The magnitude of the angle between two vectors is determined by
the cosine, formed from the inner product. Show that in the special
case of IR3, the angle is also determined by the sine and the cross
product, and show that this method can determine both the mag-
nitude and the direction of the angle; that is, the way a particular
vector is rotated into the other.
f) In a Cartesian coordinate system in IR3, the principal axes cor-
respond to the unit vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and
e3 = (0, 0, 1). This system has an indeterminate correspondence to
a physical three-dimensional system; if the plane determined by e1
and e2 is taken as horizontal, then e3 could “point upward” or “point
downward”. A simple way that this indeterminacy can be resolved is
to require that the principal axes have the orientation of the thumb,
index ﬁnger, and middle ﬁnger of the right hand when those digits
are spread in orthogonal directions, where e1 corresponds to the in-
dex ﬁnger, e2 corresponds to the middle ﬁnger, and e3 corresponds
to the thumb. This is called a “right-hand” coordinate system.
Show that in a right-hand coordinate system, if we interpret the
angle between ei and ej to be measured in the direction from ei to
ej, then e3 = e1 × e2 and e3 = −e2 × e1.
2.20. Using equations (2.46) and (2.68), establish equation (2.69).
2.21. Show that the angle between the centered vectors xc and yc is not the
same in general as the angle between the uncentered vectors x and y of
the same order.
2.22. Formally prove equation (2.77) (and hence equation (2.79)).
2.23. Let x and y be any vectors of the same order over the same ﬁeld.
a) Prove
(Cov(x, y))2 ≤V(x)V(y).
b) Hence, prove
−1 ≤Cor(x, y)) ≤1.

3
Basic Properties of Matrices
In this chapter, we build on the notions introduced on page 5, and discuss
a wide range of basic topics related to matrices with real elements. Some of
the properties carry over to matrices with complex elements, but the reader
should not assume this. Occasionally, for emphasis, we will refer to “real”
matrices, but unless it is stated otherwise, we are assuming the matrices are
real.
The topics and the properties of matrices that we choose to discuss are
motivated by applications in the data sciences. In Chap. 8, we will consider in
more detail some special types of matrices that arise in regression analysis and
multivariate data analysis, and then in Chap. 9 we will discuss some speciﬁc
applications in statistics.
3.1 Basic Deﬁnitions and Notation
It is often useful to treat the rows or columns of a matrix as vectors. Terms
such as linear independence that we have deﬁned for vectors also apply to
rows and/or columns of a matrix. The vector space generated by the columns
of the n × m matrix A is of order n and of dimension m or less, and is called
the column space of A, the range of A, or the manifold of A. This vector space
is denoted by
V(A)
or
span(A).
I make no distinction between these two notations. The notation V(·) em-
phasizes that the result is a vector space. Note that if A ∈IRn×m, then
V(A) ⊆IRn.
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 3
55

56
3 Basic Properties of Matrices
The argument of V(·) or span(·) can also be a set of vectors instead of a
matrix. Recall from Sect. 2.1.3 that if G is a set of vectors, the symbol span(G)
denotes the vector space generated by the vectors in G.
We also deﬁne the row space of A to be the vector space of order m
(and of dimension n or less) generated by the rows of A; notice, however, the
preference given to the column space.
Many of the properties of matrices that we discuss hold for matrices with
an inﬁnite number of elements, but throughout this book we will assume that
the matrices have a ﬁnite number of elements, and hence the vector spaces
are of ﬁnite order and have a ﬁnite number of dimensions.
Given an n×m matrix A with elements aij, the m×n matrix with elements
aji is called the transpose of A. We use a superscript “T” to denote the
transpose of a matrix; thus, if A = (aij), then
AT = (aji).
(3.1)
(In other literature, the transpose is often denoted by a prime, as in A′ =
(aji) = AT.)
If, in the matrix A with elements aij for all i and j, aij = aji, A is said
to be symmetric. A symmetric matrix is necessarily square. A matrix A such
that aij = −aji is said to be skew symmetric. Obviously, the diagonal entries
of a skew symmetric matrix must be 0. If aij = ¯aji (where ¯a represents the
conjugate of the complex number a), A is said to be Hermitian. A Hermitian
matrix is also necessarily square with real elements on the diagonal, and, of
course, a real symmetric matrix is Hermitian. A Hermitian matrix is also
called a self-adjoint matrix.
3.1.1 Multiplication of a Matrix by a Scalar
Similar to our deﬁnition of multiplication of a vector by a scalar, we deﬁne
the multiplication of a matrix A by a scalar c as
cA = (caij).
3.1.2 Diagonal Elements: diag(·) and vecdiag(·)
The aii elements of a matrix are called diagonal elements. An element aij
with i < j is said to be “above the diagonal”, and one with i > j is said to
be “below the diagonal”. The vector consisting of all of the aii’s is called the
principal diagonal or just the diagonal. This deﬁnition of principal diagonal
applies whether or not the matrix is square.
We denote the principal diagonal of a matrix A by diag(A) or by
vecdiag(A). The latter notation is sometimes used because, as we will see on
page 60, diag(˙) is also used for an argument that is a vector (and the function
produces a matrix). The diag or vecdiag function deﬁned here is a mapping
IRn×m →IRmin(n,m).

3.1 Basic Deﬁnitions and Notation
57
If A is an n × m matrix, and k = min(n, m),
diag(A) = (a11, . . . , akk).
(3.2)
As noted above, we may also denote this as vecdiag(A), but I will generally
use the notation “diag(·)”.
Note from the deﬁnition that
diag(AT) = diag(A),
(3.3)
and this is true whether or not A is square.
The diagonal begins in the ﬁrst row and ﬁrst column (that is, a11), and
ends at akk, where k is the minimum of the number of rows and the number
of columns.
For c = ±1, . . ., the elements ai,i+c are called “codiagonals” or “minor
diagonals”. The codiagonals ai,i+1 are called “supradiagonals”, and the codi-
agonals ai,i−1 are called “infradiagonals” If the matrix has m columns, the
ai,m+1−i elements of the matrix are called skew diagonal elements. We use
terms similar to those for diagonal elements for elements above and below
the skew diagonal elements. These phrases are used with both square and
nonsquare matrices.
3.1.3 Diagonal, Hollow, and Diagonally Dominant Matrices
If all except the principal diagonal elements of a matrix are 0, the matrix is
called a diagonal matrix. A diagonal matrix is the most common and most
important type of “sparse matrix”. If all of the principal diagonal elements of
a matrix are 0, the matrix is called a hollow matrix. A skew symmetric matrix
is hollow, for example. If all except the principal skew diagonal elements of a
matrix are 0, the matrix is called a skew diagonal matrix.
An n × m matrix A for which
|aii| >
m

j̸=i
|aij|
for each i = 1, . . . , n
(3.4)
is said to be row diagonally dominant; and a matrix A for which |ajj| >
n
i̸=j |aij| for each j = 1, . . . , m is said to be column diagonally dominant.
(Some authors refer to this as strict diagonal dominance and use “diagonal
dominance” without qualiﬁcation to allow the possibility that the inequalities
in the deﬁnitions are not strict.) Most interesting properties of such matrices
hold whether the dominance is by row or by column. If A is symmetric, row
and column diagonal dominances are equivalent, so we refer to row or column
diagonally dominant symmetric matrices without the qualiﬁcation; that is, as
just diagonally dominant.

58
3 Basic Properties of Matrices
3.1.4 Matrices with Special Patterns of Zeroes
If all elements below the diagonal are 0, the matrix is called an upper triangular
matrix; and a lower triangular matrix is deﬁned similarly. If all elements of a
column or row of a triangular matrix are zero, we still refer to the matrix as
triangular, although sometimes we speak of its form as trapezoidal. Another
form called trapezoidal is one in which there are more columns than rows,
and the additional columns are possibly nonzero. The four general forms of
triangular or trapezoidal matrices are shown below, using an intuitive notation
with X and 0 to indicate the pattern.
⎡
⎣
X X X
0 X X
0 0 X
⎤
⎦
⎡
⎣
X X X
0 X X
0 0 0
⎤
⎦
⎡
⎢⎢⎣
X X X
0 X X
0 0 X
0 0 0
⎤
⎥⎥⎦
⎡
⎣
X X X X
0 X X X
0 0 X X
⎤
⎦
In this notation, X indicates that the element is possibly not zero. It does
not mean each element is the same. In some cases, X and 0 may indicate
“submatrices”, which we discuss in the section on partitioned matrices.
If all elements are 0 except ai,i+ck for some small number of integers ck,
the matrix is called a band matrix (or banded matrix). In many applications,
ck ∈{−wl, −wl + 1, . . . , −1, 0, 1, . . ., wu −1, wu}. In such a case, wl is called
the lower band width and wu is called the upper band width. These patterned
matrices arise in time series and other stochastic process models as well as in
solutions of diﬀerential equations, and so they are very important in certain
applications. Although it is often the case that interesting band matrices are
symmetric, or at least have the same number of codiagonals that are nonzero,
neither of these conditions always occurs in applications of band matrices. If
all elements below the principal skew diagonal elements of a matrix are 0, the
matrix is called a skew upper triangular matrix. A common form of Hankel
matrix, for example, is the skew upper triangular matrix (see page 390). Notice
that the various terms deﬁned here, such as triangular and band, also apply
to nonsquare matrices.
Band matrices occur often in numerical solutions of partial diﬀerential
equations. A band matrix with lower and upper band widths of 1 is a tridi-
agonal matrix. If all diagonal elements and all elements ai,i±1 are nonzero, a
tridiagonal matrix is called a “matrix of type 2”. The inverse of a covariance
matrix that occurs in common stationary time series models is a matrix of
type 2 (see page 385).
Using the intuitive notation of X and 0 as above, a band matrix may be
written as
⎡
⎢⎢⎢⎢⎢⎣
X X 0 · · · 0 0
X X X · · · 0 0
0 X X · · · 0 0
...
...
0 0 0 · · · X X
⎤
⎥⎥⎥⎥⎥⎦
.

3.1 Basic Deﬁnitions and Notation
59
Computational methods for matrices may be more eﬃcient if the patterns are
taken into account.
A matrix is in upper Hessenberg form, and is called a Hessenberg matrix, if
it is upper triangular except for the ﬁrst subdiagonal, which may be nonzero.
That is, aij = 0 for i > j + 1:
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
X X X · · · X X
X X X · · · X X
0 X X · · · X X
0 0 X · · · X X
...
...
... ...
...
0 0 0 · · · X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
A symmetric matrix that is in Hessenberg form is necessarily tridiagonal.
Hessenberg matrices arise in some methods for computing eigenvalues (see
Chap. 7).
Many matrices of interest are sparse; that is, they have a large propor-
tion of elements that are 0. The matrices discussed above are generally not
considered sparse. (“A large proportion” is subjective, but generally means
more than 75%, and in many interesting cases is well over 95%.) Eﬃcient and
accurate computations often require that the sparsity of a matrix be accom-
modated explicitly.
3.1.5 Matrix Shaping Operators
In order to perform certain operations on matrices and vectors, it is often
useful ﬁrst to reshape a matrix. The most common reshaping operation is
the transpose, which we deﬁne in this section. Sometimes we may need to
rearrange the elements of a matrix or form a vector into a special matrix. In
this section, we deﬁne three operators for doing this.
3.1.5.1 Transpose
As deﬁned above, the transpose of a matrix is the matrix whose ith row is the
ith column of the original matrix and whose jth column is the jth row of the
original matrix. We note immediately that
(AT)T = A.
(3.5)
If the elements of the matrix are from the ﬁeld of complex numbers, the
conjugate transpose, also called the adjoint, is more useful than the transpose.
(“Adjoint” is also used to denote another type of matrix, so we will generally
avoid using that term. This meaning of the word is the origin of the other
term for a Hermitian matrix, a “self-adjoint matrix”.) We use a superscript
“H” to denote the conjugate transpose of a matrix; thus, if A = (aij), then

60
3 Basic Properties of Matrices
AH = (¯aji).
(3.6)
We also use a similar notation for vectors. (The conjugate transpose is often
denoted by an asterisk, as in A∗= (¯aji) = AH. This notation is more common
if a prime is used to denote the transpose. We sometimes use the notation A∗
to denote a g2 inverse of the matrix A; see page 128.) As with the transponse,
(AH)H = A. If (and only if) all of the elements of A are all real, then AH = AT.
If (and only if) A is symmetric, A = AT; if (and only if) A is skew sym-
metric, AT = −A; and if (and only if) A is Hermitian, A = AH (and, in that
case, all of the diagonal elements are real).
3.1.5.2 Diagonal Matrices and Diagonal Vectors: diag(·) (Again)
A square diagonal matrix can be speciﬁed by a constructor function that
operates on a vector and forms a diagonal matrix with the elements of the
vector along the diagonal. We denote that constructor function by diag(·),
just as we used this name to denote a somewhat similar function on page 57.
diag

(d1, d2, . . . , dn)

=
⎡
⎢⎢⎢⎣
d1 0 · · · 0
0 d2 · · · 0
...
0 0 · · · dn
⎤
⎥⎥⎥⎦.
(3.7)
(Notice that the argument of diag here is a vector; that is why there are
two sets of parentheses in the expression above, although sometimes we omit
one set without loss of clarity.) The diag function deﬁned here is a mapping
IRn →IRn×n. Later we will extend this deﬁnition slightly.
A very important diagonal matrix has all 1s along the diagonal. If it has
n diagonal elements, it is denoted by In; so In = diag(1n). This is called the
identity matrix of order n. The size is often omitted, and we call it the identity
matrix, and denote it by I.
Note that we have overloaded diag(·), which we deﬁned on page 57 with a
matrix argument, to allow its argument to be a vector. (Recall that vecdiag(·)
is the same as diag(·) when the argument is a matrix.) Both the R and Mat-
lab computing systems, for example, use this overloading; that is, they each
provide a single function (called diag in each case).
Note further that over IRn and IRn×n, diag(·) is its own inverse; that is, if
v is a vector,
diag(diag(v)) = v,
(3.8)
and if A is a square matrix,
diag(diag(A)) = A.
(3.9)

3.1 Basic Deﬁnitions and Notation
61
3.1.5.3 Forming a Vector from the Elements of a Matrix: vec(·)
and vech(·)
It is sometimes useful to consider the elements of a matrix to be elements of
a single vector. The most common way this is done is to string the columns
of the matrix end-to-end into a vector. The vec(·) function does this:
vec(A) = (aT
1 , aT
2 , . . . , aT
m),
(3.10)
where a1, a2, . . . , am are the column vectors of the matrix A. The vec function
is also sometimes called the “pack” function. (A note on the notation: the
right side of equation (3.10) is the notation for a column vector with elements
aT
i ; see Chap. 1.) The vec function is a mapping IRn×m →IRnm.
For a symmetric matrix A with elements aij, the “vech” function stacks
the unique elements into a vector:
vech(A) = (a11, a21, . . . , am1, a22, . . . , am2, . . . , amm).
(3.11)
There are other ways that the unique elements could be stacked that would
be simpler and perhaps more useful (see the discussion of symmetric storage
mode on page 548), but equation (3.11) is the standard deﬁnition of vech(·).
The vech function is a mapping IRn×n →IRn(n+1)/2.
3.1.6 Partitioned Matrices
We often ﬁnd it useful to partition a matrix into submatrices; for example,
in many applications in data analysis, it is often convenient to work with
submatrices of various types representing diﬀerent subsets of the data.
We usually denote the submatrices with capital letters with subscripts
indicating the relative positions of the submatrices. Hence, we may write
A =
!
A11 A12
A21 A22
"
,
(3.12)
where the matrices A11 and A12 have the same number of rows, A21 and
A22 have the same number of rows, A11 and A21 have the same number of
columns, and A12 and A22 have the same number of columns. Of course, the
submatrices in a partitioned matrix may be denoted by diﬀerent letters. Also,
for clarity, sometimes we use a vertical bar to indicate a partition:
A = [ B | C ].
The vertical bar is used just for clarity and has no special meaning in this
representation.
The term “submatrix” is also used to refer to a matrix formed from a
given matrix by deleting various rows and columns of the given matrix. In this
terminology, B is a submatrix of A if for each element bij there is an akl with

62
3 Basic Properties of Matrices
k ≥i and l ≥j such that bij = akl; that is, the rows and/or columns of the
submatrix are not necessarily contiguous in the original matrix. A more precise
notation speciﬁes the rows and columns of the original matrix. For example,
A(i1,...,ik)(j1,...,jl) denotes the submatrix of A formed by rows i1, . . . , ik and
columns j1, . . . , jl. When the entire rows are included, A(i1,...,ik)(∗) denotes
the submatrix of A formed from rows i1, . . . , ik; and A(∗)(j1,...,jl) denotes the
submatrix formed from columns j1, . . . , jl with elements from all rows. Finally,
ai∗denotes the vector whose elements correspond to those in the ith row of
the matrix A. We sometimes emphasize that it is a vector by writing it in
the form aT
i∗. Likewise, a∗j denotes the vector whose elements correspond to
those in the jth column of A. See page 599 for a summary of this notation.
This kind of subsetting is often done in data analysis, for example, in variable
selection in linear regression analysis.
A square submatrix whose principal diagonal elements are elements of the
principal diagonal of the given matrix is called a principal submatrix. If A11 in
the example above is square, it is a principal submatrix, and if A22 is square,
it is also a principal submatrix. Sometimes the term “principal submatrix” is
restricted to square submatrices. If a matrix is diagonally dominant, then it
is clear that any principal submatrix of it is also diagonally dominant.
A principal submatrix that contains the (1, 1) element and whose rows
and columns are contiguous in the original matrix is called a leading principal
submatrix. If A11 is square, it is a leading principal submatrix in the example
above.
Partitioned matrices may have useful patterns. A “block diagonal” matrix
is one of the form
⎡
⎢⎢⎢⎣
X 0 · · · 0
0 X · · · 0
...
0 0 · · · X
⎤
⎥⎥⎥⎦,
where 0 represents a submatrix with all zeros and X represents a general
submatrix with at least some nonzeros.
The diag(·) function previously introduced for a vector is also deﬁned for
a list of matrices:
diag(A1, A2, . . . , Ak)
denotes the block diagonal matrix with submatrices A1, A2, . . . , Ak along the
diagonal and zeros elsewhere. A matrix formed in this way is sometimes called
a direct sum of A1, A2, . . . , Ak, and the operation is denoted by ⊕:
A1 ⊕· · · ⊕Ak = diag(A1, . . . , Ak).
(3.13)
Although the direct sum is a binary operation, we are justiﬁed in deﬁning
it for a list of matrices because the operation is clearly associative.
The Ai may be of diﬀerent sizes and they may not be square, although in
most applications the matrices are square (and some authors deﬁne the direct
sum only for square matrices).

3.1 Basic Deﬁnitions and Notation
63
We will deﬁne vector spaces of matrices below and then recall the deﬁnition
of a direct sum of vector spaces (page 18), which is diﬀerent from the direct
sum deﬁned above in terms of diag(·).
3.1.6.1 Transposes of Partitioned Matrices
The transpose of a partitioned matrix is formed in the obvious way; for ex-
ample,
!A11 A12 A13
A21 A22 A23
"T
=
⎡
⎣
AT
11 AT
21
AT
12 AT
22
AT
13 AT
23
⎤
⎦.
(3.14)
3.1.7 Matrix Addition
The sum of two matrices of the same shape is the matrix whose elements
are the sums of the corresponding elements of the addends. As in the case of
vector addition, we overload the usual symbols for the operations on the reals
to signify the corresponding operations on matrices when the operations are
deﬁned; hence, addition of matrices is also indicated by “+”, as with scalar
addition and vector addition. We assume throughout that writing a sum of
matrices A+ B implies that they are of the same shape; that is, that they are
conformable for addition.
The “+” operator can also mean addition of a scalar to a matrix, as in
A + a, where A is a matrix and a is a scalar. Although this meaning of “+”
is generally not used in mathematical treatments of matrices, in this book
we use it to mean the addition of the scalar to each element of the matrix,
resulting in a matrix of the same shape. This meaning is consistent with the
semantics of modern computer languages such as Fortran and R.
The addition of two n×m matrices or the addition of a scalar to an n×m
matrix requires nm scalar additions.
The matrix additive identity is a matrix with all elements zero. We some-
times denote such a matrix with n rows and m columns as 0n×m, or just as 0.
We may denote a square additive identity as 0n.
3.1.7.1 The Transpose of the Sum of Matrices
The transpose of the sum of two matrices is the sum of the transposes:
(A + B)T = AT + BT.
(3.15)
The sum of two symmetric matrices is therefore symmetric.

64
3 Basic Properties of Matrices
3.1.7.2 Rank Ordering Matrices
There are several possible ways to form a rank ordering of matrices of the
same shape, but no complete ordering is entirely satisfactory. If all of the
elements of the matrix A are positive, we write
A > 0;
(3.16)
if all of the elements are nonnegative, we write
A ≥0.
(3.17)
The terms “positive” and “nonnegative” and these symbols are not to be
confused with the terms “positive deﬁnite” and “nonnegative deﬁnite” and
similar symbols for important classes of matrices having diﬀerent properties
(which we will introduce on page 92, and discuss further in Sect. 8.3.)
3.1.7.3 Vector Spaces of Matrices
Having deﬁned scalar multiplication and matrix addition (for conformable
matrices), we can deﬁne a vector space of n × m matrices as any set that is
closed with respect to those operations. The individual operations of scalar
multiplication and matrix addition allow us to deﬁne an axpy operation on
the matrices, as in equation (2.1) on page 12. Closure of this space implies
that it must contain the additive identity, just as we saw on page 13). The
matrix additive identity is the 0 matrix.
As with any vector space, we have the concepts of linear independence,
generating set or spanning set, basis set, essentially disjoint spaces, and direct
sums of matrix vector spaces (as in equation (2.13), which is diﬀerent from
the direct sum of matrices deﬁned in terms of diag(·) as in equation (3.13)).
An important vector space of matrices is IRn×m. For matrices X, Y ∈
IRn×m and a ∈IR, the axpy operation is aX + Y .
If n ≥m, a set of nm n×m matrices whose columns consist of all combina-
tions of a set of n n-vectors that span IRn is a basis set for IRn×m. If n < m,
we can likewise form a basis set for IRn×m or for subspaces of IRn×m in a
similar way. If {B1, . . . , Bk} is a basis set for IRn×m, then any n × m matrix
can be represented as k
i=1 ciBi. Subsets of a basis set generate subspaces of
IRn×m.
Because the sum of two symmetric matrices is symmetric, and a scalar
multiple of a symmetric matrix is likewise symmetric, we have a vector space
of the n × n symmetric matrices. This is clearly a subspace of the vector
space IRn×n. All vectors in any basis for this vector space must be symmetric.
Using a process similar to our development of a basis for a general vector
space of matrices, we see that there are n(n + 1)/2 matrices in the basis (see
Exercise 3.1).

3.1 Basic Deﬁnitions and Notation
65
3.1.8 Scalar-Valued Operators on Square Matrices:
The Trace
There are several useful mappings from matrices to real numbers; that is, from
IRn×m to IR. Some important ones are norms, which are similar to vector
norms and which we will consider later. In this section and the next, we
deﬁne two scalar-valued operators, the trace and the determinant, that apply
to square matrices.
3.1.8.1 The Trace: tr(·)
The sum of the diagonal elements of a square matrix is called the trace of the
matrix. We use the notation “tr(A)” to denote the trace of the matrix A:
tr(A) =

i
aii.
(3.18)
3.1.8.2 The Trace of the Transpose of Square Matrices
From the deﬁnition, we see
tr(A) = tr(AT).
(3.19)
3.1.8.3 The Trace of Scalar Products of Square Matrices
For a scalar c and an n × n matrix A,
tr(cA) = c tr(A).
This follows immediately from the deﬁnition because for tr(cA) each diagonal
element is multiplied by c.
3.1.8.4 The Trace of Partitioned Square Matrices
If the square matrix A is partitioned such that the diagonal blocks are square
submatrices, that is,
A =
!A11 A12
A21 A22
"
,
(3.20)
where A11 and A22 are square, then from the deﬁnition, we see that
tr(A) = tr(A11) + tr(A22).
(3.21)
3.1.8.5 The Trace of the Sum of Square Matrices
If A and B are square matrices of the same order, a useful (and obvious)
property of the trace is
tr(A + B) = tr(A) + tr(B).
(3.22)

66
3 Basic Properties of Matrices
3.1.9 Scalar-Valued Operators on Square Matrices:
The Determinant
The determinant, like the trace, is a mapping from IRn×n to IR. Although
it may not be obvious from the deﬁnition below, the determinant has far-
reaching applications in matrix theory.
3.1.9.1 The Determinant: det(·)
For an n × n (square) matrix A, consider the product a1j1 · · · anjn, where
πj = (j1, . . . , jn) is one of the n! permutations of the integers from 1 to n.
Deﬁne a permutation to be even or odd according to the number of times
that a smaller element follows a larger one in the permutation. For example,
given the tuple (1, 2, 3), then (1, 3, 2) is an odd permutation, and (3, 1, 2) and
(1, 2, 3) are even permutations. Let
σ(π) =
#
1 if π is an even permutation
−1 otherwise.
(3.23)
Then the determinant of A, denoted by det(A), is deﬁned by
det(A) =

all permutations πj
σ(πj)a1j1 · · · anjn.
(3.24)
This simple function has many remarkable relationships to various prop-
erties of matrices.
3.1.9.2 Notation and Simple Properties of the Determinant
The determinant is also sometimes written as |A|.
I prefer the notation det(A), because of the possible confusion between
|A| and the absolute value of some quantity. The latter notation, however,
is recommended by its compactness, and I do use it in expressions such as
the PDF of the multivariate normal distribution (see equation (4.73)) that
involve nonnegative deﬁnite matrices (see page 91 for the deﬁnition). The
determinant of a matrix may be negative, and sometimes, as in measuring
volumes (see page 74 for simple areas and page 215 for special volumes called
Jacobians), we need to specify the absolute value of the determinant, so we
need something of the form |det(A)|.
The deﬁnition of the determinant is not as daunting as it may appear
at ﬁrst glance. Many properties become obvious when we realize that σ(·)
is always ±1, and it can be built up by elementary exchanges of adjacent
elements. For example, consider σ(3, 2, 1). There are two ways we can use
three elementary exchanges, each beginning with the natural ordering:
(1, 2, 3) →(2, 1, 3) →(2, 3, 1) →(3, 2, 1),

3.1 Basic Deﬁnitions and Notation
67
or
(1, 2, 3) →(1, 3, 2) →(3, 1, 2) →(3, 2, 1);
hence, either way, σ(3, 2, 1) = (−1)3 = −1.
If πj consists of the interchange of exactly two elements in (1, . . . , n), say
elements p and q with p < q, then there are q −p elements before p that
are larger than p, and there are q −p −1 elements between q and p in the
permutation each with exactly one larger element preceding it. The total
number is 2q −2p + 1, which is an odd number. Therefore, if πj consists of
the interchange of exactly two elements, then σ(πj) = −1.
If the integers 1, . . . , m occur sequentially in a given permutation and are
followed by m + 1, . . . , n which also occur sequentially in the permutation,
they can be considered separately:
σ(j1, . . . , jn) = σ(j1, . . . , jm)σ(jm+1, . . . , jn).
(3.25)
Furthermore, we see that the product a1j1 · · · anjn has exactly one factor from
each unique row-column pair. These observations facilitate the derivation of
various properties of the determinant (although the details are sometimes
quite tedious).
We see immediately from the deﬁnition that the determinant of an upper
or lower triangular matrix (or a diagonal matrix) is merely the product of the
diagonal elements (because in each term of equation (3.24) there is a 0, except
in the term in which the subscripts on each factor are the same).
3.1.9.3 Minors, Cofactors, and Adjugate Matrices
Consider the 2 × 2 matrix
A =
! a11 a12
a21 a22
"
.
From the deﬁnition of the determinant, we see that
det(A) = a11a22 −a12a21.
(3.26)
Now let A be a 3 × 3 matrix:
A =
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦.
In the deﬁnition of the determinant, consider all of the terms in which the
elements of the ﬁrst row of A appear. With some manipulation of those terms,
we can express the determinant in terms of determinants of submatrices as

68
3 Basic Properties of Matrices
det(A) = a11(−1)1+1det
! a22 a23
a32 a33
"
+ a12(−1)1+2det
! a21 a23
a31 a33
"
+ a13(−1)1+3det
!a21 a22
a31 a32
"
.
(3.27)
Notice that this is the same form as in equation (3.26):
det(A) = a11(1)det(a22) + a12(−1)det(a21).
The manipulation in equation (3.27) of the terms in the determinant could
be carried out with other rows of A.
The determinants of the 2 × 2 submatrices in equation (3.27) are called
minors or complementary minors of the associated element. The deﬁnition
can be extended to (n−1)×(n−1) submatrices of an n×n matrix, for n ≥2.
We denote the minor associated with the aij element as
det

A−(i)(j)

,
(3.28)
in which A−(i)(j) denotes the submatrix that is formed from A by removing
the ith row and the jth column. The sign associated with the minor corre-
sponding to aij is (−1)i+j. The minor together with its appropriate sign is
called the cofactor of the associated element; that is, the cofactor of aij is
(−1)i+jdet

A−(i)(j)

. We denote the cofactor of aij as a(ij):
a(ij) = (−1)i+jdet

A−(i)(j)

.
(3.29)
Notice that both minors and cofactors are scalars.
The manipulations leading to equation (3.27), though somewhat tedious,
can be carried out for a square matrix of any size larger than 1×1, and minors
and cofactors are deﬁned as above. An expression such as in equation (3.27)
is called an expansion in minors or an expansion in cofactors.
The extension of the expansion (3.27) to an expression involving a sum
of signed products of complementary minors arising from (n −1) × (n −1)
submatrices of an n × n matrix A is
det(A) =
n

j=1
aij(−1)i+jdet

A−(i)(j)

=
n

j=1
aija(ij),
(3.30)
or, over the rows,

3.1 Basic Deﬁnitions and Notation
69
det(A) =
n

i=1
aija(ij).
(3.31)
These expressions are called Laplace expansions. Each determinant det

A−(i)(j)

can likewise be expressed recursively in a similar expansion.
Expressions (3.30) and (3.31) are special cases of a more general Laplace
expansion based on an extension of the concept of a complementary minor
of an element to that of a complementary minor of a minor. The derivation
of the general Laplace expansion is straightforward but rather tedious (see
Harville 1997, for example, for the details).
Laplace expansions could be used to compute the determinant, but the
main value of these expansions is in proving properties of determinants. For
example, from the special Laplace expansion (3.30) or (3.31), we can quickly
see that the determinant of a matrix with two rows that are the same is zero.
We see this by recursively expanding all of the minors until we have only 2×2
matrices consisting of a duplicated row. The determinant of such a matrix is
0, so the expansion is 0.
The expansion in equation (3.30) has an interesting property: if instead of
the elements aij from the ith row we use elements from a diﬀerent row, say
the kth row, the sum is zero. That is, for k ̸= i,
n

j=1
akj(−1)i+jdet

A−(i)(j)

=
n

j=1
akja(ij)
= 0.
(3.32)
This is true because such an expansion is exactly the same as an expansion for
the determinant of a matrix whose kth row has been replaced by its ith row;
that is, a matrix with two identical rows. The determinant of such a matrix
is 0, as we saw above.
A certain matrix formed from the cofactors has some interesting properties.
We deﬁne the matrix here but defer further discussion. The adjugate of the
n × n matrix A is deﬁned as
adj(A) = (a(ji)),
(3.33)
which is an n × n matrix of the cofactors of the elements of the transposed
matrix. (The adjugate is also called the adjoint or sometimes “classical ad-
joint”, but as we noted above, the term adjoint may also mean the conjugate
transpose. To distinguish it from the conjugate transpose, the adjugate is also
sometimes called the “classical adjoint”. We will generally avoid using the
term “adjoint”.) Note the reversal of the subscripts; that is,

70
3 Basic Properties of Matrices
adj(A) = (a(ij))T.
The adjugate has an interesting property involving matrix multiplication
(which we will deﬁne below in Sect. 3.2) and the identity matrix:
A adj(A) = adj(A)A = det(A)I.
(3.34)
To see this, consider the (i, j)th element of A adj(A). By the deﬁnition of
the multiplication of A and adj(A), that element is 
k aik(adj(A))kj. Now,
noting the reversal of the subscripts in adj(A) in equation (3.33), and using
equations (3.30) and (3.32), we have

k
aik(adj(A))kj =
#
det(A) if i = j
0
if i ̸= j;
that is, A adj(A) = det(A)I.
The adjugate has a number of other useful properties, some of which we
will encounter later, as in equation (3.172).
3.1.9.4 The Determinant of the Transpose of Square Matrices
One important property we see immediately from a manipulation of the deﬁ-
nition of the determinant is
det(A) = det(AT).
(3.35)
3.1.9.5 The Determinant of Scalar Products of Square Matrices
For a scalar c and an n × n matrix A,
det(cA) = cndet(A).
(3.36)
This follows immediately from the deﬁnition because, for det(cA), each factor
in each term of equation (3.24) is multiplied by c.
3.1.9.6 The Determinant of an Upper (or Lower) Triangular
Matrix
If A is an n × n upper (or lower) triangular matrix, then
det(A) =
n
$
i=1
aii.
(3.37)
This follows immediately from the deﬁnition. It can be generalized, as in the
next section.

3.1 Basic Deﬁnitions and Notation
71
3.1.9.7 The Determinant of Certain Partitioned Square Matrices
Determinants of square partitioned matrices that are block diagonal or upper
or lower block triangular depend only on the diagonal partitions:
det(A) = det
!
A11
0
0
A22
"
= det
!
A11
0
A21 A22
"
= det
!
A11 A12
0
A22
"
= det(A11)det(A22).
(3.38)
We can see this by considering the individual terms in the determinant, equa-
tion (3.24). Suppose the full matrix is n × n, and A11 is m × m. Then A22
is (n −m) × (n −m),
A21 is (n −m) × m,
and A12 is m × (n −m). In
equation (3.24), any addend for which (j1, . . . , jm) is not a permutation of the
integers 1, . . . , m contains a factor aij that is in a 0 diagonal block, and hence
the addend is 0. The determinant consists only of those addends for which
(j1, . . . , jm) is a permutation of the integers 1, . . . , m, and hence (jm+1, . . . , jn)
is a permutation of the integers m + 1, . . . , n,
det(A) =
 
σ(j1, . . . , jm, jm+1, . . . , jn)a1j1 · · · amjmam+1,jn · · · anjn,
where the ﬁrst sum is taken over all permutations that keep the ﬁrst m in-
tegers together while maintaining a ﬁxed ordering for the integers m + 1
through n, and the second sum is taken over all permutations of the inte-
gers from m + 1 through n while maintaining a ﬁxed ordering of the integers
from 1 to m. Now, using equation (3.25), we therefore have for A of this
special form
det(A) =
 
σ(j1, . . . , jm, jm+1, . . . , jn)a1j1 · · · amjmam+1,jm+1 · · · anjn
=

σ(j1, . . . , jm)a1j1 · · · amjm

σ(jm+1, . . . , jn)am+1,jm+1 · · · anjn
= det(A11)det(A22),
which is equation (3.38). We use this result to give an expression for the
determinant of more general partitioned matrices in Sect. 3.4.2.
Another useful partitioned matrix of the form of equation (3.20) has A11 =
0 and A21 = −I:
A =
!
0 A12
−I A22
"
.
In this case, using equation (3.30), we get
det(A) = ((−1)n+1+1(−1))ndet(A12)
= (−1)n(n+3)det(A12)
= det(A12).
(3.39)

72
3 Basic Properties of Matrices
We will consider determinants of a more general partitioning in Sect. 3.4.2,
beginning on page 122.
3.1.9.8 The Determinant of the Sum of Square Matrices
Occasionally it is of interest to consider the determinant of the sum of square
matrices. We note in general that
det(A + B) ̸= det(A) + det(B),
which we can see easily by an example. (Consider matrices in IR2×2, for ex-
ample, and let A = I and B =
! −1 0
0 0
"
.)
In some cases, however, simpliﬁed expressions for the determinant of a
sum can be developed. We consider one in the next section.
3.1.9.9 A Diagonal Expansion of the Determinant
A particular sum of matrices whose determinant is of interest is one in which
a diagonal matrix D is added to a square matrix A, that is, det(A+D). (Such
a determinant arises in eigenanalysis, for example, as we see in Sect. 3.8.4.)
For evaluating the determinant det(A+D), we can develop another expan-
sion of the determinant by restricting our choice of minors to determinants of
matrices formed by deleting the same rows and columns and then continuing
to delete rows and columns recursively from the resulting matrices. The ex-
pansion is a polynomial in the elements of D; and for our purposes later, that
is the most useful form.
Before considering the details, let us develop some additional notation.
The matrix formed by deleting the same row and column of A is denoted
A−(i)(i) as above (following equation (3.28)). In the current context, however,
it is more convenient to adopt the notation A(i1,...,ik) to represent the matrix
formed from rows i1, . . . , ik and columns i1, . . . , ik from a given matrix A.
That is, the notation A(i1,...,ik) indicates the rows and columns kept rather
than those deleted; and furthermore, in this notation, the indexes of the rows
and columns are the same. We denote the determinant of this k × k matrix
in the obvious way, det(A(i1,...,ik)). Because the principal diagonal elements
of this matrix are principal diagonal elements of A, we call det(A(i1,...,ik)) a
principal minor of A.
Now consider det(A + D) for the 2 × 2 case:
det
! a11 + d1
a12
a21
a22 + d2
"
.
Expanding this, we have
det(A + D) = (a11 + d1)(a22 + d2) −a12a21

3.1 Basic Deﬁnitions and Notation
73
= det
!a11 a12
a21 a22
"
+ d1d2 + a22d1 + a11d2
= det(A(1,2)) + d1d2 + a22d1 + a11d2.
Of course, det(A(1,2)) = det(A), but we are writing it this way to develop the
pattern. Now, for the 3 × 3 case, we have
det(A + D) = det(A(1,2,3))
+ det(A(2,3))d1 + det(A(1,3))d2 + det(A(1,2))d3
+ a33d1d2 + a22d1d3 + a11d2d3
+ d1d2d3.
(3.40)
In the applications of interest, the elements of the diagonal matrix D may be
a single variable: d, say. In this case, the expression simpliﬁes to
det(A + D) = det(A(1,2,3)) +

i̸=j
det(A(i,j))d +

i
ai,id2 + d3.
(3.41)
Carefully continuing in this way for an n×n matrix, either as in equation (3.40)
for n variables or as in equation (3.41) for a single variable, we can make use
of a Laplace expansion to evaluate the determinant.
Consider the expansion in a single variable because that will prove most
useful. The pattern persists; the constant term is |A|, the coeﬃcient of the
ﬁrst-degree term is the sum of the (n −1)-order principal minors, and, at
the other end, the coeﬃcient of the (n −1)th-degree term is the sum of the
ﬁrst-order principal minors (that is, just the diagonal elements), and ﬁnally
the coeﬃcient of the nth-degree term is 1.
This kind of representation is called a diagonal expansion of the determi-
nant because the coeﬃcients are principal minors. It has occasional use for
matrices with large patterns of zeros, but its main application is in analysis
of eigenvalues, which we consider in Sect. 3.8.4.
3.1.9.10 Computing the Determinant
For an arbitrary matrix, the determinant is rather diﬃcult to compute. The
method for computing a determinant is not the one that would arise directly
from the deﬁnition or even from a Laplace expansion. The more eﬃcient meth-
ods involve ﬁrst factoring the matrix, as we discuss in later sections.
The determinant is not very often directly useful, but although it may not
be obvious from its deﬁnition, the determinant, along with minors, cofactors,
and adjoint matrices, is very useful in discovering and proving properties of
matrices. The determinant is used extensively in eigenanalysis (see Sect. 3.8).

74
3 Basic Properties of Matrices
3.1.9.11 A Geometrical Perspective of the Determinant
In Sect. 2.2, we discussed a useful geometric interpretation of vectors in a
linear space with a Cartesian coordinate system. The elements of a vec-
tor correspond to measurements along the respective axes of the coordinate
system. When working with several vectors, or with a matrix in which the
columns (or rows) are associated with vectors, we may designate a vector
xi as xi = (xi1, . . . , xid). A set of d linearly independent d-vectors deﬁne a
parallelotope in d dimensions. For example, in a two-dimensional space, the
linearly independent 2-vectors x1 and x2 deﬁne a parallelogram, as shown in
Fig. 3.1.
The area of this parallelogram is the base times the height, bh, where, in
this case, b is the length of the vector x1, and h is the length of x2 times the
sine of the angle θ. Thus, making use of equation (2.54) on page 37 for the
cosine of the angle, we have
x2
x1
q
a
h
b
e1
e2
Figure 3.1. Volume (area) of region determined by x1 and x2
area = bh
= ∥x1∥∥x2∥sin(θ)
= ∥x1∥∥x2∥
%
1 −
 ⟨x1, x2⟩
∥x1∥∥x2∥
2
=

∥x1∥2∥x2∥2 −(⟨x1, x2⟩)2
=
&
(x2
11 + x2
12)(x2
21 + x2
22) −(x11x21 −x12x22)2

3.2 Multiplication of Matrices
75
= |x11x22 −x12x21|
= |det(X)|,
(3.42)
where x1 = (x11, x12), x2 = (x21, x22), and
X = [x1 | x2]
=
!
x11 x21
x12 x22
"
.
Although we will not go through the details here, this equivalence of a
volume of a parallelotope that has a vertex at the origin and the absolute
value of the determinant of a square matrix whose columns correspond to the
vectors that form the sides of the parallelotope extends to higher dimensions.
In making a change of variables in integrals, as in equation (4.62) on
page 215, we use the absolute value of the determinant of the Jacobian as a
volume element. Another instance of the interpretation of the determinant as
a volume is in the generalized variance, discussed on page 368.
3.2 Multiplication of Matrices and
Multiplication of Vectors and Matrices
The elements of a vector or matrix are elements of a ﬁeld, and most matrix
and vector operations are deﬁned in terms of the two operations of the ﬁeld.
Of course, in this book, the ﬁeld of most interest is the ﬁeld of real numbers.
3.2.1 Matrix Multiplication (Cayley)
There are various kinds of multiplication of matrices that may be useful. The
most common kind of multiplication is Cayley multiplication. If the number
of columns of the matrix A, with elements aij, and the number of rows of the
matrix B, with elements bij, are equal, then the (Cayley) product of A and B
is deﬁned as the matrix C with elements
cij =

k
aikbkj.
(3.43)
This is the most common type of matrix product, and we refer to it by the
unqualiﬁed phrase “matrix multiplication”.
Cayley matrix multiplication is indicated by juxtaposition, with no inter-
vening symbol for the operation: C = AB.
If the matrix A is n × m and the matrix B is m × p, the product C = AB
is n × p:
C
= A
B
!
"
n×p
=
! "
n×m
[
]m×p
.

76
3 Basic Properties of Matrices
Cayley matrix multiplication is a mapping,
IRn×m × IRm×p →IRn×p.
The multiplication of an n × m matrix and an m × p matrix requires
nmp scalar multiplications and np(m −1) scalar additions. Here, as always
in numerical analysis, we must remember that the deﬁnition of an operation,
such as matrix multiplication, does not necessarily deﬁne a good algorithm
for evaluating the operation.
It is obvious that while the product AB may be well-deﬁned, the product
BA is deﬁned only if n = p; that is, if the matrices AB and BA are square.
We assume throughout that writing a product of matrices AB implies that
the number of columns of the ﬁrst matrix is the same as the number of rows of
the second; that is, they are conformable for multiplication in the order given.
It is easy to see from the deﬁnition of matrix multiplication (3.43) that
in general, even for square matrices, AB ̸= BA. It is also obvious that if AB
exists, then BTAT exists and, in fact,
BTAT = (AB)T.
(3.44)
The product of symmetric matrices is not, in general, symmetric. If (but not
only if) A and B are symmetric, then AB = (BA)T.
Because matrix multiplication is not commutative, we often use the terms
“premultiply” and “postmultiply” and the corresponding nominal forms of
these terms. Thus, in the product AB, we may say B is premultiplied by A,
or, equivalently, A is postmultiplied by B.
Although matrix multiplication is not commutative, it is associative; that
is, if the matrices are conformable,
A(BC) = (AB)C.
(3.45)
It is also distributive over addition; that is,
A(B + C) = AB + AC
(3.46)
and
(B + C)A = BA + CA.
(3.47)
These properties are obvious from the deﬁnition of matrix multiplication.
(Note that left-sided distribution is not the same as right-sided distribution
because the multiplication is not commutative.)
An n×n matrix consisting of 1s along the diagonal and 0s everywhere else is
a multiplicative identity for the set of n×n matrices and Cayley multiplication.
Such a matrix is called the identity matrix of order n, and is denoted by In,
or just by I. The columns of the identity matrix are unit vectors.
The identity matrix is a multiplicative identity for any matrix so long as
the matrices are conformable for the multiplication. If A is n × m, then

3.2 Multiplication of Matrices
77
InA = AIm = A.
Another matrix of interest is a zero matrix, which is any matrix consisting
of all zeros. We denote a zero matrix as 0, with its shape being implied by the
context. Two properties for any matrix A and a zero matrix of the appropriate
shape are immediately obvious:
0A = 0
and
0 + A = A.
3.2.1.1 Powers of Square Matrices
For a square matrix A, its product with itself is deﬁned, and so we will use the
notation A2 to mean the Cayley product AA, with similar meanings for Ak
for a positive integer k. As with the analogous scalar case, Ak for a negative
integer may or may not exist, and when it exists, it has a meaning for Cayley
multiplication similar to the meaning in ordinary scalar multiplication. We
will consider these issues later (in Sect. 3.3.6).
For an n × n matrix A, if Ak exists for negative integral values of k, we
deﬁne A0 by
A0 = In.
(3.48)
For a diagonal matrix D = diag ((d1, . . . , dn)), we have
Dk = diag

(dk
1, . . . , dk
n)

.
(3.49)
3.2.1.2 Nilpotent Matrices
For an n × n matrix A, it may be the case for some positive integer k that
Ak = 0. Such a matrix is said to be nilpotent; more generally, we deﬁne a
nilpotent matrix of index k, for integer k > 1, as a square matrix A such that
Ak = 0,
but
Ak−1 ̸= 0.
(3.50)
We may use the term “nilpotent” without qualiﬁcation to refer to a matrix
that is nilpotent of any index; that is, strictly speaking, a nilpotent matrix is
nilpotent of index 2.
A simple example of a matrix that is nilpotent of index 3 is
A =
⎡
⎢⎢⎢⎢⎣
0 1 0 0 0
0 0 1 0 0
0 0 0 0 0
0 0 0 0 1
0 0 0 0 0
⎤
⎥⎥⎥⎥⎦
,
(3.51)

78
3 Basic Properties of Matrices
in which I have indicated four submatrices of interest.
All nilpotent matrices have a certain relationship to matrices of the form
of A in equation (3.51). We will identify that form here, but we will not discuss
that form further. Notice two submatrices of A:
N1 =
⎡
⎣
0 1 0
0 0 1
0 0 0
⎤
⎦
and
N2 =
! 0 1
0 0
"
(3.52)
so
A =
!
N1 0
0 N2
"
.
Matrices of the form of N1 and N2, consisting of all 0s except for 1s in the
supradiagonal, are called Jordan blocks and the nilpotent matrix A is said be
in Jordan form. An important property, which we will merely state without
proof, is that the index of a nilpotent matrix in Jordan form is the number of
1s in the largest Jordan block.
A nilpotent matrix is necessarily singular. Nilpotent matrices have many
other simple properties, some of which we will list on page 174.
3.2.1.3 Matrix Polynomials
Polynomials in square matrices are similar to the more familiar polynomials
in scalars. We may consider
p(A) = b0I + b1A + · · · bkAk.
The value of this polynomial is a matrix.
The theory of polynomials in general holds, and in particular, we have the
useful factorizations of monomials: for any positive integer k,
I −Ak = (I −A)(I + A + · · · Ak−1),
(3.53)
and for an odd positive integer k,
I + Ak = (I + A)(I −A + · · · Ak−1).
(3.54)
3.2.2 Multiplication of Matrices with Special Patterns
Various properties of matrices may or may not be preserved under matrix
multiplication. We have seen already that the product of symmetric matrices
is not in general symmetric.
Many of the various patterns of zeroes in matrices discussed on page 58 are
preserved under matrix multiplication. Assume A and B are square matrices
of the same number of rows.

3.2 Multiplication of Matrices
79
•
If A and B are diagonal, AB is diagonal and the (i, i) element of AB is
aiibii;
•
if A and B are block diagonal with conformable blocks, AB is block diag-
onal;
•
if A and B are upper triangular, AB is upper triangular and the (i, i)
element of AB is aiibii;
•
if A and B are lower triangular, AB is lower triangular and the (i, i)
element of AB is aiibii;
•
if A is upper triangular and B is lower triangular, in general, none of AB,
BA, ATA, BTB, AAT, and BBT is triangular.
Each of these statements can be easily proven using the deﬁnition of multipli-
cation in equation (3.43). An important special case of diagonal and triangular
matrices is one in which all diagonal elements are 1. Such a diagonal matrix is
the identity, of course, so it a very special multiplicative property. Triangular
matrices with 1s on the diagonal are called “unit triangular” matrices, and
they are often used in matrix factorizations, as we see later in this chapter
and in Chap. 5.
The products of banded matrices are generally banded with a wider band-
width. If the bandwidth is too great, obviously the matrix can no longer be
called banded.
3.2.2.1 Multiplication of Partitioned Matrices
Multiplication and other operations with partitioned matrices are carried out
with their submatrices in the obvious way. Thus, assuming the submatrices
are conformable for multiplication,
!A11 A12
A21 A22
" ! B11 B12
B21 B22
"
=
!A11B11 + A12B21
A11B12 + A12B22
A21B11 + A22B21
A21B12 + A22B22
"
.
It is clear that the product of conformable block diagonal matrices is block
diagonal.
Sometimes a matrix may be partitioned such that one partition is just a
single column or row, that is, a vector or the transpose of a vector. In that
case, we may use a notation such as
[X y]
or
[X | y],
where X is a matrix and y is a vector. We develop the notation in the obvious
fashion; for example,
[X y]T [X y] =
!XTX XTy
yTX yTy
"
.
(3.55)

80
3 Basic Properties of Matrices
3.2.3 Elementary Operations on Matrices
Many common computations involving matrices can be performed as a se-
quence of three simple types of operations on either the rows or the columns
of the matrix:
•
the interchange of two rows (columns),
•
a scalar multiplication of a given row (column), and
•
the replacement of a given row (column) by the sum of that row (columns)
and a scalar multiple of another row (column); that is, an axpy operation.
Such an operation on the rows of a matrix can be performed by premultipli-
cation by a matrix in a standard form, and an operation on the columns of
a matrix can be performed by postmultiplication by a matrix in a standard
form. To repeat:
•
premultiplication: operation on rows;
•
postmultiplication: operation on columns.
The matrix used to perform the operation is called an elementary trans-
formation matrix or elementary operator matrix. Such a matrix is the identity
matrix transformed by the corresponding operation performed on its unit
rows, eT
p , or columns, ep.
In actual computations, we do not form the elementary transformation
matrices explicitly, but their formulation allows us to discuss the operations
in a systematic way and better understand the properties of the operations.
Products of any of these elementary operator matrices can be used to eﬀect
more complicated transformations.
Operations on the rows are more common, and that is what we will dis-
cuss here, although operations on columns are completely analogous. These
transformations of rows are called elementary row operations.
3.2.3.1 Interchange of Rows or Columns: Permutation Matrices
By ﬁrst interchanging the rows or columns of a matrix, it may be possible
to partition the matrix in such a way that the partitions have interesting
or desirable properties. Also, in the course of performing computations on a
matrix, it is often desirable to interchange the rows or columns of the matrix.
(This is an instance of “pivoting”, which will be discussed later, especially
in Chap. 6.) In matrix computations, we almost never actually move data
from one row or column to another; rather, the interchanges are eﬀected by
changing the indexes to the data.
Interchanging two rows of a matrix can be accomplished by premultiply-
ing the matrix by a matrix that is the identity with those same two rows
interchanged; for example,

3.2 Multiplication of Matrices
81
⎡
⎢⎢⎣
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
a41 a42 a43
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
a11 a12 a13
a31 a32 a33
a21 a22 a23
a41 a42 a43
⎤
⎥⎥⎦.
The ﬁrst matrix in the expression above is called an elementary permutation
matrix. It is the identity matrix with its second and third rows (or columns)
interchanged. An elementary permutation matrix, which is the identity with
the pth and qth rows interchanged, is denoted by Epq. That is, Epq is the
identity, except the pth row is eT
q and the qth row is eT
p . Note that Epq = Eqp.
Thus, for example, if the given matrix is 4×m, to interchange the second and
third rows, we use
E23 = E32 =
⎡
⎢⎢⎣
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
⎤
⎥⎥⎦.
It is easy to see from the deﬁnition that an elementary permutation matrix
is symmetric. Note that the notation Epq does not indicate the order of the
elementary permutation matrix; that must be speciﬁed in the context.
Premultiplying a matrix A by a (conformable) Epq results in an inter-
change of the pth and qth rows of A as we see above. Any permutation of rows
of A can be accomplished by successive premultiplications by elementary per-
mutation matrices. Note that the order of multiplication matters. Although
a given permutation can be accomplished by diﬀerent elementary permuta-
tions, the number of elementary permutations that eﬀect a given permutation
is always either even or odd; that is, if an odd number of elementary per-
mutations results in a given permutation, any other sequence of elementary
permutations to yield the given permutation is also odd in number. Any given
permutation can be eﬀected by successive interchanges of adjacent rows.
Postmultiplying a matrix A by a (conformable) Epq results in an inter-
change of the pth and qth columns of A:
⎡
⎢⎢⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
a41 a42 a43
⎤
⎥⎥⎦
⎡
⎣
1 0 0
0 0 1
0 1 0
⎤
⎦=
⎡
⎢⎢⎣
a11 a13 a12
a21 a23 a22
a31 a33 a32
a41 a43 a42
⎤
⎥⎥⎦.
Note that
A = EpqEpqA = AEpqEpq;
(3.56)
that is, as an operator, an elementary permutation matrix is its own inverse
operator: EpqEpq = I.
Because all of the elements of a permutation matrix are 0 or 1, the trace
of an n × n elementary permutation matrix is n −2.
The product of elementary permutation matrices is also a permutation
matrix in the sense that it permutes several rows or columns. For example,

82
3 Basic Properties of Matrices
premultiplying A by the matrix Q = EpqEqr will yield a matrix whose pth row
is the rth row of the original A, whose qth row is the pth row of A, and whose
rth row is the qth row of A. We often use the notation E(π) to denote a more
general permutation matrix. This expression will usually be used generically,
but sometimes we will specify the permutation, π.
A general permutation matrix (that is, a product of elementary permuta-
tion matrices) is not necessarily symmetric, but its transpose is also a per-
mutation matrix. It is not necessarily its own inverse, but its permutations
can be reversed by a permutation matrix formed by products of permutation
matrices in the opposite order; that is,
ET
(π)E(π) = I.
As a prelude to other matrix operations, we often permute both rows and
columns, so we often have a representation such as
B = E(π1)AE(π2),
(3.57)
where E(π1) is a permutation matrix to permute the rows and E(π2) is a per-
mutation matrix to permute the columns. We use these kinds of operations to
form a full rank partitioning as in equation (3.131) on page 104, to obtain an
equivalent canonical form as in equation (3.151) on page 110 and LDU decom-
position of a matrix as in equation (5.32) on page 246. These equations are
used to determine the number of linearly independent rows and columns and
to represent the matrix in a form with a maximal set of linearly independent
rows and columns clearly identiﬁed.
3.2.3.2 The Vec-Permutation Matrix
A special permutation matrix is the matrix that transforms the vector vec(A)
into vec(AT). If A is n × m, the matrix Knm that does this is nm × nm. We
have
vec(AT) = Knmvec(A).
(3.58)
The matrix Knm is called the nm vec-permutation matrix.
3.2.3.3 Scalar Row or Column Multiplication
Often, numerical computations with matrices are more accurate if the rows
have roughly equal norms. For this and other reasons, we often transform a
matrix by multiplying one of its rows by a scalar. This transformation can also
be performed by premultiplication by an elementary transformation matrix.
For multiplication of the pth row by the scalar, the elementary transformation
matrix, which is denoted by Ep(a), is the identity matrix in which the pth
diagonal element has been replaced by a. Thus, for example, if the given
matrix is 4 × m, to multiply the second row by a, we use

3.2 Multiplication of Matrices
83
E2(a) =
⎡
⎢⎢⎣
1 0 0 0
0 a 0 0
0 0 1 0
0 0 0 1
⎤
⎥⎥⎦.
Postmultiplication of a given matrix by the multiplier matrix Ep(a) results
in the multiplication of the pth column by the scalar. For this, Ep(a) is a square
matrix of order equal to the number of columns of the given matrix.
Note that the notation Ep(a) does not indicate the number of rows and
columns. This must be speciﬁed in the context.
Note that, if a ̸= 0,
A = Ep(1/a)Ep(a)A,
(3.59)
that is, as an operator, the inverse operator is a row multiplication matrix on
the same row and with the reciprocal as the multiplier.
3.2.3.4 Axpy Row or Column Transformations
The other elementary operation is an axpy on two rows and a replacement of
one of those rows with the result
ap ←aaq + ap.
This operation also can be eﬀected by premultiplication by a matrix formed
from the identity matrix by inserting the scalar in the (p, q) position. Such a
matrix is denoted by Epq(a). Thus, for example, if the given matrix is 4 × m,
to add a times the third row to the second row, we use
E23(a) =
⎡
⎢⎢⎣
1 0 0 0
0 1 a 0
0 0 1 0
0 0 0 1
⎤
⎥⎥⎦.
Premultiplication of a matrix A by such a matrix,
Epq(a)A,
(3.60)
yields a matrix whose pth row is a times the qth row plus the original row.
Given the 4 × 3 matrix A = (aij), we have
E23(a)A =
⎡
⎢⎢⎣
a11
a12
a13
a21 + aa31 a22 + aa32 a23 + aa33
a31
a32
a33
a41
a42
a43
⎤
⎥⎥⎦.
Postmultiplication of a matrix A by an axpy operator matrix,
AEpq(a),

84
3 Basic Properties of Matrices
yields a matrix whose qth column is a times the pth column plus the original
column. For this, Epq(a) is a square matrix of order equal to the number of
columns of the given matrix. Note that the column that is changed corresponds
to the second subscript in Epq(a).
Note that
A = Epq(−a)Epq(a)A;
(3.61)
that is, as an operator, the inverse operator is the same axpy elementary
operator matrix with the negative of the multiplier.
A common use of axpy operator matrices is to form a matrix with zeros
in all positions of a given column below a given position in the column. These
operations usually follow an operation by a scalar row multiplier matrix that
puts a 1 in the position of interest. For example, given an n × m matrix A
with aij ̸= 0, to put a 1 in the (i, j) position and 0s in all positions of the jth
column below the ith row, we form the product
Eni(−anj) · · · Ei+1,i(−ai+1,j)Ei(1/aij)A.
(3.62)
This process is called Gaussian elimination.
The matrix
Gij = Eni(−anj) · · · Ei+1,i(−ai+1,j)Ei(1/aij)
(3.63)
is called a Gaussian transformation matrix. Notice that it is lower triangular,
and its inverse, also lower triangular, is
G−1
ij = Ei(aij)Ei+1,i(ai+1,j) · · · Eni(anj)
(3.64)
Gaussian elimination is often performed sequentially down the diagonal
elements of a matrix (see its use in the LU factorization on page 244, for
example).
To form a matrix with zeros in all positions of a given column except one,
we use additional matrices for the rows above the given element:
Gij = Eni(−anj) · · · Ei+1,i(−ai+1,j)Ei−1,i(−ai−1,j) · · · E1i(−a1j)Ei(1/aij).
(3.65)
This is also called a Gaussian transformation matrix.
We can likewise zero out all elements in the ith row except the one in the
(ij)th position by similar postmultiplications.
If at some point aii = 0, the operations of equation (3.62) cannot be
performed. In that case, we may ﬁrst interchange the ith row with the kth
row, where k > i and aki ̸= 0. Such an interchange is called pivoting. We will
discuss pivoting in more detail on page 277 in Chap. 6.
As we mentioned above, in actual computations, we do not form the ele-
mentary transformation matrices explicitly, but their formulation allows us to
discuss the operations in a systematic way and better understand the prop-
erties of the operations.

3.2 Multiplication of Matrices
85
This is an instance of a principle that we will encounter repeatedly: the
form of a mathematical expression and the way the expression should be eval-
uated in actual practice may be quite diﬀerent.
These elementary transformations are the basic operations in Gaussian
elimination, which is discussed in Sects. 5.7 and 6.2.1.
3.2.3.5 Elementary Operator Matrices: Summary of Notation and
Properties
Because we have introduced various notation for elementary operator matri-
ces, it may be worthwhile to review the notation. The notation is useful and
I will use it from time to time, but unfortunately, there is no general form for
the notation. I will generally use an “E” as the root symbol for the matrix,
but the speciﬁc type is indicated by various other symbols.
Referring back to the listing of the types of operations on page 80, we have
the various elementary operator matrices:
•
Epq: the interchange of rows p and q (Epq is the same as Eqp)
Epq = Eqp =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 · · · 0 · · · 0 0
0 1 · · · 0 · · · 0 · · · 0 0
...
... ... ... ... ... ... ...
...
0 0 · · · 0 · · · 1 · · · 0 0
... ... ... ... ... ... ... ... ...
0 0 · · · 1 · · · 0 · · · 0 0
...
... ... ... ... ... ... ...
...
0 0 · · · 0 · · · 0 · · · 1 0
0 0 · · · 0 · · · 0 · · · 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
p
q
(3.66)
It is symmetric,
ET
pq = Epq,
(3.67)
and it is its own inverse,
E−1
pq = Epq,
(3.68)
that is, it is orthogonal.
E(π): a general permutation of rows, where π denotes a permutation. We
have
E(π) = Ep1q1 · · · Epkqk, for some p1, . . . , pk and q1, . . . , qk.
(3.69)
•
Ep(a): multiplication of row p by a.

86
3 Basic Properties of Matrices
Ep(a) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 · · · 0 0
0 1 · · · 0 · · · 0 0
... ... ... ... ... ... ...
0 0 · · · a · · · 0 0
... ... ... ... ... ... ...
0 0 · · · 0 · · · 1 0
0 0 · · · 0 · · · 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
p
(3.70)
Its inverse is
E−1
p (a) = Ep(1/a).
(3.71)
•
Epq(a): the replacement of row p by the sum of row p and a times row q.
If q > p,
Epq(a) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 · · · 0 · · · 0 0
0 1 · · · 0 · · · 0 · · · 0 0
... ... ... ... ... ... ... ... ...
0 0 · · · 1 · · · a · · · 0 0
...
... ... ... ... ... ... ...
...
0 0 · · · 0 · · · 1 · · · 0 0
... ... ... ... ... ... ... ... ...
0 0 · · · 0 · · · 0 · · · 1 0
0 0 · · · 0 · · · 0 · · · 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
p
q
(3.72)
Its inverse is
E−1
pq (a) = Epq(−a).
(3.73)
Recall that these operations are eﬀected by premultiplication. The same kinds
of operations on the columns are eﬀected by postmultiplication.
3.2.3.6 Determinants of Elementary Operator Matrices
The determinant of an elementary permutation matrix Epq has only one term
in the sum that deﬁnes the determinant (equation (3.24), page 66), and that
term is 1 times σ evaluated at the permutation that exchanges p and q. As
we have seen (page 67), this is an odd permutation; hence, for an elementary
permutation matrix Epq,
det(Epq) = −1.
(3.74)
Because a general permutation matrix E(π) can be formed as the product
of elementary permutation matrices which together form the permutation π,
we have from equation (3.74)
det(Eπ) = σ(π),
(3.75)

3.2 Multiplication of Matrices
87
where σ(π) = 1 if π is an even permutation and −1 otherwise, as deﬁned in
equation (3.23).
Because all terms in det(EpqA) are exactly the same terms as in det(A)
but with one diﬀerent permutation in each term, we have
det(EpqA) = −det(A).
More generally, if A and E(π) are n × n matrices, and E(π) is any permuta-
tion matrix (that is, any product of Epq matrices), then det(E(π)A) is either
det(A) or −det(A) because all terms in det(E(π)A) are exactly the same as
the terms in det(A) but possibly with diﬀerent signs because the permutations
are diﬀerent. In fact, the diﬀerences in the permutations are exactly the same
as the permutation of 1, . . . , n in E(π); hence,
det(E(π)A) = det(E(π)) det(A)
= σ(π)det(A).
(In equation (3.81) below, we will see that this equation holds more generally.)
The determinant of an elementary row multiplication matrix Ep(a) is
det(Ep(a)) = a.
(3.76)
If A and Ep(a) are n × n matrices, then
det(Ep(a)A) = adet(A),
as we see from the deﬁnition of the determinant, equation (3.24). (Again, this
also follows from the general result in equation (3.81) below.)
The determinant of an elementary axpy matrix Epq(a) is 1,
det(Epq(a)) = 1,
(3.77)
because the term consisting of the product of the diagonals is the only term
in the determinant.
Now consider det(Epq(a)A) for an n×n matrix A. Expansion in the minors
(equation (3.30)) along the pth row yields
det(Epq(a)A) =
n

j=1
(apj + aaqj)(−1)p+jdet(A(ij))
=
n

j=1
apj(−1)p+jdet(A(ij)) + a
n

j=1
aqj(−1)p+jdet(A(ij)).
From equation (3.32) on page 69, we see that the second term is 0, and since
the ﬁrst term is just the determinant of A, we have
det(Epq(a)A) = det(A).
(3.78)
(Again, this also follows from the general result in equation (3.81) below. I
have shown the steps in the speciﬁc case because I think they help to see the
eﬀect of the elementary operator matrix.)

88
3 Basic Properties of Matrices
3.2.4 The Trace of a Cayley Product That Is Square
A useful property of the trace for the matrices A and B that are conformable
for the multiplications AB and BA is
tr(AB) = tr(BA).
(3.79)
This is obvious from the deﬁnitions of matrix multiplication and the trace.
Note that A and B may not be square (so the trace is not deﬁned for them),
but if they are conformable for the multiplications, then both AB and BA
are square.
Because of the associativity of matrix multiplication, this relation can be
extended as
tr(ABC) = tr(BCA) = tr(CAB)
(3.80)
for matrices A, B, and C that are conformable for the multiplications indi-
cated. Notice that the individual matrices need not be square. This fact is
very useful in working with quadratic forms, as in equation (3.90).
3.2.5 The Determinant of a Cayley Product of Square Matrices
An important property of the determinant is
det(AB) = det(A) det(B)
(3.81)
if A and B are square matrices conformable for multiplication. We see this by
ﬁrst forming
det
!
I A
0 I
" !
A 0
−I B
"
= det
!
0 AB
−I
B
"
(3.82)
and then observing from equation (3.39) that the right-hand side is det(AB).
Now consider the left-hand side. The matrix that is the ﬁrst factor on the
left-hand side is a product of elementary axpy transformation matrices; that
is, it is a matrix that when postmultiplied by another matrix merely adds
multiples of rows in the lower part of the matrix to rows in the upper part of
the matrix. If A and B are n × n (and so the identities are likewise n × n),
the full matrix is the product:
!
I A
0 I
"
= E1,n+1(a11) · · · E1,2n(a1n)E2,n+1(a21) · · · E2,2n(a2,n) · · · En,2n(ann).
Hence, applying equation (3.78) recursively, we have
det
!
I A
0 I
" !
A 0
−I B
"
= det
!
A 0
−I B
"
,
and from equation (3.38) we have

3.2 Multiplication of Matrices
89
det
! A 0
−I B
"
= det(A)det(B),
and so ﬁnally we have equation (3.81).
From equation (3.81), we see that if A and B are square matrices con-
formable for multiplication, then
det(AB) = det(BA).
(3.83)
(Recall, in general, even in the case of square matrices, AB ̸= BA.) This
equation is to be contrasted with equation (3.79), tr(AB) = tr(BA), which
does not even require that the matrices be square. A simple counterexample
for nonsquare matrices is det(xxT) ̸= det(xTx), where x is a vector with
at least two elements. (Here, think of the vector as an n × 1 matrix. This
counterexample can be seen in various ways. One way is to use a fact that we
will encounter on page 117, and observe that det(xxT) = 0 for any x with at
least two elements.)
3.2.6 Multiplication of Matrices and Vectors
It is often convenient to think of a vector as a matrix with only one element
in one of its dimensions. This provides for an immediate extension of the def-
initions of transpose and matrix multiplication to include vectors as either
or both factors. In this scheme, we follow the convention that a vector corre-
sponds to a column; that is, if x is a vector and A is a matrix, Ax or xTA may
be well-deﬁned, but neither xA nor AxT would represent anything, except
in the case when all dimensions are 1. In some computer systems for matrix
algebra, these conventions are not enforced; in others, they are not. (R, for
example sometimes does and sometimes does not; see the discussion beginning
on page 572.) The alternative notation xTy we introduced earlier for the dot
product or inner product, ⟨x, y⟩, of the vectors x and y is consistent with this
paradigm.
Vectors and matrices are fundamentally diﬀerent kinds of mathematical
objects. In general, it is not relevant to say that a vector is a “column” or
a “row”; it is merely a one-dimensional (or rank 1) object. We will continue
to write vectors as x = (x1, . . . , xn), but this does not imply that the vector
is a “row vector”. Matrices with just one row or one column are diﬀerent
objects from vectors. We represent a matrix with one row in a form such as
Y = [y11 . . . y1n], and we represent a matrix with one column in a form such
as Z =
⎡
⎢⎣
z11
...
zm1
⎤
⎥⎦or as Z = [z11 . . . zm1]T.
(Compare the notation in equations (1.1) and (1.2) on page 4.)

90
3 Basic Properties of Matrices
3.2.6.1 The Matrix/Vector Product as a Linear Combination
If we represent the vectors formed by the columns of an n × m matrix A
as a1, . . . , am, the matrix/vector product Ax is a linear combination of these
columns of A:
Ax =
m

i=1
xiai.
(3.84)
(Here, each xi is a scalar, and each ai is a vector.)
Given the equation Ax = b, we have b ∈span(A); that is, the n-vector b
is in the k-dimensional column space of A, where k ≤m.
3.2.6.2 The Matrix as a Mapping on Vector Spaces
In this chapter we have considered matrices to be fundamental objects. Only
after deﬁning operations on matrices themselves have we deﬁned an operation
by a matrix on a vector. Another way of thinking about matrices is as a class
of functions or mappings on vector spaces. In this approach, we give primacy
to the vector spaces.
Let V1 and V2 be vector spaces of order m and n respectively. Then an
n × m matrix A is a function from V1 to V2 deﬁned for x ∈V1 as
x →Ax.
(3.85)
Matrices are “transformations” of vectors. There is nothing essentially
diﬀerent in this development of concepts about matrices; it does, however,
motivate terminology based in geometry that we will use from time to time
(“rotations”, “projections”, and so on; see Sect. 5.3).
A matrix changes the “direction” of a vector. The cosine of the angle
between the vector x and the vector Ax is the correlation
Cor(x, Ax) = (x −¯x)TA(x −¯x)
(x −¯x)T(x −¯x) ,
see page 51. (This expression is the Rayleigh quotient, RA(x −¯x), page 157.)
3.2.7 Outer Products
The outer product of the vectors x and y is the matrix
xyT.
(3.86)
Note that the deﬁnition of the outer product does not require the vectors to
be of equal length. Note also that while the inner product is commutative,
the outer product is not commutative (although it does have the property
xyT = (yxT)T).

3.2 Multiplication of Matrices
91
While the inner product is a mapping from IRn × IRn to IR, the outer
product of two vectors is a mapping
IRn × IRm →M ⊆IRn×m,
where M is the set of n× m matrices of rank one. (We will deﬁne and discuss
matrix rank in Sect. 3.3, beginning on page 99. Also, see Exercise 3.14.)
A very common outer product is of a vector with itself:
xxT.
The outer product of a vector with itself is obviously a symmetric matrix.
We should again note some subtleties of diﬀerences in the types of objects
that result from operations. If A and B are matrices conformable for the
operation, the product ATB is a matrix even if both A and B are n × 1 and
so the result is 1 × 1. For the vectors x and y and matrix C, however, xTy
and xTCy are scalars; hence, the dot product and a quadratic form are not
the same as the result of a matrix multiplication. The dot product is a scalar,
and the result of a matrix multiplication is a matrix. The outer product of
vectors is a matrix, even if both vectors have only one element. Nevertheless,
as we have mentioned before, we will treat a one by one matrix or a vector
with only one element as a scalar whenever it is convenient to do so.
3.2.8 Bilinear and Quadratic Forms: Deﬁniteness
Given a matrix A of conformable shape, a variation of the vector dot product,
xTAy, is called a bilinear form, and the special bilinear form xTAx is called
a quadratic form. Note
xTATx = xTAx
and
xTATy = yTAx ̸= xTAy
in general.
Although in the deﬁnition of quadratic form we do not require A to be
symmetric—because for a given value of x and a given value of the quadratic
form xTAx there is a unique symmetric matrix As such that xTAsx = xTAx—
we generally work only with symmetric matrices in dealing with quadratic
forms. (The matrix As is 1
2(A + AT); see Exercise 3.3.) Quadratic forms cor-
respond to sums of squares and hence play an important role in statistical
applications.
3.2.8.1 Nonnegative Deﬁnite and Positive Deﬁnite Matrices
A symmetric matrix A such that for any (conformable and real) vector x the
quadratic form xTAx is nonnegative, that is,
xTAx ≥0,
(3.87)

92
3 Basic Properties of Matrices
is called a nonnegative deﬁnite matrix. (There is another term, “positive
semideﬁnite matrix” and its acronym PSD, that is often used to mean “non-
negative deﬁnite matrix”, but the term is not used consistently in the litera-
ture. I will generally avoid the term “semideﬁnite”.) We denote the fact that
A is nonnegative deﬁnite by
A ⪰0.
(Note that we consider 0n×n to be nonnegative deﬁnite.)
A symmetric matrix A such that for any (conformable) vector x ̸= 0 the
quadratic form
xTAx > 0
(3.88)
is called a positive deﬁnite matrix. We denote the fact that A is positive
deﬁnite by
A ≻0.
(Recall that A ≥0 and A > 0 mean, respectively, that all elements of A are
nonnegative and positive.)
Nonnegative and positive deﬁnite matrices are very important in applica-
tions. We will encounter them from time to time in this chapter, and then we
will discuss more of their properties in Sect. 8.3.
In this book we use the terms “nonnegative deﬁnite” and “positive deﬁ-
nite” only for symmetric matrices. In other literature, these terms may be used
more generally; that is, for any (square) matrix that satisﬁes (3.87) or (3.88).
3.2.8.2 Ordinal Relations among Symmetric Matrices
When A and B are symmetric matrices of the same order, we write A ⪰B to
mean A −B ⪰0 and A ≻B to mean A −B ≻0.
The ⪰relationship is a partial ordering and the ≻relationship is transi-
tive; that is, if for conformable matrices, A ≻B and B ≻C, then A ≻C
(See Exercise 8.2 on page 396; also compare ordinal relations among vectors,
page 16.)
3.2.8.3 The Trace of Inner and Outer Products
The invariance of the trace to permutations of the factors in a product (equa-
tion (3.79)) is particularly useful in working with bilinear and quadratic forms.
Let A be an n × m matrix, x be an n-vector, and y be an m-vector. Because
the bilinear form is a scalar (or a 1×1 matrix), and because of the invariance,
we have the very useful fact
xTAy = tr(xTAy)
= tr(AyxT).
(3.89)
A common instance is when A is square and x = y. We have for the quadratic
form the equality

3.2 Multiplication of Matrices
93
xTAx = tr(AxxT).
(3.90)
In equation (3.90), if A is the identity I, we have that the inner product of
a vector with itself is the trace of the outer product of the vector with itself,
that is,
xTx = tr(xxT).
(3.91)
Also, by letting A be the identity in equation (3.90), we have an alternative
way of showing that for a given vector x and any scalar a, the norm ∥x −a∥
is minimized when a = ¯x:
(x −a)T(x −a) = tr(xcxT
c ) + n(a −¯x)2.
(3.92)
(Here, “¯x” denotes the mean of the elements in x, and “xc” is x−¯x. Compare
this with equation (2.71) on page 48.)
3.2.9 Anisometric Spaces
In Sect. 2.1, we considered various properties of vectors that depend on the
inner product, such as orthogonality of two vectors, norms of a vector, angles
between two vectors, and distances between two vectors. All of these prop-
erties and measures are invariant to the orientation of the vectors; the space
is isometric with respect to a Cartesian coordinate system. Noting that for
real vectors the inner product is the bilinear form xTIy, we have a heuristic
generalization to an anisometric space. Suppose, for example, that the scales
of the coordinates diﬀer; say, a given distance along one axis in the natural
units of the axis is equivalent (in some sense depending on the application) to
twice that distance along another axis, again measured in the natural units of
the axis. The properties derived from the inner product, such as a norm and
a metric, may correspond to the application better if we use a bilinear form in
which the matrix reﬂects the diﬀerent eﬀective distances along the coordinate
axes. A diagonal matrix whose entries have relative values corresponding to
the inverses of the relative scales of the axes may be more useful. Instead of
xTy, we may use xTDy, where D is this diagonal matrix.
Rather than diﬀerences in scales being just in the directions of the co-
ordinate axes, more generally we may think of anisometries being measured
by general (but perhaps symmetric) matrices. (The covariance and correla-
tion matrices deﬁned on page 367 come to mind.) Any such matrix to be
used in this context should be positive deﬁnite because we will generalize the
dot product, which is necessarily nonnegative, in terms of a quadratic form.
A bilinear form xTAy may correspond more closely to the properties of the
application than the standard inner product.
3.2.9.1 Conjugacy
We deﬁne orthogonality of two vectors real vectors x and y with respect
to A by

94
3 Basic Properties of Matrices
xTAy = 0.
(3.93)
In this case, we say x and y are A-conjugate.
The L2 norm of a vector is the square root of the quadratic form of the
vector with respect to the identity matrix. A generalization of the L2 vector
norm, called an elliptic norm or a conjugate norm, is deﬁned for the vector
x as the square root of the quadratic form xTAx for any symmetric positive
deﬁnite matrix A. It is sometimes denoted by ∥x∥A:
∥x∥A =
√
xTAx.
(3.94)
It is easy to see that ∥x∥A satisﬁes the deﬁnition of a norm given on page 25. If
A is a diagonal matrix with elements wi ≥0, the elliptic norm is the weighted
L2 norm of equation (2.37).
The elliptic norm yields an elliptic metric in the usual way of deﬁning a
metric in terms of a norm. The distance between the real vectors x and y
with respect to A is

(x −y)TA(x −y). It is easy to see that this satisﬁes
the deﬁnition of a metric given on page 32.
A metric that is widely useful in statistical applications is the Mahalanobis
distance, which uses a covariance matrix as the scale for a given space. (The
sample covariance matrix is deﬁned in equation (8.67) on page 367.) If S is
the covariance matrix, the Mahalanobis distance, with respect to that matrix,
between the vectors x and y is
&
(x −y)TS−1(x −y).
(3.95)
3.2.10 Other Kinds of Matrix Multiplication
The most common kind of product of two matrices is the Cayley product,
and when we speak of matrix multiplication without qualiﬁcation, we mean
the Cayley product. Three other types of matrix multiplication that are use-
ful are Hadamard multiplication, Kronecker multiplication, and inner product
multiplication.
3.2.10.1 The Hadamard Product
Hadamard multiplication is deﬁned for matrices of the same shape as the
multiplication of each element of one matrix by the corresponding element
of the other matrix. Hadamard multiplication is often denoted by ⊙; for two
matrices An×m and Bn×m we have
A ⊙B =
⎡
⎢⎣
a11b11 . . . a1mb1m
...
. . .
...
an1bn1 . . . anmbnm
⎤
⎥⎦.

3.2 Multiplication of Matrices
95
Hadamard multiplication immediately inherits the commutativity, asso-
ciativity, and distribution over addition of the ordinary multiplication of the
underlying ﬁeld of scalars. Hadamard multiplication is also called array mul-
tiplication and element-wise multiplication. Hadamard matrix multiplication
is a mapping
IRn×m × IRn×m →IRn×m.
The identity for Hadamard multiplication is the matrix of appropriate
shape whose elements are all 1s.
3.2.10.2 The Kronecker Product
Kronecker multiplication, denoted by ⊗, is deﬁned for any two matrices An×m
and Bp×q as
A ⊗B =
⎡
⎢⎣
a11B . . . a1mB
...
. . .
...
an1B . . . anmB
⎤
⎥⎦.
The Kronecker product of A and B is np × mq; that is, Kronecker matrix
multiplication is a mapping
IRn×m × IRp×q →IRnp×mq.
The Kronecker product is also called the “right direct product” or just
direct product. (A left direct product is a Kronecker product with the factors
reversed. In some of the earlier literature, “Kronecker product” was used to
mean a left direct product.) Note the similarity of the Kronecker product of
matrices with the direct product of sets, deﬁned on page 5, in the sense that
the result is formed from ordered pairs of elements from the two operands.
Kronecker multiplication is not commutative, but it is associative and it
is distributive over addition, as we will see below. (Again, this parallels the
direct product of sets.)
The identity for Kronecker multiplication is the 1 × 1 matrix with the
element 1; that is, it is the same as the scalar 1.
We can understand the properties of the Kronecker product by expressing
the (i, j) element of A ⊗B in terms of the elements of A and B,
(A ⊗B)i,j = A⌊(i−1)/p⌋+1, ⌊(j−1)/q⌋+1Bi−p⌊(i−1)/p⌋, j−q⌊(j−1)/q⌋.
(3.96)
Some additional properties of Kronecker products that are immediate re-
sults of the deﬁnition are, assuming the matrices are conformable for the
indicated operations,
(aA) ⊗(bB) = ab(A ⊗B)
= (abA) ⊗B
= A ⊗(abB), for scalars a, b,
(3.97)

96
3 Basic Properties of Matrices
(A + B) ⊗(C) = A ⊗C + B ⊗C,
(3.98)
(A ⊗B) ⊗C = A ⊗(B ⊗C),
(3.99)
(A ⊗B)T = AT ⊗BT,
(3.100)
(A ⊗B)(C ⊗D) = AC ⊗BD.
(3.101)
I ⊗A = diag(A, . . . , A).
(3.102)
A ⊗I = (aijI) .
(3.103)
These properties are all easy to see by using equation (3.96) to express the
(i, j) element of the matrix on either side of the equation, taking into account
the size of the matrices involved. For example, in the ﬁrst equation, if A is
n × m and B is p × q, the (i, j) element on the left-hand side is
aA[(i−1)/p]+1, [(j−1)/q]+1bBi−p[(i−1)/p], j−q[(j−1)/q]
and that on the right-hand side is
abA[(i−1)/p]+1, [(j−1)/q]+1Bi−p[(i−1)/p], j−q[(j−1)/q].
They are all this easy! Hence, they are Exercise 3.6.
The determinant of the Kronecker product of two square matrices An×n
and Bm×m has a simple relationship to the determinants of the individual
matrices:
det(A ⊗B) = det(A)mdet(B)n.
(3.104)
The proof of this, like many facts about determinants, is straightforward but
involves tedious manipulation of cofactors. The manipulations in this case can
be facilitated by using the vec-permutation matrix. See Harville (1997) for a
detailed formal proof.
From equation (3.100) we see that the Kronecker product of symmetric
matrices is symmetric.
Another property of the Kronecker product of square matrices is
tr(A ⊗B) = tr(A)tr(B).
(3.105)
This is true because the trace of the product is merely the sum of all possible
products of the diagonal elements of the individual matrices.
The Kronecker product and the vec function often ﬁnd uses in the same
application. For example, an n × m normal random matrix X with parameters

3.2 Multiplication of Matrices
97
M, Σ, and Ψ can be expressed in terms of an ordinary np-variate normal
random variable Y = vec(X) with parameters vec(M) and Σ⊗Ψ. (We discuss
matrix random variables brieﬂy on page 220. For a fuller discussion, the reader
is referred to a text on matrix random variables such as Carmeli 1983, or Kollo
and von Rosen 2005.)
A useful relationship between the vec function and Kronecker multiplica-
tion is
vec(ABC) = (CT ⊗A)vec(B)
(3.106)
for matrices A, B, and C that are conformable for the multiplication indicated.
This is easy to show and is left as an exercise.
3.2.10.3 The Inner Product of Matrices
An inner product of two matrices of the same shape is deﬁned as the sum of
the dot products of the vectors formed from the columns of one matrix with
vectors formed from the corresponding columns of the other matrix; that is,
if a1, . . . , am are the columns of A and b1, . . . , bm are the columns of B, then
the inner product of A and B, denoted ⟨A, B⟩, is
⟨A, B⟩=
m

j=1
⟨aj, bj⟩.
(3.107)
Similarly as for vectors (page 23), the inner product is sometimes called a “dot
product”, and the notation A·B is sometimes used to denote the matrix inner
product. (I generally try to avoid use of the term dot product for matrices
because the term may be used diﬀerently by diﬀerent people. In Matlab, for
example, “dot product”, implemented in the dot function, can refer either to
1×m matrix consisting of the individual terms in the sum in equation (3.107),
or to the n×1 matrix consisting of the dot products of the vectors formed from
the rows of A and B. In the NumPy linear algebra package, the dot function
implements Cayley multiplication! This is probably because someone working
with Python realized the obvious fact that the deﬁning equation of Cayley
multiplication, equation (3.43) on page 75, is actually the dot product of the
vector formed from the elements in the ith row in the ﬁrst matrix and the
vector formed from the elements in the jth column in the ﬁrst matrix.)
For real matrices, equation (3.107) can be written as
⟨A, B⟩=
m

j=1
aT
j bj.
(3.108)
As in the case of the product of vectors, the product of matrices deﬁned as in
equation (3.108) over the complex ﬁeld is not an inner product because the
ﬁrst property (on page 24 or as listed below) does not hold.
For conformable matrices A, B, and C, we can easily conﬁrm that this
product satisﬁes the general properties of an inner product listed on page 24:

98
3 Basic Properties of Matrices
•
If A ̸= 0, ⟨A, A⟩> 0, and ⟨0, A⟩= ⟨A, 0⟩= ⟨0, 0⟩= 0.
•
⟨A, B⟩= ⟨B, A⟩.
•
⟨sA, B⟩= s⟨A, B⟩, for a scalar s.
•
⟨(A + B), C⟩= ⟨A, C⟩+ ⟨B, C⟩.
As with any inner product (restricted to objects in the ﬁeld of the reals),
its value is a real number. Thus the matrix inner product is a mapping
IRn×m × IRn×m →IR.
We see from the deﬁnition above that the inner product of real matrices
satisﬁes
⟨A, B⟩= tr(ATB),
(3.109)
which could alternatively be taken as the deﬁnition.
Rewriting the deﬁnition of ⟨A, B⟩as m
j=1
n
i=1 aijbij, we see that for real
matrices
⟨A, B⟩= ⟨AT, BT⟩.
(3.110)
Like any inner product, inner products of matrices obey the Cauchy-
Schwarz inequality (see inequality (2.26), page 24),
⟨A, B⟩≤⟨A, A⟩
1
2 ⟨B, B⟩
1
2 ,
(3.111)
with equality holding only if A = 0 or B = sA for some scalar s.
3.2.10.4 Orthonormal Matrices
In Sect. 2.1.8, we deﬁned orthogonality and orthonormality of two or more
vectors in terms of inner products. We can likewise deﬁne an orthogonal binary
relationship between two matrices in terms of inner products of matrices. We
say the matrices A and B of the same shape are orthogonal to each other if
⟨A, B⟩= 0.
(3.112)
We also use the term “orthonormal” to refer to matrices that are orthogonal
to each other and for which each has an inner product with itself of 1. In
Sect. 3.7, we will deﬁne orthogonality as a unary property of matrices. The
term “orthogonal”, when applied to matrices, generally refers to that property
rather than the binary property we have deﬁned here. “Orthonormal”, on the
other hand, refers to the binary property.
3.2.10.5 Orthonormal Basis: Fourier Expansion
On page 64 we identiﬁed a vector space of matrices and deﬁned a basis for the
space IRn×m. If {U1, . . . , Uk} is a basis set for M ⊆IRn×m with the property
that ⟨Ui, Uj⟩= 0 for i ̸= j and ⟨Ui, Ui⟩= 1, then the set is an orthonormal
basis set.

3.3 Matrix Rank and the Inverse of a Matrix
99
If A is an n × m matrix, with the Fourier expansion
A =
k

i=1
ciUi,
(3.113)
we have, analogous to equation (2.59) on page 41,
ci = ⟨A, Ui⟩.
(3.114)
The ci have the same properties (such as the Parseval identity, equation (2.60),
for example) as the Fourier coeﬃcients in any orthonormal expansion. Best
approximations within M can also be expressed as truncations of the sum in
equation (3.113) as in equation (2.63). The objective of course is to reduce the
truncation error, and the optimality of the Fourier expansion in this regard
discussed on page 42 holds in the matrix case as well. (The norms in Parseval’s
identity and in measuring the goodness of an approximation are matrix norms
in this case. We discuss matrix norms in Sect. 3.9 beginning on page 164.)
3.3 Matrix Rank and the Inverse of a Matrix
The linear dependence or independence of the vectors forming the rows or
columns of a matrix is an important characteristic of the matrix.
The maximum number of linearly independent vectors (those forming ei-
ther the rows or the columns) is called the rank of the matrix. We use the
notation
rank(A)
to denote the rank of the matrix A. (We have used the term “rank” before to
denote dimensionality of an array. “Rank” as we have just deﬁned it applies
only to a matrix or to a set of vectors, and this is by far the more common
meaning of the word. The meaning is clear from the context, however.)
Because multiplication by a nonzero scalar does not change the linear
independence of vectors, for the scalar a with a ̸= 0, we have
rank(aA) = rank(A).
(3.115)
From results developed in Sect. 2.1, we see that for the n × m matrix A,
rank(A) ≤min(n, m).
(3.116)
The rank of the zero matrix is 0, and the rank of any nonzero matrix is
positive.

100
3 Basic Properties of Matrices
3.3.1 Row Rank and Column Rank
We have deﬁned matrix rank in terms of numbers of linearly independent rows
or columns. This is because the number of linearly independent rows is the
same as the number of linearly independent columns. Although we may use
the terms “row rank” or “column rank”, the single word “rank” is suﬃcient
because they are the same. To see this, assume we have an n × m matrix A
and that there are exactly p linearly independent rows and exactly q linearly
independent columns. We can permute the rows and columns of the matrix
so that the ﬁrst p rows are linearly independent rows and the ﬁrst q columns
are linearly independent and the remaining rows or columns are linearly de-
pendent on the ﬁrst ones. (Recall that applying the same permutation to all
of the elements of each vector in a set of vectors does not change the linear
dependencies over the set.) After these permutations, we have a matrix B
with submatrices W, X, Y , and Z,
B =
! Wp×q
Xp×m−q
Yn−p×q Zn−p×m−q
"
,
(3.117)
where the rows of R = [W|X] correspond to p linearly independent m-vectors
and the columns of C =
!
W
Y
"
correspond to q linearly independent n-vectors.
Without loss of generality, we can assume p ≤q. Now, if p < q, it must be
the case that the columns of W are linearly dependent because there are q
of them, but they have only p elements. Therefore, there is some q-vector
a ̸= 0 such that Wa = 0. Now, since the rows of R are the full set of linearly
independent rows, any row in [Y |Z] can be expressed as a linear combination
of the rows of R, and any row in Y can be expressed as a linear combination
of the rows of W. This means, for some n−p × p matrix T , that Y = T W.
In this case, however, Ca = 0. But this contradicts the assumption that the
columns of C are linearly independent; therefore it cannot be the case that
p < q. We conclude therefore that p = q; that is, that the maximum number
of linearly independent rows is the same as the maximum number of linearly
independent columns.
Because the row rank, the column rank, and the rank of A are all the
same, we have
rank(A) = dim(V(A)),
(3.118)
rank(AT) = rank(A),
(3.119)
dim(V(AT)) = dim(V(A)).
(3.120)
(Note, of course, that in general V(AT) ̸= V(A); the orders of the vector spaces
are possibly diﬀerent.)

3.3 Matrix Rank and the Inverse of a Matrix
101
3.3.2 Full Rank Matrices
If the rank of a matrix is the same as its smaller dimension, we say the matrix
is of full rank. In the case of a nonsquare matrix, we may say the matrix is
of full row rank or full column rank just to emphasize which is the smaller
number.
If a matrix is not of full rank, we say it is rank deﬁcient and deﬁne the
rank deﬁciency as the diﬀerence between its smaller dimension and its rank.
A full rank matrix that is square is called nonsingular, and one that is not
nonsingular is called singular.
A square matrix that is either row or column diagonally dominant is non-
singular. The proof of this is Exercise 3.9. (It’s easy!)
A positive deﬁnite matrix is nonsingular. The proof of this is Exercise 3.10.
Later in this section, we will identify additional properties of square full
rank matrices. (For example, they have inverses and their determinants are
nonzero.)
3.3.3 Rank of Elementary Operator Matrices and Matrix
Products Involving Them
Because within any set of rows of an elementary operator matrix (see
Sect. 3.2.3), for some given column, only one of those rows contains a nonzero
element, the elementary operator matrices are all obviously of full rank (with
the proviso that a ̸= 0 in Ep(a)).
Furthermore, the rank of the product of any given matrix with an elemen-
tary operator matrix is the same as the rank of the given matrix. To see this,
consider each type of elementary operator matrix in turn. For a given matrix
A, the set of rows of EpqA is the same as the set of rows of A; hence, the rank
of EpqA is the same as the rank of A. Likewise, the set of columns of AEpq
is the same as the set of columns of A; hence, again, the rank of AEpq is the
same as the rank of A.
The set of rows of Ep(a)A for a ̸= 0 is the same as the set of rows of A,
except for one, which is a nonzero scalar multiple of the corresponding row
of A; therefore, the rank of Ep(a)A is the same as the rank of A. Likewise,
the set of columns of AEp(a) is the same as the set of columns of A, except
for one, which is a nonzero scalar multiple of the corresponding row of A;
therefore, again, the rank of AEp(a) is the same as the rank of A.
Finally, the set of rows of Epq(a)A for a ̸= 0 is the same as the set of
rows of A, except for one, which is a nonzero scalar multiple of some row of
A added to the corresponding row of A; therefore, the rank of Epq(a)A is the
same as the rank of A. Likewise, we conclude that the rank of AEpq(a) is the
same as the rank of A.
We therefore have that if P and Q are the products of elementary operator
matrices,

102
3 Basic Properties of Matrices
rank(PAQ) = rank(A).
(3.121)
On page 113, we will extend this result to products by any full rank matrices.
3.3.4 The Rank of Partitioned Matrices, Products of
Matrices, and Sums of Matrices
The partitioning in equation (3.117) leads us to consider partitioned matrices
in more detail.
3.3.4.1 Rank of Partitioned Matrices and Submatrices
Let the matrix A be partitioned as
A =
!
A11 A12
A21 A22
"
,
where any pair of submatrices in a column or row may be null (that is, where
for example, it may be the case that A = [A11|A12]). Then the number of
linearly independent rows of A must be at least as great as the number of
linearly independent rows of [A11|A12] and the number of linearly independent
rows of [A21|A22]. By the properties of subvectors in Sect. 2.1.1, the number
of linearly independent rows of [A11|A12] must be at least as great as the
number of linearly independent rows of A11 or A21. We could go through a
similar argument relating to the number of linearly independent columns and
arrive at the inequality
rank(Aij) ≤rank(A).
(3.122)
Furthermore, we see that
rank(A) ≤rank([A11|A12]) + rank([A21|A22])
(3.123)
because rank(A) is the number of linearly independent columns of A, which
is less than or equal to the number of linearly independent rows of [A11|A12]
plus the number of linearly independent rows of [A12|A22]. Likewise, we have
rank(A) ≤rank
!
A11
A21
"
+ rank
!
A12
A22
"
.
(3.124)
In a similar manner, by merely counting the number of independent rows,
we see that, if
V

[A11|A12]T
⊥V

[A21|A22]T
,
then
rank(A) = rank([A11|A12]) + rank([A21|A22]);
(3.125)
and, if
V
!A11
A21
"
⊥V
!A12
A22
"
,
then
rank(A) = rank
!A11
A21
"
+ rank
!A12
A22
"
.
(3.126)

3.3 Matrix Rank and the Inverse of a Matrix
103
3.3.4.2 An Upper Bound on the Rank of Products of Matrices
Because the columns of the product AB are linear combinations of the columns
of A, it is clear that
V(AB) ⊆V(A).
(3.127)
The rank of the product of two matrices is less than or equal to the lesser
of the ranks of the two:
rank(AB) ≤min(rank(A), rank(B)).
(3.128)
This follows from equation (3.127). We can also show this by separately con-
sidering two cases for the n × k matrix A and the k × m matrix B. In one
case, we assume k is at least as large as n and n ≤m, and in the other case
we assume k < n ≤m. In both cases, we represent the rows of AB as k linear
combinations of the rows of B.
From inequality (3.128), we see that the rank of a nonzero outer product
matrix (that is, a matrix formed as the outer product of two nonzero vectors)
is 1.
The bound in inequality (3.128) is sharp, as we can see by exhibiting
matrices A and B such that rank(AB) = min(rank(A), rank(B)), as you are
asked to do in Exercise 3.12a.
Inequality (3.128) provides a useful upper bound on rank(AB). In
Sect. 3.3.11, we will develop a lower bound on rank(AB).
3.3.4.3 An Upper and a Lower Bound on the Rank of Sums of
Matrices
The rank of the sum of two matrices is less than or equal to the sum of their
ranks; that is,
rank(A + B) ≤rank(A) + rank(B).
(3.129)
We can see this by observing that
A + B = [A|B]
!I
I
"
,
and so rank(A + B) ≤rank([A|B]) by equation (3.128), which in turn is
≤rank(A) + rank(B) by equation (3.124).
The bound in inequality (3.129) is sharp, as we can see by exhibiting
matrices A and B such that rank(A + B) = rank(A) + rank(B), as you are
asked to do in Exercise 3.12c.
Using inequality (3.129) and the fact that rank(−B) = rank(B), we write
rank(A−B) ≤rank(A)+rank(B), and so, replacing A in (3.129) by A+B, we
have rank(A) ≤rank(A+B)+rank(B), or rank(A+B) ≥rank(A)−rank(B).
By a similar procedure, we get rank(A + B) ≥rank(B) −rank(A), or

104
3 Basic Properties of Matrices
rank(A + B) ≥|rank(A) −rank(B)|.
(3.130)
The bound in inequality (3.130) is sharp, as we can see by exhibiting
matrices A and B such that rank(A + B) = |rank(A) −rank(B)|, as you are
asked to do in Exercise 3.12d.
3.3.5 Full Rank Partitioning
As we saw above, the matrix W in the partitioned B in equation (3.117) is
square; in fact, it is r × r, where r is the rank of B:
B =
!
Wr×r
Xr×m−r
Yn−r×r Zn−r×m−r
"
.
(3.131)
This is called a full rank partitioning of B.
The matrix B in equation (3.131) has a very special property: the full set
of linearly independent rows are the ﬁrst r rows, and the full set of linearly
independent columns are the ﬁrst r columns.
Any rank r matrix can be put in the form of equation (3.131) by using
permutation matrices as in equation (3.57), assuming that r ≥1. That is, if
A is a nonzero matrix, there is a matrix of the form of B above that has the
same rank. For some permutation matrices E(π1) and E(π2),
B = E(π1)AE(π2).
(3.132)
The inverses of these permutations coupled with the full rank partitioning of
B form a full rank partitioning of the original matrix A.
For a square matrix of rank r, this kind of partitioning implies that there
is a full rank r × r principal submatrix, and the principal submatrix formed
by including any of the remaining diagonal elements is singular. The princi-
pal minor formed from the full rank principal submatrix is nonzero, but if
the order of the matrix is greater than r, a principal minor formed from a
submatrix larger than r × r is zero.
The partitioning in equation (3.131) is of general interest, and we will
use this type of partitioning often. We express an equivalent partitioning of a
transformed matrix in equation (3.151) below.
The same methods as above can be used to form a full rank square subma-
trix of any order less than or equal to the rank. That is, if the n × m matrix
A is of rank r and q ≤r, we can form
E(πr)AE(πc) =
!Sq×q
Tq×m−q
Un−q×r Vn−q×m−q
"
,
(3.133)
where S is of rank q.
It is obvious that the rank of a matrix can never exceed its smaller dimen-
sion (see the discussion of linear independence on page 12). Whether or not

3.3 Matrix Rank and the Inverse of a Matrix
105
a matrix has more rows than columns, the rank of the matrix is the same as
the dimension of the column space of the matrix. (As we have just seen, the
dimension of the column space is necessarily the same as the dimension of the
row space, but the order of the column space is diﬀerent from the order of the
row space unless the matrix is square.)
3.3.6 Full Rank Matrices and Matrix Inverses
We have already seen that full rank matrices have some important properties.
In this section, we consider full rank matrices and matrices that are their
Cayley multiplicative inverses.
3.3.6.1 Solutions of Linear Equations
Important applications of vectors and matrices involve systems of linear equa-
tions:
a11x1 + · · · + a1mxm
?= b1
...
...
...
an1x1 + · · · + anmxm
?= bn
(3.134)
or
Ax
?= b.
(3.135)
In this system, A is called the coeﬃcient matrix. An x that satisﬁes this
system of equations is called a solution to the system. For given A and b, a
solution may or may not exist. From equation (3.84), a solution exists if and
only if the n-vector b is in the k-dimensional column space of A, where k ≤m.
A system for which a solution exists is said to be consistent; otherwise, it is
inconsistent.
We note that if Ax = b, for any conformable y,
yTAx = 0 ⇐⇒yTb = 0.
(3.136)
3.3.6.2 Consistent Systems
A linear system An×mx = b is consistent if and only if
rank([A | b]) = rank(A).
(3.137)
We can see this following the argument above that b ∈V(A); that is, the space
spanned by the columns of A is the same as that spanned by the columns of
A and the vector b. Therefore b must be a linear combination of the columns
of A, and furthermore, the linear combination is a solution to the system
Ax = b. (Note, of course, that it is not necessary that it be a unique linear
combination.)

106
3 Basic Properties of Matrices
Equation (3.137) implies the equivalence of the conditions
[A | b]y = 0 for some y ̸= 0
⇔
Ax = 0 for some x ̸= 0.
(3.138)
A special case that yields equation (3.137) for any b is
rank(An×m) = n,
(3.139)
and so if A is of full row rank, the system is consistent regardless of the value
of b. In this case, of course, the number of rows of A must be no greater than
the number of columns (by inequality (3.116)). A square system in which A is
nonsingular is clearly consistent. (The condition of consistency is also called
“compatibility” of the system; that is, the linear system Ax = b is said to be
compatible if it is consistent.)
A generalization of the linear system Ax = b is AX = B, where B is an
n × k matrix. This is the same as k systems Ax1 = b1, . . . , Axk = bk, where
the xi and the bi are the columns of the respective matrices. Consistency of
AX = B, as above, is the condition for a solution in X to exist, and in that
case the system is also said to be compatible.
It is clear that the system AX = B is consistent if each of the Axi = bi
systems is consistent. Furthermore, if the system is consistent, then every
linear relationship among the rows of A exists among the rows of B; that is,
for any c such that cTA = 0, then cTB = 0. To see this, let c be such that
cTA = 0. We then have cTAX = cTB = 0, and so the same linear relationship
that exists among the rows of A exists among the rows of B.
As above for Ax = b, we also see that the system AX = B is consistent if
and only if any of the following conditions hold:
V(B) ⊆V(A)
(3.140)
V([A | B]) = V(A)
(3.141)
rank([A | B]) = rank(A).
(3.142)
These relations imply that if AX = B is consistent, then for any con-
formable vector c,
cTA = 0 ⇐⇒cTB = 0.
(3.143)
It is clear that this condition also implies that AX = B is consistent (because
right-hand implication of the condition implies the relationship (3.140)).
We discuss methods for solving linear systems in Sect. 3.5 and in Chap. 6.
In the next section, we consider a special case of n × n (square) A when
equation (3.139) is satisﬁed (that is, when A is nonsingular).
3.3.6.3 Matrix Inverses
Let A be an n × n nonsingular matrix, and consider the linear systems
Axi = ei,

3.3 Matrix Rank and the Inverse of a Matrix
107
where ei is the ith unit vector. For each ei, this is a consistent system by
equation (3.137).
We can represent all n such systems as
A
'
x1| · · · |xn
(
=
'
e1| · · · |en
(
or
AX = In,
and this full system must have a solution; that is, there must be an X such
that AX = In. Because AX = I, we call X a “right inverse” of A. The matrix
X must be n × n and nonsingular (because I is); hence, it also has a right
inverse, say Y , and XY = I. From AX = I, we have AXY = Y , so A = Y ,
and so ﬁnally XA = I; that is, the right inverse of A is also the “left inverse”.
We will therefore just call it the inverse of A and denote it as A−1. This is
the Cayley multiplicative inverse. Hence, for an n × n nonsingular matrix A,
we have a matrix A−1 such that
A−1A = AA−1 = In.
(3.144)
The inverse of the nonsingular square matrix A is unique. (This follows from
the argument above about a “right inverse” and a “left inverse”.)
We have already encountered the idea of a matrix inverse in our discussions
of elementary transformation matrices. The matrix that performs the inverse
of the elementary operation is the inverse matrix.
From the deﬁnitions of the inverse and the transpose, we see that
(A−1)T = (AT)−1,
(3.145)
and because in applications we often encounter the inverse of a transpose of
a matrix, we adopt the notation
A−T
to denote the inverse of the transpose.
In the linear system (3.135), if n = m and A is nonsingular, the solution
is
x = A−1b.
(3.146)
For scalars, the combined operations of inversion and multiplication are
equivalent to the single operation of division. From the analogy with scalar op-
erations, we sometimes denote AB−1 by A/B. Because matrix multiplication
is not commutative, we often use the notation “\” to indicate the combined
operations of inversion and multiplication on the left; that is, B\A is the same
as B−1A. The solution given in equation (3.146) is also sometimes represented
as A\b.
We discuss the solution of systems of equations in Chap. 6, but here we
will point out that when we write an expression that involves computations to

108
3 Basic Properties of Matrices
evaluate it, such as A−1b or A\b, the form of the expression does not specify
how to do the computations. This is an instance of a principle that we will
encounter repeatedly: the form of a mathematical expression and the way the
expression should be evaluated in actual practice may be quite diﬀerent.
3.3.6.4 Nonsquare Full Rank Matrices: Right and Left Inverses
Suppose A is n × m and rank(A) = n; that is, n ≤m and A is of full row
rank. Then rank([A | ei]) = rank(A), where ei is the ith unit vector of length
n; hence the system
Axi = ei
is consistent for each ei, and, as before, we can represent all n such systems
as
A
'
x1| · · · |xn
(
=
'
e1| · · · |en
(
or
AX = In.
As above, there must be an X such that AX = In, and we call X a right
inverse of A. The matrix X must be m × n and it must be of rank n (because
I is). This matrix is not necessarily the inverse of A, however, because A and
X may not be square. We denote the right inverse of A as
A−R.
Furthermore, we could only have solved the system AX if A was of full row
rank because n ≤m and n = rank(I) = rank(AX) ≤rank(A). To summarize,
A has a right inverse if and only if A is of full row rank.
Now, suppose A is n × m and rank(A) = m; that is, m ≤n and A is of
full column rank. Writing Y A = Im and reversing the roles of the coeﬃcient
matrix and the solution matrix in the argument above, we have that Y exists
and is a left inverse of A. We denote the left inverse of A as
A−L.
Also, using a similar argument as above, we see that the matrix A has a left
inverse if and only if A is of full column rank.
We also note that if AAT is of full rank, the right inverse of A is
A−R = AT(AAT)−1.
(3.147)
Likewise, if ATA is of full rank, the left inverse of A is
A−L = (ATA)−1AT.
(3.148)

3.3 Matrix Rank and the Inverse of a Matrix
109
3.3.7 Full Rank Factorization
For a given matrix A, it is often of interest to ﬁnd matrices A1, . . . , Ak such
that A1, . . . , Ak have some useful properties and A = A1 · · · Ak. This is called
a factorization or decomposition of A. (We will usually use these two words
interchangeably; that is, by “decomposition”, we will usually mean “multi-
plicative decomposition”. Occasionally we will be interested in an additive
decomposition of a matrix, as in Cochran’s theorem, discussed on page 401
and later in Sect. 9.2.3.)
In most cases, the number of factors in A = A1 · · · Ak is either 2 or 3.
In this chapter, we will discuss some factorizations as they arise naturally in
the development, and then in Chap. 5 we will discuss factorizations in more
detail.
The partitioning of an n × m matrix as in equation (3.131) on page 104
leads to an interesting factorization of a matrix. Recall that we had an n × m
matrix B partitioned as
B =
!
Wr×r
Xr×m−r
Yn−r×r Zn−r×m−r
"
,
where r is the rank of B, W is of full rank, the rows of R = [W|X] span the
full row space of B, and the columns of C =
!W
Y
"
span the full column space
of B.
Therefore, for some T , we have [Y |Z] = T R, and for some S, we have
! X
Z
"
= CS. From this, we have Y = T W, Z = T X, X = WS, and Z = Y S,
so Z = T WS. Since W is nonsingular, we have T = Y W −1 and S = W −1X,
so Z = Y W −1X.
We can therefore write the partitions as
B =
!W
X
Y Y W −1X
"
=
!
I
Y W −1
"
W
'
I | W −1X
(
.
(3.149)
From this, we can form two equivalent factorizations of B:
B =
!W
Y
" '
I | W −1X
(
=
!
I
Y W −1
" '
W | X
(
.
The matrix B has a very special property: the full set of linearly indepen-
dent rows are the ﬁrst r rows, and the full set of linearly independent columns
are the ﬁrst r columns. We have seen, however, that any matrix A of rank
r can be put in this form, and A = E(π2)BE(π1) for an n × n permutation
matrix E(π2) and an m × m permutation matrix E(π1).

110
3 Basic Properties of Matrices
We therefore have, for the n × m matrix A with rank r, two equivalent
factorizations,
A =
! Q1W
Q2Y
" '
P1 | W −1XP2
(
=
!
Q1
Q2Y W −1
" '
WP1 | XP2
(
,
both of which are in the general form
An×m = Ln×r Rr×m,
(3.150)
where L is of full column rank and R is of full row rank. This is called a full rank
factorization of the matrix A. We will use a full rank factorization in proving
various properties of matrices. We will consider other factorizations later in
this chapter and in Chap. 5 that have more practical uses in computations.
3.3.8 Equivalent Matrices
Matrices of the same order that have the same rank are said to be equivalent
matrices.
3.3.8.1 Equivalent Canonical Forms
For any n×m matrix A with rank(A) = r > 0, by combining the permutations
that yield equation (3.131) with other operations, we have, for some matrices
P and Q that are products of various elementary operator matrices,
PAQ =
! Ir 0
0 0
"
.
(3.151)
This is called an equivalent canonical form of A, and it exists for any matrix
A that has at least one nonzero element (which is the same as requiring
rank(A) > 0).
We can see by construction that an equivalent canonical form exists for
any n × m matrix A that has a nonzero element. First, assume aij ̸= 0. By
two successive permutations, we move aij to the (1, 1) position; speciﬁcally,
(Ei1AE1j)11 = aij. We then divide the ﬁrst row by aij; that is, we form
E1(1/aij)Ei1AE1j. We then proceed with a sequence of n −1 premultipli-
cations by axpy matrices to zero out the ﬁrst column of the matrix, as in
expression (3.62), followed by a sequence of (m −1) postmultiplications by
axpy matrices to zero out the ﬁrst row. We then have a matrix of the form
⎡
⎢⎢⎢⎣
1 0 · · · 0
0
... [ X ]
0
⎤
⎥⎥⎥⎦.
(3.152)

3.3 Matrix Rank and the Inverse of a Matrix
111
If X = 0, we are ﬁnished; otherwise, we perform the same kinds of operations
on the (n −1) × (m −1) matrix X and continue until we have the form of
equation (3.151).
The matrices P and Q in equation (3.151) are not unique. The order in
which they are built from elementary operator matrices can be very important
in preserving the accuracy of the computations.
Although the matrices P and Q in equation (3.151) are not unique, the
equivalent canonical form itself (the right-hand side) is obviously unique be-
cause the only thing that determines it, aside from the shape, is the r in Ir,
and that is just the rank of the matrix. There are two other, more general,
equivalent forms that are often of interest. These equivalent forms, “row ech-
elon form” and “Hermite form”, are not unique. A matrix R is said to be in
row echelon form, or just echelon form, if
•
rij = 0 for i > j, and
•
if k is such that rik ̸= 0 and ril = 0 for l < k, then ri+1,j = 0 for j ≤k.
A matrix in echelon form is upper triangular. An upper triangular matrix H
is said to be in Hermite form if
•
hii = 0 or 1,
•
if hii = 0, then hij = 0 for all j, and
•
if hii = 1, then hki = 0 for all k ̸= i.
If H is in Hermite form, then H2 = H, as is easily veriﬁed. (A matrix H
such that H2 = H is said to be idempotent. We discuss idempotent matrices
beginning on page 352.) Another, more speciﬁc, equivalent form, called the
Jordan form, is a special row echelon form based on eigenvalues, which we
show on page 151.
Any of these equivalent forms is useful in determining the rank of a ma-
trix. Each form may have special uses in proving properties of matrices. We
will often make use of the equivalent canonical form in other sections of this
chapter.
3.3.8.2 Products with a Nonsingular Matrix
It is easy to see that if A is a square full rank matrix (that is, A is nonsingular),
and if B and C are conformable matrices for the multiplications AB and CA,
respectively, then
rank(AB) = rank(B)
(3.153)
and
rank(CA) = rank(C).
(3.154)
This is true because, for a given conformable matrix B, by the inequal-
ity (3.128), we have rank(AB) ≤rank(B). Forming B = A−1AB, and again
applying the inequality, we have rank(B) ≤rank(AB); hence, rank(AB) =

112
3 Basic Properties of Matrices
rank(B). Likewise, for a square full rank matrix A, we have rank(CA) =
rank(C). (Here, we should recall that all matrices are real.)
On page 113, we give a more general result for products with general full
rank matrices.
3.3.8.3 A Factorization Based on an Equivalent Canonical Form
Elementary operator matrices and products of them are of full rank and thus
have inverses. When we introduced the matrix operations that led to the
deﬁnitions of the elementary operator matrices in Sect. 3.2.3, we mentioned
the inverse operations, which would then deﬁne the inverses of the matrices.
The matrices P and Q in the equivalent canonical form of the matrix
A, PAQ in equation (3.151), have inverses. From an equivalent canonical
form of a matrix A with rank r, we therefore have the equivalent canonical
factorization of A:
A = P −1
!
Ir 0
0 0
"
Q−1.
(3.155)
A factorization based on an equivalent canonical form is also a full rank fac-
torization and could be written in the same form as equation (3.150).
3.3.8.4 Equivalent Forms of Symmetric Matrices
If A is symmetric, the equivalent form in equation (3.151) can be written
as PAP T = diag(Ir, 0) and the equivalent canonical factorization of A in
equation (3.155) can be written as
A = P −1
!
Ir 0
0 0
"
P −T.
(3.156)
These facts follow from the same process that yielded equation (3.151) for a
general matrix.
Also a full rank factorization for a symmetric matrix, as in equa-
tion (3.150), can be given as
A = LLT.
(3.157)
3.3.9 Multiplication by Full Rank Matrices
We have seen that a matrix has an inverse if it is square and of full rank.
Conversely, it has an inverse only if it is square and of full rank. We see that
a matrix that has an inverse must be square because A−1A = AA−1, and
we see that it must be full rank by the inequality (3.128). In this section, we
consider other properties of full rank matrices. In some cases, we require the
matrices to be square, but in other cases, these properties hold whether or
not they are square.
Using matrix inverses allows us to establish important properties of prod-
ucts of matrices in which at least one factor is a full rank matrix.

3.3 Matrix Rank and the Inverse of a Matrix
113
3.3.9.1 Products with a General Full Rank Matrix
If C is a full column rank matrix and if B is a matrix conformable for the
multiplication CB, then
rank(CB) = rank(B).
(3.158)
To see this, consider a full rank n × m matrix C with rank(C) = m (that is,
m ≤n) and let B be conformable for the multiplication CB. Because C is
of full column rank, it has a left inverse (see page 108); call it C−L, and so
C−LC = Im. From inequality (3.128), we have rank(CB) ≤rank(B), and ap-
plying the inequality again, we have rank(B) = rank(C−LCB) ≤rank(CB);
hence rank(CB) = rank(B).
If R is a full row rank matrix and if B is a matrix conformable for the
multiplication BR, then
rank(BR) = rank(B).
(3.159)
To see this, consider a full rank n × m matrix R with rank(R) = n (that is,
n ≤m) and let B be conformable for the multiplication BR. Because R is of
full row rank, it has a right inverse; call it R−R, and so RR−R = In. From
inequality (3.128), we have rank(BR) ≤rank(B), and applying the inequality
again, we have rank(B) = rank(BRR−L) ≤rank(BR); hence rank(BR) =
rank(B).
To state this more simply:
•
Premultiplication of a given matrix by a full column rank matrix yields a
product with the same rank as the given matrix, and postmultiplication
of a given matrix by a full row rank matrix yields a product with the same
rank as the given matrix.
From this we see that, given any matrix B, if A is a square matrix of full
rank that is compatible for the multiplication AB = D, then B and D are
equivalent matrices. (And, of course, a similar statement for postmultiplica-
tion by a full-rank matrix holds.)
Furthermore, if the matrix B is square and A is a square matrix of the
same order that is full rank, then
rank(AB) = rank(BA) = rank(B).
(3.160)
3.3.9.2 Preservation of Positive Deﬁniteness
A certain type of product of a full rank matrix and a positive deﬁnite matrix
preserves not only the rank, but also the positive deﬁniteness: if A is n × n
and positive deﬁnite, and C is n × m and of rank m (hence, m ≤n), then
CTAC is positive deﬁnite. (Recall from inequality (3.88) that a matrix A is
positive deﬁnite if it is symmetric and for any x ̸= 0, xTAx > 0.)

114
3 Basic Properties of Matrices
To see this, assume matrices A and C as described. Let x be any m-vector
such that x ̸= 0, and let y = Cx. Because C is of full column rank, y ̸= 0. We
have
xT(CTAC)x = (Cx)TA(Cx)
= yTAy
> 0.
(3.161)
Therefore, since CTAC is symmetric,
•
if A is positive deﬁnite and C is of full column rank, then CTAC is positive
deﬁnite.
Furthermore, we have the converse:
•
if CTAC is positive deﬁnite, then C is of full column rank,
for otherwise there exists an x ̸= 0 such that Cx = 0, and so xT(CTAC)x = 0.
3.3.9.3 The General Linear Group
Consider the set of all square n × n full rank matrices together with the usual
(Cayley) multiplication. As we have seen, this set is closed under multiplica-
tion. (The product of two square matrices of full rank is of full rank, and of
course the product is also square.) Furthermore, the (multiplicative) identity
is a member of this set, and each matrix in the set has a (multiplicative)
inverse in the set; therefore, the set together with the usual multiplication is
a mathematical structure called a group. (See any text on modern algebra.)
This group is called the general linear group and is denoted by GL(n). The
order of the group is n, the order of the square matrices in the group. General
group-theoretic properties can be used in the derivation of properties of these
full-rank matrices. Note that this group is not commutative.
We note that all matrices in the general linear group of order n are equiv-
alent.
As we mentioned earlier (before we had considered inverses in general), if
A is an n × n matrix and if A−1 exists, we deﬁne A0 to be In (otherwise, A0
does not exist).
The n× n elementary operator matrices are members of the general linear
group GL(n).
The elements in the general linear group are matrices and, hence, can be
viewed as transformations or operators on n-vectors. Another set of linear
operators on n-vectors are the doubletons (A, v), where A is an n × n full-
rank matrix and v is an n-vector. As an operator on x ∈IRn, (A, v) is the
transformation Ax + v, which preserves aﬃne spaces. Two such operators,
(A, v) and (B, w), are combined by composition: (A, v)((B, w)(x)) = ABx +
Aw + v. The set of such doubletons together with composition forms a group,
called the aﬃne group. It is denoted by AL(n). A subset of the elements of

3.3 Matrix Rank and the Inverse of a Matrix
115
the aﬃne group with the same ﬁrst element, together with the axpy operator,
constitute a quotient space.
3.3.10 Gramian Matrices: Products of the Form ATA
Given a real matrix A, an important matrix product is ATA. (This is called
a Gramian matrix, or just a Gram matrix. We will discuss this kind of matrix
in more detail beginning on page 359. I should note here that this is not a
deﬁnition of “Gramian” or “Gram”; these terms have more general meanings,
but they do include any matrix expressible as ATA.)
We ﬁrst note that AAT is a Gramian matrix, and has the same properties
as ATA with any dependencies on A being replaced with dependencies on AT.
3.3.10.1 General Properties of Gramian Matrices
Gramian matrices have several interesting properties. First of all, we note that
for any A, because
(ATA)ij = aT
∗ia∗j = aT
∗ja∗i = (ATA)ji
(recall notation, page 600),
ATA is symmetric, and hence has all of the useful properties of symmetric
matrices. (These properties are shown in various places in this book, but
are summarized conveniently in Sect. 8.2 beginning on page 340.) Further-
more, ATA is nonnegative deﬁnite, as we see by observing that for any y,
yT(ATA)y = (Ay)T(Ay) ≥0.
Another interesting property of a Gramian matrix is that, for any matrices
C and D (that are conformable for the operations indicated),
CATA = DATA
⇐⇒
CAT = DAT.
(3.162)
The implication from right to left is obvious, and we can see the left to right
implication by writing
(CATA −DATA)(CT −DT) = (CAT −DAT)(CAT −DAT)T,
and then observing that if the left-hand side is null, then so is the right-
hand side, and if the right-hand side is null, then CAT −DAT = 0 because
ATA = 0 =⇒A = 0, as above.
Similarly, we have
ATAC = ATAD
⇐⇒
AC = AD.
(3.163)
3.3.10.2 Rank of ATA
Consider the linear system ATAX = ATB. Suppose that c is such that
cTATA = 0. Then by (3.162), cTAT = 0, which by (3.143) on page 106, implies

116
3 Basic Properties of Matrices
that ATAX = ATB is consistent. Letting B = I, we have that ATAX = AT
is consistent.
Now if ATAX = AT, for any conformable matrix K,
V(KTAT) = V(KTATAX).
By (3.127) on page 103, V(KTATAX) ⊆V(KTATA) and V(KTATA) ⊆
V(KTAT); hence V(KTATA) = V(KTAT). By similar arguments applied to
the transposes we have V(ATAK) = V(AK).
With K = I, this yields
rank(ATA) = rank(A).
(3.164)
In a similar manner, we have rank(AAT) = rank(A), and hence,
rank(ATA) = rank(AAT).
(3.165)
It is clear from the statements above that (ATA) is of full rank if and only
if A is of full column rank.
We also see that ATA is positive deﬁnite, that is, for any y ̸= 0 yTATAy >
0, if and only if A is of full column rank. This follows from (3.167), and if A
is of full column rank, Ay = 0 ⇒y = 0.
3.3.10.3 Zero Matrices and Equations Involving Gramians
First of all, for any n×m matrix A, we have the fact that ATA = 0 if and only
if A = 0. We see this by noting that if A = 0, then tr(ATA) = 0. Conversely,
if tr(ATA) = 0, then a2
ij = 0 for all i, j, and so aij = 0, that is, A = 0.
Summarizing, we have
tr(ATA) = 0 ⇔A = 0
(3.166)
and
ATA = 0 ⇔A = 0.
(3.167)
Now consider the equation ATA = 0. We have for any conformable B and
C
ATA(B −C) = 0.
Multiplying by BT −CT and factoring (BT −CT)ATA(B −C), we have
(AB −AC)T(AB −AC) = 0;
hence, from (3.167), we have AB −AC = 0. Furthermore, if AB −AC = 0,
then clearly ATA(B −C) = 0. We therefore conclude that
ATAB = ATAC ⇔AB = AC.
(3.168)
By the same argument, we have

3.3 Matrix Rank and the Inverse of a Matrix
117
BATA = CATA ⇔BAT = CAT.
From equation (3.164), we have another useful fact for Gramian matrices.
The system
ATAx = ATb
(3.169)
is consistent for any A and b.
3.3.11 A Lower Bound on the Rank of a Matrix Product
Equation (3.128) gives an upper bound on the rank of the product of two
matrices; the rank cannot be greater than the rank of either of the factors.
Now, using equation (3.155), we develop a lower bound on the rank of the
product of two matrices if one of them is square.
If A is n × n (that is, square) and B is a matrix with n rows, then
rank(AB) ≥rank(A) + rank(B) −n.
(3.170)
We see this by ﬁrst letting r = rank(A), letting P and Q be matrices that form
an equivalent canonical form of A (see equation (3.155)), and then forming
C = P −1
!0 0
0 In−r
"
Q−1,
so that A + C = P −1Q−1. Because P −1 and Q−1 are of full rank, rank(C) =
rank(In−r) = n −rank(A). We now develop an upper bound on rank(B),
rank(B) = rank(P −1Q−1B)
= rank(AB + CB)
≤rank(AB) + rank(CB), by equation (3.129)
≤rank(AB) + rank(C), by equation (3.128)
= rank(AB) + n −rank(A),
yielding (3.170), a lower bound on rank(AB).
The inequality (3.170) is called Sylvester’s law of nullity. It provides a
lower bound on rank(AB) to go with the upper bound of inequality (3.128),
min(rank(A), rank(B)). The bound in inequality (3.170) is also sharp, as we
can see by exhibiting matrices A and B such that rank(AB) = rank(A) +
rank(B) −n, as you are asked to do in Exercise 3.12b.
3.3.12 Determinants of Inverses
From the relationship det(AB) = det(A) det(B) for square matrices men-
tioned earlier, it is easy to see that for nonsingular square A,
det(A−1) = (det(A))−1,
(3.171)
and so

118
3 Basic Properties of Matrices
•
det(A) = 0 if and only if A is singular.
(From the deﬁnition of the determinant in equation (3.24), we see that the
determinant of any ﬁnite-dimensional matrix with ﬁnite elements is ﬁnite. We
implicitly assume that the elements are ﬁnite.)
For an n × n matrix with n ≥2 whose determinant is nonzero, from
equation (3.34) we have
A−1 =
1
det(A)adj(A).
(3.172)
If det(A) = 1, this obviously implies
A−1 = adj(A).
See Exercise 3.15 on page 179 for an interesting consequence of this.
3.3.13 Inverses of Products and Sums of Nonsingular Matrices
In linear regression analysis and other applications, we sometimes need in-
verses of various sums or products of matrices. In regression analysis, this
may be because we wish to update regression estimates based on additional
data or because we wish to delete some observations.
There is no simple relationship between the inverses of factors in a
Hadamard product and the product matrix, but there are simple relation-
ships between the inverses of factors in Cayley and Kronecker products and
the product matrices.
3.3.13.1 Inverses of Cayley Products of Matrices
The inverse of the Cayley product of two nonsingular matrices of the same
size is particularly easy to form. If A and B are square full rank matrices of
the same size,
(AB)−1 = B−1A−1.
(3.173)
We can see this by multiplying B−1A−1 and (AB). This, of course, generalizes
to
(A1 · · · An)−1 = A−1
n · · · A−1
1
if A1, · · · , An are all full rank and conformable.
3.3.13.2 Inverses of Kronecker Products of Matrices
If A and B are square full rank matrices, then
(A ⊗B)−1 = A−1 ⊗B−1.
(3.174)
We can see this by multiplying A−1 ⊗B−1 and A ⊗B using equation (3.101)
on page 96.

3.3 Matrix Rank and the Inverse of a Matrix
119
3.3.13.3 Inverses of Sums of Matrices and Their Inverses
The inverse of the sum of two nonsingular matrices is somewhat more com-
plicated. The ﬁrst question of course is whether the sum is nonsingular. We
can develop several useful relationships of inverses of sums and the sums and
products of the individual matrices.
The simplest case to get started is I + A. Let A and I + A be nonsingular.
Then it is easy to derive (I + A)−1 by use of I = AA−1 and equation (3.173).
We get
(I + A)−1 = A−1(I + A−1)−1.
(3.175)
If A and B are full rank matrices of the same size and such sums as I + A,
A + B, and so on, are full rank, the following relationships are easy to show
(and are easily proven in the order given, using equations (3.173) and (3.175);
see Exercise 3.16):
A(I + A)−1 = (I + A−1)−1,
(3.176)
(A + B)−1 = A−1 −A−1(A−1 + B−1)−1A−1,
(3.177)
(A + BBT)−1B = A−1B(I + BTA−1B)−1,
(3.178)
(A−1 + B−1)−1 = A(A + B)−1B,
(3.179)
A −A(A + B)−1A = B −B(A + B)−1B,
(3.180)
A−1 + B−1 = A−1(A + B)B−1,
(3.181)
(I + AB)−1 = I −A(I + BA)−1B,
(3.182)
(I + AB)−1A = A(I + BA)−1.
(3.183)
When A and/or B are not of full rank, the inverses may not exist, but in that
case these equations may or may not hold for a generalized inverse, which we
will discuss in Sect. 3.6.
Another simple general result, this time involving some non-square matri-
ces, is that if A is a full-rank n × n matrix, B is a full-rank m × m matrix, C
is any n × m matrix, and D is any m × n matrix such that A + CBD is full
rank, then
(A + CBD)−1 = A−1 −A−1C(B−1 + DA−1C)−1DA−1.
(3.184)
This can be derived from equation (3.176), which is a special case of it. We
can verify this by multiplication (Exercise 3.17).

120
3 Basic Properties of Matrices
From this it also follows that if A is a full-rank n × n matrix and b and c
are n-vectors such that (A + bcT) is full rank, then
(A + bcT)−1 = A−1 −A−1bcTA−1
1 + cTA−1b .
(3.185)
This fact has application in adding an observation to a least squares linear
regression problem (page 418).
3.3.13.4 An Expansion of a Matrix Inverse
There is also an analogue to the expansion of the inverse of (1−a) for a scalar
a:
(1 −a)−1 = 1 + a + a2 + a3 + · · · ,
if |a| < 1.
This expansion for the scalar a comes from a factorization of the binomial
1 −ak and the fact that ak →0 if |a| < 1.
To extend this to (I + A)−1 for a matrix A, we need a similar condition
on Ak as k increases without bound. In Sect. 3.9 on page 164, we will discuss
conditions that ensure the convergence of Ak for a square matrix A. We will
deﬁne a norm ∥A∥on A and show that if ∥A∥< 1, then Ak →0. Then,
analogous to the scalar series, using equation (3.53) on page 78 for a square
matrix A, we have
(I −A)−1 = I + A + A2 + A3 + · · · ,
if ∥A∥< 1.
(3.186)
We include this equation here because of its relation to equations (3.176)
through (3.182). We will discuss it further on page 171, after we have intro-
duced and discussed ∥A∥and other conditions that ensure convergence. This
expression and the condition that determines it are very important in the
analysis of time series and other stochastic processes.
Also, looking ahead, we have another expression similar to equa-
tions (3.176) through (3.182) and (3.186) for a special type of matrix. If
A2 = A, for any a ̸= −1,
(I + aA)−1 = I −
a
a + 1A
(see page 354).
3.3.14 Inverses of Matrices with Special Forms
Matrices with various special patterns may have inverses with similar patterns.
•
The inverse of a nonsingular symmetric matrix is symmetric.
•
The inverse of a diagonal matrix with nonzero entries is a diagonal matrix
consisting of the reciprocals of those elements.

3.4 The Schur Complement
121
•
The inverse of a block diagonal matrix with nonsingular submatrices along
the diagonal is a block diagonal matrix consisting of the inverses of the
submatrices.
•
The inverse of a nonsingular triangular matrix is a triangular matrix with
the same pattern; furthermore, the diagonal elements in the inverse are
the reciprocals of the diagonal elements in the original matrix.
Each of these statements can be easily proven by multiplication (using the
fact that the inverse is unique). See also Exercise 3.19 (and the hint).
The inverses of other matrices with special patterns, such as banded ma-
trices, may not have those patterns.
In Chap. 8, we discuss inverses of various other special matrices that arise
in applications in statistics.
3.3.15 Determining the Rank of a Matrix
Although the equivalent canonical form (3.151) immediately gives the rank
of a matrix, in practice the numerical determination of the rank of a matrix
is not an easy task. The problem is that rank is a mapping IRn×m →ZZ+,
where ZZ+ represents the positive integers. Such a function is often diﬃcult to
compute because the domain is dense and the range is sparse. Small changes
in the domain may result in large discontinuous changes in the function value.
(In Hadamard’s sense, the problem is ill-posed.) The common way that the
rank of a matrix is evaluated is by use of the QR decomposition; see page 252.
It is not even always clear whether a matrix is nonsingular. Because of
rounding on the computer, a matrix that is mathematically nonsingular may
appear to be singular. We sometimes use the phrase “nearly singular” or
“algorithmically singular” to describe such a matrix. In Sects. 6.1 and 11.4,
we consider this kind of problem in more detail.
3.4 More on Partitioned Square Matrices:
The Schur Complement
A square matrix A that can be partitioned as
A =
!
A11 A12
A21 A22
"
,
(3.187)
where A11 is nonsingular, has interesting properties that depend on the matrix
Z = A22 −A21A−1
11 A12,
(3.188)
which is called the Schur complement of A11 in A.
We ﬁrst observe from equation (3.149) that if equation (3.187) represents
a full rank partitioning (that is, if the rank of A11 is the same as the rank of
A), then

122
3 Basic Properties of Matrices
A =
!A11
A12
A21
A21A−1
11 A12
"
,
(3.189)
and Z = 0.
There are other useful properties of the Schur complement, which we men-
tion below. There are also some interesting properties of certain important
random matrices partitioned in this way. For example, suppose A22 is k × k
and A is an m × m Wishart matrix with parameters n and Σ partitioned
like A in equation (3.187). (This of course means A is symmetrical, and so
A12 = AT
21.) Then Z has a Wishart distribution with parameters n −m + k
and Σ22 −Σ21Σ−1
11 Σ12, and is independent of A21 and A11. (See Exercise 4.12
on page 224 for the probability density function for a Wishart distribution.)
3.4.1 Inverses of Partitioned Matrices
Suppose A is nonsingular and can be partitioned as above with both A11 and
A22 nonsingular. It is easy to see (Exercise 3.20, page 180) that the inverse of
A is given by
A−1 =
⎡
⎣
A−1
11 + A−1
11 A12Z−1A21A−1
11
−A−1
11 A12Z−1
−Z−1A21A−1
11
Z−1
⎤
⎦,
(3.190)
where Z is the Schur complement of A11.
If
A = [X y]T [X y]
and is partitioned as in equation (3.55) on page 79 and X is of full column
rank, then the Schur complement of XTX in [X y]T [X y] is
yTy −yTX(XTX)−1XTy.
(3.191)
This particular partitioning is useful in linear regression analysis (see, for ex-
ample, page 363), where this Schur complement is the residual sum of squares
and the more general Wishart distribution mentioned above reduces to a chi-
squared distribution. (Although the expression is useful, this is an instance
of a principle that we will encounter repeatedly: the form of a mathematical
expression and the way the expression should be evaluated in actual practice
may be quite diﬀerent.)
3.4.2 Determinants of Partitioned Matrices
If the square matrix A is partitioned as
A =
!
A11 A12
A21 A22
"
,
and A11 is square and nonsingular, then

3.5 Linear Systems of Equations
123
det(A) = det(A11) det

A22 −A21A−1
11 A12

;
(3.192)
that is, the determinant is the product of the determinant of the principal
submatrix and the determinant of its Schur complement.
This result is obtained by using equation (3.38) on page 71 and the fac-
torization
!
A11 A12
A21 A22
"
=
! A11
0
A21
A22 −A21A−1
11 A12
" !
I
A−1
11 A12
0
I
"
.
(3.193)
The factorization in equation (3.193) is often useful in other contexts as well.
3.5 Linear Systems of Equations
Some of the most important applications of matrices are in representing and
solving systems of n linear equations in m unknowns,
Ax = b,
where A is an n × m matrix, x is an m-vector, and b is an n-vector. As
we observed in equation (3.84), the product Ax in the linear system is a
linear combination of the columns of A; that is, if aj is the jth column of A,
Ax = m
j=1 xjaj.
If b = 0, the system is said to be homogeneous. In this case, unless x = 0,
the columns of A must be linearly dependent.
3.5.1 Solutions of Linear Systems
When in the linear system Ax = b, A is square and nonsingular, the solution is
obviously x = A−1b. We will not discuss this simple but common case further
here. Rather, we will discuss it in detail in Chap. 6 after we have discussed
matrix factorizations later in this chapter and in Chap. 5.
When A is not square or is singular, the system may not have a solution or
may have more than one solution. A consistent system (see equation (3.137))
has a solution. For consistent systems that are singular or not square, the
generalized inverse is an important concept. We introduce it in this section
but defer its discussion to Sect. 3.6.
3.5.1.1 Underdetermined Systems
A consistent system in which rank(A) < m is said to be underdetermined.
An underdetermined system may have fewer equations than variables, or the
coeﬃcient matrix may just not be of full rank. For such a system there is
more than one solution. In fact, there are inﬁnitely many solutions because if

124
3 Basic Properties of Matrices
the vectors x1 and x2 are solutions, the vector wx1 + (1 −w)x2 is likewise a
solution for any scalar w.
Underdetermined systems arise in analysis of variance in statistics, and it
is useful to have a compact method of representing the solution to the system.
It is also desirable to identify a unique solution that has some kind of optimal
properties. Below, we will discuss types of solutions and the number of linearly
independent solutions and then describe a unique solution of a particular type.
3.5.1.2 Overdetermined Systems
Often in mathematical modeling applications, the number of equations in the
system Ax = b is not equal to the number of variables; that is the coeﬃcient
matrix A is n×m and n ̸= m. If n > m and rank([A | b]) > rank(A), the system
is said to be overdetermined. There is no x that satisﬁes such a system, but
approximate solutions are useful. We discuss approximate solutions of such
systems in Sect. 6.6 on page 289 and in Sect. 9.3.2 on page 408.
3.5.1.3 Generalized Inverses
A matrix G such that AGA = A is called a generalized inverse and is denoted
by A−:
AA−A = A.
(3.194)
Note that if A is n × m, then A−is m × n. If A is nonsingular (square and of
full rank), then obviously A−= A−1.
Without additional restrictions on A, the generalized inverse is not unique.
Various types of generalized inverses can be deﬁned by adding restrictions to
the deﬁnition of the inverse. In Sect. 3.6, we will discuss various types of
generalized inverses and show that A−exists for any n × m matrix A. Here
we will consider some properties of any generalized inverse.
From equation (3.194), we see that
AT(A−)TAT = AT;
thus, if A−is a generalized inverse of A, then (A−)T is a generalized inverse
of AT.
The m × m square matrices A−A and (I −A−A) are often of interest. By
using the deﬁnition (3.194), we see that
(A−A)(A−A) = A−A.
(3.195)
(Such a matrix is said to be idempotent. We discuss idempotent matrices
beginning on page 352.) From equation (3.128) together with the fact that
AA−A = A, we see that
rank(A−A) = rank(A).
(3.196)

3.5 Linear Systems of Equations
125
By multiplication as above, we see that
A(I −A−A) = 0,
(3.197)
that
(I −A−A)(A−A) = 0,
(3.198)
and that (I −A−A) is also idempotent:
(I −A−A)(I −A−A) = (I −A−A).
(3.199)
The fact that (A−A)(A−A) = A−A yields the useful fact that
rank(I −A−A) = m −rank(A).
(3.200)
This follows from equations (3.198), (3.170), and (3.196), which yield
0 ≥rank(I −A−A) + rank(A) −m,
and from equation (3.129), which gives
m = rank(I) ≤rank(I −A−A) + rank(A).
The two inequalities result in the equality of equation (3.200).
3.5.1.4 Multiple Solutions in Consistent Systems
Suppose the system Ax = b is consistent and A−is a generalized inverse of
A; that is, it is any matrix such that AA−A = A. Then
x = A−b
(3.201)
is a solution to the system because if AA−A = A, then AA−Ax = Ax and
since Ax = b,
AA−b = b;
(3.202)
that is, A−b is a solution.
Furthermore, if x = Gb is any solution, then AGA = A; that is, G is a
generalized inverse of A. This can be seen by the following argument. Let aj
be the jth column of A. The m systems of n equations, Ax = aj, j = 1, . . . , m,
all have solutions. (Each solution is a vector with 0s in all positions except
the jth position, which is a 1.) Now, if Gb is a solution to the original system,
then Gaj is a solution to the system Ax = aj. So AGaj = aj for all j; hence
AGA = A.
If Ax = b is consistent, not only is A−b a solution but also, for any z,
A−b + (I −A−A)z
(3.203)
is a solution because A(A−b + (I −A−A)z) = AA−b + (A −AA−A)z = b.
Furthermore, any solution to Ax = b can be represented as A−b+(I −A−A)z
for some z. This is because if y is any solution (that is, if Ay = b), we have
y = A−b −A−Ay + y = A−b −(A−A −I)y = A−b + (I −A−A)z.
The number of linearly independent solutions arising from (I −A−A)z is
just the rank of (I −A−A), which from equation (3.200) is m −rank(A).

126
3 Basic Properties of Matrices
3.5.2 Null Space: The Orthogonal Complement
The solutions of a consistent system Ax = b, which we characterized in equa-
tion (3.203) as A−b + (I −A−A)z for any z, are formed as a given solution to
Ax = b plus all solutions to Az = 0.
For an n × m matrix A, the set of vectors generated by all solutions, z, of
the homogeneous system
Az = 0
(3.204)
is called the null space of A. We denote the null space of A by
N(A).
The null space is either the single 0 vector (in which case we say the null
space is empty or null) or it is a vector space. (It is actually a vector space in
either case, but recall our ambiguity about the null vector space, page 13.)
We see that N(A) is a vector space (if it is not empty) because the zero
vector is in N(A), and if x and y are in N(A) and a is any scalar, ax + y is
also a solution of Az = 0, and hence in N(A). We call the dimension of N(A)
the nullity of A. The nullity of A is
dim(N(A)) = rank(I −A−A)
= m −rank(A)
(3.205)
from equation (3.200).
If Ax = b is consistent, any solution can be represented as A−b + z, for
some z in the null space of A, because if y is some solution, Ay = b = AA−b
from equation (3.202), and so A(y −A−b) = 0; that is, z = y −A−b is in the
null space of A. If A is nonsingular, then there is no such z, and the solution is
unique. The number of linearly independent solutions to Az = 0, is the same
as the nullity of A.
The order of N(A) is m. (Recall that the order of V(A) is n. The order of
V(AT) is m.)
If A is square, we have
N(A) ⊆N(A2) ⊆N(A3) ⊆· · ·
(3.206)
and
V(A) ⊇V(A2) ⊇V(A3) ⊇· · · .
(3.207)
(We see this easily from the inequality (3.128) on page 103.)
If a is in V(AT) and b is in N(A), we have bTa = bTATx = 0. In other
words, the null space of A is orthogonal to the row space of A; that is, N(A) ⊥
V(AT). This is because ATx = a for some x, and Ab = 0 or bTAT = 0. For
any matrix B whose columns are in N(A), AB = 0, and BTAT = 0.
Because dim(N(A)) + dim(V(AT)) = m and N(A) ⊥V(AT), by equa-
tion (2.44) we have

3.6 Generalized Inverses
127
N(A) ⊕V(AT) = IRm;
(3.208)
that is, the null space of A is the orthogonal complement of V(AT). All vectors
in the null space of the matrix AT are orthogonal to all vectors in the column
space of A.
3.6 Generalized Inverses
On page 124, we deﬁned a generalized inverse of a matrix A as a matrix
A−such that AA−A = A, and we observed several interesting properties of
generalized inverses. We will now consider some additional properties, after
quickly summarizing some we observed previously.
3.6.1 Immediate Properties of Generalized Inverses
Let A be an n×m matrix, and let A−be a generalized inverse of A. The prop-
erties of a generalized inverse A−derived in equations (3.195) through (3.203)
include:
•
(A−)T is a generalized inverse of AT.
•
rank(A−A) = rank(A).
•
A−A is idempotent.
•
I −A−A is idempotent.
•
rank(I −A−A) = m −rank(A).
We note that if A is square (that is, n = m) and nonsingular, then A−= A−1,
and so all of these properties apply to ordinary inverses.
In this section, we will ﬁrst consider some special types of generalized
inverses. Two of these special types of generalized inverses are unique. We will
then discuss some more properties of “general” generalized inverses, which are
analogous to properties of inverses. (We will call general generalized inverses
“g1 inverses”.)
3.6.2 Special Generalized Inverses: The Moore-Penrose Inverse
A generalized inverse is not unique in general. As we have seen on page 125,
a generalized inverse determines a set of linearly independent solutions to a
linear system Ax = b. We may impose other conditions on the generalized
inverse to arrive at a unique matrix that yields a solution that has some
desirable properties. If we impose three more conditions, we have a unique
matrix, denoted by A+, that yields a solution A+b that has the minimum
length of any solution to Ax = b. We deﬁne this matrix and discuss some
of its properties below, and in Sect. 6.6 we discuss properties of the solution
A+b.

128
3 Basic Properties of Matrices
3.6.2.1 Deﬁnitions and Terminology
To the general requirement AA−A = A, we successively add three require-
ments that deﬁne special generalized inverses, sometimes called respectively
g2 or g12, g3 or g123, and g4 or g1234 inverses. The “general” generalized inverse
is sometimes called a g1 inverse. The g4 inverse is called the Moore-Penrose
inverse. As we will see below, it is unique. The terminology distinguishing the
various types of generalized inverses is not used consistently in the literature.
I will indicate some alternative terms in the deﬁnition below.
For a matrix A, a Moore-Penrose inverse, denoted by A+, is a matrix that
has the following four properties.
1. AA+A = A. Any matrix that satisﬁes this condition is called a gener-
alized inverse, and as we have seen above is denoted by A−. For many
applications, this is the only condition necessary. Such a matrix is also
called a g1 inverse, an inner pseudoinverse, or a conditional inverse.
2. A+AA+ = A+. A matrix A+ that satisﬁes this condition is called an
outer pseudoinverse. A g1 inverse that also satisﬁes this condition is
called a g2 inverse or reﬂexive generalized inverse, and is denoted by
A∗.
3. A+A is symmetric.
4. AA+ is symmetric.
The Moore-Penrose inverse is also called the pseudoinverse, the p-inverse,
and the normalized generalized inverse. (My current preferred term is “Moore-
Penrose inverse”, but out of habit, I often use the term “pseudoinverse” for this
special generalized inverse. I generally avoid using any of the other alternative
terms introduced above. I use the term “generalized inverse” to mean the
“general generalized inverse”, the g1.) The name Moore-Penrose derives from
the preliminary work of Moore (1920) and the more thorough later work of
Penrose (1955), who laid out the conditions above and proved existence and
uniqueness.
3.6.2.2 Existence
We can see by construction that the Moore-Penrose inverse exists for any
matrix A. First, if A = 0, note that A+ = 0. If A ̸= 0, it has a full rank
factorization, A = LR, as in equation (3.150), so LTART = LTLRRT. Be-
cause the n × r matrix L is of full column rank and the r × m matrix R is
of full row rank, LTL and RRT are both of full rank, and hence LTLRRT
is of full rank. Furthermore, LTART = LTLRRT, so it is of full rank, and
(LTART)−1 exists. Now, form RT(LTART)−1LT. By checking properties 1
through 4 above, we see that
A+ = RT(LTART)−1LT
(3.209)
is a Moore-Penrose inverse of A. This expression for the Moore-Penrose inverse
based on a full rank decomposition of A is not as useful as another expres-
sion we will consider later, based on QR decomposition (equation (5.45) on
page 251).

3.6 Generalized Inverses
129
3.6.2.3 Uniqueness
We can see that the Moore-Penrose inverse is unique by considering any matrix
G that satisﬁes the properties 1 through 4 for A ̸= 0. (The Moore-Penrose
inverse of A = 0 (that is, A+ = 0) is clearly unique, as there could be no other
matrix satisfying property 2.) By applying the properties and using A+ given
above, we have the following sequence of equations:
G =
GAG = (GA)TG = ATGTG = (AA+A)TGTG = (A+A)TATGTG =
A+AATGTG = A+A(GA)TG = A+AGAG = A+AG = A+AA+AG =
A+(AA+)T(AG)T = A+(A+)TATGTAT = A+(A+)T(AGA)T =
A+(A+)TAT = A+(AA+)T = A+AA+
= A+.
3.6.2.4 Other Properties
Similarly to the property for inverses expressed in equation (3.145), we have
(A+)T = (AT)+.
(3.210)
This is easily seen from the deﬁning properties of the Moore-Penrose inverse.
If A is nonsingular, then obviously A+ = A−1, just as for any generalized
inverse.
Because A+ is a generalized inverse, all of the properties for a generalized
inverse A−discussed above hold; in particular, A+b is a solution to the linear
system Ax = b (see equation (3.201)). In Sect. 6.6, we will show that this
unique solution has a kind of optimality.
Moore-Penrose inverses also have a few additional interesting properties
not shared by generalized inverses; for example
(I −A+A)A+ = 0.
(3.211)
3.6.2.5 Drazin Inverses
A Drazin inverse of a square matrix A is a matrix, which we will denote as
AD, such that
1. ADAAD = AD; that is, it is an outer pseudoinverse,
2. AAD = ADA, and
3. Ak+1AD = Ak for any positive integer k.
Notice that these conditions together imply that a Drazin inverse is a g1
inverse (that is, AADA = A; see the conditions for the Moore-Penrose inverse
on page 128). Because of this, a Drazin inverse satisﬁes most of the properties

130
3 Basic Properties of Matrices
listed for any generalized inverse on page 127; for example, ADA is idempotent.
The ﬁrst property listed there is also satisﬁed; that is, (AD)T is the Drazin
inverse of AT. (This does not follow just because the Drazin inverse is a
generalized inverse, however.)
Other important properties of Drazin inverses include
•
AD = A−1 if A is nonsingular.
•
(AD)D = A.
•
For any square matrix, the Drazin inverse is unique.
These are left as exercises.
There is an interesting relationship between Drazin inverses and Moore-
Penrose inverses. If A is any square matrix, for any positive integer k, its
Drazin inverse is the matrix
AD = Ak(A2k+1)+Ak.
(3.212)
Drazin inverses arise in the solutions of linear systems of diﬀerential equa-
tions. See Campbell and Meyer (1991) for further discussions of properties and
applications of Drazin inverses and of their relationship to Moore-Penrose in-
verses.
3.6.3 Generalized Inverses of Products and Sums of Matrices
We often need to perform various operations on a matrix that is expressed
as sums or products of various other matrices. Some operations are rather
simple, for example, the transpose of the sum of two matrices is the sum
of the transposes (equation (3.15)), and the transpose of the product is the
product of the transposes in reverse order (equation (3.44)). Once we know
the relationships for a single sum and a single product, we can extend those
relationships to various sums and products of more than just two matrices.
In Sect. 3.3.13, beginning on page 118, we gave a number of relationships
between inverses of sums and/or products and sums and/or products of sums.
The two basic relationships were equations (3.173) and (3.175):
(AB)−1 = B−1A−1
and
(I + A)−1 = A−1(I + A−1)−1.
These same relations hold with the inverse replaced by generalized inverses.
We can relax the conditions on nonsingularity of A, B, I + A and so on,
but because of the nonuniqueness of generalized inverses, in some cases we
must interpret the equations as “holding for some generalized inverse”.
With the relaxation on the nonsingularity of constituent matrices, equa-
tions (3.176) through (3.182) do not necessarily hold for generalized inverses
of general matrices, but some do. For example,

3.7 Orthogonality
131
A(I + A)−= (I + A−)−.
(Again, the true relationships are easily proven if taken in the order given on
page 119, and in Exercise 3.18 you are asked to determine which are true for
generalized inverses of general matrices and to prove that those are.)
3.6.4 Generalized Inverses of Partitioned Matrices
If A is partitioned as
A =
!A11 A12
A21 A22
"
,
(3.213)
then, similar to equation (3.190), a generalized inverse of A is given by
A−=
⎡
⎣
A−
11 + A−
11A12Z−A21A−
11 −A−
11A12Z−
−Z−A21A−
11
Z−
⎤
⎦,
(3.214)
where Z = A22 −A21A−
11A12 (see Exercise 3.23, page 180).
If the inverses on the right-hand side of equation (3.214) are Moore-Penrose
inverses, then the result is the Moore-Penrose inverse of A.
If the partitioning in (3.213) happens to be such that A11 is of full rank
and of the same rank as A, a generalized inverse of A is given by
A−=
⎡
⎣
A−1
11 0
0
0
⎤
⎦,
(3.215)
where 0 represents matrices of the appropriate shapes. The generalized inverse
given in equation (3.215) is the same as the Moore-Penrose inverse given in
equation (3.209), but it is not necessarily the same generalized inverse as in
equation (3.214). The fact that it is a generalized inverse is easy to establish
by using the deﬁnition of generalized inverse and equation (3.189).
3.7 Orthogonality
In Sect. 2.1.8, we deﬁned orthogonality and orthonormality of two or more
vectors in terms of dot products. On page 98, in equation (3.112), we also
deﬁned the orthogonal binary relationship between two matrices. Now we
deﬁne the orthogonal unary property of a matrix. This is the more important
property and is what is commonly meant when we speak of orthogonality of
matrices. We use the orthonormality property of vectors, which is a binary
relationship, to deﬁne orthogonality of a single matrix.

132
3 Basic Properties of Matrices
3.7.1 Orthogonal Matrices: Deﬁnition and Simple Properties
A matrix whose rows or columns constitute a set of orthonormal vectors is
said to be an orthogonal matrix. If Q is an n × m orthogonal matrix, then
QQT = In if n ≤m, and QTQ = Im if n ≥m. If Q is a square orthogonal
matrix, then QQT = QTQ = I.
The determinant of a square orthogonal matrix is ±1 (because the deter-
minant of the product is the product of the determinants and the determinant
of I is 1).
When n ≥m, the matrix inner product of an n × m orthogonal matrix Q
with itself is its number of columns:
⟨Q, Q⟩= m.
(3.216)
This is because QTQ = Im. If n ≤m, the matrix inner product of Q with
itself is its number of rows.
Recalling the deﬁnition of the orthogonal binary relationship from page 98,
we note that if Q is an orthogonal matrix, then Q is not orthogonal to itself
in that sense.
A permutation matrix (see page 81) is orthogonal. We can see this by
building the permutation matrix as a product of elementary permutation ma-
trices, and it is easy to see that they are all orthogonal.
One further property we see by simple multiplication is that if A and B
are orthogonal, then A ⊗B is orthogonal.
The deﬁnition of orthogonality is sometimes made more restrictive to re-
quire that the matrix be square.
An Aside: Unitary Matrices
For square matrices whose elements are complex numbers, a matrix
is said to be unitary if the matrix times its conjugate transpose is
the identity; that is, if UU H = U HU = I. Transformations using
unitary matrices are analogous in many ways to transformations using
orthogonal matrices, but there are important diﬀerences.
An orthogonal matrix with real elements is also a unitary matrix.
The deﬁnition of orthogonality of vectors is the same for complex vec-
tors as it is for real vectors; in both cases, it is that the inner product
is 0. Because of our emphasis on real vectors and matrices, we often
think of orthogonality of vectors in terms of xTy, but this only applies
to real vectors. In general, x and y are orthogonal if xHy = 0, which is
the inner product. The corresponding binary relationship of orthogo-
nality for matrices, as deﬁned in equation (3.112) on page 98, likewise
depends on an inner product, which is given in equation (3.107). The
relationship in equation (3.108) may not be correct if the elements are
not real.

3.7 Orthogonality
133
For matrices, orthogonality is both a type of binary relationship and
a unary property. The unary property of orthogonality is deﬁned in
terms of a transpose. A matrix that is orthogonal is also unitary only
if it is real.
3.7.2 Orthogonal and Orthonormal Columns
The deﬁnition given above for orthogonal matrices is sometimes relaxed to re-
quire only that the columns or rows be orthogonal (rather than orthonormal).
If orthonormality is not required, the determinant is not necessarily ±1. If Q
is a matrix that is “orthogonal” in this weaker sense of the deﬁnition, and Q
has more rows than columns, then
QTQ =
⎡
⎢⎢⎢⎣
X 0 · · · 0
0 X · · · 0
...
0 0 · · · X
⎤
⎥⎥⎥⎦.
Unless stated otherwise, I use the term “orthogonal matrix” to refer to a
matrix whose columns are orthonormal; that is, for which QTQ = I.
3.7.3 The Orthogonal Group
The set of n×m orthogonal matrices for which n ≥m is called an (n, m) Stiefel
manifold, and an (n, n) Stiefel manifold together with Cayley multiplication
is a group, sometimes called the orthogonal group and denoted as O(n). The
orthogonal group O(n) is a subgroup of the general linear group GL(n), deﬁned
on page 114. The orthogonal group is useful in multivariate analysis because of
the invariance of the so-called Haar measure over this group (see Sect. 4.5.1).
Because the Euclidean norm of any column of an n×m orthogonal matrix
with n ≥m is 1, no element in the matrix can be greater than 1 in absolute
value. We therefore have an analogue of the Bolzano-Weierstrass theorem for
sequences of orthogonal matrices. The standard Bolzano-Weierstrass theorem
for real numbers states that if a sequence ai is bounded, then there exists a
subsequence aij that converges. (See any text on real analysis.) From this, we
conclude that if Q1, Q2, . . . is a sequence of n × n orthogonal matrices, then
there exists a subsequence Qi1, Qi2, . . ., such that
lim
j→∞Qij = Q,
(3.217)
where Q is some ﬁxed matrix. The limiting matrix Q must also be orthogonal
because QT
ijQij = I, and so, taking limits, we have QTQ = I. The set of n×n
orthogonal matrices is therefore compact.

134
3 Basic Properties of Matrices
3.7.4 Conjugacy
Instead of deﬁning orthogonality of vectors in terms of dot products as in
Sect. 2.1.8, we could deﬁne it more generally in terms of a bilinear form as
in Sect. 3.2.9. If the bilinear form xTAy = 0, we say x and y are orthogonal
with respect to the matrix A. We also often use a diﬀerent term and say
that the vectors are conjugate with respect to A, as in equation (3.93). The
usual deﬁnition of orthogonality in terms of a dot product is equivalent to the
deﬁnition in terms of a bilinear form in the identity matrix.
Likewise, but less often, orthogonality of matrices is generalized to conju-
gacy of two matrices with respect to a third matrix: QTAQ = I.
3.8 Eigenanalysis: Canonical Factorizations
Throughout this section on eigenanalysis, we will generally implicitly assume
that the matrices we discuss are square, unless we state otherwise.
Multiplication of a given vector by a square matrix may result in a scalar
multiple of the vector. Stating this more formally, and giving names to such
a special vector and scalar, if A is an n × n (square) matrix, v is a vector not
equal to 0, and c is a scalar such that
Av = cv,
(3.218)
we say v is an eigenvector of the matrix A, and c is an eigenvalue of the matrix
A. We refer to the pair c and v as an associated eigenvector and eigenvalue
or as an eigenpair.
We immediately note that if v is an eigenvector of A, then for any scalar,
b, because A(bv) = c(bv), bv is also an eigenvector of A. (We will exclude the
case b = 0, so that we do not consider the 0 vector to be an eigenvector of A.)
That is, any vector in the double cone generated by an eigenvector, except
the 0 vector, is an eigenvector (see discussion of cones, beginning on page 43).
While we restrict an eigenvector to be nonzero (or else we would have 0 as
an eigenvector associated with any number being an eigenvalue), an eigenvalue
can be 0; in that case, of course, the matrix must be singular. (Some authors
restrict the deﬁnition of an eigenvalue to real values that satisfy (3.218), and
there is an important class of matrices for which it is known that all eigenvalues
are real. In this book, we do not want to restrict ourselves to that class; hence,
we do not require c or v in equation (3.218) to be real.)
We use the term “eigenanalysis” or “eigenproblem” to refer to the gen-
eral theory, applications, or computations related to either eigenvectors or
eigenvalues.
There are various other terms used for eigenvalues and eigenvectors. An
eigenvalue is also called a characteristic value (that is why I use a “c” to
represent an eigenvalue), a latent root (that is why I also might use a “λ” to

3.8 Eigenanalysis: Canonical Factorizations
135
represent an eigenvalue), or a proper value, and similar synonyms exist for an
eigenvector. An eigenvalue is also sometimes called a singular value, but the
latter term has a diﬀerent meaning that we will use in this book (see page 161;
the absolute value of an eigenvalue is a singular value, and singular values are
also deﬁned for nonsquare matrices).
Although generally throughout this chapter we have assumed that vectors
and matrices are real, in eigenanalysis, even if A is real, it may be the case
that c and v are complex. Therefore, in this section, we must be careful about
the nature of the eigenpairs, even though we will continue to assume the basic
matrices are real.
3.8.1 Eigenvalues and Eigenvectors Are Remarkable
Before proceeding to consider properties of eigenvalues and eigenvectors, we
should note how remarkable the relationship Av = cv is.
The eﬀect of a matrix multiplication of an eigenvector is the same as a
scalar multiplication of the eigenvector.
The eigenvector is an invariant of the transformation in the sense that its
direction does not change. This would seem to indicate that the eigenvalue
and eigenvector depend on some kind of deep properties of the matrix, and
indeed this is the case, as we will see.
Of course, the ﬁrst question is, for a given matrix, do such special vectors
and scalars exist?
The answer is yes.
The next question is, for a given matrix, what is the formula for the eigen-
values (or what is a ﬁnite sequence of steps to compute the eigenvalues)?
The answer is a formula does not exist and there is no ﬁnite sequence
of steps, in general, for determining the eigenvalues (if the matrix is
bigger than 4 × 4).
Before considering these and other more complicated issues, we will state
some simple properties of any scalar and vector that satisfy Av = cv and
introduce some additional terminology.
3.8.2 Left Eigenvectors
In the following, when we speak of an eigenvector or eigenpair without qual-
iﬁcation, we will mean the objects deﬁned by equation (3.218). There is
another type of eigenvector for A, however, a left eigenvector, deﬁned as a
nonzero w in
wTA = cwT.
(3.219)
For emphasis, we sometimes refer to the eigenvector of equation (3.218), Av =
cv, as a right eigenvector.

136
3 Basic Properties of Matrices
We see from the deﬁnition of a left eigenvector, that if a matrix is sym-
metric, each left eigenvector is an eigenvector (a right eigenvector).
If v is an eigenvector of A and w is a left eigenvector of A with a diﬀerent
associated eigenvalue, then v and w are orthogonal; that is, if Av = c1v,
wTA = c2wT, and c1 ̸= c2, then wTv = 0. We see this by multiplying both
sides of wTA = c2wT by v to get wTAv = c2wTv and multiplying both sides
of Av = c1v by wT to get wTAv = c1wTv. Hence, we have c1wTv = c2wTv,
and because c1 ̸= c2, we have wTv = 0.
3.8.3 Basic Properties of Eigenvalues and Eigenvectors
If c is an eigenvalue and v is a corresponding eigenvector for a real matrix
A, we see immediately from the deﬁnition of eigenvector and eigenvalue in
equation (3.218) the following properties. (In Exercise 3.25, you are asked to
supply the simple proofs for these properties, or you can see the proofs in a
text such as Harville 1997, for example.)
Assume that Av = cv and that all elements of A are real.
1. bv is an eigenvector of A, where b is any nonzero scalar.
It is often desirable to scale an eigenvector v so that vTv = 1. Such
an eigenvector is also called a “unit eigenvector”, but I prefer the term
“normalized eigenvector” because of the use of the phrase “unit vector”
to refer to the special vectors ei.
For a given eigenvector, there is always a particular eigenvalue associated
with it, but for a given eigenvalue there is a space of associated eigen-
vectors. (The space is a vector space if we consider the zero vector to
be a member.) It is therefore not appropriate to speak of “the” eigen-
vector associated with a given eigenvalue—although we do use this term
occasionally. (We could interpret it as referring to the normalized eigen-
vector.) There is, however, another sense in which an eigenvalue does not
determine a unique eigenvector, as we discuss below.
2. bc is an eigenvalue of bA, where b is any nonzero scalar.
3. 1/c and v are an eigenpair of A−1 (if A is nonsingular).
4. 1/c and v are an eigenpair of A+ if A (and hence A+) is square and c is
nonzero.
5. If A is diagonal or triangular with elements aii, the eigenvalues are aii, and
for diagonal A the corresponding eigenvectors are ei (the unit vectors).
6. c2 and v are an eigenpair of A2. More generally, ck and v are an eigenpair
of Ak for k = 1, 2, . . ..
7. c −d and v are an eigenpair of A −dI. This obvious fact is useful in
computing eigenvalues (see Sect. 7.1.5).
8. If A and B are conformable for the multiplications AB and BA, the
nonzero eigenvalues of AB are the same as the nonzero eigenvalues of
BA. (Note that A and B are not necessarily square.) All of eigenvalues
are the same if A and B are square. (Note, however, that if A and B are

3.8 Eigenanalysis: Canonical Factorizations
137
square and d is an eigenvalue of B, d is not necessarily an eigenvalue of
AB.)
9. If A and B are square and of the same order and if B−1 exists, then the
eigenvalues of BAB−1 are the same as the eigenvalues of A. (This is called
a similarity transformation; see page 146.)
List continued on page 140.
3.8.3.1 Eigenvalues of Elementary Operator Matrices
For a matrix with a very simple pattern, such as a disagonal matrix, whose
determinant is just the product of the elements, we can determine the eigen-
values by inspection. For example, it is clear immediately that all eigenvalues
of the identity matrix are 1s. (Although they are all the same, we still say there
are n of them, if n is the order of the identity. Multiplicity of eigenvalues is
an important property, which we will discuss beginning on page 143.)
Because of their simple patterns, we can also easily determine the eigen-
values of elementary operator matrices, possibly by considering one or two
adjugates that arise from submatrices that are identity matrices.
The eigenvalues of the 2 × 2 permutation
!
0 1
1 0
"
are just the two square roots of 1; that is, 1 and −1. From this, using partitions
of an elementary permutation matrix Epq of order n to form adjugates that are
identities, we see that the eigenvalues of an elementary permutation matrix
Epq are n −1 1s and one −1.
With a little more eﬀort we can determine the eigenvalues of general per-
mutation matrices. Following the preceding approach, we immediately see that
the eigenvalues of the matrix
⎡
⎣
0 0 1
0 1 0
1 0 0
⎤
⎦
are the three cube roots of 1, two of which contain imaginary components. In
Chap. 8, on page 388, we describe the full set of eigenvalues for a permutation
matrix in which all rows are moved.
By inspection of the determinant, we see that the eigenvalues of an
elementary row-multiplication matrix Ep(a) of order n are n −1 1s and
one a.
Again by inspection of the determinant, we see that the eigenvalues of an
elementary axpy matrix Epq(a) of order n are n 1s, the same as the identity
itself.

138
3 Basic Properties of Matrices
3.8.4 The Characteristic Polynomial
From the equation (A −cI)v = 0 that deﬁnes eigenvalues and eigenvectors,
we see that in order for v to be nonnull, (A −cI) must be singular, and hence
det(A −cI) = det(cI −A) = 0.
(3.220)
Equation (3.220) is sometimes taken as the deﬁnition of an eigenvalue c. It is
deﬁnitely a fundamental relation, and, as we will see, allows us to identify a
number of useful properties.
For the n×n matrix A, the determinant in equation (3.220) is a polynomial
of degree n in c, pA(c), called the characteristic polynomial, and when it is
equated to 0, it is called the characteristic equation:
pA(c) = s0 + s1c + · · · + sncn = 0.
(3.221)
From the expansion of the determinant det(cI −A), as in equation (3.41)
on page 73, we see that s0 = (−1)ndet(A) and sn = 1, and, in general,
sk = (−1)n−k times the sums of all principal minors of A of order n −k.
(Note that the signs of the si are diﬀerent depending on whether we use
det(cI −A) or det(A −cI).)
An eigenvalue of A is a root of the characteristic polynomial. The existence
of n roots of the polynomial (by the Fundamental Theorem of Algebra) allows
the characteristic polynomial to be written in factored form as
pA(c) = (−1)n(c −c1) · · · (c −cn),
(3.222)
and establishes the existence of n eigenvalues. Some may be complex, some
may be zero, and some may be equal to others. We call the set of all eigen-
values the spectrum of the matrix. The “number of eigenvalues” must be
distinguished from the cardinality of the spectrum, which is the number of
unique values.
A real matrix may have complex eigenvalues (and, hence, eigenvectors),
just as a polynomial with real coeﬃcients can have complex roots. Clearly, the
eigenvalues of a real matrix must occur in conjugate pairs just as in the case of
roots of polynomials with real coeﬃcients. (As mentioned above, some authors
restrict the deﬁnition of an eigenvalue to real values that satisfy (3.218). We
will see below that the eigenvalues of a real symmetric matrix are always real,
and this is a case that we will emphasize, but in this book we do not restrict
the deﬁnition.)
The characteristic polynomial has many interesting properties. One, stated
in the Cayley-Hamilton theorem, is that the matrix itself is a root of the matrix
polynomial formed by the characteristic polynomial; that is,
pA(A) = s0I + s1A + · · · + snAn = 0n.
(3.223)
We see this by using equation (3.34) to write the matrix in equation
(3.220) as

3.8 Eigenanalysis: Canonical Factorizations
139
(A −cI)adj(A −cI) = pA(c)I.
(3.224)
Hence adj(A −cI) is a polynomial in c of degree less than or equal to n −1,
so we can write it as
adj(A −cI) = B0 + B1c + · · · + Bn−1cn−1,
where the Bi are n × n matrices. Now, equating the coeﬃcients of c on the
two sides of equation (3.224), we have
AB0 = s0I
AB1 −B0 = s1I
...
ABn−1 −Bn−2 = sn−1I
Bn−1 = snI.
Now, multiply the second equation by A, the third equation by A2, and the ith
equation by Ai−1, and add all equations. We get the desired result: pA(A) = 0.
See also Exercise 3.26.
Another interesting fact is that any given nth-degree polynomial, p, is the
characteristic polynomial of an n × n matrix, A, of particularly simple form.
Consider the polynomial
p(c) = s0 + s1c + · · · + sn−1cn−1 + cn
and the matrix
A =
⎡
⎢⎢⎢⎢⎢⎣
0
1
0
· · ·
0
0
0
1
· · ·
0
...
0
0
0
· · ·
1
−s0 −s1 −s2 · · · −sn−1
⎤
⎥⎥⎥⎥⎥⎦
.
(3.225)
(Note that this matrix is the same as the Jordan block (see page 78), ex-
cept that the last row of 0s is replaced with the row of coeﬃcients of the
characteristic equation.) The matrix A is called the companion matrix of the
polynomial p, and it is easy to see (by a tedious expansion) that the char-
acteristic polynomial of A is p. This, of course, shows that a characteristic
polynomial does not uniquely determine a matrix, although the converse is
true (within signs).
3.8.4.1 Additional Properties of Eigenvalues and Eigenvectors
Using the characteristic polynomial yields the following properties. This is a
continuation of the list we began on page 136. We assume A is a real matrix
with eigenpair (c, v).

140
3 Basic Properties of Matrices
10. c is an eigenvalue of AT (because det(AT−cI) = det(A−cI) for any c). The
eigenvectors of AT, which are left eigenvectors of A, are not necessarily
the same as the eigenvectors of A, however.
11. There is a left eigenvector such that c is the associated eigenvalue.
12. (¯c, ¯v) is an eigenpair of A, where ¯c and ¯v are the complex conjugates
and A, as usual, consists of real elements. (If c and v are real, this is a
tautology.)
13. c¯c is an eigenvalue of the Gramian matrix ATA.
14. The nonzero eigenvalues of the Gramian matrix ATA are the same as
the nonzero eigenvalues of the Gramian matrix AAT. (This is actually
property 8 on page 136.)
15. c is real if A is symmetric or if A is triangular (the elements of A are
assumed to be real, of course).
In Exercise 3.27, you are asked to supply the simple proofs for these properties,
or you can see the proofs in a text such as Harville (1997), for example.
A further comment on property 12 may be worthwhile. Throughout this
book, we assume we begin with real numbers. There are some times, however,
when standard operations in the real domain carry us outside the reals. The
simplest situations, which of course are related, are roots of polynomial equa-
tions with real coeﬃcients and eigenpairs of matrices with real elements. In
both of these situations, because sums must be real, the complex values occur
in conjugate pairs.
There are many additional interesting properties of eigenvalues and eigen-
vectors that we will encounter in later sections, but there is one more that I
want to list here with these very basic and important properties:
16. |c| ≤∥A∥, where ∥· ∥is any consistent matrix norm.
(We will discuss matrix norms in Sect. 3.9 beginning on page 164, and this
particular bound is given in equation (3.310) in that section. In my deﬁnition
of matrix norm, all norms are required to be consistent.)
3.8.4.2 Eigenvalues and the Trace and Determinant
If the eigenvalues of the matrix A are c1, . . . , cn, because they are the roots
of the characteristic polynomial, we can readily form that polynomial as
pA(c) = (c −c1) · · · (c −cn)
= (−1)n $
ci + · · · + (−1)n−1 
cicn−1 + cn.
(3.226)
Because this is the same polynomial as obtained by the expansion of the
determinant in equation (3.221), the coeﬃcients must be equal. In particular,
by simply equating the corresponding coeﬃcients of the constant terms and
(n −1)th-degree terms, we have the two very important facts:
det(A) =
$
ci
(3.227)

3.8 Eigenanalysis: Canonical Factorizations
141
and
tr(A) =

ci.
(3.228)
It might be worth recalling that we have assumed that A is real, and
therefore det(A) and tr(A) are real, but the eigenvalues ci may not be real.
Nonreal eigenvalues, however, occur in conjugate pairs (property 12 above);
hence ) ci and  ci are real even though the individual elements may not be.
3.8.5 The Spectrum
Although, for an n × n matrix, from the characteristic polynomial we have
n roots, and hence n eigenvalues, some of these roots may be the same. It
may also be the case that more than one eigenvector corresponds to a given
eigenvalue. As we mentioned above, the set of all the distinct eigenvalues of a
matrix is called the spectrum of the matrix.
3.8.5.1 Notation
Sometimes it is convenient to refer to the distinct eigenvalues and sometimes
we wish to refer to all eigenvalues, as in referring to the number of roots of the
characteristic polynomial. To refer to the distinct eigenvalues in a way that
allows us to be consistent in the subscripts, we will call the distinct eigenvalues
λ1, . . . , λk. The set of these constitutes the spectrum.
We denote the spectrum of the matrix A by σ(A):
σ(A) = {λ1, . . . , λk}.
(3.229)
We see immediately that σ(AT) = σ(A) (property 10 above).
In terms of the spectrum, equation (3.222) becomes
pA(c) = (−1)n(c −λ1)m1 · · · (c −λk)mk,
(3.230)
for mi ≥1.
We label the ci and vi so that
|c1| ≥· · · ≥|cn|.
(3.231)
We likewise label the λi so that
|λ1| > · · · > |λk|.
(3.232)
With this notation, we have
|λ1| = |c1|
and
|λk| = |cn|,
but we cannot say anything about the other λs and cs.

142
3 Basic Properties of Matrices
3.8.5.2 The Spectral Radius
For the matrix A with these eigenvalues, |c1| is called the spectral radius and
is denoted by ρ(A):
ρ(A) = max |ci| = |c1| = |λ1|.
(3.233)
We immediately note that ρ(AT) = ρ(A).
The set of complex numbers
{z : |z| = ρ(A)}
(3.234)
is called the spectral circle of A.
An eigenvalue equal to ± max|ci| (that is, equal to ±c1) is called a domi-
nant eigenvalue. We are more often interested in the absolute value (or mod-
ulus) of a dominant eigenvalue rather than the eigenvalue itself; that is, ρ(A)
(or |c1|) is more often of interest than c1.
Interestingly, we have for all i
|ci| ≤max
j

k
|akj|
(3.235)
and
|ci| ≤max
k

j
|akj|.
(3.236)
The inequalities of course also hold for ρ(A) on the left-hand side. Rather
than proving this here, we show this fact in a more general setting relating
to matrix norms in inequality (3.310) on page 171. (The two bounds above
relate to the L1 and L∞matrix norms, respectively, as we will see.)
The spectral radius gives a very simple indication of the region in the
complex plane in which the entire spectrum lies. Consider, for example, the
matrix
A =
⎡
⎣
9
−6
1
−2
9
−5
10
−2
4
⎤
⎦.
(3.237)
(See Exercise 3.24 for comments on the origins of this matrix.)
From equation (3.235), we see that all eigenvalues are less than or equal
to 16 in modulus. In fact, the eigenvalues are σ(A) = {16, 3 + 4i, 3 −4i}, and
ρ(A) = 16.
On page 145, we will discuss other regions of the complex plane in which
all eigenvalues necessarily lie.
A matrix may have all eigenvalues equal to 0 but yet the matrix itself may
not be 0. (The matrix must be singular, however.) A nilpotent matrix (see
page 77), as well as any upper triangular matrix with all 0s on the diagonal
are examples.

3.8 Eigenanalysis: Canonical Factorizations
143
Because, as we saw on page 136, if c is an eigenvalue of A, then bc is an
eigenvalue of bA where b is any nonzero scalar, we can scale a matrix with a
nonzero eigenvalue so that its spectral radius is 1. The scaled matrix is simply
S = A/|c1|, and ρ(S) = 1.
The spectral radius, is one of the most important properties of a matrix.
As we will see in Sect. 3.9.1, it is the the Lp norm for a symmetric matrix.
From equations (3.235) and (3.236), we have seen in any event that it is
bounded from above by the L1 and L∞matrix norms (which we will deﬁne
formally in Sect. 3.9.1), and, in fact, in equation (3.310) we will see that the
spectral radius is bounded from above by any matrix norm. We will discuss
the spectral radius further in Sects. 3.9.5 and 3.9.6. In Sect. 3.9.6 we will see
that the spectral radius determines the convergence of a matrix power series
(and this fact is related to the behavior of autoregressive processes).
3.8.5.3 Linear Independence of Eigenvectors Associated with
Distinct Eigenvalues
Suppose that {λ1, . . . , λk} is a set of distinct eigenvalues of the matrix A
and {x1, . . . , xk} is a set of eigenvectors such that (λi, xi) is an eigenpair.
Then x1, . . . , xk are linearly independent; that is, eigenvectors associated with
distinct eigenvalues are linearly independent.
We can see that this must be the case by assuming that the eigenvectors
are not linearly independent. In that case, let {y1, . . . , yj} ⊂{x1, . . . , xk}, for
some j < k, be a maximal linearly independent subset. Let the corresponding
eigenvalues be {μ1, . . . , μj} ⊂{λ1, . . . , λk}. Then, for some eigenvector yj+1,
we have
yj+1 =
j

i=1
tiyi
for some ti. Now, multiplying both sides of the equation by A −μj+1I, where
μj+1 is the eigenvalue corresponding to yj+1, we have
0 =
j

i=1
ti(μi −μj+1)yi.
If the eigenvalues are distinct (that is, for each i ≤j), we have μi ̸= μj+1,
then the assumption that the eigenvalues are not linearly independent is con-
tradicted because otherwise we would have a linear combination with nonzero
coeﬃcients equal to zero.
3.8.5.4 The Eigenspace and Geometric Multiplicity
Rewriting the deﬁnition (3.218) for the ith eigenvalue and associated eigen-
vector of the n × n matrix A as

144
3 Basic Properties of Matrices
(A −ciI)vi = 0,
(3.238)
we see that the eigenvector vi is in N(A −ciI), the null space of (A −ciI).
For such a nonnull vector to exist, of course, (A −ciI) must be singular; that
is, rank(A −ciI) must be less than n. This null space is called the eigenspace
of the eigenvalue ci.
It is possible that a given eigenvalue may have more than one associated
eigenvector that are linearly independent of each other. For example, we eas-
ily see that the identity matrix has only one distinct eigenvalue, namely 1,
but any vector is an eigenvector, and so the number of linearly independent
eigenvectors is equal to the number of rows or columns of the identity. If u
and v are eigenvectors corresponding to the same eigenvalue λ, then any lin-
ear combination of u and v is an eigenvector corresponding to λ; that is, if
Au = λu and Av = λv, for any scalars a and b,
A(au + bv) = λ(au + bv).
The dimension of the eigenspace corresponding to the eigenvalue ci is
called the geometric multiplicity of ci;
that is, the geometric multiplicity
of ci is the nullity of A −ciI. If gi is the geometric multiplicity of ci, an
eigenvalue of the n × n matrix A, then we can see from equation (3.205) that
rank(A −ciI) + gi = n.
The multiplicity of 0 as an eigenvalue is just the nullity of A. If A is of full
rank, the multiplicity of 0 will be 0, but, in this case, we do not consider 0 to
be an eigenvalue. If A is singular, however, we consider 0 to be an eigenvalue,
and the multiplicity of the 0 eigenvalue is the rank deﬁciency of A.
Multiple linearly independent eigenvectors corresponding to the same
eigenvalue can be chosen to be orthogonal to each other using, for example,
the Gram-Schmidt transformations, as in equation (2.56) on page 38. These
orthogonal eigenvectors span the same eigenspace. They are not unique, of
course, as any sequence of Gram-Schmidt transformations could be applied.
3.8.5.5 Algebraic Multiplicity
A single value that occurs as a root of the characteristic equation m times
is said to have algebraic multiplicity m. Although we sometimes refer to this
as just the multiplicity, algebraic multiplicity should be distinguished from
geometric multiplicity, deﬁned above. These are not the same, as we will see
in an example later (page 150). The algebraic multiplicity of a given eigenvalue
is at least as great as its geometric multiplicity (exercise).
An eigenvalue whose algebraic multiplicity and geometric multiplicity are
the same is called a semisimple eigenvalue. An eigenvalue with algebraic mul-
tiplicity 1 is called a simple eigenvalue (hence, a simple eigenvalue semisimple
eigenvalue).
Because the determinant that deﬁnes the eigenvalues of an n×n matrix is
an nth-degree polynomial, we see that the sum of the multiplicities of distinct
eigenvalues is n.

3.8 Eigenanalysis: Canonical Factorizations
145
3.8.5.6 Gershgorin Disks
In addition to the spectral circle, there is a another speciﬁcation of regions in
the complex plane that regions in the complex plane that contain the spectrum
of an n × n matrix A. This is the union of the n Gershgorin disks, where for
i = 1, . . . , n, the ith of which is the disk
|z −aii| ≤ri
where ri =

1≤j≤n; j̸=i
|aij|.
(3.239)
(“Gershgorin” is often spelled as “Gerschgorin” or “Gersgorin” or even
“Gerˇsgorin”; he was Russian.)
To see that this is the case, let (c, v) be an arbitrary eigenpair of A with v
normalized by the L∞norm (that is, max(v) = 1). Let k be such that |vk| = 1.
Then
cvk = (Av)k =
n

j=1
akjvj;
hence
(c −akk)vk =

1≤j≤n; j̸=k
akjvj.
Now introduce the modulus, and we get the desired inequality:
|c −akk| = |c −akk||vk|
=


1≤j≤n; j̸=k
akjvj

≤

1≤j≤n; j̸=k
|akj||vj|
≤

1≤j≤n; j̸=k
|akj|
= rk.
We conclude that every eigenvalue lies in some similar disk; that is, the spec-
trum lies in the union of such disks.
Since σ(AT) = σ(A), using the same argument as above, we can deﬁne
another collection of n Gershgorin disks based on column sums:
|z −ajj| ≤sj
where sj =

1≤i≤n; i̸=j
|aij|.
(3.240)
All eigenvalues of A lie within the union of these disks.
Combining the two restrictions, we see that all eigenvalues of A lie within
the intersection of these two unions of Gershgorin disks.

146
3 Basic Properties of Matrices
3.8.6 Similarity Transformations
Two n×n matrices, A and B, are said to be similar if there exists a nonsingular
matrix P such that
B = P −1AP.
(3.241)
The transformation in equation (3.241) is called a similarity transformation.
(Compare similar matrices with equivalent matrices on page 110. The matri-
ces A and B in equation (3.241) are also equivalent, as we see using equa-
tions (3.153) and (3.154).)
It is clear from the deﬁnition that the similarity relationship is both com-
mutative and transitive.
If A and B are similar, as in equation (3.241), then for any scalar c
det(A −cI) = det(P −1)det(A −cI)det(P)
= det(P −1AP −cP −1IP)
= det(B −cI),
and, hence, A and B have the same eigenvalues. (This simple fact was stated
as property 9 on page 137.)
3.8.6.1 Orthogonally and Unitarily Similar Transformations
An important type of similarity transformation is based on an orthogonal
matrix in equation (3.241). If Q is orthogonal and
B = QTAQ,
(3.242)
A and B are said to be orthogonally similar.
If B in equation (3.242) B = QTAQ is a diagonal matrix, A is said to
be orthogonally diagonalizable, and QBQT is called the orthogonally diagonal
factorization or orthogonally similar factorization of A.
The concepts of orthogonally similar and orthogonal diagonalization are
very important, but for matrices with complex entries or for real matrices
with complex eigenvalues, generalizations of the concepts based on unitary
matrices are more useful. If U is unitary and
B = U HAU,
(3.243)
A and B are said to be unitarily similar. Since an orthogonal matrix is unitary,
two matrices that are orthogonally similar are also unitarily similar.
If B in equation (3.243) B = U HAU is a diagonal matrix, A is said to
be unitarily diagonalizable, and QBQT is called the unitarily diagonal factor-
ization or unitarily similar factorization of A. A matrix that is orthogonally
diagonalizable is also unitarily diagonalizable.

3.8 Eigenanalysis: Canonical Factorizations
147
We will discuss characteristics of orthogonally diagonalizable matrices in
Sects. 3.8.8 through 3.8.10 below. The signiﬁcant fact that we will see there
is that a matrix is orthogonally diagonalizable if and only if it is symmetric.
We will discuss characteristics of unitarily diagonalizable matrices in
Sect. 8.2.3 on page 345. The signiﬁcant fact that we will see there is that
a matrix is unitarily diagonalizable if and only if it is normal (which includes
symmetric matrices).
3.8.6.2 Uses of Similarity Transformations
Similarity transformations are very useful in establishing properties of matri-
ces, such as convergence properties of sequences (see, for example, Sect. 3.9.6).
Similarity transformations are also used in algorithms for computing eigenval-
ues (see, for example, Sect. 7.3). In an orthogonally similar factorization, the
elements of the diagonal matrix are the eigenvalues. Although the diagonals
in the upper triangular matrix of the Schur factorization are the eigenvalues,
that particular factorization is rarely used in computations.
Although similar matrices have the same eigenvalues, they do not neces-
sarily have the same eigenvectors. If A and B are similar, for some nonzero
vector v and some scalar c, Av = cv implies that there exists a nonzero vector
u such that Bu = cu, but it does not imply that u = v (see Exercise 3.29b).
3.8.7 Schur Factorization
If B in equation (3.242) is an upper triangular matrix, QBQT is called the
Schur factorization of A:
A = QBQT.
(3.244)
This is also called the “Schur form” of A.
For any square matrix, the Schur factorization exists. Although the ma-
trices in the factorization are not unique, the diagonal elements of the upper
triangular matrix B are the eigenvalues of A.
There are various forms of the Schur factorization. Because in general the
eigenvalues and eigenvectors may contain imaginary components, the orthog-
onal matrices in equation (3.244) may contain imaginary components, and
furthermore, the Schur factorization is particularly useful in studying factor-
izations involving unitary matrices, we will describe the Schur factorization
that use unitary matrices.
The general form of the Schur factorization for a square matrix A is
A = UT U H,
(3.245)
where U is a unitary matrix and T is an upper triangular matrix whose
diagonal entries are the eigenvalues of A.
The Schur factorization exists for any square matrix, which we can see by
induction. It clearly exists in the degenerate case of a 1×1 matrix. To see that

148
3 Basic Properties of Matrices
it exists for any n × n matrix A, let (c, v) be an arbitrary eigenpair of A with
v normalized, and form a unitary matrix U1 with v as its ﬁrst column. Let U2
be the matrix consisting of the remaining columns; that is, U1 is partitioned
as [v | U2].
U H
1 AU1 =
!
vHAv
vHAU2
U H
2 Av
U H
2 AU2
"
=
! c
vHAU2
0
U H
2 AU2
"
= T,
where U H
2 AU2 is an (n −1) × (n −1) matrix. Now the eigenvalues of U HAU
are the same as those of A; hence, if n = 2, then U H
2 AU2 is a scalar and must
equal the other eigenvalue, and so the statement is proven for a 2 × 2 matrix.
We now use induction on n to establish the general case. Assume that the
factorization exists for any (n −1) × (n −1) matrix, and let A be any n × n
matrix. We let (c, v) be an arbitrary eigenpair of A (with v normalized), follow
the same procedure as in the preceding paragraph, and get
U H
1 AU1 =
! c
vHAU2
0
U H
2 AU2
"
.
Now, since U H
2 AU2 is an (n−1)× (n−1) matrix, by the induction hypothesis
there exists an (n−1)×(n−1) Hermitian matrix V such that V H(U H
2 AU2)V =
T1, where T1 is upper triangular. Now let
U = U1
!1 0
0 V
"
.
By multiplication, we see that U HU = I (that is, Q is Hermitian). Now form
U HAU =
!
c
vHAU2V
0
V HU H
2 AU2V
"
=
!
c
vHAU2V
0
T1
"
= T.
We see that T is upper triangular because T1 is, and so by induction the Schur
factorization exists for any n × n matrix.
The steps in the induction did not necessarily involve unique choices except
for the eigenvalues on the diagonal of T .
Note that the Schur factorization is also based on unitarily similar trans-
formations, but the term “unitarily similar factorization” is generally used
only to refer to the diagonal factorization.
3.8.8 Similar Canonical Factorization: Diagonalizable Matrices
If V is a matrix whose columns correspond to the eigenvectors of A, and C
is a diagonal matrix whose entries are the eigenvalues corresponding to the
columns of V , using the deﬁnition (equation (3.218)) we can write

3.8 Eigenanalysis: Canonical Factorizations
149
AV = V C.
(3.246)
Now, if V is nonsingular, we have
A = VCV −1.
(3.247)
Expression (3.247) represents a diagonal factorization of the matrix A. We see
that a matrix A with eigenvalues c1, . . . , cn that can be factorized this way is
similar to the matrix diag((c1, . . . , cn)), and this representation is sometimes
called the similar canonical form of A or the similar canonical factorization
of A.
Not all matrices can be factored as in equation (3.247). It obviously de-
pends on V being nonsingular; that is, that the eigenvectors are linearly inde-
pendent. If a matrix can be factored as in (3.247), it is called a diagonalizable
matrix, a simple matrix, or a regular matrix (the terms are synonymous, and
we will generally use the term “diagonalizable”); a matrix that cannot be fac-
tored in that way is called a deﬁcient matrix or a defective matrix (the terms
are synonymous).
Any matrix all of whose eigenvalues are unique (that is, distinct) is di-
agonalizable (because, as we saw on page 143, in that case the eigenvectors
are linearly independent), but uniqueness of the eigenvalues is not a necessary
condition.
A necessary and suﬃcient condition for a matrix to be diagonalizable can
be stated in terms of the unique eigenvalues and their multiplicities: suppose
for the n × n matrix A that the distinct eigenvalues λ1, . . . , λk have algebraic
multiplicities m1, . . . , mk. If, for l = 1, . . . , k,
rank(A −λlI) = n −ml
(3.248)
(that is, if all eigenvalues are semisimple), then A is diagonalizable, and this
condition is also necessary for A to be diagonalizable. This fact is called the
“diagonalizability theorem”. Recall that A being diagonalizable is equivalent
to V in AV = V C (equation (3.246)) being nonsingular.
To see that the condition is suﬃcient, assume, for each i, rank(A −ciI) =
n −mi, and so the equation (A −ciI)x = 0 has exactly n −(n −mi) linearly
independent solutions, which are by deﬁnition eigenvectors of A associated
with ci. (Note the somewhat complicated notation. Each ci is the same as
some λl, and for each λl, we have λl = cl1 = clml for 1 ≤l1 < · · · < lml ≤n.)
Let w1, . . . , wmi be a set of linearly independent eigenvectors associated with
ci, and let u be an eigenvector associated with cj and cj ̸= ci. (The vectors
w1, . . . , wmi and u are columns of V .) We have already seen on page 143 that
u must be linearly independent of the other eigenvectors, but we can also
use a slightly diﬀerent argument here. Now if u is not linearly independent of
w1, . . . , wmi, we write u =  bkwk, and so Au = A  bkwk = ci
 bkwk =
ciu, contradicting the assumption that u is not an eigenvector associated with
ci. Therefore, the eigenvectors associated with diﬀerent eigenvalues are linearly
independent, and so V is nonsingular.

150
3 Basic Properties of Matrices
Now, to see that the condition is necessary, assume V is nonsingular; that
is, V −1 exists. Because C is a diagonal matrix of all n eigenvalues, the matrix
(C −ciI) has exactly mi zeros on the diagonal, and hence, rank(C −ciI) =
n−mi. Because V (C −ciI)V −1 = (A−ciI), and multiplication by a full rank
matrix does not change the rank (see page 113), we have rank(A −ciI) =
n −mi.
3.8.8.1 Symmetric Matrices
A symmetric matrix is a diagonalizable matrix. We see this by ﬁrst letting A
be any n × n symmetric matrix with eigenvalue c of multiplicity m. We need
to show that rank(A −cI) = n −m. Let B = A −cI, which is symmetric
because A and I are. First, we note that c is real, and therefore B is real. Let
r = rank(B). From equation (3.164), we have
rank

B2
= rank

BTB

= rank(B) = r.
In the full rank partitioning of B, there is at least one r×r principal submatrix
of full rank. The r-order principal minor in B2 corresponding to any full rank
r × r principal submatrix of B is therefore positive. Furthermore, any j-order
principal minor in B2 for j > r is zero. Now, rewriting the characteristic
polynomial in equation (3.221) slightly by attaching the sign to the variable
w, we have
pB2(w) = tn−r(−w)n−r + · · · + tn−1(−w)n−1 + (−w)n = 0,
where tn−j is the sum of all j-order principal minors. Because tn−r ̸= 0, w = 0
is a root of multiplicity n−r. It is likewise an eigenvalue of B with multiplicity
n −r. Because A = B + cI, 0 + c is an eigenvalue of A with multiplicity n −r;
hence, m = n −r. Therefore n −m = r = rank(A −cI).
As we will see below in Sect. 3.8.10, a symmetric matrix A is not only
diagonalizable in the form (3.247), A = VCV −1, the matrix V can be chosen as
an orthogonal matrix, so we have A = UCU T. We will say that the symmetric
matrix is orthogonally diagonalizable.
3.8.8.2 A Defective Matrix
Although most matrices encountered in statistics applications are diagonal-
izable, it may be of interest to consider an example of a matrix that is not
diagonalizable. Searle (1982) gives an example of a small matrix:
A =
⎡
⎣
0 1 2
2 3 0
0 4 5
⎤
⎦.
The three strategically placed 0s make this matrix easy to work with, and the
determinant of (cI −A) yields the characteristic polynomial equation

3.8 Eigenanalysis: Canonical Factorizations
151
c3 −8c2 + 13c −6 = 0.
This can be factored as (c−6)(c−1)2, hence, we have eigenvalues c1 = 6 with
algebraic multiplicity m1 = 1, and c2 = 1 with algebraic multiplicity m2 = 2.
Now, consider A −c2I:
A −I =
⎡
⎣
−1 1 2
2 2 0
0 4 4
⎤
⎦.
This is clearly of rank 2; hence the rank of the null space of A −c2I (that
is, the geometric multiplicity of c2) is 3 −2 = 1. The matrix A is not
diagonalizable.
3.8.8.3 The Jordan Decomposition
Although not all matrices can be diagonalized in the form of equation (3.247),
V −1AV = C = diag(ci), any square matrix A can be expressed in the form
X−1AX = diag(Jji),
(3.249)
where the Jji are Jordan blocks associated with a single eigenvalue λj, of the
form
Jji(λj) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
λj 1
0 · · · 0
0
0 λj
1 · · · 0
0
0
0 λj · · · 0
0
...
...
...
... ... ...
0
0 · · ·
0
λj 1
0
0 · · ·
0
0 λj
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
or, in the degenerate case, Jji(λj) = [λj], where λj is a speciﬁc distinct eigen-
value (that is, λj ̸= λk if j ̸= k). (Compare this with the Jordan form of a
nilpotent matrix following equation (3.51) on page 77, in which the diagonal
elements are 0s.) If each Jordan block Jji is 1 × 1, the Jordan decomposition
is a diagonal decomposition.
There are some interesting facts about the Jordan decomposition. If there
are gj Jordan blocks associated with the eigenvalue λj, then λj has geometric
multiplicity gj. The algebraic multiplicity of λj is the total number of diagonal
elements in all the Jordan blocks associated with λj; hence, if each Jordan
block Jji is 1 × 1 then all eigenvalues are semisimple. While these two facts
appear rather profound, they are of little interest for our purposes, and we
will not give proofs. (Proofs can be found in Horn and Johnson (1991).) The
problem of computing a Jordan decomposition is ill-conditioned because slight
perturbations in the elements of A can obviously result in completely diﬀerent
sets of Jordan blocks.

152
3 Basic Properties of Matrices
3.8.9 Properties of Diagonalizable Matrices
If the matrix A has the similar canonical factorization VCV −1 of equa-
tion (3.247), some important properties are immediately apparent. First of
all, this factorization implies that the eigenvectors of a diagonalizable matrix
are linearly independent.
Other properties are easy to derive or to show because of this factorization.
For example, the general equations (3.227) and (3.228) concerning the product
and the sum of eigenvalues follow easily from
det(A) = det(VCV −1) = det(V ) det(C) det(V −1) = det(C)
and
tr(A) = tr(VCV −1) = tr(V −1VC) = tr(C).
One important fact is that the number of nonzero eigenvalues of a diago-
nalizable matrix A is equal to the rank of A. This must be the case because
the rank of the diagonal matrix C is its number of nonzero elements and the
rank of A must be the same as the rank of C. Another way of saying this is
that the sum of the multiplicities of the unique nonzero eigenvalues is equal
to the rank of the matrix; that is, k
i=1 mi = rank(A), for the matrix A with
k distinct eigenvalues with multiplicities mi.
3.8.9.1 Matrix Functions
We can use the diagonal factorization (3.247) of the matrix A = VCV −1 to
deﬁne a function of the matrix that corresponds to a scalar-valued function
of a scalar, f(x),
f(A) = V diag((f(c1), . . . , f(cn)))V −1,
(3.250)
if f(·) is deﬁned for each eigenvalue ci. (Notice the relationship of this deﬁni-
tion to the Cayley-Hamilton theorem, page 138, and to Exercise 3.26.)
Another useful feature of the diagonal factorization of the matrix A in
equation (3.247) is that it allows us to study functions of powers of A because
Ak = VCkV −1. In particular, we may assess the convergence of a function of
a power of A,
lim
k→∞g(k, A).
Functions deﬁned by elementwise operations have limited applications.
Functions of real numbers that have power series expansions may be deﬁned
for matrices in terms of power series expansions in A, which are eﬀectively
power series in the diagonal elements of C. For example, using the power
series expansion of ex = ∞
k=0
xk
k! , we can deﬁne the matrix exponential for
the square matrix A as the matrix

3.8 Eigenanalysis: Canonical Factorizations
153
eA =
∞

k=0
Ak
k! ,
(3.251)
where A0/0! is deﬁned as I. (Recall that we did not deﬁne A0 if A is singular.)
If A is represented as VCV −1, this expansion becomes
eA = V
∞

k=0
Ck
k! V −1
= V diag ((ec1, . . . , ecn)) V −1.
This is called the matrix exponential for the square matrix A.
The expression exp(A) is generally interpreted as exp(A) = (exp(aij)),
while the expression eA is interpreted as in equation (3.251), but often each
expression is used in the opposite way. As mentioned above, the standard exp
function in software systems, when evaluated for a matrix A, yields (exp(aij)).
Both R and Matlab have a function expm for the matrix exponential. (In R,
expm is in the Matrix package.)
3.8.10 Eigenanalysis of Symmetric Matrices
The eigenvalues and eigenvectors of symmetric matrices have some interesting
properties. First of all, as we have already observed, for a real symmetric
matrix, the eigenvalues are all real. We have also seen that symmetric matrices
are diagonalizable; therefore all of the properties of diagonalizable matrices
carry over to symmetric matrices.
3.8.10.1 Orthogonality of Eigenvectors: Orthogonal
Diagonalization
In the case of a symmetric matrix A, any eigenvectors corresponding to dis-
tinct eigenvalues are orthogonal. This is easily seen by assuming that c1 and
c2 are unequal eigenvalues with corresponding eigenvectors v1 and v2. Now
consider vT
1 v2. Multiplying this by c2, we get
c2vT
1 v2 = vT
1 Av2 = vT
2 Av1 = c1vT
2 v1 = c1vT
1 v2.
Because c1 ̸= c2, we have vT
1 v2 = 0.
Now, consider two eigenvalues ci = cj, that is, an eigenvalue of multiplicity
greater than 1 and distinct associated eigenvectors vi and vj. By what we
just saw, an eigenvector associated with ck ̸= ci is orthogonal to the space
spanned by vi and vj. Assume vi is normalized and apply a Gram-Schmidt
transformation to form
˜vj =
1
∥vj −⟨vi, vj⟩vi∥(vj −⟨vi, vj⟩vi),

154
3 Basic Properties of Matrices
as in equation (2.56) on page 38, yielding a vector orthogonal to vi. Now, we
have
A˜vj =
1
∥vj −⟨vi, vj⟩vi∥(Avj −⟨vi, vj⟩Avi)
=
1
∥vj −⟨vi, vj⟩vi∥(cjvj −⟨vi, vj⟩civi)
= cj
1
∥vj −⟨vi, vj⟩vi∥(vj −⟨vi, vj⟩vi)
= cj˜vj;
hence, ˜vj is an eigenvector of A associated with cj. We conclude therefore that
the eigenvectors of a symmetric matrix can be chosen to be orthogonal.
A symmetric matrix is orthogonally diagonalizable, because the V in equa-
tion (3.247) can be chosen to be orthogonal, and can be written as
A = VCV T,
(3.252)
where V V T = V TV = I, and so we also have
V TAV = C.
(3.253)
Such a matrix is orthogonally similar to a diagonal matrix formed from its
eigenvalues.
Not only is a symmetric matrix orthogonally diagonalizable, any matrix
that is orthogonally diagonalizable is symmetric. This is easy to see. Suppose
B = VCV T, where V is orthogonal and C is diagonal. Then
BT = (VCV T)T = V CV T = B;
(3.254)
hence, B is symmetric.
3.8.10.2 Spectral Decomposition
When A is symmetric and the eigenvectors vi are chosen to be orthonormal,
I =

i
vivT
i ,
(3.255)
so
A = A

i
vivT
i
=

i
AvivT
i
=

i
civivT
i .
(3.256)

3.8 Eigenanalysis: Canonical Factorizations
155
This representation is called the spectral decomposition of the symmetric ma-
trix A. It is essentially the same as equation (3.252), so A = VCV T is also
called the spectral decomposition.
The representation is unique except for the ordering and the choice of
eigenvectors for eigenvalues with multiplicities greater than 1. If the rank of
the matrix is r, we have |c1| ≥· · · ≥|cr| > 0, and if r < n, then cr+1 = · · · =
cn = 0.
Note that the matrices in the spectral decomposition are projection matri-
ces that are orthogonal to each other (but they are not orthogonal matrices)
and they sum to the identity. Let
Pi = vivT
i .
(3.257)
Then we have
PiPi = Pi,
(3.258)
PiPj = 0 for i ̸= j,
(3.259)

i
Pi = I,
(3.260)
and the spectral decomposition,
A =

i
ciPi.
(3.261)
The Pi are called spectral projectors.
The spectral decomposition also applies to powers of A,
Ak =

i
ck
i vivT
i ,
(3.262)
where k is an integer. If A is nonsingular, k can be negative in the expression
above.
The spectral decomposition is one of the most important tools in working
with symmetric matrices.
Although we will not prove it here, all diagonalizable matrices have a spec-
tral decomposition in the form of equation (3.261) with projection matrices
that satisfy properties (3.258) through (3.260). These projection matrices can-
not necessarily be expressed as outer products of eigenvectors, however. The
eigenvalues and eigenvectors of a nonsymmetric matrix might not be real, the
left and right eigenvectors might not be the same, and two eigenvectors might
not be mutually orthogonal. In the spectral representation A = 
i ciPi, how-
ever, if cj is a simple eigenvalue with associated left and right eigenvectors yj
and xj, respectively, then the projection matrix Pj is xjyH
j /yH
j xj. (Note that
because the eigenvectors may not be real, we take the conjugate transpose.)
This is Exercise 3.30.

156
3 Basic Properties of Matrices
3.8.10.3 Kronecker Products of Symmetric Matrices: Orthogonal
Diagonalization
If A and B are symmetric, then A ⊗B is symmetric. (We have already men-
tioned this fact, but it is easy to see using equation (3.100) on page 96.)
Now if A and B are symmetric, we can orthogonally diagonalize them as
in equation (3.252): A = VCV T and B = UDU T. This immediately yields an
orthogonal diagonalization of the symmetric matrix A ⊗B:
A ⊗B = VCV T ⊗UDU T
= (V ⊗U)(C ⊗D)(V T ⊗U T),
(3.263)
which we obtain by using equation (3.101) twice. Using equation (3.101) again,
we have (V ⊗U)(V T ⊗U T) = (V V T ⊗UU T) = I and since C ⊗D is obvi-
ously diagonal, equation (3.263) is in the orthogonally diagonalized form of
equation (3.252).
3.8.10.4 Quadratic Forms and the Rayleigh Quotient
Equation (3.256) yields important facts about quadratic forms in A. Because
V is of full rank, an arbitrary vector x can be written as V b for some vector
b. Therefore, for the quadratic form xTAx we have
xTAx = xT 
i
civivT
i x
=

i
bTV TvivT
i V bci
=

i
b2
i ci.
This immediately gives the inequality
xTAx ≤max{ci}bTb.
(Notice that max{ci} here is not necessarily c1; in the important case when
all of the eigenvalues are nonnegative, it is, however.) Furthermore, if x ̸= 0,
bTb = xTx, and we have the important inequality
xTAx
xTx ≤max{ci}.
(3.264)
Equality is achieved if x is the eigenvector corresponding to max{ci}, so we
have
max
x̸=0
xTAx
xTx = max{ci}.
(3.265)
If c1 > 0, this is the spectral radius, ρ(A).

3.8 Eigenanalysis: Canonical Factorizations
157
The expression on the left-hand side in (3.264) as a function of x is called
the Rayleigh quotient of the symmetric matrix A and is denoted by RA(x):
RA(x) = xTAx
xTx
= ⟨x, Ax⟩
⟨x, x⟩.
(3.266)
Because if x ̸= 0, xTx > 0, it is clear that the Rayleigh quotient is nonnegative
for all x if and only if A is nonnegative deﬁnite, and it is positive for all x if
and only if A is positive deﬁnite.
3.8.10.5 The Fourier Expansion
The vivT
i matrices in equation (3.256) have the property that ⟨vivT
i , vjvT
j ⟩= 0
for i ̸= j and ⟨vivT
i , vivT
i ⟩= 1, and so the spectral decomposition is a Fourier
expansion as in equation (3.113) and the eigenvalues are Fourier coeﬃcients.
From equation (3.114), we see that the eigenvalues can be represented as the
inner product
ci = ⟨A, vivT
i ⟩.
(3.267)
The eigenvalues ci have the same properties as the Fourier coeﬃcients
in any orthonormal expansion. In particular, the best approximating matrices
within the subspace of n×n symmetric matrices spanned by {v1vT
1 , . . . , vnvT
n }
are partial sums of the form of equation (3.256). In Sect. 3.10, however, we
will develop a stronger result for approximation of matrices that does not rely
on the restriction to this subspace and which applies to general, nonsquare
matrices.
3.8.10.6 Powers of a Symmetric Matrix
If (c, v) is an eigenpair of the symmetric matrix A with vTv = 1, then for any
k = 1, 2, . . .,

A −cvvTk = Ak −ckvvT.
(3.268)
This follows from induction on k, for it clearly is true for k = 1, and if for a
given k it is true that for k −1

A −cvvTk−1 = Ak−1 −ck−1vvT,
then by multiplying both sides by (A −cvvT), we see it is true for k:

A −cvvTk =

Ak−1 −ck−1vvT
(A −cvvT)
= Ak −ck−1vvTA −cAk−1vvT + ckvvT
= Ak −ckvvT −ckvvT + ckvvT

158
3 Basic Properties of Matrices
= Ak −ckvvT.
There is a similar result for nonsymmetric square matrices, where w and
v are left and right eigenvectors, respectively, associated with the same eigen-
value c that can be scaled so that wTv = 1. (Recall that an eigenvalue of A
is also an eigenvalue of AT, and if w is a left eigenvector associated with the
eigenvalue c, then ATw = cw.) The only property of symmetry used above
was that we could scale vTv to be 1; hence, we just need wTv ̸= 0. This is
clearly true for a diagonalizable matrix (from the deﬁnition). It is also true
if c is simple (which is somewhat harder to prove). It is thus true for the
dominant eigenvalue, which is simple, in two important classes of matrices
we will consider in Sects. 8.7.2 and 8.7.3, positive matrices and irreducible
nonnegative matrices.
If w and v are left and right eigenvectors of A associated with the same
eigenvalue c and wTv = 1, then for k = 1, 2, . . .,

A −cvwTk = Ak −ckvwT.
(3.269)
We can prove this by induction as above.
3.8.10.7 The Trace and Sums of Eigenvalues
For a general n × n matrix A with eigenvalues c1, . . . , cn, we have tr(A) =
n
i=1 ci. (This is equation (3.228).) This is particularly easy to see for sym-
metric matrices because of equation (3.252), rewritten as V TAV = C, the
diagonal matrix of the eigenvalues. For a symmetric matrix, however, we have
a stronger result.
If A is an n × n symmetric matrix with eigenvalues c1 ≥· · · ≥cn, and U
is an n × k orthogonal matrix, with k ≤n, then
tr(U TAU) ≤
k

i=1
ci.
(3.270)
To see this, we represent U in terms of the columns of V , which span IRn, as
U = V X. Hence,
tr(U TAU) = tr(XTV TAV X)
= tr(XTCX)
=
n

i=1
xT
i xi ci,
(3.271)
where xT
i is the ith row of X.
Now XTX = XTV TV X = U TU = Ik, so either xT
i xi = 0 or xT
i xi = 1,
and n
i=1 xT
i xi = k. Because c1 ≥· · · ≥cn, therefore n
i=1 xT
i xi ci ≤k
i=1 ci,
and so from equation (3.271) we have tr(U TAU) ≤k
i=1 ci.

3.8 Eigenanalysis: Canonical Factorizations
159
3.8.11 Positive Deﬁnite and Nonnegative Deﬁnite Matrices
The factorization of symmetric matrices in equation (3.252) yields some useful
properties of positive deﬁnite and nonnegative deﬁnite matrices (introduced
on page 91). We will brieﬂy discuss these properties here and then return to
the subject in Sect. 8.3 and discuss more properties of positive deﬁnite and
nonnegative deﬁnite matrices.
3.8.11.1 Eigenvalues of Positive and Nonnegative Deﬁnite Matrices
In this book, we use the terms “nonnegative deﬁnite” and “positive deﬁnite”
only for real symmetric matrices, so the eigenvalues of nonnegative deﬁnite or
positive deﬁnite matrices are real.
Any real symmetric matrix is positive (nonnegative) deﬁnite if and only
if all of its eigenvalues are positive (nonnegative). We can see this using the
factorization (3.252) of a symmetric matrix. One factor is the diagonal matrix
C of the eigenvalues, and the other factors are orthogonal. Hence, for any x,
we have xTAx = xTVCV Tx = yTCy, where y = V Tx, and so
xTAx > (≥) 0
if and only if
yTCy > (≥) 0.
This, together with the resulting inequality (3.161) on page 114, implies
that if P is a nonsingular matrix and D is a diagonal matrix, P TDP is positive
(nonnegative) if and only if the elements of D are positive (nonnegative).
A matrix (whether symmetric or not and whether real or not) all of whose
eigenvalues have positive real parts is said to be positive stable. Positive stabil-
ity is an important property in some applications, such as numerical solution
of systems of nonlinear diﬀerential equations. Clearly, a positive deﬁnite ma-
trix is positive stable.
3.8.11.2 Inverse of Positive Deﬁnite Matrices
If A is positive deﬁnite and A = VCV T as in equation (3.252), then
A−1 = VC−1V T, and A−1 is positive deﬁnite because the elements of C−1
are positive.
3.8.11.3 Diagonalization of Positive Deﬁnite Matrices
If A is positive deﬁnite, the elements of the diagonal matrix C in equa-
tion (3.252) are positive, and so their square roots can be absorbed into V
to form a nonsingular matrix P. The diagonalization in equation (3.253),
V TAV = C, can therefore be reexpressed as
P TAP = I.
(3.272)

160
3 Basic Properties of Matrices
3.8.11.4 Square Roots of Positive and Nonnegative Deﬁnite
Matrices
The factorization (3.252) together with the nonnegativity of the eigenvalues
of positive and nonnegative deﬁnite matrices allows us to deﬁne a square root
of such a matrix.
Let A be a nonnegative deﬁnite matrix and let V and C be as in equa-
tion (3.252): A = VCV T. Now, let S be a diagonal matrix whose elements
are the nonnegative square roots of the corresponding elements of C. Then
(VSV T)2 = A; hence, we write
A
1
2 = VSV T
(3.273)
and call this matrix the square root of A. This deﬁnition of the square root
of a matrix is an instance of equation (3.250) with f(x) = √x. We also can
similarly deﬁne A
1
r for r > 0.
We see immediately that A
1
2 is symmetric because A is symmetric.
Notice that if A = I2 (the identity) and if J =
!
0 1
1 0
"
, then J2 = A, but
by the deﬁnition, J is not a square root of A.
If A is positive deﬁnite, A−1 exists and is positive deﬁnite. It therefore has
a square root, which we denote as A−1
2 .
The square roots are nonnegative, and so A
1
2 is nonnegative deﬁnite. Fur-
thermore, A
1
2 and A−1
2 are positive deﬁnite if A is positive deﬁnite.
In Sect. 5.9.1, we will show that this A
1
2 is unique, so our reference to it as
the square root is appropriate. (There is occasionally some ambiguity in the
terms “square root” and “second root” and the symbols used to denote them.
If x is a nonnegative scalar, the usual meaning of its square root, denoted by
√x, is a nonnegative number, while its second roots, which may be denoted by
x
1
2 , are usually considered to be either of the numbers ±√x. In our notation
A
1
2 , we mean the square root; that is, the nonnegative matrix, if it exists.
Otherwise, we say the square root of the matrix does not exist.)
3.8.12 Generalized Eigenvalues and Eigenvectors
The characterization of an eigenvalue as a root of the determinant equa-
tion (3.220) can be extended to deﬁne a generalized eigenvalue of the square
matrices A and B to be a root in c of the equation
det(A −cB) = 0
(3.274)
if a root exists.
Equation (3.274) is equivalent to A −cB being singular; that is, for some
c and some nonzero, ﬁnite v,
Av = cBv.
(3.275)

3.8 Eigenanalysis: Canonical Factorizations
161
Such a c (if it exists) is called a generalized eigenvalue of A and B, and such
a v (if it exists) is called a generalized eigenvector of A and B. In contrast
to the existence of eigenvalues of any square matrix with ﬁnite elements, the
generalized eigenvalues of two matrices may not exist; that is, they may be
inﬁnite. Notice that if (and only if) c is nonzero and ﬁnite, the roles of A and
B can be interchanged in equation (3.275), and the generalized eigenvalue of
B and A is 1/c.
If A and B are n × n (that is, square) and B is nonsingular, then all
n generalized eigenvalues of A and B exist (and are ﬁnite). These general-
ized eigenvalues are the eigenvalues of AB−1 or B−1A. We see this because
det(B) ̸= 0, and so if c0 is any of the n (ﬁnite) eigenvalues of AB−1 or B−1A,
then 0 = det(AB−1 −c0I) = det(B−1A −c0I) = det(A −c0B) = 0. Likewise,
we see that any eigenvector of AB−1 or B−1A is a generalized eigenvector of
A and B.
In the case of ordinary eigenvalues, we have seen that symmetry of the
matrix induces some simpliﬁcations. In the case of generalized eigenvalues,
symmetry together with positive deﬁniteness also yields some useful proper-
ties, which we will discuss in Sect. 7.6.
Generalized eigenvalue problems often arise in multivariate statistical ap-
plications. Roy’s maximum root statistic, for example, is the largest general-
ized eigenvalue of two matrices that result from operations on a partitioned
matrix of sums of squares.
3.8.12.1 Matrix Pencils
As c ranges over the reals (or, more generally, the complex numbers), the set
of matrices of the form A −cB is called the matrix pencil, or just the pencil,
generated by A and B, denoted as
(A, B).
(In this deﬁnition, A and B do not need to be square.) A generalized eigenvalue
of the square matrices A and B is called an eigenvalue of the pencil.
A pencil is said to be regular if det(A −cB) is not identically 0 (assuming,
of course, that det(A −cB) is deﬁned, meaning A and B are square). An
interesting special case of a regular pencil is when B is nonsingular. As we
have seen, in that case, eigenvalues of the pencil (A, B) exist (and are ﬁnite)
and are the same as the ordinary eigenvalues of AB−1 or B−1A, and the
ordinary eigenvectors of AB−1 or B−1A are eigenvectors of the pencil (A, B).
3.8.13 Singular Values and the Singular Value Decomposition
(SVD)
An n × m matrix A can be factored as
A = UDV T,
(3.276)

162
3 Basic Properties of Matrices
where U is an n×n orthogonal matrix, V is an m×m orthogonal matrix, and
D is an n × m diagonal matrix with nonnegative entries. (An n × m diagonal
matrix has min(n, m) elements on the diagonal, and all other entries are zero.)
The factorization (3.276) is called the singular value decomposition (SVD)
or the canonical singular value factorization of A. The elements on the diag-
onal of D, di, are called the singular values of A.
The SVD, which is unique, as we establish below, is one of the most impor-
tant and most useful decompositions in all of matrix theory and applications.
There are min(n, m) singular values. We can rearrange the entries in D so
that d1 ≥· · · ≥dmin(n,m), and by rearranging the columns of U correspond-
ingly, nothing is changed.
We see that the SVD exists for any matrix by forming a square symmetric
matrix and then using the decomposition in equation (3.252) on page 154. Let
A be an n × m matrix A, and form ATA = V CV T. If n ≥m, we have
ATA = V CV T
= V [I 0]
!
C
0
"
V T
=
!V 0
0 I
" !C
0
"
V T
= UDV T,
as above. Note if n = m, the 0 partitions in the matrices are nonexistent. If,
on the other hand, n < m, we form D = [C 0] and proceed as before.
This same development follows for AAT, and it is clear that the nonzero
elements of the corresponding “C” matrix are the same (or property 14 on
page 140 ensures that they are the same.)
The number of positive entries in D is the same as the rank of A. (We see
this by ﬁrst recognizing that the number of nonzero entries of D is obviously
the rank of D, and multiplication by the full rank matrices U and V T yields
a product with the same rank from equations (3.158) and (3.159).)
From this development, we see that the squares of the singular values of A
are the eigenvalues of ATA and of AAT, which are necessarily nonnegative. To
state this more clearly (and using some additional facts developed previously,
including property 13 on page 140), let A be an n × m matrix with rank r,
and let d be a singular value of A. We have
•
c = d2 is an eigenvalue of ATA;
•
c = d2 is an eigenvalue of AAT;
•
if c is a nonzero eigenvalue of ATA, then there is a singular value d of A
such that d2 = c; and
•
there are r nonzero singular values of A, and r nonzero eigenvalues of ATA
and of AAT.
These relationships between singular values and eigenvalues are some of
the most important properties of singular values and the singular value de-

3.8 Eigenanalysis: Canonical Factorizations
163
composition. In particular, from these we can see that the singular value
decomposition is unique (with the same qualiﬁcations attendant to unique-
ness of eigenvalues and eigenvectors, relating to ordering of the elements and
selection of vectors corresponding to nonunique values).
An additional observation is that if A is symmetric, the singular values of
A are the absolute values of the eigenvalues of A.
From the factorization (3.276) deﬁning the singular values, we see that
•
the singular values of AT are the same as those of A.
As pointed out above, for a matrix with more rows than columns, in an
alternate deﬁnition of the singular value decomposition, the matrix U is n×m
with orthogonal columns, and D is an m×m diagonal matrix with nonnegative
entries. Likewise, for a matrix with more columns than rows, the singular value
decomposition can be deﬁned as above but with the matrix V being m × n
with orthogonal columns and D being n × n and diagonal with nonnegative
entries.
We often partition D to separate the zero singular values. If the rank of
the matrix is r, we have d1 ≥· · · ≥dr > 0 (with the common indexing), and
if r < min(n, m), then dr+1 = · · · = dmin(n,m) = 0. In this case
D =
! Dr 0
0 0
"
,
where Dr = diag((d1, . . . , dr)).
3.8.13.1 The Fourier Expansion in Terms of the Singular Value
Decomposition
From equation (3.276), we see that the general matrix A with rank r also
has a Fourier expansion, similar to equation (3.256), in terms of the singular
values and outer products of the columns of the U and V matrices:
A =
r

i=1
diuivT
i .
(3.277)
This is also called a spectral decomposition. The uivT
i
matrices in equa-
tion (3.277) have the property that ⟨uivT
i , ujvT
j ⟩= 0 for i ̸= j and
⟨uivT
i , uivT
i ⟩= 1, and so the spectral decomposition is a Fourier expan-
sion as in equation (3.113), and the singular values are Fourier coeﬃcients.
The singular values di have the same properties as the Fourier coeﬃcients
in any orthonormal expansion. For example, from equation (3.114), we see
that the singular values can be represented as the inner product
di = ⟨A, uivT
i ⟩.

164
3 Basic Properties of Matrices
After we have discussed matrix norms in the next section, we will formulate
Parseval’s identity for this Fourier expansion.
The spectral decomposition is a rank-one decomposition, since each of the
matrices uivT
i has rank one.
3.9 Matrix Norms
Norms on matrices are scalar functions of matrices with the three properties
on page 25 that deﬁne a norm in general. Matrix norms are often required
to have another property, called the consistency property, in addition to the
properties listed on page 25, which we repeat here for convenience. Assume A
and B are matrices conformable for the operations shown.
1. Nonnegativity and mapping of the identity:
if A ̸= 0, then ∥A∥> 0, and ∥0∥= 0.
2. Relation of scalar multiplication to real multiplication:
∥aA∥= |a| ∥A∥for real a.
3. Triangle inequality:
∥A + B∥≤∥A∥+ ∥B∥.
4. Consistency property:
∥AB∥≤∥A∥∥B∥.
Some people do not require the consistency property for a matrix norm. Most
useful matrix norms have the property, however, and we will consider it to be
a requirement in the deﬁnition. The consistency property for multiplication is
similar to the triangular inequality for addition.
Any function from IRn×m to IR that satisﬁes these four properties is a
matrix norm.
A matrix norm, as any norm, is necessarily convex. (See page 26.)
We note that the four properties of a matrix norm do not imply that it
is invariant to transposition of a matrix, and in general, ∥AT∦= ∥A∥. Some
matrix norms are the same for the transpose of a matrix as for the original
matrix. For instance, because of the property of the matrix inner product
given in equation (3.110), we see that a norm deﬁned by that inner product
would be invariant to transposition.
For a square matrix A, the consistency property for a matrix norm yields
∥Ak∥≤∥A∥k
(3.278)
for any positive integer k.
A matrix norm ∥·∥is orthogonally invariant if A and B being orthogonally
similar implies ∥A∥= ∥B∥.

3.9 Matrix Norms
165
3.9.1 Matrix Norms Induced from Vector Norms
Some matrix norms are deﬁned in terms of vector norms. For clarity, we will
denote a vector norm as ∥· ∥v and a matrix norm as ∥· ∥M. (This notation is
meant to be generic; that is, ∥·∥v represents any vector norm.) For the matrix
A ∈IRn×m, the matrix norm ∥· ∥M induced by ∥· ∥v is deﬁned by
∥A∥M = max
x̸=0
∥Ax∥v
∥x∥v
.
(3.279)
(Note that there are some minor subtleties here; Ax ∈IRn while x ∈IRm, so
the two vector norms are actually diﬀerent. Of course, in practice, an induced
norm is deﬁned in terms of vector norms of the same “type”, for example Lp
norms with the same p.)
An induced matrix norm is also sometimes called an operator norm.
It is easy to see that an induced norm is indeed a matrix norm. The ﬁrst
three properties of a norm are immediate, and the consistency property can
be veriﬁed by applying the deﬁnition (3.279) to AB and replacing Bx with y;
that is, using Ay.
We usually drop the v or M subscript, and the notation ∥· ∥is overloaded
to mean either a vector or matrix norm. (Overloading of symbols occurs in
many contexts, and we usually do not even recognize that the meaning is
context-dependent. In computer language design, overloading must be recog-
nized explicitly because the language speciﬁcations must be explicit.)
The induced norm of A given in equation (3.279) is sometimes called the
maximum magniﬁcation by A. The expression looks very similar to the max-
imum eigenvalue, and indeed it is in some cases.
For any vector norm and its induced matrix norm, we see from equa-
tion (3.279) that
∥Ax∥≤∥A∥∥x∥
(3.280)
because ∥x∥≥0.
3.9.1.1 Lp Matrix Norms
The matrix norms that correspond to the Lp vector norms are deﬁned for the
n × m matrix A as
∥A∥p = max
∥x∥p=1 ∥Ax∥p.
(3.281)
(Notice that the restriction on ∥x∥p makes this an induced norm as deﬁned
in equation (3.279). Notice also the overloading of the symbols; the norm on
the left that is being deﬁned is a matrix norm, whereas those on the right of
the equation are vector norms.) It is clear that the Lp matrix norms satisfy
the consistency property, because they are induced norms.
The L1 and L∞norms have interesting simpliﬁcations of equation (3.279):

166
3 Basic Properties of Matrices
∥A∥1 = max
j

i
|aij|,
(3.282)
so the L1 is also called the column-sum norm; and
∥A∥∞= max
i

j
|aij|,
(3.283)
so the L∞is also called the row-sum norm. We see these relationships by
considering the Lp norm of the vector
v = (aT
1∗x, . . . , aT
n∗x),
where ai∗is the ith row of A, with the restriction that ∥x∥p = 1. The Lp
norm of this vector is based on the absolute values of the elements; that is,
| 
j aijxj| for i = 1, . . . , n. Because we are free to choose x (subject to the
restriction that ∥x∥p = 1), for a given i, we can choose the sign of each xj to
maximize the overall expression. For example, for a ﬁxed i, we can choose each
xj to have the same sign as aij, and so | 
j aijxj| is the same as 
j |aij| |xj|.
For the column-sum norm, the L1 norm of v is 
i |aT
i∗x|. The elements
of x are chosen to maximize this under the restriction that  |xj| = 1. The
maximum of the expression is attained by setting xk = sign(
i aik), where k
is such that |
i aik| ≥|
i aij|, for j = 1, . . . , m, and xq = 0 for q = 1, . . . m
and q ̸= k. (If there is no unique k, any choice will yield the same result.)
This yields equation (3.282).
For the row-sum norm, the L∞norm of v is
max
i
|aT
i∗x| = max
i

j
|aij| |xj|
when the sign of xj is chosen appropriately (for a given i). The elements of
x must be chosen so that max |xj| = 1; hence, each xj is chosen as ±1. The
maximum |aT
i∗x| is attained by setting xj = sign(akj), for j = 1, . . . m, where k
is such that 
j |akj| ≥
j |aij|, for i = 1, . . . , n. This yields equation (3.283).
From equations (3.282) and (3.283), we see that
∥AT∥∞= ∥A∥1.
(3.284)
Alternative formulations of the L2 norm of a matrix are not so obvious
from equation (3.281). It is related to the eigenvalues (or the singular values)
of the matrix. The L2 matrix norm is related to the spectral radius (page 142):
∥A∥2 =
&
ρ(ATA),
(3.285)
(see Exercise 3.34, page 182). Because of this relationship, the L2 matrix norm
is also called the spectral norm.

3.9 Matrix Norms
167
From the invariance of the singular values to matrix transposition, we
see that positive eigenvalues of ATA are the same as those of AAT; hence,
∥AT∥2 = ∥A∥2.
For Q orthogonal, the L2 vector norm has the important property
∥Qx∥2 = ∥x∥2
(3.286)
(see Exercise 3.35a, page 182). For this reason, an orthogonal matrix is some-
times called an isometric matrix. By the proper choice of x, it is easy to see
from equation (3.286) that
∥Q∥2 = 1.
(3.287)
Also from this we see that if A and B are orthogonally similar, then ∥A∥2 =
∥B∥2; hence, the spectral matrix norm is orthogonally invariant.
The L2 matrix norm is a Euclidean-type norm since it is induced by the
Euclidean vector norm (but it is not called the Euclidean matrix norm; see
below).
3.9.1.2 L1, L2, and L∞Norms of Symmetric Matrices
For a symmetric matrix A, we have the obvious relationships
∥A∥1 = ∥A∥∞
(3.288)
and, from equation (3.285),
∥A∥2 = ρ(A).
(3.289)
3.9.2 The Frobenius Norm—The “Usual” Norm
The Frobenius norm is deﬁned as
∥A∥F =
%
i,j
a2
ij.
(3.290)
It is easy to see that this measure has the consistency property (Exercise 3.37),
as a norm must. The Frobenius norm is sometimes called the Euclidean matrix
norm and denoted by ∥· ∥E, although the L2 matrix norm is more directly
based on the Euclidean vector norm, as we mentioned above. We will usually
use the notation ∥· ∥F to denote the Frobenius norm. Occasionally we use
∥· ∥without the subscript to denote the Frobenius norm, but usually the
symbol without the subscript indicates that any norm could be used in the
expression. The Frobenius norm is also often called the “usual norm”, which
emphasizes the fact that it is one of the most useful matrix norms. Other
names sometimes used to refer to the Frobenius norm are Hilbert-Schmidt
norm and Schur norm.

168
3 Basic Properties of Matrices
From the deﬁnition, we have ∥AT∥F = ∥A∥F. We have seen that the L2
matrix norm also has this property.
Another important property of the Frobenius norm that is obvious from
the deﬁnition is
∥A∥F =
&
tr(ATA)
(3.291)
=

⟨A, A⟩;
(3.292)
that is,
•
the Frobenius norm is the norm that arises from the matrix inner product
(see page 97).
The complete vector space IRn×m with the Frobenius norm is therefore a
Hilbert space.
Another thing worth noting for a square A is the relationship of the Frobe-
nius norm to the eigenvalues ci of A:
∥A∥F =
&
ci¯ci,
(3.293)
and if A is also symmetric,
∥A∥F =
&
c2
i ,
(3.294)
These follow from equation (3.291) and equation (3.228) on page 141.
Similar to deﬁning the angle between two vectors in terms of the inner
product and the norm arising from the inner product, we deﬁne the angle
between two matrices A and B of the same size and shape as
angle(A, B) = cos−1

⟨A, B⟩
∥A∥F∥B∥F

.
(3.295)
If Q is an n × m orthogonal matrix, then
∥Q∥F = √m
(3.296)
(see equation (3.216)).
If A and B are orthogonally similar (see equation (3.242)), then
∥A∥F = ∥B∥F;
that is, the Frobenius norm is an orthogonally invariant norm. To see this, let
A = QTBQ, where Q is an orthogonal matrix. Then
∥A∥2
F = tr(ATA)
= tr(QTBTQQTBQ)

3.9 Matrix Norms
169
= tr(BTBQQT)
= tr(BTB)
= ∥B∥2
F.
(The norms are nonnegative, of course, and so equality of the squares is suf-
ﬁcient.)
3.9.2.1 The Frobenius Norm and the Singular Values
Several important properties result because the Frobenius norm arises from
an inner product. For example, following the Fourier expansion in terms of
the singular value decomposition, equation (3.277), we mentioned that the
singular values have the general properties of Fourier coeﬃcients; for example,
they satisfy Parseval’s identity, equation (2.60), on page 41. This identity
states that the sum of the squares of the Fourier coeﬃcients is equal to the
square of the norm that arises from the inner product used in the Fourier
expansion. Hence, we have the important property of the Frobenius norm
that it is the L2 norm of the vector of singular values of the matrix. For the
n × m matrix A, let d be the min(n, m)-vector of singular values of A. Then
∥A∥2
F = ∥d∥2.
(3.297)
Compare equations (3.293) and (3.294) for square matrices.
3.9.3 Other Matrix Norms
There are two diﬀerent ways of generalizing the Frobenius norm. One is a
simple generalization of the deﬁnition in equation (3.290). For p ≥1, it is the
Frobenius p norm:
∥A∥Fp =
⎛
⎝
i,j
|aij|p
⎞
⎠
1/p
.
(3.298)
Some people refer to this as the Lp norm of the matrix. As we have seen, the
Lp matrix norm is diﬀerent, but there is a simple relationship of the Frobenius
p matrix norm to the Lp vector norm:
∥A∥Fp = ∥vec(A)∥p.
(3.299)
This relationship of the matrix norm to a vector norm sometimes makes com-
putational problems easier.
The Frobenius 2 norm is the ordinary Frobenius norm.
Another generalization of the Frobenius norm arises from its relation to
the singular values given in equation (3.297). For p ≥1, it is the Schatten p
norm:

170
3 Basic Properties of Matrices
∥A∥Sp = ∥d∥p,
(3.300)
where d is the vector of singular values of A.
The Schatten 2 norm is the ordinary Frobenius norm.
The Schatten 1 norm is called the nuclear norm (because of its relation-
ship to “nuclear operators”, which are linear operators that preserve local
convexity). It is also sometimes called the trace norm, because
∥d∥1 = tr

(ATA)1/2
.
(3.301)
The Schatten ∞norm is the spectral norm.
3.9.4 Matrix Norm Inequalities
There is an equivalence among any two matrix norms similar to that of expres-
sion (2.39) for vector norms (over ﬁnite-dimensional vector spaces). If ∥· ∥a
and ∥· ∥b are matrix norms, then there are positive numbers r and s such
that, for any matrix A,
r∥A∥b ≤∥A∥a ≤s∥A∥b.
(3.302)
We will not prove this result in general but, in Exercise 3.39, ask the reader
to do so for matrix norms induced by vector norms. These induced norms
include the matrix Lp norms of course.
If A is an n × m real matrix, we have some speciﬁc instances of (3.302):
∥A∥∞≤√m ∥A∥F,
(3.303)
∥A∥F ≤

min(n, m) ∥A∥2,
(3.304)
∥A∥2 ≤√m ∥A∥1,
(3.305)
∥A∥1 ≤√n ∥A∥2,
(3.306)
∥A∥2 ≤∥A∥F,
(3.307)
∥A∥F ≤√n ∥A∥∞.
(3.308)
See Exercise 3.40 on page 182.
Compare these inequalities with those for Lp vector norms on page 28.
Recall speciﬁcally that for vector Lp norms we had the useful fact that for a
given x and for p ≥1, ∥x∥p is a nonincreasing function of p; and speciﬁcally
we had inequality (2.34):

3.9 Matrix Norms
171
∥x∥∞≤∥x∥2 ≤∥x∥1.
There is a related inequality involving matrices:
∥A∥2
2 ≤∥A∥1∥A∥∞.
(3.309)
3.9.5 The Spectral Radius
The spectral radius is the appropriate measure of the condition of a square
matrix for certain iterative algorithms. Except in the case of symmetric ma-
trices, as shown in equation (3.289), the spectral radius is not a norm (see
Exercise 3.41a).
We have for any norm ∥· ∥and any square matrix A that
ρ(A) ≤∥A∥.
(3.310)
To see this, we consider the associated eigenvalue and eigenvector, ci and vi,
and form the matrix V = [vi|0| · · · |0]. This yields ciV = AV , and by the
consistency property of any matrix norm,
|ci|∥V ∥= ∥ciV ∥
= ∥AV ∥
≤∥A∥∥V ∥,
or
|ci| ≤∥A∥,
(see also Exercise 3.41b).
The inequality (3.310) and the L1 and L∞norms yield useful bounds on
the eigenvalues and the maximum absolute row and column sums of matrices:
the modulus of any eigenvalue is no greater than the largest sum of absolute
values of the elements in any row or column. (These were inequalities (3.235)
and (3.236) on page 142.)
The inequality (3.310) and equation (3.289) also yield a minimum property
of the L2 norm of a symmetric matrix A:
∥A∥2 ≤∥A∥.
3.9.6 Convergence of a Matrix Power Series
We deﬁne the convergence of a sequence of matrices in terms of the conver-
gence of a sequence of their norms, just as we did for a sequence of vectors (on
page 32). We say that a sequence of matrices A1, A2, . . . (of the same shape)
converges to the matrix A with respect to the norm ∥· ∥if the sequence of
real numbers ∥A1 −A∥, ∥A2 −A∥, . . . converges to 0. Because of the equiv-
alence property of norms, the choice of the norm is irrelevant. Also, because
of inequality (3.310), we see that the convergence of the sequence of spectral
radii ρ(A1 −A), ρ(A2 −A), . . . to 0 must imply the convergence of A1, A2, . . .
to A.

172
3 Basic Properties of Matrices
3.9.6.1 Conditions for Convergence of a Sequence of Powers to 0
For a square matrix A, we have the important fact that
Ak →0,
if ∥A∥< 1,
(3.311)
where 0 is the square zero matrix of the same order as A and ∥·∥is any matrix
norm. (The consistency property is required.) This convergence follows from
inequality (3.278) because that yields limk→∞∥Ak∥≤limk→∞∥A∥k, and so
if ∥A∥< 1, then limk→∞∥Ak∥= 0.
Now consider the spectral radius. Because of the spectral decomposition,
we would expect the spectral radius to be related to the convergence of a
sequence of powers of a matrix. If Ak →0, then for any conformable vector
x, Akx →0; in particular, for the eigenvector v1 ̸= 0 corresponding to the
dominant eigenvalue c1, we have Akv1 = ck
1v1 →0. For ck
1v1 to converge to
zero, we must have |c1| < 1; that is, ρ(A) < 1. We can also show the converse:
Ak →0
if ρ(A) < 1.
(3.312)
We will do this by deﬁning a norm ∥· ∥d in terms of the L1 matrix norm in
such a way that ρ(A) < 1 implies ∥A∥d < 1. Then we can use equation (3.311)
to establish the convergence.
Let A = QT QT be the Schur factorization of the n × n matrix A,
where Q is orthogonal and T is upper triangular with the same eigen-
values as A, c1, . . . , cn. Now for any d > 0, form the diagonal matrix
D = diag((d1, . . . , dn)). Notice that DT D−1 is an upper triangular matrix
and its diagonal elements (which are its eigenvalues) are the same as the
eigenvalues of T and A. Consider the column sums of the absolute values of
the elements of DT D−1:
|cj| +
j−1

i=1
d−(j−i)|tij|.
Now, because |cj| ≤ρ(A) for given ϵ > 0, by choosing d large enough, we have
|cj| +
j−1

i=1
d−(j−i)|tij| < ρ(A) + ϵ,
or
∥DT D−1∥1 = max
j

|cj| +
j−1

i=1
d−(j−i)|tij|

< ρ(A) + ϵ.
Now deﬁne ∥· ∥d for any n × n matrix X, where Q is the orthogonal matrix
in the Schur factorization and D is as deﬁned above, as
∥X∥d = ∥(QD−1)−1X(QD−1)∥1.
(3.313)

3.9 Matrix Norms
173
It is clear that ∥· ∥d is a norm (Exercise 3.42). Furthermore,
∥A∥d = ∥(QD−1)−1A(QD−1)∥1
= ∥DT D−1∥1
< ρ(A) + ϵ,
and so if ρ(A) < 1, ϵ and d can be chosen so that ∥A∥d < 1, and by equa-
tion (3.311) above, we have Ak →0; hence, we conclude that
Ak →0
if and only if ρ(A) < 1.
(3.314)
Informally, we see that Ak goes to 0 more rapidly the smaller is ρ(A).
We will discuss convergence of a sequence of powers of an important special
class of matrices with spectral radii possibly greater than or equal to 1 on
page 378.
3.9.6.2 Another Perspective on the Spectral Radius: Relation to
Norms
From inequality (3.310) and the fact that ρ(Ak) = ρ(A)k, we have
ρ(A) ≤∥Ak∥1/k,
(3.315)
where ∥· ∥is any matrix norm. Now, for any ϵ > 0, ρ

A/(ρ(A) + ϵ)

< 1 and
so
lim
k→∞

A/(ρ(A) + ϵ)
k = 0
from expression (3.314); hence,
lim
k→∞
∥Ak∥
(ρ(A) + ϵ)k = 0.
There is therefore a positive integer Mϵ such that ∥Ak∥/(ρ(A) + ϵ)k < 1 for
all k > Mϵ, and hence ∥Ak∥1/k < (ρ(A) + ϵ) for k > Mϵ. We have therefore,
for any ϵ > 0,
ρ(A) ≤∥Ak∥1/k < ρ(A) + ϵ
for k > Mϵ,
and thus
lim
k→∞∥Ak∥1/k = ρ(A).
(3.316)
Compare this with the inequality (3.310).

174
3 Basic Properties of Matrices
3.9.6.3 Convergence of a Power Series: Inverse of I −A
Consider the power series in an n × n matrix such as in equation (3.186) on
page 120,
I + A + A2 + A3 + · · · .
In the standard fashion for dealing with series, we form the partial sum
Sk = I + A + A2 + A3 + · · · Ak
and consider limk→∞Sk. We ﬁrst note that
(I −A)Sk = I −Ak+1
and observe that if Ak+1 →0, then Sk →(I−A)−1, which is equation (3.186).
Therefore,
(I −A)−1 = I + A + A2 + A3 + · · ·
if ∥A∥< 1.
(3.317)
3.9.6.4 Nilpotent Matrices
As we discussed on page 77, for some nonzero square matrices, Ak = 0 for
a ﬁnite integral value of k. If A2 = 0, such a matrix is a nilpotent matrix
(otherwise, it is nilpotent with an index greater than 2). A matrix such as we
discussed above for which Ak →0, but for any ﬁnite k, Ak ̸= 0, is not called
a nilpotent matrix.
From the deﬁnition, it is clear that the Drazin inverse of any nilpotent
matrix is 0.
We have seen in equation (3.314) that Ak →0 if and only if ρ(A) < 1. The
condition in equation (3.311) on any norm is not necessary, however; that is,
if Ak →0, it may be the case that, for some norm, ∥A∥> 1. In fact, even for
an idempotent matrix (for which Ak = 0 for ﬁnite k), it may be the case that
∥A∥> 1. A simple example is
A =
!
0 2
0 0
"
.
For this matrix, A2 = 0, yet ∥A∥1 = ∥A∥2 = ∥A∥∞= ∥A∥F = 2.
At this point, I list some more properties of nilpotent matrices that involve
concepts we had not introduced when we ﬁrst discussed nilpotent matrices.
It is easy to see that if An×n is nilpotent, then
tr(A) = 0,
(3.318)
det(A) = 0,
(3.319)
ρ(A) = 0,
(3.320)

3.10 Approximation of Matrices
175
(that is, all eigenvalues of A are 0), and
rank(A) ≤n −1.
(3.321)
You are asked to supply the proofs of these statements in Exercise 3.43b.
In applications, for example in time series or other stochastic processes,
because of expression (3.314), the spectral radius is often the most useful.
Stochastic processes may be characterized by whether the absolute value of
the dominant eigenvalue (spectral radius) of a certain matrix is less than 1.
Interesting special cases occur when the dominant eigenvalue is equal to 1.
3.10 Approximation of Matrices
In Sect. 2.2.6, we discussed the problem of approximating a given vector in
terms of vectors from a lower dimensional space. Likewise, it is often of interest
to approximate one matrix by another.
In statistical applications, we may wish to ﬁnd a matrix of smaller rank
that contains a large portion of the information content of a matrix of larger
rank (“dimension reduction” as on page 428; or variable selection as in
Sect. 9.5.2, for example), or we may want to impose conditions on an esti-
mate that it have properties known to be possessed by the estimand (positive
deﬁniteness of the correlation matrix, for example, as in Sect. 9.5.6).
In numerical linear algebra, we may wish to ﬁnd a matrix that is easier to
compute or that has properties that ensure more stable computations.
Finally, we may wish to represent a matrix as a sum or a product of
other matrices with restrictions on those matrices that do not allow an exact
representation. (A nonnegative factorization as discussed in Sect. 5.10.1 is an
example.)
3.10.1 Measures of the Diﬀerence Between Two Matrices
A natural way to assess the goodness of the approximation is by a norm
of the diﬀerence (that is, by a metric induced by a norm), as discussed on
page 32. If A is an approximation to A, we could measure the quality of
the approximation by Δ(A, A) = ∥A −A∥for some norm ∥· ∥. The measure
Δ(A, 
A) is a metric, as deﬁned on page 32, and is a common way of measuring
the “distance” between two matrices.
Other ways of measuring the diﬀerence between two matrices may be based
on how much the entropy of one divergences from that of the other. This may
make sense if all elements in the matrices are positive. The Kullback-Leibler
divergence between distributions is based on this idea. Because one distribu-
tion is used to normalize the other one, the Kullback-Leibler divergence is not
a metric. If all elements of the matrices A and A are positive, the Kullback-
Leibler divergence measure for how much the matrix A diﬀers from A is

176
3 Basic Properties of Matrices
dKL(A −A) =

ij

˜aij log
˜aij
aij

−˜aij + aij

.
(3.322)
The most commonly-used measure of the goodness of an approximation
uses the norm that arises from the inner product (the Frobenius norm).
3.10.2 Best Approximation with a Matrix of Given Rank
Suppose we want the best approximation to an n × m matrix A of rank r by
a matrix A in IRn×m but with smaller rank, say k; that is, we want to ﬁnd A
of rank k such that
∥A −A∥F
(3.323)
is a minimum for all A ∈IRn×m of rank k.
We have an orthogonal basis in terms of the singular value decomposition,
equation (3.277), for some subspace of IRn×m, and we know that the Fourier
coeﬃcients provide the best approximation for any subset of k basis matrices,
as in equation (2.65). This Fourier ﬁt would have rank k as required, but it
would be the best only within that set of expansions. (This is the limitation
imposed in equation (2.65).) Another approach to determine the best ﬁt could
be developed by representing the columns of the approximating matrix as
linear combinations of the given matrix A and then expanding ∥A −A∥2
F.
Neither the Fourier expansion nor the restriction V( A) ⊂V(A) permits us to
address the question of what is the overall best approximation of rank k within
IRn×m. As we see below, however, there is a minimum of expression (3.323)
that occurs within V(A), and a minimum is at the truncated Fourier expansion
in the singular values (equation (3.277)).
To state this more precisely, let A be an n × m matrix of rank r with
singular value decomposition
A = U
!
Dr 0
0 0
"
V T,
where Dr = diag((d1, . . . , dr)), and the singular values are indexed so that
d1 ≥· · · ≥dr > 0. Then, for all n × m matrices X with rank k < r,
∥A −X∥2
F ≥
r

i=k+1
d2
i ,
(3.324)
and this minimum occurs for X = A, where
A = U
!Dk 0
0 0
"
V T.
(3.325)
To see this, for any X, let Q be an n × k matrix whose columns are an
orthonormal basis for V(X), and let X = QY , where Y is a k × m matrix,
also of rank k. The minimization problem now is

3.10 Approximation of Matrices
177
min
Y
∥A −QY ∥F
with the restriction rank(Y ) = k.
Now, expanding, completing the Gramian and using its nonnegative deﬁ-
niteness, and permuting the factors within a trace, we have
∥A −QY ∥2
F = tr

(A −QY )T(A −QY )

= tr

ATA

+ tr

Y TY −ATQY −Y TQTA

= tr

ATA

+ tr

(Y −QTA)T(Y −QTA)

−tr

ATQQTA

≥tr

ATA

−tr

QTAATQ

.
The squares of the singular values of A are the eigenvalues of ATA, and so
tr(ATA) = r
i=1 d2
i . The eigenvalues of ATA are also the eigenvalues of AAT,
and so, from inequality (3.270), tr(QTAATQ) ≤k
i=1 d2
i , and so
∥A −X∥2
F ≥
r

i=1
d2
i −
k

i=1
d2
i ;
hence, we have inequality (3.324). (This technique of “completing the
Gramian” when an orthogonal matrix is present in a sum is somewhat
similar to the technique of completing the square; it results in the diﬀerence
of two Gramian matrices, which are deﬁned in Sect. 3.3.10.)
Direct expansion of ∥A −A∥2
F yields
tr

ATA

−2tr

AT A

+ tr

AT A

=
r

i=1
d2
i −
k

i=1
d2
i ,
and hence A is the best rank k approximation to A under the Frobenius norm.
Equation (3.325) can be stated another way: the best approximation of A
of rank k is
A =
k

i=1
diuivT
i .
(3.326)
This result for the best approximation of a given matrix by one of lower
rank was ﬁrst shown by Eckart and Young (1936).
On page 342, we will
discuss a bound on the diﬀerence between two symmetric matrices whether
of the same or diﬀerent ranks.
In applications, the rank k may be stated a priori or we examine a sequence
k = r −1, r −2, . . ., and determine the norm of the best ﬁt at each rank. If
sk is the norm of the best approximating matrix, the sequence sr−1, sr−2, . . .
may suggest a value of k for which the reduction in rank is suﬃcient for
our purposes and the loss in closeness of the approximation is not too great.
Principal components analysis is a special case of this process (see Sect. 9.4).

178
3 Basic Properties of Matrices
Exercises
3.1. Vector spaces of matrices.
a) Exhibit a basis set for IRn×m for n ≥m.
b) Does the set of n × m diagonal matrices form a vector space? (The
answer is yes.) Exhibit a basis set for this vector space (assuming
n ≥m).
c) Exhibit a basis set for the vector space of n × n symmetric matrices.
(First, of course, we must ask is this a vector space. The answer is
yes.)
d) Show that the cardinality of any basis set for the vector space of
n × n symmetric matrices is n(n + 1)/2.
3.2. By expanding the expression on the left-hand side, derive equation (3.92)
on page 93.
3.3. Show that for any quadratic form xTAx there is a symmetric matrix
As such that xTAsx = xTAx. (The proof is by construction, with As =
1
2(A+AT), ﬁrst showing As is symmetric and then that xTAsx = xTAx.)
3.4. For a, b, c ∈IR, give conditions on a, b, and c for the matrix below to be
positive deﬁnite.
! a b
b c
"
.
3.5. Show that the Mahalanobis distance deﬁned in equation (3.95) is a
metric (that is, show that it satisﬁes the properties listed on page 32).
3.6. Verify the relationships for Kronecker products shown in equa-
tions (3.97) through (3.103) on page 95.
Hint: Make liberal use of equation (3.96) and previously veriﬁed equa-
tions.
3.7. Verify the relationship between the vec function and Kronecker multi-
plication given in equation (3.106), vec(ABC) = (CT ⊗A)vec(B).
Hint: Just determine an expression for the ith term in the vector on
either side of the equation.
3.8. Cauchy-Schwarz inequalities for matrices.
a) Prove the Cauchy-Schwarz inequality for the dot product of matrices
((3.111), page 98), which can also be written as
(tr(ATB))2 ≤tr(ATA)tr(BTB).
b) Prove the Cauchy-Schwarz inequality for determinants of matrices
A and B of the same shape:
det(ATB)2 ≤det(ATA)det(BTB).
Under what conditions is equality achieved?
c) Let A and B be matrices of the same shape, and deﬁne
p(A, B) = det(ATB).
Is p(·, ·) an inner product? Why or why not?

Exercises
179
3.9. Prove that a square matrix that is either row or column (strictly) diag-
onally dominant is nonsingular.
3.10. Prove that a positive deﬁnite matrix is nonsingular.
3.11. Let A be an n × m matrix.
a) Under what conditions does A have a Hadamard multiplicative in-
verse?
b) If A has a Hadamard multiplicative inverse, what is it?
3.12. Bounds on ranks.
a) Show that the bound in inequality (3.128) is sharp by ﬁnding two
matrices A and B such that rank(AB) = min(rank(A), rank(B)).
b) Show that the bound in inequality (3.170) is sharp by ﬁnding an
n × n matrix A and a matrix B with n rows such that rank(AB) =
rank(A) + rank(B) −n.
c) Show that the bound in inequality (3.129) is sharp by ﬁnding two
matrices A and B such that rank(A + B) = rank(A) + rank(B).
d) Show that the bound in inequality (3.130) is sharp by ﬁnding two
matrices A and B such that rank(A + B) = |rank(A) −rank(B)|.
3.13. The aﬃne group AL(n).
a) What is the identity in AL(n)?
b) Let (A, v) be an element of AL(n). What is the inverse of (A, v)?
3.14. Let A be an n × m matrix of rank one. Show that A can be written as
an outer product
A = xyT,
where x is some n-vector and y is some m-vector.
3.15. In computational explorations involving matrices, it is often convenient
to work with matrices whose elements are integers. If an inverse is in-
volved, it would be nice to know that the elements of the inverse are also
integers. Equation (3.172) on page 118 provides us a way of ensuring
this.
Show that if the elements of the square matrix A are integers and if
det(A) = ±1, then (A−1 exists and) the elements of A−1 are integers.
3.16. Verify the relationships shown in equations (3.176) through (3.183) on
page 119. Do this by multiplying the appropriate matrices. For example,
equation (3.176) is veriﬁed by the equations
(I + A−1)A(I + A)−1 = (A + I)(I + A)−1 = (I + A)(I + A)−1 = I.
Make liberal use of equation (3.173) and previously veriﬁed equations.
Of course it is much more interesting to derive relationships such as these
rather than merely to verify them. The veriﬁcation, however, often gives
an indication of how the relationship would arise naturally.
3.17. Verify equation (3.184).
3.18. In equations (3.176) through (3.183) on page 119, drop the assumptions
of nonsingularity of matrices, and assume only that the matrices are

180
3 Basic Properties of Matrices
conformable for the operations in the expressions. Replace the inverse
with the Moore-Penrose inverse.
Now, determine which of these relationships are true. For those that are
true, show that they are (for general but conformable matrices). If the
relationship does not hold, give a counterexample.
3.19. Prove that if A is nonsingular and lower triangular, then A−1 is lower
triangular.
3.20. By writing AA−1 = I, derive the expression for the inverse of a parti-
tioned matrix given in equation (3.190).
3.21. Show that the expression given in equation (3.209) on page 128 is a
Moore-Penrose inverse of A. (Show that properties 1 through 4 hold.)
3.22. Properties of Drazin inverses (page 129).
a) Show that a Drazin inverse is a g1 inverse; that is,
AADA = A.
b) Prove equation (3.212), for any square matrix A and any positive
integer k,
AD = Ak(A2k+1)+Ak;
inter alia, show that for positive integers j and k,
Aj(A2j+1)+Aj = Ak(A2k+1)+Ak.
3.23. Show that the expression given for the generalized inverse in equa-
tion (3.214) on page 131 is correct.
3.24. In computational explorations involving matrices, it is often convenient
to work with matrices whose elements are integers. If eigenvalues are
involved, it would be nice to know that the eigenvalues are also inte-
gers. This is similar in spirit to matrices with integral elements whose
inverses also have integral elements, as was the problem considered in
Exercise 3.15. Matrices like this also provide convenient test problems
for algorithms or sogtware.
The use of the companion matrix (equation (3.225)) gives us a conve-
nient way of obtaining “nice” matrices for numerically exploring proper-
ties of eigenvalues. Using other properties of eigenvalues/vectors such as
those listed on page 136 and with similarity transforms, we can generate
“interesting” matrices that have nice eigenvalues.
For instance, a 3×3 matrix in equation (3.237) was generated by choos-
ing a set of eigenvalues {a, 1 + i, 1 −i}.
Next, I used the relationship between the eigenvalues of A and A −dI,
and ﬁnally, I squared the matrix, so that the eigenvalues are squared.
The resulting spectrum is {(a−d)2, (1−d+i)2, (1−d−i}. After initializing
a and d, the R statements are
B <- matrix(c(-d,0,2*a, 1,-d,-2*a+2, 0,1,a+2-d),nrow=3)
A <- B%*%B

Exercises
181
eigen(A)
and the Matlab statements are
B = [-d, 0, 2*a;
1, -d, -2*a+2;
0, 1, a+2-d];
A = B*B;
eigen(A)
Can you tell what values of a and d were used in generating the matrix
in equation (3.237) following these steps?
a) Using R, Matlab, or some other system you like, construct two dif-
ferent 3 × 3 matrices whose elements are all integers and whose
eigenvalues are {7, 5, 3}.
b) Determine the six Gershgorin disks for each of your matrices.
(Are they the same?)
3.25. Write formal proofs of the properties of eigenvalues/vectors listed on
page 136.
3.26. Let A be a square matrix with an eigenvalue c and corresponding eigen-
vector v. Consider the matrix polynomial in A
p(A) = b0I + b1A + · · · + bkAk.
Show that if (c, v) is an eigenpair of A, then p(c), that is,
b0 + b1c + · · · + bkck,
is an eigenvalue of p(A) with corresponding eigenvector v. (Technically,
the symbol p(·) is overloaded in these two instances.)
3.27. Write formal proofs of the properties of eigenvalues/vectors listed on
page 139.
3.28. Prove that for any square matrix, the algebraic multiplicity of a given
eigenvalue is at least as great as the geometric multiplicity of that eigen-
value.
3.29. a) Show that the unit vectors are eigenvectors of a diagonal matrix.
b) Give an example of two similar matrices whose eigenvectors are not
the same.
Hint: In equation (3.241), let A be a 2 × 2 diagonal matrix (so you
know its eigenvalues and eigenvectors) with unequal values along
the diagonal, and let P be a 2 × 2 upper triangular matrix, so that
you can invert it. Form B and check the eigenvectors.
3.30. Let A be a diagonalizable matrix (not necessarily symmetric) with a
spectral decomposition of the form of equation (3.261), A = 
i ciPi.
Let cj be a simple eigenvalue with associated left and right eigenvectors
yj and xj, respectively. (Note that because A is not symmetric, it may
have nonreal eigenvalues and eigenvectors.)

182
3 Basic Properties of Matrices
a) Show that yH
j xj ̸= 0.
b) Show that the projection matrix Pj is xjyH
j /yH
j xj.
3.31. If A is nonsingular, show that for any (conformable) vector x
(xTAx)(xTA−1x) ≥(xTx)2.
Hint: Use the square roots and the Cauchy-Schwarz inequality.
3.32. Prove that the induced norm (page 165) is a matrix norm; that is, prove
that it satisﬁes the consistency property.
3.33. Prove the inequality (3.280) for an induced matrix norm on page 165:
∥Ax∥≤∥A∥∥x∥.
3.34. Prove that, for the square matrix A,
∥A∥2
2 = ρ(ATA).
Hint: Show that ∥A∥2
2 = max xTATAx for any normalized vector x.
3.35. Let Q be an n × n orthogonal matrix, and let x be an n-vector.
a) Prove equation (3.286):
∥Qx∥2 = ∥x∥2.
Hint: Write ∥Qx∥2 as

(Qx)TQx.
b) Give examples to show that this does not hold for other norms.
3.36. The triangle inequality for matrix norms: ∥A + B∥≤∥A∥+ ∥B∥.
a) Prove the triangle inequality for the matrix L1 norm.
b) Prove the triangle inequality for the matrix L∞norm.
c) Prove the triangle inequality for the matrix Frobenius norm.
3.37. Prove that the Frobenius norm satisﬁes the consistency property.
3.38. The Frobenius p norm and the Shatten p norm.
a) Prove that the expression in equation (3.298), the “Frobenius p
norm”, is indeed a norm.
b) Prove that the expression in equation (3.300), the “Shatten p norm”,
is indeed a norm.
c) Prove equation (3.301).
3.39. If ∥· ∥a and ∥· ∥b are matrix norms induced respectively by the vector
norms ∥· ∥va and ∥· ∥vb, prove inequality (3.302); that is, show that
there are positive numbers r and s such that, for any A,
r∥A∥b ≤∥A∥a ≤s∥A∥b.
3.40. Prove inequalities (3.303) through (3.309), and show that the bounds
are sharp by exhibiting instances of equality.
3.41. The spectral radius, ρ(A).

Exercises
183
a) We have seen by an example that ρ(A) = 0 does not imply A = 0.
What about other properties of a matrix norm? For each, either
show that the property holds for the spectral radius or, by means
of an example, that it does not hold.
b) Use the outer product of an eigenvector and the one vector to show
that for any norm ∥· ∥and any matrix A, ρ(A) ≤∥A∥.
3.42. Show that the function ∥· ∥d deﬁned in equation (3.313) is a norm.
Hint: Just verify the properties on page 164 that deﬁne a norm.
3.43. Nilpotent matrices.
a) Prove that a nilpotent matrix is singular without using the proper-
ties listed on page 174.
b) Prove equations (3.318) through (3.321).
3.44. Prove equations (3.324) and (3.325) under the restriction that V(X) ⊆
V(A); that is, where X = BL for a matrix B whose columns span V(A).

4
Vector/Matrix Derivatives and Integrals
The operations of diﬀerentiation and integration of vectors and matrices are
logical extensions of the corresponding operations on scalars. There are three
objects involved in this operation:
•
the variable of the operation;
•
the operand (the function being diﬀerentiated or integrated); and
•
the result of the operation.
In the simplest case, all three of these objects are of the same type, and
they are scalars. If either the variable or the operand is a vector or a matrix,
however, the structure of the result may be more complicated. This statement
will become clearer as we proceed to consider speciﬁc cases.
In this chapter, we state or show the form that the derivative takes in
terms of simpler derivatives. We state high-level rules for the nature of the
diﬀerentiation in terms of simple partial diﬀerentiation of a scalar with respect
to a scalar. We do not consider whether or not the derivatives exist. In general,
if the simpler derivatives we write that comprise the more complicated object
exist, then the derivative of that more complicated object exists. Once a shape
of the derivative is determined, deﬁnitions or derivations in ϵ-δ terms could
be given, but we will refrain from that kind of formal exercise. The purpose of
this chapter is not to develop a calculus for vectors and matrices but rather to
consider some cases that ﬁnd wide applications in statistics. For a more careful
treatment of diﬀerentiation of vectors and matrices, the reader is referred to
Magnus and Neudecker (1999) or to Kollo and von Rosen (2005). Anderson
(2003), Muirhead (1982), and Nachbin (1965) also cover various aspects of
integration with respect to vector or matrix diﬀerentials.
Diﬀerentiation is a common operation in solving optimization problems
such as least squares or maximum likelihood, and in Sect. 4.4 we illustrate
some of the uses of vector/matrix diﬀerentiation.
Because integration is a key operation in work with probability distribu-
tions (it is the deﬁnition of an expected value), in Sect. 4.5, we brieﬂy discuss
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 4
185

186
4 Vector/Matrix Derivatives and Integrals
multivariate probability distributions and some expectations of multivariate
random variables.
4.1 Functions of Vectors and Matrices
There are various types of functions of vectors and matrices. Some functions
of matrices, such as the trace, the determinant, and the diﬀerent norms are
all functions from IRn×n into IR or into the nonnegative reals, ¯IR+. Other
functions of vectors and matrices are just those deﬁned by elementwise opera-
tions, such as sin(A) = (sin(aij)) and exp(A) = (exp(aij)). That is, a standard
function that maps IR to IR, when evaluated on IRn×m maps to IRn×m in a
very direct way. The derivative of a function deﬁned in this way is simply
the derivative of the individual elements if they all exist. Most of the math-
ematical software, such as R, Matlab, and modern Fortran, interpret builtin
functions this way when a matrix if given as the argument.
For a diagonalizable matrix, another way of deﬁning a function of a matrix
that corresponds to some function of a scalar, f(x), is to use the diagonal
factorization as in equation (3.250) on page 152:
f(A) = V diag((f(c1), . . . , f(cn)))V −1,
if f(·) is deﬁned for each eigenvalue ci.
If a function of a scalar f(x) has a convergent series expansion, another way
of deﬁning a corresponding function of a matrix is to use the series expansion.
For example, using the power series expansion of ex = ∞
k=0
xk
k! , we deﬁned
the matrix exponential for the square matrix A in equation (3.251) as the
matrix
eA =
∞

k=0
Ak
k! .
(Both R and Matlab have a function expm for the matrix exponential.) Of
course, eA may also be interpreted as the matrix with elements (eaij), as
mentioned above. The derivative of a function deﬁned as a convergent series
is simply the series of the derivatives of the individual terms, if they all exist
and if they converge. (Otherwise, it may not be deﬁned.)
The form of the vector or matrix function obviously determines how we
must interpret a derivative or an integral involving the function. If diﬀerenti-
ation or integration can be done term by term, the nature of the terms must
be taken into account.
An extensive coverage of matrix functions is given in Higham (2008).
4.2 Basics of Diﬀerentiation
It is useful to recall the heuristic interpretation of a derivative. A derivative
of a function is the inﬁnitesimal rate of change of the function with respect

4.2 Basics of Diﬀerentiation
187
to the variable with which the diﬀerentiation is taken. If both the function
and the variable are scalars, this interpretation is unambiguous. If, however,
the operand of the diﬀerentiation, Φ, is a more complicated function, say a
vector or a matrix, and/or the variable of the diﬀerentiation, Ξ, is a more
complicated object, the changes are more diﬃcult to measure. Change in the
value both of the function,
δΦ = Φnew −Φold,
and of the variable,
δΞ = Ξnew −Ξold,
could be measured in various ways; for example, by using various norms, as
discussed in Sects. 2.1.5 and 3.9. (Note that the subtraction is not necessarily
ordinary scalar subtraction.)
Furthermore, we cannot just divide the function values by δΞ. We do not
have a deﬁnition for division by that kind of object. We need a mapping,
possibly a norm, that assigns a positive real number to δΞ. We can deﬁne
the change in the function value as just the simple diﬀerence of the function
evaluated at the two points. This yields
lim
∥δΞ∥→0
Φ(Ξ + δΞ) −Φ(Ξ)
∥δΞ∥
.
(4.1)
So long as we remember the complexity of δΞ, however, we can adopt a
simpler approach. Since for both vectors and matrices, we have deﬁnitions of
multiplication by a scalar and of addition, we can simplify the limit in the
usual deﬁnition of a derivative, δΞ →0. Instead of using δΞ as the element
of change, we will use tΥ, where t is a scalar and Υ is an element to be added
to Ξ. The limit then will be taken in terms of t →0. This leads to
lim
t→0
Φ(Ξ + tΥ) −Φ(Ξ)
t
(4.2)
as a formula for the derivative of Φ with respect to Ξ.
The expression (4.2) may be a useful formula for evaluating a derivative,
but we must remember that it is not the derivative. The type of object of
this formula is the same as the type of object of the function, Φ; it does not
accommodate the type of object of the argument, Ξ, unless Ξ is a scalar. As
we will see below, for example, if Ξ is a vector and Φ is a scalar, the derivative
must be a vector, yet in that case the expression (4.2) is a scalar.
The expression (4.1) is rarely directly useful in evaluating a derivative, but
it serves to remind us of both the generality and the complexity of the concept.
Both Φ and its arguments could be functions, for example. (In functional
analysis, various kinds of functional derivatives are deﬁned, such as a Gˆateaux
derivative. These derivatives ﬁnd applications in developing robust statistical
methods; see Shao 2003, for example.) In this chapter, we are interested in the
combinations of three possibilities for Φ, namely scalar, vector, and matrix,
and the same three possibilities for Ξ and Υ.

188
4 Vector/Matrix Derivatives and Integrals
4.2.1 Continuity
For the derivative of a function to exist at a point, the function must be
continuous at that point. A function of a vector or a matrix is continuous
if it is continuous for each element of the vector or matrix. Just as scalar
sums and products are continuous, vector/matrix sums and all of the types
of vector/matrix products we have discussed are continuous. A continuous
function of a continuous function is continuous.
Many of the vector/matrix functions we have discussed are clearly con-
tinuous. For example, the Lp vector norms in equation (2.33) are continuous.
The determinant of a matrix is continuous, as we see from the deﬁnition of
the determinant and the fact that sums and scalar products are continuous.
The fact that the determinant is a continuous function immediately yields
the result that cofactors and hence the adjugate are continuous. From the
relationship between an inverse and the adjugate (equation (3.172)), we see
that the inverse is a continuous function.
One important function that is not continuous is the rank of a matrix.
4.2.2 Notation and Properties
We write the diﬀerential operator with respect to the dummy variable x as
∂/∂x or ∂/∂xT. We usually denote diﬀerentiation using the symbol for “par-
tial” diﬀerentiation, ∂, whether the operator is written ∂xi for diﬀerentiation
with respect to a speciﬁc scalar variable or ∂x for diﬀerentiation with respect
to the array x that contains all of the individual elements. Sometimes, how-
ever, if the diﬀerentiation is being taken with respect to the whole array (the
vector or the matrix), we use the notation d/dx.
The operand of the diﬀerential operator ∂/∂x is a function of x. (If it is not
a function of x—that is, if it is a constant function with respect to x—then the
operator evaluates to 0.) The result of the operation, written ∂f/∂x, is also a
function of x, with the same domain as f, and we sometimes write ∂f(x)/∂x
to emphasize this fact. The value of this function at the ﬁxed point x0 is
written as ∂f(x0)/∂x. (The derivative of the constant f(x0) is identically 0,
but it is not necessary to write ∂f(x)/∂x|x0 because ∂f(x0)/∂x is interpreted
as the value of the function ∂f(x)/∂x at the ﬁxed point x0.)
If ∂/∂x operates on f, and f : S →T , then ∂/∂x : S →U. The nature
of S, or more directly the nature of x, whether it is a scalar, a vector, or
a matrix, and the nature of T determine the structure of the result U. For
example, if x is an n-vector and f(x) = xTx, then
f : IRn →IR
and
∂f/∂x : IRn →IRn,

4.2 Basics of Diﬀerentiation
189
as we will see. The outer product, h(x) = xxT, is a mapping to a higher rank
array, but the derivative of the outer product is a mapping to an array of the
same rank; that is,
h : IRn →IRn×n
and
∂h/∂x : IRn →IRn.
(Note that “rank” here means the number of dimensions; see page 5.)
As another example, consider g(·) = det(·), so
g : IRn×n →IR.
In this case,
∂g/∂X : IRn×n →IRn×n;
that is, the derivative of the determinant of a square matrix is a square matrix,
as we will see later.
Higher-order diﬀerentiation is a composition of the ∂/∂x operator with
itself or of the ∂/∂x operator with the ∂/∂xT operator. For example, consider
the familiar function in linear least squares
f(b) = (y −Xb)T(y −Xb).
This is a mapping from IRm to IR. The ﬁrst derivative with respect to the m-
vector b is a mapping from IRm to IRm, namely 2XTXb −2XTy. The second
derivative with respect to bT is a mapping from IRm to IRm×m, namely, 2XTX.
(Many readers will already be familiar with these facts. We will discuss the
general case of diﬀerentiation with respect to a vector in Sect. 4.3.2.)
We see from expression (4.1) that diﬀerentiation is a linear operator; that
is, if D(Φ) represents the operation deﬁned in expression (4.1), Ψ is another
function in the class of functions over which D is deﬁned, and a is a scalar
that does not depend on the variable Ξ, then D(aΦ + Ψ) = aD(Φ) + D(Ψ).
This yields the familiar rules of diﬀerential calculus for derivatives of sums or
constant scalar products. Other usual rules of diﬀerential calculus apply, such
as for diﬀerentiation of products and composition (the chain rule). We can
use expression (4.2) to work these out. For example, for the derivative of the
product ΦΨ, after some rewriting of terms, we have the numerator
Φ(Ξ)

Ψ(Ξ + tΥ) −Ψ(Ξ)

+ Ψ(Ξ)

Φ(Ξ + tΥ) −Φ(Ξ)

+

Φ(Ξ + tΥ) −Φ(Ξ)

Ψ(Ξ + tΥ) −Ψ(Ξ)

.
Now, dividing by t and taking the limit, assuming that as
t →0,
(Φ(Ξ + tΥ) −Φ(Ξ)) →0,

190
4 Vector/Matrix Derivatives and Integrals
we have
D(ΦΨ) = D(Φ)Ψ + ΦD(Ψ),
(4.3)
where again D represents the diﬀerentiation operation.
4.2.3 Diﬀerentials
For a diﬀerentiable scalar function of a scalar variable, f(x), the diﬀerential
of f at c with increment u is udf/dx|c. This is the linear term in a truncated
Taylor series expansion:
f(c + u) = f(c) + u d
dxf(c) + r(c, u).
(4.4)
Technically, the diﬀerential is a function of both x and u, but the notation
df is used in a generic sense to mean the diﬀerential of f. For vector/matrix
functions of vector/matrix variables, the diﬀerential is deﬁned in a similar
way. The structure of the diﬀerential is the same as that of the function; that
is, for example, the diﬀerential of a matrix-valued function is a matrix.
4.3 Types of Diﬀerentiation
In the following sections we consider diﬀerentiation with respect to diﬀerent
types of objects ﬁrst, and then we consider diﬀerentiation of diﬀerent types
of objects.
4.3.1 Diﬀerentiation with Respect to a Scalar
Diﬀerentiation of a structure (vector or matrix, for example) with respect to
a scalar is quite simple; it just yields the ordinary derivative of each element
of the structure in the same structure. Thus, the derivative of a vector or a
matrix with respect to a scalar variable is a vector or a matrix, respectively,
of the derivatives of the individual elements.
Diﬀerentiation with respect to a vector or matrix, which we will consider
below, is often best approached by considering diﬀerentiation with respect to
the individual elements of the vector or matrix, that is, with respect to scalars.
4.3.1.1 Derivatives of Vectors with Respect to Scalars
The derivative of the vector y(x) = (y1, . . . , yn) with respect to the scalar x
is the vector
∂y/∂x = (∂y1/∂x, . . . , ∂yn/∂x).
(4.5)
The second or higher derivative of a vector with respect to a scalar is
likewise a vector of the derivatives of the individual elements; that is, it is an
array of higher rank.

4.3 Types of Diﬀerentiation
191
4.3.1.2 Derivatives of Matrices with Respect to Scalars
The derivative of the matrix Y (x) = (yij) with respect to the scalar x is the
matrix
∂Y (x)/∂x = (∂yij/∂x).
(4.6)
The second or higher derivative of a matrix with respect to a scalar is
likewise a matrix of the derivatives of the individual elements.
4.3.1.3 Derivatives of Functions with Respect to Scalars
Diﬀerentiation of a function of a vector or matrix that is linear in the elements
of the vector or matrix involves just the diﬀerentiation of the elements, fol-
lowed by application of the function. For example, the derivative of a trace of
a matrix is just the trace of the derivative of the matrix. On the other hand,
the derivative of the determinant of a matrix is not the determinant of the
derivative of the matrix (see below).
4.3.1.4 Higher-Order Derivatives with Respect to Scalars
Because diﬀerentiation with respect to a scalar does not change the rank
of the object (“rank” here means rank of an array or “shape”), higher-order
derivatives ∂k/∂xk with respect to scalars are merely objects of the same rank
whose elements are the higher-order derivatives of the individual elements.
4.3.2 Diﬀerentiation with Respect to a Vector
Diﬀerentiation of a given object with respect to an n-vector yields a vector
for each element of the given object. The basic expression for the derivative,
from formula (4.2), is
lim
t→0
Φ(x + ty) −Φ(x)
t
(4.7)
for an arbitrary conformable vector y. The arbitrary y indicates that the
derivative is omnidirectional; it is the rate of change of a function of the
vector in any direction.
4.3.2.1 Derivatives of Scalars with Respect to Vectors: The
Gradient
The derivative of a scalar-valued function with respect to a vector is a vector
of the partial derivatives of the function with respect to the elements of the
vector. If f(x) is a scalar function of the vector x = (x1, . . . , xn),
∂f
∂x =
 ∂f
∂x1
, . . . , ∂f
∂xn

,
(4.8)

192
4 Vector/Matrix Derivatives and Integrals
if those derivatives exist. This vector is called the gradient of the scalar-valued
function, and is sometimes denoted by gf(x) or ∇f(x), or sometimes just gf
or ∇f:
gf = ∇f = ∂f
∂x.
(4.9)
The notation gf or ∇f implies diﬀerentiation with respect to “all” arguments
of f, hence, if f is a scalar-valued function of a vector argument, they represent
a vector. The symbol ∇with this interpretation is called “nabla”.
This derivative is useful in ﬁnding the maximum or minimum of a func-
tion. Such applications arise throughout statistical and numerical analysis. In
Sect. 6.3.2, we will discuss a method of solving linear systems of equations by
formulating the problem as a minimization problem.
Inner products, bilinear forms, norms, and variances are interesting scalar-
valued functions of vectors. In these cases, the function Φ in equation (4.7) is
scalar-valued and the numerator is merely Φ(x + ty) −Φ(x). Consider, for ex-
ample, the quadratic form xTAx. Using equation (4.7) to evaluate ∂xTAx/∂x,
we have
lim
t→0
(x + ty)TA(x + ty) −xTAx
t
= lim
t→0
xTAx + tyTAx + tyTATx + t2yTAy −xTAx
t
= yT(A + AT)x,
(4.10)
for an arbitrary y (that is, “in any direction”), and so ∂xTAx/∂x = (A+AT)x.
This immediately yields the derivative of the square of the Euclidean norm
of a vector, ∥x∥2
2, and the derivative of the Euclidean norm itself by using
the chain rule. Other Lp vector norms may not be diﬀerentiable everywhere
because of the presence of the absolute value in their deﬁnitions. The fact that
the Euclidean norm is diﬀerentiable everywhere is one of its most important
properties.
The derivative of the quadratic form also immediately yields the derivative
of the variance. The derivative of the correlation, however, is slightly more
diﬃcult because it is a ratio (see Exercise 4.2).
The operator ∂/∂xT applied to the scalar function f results in gT
f .
The second derivative of a scalar-valued function with respect to a vector
is a derivative of the ﬁrst derivative, which is a vector. We will now consider
derivatives of vectors with respect to vectors.
4.3.2.2 Derivatives of Vectors with Respect to Vectors: The
Jacobian
The derivative of an m-vector-valued function of an n-vector argument con-
sists of nm scalar derivatives. These derivatives could be put into various

4.3 Types of Diﬀerentiation
193
structures. Two obvious structures are an n×m matrix and an m×n matrix.
For a function f : S ⊆IRn →IRm, we deﬁne ∂f T/∂x to be the n × m ma-
trix, which is the natural extension of ∂/∂x applied to a scalar function, and
∂f/∂xT to be its transpose, the m×n matrix. Although the notation ∂f T/∂x
is more precise because it indicates that the elements of f correspond to the
columns of the result, we often drop the transpose in the notation. We have
∂f
∂x = ∂f T
∂x
by convention
=
!∂f1
∂x . . . ∂fm
∂x
"
=
⎡
⎢⎢⎢⎢⎢⎣
∂f1
∂x1
∂f2
∂x1 · · ·
∂fm
∂x1
∂f1
∂x2
∂f2
∂x2 · · ·
∂fm
∂x2
· · ·
∂f1
∂xn
∂f2
∂xn · · ·
∂fm
∂xn
⎤
⎥⎥⎥⎥⎥⎦
(4.11)
if those derivatives exist. This derivative is called the matrix gradient and is
denoted by Gf or ∇f for the vector-valued function f. (Note that the nabla
symbol can denote either a vector or a matrix, depending on whether the
function being diﬀerentiated is scalar-valued or vector-valued.)
The m × n matrix ∂f/∂xT = (∇f)T is called the Jacobian of f and is
denoted by Jf:
Jf = GT
f = (∇f)T.
(4.12)
The absolute value of the determinant of the Jacobian appears in integrals
involving a change of variables. (Occasionally, the term “Jacobian” is used
to refer to the absolute value of the determinant rather than to the matrix
itself.)
To emphasize that the quantities are functions of x, we sometimes write
∂f(x)/∂x, Jf(x), Gf(x), or ∇f(x).
4.3.2.3 Derivatives of Vectors with Respect to Vectors in IR3:
The Divergence and the Curl
In some applications in which the order of f and x are the same, the trace of
the matrix gradient is of interest. This is particularly the case in IR3, where,
for example, the three elements of f may represent the expansion of a gas in
the three directions of an orthogonal Cartesian coordinate system. The change
in the density of the gas at the point x is the sum of the changes of the density
in each of the orthogonal directions:
∂f1
∂x1
(x) + ∂f2
∂x2
(x) + ∂f3
∂x3
(x) = tr(∇f(x)).

194
4 Vector/Matrix Derivatives and Integrals
This type of expression arises so often in physical applications that it is
given a name, “divergence”, and a special symbol for the operation on the
function f, div. Another expression for the divergence that is common in
physics arises from the interpretation of ∇as an ordered list of operators,
(∂/∂x1, ∂/∂x2, . . .). The nabla symbol with this interpretation is called “del”.
The application of ∇in this form to the function f is analogous to the dot
product of ∇and f; hence, it is often written as ∇· f; that is, in diﬀerent
notations, we have
∇· f = div(f) = tr(∇f).
In physical applications, when the vector function f above is actually the
gradient of a scalar function, the divergence of f determines some interesting
characteristics of the underlying scalar function. For example, if u(x) is the
temperature at x, then the gradient ∇u is the change in temperature mea-
sured in each of the components of x. The divergence of this gradient can be
interpreted as the total change in the heat. In a closed system, this should
be zero, and if the total heat is changing, this should be proportional to the
change in heat. (This fact is called the “heat equation”.) There are similar
situations in which the divergence of the gradient of a scalar function is of
interest.
The operator representing the operation just described is called the
Laplace operator or Laplacian operator. In the notation above, it can be rep-
resented as ∇· ∇u, and so the operator itself is sometimes represented as ∇2,
which is called “del-squared”; however, it is sometimes also represented as Δ.
We also use the symbol ∇2 to denote the Hessian of a scalar function. In that
context it is called “nabla-squared”. The Laplace operator as deﬁned here is
the trace of the Hessian matrix as deﬁned in equation (4.16).
There are many important physical applications and special operations for
vectors in IR3, as we have discussed in Sect. 2.2.9, beginning on page 46. In
particular, we deﬁned the cross product of the vectors x and y, as
x × y = (x2y3 −x3y2, x3y1 −x1y3, x1y2 −x2y1).
Now, suppose that y is a 3-vector function of x, say, f(x), and as we did with
the del operator ∇= (∂/∂x1, ∂/∂x2, ∂/∂x3) above, let us consider a formal
substitution:
∇× f = (∂f3/∂x2 −∂f2/∂x3,
∂f1/∂x3 −∂f3/∂x1,
∂f2/∂x1 −∂f1/∂x2).
We call this the curl of f with respect to x and write
curl(f) = ∇× f.
The uses of the curl in both physical and geometrical applications are far-
reaching, but we will not go into them here. An interesting mathematical fact
is that the curl of the gradient of a twice-diﬀerential scalar function is 0:
curl(gf) = 0,
(4.13)

4.3 Types of Diﬀerentiation
195
where f is a twice-diﬀerential scalar function. You are asked to show this in
Exercise 4.3.
In IR3, for a vector function f that is twice diﬀerential, there is an inter-
esting relationship between the divergence and the curl:
div(curl(f)) = 0
(4.14)
which you are asked to show in Exercise 4.4.
4.3.2.4 Derivatives of Matrices with Respect to Vectors
The derivative of a matrix with respect to a vector is a three-dimensional
object that results from applying equation (4.8) to each of the elements of the
matrix. For this reason, it is simpler to consider only the partial derivatives
of the matrix Y with respect to the individual elements of the vector x; that
is, ∂Y/∂xi. The expressions involving the partial derivatives can be thought
of as deﬁning one two-dimensional layer of a three-dimensional object.
Using the rules for diﬀerentiation of powers that result directly from the
deﬁnitions, we can write the partial derivatives of the inverse of the matrix Y
as
∂
∂xY −1 = −Y −1
 ∂
∂xY

Y −1
(4.15)
(see Exercise 4.5).
Beyond the basics of diﬀerentiation of constant multiples or powers of a
variable, the two most important properties of derivatives of expressions are
the linearity of the operation and the chaining of the operation. These yield
rules that correspond to the familiar rules of the diﬀerential calculus. A simple
result of the linearity of the operation is the rule for diﬀerentiation of the trace:
∂
∂xtr(Y ) = tr
 ∂
∂xY

.
4.3.2.5 Higher-Order Derivatives with Respect to Vectors: The
Hessian
Higher-order derivatives are derivatives of lower-order derivatives. As we have
seen, a derivative of a given function with respect to a vector is a more compli-
cated object than the original function. The simplest higher-order derivative
with respect to a vector is the second-order derivative of a scalar-valued func-
tion. Higher-order derivatives may become uselessly complicated.
In accordance with the meaning of derivatives of vectors with respect to
vectors, the second derivative of a scalar-valued function with respect to a
vector is a matrix of the partial derivatives of the function with respect to the
elements of the vector. This matrix is called the Hessian, and is denoted by

196
4 Vector/Matrix Derivatives and Integrals
Hf or sometimes by ∇∇f or ∇2f (“nabla-squared”, not to confused with the
Laplace operator, “del-squared”):
Hf =
∂2f
∂x∂xT =
⎡
⎢⎢⎢⎢⎢⎢⎣
∂2f
∂x2
1
∂2f
∂x1∂x2 · · ·
∂2f
∂x1∂xm
∂2f
∂x2∂x1
∂2f
∂x2
2
· · ·
∂2f
∂x2∂xm
· · ·
∂2f
∂xm∂x1
∂2f
∂xm∂x2 · · ·
∂2f
∂x2m
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(4.16)
To emphasize that the Hessian is a function of x, we sometimes write
Hf(x) or ∇∇f(x) or ∇2f(x).
4.3.2.6 Summary of Derivatives with Respect to Vectors
As we have seen, the derivatives of functions are complicated by the problem
of measuring the change in the function, but often the derivatives of functions
with respect to a vector can be determined by using familiar scalar diﬀeren-
tiation. In general, we see that
•
the derivative of a scalar (a quadratic form) with respect to a vector is a
vector and
•
the derivative of a vector with respect to a vector is a matrix.
Table 4.1 lists formulas for the vector derivatives of some common expres-
sions. The derivative ∂f/∂xT is the transpose of ∂f/∂x.
As noted above, some authors express derivatives in diﬀerent structures.
The values of the individual elements are the same, but their organization in
a vector or matrix may be diﬀerent.
4.3.3 Diﬀerentiation with Respect to a Matrix
The derivative of a function with respect to a matrix is a matrix with the same
shape consisting of the partial derivatives of the function with respect to the
elements of the matrix. This rule deﬁnes what we mean by diﬀerentiation with
respect to a matrix.
For scalar-valued functions, this rule is fairly simple:
∂f(X)
∂X
=
∂f(X)
∂xij

.
(4.17)
For example, consider the trace. If X is a square matrix and we apply this
rule to evaluate ∂tr(X)/∂X, we get the identity matrix, where the nonzero
elements arise only when j = i in ∂( xii)/∂xij. If AX is a square matrix,
we have for the (i, j) term in ∂tr(AX)/∂X, ∂
i

k aikxki/∂xij = aji, and
so ∂tr(AX)/∂X = AT, and likewise, inspecting ∂
i

k xikxki/∂xij, we get

4.3 Types of Diﬀerentiation
197
Table 4.1. Formulas for some vector derivatives
f(x)
∂f/∂x
ax
aI
bTx
b
xTb
b
xTx
I ⊗x + x ⊗I
xxT
2xT
bTAx
ATb
xTAb
Ab
xTAx
(A + AT)x
2Ax, if A is symmetric
exp(−1
2xTAx)
−exp(−1
2xTAx)Ax, if A is symmetric
∥x∥2
2
2x
V(x)
2x/(n −1)
In this table, x is an n-vector, a is a constant scalar, b is a
constant conformable vector, and A is a constant conformable
matrix.
∂tr(XTX)/∂X = 2XT. Likewise for the scalar-valued aTXb, where a and
b are conformable constant vectors, for ∂
m(
k akxkm)bm/∂xij = aibj, so
∂aTXb/∂X = abT.
Now consider ∂det(X)/∂X. Using an expansion in cofactors (equa-
tion (3.30) or (3.31)), the only term in det(X) that involves xij
is
xij(−1)i+jdet(X−(i)(j)), and the cofactor (x(ij))
=
(−1)i+jdet(X−(i)(j))
does not involve xij. Hence, ∂det(X)/∂xij = (x(ij)), and so ∂det(X)/∂X =
(adj(X))T from equation (3.33). Using equation (3.172), we can write this as
∂det(X)/∂X = det(X)X−T.
By the deﬁnition of diﬀerentiation with respect to a matrix X, we see that
the derivative ∂f/∂XT is the transpose of ∂f/∂X.
The chain rule can be used to evaluate ∂log(det(X))/∂X.
Applying the rule stated at the beginning of this section, we see that the
derivative of a matrix Y with respect to the matrix X is
dY
dX = d(vec(Y ))
d(vec(X)),
(4.18)
or
dY
dX = Y ⊗
d
dX ,
(4.19)
especially in statistical applications. (Recall the comment above about the use
by diﬀerent authors of diﬀerent structures for derivatives.)
Tables 4.2 and 4.3 list some formulas for the matrix derivatives of some
common expressions. The derivatives shown in those tables can be obtained
by using equation (4.17) or (4.18), possibly also using the chain rule.

198
4 Vector/Matrix Derivatives and Integrals
Table 4.2. Formulas for some matrix derivatives
General X
f(X)
∂f/∂X
aTXb
abT
tr(AX)
AT
tr(XTX)
2XT
BX
In ⊗B
XC
CT ⊗Im
BXC
CT ⊗B
In this table, X is an n × m matrix, a is a
constant n-vector, b is a constant m-vector,
A is a constant m×n matrix, B is a constant
p×n matrix, and C is a constant m×q matrix.
Table 4.3. Formulas for some matrix derivatives
Square and possibly invertible X
f(X)
∂f/∂X
tr(X)
In
tr(Xk)
kXk−1
tr(BX−1C)
−(X−1CBX−1)T
det(X)
det(X)X−T
log(det(X))
X−T
(det(X))k
k(det(X))kX−T
BX−1C
−(X−1C)T ⊗BX−1
In this table, X is an n × n matrix, B is a
constant p × n matrix, and C is a constant
n × q matrix.
4.4 Optimization of Scalar-Valued Functions
A common problem in statistics, as well as in science generally, is to ﬁnd the
minimum or maximum of some scalar-valued function. In statistical applica-
tions, for example, we may seek to ﬁt a model to data by determining the
values of model parameters that yield the minimum of the sum of squares of
residuals. Alternatively, we may seek to determine the values of the parame-
ters in a likelihood function that yield the maximum of the likelihood function
for a given set of data.
In optimization problems, we refer to the function of interest as the ob-
jective function. In the least squares ﬁtting problem, the objective function
is the sum of squares, which, for given data, is a function of the parameters
in the model. In the maximum likelihood estimation problem, the objective

4.4 Optimization of Scalar-Valued Functions
199
function is the likelihood function, which, for given data, is a function of the
parameters in the probability model.
Because the function may have many ups and downs, we often use the
phrases local minimum and global minimum (or global maximum or global
optimum), with obvious meanings. Our discussion focuses on local optima,
and we will not consider the problem of ﬁnding a global optimum of a function
with multiple local optima.
Since maximizing a scalar function f(x) is equivalent to minimizing its
negative, −f(x), we can always just consider the basic problem to be mini-
mization of a function. Thus, we can use terminology for the problem of ﬁnding
a minimum of the objective function, and write the general problem as
min
x∈D f(x).
(4.20)
Often, D in the expression above is just the full domain of f, and often that
is IRm for some m. In the next few subsections, we will consider D to be
the full domain of f. Problems in which D is the full domain of f are called
unconstrained optimization problems. We consider them ﬁrst, and then in
Sect. 4.4.5 we will brieﬂy discuss constrained optimization problems.
Methods to approach the optimization problem obviously depend on the
nature of the objective function. Whether or not the function is convex and
whether or not it is diﬀerentiable are important distinctions.
In this section we consider only scalar-valued objective functions that are
twice-diﬀerentiable. For a scalar-valued objective function f(x) of m variables,
we will denote the m-vector of ﬁrst derivatives, that is, the gradient, as gf(x),
and the m × m matrix of second derivatives, the Hessian, as Hf(x), as above.
For a twice-diﬀerentiable function, the Hessian is nonnegative (positive) def-
inite everywhere on a given domain if and only if the function is (strictly)
convex on that domain.
Because a derivative measures the rate of change of a function, a point at
which the ﬁrst derivative is equal to 0 is a stationary point, which may be a
maximum or a minimum of the function. Diﬀerentiation is therefore a very
useful tool for ﬁnding the optima of functions, and so, for a given function
f(x), the gradient vector function, gf(x), and the Hessian matrix function,
Hf(x), play important roles in optimization methods.
Because except in the very simplest of cases, determining a solution to the
equation deﬁning a stationary point, gf(x) = 0 cannot be done in closed form,
the optimization method itself must be iterative, moving through a sequence
of points, x(0), x(1), x(2), . . ., that approaches the optimum point arbitrarily
closely. At the point x(k), the direction of steepest descent is clearly −gf(x(k)),
but because this direction may be continuously changing, the steepest descent
direction may not be the best direction in which to seek the next point, x(k+1).
In most cases, however, we move in a downward path in the general direction
of the gradient. We call any such method a gradient-descent method.

200
4 Vector/Matrix Derivatives and Integrals
4.4.1 Stationary Points of Functions
The optimum point is a stationary point (assuming that it occurs at an interior
point of the domain), but a stationary point is not necessarily an optimum
point.
The ﬁrst derivative of the objective function helps only in ﬁnding a station-
ary point. The matrix of second derivatives, the Hessian, provides information
about the nature of the stationary point, which may be a local minimum or
maximum, a saddlepoint, or only an inﬂection point.
The so-called second-order optimality conditions are the following (see a
general text on optimization, such as Griva, Nash, and Sofer 2009, for the
proofs).
•
If (but not only if) the stationary point is a local minimum, then the
Hessian is nonnegative deﬁnite at the stationary point.
•
If the Hessian is positive deﬁnite at the stationary point, then the station-
ary point is a local minimum.
•
Likewise, if the stationary point is a local maximum, then the Hessian
is nonpositive deﬁnite, and if the Hessian is negative deﬁnite, then the
stationary point is a local maximum.
•
If the Hessian has both positive and negative eigenvalues at the stationary
point, then the stationary point is a saddlepoint.
4.4.2 Newton’s Method
We consider a twice-diﬀerentiable scalar-valued function of a vector argument,
f(x). By a Taylor series expansion about a stationary point x∗, truncated after
the second-order term
f(x) ≈f(x∗) + (x −x∗)Tgf

x∗

+ 1
2(x −x∗)THf

x∗

(x −x∗),
(4.21)
because gf

x∗

= 0, we have a general method of ﬁnding a stationary point
for the function f(·), called Newton’s method. If x is an m-vector, gf(x) is an
m-vector and Hf(x) is an m × m matrix.
Newton’s method is to choose a starting point x(0), then, for k = 0, 1, . . .,
to solve the linear systems
Hf

x(k)
p(k) = −gf

x(k)
(4.22)
for p(k), and then to update the point in the domain of f(·) by
x(k+1) = x(k) + α(k)p(k),
(4.23)
where α(k) is a scalar such that f(x(k+1)) < f(x(k)). The two steps are re-
peated until there is essentially no change from one iteration to the next.
These two steps have a very simple form for a function of one variable (see
Exercise 4.6a).

4.4 Optimization of Scalar-Valued Functions
201
We can stop the iterative progression based on either the change in the
domain or the amount of change in the value of the function. For speciﬁed
constants ϵ1 and ϵ2, we can stop when
x(k+1) −x(k) < ϵ1
(4.24)
or when
f

x(k+1)
−f

x(k) < ϵ2.
(4.25)
If f(·) is a quadratic function, the solution using Newton’s method is not
iterative; it is obtained in one step because equation (4.21) is exact.
4.4.2.1 Quasi-Newton Methods
All gradient-descent methods determine the path p(k) to take toward the next
point by a system of equations of the form
R(k)p(k) = −gf

x(k)
.
In the steepest-descent method, R(k) is the identity, I, in these equations.
For functions with eccentric contours, the steepest-descent method traverses
a zigzag path to the minimum. In Newton’s method (equation (4.22)), R(k)
is the Hessian evaluated at the previous point, Hf

x(k)
, which results in a
more direct path to the minimum than the steepest descent follows.
Aside from the issue of consistency of the resulting equation, a major dis-
advantage of Newton’s method is the computational burden of computing the
Hessian, which requires O(m2) function evaluations, and solving the system,
which requires O(m3) arithmetic operations, at each iteration.
Instead of using the Hessian at each iteration, we may use an approxima-
tion, B(k). We may choose approximations that are simpler to update and/or
that allow the equations for the step to be solved more easily. Methods us-
ing such approximations are called quasi-Newton methods or variable metric
methods.
One simple approximation to the Hessian is based on the fact that
Hf

x(k)
x(k) −x(k−1)
≈gf

x(k)
−gf

x(k−1)
;
hence, we choose B(k) so that
B(k)
x(k) −x(k−1)
= gf

x(k)
−gf

x(k−1)
.
(4.26)
This is called the secant condition.
We express the secant condition as
B(k)s(k) = y(k),
(4.27)

202
4 Vector/Matrix Derivatives and Integrals
where
s(k) = x(k) −x(k−1)
and
y(k) = gf(x(k)) −gf(x(k−1)),
as above.
The system of equations in (4.27) does not fully determine B(k) of course.
Because B(k) should approximate the Hessian, we may require that it be
symmetric and positive deﬁnite.
The most common approach in quasi-Newton methods is ﬁrst to choose
a reasonable starting matrix B(0) and then to choose subsequent matrices by
additive updates,
B(k+1) = B(k) + B(k)
a ,
(4.28)
subject to preservation of symmetry and positive deﬁniteness. An approximate
Hessian B(k) may be used for several iterations before it is updated; that is,
B(k)
a
may be taken as 0 for several successive iterations.
4.4.3 Least Squares
One of the most important applications that involve minimization is the ﬁtting
of a model to data. In this problem, we have a function s that relates one
variable, say y, to other variables, say the m-vector x. The function involves
some unknown parameters, say the d-vector θ, so the model is
y ≈s(x; θ).
(4.29)
The data consists of n observations on the variables y and x.
Fitting the model is usually done by minimizing some norm of the vector
of residuals
ri(θ) = yi −s(xi; θ).
(4.30)
The decision variables are the parameters θ. The optimal values of θ, often
denoted as *θ, are called “estimates”.
Because the data are observed and therefore are constants, the residuals
are functions of θ only. The vector-valued function r(θ) maps IRd into IRn.
The most common norm to minimize to obtain a ﬁt is the L2 or Euclidean
norm. The scalar-valued objective function then is
f(θ) =
n

i=1

yi −s(xi; θ)
2
=
n

i=1

ri(θ)
2
(4.31)
=

r(θ)
Tr(θ).
This problem is called least squares regression. If the function s is nonlinear
in θ, the functions ri are also nonlinear in θ, and the problem is called nonlinear
least squares regression.

4.4 Optimization of Scalar-Valued Functions
203
4.4.3.1 Linear Least Squares
A very common form of the model (4.29) is the linear regression model
y ≈xTθ.
In this form of the model, we usually use β in place of θ, and we write the model
as an equality with an additive “error” term instead of the approximation. In
this case also, the order of β is the same as that of x, which we will continue
to denote as m.
In statistical applications, we generally have n observations of pairs of y
and x. We form an n-vector of the y observations, and an n × m matrix X of
the x observations. Thus, we form the linear model of the data
y = Xβ + ϵ,
(4.32)
where y is an n-vector, X is an n × m matrix, β is an m-vector, and ϵ is an
n-vector.
For any value of β, say b, we have the residual vector
r = y −Xb.
(4.33)
For a least squares ﬁt of the regression model, we minimize its Euclidean
norm,
f(b) = rTr,
(4.34)
with respect to the variable b. We can solve this optimization problem by
taking the derivative of this sum of squares and equating it to zero. Doing
this, we get
d(y −Xb)T(y −Xb)
db
= d(yTy −2bTXTy + bTXTXb)
db
= −2XTy + 2XTXb
= 0,
which yields the normal equations
XTXb = XTy.
(4.35)
The solution to the normal equations is a stationary point of the func-
tion (4.34). The Hessian of (y −Xb)T(y −Xb) with respect to b is 2XTX
and
XTX ⪰0.
Because the matrix of second derivatives is nonnegative deﬁnite, the value of b
that solves the system of equations arising from the ﬁrst derivatives is a local

204
4 Vector/Matrix Derivatives and Integrals
minimum of equation (4.34). We discuss these equations further in Sects. 6.6
and 9.3.2.
The normal equations also imply one of the most important properties of
a linear least squares solution. The residuals are orthogonal to the column
space of X:
XT(y −Xb) = 0.
(4.36)
Notice, of course, that this is just the gradient of the objective function, and
the equation just states that the gradient is 0 at the optimum.
4.4.3.2 Nonlinear Least Squares: The Gauss-Newton Method
If the function in the model (4.29) is not linear, the problem may be quite
diﬀerent, both for solving the least squares problem and for issues involved
in statistical inference. We will not consider the inference problems here, but
rather, just discuss methods for obtaining the least squares ﬁt.
The gradient and the Hessian for a least squares problem have special
structures that involve the Jacobian of the residuals, which is a vector function
of the parameters. The gradient of f(θ) is
gf(θ) =

Jr(θ)
Tr(θ).
The Jacobian of r is also part of the Hessian of f:
Hf(θ) =

Jr(θ)
TJr(θ) +
n

i=1
ri(θ)Hri(θ).
(4.37)
In this maze of notation the reader should pause to remember the shapes
of these arrays, and their meanings in the context of ﬁtting a model to data.
Notice, in particular, that the dimension of the space of the optimization
problem is d, instead of m as in the previous problems. We purposely chose
a diﬀerent letter to represent the dimension so as to emphasize that the de-
cision variables may have a diﬀerent dimension from that of the independent
(observable) variables. The space of an observation has dimension m + 1 (the
m elements of x, plus the response y); and the space of the observations as
points yi and corresponding model values s(xi, θ) has dimension n.
•
xi is an m-vector. In the modeling context, these are the independent
variables.
•
y is an n-vector, and it together with the n xi vectors are constants in the
optimization problem. In the modeling context, these are observations.
•
θ is a d-vector. This is the vector of parameters.
•
r(·) is an n-vector. This is the vector of residuals.
•
Jr(·) is an n × d matrix.
•
Hri(·) is a d × d matrix.

4.4 Optimization of Scalar-Valued Functions
205
•
f(·) is a scalar. This is the objective function for the data-ﬁtting criterion.
•
gf(·) is a d-vector.
•
Hf(·) is a d × d matrix.
In the vicinity of the solution θ∗, the residuals ri(θ) should be small, and
Hf(θ) may be approximated by neglecting the second term in equation (4.37).
Using this approximation and the gradient descent equation, we have

Jr(θ(k))
TJr(θ(k)) p(k) = −

Jr(θ(k))
Tr(θ(k)).
(4.38)
It is clear that the solution p(k) is a descent direction and so gf(θ(k)) ̸= 0, and
(p(k))Tgf(θ(k)) = −

Jr(θ(k))
Tp(k)T 
Jr(θ(k))
Tp(k)
< 0.
The update step is determined by a line search in the direction of the
solution of equation (4.38):
x(k+1) −x(k) = α(k)p(k).
This method is called the Gauss-Newton algorithm. Because years ago the
step was often taken simply as p(k), a method that uses a variable step length
factor α(k) is sometimes called a “modiﬁed Gauss-Newton algorithm”. It is
the only kind to use, so we just call it the “Gauss-Newton algorithm”.
If the residuals are small and if the Jacobian is nonsingular, the Gauss-
Newton method behaves much like Newton’s method near the solution. The
major advantage is that second derivatives are not computed.
If the residuals are not small or if Jr(θ(k)) is poorly conditioned, the Gauss-
Newton method can perform very poorly. If Jr(θ(k)) is not of full rank, we
could choose the solution corresponding to the Moore-Penrose inverse:
p(k) =

Jr(θ(k))
T+
r(θ(k)).
(4.39)
If the matrix is nonsingular, the Moore-Penrose inverse is the usual inverse.
(We consider the linear case more thoroughly in Sect. 6.6.3, beginning on
page 293, and show that the Moore-Penrose yields the solution that has the
shortest Euclidean length.)
In the case of a linear model, the data, consisting of n observations on y
and the m-vector x, result in an n-vector of residuals,
r = y −Xb,
where X is the n×m matrix whose rows are the observed xTs (and b is used in
place of θ). The Gauss-Newton algorithm, which is the same as the ordinary
Newton method for this linear least squares problem, yields the solution in
one step.

206
4 Vector/Matrix Derivatives and Integrals
4.4.3.3 Levenberg-Marquardt Method
Another possibility is to add a conditioning matrix to

Jr(θ(k))
TJr(θ(k)) on
the left side of equation (4.38). A simple choice is τ(k)Id for some scalar τ (k),
and the equation for the update direction becomes

Jr(θ(k))
TJr(θ(k)) + τ (k)Id

p(k) = −

Jr(θ(k))
Tr(θ(k)).
A better choice may be a scaling matrix, S(k), that takes into account the
variability in the columns of Jr(θ(k)); hence we have for the update

Jr(θ(k))
TJr(θ(k)) + λ(k)
S(k)TS(k)
p(k) = −

Jr(θ(k))
Tr(θ(k)). (4.40)
The basic requirement for the matrix

S(k)TS(k) is that it improve the condi-
tion of the coeﬃcient matrix. There are various ways of choosing this matrix.
One is to transform the matrix

Jr(θ(k))
TJr(θ(k)) so it has 1’s along the
diagonal (this is equivalent to forming a correlation matrix from a variance-
covariance matrix), and to use the scaling vector to form S(k). The nonneg-
ative factor λ(k) can be chosen to control the extent of the adjustment. The
sequence λ(k) obviously must go to 0 for the solution sequence to converge to
the optimum.
Use of an adjustment such as in equation (4.40) is called the Levenberg-
Marquardt method. This is probably the most widely used method for nonlin-
ear least squares.
The Levenberg-Marquardt adjustment is similar to the regularization done
in ridge regression that we will discuss from time to time (see, for example,
equation (8.59) on page 364). Just as in ridge regression the computations for
equation (4.40) can be performed eﬃciently by recognizing that the system is
the normal equations for the least squares ﬁt of
⎛
⎝
r(θ(k))
0
⎞
⎠≈
⎡
⎣
Jr(θ(k))
√
λ(k)
S(k)
⎤
⎦p.
(See page 432 and Exercise 9.11.)
Equation (4.40), as indeed regularized solutions such as ridge regression
generally, can be thought of as a Lagrange multiplier formulation of a con-
strained problem, as we will discuss in Sect. 4.4.5, beginning on page 208.
4.4.4 Maximum Likelihood
For a sample y = (y1, . . . , yn) from a probability distribution with probability
density function p(·; θ), the likelihood function is

4.4 Optimization of Scalar-Valued Functions
207
L(θ; y) =
n
$
i=1
p(yi; θ),
(4.41)
and the log-likelihood function is l(θ; y) = log(L(θ; y)). It is often easier to
work with the log-likelihood function.
The log-likelihood is an important quantity in information theory and in
unbiased estimation. If Y is a random variable having the given probability
density function with the r-vector parameter θ, the Fisher information matrix
that Y contains about θ is the r × r matrix
I(θ) = Covθ
∂l(t, Y )
∂ti
, ∂l(t, Y )
∂tj

,
(4.42)
where Covθ represents the variance-covariance matrix of the functions of Y
formed by taking expectations for the given θ. (I use diﬀerent symbols here
because the derivatives are taken with respect to a variable, but the θ in Covθ
cannot be the variable of the diﬀerentiation. This distinction is somewhat
pedantic, and sometimes I follow the more common practice of using the
same symbol in an expression that involves both Covθ and ∂l(θ, Y )/∂θi.)
For example, if the distribution of Y is the d-variate normal distribution
with mean d-vector μ and d × d positive deﬁnite variance-covariance matrix
Σ, from the multivariate normal PDF, equation (4.73), the likelihood, equa-
tion (4.41), is
L(μ, Σ; y) =
1

(2π)d/2|Σ|1/2n exp

−1
2
n

i=1
(yi −μ)TΣ−1(yi −μ)

.
(Note that |Σ|1/2 = |Σ
1
2 |, where here I am using | · | to represent the de-
terminant. The square root matrix Σ
1
2 is often useful in transformations of
variables.)
Anytime we have a quadratic form that we need to simplify, we should
recall equation (3.90): xTAx = tr(AxxT). Using this, and because the log-
likelihood is easier to work with here, we write
l(μ, Σ; y) = c −n
2 log |Σ| −1
2tr

Σ−1
n

i=1
(yi −μ)(yi −μ)T

,
(4.43)
where we have used c to represent the constant portion. Next, we use the
Pythagorean equation (2.70) or equation (3.92) on the outer product to get
l(μ, Σ; y) = c −n
2 log |Σ| −1
2tr

Σ−1
n

i=1
(yi −¯y)(yi −¯y)T

−n
2 tr

Σ−1(¯y −μ)(¯y −μ)T
.
(4.44)
In maximum likelihood estimation, we seek the maximum of the likeli-
hood function (4.41) with respect to θ while we consider y to be ﬁxed. If the

208
4 Vector/Matrix Derivatives and Integrals
maximum occurs within an open set and if the likelihood is diﬀerentiable, we
might be able to ﬁnd the maximum likelihood estimates by diﬀerentiation. In
the log-likelihood for the d-variate normal distribution, we consider the pa-
rameters μ and Σ to be variables. To emphasize that perspective, we replace
the parameters μ and Σ by the variables ˆμ and *Σ. Now, to determine the
maximum, we could take derivatives with respect to ˆμ and *Σ, set them equal
to 0, and solve for the maximum likelihood estimates. Some subtle problems
arise that depend on the fact that for any constant vector a and scalar b,
Pr(aTX = b) = 0, but we do not interpret the likelihood as a probability. In
Exercise 4.7b you are asked to determine the values of ˆμ and *Σ using proper-
ties of traces and positive deﬁnite matrices without resorting to diﬀerentiation.
(This approach does not avoid the subtle problems, however.)
4.4.5 Optimization of Functions with Constraints
Instead of the problem shown in expression (4.20) in which we seek a minimum
point anywhere in the domain of the objective function, which often is IRm,
we may constrain the search for the minimum point to some subset of the
domain. The problem is
min
x∈C f(x),
(4.45)
where C ⊂D, the domain of f. This is a constrained optimization problem.
To emphasize the constraints or to make them more explicit, we often write
the problem as
min
x
f(x)
s.t. x ∈C
(4.46)
Optimization is a large area. It is our intent here only to cover some of the
subareas most closely related to our main subject of matrix algebra. Hence, we
will consider only a simple type of equality-constrained optimization problem.
4.4.5.1 Equality-Constrained Linear Least Squares Problems
Instead of the simple least squares problem of determining a value of b that
minimizes the sum of squares, we may have some restrictions that b must
satisfy; for example, we may have the requirement that the elements of b
sum to 1. More generally, consider the least squares problem for the linear
model (4.32) with the requirement that b satisfy some set of linear restrictions,
Ab = c, where A is a full-rank k × m matrix (with k ≤m). (The rank of A
must be less than m or else the constraints completely determine the solution
to the problem. If the rank of A is less than k, however, some rows of A and
some elements of b could be combined into a smaller number of constraints.
We can therefore assume A is of full row rank. Furthermore, we assume the
linear system is consistent (that is, rank([A|c]) = k) for otherwise there could
be no solution.) We call any point b that satisﬁes Ab = c a feasible point.

4.4 Optimization of Scalar-Valued Functions
209
We write the equality-constrained least squares optimization problem as
min
b
f(b) = (y −Xb)T(y −Xb)
s.t. Ab = c.
(4.47)
If bc is any feasible point (that is, Abc = c), then any other feasible point
can be represented as bc+p, where p is any vector in the null space of A, N(A).
From our discussion in Sect. 3.5.2, we know that the dimension of N(A) is
m −k, and its order is m. If N is an m × (m −k) matrix whose columns
form a basis for N(A), all feasible points can be generated by bc + Nz, where
z ∈IRm−k. Hence, we need only consider the restricted variables
b = bc + Nz
and the “reduced” function
h(z) = f(bc + Nz).
The argument of this function is a vector with only m −k elements instead
of m elements as in the unconstrained problem. It is clear, however, that an
unconstrained minimum point, z∗, of h(z) yields a solution, b∗= bc + Nz∗, of
the original constrained minimization problem.
4.4.5.2 The Reduced Gradient and Reduced Hessian
If we assume diﬀerentiability, the gradient and Hessian of the reduced function
can be expressed in terms of the original function:
gh(z) = N Tgf(bc + Nz)
= N Tgf(b)
(4.48)
and
Hh(z) = N THf(bc + Nz)N
= N THf(b)N.
(4.49)
In equation (4.48), N Tgf(b) is called the reduced gradient or projected gradient,
and N THf(b)N in equation (4.49) is called the reduced Hessian or projected
Hessian.
The properties of stationary points referred to above are the conditions
that determine a minimum of this reduced objective function; that is, b∗is a
minimum if and only if
N Tgf(b∗) = 0,
(4.50)
N THf(b∗)N ≻0,
(4.51)
and
Ab∗= c.
(4.52)
These relationships then provide the basis for the solution of the optimiza-
tion problem.

210
4 Vector/Matrix Derivatives and Integrals
4.4.5.3 Lagrange Multipliers
Because the m × m matrix [N|AT] spans IRm, we can represent the vector
gf(b∗) as a linear combination of the columns of N and AT, that is,
gf(b∗) = [N|AT]

z∗
λ∗

=
 Nz∗
ATλ∗

,
where z∗is an (m −k)-vector and λ∗is a k-vector. Because gh(z∗) = 0, Nz∗
must also vanish (that is, Nz∗= 0), and thus, at the optimum, the nonzero
elements of the gradient of the objective function are linear combinations of
the rows of the constraint matrix, ATλ∗.
The k elements of the linear combination vector λ∗are called Lagrange
multipliers.
4.4.5.4 The Lagrangian
Let us now consider a simple generalization of the constrained problem above
and an abstraction of the results above so as to develop a general method. We
consider the problem
min
x
f(x)
s.t. c(x) = 0,
(4.53)
where f is a scalar-valued function of an m-vector variable and c is a k-
vector-valued function of the variable. There are some issues concerning the
equation c(x) = 0 that we will not go into here. Obviously, we have the same
concerns as before; that is, whether c(x) = 0 is consistent and whether the
individual equations ci(x) = 0 are independent. Let us just assume they are,
and proceed. (Again, we refer the interested reader to a more general text on
optimization.)
Motivated by the results above, we form a function that incorporates a
dot product of Lagrange multipliers and the function c(x):
F(x) = f(x) + λTc(x).
(4.54)
This function is called the Lagrangian. The solution, (x∗, λ∗), of the optimiza-
tion problem occurs at a stationary point of the Lagrangian,
gf(x∗) =

0
Jc(x∗)Tλ∗

.
(4.55)
Thus, at the optimum, the gradient of the objective function is a linear com-
bination of the columns of the Jacobian of the constraints.

4.4 Optimization of Scalar-Valued Functions
211
4.4.5.5 Another Example: The Rayleigh Quotient
The important equation (3.265) on page 156 can also be derived by using
diﬀerentiation. This equation involves maximization of the Rayleigh quotient
(equation (3.266)),
xTAx/xTx
under the constraint that x ̸= 0. In this function, this constraint is equivalent
to the constraint that xTx equal a ﬁxed nonzero constant, which is canceled
in the numerator and denominator. We can arbitrarily require that xTx = 1,
and the problem is now to determine the maximum of xTAx subject to the
constraint xTx = 1. We now formulate the Lagrangian
xTAx −λ(xTx −1),
(4.56)
diﬀerentiate, and set it equal to 0, yielding
Ax −λx = 0.
This implies that a stationary point of the Lagrangian occurs at an eigenvector
and that the value of xTAx is an eigenvalue. This leads to the conclusion that
the maximum of the ratio is the maximum eigenvalue. We also see that the
second order necessary condition for a local maximum is satisﬁed; A −λI
is nonpositive deﬁnite when λ is the maximum eigenvalue. (We can see this
using the spectral decomposition of A and then subtracting λI.) Note that we
do not have the suﬃcient condition that A −λI is negative deﬁnite (A −λI
is obviously singular), but the fact that it is a maximum is established by
inspection of the ﬁnite set of stationary points.
4.4.5.6 Optimization of Functions with Inequality Constraints
Inequality constraints are much more diﬃcult to deal with than equality con-
straints. The feasible region, instead of being a manifold of dimension m −1
or less, may be a subset of with dimension m.
4.4.5.7 Inequality-Constrained Linear Least Squares Problems
Similarly to the equality-constrained least squares optimization prob-
lem (4.47), an inequality-constrained least squares optimization problem
can be written as
min
b
f(b) = (y −Xb)T(y −Xb)
s.t. Ab ≤c,
(4.57)
where A is a k×m matrix. There are no restrictions on k or on the rank of A as
before, except as required to make the feasible region convex. (If the feasible
region is not convex or it it is not connected, we may need to decompose

212
4 Vector/Matrix Derivatives and Integrals
the optimization problem into a set of optimization problems.) In the most
common cases the region is a convex cone.
The analysis leading to a reduced gradient and a reduced Hessian and
to the conditions given on page 209 that characterize a solution does not
apply. In an equality-constrained problem, all of the constraints are active,
in the sense that a feasible point can change only in limited ways and still
satisfy the constraint. Geometrically, the feasible region lies in a hyperplane.
(Of course, it may be restricted even further within the hyperplane.) In an
inequality-constrained problem, however, the feasible region deﬁned by Ab ≤c
consists of the intersection of halfspaces deﬁned by the rows of A. For the ith
row of A, ai, the halfspace consists of all b satisfying aT
i b ≤ci. The outward-
pointing normal to the hyperplane deﬁning the halfspace is ai. For any point
b(0) in the feasible region, a particular constraint, say aT
i b ≤ci, is “active” if
aT
i b(0) = ci; otherwise, that particular constraint is not active, and points in
a neighborhood of b(0) are also feasible.
If b∗is a solution to the constrained optimization problem (4.57), then the
gradient at b∗, XT(y −Xb∗), would be zero except for the active constraints.
The negative of the gradient, therefore can be represented as a linear com-
bination of the normals to the hyperplanes deﬁning the constraints that are
active. Now, deﬁne a k-vector e such that ei ≥0 if aT
i b∗= ci and ei = 0 if
aT
i b∗< ci. Let S (for “slack”) represent the set of all i such that ei = 0, and
let E (for “equal”) represent the set of all i such that ei ≥0. Then there are
values of ei as deﬁned above such that

i
eiai = XT(y −Xb∗).
(4.58)
Now consider a perturbation of the optimal b∗, δb∗. In order for b∗+ δb∗
to remain feasible, the perturbation must satisfy δbT
∗ai ≤0 for all i ∈E.
Multiplying both sides of equation (4.58) by δbT
∗, because ei ≥0, we have
δbT
∗XT(y −Xb∗) ≤0. Because
(y −X(b∗+ δb∗))T(y −X(b∗+ δb∗)) =
(y −Xb∗)T(y −Xb∗) −2δbT
∗XT(y −Xb∗) + (Xδb∗)TXδb∗,
no feasible solution will further decrease the value of the objective function
(y −Xb)T(y −Xb); that is, b∗is optimal.
We can summarize this discussion by the statement that b∗is a solution
for the problem (4.57) if and only if there exists a k-vector e such that ei ≥0
if aT
i b∗= ci and ei = 0 if aT
i b∗< ci, and
ATe = XT(y −Xb∗).
(4.59)
These conditions are special cases of the Kuhn-Tucker or Karush-Kuhn-Tucker
conditions for constrained optimization problems (see Griva, Nash, and Sofer,
2009, for example).
The variables in e are called dual variables. The non-zero dual variables
are slack variables.

4.4 Optimization of Scalar-Valued Functions
213
4.4.5.8 Nonlinear Least Squares as an Inequality-Constrained
Problem
The Levenberg-Marquardt adjustment in equation (4.40) can be thought of
as a Lagrangian multiplier formulation of the constrained problem:
min
x
1
2
Jr(θ(k))x + r(θ(k))

(4.60)
s.t.
S(k)x
 ≤δk.
See Exercise 4.8.
The Lagrange multiplier λ(k) is zero if p(k) from equation (4.39) satisﬁes
∥p(k)∥≤δk; otherwise, it is chosen so that
S(k)p(k) = δk.
4.4.6 Optimization Without Diﬀerentiation
Diﬀerentiation is often an eﬀective method for solving an optimization prob-
lem. It may lead us to a stationary point, and then in optimization problems,
we must establish that the stationary point is indeed a maximum or minimum,
which we do by use of second-order derivatives or else by inspection.
If the objective function is not diﬀerentiable, then of course diﬀerentiation
cannot be used. Also, even if the function is diﬀerentiable, if the optimum
occurs on the boundary of the domain of the objective function, then diﬀer-
entiation may not ﬁnd it. Before attempting to solve an optimization problem,
we should do some preliminary analysis of the problem. Often in working out
maximum likelihood estimates, for example, students immediately think of
diﬀerentiating, setting to 0, and solving. This requires that the likelihood
function be diﬀerentiable, that it be concave, and that the maximum occur at
an interior point of the parameter space. Keeping in mind exactly what the
problem is—one of ﬁnding a maximum—often leads to the correct solution
more quickly.
Sometimes in optimization problems, even in the simple cases of diﬀeren-
tiable objective functions, it may be easier to use other methods to determine
the optimum; that is, methods that do not depend on derivatives. For example,
the Rayleigh quotient for which we determined the maximum by diﬀerentia-
tion above, could be determined easily without diﬀerentiation, as we did on
page 156.
For another example, a constrained minimization problem we encounter
occasionally is
min
X≻0

log |X| + tr(X−1A)

(4.61)
for a given positive deﬁnite matrix A and subject to X being positive deﬁnite.
(The canonical problem giving rise to this minimization problem is the use of
likelihood methods with a multivariate normal distribution. Also recall that
an alternate notation for det(X) is |X|, which I am using here, as I often do
in working with the multivariate normal PDF, for example in equation (4.73)
on page 219.)

214
4 Vector/Matrix Derivatives and Integrals
The derivatives given in Tables 4.2 and 4.3 could be used to minimize
the function in (4.61). The derivatives set equal to 0 immediately yield X =
A. This means that X = A is a stationary point, but whether or not it is
a minimum would require further analysis. As is often the case with such
problems, an alternate approach leaves no such pesky complications. Let A
and X be n×n positive deﬁnite matrices, and let c1, . . . , cn be the eigenvalues
of X−1A. Now, by property 8 on page 136 these are also the eigenvalues of
X−1/2AX−1/2, which is positive deﬁnite (see inequality (3.161) on page 114).
Now, consider the expression (4.61) with general X minus the expression with
X = A:
log |X| + tr(X−1A) −log |A| −tr(A−1A) = log |XA−1| + tr(X−1A) −tr(I)
= −log |X−1A| + tr(X−1A) −n
= −log
$
i
ci

+

i
ci −n
=

i
(−log ci + ci −1)
≥0
because if c > 0, then log c ≤c −1, and the minimum occurs when each
ci = 1; that is, when X−1A = I. Thus, the minimum of expression (4.61)
occurs uniquely at X = A.
4.5 Integration and Expectation: Applications to
Probability Distributions
Just as we can take derivatives with respect to vectors or matrices, we can
also take antiderivatives or deﬁnite integrals with respect to vectors or ma-
trices. Our interest is in integration of functions weighted by a multivariate
probability density function, and for our purposes we will be interested only
in deﬁnite integrals.
Again, there are three components:
•
the diﬀerential (the variable of the operation) and its domain (the range
of the integration),
•
the integrand (the function), and
•
the result of the operation (the integral).
In the simplest case, all three of these objects are of the same type; they are
scalars. In the happy cases that we consider, each deﬁnite integral within the
nested sequence exists, so convergence and order of integration are not issues.
(The implication of these remarks is that while there is a much bigger ﬁeld
of mathematics here, we are concerned about the relatively simple cases that
suﬃce for our purposes.)

4.5 Integration and Expectation: Applications to Probability Distributions
215
In this section we ﬁrst consider some general issues relating to integra-
tion with respect to a vector, and then we consider applications to probabil-
ity, speciﬁcally, to the computation of moments of a vector-valued random
variable. The moments of a scalar-values random variable X are deﬁned as
E(Xk) (where the expectation operator, E(·), is an integral, as deﬁned in
equation (4.68)). Obviously the moments of vector-valued random variables
must be deﬁned slightly diﬀerently.
In some cases of interest involving vector-valued random variables, the
diﬀerential is the vector representing the values of the random variable and
the integrand has a scalar function (the probability density) as a factor. In one
type of such an integral, the integrand is only the probability density function,
and the integral evaluates to a probability, which of course is a scalar. In
another type of such an integral, the integrand is a vector representing the
values of the random variable times the probability density function. The
integral in this case evaluates to a vector, namely the expectation of the
random variable over the domain of the integration. Finally, in an example of
a third type of such an integral, the integrand is an outer product with itself
of a vector representing the values of the random variable minus its mean
times the probability density function. The integral in this case evaluates to
a variance-covariance matrix. In each of these cases, the integral is the same
type of object as the integrand.
4.5.1 Multidimensional Integrals and Integrals Involving
Vectors and Matrices
An integral of the form + f(v) dv, where v is a vector, can usually be evaluated
as a multiple integral with respect to each diﬀerential dvi. Likewise, an integral
of the form
+
f(M) dM, where M is a matrix can usually be evaluated by
“unstacking” the columns of dM, evaluating the integral as a multiple integral
with respect to each diﬀerential dmij, and then possibly “restacking” the
result.
Probabilities and expectations in multivariate probability distributions are
deﬁned in terms of multivariate integrals. As with many well-known univariate
integrals, such as Γ(·), that relate to univariate probability distributions, there
are standard multivariate integrals, such as the multivariate gamma, Γd(·),
that relate to multivariate probability distributions. Using standard integrals
often facilitates the computations.
4.5.1.1 Change of Variables: Jacobians
When evaluating an integral of the form
+
f(x) dx, where x is a vector, for
various reasons, we may form a one-to-one diﬀerentiable transformation of
the variables of integration; that is, of x. We write x as a function of the new
variables; that is, x = g(y), and so y = g−1(x). A simple fact from elementary
multivariable calculus is
,
R(x)
f(x) dx =
,
R(y)
f(g(y)) |det(Jg(y))|dy,
(4.62)

216
4 Vector/Matrix Derivatives and Integrals
where R(y) is the image of R(x) under g−1 and Jg(y) is the Jacobian of g (see
equation (4.12)). (This is essentially a chain rule result for dx = d(g(y)) =
Jgdy under the interpretation of dx and dy as positive diﬀerential elements and
the interpretation of |det(Jg)| as a volume element, as discussed on page 74.)
In the simple case of a full rank linear transformation of a vector, the
Jacobian is constant, and so for y = Ax with A a ﬁxed matrix, we have
,
f(x) dx = |det(A)|−1
,
f(A−1y) dy.
(4.63)
(Note that we write det(A) instead of |A| for the determinant if we are to
take the absolute value of it because otherwise we would have ||A||, which is
a symbol for a norm. However, |det(A)| is not a norm; it lacks each of the
properties listed on page 25.)
In the case of a full rank linear transformation of a matrix variable of
integration, the Jacobian is somewhat more complicated, but the Jacobian is
constant for a ﬁxed transformation matrix. For a transformation Y = AX,
we determine the Jacobian as above by considering the columns of X one by
one. Hence, if X is an n × m matrix and A is a constant nonsingular matrix,
we have
,
f(X) dX = |det(A)|−m
,
f(A−1Y ) dY.
(4.64)
For a transformation of the form Z = XB, we determine the Jacobian by
considering the rows of X one by one.
4.5.2 Integration Combined with Other Operations
Integration and another ﬁnite linear operator can generally be performed in
any order. For example, because the trace is a ﬁnite linear operator, integra-
tion and the trace can be performed in either order:
,
tr(A(x))dx = tr
,
A(x)dx

.
(4.65)
For a scalar function of two vectors x and y, it is often of interest to perform
diﬀerentiation with respect to one vector and integration with respect to the
other vector. In such cases, it is of interest to know when these operations
can be interchanged. The answer is given in the following theorem, which is
a consequence of the Lebesgue dominated convergence theorem. Its proof can
be found in any standard text on real analysis.
Let X be an open set, and let f(x, y) and ∂f/∂x be scalar-valued
functions that are continuous on X × Y for some set Y. Now suppose
there are scalar functions g0(y) and g1(y) such that
|f(x, y)| ≤g0(y)
∥∂
∂xf(x, y)∥≤g1(y)
⎫
⎬
⎭
for all (x, y) ∈X × Y,

4.5 Integration and Expectation: Applications to Probability Distributions
217
,
Y
g0(y) dy < ∞,
and
,
Y
g1(y) dy < ∞.
Then
∂
∂x
,
Y
f(x, y) dy =
,
Y
∂
∂xf(x, y) dy.
(4.66)
An important application of this interchange is in developing the information
inequality for “regular” families of probability distributions.
4.5.3 Random Variables and Probability Distributions
Note on notation: I generally use upper-case letters to represent matrices
and lower-case letters to represent vectors and scalars. I also generally use
upper-case letters to represent random variables and corresponding lower-case
letters to represent their realizations. I will generally follow this convention
in this section, but where the two conventions come in conﬂict, I will give
precedence to the upper-case representation; that is, an upper-case letter may
be a matrix, random or ﬁxed, or it may be a random variable with any struc-
ture. I also generally use Greek letters to represent parameters of probability
distributions, and for parameters, I also use my convention of upper-case for
matrices and lower-case for vectors or scalars.
Random variables are functions from a “sample space” into the reals. A
d-vector random variable is a function from some sample space into IRd, and
an n×d matrix random variable is a function from a sample space into IRn×d.
(Technically, in each case, the function is required to be measurable with re-
spect to a measure deﬁned in the context of the sample space and an appropri-
ate collection of subsets of the sample space. A careful development of the the-
ory is available at http://mason.gmu.edu/~jgentle/books/MathStat.pdf.)
Associated with each vector or matrix random variable is a distribution
function, or cumulative distribution function (CDF), which represents the
probability that the random variable takes values less than or equal to a
speciﬁed value. The derivative of the distribution function with respect to an
appropriate measure is nonnegative and integrates to 1 over the full space
formed by IR (that is, IR, IRd, or IRn×d). This derivative is called the proba-
bility density function, or PDF. (There are some technical issues that arise in
some cases, but I will ignore those issues here.)
If X is a random variable, and pX(x) is the PDF of X, we have
,
D(X)
pX(x) dx = 1,
(4.67)

218
4 Vector/Matrix Derivatives and Integrals
where D(X) is the set of the image of X in which pX(x) > 0 and is called the
support of the distribution.
The expected value of a function f of the random variable X is
E(f(X)) =
,
D(X)
f(x)pX(x) dx,
(4.68)
if the integral exists.
We see immediately that the expected value operator is linear; that is, if a
is independent of X and consists of real elements of the appropriate structure
that af(X) makes sense (and exists), and if g(X) has the structure such that
af(X) + g(X) makes sense (and exists), then
E (af(X) + g(X)) = aE(f(X)) + E(g(X)).
(4.69)
Some special forms of f in equation (4.68) yield the (raw) moments of the
random variable. In the case of a scalar random variable, these are just the
power functions, and E(Xk) is the kth moment of X.
4.5.3.1 Vector Random Variables
If the random variable X, and consequently, x in equation (4.68), is a vector,
then if we interpret
+
D(X) dx as a nest of univariate integrals, the result of the
integration of the vector f(x)pX(x) is clearly the same type of mathematical
object as f(x). For example, if f(x) = x, the expectation is the mean, which
is a vector.
The ﬁrst consideration, for a vector-valued random variable X, is how
should the second moment be represented. (This is not a question of what are
the individual elements that must comprise this moment; those are obvious.
It is merely a question of representation.) Kollo and von Rosen (2005), page
173, suggest three obvious ways. The most commonly used method, and the
only one I will consider here, is the expectation of the outer product,
E(XXT),
(4.70)
which of course is a matrix.
From (4.70) we get the variance-covariance matrix for the vector random
variable X,
V(X) = E

(X −E(X))(X −E(X))T
.
(4.71)
We also have, from the linearity of the expectation operator, if A is a ﬁxed
matrix not dependent on X, with a number of columns equal to the order of
X,
E(AX) = AE(X)
and
V(AX) = AV(X)AT.
(4.72)

4.5 Integration and Expectation: Applications to Probability Distributions
219
4.5.3.2 The Multivariate Normal Distribution
The most important vector random variables are those whose distributions are
in the multivariate normal family. In the case of the multivariate normal dis-
tribution, the variances and covariances together with the means completely
characterize the distribution. The PDF of the multivariate normal with mean
μ and variance-covariance matrix Σ is
pX(x) = (2π)−d/2|Σ|−1/2e−(x−μ)TΣ−1(x−μ)/2,
(4.73)
where |Σ| = det(Σ). (I prefer the notation “det(·)” for a determinant, but in
this section, I will mix the two notations.) In this notation, we are obviously
assuming that the variance-covariance matrix Σ is nonsingular, and we will
assume this anytime we refer to the “multivariate normal distribution”. There
is a related normal distribution, called a “singular normal distribution” or a
“curved normal distribution”, in which this is not the case (and |Σ|−1/2 does
not exist), but we will not discuss that family of distributions.
For a vector of order d, we often denote the family of multivariate normal
distributions by Nd(μ, Σ). We write
X ∼Nd(μ, Σ).
(4.74)
Each member of this family of distributions corresponds to a speciﬁc μ and
Σ.
As with any PDF, the function pX in (4.73) integrates to 1. The funda-
mental integral associated with the d-variate normal distribution, sometimes
called Aitken’s integral, is
,
IRd e−(x−μ)TΣ−1(x−μ)/2 dx.
(4.75)
The rank of the integral is the same as the rank of the integrand. (“Rank” is
used here in the sense of “number of dimensions”.) In this case, the integrand
and the integral are scalars.
We can evaluate the integral (4.75) by evaluating the individual single
integrals after making the change of variables yi = xi −μi. It can also be seen
by ﬁrst noting that because Σ−1 is positive deﬁnite, as in equation (3.272),
it can be written as P TΣ−1P = I for some nonsingular matrix P. Now, after
the translation y = x −μ, which leaves the integral unchanged, we make the
linear change of variables z = P −1y, with the associated Jacobian |det(P)|,
as in equation (4.62). From P TΣ−1P = I, we have |det(P)| = (det(Σ))1/2
because the determinant is positive. (Note that (det(Σ))1/2 is the same as
|Σ|−1/2 above.) Aitken’s integral therefore is
,
IRd e−yTΣ−1y/2 dy =
,
IRd e−(P z)TΣ−1P z/2 (det(Σ))1/2dz

220
4 Vector/Matrix Derivatives and Integrals
=
,
IRd e−zTz/2 dz (det(Σ))1/2
= (2π)d/2(det(Σ))1/2.
(4.76)
We stated that this normal distribution has mean μ and variance-
covariance matrix Σ. We can see this directly:
E(X) = (2π)−d/2|Σ|−1/2
,
IRd xe−(x−μ)TΣ−1(x−μ)/2 dx
= μ;
and for the variance we have
V(X) = E

(X −E(X))(X −E(X))T
= (2π)−d/2|Σ|−1/2
,
IRd

(x −μ)(x −μ)T
e−(x−μ)TΣ−1(x−μ)/2 dx
= Σ.
4.5.3.3 Matrix Random Variables
The relationships of the elements of a matrix-valued random variable to each
other may be useful in modeling interrelated populations. In some cases, a ma-
trix is the obvious structure for a random variable, such as a Wishart matrix
for modeling the distribution of a sample variance-covariance matrix. Kollo
and von Rosen (2005) take a matrix as the basic structure for a multivari-
ate random variable, in particular, for a random variable with a multivariate
normal distribution.
A matrix random variable is often useful in certain applications, such as
when p n-variate random variables may be related. The columns of an n × p
matrix-valued random variable X may correspond to these n-variate random
variables. In applications, it may be reasonable to allow the columns to have
diﬀerent means, but to assume that they have a common variance-covariance
matrix Σ. The mean of X is the matrix M, whose columns, μ1, . . . , μp, are
the means of the columns of X.
The probability distributions governing random matrices determine vari-
ous properties of the matrices, such as whether or not the matrices are positive
deﬁnite, or the probability that the matrices are positive deﬁnite, for example.
One fact that is fairly obvious is that if the individual elements of a matrix
have continuous probability distributions, both marginal and all nondegener-
ate conditional distributions, then the probability that the matrix is full rank
is 1. (You are asked to write a proof of this in Exercise 4.11.)
The matrix normal distribution can be deﬁned from a set of s ran-
dom r-vectors Y1, . . . , Ys that are independently and identically distributed
as Nr(0, Ir). For a ﬁxed n × r matrix A and ﬁxed n-vector μi, the vector
Yi = μi + AYi is distributed as Nn(μi, AAT). Now consider the s-vector Yj∗

4.5 Integration and Expectation: Applications to Probability Distributions
221
consisting of the jth element of each Yi. It is clear that Yj∗∼Nr(0, Is), and
hence for a given d×s matrix B, BYj∗∼Nd(0, BBT). Now, forming a matrix
Y with the Yis as the columns and a matrix M with the μis as the columns,
and putting these two transformations together, we have a matrix normal
random variable:
X = M + AY BT.
(4.77)
Let Σ = AAT and Ψ = BBT denote the variance-covariance matrices in the
multivariate distributions above. These are parameters in the distribution of
X. We denote the distribution of the random matrix X with three parameters
as
X ∼Nn,d(M, Σ, Ψ).
(4.78)
We may also wish to require that Σ and Ψ be positive deﬁnite. This require-
ment would be satisﬁed if A and B are each of full row rank.
There are of course various ways that relationships among the individual
multivariate random variables (that is, the columns of X) could be modeled.
A useful model is a multivariate normal distribution for vec(X) as in expres-
sion (4.74), instead of the matrix normal distribution for X as above:
vec(X) ∼Nnd (vec(M), Ψ ⊗Σ) .
(4.79)
This distribution is easy to work with because it is just an ordinary multivari-
ate (vector) normal distribution. The variance-covariance matrix has a special
structure, called a Kronecker structure, that incorporates the homoscedastic-
ity of the columns as well as their interdependence.
An interesting and useful matrix distribution is that of a random variance-
covariance matrix formed from n independent d-variate normal random vari-
ables distributed as Nd(0, Σ). This is the Wishart distribution, see Exer-
cise 4.12.
A common matrix integral that arises in the Wishart and related distribu-
tions is the complete d-variate gamma function, denoted by Γd(x) and deﬁned
as
Γd(x) =
,
D
e−tr(A)|A|x−(d+1)/2 dA,
(4.80)
where D is the set of all d × d positive deﬁnite matrices, A ∈D, and
x > (d −1)/2. A multivariate gamma distribution can be deﬁned in terms
of the integrand. (There are diﬀerent deﬁnitions of a multivariate gamma dis-
tribution.) The multivariate gamma function also appears in the probability
density function for a Wishart random variable (see Muirhead 1982, Carmeli
1983, or Kollo and von Rosen 2005, for example).
Another distribution for random matrices is one in which the individual el-
ements have identical and independent normal distributions. This distribution
of matrices was named the BMvN distribution by Birkhoﬀand Gulati (1979)
(from the last names of three mathematicians who used such random matrices
in numerical studies). Birkhoﬀand Gulati (1979) showed that if the elements

222
4 Vector/Matrix Derivatives and Integrals
of the n×n matrix X are i.i.d. N(0, σ2), and if Q is an orthogonal matrix and
R is an upper triangular matrix with positive elements on the diagonal such
that QR = X, then Q has the Haar distribution. (The factorization X = QR
is called the QR decomposition and is discussed on page 251. If X is a random
matrix as described, this factorization exists with probability 1.) The Haar(n)
distribution is uniform over the space of n × n orthogonal matrices.
The measure
μ(D) =
,
D
HT dH,
(4.81)
where D is a subset of the orthogonal group O(n) (see page 133), is called the
Haar measure. This measure is used to deﬁne a kind of “uniform” probability
distribution for orthogonal factors of random matrices. For any Q ∈O(n),
let QD represent the subset of O(n) consisting of the matrices ˜H = QH for
H ∈D and DQ represent the subset of matrices formed as HQ. From the
integral, we see that
μ(QD) = μ(DQ) = μ(D),
so the Haar measure is invariant to multiplication within the group. The mea-
sure is therefore also called the Haar invariant measure over the orthogonal
group. (See Muirhead 1982, for more properties of this measure.)
Exercises
4.1. Use equation (4.6), which deﬁnes the derivative of a matrix with respect
to a scalar, to show the product rule equation (4.3) directly:
∂Y W
∂x
= ∂Y
∂x W + Y ∂W
∂x .
4.2. For the n-vector x, compute the gradient gV(x), where V(x) is the vari-
ance of x, as given in equation (2.76).
Hint: Use the chain rule.
4.3. Prove ∇× (∇f) = 0.
(This is equation (4.13) expressed in diﬀerent notation).
4.4. Prove ∇· (∇× f) = 0.
(This is equation (4.14) expressed in diﬀerent notation).
4.5. For the square, nonsingular matrix Y , derive equation (4.15):
∂Y −1
∂x
= −Y −1 ∂Y
∂x Y −1.
Hint: Diﬀerentiate Y Y −1 = I.
4.6. Newton’s method.
You should not, of course, just blindly pick a starting point and begin
iterating. How can you be sure that your solution is a local optimum?

Exercises
223
Can you be sure that your solution is a global optimum? It is often a
good idea to make some plots of the function. In the case of a function
of a single variable, you may want to make plots in diﬀerent scales. For
functions of more than one variable, proﬁle plots may be useful (that
is, plots of the function in one variable with all the other variables held
constant).
a) Use Newton’s method to determine the maximum of the function
f(x) = sin(4x) −x4/12.
b) Use Newton’s method to determine the minimum of
f(x1, x2) = 2x4
1 + 3x3
1 + 2x2
1 + x2
2 −4x1x2.
What is the Hessian at the minimum?
4.7. Consider the log-likelihood l(μ, Σ; y) for the d-variate normal distribu-
tion, equation (4.43). Be aware of the subtle issue referred to in the text.
It has to do with whether n
i=1(yi −¯y)(yi −¯y)T is positive deﬁnite.
a) Replace the parameters μ and Σ by the variables ˆμ and *Σ, take
derivatives with respect to ˆμ and *Σ, set them equal to 0, and solve
for the maximum likelihood estimates. What assumptions do you
have to make about n and d?
b) Another approach to maximizing the expression in equation (4.43)
is to maximize the last term with respect to ˆμ (this is the only term
involving μ) and then, with the maximizing value substituted, to
maximize
−n
2 log |Σ| −1
2tr

Σ−1
n

i=1
(yi −¯y)(yi −¯y)T

.
Use this approach to determine the maximum likelihood estimates
ˆμ and *Σ.
4.8. Compare
the
constrained
optimization
problem
(4.60)
with
the
Levenberg-Marquardt regularization in equation (4.40). Carefully iden-
tify the corresponding terms in the two diﬀerent expressions.
4.9. Let
D =
#!
c −s
s
c
"
: −1 ≤c ≤1, c2 + s2 = 1
0
.
Evaluate the Haar measure μ(D). (This is the class of 2 × 2 rotation
matrices; see equation (5.3), page 232.)
4.10. Write a program (use R, Matlab, Python, Fortran, C) to generate n× n
random orthogonal matrices with the Haar uniform distribution. Use
the following method due to Heiberger (1978), which was modiﬁed by
Stewart (1980). (See also Tanner and Thisted 1982.)
a) Generate n −1 independent i-vectors, x2, x3, . . . , xn, from Ni(0, Ii).
(xi is of length i.)

224
4 Vector/Matrix Derivatives and Integrals
b) Let ri = ∥xi∥2, and let Hi be the i × i reﬂection matrix that trans-
forms xi into the i-vector (ri, 0, 0, . . ., 0).
c) Let Hi be the n × n matrix
!In−i 0
0
Hi
"
,
and form the diagonal matrix,
J = diag

(−1)b1, (−1)b2, . . . , (−1)bn
,
where the bi are independent realizations of a Bernoulli random
variable.
d) Deliver the orthogonal matrix Q = JH1H2 · · · Hn.
The matrix Q generated in this way is orthogonal and has a Haar dis-
tribution.
Can you think of any way to test the goodness-of-ﬁt of samples from this
algorithm? Generate a sample of 1,000 2×2 random orthogonal matrices,
and assess how well the sample follows a Haar uniform distribution.
4.11. Let A be an n × m random matrix whose elements all have continu-
ous marginal distributions. (By “continuous distribution” we mean that
any set of zero Lebesgue measure has zero probability.) In addition, as-
sume that all nondegenerate conditional distributions of all elements are
continuous (that is, assume that the joint distribution is nonsingular).
Show that Pr(A is full rank) = 1.
Hint: Let r = min(n, m). If r = 1 it is proven. Otherwise, without loss
of generality, assume r = m, and show that for any two ai∗and aj∗and
any ﬁxed ci, cj ̸= 0, Pr(ciai∗+ cjaj∗= 0) = 0. Then extend this to all
sets of columns.
4.12. The Wishart distribution.
The Wishart distribution is a useful distribution of random matrices of
sums of squares and cross products. It can be thought of as a multivari-
ate generalization of the chi-squared distribution.
There are various ways of deﬁning the Wishart distribution, each re-
sulting in the same distribution. A natural way to deﬁne it is in terms
of transformations of a multivariate normal distribution. Let
X1, . . . , Xn
iid
∼Nd(μ, Σ),
and let X = [X1 . . . Xn]. Now, let W = XXT.
First, assume μ = 0. Then W has a central Wishart distribution with
parameters d, n, where d < n, and Σ. We denote this as
W ∼Wd(Σ, n),
and we often just refer to it as the Wishart distribution. (There are
related distributions in which d ≥n, which we call “pseudo-Wishart”
distributions.)

Exercises
225
In the general case, we denote the distribution as
W ∼Wd(Σ, n, Δ),
where Δ = μμT. If Δ ̸= 0; that is, if μ ̸= 0, we call the distribution the
noncentral Wishart distribution.
The importance of the Wishart distribution is that the Wishart matrix
can be used to make inferences on the variance-covariance matrix Σ of
an underlying multivariate normal distribution.
a) Show that the probability density function for the (central) Wishart
distribution is proportional to
e−tr(Σ−1W/2)|W|(n−d−1)/2,
and determine the constant of proportionality.
b) Let W1 ∼Wd(Σ, n, Δ1) and let W2 ∼Wd(Σ, m, Δ2) and be inde-
pendent of W1. Show that
W1 + W2 ∼Wd(Σ, n + m, Δ1 + Δ2).
Hint: Using the deﬁnition of the Wishart distribution, you can as-
sume that there are n + m independent d-variate normal random
variables.
c) Let W ∼Wd(Σ, n, Δ) and let A be a ﬁxed q × d matrix. Show that
AWAT ∼Wq(AΣAT, n, AΔAT).
d) Let W ∼Wd(σ2I, n, Δ). Show that
1
σ2 Wii ∼χ2
n(Δii).
(The PDF of the noncentral chi-squared distribution is given in
equation (9.3) on page 402.)

5
Matrix Transformations and Factorizations
In most applications of linear algebra, problems are solved by transforma-
tions of matrices. A given matrix (which represents some transformation of
a vector) is itself transformed. The simplest example of this is in solving the
linear system Ax = b, where the matrix A represents a transformation of the
vector x to the vector b. The matrix A is transformed through a succession of
linear operations until x is determined easily by the transformed A and the
transformed b. Each operation in the transformation of A is a pre- or post-
multiplication by some other matrix. Each matrix formed as a product must
be equivalent to A; therefore, in order to ensure this in general, each trans-
formation matrix must be of full rank. In eigenproblems, we likewise perform
a sequence of pre- or postmultiplications. In this case, each matrix formed as
a product must be similar to A; therefore each transformation matrix must
be orthogonal. We develop transformations of matrices by transformations on
the individual rows or columns.
5.1 Factorizations
Given a matrix A, it is often useful to decompose A into the product of other
matrices; that is, to form a factorization A = BC, where B and C are ma-
trices. We refer to this as “matrix factorization”, or sometimes as “matrix
decomposition”, although this latter term includes more general representa-
tions of the matrix, such as the spectral decomposition (page 154).
Most methods for eigenanalysis and for solving linear systems proceed by
factoring the matrix, as we see in Chaps. 6 and 7.
In Chap. 3, we discussed some factorizations including
•
the full rank factorization (equation (3.150)) of a general matrix,
•
the equivalent canonical factorization (equation (3.155)) of a general ma-
trix,
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 5
227

228
5 Transformations and Factorizations
•
the Schur factorization (equation (3.244)) of a square matrix,
•
the similar canonical factorization (equation (3.247)) or “diagonal factor-
ization” of a diagonalizable matrix (which is necessarily square),
•
the orthogonally similar canonical factorization (equation (3.252)) of a
symmetric matrix (which is necessarily diagonalizable),
•
the square root (equation (3.273)) of a nonnegative deﬁnite matrix (which
is necessarily symmetric), and
•
the singular value factorization (equation (3.276)) of a general matrix.
In this chapter we describe three additional factorizations:
•
the LU (and LR and LDU) factorization of a general matrix,
•
the QR factorization of a general matrix, and
•
the Cholesky factorization of a nonnegative deﬁnite matrix.
These factorizations are useful both in theory and in practice.
5.2 Computational Methods: Direct and Iterative
In our previous discussions of matrix factorizations and other operations, we
have shown derivations that may indicate computational methods, but we
have not speciﬁed the computational details. There are many important com-
putational issues, some of which we will discuss in Part III. At this point, again
without getting into the details, we want to note a fundamental diﬀerence in
the types of computational methods.
The developments of the full rank factorization (equation (3.150)) and the
equivalent canonical factorization (equation (3.155)) were constructive, and
indeed those factorizations could be computed following those constructions.
On the other hand, our developments of the diagonalizing transformations,
such as the orthogonally similar canonical factorization (equation (3.252)),
and other factorizations related to eigenvalues, such as the singular value
factorization (equation (3.276)), were not constructive.
As it turns out, the factorizations involving eigenvalues or singular values
cannot in general be computed using a ﬁnite set of arithmetic operations. If
they could be, then the characteristic polynomial equation could be solved
in a ﬁnite set of arithmetic operations, and the Abel-Ruﬃni theorem states
that such a solution does not exist for polynomials of degree ﬁve or higher.
For factorizations of this type we must use iterative methods, at least to get
the eigenvalues or singular values. We will describe some of those methods
in Chap. 7. (In this chapter I do one factorization based on an eigendecom-
position. It is the square root, described in Sect. 5.9.1. This is because it so
naturally goes with a Cholesky factorization of a nonnegative deﬁnite matrix.)
In this chapter we ﬁrst discuss some transformations and important fac-
torizations that can be carried out in a ﬁnite number of arithmetic steps; that
is, by the use of direct methods. The factorizations themselves can be used it-
eratively; indeed, as we will discuss in Chap. 7, the QR is the most important
factorization used iteratively to obtain eigenvalues or singular values.

5.3 Linear Geometric Transformations
229
A factorization of a given matrix A is generally eﬀected by a series of
pre- or postmultiplications by transformation matrices with simple and desir-
able properties. One such transformation matrix is the Gaussian matrix, Gij
of equation (3.63) or Gij of equation (3.65) on page 84. Another important
class of transformation matrices are orthogonal matrices. Orthogonal trans-
formation matrices have some desirable properties. In this chapter, before
discussing factorizations, we ﬁrst consider some general properties of various
types of transformations, and then we describe two speciﬁc types of orthogo-
nal transformations, Householder reﬂections (Sect. 5.4) and Givens rotations
(Sect. 5.5). As we will see, the Householder reﬂections are very similar to the
Gram-Schmidt transformations that we discussed beginning on page 38.
5.3 Linear Geometric Transformations
In many important applications of linear algebra, a vector represents a point
in space, with each element of the vector corresponding to an element of a
coordinate system, usually a Cartesian system. A set of vectors describes a
geometric object, such as a polyhedron or a Lorentz cone, as on page 44. Alge-
braic operations can be thought of as geometric transformations that rotate,
deform, or translate the object. While these transformations are often used in
the two or three dimensions that correspond to the easily perceived physical
space, they have similar applications in higher dimensions. Thinking about
operations in linear algebra in terms of the associated geometric operations
often provides useful intuition.
A linear transformation of a vector x is eﬀected by multiplication by a
matrix A. Any n × m matrix A is a function or transformation from V1 to V2,
where V1 is a vector space of order m and V2 is a vector space of order n.
5.3.1 Invariance Properties of Linear Transformations
An important characteristic of a transformation is what it leaves unchanged;
that is, its invariance properties (see Table 5.1). All of the transformations we
will discuss are linear transformations because they preserve straight lines. A
set of points that constitute a straight line is transformed into a set of points
that constitute a straight line.
As mentioned above, reﬂections and rotations are orthogonal transforma-
tions, and we have seen that an orthogonal transformation preserves lengths
of vectors (equation (3.286)). We will also see that an orthogonal transfor-
mation preserves angles between vectors (equation (5.1)). A transformation
that preserves lengths and angles is called an isometric transformation. Such
a transformation also preserves areas and volumes.
Another isometric transformation is a translation, which is essentially the
addition of another vector (see Sect. 5.3.5).

230
5 Transformations and Factorizations
Table 5.1. Invariance properties of transformations
Transformation
Preserves
Linear
Lines
Projective
Lines
Aﬃne
Lines, Collinearity
Shearing
Lines, Collinearity
Scaling
Lines, Angles (and, hence, Collinearity)
Rotation
Lines, Angles, Lengths
Reﬂection
Lines, Angles, Lengths
Translation
Lines, Angles, Lengths
A transformation that preserves angles is called an isotropic transforma-
tion. An example of an isotropic transformation that is not isometric is a
uniform scaling or dilation transformation, ˜x = ax, where a is a scalar.
The transformation ˜x = Ax, where A is a diagonal matrix with not all ele-
ments the same, does not preserve angles; it is an anisotropic scaling. Another
anisotropic transformation is a shearing transformation, ˜x = Ax, where A is
the same as an identity matrix, except for a single row or column that has
a one on the diagonal but nonzero, possibly constant, elements in the other
positions; for example,
⎡
⎣
1 0 a1
0 1 a1
0 0 1
⎤
⎦.
Although they do not preserve angles, both anisotropic scaling and shear-
ing transformations preserve parallel lines. A transformation that preserves
parallel lines is called an aﬃne transformation. Preservation of parallel lines
is equivalent to preservation of collinearity, and so an alternative character-
ization of an aﬃne transformation is one that preserves collinearity. More
generally, we can combine nontrivial scaling and shearing transformations to
see that the transformation Ax for any nonsingular matrix A is aﬃne. It is
easy to see that addition of a constant vector to all vectors in a set pre-
serves collinearity within the set, so a more general aﬃne transformation is
˜x = Ax + t for a nonsingular matrix A and a vector t.
A projective transformation, which uses the homogeneous coordinate sys-
tem of the projective plane (see Sect. 5.3.5), preserves straight lines, but does
not preserve parallel lines. Projective transformations are very useful in com-
puter graphics. In those applications we do not always want parallel lines to
project onto the display plane as parallel lines.
5.3.2 Transformations by Orthogonal Matrices
We deﬁned orthogonal matrices and considered some basic properties on
page 132. Orthogonal matrices are not necessarily square; they may have

5.3 Linear Geometric Transformations
231
more rows than columns or may have fewer. In the following, we will consider
only orthogonal matrices with at least as many rows as columns; that is, if Q
is an orthogonal transformation matrix, then QTQ = I. This means that an
orthogonal matrix is of full rank. Of course, many useful orthogonal matrices
are square (and, obviously, nonsingular). There are many types of orthogo-
nal transformation matrices. As noted previously, permutation matrices are
square orthogonal matrices, and we have used them extensively in rearranging
the columns and/or rows of matrices.
As we stated, transformations by orthogonal matrices preserve lengths of
vectors. Orthogonal transformations also preserve angles between vectors, as
we can easily see. If Q is an orthogonal matrix, then, for vectors x and y, we
have
⟨Qx, Qy⟩= (Qx)T(Qy) = xTQTQy = xTy = ⟨x, y⟩,
and hence,
arccos

⟨Qx, Qy⟩
∥Qx∥2 ∥Qy∥2

= arccos

⟨x, y⟩
∥x∥2 ∥y∥2

.
(5.1)
Thus, orthogonal transformations preserve angles.
We have seen that if Q is an orthogonal matrix and
B = QTAQ,
then A and B have the same eigenvalues (and A and B are said to be or-
thogonally similar). By forming the transpose, we see immediately that the
transformation QTAQ preserves symmetry; that is, if A is symmetric, then B
is symmetric.
From equation (3.287), we see that ∥Q−1∥2 = 1. This has important im-
plications for the accuracy of numerical computations. (Using computations
with orthogonal matrices will not make problems more “ill-conditioned”.)
We often use orthogonal transformations that preserve lengths and an-
gles while rotating IRn or reﬂecting regions of IRn. The transformations are
appropriately called rotators and reﬂectors, respectively.
5.3.3 Rotations
The simplest rotation of a vector can be thought of as the rotation of a plane
deﬁned by two coordinates about the other principal axes. Such a rotation
changes two elements of all vectors in that plane and leaves all the other
elements, representing the other coordinates, unchanged. This rotation can
be described in a two-dimensional space deﬁned by the coordinates being
changed, without reference to the other coordinates.
Consider the rotation of the vector x through the angle θ into the vector
˜x. The length is preserved, so we have ∥˜x∥= ∥x∥. Referring to Fig. 5.1, we
can write

232
5 Transformations and Factorizations
f
q
x
x~
x1
x2
Figure 5.1. Rotation of x
˜x1 = ∥x∥cos(φ + θ),
˜x2 = ∥x∥sin(φ + θ).
Now, from elementary trigonometry, we know
cos(φ + θ) = cos φ cos θ −sin φ sin θ,
sin(φ + θ) = sin φ cos θ + cos φ sin θ.
Because cos φ = x1/∥x∥and sin φ = x2/∥x∥, we can combine these equations
to get
˜x1 = x1 cos θ −x2 sin θ,
˜x2 = x1 sin θ + x2 cos θ.
(5.2)
Hence, multiplying x by the orthogonal matrix
!cos θ −sin θ
sin θ
cos θ
"
(5.3)
performs the rotation of x.
This idea easily extends to the rotation of a plane formed by two coordi-
nates about all of the other (orthogonal) principal axes. By convention, we
assume clockwise rotations for axes that increase in the direction from which
the system is viewed. For example, if there were an x3 axis in Fig. 5.1, it
would point toward the viewer. (This is called a “right-hand” coordinate sys-
tem, because if the viewer’s right-hand ﬁngers point in the direction of the
rotation, the thumb points toward the viewer.)

5.3 Linear Geometric Transformations
233
The rotation matrix about principal axes is the same as an identity ma-
trix with two diagonal elements changed to cos θ and the corresponding oﬀ-
diagonal elements changed to sin θ and −sin θ.
To rotate a 3-vector, x, about the x2 axis in a right-hand coordinate sys-
tem, we would use the rotation matrix
⎡
⎣
cos θ
0
sin θ
0
1
0
−sin θ
0
cos θ
⎤
⎦.
A rotation of any hyperplane in n-space can be formed by n successive
rotations of hyperplanes formed by two principal axes. (In 3-space, this fact is
known as Euler’s rotation theorem. We can see this to be the case, in 3-space
or in general, by construction.)
A rotation of an arbitrary plane can be deﬁned in terms of the direction
cosines of a vector in the plane before and after the rotation. In a coordinate
geometry, rotation of a plane can be viewed equivalently as a rotation of the
coordinate system in the opposite direction. This is accomplished by rotating
the unit vectors ei into ˜ei.
A special type of transformation that rotates a vector to be perpendicular
to a principal axis is called a Givens rotation. We discuss the use of this type
of transformation in Sect. 5.5 on page 238. Another special rotation is the
“reﬂection” of a vector about another vector. We discuss this kind of rotation
next.
5.3.4 Reﬂections
Let u and v be orthonormal vectors, and let x be a vector in the space spanned
by u and v, so
x = c1u + c2v
for some scalars c1 and c2. The vector
˜x = −c1u + c2v
(5.4)
is a reﬂection of x through the line deﬁned by the vector v, or u⊥. This
reﬂection is a rotation in the plane deﬁned by u and v through an angle of
twice the size of the angle between x and v.
The form of ˜x of course depends on the vector v and its relationship to x.
In a common application of reﬂections in linear algebraic computations, we
wish to rotate a given vector into a vector collinear with a coordinate axis;
that is, we seek a reﬂection that transforms a vector
x = (x1, x2, . . . , xn)

234
5 Transformations and Factorizations
into a vector collinear with a unit vector,
˜x = (0, . . . , 0, ˜xi, 0, . . . , 0)
= ±∥x∥2ei.
(5.5)
Geometrically, in two dimensions we have the picture shown in Fig. 5.2,
where i = 1. Which vector that x is rotated through (that is, which is u and
which is v) depends on the choice of the sign in ±∥x∥2. The choice that was
made yields the ˜x shown in the ﬁgure, and from the ﬁgure, this can be seen
to be correct. Note that
v =
1
|2c2|(x + ˜x)
If the opposite choice is made, we get the ˜˜x shown. In the simple two-
dimensional case, this is equivalent to reversing our choice of u and v.
x2
x1
x
u
v
~~x
~x
Figure 5.2. Reﬂections of x about v (or u⊥) and about u
To accomplish this special rotation of course, we ﬁrst choose an appropriate
vector about which to reﬂect our given vector, and then perform the rotation.
We will describe this process in Sect. 5.4 below.
5.3.5 Translations: Homogeneous Coordinates
A translation of a vector is a relatively simple transformation in which the
vector is transformed into a parallel vector. It involves a type of addition of
vectors. Rotations, as we have seen, and other geometric transformations such
as shearing, as we have indicated, involve multiplication by an appropriate
matrix. In applications where several geometric transformations are to be
made, it would be convenient if translations could also be performed by matrix
multiplication. This can be done by using homogeneous coordinates.
Homogeneous coordinates, which form the natural coordinate system for
projective geometry, have a very simple relationship to Cartesian coordinates.

5.4 Householder Transformations (Reﬂections)
235
The point with Cartesian coordinates (x1, x2, . . . , xd) is represented in homo-
geneous coordinates as (xh
0, xh
1, . . . , xh
d), where, for arbitrary xh
0 not equal to
zero, xh
1 = xh
0x1, and so on. Because the point is the same, the two diﬀerent
symbols represent the same thing, and we have
(x1, . . . , xd) = (xh
0, xh
1, . . . , xh
d).
(5.6a)
Alternatively, the hyperplane coordinate may be added at the end, and we
have
(x1, . . . , xd) = (xh
1, . . . , xh
d, xh
0).
(5.6b)
Each value of xh
0 corresponds to a hyperplane in the ordinary Cartesian co-
ordinate system. The most common choice is xh
0 = 1, and so xh
i = xi. The
special plane xh
0 = 0 does not have a meaning in the Cartesian system, but in
projective geometry it corresponds to a hyperplane at inﬁnity.
We can easily eﬀect the translation ˜x = x + t by ﬁrst representing the
point x as (1, x1, . . . , xd) and then multiplying by the (d + 1) × (d + 1) matrix
T =
⎡
⎢⎢⎣
1 0 · · · 0
t1 1 · · · 0
· · ·
td 0 · · · 1
⎤
⎥⎥⎦.
We will use the symbol xh to represent the vector of corresponding homoge-
neous coordinates:
xh = (1, x1, . . . , xd).
We must be careful to distinguish the point x from the vector that represents
the point. In Cartesian coordinates, there is a natural correspondence and
the symbol x representing a point may also represent the vector (x1, . . . , xd).
The vector of homogeneous coordinates of the result T xh corresponds to the
Cartesian coordinates of ˜x, (x1 + t1, . . . , xd + td), which is the desired result.
Homogeneous coordinates are used extensively in computer graphics not
only for the ordinary geometric transformations but also for projective trans-
formations, which model visual properties. Hill and Kelley (2006) describe
many of these applications. See Exercise 5.2 for a simple example.
5.4 Householder Transformations (Reﬂections)
We have brieﬂy discussed geometric transformations that reﬂect a vector
through another vector. We now consider some properties and uses of these
transformations.
Consider the problem of reﬂecting x through the vector v. As before, we
assume that u and v are orthonormal vectors and that x lies in a space spanned
by u and v, and x = c1u + c2v. Form the matrix

236
5 Transformations and Factorizations
H = I −2uuT,
(5.7)
and note that
Hx = c1u + c2v −2c1uuTu −2c2uuTv
= c1u + c2v −2c1uTuu −2c2uTvu
= −c1u + c2v
= ˜x,
as in equation (5.4). The matrix H is a reﬂector; it has transformed x into its
reﬂection ˜x about v.
A reﬂection is also called a Householder reﬂection or a Householder trans-
formation, and the matrix H is called a Householder matrix or a Householder
reﬂector. The following properties of H are immediate:
•
Hu = −u.
•
Hv = v for any v orthogonal to u.
•
H = HT (symmetric).
•
HT = H−1 (orthogonal).
Because H is orthogonal, if Hx = ˜x, then ∥x∥2 = ∥˜x∥2 (see equation (3.286)),
so ˜x1 = ±∥x∥2.
The matrix uuT is symmetric, idempotent, and of rank 1. A transformation
by a matrix of the form A−vwT is often called a “rank-one” update, because
vwT is of rank 1. Thus, a Householder reﬂection is a special rank-one update.
5.4.1 Zeroing All Elements But One in a Vector
The usefulness of Householder reﬂections results from the fact that it is easy
to construct a reﬂection that will transform a vector x into a vector ˜x that has
zeros in all but one position, as in equation (5.5). To construct the reﬂector
of x into ˜x, we ﬁrst need to determine a vector v as in Fig. 5.2 about which
to reﬂect x. That vector is merely
x + ˜x.
Because ∥˜x∥2 = ∥x∥2, we know ˜x to within the sign; that is,
˜x = (0, . . . , 0, ±∥x∥2, 0, . . . , 0).
We choose the sign so as not to add quantities of diﬀerent signs and possibly
similar magnitudes. (See the discussions of catastrophic cancellation below
and beginning on page 488, in Chap. 10.) Hence, we have
q = (x1, . . . , xi−1, xi + sign(xi)∥x∥2, xi+1, . . . , xn).
(5.8)
We normalize this to obtain
u = q/∥q∥2,
(5.9)

5.4 Householder Transformations (Reﬂections)
237
and ﬁnally form
H = I −2uuT.
(5.10)
Consider, for example, the vector
x = (3, 1, 2, 1, 1),
which we wish to transform into
˜x = (˜x1, 0, 0, 0, 0).
We have
∥x∥= 4,
so we form the vector
u =
1
√
56
(7, 1, 2, 1, 1)
and the Householder reﬂector
H = I −2uuT
=
⎡
⎢⎢⎢⎢⎣
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
⎤
⎥⎥⎥⎥⎦
−1
28
⎡
⎢⎢⎢⎢⎣
49 7 14 7 7
7 1
2 1 1
14 2
4 2 2
7 1
2 1 1
7 1
2 1 1
⎤
⎥⎥⎥⎥⎦
= 1
28
⎡
⎢⎢⎢⎢⎣
−21 −7 −14 −7 −7
−7 27
−2 −1 −1
−14 −2
24 −2 −2
−7 −1
−2 27 −1
−7 −1
−2 −1 27
⎤
⎥⎥⎥⎥⎦
to yield Hx = (−4, 0, 0, 0, 0).
The procedure described by equations (5.8), (5.9), and (5.10), zeroes out all
but the ith element of the given vector. We could of course modify the reﬂector
matrix so that certain elements of the reﬂected vector are unchanged.
We will consider these reﬂections further in Sect. 5.8.8, beginning on
page 252.
5.4.2 Computational Considerations
Notice that if we had chosen ˜x as (−4, 0, 0, 0, 0), then u would have been
(−1, 1, 2, 1, 1)/
√
8, and Hx would have been (4, 0, 0, 0, 0), and our objective
would also have been achieved. In this case, there would have been no nu-
merical rounding problems. If, however, x were such that x1 ≈−∥x∥2 in

238
5 Transformations and Factorizations
the addition x1 + ∥x∥2, “catastrophic cancellation” would occur. For exam-
ple, if x1 = −3, and ∥x∥is computed as 3.0000002, the computation of
x1 + ∥x∥2 would lose seven signiﬁcant digits. Of course, it can be the case
that x1 ≈−∥x∥2, only if x2 ≈x3 ≈· · · ≈0. Nevertheless, we should perform
computations in such a way as to protect against the worst cases, especially
if it is easy to do so.
Standard Householder computations are performed generally as indicated
above, but there may be minor variations in the order of performing the com-
putations that take advantage of speciﬁc computer architectures. There are
variants of the Householder transformations that are more eﬃcient by taking
advantage of such architectures as a cache memory or a bank of ﬂoating-point
registers whose contents are immediately available to the computational unit.
5.5 Givens Transformations (Rotations)
We have brieﬂy discussed geometric transformations that rotate a vector in
such a way that a speciﬁed element becomes 0 and only one other element in
the vector is changed. Such a method may be particularly useful if only part
of the matrix to be transformed is available. These transformations are called
Givens transformations, or Givens rotations, or sometimes Jacobi transforma-
tions.
The basic idea of the rotation, which is a special case of the rotations dis-
cussed on page 231, can be seen in the case of a vector of length 2. Given the
vector x = (x1, x2), we wish to rotate it to ˜x = (˜x1, 0). As with a reﬂection,
in the rotation we also have ˜x1 = ∥x∥. Geometrically, we have the picture
shown in Fig. 5.3.
x2
x1
x
x
θ
~
Figure 5.3. Rotation of x onto a coordinate axis
It is easy to see that the orthogonal matrix
Q =
!
cos θ sin θ
−sin θ cos θ
"
(5.11)
will perform this rotation of x if cos θ = x1/r and sin θ = x2/r, where r =
∥x∥=

x2
1 + x2
2. (This is the same matrix as in equation (5.3), except that

5.5 Givens Transformations (Rotations)
239
the rotation is in the opposite direction.) Notice that θ is not relevant; we
only need real numbers c and s such that c2 + s2 = 1.
We have
˜x1 = x2
1
r + x2
2
r
= ∥x∥,
˜x2 = −x2x1
r
+ x1x2
r
= 0;
that is,
Q

x1
x2

=

∥x∥
0

.
5.5.1 Zeroing One Element in a Vector
As with the Householder reﬂection that transforms a vector
x = (x1, x2, x3, . . . , xn)
into a vector
˜xH = (˜xH1, 0, 0, . . ., 0),
it is easy to construct a Givens rotation that transforms x into
˜xG = (˜xG1, 0, x3, . . . , xn).
We can construct an orthogonal matrix Gpq similar to that shown in equa-
tion (5.11) that will transform the vector
x = (x1, . . . , xp, . . . , xq, . . . , xn)
into
˜x = (x1, . . . , ˜xp, . . . , 0, . . . , xn).
The orthogonal matrix that will do this is
Gpq(θ) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 0 0 · · · 0 0 0 · · · 0
0 1 · · · 0 0 0 · · · 0 0 0 · · · 0
...
0 0 · · · 1 0 0 · · · 0 0 0 · · · 0
0 0 · · · 0 c 0 · · · 0 s 0 · · · 0
0 0 · · · 0 0 1 · · · 0 0 0 · · · 0
...
0 0 · · · 0 0 0 · · · 1 0 0 · · · 0
0 0 · · · 0 −s 0 · · · 0 c 0 · · · 0
0 0 · · · 0 0 0 · · · 0 0 1 · · · 0
...
0 0 · · · 0 0 0 · · · 0 0 0 · · · 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(5.12)

240
5 Transformations and Factorizations
where the entries in the pth and qth rows and columns are
c = xp
r
and
s = xq
r ,
where r =
&
x2p + x2q. A rotation matrix is the same as an identity matrix
with four elements changed.
Considering x to be the pth column in a matrix X, we can easily see that
GpqX results in a matrix with a zero as the qth element of the pth column,
and all except the pth and qth rows and columns of GpqX are the same as
those of X.
5.5.2 Givens Rotations That Preserve Symmetry
If X is a symmetric matrix, we can preserve the symmetry by a transformation
of the form QTXQ, where Q is any orthogonal matrix. The elements of a
Givens rotation matrix that is used in this way and with the objective of
forming zeros in two positions in X simultaneously would be determined in
the same way as above, but the elements themselves would not be the same.
We illustrate that below, while at the same time considering the problem of
transforming a value into something other than zero.
5.5.3 Givens Rotations to Transform to Other Values
Consider a symmetric matrix X that we wish to transform to the symmetric
matrix 
X that has all rows and columns except the pth and qth the same as
those in X, and we want a speciﬁed value in the (p, p)th position of 
X, say
xpp = a. We seek a rotation matrix G such that 
X = GTXG. We have
!
c s
−s c
"T !
xpp xpq
xpq xqq
" !
c s
−s c
"
=
!
a ˜xpq
˜xpq ˜xqq
"
(5.13)
and
c2 + s2 = 1.
Hence
a = c2xpp −2csxpq + s2xqq.
(5.14)
Writing t = s/c (the tangent), we have the quadratic
(xqq −a)t2 −2xpqt + xpp −a = 0
(5.15)

5.6 Factorization of Matrices
241
with roots
t =
xpq ±
&
x2pq −(xpp −a)(xqq −a)
(xqq −a)
.
(5.16)
The roots are real if and only if
x2
pq ≥(xpp −a)(xqq −a).
If the roots in equation (5.16) are real, we choose the nonnegative one. (See
the discussion of equation (10.3) on page 488.) We then form
c =
1
√
1 + t2
(5.17)
and
s = ct.
(5.18)
The rotation matrix G formed from c and s will transform X into 
X.
5.5.4 Fast Givens Rotations
Often in applications we need to perform a succession of Givens transforma-
tions. The overall number of computations can be reduced using a succession
of “fast Givens rotations”. We write the matrix Q in equation (5.11) as CT ,
!
cos θ sin θ
−sin θ cos θ
"
=
!
cos θ
0
0
cos θ
" !
1
tan θ
−tan θ
1
"
,
(5.19)
and instead of working with matrices such as Q, which require four multipli-
cations and two additions, we work with matrices such as T , involving the
tangents, which require only two multiplications and two additions. After a
number of computations with such matrices, the diagonal matrices of the form
of C are accumulated and multiplied together.
The diagonal elements in the accumulated C matrices in the fast Givens
rotations can become widely diﬀerent in absolute values, so to avoid excessive
loss of accuracy, it is usually necessary to rescale the elements periodically.
5.6 Factorization of Matrices
It is often useful to represent a matrix A in a factored form,
A = BC,
where B and C have some speciﬁed desirable properties, such as being orthog-
onal or being triangular. We generally seek B and C such that B and C have
useful properties for some particular aspect of the problem being addressed.

242
5 Transformations and Factorizations
Most direct methods of solving linear systems (discussed in Chap. 6) are
based on factorizations (or, equivalently, “decompositions”) of the matrix of
coeﬃcients. Matrix factorizations are also performed for reasons other than
to solve a linear system, such as in eigenanalysis (discussed in Chap. 7).
Notice an indeterminacy in the factorization A = BC; if B and C are
factors of A, then so are −B and −C. This indeterminacy includes not only
the negatives of the matrices themselves, but also the negatives of various
rows and columns properly chosen. More generally, if D and E are matrices
such that DE = Im, where m is the number of columns in B and rows in C,
then A = BDEC, and so A can be factored as the product of BD and EC.
Hence, in general, a factorization is not unique. If restrictions are placed on
certain properties of the factors, however, then under those restrictions, the
factorizations may be unique. Also, if one factor is given, the other factor may
be unique. (For example, in the case of nonsingular matrices, we can see this
by taking the inverse.)
Invertible transformations result in a factorization of a matrix. For an n×k
matrix B, if D is a k × n matrix such that BD = In, then a given n × m
matrix A can be factorized as A = BDA = BC, where C = DA.
Some important matrix factorizations were listed at the beginning of this
chapter. Of those, we have already discussed the full rank and the diagonal
canonical factorizations in Chap. 3. Also in Chap. 3, we have brieﬂy described
the orthogonally similar canonical factorization and the SVD. We will discuss
these factorizations, which require iterative methods, further in Chap. 7. In
the next few sections we will introduce the LU, LDU, QR, and Cholesky
factorizations. We will also describe the square root factorization, even though
it uses eigenvalues, which require the iterative methods.
Matrix factorizations are generally performed by a sequence of full-rank
transformations and their inverses.
5.7 LU and LDU Factorizations
For any matrix (whether square or not) that can be expressed as LU, where L
is lower triangular (or lower trapezoidal) and U is upper triangular (or upper
trapezoidal), the product LU is called the LU factorization. We also generally
restrict either L or U to have 0s or 1s on the diagonal. If an LU factorization
exists, it is clear that either L or U (but not necessarily both) can be made
to have only 1s and 0s on its diagonal.
If an LU factorization exists, both the lower triangular matrix, L, and the
upper triangular matrix, U, can be made to have only 1s or 0s on their diag-
onals (that is, be made to be unit lower triangular or unit upper triangular)
by putting the products of any non-unit diagonal elements into a diagonal
matrix D and then writing the factorization as LDU, where now L and U are
unit triangular matrices (that is, matrices with 1s on the diagonal). This is
called the LDU factorization.

5.7 LU and LDU Factorizations
243
If a matrix is not square, or if the matrix is not of full rank, in its LU
decomposition, L and/or U may have zero diagonal elements or will be of
trapezoidal form. An example of a singular matrix and its LU factorization is
A =
!
0 3 2
0 0 0
"
=
!1 0
0 1
" ! 0 3 2
0 0 0
"
= LU.
(5.20)
In this case, U is an upper trapezoidal matrix.
5.7.1 Properties: Existence
Existence and uniqueness of an LU factorization (or LDU factorization) are
interesting questions. It is neither necessary nor suﬃcient that a matrix be
nonsingular for it to have an LU factorization. The example above shows the
LU factorization for a matrix not of full rank. Furthermore, a full rank matrix
does not necessarily have an LU factorization, as we see next.
An example of a nonsingular matrix that does not have an LU factorization
is an identity matrix with permuted rows or columns:
B =
! 0 1
1 0
"
.
(5.21)
The conditions for the existence of an LU factorization are not so easy to state
(see Harville 1997, for example), but in practice, as we will see, the question is
not very relevant. First, however, we will consider a matrix that is guaranteed
to have an LU factorization, and show one method of obtaining it.
A suﬃcient condition for an n × m matrix A to have an LU factorization
is that for k = 1, 2, . . . , min(n, m), each k × k principal submatrix of A be
nonsingular.
The proof is by construction. We assume that all principal submatrices
are nonsingular. This means that a11 ̸= 0, and so the Gaussian matrix G11
exists. (See equation (3.63) on page 84, where we also set the notation.) We
multiply A by G11, obtaining
G11A = A(1),
in which a(1)
i1 = 0 for i = 2, . . . , n and a(1)
22 ̸= 0 (otherwise the 2 × 2 principal
submatrix would be singular, which by assumption it is not).
Since a(1)
22 ̸= 0, the Gaussian matrix G22 exists, and now we multiply G11A
by G22, obtaining
G22G11A = A(2),
in which a(2)
i2
= 0 for i = 3, . . . , n and a(2)
33
̸= 0 as before. (All a(1)
i1
are
unchanged.)

244
5 Transformations and Factorizations
We continue in this way for k = min(n, m) steps, to obtain
Gkk · · · G22G11A = A(k),
(5.22)
in which A(k) is upper triangular, and the matrix Gkk · · · G22G11 is lower
triangular because each matrix in the product is lower triangular. (Note that
if m > n, Gkk = En(1/a(n−1)
nn
).)
Furthermore each matrix in the product Gkk · · · G22G11 is nonsingular,
and the matrix G−1
11 G−1
22 · · · G−1
kk is lower triangular (see equation (3.64) on
page 84). We complete the factorization by multiplying both sides of equa-
tion (5.22) by G−1
11 G−1
22 · · · G−1
kk :
A = G−1
11 G−1
22 · · · G−1
kk A(k)
= LA(k)
= LU.
(5.23)
Hence, we see that an n × m matrix A has an LU factorization if for k =
1, 2, . . . , min(n, m), each k × k principal submatrix of A is nonsingular.
The elements on the diagonal of A(k), that is, U, are all 1s; hence, we note
a useful property for square matrices. If the matrix is square (k = n), then
det(A) = det(L)det(U) = l11l22 · · · lnn.
(5.24)
An alternate construction leaves 1s on the diagonal of L, and then the
determinant of A is the product of the diagonal elements of U. One way of
achieving this factorization, again in the case in which the principal subma-
trices are all nonsingular, is to form the matrices
Lj = En,j

−a(j−1)
n,j
/a(j−1)
jj

· · · Ej+1,j

−a(j−1)
j+1,j/a(j−1)
jj

;
(5.25)
that is,
Lj =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 · · ·
0
0 · · · 0
...
0 · · ·
1
0 · · · 0
0 · · · −
a(j−1)
j+1,j
a(j−1)
jj
1 · · · 0
...
0 · · · −
a(j−1)
nj
a(j−1)
jj
0 · · · 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(5.26)
(See page 86 for the notation “Epq(a)”, representing the elementary axpy
matrix.)
Each Lj is nonsingular, with a determinant of 1. The whole process of
forward reduction can be expressed as a matrix product,

5.7 LU and LDU Factorizations
245
U = Lk−1Lk−2 . . . L2L1A,
(5.27)
and by the way we have performed the forward reduction, U is an upper
triangular matrix. The matrix Lk−1Lk−2 . . . L2L1 is nonsingular and is unit
lower triangular (all 1s on the diagonal). Its inverse therefore is also unit lower
triangular. Call its inverse L; that is,
L = (Lk−1Lk−2 . . . L2L1)−1.
(5.28)
Thus, the forward reduction is equivalent to expressing A as LU,
A = LU.
(5.29)
In this case, the diagonal elements of the lower triangular matrix L in the LU
factorization are all 1s by the method of construction, and if A is square,
det(A) = u11u22 · · · unn.
Even if the principal submatrices of a matrix are not nonsingular, the
matrix may have an LU decomposition, and it may be computable using a
sequence of Gaussian matrices as we did above. Consider, for example,
C =
⎡
⎣
2 1 0
1 0 0
1 1 1
⎤
⎦,
(5.30)
whose 0 in the (2, 2) position violates the nonsingularity condition. After three
Gaussian steps as above, we have
⎡
⎣
1
2
0 0
1 −2 0
−1
1 1
⎤
⎦C =
⎡
⎣
1 1
2 0
0 1 0
0 0 1
⎤
⎦,
from which we get
C =
⎡
⎣
2
0 0
1 −1
2 0
1
1
2 1
⎤
⎦
⎡
⎣
1 1
2 0
0 1 0
0 0 1
⎤
⎦= LU.
(5.31)
We also note that
det(C) = 2 ·

−1
2

· 1 = −1.
The method of constructing the LU factorization described above is guar-
anteed to work in the case of matrices with all nonsingular principal subma-
trices and in some other cases, such as for C in equation (5.30). The fact that
C is nonsingular is not suﬃcient to ensure that the process works or even that
the factorization exists. (As we indicated above, the suﬃcient conditions are
rather complicated, but not very important in practice.)

246
5 Transformations and Factorizations
If the matrix is not of full rank, as the Gaussian matrices are being formed,
at some point the diagonal element a(k)
ii
will be zero, so the matrix Gkk cannot
be formed. For such cases, we merely form a row of zeros in the lower triangular
matrix, and proceed to the next diagonal element. Even if the matrix is of full
rank, but not all principal submatrices are of full rank, we would encounter
this same kind of problem. In applications, we may address these two problems
similarly, using a technique of pivoting.
5.7.2 Pivoting
As we have seen, the suﬃcient condition of nonsingularity of all principal
submatrices is not a necessary requirement for the existence of an LU factor-
ization. We have also seen that, at least in some cases, if it exists, the factor-
ization, can be performed using Gaussian steps. There are matrices such as
B in equation (5.21), however, for which no LU factorization exists.
Does this matter? Obviously, it depends on the application; that is, it
depends on the purpose of using an LU factorization.
One of the most common applications of an LU factorization is to solve a
system of linear equations Ax = b. In such applications, interchange of rows
or columns does not change the problem, so long as the interchanges are made
appropriately over the entire system. Such an interchange is called pivoting.
Pivoting is often done not just to yield a matrix with an LU decomposi-
tion, it is routinely done in computations to improve the numerical accuracy.
Pivoting is generally eﬀected by premultiplication by an elementary permuta-
tion matrix Epq (see equation (3.66) on page 85 for notation and deﬁnitions).
Hence, instead of factoring A, we factor an equivalent matrix E(π)A:
E(π)A = LU,
where L and U are lower triangular or trapezoidal and upper triangular or
trapezoidal respectively, and satisfying other restrictions we wish to impose on
the LU factorization. In an LDU decomposition, we often choose a permuta-
tion matrix so that the diagonal elements of D are nonincreasing. Depending
on the shape and other considerations of numerical computations, we may
also permute the columns of the matrix, by postmultiplying by a permutation
matrix. In its most general form, we may express the LDU decomposition of
the matrix A as
E(π1)AE(π2) = LDU.
(5.32)
We will discuss pivoting in more detail on page 277 in Chap. 6.
As we mentioned in Chap. 3, in actual computations, we do not form the
elementary transformation matrices or the Gaussian matrices explicitly, but
their formulation in the text allows us to discuss the operations in a systematic
way and better understand the properties of the operations.

5.7 LU and LDU Factorizations
247
This is an instance of a principle that we will encounter repeatedly: the
form of a mathematical expression and the way the expression should
be evaluated in actual practice may be quite diﬀerent.
5.7.3 Use of Inner Products
The use of the elementary matrices described above is eﬀectively a series of
outer products (columns of the elementary matrices with rows of the matrices
being operated on).
The LU factorization can also be performed by using inner products. From
equation (5.29), we see
aij =
i−1

k=1
likukj + uij,
so
lij = aij −j−1
k=1 likukj
ujj
for i = j + 1, j + 2, . . . , n.
(5.33)
The use of computations implied by equation (5.33) is called the Doolittle
method or the Crout method. (There is a slight diﬀerence between the Doolit-
tle method and the Crout method: the Crout method yields a decomposition
in which the 1s are on the diagonal of the U matrix rather than the L matrix.)
Whichever method is used to form the LU decomposition, n3/3 multiplications
and additions are required.
5.7.4 Properties: Uniqueness
There are clearly many ways indeterminacies can occur in L, D, or U in an LU
or LDU factorization in general. (Recall the simple replacement of L and U by
−L and −U.) In some cases, indeterminacies can be eliminated or reduced by
putting restrictions on the factors, but any uniqueness of an LU factorization
is rather limited.
If a nonsingular matrix has an LU factorization, the factorization itself in
general is not unique, but given either L or U, the other factor is unique, as
we can see by use of inverses. (Recall the form of the inverse of a triangular
matrix, page 120.)
For the LDU factorization of a general square matrix, if L and U are
restricted to be unit triangular matrices, then D is unique. To see this, let
A be an n × n matrix for which an LDU factorization, and let A = LDU,
with L a lower unit triangular matrix and U an upper unit triangular matrix
and D a diagonal matrix. (All matrices are n × n.) Now, suppose A = L D U,
where L, D, and U have the same patterns as L, D, and U. All of these unit
triangular matrices have inverses of the same type (see page 120). Now, since
LDU = L D U, premultiplying by L−1 and postmultiplying by U −1, we have
L−1LD = D UU −1;

248
5 Transformations and Factorizations
that is, the diagonal elements of L−1LD and D UU −1 are the same. By the
properties of unit triangular matrices given on page 78, we see that those
diagonal elements are the diagonal elements of D and D. Since, therefore,
D = D, D in the LDU factorization of a general square matrix is unique.
5.7.5 Properties of the LDU Factorization of a Square Matrix
The uniqueness of D in the LDU factorization of a general square matrix is an
important fact. A related useful fact about the LDU factorization of a general
square matrix A is
det(A) =
$
dii,
(5.34)
which we see from equations (3.81) and (3.37) on pages 88 and 70 respectively.
There are other useful properties of the LDU factorization of square ma-
trices with special properties, such as positive deﬁniteness, but we will not
pursue them here.
5.8 QR Factorization
A very useful factorization of a matrix is the product of an orthogonal matrix
and an upper triangular matrix with nonnegative diagonal elements. Depend-
ing on the shape of A, the shapes of the factors may vary, and even the
deﬁnition of the factors themselves may be stated diﬀerently.
Let A be an n × m matrix and suppose
A = QR,
(5.35)
where Q is an orthogonal matrix and R is an upper triangular or trapezoidal
matrix with nonnegative diagonal elements. This is called the QR factorization
of A. In most applications, n ≥m, but if this is not the case, we still have a
factorization into similar matrices.
The QR factorization is useful for many tasks in linear algebra. It can
be used to determine the rank of a matrix (see page 252 below), to extract
eigenvalues and eigenvectors (see page 318), to form the singular value decom-
position (see page 322), and to show various theoretical properties of matrices
(see, for example, Exercise 5.5 on page 262). The QR factorization is partic-
ularly useful in computations for overdetermined systems, as we will see in
Sect. 6.6 on page 289, and in other computations involving nonsquare matri-
ces.
If A is square, both factors are square, but when A is not square, there
are some variations in the form of the factorization. I will consider only the
case in which the number of rows in A is at least as great as the number of
columns. The other case is logically similar.
If n > m, there are two diﬀerent forms of the QR factorization. In one
form Q is an n × n matrix and R is an n × m upper trapezoidal matrix with

5.8 QR Factorization
249
with zeroes in the lower rows. In the other form, Q is an n × m matrix with
orthonormal columns and R is an m × m upper triangular matrix. This latter
form is sometimes called a “skinny” QR factorization. When n > m, the
skinny QR factorization is more commonly used than one with a square Q.
The two factorizations are essentially the same. If R1 is the matrix in the
skinny factorization and R is the matrix in the full form, they are related as
R =
!
R1
0
"
.
(5.36)
Likewise the square Q can be partitioned as [Q1 | Q2], and the skinny factor-
ization written as
A = Q1R1.
(5.37)
In the full form QTQ = In and in the skinny form, QT
1 Q1 = Im.
The existence of the QR factorization can be shown by construction using,
for example, Householder reﬂections, as in Sect. 5.8.8 below.
5.8.1 Related Matrix Factorizations
For the n × m matrix, similar to the factorization in equation (5.35), we may
have
A = RQ,
where Q is an orthogonal matrix and R is an upper triangular or trapezoidal
matrix with nonnegative diagonal elements. This is called the RQ factorization
of A.
Two other related factorizations, with obvious names, are
A = QL,
and
A = LQ,
where Q is an orthogonal matrix and L is an lower triangular or trapezoidal
matrix with nonnegative diagonal elements.
5.8.2 Matrices of Full Column Rank
If the matrix A is of full column rank (meaning that there are at least as
many rows as columns and the columns are linearly independent), as in many
applications in statistics, the R matrix in the QR factorization is full rank.
Furthermore, in the skinny QR factorization A = Q1R1, R1 is nonsingular and
the factorization is unique. (Recall that the diagonal elements are required to
be nonnegative.)

250
5 Transformations and Factorizations
We see that the factorization is unique by forming A = Q1R1 and then
letting Q1 and R1 be the skinny QR factorization
A = Q1 R1,
and showing that Q1 = Q1 and R1 = R1. Since Q1R1 = Q1 R1, we have
Q1 = Q1 R1R−1
1
and so R1R−1
1
= QT
1 Q1. As we saw on page 120, since R1 is
upper triangular, R−1
1
is upper triangular; and as we saw on page 78, since R1
is upper triangular, R1R−1
1
is upper triangular. Let T be this upper triangular
matrix,
T = R1R−1
1 .
Now consider T TT . (This is a Cholesky factorization; see Sect. 5.9.2.)
Since QT
1 Q1 = Im, we have T TT = T T QT
1 Q1T . Now, because Q1 = Q1T ,
we have
T TT = QT
1 Q1 = Im.
The only upper triangular matrix T such that T TT = I is the identity I itself.
(This is from the deﬁnition of matrix multiplication. First, we see that t11 = 1,
and since all oﬀ-diagonal elements in the ﬁrst row of I are 0, all oﬀ-diagonal
elements in the ﬁrst row of T T must be 0. Continuing in this way, we see that
T = I.) Hence,
R1R−1
1
= I,
and so
R1 = R1.
Now,
Q1 = Q1T = Q1;
hence, the factorization is unique.
5.8.3 Relation to the Moore-Penrose Inverse for Matrices of Full
Column Rank
If the matrix A is of full column rank, the Moore-Penrose inverse of A is
immediately available from the QR factorization:
A+ =
'
R−1
1
0
(
QT.
(5.38)
(The four properties of a Moore-Penrose inverse listed on page 128 are easily
veriﬁed, and you are asked to do so in Exercise 5.8.)

5.8 QR Factorization
251
5.8.4 Nonfull Rank Matrices
If A is square but not of full rank, R has the form
⎡
⎣
X X X
0 X X
0 0 0
⎤
⎦.
(5.39)
In the common case in which A has more rows than columns, if A is
not of full (column) rank, R1 in equation (5.36) will have the form shown in
matrix (5.39).
If A is not of full rank, we apply permutations to the columns of A by
multiplying on the right by a permutation matrix. The permutations can be
taken out by a second multiplication on the right. If A is of rank r (≤m),
the resulting decomposition consists of three matrices: an orthogonal Q, a T
with an r × r upper triangular submatrix, and a permutation matrix ET
(π),
A = QT ET
(π).
(5.40)
The matrix T has the form
T =
!T1 T2
0
0
"
,
(5.41)
where T1 is upper triangular and is r×r. The decomposition in equation (5.40)
is not unique because of the permutation matrix. The choice of the permuta-
tion matrix is the same as the pivoting that we discussed in connection with
Gaussian elimination. A generalized inverse of A is immediately available from
equation (5.40):
A−= P
!
T −1
1
0
0
0
"
QT.
(5.42)
Additional orthogonal transformations can be applied from the right-hand
side of the n × m matrix A in the form of equation (5.40) to yield
A = QRU T,
(5.43)
where R has the form
R =
!
R1 0
0 0
"
,
(5.44)
where R1 is r × r upper triangular, Q is n × n and as in equation (5.40), and
U T is n × m and orthogonal. (The permutation matrix in equation (5.40) is
also orthogonal, of course.)
5.8.5 Relation to the Moore-Penrose Inverse
The decomposition (5.43) is unique, and it provides the unique Moore-Penrose
generalized inverse of A:
A+ = U
!
R−1
1
0
0
0
"
QT.
(5.45)

252
5 Transformations and Factorizations
5.8.6 Determining the Rank of a Matrix
It is often of interest to know the rank of a matrix. Given a decomposition
of the form of equation (5.40), the rank is obvious, and in practice, this QR
decomposition with pivoting is a good way to determine the rank of a matrix.
The QR decomposition is said to be “rank-revealing”.
For many matrices, the computations are quite sensitive to rounding. Piv-
oting is often required, and even so, the pivoting must be done with some
care (see Hong and Pan 1992; Section 2.7.3 of Bj¨orck 1996; and Bischof and
Quintana-Ort´ı 1998a,b). (As we pointed out on page 121, the problem itself is
ill-posed in Hadamard’s sense because the rank is not a continuous function
of any of the quantities that determine it. For a given matrix, the problem
can also be ill-conditioned in the computational sense. Ill-conditioning is a
major concern, and we will discuss it often in latter chapters of this book. We
introduce some of the concepts of ill-conditioning formally in Sect. 6.1.)
5.8.7 Formation of the QR Factorization
There are three good methods for obtaining the QR factorization: Householder
transformations or reﬂections; Givens transformations or rotations; and the
(modiﬁed) Gram-Schmidt procedure. Diﬀerent situations may make one of
these procedures better than the two others. The Householder transforma-
tions described in the next section are probably the most commonly used.
If the data are available only one row at a time, the Givens transformations
discussed in Sect. 5.8.9 are very convenient. Whichever method is used to
compute the QR decomposition, at least 2n3/3 multiplications and additions
are required. The operation count is therefore about twice as great as that for
an LU decomposition.
5.8.8 Householder Reﬂections to Form the QR Factorization
To use reﬂectors to compute a QR factorization, we form in sequence the
reﬂector for the ith column that will produce 0s below the (i, i) element.
For a convenient example, consider the matrix
A =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
3 −98
28 X X X
1
122
28 X X X
2 −8
28 X X X
1
66
28 X X X
1
10
28 X X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

5.8 QR Factorization
253
The ﬁrst transformation would be determined so as to transform (3, 1, 2, 1, 1)
to (X, 0, 0, 0, 0). We use equations (5.8) through (5.10) to do this. Call this
ﬁrst Householder matrix P1. We have
P1A =
⎡
⎢⎢⎢⎢⎣
−4 1 X X X
0 5 X X X
0 1 X X X
0 3 X X X
0 1 X X X
⎤
⎥⎥⎥⎥⎦
.
We now choose a reﬂector to transform (5, 1, 3, 1) to (−6, 0, 0, 0). We do not
want to disturb the ﬁrst column in P1A shown above, so we form P2 as
P2 =
⎡
⎢⎢⎢⎣
1 0 . . . 0
0
...
H2
0
⎤
⎥⎥⎥⎦.
Forming the vector (11, 1, 3, 1)/
√
132 and proceeding as before, we get the
reﬂector
H2 = I −1
66(11, 1, 3, 1)(11, 1, 3, 1)T
= 1
66
⎡
⎢⎢⎣
−55 −11 −33 −11
−11
65 −3
−1
−33 −3
57
−3
−11 −1 −3
65
⎤
⎥⎥⎦.
Now we have
P2P1A =
⎡
⎢⎢⎢⎢⎣
−4
1 X X X
0 −6 X X X
0
0 X X X
0
0 X X X
0
0 X X X
⎤
⎥⎥⎥⎥⎦
.
Continuing in this way for three more steps, we would have the QR decom-
position of A with QT = P5P4P3P2P1.
The number of computations for the QR factorization of an n × n matrix
using Householder reﬂectors is 2n3/3 multiplications and 2n3/3 additions.
5.8.9 Givens Rotations to Form the QR Factorization
Just as we built the QR factorization by applying a succession of Householder
reﬂections, we can also apply a succession of Givens rotations to achieve the
factorization. If the Givens rotations are applied directly, the number of com-
putations is about twice as many as for the Householder reﬂections, but if

254
5 Transformations and Factorizations
fast Givens rotations are used and accumulated cleverly, the number of com-
putations for Givens rotations is not much greater than that for Householder
reﬂections. As mentioned on page 241, it is necessary to monitor the diﬀer-
ences in the magnitudes of the elements in the C matrix and often necessary
to rescale the elements. This additional computational burden is excessive
unless done carefully (see Bindel et al. 2002, for a description of an eﬃcient
method).
5.8.10 Gram-Schmidt Transformations to Form the
QR Factorization
Gram-Schmidt transformations yield a set of orthonormal vectors that span
the same space as a given set of linearly independent vectors, {x1, x2, . . . , xm}.
Application of these transformations is called Gram-Schmidt orthogonaliza-
tion. If the given linearly independent vectors are the columns of a matrix A,
the Gram-Schmidt transformations ultimately yield the QR factorization of
A. The basic Gram-Schmidt transformation is shown in equation (2.56) on
page 38.
The Gram-Schmidt algorithm for forming the QR factorization is just a
simple extension of equation (2.56); see Exercise 5.10 on page 263.
5.9 Factorizations of Nonnegative
Deﬁnite Matrices
There are factorizations that may not exist except for nonnegative deﬁnite
matrices, or may exist only for such matrices. The LU decomposition, for
example, exists and is unique for a nonnegative deﬁnite matrix; but may not
exist for general matrices (without permutations). In this section we discuss
two important factorizations for nonnegative deﬁnite matrices, the square root
and the Cholesky factorization.
5.9.1 Square Roots
On page 160, we deﬁned the square root of a nonnegative deﬁnite matrix in
the natural way and introduced the notation A
1
2 as the square root of the
nonnegative deﬁnite n × n matrix A:
A =

A
1
2
2
.
(5.46)
Just as the computation of a square root of a general real number requires
iterative methods, the computation of the square root of a matrix requires
iterative methods. In this case, the iterative methods are required for the
evaluation of the eigenvalues (as we will describe in Chap. 7). Once the eigen-
values are available, the computations are simple, as we describe below.

5.9 Factorizations of Nonnegative Deﬁnite Matrices
255
Because A is symmetric, it has a diagonal factorization, and because it
is nonnegative deﬁnite, the elements of the diagonal matrix are nonnegative.
In terms of the orthogonal diagonalization of A, as on page 160, we write
A
1
2 = VC
1
2 V T.
We now show that this square root of a nonnegative deﬁnite matrix is
unique among nonnegative deﬁnite matrices. Let A be a (symmetric) nonneg-
ative deﬁnite matrix and A = VCV T, and let B be a symmetric nonnegative
deﬁnite matrix such that B2 = A. We want to show that B = VC
1
2 V T or that
B −VC
1
2 V T = 0. Form

B −VC
1
2 V T 
B −VC
1
2 V T
= B2 −VC
1
2 V TB −BVC
1
2 V T +

VC
1
2 V T2
= 2A −VC
1
2 V TB −

VC
1
2 V TB
T
.
(5.47)
Now, we want to show that VC
1
2 V TB = A. The argument below follows
(Harville 1997). Because B is nonnegative deﬁnite, we can write B = UDU T
for an orthogonal n × n matrix U and a diagonal matrix D with nonnegative
elements, d1, . . . dn. We ﬁrst want to show that V TUD = C
1
2 V TU. We have
V TUD2 = V TUDU TUDU TU
= V TB2U
= V TAU
= V T(VC
1
2 V T)2U
= V TVC
1
2 V TVC
1
2 V TU
= CV TU.
Now consider the individual elements in these matrices. Let zij be the (ij)th
element of V TU, and since D2 and C are diagonal matrices, the (ij)th element
of V TUD2 is d2
jzij and the corresponding element of CV TU is cizij, and these
two elements are equal, so djzij = √cizij. These, however, are the (ij)th
elements of V TUD and C
1
2 V TU, respectively; hence V TUD = C
1
2 V TU. We
therefore have
V C
1
2 V TB = V C
1
2 V TUDU T = V C
1
2 C
1
2 V TUU T = V CV T = A.
We conclude that VC
1
2 V T is the unique square root of A.
If A is positive deﬁnite, it has an inverse, and the unique square root of
the inverse is denoted as A−1
2 .
5.9.2 Cholesky Factorization
If the matrix A is symmetric and nonnegative deﬁnite, another important
factorization is the Cholesky decomposition. In this factorization,

256
5 Transformations and Factorizations
A = T TT,
(5.48)
where T is an upper triangular matrix with nonnegative diagonal elements.
We occasionally denote the Cholesky factor of A (that is, T in the expression
above) as AC. (Notice on page 48 and later on page 366 that we use a lowercase
c subscript to represent a centered vector or matrix.)
The factor T in the Cholesky decomposition is sometimes called the square
root, but we have deﬁned a diﬀerent matrix as the square root, A
1
2 (page 160
and Sect. 5.9.1). The Cholesky factor is more useful in practice, but the square
root has more applications in the development of the theory.
We ﬁrst consider the Cholesky decomposition of a positive deﬁnite matrix
A. In that case, a factor of the form of T in equation (5.48) is unique up to the
sign, just as a square root is. To make the Cholesky factor unique, we require
that the diagonal elements be positive. The elements along the diagonal of T
will be square roots. Notice, for example, that t11 is √a11.
Algorithm 5.1 is a method for constructing the Cholesky factorization of
a positive deﬁnite matrix A. The algorithm serves as the basis for a construc-
tive proof of the existence and uniqueness of the Cholesky factorization (see
Exercise 5.6 on page 262). The uniqueness is seen by factoring the principal
square submatrices.
Algorithm 5.1 Cholesky factorization of a positive deﬁnite matrix
1. Let t11 = √a11.
2. For j = 2, . . . , n, let t1j = a1j/t11.
3. For i = 2, . . . , n,
{
let tii =
&
aii −i−1
k=1 t2
ki, and
for j = i + 1, . . . , n,
{
let tij = (aij −i−1
k=1 tkitkj)/tii.
}
}
There are other algorithms for computing the Cholesky decomposition.
The method given in Algorithm 5.1 is sometimes called the inner product
formulation because the sums in step 3 are inner products. The algorithms
for computing the Cholesky decomposition are numerically stable. Although
the order of the number of computations is the same, there are only about half
as many computations in the Cholesky factorization as in the LU factorization.
Another advantage of the Cholesky factorization is that there are only n(n +
1)/2 unique elements as opposed to n2 + n in the LU factorization.
The Cholesky decomposition can also be formed as T TD T , where D is a di-
agonal matrix that allows the diagonal elements of T to be computed without
taking square roots. This modiﬁcation is sometimes called a Banachiewicz

5.9 Factorizations of Nonnegative Deﬁnite Matrices
257
factorization or root-free Cholesky. The Banachiewicz factorization can be
formed in essentially the same way as the Cholesky factorization shown in
Algorithm 5.1: just put 1s along the diagonal of T and store the squared
quantities in a vector d.
5.9.2.1 Cholesky Decomposition of Singular Nonnegative Deﬁnite
Matrices
Any symmetric nonnegative deﬁnite matrix has a decomposition similar to
the Cholesky decomposition for a positive deﬁnite matrix. If A is n × n with
rank r, there exists a unique matrix T such that A = T TT , where T is an
upper triangular matrix with r positive diagonal elements and n −r rows
containing all zeros. The algorithm is the same as Algorithm 5.1, except that
in step 3 if tii = 0, the entire row is set to zero. The algorithm serves as a
constructive proof of the existence and uniqueness.
5.9.2.2 Relations to Other Factorizations
For a symmetric matrix, the LDU factorization is U TDU; hence, we have for
the Cholesky factor
T = D
1
2 U,
where D
1
2 is the matrix whose elements are the square roots of the correspond-
ing elements of D. (This is consistent with our notation above for Cholesky
factors; D
1
2 is the Cholesky factor of D, and it is symmetric.)
The LU and Cholesky decompositions generally are applied to square ma-
trices. However, many of the linear systems that occur in scientiﬁc applications
are overdetermined; that is, there are more equations than there are variables,
resulting in a nonsquare coeﬃcient matrix.
For the n × m matrix A with n ≥m, we can write
ATA = RTQTQR
= RTR,
(5.49)
so we see that the matrix R in the QR factorization is (or at least can be)
the same as the matrix T in the Cholesky factorization of ATA. There is
some ambiguity in the Q and R matrices, but if the diagonal entries of R are
required to be nonnegative, the ambiguity disappears and the matrices in the
QR decomposition are unique.
An overdetermined system may be written as
Ax ≈b,
where A is n × m (n ≥m), or it may be written as
Ax = b + e,

258
5 Transformations and Factorizations
where e is an n-vector of possibly arbitrary “errors”. Because not all equations
can be satisﬁed simultaneously, we must deﬁne a meaningful “solution”. A
useful solution is an x such that e has a small norm. The most common
deﬁnition is an x such that e has the least Euclidean norm; that is, such that
the sum of squares of the eis is minimized.
It is easy to show that such an x satisﬁes the square system ATAx = ATb,
the “normal equations”. This expression is important and allows us to analyze
the overdetermined system (not just to solve for the x but to gain some better
understanding of the system). It is easy to show that if A is of full rank (i.e.,
of rank m, all of its columns are linearly independent, or, redundantly, “full
column rank”), then ATA is positive deﬁnite. Therefore, we could apply either
Gaussian elimination or the Cholesky decomposition to obtain the solution.
As we have emphasized many times before, however, useful conceptual
expressions are not necessarily useful as computational formulations. That is
sometimes true in this case also. In Sect. 6.1, we will discuss issues relating to
the expected accuracy in the solutions of linear systems. There we will deﬁne
a “condition number”. Larger values of the condition number indicate that
the expected accuracy is less. We will see that the condition number of ATA is
the square of the condition number of A. Given these facts, we conclude that
it may be better to work directly on A rather than on ATA, which appears
in the normal equations. We discuss solutions of overdetermined systems in
Sect. 6.6, beginning on page 289, and in Sect. 6.7, beginning on page 296.
Overdetermined systems are also a main focus of the statistical applications
in Chap. 9.
5.9.3 Factorizations of a Gramian Matrix
The sums of squares and cross products matrix, the Gramian matrix XTX,
formed from a given matrix X, arises often in linear algebra. We discuss
properties of the sums of squares and cross products matrix beginning on
page 359. Now we consider some additional properties relating to various
factorizations.
First we observe that XTX is symmetric and hence has an orthogonally
similar canonical factorization,
XTX = V CV T.
We have already observed that XTX is nonnegative deﬁnite, and so it has
the LU factorization
XTX = LU,
with L lower triangular and U upper triangular, and it has the Cholesky
factorization
XTX = T TT
with T upper triangular. With L = T T and U = T , both factorizations are
the same. In the LU factorization, the diagonal elements of either L or U

5.10 Approximate Matrix Factorization
259
are often constrained to be 1, and hence the two factorizations are usually
diﬀerent.
It is instructive to relate the factors of the m × m matrix XTX to the
factors of the n × m matrix X. Consider the QR factorization
X = QR,
where R is upper triangular. Then XTX = (QR)TQR = RTR, so R is the
Cholesky factor T because the factorizations are unique (again, subject to the
restrictions that the diagonal elements be nonnegative).
Consider the SVD factorization
X = UDV T.
We have XTX = (UDV T)TUDV T = V D2V T, which is the orthogonally sim-
ilar canonical factorization of XTX. The eigenvalues of XTX are the squares
of the singular values of X, and the condition number of XTX (which we
deﬁne in Sect. 6.1) is the square of the condition number of X.
5.10 Approximate Matrix Factorization
It is occasionally of interest to form a factorization that approximates a given
matrix. For a given matrix A, we may have factors B and C, where
A = BC,
and A is an approximation to A. (See Sect. 3.10, beginning on page 175, for
discussions of approximation of matrices.)
The approximate factorization
A ≈BC
may be useful for various reasons. The computational burden of an exact
factorization may be excessive. Alternatively, the matrices B and C may have
desirable properties that no exact factors of A possess.
In this section we discuss two kinds of approximate factorizations, one mo-
tivated by the properties of the matrices, and the other merely an incomplete
factorization, which may be motivated by computational expediency or by
other considerations.
5.10.1 Nonnegative Matrix Factorization
If A in an n × m matrix all of whose elements are nonnegative, it may be of
interest to approximate A as
A ≈WH,

260
5 Transformations and Factorizations
where W is n × r and H r × m, and both W and H have only nonnegative
elements. Such matrices are called nonnegative matrices. If all elements of a
matrix are positive, the matrix is called a positive matrix. Nonnegative and
positive matrices arise often in applications and have a number of interesting
properties. These kinds of matrices are the subject of Sect. 8.7, beginning on
page 372.
Clearly, if r ≥min(n, m), the factorization A = WH exists exactly, for
if r = min(n, m) then A = IrW, which is not unique since for a > 0, A =
( 1
a)Ir(aH). If, however, r < min(n, m), the factorization may not exist.
A nonnegative matrix factorization (NMF) of the nonnegative matrix A
for a given r is the expression WH, where the n × r matrix W and the r × m
matrix H are nonnegative, and the diﬀerence A −WH is minimum according
to some criterion (see page 175); that is, given r, the NMF factorization of
the n × m nonnegative matrix A are the matrices W and H satisfying
min
W∈Rn×r,H∈Rr×m ρ(A −WH)
s.t. W, H ≥0,
(5.50)
where ρ is a measure of the size of A −WH. Interest in this factorization
arose primarily in the problem of analysis of text documents (see page 339).
Most methods for solving the optimization problem (5.50) follow the al-
ternating variables approach: for ﬁxed W (0) determine an optimal H(1); then
given optimal H(k) determine an optimal W (k+1) and for optimal W (k+1)
determine an optimal H(k+1).
The ease of solving the optimization problem (5.50), whether or not the
alternating variables approach is used, depends on the nature of ρ. Generally,
ρ is a norm, but Lee and Seung (2001) considered ρ to be the Kullback-
Leibler divergence (see page 176), and described a computational method for
solving the optimization problem. Often ρ is chosen to be a Frobenius p norm,
because that matrix norm can be expressed as a Lp vector norm, as shown in
equation (3.299) on page 169. Furthermore, if the ordinary Frobenius norm is
chosen (that is, p = 2), then each subproblem is just a constrained linear least
squares problem (as discussed on page 211). Kim and Park (2008) described
an alternating variables approach for nonnegative matrix factorization based
on the ordinary Frobenius norm.
Often in applications, the matrix A to be factored is sparse. Computational
methods for the factorization that take advantage of the sparsity can lead to
improvements in the computational eﬃciency of orders of magnitude.
5.10.2 Incomplete Factorizations
Often instead of an exact factorization, an approximate or “incomplete” fac-
torization may be more useful because of its computational eﬃciency. This
may be the case in the context of an iterative algorithm in which a matrix
is being successively transformed, and, although a factorization is used in

Exercises
261
each step, the factors from a previous iteration are adequate approximations.
Another common situation is in working with sparse matrices. Many exact
operations on a sparse matrix yield a dense matrix; however, we may want to
preserve the sparsity, even at the expense of losing exact equalities. When a
zero position in a sparse matrix becomes nonzero, this is called “ﬁll-in”, and
we want to avoid that.
For example, instead of an LU factorization of a sparse matrix A, we may
seek lower and upper triangular factors L and U, such that
A ≈L U,
(5.51)
and if aij = 0, then ˜lij = ˜uij = 0. This approximate factorization is easily
accomplished by modifying the Gaussian elimination step that leads to the
outer product algorithm of equations (5.27) and (5.28).
More generally, we may choose a set of indices S = {(p, q)} and modify
the elimination step, for m ≥i, to be
a(k+1)
ij
←
#
a(k)
ij −a(k)
mja(k)
ij /a(k)
jj
if (i, j) ∈S
aij
otherwise.
(5.52)
Note that aij does not change unless (i, j) is in S. This allows us to preserve
0s in L and U corresponding to given positions in A.
Exercises
5.1. Consider the transformation of the 3-vector x that ﬁrst rotates the vector
30◦about the x1 axis, then rotates the vector 45◦about the x2 axis, and
then translates the vector by adding the 3-vector y. Find the matrix
A that eﬀects these transformations by a single multiplication. Use the
vector xh of homogeneous coordinates that corresponds to the vector x.
(Thus, A is 4 × 4.)
5.2. Homogeneous coordinates are often used in mapping three-dimensional
graphics to two dimensions. The perspective plot function persp in R, for
example, produces a 4×4 matrix for projecting three-dimensional points
represented in homogeneous coordinates onto two-dimensional points in
the displayed graphic. R uses homogeneous coordinates in the form of
equation (5.6b) rather than equation (5.6a). If the matrix produced is
T and if ah is the representation of a point (xa, ya, za) in homogeneous
coordinates, in the form of equation (5.6b), then ahT yields transformed
homogeneous coordinates that correspond to the projection onto the two-
dimensional coordinate system of the graphical display. Consider the two
graphs in Fig. 5.4. The graph on the left in the unit cube was produced
by the simple R statements
x<-c(0,1)

262
5 Transformations and Factorizations
x
y
z
x
y
z
Figure 5.4. Illustration of the use of homogeneous coordinates to locate three-
dimensional points on a two-dimensional graph
y<-c(0,1)
z<-matrix(c(0,0,1,1),nrow=2)
persp(x, y, z, theta = 45, phi = 30)
(The angles theta and phi are the azimuthal and latitudinal viewing
angles, respectively, in degrees.) The graph on the right is the same with
a heavy line going down the middle of the surface; that is, from the point
(0.5, 0, 0) to (0.5, 1, 1). Obtain the transformation matrix necessary to
identify the rotated points and produce the graph on the right.
5.3. Determine the rotation matrix that rotates 3-vectors through an angle of
30◦in the plane x1 + x2 + x3 = 0.
5.4. Let A = LU be the LU decomposition of the n × n matrix A.
a) Suppose we multiply the jth column of A by cj, j = 1, 2, . . .n, to
form the matrix Ac. What is the LU decomposition of Ac? Try to
express your answer in a compact form.
b) Suppose we multiply the ith row of A by ci, i = 1, 2, . . . n, to form
the matrix Ar. What is the LU decomposition of Ar? Try to express
your answer in a compact form.
c) What application might these relationships have?
5.5. Use the QR decomposition to prove Hadamard’s inequality:
|det(A)| ≤
n
$
j=1
∥aj∥2,
where A is an n × n matrix, whose columns are the same as the vectors
aj. Equality holds if and only if either the aj are mutually orthogonal or
some aj is zero.
5.6. Show that if A is positive deﬁnite, there exists a unique upper triangular
matrix T with positive diagonal elements such that
A = T TT.
Hint: Show that aii > 0. Show that if A is partitioned into square sub-
matrices A11 and A22,

Exercises
263
A =
! A11 A12
A21 A22
"
,
that A11 and A22 are positive deﬁnite. Use Algorithm 5.1 (page 256) to
show the existence of a T , and ﬁnally show that T is unique.
5.7. Let X1, X2, and X3 be independent random variables identically dis-
tributed as standard normals.
a) Determine a matrix A such that the random vector
A
⎡
⎣
X1
X2
X3
⎤
⎦
has a multivariate normal distribution with variance-covariance ma-
trix
⎡
⎣
4
2
8
2 10
7
8
7 21
⎤
⎦.
b) Is your solution unique? (The answer is no.) Determine a diﬀerent
solution.
5.8. Generalized inverses.
a) Prove equation (5.38) on page 250 (Moore-Penrose inverse of a full
column rank matrix).
b) Prove equation (5.42) on page 251 (generalized inverse of a nonfull
rank matrix).
c) Prove equation (5.45) on page 251, (Moore-Penrose inverse of a non-
full rank matrix).
5.9. Determine the Givens transformation matrix that will rotate the matrix
A =
⎡
⎢⎢⎣
3 5 6
6 1 2
8 6 7
2 3 1
⎤
⎥⎥⎦
so that the second column becomes (5, ˜a22, 6, 0) (see also Exercise 12.5).
5.10. Gram-Schmidt transformations.
a) Use Gram-Schmidt transformations to determine an orthonormal ba-
sis for the space spanned by the vectors
v1 = (3, 6, 8, 2),
v2 = (5, 1, 6, 3),
v3 = (6, 2, 7, 1).
b) Write out a formal algorithm for computing the QR factorization of
the n × m full rank matrix A. Assume n ≥m.
c) Write a Fortran or C function to implement the algorithm you de-
scribed.

6
Solution of Linear Systems
One of the most common problems in numerical computing is to solve the
linear system
Ax = b;
that is, for given A and b, to ﬁnd x such that the equation holds. The system
is said to be consistent if there exists such an x, and in that case a solution x
may be written as A−b, where A−is some inverse of A. If A is square and of
full rank, we can write the solution as A−1b.
It is important to distinguish the expression A−1b or A−b, which represents
the solution, from the method of computing the solution. We would never
compute A−1 just so we could multiply it by b to form the solution A−1b.
Two topics we discuss in this chapter have wider relevance in computations
for scientiﬁc applications. The ﬁrst is how to assess the expected numerical
diﬃculties in solving a speciﬁc problem. A speciﬁc problem is comprised of
a task and data, or input to the task. Some tasks are naturally harder than
others; for example, it is generally more diﬃcult to solve a constrained op-
timization problem than it is to solve a system of linear equations. Some
constrained optimization problems, however, may be very “small” and even
have a simple closed-form solution; while a system of linear equations may
be very large and be subject to severe rounding errors (for reasons we will
discuss). For any speciﬁc task, there may be ways of assessing the “diﬃculty”
of a speciﬁc input. One measure of diﬃculty is a “condition number”, and we
discuss a condition number for the task of solving a linear system in Sect. 6.1.
Another topic that is illustrated by the techniques discussed in this chapter
is the diﬀerence between direct methods and iterative methods. The methods
for factoring a matrix discussed in Chap. 5 are direct methods; that is, there
is a ﬁxed sequence of operations (possibly with some pivoting along the way)
that yields the solution. Iterative methods do not have a ﬁxed, predetermined
number of steps; rather, they must have a stopping rule that depends on
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 6
265

266
6 Solution of Linear Systems
the steps themselves. Most optimization methods discussed in Sect. 4.4 are
examples of iterative methods. In Chap. 7, we will see that all methods for
obtaining eigenvalues (for general square matrices larger than 4 × 4) are nec-
essarily iterative.
An iterative method is characterized by a sequence of partial or ap-
proximate solutions, which I index by a superscript enclosed in parentheses:
s(0), s(1), s(2), . . .. A starting point is necessary. I usually index it as “(0)”. The
iterative algorithm is essentially a rule for computing s(k) given s(k−1).
A stopping criterion is also necessary. The partial solution at any point in
the sequence often consists of multiple parts; for example, in an optimization
problem, there are two parts, the value of the decision variable and the value
of the objective function. The stopping rule may be based on the magnitude
of the change in one or more parts of the solution, as shown in the two crite-
ria (4.24) and (4.25) on page 201. Of course, a stopping criterion based simply
on the number of iterations is also possible, and, in fact, is usually a good idea
in addition to any other stopping criterion.
We discuss direct methods for solving systems of linear equations in
Sect. 6.2. A direct method uses a ﬁxed number of computations that would
in exact arithmetic lead to the solution. We may also refer to the steps in
a direct method as “iterations”; but there is a ﬁxed limit on the number of
them that depends only on the size of the input.
We discuss iterative methods for solving systems of linear equations in
Sect. 6.3. Iterative methods often work well for very large and/or sparse
matrices.
Another general principle in numerical computations is that once an ap-
proximate solution has been obtained (in numerical computing, almost all
solutions are approximate), it is often useful to see if the solution can be im-
proved. This principle is well-illustrated by “iterative reﬁnement” of a solution
to a linear system, which we discuss in Sect. 6.4. Iterative reﬁnement is an
iterative method, but the method yielding the starting approximation may be
a direct method.
6.1 Condition of Matrices
Data are said to be “ill-conditioned” for a particular problem or computation
if the data are likely to cause diﬃculties in the computations, such as severe
loss of precision. More generally, the term “ill-conditioned” is applied to a
problem in which small changes to the input result in large changes in the
output. In the case of a linear system
Ax = b,
the problem of solving the system is ill-conditioned if small changes to some
elements of A or b will cause large changes in the solution x. The concept

6.1 Condition of Matrices
267
of ill-conditionedness is a heuristic generalization of Hadamard’s property of
ill-posedness (see page 121).
Consider, for example, the system of equations
1.000x1 + 0.500x2 = 1.500,
0.667x1 + 0.333x2 = 1.000.
(6.1)
The solution is easily seen to be x1 = 1.000 and x2 = 1.000.
Now consider a small change in the right-hand side:
1.000x1 + 0.500x2 = 1.500,
0.667x1 + 0.333x2 = 0.999.
(6.2)
This system has solution x1 = 0.000 and x2 = 3.000. That is a relatively large
change.
Alternatively, consider a small change in one of the elements of the coeﬃ-
cient matrix:
1.000x1 + 0.500x2 = 1.500,
0.667x1 + 0.334x2 = 1.000.
(6.3)
The solution now is x1 = 2.000 and x2 = −1.000; again, a relatively large
change.
In both cases, small changes of the order of 10−3 in the input (the el-
ements of the coeﬃcient matrix or the right-hand side) result in relatively
large changes (of the order of 1) in the output (the solution). Solving the
system (either one of them) is an ill-conditioned problem when our relevant
scales are of order 3.
The nature of the data that cause ill-conditioning depends on the type
of problem. The case above can be considered as a geometric problem of
determining the point of intersection of two lines. The problem is that the
lines represented by the equations are almost parallel, as seen in Fig. 6.1,
and so their point of intersection is very sensitive to slight changes in the
coeﬃcients deﬁning the lines.
The problem can also be described in terms of the angle between the lines.
When the angle is small, but not necessarily 0, we refer to the condition as
“collinearity”. (This term is somewhat misleading because, strictly speaking,
it should indicate that the angle is exactly 0.) In this example, the cosine of
the angle between the lines, from equation (2.54), is 1 −2 × 10−7. In general,
collinearity (or “multicollinearity”) exists whenever the angle between any
line (that is, vector) and the subspace spanned by any other set of vectors is
small.
6.1.1 Condition Number
For a speciﬁc problem such as solving a system of equations, we may quantify
the condition of the matrix by a condition number. To develop this quantiﬁ-
cation for the problem of solving linear equations, consider a linear system

268
6 Solution of Linear Systems
Ax = b, with A nonsingular and b ̸= 0, as above. Now perturb the system
slightly by adding a small amount, δb, to b, and let ˜b = b + δb. The system
0
1
2
−2
−1
0
1
2
3
4
x 1
x2
0
1
2
−2
−1
0
1
2
3
4
x 1
x2
Figure
6.1.
Almost parallel lines: ill-conditioned coeﬃcient matrices, equa-
tions (6.1) and (6.2)
A˜x = ˜b
has a solution ˜x = δx + x = A−1˜b. (Notice that δb and δx do not necessarily
represent scalar multiples of the respective vectors.) If the system is well-
conditioned, for any reasonable norm, if ∥δb∥/∥b∥is small, then ∥δx∥/∥x∥is
likewise small.
From δx = A−1δb and the inequality (3.280) (page 165), for an induced
norm on A, we have
∥δx∥≤∥A−1∥∥δb∥.
(6.4)
Likewise, because b = Ax, we have
1
∥x∥≤∥A∥1
∥b∥,
(6.5)
and equations (6.4) and (6.5) together imply
∥δx∥
∥x∥≤∥A∥∥A−1∥∥δb∥
∥b∥.
(6.6)

6.1 Condition of Matrices
269
This provides a bound on the change in the solution ∥δx∥/∥x∥in terms of the
perturbation ∥δb∥/∥b∥.
The bound in equation (6.6) motivates us to deﬁne the condition number
with respect to inversion, denoted by κ(·), as
κ(A) = ∥A∥∥A−1∥
(6.7)
for nonsingular A. In the context of linear algebra, the condition number with
respect to inversion is so dominant in importance that we generally just refer
to it as the “condition number”. This condition number also provides a bound
on changes in eigenvalues due to perturbations of the matrix, as we will see in
inequality (7.1). (That particular bound is of more theoretical interest than
of practical value.)
A condition number is a useful measure of the condition of A for the
problem of solving a linear system of equations. There are other condition
numbers useful in numerical analysis, however, such as the condition number
for computing the sample variance (see equation (10.3.2.7) on page 504) or a
condition number for a root of a function.
We can write equation (6.6) as
∥δx∥
∥x∥≤κ(A)∥δb∥
∥b∥,
(6.8)
and, following a development similar to that above, write
∥δb∥
∥b∥≤κ(A)∥δx∥
∥x∥.
(6.9)
These inequalities, as well as the other ones we write in this section, are sharp,
as we can see by letting A = I.
Because the condition number is an upper bound on a quantity that we
would not want to be large, a large condition number is “bad”.
Notice that our deﬁnition of the condition number does not specify the
norm; it only requires that the norm be an induced norm. (An equivalent
deﬁnition does not rely on the norm being an induced norm.) We sometimes
specify a condition number with regard to a particular norm, and just as we
sometimes denote a speciﬁc norm by a special symbol, we may use a special
symbol to denote a speciﬁc condition number. For example, κp(A) may denote
the condition number of A in terms of an Lp norm. Most of the properties of
condition numbers (but not their actual values) are independent of the norm
used.
The coeﬃcient matrix in equations (6.1) and (6.2) is
A =
!
1.000 0.500
0.667 0.333
"
,

270
6 Solution of Linear Systems
and its inverse is
A−1 =
! −666 1000
1344 −2000
"
.
It is easy to see that
∥A∥1 = 1.667
and
∥A−1∥1 = 3000;
hence,
κ1(A) = 5001.
Likewise,
∥A∥∞= 1.500
and
∥A−1∥∞= 3344;
hence,
κ∞(A) = 5016.
Notice that the condition numbers are not exactly the same, but they are close.
Notice also that the condition numbers are of the order of magnitude of the
ratio of the output perturbation to the input perturbation in those equations.
Although we used this matrix in an example of ill-conditioning, these con-
dition numbers, although large, are not so large as to cause undue concern for
numerical computations. Indeed, solving the systems of equations (6.1), (6.2),
and (6.3) would not cause problems for a computer program.
An interesting relationship for the L2 condition number is
κ2(A) =
maxx̸=0
∥Ax∥
∥x∥
minx̸=0
∥Ax∥
∥x∥
(6.10)
(see Exercise 6.1, page 305). The numerator and denominator in equa-
tion (6.10) look somewhat like the maximum and minimum eigenvalues, as
we have suggested. Indeed, the L2 condition number is just the ratio of the
largest eigenvalue in absolute value to the smallest (see page 166). The L2
condition number is also called the spectral condition number.
The eigenvalues of the coeﬃcient matrix in equations (6.1) and (6.2) are
1.333375 and −0.0003750, and so
κ2(A) = 3555.67,
which is the same order of magnitude as κ∞(A) and κ1(A) computed above.
Some useful facts about condition numbers are:

6.1 Condition of Matrices
271
•
κ(A) = κ(A−1),
•
κ(cA) = κ(A),
for c ̸= 0,
•
κ(A) ≥1,
•
κ1(A) = κ∞(AT),
•
κ2(AT) = κ2(A),
•
κ2(ATA) = κ2
2(A)
≥κ2(A), and
•
if A and B are orthogonally similar (equation (3.242)), then
∥A∥2 = ∥B∥2
and
κ2(A) = κ2(B)
(see equation (3.286)).
Even though the condition number provides a very useful indication of the
condition of the problem of solving a linear system of equations, it can be
misleading at times. Consider, for example, the coeﬃcient matrix
A =
!1 0
0 ϵ
"
,
where 0 < ϵ < 1. The condition numbers are
κ1(A) = κ2(A) = κ∞(A) = 1
ϵ ,
and so if ϵ is small, the condition number is large. It is easy to see, however,
that small changes to the elements of A or b in the system Ax = b do not cause
undue changes in the solution (our heuristic deﬁnition of ill-conditioning). In
fact, the simple expedient of multiplying the second row of A by 1/ϵ (that is,
multiplying the second equation, a21x1 + a22x2 = b2, by 1/ϵ) yields a linear
system that is very well-conditioned.
This kind of apparent ill-conditioning is called artiﬁcial ill-conditioning.
It is due to the diﬀerent rows (or columns) of the matrix having a very dif-
ferent scale; the condition number can be changed just by scaling the rows or
columns. This usually does not make a linear system any better or any worse
conditioned.
In Sect. 6.1.3 we relate the condition number to bounds on the numerical
accuracy of the solution of a linear system of equations.
The relationship between the size of the matrix and its condition number
is interesting. In general, we would expect the condition number to increase
as the size increases. This is the case, but the nature of the increase depends
on the type of elements in the matrix. If the elements are randomly and
independently distributed as normal or uniform with a mean of zero and
variance of one, the increase in the condition number is approximately linear
in the size of the matrix (see Exercise 10.23, page 521).

272
6 Solution of Linear Systems
Our deﬁnition of condition number given above is for nonsingular matri-
ces. We can formulate a useful alternate deﬁnition that extends to singular
matrices and to nonsquare matrices: the condition number of a matrix is the
ratio of the largest singular value to the smallest nonzero singular value, and
of course this is the same as the deﬁnition for square nonsingular matrices.
This is also called the spectral condition number.
The condition number, like the determinant, is not easy to compute
(see page 535 in Sect. 11.4).
6.1.2 Improving the Condition Number
Sometimes the condition number reﬂects an essential characteristic of the
problem being addressed. In other cases, it is only an artifact of the model
formulation for the problem, as in the case of artiﬁcial ill-conditioning, in
which a simple change of units removes the ill-conditioning.
In other cases, we change the problem slightly. For example, we can im-
prove the condition number of a matrix by adding a diagonal matrix. Although
this fact holds in general, we consider only one special case. Let A be positive
deﬁnite, and let d > 0. Then, for the spectral condition number, we have
κ2(A + dI) < κ2(A).
(6.11)
To see this, let ci be an eigenvalue of A (which is positive). We have
max(ci + d)
min(ci + d) < max(ci)
min(ci)
for d > 0.
6.1.2.1 Ridge Regression and the Condition Number
Ridge regression, which we will discuss brieﬂy beginning on page 364, and
again beginning on page 431, is a form of regularization of an inverse prob-
lem for which we have developed a set of approximate linear equations in the
variable β, y ≈Xβ. One possible solution to the underlying problem is char-
acterized by the equations XTXβ = XTy. This solution satisﬁes a criterion
that in many cases is not an essential aspect of the inverse problem itself.
(I am using “inverse problem” here in its usual sense; here “inverse” does not
refer to an inverse of a matrix.)
The matrix XTX may be ill-conditioned, or even singular, but its eigenval-
ues are nonnegative. The matrix XTX +λI for λ > 0, however, is nonsingular
and has a smaller condition number,
κ2(XTX + λI) < κ2(XTX),
(6.12)
as we have seen above.
In applications, the ill-conditioning of XTX may imply other issues relat-
ing to the statistical questions being addressed in the inverse problem.

6.1 Condition of Matrices
273
6.1.3 Numerical Accuracy
The condition numbers we have deﬁned are useful indicators of the accuracy
we may expect when solving a linear system Ax = b. Suppose the entries of
the matrix A and the vector b are accurate to approximately p decimal digits,
so we have the system
(A + δA) (x + δx) = b + δb
with
∥δA∥
∥A∥≈10−p
and
∥δb∥
∥b∥≈10−p.
Assume A is nonsingular, and suppose that the condition number with respect
to inversion, κ(A), is approximately 10t, so
κ(A)∥δA∥
∥A∥≈10t−p.
Ignoring the approximation of b (that is, assuming δb = 0), we can write
δx = −A−1δA(x + δx),
which, together with the triangular inequality and inequality (3.280) on
page 165, yields the bound
∥δx∥≤∥A−1∥∥δA∥

∥x∥+ ∥δx∥

.
Using equation (6.7) with this, we have
∥δx∥≤κ(A)∥δA∥
∥A∥

∥x∥+ ∥δx∥

or

1 −κ(A)∥δA∥
∥A∥

∥δx∥≤κ(A)∥δA∥
∥A∥∥x∥.
If the condition number is not too large relative to the precision (that is, if
10t−p ≪1), then we have
∥δx∥
∥x∥≈κ(A)∥δA∥
∥A∥
≈10t−p.
(6.13)

274
6 Solution of Linear Systems
Expression (6.13) provides a rough bound on the accuracy of the solution
in terms of the precision of the data and the condition number of the coeﬃcient
matrix. This result must be used with some care, however, and there are cases
where the bound is violated or where it is not very tight. Rust (1994), among
others, points out failures of the condition number for setting bounds on the
accuracy of the solution.
Another consideration in the practical use of expression (6.13) is the fact
that the condition number is usually not known, and methods for computing
it suﬀer from the same rounding problems as the solution of the linear system
itself. In Sect. 11.4, we describe ways of estimating the condition number, but
as the discussion there indicates, these estimates are often not very reliable.
We would expect the norms in the expression (6.13) to be larger for larger
size problems. The approach taken above addresses a type of “total” error.
It may be appropriate to scale the norms to take into account the number of
elements. Chaitin-Chatelin and Frayss´e (1996) discuss error bounds for indi-
vidual elements of the solution vector and condition measures for elementwise
error.
Another approach to determining the accuracy of a solution is to use ran-
dom perturbations of A and/or b and then to estimate the eﬀects of the
perturbations on x. Stewart (1990) discusses ways of doing this. Stewart’s
method estimates error measured by a norm, as in expression (6.13). Kenney
and Laub (1994) and Kenney, Laub, and Reese (1998) describe an estimation
method to address elementwise error.
Higher accuracy in computations for solving linear systems can be achieved
in various ways: multiple precision, interval arithmetic, and residue arithmetic
(see pages 495 and 493). Stallings and Boullion (1972) and Keller-McNulty
and Kennedy (1986) describe ways of using residue arithmetic in some linear
computations for statistical applications.
Another way of improving the accuracy is by using iterative reﬁnement,
which we discuss in Sect. 6.4.
6.2 Direct Methods for Consistent Systems
There are two general approaches to solving the linear system Ax = b, direct
and iterative. In this section, we discuss direct methods, which usually proceed
by a factorization of the coeﬃcient matrix.
6.2.1 Gaussian Elimination and Matrix Factorizations
The most common direct method for the solution of linear systems is Gaus-
sian elimination. The basic idea in this method is to form equivalent sets of
equations, beginning with the system to be solved, Ax = b, or
aT
1∗x = b1

6.2 Direct Methods for Consistent Systems
275
aT
2∗x = b2
. . . = . . .
aT
n∗x = bn,
where aj∗is the jth row of A. An equivalent set of equations can be formed
by a sequence of elementary operations on the equations in the given set.
These elementary operations on equations are essentially the same as the
elementary operations on the rows of matrices discussed in Sect. 3.2.3 and in
Sect. 5.7. There are three kinds of elementary operations:
•
an interchange of two equations,
aT
j∗x = bj ←aT
k∗x = bk,
aT
k∗x = bk ←aT
j∗x = bj,
which aﬀects two equations simultaneously,
•
a scalar multiplication of a given equation,
aT
j∗x = bj
←
caT
j∗x = cbj,
and
•
a replacement of a single equation by an axpy operation (a sum of it and
a scalar multiple of another equation),
aT
j∗x = bj
←
aT
j∗x + caT
k∗x = bj + cbk.
The interchange operation can be accomplished by premultiplication by
an elementary permutation matrix (see page 81):
EjkAx = Ejkb.
The scalar multiplication can be performed by premultiplication by an ele-
mentary transformation matrix Ej(c), and the axpy operation can be eﬀected
by premultiplication by an Ejk(c) elementary transformation matrix.
The elementary operation on the equation
aT
2∗x = b2
in which the ﬁrst equation is combined with it using c1 = −a21/a11 and
c2 = 1 will yield an equation with a zero coeﬃcient for x1. Generalizing this,
we perform elementary operations on the second through the nth equations
to yield a set of equivalent equations in which all but the ﬁrst have zero
coeﬃcients for x1.
Next, we perform elementary operations using the second equation with
the third through the nth equations, so that the new third through the nth
equations have zero coeﬃcients for x2. This is the kind of sequence of multipli-
cations by elementary operator matrices shown in equation (3.62) on page 84
and grouped together as Lk in equation (5.25) on page 244.

276
6 Solution of Linear Systems
The sequence of equivalent equations, beginning with Ax = b, is
a11x1 + a12x2 +
· · ·
+
a1nxn
=
b1
a21x1 + a22x2 +
· · ·
+
a2nxn
=
b2
(0)
...
+
...
...
...
an1x1 + an2x2 +
· · ·
+
annxn
=
bn
,
(6.14)
then A(1)x = b(1), or L1Ax = L1b,
a11x1 + a12x2 +
· · ·
+
a1nxn
=
b1
a(1)
22 x2 +
· · ·
+
a(1)
2n xn
=
b(1)
2
(1)
...
+
· · ·
+
...
...
a(1)
n2 x2 +
· · ·
+
a(1)
nnxn
=
b(1)
n
,
(6.15)
...
...
and ﬁnally A(n)x = b(n), or Ln−1 · · · L1Ax = Ln−1 · · · L1b, or Ux =
Ln−1 · · · L1b,
a11x1 + a12x2 +
· · ·
+
a1nxn
=
b1
a(1)
22 x2 +
· · ·
+
a(1)
2n xn
=
b(1)
2
(n −1)
...
...
...
a(n−2)
n−1,n−1xn−1 + a(n−2)
n−1,nxn = b(n−2)
n−1
a(n−1)
nn
xn = b(n−1)
n
.
(6.16)
Recalling equation (5.28), we see that the last system is Ux = L−1b. This
system is easy to solve because the coeﬃcient matrix is upper triangular. The
last equation in the system yields
xn = b(n−1)
n
a(n−1)
nn
.
By back substitution, we get
xn−1 = (b(n−2)
n−1
−a(n−2)
n−1,nxn)
a(n−2)
n−1,n−1
,
and we obtain the rest of the xs in a similar manner. This back substitution
is equivalent to forming
x = U −1L−1b,
(6.17)
or x = A−1b with A = LU.
Gaussian elimination consists of two steps: the forward reduction, which
is of order O(n3), and the back substitution, which is of order O(n2).

6.2 Direct Methods for Consistent Systems
277
6.2.1.1 Pivoting
The only obvious problem with this method arises if some of the a(k−1)
kk
s used
as divisors are zero (or very small in magnitude). These divisors are called
“pivot elements”.
Suppose, for example, we have the equations
0.0001x1 + x2 = 1,
x1 + x2 = 2.
The solution is x1 = 1.0001 and x2 = 0.9999. Suppose we are working with
three digits of precision (so our solution is x1 = 1.00 and x2 = 1.00). After
the ﬁrst step in Gaussian elimination, we have
0.0001x1 +
x2 =
1,
−10, 000x2 = −10, 000,
and so the solution by back substitution is x2 = 1.00 and x1 = 0.000. The
L2 condition number of the coeﬃcient matrix is 2.618, so even though the
coeﬃcients vary greatly in magnitude, we certainly would not expect any
diﬃculty in solving these equations.
A simple solution to this potential problem is to interchange the equation
having the small leading coeﬃcient with an equation below it. Thus, in our
example, we ﬁrst form
x1 + x2 = 2,
0.0001x1 + x2 = 1,
so that after the ﬁrst step we have
x1 + x2 = 2,
x2 = 1,
and the solution is x2 = 1.00 and x1 = 1.00, which is correct to three digits.
Another strategy would be to interchange the column having the small
leading coeﬃcient with a column to its right. Both the row interchange and the
column interchange strategies could be used simultaneously, of course. These
processes, which obviously do not change the solution, are called pivoting.
The equation or column to move into the active position may be chosen in
such a way that the magnitude of the new diagonal element is the largest
possible.
Performing only row interchanges, so that at the kth stage the equation
with
n
max
i=k |a(k−1)
ik
|
is moved into the kth row, is called partial pivoting. Performing both row
interchanges and column interchanges, so that

278
6 Solution of Linear Systems
n;n
max
i=k;j=k |a(k−1)
ij
|
is moved into the kth diagonal position, is called complete pivoting. See Exer-
cises 6.2a and 6.2b.
It is always important to distinguish descriptions of eﬀects of actions from
the actions that are actually carried out in the computer. Pivoting is “inter-
changing” rows or columns. We would usually do something like that in the
computer only when we are ﬁnished and want to produce some output. In the
computer, a row or a column is determined by the index identifying the row
or column. All we do for pivoting is to keep track of the indices that we have
permuted.
There are many more computations required in order to perform complete
pivoting than are required to perform partial pivoting. Gaussian elimination
with complete pivoting can be shown to be stable; that is, the algorithm yields
an exact solution to a slightly perturbed system, (A + δA)x = b. (We discuss
stability on page 502.) For Gaussian elimination with partial pivoting, there
are examples that show that it is not stable. These examples are somewhat
contrived, however, and experience over many years has indicated that Gaus-
sian elimination with partial pivoting is stable for most problems occurring in
practice. For this reason, together with the computational savings, Gaussian
elimination with partial pivoting is one of the most commonly used meth-
ods for solving linear systems. See Golub and Van Loan (1996) for a further
discussion of these issues.
There are two modiﬁcations of partial pivoting that result in stable algo-
rithms. One is to add one step of iterative reﬁnement (see Sect. 6.4, page 286)
following each pivot. It can be shown that Gaussian elimination with partial
pivoting together with one step of iterative reﬁnement is unconditionally sta-
ble (Skeel 1980). Another modiﬁcation is to consider two columns for possible
interchange in addition to the rows to be interchanged. This does not re-
quire nearly as many computations as complete pivoting does. Higham (1997)
shows that this method, suggested by Bunch and Kaufman (1977) and used
in LINPACK and LAPACK (see page 558), is stable.
6.2.1.2 Nonfull Rank and Nonsquare Systems
The existence of an x that solves the linear system Ax = b depends on that
system being consistent; it does not depend on A being square or of full rank.
The methods discussed above also apply in this case. (See the discussion of LU
and QR factorizations for nonfull rank and nonsquare matrices on pages 243
and 251.) In applications, it is often annoying that many software developers
do not provide capabilities for handling such systems. Many of the standard
programs for solving systems provide solutions only if A is square and of full
rank. This is a poor design decision.

6.3 Iterative Methods for Consistent Systems
279
6.2.2 Choice of Direct Method
Direct methods of solving linear systems all use some form of matrix factor-
ization, as discussed in Chap. 5. The LU factorization is the most commonly
used method to solve a linear system.
For certain patterned matrices, other direct methods may be more eﬃcient.
If a given matrix initially has a large number of zeros, it is important to
preserve the zeros in the same positions (or in other known positions) in
the matrices that result from operations on the given matrix. This helps to
avoid unnecessary computations. The iterative methods discussed in the next
section are often more useful for sparse matrices.
Another important consideration is how easily an algorithm lends itself to
implementation on advanced computer architectures. Many of the algorithms
for linear algebra can be vectorized easily. It is now becoming more important
to be able to parallelize the algorithms. The iterative methods discussed in
the next section can often be parallelized more easily.
6.3 Iterative Methods for Consistent Systems
In iterative methods for solving the linear system Ax = b, we begin with
starting point x(0), which we consider to be an approximate solution, and
then move through a sequence of successive approximations x(1), x(2), . . ., that
ultimately (it is hoped!) converge to a solution. The user must specify a con-
vergence criterion to determine when the approximation is close enough to
the solution. The criterion may be based on successive changes in the solution
x(k) −x(k−1) or on the diﬀerence ∥Ax(k) −b∥.
Iterative methods may be particularly useful for very large systems because
it may not be necessary to have the entire A matrix available for computa-
tions in each step. These methods are also useful for sparse systems. Also, as
mentioned above, the iterative algorithms can often be parallelized. Heath,
Ng, and Peyton (1991) review various approaches to parallelizing iterative
methods for solving sparse systems.
6.3.1 The Gauss-Seidel Method with Successive Overrelaxation
One of the simplest iterative procedures is the Gauss-Seidel method. In this
method, ﬁrst rearrange the equations if necessary so that no diagonal element
of the coeﬃcient matrix is 0. We then begin with an initial approximation to
the solution, x(0). We next compute an update for the ﬁrst element of x:
x(1)
1
=
1
a11
⎛
⎝b1 −
n

j=2
a1jx(0)
j
⎞
⎠.

280
6 Solution of Linear Systems
(If a11 is zero or very small in absolute value, we ﬁrst rearrange the equations;
that is, we pivot.)
Continuing in this way for the other elements of x, we have for i = 1, . . . , n
x(1)
i
= 1
aii
⎛
⎝bi −
i−1

j=1
aijx(1)
j
−
n

j=i+1
aijx(0)
j
⎞
⎠,
where no sums are performed if the upper limit is smaller than the lower
limit. After getting the approximation x(1), we then continue this same kind
of iteration for x(2), x(3), . . ..
We continue the iterations until a convergence criterion is satisﬁed. As we
discuss in Sect. 10.3.4, this criterion may be of the form
Δ

x(k), x(k−1)
≤ϵ,
where Δ

x(k), x(k−1)
is a measure of the diﬀerence of x(k) and x(k−1), such as
∥x(k) −x(k−1)∥. We may also base the convergence criterion on ∥r(k) −r(k−1)∥,
where r(k) = b −Ax(k).
The Gauss-Seidel iterations can be thought of as beginning with a rear-
rangement of the original system of equations as
a11x1
= b1 −a12x2 · · · −a1nxn
a21x1
+
a22x2
= b2
· · · −a2nxn
...
+
...
...
...
a(n−1)1x1 + a(n−1)2x2 + · · ·
= bn−1
−annxn
an1x1
+
an2x2
+ · · · + annxn = bn
(6.18)
In this form, we identify three matrices: a diagonal matrix D, a lower trian-
gular L with 0s on the diagonal, and an upper triangular U with 0s on the
diagonal:
(D + L)x = b −Ux.
We can write this entire sequence of Gauss-Seidel iterations in terms of these
three ﬁxed matrices:
x(k+1) = (D + L)−1
−Ux(k) + b

.
(6.19)
6.3.1.1 Convergence of the Gauss-Seidel Method
This method will converge for any arbitrary starting value x(0) if and only if
ρ((D + L)−1U) < 1.
(6.20)

6.3 Iterative Methods for Consistent Systems
281
We see this by considering the diﬀerence x(k) −x and writing
x(k+1) −x = (D + L)−1U(x(k) −x) = ((D + L)−1U)k(x(0) −x).
From (3.314) on page 173, we see that this residual goes to 0 if and only if
ρ((D + L)−1U) < 1.
Moreover, as we stated informally on page 173, the rate of convergence
increases with decreasing spectral radius.
Notice that pivoting rearranges the equations (6.18) (resulting in diﬀerent
D, L, and U) and thus changes the value of ρ((D + L)−1U); hence pivoting
can cause the Gauss-Seidel method to be viable even when it is not with the
original system (see Exercise 6.2e).
6.3.1.2 Successive Overrelaxation
The Gauss-Seidel method may be unacceptably slow, so it may be modiﬁed
so that the update is a weighted average of the regular Gauss-Seidel update
and the previous value. This kind of modiﬁcation is called successive overre-
laxation, or SOR. Instead of equation (6.19), the update is given by
1
ω (D + ωL) x(k+1) = 1
ω

(1 −ω)D −ωU

x(k) + b,
(6.21)
where the relaxation parameter ω is usually chosen to be between 0 and 1.
For ω = 1 the method is the ordinary Gauss-Seidel method; see Exer-
cises 6.2c, 6.2d, and 6.2g.
6.3.2 Conjugate Gradient Methods for Symmetric
Positive Deﬁnite Systems
In the Gauss-Seidel methods the convergence criterion is based on successive
diﬀerences in the solutions x(k) and x(k−1) or in the residuals r(k) and r(k−1).
Other iterative methods focus directly on the magnitude of the residual
r(k) = b −Ax(k).
(6.22)
We seek a value x(k) such that the residual is small (in some sense). Methods
that minimize ∥r(k)∥2 are called minimal residual methods. Two names asso-
ciated with minimum residual methods are MINRES and GMRES, which are
names of speciﬁc algorithms.
For a system with a symmetric positive deﬁnite coeﬃcient matrix A, it
turns out that the best iterative method is based on minimizing the conjugate
L2 norm (see equation (3.94))
∥r(k)TA−1r(k)∥2.
A method based on this minimization problem is called a conjugate gradient
method.

282
6 Solution of Linear Systems
6.3.2.1 The Conjugate Gradient Method
The problem of solving the linear system Ax = b is equivalent to ﬁnding the
minimum of the function
f(x) = 1
2xTAx −xTb.
(6.23)
By setting the derivative of f to 0, we see that a stationary point of f occurs
at the point x where Ax = b (see Sect. 4.4).
If A is positive deﬁnite, the (unique) minimum of f is at x = A−1b,
and the value of f at the minimum is −1
2bTAb. The minimum point can be
approached iteratively by starting at a point x(0), moving to a point x(1)
that yields a smaller value of the function, and continuing to move to points
yielding smaller values of the function. The kth point is x(k−1) +α(k−1)p(k−1),
where α(k−1) is a scalar and p(k−1) is a vector giving the direction of the
movement. Hence, for the kth point, we have the linear combination
x(k) = x(0) + α(1)p(1) + · · · + α(k−1)p(k−1).
At the point x(k), the function f decreases most rapidly in the direction
of the negative gradient, −∇f(x(k)), which is just the residual,
−∇f(x(k)) = r(k).
If this residual is 0, no movement is indicated because we are at the solution.
Moving in the direction of steepest descent may cause a very slow con-
vergence to the minimum. (The curve that leads to the minimum on the
quadratic surface is obviously not a straight line. The direction of steepest
descent changes as we move to a new point x(k+1).) A good choice for the
sequence of directions p(1), p(2), . . . is such that
(p(k))TAp(i) = 0,
for i = 1, . . . , k −1.
(6.24)
Such a vector p(k) is A-conjugate to p(1), p(2), . . . p(k−1) (see page 94). Given
a current point x(k) and a direction to move p(k) to the next point, we must
also choose a distance α(k)∥p(k)∥to move in that direction. We then have the
next point,
x(k+1) = x(k) + α(k)p(k).
(6.25)
(Notice that here, as often in describing algorithms in linear algebra, we use
Greek letters, such as α, to denote scalar quantities.)
We choose the directions as in Newton steps, so the ﬁrst direction is Ar(0)
(see Sect. 4.4.2). The paths deﬁned by the directions p(1), p(2), . . . in equa-
tion (6.24) are called the conjugate gradients. A conjugate gradient method
for solving the linear system is shown in Algorithm 6.1.

6.3 Iterative Methods for Consistent Systems
283
Algorithm 6.1 The conjugate gradient method for solving the sym-
metric positive deﬁnite system Ax = b, starting with x(0)
0. Input stopping criteria, ϵ and kmax.
Set k = 0; r(k) = b−Ax(k); s(k) = Ar(k); p(k) = s(k); and γ(k) = ∥s(k)∥2.
1. If γ(k) ≤ϵ, set x = x(k) and terminate.
2. Set q(k) = Ap(k).
3. Set α(k) =
γ(k)
∥q(k)∥2 .
4. Set x(k+1) = x(k) + α(k)p(k).
5. Set r(k+1) = r(k) −α(k)q(k).
6. Set s(k+1) = Ar(k+1).
7. Set γ(k+1) = ∥s(k+1)∥2.
8. Set p(k+1) = s(k+1) + γ(k+1)
γ(k) p(k).
9. If k < kmax,
set k = k + 1 and go to step 1;
otherwise
issue message that
“algorithm did not converge in kmax iterations”.
There are various ways in which the computations in Algorithm 6.1 could
be arranged. Although any vector norm could be used in Algorithm 6.1, the
L2 norm is the most common one.
This method, like other iterative methods, is more appropriate for large
systems. (“Large” in this context means bigger than 1000 × 1000.)
In exact arithmetic, the conjugate gradient method should converge in n
steps for an n × n system. In practice, however, its convergence rate varies
widely, even for systems of the same size. Its convergence rate generally de-
creases with increasing L2 condition number (which is a function of the max-
imum and minimum nonzero eigenvalues), but that is not at all the complete
story. The rate depends in a complicated way on all of the eigenvalues. The
more spread out the eigenvalues are, the slower the rate. For diﬀerent sys-
tems with roughly the same condition number, the convergence is faster if all
eigenvalues are in two clusters around the maximum and minimum values.
See Greenbaum and Strakoˇs (1992) for an analysis of the convergence rates.
6.3.2.2 Krylov Methods
Notice that the steps in the conjugate gradient algorithm involve the matrix
A only through linear combinations of its rows or columns; that is, in any
iteration, only a vector of the form Av or ATw is used. The conjugate gra-
dient method and related procedures, called Lanczos methods, move through
a Krylov space in the progression to the solution. A Krylov space is the k-
dimensional vector space of order n generated by the n × n matrix A and the
vector v by forming the basis {v, Av, A2v, . . . , Ak−1v}. We often denote this
space as Kk(A, v) or just as Kk:

284
6 Solution of Linear Systems
Kk = V({v, Av, A2v, . . . , Ak−1v}).
(6.26)
Methods for computing eigenvalues are often based on Krylov spaces.
6.3.2.3 GMRES Methods
The conjugate gradient method seeks to minimize the residual vector in equa-
tion (6.22), r(k) = b −Ax(k), and the convergence criterion is based on the
linear combinations of the columns of the coeﬃcient matrix formed by that
vector, ∥Ar(k)∥.
The generalized minimal residual (GMRES) method of Saad and Schultz
(1986) for solving Ax = b begins with an approximate solution x(0) and takes
x(k) as x(k−1) + z(k), where z(k) is the solution to the minimization problem,
min
z∈Kk(A,r(k−1)) ∥r(k−1) −Az∥,
where, as before, r(k) = b−Ax(k). This minimization problem is a constrained
least squares problem. The speed of convergence of GMRES depends very
strongly on the arrangements of the computations. See Walker (1988) and
Walker and Zhou (1994) for details of the methods. Brown and Walker (1997)
consider the behavior of GMRES when the coeﬃcient matrix is singular and
give conditions for GMRES to converge to a solution of minimum length
(the solution corresponding to the Moore-Penrose inverse; see Sect. 6.6.3,
page 293).
6.3.2.4 Preconditioning
As we mentioned above, the convergence rate of the conjugate gradient
method depends on the distribution of the eigenvalues in rather complicated
ways. The ratio of the largest to the smallest (that is, the L2 condition number
is important) and the convergence rate for the conjugate gradient method is
slower for larger L2 condition numbers. The rate also is slower if the eigenval-
ues are spread out, especially if there are several eigenvalues near the largest
or smallest. This phenomenon is characteristic of other Krylov space methods.
One way of addressing the problem of slow convergence of iterative meth-
ods is by preconditioning; that is, by replacing the system Ax = b with another
system,
M −1Ax = M −1b,
(6.27)
where M is very similar (by some measure) to A, but the system M −1Ax =
M −1b has a better condition for the problem at hand. We choose M to be
symmetric and positive deﬁnite, and such that Mx = b is easy to solve. If M is
an approximation of A, then M −1A should be well-conditioned; its eigenvalues
should all be close to each other.

6.3 Iterative Methods for Consistent Systems
285
A problem with applying the conjugate gradient method to the precondi-
tioned system M −1Ax = M −1b is that M −1A may not be symmetric. We can
form an equivalent symmetric system, however, by decomposing the symmet-
ric positive deﬁnite M as M = VCV T and then
M −1/2 = V diag((1/√c11, . . . , 1/√cnn))V T,
as in equation (3.273), after inverting the positive square roots of C. Multiply-
ing both sides of M −1Ax = M −1b by M 1/2, inserting the factor M −1/2M 1/2,
and arranging terms yields
(M −1/2AM −1/2)M 1/2x = M −1/2b.
This can all be done and Algorithm 6.1 can be modiﬁed without explicit for-
mation of and multiplication by M 1/2. The preconditioned conjugate gradient
method is shown in Algorithm 6.2.
Algorithm 6.2 The preconditioned conjugate gradient method for
solving the symmetric positive deﬁnite system Ax = b, starting
with x(0)
0. Input stopping criteria, ϵ and kmax.
Set k = 0; r(k) = b −Ax(k); s(k) = Ar(k); p(k) = M −1s(k); y(k) =
M −1r(k); and γ(k) = y(k)Ts(k).
1. If γ(k) ≤ϵ, set x = x(k) and terminate.
2. Set q(k) = Ap(k).
3. Set α(k) =
γ(k)
∥q(k)∥2 .
4. Set x(k+1) = x(k) + α(k)p(k).
5. Set r(k+1) = r(k) −α(k)q(k).
6. Set s(k+1) = Ar(k+1).
7. Set y(k+1) = M −1r(k+1).
8. Set γ(k+1) = y(k+1)Ts(k+1).
9. Set p(k+1) = M −1s(k+1) + γ(k+1)
γ(k) p(k).
10. If k < kmax,
set k = k + 1 and go to step 1;
otherwise
issue message that
“algorithm did not converge in kmax iterations”.
The choice of an appropriate matrix M is not an easy problem, and we will
not consider the methods here. Benzi (2002) provides a survey of precondi-
tioning methods. We will also mention the preconditioned conjugate gradient
method in Sect. 7.1.4, but there, again, we will refer the reader to other sources
for details.

286
6 Solution of Linear Systems
6.3.3 Multigrid Methods
Iterative methods have important applications in solving diﬀerential equa-
tions. The solution of diﬀerential equations by a ﬁnite diﬀerence discretiza-
tion involves the formation of a grid. The solution process may begin with a
fairly coarse grid on which a solution is obtained. Then a ﬁner grid is formed,
and the solution is interpolated from the coarser grid to the ﬁner grid to be
used as a starting point for a solution over the ﬁner grid. The process is then
continued through ﬁner and ﬁner grids. If all of the coarser grids are used
throughout the process, the technique is a multigrid method. There are many
variations of exactly how to do this. Multigrid methods are useful solution
techniques for diﬀerential equations.
6.4 Iterative Reﬁnement
Once an approximate solution x(0) to the linear system Ax = b is available,
iterative reﬁnement can yield a solution that is closer to the true solution.
The residual
r = b −Ax(0)
is used for iterative reﬁnement. Clearly, if h = A+r, then x(0) +h is a solution
to the original system.
The problem considered here is not just an iterative solution to the linear
system, as we discussed in Sect. 6.3. Here, we assume x(0) was computed
accurately given the ﬁnite precision of the computer. In this case, it is likely
that r cannot be computed accurately enough to be of any help. If, however,
r can be computed using a higher precision, then a useful value of h can be
computed. This process can then be iterated as shown in Algorithm 6.3.
Algorithm 6.3 Iterative reﬁnement of the solution to Ax = b,
starting with x(0)
0. Input stopping criteria, ϵ and kmax.
Set k = 0.
1. Compute r(k) = b −Ax(k) in higher precision.
2. Compute h(k) = A+r(k).
3. Set x(k+1) = x(k) + h(k).
4. If ∥h(k)∥≤ϵ∥x(k+1)∥, then
set x = x(k+1) and terminate; otherwise,
if k < kmax,
set k = k + 1 and go to step 1;
otherwise,
issue message that
“algorithm did not converge in kmax iterations”.

6.5 Updating a Solution to a Consistent System
287
In step 2, if A is of full rank then A+ is A−1. Also, as we have emphasized
already, the fact that we write an expression such as A+r does not mean that
we compute A+. The norm in step 4 is usually chosen to be the ∞norm.
The algorithm may not converge, so it is necessary to have an alternative exit
criterion, such as a maximum number of iterations.
The use of iterative reﬁnement as a general-purpose method is severely
limited by the need for higher precision in step 1. On the other hand, if
computations in higher precision can be performed, they can be applied to
step 2—or just in the original computations for x(0). In terms of both accu-
racy and computational eﬃciency, using higher precision throughout is usually
better.
6.5 Updating a Solution to a Consistent System
In applications of linear systems, it is often the case that after the system
Ax = b has been solved, the right-hand side is changed and the system Ax = c
must be solved. If the linear system Ax = b has been solved by a direct
method using one of the factorizations discussed in Chap. 5, the factors of A
can be used to solve the new system Ax = c. If the right-hand side is a small
perturbation of b, say c = b+ δb, an iterative method can be used to solve the
new system quickly, starting from the solution to the original problem.
If the coeﬃcient matrix in a linear system Ax = b is perturbed to result
in the system (A + δA)x = b, it may be possible to use the solution x0 to the
original system eﬃciently to arrive at the solution to the perturbed system.
One way, of course, is to use x0 as the starting point in an iterative procedure.
Often, in applications, the perturbations are of a special type, such as
A = A −uvT,
where u and v are vectors. (This is a “rank-one” perturbation of A, and
when the perturbed matrix is used as a transformation, it is called a “rank-
one” update. As we have seen, a Householder reﬂection is a special rank-one
update.) Assuming A is an n × n matrix of full rank, it is easy to write A−1
in terms of A−1:
A−1 = A−1 + α(A−1u)(vTA−1)
(6.28)
with
α =
1
1 −vTA−1u.
These are called the Sherman-Morrison formulas (from Sherman and Morrison
1950). A−1 exists so long as vTA−1u ̸= 1. Because x0 = A−1b, the solution to
the perturbed system is
˜x0 = x0 + (A−1u)(vTx0)
(1 −vTA−1u) .

288
6 Solution of Linear Systems
If the perturbation is more than rank one (that is, if the perturbation is
A = A −UV T,
(6.29)
where U and V are n × m matrices with n ≥m), a generalization of the
Sherman-Morrison formula, sometimes called the Woodbury formula, is
A−1 = A−1 + A−1U(Im −V TA−1U)−1V TA−1
(6.30)
(from Woodbury 1950). The solution to the perturbed system is easily seen
to be
˜x0 = x0 + A−1U(Im −V TA−1U)−1V Tx0.
As we have emphasized many times, we rarely compute the inverse of a ma-
trix, and so the Sherman-Morrison-Woodbury formulas are not used directly.
Having already solved Ax = b, it should be easy to solve another system,
say Ay = ui, where ui is a column of U. If m is relatively small, as it is in
most applications of this kind of update, there are not many systems Ay = ui
to solve. Solving these systems, of course, yields A−1U, the most formidable
component of the Sherman-Morrison-Woodbury formula. The system to solve
is of order m also.
Occasionally the updating matrices in equation (6.29) may be used with a
weighting matrix, so we have A = A −UWV T. An extension of the Sherman-
Morrison-Woodbury formula is
(A −UWV T)−1 = A−1 + A−1U(W −1 −V TA−1U)−1V TA−1.
(6.31)
This is sometimes called the Hemes formula. (The attributions of discovery
are somewhat murky, and statements made by historians of science of the
form “
was the ﬁrst to
” must be taken with a grain of salt; not every
discovery has resulted in an available publication. This is particularly true in
numerical analysis, where scientiﬁc programmers often just develop a method
in the process of writing code and have neither the time nor the interest in
getting a publication out of it.)
Another situation that requires an update of a solution occurs when the
system is augmented with additional equations and more variables:
!
A A12
A21 A22
" !
x
x+
"
=
!
b
b+
"
.
A simple way of obtaining the solution to the augmented system is to use the
solution x0 to the original system in an iterative method. The starting point
for a method based on Gauss-Seidel or a conjugate gradient method can be
taken as (x0, 0), or as (x0, x(0)
+ ) if a better value of x(0)
+ is known.
In many statistical applications, the systems are overdetermined, with A
being n×m and n > m. In the next section, we consider the general problem of
solving overdetermined systems by using least squares, and then in Sect. 6.6.5
we discuss updating a least squares solution to an overdetermined system.

6.6 Overdetermined Systems: Least Squares
289
6.6 Overdetermined Systems: Least Squares
In applications, linear systems are often used as models of relationships be-
tween one observable variable, a “response”, and another group of observable
variables, “predictor variables”. The model is unlikely to ﬁt exactly any set
of observed values of responses and predictor variables. This may be due to
eﬀects of other predictor variables that are not included in the model, mea-
surement error, the relationship among the variables being nonlinear, or some
inherent randomness in the system. In such applications, we generally take
a larger number of observations than there are variables in the system; thus,
with each set of observations on the response and associated predictors making
up one equation, we have a system with more equations than variables.
An overdetermined system may be written as
Xb ≈y,
(6.32)
where X is n×m and rank(X|y) > m; that is, the system is not consistent. We
have changed the notation slightly from the consistent systems Ax = b that
we have been using because now we have in mind statistical applications and
in those the notation y ≈Xβ is more common. The problem is to determine
a value of b that makes the approximation close in some sense. In applications
of linear systems, we refer to this as “ﬁtting” the system, which is referred to
as a “model”.
Overdetermined systems abound in ﬁtting equations to data. The usual
linear regression model is an overdetermined system and we discuss regression
problems further in Sect. 9.3.2. We should not confuse statistical inference
with ﬁtting equations to data, although the latter task is a component of the
former activity. In this section, we consider some of the more mechanical and
computational aspects of the problem.
6.6.0.1 Accounting for an Intercept
Given a set of observations, the ith row of the system Xb ≈y represents the
linear relationship between yi and the corresponding xs in the vector xi:
yi ≈b1x1i + · · · + bmxmi.
A diﬀerent formulation of the relationship between yi and the corresponding
xs might include an intercept term:
yi ≈˜b0 + ˜b1x1i + · · · + ˜bmxmi.
There are two ways to incorporate this intercept term. One way is just to
include a column of 1s in the X matrix. This approach makes the matrix X
in equation (6.32) n × (m + 1), or else it means that we merely redeﬁne x1i
to be the constant 1. Another way is to assume that the model is an exact ﬁt

290
6 Solution of Linear Systems
for some set of values of y and the xs. If we assume that the model ﬁts y = 0
and x = 0 exactly, we have a model without an intercept (that is, with a zero
intercept).
Often, a reasonable assumption is that the model may have a nonzero
intercept, but it ﬁts the means of the set of observations; that is, the equation
is exact for y = ¯y and x = ¯x, where the jth element of ¯x is the mean of
the jth column vector of X. (Students with some familiarity with the subject
may think this is a natural consequence of ﬁtting the model. It is not unless
the model ﬁtting is by ordinary least squares.) If we require that the ﬁtted
equation be exact for the means (or if this happens naturally, as in the case of
ordinary least squares), we may center each column by subtracting its mean
from each element in the same manner as we centered vectors on page 48. In
place of y, we have the vector y −¯y. The matrix formed by centering all of
the columns of a given matrix is called a centered matrix, and if the original
matrix is X, we represent the centered matrix as Xc in a notation analogous
to what we introduced for centered vectors. If we represent the matrix whose
ith column is the constant mean of the ith column of X as X,
Xc = X −X.
Using the centered data provides two linear systems: a set of approximate
equations in which the intercept is ignored and an equation that ﬁts the point
that is assumed to be satisﬁed exactly:
¯y = Xb.
In the rest of this section, we will generally ignore the question of an
intercept. Except in a method discussed on page 304, the X can be considered
to include a column of 1s, to be centered, or to be adjusted by any other point.
We will return to this idea of centering the data in Sect. 8.6.3.
6.6.1 Least Squares Solution of an Overdetermined System
Although there may be no b that will make the system in (6.32) an equation,
the system can be written as the equation
Xb = y −r,
(6.33)
where r is an n-vector of possibly arbitrary residuals or “errors”.
A least squares solution *b to the system in (6.32) is one such that the
Euclidean norm of the vector of residuals is minimized; that is, the solution
to the problem
min
b
∥y −Xb∥2.
(6.34)
The least squares solution is also called the “ordinary least squares” (OLS) ﬁt.

6.6 Overdetermined Systems: Least Squares
291
By rewriting the square of this norm as
(y −Xb)T(y −Xb),
(6.35)
diﬀerentiating, and setting it equal to 0, we see that the minimum (of both
the norm and its square) occurs at the *b that satisﬁes the square system
XTX*b = XTy.
(6.36)
The system (6.36) is called the normal equations. The matrix XTX is
called the Gram matrix or the Gramian (see Sect. 8.6.1). Its condition deter-
mines the expected accuracy of a solution to the least squares problem. As
we mentioned in Sect. 6.1, however, because the condition number of XTX is
the square of the condition number of X, it may be better to work directly on
X in (6.32) rather than to use the normal equations. The normal equations
are useful expressions, however, whether or not they are used in the compu-
tations. This is another case where a formula does not deﬁne an algorithm, as
with other cases we have encountered many times. We should note, of course,
that any information about the stability of the problem that the Gramian
may provide can be obtained from X directly.
6.6.1.1 Orthogonality of Least Squares Residuals to span(X)
The least squares ﬁt to the overdetermined system has a very useful prop-
erty with important consequences. The least squares ﬁt partitions the space
into two interpretable orthogonal spaces. As we see from equation (6.36), the
residual vector y −X*b is orthogonal to each column in X:
XT(y −X*b) = 0.
(6.37)
This fact illustrates the close relationship of least squares approximations to
the Gram-Schmidt transformations discussed on page 38.
A consequence of this orthogonality for models that include an intercept
is that the sum of the residuals is 0. (The residual vector is orthogonal to the
1 vector.) Another consequence for models that include an intercept is that
the least squares solution provides an exact ﬁt to the mean.
The orthogonality of the residuals of a least squares ﬁt to all columns of X
characterizes the least squares ﬁt. To see this, let b be such that XT(y−Xb) =
0. Now, for any b,
∥y −Xb∥2
2 = ∥y −Xb + (Xb −Xb)∥2
2
= ∥y −Xb∥2
2 + 2(y −Xb)T(Xb −Xb) + ∥Xb −Xb∥2
2
= ∥y −Xb∥2
2 + 2(y −Xb)TX(b −b) + ∥Xb −Xb∥2
2
= ∥y −Xb∥2
2 + 2(XT(y −Xb))T(b −b) + ∥Xb −Xb∥2
2
= ∥y −Xb∥2
2 + ∥Xb −Xb∥2
2
≥∥y −Xb∥2
2;

292
6 Solution of Linear Systems
hence, b is a least squares ﬁt.
These properties are so familiar to statisticians that some think that they
are essential characteristics of any regression modeling; they are not. We will
see in later sections that they do not hold for other approaches to ﬁtting the
basic model y ≈Xb. The least squares solution, however, has some desir-
able statistical properties under fairly common distributional assumptions, as
we discuss in Chap. 9.
6.6.1.2 Numerical Accuracy in Overdetermined Systems
In Sect. 6.1.3, we discussed numerical accuracy in computations for solving
a consistent (square) system of equations and showed how bounds on the
numerical error could be expressed in terms of the condition number of the
coeﬃcient matrix, which we had deﬁned (on page 269) as the product of norms
of the coeﬃcient matrix and its inverse. One of the most useful versions of
this condition number is the one using the L2 matrix norm, which is called
the spectral condition number. This is the most commonly used condition
number, and we generally just denote it by κ(·). The spectral condition num-
ber is the ratio of the largest eigenvalue in absolute value to the smallest in
absolute value, and this extends easily to a deﬁnition of the spectral condition
number that applies both to nonsquare matrices and to singular matrices: the
condition number of a matrix is the ratio of the largest singular value to the
smallest nonzero singular value. As we saw on page 162, the nonzero singular
values of X are the square roots of the nonzero eigenvalues of XTX; hence
κ(XTX) = (κ(X))2.
(6.38)
The condition number of XTX is a measure of the numerical accuracy
we can expect in solving the normal equations (6.36). Because the condition
number of X is smaller, we have an indication that it might be better not
to form the normal equations unless we must. It might be better to work
just with X. This is one of the most important principles in numerical linear
algebra. We will work with X instead of XTX in the next sections.
6.6.2 Least Squares with a Full Rank Coeﬃcient Matrix
If the n × m matrix X is of full column rank, the least squares solution, from
equation (6.36), is *b = (XTX)−1XTy and is obviously unique. A good way to
compute this is to form the QR factorization of X.
First we write X = QR, as in equation (5.35) on page 248, where R is as
in equation (5.36),
R =
!R1
0
"
,

6.6 Overdetermined Systems: Least Squares
293
with R1 an m×m upper triangular matrix. The squared residual norm (6.35)
can be written as
(y −Xb)T(y −Xb) = (y −QRb)T(y −QRb)
= (QTy −Rb)T(QTy −Rb)
= (c1 −R1b)T(c1 −R1b) + cT
2 c2,
(6.39)
where c1 is a vector with m elements and c2 is a vector with n −m elements,
such that
QTy =
 c1
c2

.
(6.40)
Because the squared norm (6.35), and hence, the expression (6.39), is nonneg-
ative, the minimum of the squared residual norm in equation (6.39) occurs
when (c1 −R1b)T(c1 −R1b) = 0; that is, when (c1 −R1b) = 0, or
R1b = c1.
(6.41)
We could also use diﬀerentiation to ﬁnd the minimum of equation (6.39),
because in that case the derivative of cT
2 c2 with respect to b is 0.
Because R1 is triangular, the system is easy to solve: *b = R−1
1 c1. From
equation (5.38), we have
X+ =
'
R−1
1
0
(
QT,
(6.42)
and so we have
*b = X+y.
(6.43)
We also see from equation (6.39) that the minimum of the residual norm
is cT
2 c2. This is called the residual sum of squares in the least squares ﬁt.
6.6.3 Least Squares with a Coeﬃcient Matrix
Not of Full Rank
If X is not of full rank (that is, if X has rank r < m), the least squares solution
is not unique, and in fact a solution is any vector *b = (XTX)−XTy, where
(XTX)−is any generalized inverse. This is a solution to the normal equa-
tions (6.36). If X is not of full rank, equation (6.43), *b = X+y, still provides a
least squares solution, however the Moore-Penrose inverse in equation (6.42)
would be replaced by one as shown in equation (5.45) on page 251.
The residual corresponding to this solution is
y −X(XTX)−XTy = (I −X(XTX)−XT)y.
The residual vector is invariant to the choice of generalized inverse. (We will
see this important fact in equation (8.48) on page 361.)

294
6 Solution of Linear Systems
6.6.3.1 An Optimal Property of the Solution Using
the Moore-Penrose Inverse
The solution corresponding to the Moore-Penrose inverse is unique because, as
we have seen, that generalized inverse is unique. That solution is interesting
for another reason, however: the b from the Moore-Penrose inverse has the
minimum L2-norm of all solutions.
To see that this solution has minimum norm, ﬁrst factor X, as in equa-
tion (5.43) on page 251,
X = QRU T,
and form the Moore-Penrose inverse as in equation (5.45):
X+ = U
!
R−1
1
0
0
0
"
QT.
Then
*b = X+y
(6.44)
is a least squares solution, just as in the full rank case. Now, let
QTy =

c1
c2

,
as in equation (6.40), except ensure that c1 has exactly r elements and c2 has
n −r elements, and let
U Tb =

z1
z2

,
where z1 has r elements. We proceed as in the equations (6.39). We seek
to minimize ∥y −Xb∥2 (which is the square root of the expression in equa-
tions (6.39)); and because multiplication by an orthogonal matrix does not
change the norm, we have
∥y −Xb∥2 = ∥QT(y −XUU Tb)∥2
=



c1
c2

−
!
R1 0
0 0
" 
z1
z2


2
=



c1 −R1z1
c2


2
.
(6.45)
The residual norm is minimized for z1 = R−1
1 c1 and z2 arbitrary. However, if
z2 = 0, then ∥z∥2 is also minimized. Because U Tb = z and U is orthogonal,
∥*b∥2 = ∥z∥2, and so ∥*b∥2 is the minimum among all least squares solutions.

6.6 Overdetermined Systems: Least Squares
295
6.6.4 Weighted Least Squares
One of the simplest variations on ﬁtting the linear model Xb ≈y is to allow
diﬀerent weights on the observations; that is, instead of each row of X and
corresponding element of y contributing equally to the ﬁt, the elements of
X and y are possibly weighted diﬀerently. The relative weights can be put
into an n-vector w and the squared norm in equation (6.35) replaced by a
quadratic form in diag(w). More generally, we form the quadratic form as
(y −Xb)TW(y −Xb),
(6.46)
where W is a positive deﬁnite matrix. Because the weights apply to both y
and Xb, there is no essential diﬀerence in the weighted or unweighted versions
of the problem.
The use of the QR factorization for the overdetermined system in which the
weighted norm (6.46) is to be minimized is similar to the development above.
It is exactly what we get if we replace y −Xb in equation (6.39) or (6.45) by
WC(y −Xb), where WC is the Cholesky factor of W.
6.6.5 Updating a Least Squares Solution of an
Overdetermined System
In Sect. 6.5 on page 287, we considered the problem of updating a given
solution to be a solution to a perturbed consistent system. An overdetermined
system is often perturbed by adding either some rows or some columns to the
coeﬃcient matrix X. This corresponds to including additional equations in
the system,
! X
X+
"
b ≈
! y
y+
"
,
or to adding variables,
' X X+
( !
b
b+
"
≈y.
In either case, if the QR decomposition of X is available, the decomposition
of the augmented system can be computed readily. Consider, for example,
the addition of k equations to the original system Xb ≈y, which has n
approximate equations. With the QR decomposition, for the original full rank
system, putting QTX and QTy as partitions in a matrix, we have
!
R1 c1
0 c2
"
= QT ' X y (
.
Augmenting this with the additional rows yields
⎡
⎣
R
c1
0
c2
X+ y+
⎤
⎦=
!
QT 0
0 I
" !
X
y
X+ y+
"
.
(6.47)

296
6 Solution of Linear Systems
All that is required now is to apply orthogonal transformations, such as Givens
rotations, to the system (6.47) to produce
!R∗c1∗
0 c2∗
"
,
where R∗is an m × m upper triangular matrix and c1∗is an m-vector as
before but c2∗is an (n −m + k)-vector.
The updating is accomplished by applying m rotations to system (6.47) so
as to zero out the (n+q)th row for q = 1, 2, . . . , k. These operations go through
an outer loop with p = 1, 2, . . . , n and an inner loop with q = 1, 2, . . . , k. The
operations rotate R through a sequence R(p,q) into R∗, and they rotate X+
through a sequence X(p,q)
+
into 0. At the p, q step, the rotation matrix Qpq
corresponding to equation (5.12) on page 239 has
cos θ = R(p,q)
pp
r
and
sin θ =

X(p,q)
+

qp
r
,
where
r =
1
R(p,q)
pp
2
+

X(p,q)
+

qp
2
.
Gentleman (1974) and Miller (1992) give Fortran programs that implement
this kind of updating. The software, which was published in Applied Statistics,
is available in statlib (see page 619).
6.7 Other Solutions of Overdetermined Systems
The basic form of an overdetermined linear system may be written as in
equation (6.32) as
Xb ≈y,
where X is n × m and rank(X|y) > m.
As in equation (6.33) in Sect. 6.6.1, we can write this as an equation,
Xb = y −r,
where r is a vector of residuals. Fitting the equation y = Xb means minimizing
r; that is, minimizing some norm of r.

6.7 Other Solutions of Overdetermined Systems
297
There are various norms that may provide a reasonable ﬁt. In Sect. 6.6,
we considered use of the L2 norm; that is, an ordinary least squares (OLS) ﬁt.
There are various other ways of approaching the problem, and we will brieﬂy
consider a few of them in this section.
As we have stated before, we should not confuse statistical inference with
ﬁtting equations to data, although the latter task is a component of the former
activity. Applications in statistical data analysis are discussed in Chap. 9. In
those applications, we need to make statements (that is, assumptions) about
relevant probability distributions. These probability distributions, together
with the methods used to collect the data, may indicate speciﬁc methods for
ﬁtting the equations to the given data. In this section, we continue to address
the more mechanical aspects of the problem of ﬁtting equations to data.
6.7.1 Solutions that Minimize Other Norms of
the Residuals
A solution to an inconsistent, overdetermined system
Xb ≈y,
where X is n × m and rank(X|y) > m, is some value b that makes y −Xb
close to zero. We deﬁne “close to zero” in terms of a norm on y −Xb. The
most common norm, of course, is the L2 norm as in expression (6.34), and
the minimization of this norm is straightforward, as we have seen. In addition
to the simple analytic properties of the L2 norm, the least squares solution
has some desirable statistical properties under fairly common distributional
assumptions, as we have seen.
6.7.1.1 Minimum L1 Norm Fitting: Least Absolute Values
A common alternative norm is the L1 norm. The minimum L1 norm solution
is called the least absolute values ﬁt or the LAV ﬁt. It is not as aﬀected by
outlying observations as the least squares ﬁt is.
Consider a simple example. Assume we have observations on a response,
y = (0, 3, 4, 0, 8), and on a single predictor variable, x = (1, 3, 4, 6, 7). We have
¯y = 3 and ¯x = 4.2. We write the model equation as
y ≈b0 + b1x.
(6.48)
The model with the data is
⎡
⎢⎢⎢⎢⎣
0
3
4
0
8
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
1 1
1 3
1 4
1 6
1 7
⎤
⎥⎥⎥⎥⎦
b + r.
(6.49)

298
6 Solution of Linear Systems
A least squares solution yields ˆb0 = −0.3158 and ˆb1 = 0.7895. With these
values, equation (6.48) goes through the mean (4.2, 3). The residual vector
from this ﬁt is orthogonal to 1 (that is, the sum of the residuals is 0) and to x.
A solution that minimizes the L1 norm is ˜b0 = −1.333 and ˜b1 = 1.333.
The LAV ﬁt may not be unique (although it is in this case). We immediately
note that the least absolute deviations ﬁt does not go through the mean of
the data, nor is the residual vector orthogonal to the 1 vector and to x. The
LAV ﬁt does go through two points in the dataset, however. (This is a special
case of one of several interesting properties of LAV ﬁts, which we will not
discuss here. The interested reader is referred to Kennedy and Gentle 1980,
Chapter 11, for discussion of some of these properties, as well as assumptions
0
2
4
6
8
0
2
4
6
8
x
y
L2
L1
mean
Figure 6.2. OLS and minimum L1 norm ﬁts
about probability distributions that result in desirable statistical properties
for LAV estimators.) A plot of the data, the two ﬁtted lines, and the residuals
is shown in Fig. 6.2.
The problem of minimizing the L1 norm can be formulated as the linear
programming problem
min
b
1T(e+ + e−)
s.t. Xb + Ie+ −Ie−= y
e+, e−≥0
(6.50)
b
unrestricted,

6.7 Other Solutions of Overdetermined Systems
299
where e+ and e−are nonnegative n-vectors. There are special algorithms
that take advantage of the special structure of the problem to speed up the
basic linear programming simplex algorithm (see Kennedy and Gentle 1980;
Chapter 11).
6.7.1.2 Minimum L∞Norm Fitting: Minimax
Another norm that may be useful in some applications is the L∞norm. A so-
lution minimizing that norm is called the least maximum deviation ﬁt. A least
maximum deviation ﬁt is greatly aﬀected by outlying observations. As with
the LAV ﬁt, the least maximum deviation ﬁt does not necessarily go through
the mean of the data. The least maximum deviation ﬁt also may not be unique.
This problem can also be formulated as a linear programming problem, and
as with the least absolute deviations problem, there are special algorithms that
take advantage of the special structure of the problem to speed up the basic
linear programming simplex algorithm. Again, the interested reader is referred
to Kennedy and Gentle (1980; Chapter 11) for a discussion of some of the
properties of minimum L∞norm ﬁts, as well as assumptions about probability
distributions that result in desirable statistical properties for minimum L∞
norm estimators.
6.7.1.3 Lp Norms and Iteratively Reweighted Least Squares
More general Lp norms may also be of interest. For 1 < p < 2, if no element
of y −Xb is zero, we can formulate the pth power of the norm as
∥y −Xb∥p
p = (y −Xb)TW(y −Xb),
(6.51)
where
W = diag(| yi −xT
i b|2−p)
(6.52)
and xT
i is the ith row of X. The formulation (6.51) leads to the iteratively
reweighted least squares (IRLS) algorithm, in which a sequence of weights
W (k) and weighted least squares solutions b(k) are formed as in Algorithm 6.4.
Algorithm 6.4 Iteratively reweighted least squares
0. Input a threshold for a zero residual, ϵ1 (which may be data dependent),
and a large value for weighting zero residuals, wbig. Input stopping
criteria, ϵ2 and kmax.
Set k = 0. Choose an initial value b(k), perhaps as the OLS solution.
1. Compute the diagonal matrix W (k): w(k)
i
= |yi −xT
i b(k)|2−p, except, if
|yi −xT
i b(k)| < ϵ1, set w(k)
i
= wbig.
2. Compute b(k+1) by solving the weighted least squares problem: mini-
mize (y −Xb)TW (k)(y −Xb).

300
6 Solution of Linear Systems
3. If ∥b(k+1) −b(k)∥≤ϵ2, set b = b(k+1) and terminate.
4. If k < kmax,
set k = k + 1 and go to step 1;
otherwise,
issue message that
“algorithm did not converge in kmax iterations”.
Compute b(1) by minimizing equation (6.51) with W = W (0); then compute
W (1) from b(1), and iterate in this fashion. This method is easy to implement
and will generally work fairly well, except for the problem of zero (or small)
residuals. The most eﬀective way of dealing with zero residuals is to set them
to some large value.
Algorithm 6.4 will work for LAV ﬁtting, although the algorithms based on
linear programming alluded to above are better for this task. As mentioned
above, LAV ﬁts generally go through some observations; that is, they ﬁt them
exactly, yielding zero residuals. This means that in using Algorithm 6.4, the
manner of dealing with zero residuals may become an important aspect of the
eﬃciency of the algorithm.
6.7.2 Regularized Solutions
Overdetermined systems often arise because of a belief that some response y
is linearly related to some other set of variables. This relation is expressed in
the system
y ≈Xb.
The fact that y ̸= Xb for any b results because the relationship is not exact.
There is perhaps some error in the measurements. It is also possible that there
is some other variable not included in the columns of X. In addition, there
may be some underlying randomness that could never be accounted for.
In any application in which we ﬁt an overdetermined system, it is likely
that the given values of X and y are only a sample (not necessarily a random
sample) from some universe of interest. Whatever value of b provides the best
ﬁt (in terms of the criterion chosen) may not provide the best ﬁt if some other
equally valid values of X and y were used. The given dataset is ﬁt optimally,
but the underlying phenomenon of interest may not be modeled very well.
The given dataset may suggest relationships among the variables that are not
present in the larger universe of interest. Some element of the “true” b may
be zero, but in the best ﬁt for a given dataset, the value of that element may
be signiﬁcantly diﬀerent from zero. Deciding on the evidence provided by a
given dataset that there is a relationship among certain variables when indeed
there is no relationship in the broader universe is an example of overﬁtting.
There are various approaches we may take to avoid overﬁtting, but there
is no panacea. The problem is inherent in the process.
One approach to overﬁtting is regularization. In this technique, we restrain
the values of b in some way. Minimizing ∥y −Xb∥may yield a b with large

6.7 Other Solutions of Overdetermined Systems
301
elements, or values that are likely to vary widely from one dataset to another.
One way of “regularizing” the solution is to minimize also some norm of b.
The general formulation of the problem then is
min
b (∥y −Xb∥r + λ∥b∥b),
(6.53)
where λ is some appropriately chosen nonnegative number. The norm on the
residuals, ∥· ∥r, and that on the solution vector b, ∥· ∥b, are often chosen
to be the same, and, of course, most often, they are chosen as the L2 norm.
If both norms are the L2 norm, the ﬁtting is called Tikhonov regularization.
In statistical applications, this leads to “ridge regression”. If ∥· ∥r is the L2
norm and ∥· ∥b is the L1 norm, the statistical method is called the “lasso”.
We discuss these formulations brieﬂy in Sect. 9.5.4.
As an example, let us consider the data in equation (6.49) for the equation
y = b0 + b1x.
We found the least squares solution to be ˆb0 = −0.3158 and ˆb1 = 0.7895,
which ﬁts the means and has a residual vector that is orthogonal to 1 and to
x. Now let us regularize the least squares ﬁt with an L2 norm on b and with
λ = 5. (The choice of λ depends on the scaling of the data and a number of
other things we will not consider here. Typically, in an application, various
values of λ are considered.) Again, we face the question of treating b0 and b1
diﬀerently. The regularization, which is a shrinkage, can be applied to both
or just to b1. Furthermore, we have the question of whether we want to force
the equation to ﬁt some data point exactly. In statistical applications, it is
common not to apply the shrinkage to the intercept term and to force the
ﬁtted equation to ﬁt the means exactly. Doing that, we get ˆb1λ = 0.6857,
which is shrunken from the value of ˆb1, and ˆb0λ = 0.1200, which is chosen
so as to ﬁt the mean. A plot of the data and the two ﬁtted lines is shown in
Fig. 6.3.
6.7.3 Minimizing Orthogonal Distances
In writing the equation Xb = y + r in place of the overdetermined linear
system Xb ≈y, we are allowing adjustments to y so as to get an equation.
Another way of making an equation out of the overdetermined linear system
Xb ≈y is to write it as
(X + E)b = y + r;
(6.54)
that is, to allow adjustments to both X and y. Both X and E are in IRn×m
(and we assume n > m).
In ﬁtting the linear model only with adjustments to y, we determine b so
as to minimize some norm of r. Likewise, with adjustments to both X and
y, we seek b so as to minimize some norm of the matrix E and the vector r.
There are obviously several ways to approach this. We could take norms of

302
6 Solution of Linear Systems
E and r separately and consider some weighted combination of the norms.
Another way is to adjoin r to E and minimize some norm of the n × (m + 1)
matrix [E|r].
A common approach is to minimize ∥[E|r]∥F. This, of course, is the sum
of squares of all elements in [E|r]. The method is therefore sometimes called
“total least squares”.
0
2
4
6
8
0
2
4
6
8
x
y
L2
L2
shrunk
mean
Figure 6.3. OLS and L2 norm regularized minimum L2 norm ﬁts
If it exists, the minimum of ∥[E|r]∥F is achieved at
b = −v2∗/v22,
(6.55)
where
[X|y] = UDV T
(6.56)
is the singular value decomposition (see equation (3.276) on page 161), and
V is partitioned as
V =
!V11 v∗2
v2∗v22
"
.
If E has some special structure, the problem of minimizing the orthogonal
residuals may not have a solution. Golub and Van Loan (1980) show that a
suﬃcient condition for a solution to exist is that dm > dm+1. (Recall that the

6.7 Other Solutions of Overdetermined Systems
303
ds in the SVD are nonnegative and they are indexed so as to be nonincreasing.
If dm = dm+1, a solution may or may not exist.)
Again, as an example, let us consider the data in equation (6.49) for the
equation
y = b0 + b1x.
We found the least squares solution to be ˆb0 = −0.3158 and ˆb1 = 0.7895,
which ﬁts the mean and has a residual vector that is orthogonal to 1 and to x.
Now we determine a ﬁt so that the L2 norm of the orthogonal residuals is
minimized. Again, we will force the equation to ﬁt the mean exactly. We get
ˆb0orth = −4.347 and ˆb0orth = 1.749. A plot of the data, the two ﬁtted lines,
and the residuals is shown in Fig. 6.4.
0
2
4
6
8
0
2
4
6
8
x
y
L2
L2
orthogonal
mean
Figure 6.4. OLS and minimum orthogonal L2-norm ﬁts
The orthogonal residuals can be weighted in the usual way by premulti-
plication by a Cholesky factor of a weight matrix, as discussed on page 295.
If some norm other than the L2 norm is to be minimized, an iterative
approach must be used. Ammann and Van Ness (1988) describe an iterative
method that is applicable to any norm, so long as a method is available to
compute a value of b that minimizes the norm of the usual vertical distances
in a model such as equation (9.11). The method is simple. We ﬁrst ﬁt y = Xb,
minimizing the vertical distances in the usual way; we then rotate y into ˜y
and X into 
X, so that the ﬁtted plane is horizontal. Next, we ﬁt ˜y = 
Xb and
repeat. After continuing this way until the ﬁts in the rotated spaces do not

304
6 Solution of Linear Systems
change from step to step, we adjust the ﬁtted b back to the original unrotated
space. Because of these rotations, if we assume that the model ﬁts some point
exactly, we must adjust y and X accordingly (see the discussion on page 289).
In the following, we assume that the model ﬁts the means exactly, so we center
the data. We let m be the number of columns in the centered data matrix.
(The centered matrix does not contain a column of 1s. If the formulation of
the model y = Xb includes an intercept term, then X is n × (m + 1).)
Algorithm 6.5 Iterative orthogonal residual ﬁtting
through the means
0. Input stopping criteria, ϵ and kmax.
Set k = 1, y(0)
c
= yc, X(0)
c
= Xc, and D(0) = Im+1.
1. Determine a value b(k)
c
that minimizes the norm of

y(k−1)
c
−X(k−1)
c
b(k)
c

.
2. If b(k)
c
≤ϵ, go to step 7.
3. Determine a rotation matrix Q(k) that makes the kth ﬁt horizontal.
4. Transform the matrix
2
y(k−1)
c
|X(k−1)
c
3 2
y(k)
c
|X(k)
c
3
by a rotation matrix:
2
y(k)
c
|X(k)
c
3
=
2
y(k−1)
c
|X(k−1)
c
3
Q(k).
5. Transform D(k−1) by the same rotation: D(k) = D(k−1)Q(k).
6. If k < kmax,
set k = k + 1 and go to step 1;
otherwise,
issue message that
“algorithm did not converge in kmax iterations”.
7. For j = 2, . . . , m, choose bj = dj,m+1/dm+1,m+1 (So long as the ro-
tations have not produced a vertical plane in the unrotated space,
dm+1,m+1 will not be zero.)
8. Compute b1 = ¯y−k
j=2 bj ∗¯xj (where ¯xj is the mean of the jth column
of the original uncentered X).
An appropriate rotation matrix for Algorithm 6.5 is Q in the QR decomposi-
tion of
⎡
⎣
Im
0
(b(k))T 1
⎤
⎦.
Note that forcing the ﬁt to go through the means, as we do in Algo-
rithm 6.5, is not usually done for norms other than the L2 norm (see Fig. 6.2).

Exercises
305
Exercises
6.1. Let A be nonsingular, and let κ(A) = ∥A∥∥A−1∥.
a) Prove equation (6.10):
κ2(A) =
maxx̸=0
∥Ax∥
∥x∥
minx̸=0
∥Ax∥
∥x∥
.
b) Using the relationship above, explain heuristically why κ(A) is called
the “condition number” of A.
6.2. Consider the system of linear equations
x1 + 4x2 +
x3 = 12,
2x1 + 5x2 + 3x3 = 19,
x1 + 2x2 + 2x3 =
9.
(6.57)
a) Solve the system using Gaussian elimination with partial pivoting.
b) Solve the system using Gaussian elimination with complete pivoting.
c) Determine the D, L, and U matrices of the Gauss-Seidel method
(equation (6.19), page 280) and determine the spectral radius of
(D + L)−1U.
d) Do three steps of the Gauss-Seidel method using the original formu-
lation (6.57), and starting with x(0) = (1, 1, 1), and evaluate the L2
norm of the diﬀerence of two successive approximate solutions.
e) Now do one partial pivot on the system (6.57) to form the coeﬃcient
matrix
A =
⎡
⎣
2 5 3
1 4 1
1 2 2
⎤
⎦.
Write the corresponding D, L, and U, and determine ρ(( D+ L)−1 U).
f) Do three steps of the Gauss-Seidel method using the one-pivot formu-
lation of Exercise 6.2e, and starting with x(0) = (1, 1, 1), and evaluate
the L2 norm of the diﬀerence of two successive approximate solutions.
g) Do three steps of the Gauss-Seidel method with successive overre-
laxation using the one-pivot formulation of Exercise 6.2e and with
ω = 0.1, starting with x(0) = (1, 1, 1), and evaluate the L2 norm of
the diﬀerence of two successive approximate solutions.
h) Do three steps of the conjugate gradient method using the original
formulation(6.57), starting with x(0) = (1, 1, 1), and evaluate the L2
norm of the diﬀerence of two successive approximate solutions.

306
6 Solution of Linear Systems
6.3. The normal equations.
a) For any matrix X with real elements, show that XTX is nonnegative
deﬁnite.
b) For any n × m matrix X with real elements and with n < m, show
that XTX is not positive deﬁnite.
c) Let X be an n × m matrix of full column rank. Show that XTX is
positive deﬁnite.
6.4. Solving an overdetermined system Xb = y, where X is n × m.
a) Count how many multiplications and additions are required to form
XTX. (A multiplication or addition such as this is performed in ﬂoat-
ing point on a computer, so the operation is called a “ﬂop”. Some-
times a ﬂop is considered a combined operation of multiplication
and addition; at other times, each is considered a separate ﬂop. See
page 507. The distinction is not important here; just count the total
number.)
b) Count how many ﬂops are required to form XTy.
c) Count how many ﬂops are required to solve XTXb = XTy using a
Cholesky decomposition.
d) Count how many ﬂops are required to form a QR decomposition of
X using reﬂectors.
e) Count how many ﬂops are required to form a QTy.
f) Count how many ﬂops are required to solve R1b = c1 (equation (6.41),
page 293).
g) If n is large relative to m, what is the ratio of the total number of ﬂops
required to form and solve the normal equations using the Cholesky
method to the total number required to solve the system using a QR
decomposition? Why is the QR method generally preferred?
6.5. On page 293, we derived the least squares solution, *b = X+y, to the
overdetermined system Xb = y by use of the QR decomposition of X.
This equation for the least squares solution can also be shown to be
correct in other ways.
a) For simplicity in this part, let us assume that X is of full rank;
that is, we will write *b as the solution to the normal equations,
*b = (XTX)−1XTy. (This assumption is not necessary, but the proof
without it is much messier.)
Show that *b = X+y by showing that (XTX)−1XT = X+.
b) On page 291 we showed that orthogonality of the residuals charac-
terized a least squares solution. Show that *b = X+y is a least squares
solution by showing that in this case XT(y −X*b) = 0.
6.6. Verify equation (6.51).

7
Evaluation of Eigenvalues and Eigenvectors
Before we discuss methods for computing eigenvalues, we recall a remark made
in Chap. 5. A given nth-degree polynomial p(c) is the characteristic polynomial
of some matrix. The companion matrix of equation (3.225) is one such matrix.
Thus, given a general polynomial p, we can form a matrix A whose eigenvalues
are the roots of the polynomial; and likewise, given a square matrix, we can
write a polynomial in its eigenvalues. It is a well-known fact in the theory
of equations that there is no general formula for the roots of a polynomial
of degree greater than 4. This means that we cannot expect to have a direct
method for calculating eigenvalues of any given matrix.
The eigenvalues of some matrices, of course, can be evaluated directly.
The eigenvalues of a diagonal matrix, for example, are merely the diagonal
elements. In that case, the characteristic polynomial is of the factored form
)(aii −c), whose roots are immediately obtainable. For general eigenvalue
computations, however, we must use an iterative method.
In statistical applications, the matrices whose eigenvalues are of interest
are often symmetric. Symmetric matrices are diagonalizable and have only
real eigenvalues. (As usual, we will assume the matrices themselves are real.)
The problem of determining the eigenvalues of a symmetric matrix therefore
is simpler than the corresponding problem for a general matrix. In many
statistical applications, the symmetric matrices of interest are nonnegative
deﬁnite, and this can allow use of simpler methods for computing eigenvalues
and eigenvectors. In addition, nonsymmetric matrices of interest in statistical
applications are often irreducible nonnegative matrices, and computations for
eigenvalues and eigenvectors for matrices of this type are also often simpler.
(We will discuss such matrices in Sect. 8.7.3.)
In this chapter, we describe various methods for computing eigenvalues.
A given method may have some desirable property for particular applica-
tions, and in some cases, the methods may be used in combination. Some
of the methods rely on sequences that converge to a particular eigenvalue or
eigenvector. The power method, discussed in Sect. 7.2, is of this type; one
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 7
307

308
7 Evaluation of Eigenvalues
eigenpair at a time is computed. Other methods are based on sequences of
orthogonally similar matrices that converge to a diagonal matrix. An example
of such a method is called the LR method. This method, which we will not
consider in detail, is based on a factorization of A into left and right factors,
FL and FR, and the fact that if c is an eigenvalue of FLFR, then it is also an
eigenvalue of FRFL (property 8, page 136). If A = L(0)U (0) is an LU decom-
position of A with 1s on the diagonal of either L(0) or U (0), iterations of LU
decompositions of the similar matrices
L(k+1)U (k+1) = U (k)L(k),
under some conditions, will converge to a similar diagonal matrix. The suﬃ-
cient conditions for convergence include nonnegative deﬁniteness.
7.1 General Computational Methods
For whatever approach is taken for ﬁnding eigenpairs, there are some general
methods that may speed up the process or that may help in achieving higher
numerical accuracy. Before describing some of the techniques, we consider a
bound on the sensitivity of eigenvalues to perturbations of the matrix.
7.1.1 Numerical Condition of an Eigenvalue Problem
The upper bounds on the largest eigenvalue, given in inequalities (3.235)
and (3.236) on page 142, provide a simple indication of the region in the
complex plane in which the eigenvalues lie.
The Gershgorin disks (inequalities (3.239) and (3.240), page 145) pro-
vide additional information about the regions of the complex plane in which
the eigenvalues lie. The Gershgorin disks can be extended to deﬁne separate
regions that contain eigenvalues, but we will not consider those reﬁnements
here. The spectral radius and/or Gershgorin disks can be used to obtain ap-
proximate values to use in some iterative approximation methods; see equa-
tion (7.13) below, for example.
In any computational problem, it is of interest to know what is the eﬀect
on the solution when there are small changes in the problem itself. This leads
to the concept of a condition number, as we discussed in Sect. 6.1.1 beginning
on page 267. The objective is to quantify or at least determine bounds on the
rate of change in the “output” relative to changes in the “input”.
In the eigenvalue problem, we begin with a square matrix A. We assume
that A is diagonalizable. (All symmetric matrices are diagonalizable, and equa-
tion (3.248) on page 149 gives necessary and suﬃcient conditions which many
other matrices encountered in statistical applications also satisfy.) We form
V −1AV = C = diag((c1, . . . , cn)), where the ci are the eigenvalues of A.

7.1 General Computational Methods
309
The approach, as in Sect. 6.1.1, is to perturb the problem slightly by
adding a small amount δA to A. Let A = A + δA. (Notice that δA does not
necessarily represent a scalar multiple of the matrix.)
If A is well-conditioned for the eigenvalue problem, then if ∥δA∥is small
relative to ∥A∥, the diﬀerences in the eigenvalues of A and of A are likewise
small. Let d be any eigenvalue of A that is not an eigenvalue of A. (If all
eigenvalues of A are eigenvalues of A, then the perturbation has had no eﬀect,
and the question we are addressing is not of interest.) Our interest will be in
min
c∈σ(A) |c −d|.
If d is an eigenvalue of A, then A + δA −dI is singular and so V −1(A +
δA −dI)V is also singular. Simplifying this latter expression, we have that
C −dI + V −1δAV is singular. Since d is not an eigenvalue of A, however,
C −dI must be nonsingular, and so (C −dI)−1 exists. Multiplying the two
expressions we have that I + (C −dI)−1V −1δAV is also singular; hence −1
is an eigenvalue of (C −dI)−1V −1δAV , and so by property 16 on page 140,
we have
1 ≤∥(C −dI)−1V −1δAV ∥,
for any consistent norm. (Recall that all matrix norms are consistent in my
deﬁnition.) Furthermore, again using the consistency property multiple times,
∥(C −dI)−1V −1δAV ∥≤∥(C −dI)−1∥∥V −1∥∥δA∥∥V ∥.
In equation (6.7) on page 269, we deﬁned “the” condition number for a non-
singular matrix V as κ(V ) = ∥V ∥∥V −1∥. Now, since C −dI is a diagonal
matrix, we can rewrite the two inequalities above as
min
c∈σ(A) |c −d| ≤κ(V )∥δA∥;
(7.1)
that is, the eigenvalues of the perturbed matrix are within given bounds from
the eigenvalues of the original matrix.
This fact is called the Bauer-Fike theorem, and it has several variations
and ramiﬁcations. It is closely related to Gershgorin disks. Our interest here is
just to provide a perturbation bound that conveniently relates to the condition
number of the diagonalizing matrix.
If A is symmetric, it is orthogonally diagonalizable, and the V above is an
orthogonal matrix. Hence, if A is a symmetric matrix, A = A + δA, and d is
an eigenvalue of A = A + δA, then
min
c∈σ(A) |c −d| ≤∥δA∥.

310
7 Evaluation of Eigenvalues
7.1.2 Eigenvalues from Eigenvectors and Vice Versa
Some methods for eigenanalysis yield the eigenvalues, and other methods yield
the eigenvectors. Given one member of an eigenpair, we usually want to ﬁnd
the other member.
If we are given an eigenvector v of the matrix A, there must be some
element vj that is not zero. For any nonzero element of the eigenvector, the
eigenvalue corresponding to v is
(Av)j/vj.
(7.2)
Likewise, if the eigenvalue c is known, a corresponding eigenvector is any
solution to the singular system
(A −cI)v = 0.
(7.3)
(It is relevant to note that the system is singular because many standard
software packages will refuse to solve singular systems whether or not they
are consistent!)
An eigenvector associated with the eigenvalue c can be found using equa-
tion (7.3) if we know the position of any nonzero element in the vector. Sup-
pose, for example, it is known that v1 ̸= 0. We can set v1 = 1 and form
another system to solve for the remaining elements of v by writing
!a11 −1
aT
1
a2
A22 −cIn−1
" ! 1
v2
"
=
!0
0
"
,
(7.4)
where v2 is an (n −1)-vector and aT
1 and a2 are the remaining elements in
the ﬁrst row and ﬁrst column, respectively, of A. Rearranging this, we get the
(n −1) × (n −1) system
(A22 −cIn−1)v2 = −a2.
(7.5)
The locations of any zero elements in the eigenvector are critical for using
this method. To form a system as in equation (7.4), the position of some
nonzero element must be known. Another problem in using this method arises
when the geometric multiplicity of the eigenvalue is greater than 1. In that
case, the system in equation (7.5) is also singular, and the process must be
repeated to form an (n−2)×(n−2) system. If the multiplicity of the eigenvalue
is k, the ﬁrst full rank system encountered while continuing in this way is
the one that is (n −k) × (n −k).
7.1.3 Deﬂation
Whenever an eigenvalue together with its associated left and right eigenvectors
for a real matrix A are available, another matrix can be formed for which all
the other nonzero eigenvalues and corresponding eigenvectors are the same

7.1 General Computational Methods
311
as for A. (Of course the left and right eigenvalues for many matrices are
the same.)
Suppose ci is an eigenvalue of A with associated right and left eigenvectors
vi and wi, respectively. Now, suppose that cj is a nonzero eigenvalue of A
such that cj ̸= ci. Let vj and wj be, respectively, right and left eigenvectors
associated with cj. Now,
⟨Avi, wj⟩= ⟨civi, wj⟩= ci⟨vi, wj⟩,
but also
⟨Avi, wj⟩= ⟨vi, ATwj⟩= ⟨vi, cjwj⟩= cj⟨vi, wj⟩.
But if
ci⟨vi, wj⟩= cj⟨vi, wj⟩
and cj ̸= ci, then ⟨vi, wj⟩= 0. Consider the matrix
B = A −civiwH
i .
(7.6)
We see that
Bwj = Awj −civiwH
i wj
= Awj
= cjwj,
so cj and wj are, respectively, an eigenvalue and an eigenvector of B.
The matrix B has some of the ﬂavor of the sum of some terms in a spectral
decomposition of A. (Recall that the spectral decomposition is guaranteed to
exist only for matrices with certain properties. In Chap. 3, we stated the exis-
tence for diagonalizable matrices but derived it only for symmetric matrices.)
The ideas above lead to a useful method for ﬁnding eigenpairs of a diag-
onalizable matrix. (The method also works if we begin with a simple eigen-
value.) We will show the details only for a real symmetric matrix.
7.1.3.1 Deﬂation of Symmetric Matrices
Let A be an n × n symmetric matrix. A therefore is diagonalizable, its eigen-
values and eigenvectors are real, and the left and right eigenvalues are the
same.
Let (c, v), with vTv = 1, be an eigenpair of A. Now let X be an n × n −1
matrix whose columns form an orthogonal basis for V(A−vvT). One easy way
of doing this is to choose n −1 of the n unit vectors of order n such that none
are equal to v and then, beginning with v, use Gram-Schmidt transformations
to orthogonalize the vectors, using Algorithm 2.1 on page 39. (Assuming v is
not a unit vector, we merely choose e1, . . . , en−1 together with v as the starting
set of linearly independent vectors.) Now let P = [v|X]. We have

312
7 Evaluation of Eigenvalues
P −1 =
!
vT
XT(I −vvT)
"
,
as we see by direct multiplication, and
P −1AP =
! c 0
0 B
"
,
(7.7)
where B is the (n −1) × (n −1) matrix XTAX.
Clearly, B is symmetric and the eigenvalues of B are the same as the other
n −1 eigenvalues of A. The important point is that B is (n −1) × (n −1).
7.1.4 Preconditioning
The convergence of iterative methods applied to a linear system Ax = b
can often be speeded up by replacing the system by an equivalent system
M −1Ax = M −1b. The iterations then depend on the properties, such as the
relative magnitudes of the eigenvalues, of M −1A rather than A. The replace-
ment of the system Ax = b by M −1Ax = M −1b is called preconditioning.
(It is also sometimes called left preconditioning, and the use of the system
AM −1y = b with y = Mx is called right preconditioning. Either or both kinds
of preconditioning may be used in a given iterative algorithm.) The matrix
M is called a preconditioner.
Determining an eﬀective preconditioner matrix M −1 for eigenvalue compu-
tations is not straightforward. In general, the objective would be to determine
M −1A so that it is “close” to I, because then the eigenvalues might be easier
to obtain by whatever method we may use. The salient properties of I are
that it is normal (see Sect. 8.2.3 beginning on page 345) and its eigenvalues
are clustered.
There are various kinds of preconditioning. We have considered precondi-
tioning in the context of an iterative algorithm for solving linear systems on
page 284. Some preconditioning methods work better as an adjunct to one
algorithm, and others work better in conjunction with some other algorithm.
Obviously, the eﬃcacy depends on the nature of the data input to the prob-
lem. In the case of a sparse matrix A, for example an incomplete factorization
A ≈L U where both L and U are sparse, M = L U may be a good precondi-
tioner. We will not consider any of the details here. Benzi (2002) provides a
good survey of techniques, but it is diﬃcult to identify general methods that
work well.
7.1.5 Shifting
If c is an eigenvalue of A, then c −d is an eigenvalue of A −dI, and the
associated eigenvectors are the same. (This is property 7 on page 136.) Hence,
instead of seeking an eigenvalue of A, we might compute (or approximate) an

7.2 Power Method
313
eigenvalue of A −dI. (We recall also, from equation (6.11) on page 272, that,
for appropriate signs of d and the eigenvalues, the condition number of A−dI
is better than the condition number of A.)
Use of A−dI amounts to a “shift” in the eigenvalue. This can often improve
the convergence rate in an algorithm to compute an eigenvalue. (Remember
that all general algorithms to compute eigenvalues are iterative.)
The best value of d in the shift depends on both the algorithm and the
characteristics of the matrix. Various shifts have been suggested. One common
value of the shift is based on the Rayleigh quotient shift; another common
value is called the “Wilkinson shift”, after James Wilkinson. We will not
discuss any of the particular shift values here.
7.2 Power Method
The power method is a straightforward method that can be used for a real
diagonalizable matrix with a simple dominant eigenvalue. A symmetric matrix
is diagonalizable, of course, but it may not have a simple dominant eigenvalue.
The power method ﬁnds the dominant eigenvalue. In some applications,
only the dominant eigenvalue is of interest. If other eigenvalues are needed,
however, we can ﬁnd them one at a time by deﬂation.
Let A be a real n×n diagonalizable matrix with a simple dominant eigen-
value. Index the eigenvalues ci so that |c1| > |c2| ≥· · · |cn|, with corresponding
normalized eigenvectors vi. Note that the requirement for the dominant eigen-
value that c1 > c2 implies that c1 and the dominant eigenvector v1 are unique
and that c1 is real (because otherwise ¯c1 would also be an eigenvalue, and
that would violate the requirement).
Now let x be an n-vector that is not orthogonal to v1. Because A is assumed
to be diagonalizable, the eigenvectors are linearly independent and so x can
be represented as a linear combination of the eigenvectors,
x = b1v1 + · · · + bnvn.
(7.8)
Because x is not orthogonal to v1, b1 ̸= 0. The power method is based on a
sequence
x, Ax, A2x, . . . .
(This sequence is a ﬁnite Krylov space generating set; see equation (6.26).)
From the relationships above and the deﬁnition of eigenvalues and eigenvec-
tors, we have
Ax = b1Av1 + · · · + bnAvn
= b1c1v1 + · · · + bncnvn
A2x = b1c2
1v1 + · · · + bnc2
nvn
· · · = · · ·

314
7 Evaluation of Eigenvalues
Ajx = b1cj
1v1 + · · · + bncj
nvn
= cj
1

b1v1 + · · · + bn
cn
c1
j
vn

.
(7.9)
To simplify the notation, let
u(j) = Ajx/cj
1
(7.10)
(or, equivalently, u(j) = Au(j−1)/c1). From equations (7.9) and the fact that
|c1| > |ci| for i > 1, we see that u(j) →b1v1, which is the nonnormalized
dominant eigenvector.
We have the bound
u(j) −b1v1
 =
b2
c2
c1
j
v2 + · · ·
· · · + bn
cn
c1
j
vn

≤|b2|

c2
c1

j
∥v2∥+ · · ·
· · · + |bn|

cn
c1

j
∥vn∥
≤(|b2| + · · · + |bn|)

c2
c1

j
.
(7.11)
The last expression results from the fact that |c2| ≥|ci| for i > 2 and that the
vi are unit vectors.
From equation (7.11), we see that the norm of the diﬀerence of u(j) and
b1v1 decreases by a factor of approximately |c2/c1| with each iteration; hence,
this ratio is an important indicator of the rate of convergence of u(j) to the
dominant eigenvector.
If |c1| > |c2| > |c3|, b2 ̸= 0, and b1 ̸= 0, the power method converges
linearly (see page 511); that is,
0 < lim
j→∞
∥u(j+1) −b1v1∥
∥u(j) −b1v1∥
< 1
(7.12)
(see Exercise 7.1c, page 324). Shifting the matrix to form A −dI results in
a matrix with eigenvalues with diﬀerent relative sizes, and may be useful in
speeding up the convergence.
If an approximate value of the eigenvector v1 is available and x is taken to
be that approximate value, the convergence will be faster. If an approximate
value of the dominant eigenvalue, *c1, is available, starting with any y(0), a few
iterations on

7.3 Jacobi Method
315
(A −*c1I)y(k) = y(k−1)
(7.13)
may yield a better starting value for x. Once the eigenvector associated with
the dominant eigenvalue is determined, the eigenvalue c1 can easily be deter-
mined, as described above.
7.2.1 Inverse Power Method
If A is nonsingular, we can also use the power method on A−1 to determine
the smallest eigenvalue of A. This is called the “inverse power method”.
The rate of convergence may be very diﬀerent from that of the power
method applied to A. Shifting is also generally important in the inverse power
method. Of course this method only determines the eigenvalue with the small-
est absolute value. If other eigenvalues are needed, we can ﬁnd them one at a
time by deﬂation.
7.3 Jacobi Method
The Jacobi method for determining the eigenvalues of a simple symmetric ma-
trix A uses a sequence of orthogonal similarity transformations that eventually
results in the transformation
A = PCP −1
(see equation (3.247) on page 149) or
C = P −1AP,
where C is diagonal. Recall that similar matrices have the same eigenvalues.
The matrices for the similarity transforms are the Givens rotation or Ja-
cobi rotation matrices discussed on page 238. The general form of one of these
orthogonal matrices, Gpq(θ), given in equation (5.12) on page 239, is the iden-
tity matrix with cos θ in the (p, p)th and (q, q)th positions, sin θ in the (p, q)th
position, and −sin θ in the (q, p)th position:
Gpq(θ) =
p
q
I
0
0
0
0
p 0 cos θ 0 sin θ 0
0
0
I
0
0
q 0 −sin θ 0 cos θ 0
0
0
0
0
I
.
The Jacobi iteration is
A(k) = GT
pkqk(θk)A(k−1)Gpkqk(θk),

316
7 Evaluation of Eigenvalues
where pk, qk, and θk are chosen so that the A(k) is “more diagonal” than
A(k−1). Speciﬁcally, the iterations will be chosen so as to reduce the sum of
the squares of the oﬀ-diagonal elements, which for any square matrix A is
∥A∥2
F −

i
a2
ii.
The orthogonal similarity transformations preserve the Frobenius norm
A(k)
F =
A(k−1)
F.
Because the rotation matrices change only the elements in the (p, p)th, (q, q)th,
and (p, q)th positions (and also the (q, p)th position since both matrices are
symmetric), we have

a(k)
pp
2
+

a(k)
qq
2
+ 2

a(k)
pq
2
=

a(k−1)
pp
2
+

a(k−1)
qq
2
+ 2

a(k−1)
pq
2
.
The oﬀ-diagonal sum of squares at the kth stage in terms of that at the (k−1)th
stage is
A(k)
2
F −

i

a(k)
ii
2
=
A(k)
2
F −

i̸=p,q

a(k)
ii
2
−

a(k)
pp
2
+

a(k)
qq
2
=
A(k−1)
2
F −

i

a(k−1)
ii
2
−2

a(k−1)
pq
2
+ 2

a(k)
pq
2
.
(7.14)
Hence, for a given index pair, (p, q), at the kth iteration, the sum of the squares
of the oﬀ-diagonal elements is minimized by choosing the rotation matrix so
that
a(k)
pq = 0.
(7.15)
As we saw on page 239, it is easy to determine the angle θ so as to intro-
duce a zero in a single Givens rotation. Here, we are using the rotations in a
similarity transformation, so it is a little more complicated.
The requirement that a(k)
pq = 0 implies
a(k−1)
pq

cos2 θ −sin2 θ

+

a(k−1)
pp
−a(k−1)
qq

cos θ sin θ = 0.
(7.16)
Using the trigonometric identities
cos(2θ) = cos2 θ −sin2 θ
sin(2θ) = 2 cosθ sin θ,

7.3 Jacobi Method
317
in equation (7.16), we have
tan(2θ) =
2a(k−1)
pq
a(k−1)
pp
−a(k−1)
qq
,
which yields a unique angle in [−π/4, π/4]. Of course, the quantities we need
are cos θ and sin θ, not the angle itself. First, using the identity
tan θ =
tan(2θ)
1 +

1 + tan2(2θ)
,
we get tan θ from tan(2θ); and then from tan θ we can compute the quantities
required for the rotation matrix Gpq(θ):
cos θ =
1
√
1 + tan2 θ
,
sin θ = cos θ tan θ.
Convergence occurs when the oﬀ-diagonal elements are suﬃciently small.
The quantity (7.14) using the Frobenius norm is the usual value to compare
with a convergence criterion, ϵ.
From equation (7.15), we see that the best index pair, (p, q), is such that
a(k−1)
pq
 = max
i<j
a(k−1)
ij
.
If this choice is made, the Jacobi method can be shown to converge (see
Watkins 2002). The method with this choice is called the classical Jacobi
method.
For an n × n matrix, the number of operations to identify the maximum
oﬀ-diagonal is O(n2). The computations for the similarity transform itself are
only O(n) because of the sparsity of the rotators. Of course, the computations
for the similarity transformations are more involved than those to identify the
maximum oﬀ-diagonal, so, for small n, the classical Jacobi method should
be used. If n is large, however, it may be better not to spend time look-
ing for the maximum oﬀ-diagonal. Various cyclic Jacobi methods have been
proposed in which the pairs (p, q) are chosen systematically without regard
to the magnitude of the oﬀ-diagonal being zeroed. Depending on the nature
of the cyclic Jacobi method, it may or may not be guaranteed to converge.
For certain schemes, quadratic convergence has been proven; for at least one
other scheme, an example showing failure of convergence has been given. See
Watkins (2002) for a discussion of the convergence issues.
The Jacobi method is one of the oldest algorithms for computing eigenval-
ues, and has recently become important again because it lends itself to easy
implementation on parallel processors (see Zhou and Brent 2003).
Notice that at the kth iteration, only two rows and two columns of A(k) are
modiﬁed. This is what allows the Jacobi method to be performed in parallel.

318
7 Evaluation of Eigenvalues
We can form ⌊n/2⌋pairs and do ⌊n/2⌋rotations simultaneously. Thus, each
parallel iteration consists of a choice of a set of index pairs and then a batch
of rotations. Although, as we have indicated, the convergence may depend on
which rows are chosen for the rotations, if we are to achieve much eﬃciency by
performing the operations in parallel, we cannot spend much time in deciding
how to form the pairs for the rotations. Various schemes have been suggested
for forming the pairs for a parallel iteration. A simple scheme, called “mobile
Jacobi” (see Watkins 2002), is:
1. Perform ⌊n/2⌋rotations using the pairs
(1, 2), (3, 4), (5, 6), . . . .
2. Interchange all rows and columns that were rotated.
3. Perform ⌊(n −1)/2⌋rotations using the pairs
(2, 3), (4, 5), (6, 7), . . . .
4. Interchange all rows and columns that were rotated.
5. If convergence has not been achieved, go to 1.
The notation above that speciﬁes the pairs refers to the rows and columns
at the current state; that is, after the interchanges up to that point. The
interchange operation is a similarity transformation using an elementary per-
mutation matrix (see page 81), and hence the eigenvalues are left unchanged
by this operation. The method described above is a good one, but there are
other ways of forming pairs. Some of the issues to consider are discussed by
Luk and Park (1989), who analyzed and compared some proposed schemes.
7.4 QR Method
The most common algorithm for extracting eigenvalues is the QR method.
While the power method and the Jacobi method require diagonalizable matri-
ces, which restricts their practical use to symmetric matrices, the QR method
can be used for nonsymmetric matrices. It is simpler for symmetric matrices,
of course, because the eigenvalues are real. Also, for symmetric matrices the
computer storage is less, the computations are fewer, and some transforma-
tions are particularly simple. In the following description, we will assume that
the matrix is symmetric.
The basic idea behind the use of the QR method is that for a symmetric
matrix A, the simple iterations beginning with A(0) = A, for k = 1, 2, . . .,
Q(k)R(k) = A(k−1)
A(k) = R(k)Q(k)
lead to an orthogonal triangularization of A.

7.4 QR Method
319
These iterations by themselves would be slow and would only work for
certain matrices, so the QR method requires that the matrix ﬁrst be trans-
formed into upper Hessenberg form (see page 59). A matrix can be reduced to
Hessenberg form in a ﬁnite number of similarity transformations using either
Householder reﬂections or Givens rotations.
The Hessenberg form for a symmetric matrix is tridiagonal. The Hessen-
berg form allows a large savings in the subsequent computations, even for
nonsymmetric matrices.
Even in the Hessenberg form, the matrices A(k) are shifted by c(k)I, where
c(k)I is an approximation of an eigenvalue, which can be obtained in various
ways (see Trefethen and Bau 1997; pages 219 and following).
After the matrix has been transformed into a similar Hessenberg matrix,
a sequence of similar Hessenberg matrices that converge to triangular matrix
is formed. The QR method for determining the eigenvalues is iterative and
produces a sequence of Hessenberg matrices that converge to a triangular ma-
trix. An upper Hessenberg matrix is formed and its eigenvalues are extracted
by a process called “chasing”, which consists of steps that alternate between
creating nonzero entries in positions (i + 2, i), (i + 3, i), and (i + 3, i + 1) and
restoring these entries to zero, as the nonzero entries are moved farther down
the matrix. For example,
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X X X X X X
X X X X X X X
0 X X X X X X
0 Y X X X X X
0 Y Y X X X X
0 0 0 0 X X X
0 0 0 0 0 X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
→
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X X X X X X
X X X X X X X
0 X X X X X X
0 0 X X X X X
0 0 Y X X X X
0 0 Y Y X X X
0 0 0 0 0 X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
In the jth step of the QR method, a bulge is created and is chased down the
matrix by similarity transformations, usually Givens transformations,
G−1
k A(j−1,k)Gk.
The transformations are based on the eigenvalues of 2 × 2 matrices in the
lower right-hand part of the matrix.
There are some variations on the way the chasing occurs. Haag and
Watkins (1993) describe an eﬃcient modiﬁed QR algorithm that uses both
Givens transformations and Gaussian elimination transformations, with or
without pivoting. For the n × n Hessenberg matrix A(0,0), the ﬁrst step of the
Haag-Watkins procedure begins with a 3 × 3 Householder reﬂection matrix,
G0, whose ﬁrst column is
(A(0,0) −σ1I)(A(0,0) −σ2I)e1,

320
7 Evaluation of Eigenvalues
where σ1 and σ2 are the eigenvalues of the 2×2 matrix
!
an−1,n−1 an−1,n
an−1,n
an,n
"
,
and e1 is the ﬁrst unit vector of length n. The n × n matrix G0 is diag( G0, I).
The initial transformation G−1
0 A(0,0)G0 creates a bulge with nonzero elements
a(0,1)
31
, a(0,1)
41
, and a(0,1)
42
.
After the initial transformation, the Haag-Watkins procedure makes n −3
transformations
A(0,k+1) = G−1
k A(0,k)Gk,
for k = 1, 2, . . . , n−3, that chase the bulge diagonally down the matrix, so that
A(0,k+1) diﬀers from Hessenberg form only by the nonzero elements a(0,k+1)
k+3,k+1,
a(0,k+1)
k+4,k+1, and a(0,k+1)
k+4,k+2. To accomplish this, the matrix Gk diﬀers from the
identity only in rows and columns k + 1, k + 2, and k + 3. The transformation
G−1
k A(0,k)
annihilates the entries a(0,k)
k+2,k and a(0,k)
k+3,k, and the transformation
(G−1
k A(0,k))Gk
produces A(0,k+1) with two new nonzero elements, a(0,k+1)
k+4,k+1 and a(0,k+1)
k+4,k+2. The
ﬁnal transformation in the ﬁrst step, for k = n −2, annihilates a(0,k)
n,n−2. The
transformation matrix Gn−2 diﬀers from the identity only in rows and columns
n −1 and n. These steps are iterated until the matrix becomes triangular.
As the subdiagonal elements converge to zero, the shifts for use in the ﬁrst
transformation of a step (corresponding to σ1 and σ2) are determined by
2 × 2 submatrices higher on the diagonal. Special consideration must be given
to situations in which these submatrices contain zero elements. For this, the
reader is referred to Watkins (2002) or Golub and Van Loan (1996).
This description has just indicated the general ﬂavor of the QR method.
There are diﬀerent variations on the overall procedure and then many com-
putational details that must be observed. In the Haag-Watkins procedure, for
example, the Gks are not unique, and their form can aﬀect the eﬃciency and
the stability of the algorithm. Haag and Watkins (1993) describe criteria for
the selection of the Gks. They also discuss some of the details of programming
the algorithm. A very careful description of the basic algorithm and various
modiﬁcations is provided in Trefethen and Bau (1997), pages 196 through 224.

7.6 Generalized Eigenvalues
321
7.5 Krylov Methods
In the power method, we encountered the sequence
x, Ax, A2x, . . . .
This sequence is a ﬁnite Krylov space generating set. As we mentioned on
page 284, several methods for computing eigenvalues are often based on a
Krylov space,
Kk = V({v, Av, A2v, . . . , Ak−1v}).
(Aleksei Krylov used these vectors to construct the characteristic polynomial.)
The two most important Krylov methods are the Lanczos tridiagonal-
ization algorithm and the Arnoldi orthogonalization algorithm. We will not
discuss these methods here but rather refer the interested reader to Golub
and Van Loan (1996).
7.6 Generalized Eigenvalues
In Sect. 3.8.12, we deﬁned the generalized eigenvalues and eigenvectors by
replacing the identity in the deﬁnition of ordinary eigenvalues and eigenvectors
by a general (square) matrix B:
|A −cB| = 0.
(7.17)
If there exists a ﬁnite c such that this determinant is zero, then there is some
nonzero, ﬁnite vector v such that
Av = cBv.
(7.18)
As we have seen in the case of ordinary eigenvalues, symmetry of the
matrix, because of diagonalizability, allows for simpler methods to evaluate
the eigenvalues. In the case of generalized eigenvalues, symmetry together
with positive deﬁniteness allows us to reformulate the problem to be much
simpler. If A and B are symmetric and B is positive deﬁnite, we refer to the
pair (A, B) as symmetric.
If A and B are a symmetric pair, B has a Cholesky decomposition, B =
T TT , where T is an upper triangular matrix with positive diagonal elements.
We can therefore rewrite equation (7.18) as
T −TAT −1u = cu,
(7.19)
where u = T v. Note that because A is symmetric, T −TAT −1 is symmetric,
and since c is an eigenvalue of this matrix, it is real. Its associated eigenvector
(with respect to T −TAT −1) is likewise real, and therefore so is the generalized
eigenvector v. Because T −TAT −1 is symmetric, the ordinary eigenvectors can

322
7 Evaluation of Eigenvalues
be chosen to be orthogonal. (Recall from page 153 that eigenvectors corre-
sponding to distinct eigenvalues are orthogonal, and those corresponding to
a multiple eigenvalue can be chosen to be orthogonal.) This implies that the
generalized eigenvectors of the symmetric pair (A, B) can be chosen to be
B-conjugate.
Because of the equivalence of a generalized eigenproblem for a symmetric
pair to an ordinary eigenproblem for a symmetric matrix, any of the methods
discussed in this chapter can be used to evaluate the generalized eigenpairs
of a symmetric pair. The matrices in statistical applications for which the
generalized eigenvalues are required are often symmetric pairs. For example,
Roy’s maximum root statistic, which is used in multivariate analysis, is a
generalized eigenvalue of two Wishart matrices.
The generalized eigenvalues of a pair that is not symmetric are more dif-
ﬁcult to evaluate. The approach of forming upper Hessenberg matrices, as
in the QR method, is also used for generalized eigenvalues. We will not dis-
cuss this method here but instead refer the reader to Watkins (2002) for a
description of the method, which is called the QZ algorithm.
7.7 Singular Value Decomposition
The standard algorithm for computing the singular value decomposition
A = UDV T
is due to Golub and Reinsch (1970) and is built on ideas of Golub and Kahan
(1965). The ﬁrst step in the Golub-Reinsch algorithm for the singular value
decomposition of the n×m matrix A is to reduce A to upper bidiagonal form:
A(0) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X 0 · · · 0 0
0 X X · · · 0 0
0 0 X · · · 0 0
...
...
0 0 0 · · · X X
0 0 0 · · · 0 X
0 0 0 · · · 0 0
...
...
... · · ·
...
...
0 0 0 · · · 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
We assume n ≥m. (If this is not the case, we merely use AT.) This algorithm is
basically a factored form of the QR algorithm for the eigenvalues of A(0)TA(0),
which would be symmetric and tridiagonal.
The Golub-Reinsch method produces a sequence of upper bidiagonal ma-
trices, A(0), A(1), A(2), . . ., which converges to the diagonal matrix D. (Each
of these has a zero submatrix below the square submatrix.) Similar to the QR
method for eigenvalues, the transformation from A(j) to A(j+1) is eﬀected by
a sequence of orthogonal transformations,

7.7 Singular Value Decomposition
323
A(j+1) = RT
m−2RT
m−3 · · · RT
0 A(j)T0T1 · · · Tm−2
= RTA(j)T,
which ﬁrst introduces a nonzero entry below the diagonal (T0 does this) and
then chases it down the diagonal. After T0 introduces a nonzero entry in the
(2, 1) position, RT
0 annihilates it and produces a nonzero entry in the (1, 3)
position; T1 annihilates the (1, 3) entry and produces a nonzero entry in the
(3, 2) position, which RT
1 annihilates, and so on. Each of the Rks and Tks are
Givens transformations, and, except for T0, it should be clear how to form
them.
If none of the elements along the main diagonal or the diagonal above the
main diagonal is zero, then T0 is chosen as the Givens transformation such
that T T
0 will annihilate the second element in the vector
(a2
11 −σ1, a11a12, 0, · · · , 0),
where σ1 is the eigenvalue of the lower right-hand 2×2 submatrix of A(0)TA(0)
that is closest in value to the (m, m) element of A(0)TA(0). This is easy to
compute (see Exercise 7.6).
If an element along the main diagonal or the diagonal above the main diag-
onal is zero, we must proceed slightly diﬀerently. (Remember that for purposes
of computations “zero” generally means “near zero”; that is, to within some
set tolerance.)
If an element above the main diagonal is zero, the bidiagonal matrix is
separated at that value into a block diagonal matrix, and each block (which
is bidiagonal) is treated separately.
If an element on the main diagonal, say akk, is zero, then a singular value
is zero. In this case, we apply a set of Givens transformations from the left.
We ﬁrst use G1, which diﬀers from the identity only in rows and columns k
and k + 1, to annihilate the (k, k + 1) entry and introduce a nonzero in the
(k, k + 2) position. We then use G2, which diﬀers from the identity only in
rows and columns k and k +2, to annihilate the (k, k +2) entry and introduce
a nonzero in the (k, k +3) position. Continuing this process, we form a matrix
of the form
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X 0 0 0 0 0 0
0 X X 0 0 0 0 0
0 0 X Y 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 X X 0 0
0 0 0 0 0 X X 0
0 0 0 0 0 0 X X
0 0 0 0 0 0 0 X
...
...
... · · ·
...
...
...
...
0 0 0 0 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

324
7 Evaluation of Eigenvalues
The Y in this matrix (in position (k−1, k)) is then chased up the upper block
consisting of the ﬁrst k rows and columns of the original matrix by using
Givens transformations applied from the right. This then yields two block
bidiagonal matrices (and a 1 × 1
0 matrix). We operate on the individual
blocks as before.
After the steps have converged to yield a diagonal matrix, D, all of the
Givens matrices applied from the left are accumulated into a single matrix and
all from the right are accumulated into a single matrix to yield a decomposition
A = U D V T.
There is one last thing to do. The elements of D may not be nonnegative.
This is easily remedied by postmultiplying by a diagonal matrix G that is the
same as the identity except for having a −1 in any position corresponding
to a negative value in D. In addition, we generally form the singular value
decomposition is such a way that the elements in D are nonincreasing. The
entries in D can be rearranged by a permutation matrix E(π) so they are in
nonincreasing order. So we have
D = ET
(π) DGE(π),
and the ﬁnal decomposition is
A = UE(π)GDET
(π) V T
= UDV T.
If n ≥5
3m, a modiﬁcation of this algorithm by Chan (1982a,b) is more
eﬃcient than the standard Golub-Reinsch method.
Exercises
7.1. Simple matrices and the power method.
a) Let A be an n × n matrix whose elements are generated indepen-
dently (but not necessarily identically) from real-valued continuous
distributions. What is the probability that A is simple?
b) Under the same conditions as in Exercise 7.1a, and with n ≥3, what
is the probability that |cn−2| < |cn−1| < |cn|, where cn−2, cn−1, and
cn are the three eigenvalues with the largest absolute values?
c) Prove that the power method converges linearly if |cn−2| < |cn−1| <
|cn|, bn−1 ̸= 0, and bn ̸= 0. (The bs are the coeﬃcients in the expan-
sion of x(0).)
Hint: Substitute the expansion in equation (7.11) on page 314 into
the expression for the convergence ratio in equation (7.12).

Exercises
325
d) Suppose A is simple and the elements of x(0) are generated indepen-
dently (but not necessarily identically) from continuous distributions.
What is the probability that the power method will converge linearly?
7.2. Consider the matrix
⎡
⎢⎢⎣
4 1 2 3
1 5 3 2
2 3 6 1
3 2 1 7
⎤
⎥⎥⎦.
a) Use the power method to determine the largest eigenvalue and an
associated eigenvector of this matrix.
b) Find a 3×3 matrix, as in equation (7.7), that has the same eigenvalues
as the remaining eigenvalues of the matrix above.
c) Using Givens transformations, reduce the matrix to upper Hessenberg
form.
7.3. In the matrix
⎡
⎢⎢⎣
2 1 0 0
1 5 2 0
3 2 6 1
0 0 1 8
⎤
⎥⎥⎦,
determine the Givens transformations to chase the 3 in the (3, 1) position
out of the matrix.
7.4. In the matrix
⎡
⎢⎢⎢⎢⎣
2 1 0 0
3 5 2 0
0 0 6 1
0 0 0 8
0 0 0 0
⎤
⎥⎥⎥⎥⎦
,
determine the Givens transformations to chase the 3 in the (2, 1) position
out of the matrix.
7.5. In the QR methods for eigenvectors and singular values, why can we
not just use additional orthogonal transformations to triangularize the
given matrix (instead of just forming a similar Hessenberg matrix, as in
Sect. 7.4) or to diagonalize the given matrix (instead of just forming the
bidiagonal matrix, as in Sect. 7.7)?
7.6. Determine the eigenvalue σ1 (on page 323) used in forming the matrix
T0 for initiating the chase in the algorithm for the singular value decom-
position. Express it in terms of am,m, am−1,m−1, am−1,m, and am−1,m−2.

Part II
Applications in Data Analysis

8
Special Matrices and Operations Useful
in Modeling and Data Analysis
In previous chapters, we encountered a number of special matrices, such as
symmetric matrices, banded matrices, elementary operator matrices, and so
on. In this chapter, we will discuss some of these matrices in more detail and
also introduce some other special matrices and data structures that are useful
in statistics.
There are a number of special kinds of matrices that are useful in statistical
applications. In statistical applications in which data analysis is the objective,
the initial step is the representation of observational data in some convenient
form, which often is a matrix. The matrices for operating on observational
data or summarizing the data often have special structures and properties.
We discuss the representation of observations using matrices in Sect. 8.1.
In Sect. 8.2, we review and discuss some of the properties of symmetric
matrices.
One of the most important properties of many matrices occurring in sta-
tistical data analysis is nonnegative or positive deﬁniteness; this is the subject
of Sects. 8.3 and 8.4.
Fitted values of a response variable that are associated with given values
of covariates in linear models are often projections of the observations onto
a subspace determined by the covariates. Projection matrices and Gramian
matrices useful in linear models are considered in Sects. 8.5 and 8.6.
Another important property of many matrices occurring in statistical
modeling is irreducible nonnegativeness or positiveness; this is the subject
of Sect. 8.7.
Many of the deﬁning properties of the special matrices discussed in this
chapter are invariant under scalar multiplication; hence, the special matrices
are members of cones. Interestingly even further, convex combinations of some
types of special matrices yield special matrices of the same type; hence, those
special matrices are members of convex cones (see Sect. 2.2.8 beginning on
page 43).
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 8
329

330
8 Matrices with Special Properties
8.1 Data Matrices and Association Matrices
There are several ways that data can be organized for representation in the
computer. We distinguish logical structures from computer-storage structures.
Data structure in computers is an important concern and can greatly aﬀect
the eﬃciency of computer processing. We discuss some simple aspects of the
organization for computer storage in Sect. 11.1, beginning on page 523. In the
present section, we consider some general issues of logical organization and
structure.
There are two important aspects of data in applications that we will not
address here. One is metadata; that is, data about the data. Metadata includes
names or labels associated with data, information about how and when the
data were collected, information about how the data are stored in the com-
puter, and so on. Another important concern in applications is missing data.
In real-world applications it is common to have incomplete data. If the data
are stored in some structure that naturally contains a cell or a region for the
missing data, the computer representation of the dataset must contain some
indication that the cell is empty. For numeric data, the convenient way of
doing this is by using “not-available”, NA (see page 464), or “not-a-number”,
NaN (see page 475). The eﬀect on a statistical analysis when some data are
missing varies with the type of analysis. We consider some eﬀects of missing
data on the estimation of variance-covariance matrices in Sect. 9.5.6, begin-
ning on page 437.
8.1.1 Flat Files
If several features or attributes are observed on each of several entities, a
convenient way of organizing the data is as a two-dimensional array with each
column corresponding to a speciﬁc feature and each row corresponding to a
speciﬁc observational entity. In the ﬁeld of statistics, data for the features are
stored in “variables”, the entities are called “observational units”, and a row
of the array is called an “observation” (see Fig. 8.1).
Var 1 Var 2 . . . Var m
Obs 1
. . .
x
Obs 2
. . .
x
...
...
...
. . .
...
Obs n
x
x
x
x
x
x
. . .
x
Figure 8.1. Data appropriate for representation in a ﬂat ﬁle

8.1 Data Matrices and Association Matrices
331
The data may be various types of objects, such as names, real numbers,
numbers with associated measurement units, sets, vectors, and so on. If the
data are represented as real numbers, the data array is a matrix. (Note again
our use of the word “matrix”; not just any rectangular array is a matrix in the
sense used in this book.) Other types of data can often be made equivalent to
a matrix in an intuitive manner.
The ﬂat ﬁle arrangement emphasizes the relationships of the data both
within an observational unit or row and within a variable or column. Simple
operations on the data matrix may reveal relationships among observational
units or among variables.
Flat ﬁles are the appropriate data structure for the analysis of linear mod-
els, but statistics is not just about analysis of linear models anymore. (It
never was.)
8.1.2 Graphs and Other Data Structures
If the numbers of measurements on the observational units varies or if the in-
terest is primarily in simple relationships among observational units or among
variables, the ﬂat ﬁle structure may not be very useful. Sometimes a graph
structure can be used advantageously.
A graph is a nonempty set V of points, called vertices, together with a
collection E of unordered pairs of elements of V , called edges. (Other deﬁni-
tions of “graph” allow the null set to be a graph.) If we let G be a graph, we
represent it as (V, E). We often represent the set of vertices as V (G) and the
collection of edges as E(G). An edge is said to be incident on each vertex in
the edge. The number of vertices (that is, the cardinality of V ) is the order
of the graph, and the number of edges, the cardinality of E, is the size of the
graph.
An edge in which the two vertices are the same is called a loop. If two or
more elements of E(G) contain the same two vertices, those edges are called
multiple edges, and the graph itself is called a multigraph. A graph with no
loops and with no multiple edges is called a simple graph. In some literature,
“graph” means “simple graph”, as I have deﬁned it; and a graph as I have
deﬁned it that is not simple is called a “pseudograph”
A path or walk is a sequence of edges, e1, . . . , en, such that for i ≥2 one
vertex in ei is a vertex in edge ei−1. Alternatively, a path or walk is deﬁned
as a sequence of vertices with common edges.
A graph such that there is a path that includes any pair of vertices is said
to be connected.
A graph with more than one vertex such that all possible pairs of vertices
occur as edges is a complete graph.
A closed path or closed walk is a path such that a vertex in the ﬁrst edge
(or the ﬁrst vertex in the alternate deﬁnition) is in the last edge (or the last
vertex).

332
8 Matrices with Special Properties
A cycle is a closed path in which all vertices occur exactly twice (or in
the alternate deﬁnition, in which all vertices except the ﬁrst and the last are
distinct). A graph with no cycles is said to be acyclic. An acyclic graph is also
called a tree. Trees are used extensively in statistics to represent clusters.
The number of edges that contain a given vertex (that is, the number of
edges incident on the vertex v) denoted by d(v) is the degree of the vertex.
A vertex with degree 0 is said to be isolated.
We see immediately that the sum of the degrees of all vertices equals twice
the number of edges, that is,

d(vi) = 2#(E).
The sum of the degrees hence must be an even number.
A regular graph is one for which d(vi) is constant for all vertices vi; more
speciﬁcally, a graph is k-regular if d(vi) = k for all vertices vi.
The natural data structure for a graph is a pair of lists, but a graph is
often represented graphically (no pun!) as in Fig. 8.2, which shows a graph
with ﬁve vertices and seven edges. While a matrix is usually not an appropriate
structure for representing raw data from a graph, there are various types of
matrices that are useful for studying the data represented by the graph, which
we will discuss in Sect. 8.8.9.
c
a
b
d
e
Figure 8.2. A simple graph
If G is the graph represented in Fig. 8.2, the vertices are
V (G) = {a, b, c, d, e}
and the edges are
E(G) = {(a, b), (a, c), (a, d), (a, e), (b, e), (c, d), (d, e)}.
The presence of an edge between two vertices can indicate the existence
of a relationship between the objects represented by the vertices. The graph
represented in Fig. 8.2 may represent ﬁve observational units for which our
primary interest is in their relationships with one another. For example, the

8.1 Data Matrices and Association Matrices
333
observations may be authors of scientiﬁc papers, and an edge between two
authors may represent the fact that the two have been coauthors on some
paper.
a b c d e
a
Y Y Y Y
b
c
d
e
Y
Y
Y
Y
Y
Y
Y
Y Y
Y
Figure 8.3. An alternate representation
The same information represented in the 5-order graph of Fig. 8.2 may be
represented in a 5 × 5 rectangular array, as in Fig. 8.3.
In the graph represented in Fig. 8.2, there are no isolated vertices and
the graph is connected. (Note that a graph with no isolated vertices is not
necessarily connected.) The graph represented in Fig. 8.2 is not complete
because, for example, there is no edge that contains vertices c and e. The
graph is cyclic because of the closed path (deﬁned by vertices) (c, d, e, b, a, c).
Note that the closed path (c, d, a, e, b, a, c) is not a cycle.
This use of a graph immediately suggests various extensions of a basic
graph. For example, E may be a multiset, with multiple instances of edges
containing the same two vertices, perhaps, in the example above, represent-
ing multiple papers in which the two authors are coauthors. As we stated
above, a graph in which E is a multiset is called a multigraph. Instead of
just the presence or absence of edges between vertices, a weighted graph may
be more useful; that is, one in which a real number is associated with a pair
of vertices to represent the strength of the relationship, not just presence or
absence, between the two vertices. A degenerate weighted graph (that is, an
unweighted graph as discussed above) has weights of 0 or 1 between all ver-
tices. A multigraph is a weighted graph in which the weights are restricted
to nonnegative integers. Although the data in a weighted graph carry much
more information than a graph with only its edges, or even a multigraph that
allows strength to be represented by multiple edges, the simplicity of a graph
sometimes recommends its use even when there are varying degrees of strength
of relationships. A standard approach in applications is to set a threshold for
the strength of relationship and to deﬁne an edge only when the threshold is
exceeded.

334
8 Matrices with Special Properties
8.1.2.1 Adjacency Matrix: Connectivity Matrix
The connections between vertices in the graphs shown in Fig. 8.2 or in Fig. 8.4
can be represented in an association matrix called an adjacency matrix, a con-
nectivity matrix, or an incidence matrix to represent edges between vertices,
as shown in equation (8.1). (The terms “adjacency”, “connectivity”, and “in-
cidence” are synonymous. “Adjacency” is perhaps the most commonly used
term, but I will naturally use both that term and “connectivity” because of
the connotative value of the latter term.) The graph, G, represented in Fig. 8.2
has the symmetric adjacency matrix
A(G) =
⎡
⎢⎢⎢⎢⎣
0 1 1 1 1
1 0 0 0 1
1 0 0 1 0
1 0 1 0 1
1 1 0 1 0
⎤
⎥⎥⎥⎥⎦
.
(8.1)
As above, we often use this kind of notation; a symbol, such as G, represents a
particular graph, and other objects that relate to the graph make use of that
symbol.
There is no diﬀerence in the connectivity matrix and a table such as in
Fig. 8.3 except for the metadata.
A graph as we have described it is “nondirected”; that is, an edge has
no direction. The edge (a, b) is the same as (b, a). An adjacency matrix for a
nondirected graph is symmetric.
An interesting property of an adjacency matrix, which we will discuss
further on page 393, is that if A is the adjacency matrix of a graph, then Ak
ij
is the number of paths of length k between nodes i and j in that graph. (See
Exercise 8.20.)
The adjacency matrix for graph with no loops is hollow; that is, all diagonal
elements are 0s. Another common way of representing a hollow adjacency
matrix is to use −1s in place of the oﬀ-diagonal zeroes; that is, the absence
of a connection between two diﬀerent vertices is denoted by −1 instead of
by 0. Such a matrix is called a Seidel adjacency matrix. (This matrix has no
relationship to the Gauss-Seidel method discussed in Chap. 6.)
The relationship can obviously be deﬁned in the other direction; that is,
given an n × n symmetric matrix A, we deﬁne the graph of the matrix as the
graph with n vertices and edges between vertices i and j if aij ̸= 0. We often
denote the graph of the matrix A by G(A).
Generally we restrict the elements of the connectivity matrix to be 1 or 0 to
indicate only presence or absence of a connection, but not to indicate strength
of the connection. In this case, a connectivity matrix is a nonnegative matrix;
that is, all of its elements are nonnegative. We indicate that a matrix A is
nonnegative by
A ≥0.
We discuss the notation and properties of nonnegative (and positive) matrices
in Sect. 8.7.

8.1 Data Matrices and Association Matrices
335
8.1.2.2 Digraphs
Another extension of a basic graph is one in which the relationship may not
be the same in both directions. This yields a digraph, or “directed graph”,
in which the edges are ordered pairs called directed edges. The vertices in
a digraph have two kinds of degree, an indegree and an outdegree, with the
obvious meanings.
The simplest applications of digraphs are for representing networks. Con-
sider, for example, the digraph represented by the network in Fig. 8.4. This is a
network with ﬁve vertices, perhaps representing cities, and directed edges be-
tween some of the vertices. The edges could represent airline connections be-
tween the cities; for example, there are ﬂights from x to u and from u to x,
and from y to z, but not from z to y.
x
u
w
y
z
Figure 8.4. A simple digraph
Figure 8.4 represents a digraph with order 5 (there are ﬁve vertices) and
size 11 (eleven directed edges). A sequence of edges, e1, . . . , en, constituting
a path in a digraph must be such that for i ≥2 the ﬁrst vertex in ei is the
second vertex in edge ei−1. For example, the sequence x, y, z, w, u, x in the
graph of Fig. 8.4 is a path (in fact, a cycle) but the sequence x, u, w, z, y, x is
not a path.
The connectivity matrix for the digraph in Fig. 8.4 with nodes ordered as
u, w, x, y, z is
C =
⎡
⎢⎢⎢⎢⎣
0 1 1 1 1
1 0 0 0 0
1 0 0 1 0
1 0 0 0 1
1 1 0 0 0
⎤
⎥⎥⎥⎥⎦
.
(8.2)
A connectivity matrix for a (nondirected) graph is symmetric, but for a di-
graph it is not necessarily symmetric. Given an n × n matrix A, we deﬁne the
digraph of the matrix as the digraph with n vertices and edges from vertex i
to j if aij ̸= 0. We use the same notation for a digraph as we used above for
a graph, G(A).

336
8 Matrices with Special Properties
In statistical applications, graphs are used for representing symmetric
associations. Digraphs are used for representing asymmetric associations or
one-way processes such as a stochastic process.
In a simple digraph, the edges only indicate the presence or absence of a
relationship, but just as in the case of a simple graph, we can deﬁne a weighted
digraph by associating nonnegative numbers with each directed edge.
Graphical modeling is useful for analyzing relationships between elements
of a collection of sets. For example, in an analysis of internet traﬃc, proﬁles
of users may be constructed based on the set of web sites each user visits in
relation to the sets visited by other users. For this kind of application, an in-
tersection graph may be useful. An intersection graph, for a given collection
of sets S, is a graph whose vertices correspond to the sets in S and whose
edges between any two sets have a common element.
The word “graph” is often used without qualiﬁcation to mean any of these
types.
8.1.2.3 Connectivity of Digraphs
There are two kinds of connected digraphs. A digraph such that there is
a (directed) path that includes any pair of vertices is said to be strongly
connected. A digraph such that there is a path without regard to the direction
of any edge that includes any pair of vertices is said to be weakly connected.
The digraph shown in Fig. 8.4 is strongly connected. The digraph shown in
Fig. 8.5 is weakly connected but not strongly connected.
A digraph that is not weakly connected must have two sets of nodes with
no edges between any nodes in one set and any nodes in the other set.
x
u
w
y
z
Figure 8.5. A digraph that is not strongly connected
The connectivity matrix of the digraph in Fig. 8.5 is
C =
⎡
⎢⎢⎢⎢⎣
0 1 1 0 1
0 0 0 0 0
0 0 0 1 0
1 0 0 0 1
0 1 0 0 0
⎤
⎥⎥⎥⎥⎦
.
(8.3)

8.1 Data Matrices and Association Matrices
337
The matrix of a digraph that is not strongly connected can always be reduced
to a special block upper triangular form by row and column permutations; that
is, if the digraph G is not strongly connected, then there exists a permutation
matrix E(π) such that
E(π)A(G)E(π) =
!B11 B12
0
B22
"
,
(8.4)
where B11 and B22 are square. Such a transformation is called a symmetric
permutation.
Later we will formally prove this relationship between strong connectivity
and this reduced form of the matrix, but ﬁrst we consider the matrix in
equation (8.3). If we interchange the second and fourth columns and rows, we
get the reduced form
E24CE24 =
⎡
⎢⎢⎢⎢⎣
0 0 1 1 1
1 0 0 0 1
0 1 0 0 0
0 0 0 0 0
0 0 0 1 0
⎤
⎥⎥⎥⎥⎦
.
8.1.2.4 Irreducible Matrices
Any nonnegative square matrix that can be permuted into the form in equa-
tion (8.4) with square diagonal submatrices is said to be reducible; a matrix
that cannot be put into that form is irreducible. We also use the terms re-
ducible and irreducible to refer to the graph itself.
Irreducible matrices have many interesting properties, some of which we
will discuss in Sect. 8.7.3, beginning on page 375. The implication (8.77) in
that section provides a simple characterization of irreducibility.
8.1.2.5 Strong Connectivity of Digraphs and Irreducibility
of Matrices
A nonnegative matrix is irreducible if and only if its digraph is strongly con-
nected. Stated another way, a digraph is not strongly connected if and only if
its matrix is reducible.
To see this, ﬁrst consider a reducible matrix. In its reduced form of equa-
tion (8.4), none of the nodes corresponding to the last rows have directed
edges leading to any of the nodes corresponding to the ﬁrst rows; hence, the
digraph is not strongly connected.
Now, assume that a given digraph G is not strongly connected. In that
case, there is some node, say the ith node, from which there is no directed
path to some other node. Assume that there are m −1 nodes that can be
reached from node i. If m = 1, then we have a trivial partitioning of the n× n
connectivity in which B11 of equation (8.4) is (n −1) × (n −1) and B22 is

338
8 Matrices with Special Properties
a 1 × 1 0 matrix (that is, 01). If m ≥1, perform symmetric permutations
so that the row corresponding to node i and all other m −1 nodes are the
last m rows of the permuted connectivity matrix. In this case, the ﬁrst n −m
elements in each of those rows must be 0. To see that this must be the case,
let k > n −m and j ≤n −m and assume that the element in the (k, j)th
position is nonzero. In that case, there is a path from node i to node k to node
j, which is in the set of nodes not reachable from node i; hence the (k, j)th
element (in the permuted matrix) must be 0. The submatrix corresponding
to B11 is n −m × n −m, and that corresponding to B22 is m × m. These
properties also hold for connectivity matrices with simple loops (with 1s on
the diagonal) and for an augmented connectivity matrix (see page 393).
Reducibility plays an important role in the analysis of Markov chains (see
Sect. 9.8.1).
8.1.3 Term-by-Document Matrices
An interesting area of statistical application is in clustering and classifying
documents. In the simpler cases, the documents consist of text only, and
much recent research has been devoted to “text data-mining”. (The problem
is not a new one; Mosteller and Wallace 1963, studied a related problem.)
We have a set of text documents (often called a “corpus”). A basic set
of data to use in studying the text documents is the term-document matrix,
which is a matrix whose columns correspond to documents and whose rows
correspond to the various terms used in the documents. The terms are usu-
ally just words. The entries in the term-document matrix are measures of
the importance of the term in the document. Importance may be measured
simply by the number of times the word occurs in the document, possibly
weighted by some measure of the total number of words in the document. In
other measures of the importance, the relative frequency of the word in the
given document may be adjusted for the relative frequency of the word in the
corpus. (Such a measure is called term frequency-inverse document frequency
or tf-idf.) Certain common words, such as “the”, called “stop-words”, may be
excluded from the data. Also, words may be “stemmed”; that is, they may
be associated with a root that ignores modifying letters such as an “s” in a
plural or an “ed” in a past-tense verb. Other variations include accounting for
sequences of terms (called “collocation”).
There are several interesting problems that arise in text analysis. Term-
document matrices can be quite large. Terms are often misspelled. The docu-
ments may be in diﬀerent languages. Some terms may have multiple meanings.
The documents themselves may be stored on diﬀerent computer systems.
Two types of manipulation of the term-document matrix are commonly
employed in the analysis. One is singular-value decomposition (SVD), and the
other is nonnegative matrix factorization (NMF).
SVD is used in what is called “latent semantic analysis” (“LSA”), or “la-
tent semantic indexing” (“LSI”). A variation of latent semantic analysis is

8.1 Data Matrices and Association Matrices
339
“probabilistic” latent semantic analysis, in which a probability distribution is
assumed for the words. In a speciﬁc instance of probabilistic latent semantic
analysis, the probability distribution is a Dirichlet distribution. (Most users
of this latter method are not statisticians; they call the method “LDA”, for
“latent Dirichlet allocation”.)
NMF is often used in determining which documents are similar to each
other or, alternatively, which terms are clustered in documents. If A is the
term-document matrix, and A = WH is a nonnegative factorization, then
the element wij can be interpreted as the degree to which term i belongs
to cluster j, while element hij can be interpreted as the degree to which
document j belongs to cluster i. A method of clustering the documents is to
assign document j (corresponding to the jth column of A) to the kth cluster
if hkj is the maximum element in the column h∗j.
There is a wealth of literature on text data-mining, but we will not discuss
the analysis methods further.
8.1.4 Probability Distribution Models
Probability models in statistical data analysis are often multivariate distribu-
tions, and hence, matrices arise in the model. In the analysis itself, matrices
that represent associations are computed from the observational data. In this
section we mention some matrices in the models, and in the next section we
refer to some association matrices that are computed in the analysis.
Data in rows of ﬂat ﬁles are often assumed to be realizations of vector
random variables, some elements of which may have a degenerate distribution
(that is, the elements in some columns of the data matrix may be considered
to be ﬁxed rather than random). The data in one row are often considered
independent of the data in another row. Statistical data analysis is generally
concerned with studying various models of relationships among the elements of
the vector random variables. For example, the familiar linear regression model
relates one variable (one column) to a linear combination of other variables
plus a translation and random noise.
A random graph of ﬁxed order is a discrete probability space over all possi-
ble graphs of that order. For a graph of order n, there are 2(n
2) possible graphs.
Asymptotic properties of the probability distribution refer to the increase of
the order without limit. Occasionally it is useful to consider the order of the
graph to be random also. If the order is unrestricted, the sample space for
a random graph of random order is inﬁnite but countable. The number of
digraphs of order n is 4(n
2).
Random graphs have many uses in the analysis of large systems of in-
teracting objects; for example, a random intersection graph may be used to
make inferences about the clustering of internet users based on the web sites
they visit.

340
8 Matrices with Special Properties
8.1.5 Derived Association Matrices
In data analysis, the interesting questions usually involve the relationships
among the variables or among the observational units. Matrices formed from
the original data matrix for the purpose of measuring these relationships
are called association matrices. There are basically two types: similarity and
dissimilarity matrices. The variance-covariance matrix, which we discuss in
Sect. 8.6.3, is an example of an association matrix that measures similarity.
We discuss dissimilarity matrices in Sect. 8.6.6 and in Sect. 8.8.9 discuss a
type of similarity matrix for data represented in graphs.
In addition to the distinction between similarity and dissimilarity associ-
ation matrices, we may identify two types of association matrices based on
whether the relationships of interest are among the rows (observations) or
among the columns (variables or features). In applications, dissimilarity rela-
tionships among rows tend to be of more interest, and similarity relationships
among columns are usually of more interest. (The applied statistician may
think of clustering, multidimensional scaling, or Q factor analysis for the for-
mer and correlation analysis, principal components analysis, or factor analysis
for the latter.)
8.2 Symmetric Matrices and Other Unitarily
Diagonalizable Matrices
Most association matrices encountered in applications are real and symmetric.
Because real symmetric matrices occur so frequently in statistical applications
and because such matrices have so many interesting properties, it is useful to
review some of those properties that we have already encountered and to state
some additional properties.
First, perhaps, we should iterate a trivial but important fact: the product
of symmetric matrices is not, in general, symmetric. A power of a symmetric
matrix, however, is symmetric.
We should also emphasize that some of the special matrices we have dis-
cussed are assumed to be symmetric because, if they were not, we could deﬁne
equivalent symmetric matrices. This includes positive deﬁnite matrices and
more generally the matrices in quadratic forms.
8.2.1 Some Important Properties of Symmetric Matrices
For convenience, here we list some of the important properties of symmetric
matrices, many of which concern their eigenvalues. In the following, let A be a
real symmetric matrix with eigenvalues ci and corresponding eigenvectors vi.
•
If k is any positive integer, Ak is symmetric.
•
AB is not necessarily symmetric even if B is a symmetric matrix.

8.2 Symmetric Matrices and Other Unitarily Diagonalizable Matrices
341
•
A ⊗B is symmetric if B is symmetric.
•
If A is nonsingular, then A−1 is also symmetric because (A−1)T =
(AT)−1 = A−1.
•
If A is nonsingular (so that Ak is deﬁned for nonpositive integers), Ak is
symmetric and nonsingular for any integer k.
•
All eigenvalues of A are real (see page 140).
•
A is diagonalizable (or simple), and in fact A is orthogonally diago-
nalizable; that is, it has an orthogonally similar canonical factorization,
A = VCV T (see page 154).
•
A has the spectral decomposition A = 
i civivT
i , where the ci are the
eigenvalues and vi are the corresponding eigenvectors (see page 155).
•
A power of A has the spectral decomposition Ak = 
i ck
i vivT
i .
•
Any quadratic form xTAx can be expressed as 
i b2
i ci, where the bi are
elements in the vector V −1x.
•
We have
max
x̸=0
xTAx
xTx = max{ci}
(see page 156). If A is nonnegative deﬁnite, this is the spectral radius ρ(A).
•
For the L2 norm of the symmetric matrix A, we have
∥A∥2 = ρ(A).
•
For the Frobenius norm of the symmetric matrix A, we have
∥A∥F =
&
c2
i .
This follows immediately from the fact that A is diagonalizable, as do the
following properties.
•
tr(A) =

ci
•
|A| =
$
ci
(see equations (3.227) and (3.228) on page 140).
8.2.2 Approximation of Symmetric Matrices and an Important
Inequality
In Sect. 3.10, we considered the problem of approximating a given matrix by
another matrix of lower rank. There are other situations in statistics in which
we need to approximate one matrix by another one. In data analysis, this may
be because our given matrix arises from poor observations and we know the
“true” matrix has some special properties not possessed by the given matrix
computed from the data. A familiar example is a sample variance-covariance

342
8 Matrices with Special Properties
matrix computed from incomplete data (see Sect. 9.5.6). Other examples in
statistical applications occur in the simulation of random matrices (see Gentle
2003; Section 5.3.3). In most cases of interest, the matrix to be approximated
is a symmetric matrix.
Consider the diﬀerence of two symmetric n×n matrices, A and A; that is,
E = A −A.
(8.5)
The matrix of the diﬀerences, E, is also symmetric. We measure the “close-
ness” of A and A by some norm of E.
The Hoﬀman-Wielandt theorem gives a lower bound on the Frobenius
norm of E in terms of the diﬀerences of the eigenvalues of A and A: if the
eigenvalues of A are c1, . . . cn and the eigenvalues of A are ˜c1, . . . ˜cn, each set
being arranged in nonincreasing order, we have
n

i=1
(ci −˜ci)2 ≤∥E∥2
F.
(8.6)
This fact was proved by Hoﬀman and Wielandt (1953) using techniques from
linear programming. Wilkinson (1965) gives a simpler proof (which he at-
tributes to Wallace Givens) along the following lines.
Because A, A, and E are symmetric, they are all orthogonally diagonaliz-
able. Let the diagonal factorizations of A and E, respectively, be VCV T and
Udiag((e1, . . . , en))U T, where e1, . . . en are the eigenvalues of E in nonincreas-
ing order. Hence, we have
Udiag((e1, . . . , en))U T = U(A −A)U T
= U(VCV T −A)U T
= UV(C −V T AV )V TU T.
Taking norms of both sides, we have
n

i=1
e2
i = ∥C −V T AV ∥2.
(8.7)
(All norms in the remainder of this section will be the Frobenius norm.) Now,
let
f(Q) = ∥C −QT AQ∥2
(8.8)
be a function of any n × n orthogonal matrix, Q. (Equation (8.7) yields
f(V ) =  e2
i .) To arrive at inequality (8.6), we show that this function is
bounded below by the sum of the diﬀerences in the squares of the elements of
C (which are the eigenvalues of A) and the eigenvalues of QT AQ (which are
the eigenvalues of the matrix approximating A).
Because the elements of Q are bounded, f(·) is bounded, and because the
set of orthogonal matrices is compact (see page 133) and f(·) is continuous,
f(·) must attain its lower bound, say l. To simplify the notation, let

8.2 Symmetric Matrices and Other Unitarily Diagonalizable Matrices
343
X = QT AQ.
Now suppose that there are r distinct eigenvalues of A (that is, the diagonal
elements in C):
d1 > · · · > dr.
We can write C as diag(diImi), where mi is the multiplicity of di. We
now partition QT AQ to correspond to the partitioning of C represented by
diag(diImi):
X =
⎡
⎢⎣
X11 · · · X1r
...
...
...
Xr1 · · · Xrr
⎤
⎥⎦.
(8.9)
In this partitioning, the diagonal blocks, Xii, are mi×mi symmetric matrices.
The submatrix Xij, is an mi × mj matrix.
We now proceed in two steps to show that in order for f(Q) to attain its
lower bound l, X must be diagonal. First we will show that when f(Q) = l,
the submatrix Xij in equation (8.9) must be null if i ̸= j. To this end, let Q∇
be such that f(Q∇) = l, and assume the contrary regarding the corresponding
X∇= QT
∇AQ∇; that is, assume that in some submatrix Xij∇where i ̸= j, there
is a nonzero element, say x∇. We arrive at a contradiction by showing that
in this case there is another X0 of the form QT
0 AQ0, where Q0 is orthogonal
and such that f(Q0) < f(Q∇).
To establish some useful notation, let p and q be the row and column,
respectively, of X∇where this nonzero element x∇occurs; that is, xpq = x∇̸=
0 and p ̸= q because xpq is in Xij∇. (Note the distinction between uppercase
letters, which represent submatrices, and lowercase letters, which represent
elements of matrices.) Also, because X∇is symmetric, xqp = x∇. Now let a∇=
xpp and b∇= xqq. We form Q0 as Q∇R, where R is an orthogonal rotation
matrix of the form Gpq in equation (5.12) on page 239. We have, therefore,
∥QT
0 AQ0∥2 = ∥RTQT
∇AQ∇R∥2 = ∥QT
∇AQ∇∥2. Let a0, b0, and x0 represent
the elements of QT
0 AQ0 that correspond to a∇, b∇, and x∇in QT
∇AQ∇.
From the deﬁnition of the Frobenius norm, we have
f(Q0) −f(Q∇) = 2(a∇−a0)di + 2(b∇−b0)dj
because all other terms cancel. If the angle of rotation is θ, then
a0 = a∇cos2 θ −2x∇cos θ sin θ + b∇sin2 θ,
b0 = a∇sin2 θ −2x∇cos θ sin θ + b∇cos2 θ,
and so for a function h of θ we can write
h(θ) = f(Q0) −f(Q∇)
= 2di((a∇−b∇) sin2 θ + x∇sin 2θ) + 2dj((b∇−b0) sin2 θ −x∇sin 2θ)
= 2di((a∇−b∇) + 2dj(b∇−b0)) sin2 θ + 2x∇(di −dj) sin 2θ,

344
8 Matrices with Special Properties
and so
d
dθ h(θ) = 2di((a∇−b∇) + 2dj(b∇−b0)) sin 2θ + 4x∇(di −dj) cos 2θ.
The coeﬃcient of cos 2θ, 4x∇(di−dj), is nonzero because di and dj are distinct,
and x∇is nonzero by the second assumption to be contradicted, and so the
derivative at θ = 0 is nonzero. Hence, by the proper choice of a direction of
rotation (which eﬀectively interchanges the roles of di and dj), we can make
f(Q0)−f(Q∇) positive or negative, showing that f(Q∇) cannot be a minimum
if some Xij in equation (8.9) with i ̸= j is nonnull; that is, if Q∇is a matrix
such that f(Q∇) is the minimum of f(Q), then in the partition of QT
∇AQ∇
only the diagonal submatrices Xii∇can be nonnull:
QT
∇AQ∇= diag(X11∇, . . . , Xrr∇).
The next step is to show that each Xii∇must be diagonal. Because it is
symmetric, we can diagonalize it with an orthogonal matrix Pi as
P T
i Xii∇Pi = Gi.
Now let P be the direct sum of the Pi and form
P TCP −P TQT
∇AQ∇P = diag(d1I, . . . , drI) −diag(G1, . . . , Gr)
= C −P TQT
∇AQ∇P.
Hence,
f(Q∇P) = f(Q∇),
and so the minimum occurs for a matrix Q∇P that reduces A to a diagonal
form. The elements of the Gi must be the ˜ci in some order, so the minimum
of f(Q), which we have denoted by f(Q∇), is (ci −˜cpi)2, where the pi are a
permutation of 1, . . . , n. As the ﬁnal step, we show pi = i. We begin with p1.
Suppose p1 ̸= 1 but ps = 1; that is, ˜c1 ≥˜cp1. Interchange p1 and ps in the
permutation. The change in the sum (ci −˜cpi)2 is
(c1 −˜c1)2 + (cs −˜cps)2 −(c1 −˜cps)2 −(cs −˜c1)2 = −2(cs −c1)(˜cp1 −˜c1)
≤0;
that is, the interchange reduces the value of the sum. Similarly, we proceed
through the pi to pn, getting pi = i.
We have shown, therefore, that the minimum of f(Q) is n
i=1(ci −˜ci)2,
where both sets of eigenvalues are ordered in nonincreasing value. From equa-
tion (8.7), which is f(V ), we have the inequality (8.6).
While an upper bound may be of more interest in the approximation prob-
lem, the lower bound in the Hoﬀman-Wielandt theorem gives us a measure of
the goodness of the approximation of one matrix by another matrix. There are
various extensions and other applications of the Hoﬀman-Wielandt theorem,
see Chu (1991).

8.2 Symmetric Matrices and Other Unitarily Diagonalizable Matrices
345
8.2.3 Normal Matrices
A real square matrix A is said to be normal if ATA = AAT. (In general,
a square matrix is normal if AHA = AAH.) The Gramian matrix formed
from a normal matrix is the same as the Gramian formed from the transpose
(or conjugate transpose) of the matrix. Normal matrices include symmetric
(and Hermitian), skew symmetric (and Hermitian), square orthogonal (and
unitary) matrices, and circulant matrices. The identity is also obviously a
normal matrix.
There are a number of interesting properties possessed by a normal ma-
trix, but the most important property is that it can be diagonalized by a
unitary matrix. Recall from page 154 that a matrix can be orthogonally diag-
onalized if and only if the matrix is symmetric. Not all normal matrices can
be orthogonally diagonalized, but all can be diagonalized by a unitary matrix
(“unitarily diagonalized”). In fact, a matrix can be unitarily diagonalized if
and only if the matrix is normal. (This is the reason the word “normal” is
used to describe these matrices; and a matrix that is unitarily diagonalizable
is an alternate, and more meaningful way of deﬁning a normal matrix.)
It is easy to see that a matrix A is unitarily diagonalizable if and only if
AHA = AAH (and for real A, ATA = AAT implies AHA = AAH).
First suppose A is unitarily diagonalizable. Let A = P D P H, where P is
unitary and D is diagonal. In that case, the elements of D are the eigenvalues
of A, as we see by considering each column in AP. Now consider AAH:
AAH = P D P HP DH P H = P D DH P H =
P DHD P H = P DH P HP D P H = AHA.
Next, suppose AHA = AAH. To see that A is unitarily diagonalizable, form
the Schur factorization, A = UT U H (see Sect. 3.8.7 on page 147). We have
AHA = UT HU HUT U H = UT HT U H
and
AAH = UT U HUT HU H = UT T HU H.
Now under the assumption that AHA = AAH,
UT HT U H = UT T HU H,
which implies T HT = T T H, in which case
|tii|2 =
n

j=1
|tij|2,
that is, tij = 0 unless j = i. We conclude that T is diagonal, and hence, the
Schur factorization, A = UT U H a is unitary diagonalization of A, so A is
unitarily diagonalizable.

346
8 Matrices with Special Properties
Spectral methods, based on the unitary diagonalization, are useful in many
areas of applied mathematics. The spectra of nonnormal matrices, however,
are quite diﬀerent (see Trefethen and Embree (2005)).
8.3 Nonnegative Deﬁnite Matrices: Cholesky
Factorization
We deﬁned nonnegative deﬁnite and positive deﬁnite matrices on page 91, and
discussed some of their properties, particularly in Sect. 3.8.11. We have seen
that these matrices have useful factorizations, in particular, the square root
and the Cholesky factorization. In this section, we recall those deﬁnitions,
properties, and factorizations.
A symmetric matrix A such that any quadratic form involving the matrix
is nonnegative is called a nonnegative deﬁnite matrix. That is, a symmetric
matrix A is a nonnegative deﬁnite matrix if, for any (conformable) vector x,
xTAx ≥0.
(8.10)
(We remind the reader that there is a related term, positive semideﬁnite ma-
trix, that is not used consistently in the literature. We will generally avoid the
term “semideﬁnite”.)
We denote the fact that A is nonnegative deﬁnite by
A ⪰0.
(8.11)
(Some people use the notation A ≥0 to denote a nonnegative deﬁnite matrix,
but we have decided to use this notation to indicate that each element of A
is nonnegative; see page 64.)
There are several properties that follow immediately from the deﬁnition.
•
The sum of two (conformable) nonnegative matrices is nonnegative deﬁ-
nite.
•
All diagonal elements of a nonnegative deﬁnite matrix are nonnegative.
Hence, if A is nonnegative deﬁnite, tr(A) ≥0.
•
Any square submatrix whose principal diagonal is a subset of the principal
diagonal of a nonnegative deﬁnite matrix is nonnegative deﬁnite. In par-
ticular, any square principal submatrix of a nonnegative deﬁnite matrix is
nonnegative deﬁnite.
It is easy to show that the latter two facts follow from the deﬁnition by
considering a vector x with zeros in all positions except those corresponding
to the submatrix in question. For example, to see that all diagonal elements
of a nonnegative deﬁnite matrix are nonnegative, assume the (i, i) element is
negative, and then consider the vector x to consist of all zeros except for a 1
in the ith position. It is easy to see that the quadratic form is negative, so the
assumption that the (i, i) element is negative leads to a contradiction.

8.3 Nonnegative Deﬁnite Matrices: Cholesky Factorization
347
•
A diagonal matrix is nonnegative deﬁnite if and only if all of the diagonal
elements are nonnegative.
This must be true because a quadratic form in a diagonal matrix is the sum
of the diagonal elements times the squares of the elements of the vector.
We can also form other submatrices that are nonnegative deﬁnite:
•
If A is nonnegative deﬁnite, then A−(i1,...,ik)(i1,...,ik) is nonnegative deﬁnite.
(See page 599 for notation.)
Again, we can see this by selecting an x in the deﬁning inequality (8.10)
consisting of 1s in the positions corresponding to the rows and columns of A
that are retained and 0s elsewhere.
By considering xTCTACx and y = Cx, we see that
•
if A is nonnegative deﬁnite, and C is conformable for the multiplication,
then CTAC is nonnegative deﬁnite.
From equation (3.252) and the fact that the determinant of a product is
the product of the determinants, we have that
•
the determinant of a nonnegative deﬁnite matrix is nonnegative.
Finally, for the nonnegative deﬁnite matrix A, we have
a2
ij ≤aiiajj,
(8.12)
as we see from the deﬁnition xTAx ≥0 and choosing the vector x to have a
variable y in position i, a 1 in position j, and 0s in all other positions. For
a symmetric matrix A, this yields the quadratic aiiy2 + 2aijy + ajj. If this
quadratic is to be nonnegative for all y, then the discriminant 4a2
ij −4aiiajj
must be nonpositive; that is, inequality (8.12) must be true.
8.3.1 Eigenvalues of Nonnegative Deﬁnite Matrices
We have seen on page 159 that a real symmetric matrix is nonnegative (pos-
itive) deﬁnite if and only if all of its eigenvalues are nonnegative (positive).
This fact allows a generalization of the statement above: a triangular ma-
trix is nonnegative (positive) deﬁnite if and only if all of the diagonal elements
are nonnegative (positive).
8.3.2 The Square Root and the Cholesky Factorization
Two important factorizations of nonnegative deﬁnite matrices are the square
root,
A = (A
1
2 )2,
(8.13)
discussed in Sect. 5.9.1, and the Cholesky factorization,
A = T TT,
(8.14)

348
8 Matrices with Special Properties
discussed in Sect. 5.9.2. If T is as in equation (8.14), the symmetric matrix
T + T T is also nonnegative deﬁnite, or positive deﬁnite if A is.
The square root matrix is used often in theoretical developments, such as
Exercise 4.7b for example, but the Cholesky factor is more useful in practice.
The Cholesky factorization also has a prominent role in multivariate analysis,
where it appears in the Bartlett decomposition. If W is a Wishart matrix
with variance-covariance matrix Σ (see Exercise 4.12 on page 224), then the
Bartlett decomposition of W is
W = (T U)TT U,
where U is the Cholesky factor of Σ, and T is an upper triangular matrix with
positive diagonal elements. The diagonal elements of T have independent chi-
squared distributions and the oﬀ-diagonal elements of T have independent
standard normal distributions.
8.3.3 The Convex Cone of Nonnegative Deﬁnite Matrices
The class of all n × n nonnegative deﬁnite matrices is a cone because if X is
a nonnegative deﬁnite matrix and a > 0, then aX is a nonnegative deﬁnite
matrix (see page 43). Furthermore, it is convex cone in IRn×n, because if X1
and X2 are n × n nonnegative deﬁnite matrices and a, b ≥0, then aX1 + bX2
is nonnegative deﬁnite so long as either a > 0 or b > 0.
This class is not closed under Cayley multiplication (that is, in particu-
lar, it is not a group with respect to that operation). The product of two
nonnegative deﬁnite matrices might not even be symmetric.
The convex cone of nonnegative deﬁnite matrices is an important object in
a common optimization problem called convex cone programming (“program-
ming” here means “optimization”). A special case of convex cone program-
ming is called “semideﬁnite programming”, or “SDP” (where “semideﬁnite”
comes from the alternative terminology for nonnegative deﬁnite). The canon-
ical SDP problem for given n × n symmetric matrices C and Ai, and real
numbers bi, for i = 1, . . . , m, is
minimize ⟨C, X⟩
subject to ⟨Ai, X⟩= bi
i = 1, . . . , m
X ⪰0.
The notation ⟨C, X⟩here means the matrix inner product (see page 97). SDP
includes linear programming as a simple special case, in which C and X are
vectors. See Vandenberghe and Boyd (1996) for further discussion of the SDP
problem and applications.
8.4 Positive Deﬁnite Matrices
An important class of nonnegative deﬁnite matrices are those that satisfy
strict inequalities in the deﬁnition involving xTAx. These matrices are called

8.4 Positive Deﬁnite Matrices
349
positive deﬁnite matrices and they have all of the properties discussed above
for nonnegative deﬁnite matrices as well as some additional useful properties.
A symmetric matrix A is called a positive deﬁnite matrix if, for any (con-
formable) vector x ̸= 0, the quadratic form is positive; that is,
xTAx > 0.
(8.15)
We denote the fact that A is positive deﬁnite by
A ≻0.
(8.16)
(Some people use the notation A > 0 to denote a positive deﬁnite matrix,
but we have decided to use this notation to indicate that each element of A
is positive.)
The properties of nonnegative deﬁnite matrices noted above hold also for
positive deﬁnite matrices, generally with strict inequalities. It is obvious that
all diagonal elements of a positive deﬁnite matrix are positive. Hence, if A is
positive deﬁnite, tr(A) > 0. Furthermore, as above and for the same reasons, if
A is positive deﬁnite, then A−(i1,...,ik)(i1,...,ik) is positive deﬁnite. In particular,
•
Any square principal submatrix of a positive deﬁnite matrix is positive
deﬁnite.
Because a quadratic form in a diagonal matrix is the sum of the diagonal
elements times the squares of the elements of the vector, a diagonal matrix is
positive deﬁnite if and only if all of the diagonal elements are positive.
From equation (3.252) and the fact that the determinant of a product is
the product of the determinants, we have
•
The determinant of a positive deﬁnite matrix is positive.
•
If A is positive deﬁnite,
a2
ij < aiiajj,
(8.17)
which we see using the same argument as for inequality (8.12).
We have a slightly stronger statement regarding sums involving positive
deﬁnite matrices than what we could conclude about nonnegative deﬁnite
matrices:
•
The sum of a positive deﬁnite matrix and a (conformable) nonnegative
deﬁnite matrix is positive deﬁnite.
That is,
xTAx > 0 ∀x ̸= 0
and
yTBy ≥0 ∀y =⇒zT(A + B)z > 0 ∀z ̸= 0. (8.18)
•
A positive deﬁnite matrix is necessarily nonsingular. (We see this from
the fact that no nonzero combination of the columns, or rows, can be 0.)
Furthermore, if A is positive deﬁnite, then A−1 is positive deﬁnite. (We
showed this is Sect. 3.8.11, but we can see it in another way: because for
any y ̸= 0 and x = A−1y, we have yTA−1y = xTy = xTAx > 0.)

350
8 Matrices with Special Properties
•
A (strictly) diagonally dominant symmetric matrix with positive diagonals
is positive deﬁnite. The proof of this is Exercise 8.3.
•
A positive deﬁnite matrix is orthogonally diagonalizable.
•
A positive deﬁnite matrix has a square root.
•
A positive deﬁnite matrix Cholesky factorization.
We cannot conclude that the product of two positive deﬁnite matrices is
positive deﬁnite, but we do have the useful fact:
•
If A is positive deﬁnite, and C is of full rank and conformable for the
multiplication AC, then CTAC is positive deﬁnite (see page 114).
We have seen from the deﬁnition of positive deﬁniteness and the distribu-
tion of multiplication over addition that the sum of a positive deﬁnite matrix
and a nonnegative deﬁnite matrix is positive deﬁnite. We can deﬁne an or-
dinal relationship between positive deﬁnite and nonnegative deﬁnite matrices
of the same size. If A is positive deﬁnite and B is nonnegative deﬁnite of the
same size, we say A is strictly greater than B and write
A ≻B
(8.19)
if A −B is positive deﬁnite; that is, if A −B ≻0.
We can form a partial ordering of nonnegative deﬁnite matrices of the same
order based on this additive property. We say A is greater than B and write
A ⪰B
(8.20)
if A −B is either the 0 matrix or is nonnegative deﬁnite; that is, if A −B ⪰0
(see Exercise 8.2a). The “strictly greater than” relation implies the “greater
than” relation. These relations are partial in the sense that they do not apply
to all pairs of nonnegative matrices; that is, there are pairs of matrices A and
B for which neither A ⪰B nor B ⪰A.
If A ≻B, we also write B ≺A; and if A ⪰B, we may write B ⪯A.
8.4.1 Leading Principal Submatrices of Positive Deﬁnite Matrices
A suﬃcient condition for a symmetric matrix to be positive deﬁnite is that
the determinant of each of the leading principal submatrices be positive. To
see this, ﬁrst let the n × n symmetric matrix A be partitioned as
A =
!
An−1
a
aT
ann
"
,
and assume that An−1 is positive deﬁnite and that |A| > 0. (This is not the
same notation that we have used for these submatrices, but the notation is
convenient in this context.) From equation (3.192) on page 123,

8.4 Positive Deﬁnite Matrices
351
|A| = |An−1|(ann −aTA−1
n−1a).
Because An−1 is positive deﬁnite, |An−1| > 0, and so (ann −aTA−1
n−1a) > 0;
hence, the 1× 1 matrix (ann −aTA−1
n−1a) is positive deﬁnite. That any matrix
whose leading principal submatrices have positive determinants is positive
deﬁnite follows from this by induction, beginning with a 2 × 2 matrix.
8.4.2 The Convex Cone of Positive Deﬁnite Matrices
The class of all n × n positive deﬁnite matrices is a cone because if X is a
positive deﬁnite matrix and a > 0, then aX is a positive deﬁnite matrix (see
page 43). Furthermore, this class is a convex cone in IRn×n because if X1 and
X2 are n×n positive deﬁnite matrices and a, b ≥0, then aX1+bX2 is positive
deﬁnite so long as either a ̸= 0 or b ̸= 0.
As with the cone of nonnegative deﬁnite matrices this class is not closed
under Cayley multiplication.
8.4.3 Inequalities Involving Positive Deﬁnite Matrices
Quadratic forms of positive deﬁnite matrices and nonnegative matrices occur
often in data analysis. There are several useful inequalities involving such
quadratic forms.
On page 156, we showed that if x ̸= 0, for any symmetric matrix A with
eigenvalues ci,
xTAx
xTx ≤max{ci}.
(8.21)
If A is nonnegative deﬁnite, by our convention of labeling the eigenvalues, we
have max{ci} = c1. If the rank of A is r, the minimum nonzero eigenvalue
is denoted cr. Letting the eigenvectors associated with c1, . . . , cr be v1, . . . , vr
(and recalling that these choices may be arbitrary in the case where some
eigenvalues are not simple), by an argument similar to that used on page 156,
we have that if A is nonnegative deﬁnite of rank r,
vT
i Avi
vT
i vi
≥cr,
(8.22)
for 1 ≤i ≤r.
If A is positive deﬁnite and x and y are conformable nonzero vectors, we
see that
xTA−1x ≥(yTx)2
yTAy
(8.23)
by using the same argument as used in establishing the Cauchy-Schwarz in-
equality (2.26). We ﬁrst obtain the Cholesky factor T of A (which is, of course,
of full rank) and then observe that for every real number t

352
8 Matrices with Special Properties

tT y + T −Tx
T 
tT y + T −Tx

≥0,
and hence the discriminant of the quadratic equation in t must be nonnegative:
4

(T y)TT −Tx
2 −4

T −Tx
T 
T −T −x

(T y)TT y ≤0.
The inequality (8.23) is used in constructing Scheﬀ´e simultaneous conﬁdence
intervals in linear models.
The Kantorovich inequality for positive numbers has an immediate exten-
sion to an inequality that involves positive deﬁnite matrices. The Kantorovich
inequality, which ﬁnds many uses in optimization problems, states, for pos-
itive numbers c1 ≥c2 ≥· · · ≥cn and nonnegative numbers y1, . . . , yn such
that  yi = 1, that
 n

i=1
yici
  n

i=1
yic−1
i

≤(c1 + c2)2
4c1c2
.
Now let A be an n×n positive deﬁnite matrix with eigenvalues c1 ≥c2 ≥· · · ≥
cn > 0. We substitute x2 for y, thus removing the nonnegativity restriction,
and incorporate the restriction on the sum directly into the inequality. Then,
using the similar canonical factorization of A and A−1, we have

xTAx
 
xTA−1x

(xTx)2
≤(c1 + cn)2
4c1cn
.
(8.24)
This Kantorovich matrix inequality likewise has applications in optimization;
in particular, for assessing convergence of iterative algorithms.
The left-hand side of the Kantorovich matrix inequality also has a lower
bound,

xTAx
 
xTA−1x

(xTx)2
≥1,
(8.25)
which can be seen in a variety of ways, perhaps most easily by using the
inequality (8.23). (You were asked to prove this directly in Exercise 3.31.)
All of the inequalities (8.21) through (8.25) are sharp. We know that (8.21)
and (8.22) are sharp by using the appropriate eigenvectors. We can see the
others are sharp by using A = I.
There are several variations on these inequalities and other similar inequal-
ities that are reviewed by Marshall and Olkin (1990) and Liu and Neudecker
(1996).
8.5 Idempotent and Projection Matrices
An important class of matrices are those that, like the identity, have the
property that raising them to a power leaves them unchanged. A matrix A
such that

8.5 Idempotent and Projection Matrices
353
AA = A
(8.26)
is called an idempotent matrix. An idempotent matrix is square, and it is either
singular or the identity matrix. (It must be square in order to be conformable
for the indicated multiplication. If it is not singular, we have A = (A−1A)A =
A−1(AA) = A−1A = I; hence, an idempotent matrix is either singular or the
identity matrix.)
From the deﬁnition, it is clear that an idempotent matrix is its own Drazin
inverse: AD = A (see page 129).
An idempotent matrix that is symmetric is called a projection matrix.
8.5.1 Idempotent Matrices
Many matrices encountered in the statistical analysis of linear models are
idempotent. One such matrix is X−X (see page 124 and Sect. 9.3.2). This
matrix exists for any n × m matrix X, and it is square. (It is m × m.)
Because the eigenvalues of A2 are the squares of the eigenvalues of A, all
eigenvalues of an idempotent matrix must be either 0 or 1.
Any vector in the column space of an idempotent matrix A is an eigenvec-
tor of A. (This follows immediately from AA = A.) More generally, if x and
y are vectors in span(A) and a is a scalar, then
A(ax + y) = ax + y.
(8.27)
(To see this, we merely represent x and y as linear combinations of columns
(or rows) of A and substitute in the equation.)
The number of eigenvalues that are 1 is the rank of an idempotent matrix.
(Exercise 8.4 asks why this is the case.) We therefore have, for an idempotent
matrix A,
tr(A) = rank(A).
(8.28)
Because the eigenvalues of an idempotent matrix are either 0 or 1, a symmetric
idempotent matrix is nonnegative deﬁnite.
If A is idempotent and n × n, then
rank(I −A) = n −rank(A).
(8.29)
We showed this in equation (3.200) on page 125. (Although there we were
considering the special matrix A−A, the only properties used were the idem-
potency of A−A and the fact that rank(A−A) = rank(A).)
Equation (8.29) together with the diagonalizability theorem (equa-
tion (3.248)) implies that an idempotent matrix is diagonalizable.
If A is idempotent and V is an orthogonal matrix of the same size, then
V TAV is idempotent (whether or not V is a matrix that diagonalizes A)
because
(V TAV )(V TAV ) = V TAAV = V TAV.
(8.30)

354
8 Matrices with Special Properties
If A is idempotent, then (I −A) is also idempotent, as we see by mul-
tiplication. This fact and equation (8.29) have generalizations for sums of
idempotent matrices that are parts of Cochran’s theorem, which we consider
below.
Although if A is idempotent so (I −A) is also idempotent and hence is
not of full rank (unless A = 0), for any scalar a ̸= −1, (I + aA) is of full rank,
and
(I + aA)−1 = I −
a
a + 1A,
(8.31)
as we see by multiplication.
On page 146, we saw that similar matrices are equivalent (have the same
rank). For idempotent matrices, we have the converse: idempotent matrices
of the same rank (and size) are similar (see Exercise 8.5).
If A1 and A2 are matrices conformable for addition, then A1 + A2 is idem-
potent if and only if A1A2 = A2A1 = 0. It is easy to see that this condition
is suﬃcient by multiplication:
(A1 + A2)(A1 + A2) = A1A1 + A1A2 + A2A1 + A2A2 = A1 + A2.
To see that it is necessary, we ﬁrst observe from the expansion above that
A1 + A2 is idempotent only if A1A2 + A2A1 = 0. Multiplying this necessary
condition on the left by A1 yields
A1A1A2 + A1A2A1 = A1A2 + A1A2A1 = 0,
and multiplying on the right by A1 yields
A1A2A1 + A2A1A1 = A1A2A1 + A2A1 = 0.
Subtracting these two equations yields
A1A2 = A2A1,
and since A1A2 + A2A1 = 0, we must have A1A2 = A2A1 = 0.
8.5.1.1 Symmetric Idempotent Matrices
Many of the idempotent matrices in statistical applications are symmetric,
and such matrices have some useful properties.
Because the eigenvalues of an idempotent matrix are either 0 or 1, the
spectral decomposition of a symmetric idempotent matrix A can be written as
V TAV = diag(Ir, 0),
(8.32)
where V is a square orthogonal matrix and r = rank(A). (This is from equa-
tion (3.253) on page 154.)

8.5 Idempotent and Projection Matrices
355
For symmetric matrices, there is a converse to the fact that all eigenvalues
of an idempotent matrix are either 0 or 1. If A is a symmetric matrix all of
whose eigenvalues are either 0 or 1, then A is idempotent. We see this from the
spectral decomposition of A, A = V diag(Ir, 0)V T, and, with C = diag(Ir, 0),
by observing
AA = V CV TV CV T = V CCV T = V CV T = A,
because the diagonal matrix of eigenvalues C contains only 0s and 1s.
If A is symmetric and p is any positive integer,
Ap+1 = Ap =⇒A is idempotent.
(8.33)
This follows by considering the eigenvalues of A, c1, . . . , cn. The eigenvalues
of Ap+1 are cp+1
1
, . . . , cp+1
n
and the eigenvalues of Ap are cp
1, . . . , cp
n, but since
Ap+1 = Ap, it must be the case that cp+1
i
= cp
i for each i = 1, . . . , n. The only
way this is possible is for each eigenvalue to be 0 or 1, and in this case the
symmetric matrix must be idempotent.
There are bounds on the elements of a symmetric idempotent matrix.
Because A is symmetric and ATA = A,
aii =
n

j=1
a2
ij;
(8.34)
hence, 0 ≤aii. Rearranging equation (8.34), we have
aii = a2
ii +

j̸=i
a2
ij,
(8.35)
so a2
ii ≤aii or 0 ≤aii(1 −aii); that is, aii ≤1. Now, if aii = 0 or aii = 1, then
equation (8.35) implies

j̸=i
a2
ij = 0,
and the only way this can happen is if aij = 0 for all j ̸= i. So, in summary,
if A is an n × n symmetric idempotent matrix, then
0 ≤aii ≤1 for i = 1, . . . , m,
(8.36)
and
if aii = 0 or aii = 1, then aij = aji = 0 for all j ̸= i.
(8.37)
8.5.1.2 Cochran’s Theorem
There are various facts that are sometimes called Cochran’s theorem. The
simplest one concerns k symmetric idempotent n × n matrices, A1, . . . , Ak,
such that

356
8 Matrices with Special Properties
In = A1 + · · · + Ak.
(8.38)
Under these conditions, we have
AiAj = 0 for all i ̸= j.
(8.39)
We see this by the following argument. For an arbitrary j, as in equa-
tion (8.32), for some matrix V , we have
V TAjV = diag(Ir, 0),
where r = rank(Aj). Now
In = V TInV
=
k

i=1
V TAiV
= diag(Ir, 0) +

i̸=j
V TAiV,
which implies

i̸=j
V TAiV = diag(0, In−r).
(8.40)
Now, from equation (8.30), for each i, V TAiV is idempotent, and so from
equation (8.36) the diagonal elements are all nonnegative, and hence equa-
tion (8.40) implies that for each i ̸= j, the ﬁrst r diagonal elements are 0.
Furthermore, since these diagonal elements are 0, equation (8.37) implies that
all elements in the ﬁrst r rows and columns are 0. We have, therefore, for each
i ̸= j,
V TAiV = diag(0, Bi)
for some (n −r) × (n −r) symmetric idempotent matrix Bi. Now, for any
i ̸= j, consider AiAj and form V TAiAjV . We have
V TAiAjV = (V TAiV )(V TAjV )
= diag(0, Bi)diag(Ir, 0)
= 0.
Because V is nonsingular, this implies the desired conclusion; that is, that
AiAj = 0 for any i ̸= j.
We can now extend this result to an idempotent matrix in place of I;
that is, for an idempotent matrix A with A = A1 + · · · + Ak. Rather than
stating it simply as in equation (8.39), however, we will state the implications
diﬀerently.
Let A1, . . . , Ak be n × n symmetric matrices and let
A = A1 + · · · + Ak.
(8.41)

8.5 Idempotent and Projection Matrices
357
Then any two of the following conditions imply the third one:
(a). A is idempotent.
(b). Ai is idempotent for i = 1, . . . , k.
(c). AiAj = 0 for all i ̸= j.
This is also called Cochran’s theorem. (The theorem also applies to non-
symmetric matrices if condition (c) is augmented with the requirement that
rank(A2
i ) = rank(Ai) for all i. We will restrict our attention to symmetric
matrices, however, because in most applications of these results, the matrices
are symmetric.)
First, if we assume properties (a) and (b), we can show that property (c)
follows using an argument similar to that used to establish equation (8.39) for
the special case A = I. The formal steps are left as an exercise.
Now, let us assume properties (b) and (c) and show that property (a)
holds. With properties (b) and (c), we have
AA = (A1 + · · · + Ak) (A1 + · · · + Ak)
=
k

i=1
AiAi +

i̸=j
k

j=1
AiAj
=
k

i=1
Ai
= A.
Hence, we have property (a); that is, A is idempotent.
Finally, let us assume properties (a) and (c). Property (b) follows imme-
diately from
A2
i = AiAi = AiA = AiAA = A2
i A = A3
i
and the implication (8.33).
Any two of the properties (a) through (c) also imply a fourth property for
A = A1 + · · · + Ak when the Ai are symmetric:
(d). rank(A) = rank(A1) + · · · + rank(Ak).
We ﬁrst note that any two of properties (a) through (c) imply the third one,
so we will just use properties (a) and (b). Property (a) gives
rank(A) = tr(A) = tr(A1 + · · · + Ak) = tr(A1) + · · · + tr(Ak),
and property (b) states that the latter expression is rank(A1)+· · ·+rank(Ak),
thus yielding property (d).
There is also a partial converse: properties (a) and (d) imply the other
properties.
One of the most important special cases of Cochran’s theorem is when
A = I in the sum (8.41):

358
8 Matrices with Special Properties
In = A1 + · · · + Ak.
The identity matrix is idempotent, so if rank(A1) + · · · + rank(Ak) = n, all
the properties above hold.
The most important statistical application of Cochran’s theorem is for the
distribution of quadratic forms of normally distributed random vectors. These
distribution results are also called Cochran’s theorem. We brieﬂy discuss it in
Sect. 9.2.3.
8.5.2 Projection Matrices: Symmetric Idempotent Matrices
For a given vector space V, a symmetric idempotent matrix A whose columns
span V is said to be a projection matrix onto V; in other words, a matrix A is a
projection matrix onto span(A) if and only if A is symmetric and idempotent.
(Some authors do not require a projection matrix to be symmetric. In that
case, the terms “idempotent” and “projection” are synonymous.)
It is easy to see that, for any vector x, if A is a projection matrix onto
V, the vector Ax is in V, and the vector x −Ax is in V⊥(the vectors Ax
and x−Ax are orthogonal). For this reason, a projection matrix is sometimes
called an “orthogonal projection matrix”. Note that an orthogonal projection
matrix is not an orthogonal matrix, however, unless it is the identity matrix.
Stating this in alternative notation, if A is a projection matrix and A ∈IRn×n,
then A maps IRn onto V(A) and I −A is also a projection matrix (called the
complementary projection matrix of A), and it maps IRn onto the orthogonal
complement, N(A). These spaces are such that V(A) ⊕N(A) = IRn.
In this text, we use the term “projection” to mean “orthogonal projection”,
but we should note that in some literature “projection” can include “oblique
projection”. In the less restrictive deﬁnition, for vector spaces V, X, and Y, if
V = X ⊕Y and v = x + y with x ∈X and y ∈Y, then the vector x is called
the projection of v onto X along Y. In this text, to use the unqualiﬁed term
“projection”, we require that X and Y be orthogonal; if they are not, then
we call x the oblique projection of v onto X along Y. The choice of the more
restrictive deﬁnition is because of the overwhelming importance of orthogonal
projections in statistical applications. The restriction is also consistent with
the deﬁnition in equation (2.51) of the projection of a vector onto another
vector (as opposed to the projection onto a vector space).
Because a projection matrix is idempotent, the matrix projects any of
its columns onto itself, and of course it projects the full matrix onto itself:
AA = A (see equation (8.27)).
If x is a general vector in IRn, that is, if x has order n and belongs to an
n-dimensional space, and A is a projection matrix of rank r ≤n, then Ax has
order n and belongs to span(A), which is an r-dimensional space. Thus, we
say projections are dimension reductions.

8.6 Special Matrices Occurring in Data Analysis
359
Useful projection matrices often encountered in statistical linear models
are X+X and XX+. (Recall that for any generalized inverse X−X is an
idempotent matrix.) The matrix X+ exists for any n × m matrix X, and
X+X is square (m × m) and symmetric.
8.5.2.1 Projections onto Linear Combinations of Vectors
On page 36, we gave the projection of a vector y onto a vector x as
xTy
xTxx.
The projection matrix to accomplish this is the “outer/inner products ma-
trix”,
1
xTxxxT.
(8.42)
The outer/inner products matrix has rank 1. It is useful in a variety of matrix
transformations. If x is normalized, the projection matrix for projecting a
vector on x is just xxT. The projection matrix for projecting a vector onto a
unit vector ei is eieT
i , and eieT
i y = (0, . . . , yi, . . . , 0).
This idea can be used to project y onto the plane formed by two vectors, x1
and x2, by forming a projection matrix in a similar manner and replacing x in
equation (8.42) with the matrix X = [x1|x2]. On page 409, we will view linear
regression ﬁtting as a projection onto the space spanned by the independent
variables.
The angle between vectors we deﬁned on page 37 can be generalized to
the angle between a vector and a plane or any linear subspace by deﬁning
it as the angle between the vector and the projection of the vector onto the
subspace. By applying the deﬁnition (2.54) to the projection, we see that the
angle θ between the vector y and the subspace spanned by the columns of a
projection matrix A is determined by the cosine
cos(θ) = yTAy
yTy .
(8.43)
8.6 Special Matrices Occurring in Data Analysis
Some of the most useful applications of matrices are in the representation of
observational data, as in Fig. 8.1 on page 330. If the data are represented as
real numbers, the array is a matrix, say X. The rows of the n×m data matrix
X are “observations” and correspond to a vector of measurements on a single
observational unit, and the columns of X correspond to n measurements of a
single variable or feature. In data analysis we may form various association

360
8 Matrices with Special Properties
matrices that measure relationships among the variables or the observations
that correspond to the columns or the rows of X. Many summary statistics
arise from a matrix of the form XTX. (If the data in X are incomplete—
that is, if some elements are missing—problems may arise in the analysis. We
discuss some of these issues in Sect. 9.5.6.)
8.6.1 Gramian Matrices
A (real) matrix A such that for some (real) matrix B, A = BTB, is called
a Gramian matrix. Any nonnegative deﬁnite matrix is Gramian (from equa-
tion (8.14) and Sect. 5.9.2 on page 256). (This is not a deﬁnition of “Gramian”
or “Gram” matrix; these terms have more general meanings, but they do in-
clude any matrix expressible as BTB.)
8.6.1.1 Sums of Squares and Cross Products
Although the properties of Gramian matrices are of interest, our starting point
is usually the data matrix X, which we may analyze by forming a Gramian
matrix XTX or XXT (or a related matrix). These Gramian matrices are also
called sums of squares and cross products matrices. (The term “cross product”
does not refer to the cross product of vectors deﬁned on page 47, but rather
to the presence of sums over i of the products xijxik along with sums of
squares x2
ij.) These matrices and other similar ones are useful association
matrices in statistical applications.
8.6.1.2 Some Immediate Properties of Gramian Matrices
Some interesting properties of a Gramian matrix XTX that we have discussed
are:
•
XTX is symmetric.
•
rank(XTX) = rank(X).
•
XTX is of full rank if and only if X is of full column rank.
•
XTX is nonnegative deﬁnite.
•
XTX is positive deﬁnite if and only if X is of full column rank.
•
XTX = 0
⇐⇒
X = 0.
•
BXTX = CXTX
⇐⇒
BXT = CXT.
•
XTXB = XTXC
⇐⇒
XB = XC.
•
If d is a singular value of X, then c = d2 is an eigenvalue of XTX; or,
expressed another way,
if c is a nonzero eigenvalue of XTX, then there is a singular value d of X
such that d2 = c.
These properties were shown in Sect. 3.3.10, beginning on page 115, except
for the last one, which was shown on page 162.

8.6 Special Matrices Occurring in Data Analysis
361
Each element of a Gramian matrix is the dot product of columns of the
constituent matrix. If x∗i and x∗j are the ith and jth columns of the matrix
X, then
(XTX)ij = xT
∗ix∗j.
(8.44)
A Gramian matrix is also the sum of the outer products of the rows of the
constituent matrix. If xi∗is the ith row of the n × m matrix X, then
XTX =
n

i=1
xi∗xT
i∗.
(8.45)
This is generally the way a Gramian matrix is computed.
By equation (8.14), we see that any Gramian matrix formed from a general
matrix X is the same as a Gramian matrix formed from a square upper
triangular matrix T :
XTX = T TT.
8.6.1.3 Generalized Inverses of Gramian Matrices
The generalized inverses of XTX have useful properties. First, we see from
the deﬁnition, for any generalized inverse (XTX)−, that ((XTX)−)T is also a
generalized inverse of XTX. (Note that (XTX)−is not necessarily symmet-
ric.) Also, we have, from equation (3.162),
X(XTX)−XTX = X.
(8.46)
This means that (XTX)−XT is a generalized inverse of X.
The Moore-Penrose inverse of X has an interesting relationship with a
generalized inverse of XTX:
XX+ = X(XTX)−XT.
(8.47)
This can be established directly from the deﬁnition of the Moore-Penrose
inverse.
An important property of X(XTX)−XT is its invariance to the choice of
the generalized inverse of XTX. Suppose G is any generalized inverse of XTX.
Then, from equation (8.46), we have X(XTX)−XTX = XGXTX, and from
the implication (3.162), we have
XGXT = X(XTX)−XT;
(8.48)
that is, X(XTX)−XT is invariant to the choice of generalized inverse.

362
8 Matrices with Special Properties
8.6.1.4 Eigenvalues of Gramian Matrices
The nonzero eigenvalues of XTX are the same as the nonzero eigenvalues of
XXT (property 14 on page 140).
If the singular value decomposition of X is UDV T (page 161), then the sim-
ilar canonical factorization of XTX (equation (3.252)) is V DTDV T. Hence,
we see that the nonzero singular values of X are the square roots of the
nonzero eigenvalues of the symmetric matrix XTX. By using DDT similarly,
we see that they are also the square roots of the nonzero eigenvalues of XXT.
8.6.2 Projection and Smoothing Matrices
It is often of interest to approximate an arbitrary n-vector in a given m-
dimensional vector space, where m < n. An n × n projection matrix of rank
m clearly does this.
8.6.2.1 A Projection Matrix Formed from a Gramian Matrix
An important matrix that arises in analysis of a linear model of the form
y = Xβ + ϵ
(8.49)
is
H = X(XTX)−XT,
(8.50)
where (XTX)−is any generalized inverse. From equation (8.48), H is invariant
to the choice of generalized inverse. By equation (8.47), this matrix can be
obtained from the pseudoinverse and so
H = XX+.
(8.51)
In the full rank case, this is uniquely
H = X(XTX)−1XT.
(8.52)
Whether or not X is of full rank, H is a projection matrix onto span(X).
It is called the “hat matrix” because it projects the observed response vector,
often denoted by y, onto a predicted response vector, often denoted by *y in
span(X):
*y = Hy.
(8.53)
Because H is invariant, this projection is invariant to the choice of generalized
inverse. (In the nonfull rank case, however, we generally refrain from referring
to the vector Hy as the “predicted response”; rather, we may call it the “ﬁtted
response”.)

8.6 Special Matrices Occurring in Data Analysis
363
The rank of H is the same as the rank of X, and its trace is the same as
its rank (because it is idempotent). When X is of full column rank, we have
tr(H) = number of columns of X.
(8.54)
(This can also be seen by using the invariance of the trace to permutations of
the factors in a product as in equation (3.79).)
In linear models, tr(H) is the model degrees of freedom, and the sum of
squares due to the model is just yTHy.
The complementary projection matrix,
I −H,
(8.55)
also has interesting properties that relate to linear regression analysis. In
geometrical terms, this matrix projects a vector onto N(XT), the orthogonal
complement of span(X). We have
y = Hy + (I −H)y
= *y + r,
(8.56)
where r = (I −H)y ∈N(XT). The orthogonal complement is called the
residual vector space, and r is called the residual vector. Both the rank and
the trace of the orthogonal complement are the number of rows in X (that
is, the number of observations) minus the regression degrees of freedom. This
quantity is the “residual degrees of freedom” (unadjusted).
These two projection matrices (8.50) or (8.52) and (8.55) partition the
total sum of squares:
yTy = yTHy + yT(I −H)y.
(8.57)
This partitioning yields the total sum of squares into a sum of squares due
to the ﬁtted relationship between y and X and a “residual” sum of squares.
The analysis of these two sums of squares is one of the most fundamental and
important techniques in statistics. Note that the second term in this parti-
tioning is the Schur complement of XTX in [X y]T [X y] (see equation (3.191)
on page 122).
8.6.2.2 Smoothing Matrices
The hat matrix, either from a full rank X as in equation (8.52) or formed
by a generalized inverse as in equation (8.50), smoothes the vector y onto
the hyperplane deﬁned by the column space of X. It is therefore a smoothing
matrix. (Note that the rank of the column space of X is the same as the rank
of XTX.)
A useful variation of the cross products matrix XTX is the matrix formed
by adding a nonnegative (positive) deﬁnite matrix A to it. Because XTX is

364
8 Matrices with Special Properties
nonnegative (positive) deﬁnite, XTX + A is nonnegative deﬁnite, as we have
seen (page 349), and hence XTX + A is a Gramian matrix.
Because the square root of the nonnegative deﬁnite A exists, we can express
the sum of the matrices as
XTX + A =
! X
A
1
2
"T ! X
A
1
2
"
.
(8.58)
In a common application, a positive deﬁnite matrix λI, with λ > 0, is
added to XTX, and this new matrix is used as a smoothing matrix. The
analogue of the hat matrix (8.52) is
Hλ = X(XTX + λI)−1XT,
(8.59)
and the analogue of the ﬁtted response is
*yλ = Hλy.
(8.60)
This has the eﬀect of shrinking the *y of equation (8.53) toward 0. (In regression
analysis, this is called “ridge regression”; see page 364 or 431.)
Any matrix such as Hλ that is used to transform the observed vector y
onto a given subspace is called a smoothing matrix.
8.6.2.3 Eﬀective Degrees of Freedom
Because of the shrinkage in ridge regression (that is, because the ﬁtted model is
less dependent just on the data in X) we say the “eﬀective” degrees of freedom
of a ridge regression model decreases with increasing λ. We can formally deﬁne
the eﬀective model degrees of freedom of any linear ﬁt *y = Hλy as
tr(Hλ),
(8.61)
analogous to the model degrees of freedom in linear regression above. This
deﬁnition of eﬀective degrees of freedom applies generally in data smoothing.
In fact, many smoothing matrices used in applications depend on a single
smoothing parameter such as the λ in ridge regression, and so the same no-
tation Hλ is often used for a general smoothing matrix.
To evaluate the eﬀective degrees of freedom in the ridge regression model
for a given λ and X, for example, using the singular value decomposition of
X, X = UDV T, we have
tr(X(XTX + λI)−1XT)
= tr

UDV T(V D2V T + λV V T)−1V DU T
= tr

UDV T(V (D2 + λI)V T)−1V DU T
= tr

UD(D2 + λI)−1DU T
= tr

D2(D2 + λI)−1
=

d2
i
d2
i + λ,
(8.62)

8.6 Special Matrices Occurring in Data Analysis
365
where the di are the singular values of X.
When λ = 0, this is the same as the ordinary model degrees of freedom,
and when λ is positive, this quantity is smaller, as we would want it to be by
the argument above. The d2
i /(d2
i + λ) are called shrinkage factors.
If XTX is not of full rank, the addition of λI to it also has the eﬀect
of yielding a full rank matrix, if λ > 0, and so the inverse of XTX + λI
exists even when that of XTX does not. In any event, the addition of λI to
XTX yields a matrix with a better condition number, which we discussed in
Sect. 6.1. (On page 272, we showed that the condition number of XTX + λI
is better than that of XTX.)
8.6.2.4 Residuals from Smoothed Data
Just as in equation (8.56), we can write
y = *yλ + rλ.
(8.63)
Notice, however, that in *yλ = Hλy, Hλ is not in general a projection matrix.
Unless Hλ is a projection matrix, *yλ and rλ are not orthogonal in general
as are *y and r, and we do not have the additive partitioning of the sum of
squares as in equation (8.57).
The rank of Hλ is the same as the number of columns of X, but the trace,
and hence the model degrees of freedom, is less than this number.
8.6.3 Centered Matrices and Variance-Covariance Matrices
In Sect. 2.3, we deﬁned the variance of a vector and the covariance of two vec-
tors. These are the same as the “sample variance” and “sample covariance” in
statistical data analysis and are related to the variance and covariance of ran-
dom variables in probability theory. We now consider the variance-covariance
matrix associated with a data matrix. We occasionally refer to the variance-
covariance matrix simply as the “variance matrix” or just as the “variance”.
First, we consider centering and scaling data matrices.
8.6.3.1 Centering and Scaling of Data Matrices
When the elements in a vector represent similar measurements or observa-
tional data on a given phenomenon, summing or averaging the elements in the
vector may yield meaningful statistics. In statistical applications, the columns
in a matrix often represent measurements on the same feature or on the same
variable over diﬀerent observational units as in Fig. 8.1, and so the mean of a
column may be of interest.
We may center the column by subtracting its mean from each element in
the same manner as we centered vectors on page 48. The matrix formed by
centering all of the columns of a given matrix is called a centered matrix,

366
8 Matrices with Special Properties
and if the original matrix is X, we represent the centered matrix as Xc in a
notation analogous to what we introduced for centered vectors. If we represent
the matrix whose ith column is the constant mean of the ith column of X as X,
Xc = X −X.
(8.64)
Here is an R statement to compute this:
Xc <- X-rep(1,n)%*%t(apply(X,2,mean))
If the unit of a measurement is changed, all elements in a column of the
data matrix in which the measurement is used will change. The amount of
variation of elements within a column or the relative variation among dif-
ferent columns ideally should not be measured in terms of the basic units
of measurement, which can diﬀer irreconcilably from one column to another.
(One column could represent scores on an exam and another column could
represent weight, for example.)
In analyzing data, it is usually important to scale the variables so that
their variations are comparable. We do this by using the standard deviation
of the column. If we have also centered the columns, the column vectors are
the centered and scaled vectors of the form of those in equation (2.74),
xcs = xc
sx
,
where sx is the standard deviation of x,
sx =
∥xc∥
√n −1.
If all columns of the data matrix X are centered and scaled, we denote the
resulting matrix as Xcs. If si represents the standard deviation of the ith
column, this matrix is formed as
Xcs = Xcdiag(1/si).
(8.65)
Here is an R statement to compute this:
Xcs <- Xc%*%diag(1/apply(X,2,sd))
If the rows of X are taken as representative of a population of similar vectors,
it is often useful to center and scale any vector from that population in the
manner of equation (8.65):
˜x = diag(1/si)xc.
(8.66)
(Note that xc is a vector of the same order as a row of Xc.)

8.6 Special Matrices Occurring in Data Analysis
367
8.6.3.2 Gramian Matrices Formed from Centered Matrices:
Covariance Matrices
An important Gramian matrix is formed as the sums of squares and cross
products matrix from a centered matrix and scaled by (n −1), where n is the
number of rows of the original matrix:
SX =
1
n −1XT
c Xc
= (sij).
(8.67)
This matrix is called the variance-covariance matrix associated with the given
matrix X, or the sample variance-covariance matrix, to distinguish it from
the variance-covariance matrix of the distribution given in equation (4.71)
on page 218. We denote it by SX or just S. If x∗i and x∗j are the vectors
corresponding to the ith and jth columns of X, then sij = Cov(x∗i, x∗j); that
is, the oﬀ-diagonal elements are the covariances between the column vectors,
as in equation (2.78), and the diagonal elements are variances of the column
vectors.
This matrix and others formed from it, such as RX in equation (8.69)
below, are called association matrices because they are based on measures
of association (covariance or correlation) among the columns of X. We could
likewise deﬁne a Gramian association matrix based on measures of association
among the rows of X.
A transformation using the Cholesky factor of SX or the square root of
SX (assuming SX is full rank) results in a matrix whose associated variance-
covariance is the identity. We call this a sphered matrix:
Xsphered = XcS
−1
2
X .
(8.68)
The matrix SX is a measure of the anisometry of the space of vectors
represented by the rows of X as mentioned in Sect. 3.2.9. The inverse, S−1
X ,
in some sense evens out the anisometry. Properties of vectors in the space
represented by the rows of X are best assessed following a transformation as in
equation (8.66). For example, rather than orthogonality of two vectors u and v,
a more interesting relationship would be S−1
X -conjugacy (see equation (3.93)):
uTS−1
X v = 0.
Also, the Mahalanobis distance,
&
(u −v)TS−1
X (u −v), may be more relevant
for measuring the diﬀerence in two vectors than the standard Euclidean dis-
tance.

368
8 Matrices with Special Properties
8.6.3.3 Gramian Matrices Formed from Scaled Centered Matrices:
Correlation Matrices
If the columns of a centered matrix are standardized (that is, divided by their
standard deviations, assuming that each is nonconstant, so that the stan-
dard deviation is positive), the scaled cross products matrix is the correlation
matrix, often denoted by RX or just R,
RX =
1
n −1XT
csXcs
= (rij),
(8.69)
where if x∗i and x∗j are the vectors corresponding to the ith and jth columns of
X, rij = Cor(x∗i, x∗j). The correlation matrix can also be expressed as RX =
XT
c DXc, where D is the diagonal matrix whose kth diagonal is 1/

V(x∗k),
where V(x∗k) is the sample variance of the kth column; that is, V(x∗k) =

i(xik −¯x∗k)2/(n −1). This Gramian matrix RX is based on measures of
association among the columns of X.
The elements along the diagonal of the correlation matrix are all 1, and the
oﬀ-diagonals are between −1 and 1, each being the correlation between a pair
of column vectors, as in equation (2.80). The correlation matrix is nonnegative
deﬁnite because it is a Gramian matrix.
The trace of an n×n correlation matrix is n, and therefore the eigenvalues,
which are all nonnegative, sum to n.
Without reference to a data matrix, any nonnegative deﬁnite matrix with
1s on the diagonal and with all elements less than or equal to 1 in absolute
value is called a correlation matrix.
8.6.4 The Generalized Variance
The diagonal elements of the variance-covariance matrix S associated with
the n × m data matrix X are the second moments of the centered columns
of X, and the oﬀ-diagonal elements are pairwise second central moments of
the columns. Each element of the matrix provides some information about
the spread of a single column or of two columns of X. The determinant of
S provides a single overall measure of the spread of the columns of X. This
measure, |S|, is called the generalized variance, or generalized sample variance,
to distinguish it from an analogous measure of a distributional model.
On page 74, we discussed the equivalence of a determinant and the vol-
ume of a parallelotope. The generalized variance captures this, and when the
columns or rows of S are more orthogonal to each other, the volume of the
parallelotope determined by the columns or rows of S is greater, as shown in
Fig. 8.6 for m = 3.

8.6 Special Matrices Occurring in Data Analysis
369
s1
s2
s3
e1
e2
e3
s1
s2
s3
e1
e2
e3
Figure 8.6. Generalized variances in terms of the columns of S
The columns or rows of S are generally not of much interest in themselves.
Our interest is in the relationship of the centered columns of the n × m data
matrix X. Let us consider the case of m = 2. Let z∗1 and z∗2 represent the
centered column vectors of X; that is, for z∗1, we have z∗1i = x∗1i −¯x1. Now,
as in equation (3.42), consider the parallelogram formed by z∗1 and z∗2. For
computing the area, consider z∗1 as forming the base. The length of the base is
∥z∗1∥=

(n −1)s11,
and the height is
∥z∗2∥| sin(θ)| =
&
(n −1)s22(1 −r2
12).
(Recall the relationship between the angle and the correlation from equa-
tion (2.81).)
The area of the parallelogram therefore is
area = (n −1)
&
s11s22(1 −r2
12).
Now, consider S:
S =
!s11 s21
s12 s22
"
=
!
s11
√s11s22r12
√s11s22r12
s22
"
.
The determinant of S is therefore
s11s22(1 −r2
12),

370
8 Matrices with Special Properties
that is,
|S| =
1
(n −1)m volume2.
(8.70)
Although we considered only the case m = 2, equation (8.70) holds gen-
erally, as can be seen by induction on m (see Anderson, 2003).
8.6.4.1 Comparing Variance-Covariance Matrices
Many standard statistical procedures for comparing groups of data rely on the
assumption that the population variance-covariance matrices of the groups are
all the same. (The simplest example of this is the two-sample t-test, in which
the concern is just that the population variances of the two groups be equal.)
Occasionally, the data analyst wishes to test this assumption of homogeneity
of variances.
On page 175, we considered the problem of comparing two matrices of
the same size. There we deﬁned a metric based on a matrix norm. For the
problem of comparing variance-covariance matrices, a measure based on the
generalized variances is more commonly used.
In the typical situation, we have an n×m data matrix X in which the ﬁrst
n1 rows represent observations from one group, the next n2 rows represent
observations from another group, and the last ng rows represent observations
from the gth group. For each group, we form a sample variance-covariance ma-
trix Si as in equation (8.67) using the ith submatrix of X. Whenever we have
individual variance-covariance matrices in situations similar to this, we deﬁne
the pooled variance-covariance matrix:
Sp =
1
(n −g)
g

i=1
(ni −1)Si.
(8.71)
Bartlett suggested a test based on the determinants of (ni −1)Si. (From
equation (3.36), |(ni −1)Si| = (ni −1)m|Si|.) A similar test suggested by
Box also uses the generalized variances. One form of the Box M statistic for
testing for homogeneity of variances is
M = (n −g) log(|Sp|) −
g

i=1
(ni −1)Si.
(8.72)
8.6.5 Similarity Matrices
Covariance and correlation matrices are examples of similarity association ma-
trices: they measure the similarity or closeness of the columns of the matrices
from which they are formed.
The cosine of the angle between two vectors is related to the correlation
between the vectors, so a matrix of the cosine of the angle between the columns

8.6 Special Matrices Occurring in Data Analysis
371
of a given matrix would also be a similarity matrix. (The angle is exactly the
same as the correlation if the vectors are centered; see equation (2.80).)
Similarity matrices can be formed in many ways, and some are more useful
in particular applications than in others. They may not even arise from a
standard dataset in the familiar form in statistical applications. For example,
we may be interested in comparing text documents. We might form a vector-
like object whose elements are the words in the document. The similarity
between two such ordered tuples, generally of diﬀerent lengths, may be a
count of the number of words, word pairs, or more general phrases in common
between the two documents.
It is generally reasonable that similarity between two objects be symmet-
ric; that is, the ﬁrst object is as close to the second as the second is to the ﬁrst.
We reserve the term similarity matrix for matrices formed from such measures
and, hence, that themselves are symmetric. Occasionally, for example in psy-
chometric applications, the similarities are measured relative to rank order
closeness within a set. In such a case, the measure of closeness may not be
symmetric. A matrix formed from such measures is called a directed similarity
matrix.
8.6.6 Dissimilarity Matrices
The elements of similarity generally increase with increasing “closeness”. We
may also be interested in the dissimilarity. Clearly, any decreasing function of
similarity could be taken as a reasonable measure of dissimilarity. There are,
however, other measures of dissimilarity that often seem more appropriate.
In particular, the properties of a metric (see page 32) may suggest that a
dissimilarity be deﬁned in terms of a metric.
In considering either similarity or dissimilarity for a data matrix, we could
work with either rows or columns, but the common applications make one
or the other more natural for the speciﬁc application. Because of the types
of the common applications, we will discuss dissimilarities and distances in
terms of rows instead of columns; thus, in this section, we will consider the
dissimilarity of xi∗and xj∗, the vectors corresponding to the ith and jth rows
of X.
Dissimilarity matrices are also association matrices in the general sense of
Sect. 8.1.5.
Dissimilarity or distance can be measured in various ways. A metric is
the most obvious measure, although in certain applications other measures
are appropriate. The measures may be based on some kind of ranking, for
example. If the dissimilarity is based on a metric, the association matrix is
often called a distance matrix. In most applications, the Euclidean distance,
∥xi∗−xj∗∥2, is the most commonly used metric, but others, especially ones
based on L1 or L∞norms, are often useful.

372
8 Matrices with Special Properties
Any hollow matrix with nonnegative elements is a directed dissimilarity
matrix. A directed dissimilarity matrix is also called a cost matrix. If the
matrix is symmetric, it is a dissimilarity matrix.
An n × n matrix D = (dij) is an m-dimensional distance matrix if there
exists m-vectors x1 . . . , xn such that, for some metric Δ, dij = Δ(xi, xj).
A distance matrix is necessarily a dissimilarity matrix. If the metric is the
Euclidean distance, the matrix D is called a Euclidean distance matrix.
The matrix whose rows are the vectors xT
1 , . . . , xT
n is called an associated
conﬁguration matrix of the given distance matrix. In metric multidimensional
scaling, we are given a dissimilarity matrix and seek to ﬁnd a conﬁguration
matrix whose associated distance matrix is closest to the dissimilarity matrix,
usually in terms of the Frobenius norm of the diﬀerence of the matrices (see
Trosset 2002; for basic deﬁnitions and extensions).
8.7 Nonnegative and Positive Matrices
A nonnegative matrix, as the name suggests, is a real matrix all of whose
elements are nonnegative, and a positive matrix is a real matrix all of whose
elements are positive. In some other literature, the latter type of matrix is
called strictly positive, and a nonnegative matrix with a positive element is
called positive.
Many useful matrices are nonnegative, and we have already considered
various kinds of nonnegative matrices. The adjacency or connectivity matrix of
a graph is nonnegative. Dissimilarity matrices, including distance matrices, are
nonnegative. Matrices used in modeling stochastic processes are nonnegative.
While many of these matrices are square, we do not restrict the deﬁnitions to
square matrices.
If A is nonnegative, we write
A ≥0,
(8.73)
and if it is positive, we write
A > 0.
(8.74)
Notice that A ≥0 and A ̸= 0 together do not imply A > 0.
We write
A ≥B
to mean (A −B) ≥0 and
A > B
to mean (A−B) > 0. (Recall the deﬁnitions of nonnegative deﬁnite and posi-
tive deﬁnite matrices, and, from equations (8.11) and (8.16), the notation used
to indicate those properties, A ⪰0 and A ≻0. Furthermore, notice that these
deﬁnitions and this notation for nonnegative and positive matrices are con-
sistent with analogous deﬁnitions and notation involving vectors on page 16.

8.7 Nonnegative and Positive Matrices
373
Some authors, however, use the notation of equations (8.73) and (8.74) to
mean “nonnegative deﬁnite” and “positive deﬁnite”. We should also note that
some authors use somewhat diﬀerent terms for these and related properties.
“Positive” for these authors means nonnegative with at least one positive
element, and “strictly positive” means positive as we have deﬁned it.)
Notice that positiveness (nonnegativeness) has nothing to do with posi-
tive (nonnegative) deﬁniteness. A positive or nonnegative matrix need not be
symmetric or even square, although most such matrices useful in applications
are square. A square positive matrix, unlike a positive deﬁnite matrix, need
not be of full rank.
The following properties are easily veriﬁed.
1. If A ≥0 and u ≥v ≥0, then Au ≥Av.
2. If A ≥0, A ̸= 0, and u > v > 0, then Au > Av.
3. If A > 0 and v ≥0, then Av ≥0.
4. If A > 0 and A is square, then ρ(A) > 0.
Whereas most of the important matrices arising in the analysis of linear
models are symmetric, and thus have the properties listed in Sect. 8.2.1 begin-
ning on page 340, many important nonnegative matrices, such as those used in
studying stochastic processes, are not necessarily symmetric. The eigenvalues
of real symmetric matrices are real, but the eigenvalues of real nonsymmet-
ric matrices may have imaginary components. In the following discussion, we
must be careful to remember the meaning of the spectral radius. The deﬁnition
in equation (3.233) for the spectral radius of the matrix A with eigenvalues ci,
ρ(A) = max |ci|,
is still correct, but the operator “| · |” must be interpreted as the modulus of
a complex number.
8.7.1 The Convex Cones of Nonnegative and Positive Matrices
The class of all n × m nonnegative (positive) matrices is a cone because if
X is a nonnegative (positive) matrix and a > 0, then aX is a nonnegative
(positive) matrix (see page 43). Furthermore, it is a convex cone in IRn×m,
because if X1 and X2 are n × m nonnegative (positive) matrices and a, b ≥0,
then aX1 + bX2 is nonnegative (positive) so long as either a ̸= 0 or b ̸= 0.
Both of these classes are closed under Cayley multiplication, but they may
not have inverses in the class (that is, in particular, they are not groups with
respect to that operation).
8.7.2 Properties of Square Positive Matrices
We have the following important properties for square positive matrices. These
properties collectively are the conclusions of the Perron theorem.

374
8 Matrices with Special Properties
Let A be a square positive matrix and let r = ρ(A). Then:
1. r is an eigenvalue of A. The eigenvalue r is called the Perron root. Note
that the Perron root is real and positive (although other eigenvalues of
A may not be).
2. There is an eigenvector v associated with r such that v > 0.
3. The Perron root is simple. (That is, the algebraic multiplicity of the
Perron root is 1.)
4. The dimension of the eigenspace of the Perron root is 1. (That is, the
geometric multiplicity of ρ(A) is 1.) Hence, if v is an eigenvector associ-
ated with r, it is unique except for scaling. This associated eigenvector
is called the Perron vector. Note that the Perron vector is real (although
other eigenvectors of A may not be). The elements of the Perron vector
all have the same sign, which we usually take to be positive; that is,
v > 0.
5. If ci is any other eigenvalue of A, then |ci| < r. (That is, r is the only
eigenvalue on the spectral circle of A.)
We will give proofs only of properties 1 and 2 as examples. Proofs of all
of these facts are available in Bapat and Raghavan (1997).
To see properties 1 and 2, ﬁrst observe that a positive matrix must have
at least one nonzero eigenvalue because the coeﬃcients and the constant in
the characteristic equation must all be positive. Now scale the matrix so that
its spectral radius is 1 (see page 142). So without loss of generality, let A be
a scaled positive matrix with ρ(A) = 1. Now let (c, x) be some eigenpair of A
such that |c| = 1. First, we want to show, for some such c, that c = ρ(A).
Because all elements of A are positive,
|x| = |Ax| ≤A|x|,
and so
A|x| −|x| ≥0.
(8.75)
An eigenvector must be nonzero, so we also have
A|x| > 0.
Now we want to show that A|x| −|x| = 0. To that end, suppose the con-
trary; that is, suppose A|x| −|x| ̸= 0. In that case, A(A|x| −|x|) > 0 from
equation (8.75), and so there must be a positive number ϵ such that
A
1 + ϵA|x| > A|x|
or
By > y,
where B = A/(1 + ϵ) and y = A|x|. Now successively multiplying both sides
of this inequality by the positive matrix B, we have

8.7 Nonnegative and Positive Matrices
375
Bky > y
for all k = 1, 2, . . ..
Because ρ(B) = ρ(A)/(1+ϵ) < 1, from equation (3.314) on page 173, we have
limk→∞Bk = 0; that is, limk→∞Bky = 0 > y. This contradicts the fact that
y > 0. Because the supposition A|x| −|x| ̸= 0 led to this contradiction, we
must have A|x| −|x| = 0. Therefore 1 = ρ(A) must be an eigenvalue of A,
and |x| must be an associated eigenvector; hence, with v = |x|, (ρ(A), v) is
an eigenpair of A and v > 0, and this is the statement made in properties 1
and 2.
The Perron-Frobenius theorem, which we consider below, extends these
results to a special class of square nonnegative matrices. (This class includes
all positive matrices, so the Perron-Frobenius theorem is an extension of the
Perron theorem.)
8.7.3 Irreducible Square Nonnegative Matrices
Nonnegativity of a matrix is not a very strong property. First of all, note that
it includes the zero matrix; hence, clearly none of the properties of the Perron
theorem can hold. Even a nondegenerate, full rank nonnegative matrix does
not necessarily possess those properties. A small full rank nonnegative matrix
provides a counterexample for properties 2, 3, and 5:
A =
!1 1
0 1
"
.
The eigenvalues are 1 and 1; that is, 1 with an algebraic multiplicity of 2
(so property 3 does not hold). There is only one nonnull eigenvector, (1, −1),
(so property 2 does not hold, but property 4 holds), but the eigenvector is not
positive (or even nonnegative). Of course property 5 cannot hold if property 3
does not hold.
We now consider irreducible square nonnegative matrices. This class in-
cludes positive matrices, and irreducibility yields some of the properties of pos-
itivity to matrices with some zero elements. (Heuristically, irreducibility puts
some restrictions on the nonpositive elements of the matrix.) On page 337, we
deﬁned reducibility of a nonnegative square matrix and we saw that a matrix
is irreducible if and only if its digraph is strongly connected.
To recall the deﬁnition, a nonnegative matrix is said to be reducible if by
symmetric permutations it can be put into a block upper triangular matrix
with square blocks along the diagonal; that is, the nonnegative matrix A is
reducible if and only if there is a permutation matrix E(π) such that
E(π)AE(π) =
! B11 B12
0
B22
"
,
(8.76)
where B11 and B22 are square. A matrix that cannot be put into that form
is irreducible. An alternate term for reducible is decomposable, with the as-
sociated term indecomposable. (There is an alternate meaning for the term

376
8 Matrices with Special Properties
“reducible” applied to a matrix. This alternate use of the term means that
the matrix is capable of being expressed by a similarity transformation as the
sum of two matrices whose columns are mutually orthogonal.)
We see from the deﬁnition in equation (8.76) that a positive matrix is
irreducible.
Irreducible matrices have several interesting properties. An n × n nonneg-
ative matrix A is irreducible if and only if (I + A)n−1 is a positive matrix;
that is,
A is irreducible ⇐⇒(I + A)n−1 > 0.
(8.77)
To see this, ﬁrst assume (I +A)n−1 > 0; thus, (I +A)n−1 clearly is irreducible.
If A is reducible, then there exists a permutation matrix E(π) such that
E(π)AE(π) =
!
B11 B12
0
B22
"
,
and so
E(π)(I + A)n−1E(π) =

E(π)(I + A)E(π)
n−1
=

I + E(π)AE(π)
n−1
=
! In1 + B11
B12
0
In2 + B22
"
.
This decomposition of (I +A)n−1 cannot exist because it is irreducible; hence
we conclude A is irreducible if (I + A)n−1 > 0.
Now, if A is irreducible, we can see that (I + A)n−1 must be a positive
matrix either by a strictly linear-algebraic approach or by couching the argu-
ment in terms of the digraph G(A) formed by the matrix, as in the discussion
on page 337 that showed that a digraph is strongly connected if (and only if)
it is irreducible. We will use the latter approach in the spirit of applications
of irreducibility in stochastic processes.
For either approach, we ﬁrst observe that the (i, j)th element of (I +A)n−1
can be expressed as

(I + A)n−1
ij =
n−1

k=0
n −1
k

Ak

ij
.
(8.78)
Hence, for k = 1, . . . , n −1, we consider the (i, j)th entry of Ak. Let a(k)
ij
represent this quantity.
Given any pair (i, j), for some l1, l2, . . . , lk−1, we have
a(k)
ij =

l1,l2,...,lk−1
a1l1al1l2 · · · alk−1j.
Now a(k)
ij
> 0 if and only if a1l1, al1l2, . . . , alk−1j are all positive; that is, if
there is a path v1, vl1, . . . , vlk−1, vj in G(A). If A is irreducible, then G(A) is

8.7 Nonnegative and Positive Matrices
377
strongly connected, and hence the path exists. So, for any pair (i, j), we have
from equation (8.78)

(I + A)n−1
ij > 0; that is, (I + A)n−1 > 0.
The positivity of (I + A)n−1 for an irreducible nonnegative matrix A is a
very useful property because it allows us to extend some conclusions of the
Perron theorem to irreducible nonnegative matrices.
8.7.3.1 Properties of Square Irreducible Nonnegative Matrices:
The Perron-Frobenius Theorem
If A is a square irreducible nonnegative matrix, then we have the following
properties, which are similar to properties 1 through 4 on page 374 for pos-
itive matrices. These following properties are the conclusions of the Perron-
Frobenius theorem.
1. ρ(A) is an eigenvalue of A. This eigenvalue is called the Perron root, as
before (and, as before, it is real and positive).
2. The Perron root ρ(A) is simple. (That is, the algebraic multiplicity of
the Perron root is 1.)
3. The dimension of the eigenspace of the Perron root is 1. (That is, the
geometric multiplicity of ρ(A) is 1.)
4. The eigenvector associated with ρ(A) is positive. This eigenvector is
called the Perron vector, as before.
The relationship (8.77) allows us to prove properties 1 and 4 in a method
similar to the proofs of properties 1 and 2 for positive matrices. (This is
Exercise 8.9.) Complete proofs of all of these facts are available in Bapat and
Raghavan (1997). See also the solution to Exercise 8.10b on page 611 for a
special case.
The one property of square positive matrices that does not carry over
to square irreducible nonnegative matrices is property 5: r = ρ(A) is the
only eigenvalue on the spectral circle of A. For example, the small irreducible
nonnegative matrix
A =
! 0 1
1 0
"
has eigenvalues 1 and −1, and so both are on the spectral circle.
8.7.3.2 Primitive Matrices
Square irreducible nonnegative matrices that have only one eigenvalue on the
spectral circle, that is, that do satisfy property 5 on page 374, also have other
interesting properties that are important, for example, in Markov chains. We
therefore give a name to matrices with that property:
A square irreducible nonnegative matrix is said to be primitive if it
has only one eigenvalue on the spectral circle.

378
8 Matrices with Special Properties
8.7.3.3 Limiting Behavior of Primitive Matrices
In modeling with Markov chains and other applications, the limiting behavior
of Ak is an important property.
On page 172, we saw that limk→∞Ak = 0 if ρ(A) < 1. For a primitive
matrix, we also have a limit for Ak if ρ(A) = 1. (As we have done above, we
can scale any matrix with a nonzero eigenvalue to a matrix with a spectral
radius of 1.)
If A is a primitive matrix, then we have the useful result
lim
k→∞
 A
ρ(A)
k
= vwT,
(8.79)
where v is an eigenvector of A associated with ρ(A) and w is an eigenvector
of AT associated with ρ(A), and w and v are scaled so that wTv = 1. (As we
mentioned on page 158, such eigenvectors exist because ρ(A) is a simple eigen-
value. They also exist because of property 4; they are both positive. Note that
A is not necessarily symmetric, and so its eigenvectors may include imaginary
components; however, the eigenvectors associated with ρ(A) are real, and so
we can write wT instead of wH.)
To see equation (8.79), we consider

A −ρ(A)vwT
. First, if (ci, vi) is an
eigenpair of

A −ρ(A)vwT
and ci ̸= 0, then (ci, vi) is an eigenpair of A. We
can see this by multiplying both sides of the eigen-equation by vwT:
civwTvi = vwT 
A −ρ(A)vwT
vi
=

vwTA −ρ(A)vwTvwT
vi
=

ρ(A)vwT −ρ(A)vwT
vi
= 0;
hence,
Avi =

A −ρ(A)vwT
vi
= civi.
Next, we show that
ρ

A −ρ(A)vwT
< ρ(A).
(8.80)
If ρ(A) were an eigenvalue of

A −ρ(A)vwT
, then its associated eigenvector,
say w, would also have to be an eigenvector of A, as we saw above. But since
as an eigenvalue of A the geometric multiplicity of ρ(A) is 1, for some scalar
s, w = sv. But this is impossible because that would yield
ρ(A)sv =

A −ρ(A)vwT
sv
= sAv −sρ(A)v
= 0,

8.7 Nonnegative and Positive Matrices
379
and neither ρ(A) nor sv is zero. But as we saw above, any eigenvalue of

A −ρ(A)vwT
is an eigenvalue of A and no eigenvalue of

A −ρ(A)vwT
can be as large as ρ(A) in modulus; therefore we have inequality (8.80).
Finally, we recall equation (3.269), with w and v as deﬁned above, and
with the eigenvalue ρ(A),

A −ρ(A)vwTk = Ak −(ρ(A))kvwT,
(8.81)
for k = 1, 2, . . ..
Dividing both sides of equation (8.81) by (ρ(A))k and rearranging terms,
we have
 A
ρ(A)
k
= vwT +

A −ρ(A)vwT
ρ(A)
.
(8.82)
Now
ρ

A −ρ(A)vwT
ρ(A)

= ρ

A −ρ(A)vwT
ρ(A)
,
which is less than 1; hence, from equation (3.312) on page 172, we have
lim
k→∞

A −ρ(A)vwT
ρ(A)
k
= 0;
so, taking the limit in equation (8.82), we have equation (8.79).
Applications of the Perron-Frobenius theorem are far-ranging. It has im-
plications for the convergence of some iterative algorithms, such as the power
method discussed in Sect. 7.2. The most important applications in statistics
are in the analysis of Markov chains, which we discuss in Sect. 9.8.1.
8.7.4 Stochastic Matrices
A nonnegative matrix P such that
P1 = 1
(8.83)
is called a stochastic matrix. The deﬁnition means that (1, 1) is an eigenpair
of any stochastic matrix. It is also clear that if P is a stochastic matrix,
then ∥P∥∞= 1 (see page 166), and because ρ(P) ≤∥P∥for any norm (see
page 171) and 1 is an eigenvalue of P, we have ρ(P) = 1.
A stochastic matrix may not be positive, and it may be reducible or irre-
ducible. (Hence, (1, 1) may not be the Perron root and Perron eigenvector.)
If P is a stochastic matrix such that
1TP = 1T,
(8.84)

380
8 Matrices with Special Properties
it is called a doubly stochastic matrix. If P is a doubly stochastic matrix,
∥P∥1 = 1, and, of course, ∥P∥∞= 1 and ρ(P) = 1.
A permutation matrix is a doubly stochastic matrix; in fact, it is the sim-
plest and one of the most commonly encountered doubly stochastic matrices.
A permutation matrix is clearly reducible.
Stochastic matrices are particularly interesting because of their use in
deﬁning a discrete homogeneous Markov chain. In that application, a stochas-
tic matrix and distribution vectors play key roles. A distribution vector is a
nonnegative matrix whose elements sum to 1; that is, a vector v such that
1Tv = 1. In Markov chain models, the stochastic matrix is a probability tran-
sition matrix from a distribution at time t, πt, to the distribution at time
t + 1,
πt+1 = Pπt.
In Sect. 9.8.1, we deﬁne some basic properties of Markov chains. Those prop-
erties depend in large measure on whether the transition matrix is reducible
or not.
8.7.5 Leslie Matrices
Another type of nonnegative transition matrix, often used in population stud-
ies, is a Leslie matrix, after P. H. Leslie, who used it in models in demography.
A Leslie matrix is a matrix of the form
⎡
⎢⎢⎢⎢⎢⎣
α1 α2 · · · αm−1 αm
σ1 0 · · ·
0
0
0 σ2 · · ·
0
0
...
...
...
...
...
0
0 · · · σm−1
0
⎤
⎥⎥⎥⎥⎥⎦
,
(8.85)
where all elements are nonnegative, and additionally σi ≤1.
A Leslie matrix is clearly reducible. Furthermore, a Leslie matrix has a
single unique positive eigenvalue (see Exercise 8.10), which leads to some
interesting properties (see Sect. 9.8.2).
8.8 Other Matrices with Special Structures
Matrices of a variety of special forms arise in statistical analyses and other
applications. For some matrices with special structure, specialized algorithms
can increase the speed of performing a given task considerably. Many tasks
involving matrices require a number of computations of the order of n3, where
n is the number of rows or columns of the matrix. For some of the matrices
discussed in this section, because of their special structure, the order of com-
putations may be n2. The improvement from O(n3) to O(n2) is enough to

8.8 Other Matrices with Special Structures
381
make some tasks feasible that would otherwise be infeasible because of the
time required to complete them. The collection of papers in Olshevsky (2003)
describe various specialized algorithms for the kinds of matrices discussed in
this section.
8.8.1 Helmert Matrices
A Helmert matrix is a square orthogonal matrix that partitions sums of
squares. Its main use in statistics is in deﬁning contrasts in general linear
models to compare the second level of a factor with the ﬁrst level, the third
level with the average of the ﬁrst two, and so on. (There is another meaning
of “Helmert matrix” that arises from so-called Helmert transformations used
in geodesy.)
For example, a partition of the sum n
i=1 y2
i into orthogonal sums each
involving ¯y2
k and k
i=1(yi −¯yk)2 is
˜yi = (i(i + 1))−1/2
⎛
⎝
i+1

j=1
yj −(i + 1)yi+1
⎞
⎠
for i = 1, . . . n −1,
˜yn = n−1/2
n

j=1
yj.
(8.86)
These expressions lead to a computationally stable one-pass algorithm for
computing the sample variance (see equation (10.8) on page 504).
The Helmert matrix that corresponds to this partitioning has the form
Hn =
⎡
⎢⎢⎢⎢⎢⎢⎣
1/√n
1/√n
1/√n
· · ·
1/√n
1/
√
2
−1/
√
2
0
· · ·
0
1/
√
6
1/
√
6
−2/
√
6 · · ·
0
...
...
...
...
...
1
√
n(n−1)
1
√
n(n−1)
1
√
n(n−1) · · · −
(n−1)
√
n(n−1)
⎤
⎥⎥⎥⎥⎥⎥⎦
=
!
1/√n 1T
n
Kn−1
"
,
(8.87)
where Kn−1 is the (n−1)×n matrix below the ﬁrst row. For the full n-vector
y, we have
yTKT
n−1Kn−1y =

(yi −¯y)2
=

(yi −¯y)2
= (n −1)s2
y.

382
8 Matrices with Special Properties
The rows of the matrix in equation (8.87) correspond to orthogonal con-
trasts in the analysis of linear models (see Sect. 9.3.2).
Obviously, the sums of squares are never computed by forming the Helmert
matrix explicitly and then computing the quadratic form, but the computa-
tions in partitioned Helmert matrices are performed indirectly in analysis of
variance, and representation of the computations in terms of the matrix is
often useful in the analysis of the computations.
8.8.2 Vandermonde Matrices
A Vandermonde matrix is an n × m matrix with columns that are deﬁned by
monomials,
Vn×m =
⎡
⎢⎢⎢⎣
1 x1 x2
1 · · · xm−1
1
1 x2 x2
2 · · · xm−1
2
...
...
...
...
...
1 xn x2
n · · · xm−1
n
⎤
⎥⎥⎥⎦,
(8.88)
where xi ̸= xj if i ̸= j. The Vandermonde matrix arises in polynomial regres-
sion analysis. For the model equation yi = β0 + β1xi + · · · + βpxp
i + ϵi, given
observations on y and x, a Vandermonde matrix is the matrix in the standard
representation y = Xβ + ϵ.
Because of the relationships among the columns of a Vandermonde matrix,
computations for polynomial regression analysis can be subject to numerical
errors, and so sometimes we make transformations based on orthogonal poly-
nomials. The condition number (see Sect. 6.1, page 266) for a Vandermonde
matrix is large. A Vandermonde matrix, however, can be used to form simple
orthogonal vectors that correspond to orthogonal polynomials. For example,
if the xs are chosen over a grid on [−1, 1], a QR factorization (see Sect. 5.8 on
page 248) yields orthogonal vectors that correspond to Legendre polynomials.
These vectors are called discrete Legendre polynomials. Although not used
in regression analysis so often now, orthogonal vectors are useful in selecting
settings in designed experiments.
Vandermonde matrices also arise in the representation or approximation
of a probability distribution in terms of its moments.
The determinant of a square Vandermonde matrix has a particularly sim-
ple form (see Exercise 8.11).
8.8.3 Hadamard Matrices and Orthogonal Arrays
In a wide range of applications, including experimental design, cryptology, and
other areas of combinatorics, we often encounter matrices whose elements are
chosen from a set of only a few diﬀerent elements. In experimental design,
the elements may correspond to the levels of the factors; in cryptology, they
may represent the letters of an alphabet. In two-level factorial designs, the
entries may be either 0 or 1. Matrices all of whose entries are either 1 or

8.8 Other Matrices with Special Structures
383
−1 can represent the same layouts, and such matrices may have interesting
mathematical properties.
An n × n matrix with −1, 1 entries whose determinant is nn/2 is called a
Hadamard matrix. Hadamard’s name is associated with this matrix because
of the bound derived by Hadamard for the determinant of any matrix A
with |aij| ≤1 for all i, j: |det(A)| ≤nn/2. A Hadamard matrix achieves this
upper bound. A maximal determinant is often used as a criterion for a good
experimental design.
We often denote an n × n Hadamard matrix by Hn, which is the same no-
tation often used for a Helmert matrix, but in the case of Hadamard matrices,
the matrix is not unique. All rows are orthogonal and so are all columns. The
norm of each row or column is n, so
HT
n Hn = HnHT
n = nI.
(8.89)
It is clear that if Hn is a Hadamard matrix then so is HT
n , and a Hadamard
matrix is a normal matrix (page 345). Symmetric Hadamard matrices are often
of special interest. In a special type of n × n Hadamard matrix, one row and
one column consist of all 1s; all n −1 other rows and columns consist of n/2
1s and n/2 −1s. Such a matrix is called a normalized Hadamard matrix. Most
Hadamard matrices occurring in statistical applications are normalized, and
also most have all 1s on the diagonal.
A Hadamard matrix is often represented as a mosaic of black and white
squares, as in Fig. 8.7.
1 1 1 1
1 -1 1 -1
1 1 -1 -1
1 -1 -1 1
Figure 8.7. A 4 × 4 Hadamard matrix
Hadamard matrices do not exist for all n. Clearly, n must be even because
|Hn| = nn/2, but some experimentation (or an exhaustive search) quickly
shows that there is no Hadamard matrix for n = 6. It has been conjectured,
but not proven, that Hadamard matrices exist for any n divisible by 4. Given
any n × n Hadamard matrix, Hn, and any m × m Hadamard matrix, Hm, an
nm × nm Hadamard matrix can be formed as a partitioned matrix in which
each 1 in Hn is replaced by the block submatrix Hm and each −1 is replaced
by the block submatrix −Hm. For example, the 4×4 Hadamard matrix shown
in Fig. 8.7 is formed using the 2 × 2 Hadamard matrix
!
1 −1
1
1
"

384
8 Matrices with Special Properties
as both Hn and Hm. Since an H2 exists, this means that for any n = 2k,
an n × n Hadamard matrix exists. Not all Hadamard matrices can be formed
from other Hadamard matrices in this way, however; that is, they are not
necessarily 2k × 2k. A Hadamard matrix exists for n = 12, for example.
A related type of orthogonal matrix, called a conference matrix, is a hollow
matrix Cn with −1, 1 entries on the oﬀ-diagonals, and such that
CT
n Cn = (n −1)I.
(8.90)
A conference matrix is said to be normalized if the ﬁrst row and the ﬁrst
column consist of all 1s, except for the (1, 1) element. Conference matrices
arise in circuit design and other applications of graph theory. They are related
to the adjacency matrices occurring in such applications. The (n−1)×(n−1)
matrix formed by removing the ﬁrst row and ﬁrst column of a symmetric
conference matrix is a Seidel adjacency matrix (page 334).
A somewhat more general type of matrix corresponds to an n × m array
with the elements in the jth column being members of a set of kj elements
and such that, for some ﬁxed p ≤m, in every n × p submatrix all possible
combinations of the elements of the m sets occur equally often as a row.
(I make a distinction between the matrix and the array because often in
applications the elements in the array are treated merely as symbols without
the assumptions of an algebra of a ﬁeld. A terminology for orthogonal arrays
has evolved that is diﬀerent from the terminology for matrices; for example, a
symmetric orthogonal array is one in which k1 = · · · = km. On the other hand,
treating the orthogonal arrays as matrices with real elements may provide
solutions to combinatorial problems such as may arise in optimal design.)
The 4 × 4 Hadamard matrix shown in Fig. 8.7 is a symmetric orthogonal
array with k1 = · · · = k4 = 2 and p = 4, so in the array each of the possible
combinations of elements occurs exactly once. This array is a member of a
simple class of symmetric orthogonal arrays that has the property that in any
two rows each ordered pair of elements occurs exactly once.
Orthogonal arrays are particularly useful in developing fractional factorial
plans. (The robust designs of Taguchi correspond to orthogonal arrays.) Dey
and Mukerjee (1999) discuss orthogonal arrays with an emphasis on the ap-
plications in experimental design, and Hedayat et al. (1999) provide extensive
an discussion of the properties of orthogonal arrays.
8.8.4 Toeplitz Matrices
A Toeplitz matrix is a square matrix with constant codiagonals:
⎡
⎢⎢⎢⎢⎢⎢⎣
d
u1
u2 · · · un−1
l1
d
u1 · · · un−2
...
...
...
...
...
ln−2 ln−3 ln−4
...
u1
ln−1 ln−2 ln−3 · · ·
d
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(8.91)

8.8 Other Matrices with Special Structures
385
An n × n Toeplitz matrix is characterized by a diagonal element d and two
(n −1)-vectors, u and l, as indicated in the expression above. If u = 0, the
Toeplitz matrix is lower triangular; if l = 0, the matrix is upper triangular; and
if u = l, the matrix is symmetric. A Toeplitz matrix is a banded matrix, but
it may or may not be a “band matrix”, that is, one with many 0 codiagonals,
which we discuss in Chaps. 3 and 12.
Banded Toeplitz matrices arise frequently in time series studies. The
covariance matrix in an ARMA(p, q) process, for example, is a symmetric
Toeplitz matrix with 2 max(p, q) nonzero oﬀ-diagonal bands. See page 451 for
an example and further discussion.
8.8.4.1 Inverses of Certain Toeplitz Matrices and Other Banded
Matrices
A Toeplitz matrix that occurs often in stationary time series is the n × n
variance-covariance matrix of the form
V = σ2
⎡
⎢⎢⎢⎣
1
ρ
ρ2
· · · ρn−1
ρ
1
ρ
· · · ρn−2
...
...
...
...
...
ρn−1 ρn−2 ρn−3 · · ·
1
⎤
⎥⎥⎥⎦.
It is easy to see that V −1 exists if σ ̸= 0 and |ρ| < 1, and that it is the type
2 matrix
V −1 =
1
(1 −ρ2)σ2
⎡
⎢⎢⎢⎢⎢⎣
1
−ρ
0
· · · 0
−ρ 1 + ρ2
−ρ
· · · 0
0
−ρ
1 + ρ2 · · · 0
...
...
...
...
...
0
0
0
· · · 1
⎤
⎥⎥⎥⎥⎥⎦
.
(8.92)
Type 2 matrices also occur as the inverses of other matrices with special
patterns that arise in other common statistical applications (see Graybill 1983;
for examples).
The inverses of all banded invertible matrices have some oﬀ-diagonal sub-
matrices that are zero or have low rank, depending on the bandwidth of the
original matrix (see Strang and Nguyen 2004, for further discussion and ex-
amples).

386
8 Matrices with Special Properties
8.8.5 Circulant Matrices
A Toeplitz matrix having the form
⎡
⎢⎢⎢⎢⎢⎣
c1 c2 c3 · · · cn−1
cn
cn c1 c2 · · · cn−2 cn−1
...
...
... ...
...
...
c3 c4 c5 · · ·
c1
c2
c2 c3 c4 · · ·
cn
c1
⎤
⎥⎥⎥⎥⎥⎦
(8.93)
is called a circulant matrix. Beginning with a ﬁxed ﬁrst row, each subsequent
row is obtained by a right circular shift of the row just above it.
A useful n × n circulant matrix is the permutation matrix
E(n,1,2,...,n−1) =
n
$
k=2
Ek,k−1,
(8.94)
which is the identity transformed by moving each row downward into the row
below it and the last row into the ﬁrst row. (See page 82.) Let us denote this
permutation as πc; hence, we denote the elementary circulant matrix in (8.94)
as E(πc).
By ﬁrst recalling that, for any permutation matrix, E−1
(π) = ET
(π), and then
considering the eﬀects of the multiplications AE(π) and E(π)A, it is easy to
see that A is circulant if and only if
A = E(πc)AET
(πc).
(8.95)
Circulant matrices have several straightforward properties. If A is circu-
lant, then
•
AT is circulant
•
A2 is circulant
•
if A is nonsingular, then A−1 is circulant
You are asked to prove these simple results in Exercise 8.13.
Any linear combination of two circulant matrices of the same order is
circulant (that is, they form a vector space, see Exercise 8.14).
If A and B are circulant matrices of the same order, then AB is circulant
(Exercise 8.15).
Another important property of a circulant matrix is that it is normal, as
we can see by writing the (i, j) element of AAT and of ATA as a sum of
products of elements of A (Exercise 8.16). This has an important implication:
a circulant matrix is unitarily diagonalizable.

8.8 Other Matrices with Special Structures
387
8.8.6 Fourier Matrices and the Discrete Fourier Transform
A special Vandermonde matrix (equation (8.88)) is an n × n matrix whose
entries are the nth roots of unity, that is {1, ω, ω2, . . . , ωn−1}, where
ω = e2πi/n = cos
2π
n

+ i sin
2π
n

.
(8.96)
The matrix is called a Fourier matrix. The (j, k) entry of a Fourier matrix is
ω(j−1)(k−1)/√n:
Fn =
1
√n
⎡
⎢⎢⎢⎢⎢⎣
1
1
1
1
· · ·
1
1
ω1
ω2
ω3
· · ·
ω(n−1)
1
ω2
ω4
ω6
· · ·
ω2(n−1)
...
...
...
...
...
...
1 ωn−1 ω2(n−1) ω3(n−1) · · · ω(n−1)(n−1)
⎤
⎥⎥⎥⎥⎥⎦
,
(8.97)
(The Fourier matrix is sometimes deﬁned with a negative sign in the expo-
nents; that is, such that the (j, k)th entry is ω−(j−1)(k−1)/√n. The normal-
izing factor 1/√n is also sometimes omitted. In fact, in many applications
of Fourier matrices and various Fourier forms, there is inconsequential, but
possibly annoying, variation in the notation.)
Notice that the Fourier matrix is symmetric, and any entry raised to the
nth power is 1. Although the Fourier matrix is symmetric, its eigenvalues are
not necessarily real, because it itself is not a real matrix.
Fourier matrices whose order is a power of 2 tend to have a propensity of
elements that are either ±1 or ±i. For example, the 4 × 4 Fourier matrix is
F4 = 1
2
⎡
⎢⎢⎣
1
1
1
1
1
i −1 −i
1 −1
1 −1
1 −i −1
i
⎤
⎥⎥⎦.
(Recall that there are diﬀerent deﬁnitions of the elements of a Fourier matrix;
for the 4 × 4, they all are as shown above, but the patterns of positive and
negative values may be diﬀerent. Most of the elements of the 8 × 8 Fourier
matrix are either ±1 or ±i. The 16 that are not are ±1/
√
2 ± i/
√
2.
The Fourier matrix has many useful properties, such as being unitary;
that is, row and columns are orthonormal: f H
∗jf∗k = f H
j∗fk∗= 0 for j ̸= k, and
f H
∗jf∗j = 1 (Exercise 8.17).
The most interesting feature of the Fourier matrix is its relationship to the
Fourier transform. For an integrable function f(x), the Fourier transform is
Ff(s) =
, ∞
−∞
e−2πisxf(x) dx.
(8.98)
(Note that the characteristic function in probability theory is this same trans-
form applied to a probability density function, with argument t = −2πs.)

388
8 Matrices with Special Properties
8.8.6.1 Fourier Matrices and Elementary Circulant Matrices
The Fourier matrix and the elementary circulant matrix E(πc) of corre-
sponding order are closely related. Being a normal matrix, E(πc) is unitarily
diagonalizable, and the Fourier matrix and its conjugate transpose are the di-
agonalizing matrices, and the diagonal matrix itself, that is, the eigenvalues,
are elements of the Fourier matrix:
E(πc) = F H
n diag((1, ω, ω2, . . . , ωn−1))Fn.
(8.99)
This is easy to see by performing the multiplications on the right side of the
equation, and you are asked to do this in Exercise 8.18.
We see that the eigenvalues of E(πc) are what we would expect if we con-
tinued the development from page 137 where we determined the eigenvalues
of an elementary permutation matrix. (An elementary permutation matrix of
order 2 has the two eigenvalues
√
1 and −
√
1.)
Notice that the modulus of all eigenvalues of E(πc) is the same, 1. Hence,
all eigenvalues of E(πc) lie on its spectral circle.
8.8.6.2 The Discrete Fourier Transform
Fourier transforms are invertible transformations of functions that often al-
low operations on those functions to be performed more easily. The Fourier
transform shown in equation (8.98) may allow for simpler expressions of con-
volutions, for example. An n-vector is equivalent to a function whose domain
is {1, . . . , n}, and Fourier transforms of vectors are useful in various opera-
tions on the vectors. More importantly, if the vector represents observational
data, certain properties of the data may become immediately apparent in the
Fourier transform of the data vector.
The Fourier transform, at a given value s, of the function as shown in
equation (8.98) is an integral. The argument of the transform s may or may
not range over the same domain as x the argument of the function. The
analogue of the Fourier transform of a vector is a sum, again at some given
value, which is eﬀectively the argument of the transform. For an n-vector x,
the discrete Fourier transform at n points is
Fx = Fnx.
(8.100)
Transformations of this form are widely used in time series, where the vector
x contains observations at equally spaced points in time. While the elements
of x represent measurements at distinct points in time, the elements of the
vector Fx represent values at diﬀerent points in the period of a sine and/or
a cosine curve, as we see from the relation of the roots of unity given in
equation (8.96). Many time series, especially those relating to measurement
of waves propagating through matter or of vibrating objects, exhibit period-
icities, which can be used to distinguish diﬀerent kinds of waves and possibly

8.8 Other Matrices with Special Structures
389
to locate their source. Many waves and other time series have multiple peri-
odicities, at diﬀerent frequencies. The Fourier transform is sometimes called
a frequency ﬁlter.
Our purpose here is just to indicate the relation of Fourier transforms to
matrices, and to suggest how this might be useful in applications. There is a
wealth of literature on Fourier transforms and their applications, but we will
not pursue those topics here.
As often when writing expressions involving matrices, we must emphasize
that the form of a mathematical expression and the way the expression should
be evaluated in actual practice may be quite diﬀerent. This is particularly true
in the case of the discrete Fourier transform. There would never be a reason to
form the Fourier matrix for any computations, but more importantly, rather
than evaluating the elements in the Fourier transform Fx using the right side
of equation (8.100), we take advantage of properties of the powers of roots of
unity to arrive at a faster method of computing them—so much faster, in fact,
the method is called the “fast” Fourier transform, or FFT. The FFT is one
of the most important computational methods in all of applied mathematics.
I will not discuss it further here, however. Descriptions of it can be found
throughout the literature on computational science (including other books
that I have written).
An Aside: Complex Matrices
The Fourier matrix, I believe, is the only matrix I discuss in this book
that has complex entries. The purpose is just to relate the discrete
Fourier transform to matrix multiplication. We have already indicated
various diﬀerences in operations on vectors and matrices over the com-
plex plane. We often form the conjugate of a complex number, which
we denote by an overbar: ¯z. The conjugate of an object such as a vec-
tor or a matrix is the result of taking the conjugate of each element
individually. Instead of a transpose, we usually work with a conjugate
transpose, AH = AT. (See the discussion on pages 33 and 60.) We
deﬁne the inner product of vectors x and y as ⟨x, y⟩= ¯xTy; instead
of symmetric matrices, we focus on Hermitian matrices, that is, ones
such that the conjugate transpose is the same as the original matrices,
and instead of orthogonal matrices, we focus on unitary matrices, that
is, ones whose product with its conjugate transpose is the identity. All
of the general results that we have stated for real matrices of course
hold for complex matrices. Some of the analogous operations, how-
ever, have diﬀerent properties. For example, the property of a matrix
being unitarily diagonalizable is an analogue of the property of being
orthogonally diagonalizable, but, as we have seen, a wider class of ma-
trices (normal matrices) are unitarily diagonalizable than those that
are orthogonally diagonalizable (symmetric matrices).
Most scientiﬁc software systems support computations with complex
numbers. Both R and Matlab, for example, provide full capabilities for

390
8 Matrices with Special Properties
working with complex numbers. The imaginary unit in both is denoted
by “i”, which of course must be distinguished from “i” denoting a
variable. In R, the function complex can be used to initialize a complex
number; for example,
z<-complex(re=3,im=2)
assigns the value 3+2i to the variable z. (Another simple way of doing
this is to juxtapose a numeric literal in front of the symbol i; in R, for
example, z<-3+2i assigns the same value to z. I do not recommend the
latter construct because of possible confusion with other expressions.)
The jth row of an nth order Fourier matrix can be generated by the
R expression
c(1,exp(2*pi*complex(im=seq(j,j*(n-1),j)/n)))
As mentioned above, there would almost never be a reason to form the
Fourier matrix for any computations. It is instructive, however, to note
its form and to do some simple manipulations with the Fourier ma-
trix in R or some other software system. The Matlab function dftmtx
generates a Fourier matrix (with a slightly diﬀerent deﬁnition, result-
ing in a diﬀerent pattern of positives and negatives than what I have
shown above).
Some additional R code for manipulating complex matrices is given in
the hints for Exercise 8.18 on page 611. (Note that there I do form a
Fourier matrix and use it multiplications; but it is just for illustration.)
8.8.7 Hankel Matrices
A Hankel matrix is a square matrix with constant “anti”-codiagonals:
⎡
⎢⎢⎢⎢⎢⎢⎣
un−1 un−2 · · · u1
d
un−2 un−3 · · ·
d
l1
...
...
...
...
...
u1
d
l1
... ln−2
d
l1
l2 · · · ln−1
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(8.101)
Notice that a Hankel matrix is similar to a Toeplitz matrix, except that the
axial diagonal is not the principal diagonal; rather, for an n×n Hankel matrix
H, it is the “anti-diagonal”, consisting of the elements h1,n, h2,n−1, . . . , hn,1.
Although this “diagonal” is visually apparent, and symmetries about it are
intuitively obvious, there is no commonly-used term for this diagonal or for
symmetries about it. We can also observe analogues of lower triangular, up-
per triangular, and symmetric matrices based on u = 0, l = 0, and u = l;
but these are not the kinds of matrices that we have identiﬁed with these
terms. Words such as “anti” and “skew” are sometimes used to qualify names

8.8 Other Matrices with Special Structures
391
of these properties or objects. A triangular counterpart is sometimes called
“skew . . . diagonal”. There is no standard term for symmetry about the anti-
diagonal, however. The term “skew symmetric” has already been taken. (It is
a matrix A with aij = −aji.)
As with a Toeplitz matrix, a Hankel matrix is characterized by a “diago-
nal” element and two vectors, and can be generalized to n×m matrices based
on vectors of diﬀerent orders. As in the expression (8.101) above, an n × n
Toeplitz matrix can be deﬁned by a scalar d and two (n −1)-vectors, u and
l. There are other, perhaps more common, ways of putting the elements of a
Hankel matrix into two vectors. In Matlab, the function hankel produces a
Hankel matrix, but the elements are speciﬁed in a diﬀerent way from that in
expression (8.101).
A common form of Hankel matrix is an n×n skew upper triangular matrix,
and it is formed from the u vector only. The simplest form of the square skew
upper triangular Hankel matrix is formed from the vector u = (n −1, n −
2, . . . , 1) and d = n:
⎡
⎢⎢⎢⎣
1 2 3 · · · n
2 3 4 · · · 0
... ... ... · · · ...
n 0 0 · · · 0
⎤
⎥⎥⎥⎦.
(8.102)
A skew upper triangular Hankel matrix occurs in the spectral analysis of
time series. If x(t) is a (discrete) time series, for t = 0, 1, 2, . . ., the Hankel
matrix of the time series has as the (i, j) element
x(i + j −2)
if i + j −1 ≤n,
0
otherwise.
The L2 norm of the Hankel matrix of the time series is called the Hankel norm
of the frequency ﬁlter response (the Fourier transform).
8.8.8 Cauchy Matrices
Another type of special n × m matrix whose elements are determined by a
few n-vectors and m-vectors is a Cauchy-type matrix. The standard Cauchy
matrix is built from two vectors, x and y. The more general form deﬁned
below uses two additional vectors.
A Cauchy matrix is an n × m matrix C(x, y, v, w) generated by n-vectors
x and v and m-vectors y and w of the form
C(x, y, v, w) =
⎡
⎢⎢⎢⎣
v1w1
x1 −y1
· · ·
v1wm
x1 −ym
...
· · ·
...
vnw1
xn −y1
· · ·
vnwm
xn −ym
⎤
⎥⎥⎥⎦.
(8.103)

392
8 Matrices with Special Properties
Cauchy-type matrices often arise in the numerical solution of partial dif-
ferential equations (PDEs). For Cauchy matrices, the order of the number of
computations for factorization or solutions of linear systems can be reduced
from a power of three to a power of two. This is a very signiﬁcant improve-
ment for large matrices. In the PDE applications, the matrices are generally
not large, but nevertheless, even in those applications, it worthwhile to use al-
gorithms that take advantage of the special structure. Fasino and Gemignani
(2003) describe such an algorithm.
8.8.9 Matrices Useful in Graph Theory
Many problems in statistics and applied mathematics can be posed as graphs,
and various methods of graph theory can be used in their solution.
Graph theory is particularly useful in cluster analysis or classiﬁcation.
These involve the analysis of relationships of objects for the purpose of iden-
tifying similar groups of objects. The objects are associated with vertices of
the graph, and an edge is generated if the relationship (measured somehow)
between two objects is suﬃciently great. For example, suppose the question of
interest is the authorship of some text documents. Each document is a vertex,
and an edge between two vertices exists if there are enough words in common
between the two documents. A similar application could be the determination
of which computer user is associated with a given computer session. The ver-
tices would correspond to login sessions, and the edges would be established
based on the commonality of programs invoked or ﬁles accessed. In applica-
tions such as these, there would typically be a training dataset consisting of
text documents with known authors or consisting of session logs with known
users. In both of these types of applications, decisions would have to be made
about the extent of commonality of words, phrases, programs invoked, or ﬁles
accessed in order to establish an edge between two documents or sessions.
Unfortunately, as is often the case for an area of mathematics or statistics
that developed from applications in diverse areas or through the eﬀorts of ap-
plied mathematicians somewhat outside of the mainstream of mathematics,
there are major inconsistencies in the notation and terminology employed in
graph theory. Thus, we often ﬁnd diﬀerent terms for the same object; for ex-
ample, adjacency matrix and connectivity matrix. This unpleasant situation,
however, is not so disagreeable as a one-to-many inconsistency, such as the
designation of the eigenvalues of a graph to be the eigenvalues of one type
of matrix in some of the literature and the eigenvalues of diﬀerent types of
matrices in other literature.
Refer to Sect. 8.1.2 beginning on page 331 for terms and notation that we
will use in the following discussion.

8.8 Other Matrices with Special Structures
393
8.8.9.1 Adjacency Matrix: Connectivity Matrix
We discussed adjacency or connectivity matrices on page 334. A matrix, such
as an adjacency matrix, that consists of only 1s and 0s is called a Boolean
matrix.
Two vertices that are not connected and hence correspond to a 0 in a
connectivity matrix are said to be independent.
If no edges connect a vertex with itself, the adjacency matrix is a hollow
matrix.
Because the 1s in a connectivity matrix indicate a strong association, and
we would naturally think of a vertex as having a strong association with
itself, we sometimes modify the connectivity matrix so as to have 1s along the
diagonal. Such a matrix is sometimes called an augmented connectivity matrix
or augmented adjacency matrix.
The eigenvalues of the adjacency matrix reveal some interesting properties
of the graph and are sometimes called the eigenvalues of the graph. The eigen-
values of another matrix, which we discuss below, are more useful, however,
and we will refer to those eigenvalues as the eigenvalues of the graph.
8.8.9.2 Digraphs
The digraph represented in Fig. 8.4 on page 335 is a network with ﬁve vertices,
perhaps representing cities, and directed edges between some of the vertices.
The edges could represent airline connections between the cities; for example,
there are ﬂights from x to u and from u to x, and from y to z, but not from
z to y.
In a digraph, the relationships are directional. (An example of a directional
relationship that might be of interest is when each observational unit has a
diﬀerent number of measured features, and a relationship exists from vi to vj
if a majority of the features of vi are identical to measured features of vj.)
8.8.9.3 Use of the Connectivity Matrix
The analysis of a network may begin by identifying which vertices are con-
nected with others; that is, by construction of the connectivity matrix.
The connectivity matrix can then be used to analyze other levels of as-
sociation among the data represented by the graph or digraph. For example,
from the connectivity matrix in equation (8.2) on page 335, we have
A2 =
⎡
⎢⎢⎢⎢⎣
4 1 0 0 1
0 1 1 1 1
1 1 1 1 2
1 2 1 1 1
1 1 1 1 1
⎤
⎥⎥⎥⎥⎦
.

394
8 Matrices with Special Properties
In terms of the application suggested on page 335 for airline connections,
the matrix A2 represents the number of connections between the cities that
consist of exactly two ﬂights. From A2 we see that there are two ways to go
from city y to city w in just two ﬂights but only one way to go from w to y
in two ﬂights.
This property extends to multiple connections. If A is the adjacency matrix
of a graph, then Ak
ij is the number of paths of length k between nodes i and
j in that graph. (See Exercise 8.20.) Important areas of application of this
fact are in DNA sequence comparisons and measuring “centrality” of a node
within a complex social network.
8.8.9.4 The Laplacian Matrix of a Graph
Spectral graph theory is concerned with the analysis of the eigenvalues of a
graph. As mentioned above, there are two diﬀerent deﬁnitions of the eigenval-
ues of a graph. The more useful deﬁnition, and the one we use here, takes the
eigenvalues of a graph to be the eigenvalues of a matrix, called the Laplacian
matrix, formed from the adjacency matrix and a diagonal matrix consisting
of the degrees of the vertices.
Given the graph G, let D(G) be a diagonal matrix consisting of the degrees
of the vertices of G (that is, D(G) = diag(d(G))) and let C(G) be the adjacency
matrix of G. If there are no isolated vertices (that is if d(G) > 0), then the
Laplacian matrix of the graph, L(G) is given by
L(G) = I −D(G)−1
2 C(G)D(G)−1
2 .
(8.104)
Some authors deﬁne the Laplacian in other ways:
La(G) = I −D(G)−1C(G)
(8.105)
or
Lb(G) = D(G) −C(G).
(8.106)
The eigenvalues of the Laplacian matrix are the eigenvalues of a graph.
The deﬁnition of the Laplacian matrix given in equation (8.104) seems to
be more useful in terms of bounds on the eigenvalues of the graph. The set of
unique eigenvalues (the spectrum of the matrix L) is called the spectrum of
the graph.
So long as d(G) > 0, L(G) = D(G)−1
2 La(G)D(G)−1
2 . Unless the graph
is regular, the matrix Lb(G) is not symmetric. Note that if G is k-regular,
L(G) = I −C(G)/k, and Lb(G) = L(G).
For a digraph, the degrees are replaced by either the indegrees or the
outdegrees. (Some authors deﬁne it one way and others the other way. The
essential properties hold either way.)
The Laplacian can be viewed as an operator on the space of functions
f : V (G) →IR such that for the vertex v

8.8 Other Matrices with Special Structures
395
L(f(v)) =
1
√dv

w,w∼v
f(v)
√dv
−f(w)
√dw

,
where w ∼v means vertices w and v that are adjacent, and du is the degree
of the vertex u.
For a symmetric graph, the Laplacian matrix is symmetric, so its eigen-
values are all real. We can see that the eigenvalues are all nonnegative by
forming the Rayleigh quotient (equation (3.266)) using an arbitrary vector g,
which can be viewed as a real-valued function over the vertices,
RL(g) = ⟨g, Lg⟩
⟨g, g⟩
= ⟨g, D−1
2 LaD−1
2 g⟩
⟨g, g⟩
=
⟨f, Laf⟩
⟨D
1
2 f, D
1
2 f⟩
=

v∼w(f(v) −f(w))2
f TDf
,
(8.107)
where f = D−1
2 g, and f(u) is the element of the vector corresponding to
vertex u. Because the Raleigh quotient is nonnegative, all eigenvalues are
nonnegative, and because there is an f ̸= 0 for which the Rayleigh quotient is
0, we see that 0 is an eigenvalue of a graph. Furthermore, using the Cauchy-
Schwartz inequality, we see that the spectral radius is less than or equal to 2.
The eigenvalues of a matrix are the basic objects in spectral graph theory.
They provide information about the properties of networks and other systems
modeled by graphs. We will not explore them further here, and the interested
reader is referred to Bollob´as (2013) or other general texts on the subject.
If G is the graph represented in Fig. 8.2 on page 332, with V (G) =
{a, b, c, d, e}, the degrees of the vertices of the graph are d(G) = (4, 2, 2, 3, 3).
Using the adjacency matrix given in equation (8.1), we have
L(G) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −
√
2
4 −
√
2
4 −
√
3
6 −
√
3
6
−
√
2
4
1
0
0 −
√
6
6
−
√
2
4
0
1 −
√
6
6
0
−
√
3
6
0 −
√
6
6
1
−1
3
−
√
3
6 −
√
6
6
0
−1
3
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.108)
This matrix is singular, and the unnormalized eigenvector corresponding to
the 0 eigenvalue is (2
√
14, 2
√
7, 2
√
7,
√
42,
√
42).

396
8 Matrices with Special Properties
8.8.10 Z-Matrices and M-Matrices
In certain applications in physics and in the solution of systems of nonlinear
diﬀerential equations, a class of matrices called M-matrices is important.
The matrices in these applications have nonpositive oﬀ-diagonal elements.
A square matrix all of whose oﬀ-diagonal elements are nonpositive is called a
Z-matrix.
A Z-matrix that is positive stable (see page 159) is called an M-matrix. A
real symmetric M-matrix is positive deﬁnite.
In addition to the properties that constitute the deﬁnition, M-matrices
have a number of remarkable properties, which we state here without proof.
If A is a real M-matrix, then
•
all principal minors of A are positive;
•
all diagonal elements of A are positive;
•
all diagonal elements of L and U in the LU decomposition of A are positive;
•
for some i, 
j aij ≥0; and
•
A is nonsingular and A−1 ≥0.
Proofs of some of these facts can be found in Horn and Johnson (1991).
Exercises
8.1. Normal matrices.
a) Show that a skew symmetric matrix is normal.
b) Show that a skew Hermitian matrix is normal.
8.2. Ordering of nonnegative deﬁnite matrices.
a) A relation ▷◁on a set is a partial ordering if, for elements a, b, and c,
•
it is reﬂexive: a ▷◁a;
•
it is antisymmetric: a ▷◁b ▷◁a =⇒a = b; and
•
it is transitive: a ▷◁b ▷◁c =⇒a ▷◁c.
Show that the relation ⪰(equation (8.19)) is a partial ordering.
b) Show that the relation ≻(equation (8.20)) is transitive.
8.3. a) Show that a (strictly) diagonally dominant symmetric matrix is pos-
itive deﬁnite.
b) Show that if the real n × n symmetric matrix A is such that
aii ≥
n

j̸=i
|aij|
for each i = 1, . . . , n
then A ⪰0.
8.4. Show that the number of positive eigenvalues of an idempotent matrix
is the rank of the matrix.
8.5. Show that two idempotent matrices of the same rank are similar.

Exercises
397
8.6. Under the given conditions, show that properties (a) and (b) on page 357
imply property (c).
8.7. Projections.
a) Show that the matrix given in equation (8.42) (page 359) is a pro-
jection matrix.
b) Write out the projection matrix for projecting a vector onto the
plane formed by two vectors, x1 and x2, as indicated on page 359,
and show that it is the same as the hat matrix of equation (8.52).
8.8. Correlation matrices.
A correlation matrix can be deﬁned in terms of a Gramian matrix formed
by a centered and scaled matrix, as in equation (8.69). Sometimes in the
development of statistical theory, we are interested in the properties of
correlation matrices with given eigenvalues or with given ratios of the
largest eigenvalue to other eigenvalues.
Write a program to generate n × n random correlation matrices R with
speciﬁed eigenvalues, c1, . . . , cn. The only requirements on R are that its
diagonals be 1, that it be symmetric, and that its eigenvalues all be posi-
tive and sum to n. Use the following method due to Davies and Higham
(2000) that uses random orthogonal matrices with the Haar uniform
distribution generated using the method described in Exercise 4.10.
0. Generate a random orthogonal matrix Q; set k = 0, and form
R(0) = Qdiag((c1, . . . , cn))QT.
1. If r(k)
ii
= 1 for all i in {1, . . . , n}, go to step 3.
2. Otherwise, choose p and q with p < j, such that r(k)
pp < 1 < r(k)
qq or
r(k)
pp > 1 > r(k)
qq , and form G(k) as in equation (5.13), where c and s
are as in equations (5.17) and (5.18), with a = 1.
Form R(k+1) = (G(k))TR(k)G(k).
Set k = k + 1, and go to step 1.
3. Deliver R = R(k).
8.9. Use the relationship (8.77) to prove properties 1 and 4 on page 377.
8.10. Leslie matrices.
a) Write the characteristic polynomial of the Leslie matrix, equa-
tion (8.85).
b) Show that the Leslie matrix has a single, unique positive eigenvalue.
8.11. Write out the determinant for an n × n Vandermonde matrix.
Hint: The determinant of an n × n Vandermonde matrix as in equa-
tion (8.88) is (xn −x1)(xn −x2) · · · (xn −xn−1) times the determinant
of the n −1 × n −1 Vandermonde matrix formed by removing the last
row and column. Show this by multiplying the original Vandermonde
matrix by B = I + D, where D is the matrix with 0s in all positions ex-
cept for the ﬁrst supradiagonal, which consists of −xn, replicated n −1
times. Clearly, the determinant of B is 1.

398
8 Matrices with Special Properties
8.12. Consider the 3 × 3 symmetric Toeplitz matrix with elements a, b, and
c; that is, the matrix that looks like this:
⎡
⎣
1 b c
b 1 b
c b 1
⎤
⎦.
a) Invert this matrix.
See page 385.
b) Determine conditions for which the matrix would be singular.
8.13. If A is circulant, show that
•
AT is circulant
•
A2 is circulant
•
if A is nonsingular, then A−1 is circulant
Hint: Use equation (8.95).
8.14. Show that the set of all n × n circulant matrices is a vector space along
with the axpy operation. (Just show that it is closed with respect to
that operation.)
8.15. If A and B are circulant matrices of the same order, show AB is circu-
lant.
8.16. Show that a circulant matrix is normal.
8.17. Show that a Fourier matrix, as in equation (8.97), is unitary by showing
f H
∗jf∗k = f H
j∗fk∗= 0 for j ̸= k, and f H
∗jf∗j = 1.
8.18. Show that equation (8.99) is correct by performing the multiplications
on the right side of the equation.
8.19. Write out the determinant for the n × n skew upper triangular Hankel
matrix in (8.102).
8.20. Graphs. Let A be the adjacency matrix of an undirected graph.
a) Show that A2
ij is the number of paths of length 2 between nodes i
and j.
Hint: Construct a general diagram similar to Fig. 8.2 on page 332,
and count the paths between two arbitrary nodes.
b) Show that Ak
ij is the number of paths of length k between nodes i
and j.
Hint: Use Exercise 8.20a and mathematical induction on k.

9
Selected Applications in Statistics
Data come in many forms. In the broad view, the term “data” embraces all
representations of information or knowledge. There is no single structure that
can eﬃciently contain all of these representations. Some data are in free-form
text (for example, the Federalist Papers, which was the subject of a famous
statistical analysis), other data are in a hierarchical structure (for example,
political units and subunits), and still other data are encodings of methods
or algorithms. (This broad view is entirely consistent with the concept of a
“stored-program computer”; the program is the data.)
Several of the results in this chapter have already been presented in Chap. 8
or even in previous chapters, for example, the smoothing matrix Hλ that we
discuss in Sect. 9.3.8 has already been encountered on page 364 in Chap. 8.
The purpose of the apparent redundancy is to present the results from a
diﬀerent perspective. (None of the results are new; all are standard in the
statistical literature.)
9.1 Structure in Data and Statistical Data Analysis
Data often have a logical structure as described in Sect. 8.1.1; that is, a two-
dimensional array in which columns correspond to variables or measurable
attributes and rows correspond to an observation on all attributes taken to-
gether. A matrix is obviously a convenient object for representing numeric
data organized this way. An objective in analyzing data of this form is to un-
cover relationships among the variables, or to characterize the distribution of
the sample over IRm. Interesting relationships and patterns are called “struc-
ture” in the data. This is a diﬀerent meaning from that of the word used in the
phrase “logical structure” or in the phrase “data structure” used in computer
science.
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 9
399

400
9 Selected Applications in Statistics
Another type of pattern that may be of interest is a temporal pattern;
that is, a set of relationships among the data and the time or the sequence in
which the data were observed.
The objective of this chapter is to illustrate how some of the properties of
matrices and vectors that were covered in previous chapters relate to statistical
models and to data analysis procedures. The ﬁeld of statistics is far too large
for a single chapter on “applications” to cover more than just a small part
of the area. Similarly, the topics covered previously are too extensive to give
examples of applications of all of them.
A probability distribution is a speciﬁcation of the stochastic structure of
random variables, so we begin with a brief discussion of properties of multi-
variate probability distributions. The emphasis is on the multivariate normal
distribution and distributions of linear and quadratic transformations of nor-
mal random variables. We then consider an important structure in multivari-
ate data, a linear model. We discuss some of the computational methods used
in analyzing the linear model. We then describe some computational method
for identifying more general linear structure and patterns in multivariate data.
Next we consider approximation of matrices in the absence of complete data.
Finally, we discuss some models of stochastic processes. The special matrices
discussed in Chap. 8 play an important role in this chapter.
9.2 Multivariate Probability Distributions
Most methods of statistical inference are based on assumptions about some
underlying probability distribution of a random variable. In some cases these
assumptions completely specify the form of the distribution, and in other
cases, especially in nonparametric methods, the assumptions are more general.
Many statistical methods in estimation and hypothesis testing rely on the
properties of various transformations of a random variable.
In this section, we do not attempt to develop a theory of probability dis-
tribution; rather we assume some basic facts and then derive some important
properties that depend on the matrix theory of the previous chapters.
9.2.1 Basic Deﬁnitions and Properties
One of the most useful descriptors of a random variable is its probability
density function (PDF), or probability function. Various functionals of the
PDF deﬁne standard properties of the random variable, such as the mean and
variance, as we discussed in Sect. 4.5.3.
If X is a random variable over IRd with PDF pX(·) and f(·) is a measurable
function (with respect to a dominating measure of pX(·)) from IRd to IRk, the
expected value of f(X), which is in IRk and is denoted by E(g(X)), is deﬁned by
E(f(X)) =
,
IRd f(t)pX(t) dt.

9.2 Multivariate Probability Distributions
401
The mean of X is the d-vector E(X), and the variance or variance-
covariance of X, denoted by V(X), is the d × d matrix
V(X) = E

(X −E(X)) (X −E(X))T
.
Given a random variable X, we are often interested in a random variable
deﬁned as a function of X, say Y = g(X). To analyze properties of Y , we
identify g−1, which may involve another random variable. (For example, if
g(x) = x2 and the support of X is IR, then g−1(Y ) = (−1)α√
Y , where α = 1
with probability Pr(X < 0) and α = 0 otherwise.) Properties of Y can be
evaluated using the Jacobian of g−1(·), as in equation (4.12).
9.2.2 The Multivariate Normal Distribution
The most important multivariate distribution is the multivariate normal,
which we denote as Nd(μ, Σ) for d dimensions; that is, for a random d-vector.
The PDF for the d-variate normal distribution, as we have discussed before, is
pX(x) = (2π)−d/2|Σ|−1/2e−(x−μ)TΣ−1(x−μ)/2,
(9.1)
where the normalizing constant is Aitken’s integral given in equation (4.75).
The multivariate normal distribution is a good model for a wide range of
random phenomena.
9.2.3 Derived Distributions and Cochran’s Theorem
If X is a random variable with distribution Nd(μ, Σ), A is a q × d matrix
with rank q (which implies q ≤d), and Y = AX, then the straightforward
change-of-variables technique yields the distribution of Y as Nd(Aμ, AΣAT).
Useful transformations of the random variable X with distribution
Nd(μ, Σ) are Y1 = Σ−1/2X and Y2 = Σ−1
C X, where ΣC is a Cholesky
factor of Σ. In either case, the variance-covariance matrix of the transformed
variate Y1 or Y2 is Id.
Quadratic forms involving a Y that is distributed as Nd(μ, Id) have useful
properties. For statistical inference it is important to know the distribution
of these quadratic forms. The simplest quadratic form involves the identity
matrix: Sd = Y TY .
We can derive the PDF of Sd by beginning with d = 1 and using induction.
If d = 1, for t > 0, we have
Pr(S1 ≤t) = Pr(Y ≤
√
t) −Pr(Y ≤−
√
t),
where Y ∼N1(μ, 1), and so the PDF of S1 is
pS1(t) =
1
2
√
2πt

e−(
√
t−μ)2/2 + e−(−
√
t−μ)2/2

402
9 Selected Applications in Statistics
= e−μ2/2e−t/2
2
√
2πt

eμ
√
t + e−μ
√
t
= e−μ2/2e−t/2
2
√
2πt
⎛
⎝
∞

j=0
(μ
√
t)j
j!
+
∞

j=0
(−μ
√
t)j
j!
⎞
⎠
= e−μ2/2e−t/2
√
2t
∞

j=0
(μ2t)j
√π(2j)!
= e−μ2/2e−t/2
√
2t
∞

j=0
(μ2t)j
j!Γ(j + 1/2)22j ,
in which we use the fact that
Γ(j + 1/2) =
√π(2j)!
j!22j
(see page 595). This can now be written as
pS1(t) = e−μ2/2
∞

j=0
(μ2)j
j!2j
1
Γ(j + 1/2)2j+1/2 tj−1/2e−t/2,
(9.2)
in which we recognize the PDF of the central chi-squared distribution with
2j + 1 degrees of freedom,
pχ2
2j+1(t) =
1
Γ(j + 1/2)2j+1/2 tj−1/2e−t/2.
(9.3)
A similar manipulation for d = 2 (that is, for Y ∼N2(μ, 1), and maybe
d = 3, or as far as you need to go) leads us to a general form for the PDF of
the χ2
d(δ) random variable Sd:
pSd(t) = e−μ2/2
∞

j=0
(μ2/2)j
j!
pχ2
2j+1(t).
(9.4)
We can show that equation (9.4) holds for any d by induction. The distribu-
tion of Sd is called the noncentral chi-squared distribution with d degrees of
freedom and noncentrality parameter δ = μTμ. We denote this distribution
as χ2
d(δ).
The induction method above involves a special case of a more general fact:
if Xi for i = 1, . . . , k are independently distributed as χ2
ni(δi), then 
i Xi is
distributed as χ2
n(δ), where n = 
i ni and δ = 
i δi. (Compare this with the
result for Wishart distributions in Exercise 4.12b on page 225.)
In applications of linear models, a quadratic form involving Y is often
partitioned into a sum of quadratic forms. Assume that Y is distributed as
Nd(μ, Id), and for i = 1, . . . k, let Ai be a d × d symmetric matrix with rank

9.3 Linear Models
403
ri such that 
i Ai = Id. This yields a partition of the total sum of squares
Y TY into k components:
Y TY = Y TA1Y + · · · + Y TAkY.
(9.5)
One of the most important results in the analysis of linear models states
that the Y TAiY have independent noncentral chi-squared distributions χ2
ri(δi)
with δi = μTAiμ if and only if 
i ri = d.
This is called Cochran’s theorem. Beginning on page 355, we discussed a
form of Cochran’s theorem that applies to properties of idempotent matrices.
Those results immediately imply the conclusion above.
9.3 Linear Models
Some of the most important applications of statistics involve the study of the
relationship of one variable, often called a “response variable”, to other vari-
ables. The response variable is usually modeled as a random variable, which
we indicate by using a capital letter. A general model for the relationship of
a variable, Y , to other variables, x (a vector), is
Y ≈f(x).
(9.6)
In this asymmetric model and others like it, we call Y the dependent variable
and the elements of x the independent variables.
It is often reasonable to formulate the model with a systematic component
expressing the relationship and an additive random component or “additive
error”. We write
Y = f(x) + E,
(9.7)
where E is a random variable with an expected value of 0; that is,
E(E) = 0.
(Although this is by far the most common type of model used by data analysts,
there are other ways of building a model that incorporates systematic and
random components.) The zero expectation of the random error yields the
relationship
E(Y ) = f(x),
although this expression is not equivalent to the additive error model above
because the random component could just as well be multiplicative (with an
expected value of 1) and the same value of E(Y ) would result.
Because the functional form f of the relationship between Y and x may
contain a parameter, we may write the model as
Y = f(x; θ) + E.
(9.8)

404
9 Selected Applications in Statistics
A speciﬁc form of this model is
Y = βTx + E,
(9.9)
which expresses the systematic component as a linear combination of the xs
using the vector parameter β.
A model is more than an equation; there may be associated statements
about the distribution of the random variable or about the nature of f or x.
We may assume β (or θ) is a ﬁxed but unknown constant, or we may assume
it is a realization of a random variable. Whatever additional assumptions we
may make, there are some standard assumptions that go with the model.
We assume that Y and x are observable and θ and E are unobservable.
Models such as these that express an asymmetric relationship between
some variables (“dependent variables”) and other variables (“independent
variables”) are called regression models. A model such as equation (9.9) is
called a linear regression model. There are many useful variations of the
model (9.6) that express other kinds of relationships between the response
variable and the other variables.
Notation
In data analysis with regression models, we have a set of observations {yi, xi}
where xi is an m-vector. One of the primary tasks is to determine a reasonable
value of the parameter. That is, in the linear regression model, for example,
we think of β as an unknown variable (rather than as a ﬁxed constant or a
realization of a random variable), and we want to ﬁnd a value of it such that
the model ﬁts the observations well,
yi = βTxi + ϵi,
(9.10)
where β and xi are m-vectors. (In the expression (9.9), “E” is an uppercase
epsilon. We attempt to use notation consistently; “E” represents a random
variable, and “ϵ” represents a realization, though an unobservable one, of the
random variable. We will not always follow this convention, however; some-
times it is convenient to use the language more loosely and to speak of ϵi as a
random variable.) The meaning of the phrase “the model ﬁts the observations
well” may vary depending on other aspects of the model, in particular, on any
assumptions about the distribution of the random component E. If we make
assumptions about the distribution, we have a basis for statistical estimation
of β; otherwise, we can deﬁne some purely mathematical criterion for “ﬁtting
well” and proceed to determine a value of β that optimizes that criterion.
For any choice of β, say b, we have yi = bTxi + ri. The ris are determined
by the observations. An approach that does not depend on any assumptions
about the distribution but can nevertheless yield optimal estimators under
many distributions is to choose the estimator so as to minimize some measure
of the set of ris.

9.3 Linear Models
405
Given the observations {yi, xi}, we can represent the regression model and
the data as
y = Xβ + ϵ,
(9.11)
where X is the n × m matrix whose rows are the xis and ϵ is the vector of
deviations (“errors”) of the observations from the functional model. Through-
out the rest of this section, we will assume that the number of rows of X (that
is, the number of observations n) is greater than the number of columns of X
(that is, the number of variables m).
We will occasionally refer to submatrices of the basic data matrix X using
notation developed in Chap. 3. For example, X(i1,...,ik)(j1,...,jl) refers to the
k × l matrix formed by retaining only the i1, . . . , ik rows and the j1, . . . , jl
columns of X, and X−(i1,...,ik)(j1,...,jl) refers to the matrix formed by deleting
the i1, . . . , ik rows and the j1, . . . , jl columns of X. We also use the notation
xi∗to refer to the ith row of X (the row is a vector, a column vector), and x∗j
to refer to the jth column of X. See page 599 for a summary of this notation.
9.3.1 Fitting the Model
In a model for a given dataset as in equation (9.11), although the errors are
no longer random variables (they are realizations of random variables), they
are not observable. To ﬁt the model, we replace the unknowns with variables:
β with b and ϵ with r. This yields
y = Xb + r.
(9.12)
We then proceed by applying some criterion for ﬁtting.
The criteria generally focus on the “residuals” r = y −Xb. Two general
approaches to ﬁtting are:
•
Deﬁne a likelihood function of r based on an assumed distribution of E,
and determine a value of b that maximizes that likelihood.
•
Decide on an appropriate norm on r, and determine a value of b that
minimizes that norm.
There are other possible approaches, and there are variations on these two
approaches. For the ﬁrst approach, it must be emphasized that r is not a
realization of the random variable E. Our emphasis will be on the second
approach, that is, on methods that minimize a norm on r.
9.3.1.1 Statistical Estimation
The statistical problem is to estimate β. (Notice the distinction between the
phrases “to estimate β” and “to determine a value of β that minimizes . . . ”.
The mechanical aspects of the two problems may be the same, of course.) The
statistician uses the model and the given observations to explore relationships
between the response and the regressors. Considering ϵ to be a realization of
a random variable E (a vector) and assumptions about a distribution of the
random variable ϵ allow us to make statistical inferences about a “true” β.

406
9 Selected Applications in Statistics
9.3.1.2 Ordinary Least Squares
The r vector contains the distances of the observations on y from the values
of the variable y deﬁned by the hyperplane bTx, measured in the direction of
the y axis. The objective is to determine a value of b that minimizes some
norm of r. The use of the L2 norm is called “least squares”. The estimate is
the b that minimizes the dot product
(y −Xb)T(y −Xb) =
n

i=1
(yi −xT
i∗b)2.
(9.13)
As we saw in Sect. 6.6 (where we used slightly diﬀerent notation), using
elementary calculus to determine the minimum of equation (9.13) yields the
“normal equations”
XTX *β = XTy.
(9.14)
9.3.1.3 Weighted Least Squares
The elements of the residual vector may be weighted diﬀerently. This is ap-
propriate if, for instance, the variance of the residual depends on the value of
x; that is, in the notation of equation (9.7), V(E) = g(x), where g is some
function. If the function is known, we can address the problem almost identi-
cally as in the use of ordinary least squares, as we saw on page 295. Weighted
least squares may also be appropriate if the observations in the sample are
not independent. In this case also, if we know the variance-covariance struc-
ture, after a simple transformation, we can use ordinary least squares. If the
function g or the variance-covariance structure must be estimated, the ﬁtting
problem is still straightforward, but formidable complications are introduced
into other aspects of statistical inference. We discuss weighted least squares
further in Sect. 9.3.6.
9.3.1.4 Variations on the Criteria for Fitting
Rather than minimizing a norm of r, there are many other approaches we could
use to ﬁt the model to the data. Of course, just the choice of the norm yields
diﬀerent approaches. Some of these approaches may depend on distributional
assumptions, which we will not consider here. The point that we want to
emphasize here, with little additional comment, is that the standard approach
to regression modeling is not the only one. We mentioned some of these other
approaches and the computational methods of dealing with them in Sect. 6.7.
Alternative criteria for ﬁtting regression models are sometimes considered in
the many textbooks and monographs on data analysis using a linear regression
model. This is because the ﬁts may be more “robust” or more resistant to the
eﬀects of various statistical distributions.

9.3 Linear Models
407
9.3.1.5 Regularized Fits
Some variations on the basic approach of minimizing residuals involve a kind of
regularization that may take the form of an additive penalty on the objective
function. Regularization often results in a shrinkage of the estimator toward 0.
One of the most common types of shrinkage estimator is the ridge regression
estimator, which for the model y = Xβ + ϵ is the solution of the modiﬁed
normal equations (XTX +λI)β = XTy. We discuss this further in Sect. 9.5.4.
9.3.1.6 Orthogonal Distances
Another approach is to deﬁne an optimal value of β as one that minimizes a
norm of the distances of the observed values of y from the vector Xβ. This
is sometimes called “orthogonal distance regression”. The use of the L2 norm
on this vector is sometimes called “total least squares”. This is a reasonable
approach when it is assumed that the observations in X are realizations of
some random variable; that is, an “errors-in-variables” model is appropriate.
The model in equation (9.11) is modiﬁed to consist of two error terms: one for
the errors in the variables and one for the error in the equation. The methods
discussed in Sect. 6.7.3 can be used to ﬁt a model using a criterion of mini-
mum norm of orthogonal residuals. As we mentioned there, weighting of the
orthogonal residuals can be easily accomplished in the usual way of handling
weights on the diﬀerent observations.
The weight matrix often is formed as an inverse of a variance-covariance
matrix Σ; hence, the modiﬁcation is to premultiply the matrix [X|y] in equa-
tion (6.56) by the Cholesky factor Σ−1
C . In the case of errors-in-variables,
however, there may be another variance-covariance structure to account for.
If the variance-covariance matrix of the columns of X (that is, the indepen-
dent variables) together with y is T , then we handle the weighting for vari-
ances and covariances of the columns of X in the same way, except of course
we postmultiply the matrix [X|y] in equation (6.56) by T −1
C . This matrix is
(m + 1) × (m + 1); however, it may be appropriate to assume any error in y is
already accounted for, and so the last row and column of T may be 0 except
for the (m + 1, m + 1) element, which would be 1. The appropriate model
depends on the nature of the data, of course.
9.3.1.7 Collinearity
A major problem in regression analysis is collinearity (or “multicollinearity”),
by which we mean a “near singularity” of the X matrix. This can be made
more precise in terms of a condition number, as discussed in Sect. 6.1. Ill-
conditioning may not only present computational problems, but also may
result in an estimate with a very large variance.

408
9 Selected Applications in Statistics
9.3.2 Linear Models and Least Squares
The most common estimator of β is one that minimizes the L2 norm of the
vertical distances in equation (9.11); that is, the one that forms a least squares
ﬁt. This criterion leads to the normal equations (9.14), whose solution is
*β = (XTX)−XTy.
(9.15)
(As we have pointed out many times, we often write formulas that are not
to be used for computing a result; this is the case here.) If X is of full rank,
the generalized inverse in equation (9.15) is, of course, the inverse, and *β
is the unique least squares estimator. If X is not of full rank, we generally
use the Moore-Penrose inverse, (XTX)+, in equation (9.15).
As we saw in equations (6.43) and (6.44), we also have
*β = X+y.
(9.16)
On page 293, we derived this least squares solution by use of the QR decom-
position of X. In Exercises 6.5a and 6.5b we mentioned two other ways to
derive this important expression.
Equation (9.16) indicates the appropriate way to compute *β. As we have
seen many times before, however, we often use an expression without com-
puting the individual terms. Instead of computing X+ in equation (9.16)
explicitly, we use either Householder or Givens transformations to obtain the
orthogonal decomposition
X = QR,
or
X = QRU T
if X is not of full rank. As we have seen, the QR decomposition of X can be
performed row-wise using Givens transformations. This is especially useful if
the data are available only one observation at a time. The equation used for
computing *β is
R*β = QTy,
(9.17)
which can be solved by back substitution in the triangular matrix R.
Because
XTX = RTR,
the quantities in XTX or its inverse, which are useful for making inferences
using the regression model, can be obtained from the QR decomposition.
If X is not of full rank, the expression (9.16) not only is a least squares
solution but the one with minimum length (minimum Euclidean norm), as we
saw in equations (6.44) and (6.45).
The vector *y = X *β is the projection of the n-vector y onto a space of di-
mension equal to the (column) rank of X, which we denote by rX. The vector

9.3 Linear Models
409
of the model, E(Y ) = Xβ, is also in the rX-dimensional space span(X). The
projection matrix I −X(XTX)+XT projects y onto an (n −rX)-dimensional
residual space that is orthogonal to span(X). Figure 9.1 represents these sub-
spaces and the vectors in them.
span(X)
0
span(X)
Xβ?
y
y
y
y
θ
Figure 9.1. The linear least squares ﬁt of y with X
Recall from page 291 that orthogonality of the residuals to span(X) is
not only a property of a least squares solution, it actually characterizes a
least squares solution; that is, if *b is such that XT(y −X*b) = 0, then *b is a
least squares solution.
In the (rX + 1)-order vector space of the variables, the hyperplane deﬁned
by *βTx is the estimated model (assuming *β ̸= 0; otherwise, the space is of
order rX).
9.3.2.1 Degrees of Freedom
In general, the vector y can range freely over an n-dimensional space. We say
the degrees of freedom of y, or the total degrees of freedom, is n. If we ﬁx the
mean of y, then the adjusted total degrees of freedom is n −1.
The model Xβ can range over a space with dimension equal to the (col-
umn) rank of X; that is, rX. We say that the model degrees of freedom is rX.
Note that the space of X *β is the same as the space of Xβ.
Finally, the space orthogonal to X *β (that is, the space of the residuals
y −X *β) has dimension n −rX. We say that the residual (or error) degrees

410
9 Selected Applications in Statistics
of freedom is n −rX. (Note that the error vector ϵ can range over an n-
dimensional space, but because *β is a least squares ﬁt, y −X *β can only range
over an (n −rX)-dimensional space.)
9.3.2.2 The Hat Matrix and Leverage
The projection matrix H = X(XTX)+XT is sometimes called the “hat ma-
trix” because
*y = X *β
= X(XTX)+XTy
= Hy,
(9.18)
that is, it projects y onto *y in the span of X. Notice that the hat matrix can
be computed without knowledge of the observations in y.
The elements of H are useful in assessing the eﬀect of the particular pattern
of the regressors on the predicted values of the response. The extent to which
a given point in the row space of X aﬀects the regression ﬁt is called its
“leverage”. The leverage of the ith observation is
hii = xT
i∗(XTX)+xi∗.
(9.19)
This is just the partial derivative of ˆyi with respect to yi (Exercise 9.2).
A relatively large value of hii compared with the other diagonal elements of
the hat matrix means that the ith observed response, yi, has a correspondingly
relatively large eﬀect on the regression ﬁt.
9.3.3 Statistical Inference
Fitting a model by least squares or by minimizing some other norm of the
residuals in the data might be a sensible thing to do without any concern for
a probability distribution. “Least squares” per se is not a statistical criterion.
Certain statistical criteria, such as maximum likelihood or minimum variance
estimation among a certain class of unbiased estimators, however, lead to an
estimator that is the solution to a least squares problem for speciﬁc probability
distributions.
For statistical inference about the parameters of the model y = Xβ + ϵ
in equation (9.11), we must add something to the model. As in statistical
inference generally, we must identify the random variables and make some
statements (assumptions) about their distribution. The simplest assumptions
are that ϵ is a random variable and E(ϵ) = 0. Whether or not the matrix X is
random, our interest is in making inference conditional on the observed values
of X.

9.3 Linear Models
411
9.3.3.1 Estimability
One of the most important questions for statistical inference involves esti-
mating or testing some linear combination of the elements of the parameter
β; for example, we may wish to estimate β1 −β2 or to test the hypothesis
that β1 −β2 = c1 for some constant c1. In general, we will consider the linear
combination lTβ. Whether or not it makes sense to estimate such a linear
combination depends on whether there is a function of the observable random
variable Y such that g(E(Y )) = lTβ.
We generally restrict our attention to linear functions of E(Y ) and formally
deﬁne a linear combination lTβ to be linearly estimable if there exists a vector
t such that
tTE(Y ) = lTβ
(9.20)
for any β.
It is clear that if X is of full column rank, lTβ is linearly estimable for any
l or, more generally, lTβ is linearly estimable for any l ∈span(XT). (The t
vector is just the normalized coeﬃcients expressing l in terms of the columns
of X.)
Estimability depends only on the simplest distributional assumption about
the model; that is, that E(ϵ) = 0. Under this assumption, we see that the
estimator *β based on the least squares ﬁt of β is unbiased for the linearly
estimable function lTβ. Because l ∈span(XT) = span(XTX), we can write
l = XTX˜t. Now, we have
E(lT *β) = E(lT(XTX)+XTy)
= ˜tTXTX(XTX)+XTXβ
= ˜tTXTXβ
= lTβ.
(9.21)
Although we have been taking *β to be (XTX)+XTy, the equations above
follow for other least squares ﬁts, b = (XTX)−XTy, for any generalized in-
verse. In fact, the estimator of lTβ is invariant to the choice of the generalized
inverse. This is because if b = (XTX)−XTy, we have XTXb = XTy, and so
lT *β −lTb = ˜tTXTX(*β −b) = ˜tT(XTy −XTy) = 0.
(9.22)
In the context of the linear model, we call an estimator of β a linear
estimator if it can be expressed as Ay for some matrix A, and we call an
estimator of lTβ a linear estimator if it can be expressed as aTy for some
vector a. It is clear that the least squares estimators *β and lT *β are linear
estimators.
Other properties of the estimators depend on additional assumptions
about the distribution of ϵ, and we will consider some of them below.
When X is not of full rank, we often are interested in an orthogonal basis
for span(XT). If X includes a column of 1s, the elements of any vector in

412
9 Selected Applications in Statistics
the basis must sum to 0. Such vectors are called contrasts. The second and
subsequent rows of the Helmert matrix (see Sect. 8.8.1 on page 381) are con-
trasts that are often of interest because of their regular patterns and their
interpretability in applications involving the analysis of levels of factors in
experiments.
9.3.3.2 Testability
We deﬁne a linear hypothesis lTβ = c1 as testable if lTβ is estimable. We
generally restrict our attention to testable hypotheses.
It is often of interest to test multiple hypotheses concerning linear combi-
nations of the elements of β. For the model (9.11), the general linear hypoth-
esis is
H0 : LTβ = c,
where L is m × q, of rank q, and such that span(L) ⊆span(X).
The test for a hypothesis depends on the distributions of the random
variables in the model. If we assume that the elements of ϵ are i.i.d. normal
with a mean of 0, then the general linear hypothesis is tested using an F
statistic whose numerator is the diﬀerence in the residual sum of squares from
ﬁtting the model with the restriction LTβ = c and the residual sum of squares
from ﬁtting the unrestricted model. This reduced sum of squares is
(LT *β −c)T (LT(XTX)∗L)−1 (LT *β −c),
(9.23)
where (XTX)∗is any g2 inverse of XTX. This test is a likelihood ratio test.
(See a text on linear models, such as Searle 1971, for more discussion on this
testing problem.)
To compute the quantity in expression (9.23), ﬁrst observe
LT(XTX)∗L = (X(XTX)∗L)T (X(XTX)∗L).
(9.24)
Now, if X(XTX)∗L, which has rank q, is decomposed as
X(XTX)∗L = P
!T
0
"
,
where P is an m × m orthogonal matrix and T is a q × q upper triangular
matrix, we can write the reduced sum of squares (9.23) as
(LT *β −c)T (T TT )−1 (LT *β −c)
or

T −T(LT *β −c)
T 
T −T(LT *β −c)


9.3 Linear Models
413
or
vTv.
(9.25)
To compute v, we solve
T Tv = LT *β −c
(9.26)
for v, and the reduced sum of squares is then formed as vTv.
9.3.3.3 The Gauss-Markov Theorem
The Gauss-Markov theorem provides a restricted optimality property for es-
timators of estimable functions of β under the condition that E(ϵ) = 0 and
V(ϵ) = σ2I; that is, in addition to the assumption of zero expectation, which
we have used above, we also assume that the elements of ϵ have constant vari-
ance and that their covariances are zero. (We are not assuming independence
or normality, as we did in order to develop tests of hypotheses.)
Given y = Xβ + ϵ and E(ϵ) = 0 and V(ϵ) = σ2I, the Gauss-Markov
theorem states that lT *β is the unique best linear unbiased estimator (BLUE)
of the estimable function lTβ.
“Linear” estimator in this context means a linear combination of y; that
is, an estimator in the form aTy. It is clear that lT *β is linear, and we have
already seen that it is unbiased for lTβ. “Best” in this context means that
its variance is no greater than any other estimator that ﬁts the requirements.
Hence, to prove the theorem, ﬁrst let aTy be any unbiased estimator of lTβ,
and write l = XTX˜t as above. Because aTy is unbiased for any β, as we saw
above, it must be the case that aTX = lT. Recalling that XTX *β = XTy, we
have
V(aTy) = V(aTy −lT *β + lT *β)
= V(aTy −˜tTXTy + lT *β)
= V(aTy −˜tTXTy) + V(lT *β) + 2Cov(aTy −˜tTXTy, ˜tTXTy).
Now, under the assumptions on the variance-covariance matrix of ϵ, which is
also the (conditional, given X) variance-covariance matrix of y, we have
Cov(aTy −˜tTXTy, lT *β) = (aT −˜tTXT)σ2IX˜t
= (aTX −˜tTXTX)σ2I˜t
= (lT −lT)σ2I˜t
= 0;
that is,
V(aTy) = V(aTy −˜tTXTy) + V(lT *β).

414
9 Selected Applications in Statistics
This implies that
V(aTy) ≥V(lT *β);
that is, lT *β has minimum variance among the linear unbiased estimators of
lTβ. To see that it is unique, we consider the case in which V(aTy) = V(lT *β);
that is, V(aTy −˜tTXTy) = 0. For this variance to equal 0, it must be the case
that aT −˜tTXT = 0 or aTy = ˜tTXTy = lT *β; that is, lT *β is the unique linear
unbiased estimator that achieves the minimum variance.
If we assume further that ϵ ∼Nn(0, σ2I), we can show that lT *β is the
uniformly minimum variance unbiased estimator (UMVUE) for lTβ. This is
because (XTy, (y −X *β)T(y −X *β)) is complete and suﬃcient for (β, σ2).
This line of reasoning also implies that (y −X *β)T(y −X *β)/(n −r), where
r = rank(X), is UMVUE for σ2. We will not go through the details here. The
interested reader is referred to a text on mathematical statistics, such as Shao
(2003).
9.3.4 The Normal Equations and the Sweep Operator
The coeﬃcient matrix in the normal equations, XTX, or the adjusted version
XT
c Xc, where Xc is the centered matrix as in equation (8.64) on page 366, is
often of interest for reasons other than just to compute the least squares esti-
mators. The condition number of XTX is the square of the condition number
of X, however, and so any ill-conditioning is exacerbated by formation of the
sums of squares and cross products matrix. The adjusted sums of squares and
cross products matrix, XT
c Xc, tends to be better conditioned, so it is usually
the one used in the normal equations, but of course the condition number of
XT
c Xc is the square of the condition number of Xc.
A useful matrix can be formed from the normal equations:
!
XTX XTy
yTX yTy
"
.
(9.27)
Applying m elementary operations on this matrix, we can get
!
(XTX)+
X+y
yTX+T yTy −yTX(XTX)+XTy
"
.
(9.28)
(If X is not of full rank, in order to get the Moore-Penrose inverse in this
expression, the elementary operations must be applied in a ﬁxed manner;
otherwise, we get a diﬀerent generalized inverse.)
The matrix in the upper left of the partition (9.28) is related to the es-
timated variance-covariance matrix of the particular solution of the normal
equations, and it can be used to get an estimate of the variance-covariance
matrix of estimates of any independent set of linearly estimable functions of

9.3 Linear Models
415
β. The vector in the upper right of the partition is the unique minimum-
length solution to the normal equations, *β. The scalar in the lower right par-
tition, which is the Schur complement of the full inverse (see equations (3.190)
and (3.214)), is the square of the residual norm. The squared residual norm
provides an estimate of the variance of the errors in equation (9.11) after
proper scaling.
The partitioning in expression (9.28) is the same that we encountered on
page 363.
The elementary operations can be grouped into a larger operation, called
the “sweep operation”, which is performed for a given row. The sweep opera-
tion on row i, Si, of the nonnegative deﬁnite matrix A to yield the matrix B,
which we denote by
Si(A) = B,
is deﬁned in Algorithm 9.1.
Algorithm 9.1 Sweep of the ith row ’
1. If aii = 0, skip the following operations.
2. Set bii = a−1
ii .
3. For j ̸= i, set bij = a−1
ii aij.
4. For k ̸= i, set bkj = akj −akia−1
ii aij.
Skipping the operations if aii = 0 allows the sweep operator to handle
non-full rank problems. The sweep operator is its own inverse:
Si(Si(A)) = A.
The sweep operator applied to the matrix (9.27) corresponds to adding or
removing the ith variable (column) of the X matrix to the regression equation.
9.3.5 Linear Least Squares Subject to Linear
Equality Constraints
In the regression model (9.11), it may be known that β satisﬁes certain con-
straints, such as that all the elements be nonnegative. For constraints of the
form g(β) ∈C, where C is some m-dimensional space, we may estimate β by
the constrained least squares estimator; that is, the vector *βC that minimizes
the dot product (9.13) among all b that satisfy g(b) ∈C.
The nature of the constraints may or may not make drastic changes to the
computational problem. (The constraints also change the statistical inference
problem in various ways, but we do not address that here.) If the constraints
are nonlinear, or if the constraints are inequality constraints (such as that all
the elements be nonnegative), there is no general closed-form solution.
It is easy to handle linear equality constraints of the form
g(β) = Lβ
= c,

416
9 Selected Applications in Statistics
where L is a q × m matrix of full rank. The solution is, analogous to equa-
tion (9.15),
*βC = (XTX)+XTy + (XTX)+LT(L(XTX)+LT)+(c −L(XTX)+XTy).
(9.29)
When X is of full rank, this result can be derived by using Lagrange multipliers
and the derivative of the norm (9.13) (see Exercise 9.4 on page 452). When X
is not of full rank, it is slightly more diﬃcult to show this, but it is still true.
(See a text on linear regression, such as Draper and Smith 1998).
The restricted least squares estimate, *βC, can be obtained (in the (1, 2)
block) by performing m + q sweep operations on the matrix,
⎡
⎣
XTX XTy LT
yTX yTy cT
L
c
0
⎤
⎦,
(9.30)
analogous to matrix (9.27).
9.3.6 Weighted Least Squares
In ﬁtting the regression model y ≈Xβ, it is often desirable to weight the obser-
vations diﬀerently, and so instead of minimizing equation (9.13), we minimize

wi(yi −xT
i∗b)2,
where wi represents a nonnegative weight to be applied to the ith observation.
One purpose of the weight may be to control the eﬀect of a given observation
on the overall ﬁt. If a model of the form of equation (9.11),
y = Xβ + ϵ,
is assumed, and ϵ is taken to be a random variable such that ϵi has variance σ2
i ,
an appropriate value of wi may be 1/σ2
i . (Statisticians almost always naturally
assume that ϵ is a random variable. Although usually it is modeled this way,
here we are allowing for more general interpretations and more general motives
in ﬁtting the model.)
The normal equations can be written as

XTdiag((w1, w2, . . . , wn))X

*β = XTdiag((w1, w2, . . . , wn))y.
More generally, we can consider W to be a weight matrix that is not
necessarily diagonal. We have the same set of normal equations:
(XTWX)*βW = XTWy.
(9.31)
When W is a diagonal matrix, the problem is called “weighted least squares”.
Use of a nondiagonal W is also called weighted least squares but is sometimes

9.3 Linear Models
417
called “generalized least squares”. The weight matrix is symmetric and gen-
erally positive deﬁnite, or at least nonnegative deﬁnite. The weighted least
squares estimator is
*βW = (XTWX)+XTWy.
As we have mentioned many times, an expression such as this is not necessar-
ily a formula for computation. The matrix factorizations discussed above for
the unweighted case can also be used for computing weighted least squares
estimates.
In a model y = Xβ + ϵ, where ϵ is taken to be a random variable with
variance-covariance matrix Σ, the choice of W as Σ−1 yields estimators with
certain desirable statistical properties. (Because this is a natural choice for
many models, statisticians sometimes choose the weighting matrix without
fully considering the reasons for the choice.) As we pointed out on page 295,
weighted least squares can be handled by premultiplication of both y and
X by the Cholesky factor of the weight matrix. In the case of an assumed
variance-covariance matrix Σ, we transform each side by Σ−1
C , where ΣC is
the Cholesky factor of Σ. The residuals whose squares are to be minimized
are Σ−1
C (y −Xb). Under the assumptions, the variance-covariance matrix of
the residuals is I.
9.3.7 Updating Linear Regression Statistics
In Sect. 6.6.5 on page 295, we discussed the general problem of updating a
least squares solution to an overdetermined system when either the number
of equations (rows) or the number of variables (columns) is changed. In the
linear regression problem these correspond to adding or deleting observations
and adding or deleting terms in the linear model, respectively.
9.3.7.1 Adding More Variables
Suppose ﬁrst that more variables are added, so the regression model is
y ≈
'
X X+
(
θ,
where X+ represents the observations on the additional variables. (We use θ
to represent the parameter vector; because the model is diﬀerent, it is not just
β with some additional elements.)
If XTX has been formed and the sweep operator is being used to perform
the regression computations, it can be used easily to add or delete variables
from the model, as we mentioned above. The Sherman-Morrison-Woodbury
formulas (6.28) and (6.30) and the Hemes formula (6.31) (see page 288) can
also be used to update the solution.
In regression analysis, one of the most important questions is the identiﬁ-
cation of independent variables from a set of potential explanatory variables
that should be in the model. This aspect of the analysis involves adding and
deleting variables. We discuss this further in Sect. 9.5.2.

418
9 Selected Applications in Statistics
9.3.7.2 Adding More Observations
If we have obtained more observations, the regression model is
! y
y+
"
≈
! X
X+
"
β,
where y+ and X+ represent the additional observations.
We ﬁrst note some properties of the new XTX matrix, although we will
make direct use of the new X matrix, as usual. We see that
! X
X+
"T ! X
X+
"
= XTX + XT
+X+.
The relation of the inverse of XTX + XT
+X+ to the inverse of XTX can be
seen in equation (3.177) on page 119, or in equation (3.185) for the vector
corresponding to a single additional row.
If the QR decomposition of X is available, we simply augment it as in
equation (6.47):
⎡
⎣
R
c1
0
c2
X+ y+
⎤
⎦=
!QT 0
0 I
" ! X
y
X+ y+
"
.
We now apply orthogonal transformations to this to zero out the last rows
and produce
!
R∗c1∗
0 c2∗
"
,
where R∗is an m × m upper triangular matrix and c1∗is an m-vector as
before, but c2∗is an (n −m + k)-vector. We then have an equation of the
form (9.17) and we use back substitution to solve it.
9.3.7.3 Adding More Observations Using Weights
Another way of approaching the problem of adding or deleting observations
is by viewing the problem as weighted least squares. In this approach, we also
have more general results for updating regression statistics. Following Escobar
and Moser (1993), we can consider two weighted least squares problems: one
with weight matrix W and one with weight matrix V . Suppose we have the
solutions *βW and *βV . Now let
Δ = V −W,
and use the subscript ∗on any matrix or vector to denote the subarray that
corresponds only to the nonnull rows of Δ. The symbol Δ∗, for example, is
the square subarray of Δ consisting of all of the nonzero rows and columns of
Δ, and X∗is the subarray of X consisting of all the columns of X and only

9.3 Linear Models
419
the rows of X that correspond to Δ∗. From the normal equations (9.31) using
W and V , and with the solutions *βW and *βV plugged in, we have
(XTWX)*βV + (XTΔX)*βV = XTWy + XTΔy,
and so
*βV −*βW = (XTWX)+XT
∗Δ∗(y −X *βV )∗.
This gives
(y −X *βV )∗= (I + X(XTWX)+XT
∗Δ∗)+(y −X *βW )∗,
and ﬁnally
*βV = *βW + (XTWX)+XT
∗Δ∗

I + X∗(XTWX)+XT
∗Δ∗
+
(y −X *βW)∗.
If Δ∗can be written as ±GGT, using this equation and the equa-
tions (3.176) on page 119 (which also apply to pseudoinverses), we have
*βV = *βW ± (XTWX)+XT
∗G(I ± GTX∗(XTWX)+XT
∗G)+GT(y −X *βW )∗.
(9.32)
The sign of GGT is positive when observations are added and negative when
they are deleted.
Equation (9.32) is particularly simple in the case where W and V are
identity matrices (of diﬀerent sizes, of course). Suppose that we have ob-
tained more observations in y+ and X+. (In the following, the reader must
be careful to distinguish “+” as a subscript to represent more data and “+”
as a superscript with its usual meaning of a Moore-Penrose inverse.) Suppose
we already have the least squares solution for y ≈Xβ, say *βW . Now *βW is
the weighted least squares solution to the model with the additional data and
with weight matrix
W =
!I 0
0 0
"
.
We now seek the solution to the same system with weight matrix V , which is
a larger identity matrix. From equation (9.32), the solution is
*β = *βW + (XTX)+XT
+(I + X+(XTX)+XT
+)+(y −X *βW )∗.
(9.33)
9.3.8 Linear Smoothing
The interesting reasons for doing regression analysis are to understand rela-
tionships and to predict a value of the dependent value given a value of the

420
9 Selected Applications in Statistics
independent variable. As a side beneﬁt, a model with a smooth equation f(x)
“smoothes” the observed responses; that is, the elements in ˆy = 4
f(x) exhibit
less variation than the elements in y. Of course, the important fact for our
purposes is that ∥y −ˆy∥is smaller than ∥y∥or ∥y −¯y∥.
The use of the hat matrix emphasizes the smoothing perspective as a
projection of the original y:
ˆy = Hy.
The concept of a smoothing matrix was discussed in Sect. 8.6.2. From this
perspective, using H, we project y onto a vector in span(H), and that vector
has a smaller variation than y; that is, H has smoothed y. It does not matter
what the speciﬁc values in the vector y are so long as they are associated with
the same values of the independent variables.
We can extend this idea to a general n × n smoothing matrix Hλ:
˜y = Hλy.
The smoothing matrix depends only on the kind and extent of smoothing to
be performed and on the observed values of the independent variables. The
extent of the smoothing may be indicated by the indexing parameter λ. Once
the smoothing matrix is obtained, it does not matter how the independent
variables are related to the model.
In Sect. 6.7.2, we discussed regularized solutions of overdetermined systems
of equations, which in the present case is equivalent to solving
min
b

(y −Xb)T(y −Xb) + λbTb

.
The solution of this yields the smoothing matrix
Sλ = X(XTX + λI)−1XT,
as we have seen on page 364. This has the eﬀect of shrinking the *y toward 0.
(In regression analysis, this is called “ridge regression”.)
We discuss ridge regression and general shrinkage estimation in Sect. 9.5.4.
Loader (2012) provides additional background and discusses more general
issues in smoothing.
9.3.9 Multivariate Linear Models
A simple modiﬁcation of the model (9.10), yi = βTxi+ϵi, on page 404, extends
the scalar responses to vector responses; that is, yi is a vector, and of course,
the vector of parameters β must be replaced by a matrix. Let d be the order
of yi. Similarly, ϵi is a d-vector.
This is a “multivariate” linear model, meaning among other things, that
the error term has a multivariate distribution (it is not a set of i.i.d. scalars).

9.3 Linear Models
421
A major diﬀerence in the multivariate linear model arises from the struc-
ture of the vector ϵi. It may be appropriate to assume that the ϵis are indepen-
dent from one observation to another, but it is not likely that the individual
elements within an ϵi vector are independent from each other or even that
they have zero correlations. A reasonable assumption to complete the model
is that the vectors ϵis are independently and identically distributed with mean
0 and variance-covariance matrix Σ. It might be reasonable also to assume
that they have a normal distribution.
In statistical applications in which univariate responses are modeled, in-
stead of the model for a single observation, we are more likely to write the
model for a set of observations on y and x in the form of equation (9.11),
y = Xβ + ϵ, in which y and ϵ are d-vectors, X is a matrix in which the
rows correspond to the individual xi. Extending this form to the multivariate
model, we write
Y = XB + E,
(9.34)
where now Y is an n×d matrix, X is an n×m matrix as before, B is an m×d
matrix and E is an n × d matrix. Under the assumptions on the distribution
of the vector ϵi above, and including the assumption of normality, E in (9.34)
has a matrix normal distribution (see expression (4.78) on page 221):
E ∼Nn,d(0, I, Σ),
(9.35)
or in the form of (4.79),
vec

ET
∼Ndn (0, diag(Σ, . . . , Σ)) .
Note that the variance-covariance matrix in this distribution has Kronecker
structure, since diag(Σ, . . . , Σ) = I ⊗Σ (see also equation (3.102)).
9.3.9.1 Fitting the Model
Fitting multivariate linear models is done in the same way as ﬁtting univariate
linear models. The most common criterion for ﬁtting is least squares, which
as we have pointed out before is the same as a maximum likelihood criterion
if the errors are identically and independently normally distributed (which
follows from the identity matrix I in expression (9.35)). This is the same
ﬁtting problem that we considered in Sect. 9.3.1 in this chapter or, earlier in
Sect. 6.6 on page 289.
In an approach similar to the development in Sect. 6.6, for a given choice
of B, say B, we have, corresponding to equation (6.33),
X B = Y −R,
(9.36)
where R is an n × d matrix of residuals.
A least squares solution *B is one that minimizes the sum of squares of the
residuals (or, equivalently, the square root of the sum of the squares, that is,
∥R∥F). Hence, we have the optimization problem

422
9 Selected Applications in Statistics
min

B
Y −X B

F .
(9.37)
As in Sect. 6.6, we rewrite the square of this norm, using equation (3.291)
from page 168, as
tr

(Y −X B)T(Y −X B)

.
(9.38)
This is similar to equation (6.35), which, as before, we diﬀerentiate and set
equal to zero, getting the normal equations in the vector *B,
XTX *B = XTY.
(9.39)
(Exercise.)
We note that the columns of the matrices in these equations are each the
same as the univariate normal equations (6.36):
XTX[ *B∗1, . . . , *B∗d] = XT[Y∗1, . . . , Y∗d].
9.3.9.2 Partitioning the Sum of Squares
On page 363, we discussed the partitioning of the sum of squares of an ob-
served vector of data, yTy. We did this in the context of the Gramian of the
partitioned matrix [X y]. In the multivariate case, ﬁrst of all, instead of the
sum of squares yTy, we have the matrix of sums of squares and cross products,
Y TY . We now consider the Gramian matrix [X Y ]T[X Y ], and partition it as
in expression (9.27),
!XTX XTY
Y TX Y TY
"
.
(9.40)
From this we can get
!
(XTX)+
X+Y
Y TX+T
Y TY −Y TX(XTX)+XTY
"
.
(9.41)
Note that the term in the lower right side in this partitioning is the Schur
complement of XTX in [X Y ]T [X Y ] (see equation (3.191) on page 122). This
matrix of residual sums of squares and cross products provides a maximum
likelihood estimator of Σ:
*Σ =

Y TY −Y TX(XTX)+XTY

/n.
(9.42)
If the normalizing factor is 1/(n−d) instead of 1/n, the estimator is unbiased.
This partitioning breaks the matrix of total sums of squares and cross
products into a sum of a matrix of sums of squares and cross products due
to the ﬁtted relationship between Y and X and a matrix of residual sums
of squares and cross products. The analysis of these two matrices of sums

9.3 Linear Models
423
of squares and cross products is one of the most fundamental and important
techniques in multivariate statistics.
The matrix in the upper left of the partition (9.41) can be used to get an
estimate of the variance-covariance matrix of estimates of any independent
set of linearly estimable functions of B. The matrix in the upper right of
the partition is the solution to the normal equations, *B. The matrix in the
lower right partition, which is the Schur complement of the full inverse (see
equations (3.190) and (3.214)), is the matrix of sums of squares and cross
products of the residuals. With proper scaling, it provides an estimate of the
variance-covariance Σ of each row of E in equation (9.34).
9.3.9.3 Statistical Inference
Statistical inference for multivariate linear models is similar to what is de-
scribed in Sect. 9.3.3 with some obvious changes and extensions. First order
properties of distributions of the analogous statistics are almost the same. Sec-
ond order properties (variances), however, are rather diﬀerent. The solution of
the normal equations *B has a matrix normal distribution with expectation B.
The scaled matrix of sums of squares and cross products of the residuals, call
it *Σ, has a Wishart distribution with parameter Σ.
The basic null hypothesis of interest that the distribution of Y is not
dependent on X is essentially the same as in the univariate case. The F-test
in the corresponding univariate case, which is the ratio of two independent chi-
squared random variables, has an analogue in a comparison of two matrices
of sums of squares and cross products. In the multivariate case, the basis for
statistical inference is *Σ, and it can be used in various ways. The relevant
fact is that *Σ ∼Wd(Σ, n −d), that is, it has a Wishart distribution with
variance-covariance matrix Σ and parameters d and n −d (see Exercise 4.12
on page 224).
In hypothesis testing, depending on the null hypothesis, there are other
matrices that have Wishart distributions. There are various scalar transfor-
mations of Wishart matrices whose distributions are known (or which have
been approximated). One of the most common ones, and which even has some
of the ﬂavor of an F statistic, is Wilk’s Λ,
Λ =
det

*Σ

det

*Σ0
,
(9.43)
where *Σ0 is a scaled Wishart matrix yielding a maximum of the likelihood
under a null hypothesis. Other related test statistics involving *Σ are Pillai’s
trace, the Lawley-Hotelling trace (and Hotelling’s T 2), and Roy’s maximum
root. We will not discuss these here, and the interested reader is referred to a
text on multivariate analysis.

424
9 Selected Applications in Statistics
In multivariate analysis, there are other properties of the model that are
subject to statistical inference. For example, we may wish to estimate or test
the rank of the coeﬃcient matrix, B. Even in the case of a single multivariate
random variable, we may wish to test whether the variance-covariance matrix
is of full rank. If it is not, there are ﬁxed relationships among the elements of
the random variable, and the distribution is said to be singular. We will discuss
the problem of testing the rank of a matrix brieﬂy in Sect. 9.5.5, beginning on
page 433, but for more discussion on issues of statistical inference, we again
refer the reader to a text on multivariate statistical inference.
9.4 Principal Components
The analysis of multivariate data involves various linear transformations that
help in understanding the relationships among the features that the data
represent. The second moments of the data are used to accommodate the
diﬀerences in the scales of the individual variables and the covariances among
pairs of variables.
If X is the matrix containing the data stored in the usual way, a useful
statistic is the sums of squares and cross products matrix, XTX, or the “ad-
justed” squares and cross products matrix, XT
c Xc, where Xc is the centered
matrix formed by subtracting from each element of X the mean of the col-
umn containing that element. The sample variance-covariance matrix, as in
equation (8.67), is the Gramian matrix
SX =
1
n −1XT
c Xc,
(9.44)
where n is the number of observations (the number of rows in X).
In data analysis, the sample variance-covariance matrix SX in equa-
tion (9.44) plays an important role. In more formal statistical inference, it
is a consistent estimator of the population variance-covariance matrix (if it
is positive deﬁnite), and under assumptions of independent sampling from a
normal distribution, it has a known distribution. It also has important numer-
ical properties; it is symmetric and positive deﬁnite (or, at least, nonnegative
deﬁnite; see Sect. 8.6). Other estimates of the variance-covariance matrix or
the correlation matrix of the underlying distribution may not be positive def-
inite, however, and in Sect. 9.5.6 and Exercise 9.15 we describe possible ways
of adjusting a matrix to be positive deﬁnite.
9.4.1 Principal Components of a Random Vector
It is often of interest to transform a given random vector into a vector whose el-
ements are independent. We may also be interested in which of those elements
of the transformed random vector have the largest variances. The transformed

9.4 Principal Components
425
vector may be more useful in making inferences about the population. In more
informal data analysis, it may allow use of smaller observational vectors with-
out much loss in information.
Stating this more formally, if Y is a random d-vector with variance-
covariance matrix Σ, we seek a transformation matrix A such that Y = AY
has a diagonal variance-covariance matrix. We are additionally interested in
a transformation aTY that has maximal variance for a given ∥a∥.
Because the variance of aTY is V(aTY ) = aTΣa, we have already obtained
the solution in equation (3.265). The vector a is the eigenvector corresponding
to the maximum eigenvalue of Σ, and if a is normalized, the variance of aTY
is the maximum eigenvalue.
Because Σ is symmetric, it is orthogonally diagonalizable and the proper-
ties discussed in Sect. 3.8.10 on page 153 not only provide the transformation
immediately but also indicate which elements of Y have the largest variances.
We write the orthogonal diagonalization of Σ as (see equation (3.252))
Σ = ΓΛΓ T,
(9.45)
where ΓΓ T = Γ TΓ = I, and Λ is diagonal with elements λ1 ≥· · · ≥λm ≥0
(because a variance-covariance matrix is nonnegative deﬁnite). Choosing the
transformation as
Y = Γ TY,
(9.46)
we have V(Y ) = Λ; that is, the ith element of Y has variance λi, and
Cov(Yi, Yj) = 0
if i ̸= j.
The elements of Y are called the principal components of Y . The ﬁrst principal
component, Y1, which is the signed magnitude of the projection of Y in the
direction of the eigenvector corresponding to the maximum eigenvalue, has
the maximum variance of any of the elements of Y , and V(Y1) = λ1. (It is,
of course, possible that the maximum eigenvalue is not simple. In that case,
there is no one-dimensional ﬁrst principal component. If m1 is the multiplicity
of λ1, all one-dimensional projections within the m1-dimensional eigenspace
corresponding to λ1 have the same variance, and m1 projections can be chosen
as mutually independent.)
The second and third principal components, and so on, are likewise deter-
mined directly from the spectral decomposition.
9.4.2 Principal Components of Data
The same ideas of principal components in probability models carry over to
observational data. Given an n×d data matrix X, we seek a transformation as
above that will yield the linear combination of the columns that has maximum
sample variance, and other linear combinations that are independent. This
means that we work with the centered matrix Xc (equation (8.64)) and the

426
9 Selected Applications in Statistics
variance-covariance matrix SX, as above, or the centered and scaled matrix
Xcs (equation (8.65)) and the correlation matrix RX (equation (8.69)). See
Section 3.3 in Jolliﬀe (2002) for discussions of the diﬀerences in using the
centered but not scaled matrix and using the centered and scaled matrix.
In the following, we will use SX, which plays a role similar to Σ for the ran-
dom variable. (This role could be stated more formally in terms of statistical
estimation. Additionally, the scaling may require more careful consideration.
The issue of scaling naturally arises from the arbitrariness of units of mea-
surement in data. Random variables discussed in Sect. 9.4.1 have no units of
measurement.)
In data analysis, we seek a normalized transformation vector a to apply
to any centered observation xc, so that the sample variance of aTxc, that is,
aTSXa,
(9.47)
is maximized.
From equation (3.265) or the spectral decomposition equation (3.256), we
know that the solution to this maximization problem is the eigenvector, v1,
corresponding to the largest eigenvalue, c1, of SX, and the value of the ex-
pression (9.47); that is, vT
1 SXv1 at the maximum is the largest eigenvalue. In
applications, this vector is used to transform the rows of Xc into scalars. If
we think of a generic row of Xc as the vector x, we call vT
1 x the ﬁrst principal
component of x. There is some ambiguity about the precise meaning of “prin-
cipal component”. The deﬁnition just given is a scalar; that is, a combination
of values of a vector of variables. This is consistent with the deﬁnition that
arises in the population model in Sect. 9.4.1. Sometimes, however, the eigen-
vector v1 itself is referred to as the ﬁrst principal component. More often, the
vector Xcv1 of linear combinations of the columns of Xc is called the ﬁrst
principal component. We will often use the term in this latter sense.
If the largest eigenvalue, c1, is of algebraic multiplicity m1 > 1, we have
seen that we can choose m1 orthogonal eigenvectors that correspond to c1
(because SX, being symmetric, is simple). Any one of these vectors may be
called a ﬁrst principal component of X.
The second and third principal components, and so on, are likewise de-
termined directly from the nonzero eigenvalues in the spectral decomposition
of SX. Because the eigenvectors are orthogonal (or can be chosen to be), the
principal components have the property
zT
i SXzj = zT
i zj = 0,
for i ̸= j.
The full set of principal components of Xc, analogous to equation (9.46)
except that here the random vectors correspond to the rows in Xc, is
Z = XcV,
(9.48)
where V has rX columns. (As before, rX is the rank of X.). Figure 9.2 shows
two principal components, z1 and z2, formed from the data represented in x1
and x2.

9.4 Principal Components
427
9.4.2.1 Principal Components Directly from the Data Matrix
Formation of the SX matrix emphasizes the role that the sample covariances
play in principal component analysis. However, there is no reason to form
x1
x2
z1
z2
Figure 9.2. Principal components
a matrix such as XT
c Xc, and indeed we may introduce signiﬁcant rounding
errors by doing so. (Recall our previous discussions of the condition numbers
of XTX and X.)
The singular value decomposition of the n×m matrix Xc yields the square
roots of the eigenvalues of XT
c Xc and the same eigenvectors. (The eigenvalues
of XT
c Xc are (n −1) times the eigenvalues of SX.) We will assume that there
are more observations than variables (that is, that n > m). In the SVD of the
centered data matrix Xc = UAV T, U is an n × rX matrix with orthogonal
columns, V is an m × rX matrix whose ﬁrst rX columns are orthogonal and
the rest are 0, and A is an rX × rX diagonal matrix whose entries are the
nonnegative singular values of X−X. (As before, rX is the column rank of X.)
The spectral decomposition in terms of the singular values and outer prod-
ucts of the columns of the factor matrices is
Xc =
rX

i
σiuivT
i .
(9.49)
The vectors ui are the same as the eigenvectors of SX.

428
9 Selected Applications in Statistics
9.4.2.2 Dimension Reduction
If the columns of a data matrix X are viewed as variables or features that are
measured for each of several observational units, which correspond to rows
in the data matrix, an objective in principal components analysis may be to
determine some small number of linear combinations of the columns of X
that contain almost as much information as the full set of columns. (Here we
are not using “information” in a precise sense; in a general sense, it means
having similar statistical properties.) Instead of a space of dimension equal
to the (column) rank of X (that is, rX), we seek a subspace of span(X) with
rank less than rX that approximates the full space (in some sense). As we
discussed on page 176, the best approximation in terms of the usual norm
(the Frobenius norm) of Xc by a matrix of rank p is

Xp =
p

i
σiuivT
i
(9.50)
for some p < min(n, m).
Principal components analysis is often used for “dimension reduction” by
using the ﬁrst few principal components in place of the original data. There
are various ways of choosing the number of principal components (that is, p
in equation (9.50)). There are also other approaches to dimension reduction.
A general reference on this topic is Mizuta (2012).
9.5 Condition of Models and Data
In Sect. 6.1, we describe the concept of “condition” of a matrix for certain
kinds of computations. In Sect. 6.1.3, we discuss how a large condition num-
ber may indicate the level of numerical accuracy in the solution of a system
of linear equations, and on page 292 we extend this discussion to overdeter-
mined systems such as those encountered in regression analysis. (We return
to the topic of condition in Sect. 11.2 with even more emphasis on the numer-
ical computations.) The condition of the X matrices has implications for the
accuracy we can expect in the numerical computations for regression analysis.
There are other connections between the condition of the data and statis-
tical analysis that go beyond just the purely computational issues. Analysis
involves more than just computations. Ill-conditioned data also make inter-
pretation of relationships diﬃcult because we may be concerned with both
conditional and marginal relationships. In ill-conditioned data, the relation-
ships between any two variables may be quite diﬀerent depending on whether
or not the relationships are conditioned on relationships with other variables
in the dataset.

9.5 Condition of Models and Data
429
9.5.1 Ill-Conditioning in Statistical Applications
We have described ill-conditioning heuristically as a situation in which small
changes in the input data may result in large changes in the solution. Ill-
conditioning in statistical modeling is often the result of high correlations
among the independent variables. When such correlations exist, the compu-
tations may be subject to severe rounding error. This was a problem in using
computer software many years ago, as Longley (1967) pointed out. When there
are large correlations among the independent variables, the model itself must
be examined, as Beaton, Rubin, and Barone (1976) emphasize in reviewing
the analysis performed by Longley. Although the work of Beaton, Rubin, and
Barone was criticized for not paying proper respect to high-accuracy compu-
tations, ultimately it is the utility of the ﬁtted model that counts, not the
accuracy of the computations.
Large correlations are reﬂected in the condition number of the X matrix.
A large condition number may indicate the possibility of harmful numerical
errors. Some of the techniques for assessing the accuracy of a computed result
may be useful. In particular, the analyst may try the suggestion of Mullet and
Murray (1971) to regress y + dxj on x1, . . . , xm, and compare the results with
the results obtained from just using y.
Other types of ill-conditioning may be more subtle. Large variations in the
leverages may be the cause of ill-conditioning.
Often, numerical problems in regression computations indicate that the
linear model may not be entirely satisfactory for the phenomenon being stud-
ied. Ill-conditioning in statistical data analysis often means that the approach
or the model is not appropriate.
9.5.2 Variable Selection
Starting with a model such as equation (9.9),
Y = βTx + E,
we are ignoring the most fundamental problem in data analysis: which vari-
ables are really related to Y , and how are they related?
We often begin with the premise that a linear relationship is at least a good
approximation locally; that is, with restricted ranges of the variables. This
leaves us with one of the most important tasks in linear regression analysis:
selection of the variables to include in the model. There are many statistical
issues that must be taken into consideration. We will not discuss these issues
here; rather we refer the reader to a comprehensive text on regression analy-
sis, such as Draper and Smith (1998), or to a text speciﬁcally on this topic,
such as Miller (2002). Some aspects of the statistical analysis involve tests of
linear hypotheses, such as discussed in Sect. 9.3.3. There is a major diﬀerence,
however; those tests were based on knowledge of the correct model. The basic

430
9 Selected Applications in Statistics
problem in variable selection is that we do not know the correct model. Most
reasonable procedures to determine the correct model yield biased statistics.
Some people attempt to circumvent this problem by recasting the problem in
terms of a “full” model; that is, one that includes all independent variables
that the data analyst has looked at. (Looking at a variable and then making
a decision to exclude that variable from the model can bias further analyses.)
We generally approach the variable selection problem by writing the model
with the data as
y = Xiβi + Xoβo + ϵ,
(9.51)
where Xi and Xo are matrices that form some permutation of the columns of
X, Xi|Xo = X, and βi and βo are vectors consisting of corresponding elements
from β. (The i and o are “in” and “out”.) We then consider the model
y = Xiβi + ϵi.
(9.52)
It is interesting to note that the least squares estimate of βi in the
model (9.52) is the same as the least squares estimate in the model
ˆyio = Xiβi + ϵi,
where ˆyio is the vector of predicted values obtained by ﬁtting the full
model (9.51). An interpretation of this fact is that ﬁtting the model (9.52)
that includes only a subset of the variables is the same as using that subset
to approximate the predictions of the full model. The fact itself can be seen
from the normal equations associated with these two models. We have
XT
i X(XTX)−1XT = XT
i .
(9.53)
This follows from the fact that X(XTX)−1XT is a projection matrix, and Xi
consists of a set of columns of X (see Sect. 8.5 and Exercise 9.12 on page 455).
As mentioned above, there are many diﬃcult statistical issues in the vari-
able selection problem. The exact methods of statistical inference generally
do not apply (because they are based on a model, and we are trying to choose
a model). In variable selection, as in any statistical analysis that involves the
choice of a model, the eﬀect of the given dataset may be greater than war-
ranted, resulting in overﬁtting. One way of dealing with this kind of problem is
to use part of the dataset for ﬁtting and part for validation of the ﬁt. There are
many variations on exactly how to do this, but in general, “cross validation”
is an important part of any analysis that involves building a model.
The computations involved in variable selection are the same as those
discussed in Sects. 9.3.3 and 9.3.7.
9.5.3 Principal Components Regression
A somewhat diﬀerent approach to the problem of variable selection involves
selecting some linear combinations of all of the variables. The ﬁrst p princi-
pal components of X cover the space of span(X) optimally (in some sense),

9.5 Condition of Models and Data
431
and so these linear combinations themselves may be considered as the “best”
variables to include in a regression model. If Vp is the ﬁrst p columns from
V in the full set of principal components of X, equation (9.48), we use the
regression model
y ≈Zpγ,
(9.54)
where
Zp = XVp.
(9.55)
This is the idea of principal components regression.
In principal components regression, even if p < m (which is the case, of
course; otherwise principal components regression would make no sense), all
of the original variables are included in the model. Any linear combination
forming a principal component may include all of the original variables. The
weighting on the original variables tends to be such that the coeﬃcients of
the original variables that have extreme values in the ordinary least squares
regression are attenuated in the principal components regression using only
the ﬁrst p principal components.
The principal components do not involve y, so it may not be obvious that a
model using only a set of principal components selected without reference to y
would yield a useful regression model. Indeed, sometimes important indepen-
dent variables do not get suﬃcient weight in principal components regression.
9.5.4 Shrinkage Estimation
As mentioned in the previous section, instead of selecting speciﬁc independent
variables to include in the regression model, we may take the approach of
shrinking the coeﬃcient estimates toward zero. This of course has the eﬀect of
introducing a bias into the estimates (in the case of a true model being used),
but in the process of reducing the inherent instability due to collinearity in
the independent variables, it may also reduce the mean squared error of linear
combinations of the coeﬃcient estimates. This is one approach to the problem
of overﬁtting.
The shrinkage can also be accomplished by a regularization of the ﬁtting
criterion. If the ﬁtting criterion is minimization of a norm of the residuals, we
add a norm of the coeﬃcient estimates to minimize
∥r(b)∥f + λ∥b∥b,
(9.56)
where λ is a tuning parameter that allows control over the relative weight
given to the two components of the objective function. This regularization is
also related to the variable selection problem by the association of superﬂuous
variables with the individual elements of the optimal b that are close to zero.

432
9 Selected Applications in Statistics
9.5.4.1 Ridge Regression
If the ﬁtting criterion is least squares, we may also choose an L2 norm on b,
and we have the ﬁtting problem
min
b

(y −Xb)T(y −Xb) + λbTb

.
(9.57)
This is called Tikhonov regularization (from A. N. Tikhonov), and it is by far
the most commonly used regularization. This minimization problem yields the
modiﬁed normal equations
(XTX + λI)b = XTy,
(9.58)
obtained by adding λI to the sums of squares and cross products matrix. This
is the ridge regression we discussed on page 364, and as we saw in Sect. 6.1, the
addition of this positive deﬁnite matrix has the eﬀect of reducing numerical
ill-conditioning.
Interestingly, these normal equations correspond to a least squares approx-
imation for
⎛
⎝
y
0
⎞
⎠≈
⎡
⎣
X
√
λI
⎤
⎦β.
(9.59)
(See Exercise 9.11.) The shrinkage toward 0 is evident in this formulation.
Because of this, we say the “eﬀective” degrees of freedom of a ridge regression
model decreases with increasing λ. In equation (8.61), we formally deﬁned the
eﬀective model degrees of freedom of any linear ﬁt
*y = Sλy
as
tr(Sλ),
and we saw in equation (8.62) that indeed it does decrease with increasing λ.
Even if all variables are left in the model, the ridge regression approach
may alleviate some of the deleterious eﬀects of collinearity in the independent
variables.
9.5.4.2 Lasso Regression
The norm for the regularization in expression (9.56) does not have to be
the same as the norm applied to the model residuals. An alternative ﬁtting
criterion, for example, is to use an L1 norm,
min
b (y −Xb)T(y −Xb) + λ∥b∥1.

9.5 Condition of Models and Data
433
Rather than strictly minimizing this expression, we can formulate a con-
strained optimization problem
min
∥b∥1<t(y −Xb)T(y −Xb),
(9.60)
for some tuning constant t. The solution of this quadratic programming prob-
lem yields a b with some elements identically 0, depending on t. As t de-
creases, more elements of the optimal b are identically 0, and thus this is an
eﬀective method for variable selection. The use of expression (9.60) is called
lasso regression. (“Lasso” stands for “least absolute shrinkage and selection
operator”.)
Lasso regression is computationally expensive if several values of t are
explored. Efron et al. (2004)
propose “least angle regression” (LAR), the
steps of which eﬀectively yield the entire lasso regularization path.
9.5.5 Statistical Inference about the Rank of a Matrix
An interesting problem in numerical linear algebra is to approximate the rank
of a given matrix. A related problem in statistical inference is to estimate or to
test an hypothesis concerning the rank of an unknown matrix. For example,
in the multivariate regression model discussed in Sect. 9.3.9 (beginning on
page 420), we may wish to test whether the coeﬃcient matrix B is of full
rank.
In statistical inference, we use observed data to make inferences about a
model, but we do not “estimate” or “test an hypothesis” concerning the rank
of a given matrix of data.
9.5.5.1 Numerical Approximation and Statistical Inference
The rank of a matrix is not a continuous function of the elements of the matrix.
It is often diﬃcult to compute the rank of a given matrix; hence, we often seek
to approximate the rank. We alluded to the problem of approximating the rank
of a matrix on page 252, and indicated that a QR factorization of the given
matrix might be an appropriate approach to the problem. (In Sect. 11.4, we
discuss the rank-revealing QR (or LU) method for approximating the rank of
a matrix.)
The SVD can also be used to approximate the rank of a given n × m
matrix. The approximation would be based on a decision that either the rank
is min(n, m), or that the rank is r because di = 0 for i > r in the decomposition
UDV T given in equation (3.276) on page 161.
Although we sometimes refer to the problem as one of “estimating the
rank of a matrix”, “estimation” in the numerical-analytical sense refers to
“approximation”, rather than to statistical estimation. This is an important
distinction that is often lost. Estimation and testing in a statistical sense do
not apply to a given entity; these methods of inference apply to properties

434
9 Selected Applications in Statistics
of a random variable. We use observed realizations of the random variable to
make inferences about unobserved properties or parameters that describe the
distribution of the random variable.
A statistical test is a decision rule for rejection of an hypothesis about
which empirical evidence is available. The empirical evidence consists of ob-
servations on some random variable, and the hypothesis is a statement about
the distribution of the random variable. In simple cases of hypothesis testing,
the distribution is assumed to be characterized by a parameter, and the hy-
pothesis merely speciﬁes the value of that parameter. The statistical test is
based on the distribution of the underlying random variable if the hypothesis
is true.
9.5.5.2 Statistical Tests of the Rank of a Class of Matrices
Most common statistical tests involve hypotheses concerning a scalar param-
eter. We have encountered two examples that involve tests of hypotheses con-
cerning matrix parameters. One involved tests of the variance-covariance ma-
trix Σ in a multivariate distribution (Exercise 4.12 on page 224), and the
other was for tests of the coeﬃcient matrix B in multivariate linear regres-
sion (see page 423). The tests of the variance-covariance matrix are based
on a Wishart matrix W, but for a speciﬁc hypothesis, the test statistic is
a chi-squared statistic. In the multivariate linear regression testing problem,
the least-squares estimator of the coeﬃcient matrix, which has a matrix nor-
mal distribution, is used to form two matrices that have independent Wishart
distributions. The hypotheses of interest are that certain elements of the coef-
ﬁcient matrix are zero, and the test statistics involve functions of the Wishart
matrices, such as Wilk’s Λ, which is the ratio of the determinants.
In multivariate linear regression, given n observations on the vectors y and
x, we use the model for the data given in equation (9.34), on page 421,
Y = XB + E,
where Y is an n× d matrix and X is an n× m matrix of observations, B is an
m × d unknown matrix, E is an n × d matrix of n unobserved realizations of
a d-variate random variable. The canonical problem in statistical applications
is to test whether B = 0, that is, whether there is any linear relationship
between y and x. A related but less encompassing question is whether B is of
full rank. (If B = 0, its rank is zero.) Testing whether B is of full rank is similar
to the familiar univariate statistical problem of testing if some elements of β
in the model y = xTβ + ϵ are zero. In the multivariate case, this is sometimes
referred to as the “reduced rank regression” problem. The null hypothesis of
interest is
H0 : rank(B) ≤min(m, d) −1.
One approach is to test sequentially the null hypotheses H0i : rank(B) = i
for i = 1, . . . min(m, d) −1.

9.5 Condition of Models and Data
435
The other problem referred to above is, given n d-vectors y1, . . . yn assumed
to be independent realizations of random vectors distributed as Nd(μ, Σ),
to test whether Σ is of full rank (that is, whether the multivariate normal
distribution is singular; see page 219).
In other applications, in vector stochastic processes, the matrix of interest
is one that speciﬁes the relationship of one time series to another. In such
applications the issue of stationarity is important, and may be one of the
reasons for performing the rank test.
The appropriate statistical models in these settings are diﬀerent, and the
forms of the models in the diﬀerent applications aﬀect the distributions of any
test statistics. The nature of the subject of the hypothesis, that is, the rank of
a matrix, poses some diﬃculty. Much of the statistical theory on hypothesis
testing involves an open parameter space over a dense set of reals, but of course
the rank is an integer. Because of this, even if for no other reason, we would
not expect to be able to work out an exact distribution of any estimator or
test statistic for the rank. At best we would seek an estimator or test statistic
for which we could derive, or at least approximate, an asymptotic distribution.
Problems of testing the rank of a matrix have been addressed in the sta-
tistical literature for some time; see, for example, Anderson (1951), Gill and
Lewbel (1992), and Cragg and Donald (1996). They have also been discussed
frequently in econometric applications; see, for example, Robin and Smith
(2000) and Kleibergen and Paap (2006). In some of the literature, it is not
clear whether or not the authors are describing a test for the rank of a given
matrix, which, as pointed out above, is not a statistical procedure, even if a
“test statistic” and a “null probability distribution” are involved.
My purpose in this section is not to review the various approaches to
statistical inference about the rank of matrices or to discuss the “best” tests
under various scenarios, but rather to describe one test in order to give the
ﬂavor of the approaches.
9.5.5.3 Statistical Tests of the Rank Based on an LDU
Factorization
Gill and Lewbel (1992) and Cragg and Donald (1996) describe tests of the
rank of a matrix that use factors from an LDU factorization. For an m × d
matrix Θ, the tests are of the null hypothesis H0 :
rank(Θ) = r, where
r < min(m, d). (There are various ways an alternative hypothesis could be
phrased, but we will not specify one here.)
We ﬁrst decompose the unknown matrix Θ as in equation (5.32), using
permutation matrices so that the diagonal elements of D are nonincreasing in
absolute value: E(π1)ΘE(π2) = LDU.
The m × d matrix Θ (with m ≥d without loss of generality) can be
decomposed as

436
9 Selected Applications in Statistics
E(π1)ΘE(π2) = LDU
=
⎡
⎣
L11
0
0
L21 L22
0
L31 L32 Im−d
⎤
⎦
⎡
⎣
D1 0 0
0 D2 0
0
0 0
⎤
⎦
⎡
⎣
U11 U12
0
U22
0
0
⎤
⎦,
(9.61)
where the unknown matrices L11, U11, and D1 are r × r, and the elements of
the diagonal submatrices D1 and D2 are arranged in nonincreasing order. If
the rank of Θ is r, D2 = 0, but no diagonal element of D1 is 0.
For a statistical test of the rank of Θ, we need to identify an observable
random variable (random vector) whose distribution depends on Θ. To pro-
ceed, we take a sample of realizations of this random variable. We let *Θ be
an estimate of Θ based on n such realizations, and assume the central limit
property,
√
k vec( *Θ −Θ) →d N(0, V ),
(9.62)
where V is nm×nm and positive deﬁnite. (For example, if B is the coeﬃcient
matrix in the multivariate linear regression model (9.34) and *B is the least-
squares estimator from expression (9.37), then *B and B have this property. If
Σ is the variance-covariance matrix in the multivariate normal distribution,
expression (4.74), then we use the sample variance-covariance matrix, equa-
tion (8.67), page 367; but in this case, the analogous asymptotic distribution
relating *Σ and Σ is a Wishart distribution.)
Now if D2 = 0 (that is, if Θ has rank r) and *Θ is decomposed in the same
way as Θ in equation (9.61), then
√
k diag( *D2) →d N(0, W)
for some positive deﬁnite matrix W, and the quantity
n*dT
2 W −1 *d2,
(9.63)
where
*d2 = diag( *D2),
has an asymptotic chi-squared distribution with (m −r) degrees of freedom.
If a consistent and independent estimator of W, say 5
W, is used in place of W
in the expression (9.63), this would be a test statistic for the hypothesis that
the rank of Θ is r. (Note that W is m −r × m −r.)
Gill and Lewbel (1992) derive a consistent estimator to use in expres-
sion (9.63) as a test statistic. Following their derivation, ﬁrst let *V be a con-
sistent estimator of V . (It would typically be a sample variance-covariance
matrix.) Then

*QT ⊗*P

*V

*Q ⊗*P T
is a consistent estimator of the variance-covariance of vec( *P( *Θ −Θ) *Q). Next,
deﬁne the matrices

9.5 Condition of Models and Data
437
*H =
2
−*L−1
22 *L21*L−1
11
 *L−1
22
 0
3
,
*K =
6
−*U −1
11 *U12 *U −1
22
*U −1
22
7
,
and T such that
vec( *D2) = T *d2.
The matrix T is (m −r)2 × (m −r), consisting of a stack of square matrices
with 0s in all positions except for a 1 in one diagonal element. The matrix is
orthogonal; that is,
T TT = Im−r.
The matrix
( *K ⊗*HT)T
transforms vec( *P( *Θ −Θ) *Q) into *d2; hence the variance-covariance estimator,
( *QT ⊗*P)*V ( *Q ⊗*P T), is adjusted by this matrix. The estimator 5
W therefore
is given by
5
W = T T( *KT ⊗*H)( *QT ⊗*P)*V ( *Q ⊗*P T)( *
K ⊗*HT)T.
The test statistic is
n*dT
2 5
W −1 *d2,
(9.64)
with an approximate chi-squared distribution with (m−r) degrees of freedom.
Cragg and Donald (1996), however, have pointed out that the indetermi-
nacy of the LDU decomposition casts doubts on the central limiting distribu-
tion in (9.62). Kleibergen and Paap (2006) proposed a related test for a certain
class of matrices based on the SVD. Because the SVD is unique (within the
limitations mentioned on page 163), it does not suﬀer from the indeterminacy.
9.5.6 Incomplete Data
Missing values in a dataset can not only result in ill-conditioned problems but
can cause some matrix statistics to lack their standard properties, such as
covariance or correlation matrices formed from the available data not being
positive deﬁnite.
In the standard ﬂat data ﬁle represented in Fig. 8.1, where a row holds
data from a given observation and a column represents a speciﬁc variable or
feature, it is often the case that some values are missing for some observa-
tion/variable combination. This can occur for various reasons, such as a failure
of a measuring device, refusal to answer a question in a survey, or an inde-
terminate or inﬁnite value for a derived variable (for example, a coeﬃcient of

438
9 Selected Applications in Statistics
variation when the mean is 0). This causes problems for our standard storage
of data in a matrix. The values for some cells are not available.
The need to make provisions for missing data is one of the important
diﬀerences between statistical numerical processing and ordinary numerical
analysis. First of all, we need a method for representing a “not available”
(NA) value, and then we need a mechanism for avoiding computations with
this NA value. There are various ways of doing this, including the use of
special computer numbers (see pages 464 and 475).
The layout of the data may be of the form
X =
⎡
⎢⎢⎣
X X NA
X NA NA
X NA X
X X
X
⎤
⎥⎥⎦.
(9.65)
In the data matrix of equation (9.65), all rows could be used for summary
statistics relating to the ﬁrst variable, but only two rows could be used for
summary statistics relating to the second and third variables. For summary
statistics such as the mean or variance for any one variable, it would seem to
make sense to use all of the available data.
The picture is not so clear, however, for statistics on two variables, such as
the covariance. If all observations that contain data on both variables are used
for computing the covariance, then the covariance matrix may not be positive
deﬁnite. If the correlation matrix is computed using covariances computed in
this way but variances computed on all of the data, some oﬀ-diagonal elements
may be larger than 1. If the correlation matrix is computed using covariances
from all available pairs and variances computed only from the data in complete
pairs (that is, the variances used in computing correlations involving a given
variable are diﬀerent for diﬀerent variables), then no oﬀ-diagonal element can
be larger than 1, but the correlation matrix may not be nonnegative deﬁnite.
An alternative, of course, is to use only data in records that are complete.
This is called “casewise deletion”, whereas use of all available data for bivariate
statistics is called “pairwise deletion”. One must be very careful in computing
bivariate statistics from data with missing values; see Exercise 9.14 (and a
solution on page 612).
Estimated or approximate variance-covariance or correlation matrices that
are not positive deﬁnite can arise in other ways in applications. For example,
the data analyst may have an estimate of the correlation matrix that was not
based on a single sample.
Various approaches to handling an approximate correlation matrix that
is not positive deﬁnite have been considered. Devlin et al. (1975) describe a
method of shrinking the given R toward a chosen positive deﬁnite matrix, R1,
which may be an estimator of a correlation matrix computed in other ways
(perhaps a robust estimator) or may just be chosen arbitrarily; for example,
R1 may just be the identity matrix. The method is to choose the largest value
α in [0, 1] such that the matrix

9.5 Condition of Models and Data
439
R = αR + (1 −α)R1
(9.66)
is positive deﬁnite. This optimization problem can be solved iteratively start-
ing with α = 1 and decreasing α in small steps while checking whether R is
positive deﬁnite. (The checks may require several computations.) A related
method is to use a modiﬁed Cholesky decomposition. If the symmetric matrix
S is not positive deﬁnite, a diagonal matrix D can be determined so that
S + D is positive deﬁnite. Eskow and Schnabel (1991), for example, describe
one way to determine D with values near zero and to compute a Cholesky
decomposition of S + D.
Devlin, Gnanadesikan, and Kettenring (1975) also describe nonlinear
shrinking methods in which all of the oﬀ-diagonal elements rij are replaced
iteratively, beginning with r(0)
ij = rij and proceeding with
r(k)
ij
=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
f −1 
f

r(k−1)
ij

+ δ

if r(k−1)
ij
< −f −1(δ)
0
if
r(k−1)
ij
 ≤f −1(δ)
f −1 
f

r(k−1)
ij

−δ

if r(k−1)
ij
> f −1(δ)
(9.67)
for some invertible positive-valued function f and some small positive con-
stant δ (for example, 0.05). The function f may be chosen in various ways;
one suggested function is the hyperbolic tangent, which makes f −1 Fisher’s
variance-stabilizing function for a correlation coeﬃcient; see Exercise 9.19b.
Rousseeuw and Molenberghs (1993) suggest a method in which some ap-
proximate correlation matrices can be adjusted to a nearby correlation matrix,
where closeness is determined by the Frobenius norm. Their method applies to
pseudo-correlation matrices. Recall that any symmetric nonnegative deﬁnite
matrix with ones on the diagonal is a correlation matrix. A pseudo-correlation
matrix is a symmetric matrix R with positive diagonal elements (but not nec-
essarily 1s) and such that r2
ij ≤riirjj. (This is inequality (8.12), which is a
necessary but not suﬃcient condition for the matrix to be nonnegative deﬁ-
nite.)
The method of Rousseeuw and Molenberghs adjusts an m × m pseudo-
correlation matrix R to the closest correlation matrix R, where closeness is
determined by the Frobenius norm; that is, we seek R such that
∥R −R∥F
(9.68)
is minimum over all choices of R that are correlation matrices (that is, ma-
trices with 1s on the diagonal that are positive deﬁnite). The solution to this
optimization problem is not as easy as the solution to the problem we consider
on page 176 of ﬁnding the best approximate matrix of a given rank. Rousseeuw
and Molenberghs describe a computational method for ﬁnding R to minimize

440
9 Selected Applications in Statistics
expression (9.68). A correlation matrix R can be formed as a Gramian matrix
formed from a matrix U whose columns, u1, . . . , um, are normalized vectors,
where
˜rij = uT
i uj.
If we choose the vector ui so that only the ﬁrst i elements are nonzero, then
they form the Cholesky factor elements of R with nonnegative diagonal ele-
ments,
R = U TU,
and each ui can be completely represented in IRi. We can associate the m(m−
1)/2 unknown elements of U with the angles in their spherical coordinates. In
ui, the jth element is 0 if j > i and otherwise is
sin(θi1) · · · sin(θi,i−j) cos(θi,i−j+1),
where θi1, . . . , θi,i−j, θi,i−j+1 are the unknown angles that are the variables in
the optimization problem for the Frobenius norm (9.68). The problem now is
to solve
min
m

i=1
i

j=1
(rij −sin(θi1) · · · sin(θi,i−j) cos(θi,i−j+1))2.
(9.69)
This optimization problem is well-behaved and can be solved by steepest
descent (see page 201). Rousseeuw and Molenberghs (1993) also mention that
a weighted least squares problem in place of equation (9.69) may be more
appropriate if the elements of the pseudo-correlation matrix R result from
diﬀerent numbers of observations.
In Exercise 9.15, we describe another way of converting an approximate
correlation matrix that is not positive deﬁnite into a correlation matrix by
iteratively replacing negative eigenvalues with positive ones.
9.6 Optimal Design
When an experiment is designed to explore the eﬀects of some variables (usu-
ally called “factors”) on another variable, the settings of the factors (inde-
pendent variables) should be determined so as to yield a maximum amount
of information from a given number of observations. The basic problem is to
determine from a set of candidates the best rows for the data matrix X. For
example, if there are six factors and each can be set at three diﬀerent levels,
there is a total of 36 = 729 combinations of settings. In many cases, because
of the expense in conducting the experiment, only a relatively small number
of runs can be made. If, in the case of the 729 possible combinations, only 30
or so runs can be made, the scientist must choose the subset of combinations
that will be most informative. A row in X may contain more elements than

9.6 Optimal Design
441
just the number of factors (because of interactions), but the factor settings
completely determine the row.
We may quantify the information in terms of variances of the estimators.
If we assume a linear relationship expressed by
y = β01 + Xβ + ϵ
and make certain assumptions about the probability distribution of the resid-
uals, the variance-covariance matrix of estimable linear functions of the least
squares solution (9.15) is formed from
(XTX)−σ2.
(The assumptions are that the residuals are independently distributed with
a constant variance, σ2. We will not dwell on the statistical properties here,
however.) If the emphasis is on estimation of β, then X should be of full rank.
In the following, we assume X is of full rank; that is, that (XTX)−1 exists.
An objective is to minimize the variances of estimators of linear combina-
tions of the elements of β. We may identify three types of relevant measures of
the variance of the estimator *β: the average variance of the elements of *β, the
maximum variance of any elements, and the “generalized variance” of the vec-
tor *β. The property of the design resulting from maximizing the information
by reducing these measures of variance is called, respectively, A-optimality,
E-optimality, and D-optimality. They are achieved when X is chosen as fol-
lows:
•
A-optimality: minimize tr((XTX)−1).
•
E-optimality: minimize ρ((XTX)−1).
•
D-optimality: minimize det((XTX)−1).
Using the properties of eigenvalues and determinants that we discussed in
Chap. 3, we see that E-optimality is achieved by maximizing ρ(XTX) and
D-optimality is achieved by maximizing det(XTX).
9.6.1 D-Optimal Designs
The D-optimal criterion is probably used most often. If the residuals have a
normal distribution (and the other distributional assumptions are satisﬁed),
the D-optimal design results in the smallest volume of conﬁdence ellipsoids
for β.
(See Titterington 1975; Nguyen and Miller 1992; and Atkinson and
Donev 1992. Identiﬁcation of the D-optimal design is related to determina-
tion of a minimum-volume ellipsoid for multivariate data.) The computations
required for the D-optimal criterion are the simplest, and this may be another
reason it is used often.
To construct an optimal X with a given number of rows, n, from a set of
N potential rows, one usually begins with an initial choice of rows, perhaps
random, and then determines the eﬀect on the determinant by exchanging a

442
9 Selected Applications in Statistics
selected row with a diﬀerent row from the set of potential rows. If the matrix
X has n rows and the row vector xT is appended, the determinant of interest is
det(XTX + xxT)
or its inverse. Using the relationship det(AB) = det(A) det(B), it is easy to
see that
det(XTX + xxT) = det(XTX)(1 + xT(XTX)−1x).
(9.70)
Now, if a row xT
+ is exchanged for the row xT
−, the eﬀect on the determinant
is given by
det(XTX + x+xT
+ −x−xT
−) = det(XTX) ×

1 + xT
+(XTX)−1x+ −
xT
−(XTX)−1x−(1 + xT
+(XTX)−1x+) +
(xT
+(XTX)−1x−)2 
(9.71)
(see Exercise 9.8).
Following Miller and Nguyen (1994), writing XTX as RTR from the QR
decomposition of X, and introducing z+ and z−as
Rz+ = x+
and
Rz−= x−,
we have the right-hand side of equation (9.71):
zT
+z+ −zT
−z−(1 + zT
+z+) + (zT
−z+)2.
(9.72)
Even though there are n(N −n) possible pairs (x+, x−) to consider for ex-
changing, various quantities in (9.72) need be computed only once. The corre-
sponding (z+, z−) are obtained by back substitution using the triangular ma-
trix R. Miller and Nguyen use the Cauchy-Schwarz inequality (2.26) (page 24)
to show that the quantity (9.72) can be no larger than
zT
+z+ −zT
−z−;
(9.73)
hence, when considering a pair (x+, x−) for exchanging, if the quantity (9.73)
is smaller than the largest value of (9.72) found so far, then the full compu-
tation of (9.72) can be skipped. Miller and Nguyen also suggest not allowing
the last point added to the design to be considered for removal in the next
iteration and not allowing the last point removed to be added in the next
iteration.

9.7 Multivariate Random Number Generation
443
The procedure begins with an initial selection of design points, yielding
the n × m matrix X(0) that is of full rank. At the kth step, each row of X(k)
is considered for exchange with a candidate point, subject to the restrictions
mentioned above. Equations (9.72) and (9.73) are used to determine the best
exchange. If no point is found to improve the determinant, the process termi-
nates. Otherwise, when the optimal exchange is determined, R(k+1) is formed
using the updating methods discussed in the previous sections. (The programs
of Gentleman 1974, referred to in Sect. 6.6.5 can be used.)
9.7 Multivariate Random Number Generation
The need to simulate realizations of random variables arises often in statistical
applications, both in the development of statistical theory and in applied
data analysis. In this section, we will illustrate only a couple of problems
in multivariate random number generation. These make use of some of the
properties we have discussed previously.
Most methods for random number generation assume an underlying source
of realizations of a uniform (0, 1) random variable. If U is a uniform (0, 1)
random variable, and F is the cumulative distribution function of a continuous
random variable, then the random variable
X = F −1(U)
has the cumulative distribution function F. (If the support of X is ﬁnite,
F −1(0) and F −1(1) are interpreted as the limits of the support.) This same
idea, the basis of the so-called inverse CDF method, can also be applied to
discrete random variables.
9.7.1 The Multivariate Normal Distribution
If Z has a multivariate normal distribution with the identity as variance-
covariance matrix, then for a given positive deﬁnite matrix Σ, both
Y1 = Σ1/2Z
(9.74)
and
Y2 = ΣCZ,
(9.75)
where ΣC is a Cholesky factor of Σ, have a multivariate normal distribu-
tion with variance-covariance matrix Σ (see page 401). The mean of Y1 is
Σ1/2μ, where μ is the mean of Z, and the mean of Y1 is ΣCμ. If Z has 0
mean, then the distributions are identical, that is, Y1
d= Y2.
This leads to a very simple method for generating a multivariate normal
random d-vector: generate into a d-vector z d independent N1(0, 1). Then form
a vector from the desired distribution by the transformation in equation (9.74)
or (9.75) together with the addition of a mean vector if necessary.

444
9 Selected Applications in Statistics
9.7.2 Random Correlation Matrices
Occasionally we wish to generate random numbers but do not wish to specify
the distribution fully. We may want a “random” matrix, but we do not know an
exact distribution that we wish to simulate. (There are only a few “standard”
distributions of matrices. The Wishart distribution and the Haar distribution
(page 222) are the only two common ones. We can also, of course, specify the
distributions of the individual elements.)
We may want to simulate random correlation matrices. Although we do
not have a speciﬁc distribution, we may want to specify some characteristics,
such as the eigenvalues. (All of the eigenvalues of a correlation matrix, not
just the largest and smallest, determine the condition of data matrices that
are realizations of random variables with the given correlation matrix.)
Any nonnegative deﬁnite (symmetric) matrix with 1s on the diagonal is a
correlation matrix. A correlation matrix is diagonalizable, so if the eigenvalues
are c1, . . . , cd, we can represent the matrix as
V diag((c1, . . . , cd))V T
for an orthogonal matrix V . (For a d×d correlation matrix, we have  ci = d;
see page 368.) Generating a random correlation matrix with given eigenvalues
becomes a problem of generating the random orthogonal eigenvectors and then
forming the matrix V from them. (Recall from page 153 that the eigenvectors
of a symmetric matrix can be chosen to be orthogonal.) In the following, we
let C = diag((c1, . . . , cd)) and begin with E = I (the d×d identity) and k = 1.
The method makes use of deﬂation in step 6 (see page 310). The underlying
randomness is that of a normal distribution.
Algorithm 9.2 Random correlation matrices with given eigenvalues
1. Generate a d-vector w of i.i.d. standard normal deviates, form x = Ew,
and compute a = xT(I −C)x.
2. Generate a d-vector z of i.i.d. standard normal deviates, form y = Ez,
and compute b = xT(I −C)y, c = yT(I −C)y, and e2 = b2 −ac.
3. If e2 < 0, then go to step 2.
4. Choose a random sign, s = −1 or s = 1. Set r = b + se
a
x −y.
5. Choose another random sign, s = −1 or s = 1, and set vk =
sr
(rTr)
1
2 .
6. Set E = E −vkvT
k , and set k = k + 1.
7. If k < d, then go to step 1.
8. Generate a d-vector w of i.i.d. standard normal deviates, form x = Ew,
and set vd =
x
(xTx)
1
2 .
9. Construct the matrix V using the vectors vk as its rows. Deliver V CV T
as the random correlation matrix.

9.8 Stochastic Processes
445
9.8 Stochastic Processes
Many stochastic processes are modeled by a “state vector” and rules for up-
dating the state vector through a sequence of discrete steps. At time t, the
elements of the state vector xt are values of various characteristics of the sys-
tem. A model for the stochastic process is a probabilistic prescription for xta
in terms of xtb, where ta > tb; that is, given observations on the state vec-
tor prior to some point in time, the model gives probabilities for, or predicts
values of, the state vector at later times.
A stochastic process is distinguished in terms of the countability of the
space of states, X, and the index of the state (that is, the parameter space,
T ); either may or may not be countable. If the parameter space is continuous,
the process is called a diﬀusion process. If the parameter space is countable,
we usually consider it to consist of the nonnegative integers.
If the properties of a stochastic process do not depend on the index, the
process is said to be stationary. If the properties also do not depend on any
initial state, the process is said to be time homogeneous or homogeneous with
respect to the parameter space. (We usually refer to such processes simply as
“homogeneous”.)
9.8.1 Markov Chains
The Markov (or Markovian) property in a stochastic process is the condition in
which the current state does not depend on any states prior to the immediately
previous state; that is, the process is memoryless. If the transitions occur at
discrete intervals, the Markov property is the condition where the probability
distribution of the state at time t + 1 depends only on the state at time t.
In what follows, we will brieﬂy consider some Markov processes in which
both the set of states is countable and the transitions occur at discrete inter-
vals (discrete times). Such a process is called a Markov chain. (Some authors’
use of the term “Markov chain” allows the state space to be continuous, and
others’ allows time to be continuous; here we are not deﬁning the term. We
will be concerned with only a subclass of Markov chains, whichever way they
are deﬁned. The models for this subclass are easily formulated in terms of
vectors and matrices.)
If the state space is countable, it is equivalent to X = {1, 2, . . .}. If X is a
random variable from some sample space to X, and
πi = Pr(X = i),
then the vector π deﬁnes a distribution of X on X. (A vector of nonnegative
numbers that sum to 1 is a distribution.)
Formally, we deﬁne a Markov chain (of random variables) X0, X1, . . . in
terms of an initial distribution π and a conditional distribution for Xt+1 given
Xt. Let X0 have distribution π, and given Xt = i, let Xt+1 have distribution

446
9 Selected Applications in Statistics
(pij; j ∈X); that is, pij is the probability of a transition from state i at time
t to state j at time t + 1. Let
P = (pij).
This square matrix is called the transition matrix of the chain. It is clear that
P is a stochastic matrix (it is nonnegative and the elements in any row sum to
1), and hence ρ(P) = ∥P∥∞= 1, and (1, 1) is an eigenpair of P (see page 379).
If P does not depend on the time (and our notation indicates that we are
assuming this), the Markov chain is stationary.
The initial distribution π and the transition matrix P characterize the
chain, which we sometimes denote as Markov(π, P).
If the set of states is countably inﬁnite, the vectors and matrices have
inﬁnite order; that is, they have “inﬁnite dimension”. (Note that this use of
“dimension” is diﬀerent from our standard deﬁnition that is based on linear
independence.)
We denote the distribution at time t by π(t) and hence often write the
initial distribution as π(0). A distribution at time t can be expressed in terms
of π and P if we extend the deﬁnition of (Cayley) matrix multiplication in
equation (3.43) in the obvious way to handle any countable number of elements
so that PP or P 2 is the matrix deﬁned by
(P 2)ij =

k∈X
pikpkj.
We see immediately that
π(t) = (P t)Tπ(0).
(9.76)
Because of equation (9.76), P t is often called the t-step transition matrix.
(The somewhat awkward notation with the transpose results from the his-
torical convention in Markov chain theory of expressing distributions as “row
vectors”.)
9.8.1.1 Properties of Markov Chains
The transition matrix determines various relationships among the states of a
Markov chain. State j is said to be accessible from state i if it can be reached
from state i in a ﬁnite number of steps. This is equivalent to (P t)ij > 0
for some t. If state j is accessible from state i and state i is accessible from
state j, states j and i are said to communicate. Communication is clearly
an equivalence relation. (A binary relation ∼is an equivalence relation over
some set S if for x, y, z ∈S, (1) x ∼x, (2) x ∼y ⇒y ∼x, and (3)
x ∼y ∧y ∼z ⇒x ∼z; that is, it is reﬂexive, symmetric, and transitive.)
The set of all states that communicate with each other is an equivalence class.
States belonging to diﬀerent equivalence classes do not communicate, although
a state in one class may be accessible from a state in a diﬀerent class.

9.8 Stochastic Processes
447
Identiﬁcation and analysis of states that communicate can be done by
the reduction of the transition matrix in the manner discussed on page 375
and illustrated in equation (8.76), in which by permutations of the rows and
columns, square 0 submatrices are formed. If the transition matrix is irre-
ducible, that is, if no such 0 submatrices can be formed, then all states in a
Markov chain are in a single equivalence class. In that case the chain is said to
be irreducible. Irreducible matrices are discussed in Sect. 8.7.3, beginning on
page 375, and the implication (8.77) in that section provides a simple charac-
terization of irreducibility. Reducibility of Markov chains is also clearly related
to the reducibility in graphs that we discussed in Sect. 8.1.2. (In graphs, the
connectivity matrix is similar to the transition matrix in Markov chains.)
If the transition matrix is primitive (that is, it is irreducible and its eigen-
value with maximum modulus has algebraic multiplicity of 1, see page 377),
then the Markov chain is said to be primitive.
Primitivity and irreducibility are important concepts in analysis of Markov
chains because they imply interesting limiting behavior of the chains.
9.8.1.2 Limiting Behavior of Markov Chains
The limiting behavior of the Markov chain is of interest. This of course can be
analyzed in terms of limt→∞P t. Whether or not this limit exists depends on
the properties of P. If P is primitive and irreducible, we can make use of the
results in Sect. 8.7.3. In particular, because 1 is an eigenvalue and the vector
1 is the eigenvector associated with 1, from equation (8.79), we have
lim
t→∞P t = 1πT
s ,
(9.77)
where πs is the Perron vector of P T.
This also gives us the limiting distribution for an irreducible, primitive
Markov chain,
lim
t→∞π(t) = πs.
The Perron vector has the property πs = P Tπs of course, so this distribu-
tion is the invariant distribution of the chain. This invariance is a necessary
condition for most uses of Markov chains in Monte Carlo methods for gener-
ating posterior distributions in Bayesian statistical analysis. These methods
are called Markov chain Monte Carlo (MCMC) methods, and are widely used
in Bayesian analyses.
There are many other interesting properties of Markov chains that follow
from various properties of nonnegative matrices that we discuss in Sect. 8.7,
but rather than continuing the discussion here, we refer the interested reader
to a text on Markov chains, such as Meyn and Tweedie (2009).

448
9 Selected Applications in Statistics
9.8.2 Markovian Population Models
A simple but useful model for population growth measured at discrete points
in time, t, t + 1, . . ., is constructed as follows. We identify k age groupings for
the members of the population; we determine the number of members in each
age group at time t, calling this p(t),
p(t) =

p(t)
1 , . . . , p(t)
k

;
determine the reproductive rate in each age group, calling this α,
α = (α1, . . . , αk);
and determine the survival rate in each of the ﬁrst k −1 age groups, calling
this σ,
σ = (σ1, . . . , σk−1).
It is assumed that the reproductive rate and the survival rate are constant
in time. (There are interesting statistical estimation problems here that are
described in standard texts in demography or in animal population models.)
The survival rate σi is the proportion of members in age group i at time t
who survive to age group i + 1. (It is assumed that the members in the last
age group do not survive from time t to time t + 1.) The total size of the
population at time t is N (t) = 1Tp(t). (The use of the capital letter N for
a scalar variable is consistent with the notation used in the study of ﬁnite
populations.)
If the population in each age group is relatively large, then given the sizes
of the population age groups at time t, the approximate sizes at time t + 1
are given by
p(t+1) = Ap(t),
(9.78)
where A is a Leslie matrix as in equation (8.85),
A =
⎡
⎢⎢⎢⎢⎢⎣
α1 α2 · · · αm−1 αm
σ1 0 · · ·
0
0
0 σ2 · · ·
0
0
...
...
...
...
...
0
0 · · · σm−1
0
⎤
⎥⎥⎥⎥⎥⎦
,
(9.79)
where 0 ≤αi and 0 ≤σi ≤1.
The Leslie population model can be useful in studying various species of
plants or animals. The parameters in the model determine the vitality of the
species. For biological realism, at least one αi and all σi must be positive. This
model provides a simple approach to the study and simulation of population
dynamics. The model depends critically on the eigenvalues of A.
As we have seen (Exercise 8.10), the Leslie matrix has a single unique
positive eigenvalue. If that positive eigenvalue is strictly greater in modulus

9.8 Stochastic Processes
449
than any other eigenvalue, then given some initial population size, p(0), the
model yields a few damping oscillations and then an exponential growth,
p(t0+t) = p(t0)ert,
(9.80)
where r is the rate constant. The vector p(t0) (or any scalar multiple) is called
the stable age distribution. (You are asked to show this in Exercise 9.22a.) If 1
is an eigenvalue and all other eigenvalues are strictly less than 1 in modulus,
then the population eventually becomes constant; that is, there is a stable
population. (You are asked to show this in Exercise 9.22b.)
The survival rates and reproductive rates constitute an age-dependent life
table, which is widely used in studying population growth. The age groups
in life tables for higher-order animals are often deﬁned in years, and the pa-
rameters often are deﬁned only for females. The ﬁrst age group is generally
age 0, and so α1 = 0. The net reproductive rate, r0, is the average number
of (female) oﬀspring born to a given (female) member of the population over
the lifetime of that member; that is,
r0 =
m

i=2
αiσi−1.
(9.81)
The average generation time, T , is given by
T =
m

i=2
iαiσi−1/r0.
(9.82)
The net reproductive rate, average generation time, and exponential growth
rate constant are related by
r = log(r0)/T.
(9.83)
(You are asked to show this in Exercise 9.22c.)
Because the process being modeled is continuous in time and this model
is discrete, there are certain averaging approximations that must be made.
There are various reﬁnements of this basic model to account for continuous
time. There are also reﬁnements to allow for time-varying parameters and for
the intervention of exogenous events. Of course, from a statistical perspective,
the most interesting questions involve the estimation of the parameters. See
Cullen (1985), for example, for further discussions of this modeling problem.
Various starting age distributions can be used in this model to study the
population dynamics.
9.8.3 Autoregressive Processes
An interesting type of stochastic process is the pth-order autoregressive time
series, deﬁned by the stochastic diﬀerence equation

450
9 Selected Applications in Statistics
xt = φ0 + φ1xt−1 + · · · + φpxt−p + et,
(9.84)
where φp ̸= 0, the et have constant mean of 0 and constant variance of σ2 > 0,
and any two diﬀerent es and et have zero correlation. This model is called an
autoregressive model of order p or AR(p).
Comments on the model and notation:
I have implied that et is considered to be a random variable, even
though it is not written in upper case, and of course, if et is a random
variable then xt is also, even though it is not written in upper case
either; likewise, I will sometimes consider xt−1 and the other xt−j to
be random variables.
Stationarity (that is, constancy over time) is an issue. In the simple
model above all of the simple parameters are constant; however, un-
less certain conditions are met, the moments of xt can grow without
bounds. (This is related to the “unit roots” mentioned below. Some
authors require that some other conditions be satisﬁed in an AR(p)
model so that moments of the process do not grow without bounds.
Also, many authors omit the φ0 term in the AR(p) model.
The most important properties of this process arise from the autocorrela-
tions, Cor(xs, xt). If these autocorrelations depend on s and t only through
|s −t| and if for given h = |s −t| the autocorrelation,
ρh = Cor(xt+h, xt),
is constant, the autoregressive process has some simple, but useful properties.
The model (9.84) is a little more complicated than it appears. This is
because the speciﬁcation of xt is conditional on xt−1, . . . , xt−p. Presumably,
also, xt−1 is dependent on xt−2, . . . , xt−p−1, and so on. There are no marginal
(unconditional) properties of the xs that are speciﬁed in the model; that is,
we have not speciﬁed a starting point.
The stationarity of the et (constant mean and constant variance) does not
imply that the xt are stationary. We can make the model more speciﬁc by
adding a condition of stationarity on the xt. Let us assume that the xt have
constant and ﬁnite means and variances; that is, the {xt} process is (weakly)
stationary.
To continue the analysis, consider the AR(1) model. If the xt have constant
means and variances, then
E(xt) =
φ0
1 −φ1
(9.85)
and
V(xt) =
σ2
1 −φ2
1
.
(9.86)
The ﬁrst equation indicates that we cannot have φ1 = 1 and the second
equation makes sense only if |φ1| < 1. For AR(p) models in general, similar

9.8 Stochastic Processes
451
situations can occur. The denominators in the expressions for the mean and
variance involve p-degree polynomials , similar to the ﬁrst degree polynomials
in the denominators of equations (9.85) and (9.86).
We call f(z) = 1−φ1z1 −· · ·−φpzp the associated polynomial. If f(z) = 0,
we have situations similar to a 0 in the denominator of equation (9.86). If
a root of f(z) = 0 is 1, the expression for a variance is inﬁnite (which we
see immediately from equation (9.86) for the AR(1) model). This situation is
called a “unit root”. If some root is greater than 1, we have an expression for
a variance that is negative. Hence, in order for the model to make sense in
all respects, all roots of of the associated polynomial must be less than 1 in
modulus. (Note some roots can contain imaginary components.)
Although many of the mechanical manipulations in the analysis of the
model may be unaﬀected by unit roots, they have serious implications for the
interpretation of the model.
9.8.3.1 Relation of the Autocorrelations to the Autoregressive
Coeﬃcients
From the model (9.84) we can see that ρh = 0 for h > p, and ρ1, . . . ρp are
determined by φ1, . . . φp by the relationship
Rφ = ρ,
(9.87)
where φ and ρ are the p-vectors of the φis (i ̸= 0) and the ρis, and R is the
Toeplitz matrix (see Sect. 8.8.4)
R =
⎡
⎢⎢⎢⎢⎢⎣
1
ρ1
ρ2
· · · ρp−1
ρ1
1
ρ1
· · · ρp−2
ρ2
ρ1
1
· · · ρp−3
...
...
...
ρp−1 ρp−2 ρp−3 · · ·
1
⎤
⎥⎥⎥⎥⎥⎦
.
This system of equations is called the Yule-Walker equations. Notice the re-
lationship of the Yule-Walker equations to the unit root problem mentioned
above. For example, for p = 1, we have φ1 = ρ1. In order for to be a correla-
tion, it must be the case that |φ1| ≤1.
For a given set of ρs, possibly estimated from some observations on the
time series, Algorithm 9.3 can be used to solve the system (9.87).
Algorithm 9.3 Solution of the Yule-Walker system (9.87)
1. Set k = 0; φ(k)
1
= −ρ1; b(k) = 1; and a(k) = −ρ1.
2. Set k = k + 1.
3. Set b(k) =

1 −

a(k−1)2
b(k−1).

452
9 Selected Applications in Statistics
4. Set a(k) = −

ρk+1 + k
i=1 ρk+1−iφ(k−1)
1

/b(k).
5. For i = 1, 2, . . ., k
set yi = φ(k−1)
i
+ a(k)φ(k−1)
k+1−i.
6. For i = 1, 2, . . ., k
set φ(k)
i
= yi.
7. Set φ(k)
k+1 = a(k).
8. If k < p −1, go to step 1; otherwise terminate.
This algorithm is O(p) (see Golub and Van Loan 1996).
The Yule-Walker equations arise in many places in the analysis of stochas-
tic processes. Multivariate versions of the equations are used for a vector time
series (see Fuller 1995; for example).
Exercises
9.1. Let X be an n × m matrix with n > m and with entries sampled
independently from a continuous distribution (of a real-valued random
variable). What is the probability that XTX is positive deﬁnite?
9.2. From equation (9.18), we have ˆyi = yTX(XTX)+xi∗. Show that hii in
equation (9.19) is ∂ˆyi/∂yi.
9.3. Formally prove from the deﬁnition that the sweep operator is its own
inverse.
9.4. Consider the regression model
y = Xβ + ϵ
(9.88)
subject to the linear equality constraints
Lβ = c,
(9.89)
and assume that X is of full column rank.
a) Let λ be the vector of Lagrange multipliers. Form
(bTLT −cT)λ
and
(y −Xb)T(y −Xb) + (bTLT −cT)λ.
Now diﬀerentiate these two expressions with respect to λ and b,
respectively, set the derivatives equal to zero, and solve to obtain
*βC = (XTX)−1XTy −1
2(XTX)−1LT*λC
= *β −1
2(XTX)−1LT*λC

Exercises
453
and
*λC = −2(L(XTX)−1LT)−1(c −L*β).
Now combine and simplify these expressions to obtain expres-
sion (9.29) (on page 416).
b) Prove that the stationary point obtained in Exercise 9.4a actually
minimizes the residual sum of squares subject to the equality con-
straints.
Hint: First express the residual sum of squares as
(y −X *β)T(y −X *β) + (*β −b)TXTX(*β −b),
and show that is equal to
(y−X *β)T(y−X *β)+(*β−*βC)TXTX(*β−*βC)+(*βC−b)TXTX(*βC−b),
which is minimized when b = *βC.
c) Show that sweep operations applied to the matrix (9.30) on
page 416 yield the restricted least squares estimate in the (1,2)
block.
d) For the weighting matrix W, derive the expression, analogous
to equation (9.29), for the generalized or weighted least squares
estimator for β in equation (9.88) subject to the equality con-
straints (9.89).
9.5. Derive a formula similar to equation (9.33) to update *β due to the
deletion of the ith observation.
9.6. When data are used to ﬁt a model such as y = Xβ + ϵ, a large leverage
of an observation is generally undesirable. If an observation with large
leverage just happens not to ﬁt the “true” model well, it will cause *β to
be farther from β than a similar observation with smaller leverage.
a) Use artiﬁcial data to study inﬂuence. There are two main aspects
to consider in choosing the data: the pattern of X and the values
of the residuals in ϵ. The true values of β are not too important,
so β can be chosen as 1. Use 20 observations. First, use just one
independent variable (yi = β0 + β1xi + ϵi). Generate 20 xis more or
less equally spaced between 0 and 10, generate 20 ϵis, and form the
corresponding yis. Fit the model, and plot the data and the model.
Now, set x20 = 20, set ϵ20 to various values, form the yi’s and ﬁt the
model for each value. Notice the inﬂuence of x20.
Now, do similar studies with three independent variables. (Do not
plot the data, but perform the computations and observe the eﬀect.)
Carefully write up a clear description of your study with tables and
plots.
b) Heuristically, the leverage of a point arises from the distance from
the point to a fulcrum. In the case of a linear regression model, the
measure of the distance of observation i is

454
9 Selected Applications in Statistics
Δ(xi, X1/n) = ∥xi, X1/n∥.
(This is not the same quantity from the hat matrix that is deﬁned as
the leverage on page 410, but it should be clear that the inﬂuence of
a point for which Δ(xi, X1/n) is large is greater than that of a point
for which the quantity is small.) It may be possible to overcome some
of the undesirable eﬀects of diﬀerential leverage by using weighted
least squares to ﬁt the model. The weight wi would be a decreasing
function of Δ(xi, X1/n).
Now, using datasets similar to those used in the previous part of this
exercise, study the use of various weighting schemes to control the
inﬂuence. Weight functions that may be interesting to try include
wi = e−Δ(xi,X1/n)
and
wi = max(wmax, ∥Δ(xi, X1/n)∥−p)
for some wmax and some p > 0. (Use your imagination!)
Carefully write up a clear description of your study with tables and
plots.
c) Now repeat Exercise 9.6b except use a decreasing function of the
leverage, hii from the hat matrix in equation (9.18) instead of the
function Δ(xi, X1/n).
Carefully write up a clear description of this study, and compare it
with the results from Exercise 9.6b.
9.7. By diﬀerentiating expression (9.38), derive the normal equations (9.39)
for the multivariate linear model.
9.8. Formally prove the relationship expressed in equation (9.71) on
page 442.
Hint: Use equation (9.70) twice.
9.9. On page 211, we used Lagrange multipliers to determine the normalized
vector x that maximized xTAx. If A is SX, this is the ﬁrst principal
component. We also know the principal components from the spectral
decomposition. We could also ﬁnd them by sequential solutions of La-
grangians. After ﬁnding the ﬁrst principal component, we would seek the
linear combination z such that Xcz has maximum variance among all
normalized z that are orthogonal to the space spanned by the ﬁrst prin-
cipal component; that is, that are XT
c Xc-conjugate to the ﬁrst principal
component (see equation (3.93) on page 94). If V1 is the matrix whose
columns are the eigenvectors associated with the largest eigenvector, this
is equivalent to ﬁnding z so as to maximize zTSz subject to V T
1 z = 0.
Using the method of Lagrange multipliers as in equation (4.54), we form
the Lagrangian corresponding to equation (4.56) as
zTSz −λ(zTz −1) −φV T
1 z,

Exercises
455
where λ is the Lagrange multiplier associated with the normalization
requirement zTz = 1, and φ is the Lagrange multiplier associated with
the orthogonality requirement. Solve this for the second principal com-
ponent, and show that it is the same as the eigenvector corresponding
to the second-largest eigenvalue.
9.10. Obtain the “Longley data”. (It is a dataset in R, and it is also available
from statlib.) Each observation is for a year from 1947 to 1962 and
consists of the number of people employed, ﬁve other economic variables,
and the year itself. Longley (1967) ﬁtted the number of people employed
to a linear combination of the other variables, including the year.
a) Use a regression program to obtain the ﬁt.
b) Now consider the year variable. The other variables are measured
(estimated) at various times of the year, so replace the year vari-
able with a “midyear” variable (i.e., add 1
2 to each year). Redo the
regression. How do your estimates compare?
c) Compute the L2 condition number of the matrix of independent
variables. Now add a ridge regression diagonal matrix, as in the
matrix (9.90), and compute the condition number of the resulting
matrix. How do the two condition numbers compare?
9.11. Consider the least squares regression estimator (9.15) for full rank n×m
matrix X (n > m):
*β = (XTX)−1XTy.
a) Compare this with the ridge estimator
*βR(d) = (XTX + dIm)−1XTy
for d ≥0. Show that
∥*βR(d)∥≤∥*β∥.
b) Show that *βR(d) is the least squares solution to the regression model
similar to y = Xβ + ϵ except with some additional artiﬁcial data;
that is, y is replaced with
 y
0

,
where 0 is an m-vector of 0s, and X is replaced with
!X
dIm
"
.
(9.90)
Now explain why *βR(d) is shorter than *β.
9.12. Use the Schur decomposition (equation (3.190), page 122) of the inverse
of (XTX) to prove equation (9.53).

456
9 Selected Applications in Statistics
9.13. Given the matrix
A =
⎡
⎢⎢⎣
2 1 3
1 2 3
1 1 1
1 0 1
⎤
⎥⎥⎦,
assume the random 3 × 2 matrix X is such that
vec(X −A)
has a N(0, V ) distribution, where V is block diagonal with the matrix
⎡
⎢⎢⎣
2 1 1 1
1 2 1 1
1 1 2 1
1 1 1 2
⎤
⎥⎥⎦
along the diagonal. Generate ten realizations of X matrices, and use
them to test that the rank of A is 2. Use the test statistic (9.64) on
page 437.
9.14. Construct a 9×2 matrix X with some missing values, such that SX com-
puted using all available data for the covariance or correlation matrix is
not nonnegative deﬁnite.
9.15. Consider an m×m, symmetric nonsingular matrix, R, with 1s on the di-
agonal and with all oﬀ-diagonal elements less than 1 in absolute value.
If this matrix is positive deﬁnite, it is a correlation matrix. Suppose,
however, that some of the eigenvalues are negative. Iman and Daven-
port (1982) describe a method of adjusting the matrix to a “near-by”
matrix that is positive deﬁnite. (See Ronald L. Iman and James M. Dav-
enport, 1982,
An Iterative Algorithm to Produce a Positive Deﬁnite
Correlation Matrix from an “Approximate Correlation Matrix”, San-
dia Report SAND81-1376, Sandia National Laboratories, Albuquerque,
New Mexico.) For their method, they assumed the eigenvalues are
unique, but this is not necessary in the algorithm.
Before beginning the algorithm, choose a small positive quantity, ϵ, to
use in the adjustments, set k = 0, and set R(k) = R.
1. Compute the eigenvalues of R(k),
c1 ≥c2 ≥. . . ≥cm,
and let p be the number of eigenvalues that are negative. If p = 0,
stop. Otherwise, set
c∗
i =
#
ϵ if ci < ϵ
ci otherwise
for i = p1, . . . , m −p,
(9.91)
where p1 = max(1, m −2p).

Exercises
457
2. Let

i
civivT
i
be the spectral decomposition of R (equation (3.256), page 154),
and form the matrix R∗:
R∗=
p1

i=1
civivT
i +
m−p

i=p1+1
c∗
i vivT
i +
m

i=m−p+1
ϵvivT
i .
3. Form R(k) from R∗by setting all diagonal elements to 1.
4. Set k = k + 1, and go to step 1. (The algorithm iterates on k until
p = 0.)
Write a program to implement this adjustment algorithm. Write your
program to accept any size matrix and a user-chosen value for ϵ. Test
your program on the correlation matrix from Exercise 9.14.
9.16. Consider some variations of the method in Exercise 9.15. For example,
do not make the adjustments as in equation (9.91), or make diﬀerent
ones. Consider diﬀerent adjustments of R∗; for example, adjust any oﬀ-
diagonal elements that are greater than 1 in absolute value.
Compare the performance of the variations.
9.17. Investigate the convergence of the method in Exercise 9.15. Note that
there are several ways the method could converge.
9.18. Suppose the method in Exercise 9.15 converges to a positive deﬁnite
matrix R(n). Prove that all oﬀ-diagonal elements of R(n) are less than
1 in absolute value. (This is true for any positive deﬁnite matrix with
1s on the diagonal.)
9.19. Shrinkage adjustments of approximate correlation matrices.
a) Write a program to implement the linear shrinkage adjustment of
equation (9.66). Test your program on the correlation matrix from
Exercise 9.14.
b) Write a program to implement the nonlinear shrinkage adjustment
of equation (9.67). Let δ = 0.05 and
f(x) = tanh(x).
Test your program on the correlation matrix from Exercise 9.14.
c) Write a program to implement the scaling adjustment of equa-
tion (9.68). Recall that this method applies to an approximate corre-
lation matrix that is a pseudo-correlation matrix. Test your program
on the correlation matrix from Exercise 9.14.
9.20. Show that the matrices generated in Algorithm 9.2 are correlation ma-
trices. (They are clearly nonnegative deﬁnite, but how do we know that
they have 1s on the diagonal?)

458
9 Selected Applications in Statistics
9.21. Consider a two-state Markov chain with transition matrix
P =
!
1 −α
α
β
1 −β
"
for 0 < α < 1 and 0 < β < 1. Does an invariant distribution exist, and
if so what is it?
9.22. Recall from Exercise 8.10 that a Leslie matrix has a single unique posi-
tive eigenvalue.
a) What are the conditions on a Leslie matrix A that allow a stable
age distribution? Prove your assertion.
Hint: Review the development of the power method in equa-
tions (7.9) and (7.10).
b) What are the conditions on a Leslie matrix A that allow a stable
population, that is, for some xt, xt+1 = xt?
c) Derive equation (9.83). (Recall that there are approximations that
result from the use of a discrete model of a continuous process.)
9.23. Derive equations (9.85) and (9.86) under the stationarity assumptions
for the model (9.84).
9.24. Derive the Yule-Walker equations (9.87) for the model (9.84).

Part III
Numerical Methods and Software

10
Numerical Methods
The computer is a tool for storage, manipulation, and presentation of data.
The data may be numbers, text, or images, but no matter what the data are,
they must be coded into a sequence of 0s and 1s because that is what the
computer stores. For each type of data, there are several ways of coding. For
any unique coding scheme, the primary considerations are eﬃciency in storage,
retrieval, and computations. Each of these considerations may depend on the
computing system to be used. Another important consideration is coding that
can be shared or transported to other systems.
How much a computer user needs to know about the way the computer
works depends on the complexity of the use and the extent to which the
necessary operations of the computer have been encapsulated in software that
is oriented toward the speciﬁc application. This chapter covers many of the
basics of how digital computers represent data and perform operations on the
data. Although some of the speciﬁc details we discuss will not be important
for a computational scientist or for someone doing statistical computing, the
consequences of those details are important, and the serious computer user
must be at least vaguely aware of the consequences.
There are some interesting facts that should cause anyone who programs
a computer to take care:
•
adding a positive number to a given positive number may not change that
positive number;
•
performing two multiplications in one order may yield a diﬀerent result
from doing the multiplications in the other order;
•
for any given number, there is a “next” number;
•
the next number after 1 is farther from 1 than the next number before 1;
•
the next number after 1 is closer to 1 than the next number after 2 is to 2;
. . . and so on. . .
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 10
461

462
10 Numerical Methods
Some of the material in this chapter and the next chapter is covered more
extensively in Gentle (2009), Chapters 2 through 6.
Standards
Standards are agreed-upon descriptions of processes, languages, or hardware.
They facilitate development of computers and software. Most engineers are
very familiar with standards for various types of hardware, such as electronic
components, in which case the standard may specify a range for input voltage
and current and a range for generated heat. In computing, standards may
allow a software developer to know exactly what a piece of Fortran code will
do, for example.
Development of a standard often begins with one or more working groups
of experts, then evolves through a process of public comment, and ﬁnally is
adopted by a standards agency. Working groups are often formed by profes-
sional societies, for example, the International Federation for Information Pro-
cessing has a number of working groups, including the IFIP Working Group
2.5 on Numerical Software. Standards agencies may be formed by professional
societies or by governments. How useful a standard is depends on how widely
the standard is followed; hence, larger, stronger, and fewer standards agencies
are desirable.
In this and the next two chapters we will mention a number of standards,
the most important of which are the IEEE standards for computer represen-
tation of numeric values and computations with them, and the ISO (formerly
ANSI) standards for computer languages.
Coding Systems
Data of whatever form are represented by groups of 0s and 1s, called bits
from the words “binary” and “digits”. (The word was coined by John Tukey.)
For representing simple text (that is, strings of characters each of which has
no separate visual distinctions), the bits are usually taken in groups of eight,
called bytes, and associated with a speciﬁc character according to a ﬁxed
coding rule. Because of the common association of a byte with a character,
those two words are often used synonymously.
The most widely used code for representing characters in bytes is “ASCII”
(pronounced “askey”, from American Standard Code for Information Inter-
change). Because the code is so widely used, the phrase “ASCII data” is some-
times used as a synonym for “text data” or “character data”. The ASCII code
for the character “A”, for example, is 01000001; for “a” it is 01100001; and for
“5” it is 00110101. Humans can more easily read shorter strings with several
diﬀerent characters than they can longer strings, even if those longer strings
consist of only two characters. Bits, therefore, are often grouped into strings
of fours; a four-bit string is equivalent to a hexadecimal digit, 1, 2, . . . , 9, A,

10 Numerical Methods
463
B, . . . , or F. Thus, the ASCII codes above could be written in hexadecimal
notation as 41 (“A”), 61 (“a”), and 35 (“5”).
Because the common character sets diﬀer from one language to another
(both natural languages and computer languages), there are several modiﬁca-
tions of the basic ASCII code set. Also, when there is a need for more diﬀerent
characters than can be represented in a byte (28), codes to associate characters
with larger groups of bits are necessary. For compatibility with the commonly
used ASCII codes using groups of 8 bits, these codes usually are for groups of
16 bits. These codes for “16-bit characters” are useful for representing charac-
ters in some Oriental languages, for example. The Unicode Consortium (1990,
1992) has developed a 16-bit standard, called Unicode, that is widely used for
representing characters from a variety of languages. For any ASCII character,
the Unicode representation uses eight leading 0s and then the same eight bits
as the ASCII representation.
A standard scheme for representing data is very important when data are
moved from one computer system to another or when researchers at diﬀerent
sites want to share data. Except for some bits that indicate how other bits
are to be formed into groups (such as an indicator of the end of a ﬁle, or the
delimiters of a record within a ﬁle), a set of data in ASCII representation is the
same on diﬀerent computer systems. Software systems that process documents
either are speciﬁc to a given computer system or must have some standard
coding to allow portability. The Java system, for example, uses Unicode to
represent characters so as to ensure that documents can be shared among
widely disparate platforms.
In addition to standard schemes for representing the individual data el-
ements, there are some standard formats for organizing and storing sets of
data. These standards specify properties of “data structure”.
Types of Data
Bytes that correspond to characters are often concatenated to form character
string data (or just “strings”). Strings represent text without regard to the
appearance of the text if it were to be printed. Thus, a string representing
“ABC” does not distinguish between “ABC”, “ABC ”, and “ABC”. The ap-
pearance of the printed character must be indicated some other way, perhaps
by additional bit strings designating a font.
The appearance of characters or other visual entities such as graphs or
pictures is often represented more directly as a “bitmap”. Images on a display
medium such as paper or a computer screen consist of an arrangement of
small dots, possibly of various colors. The dots must be coded into a sequence
of bits, and there are various coding schemes in use, such as JPEG (for Joint
Photographic Experts Group). Image representations of “ABC”, “ABC”, and
“ABC” would all be diﬀerent. The computer’s internal representation may
correspond directly to the dots that are displayed or it may be a formula to
generate the dots, but in each case, the data are represented as a set of dots

464
10 Numerical Methods
located with respect to some coordinate system. More dots would be turned
on to represent “ABC” than to represent “ABC”. The location of the dots
and the distance between the dots depend on the coordinate system; thus the
image can be repositioned or rescaled.
Computers initially were used primarily to process numeric data, and num-
bers are still the most important type of data in statistical computing. There
are important diﬀerences between the numerical quantities with which the
computer works and the numerical quantities of everyday experience. The fact
that numbers in the computer must have a ﬁnite representation has very im-
portant consequences.
Missing Data
In statistical computing and in any applications of the data sciences, we must
deal with the reality of missing data. For any of a number of reasons, certain
pieces of a dataset to be analyzed may be missing. Exactly how to proceed
with an analysis when some of the data elements are missing depends on the
nature of the data and the analysis to be performed and which data elements
are missing. For example, there are three diﬀerent ways to compute a sample
variance-covariance matrix from a dataset with missing elements, as discussed
in Sect. 9.5.6, beginning on page 437.
In this chapter, we are concerned with computer representation of data,
so the ﬁrst issue is what to store in a computer memory location when the
value that should go there is missing. We need a method for representing
a “not available” (NA) value, and then we need a mechanism for avoiding
operations with this NA value. There are various ways of doing this, and
diﬀerent software systems provide diﬀerent methods. The standard computer
number system itself provides for special computer numbers, as we will discuss
on page 475.
Data Structures
The user’s interactions with the computing system may include data in-
put/output and other transmissions and various manipulations of the data.
The organization of data is called the data structure, and there are various
standard types of data structures that are useful for diﬀerent types of compu-
tations. In many numerical computations, such as those involving vectors and
matrices, the data structures are usually very simple and correspond closely
to the ordinary mathematical representation of the entities. In some cases,
however, the data may be organized so as to optimize for sparsity or in or-
der to take advantage of some aspect of the computer’s architecture, such as
interleaved memory banks. In large-scale computational problems, the data
may be stored in separate ﬁles that reside on separate computing systems.

10 Numerical Methods
465
For more general databases, commercial software vendors have deﬁned
data structures and developed data management systems to support access-
ing and updating the datasets. Some of the details of those systems may be
proprietary.
There are also some open systems that are widely used in science appli-
cations. One is the Common Data Format (CDF), developed by the NASA’s
National Space Science Data Center, and date from the 1980s. It is essentially
a programming interface for storage and manipulation of multidimensional
data. A related system, NetCDF, is built on CDF. There are R packages
(ncdf.tools and ncdf4) to read and write NetCDF ﬁles. NetCDF is the
standard data structure for some software systems for spatial statistics.
Another standard structure is the Hierarchical Data Format (HDF), devel-
oped by the National Center for Supercomputing Applications. The current
version of the Hierarchical Data Format, HDF5, which is built on NetCDF,
is supported in many software systems including R and Matlab, as well as
Fortran, C, Java, and Python.
Both CDF and HDF standards allow a variety of types and structures of
data; the standardization is in the descriptions that accompany the datasets.
Computer Architectures and File Systems
The user interacts with the computing system at one level through an operat-
ing system, such as Linux, Microsoft Windows, or Apple iOS, and at diﬀerent
levels through applications software or other types of software systems. The
general organizational structure or architecture of the computing system ulti-
mately determines how the user interacts with the computing system. We call
the way that the user interacts with the computing system the programming
model, and it generally consists of multiple simpler programming models. The
simplest and probably most common programming model applies to a single
user on a single computing system running a single process. In this model,
the computing system appears to the user as a single entity, although the pro-
cessing unit itself make consist of multiple processors (or “cores”). Multiple
processors allow for various types of parallel processing. Even if the comput-
ing system actually consists of several separate “computers”, the same model
can apply. If the computing system allows for multiple computations to occur
simultaneously or when more than one computer comprises the system, a pro-
gramming model that allows control of the separate processors may be more
appropriate. This is especially true if there are diﬀerent computers that are
linked to make up the computer system. This kind of setup is called distributed
computing.
How data input and output are performed depend on a ﬁle system. A
single computer may have a simple ﬁle system that the user is often not even
aware of.
More interesting computing applications may involve a distributed envi-
ronment across many computers or clusters of computers and many program-

466
10 Numerical Methods
ming models. Instead of an ad hoc top-level programming model, it is desirable
to have a model that scales from a single system to many machines, each with
its own local computing and storage resources. The challenge is to access each
resource seamlessly. MapReduce is an eﬃcient method for doing this that was
developed by Google, Inc. (now Alphabet, Inc.). Hadoop is an open-source
system based on MapReduce methods that provides a scalable programming
model. The basic feature of Hadoop is a ﬁle system, called the Hadoop Dis-
tributed File System or HDFS. (We will discuss MapReduce on page 515 and
again brieﬂy on page 533.)
10.1 Digital Representation of Numeric Data
For representing a number in a ﬁnite number of digits or bits, the two most
relevant things are the magnitude of the number and the precision with which
the number is to be represented. Whenever a set of numbers are to be used
in the same context, we must ﬁnd a method of representing the numbers that
will accommodate their full range and will carry enough precision for all of
the numbers in the set.
Another important aspect in the choice of a method to represent data is the
way data are communicated within a computer and between the computer and
peripheral components such as data storage units. Data are usually treated
as a ﬁxed-length sequence of bits. The basic grouping of bits in a computer
is sometimes called a “word” or a “storage unit”. The length of words or
storage units commonly used in computers is 64 bits, although some systems
use 32 bits or other lengths. (In the following examples, I will use 32 bits for
simplicity.)
It is desirable to have diﬀerent kinds of representations for diﬀerent sets of
numbers, even on the same computer. Like the ASCII standard for characters,
however, there are some standards for representation of, and operations on,
numeric data. The Institute of Electrical and Electronics Engineers (IEEE)
and, subsequently, the International Electrotechnical Commission (IEC) have
been active in promulgating these standards, and the standards themselves
are designated by an IEEE number and/or an IEC number.
The two mathematical models that are often used for numeric data are the
ring of integers, ZZ, and the ﬁeld of reals, IR. We use two computer models, II
and IF, to simulate these mathematical entities. (Neither II nor IF is a simple
mathematical construct such as a ring or ﬁeld.)
10.1.1 The Fixed-Point Number System
Because an important set of numbers is a ﬁnite set of reasonably-sized inte-
gers, eﬃcient schemes for representing these special numbers are available in
most computing systems. The scheme is usually some form of a base 2 repre-
sentation and may use one storage unit (this is most common), two storage

10.1 Digital Representation of Numeric Data
467
units, or one half of a storage unit. For example, if a storage unit consists of 32
bits and one storage unit is used to represent an integer, the integer 5 may be
represented in binary notation using the low-order bits, as shown in Fig. 10.1.
0 0 0 0 0 0 00 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1
Figure 10.1. The value 5 in a binary representation
The sequence of bits in Fig. 10.1 represents the value 5, using one storage
unit. The character “5” is represented in the ASCII code shown previously,
00110101.
If the set of integers includes the negative numbers also, some way of
indicating the sign must be available. The ﬁrst bit in the bit sequence (usually
one storage unit) representing an integer is usually used to indicate the sign;
if it is 0, a positive number is represented; if it is 1, a negative number is
represented. In a common method for representing negative integers, called
“twos-complement representation”, the sign bit is set to 1 and the remaining
bits are set to their opposite values (0 for 1; 1 for 0), and then 1 is added to
the result. If the bits for 5 are . . . 00101, the bits for −5 would be . . . 11010
+ 1, or . . . 11011. If there are k bits in a storage unit (and one storage unit
is used to represent a single integer), the integers from 0 through 2k−1 −1
would be represented in ordinary binary notation using k −1 bits. An integer
i in the interval [−2k−1, −1] would be represented by the same bit pattern
by which the nonnegative integer 2k−1 + i is represented, except the sign bit
would be 1.
The sequence of bits in Fig. 10.2 represents the value −5 using twos-
complement notation in 32 bits, with the leftmost bit being the sign bit and
the rightmost bit being the least signiﬁcant bit; that is, the 1 position. The
ASCII code for “−5” consists of the codes for “−” and “5”; that is,
00101101 00110101.
1 1 1 1 1 1 11 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 11 1 1 0 1 1
Figure 10.2. The value −5 in a twos-complement representation
The special representations for numeric data are usually chosen so as to
facilitate manipulation of data. The twos-complement representation makes
arithmetic operations particularly simple. It is easy to see that the largest
integer that can be represented in the twos-complement form is 2k−1 −1 and
that the smallest integer is −2k−1.
A representation scheme such as that described above is called ﬁxed-point
representation or integer representation, and the set of such numbers is de-
noted by II. The notation II is also used to denote the system built on this
set. This system is similar in some ways to a ring, which is what the integers
ZZ are.

468
10 Numerical Methods
10.1.1.1 Software Representation and Big Integers
The description above relates directly to how the bits in computer memory
are stored in a ﬁxed-point representation. The number of bits used and the
method of representing negative numbers are two aspects that may vary from
one computer to another. Various software systems provide other options,
such as “long”, “short”, “signed”, and “unsigned”, so that even within a
single computer system, the number of bits used in ﬁxed-point representation
may vary. The common number of bits is typically one or two storage units
or half of a storage unit.
A diﬀerent approach implemented in the software is to use as much mem-
ory as necessary to represent any integer value, if the memory is available.
Such a scheme is called big integer representation. The representation is also
known as multiple precision, arbitrary precision, or inﬁnite precision integer.
GMP, for “GNU Multiple Precision”, is a C and C++ library that provides
mathematical operations involving big integers, as well as multiple precision
ﬂoating-point numbers. The Python interpreter seamlessly moves between
standard ﬁxed-width integers and big integers depending upon the result of a
calculation. The gmp package in R supports big integers in the bigz class.
We discuss the operations with numbers in the ﬁxed-point system in
Sect. 10.2.1.
10.1.2 The Floating-Point Model for Real Numbers
In a ﬁxed-point representation, all bits represent values greater than or equal
to 1; the base point or radix point is at the far right, before the ﬁrst bit. In
a ﬁxed-point representation scheme using k bits, the range of representable
numbers is of the order of 2k, usually from approximately −2k−1 to 2k−1.
Numbers outside of this range cannot be represented directly in the ﬁxed-
point scheme. Likewise, nonintegral numbers cannot be represented. Large
numbers and fractional numbers are generally represented in a scheme similar
to what is sometimes called “scientiﬁc notation” or in a type of logarithmic
notation. Because within a ﬁxed number of digits the radix point is not ﬁxed,
this scheme is called ﬂoating-point representation, and the set of such numbers
is denoted by IF. The notation IF is also used to denote the system built on
this set.
In a misplaced analogy to the real numbers, a ﬂoating-point number is also
called “real”. Both computer “integers”, II, and “reals”, IF, represent useful
subsets of the corresponding mathematical entities, ZZ and IR, but while the
computer numbers called “integers” do constitute a fairly simple subset of the
integers, the computer numbers called “real” do not correspond to the real
numbers in a natural way. In particular, the ﬂoating-point numbers do not
occur uniformly over the real number line.

10.1 Digital Representation of Numeric Data
469
Within the allowable range, a mathematical integer is exactly represented
by a computer ﬁxed-point number, but a given real number, even a rational,
of any size may or may not have an exact representation by a ﬂoating-point
number. This is the familiar situation where fractions such as 1
3 have no ﬁnite
representation in base 10. The simple rule, of course, is that the number must
be a rational number whose denominator in reduced form factors into only
primes that appear in the factorization of the base. In base 10, for exam-
ple, only rational numbers whose factored denominators contain only 2s and
5s have an exact, ﬁnite representation; and in base 2, only rational numbers
whose factored denominators contain only 2s have an exact, ﬁnite represen-
tation.
For a given real number x, we will occasionally use the notation
[x]c
to indicate the ﬂoating-point number used to approximate x, and we will refer
to the exact value of a ﬂoating-point number as a computer number. We will
also use the phrase “computer number” to refer to the value of a computer
ﬁxed-point number. It is important to understand that the sets of computer
numbers II and IF are ﬁnite. The set of ﬁxed-point numbers II is a proper subset
of ZZ. The set of ﬂoating-point numbers is almost a proper subset of IR, but
it is not a subset because it contains some numbers not in IR; see the special
ﬂoating-point numbers discussed on page 475.
Our main task in using computers, of course, is not to evaluate functions
of the set of computer ﬂoating-point numbers or the set of computer integers;
the main immediate task usually is to perform operations in the ﬁeld of real
(or complex) numbers or occasionally in the ring of integers. Doing computa-
tions on the computer, then, involves using the sets of computer numbers to
simulate the sets of reals or integers.
10.1.2.1 The Parameters of the Floating-Point Representation
The parameters necessary to deﬁne a ﬂoating-point representation are the
base or radix, the range of the mantissa or signiﬁcand, and the range of the
exponent. Because the number is to be represented in a ﬁxed number of bits,
such as one storage unit or word, the ranges of the signiﬁcand and exponent
must be chosen judiciously so as to ﬁt within the number of bits available. If
the radix is b and the integer digits di are such that 0 ≤di < b, and there
are enough bits in the signiﬁcand to represent p digits, then a real number is
approximated by
± 0.d1d2 · · · dp × be,
(10.1)
where e is an integer. This is the standard model for the ﬂoating-point repre-
sentation. (The di are called “digits” from the common use of base 10.)
The number of bits allocated to the exponent e must be suﬃcient to rep-
resent numbers within a reasonable range of magnitudes; that is, so that the

470
10 Numerical Methods
smallest number in magnitude that may be of interest is approximately bemin
and the largest number of interest is approximately bemax, where emin and emax
are, respectively, the smallest and the largest allowable values of the exponent.
Because emin is likely negative and emax is positive, the exponent requires a
sign. In practice, most computer systems handle the sign of the exponent by
deﬁning a bias and then subtracting the bias from the value of the exponent
evaluated without regard to a sign.
The parameters b, p, and emin and emax are so fundamental to the oper-
ations of the computer that on most computers they are ﬁxed, except for a
choice of two or three values for p and maybe two choices for the range of e.
In order to ensure a unique representation for all numbers (except 0),
most ﬂoating-point systems require that the leading digit in the signiﬁcand
be nonzero unless the magnitude is less than bemin. A number with a nonzero
leading digit in the signiﬁcand is said to be normalized.
The most common value of the base b is 2, although 16 and even 10 are
sometimes used. If the base is 2, in a normalized representation, the ﬁrst
digit in the signiﬁcand is always 1; therefore, it is not necessary to ﬁll that
bit position, and so we eﬀectively have an extra bit in the signiﬁcand. The
leading bit, which is not represented, is called a “hidden bit”. This requires a
special representation for the number 0, however.
In a typical computer using a base of 2 and 64 bits to represent one ﬂoating-
point number, 1 bit may be designated as the sign bit, 52 bits may be allocated
to the signiﬁcand, and 11 bits allocated to the exponent. The arrangement of
these bits is somewhat arbitrary, and of course the physical arrangement on
some kind of storage medium would be diﬀerent from the “logical” arrange-
ment. A common logical arrangement assigns the ﬁrst bit as the sign bit, the
next 11 bits as the exponent, and the last 52 bits as the signiﬁcand. (Com-
puter engineers sometimes label these bits as 0, 1, . . . , and then get confused
as to which is the ith bit. When we say “ﬁrst”, we mean “ﬁrst”, whether
an engineer calls it the “0th” or the “1st”.) The range of exponents for the
base of 2 in this typical computer would be 2,048. If this range is split evenly
between positive and negative values, the range of orders of magnitude of
representable numbers would be from −308 to 308. The bits allocated to the
signiﬁcand would provide roughly 16 decimal places of precision.
Figure 10.3 shows the bit pattern to represent the number 5, using b = 2,
p = 24, emin = −126, and a bias of 127, in a word of 32 bits. The ﬁrst bit on the
left is the sign bit, the next 8 bits represent the exponent, 129, in ordinary base
2 with a bias, and the remaining 23 bits represent the signiﬁcand beyond the
leading bit, known to be 1. (The binary point is to the right of the leading bit
that is not represented.) The value is therefore +1.01 × 22 in binary notation.
While in ﬁxed-point twos-complement representations there are consider-
able diﬀerences between the representation of a given integer and the nega-
tive of that integer (see Figs. 10.1 and 10.2), the only diﬀerence between the
ﬂoating-point representation of a number and its additive inverse is usually

10.1 Digital Representation of Numeric Data
471
0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Figure 10.3. The value 5 in a ﬂoating-point representation
just in one bit. In the example of Fig. 10.3, only the ﬁrst bit would be changed
to represent the number −5.
As mentioned above, the set of ﬂoating-point numbers is not uniformly
distributed over the ordered set of the reals. There are the same number of
ﬂoating-point numbers in the interval [bi, bi+1] as in the interval [bi+1, bi+2],
even though the second interval is b times as long as the ﬁrst. Figures 10.4,
10.5, 10.6 illustrate this.
. . .
0
2
2
20
21
Figure 10.4. The ﬂoating-point number line, nonnegative half
. . .
0
2
2
2
1
20
21
Figure 10.5. The ﬂoating-point number line, nonpositive half
. . .
0
4
8
16
32
Figure 10.6. The ﬂoating-point number line, nonnegative half; another view
The ﬁxed-point numbers, on the other hand, are uniformly distributed
over their range, as illustrated in Fig. 10.7.
. . .
0
4
8
16
32
Figure 10.7. The ﬁxed-point number line, nonnegative half
The density of the ﬂoating-point numbers is generally greater closer to
zero. Notice that if ﬂoating-point numbers are all normalized, the spacing
between 0 and bemin is bemin (that is, there is no ﬂoating-point number in that
open interval), whereas the spacing between bemin and bemin+1 is bemin−p+1.

472
10 Numerical Methods
Most systems do not require ﬂoating-point numbers less than bemin in mag-
nitude to be normalized. This means that the spacing between 0 and bemin can
be bemin−p, which is more consistent with the spacing just above bemin. When
these nonnormalized numbers are the result of arithmetic operations, the re-
sult is called “graceful” or “gradual” underﬂow.
The spacing between ﬂoating-point numbers has some interesting (and,
for the novice computer user, surprising!) consequences. For example, if 1 is
repeatedly added to x, by the recursion
x(k+1) = x(k) + 1,
the resulting quantity does not continue to get ever larger. Obviously, it could
not increase without bound because of the ﬁnite representation. It does not
even approach the largest number representable, however! (This is assuming
that the parameters of the ﬂoating-point representation are reasonable ones.)
In fact, if x is initially smaller in absolute value than bemax−p (approximately),
the recursion
x(k+1) = x(k) + c
will converge to a stationary point for any value of c smaller in absolute value
than bemax−p.
The way the arithmetic is performed would determine these values pre-
cisely; as we shall see below, arithmetic operations may utilize more bits than
are used in the representation of the individual operands.
The spacings of numbers just smaller than 1 and just larger than 1 are
particularly interesting. This is because we can determine the relative spac-
ing at any point by knowing the spacing around 1. These spacings at 1 are
sometimes called the “machine epsilons”, denoted ϵmin and ϵmax (not to be
confused with emin and emax deﬁned earlier). It is easy to see from the model
for ﬂoating-point numbers on page 469 that
ϵmin = b−p
and
ϵmax = b1−p;
see Fig. 10.8. The more conservative value, ϵmax, sometimes called “the ma-
chine epsilon”, ϵ or ϵmach, provides an upper bound on the rounding that
occurs when a ﬂoating-point number is chosen to represent a real number. A
ﬂoating-point number near 1 can be chosen within ϵmax/2 of a real number
that is near 1. This bound, 1
2b1−p, is called the unit roundoﬀ.
These machine epsilons are also called the “smallest relative spacing” and
the “largest relative spacing” because they can be used to determine the
relative spacing at the point x (Fig. 10.8).
If x is not zero, the relative spacing at x is approximately
x −(1 −ϵmin)x
x

10.1 Digital Representation of Numeric Data
473
. . .
0
1
4
1
2
min
1
max
2
Figure 10.8. Relative spacings at 1: “Machine Epsilons”
. . .
. . .
x
[[x]c
(1
min )[x]c]c
[(1 +
max )[x]c
[x]c]c
Figure 10.9. Relative spacings
or
(1 + ϵmax)x −x
x
.
Notice that we say “approximately”. First of all, we do not even know that x
is representable (Fig. 10.9). Although (1 −ϵmin) and (1 + ϵmax) are members
of the set of ﬂoating-point numbers by deﬁnition, that does not guarantee
that the product of either of these numbers and [x]c is also a member of the
set of ﬂoating-point numbers. However, the quantities [(1 −ϵmin)[x]c]c and
[(1 + ϵmax)[x]c]c are representable (by the deﬁnition of [·]c as a ﬂoating point
number approximating the quantity within the brackets); and, in fact, they
are respectively the next smallest number than [x]c (if [x]c is positive, or the
next largest number otherwise) and the next largest number than [x]c (if [x]c
is positive). The spacings at [x]c therefore are
[x]c −[(1 −ϵmin)[x]c]c
and
[(1 + ϵmax)[x]c −[x]c]c.
As an aside, note that this implies it is probable that
[(1 −ϵmin)[x]c]c = [(1 + ϵmin)[x]c]c.
In practice, to compare two numbers x and y, we must compare [x]c
and [y]c. We consider x and y diﬀerent if
[|y|]c < [|x|]c −[ϵmin[|x|]c]c
or if
[|y|]c > [|x|]c + [ϵmax[|x|]c]c.
The relative spacing at any point obviously depends on the value repre-
sented by the least signiﬁcant digit in the signiﬁcand. This digit (or bit) is
called the “unit in the last place”, or “ulp”. The magnitude of an ulp depends

474
10 Numerical Methods
of course on the magnitude of the number being represented. Any real number
within the range allowed by the exponent can be approximated within 1
2 ulp
by a ﬂoating-point number.
The subsets of numbers that we need in the computer depend on the kinds
of numbers that are of interest for the problem at hand. Often, however, the
kinds of numbers of interest change dramatically within a given problem.
For example, we may begin with integer data in the range from 1 to 50.
Most simple operations, such as addition, squaring, and so on, with these
data would allow a single paradigm for their representation. The ﬁxed-point
representation should work very nicely for such manipulations.
Something as simple as a factorial, however, immediately changes the
paradigm. It is unlikely that the ﬁxed-point representation would be able to
handle the resulting large numbers. When we signiﬁcantly change the range
of numbers that must be accommodated, another change that occurs is the
ability to represent the numbers exactly. If the beginning data are integers be-
tween 1 and 50, and no divisions or operations leading to irrational numbers
are performed, one storage unit would almost surely be suﬃcient to represent
all values exactly. If factorials are evaluated, however, the results cannot be
represented exactly in one storage unit and so must be approximated (even
though the results are integers). When data are not integers, it is usually ob-
vious that we must use approximations, but it may also be true for integer
data.
10.1.2.2 Standardization of Floating-Point Representation
Over the past several years, standards for ﬂoating-point representation have
evolved. In the mid 1980s, IEEE promulgated a standard, called the IEEE
Standard 754, that speciﬁes the exact layout of the bits in ﬂoating-point rep-
resentations. This standard, which also became the IEC 60559 Standard, was
modiﬁed slightly in 2008 (IEEE 2008), and is now sometimes referred to as
IEEE Standard 754-2008.
The IEEE Standard 754 speciﬁes characteristics for two diﬀerent preci-
sions, “single” and “double”. In both cases, the standard requires that the
radix be 2; hence, it is sometimes called the “binary standard”. For single
precision, p must be 24, emax must be 127, and emin must be −126. For dou-
ble precision, p must be 53, emax must be 1023, and emin must be −1022. The
standard also deﬁnes two additional precisions, “single extended” and “double
extended”. For each of the extended precisions, the standard sets bounds on
the precision and exponent ranges rather than specifying them exactly. The
extended precisions have larger exponent ranges and greater precision than
the corresponding precision that is not “extended”.
The standard also speciﬁes how a number is to be represented when it can-
not be represented exactly, that is, how rounding should occur. In practice,
this is most relevant in deﬁning the result of a numerical operation. The stan-
dard allows for round-to-nearest (with ties going to the smallest in absolute
value), round-up, and round-down, but it requires that the default rounding
be round-to-nearest.

10.1 Digital Representation of Numeric Data
475
Most of the computers developed in the past few years comply with the
standards, but it is up to the computer manufacturers to conform voluntarily
to these standards. We would hope that the marketplace would penalize the
manufacturers who do not conform.
Additional information about the IEEE standards for ﬂoating-point num-
bers can be found in Overton (2001).
10.1.2.3 Special Floating-Point Numbers
It is necessary to be able to represent certain special entities, such as inﬁnity,
indeterminate (0/0), or simply missing, which do not have ordinary represen-
tations in any base-digit system. Although 8 bits are available for the exponent
in the single-precision IEEE binary standard, emax = 127 and emin = −126.
This means there are two unused possible values for the exponent; likewise,
for the double-precision standard, there are two unused possible values for
the exponent. These extra possible values for the exponent allow us to rep-
resent certain special ﬂoating-point numbers. An exponent of emin −1 allows
us to handle 0 and the numbers between 0 and bemin unambiguously even
though there is a hidden bit (see the discussion above about normalization
and gradual underﬂow). The special number 0 is represented with an exponent
of emin −1 and a signiﬁcand of 00 . . .0.
An exponent of emax + 1 allows us to represent ±∞and indeterminate or
missing values. A ﬂoating-point number with this exponent and a signiﬁcand
of 0 represents ±∞(the sign bit determines the sign, as usual). A ﬂoating-
point number with this exponent and a nonzero signiﬁcand represents an
indeterminate value such as 0
0. This value is called “not-a-number”, or NaN.
There are various ways of representing NaNs. In statistical data processing, a
NaN is sometimes used to represent a missing value, but a particular software
system may use a speciﬁc NaN representation to distinguish diﬀerent kinds
of indeterminate values. (R does this, for example, using a special coding to
represent “not available”, NA. The concept of missing values can also apply
to other things that are not numbers, such as character data.)
Because a NaN is an indeterminate value, if a variable x has a value of
NaN, it is neither true that x = x nor that x ̸= x. Also, because a NaN
can be represented in diﬀerent ways, however, a programmer must be care-
ful in testing for NaNs. Some software systems provide explicit functions for
testing for a NaN. The IEEE binary standard recommended that a function
isnan be provided to test for a NaN. Cody and Coonen (1993) provide C
programs for isnan and other functions useful in working with ﬂoating-point
numbers. (R provides is.nan for this purpose, but also provides is.na for
values that have been designated as missing; that is, as not available or NA.
Both is.na(0/0) and is.nan(0/0) are true, but if x = NA, then is.na(x)
is true, but is.nan(x) is false.)
We discuss computations with ﬂoating-point numbers in Sect. 10.2.2

476
10 Numerical Methods
10.1.3 Language Constructs for Representing Numeric Data
Most general-purpose computer programming languages, such as Fortran, C,
and Python, provide constructs for the user to specify the type of represen-
tation for numeric quantities. Within the two basic types supported by the
computer hardware and described in the preceding sections, the computer lan-
guage may support additional types of numeric quantities, such as complex
numbers.
These speciﬁcations of the type are made in declaration statements that
are made at the beginning of some section of the program for which they
apply, or they are made by “casting” functions or operations.
The diﬀerence between ﬁxed-point and ﬂoating-point representations has
a conceptual basis that may correspond to the problem being addressed. The
diﬀerences between other kinds of representations often are not because of
conceptual diﬀerences; rather, they are the results of increasingly irrelevant
limitations of the computer. The reasons there are “short” and “long”, or
“signed” and “unsigned”, representations do not arise from the problem the
user wishes to solve; the representations are to allow more eﬃcient use of
computer resources. The software designer nowadays generally eschews the
space-saving constructs that apply to only a relatively small proportion of
the data. In some applications, however, the short representations of numeric
data still have a place.
10.1.3.1 C
In C, the types of all variables must be speciﬁed with a basic declarator, which
may be qualiﬁed further. For variables containing numeric data, the possible
types are shown in Table 10.1.
Table 10.1. Numeric data types in C
Basic type
Basic
Fully qualiﬁed
declarator
declarator
Fixed-point
int
signed short int
unsigned short int
signed long int
unsigned long int
signed long long int
unsigned long long int
Floating-point float
double
double
long double
Exactly what these types mean is not speciﬁed by the language but de-
pends on the speciﬁc implementation, which associates each type with some

10.1 Digital Representation of Numeric Data
477
natural type supported by the speciﬁc computer. Common storage for a ﬁxed-
point variable of type short int uses 16 bits and for type long int uses 32
bits. An unsigned quantity of either type speciﬁes that no bit is to be used as a
sign bit, which eﬀectively doubles the largest representable number. Of course,
this is essentially irrelevant for scientiﬁc computations, so unsigned integers
are generally just a nuisance. If neither short nor long is speciﬁed, there is a
default interpretation that is implementation-dependent. The default always
favors signed over unsigned. There is a movement toward standardization
of the meanings of these types. The American National Standards Institute
(ANSI) and its international counterpart, the International Organization for
Standardization (ISO), have speciﬁed standard deﬁnitions of several program-
ming languages. ANSI (1989) is a speciﬁcation of the C language, which we
will refer to as “ANSI C”. The ISO has promulgated revisions of the C stan-
dard, called “C99” and “C11”, corresponding to the years the standards were
adopted. These standards are essentially backward-compatible with ANSI C,
which remains the most widely used version. C11 has added an additional
numeric type, called long long int.
Standards for computer languages usually provide minimum speciﬁcations,
rather than exact meanings of the various data types. ANSI C as well as C99
and C11 require that short int use at least 16 bits, that long int use at
least 32 bits, and that long int be at least as long as int, which in turn
must be least as long as short int. The long double type may or may not
have more precision and a larger range than the double type. C11 has added
an additional numeric type, called long long int, which must use at least
64 bits.
The object-oriented hybrid language built on C, C++ (ANSI 1998), pro-
vides the user with the ability also to deﬁne operator functions, so that the four
simple arithmetic operations can be implemented by the operators “+”, “−”,
“∗”, and “/”. There is no good way of deﬁning an exponentiation operator,
however, because the user-deﬁned operators are limited to extended versions
of the operators already deﬁned in the language. (The ISO has also promul-
gated revisions of the C++ standard, called “C++03”, “C++11”, “C++14”,
and “C++17”, corresponding to the years the standards were adopted. The
revisions add types among other things, but ANSI C++ remains the most
widely-used version.)
10.1.3.2 Fortran
Fortran provides three intrinsic numeric data types (and also two additional
intrinsic types for logical and character quantities), and within each type pro-
vides kinds. One intrinsic type, INTEGER, corresponds to a standard ﬁxed-point
entity, and another intrinsic type, REAL, corresponds to a standard ﬂoating-
point entity. The exact form of the storage, including the number of bits, can
vary from one platform to another. A third intrinsic type, COMPLEX, corre-
sponds to a doubleton of two units of type REAL. Arithmetic over numbers

478
10 Numerical Methods
of type COMPLEX, including also numbers of type REAL, conform to the usual
rules of arithmetic in the complex ﬁeld, within the usual limitations of arith-
metic in IF. Within each of the ﬁve intrinsic types (including LOGICAL and
CHARACTER), Fortran provides for various kinds; for example within the real
type, there must be at least two kinds available, a default kind and a kind
with greater precision (usually “double precision”). The declarator for double
precision is of the form
REAL(KIND=LONG)
There are many other kinds available for the various data types.
In Fortran, variables have a default numeric type that depends on the ﬁrst
letter in the name of the variable. The type can be explicitly declared (and, in
fact, should be in careful programming). The signed and unsigned qualiﬁers
of C, which have very little use in scientiﬁc computing, are missing in Fortran.
Basic types for variables containing numeric data are shown in Table 10.2.
Table 10.2. Numeric data types in Fortran
Basic type
Basic
Default
declarator
variable name
Fixed-point
INTEGER
Begin with i–n or I–N
Floating-point REAL
Begin with a–h or o–z
or with A–H or O–Z
REAL(KIND=LONG)
No default, although
d or D is sometimes used
Complex
COMPLEX
No default, although
c or C is sometimes used
COMPLEX(KIND=LONG)
Although the standards organizations have deﬁned these constructs for the
Fortran language, just as is the case with C, exactly what these types mean
is not speciﬁed by the language but depends on the speciﬁc implementation,
although in most implementations, the number of bytes to be used for storage
can be speciﬁed through the KIND keyword.
The kind is a qualiﬁer for the basic type; thus a ﬁxed-point number may be
an INTEGER of kind 1 or kind 2, for example. The actual value of the qualiﬁer
kind may diﬀer from one compiler to another, so the user deﬁnes a program pa-
rameter to be the kind that is appropriate to the range and precision required
for a given variable. Fortran provides the functions SELECTED INT KIND and
SELECTED REAL KIND to do this. Thus, to declare some ﬁxed-point variables
that have at least three decimal digits and some more ﬁxed-point variables
that have at least eight decimal digits, the user may write the following state-
ments:

10.1 Digital Representation of Numeric Data
479
INTEGER, PARAMETER
:: little = SELECTED_INT_KIND(3)
INTEGER, PARAMETER
:: big
= SELECTED_INT_KIND(8)
INTEGER (little)
:: ismall, jsmall
INTEGER (big)
:: itotal_accounts, igain
The variables little and big would have integer values, chosen by the com-
piler designer, that could be used in the program to qualify integer types to en-
sure that range of numbers could be handled. Thus, ismall and jsmall would
be ﬁxed-point numbers that could represent integers between −999 and 999,
and itotal accounts and igain would be ﬁxed-point numbers that could
represent integers between −99,999,999 and 99,999,999. Depending on the ba-
sic hardware, the compiler may assign two bytes as kind = little, meaning
that integers between −32,768 and 32,767 could probably be accommodated
by any variable, such as ismall, that is declared as integer (little). Like-
wise, it is probable that the range of variables declared as integer (big)
could handle numbers in the range −2,147,483,648 and 2,147,483,647. For
declaring ﬂoating-point numbers, the user can specify a minimum range and
precision with the function SELECTED REAL KIND, which takes two arguments,
the number of decimal digits of precision and the exponent of 10 for the range.
Thus, the statements
INTEGER, PARAMETER
:: real4 = SELECTED_REAL_KIND(6,37)
INTEGER, PARAMETER
:: real8 = SELECTED_REAL_KIND(15,307)
would yield designators of ﬂoating-point types that would have either six
decimals of precision and a range up to 1037 or ﬁfteen decimals of precision
and a range up to 10307. The statements
REAL (real4)
:: x, y
REAL (real8)
:: dx, dy
declare x and y as variables corresponding roughly to REAL on most systems
and dx and dy as variables corresponding roughly to REAL(KIND=LONG (or
DOUBLE PRECISION).
If the system cannot provide types matching the requirements speciﬁed in
SELECTED INT KIND or SELECTED REAL KIND, these functions return −1. Be-
cause it is not possible to handle such an error situation in the declaration
statements, the user should know in advance the available ranges. Fortran pro-
vides a number of intrinsic functions, such as EPSILON, RRSPACING, and HUGE,
to use in obtaining information about the ﬁxed- and ﬂoating-point numbers
provided by the system (see Table 10.3).
As with other language standards, the Fortran standard provides minimum
speciﬁcations, rather than exact meanings of the various data types. Metcalf
et al. (2011) summarize the Fortran standard speciﬁcations for the various
data types.
Fortran also provides a number of intrinsic functions for dealing with bits.
These functions are essentially those speciﬁed in the MIL-STD-1753 standard
of the U.S. Department of Defense. These bit functions, which have been

480
10 Numerical Methods
a part of many Fortran implementations for years, provide for shifting bits
within a string, extracting bits, exclusive or inclusive oring of bits, and so on.
(See ANSI 1992, or Lemmon and Schafer 2005, for more extensive discussions
of the intrinsic functions provided in Fortran.)
10.1.3.3 Python
The standards organizations have not deﬁned a standard for any version of
Python. Furthermore, the language has changed in incompatible ways; in par-
ticular, from the previous version into the current Python Version 3. Some of
the changes aﬀected the constructs for representing numeric data.
Python has supports three diﬀerent numerical types, an integer type, a
ﬂoating-point type, and a complex type. The integer type comes in two ver-
sions, which are not noticeable to the user. For integral values that can be ac-
commodated in the usual integer representation of the computer, the Python
integer type, int is essentially a signed long int, and arithmetic operations are
performed directly in the arithmetic unit of the computer. (The meaning of
the int type changed from Python 2 to Python 3.) For integral values outside
of the range that can be accommodated in the usual integer representation,
Python sets up exact computations on separate parts of the integer values.
The limits on the sizes of the integers depend only on the hardware resources
available. The ﬂoating-point type is the same as the native double precision
type of the computer (the same as double in C), and arithmetic operations are
performed directly in the arithmetic unit of the computer. The complex type
is composed of a doubleton of double precision ﬂoating-point numbers (the
same as COMPLEX(KIND=LONG) in Fortran), and Python sets up computations
that correspond to the rules of arithmetic on the complex numbers.
10.1.3.4 Determining the Numerical Characteristics
of a Particular Computer
The environmental inquiry program MACHAR by Cody (1988) can be used to
determine the characteristics of a speciﬁc computer’s ﬂoating-point represen-
tation and its arithmetic. Although it may be of interest to examine the source
code to see how methods to determine these characteristics work (the program
is available in CALGO from netlib, see page 619 in the Bibliography), mod-
ern systems for numerical computations provide functions to determine the
characteristics directly. In R, the results on a given system are stored in the
variable .Machine. Other R objects that provide information on a computer’s
characteristics are the variable .Platform and the function capabilities.
Modern Fortran provides a number of functions for inquiring about many of
the characteristics of the computer. The standard Fortran intrinsic functions
for numeric inquiry are shown in Table 10.3.

10.1 Digital Representation of Numeric Data
481
Table 10.3. Fortran numeric inquiry intrinsic functions
Generic intrinsic name
Description
DIGITS(x)
Number of signiﬁcant digits
EPSILON(x)
“Machine epsilon”, ϵmax
HUGE(x)
Largest number
MAXEXPONENT(x)
Maximum exponent
MINEXPONENT(x)
Minimum exponent
PRECISION(x)
Decimal precision
RADIX(x)
Base of the model
RANGE(x)
Decimal exponent range
TINY(x)
Smallest positive number
RRSPACING(x)
Reciprocal of the relative spacing of numbers near x
SPACING(x)
Absolute spacing of numbers near x
Value returned is for numbers of the same KIND as x
Many higher-level languages and application software packages do not give
the user a choice of how to represent numeric data. The software system may
consistently use a type thought to be appropriate for the kinds of applications
addressed. For example, many statistical analysis application packages choose
to use a ﬂoating-point representation with about 64 bits for all numeric data.
Making a choice such as this yields more comparable results across a range of
computer platforms on which the software system may be implemented.
Whenever the user chooses the type and precision of variables, it is a
good idea to use some convention to name the variable in such a way as to
indicate the type and precision. Books or courses on elementary programming
suggest using mnemonic names, such as “time”, for a variable that holds the
measure of time. If the variable takes ﬁxed-point values, a better name might
be “itime”. It still has the mnemonic value of “time”, but it also helps us to
remember that, in the computer, itime/length may not be the same thing as
time/xlength. Although the variables are declared in the program to be of a
speciﬁc type, the programmer can beneﬁt from a reminder of the type. Even
as we “humanize” computing, we must remember that there are details about
the computer that matter. (The operator “/” is said to be “overloaded”: in
a general way, it means “divide”, but it means diﬀerent things depending on
the contexts of the two expressions above.) Whether a quantity is a member
of II or IF may have major consequences for the computations, and a careful
choice of notation can help to remind us of that, even if the notation may look
old-fashioned.
Numerical analysts sometimes use the phrase “full precision” to refer to
a precision of about sixteen decimal digits and the phrase “half precision”to
refer to a precision of about seven decimal digits. These terms are not deﬁned
precisely, but they do allow us to speak of the precision in roughly equivalent
ways for diﬀerent computer systems without specifying the precision exactly.
Full precision is roughly equivalent to Fortran REAL(KIND=LONG) (or DOUBLE

482
10 Numerical Methods
PRECISION) on 32-bit computers, and to Fortran REAL on 64-bit machines.
Half precision corresponds roughly to Fortran REAL on 32-bit machines. Full
and half precision can be handled in a portable way in Fortran. The following
statements declare a variable x to be one with full precision:
INTEGER, PARAMETER
:: full = SELECTED\_REAL\_KIND(15,307)
REAL(full)
:: x
In a construct of this kind, the user can deﬁne “full” or “half” as appropriate.
10.1.4 Other Variations in the Representation of Data;
Portability of Data
As we have indicated already, computer designers have a great deal of latitude
in how they choose to represent data. The ASCII standards of ANSI and
ISO have provided a common representation for individual characters. The
IEEE Standard 754-2008 referred to previously (IEEE, 2008) brought some
standardization to the representation of ﬂoating-point data, but do not specify
how the available bits are to be allocated among the sign, exponent, and
signiﬁcand.
Because the number of bits used as the basic storage unit has generally
increased over time, some computer designers have arranged small groups of
bits, such as bytes, together in strange ways to form words. There are two
common schemes of organizing bits into bytes and bytes into words. In one
scheme, called “big end” or “big endian”, the bits are indexed from the “left”,
or most signiﬁcant, end of the byte, and bytes are indexed within words and
words are indexed within groups of words in the same direction.
In another scheme, called “little end” or “little endian”, the bytes are in-
dexed within the word in the opposite direction. Here’s a program in Fig. 10.10
that produces diﬀerence output on diﬀerent systems. Figures 10.11 and 10.12
illustrate some of the diﬀerences, using the program shown in Fig. 10.10.
The R function .Platform provides information on the type of endian of the
given machine on which the program is running.
These diﬀerences are important only when accessing the individual bits
and bytes, when making data type transformations directly, or when mov-
ing data from one machine to another without interpreting the data in the
process (“binary transfer”). One lesson to be learned from observing such
subtle diﬀerences in the way the same quantities are treated in diﬀerent com-
puter systems is that programs should rarely rely on the inner workings of
the computer. A program that does will not be portable; that is, it will not
give the same results on diﬀerent computer systems. Programs that are not
portable may work well on one system, and the developers of the programs
may never intend for them to be used anywhere else. As time passes, however,
systems change or users change systems. When that happens, the programs

10.2 Computer Operations on Numeric Data
483
Figure 10.10. A Fortran program illustrating bit and byte organization
Figure 10.11. Output from a Little Endian System
Figure 10.12. Output from a Big Endian System
that were not portable may cost more than they ever saved by making use of
computer-speciﬁc features.
The external data representation, or XDR, standard format, developed by
Sun Microsystems for use in remote procedure calls, is a widely used machine-
independent standard for binary data structures.
10.2 Computer Operations on Numeric Data
As we have emphasized above, the numerical quantities represented in the
computer are used to simulate or approximate more interesting quantities,
namely the real numbers or perhaps the integers. Obviously, because the sets
(computer numbers and real numbers) are not the same, we could not de-
ﬁne operations on the computer numbers that would yield the same ﬁeld as
the familiar ﬁeld of the reals. In fact, because of the nonuniform spacing of
ﬂoating-point numbers, we would suspect that some of the fundamental prop-
erties of a ﬁeld may not hold. Depending on the magnitudes of the quantities

484
10 Numerical Methods
involved, it is possible, for example, that if we compute ab and ac and then
ab + ac, we may not get the same thing as if we compute (b + c) and then
a(b + c). Just as we use the computer quantities to simulate real quantities,
we deﬁne operations on the computer quantities to simulate the familiar oper-
ations on real quantities. Designers of computers attempt to deﬁne computer
operations so as to correspond closely to operations on real numbers, but we
must not lose sight of the fact that the computer uses a diﬀerent arithmetic
system.
The basic operational objective in numerical computing, of course, is that
a computer operation, when applied to computer numbers, yield computer
numbers that approximate the number that would be yielded by a certain
mathematical operation applied to the numbers approximated by the original
computer numbers. Just as we introduced the notation
[x]c
on page 469 to denote the computer ﬂoating-point number approximation to
the real number x, we occasionally use the notation
[◦]c
to refer to a computer operation that simulates the mathematical operation ◦.
Thus,
[+]c
represents an operation similar to addition but that yields a result in a set of
computer numbers. (We use this notation only where necessary for emphasis,
however, because it is somewhat awkward to use it consistently.) The failure
of the familiar laws of the ﬁeld of the reals, such as the distributive law cited
above, can be anticipated by noting that
[a]c [+]c [b]c ̸= [a + b]c,
or by considering the simple example in which all numbers are rounded to one
decimal place and so 1
3 + 1
3 ̸= 2
3 (that is, .3 + .3 ̸= .7).
The three familiar laws of the ﬁeld of the reals (commutativity of addition
and multiplication, associativity of addition and multiplication, and distribu-
tion of multiplication over addition) result in the independence of the order
in which operations are performed; the failure of these laws implies that the
order of the operations may make a diﬀerence. When computer operations
are performed sequentially, we can usually deﬁne and control the sequence
fairly easily. If the computer performs operations in parallel, the resulting
diﬀerences in the orders in which some operations may be performed can
occasionally yield unexpected results.
Because the operations are not closed, special notice may need to be taken
when the operation would yield a number not in the set. Adding two num-
bers, for example, may yield a number too large to be represented well by

10.2 Computer Operations on Numeric Data
485
a computer number, either ﬁxed-point or ﬂoating-point. When an operation
yields such an anomalous result, an exception is said to exist.
The computer operations for the two diﬀerent types of computer numbers
are diﬀerent, and we discuss them separately.
10.2.1 Fixed-Point Operations
The operations of addition, subtraction, and multiplication for ﬁxed-point
numbers are performed in an obvious way that corresponds to the similar
operations on the ring of integers. Subtraction is addition of the additive
inverse. (In the usual twos-complement representation we described earlier, all
ﬁxed-point numbers have additive inverses except −2k−1.) Because there is no
multiplicative inverse, however, division is not multiplication by the inverse.
The result of division with ﬁxed-point numbers is the result of division with
the corresponding real numbers rounded toward zero. This is not considered
an exception.
As we indicated above, the set of ﬁxed-point numbers together with addi-
tion and multiplication is not the same as the ring of integers, if for no other
reason than that the set is ﬁnite. Under the ordinary deﬁnitions of addition
and multiplication, the set is not closed under either operation. The computer
operations of addition and multiplication, however, are deﬁned so that the set
is closed. These operations occur as if there were additional higher-order bits
and the sign bit were interpreted as a regular numeric bit. The result is then
whatever would be in the standard number of lower-order bits. If the lost
higher-order bits are necessary, the operation is said to overﬂow. If ﬁxed-
point overﬂow occurs, the result is not correct under the usual interpretation
of the operation, so an error situation, or an exception, has occurred. Most
computer systems allow this error condition to be detected, but most software
systems do not take note of the exception. The result, of course, depends on
the speciﬁc computer architecture. On many systems, aside from the interpre-
tation of the sign bit, the result is essentially the same as would result from a
modular reduction. There are some special-purpose algorithms that actually
use this modiﬁed modular reduction, although such algorithms would not be
portable across diﬀerent computer systems.
10.2.2 Floating-Point Operations
As we have seen, real numbers within the allowable range may or may not
have an exact ﬂoating-point operation, and the computer operations on the
computer numbers may or may not yield numbers that represent exactly the
real number that would result from mathematical operations on the numbers.
If the true result is r, the best we could hope for would be [r]c. As we have
mentioned, however, the computer operation may not be exactly the same as
the mathematical operation being simulated, and furthermore, there may be
several operations involved in arriving at the result. Hence, we expect some
error in the result.

486
10 Numerical Methods
10.2.2.1 Errors
If the computed value is ˜r (for the true value r), we speak of the absolute
error,
|˜r −r|,
and the relative error,
|˜r −r|
|r|
(so long as r ̸= 0). An important objective in numerical computation obviously
is to ensure that the error in the result is small.
We will discuss error in ﬂoating-point computations further in Sect. 10.3.2.
10.2.2.2 Guard Digits and Chained Operations
Ideally, the result of an operation on two ﬂoating-point numbers would be the
same as if the operation were performed exactly on the two operands (consid-
ering them to be exact also) and the result was then rounded. Attempting to
do this would be very expensive in both computational time and complexity
of the software. If care is not taken, however, the relative error can be very
large. Consider, for example, a ﬂoating-point number system with b = 2 and
p = 4. Suppose we want to add 8 and −7.5. In the ﬂoating-point system, we
would be faced with the problem
8 : 1.000 × 23
7.5 : 1.111 × 22.
To make the exponents the same, we have
8 : 1.000 × 23
7.5 : 0.111 × 23 or
8 : 1.000 × 23
7.5 : 1.000 × 23.
The subtraction will yield either 0.0002 or 1.0002 × 20, whereas the correct
value is 1.0002 × 2−1. Either way, the absolute error is 0.510, and the relative
error is 1. Every bit in the signiﬁcand is wrong. The magnitude of the error
is the same as the magnitude of the result. This is not acceptable. (More
generally, we could show that the relative error in a similar computation could
be as large as b −1 for any base b.) The solution to this problem is to use one
or more guard digits. A guard digit is an extra digit in the signiﬁcand that
participates in the arithmetic operation. If one guard digit is used (and this
is the most common situation), the operands each have p + 1 digits in the
signiﬁcand. In the example above, we would have
8 : 1.0000 × 23
7.5 : 0.1111 × 23,

10.2 Computer Operations on Numeric Data
487
and the result is exact. In general, one guard digit can ensure that the relative
error is less than 2ϵmax. The use of guard digits requires that the operands
be stored in special storage units. Whenever multiple operations are to be
performed together, the operands and intermediate results can all be kept
in the special registers to take advantage of the guard digits or even longer
storage units. This is called chaining of operations.
10.2.2.3 Addition of Several Numbers
When several numbers xi are to be summed, it is likely that as the operations
proceed serially, the magnitudes of the partial sum and the next summand
will be quite diﬀerent. In such a case, the full precision of the next summand
is lost. This is especially true if the numbers are of the same sign. As we
mentioned earlier, a computer program to implement serially the algorithm
implied by ∞
i=1 i will converge to some number much smaller than the largest
ﬂoating-point number.
If the numbers to be summed are not all the same constant (and if they
are constant, just use multiplication!), the accuracy of the summation can
be increased by ﬁrst sorting the numbers and summing them in order of
increasing magnitude. If the numbers are all of the same sign and have roughly
the same magnitude, a pairwise “fan-in” method may yield good accuracy. In
the fan-in method, the n numbers to be summed are added two at a time to
yield ⌈n/2⌉partial sums. The partial sums are then added two at a time, and
so on, until all sums are completed. The name “fan-in” comes from the tree
diagram of the separate steps of the computations:
s(1)
1
= x1 + x2
s(1)
2
= x3 + x4 . . . s(1)
2m−1 = x4m−3 + x4m−2 s(1)
2m = . . .
↘
↙
. . .
↘
↙
. . .
s(2)
1
= s(1)
1
+ s(1)
2
. . .
s(2)
m = s(1)
2m−1 + s(1)
2m
. . .
↘
. . .
↓
. . .
s(3)
1
= s(2)
1
+ s(2)
2
. . .
. . .
. . .
It is likely that the numbers to be added will be of roughly the same magnitude
at each stage. Remember we are assuming they have the same sign initially;
this would be the case, for example, if the summands are squares.
10.2.2.4 Compensated Summation
Another way of summing many numbers of varying magnitudes is due to W.
Kahan. It is called compensated summation, and for x1, . . . , xn, it follows these
steps:

488
10 Numerical Methods
s = x1
a = 0
for i = 2, . . . , n
{
y = xi −a
t = s + y
a = (t −s) −y
s = t
}.
(10.2)
Much of the work on improving the accuracy of summation was motivated
by the problem of computation of L2 norms, or more generally computation of
dot products. The exact dot product (EDP) procedure, described on page 495
addresses this problem. Both compensated summation and EDP implemented
in the hardware have been used in some versions of the BLAS. (See Sect. 12.2.1
on page 555 for descriptions of the BLAS.)
In Sect. 10.2.5 we discuss exact computation, and obviously any of the
methods of exact computation can also be used to increase the accuracy of
summations, and, indeed, in most cases, to make the summations exact.
10.2.2.5 Catastrophic Cancellation
Another kind of error that can result because of the ﬁnite precision used
for ﬂoating-point numbers is catastrophic cancellation. This can occur when
two rounded values of approximately equal magnitude and opposite signs are
added. (If the values are exact, cancellation can also occur, but it is benign.)
After catastrophic cancellation, the digits left are just the digits that repre-
sented the rounding. Suppose x ≈y and that [x]c = [y]c. The computed result
will be zero, whereas the correct (rounded) result is [x−y]c. The relative error
is 100%. This error is caused by rounding, but it is diﬀerent from the “round-
ing error” discussed above. Although the loss of information arising from the
rounding error is the culprit, the rounding would be of little consequence were
it not for the cancellation.
To avoid catastrophic cancellation, watch for possible additions of quanti-
ties of approximately equal magnitude and opposite signs, and rearrange the
computations if possible. Consider the problem of computing the roots of a
quadratic polynomial, ax2 + bx + c. In the quadratic formula
x = −b ±
√
b2 −4ac
2a
,
(10.3)
the square root of the discriminant, (b2 −4ac), may be approximately equal to
b in magnitude, meaning that one of the roots is close to zero and, in fact, may
be computed as zero. The solution is to compute only one of the roots, x1, by
the formula (the “−” root if b is positive and the “+” root if b is negative)
and then compute the other root, x2 by the relationship x1x2 = c/a.

10.2 Computer Operations on Numeric Data
489
In discussing Householder reﬂections, we have seen another example where
catastrophic cancellation could occur, and how we can avoid it; in particular,
in equation (5.8) on page 236.
10.2.2.6 Standards for Floating-Point Operations
The IEEE Binary Standard 754 (IEEE,2008) applies not only to the represen-
tation of ﬂoating-point numbers but also to certain operations on those num-
bers. The standard requires correct rounded results for addition, subtraction,
multiplication, division, remaindering, and extraction of the square root. It
also requires that conversion between ﬁxed-point numbers and ﬂoating-point
numbers yield correct rounded results.
An inexact operation is one for which the result must be rounded. For
example, in a multiplication operation, if all p bits of the signiﬁcand are
required to represent both the multiplier and multiplicand, approximately 2p
bits would be required to represent the product. Because only p are available,
however, the result generally must be rounded.
If a ﬂoating-point operation does not yield the exact result of the cor-
responding mathematical operation, then an exception is said to occur. In
this sense, inexact operations are exceptions; but that kind of exception is
generally ignored.
The IEEE Binary Standard deﬁnes how exceptions should be handled. The
exceptions are divided into four types: division of a nonzero number by zero,
overﬂow, underﬂow, and invalid operation.
Division by zero results in a special number if the dividend is nonzero. The
result is either ∞or −∞, and these have special representations, as we have
seen.
If an operation on ﬂoating-point numbers would result in a number beyond
the range of representable ﬂoating-point numbers, the exception, called over-
ﬂow, is generally very serious. (Overﬂow is serious in ﬁxed-point operations
also if it is unplanned. Because we have the alternative of using ﬂoating-point
numbers if the magnitude of the numbers is likely to exceed what is repre-
sentable in ﬁxed-point numbers, the user is expected to use this alternative. If
the magnitude exceeds what is representable in ﬂoating-point numbers, how-
ever, the user must resort to some indirect means, such as scaling, to solve
the problem.)
Underﬂow occurs whenever the result is too small to be represented as a
normalized ﬂoating-point number. As we have seen, a nonnormalized repre-
sentation can be used to allow a gradual underﬂow.
An invalid operation is one for which the result is not deﬁned because of
the values of the operands. The invalid operations are addition of ∞to −∞,
multiplication of ±∞and 0, 0 divided by 0 or by ±∞, ±∞divided by 0 or by
±∞, extraction of the square root of a negative number (some systems, such
as Fortran, R, and Matlab, have a special type for complex numbers and deal
correctly with them), and remaindering any quantity with 0 or remaindering

490
10 Numerical Methods
±∞with any quantity. An invalid operation results in a NaN. The IEEE
Binary Standard 754 provides for two classes of NaNs, a “quiet NaN” and a
“signaling NaN”, for which the processing unit will raise an exception; that
is, set a condition that can be queried by a program. (As it turns out, this
feature is rarely useful except in debugging programs.)
Conformance to the IEEE Binary Standard 754 does not ensure that the
results of multiple ﬂoating-point computations will be the same on all com-
puters. The standard does not specify the order of the computations, and
diﬀerences in the order can change the results. The slight diﬀerences are usu-
ally unimportant, but Blackford et al. (1997a) describe some examples of
problems that occurred when computations were performed in parallel using
a heterogeneous network of computers all of which conformed to the IEEE
standard. See also Gropp (2005) for further discussion of some of these issues.
10.2.2.7 Operations Involving Special Floating-Point Numbers
Operations involving any of the special ﬂoating-point numbers result in values
consistent with the meaning of the special value. For example, ±∞+1 = ±∞,
±∞∗1 = ±∞, and ±∞∗(−1) = ∓∞.
Any operation involving a single NaN results in a NaN of that same type.
In systems that distinguish various types of NaNs, an operation involving
diﬀerent types results in a value consistent with a hierarchy of NaNs. (In such
cases, the terminology may diﬀer from that of the IEEE Binary Standard. For
example, in R, any NaN is an NA, but an NA is not a NaN. Adding an NaN
to an NA results in an NA, consistent with the relationship of not-available
to not-a-number.)
10.2.2.8 Comparison of Real Numbers and Floating-Point
Numbers
For most applications, the system of ﬂoating-point numbers simulates the ﬁeld
of the reals very well. It is important, however, to be aware of some of the
diﬀerences in the two systems. There is a very obvious useful measure for
the reals, namely the Lebesgue measure, based on lengths of open intervals.
An approximation of this measure is appropriate for ﬂoating-point numbers,
even though the set of ﬂoating-point numbers within any interval is ﬁnite.
(Because the set is ﬁnite, we might consider basing a measure on a counting
measure, but a counting measure does not work well at all. For one thing,
the ﬁniteness of the set of ﬂoating-point numbers means that there may be a
diﬀerence in the cardinality of an open interval and a closed interval with the
same endpoints. Also, the uneven distribution of ﬂoating-point values relative
to the reals (Figs. 10.4 and 10.5) means that the cardinalities of two interval-
bounded sets with the same interval length may be diﬀerent.)

10.2 Computer Operations on Numeric Data
491
Some general diﬀerences in the real numbers and the ﬂoating-point sys-
tem are exhibited in Table 10.4. The last four properties in Table 10.4 are
properties of a ﬁeld. The important facts are that IR is an uncountable ﬁeld
and that IF is a more complicated ﬁnite mathematical structure.
10.2.3 Language Constructs for Operations on
Numeric Data
Most general-purpose computer programming languages provide constructs
for operations that correspond to the common operations on scalar numeric
data, such as “+”, “-”, “*” (multiplication), and “/”. These operators simulate
the corresponding mathematical operations. As we mentioned on page 484,
we will occasionally use notation such as
[+]c
to indicate the computer operator. The operators have slightly diﬀerent mean-
ings depending on the operand objects; that is, the operations are “over-
loaded”. Most of these operators are binary inﬁx operators, meaning that the
operator is written between the two operands.
Most languages provide operations beyond the four basic scalar arithmetic
operations, especially for exponentiation, usually speciﬁed as “**” or “^”. (In
C, exponentiation is handled by a function provided in a standard supple-
mental library <math.h>.) Exactly what the exponentiation operator means
may be slightly diﬀerent. Some versions of Fortran, for example, interpret the
operator always to mean
1. take log
2. multiply by power
3. exponentiate
if the base and the power are both ﬂoating-point types. This, of course, will
not work if the base is negative, even if the power is an integer. Most ver-
sions of Fortran will determine at run time if the power is an integer and use
repeated multiplication if it is.
C provides some specialized operations, such as the unary preﬁx and post-
ﬁx increment “++” and decrement “--” operators, for adding or subtracting 1.
Although these operations can easily be performed directly by standard oper-
ators, the operators look cool, and some other languages have adopted them.
Related augmented assignment operators, “+=”, “-=”, and so on, were also
deﬁned in C, and adopted by some other languages.
C also overloads the basic multiplication operator so that it can indicate
a change of meaning of a variable in addition to indicating the multiplication
of two scalar numbers. A standard library in C (<signal.h>) allows easy
handling of arithmetic exceptions. With this facility, for example, the user
can distinguish a quiet NaN from a signaling NaN.

492
10 Numerical Methods
Table 10.4. Diﬀerences in real numbers and ﬂoating-point numbers
IR
IF
Cardinality:
Uncountable
Finite
Measure:
μ((x, y)) = |x −y|
ν((x, y)) = ν([x, y]) = |x −y|
μ((x, y)) = μ([x, y])
∃x, y, z, w ∋|x −y| = |z −w|,
but #(x, y) ̸= #(z, w)
Continuity:
if x < y, ∃z ∋x < z < y
x < y, but no z ∋x < z < y
and
and
μ([x, y]) = μ((x, y))
#[x, y] > #(x, y)
Convergence
∞
x=1 x
diverges
∞
x=1 x
converges,
if interpreted as
(· · · ((1 + 2) + 3) · · · )
Closure:
x, y ∈IR ⇒x + y ∈IR
Not closed wrt addition
x, y ∈IR ⇒xy ∈IR
Not closed wrt multiplication
(exclusive of inﬁnities)
Operations
a = 0, unique
a + x = b + x, but b ̸= a
with an
a + x = x, for any x
a + x = x, but a + y ̸= y
identity, a or a:
x −x = a, for any x
a + x = x, but x −x ̸= a
Associativity:
x, y, z ∈IR ⇒
(x + y) + z = x + (y + z) Not associative
(xy)z = x(yz)
Not associative
Distributivity:
x, y, z ∈IR ⇒
x(y + z) = xy + xz
Not distributive
Fortran and Python provide the usual ﬁve operators (the basic four plus
exponentiation) for complex data . As mentioned before, the actual arithmetic

10.2 Computer Operations on Numeric Data
493
is set up to use the ordinary arithmetic operations in such a way as to conform
to the rules of complex arithmetic.
Modern Fortran also provides numeric operators for vectors and matrices.
The usual vector/matrix operators are implemented as intrinsic functions or
as preﬁx operators in Fortran.
In addition to the basic arithmetic operators, the common programming
languages provide several other types of operators, including relational oper-
ators and operators for manipulating structures of data.
10.2.4 Software Methods for Extending the Precision
Software packages have been built to extend the numerical accuracy of com-
putations, possibly to the extent that the computations are exact. Four ways
in which this is done are by use of multiple precision, rational fractions, in-
terval arithmetic, and residue arithmetic. We discuss the ﬁrst three of these
in this section, and then in the next section brieﬂy discuss residue arithmetic,
which may also be implemented at the microcode or hardware level.
10.2.4.1 Multiple Precision
Multiple-precision operations are performed in the software by combining
more than one computer-storage unit to represent a single number. For ex-
ample, to operate on x and y, we may represent x as a · 10p + b and y as
c·10p +d. The product xy then is formed as ac·102p +(ad+bc)·10p +bd. The
representation is chosen so that any of the coeﬃcients of the scaling factors
(in this case powers of 10) can be represented to within the desired accuracy.
Multiple precision is diﬀerent from “extended precision”, discussed earlier;
extended precision is implemented at the hardware level or at the microcode
level. A multiple-precision package may allow the user to specify the number
of digits to use in representing data and performing computations.
Bailey (1993, 1995)gives software for instrumenting Fortran code to use
multiple-precision operations. The C and C++ library GMP, mentioned ear-
lier, provides support for multiple precision ﬂoating-point numbers in its mpf
package. The software packages for symbolic computations, such as Maple or
Mathematica, generally provide multiple precision capabilities also.
10.2.4.2 Rational Fractions
Rational fractions are ratios of two integers. If the input data can be rep-
resented exactly as rational fractions, it may be possible to preserve exact
values of the results of computations. Using rational fractions allows avoid-
ance of reciprocation, which is the operation that most commonly yields a
nonrepresentable value from one that is representable. Of course, any addi-
tion or multiplication that increases the magnitude of an integer in a rational

494
10 Numerical Methods
fraction beyond a value that can be represented exactly (that is, beyond ap-
proximately 223, 231, or 253, depending on the computing system) may break
the error-free chain of operations.
Computations with rational fractions are often performed using a ﬁxed-
point representation. As mentioned earlier, the GMP library supports mathe-
matical operations involving big integers. This facility is what is needed to use
rational fractions to achieve high accuracy or even exactness in computations.
The mpq package in GMP facilitates use of big integers in operations involving
rational fractions.
10.2.4.3 Interval Arithmetic
Interval arithmetic maintains intervals in which the data and results of compu-
tations are known to lie. Instead of working with single-point approximations,
for which we used notation such as
[x]c
on page 469 for the value of the ﬂoating-point approximation to the real
number x and
[◦]c
on page 484 for the simulated operation ◦, we can approach the problem by
identifying a closed interval in which x lies and a closed interval in which the
result of the operation ◦lies. We denote the interval operation as
[◦]I.
For the real number x, we identify two ﬂoating-point numbers, xl and xu,
such that xl ≤x ≤xu. (This relationship also implies xl ≤[x]c ≤xu.) The
real number x is then considered to be the interval [xl, xu]. For this approach
to be useful, of course, we seek tight bounds. If x = [x]c, the best interval is
degenerate. In some other cases, either xl or xu is [x]c, and the length of the
interval is the ﬂoating-point spacing from [x]c in the appropriate direction.
Addition and multiplication in interval arithmetic yield the intervals
x [+]I y = [xl + yl, xu + yu]
and
x [∗]I y = [min(xlyl, xlyu, xuyl, xuyu), max(xlyl, xlyu, xuyl, xuyu)].
A change of sign results in [−xu, −xl] and if 0 ̸∈[xl, xu], reciprocation
results in [1/xu, 1/xl]. See Moore (1979) or Alefeld and Herzberger (1983)
for discussions of these kinds of operations and an extensive treatment of
interval arithmetic. The journal Reliable Computing is devoted to interval
computations. The book edited by Kearfott and Kreinovich (1996) addresses

10.2 Computer Operations on Numeric Data
495
various aspects of interval arithmetic. One chapter in that book, by Walster
(1996), discusses how both hardware and system software could be designed
to implement interval arithmetic.
Most software support for interval arithmetic is provided through subrou-
tine libraries. The ACRITH package of IBM (see Jansen and Weidner 1986) is
a library of Fortran subroutines that perform computations in interval arith-
metic and also in multiple precision. Kearfott et al. (1994) have produced a
portable Fortran library of basic arithmetic operations and elementary func-
tions in interval arithmetic, and Kearfott (1996) gives a Fortran 90 module
deﬁning an interval data type. Jaulin et al. (2001) describe additional sources
of software. Sun Microsystems Inc. provided full intrinsic support for interval
data types in their Fortran compiler SunTM ONE Studio Fortran 95 (now
distributed and maintained by Oracle, Inc.); see Walster (2005) for a descrip-
tion of the compiler extensions. The IEEE Standard P1788 speciﬁes interval
arithmetic operations based on intervals whose endpoints are IEEE binary64
ﬂoating-point numbers. The standard ensures exact propagation of properties
of the computed results.
10.2.5 Exact Computations
Computations involving integer quantities can easily be performed exactly
if the number of available bits to represent the integers is large enough. The
use of rational fractions described above is the most common way of achieving
exact computations. Gregory and Krishnamurthy (1984) discuss in detail these
and other methods for performing error-free computations.
Another way of performing exact computations is by using residue arith-
metic, in which each quantity is represented as a vector of residues, all from a
vector of relatively prime integer moduli. For details of the use of residue arith-
metic in numerical computations, we refer the reader to Szab´o and Tanaka
(1967), and for examples of applications of this technology in matrix com-
putations, we refer to Stallings and Boullion (1972) and Keller-McNulty and
Kennedy (1986).
10.2.5.1 Exact Dot Product (EDP)
As mentioned before, a common computational problem that motivates ef-
forts for exact (or at least highly accurate) computations is computing a dot
product. The ability to compute an exact dot product, or EDP, is desirable.
One approach, as indicated above, is to store every bit of the input ﬂoating-
point numbers in very long vectors of bits, called accumulators. Kulisch (2011)
proposed accumulators of 4288 bits each. The Kulisch accumulator can handle
the exact accumulation of products of 64-bit IEEE ﬂoating-point values. This
approach incurs a large memory overhead, however. Also, unless it is accom-
plished directly in the hardware, vectorization of computations would be very
diﬃcult. The IFIP Working Group 2.5 on Numerical Software has proposed
that the EDP be incorporated into a new IEEE 754 standard (see page 474).

496
10 Numerical Methods
10.3 Numerical Algorithms and Analysis
We will use the term “algorithm” rather loosely but always in the general
sense of a method or a set of instructions for doing something. (Formally,
an “algorithm” must terminate; however, respecting that deﬁnition would
not allow us to refer to a method as an algorithm until it has been proven
to terminate.) Algorithms are sometimes distinguished as “numerical”, “semi-
numerical”, and “nonnumerical”, depending on the extent to which operations
on real numbers are simulated.
10.3.1 Algorithms and Programs
Algorithms are expressed by means of a ﬂowchart, a series of steps, or in a
computer language or pseudolanguage. The expression in a computer language
is a source program or module; hence, we sometimes use the words “algorithm”
and “program” synonymously.
The program is the set of computer instructions that implement the algo-
rithm. A poor implementation can render a good algorithm useless. A good
implementation will preserve the algorithm’s accuracy and eﬃciency and will
detect data that are inappropriate for the algorithm. Robustness is more a
property of the program than of the algorithm.
The exact way an algorithm is implemented in a program depends of course
on the programming language, but it also may depend on the computer and
associated system software. A program that will run on most systems without
modiﬁcation is said to be portable.
The two most important aspects of a computer algorithm are its accuracy
and its eﬃciency. Although each of these concepts appears rather simple on
the surface, both are actually fairly complicated, as we shall see.
10.3.2 Error in Numerical Computations
An “accurate” algorithm is one that gets the “right” answer. Knowing that
the right answer may not be representable and that rounding within a set of
operations may result in variations in the answer, we often must settle for an
answer that is “close”. As we have discussed previously, we measure error, or
closeness, as either the absolute error or the relative error of a computation.
Another way of considering the concept of “closeness” is by looking back-
ward from the computed answer and asking what perturbation of the original
problem would yield the computed answer exactly. This approach, developed
by Wilkinson (1963), is called backward error analysis. The backward anal-
ysis is followed by an assessment of the eﬀect of the perturbation on the
solution. Although backward error analysis may not seem as natural as “for-
ward” analysis (in which we assess the diﬀerence between the computed and
true solutions), it is easier to perform because all operations in the backward

10.3 Numerical Algorithms and Analysis
497
analysis are performed in IF instead of in IR. Each step in the backward anal-
ysis involves numbers in the set IF, that is, numbers that could actually have
participated in the computations that were performed. Because the properties
of the arithmetic operations in IR do not hold and, at any step in the sequence
of computations, the result in IR may not exist in IF, it is very diﬃcult to carry
out a forward error analysis.
There are other complications in assessing errors. Suppose the answer is
a vector, such as a solution to a linear system. What norm do we use to
compare the closeness of vectors? Another, more complicated situation for
which assessing correctness may be diﬃcult is random number generation. It
would be diﬃcult to assign a meaning to “accuracy” for such a problem.
The basic source of error in numerical computations is the inability to work
with the reals. The ﬁeld of reals is simulated with a ﬁnite set. This has several
consequences. A real number is rounded to a ﬂoating-point number; the result
of an operation on two ﬂoating-point numbers is rounded to another ﬂoating-
point number; and passage to the limit, which is a fundamental concept in
the ﬁeld of reals, is not possible in the computer.
Rounding errors that occur just because the result of an operation is not
representable in the computer’s set of ﬂoating-point numbers are usually not
too bad. Of course, if they accumulate through the course of many operations,
the ﬁnal result may have an unacceptably large rounding error.
A natural approach to studying errors in ﬂoating-point computations is
to deﬁne random variables for the rounding at all stages, from the initial
representation of the operands, through any intermediate computations, to
the ﬁnal result. Given a probability model for the rounding error in the rep-
resentation of the input data, a statistical analysis of rounding errors can
be performed. Wilkinson (1963) introduced a uniform probability model for
rounding of input and derived distributions for computed results based on
that model. Linnainmaa (1975) discusses the eﬀects of accumulated errors in
ﬂoating-point computations based on a more general model of the rounding
for the input. This approach leads to a forward error analysis that provides a
probability distribution for the error in the ﬁnal result.
The obvious probability model for ﬂoating-point representations is that
the reals within an interval between any two ﬂoating-point numbers have a
uniform distribution (see Fig. 10.4 on page 471 and Calvetti 1991). A prob-
ability model for the real line can be built up as a mixture of the uniform
distributions (see Exercise 10.10 on page 519). The density is obviously 0 in
the tails. While a model based on simple distributions may be appropriate
for the rounding error due to the ﬁnite-precision representation of real num-
bers, probability models for rounding errors in ﬂoating point computations are
not so simple. This is because the rounding errors in computations are not
random. See Chaitin-Chatelin and Frayss´e (1996) for a further discussion of
probability models for rounding errors. Dempster and Rubin (1983) discuss
the application of statistical methods for dealing with grouped data to the
data resulting from rounding in ﬂoating-point computations.

498
10 Numerical Methods
Another, more pernicious, eﬀect of rounding can occur in a single oper-
ation, resulting in catastrophic cancellation, as we have discussed previously
(see page 488).
10.3.2.1 Measures of Error and Bounds for Errors
For the simple case of representing the real number r by an approximation
˜r, we deﬁne absolute error, |˜r −r|, and relative error, |˜r −r|/|r| (so long
as r ̸= 0). These same types of measures are used to express the errors in
numerical computations. As we indicated above, however, the result may not
be a simple real number; it may consist of several real numbers. For example,
in statistical data analysis, the numerical result, ˜r, may consist of estimates
of several regression coeﬃcients, various sums of squares and their ratio, and
several other quantities. We may then be interested in some more general
measure of the diﬀerence of ˜r and r,
Δ(˜r, r),
where Δ(·, ·) is a nonnegative, real-valued function. This is the absolute error,
and the relative error is the ratio of the absolute error to Δ(r, r0), where r0
is a baseline value, such as 0. When r, instead of just being a single number,
consists of several components, we must measure error diﬀerently. If r is a
vector, the measure may be based on some norm, and in that case, Δ(˜r, r)
may be denoted by ∥(˜r −r)∥. A norm tends to become larger as the number
of elements increases, so instead of using a raw norm, it may be appropriate
to scale the norm to reﬂect the number of elements being computed.
However the error is measured, for a given algorithm, we would like to
have some knowledge of the amount of error to expect or at least some bound
on the error. Unfortunately, almost any measure contains terms that depend
on the quantity being evaluated. Given this limitation, however, often we
can develop an upper bound on the error. In other cases, we can develop an
estimate of an “average error” based on some assumed probability distribution
of the data comprising the problem.
In a Monte Carlo method, we estimate the solution based on a “random”
sample, so one source of error in addition to any numerical computational
errors, is the “sampling error”. Just as in ordinary statistical estimation, we
are concerned about the variance of the estimate, which we relate to the
sampling error. We can usually derive expressions for the variance of the
estimator in terms of the quantity being evaluated, and of course we can
estimate the variance of the estimator using the realized random sample. The
standard deviation of the estimator provides an indication of the distance
around the computed quantity within which we may have some conﬁdence that
the true value lies. The standard deviation is sometimes called the “standard
error”, or the “probabilistic error bound”.
It is often useful to identify the “order of the error” whether we are con-
cerned about error bounds, average expected error, or the standard deviation

10.3 Numerical Algorithms and Analysis
499
of an estimator. In general, we speak of the order of one function in terms of
another function as a common argument of the functions approaches a given
value. A function f(t) is said to be of order g(t) at t0, written O(g(t)) (“big O
of g(t)”), if there exists a positive constant M such that
|f(t)| ≤M|g(t)|
as t →t0.
This is the order of convergence of one function to another function at a given
point.
If our objective is to compute f(t) and we use an approximation ˜f(t), the
order of the error due to the approximation is the order of the convergence.
In this case, the argument of the order of the error may be some variable that
deﬁnes the approximation. For example, if ˜f(t) is a ﬁnite series approximation
to f(t) using, say, k terms, we may express the error as O(h(k)) for some func-
tion h(k). Typical orders of errors due to the approximation may be O(1/k),
O(1/k2), or O(1/k!). An approximation with order of error O(1/k!) is to be
preferred over one order of error O(1/k) because the error is decreasing more
rapidly. The order of error due to the approximation is only one aspect to
consider; roundoﬀerror in the representation of any intermediate quantities
must also be considered.
We will discuss the order of error in iterative algorithms further in
Sect. 10.3.4 beginning on page 510. (We will discuss order also in measur-
ing the speed of an algorithm in Sect. 10.3.3.)
The special case of convergence to the constant zero is often of interest. A
function f(t) is said to be “little o of g(t)” at t0, written o(g(t)), if
f(t)/g(t) →0
as t →t0.
If the function f(t) approaches 0 at t0, g(t) can be taken as a constant and
f(t) is said to be o(1).
Big O and little o convergences are deﬁned in terms of dominating func-
tions. In the analysis of algorithms, it is often useful to consider analogous
types of convergence in which the function of interest dominates another func-
tion. This type of relationship is similar to a lower bound. A function f(t) is
said to be Ω(g(t)) (“big omega of g(t)”) if there exists a positive constant m
such that
|f(t)| ≥m|g(t)|
as t →t0.
Likewise, a function f(t) is said to be “little omega of g(t)” at t0, written
ω(g(t)), if
g(t)/f(t) →0
as t →t0.
Usually the limit on t in order expressions is either 0 or ∞, and because it
is obvious from the context, mention of it is omitted. The order of the error
in numerical computations usually provides a measure in terms of something
that can be controlled in the algorithm, such as the point at which an inﬁnite
series is truncated in the computations. The measure of the error usually also
contains expressions that depend on the quantity being evaluated, however.

500
10 Numerical Methods
10.3.2.2 Error of Approximation
Some algorithms are exact, such as an algorithm to multiply two matrices
that just uses the deﬁnition of matrix multiplication. In this case, there may
be numerical errors due to rounding or other computational events, but no
errors due to the computational approach. Here we focus on additional errors
that are due to the algorithms not being exact.
Algorithms may be approximate either because the algorithm itself is iter-
ative, such as one using the Gauss-Seidel method (page 280), or because the
algorithm makes use of approximations to the desired value, possibly because
the result to be computed does not have a ﬁnite closed-form expression. An
example of the latter is the evaluation of the normal cumulative distribution
function. One way of evaluating this is by using a rational polynomial approxi-
mation to the distribution function. Such an expression may be evaluated with
very little rounding error, but the expression has an error of approximation.
On the other hand, the component of the error in an iterative algorithm due
to eventually having to halt the iterations is an error of truncation. The trun-
cation error is also an error of approximation.
When solving a diﬀerential equation on the computer, the diﬀerential equa-
tion is often approximated by a diﬀerence equation. Even though the diﬀer-
ences used may not be constant, they are ﬁnite and the passage to the limit
can never be eﬀected. This kind of approximation leads to a discretization
error. The amount of the discretization error has nothing to do with rounding
error. If the last diﬀerences used in the algorithm are δt, then the error is
usually of order O(δt), even if the computations are performed exactly.
The type of error of approximation that occurs when an algorithm uses
a series expansion is similar to the error that occurs in using an iterative
algorithm. The series may be exact, and in principle the evaluation of all terms
would yield an exact result. The algorithm uses only a ﬁnite number of terms,
and the resulting component of the error is truncation error. This is the type of
error we discussed in connection with Fourier expansions on pages 42 and 99.
Often the exact expansion is an inﬁnite series, and we approximate it with
a ﬁnite series. When a truncated Taylor series is used to evaluate a function
at a given point x0, the order of the truncation error is the derivative of the
function that would appear in the ﬁrst unused term of the series, evaluated
at x0.
We need to have some knowledge of the magnitude of the error. For al-
gorithms that use approximations, it is often useful to express the order of
the error in terms of some quantity used in the algorithm or in terms of some
aspect of the problem itself. We must be aware, however, of the limitations
of such measures of the errors or error bounds. For an oscillating function,
for example, the truncation error may never approach zero over any nonzero
interval.

10.3 Numerical Algorithms and Analysis
501
10.3.2.3 Algorithms and Data
The performance of an algorithm may depend on the data. We have seen that
even the simple problem of computing the roots of a quadratic polynomial,
ax2 + bx + c, using the quadratic formula, equation (10.3), can lead to severe
cancellation. For many values of a, b, and c, the quadratic formula works per-
fectly well. Data that are likely to cause computational problems are referred
to as ill-conditioned data, and, more generally, we speak of the “condition”
of data. The concept of condition is understood in the context of a particular
set of operations. Heuristically, data for a given problem are ill-conditioned if
small changes in the data may yield large changes in the solution.
Consider the problem of ﬁnding the roots of a high-degree polynomial,
for example. Wilkinson (1959) gave an example of a polynomial that is very
simple on the surface yet whose solution is very sensitive to small changes of
the values of the coeﬃcients:
f(x) = (x −1)(x −2) · · · (x −20)
= x20 −210x19 + · · · + 20!.
(10.4)
While the solution is easy to see from the factored form, the solution is very
sensitive to perturbations of the coeﬃcients. For example, changing the coef-
ﬁcient 210 to 210+2−23 changes the roots drastically; in fact, ten of them are
now complex. Of course, the extreme variation in the magnitudes of the coeﬃ-
cients should give us some indication that the problem may be ill-conditioned.
10.3.2.4 Condition of Data
We attempt to quantify the condition of a set of data for a particular set of
operations by means of a condition number. Condition numbers are deﬁned
to be positive and in such a way that large values of the numbers mean
that the data or problems are ill-conditioned. A useful condition number for
the problem of ﬁnding roots of a diﬀerentiable function can be deﬁned to
be increasing as the reciprocal of the absolute value of the derivative of the
function in the vicinity of a root.
In the solution of a linear system of equations, the coeﬃcient matrix de-
termines the condition of the problem. The most commonly used condition
number for this problem is the one based on ratio of eigenvalues (or singular
values) that we discussed in Sect. 6.1.1 on page 267.
Condition numbers are only indicators of possible numerical diﬃculties for
a given problem. They must be used with some care. For example, according
to the condition number for ﬁnding roots based on the derivative, Wilkinson’s
polynomial is well-conditioned.

502
10 Numerical Methods
10.3.2.5 Robustness of Algorithms
The ability of an algorithm to handle a wide range of data and either to solve
the problem as requested or to determine that the condition of the data does
not allow the algorithm to be used is called the robustness of the algorithm.
10.3.2.6 Stability of Algorithms
Another concept that is quite diﬀerent from robustness is stability. An algo-
rithm is said to be stable if it always yields a solution that is an exact solution
to a perturbed problem; that is, for the problem of computing f(x) using the
input data x, an algorithm is stable if the result it yields, ˜f(x), is
f(x + δx)
for some (bounded) perturbation δx of x. Stated another way, an algorithm
is stable if small perturbations in the input or in intermediate computations
do not result in large diﬀerences in the results.
The concept of stability for an algorithm should be contrasted with the
concept of condition for a problem or a dataset. If a problem is ill-conditioned,
a stable algorithm (a “good algorithm”) will produce results with large dif-
ferences for small diﬀerences in the speciﬁcation of the problem. This is be-
cause the exact results have large diﬀerences. An algorithm that is not sta-
ble, however, may produce large diﬀerences for small diﬀerences in the com-
puter description of the problem, which may involve rounding, truncation,
or discretization, or for small diﬀerences in the intermediate computations
performed by the algorithm.
The concept of stability arises from backward error analysis. The stability
of an algorithm may depend on how continuous quantities are discretized,
such as when a range is gridded for solving a diﬀerential equation. See Higham
(2002) for an extensive discussion of stability.
10.3.2.7 Reducing the Error in Numerical Computations
An objective in designing an algorithm to evaluate some quantity is to avoid
accumulated rounding error and to avoid catastrophic cancellation. In the
discussion of ﬂoating-point operations above, we have seen two examples of
how an algorithm can be constructed to mitigate the eﬀect of accumulated
rounding error (using equations (10.2) on page 488 for computing a sum) and
to avoid possible catastrophic cancellation in the evaluation of the expres-
sion (10.3) for the roots of a quadratic equation.
Another example familiar to statisticians is the computation of the sample
sum of squares:
n

i=1
(xi −¯x)2 =
n

i=1
x2
i −n¯x2.
(10.5)

10.3 Numerical Algorithms and Analysis
503
This quantity is (n −1)s2, where s2 is the sample variance.
Either expression in equation (10.5) can be thought of as describing an
algorithm. The expression on the left-hand side implies the “two-pass” algo-
rithm:
a = x1
for i = 2, . . . , n
{
a = xi + a
}
a = a/n
b = (x1 −a)2
for i = 2, . . . , n
{
b = (xi −a)2 + b
}.
(10.6)
This algorithm yields ¯x = a and (n −1)s2 = b. Each of the sums computed
in this algorithm may be improved by using equations (10.2). A problem
with this algorithm is the fact that it requires two passes through the data.
Because the quantities in the second summation are squares of residuals, they
are likely to be of relatively equal magnitude. They are of the same sign, so
there will be no catastrophic cancellation in the early stages when the terms
being accumulated are close in size to the current value of b. There will be
some accuracy loss as the sum b grows, but the addends (xi −a)2 remain
roughly the same size. The accumulated rounding error, however, may not be
too bad.
The expression on the right-hand side of equation (10.5) implies the “one-
pass” algorithm:
a = x1
b = x2
1
for i = 2, . . . , n
{
a = xi + a
b = x2
i + b
}
a = a/n
b = b −na2.
(10.7)
This algorithm requires only one pass through the data, but if the xis have
magnitudes larger than 1, the algorithm has built up two relatively large
quantities, b and na2. These quantities may be of roughly equal magnitudes;
subtracting one from the other may lead to catastrophic cancellation (see
Exercise 10.17, page 520).
Another algorithm is shown in equations (10.8). It requires just one pass
through the data, and the individual terms are generally accumulated fairly

504
10 Numerical Methods
accurately. Equations (10.8) are a form of the Kalman ﬁlter (see, for example,
Grewal and Andrews 1993).
a = x1
b = 0
for i = 2, . . . , n
{
d = (xi −a)/i
a = d + a
b = i(i −1)d2 + b
}.
(10.8)
A useful measure to quantify the sensitivity of s, the sample standard
deviation, to the data, the xis, is the condition number
κ =
n
i=1 x2
i
√n −1s.
(10.9)
This is a measure of the “stiﬀness” of the data. It is clear that if the mean
is large relative to the variance, this condition number will be large. (Recall
that large condition numbers imply ill-conditioning, and also recall that condi-
tion numbers must be interpreted with some care.) Notice that this condition
number achieves its minimum value of 1 for the data xi −¯x, so if the computa-
tions for ¯x and xi −¯x were exact, the data in the last part of the algorithm in
equations (10.6) would be perfectly conditioned. A dataset with a large mean
relative to the variance is said to be stiﬀ.
Often when a ﬁnite series is to be evaluated, it is necessary to accumulate
a set of terms of the series that have similar magnitudes, and then combine
this with similar partial sums. It may also be necessary to scale the individual
terms by some very large or very small multiplicative constant while the terms
are being accumulated and then remove the scale after some computations
have been performed.
Chan, Golub, and LeVeque (1982) propose a modiﬁcation of the algorithm
in equations (10.8) to use pairwise accumulations (as in the fan-in method
discussed previously). Chan, Gene, and LeVeque (1983) make extensive com-
parisons of the methods and give error bounds based on the condition number.
10.3.3 Eﬃciency
The eﬃciency of an algorithm refers to its usage of computer resources. The
two most important resources are the processing units and the memory. The
amount of time the processing units are in use and the amount of memory
required are the key measures of eﬃciency. A limiting factor for the time
the processing units are in use is the number and type of operations required.
Some operations take longer than others; for example, the operation of adding
ﬂoating-point numbers may take more time than the operation of adding ﬁxed-
point numbers. This, of course, depends on the computer system and on what

10.3 Numerical Algorithms and Analysis
505
kinds of ﬂoating-point or ﬁxed-point numbers we are dealing with. If we have
a measure of the size of the problem, we can characterize the performance of
a given algorithm by specifying the number of operations of each type or just
the number of operations of the slowest type.
10.3.3.1 Measuring Eﬃciency
Often, instead of the exact number of operations, we use the order of the
number of operations in terms of the measure of problem size. If n is some
measure of the size of the problem, an algorithm has order O(f(n)) if, as
n →∞, the number of computations →cf(n), where c is some constant
that does not depend on n. For example, to multiply two n × n matrices
in the obvious way requires O(n3) multiplications and additions; to multiply
an n × m matrix and an m × p matrix requires O(nmp) multiplications and
additions. In the latter case, n, m, and p are all measures of the size of the
problem.
Notice that in the deﬁnition of order there is a constant c. Two algorithms
that have the same order may have diﬀerent constants and in that case are
said to “diﬀer only in the constant”. The order of an algorithm is a measure
of how well the algorithm “scales”; that is, the extent to which the algorithm
can deal with truly large problems.
Let n be a measure of the problem size, and let b and q be positive
constants. An algorithm of order O(bn) has exponential order, one of order
O(nq) has polynomial order, and one of order O(log n) has log order. No-
tice that for log order it does not matter what the base is. Also, notice that
O(log nq) = O(log n). For a given task with an obvious algorithm that has
polynomial order, it is often possible to modify the algorithm to address parts
of the problem so that in the order of the resulting algorithm one n factor is
replaced by a factor of log n.
Although it is often relatively easy to determine the order of an algo-
rithm, an interesting question in algorithm design involves the order of the
problem; that is, the order of the most eﬃcient algorithm possible. A problem
of polynomial order is usually considered tractable, whereas one of exponen-
tial order may require a prohibitively excessive amount of time for its solution.
An interesting class of problems are those for which a solution can be veriﬁed
in polynomial time yet for which no polynomial algorithm is known to ex-
ist. Such a problem is called a nondeterministic polynomial, or NP, problem.
“Nondeterministic” does not imply any randomness; it refers to the fact that
no polynomial algorithm for determining the solution is known. Most inter-
esting NP problems can be shown to be equivalent to each other in order by
reductions that require polynomial time. Any problem in this subclass of NP
problems is equivalent in some sense to all other problems in the subclass and
so such a problem is said to be NP-complete.
For many problems it is useful to measure the size of a problem in some
standard way and then to identify the order of an algorithm for the problem

506
10 Numerical Methods
with separate components. A common measure of the size of a problem is L,
the length of the stream of data elements. An n× n matrix would have length
proportional to L = n2, for example. To multiply two n × n matrices in the
obvious way requires O(L3/2) multiplications and additions, as we mentioned
above.
In analyzing algorithms for more complicated problems, we may wish to
determine the order in the form
O(f(n)g(L))
because L is an essential measure of the problem size and n may depend on
how the computations are performed. For example, in the linear programming
problem, with n variables and m constraints with a dense coeﬃcient matrix,
there are order nm data elements. Algorithms for solving this problem gen-
erally depend on the limit on n, so we may speak of a linear programming
algorithm as being O(n3L), for example, or of some other algorithm as being
O(√nL). (In deﬁning L, it is common to consider the magnitudes of the data
elements or the precision with which the data are represented, so that L is
the order of the total number of bits required to represent the data. This level
of detail can usually be ignored, however, because the limits involved in the
order are generally not taken on the magnitude of the data but only on the
number of data elements.)
The order of an algorithm (or, more precisely, the “order of operations of
an algorithm”) is an asymptotic measure of the operation count as the size
of the problem goes to inﬁnity. The order of an algorithm is important, but
in practice the actual count of the operations is also important. In practice,
an algorithm whose operation count is approximately n2 may be more useful
than one whose count is 1000(n log n + n), although the latter would have
order O(n log n), which is much better than that of the former, O(n2). When
an algorithm is given a ﬁxed-size task many times, the ﬁnite eﬃciency of the
algorithm becomes very important.
The number of computations required to perform some tasks depends not
only on the size of the problem but also on the data. For example, for most
sorting algorithms, it takes fewer computations (comparisons) to sort data
that are already almost sorted than it does to sort data that are completely
unsorted. We sometimes speak of the average time and the worst-case time of
an algorithm. For some algorithms, these may be very diﬀerent, whereas for
other algorithms or for some problems these two may be essentially the same.
Our main interest is usually not in how many computations occur but
rather in how long it takes to perform the computations. Because some com-
putations can take place simultaneously, even if all kinds of computations
required the same amount of time, the order of time could be diﬀerent from
the order of the number of computations.
The actual number of ﬂoating-point operations divided by the time in
seconds required to perform the operations is called the FLOPS (ﬂoating-point
operations per second) rate. Confusingly, “FLOP” also means “ﬂoating-point

10.3 Numerical Algorithms and Analysis
507
operation”, and “FLOPs” is the plural of “FLOP”. Of course, as we tend to
use lowercase more often, we must use the context to distinguish “ﬂops” as a
rate from “ﬂops” the plural of “ﬂop”.
In addition to the actual processing, the data may need to be copied from
one storage position to another. Data movement slows the algorithm and may
cause it not to use the processing units to their fullest capacity. When groups
of data are being used together, blocks of data may be moved from ordinary
storage locations to an area from which they can be accessed more rapidly. The
eﬃciency of a program is enhanced if all operations that are to be performed
on a given block of data are performed one right after the other. Sometimes a
higher-level language prevents this from happening. For example, to add two
arrays (matrices) in modern Fortran, a single statement is suﬃcient:
A = B + C
Now, if we also want to add B to the array E, we may write
A = B + C
D = B + E
These two Fortran statements together may be less eﬃcient than writing a
traditional loop in Fortran or in C because the array B may be accessed a
second time needlessly. (Of course, this is relevant only if these arrays are
very large.)
10.3.3.2 Improving Eﬃciency
There are many ways to attempt to improve the eﬃciency of an algorithm.
Often the best way is just to look at the task from a higher level of detail and
attempt to construct a new algorithm. Many obvious algorithms are serial
methods that would be used for hand computations, and so are not the best
for use on the computer.
An eﬀective general method of developing an eﬃcient algorithm is called
divide and conquer. In this method, the problem is broken into subproblems,
each of which is solved, and then the subproblem solutions are combined into
a solution for the original problem. In some cases, this can result in a net
savings either in the number of computations, resulting in an improved order
of computations, or in the number of computations that must be performed
serially, resulting in an improved order of time.
Let the time required to solve a problem of size n be t(n), and consider
the recurrence relation
t(n) = pt(n/p) + cn
for p positive and c nonnegative. Then t(n) ∈O(n log n) (see Exercise 10.19,
page 521). Divide and conquer strategies can sometimes be used together with
a simple method that would be O(n2) if applied directly to the full problem
to reduce the order to O(n log n).

508
10 Numerical Methods
The “fan-in algorithm” (see page 487) is an example of a divide and con-
quer strategy that allows O(n) operations to be performed in O(log n) time
if the operations can be performed simultaneously. The number of operations
does not change materially; the improvement is in the time.
Although there have been orders of magnitude improvements in the speed
of computers because the hardware is better, the order of time required to
solve a problem is almost entirely dependent on the algorithm. The improve-
ments in eﬃciency resulting from hardware improvements are generally diﬀer-
ences only in the constant. The practical meaning of the order of the time must
be considered, however, and so the constant may be important. In the fan-in
algorithm, for example, the improvement in order is dependent on the unreal-
istic assumption that as the problem size increases without bound, the number
of processors also increases without bound. Divide and conquer strategies do
not require multiple processors for their implementation, of course.
Some algorithms are designed so that each step is as eﬃcient as possi-
ble, without regard to what future steps may be part of the algorithm. An
algorithm that follows this principle is called a greedy algorithm. A greedy
algorithm is often useful in the early stages of computation for a problem or
when a problem lacks an understandable structure.
10.3.3.3 Scalability
We expect to devote more resources to solving large-scale problems than to
solving smaller problems. Whether or not the additional resources allow the
larger problems to be solved in an acceptable length of time depends on the
system or process being used to solve the problem, as well as on the nature of
the problem itself. For example, in adding a set of numbers, the fan-in algo-
rithm requires additional adders as the size of the problem grows. Assuming
that these additional resources can be provided, the time required is of log
order.
A system or process is called scalable if, as the size of the problem increases,
additional resources can be provided to the system or process so that its
performance is not badly degraded. The addition of resources is called scaling
the system.
Scaling a system can be done in various ways. More random-access memory
can be added to a computer or more cores can be added to the CPU, for
example. This is sometimes called scaling up the system, because the basic
structure of the system does not change. In very large-scale problems, the
system itself can be expanded into one that consists of distributed computing
systems. This is called scaling out the system
As a general concept, the words scalable and scalability are useful, but
because of the imprecision, I do not use the words often. The order of compu-
tations or of time expressed as a function of the size of the problem and the
resources available are more meaningful measures, although, as we have seen
above, there may be inherent ambiguities in those measures.

10.3 Numerical Algorithms and Analysis
509
10.3.3.4 Bottlenecks and Limits
There is a maximum FLOPS rate possible for a given computer system. This
rate depends on how fast the individual processing units are, how many pro-
cessing units there are, and how fast data can be moved around in the system.
The more eﬃcient an algorithm is, the closer its achieved FLOPS rate is to
the maximum FLOPS rate.
For a given computer system, there is also a maximum FLOPS rate possi-
ble for a given problem. This has to do with the nature of the tasks within the
given problem. Some kinds of tasks can utilize various system resources more
easily than other tasks. If a problem can be broken into two tasks, T1 and
T2, such that T1 must be brought to completion before T2 can be performed,
the total time required for the problem depends more on the task that takes
longer. This tautology (which is sometimes called someone’s “law”) has im-
portant implications for the limits of eﬃciency of algorithms. The speedup of
problems that consist of both tasks that must be performed sequentially and
tasks that can be performed in parallel is limited by the time required for the
sequential tasks.
The eﬃciency of an algorithm may depend on the organization of the
computer, the implementation of the algorithm in a programming language,
and the way the program is compiled.
10.3.3.5 High-Performance Computing
In “high-performance” computing major emphasis is placed on computational
eﬃciency. The architecture of the computer becomes very important, and the
software is designed to take advantage of the particular characteristics of the
computer on which it is to run.
The three main architectural elements are memory, central processing
units, and communication paths. A controlling unit oversees how these el-
ements work together.
There are various ways memory can be organized. There is usually a hier-
archy of types of memory with diﬀerent speeds of access. The various levels
can also be organized into banks with separate communication links to the
processing units.
The processing units can be constructed and organized in various ways. A
single processor can be “vectorized”, so that it can perform the same operation
on all elements of two vectors at the same time. Vectorized processing is very
important in numerical linear algebra, because so many of the computations
naturally are performed on vectors. An EDP processor would also be very
useful.
There may be multiple central processing units. The units may consist of
multiple cores within the same processor. The processing units may include
vector processors.

510
10 Numerical Methods
If more than one processing unit is available, it may be possible to perform
diﬀerent kinds of operations simultaneously. In this case, the amount of time
required may be drastically smaller for an eﬃcient parallel algorithm than it
would for the most eﬃcient serial algorithm that utilizes only one processor at
a time. An analysis of the eﬃciency must take into consideration how many
processors are available, how many computations can be performed in parallel,
and how often they can be performed in parallel.
Levesque and Wagenbreth (2010) provide a good overview of the various
designs and their relevance to high-performance computing.
10.3.3.6 Computations in Parallel
The most eﬀective way of decreasing the time required for solving a computa-
tional problem is to perform the computations in parallel if possible. There are
some computations that are essentially serial, but in almost any problem there
are subtasks that are independent of each other and can be performed in any
order. Parallel computing remains an important research area. See Nakano
(2012) for a summary discussion of parallel computing. In an increasing num-
ber of problems in the data sciences, distributed computing, which can be
considered an extreme form of parallel computing, is used because the data
for the problems reside on diﬀerent servers. See Kshemkalyani and Singhal
(2011) for discussions of distributed computing.
10.3.4 Iterations and Convergence
Many numerical algorithms are iterative; that is, groups of computations form
successive approximations to the desired solution. In a program this usually
means a loop through a common set of instructions in which each pass through
the loop changes the initial values of operands in the instructions.
We will generally use the notation x(k) to refer to the computed value of
x at the kth iteration.
An iterative algorithm terminates when some convergence criterion or
stopping criterion is satisﬁed. An example is to declare that an algorithm
has converged when
Δ(x(k), x(k−1)) ≤ϵ,
where Δ(x(k), x(k−1)) is some measure of the diﬀerence of x(k) and x(k−1) and
ϵ is a small positive number. Because x may not be a single number, we must
consider general measures of the diﬀerence of x(k) and x(k−1). For example, if
x is a vector, the measure may be some metric, such as we discuss in Chap. 2.
In that case, Δ(x(k), x(k−1)) may be denoted by ∥x(k) −x(k−1)∥.
An iterative algorithm may have more than one stopping criterion. Often,
a maximum number of iterations is set so that the algorithm will be sure to
terminate whether it converges or not. (Some people deﬁne the term “algo-
rithm” to refer only to methods that converge. Under this deﬁnition, whether

10.3 Numerical Algorithms and Analysis
511
or not a method is an “algorithm” may depend on the input data unless a
stopping rule based on something independent of the data, such as the num-
ber of iterations, is applied. In any event, it is always a good idea, in addition
to stopping criteria based on convergence of the solution, to have a stopping
criterion that is independent of convergence and that limits the number of
operations.)
The convergence ratio of the sequence x(k) to a constant x0 is
lim
k→∞
Δ(x(k+1), x0)
Δ(x(k), x0)
if this limit exists. If the convergence ratio is greater than 0 and less than
1, the sequence is said to converge linearly. If the convergence ratio is 0, the
sequence is said to converge superlinearly.
Other measures of the rate of convergence are based on
lim
k→∞
Δ(x(k+1), x0)
(Δ(x(k), x0))r = c
(10.10)
(again, assuming the limit exists; i.e., c < ∞). In equation (10.10), the expo-
nent r is called the rate of convergence, and the limit c is called the rate con-
stant. If r = 2 (and c is ﬁnite), the sequence is said to converge quadratically.
It is clear that for any r > 1 (and ﬁnite c), the convergence is superlinear.
Convergence deﬁned in terms of equation (10.10) is sometimes referred to
as “Q-convergence” because the criterion is a quotient. Types of convergence
may then be referred to as “Q-linear”, “Q-quadratic”, and so on.
The convergence rate is often a function of k, say h(k). The convergence
is then expressed as an order in k, O(h(k)).
10.3.4.1 Extrapolation
As we have noted, many numerical computations are performed on a discrete
set that approximates the reals or IRd, resulting in discretization errors. By
“discretization error”, we do not mean a rounding error resulting from the
computer’s ﬁnite representation of numbers. The discrete set used in com-
puting some quantity such as an integral is often a grid. If h is the interval
width of the grid, the computations may have errors that can be expressed
as a function of h. For example, if the true value is x and, because of the
discretization, the exact value that would be computed is xh, then we can
write
x = xh + e(h).
For a given algorithm, suppose the error e(h) is proportional to some power
of h, say hn, and so we can write
x = xh + chn
(10.11)

512
10 Numerical Methods
for some constant c. Now, suppose we use a diﬀerent discretization, with
interval length rh having 0 < r < 1. We have
x = xrh + c(rh)n
and, after subtracting from equation (10.11),
0 = xh −xrh + c(hn −(rh)n)
or
chn = (xh −xrh)
rn −1
.
(10.12)
This analysis relies on the assumption that the error in the discrete algo-
rithm is proportional to hn. Under this assumption, chn in equation (10.12)
is the discretization error in computing x, using exact computations, and is
an estimate of the error due to discretization in actual computations. A more
realistic regularity assumption is that the error is O(hn) as h →0; that is,
instead of (10.11), we have
x = xh + chn + O(hn+α)
for α > 0.
Whenever this regularity assumption is satisﬁed, equation (10.12) provides
us with an inexpensive improved estimate of x:
xR = xrh −rnxh
1 −rn
.
(10.13)
It is easy to see that |x −xR| is less than the absolute error using an interval
size of either h or rh.
The process described above is called Richardson extrapolation, and the
value in equation (10.13) is called the Richardson extrapolation estimate.
Richardson extrapolation is also called “Richardson’s deferred approach to the
limit”. It has general applications in numerical analysis, but is most widely
used in numerical quadrature. Bickel and Joseph (1988) use Richardson ex-
trapolation to reduce the computations in a bootstrap. Extrapolation can be
extended beyond just one step, as in the presentation above.
Reducing the computational burden by using extrapolation is very impor-
tant in higher dimensions. In many cases, for example in direct extensions
of quadrature rules, the computational burden grows exponentially with the
number of dimensions. This is sometimes called “the curse of dimensionality”
and can render a fairly straightforward problem in one or two dimensions
unsolvable in higher dimensions.
A direct extension of Richardson extrapolation in higher dimensions would
involve extrapolation in each direction, with an exponential increase in the
amount of computation. An approach that is particularly appealing in higher
dimensions is splitting extrapolation, which avoids independent extrapolations
in all directions. See Liem, L¨u, and Shih (1995) for an extensive discussion of
splitting extrapolation, with numerous applications.

10.3 Numerical Algorithms and Analysis
513
10.3.5 Other Computational Techniques
In addition to techniques to improve the eﬃciency and the accuracy of com-
putations, there are also special methods that relate to the way we build
programs or store and access data.
10.3.5.1 Recursion
The algorithms for many computations perform some operation, update the
operands, and perform the operation again.
1. perform operation
2. test for exit
3. update operands
4. go to 1
If we give this algorithm the name doit and represent its operands by x, we
could write the algorithm as
Algorithm doit(x)
1. operate on x
2. test for exit
3. update x: x′
4. doit(x′)
The algorithm for computing the mean and the sum of squares (10.8) on
page 504 can be derived as a recursion. Suppose we have the mean ak and
the sum of squares sk for k elements x1, x2, . . . , xk, and we have a new value
xk+1 and wish to compute ak+1 and sk+1. The obvious solution is
ak+1 = ak + xk+1 −ak
k + 1
and
sk+1 = sk + k(xk+1 −ak)2
k + 1
.
These are the same computations as in equations (10.8) on page 504.
Another example of how viewing the problem as an update problem can
result in an eﬃcient algorithm is in the evaluation of a polynomial of degree d,
pd(x) = cdxd + cd−1xd−1 + · · · + c1x + c0.
Doing this in a naive way would require d−1 multiplications to get the powers
of x, d additional multiplications for the coeﬃcients, and d additions. If we
write the polynomial as
pd(x) = x(cdxd−1 + cd−1xd−2 + · · · + c1) + c0,

514
10 Numerical Methods
we see a polynomial of degree d−1 from which our polynomial of degree d can
be obtained with but one multiplication and one addition; that is, the number
of multiplications is equal to the increase in the degree—not two times the
increase in the degree. Generalizing, we have
pd(x) = x(· · · x(x(cdx + cd−1) + · · · ) + c1) + c0,
(10.14)
which has a total of d multiplications and d additions. The method for eval-
uating polynomials in equation (10.14) is called Horner’s method.
A computer subprogram that implements recursion invokes itself. Not only
must the programmer be careful in writing the recursive subprogram, but the
programming system must maintain call tables and other data properly to
allow for recursion. Once a programmer begins to understand recursion, there
may be a tendency to overuse it. To compute a factorial, for example, the
inexperienced C programmer may write
float Factorial(int n)
{
if(n==0)
return 1;
else
return n*Factorial(n-1);
}
The problem is that this is implemented by storing a stack of statements.
Because n may be relatively large, the stack may become quite large and
ineﬃcient. It is just as easy to write the function as a simple loop, and it
would be a much better piece of code.
Fortran, R, Python, and C all allow for recursion.
10.3.5.2 Computations Without Storing Data
For computations involving large sets of data, it is desirable to have algorithms
that sequentially use a single data record, update some cumulative data, and
then discard the data record. Such an algorithm is called a real-time algorithm,
and operation of such an algorithm is called online processing. An algorithm
that has all of the data available throughout the computations is called a batch
algorithm.
An algorithm that generally processes data sequentially in a similar man-
ner as a real-time algorithm but may have subsequent access to the same data
is called an online algorithm or an “out-of-core” algorithm. (This latter name
derives from the erstwhile use of “core” to refer to computer memory.) Any
real-time algorithm is an online or out-of-core algorithm, but an online or
out-of-core algorithm may make more than one pass through the data. (Some
people restrict “online” to mean “real-time” as we have deﬁned it above.)
If the quantity t is to be computed from the data x1, x2, . . . , xn, a real-
time algorithm begins with a quantity t(0), and from t(0) and x1 computes t(1).

10.3 Numerical Algorithms and Analysis
515
The algorithm proceeds to compute t(2) using x2 and so on, never retaining
more than just the current value, t(k). The quantities t(k) may of course consist
of multiple elements, but the point is that the number of elements in each t(k)
is independent of n.
Many summary statistics can be computed in online or real-time processes.
For example, the algorithms discussed beginning on page 503 for computing
the sample sum of squares are real-time algorithms. The algorithm in equa-
tions (10.6) requires two passes through the data so it is not a real-time
algorithm, although it is out-of-core in each pass. There are stable online al-
gorithms for other similar statistics, such as the sample variance-covariance
matrix. The least squares linear regression estimates can also be computed by
a stable one-pass algorithm that, incidentally, does not involve computation
of the variance-covariance matrix (or the sums of squares and cross products
matrix). There is no real-time algorithm for ﬁnding the median. The number
of data records that must be retained and reexamined depends on n.
It is interesting to note that any suﬃcient statistic can be computed by
an out-of-core algorithm.
In addition to the reduced storage burden, a real-time algorithm allows
a statistic computed from one sample to be updated using data from a new
sample. A real-time algorithm is necessarily O(n).
10.3.5.3 MapReduce
In very-large-scale computational problems, the data may be stored in dif-
ferent locations and in diﬀerent formats. In order to process the data in any
systematic way, we need to scale out the processing system. To do this, we ﬁrst
need to map it into some common structure and then combine the individual
pieces. One standard way of doing this is called MapReduce, because of these
two separate types of operations.
In an application of MapReduce, it is ﬁrst assumed that the problem con-
sists of, or can be divided into, separate parts. The mapping phase of MapRe-
duce operates on individual parts of the problem, and within each part of the
problem, it assigns identifying keys to the separate values. The result of this
phase is a collection of sets of key-value pairs.
The next step is to “shuﬄe” the elements in the sets of key-value pairs to
form a new collection of sets each of which has a single key. These individ-
ual sets are then “reduced”, that is, the actual computations are performed.
Finally, the individual computed results are combined.
Here we will consider a simpler example, so that the individual steps are
clear. Consider the problem of counting how many diﬀerent numbers there are
in a given matrix. In Fig. 10.13, we show a 4 × 4 matrix with three diﬀerent
elements, −1, 0, and 1. The problem is to determine how many elements of
each value are in the matrix. In the MapReduce method, each key-value pair
is an element count.

516
10 Numerical Methods
Input
Splitting
Mapping
Shuﬄing
Reducing
Output
1 -1 0 0
-1 0 1 1
0 1 -1 1
-1 -1 1 1
( 1-1 0 0)
(-1 0 1 1)
( 0 1 -1 1)
(-1-1 1 1)
-1,1
0,2
1,1
-1,1
0,1
1,2
-1,1
0,1
1,2
-1,2
1,2
etc.
etc.
-1,1
-1,1
-1,1
-1,2
0,2
0,1
0,1
1,1
1,2
1,2
1,2
-1,5
0,4
1,7
-1,5
0,4
1,7
Figure 10.13. MapReduce to count the number of diﬀerent elements in a matrix
The implementation of this procedure in a distributed computing environ-
ment would entail consideration of many details about the component com-
puting environments and the interaction with the ﬁle system. As mentioned
earlier, the Hadoop Distributed File System (HDFS) is designed for this kind
of process.
This kind of problem does not arise often in numerical computations, but
it illustrates a scalable approach that could be used in a similar problem in
which the matrix is composed of elements distributed over multiple computer
systems.
Our purpose here is only to get an overview of the big picture; not to
discuss the details of the implementation. We will consider the MapReduce
method in the speciﬁc context of matrix multiplication on page 533. Addi-
tional discussion of the MapReduce method is available in Parsian (2015).
Exercises
10.1. An important attitude in the computational sciences is that the com-
puter is to be used as a tool for exploration and discovery. The com-
puter should be used to check out “hunches” or conjectures, which then
later should be subjected to analysis in the traditional manner. There
are limits to this approach, however. An example is in limiting pro-
cesses. Because the computer deals with ﬁnite quantities, the results
of a computation may be misleading. Explore each of the situations
below using C or Fortran. A few minutes or even seconds of computing
should be enough to give you a feel for the nature of the computations.
In these exercises, you may write computer programs in which you per-
form tests for equality. A word of warning is in order about such tests.

Exercises
517
If a test involving a quantity x is executed soon after the computation
of x, the test may be invalid within the set of ﬂoating-point numbers
with which the computer nominally works. This is because the test
may be performed using the extended precision of the computational
registers.
a) Consider the question of the convergence of the series
∞

i=1
i.
Obviously, this series does not converge in IR. Suppose, however,
that we begin summing this series using ﬂoating-point numbers.
Will the computations overﬂow? If so, at what value of i (approx-
imately)? Or will the series converge in IF? If so, to what value,
and at what value of i (approximately)? In either case, state your
answer in terms of the standard parameters of the ﬂoating-point
model, b, p, emin, and emax (page 469).
b) Consider the question of the convergence of the series
∞

i=1
2−2i
and answer the same questions as in Exercise 10.1a.
c) Consider the question of the convergence of the series
∞

i=1
1
i
and answer the same questions as in Exercise 10.1a.
d) Consider the question of the convergence of the series
∞

i=1
1
ix ,
for x ≥1. Answer the same questions as in Exercise 10.1a, except
address the variable x.
10.2. We know, of course, that the harmonic series in Exercise 10.1c does
not converge (although the naive program to compute it does). It is,
in fact, true that
Hn =
n

i=1
1
i
= f(n) + γ + o(1),
where f is an increasing function and γ is Euler’s constant. For various
n, compute Hn. Determine a function f that provides a good ﬁt and
obtain an approximation of Euler’s constant.

518
10 Numerical Methods
10.3. Machine characteristics.
a) Write a program to determine the smallest and largest relative spac-
ings. Use it to determine them on the machine you are using.
b) Write a program to determine whether your computer system im-
plements gradual underﬂow.
c) Write a program to determine the bit patterns of +∞, −∞, and
NaN on a computer that implements the IEEE binary standard.
(This may be more diﬃcult than it seems.)
d)
i. Obtain the program MACHAR (Cody 1988) and use it to de-
termine the smallest positive ﬂoating-point number on the com-
puter you are using. (MACHAR is included in CALGO, which
is available from netlib. See the Bibliography.)
ii. Alternatively, or in addition, determine these values on the
computer you are using by use of the Fortran intrinsic functions
listed in Table 10.3.
iii. Alternatively, or in addition, determine these values on the
computer you are using by use of the R variable .Machine.
10.4. Write a program in Fortran or C to determine the bit patterns of ﬁxed-
point numbers, ﬂoating-point numbers, and character strings. Run your
program on diﬀerent computers, and compare your results with those
shown in Figs. 10.1 through 10.3 and Figs. 10.11 and 10.12.
10.5. Install and load the gmp package in R. Store 112000 and 351500 as bigz
values and compute their product. How many digits are in the product?
10.6. What is the numerical value of the rounding unit ( 1
2 ulp) in the IEEE
Standard 754 double precision?
10.7. Consider the standard model (10.1) for the ﬂoating-point representa-
tion,
±0.d1d2 · · · dp × be,
with emin ≤e ≤emax. Your answers to the following questions may
depend on an additional assumption or two. Either choice of (standard)
assumptions is acceptable.
a) How many ﬂoating-point numbers are there?
b) What is the smallest positive number?
c) What is the smallest number larger than 1?
d) What is the smallest number X such that X + 1 = X?
e) Suppose p = 4 and b = 2 (and emin is very small and emax is very
large). What is the next number after 20 in this number system?
10.8. a) Deﬁne parameters of a ﬂoating-point model so that the number
of numbers in the system is less than the largest number in the
system.
b) Deﬁne parameters of a ﬂoating-point model so that the number of
numbers in the system is greater than the largest number in the
system.

Exercises
519
10.9. Suppose that a certain computer represents ﬂoating-point numbers in
base 10 using eight decimal places for the mantissa, two decimal places
for the exponent, one decimal place for the sign of the exponent, and
one decimal place for the sign of the number.
a) What are the “smallest relative spacing” and the “largest relative
spacing”? (Your answer may depend on certain additional assump-
tions about the representation; state any assumptions.)
b) What is the largest number g such that 417 + g = 417?
c) Discuss the associativity of addition using numbers represented in
this system. Give an example of three numbers, a, b, and c, such
that using this representation (a + b) + c ̸= a + (b + c) unless
the operations are chained. Then show how chaining could make
associativity hold for some more numbers but still not hold for
others.
d) Compare the maximum rounding error in the computation x+x+
x + x with that in 4 ∗x. (Again, you may wish to mention the
possibilities of chaining operations.)
10.10. Consider the same ﬂoating-point system as in Exercise 10.9.
a) Let X be a random variable uniformly distributed over the interval
[1 −.000001, 1 + .000001].
Develop a probability model for the representation [X]c. (This is a
discrete random variable with 111 mass points.)
b) Let X and Y be random variables uniformly distributed over the
same interval as above. Develop a probability model for the rep-
resentation [X + Y ]c. (This is a discrete random variable with 41
mass points.)
c) Develop a probability model for [X]c [+]c [Y ]c. (This is also a
discrete random variable with 41 mass points.)
10.11. Give an example to show that the sum of three ﬂoating-point numbers
can have a very large relative error.
10.12. Write a single program in Fortran or C to compute the following
a)
5

i=0
10
i

0.25i0.7520−i.
b)
10

i=0
20
i

0.25i0.7520−i.
c)
50

i=0
 100
i

0.25i0.7520−i.

520
10 Numerical Methods
10.13. In standard mathematical libraries, there are functions for log(x) and
exp(x) called log and exp respectively. There is a function in the IMSL
Libraries to evaluate log(1 + x) and one to evaluate (exp(x) −1)/x.
(The names in Fortran for single precision are alnrel and exprl.)
a) Explain why the designers of the libraries included those functions,
even though log and exp are available.
b) Give an example in which the standard log loses precision. Evaluate
it using log in the standard math library of Fortran or C. Now
evaluate it using a Taylor series expansion of log(1 + x).
10.14. Suppose you have a program to compute the cumulative distribution
function for the chi-squared distribution. The input for the program
is x and df, and the output is Pr(X ≤x). Suppose you are interested
in probabilities in the extreme upper range and high accuracy is very
important. What is wrong with the design of the program for this
problem? What kind of program would be better?
10.15. Write a program in Fortran or C to compute e−12 using a Taylor series
directly, and then compute e−12 as the reciprocal of e12, which is also
computed using a Taylor series. Discuss the reasons for the diﬀerences
in the results. To what extent is truncation error a problem?
10.16. Errors in computations.
a) Explain the diﬀerence in truncation and cancellation.
b) Why is cancellation not a problem in multiplication?
10.17. Assume we have a computer system that can maintain seven
digits of precision. Evaluate the sum of squares for the dataset
{9000, 9001, 9002}.
a) Use the algorithm in equations (10.6) on page 503.
b) Use the algorithm in equations (10.7) on page 503.
c) Now assume there is one guard digit. Would the answers change?
10.18. Develop algorithms similar to equations (10.8) on page 504 to evaluate
the following.
a) The weighted sum of squares
n

i=1
wi(xi −¯x)2.
b) The third central moment
n

i=1
(xi −¯x)3.
c) The sum of cross products
n

i=1
(xi −¯x)(yi −¯y).

Exercises
521
Hint: Look at the diﬀerence in partial sums,
j

i=1
(·) −
j−1

i=1
(·).
10.19. Given the recurrence relation
t(n) = pt(n/p) + cn
for p positive and c nonnegative, show that t(n) is O(n log n).
Hint: First assume n is a power of p.
10.20. In statistical data analysis, it is common to have some missing data.
This may be because of nonresponse in a survey questionnaire or be-
cause an experimental or observational unit dies or discontinues partic-
ipation in the study. When the data are recorded, some form of missing-
data indicator must be used. Discuss the use of NaN as a missing-value
indicator. What are some of its advantages and disadvantages?
10.21. Consider the four properties of a dot product listed on page 24. For
each one, state whether the property holds in computer arithmetic.
Give examples to support your answers.
10.22. Assuming the model (10.1) on page 469 for the ﬂoating-point number
system, give an example of a nonsingular 2 × 2 matrix that is algorith-
mically singular.
10.23. A Monte Carlo study of condition number and size of the matrix.
For n = 5, 10, . . ., 30, generate 100 n×n matrices whose elements have
independent N(0, 1) distributions. For each, compute the L2 condition
number and plot the mean condition number versus the size of the ma-
trix. At each point, plot error bars representing the sample “standard
error” (the standard deviation of the sample mean at that point). How
would you describe the relationship between the condition number and
the size?
In any such Monte Carlo study we must consider the extent to which
the random samples represent situations of interest. (How often do we
have matrices whose elements have independent N(0, 1) distributions?)

11
Numerical Linear Algebra
Many scientiﬁc computational problems in various areas of application involve
vectors and matrices. Programming languages such as C provide the capabili-
ties for working with the individual elements but not directly with the arrays.
Modern Fortran and higher-level languages such as Octave or Matlab and R
allow direct manipulation of objects that represent vectors and matrices. The
vectors and matrices are arrays of ﬂoating-point numbers.
The distinction between the set of real numbers, IR, and the set of ﬂoating-
point numbers, IF, that we use in the computer has important implications for
numerical computations. As we discussed in Sect. 10.2, beginning on page 483,
an element x of a vector or matrix is approximated by a computer number [x]c,
and a mathematical operation ◦is simulated by a computer operation [◦]c. The
familiar laws of algebra for the ﬁeld of the reals do not hold in IF, especially
if uncontrolled parallel operations are allowed. These distinctions, of course,
carry over to arrays of ﬂoating-point numbers that represent real numbers,
and the properties of vectors and matrices that we discussed in earlier chapters
may not hold for their computer counterparts. For example, the dot product
of a nonzero vector with itself is positive (see page 24), but ⟨xc, xc⟩c = 0 does
not imply xc = 0.
A good general reference on the topic of numerical linear algebra is ˇC´ıˇzkov´a
and ˇC´ıˇzek (2012).
11.1 Computer Storage of Vectors and Matrices
The elements of vectors and matrices are represented as ordinary numeric
data, as we described in Sect. 10.1, in either ﬁxed-point or ﬂoating-point
representation.
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 11
523

524
11 Numerical Linear Algebra
11.1.1 Storage Modes
The elements of vectors and matrices are generally stored in a logically con-
tiguous area of the computer’s memory. What is logically contiguous may not
be physically contiguous, however.
Accessing data from memory in a single pipeline may take more computer
time than the computations themselves. For this reason, computer memory
may be organized into separate modules, or banks, with separate paths to the
central processing unit. Logical memory is interleaved through the banks; that
is, two consecutive logical memory locations are in separate banks. In order
to take maximum advantage of the computing power, it may be necessary to
be aware of how many interleaved banks the computer system has.
There are no convenient mappings of computer memory that would allow
matrices to be stored in a logical rectangular grid, so matrices are usually
stored either as columns strung end-to-end (a “column-major” storage) or as
rows strung end-to-end (a “row-major” storage). In using a computer language
or a software package, sometimes it is necessary to know which way the matrix
is stored. The type of matrix computation to be performed may determine
whether a vectorized processor should operate on rows or on columns.
For some software to deal with matrices of varying sizes, the user must
specify the length of one dimension of the array containing the matrix. (In
general, the user must specify the lengths of all dimensions of the array except
one.) In Fortran subroutines, it is common to have an argument specifying
the leading dimension (number of rows), and in C functions it is common
to have an argument specifying the column dimension. (See the examples in
Fig. 12.2 on page 563 and Fig. 12.3 on page 564 for illustrations of the leading
dimension argument.)
11.1.2 Strides
Sometimes in accessing a partition of a given matrix, the elements occur at
ﬁxed distances from each other. If the storage is row-major for an n × m
matrix, for example, the elements of a given column occur at a ﬁxed distance
of m from each other. This distance is called the “stride”, and it is often more
eﬃcient to access elements that occur with a ﬁxed stride than it is to access
elements randomly scattered.
Just accessing data from the computer’s memory contributes signiﬁcantly
to the time it takes to perform computations. A stride that is not a multi-
ple of the number of banks in an interleaved bank memory organization can
measurably increase the computational time in high-performance computing.
11.1.3 Sparsity
If a matrix has many elements that are zeros, and if the positions of those
zeros are easily identiﬁed, many operations on the matrix can be speeded up.

11.2 General Computational Considerations
525
Matrices with many zero elements are called sparse matrices. They occur of-
ten in certain types of problems; for example in the solution of diﬀerential
equations and in statistical designs of experiments. The ﬁrst consideration is
how to represent the matrix and to store the matrix and the location infor-
mation. Diﬀerent software systems may use diﬀerent schemes to store sparse
matrices. The method used in the IMSL Libraries, for example, is described on
page 550. An important consideration is how to preserve the sparsity during
intermediate computations.
11.2 General Computational Considerations
for Vectors and Matrices
All of the computational methods discussed in Chap. 10 apply to vectors and
matrices, but there are some additional general considerations for vectors and
matrices.
11.2.1 Relative Magnitudes of Operands
One common situation that gives rise to numerical errors in computer opera-
tions is when a quantity x is transformed to t(x) but the value computed is
unchanged:
[t(x)]c = [x]c;
(11.1)
that is, the operation actually accomplishes nothing. A type of transformation
that has this problem is
t(x) = x + ϵ,
(11.2)
where |ϵ| is much smaller than |x|. If all we wish to compute is x + ϵ, the
fact that [x + ϵ]c = [x]c is probably not important. Usually, of course, this
simple computation is part of some larger set of computations in which ϵ was
computed. This, therefore, is the situation we want to anticipate and avoid.
Another type of problem is the addition to x of a computed quantity y
that overwhelms x in magnitude. In this case, we may have
[x + y]c = [y]c.
(11.3)
Again, this is a situation we want to anticipate and avoid.
11.2.1.1 Condition
A measure of the worst-case numerical error in numerical computation in-
volving a given mathematical entity is the “condition” of that entity for the
particular computations. The condition number of a matrix is the most gener-
ally useful such measure. For the matrix A, we denote the condition number
as κ(A). We discussed the condition number in Sect. 6.1 and illustrated it in

526
11 Numerical Linear Algebra
the toy example of equation (6.1). The condition number provides a bound
on the relative norms of a “correct” solution to a linear system and a solution
to a nearby problem. A speciﬁc condition number therefore depends on the
norm, and we deﬁned κ1, κ2, and κ∞condition numbers (and saw that they
are generally roughly of the same magnitude). We saw in equation (6.10) that
the L2 condition number, κ2(A), is the ratio of magnitudes of the two extreme
eigenvalues of A.
The condition of data depends on the particular computations to be per-
formed. The relative magnitudes of other eigenvalues (or singular values) may
be more relevant for some types of computations. Also, we saw in Sect. 10.3.2
that the “stiﬀness” measure in equation (10.3.2.7) is a more appropriate mea-
sure of the extent of the numerical error to be expected in computing vari-
ances.
11.2.1.2 Pivoting
Pivoting, discussed on page 277, is a method for avoiding a situation like that
in equation (11.3). In Gaussian elimination, for example, we do an addition,
x+y, where the y is the result of having divided some element of the matrix by
some other element and x is some other element in the matrix. If the divisor is
very small in magnitude, y is large and may overwhelm x as in equation (11.3).
11.2.1.3 “Modiﬁed” and “Classical” Gram-Schmidt
Transformations
Another example of how to avoid a situation similar to that in equation (11.1)
is the use of the correct form of the Gram-Schmidt transformations.
The orthogonalizing transformations shown in equations (2.56) on page 38
are the basis for Gram-Schmidt transformations of matrices. These transfor-
mations in turn are the basis for other computations, such as the QR factor-
ization. (Exercise 5.10 required you to apply Gram-Schmidt transformations
to develop a QR factorization.)
As mentioned on page 38, there are two ways we can extend equa-
tions (2.56) to more than two vectors, and the method given in Algorithm 2.1
is the correct way to do it. At the kth stage of the Gram-Schmidt method, the
vector x(k)
k
is taken as x(k−1)
k
and the vectors x(k)
k+1, x(k)
k+2, . . . , x(k)
m are all made
orthogonal to x(k)
k . After the ﬁrst stage, all vectors have been transformed.
This method is sometimes called “modiﬁed Gram-Schmidt” because some
people have performed the basic transformations in a diﬀerent way, so that at
the kth iteration, starting at k = 2, the ﬁrst k −1 vectors are unchanged (i.e.,
x(k)
i
= x(k−1)
i
for i = 1, 2, . . . , k −1), and x(k)
k
is made orthogonal to the k −1
previously orthogonalized vectors x(k)
1 , x(k)
2 , . . . , x(k)
k−1. This method is called
“classical Gram-Schmidt” for no particular reason. The “classical” method is
not as stable, and should not be used; see Rice (1966) and Bj¨orck (1967) for

11.2 General Computational Considerations
527
discussions. In this book, “Gram-Schmidt” is the same as what is sometimes
called “modiﬁed Gram-Schmidt”. In Exercise 11.1, you are asked to experi-
ment with the relative numerical accuracy of the “classical Gram-Schmidt”
and the correct Gram-Schmidt. The problems with the former method show
up with the simple set of vectors x1 = (1, ϵ, ϵ), x2 = (1, ϵ, 0), and x3 = (1, 0, ϵ),
with ϵ small enough that
[1 + ϵ2]c = 1.
11.2.2 Iterative Methods
As we saw in Chap. 6, we often have a choice between direct methods (that is,
methods that compute a closed-form solution) and iterative methods. Iterative
methods are usually to be favored for large, sparse systems.
Iterative methods are based on a sequence of approximations that (it is
hoped) converge to the correct solution. The fundamental trade-oﬀin iter-
ative methods is between the amount of work expended in getting a good
approximation at each step and the number of steps required for convergence.
11.2.2.1 Preconditioning
In order to achieve acceptable rates of convergence for iterative algorithms, it
is often necessary to precondition the system; that is, to replace the system
Ax = b by the system
M −1Ax = M −1b
for some suitable matrix M. As we indicated in Chaps. 6 and 7, the choice of
M involves some art, and we will not consider any of the results here. Benzi
(2002) provides a useful survey of the general problem and work up to that
time, but this is an area of active research.
11.2.2.2 Restarting and Rescaling
In many iterative methods, not all components of the computations are up-
dated in each iteration. An approximation to a given matrix or vector may be
adequate during some sequence of computations without change, but then at
some point the approximation is no longer close enough, and a new approxi-
mation must be computed. An example of this is in the use of quasi-Newton
methods in optimization in which an approximate Hessian is updated, as in-
dicated in equation (4.28) on page 202. We may, for example, just compute
an approximation to the Hessian every few iterations, perhaps using second
diﬀerences, and then use that approximate matrix for a few subsequent iter-
ations.
Another example of the need to restart or to rescale is in the use of fast
Givens rotations. As we mentioned on page 241 when we described the fast
Givens rotations, the diagonal elements in the accumulated C matrices in

528
11 Numerical Linear Algebra
the fast Givens rotations can become widely diﬀerent in absolute values, so
to avoid excessive loss of accuracy, it is usually necessary to rescale the el-
ements periodically. Anda and Park (1994, 1996) describe methods of doing
the rescaling dynamically. Their methods involve adjusting the ﬁrst diagonal
element by multiplication by the square of the cosine and adjusting the second
diagonal element by division by the square of the cosine. Bindel et al. (2002)
discuss in detail techniques for performing Givens rotations eﬃciently while
still maintaining accuracy. (The BLAS routines (see Sect. 12.2.1) rotmg and
rotm, respectively, set up and apply fast Givens rotations.)
11.2.2.3 Preservation of Sparsity
In computations involving large sparse systems, we may want to preserve
the sparsity, even if that requires using approximations, as discussed in
Sect. 5.10.2. Fill-in (when a zero position in a sparse matrix becomes nonzero)
would cause loss of the computational and storage eﬃciencies of software for
sparse matrices.
In forming a preconditioner for a sparse matrix A, for example, we may
choose a matrix M = L U, where L and U are approximations to the matrices
in an LU decomposition of A, as in equation (5.51). These matrices are con-
structed as indicated in equation (5.52) so as to have zeros everywhere A has,
and A ≈LU. This is called incomplete factorization, and often, instead of an
exact factorization, an approximate factorization may be more useful because
of computational eﬃciency.
11.2.2.4 Iterative Reﬁnement
Even if we are using a direct method, it may be useful to reﬁne the solution by
one step computed in extended precision. A method for iterative reﬁnement
of a solution of a linear system is given in Algorithm 6.3.
11.2.3 Assessing Computational Errors
As we discuss in Sect. 10.2.2 on page 485, we measure error by a scalar quan-
tity, either as absolute error, |˜r −r|, where r is the true value and ˜r is the
computed or rounded value, or as relative error, |˜r −r|/r (as long as r ̸= 0).
We discuss general ways of reducing them in Sect. 10.3.2.
11.2.3.1 Errors in Vectors and Matrices
The errors in vectors or matrices are generally expressed in terms of norms;
for example, the relative error in the representation of the vector v, or as a
result of computing v, may be expressed as ∥˜v −v∥/∥v∥(as long as ∥v∦= 0),
where ˜v is the computed vector. We often use the notation ˜v = v + δv, and

11.3 Multiplication of Vectors and Matrices
529
so ∥δv∥/∥v∥is the relative error. The choice of which vector norm to use may
depend on practical considerations about the errors in the individual elements.
The L∞norm, for example, gives weight only to the element with the largest
single error, while the L1 norm gives weights to all magnitudes equally.
11.2.3.2 Assessing Errors in Given Computations
In real-life applications, the correct solution is not known, but we would still
like to have some way of assessing the accuracy using the data themselves.
Sometimes a convenient way to do this in a given problem is to perform inter-
nal consistency tests. An internal consistency test may be an assessment of the
agreement of various parts of the output. Relationships among the output are
exploited to ensure that the individually computed quantities satisfy these re-
lationships. Other internal consistency tests may be performed by comparing
the results of the solutions of two problems with a known relationship.
The solution to the linear system Ax = b has a simple relationship to
the solution to the linear system Ax = b + caj, where aj is the jth column
of A and c is a constant. A useful check on the accuracy of a computed
solution to Ax = b is to compare it with a computed solution to the modiﬁed
system. Of course, if the expected relationship does not hold, we do not know
which solution is incorrect, but it is probably not a good idea to trust either.
To test the accuracy of the computed regression coeﬃcients for regressing
y on x1, . . . , xm, they suggest comparing them to the computed regression
coeﬃcients for regressing y + dxj on x1, . . . , xm. If the expected relationships
do not obtain, the analyst has strong reason to doubt the accuracy of the
computations.
Another simple modiﬁcation of the problem of solving a linear system with
a known exact eﬀect is the permutation of the rows or columns. Although this
perturbation of the problem does not change the solution, it does sometimes
result in a change in the computations, and hence it may result in a diﬀerent
computed solution. This obviously would alert the user to problems in the
computations.
A simple internal consistency test that is applicable to many problems is
to use two levels of precision in some of the computations. In using this test,
one must be careful to make sure that the input data are the same. Rounding
of the input data may cause incorrect output to result, but that is not the
fault of the computational algorithm.
Internal consistency tests cannot conﬁrm that the results are correct; they
can only give an indication that the results are incorrect.
11.3 Multiplication of Vectors and Matrices
Arithmetic on vectors and matrices involves arithmetic on the individual el-
ements. The arithmetic on the individual elements is performed as we have
discussed in Sect. 10.2.

530
11 Numerical Linear Algebra
The way the storage of the individual elements is organized is very impor-
tant for the eﬃciency of computations. Also, the way the computer memory is
organized and the nature of the numerical processors aﬀect the eﬃciency and
may be an important consideration in the design of algorithms for working
with vectors and matrices.
The best methods for performing operations on vectors and matrices in
the computer may not be the methods that are suggested by the deﬁnitions
of the operations.
In most numerical computations with vectors and matrices, there is more
than one way of performing the operations on the scalar elements. Consider
the problem of evaluating the matrix times vector product, c = Ab, where A
is n × m. There are two obvious ways of doing this:
•
compute each of the n elements of c, one at a time, as an inner product of
m-vectors, ci = aT
i b = 
j aijbj, or
•
update the computation of all of the elements of c simultaneously as
1. For i = 1, . . . , n, let c(0)
i
= 0.
2. For j = 1, . . . , m,
{
for i = 1, . . . , n,
{
let c(i)
i
= c(i−1)
i
+ aijbj.
}
}
If there are p processors available for parallel processing, we could use a fan-in
algorithm (see page 487) to evaluate Ax as a set of inner products:
c(1)
1
=
c(1)
2
=
. . . c(1)
2m−1 =
c(1)
2m = . . .
ai1b1 + ai2b2
ai3b3 + ai4b4 . . . ai,4m−3b4m−3 + ai,4m−2b4m−2
. . .
. . .
↘
↙
. . .
↘
↙
. . .
c(2)
1
=
. . .
c(2)
m =
. . .
c(1)
1
+ c(1)
2
. . .
c(1)
2m−1 + c(1)
2m
. . .
↘
. . .
↓
. . .
c(3)
1
= c(2)
1
+ c(2)
2
. . .
. . .
. . .
The order of the computations is nm (or n2).
Multiplying two matrices A and B can be considered as a problem of mul-
tiplying several vectors bi by a matrix A, as described above. In the following
we will assume A is n × m and B is m × p, and we will use the notation
ai to represent the ith column of A, aT
i to represent the ith row of A, bi to
represent the ith column of B, ci to represent the ith column of C = AB, and
so on. (This notation is somewhat confusing because here we are not using aT
i
to represent the transpose of ai as we normally do. The notation should be

11.3 Multiplication of Vectors and Matrices
531
clear in the context of the diagrams below, however.) Using the inner product
method above results in the ﬁrst step of the matrix multiplication forming
⎡
⎢⎢⎢⎣
aT
1
· · ·
...
· · ·
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
· · ·
b1 · · ·
...
· · ·
⎤
⎥⎥⎥⎦−→
⎡
⎢⎢⎢⎣
c11 = aT
1 b1
· · ·
· · ·
...
...
· · ·
⎤
⎥⎥⎥⎦.
Using the second method above, in which the elements of the product vec-
tor are updated all at once, results in the ﬁrst step of the matrix multiplication
forming
⎡
⎢⎢⎢⎣
· · ·
a1 · · ·
...
· · ·
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
b11 · · ·
· · ·
...
...
· · ·
⎤
⎥⎥⎥⎦−→
⎡
⎢⎢⎢⎢⎣
c(1)
11 = a11b11 · · ·
c(1)
21 = a21b11 · · ·
...
...
c(1)
n1 = an1b11 · · ·
⎤
⎥⎥⎥⎥⎦
.
The next and each successive step in this method are axpy operations:
c(k+1)
1
= b(k+1),1a1 + c(k)
1 ,
for k going to m −1.
Another method for matrix multiplication is to perform axpy operations
using all of the elements of bT
1 before completing the computations for any of
the columns of C. In this method, the elements of the product are built as
the sum of the outer products aibT
i . In the notation used above for the other
methods, we have
⎡
⎢⎢⎢⎣
· · ·
a1 · · ·
...
· · ·
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
bT
1
· · ·
...
· · ·
⎤
⎥⎥⎥⎦−→
⎡
⎢⎢⎣
c(1)
ij = a1bT
1
⎤
⎥⎥⎦,
and the update is
c(k+1)
ij
= ak+1bT
k+1 + c(k)
ij .
The order of computations for any of these methods is O(nmp), or just
O(n3), if the dimensions are all approximately the same. Strassen’s method,
discussed next, reduces the order of the computations.
11.3.1 Strassen’s Algorithm
Another method for multiplying matrices that can be faster for large matrices
is the so-called Strassen algorithm (from Strassen 1969). Suppose A and B
are square matrices with equal and even dimensions. Partition them into sub-
matrices of equal size, and consider the block representation of the product,

532
11 Numerical Linear Algebra
!C11 C12
C21 C22
"
=
! A11 A12
A21 A22
" !B11 B12
B21 B22
"
,
where all blocks are of equal size. Form
P1 = (A11 + A22)(B11 + B22),
P2 = (A21 + A22)B11,
P3 = A11(B12 −B22),
P4 = A22(B21 −B11),
P5 = (A11 + A12)B22,
P6 = (A21 −A11)(B11 + B12),
P7 = (A12 −A22)(B21 + B22).
Then we have (see the discussion on partitioned matrices in Sect. 3.1)
C11 = P1 + P4 −P5 + P7,
C12 = P3 + P5,
C21 = P2 + P4,
C22 = P1 + P3 −P2 + P6.
Notice that the total number of multiplications is 7 instead of the 8 it would
be in forming
!A11 A12
A21 A22
" ! B11 B12
B21 B22
"
directly. Whether the blocks are matrices or scalars, the same analysis holds.
Of course, in either case there are more additions. The addition of two k ×
k matrices is O(k2), so for a large enough value of n the total number of
operations using the Strassen algorithm is less than the number required for
performing the multiplication in the usual way.
The partitioning of the matrix factors can also be used recursively; that
is, in the formation of the P matrices. If the dimension, n, contains a factor
2e, the algorithm can be used directly e times, and then conventional matrix
multiplication can be used on any submatrix of dimension ≤n/2e.) If the
dimension of the matrices is not even, or if the matrices are not square, it
may be worthwhile to pad the matrices with zeros, and then use the Strassen
algorithm recursively.
The order of computations of the Strassen algorithm is O(nlog2 7), instead
of O(n3) as in the ordinary method (log2 7 = 2.81). The algorithm can be
implemented in parallel (see Bailey et al. 1990), and this algorithm is actually
used in some software systems.
Several algorithms have been developed that use similar ideas to Strassen’s
algorithm and are asymptotically faster; that is, with order of computations
O(nk) where k < log2 7). (Notice that k must be at least 2 because there

11.4 Other Matrix Computations
533
are n2 elements.) None of the algorithms that are asymptotically faster than
Strassen’s are competitive in practice, however, because they all have much
larger start-up costs.
11.3.2 Matrix Multiplication Using MapReduce
While methods such as Strassen’s algorithm achieve speedup by decreasing
the total number of computations, other methods increase the overall speed
by performing computations in parallel. Although not all computations can be
performed in parallel and there is some overhead in additional computations
for setting up the job, when multiple processors are available, the total number
of computations may not be very important. One of the major tasks in parallel
processing is just keeping track of the individual computations. MapReduce
(see page 515) can sometimes be used in coordinating these operations.
For the matrix multiplication AB, in the view that the multiplication is a
set of inner products, for i running over the indexes of the rows of A and j
running over the indexes of the columns of B, we merely access the ith row of
A, ai∗, and the jth column of B, b∗j, and form the inner product aT
i∗b∗j as the
(i, j)th element of the product AB. In the language of relational databases in
which the two matrices are sets of data with row and column identiﬁers, this
amounts to accessing the rows of A and the columns of B one by one, matching
the elements of the row and the column so that the column designator of the
row element matches the row designator of the column element, summing the
product of the A row elements and the B column elements, and then grouping
the sums of the products (that is, the inner products) by the A row designators
and the B column designators. In SQL, it is
SELECT A.row, B.col
SUM(A.value*B.value) FROM A,B WHERE A.col=B.row
GROUP BY A.row, B.col;
In a distributed computing environment, MapReduce could be used to
perform these operations. However the matrices are stored, possibly each over
multiple environments, MapReduce would ﬁrst map the matrix elements using
their respective row and column indices as keys. It would then make the
appropriate associations of row element from A with the column elements
from B and perform the multiplications and the sum. Finally, the sums of
the multiplications (that is, the inner products) would be associated with the
appropriate keys for the output. This process is described in many elementary
descriptions of Hadoop, such as in Leskovec, Rajaraman, and Ullman (2014)
(Chapter 2).
11.4 Other Matrix Computations
Many other matrix computations depend on a matrix factorization. The most
useful factorization is the QR factorization. It can be computed stably using

534
11 Numerical Linear Algebra
either Householder reﬂections, Givens rotations, or the Gram-Schmidt pro-
cedure, as described respectively in Sects. 5.8.8, 5.8.9, and 5.8.10 (beginning
on page 252). This is one time when the computational methods can follow
the mathematical descriptions rather closely. Iterations using the QR factor-
ization are used in a variety of matrix computations; for example, they are
used in the most common method for evaluating eigenvalues, as described in
Sect. 7.4, beginning on page 318.
Another very useful factorization is the singular value decomposition
(SVD). The computations for SVD described in Sect. 7.7 beginning on
page 322, are eﬃcient and preserve numerical accuracy. A major diﬀerence
in the QR factorization and the SVD is that the computations for SVD are
necessarily iterative (recall the remarks at the beginning of Chap. 7).
11.4.1 Rank Determination
It is often easy to determine that a matrix is of full rank. If the matrix is
not of full rank, however, or if it is very ill-conditioned, it is often diﬃcult to
determine its rank. This is because the computations to determine the rank
eventually approximate 0. It is diﬃcult to approximate 0; the relative error
(if deﬁned) would be either 0 or inﬁnite. The rank-revealing QR factorization
(equation (5.43), page 251) is the preferred method for estimating the rank.
(Although I refer to this as “estimation”, it more properly should be called
“approximation”. “Estimation” and the related term “testing”, as used in
statistical applications, apply to an unknown object, as in estimating or testing
the rank of a model matrix as discussed in Sect. 9.5.5, beginning on page 433.)
When this decomposition is used to estimate the rank, it is recommended
that complete pivoting be used in computing the decomposition. The LDU
decomposition, described on page 242, can be modiﬁed the same way we used
the modiﬁed QR to estimate the rank of a matrix. Again, it is recommended
that complete pivoting be used in computing the decomposition.
The singular value decomposition (SVD) shown in equation (3.276) on
page 161 also provides an indication of the rank of the matrix. For the n × m
matrix A, the SVD is
A = UDV T,
where U is an n×n orthogonal matrix, V is an m×m orthogonal matrix, and
D is a diagonal matrix of the singular values. The number of nonzero singular
values is the rank of the matrix. Of course, again, the question is whether or
not the singular values are zero. It is unlikely that the values computed are
exactly zero.
A problem related to rank determination is to approximate the matrix
A with a matrix Ar of rank r ≤rank(A). The singular value decomposition
provides an easy way to do this,
Ar = UDrV T,

11.4 Other Matrix Computations
535
where Dr is the same as D, except with zeros replacing all but the r largest
singular values. A result of Eckart and Young (1936) guarantees Ar is the
rank r matrix closest to A as measured by the Frobenius norm,
∥A −Ar∥F,
(see Sect. 3.10). This kind of matrix approximation is the basis for dimension
reduction by principal components.
11.4.2 Computing the Determinant
The determinant of a square matrix can be obtained easily as the product of
the diagonal elements of the triangular matrix in any factorization that yields
an orthogonal matrix times a triangular matrix. As we have stated before,
however, it is not often that the determinant need be computed.
One application in statistics is in optimal experimental designs. The D-
optimal criterion, for example, chooses the design matrix, X, such that |XTX|
is maximized (see Sect. 9.3.2).
11.4.3 Computing the Condition Number
The computation of a condition number of a matrix can be quite involved.
Clearly, we would not want to use the deﬁnition, κ(A) = ∥A∥∥A−1∥, directly.
Although the choice of the norm aﬀects the condition number, recalling the
discussion in Sect. 6.1, we choose whichever condition number is easiest to
compute or estimate.
Various methods have been proposed to estimate the condition number
using relatively simple computations. Cline et al. (1979) suggest a method
that is easy to perform and is widely used. For a given matrix A and some
vector v, solve
ATx = v
and then
Ay = x.
By tracking the computations in the solution of these systems, Cline et al.
conclude that
∥y∥
∥x∥
is approximately equal to, but less than, ∥A−1∥. This estimate is used with
respect to the L1 norm in the LINPACK software library (see page 558 and
Dongarra et al. 1979), but the approximation is valid for any norm. Solving the
two systems above probably does not require much additional work because
the original problem was likely to solve Ax = b, and solving a system with

536
11 Numerical Linear Algebra
multiple right-hand sides can be done eﬃciently using the solution to one of
the right-hand sides. The approximation is better if v is chosen so that ∥x∥is
as large as possible relative to ∥v∥.
Stewart (1980) and Cline and Rew (1983) investigated the validity of the
approximation. The LINPACK estimator can underestimate the true condi-
tion number considerably, although generally not by an order of magnitude.
Cline et al. (1982) give a method of estimating the L2 condition number of
a matrix that is a modiﬁcation of the L1 condition number used in LIN-
PACK. This estimate generally performs better than the L1 estimate, but the
Cline/Conn/Van Loan estimator still can have problems (see Bischof 1990).
Hager (1984) gives another method for an L1 condition number. Higham
(1988) provides an improvement of Hager’s method, given as Algorithm 11.1
below, which is used in the LAPACK software library (Anderson et al. 2000).
Algorithm 11.1 The Hager/Higham LAPACK condition number
estimator γ of the n × n matrix A
Assume n > 1; otherwise set γ = ∥A∥. (All norms are L1 unless speciﬁed
otherwise.)
0. Set k = 1; v(k) = 1
nA1; γ(k) = ∥v(k)∥; and x(k) = ATsign(v(k)).
1. Set j = min{i, s.t. |x(k)
i
| = ∥x(k)∥∞}.
2. Set k = k + 1.
3. Set v(k) = Aej.
4. Set γ(k) = ∥v(k)∥.
5. If sign(v(k)) = sign(v(k−1)) or γ(k) ≤γ(k−1), then go to step 8.
6. Set x(k) = ATsign(v(k)).
7. If ∥x(k)∥∞̸= x(k)
j
and k ≤kmax, then go to step 1.
8. For i = 1, 2, . . ., n, set xi = (−1)i+1 
1 + i−1
n−1

.
9. Set x = Ax.
10. If 2∥x∥
(3n) > γ(k), set γ(k) = 2∥x∥
(3n) .
11. Set γ = γ(k).
Higham (1987) compares Hager’s condition number estimator with that of
Cline et al. (1979) and ﬁnds that the Hager LAPACK estimator is generally
more useful. Higham (1990) gives a survey and comparison of the various
ways of estimating and computing condition numbers. You are asked to study
the performance of the LAPACK estimate using Monte Carlo methods in
Exercise 11.5 on page 538.

Exercises
537
Exercises
11.1. Gram-Schmidt orthonormalization.
a) Write a program module (in Fortran, C, R, Octave or Matlab, or
whatever language you choose) to implement Gram-Schmidt or-
thonormalization using Algorithm 2.1. Your program should be for
an arbitrary order and for an arbitrary set of linearly independent
vectors.
b) Write a program module to implement Gram-Schmidt orthonormal-
ization using equations (2.56) and (2.57).
c) Experiment with your programs. Do they usually give the same re-
sults? Try them on a linearly independent set of vectors all of which
point “almost” in the same direction. Do you see any diﬀerence in
the accuracy? Think of some systematic way of forming a set of
vectors that point in almost the same direction. One way of doing
this would be, for a given x, to form x + ϵei for i = 1, . . . , n −1,
where ei is the ith unit vector and ϵ is a small positive number. The
diﬀerence can even be seen in hand computations for n = 3. Take
x1 = (1, 10−6, 10−6), x2 = (1, 10−6, 0), and x3 = (1, 0, 10−6).
11.2. Given the n × k matrix A and the k-vector b (where n and k are large),
consider the problem of evaluating c = Ab. As we have mentioned, there
are two obvious ways of doing this: (1) compute each element of c, one
at a time, as an inner product ci = aT
i b = 
j aijbj, or (2) update
the computation of all of the elements of c in the inner loop.
a) What is the order of computation of the two algorithms?
b) Why would the relative eﬃciencies of these two algorithms be dif-
ferent for diﬀerent programming languages, such as Fortran and C?
c) Suppose there are p processors available and the fan-in algorithm
on page 530 is used to evaluate Ax as a set of inner products. What
is the order of time of the algorithm?
d) Give a heuristic explanation of why the computation of the inner
products by a fan-in algorithm is likely to have less roundoﬀerror
than computing the inner products by a standard serial algorithm.
(This does not have anything to do with the parallelism.)
e) Describe how the following approach could be parallelized. (This is
the second general algorithm mentioned above.)
for i = 1, . . . , n
{
ci = 0
for j = 1, . . . , k
{
ci = ci + aijbj
}
}
f) What is the order of time of the algorithms you described?

538
11 Numerical Linear Algebra
11.3. Consider the problem of evaluating C = AB, where A is n × m and B
is m × q. Notice that this multiplication can be viewed as a set of ma-
trix/vector multiplications, so either of the algorithms in Exercise 11.2d
above would be applicable. There is, however, another way of performing
this multiplication, in which all of the elements of C could be evaluated
simultaneously.
a) Write pseudocode for an algorithm in which the nq elements of C
could be evaluated simultaneously. Do not be concerned with the
parallelization in this part of the question.
b) Now suppose there are nmq processors available. Describe how the
matrix multiplication could be accomplished in O(m) steps (where
a step may be a multiplication and an addition).
Hint: Use a fan-in algorithm.
11.4. Write a Fortran or C program to compute an estimate of the L1 LA-
PACK condition number γ using Algorithm 11.1 on page 536.
11.5. Design and conduct a Monte Carlo study to assess the performance of
the LAPACK estimator of the L1 condition number using your program
from Exercise 11.4. Consider a few diﬀerent sizes of matrices, say 5 × 5,
10×10, and 20×20, and consider a range of condition numbers, say 10,
104, and 108. In order to assess the accuracy of the condition number
estimator, the random matrices in your study must have known con-
dition numbers. It is easy to construct a diagonal matrix with a given
condition number. The condition number of the diagonal matrix D, with
nonzero elements d1, . . . , dn, is max |di|/ min |di|. It is not so clear how
to construct a general (square) matrix with a given condition number.
The L2 condition number of the matrix UDV , where U and V are or-
thogonal matrices is the same as the L2 condition number of U. We
can therefore construct a wide range of matrices with given L2 condi-
tion numbers. In your Monte Carlo study, use matrices with known L2
condition numbers. The next question is what kind of random matri-
ces to generate. Again, make a choice of convenience. Generate random
diagonal matrices D, subject to ﬁxed κ(D) = max |di|/ min |di|. Then
generate random orthogonal matrices as described in Exercise 4.10 on
page 223. Any conclusions made on the basis of a Monte Carlo study, of
course, must be restricted to the domain of the sampling of the study.
(See Stewart, 1980, for a Monte Carlo study of the performance of the
LINPACK condition number estimator.)

12
Software for Numerical Linear Algebra
There is a variety of computer software available to perform the operations
on vectors and matrices discussed in Chap. 11 and previous chapters. We
can distinguish software based on various dimensions, including the kinds of
applications that the software emphasizes, the level of the objects it works
with directly, and whether or not it is interactive. We can also distinguish
software based on who “owns” the software and its availability to other users.
Many commercial software systems are available from the developers/owners
through licensing agreements, and the rights of the user are restricted by the
terms of the license, in addition to any copyright.
Some software is designed only to perform a limited number of functions,
such as for eigenanalysis, while other software provides a wide range of com-
putations for linear algebra. Some software supports only real matrices and
real associated values, such as eigenvalues. In some software systems, the basic
units must be scalars, and so operations on matrices or vectors must be per-
formed on individual elements. In these systems, higher-level functions to work
directly on the arrays are often built and stored in libraries or supplemental
software packages. In other software systems, the array itself is a fundamental
operand. Finally, some software for linear algebra is interactive and computa-
tions are performed immediately in response to the user’s input, while other
software systems provide a language for writing procedural descriptions that
are to be compiled for later invocation through some interface within the same
system or through linkage from a diﬀerent software system. We often refer to
the languages of interactive systems as “interpretive” or “scripting” languages.
12.1 General Considerations
A person’s interaction with a software system represents a series of invest-
ments. The ﬁrst investment is in getting access to the system. This may involve
spending money, spending time downloading ﬁles, and installing the software.
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5 12
539

540
12 Software for Numerical Linear Algebra
The investment to acquire access involves “learning” the system: how to start
it, how to input data, how to do something, how to output results, and how to
shut it down. The next type of investment is usually just the time to use the
system to solve problems. The returns on these investments are the solutions
to the problems, or just the personal satisfaction in having solved them. A
continuing investment is in becoming proﬁcient in use of the software. The
return is in time saved. Another important investment is the development
of a set of reusable functions and scripts. Functions can often be used in a
wide variety of settings far beyond the one that prompted the writing of the
function in the ﬁrst place. Scripts for a particular task can serve as templates
for programs to solve a diﬀerent problem. The return is in time saved and,
possibly, improved accuracy, in solving future problems.
As with other investments, an objective is to maximize returns. Although
the computing environment is constantly changing, as with other investments,
some degree of stability of the investment environment is desirable in order to
protect the returns. Formal standards (see pages 462 and 564) help to ensure
this.
The software user’s primary goal is not investment (usually), but never-
theless the user is making an investment, and with a little forethought can
increase the returns on this investment.
Investors’ choices depend on the stage of their investment career. If a
software investor has not become proﬁcient in any language, that investor has
diﬀerent investment choices from an investor who is very ﬂuent in C, maybe
knows a little Python, but doesn’t know a word of Fortran.
12.1.1 Software Development and Open Source Software
An important distinction among software systems is the nature of the user’s
access to the system. Software, as intellectual property, is protected by copy-
right, just as any other written work (as opposed to an algorithm, which may
be protected by patent). Copyright is an unusual type of legal title. In most le-
gal jurisdictions, it is initiated without any explicit registration by the holder;
it just comes into being at the time of the creation of the original work.
There are various ways that a copyright of software can be modiﬁed. At one
extreme is complete revocation of copyright by the author; that is, the author
puts the work in the “public domain”. Even if the copyright is retained, the
owner may allow free use, either with or without the rights to modify. We will
not delve into the variations in these schemes, but rather use the phrase “open
source” to refer to systems to which the authors have give users free access
to the full source code of the system. Many important systems ranging from
operating systems, such as Linux, to programming and application-oriented
systems, such as R, are open source. Because software by its nature is often
developed in open collaboration by many people who may have no connections
other than just the work on the software, open-source software is often to be
preferred.

12.1 General Considerations
541
12.1.2 Collaborative Research and Version Control
In collaborative development, version-control systems are especially useful.
For software development, Git is probably the best. (It was built by Linus
Torvalds, the creator of Linux.) The software itself is free and open source. A
good interface to the software and to a hosting service is provided at GitHub:
https://github.com/
For general collaborative document development (including source pro-
grams), Overleaf is particularly useful. Overleaf also provides a TEX processor
with most of the common packages attached. It also provides PDF rendering.
The web site is
https://www.overleaf.com/
The Reproducible R Toolkit (see page 580) is useful for version control in
collaborative work as well as for a single user to deal with diﬀerent versions
of R and R packages.
12.1.3 Finding Software
The quickest way to ﬁnd software is usually to use a web search program.
The Guide to Available Mathematical Software (GAMS) is also a good
source of information about software. This guide is organized by types of
computations. Computations for linear algebra are in Class D, for example.
The web site is
http://gams.nist.gov/serve.cgi/Class/D/
Much of the software is available through statlib or netlib (see page 619
in the Bibliography).
12.1.4 Software Design
There are many properties of software that aﬀect its usefulness. Although we
emphasize eﬃcient use of computer resources (fast algorithms and minimal
memory usage), it is more important that the user’s time and eﬀorts are not
wasted. If the general design of the software is consistent with the user’s ways
of thinking about the application, then the user is more likely to be eﬃcient
in the use of the the software and to make fewer errors.
12.1.4.1 Interoperability
For some types of software, it is important to be aware of the way the data are
stored in the computer, as we discussed in Sect. 11.1 beginning on page 523.
This may include such things as whether the storage is row-major or column-
major, which will determine the stride and may determine the details of an

542
12 Software for Numerical Linear Algebra
algorithm so as to enhance the eﬃciency. Software written in a language such
as Fortran or C often requires the speciﬁcation of the number of rows (in
Fortran) or columns (in C) that have been allocated for the storage of a
matrix. The amount of space allocated for the storage of a matrix may not
correspond exactly to the size of the matrix.
There are many issues to consider in evaluating software or to be aware
of when developing software. The portability of the software is an important
consideration because a user’s programs are often moved from one computing
environment to another.
12.1.4.2 Eﬃciency
Computational eﬃciency, the topic of Sect. 10.3.3, is a very important con-
sideration in the development and use of numerical methods. In that context,
eﬃciency refers to the use of computing resources, which ones are used and for
how long they are used for a speciﬁc task. In the selection and use of software,
the eﬃciency of the methods implemented in the software is an important
consideration, but, depending on the task and the available resources, the
user’s time to set up the computations and access the results is often more
important. Higher-level software and more application-speciﬁc software, while
the actual computations may be slower, may be more eﬃcient overall.
Some situations require special software that is more eﬃcient than general-
purpose software would be for the given task. Software for sparse matrices,
for example, is specialized to take advantage of the zero entries.
12.1.4.3 Writing Mathematics and Writing Programs
Although one of the messages of this book is that we should not ignore compu-
tational details, there are some trivial details that should not have to enter our
conscious thought processes. In well-designed software systems, these details
ﬂow naturally in the process of converting mathematics to computer code.
In writing either mathematics or programs, it is generally best to think of
objects at the highest level that is appropriate for the problem at hand. The
details of some computational procedure may be of the form

i

j

k
akixkj.
(12.1)
We sometimes think of the computations in this form because we have pro-
grammed them in some low-level language at some time. In some cases, it is
important to look at the computations in this form, but usually it is better
to think of the computations at a higher level, say
ATX.
(12.2)

12.1 General Considerations
543
The compactness of the expression is not the issue (although it certainly is
more pleasant to read). The issue is that expression (12.1) leads us to think of
some nested computational loops, while expression (12.2) leads us to look for
more eﬃcient computational modules, such as the BLAS, which we discuss
below. In a higher-level language system such as R, the latter expression is
more likely to cause us to use the system more eﬃciently.
12.1.4.4 Numerical Mathematical Objects and Computer Objects
Some diﬃcult design decisions must be made when building systems that
provide objects that simulate mathematical objects. One issue is how to treat
diﬀerent mathematical objects that are based on the real number system, such
as scalars, vectors, and matrices, when their sizes happen to coincide.
•
Is a vector with one element a scalar?
•
Is a 1 × 1 matrix a scalar?
•
Is a 1 × n matrix a “row vector”?
•
Is an n × 1 matrix a “column vector”?
•
Is a “column vector” diﬀerent from a “row vector”?
While the obvious answer to all these questions is “no”, it is often convenient
to design software systems as if the answer, at least to some of the questions
some of the time, is “yes”. The answer to any such software design question
always must be made in the context of the purpose and intended use (and
users) of the software. The issue is not the purity of a mathematical deﬁnition.
We have already seen that most computer objects and operators do not behave
exactly like the mathematical entities they simulate anyway.
The experience of most people engaged in scientiﬁc computations over
many years has shown that the convenience resulting from the software’s
equivalent treatment of such diﬀerent objects as a 1 × 1 matrix and a scalar
outweighs the programming error detection that could be possible if the ob-
jects were made to behave as nearly as possible to the way the mathematical
entities they simulate behave.
Consider, for example, the following arrays of numbers:
A = [1 2],
B =
!
1
2
"
,
C =
⎡
⎣
1
2
3
⎤
⎦.
(12.3)
If these arrays are matrices with the usual matrix algebra, then ABC, where
juxtaposition indicates Cayley multiplication, is not a valid expression.
If, however, we are willing to allow mathematical objects to change types,
we come up with a reasonable interpretation of ABC. If the 1 × 1 matrix AB
is interpreted as the scalar 5, then the expression (AB)C can be interpreted as
5C, that is, a scalar times a matrix. (Under Cayley multiplication, of course,
we do not need to indicate the order of the operations because the operation
is associative.)

544
12 Software for Numerical Linear Algebra
There is no (reasonable) interpretation that would make the expression
A(BC) valid.
If A is a “row vector” and B is a “column vector”, it hardly makes sense
to deﬁne an operation on them that would yield another vector. A vector
space cannot consist of such mixtures. Under a strict interpretation of the
operations, (AB)C is not a valid expression.
We often think of the “transpose” of a vector (although this may not be
a viable concept in a vector space), and we denote a dot product in a vector
space as xTy. If we therefore interpret a row vector such as A in (12.3) as
xT for some x in the vector space of which B is a member, then AB can be
interpreted as a dot product (that is, as a scalar) and again (AB)C is a valid
expression.
Many software packages have a natural atomic numerical type. For exam-
ple, in Fortran and C the atomic numeric type is a scalar. Other structures
such as vectors and matrices are built as simple arrays from scalars. Other
software systems are array-oriented. The basic atomic numeric type in Matlab
is a matrix. In Matlab, a scalar is a 1 × 1 matrix. A one-dimensional array in
Matlab is a matrix with either one row or one column (Fig. 12.1).
>> a = [1;2;3]
a =
1
2
3
>> b = [1,2,3]
b =
1 2 3
>> a(1,3)
** error **
>> a(3,1)
ans =
3
>> b(1,3)
ans =
3
Figure 12.1. Indexing in Matlab
The basic atomic numeric type in R is a vector. In R, a scalar is a vector
of length 1. A 1 × n matrix and an n × 1 matrix may be cast as vectors (that
is, one-dimensional arrays). This is often exactly what we would want, but
sometimes we want to preserve the matrix structure (see page 576).

12.1 General Considerations
545
12.1.4.5 Other Mathematical Objects and Computer Objects
Most mathematical operations performed on a computer are based on the
two operations of the ﬁeld of the reals, and the design of scientiﬁc software
has been focused on these operations. Operations on abstract variables are
supported by some software packages. Maple and Mathematica are two such
packages, and Matlab also supports some “symbolic computations”, as these
are called.
Most of our interests in numerical linear algebra are focused on numerical
computations of course, but occasionally we want to do other kinds of math-
ematical operations. Operations on sets, for example, are also supported by
some software systems, such as Python and R. The ﬁrst issue in working with
sets, of course, is how to initialize the set. R does not have an object class
of “set”, but it does provide functions for the standard set operations (see
page 578).
12.1.4.6 Software for Statistical Applications
Statistical applications have requirements that go beyond simple linear alge-
bra. The two most common additional requirements are for
•
handling metadata and
•
accommodating missing data.
Software packages designed for data analysis, such as SAS and R, generally
provide for metadata and missing values. Fortran/C libraries generally do not
provide for metadata or for handling missing data.
Two other needs that often arise in statistical analysis but often are not
dealt with adequately in available software, are
•
graceful handling of nonfull rank matrices and
•
working with nonsquare matrices.
Aside from these general capabilities, of course, software packages for sta-
tistical applications, even if they are designed for some speciﬁc type of anal-
ysis, should provide the common operations such as computation of simple
univariate statistics, linear regression computations, and some simple graph-
ing capabilities.
12.1.4.7 Robustness
Operations on matrices are often viewed from the narrow perspective of the
numerical analyst rather than from the broader perspective of a user with
a task to perform. For example, the user may seek a solution to the linear
system Ax = b. Many software packages to solve a linear system requires A
to be square and of full rank. If this is not the case, then there are three
possibilities: the system has no solution, the system has multiple solutions,

546
12 Software for Numerical Linear Algebra
or the system has a unique solution. Software to solve a linear system of
equations that requires A to be square and of full rank does not distinguish
among these possibilities but rather always refuses to provide any solution.
This can be quite annoying to a user who wants to solve a large number
of diﬀerent systems with diﬀerent properties using the same code. A better
design of the software would allow solution of consistent non-square systems
and of consistent non-full-rank systems, and report an inconsistent of any
structure as such.
A related problem occurs when A is a scalar. Some software to solve the
linear system will fail on this degenerate case. (Note, for example, solve in
R handles this correctly.)
Good software is robust both to extended cases and to degenerate cases.
12.1.4.8 Computing Paradigms
A computational task may involve several sequences of computations, some of
which can be performed independently of others. We speak of the separate se-
quences of operations as threads. If the threads can be executed independently,
they can easily be performed at the same time, that is, in parallel. Even if they
cannot be performed independently, by sharing results among the threads, it
may be possible to execute them in parallel. Parallel processing is one on the
most important ways of speeding up computations.
Many computational tasks follow a very simple pattern; all data and all
processors are together in the same system. The programming model is sim-
ple. For many interesting problems, however, the data are stored in diﬀerent
places and in a variety of formats. A simple approach, of course, is to col-
lect and organize the data prior to any actual processing. In many large-scale
problems this is not practical, however, due either to the size of the data or
to the fact that the process of data generation is ongoing. If the data are not
collected in one place, the processing may occur at diﬀerent locations also.
Such distributed computing, which can be thought of as an extreme form of
parallel computing, requires a completely diﬀerent paradigm.
Spark and Hadoop provide such a paradigm. Hadoop and MapReduce,
discussed on page 515, together are ideally suited to distributed computing
tasks. We will not discuss these methods further here, but White (2015) dis-
cusses Hadoop and gives several examples of its use, and Karau et al. (2015)
provide a good introduction to Spark.
For most computations in numerical linear algebra, these considerations
are not relevant, but as we address larger problems, these issues become much
more important.
12.1.4.9 Array Structures and Indexes
Numerical operations on vectors and matrices in most general-purpose pro-
cedural languages are performed either within loops of operations on the in-

12.1 General Considerations
547
dividual elements or by invocation of a separate program module. A natural
way of representing vectors and matrices is as array variables with indexes.
One of the main diﬀerences in various computing systems is how they
handle arrays. The most common impact on the user is the indexing of the
arrays, and here the most common diﬀerences are in where the index begins (0
or 1) and how sequences are handled. To process a mathematical expression
f(xi) over n values of a vector x we use programming statements of the form
of either
for i = 1:n, compute f(x[i])
or
for i = 0:(n-1), compute f(x[i])
depending on the beginning index.
Indexing in Fortran (by default), Matlab, and R begins with “1”, just as in
most mathematical notation (as throughout this book, for example). Fortran
allows indexing to start at any integer. Indexing for ordinary arrays begins
with “0” in Python, C, and C++.
Fortran handles arrays as multiply indexed memory locations, consistent
with the nature of the object. The storage of two-dimensional arrays in Fortran
is column-major; that is, the array A is stored as vec(A). To reference the
contiguous memory locations, the ﬁrst subscript varies fastest. In general-
purpose software consisting of Fortran subprograms, it is often necessary to
specify the lengths of all dimensions of a Fortran array except the last one.
An array in C is an ordered set of memory locations referenced by a pointer
or by a name and an index. The indexes are enclosed in rectangular brackets
following the variable name. An element of a multidimensional array in C is
indexed by multiple indexes, each within rectangular brackets. If the 3 × 4
matrix A is as stored in the C array A, the (2, 3) element A2,3 is referenced as
A[1][2]. This disconnect between the usual mathematical representations and
the C representations results from the historical development of C by computer
scientists, who deal with arrays, rather than by mathematical scientists, who
deal with matrices and vectors.
Multidimensional arrays in C are arrays of arrays, in which the array con-
structors operate from right to left. This results in two-dimensional C arrays
being stored in row-major order, that is, the array A is stored as vec(AT). To
reference the contiguous memory locations, the last subscript varies fastest.
In general-purpose software consisting of C functions, it is often necessary to
specify the lengths of all dimensions of a C array except the ﬁrst one.
Python provides an array construct similar to that of C, but in addition it
also provides several other constructs for multiple values, including one that
is eﬀectively a mathematical set; that is, a collection of unique elements.
Subarrays are formed slightly diﬀerently in diﬀerent systems. Consider the
vector x with ten elements, which we have stored in a linear array called x.
The number of elements in the array is len, 10 in this case. Suppose we want

548
12 Software for Numerical Linear Algebra
to reference various elements within the array. In diﬀerent systems this is done
as shown in Table 12.1.
Table 12.1. Subarrays
The last ﬁve elements All but last ﬁve elements First, third, . . . elements
Fortran
x(len-4:)
x(:len-5)
x(::2)
Matlab
x(len-4:len)
x(1:len-5)
x(1:2:len)
R
x[(len-4):len]
x[1:(len-5)]
x[seq(1,len,2)]
Python
x[len-5:]
x[:len-5]
x[::2]
For people who use diﬀerent systems, the diﬀerences in the indexing is not
just annoying; the main problem is that it leads to programming errors. (The
reader who does not know Python may also be somewhat perplexed by the
ﬁrst two entries for Python in Table 12.1.)
12.1.4.10 Matrix Storage Modes
There are various ways of indexing matrices that have special structure. Al-
though one of the purposes of special storage modes is to use less computer
memory, sometimes it is more natural to think of the elements of a matrix
with a special structure in a way that conforms to that structure.
Matrices that have multiple elements with the same value can often be
stored in the computer in such a way that the individual elements do not all
have separate locations. Symmetric matrices and matrices with many zeros,
such as the upper or lower triangular matrices of the various factorizations we
have discussed, are examples of matrices that do not require full rectangular
arrays for their storage. There are several special modes for storage of matrices
with repeated elements, especially if those elements are zeroes.
•
symmetric mode
•
Hermitian mode
•
triangular mode
•
band mode
•
sparse storage mode
For a symmetric or Hermitian matrix or a triangular matrix, only about
half of the elements need to be stored. Although the amount of space saved
by not storing the full symmetric matrix is only about one half of the amount
of space required, the use of rank 1 arrays rather than rank 2 arrays can yield
some reference eﬃciencies. (Recall that in discussions of computer software
objects, “rank” usually means the number of dimensions.) For band matrices
and other sparse matrices, the savings in storage can be much larger.
For a symmetric matrix such as

12.1 General Considerations
549
A =
⎡
⎢⎢⎣
1 2 4 · · ·
2 3 5 · · ·
4 5 6 · · ·
· · ·
⎤
⎥⎥⎦,
(12.4)
only the unique elements
v = (1, 2, 3, 4, 5, 6, · · ·)
(12.5)
need to be stored.
A special indexing method for storing symmetric matrices, called sym-
metric storage mode, uses a linear array to store only the unique elements.
Symmetric storage mode can be set up in various ways.
One form of symmetric storage mode corresponds directly to an arrange-
ment of the vector v in equation (12.5) to hold the data in matrix A in
equation (12.4). This symmetric storage mode is a much more eﬃcient and
useful method of storing a symmetric matrix than would be achieved by a
vech(·) operator because with symmetric storage mode, the size of the matrix
aﬀects only the elements of the vector near the end. If the number of rows
and columns of the matrix is increased, the length of the vector is increased,
but the elements are not rearranged.
For an n×n symmetric matrix A, the correspondence with the n(n+1)/2-
vector v is
vi(i−1)/2+j = ai,j,
for i ≥j.
Notice that the relationship does not involve n. For i ≥j, in Fortran, it is
v(i*(i-1)/2+j) = a(i,j)
In C, because the indexing begins at 0, the correspondence in C arrays is
v[i*(i+1)/2+j] = a[i][j]
In R, a symmetric matrix such as A in equation (12.4) is often represented
in a vector of the form
˜v = (1, 2, 4, · · · , 3, 5, · · · , 6, · · · ).
(12.6)
Notice that this is exactly what the vech(·) operator yields. (There is no vech
function in any of the common R packages, however.) For an n×n symmetric
matrix A, the correspondence with the n(n + 1)/2-vector ˜v is
˜vi+n(j−1)−j(j−1)/2 = ai,j,
for i ≥j.
Notice that the relationship involves n. For a hollow matrix, such as a dis-
tance matrix (produced by the dist function in R, for example), the diagonal
elements are not stored.
In a band storage mode, for the n × m band matrix A with lower band
width wl and upper band width wu, an wl + wu × m array is used to store the

550
12 Software for Numerical Linear Algebra
elements. The elements are stored in the same column of the array, say A, as
they are in the matrix; that is,
A(i −j + wu + 1, j) = ai,j
for i = 1, 2, . . . , wl + wu + 1. Band symmetric, band Hermitian, and band
triangular modes are all deﬁned similarly. In each case, only the upper or
lower bands are referenced.
There are several diﬀerent schemes for representing sparse matrices, but
the most common way is the index-index-value method. This method uses
three arrays, each of rank 1 and with length equal to the number of nonzero
elements. The integer array i contains the row indicator, the integer array
j contains the column indicator, and the ﬂoating-point array a contains the
corresponding values; that is, the (i(k), j(k)) element of the matrix is stored
in a(k). The level 3 BLAS (see page 557) for sparse matrices have an argument
to allow the user to specify the type of storage mode.
12.1.5 Software Development, Maintenance, and Testing
In software development as in other production processes, good initial design
of both the product and the process reduces the need for “acceptance sam-
pling”. Nevertheless, there is a need for thorough and systematic testing of
software. Tests should be part of the development phase, but can continue
throughout the life of the software.
12.1.5.1 Test Data
Testbeds for software consist of test datasets that vary in condition but have
known solutions or for which there is an easy way of verifying the solution.
Test data may be ﬁxed datasets or randomly generated datasets over some
population with known and controllable properties.
For testing software for some matrix computations, a very common matrix
is the special Hankel matrix, called the Hilbert matrix H that has elements
hij =
1
i + j −1.
Hilbert matrices have large condition numbers; for example, the 10×10 Hilbert
matrix has condition number of order 1013. The Matlab function hilb(n)
generates an n × n Hilbert matrix. A Hilbert matrix is also a special case of
a square Cauchy matrix with x = (n + 1, n + 2, . . . , 2n), y = (n, n −1, . . . , 1),
v = 1, and w = 1 (see Sect. 8.8.8 on page 391).
Randomly generated test data can provide general information about the
performance of a computational method over a range of datasets with spec-
iﬁed characteristics. Examples of studies using randomly generated datasets
are the paper by Birkhoﬀand Gulati (1979) on the accuracy of computed

12.1 General Considerations
551
solutions xc of the linear system Ax = b, where A is n × n from a BMvN
distribution, and the paper by Stewart (1980) using random matrices from a
Haar distribution to study approximations to condition numbers (see page 221
and Exercise 4.10). As it turns out, matrices from the BMvN distribution are
not suﬃciently ill-conditioned often enough to be useful in studies of the accu-
racy of solutions of linear systems. Birkhoﬀand Gulati developed a procedure
to construct arbitrarily ill-conditioned matrices from ones with a BMvN dis-
tribution and then used these matrices in their empirical studies.
Ericksen (1985) describes a method to generate matrices with known in-
verses in such a way that the condition numbers vary widely. To generate an
n×n matrix A, choose x1, x2, . . . , xn arbitrarily, except such that x1 ̸= 0, and
then take
a1j = x1
for j = 1, . . . , n,
ai1 = xi
for i = 2, . . . , n,
aij = ai,j−1 + ai−1,j−1 for i, j = 2, . . . , n.
(12.7)
To represent the elements of the inverse, ﬁrst deﬁne y1 = x−1
1 , and for i =
2, . . . , n,
yi = −y1
i−1

k=0
xi−kyk.
Then the elements of the inverse of A, B = (bij), are given by
bin = (−1)i+k

n −1
i −1

y1
for i = 1, . . . , n,
bnj = yn+1−j
for j = 1, . . . , n −1,
bij = x1binbnj + n
k=i+1 bk,j+1 for i, j = 1, . . . , n −1,
(12.8)
where the binomial coeﬃcient,

k
m

, is deﬁned to be 0 if k < m or m < 0.
The nonzero elements of L and U in the LU decomposition of A are eas-
ily seen to be lij = xi+1−j and uij =

j −1
i −1

. The nonzero elements of
the inverses of L and U are then seen to have (i, j) elements yi+1−j and
(−1)i−j
 j −1
i −1

. The determinant of A is xn
1 .
For some choices of x1, . . . , xn, it is easy to determine the condition num-
bers, especially with respect to the L1 norm, of the matrices A generated in
this way. Ericksen (1985) suggests that the xs be chosen as
x1 = 2m
for m ≤0
and
xi =

k
i −1

for i = 2, . . . , n
and k ≥2,

552
12 Software for Numerical Linear Algebra
in which case the L1 condition number of 10 × 10 matrices will range from
about 107 to 1017 as n ranges from 2 to 20 for m = 0 and will range from
about 1011 to 1023 as n ranges from 2 to 20 for m = −1.
For testing algorithms for computing eigenvalues, a useful matrix is a
Wilkinson matrix, which is a symmetric, tridiagonal matrix with 1s on the
oﬀ-diagonals. For an n × n Wilkinson matrix, the diagonal elements are
n −1
2
, n −3
2
, n −5
2
, . . . , n −5
2
, n −3
2
, n −1
2
.
If n is odd, the diagonal includes 0, otherwise all of the diagonal elements are
positive. The 5 × 5 Wilkinson matrix, for example, is
⎡
⎢⎢⎢⎢⎣
2 1 0 0 0
1 1 1 0 0
0 1 0 1 0
0 0 1 1 1
0 0 0 1 2
⎤
⎥⎥⎥⎥⎦
.
The two largest eigenvalues of a Wilkinson matrix are very nearly equal. Other
pairs are likewise almost equal to each other: the third and fourth largest
eigenvalues are also close in size, the ﬁfth and sixth largest are likewise, and
so on. The largest pair is closest in size, and each smaller pair is less close
in size. The Matlab function wilkinson(n) generates an n × n Wilkinson
matrix.
Another test matrix available in Matlab is the Rosser test matrix, which
is an 8 × 8 matrix with an eigenvalue of multiplicity 2 and three nearly equal
eigenvalues. It is constructed by the Matlab function rosser.
A well-known, large, and wide-ranging set of test matrices for computa-
tional algorithms for various problems in linear algebra was compiled and
described by Gregory and Karney (1969). Higham (1991, 2002) also describes
a set of test matrices and provides Matlab programs to generate the matrices.
Another set of test matrices is available through the “Matrix Market”, de-
signed and developed by Boisvert, Pozo, and Remington of the U.S. National
Institute of Standards and Technology with contributions by various other
people. The test matrices can be accessed at
http://math.nist.gov/MatrixMarket
The database can be searched by specifying characteristics of the test matrix,
such as size, symmetry, and so on. Once a particular matrix is found, its
sparsity pattern can be viewed at various levels of detail, and other pertinent
data can be reviewed. If the matrix seems to be what the user wants, it can be
downloaded. The initial database for the Matrix Market is the approximately
300 problems from the Harwell-Boeing Sparse Matrix Collection.
A set of test datasets for statistical analyses has been developed by the
National Institute of Standards and Technology. This set, called “statistical

12.1 General Considerations
553
reference datasets” (StRD), includes test datasets for linear regression, anal-
ysis of variance, nonlinear regression, Markov chain Monte Carlo estimation,
and univariate summary statistics. It is available at
http://www.itl.nist.gov/div898/strd/
12.1.5.2 Assessing the Accuracy of a Computed Result
In real-life applications, the correct solution is not known, and this may also
be the case for randomly generated test datasets. If the correct solution is not
known, internal consistency tests as discussed in Sect. 11.2.3 may be used to
assess the accuracy of the computations in a given problem.
12.1.5.3 Software Reviews
Reviews of available software play an important role in alerting the user to
both good software to be used and bad software to be avoided. Software
reviews also often have the salutary eﬀect of causing the software producers
to improve their products.
Software reviews vary in quality. Some reviews are rather superﬁcial, and
reﬂect mainly the overall impressions of the reviewer. For software described in
the peer-reviewed literature, eﬀorts like the ACM’s Replicated Computational
Results review process can improve the quality of reviews (see below).
12.1.6 Reproducible Research
As the research of statisticians, computer scientists, and applied mathemati-
cians has become ever more computationally-oriented, the reproducibility of
the research has become much more of an issue. Research involving only logi-
cal reasoning or mathematical manipulations is immediately reproducible by
it essential nature. Research involving mathematical manipulations that also
include some assumptions about underlying physical phenomena must be val-
idated by observational studies. Experiments or observational studies in the
natural sciences, however, by long-standing tradition, must also be repro-
ducible by experimentation or through other observational studies in order to
be accepted by other scientists.
Reproducibility of research based on computational results requires access
to the original data or to data substantially similar, and access to the entire
workﬂow, including particularly the computational methods. Data may have
substantial value, and the owner of the data, in order to protect that value,
may impose strict limits on its usage and distribution. Data may also have
conﬁdential, possibly personal, information that must be protected. Often the
only way to ensure protection is to restrict the distribution of the data. Just
because there are impediments to the sharing of data, however, does not mean

554
12 Software for Numerical Linear Algebra
that sharing of data or at least of substantially similar data or anonymized
data is not necessary.
The actual computations may depend on the computing system, both
the hardware and the software. It is an easy matter to include all of the
technical speciﬁcations in a description of the research. These include version
numbers of published software and exact copies of any ad hoc software. In
the case of simulation studies, reproducibility requires speciﬁcation of the
random number generator and any settings used. GitHub, mentioned above,
is not only a convenient tool for collaborative research, it also provides a good
system of version control and a repository for software (and other documents)
to be part of the published research.
One of the main issues in ensuring that research is reproducible is the
simple housekeeping involved in keeping correct versions of software and doc-
uments together. There are various tools for doing this. Two systems that
are helpful in integrating R scripts with LATEX documents are Sweave and
knitr. Xie (2015) describes knitr in detail. Stodden et al. (2014) and Gan-
drud (2015) discuss Sweave, knitr, and other tools for integrating programs
with documents, and also provide general recommendations for ensuring re-
producibility of research. The Reproducible R Toolkit (see page 580) is useful
for version control.
Another major way in which the quality of research is ensured is through
the peer review and editorial process. There is a variety of publication outlets,
and in many, this process is not carried out in an ideal fashion. The publication
of algorithms and computer programs is often taken rather lightly. One form
of publication, for example, is an R package and associated documentation in
CRAN, the Comprehensive R Archive Network. This repository is extremely
useful, not only because of the number of high-quality packages it contains,
but in the maintenance of the system provided by the R Project team. As the
various packages are used in diﬀerent settings, and because of the open-source
nature of the software, there is a certain amount of quality control. Software
bugs, however, are notoriously diﬃcult to ferret out, and some R packages are
less than reliable.
The ACM, through its journals such as Transactions on Mathematical
Software (TOMS), publishes software that has at least been through a stan-
dard peer-review refereeing process. The refereeing has been enhanced with a
“Replicated Computational Results” or “RCR” review process, in which a ref-
eree performs a more extensive review that includes execution of the program
in various settings. The RCR reviewer writes a report that is accompanied by
the name of the reviewer. The paper/software itself receives a special RCR
designation in TOMS. See Heroux (2015) for a description of the process.

12.2 Software Libraries
555
12.2 Software Libraries
There are a number of libraries of subprograms for carrying out common oper-
ations in linear algebra and other scientiﬁc computations. The libraries vary in
several ways: free or with licensing costs or user fees; low-level computational
modules or higher-level, more application-oriented programs; specialized or
general purpose; and quality, from high to low. Many of these subprograms
were originally written in Fortran and later translated into C. Now many of
the common libraries are available in other general-purpose programming lan-
guages, such as Python. The basic libraries have also been adapted for use
in higher-level software systems such as R and Matlab. In the higher-level
systems and in Python, libraries are usually called “packages”.
In any system, when multiple packages or libraries are installed, there
may be multiple versions of some of the components. Management of the sep-
arate packages in a single system can be carried out by a package manager
such as Conda. Conda is written in Python, and generally oriented toward
Python packages but it can be used to manage packages for other software
systems, including R. A version of Conda, called Anaconda, includes several
Python packages in addition to the package manager. Anaconda and informa-
tion about it are available at
https://www.continuum.io/
12.2.1 BLAS
There are several basic computations for vectors and matrices that are very
common across a wide range of scientiﬁc applications. Computing the dot
product of two vectors, for example, is a task that may occur in such diverse
areas as ﬁtting a linear model to data or determining the maximum value
of a function. While the dot product is relatively simple, the details of how
the computations are performed and the order in which they are performed
can have eﬀects on both the eﬃciency and the accuracy; see the discussion
beginning on page 487 about the order of summing a list of numbers.
There is a widely-used standard set of routines called “basic linear algebra
subprograms” (BLAS) implement many of the standard operations for vectors
and matrices. The BLAS represent a very signiﬁcant step toward software
standardization because the deﬁnitions of the tasks and the user interface
are the same on all computing platforms. The BLAS can be thought of as
“software parts” (Rice 1993). They are described in terms of the interface
to a general-purpose programming language (an “application programming
interface”, or API). A software part, similar to an electrical component in
an electronic device, conforms to functional speciﬁcations, which allow an
engineer to construct a larger system using that part, possibly together with
other parts. The speciﬁc “binding”, or name and list of arguments, may be
diﬀerent for diﬀerent software systems, and certainly the actual coding may

556
12 Software for Numerical Linear Algebra
be quite diﬀerent to take advantage of special features of the hardware or
underlying software such as compilers.
The operations performed by the BLAS often cause an input variable to be
updated. For example, in a Givens rotation, two input vectors are rotated into
two new vectors (see rot below). In this case, it is natural and eﬃcient just to
replace the input values with the output values. A natural implementation of
such an operation is to use an argument that is both input and output. In some
programming paradigms, such a “side eﬀect” can be somewhat confusing, but
the value of this implementation outweighs the undesirable properties.
There is a consistency of the interface among the BLAS routines. The
nature of the arguments and their order in the reference are similar from one
routine to the next. The general order of the arguments is:
1. the size or shape of the vector or matrix,
2. the array itself, which may be either input or output,
3. the stride, and
4. other input arguments.
The ﬁrst and second types of arguments are repeated as necessary for each of
the operand arrays and the resultant array.
A BLAS routine is identiﬁed by a root character string that indicates
the operation, for example, dot or axpy. The name of the BLAS program
module may depend on the programming language. In some languages, the
root may be preﬁxed by s to indicate single precision, by d to indicate double
precision, or by c to indicate complex, for example. If the language allows
generic function and subroutine references, just the root of the name is used.
The axpy operation we referred to on page 12 multiplies one vector by
a constant and then adds another vector (ax + y). The BLAS routine axpy
performs this operation. The interface is
axpy(n, a, x, incx, y, incy)
where
n,
the number of elements in each vector,
a,
the scalar constant,
x,
input/output one-dimensional array that contains
the elements of the vector x,
incx,
the stride in the array x that deﬁnes the vector,
y,
the input/output one-dimensional array that contains
the elements of the vector y, and
incy,
the stride in the array y that deﬁnes the vector.
As another example, the routine rot to apply a Givens rotation has the
interface

12.2 Software Libraries
557
rot(n, x, incx, y, incy, c, s)
where
n,
the number of elements in each vector,
x,
the input/output one-dimensional array that contains
the elements of the vector x,
incx,
the stride in the array x that deﬁnes the vector,
y,
the input/output one-dimensional array that contains
the elements of the vector y,
incy,
the stride in the array y that deﬁnes the vector,
c,
the cosine of the rotation, and
s,
the sine of the rotation.
This routine is invoked after rotg has been called to determine the cosine and
the sine of the rotation (see Exercise 12.5, page 583).
Source programs and additional information about the BLAS can be ob-
tained at
http://www.netlib.org/blas/
There is a software suite called ATLAS (Automatically Tuned Linear Al-
gebra Software) that provides Fortran and C interfaces to a portable BLAS
binding as well as to other software for linear algebra for various processors.
Information about the ATLAS software can be obtained at
http://math-atlas.sourceforge.net/
Intel Math Kernel Library (MKL) is a library of optimized routines that
takes advantage of the architecture of Intel processors, or other processors
built on the same architecture. The functions include the BLAS, LAPACK (see
next section), FFT, and various miscellaneous computations. The BLAS and
LAPACK routines use the standard interfaces, so programs using BLAS and
LAPACK can simply be re-linked with the MKL. The functions are designed
to perform multithreaded operations using all of the available cores in an
Intel CPU. There is an R package RevoUtilsMath which provides for the
incorporation of the Math Kernel Library into R on systems running on Intel
or compatible processors.
12.2.2 Level 2 and Level 3 BLAS, LAPACK, and Related Libraries
The level 1 BLAS or BLAS-1, the original set of the BLAS, are for vector
operations. They were deﬁned by Lawson et al. (1979). The basic operation
is axpy, for vectors and scalars:
αx + y.
(Here I have adopted the notation commonly used now by the numerical an-
alysts who work on the BLAS.) General operations on a matrix and a vector

558
12 Software for Numerical Linear Algebra
or on two matrices can be put together using the BLAS-1. Depending on the
architecture of the computer, greater eﬃciency can be achieved by tighter
coupling of the computations involving all elements in a matrix without re-
gard to whether they are in the same row or same column. Dongarra et al.
(1988) deﬁned a set of the BLAS, called level 2 or the BLAS-2, for opera-
tions involving a matrix and a vector. The basic operation is “gemv” (general
matrix-vector),
αAx + βy.
Later, a set called the level 3 BLAS or the BLAS-3, for operations involving
two dense matrices, was deﬁned by Dongarra et al. (1990), and a set of the level
3 BLAS for sparse matrices was proposed by Duﬀet al. (1997). An updated
set of BLAS is described by Blackford et al. (2002). The basic operation is
“gemm” (general matrix-matrix),
αAB + βC.
Duﬀand V¨omel (2002) provide a set of Fortran BLAS for sparse matrices.
When work was being done on the BLAS-1 in the 1970s, those lower-level
routines were being incorporated into a higher-level set of Fortran routines for
matrix eigensystem analysis called EISPACK (Smith et al. 1976) and into a
higher-level set of Fortran routines for solutions of linear systems called LIN-
PACK (Dongarra et al. 1979). As work progressed on the BLAS-2 and BLAS-3
in the 1980s and later, a uniﬁed set of Fortran routines for both eigenvalue
problems and solutions of linear systems was developed, called LAPACK (An-
derson et al. 2000). A Fortran 95 version, LAPACK95, is described by Barker
et al. (2001). Current information about LAPACK is available at
http://www.netlib.org/lapack/
There is a graphical user interface to help the user navigate the LAPACK site
and download LAPACK routines.
There are several other libraries that provide functions not only for nu-
merical linear algebra but also for a wide range of scientiﬁc computations.
Two of the most widely-used proprietary Fortran and C libraries are the
IMSL Libraries and the Nag Library. The GNU Scientiﬁc Library (GSL) is a
widely-used and freely-distributed C library. See Galassi et al. (2002) and the
web site
http://www.gnu.org/gsl/
Many of the GSL functions, as well as others, have been incorporated
into the Python package called numpy. For a person with limited resources,
Python together with packages such as numpy is one of the cheapest and easiest
ways to get up and running with numerical computations and graphics on a
general-purpose personal computer. The free Anaconda system will quickly
install Python along with the packages.

12.2 Software Libraries
559
All of these libraries provide large numbers of routines for numerical lin-
ear algebra, ranging from very basic computations as provided in the BLAS
through complete routines for solving various types of systems of equations
and for performing eigenanalysis.
12.2.3 Libraries for High Performance Computing
Because computations for linear algebra are so pervasive in scientiﬁc appli-
cations, it is important to have very eﬃcient software for carrying out these
computations. We have discussed several considerations for software eﬃciency
in previous chapters.
If the matrices in large-scale problems are sparse, it is important to take
advantage of that sparsity both in the storage and in all computations. We
discussed storage schemes on page 548. For sparse matrices it is necessary to
have a scheme for identifying the locations of the nonzeros and for specifying
their values. The nature of storage schemes varies from one software package
to another, but the most common is index-index-value (see page 550). It is
also important to preserve the sparsity during intermediate computations.
Software for performing computations on sparse matrices is specialized to
take advantage of the zero entries. Duﬀ, Heroux, and Pozo (2002) discuss
special software for sparse matrices.
12.2.3.1 Libraries for Parallel Processing
In parallel processing, the main issues are how to divide up the computa-
tions and how to share the results. A widely-used standard for communica-
tion among processors is the Message-Passing Interface (MPI) developed in
the 1990s. MPI allows for standardized message passing across languages and
systems. See Gropp et al. (2014) for a description of the MPI system. Current
information is available at
https://www.open-mpi.org/
IBM has built the Message Passing Library (MPL) in both Fortran and
C, which provides message-passing kernels.
A system such as MPL that implements the MPI paradigm is essentially a
library of functions. Another approach to parallel processing is to incorporate
language constructs into the programming language itself, thus providing a
higher-level facility. OpenMP is a language extension for expressing parallel
operations. See Chapman et al. (2007) for a description of OpenMP. Current
information is available at
http://www.openmp.org/
A standard set of library routines that are more application-oriented, called
the BLACS (Basic Linear Algebra Communication Subroutines), provides a
portable message-passing interface primarily for linear algebra computations

560
12 Software for Numerical Linear Algebra
with a user interface similar to that of the BLAS. A slightly higher-level set
of routines, the PBLAS, combine both the data communication and compu-
tation into one routine, also with a user interface similar to that of the BLAS.
Filippone and Colajanni (2000) provide a set of parallel BLAS for sparse ma-
trices. Their system, called PSBLAS, shares the general design of the PBLAS
for dense matrices and the design of the level 3 BLAS for sparse matrices.
ScaLAPACK, described by Blackford et al. (1997b), is a distributed mem-
ory version of LAPACK that uses the BLACS and the PBLAS modules. The
computations in ScaLAPACK are organized as if performed in a “distributed
linear algebra machine” (DLAM), which is constructed by interconnecting
BLAS with a BLACS network. The BLAS perform the usual basic computa-
tions and the BLACS network exchanges data using primitive message-passing
operations. The DLAM can be constructed either with or without a host pro-
cess. If a host process is present, it would act like a server in receiving a
user request, creating the BLACS network, distributing the data, starting
the BLAS processes, and collecting the results. ScaLAPACK has routines for
LU, Cholesky, and QR decompositions and for computing eigenvalues of a
symmetric matrix. The routines are similar to the corresponding routines in
LAPACK (Table 12.2). Even the names are similar, for example, in Fortran:
Table 12.2. LAPACK and ScaLAPACK
LAPACK ScaLAPACK
dgetrf
pdgetrf
LU factorization
dpotrf
pdpotrf
Cholesky factorization
dgeqrf
pdgeqrf
QR factorization
dsyevx
pdsyevx
Eigenvalues/vectors of symmetric matrix
Three other packages for numerical linear algebra that deserve mention
are Trilinos, Blaze, and PLASMA. Trilinos is a collection of compatible soft-
ware packages that support parallel linear algebra computations, solution of
linear and nonlinear equations and eigensystems of equations and related ca-
pabilities. The majority of packages are written in C++ using object-oriented
techniques. All packages are self-contained, with the Trilinos top layer provid-
ing a common look and feel and infrastructure.
The main Trilinos web site is
http://software.sandia.gov/trilinos/
All of the Trilinos packages are available on a range of platforms, especially
on high-performance computers.
Blaze is an open-source, high-performance C++ library for dense and
sparse matrices. Blaze consists of a number of LAPACK wrapper functions
that simplify the use of LAPACK.
The main Blaze web site is

12.2 Software Libraries
561
http://bitbucket.org/blaze-lib/blaze/
There is also an R package RcppBlaze which provides for the incorporation
of the Blaze software into R using the Rcpp package for use of C++ code in
R.
The
Parallel Linear
Algebra
for
Scalable
Multi-core
Architectures
(PLASMA) system is based on OpenMP multithreading directives.
It in-
cludes linear systems solvers, mixed precision iterative reﬁnement, matrix
norms (fast, multithreaded), a full set of multithreaded BLAS 3, matrix
inversion, least squares, and band linear solvers. See Buttari et al. (2009) for
a description of the overall design. The functionality of PLASMA continues
to increase.
The constructs of an array-oriented language such as R or modern Fortran
are helpful in thinking of operations in such a way that they are naturally
parallelized. While the addition of arrays in FORTRAN 77 or C is an operation
that leads to loops of sequential scalar operations, in modern Fortran it is
thought of as a single higher-level operation. How to perform operations in
parallel eﬃciently is still not a natural activity, however. For example, the two
modern Fortran statements to add the arrays a and b and then to add a and
c
d = a + b
e = a + c
may be less eﬃcient than loops because the array a may be accessed twice.
General references that describe parallel computations and software for
linear algebra include Nakano (2012), Quinn (2003), and Roosta (2000).
12.2.3.2 Clusters of Computers
The software package PVM, or Parallel Virtual Machine, which was developed
at Oak Ridge National Laboratory, the University of Tennessee, and Emory
University, provides a set of C functions or Fortran subroutines that allow a
heterogeneous collection of Unix or Linux computers to operate smoothly as
a multicomputer (see Geist et al. 1994). The development of PVM is ongoing.
There is currently a set of bindings to PVM from R, called RPVM.
A cluster of computers is a very cost-eﬀective method for high-performance
computing. A standard technology for building a cluster of Unix or Linux
computers is called Beowulf (see Gropp, Lusk, and Sterling 2003). A system
called Pooch is available for linking Apple computers into clusters (see Dauger
and Decyk 2005).
12.2.3.3 Graphical Processing Units
A typical central processing unit (CPU) consists of registers into which
operands are brought, manipulated as necessary, and then combined in a

562
12 Software for Numerical Linear Algebra
manner simulating the mathematical operation to be performed. The regis-
ters themselves may be organized into a small number of “cores”, each of
which can perform separate arithmetical operations. In the late 1990s, Nvidia
extended this type of design to put thousands of cores on a single unit. This
was done initially so as to update graphical images, which only requires simple
operations on data representing states of pixels. This type of processing unit
is called a graphical processing unit or GPU.
Over time, the cores of a GPU were developed further so as to be able
to perform more general arithmetic operations. Nvidia integrated GPUs with
CPUs in a “compute uniﬁed device architecture”, or CUDA. CUDA now refers
to a programming interface at both a low and a high level to allow Fortran
or C programs to utilize an architecture that combines GPUs and CPUs.
Various CUDA libraries are available, such as BLAS (cuBLAS) and routines
for computations with sparse matrices (cuSPARSE).
Cheng, Grossman, and McKercher (2014) provide an introduction to
CUDA programming in C. They discuss matrix multiplication in CUDA on
both shared-memory and unshared-memory architectures. In a CUDA imple-
mentation, each core within the GPU accumulates one entry of the product
matrix.
12.2.4 The IMSL Libraries
The IMSL libraries are available in both Fortran and C versions and in both
single and double precisions. These libraries use the BLAS and other software
from LAPACK. Sparse matrices are generally stored in the index-index-value
scheme, although other storage modes are also implemented.
12.2.4.1 Examples of Use of the IMSL Libraries
There are separate IMSL routines for single and double precisions. The names
of the Fortran routines share a common root; the double-precision version
has a D as its ﬁrst character, usually just placed in front of the common
root. Functions that return a ﬂoating-point number but whose mnemonic
root begins with an I through an N have an A in front of the mnemonic root
for the single-precision version and have a D in front of the mnemonic root for
the double-precision version. Likewise, the names of the C functions share a
common root. The function name is of the form imsl f root name for single
precision and imsl d root name for double precision.
Consider the problem of solving the system of linear equations
x1 + 4x2 + 7x3 = 10,
2x1 + 5x2 + 8x3 = 11,
3x1 + 6x2 + 9x3 = 12.

12.2 Software Libraries
563
Write the system as Ax = b. The coeﬃcient matrix A is real (not necessarily
REAL) and square. We can use various IMSL subroutines to solve this problem.
The two simplest basic routines are lslrg/dlslrg and lsarg/dlsarg. Both
have the same set of arguments:
n,
the problem size.
A,
the coeﬃcient matrix.
lda,
the leading dimension of A (A can be deﬁned to be bigger
than it actually is in the given problem).
b,
the right-hand sides.
ipath,
an indicator of whether Ax = b or Atx = b is to be solved.
x,
the solution.
The diﬀerence in the two routines is whether or not they do iterative reﬁne-
ment. A program to solve the system using lsarg (without iterative reﬁne-
ment) is shown in Fig. 12.2.
C
Fortran 77 program
PARAMETER (ida=3)
INTEGER
n, ipath
REAL
A(ida, ida), b(ida), x(ida)
C
Storage is by column;
C
nonblank character in column 6 indicates continuation
DATA
A/1.0,
2.0,
3.0,
+
4.0,
5.0,
6.0,
+
7.0,
8.0,
9.0/
DATA
b/10.0, 11.0, 12.0/
n
= 3
ipath = 1
CALL lsarg (n, A, lda, b, ipath, x)
PRINT *, ’The solution is’, x
END
Figure 12.2. IMSL Fortran program to solve the system of linear equations
The IMSL C function to solve this problem is lin sol gen, which is avail-
able as ﬂoat *imsl f lin sol gen or double *imsl d lin sol gen. The only
required arguments for *imsl f lin sol gen are:
int n,
the problem size;
ﬂoat a[],
the coeﬃcient matrix; and
ﬂoat b[],
the right-hand sides.
Either function will allow the array a to be larger than n, in which case the
number of columns in a must be supplied in an optional argument. Other

564
12 Software for Numerical Linear Algebra
optional arguments allow the speciﬁcation of whether Ax = b or ATx = b is
to be solved (corresponding to the argument ipath in the Fortran subroutines
lslrg/dlslrg and lsarg/dlsarg), the storage of the LU factorization, the
storage of the inverse, and so on. A program to solve the system is shown in
Fig. 12.3. Note the diﬀerence between the column orientation of Fortran and
the row orientation of C.
\*
C program *\
#include <imsl.h>
#include <stdio.h>
main()
{
int
n = 3;
float *x;
\* Storage is by row;
statements are delimited by ’;’,
so statements continue automatically. */
float a[] = {1.0,
4.0,
7.0,
2.0,
5.0,
8.0,
3.0,
6.0,
9.0};
float b[] = {10.0, 11.0, 12.0};
x = imsl_f_lin_sol_gen (n, a, IMSL_A_COL_DIM, 3, b, 0);
printf ("The solution is %10.4f%10.4f%10.4f\n",
x[0], x[1], x[2]);
}
Figure 12.3. IMSL C program to solve the system of linear equations
The argument IMSL A COL DIM is optional, taking the value of n, the num-
ber of equations, if it is not speciﬁed. It is used in Fig. 12.3 only for illustration.
12.3 General Purpose Languages
Fortran, C, and Python are the most commonly used procedural languages
for scientiﬁc computation. The International Organization for Standardiza-
tion (ISO), has speciﬁed standard deﬁnitions of Fortran and C, so users’ and
developers’ investments in these two languages have a certain amount of pro-
tection. A standard version of Python is controlled by the nonproﬁt Python
Software Foundation.
The development of accepted standards for computer languages began
with the American National Standards Institute (ANSI). Now its international
counterpart, ISO, is the main body developing and promoting standards for

12.3 General Purpose Languages
565
computer languages. Whenever ANSI and ISO both have a standard for a
given version of a language, the standards are the same.
There are various dialects of both Fortran and C, most of which result
from “extensions” provided by writers of compilers. While these extensions
may make program development easier and occasionally provide modest en-
hancements to execution eﬃciency, a major eﬀect of the extensions is to lock
the user into a speciﬁc compiler. Because users usually outlive compilers, it
is best to eschew the extensions and to program according to the ANSI/ISO
standards. Several libraries of program modules for numerical linear algebra
are available both in standard Fortran and in standard C.
C began as a low-level language that provided many of the capabilities
of a higher-level language together with more direct access to the operating
system. It still lacks some of the facilities that are very useful in scientiﬁc
computation.
C++ is an object-oriented programming language built on C. The object-
oriented features make it much more useful in computing with vectors and
matrices or other arrays and more complicated data structures. Class libraries
can be built in C++ to provide capabilities similar to those available in For-
tran. There are also ANSI (or ISO) standard versions of C++.
An advantage of C over Fortran is that it provides for easier communica-
tion between program units, so it is often used when larger program systems
are being put together. Other advantages of C are the existence of widely-
available, inexpensive compilers, and the fact that it is widely taught as a
programming language in beginning courses in computer science.
Fortran has evolved over many years of use by scientists and engineers.
Both ANSI and ISO have speciﬁed standard deﬁnitions of various versions of
Fortran. A version called FORTRAN was deﬁned in 1977 (see ANSI 1978).
We refer to this version along with a modest number of extensions as FOR-
TRAN 77. If we mean to exclude any extensions or modiﬁcations, we refer
to it as ANSI FORTRAN 77. (Although the formal name of that version of
the language is written in upper case, most people and I generally write it
in lower case except for the ﬁrst letter.) A new standard (not a replacement
standard) was adopted in 1990 by ANSI, at the insistence of ISO. This stan-
dard language is called ANSI Fortran 90 (written in lower case except for the
ﬁrst letter) or ISO Fortran 90 (see ANSI 1992). It has a number of features
that extend its usefulness, especially in numerical linear algebra. There have
been a few revisions of Fortran 90 in the past several years. There are only
small diﬀerences between Fortran 90 and subsequent versions, which are called
Fortran 95, Fortran 2000, Fortran 2003, Fortran 2008, and Fortran 2015. For-
tran 2008 introduced the concept of multiple images for parallel processing.
Each image can be thought of as a separate program executing in its own
environment. Data are shared by a special construct, called a coarray.
Most of the features I discuss are in all versions after Fortran 95, which I
will generally refer to just as “Fortran” or, possibly, “modern Fortran”.

566
12 Software for Numerical Linear Algebra
Modern Fortran provides additional facilities for working directly with
arrays. For example, to add matrices A and B we can write the Fortran
expression A+B.
Compilers for Fortran are often more expensive and less widely available
than compilers for C/C++. An open-source free compiler for Fortran 95 is
available at
http://www.g95.org/
This compiler also implements some of the newer features, such as coarrays,
which were introduced in the later versions of Fortran 2003 and Fortran 2008.
A disadvantage of Fortran compared with C/C++ is that fewer people
outside of the numerical computing community know the language.
12.3.1 Programming Considerations
Both users and developers of software need to be aware of a number of pro-
gramming details.
Sometimes within the execution of an iterative algorithm it is necessary
to perform some operation outside of the basic algorithm itself. A common
example of this is in an online algorithm, in which more data must be brought
in between the operations of the online algorithm. A simple example of this
is the online computation of a correlation matrix using an algorithm simi-
lar to equations (10.8) on page 504. When the ﬁrst observation is passed to
the program doing the computations, that program must be told that this
is the ﬁrst observation (or, more generally, the ﬁrst n1 observations). Then,
for each subsequent observation (or set of observations), the program must
be told that these are intermediate observations. Finally, when the last ob-
servation (or set of observations, or even a null set of observations) is passed
to the computational program, the program must be told that these are the
last observations, and wrap-up computations must be performed (computing
covariances and correlations from sums of squares). Between the ﬁrst and last
invocations of the computational program, it may preserve intermediate re-
sults that are not passed back to the calling program. In this simple example,
the communication is one-way, from calling routine to called routine.
12.3.1.1 Reverse Communication in Iterative Algorithms
In more complicated cases using an iterative algorithm, the computational
routine may need more general input or auxiliary computations, and hence
there may be two-way communication between the calling routine and the
called routine. A two-way communication between the routines is sometimes
called reverse communication. An example is the repetition of a precondition-
ing step in a routine using a conjugate gradient method; as the computations
proceed, the computational routine may detect a need for rescaling and so
return to a calling routine to perform those services. Barrett et al. (1994) and

12.3 General Purpose Languages
567
Dongarra and Eijkhout (2000) describe a variety of uses of reverse communi-
cation in software for numerical linear algebra.
12.3.1.2 Computational Eﬃciency
Two seemingly trivial things can have major eﬀects on computational ef-
ﬁciency. One is movement of data between the computer’s memory and the
computational units. How quickly this movement occurs depends, among other
things, on the organization of the data in the computer. Multiple elements of
an array can be retrieved from memory more quickly if they are in contiguous
memory locations. (Location in computer memory does not necessarily refer
to a physical place; in fact, memory is often divided into banks, and adjacent
“locations” are in alternate banks. Memory is organized to optimize access.)
The main reason that storage of data in contiguous memory locations aﬀects
eﬃciency involves the diﬀerent levels of computer memory. A computer often
has three general levels of randomly accessible memory, ranging from “cache”
memory, which is very fast, to “disk” memory, which is relatively slower.
When data are used in computations, they may be moved in blocks, or pages,
from contiguous locations in one level of memory to a higher level. This allows
faster subsequent access to other data in the same page. When one block of
data is moved into the higher level of memory, another block is moved out.
The movement of data (or program segments, which are also data) from one
level of memory to another is called “paging”.
In Fortran, a column of a matrix occupies contiguous locations, so when
paging occurs, elements in the same column are moved. Hence, a column of
a matrix can often be operated on more quickly in Fortran than a row of a
matrix. In C, a row can be operated on more quickly for similar reasons.
Some computers have array processors that provide basic arithmetic oper-
ations for vectors. The processing units are called vector registers and typically
hold 128 or 256 full-precision ﬂoating-point numbers (see Sect. 10.1). For soft-
ware to achieve high levels of eﬃciency, computations must be organized to
match the length of the vector processors as often as possible.
Another thing that aﬀects the performance of software is the execution of
loops. In the simple loop
do i = 1, n
sx(i) = sin(x(i))
end do
it may appear that the only computing is just the evaluation of the sine of
the elements in the vector x. In fact, a nonnegligible amount of time may be
spent in keeping track of the loop index and in accessing memory. A compiler
on a vector computer may organize the computations so that they are done
in groups corresponding to the length of the vector registers. On a computer
that does not have vector processors, a technique called “unrolling do-loops”

568
12 Software for Numerical Linear Algebra
is sometimes used. For the code segment above, unrolling the do-loop to a
depth of 7, for example, would yield the following code:
do i = 1, n, 7
sx(i)
= sin(x(i))
sx(i+1) = sin(x(i+1))
sx(i+2) = sin(x(i+2))
sx(i+3) = sin(x(i+3))
sx(i+4) = sin(x(i+4))
sx(i+5) = sin(x(i+5))
sx(i+6) = sin(x(i+6))
end do
plus a short loop for any additional elements in x beyond 7⌊n/7⌋. Obviously,
this kind of programming eﬀort is warranted only when n is large and when the
code segment is expected to be executed many times. The extra programming
is deﬁnitely worthwhile for programs that are to be widely distributed and
used, such as the BLAS.
For matrices with special patterns, the storage mode (see page 548) can
have a major eﬀect on the computational eﬃciency, both in memory access
and in the number of computations that can be avoided.
12.3.2 Modern Fortran
Fortran began as a simple language to translate common mathematical ex-
pressions into computer instructions. This heritage ensures that much of the
language “looks like” what most people write in standard mathematics. Early
versions of Fortran had a lot of “computerese” restrictions, however (parts of
statements had to be in certain columns, and so on). Statement grouping was
done in an old-fashioned way, using numbers. More seriously, many things
that we might want to do were very diﬃcult.
Fortran has evolved, and modern Fortran is now one of the most useful
general-purpose compiler languages for numerical computations. Some books
that describe modern Fortran are Clerman and Spector (2012), Hanson and
Hopkins (2013), Lemmon and Schafer (2005), Markus (2012), and Metcalf,
Reid, and Cohen (2011).
For the scientiﬁc programmer, one of the most useful features of mod-
ern Fortran is the provision of primitive constructs for vectors and matrices.
Whereas all of the FORTRAN 77 intrinsics are scalar-valued functions, For-
tran 95 provides intrinsic array-valued functions. For example, if A and B
represent matrices conformable for addition, the statement D = A+B performs
the operation; if they are conformable for multiplication, the statement
C = MATMUL(A, B)
yields the Cayley product in C. The MATMUL function also allows multiplication
of vectors and matrices.

12.3 General Purpose Languages
569
Indexing of arrays starts at 1 by default (any starting value can be speci-
ﬁed, however), and storage is column-major.
Space must be allocated for arrays in modern Fortran, but this can be done
at run time. An array can be initialized either in the statement allocating the
space or in a regular assignment statement. A vector can be initialized by
listing the elements between “(/” and “/)”. This list can be generated in
various ways. The RESHAPE function can be used to initialize matrices.
For example, a modern Fortran statement to declare that the variable A is
to be used as a 3 × 4 array and to allocate the necessary space is
REAL, DIMENSION(3,4) :: A
A Fortran statement to initialize A with the matrix
⎡
⎣
1 4 7 10
2 5 8 11
3 6 9 12
⎤
⎦
is
A = RESHAPE( (/ 1., 2., 3., &
4., 5., 6., &
7., 8., 9., &
10.,11.,12./), &
(/3,4/) )
Fortran has an intuitive syntax for referencing subarrays, shown in Ta-
ble 12.3. (See also Table 12.1 on page 548.)
Table 12.3. Subarrays in modern Fortran
A(2:3,1:3) The 2 × 3 submatrix in rows 2 and 3
and columns 1 to 3 of A
A(:,1:4:2) Refers to the submatrix with all three rows
and the ﬁrst and third columns of A
A(:,4)
Refers to the column vector that is the fourth column of A
Notice that because the indexing starts with 1 (instead of 0) the corre-
spondence between the computer objects and the mathematical objects is a
natural one. The subarrays can be used directly in functions. For example, if
A is as shown above, and B is the matrix
⎡
⎢⎢⎣
1 5
2 6
3 7
4 8
⎤
⎥⎥⎦,

570
12 Software for Numerical Linear Algebra
the Fortran function reference
MATMUL(A(1:2,2:3), B(3:4,:))
yields the Cayley product
!
4 7
5 8
" !
3 7
4 8
"
.
(12.9)
Fortran also contains some of the constructs, such as FORALL, that have
evolved to support parallel processing.
More extensive later revisions (Fortran 2000 and subsequent versions) in-
clude such features as exception handling, better interoperability with C, al-
locatable components, parameterized derived types, object-oriented program-
ming, and coarrays.
A good index of freely-available Fortran software (and also C and C++
software) is
http://www.netlib.org/utk/people/JackDongarra/la-sw.html
12.3.3 C and C++
C is a computer language ideally suited to computer systems operation and
management. It is stable and widely used, meaning that an investment in C
software is likely to continue being worthwhile. There are many libraries of C
programs for a wide range of applications.
Indexes of arrays in C begin at 0. (That is the way that positions in
hardware registers are normally counted, but not the way we usually index
mathematical entities.) Two-dimensional arrays in C are row-major, and in
general, the indexes in multi-dimensional arrays as a mapping to computer
memory vary fastest from right to left. A programmer mixing Fortran code
with C code must be constantly aware of these two properties.
C++ is an object-oriented dialect of C. In an object-oriented language, it
is useful to deﬁne classes corresponding to matrices and vectors. Operators
and/or functions corresponding to the usual operations in linear algebra can
be deﬁned so as to allow use of simple expressions to perform these operations.
A class library in C++ can be deﬁned in such a way that the computer
code corresponds more closely to mathematical code. The indexes to the arrays
can be deﬁned to start at 1, and the double index of a matrix can be written
within a single pair of parentheses. For example, in a C++ class deﬁned for
use in scientiﬁc computations, the (10, 10) element of the matrix A (that is,
a10,10) can be referenced in a natural way as
A(10,10)
instead of as
A[9][9]

12.3 General Purpose Languages
571
as it would be in ordinary C. Many computer engineers prefer the latter
notation, however.
There are various C++ class libraries or templates for matrix and vector
computations. One is the Armadillo C++ Matrix Library, which provides a
syntax similar to that of Matlab.
The Template Numerical Toolkit
http://math.nist.gov/tnt/
and the Matrix Template Library
http://www.osl.iu.edu/research/mtl/
are templates based on the design approach of the C++ Standard Template
Library
http://www.sgi.com/tech/stl/
Use of a C++ class library for linear algebra computations may carry a
computational overhead that is unacceptable for large arrays. Both the Tem-
plate Numerical Toolkit and the Matrix Template Library are very eﬃcient
computationally, however (see Siek and Lumsdaine 2000). Eubank and Kupre-
sanin (2012) discuss many computational details relating to the usage of C++
in numerical linear algebra.
12.3.4 Python
Python, which is an interpretive system, is one of the most popular languages.
It is currently the most commonly-used language in academic courses to in-
troduce students to programming. There are several free Python interpreters
for a wide range of platforms. There is an active development community that
has produced a large number of open-source Python functions for scientiﬁc
applications. While all language systems, such as Fortran and C, undergo re-
visions from time to time, the rate and nature of the revisions to Python have
exceeded those of other systems. Some of these changes have been disruptive
because of lack of backward compatibility. Many changes are useful enhance-
ments, of course. In a recent update, a Cayley multiplication operator, “@”
(of all symbols!), was introduced.
For persons who program in other languages, the many diﬀerences in sym-
bols and in syntax are annoying and induce programming errors.
Python has a number of array constructs, some that act similarly to vec-
tors, some that are like sets, and so on. The native index for all of them begins
at 0.
One of the most useful freely-distributed Python libraries is numpy, which
supports a wide range of functionality including general numerical linear alge-
bra, statistical applications, optimization, solutions of diﬀerential equations,
and so on. The free Anaconda system will quickly install Python along with
the packages. (See page 555.)

572
12 Software for Numerical Linear Algebra
There is also a set of Python wrappers for the Armadillo C++ Matrix
Library.
12.4 Interactive Systems for Array Manipulation
Many of the computations for linear algebra are implemented as simple oper-
ators on vectors and matrices in some interactive systems. Some of the more
common interactive systems that provide for direct array manipulation are
Octave or Matlab, R, SAS IML, APL, Lisp-Stat, Gauss, Python, IDL, and
PV-Wave. There is no need to allocate space for the arrays in these systems
as there is for arrays in Fortran and C.
Occasionally we need to operate on vectors or matrices whose elements are
variables. Software for symbolic manipulation, such as Maple or Mathematica,
can perform vector/matrix operations on variables. See Exercise 12.11 on
page 585.
12.4.1 R
The software system called S was developed at Bell Laboratories, primarily
by John Chambers, in the mid-1970s. S is both a data analysis system and an
object-oriented programming language. In the mid-1990s, Robert Gentleman
and Ross Ihaka produced an open-source and freely available system called
R that provided generally the same functionality in the same language as
S. This system has become one of the most widely-used statistical software
packages. Its development and maintenance has been subsumed by a group of
statisticians and computer scientists called the R Project Team. The system,
along with various associated packages, is available at
http://www.r-project.org/
There are graphical interfaces for installation and maintenance of R that inter-
act well with the operating system, whether Unix/Linus, Microsoft Windows,
or MacOS.
In this section I discuss some of the salient properties of R. I describe
some features at a high-level, and I also mention some more obtuse properties.
The purpose is neither to be a tutorial nor to be a general reference for the
language. Venables and Ripley (2003) is one of several good books to use for
learning R. Wickham (2015) and Chambers (2016) provide many more details
and general insights to go much deeper in understanding and using R.
12.4.1.1 General Properties
The basic numeric object in R is a vector. The most important R entity is
the function. In R, all actions are “functions”, and R has an extensive set of

12.4 Interactive Systems for Array Manipulation
573
functions. Many functions are provided through packages that, although not
part of the core R, can be easily installed.
R statements are line-oriented but continue from one line to another until
a syntactically complete statement is terminated by an end-of-line.
Assignment is made by “<-” or by “=”.
A comment statement in R begins with a pound sign, “#”.
R has a number of built-in object classes, and the user can deﬁne additional
classes that determine how generic functions operate on the objects.
R has functions for printing, but if a statement consists of just the name
of an object, the object is printed to the standard output device (which is
likely to be the monitor).
One of the most important and useful properties of R is the ways it can
handle missing data or values that do not exist as ordinary numbers or charac-
ters, such as ∞or 0/0. (See Sect. 9.5.6, beginning on page 437, and pages 464
and 475.) Often when data contain missing values, we wish to ignore those
values and process all of the valid data. Many functions in R provide a stan-
dard way of requesting that this be done, by means of the logical argument
na.rm. If na.rm is true, then the function ignores the fact that some data may
be missing, and performs its operations only on the valid data.
12.4.1.2 Documentation
There are a number of books and tutorials on R, and a vast array of sources
of information on the internet. R also has an integrated help system. Docu-
mentation for R functions is generally stored in the form of man pages and
can be accessed through the R help function or by the R “?” operator.
12.4.1.3 Basic Operations with Vectors and Matrices and for
Subarrays
A list is constructed by the c function. A list of scalar numeric values can be
treated as a vector without modiﬁcation. A matrix is constructed from a list
of numeric values by the matrix function. A matrix can also be constructed
by binding vectors as the columns of the matrix (the cbind function) or by
binding vectors as the rows of the matrix (the rbind function).
Indexing of arrays starts at 1, and storage is column-major. Indexes are
indicated by “[ ]”; for example, x[1] refers to the ﬁrst element of the one-
dimensional array x. Any set of valid indexes can be speciﬁed; for exam-
ple, x[c(3,1,3)] refers to the third, the ﬁrst, and again the third elements
of the one-dimensional array x. Negative values can be used to indicate re-
moval of speciﬁed elements; for example, x[c(-1,-3)] refers to the same
one-dimensional array x with the ﬁrst and third elements removed. The order
of negative indexes or the repetition of negative indexes has no eﬀect; for ex-
ample, x[c(-3,-1,-3)] is the same as x[c(-1,-3)]. Positive and negative
values cannot be mixed as indexes.

574
12 Software for Numerical Linear Algebra
Cayley multiplication is indicated by the symbol “%*%”. Most operators
with array operands are applied elementwise; for example, the symbol “*”
indicates the Hadamard product of two matrices. The expression
A %*% B
indicates the Cayley product of the matrices, where the number of columns
of A must be the same as the number of rows of B. The expression
A * B
indicates the Hadamard product of the matrices, where the number of rows
and columns of A must be the same as the number of rows and columns of B.
The transpose of a vector or matrix is obtained by using the function “t”:
t(A)
Although R stores matrices by columns and converts lists into matrices in
column order by default, a matrix can be constructed by rows, and doing so
often results in more readable code:
A <- matrix(c( 1, 4, 7, 10,
2, 5, 8, 11,
3, 6, 9, 12),
nrow=3, byrow=T)
To the extent that R distinguishes between row vectors and column vec-
tors, a vector is considered to be a column vector. In many cases, however, it
does not distinguish. For example, if
x <- c(1,2)
y <- c(1,2)
the expression x %*% y is the dot product; that is,
x %*% y
is the same as
t(x) %*% y
The transpose operator is not required in this case.
The outer product, however, requires either explicit transposition or use
of a special binary operator. The outer product is formed by x %*% t(y) or
by using the special outer product operator %o%; thus,
x %o% y
is the same as
x %*% t(y)

12.4 Interactive Systems for Array Manipulation
575
There is also a useful function, outer, that allows more general combinations
of the elements of two vectors. For example, if func is a scalar function of two
scalar variables, outer(x,y,FUN=func) forms a matrix with the rows corre-
sponding to x and the columns corresponding to y, and whose (ij)th element
corresponds to func(x[i],y[j]). Strings can be used as the argument FUN;
thus,
outer(x,y,FUN="*")
is also the same as
x %o% y
In the expressions
y <- a %*% x
and
y <- x[c(1,2,3)] %*% a
the vector is interpreted as a row or column as appropriate for the multipli-
cation to be deﬁned.
Like many other software systems for array manipulation, R usually does
not distinguish between scalars and arrays of size 1. For example, if
x <- c(1,2)
y <- c(1,2)
z <- c(1,2,3)
the expression x %*% y %*% z yields the same value as 5*z because the ex-
pression x %*% y %*% z is interpreted as (x %*% y) %*% z and (x %*% y)
is a scalar. The expression x %*% (y %*% z) is invalid because y and z are
not conformable for multiplication. In the case of one-dimensional and two-
dimensional arrays, R sometimes allows the user to treat them in a general
fashion, and sometimes makes a hard distinction. For example, the func-
tion length returns the number of elements in an array, whether it is one-
dimensional or two-dimensional. The function dim, however, returns the num-
bers of rows and columns in a two-dimensional array, but returns NULL for a
one-dimensional array. See page 576 for some additional comments about one-
dimensional and two-dimensional arrays in R.
Examples of subarray references in R are shown in Table 12.4. Compare
these with the Fortran references shown in Table 12.3. In R, a missing index
indicates that the entire corresponding dimension is to be used. Groups of
indices can be formed by the c function or the seq function, which is similar
to the i:j:k notation of Fortran.
The subarrays can be used directly in expressions. For example, the ex-
pression
A[c(1,2),c(2,3)] %*% B[c(3,4),]

576
12 Software for Numerical Linear Algebra
Table 12.4. Subarrays in R
A[c(2,3),c(1,3)] The 2 × 3 submatrix in rows 2 and 3
and columns 1 to 3 of A
A[,seq(1,4,2)]
The submatrix with all 3 rows
and the 1st and 3rd columns of A
A[,4]
The column vector that is the 4th column of A
yields the product
!4 7
5 8
" !3 7
4 8
"
as on page 570.
The basic atomic numeric type in R is a vector, and a 1 × n matrix or
an n × 1 matrix may be cast as vectors (that is, one-dimensional arrays).
This is often exactly what we would want. There are, however, cases where
this casting is disastrous, for example if we use the dim function, as shown in
Fig. 12.4.
> A <−matrix(c(1,2,3,4,5,6), nrow=3)
> dim(A)
[1] 3 2
> b <−A[,2]
> dim(b)
NULL
> length(b)
[1] 3
Figure 12.4. Downcasting in R
To prevent this casting, we can use the drop keyword in the subsetting
operator, as in Fig. 12.5.
> C <−A[,2, drop=FALSE]
> dim(C)
[1] 3 1
Figure 12.5. Preserving the class in R

12.4 Interactive Systems for Array Manipulation
577
12.4.1.4 R Functions for Numerical Linear Algebra
R has functions for many of the basic operations on vectors and matrices.
Some of the R functions are shown in Table 12.5.
Table 12.5. Some R functions for vector/matrix computations
norm
Matrix norm.
The L1, L2, L∞, and Frobenius norms are available.
t
Transpose of a matrix or preparation of a vector for multiplication.
diag
Principal diagonal of a matrix.
tr {psych}
Trace of a square matrix.
vecnorm
Vector Lp norm.
det
Determinant.
rcond.Matrix Matrix condition number.
lu
LU decomposition.
qr
QR decomposition.
chol
Cholesky factorization.
svd
Singular value decomposition.
eigen
Eigenvalues and eigenvectors.
as.vector
vec(·).
solve.Matrix Solve system of linear equations,
or compute matrix inverse or pseudoinverse.
lsfit
Ordinary or weighted least squares ﬁt.
l1fit
Least absolute values ﬁt.
nnls.fit
Nonnegative least squares ﬁt.
12.4.1.5 Other Mathematical Objects and Operations
In addition to numerical computations, there are other operations we may
wish to perform. R provides facilities for many such operations. One of the
most common non-computational operations is sorting. R does sorting very
eﬃciently. A useful R function is is.unsorted, which checks if an object is
sorted without sorting it.
R does not have an object class of “set” (although we could create one).
R, however, does provide functions for the standard set operations such as
union and intersection, and the logical operators of set equality and inclusion,
as illustrated in Fig. 12.6.
The R function unique is useful for removing duplicate elements in an R
object.
12.4.1.6 Extensibility
On of the most important features of R is its extensibility. Operations in R are
performed by a function by invoking the function by its name. New functions

578
12 Software for Numerical Linear Algebra
> S1 <−c(1,2,3,2)
> S2 <−c(3,2,1)
> S3 <−c(4,3,2,1)
> setequal(S1,S2)
[1] TRUE
> union(S1,S3)
[1] 1 2 3 4
> union(S1,S1)
[1] 1 2 3
> intersect(S1,S3)
[1] 1 2 3
> intersect(S1,S1)
[1] 1 2 3
> 1 %in% S1
[1] TRUE
> 5 %in% S1
[1] FALSE
Figure 12.6. Set functions in R: setequal, union, intersect, %in%
can be added to an R session with ease. (Multiple functions may have the same
name, but only one is active at a time; thus, the function name is unique.)
A simple way a user can extend R is to write a function, using the R
function constructor called function:
myFun <- function( ...){
...
}
In a simple case, the computations of the function can be expressed in
ordinary R statements. In other cases, the computations can be expressed in
a compiler language such as Fortran or C, compiled, and then linked into R.
In either case, the user’s function is invoked by its name, just as if it were
part of the original R system.
12.4.1.7 Packages
A group of functions written either in R or in a compiler language can be
packaged together in a “library” or “package”, and then all of the functions,
together with associated documentation, can be loaded into R just by loading
the package.
Previously developed packages can be installed with the local system, and
then can be loaded in an R session. These packages can be stored in the
Comprehensive R Archive Network (CRAN) and an R system program can
install them on a local system.
The R system generally consists of a set of packages, a “base” package, a
“stats” package, and so on. When the R program is invoked, a standard set

12.4 Interactive Systems for Array Manipulation
579
of packages are loaded. Beyond those, the user can load any number of other
packages that have been installed on the local system.
Within an R session, the available functions include all of those in the
loaded packages. Sometimes, there may be name conﬂicts (two functions with
the same name). The conﬂicts are resolved in the order of the loading of the
packages; last-in gets preference. Reference to a function name often includes
the package that includes the function; for example, “tr {psych}” refers to
the tr function in the psych package. (This notation is not used in the R
statement itself; the R statement invoking tr brings in the currently active
version of tr.)
Documentation for a new package can be developed (using the R package
roxygen or by other methods) and when the new package is loaded, the doc-
umentation becomes available to the user. This package, which is available on
CRAN, can be described as a documentation system. It is also available at
http://roxygen.org/
There are very many R packages available on CRAN. Some packages ad-
dress a fairly narrow area of application, but some provide a wide range of
useful functions. Two notable ones of the latter type are MASS, developed pri-
marily by Bill Venables and Brian Ripley, and Hmisc, developed primarily by
Frank Harrell.
A problem with many of the packages at CRAN is that some of them
were not written, developed, or tested by people with knowledge and skill in
software development. Some of these packages are not of high quality. Another
problem with packages on CRAN is that they may change from time to time,
so that computed results may not be easily replicated.
12.4.1.8 Sharable Libraries and Rcpp
The functionality of R can also be extended by linking functions written in
general-purpose compiled languages into R. The general way this is done is
by building a sharable library (or “dynamic” library) in the language and
then loading it into R using the R function dyn.load. (The sharable library
can be built within the R script by use of R CMD.) For Fortran and C there
are R functions, .Fortran and .C, to access any program unit in the shared
library after it is loaded. The arguments of these functions are the program
unit name followed by the names of the program unit’s arguments set to the
appropriately-cast R variables. For example, if a Fortran subroutine is named
myFun and it has a double precision argument x and an integer argument n,
and it has been compiled and linked into a sharable library called myLib.so,
it could be invoked in R by the statements
dyn.load("myLib.so")
...
initialize relevant R variables, say xr and nr
ans <- .Fortran("myFun", x=as.double(xr), n=as.integer(nr))

580
12 Software for Numerical Linear Algebra
There is an R package, Rcpp, that facilitates interoperability between C++
and R. This package allows the R user to write C++ code directly in the R
script, so it is much simpler than the sharable library approach described
above. This package can be used to produce other R packages written in
C++ or other compiler languages. See Eddelbuettel (2013), and also Cham-
bers (2016) and Wickham (2015) for discussion and examples of the use of
Rcpp. There is also an R package, RcppArmadillo, that provides access to the
functions in the Armadillo C++ Matrix Library.
Eubank and Kupresanin (2012) discuss use of C++ functions in R and
various programming issues for both C++ and R.
There is a Python package, RPy, that allows access of the R system from
Python.
12.4.1.9 Other Versions of R and Other Distributions
In addition to the standard distribution versions of R available from CRAN,
there are various add-ons and enhancements, some also available from CRAN.
One of the most notable of these is RStudio, which is an integrated R devel-
opment system including a sophisticated program editor.
Spotﬁre S+
R
⃝is an enhancement of S, originally developed as S-PLUS
R
⃝
by StatSci, Inc. It is now distributed and maintained by TIBCO Software
Inc. The enhancements include graphical interfaces with menus for common
analyses, more statistical analysis functionality, and support.
Microsoft R Open (MRO), formerly known as Revolution R Open (RRO),
is an enhanced distribution of R from Microsoft Corporation. An important
feature of Microsoft R Open is the use of the Intel Math Kernel Library (MKL)
to allow multiple threads if the CPU follows the Intel multi-core architecture.
This can result in signiﬁcant speedup of standard computations for linear
algebra.
Another useful feature of Microsoft R Open is the Reproducible R Toolkit,
which can ensure that the same version of all R packages are used consistently.
This is done by means of a CRAN repository snapshot, so that even if some
packages are changed, the same computations of an R program on a given
system at a given time can be performed again on a diﬀerent system at a
diﬀerent time. A related component of the Reproducible R Toolkit is the
checkpoint package, which allows the user to go back and forth to retrieve
the exact versions of R packages. The user can specify the version of a package
directly in the R code.
12.4.2 MATLAB and Octave
MATLAB
R
⃝, or Matlab
R
⃝, is a proprietary software package distributed by The
Mathworks, Inc. It is built on an interactive, interpretive expression language,
and is one of the most widely-used computer systems supporting numerical
linear algebra and other scientiﬁc computations

12.4 Interactive Systems for Array Manipulation
581
Octave is a freely-available package that provides essentially the same core
functionality in the same language as Matlab. The graphical interfaces for
Octave are more primitive than those for Matlab and do not interact as seam-
lessly with the operating system. Most of the discussion in this section applies
to Octave as well as to Matlab.
The basic object in Matlab and Octave is a rectangular array of numbers
(possibly complex). Scalars (even indices) are 1 × 1 matrices; equivalently, a
1 × 1 matrix can be treated as a scalar. A vector is a matrix with one column.
Statements in Matlab are line-oriented. A statement is assumed to end at
the end of the line, unless the last three characters on the line are periods
(. . . ) or a matrix is being initialized. If an assignment statement in Matlab
is not terminated with a semicolon, the matrix on the left-hand side of the
assignment is printed. If a statement consists only of the name of a matrix,
the object is printed to the standard output device (which is likely to be the
monitor).
The indexing of arrays in Matlab starts with 1, and indexes are indicated
by “( )”.
A matrix is initialized in Matlab by listing the elements row-wise within
brackets and with a semicolon marking the end of a row. (Matlab also has
a reshape function similar to that of Fortran that treats the matrix in a
column-major fashion.)
Matlab has a number of functions to operate on vectors and matrices
to perform such operations as various matrix factorizations, computations of
eigenvalues and eigenvectors, solution of linear systems, and so on. Many op-
erations are performed by operators of the language. In general, the operators
in the Matlab language refer to the common vector/matrix operations. For
example, the usual multiplication symbol, “*”, means Cayley multiplication
when the operands are matrices. The meaning of an operator can often be
changed to become the corresponding element-by-element operation by pre-
ceding the operator with a period; for example, the symbol “.*” indicates the
Hadamard product of two matrices. The expression
A * B
indicates the Cayley product of the matrices, where the number of columns
of A must be the same as the number of rows of B; and the expression
A .* B
indicates the Hadamard product of the matrices, where the number of rows
and columns of A must be the same as the number of rows and columns of B.
The transpose of a vector or matrix is obtained by using a postﬁx operator
“′”, which is the same ASCII character as the apostrophe:
A’
Matlab has special operators “\” and “/” for solving linear systems or for
multiplying one matrix by the inverse of another. The operator “/” means

582
12 Software for Numerical Linear Algebra
“divide”, as in A/B = AB−1, while “\” means “divide from the left”, as in
A\B = A−1B.
While the statement
A\B
refers to a quantity that has the same value as the quantity indicated by
inv(A)*B
the computations performed are diﬀerent (and, hence, the values produced
may be diﬀerent). The second expression is evaluated by performing the two
operations indicated: A is inverted, and the inverse is then used as the left
factor in matrix or matrix/vector multiplication. The ﬁrst expression, A\B,
indicates that the appropriate computations to evaluate x in Ax = b should
be performed to evaluate the expression. (Here, x and b may be matrices
or vectors.) Another diﬀerence between the two expressions is that inv(A)
requires A to be square and algorithmically nonsingular, whereas A\B produces
a value that simulates A−b.
Matlab distinguishes between row vectors and column vectors. A row vec-
tor is a matrix whose ﬁrst dimension is 1, and a column vector is a matrix
whose second dimension is 1. In either case, an element of the vector is refer-
enced by a single index.
Subarrays in Matlab are deﬁned in much the same way as in modern
Fortran. The subarrays can be used directly in expressions. For example, the
expression
A(1:2,2:3) * B(3:4,:)
yields the product
!
4 7
5 8
" !
3 7
4 8
"
as on page 570. There is one major diﬀerence in how Matlab and Fortran
denote sequences with a stride greater than 1, however. The upper limit and
the stride are reversed in the triplet. For example, if x contains the elements
1,2,3,4,5, in Matlab, x(1:2:5) is the list 1,3,5. The same subarray in Fortran
is obtained by x(1:5:2).
There are a number of books on Matlab, including, for example, Attaway
(2016). Many books on numerical linear algebra use the Matlab language
in the expositions. For example, the widely-used book by Coleman and Van
Loan (1988), while not speciﬁcally on Matlab, shows how to perform matrix
computations in Matlab.
Exercises
12.1. Write a function in R to ﬁnd the square root of a symmetric nonnega-
tive deﬁnite matrix (see page 160). Make sure that your function will

Exercises
583
determine the square root of a nonnegative 1 × 1 matrix. Also, write
your function so that if the matrix is not square or is not nonnegative
deﬁnite, it prints an appropriate message and returns an appropriate
value.
12.2. Write a recursive function in Fortran, C, or C++ to multiply two
square matrices using the Strassen algorithm (page 531). Write the
function so that it uses an ordinary multiplication method if the size
of the matrices is below a threshold that is supplied by the user.
12.3. Set up an account on GitHub, and upload the function you wrote in
Exercise 12.2 to your account. Share this function with another person.
(This would probably be the instructor if you are using this book as a
textbook in a course.)
12.4. There are various ways to evaluate the eﬃciency of a program: count-
ing operations, checking the “wall time”, using a shell level timer, and
using a call within the program. In C, the timing routine is ctime, and
in modern Fortran it is the subroutine system clock. FORTRAN 77
does not have a built-in timing routine, but the IMSL Fortran Library
provides one. For this exercise, you are to write six short C programs
and six short Fortran programs. The programs in all cases are to
initialize an n × m matrix so that the entries are equal to the column
numbers; that is, all elements in the ﬁrst column are 1s, all in the
second column are 2s, etc. The six programs arise from three matrices
of diﬀerent sizes 10,000 × 10,000, 100 × 1,000,000, and 1,000,000 × 100;
and from two diﬀerent ways of nesting the loops: for each size matrix,
ﬁrst nest the row loop within the column loop and then reverse the
loops. The number of operations is the same for all programs. For each
program, use both a shell level timer (e.g., in Unix or Linux, use time)
and a timer called from within your program. Make a table of the times:
10000 × 10000 100 × 1000000 1000000 × 100
Fortran column-in-row
–
–
–
row-in-column
–
–
–
C
column-in-row
–
–
–
row-in-column
–
–
–
12.5. Obtain the BLAS routines rotg and rot for constructing and applying
a Givens rotation. These routines exist in both Fortran and C; they are
available in the IMSL Libraries or from CALGO (Collected Algorithms
of the ACM; see the Bibliography).
a) Using these two routines, apply a Givens rotation to the matrix
used in Exercise 5.9 in Chap. 5,
A =
⎡
⎢⎢⎣
3 5 6
6 1 2
8 6 7
2 3 1
⎤
⎥⎥⎦,
so that the second column becomes (5, ˜a22, 6, 0).

584
12 Software for Numerical Linear Algebra
b) Write a routine in Fortran or C that accepts as input a matrix
and its dimensions and uses the BLAS routines rotg and rot to
produce its QR decomposition. There are several design issues you
should address: how the output is returned (for purposes of this
exercise, just return two arrays or pointers to the arrays in full
storage mode), how to handle nonfull rank matrices (for this ex-
ercise, assume that the matrix is of full rank, so return an error
message in this case), how to handle other input errors (what do
you do if the user inputs a negative number for a dimension?), and
others.
12.6. Using the BLAS routines rotg and rot for constructing and applying
a Givens rotation and the program you wrote in Exercise 12.5, write
a Fortran or C routine that accepts a simple symmetric matrix and
computes its eigenvalues using the mobile Jacobi scheme. The outer
loop of your routine consists of the steps shown on page 318, and the
multiple actions of each of those steps can be implemented in a loop
in serial mode. The importance of this algorithm, however, is realized
when the actions in the individual steps on page 318 are performed in
parallel.
12.7. Sparse matrices.
a) Represent the order 10 identity matrix I100 using the index-index-
value notation. (You may want to use R or the IMSL libraries to
ensure the correctness of the representation.)
b) Represent the 10 × 10 type 2 tridiagonal matrix of equation (8.92)
on page 385 using the index-index-value notation.
12.8. a) Show that the Ericksen matrix (expression (12.7)) has the inverse
shown (expression (12.8)).
b) Show that the Ericksen matrix has determinant xn
1.
c) For m = −1 and k = 2, form a 10 × 10 Ericksen matrix using the
suggested values for the xs. (Recall the condition on the binomial
coeﬃcients.)
Compute the inverse, the determinant, and the L2 condition num-
ber.
12.9. Compute the two largest eigenvalues of the 21 × 21 Wilkinson matrix
to 15 digits.
12.10. Develop a class library in C++ for matrix and vector operations. Dis-
cuss carefully the issues you consider in designing the class construc-
tors. Design them in such a way that the references
xx(1)
YY(1,1)
refer to the implied mathematical entities. Design the operators “+”
and “*” so that the references
aa + bb
aa * bb

Exercises
585
will determine whether a and b are matrices and/or vectors con-
formable for the implied mathematical operations and, if so, will pro-
duce the object corresponding to the implied mathematical entity rep-
resented by the expression.
See Eubank and Kupresanin (2012), Appendix D, for a full-blown so-
lution to this exercise.
12.11. Use a symbolic manipulation software package such as Maple to deter-
mine the inverse of the matrix:
⎡
⎣
a b c
d e f
g h i
⎤
⎦.
Determine conditions for which the matrix would be singular. (You
can use the solve() function in Maple on certain expressions in the
symbolic solution you obtained.)
12.12. Consider the 3 × 3 symmetric Toeplitz matrix with elements a, b, and
c; that is, the matrix that looks like this:
⎡
⎣
a b c
b a b
c b a
⎤
⎦.
See Exercise 8.12 on page 398.
a) Use a symbolic manipulation software package such as Maple to
determine the inverse of this matrix.
See page 385.
b) Determine conditions for which the matrix would be singular.
12.13. a) Incorporate the function you developed in Exercise 12.2 into R.
If you used Fortran or C, ﬁrst build a sharable library. (You gen-
erally do this with a compiler directive.) Then use dyn.load and
.Fortran or .C. If you used C++ in Exercise 12.2, use Rcpp.
b) Now write an R function to multiple two matrices using the
Strassen algorithm as in Exercise 12.2.
c) Now compare the eﬃciency of your R function with the Fortran,
C, or C++ function invoked from R and with the standard R
multiplication operator, %*%. (The proc.time function in the R
base package is probably the easiest way to do the timing.)

Appendices

A
Notation and Deﬁnitions
All notation used in this work is “standard”, and I have endeavored to use
notation consistently. I have opted for simple notation, which, of course, results
in a one-to-many map of notation to object classes. Within a given context,
however, the overloaded notation is generally unambiguous.
This appendix is not intended to be a comprehensive listing of deﬁnitions.
The Index is a more reliable set of pointers to deﬁnitions, except for symbols
that are not words.
A.1 General Notation
Uppercase italic Latin and Greek letters, such as A, B, E, Λ, etc., are generally
used to represent either matrices or random variables. Random variables are
usually denoted by letters nearer the end of the Latin alphabet, such X, Y , and
Z, and by the Greek letter E. Parameters in models (that is, unobservables
in the models), whether or not they are considered to be random variables,
are generally represented by lowercase Greek letters. Uppercase Latin and
Greek letters are also used to represent cumulative distribution functions.
Also, uppercase Latin letters are used to denote sets.
Lowercase Latin and Greek letters are used to represent ordinary scalar or
vector variables and functions. No distinction in the notation is made
between scalars and vectors; thus, β may represent a vector and βi may
represent the ith element of the vector β. In another context, however, β may
represent a scalar. All vectors are considered to be column vectors, although
we may write a vector as x = (x1, x2, . . . , xn). Transposition of a vector or a
matrix is denoted by the superscript “T”.
Uppercase calligraphic Latin letters, such as D, V, and W, are generally
used to represent either vector spaces or transforms (functionals).
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5
589

590
Appendix A: Notation and Deﬁnitions
Subscripts generally represent indexes to a larger structure; for example,
xij may represent the (i, j)th element of a matrix, X. A subscript in paren-
theses represents an order statistic. A superscript in parentheses represents
an iteration; for example, x(k)
i
may represent the value of xi at the kth step
of an iterative process.
xi
The ith element of a structure (including a sample, which is a multiset).
x(i) The ith order statistic.
x(i) The value of x at the ith iteration.
Realizations of random variables and placeholders in functions associated
with random variables are usually represented by lowercase letters correspond-
ing to the uppercase letters; thus, ϵ may represent a realization of the random
variable E.
A single symbol in an italic font is used to represent a single variable. A
Roman font or a special font is often used to represent a standard operator
or a standard mathematical structure. Sometimes a string of symbols in a
Roman font is used to represent an operator (or a standard function); for
example, exp(·) represents the exponential function. But a string of symbols
in an italic font on the same baseline should be interpreted as representing
a composition (probably by multiplication) of separate objects; for example,
exp represents the product of e, x, and p. Likewise a string of symbols in
a Roman font (usually a single symbol) is used to represent a fundamental
constant; for example, e represents the base of the natural logarithm, while e
represents a variable.
A ﬁxed-width font is used to represent computer input or output, for
example,
a = bx + sin(c).
In computer text, a string of letters or numerals with no intervening spaces
or other characters, such as bx above, represents a single object, and there is
no distinction in the font to indicate the type of object.
Some important mathematical structures and other objects are:
IR
The ﬁeld of reals or the set over which that ﬁeld is deﬁned.
IR+
The set of positive reals.
¯IR+
The nonnegative reals; ¯IR+ = IR+ ∪{0}.

A.2 Computer Number Systems
591
IRd
The usual d-dimensional vector space over the reals or the set of
all d-tuples with elements in IR.
IRn×m
The vector space of real n × m matrices.
ZZ
The ring of integers or the set over which that ring is deﬁned.
GL(n)
The general linear group; that is, the group of n × n full rank
(real) matrices with Cayley multiplication.
O(n)
The orthogonal group; that is, the group of n × n orthogonal
(orthonormal) matrices with Cayley multiplication.
e
The base of the natural logarithm. This is a constant; e may be
used to represent a variable. (Note the diﬀerence in the font.)
i
The imaginary unit, √−1. This is a constant; i may be used to
represent a variable. (Note the diﬀerence in the font.)
A.2 Computer Number Systems
Computer number systems are used to simulate the more commonly used
number systems. It is important to realize that they have diﬀerent properties,
however. Some notation for computer number systems follows.
IF
The set of ﬂoating-point numbers with a given precision,
on a given computer system, or this set together with
the four operators +, -, *, and /. (IF is similar to IR
in some useful ways; see Sect. 10.1.2 and Table 10.4 on
page 492.)
II
The set of ﬁxed-point numbers with a given length, on
a given computer system, or this set together with the
four operators +, -, *, and /. (II is similar to ZZ in some
useful ways; see Sect. 10.1.1.)
emin and emax
The minimum and maximum values of the exponent in
the set of ﬂoating-point numbers with a given length
(see page 470).
ϵmin and ϵmax
The minimum and maximum spacings around 1 in the
set of ﬂoating-point numbers with a given length (see
page 472).

592
Appendix A: Notation and Deﬁnitions
ϵ or ϵmach
The machine epsilon, the same as ϵmin (see page 472).
[·]c
The computer version of the object · (see page 484).
NA
Not available; a missing-value indicator.
NaN
Not-a-number (see page 475).
A.3 General Mathematical Functions and Operators
Functions such as sin, max, span, and so on that are commonly associated
with groups of Latin letters are generally represented by those letters in a
Roman font.
Operators such as d (the diﬀerential operator) that are commonly associ-
ated with a Latin letter are generally represented by that letter in a Roman
font.
Note that some symbols, such as | · |, are overloaded; such symbols are
generally listed together below.
×
Cartesian or cross product of sets, or multiplication of
elements of a ﬁeld or ring.
|x|
The modulus of the real or complex number x; if x is
real, |x| is the absolute value of x.
⌈x⌉
The ceiling function evaluated at the real number x: ⌈x⌉
is the smallest integer greater than or equal to x.
For any x, ⌊x⌋≤x ≤⌈x⌉.
⌊x⌋
The ﬂoor function evaluated at the real number x: ⌊x⌋
is the largest integer less than or equal to x.
x!
The factorial of x. If x is a positive integer, x! = x(x −
1) · · · 2 · 1. For all other values except negative integers,
x! is deﬁned by x! = Γ(x + 1).

A.3 General Mathematical Functions and Operators
593
O(f(n))
The order class big O with respect to f(n).
g(n) ∈O(f(n))
means there exists some ﬁxed c such that ∥g(n)∥≤c∥f(n)∥∀n.
In particular, g(n) ∈O(1) means g(n) is bounded.
In one special case, we will use O(f(n)) to represent some
unspeciﬁed scalar or vector x ∈O(f(n)). This is the case of a
convergent series. An example is
s = f1(n) + · · · + fk(n) + O(f(n)),
where f1(n), . . . , fk(n) are ﬁnite constants.
We may also express the order class deﬁned by convergence
as x →a as O(f(x))x→a (where a may be inﬁnite). Hence,
g ∈O(f(x))x→a iﬀ
lim sup
x→a
∥g(n)∥/∥f(n)∥< ∞.
o(f(n))
Little o; g(n) ∈o(f(n)) means for all c > 0 there exists some
ﬁxed N such that 0 ≤g(n) < cf(n) ∀n ≥N. (The functions
f and g and the constant c could all also be negative, with
a reversal of the inequalities.) Hence, g(n) ∈o(f(n)) means
∥g(n)∥/∥f(n)∥→0 as n →∞.
In particular, g(n) ∈o(1) means g(n) →0.
We also use o(f(n)) to represent some unspeciﬁed scalar or
vector x ∈o(f(n)) in special case of a convergent series, as
above:
s = f1(n) + · · · + fk(n) + o(f(n)).
We may also express this kind of convergence in the form g ∈
o(f(x))x→a as x →a (where a may be inﬁnite).
OP (f(n))
Bounded convergence in probability; X(n) ∈OP (f(n)) means
that for any positive ϵ, there is a constant Cϵ such that
supn Pr(∥X(n)∥≥Cϵ∥f(n)∥) < ϵ.
oP (f(n))
Convergent in probability; X(n) ∈oP (f(n)) means that for
any positive ϵ, Pr(∥X(n) −f(n)∥> ϵ) →0 as n →∞.
d
The diﬀerential operator.
Δ or δ
A perturbation operator; Δx (or δx) represents a perturbation
of x and not a multiplication of x by Δ (or δ), even if x is a
type of object for which a multiplication is deﬁned.

594
Appendix A: Notation and Deﬁnitions
Δ(·, ·)
A real-valued diﬀerence function; Δ(x, y) is a measure of the
diﬀerence of x and y. For simple objects, Δ(x, y) = |x−y|. For
more complicated objects, a subtraction operator may not be
deﬁned, and Δ is a generalized diﬀerence.
˜x
A perturbation of the object x;
Δ(x, ˜x) = Δx.
˜x
An average of a sample of objects generically denoted by x.
¯x
The mean of a sample of objects generically denoted by x.
¯x
The complex conjugate of the complex number x; that is, if
x = r + ic, then ¯x = r −ic.
sign(x)
For the vector x, a vector of units corresponding to the signs:
sign(x)i =
1
if xi > 0,
=
0
if xi = 0,
= −1
if xi < 0,
with a similar meaning for a scalar.
A.3.1 Special Functions
A good general reference on special functions in mathematics is the NIST
Handbook of Mathematical Functions, edited by Olver et al. (2010). An-
other good reference on special functions is the venerable book edited by
Abramowitz and Stegun (1964), which has been kept in print by Dover Pub-
lications.
log x
The natural logarithm evaluated at x.
sin x
The sine evaluated at x (in radians) and similarly for other trigono-
metric functions.

A.4 Linear Spaces and Matrices
595
Γ(x)
The complete gamma function: Γ(x) =
+ ∞
0
tx−1e−tdt. (This is called
Euler’s integral.) Integration by parts immediately gives the repli-
cation formula Γ(x + 1) = xΓ(x), and so if x is a positive inte-
ger, Γ(x + 1) = x!, and more generally, Γ(x + 1) deﬁnes x! for all
x except negative integers. Direct evaluation of the integral yields
Γ(1/2) = √π. Using this and the replication formula, with some ma-
nipulation we get for the positive integer j
Γ(j + 1/2) = 1 · 2 · · · (2j −1)
2j
√π.
The integral does not exist, and thus, the gamma function is not de-
ﬁned at the nonpositive integers.
The notation Γd(x) denotes the multivariate gamma function
(page 221), although in other literature this notation denotes the
incomplete univariate gamma function,
+ d
0 tx−1e−tdt.
A.4 Linear Spaces and Matrices
V(G)
For the set of vectors (all of the same order) G, the vector
space generated by that set.
V(X)
For the matrix X, the vector space generated by the columns
of X.
dim(V)
The dimension of the vector space V; that is, the maximum
number of linearly independent vectors in the vector space.
span(Y )
For Y either a set of vectors or a matrix, the vector space V(Y ).
⊥
Orthogonality relationship (vectors, see page 33; vector spaces,
see page 34).
V⊥
The orthogonal complement of the vector space V
(see
page 34).
N(A)
The null space of the matrix A; that is, the set of vectors
generated by all solutions, z, of the homogeneous system Az =
0; N(A) is the orthogonal complement of V(AT).
tr(A)
The trace of the square matrix A, that is, the sum of the di-
agonal elements.
rank(A)
The rank of the matrix A, that is, the maximum number of
independent rows (or columns) of A.

596
Appendix A: Notation and Deﬁnitions
σ(A)
The spectrum of the matrix A (the set of (unique) eigenvalues).
ρ(A)
The spectral radius of the matrix A (the maximum absolute
value of its eigenvalues).
A > 0
A ≥0
If A is a matrix, this notation means, respectively, that each
element of A is positive or nonnegative.
A ≻0
A ⪰0
This notation means that A is a symmetric matrix and that it
is, respectively, positive deﬁnite or nonnegative deﬁnite.
AT
For the matrix A, its transpose (also used for a vector to rep-
resent the corresponding row vector).
AH
The conjugate transpose, also called the adjoint, of the matrix
A;
AH = ¯AT = AT.
A−1
The inverse of the square, nonsingular matrix A.
A−R
The right inverse of the n×m matrix A, of rank n; AA−R = In.
The right inverse is m × n and of full column rank.
A−L
The left inverse of the n×m matrix A, of rank m; A−LA = Im.
The right inverse is m × n and of full row rank.
A−T
The inverse of the transpose of the square, nonsingular matrix
A.
A+
The g4 inverse, the Moore-Penrose inverse, or the pseudoin-
verse of the matrix A (see page 128).
A−
A g1, or generalized, inverse of the matrix A (see page 128).
A
1
2
The square root of a nonnegative deﬁnite or positive deﬁnite
matrix A; (A
1
2 )2 = A.
A−1
2
The square root of the inverse of a positive deﬁnite matrix A;
(A−1
2 )2 = A−1.

A.4 Linear Spaces and Matrices
597
⊙
Hadamard multiplication (see page 94).
⊗
Kronecker multiplication (see page 95).
⊕
The direct sum of two matrices; A ⊕B = diag(A, B) (see
page 63).
⊕
Direct sum of vector spaces (see page 18).
A.4.1 Norms and Inner Products
Lp
For real p ≥1, a norm formed by accumulating the pth powers
of the moduli of individual elements in an object and then
taking the (1/p)th power of the result (see page 27).
∥· ∥
In general, the norm of the object ·.
∥· ∥p
In general, the Lp norm of the object ·.
∥x∥p
For the vector x, the Lp norm
∥x∥p =

|xi|p 1
p
(see page 27).
∥X∥p
For the matrix X, the Lp norm
∥X∥p = max
∥v∥p=1 ∥Xv∥p
(see page 165).
∥X∥F
For the matrix X, the Frobenius norm
∥X∥F =
%
i,j
x2
ij
(see page 167).

598
Appendix A: Notation and Deﬁnitions
∥X∥Fp
For the matrix X, the Frobenius p norm
∥X∥Fp =
⎛
⎝
i,j
|xij|p
⎞
⎠
1/p
.
∥X∥Sp
For the n × m matrix X, the Schatten p norm
∥X∥Sp =
⎛
⎝
min(n,m)

i=1
dp
i
⎞
⎠
1/p
,
where the di are the singular values of X.
⟨x, y⟩
The inner product or dot product of x and y (see page 23; and
see page 97 for matrices).
κp(A)
The Lp condition number of the nonsingular square matrix A
with respect to inversion (see page 269).
A.4.2 Matrix Shaping Notation
diag(A)
or
vecdiag(A)
For the vector A, the vector consisting of the el-
ements of the principal diagonal of A;
diag(A) = vecdiag(A) = (a11, . . . , akk),
where k is the minimum of the number of rows
and the number of columns of A.
diag(v)
For the vector v, the diagonal matrix whose
nonzero elements are those of v; that is, the
square matrix, A, such that Aii = vi and for i ̸= j,
Aij = 0.
diag(A1, A2, . . . , Ak)
The block diagonal matrix whose submatrices
along the diagonal are A1, A2, . . . , Ak.

A.4 Linear Spaces and Matrices
599
vec(A)
The vector consisting of the columns of the matrix
A all strung into one vector; if the column vectors
of A are a1, a2, . . . , am, then
vec(A) = (aT
1 , aT
2 , . . . , aT
m).
vech(A)
For the m × m symmetric matrix A, the vec-
tor consisting of the lower triangular elements all
strung into one vector:
vech(A) = (a11, a21, . . . , am1, a22, . . . , am2, . . . , amm).
A(i1,...,ik)
The matrix formed from rows i1, . . . , ik and
columns i1, . . . , ik from a given matrix A. This
kind of submatrix and the ones below occur often
when working with determinants (for square ma-
trices). If A is square, the determinants of these
submatrices are called minors (see page 67). Be-
cause the principal diagonal elements of this ma-
trix are principal diagonal elements of A, it is
called a principal submatrix of A. Generally, but
not necessarily, ij < ij+1.
A(i1,...,ik)(j1,...,jl)
The submatrix of a given matrix A formed from
rows i1, . . . , ik and columns j1, . . . , jl from A.
A(i1,...,ik)(∗)
or
A(∗)(j1,...,jl)
The submatrix of a given matrix A formed from
rows i1, . . . , ik and all columns or else all rows and
columns j1, . . . , jl from A.
A−(i1,...,ik)(j1,...,jl)
The submatrix formed from a given matrix A by
deleting rows i1, . . . , ik and columns j1, . . . , jl.
A−(i1,...,ik)()
or
A−()(j1,...,jl)
The submatrix formed from a given matrix A by
deleting rows i1, . . . , ik (and keeping all columns)
or else by deleting columns j1, . . . , jl from A.

600
Appendix A: Notation and Deﬁnitions
A.4.3 Notation for Rows or Columns of Matrices
ai∗
The vector that corresponds to the ith row of the matrix A. As
with all vectors, this is a column vector, so it often appears in the
form aT
i∗.
a∗j
The vector that corresponds to the jth column of the matrix A.
A.4.4 Notation Relating to Matrix Determinants
|A|
The determinant of the square matrix A,
|A| = det(A).
det(A)
The determinant of the square matrix A,
det(A) = |A|.
det

A(i1,...,ik)

A principal minor of a square matrix A; in this case, it is
the minor corresponding to the matrix formed from rows
i1, . . . , ik and columns i1, . . . , ik from a given matrix A.
The notation |A(i1,...,ik)| is also used synonymously.
det

A−(i)(j)

The minor associated with the (i, j)th element of a
square matrix A. The notation |A−(i)(j)| is also used
synonymously.
a(ij)
The cofactor associated with the (i, j)th element of a
square matrix A; that is, a(ij) = (−1)i+j|A−(i)(j)|.
adj(A)
The adjugate, also called the classical adjoint, of the
square matrix A: adj(A) = (a(ji)); that is, the matrix
of the same size as A formed from the cofactors of the
elements of AT.
A.4.5 Matrix-Vector Diﬀerentiation
dt
The diﬀerential operator on the scalar, vector, or matrix t. This
is an operator; d may be used to represent a variable. (Note
the diﬀerence in the font.)
gf
or ∇f
For the scalar-valued function f of a vector variable, the vector
whose ith element is ∂f/∂xi. This is the gradient, also often
denoted as gf.

A.4 Linear Spaces and Matrices
601
∇f
For the vector-valued function f of a vector variable, the ma-
trix whose element in position (i, j) is
∂fj(x)
∂xi
.
This is also written as ∂f T/∂x or just as ∂f/∂x. This is the
transpose of the Jacobian of f.
Jf
For the vector-valued function f of a vector variable, the Ja-
cobian of f denoted as Jf. The element in position (i, j) is
∂fi(x)
∂xj
.
This is the transpose of (∇f): Jf = (∇f)T.
Hf
or ∇∇f
or ∇2f
The Hessian of the scalar-valued function f of a vector variable.
The Hessian is the transpose of the Jacobian of the gradient.
Except in pathological cases, it is symmetric. The element in
position (i, j) is
∂2f(x)
∂xi∂xj
.
The symbol ∇2f is sometimes also used to denote the trace of
the Hessian, in which case it is called the Laplace operator.
A.4.6 Special Vectors and Matrices
1 or 1n
A vector (of length n) whose elements are all 1s.
0 or 0n
A vector (of length n) whose elements are all 0s.
I or In
The (n × n) identity matrix.
ei
The ith unit vector (with implied length) (see page 16).
A.4.7 Elementary Operator Matrices
Epq
The (p, q)th elementary permutation matrix (see page 81).
E(π)
The permutation matrix that permutes the rows according to
the permutation π.

602
Appendix A: Notation and Deﬁnitions
Ep(a)
The pth elementary scalar multiplication matrix (see page 82).
Epq(a)
The (p, q)th elementary axpy matrix (see page 83).
A.5 Models and Data
A form of model used often in statistics and applied mathematics has three
parts: a left-hand side representing an object of primary interest; a function
of another variable and a parameter, each of which is likely to be a vector;
and an adjustment term to make the right-hand side equal the left-hand side.
The notation varies depending on the meaning of the terms. One of the most
common models used in statistics, the linear regression model with normal
errors, is written as
Y = βTx + E.
(A.1)
The adjustment term is a random variable, denoted by an uppercase epsilon.
The term on the left-hand side is also a random variable. This model does not
represent observations or data. A slightly more general form is
Y = f(x; θ) + E.
(A.2)
A single observation or a single data item that corresponds to model (A.1)
may be written as
y = βTx + ϵ,
or, if it is one of several,
yi = βTxi + ϵi.
Similar expressions are used for a single data item that corresponds to
model (A.2).
In these cases, rather than being a random variable, ϵ or ϵi may be a
realization of a random variable, or it may just be an adjustment factor with
no assumptions about its origin.
A set of n such observations is usually represented in an n-vector y, a
matrix X with n rows, and an n-vector ϵ:
y = Xβ + ϵ
or
y = f(X; θ) + ϵ.

B
Solutions and Hints for Selected Exercises
Exercises Beginning on Page 52
2.3b.
Let one vector space consist of all vectors of the form (a, 0) and the
other consist of all vectors of the form (0, b). The vector (a, b) is not in
the union if a ̸= 0 and b ̸= 0.
2.8.
Give a counterexample to the triangle inequality; for example, let x =
(9, 25) and y = (16, 144).
2.11a. We ﬁrst observe that if ∥x∥p = 0 or ∥y∥q = 0, we have x = 0 or y = 0,
and so the inequality is satisﬁed because both sides are 0; hence, we
need only consider the case ∥x∥p > 0 and ∥y∥q > 0. We also observe
that if p = 1 or q = 1, we have the Manhattan and Chebyshev norms
and the inequality is satisﬁed; hence we need only consider the case
1 < p < ∞.
Now, for p and q as given, for any numbers ai and bi, there are numbers
si and ti such that |ai| = esi/p and |bi| = eti/q. Because ex is a convex
function, we have esi/p+ti/q ≤1
pes
i + 1
q et
i, or
aibi ≤|ai||bi| ≤|ai|p/p + |bi|q/q.
Now let
ai =
xi
∥x∥p
and
bi =
yi
∥y∥q
,
and so
xi
∥x∥p
yi
∥y∥q
≤1
p
|xi|p
∥x∥p
p + 1
q
|yi|q
∥y∥q
q .
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5
603

604
Appendix B: Solutions and Hints for Exercises
Now, summing these equations over i, we have
⟨x, y⟩
∥x∥p∥y∥q
≤1
p
∥x∥p
p
∥x∥p
p + 1
q
∥y∥q
q
∥y∥q
q
= 1.
Hence, we have the desired result.
As we see from this proof, the inequality is actually a little stronger
than stated. If we deﬁne u and v by ui = |xi| and vi = |yi|, we have
⟨x, y⟩≤⟨u, v⟩≤∥x∥p∥y∥q.
We observe that equality occurs if and only if
 |xi|
∥x∥p
 1
q
=
 |yi|
∥y∥q
 1
p
and
sign(xi) = sign(yi)
for all i.
We note a special case by letting y = 1:
¯x ≤∥x∥p,
and with p = 2, we have a special case of the Cauchy-Schwarz inequal-
ity,
n¯x2 ≤∥x∥2
2,
which guarantees that V(x) ≥0.
2.11b. Using the triangle inequality for the absolute value, we have |xi +yi| ≤
|xi| + |yi|. This yields the result for p = 1 and p = ∞(in the limit).
Now assume 1 < p < ∞. We have
∥x + y∥p
p ≤
n

i=1
|xi + yi|p−1|xi| +
n

i=1
|xi + yi|p−1|yi|.
Now, letting q = p/(p −1), we apply H¨older’s inequality to each of the
terms on the right:
n

i=1
|xi + yi|p−1|xi| ≤
 n

i=1
|xi + yi|(p−1)q
 1
q  n

i=1
|xi|p
 1
p
and
n

i=1
|xi + yi|p−1|xi| ≤
 n

i=1
|xi + yi|(p−1)q
 1
q  n

i=1
|yi|p
 1
p
,

Appendix B: Solutions and Hints for Exercises
605
so
n

i=1
|xi+yi|p ≤
 n

i=1
|xi + yi|(p−1)q
 1
q ⎛
⎝
 n

i=1
|xi|p
 1
p
+
 n

i=1
|yi|p
 1
p ⎞
⎠
or, because (p −1)q = p and 1 −1
q = 1
p,
 n

i=1
|xi + yi|p
 1
p
≤
 n

i=1
|xi|p
 1
p
+
 n

i=1
|yi|p
 1
p
,
which is the same as
∥x + y∥p ≤∥x∥p + ∥y∥p,
the triangle inequality.
2.19e. In IR3,
angle(x, y) = sin−1
∥x × y∥
∥x∥∥y|

.
Because x × y = −y × x, this allows us to determine the angle from x
to y; that is, the direction within (−π, π] in which x would be rotated
to y.
2.21.
Just consider the orthogonal vectors x = (1, 0) and y = (0, 1). The
centered vectors are xc = ( 1
2, −1
2) and yc = (−1
2, 1
2). The angle between
the uncentered vectors is π/2, while that between the centered vectors
is π.
Exercises Beginning on Page 178
3.2.
This exercise occurs in various guises in many diﬀerent places, and the
simple approach is to add and subtract ¯x:
(x −a)T(x −a) = (x −¯x −(a + ¯x))T(x −¯x −(a + ¯x))
= (xc −(a + ¯x))T(xc −(a + ¯x))
= xT
c xc + (a + ¯x)T(a + ¯x)) −2(a + ¯x)Txc
= xT
c xc + n(a + ¯x)2.
Finally, we get the expressions in equation (3.92) by writing xT
c xc as
tr(xcxT
c ).
3.14. Write the n × m matrix A as
A = (aij) = [a∗1, . . . , a∗m].
If A is of rank one, the maximum number of linearly independent
columns is one; hence, for k = 2, . . . , m, a∗k = cka∗1, for some ck.

606
Appendix B: Solutions and Hints for Exercises
Now let x = a∗1, which is an n-vector, and let y be an m-vector whose
ﬁrst element is 1 and whose k = 2, . . . , m elements are the cks. We see
that A = xyT by direct multiplication.
This decomposition is not unique, of course.
3.15. If the elements of the square matrix A are integers then each cofactor
a(ij) is an integer, and hence the elements of adj(A) are integers. If
|A| = ±1, then A−1 = ±adj(A), and so the elements of A−1 are integers.
An easy way to form a matrix whose determinant is ±1 is to form an
upper triangular matrix with either 1 or −1 on the diagonal. A more
“interesting matrix” that has the same determinant can then be formed
by use of elementary operations. (People teaching matrix algebra ﬁnd
this useful!)
3.16. Because the inverse of a matrix is unique, we can verify each equation
by multiplication by the inverse at the appropriate place. We can of-
ten reduce an expression to a desired form by multiplication by the
identity MM −1, where M is some appropriate matrix. For example,
equation (3.177) can be veriﬁed by the equations
(A + B)(A−1 −A−1(A−1 + B−1)−1A−1)
=
I −(A−1 + B−1)−1A−1 + BA−1 −BA−1(A−1 + B−1)−1A−1 =
I + BA−1 −(I + BA−1)(A−1 + B−1)−1A−1 =
(I + BA−1)(I −(A−1 + B−1)−1A−1) =
B(B−1 + A−1)(I −(A−1 + B−1)−1A−1) =
B(B−1 + A−1) −BA−1 = I
3.19. Express the nonzero elements of row i in the n × n matrix A as aibk
for k = i, . . . , n, and the nonzero elements of column j as akbj for
k = j, . . . , n. Then obtain expressions for the elements of A−1. Show,
for example, that the diagonal elements of A−1 are (aibi)−1.
3.25. For property 8, let c be a nonzero eigenvalue of AB. Then there exists
v (̸= 0) such that ABv = cv, that is, BABv = Bcv. But this means
BAw = cw, where w = Bv ̸= 0 (because ABv ̸= 0) and so c is an
eigenvalue of BA. We use the same argument starting with an eigenvalue
of BA. For square matrices, there are no other eigenvalues, so the set
of eigenvalues is the same.
For property 9, see the discussion of similarity transformations on
page 146.
3.37. Let A and B be such that AB is deﬁned.
∥AB∥2
F =

ij


k
aikbkj

2

Appendix B: Solutions and Hints for Exercises
607
≤

ij

k
a2
ik
 
k
b2
kj

(Cauchy-Schwarz)
=
⎛
⎝
i,k
a2
ik
⎞
⎠
⎛
⎝
k,j
b2
kj
⎞
⎠
= ∥A∥2
F∥B∥2
F.
3.40. Hints.
For inequality (3.307), use the Cauchy-Schwartz inequality.
For inequality (3.309), use H¨older’s inequality.
Exercises Beginning on Page 222
4.7b. The ﬁrst step is to use the trick of equation (3.90), xTAx = tr(AxxT),
again to undo the earlier expression, and write the last term in equa-
tion (4.44) as
−n
2 tr

Σ−1(¯y −μ)(¯y −μ)T
= −n
2 (¯y −μ)Σ−1(¯y −μ)T.
Now Σ−1 is positive deﬁnite, so (¯y −μ)Σ−1(¯y −μ)T ≥0 and hence
is minimized for ˆμ = ¯y. Decreasing this term increases the value of
l(μ, Σ; y), and so l(ˆμ, Σ; y) ≥l(μ, Σ; y) for all positive deﬁnite Σ−1.
Now, we consider the other term. Let A = n
i=1(yi −¯y)(yi −¯y)T. The
ﬁrst question is whether A is positive deﬁnite. We will refer to a text
on multivariate statistics for the proof that A is positive deﬁnite with
probability 1 (see Muirhead 1982, for example). We have
l(ˆμ, Σ; y) = c −n
2 log |Σ| −1
2tr

Σ−1A

= c −n
2

log |Σ| + tr

Σ−1A/n

.
Because c is constant, the function is maximized at the minimum of the
latter term subject to Σ being positive deﬁnite, which, as shown for
expression (4.61), occurs at *Σ = A/n.
4.12. 2dn/2Γd(n/2)|Σ|n/2.
Make the change of variables W = 2Σ
1
2 Y Σ
1
2 , determine the Jacobian,
and integrate.

608
Appendix B: Solutions and Hints for Exercises
Exercises Beginning on Page 261
5.2. The R code that will produce the graph is
x<-c(0,1)
y<-c(0,1)
z<-matrix(c(0,0,1,1),nrow=2)
trans<-persp(x, y, z, theta = 45, phi = 30)
bottom<-c(.5,0,0,1)%*%trans
top<-c(.5,1,1,1)%*%trans
xends<-c(top[,1]/top[,4],bottom[,1]/bottom[,4])
yends<-c(top[,2]/top[,4],bottom[,2]/bottom[,4])
lines(xends,yends,lwd=2)
5.5. Let A is an n × n matrix, whose columns are the same as the vectors
aj, and let QR be the QR factorization of A. Because Q is orthogonal,
det(Q) = 1, and det(R) = det(A). Hence, we have
|det(A)| = |det(R)|
=
n
$
j=1
|rjj|
≤
n
$
j=1
∥rj∥2
=
n
$
j=1
∥aj∥2,
where rj is the vector whose elements are the same as the elements in
the jth column of R.
If equality holds, then either some aj is zero, or else rjj = ∥rj∥for
j = 1, . . . , n. In the latter case, R is diagonal, and hence ATA is diagonal,
and so the columns of A are orthogonal.
Exercises Beginning on Page 305
6.1.
First, show that
max
x̸=0
∥Ax∥
∥x∥=

min
x̸=0
∥A−1x∥
∥x∥
−1
and
max
x̸=0
∥A−1x∥
∥x∥
=

min
x̸=0
∥Ax∥
∥x∥
−1
.

Appendix B: Solutions and Hints for Exercises
609
6.2a. The matrix prior to the ﬁrst elimination is
⎡
⎣
2 5 3 19
1 4 1 12
1 2 2
9
⎤
⎦.
The solution is (3, 2, 1).
6.2b. The matrix prior to the ﬁrst elimination is
⎡
⎣
5 2 3 19
4 1 1 12
2 1 2
9
⎤
⎦,
and x1 and x2 have been interchanged.
6.2c.
D =
⎡
⎣
1 0 0
0 5 0
0 0 2
⎤
⎦,
L =
⎡
⎣
0 0 0
2 0 0
1 2 0
⎤
⎦,
U =
⎡
⎣
0 4 1
0 0 3
0 0 0
⎤
⎦,
ρ((D + L)−1U) = 1.50.
6.2e. ρ(( D + L)−1 U) = 0.9045.
6.2g. Some R code for this is
tildeD <- matrix(c(2, 0, 0,
0, 4, 0,
0, 0, 2), nrow=3, byrow=T)
tildeL <- matrix(c(0, 0, 0,
1, 0, 0,
1, 2, 0), nrow=3, byrow=T)
tildeU <- matrix(c(0, 5, 3,
0, 0, 1,
0, 0, 0), nrow=3, byrow=T)
b <- c(12,19,9)
omega <- 0.1
tildeDUadj <- (1-omega)*tildeD - omega*tildeU
tildeAk <- tildeD+omega*tildeL
badj <- omega*b
xk <- c(1,1,1)

610
Appendix B: Solutions and Hints for Exercises
nstep <- 2
for (i in 1:nstep){
bk <- tildeDUadj%*%xk + badj
xkp1 <- solve(tildeAk,bk)
dif <- sqrt(sum((xkp1-xk)^2))
print(dif)
xk <- xkp1
}
6.4a. nm(m + 1) −m(m + 1)/2. (Remember ATA is symmetric.)
6.4g. Using the normal equations with the Cholesky decomposition requires
only about half as many ﬂops as the QR, when n is much larger than
m. The QR method oftens yields better accuracy, however.
6.5a.
1. X(XTX)−1XTX = X
2. (XTX)−1XTX(XTX)−1XT = (XTX)−1XT
3. X(XTX)−1XT is symmetric (take its transpose).
4. (XTX)−1XTX is symmetric.
Therefore, (XTX)−1XT = X+.
6.5b. We want to show XT(y −XX+y) = 0. Using the properties of X+, we
have
XT(y −XX+y) = XTy −XTXX+y
= XTy −XT(XX+)Ty
because of symmetry
= XTy −XT(X+)TXTy
= XTy −XT(XT)+XTy
property of Moore-Penrose
inverses and transposes
= XTy −XTy
property of Moore-Penrose inverses
= 0
Exercises Beginning on Page 324
7.1a. 1.
7.1b. 1.
7.1d. 1. (All that was left was to determine the probability that cn ̸= 0 and
cn−1 ̸= 0.)
7.2a. 11.6315.
7.3.
⎡
⎢⎢⎣
3.08 −0.66
0
0
−0.66
4.92 −3.27
0
0 −3.27
7.00 −3.74
0
0 −3.74
7.00
⎤
⎥⎥⎦.

Appendix B: Solutions and Hints for Exercises
611
Exercises Beginning on Page 396
8.10a.
p(c) = cm−α1cm−1−α2σ1cm−2−α3σ1σ2cm−3−· · ·−αmσ1σ2 · · · σm−1.
8.10b. Deﬁne
f(c) = 1 −p(c)
cm .
This is a monotone decreasing continuous function in c, with f(c) →∞
as c →0+ and f(c) →0 as c →∞. Therefore, there is a unique value
c∗for which f(c∗) = 1. The uniqueness also follows from Descartes’
rule of signs, which states that the maximum number of positive roots
of a polynomial is the number of sign changes of the coeﬃcients, and
in the case of the polynomial p(c), this is one.
8.18.
This is a simple case of matrix multiplication.
To illustrate the use of R in complex matrices, I will show some code
that is relevant to this problem, for a given order, of course.
omegajn <- function(n){
#####
Function to create the n^th roots of 1
#####
omegajn <- complex(n)
omegajn[1] <- 1
if (n>=2) omegajn[2] <- complex(re=cos(2*pi/n),
im=sin(2*pi/n))
if (n>=3) for (j in 3:n) omegajn[j] <- omegajn[2]
*omegajn[j-1]
return(omegajn)
}
Fn <- function(n){
#####
Function to create a Fourier matrix
#####
rts <- omegajn(n)
Fn <- matrix(c(rep(1,n),rts),nrow=n)
if (n>=3) for (j in 3:n) Fn <- cbind(Fn,rts^(j-1))
Fn <- Fn/sqrt(n)
return(Fn)
}
# perform multiplications to get the elementary
circulant matrix or order 5
round(Conj(t(F5))%*%diag(rts5)%*%F5)
8.19.
(−1)⌊n/2⌋nn, where ⌊·⌋is the ﬂoor function (the greatest integer func-
tion). For n = 1, 2, 3, 4, the determinants are 1, −4, −27, 256.

612
Appendix B: Solutions and Hints for Exercises
Exercises Beginning on Page 452
9.1.
1. This is because the subspace that generates a singular matrix is a
lower dimensional space than the full sample space, and so its measure
is 0.
9.4d.
Assuming W is positive deﬁnite, we have
*βW,C = (XTWX)−1XTWy +
(XTWX)−1LT(L(XTWX)+LT)−1(c −L(XTWX)+XTWy).
9.12.
Let X = [Xi | Xo] and Z = XT
o Xo −XT
o Xi(XT
i Xi)−1XT
i Xo. Note that
XT
o Xi = XT
i Xo. We have
XT
i X(XTX)−1XT
= XT
i [Xi | Xo]
⎡
⎣
XT
i Xi
XT
o Xi

XT
i Xo
XT
o Xo
−1
[Xi | Xo]T
=

XT
i Xi | XT
i Xo

	
(XT
i Xi)−1 −(XT
i Xi)−1(XT
o Xi)Z−1(XT
i Xo)(XT
i Xi)−1
−Z−1(XT
o Xi)(XT
i Xi)−1

−(XT
i Xi)−1(XT
i Xo)Z−1
Z−1

	
XT
i
XT
o

=

I −(XT
o Xi)Z−1(XT
i Xo)(XT
i Xi)−1 −XT
i XoZ−1(XT
o Xi)(XT
i Xi)−1
|
−XT
i XoZ−1 + XT
i XoZ−1
	
XT
i
XT
o

= XT
i ,
9.14.
One possibility is
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
20 100
5
25
5
25
10 NA
10 NA
10 NA
NA 10
NA 10
NA 10
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.

Appendix B: Solutions and Hints for Exercises
613
The variance-covariance matrix computed from all pairwise complete
observations is
! 30 375
375 1230
"
,
while that computed only from complete cases is
! 75 375
375 1875
"
.
The
correlation
matrix
computed
from
all
pairwise
complete
observations is
!1.00 1.95
1.95 1.00
"
.
Note that this example is not a pseudo-correlation matrix.
In the R software system, the cov and cor functions have an argument
called “use”, which can take the values “all.obs”, “complete.obs”, or
“pairwise.complete.obs”. The value “all.obs” yields an error if the data
matrix contains any missing values. In cov, the values “complete.obs”
and “pairwise.complete.obs” yield the variance-covariances shown
above. The function cor with use=‘‘pairwise.complete.obs’’
yields
!1.00 1.00
1.00 1.00
"
.
However, if cov is invoked with use=‘‘pairwise.complete.obs’’and
the function cov2cor is applied to the result, the correlations are 1.95,
as in the ﬁrst correlation matrix above.
9.17.
This is an open question. If you get a proof of convergence, submit it
for publication. You may wish to try several examples and observe the
performance of the intermediate steps. I know of no case in which the
method has not converged.
9.19b. Starting with the correlation matrix given above as a possible solution
for Exercise 9.14, four iterations of equation (9.67) using δ = 0.05 and
f(x) = tanh(x) yield
! 1.00 0.997
0.997 1.00
"
.
9.21.
We can develop a recursion for pt
11 based on pt−1
11
and pt−1
12 ,
pt
11 = pt−1
11 (1 −α) + pt−1
12 β,
and because p11 + p12 = 1, we have pt
11 = pt−1
11 (1 −α −β) + β. Putting
this together, we have
lim
t→∞P =
!β/(α + β) α/(α + β)
β/(α + β) α/(α + β)
"
,
and so the limiting (and invariant) distribution is πs = (β/(α +
β), α/(α + β)).

614
Appendix B: Solutions and Hints for Exercises
9.22c. From the exponential growth, we have N (T ) = N (0)erT; hence,
r = 1
T log

N (T )/N (0)
= 1
T log(r0).
Exercises Beginning on Page 516
10.1a.
The computations do not overﬂow. The ﬁrst ﬂoating-point number x
such that x + 1 = x is
0.10 · · ·0 × bp+1.
Therefore, the series converges at the value of i such that i(i+1)/2 =
x. Now solve for i.
10.2.
The function is log(n), and Euler’s constant is 0.57721 . . ..
10.6.
2−56. (The standard has 53 bits normalized, so the last bit is 2−55,
and half of that is 2−56.)
10.7a.
Normalized: 2bp−1(b −1)(emax −emin + 1) + 1.
Nonnormalized: 2bp−1(b −1)(emax −emin + 1) + 1 + 2bp−1.
10.7b.
Normalized: bemin−1.
Nonnormalized: bemin−p.
10.7c.
1 + b−p+1 or 1 + b−p when b = 2 and the ﬁrst bit is hidden.
10.7d.
bp.
10.7e.
22.
10.12.
First of all, we recognize that the full sum in each case is 1. We
therefore accumulate the sum from the direction in which there are
fewer terms. After computing the ﬁrst term from the appropriate
direction, take a logarithm to determine a scaling factor, say sk. (This
term will be the smallest in the sum.) Next, proceed to accumulate
terms until the sum is of a diﬀerent order of magnitude than the
next term. At that point, perform a scale adjustment by dividing by
s. Resume summing, making similar scale adjustments as necessary,
until the limit of the summation is reached.
10.14.
The result is close to 1.
What is relevant here is that numbers close to 1 have only a very
few digits of accuracy; therefore, it would be better to design this
program so that it returns 1 −Pr(X ≤x) (the “signiﬁcance level”).
The purpose and the anticipated use of a program determine how it
should be designed.
10.17a. 2.
10.17b. 0.
10.17c. No (because the operations in the “for” loop are not chained).

Appendix B: Solutions and Hints for Exercises
615
10.18c.
a = x1
b = y1
s = 0
for i = 2, n
{
d = (xi −a)/i
e = (yi −b)/i
a = d + a
b = e + b
s = i(i −1)de + s
}.
10.21.
1. No; 2. Yes; 3. No; 4. No.
10.22.
A very simple example is
!1 1 + ϵ
1
1
"
,
where ϵ < b−p, because in this case the matrix stored in the computer
would be singular. Another example is
!
1
a(1 + ϵ)
a(1 + ϵ) a2(1 + 2ϵ)
"
,
where ϵ is the machine epsilon.
Exercises Beginning on Page 537
11.2a. O(nk).
11.2c. At each successive stage in the fan-in, the number of processors doing
the additions goes down by approximately one-half.
If p ≈k, then O(n log k) (fan-in on one element of c at a time)
If p ≈nk, then O(log k) (fan-in on all elements of c simultaneously)
If p is a ﬁxed constant smaller than k, the order of time does not
change; only the multiplicative constant changes.
Notice the diﬀerence in the order of time and the order of the number
of computations. Often there is very little that can be done about the
order of computations.
11.2d. Because in a serial algorithm the magnitudes of the summands become
more and more diﬀerent. In the fan-in, they are more likely to remain
relatively equal. Adding magnitudes of diﬀerent quantities results in
benign roundoﬀ, but many benign roundoﬀs become bad. (This is not

616
Appendix B: Solutions and Hints for Exercises
catastrophic cancellation.) Clearly, if all elements are nonnegative, this
argument would hold. Even if the elements are randomly distributed,
there is likely to be a drift in the sum (this can be thought of as a
random walk). There is no diﬀerence in the number of computations.
11.2e. Case 1: p ≈n. Give each ci a processor – do an outer loop on each.
This would likely be more eﬃcient because all processors are active at
once.
Case 2: p ≈nk. Give each aijbj a processor – fan-in for each. This
would be the same as the other.
If p is a ﬁxed constant smaller than n, set it up as in Case 1, using n/p
groups of ci’s.
11.2f. If p ≈n, then O(k).
If p ≈nk, then O(log k).
If p is some small ﬁxed constant, the order of time does not change;
only the multiplicative constant changes.
Exercises Beginning on Page 582
12.2.
Here is a recursive Matlab function for the Strassen algorithm due
to Coleman and Van Loan. When it uses the Strassen algorithm, it
requires the matrices to have even dimension.
function C = strass(A,B,nmin)
%
% Strassen matrix multiplication C=AB
%
A, B must be square and of even dimension
% From Coleman and Van Loan
% If n <= nmin, the multiplication is done conventionally
%
[n n ] = size(A);
if n <= nmin
C = A * B;
% n is small, get C conventionally
else
m = n/2; u = 1:m; v = m+1:n;
P1 = strass(A(u,u)+A(v,v), B(u,u)+B(v,v), nmin);
P2 = strass(A(v,u)+A(v,v), B(u,u),
nmin);
P3 = strass(A(u,u),
B(u,v)-B(v,v), nmin);
P4 = strass(A(v,v),
B(v,u)-B(u,u), nmin);
P5 = strass(A(u,u)+A(u,v), B(v,v),
nmin);
P6 = strass(A(v,u)-A(u,u), B(u,u)+B(u,v), nmin);
P7 = strass(A(u,v)-A(v,v), B(v,u)+B(v,v), nmin);
C = [P1+P4-P5+P7 P3+P5; P2+P4 P1+P3-P2+P6];
end

Appendix B: Solutions and Hints for Exercises
617
12.5a.
real a(4,3)
data a/3.,6.,8.,2.,5.,1.,6.,3.,6.,2.,7.,1./
n = 4
m = 3
x1 = a(2,2) ! Temporary variables must be used
because of
x2 = a(4,2) ! the side effects of srotg.
call srotg(x1, x2,, c, s)
call srot(m, a(2,1), n, a(4,1), n, c, s)
print *, c, s
print *, a
end
This yields 0.3162278 and 0.9486833 for c and s. The transformed
matrix is
⎡
⎢⎢⎢⎢⎢⎣
3.000000
5.000000
6.000000
3.794733
3.162278
1.581139
8.000000
6.000000
7.000000
−5.059644 −0.00000002980232 −1.581139
⎤
⎥⎥⎥⎥⎥⎦
.
12.7b. Using the Matrix package in R, after initializing rho and sig2, this is
Vinv <- sparseMatrix(i=c(1,1,2,2,2,3,3,3,4,4,4,5,5,5,6,6,6,7,
7,7,8,8,8,9,9,9,10,10),
j=c(1,2,1:3,2:4,3:5,4:6,5:7,6:8,7:9,8:10,
9,10),
x=c(1,-rho,rep(c(-rho,1+rho^2,-rho),8),
-rho,1))/((1-rho^2)*sig2)
12.9.
10.7461941829033 and 10.7461941829034.
12.11.
−(fh)+ei
−(ceg)+bfg+cdh−afh−bdi+aei
ch−bi
−(ceg)+bfg+cdh−afh−bdi+aei
−(ce)+bf
−(ceg)+bfg+cdh−afh−bdi+aei
fg−di
−(ceg)+bfg+cdh−afh−bdi+aei
−(cg)+ai
−(ceg)+bfg+cdh−afh−bdi+aei
cd−af
−(ceg)+bfg+cdh−afh−bdi+aei
−(eg)+dh
−(ceg)+bfg+cdh−afh−bdi+aei
bg−ah
−(ceg)+bfg+cdh−afh−bdi+aei
−(bd)+ae
−(ceg)+bfg+cdh−afh−bdi+aei

Bibliography
Many of the most useful background material is available on the internet.
For statistics, one of the most useful sites on the internet is the electronic
repository statlib, maintained at Carnegie Mellon University, which contains
programs, datasets, and other items of interest. The URL is
http://lib.stat.cmu.edu.
The collection of algorithms published in Applied Statistics is available in
statlib. These algorithms are sometimes called the ApStat algorithms.
Another very useful site for scientiﬁc computing is netlib,. The URL is
http://www.netlib.org
The Collected Algorithms of the ACM (CALGO), which are the Fortran, C,
and Algol programs published in ACM Transactions on Mathematical Soft-
ware (or in Communications of the ACM prior to 1975), are available in
netlib under the TOMS link.
A wide range of software is used in the computational sciences. Some of
the software is produced by a single individual who is happy to share the
software, sometimes for a fee, but who has no interest in maintaining it. At
the other extreme is software produced by large commercial companies whose
continued existence depends on a process of production, distribution, and
maintenance of the software. Information on much of the software can be
obtained from GAMS, as we mentioned at the beginning of Chap. 12. Some
of the free software can be obtained from statlib or netlib.
The following bibliography obviously covers a wide range of topics in statis-
tical computing and computational statistics. Except for a few of the general
references, all of these entries have been cited in the text.
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5
619

620
Bibliography
Abramowitz, Milton, and Irene A. Stegun, eds. 1964. Handbook of Mathemat-
ical Functions with Formulas, Graphs, and Mathematical Tables. Washing-
ton: National Bureau of Standards (NIST). (Reprinted in 1965 by Dover
Publications, Inc., New York.)
Alefeld, G¨oltz, and J¨urgen Herzberger. (1983). Introduction to Interval Com-
putation. New York: Academic Press.
Ammann, Larry, and John Van Ness. 1988. A routine for converting regres-
sion algorithms into corresponding orthogonal regression algorithms. ACM
Transactions on Mathematical Software 14:76–87.
Anda, Andrew A., and Haesun Park. 1994. Fast plane rotations with dynamic
scaling. SIAM Journal of Matrix Analysis and Applications 15:162–174.
Anda, Andrew A., and Haesun Park. 1996. Self-scaling fast rotations for stiﬀ
least squares problems. Linear Algebra and Its Applications 234:137–162.
Anderson, E., Z. Bai, C. Bischof, L. S. Blackford, J. Demmel, J. Dongarra, J.
Du Croz, A. Greenhaum, S. Hammarling, A. McKenney, and D. Sorensen.
2000. LAPACK Users’ Guide, 3rd ed. Philadelphia: Society for Industrial
and Applied Mathematics.
Anderson, T. W. 1951. Estimating linear restrictions on regression coeﬃ-
cients for multivariate nomal distributions. Annals of Mthematical Statistics
22:327–351.
Anderson, T. W. 2003. An Introduction to Multivariate Statistical Analysis,
3rd ed. New York: John Wiley and Sons.
ANSI. 1978. American National Standard for Information Systems — Pro-
gramming Language FORTRAN, Document X3.9-1978. New York: Ameri-
can National Standards Institute.
ANSI. 1989. American National Standard for Information Systems — Pro-
gramming Language C, Document X3.159-1989. New York: American Na-
tional Standards Institute.
ANSI. 1992. American National Standard for Information Systems — Pro-
gramming Language Fortran-90, Document X3.9-1992. New York: Ameri-
can National Standards Institute.
ANSI. 1998. American National Standard for Information Systems — Pro-
gramming Language C++, Document ISO/IEC 14882-1998. New York:
American National Standards Institute.
Atkinson, A. C., and A. N. Donev. 1992. Optimum Experimental Designs.
Oxford, United Kingdom: Oxford University Press.
Attaway, Stormy. 2016. Matlab: A Practical Introduction to Programming
and Problem Solving, 4th ed. Oxford, United Kingdom: Butterworth-
Heinemann.
Bailey, David H. 1993. Algorithm 719: Multiprecision translation and execu-
tion of FORTRAN programs. ACM Transactions on Mathematical Software
19:288–319.
Bailey, David H. 1995. A Fortran 90-based multiprecision system. ACM Trans-
actions on Mathematical Software 21:379–387.

Bibliography
621
Bailey, David H., King Lee, and Horst D. Simon. 1990. Using Strassen’s al-
gorithm to accelerate the solution of linear systems. Journal of Supercom-
puting 4:358–371.
Bapat, R. B., and T. E. S. Raghavan. 1997. Nonnegative Matrices and Appli-
cations. Cambridge, United Kingdom: Cambridge University Press.
Barker, V. A., L. S. Blackford, J. Dongarra, J. Du Croz, S. Hammarling, M.
Marinova, J. Wasniewsk, and P. Yalamov. 2001. LAPACK95 Users’ Guide.
Philadelphia: Society for Industrial and Applied Mathematics.
Barrett, R., M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V.
Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst. 1994. Templates for
the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd
ed. Philadelphia: Society for Industrial and Applied Mathematics.
Basilevsky, A. 1983. Applied Matrix Algebra in the Statistical Sciences. New
York: North Holland
Beaton, Albert E., Donald B. Rubin, and John L. Barone. 1976. The ac-
ceptability of regression solutions: Another look at computational accuracy.
Journal of the American Statistical Association 71:158–168.
Benzi, Michele. 2002. Preconditioning techniques for large linear systems: A
survey. Journal of Computational Physics 182:418–477.
Bickel, Peter J., and Joseph A. Yahav. 1988. Richardson extrapolation and
the bootstrap. Journal of the American Statistical Association 83:387–393.
Bindel, David, James Demmel, William Kahan, and Osni Marques. 2002. On
computing Givens rotations reliably and eﬃciently. ACM Transactions on
Mathematical Software 28:206–238.
Birkhoﬀ, Garrett, and Surender Gulati. 1979. Isotropic distributions of test
matrices. Journal of Applied Mathematics and Physics (ZAMP) 30:148–158.
Bischof, Christian H. 1990. Incremental condition estimation. SIAM Journal
of Matrix Analysis and Applications 11:312–322.
Bischof, Christian H., and Gregorio Quintana-Ort´ı. 1998a. Computing rank-
revealing QR factorizations. ACM Transactions on Mathematical Software
24:226–253.
Bischof, Christian H., and Gregorio Quintana-Ort´ı. 1998b. Algorithm 782:
Codes for rank-revealing QR factorizations of dense matrices. ACM Trans-
actions on Mathematical Software 24:254–257.
Bj¨orck, ˚Ake. 1967. Solving least squares problems by Gram-Schmidt orthog-
onalization. BIT 7:1–21.
Bj¨orck, ˚Ake. 1996. Numerical Methods for Least Squares Problems. Philadel-
phia: Society for Industrial and Applied Mathematics.
Blackford, L. S., J. Choi, A. Cleary, E. D’Azevedo, J. Demmel, I. Dhillon, J.
Dongarra, S. Hammarling, G. Henry, A. Petitet, K. Stanley, D. Walker, and
R. C. Whaley. 1997a. ScaLAPACK Users’ Guide. Philadelphia: Society for
Industrial and Applied Mathematics.
Blackford, L. S., A. Cleary, A. Petitet, R. C. Whaley, J. Demmel, I. Dhillon,
H. Ren, K. Stanley, J. Dongarra, and S. Hammarling. 1997b. Practical ex-

622
Bibliography
perience in the numerical dangers of heterogeneous computing. ACM Trans-
actions on Mathematical Software 23:133–147.
Blackford, L. Susan, Antoine Petitet, Roldan Pozo, Karin Remington, R. Clint
Whaley, James Demmel, Jack Dongarra, Iain Duﬀ, Sven Hammarling, Greg
Henry, Michael Heroux, Linda Kaufman, and Andrew Lumsdaine. 2002. An
updated set of basic linear algebra subprograms (BLAS). ACM Transactions
on Mathematical Software 28:135–151.
Bollob´as, B´ela. 2013. Modern Graph Theory. New York: Springer-Verlag.
Brown, Peter N., and Homer F. Walker. 1997. GMRES on (nearly) sin-
gular systems. SIAM Journal of Matrix Analysis and Applications 18:
37–51.
Bunch, James R., and Linda Kaufman. 1977. Some stable methods for calcu-
lating inertia and solving symmetric linear systems. Mathematics of Com-
putation 31:163–179.
Buttari, Alfredo, Julien Langou, Jakub Kurzak, and Jack Dongarra. 2009. A
class of parallel tiled linear algebra algorithms for multicore architectures.
Parallel Computing 35:38–53.
Calvetti, Daniela. 1991. Roundoﬀerror for ﬂoating point representation of
real data. Communications in Statistics 20:2687–2695.
Campbell, S. L., and C. D. Meyer, Jr. 1991. Generalized Inverses of Linear
Transformations. New York: Dover Publications, Inc.
Carmeli, Moshe. 1983. Statistical Theory and Random Matrices. New York:
Marcel Dekker, Inc.
Chaitin-Chatelin, Fran¸coise, and Val´erie Frayss´e. 1996. Lectures on Finite
Precision Computations. Philadelphia: Society for Industrial and Applied
Mathematics.
Chambers, John M. 2016. Extending R. Boca Raton: Chapman and Hall/CRC
Press.
Chan, T. F. 1982a. An improved algorithm for computing the singular value
decomposition. ACM Transactions on Mathematical Software 8:72–83.
Chan, T. F. 1982b. Algorithm 581: An improved algorithm for computing the
singular value decomposition. ACM Transactions on Mathematical Software
8:84–88.
Chan, T. F., G. H. Golub, and R. J. LeVeque. 1982. Updating formulae and
a pairwise algorithm for computing sample variances. In Compstat 1982:
Proceedings in Computational Statistics, ed. H. Caussinus, P. Ettinger, and
R. Tomassone, 30–41. Vienna: Physica-Verlag.
Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. 1983. Algorithms
for computing the sample variance: Analysis and recommendations. The
American Statistician 37:242–247.
Chapman, Barbara, Gabriele Jost, and Ruud van der Pas. 2007. Using
OpenMP: Portable Shared Memory Parallel Programming. Cambridge,
Massachusetts: The MIT Press.

Bibliography
623
Cheng, John, Max Grossman, and Ty McKercher. 2014. Professional CUDA
C Programming. New York: Wrox Press, an imprint of John Wiley and
Sons.
Chu, Moody T. 1991. Least squares approximation by real normal matrices
with speciﬁed spectrum. SIAM Journal on Matrix Analysis and Applica-
tions 12:115–127.
ˇC´ıˇzkov´a, Lenka, and Pavel ˇC´ıˇzek. 2012. Numerical linear algebra. In Hand-
book of Computational Statistics: Concepts and Methods, 2nd revised and
updated ed., ed. James E. Gentle, Wolfgang H¨ardle, and Yuichi Mori, 105–
137. Berlin: Springer.
Clerman, Norman, and Walter Spector. 2012. Modern Fortran. Cambridge,
United Kingdom: Cambridge University Press.
Cline, Alan K., Andrew R. Conn, and Charles F. Van Loan. 1982. Generalizing
the LINPACK condition estimator. In Numerical Analysis, Mexico, 1981,
ed. J. P. Hennart, 73–83. Berlin: Springer-Verlag.
Cline, A. K., C. B. Moler, G. W. Stewart, and J. H. Wilkinson. 1979. An
estimate for the condition number of a matrix. SIAM Journal of Numerical
Analysis 16:368–375.
Cline, A. K., and R. K. Rew. 1983. A set of counter-examples to three condi-
tion number estimators. SIAM Journal on Scientiﬁc and Statistical Com-
puting 4:602–611.
Cody, W. J. 1988. Algorithm 665: MACHAR: A subroutine to dynamically de-
termine machine parameters. ACM Transactions on Mathematical Software
14:303–329.
Cody, W. J., and Jerome T. Coonen. 1993. Algorithm 722: Functions to sup-
port the IEEE standard for binary ﬂoating-point arithmetic. ACM Trans-
actions on Mathematical Software 19:443–451.
Coleman, Thomas F., and Charles Van Loan. 1988. Handbook for Matrix
Computations. Philadelphia: Society for Industrial and Applied Mathemat-
ics.
Cragg, John G., and Stephen G. Donald. 1996. On the asymptotic proper-
ties of LDU-based tests of the rank of a matrix. Journal of the American
Statistical Association 91:1301–1309.
Cullen, M. R. 1985. Linear Models in Biology. New York: Halsted Press.
Dauger, Dean E., and Viktor K. Decyk. 2005. Plug-and-play cluster com-
puting: High-performance computing for the mainstream. Computing in
Science and Engineering 07(2):27–33.
Davies, Philip I., and Nicholas J. Higham. 2000. Numerically stable generation
of correlation matrices and their factors. BIT 40:640–651.
Dempster, Arthur P., and Donald B. Rubin. 1983. Rounding error in regres-
sion: The appropriateness of Sheppard’s corrections. Journal of the Royal
Statistical Society, Series B 39:1–38.
Devlin, Susan J., R. Gnanadesikan, and J. R. Kettenring. 1975. Robust estima-
tion and outlier detection with correlation coeﬃcients. Biometrika 62:531–
546.

624
Bibliography
Dey, Aloke, and Rahul Mukerjee. 1999. Fractional Factorial Plans. New York:
John Wiley and Sons.
Dongarra, J. J., J. R. Bunch, C. B. Moler, and G. W. Stewart. 1979. LINPACK
Users’ Guide. Philadelphia: Society for Industrial and Applied Mathemat-
ics.
Dongarra, J. J., J. DuCroz, S. Hammarling, and I. Duﬀ. 1990. A set of level 3
basic linear algebra subprograms. ACM Transactions on Mathematical Soft-
ware 16:1–17.
Dongarra, J. J., J. DuCroz, S. Hammarling, and R. J. Hanson. 1988. An ex-
tended set of Fortran basic linear algebra subprograms. ACM Transactions
on Mathematical Software 14:1–17.
Dongarra, Jack J., and Victor Eijkhout. 2000. Numerical linear algebra algo-
rithms and software. Journal of Computational and Applied Mathematics
123:489–514.
Draper, Norman R., and Harry Smith. 1998. Applied Regression Analysis, 3rd
ed. New York: John Wiley and Sons.
Duﬀ, Iain S., Michael A. Heroux, and Roldan Pozo. 2002. An overview of the
sparse basic linear algebra subprograms: the new standard from the BLAS
technical forum. ACM Transactions on Mathematical Software 28:239–267.
Duﬀ, Iain S., Michele Marrone, Guideppe Radicati, and Carlo Vittoli. 1997.
Level 3 basic linear algebra subprograms for sparse matrices: A user-level
interface. ACM Transactions on Mathematical Software 23:379–401.
Duﬀ, Iain S., and Christof V¨omel. 2002. Algorithm 818: A reference model
implementation of the sparse BLAS in Fortran 95. ACM Transactions on
Mathematical Software 28:268–283.
Eckart, Carl, and Gale Young. 1936. The approximation of one matrix by
another of lower rank. Psychometrika 1:211–218.
Eddelbuettel, Dirk. 2013. Seamless R and C++ Integration with Rcpp. New
York: Springer-Verlag.
Ericksen, Wilhelm S. 1985. Inverse pairs of test matrices. ACM Transactions
on Mathematical Software 11:302–304.
Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004.
Least angle regression. The Annals of Statistics 32:407–499.
Escobar, Luis A., and E. Barry Moser. 1993. A note on the updating of re-
gression estimates. The American Statistician 47:192–194.
Eskow, Elizabeth, and Robert B. Schnabel. 1991. Algorithm 695: Software for
a new modiﬁed Cholesky factorization. ACM Transactions on Mathematical
Software 17:306–312.
Eubank, Randall L., and Ana Kupresanin. 2012. Statistical Computing in
C++ and R. Boca Raton: Chapman and Hall/CRC Press.
Fasino, Dario, and Luca Gemignani. 2003. A Lanczos-type algorithm for the
QR factorization of Cauchy-like matrices. In Fast Algorithms for Struc-
tured Matrices: Theory and Applications, ed. Vadim Olshevsky, 91–104.
Providence, Rhode Island: American Mathematical Society.

Bibliography
625
Filippone, Salvatore, and Michele Colajanni. 2000. PSBLAS: A library for
parallel linear algebra computation on sparse matrices. ACM Transactions
on Mathematical Software 26:527–550.
Fuller, Wayne A. 1995. Introduction to Statistical Time Series, 2nd ed. New
York: John Wiley and Sons.
Galassi, Mark, Jim Davies, James Theiler, Brian Gough, Gerard Jungman,
Michael Booth, and Fabrice Rossi. 2002. GNU Scientiﬁc Library Reference
Manual, 2nd ed. Bristol, United Kingdom: Network Theory Limited.
Gandrud, Christopher. 2015. Reproducible Research with R and R Studio,
2nd ed. Boca Raton: Chapman and Hall/CRC Press.
Gantmacher, F. R. 1959. The Theory of Matrices, Volumes I and II, translated
by K. A. Hirsch, Chelsea, New York.
Geist, Al, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek,
and Vaidy Sunderam. 1994. PVM. Parallel Virtual Machine. A Users’ Guide
and Tutorial for Networked Parallel Computing. Cambridge, Massachusetts:
The MIT Press.
Gentle, James E. 2003. Random Number Generation and Monte Carlo Meth-
ods, 2nd ed. New York: Springer-Verlag.
Gentle, James E. 2009. Computational Statistics. New York: Springer-Verlag.
Gentleman, W. M. 1974. Algorithm AS 75: Basic procedures for large, sparse
or weighted linear least squares problems. Applied Statistics 23:448–454.
Gill, Len, and Arthur Lewbel. 1992. Testing the rank and deﬁniteness of esti-
mated matrices with applications to factor, state-space and ARMA models.
Journal of the American Statistical Association 87:766–776.
Golub, G., and W. Kahan. 1965. Calculating the singular values and pseudo-
inverse of a matrix. SIAM Journal of Numerical Analysis, Series B 2:205–
224.
Golub, G. H., and C. Reinsch. 1970. Singular value decomposition and least
squares solutions. Numerische Mathematik 14:403–420.
Golub, G. H., and C. F. Van Loan. 1980. An analysis of the total least squares
problem. SIAM Journal of Numerical Analysis 17:883–893.
Golub, Gene H., and Charles F. Van Loan. 1996. Matrix Computations, 3rd
ed. Baltimore: The Johns Hopkins Press.
Graybill, Franklin A. 1983. Introduction to Matrices with Applications in
Statistics, 2nd ed. Belmont, California: Wadsworth Publishing Company.
Greenbaum, Anne, and Zdenˇek Strakoˇs. 1992. Predicting the behavior of ﬁnite
precision Lanczos and conjugate gradient computations. SIAM Journal for
Matrix Analysis and Applications 13:121–137.
Gregory, Robert T., and David L. Karney. 1969. A Collection of Matrices for
Testing Computational Algorithms. New York: John Wiley and Sons.
Gregory, R. T., and E. V. Krishnamurthy. 1984. Methods and Applications of
Error-Free Computation. New York: Springer-Verlag.
Grewal, Mohinder S., and Angus P. Andrews. 1993. Kalman Filtering Theory
and Practice. Englewood Cliﬀs, New Jersey: Prentice-Hall.

626
Bibliography
Griva, Igor, Stephen G. Nash, and Ariela Sofer. 2009. Linear and Nonlin-
ear Optimization, 2nd ed. Philadelphia: Society for Industrial and Applied
Mathematics.
Gropp, William D. 2005. Issues in accurate and reliable use of parallel com-
puting in numerical programs. In Accuracy and Reliability in Scientiﬁc
Computing, ed. Bo Einarsson, 253–263. Philadelphia: Society for Industrial
and Applied Mathematics.
Gropp, William, Ewing Lusk, and Anthony Skjellum. 2014. Using MPI:
Portable Parallel Programming with the Message-Passing Interface, 3rd ed.
Cambridge, Massachusetts: The MIT Press.
Gropp, William, Ewing Lusk, and Thomas Sterling (Editors). 2003. Beowulf
Cluster Computing with Linux, 2nd ed. Cambridge, Massachusetts: The
MIT Press.
Haag, J. B., and D. S. Watkins. 1993. QR-like algorithms for the nonsymmetric
eigenvalue problem. ACM Transactions on Mathematical Software 19:407–
418.
Hager, W. W. 1984. Condition estimates. SIAM Journal on Scientiﬁc and
Statistical Computing 5:311–316.
Hanson, Richard J., and Tim Hopkins. 2013. Numerical Computing with Mod-
ern Fortran. Philadelphia: Society for Industrial and Applied Mathematics.
Harville, David A. 1997. Matrix Algebra from a Statistician’s Point of View.
New York: Springer-Verlag.
Heath, M. T., E. Ng, and B. W. Peyton. 1991. Parallel algorithms for sparse
linear systems. SIAM Review 33:420–460.
Hedayat, A. S., N. J. A. Sloane, and John Stufken. 1999. Orthogonal Arrays:
Theory and Applications. New York: Springer-Verlag.
Heiberger, Richard M. 1978. Algorithm AS127: Generation of random orthog-
onal matrices. Applied Statistics 27:199–205.
Heroux, Michael A. 2015. Editorial: ACM TOMS replicated computational
results initiative. ACM Transactions on Mathematical Software 41:Article
No. 13.
Higham, Nicholas J. 1987. A survey of condition number estimation for tri-
angular matrices. SIAM Review 29:575–596.
Higham, Nicholas J. 1988. FORTRAN codes for estimating the one-norm of
a real or complex matrix, with applications to condition estimation. ACM
Transactions on Mathematical Software 14:381–386.
Higham, Nicholas J. 1990. Experience with a matrix norm estimator. SIAM
Journal on Scientiﬁc and Statistical Computing 11:804–809.
Higham, Nicholas J. 1991. Algorithm 694: A collection of test matrices in
Matlab. ACM Transactions on Mathematical Software 17:289–305.
Higham, Nicholas J. 1997. Stability of the diagonal pivoting method with
partial pivoting. SIAM Journal of Matrix Analysis and Applications 18:52–
65.
Higham, Nicholas J. 2002. Accuracy and Stability of Numerical Algorithms,
2nd ed. Philadelphia: Society for Industrial and Applied Mathematics.

Bibliography
627
Higham, Nicholas J. 2008. Functions of Matrices. Theory and Computation.
Philadelphia: Society for Industrial and Applied Mathematics.
Hill, Francis S., Jr., and Stephen M Kelley. 2006. Computer Graphics Using
OpenGL, 3rd ed. New York: Pearson Education.
Hoﬀman, A. J., and H. W. Wielandt. 1953. The variation of the spectrum of
a normal matrix. Duke Mathematical Journal 20:37–39.
Hong, H. P., and C. T. Pan. 1992. Rank-revealing QR factorization and SV D.
Mathematics of Computation 58:213–232.
Horn, Roger A., and Charles R. Johnson. 1991. Topics in Matrix Analysis.
Cambridge, United Kingdom: Cambridge University Press.
IEEE. 2008. IEEE Standard for Floating-Point Arithmetic, Std 754-2008. New
York: IEEE, Inc.
Jansen, Paul, and Peter Weidner. 1986. High-accuracy arithmetic software —
some tests of the ACRITH problem-solving routines. ACM Transactions on
Mathematical Software 12:62–70.
Jaulin, Luc, Michel Kieﬀer, Olivier Didrit, and Eric Walter. (2001). Applied
Interval Analysis. New York: Springer.
Jolliﬀe, I. T. 2002. Principal Component Analysis, 2nd ed. New York:
Springer-Verlag.
Karau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015.
Learning Spark. Sabastopol, California: O’Reilly Media, Inc.
Kearfott, R. Baker. 1996. Interval arithmetic: A Fortran 90 module for an
interval data type. ACM Transactions on Mathematical Software 22:385–
392.
Kearfott, R. Baker, and Vladik Kreinovich (Editors). 1996. Applications of
Interval Computations. Netherlands: Kluwer, Dordrecht.
Kearfott, R. B., M. Dawande, K. Du, and C. Hu. 1994. Algorithm 737:
INTLIB: A portable Fortran 77 interval standard-function library. ACM
Transactions on Mathematical Software 20:447–459.
Keller-McNulty, Sallie, and W. J. Kennedy. 1986. An error-free generalized
matrix inversion and linear least squares method based on bordering. Com-
munications in Statistics — Simulation and Computation 15:769–785.
Kennedy, William J., and James E. Gentle. 1980. Statistical Computing. New
York: Marcel Dekker, Inc.
Kenney, C. S., and A. J. Laub. 1994. Small-sample statistical condition esti-
mates for general matrix functions. SIAM Journal on Scientiﬁc Computing
15:191–209.
Kenney, C. S., A. J. Laub, and M. S. Reese. 1998. Statistical condition estima-
tion for linear systems. SIAM Journal on Scientiﬁc Computing 19:566–583.
Kim, Hyunsoo, and Haesun Park. 2008. Nonnegative matrix factorization
based on alternating non-negativity-constrained least squares and the active
set method. SIAM Journal on Matrix Analysis and Applications 30:713–730.
Kleibergen, Frank, and Richard Paap. 2006. Generalized reduced rank tests
using the singular value decomposition. Journal of Econometrics 133:97–
126.

628
Bibliography
Kollo, T˜onu, and Dietrich von Rosen. 2005. Advanced Multivariate Statistics
with Matrices. Amsterdam: Springer.
Kshemkalyani, Ajay D., and Mukesh Singhal. 2011. Distributed Computing:
Principles, Algorithms, and Systems. Cambridge, United Kingdom: Cam-
bridge University Press.
Kulisch, Ulrich. 2011. Very fast and exact accumulation of products. Com-
puting 91:397–405.
Lawson, C. L., R. J. Hanson, D. R. Kincaid, and F. T. Krogh. 1979. Basic
linear algebra subprograms for Fortran usage. ACM Transactions on Math-
ematical Software 5:308–323.
Lee, Daniel D., and H. Sebastian Seung. 2001. Algorithms for non-negative
matrix factorization. Advances in Neural Information Processing Systems,
556–562. Cambridge, Massachusetts: The MIT Press.
Lemmon, David R., and Joseph L. Schafer. 2005. Developing Statistical Soft-
ware in Fortran 95. New York: Springer-Verlag.
Leskovec, Jure, Anand Rajaraman, and Jeﬀrey David Ullman. 2014. Min-
ing of Massive Datasets, 2nd ed. Cambridge, United Kingdom: Cambridge
University Press.
Levesque, John, and Gene Wagenbreth. 2010. High Performance Comput-
ing: Programming and Applications. Boca Raton: Chapman and Hall/CRC
Press.
Liem, C. B., T. L¨u, and T. M. Shih. 1995. The Splitting Extrapolation
Method. Singapore: World Scientiﬁc.
Linnainmaa, Seppo. 1975. Towards accurate statistical estimation of rounding
errors in ﬂoating-point computations. BIT 15:165–173.
Liu, Shuangzhe and Heinz Neudecker. 1996. Several matrix Kantorovich-type
inequalities. Journal of Mathematical Analysis and Applications 197:23–26.
Loader, Catherine. 2012. Smoothing: Local regression techniques. In Hand-
book of Computational Statistics: Concepts and Methods, 2nd revised and
updated ed., ed. James E. Gentle, Wolfgang H¨ardle, and Yuichi Mori, 571–
596. Berlin: Springer.
Longley, James W. 1967. An appraisal of least squares problems for the elec-
tronic computer from the point of view of the user. Journal of the American
Statistical Association 62:819–841.
Luk, F. T., and H. Park. 1989. On parallel Jacobi orderings. SIAM Journal
on Scientiﬁc and Statistical Computing 10:18–26.
Magnus, Jan R., and Heinz Neudecker. 1999. Matrix Diﬀerential Calculus
with Applications in Statistics and Econometrics, revised ed. New York:
John Wiley and Sons.
Markus, Arjen. 2012. Modern Fortran in Practice. Cambridge, United King-
dom: Cambridge University Press.
Marshall, A. W., and I. Olkin. 1990. Matrix versions of the Cauchy and Kan-
torovich inequalities. Aequationes Mathematicae 40:89–93.
Metcalf, Michael, John Reid, and Malcolm Cohen. 2011. Modern Fortran Ex-
plained. Oxford, United Kingdom: Oxford University Press.

Bibliography
629
Meyn, Sean, and Richard L. Tweedie. 2009. Markov Chains and Stochas-
tic Stability, 2nd ed. Cambridge, United Kingdom: Cambridge University
Press.
Miller, Alan J. 1992. Algorithm AS 274: Least squares routines to supplement
those of Gentleman. Applied Statistics 41:458–478 (Corrections, 1994, ibid.
43:678).
Miller, Alan. 2002. Subset Selection in Regression, 2nd ed. Boca Raton: Chap-
man and Hall/CRC Press.
Miller, Alan J., and Nam-Ky Nguyen. 1994. A Fedorov exchange algorithm
for D-optimal design. Applied Statistics 43:669–678.
Mizuta, Masahiro. 2012. Dimension reduction methods. In Handbook of Com-
putational Statistics: Concepts and Methods, 2nd revised and updated ed.,
ed. James E. Gentle, Wolfgang H¨ardle, and Yuichi Mori, 619–644. Berlin:
Springer.
Moore, E. H. 1920. On the reciprocal of the general algebraic matrix. Bulletin
of the American Mathematical Society 26:394–395.
Moore, Ramon E. (1979). Methods and Applications of Interval Analysis.
Philadelphia: Society for Industrial and Applied Mathematics.
Mosteller, Frederick, and David L. Wallace. 1963. Inference in an authorship
problem. Journal of the American Statistical Association 58:275–309.
Muirhead, Robb J. 1982. Aspects of Multivariate Statistical Theory. New
York: John Wiley and Sons.
Mullet, Gary M., and Tracy W. Murray. 1971. A new method for examining
rounding error in least-squares regression computer programs. Journal of
the American Statistical Association 66:496–498.
Nachbin, Leopoldo. 1965. The Haar Integral, translated by Lulu Bechtolsheim.
Princeton, New Jersey: D. Van Nostrand Co Inc.
Nakano, Junji. 2012. Parallel computing techniques. In Handbook of Compu-
tational Statistics: Concepts and Methods, 2nd revised and updated ed.,
ed. James E. Gentle, Wolfgang H¨ardle, and Yuichi Mori, 243–272. Berlin:
Springer.
Nguyen, Nam-Ky, and Alan J. Miller. 1992. A review of some exchange al-
gorithms for constructing D-optimal designs. Computational Statistics and
Data Analysis 14:489–498.
Olshevsky, Vadim (Editor). 2003. Fast Algorithms for Structured Matrices:
Theory and Applications. Providence, Rhode Island: American Mathemat-
ical Society.
Olver, Frank W. J., Daniel w. Lozier, Ronald F. Boisvert, and Charles W.
Clark. 2010. NIST Handbook of Mathematical Functions. Cambridge: Cam-
bridge University Press.
Overton, Michael L. 2001. Numerical Computing with IEEE Floating Point
Arithmetic. Philadelphia: Society for Industrial and Applied Mathematics.
Parsian, Mahmoud. 2015. Data Algorithms. Sabastopol, California: O’Reilly
Media, Inc.

630
Bibliography
Penrose, R. 1955. A generalized inverse for matrices. Proceedings of the Cam-
bridge Philosophical Society 51:406–413.
Quinn, Michael J. 2003. Parallel Programming in C with MPI and OpenMP.
New York: McGraw-Hill.
Rice, John R. 1966. Experiments on Gram-Schmidt orthogonalization. Math-
ematics of Computation 20:325–328.
Rice, John R. 1993. Numerical Methods, Software, and Analysis, 2nd ed. New
York: McGraw-Hill Book Company.
Robin, J. M., and R. J. Smith. 2000. Tests of rank. Econometric Theory
16:151–175.
Roosta, Seyed H. 2000. Parallel Processing and Parallel Algorithms: Theory
and Computation. New York: Springer-Verlag.
Rousseeuw, Peter J., and Geert Molenberghs. 1993. Transformation of non-
positive semideﬁnite correlation matrices. Communications in Statistics —
Theory and Methods 22:965–984.
Rust, Bert W. 1994. Perturbation bounds for linear regression problems. Com-
puting Science and Statistics 26:528–532.
Saad, Y., and M. H. Schultz. 1986. GMRES: A generalized minimal resid-
ual algorithm for solving nonsymmetric linear systems. SIAM Journal on
Scientiﬁc and Statistical Computing 7:856–869.
Schott, James R. 2004. Matrix Analysis for Statistics, 2nd ed. New York: John
Wiley and Sons.
Searle, S. R. 1971. Linear Models. New York: John Wiley and Sons.
Searle, Shayle R. 1982. Matrix Algebra Useful for Statistics. New York: John
Wiley and Sons.
Shao, Jun. 2003. Mathematical Statistics, 2nd ed. New York: Springer-Verlag.
Sherman, J., and W. J. Morrison. 1950. Adjustment of an inverse matrix
corresponding to a change in one element of a given matrix. Annals of
Mathematical Statistics 21:124–127.
Siek, Jeremy, and Andrew Lumsdaine. 2000. A modern framework for portable
high-performance numerical linear algebra. In Advances in Software Tools
for Scientiﬁc Computing, ed. Are Bruaset, H. Langtangen, and E. Quak,
1–56. New York: Springer-Verlag.
Skeel, R. D. 1980. Iterative reﬁnement implies numerical stability for Gaussian
elimination. Mathematics of Computation 35:817–832.
Smith, B. T., J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C.
Klema, and C. B. Moler. 1976. Matrix Eigensystem Routines — EISPACK
Guide. Berlin: Springer-Verlag.
Stallings, W. T., and T. L. Boullion. 1972. Computation of pseudo-inverse
using residue arithmetic. SIAM Review 14:152–163.
Stewart, G. W. 1980. The eﬃcient generation of random orthogonal matrices
with an application to condition estimators. SIAM Journal of Numerical
Analysis 17:403–409.
Stewart, G. W. 1990. Stochastic perturbation theory. SIAM Review 32:579–
610.

Bibliography
631
Stodden, Victoria, Friedrich Leisch, and Roger D. Peng. 2014. Implementing
Reproducible Research. Boca Raton: Chapman and Hall/CRC Press.
Strang, Gilbert, and Tri Nguyen. 2004. The interplay of ranks of submatrices.
SIAM Review 46:637–646.
Strassen, V. 1969, Gaussian elimination is not optimal. Numerische Mathe-
matik 13:354–356.
Szab´o, S., and R. Tanaka. 1967. Residue Arithmetic and Its Application to
Computer Technology. New York: McGraw-Hill.
Tanner, M. A., and R. A. Thisted. 1982. A remark on AS127. Generation of
random orthogonal matrices. Applied Statistics 31:190–192.
Titterington, D. M. 1975. Optimal design: Some geometrical aspects of D-
optimality. Biometrika 62:313–320.
Trefethen, Lloyd N., and Mark Embree. 2005. Spectra and Pseudospectra:
The Behavior of Nonnormal Matrices and Operators. Princeton: Princeton
University Press.
Trefethen, Lloyd N., and David Bau III. 1997. Numerical Linear Algebra.
Philadelphia: Society for Industrial and Applied Mathematics.
Trosset, Michael W. 2002. Extensions of classical multidimensional scaling via
variable reduction. Computational Statistics 17:147–163.
Unicode Consortium. 1990. The Unicode Standard, Worldwide Character En-
coding, Version 1.0, Volume 1. Reading, Massachusetts: Addison-Wesley
Publishing Company.
Unicode Consortium. 1992. The Unicode Standard, Worldwide Character En-
coding, Version 1.0, Volume 2. Reading, Massachusetts: Addison-Wesley
Publishing Company.
Vandenberghe, Lieven, and Stephen Boyd. 1996. Semideﬁnite programming.
SIAM Review 38:49–95.
Venables, W. N., and B. D. Ripley. 2003. Modern Applied Statistics with S,
4th ed. New York: Springer-Verlag.
Walker, Homer F. 1988. Implementation of the GMRES method using House-
holder transformations. SIAM Journal on Scientiﬁc and Statistical Com-
puting 9:152–163.
Walker, Homer F., and Lu Zhou. 1994. A simpler GMRES. Numerical Linear
Algebra with Applications 1:571–581.
Walster, G. William. 1996. Stimulating hardware and software support for
interval arithmetic. In Applications of Interval Computations, ed. R. Baker
Kearfott and Vladik Kreinovich, 405–416. Dordrecht, Netherlands: Kluwer.
Walster, G. William. 2005. The use and implementation of interval data types.
In Accuracy and Reliability in Scientiﬁc Computing, ed. Bo Einarsson, 173–
194. Philadelphia: Society for Industrial and Applied Mathematics.
Watkins, David S. 2002. Fundamentals of Matrix Computations, 2nd ed. New
York: John Wiley and Sons.
White, Tom. 2015. Hadoop: The Deﬁnitive Guide, 4th ed. Sabastopol, Cali-
fornia: O’Reilly Media, Inc.

632
Bibliography
Wickham, Hadley. 2015) Advanced R. Boca Raton: Chapman and Hall/CRC
Press.
Wilkinson, J. H. 1959. The evaluation of the zeros of ill-conditioned polyno-
mials. Numerische Mathematik 1:150–180.
Wilkinson, J. H. 1963. Rounding Errors in Algebraic Processes. Englewood
Cliﬀs, New Jersey: Prentice-Hall. (Reprinted by Dover Publications, Inc.,
New York, 1994).
Wilkinson, J. H. 1965. The Algebraic Eigenvalue Problem. New York: Oxford
University Press.
Woodbury, M. A. 1950. “Inverting Modiﬁed Matrices”, Memorandum Report
42, Statistical Research Group, Princeton University.
Wynn, P. 1962. Acceleration techniques for iterated vector and matrix prob-
lems. Mathematics of Computation 16:301–322.
Xie, Yihui. 2015. Dynamic Documents with R and knitr, 2nd ed. Boca Raton:
Chapman and Hall/CRC Press.
Zhou, Bing Bing, and Richard P. Brent. 2003. An eﬃcient method for comput-
ing eigenvalues of a real normal matrix. Journal of Parallel and Distributed
Computing 63:638–648.

Index
A
A-optimality, 441
absolute error, 486, 496, 528
ACM Transactions on Mathematical
Software, 619
ACM Transactions on Mathematical
Software, 554
adj(·), 69
adjacency matrix, 334–336, 393
Exercise 8.20:, 398
augmented, 393
adjoint (see also conjugate transpose),
59
adjoint, classical (see also adjugate), 69
adjugate, 69, 600
adjugate and inverse, 118
aﬃne group, 115, 179
aﬃne space, 43
aﬃne transformation, 230
Aitken’s integral, 219
AL(·) (aﬃne group), 115
algebraic multiplicity, 144
algorithm, 266, 501–515
batch, 514
deﬁnition, 511
direct, 266
divide and conquer, 507
greedy, 508
iterative, 266, 510–512, 566
reverse communication, 566
online, 514
out-of-core, 514
real-time, 514
algorithmically singular, 121
Anaconda, 555, 558, 571
angle(·, ·), 37
angle between matrices, 168
angle between vectors, 37, 231, 359
ANSI (standards), 477, 564, 565
Applied Statistics algorithms, 619
approximation and estimation, 433
approximation of a matrix, 175, 259,
341, 439, 535
approximation of a vector, 41–43
arithmetic mean, 35, 37
Arnoldi method, 321
artiﬁcial ill-conditioning, 271
ASCII code, 462
association matrix, 330–340, 359, 367,
368, 371–373
adjacency matrix, 334, 393
connectivity matrix, 334, 393
dissimilarity matrix, 371
distance matrix, 371
incidence matrix, 334, 393
similarity matrix, 371
ATLAS (Automatically Tuned Linear
Algebra Software), 557
augmented adjacency matrix, 393
augmented connectivity matrix, 393
Automatically Tuned Linear Algebra
Software (ATLAS), 557
autoregressive process, 449–452
© Springer International Publishing AG 2017
J.E. Gentle, Matrix Algebra, Springer Texts in Statistics,
DOI 10.1007/978-3-319-64867-5
633

634
Index
axpy, 12, 50, 83, 556, 557
axpy elementary operator matrix, 83
B
back substitution, 276, 408
backward error analysis, 496, 502
Banach space, 33
Banachiewicz factorization, 257
banded matrix, 58
inverse, 121
Bartlett decomposition, 348
base, 469
base point, 468
basis, 21–23
Exercise 2.6:, 52
orthonormal, 40–41
batch algorithm, 514
Bauer-Fike theorem, 309
Beowulf (cluster computing), 561
bias, in exponent of ﬂoating-point
number, 470
big endian, 482
big integer, 468, 494
big O (order), 499, 505, 593
big omega (order), 499
bilinear form, 91, 134
bit, 462
bitmap, 463
BLACS (software), 559, 560
BLAS (software), 555–558
CUDA, 562
PBLAS, 560
PLASMA, 561
PSBLAS, 560
block diagonal matrix, 62
determinant of, 71
inverse of, 121
multiplication, 79
BMvN distribution, 221, 550
Bolzano-Weierstrass theorem for
orthogonal matrices, 133
Boolean matrix, 393
Box M statistic, 370
bra·ket notation, 24
byte, 462
C
C (programming language), 476, 491,
570–571
C++ (programming language), 477,
570–571
CALGO (Collected Algorithms of the
ACM), 619
cancellation error, 489, 502
canonical form, equivalent, 110
canonical form, similar, 149
canonical singular value factorization,
162
Cartesian geometry, 35, 74
catastrophic cancellation, 488
Cauchy matrix, 391
Cauchy-Schwarz inequality, 24, 98
Cauchy-Schwarz inequality for matrices,
98, 178
Cayley multiplication, 75, 94
Cayley-Hamilton theorem, 138
CDF (Common Data Format), 465
centered matrix, 290, 366
centered vector, 49
chaining of operations, 487
character data, 463
character string, 463
characteristic equation, 138
characteristic polynomial, 138
characteristic value (see also eigenvalue),
135
characteristic vector (see also eigenvec-
tor), 135
chasing, 319
Chebyshev norm, 28
chi-squared distribution, 402
noncentral, 402
PDF, equation (9.3), 402
Cholesky decomposition, 255–258, 347,
439
computing, 560, 577
root-free, 257
circulant matrix, 386
classiﬁcation, 392
cluster analysis, 392
cluster computing, 561
coarray (Fortran construct), 565
Cochran’s theorem, 355–358, 403
cofactor, 68, 600
Collected Algorithms of the ACM
(CALGO), 619
collinearity, 267, 407, 432
column rank, 100

Index
635
column space, 55, 90, 105
column-major, 524, 541, 547
column-sum norm, 166
Common Data Format (CDF), 465
companion matrix, 139, 307
compatible linear systems, 106
compensated summation, 487
complementary projection matrix, 358
complementary vector spaces, 19
complete graph, 331
complete pivoting, 278
complete space, 33
completing the Gramian, 177
complex data type, 492
complex vectors/matrices, 33, 132, 389
Conda, 555
condition (problem or data), 501
condition number, 267, 273, 292, 428,
429, 501, 504, 525, 535
computing the number, 535, 577
inverse of matrix, 269, 273
nonfull rank matrices, 292
nonsquare matrices, 292
sample standard deviation, 504
conditional inverse, 128
cone, 43–46, 329
Exercise 2.18:, 53
convex cone, 44, 348
of nonnegative deﬁnite matrices, 348
of nonnegative matrices, 373
of positive deﬁnite matrices, 351
of positive matrices, 373
conference matrix, 384
conﬁguration matrix, 372
conjugate gradient method, 281–285
preconditioning, 284
conjugate norm, 94
conjugate transpose, 59, 132
conjugate vectors, 94, 134
connected vertices, 331, 336
connectivity matrix, 334–336, 393
Exercise 8.20:, 398
augmented, 393
consistency property of matrix norms,
164
consistency test, 529, 553
consistent system of equations, 105, 274,
279
constrained least squares, equality
constraints, 415
Exercise 9.4d:, 453
continuous function, 188
contrast, 412
convergence criterion, 510
convergence of a sequence of matrices,
133, 152, 171
convergence of a sequence of vectors,
32
convergence of powers of a matrix, 172,
378
convergence rate, 511
convex combination, 12
convex cone, 44–46, 348, 351, 373
convex function, 26, 199
convex optimization, 348
convex set, 12
convexity, 26
coordinate, 5
Cor(·, ·), 51
correlation, 51, 90
correlation matrix, 368, 424
Exercise 8.8:, 397
positive deﬁnite approximation, 438
pseudo-correlation matrix, 439
sample, 424
cost matrix, 372
Cov(·, ·), 50
covariance, 50
covariance matrix, see variance-
covariance matrix
CRAN (Comprehensive R Archive
Network), 554, 578
cross product of vectors, 47
Exercise 2.19:, 54
cross products matrix, 258, 360
cross products, computing sum of
Exercise 10.18c:, 520
Crout method, 247
cuBLAS, 562
CUDA, 561
curl, 194
curse of dimensionality, 512
cuSPARSE, 562
D
D-optimality, 441–443, 535
daxpy, 12

636
Index
decomposable matrix, 375
decomposition, see also factorization of
a matrix
additive, 356
Bartlett decomposition, 348
multiplicative, 109
nonnegative matrix factorization,
259, 339
singular value decomposition,
161–164, 322, 339, 427, 534
spectral decomposition, 155
defective (deﬁcient) matrix, 149, 150
deﬁcient (defective) matrix, 149, 150
deﬂation, 310–312
degrees of freedom, 363, 364, 409, 432
del, 194
derivative with respect to a matrix,
196–197
derivative with respect to a vector,
191–196
det(·), 66
determinant, 66–75
as criterion for optimal design, 441
computing, 535
derivative of, 197
Jacobian, 219
of block diagonal matrix, 71
of Cayley product, 88
of diagonal matrix, 71
of elementary operator matrix, 86
of inverse, 117
of Kronecker product, 96
of nonnegative deﬁnite matrix, 347
of partitioned matrix, 71, 122
of permutation matrix, 87
of positive deﬁnite matrix, 349
of transpose, 70
of triangular matrix, 70
relation to eigenvalues, 141
relation to geometric volume, 74, 219
diag(·), 56, 60, 598
with matrix arguments, 62
diagonal element, 56
diagonal expansion, 73
diagonal factorization, 148, 152
diagonal matrix, 57
determinant of, 71
inverse of, 120
multiplication, 79
diagonalizable matrix, 148–152, 308, 346
orthogonally, 154
unitarily, 147, 346, 389
diagonally dominant matrix, 57, 62, 101,
350
diﬀerential, 190
diﬀerentiation of vectors and matrices,
185–222
digraph, 335
of a matrix, 335
dim(·), 15
dimension of vector space, 14
dimension reduction, 20, 358, 428
direct method for solving linear systems,
274–279
direct product (of matrices), 95
direct product (of sets), 5
direct product (of vector spaces), 20
basis for, 23
direct sum decomposition of a vector
space, 19
direct sum of matrices, 63
direct sum of vector spaces, 18–20, 64
basis for, 22
direct sum decomposition, 19
directed dissimilarity matrix, 372
direction cosines, 38, 233
discrete Fourier transform, 387
discrete Legendre polynomials, 382
discretization error, 500, 511
dissimilarity matrix, 371, 372
distance, 32
between matrices, 175
between vectors, 32
distance matrix, 371, 372
distributed computing, 465, 510, 546
distributed linear algebra machine, 560
distribution vector, 380
div, 194
divergence, 194
divide and conquer, 507
document-term matrix, 338
dominant eigenvalue, 142
Doolittle method, 247
dot product of matrices, 97
dot product of vectors, 23, 91
double cone, 43
double precision, 474, 482
doubly stochastic matrix, 379

Index
637
Drazin inverse, 129–130
dual cone, 44
E
Epq, E(π), Ep(a), Epq(a) (elementary
operator matrices), 85
E(·) (expectation operator), 218
E-optimality, 441
echelon form, 111
edge of a graph, 331
EDP (exact dot product), 495
eﬀective degrees of freedom, 364, 432
eﬃciency, computational, 504–510
eigenpair, 134
eigenspace, 144
eigenvalue, 134–164, 166, 307–324
computing, 308–321, 560, 577
Jacobi method, 315–318
Krylov methods, 321
power method, 313–315
QR method, 318–320
of a graph, 394
of a polynomial Exercise 3.26:, 181
relation to singular value, 163
upper bound on, 142, 145, 308
eigenvector, 134–164, 307–324
left eigenvector, 135, 158
eigenvectors, linear independence of,
143
EISPACK, 558
elementary operation, 80
elementary operator matrix, 80–87, 101,
244, 275
eigenvalues, 137
elliptic metric, 94
elliptic norm, 94
endian, 482
equivalence of norms, 29, 33, 170
equivalence relation, 446
equivalent canonical factorization, 112
equivalent canonical form, 110, 112
equivalent matrices, 110
error bound, 498
error in computations
cancellation, 489, 502
error-free computations, 495
measures of, 486, 496–499, 528
rounding, 489, 496, 497
Exercise 10.10:, 519
error of approximation, 500
discretization, 500
truncation, 500
error, measures of, 274, 486, 496–499,
528
error-free computations, 495
errors-in-variables, 407
essentially disjoint vector spaces, 15,
64
estimable combinations of parameters,
411
estimation and approximation, 433
Euclidean distance, 32, 371
Euclidean distance matrix, 371
Euclidean matrix norm (see also
Frobenius norm), 167
Euclidean vector norm, 27
Euler’s constant Exercise 10.2:, 517
Euler’s integral, 595
Euler’s rotation theorem, 233
exact computations, 495
exact dot product (EDP), 495
exception, in computer operations, 485,
489
expectation, 214–222
exponent, 469
exponential order, 505
exponential, matrix, 153, 186
extended precision, 474
extrapolation, 511
F
factorization of a matrix, 109, 112, 147,
148, 161, 227–229, 241–261, 274,
276
Banachiewicz factorization, 257
Bartlett decomposition, 348
canonical singular value factorization,
162
Cholesky factorization, 255–258
diagonal factorization, 148
equivalent canonical factorization,
112
full rank factorization, 109, 112
Gaussian elimination, 274
LQ factorization, 249
LU or LDU factorization, 242–248

638
Index
factorization of a matrix (cont.)
nonnegative matrix factorization,
259, 339
orthogonally diagonal factorization,
147
QL factorization, 249
QR factorization, 248–254
root-free Cholesky, 257
RQ factorization, 249
Schur factorization, 147
singular value factorization, 161–164,
322, 339, 427, 534
square root factorization, 160
unitarily diagonal factorization, 147
fan-in algorithm, 487, 508
fast Fourier transform (FFT), 389
fast Givens rotation, 241, 527
ﬁll-in, 261, 528
Fisher information, 207
ﬁxed-point representation, 467
ﬂat, 43
ﬂoating-point representation, 468
FLOP, or ﬂop, 507
FLOPS, or ﬂops, 506
Fortran, 477–480, 507, 548, 568–570
Fourier coeﬃcient, 41, 42, 99, 157, 163,
169
Fourier expansion, 36, 41, 99, 157, 163,
169
Fourier matrix, 387
Frobenius norm, 167–169, 171, 176, 316,
342, 372
Frobenius p norm, 169
full precision, 481
full rank, 101, 104, 111–113
full rank factorization, 109
symmetric matrix, 112
full rank partitioning, 104, 122
G
g1 inverse, 128, 129
g2 inverse, 128
g4 inverse (see also Moore-Penrose
inverse), 128
gamma function, 222, 595
GAMS (Guide to Available Mathemati-
cal Software), 541
Gauss (software), 572
Gauss-Markov theorem, 413
Gauss-Newton method, 205
Gauss-Seidel method, 279
Gaussian elimination, 84, 274, 319
Gaussian matrix, 84, 243
gemm (general matrix-matrix), 558
gemv (general matrix-vector), 558
general linear group, 114, 133
generalized eigenvalue, 160, 321
generalized inverse, 124–125, 127–131,
251, 361
relation to QR factorization, 251
generalized least squares, 416
generalized least squares with equality
constraints Exercise 9.4d:, 453
generalized variance, 368
generating set, 14, 21
of a cone, 44
generation of random numbers, 443
geometric multiplicity, 144
geometry, 35, 74, 229, 233
Gershgorin disks, 145
GitHub, 541
Exercise 12.3:, 583
Givens transformation (rotation),
238–241, 319
QR factorization, 253
GL(·) (general linear group), 114
GMP (software library), 468, 493, 494
Exercise 10.5:, 518
GMRES, 284
GNU Scientiﬁc Library (GSL), 558
GPU (graphical processing unit), 561
graceful underﬂow, 472
gradient, 191
projected gradient, 209
reduced gradient, 209
gradient descent, 199, 201
gradient of a function, 192, 193
gradual underﬂow, 472, 489
Gram-Schmidt transformation, 39, 40,
526
linear least squares, 291
QR factorization, 254
Gramian matrix, 115, 117, 258, 291,
360–362
completing the Gramian, 177
graph of a matrix, 334
graph theory, 331–338, 392
graphical processing unit (GPU), 561

Index
639
greedy algorithm, 508
group, 114, 133
GSL (GNU Scientiﬁc Library), 558
guard digit, 486
H
Haar distribution, 222, 551
Exercise 4.10:, 223
Exercise 8.8:, 397
Haar invariant measure, 222
Hadamard matrix, 382
Hadamard multiplication, 94
Hadamard’s inequality Exercise 5.5:,
262, 608
Hadoop, 466, 546
Hadoop Distributed File System
(HDFS), 466, 516
half precision, 481
Hankel matrix, 390
Hankel norm, 391
hat matrix, 362, 410
HDF, HDF5 (Hierarchical Data
Format), 465
HDFS (Hadoop Distributed File
System), 466, 516
Helmert matrix, 381, 412
Hemes formula, 288, 417
Hermite form, 111
Hermitian matrix, 56, 60
Hessenberg matrix, 59, 319
Hessian matrix, 196
projected Hessian, 209
reduced Hessian, 209
Hessian of a function, 196
hidden bit, 470
Hierarchical Data Format (HDF), 465
high-performance computing, 509
Hilbert matrix, 550
Hilbert space, 33, 168
Hilbert-Schmidt norm (see also
Frobenius norm), 167
Hoﬀman-Wielandt theorem, 342
H¨older norm, 27
H¨older’s inequality Exercise 2.11a:, 52
hollow matrix, 57, 372
homogeneous coordinates, 234
in graphics applications, Exercise 5.2:,
261
homogeneous system of equations, 43,
123
Horner’s method, 514
Householder transformation (reﬂection),
235–238, 252, 320
hyperplane, 43
hypothesis testing, 410
I
idempotent matrix, 352–359
identity matrix, 60, 76
IDL (software), 6, 572
IEC standards, 466
IEEE standards, 466, 489
Standard 754, 474, 482, 489, 495
Standard P1788, 495
IFIP Working Group 2.5, 462, 495
ill-conditioned (problem or data), 266,
429, 501, 525
artiﬁcial, 271
stiﬀdata, 504
ill-posed problem, 121
image data, 463
IMSL Libraries, 558, 562–564
incidence matrix, 334–336, 393
incomplete data, 437–440
incomplete factorization, 260, 528
independence, linear, see linear
independence
independent vertices, 393
index-index-value (sparse matrices), 550
induced matrix norm, 165
inﬁnity, ﬂoating-point representation,
475, 489
inﬁx operator, 491
inner product, 23, 247
inner product of matrices, 97–99
inner product space, 24
inner pseudoinverse, 128
integer representation, 467
integration and expectation, 214–222
integration of vectors and matrices, 215
Intel Math Kernel Library (MKL), 557,
580
intersection graph, 336
intersection of vector spaces, 18
interval arithmetic, 494
invariance property, 229
invariant distribution, 447

640
Index
invariant vector (eigenvector), 135
inverse of a matrix, 107
determinant of, 117
Drazin inverse, 129–130
generalized inverse, 124–125, 127–131
Drazin inverse, 129–130
Moore-Penrose inverse, 127–129
pseudoinverse, 128
Kronecker product, 118
left inverse, 108
Moore-Penrose inverse, 127–129
partitioned matrix, 122
products or sums of matrices, 118
pseudoinverse, 128
right inverse, 108
transpose, 107
triangular matrix, 121
inverse of a vector, 31
IRLS (iteratively reweighted least
squares), 299
irreducible Markov chain, 447
irreducible matrix, 313, 337–338,
375–379, 447
is.na, 475
isnan, is.nan, 475
ISO (standards), 477, 564, 565
isometric matrix, 167
isometric transformation, 229
isotropic transformation, 230
iterative method, 279, 286, 307, 510–512,
527
for solving linear systems, 279–286
iterative reﬁnement, 286
iteratively reweighted least squares, 299
J
Jacobi method for eigenvalues, 315–318
Jacobi transformation (rotation), 238
Jacobian, 193, 219
Jordan block, 78, 139
Jordan decomposition, 151
Jordan form, 78, 111
of nilpotent matrix, 78
K
Kalman ﬁlter, 504
Kantorovich inequality, 352
Karush-Kuhn-Tucker conditions, 212
kind (for data types), 478
Kronecker multiplication, 95–97
inverse, 118
properties, 95
symmetric matrices, 96, 156
diagonalization, 156
Kronecker structure, 221, 421
Krylov method, 283, 321
Krylov space, 283
Kuhn-Tucker conditions, 212
Kulisch accumulator, 495
Kullback-Leibler divergence, 176
L
L1, L2, and L∞norms
of a matrix, 166
of a symmetric matrix, 167
of a vector, 27
relations among, 170
L2 norm of a matrix (see also spectral
norm), 166
Lagrange multiplier, 210, 416
Exercise 9.4a:, 452
Lagrangian function, 210
Lanczos method, 321
LAPACK, 278, 536, 558, 560
LAPACK95, 558
Laplace expansion, 69
Laplace operator (∇2), 601
Laplace operator (∇2), 194
Laplacian matrix, 394
lasso regression, 432
latent root (see also eigenvalue), 135
LAV (least absolute values), 297
LDU factorization, 242–248
leading principal submatrix, 62, 350
least absolute values, 297
least squares, 202–206, 258, 289–297
constrained, 208–213
nonlinear, 204–206
least squares regression, 202
left eigenvector, 135, 158
left inverse, 108
length of a vector, 4, 27, 31
Leslie matrix, 380, 448
Exercise 8.10:, 397
Exercise 9.22:, 458
Levenberg-Marquardt method, 206
leverage, 410
Exercise 9.6:, 453

Index
641
life table, 449
likelihood function, 206
line, 43
linear convergence, 511
linear estimator, 411
linear independence, 12, 99
linear independence of eigenvectors, 143
linear programming, 348
linear regression, 403–424, 428–433
variable selection, 429
LINPACK, 278, 535, 558
Lisp-Stat (software), 572
little endian, 482
little o (order), 499, 593
little omega (order), 499
log order, 505
log-likelihood function, 207
Longley data Exercise 9.10:, 455
loop unrolling, 568
Lorentz cone, 44
lower triangular matrix, 58
Lp norm
of a matrix, 165
of a vector, 27–28, 188
LQ factorization, 249
LR method, 308
LU factorization, 242–248
computing, 560, 577
M
M-matrix, 396
MACHAR, 480
Exercise 10.3(d)i:, 518
machine epsilon, 472
Mahalanobis distance, 94, 367
Manhattan norm, 27
manifold of a matrix, 55
Maple (software), 493, 572
MapReduce, 466, 515, 533, 546
Markov chain, 445–447
Markov chain Monte Carlo (MCMC),
447
Mathematica (software), 493, 572
Matlab (software), 548, 580–582
matrix, 5
matrix derivative, 185–222
matrix exponential, 153, 186
matrix factorization, 109, 112, 147, 148,
161, 227–229, 241–261, 274, 276
matrix function, 152
matrix gradient, 193
matrix inverse, 107
matrix multiplication, 75–99, 530
Cayley, 75, 94
CUDA, 562
Hadamard, 94
inner product, 97–99
Kronecker, 95–97
MapReduce, 533
Strassen algorithm, 531–533
matrix norm, 164–171
orthogonally invariant, 164
matrix normal distribution, 220
matrix of type 2, 58, 385
matrix pencil, 161
matrix polynomial, 78
Exercise 3.26:, 181
matrix random variable, 220–222
matrix storage mode, 548–550
Matrix Template Library, 571
max norm, 28
maximal linearly independent subset,
13
maximum likelihood, 206–208
MCMC (Markov chain Monte Carlo),
447
mean, 35, 37
mean vector, 35
message passing, 559
Message Passing Library, 559
metric, 32, 175
metric space, 32
Microsoft R Open, 580
MIL-STD-1753 standard, 479
Minkowski inequality, 27
Exercise 2.11b:, 52
Minkowski norm, 27
minor, 67, 599
missing data, 437–440, 573
representation of, 464, 475
MKL (Intel Math Kernel Library), 557,
580
mobile Jacobi scheme, 318
modiﬁed Cholesky decomposition, 439
“modiﬁed” Gauss-Newton, 205
“modiﬁed” Gram-Schmidt (see also
Gram-Schmidt transformation), 40

642
Index
Moore-Penrose inverse, 127–129, 250,
251, 294
relation to QR factorization, 251
MPI (message passing interface), 559,
561
MPL (Message Passing Library), 559
multicollinearity, 267, 407
multigrid method, 286
multiple precision, 468, 493
multiplicity of an eigenvalue, 144
multivariate gamma function, 222
multivariate linear regression, 420–424
multivariate normal distribution,
219–221, 401, 443
singular, 219, 435
multivariate random variable, 217–222
N
N(·), 126
NA (“Not Available”), 464, 475, 573
nabla (∇), 192, 193
Nag Libraries, 558
NaN (“Not-a-Number”), 475, 490
NetCDF, 465
netlib, 619
netlib, xiv
network, 331–338, 394
Newton’s method, 200
nilpotent matrix, 77, 174
NMF (nonnegative matrix factoriza-
tion), 259, 339
noncentral chi-squared distribution, 402
PDF, equation (9.3), 402
noncentral Wishart distribution
Exercise 4.12:, 224
nonlinear regression, 202
nonnegative deﬁnite matrix, 92, 159,
255, 346–352
summary of properties, 346–347
nonnegative matrix, 260, 372
nonnegative matrix factorization, 259,
339
nonsingular matrix, 101, 111
norm, 25–30
convexity, 26
equivalence of norms, 29, 33, 170
of a matrix, 164–171
orthogonally invariant, 164
of a vector, 27–31
weighted, 28, 94
normal distribution, 219–221
matrix, 220
multivariate, 219–221
normal equations, 258, 291, 406, 422
normal matrix, 345
Exercise 8.1:, 396
circulant matrix, 386
normal vector, 34
normalized ﬂoating-point numbers, 470
normalized generalized inverse (see also
Moore-Penrose inverse), 128
normalized vector, 31
normed space, 25
not-a-number (“NaN”), 475
NP-complete problem, 505
nuclear norm, 169
null space, 126, 127, 144
nullity, 126
numpy, 558, 571
Nvidia, 561
O
O(·), 499, 505, 593
o(·), 499, 593
oblique projection, 358
Octave (software), 581
OLS (ordinary least squares), 290
one vector, 16, 34
online algorithm, 514
online processing, 514
open-source, 540
OpenMP, 559, 561
operator matrix, 80, 275
operator norm, 165
optimal design, 440–443
optimization of vector/matrix functions,
198–214
constrained, 208–213
least squares, 202–206, 208–213
order of a graph, 331
order of a vector, 4
order of a vector space, 15
order of computations, 505
order of convergence, 499
order of error, 499
ordinal relations among matrices, 92,
350
ordinal relations among vectors, 16

Index
643
orthogonal array, 382
orthogonal basis, 40–41
orthogonal complement, 34, 126, 131
orthogonal distance regression, 301–304,
407
orthogonal group, 133, 222
orthogonal matrices, binary relation-
ship, 98
orthogonal matrix, 131–134, 230
orthogonal residuals, 301–304, 407
orthogonal transformation, 230
orthogonal vector spaces, 34, 131
orthogonal vectors, 33
Exercise 2.6:, 52
orthogonalization (Gram-Schmidt
transformations), 38, 254, 526
orthogonally diagonalizable, 147, 154,
341, 346, 425
orthogonally invariant norm, 164, 167,
168
orthogonally similar, 146, 154, 164, 168,
271, 346
orthonormal vectors, 33
out-of-core algorithm, 514
outer product, 90, 247
Exercise 3.14:, 179
outer product for matrix multiplication,
531
outer pseudoinverse, 128, 129
outer/inner products matrix, 359
overdetermined linear system, 124, 257,
289
overﬁtting, 300, 431
overﬂow, in computer operations, 485,
489
Overleaf, 541
overloading, 11, 63, 165, 481, 491
P
p-inverse (see also Moore-Penrose
inverse), 128
paging, 567
parallel processing, 509, 510, 530, 532,
546, 559
parallelogram equality, 27
parallelotope, 75
Parseval’s identity, 41, 169
partial ordering, 16, 92, 350
Exercise 8.2a:, 396
partial pivoting, 277
partitioned matrix, 61, 79, 131
determinant, 71, 122
sum of squares, 363, 415, 422
partitioned matrix, inverse, 122, 131
partitioning sum of squares, 363, 415,
422
PBLAS (parallel BLAS), 560
pencil, 161
permutation, 66
permutation matrix, 81, 87, 275, 380
Perron root, 374, 377
Perron theorem, 373
Perron vector, 374, 377, 447
Perron-Frobenius theorem, 377
pivoting, 84, 246, 251, 277
PLASMA, 561
polar cone, 45
polynomial in a matrix, 78
Exercise 3.26:, 181
polynomial order, 505
polynomial regression, 382
polynomial, evaluation of, 514
pooled variance-covariance matrix, 370
population model, 448
portability, 482, 496, 542
positive deﬁnite matrix, 92, 101,
159–160, 255, 348–352, 424
summary of properties, 348–350
positive matrix, 260, 372
positive semideﬁnite matrix, 92
positive stable, 159, 396
power method for eigenvalues, 313–315
precision, 474–482, 493
arbitrary, 468
double, 474, 482
extended, 474
half precision, 481
inﬁnite, 468
multiple, 468, 493
single, 474, 482
preconditioning, 284, 312, 527
for eigenvalue computations, 312
in the conjugate gradient method,
284
primitive Markov chain, 447
primitive matrix, 377, 447
principal axis, 36

644
Index
principal components, 424–428
principal components regression, 430
principal diagonal, 56
principal minor, 72, 104, 600
principal submatrix, 62, 104, 243, 346,
349
leading, 62, 350
probabilistic error bound, 498
programming model, 465, 546
projected gradient, 209
projected Hessian, 209
projection (of a vector), 20, 36
projection matrix, 358–359, 410
projective transformation, 230
proper value (see also eigenvalue), 135
PSBLAS (parallel sparse BLAS), 560
pseudo-correlation matrix, 439
pseudoinverse (see also Moore-Penrose
inverse), 128
PV-Wave (software), 6, 572
Pythagorean theorem, 27
Python, 480, 548, 571, 572
Q
Q-convergence, 511
QL factorization, 249
QR factorization, 248–254
and a generalized inverse, 251
computing, 560, 577
matrix rank, 252
skinny, 249
QR method for eigenvalues, 318–320
quadratic convergence, 511
quadratic form, 91, 94
quasi-Newton method, 201
quotient space, 115
R
R (software), 548, 572–580
Microsoft R Open, 580
Rcpp, 580
RcppArmadillo, 580
roxygen, 579
RPy, 580
RStudio, 580
Spotﬁre S+ (software), 580
radix, 469
random graph, 339
random matrix, 220–222
BMvN distribution, 221
computer generation Exercise 4.10:,
223
correlation matrix, 444
Haar distribution, 221
Exercise 4.10:, 223
normal, 220
orthogonal Exercise 4.10:, 223
rank Exercise 4.11:, 224
Wishart, 122, 348, 423, 434
Exercise 4.12:, 224
random number generation, 443–444
random matrices Exercise 4.10:, 223
random variable, 217
range of a matrix, 55
rank deﬁciency, 101, 144
rank determination, 534
rank of a matrix, 99–122, 252, 433, 534
of idempotent matrix, 353
rank-revealing QR, 252, 433
statistical tests, 433–437
rank of an array, 5
rank reduction, 535
rank(·), 99
rank, linear independence, 99, 534
rank, number of dimensions, 5
rank-one decomposition, 164
rank-one update, 236, 287
rank-revealing QR, 252, 433, 534
rate constant, 511
rate of convergence, 511
rational fraction, 493
Rayleigh quotient, 90, 157, 211, 395
Rcpp, 580
RcppBlaze, 561
RCR (Replicated Computational
Results), 554
real numbers, 468
real-time algorithm, 514
recursion, 513
reduced gradient, 209
reduced Hessian, 209
reduced rank regression problem, 434
reducibility, 313, 336–338, 375
Markov chains, 447
reﬂection, 233–235
reﬂector, 235
reﬂexive generalized inverse, 128
register, in computer processor, 487

Index
645
regression, 403–424, 428–433
regression variable selection, 429
regression, nonlinear, 202
regular graph, 331
regular matrix (see also diagonalizable
matrix), 149
regularization, 300, 407, 431
relative error, 486, 496, 528
relative spacing, 472
Reliable Computing, 494
Replicated Computational Results
(RCR), 554
Reproducible R Toolkit, 580
reproducible research, 553–554, 580
residue arithmetic, 495
restarting, 527
reverse communication, 566
ρ(·) (spectral radius), 142
Richardson extrapolation, 512
ridge regression, 272, 364, 407, 420, 431
Exercise 9.11a:, 455
right direct product, 95
right inverse, 108
robustness (algorithm or software), 502
root of a function, 488
root-free Cholesky, 257
Rosser test matrix, 552
rotation, 231–234, 238
rounding, 474
rounding error, 489, 497
row echelon form, 111
row rank, 100
row space, 56
row-major, 524, 541, 547
row-sum norm, 166
roxygen, 579
RPy, 580
RQ factorization, 249
RStudio, 580
S
S (software), 572
Samelson inverse, 31
sample variance, computing, 503
saxpy, 12
scalability, 508
ScaLAPACK, 560
scalar, 11
scalar product, 23
scaled matrix, 367
scaled vector, 49
scaling of a vector or matrix, 271
scaling of an algorithm, 505, 508
Schatten p norm, 169
Schur complement, 121, 415, 423
Schur factorization, 147–148
Schur norm (see also Frobenius norm),
167
SDP (semideﬁnite programming), 348
Seidel adjacency matrix, 334
self-adjoint matrix (see also Hermitian
matrix), 56
semideﬁnite programming (SDP), 348
seminorm, 25
semisimple eigenvalue, 144, 149
sequences of matrices, 171
sequences of vectors, 32
shape of matrix, 6
shearing transformation, 230
Sherman-Morrison formula, 287, 417
shifting eigenvalues, 312
shrinkage, 407
side eﬀect, 556
σ(·) (sign of permutation), 66, 87
σ(·) (spectrum of matrix), 141
sign bit, 467
sign(·), 16
signiﬁcand, 469
similar canonical form, 149
similar matrices, 146
similarity matrix, 371
similarity transformation, 146–148, 315,
319
simple eigenvalue, 144
simple graph, 331
simple matrix (see also diagonalizable
matrix), 149
single precision, 474, 482
singular matrix, 101
singular multivariate normal distribu-
tion, 219, 435
singular value, 162, 427, 534
relation to eigenvalue, 163
singular value decomposition, 161–164,
322, 339, 427, 534
uniqueness, 163
skew diagonal element, 57
skew diagonal matrix, 57

646
Index
skew symmetric matrix, 56, 60
skew upper triangular matrix, 58, 391
skinny QR factorization, 249
smoothing matrix, 363, 420
software testing, 550–553
SOR (method), 281
span(·), 14, 21, 55
spanning set, 14, 21
of a cone, 44
Spark (software system), 546
sparse matrix, 59, 261, 279, 525, 528,
558, 559
index-index-value, 550
software, 559
CUDA, 562
storage mode, 550
spectral circle, 142
spectral condition number, 270, 272, 292
spectral decomposition, 155, 163
spectral norm, 166, 169
spectral projector, 155
spectral radius, 142, 166, 171, 280
spectrum of a graph, 394
spectrum of a matrix, 141–145
splitting extrapolation, 512
Spotﬁre S+ (software), 580
square root matrix, 160, 254, 256, 347
stability, 278, 502
standard deviation, 49, 366
computing the standard deviation,
503
Standard Template Library, 571
standards (see also speciﬁc standard),
462
stationary point of vector/matrix
functions, 200
statistical reference datasets (StRD),
553
statlib, 619
statlib, xiv
steepest descent, 199, 201
Stiefel manifold, 133
stiﬀdata, 504
stochastic matrix, 379
stochastic process, 445–452
stopping criterion, 510
storage mode, for matrices, 548–550
storage unit, 466, 469, 482
Strassen algorithm, 531–533
StRD (statistical reference datasets),
553
stride, 524, 541, 556
string, character, 463
strongly connected graph, 336
submatrix, 61, 79
subspace of vector space, 17
successive overrelaxation, 281
summation, 487
summing vector, 34
Sun ONE Studio Fortran 95, 495
superlinear convergence, 511
SVD (singular value decomposition),
161–164, 322, 339, 427, 534
uniqueness, 163
sweep operator, 415
Sylvester’s law of nullity, 117
symmetric matrix, 56, 60, 112, 153–160,
340–346
eingenvalues/vectors, 153–160
equivalent forms, 112
inverse of, 120
summary of properties, 340
symmetric pair, 321
symmetric storage mode, 61, 549
T
Taylor series, 190, 200
Template Numerical Toolkit, 571
tensor, 5
term-document matrix, 338
test problems for algorithms or software,
529, 550–553
Exercise 3.24:, 180
consistency test, 529, 553
Ericksen matrix, 551
Exercise 12.8:, 584
Hilbert matrix, 550
Matrix Market, 552
randomly generated data, 551
Exercise 11.5:, 538
Rosser matrix, 552
StRD (statistical reference datasets),
553
Wilkinson matrix, 552
Exercise 12.9:, 584
Wilkinson’s polynomial, 501
testable hypothesis, 412
testing software, 550–553

Index
647
thread, 546
Tikhonov regularization, 301, 431
time series, 449–452
variance-covariance matrix, 385, 451
Toeplitz matrix, 384, 451
circulant matrix, 386
inverse of, 385
Exercise 8.12:, 398
Exercise 12.12:, 585
total least squares, 302, 407
tr(·), 65
trace, 65
derivative of, 197
of Cayley product, 88
of idempotent matrix, 353
of inner product, 92
of Kronecker product, 96
of matrix inner product, 98
of outer product, 92
relation to eigenvalues, 141
trace norm, 169
transition matrix, 446
translation transformation, 234
transpose, 59
determinant of, 70
generalized inverse of, 124
inverse of, 107
norm of, 164
of Cayley product of matrices, 76
of Kronecker product, 95
of partitioned matrices, 63
of sum of matrices, 63
trace of, 65
trapezoidal matrix, 58, 243, 248
triangle inequality, 25, 164
Exercise 2.11b:, 52
triangular matrix, 58, 84, 242
determinant of, 70
inverse of, 121
multiplication, 79
tridiagonal matrix, 58
triple scalar product Exercise 2.19c:, 54
triple vector product Exercise 2.19d:, 54
truncation error, 42, 99, 500
twos-complement representation, 467,
485
type 2 matrix, 58, 385
U
ulp (“unit in the last place”), 473
underdetermined linear system, 123
underﬂow, in computer operations, 472,
489
Unicode, 463
union of vector spaces, 18
unit in the last place (ulp), 473
unit roundoﬀ, 472
unit vector, 16, 22, 36, 76
unitarily diagonalizable, 147, 346, 389
unitarily similar, 146
unitary matrix, 132
unrolling do-loop, 568
updating a solution, 287, 295, 417–419
regression computations, 417–419
upper Hessenberg form, 59, 319
upper triangular matrix, 58
usual norm (see also Frobenius norm),
167
V
V(·) (variance operator), 49, 218
V(·) (vector space), 21, 55
Vandermonde matrix, 382
Fourier matrix, 387
variable metric method, 201
variable selection, 429
variance, computing, 503
variance-covariance matrix, 218, 221
Kronecker structure, 221, 421
positive deﬁnite approximation, 438
sample, 367, 424
vec(·), 61
vec-permutation matrix, 82
vecdiag(·), 56
vech(·), 61
vector, 4
centered vector, 48
mean vector, 35
normal vector, 34
normalized vector, 31
null vector, 15
one vector, 16, 34
“row vector”, 89
scaled vector, 49
sign vector, 16
summing vector, 16, 34

648
Index
vector (cont.)
unit vector, 16
zero vector, 15
vector derivative, 185–222
vector processing, 559
vector space, 13–15, 17–23, 55, 64, 126,
127
basis, 21–23
deﬁnition, 13
dimension, 14
direct product, 20
direct sum, 18–20
direct sum decomposition, 19
essentially disjoint, 15
intersection, 18
null vector space, 13
of matrices, 64
order, 15
set operations, 17
subspace, 17
union, 18
vector subspace, 17
vectorized processor, 509
vertex of a graph, 331
volume as a determinant, 74, 219
W
weighted graph, 331
weighted least squares, 416
with equality constraints Exer-
cise 9.4d:, 453
weighted norm, 28, 94
Wilk’s Λ, 423
Wilkinson matrix, 552
Wishart distribution, 122, 348
Exercise 4.12:, 224
Woodbury formula, 288, 417
word, computer, 466, 469, 482
X
XDR (external data representation),
483
Y
Yule-Walker equations, 451
Z
Z-matrix, 396
zero matrix, 77, 99
zero of a function, 488
zero vector, 15

