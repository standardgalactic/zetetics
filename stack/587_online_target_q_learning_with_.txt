Published as a conference paper at ICLR 2022
ONLINE TARGET Q-LEARNING WITH REVERSE EXPE-
RIENCE REPLAY: EFFICIENTLY FINDING THE OPTIMAL
POLICY FOR LINEAR MDPS
Naman Agarwal, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli
Google Research
{namanagarwal,prajain,dheeraj,pnetrapalli}@google.com
Syomantak Chaudhuri
University of California, Berkeley
syomantak@berkeley.edu
ABSTRACT
Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely
deployed with function approximation (Mnih et al., 2015). In contrast, existing
theoretical results are pessimistic about Q-learning. For example, Q-learning does
not converge even with linear function approximation for linear MDPs ((Baird,
1995)) and even for tabular MDPs with synchronous updates, Q-learning has sub-
optimal sample complexity (Li et al., 2021; Azar et al., 2013). The goal of this
work is to bridge the gap between practical success of Q-learning and the relatively
pessimistic theoretical results. The starting point of our work is the observation
that in practice, Q-learning is used with two important modiﬁcations: (i) train-
ing with two networks, called online network and target network simultaneously
(online target learning, or OTL) , and (ii) experience replay (ER) (Mnih et al.,
2015). While they play a signiﬁcant role in the practical success of Q-learning,
a thorough theoretical understanding of how these two modiﬁcations improve the
convergence behavior of Q-learning has been missing in literature. By carefully
combining Q-learning with OTL and reverse experience replay (RER) (a form of
experience replay), we present novel methods Q-Rex and Q-RexDaRe (Q-Rex+
data reuse). We show that Q-Rex efﬁciently ﬁnds the optimal policy for linear
MDPs (or more generally for MDPs with zero inherent Bellman error with lin-
ear approximation (ZIBEL)) and provide non-asymptotic bounds on sample com-
plexity – the ﬁrst such result for a Q-learning method for this class of MDPs un-
der standard assumptions. Furthermore, we demonstrate that Q-RexDaRe in fact
achieves near optimal sample complexity in the tabular setting, improving upon
the existing results for vanilla Q-learning.
1
INTRODUCTION
Reinforcement Learning (RL) has been shown to be highly successful in practice for a variety of
long term decision making problems (Mnih et al., 2015). Several classical works have studied RL
methods like TD-learning, Q-learning and their variants (Sutton & Barto, 2018; Bertsekas, 2011;
Borkar & Meyn, 2000; Sutton, 1988; Tsitsiklis & Van Roy, 1997; Watkins & Dayan, 1992; Watkins,
1989) but the guarantees are mostly asymptotic and therefore do not sufﬁciently answer important
questions that are relevant to practitioners who struggle with constraints on the number of data points
and the computation power. Recent works provide non-asymptotic results for a variety of important
settings (Kearns & Singh, 1999; Beck & Srikant, 2012; Qu & Wierman, 2020; Ghavamzadeh et al.,
2011; Bhandari et al., 2018; Chen et al., 2020; 2019; Dalal et al., 2018a;b; Doan et al., 2020; Gupta
et al., 2019; Srikant & Ying, 2019; Weng et al., 2020; Yang & Wang, 2019; Zou et al., 2019a).
Despite a large body of work, several aspects of fundamental methods like Q-learning (Watkins &
Dayan, 1992) are still ill-understood. Q-learning’s simplicity and the ability to learn from off-policy
1

Published as a conference paper at ICLR 2022
data makes it widely applicable. However, theoretical analyses show that even with linear function
approximation and when the approximation is exact, Q-learning can fail to converge even in simple
examples (Baird, 1995; Boyan & Moore, 1995; Tsitsiklis & Van Roy, 1996). Furthermore, even in
the simple case of tabular RL with synchronous updates, Q-learning is known to have sub-optimal
sample complexity (Wainwright, 2019a; Li et al., 2021).
However, Q-learning has seen tremendous practical success when deployed with “heuristic” mod-
iﬁcations like experience replay (ER) and online target learning (OTL). ER is used to alleviate the
issues arising due highly dependent samples in an episode whereas OTL helps stabilize the Q iter-
ation. Mnih et al. (2015) conducted extensive experiments to show that both these techniques, are
essential for the success of Q-learning. But, existing analyses for ER with Q-learning either require
stringent assumptions (Carvalho et al., 2020) to ensure convergence to a good Q value, or assume
that ER provides i.i.d. samples which might not hold in practice (Fan et al., 2020; Carvalho et al.,
2020). In this paper, we attempt to bridge the gap between theory and practice, by rigorously in-
vestigating how Q-learning performs with these practical heuristics. We thus introduce two model
free algorithms: Q-Rex and Q-RexDaRe that combine the standard Q-learning with OTL and re-
verse experience replay (RER). RER is a form of ER which was recently studied to unravel spurious
correlations present while learning form Markovian data in the context of system identiﬁcation (Jain
et al., 2021b). We show that OTL stabilizes the Q value by essentially serving as a variance reduc-
tion technique and RER unravels the spurious correlations present in the Markovian data to remove
inherent biases introduced in vanilla Q learning.
These simple modiﬁcations have surprisingly far-reaching consequences. Firstly, this allows us to
show that unlike vanilla Q-learning, Q-Rex ﬁnds the optimal policy for MDPs with an exact linear
function representation of the Bellman operator. and allows us to derive non-asymptotic sample
complexity bounds. In the tabular setting, Q-Rex even with asynchronous data is able to match the
best known bounds for Q-learning with synchronous data. Its variant Q-RexDaRe , which reuses
old samples, admits nearly optimal sample complexity for recovering the optimal Q-function in the
tabular setting. Previously, only Q-learning methods with explicit variance-reduction techniques (not
popular in practice) (Wainwright, 2019b; Li et al., 2020b) or model based methods (Agarwal et al.,
2020; Li et al., 2020a) were known to achieve such a sample complexity bound. Our experiments
show that when the algorithmic parameters are chosen carefully, Q-Rex and its variants outperform
both vanilla Q-learning and OTL+ER+Q-learning with the same parameters (see Appendix A).
To summarize, in this work, we study Q-learning with practical heuristics like ER and OTL, and
propose two concrete methods Q-Rex and Q-RexDaRe based on OTL and reverse experience
replay – a modiﬁcation of the standard ER used in practice. We show that Q-Rex is able to ﬁnd
the optimal policy for ZIBEL MDPs, with a strong sample complexity bound which is the ﬁrst such
result for Q-learning. We also show that Q-RexDaRe obtains nearly optimal sample complexity for
the simpler tabular setting despite not using any explicit variance reduction technique. See Table 1
for a comparison of our guarantees against the state-of-the-results for the tabular setting.
Organization
We review related works in next subsection. In Section 2 we develop the MDP
problem which we seek to solve and present our algorithm, Q-Rex in Section 3. The main theoretical
results are presented in Section 4. We present a brief overview of the analysis in Section 5 and
present our experiments in Section A. We provide minimax lower bounds for the asynchronous
tabular setting in Section K. Most of the formal proofs are relegated to the appendix.
1.1
RELATED WORKS
Tabular Q-learning
Tabular MDPs are the most basic examples of MDPs where the state space
(S) and the action space (A) are both ﬁnite and the Q-values are represented by assigning a unique
co-ordinate to each state-action pair. This setting has been well studied over the last few decades and
convergence guarantees have been derived in both asymptotic and non-asymptotic regimes for popu-
lar model-free and model-based algorithms. Azar et al. (2013) shows that the minimax lower bounds
on the sample complexity of obtaining the optimal Q-function up-to ϵ error is
|S||A|
(1−γ)3ϵ2 , where γ is
the discount factor. Near sample-optimal estimation is achieved by several model-based algorithms
(Agarwal et al., 2020; Li et al., 2020a) and model-free algorithms like variance reduced Q-learning
(Wainwright, 2019b; Li et al., 2020b). (Li et al., 2021) also shows that vanilla Q-learning with stan-
dard step sizes, even in the synchronous data setting – where transitions corresponding to each state
2

Published as a conference paper at ICLR 2022
Paper
Algorithm
Data Type
Sample Complexity
(GHAVAMZADEH ET AL., 2011)
SPEEDY Q-LEARNING
SYNCHRONOUS
|S||A|
ϵ2(1−γ)4
(WAINWRIGHT, 2019B)
VARIANCE REDUCED
Q-LEARNING
SYNCHRONOUS
|S||A|
ϵ2(1−γ)3
(LI ET AL., 2020B)
VARIANCE REDUCED
Q-LEARNING
ASYNCHRONOUS
1
µminϵ2(1−γ)3
(LI ET AL., 2020B)
Q-LEARNING
ASYNCHRONOUS
1
µminϵ2(1−γ)5
(LI ET AL., 2021)
Q-LEARNING
SYNCHRONOUS
|S||A|
ϵ2(1−γ)4
THIS WORK, THEOREM 2
Q-LEARNING+ OTL
+ RER (Q-REX)
ASYNCHRONOUS
|S||A|
ϵ2(1−γ)4
THIS WORK, THEOREM 3
Q-REX+ DATA-REUSE
(Q-REXDARE)
ASYNCHRONOUS
max( ¯d, 1
ϵ2 )
µmin(1−γ)3
Table 1: Comparison of tabular Q-learning based algorithms. ¯d ≤|S| is maximum size of support
of P(·|s, a). In the case of asynchronous setting,
1
µmin is roughly equivalent to |S||A| in the syn-
chronous setting. We use the color green to represent results with optimal dependence on (1−γ)−1.
action pair are sampled independently at each step – suffers from a sample complexity of
|S||A|
(1−γ)4ϵ2
and the best known bounds in the asynchronous setting – where data is derived from a Markovian
trajectory and only one Q value is updated in each step – is
|S||A|
(1−γ)5ϵ2 . These results seem unsatis-
factory since γ ∈(0.99, 1) in most practical applications. In contrast, our algorithm Q-Rex with
asynchronous data has a sample complexity that matches Q-learning bound with synchronous data
and its data-efﬁcient variant Q-RexDaRe has near minimax optimal sample complexity (see Ta-
ble 1). For details on model based algorithms, and previous works with sub-optimal guarantees we
refer to (Agarwal et al., 2020; Li et al., 2020b). We note that the lower bounds apply only to the
synchronous case (i.e, when every state-action pair is sampled at every step). We provide minimax
lower-bounds which show that the bound is tight in the asynchronous case too (see Theorem 5 in
Section K), where |S||A| in the synchronous case of (Azar et al., 2013) is replaced by
1
µmin .
Q-learning with Linear Function Approximation
Since tabular Q-learning is intractable in most
practical RL problems due to a large state space S, function approximation is deployed. Linear
function approximation is the simplest such case where the Q-function is approximated with a linear
function of the ‘feature embedding’ associated with each state-action pair. However, Q-learning can
be shown to diverge even in the simplest cases as was ﬁrst noticed in (Baird, 1995), which also
introduced residual gradient methods which converged rather slowly but provably. We will only
discuss recent works closest to our work and refer the reader to (Carvalho et al., 2020; Yang &
Wang, 2019) for a full survey of various works in this direction. SARSA is the on-policy control
variant of Q-learning where the challenge is to explore the state-space while learning the optimal
policy. Unlike Q-learning, SARSA is inherently stable due to its on-policy nature (Gordon, 2000).
Therefore, we do not compare our results to the results of on-policy control algorithms like SARSA.
We refer to (Zou et al., 2019b; Perkins & Precup, 2002; Melo et al., 2008) for further details.
Yang & Wang (2019) consider MDPs with approximate linear function representation. They require
additional assumptions like ﬁnite state-action space and existence of known anchor subsets which
might not hold in practice. Our results on the other hand hold with standard assumptions, with
asynchronous updates and can handle inﬁnite state-action spaces (see Theorem 1). Similarly, Chen
et al. (2019) consider Q-learning with linear function approximation which need not be exact. But
the result requires the restrictive assumption that the ofﬂine policy is close to the optimal policy. In
contrast, we consider the less general but well-studied case of MDPs with zero inherent Bellman
error and provide global convergence without restrictive assumptions on the behaviour policy.
Under the most general conditions Maei et al. (2010) present the Greedy-GQ algorithm which con-
verges to a point asymptotically instead of diverging. Similar results are obtained by Carvalho et al.
(2020) for Coupled Q-learning, a 2-timescale variant of Q-learning which uses a version of OTL and
ER1. This algorithm experimentally resolves the popular counter-examples provided by (Tsitsiklis
& Van Roy, 1996; Baird, 1995). However, the value function guarantees in Carvalho et al. (2020,
1The version of ER used in Carvalho et al. (2020) makes the setting completely synchronous as opposed to
the asynchronous setting considered by us.
3

Published as a conference paper at ICLR 2022
Theorem 2) (albeit without sample complexity guarantees) requires very stringent assumptions and
even in the case of tabular Q-learning might not converge to the optimal policy.
Experience Replay and Reverse Experience Replay
Reinforcement learning involves learning
on-the-go with highly correlated correlated data. Iterative learning algorithms like Q-learning can
sometimes get coupled to the Markov chain resulting in sub-optimal convergence. Experience replay
(ER) was introduced in order to mitigate this drawback (Lin, 1992) – here a large FIFO buffer of
a ﬁxed size stores the streaming data and the learning algorithm samples a data point uniformly at
random from this buffer at each step. This makes the samples look roughly i.i.d., thus breaking the
harmful correlations. Reverse experience replay (RER) is a form of ER which stores data in a buffer
but processes the data points in the reverse order as stored in the buffer. This was introduced in
entirely different contexts by (Rotinov, 2019; Jain et al., 2021b;a). In this work, we note that reverse
order traversal endows a super-martingale structure which yields the strong concentration result in
Theorem 4, which is not possible with forward order traversal (see (Jain et al., 2021b, Section 3.1)
for a brief demonstration of this fact). We can also look at RER is through the lens of Dynamic
programming (Bertsekas, 2011) – where the value function is evaluated backwards starting from
time T to time 1 - similar to how RER bootstraps present to the future.
In the context of reinforcement learning, works like Bhandari et al. (2018); Zou et al. (2019b) obtain
ﬁnite time convergence guarantees for RL algorithms under the mixing assumptions just like this
work. The strategy followed in these works is that if two samples are ˜O(τmix) time apart, then
they are approximately independent and thus analysis for i.i.d. data can be applied. The sample
complexity to obtain ϵ error here is O( τmix
ϵ2 ). Note that this is no better than keeping one every τmix
samples and throwing away the rest and under general mis-speciﬁed linear function representation,
we might not be able to do any better (Bresler et al., 2020). In this work, we show that when the
linear approximation is well specifed (ZIBEL), we can use RER to obtain a sample complexity of
O(τmix + 1
ϵ2 ), where the mixing time serves as a cut-off.
Online Target Learning
OTL (Mnih et al., 2015) maintains two different Q-values (called online
Q-value and target Q-value) where the target Q-value is held constant for some time and only the
online Q-value is updated by ‘bootstrapping’ to the target. After a number of such iterations, the
target Q-value is set to the current online Q value. OTL thus attempts to mitigate the destabilizing
effects of bootstrapping by removing the ‘moving target’. This technique has been noted to allow
for an unbiased estimation of the bellman operator (Fan et al., 2020) and when trained with large
batch sizes is similar to the well known neural ﬁtted Q-iteration (Riedmiller, 2005).
2
PROBLEM SETTING
Markov Decision Process
We consider inﬁnite horizon, time homogenous Markov Decision Pro-
cesses (MDPs) and we denote an MDP by MDP(S, A, γ, P, R) where S is the state space, A is the
action space, γ ∈[0, 1) is the discount factor, P(s′|s, a) is the probability of transition to state s′
from the state s on action a. We assume that S and A are compact subsets of Rn (for some n ∈N).
R : S × A →[0, 1] is the deterministic reward associated with every state-action pair.
We will think of an MDP as an agent that is aware of its current state and it can choose the action to
be taken. Suppose the agent takes action π(s), where π : S →A, at state s ∈S, then P along with
the ‘policy’ π induces a Markov chain over S, whose transition kernel is denoted by P π. We write
the γ-discounted value function of the MDP starting at state s to be:
V (s, π) = E[
∞
X
t=0
γtR(St, At)|S0 = s, At = π(St)∀t].
(1)
It is well-known that under mild assumptions, there exists at least one optimal policy π∗such that
the value function V (s, π) is maximized for every sand that there is an optimal Q-function, Q∗:
S × A →R, such that one can ﬁnd the optimal policy as π∗(s) = arg maxa∈A Q∗(s, a), optimal
value function as V ∗(s) = maxa∈A Q∗(s, a) and it satisﬁes the following ﬁxed point equation.
Q∗(s, a) = R(s, a) + γEs′∼P (·|s,a)[max
a′∈A Q∗(s′, a′)]
∀(s, a) ∈S × A.
(2)
(2) can be alternately viewed as Q∗being the ﬁxed point of the Bellman operator T , where
T (Q)(s, a) = R(s, a) + γEs′∼P (·|s,a)[max
a′ Q(s′, a′)].
4

Published as a conference paper at ICLR 2022
The basic task at hand is to estimate Q∗(s, a) from a single trajectory (st, at)T
t=1 such that
st+1 ∼P(·|st, at) along with rewards (rt)T
t=1, where r1, . . . , rT are random variables such that
E [rt|st = s, at = a] = R(s, a). We refer to Section B for rigorous deﬁnitions.
Q-learning
Since the transition kernel P (and hence the Bellman operator T ) is often unknown
in practice, Equation (2) cannot be directly used to to estimate the optimal Q-function. To this end,
we resort to estimating Q∗using observations from the MDP. An agent traverses the MDP and we
obtain the state, action, and the reward obtained at each time step. We assume the off-policy setting
which means that the agent is not in our control, i.e., it is not possible to choose the agent’s actions;
rather, we just observe the state, the action, and the corresponding reward. Further, we assume that
the agent follows a time homogeneous policy π(s) for choosing its action at state s.
Given a trajectory {st, at, rt}T
t=1 generated using some unknown behaviour policy π, we aim to
estimate Q∗in a model-free manner - i.e, estimate Q∗without directly estimating P. We further
assume that the trajectory is given to us as a data stream so we can not arbitrarily fetch the data for
any time instant. A popular method to estimate Q∗is using the Q-learning algorithm. In this online
algorithm, we maintain an estimate of Q∗(s, a) at time t, Qt(s, a) and the estimate is updated at
time t for (s, a) = (st, at) in the trajectory. Formally, with step-sizes given as {ηt}, Q-learning
performs the following update at time t,
Qt+1(st, at) = (1 −ηt)Qt(st, at) + ηt

rt + γ max
a′∈A Qt(st+1, a′)

Qt+1(s, a) = Qt(s, a)
∀(s, a) ̸= (st, at).
(3)
In this work, we focus on two special classes of MDPs which are popular in literature.
Linear Markov Decision Process
Linear MDPs (Jin et al., 2020) is a popular example of exact
linear approximation for which statistically and computationally tractable algorithms are available.
We use the deﬁnition from Jin et al. (2020), stated as Deﬁnition 1.
Deﬁnition 1. An MDP(S, A, γ, P, R) is a linear MDP with feature map φ : S × A →Rd, if
1. there exists a vector θ ∈Rd such that R(s, a) = ⟨φ(s, a), θ⟩, and
2. there exists d unknown (signed) measures over S β(·) = {β1(·), . . . , βd(·)} such that the
transition probability P(·|s, a) = ⟨φ(s, a), β(·)⟩.
In the rest of this paper, in the tabular setting we assume that the dimension d = |S × A| and we use
a one hot embedding where we map (s, a) →φ(s, a) = es,a, a unique standard basis vector. It is
easy to show that this system is a linear MDP (Jin et al., 2020) and Q-learning in this setting reduces
to the standard tabular Q-learning (3). However, when the assumption of a tabular MDP allows us
to obtain stronger results, we will present the analysis separately.
Inherent Bellman Error
There is another widely studied class of MDPs which admit a good
linear representation (Zanette et al., 2020; Munos & Szepesv´ari, 2008; Szepesv´ari & Smart, 2004).
Deﬁnition 2. (ZIBEL MDP) For an M = MDP(S, A, γ, P, R) with a feature map φ : S ×A →Rd,
we deﬁne the inherent Bellman error (IBE(M)) as:
sup
θ∈Rd inf
θ′∈Rd
sup
(s,a)∈S×A
⟨φ(s, a), θ′⟩−R(s, a) −γEs′∼P (·|s,a) sup
a′∈A
⟨θ, φ(s′, a′)⟩

If IBE(M) = 0, then call this MDP a ZIBEL (zero inherent Bellman error with linear function
approximation) MDP.
The class of ZIBEL MDPs is strictly more general than the class of linear MDPs (Zanette et al.,
2020). Both these classes of MDPs have the property that there exists a vector w∗∈Rd such that
the optimal Q-function, Q∗(s, a) = ⟨φ(s, a), w∗⟩for every (s, a) ∈S × A, which can be explicity
expressed as a function of θ, β and Q∗. More generally, they allow us to lift the Bellman iteration to
Rd exactly and update our estimates for w∗values directly (Lemmas 3, 4).
Hence, we can focus on estimating Q∗by estimating w∗. To this end, the standard Q-Learning
approach to learning the Q function can be extended to the linear case as follows:
5

Published as a conference paper at ICLR 2022
wt+1 = wt + ηt

rt + γ max
a′∈A⟨φ(st+1, a′), wt⟩−⟨φ(st, at), wt⟩

φ(st, at).
The above update can be seen as a gradient descent step on the loss function f(wt)
=
(⟨φ(st, at), wt⟩−target)2 where target = rt+γ maxa′⟨φ(st+1, a′), wt⟩. This update while heavily
used in practice, has been known to be unstable and does not converge to w∗in general. The rea-
son often cited for this phenomenon is the presence of the ‘deadly triad’ of bootstrapping, function
approximation, and off-policy learning.
2.1
ASSUMPTIONS
We make the following assumptions on the MDPs considered through the paper in order to present
our theoretical results.
Assumption 1. The MDP M has IBE(M) = 0 (Deﬁnition 2), ∥φ(s, a)∥2 ≤1. Furthermore,
R(s, a) ∈[0, 1].
Assumption 2. Let Φ := {φ(s, a) : (s, a) ∈S × A}. Φ is compact, span(Φ) = Rd and (s, a) →
φ(s, a) is measurable.
Even when span(Φ) ̸= Rd, our results hold after we discard the space orthogonal to the span of em-
bedding vectors in Assumption 4 and note that Q-Rex does not update the iterates along span(Φ)⊥.
Deﬁnition 3. For r > 0, let N(Φ, ∥· ∥2, r) be the r-covering number under the standard Euclidean
norm over Rd. Deﬁne:
CΦ :=
Z ∞
0
p
log N(Φ, ∥∥2, r)dr
Observe that since Φ is a subset of the unit Euclidean ball in Rd, CΦ ≤C
√
d. However, in the case
of tabular MDPs it is easy to show that CΦ ≤C√log d.
Deﬁnition 4. We deﬁne the norm ∥· ∥φ over Rd by ∥x∥φ = sup(s,a) |⟨φ(s, a), x⟩|.
This is the natural norm of interest for the problem (Lemmas 2 and 4). We assume the existence
of a ﬁxed (random) behaviour policy π : S →∆(A) which selects a random action corresponding
to each state. At each step, given (st, at) = (s, a), st+1 ∼P(·|s, a) and at+1 ∼π(st+1). This
gives us a Markov kernel over S ×A which speciﬁes the law of (st+1, at+1) conditioned on (st, at).
We will denote this kernel by P π. This setting is commonly known as the off-policy asynchronous
setting. We make the following assumption which is standard in this line of work.
Assumption 3. There exists a unique stationary distribution µ for the kernel P π. Moreover, this
Markov chain is exponentially ergodic in the total variation distance with mixing time τmix. That is,
there exist a constant Cmix for every t ∈N
sup
x∈S×A
TV((P π)t(x, ·), µ) ≤Cmix exp(−t/τmix)
In the tabular setting, we will use the standard deﬁnition of τmix instead:
τmix = inf{t :
sup
x∈S×A
TV((P π)t(x, ·), µ) ≤1/4} .
Here TV refers to the total variation distance.
Assumption 4. There exists κ > 0 such that: E(s,a)∼µφ(s, a)φ⊤(s, a) ⪰I
κ .
Remark 1. (Bresler et al., 2020, Theorem 1) shows that even linear regression with Markovian data,
zero noise and ℓ2 recovery is hard when the condition number κ or the mixing time τmix are too large.
Hence, our setup of noisy reinforcement learning with ℓ∞error also requires these quantities to be
small. Therefore, Assumptions 3 and 4 are necessary in order to obtain non-trivial bounds.
In the tabular setting, Assumption 4 manifests itself as 1
κ = µmin := min(s,a) µ(s, a) which is also
standard (Li et al., 2020b). Whenever we discuss high probability bounds (i.e, probability at least
1 −δ), we assume that δ ∈(0, 1/2). Similarly, we will assume that the discount factor γ ∈(1/2, 1)
so that we can absorb poly(1/γ) factors into constants.
6

Published as a conference paper at ICLR 2022
Algorithm 1 Q-Rex
1: Input: learning rates η, horizon T, discount factor γ, trajectory Xt = {st, at, rt}, Buffer size
B, Buffer gap u, Number of inner loop buffers N
2: Total buffer size: S ←B + u, Outer-loop length: K ←
T
NS , Initialization w1,1
1
= 0
3: for k = 1, . . . , K do
4:
for j = 1, . . . , N do
5:
Form buffer Buf = {Xk,j
1 , . . . , Xk,j
S }, where, Xk,j
i
←XNS(k−1)+S(j−1)+i
6:
Deﬁne for all i ∈[1, S], φk,j
i
≜φ(sk,j
i
, ak,j
i
).
7:
for i = 1, . . . , B do
8:
wk,j
i+1 = wk,j
i
+η

rk,j
B+1−i + γ max
a′∈A⟨φ(sk,j
B+2−i, a′), wk,1
1 ⟩−⟨φk,j
B+1−i, wk,j
i
⟩

φk,j
B+1−i
9:
Option I: wk,j+1
1
= wk,j
B+1
10:
Option II: wk,j+1
1
= 1
B
PB
i=1 wk,j
i+1
11:
Option I: wk+1,1
1
= wk,N+1
1
12:
Option II: wk+1,1
1
= 1
N
PN+1
l=2 wk,l
1
13: Return wK+1,1
1
Figure 1: Illustration of Online Target Q-learning with Reverse Experience Replay
3
OUR ALGORITHM
As discussed in the introduction, we incorporate RER and OTL into Q-learning and introduce the
algorithms Q-Rex (Online Target Q-learning with reverse experience replay, Algorithm 1), its sam-
ple efﬁcient variant Q-RexDaRe (Q-Rex + data reuse, Algorithm 2) and its episodic variant EpiQ-
Rex (Episodic Q-Rex, Algorithm 3). Since Q-RexDaRe and EpiQ-Rex are only minor modiﬁcations
of Q-Rex, we refer the reader to the appendix for their pseudocode.
Q-Rex is parametrized by K the number of iterations in the outer-loop, N the number of buffers
within an outer-loop iteration, B the size of a buffer and u the gap between the buffers. The algorithm
has a three-loop structure where at the start of every outer-loop iteration (indexed by k ∈[K]), we
checkpoint our current guess of the Q function given by wk,1
1 . Each outer-loop iteration corresponds
to an inner-loop over the buffer collection with N buffers, i.e. at iteration j ∈[N], we collect a
buffer of size B + u consecutive state-action-reward tuples. For every collected buffer we consider
the ﬁrst B collected experiences and perform the target based Q-learning update in the reverse order
for these experiences. We refer Figure 1 for an illustration of the processing order. Of note, is the
usage of checkpointed target network in the RHS of the Q-learning update through the entirety of
the outer-loop iteration, i.e. for a ﬁxed k and for all j, i, our algorithm sets
wk,j
i+1 = wk,j
i
+ η

rk,j
B+1−i + γ max
a′∈A⟨φ(sk,j
B+2−i, a′), wk,1
1 ⟩−⟨φk,j
B+1−i, wk,j
i
⟩

φk,j
B+1−i
Figure 1 provides an illustration of the processing order for our updates. It can be seen that the
number of experiences collected through the run of the algorithm is T = KN(B + u). For the sake
of simplicity, we will assume that the initial point, w1,1
1
= 0. Essentially the same results hold for
arbitrary initial conditions. Q-RexDaRe is a modiﬁcation of Q-Rex where we re-use the data from
the ﬁrst outer-loop iteration (i.e, data from k = 1) in every outer-loop iteration (i.e, k > 1).
7

Published as a conference paper at ICLR 2022
Setting
K
N
u
B
η
ZIBEL MDP
(THEOREM 1)
≥1
> C3
B
κ
η log

Kκ
δ(1−γ)

≥C1τmix log( CmixKN
δ
)
= 10u
< C2 min(
(1−γ)2
C2
Φ+log(K/δ)), 1
B)
TABULAR MDP
(THEOREM 2)
≥C2
 log
 1
1−γ
2
1−γ
> C4
B
τmix
µmin log( |S||A|K
δ
)
≥C1τmix log( KN
δ )
= 10u
<
C3
log
  |S||A|K
δ

TABULAR MDP
(THEOREM 3)
≥C2
log
 1
1−γ

1−γ
> C4
B
τmix
µmin log( |S||A|
δ
)
≥C1τmix log( KN
δ )
= 10u
< C3
(1−γ)2
¯d log
  |S||A|
δ

ZIBEL MDP
(THEOREM 1)
β1
(1−γ)
κβ2 max
 C2
Φ+β2
ϵ2(1−γ)4τmix , 1

τmix log
  KN
δ

10u
min
  (1−γ)4ϵ2
C2
Φ+β3 , 1
B

TABULAR MDP
(THEOREM 2)
β2
1
1−γ
1
µmin max

β5,
β1
ητmix

τmix log
  KN
δ

10u
(1−γ)3
β5
min(ϵ, ϵ2)
TABULAR MDP
(THEOREM 3)
β1
(1−γ)
1
µmin max

β5,
β1
ητmix

τmix log
  KN
δ

10u
min

ϵ2(1−γ)3
β4
, (1−γ)2
¯dβ5 , ϵ(1−γ)3
√¯dβ4

Table 2: Parameter constraints (ﬁrst 3 rows) and choice for < ϵ error (last 3 rows) for our algorithms.
Here the poly-log factors βi are given by β1 = log
 1
(1−γ) min(ϵ,1)

, β3 = log
 1
(1−γ)δ min(ϵ,1)

,
β2 = log(κ) + β3, β4 = log
  |S||A|K
δ

, β5 = log
  |S||A|
δ

.
Remark 2. For the sake of clarity, we only analyze the algorithms Q-Rex and Q-RexDaRe for data
from a single trajectory with Option I. Option II involves averaging of the iterates which boosts the
convergence – indeed we can obtain much better bounds in this setting by using standard analysis.
4
MAIN RESULTS
We will now provide ﬁnite time convergence analysis and sample complexity for the algorithms
Q-Rex and Q-RexDaRe. Recall that K is the number of outer-loops, N is the number of buffers
inside an outer-loop iteration, B is the buffer size and u is the size of the gap. In what follows, we
will take u = ˜O(τmix), B = 10u, K = ˜O(
1
1−γ ). We also note that the total number of samples used
is NK(B + u) for Q-Rex and N(B + u) for Q-RexDaRe since we reuse data in each outer-loop
iteration. In what follows, by QK+1,1
1
(s, a), we denote ⟨φ(s, a), wK+1,1
1
⟩which is our estimate for
the optimal Q function. Here wK+1,1
1
is the output of either Q-Rex or Q-RexDaRe at the end of
K outer-loop iterations. Deﬁne ∥QK+1,1
1
−Q∥∞:= sup(s,a)∈S×A
QK+1,1
1
(s, a) −Q∗(s, a)
. We
ﬁrst consider the performance of Q-Rex with data derived from a linear MDP (Deﬁntion 1) or a
ZIBEL MDP (Deﬁnition 2) and satisfying the Assumptions in Section 2.1.
Theorem 1 (ZIBEL /Linear MDP). Suppose we run Q-Rex using Option I with data from an MDP
with IBE = 0. There exists constants C1, C2, C3, C4, C5 > 0 such that whenever the parameter
bounds given in Table 2 (row 1) are satisﬁed, then with probability at-least 1 −δ, we must have:
∥QK+1,1
1
−Q∗∥∞≤
γK
1−γ + C4
q
Kκ
δ(1−γ)4 exp

−ηNB
κ

+ C5
r
η
h
C2
Φ+log
 K
δ
i
(1−γ)4
Given ϵ ∈(0,
1
(1−γ)], and the parameters as given in Table 2 (row 4)(up to constant factors), then
with probability at-least 1 −δ: ∥QK+1,1
1
(s, a) −Q∗(s, a)∥∞< ϵ. This has a sample complexity
Θ(NKB) = ˜O

κ max

C2
Φ+1
(1−γ)5ϵ2 , τmix
1−γ

We now consider the performance of Q-Rex and Q-RexDaRe in the case of tabular MDPs. We
refer to Table 1 for a comparison of our results to the state-of-art results provided in literature for
Q-learning based algorithms.
Remark 3. To the best of our knowledge, Theorem 1 presents the ﬁrst non-asymptotic convergence
results for Q-learning based methods for ZIBEL MDPs under standard assumptions. Notice that
the sample complexity scales as 1
ϵ2 + τmix instead of τmix
ϵ2 like in Zou et al. (2019b); Bhandari et al.
(2018). This is because in the case of ZIBEL MDPs RER brings out the super-martingale structure
present in the problem which forward pass does not.
Theorem 2 (Tabular MDP). Suppose we run Q-Rex using Option I with data derived from tabular
MDPs. Whenever the algorithmic parameters are picked as given in Table 2 (row 2) for some
universal constants C1, . . . , C5, we obtain with probability at-least 1 −δ:
∥QK+1 −Q∗∥∞< C5
"
γL
1−γ +
exp
 −ηµminNB
2

(1−γ)2
+
η log
  K|S||A|
δ

(1−γ)3
+
r
η log
  K|S||A|
δ

(1−γ)3
#
8

Published as a conference paper at ICLR 2022
Where L =
b1K
log
1
1−γ
. Given ϵ ∈(0,
1
1−γ ], and the parameters are picked as given in Table 2 (row
5), then with probability at-least 1 −δ, we have: ∥QK+1 −Q∗∥∞< ϵ . This gives us a sample
complexity of
Θ(NKB) = ˜O

1
µmin max

1
(1−γ)4 min(ϵ,ϵ2), τmix
1−γ

.
Even though the sample complexity provided in Theorem 2 matches the sample complexity of syn-
chronous Q-learning even with asynchronous data, it is still sub-optimal with respect to the min-max
lower bounds (i.e, it has a dependence of has a dependence of
1
(1−γ)4 instead of the optimal
1
(1−γ)3 ).
We resolve this gap for Q-RexDaRe in Theorem 3. For tabular MDPs, the number states can be
large but the support of P(·|s, a) is bounded in most problems of practical interest. Consider the
following assumption (note that this holds for every tabular MDP with ¯d = |S|.)
Assumption 5. Tabular MDP is such that |supp(P(·|s, a))| ≤¯d ∈N.
Theorem 3 (Tabular MDP with Data Reuse). For tabular MDPs, suppose additionally Assumption 5
holds and we run Q-RexDaRe using Option I. There exist universal constants C1, C2, C3, C4 such
that when the parameter values satisfy the bounds in Table 2 (row 3), with probability at-least 1−δ:
∥QK+1,1
1
−Q∗∥∞≤C
"
exp
 −ηµminNB
2

+γK
(1−γ)2
+
η log
  |S||A|K
δ

(1−γ)3
p
¯d +
q
η
(1−γ)3 log
  K|S||A|
δ

#
Suppose ϵ ∈(0,
1
1−γ ]. If we choose the parameters as per Table 2 (row 6), then with probability
at-least 1 −δ we have: ∥QK+1(s, a) −Q∗∥∞< ϵ. The sample complexity in this case is
Θ(NB) = ˜O

1
µmin max

τmix,
1
ϵ2(1−γ)3 ,
¯d
(1−γ)2 ,
√¯d
ϵ(1−γ)3

.
5
OVERVIEW OF THE ANALYSIS
We divide the analysis of Q-Rex and Q-RexDaRe into two parts: Analysis of wk,1
1
obtained at the
end of outer-loop iteration k and the analysis of the algorithm within the outer-loop. The algorithm
reduces to SGD for linear regression with Markovian data within an outer-loop due to OTL. That is,
we try to ﬁnd wk+1,1
1
such that ⟨wk+1,1
1
, φ(s, a)⟩≈R(s, a) + Es′∼P (·|s,a) supa′⟨wk,1
1 , φ(s′, a′)⟩.
Therefore, we write wk+1,1
1
= T (wk,1
1 ) + ϵk(wk,1
1 ), where T is the γ contractive Bellman operator
whose unique ﬁxed point is w∗and ϵk is the noise to be controlled. Following a similar setting in
in (Jain et al., 2021b), we control ϵk with the following steps:
(1) We introduce a ﬁctitious coupled process (see Section C) (˜st, ˜at, ˜rt) where the data in different
buffers are exactly independent (since the gaps of size u make the buffers approximately indepen-
dent) and show that the algorithm run with the ﬁctitious data has the same output as the algorithm
run with the actual data with high probability when u is large enough.
(2) We give a bias-variance decomposition (Lemma 5) for the error ϵk where the exponentially de-
caying bias term helps forget the initial condition and the variance term arises due the inherent noise
in the samples.
(3) We control the bias and variance terms separately in order to ensure that the noise ϵk is small
enough.
RER plays a key role in controlling the variance term by endowing it with a super-
martingale structure, which is not possible with forward order traversal (see Theorem 4).
The procedure described above allows us to show that wk+1,1
1
≈T (wk,1
1 ) uniformly for k ≤K,
which directly gives us a convergence bound to the ﬁxed point of T i.e, w∗(Theorem 1). In the
tabular case, the approximate Bellman iteration connects to the analysis of synchronous Q-learning
in (Li et al., 2021), which allows us to obtain a better convergence guarantee (Theorem 2). To obtain
convergence guarantees for Q-RexDaRe, we ﬁrst observe that if we re-use the data used in outer-
loop iteration 1 in all future outer-loop iterations k > 1, ϵk(wk,1
1 ) might not be small since wk,1
1
depends on ϵk(·). However, (wk,1
1 )k approximates the deterministic path of the noiseless Bellman
iterates: ¯w1,1
1
:= w1,1
1
and ¯wk+1,1
1
:= T ( ¯wk,1
1 ). Since ∥ϵk(wk,1
1 )∥∞≤∥ϵk(wk,1
1 ) −ϵk( ¯wk,1
1 )∥∞+
∥ϵk( ¯wk,1
1 )∥∞, we argue inductively that ∥ϵk(wk,1
1 ) −ϵk( ¯wk,1
1 )∥∞≈0 since wk,1
1
≈¯wk,1
1
and
∥ϵk( ¯wk,1
1 )∥∞≈0 since ¯wk,1
1
is a deterministic sequence and hence wk+1,1
1
≈¯wk+1,1
1
.
9

Published as a conference paper at ICLR 2022
ACKNOWLEDGMENTS
Most of this work was done when D.N. was a graduate student at MIT and was supported in part
by NSF grant DMS-2022448. Part of this work was done when D.N. was a visitor at the Simons
Institute for Theory of Computing, Berkeley. We would also like to thank Gaurav Mahajan for
introducing us to low-inherent Bellman error setting, and providing intuition that our technique
might be applicable in this more general setting (than linear MDP) as well.
10

Published as a conference paper at ICLR 2022
REFERENCES
Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a gener-
ative model is minimax optimal. In Conference on Learning Theory, pp. 67–83. PMLR, 2020.
Mohammad Gheshlaghi Azar, R´emi Munos, and Hilbert J Kappen. Minimax pac bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325–349, 2013.
Leemon Baird.
Residual algorithms: Reinforcement learning with function approximation.
In
Machine Learning Proceedings 1995, pp. 30–37. Elsevier, 1995.
Carolyn L Beck and Rayadurgam Srikant. Error bounds for constant step-size q-learning. Systems
& control letters, 61(12):1203–1208, 2012.
Dimitri P Bertsekas. Dynamic programming and optimal control 3rd edition, volume ii. Belmont,
MA: Athena Scientiﬁc, 2011.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A ﬁnite time analysis of temporal difference
learning with linear function approximation. In Conference on learning theory, pp. 1691–1692.
PMLR, 2018.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000.
Justin Boyan and Andrew W Moore. Generalization in reinforcement learning: Safely approximat-
ing the value function. Advances in neural information processing systems, pp. 369–376, 1995.
Guy Bresler, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli, and Xian Wu. Least squares regres-
sion with markovian data: Fundamental limits and algorithms. arXiv preprint arXiv:2006.08916,
2020.
Diogo Carvalho, Francisco S. Melo, and Pedro Santos. A new convergent variant of q-learning with
linear function approximation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19412–19421. Cur-
ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/
file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf.
Zaiwei Chen, Sheng Zhang, Thinh T Doan, John-Paul Clarke, and Siva Theja Maguluri. Finite-
sample analysis of nonlinear stochastic approximation with applications in reinforcement learn-
ing. arXiv preprint arXiv:1905.11425, 2019.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample
analysis of contractive stochastic approximation using smooth convex envelopes. arXiv preprint
arXiv:2002.00874, 2020.
Gal Dalal, Bal´azs Sz¨or´enyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with
function approximation. In Thirty-second AAAI conference on artiﬁcial intelligence, 2018a.
Gal Dalal, Gugan Thoppe, Bal´azs Sz¨or´enyi, and Shie Mannor.
Finite sample analysis of two-
timescale stochastic approximation with applications to reinforcement learning.
In S´ebastien
Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On
Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 1199–1233.
PMLR, 06–09 Jul 2018b. URL https://proceedings.mlr.press/v75/dalal18a.
html.
Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Convergence rates of ac-
celerated markov gradient descent with applications in reinforcement learning. arXiv preprint
arXiv:2002.02873, 2020.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-
learning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020.
11

Published as a conference paper at ICLR 2022
David A Freedman. On tail probabilities for martingales. the Annals of Probability, pp. 100–118,
1975.
Mohammad Ghavamzadeh, Hilbert Kappen, Mohammad Azar, and R´emi Munos.
Speedy q-
learning. Advances in neural information processing systems, 24:2411–2419, 2011.
Sheldon Goldstein. Maximal coupling. Zeitschrift f¨ur Wahrscheinlichkeitstheorie und verwandte
Gebiete, 46(2):193–204, 1979.
Geoffrey J Gordon. Reinforcement learning with function approximation converges to a region.
Advances in neural information processing systems, 13, 2000.
Harsh Gupta,
R. Srikant,
and Lei Ying.
Finite-time performance bounds and adap-
tive learning rate selection for two time-scale reinforcement learning.
In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.),
Advances in Neural Information Processing Systems,
volume 32. Curran Associates,
Inc.,
2019.
URL
https://proceedings.neurips.cc/paper/2019/file/
e354fd90b2d5c777bfec87a352a18976-Paper.pdf.
Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, and Praneeth Netrapalli.
Near-optimal of-
ﬂine and streaming algorithms for learning non-linear dynamical systems.
arXiv preprint
arXiv:2105.11558, 2021a.
Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, and Praneeth Netrapalli. Streaming linear system
identiﬁcation with reverse experience replay. arXiv preprint arXiv:2103.05896, 2021b.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020.
Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect
algorithms. Advances in neural information processing systems, pp. 996–1002, 1999.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier
in model-based reinforcement learning with a generative model. Advances in neural information
processing systems, 33, 2020a.
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous
q-learning: Sharper analysis and variance reduction. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 7031–7043. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.
cc/paper/2020/file/4eab60e55fe4c7dd567a0be28016bff3-Paper.pdf.
Gen Li, Changxiao Cai, Yuxin Chen, Yuantao Gu, Yuting Wei, and Yuejie Chi. Is q-learning mini-
max optimal? a tight sample complexity analysis. arXiv preprint arXiv:2102.06548, 2021.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine learning, 8(3-4):293–321, 1992.
Hamid Reza Maei, Csaba Szepesv´ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy
learning control with function approximation. In Proceedings of the 27th International Con-
ference on International Conference on Machine Learning, ICML’10, pp. 719–726. Omnipress,
2010. ISBN 9781605589077.
Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning
with function approximation. In Proceedings of the 25th international conference on Machine
learning, pp. 664–671, 2008.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015.
R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for ﬁtted value iteration. Journal of Machine
Learning Research, 9(5), 2008.
12

Published as a conference paper at ICLR 2022
Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral meth-
ods. Electronic Journal of Probability, 20:1–32, 2015.
Theodore Perkins and Doina Precup. A convergent form of approximate policy iteration. Advances
in neural information processing systems, 15, 2002.
Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation
and q-learning. In Conference on Learning Theory, pp. 3185–3205. PMLR, 2020.
Martin Riedmiller. Neural ﬁtted q iteration–ﬁrst experiences with a data efﬁcient neural reinforce-
ment learning method. In European conference on machine learning, pp. 317–328. Springer,
2005.
Egor Rotinov. Reverse experience replay. arXiv preprint arXiv:1910.08780, 2019.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Conference on Learning Theory, pp. 2803–2830. PMLR, 2019.
Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9–44, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Csaba Szepesv´ari and William D Smart. Interpolation-based q-learning. In Proceedings of the
twenty-ﬁrst international conference on Machine learning, pp. 100, 2004.
John N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic program-
ming. Machine Learning, 22(1):59–94, 1996.
John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674–690, 1997.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Martin J Wainwright. Stochastic approximation with cone-contractive operators: Sharp ℓ∞-bounds
for q-learning. arXiv preprint arXiv:1905.06265, 2019a.
Martin J Wainwright.
Variance-reduced q-learning is minimax optimal.
arXiv preprint
arXiv:1906.04697, 2019b.
Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019c. doi: 10.1017/
9781108627771.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.
Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.
Bowen Weng, Huaqing Xiong, Lin Zhao, Yingbin Liang, and Wei Zhang. Momentum q-learning
with ﬁnite-sample convergence guarantee. arXiv preprint arXiv:2007.15418, 2020.
Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995–7004. PMLR, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near op-
timal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978–10989. PMLR, 2020.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear func-
tion approximation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso-
ciates, Inc., 2019a. URL https://proceedings.neurips.cc/paper/2019/file/
9f9e8cba3700df6a947a8cf91035ab84-Paper.pdf.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function
approximation. Advances in neural information processing systems, 32, 2019b.
13

Published as a conference paper at ICLR 2022
A
EXPERIMENTS
Even though OTL has a stabilizing effect on the Q-iteration, it reduces the rate of bias decay since
the values are not updated for a long time. Therefore, the success of our procedure depends on
picking the right values for the parameters N, B and Option I vs. Option II. However, under the
right conditions the algorithms which include OTL+RER converge to a much smaller ﬁnal error
as illustrated by the examples we provide in this section. If a better sample complexity is desired,
then Q-RexDaRe can be used as shown by Theorem 3. Further research is needed to identify the
practical conditions such as (function approximation coarseness, MDP reward structure etc.) under
which techniques like OTL+RER help.
100-State Random Walk
We ﬁrst consider the episodic MDP (Sutton & Barto, 2018, example
6.2) – but with 100 states instead of 1000. Here the agent can either move left or right on a straight
line, receiving a reward of 0 at each step. Reaching the right terminal point ends the episode with
reward 1, while the left terminal point ends the episode with reward 0. In each episode, the initial
point is uniformly random and the ofﬂine policy chooses right and left directions uniformly at ran-
dom. We use state aggregation to obtain a linear function representation (Sutton & Barto, 2018)
with total 10 aggregate states. Along with 2 actions, this leads to a 20 dimensional embedding.
We compare vanilla Q-learning, OTL+ER+Q-learning and EpiQ-Rex (Option II, N = 1). The
same step size (0.01) was chosen for all the algorithms. OTL+ER+Q has the same structure as
EpiQ-Rex and was run with the same parameters as EpiQ-Rex . The main difference between the
two algorithms is that OTL+ER+Q processes the experiences collected in each episode in a random
order processing instead of reverse order. Refering to Figure 4, we note that the bias decay for EpiQ-
Rex and OTL+ER+Q are slower than vanilla Q-learning due to the online target structure. However
EpiQ-Rex converges to a better solution than the other algorithms.
Mountain Car
We run an online control type experiment with the Mountain car problem (Sutton
& Barto, 2018, Example 10.1). The task here is to control the car and help it reach the correct peak
(which ends the episode) as soon as possible (i.e, terminate the episode with fewest steps). The agent
receives a reward of −1 unless the correct peak is reached. Here we run EpiQ-Rex with Option I
and N = 1, OTL+ER+Q-learning and vanilla Q-learning. The k-th episode is generated with the
policy at the end of k −1-th episode for each of the three algorithms. We use a n = 4 tile coding
to represent the Q values for a given action, each of which has 4 × 4 squares. The step-size was
picked as 0.1/n and the result was averaged over 500 runs of the experiment. We refer to Figure 2
for the outcomes. For the last 300 episodes, the mean episode length for Vanilla Q-learning was
145, EpiQ-Rex was 136 and OTL+ER+Q-learning was 143 (rounded to the nearest integer).
Grid-World
We consider the grid-world problem which is a tabular MDP (Sutton & Barto, 2018,
Example 3.5), which is a continuing task. Here, an agent can walk north, south, east or west in a
5 × 5 grid. Trying to fall off the grid accrues a reward of −1, while reaching certain special states
accrues a reward of 10 or 5. In our example, we also add a unif[−0.5, 0.5] noise to all rewards to
make the problem harder. We run the grid world experiment with the discount factor γ = 0.9 and
step size 0.05 for Vanilla Q-learning, Q-Rex (Option II, B = 3000, N = 1) and OTL+ER+Q-
learning which is run with the same parameters as Q-Rex but with random samples from the buffer.
We run the experiment 30 times and plot the error in the Q-values vs. time in Figure 3.
Baird’s Counter-Example
Consider the famous Baird’s counter-example shown in Figure 6. The
features corresponding to each state is shown and the reward for any transition is 0. Thus, w∗= 0
is the optimal solution. Since Assumption 4 made in this work is violated for this example, we
consider the analog of the problem where at each step, a state is sampled uniformly at random and
the corresponding transition, along with the 0 reward is used to learn the vector w. In the experiment,
we set the problem and algorithmic parameters to be γ = 0.99 and η = 0.01/
√
5 (the factor of
√
5
normalizes the features to satisfy Assumption 1). We set B = 50, u = 0, and N = 5 since there is
no need for keeping a gap between the buffers in this experiment. Figure 7 shows the non-convergent
behavior of vanilla Q-learning while OTL-based Q-learning converges. Note that since the sampling
of state, action and reward is done in a uniformly random fashion, it is not relevant to use reverse
experience replay. It is easy to see that with data reuse, we only need few samples to ensure all states
are covered; the rate of convergence would be same as that of OTL+Q-learning.
14

Published as a conference paper at ICLR 2022
Figure 2: Mountain Car problem
Figure 3: Grid-world problem
Figure 4: 100 State Random Walk
Figure 5: Linear System Problem
Figure 6: Feature embedding in the modiﬁed
Baird’s counter-example; ⃗ei represents the i-
th canonical basis vector in R7. Each transi-
tion shown occurs with probability 1.
Figure 7: Performance of vanilla Q-learning
as compared to OTL+Q-learning, averaged
over 10 independent runs
Linear Dynamical System with Linear Rewards
We also compare the algorithms on a linear
dynamical systems problem. While the problem described next is strictly not a linear MDP, the
value functions for certain policies can be written as a linearly in terms of the initial condition; this
is made precise next. Consider a linear dynamical system with initial state being X0 ∈Rd. The
state evolves as Xt+1 = AXt + ηt, where A ∈Rd×d. The reward obtained for such transition
is given by ⟨Xt, θ⟩for a ﬁxed θ ∈Rd. The maximum singular value of A is chosen less than 1
to ensure that the system is stable. The inﬁnite horizon γ-discounted expected reward is given by
⟨X0, P∞
i=0(γAT )iθ⟩. Thus, the expected reward can be written as ⟨X0, w∗⟩, where
w∗= (I −γAT )−1θ.
Since there are no actions in this case, the Q learning algorithms reduce to value function approx-
imation (i.e, TD(0) type algorithms). We take the embedding φ(Xt) = Xt, the identity mapping.
We considered d = 5, γ = 0.99, η = 0.01, and a randomly generated normal matrix A and θ. We
Option II for Q-Rex along with B = 75, u = 25, and N = 5 for the experiments. For OTL+ER+Q-
learning, we keep the same parameters, but with random order sampling, while ER+Q-learning does
not include OTL. The results shown in Figure 5 are averaged over 100 independent runs of the
experiment. Note that the errors considered are using iterate averaging.
15

Published as a conference paper at ICLR 2022
As seen from Figure 5, Q-Rex outperforms vanilla Q-learning and OTL+ER-based Q-learning as
one would expect based on the theory presented in this work. However, it is interesting to note that
ER+Q-learning performs slightly better than Q-Rex. One possible reason could be due to the fact
that in Q-Rex, the target gets updated at a slower rate at the cost of reducing bias. However, setting
a smaller value of NB might resolve the issue.
B
DEFINITIONS AND NOTATIONS
B.1
Q-REXDARE
The psuedocode for Q-RexDaRe is given in Algorithm 2. Note that we reuse the sample data in
every outer-loop iteration instead of drawing fresh samples.
Algorithm 2 Q-RexDaRe
1: Input: learning rates η, horizon T, discount factor γ, trajectory Xt = {st, at, rt}, number of
outer-loops K buffer size B, buffer gap u
2: Total buffer size: S ←B + u
3: Number of buffers loops: N ←T/S
4: w1,1
1
= 0 initialization
5: for k = 1, . . . , K do
6:
for j = 1, . . . , N do
7:
Form buffer Buf = {Xk,j
1 , . . . , Xk,j
S }, where,
Xk,j
i
←XS(j−1)+i
8:
Deﬁne for all i ∈[1, S], φk,j
i
≜φ(sk,j
i
, ak,j
i
).
9:
for i = 1, . . . , B do
10:
wk,j
i+1 = wk,j
i
+η
h
rk,j
B+1−i + γ maxa′∈A⟨φ(sk,j
B+2−i, a′), wk,1
1 ⟩−⟨φk,j
B+1−i, wk,j
i
⟩
i
φk,j
B+1−i
11:
wk,j+1
1
= wk,j
B+1
12:
wk+1,1
1
= wk,N+1
1
13: Return wT +1
0
B.2
EPIQ-REX
The psuedocode for EpiQ-Rex is given in Algorithm 3. Note that the buffers here are individual
episodes and can vary in size due to inherent randomness. We do not require a gap here since
separate episodes are assumed to be independent.
MDP deﬁnition
Here we construct the MDP with a (possibly) random reward r. We consider
non-episodic, i.e. inﬁnite horizon, time homogenous Markov Decision Processes (MDPs) and we
denote an MDP by MDP(S, A, γ, P, r) where S denotes the state space, A denotes the action space,
γ represents the discount factor, P(s′|s, a) represents the probability of transition to state s′ from
the state s on action a. We assume for purely technical reasons that S and A are compact subsets
of Rn for some n ∈N). r is a reward process (not to be confused with Markov Reward Processes
i.e, MRP) indexed by S × A × S, such that r(s, a, s′) ∈[0, 1] almost surely. We will skip the
measure theoretic details of the deﬁnitions and assume that the sequence of i.i.d. reward processes
(rt)t∈N can be jointly deﬁned over a Polish probability space. The function R : S × A →[0, 1]
represents the deterministic reward R(s, a) obtained on taking action a at state s and is deﬁned by
R(s, a) = E

Es′∼P (·|s,a)r(s, a, s′)

. Here the expectation is with respect to both the randomness
in the reward process and in state s′.
Now, given a trajectory, (st, at)T
t=1, which is independent of i.i.d sequence of rewards processes
(rt)T
t=1. We observe (st, at, rt(st, at, st+1))T
t=1, and we will henceforth denote rt(st, at, st+1) by
just rt.
16

Published as a conference paper at ICLR 2022
Algorithm 3 EpiQ-Rex
1: Input:
learning rates η, number of episodes T, discount factor γ, Epsiodes Et
=
{(st
1, at
1, rt
1), . . . , (sy
Bt, at
Bt, rt
Bt)}, Number of buffer per outer-loop iteration N.
2: Number of outer loop iterations: K ←T/N
3: w1,1
1
= 0 initialization
4: for k = 1, . . . , K do
5:
for j = 1, . . . , N do
6:
t ←(k −1) ∗N + j
7:
Collect experience and form buffer Buf = {Xt
1, . . . , Xt
Bt}, where,
Xt
i = (st
i, at
i, rt
i)
8:
Deﬁne for all i ∈[1, Bt −1], φk,j
i
≜φ(sk,j
i
, ak,j
i
).
9:
for i = 1, . . . , B do
10:
wk,j
i+1 = wk,j
i
+η
h
rk,j
Bt−i + γ maxa′∈A⟨φ(sk,j
Bt+1−i, a′), wk,1
1 ⟩−⟨φk,j
Bt−i, wk,j
i
⟩
i
φk,j
Bt−i
11:
Option I: wk,j+1
1
= wk,j
Bt
12:
Option II:wk,j+1
1
=
1
Bt−1
PBt
i=2 wk,j
i
13:
wk+1,1
1
= wk,N+1
1
14: Return wK,1
1
Notation
Due to three loop nature of our algorithm it will be convenient to deﬁne some simplifying
notation. To this end, consider the outer loop with index k ∈[K] and buffer number j ∈[N] inside
this outer loop. Further given an i ∈[S] deﬁne a time index as tk,j
i
= NS(k −1) + S(j −1) + i.
We now denote the i-th state-action-reward tuple inside this buffer by
(sk,j
i
, ak,j
i
, rk,j
i
) = (stk,j
i , atk,j
i , rtk,j
i ).
Similarly, Rk,j
i
= R(sk,j
i
, ak,j
i
). For conciseness, we deﬁne for all i, j, k, φk,j
i
:= φ(sk,j
i
, ak,j
i
).
Since we are processing the data in the reverse order within the buffer, the following notation will
be useful for analysis:
sk,j
−i := sk,j
B+1−i, ak,j
−i := ak,j
B+1−i, rk,j
−i := rk,j
B+1−i, Rk,j
−i := Rk,j
B+1−i and φk,j
−i := φk,j
B+1−i.
C
COUPLED PROCESS
It can be seen that the buffers are approximately i.i.d. whenever we take u = O(τmix log T
δ ) when-
ever Assumption 3 is satisﬁed. For the sake of clarity of analysis, we will consider exactly inde-
pendent buffers. That is, we assume the algorithms are run with a ﬁctitious trajectory (˜st, ˜at, ˜rt),
where we assume that the ﬁctitious trajectory is generated such that the ﬁrst state of every buffer is
sampled from the stationary distribution µ. We show that we can couple this ﬁctitious process (i.e,
deﬁne it on a common probability space as the original process (st, at, rt)) such that
P

∩K
k=1 ∩N
j=1 ∩B+1
i=1 {(sk,j
i
, rk,j
i
, ak,j
i
) = (˜sk,j
i
, ˜rk,j
i
, ˜ak,j
i
)}

≥1 −δ .
(4)
Notice that the equality does not hold within the gaps between the buffer which are of size u but
inside the buffers of size B only. That is, the sequence of iterates obtained by running the algorithm
with the original data (st, at, rt) is the same as the sequence of iterates obtained by running the algo-
rithm with the ﬁctitious coupled data (˜st, ˜at, ˜rt) with high probability. We state this result formally
in Lemma 16 and prove it in Section M.1. Henceforth, we will assume that we run the algorithm
with data (˜st, ˜at, ˜rt) and refer to Lemma 16 to carry over the results to the original data set with
high probability. We analogously deﬁne ˜φk,j
i
. We will denote the iterates of the algorithm run with
the coupled trajectory as ˜wk,j
i
instead of wk,j
i
and will focus on it entirely. We now provide some
deﬁnitions based on the above process. These deﬁnitions will be used repeatedly in our analysis.
17

Published as a conference paper at ICLR 2022
D
BASIC STRUCTURAL LEMMAS
We ﬁrst note some basic structural lemmas regarding ZIBEL MDPs under the assumptions in Sec-
tion 2.1. We refer to Section M for the proofs.
Lemma 1. For tabular MDPs satisfying the assumptions in Section 2.1, for both Q-Rex and Q-
RexDaRe, we have that for every k, j, i we have that ∥˜wk,j
i
∥φ ≤
1
1−γ
The lemma above says, in particular, that the Q-value estimate given by our algorithm never exceeds
1
1−γ due to 0 initialization. The proof is a straightforward induction argument, which we omit. We
will henceforth use Lemma 1 without explicitly mentioning it.
Lemma 2. Suppose Qw(s, a) := ⟨w, φ(s, a)⟩and let Q∗(s, a) = ⟨w∗, φ(s, a)⟩be the optimal Q
function. Then:
sup
(s,a)∈S×A
|Qw(s, a) −Q∗(s, a)| = ∥w −w∗∥φ
Moreover, we must have: ∥x∥≥∥x∥φ ≥∥x∥
√κ for any x ∈Rd.
Lemma 3. For any w0 ∈Rd, there exists a unique w1 ∈Rd such that
⟨w1, φ(s, a)⟩= R(s, a) + γEs′∼P (·|s,a) sup
a′∈A
⟨φ(s′, a′), w0⟩.
We will denote this mapping w0 →w1 by w1 = T (w0).
Lemma 4. T : Rd →Rd is γ contractive in the norm ∥· ∥φ. The unique ﬁxed point of T is w∗.
Moreover, we have: ∥w∗∥φ ≤
1
1−γ and ∥w∗∥≤
√κ
1−γ .
In view of Lemma 4, we can begin to look at the following noiseless Q-iteration. Let ¯w1 = w1,1
1
= 0
and ¯wk+1 = T ( ¯wk). This converges geometrically to w∗with contraction coefﬁcient γ under the
norm ∥· ∥φ. In our case, however, we only have sample access to the operator T . Therefore, our
Q-iteration at the end of k-th outer loop can be written as ˜wk+1,1
1
= T ( ˜wk,1
1 ) + ˜ϵk where ˜ϵk is the
error introduced via sampling which needs to be controlled.
E
BIAS VARIANCE DECOMPOSITION
We begin our analysis by providing a bias-variance decomposition of the error with respect to the
noiseless Q-iteration at every loop. We will need the following deﬁnitions which we use repeatedly
through our analysis.
Given step size η, outer loop index k and buffer index j, we deﬁne the following contraction matrices
for a, b ∈[B].
˜Hk,j
a,b :=
b
Y
i=a

I −η ˜φk,j
i
[˜φk,j
i
]⊤
.
(5)
Whenever a > b, we will deﬁne ˜Hk,j
a,b := I. In the tabular setting, we deﬁne for any (s, a) ∈S × A
by ˜N k(s, a) to be the number of samples of (s, a) seen in the outer loop k (excluding the gaps),i.e.
˜N k(s, a) =
N
X
j=1
B
X
i=1
1((˜sk,j
i
, ˜ak,j
i
) = (s, a))
Further we denote by ˜N k,j
i
(s, a), the number of samples of (s, a) seen in the outer loop k in the
buffers post the buffer j as well as the number of samples of (s, a) in buffer j before iteration i.
Formally,
˜N k,j
i
(s, a) :=
i−1
X
r=1
1
 (˜sk,j
r , ˜ak,j
r ) = (s, a)

+
N
X
l=j+1
B
X
r=1
1
 (˜sk,l
r , ˜ak,l
r ) = (s, a)

.
18

Published as a conference paper at ICLR 2022
We deﬁne the error term for any w:
˜ϵk,j
i
(w) :=

˜rk,j
i
−˜Rk,j
i
+ γ sup
a′∈A
⟨w, φ(˜sk,j
i+1, a′)⟩−γEs′∼P (·|˜sk,j
i
,˜ak,j
i
) sup
a′∈A
⟨φ(s′, a′), w⟩

. (6)
Finally we deﬁne the following shorthands for any i, j, k:
˜wk+1,∗:= T ( ˜wk,1
1 )
˜ϵk,j
i
:= ˜ϵk,j
i
( ˜wk,1
1 )
˜Lk,j := η
B
X
i=1
˜ϵk,j
i
˜Hk,j
1,i−1 ˜φk,j
i
.
Given the above deﬁnition, the following the lemma provides the Bias-Variance decomposition
which is the core of our analysis.
Lemma 5 (Bias-Variance Decomposition). For every k, we have that,
˜ϵk := ˜wk+1,1
1
−˜wk+1,∗=
1
Y
j=N
˜Hk,j
1,B( ˜wk,1
1
−˜wk+1,∗) +
N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j
(7)
Here and later on in the paper, we use the reverse order in the product to highlight the convention that
higher indices l in ˜Hk,l
1,B appear towards the left side of the product and further deﬁne QN+1
l=N ˜Hk,l
1,B =
I. We call the ﬁrst term in Equation (7) as the bias term and it decays geometrically with N and
the second term is called the variance, which has zero mean. We will bound these terms separately.
Since tabular setting allows for improved analysis of error, we will provide special cases for the
tabular setting with reﬁned bounds. We refer to Section M.5 for the proof of Lemma 5.
F
BOUNDING THE BIAS TERM
F.1
TABULAR CASE
In the tabular case, we have the following expression for bias. We omit the proof since it follows
from a simple calculation.
Lemma 6. In the tabular setting, we have:
⟨φ(s, a),
1
Y
j=N
˜Hk,j
1,B( ˜wk,1
1
−˜wk+1,∗)⟩= (1 −η)
˜
Nk(s,a)⟨˜wk,1
1
−˜wk+1,∗, φ(s, a)⟩
For a particular outer loop k, we show that ˜N k(s, a) is Ω(µminNB) with high probability whenever
NB is large enough. For this we use (Paulin, 2015, Theorem 3.4) similar to the proof of (Li et al.,
2020b, Lemma 8). We give the proof in Section M.6.
Lemma 7. There exists a constant C such that whenever B ≥τmix and NB ≥C τmix
µmin log( |S||A|
δ
),
with probability at least 1 −δ, we have that for every (s, a) ∈S × A:
˜N k(s, a) ≥1
2µ(s, a)NB ≥1
2µminNB
F.2
ZIBEL MDP CASE
Lemma 8. Suppose g ∈Rd is ﬁxed and ηB < 1
4. Then, the following hold:
1.
E∥
1
Y
j=N
˜Hk,j
1,Bg∥2 ≤exp(−ηNB
κ )∥g∥2
(8)
2. With probability at-least 1 −δ, we have:
E∥
1
Y
j=N
˜Hk,j
1,Bg∥φ ≤exp(−ηNB
κ )
rκ
δ ∥g∥φ
We refer to Section L.1 for the proof.
19

Published as a conference paper at ICLR 2022
G
BOUNDING THE VARIANCE TERM
G.1
TABULAR CASE
In the tabular case, it is clear that:
⟨φ(s, a),
N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j⟩=
N
X
j=1
B
X
i=1
(1 −η)
˜
Nk,j
i
(s,a)η˜ϵk,j
i
1((˜sk,j
i
, ˜ak,j
i
) = (s, a))
(9)
Now, we note that ˜N k,j
i
(s, a) depends on data in buffers l > j and when i1 < i inside buffer j.
Following the discussion in (Li et al., 2021), we deﬁne the vector VarP (Vk), VarP (V ∗) ∈RS×A
such that
VarP (Vk)(s, a) = Es′∼P (·|s,a)

sup
a′∈A
⟨φ(s′, a′), ˜wk,1
1 ⟩−Es′∼P (·|s,a) sup
a′∈A
⟨φ(s′, a′), ˜wk,1
1 ⟩
2
(10)
VarP (V ∗)(s, a) = Es′∼P (·|s,a)

sup
a′∈A
⟨φ(s′, a′), w∗⟩−Es′∼P (·|s,a) sup
a′∈A
⟨φ(s′, a′), w∗⟩
2
(11)
More generally, we deﬁne
VarP (V )(s, a; w) = Es′∼P (·|s,a)

sup
a′∈A
⟨φ(s′, a′), w⟩−Es′∼P (·|s,a) sup
a′∈A
⟨φ(s′, a′), w⟩
2
Similarly, we deﬁne ˜Lk,j(w) by replacing wk,1
1
with any ﬁxed, arbitrary w. It is easy to see that:
E

|˜ϵk,j
i
(w)|2
(˜sk,j
i
, ˜ak,j
i
) = (s, a)

≤2(1 + γ2VarP (V )(s, a; w))
(12)
Lemma 9. In the tabular setting, suppose w is a ﬁxed vector such that ⟨w, φ(s, a)⟩∈[0,
1
1−γ ] for
every (s, a) ∈S × A. Fix (s, a) ∈S × A. Then there exists a universal constant C such that with
probability atleast 1 −δ, we have that:
Then,
⟨φ(s, a),
N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j(w)⟩
 ≤C
p
η log(2/δ)(1 + γ2VarP (V )(s, a; w)) + C η log(2/δ)
1 −γ
G.2
ZIBEL MDP CASE
We will use an appropriate exponential super-martingale to bound the error term in the ZIBEL MDP
case just like in the proof of (Jain et al., 2021b, Lemma 27). The following thereom summarizes the
result and we refer to Section L.3 for its proof.
Theorem 4. Suppose x, w ∈Rd are ﬁxed. Then, there exists a universal constant C such that with
probability at least 1 −δ, we have:
⟨x,
N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j(w)⟩
 ≤C∥x∥(1 + ∥w∥φ)
p
η log(2/δ) .
(13)
By a direct application of (Vershynin, 2018, Theorem 8.1.6), we derive the following corollary. We
remind the reader that CΦ is the covering number deﬁned in Deﬁnition 3.
Corollary 1. Suppose x, w ∈Rd are ﬁxed. Then, there exists a universal constant C such that with
probability at least 1 −δ, we have:

N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j(w)

φ
≤C(1 + ∥w∥φ)√η

CΦ +
q
log( 2
δ )

.
20

Published as a conference paper at ICLR 2022
In order to apply the theorem above, we will need to control ∥wk,1
1 ∥φ uniformly for all k. The
following lemma presents such a bound and we refer to Section L.4 for the proof.
Lemma 10. Suppose ˜wk,1
1
are the iterates of Q-Rex with coupled data from a ZIBEL MDP.
There exist universal constants C, C1, C2 such that whenever NB > C1 κ
η log

Kκ
δ(1−γ)

, η <
C2
(1−γ)2
C2
φ+log(K/δ) and ηB < 1
4, with probability at-least 1 −δ, the following hold:
1. For every 1 ≤k ≤K, ∥˜wk,1
1 ∥φ ≤
4
1−γ
2. For every 1 ≤k ≤K, ∥˜ϵk∥φ ≤
q
25Kκ
δ(1−γ)2 exp(−ηNB
κ ) + C√η
(1−γ)
h
CΦ +
q
log( 2K
δ )
i
H
PROOF OF THEOREM 1
Proof. Consider the Q learning iteration:
˜wk+1,1
1
= T ( ˜wk,1
1 ) + ˜ϵk .
Using Lemma 4, we conclude: ˜wk+1,1
1
−w∗= T ( ˜wk,1
1 ) −T (w∗) + ϵk and thence:
∥˜wk+1,1
1
−w∗∥φ ≤γ∥˜wk,1
1
−w∗∥φ + sup
l≤K
∥ϵl∥φ .
Unrolling the recursion above, we conclude:
∥˜wK+1,1
1
−w∗∥φ ≤γK∥w∗∥φ + supl≤K ∥ϵl∥φ
1 −γ
Now, we invoke item 2 of Lemma 10 along with the constraints on N, B, K, η and Lemma 2 to
conclude the result.
I
PROOF OF THEOREM 2
In this section we analyze the output of Q-Rex in the tabular setting and obtain convergence guar-
antees. To connect with the standard theory for tabular MDP Q-learning in (Li et al., 2021), let
us use the standard Q-function notation where we assume for all k, ˜Qk,1
1
∈RS×A and we have
that ˜Qk,1
1 (s, a) = ⟨˜wk,1
1 , φ(s, a)⟩.
Since φ(s, a) are the standard basis vectors, we must have
˜Qk,1
1
= ˜wk,1
1 . In the tabular setting, we see by using Lemmas 5, 6, 7, and 9 that for any δ > 0,
there exists a universal constant C, whenever NB ≥C τmix
µmin log( |S||A|K
δ
), with probability at-least
1 −δ, for every k ∈[K] and every (s, a) ∈S × A:
˜Qk+1,1
1
= T
h
˜Qk,1
1
i
+ ˜ϵk ,
(14)
where ˜ϵk ∈RS×A is such that for all (s, a),
|˜ϵk(s, a)| ≤C
r
η log

K|S||A|
δ

(1 + γ2VarP (V )(s, a; ˜Qk,1
1 )) + C η log( K|S||A|
δ
)
1 −γ
+ (1 −η)
µminNB
2
 ˜Qk,1
1
−T
h
˜Qk,1
1
i
∞
.
(15)
Now that we have set-up the notation, we will roughly follow the analysis methods used in (Li et al.,
2021). Since we start our algorithm with ˜Q1,1
1
= 0, and rt ∈[0, 1] almost surely, we can easily show
that ˜Qk,1
1 (s, a) ∈[0,
1
1−γ ] for every k, s, a. Therefore, we upper bound
 ˜Qk,1
1 −T
h
˜Qk,1
1
i
∞
≤
1
1−γ
21

Published as a conference paper at ICLR 2022
in Equation (15) to conclude that with probability at-least 1 −δ, for every k ∈[K] and every
(s, a) ∈S × A:
|˜ϵk(s, a)| ≤C
q
η log( K|S||A|
δ
)γ2 [VarP (Vk)(s, a)] + αη
(16)
Where αη := C
η log
  K|S||A|
δ

1−γ
+ C
r
η log

K|S||A|
δ

+
exp
 −ηµminNB
2

1−γ
Now we deﬁne ∆k =
˜Qk,1
1
−Q∗and πk : S →A to be the deterministic policy given
by Qk,1
1
i.e, πk(s) := arg supa∈A ˜Qk,1
1 (s, a) and π∗to be optimal policy given by π∗(s) :=
arg supa∈A Q∗(s, a). We use the convention that we pick a single maximizing action using some
rule whenever there are multiple. Similarly, we let P πk, to be the Markov transition kernel over
S × A given by P πk((s, a), (s′, a′)) = P(s′|s, a)1(a′ = πk(s′)). Similarly, we deﬁne P π∗with
respect to the policy π∗. It is easy to show that:
T ( ˜Qk,1
1 ) = R + γP πk ˜Qk,1
1
.
Similarly,
Q∗= T (Q∗) = R + γP π∗Q∗.
Furthermore given any Q ∈RS×A, letting πQ being the greedy policy with respect to the function Q
we have that for any policy π, it can be easily seen from the deﬁnitions that following element-wise
inequality follows:
P πQ ≤P πQQ.
We now use Equation (14) along with the equations above to conclude:
γP π∗∆k + ˜ϵk ≤∆k+1 ≤γP πk∆k + ˜ϵk .
(17)
Here, the inequality is assumed to be point-wise. By properties of Markov transition kernels, we can
write:
K
X
k=1
γK−k 
P π∗K−k
˜ϵk + γK 
P π∗K
∆1 ≤∆K+1
≤
K
X
k=1
γK−k
 k+1
Y
l=K
P πl
!
˜ϵk + γK
 
1
Y
l=K
P πl
!
∆1 .
(18)
Here, we use the convention that QK+1
l=K P πl = I. We bound the lower bound and the upper bound
given in Equation (18) separately in order to bound ∥∆K+1∥∞.
We ﬁrst consider the lower bound. Using (Azar et al., 2013, Lemma 7), we have:
(I −γP π∗)−1p
VarP (V ∗)

∞≤
s
2
(1 −γ)3 .
(19)
We also note from (Li et al., 2021, Equation 64) and basic calculations that:
∥
p
VarP (Vk) −
p
VarP (V ∗)∥∞≤
p
∥VarP (Vk) −VarP (V ∗)∥∞
≤
r
4
1 −γ ∥∆k∥∞.
(20)
22

Published as a conference paper at ICLR 2022
Using Equations (15) (16), we conclude that there exist universal constants C, C1 such that with
probability at least 1 −δ (interpreting the inequalities as element-wise):
∆K+1 ≥−αη + γK
1 −γ
−C
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γK−k 
P π∗K−k p
VarP (Vk)
≥−αη + γK
1 −γ
−C
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γK−k 
P π∗K−k p
VarP (V ∗)
−C1
r
1
1 −γ
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γK−kp
∥∆k∥∞
≥−αη + γK
1 −γ
−C
q
ηγ2 log
  K|S||A|
δ

(I −γP π∗)−1p
VarP (V ∗)
−C1
r
1
1 −γ
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γK−kp
∥∆k∥∞
≥−αη + γK
1 −γ
−C
s
ηγ2
(1 −γ)3 log
  K|S||A|
δ

−C1
r
1
1 −γ
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γK−kp
∥∆k∥∞.
(21)
In the above chain, the ﬁrst inequality follows from Equations (16) (18), the second inequality from
Equation (20) and the fourth inequality from Equation (19). For the upper bound consider the
following set of equations interpreting them element-wise which hold for a universal constant C and
with probability at least 1 −δ.
∆K+1 ≤
K
X
k=1
γK−k
 k+1
Y
l=K
P πl
!
˜ϵk + γK
 
1
Y
l=K
P πl
!
∆1
≤αη + γK
1 −γ
+ C
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γK−k
 k+1
Y
l=K
P πl
!
p
VarP (Vk)
≤αη + γK
1 −γ
+ C
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γK−k
v
u
u
t
 k+1
Y
l=K
P πl
!
VarP (Vk)
= αη + γK
1 −γ
+ C
q
ηγ2 log
  K|S||A|
δ
 K
X
k=1
γ
K−k
2
γ
K−k
2
v
u
u
t
 k+1
Y
l=K
P πl
!
VarP (Vk)
≤αη + γK
1 −γ
+ C
q
ηγ2 log
  K|S||A|
δ

v
u
u
t
K
X
k=1
γK−k
v
u
u
t
K
X
k=1
γK−k
 k+1
Y
l=K
P πl
!
VarP (Vk)
≤αη + γK
1 −γ
+ C
s
ηγ2
1 −γ log
  K|S||A|
δ

v
u
u
t
K
X
k=1
γK−k
 k+1
Y
l=K
P πl
!
VarP (Vk)
≤αη + γK
1 −γ
+ C
s
ηγ2
1 −γ log
  K|S||A|
δ

v
u
u
t
K
X
k=K/2+1
γK−k
 k+1
Y
l=K
P πl
!
VarP (Vk) +
γK/2
(1 −γ)3 .
(22)
In the above chain, the ﬁrst inequality follows from Equation (18), the second inequality follows
from Equation (16), the third inequality follows from Jensen’s inequality and noting that P π is a
Markov operator, the fourth inequality via Cauchy-Schwartz and the last inequality by noting that
that ∥VarP (Vk)∥∞≤1/(1 −γ)2.
23

Published as a conference paper at ICLR 2022
It can now be veriﬁed that (Li et al., 2021, Lemma 5) applies in our setting to conclude that:
K
X
k=K/2+1
γK−k
 k+1
Y
l=K
P πl
!
VarP (Vk) ≤
4
γ2(1 −γ)2

1 + 2
max
K/2+1≤k≤K ∥∆k∥∞

.
Using the equation above, along with Equations (22) and (21), we conclude there exists a universal
constant C such that with probability at least 1 −δ, we have:
∥∆K+1∥∞≤αη + γK
1 −γ
+C
r
η
(1 −γ)3 log
  K|S||A|
δ

s
1 +
max
K/2+1≤k≤K ∥∆k∥∞+
γK/2
(1 −γ) (23)
We note that this works with every K replaced with l for any l ≤K, importantly under the
same event (with probability at least 1 −δ) described above for which Equation (23) holds. De-
ﬁne L =
K
2s for some s = ⌈log2(1 + C log(
1
1−γ ))⌉. Under the conditions of the Theorem, i.e,
K > C2
1
(1−γ)

log(
1
1−γ )
2
, we have γL/2
1−γ < 1. Therefore we conclude from the discussion above
that for every K ≥l ≥L, we must have:
∥∆l+1∥∞≤αη + γL
1 −γ
+ C
r
η
(1 −γ)3 log
  K|S||A|
δ
r
1 +
max
l/2+1≤k≤l ∥∆k∥∞
≤αη + γL
1 −γ
+ C
s
η log
  K|S||A|
δ

(1 −γ)3
+ C
s
η log
  K|S||A|
δ

(1 −γ)3
r
max
l/2+1≤k≤l ∥∆k∥∞
(24)
To analyze this recursion, we have the following lemma which establishes hyper-contractivity,
whose proof we defer to Section M.8.
Lemma 11. Suppose α, β ≥0. Consider the function f : R+ →R+ given by f(u) = α + β√u.
Then, f has the unique ﬁxed point: u∗:=

β+√
β2+4α
2
2
. For t ∈N, denoting f (t) to be the t fold
composition of f with itself, we have for any u ∈R+:
|f (t)(u) −u∗| ≤β(2−
1
2t−1 )|u −u∗|
1
2t .
Now consider for 0 ≤a < s,
ua :=
sup
K
2s−a ≤l≤K
∥∆l+1∥∞
In lemma 11, deﬁne f with α = αη+γL
1−γ
+ C
r
η log
  K|S||A|
δ

(1−γ)3
and β = C
r
η log
  K|S||A|
δ

(1−γ)3
. The
ﬁxed point u∗is such that:
u∗≤C

η log
  K|S||A|
δ

(1 −γ)3
+ αη + γL
1 −γ
+
s
η log
  K|S||A|
δ

(1 −γ)3


(25)
Clearly, by Equation (24), we have: ua ≤f(ua−1) and ∥∆K+1∥≤f(us−1). By monotonicity of
f(·) and the fact that u0 ≤
1
1−γ (Lemma 1):
us−1 ≤f (s−1)(u0) ≤f (s−1) 
1
1−γ

.
Therefore,
∥∆K+1∥∞≤f(us−1) ≤f (s) 
1
1−γ

24

Published as a conference paper at ICLR 2022
Applying Lemma 11, we conclude:
∥∆K+1∥∞≤u∗+ β(2−
1
2s−1 )
1
1−γ −u∗
1
2s
(26)
Note that under the constraints on the parameter η and K as stated in the Theorem, we must have
u∗≤
1
(1−γ)3 . By our choice of s, we must have: |
1
(1−γ) −u∗|
1
2s ≤C′. Using this in Equation 26
and the fact that β(2−
1
2s−1 ) ≤β + β2, we conclude that with probability at-least 1 −δ, we must
have:
∥∆K+1∥≤C

η log
  K|S||A|
δ

(1 −γ)3
+ αη + γL
1 −γ
+
s
η log
  K|S||A|
δ

(1 −γ)3


This proves the ﬁrst part of the theorem. For the second part, we directly substitute the values
provided to verify that we indeed obtain ϵ error.
J
PROOF OF THEOREM 3
We will now show uniform convergence type result under Assumption 5. For (s, a) ∈S × A and
s′ ∈supp(P(·|s, a)). We deﬁne the random variables for all k:
ˆPk(s′|s, a) = η
N
X
j=1
B
X
i=1
(1 −η)
˜
Nk,j
i
(s,a)1(˜sk,j
i+1 = s′, ˜sk,j
i
= s, ˜ak,j
i
= a)
¯Pk(s′|s, a) := η
N
X
j=1
B
X
i=1
(1 −η)
˜
Nk,j
i
(s,a)P(s′|s, a)1(˜sk,j
i
= s, ˜ak,j
i
= a).
Lemma 12. Suppose Assumption 5 holds. Then, with probability at-least 1 −δ, we must have for
any ﬁxed (s, a):
X
s′∈supp(P (·|s,a))
| ˆPk(s′|s, a) −¯Pk(s′|s, a)| ≤C
q
η ¯d log(4/δ) + Cη ¯d log(4/δ)
We refer to Section L.5 for the proof. We now proceed with the proof of Theorem 3. Recall the
noiseless iteration ¯wk deﬁned in the discussion following the statement of Lemma 4. We deﬁne
¯Dk := wk,0
0
−¯wk. Observe that we cannot apply Lemma 9 as in the proof of Theorem 2 where
we used w = wk,0
0
in order to bound ∥ϵk∥∞. This is because of data re-use which causes wk,0
0
to
depend on the ‘variance’ term.
However, note that ¯wk is a deterministic sequence and we can apply Lemma 9 and then use the fact
that ¯wk ≈wk,0
0
to show a similar concentration inequality. To this end, we prove the following
lemma:
Lemma 13. In the tabular setting, we have almost surely:


φ(s, a),
N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j(w) −
N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j(v)

≤γ∥w −v∥φ
X
s′∈supp(P (·|s,a))
| ˆPk(s′|s, a) −¯Pk(s′|s, a)|
(27)
The proof follows from elementary arguments via. the mean value theorem and triangle inequality.
We refer to Section M.7 for the proof. We are now ready to give the proof of Theorem 3.
Proof of Theorem 3. We proceed with a similar setup as the proof of Theorem 2. Notice that due to
data reuse, the noise ϵk in outer loop k is given by ϵ1( ˜wk,1
1 )
Qk+1,1
1
= T
h
Qk,1
1
i
+ ϵ1( ˜Qk,1
1 ) .
25

Published as a conference paper at ICLR 2022
We also deﬁne the noiseless Q iteration such that ¯Q1,1
1
= ˜Q1,1
1
and ¯Qk+1,1
1
= T ( ¯Qk,1
1 ). Let ¯∆k :=
˜Qk,1
1
−¯Qk,1
1 .
Now, by Lemma 5, we can write down
ϵ1( ˜Qk,1
1 )(s, a) = (1 −η)
˜
N1(s,a)( ˜Qk,1
1
−˜Qk,∗) + ⟨φ(s, a),
N
X
j=1
j+1
Y
l=N
˜H1,l
1,B ˜L1,j( ˜Qk,1
1 )⟩
= (1 −η)
˜
N1(s,a)( ˜Qk,1
1
−˜Qk,∗)+

φ(s, a),
N
X
j=1
j+1
Y
l=N
˜H1,l
1,B

˜L1,j( ˜Qk,1
1 ) −˜L1,j( ¯Qk,1
1 )

+

φ(s, a),
N
X
j=1
j+1
Y
l=N
˜H1,l
1,B ˜L1,j( ¯Qk,1
1 )

(28)
We bound each of the terms above separately. By union bound, the following statements all hold
simulataneously with probability at-least 1−δ. By Theorem 7, whenever NB > C τmix
µmin log( |S||A
δ
),
we must have
˜N (1)(s, a)
≥
µmin
2 NB.
A simple observation using recursion shows that
˜Qk,1
1 (s, a), ¯Qk,1
1 (s, a) ∈[0,
1
1−γ ]. Using Lemmas 12 and 13 we conclude that we must have:
sup
(s,a)∈S×A


φ(s, a),
N
X
j=1
j+1
Y
l=N
˜H1,l
1,B

˜L1,j( ˜Qk,1
1 ) −˜L1,j( ¯Qk,1
1 )

≤C∥¯∆k∥∞
q
η ¯d log
  |S||A|
δ

+ η ¯d log
  |S||A|
δ

(29)
Now observe that ¯Qk,1
1
is a deterministic sequence. Therefore, using Lemma 9 uniformly for every
k ≤K and (s, a) ∈S × A:


φ(s, a),
N
X
j=1
j+1
Y
l=N
˜H1,l
1,B ˜L1,j( ¯Qk,1
1 )
 ≤C
q
η log
  |S||A|K
δ

(1 + VarP ( ¯Vk)(s, a)) + C η log
  |S||A|K
δ

1 −γ
(30)
Where, VarP ( ¯Vk) := VarP (V )(s, a; ¯Qk,1
1 ).
We now combine all the above with Equation (28) to show that with probability at least (1 −δ), we
must have uniformly for every k ≤K and (s, a) ∈S × A:
|˜ϵ1( ˜Qk,1
1 )(s, a)| ≤
1
1 −γ exp

−ηµminNB
2

+ C∥¯∆k∥∞
q
η ¯d log
  |S||A|
δ

+ η ¯d log
  |S||A|
δ

+ C
q
η log
  |S||A|K
δ

(1 + VarP ( ¯Vk)(s, a)) + C η log
  |S||A|K
δ

1 −γ
(31)
We will now present a crude bound on ∥¯∆k∥∞to reduce the analysis of Q-RexDaRe to the analysis
of Q-Rex.
Claim 1. Whenever η ≤C1
(1−γ)2
¯d log
  |S||A|
δ
, with probability at-least 1 −δ, we must have for every
k ≤K uniformly:
∥¯∆k∥∞≤C
exp

−ηµminNB
2

(1−γ)2
+ C
q
η
(1−γ)4 log
  |S||A|K
δ

Applying this directly to Equation (31), we conclude that with probability at least (1 −δ), we must
have uniformly for every k ≤K and (s, a) ∈S × A:
26

Published as a conference paper at ICLR 2022
|˜ϵ1( ˜Qk,1
1 )(s, a)| ≤
C
(1 −γ) exp

−ηµminNB
2

+ C η log
  |S||A|K
δ

(1 −γ)2
p
¯d
+ C
q
η log
  |S||A|K
δ

(1 + VarP ( ¯Vk)(s, a)) + C η log
  |S||A|K
δ

1 −γ
(32)
Proof of Claim 1. First note that T is γ contractive under the sup norm. Therefore,
∥¯∆k+1∥∞=
T ( ˜wk,1
1 ) −T ( ¯wk,1
1 ) + ˜ϵk

∞
≤
T ( ˜wk,1
1 ) −T ( ¯wk,1
1 )

∞
+ ∥˜ϵk∥∞
≤γ∥¯∆k∥∞+ ∥˜ϵk∥∞
(33)
In Equation (31), note that VarP ( ¯Vk)(s, a) ≤
1
(1−γ)2 . Therefore, under the conditions of this Claim,
we can take C1 small enough so that uniformly for every (s, a) and k ≤K with probability at-least
(1 −δ), Equation (31) becomes:
∥˜ϵk( ˜Qk,1
1 )∥∞≤
exp

−ηµminNB
2

1 −γ
+ ∥¯∆k∥∞
1 −γ
2

+ C
q
η
(1−γ)2 log
  |S||A|K
δ

.
Combining the display above and the fact that ˜ϵk = ˜ϵ1( ˜Qk,1
1 ) in Equation (33), we conclude that
with probability at-least 1 −δ, for every k ≤K uniformly:
∥¯∆k+1∥∞≤1 + γ
2
∥¯∆k∥∞+
exp

−ηµminNB
2

1−γ
+ C
q
η
(1−γ)2 log
  |S||A|K
δ

(34)
Unrolling the recursion above and using the fact that ∥¯∆1∥∞= 0, we conclude the statement of the
claim.
We also note that the deterministic iterations ¯Qk,1
1
converges exponentially in sup norm to Q∗due
to γ contractivity of T . That is:
∥¯Qk,1
1
−Q∗∥∞≤
γk
(1 −γ)
(35)
We are now ready to connect up with the proof of Theorem 2 with minor modiﬁcations. We follow
the same analysis as the proof of Theorem 2 but with VarP ( ˜Vk) replaced with VarP ( ¯Vk). Similar
to the proof of Theorem 2, we can control VarP ( ¯Vk) with respect to VarP (V ∗) and ∥¯Qk,1
1
−Q∗∥∞
along with Equation (35). We also replace αη with
αdr
η :=
C
(1 −γ) exp

−ηµminNB
2

+ C η log
  |S||A|K
δ

(1 −γ)2
p
¯d + C η log
  |S||A|K
δ

1 −γ
Therefore, we conclude a version of Equation (23):
∥∆K+1∥∞≤αdr
η + γK
1 −γ
+ C
r
η
(1 −γ)3 log
  K|S||A|
δ

s
1 +
γK/2
(1 −γ)
(36)
Where ∆k := Qk,1
1 −Q∗. Using the bounds on the parameters given in the statement of the Theorem,
we conclude the result.
27

Published as a conference paper at ICLR 2022
K
MINIMAX LOWERBOUNDS FOR TABULAR MDPS
Suppose Θ denotes the class of all tuples (M, π) where M is a tabular MDP and π is an exploratory
policy such that under the policy π, the MDP achieves a stationary distribution µ with minimum
probability at-least µmin (see section 2.1). The rewards are almost surely in the set [0,1]. We
receive the stationary sequence (st, at, rt)T
t=1 from MDP M under policy π, which we denote as
(st, at, rt)T
t=1 ∼(M, π). Let F be the class of all estimators f which estimate Q∗for the MDP M
with f((st, at, rt)T
t=1). We write the minimax risk as:
L(Θ, T) := inf
f∈F
sup
(M,π)∈Θ
E(st,at,rt)T
t=1∼(M,π)∥f((st, at, rt)T
t=1) −Q∗∥∞
Theorem 5. There exists a constant C such that, for every µmin ∈(0, 1/4), γ ∈(0, 1/2) and
T ≥C
1
µmin(1−γ), we must have:
L(θ, T) ≥C
s
1
T(1 −γ)3µmin
.
Therefore, we need T ≥
1
ϵ2µmin(1−γ)3 in order to achieve ϵ error for the Q values.
Proof. We will use Le-Cam’s two point method to prove the result. Consider a class of MDPs
denoted by MDP(q, p) (p, q ∈[0, 1]) with state space S = {0, 1}, only one possible action, reward
function R : S →R, R(s) = s, and discount factor γ ∈[0, 1). The transition probability for the
MDP is given by P(0|0) = 1 −q, P(1|0) = q, P(0|1) = p and P(1|1) = 1 −p.
Under these conditions, the stationary distribution is given by µ(0) =
p
p+q and µ(1) =
q
p+q and the
value function can be written as
V (1) =
1
1 −γ(1 −p) −γph(q),
V (0) =
h(q)
1 −γ(1 −p) −γph(q)
where h(q) :=
γq
1−γ(1−q). Since there is only one action in each state, the value function coincides
with the Q function.
For showing the lower bound, consider two MDPs with parameters (p1, q) and (p2, q) with q <
min{p1, p2} and p1, p2 < 0.5. The stationary trajectories of length T under the two MDPs, denoted
by the random variables X(i)
1:T for i = 1, 2, satisﬁes
KL(X(2)
1:T ||X(1)
1:T ) ≤
2q(p1 −p2)2
(p1 + q)(p2 + q)2 ln 2 + T 2q(p1 −p2)2
(p2 + q)p1 ln 2 ,
using KL(Ber(a)||Ber(b)) ≤2(a−b)2
b ln 2
whenever b ≤1/2.
Observe that µ(2)
min = µ(2)(1) =
q
p2+q and µ(1)
min = µ(1)(1) =
q
p1+q. Combining this with the KL
divergence bound above, we conclude that KL(X(2)
1:T ||X(1)
1:T ) <
1
8 if we set Now let p1 =
1−γ
2 ,
p2 −p1 = c
q
p1
T µ(1)
min for small enough constant c and T ≳p(p+q)
q
. (notice that the equation gives
p2 < 0.5 when T satisﬁes the condition above and p1 < 0.25). By Pinsker’s inequality, we must
have TV(X(2)
1:T , X(1)
1:T ) ≤1
2.
Elementary calculations show that |V (1)(1)−V (2)(1)| ≳|p1−p2|
(1−γ)2 =
C
(1−γ)2
q
p1
T µ(1)
min since q < p1 =
1−γ
2 . Therefore:
|V (1)(1) −V (2)(1)| ≳
s
1
T(1 −γ)3µ(1)
min
28

Published as a conference paper at ICLR 2022
Consider the semi-metric ρ(x, y) =

1
1−γ(1−x)−γxh(q) −
1
1−γ(1−y)−γyh(q)
 for x, y ∈(0, 1). Let
Pi be the distribution of X(i)
1:T , and let the distribution be parameterised by θ(Pi) = V (i)(1) for
i = 1, 2. Let ˆθ be any estimator based on the observations. By (Wainwright, 2019c, Chapter 15),
min
ˆθ
max
P ∈{P1,P2} EP
h
|ˆθ −θ(P)|
i
≥C
s
1
T(1 −γ)3µ(1)
min
(1 −TV(X(2)
1:T , X(1)
1:T ))
≥C1
s
1
T(1 −γ)3µ(1)
min
(37)
=⇒for less than ϵ error, T has to be at-least
1
µ(1)
minϵ2(1−γ)3 .
q = µmin(1−γ)
1−2µmin , T ≳
1
(1−γ)µmin , we can ensure that µ(1)
min = 2µmin and µ(2)
min ≥µmin, which ensures
that MDP(p1, q) and MDP(p2, q) are both elements of Θ. Therefore, the two point risk in the LHS
of Equation (37) lower bounds L(Θ, T), which allows us to conclude the result by substituting
µ(1)
min = µmin.
L
PROOFS OF CONCENTRATION INEQUALITIES
L.1
PROOF OF LEMMA 8
First, we leverage the techniques established in (Jain et al., 2021b, Lemma 28) to show Lemma 14.
The proof follows by a simple re-writing of the proof of the aforementioned lemma which uses a
linear approximation (in η) to ˜Hk,j
1,B. We omit the proof for the sake of clarity.
Lemma 14. Suppose ηB < 1
3. Then, the following PSD inequalities hold almost surely:
I−2η

1 +
ηB
1−2ηB

B
X
i=1
˜φk,j
i

˜φk,j
i
⊤
⪯

˜Hk,j
1,B
⊤˜Hk,j
1,B ⪯I−2η

1 −
ηB
1−2ηB

B
X
i=1
˜φk,j
i

˜φk,j
i
⊤
(38)
Proof of Lemma 8. We begin by proving the ﬁrst part. Using Assumption 1 and the fact that the
decoupled trajectory (˜s, ˜a)t is assumed to be mixed at the start of every buffer, we begin by ﬁrst
noting from Lemma 14 that:
0 ⪯E( ˜Hk,j
1,B)⊤˜Hk,j
1,B ⪯I −ηBE(s,a)∼µφ(s, a)(φ(s, a))⊺⪯(1 −ηB
κ )I .
(39)
Now, observe that:
∥
1
Y
j=N
˜Hk,j
1,Bg∥2 = g⊤
N−1
Y
j=1
( ˜Hk,j
1,B)⊤h
( ˜Hk,N
1,B )⊤˜Hk,N
1,B
i
1
Y
j=N−1
˜Hk,j
1,Bg
(40)
Now note that ˜Hk,N
1,B is independent of ˜Hk,j
1,B for j ≤N −1. Therefore, taking conditional expecta-
tion conditioned on Hk,j
1,B for j ≤N −1 in Equation (40) and using Equation (39), we conclude:
E∥
1
Y
j=N
˜Hk,j
1,Bg∥2 ≤(1 −ηB
κ )E∥
1
Y
j=N−1
˜Hk,j
1,Bg∥2
Applying the equation above inductively, we conclude the result.
We now prove Part 2. We apply Markov’s inequality to Part 1 along with Lemma 2 to show that
with probability at least 1 −δ:
∥
1
Y
j=N
˜Hk,j
1,Bg∥φ ≤∥
1
Y
j=N
˜Hk,j
1,Bg∥≤
exp

−ηNB
κ

√
δ
∥g∥≤exp(−ηNB
κ )
q
κ
δ ∥g∥φ .
29

Published as a conference paper at ICLR 2022
L.2
PROOF OF LEMMA 9
Proof. We intend to apply Freedman’s inequality (Freedman, 1975) like in (Li et al., 2021, Theorem
4), but in an asynchronous fashion and with Markovian data. Here, reverse experience replay endows
our problem with the right ﬁltration structure. Using Equation (9), we can write
⟨φ(s, a),
N
X
j=1
j+1
Y
l=N
˜Hk,l
1,B ˜Lk,j(w)⟩= η
N
X
j=1
B
X
i=1
Xk,j
i
(41)
Where Xk,j
i
:= (1 −η) ˜
Nk,j
i
(s,a)˜ϵk,j
i
1
 (˜sk,j
i
, ˜ak,j
i
) = (s, a)

. This allows us to deﬁne the sequence
of sigma algebras Fk,j
i
= σ((˜sk,l
m , ˜ak,l
m ) : (l > j and m ∈[B]) or (l = j and m ≤i)) - that is,
it is the sigma algebra of all states and rewards which appeared before and including (˜sk,j
i
, ˜ak,j
i
)
inside the buffer j and all the states in buffers l > j. Notice that ˜N k,j
i
(s, a) is measurable with
respect to the sigma algebra Fk,j
i
. Using the fact that the buffers are independent, we conclude that:
E
h
Xk,j
i
|Fk,j
i
i
= 0 and
E
h
|Xk,j
i
|2|Fk,j
i
i
≤2

1 + γ2VarP (V )(s, a; w)

1
 (˜sk,j
i
, ˜ak,j
i
) = (s, a)

(1 −η)2 ˜
Nk,j
i
(s,a) .
It is also clear from our assumptions that |Xj,k
i
| ≤
2
1−γ almost surely. Consider the almost sure
inequality for the sum of conditional variances:
W k =
N
X
j=1
B
X
i=1
E
h
|Xk,j
i
|2|Fk,j
i
i
≤2

1 + γ2VarP (V )(s, a; w)
 K
X
j=1
B
X
i=1
1
 (˜sk,j
i
, ˜ak,j
i
) = (s, a)

(1 −η)2 ˜
N k,j
i
(s,a)
= 2

1 + γ2VarP (V )(s, a; w)

˜
Nk(s,a)−1
X
t=0
(1 −η)2t ≤2
1 + γ2VarP (V )(s, a; w)
η

(42)
We now apply (Li et al., 2021, Equation (144),Theorem 4) with R
=
1
1−γ and σ2
=
2

1+γ2VarP (V )(s,a;w)
η

to conclude the result.
L.3
PROOF OF THEOREM 4
Proof. For the sake of convenience, in this proof we will take σ2 := 4(1 + ∥w∥2
φ). Suppose λ ∈R.
For 1 ≤m ≤N consider:
Xm := ηλ2σ2

x,
N−m+1
Y
l=N
˜Hk,l
1,B
 N−m+1
Y
l=N
˜Hk,l
1,B
!⊤
x

+ λ

x,
N
X
j=N−m+1
 j+1
Y
l=N
˜Hk,l
1,B
!
˜Lk,j(w)

X0 := ∥x∥2ηλ2σ2
Here we use the convention that Qj+1
l=N ˜Hk,l
1,B = I whenever j + 1 > N. We claim that the sequence
exp(Xm) forms a super martingale under an appropriate ﬁltration. In this proof only, consider the
sigma algebra Fm to be the sigma algebra of all the state action reward tuples in buffers N, . . . , N −
m + 1 and let F0 be the trivial sigma algebra. Notice that exp(Xm) is Fm measurable.
Lemma 15. Fix N ≥m ≥1. Suppose Y ∈Rd is a Fm−1 measurable random vector. Then,
E

exp

ηλ2σ2⟨Y, ˜Hk,N−m+1
1,B

˜Hk,N−m+1
1,B
⊤
Y ⟩+ λ⟨Y, ˜Lk,N−m+1(w)⟩
Fm−1

≤exp
 ηλ2σ2∥Y ∥2
30

Published as a conference paper at ICLR 2022
In particular, taking Y =
QN−m+2
l=N
˜Hk,l
1,B
⊤
x , we conclude that exp(Xm) is a super martingale
with respect to the ﬁltration Fm
Proof of Lemma 15. In the proof of this lemma, we will drop the superscripts k, m for the sake of
convenience and due to conditioning on Fm−1, we will treat Y as a constant. Now we deﬁne the
natural ﬁltration on the buffer under consideration (Gi)B
i=1 where Gi is the sigma algebra of the all
state-action tuples from (s1, a1), . . . , (si, ai) and rewards (rh)1≤h<i.
Note that by the deﬁnition of ˜L(w), we write: ˜L(w) = PB
i=1 η˜ϵi(w) ˜H1,i−1 ˜φi. With this in mind,
for h ∈[B] deﬁne ˜Lh(w) = Ph
i=1 η˜ϵi(w) ˜H1,i−1 ˜φi. Now, ˜L(w) = η˜ϵB(w) ˜H1,B−1 ˜φB + ˜LB−1(w)
Now, notice that the random variables ⟨Y, ˜LB−1⟩,⟨Y, ˜H1,B−1 ˜φB⟩and ⟨Y, ˜Hk,m
1,B

˜Hk,m
1,B
⊤
Y ⟩are
GB measureable. Furthermore, we must have: E

˜ϵB(w)
GB

= 0 and |˜ϵB(w)| ≤2(1 + ∥w∥φ) ≤
√
2σ. Therefore, applying conditional Hoeffding’s lemma, we have:
E

exp

ηλ2σ2⟨Y, ˜H1,B

˜H1,B
⊤
Y ⟩+ λ⟨Y, ˜L(w)⟩
GB

= exp

ηλ2σ2⟨Y, ˜H1,B

˜H1,B
⊤
Y ⟩+ λ⟨Y, ˜LB−1(w)⟩

E

exp

λη˜ϵB(w)⟨Y, ˜H1,B−1 ˜φB⟩
GB

≤exp

ηλ2σ2⟨Y, ˜H1,B

˜H1,B
⊤
Y ⟩+ λ⟨Y, ˜LB−1(w)⟩

exp

λ2η2σ2|⟨Y, ˜H1,B−1 ˜φB⟩|2
(43)
In the third step we have used the conditional version of Hoeffding’s lemma. Now, consider Z :=
( ˜H1,B−1)⊤Y . Clearly,
⟨Y, ˜H1,B

˜H1,B
⊤
Y ⟩+ η|⟨Y, ˜H1,B−1 ˜φB⟩|2 = Z⊤˜H⊤
B,B ˜HB,BZ + ηZ⊤˜φB(˜φB)⊤Z
= Z⊤h
I −η ˜φB(˜φB)⊤+ ∥˜φB∥2η2 ˜φB(˜φB)⊤i
Z
≤∥Z∥2
Here we have used the fact that η < 1 and ∥˜φB∥< 1. Using the bounds above in Equation (43), we
conclude:
E

exp

ηλ2σ2⟨Y, ˜H1,B

˜H1,B
⊤
Y ⟩+ λ⟨Y, ˜LB(w)⟩
GB

≤exp

ηλ2σ2⟨Y, ˜H1,B−1

˜H1,B−1
⊤
Y ⟩+ λ⟨Y, ˜LB−1(w)⟩

(44)
Using Equation (44) recursively, we conclude the ﬁrst statement of the lemma. The last part of the
lemma follows easily from the deﬁnition of Xm.
By Lemma 15, we conclude that exp(Xm) is a super martingale with respect to the ﬁltration Fm.
Therefore, we must have:
E exp(XN) ≤exp(X0) = exp(η∥x∥2λ2σ2) .
It is also clear that XN ≥λ

x, PN
j=N−m+1
Qj+1
l=N ˜Hk,l
1,B

˜Lk,j(w)

. Applying Chernoff bound
with we conclude that the concentration inequality in Equation (13). We can then directly apply
(Vershynin, 2018, Theorem 8.1.6) to Equation (13) in order to obtain uniform concentration bounds.
31

Published as a conference paper at ICLR 2022
L.4
PROOF OF LEMMA 10
Proof of Lemma 10. By deﬁnition of ˜ϵk, we have: ˜wk+1,1
1
= T ( ˜wk,1
1 ) + ˜ϵk. Therefore, for any
(s, a) ∈S × A, we must have:
⟨φ(s, a), ˜wk+1,1
1
⟩= R(s, a) + γEs′∼P (·|s,a) sup
a′∈A
⟨φ(s′, a′), ˜wk,1
1 ⟩+ ⟨φ(s, a), ˜ϵk⟩.
Using the fact that R(s, a) ∈[0, 1], we conclude:
∥˜wk+1,1
1
∥φ ≤1 + γ∥˜wk,1
1 ∥φ + ∥˜ϵk∥φ
(45)
By independence of outer-loops for the coupled data, we note that ˜wk,1
1
is independent of the data
in buffer k. Therefore we can apply Theorem 4 (and resp. Lemma 8) conditionally with w = ˜wk,1
1
(and resp. g = ˜wk,1
1
−w∗), the bias variance decomposition given in Lemma 5 and the bound on
∥w∗∥φ in Lemma 4 to conclude that with probability at-least 1 −δ, for every k ≤K:
∥˜ϵk∥φ ≤
r
Kκ
δ
exp(−ηNB
κ )(∥˜wk,1
1 ∥φ +
1
1 −γ )
+ C(1 + ∥˜wk,1
1 ∥φ)√η

CΦ +
q
log( 2K
δ )

(46)
We now choose constants C1 and C2 in the statement of the Lemma such that Equation 46 implies:
∥˜ϵk∥φ ≤1 −γ
2
∥˜wk,1
1 ∥φ + 1
Using the equation above in Equation (45), we conclude:
∥˜wk+1,1
1
∥φ ≤2 + 1 + γ
2
∥˜wk,1
1 ∥φ .
Unrolling the recursion above and noting w1,1
1
= 0, we conclude that with probability at-least 1 −δ,
we must have ∥˜wk,1
1 ∥φ ≤
4
1−γ for every k ≤K. The bound in item 2 follows by using item 1,
Equation (46) and the fact that ˜wk,1
1
is independent of the data in buffer k due to our coupling.
L.5
PROOF OF LEMMA 12
Proof. For the sake of convenience, we will take ¯d = |supp(P(·|s, a))| and index supp(P(·|s, a))
by [ ¯d]. Consider Y ∈{−1, 1} ¯d. We consider the class of random variables indexed by elements of
{−1, 1} ¯d:
∆(Y ; s, a) =
X
s′
Ys′
h
ˆPk(s′|s, a) −¯P(s′|s, a)
i
The proof proceeds in a similar way to the proof of Lemma 9 via the Freedman inequality. To bring
out the similarities we deﬁne similar notation. Consider the sequence of sigma algebras Fk,j
i
for
i ∈[B] and j ∈[K] as deﬁned in the proof of Lemma 9. We now deﬁne
Xk,j
i
(Y ) := (1−η)
˜
Nk,j
i
(s,a) X
s′
Ys′

1(˜sk,j
i+1 = s′, ˜sk,j
i
= s, ˜ak,j
i
= a) −P(s′|s, a)1(˜sk,j
i
= s, ˜ak,j
i
= a)

.
We note that E
h
Xk,j
i
(Y )|Fk,j
i
i
= 0 and |Xk,j
i
(Y )| ≤2 almost surely and a simple calculation
reveals that: E
h
|Xk,j
i
(Y )|2|Fk,j
i
i
≤(1 −η)2 ˜
Nk,j
i
1(˜sk,j
i
= s, ˜ak,j
i
= a)
It is also clear that: ∆(Y ; s, a) = η PK
j=1
PB
i=1 Xk,j
i
(Y ). Apply Freedman’s concentration in-
equality, we conclude for any ﬁxed Y ∈{−1, 1} ¯d and (s, a) ∈S × A,
P(|∆(Y ; s, a)| > C
p
η log(2/δ) + η log(2/δ)) ≤δ
32

Published as a conference paper at ICLR 2022
Applying a union bound over all Y ∈−1, 1d, we conclude:
P(
sup
Y ∈{−1,1} ¯
d |∆(Y ; s, a)| > C
q
η ¯d log(4/δ) + η ¯d log(4/δ)) ≤δ .
We complete the proof by noting that
sup
Y ∈{−1,1} ¯
d |∆(Y ; s, a)| =
X
s′∈supp(P (·|s,a))
| ˆPk(s′|s, a) −¯Pk(s′|s, a)|
M
TECHNICAL LEMMAS
M.1
COUPLING LEMMA
We ﬁrst introduce some useful notation: Let Dk,j (resp.
˜Dk,j) be the tuple of random variables
(sk,j
i
, ak,j
i
, rk,j
i
)B+1
i=1 (resp. (˜sk,j
i
, ˜ak,j
i
, ˜rk,j
i
)B+1
i=1 ).
Lemma 16. Suppose
u ≥
(
Cτmix log( T
δ ) in the tabular setting
Cτmix log(
T
Cmixδ) in the general setting
(47)
Then, we can deﬁne the sequences (st, at, rt) and (˜st, ˜at, ˜rt) on a common probability space such
that:
1. The tuples Dk,j and ˜Dk,j have the same distribution for every (k, j),
2. The sequence ˜Dk,j for k ≤N, j ≤K is i.i.d.
3. Equation (4) holds
Proof. The proof of this lemma for the tabular case is a rewriting of the the proofs of Lemmas 1,2,3
in (Bresler et al., 2020). For the general state space case, we apply appropriate modiﬁcations as
pioneered in (Goldstein, 1979).
M.2
PROOF OF LEMMA 2
Proof. The ﬁrst part follows from the deﬁnitions. For the second part, note that: ∥x∥≥∥x∥φ
follows from Cauchy-Schwarz inequality and Assumption 1. For the reverse inequality we use
Assumption 4 to show that:
∥x∥2
φ ≥E(s,a)∼µ⟨φ(s, a), x⟩2 ≥∥x∥2
κ
M.3
PROOF OF LEMMA 3
Proof. Existence is guaranteed by Deﬁnition (2) and uniqueness follows from the assumption that
span(Φ) = Rd.
M.4
PROOF OF LEMMA 4
Proof. Suppose w0, ˜w0 ∈Rd are arbitrary. Let w1 = T (w0) and ˜w1 = T ( ˜w0). For any φ(s, a), we
have:
|⟨φ(s, a), w1 −˜w1⟩| = γ
Es′∼P (·|s,a) sup
a′∈A
⟨φ(s′, a′), w0⟩−Es′∼P (·|s,a) sup
a′∈A
⟨φ(s′, a′), ˜w0⟩
 (48)
33

Published as a conference paper at ICLR 2022
By Assumption 2, for every ﬁxed s′ there exist a′
max(s′), ˜a′
max(s′) such that
sup
a′∈A
⟨φ(s′, a′(s′)), w0⟩= ⟨φ(s′, ˜a′
max(s′)), w0⟩
and
sup
a′∈A
⟨φ(s′, a′), ˜w0⟩= ⟨φ(s′, ˜a′
max(s′)), ˜w0⟩.
Therefore for any s′ it holds that,
⟨φ(s′, ˜a′
max), w0 −˜w0⟩≤sup
a′∈A
⟨φ(s′, a′), w0⟩−sup
a′∈A
⟨φ(s′, a′), ˜w0⟩≤⟨φ(s′, a′
max), w0 −˜w0⟩
=⇒
 sup
a′∈A
⟨φ(s′, a′), w0⟩−sup
a′∈A
⟨φ(s′, a′), ˜w0⟩
 ≤∥w0 −˜w0∥φ .
Combining this with Equation (48), we conclude the γ-contractivity of T . By contraction map-
ping theorem, T has a unique ﬁxed point we conclude that it is w∗by considering Q(s, a) :=
⟨w∗, φ(s, a)⟩and showing that it satisﬁes the bellman optimality condition in Equation (2). For the
norm inequality, we note that since R(s, a) ∈[0, 1] by assumption and w∗= T (w∗), we have:
|⟨φ(s, a), w∗⟩| ≤1 + γ∥w∗∥φ. Therefore, ∥w∗∥≤
1
1−γ . The second norm equality follows from
Lemma 2.
M.5
PROOF OF LEMMA 5
Proof. We write the iteration in the outer-loop k as:
˜wk,j
i+1 =
h
I −η ˜φk,j
−i [˜φk,j
−i ]⊤i
˜wk,j
i
+ η ˜φk,j
−i

˜rk,j
−i + γ sup
a′∈A
⟨˜wk,1
1 , φ(˜sk,j
−(i−1), a′)⟩

Using the fact that ⟨˜wk+1,∗, ˜φk,j
−i ⟩= ˜Rk,j
−i + γEs′∼P (·|˜sk,j
−i ,˜ak,j
−i ) supa′∈A⟨φ(s′, a′), ˜wk,1
1 ⟩, and recall-
ing the notation in (5),(6), we write:
˜wk,j
i+1 −˜wk+1,∗=
h
I −η ˜φk,j
−i [˜φk,j
−i ]⊤i 
˜wk,j
i
−wk+1,∗
+ η ˜φk,j
−i ϵk,j
−i .
Therefore,
˜wk,j+1
1
−˜wk+1,∗= ˜Hk,j
1,B( ˜wk,j
1
−˜wk+1,∗) + ˜Lk,j
Unfurling further, and using the fact that ˜wk,N+1
1
= ˜wk+1,1
1
, we conclude the statement of the
lemma.
M.6
PROOF OF LEMMA 7
We let
˜N k,j(s, a) := PB
i=1 1((˜sk,j
i
, ˜ak,j
i
) = (s, a)).
We want to show that the quantity
PN
j=1 ˜N k,j(s, a) concentrates around its expectation. Note that E ˜N k,j(s, a) = Bµ(s, a) and that
˜N k,j(s, a) are i.i.d. for j ∈[N]. From the proof of (Paulin, 2015, Theorem 3.4) and the fact that
γps ≥
1
2τmix , we conclude:
E exp(λ(
N
X
j=1
˜N k,j(s, a) −Bµ(s, a))) =
N
Y
j=1
E exp(λ( ˜N k,j(s, a) −Bµ(s, a)))
≤exp
8N(B + 2τmix)µ(s, a)τmixλ2
1 −20λτmix

(49)
Following the Chernoff bound in the proof of (Paulin, 2015, Theorem 3.4), we conclude:
P(| ˜N k(s, a) −NBµ(s, a)| > t) ≤2 exp

−
t2
16τmix(B + 2τmix)Kµ(s, a) + 40tτmix

(50)
34

Published as a conference paper at ICLR 2022
Taking t = 1
2NBµ(s, a), whenever B ≥τmix (by assumption), we must have:
P

˜N k(s, a) ≤1
2NBµ(s, a)

≤2 exp

−C KBµ(s,a)
τmix

for some constant C. Further, via a union bound over all state action pairs (s, a), we conclude the
statement of the lemma.
M.7
PROOF OF LEMMA 13
We ﬁrst prove a simple consequence of multivariate calculus.
Lemma 17. Let f : Rn →R be deﬁned by f(x) = supi xi. Then, for every x, y ∈Rn, there exists
ζ ∈Rd such that ∥ζ∥1 = 1, ζi ≥0 and:
f(x) −f(y) = ⟨ζ, x −y⟩
Proof. We consider the log-sum-exp function.
Given L
∈
R+,
deﬁne fL(x)
:=
1
L log (Pn
i=1 exp(Lxi)). An elementary calculation shows that:
log(n)
L
+ sup
i
xi ≥fL(x) ≥sup
i
xi
Therefore, for any ﬁxed x,
lim
L→∞fL(x) = f(x) .
Now, ∇fL(x) = (p1, . . . , pn) where pi =
exp(Lxi)
Pn
j=1 exp(Lxj).
Clearly, ⟨∇fL(x), ei⟩≥0 and
∥∇fL(x)∥1 = 1. By the mean value theorem there exists βL such that:
fL(x) −fL(y) = ⟨∇fL(βL), x −y⟩.
(51)
Now, the simplex in Rn (denoted by ∆n) is compact.
Therefore, there exists a sub-sequence
Lk →∞such that limk→∞∇fLk(βLk) = ζ ∈∆n. Taking limit along the sub-sequence Lk
in Equation (51), we conclude the result.
Proof of Lemma 13. In the tabular setting, we have the following expression using Equation (41):
L.H.S =
ηγ
K
X
j=1
B
X
i=1
(1 −η)
˜
Nk,j
i
(s,a)1
h
(˜sk,j
i
, ˜ak,j
i
) = (s, a)
i h
˜ϵk,j
i
(w) −˜ϵk,j
i
(v)
i
(52)
Where
˜ϵk,j
i
(w) :=

sup
a′∈A
⟨w, φ(˜sk,j
i+1, a′)⟩−Es′∼P (·|˜sk,j
i
,˜ak,j
i
) sup
a′∈A
⟨φ(s′, a′), w⟩

Now, by an application of Lemma 17, we show that for some η(·|s′, w, v) ∈∆(A), depending only
on (s′, w, v), we have:
sup
a′∈A
⟨w, φ(s′, a′), w⟩−sup
a′∈A
⟨w, φ(s′, a′), v⟩=
X
a′∈A
ζ(a′|s′, w, v)⟨φ(s′, a′), w −v⟩
Now, using the deﬁnition of ˆPk(·|s, a) and ¯P(·|s, a) in the discussion preceding Lemma 12, we can
simplify Equation (52) to:
L.H.S. = γ

X
s′∈supp(P (·|s,a))
a′∈A

ˆPk(s′|s, a) −¯Pk(s′|s, a)

ζ(a′|s′, w, v)⟨φ(s′, a′), w −v⟩

(53)
We conclude the statement of the lemma by an application of the Holder inequality.
35

Published as a conference paper at ICLR 2022
M.8
PROOF OF LEMMA 11
Proof. We solve for f(u∗) = u∗to obtain the relation: u∗= α + β
√
u∗, which after discarding the
negative solution for
√
u∗yields the unique solution: u∗=

β+√
β2+4α
2
2
. To prove the second
part, consider for arbitrary x, y ∈R+, the following hyper-contractivity:
|f(x) −f(y)| = β|√x −√y|
≤β
p
|x −y|
(54)
The second step follows from the fact that |√a −
√
b| ≤
p
|a −b|. Since u∗= f(u∗), we apply the
inequality above:
|f (t)(u) −u∗| = |f (t)(u) −f (t)(u∗)|
≤β
q
|f (t−1)(u) −f (t−1)(u∗)|
(55)
We then conclude the result by induction.
36

