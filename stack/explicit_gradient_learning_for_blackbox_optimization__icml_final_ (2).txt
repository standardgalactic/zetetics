Explicit Gradient Learning for Black-Box Optimization
Elad Saraﬁan* 1 Mor Sinay* 1 Yoram Louzoun 1 Noa Agmon 1 Sarit Kraus 1
Abstract
Black-Box Optimization (BBO) methods can ﬁnd
optimal policies for systems that interact with
complex environments with no analytical repre-
sentation. As such, they are of interest in many
Artiﬁcial Intelligence (AI) domains. Yet classi-
cal BBO methods fall short in high-dimensional
non-convex problems. They are thus often over-
looked in real-world AI tasks. Here we present a
BBO method, termed Explicit Gradient Learn-
ing (EGL), that is designed to optimize high-
dimensional ill-behaved functions. We derive
EGL by ﬁnding weak spots in methods that ﬁt
the objective function with a parametric Neural
Network (NN) model and obtain the gradient sig-
nal by calculating the parametric gradient. Instead
of ﬁtting the function, EGL trains a NN to esti-
mate the objective gradient directly. We prove
the convergence of EGL to a stationary point and
its robustness in the optimization of integrable
functions. We evaluate EGL and achieve state-of-
the-art results in two challenging problems: (1)
the COCO test suite against an assortment of stan-
dard BBO methods; and (2) in a high-dimensional
non-convex image generation task.
1. Introduction
Optimization problems are prevalent in many artiﬁcial in-
telligence applications, from search-and-rescue optimal de-
ployment (Zhen et al., 2014) to triage policy in emergency
rooms (Rosemarin et al., 2019) to hyperparameter tuning
in machine learning (Bardenet et al., 2013). In these tasks,
the objective is to ﬁnd a policy that minimizes a cost or
maximizes a reward. Evaluating the cost of a single policy
is a complicated and often costly process that usually has
no analytical representation, e.g., due to interaction with
*Equal contribution. 1Department of Computer Science, Bar-
Ilan University, Israel. Correspondence to: Elad Saraﬁan, Mor
Sinay <elad.saraﬁan@gmail.com, mor.sinay@gmail.com>.
Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).
real-world physics or numerical simulation. Black-Box Op-
timization (BBO) algorithms (Audet & Hare, 2017; Golovin
et al., 2017) are designed to solve such problems, when
the analytical formulation is missing, by repeatedly query-
ing the Black-Box function and searching for an optimal
solution while minimizing the number of queries (budget).
Related Work BBO problems have been studied in multi-
ple ﬁelds with diverse approaches. Many works investigated
derivative–free methods (Rios & Sahinidis, 2013), from the
classic Nelder–Mead algorithm (Nelder & Mead, 1965) and
Powell’s method (Powell, 1964) to more recent evolutionary
algorithms such as CMA-ES (Hansen, 2006). Another line
of research is derivative-based algorithms, which ﬁrst ap-
proximate the gradient and then apply line-search methods
such as the Conjugate Gradient (CG) Method (Shewchuk
et al., 1994) and Quasi-Newton Methods, e.g. BFGS (No-
cedal & Wright, 2006). Other model-based methods such
as SLSQP (Bonnans et al., 2006) and COBYLA (Powell,
2007) iteratively solve quadratic or linear approximations
of the objective function. Some variants apply trust-region
methods and iteratively ﬁnd an optimum within a trusted
subset of the domain (Conn et al., 2009; Chen et al., 2018).
Another line of research is more focused on stochastic dis-
crete problems, e.g. Bayesian methods (Snoek et al., 2015),
and multi-armed bandit problems (Flaxman et al., 2004).
More recent works have studied the applications of NNs in
BBO algorithms. (Mania et al., 2018; Vemula et al., 2019)
showed that Markov Decision Processes could be solved by
applying random search optimization over the weights of
a parameterized policy. (Sener & Koltun, 2020) suggested
learning an intermediate low dimensional manifold with a
NN to reduce the complexity of the random search. (Mah-
eswaranathan et al., 2018) suggested learning the objective
and using the parametric gradient with respect to the inputs
(Lillicrap et al., 2015) to guide a random search. (Saremi,
2019) studied NN architectures for learning gradients.
Our contribution In this paper, we suggest a new
derivative-based algorithm, Explicit Gradient Learning
(EGL), that learns a surrogate function for the gradient by
averaging the numerical directional derivatives over small
volumes bounded by ε radius. We control the accuracy of
our model by controlling the ε radius parameter. We then use
trust-regions and dynamic scaling of the objective function

Explicit Gradient Learning
to ﬁne-tune the convergence process to the optimal solution.
This results in a theoretically guaranteed convergence of
EGL to a stationary point. We compared the performance
of EGL to eight different BBO algorithms in rigorous sim-
ulations in the COCO test suite (Hansen et al., 2019) and
show that EGL outperforms all others in terms of ﬁnal ac-
curacy. EGL was further evaluated in a high-dimensional
non-convex domain, involving searching in the latent space
of a Generative Adversarial Network (GAN) (Pan et al.,
2019) to generate an image with speciﬁc characteristics.
EGL again outperformed existing algorithms in terms of
accuracy and appearance, where other BBO methods failed
to converge to a solution in a reasonable amount of time.
The paper is organized as follows: BBO background and
motivation for the EGL algorithm are presented in Sections
2 and 3. The detailed description of the EGL algorithm
and its theoretical analysis are shown in Section 4. The
empirical evaluation of EGL’s performance and comparison
to state-of-the-art BBO algorithms are described in Section
5, and we conclude in Section 6. The proofs for all of the
theoretical statements are found in the Appendix.
2. Background
A function f : Ω→R, Ω⊆Rn is considered to be a
Black-Box if one can evaluate y = f(x) at x ∈Ω, but has
no prior knowledge of its analytical form. A BBO algorithm
seeks to ﬁnd x∗= arg minx∈Ωf(x), typically with as few
evaluations as possible (Audet & Hare, 2017). Since, in
general, it is not possible to converge to the optimal value
with a ﬁnite number of evaluations, we deﬁne a budget C
and seek to ﬁnd as good a solution as possible x⋆with less
than C evaluations (Hansen et al., 2010).
Many BBO methods operate with a two-phase iterative algo-
rithm: (1) search or collect data with some heuristic; and (2)
update a model to obtain a new candidate solution and im-
prove the heuristic. Traditionally, BBO methods are divided
into derivative-free and derivative-based methods. The for-
mer group relies on statistical models (Balandat et al., 2019),
physical models (Van Laarhoven & Aarts, 1987), or Evolu-
tionary Strategies (Back, 1996) to deﬁne the search pattern
and are not restricted to continuous domains, so they can
also be applied to discrete variables. Our algorithm, EGL,
falls under the latter category, which relies on a gradient
estimation to determine the search direction (Bertsekas &
Scientiﬁc, 2015). Formally, derivative-based methods are
restricted to differentiable functions, but here we show that
EGL can be applied successfully whenever the objective
function is merely locally integrable.
Recently, with the Reinforcement Learning renaissance (Sil-
ver et al., 2017; Schulman et al., 2015), new algorithms have
been suggested for the problem of continuous action control
(e.g., robotics control). Many of these can be viewed in the
context of BBO as derivative-based methods applied with
NN parametric models. One of the most prominent algo-
rithms is DDPG (Lillicrap et al., 2015). DDPG iteratively
collects data with some (often naive) exploration strategy
and then ﬁts a local NN parametric model fθ around a can-
didate solution xk. To update the candidate, it approximates
the gradient ∇f at xk with the parametric gradient ∇fθ and
then applies a gradient descent step (Ruder, 2016).1 Algo-
rithm 1 outlines the DDPG steps in the BBO formulation.
We denote it as Indirect Gradient Learning (IGL) since it
does not directly learn the gradient ∇f. In the next section,
we will develop arguments as to why one should learn the
gradient explicitly instead of using the parametric gradient.
Algorithm 1 Indirect Gradient Learning
Input: x0, α, C
k = 0
while budget C > 0 do
Build Local Model:
Collect data Dk = {(xk + εni, yi)}m
i=1, ni ∼N(0, I)
Fit a model fθk with
θk = arg minθ
Pm
i=1 |fθ(xk + εni) −yi|2
Gradient Descent:
xk+1 ←xk −α∇fθk(xk)
k ←k + 1
return xk
3. Motivation
In Algorithm 1 only the gradient information ∇f(xk) is
required to update the next xk candidate. However, the
gradient function is never learned directly, and it is only
inferred from the parametric model without any clear guar-
antee of its veracity. Hence we seek a method that learns the
gradient function ∇f explicitly. Clearly, directly learning
the gradient is infeasible since the Black-Box only outputs
the f(x) values. Instead, our approach would be to learn a
surrogate function for ∇f, termed the mean-gradient, by
sampling pairs of observations {(xi, yi), (xj, yj)}i,j and
averaging the numerical directional derivatives over small
volumes. This section formally deﬁnes the mean-gradient
and then formulates two arguments that motivate its use,
instead of ∇fθ. We then illustrate these arguments using
1D and 2D examples taken from the COCO test suite.
3.1. The Mean-Gradient
For any differentiable function f with a continuous gradient,
the ﬁrst order Taylor expression is
f(x + τ) = f(x) + ∇f(x) · τ + O(∥τ∥2).
(1)
1fθ is differentiable. Thus, it can be differentiated with respect
to its input x, as done in adversarial training (Yuan et al., 2019).

Explicit Gradient Learning
Figure 1. Comparing indirect gradient learning and explicit gradient learning for 4 typical functions: (a) parabolic; (b) piece-wise linear;
(c) multiple local minima; (d) step function.
Thus, locally around x, the directional derivative satisﬁes
∇f(x) · τ ≈f(x + τ) −f(x).
We deﬁne the mean-
gradient as the function that minimizes the Mean-Square-
Error (MSE) of such approximations in a vicinity of x.
Deﬁnition 1. The mean-gradient at x with ε > 0 averaging
radius is
gε(x) = arg min
g∈Rn
Z
Vε(x)
|g · τ −f(x + τ) + f(x)|2dτ (2)
where Vε(x) ⊂Rn is a convex subset s.t. ∥x′ −x∥≤ε
for all x′ ∈Vε(x) and the integral domain is over τ s.t.
x + τ ∈Vε(x).
Proposition 1 (controllable accuracy). For any differen-
tiable function f with a continuous gradient, there is
κg > 0, so that for any ε > 0 the mean-gradient satis-
ﬁes ∥gε(x) −∇f(x)∥≤κgε for all x ∈Ω.
In other words, the mean-gradient has a controllable ac-
curacy parameter ε s.t. reducing ε improves the gradient
approximation. As explained in Sec. 4.2, this property is
crucial in obtaining the convergence of EGL to stationary
points. Unlike the mean-gradient, the parametric gradient
has no such parameter and the gradient accuracy is not di-
rectly controlled. Even for zero MSE error s.t. fθ(xi) ≡yi
for all of the samples in the replay buffer (Mnih et al., 2015),
there is no guarantee of the parametric gradient accuracy.
On the contrary, overﬁtting the objective may severely hurt
the gradients approximation.
Moreover, the parametric gradient can be discontinuous
even when the parametric model has a Lipschitz continu-
ous (Hansen & Jaumard, 1995) gradient. For example, a
commonly used NN with ReLU activation is only piece-
wise differentiable. This leads to very erratic gradients even
for smooth objective functions. If the objective function
is not smooth (e.g., for noise-like functions or in singular
points), the gradient noise is exacerbated. On the other
hand, the next proposition suggests that, due to the integral
over Vε(x) that smooths the gradient, the mean gradient is
smooth whenever the objective function is continuous.
Proposition 2 (continuity). If f is continuous in V and
Vε(x) ⊂V then gε is a continuous function at x.
The mean-gradient is not necessarily continuous in disconti-
nuity points of f. One can obtain an even smoother surro-
gate for ∇f by slightly modifying the mean-gradient deﬁni-
tion
gp
ε(x) = arg min
g∈Rn
ZZ
Vε(x)Bp(x)
|g·(τ −s)−f(τ)+f(s)|2dsdτ,
where the integral domains are s ∈Bp(x) and τ ∈Vε(x).
Here, Bp(x) ⊂Vε(x) is an n-ball perturbation set with
p < ε radius that dithers the reference point (which is ﬁxed
at x in Deﬁnition 1). We term this modiﬁed version the
perturbed mean-gradient. Remarkably, while gp
ε is still a
controllably accurate model for Lipschitz continuous gra-
dients, as the integrand is x independent, gp
ε is continuous

Explicit Gradient Learning
Figure 2. Visualizing explicit gradient learning with different ε for various 2D problems from COCO test suite: (a) sharp-ridge problem
194; (b) step-ellipsoid problem 97. Comparing EGL and IGL: (c) Schaffer F7 C1000 problem 255; (d) Schaffer F7 C10 problem 240.
whenever f is merely integrable. In practice, as gε is learned
with a Lipschitz continuous model, we ﬁnd that both forms
are continuous for integrable functions. Nevertheless, Sec.
5 shows that gp
ε adds a small gain to the EGL performance.
Next, we demonstrate how the EGL smoothness (as opposed
to ∇fθ) leads to more stable and efﬁcient trajectories in both
continuous and discontinuous objective functions.
3.2. Illustrative Examples
To demonstrate these properties of the mean-gradient, we
consider 1D 2 and 2D problems from the COCO test suite.
We start by examining 4 typical 1D functions: (a) parabolic;
(b) piece-wise linear; (c) multiple local minima; and (d) step
function. We ﬁt f with a NN and compare its parametric
gradient to the mean-gradient, learned with another NN.
The NN model is identical for both functions and is based
on our spline embedding architecture (see Sec. 4.4). The
results are presented in Fig. 1. The 1st row shows that the
ﬁt ˆf is very strong s.t. the error is almost indistinguishable
to the naked eye. Nevertheless, the calculated parametric
gradient (2nd row) is noisy, even for smooth functions, as
the NN architecture is piece-wise linear. Occasionally, there
are even spikes that change the gradient sign. Traversing
the surface curves with such a function is very unstable
and inefﬁcient. On the other hand, due to the smoothing
parameter ε = 0.1 and since the NN is Lipschitz continuous,
the mean-gradient is always smooth, even for singularity
points and discontinuous gradients.
In the 3rd row we evaluate gε for different size ε parameters.
We see that by setting ε sufﬁciently high, the gradient be-
comes smooth enough, so there is no problem descending
over steps and multiple local minima functions. In practice,
we may use this property and start the descent trajectory
with a high ε. This way, the optimization process does not
commit too early to a local minimum and searches for re-
gions with lower valleys. After reﬁning ε the process will
2Since COCO does not have built-in 1D problems; we gener-
ated 1D problems based on 2D problems with f1D(x) : f2D(x, x)
settle into a local minimum, which in practice would be
much lower than minima found around the initial point.
The next experiment (Fig. 2) is executed on 2D problems
from the COCO test suite. In Fig. 2(a-b) we present the
gradient-descent steps with the mean-gradient and a constant
ε. In 2(a) the objective has a singular minimum, similar to
the |x| function’s minimum (Fig.1(a)). As ε gets smaller,
the gradients near the minimum get larger and there is a
need to reduce the learning-rate (α) in order to converge.
2(b) presents a step function. Again, we observe that for a
smaller ε the gradient has high spikes in the discontinuity
points, leading to a noisier trajectory. For that purpose, the
EGL algorithm decays both α and ε during the learning
process (see Sec. 4 for details). This lends much smoother
trajectories, as can be seen in Fig. 2(c-d). Here we executed
both EGL and IGL with the same α, ε decay pattern. We
observe that the IGL trajectories are noisy and inefﬁcient
since the parametric gradient error is not bounded. On the
other hand, EGL smoothly travels through a ravine (Fig.
2(c)) and converges to a global minimum (Fig. 2(d)).
4. Design & Analysis
In this section, we lay out the practical EGL algorithm and
analyze its asymptotic properties.
4.1. Monte-Carlo Approximation
To learn the mean-gradient, one may evaluate the integral in
Eq. (2) with Monte-Carlo samples and learn a model that
minimizes this term. Formally, for a model gθ : Ω→Rn
and a dataset Dk = {(xi, yi)}m
i=1, deﬁne the loss function
Lk,ε(θ) =
m
X
i=1
X
xj∈Vε(xi)
|(xj −xi)·gθ(xi)−yj +yi|2 (3)
and learn θ∗
k = arg minθ Lk,ε(θ), e.g. with gradient descent.
This formulation can be used to estimate the mean-gradient
for any x. Yet, practically, in each optimization iteration,
we only care about estimating it in close proximity to the

Explicit Gradient Learning
current candidate solution xk. Therefore, we assume that
the dataset Dk holds samples only from Vε(xk).
The accuracy of the learned model gθ∗
k heavily depends
on the number and locations of the evaluation points and
the speciﬁc parameterization for gθ. Still, for f with a
Lipschitz continuous gradient, i.e. f ∈C+1, we can set
bounds for the model accuracy in Vε(xk) with respect to
ε. For that purpose, we require a set of at least m ≥n + 1
evaluation points in Dk which satisfy the following poised
set deﬁnition.
Deﬁnition 2 (poised set for regression). Let Dk
=
{(xi, yi)}m
1 , m ≥n + 1 s.t. xi ∈Vε(xk) for all i. De-
ﬁne the matrix ˜Xi ∈Mm×n s.t. the j-th row is xi −xj.
Now deﬁne ˜X = ( ˜
XT
1 ···
˜
XT
m )T . The set Dk is a poised set
for regression in xk if the matrix ˜X has rank n.
Intuitively, a set is poised if its difference vectors xi −xj
span Rn. For the poised set, and a constant parameterization,
the solution of Eq. (3) is unique and it is equal to the Least-
Squares (LS) minimizer. If f has a Lipschitz continuous
gradient, then, with an admissible parametric model, the
error between gθ(x) and ∇f(x) can be proportional to ε.
We formalize this in the following theorem and corollary.
Theorem 1. Let Dk be a poised set in Vε(xk). The regres-
sion problem
gMSE = arg min
g
X
i,j∈Dk
|(xj −xi) · g −yj + yi|2
(4)
has the unique solution gMSE = ( ˜XT ˜X)−1 ˜XT δ, where
δ ∈Rm2 s.t. δi·(m−1)+j = yj −yi. Further, if f ∈C1+
and gθ ∈C0 is a parameterization with (equal or) lower
regression loss than gMSE, the following holds for all x ∈
Vε(xk):
∥∇f(x) −gθ(x)∥≤κgε
(5)
Corollary 1. For the Dk poised set, any Lipschitz contin-
uous parameterization of the form gθ(x) = F(Wx) + b
is a controllably accurate model (Audet & Hare, 2017) in
Vε(xk) for the optimal set of parameters θ∗
k.
This is obvious as we can simply set W = 0 and b = gMSE.
In this work, we are interested in using NNs which are much
stronger parameterizations than constant models. If the NN
satisﬁes the Lipschitz continuity property (e.g., with spectral
normalization) and has at least a biased output layer, then its
optimal set of parameters θ∗
k has lower regression loss than
gMSE and it is therefore a controllably accurate model.3
Finally, to incorporate learning of the perturbed mean gradi-
ent, we slightly modify Eq. (3). Note that we cannot directly
dither the reference point xi as we would need to collect
3Provided that the optimization process recovers θ∗
k, which is
not necessarily true in practice.
more samples. Instead, we may dither the gθ argument by
evaluating it in ¯xir = xi + nr where nr is uniformly sam-
pled in an n-ball with radius p. Thus, the perturbed loss
function for small p s.t. p ≪ε is
Lk,ε,p(θ) =
X
i,j∈Dk,nr
|(xj −xi) · gθ(¯xir) −yj + yi|2 (6)
4.2. Asymptotic Analysis
In this part, we assume that the function f has a Lipschitz
continuous gradient s.t. ∥∇f(x) −∇f(x′)∥≤κf∥x −x′∥.
In this case, the classical gradient descent theorem states that
the update xk+1 = xk −α∇f(xk) with learning parameter
α ≤
1
κf converges to a stationary point x∗s.t. ∥f(xk)∥→0
for k →∞(Nesterov, 2013; Lee et al., 2016).
EGL descends over a surrogate function of the gradient
that contains some amount of error. Far from x∗, where
∥∇f∥is large, the error in gε is small enough s.t. every new
candidate improves the solution, i.e. f(xk+1) ≤f(xk). If
the stationary point x∗is locally convex, then, as xk gets
closer to x∗, the gradient ∥∇f∥decreases, and eventually
the error, i.e. gε(xk) −∇f(xk), becomes so signiﬁcant that
improvement is no longer guaranteed. Nevertheless, our
analysis shows that a proper choice of εk yields monotonic
decreasing steps that converge to a stationary point.
Theorem 2. Let f : Ω→R have a Lipschitz continuous
gradient and a Lipschitz constant κf. Suppose a control-
lable mean-gradient model gε with error constant κg, the
gradient descent iteration xk+1 = xk −αgε(xk) with α s.t.
5ε
∥∇f(xk)∥≤α ≤min( 1
κg , 1
κf ) guarantees a monotonically
decreasing step s.t. f(xk+1) ≤f(xk) −2.25 ε2
α .
Corollary 2. If Theorem 2 is satisﬁed for all k, the gradi-
ent descent iteration converges to a stationary point x∗s.t.
1
K
PK
k=1 ∥f(xk)∥2 ≤12|f(x0)−f(x∗)|
αKK
.
Utilizing Theorem 2 we can design an algorithm that con-
verges to a stationary point. For that purpose we must
make sure that the learning rate abides the requirement
α ≤min( 1
κg , 1
κf ). Since this factor cannot be easily esti-
mated, a practical solution is to decay α during the optimiza-
tion process. However, to converge, the ratio ε
α must also
decay to zero. This means that ε must decay to zero faster
than α. Algorithm 2 provides both of these conditions, so it
is guaranteed to converge to a local minimum for ¯ε →0.
This analysis assumes a Lipschitz continuous gradient, but
since EGL is also designed for non-smooth functions, our
practical algorithm alleviates the requirement for strictly
monotonically decreasing steps. Instead, it calculates a
running mean over the last candidates to determine when to
decrease α and ε. The complete practical EGL algorithm
is found in the Appendix Sec. E. It also includes input and
output mapping and scaling, as described in the next section.

Explicit Gradient Learning
Algorithm 2 Convergent EGL
Input: x0, α, ε, γα < 1, γε < 1, ¯ε
k = 0
while ε ≤¯ε do
Build Model:
Collect data {(xi, yi)}m
1 , xi ∈Vε(xk)
Learn a local model gε(xk)
Gradient Descent:
xk+1 ←xk −αgε(xk)
if f(xk+1) > f(xk) −2.25 ε2
α then
α ←γαα
ε ←γαγεε
k ←k + 1
return xk
4.3. Dynamic Mappings
Unlike supervised learning with a constant dataset, BBO is
a dynamic problem. The input and output statistics change
over time as the optimization progresses. There are several
sources for this drift. First, traversing via gε changes the
input’s ﬁrst moment by updating the center of the samples
xi and the output’s ﬁrst moment by collecting smaller costs
yi. At the same time, squeezing ε and α over time decreases
the second moment statistics by reducing the variety in
{(xi, yi)}. NNs are sensitive to such distribution changes
(Brownlee, 2018) and require tweaking hyperparameters
such as learning rate and initial weight distribution to main-
tain high performance. Default numbers (e.g. learning rate
of 10−3) usually work best when the input data is normal-
ized. To regulate the statistics over the entire optimization
process, we apply a method of double dynamic mappings
for both input and output values.
From the input perspective, our dynamic mapping resembles
Trust-Region methods (Conn et al., 2000; Nocedal & Wright,
2006). Instead of searching for x∗in the entire Ωdomain by
reducing ε and α over time, we ﬁx ε and α and search for x⋆
j
in a sub-region Ωj. After ﬁnding the best candidate solution
in this sub-region we shrink Ωj by a factor of γα > 0
and ε by a factor of γε > 0. To keep the input statistics
regulated, we maintain a bijective mapping hj : Ωj →Rn :
that scales the x values to an unconstrained domain ˜x with
approximately constant ﬁrst and second moments.
In this work, Ωis assumed to be a rectangular box that de-
notes the upper and lower bounds for each entry in x. Thus
we consider element-wise bijective mappings of the form
hj(x) = (h1
j(x1), ..., hn
j (xn)) and each new sub-region,
Ωj+1 ⊂Ωj, is a smaller rectangular box centered at x⋆
j. In
the unconstrained domain ˜x, we can use the initial learning
rate, yet, due to the squeezing factor, the effective learning
rate is decayed by the γα factor. Equivalently, the effective
accuracy parameter ε is reduced by a factor of γα × γε. In
other words, we use the same NN parameters to learn a
zoomed-in problem of the original objective function.
In order to regulate the output statistics, we deﬁne a scalar,
monotonically increasing, invertible mapping rk(y) which
maps the yi samples in the dataset Dk to approximately
constant statistics. Since traversing with gε can signiﬁ-
cantly change the statistics even in the same sub-region, rk
must be dynamic and cannot be held ﬁxed for the entire
j sub-problem. Combining both input and output map-
pings, we obtain a modiﬁed set of samples {(˜xi, ˜yi)}i =
{(hj(xi), rk(yi))}i, so effectively we learn a modiﬁed
mean-gradient, denoted as ˜gεjk, of a compressed and scaled
function ˜y = ˜fjk(˜x) = rk ◦f ◦h−1
j (˜x).
If both input and output mappings are linear, then the true
mean-gradient is proportional to the modiﬁed mean-gradient
gε(x) =
∂rk
∂y
−1
∇hj(x) ⊙˜gεjk(˜x)
(7)
Even when the mappings are approximately linear inside
Vεjk(˜xk), gε(xk) can be estimated according to Eq. (7).
Hence, to maintain a controllably accurate model, we seek
mappings that preserve the linearity as much as possible. On
the other hand, strictly linear mappings may be insufﬁcient
since they are susceptible to outliers. After experimenting
with several functions, we found a sweet spot: a composition
of a linear mapping followed by a squash function that com-
presses only the outliers (see details in Appendix Sec. D).
With such mappings, we make sure both that the model is
controllably accurate and that the learning hyperparameters
are adequate for the entire optimization process.
4.4. Spline Embedding
Many black-box applications, including the experiments in
Sec. 5, have no clear inductive bias which can be exploited,
as e.g., CNNs are suitable for natural images (Cohen &
Shashua, 2016). In such cases, it is common to use feed-
forward NNs. However, we found out that the quality of
ﬁtting objectives and gradients with such NN is unsatisfac-
tory, especially for low dimensional problems. One method
to augment the input layer is through learnable embeddings
(Zhang et al., 2016). However, usually, embeddings are
applied for categorical input and they do not preserve order.
Since our input domain is continuous, we wish to design
Lipschitz continuous learnable embeddings sθ(x) s.t. for
two inputs x1 and x2 the calculated embedding satisﬁes
∥s(x1) −s(x2)∥≤κs∥x1 −x2∥for some κs > 0.
We chose to do so by learning a set of one-dimensional
splines (Reinsch, 1967). A spline is a piece-wise polynomial
deﬁned on a set of disjoint intervals with smoothness condi-
tions in the intersection points, denoted as knots. Learnable
splines were also suggested in (Fey et al., 2018) for the prob-
lem of learning over irregular grids. We used piece-wise

Explicit Gradient Learning
Figure 3. Comparing the success rate of EGL and other BBO algorithms for a budget C = 150 · 103.
linear splines of the form
sθ(x) = θi
hi
(ti+1 −x) + θi+1
hi
(x −ti)
(8)
where θ has k + 1 elements (k is the number of knots), ti
is the position of the i-th knot and hi = ti+1 −ti. Instead
of ﬁtting θ to the data as usually done in classical spline
applications, we learned these parameters as part of the op-
timization process, similar to categorical embeddings. Each
entry in the input vector was expanded by a paramterization
of several one-dimensional splines. We found this architec-
ture particularly suitable for modeling complex functions
deﬁned over unstructured input domain. Please refer to the
Appendix Sec. C for a full description and an empirical
evaluation of ﬁtting objective functions with spline-net.
5. Empirical Evaluation
In this section we describe the rigorous empirical analysis
of EGL against various BBO methods in the COCO test
suite and in a search task over the latent space of a GAN
model. The code is available at http://github.com/
MorSinay/BBO.
5.1. The COCO test suite
We tested EGL on the COCO test suite, a platform for
systematic comparison of real-parameter global optimizers.
COCO provides Black-Box functions in several dimensions
(2,3,5,10,20,40), where each dimension comprises 360 dis-
tinct problems. To test higher dimensions, we created an
extra set of 784D problems by composing a pre-trained en-
coder (Kingma & Welling, 2013) of FashionMnist images,
each with 784 pixels (Xiao et al., 2017), with a 10D COCO
problem as follows: f784D(x) : f10D(Encoder(x)).
We compared EGL with seven baselines, implemented on
Scipy and Cma Python packages: Nedler Mead, SLSQP,
POWELL, CG, COBYLA, BFGS, CMA-ES and with our
IGL implementation based on the DDPG approach. For the
784D problems we evaluated only CG, CMA-ES and IGL
which yielded results in a reasonable amount of time. We
examined two network models. To compare against other
baselines (Figues 3, 4 and 5(a)), we used our spline embed-
ding model (details in Appendix Sec. C). Since spline-net is
more time consuming, for the ablation tests in Fig. 5(b-d),
we used a lighter Fully Connected (FC) net. For additional
information, including a hyperparameters list, refer to Ap-
pendix Sec. F.
Fig. 3 presents the success rate of each algorithm with
respect to the dimension number and for a budget of C =
150 · 103. A single test-run is considered successful if: (1)
ybest −y∗≤1; and (2) ybest−y∗
y0−y∗
≤10−2. Here, y0 is the
initial score f(x0), ybest = min yk is the best-observed
value for that run, and y∗is the minimal value obtained
from all the baselines’ test-runs. This deﬁnition guarantees
that a run is successful only if its best-observed value is
near y∗, both in terms of absolute distance and in terms of
relative improvement with respect to the initial score. The
results show that EGL outperforms all other baselines for
any dimension. Most importantly, as the dimension number
increases, the performance gap between EGL and all other
baselines grows larger.
In Fig. 4 we present two performance proﬁles (Dolan &
Mor´e, 2002) for the 40D and 10D problem sets: (1) Time-To-
Solution (TTS); and (2) Best Observed (BO). TTS measures
the percentage of solved problems with respect to the time
step. We ﬁnd that both EGL and IGL have a cold start
behavior. This is a result of our design choice to start the
training with a warm-up phase where we evaluate 384 points
around x0 and train the networks before executing the ﬁrst
gradient descent step. However, as the training process
progresses, EGL outperforms all other methods and peaks
at C = 150 · 103 with the success rate of Fig. 3. The
BO proﬁle measures the percentage of problems whose
best-observed value after 150 · 103 time steps is better than
∆y ∈[y0, y∗]. We ﬁnd that for all of the possible thresholds
∆y, EGL solves more problems than any other method.
Fig.
5 visualizes the learning process of each method.
To do that, we ﬁrst calculate a scaled distance between
the best value at time t and the optimal value ∆yt
best =
mink≤t yk−y∗
y0−y∗
. We then average this number for each t, over
all runs in the same dimension problem set. This distance

Explicit Gradient Learning
Figure 4. Performance Proﬁle for 10D and 40D: Time-To-Solution (TTS) and Best-Observed (BO) after 150 · 103 steps. In BO the x-axis
linearly maps [y0, y∗] to [0, 1].
Figure 5. The scaled distance ∆yt
best as a function of t ∈[1, .., C] for: (a) EGL and baselines on 40D, (b) trust-region and output mapping
ablation test, (c) EGL with different m samples and baselines on 784D, (d) the perturbed mean-gradient and long replay buffer on 40D.
is scaled to [0, 1] and the results are presented on a log-log
scale. Fig. 5(a) presents ∆y
t
best on the 40D set. The elbow
pattern in step 384 marks the switch from x0 to x1. Unlike
the baselines that settle on a local minimum, EGL monoton-
ically decreases during the entire optimization process. It
overtakes the best baseline (CMA-ES) after ∼104 steps.
Fig. 5(b) demonstrates the advantage of output-mapping
(OM) and trust-region (TR). Here, we compared an FC net,
trained with OM and TR (FC TR OM) against the variations
FC, FC TR and FC OM. The results show a clear advantage
to using both OM and TR, yet, while OM is crucial for the
entire optimization process, the TR advantage materializes
only near the minimal value. In addition, Fig. 5(b) manifests
the gain of the spline-net (SPLINE) on top of FC TR OM.
The next experiment is executed on the high dimensional
784D problem set. In Fig. 5(c) we compare the performance
of different numbers of exploration points m = {64, 800}
against CMA-ES, CG and the IGL baselines. While Theo-
rem 1 guarantees a controllably accurate model when sam-
pling m ≥n + 1 points, remarkably, EGL generalized to an
outstanding performance and outperformed all other base-
lines even for m = 64 ≪n. Nevertheless, as expected,
more exploration points converged to a better ﬁnal value. In
practice, the choice of m should correspond to the allocated
budget size. More exploration around each candidate comes
with the cost of fewer gradient descent steps; thus, for low
budgets, one should typically choose a small m.
Next, we tested the gain of two possible modiﬁcations to
EGL: (1) perturbations with gp
ε; and (2) training gθ with
a larger replay buffer (RB) with exploration points from
the last L candidates. These tests were all executed with
the same random seed. Fig. 5(d) presents the performance
gain (1 −∆y
t
best/∆y
t
RB1 P 0) as a function of t for several
different runs. Small perturbations p = 0.01ε improved the
performance (14%), possibly since they also regulate the
NN training, yet, too large perturbations of p = 0.1ε yielded
inconsistent results. We also observed that a too long RB of
L = 16 largely hurt the performance, probably since it adds
high values to the RB which leads to a more compressed
output mapping. However, moderate RB of L = 4 had
a positive impact (10%), probably since more exploration
points near the current candidate reduce the controllable
accuracy factor κg and thus the accuracy of gε improves.
5.2. Searching the latent space of generative models
To examine EGL in a high-dimensional, complex, non-
convex and noisy domain, we experimented with the task
of searching the latent space of an image generative model

Explicit Gradient Learning
Figure 6. Searching latent space of generative models with EGL and IGL. Note that the target image is not revealed to the optimizer, only
the face attributes and landmark points. The left-hand number is the average minimal value for each algorithm over 64 different problems.
(Volz et al., 2018). Generative models learn to map between
a latent predeﬁned distribution z to a complex real-world
distribution x (e.g. images or audio). Given a trained Black-
Box generator, while it is easy to sample from the distribu-
tion of x by sampling from z, it is not straightforward to
generate an image with some desired characteristics. For
that purpose, one may apply a BBO to search the latent
space for a hidden representation z∗that generates an image
with the desired traits x∗. Here, we used a face genera-
tive model and optimized z∗to generate an image with a
required set of face attributes, landmark points and quality.
We trained a generator & discriminator for the CelebA
dataset (Liu et al., 2015) based on the BigGAN (Brock
et al., 2018) architecture and a classiﬁer for the CelebA at-
tributes. For the face landmark points, we used a pre-trained
model (Kazemi & Sullivan, 2014). The BBO was trained to
minimize the following objective:
fal(z) = λaLa(G(z))+λlLl(G(z))+λg tanh(D(G(z)))
Where: (1) La is the Cross-Entropy loss between the gen-
erated face attributes as measured by the classiﬁer and the
desired set of attributes a; (2) Ll is the MSE between the
generated landmark points and the desired set of landmarks
l ; and (3) D(G(z)) is the discriminator output, positive for
low-quality images and negative for high-equality images.
Since each evaluation of z is costly, we limited the budget
C to only 104 evaluations and the number of exploration
points in each step was only m = 32 ≪512. Such m
violates the requirement in Theorem 1. However, we found
that in practice, for ﬁnite budget and high dimensions, it
is better to take more gradient steps with a less accurate
gradient. To increase the sample efﬁciency we found that
instead of sampling points in an n-ball around the candidate
xk, it is better to sample points in a cone with an apex at
xk and a vector equal to the mean-gradient estimation at xk,
i.e. g(xk). We term this exploration strategy as gradient-
guided-exploration. For further details, please refer to the
Appendix, Sec. H.
Due to the high-dimensional problem, classic methods such
as CG and even CMA-ES fail to generate satisfying faces
(see Appendix Sec. G for sample images). In Fig. 6 we
compare the images generated by EGL and IGL. Generally,
the quality of the results depends on the image target style.
Some face traits which are more frequent in the CelebA
dataset lead to a better face quality. Some faces, speciﬁcally
with attributes such as a beard or a hat, are much harder
to ﬁnd, probably due to the suboptimality of the generator,
which usually reduces the variety of images found in the
dataset (Bau et al., 2019). Nevertheless, the results show that
EGL produces better images, both visually and according to
the ﬁnal cost value. We observed that for hard targets, IGL
frequently fails to ﬁnd any plausible solutions while EGL
yields non-perfect yet much more satisfactory candidates.
6. Conclusions
We presented EGL, a derivative-based BBO algorithm that
achieves state-of-the-art results on a wide range of optimiza-
tion problems. The essence of its success is a learnable
function that estimates the mean gradient with a control-
lable smoothness factor. Starting with a high smoothness
factor, let EGL ﬁnd global areas in the function with low
valleys. Gradually decreasing it lets EGL converge to a local
minimum. The concept of EGL can be generalized to other
related ﬁelds, such as sequential decision-making problems
(i.e. Reinforcement Learning), by directly learning the gra-
dient of the Q-function. We also demonstrated the use of
EGL in an applicative high-dimensional Black-Box prob-
lem, searching the latent space of generative models.

Explicit Gradient Learning
Acknowledgements
This work was supported in part by the Ministry of Science
& Technology, Israel.
References
Audet, C. and Hare, W. Derivative-free and blackbox opti-
mization. Springer, 2017.
Back, T. Evolutionary algorithms in theory and practice:
evolution strategies, evolutionary programming, genetic
algorithms. Oxford university press, 1996.
Balandat, M., Karrer, B., Jiang, D. R., Daulton, S., Letham,
B., Wilson, A. G., and Bakshy, E.
BoTorch: Pro-
grammable Bayesian Optimization in PyTorch. arxiv
e-prints, 2019.
URL http://arxiv.org/abs/
1910.06403.
Bardenet, R., Brendel, M., K´egl, B., and Sebag, M. Collabo-
rative hyperparameter tuning. In International conference
on machine learning, pp. 199–207, 2013.
Bau, D., Zhu, J.-Y., Wulff, J., Peebles, W., Strobelt, H.,
Zhou, B., and Torralba, A. Seeing what a gan cannot
generate. In Proceedings of the IEEE International Con-
ference on Computer Vision, pp. 4502–4511, 2019.
Bertsekas, D. P. and Scientiﬁc, A. Convex optimization
algorithms. Athena Scientiﬁc Belmont, 2015.
Bonnans, J.-F., Gilbert, J. C., Lemar´echal, C., and Sagas-
tiz´abal, C. A. Numerical optimization: theoretical and
practical aspects. Springer Science & Business Media,
2006.
Brock, A., Donahue, J., and Simonyan, K. Large scale gan
training for high ﬁdelity natural image synthesis. arXiv
preprint arXiv:1809.11096, 2018.
Brownlee, J. Better Deep Learning: Train Faster, Reduce
Overﬁtting, and Make Better Predictions. Machine Learn-
ing Mastery, 2018.
Chen, R., Menickelly, M., and Scheinberg, K. Stochastic
optimization using a trust-region method and random
models. Mathematical Programming, 169(2):447–487,
2018.
Cohen, N. and Shashua, A. Inductive bias of deep convolu-
tional networks through pooling geometry. arXiv preprint
arXiv:1605.06743, 2016.
Conn, A. R., Gould, N. I., and Toint, P. L. Trust region
methods. SIAM, 2000.
Conn, A. R., Scheinberg, K., and Vicente, L. N. Introduction
to derivative-free optimization, volume 8. Siam, 2009.
Dolan, E. D. and Mor´e, J. J. Benchmarking optimization
software with performance proﬁles. Mathematical pro-
gramming, 91(2):201–213, 2002.
Fey, M., Eric Lenssen, J., Weichert, F., and M¨uller, H.
Splinecnn: Fast geometric deep learning with continuous
b-spline kernels. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp.
869–877, 2018.
Flaxman, A. D., Kalai, A. T., and McMahan, H. B. On-
line convex optimization in the bandit setting: gradient
descent without a gradient. arXiv preprint cs/0408007,
2004.
Golovin, D., Solnik, B., Moitra, S., Kochanski, G., Karro,
J., and Sculley, D. Google vizier: A service for black-box
optimization. In Proceedings of the 23rd ACM SIGKDD
international conference on knowledge discovery and
data mining, pp. 1487–1495, 2017.
Hansen, N. The cma evolution strategy: a comparing review.
In Towards a new evolutionary computation, pp. 75–102.
Springer, 2006.
Hansen, N., Auger, A., Ros, R., Finck, S., and Poˇs´ık, P.
Comparing results of 31 algorithms from the black-box
optimization benchmarking bbob-2009. In Proceedings
of the 12th annual conference companion on Genetic and
evolutionary computation, pp. 1689–1696, 2010.
Hansen, N., Brockhoff, D., Mersmann, O., Tusar, T., Tusar,
D., ElHara, O. A., Sampaio, P. R., Atamna, A., Varelas,
K., Batu, U., Nguyen, D. M., Matzner, F., and Auger, A.
COmparing Continuous Optimizers: numbbo/COCO on
Github, March 2019. URL https://doi.org/10.
5281/zenodo.2594848.
Hansen, P. and Jaumard, B. Lipschitz optimization. In
Handbook of global optimization, pp. 407–493. Springer,
1995.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.
Howard, J. and Gugger, S. fastai: A layered api for deep
learning. arXiv preprint arXiv:2002.04688, 2020.
Kazemi, V. and Sullivan, J. One millisecond face alignment
with an ensemble of regression trees. In Proceedings
of the IEEE conference on computer vision and pattern
recognition, pp. 1867–1874, 2014.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Explicit Gradient Learning
Kingma, D. P. and Welling, M. Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114, 2013.
Lee, J. D., Simchowitz, M., Jordan, M. I., and Recht, B.
Gradient descent converges to minimizers. arXiv preprint
arXiv:1602.04915, 2016.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.
Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face
attributes in the wild. In Proceedings of International
Conference on Computer Vision (ICCV), December 2015.
Loomis, L. H. and Sternberg, S. Advanced calculus. World
Scientiﬁc, 1968.
Maheswaranathan, N., Metz, L., Tucker, G., Choi, D., and
Sohl-Dickstein, J. Guided evolutionary strategies: Aug-
menting random search with surrogate gradients. arXiv
preprint arXiv:1806.10230, 2018.
Mania, H., Guy, A., and Recht, B. Simple random search
provides a competitive approach to reinforcement learn-
ing. arXiv preprint arXiv:1803.07055, 2018.
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spec-
tral normalization for generative adversarial networks.
arXiv preprint arXiv:1802.05957, 2018.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):
529–533, 2015.
Nelder, J. A. and Mead, R. A simplex method for func-
tion minimization. The computer journal, 7(4):308–313,
1965.
Nesterov, Y. Introductory lectures on convex optimization:
A basic course, volume 87. Springer Science & Business
Media, 2013.
Nocedal, J. and Wright, S. Numerical optimization. Springer
Science & Business Media, 2006.
Pan, Z., Yu, W., Yi, X., Khan, A., Yuan, F., and Zheng,
Y. Recent progress on generative adversarial networks
(gans): A survey. IEEE Access, 7:36322–36333, 2019.
Powell, M. J. An efﬁcient method for ﬁnding the mini-
mum of a function of several variables without calculating
derivatives. The computer journal, 7(2):155–162, 1964.
Powell, M. J. A view of algorithms for optimization without
derivatives. Mathematics Today-Bulletin of the Institute of
Mathematics and its Applications, 43(5):170–174, 2007.
Reinsch, C. H. Smoothing by spline functions. Numerische
mathematik, 10(3):177–183, 1967.
Rios, L. M. and Sahinidis, N. V. Derivative-free optimiza-
tion: a review of algorithms and comparison of software
implementations. Journal of Global Optimization, 56(3):
1247–1293, 2013.
Rosemarin, H., Rosenfeld, A., and Kraus, S. Emergency de-
partment online patient-caregiver scheduling. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pp. 695–701, 2019.
Ruder, S. An overview of gradient descent optimization
algorithms. arXiv preprint arXiv:1609.04747, 2016.
Saremi, S. On approximating ∇f with neural networks.
arXiv preprint arXiv:1910.12744, 2019.
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz,
P. Trust region policy optimization. In International
Conference on Machine Learning, pp. 1889–1897, 2015.
Sener, O. and Koltun, V. Learning to guide random search.
In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?
id=B1gHokBKwS.
Shewchuk, J. R. et al. An introduction to the conjugate
gradient method without the agonizing pain, 1994.
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M.,
Bolton, A., et al. Mastering the game of go without
human knowledge. Nature, 550(7676):354, 2017.
Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N.,
Sundaram, N., Patwary, M., Prabhat, M., and Adams,
R. Scalable bayesian optimization using deep neural net-
works. In International conference on machine learning,
pp. 2171–2180, 2015.
Van Laarhoven, P. J. and Aarts, E. H. Simulated annealing.
In Simulated annealing: Theory and applications, pp.
7–15. Springer, 1987.
Vemula, A., Sun, W., and Bagnell, J. A. Contrasting explo-
ration in parameter and action space: A zeroth-order opti-
mization perspective. arXiv preprint arXiv:1901.11503,
2019.
Volz, V., Schrum, J., Liu, J., Lucas, S. M., Smith, A., and
Risi, S. Evolving mario levels in the latent space of a
deep convolutional generative adversarial network. In
Proceedings of the Genetic and Evolutionary Computa-
tion Conference, pp. 221–228, 2018.

Explicit Gradient Learning
Wang, X., Girshick, R., Gupta, A., and He, K. Non-local
neural networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 7794–
7803, 2018.
Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a
novel image dataset for benchmarking machine learning
algorithms, 2017.
Yuan, X., He, P., Zhu, Q., and Li, X. Adversarial examples:
Attacks and defenses for deep learning. IEEE transac-
tions on neural networks and learning systems, 30(9):
2805–2824, 2019.
Zhang, W., Du, T., and Wang, J. Deep learning over multi-
ﬁeld categorical data. In European conference on infor-
mation retrieval, pp. 45–57. Springer, 2016.
Zhen, L., Wang, K., Hu, H., and Chang, D. A simulation
optimization framework for ambulance deployment and
relocation problems. Computers & Industrial Engineer-
ing, 72:12–23, 2014.

Explicit Gradient Learning
A. Theoretical Analysis
A.1. The Mean-Gradient
Deﬁnition 1. The mean-gradient in a region around x of radius ε > 0 is
gε(x) = arg min
g∈Rn
Z
Vε(x)
|g · τ −f(x + τ) + f(x)|2dτ
(9)
where Vε(x) ⊂Rn is a convex subset s.t. ∥x′ −x∥≤ε for all x′ ∈Vε(x) and the integral domain is over τ s.t.
x + τ ∈Vε(x).
Proposition 1 (controllable accuracy). For any twice differentiable function f ∈C1, there is κg(x) > 0, so that for any
ε > 0 the mean-gradient satisﬁes ∥gε(x) −∇f(x)∥≤κg(x)ε for all x ∈Ω.
Proof. Recall the Taylor theorem for a twice differentiable function f(x + τ) = f(x) + ∇f(x) · τ + Rx(τ), where Rx(τ)
is the remainder. Since the gradient is continuous,by the fundamental theorem for line integrals
f(x + τ) = f(x) +
Z 1
0
∇f(x + tτ) · τdt = f(x) + ∇f(x) · τ +
Z 1
0
(∇f(x + tτ) −∇f(x)) · τdt
(10)
Since f ∈C1+, we also have |∇f(x) −∇f(x + τ)| ≤κf∥τ∥. We can use this property to bound the remainder in the
Taylor expression.
Rx(τ) =
Z 1
0
(∇f(x + tτ) −∇f(x)) · τdt
≤κf
Z 1
0
∥x + tτ −x∥· ∥τ∥dt = κf∥τ∥2
Z 1
0
tdt = 1
2κf∥τ∥2
(11)
Now, by the deﬁnition of gε, an upper bound for L(gε(x)) is
L(gε(x)) ≤L(∇f(x)) =
Z
Vε(x)
|∇f(x) · τ −f(τ) + f(s)|2dτ =
Z
Vε(x)
|Rx(τ)|2dτ
≤1
4κ2
f
Z
Vε(x)
|∥τ∥2|2dτ ≤κfε4|Vε(x)| = 1
4κ2
fεn+4|V1(x)|
To develop the lower bound we will assume that dim(span(Vε(x))) = n and we will use the following deﬁnition
Mε(x) = min
ˆn
Z
Vε(x)\V ε
2 (x)

τ
∥τ∥· ˆn

2
dτ
(12)
where ˆn ∈Rn s.t. ∥ˆn∥= 1 and we assumed that V ε
2 ⊂Vε. As the dimension of Vε(x) is n, it is obvious that Mε(x) > 0.
The lower bound is

Explicit Gradient Learning
L(gε(x)) =
Z
Vε(x)
|gε(x) · τ −∇f(x) · τ + ∇f(x) · τ −f(x + τ) + f(x)|2 dτ
≥
Z
Vε(x)
(|gε(x) · τ −∇f(x) · τ| −|∇f(x) · τ −f(x + τ) + f(x)|)2 dτ
=
Z
Vε(x)
|gε(x) · τ −∇f(x) · τ|2 dτ +
Z
Vε(x)
|∇f(x) · τ −f(x + τ) + f(x)|2 dτ
−2
Z
Vε(x)
|gε(x) · τ −∇f(x) · τ| · |∇f(x) · τ −f(x + τ) + f(x)| τ
≥
Z
Vε(x)
|gε(x) · τ −∇f(x) · τ|2 dτ −2
Z
Vε(x)
|gε(x) · τ −∇f(x) · τ| · |∇f(x) · τ −f(x + τ) + f(x)| τ
≥
Z
Vε(x)
|gε(x) · τ −∇f(x) · τ|2 dτ −κf
Z
Vε(x)
|gε(x) · τ −∇f(x) · τ| · ∥τ∥2τ
≥∥gε(x) −∇f(x)∥2
Z
Vε(x)
|ˆn(x) · τ|2 dτ −κf ∥gε(x) −∇f(x)∥
Z
Vε(x)
·∥τ∥3τ
≥∥gε(x) −∇f(x)∥2
Z
Vε(x)\V ε
2 (x)
|ˆn(x) · τ|2 dτ −κf ∥gε(x) −∇f(x)∥εn+3|V1(x)|
≥∥gε(x) −∇f(x)∥2 ε
2
2 Z
Vε(x)\V ε
2 (x)
ˆn(x) ·
τ
∥τ∥

2
dτ −κf ∥gε(x) −∇f(x)∥εn+3|V1(x)|
≥∥gε(x) −∇f(x)∥2 ε
2
2
εn
Z
V1(x)\V 1
2 (x)
ˆn(x) ·
τ
∥τ∥

2
dτ −κf ∥gε(x) −∇f(x)∥εn+3|V1(x)|
≥1
4 ∥gε(x) −∇f(x)∥2 εn+2M1(x) −κf ∥gε(x) −∇f(x)∥εn+3|V1(x)|
Combining the upper and lower bound we obtain
1
4 ∥gε(x) −∇f(x)∥2 εn+2M1(x) −κf ∥gε(x) −∇f(x)∥εn+3|V1(x)| ≤1
4κ2
fεn+4|V1(x)|
⇒
M1(x) ∥gε(x) −∇f(x)∥2 −4κfε|V1(x)| ∥gε(x) −∇f(x)∥−κ2
fε2|V1(x)| ≤0
⇒
∥gε(x) −∇f(x)∥≤εκf
2|V1(x)| +
p
4|V1(x)|2 + |V1(x)|M1(x)
M1(x)
Proposition 2 (continuity). If f(x) is continuous in V s.t. Vε(x) ⊂V , then the mean-gradient is a continuous function at
x.
Proof. Let us deﬁne the two-variable function L(x, g):
L(x, g) =
Z
Vε(x)
|g · τ −f(x + τ) + f(x)|2dτ
(13)
The mean-gradient is the global minimum of this function for each x. Notice that L(x, g) is a polynomial function in g

Explicit Gradient Learning
since if f is an integrable function, then we can write
L(x, g) =
Z
Vε(x)
|g · τ −f(x + τ) + f(x)|2dτ
=
Z
Vε(x)
|g · τ|2dτ −2
Z
Vε(x)
g · τ(f(x + τ) −f(x))dτ +
Z
Vε(x)
|f(x + τ) −f(x)|2dτ
= g · A(x)g + g · b(x) + c(x)
(14)
where A(x) =
R
Vε(x) ττ T dτ, b(x) = −2
R
Vε(x) τ(f(x + τ) −f(x))dτ and c(x) =
R
Vε(x) |f(x + τ) −f(x)|2dτ. Without
loss of generality for this proof, we can ignore the constant c(x) as it does not change the minimum point. In addition, note
that A(x) is constant for all x since the domain Vε(x) is invariant for x and the integrand does not depend on f.
Since L(x, g) is bounded from below, L(x, g) ≥0, it must have a minimum s.t. A ≥0. Assume that A > 0 (e.g. when Vε
is an n-ball of radius ε), then for each x there is a unique minimum for L(x, g) and this minimum is the mean-gradient
gε(x).
To show the continuity of gε(x), deﬁne F(x, g) = ∇gL(x, g), F maps R2n →Rn. Assume that for x0, gε(x0) is the
mean-gradient. Therefore, F(x0, gε(x0)) = 0. Since A > 0, this means that the derivative ∇gF(x0, gε(x0)) is invertible.
We will apply a version of the implicit function theorem (Loomis & Sternberg, 1968) (Theorem 9.3 pp. 230-231) to show
that there exists a unique and continuous mapping h(x) s.t. F(x, h(x)) = 0.
To apply the implicit function theorem, we need to show that F(x, g) is continuous and the derivative ∇gF is continuous
and invertible. The latter is obvious since ∇gF = A > 0 is a constant positive deﬁnite matrix. F(x, g) is also continuous
with respect to g, therefore it is left to verify that F(x, g) is continuous with respect to x.
Lemma 3. If f(x) is continuous in V s.t. Vε(x) ⊂V , then L(x, g) is continuous in x.
Proof.
|L(x, g) −L(x′, g)| = |g · A(x)g + g · b(x) −g · A(x′)g −g · b(x)|
= |g · b(x) −g · b(x)| ≤∥g∥

Z
Vε(x)
τ(f(x + τ) −f(x))dτ −
Z
Vε(x′)
τ(f(x′ + τ) −f(x′))dτ

(15)
To write both integrals with the same variable, we change variables to τ = ˜τ −x in the ﬁrst integrand and τ = ˜τ −x′ in the
second integrand.
|L(x, g) −L(x′, g)| ≤∥g∥

Z
Vε(x)
(˜τ −x)(f(˜τ) −f(x))d˜τ −
Z
Vε(x′)
(˜τ −x′)(f(˜τ) −f(x′))d˜τ

≤∥g∥C1 + ∥g∥C2 + ∥g∥C3 + ∥g∥C4 + ∥g∥C5
(16)
Where
C1 = |f(x′) −f(x)|
Z
Vε(x)∩Vε(x′)
∥˜τ∥d˜τ
(17)
C2 = ∥x −x′∥
Z
Vε(x)∩Vε(x′)
|f(˜τ)|d˜τ
(18)
C3 = ∥xf(x) −x′f(x′)∥
Z
Vε(x)∩Vε(x′)
d˜τ
(19)
C4 =
Z
Vε(x)\Vε(x′)
∥˜τ −x∥· |f(˜τ) −f(x)|d˜τ
(20)
C5 =
Z
Vε(x′)\Vε(x)
∥˜τ −x′∥· |f(˜τ) −f(x′)|d˜τ
(21)
(22)

Explicit Gradient Learning
Taking x′ →x, C1, C2, C3 all go to zero as the integral is ﬁnite but x′ →x and f(x′) →f(x). For C4 and C5, note that
the integrand is bounded but the domain size goes to zero as x′ →x. To see that we will show that |Vε(x) \ Vε(x′)| ≤
|Aε(x)| · ∥x −x′∥, where |Aε(x)| is the surface area of Vε.
Lemma 4. |Vε(x) \ Vε(x′)| ≤|Aε(x)| · ∥x −x′∥
Proof. First, note that if u ∈Vε(x), then u + x′ −x ∈Vε(x′). Take P ⊂Vε s.t. p ∈P if and only if distance(Aε(x), p) ≥
∥x −x′∥and p ∈Vε(x). For any p ∈P, p −x + x′ ∈Vε(x), thus, following our ﬁrst argument p ∈Vε(x′).
We obtain that P ∩Vε(x) \ Vε(x′) = Φ, thus |Vε(x) \ Vε(x′)| ≤|Vε(x) \ P|. However, all points q ∈Vε(x) \ P satisfy
distance(Aε(x), q) ≤∥x −x′∥, therefore |Vε(x) \ P| ≤|Aε(x)| · ∥x −x′∥.
Following Lemma 4 we obtain that the integral in C4 and C5 goes to zero and therefore the distance |L(x, g) −L(x′, g)| →0
as x →x′.
L(x, g) continuous in x and g with a continuous derivative in g implies that ∇gL(x, g) is continuous in x. We can now
apply Theorem 9.3 pp. 230-231 in (Loomis & Sternberg, 1968) and conclude that there is a unique continuous mapping
h(x) s.t. F(x, h(x)) = 0. Since A > 0, this means that such a mapping deﬁnes a local minimum for L(x, g) in g. Further,
since L(x, g) is a second degree polynomial in g, this is a unique global mapping. Therefore, it must be equal to gε(x) and
hence gε is continuous in x.
A.2. Parametric approximation of the mean-gradient
In this section we analyze the Monte-Carlo learning of the mean-gradient with a parametric model. Generally, we deﬁne a
parametric model gθ and learn θ∗by minimizing the term
L(gθ, ε) =
N
X
i=1
X
xj∈Vε(xi)
|(xj −xi) · gθ(xi) −yj + yi|2
(23)
We start by analyzing constant parameterization of the mean-gradient around a candidate xk. We consider two cases: (1)
interpolation, where there are exactly n + 1 evaluation points; and (2) regression where there are m > n + 1 evaluation
points. This line of arguments follows the same approach taken in (Audet & Hare, 2017), Chapter 9.
A.2.1. CONSTANT PARAMETERIZATION WITH n + 1 INTERPOLATION POINTS
Deﬁnition 3. A set of n + 1 points {xi}n
0, s.t. every subset of n points spans Rn, is a poised set for constant interpolation.
Proposition 3. For a constant paramterization g(x) = g, a poised set has a unique solution with zero regression error.
min
g
X
i,j∈D
|(xj −xi) · g −yj + yi|2 = 0
(24)
Proof. Deﬁne the matrix ˜Xi ∈Mn×n s.t. the j-th row is xi −xj and δi ∈Rn s.t. δi,j = yj −yi. We may transform Eq.
(24) into n + 1 sets of linear equations:
∀i ˜Xig = δi
(25)
While there are n + 1 different linear systems of equations, they all have the same solution gmin. To see that, deﬁne the
system of equation ˜X˜g = r where
˜X =





x0
1
x1
1
...
...
xn
1




,
g =







g0
g1
...
gn−1
s







,
r =





y0
y1
...
yn





(26)

Explicit Gradient Learning
and s is an additional slack variable. For all i we can apply an elementary row operation of subtracting the i-th row s.t. the
updated system is












x0 −xi
0
...
...
xi−1 −xi
0
0
0
xi+1 −xi
0
...
...
xn −xi
0

y0 −yi
...
yi−1 −yi
0
yi+1 −yi
...
yn −yi












(27)
Reducing the zeroed i-th row we get the system of equation ˜Xig = δi which has a unique solution since the set {xj} \ xi
spans Rn.
Corollary 3. For any parameterization of the form gθ = f(Wx) + b and a poised set {xj}n
0 we have an optimal solution
where W ∗= 0 and b∗= gmin. Speciﬁcally it also holds for a Neural Network with a biased output layer.
Lemma 5. For a poised set {xj}n
0 s.t. ∥xi −xj∥≤ε and a mean-gradient estimator gθ ∈C0 with zero interpolation error,
the following holds
∥∇f(x) −gθ(x)∥≤κgε
(28)
Proof. f ∈C1+, hence for any xi in the poised set and x s.t. ∥x −xi∥≤ε we have
∥∇f(x) −gθ(x)∥≤
∥∇f(x) −∇f(xi)∥+ ∥∇f(xi) −gθ(xi)∥+ ∥gθ(xi) −gθ(x)∥≤(κf + κgθ)ε + ∥∇f(xi) −gθ(xi)∥
(29)
Where κgθ is the Lipschitz constant of gθ it is left to bound the last term. First, note that for all xj in the poised set we have
that
(xj −xi) · gθ(xi) = f(xj) −f(xi) ≤(xj −xi) · ∇f(xi) + 1
2κfε2
(30)
where the last equation comes from the second error term in the Taylor series expansion in xi (see Proposition A.1).
Returning to our deﬁnition of ˜Xi (see proposition 3) we can write
∥˜Xi(∇f(xi) −gθ(xi))∥=
sX
i
|(xj −xi) · (gθ(xi) −∇f(xi))|2 ≤1
2
√nκfε2
(31)
Using that property we have
∥∇f(xi) −gθ(xi)∥= ∥˜X−1
i
˜Xi(∇f(xi) −gθ(xi))∥≤∥˜X−1
i
∥∥˜Xi(∇f(xi) −gθ(xi))∥≤1
2
√nκf∥˜X−1
i
∥ε2.
(32)
∥˜X−1
i
∥=
1
min σ( ˜
Xi), where σ is the singular values. Notice that the rows of ˜Xi are xj −xi ∝ε, thus we can scale them
by ε. In this case, since the poised set spans Rn, the minimal singular value of 1
ε ˜Xi is ﬁnite and does not depend on ε.
Therefore, we obtain
∥∇f(xi) −gθ(xi)∥≤1
2
√n∥(1
ε
˜Xi)−1∥κfε = O(nε)
(33)
Therefore,
∥∇f(x) −gθ(x)∥≤

κf + 1
2
√nκgθ + ∥(1
ε
˜Xi)−1∥κf

ε
(34)
Notice that we only required gθ to be a zero-order Lipschitz continuous and we do not set any restrictions on its gradient. For
Neural Networks, having the C0 property is relatively easy, e.g. with spectral normalization (Miyato et al., 2018). However,
many NNs are not C1, e.g. NN with ReLU activations.

Explicit Gradient Learning
A.2.2. CONSTANT PARAMETERIZATION WITH m > n + 1 REGRESSION POINTS
We can extend the results of Sec. A.2.1 to the regression problem where we have access to m > n + 1 points {xi}m−1
0
.
We wish to show that the bounds for a constant mean-gradient solution for the regression problem in Eq. (23) are also
controllably accurate, i.e. ∥∇f(x) −g∥≤κgε. As in the interpolation case, we start with the deﬁnition of the poised set for
regression.
Deﬁnition 2 (poised set for regression). Let Dk = {(xi, yi)}m
1 , m ≥n + 1 s.t. xi ∈Vε(xk) for all i. Deﬁne the matrix
˜Xi ∈Mm×n s.t. the j-th row is xi −xj. Now deﬁne ˜X = ( ˜
XT
1 ···
˜
XT
m )T . The set Dk is a poised set for regression in xk if
the matrix ˜X has rank n.
Intuitively, a set is poised if its difference vectors xi −xj span Rn. For the poised set, and a constant parameterization, the
solution of Eq. (35) is unique and it equals to the Least- Squares (LS) minimizer. If f has a Lipschitz continuous gradient,
then the error between gε(x) and ∇f(x) is proportional to ε. We formalize this argument in the next proposition.
Theorem 1. Let Dk be a poised set in Vε(xk). The regression problem
gMSE = arg min
g
X
i,j∈Dk
|(xj −xi) · g −yj + yi|2
(35)
has the unique solution gMSE = ( ˜XT ˜X)−1 ˜XT δ, where δ ∈Rm2 s.t. δi·(m−1)+j = yj −yi. Further, if f ∈C1+ and
gθ ∈C0 is a parameterization with lower regression loss gMSE, the following holds
∥∇f(x) −gθ(x)∥≤κgε
(36)
Proof. The regression problem can be written as
g = arg min
g
∥˜Xg −δ∥2
(37)
This is the formulation for the mean-square error problem with matrix ˜X and target δ. The minimizer of this function is the
standard mean-square error minimizer which is unique as ˜X has rank n.
g = ( ˜XT ˜X)−1 ˜XT δ
(38)
Lemma 6. Let f ∈C1+ on Bε(xj) with a Lipschitz constant κf. For any triplet xi, xj, xk s.t. ∥xi −xj∥≤ε and
∥xk −xj∥≤ε
|f(xk) −f(xj) −(xk −xj) · ∇f(xi)| ≤3
2κfε2
(39)
Proof.
|f(xk) −f(xj) −(xk −xj) · ∇f(xi)| =

Z 1
0
(xk −xj) · ∇f(xj + τ(xk −xj))dτ −(xk −xj) · ∇f(xi)

=

Z 1
0
(xk −xj) · (∇f(xj + τ(xk −xj)) −∇f(xi)) dτ

≤

Z 1
0
∥xk −xj∥· ∥∇f(xj + τ(xk −xj)) −∇f(xi)∥dτ

≤κfε

Z 1
0
∥xj + τ(xk −xj) −xi∥dτ

≤κfε

Z 1
0
∥xj −xi∥+ ∥τ(xk −xj)∥dτ

≤κfε
ε + ε
Z 1
0
τdτ

= 3
2κfε2
(40)

Explicit Gradient Learning
Applying the previous Lemma, for all x ∈Vε(xk)
∥˜X∇f(x) −δ∥2 =
m−1
X
k=0
m−1
X
j=0
|(xk −xj)T ∇f(x) −(f(xk) −f(xj))|2 ≤m2
3
2κfε2
2
(41)
hence ∥˜X∇f(x) −δ∥≤3m
2 κfε2.
Notice also that if Dk is a poised set s.t. the matrix ˜X has rank n s.t. ˜X† = ( ˜XT ˜X)−1 ˜XT exists and we have that
∥∇f(x) −˜X†δ∥=
 ˜X† 
˜X∇f(x) −δ
 ≤∥˜X†∥· ∥˜X∇f(xi) −δ∥≤∥˜X†∥3m
2 κfε2
(42)
As in the interpolation case, we can multiply ˜X† by ε to obtain a matrix which is invariant to the size of ε. Denote the scaled
pseudo-inverse as ˜X‡ = ε( ˜XT ˜X)−1 ˜XT . Therefore,
∥∇f(x) −gMSE∥≤∥˜X‡∥3m
2 κfε
(43)
If gθ has a lower regression error than gMSE, then there exists at least one point xi s.t. xi ∈Dk and ∥∇f(xi) −gθ(xi)∥≤
∥∇f(xi) −gMSE∥. In this case we have
∥∇f(x) −gθ(x)∥≤∥∇f(x) −∇f(xi)∥+ ∥∇f(xi) −gθ(xi)∥+ ∥gθ(xi) −gθ(x)∥
≤(κf + κgθ)ε + ∥∇f(xi) −gθ(xi)∥
≤(κf + κgθ)ε + ∥∇f(xi) −gMSE∥
≤(κf + κgθ)ε + ∥˜X‡∥3m
2 κfε
=

κf + κgθ + ∥˜X‡∥3m
2 κf

ε
Corollary 1. For the Dk poised set, any Lipschitz continuous parameterization of the form gθ(x) = F(Wx)+b, speciﬁcally
NNs, is a controllably accurate model in Vε(xk) for the optimal set of parameters θ∗.
A.3. Convergence Analysis
For clarity, we replace the subscript θ in gθ and write gε to emphasize that our model for the mean-gradient is controllably
accurate.
Theorem 2. Let f : Ω→R be a function with Lipschitz continuous gradient, i.e. f ∈C+1 and a Lipschitz constant κf.
Suppose a controllable mean-gradient model gε with error constant κg, the gradient descent iteration xk+1 = xk −αgε(xk)
with α s.t.
5ε
∥∇f(xk)∥≤α ≤min( 1
κg , 1
κf ) guarantees a monotonically decreasing step s.t. f(xk+1) ≤f(xk) −2.25 ε2
α .
Proof. For f ∈C+1 the following inequality holds for all xk (see proof in Proposition A.1)
f(x) ≤f(xk) + (x −xk) · ∇f(xk) + 1
2κf∥x −xk∥2
(44)
Plugging in the iteration update xk+1 = xk −αgε(xk) we get
f(xk+1) ≤f(xk) −αgε(xk) · ∇f(xk) + α2 1
2κf∥gε(xk)∥2
(45)
For a controllable mean-gradient we can write ∥gε(x) −∇f(x)∥≤εκg, therefore we can write gε(x) = ∇f(x) + εκgξ(x)
s.t. ∥ξ(x)∥≤1 so the inequality is
f(xk+1) ≤f(xk) −α∥∇f(xk)∥2 −αεκgξ(xk) · ∇f(xk) + α2 1
2κf∥∇f(x) + εκgξ(x)∥2
(46)

Explicit Gradient Learning
Using the equality ∥a + b∥2 = ∥a∥2 + 2a · b + ∥b∥2 and the Cauchy-Schwartz inequality inequality a · b ≤∥a∥∥b∥we can
write
f(xk+1) ≤f(xk) −α∥∇f(xk)∥2 + αεκg∥ξ(xk)∥· ∥∇f(xk)∥+ α2 1
2κf
 ∥∇f(x)∥2 + 2εκg∥ξ(x)∥· ∥∇f(xk)∥+ ε2κ2
g∥ξ(x)∥2
≤f(xk) −α∥∇f(xk)∥2 + αεκg∥∇f(xk)∥+ α2
2 κf∥∇f(x)∥2 + α2κfεκg∥∇f(xk)∥+ α2ε2
2
κfκ2
g
(47)
Using the requirement α ≤min( 1
κg , 1
κf ) it follows that ακg ≤1 and ακf ≤1 so
f(xk+1) ≤f(xk) −α∥∇f(xk)∥2 + ε∥∇f(xk)∥+ α
2 ∥∇f(x)∥2 + ε∥∇f(xk)∥+ ε2
2 κg
= f(xk) −α
2 ∥∇f(xk)∥2 + 2ε∥∇f(xk)∥+ ε2
2 κg
(48)
Now, for α s.t. α ≥
5ε
∥∇f(xk)∥then ε ≤∥∇f(xk)∥α
5 . Plugging it to our inequality we obtain
f(xk+1) ≤f(xk) −α
2 ∥∇f(xk)∥2 + 2α
5 ∥∇f(xk)∥2 + α2
100κg∥∇f(xk)∥2
≤f(xk) −α
2 ∥∇f(xk)∥2 + 2α
5 ∥∇f(xk)∥2 + α
100∥∇f(xk)∥2
= f(xk) −0.09α∥∇f(xk)∥2
≤f(xk) −2.25ε2
α
(49)
Therefore, we obtain a monotonically decreasing step with ﬁnite size improvement, hence after a ﬁnite number of steps we
obtain x⋆for which ∥∇f(x⋆)∥≤5ε
α .
Corollary 2. When Theorem 2 is satisﬁed for all k, the gradient descent iteration converges to a stationary point x∗s.t.
1
K
PK
k=1 ∥f(xk)∥2 ≤12|f(x0)−f(x∗)|
αKK
.
Proof. Recall that for all k we have
f(xk+1) ≤f(xk) −0.09αk∥∇f(xk)∥2 ⇒∥∇f(xk)∥2 ≤12
αk
(f(xk) −f(xk+1))
(50)
Summing over these terms we obtain
K−1
X
k=0
∥∇f(xk)∥2 ≤
K−1
X
k=0
12
αk
(f(xk) −f(xk+1)) ≤12
αK
K−1
X
k=0
(f(xk) −f(xk+1)) = 12
αK
(f(x0) −f(xK))
(51)
Since the domain is bounded and the gradient is Lipschitz continuous, from the fundamental theorem of line integral it
follows that the function is bounded. Hence, a monotonically decreasing sequence bounded from below, must converge to
the sequence inﬁmum denoted as x∗. Therefore,
K−1
X
k=0
∥∇f(xk)∥2 ≤12
αK
|f(x0) −f(xK)| ≤12
αK
|f(x0) −f(x∗)|
(52)

Explicit Gradient Learning
B. The Perturbed Mean-Gradient
Deﬁnition 4. The perturbed mean-gradient in x with averaging radius ε > 0 and perturbation radius p < ε is
gp
ε(x) = arg min
g∈Rn
ZZ
Vε(x)Bp(x)
|g · (τ −s) −f(τ) + f(s)|2dsdτ
(53)
where Bp(x) ⊂Vε(x) ⊂Rn are convex subsets s.t. ∥x′ −x∥≤ε for all x′ ∈Vε(x) and the integral domain is over
τ ∈Vε(x) and s ∈Bp(x).
We denote Vε(x) as the averaging domain and Bp(x) as the perturbation domain and usually set |Vε| ≫|Bp|. The purpose
of Vε is to average the gradient in a region of radius ε and the perturbation is required to obtain smooth gradients around
discontinuity points.
Proposition 4 (controllable accuracy). For any function f ∈C1, there is κg > 0, so that for any ε > 0 the perturbed
mean-gradient satisﬁes ∥gp
ε −∇f(x)∥≤κgε for all x ∈Ω.
Proof. Recall the Taylor theorem for a twice differentiable function f(τ) = f(s) + ∇f(s) · (τ −s) + Rs(τ), where Rs(τ)
is the reminder. Since the gradient is continuous, we can write
f(τ) = f(s) + ∇f(s) · (τ −s) +
Z 1
0
(∇f(s + t(τ −s)) −∇f(s)) · (τ −s)dt
(54)
Since f ∈C1, we also have |∇f(x) −∇f(s)| ≤κf∥x −s∥. We can use this property to bound the reminder in the Taylor
expression.
Rs(τ) =
Z 1
0
(∇f(s + t(τ −s)) −∇f(s)) · (τ −s)dt
≤κf
Z 1
0
∥s + t(τ −s) −s∥· ∥τ −s∥dt ≤κf
2 ∥τ −s∥2
(55)
By the deﬁnition of gp
ε, an upper bound for L(gp
ε(x)) is
L(gε(x)) ≤L(∇f(x)) =
ZZ
Vε(x)Bp(x)
|∇f(x) · (τ −s) −f(τ) + f(s)|2dsdτ
=
ZZ
Vε(x)Bp(x)
|(∇f(x) −∇f(s) + ∇f(s)) · (τ −s) −f(τ) + f(s)|2dsdτ
≤
ZZ
Vε(x)Bp(x)
|∇f(s) · (τ −s) −f(τ) + f(s)|2dsdτ
+ 2
ZZ
Vε(x)Bp(x)
∥∇f(x) −∇f(s)∥· ∥τ −s∥· |∇f(s) · (τ −s) −f(τ) + f(s)|dsdτ
+
ZZ
Vε(x)Bp(x)
∥∇f(x) −∇f(s)∥2 · ∥τ −s∥2dsdτ
≤
ZZ
Vε(x)Bp(x)
κ2
f
4 ∥τ −s∥4 + κ2
f∥x −s∥· ∥τ −s∥3 + κ2
f∥x −s∥· ∥τ −s∥2dsdτ
≤16κ2
fε4|Vε(x)||Bp(x)| = 16κ2
fεn+4pn|V1(x)||B1(x)|
Noticed that we used the inequalities: (1) ∥∇f(x) −∇f(s)∥≤κf∥x −s∥≤κfε, (2) ∥x −s∥≤ε; and (3) ∥τ −s∥≤2ε.

Explicit Gradient Learning
For the lower bound we assume that p = ε¯p and ¯p < 1
4. Note that for any other upper bound on ¯p we can derive an
alternative bound.
The lower bound is
L(gε(x)) =
ZZ
Vε(x)Bp(x)
|gε(x) · (τ −s) −f(τ) + f(s)|2dsdτ
=
ZZ
Vε(x)Bp(x)
|(gε(x) −∇f(x) + ∇f(x)) · (τ −s) −f(τ) + f(s)|2dsdτ
≥
ZZ
Vε(x)Bp(x)
|(gε(x) −∇f(x)) · (τ −s)|2 −2|(gε(x) −∇f(x)) · (τ −s)| · |∇f(x) −f(τ) + f(s)|dsdτ
≥
ZZ
Vε(x)\V 3ε
4 (x)Bp(x)
|(gε(x) −∇f(x)) · (τ −s)|2sdτ
−4ε∥(gε(x) −∇f(x))∥
ZZ
Vε(x)Bp(x)
(|∇f(s) −f(τ) + f(s)| + |∇f(x) −∇f(s)|) dsdτ
≥∥gε(x) −∇f(x)∥2 ε
2
2
ZZ
Vε(x)\V 3ε
4 (x)Bp(x)
ˆn(x) ·
τ −s
∥τ −s∥

2
sdτ
−4ε∥(gε(x) −∇f(x))∥
ZZ
Vε(x)Bp(x)
1
2κf∥τ −s∥2 + κf∥x −s∥2dsdτ
≥∥gε(x) −∇f(x)∥2εnpn ε
2
2
ZZ
V1(x)\V 3
4 (x)B1(x)
ˆn(x) ·
τ −s
∥τ −s∥

2
sdτ −2.5ε∥(gε(x) −∇f(x))∥εnpnκf
27
32ε2|V1(x)||B1(x)|
= 1
4εn+2pnM1∥(gε(x) −∇f(x))∥2 −2.5εn+3pnκf∥(gε(x) −∇f(x))∥|V1(x)||B1(x)|
Combining the lower and upper bound we obtain
M1∥(gε(x) −∇f(x))∥2 −10εκf∥(gε(x) −∇f(x))∥|V1(x)||B1(x)| −64κ2
fε2|V1(x)||B1(x)| ≤0
⇒∥(gε(x) −∇f(x))∥≤κgε
where
κg = κf
10|V1(x)||B1(x)| +
p
100|V1(x)|2|B1(x)|2 + 256|V1(x)||B1(x)|M1(x)
M1(x)
Proposition 5 (continuity). If f(x) is Riemann integrable in Vε(x) ⊂V then the perturbed mean-gradient is a continuous
function at x.
Proof. We follow the same line of arguments as in Proposition 2, yet here we need to show that L(x, g) is continuous for
any interable function f.

Explicit Gradient Learning
|L(x, g) −L(x′, g)| = |g · A(x)g + g · b(x) −g · A(x′)g −g · b(x)| = |g · b(x) −g · b(x)|
≤∥g∥

ZZ
Vε(x)Bp(x)
|g · (τ −s) −f(τ) + f(s)|2dsdτ −
ZZ
Vε(x′)Bp(x′)
|g · (τ −s) −f(τ) + f(s)|2dsdτ

≤∥g∥
ZZ
Vε(x)Bp(x)\Vε(x′)Bp(x′)
|g · (τ −s) −f(τ) + f(s)|2dsdτ
+ ∥g∥
ZZ
Vε(x′)Bp(x′)\Vε(x)Bp(x)
|g · (τ −s) −f(τ) + f(s)|2dsdτ
≤M∥g∥· |Vε(x)Bp(x) \ Vε(x′)Bp(x′)| + M∥g∥· |Vε(x′)Bp(x′) \ Vε(x)Bp(x)|
Applying the same arguments in Lemma 4, we have |Vε(x)Bp(x) \ Vε(x′)Bp(x′)| ≤|AVε(x)| · |ABp(x)|∥x −x′∥2, where
AVε(x) is the surface of Vε(x) and ABp(x) is the surface of Bp(x). Therefore, L(x, g) is continuous.
The rest of the proof, again, is identical to Proposition 2.
B.1. Monte-Carlo approximation of the perturb mean-gradient
For a parameterization gθ, we may learn the perturb mean-gradient by sampling a reference point xr and then uniformly
sampling two evaluation points xi ∼U(Bp(xr)) xj ∼U(Vε(xr)). With the tuples (xr, xi, xj) we minimize the following
loss
Lε,p(θ) =
X
xr
X
xi
X
xj
|gθ(xr) · (xj −xi) −f(xj) + f(xi)|2
Since xi ∼U(Bp(xr)), we can write xi = xr + ni where ni is uniformly sampled in an n-ball with p radius. To reduce the
number of evaluation points, we may choose to ﬁx xi and sample xr = xi + nr. If we assume that ε ≫p then for a sample
xj ∼U(Vε(xi)) with very high probability we have that ∥xr −xj∥≤ε. So we can approximate Lε,p with
Lε,p(θ) =
X
nr
X
xi
X
xj
|gθ(xi + nr) · (xj −xi) −f(xj) + f(xi)|2

Explicit Gradient Learning
C. Spline Embedding
When ﬁtting f with a NN, we found out that feeding the input vector x directly into a Fully Connected NN provides
unsatisfactory results when the dimension of the data is too small or when the target function is too complex. Speciﬁcally,
gradient descent (with Adam optimizer (Kingma & Ba, 2014)) falls short in ﬁnding the global optimum. We did not
investigate theoretically into this phenomena, but we designed an alternative architecture that signiﬁcantly improves the
learning process. This method adds a preceding embedding layer (Zhang et al., 2016) before the NN. These embeddings
represent a set of learnable Spline functions (Reinsch, 1967).
Categorical Feature embedding (Howard & Gugger, 2020) is a strong, common practice, method to learn representations of
multi-categorical information. It is equivalent to replacing the features with their corresponding one-hot vector representation
and concatenating the one-hot vectors into a single vector which is then fed to the input of a NN. An important advantage
of categorical embedding is the ability to expand the input dimension into an arbitrary large vector size. In practice, this
expansion can help in representing complex non-linear problems.
For ordinal data, however, embedding may be viewed as an unnecessary step as one can feed the data directly into a NN
input layer. Moreover, categorical feature embeddings do not preserve ordinality within each categorical variable as each
class is assigned a different independent set of learnable embeddings. Nevertheless, motivated by the ability to expand the
input dimension into an arbitrary large number, we designed an ordinal variable embedding that is Lipschitz continuous s.t.
for two relatively close inputs x1 and x2 the embedding layer outputs s(x1) and s(x2) s.t. ∥s(x1) −s(x2)∥≤κs∥x1 −x2∥.
To that end, for a given input vector x ∈Rn, we deﬁne the representation as sθ : Rn →Rns, y = sθ(x), where each entry
sj
θ(xl) is a one-dimensional learnable Spline transformation. A Spline (Reinsch, 1967) is a piecewise polynomial with some
degree of smoothness in the connection points. Spline is usually used to approximate smooth functions but here we use it to
represent a learnable function.
To deﬁne a learnable spline, we need to determine the intersection points and the spline degree. Speciﬁcally, for a domain
xi ∈[a, b] we equally divided the domain into k intersection points, where each point is also termed as knot (in this work
[a, b] = [−1, 1] and k = 21, s.t. each segment is 0.1 long). Our next step is to deﬁne the spline degree and smoothness.
We experimented with three options: (1) continuous piecewise linear splines (2) 3rd degree polynomials with continuous
second derivative, termed C2 Cubic spline and; (3) continuous C0 Cubic splines. We found out that for the purpose of EGL,
continuous piecewise linear splines yield the best performance and requires less computational effort. The explicit deﬁnition
of a piecewise linear spline is
s(x, θ) = θi
hi
(x −ti−1) + θi
hi
(ti −x)
(56)
where θ is a k elements (k is the number of knots), ti is the location of the i-th knot and hi = ti −ti−1.
We can learn more than a single spline for each element in the x vector. In this work we learned e different splines for
each entry in x s.t. the output shape of the embedding block is n × e. It is also possible to learn two or more dimensional
splines but the number of free parameters grows to the power of the splines dimensions. Therefore, it is non practical to
calculate these high degree splines. To calculate interactions between different entries of x we tested two different methods:
(1) aggregation functions and; (2) attention aggregation after a non-local blocks (Wang et al., 2018).
In the ﬁrst option, given a spline representation s(x) ∈Rn×e an average pooling aggregation is executed along the 1st
dimension s.t. we end up with a ¯s(x) ∈Re representation vector. In the second option, the aggregation takes place after
a non-local blocks which calculates interactions between each pairs of entries in the 1st dimension of s(x) (i.e. the input
dimension). To preserve the information of the input data, we concatenated x to the output of the aggregation layer. After
the concatenation, stacks of Residual blocks (He et al., 2016) (Res-Blocks) layers have been applied to calculate the output
vector (size of 1 in IGL and size of n in EGL). The complete Spline Embedding architecture that includes both average
pooling aggregation and non-local blocks is presented in Fig. 7.

Explicit Gradient Learning
Figure 7. The Spline Architecture
2
4
6
8
10
# Res Blocks
(a)
10
6
10
5
10
4
10
3
10
2
10
1
MSE
spline
fc
0
2000
4000
6000
8000
10000
step
(b)
10
3
10
2
10
1
100
101
102
Loss
spline
fc
1.0
0.5
0.0
0.5
1.0
x
(c)
20
0
20
40
60
y
y
res=1
res=2
res=3
res=4
res=5
res=6
res=7
res=8
res=9
res=10
Figure 8. Comparing Spline ﬁtting vs standard FC ﬁtting.
(a)
27.5
28.0
28.5
29.0
29.5
value
f(x)
spline
fc
(b)
50
100
150
200
(c)
1000
800
600
400
200
0
200
1.0
0.5
0.0
0.5
1.0
(d)
100
0
100
200
value
1.0
0.5
0.0
0.5
1.0
(e)
20
0
20
40
1.0
0.5
0.0
0.5
1.0
(f)
150
200
250
300
350
Figure 9. Comparing Spline ﬁtting vs standard FC ﬁtting.

Explicit Gradient Learning
In the next set of experiments, we evaluate the beneﬁt of Spline embedding in 1D COCO problems. We compared the Spline
architecture in Fig. 7 to the same architecture without the spline embedding branches (x is directly fed to the FC layer input).
We used only e = 8 splines and a Res-Block layer size of 64. In Fig. 8(a), we evaluate the learning of a single problem (246,
harmonic decaying function) with different number of Res-Blocks. Here, we used a mini-batch size of 1024 and a total of
1024 mini-batches iteration to learn the function (i.e. a total of 106 samples). We see that Spline embedding obtains much
better MSE even for a single Res-Block and maintains its advantage for all the Res-Block sizes which we evaluated. Note
that each Res-Block comprises two Fully Connected layers, thus with the additional input and output layers we have 2n + 2
FC layers for n Res-Blocks.
In Fig. 8(b) we evaluated the learning process with 2 Res-Blocks for 10240 mini-batches (107 samples). We see that Spline
embedding converges after roughly 500 minibatches while the FC layer learns very slowly. Interestingly, each signiﬁcant
drop in the loss function of the FC net corresponds to a ﬁt of a different ripple in the harmonic decaying function. It seems
like the FC architecture converges to local minima that prevent the network from ﬁtting the entire harmonic function. This
can be seen in Fig. 8(c) where we print the results of the learned FC models for different Res-Block sizes after 1024
mini-batches. The results show that all FC networks fail to ﬁt the harmonic function completely.
To demonstrate expressiveness of Spline embedding, we ﬁt the 4 functions in the 1-D illustrative examples in Sec. 3 and
two additional functions: the harmonic decaying function and a noise like function. The results are presented in Fig. 9.
Remarkably, while we use only e = 8 splines which sums up to only 680 additional weights (8 × 21 spline parameters and
additional 8 × 64 input weights), we obtain signiﬁcantly better results than the corresponding FC architecture.4
4In 1D problems there is no aggregation step.

Explicit Gradient Learning
D. Mappings
By applying the chain rule and the inverse function theorem, we can express the gradient of the original problem ∇f with
the gradient of the scaled problem ∇˜f:
∂f(x)
∂xl
=
∂rk
∂y
−1 ∂hj(x)
∂xl
∂˜fjk(˜x)
∂˜xl
(57)
Here, ∂xl is the partial derivative with respect to the l-th entry of x (we assume that h maps each element independently
s.t. the Jacobian of h is diagonal). For strictly linear mappings, it is easy to show that this property also holds for the
mean-gradients.
Proposition 6. Let hj : Rn →Rn and rk : R →R be two linear mapping functions s.t., rk(y) = y−µk
σk
and hl
j(x) =
al
jx + bl
j, then the mean-gradient gε of f can be recovered from the mean-gradient ˜g˜ε of ˜f with
gl
ε(x) = al
j
σk
˜gl
˜ε(˜x)
(58)
where Vε is the projection h−1
j (V˜ε) which is bounded by an n-ball at x with radius ε = maxl 1
al
j ˜ε, i.e. for all x′ ∈
Vε(x), ∥x′ −x∥≤maxl 1
al
j ˜ε.
Proof. Let us write the deﬁnition of g˜ε with a variable ˜τ s.t. ˜τ ∈V˜ε(˜x) (contrary to the original deﬁnition where τ denoted
the difference s.t. x + τ ∈Vε(x))
g˜ε(˜x) = arg min
g
Z
˜τ∈V˜ε(˜x)
|g · (˜τ −˜x) −˜f(˜τ) + ˜f(˜x)|2d˜τ
(59)
Recall the mapping ˜x = h(x), since it is invertable mapping, there exist τ s.t. ˜τ = h(τ). Substituting ˜τ with τ, the integral
becomes
g˜ε(˜x) = arg min
g
Z
τ∈h−1(V˜ε(˜x))
|g · (h(τ) −h(x)) −˜f(h−1(τ)) + ˜f(h−1(x))|2| det(Dh(τ))|dτ
(60)
where det(Dh(τ)) denotes the determinant of the Jacobian matrix of the mapping h. This determinant is constant for linear
mapping so we can ignore it as we search for the arg-min value. We can also multiply the integral by the inverse slope
1
σr
and get
g˜ε(˜x) = arg min
g
Z
τ∈h−1(V˜ε(˜x))
| 1
σr
g · (h(τ −x)) −1
σr
˜f(h−1(τ)) + 1
σr
˜f(h−1(x))|2dτ
= arg min
g
Z
τ∈h−1(V˜ε(˜x))
| 1
σr
aj ⊙g · (τ −x) −r−1( ˜f(h−1(τ))) + r−1( ˜f(h−1(x)))|2dτ
= arg min
g
Z
τ∈h−1(V˜ε(˜x))
| 1
σr
aj ⊙g · (τ −x) −f(τ) + f(x)|2dτ
(61)
Where the last equality holds since r−1 ◦˜f ◦h = r−1 ◦r ◦f ◦h−1 ◦h = f. Since the mapping g →
1
σr aj ⊙g is bijective,
the arg-min can be rephrased as
1
σr
aj ⊙g˜ε(˜x) = arg min
g
Z
τ∈h−1(V˜ε(˜x))
|g · (τ −x) −f(τ) + f(x)|2dτ
(62)
which is exactly the deﬁnition for gε so we get that gε =
1
σr aj ⊙g˜ε(˜x), as requested. Finally, we need to show that for all
τ ∈Vε(x), ∥τ −x∥≤maxl 1
al ˜ε.
∥τ −x∥= ∥h−1(˜τ) −h−1(˜x)∥= ∥h−1(˜τ −˜x)∥≤∥h−1∥∥˜τ −˜x∥≤max
l
1
al ˜ε
(63)

Explicit Gradient Learning
As discussed in Sec. 4.3, the design goals for mappings are twofold: (1) Fix the statistics of the input and output data and;
(2) maintain the linearity as much as possible. Following these two goals we explored mappings of the form y = q(la(x)),
where q is an expansion non-linear mapping Ω→Rn for the input mapping and a squash mapping R →R for the output
mapping. la is a linear mapping that is deﬁned by the a parameters. For example in the scalar case we can uniquely deﬁne
the linear function by mapping x1 to y1 and x2 to y2, in this case we denote a = [(x1, y1), (x2, y2)].
D.1. Input Mapping
Given a candidate solution xj−1, we ﬁrst construct a bounding-box Ωj by squeezing the previous region by a factor of γα
and placing it s.t. xj−1 is in the bounding-box center. For a region Ωj such that the upper and lower bounds are found in
[bl, bu], we, ﬁrst, construct a linear mapping of the form a = [(bl, −1), (bu, 1)]. Then, our expansion function is the inverse
hyperbolic tangent arctanh(x) = 1
2 log

1+x
1−x

. This function expands [−1, 1] →R but maintains linearity at the origin.
Given that the solution is approximately found in the center of the bounding-box we obtain high linearity except when the
solution is found on the edges.
D.2. Output Mapping
For the output mapping we ﬁrst ﬁx the statistics with a linear mapping a = [(Q0.1, −1), (Q0.9, 1)] where Q0.1 is the 0.1
quantile in the data and Q0.9 is the 0.9 quantile. This mapping is also termed as robust-scaling as unlike z-score x−µ
σ , it is
resilient to outliers. On the downside it does not necessarily ﬁx the ﬁrst and second order statistics, but these are at least
practically, bounded. The next step, i.e. squash mapping, makes sure that even outliers does not get too high values. For that
purpose, we use the squash mapping
q(x) =





−log(−x) −1,
x < −1
x,
−1 ≤x < 1
log(x) + 1,
x ≥1
(64)

Explicit Gradient Learning
E. The Practical EGL Algorithm
Algorithm 3 Explicit Gradient Learning
Input: x0, Ω, ˜α, ˜ε, γα < 1, γε < 1, nmax
k = 0
j = 0
Ωj ←Ω
Map h0 : Ω→Rn
while budget C > 0 do
Explore:
Generate samples Dk = {˜xi}m
1 , ˜xi ∈V˜ε(˜xk)
Evaluate samples yi = f(h−1
0 (˜xi)),
i = 1, ..., m
Add samples to the replay buffer D = D ∪Dk
Output Map:
rk = squash ◦l[Q0.1,Q0.9]
˜yi = rk(yi) ,
i = 1, ..., m
Mean-Gradient learning:
θk = arg minθ
Pl−1
q=0
P
i,j∈Dk−q |(˜xj −˜xi) · gθ(˜xi) −˜xj + ˜xi|2
Gradient Descent:
xk+1 ←xk −˜αgθk(xk)
if f(h−1
j (˜xk+1)) > f(h−1
j (˜xk)) for nmax times in a row then
Generate new trust-region s.t. |Ωj+1| = γα|Ωj| and its center at xbest
Map hj : Ω→Rn
j ←j + 1
˜ε ←γε˜ε
if f(h−1
j (˜xk+1)) < f(h−1
j (˜xk)) then
xbest = h−1
j (˜xk)
k ←k + 1
return xbest

Explicit Gradient Learning
F. Supplementary details: The COCO experiment
The COCO test suite provides many Black-Box optimization problems on several dimensions (2,3,5,10,20,40). For each
dimension, there are 360 distinct problems. The problems are divided into 24 different classes, each contains 15 problems.
To visualize all problem classes, we iterate over the 2D problem set and for each class we present (Fig. 10-15) a contour
plot, 3D plot and the equivalent 1D problem (f1D(x) = f2D(x, x)) combined with the log view of the normalized problem
( f1D(x)−f m
1Din
f max
1D −f min
1D
).
To visualize the average convergence rate of each method, we ﬁrst calculate a scaled distance between the best value at time
t and the optimal value ∆yt
best = mink≤t yk−y∗
y0−y∗
where y∗is the minimal value obtained from all the baselines’ test-runs. We
then average this number, for each t, over all runs in the same dimension problem set. This distance is now scaled from zero
to one and the results are presented on a log-log scale. The ﬁrst-column in Fig. 10-15 presents ∆y
t
best on each problem type
of the 2D problem set and Fig. 16 show ∆y
t
best in the 40D problem set. In table 1, we present the hyperparameters used for
EGL and IGL in all our experiments (besides the ablation tests).
In future work, we will need to design a better mechanism for the ε scheduling. In problem 19 (Griewank-Rosenbrock
F8F2), the ε scheduling was too slow, and only when we used a smaller initial ε, EGL started to converge to the global
minimum (see Fig. 17(a) where we used initial ε = 0.001 × √n, γα = 0.7 and L = 1). On the other hand, in problems, 21
(Gallagher 101 peaks) and 22 (Gallagher 21 peaks) using small ε ends up in falling to local minima, and the choice of a
larger ε could smooth the gradient which pushes xk over the local minima (see 17(b-c) respectively where we used initial
ε = 0.5 × √n and L = 4).
In Fig. 18-23 we present a histogram of the raw and scaled cost value (after the output-mapping) of a 200 samples snapshot
from the replay buffer at different periods during the learning process (t = 1K, t = 10K, t = 100K). Typically, we expect
that problems with Normal or Uniform distributions should be easier to learn with a NN (e.g. problems 15 (Rastrigin), 18
(Schaffer F7, cond1000), 23 (ats ras)), while problems with skewed distribution or multimodal distribution are much harder
(e.g. problems 2 (Ellipsoid separable), 10 (Ellipsoid) and 11 (Discus)). However, simply, mapping from a hard distribution
into a Normal distribution is not necessarily a good choice since we lose the mapping linearity s.t. the scaled mean-gradient
may not correspond to the true mean-gradient. Thus, the output-mapping must balance between linearity and normalization.
In future work, we would like to ﬁnd better, more robust output-mappings that overcome this problem. Understanding the
way that the values are distributed at run-time could also help us deﬁne a better mechanism for deciding on ε and the RB
size L. If the function outputs are close to each other, large RB could be beneﬁcial, but if the values have high variance,
large RB could add unnecessary noise.

Explicit Gradient Learning
Table 1. The COCO experiment Hyperparameters
Parameter
Value
Description
n
[2,3,5,10,20,40,784]
coco space dimension
m
64
Exploration points
mwarmup factor
5
m × mwarmup factor to adjust the network parameters
around the TR initial point
batch
1024
Minibatch of EGL/IGL training
L
32
Number of exploration steps that constitute the replay buffer for EGL/IGL
The replay memory size is: RB = L × m
C
15 × 104
Budget
α
10−2
Optimization steps’ size
g lr
10−3
gθ learning rate
γα
0.9
Trust region squeezing factor
γε
0.97
ε squeezing factor
ε
0.1 × √n
Initial exploration size
nmax
10
The number of times in a row that
f(h−1
j (˜xk+1)) > f(h−1
j (˜xk))
nmin
40
Minimum gradient descent iterations
p
0
Perturbation radius
gθ
Spline
Network architecture (SPLINE/FC)
OM
log
Output Mapping
OM lr
0.1
Moving average learning rate for the Output Mapping
N minibatches
60
# of mini-batches for the mean-gradient learning in each k step
Vε(x)
ball-explore
Vε(x) = x + ε × U[−1, 1] (see Sec. H for details)

Explicit Gradient Learning
2
16
2
13
2
10
2
7
2
4
2
1
1: Sphere
SLSQP
Nelder Mead
COBYLA
POWELL
CG
BFGS
CMA-ES
IGL
EGL
4
2
0
2
4
4
2
0
2
4
80
90
100
110
120
130
140
4
2
0
2
4
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
32
2
27
2
22
2
17
2
12
2
7
2
2
2: Ellipsoid separable
4
2
0
2
4
4
2
0
2
4
1e7
0.5
1.0
1.5
2.0
2.5
4
2
0
2
4
10
9
10
7
10
5
10
3
10
1
log view
0.0
0.2
0.4
0.6
0.8
1.0
f1D
2
20
2
17
2
14
2
11
2
8
2
5
2
2
3: Rastrigin separable
4
2
0
2
4
4
2
0
2
4
400
300
200
100
0
4
2
0
2
4
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
19
2
16
2
13
2
10
2
7
2
4
2
1
4: Skew Rastrigin-Bueche separ
4
2
0
2
4
4
2
0
2
4
400
200
0
200
400
600
4
2
0
2
4
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
Figure 10. Visualization problems type 1-4 of 2D problems. First column: The scaled distance ∆y
t
best. Second column: Counter plot.
Third column: 3D plot. Forth column: equivalent 1D problem with log view.

Explicit Gradient Learning
2
20
2
17
2
14
2
11
2
8
2
5
2
2
5: Linear slope
4
2
0
2
4
4
2
0
2
4
0
20
40
60
80
100
4
2
0
2
4
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
20
2
17
2
14
2
11
2
8
2
5
2
2
6: Attractive sector
4
2
0
2
4
4
2
0
2
4
250000
500000
750000
1000000
1250000
1500000
1750000
4
2
0
2
4
10
8
10
6
10
4
10
2
100
log view
0.2
0.4
0.6
0.8
f1D
2
18
2
15
2
12
2
9
2
6
2
3
7: Step-ellipsoid
4
2
0
2
4
4
2
0
2
4
500
1000
1500
2000
4
2
0
2
4
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
15
2
13
2
11
2
9
2
7
2
5
2
3
2
1
8: Rosenbrock original
4
2
0
2
4
4
2
0
2
4
25000
50000
75000
100000
125000
150000
4
2
0
2
4
10
7
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
Figure 11. Visualization problems type 5-8 of 2D problems. First column: The scaled distance ∆y
t
best. Second column: Counter plot.
Third column: 3D plot. Forth column: equivalent 1D problem with log view.

Explicit Gradient Learning
2
17
2
14
2
11
2
8
2
5
2
2
9: Rosenbrock rotated
4
2
0
2
4
4
2
0
2
4
50000
100000
150000
200000
250000
300000
4
2
0
2
4
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
25
2
21
2
17
2
13
2
9
2
5
2
1
10: Ellipsoid
4
2
0
2
4
4
2
0
2
4
1e7
0
1
2
3
4
5
6
4
2
0
2
4
10
9
10
7
10
5
10
3
10
1
log view
0.0
0.2
0.4
0.6
0.8
1.0
f1D
2
33
2
28
2
23
2
18
2
13
2
8
2
3
11: Discus
4
2
0
2
4
4
2
0
2
4
1e7
1
2
3
4
5
6
7
4
2
0
2
4
10
9
10
7
10
5
10
3
10
1
log view
0.0
0.2
0.4
0.6
0.8
1.0
f1D
2
32
2
27
2
22
2
17
2
12
2
7
2
2
12: Bent cigar
4
2
0
2
4
4
2
0
2
4
1e10
0
1
2
3
4
5
4
2
0
2
4
10
12
10
10
10
8
10
6
10
4
10
2
100
log view
0.0
0.2
0.4
0.6
0.8
1.0
f1D
Figure 12. Visualization problems type 9-12 of 2D problems. First column: The scaled distance ∆y
t
best. Second column: Counter plot.
Third column: 3D plot. Forth column: equivalent 1D problem with log view.

Explicit Gradient Learning
2
22
2
19
2
16
2
13
2
10
2
7
2
4
2
1
13: Sharp ridge
4
2
0
2
4
4
2
0
2
4
500
1000
1500
2000
4
2
0
2
4
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
15
2
13
2
11
2
9
2
7
2
5
2
3
2
1
14: Sum of different powers
4
2
0
2
4
4
2
0
2
4
50
0
50
100
150
200
250
300
4
2
0
2
4
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
20
2
17
2
14
2
11
2
8
2
5
2
2
15: Rastrigin
4
2
0
2
4
4
2
0
2
4
2000
3000
4000
5000
6000
7000
4
2
0
2
4
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
20
2
17
2
14
2
11
2
8
2
5
2
2
16: Weierstrass
4
2
0
2
4
4
2
0
2
4
100
200
300
400
500
600
4
2
0
2
4
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
Figure 13. Visualization problems type 13-16 of 2D problems. First column: The scaled distance ∆y
t
best. Second column: Counter plot.
Third column: 3D plot. Forth column: equivalent 1D problem with log view.

Explicit Gradient Learning
2
19
2
16
2
13
2
10
2
7
2
4
2
1
17: Schaffer F7, condition 10
4
2
0
2
4
4
2
0
2
4
0
500
1000
1500
2000
2500
4
2
0
2
4
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
18
2
15
2
12
2
9
2
6
2
3
18: Schaffer F7, condition 1000
4
2
0
2
4
4
2
0
2
4
0
5000
10000
15000
20000
25000
4
2
0
2
4
10
7
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
13
2
11
2
9
2
7
2
5
2
3
2
1
19: Griewank-Rosenbrock F8F2
4
2
0
2
4
4
2
0
2
4
100
0
100
200
300
400
500
600
4
2
0
2
4
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
13
2
11
2
9
2
7
2
5
2
3
2
1
20: Schwefel x*sin(x)
4
2
0
2
4
4
2
0
2
4
0
50000
100000
150000
200000
4
2
0
2
4
10
7
10
6
10
5
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
Figure 14. Visualization problems type 17-20 of 2D problems. First column: The scaled distance ∆y
t
best. Second column: Counter plot.
Third column: 3D plot. Forth column: equivalent 1D problem with log view.

Explicit Gradient Learning
2
5
2
4
2
3
2
2
2
1
21: Gallagher 101 peaks
4
2
0
2
4
4
2
0
2
4
50
60
70
80
90
100
4
2
0
2
4
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
5
2
4
2
3
2
2
2
1
22: Gallagher 21 peaks
4
2
0
2
4
4
2
0
2
4
980
960
940
920
4
2
0
2
4
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
15
2
13
2
11
2
9
2
7
2
5
2
3
2
1
23: ats ras
4
2
0
2
4
4
2
0
2
4
10
20
30
40
50
60
70
80
4
2
0
2
4
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
2
6
2
5
2
4
2
3
2
2
2
1
24: Lunacek bi-Rastrigin
4
2
0
2
4
4
2
0
2
4
120
140
160
180
200
4
2
0
2
4
10
4
10
3
10
2
10
1
log view
0.2
0.4
0.6
0.8
f1D
Figure 15. Visualization problems type 21-24 of 2D problems. First column: The scaled distance ∆y
t
best. Second column: Counter plot.
Third column: 3D plot. Forth column: equivalent 1D problem with log view.

Explicit Gradient Learning
2
20
2
15
2
10
2
5
1: Sphere
SLSQP
Nelder Mead
COBYLA
POWELL
CG
BFGS
CMA-ES
IGL
EGL
2
34
2
27
2
20
2
13
2
6
2: Ellipsoid separable
2
5
2
3
2
1
3: Rastrigin separable
2
22
2
17
2
12
2
7
2
2
4: Skew Rastrigin-Bueche separ
2
22
2
17
2
12
2
7
2
2
5: Linear slope
2
25
2
19
2
13
2
7
2
1
6: Attractive sector
2
23
2
18
2
13
2
8
2
3
7: Step-ellipsoid
2
28
2
22
2
16
2
10
2
4
8: Rosenbrock original
2
20
2
15
2
10
2
5
9: Rosenbrock rotated
2
16
2
12
2
8
2
4
20
10: Ellipsoid
2
32
2
25
2
18
2
11
2
4
11: Discus
2
24
2
19
2
14
2
9
2
4
12: Bent cigar
2
10
2
8
2
6
2
4
2
2
13: Sharp ridge
2
20
2
16
2
12
2
8
2
4
14: Sum of different powers
2
17
2
13
2
9
2
5
2
1
15: Rastrigin
2
15
2
12
2
9
2
6
2
3
16: Weierstrass
2
5
2
3
2
1
17: Schaffer F7, condition 10
2
15
2
12
2
9
2
6
2
3
18: Schaffer F7, condition 1000
2
12
2
9
2
6
2
3
19: Griewank-Rosenbrock F8F2
2
17
2
13
2
9
2
5
2
1
20: Schwefel x*sin(x)
2
12
2
9
2
6
2
3
21: Gallagher 101 peaks
2
10
2
7
2
4
2
1
22: Gallagher 21 peaks
2
10
2
8
2
6
2
4
2
2
23: ats ras
2
10
2
8
2
6
2
4
2
2
24: Lunacek bi-Rastrigin
Figure 16. The scaled distance ∆y
t
best per problem type on 40D
210
212
214
216
(a)
2
13
2
11
2
9
2
7
2
5
2
3
2
1
yt
best
19: Griewank-Rosenbrock
SLSQP
Nelder Mead
COBYLA
POWELL
CG
BFGS
CMA-ES
IGL
EGL
210
212
214
216
(b)
2
20
2
17
2
14
2
11
2
8
2
5
2
2
21: Gallagher 101P
210
212
214
216
(c)
2
4
2
3
2
2
2
1
22: Gallagher 21P
Figure 17. The scaled distance ∆y
t
best with different ε on 40D. (a) problem type 19, (b) problem type 21, (c) problem type 22

Explicit Gradient Learning
120
140
(a1)
1: Sphere
1K cost values
3.2
3
2.8
2.6
(a2)
1K scaled values
80
82
84
86
(a3)
10K cost values
2
1
0
1
2
(a4)
10K scaled values
79.48
79.482
79.484
79.486
(a5)
100K cost values
2
0
2
4
(a6)
100K scaled values
1 M
2 M
3 M
(b1)
2: Ellipsoid separable
2
1
0
1
(b2)
0
200 k 400 k 600 k 800 k
(b3)
2
0
2
(b4)
5 k
10 k
15 k
(b5)
5
0
5
(b6)
200
400
600
(c1)
3: Rastrigin separable
2
1
0
(c2)
200
0
200
(c3)
2
0
2
(c4)
340
338
336
334
(c5)
2
0
2
4
(c6)
200
400
600
(d1)
4: Skew Rastrigin-Bueche separ
3
2
1
0
1
(d2)
0
200
(d3)
1
0
1
2
(d4)
335
330
(d5)
2
0
2
4
(d6)
Figure 18. Histogram plot of a snapshot of 200 samples from the RB around different times (t = 1K, t = 10K, t = 100K) with and
without OM for problems 1-4

Explicit Gradient Learning
500
550
600
(a1)
5: Linear slope
1K cost values
3.8
3.6
3.4
(a2)
1K scaled values
8.3
8.2
8.1
8
(a3)
10K cost values
3.6
3.5
3.4
3.3
3.2
(a4)
10K scaled values
9.20974 9.20972
9.2097
9.20968
(a5)
100K cost values
3.8
3.7
3.6
3.5
3.4
(a6)
100K scaled values
50 k
100 k
150 k
200 k
(b1)
6: Attractive sector
3
2.8
2.6
2.4
(b2)
0
5 k
10 k
(b3)
0
2
(b4)
25
50
75
100
125
(b5)
5
0
5
10
(b6)
400
600
(c1)
7: Step-ellipsoid
2.5
2
(c2)
100
150
200
(c3)
2
0
2
(c4)
96.75
97
97.25
97.5
(c5)
500 m 250 m
0
250 m 500 m
(c6)
10 k
20 k
30 k
40 k
(d1)
8: Rosenbrock original
2.5
2
1.5
(d2)
250
500
750
1 k
(d3)
2
0
2
(d4)
183.5
184
184.5
185
185.5
(d5)
2.5
0
2.5
5
(d6)
Figure 19. Histogram plot of a snapshot of 200 samples from the RB around different times (t = 1K, t = 10K, t = 100K) with and
without OM for problems 5-8

Explicit Gradient Learning
0
1 k
2 k
3 k
(a1)
9: Rosenbrock rotated
1K cost values
2
0
2
(a2)
1K scaled values
0
500
1 k
(a3)
10K cost values
2
0
2
(a4)
10K scaled values
157.4
157.6
157.8
158
(a5)
100K cost values
2
0
2
(a6)
100K scaled values
1 M
2 M
(b1)
10: Ellipsoid
2.8
2.6
2.4
(b2)
0
500 k
1 M
(b3)
2
0
2
(b4)
8 k
10 k
(b5)
5
2.5
0
2.5
5
(b6)
0
500 k
1 M
1.5 M
(c1)
11: Discus
2
0
2
(c2)
0
500 k
1 M
1.5 M
(c3)
2
0
2
4
(c4)
0
5 k
10 k
(c5)
2.5
0
2.5
5
7.5
(c6)
60 M
80 M
100 M
120 M
(d1)
12: Bent cigar
2.5
2
1.5
(d2)
0
2.5 M
5 M
7.5 M
(d3)
2
0
2
(d4)
0
1 k
2 k
3 k
4 k
(d5)
2
0
2
(d6)
Figure 20. Histogram plot of a snapshot of 200 samples from the RB around different times (t = 1K, t = 10K, t = 100K) with and
without OM for problems 9-12

Explicit Gradient Learning
1.4 k
1.6 k
1.8 k
2 k
(a1)
13: Sharp ridge
1K cost values
3.4
3.2
3
2.8
(a2)
1K scaled values
200
400
600
(a3)
10K cost values
2
1
0
1
2
(a4)
10K scaled values
42.5
45
47.5
50
(a5)
100K cost values
2
0
2
(a6)
100K scaled values
40
35
30
25
(b1)
14: Sum of different powers
2.75
2.5
2.25
2
(b2)
52
51
50
(b3)
2
0
2
(b4)
52.34
52.33
52.32
(b5)
2
0
2
(b6)
2 k
2.2 k
2.4 k
(c1)
15: Rastrigin
3
2
1
(c2)
1.3 k
1.4 k
1.5 k
1.6 k
1.7 k
(c3)
2
0
2
(c4)
1.114 k
1.116 k
1.118 k
(c5)
0
2
4
(c6)
100
125
150
175
(d1)
16: Weierstrass
2
0
2
(d2)
80
100
120
(d3)
2
0
2
(d4)
71.6
71.7
71.8
(d5)
2
0
2
(d6)
Figure 21. Histogram plot of a snapshot of 200 samples from the RB around different times (t = 1K, t = 10K, t = 100K) with and
without OM for problems 13-16

Explicit Gradient Learning
10
8
6
4
(a1)
17: Schaffer F7, cond10
1K cost values
2
1
0
1
(a2)
1K scaled values
14
12
10
8
(a3)
10K cost values
2
0
2
(a4)
10K scaled values
15.8
15.7
15.6
(a5)
100K cost values
2
0
2
(a6)
100K scaled values
10
20
30
(b1)
18: Schaffer F7, cond1000
2
1
0
(b2)
10
5
(b3)
2
1
0
1
2
(b4)
16.2
16
15.8
(b5)
2
1
0
1
2
(b6)
96
94
92
90
88
(c1)
19: Griewank-Rosenbrock
2
0
2
(c2)
96
94
92
90
88
(c3)
2
1
0
1
2
(c4)
101
100.5
100
99.5
(c5)
2
0
2
(c6)
0
1 k
2 k
(d1)
20: Schwefel x*sin(x)
2
1.5
1
500 m
(d2)
500
400
300
200
100
(d3)
0
2
4
6
(d4)
544.23
544.22
544.21
(d5)
2
0
2
(d6)
Figure 22. Histogram plot of a snapshot of 200 samples from the RB around different times (t = 1K, t = 10K, t = 100K) with and
without OM for problems 17-20

Explicit Gradient Learning
60
70
80
90
100
(a1)
21: Gallagher 101P
1K cost values
4
3.5
(a2)
1K scaled values
41
42
43
(a3)
10K cost values
2
0
2
4
(a4)
10K scaled values
40.25
40.5
40.75
41
41.25
(a5)
100K cost values
3
3.25
3.5
3.75
(a6)
100K scaled values
940
935
930
(b1)
22: Gallagher 21P
4.25
4
3.75
3.5
3.25
(b2)
982
980
978
(b3)
2
1
0
1
2
(b4)
981.711981.71981.709981.708981.707
(b5)
2
0
2
(b6)
10
20
30
(c1)
23: ats ras
2
0
2
(c2)
10
20
30
(c3)
2
0
2
(c4)
10
12.5
15
17.5
20
(c5)
0
1
2
3
(c6)
600
700
800
900
(d1)
24: Lunacek bi-Rastrigin
2
0
2
(d2)
500
600
700
800
(d3)
2
1
0
1
2
(d4)
240
260
280
(d5)
2
0
2
(d6)
Figure 23. Histogram plot of a snapshot of 200 samples from the RB around different times (t = 1K, t = 10K, t = 100K) with and
without OM for problems 21-24

Explicit Gradient Learning
G. Supplementary details: latent space search
Figure 24. The image-generative BBO task
In this experiment, the task is to utilize a pre-trained Black-Box face image generator and generate a realistic face image
with target face attributes and face landmark points. Formally, we have 4 Black-Box networks that constitute our BBO
problem:
1. Generator G : z →x, where z ∼N(0, In) and x is an RGB image with H = 218, W = 178.
2. Discriminator D : x →R s.t. positive D(x) indicate poor fake images while negative D(x) indicates real or a good
fake image.
3. Attribute Classiﬁer A : x →R40 where each element in A(x) is the probability of a single attribute (out of 40 different
attributes).
4. Landmark points Estimator L : x →R68 predicts the location of 68 different landmark points.
In addition, every BBO problem is characterized by two external parameters
1. Target attributes a a vector of 40 Booleans.
2. Target landmark points l ∈R68 a vector of 68 landmark points locations.
The overall cost function is deﬁned as
fal(z) = λaLa(G(z)) + λlLl(G(z)) + λg tanh(D(G(z)))
Where:
1. La is the Cross-Entropy loss between the generated face attributes as measured by the classiﬁer and the desired set of
attributes a.
2. Ll is the MSE between the generated landmark points and the desired set of landmarks l.
3. D(G(z)) is the discriminator output, positive for low-quality images and negative for high-quality images.
The objective is to ﬁnd z∗that minimizes fal. A graphical description of the BBO problem is given in Fig. 24. We used a
constant starting point z0, sampled from N(0, In) for all our runs. To generate different problems, we sampled a target

Explicit Gradient Learning
Table 2. Latent-Space Search Hyperparameters
Parameter
Value
Description
n
512
Latent space dimension
λa
1
Attributes score weight
λd
2
Discriminator score weight
λl
100
Landmarks score weight
m
32
Exploration points
batch
1024
Minibatch of EGL/IGL training
L
64
Number of exploration steps that constitute the replay buffer for EGL/IGL
The replay memory size is: RB = L × m
C
104
Budget
φ
2
3π
Cone exploration angle
α
0.02
Optimization steps’ size
g lr
10−3
gθ learning rate
γα
0.9
Trust region squeezing factor
γε
0.95
ε squeezing factor
image xT from the CelebA dataset. We used its attributes as the target attribute and used its landmark points estimation
l = L(xT ) as the target landmarks. Note that the target image xT was never revealed to the EGL optimizer, only its
attributes and landmark points.
We applied the same EGL algorithm as in the COCO experiment with the Spline Embedding network architecture and with
two notable changes: (1) a set of slightly modiﬁed hyperparameters (See Table 2); and (2) a modiﬁed exploration domain
Vε. The main reason for these adjustments was the high computational cost (comparing to the COCO experiment) of each
different z vector. This led us to squeeze the budget C to only 104 evaluations and the number of exploration points to only
m = 32. For such a low number of exploration points around each candidate (in a n = 512 dimension space), we designed
an exploration domain Vε, termed cone-explore which we found out to be more efﬁcient than the uniform exploration inside
an n-ball with ε radius that was executed in the COCO experiment. A description of the cone-explore method is given in
Sec. H.
Additional results of EGL and IGL are given in Fig. 27 and Fig. 28. We also evaluated two classical algorithms: GC and
CMA-ES, both provided unsatisfying results (see Fig. 26). We observed that CG converged to local minima around the
initial point z0 while CMA-ES converged to points that have close landmark point but poor discrimination score. We did not
investigate into this phenomena but we postulate that it is the result of different landscapes statistics of the two factors in the
cost function, i.e. Ll(G(z)) and tanh(D(G(z))) which fool the CMA-ES algorithm.
0
2000
4000
6000
8000
10000
step
0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
cost
egl
igl
Figure 25. Average cost during training.

Explicit Gradient Learning
Male
Mature
BlackWavy
Sideburns
Chubby
Goatee
Male
Young
BrownWavy
Attractive
Pointy_Nose
Male
Mature
StraightRecedingBlack
Goatee
Double_Chin
High_Cheekbones
Female
Young
WavyBrown
Pointy_Nose
High_Cheekbones
Heavy_Makeup
Female
Young
StraightWavyBlond
Heavy_Makeup
Narrow_Eyes
High_Cheekbones
Female
Young
BrownWavy
Big_Lips
Pale_Skin
Heavy_Makeup
Female
Young
StraightBrown
Pointy_Nose
Narrow_Eyes
Wearing_Lipstick
Male
Young
StraightBrown
Pointy_Nose
High_Cheekbones
Big_Nose
CG
14.46
CMA
1.54
Target
Figure 26. Baselines results: CG and CMA
Female
Young
Straight
Attractive
Bangs
High_Cheekbones
Female
Young
WavyBrown
Attractive
Pointy_Nose
Female
Young
BlondWavy
Attractive
High_Cheekbones
Wearing_Lipstick
Female
Mature
WavyBrown
Wearing_Lipstick
High_Cheekbones
Heavy_Makeup
Male
Mature
Black
Mustache
Big_Lips
Goatee
Female
Young
Blond
Heavy_Makeup
Attractive
Big_Lips
Male
Young
Receding
Goatee
Double_Chin
Big_Lips
Male
Mature
Gray
Mustache
Big_Nose
Male
Young
Black
Bushy_Eyebrows
Female
Young
RecedingStraightBlack
Wearing_Lipstick
Heavy_Makeup
Bushy_Eyebrows
Male
Young
Straight
Pointy_Nose
Attractive
Female
Young
StraightBlond
Attractive
Heavy_Makeup
Wearing_Lipstick
EGL
IGL
Target
Figure 27. Searching latent space of generative models with EGL and IGL
Female
Young
Wavy
Attractive
Wearing_Lipstick
Heavy_Makeup
Female
Young
Blond
Attractive
Narrow_Eyes
Heavy_Makeup
Female
Young
Wearing_Lipstick
Wearing_Hat
Heavy_Makeup
Female
Young
BlondWavy
High_Cheekbones
Pointy_Nose
Wearing_Lipstick
Female
Young
BlondWavyReceding
Heavy_Makeup
Wearing_Lipstick
Male
Mature
High_Cheekbones
Big_Nose
Attractive
Male
Mature
WavyBrown
Goatee
Chubby
Sideburns
Male
Mature
Gray
Mustache
High_Cheekbones
Double_Chin
Male
Young
Mustache
High_Cheekbones
Bushy_Eyebrows
Female
Young
Wavy
Wearing_Lipstick
Heavy_Makeup
Attractive
Female
Young
WavyBlack
Wearing_Lipstick
Attractive
Heavy_Makeup
Female
Young
Wavy
Bushy_Eyebrows
Heavy_Makeup
Attractive
EGL
IGL
Target
Figure 28. Searching latent space of generative models with EGL and IGL

Explicit Gradient Learning
H. Gradient Guided Exploration
In high-dimensional problems and low budgets, sampling n + 1 exploration points for each new candidate xk may consume
the entire budget too fast without being able to take enough optimization steps. In practice, EGL works even with m ≪n+1
exploration points, however, we observed that one can improve the efﬁciency when m ≪n + 1 by sampling the exploration
points non uniformly around the candidate xk. Speciﬁcally, using the previous estimation of the gradient to determine the
search direction. Hence, we term this approach as gradient guided exploration.
In the COCO experiment (Sec. 5) we sampled the exploration points uniformly around each candidate. In other words, our
Vε domain was an n-ball with ε radius, we term this method as ball-explore. Ball-explore does not make any assumptions
on the gradient direction at xk and does not use any a-priori information. However, for continuous gradients, we do have
a-priori information on the gradient direction as we have our previous estimator gθk−1. Since xk is relatively close to xk−1,
the learned model gθk−1 can be used as a ﬁrst-order approximation for the gradient in xk, i.e. gθk−1(xk).
If gθk−1(xk) is a good approximation for gε(xk), then sampling points in a perpendicular direction to the mean-gradient, i.e. x
s.t. (x−xk)·gθk−1(xk) = 0, adds little information since f(x)−f(xk) ≈0 so the loss |(x−xk)·gθk−1(xk)−f(x)+f(xk)|2
is very small. Therefore, we experimented with sampling points inside the intersection of a n-ball Bε(xk) and a cone with
apex at xk, direction −gθk−1(xk) and some hyperparameter cone-angle of φ. The distance vector x −xk of a point inside
such a cone has high cosine similarity with the mean-gradient and we postulate that this should improve the efﬁciency
of the learning process. We term this alternative exploration method as cone-explore and denote the cone domain as
Cφ
ε (x, gθk−1(xk)).
For high dimensions, cone-explore signiﬁcantly reduces the exploration volume. A simple upper bound for the cone-to-ball
volume ratio show that it decays exponentially in n
|Cφ
ε |
|Bε| ≤
√πΓ( n+1
2 )
nΓ( n
2 + 1) (sin φ)n−1
(65)
Unfortunately, cone-explore is not suitable for non-continuous gradients or too large optimization steps. To take into account
gradient discontinuities, we suggest to sample half of the points inside the cone and half inside an n-ball.
In Fig. 29 we present an ablation test in the 784D COCO problem set of cone-explore and 1
2-cone- 1
2-ball explore with
respect to the standard ball-explore. In this experiment, we used only m = 32 exploration points around each candidate.
The results show that sampling half of the exploration points inside a cone improved the results by 18%. We found out that
the strategy also improves the results in the latent space search experiment, yet we did not conduct a full ablation test.
On the downside, we found out that if m ≈n then cone-explore hurts the performance. We hypothesize that near local
minima, where the exact direction of the gradient is important, the mean-gradient learned with cone-explore has lower
accuracy and therefore, ball-explore with sufﬁcient sampling points converges to better solutions.
50K
100K
150K
t
-10.0%
-5.0%
0.0%
5.0%
10.0%
15.0%
BALL_CONE
BALL
CONE
Figure 29. Ablation test on the 784D COCO problem set: Cone-explore vs Ball-explore

