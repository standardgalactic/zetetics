University of Iowa 
University of Iowa 
Iowa Research Online 
Iowa Research Online 
Theses and Dissertations 
Summer 2014 
Modified Bayesian Kriging for noisy response problems and 
Modified Bayesian Kriging for noisy response problems and 
Bayesian confidence-based reliability-based design optimization 
Bayesian confidence-based reliability-based design optimization 
Nicholas John Gaul 
University of Iowa 
Follow this and additional works at: https://ir.uiowa.edu/etd 
 Part of the Mechanical Engineering Commons 
Copyright 2014 Nicholas John Gaul 
This dissertation is available at Iowa Research Online: https://ir.uiowa.edu/etd/1322 
Recommended Citation 
Recommended Citation 
Gaul, Nicholas John. "Modified Bayesian Kriging for noisy response problems and Bayesian confidence-
based reliability-based design optimization." PhD (Doctor of Philosophy) thesis, University of Iowa, 2014. 
https://doi.org/10.17077/etd.ap2nzl1y 
Follow this and additional works at: https://ir.uiowa.edu/etd 
 Part of the Mechanical Engineering Commons 

MODIFIED BAYESIAN KRIGING FOR NOISY RESPONSE PROBLEMS AND 
BAYESIAN CONFIDENCE-BASED RELIABILITY-BASED DESIGN 
OPTIMIZATION 
by 
Nicholas John Gaul 
A thesis submitted in partial fulfillment 
of the requirements for the Doctor of 
Philosophy degree in Mechanical Engineering 
in the Graduate College of 
The University of Iowa 
August 2014 
Thesis Supervisors: Professor Kyung K. Choi 
Professor Mary Kathryn Cowles 
 
 

Graduate College 
The University of Iowa 
Iowa City, Iowa 
CERTIFICATE OF APPROVAL 
_______________________ 
PH.D. THESIS 
_______________ 
This is to certify that the Ph.D. thesis of 
Nicholas John Gaul 
has been approved by the Examining Committee 
for the thesis requirement for the Doctor of Philosophy 
degree in Mechanical Engineering at the August 2014 graduation. 
Thesis Committee:  ___________________________________ 
 
   Kyung K. Choi, Thesis Supervisor 
 
 ___________________________________ 
 
   Mary Kathryn Cowles, Thesis Supervisor 
 
 ___________________________________ 
 
   David Lamb 
 
 ___________________________________ 
 
   Jia Lu 
 
 ___________________________________ 
 
   Sharif Rahman 

To my family and friends 
 
ii 

If you can’t explain it simply, you don’t understand it well enough. 
Albert Einstein 
 
 
iii 

ABSTRACT 
The objective of this study is to develop a new modified Bayesian Kriging 
(MBKG) surrogate modeling method that can be used to carry out confidence-based 
reliability-based design optimization (RBDO) for problems in which simulation analyses 
are inherently noisy and standard Kriging approaches fail. The formulation of the MBKG 
surrogate modeling method is presented, and the full conditional distributions of the 
unknown MBKG parameters are derived and coded into a Gibbs sampling algorithm. 
Using the coded Gibbs sampling algorithm, Markov chain Monte Carlo is used to fit the 
MBKG surrogate model. 
A sequential sampling method that uses the posterior credible sets for inserting 
new design of experiment (DoE) sample points is proposed. The sequential sampling 
method is developed in such a way that the new DoE sample points added will provide 
the maximum amount of information possible to the MBKG surrogate model, making it 
an efficient and effective way to reduce the number of DoE sample points needed. 
Therefore, it improves the posterior distribution of the probability of failure efficiently. 
Finally, a confidence-based RBDO method using the posterior distribution of the 
probability of failure is developed. The confidence-based RBDO method is developed so 
that the uncertainty of the MBKG surrogate model is included in the optimization 
process. 
A 2-D mathematical example was used to demonstrate fitting the MBKG 
surrogate model and the developed sequential sampling method that uses the posterior 
credible sets for inserting new DoE. A detailed study on how the posterior distribution of 
the probability of failure changes as new DoE are added using the developed sequential 
sampling method is presented. Confidence-based RBDO is carried out using the same 2-
D mathematical example. Three different noise levels are used for the example to 
compare how the MBKG surrogate modeling method, the sequential sampling method, 
 
iv 

and the confidence-based RBDO method behave for different amounts of noise in the 
response. A comparison of the optimization results for the three different noise levels for 
the same 2-D mathematical example is presented. 
A 3-D multibody dynamics (MBD) engineering block-car example is presented. 
The example is used to demonstrate using the developed methods to carry out 
confidence-based RBDO for an engineering problem that contains noise in the response. 
The MBD simulations for this example were done using the commercially available 
MBD software package RecurDyn. Deterministic design optimization (DDO) was first 
done using the MBKG surrogate model to obtain the mean response values, which then 
were used with standard Kriging methods to obtain the sensitivity of the responses. 
Confidence-based RBDO was then carried out using the DDO solution as the initial 
design point. 
 
 
 
v 

TABLE OF CONTENTS 
LIST OF TABLES ........................................................................................................... viii 
LIST OF FIGURES .............................................................................................................x 
LIST OF ACRONYMS ................................................................................................... xiii 
CHAPTER 1 INTRODUCTION .........................................................................................1 
 
1.1 Background and Motivation .......................................................................2 
1.1.1 Reliability-Based Design Optimization ............................................2 
1.1.2 Surrogate Modeling Methods ...........................................................4 
1.2 Bayesian Statistics ......................................................................................6 
1.2.1 Likelihood and Prior Distributions ...................................................6 
1.2.2 Bayes’ Rule and Posterior Distributions ..........................................7 
1.2.3 Conjugate Priors and Markov Chain Monte Carlo ...........................8 
1.3 Objectives of the Proposed Study .............................................................10 
1.4 Organization of Thesis ..............................................................................11 
CHAPTER 2 DESIGN UNDER UNCERTAINTY ..........................................................13 
 
2.1 Introduction ...............................................................................................13 
2.2 Reliability Analysis ..................................................................................13 
2.2.1 Random Variable Transformation ..................................................14 
2.2.2 First-Order Reliability Method (FORM) and Second-Order 
Reliability Method (SORM) ....................................................................16 
2.3 Inverse Reliability Analysis ......................................................................18 
2.4 MPP-based RBDO ....................................................................................19 
2.4.1 MPP-Based RBDO Using FORM ..................................................19 
2.4.2 MPP-Based RBDO Using DRM ....................................................21 
2.5 Sampling-Based RBDO ............................................................................21 
2.5.1 Sampling-Based Probability of Failure ..........................................22 
2.5.2 Probabilistic Sensitivity Analysis ...................................................23 
CHAPTER 3 MODIFIED BAYESIAN KRIGING ...........................................................30 
 
3.1 Introduction ...............................................................................................30 
3.2 Kriging and Dynamic Kriging Methods ...................................................30 
3.2.1 Kriging Method ..............................................................................30 
3.2.2 Dynamic Kriging Method ...............................................................31 
3.3 Modified Bayesian Kriging (MBKG) .......................................................32 
3.3.1 Modified Bayesian Kriging Formulation .......................................32 
3.3.2 Prior Distributions for MBKG Parameters .....................................35 
3.4 Full Conditional Distributions for MBKG Parameters .............................39 
3.4.1 Joint Distribution of the MBKG Parameters ..................................39 
3.4.2 Full Conditional for 
c
µ  ..................................................................41 
3.4.3 Full Conditional for 
2
σ  ..................................................................44 
3.4.4 Full Conditional for λ  ...................................................................45 
3.4.5 Full Conditional for 
j
θ  ...................................................................46 
3.4.6 Full Conditional for φ  ...................................................................47 
3.4.7 Full Conditional for β  ....................................................................48 
 
vi 

CHAPTER 4 APPLICATIONS OF MODIFIED BAYESIAN KRIGING (MBKG) .......51 
 
4.1 Introduction ...............................................................................................51 
4.2 One-Dimensional Quadratic Mathematical Example ...............................51 
4.3 Two-Dimensional Mathematical Examples .............................................72 
4.3.1 First 2-D Mathematical Example ...................................................72 
4.3.2 Second 2-D Mathematical Example ...............................................77 
4.3.3 A Note about the Computational Time ..........................................80 
CHAPTER 5 SEQUENTIAL SAMPLING VIA CREDIBLE SETS ................................81 
 
5.1 Introduction ...............................................................................................81 
5.2 Sequential Sampling for the First 2-D Mathematical Example ................82 
5.3 Sequential Sampling for the Second 2-D Mathematical Example ...........89 
5.3.1 Prior Distributions and MCMC Initial Values ...............................89 
5.3.2 Sequential Sampling .......................................................................90 
5.3.3 Distribution of Probability of Failure for Small Noise ...................99 
5.3.4 Distribution of the Probability of Failure for Small, Medium, 
and Large Noise .....................................................................................105 
CHAPTER 6 CONFIDENCE-BASED RELIABILITY-BASED DESIGN 
OPTIMIZATION VIA POSTERIOR DISTRIBUTIONS ............................108 
 
6.1 Introduction .............................................................................................108 
6.2 Confidence-Based Reliability-Based Design Optimization 
Formulation ...................................................................................................108 
6.3 A 2-D Mathematical Example ................................................................115 
6.3.1 Problem Definition .......................................................................115 
6.3.2 Optimization Results for Small Noise Level ................................116 
6.3.3 Comparing Small, Medium, and Large Noise Optimization 
Results ...................................................................................................119 
6.4 A 3-D Multibody Dynamics Block-Car Example ..................................124 
CHAPTER 7 CONCLUSION, CURRENT RESEARCH, AND FUTURE 
RESEARCH .................................................................................................131 
 
7.1 Conclusion ..............................................................................................131 
7.2 Future Research ......................................................................................132 
REFERENCES ................................................................................................................134 
 
vii 

LIST OF TABLES 
Table 2.1 Probability Distribution and Its Transformation between X- and U-space .......16 
Table 2.2 Marginal PDF, CDF, and Parameters ................................................................26 
Table 2.3 First-Order Score Function for   for Independent Random Variables ...............26 
Table 2.4 Log-Derivative of Copula Density Function .....................................................28 
Table 3.1 Correlation Functions ........................................................................................34 
Table 4.1 Prior Parameter Values ......................................................................................55 
Table 4.2 Initial Values for Markov Chains ......................................................................56 
Table 4.3 Posterior Statistics for Unknown Parameters Using 25 DoE Samples ..............66 
Table 4.4 Posterior Statistics for Unknown Parameters Using 50 DoE Samples ..............72 
Table 4.5 Prior Parameter Values for Fitting Eq. (4.5) ......................................................75 
Table 4.6 Initial Values for Markov Chains for Fitting Eq. (4.5) ......................................75 
Table 5.1 Prior Parameter Values ......................................................................................90 
Table 5.2 Initial Values for Markov Chains ......................................................................90 
Table 5.3 The Three Noise Levels Used ..........................................................................101 
Table 5.4 Probability of Failure Statistics Using Different Numbers of DoE 
Samples ...................................................................................................................105 
Table 5.5 Probability of Failure Statistics for Small Noise .............................................107 
Table 5.6 Probability of Failure Statistics for Medium Noise .........................................107 
Table 5.7 Probability of Failure Statistics for Large Noise .............................................107 
Table 6.1 The Three Noise Levels Used ..........................................................................116 
Table 6.2 Optimization History for Small Noise Constraint 1 ........................................117 
Table 6.3 Optimization History for Small Noise Constraint 2 ........................................118 
Table 6.4 Optimization History for Different Numbers of DoE Constraint 1 .................119 
Table 6.5 Optimization History for Different Numbers of DoE Constraint 2 .................119 
Table 6.6 Optimization History for Medium Noise Constraint 1 ....................................120 
Table 6.7 Optimization History for Medium Noise Constraint 2 ....................................121 
 
viii 

Table 6.8 Optimization History for Large Noise Constraint 1 ........................................123 
Table 6.9 Optimization History for Large Noise Constraint 2 ........................................123 
Table 6.10 Comparing Optimization Results for Different Noise Levels Constraint 
1 ..............................................................................................................................123 
Table 6.11 Comparing Optimization Results for Different Noise Levels Constraint 
2 ..............................................................................................................................123 
Table 6.12 Design Bounds and Initial Design Point for DDO ........................................126 
Table 6.13 DDO Optimization History for Block Car .....................................................127 
Table 6.14 Design Bounds and Initial Design Point for Confidence-Based RBDO .......127 
Table 6.15 Confidence-Based RBDO Optimization History for Block-Car 
Constraint 1 .............................................................................................................129 
Table 6.16 Confidence-Based RBDO Optimization History for Block-Car 
Constraint 2 .............................................................................................................130 
 
 
ix 

LIST OF FIGURES 
Figure 2.1 MPP and Reliability Index   in the U-Space [Source: Wei 2006] ....................17 
Figure 2.2 Probabilistic Sensitivity Analysis .....................................................................25 
Figure 3.1 Correlation Contour of the Gaussian Correlation Function .............................38 
Figure 4.1 Eq. (4.1) without Noise and 25 DoE Samples ..................................................53 
Figure 4.2 Ordinary Kriging Fit of Eq. (4.1) Using 25 DoE .............................................53 
Figure 4.3 History Plot of the First 5000 Iterations for 
c
µ  ...............................................58 
Figure 4.4 History Plot of the First 200 Iterations for 
2
σ  .................................................59 
Figure 4.5 History Plot of the First 2000 Iterations for λ .................................................59 
Figure 4.6 History Plot of the First 2000 Iterations for θ  .................................................60 
Figure 4.7 History Plot of All 100,000 Iterations for 
c
µ  ..................................................60 
Figure 4.8 History Plot of All 100,000 Iterations for 
2
σ  ..................................................61 
Figure 4.9 History Plot of All 100,000 Iterations for λ  ....................................................61 
Figure 4.10 History Plot of All 100,000 Iterations for θ  ..................................................62 
Figure 4.11 BGR Plot for 
c
µ  .............................................................................................64 
Figure 4.12 BGR Plot for 
2
σ  ............................................................................................64 
Figure 4.13 BGR Plot for λ  ..............................................................................................65 
Figure 4.14 BGR Plot for θ  ..............................................................................................65 
Figure 4.15 MBKG Fit of Eq. (4.1) Using 25 DoE ...........................................................67 
Figure 4.16 MBKG Fit of Eq. (4.1) and 95% Credible Sets Using 25 DoE ......................69 
Figure 4.17 MBKG Fit of Eq. (4.1) and 95% Credible Sets Using 50 DoE ......................70 
Figure 4.18 95% Credible Sets without Noise ...................................................................70 
Figure 4.19 95% Credible Sets with Noise ........................................................................71 
Figure 4.20 Surface Plot of Eq. (4.5) and 25 DoE Samples ..............................................74 
Figure 4.21 Contour Plot of True Limit State Function for Eq. (4.5) and 25 DoE 
Samples .....................................................................................................................74 
Figure 4.22 MBKG Predicted Contour Plot of Limit State Function for Eq. (4.5) ...........76 
 
x 

Figure 4.23 Surface Plot of Eq. (4.8) and 25 DoE Samples ..............................................78 
Figure 4.24 Contour Plot of True Limit State Function for Eq. (4.8) and 25 DoE 
Samples .....................................................................................................................78 
Figure 4.25 MBKG Predicted Contour Plot of Limit State Function for Eq. (4.8) ...........79 
Figure 5.1 Contour Plot of Limit State and 400 Test Points ..............................................83 
Figure 5.2 Test Points with 95% Credible Sets that Capture the Limit State Using 
25 DoE ......................................................................................................................83 
Figure 5.3 Additional 20 DoE Samples .............................................................................86 
Figure 5.4 Contour Plot of Limit State of Eq. (4.4) Using 45 DoE Samples ....................87 
Figure 5.5 Test Points with 95% Credible Sets that Capture the Limit State Using 
45 DoE ......................................................................................................................88 
Figure 5.6 Test Points with 95% Credible Sets that Capture the Limit State Using 
25 DoE ......................................................................................................................92 
Figure 5.7 Additional 20 DoE Samples Shown as Black Squares .....................................93 
Figure 5.8 Contour Plot of Limit State Using 45 DoE Samples ........................................93 
Figure 5.9 Test Points with 95% Credible Sets that Capture the Limit State Using 
45 DoE ......................................................................................................................94 
Figure 5.10 Additional 20 DoE Samples Shown as Pink Circles ......................................95 
Figure 5.11 Contour of Limit State Using 65 DoE Samples .............................................95 
Figure 5.12 Test Points with 95% Credible Sets that Capture the Limit State Using 
65 DoE Samples Shown as the Light Blue Plus Signs .............................................96 
Figure 5.13 Additional 20 DoE Samples as Blue Diamonds and Contour of Limit 
State Using 85 DoE Samples as the Green Curve ....................................................97 
Figure 5.14 Test Points with 95% Credible Sets that Capture the Limit State Using 
85 DoE Samples Shown as Black Crosses ...............................................................98 
Figure 5.15 Additional 20 DoE Samples as Green Asterisk and Contour of Limit 
State Using 105 DoE Samples as the Blue Curve ....................................................98 
Figure 5.16 Test Points with 95% Credible Sets that Capture the Limit Statue Using 
105 DoE Samples Shown as the Red Circles ...........................................................99 
Figure 5.17 Posterior Distribution of Probability of Failure Using 25 DoE Samples .....103 
Figure 5.18 Posterior Distribution of Probability of Failure Using 45 DoE Samples .....103 
Figure 5.19 Posterior Distribution of Probability of Failure Using 65 DoE Samples .....104 
 
xi 

Figure 5.20 Posterior Distribution of Probability of Failure Using 85 DoE Samples .....104 
Figure 5.21 Posterior Distribution of Probability of Failure Using 105 DoE 
Samples ...................................................................................................................105 
Figure 6.1 Posterior Distribution of the Probability of Failure ........................................109 
Figure 6.2 Posterior Distribution of the Probability of Failure with 
. .
80%
C L =
 ............111 
Figure 6.3 Posterior Distribution of the Probability of Failure with 
. .
90%
C L =
 ............111 
Figure 6.4 Constraint Diagram for Eq. (6.4) ....................................................................113 
Figure 6.5 Multibody Dynamcis Block-Car Example .....................................................124 
 
 
xii 

LIST OF ACRONYMS 
AMV 
Advanced Mean Value Method 
BGR 
Brooks, Gelman, and Rubin 
CAE 
Computer Aided Engineering 
CDF 
Cumulative Distribution Function 
CFD 
Computational Fluid Dynamics 
CMV 
Conjugate Mean Value Method 
DDO 
Deterministic Design Optimization 
DKG 
Dynamic Kriging 
DoE 
Design of Experiment 
DRM 
Dimension Reduction Method 
FE 
Finite Element 
FORM 
First-Order Reliability Method 
HMV 
Hybrid Mean Value Method 
HMV+ 
Enhanced Hybrid Mean Value Method 
LCVT 
Latinized Centroidal Voronoi Tessellation 
MARS 
Multivariate Adaptive Regression Splines 
MBKG 
Modified Bayesian Kriging 
MCMC 
Markov Chain Monte Carlo 
MCS 
Monte Carlo Simulation 
MBD 
Multibody Dynamics 
MLS 
Moving Least Squares 
MPP 
Most Probable Point 
MV 
Mean Value Method 
NN 
Neural Networks 
OKG 
Ordinary Kriging 
xiii 
 

PDF 
Probability Density Function 
PMA 
Performance Measure Approach 
PRS 
Polynomial Response Surface 
RBF 
Radial Basis Functions 
RBDO 
Reliability-Based Design Optimization 
RIA 
Reliability Index Approach 
SORM 
Second-Order Reliability Method 
UKG 
Universal Kriging 
VSVM 
Virtual Support Vector Machine 
xiv 
 

 
 
1 
 
CHAPTER 1 
INTRODUCTION 
This study presents a new surrogate modeling method for reliability-based design 
optimization (RBDO) of problems whose simulation analyses are inherently noisy. A 
new modified Bayesian Kriging (MBKG) surrogate modeling method is proposed for 
handling performance measures of noisy simulations, which will accurately represent the 
true underlying unknown performance function without noise. Using Bayesian methods 
allows for a way to naturally characterize the uncertainty of the predicted values by 
providing the distribution of the predicted values. Credible sets produced by the surrogate 
model are used to develop a sequential sampling method and also to create a conservative 
RBDO design. The likelihood for the MBKG is proposed, and the prior distributions to 
be used with the likelihood to fit the Bayesian model are presented. A Markov chain 
Monte Carlo algorithm for fitting the MBKG model is developed as follows. Using the 
prior distributions, the full conditional distributions of the MBKG parameters are derived; 
where possible, conjugate prior distributions are used to help simplify the full 
conditionals and to ease the computational burden of fitting the surrogate model. 
Computer simulations of engineering models are computationally expensive; therefore, it 
is necessary to reduce the number of design of experiment (DoE) samples needed. An 
efficient DoE sampling method using the credible sets of the MBKG model will be 
developed to systematically reduce the uncertainty in the surrogate model. Finally, a 
confidence-based RBDO method using the posterior distribution of the probability of 
failure will be developed. 
Section 1.1 presents the background and motivation of the proposed research; 
Section 1.2 gives a brief introduction to Bayesian statistics; Section 1.3 discusses the 
objectives of the proposed research; and Section 1.4 presents the thesis organization. 
 
 

 
 
2 
 
1.1 Background and Motivation 
1.1.1 Reliability-Based Design Optimization 
Customers want products that are low cost and also reliable, whether they are 
purchasing a small kitchen appliance, smartphone, tablet, computer, vehicle, or heavy 
machinery. At the same time, manufacturers want to make products that cost less to make 
and are reliable in order to reduce both warranty and production costs. The objective to 
reduce cost naturally led to the development of optimization methods. Just carrying out 
optimization without consideration of uncertainty is referred to as deterministic design 
optimization (DDO). Deterministic design optimization solutions, i.e., designs of 
products, are only approximately 50% reliable. Safety factors are often used when 
designing products to try to ensure that they will last and be reliable; however, estimation 
of the safety factor could be heuristic, which in turn increases manufacturing cost or 
reduces reliability. Both of these realities have brought about the need for RBDO. 
Most reliability analysis methods can be classified into groups, the first being 
sensitivity-based methods and the second being sampling-based methods. The literature 
is rich with numerous sensitivity-based methods that have been developed using the most 
probable point (MPP) [Lee et al. 2010; Lee et al. 2008; Rahman and Wei 2006; Youn and 
Choi 2003; Haldar and Mahadevan 2000; Tu et al. 1999]. Some common sensitivity-
based methods are the first-order reliability method (FORM) [Haldar and Mahadevan 
2000; Hohenbichler et al. 1987; Madsen et al. 1985], the second-order reliability method 
(SORM) [Haldar and Mahadevan 2000; Hohenbichler and Rackwitz 1988; Hohenbichler 
et al. 1987; Madsen et al. 1985], and the dimension reduction method (DRM) [Lee et al. 
2010; Lee et al. 2008; Rahman and Wei 2008; Rahman and Wei 2006]. While these 
methods can be computationally cheaper than sampling-based methods, one pitfall is that 
they may not be as accurate as sampling-based methods for highly nonlinear problems 
[Lee et al. 2011]. Another shortcoming of these methods is that they require the 
 

 
 
3 
 
sensitivity of the performance measures to be available. Obtaining the sensitivity, while 
possible for some problems, can be a daunting if not impossible task for some problems 
that are highly nonlinear and/or coupled with fluid structure interaction, e.g., crash and 
blast problems. 
To overcome the shortfalls of the sensitivity-based methods, sampling-based 
methods have been developed. A brute force approach would be to do direct Monte Carlo 
simulation (MCS) using the computer-aided engineering (CAE) simulation models, e.g., 
finite element (FE) models and computational fluid dynamic (CFD) models, to calculate 
the reliability. While this method can be highly accurate using a large number of MCS 
points [Haldar and Mahadevan 2000], its limitation is the large number of MCS points 
required due to the computational cost of the CAE model simulations, thus rendering it 
impractical. In order to overcome the impractical computational cost of direct MCS using 
CAE simulations and to overcome the pitfalls of the sensitivity-based methods, the use of 
surrogate models is becoming a more common practice [Shi et al. 2012; Song et al. 2011; 
Zhao 2011; Zhao et al. 2011; An and Choi 2012; Rajashekhar and Ellingwood 1993]. 
There are three advantages of using surrogate models. The first is that the sensitivity of 
the performance measure is not needed to construct the surrogate. The second is that 
surrogate models are computationally inexpensive to use for evaluating large numbers of 
MCS points compared to the computational cost of the CAE models. The third is that 
surrogate models can be built using a limited number of CAE model simulations, 
therefore reducing the overall computational cost of performing a reliability analysis. The 
literature is full of numerous surrogate modeling methods that have been developed; 
these will be discussed in the next section. 
 
 

 
 
4 
 
1.1.2 Surrogate Modeling Methods 
Surrogate modeling methods have been under development for decades and are 
still being actively developed. The main reason for all the development is that there is not 
one surrogate modeling method that works for every problem. However, the use of 
surrogate models for product design is becoming a more common practice during the 
design stage to help engineers gain a quick understanding of a problem due to their 
relative ease of use, inexpensive computational time, and ready availability in a number 
of commercial software programs, e.g., HyperStudy, Isight, Matlab, and Mathematica. 
As previously mentioned, there are numerous surrogate modeling methods in the 
literature, such as polynomial response surface (PRS) [Forrester and Keane 2009; 
Forrester et al. 2008; Fang et al. 2005; Jin et al. 2001; Box and Draper 1987; Rajashekhar 
and Ellingwood 1993; Mullur and Messac 2006; Simpson et al. 2001; Wang and Shan 
2007], polynomial chaos [Wiener 1938; Hu and Youn 2011; Isukapalli 1999; Kewlani 
and Iagnemma 2008; Wei et al. 2008], moving least squares (MLS) [Forrester and Keane 
2009; Breitkopf et al. 2005; Lancaster and Salkauskas 1981; Levin 1998], multivariate 
adaptive regression splines (MARS) [Jin et al. 2001; Friedman 1991; Friedman and 
Roosen 1995; Lewis and Stevens 1991; Simpson et al. 2001; Wang and Shan 2007], 
support vector machine and support vector regression [Forrester and Keane 2009; 
Forrester et al. 2008; Burges 1998; Hearst et al. 1998; Wang and Shan 2007], virtual 
support vector machine (VSVM) [Song et al. 2011], radial basis functions (RBF) 
[Forrester and Keane 2009; Buhmann 2003; Forrester et al. 2008; Fang et al. 2005; Jin et 
al. 2001; Park and Sandberg 1991; Dyn et al. 1986; Mullur and Messac 2006; Wang and 
Shan 2007], neural networks (NN) [Agatonovic-Kustrin and Beresford 2000; Almeida 
2002; Fonseca et al. 2003; Liu and Fang 2009; Sakata et al. 2010; Ukrainec et al. 1989; 
van der Merwe et al. 2007; Zobel et al. 2008; Galushkin 2007; Gallant 1993; Forrester et 
al. 2008; Simpson et al. 2001; Wang and Shan 2007], ordinary Kriging (OKG) and 
universal Kriging (UKG) [Krige 1951; Cressie 1991; Sacks et al. 1989; Beers and 
 

 
 
5 
 
Kleijnen 2003; Forrester and Keane 2009; Forrester et al. 2008; Mullur and Messac 2006; 
Jin et al. 2001; Simpson et al. 2001; Wang and Shan 2007], and dynamic Kriging (DKG) 
[Zhao 2011; Zhao et al. 2011]. There have been surrogate modeling methods developed 
that use Bayesian methods [Shi et al. 2012; An and Choi 2012; Romero et al. 2012; 
Romero et al. 2003; Romero 2008; Omre and Halvorsen 1989; Currin et al. 1991]. 
However, none of the methods studied claim to do a full Bayesian analysis when creating 
the surrogate model; they are only borrowing concepts from the Bayesian methods. 
As previously described, a surrogate model is used to do the MCS prediction for 
the reliability analysis for the sampling-based RBDO method to ease the computational 
burden. Thus, the accuracy of the reliability analysis depends on the accuracy of the 
surrogate model. Kriging has become a popular surrogate modeling method because it 
offers flexibility in the choice of both the mean structure and the correlation function 
used [Forrester and Keane 2009; Forrester et al. 2008]. Dynamic Kriging was developed 
because it was found to be more accurate than OKG or UKG, giving better reliability 
analysis results [Zhao 2011; Zhao et al. 2011]. However, OKG, UKG, and DKG are 
typically formulated and used as interpolation methods and therefore break down when 
the response data contains noise [Forrester and Keane 2009; Forrester et al. 2008; Sakata 
et al. 2007; Sakata et al. 2008]. There are approximation and regression methods, e.g. 
MLS and MARS, which can be used when the response data contains noise. The 
formulation of Kriging can even be modified to change it to a regression method 
[Forrester and Keane 2009; Forrester et al. 2008; Sakata et al. 2007; Sakata et al. 2008]. 
The disadvantage of these methods is that they do not allow for a direct way to 
separate the noise from the data. Thus, when using them to predict response values, it is 
not known how much noise there may be in the predicted response value. If these 
regression surrogate models are used for MCS prediction for reliability analysis, this will 
affect the amount of uncertainty and variability that there is in the reliability analysis. It is 
not clear or easy to distinguish if the variability in the reliability analysis is due to the 
 

 
 
6 
 
noise in the predicted response or due to the uncertainty in the surrogate model itself. 
Another disadvantage of these methods is that they do not have a systematic way of 
characterizing the uncertainty of the surrogate model, the noise in the response value, or 
the uncertainty in the predicted response values using the surrogate model. Therefore, a 
surrogate modeling method that can systematically characterize all of these uncertainties 
as well as accurately predict the true underlying response value without noise needs to be 
investigated. Bayesian statistical methods allow for a natural and systematic way of 
characterizing uncertainty when predicting unknown parameters of distributions as well 
as predicting parameters that depend on these unknown distribution parameters. A brief 
introduction of Bayesian statistical methods is given in the next section. 
 
1.2 Bayesian Statistics 
1.2.1 Likelihood and Prior Distributions 
In real-world problems there is often data available or obtainable that is known or 
assumed to come from a given distribution type, e.g., the data is known to follow a 
normal distribution. The distribution from which the data comes is referred to as the 
sampling distribution or data distribution [Gelman et al. 2004; Hamada et al. 2008]. 
However, even when the distribution type of the data is known, the parameter values of 
the distribution are often unknown. When the data distribution is considered as a function 
of the unknown distribution parameters for a given data set, it is referred to as the 
likelihood function [Gelman et al. 2004; Cowles 2013; Hamada et al. 2008; Bayes and 
Price 1763]. 
The goal in Bayesian statistics is to come up with an estimate of the unknown 
parameter values using both the available data and prior knowledge about the unknown 
parameters. In Bayesian statistics the unknown parameters are treated as if they are 
random variables. Any prior knowledge or belief about the unknown parameter is 
 

 
 
7 
 
expressed using a probability distribution. The definition of subjective probability of an 
event given by Cowles is: “A probability of an event or of the truth of a statement is a 
number between 0 and 1 that quantifies a particular person’s subjective opinion as to how 
likely that event is to occur (or to have already occurred) or how likely the statement is to 
be true” [2013]. Similarly, a subjective probability distribution quantifies one’s 
knowledge about an unknown parameter that may take on any value in a continuum. 
These probability distributions that express one’s knowledge about the unknown 
parameters are referred to as the prior distributions or simply as the priors [Cowles 2013; 
Gelman et al. 2004; Hamada et al. 2008; Bayes and Price 1763]. A Bayesian analysis is 
carried out to update one’s subjective probability distribution of the unknown parameters 
by combining prior information with the new information contained in the data. The next 
section will describe how Bayes’ rule is used with the likelihood and prior distributions to 
calculate the posterior distribution. 
 
1.2.2 Bayes’ Rule and Posterior Distributions 
Bayes’ rule provides a way to mathematically and systematically update one’s 
subjective probability distribution on model parameters. All knowledge available before 
the current data is observed is encapsulated in the prior. The updating occurs by 
incorporating the new data contained in the likelihood. Bayes’ rule states that the 
posterior probability distribution of model parameters given the data is proportional to the 
product of the likelihood and prior probability distribution [Bayes and Price 1763; 
Cowles 2013; Gelman et al. 2004; Hamada et al. 2008]. Mathematically this can be 
written as shown in Eq. (1.1). 
 
 
(
)
(
)
(
)
|
|
f Parameter Data
f Parameter
f Data Parameter
∝
×
  
(1.1) 
 

 
 
8 
 
The final probability distribution given on the left side of Eq. (1.1) is called the 
posterior distribution, often referred to as the posterior. The posterior distribution 
expresses the current state of knowledge about model parameters. The posterior 
distribution can be used to obtain desired values about the event, e.g., the mean or median 
of the posterior could be used as a point estimate for the unknown parameter value. The 
posterior variance reveals the amount of uncertainty that remains about the parameter 
value—the larger the variance, the larger the uncertainty about what the true parameter 
value is. The posterior can also be used to give probability intervals, called credible sets, 
which are believed to contain the true parameter value with the specified probability, e.g., 
the 95% credible set for the parameter can easily be obtained from the posterior. Credible 
sets have a slightly different meaning than confidence intervals. For the 95% credible set, 
the probability that the true value of the parameter is in that interval is 95%. A 95% 
confidence interval means that if the same experiment or test is repeated many times to 
generate many different data sets, and each data set is used to generate a 95% confidence 
interval, only 95% of the confidence intervals generated would capture the true parameter 
value, while 5% of the confidence intervals generated would not capture the true 
parameter value [Cowles 2013; Gelman 2004]. The next section will discuss the use of 
conjugate priors and Markov chain Monte Carlo for updating the posterior distribution. 
 
1.2.3 Conjugate Priors and Markov Chain Monte Carlo 
As described in the previous section, the product of the prior distribution and 
likelihood is used in Bayes’ rule to construct the posterior distribution of the unknown 
parameter(s). When possible, it is desirable to choose the prior distribution from a 
parametric family that takes on the same functional form as the likelihood function for 
the unknown parameter(s); priors of such a form are called conjugate priors. The 
parameters of the prior distribution are then chosen such that the prior reflects the known 
 

 
 
9 
 
information and beliefs about the unknown parameter(s). When applying Bayes’ rule to 
the likelihood with a conjugate prior, the posterior distribution will belong to the same 
parametric family as the prior, i.e., the posterior distribution type will be the same 
distribution type as the prior. The parameter values of the posterior distribution will be a 
combination of the prior parameter values and the data used for the Bayesian analysis 
[Cowles 2013; Gelman 2004; Hamada et al. 2008]. 
There are scenarios in which a conjugate prior does not exist for a given problem, 
e.g., a conjugate prior does not adequately reflect the prior knowledge and belief about 
the unknown parameter(s), or there are multiple unknown parameters for which a 
conjugate joint distribution does not exist. For such scenarios, any distribution type that 
reflects the prior knowledge and belief about the unknown parameter(s) can be used as 
the prior. The use of such priors, however, leads to posterior distributions that most likely 
are not from a known distribution family type. Bayes’ rule can still be applied in such 
cases but has to be done using a numerical method. 
Markov chain Monte Carlo (MCMC) is a numerical method that can be used to 
draw samples from high-dimensional and nonstandard distribution types. One 
disadvantage of MCMC is that the samples drawn from the distribution are not 
independent, and this needs to be taken into consideration when using the samples for 
inference [Cowles 2013; Gelman 2004; Gilks et al. 1998; Hamada et al. 2008; Tierney 
1998; Feller 1968]. The Markov property states that a sample drawn at a given time point 
conditional on the sample drawn at the time point immediately before it is independent of 
all the earlier samples drawn. Under certain regularity conditions, it can be shown that a 
Markov chain will converge in distribution to samples drawn from the target, i.e., 
posterior distribution [Cowles 2013; Gelman 2004; Gilks et al. 1998; Tierney 1998; 
Feller 1968]. There is a debate over whether it is better to run one long Markov chain or 
to run multiple shorter parallel Markov chains starting at different initial values to 
attempt to determine if the Markov chain has converged to the target distribution as well 
 

 
 
10 
 
as how to best assess whether convergence has occurred [Kass et al. 1998; Cowles and 
Carlin 1996]. One common and well-accepted way of diagnosing convergence in 
distribution when using MCMC is to use the Brooks, Gelman, and Rubin (BGR) 
diagnostic [Gelman and Rubin 1992; Brooks and Gelman 1998]. The BGR diagnostic 
requires running at least two parallel Markov chains starting at over-dispersed initial 
values. The BGR diagnostic uses the samples drawn from the parallel Markov chains to 
calculate credible sets of the individual chains using an increasing number of samples 
from the chain. The parallel chains are also pooled together to form one sample set that is 
used to calculate credible sets using an increasing number of samples. If widths of the 
credible sets calculated using the two different methods stabilize and become 
approximately equal, the MCMC chains are likely to have converged in distribution 
[Gelman and Rubin 1992; Brooks and Gelman 1998; Lunn et al. 2000; Lunn et al. 2009; 
Gelman 2013; Cowles 2013]. It is possible for the BGR diagnostic to misdiagnose 
convergence, i.e., the BGR diagnostic shows that convergence has been achieved when in 
actuality convergence has not yet been reached. The next section will discuss the 
objectives of the proposed study and how Bayesian statistical methods will be used in 
creating and fitting a surrogate model for problems with noisy simulation analyses, as 
well as capturing the uncertainty in the surrogate model and predicted values using 
credible sets. 
 
1.3 Objectives of the Proposed Study 
The first objective of this study is to develop a modified Bayesian Kriging 
(MBKG) surrogate modeling method that can accurately model the true underlying 
response value for noisy simulations. The posterior credible sets of the MBKG surrogate 
model will capture and show the uncertainty in the predicted values. The posterior 
credible sets will be used for carrying out confidence-based RBDO. The different Kriging 
 

 
 
11 
 
methods, OKG, UKG, and DKG, have been shown to be more accurate while using less 
data than other existing surrogate modeling methods [Zhao 2011; Zhao et al. 2011; 
Forrester and Keane 2009; Forrester et al. 2008]. This is why this study proposes using a 
modified Bayesian Kriging method. 
The second objective is to develop an efficient method for selecting new DoE 
samples using the credible sets. This will allow for a systematic and mathematical way to 
continuously improve the MBKG surrogate model and measure the amount of 
improvement during sequential DoE sampling. Using the credible sets, new DoE samples 
will be added in areas where the uncertainty of the surrogate model is large, thereby 
decreasing the uncertainty of the surrogate model. 
The third and final objective is to develop a confidence-based RBDO method 
using the posterior distribution of the probability of failure. The confidence-based RBDO 
method will be used to generate conservative reliable optimal designs. In order to 
perform optimization, the sensitivity of the probability of failure is needed and thus will 
be derived. The next section describes the organization of the thesis. 
 
1.4 Organization of Thesis 
Chapter 2 presents fundamental concepts of reliability analysis and reliability-
based design optimization for both sensitivity-based and sampling-based methods. 
Chapter 3 presents the existing conventional Kriging methods and the proposed 
modified Bayesian Kriging method. The prior distributions and corresponding full 
conditional distributions for the modified Bayesian Kriging method are also presented. 
Chapter 4 presents three different examples demonstrating the use of the modified 
Bayesian Kriging method for fitting problems with noisy responses. 
 

 
 
12 
 
Chapter 5 presents the proposed sequential sampling method that uses the 
posterior credible sets for inserting new design of experiment sample points for updating 
the modified Bayesian Kriging surrogate model. 
Chapter 6 presents the confidence-based reliability-based design optimization 
method that uses the posterior distribution of the probability of failure. A mathematical 
example using different amounts of noise are used to demonstrate the method. A 3-D 
multibody dynamics engineering example is used to demonstrate the method. 
Chapter 7 presents the conclusions of the study and the future research to be 
carried out to enhance the modified Bayesian Kriging method to make carrying out 
confidence-based RBDO more efficient. 
 
 
 
 

 
 
13 
 
CHAPTER 2 
DESIGN UNDER UNCERTAINTY 
2.1 Introduction 
This chapter gives a summary of the fundamental concepts in design under 
uncertainty, including sensitivity-based and sampling-based RBDO methods. Sections 
2.2 and 2.3 present the basic ideas of reliability analysis and inverse reliability analysis 
methods. Section 2.4 discusses the sensitivity-based method using both the FORM and 
DRM methods. Section 2.5 introduces the sampling-based RBDO that is used for 
problems when the sensitivity cannot be calculated. As discussed in the previous chapter, 
the sampling-based method uses MCS for calculating the probability of failure and also 
the probabilistic sensitivity for the performance measures that are used for optimization. 
 
2.2 Reliability Analysis 
In order to perform a reliability analysis, the calculation of the probability of 
failure is required. The probability of failure, denoted by 
F
P , is calculated using the 
multi-dimensional integral [Madsen et al. 1986] 
 
 
(
) 0
[ ( )
0]
( )
F
G
P
P G
f
d
>
≡
>
= ∫
X
X
X
x x  
(2.1) 
 
where 
T
1
2
={
, 
,
, 
}
nr
X
X
X
X

 is an nr dimensional random vector, nr  is the number of 
random variables, 
( )
G X  is the performance measure function that is defined such that 
( )
0
G
>
X
 is failure, and 
( )
fX x  is the joint probability density function (PDF) of the 
random input variables X . For most real-world engineering problems, the exact 
evaluation of Eq. (2.1) is very difficult if not impossible to carry out since 
( )
fX x  is 
usually non-Gaussian due to correlation in the random variables and 
( )
G X  can be highly 
 

 
 
14 
 
nonlinear. The integration domain of Eq. (2.1) generally cannot be expressed analytically 
due to the nonlinearity of 
( )
G X . To handle the non-Gaussian distribution and highly 
nonlinear 
( )
G X , a transformation of the random variables, X , from the X-space to the 
independent standard normal U-space is carried out [Rosenblatt 1952; Hogg et al. 2005]. 
To deal with highly nonlinear performance measures, 
( )
G X  is approximated using first-
order Taylor series expansion in the FORM and SORM methods for the sensitivity-based 
RBDO methods, and probabilistic MCS methods are used for sampling-based RBDO 
methods. The next section will introduce the transformation of variables. 
 
2.2.1 Random Variable Transformation 
For an nr dimensional random vector X  that has a joint cumulative distribution 
function (CDF) 
( )
FX x , let 
:
T
→
X
U  denote the transformation from X-space to U-
space that is defined by the Rosenblatt transformation [Rosenblatt 1952] as 
 
 
( )
(
)
(
)
1
2
1
1
1
1
2
2
1
1
1
2
1
:
,
,
,
nr
X
X
nr
X
nr
nr
u
F
x
u
F
x x
T
u
F
x
x x
x
−
−
−
−



= Φ






= Φ








= Φ






  
(2.2) 
where 
(
)
1
2
1
,
,
,
i
X
i
i
F
x x x
x −

 is the conditional CDF given by 
 
 
(
)
1
2
1
2
1
1
2
1
1
2
1
1
2
1
( ,
,
,
, )
,
,
,
( ,
,
,
)
i
i
i
i
x
X X
X
i
X
i
i
X X
X
i
f
x x
x
d
F
x x x
x
f
x x
x
ξ
ξ
−
−
−∞
−
−
= ∫





  
(2.3) 
 
where ξ  is the random variable being integrated over and 
( )
Φ •  is the standard normal 
CDF given by 
 

 
 
15 
 
 
2
1
1
( )
( )
exp
2
2
u
u
u
d
d
φ ξ
ξ
ξ
ξ
π
−∞
−∞


Φ
=
=
−




∫
∫
  
(2.4) 
where φ  is the standard normal PDF. 
The inverse transformation of Eq. (2.2) is expressed as 
 
 
( )
1
2
1
1
1
1
2
2
1
1
1
1
2
1
(
)
:
(
,
,
,
)
nr
X
X
nr
X
nr
nr
x
F
u
x
F
u x
T
x
F
u
x x
x
−
−
−
−
−

=
Φ






=
Φ






=
Φ






  
(2.5) 
If the random variables of the X  vector are independent, then the joint PDF is given as 
the product of the marginal PDF’s 
( )
i
X
i
f
x  as 
 
 
1
2
1
2
( )
( )
(
)
(
)
nr
X
X
X
nr
f
f
x
f
x
f
x
=
×
×
×
X x

  
(2.6) 
In this case the Rosenblatt transformation and the inverse transformation simplify to 
 
 
( )
( )
1
1
  and  
i
i
i
X
i
i
X
i
u
F
x
x
F
u
−
−


= Φ
=
Φ






  
(2.7) 
where 
( )
i
X
i
F
x  are the marginal CDFs. Table 2.1 shows five representative distributions 
and their corresponding transformations, assuming the random variables are independent. 
 
 
 
 
 
 
 

 
 
16 
 
Table 2.1 Probability Distribution and Its Transformation between X- and U-space 
 
Parameters 
PDF 
Transformation 
Normal 
mean
µ =
 
standard deviation
σ =
 
2
0.5[
]
1
( )
2
x
f x
e
µ
σ
πσ
−
−
=
 
X
U
µ
σ
=
+
 
Log-
normal 
2
2
ln[1 (
) ]
σ
σ
µ
=
+
, 
2
ln( )
0.5
µ
µ
σ
=
−
 
ln
0.5[
]
1
( )
2
x
f x
e
x
µ
σ
π σ
−
−
=
 
exp(
)
X
U
µ
σ
=
+
 
Weibull 
1
(1
)
v
k
µ = Γ
+
,  
2
2
2
2
1
[ (1
)
(1
)]
v
k
k
σ
=
Γ
+
−Γ
+
 
(
)
1
( )
( )
k
x
k
v
k x
f x
e
ν ν
−
−
=
 
1
[ ln(
(
))]k
X
v
U
=
−
Φ −
 
Gumbel 
0.577
µ
ν
α
=
+
,
6
π
σ
α
=
 
(
)
(
)
( )
x
x
e
f x
e
α
ν
α
ν
α
−
−
−
−
−
=
 
1 ln[ ln(
( ))]
X
U
ν
α
=
−
−
Φ
 
Uniform 
2
a
b
µ
+
=
, 
12
b
a
σ
−
=
 
1
( )
,
f x
a
x
b
b
a
=
≤
≤
−
 
(
) ( )
X
a
b
a
U
=
+
−
Φ
 
 
 
2.2.2 First-Order Reliability Method (FORM) and Second-
Order Reliability Method (SORM) 
To calculate the probability of failure of the performance measure function 
( )
G X  
using FORM and SORM, it is first necessary to find the MPP, which is defined as the 
point 
*
u  on the limit state function, ( )
0
g
=
u
, closest to the origin in the standard normal 
U-space as shown in Figure 2.1. Using the Rosenblatt transformation, the performance 
measure function in the U-space is defined as ( )
( ( ))
( )
g
G
G
≡
=
u
x u
x . Thus, the MPP can 
be found by solving the following optimization problem: 
 
 
minimize     
subject to     g( )
0
=
u
u
  
(2.8) 
The distance from the MPP to the origin is commonly called the Hasofer-Lind 
reliability index [Hasofer and Lind 1974] and is denoted by 
HL
β
. Using this reliability 
 

 
 
17 
 
index, FORM can approximate the probability of failure using a linear approximation of 
the performance function given as 
 
 
FORM
HL
(
)
F
P
β
≅Φ −
  
(2.9) 
A quadratic approximation of the performance measure function in the U-space 
and the rotational transformation from the standard normal U-space to the rotated 
standard normal V-space is used in SORM to calculate the probability of failure 
[Breitung 1984; Hohenbichler and Rackwitz 1988; Rahman and Wei 2006]. 
 
Figure 2.1 MPP and Reliability Index 
HL
β
 in the U-Space [Source: Wei 2006] 
 
 

 
 
18 
 
2.3 Inverse Reliability Analysis 
The reliability analysis using FORM and SORM presented in the previous section 
is called the reliability index approach (RIA) [Tu et al. 1999] since it finds the reliability 
index 
HL
β
 using Eq. (2.8). The RIA method has the advantage that the probability of 
failure of the performance measure function can be calculated at the given design. 
However, the inverse reliability analysis in the performance measure approach (PMA) 
[Tu et al. 1999; Tu et al. 2001; Choi et al. 2001; Youn et al. 2003] is known to be 
numerically more efficient and stable than RIA. The probability of failure is not directly 
calculated in PMA. Instead, PMA determines if a given design satisfies the probabilistic 
constraint for a given target probability of failure 
Tar
F
P
. The optimization problem for 
PMA is defined as 
 
 
maximize    g( )
subject to    
tβ
=
u
u
  
(2.10) 
where 
tβ  is the target reliability index. This is called inverse reliability analysis because 
Eq. (2.10) is the inverse problem of Eq. (2.8). The probabilistic constraint is satisfied for 
the given target reliability index 
tβ   when the performance measure function value at the 
MPP, 
*
(
)
g u
, is less than zero, i.e., 
( )
0
G
<
X
 is defined as safe. 
There are several methods that have been developed to find the MPP using 
inverse reliability analysis with the given target reliability index 
tβ ; the different methods 
are the mean value (MV) method, the advanced mean value (AMV) method [Wu et al. 
1990; Wu 1994], the hybrid mean value (HMV) method [Youn et al 2003], and the 
enhanced hybrid mean value (HMV+) method [Youn et al. 2005]. 
The MV method creates a linear approximation of the performance measure 
function using the function value and gradient information at the mean value in the 
standard normal U-space. Thus, the MV method is a crude method for finding the MPP 
 

 
 
19 
 
of the inverse reliability analysis. Since no additional function evaluations and sensitivity 
information is needed, the MV method can be a good approximation to determine which 
constraints are active. 
The AMV method uses the MPP obtained from the MV method for the first 
iteration. AMV then uses the gradient information at the MPP provided by the MV 
method to find the next candidate MPP; the iteration continues until the approximate 
MPP converges to the correct MPP. For convex performance measure functions, the 
AMV method is known to be an efficient method. 
To overcome the issues AMV has with concave functions, the HMV method was 
developed. The HMV method uses the conjugate mean value (CMV) method for concave 
performance measure functions [Youn et al. 2003], and AMV is still used for convex 
performance measure functions. The HMV+ method uses interpolation between the two 
previous MPP candidate points for concave performance measure functions instead of 
using the CMV method [Youn et al. 2005]. 
 
2.4 MPP-based RBDO 
2.4.1 MPP-Based RBDO Using FORM 
The mathematical formulation of a general RBDO problem is expressed as 
 
 
Tar
minimize      Cost( )
subject to     [
( )
0]
,
1,
,
,
and
i
i
F
L
U
nd
nr
P G
P
i
nc
>
≤
=
≤
≤
∈
∈
d
X
d
d
d
d
X



  
(2.11) 
where 
T
{
}
,
1 ~
id
i
nd
=
=
=
d
μ(X)
 is the design vector, 
T
{
}
i
X
=
X
 is the vector of 
random variables, and nc , nd , and nr  are the number of probabilistic constraints, 
 

 
 
20 
 
design variables, and random variables, respectively. Using inverse reliability analysis, 
the 
thi  probabilistic constraint can be rewritten as 
 
 
Tar
*
[
( )
0]
0
(
)
0
i
i
F
i
P G
P
G
>
−
≤
⇒
≤
X
x
  
(2.12) 
where 
*
(
)
i
G x
 is the 
thi  probabilistic constraint evaluated at the MPP, 
*x , in the X-space, 
and 
Tar
iF
P
 is the target probability of failure for the 
thi  performance measure. 
Using FORM, Eq. (2.11) can be rewritten to give 
 
 
Tar
minimize      Cost( )
subject to     [
( )
0]
(
),
1,
,
,
and
i
i
i
F
t
L
U
nd
nr
P G
P
i
nc
β
>
≤
= Φ −
=
≤
≤
∈
∈
d
X
d
d
d
d
X



  
(2.13) 
where 
itβ  is the target reliability index for the 
thi  constraint, and the probabilistic 
constraint can be written as 
 
 
*
FORM
[
( )
0]
(
)
0
(
)
0
i
i
t
i
P G
G
β
>
−Φ −
≤
⇒
≤
X
x
  
(2.14) 
where 
*
FORM
x
 is the FORM-based MPP. 
To solve the optimization problem given in Eq. (2.13), the sensitivity of the 
probabilistic constraint in Eq. (2.14) with respect to the design variables 
(
)
i
i
d
X
µ
=
 is 
required. Using the chain rule, the sensitivity of the probabilistic constraint with respect 
to the design variable can be written as 
 
 
*
*
*
*
*
T
*
1
(
)
nr
i
i
i
G
G
G
x
G
x
=
=
=
=
=
=
∂
∂
∂
∂
∂
∂


=
=
= 

∂
∂
∂
∂
∂
∂


∑
x x
x x
x x
x x
x x
x
x
d
d
d
d
x
  
(2.15) 
which can be simplified to give Eq. (2.16) [Gumbert et al. 2003; Hou et al. 2004]. 
 
 

 
 
21 
 
 
*
*
*
T
*
(
)
G
G
G
=
=
=
∂
∂
∂
∂


=
=


∂
∂
∂
∂

x x
x x
x x
x
x
d
d
x
x
  
(2.16) 
 
2.4.2 MPP-Based RBDO Using DRM 
The dimension reduction method was developed to approximate multi-
dimensional integration of a function using a function with reduced dimension [Xu and 
Rahman 2004; Rahman and Xu 2004]. The univariate dimension reduction method is a 
decomposition of an nr  dimensional performance measure function into the summation 
of one-dimensional functions. Thus, an nr  dimensional performance measure function 
( )
G X  can be decomposed into a summation of one-dimensional functions at the MPP of 
the random vector X  given as 
 
 
*
*
*
*
*
1
1
1
1
ˆ
( )
( )
(
,
,
,
,
,
,
)
(
1) (
)
nr
i
i
i
nr
i
G
G
G x
x
X x
x
nr
G
−
+
=
≅
≡
−
−
∑
X
X
x


  
(2.17) 
where 
*
*
*
T
1
2
={
, 
,
, 
}
nr
x
x
x
*x

 is the FORM-based MPP of the performance measure 
function 
( )
G X  obtained from Eq. (2.10), and nr  is the number of random variables. The 
univariate DRM gives more accurate reliability analysis results compared to FORM [Lee 
et al. 2010; Lee et al. 2008]. 
 
2.5 Sampling-Based RBDO 
The MPP-based RBDO methods, FORM, SORM, and DRM, calculate the 
probability of failure of the performance measure function using approximations. For 
highly nonlinear problems, these approximations can be inaccurate and thus give 
unreliable designs. The MPP-based RBDO methods also require the sensitivity 
information of the performance measure functions, which can be difficult if not 
 

 
 
22 
 
impossible to calculate for real-world engineering problems. Therefore, a sampling-based 
RBDO method has been developed that uses surrogate models, MCS, and score functions 
to calculate the probability of failure and the sensitivity of the probabilistic constraints in 
RBDO [Shi et al. 2012; Song et al. 2011; Zhao 2011; Zhao et al. 2011; Lee et al. 2011]. 
 
2.5.1 Sampling-Based Probability of Failure 
In sampling-based RBDO, the reliability analysis at both the component and 
system levels involves the calculation of the probability of failure, denoted by 
F
P . The 
probability of failure is calculated using a multi-dimensional integration and can be 
written as 
 
 
( )
[
]
( )
( ;
)
( )
nr
F
F
F
F
P
P
I
f
d
E I
Ω
Ω


≡
∈Ω
=
=


∫
X
ψ
X
x
x ψ
x
X

  
(2.18) 
where ψ  is a vector of the distribution parameters, which typically includes the mean, μ, 
and the standard deviation, σ , of the random input variables 
{
}
T
1,
,
nr
X
X
=
X

, [ ]
P  
represents a probability measure, 
F
Ω is the failure set, 
(
)
;
fX x ψ  is the joint probability 
density function (PDF) of X , and [ ]
E  is the expectation operator. The failure set is 
defined as 
{
}
:
( )
0
F
i
G
Ω≡
>
x
x
 for component-level reliability analysis of the 
thi  
constraint function 
( )
i
G x . For series system-level and parallel system-level reliability 
analysis of nc  performance measure functions, the failure set is 
{
}
1
:
( )
0
nc
F
i
i G
=
Ω
≡
>
x
x

 
and 
{
}
1
:
( )
0
nc
F
i
i G
=
Ω
≡
>
x
x

, respectively. 
( )
F
IΩ
x  in Eq. (2.18) is called the indicator 
function and is defined as 
 
 
1,
( )
0, otherwise
F
F
IΩ
∈Ω

≡

x
x
  
(2.19) 
 

 
 
23 
 
In this study, since the mean of the random variables X , 
{
}
T
1,
,
nr
µ
µ
=
μ

 is the design 
variable vector, the vector of distribution parameters ψ  can be replaced with μ for the 
computation of the probability of failure given in Eq. (2.18). 
 
2.5.2 Probabilistic Sensitivity Analysis 
For the derivation of the sensitivity of the probability of failure, the following four 
regularity conditions need to be satisfied [Rubinstein and Shapiro 1993; Rahman 2009; 
Lee et al. 2011; Zhao 2011]. 
1. The joint PDF 
(
)
;
fX x μ   is continuous. 
2. The mean 
,
1,
,
i
i
i
nr
µ ∈Μ ⊂
=


, where 
i
M  is an open interval on . 
3. The partial derivative 
(
)
;
/
i
f
µ
∂
∂
X x μ
 exists and is finite for all x  and 
iµ . In 
addition, 
( )
F
P
μ  is a differentiable function of μ. 
4. There exists a Lebesgue integrable dominating function, ( )
r x  for all μ such that 
 
 
( ; )
( )
( )
i
f
g
r
µ
∂
≤
∂
X x μ
x
x   
(2.20) 
With the four conditions satisfied, taking the partial derivative of Eq. (2.18) with 
respect to 
iµ  and using the interchangeability between the differential and integral 
operators, the sensitivity of the probability of failure is given as [Rahman 2009; Lee et al. 
2011] 
 
 

 
 
24 
 
 
( )
( )
( ; )
( ; )
( )
ln
( ; )
( )
( ; )
ln
( ; )
( )
nr
F
nr
F
nr
F
F
F
i
i
i
i
i
P
I
f
d
f
I
d
f
I
f
d
f
E I
µ
µ
µ
µ
µ
Ω
Ω
Ω
Ω
∂
∂
=
∂
∂
∂
=
∂
∂
=
∂


∂
=


∂


∫
∫
∫
X
X
X
X
X
μ
x
x μ
x
x μ
x
x
x μ
x
x μ
x
x μ
x



  
(2.21) 
The partial derivative of the log function of the joint PDF in Eq. (2.21) with respect to 
iµ  
is called the first-order score function for 
iµ  and is denoted as 
 
 
(1)
ln
( ; )
( ; )
.
i
i
f
sµ
µ
∂
≡
∂
X x μ
x μ
  
(2.22) 
As shown in Eq. (2.22), using the first-order score function in the proposed 
probabilistic sensitivity analysis does not depend on the sensitivity of the performance 
measure function 
( )
G x . The sensitivity of the joint input distribution is used instead and 
can be calculated analytically. This is shown in Figure 2.2 [Song 2013]; assume the 
horizontal axis represents the multi-dimensional random variable 
T
1
2
[ ,
,...,
]
nr
x x
x
=
x
 with 
the failure region for the 
th
j  constraint 
( )
j
G
x  defined as 
( )
0
j
G
>
x
. The joint input PDF 
(
)
;
fX x μ  is shown in Figure 2.2. When doing deterministic design optimization, at the 
current design point μ, the sensitivity of the constraint function 
( )
j
G
x  at point A has to 
be used. However, when doing RBDO, probabilistic constraints are used, and the 
sensitivity of the probability of failure with respect to the current design point μ is used. 
The probability of failure is shown in the figure as the volume of the grey shaded region 
under the joint input PDF 
(
)
;
fX x μ . The joint input PDF will move as the design point μ 
moves, and the rate of change of the probabilistic constraint will depend on the slope of 
the natural logarithm of the joint input PDF 
(
)
;
fX x μ  at point B as shown in Figure 2.2 
and Eq. (2.22). 
 

 
 
25 
 
 
Figure 2.2 Probabilistic Sensitivity Analysis 
For independent input random variables, the first-order score function for 
iµ  in 
Eq. (2.22) can be written as 
 
 
(1)
ln
( ;
)
ln
( ; )
( ; )
i
i
X
i
i
i
i
f
x
f
sµ
µ
µ
µ
∂
∂
≡
=
∂
∂
X x μ
x μ
  
(2.23) 
The marginal PDF and CDF are available analytically and are listed in Table 2.2 
for some common distributions. Since they are available analytically, the derivation of 
the first-order score function for independent random variables is straightforward and is 
listed in Table 2.3 [Rahman 2009; Lee et al. 2011]. 
 
 
 
 
 
µ
Failure Region
0
x
fX(x;µ)
Gj(x) < 0 •
j
F
Ω
Gj(x) > 0
•
B
A
Gj(x)
 

 
 
26 
 
Table 2.2 Marginal PDF, CDF, and Parameters 
 
PDF, 
( )
Xf
x  
CDF, 
( )
X
F
x  
Parameters 
Normal 
2
0.5[
]
1
2
x
e
µ
σ
πσ
−
−
 
x
µ
σ
−


Φ



 
,
µ σ  
Log-normal 
2
ln
0.5[
]
1
2
x
e
x
µ
σ
π σ
−
−
 
ln x
µ
σ
−


Φ



 
2
2
ln[1 (
) ]
σ
σ
µ
=
+
, 
2
ln( )
0.5
µ
µ
σ
=
−
 
Gumbel 
(
)
(
)
x
x
e
e
α
ν
α
ν
α
−
−
−
−
−
 
(
)
x
e
e
α
ν
α
−
−
−
 
0.577
µ
ν
α
=
+
,
6
π
σ
α
=
 
Weibull 
(
)
1
( )
k
x
k
v
k x
e
ν ν
−
−
 
( )
1
k
x
v
e
−
−
 
1
(1
)
v
k
µ = Γ
+
, 
2
2
2
2
1
[ (1
)
(1
)]
v
k
k
σ =
Γ
+
−Γ
+
 
Table 2.3 First-Order Score Function for 
iµ  for Independent Random Variables 
Marginal 
Distribution 
First-Order Score Function, 
(1)( ; )
i
sµ
x μ  
Normal 
2
i
i
i
x
µ
σ
−
 
Log-normal 
2
ln
1
1
(ln
)
i
i
i
i
i
i
i
i
i
i
i
i
i
i
x
x
σ
µ
µ
σ
σ
µ
σ
µ
σ
σ
µ
µ




∂
−
∂
∂
−
+
×
+
−




∂
∂
∂




 
Gumbel 
(
)
i
i
i
x
i
ie
α
ν
α
α
−
−
−
 
Weibull 
(
1)
1
1
ln
(
)
ln
ik
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
i
k
k
x
k
x
k
x
k
k
ν
ν
ν
µ
ν
µ
µ
ν
ν
µ
ν
µ
ν
ν
µ


∂
∂
∂
−
∂
∂
∂
−
+
−
−
−


∂
∂
∂
∂
∂
∂


 
 
 
For bivariate correlated input random variables 
T
{
,
}
i
j
X X
=
X
, the joint PDF of 
X  using copula functions can be expressed as [Noh et al. 2009; Noh et al. 2010; Lee et 
al. 2011] 
 
 

 
 
27 
 
 
2
,
( , ; )
( ; )
( ;
)
(
;
)
( , ; )
( ;
)
(
;
)
i
j
i
j
X
i
i
X
j
j
uv
X
i
i
X
j
j
C u v
f
f
x
f
x
C
u v
f
x
f
x
u v
θ
µ
µ
θ
µ
µ
∂
=
=
∂∂
X x μ
  (2.24) 
where C  is the copula function; 
( ;
)
i
X
i
i
u
F
x µ
=
 and 
(
;
)
j
X
j
j
v
F
x µ
=
 are marginal CDFs 
for 
i
X  and 
j
X , respectively; and θ  is the correlation coefficient between 
i
X  and 
j
X . 
The partial derivative of the copula function with respect to u  and v  is called the copula 
density function and is written as 
 
 
2
,
( , ; )
( , ; )
( , ; ).
uv
C u v
c u v
C
u v
u v
θ
θ
θ
∂
≡
=
∂∂
  
(2.25) 
Using Eq. (2.24), the first-order score function in Eq. (2.22) for bivariate correlated 
variables X  is given as 
 
 
(1)
ln
( ;
)
ln
( ; )
ln ( , ; )
( ; )
i
i
X
i
i
i
i
i
f
x
f
c u v
sµ
µ
θ
µ
µ
µ
∂
∂
∂
≡
=
+
∂
∂
∂
X x μ
x μ
  
(2.26) 
Table 2.4 lists the derivation of the first term on the right-hand side of Eq. (2.26). 
 
 
 
 
 
 
 
 
 
 
 

 
 
28 
 
Table 2.4 Log-Derivative of Copula Density Function 
Copula Type 
ln ( , ; )
i
c u v θ
µ
∂
∂
 
Clayton 
(1
)
1
(2
1)
1
i
u
u
u
u
v
θ
θ
θ
θ
θ
µ
−
+
−
−


+
+
∂
−
+


+
−
∂


 
AMH 
(
)
(
)(
)
(
)
(
)(
)
2
2
1
(
1)
3 (1
)
1
1
1
2
1
1
1
i
v
v
v
u
u
v
u
v
uv
u
v
θ
θ
θ
θ
θ
θ
µ


−
−
+
+
−
∂
−


+
−
−
−
−
−−
−
−
−
∂


 
Frank 
(
)
(1
)
(
)
(1
)
(1
)
(
)
2
1
u
u v
u
v
u v
i
e
e
u
e
e
e
e
θ
θ
θ
θ
θ
θ
θ
µ
+
+
+
+
+


−
∂


+
−
−
+
∂




 
FGM 
(
)(
)
2 (2
1)
1
1 2
1 2
i
v
u
u
v
θ
θ
µ


−
∂


+
−
−
∂


 
Gaussian 
1
1
1
1
1
2
( )
( )
( )
(
( ))
(
( ))(1
)
i
u
v
u
u
u
u
θ
φ
φ
θ
µ
−
−
−
−
−


Φ
Φ
−Φ
∂
+


Φ
Φ
−
∂


 
Independent 
0 
 
 
Thus, the sensitivity of the probability of failure in Eq. (2.21) can be easily calculated for 
bivariate correlated random variables. 
As described previously, surrogate models are used to calculate the probability of 
failure because it is computationally too expensive to carry out direct MCS of CAE 
models. If the surrogate model for the 
th
j  constraint function 
( )
j
G
x  is denoted as 
( )
ˆ
j
G
x  
then the probability of failure can be approximated as 
 
 
(
)
Tar
ˆ
1
1
( )
0
(
)
j
j
Fj
M
m
F
j
F
m
P
P G
I
P
M
Ω
=


≡
>
≅
≤


∑
x
x
  
(2.27) 
where M  is the number of MCS samples, 
(
)
m
x
 is the 
th
m  realization of X , and the 
failure region ˆ
j
F
Ω
 for the surrogate model is defined as 
ˆ
ˆ
{ :
( )
0}
j
F
j
G
Ω
≡
>
x
x
. The 
 

 
 
29 
 
sensitivity of the probabilistic constraint in Eq. (2.21) can be approximated using 
[Rahman 2009; Lee et al. 2011] 
 
 
(
)
(1)
(
)
ˆ
1
1
(
)
(
; ).
j
i
Fj
M
F
m
m
m
i
P
I
s
M
µ
µ
Ω
=
∂
≅
∂
∑
x
x
μ   
(2.28) 
The number of MCS samples and the target probability of failure controls the 
accuracy of the MCS. Based on the 95% confidence interval of the estimated probability 
of failure, the percentage error can be defined as [Haldar and Mahadevan 2000] 
 
 
Tar
Tar
(1
)
200%
F
MCS
F
P
M
P
ε
−
=
×
×
  
(2.29) 
Thus, for a small non-zero target probability of failure, the number of MCS samples used 
needs to be increased to maintain a given target accuracy level. 
This section only considered component-level probability of failure. However, as 
long as the failure set is defined appropriately for the system level, then Eqs. (2.27) and 
(2.28) can be used for system-level RBDO. The accuracy and sensitivity of the 
probability of failure depend not only on the number of MCS samples used but also on 
the accuracy of the surrogate model used. Thus, to perform confidence-based RBDO, a 
surrogate modeling method that captures the uncertainty of the probability of failure due 
to the uncertainty in the surrogate model is required. It is developed in the next chapter. 
 
 
 
 

 
 
30 
 
CHAPTER 3 
MODIFIED BAYESIAN KRIGING 
3.1 Introduction 
As previously discussed, Kriging has become a popular surrogate modeling 
choice. However, because Kriging is typically formulated to be used as an interpolation 
method and is known to break down when the response values contain noise, it is not 
ideally suited for creating surrogate models for problems that contain noise. There is a 
way to modify the correlation matrix used in Kriging so that it becomes a regression 
method, but it does not offer a convenient way to separate the noise in the response 
values from the true underlying response values. In this chapter a modified Bayesian 
Kriging (MBKG) method is developed that can handle response values that contain noise, 
so that predicted response values without noise can be obtained from the MBKG model. 
The Kriging and dynamic Kriging methods are briefly reviewed in Section 3.2. The 
proposed modified Bayesian Kriging (MBKG) method is developed in Section 3.3, and 
the full conditional distributions used to fit the MBKG surrogate model are derived in 
Section 3.4. 
 
3.2 Kriging and Dynamic Kriging Methods 
3.2.1 Kriging Method 
The Kriging method is based on the assumption that the response values of 
interest are a realization from a stochastic process. Consider N  design of experiment 
(DoE) samples, denoted as 
T
1
2
(
,
,...,
)
DoE
N
=
x
x x
x
, and the corresponding N  responses, 
denoted as 
T
1
2
(
,
,...,
)
N
y y
y
=
y
, where 
m
DoE ∈
x
 and m  is the number of input variables, 
i.e., the spatial dimension of the input variables. The Kriging model for the responses is 
composed of two parts and is expressed mathematically as 
 

 
 
31 
 
 
=
+
y
Fβ
Z   
(3.1) 
where Fβ  is the mean structure of the responses, 
[ (
)],
1,2,...,
,
1,2,...,
j
i
i
N j
K
=
=
=
F
f x
 is 
a N
K
×
 design matrix where 
(
)
j
i
f
x
 is the 
th
j  basis function evaluated at the 
thi  DoE 
sample point, and 
T
1
2
[
,
,...,
]
K
β β
β
=
β
 are the regression coefficients from the generalized 
least square regression method. The second part, Z , is a realization from a stationary 
Gaussian random process with zero mean and covariance function given by 
 
 
2
(
,
)
( ,
,
)
i
j
i
j
R
σ
Σ
=
x x
θ x x
  
(3.2) 
where 
2
σ  is the process variance, R  is the spatial correlation function, θ  is a vector 
containing the correlation function parameters, and 
ix , 
j
x  are two DoE samples [Zhao 
2011]. There are a number of different correlation functions that can be used, e.g., 
Gaussian, exponential, general exponential, linear, spherical, cubic, and spline. However, 
for engineering problems, the Gaussian correlation function is commonly used since it is 
infinitely differentiable and provides a smooth response surface. The parameters of the 
Kriging model that best fit the DoE samples and corresponding response values are 
determined by using the maximum likelihood estimation (MLE) method. 
 
3.2.2 Dynamic Kriging Method 
As previously mentioned, there are a number of different correlation functions to 
choose from, and there are also numerous basis functions to choose from. It has been 
shown that for different problems and data some correlation functions fit better than 
others; it has also been shown that a larger basis function set is not necessarily better than 
a smaller basis function set [Song et al. 2013; Zhao 2011; Zhao et al. 2011]. The latest 
proposed method chooses the mean structure from a choice of three mean structures and 
the correlation function from seven choices that best fit the data [Song et al. 2013]. This 
 

 
 
32 
 
dynamic Kriging method provides more flexibility and an automated way to build a 
Kriging model that can fit data from a wide range of problems. However, it is an 
interpolation method and breaks down when the response values contain noise. 
 
3.3 Modified Bayesian Kriging (MBKG) 
3.3.1 Modified Bayesian Kriging Formulation 
The basic assumption of Kriging is that the response values are realizations from 
a Gaussian random process. Thus, fitting a Kriging model is ideally suited for use with 
Bayesian statistical methods. Kriging is commonly referred to as a Bayesian method, 
though it is typically fitted using MLE methods rather than Bayesian methods. Fitting a 
Kriging model using Bayesian methods is ideal because the assumption is that the 
response values are realizations from a Gaussian random process. As described in Section 
1.2, the first part of a Bayesian analysis is determining the likelihood for the data. Once 
the likelihood is known, the next step is deciding the prior distributions to be used for the 
unknown parameters in the likelihood. Then, using the likelihood and prior distributions, 
the Bayesian analysis can be carried out to determine the posterior distribution of the 
unknown parameters. 
The modified Bayesian Kriging method assumes the response values come from a 
stationary Gaussian random process with a constant mean structure in the following form: 
 
 
(
)
2
~
,
c
MVN µ
σ λ
+
 
y
1
φ
I   
(3.3) 
where y  is the vector of response values at the DoE sample points. The above 
formulation states that the response values are conditionally independent given 
parameters defining a variance of 
2
σ λ  and a mean value of 
c
µ
+
1
φ, which is composed 
of two parts; the first part is a constant value of 
c
µ , and the second part is φ , which 
 

 
 
33 
 
depends on the spatial correlation of the DoE samples x . The second assumption is that 
φ  also follows a Gaussian random process with a zero mean and a covariance matrix that 
depends on the spatial correlation of the DoE samples x ; this is expressed 
mathematically as 
 
 
(
)
2
~
,
MVN
σ 
φ
0
Ψ   
(3.4) 
where 
2
σ Ψ  is the covariance of the Gaussian process, 
2
σ  is the variance of the φ  
values, and Ψ is the spatial correlation matrix. Similar to the Kriging method, the spatial 
correlation matrix is a function of the DoE samples x  via the correlation function that is 
used. As described in Section 3.2.1, there are a number of different correlation functions 
that can be used, and the Gaussian correlation function is the most commonly used for 
engineering problems and is defined as 
 
 
2
( )
( )
1
exp
k
i
i
j
j
j
j
x
x
θ
=


Ψ
=
−
−




∑
  
(3.5) 
where 
j
θ  is the 
th
j  correlation function parameter corresponding to the 
th
j  dimension, k  
is the number of spatial dimensions, 
jx  is the value of the DoE sample for the 
th
j  
dimension, and superscript i  is for the 
thi  DoE sample. 
In this study, seven different correlation functions from the literature are 
considered and are listed in Table 3.1 [Song et al. 2013; Lophaven et al. 2002]. 
 
 
 
 
 
 

 
 
34 
 
Table 3.1 Correlation Functions 
Name 
( )i
Ψ
  
Gaussian 
( )
1
exp
j
k
p
i
j
j
j
j
x
x
θ
=


−
−




∑
 
Exponential 
(
)
( )
exp
i
j
j
j
x
x
θ
−
−
 
General Exponential 
(
)
1
( )
1
exp
, 0
2
n
i
j
j
j
n
x
x
θ
θ
θ
+
+
−
−
 <
≤
 
Linear 
{
}
( )
max 0,1
i
j
j
j
x
x
θ
−
−
 
Spherical 
{
}
3
( )
1 1.5
0.5
,
min 1,
i
j
j
j
j
j
j
x
x
ξ
ξ
ξ
θ
−
+
 
=
−
 
Cubic 
{
}
2
3
( )
1
3
2
,
min 1,
i
j
j
j
j
j
j
x
x
ξ
ξ
ξ
θ
−
+
 
=
−
 
Spline 
2
3
3
( )
1 15
30
,
for 0
0.2
1.25(1
) ,
for 0
1
0,
for
1,
where
j
j
j
j
j
j
i
j
j
j
j
x
x
ξ
ξ
ξ
ξ
ξ
ξ
ξ
θ
−
+
 ≤
≤

−
 <
<


 
≥

 
=
−
 
 
 
The unknown MBKG parameters that need to be determined in order to fit the 
model to the data, i.e., fit the model to the DoE samples x  and their corresponding 
response values y , are 
c
µ , 
2
σ , λ , and θ . The dimension of the θ  vector is the same as 
the spatial dimension of x . From the Bayesian perspective, the likelihood of the response 
values is the Gaussian process given in Eq. (3.3), and the prior for the φ  vector is the 
Gaussian process given in Eq. (3.4). The prior distributions for the remaining unknown 
parameters are presented in the next section. 
The modified Bayesian Kriging method can also be formulated to have a mean 
structure that is a function of the DoE samples x . For this formulation Eq. (3.3) is 
modified to give: 
 
 

 
 
35 
 
 
(
)
2
~
,
MVN
σ λ
+
 
y
μ
φ
I   
(3.6) 
 
where 
c
µ 1  is replaced by the vector μ, which is a function of the DoE sample points. 
The μ vector is expressed as: 
 
 
=
μ
Fβ   
(3.7) 
where F  is the design matrix composed of the polynomial basis functions evaluated at 
the DoE sample points. For first order the design matrix F  is expressed as 
 
 
(1)
(1)
1
(2)
(2)
1
(
)
(
)
1
(
1)
1
1
1
n
n
m
m
n
m
n
x
x
x
x
x
x
×
+






= 







F





  
(3.8) 
where m  is the number of DoE and n  is the number of variables, i.e., the dimension of 
the problem, and β  is a vector of unknown coefficients to be determined when fitting the 
MBKG surrogate model. 
 
3.3.2 Prior Distributions for MBKG Parameters 
For the three unknown parameters 
c
µ , 
2
σ , and λ , semi-conjugate prior 
distributions are used to help improve the efficiency of fitting the MBKG surrogate 
model. For the 
c
µ  parameter, the conjugate prior distribution is a normal distribution and 
is expressed as 
 
 
(
)
2
~
,
c
p
p
N
µ
µ σ
  
(3.9) 
 

 
 
36 
 
where 
p
µ  and 
2
p
σ  are the prior mean and variance of the 
c
µ  parameter, respectively. The 
conjugate prior distribution for the 
2
σ  parameter is an Inverse-Gamma distribution and is 
expressed as 
 
 
(
)
2 ~
,
InverseGamma
σ
σ
σ
α
β
  
(3.10) 
where 
σ
α  and 
σ
β  are the prior parameters for the distribution. The conjugate prior for 
the λ  parameter is also an Inverse-Gamma distribution and is expressed as 
 
 
(
)
~
,
InverseGamma
λ
λ
λ
α
β
  
(3.11) 
where 
λ
α  and 
λ
β  are the prior parameters for the distribution. The θ  parameters are 
embedded in the correlation matrix, and because of this there is no known conjugate 
distribution type that can be used for the prior distribution. Thus, the prior distribution for 
each of the θ  parameters is chosen to be a uniform distribution and is expressed as 
 
 
(
)
~ U
,b
j
j
j
aθ
θ
θ
  
(3.12) 
where 
j
θ  is the 
th
j  correlation function parameter, and 
j
aθ  and 
j
bθ  are the prior 
parameters for 
j
θ . 
If the mean structure is not constant, then the μ vector is used as in Eq. (3.6) and 
the conjugate prior is a multivariate normal distribution and is expressed as 
 
 
(
)
~
,
MVN
β
β
β
μ
Σ
  
(3.13) 
where 
β
μ  and 
β
Σ  are the prior mean vector and covariance matrix for the distribution. 
Prior to fitting the MBKG model, the response values y  are normalized such that 
they have a zero mean. The normalization is expressed as 
 
 

 
 
37 
 
 
( )
( )
i
i
y
mean
y
std
−
=
y
y



  
(3.14) 
where 
iy  is the normalized response value of the 
thi  response, 
iy is the un-normalized 
response value for the 
thi  response, 
( )
mean y is the mean of the un-normalized response 
values, and 
( )
std y is the standard deviation of the un-normalized response value. The 
DoE samples are also normalized in the same way as the response values. In the literature 
it is has been found that, when fitting surrogate models a better fit can usually be 
achieved when the DoE samples and response values are normalized in some way to help 
restrict the range of the possible values than when they are not normalized [Forrester et 
al. 2008]. 
In addition to the normal distribution being a conjugate prior for the 
c
µ  
parameter, it is also a good choice because of the normalization of the response values. It 
is expected that the overall mean of the normalized response values should be close to 
zero. Thus, a normal distribution with a zero mean and a standard deviation of 0.5 is a 
prior distribution that reflects this knowledge. This prior distribution can still be 
considered a relatively noninformative prior distribution; thus, it can be used in general 
when creating a surrogate model for any problem. 
The Inverse-Gamma distribution has positive support, meaning that the random 
variable of an Inverse-Gamma distribution can only take on positive values. From Eqs. 
(3.3) and (3.4) it can be seen that both parameters 
2
σ  and λ  are variance parameters to a 
Gaussian process; therefore, by definition they can only take on positive values. With the 
Inverse-Gamma distribution being a positive value distribution and a conjugate prior for 
both 
2
σ  and λ , it is ideally suited to be used as the prior distribution. 
As mentioned previously, there is no known conjugate prior distribution for the θ  
parameters. Thus, a uniform distribution was chosen mostly because it gives a direct way 
to limit the possible values that the θ  parameters can take on. Due to the normalization of 
 

 
 
38 
 
the DoE samples, the maximum distance in each of the spatial directions from any given 
DoE sample to all the others is roughly three. After studying the Gaussian correlation 
function closely, it was found that a θ  value of five and greater produces a correlation 
value that is asymptotically approaching zero regardless of the distance between the DoE 
samples. This is shown in Figure 3.1; the vertical axis is the squared distance between 
DoE sample points, and the horizontal axis is the correlation function parameter θ . As 
seen in the figure, when 
2.5
θ ≈
 and the squared distance is greater than one, the 
correlation is smaller than 0.1. Also seen in the figure is that when 
5
θ =
, the correlation 
is smaller than 0.1 except for squared distances that are approximately less than 0.5. 
Given this study, it was determined that an appropriate upper bound for θ  would be 
5
θ =
 and an appropriate lower bound would be 
0.15
θ =
; these are the bounds used in 
the uniform prior distribution. 
 
Figure 3.1 Correlation Contour of the Gaussian Correlation Function 
 

 
 
39 
 
Once the likelihood and prior distributions have been determined, the next step in 
a Bayesian analysis that uses the Gibbs sampling algorithm [Gelfand and Smith 1990] for 
Markov Chain Monte Carlo (MCMC) is to determine the full conditional distributions for 
the unknown parameters. The next section will show the derivation of the full conditional 
distributions. 
 
3.4 Full Conditional Distributions for MBKG Parameters 
3.4.1 Joint Distribution of the MBKG Parameters 
The joint posterior distribution of the unknown parameters in a Bayesian model is 
needed to derive the full conditional distributions of the unknown parameters being 
estimated. The full conditional distribution for the unknown parameter is derived from 
the joint distribution such that the full conditional distribution is only a function of the 
unknown parameters that the parameter of interest depends on. For the likelihood and 
prior distributions that were presented in Section 3.3, the joint posterior distribution of 
the MBKG surrogate model for the constant mean structure is the product of the 
likelihood and all the prior distributions, which is given as 
 
 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
2
1
2
2
2
,
, , ,
|
U
,b
*
,
*
,
*
,
*
,
*
,
j
j
k
c
j
p
p
f
a
InverseGamma
N
InverseGamma
MVN
MVN
θ
θ
σ
σ
λ
λ
µ σ
λ
α
β
µ σ
α
β
σ
σ λ
=


∝


+
∏
θ
φ y
0
Ψ
μ
φ
I
  
(3.15) 
The joint posterior distribution of the MBKG surrogate model considering the mean 
structure is expressed as 
 
 

 
 
40 
 
 
(
)
(
)
(
)
(
)
(
)
(
)
(
)
2
1
2
2
,
, , ,
|
U
,b
*
,
*
,
*
,
*
,
*
,
j
j
k
j
f
a
InverseGamma
MVN
InverseGamma
MVN
MVN
θ
θ
σ
σ
β
β
λ
λ
σ
λ
α
β
α
β
σ
σ λ
=


∝


+
∏
μ
θ
φ y
μ
Σ
0
Ψ
μ
φ
I
  
(3.16) 
Substituting in the mathematical expression for each of the distributions in Eq. (3.15) 
gives the joint distribution as a function of the unknown parameters and the prior 
parameters and is given in Eq. (3.17). 
The Bayesian model proposed for the MBKG surrogate modeling method is not a 
new Bayesian model. The full conditional distributions of this Bayesian model have been 
previously derived by others. However, the use of this Bayesian model as a surrogate 
model is a new concept that has not previously been done. The methods developed in this 
study have been coded in MATLAB so that they can be easily integrated with existing 
RBDO methods that were coded in MATLAB. In order to code the Gibbs sampling 
algorithm needed to fit the MBKG surrogate model using MCMC, the full conditional 
distributions are needed. For this reason they have been derived again and are presented 
in this study for completeness. To derive the full conditional distribution for a parameter 
of interest, only the terms from the joint distribution that contains the parameter of 
interest are used. The full conditional distribution of the parameter of interest is 
proportional to the terms taken from the joint distribution. The next sections derive the 
full conditional distributions for each of the MBKG parameters. Note that the derivations 
in the next sections only show partial steps of the derivation. 
 
 

 
 
41 
 
 
(
)
(
)
(
)
(
)
(
) (
)
2
1
(
1)
2
2
2
2
(
1)
1/2
/2
1
1/2
/2
2
1
2
1
,
, , ,
|
1
*
(
)
exp
(
)
1
1
*
exp
2
2
1
*
exp
(
)
1
* 2
exp
2
* 2
1
*exp
2
j
j
k
c
j
c
p
p
p
d
T
d
T
f
b
a
σ
σ
λ
λ
θ
θ
α
α
σ
σ
σ
α
α
λ
λ
λ
µ σ
λ
β
σ
β
α
σ
µ
µ
σ
πσ
β
λ
β
α
λ
π
π
σ λ
σ λ
=
−
+
−
+
−
−
−
−
−
−


∝


−






−
⋅


Γ




−
−








−
⋅


Γ




−




−
−
−
∏
θ
φ y
Σ
φ Σ φ
I
y
μ
φ
I
y(
)


−
−




μ
φ
  
(3.17) 
 
3.4.2 Full Conditional for 
c
µ  
Taking only the terms that depend on 
c
µ  from the joint distribution in Eq. (3.17) 
gives the terms that are a function of the full conditional for 
c
µ , which gives 
 
 
(
)
(
)
(
) (
) (
)
2
2
2
1
2
1
| ,
, , ,
exp
2
1
*exp
2
c
c
p
p
T
f µ
σ
λ
µ
µ
σ
σ λ
−


∝
−
−








−
−
−
−
−




y
θ
φ
y
μ
φ
I
y
μ
φ
  
(3.18) 
Using index notation and grouping terms that are a function 
c
µ  in Eq. (3.18), the right-
hand side can be written to give Eq. (3.19). Note that the full conditional in Eq. (3.18) is 
written as proportional to, so that constant terms, i.e., terms that are not a function of 
c
µ , 
can be dropped from the equation. 
 
 

 
 
42 
 
 
(
)
(
)
(
)
2
constan
2
2
2
2
t
1
1
1
exp
exp
2
2
2
2
1
1
exp
2
2
c
p
i ij
j
c
i ij
j
c
i ij
j
i
i ij
j
c
p
ij
j
i ij
j
I I I
I I
y I
I
I
I
y I y
y
µ
µ
µ
ϕ µ
µ
σ
σ λ
ϕ
ϕ
ϕ
σ λ




∝
−
−
−
+
−





−





+





−


  (3.19) 
After removing the constant term and defining the intermediate variables A  and B , Eq. 
(3.19) can be further simplified to give 
 
 
(
)

(
)
2
2
2
2
1
1
1
exp
exp
2
2
2
i ij
j
i ij
j
i ij
j
A
B
c
p
c
c
p
I I I
y I I
I I
µ
µ
µ
µ
σ
σ λ
ϕ










∝
−
−
−
+ −













−


  (3.20) 
Precision is defined as the inverse of the variance so that prior precision and unknown 
precision can be expressed as 
 
 
2
2
2
2
1
1
p
p
τ
τ
σ
σ λ
=
=
  
(3.21) 
Rewriting Eq. (3.20) in terms of precision and the intermediate variables A  and B  gives 
 
 
(
)
(
)
2
2
2
2
2
1
1
exp
2
exp
2
2
2
p
c
p
c
p
c
c
A
B
τ
µ
µ µ
µ
τ
µ
µ




∝
−
−
+
−
+ −








  
(3.22) 
Once again terms that depend on 
c
µ  are grouped together, and constant terms are 
dropped from the expression so that Eq. (3.22) is further simplified to give 
 
 
(
)
(
)
(
)
2
2
2
2
2
2
2
1
exp
2
2
p
p
p
c
c
p
B
A
A
τ µ
τ
τ
τ
µ
µ
τ
τ




+




∝
−
+
−




+




  
(3.23) 
The final step is then completing the square and dropping constant terms from the 
expression so that Eq. (3.23) is simplified to give 
 

 
 
43 
 
 
(
)
(
)
(
)
2
2
2
2
2
2
2
1
exp
2
p
p
p
c
p
B
A
A
τ µ
τ
τ
τ
µ
τ
τ




+




∝
−
+
−




+




  
(3.24) 
Equation (3.24) can be recognized as a kernel of a normal distribution that has a mean 
value of 
 
 
(
)
(
)
2
2
2
2
p
p
p
B
A
τ µ
τ
τ
τ
+
+
  
(3.25) 
The precision and variance of this normal distribution can be seen to be 
 
 
(
)
(
)
2
2
2
2
1
precision
var
p
p
A
A
τ
τ
τ
τ
=
+
⇒
=
+
  
(3.26) 
The intermediate variables are defined as 
 
 
2
2
2
2
1
1
i ij
j
i ij
j
i ij
j
p
p
A
I I I
B
y I I
I I ϕ
τ
τ
σ
σ λ
=
=
−
=
   
=
  
(3.27) 
The full conditional distribution of 
c
µ  written in terms of the precision, intermediate 
variables, prior parameters, and MBKG parameters can be expressed as 
 
 
(
)
(
)
(
)
(
)
2
2
2
2
2
2
2
| ,
, , ,
~
,
p
p
c
p
p
B
f
N
A
A
τ µ
τ
µ
σ
λ
τ
τ
τ
τ


+


 
+


+


y
θ
φ
  
(3.28) 
 
 

 
 
44 
 
3.4.3 Full Conditional for 
2
σ  
Taking only the terms that depend on 
2
σ  from the joint distribution in Eq. (3.17) 
gives the terms that are a function of the full conditional for 
2
σ , which gives 
 
 
(
)
(
) (
) (
)
1/2
(
1)
2
2
1
2
1
1/2
2
2
1
1
| ,
, , ,
(
)
exp
exp
2
1
*
exp
2
T
c
T
f
σ
α
σ
σ
µ
λ
σ
β
σ
σ λ
σ λ
−
−
+
−
−
−




∝
−
⋅
−










−
−
−
−
−




y
θ
φ
Σ
φ Σ φ
I
y
μ
φ
I
y
μ
φ
  (3.29) 
After grouping 
2
σ  terms and constant terms, the right-hand side of Eq. (3.29) can be 
rewritten to give 
 
 
(
)
(
)
(
) (
) (
)
(
)
/2
/2
(
1)
2
2
2
2
1
2
1/2
1/2
/2
constant
1
2
1
(
)
exp
1
*exp
2
1
*exp
2
*
n
n
T
T
n
σ
α
σ
σ
σ
σ
β
σ
σ
λ
λ
σ
−
−
−
−
+
−
−
−
−


∝
−
⋅






−
−
−
−
−






−




y
μ
φ
I
y
μ
Ψ
I
φ
φ
Ψ
φ

  
(3.30) 
Dropping the constant terms and further simplifying Eq. (3.30) can finally be written as 
 
 
(
) (
)
(
1)
2
1
2
1
1 1
1
(
)
exp
2
2
T
n
T
σ
α
σ
σ
β
λ
σ
−
+ +
−




∝
−
+
+
−
−
−
−
⋅








φ Ψ φ
y
μ
φ
y
μ
φ
  (3.31) 
Equation (3.31) can be identified as the kernel of an Inverse-Gamma distribution with the 
first parameter of the distribution given as 
n
σ
α +
 and the second parameter of the 
distribution given as 
 
 
(
) (
)
1
1
1 1
2
2
T
T
σ
β
λ
−
+
+
−
−
−
−
φ Ψ φ
y
μ
φ
y
μ
φ   
(3.32) 
 

 
 
45 
 
The full conditional distribution of 
2
σ  written in terms of the prior parameters and 
MBKG parameters can be expressed as 
 
 
(
)
(
) (
)
2
1
| ,
, , ,
~
1
1 1
,
2
2
c
T
T
f
InverseGamma
n
σ
σ
σ
µ
λ
α
β
λ
−


+   
+
+
−
−
−
−




y
θ
φ
φ Ψ φ
y
μ
φ
y
μ
φ
  (3.33) 
 
3.4.4 Full Conditional for λ  
Taking only the terms that depend on λ  from the joint distribution in Eq. (3.17) 
gives the terms that are a function of the full conditional for λ , which gives 
 
 
(
)
(
) (
) (
)
1/2
(
1)
2
2
1
2
1
| ,
,
, ,
exp
1
*exp
2
c
T
f
λ
α
λ
λ
µ σ
λ
β
σ λ
λ
σ λ
−
−
+
−


∝
−
⋅






−
−
−
−
−




y
θ φ
I
y
μ
φ
I
y
μ
φ
  
(3.34) 
Grouping terms that are a function of λ  and dropping constant terms, the right-hand side 
of Eq. (3.34) can be rewritten to give 
 
 
(
) (
)
(
1)
2
2
1 1
1
exp
2
n
T
λ
α
λ
λ
β
σ
λ
−
+
+




∝
−
+
−
−
−
−
⋅








y
μ
φ
y
μ
φ
  
(3.35) 
Equation (3.35) can be identified to be a kernel to an Inverse-Gamma distribution with 
the first parameter given as 
/ 2
n
λ
α +
 and the second parameter given as 
 
 
(
) (
)
2
1 1
2
T
λ
β
σ
+
−
−
−
−
y
μ
φ
y
μ
φ   
(3.36) 
 

 
 
46 
 
The full conditional distribution of λ  as a function of the prior parameters and MBKG 
parameters can be expressed as 
 
 
(
)
(
) (
)
2
2
| ,
,
, ,
~
1 1
,
2
2
c
T
f
n
InverseGamma
λ
λ
λ
µ σ
α
β
σ


+
  
+
−
−
−
−




y
θ φ
y
μ
φ
y
μ
φ
  
(3.37) 
 
3.4.5 Full Conditional for 
j
θ  
Taking only the terms that depend on 
j
θ  from the joint distribution in Eq. (3.17) 
gives the terms that are a function of the full conditional for 
j
θ , which gives 
 
 
(
)
1/2
2
1
1
| ,
,
,
, ,
exp
2
T
j
c
j
f θ
µ σ
λ
−
−
−


∝
−




y
θ
φ
Σ
φ Σ φ   
(3.38) 
The covariance matrix Σ  in Eq. (3.38) is calculated using the correlation matrix Ψ, 
2
σ
=
Σ
Ψ , and the correlation matrix depends on the correlation function parameter 
j
θ . 
There is no known distribution type in which the random variable, i.e., 
j
θ  in this case, 
appears in this form. To draw samples from this nonstandard distribution, the Metropolis-
Hastings algorithm [Metropolis et al. 1953; Hasting 1970] will be used. For numerical 
stability, the natural logarithm of the full conditional is used with the Metropolis-
Hastings algorithm. The natural logarithm of Eq. (3.38) can be written as 
 
 
(
)
2
2
1
| ,
,
,
, ,
2
1
1
2
2
j
c
j
T
n
Ln f
Ln
Ln
θ
µ σ
λ
σ
−
−




∝−






−

+ −






y
θ
φ
Ψ
φ Σ φ
  
(3.39) 
 
 

 
 
47 
 
3.4.6 Full Conditional for φ  
Taking only the terms that depend on φ  from the joint distribution in Eq. (3.17) 
gives the terms that are a function of the full conditional for φ , which gives 
 
 
(
)
(
) (
) (
)
2
1
1
2
1
| ,
,
, ,
exp
2
1
*exp
2
T
c
T
f
µ σ
λ
σ λ
−
−


∝
−






−
−
−
−
−




φ y
θ
φ Σ φ
y
μ
φ
I
y
μ
φ
  
(3.40) 
After grouping and rearranging terms that are a function of φ  and dropping constant 
terms, the right-hand side of Eq. (3.40) can be written as 
 
 
(
)
(
)
(
)
1
1
2
2
1
2
exp
2
T
T
T
T
σ
σ
λ
λ
−
−





−
−



∝
−
−











+

A
b
μ
y
φ
Σ
I
φ
φ


  
(3.41) 
The intermediate variables A  and b are defined in Eq. (3.41), which then simplifies to 
 
 
1
exp
2
T




∝
−
−






φ Aφ
bφ
  
(3.42) 
After completing the square and dropping constant terms, Eq. (3.42) can be expressed as 
 
(
)
(
)
1
exp
2
T


∝
−
−
−




φ
γ
A φ
γ
  
(3.43) 
where the intermediate variable, γ  is defined as 
 
 
1
1
2
−
=
γ
A b   
(3.44) 
From Eq. (3.41) the intermediate variables A  and b are defined as 
 
 

 
 
48 
 
 
(
)
(
)
1
1
2
2
2
σ λ
σ λ
−
−
=
+
−
=
−
A
Σ
I
b
μ
y
  
(3.45) 
Equation (3.43) is identified as a kernel to a multivariate normal distribution with the 
mean vector γ  and the covariance matrix 
1
−
A ; note that Eq. (3.43) is written in terms of 
the precision and is hence the reason the covariance matrix is 
1
−
A  and not A . 
Finally the full conditional of φ  as a function of the intermediate variables, prior 
parameters, and MBKG parameters can be expressed as 
 
 
(
)
(
)
2
1
| ,
,
, ,
~
,
c
f
MVN
µ σ
λ
−
φ y
θ
γ A
  
(3.46) 
 
3.4.7 Full Conditional for β  
Taking only the terms that depend on β  in Eq. (3.16) gives the terms that are a 
function of the full conditional for β , which gives 
 
 
(
)
(
) (
) (
)
(
) (
) (
)
1
2
1
2
1
| ,
, , ,
exp
2
1
*exp
2
T
T
f
β
β
β
σ
λ
σ λ
−
−


∝
−
−
−






−
−
−
−
−




β y
θ φ
β
μ
Σ
β
μ
y
μ
φ
I
y
μ
φ
  
(3.47) 
After grouping and rearranging terms that are a function of β  and dropping constant 
terms, the right-hand side of Eq. (3.47) can be written as 
 
 
(
)
(
) (
)
(
)
(
)
(
)
1
1
2
1
exp
2
2
1
1
*exp
2
2
T
T
T
T
T
β
β
β
σ λ
−
−


∝
−
−






−
−
+




β
Σ
β
μ
Σ
β
φ
y
μ
μ μ
  
(3.48) 
 

 
 
49 
 
Substituting in the value Fβ  for μ and rearranging gives 
 
 
(
)
(
) (
)
(
)
1
2
1
2
1
1
exp
2
1
2
T
T
T
T
T
T
β
β
β
σ λ
σ λ
−
−






∝
−
+


















−
−
−








A
b
β
F F
Σ
β
μ
Σ
φ
y
F β


  
(3.49) 
The intermediate variables A  and b are defined in Eq. (3.49), which then simplifies to 
 
1
exp
2
T
T




∝
−
−






β Aβ
b β
  
(3.50) 
After completing the square and dropping constant terms, Eq. (3.50) can be expressed as 
 
 
(
)
(
)
1
exp
2
T




∝
−
−
−






β
γ
A β
γ
  
(3.51) 
where the intermediate variable γ  is defined as 
 
 
1
1
2
−
=
γ
A b   
(3.52) 
From Eq. (3.49) the intermediate variables A  and b are defined as 
 
 
(
)
(
) (
)
(
)
1
2
1
2
1
1
2
T
T
T
T
β
β
β
σ λ
σ λ
−
−


=
+






=
−
−




A
F F
Σ
b
μ
Σ
φ
y
F
  
(3.53) 
 

 
 
50 
 
Equation (3.51) is identified as a kernel to a multivariate normal distribution with the 
mean vector γ  and the covariance matrix 
1
−
A ; note that Eq. (3.51) is written in terms of 
the precision and is hence the reason the covariance matrix is 
1
−
A  and not A . 
Finally, the full conditional of β  as a function of the intermediate variables, prior 
parameters, and MBKG parameters can be expressed as 
 
 
(
)
(
)
2
1
| ,
, , ,
~
,
f
MVN
σ
λ
−
β y
θ φ
γ A
  
(3.54) 
The next chapter will present examples fitting a MBKG surrogate model to some 
example problems. 
 
 
 
 

 
 
51 
 
CHAPTER 4 
APPLICATIONS OF MODIFIED BAYESIAN KRIGING (MBKG) 
4.1 Introduction 
This chapter presents fitting a modified Bayesian Kriging (MBKG) surrogate 
model to three different mathematical examples. The first example, in Section 4.2, 
presents fitting an MBKG surrogate model to a simple one-dimensional problem whose 
response contains noise. The section discusses the details of assessing the convergence of 
the Markov chain Monte Carlo (MCMC) analysis carried out to fit the MBKG surrogate 
model. Section 4.3 presents two examples, each fitting a MBKG surrogate model to a 
two-dimensional problem whose response contains noise. 
 
4.2 One-Dimensional Quadratic Mathematical Example 
This section demonstrates fitting a one-dimensional quadratic function that 
contains noise using the MBKG surrogate modeling method. The true quadratic function 
without noise is defined as 
 
 
2
( )
0.6
3
4
f x
x
x
=
−
+
  
(4.1) 
where x  is the variable. For this mathematical example, the noise is generated using a 
normal distribution with zero mean and a standard deviation of 0.1. The noise, ε , is 
generated as realizations from the distribution, which is expressed as 
 
 
(
)
2
~
0,0.1
N
ε
  
(4.2) 
The response values with noise are then calculated as a summation of the true 
response value without noise given by Eq. (4.1) and a realization of ε . This is expressed 
as 
 

 
 
52 
 
 
( )
i
i
i
y
f x
ε
=
+
  
(4.3) 
where 
iy  is the 
thi  response value with noise, 
ix  is the 
thi  DoE sample point, and 
iε  is 
the 
thi  realization of noise. 
The red curve in Figure 4.1 shows a plot of Eq. (4.1). For this example, 25 DoE 
samples are used for fitting the surrogate model. To generate the uniformly spaced and 
randomly generated 25 DoE samples, the Latinized Centroidal Voronoi Tessellation 
(LCVT) [Burkardt et al. 2002; Saka et al. 2006] method was used. The responses for the 
25 DoE samples were generated using Eq. (4.1) with realizations taken from Eq. (4.2). 
The 25 response values that contain noise are shown as the black asterisks in Figure 4.1. 
The 25 DoE samples and responses with noise are first fitted using ordinary 
Kriging to demonstrate how Kriging is not well suited for use with response values that 
contain noise. Figure 4.2 shows a plot of Eq. (4.1) as the red curve, the 25 response values 
calculated using Eq. (4.3) are shown as the black asterisks, and the ordinary Kriging 
surrogate model is shown as the light purple curve. As seen in the figure, the ordinary 
Kriging surrogate model passes through the 25 DoE samples because Kriging is an 
interpolation method. 
The ordinary Kriging model shown in Figure 4.2 will clearly not provide accurate 
predictions of the response values of the true underlying function, which does not contain 
noise. In order to carry out RBDO, a regression surrogate model is needed to capture the 
underlying true function without noise. In addition, a surrogate model that can generate 
either confidence intervals or credible sets that contain the uncertainty of the surrogate 
model is needed in order to perform confidence-based RBDO. This is why, in this study, 
an MBKG surrogate modeling method was developed; it will be used to predict the true 
function without noise and also provides credible sets that can be used for confidence-
based RBDO. 
 

 
 
53 
 
 
Figure 4.1 Eq. (4.1) without Noise and 25 DoE Samples 
 
Figure 4.2 Ordinary Kriging Fit of Eq. (4.1) Using 25 DoE 
Recall from Chapter 3 that, when carrying out a Bayesian analysis, the prior 
parameters for the prior distributions of the unknown parameters are needed. For the 
 

 
 
54 
 
example in this section, Table 4.1 lists the prior parameters used for fitting the MBKG 
surrogate model. As described in Section 3.3.2, the response values are normalized such 
that they have a zero mean. This implies that the MBKG 
c
µ  parameter should have a 
mean value close to zero. For this reason the prior parameters for the 
c
µ  prior 
distribution were chosen so that the prior has a mean value of zero. The standard 
deviation was chosen so that the 95% interval given by the prior distribution would be 
about ±1. This prior reflects that the subjective probability of the 
c
µ  parameter being in 
this interval is 95%. From previous experience of fitting ordinary Kriging models to 
mathematical examples, the 
2
σ  parameter has typically been seen to take on values 
between about five and 20. Thus, a prior distribution that has a 95% interval with a lower 
bound of about five and an upper bound of about 20 seemed to be a reasonable prior. For 
the problems being considered, it is assumed that the variance of the noise in the response 
is much smaller than the spatial variance of the response data. With this assumption the 
λ  parameter would be between zero and one. The prior for the λ  parameter was chosen 
to reflect this assumption. However, the prior being used does not restrict the value of λ  
from being greater than one. This makes it possible for the value of λ  to take on values 
larger than one should the data provide information where this is the case. Recall from 
Section 3.3.2 that a uniform prior with lower and upper bounds of 0.15 and 5, 
respectively, was chosen for the θ  parameter. These values were chosen based on a study 
done about the relationship between the correlation value, the distance between DoE 
samples, and the θ  parameter for the Gaussian correlation function. The reader is 
referred back to Section 3.3.2 for the details. 
 
 
 
 
 
 

 
 
55 
 
Table 4.1 Prior Parameter Values 
Unknown 
Parameter 
Prior 
Distribution 
Parameter 1 
Parameter 2 
95% Interval 
c
µ  
Normal 
0
p
µ =
 
2
0.5
p
σ
=
 
1.39
−
 
1.39 
2
σ  
Inverse-Gamma 
8.25
σ
α =
 
72.5
σ
β =
 
4.91 
20.04  
λ  
Inverse-Gamma 
1.9548
λ
α =
 
0.2747
λ
β =
 
0.05 
1.20 
θ  
Uniform 
0.15
aθ =
 
5
bθ =
 
0.27  
4.88  
 
 
To fit the MBKG surrogate model, Markov chain Monte Carlo (MCMC) is used. 
As described in Section 1.2.3, when using MCMC it is desirable to run at least three 
parallel Markov chains so that they can be used with the Brooks, Gelman, and Rubin 
(BGR) diagnostic [Gelman and Rubin 1992; Brooks and Gelman 1998] to help assess if 
convergence in drawing samples from the posterior distribution has been achieved. Also 
recall that, when doing this, the three parallel Markov chains should be started at over-
dispersed initial values. For this example, three parallel chains are used, and the initial 
values for the Markov chains are taken from the prior distributions such that the three 
starting points come from the left tail, right tail, and the middle of the prior distribution. 
This is done in an effort to help make sure that the samples drawn cover the entire 
posterior distribution so that no modes of the posterior distribution are missed. The initial 
values for the Markov chains used for this example are given in Table 4.2. 
 
 
 
 
 
 

 
 
56 
 
Table 4.2 Initial Values for Markov Chains 
Unknown 
Parameter 
Chain 1 
Initial Values 
Chain 2 
Initial Values 
Chain 3 
Initial Values 
c
µ  
0.00 
–1.00 
1.00 
2
σ  
4.00 
9.00 
17.0 
λ  
0.06 
0.50 
0.80 
θ  
0.20 
2.50 
5.00 
 
 
Recall from Chapter 3 that the MBKG formulation has a φ  vector that is 
unknown and needs to be estimated when fitting the MBKG surrogate model. The length 
of the φ  vector, i.e., the number of unknown parameters in the vector, is equal to the 
number of DoE samples used to fit the model. For this example, 25 DoE samples are used 
to fit the model; thus the φ  vector has 25 unknown parameters that need to be fitted in 
addition to the other four unknown parameters 
c
µ , 
2
σ , λ , and θ . The initial values used 
for the unknown φ  vector are calculated using the initial values of 
c
µ  and the 25 
response values in the y  vector. The initial values 
( )i
φ
 for the 
thi  chain are calculated as 
 
 
( )
( )
i
DoE
i
c
µ
=
−
φ
y
1  
(4.4) 
where 
( )i
φ
 are the initial values for the 
thi  chain, 
DoE
y
 are the 25 response values for the 
corresponding 25 DoE samples, 
( )i
c
µ
 is the initial value of 
c
µ  for the 
thi  chain, and 1  is 
vector filled with the value of one and has the same length as the 
DoE
y
 vector. 
Using the prior parameters in Table 4.1, the initial values in Table 4.2, and the 
initial values calculated using Eq. (4.4), MCMC is carried out to fit the MBKG surrogate 
model. For this example, 100,000 iterations were run using MCMC for each of the three 
parallel chains, giving a total of 300,000 iterations. In Bayesian analysis, when using 
MCMC, one iteration means that a sample was drawn for each of the unknown 
parameters using the full conditional distributions as described in Sections 1.2 and 3.4.  
 

 
 
57 
 
Another method to help assess if convergence in the MCMC chains has been 
achieved is to plot the history of the samples drawn for each of the MCMC iterations for 
the unknown parameters. A history plot for each of the unknown parameters can be 
created, and the three parallel chains for the parameter are plotted on the same plot. If the 
MCMC iterations are not drawing samples from the same posterior distribution for the 
three chains, then the history plots may show this clearly. Figure 4.3, Figure 4.4, Figure 
4.5, and Figure 4.6 show the history plots of the first 5000, 200, 2000, and 2000 iterations 
for the four unknown parameters 
c
µ , 
2
σ , λ , and θ , respectively. Note that the history 
plots start at the first iteration of the MCMC chains, i.e., the first sample drawn from the 
full conditionals. The initial values given in Table 4.2 are the 
th
0  iteration of the MCMC 
chains, and it is common practice in Bayesian analysis that the 
th
0  iteration is not shown 
in the history plots. In all the history plots, the three chains are plotted in three different 
colors: chain one is plotted in red, chain two is plotted in blue, and chain three is plotted 
in green. Figure 4.3 shows that, from the first iteration to about the 1500th iteration, chain 
three (green) looks to be drawing samples from a distribution that is wider than the 
distributions that the samples for chains one (red) and two (blue) are being drawn from. 
This is an indication that the three chains are not yet drawing samples from the same 
posterior distribution. However, after about iteration 1500 in Figure 4.3, the three chains 
look like white noise and look as though they may be drawing samples from the same 
posterior distribution. 
Figure 4.4 shows that chain one (red) starts off at relatively larger values than 
chains two (blue) and three (green). However, by about 40 iterations, the three chains are 
starting to come together, though they are not well mixed yet. In Figure 4.5, chain one 
(red) and two (blue) come together quickly, and by about 200 iterations they look as if 
they may be drawing samples from the same posterior distribution. Chain three (green), 
however, does not merge with chains one and two until after about 1400 iterations. After 
about 1400 iterations, it appears that the three chains are drawing samples from the same 
 

 
 
58 
 
posterior distribution. Similarly, in Figure 4.6, chains one (red) and two (blue) come 
together quickly after about 200 iterations. Chain three (green) does not merge with 
chains one and two until after about 1500 iterations. The history plots can be used to help 
determine when the chains may be starting to converge. However, it is important to run 
enough MCMC iterations to make sure that the chains stay together. Also, a large number 
of MCMC iterations are needed after the three chains have converged so that the samples 
drawn can be used to calculate statistics about the unknown parameters. 
 
Figure 4.3 History Plot of the First 5000 Iterations for
c
µ  
 

 
 
59 
 
 
Figure 4.4 History Plot of the First 200 Iterations for 
2
σ  
 
Figure 4.5 History Plot of the First 2000 Iterations for λ  
 

 
 
60 
 
 
Figure 4.6 History Plot of the First 2000 Iterations for θ  
Figures 4.7 – 4.10 are the history plots for all four unknown parameters and show 
the history for all 100,000 iterations. 
 
Figure 4.7 History Plot of All 100,000 Iterations for 
c
µ  
 

 
 
61 
 
 
Figure 4.8 History Plot of All 100,000 Iterations for 
2
σ  
 
Figure 4.9 History Plot of All 100,000 Iterations for λ  
 

 
 
62 
 
 
Figure 4.10 History Plot of All 100,000 Iterations for θ  
As seen in all four figures, the three chains all appear to have converged to 
drawing samples from the same posterior distribution after about 1500 iterations. The 
history plots are helpful for trying to determine if the three chains look like they may 
have converged to drawing samples from the same posterior distribution. However, the 
history plots only provide a visual and do not provide an analytical way to assess if the 
three chains have converged to the same posterior distribution. 
As discussed in Section 1.2, the use of the BGR diagnostic is a common and well-
accepted method to help assess if the MCMC chains have converged. The BGR 
diagnostic provides both a visual and numerical way to help assess convergence of the 
MCMC chains. Figures 4.11 – 4.14 show the BGR plots for the 
c
µ , 
2
σ , λ , and θ  
parameters. To assess convergence using a BGR plot, the red line in the BGR plot should 
level off and should have a value of one once the three chains are drawing samples from 
the same posterior distribution. A rule of thumb for the red line is that if the value is less 
than 1.05 it is taken to be convergence to one. Recall from Section 1.2.3 that the BGR 
diagnostic is calculating the widths of credible sets two different ways. The width of each 
 

 
 
63 
 
MCMC chain is calculated. Using the width for each chain, an average width is 
calculated. The widths are also calculated after the parallel Markov chains are pooled 
together to form one sample set. As the Markov chains converge to drawing samples 
from the same posterior distribution, the value of the widths calculated by the two 
different methods should approach the same number. The ratio of these two width values 
is calculated. This ratio should approach one as the parallel Markov chains converge to 
drawing samples from the same posterior distribution. The red line in the BGR plot is a 
plot of this ratio as the number of MCMC iterations increases. The blue and green lines in 
the BGR plot should come together and level off to a constant value. The value at which 
the blue and green lines level off does not matter; it is only important that they come 
together and level off to a constant value as the number of iterations increases. The 
widths calculated using the two different methods are normalized and plotted in the BGR 
plot as the blue and green lines. Thus, the blue and green lines coming together and 
leveling off to a constant value in the plot show that the widths of the credible sets are 
stabilizing and that convergence to the posterior distribution may have been reached. 
The BGR plots for all four parameters show that after about 20,000 iterations the 
red line has reached a value of one, and the blue and green lines have come together and 
leveled off to a constant value. Thus, for this example based on the history plots and BGR 
plots, it has been determined that the MCMC chains have converged to the posterior 
distribution by 20,000 iterations. That means the samples drawn after 20,000 iterations 
can be used to calculate statistics and predict response values using the surrogate model. 
The iterations before convergence in distribution has been achieved are discarded, and 
only the iterations after convergence are used for calculating statistics and plotting 
posterior distributions. The samples that are discarded are referred to as burn-in when 
performing a Bayesian analysis using MCMC. 
 

 
 
64 
 
 
Figure 4.11 BGR Plot for 
c
µ  
 
Figure 4.12 BGR Plot for 
2
σ  
 

 
 
65 
 
 
Figure 4.13 BGR Plot for λ  
 
Figure 4.14 BGR Plot for θ  
Recall that a total of 100,000 iterations were run and that the first 20,000 
iterations were discarded as burn-in. This leaves 80,000 iterations for each of the three 
 

 
 
66 
 
parallel chains that can be used to calculate statistics and plot distributions. The 
remaining iterations from the three chains are pooled together to give a total of 240,000 
samples from the posterior distribution to use for inference. Table 4.3 gives the posterior 
mean, standard deviation, and the 95% credible set using the 240,000 samples. If the λ  
parameter is considered, the posterior mean of λ  is 0.0324 and the standard deviation of 
λ  is 0.0107. The 95% credible set for λ  is [0.0176, 0.0588]
 
, which means that, for the 
priors and the 25 DoE samples used to fit the MBKG model, there is 95% posterior 
probability that the true λ  is between 0.0176 and 0.0588. From Table 4.1, the 95% 
interval for the prior distribution of λ  was [0.05,1.20]
 
. Thus, the posterior distribution is 
much narrower than the prior distribution as seen by the 95% credible set of the posterior 
distribution. This means that the 25 DoE samples provided a large amount of information 
about what the true value of λ  should be and shrank the uncertainty of the value of λ  
considerably as is reflected in the posterior 95% credible set. Similarly, for the three 
parameters 
c
µ , 
2
σ , and θ , Table 4.3 shows that the 95% credible set is rather narrow 
compared to the 95% intervals for the prior distributions given in Table 4.1. Overall, it 
can be concluded that the 25 DoE samples provided a large amount of information about 
what the true value of the unknown parameters are and shrank the uncertainty in all the 
unknown parameters. 
Table 4.3 Posterior Statistics for Unknown Parameters Using 25 DoE Samples 
Unknown 
Parameter 
Mean 
Standard 
Deviation 
95% Credible Set 
c
µ  
–3.019e –5 
0.0761 
–0.1510 
0.1507 
2
σ  
4.5542 
1.0685 
2.9146 
7.0660 
λ  
0.0324 
0.0107 
0.0176 
0.0588 
θ  
0.1968 
0.0432 
0.1516 
0.3118 
 
 

 
 
67 
 
The mean predicted response value given by the MBKG surrogate model, the true 
function, and the 25 DoE samples are shown in Figure 4.15. The true function without 
noise is shown as the red curve, the 25 DoE samples are shown as the black asterisks, and 
the mean predicted response value is shown as the blue curve. The figure shows that the 
MBKG surrogate model gives a much better fit to the true function than the ordinary 
Kriging surrogate model did in Figure 4.2. 
Figure 4.16 shows the 95% credible set without noise, the 95% credible set with 
noise in addition to the mean predicted response, the true function, and the 25 DoE 
samples. The 95% credible set without noise is shown by the green curves in Figure 4.16. 
The meaning of the 95% credible set without noise gives the band in which there is 95% 
probability that the true function lies. The 95% credible set with noise is shown by the 
pink dashed curves in Figure 4.16. The meaning of the 95% credible set with noise gives 
the band in which there is 95% probability that the noisy response values given by Eq. 
(4.3) lie. 
 
Figure 4.15 MBKG Fit of Eq. (4.1) Using 25 DoE 
 

 
 
68 
 
As shown in Figure 4.16, the 95% credible set without noise (green curves) does 
in fact contain the true function (red curve). Also, the 95% credible set with noise (pink 
dashed curves) does in fact contain both the true function and the 25 DoE sample points. 
However, it is noted that both the 95% credible sets with and without noise are rather 
wide. This is a direct measure and reflection of the uncertainty that exists in the MBKG 
surrogate model. In order to decrease the uncertainty in the MBKG surrogate model and 
therefore narrow the 95% credible sets, more DoE samples would need to be added. 
To demonstrate how the 95% credible sets narrow with more DoE samples, 25 
additional DoE samples were uniformly added to the initial 25 DoE samples, and the 
MBKG surrogate model was refitted using 50 DoE samples and the same priors as 
before. Figure 4.17 shows the fit of the new MBKG surrogate model: the red curve is the 
true function without noise, the black asterisks are the initial 25 DoE samples previously 
used, and the blue squares are the additional 25 DoE samples. The mean predicted 
response given by the MBKG surrogate model is the dark green curve. The 95% credible 
set without noise is shown as the light blue curves, and the 95% credible set with noise is 
shown as the black dashed curves. 
 
 

 
 
69 
 
 
Figure 4.16 MBKG Fit of Eq. (4.1) and 95% Credible Sets Using 25 DoE 
Figure 4.18 shows a plot of the 95% credible sets without noise when both 25 
DoE samples and 50 DoE samples are used. The figure shows how the 95% credible set 
using 50 DoE samples is in fact narrower than the 95% credible set using 25 DoE 
samples. It also shows how the credible set is narrower in some regions and wider in 
other regions. This indicates where the uncertainty in the surrogate model is smaller and 
larger in those regions, respectively. Figure 4.19 shows a plot of the 95% credible sets 
with noise when both 25 DoE samples and 50 DoE samples are used. Once again, it is 
seen how for 50 DoE samples the credible set is narrower than when 25 DoE samples are 
used. Figures 4.18 and 4.19 both show how the information in the additional 25 DoE 
samples has decreased the amount of uncertainty we have in the surrogate model. 
 

 
 
70 
 
 
Figure 4.17 MBKG Fit of Eq. (4.1) and 95% Credible Sets Using 50 DoE 
 
Figure 4.18 95% Credible Sets without Noise 
 

 
 
71 
 
 
Figure 4.19 95% Credible Sets with Noise 
Table 4.4 gives the posterior mean, the standard deviation, and the 95% credible 
set for the four unknown parameters of the MBKG surrogate model that was fitted using 
50 DoE samples. Note that 240,000 MCMC samples were used to calculate the statistics. 
Comparing the standard deviations in Table 4.3 to those in Table 4.4, it can be seen that 
the posterior standard deviations are smaller when 50 DoE samples were used. Also, 
comparing the 95% credible sets in the two tables shows that the credible sets when 50 
DoE samples were used are smaller, e.g., the lower and upper bounds of the 95% credible 
set for 
c
µ  were both reduced by a little more than half. The smaller posterior standard 
deviations and narrower 95% credible sets both show how the additional 25 DoE samples 
provided more information and reduced the overall uncertainty in the MBKG surrogate 
model. Adding more DoE samples would continue to add more information to the 
MBKG surrogate model and thus continue to reduce the posterior standard deviation and 
shrink the 95% credible sets further. 
 
 

 
 
72 
 
Table 4.4 Posterior Statistics for Unknown Parameters Using 50 DoE Samples 
Unknown 
Parameter 
Mean 
Standard 
Deviation 
95% Credible Set 
c
µ  
4.6847e –5 
0.0326 
–0.0642 
0.0643 
2
σ  
3.1863 
0.5890 
2.2301 
4.5341 
λ  
0.0169 
0.0038 
0.0110 
0.0258 
θ  
0.1880 
0.0328 
0.1513 
0.2779 
 
 
4.3 Two-Dimensional Mathematical Examples 
4.3.1 First 2-D Mathematical Example 
This section demonstrates fitting two different mathematical functions [Lee et al. 
2011] that are two-dimensional and contain noise using the MBKG surrogate modeling 
method. The first function without noise, referred to as 
( )
1
G x , is defined as 
 
 
( )
2
1
2
1
1
20
X X
G
= −
x
  
(4.5) 
where 
1
X  and 
2
X  are the two variables. For this example the two variables are correlated 
to each other by Clayton copula with a correlation coefficient of 0.5. Because of this 
correlation, the domain of interest for this example forms the raindrop shape seen in later 
figures. For this mathematical example, the noise is generated using a normal distribution 
with zero mean and a standard deviation of 0.1. The noise, ε , is generated as realizations 
from the distribution, which is expressed as 
 
 
(
)
2
~
0,0.1
N
ε
  
(4.6) 
 

 
 
73 
 
The response values with noise are then calculated as a summation of the true 
response value without noise given by Eq. (4.5) and a realization of ε . This is expressed 
as 
 
1(
)
i
i
i
y
G
ε
=
+
x
  
(4.7) 
where 
iy  is the 
thi  response value with noise, 
ix  is the 
thi  DoE sample point, and 
iε  is 
the 
thi  realization of noise. 
The red surface in Figure 4.20 shows a 3-D plot of Eq. (4.5) for the domain of 
interest for 
1
X  and 
2
X  for this example. Also shown in the figure as the light blue dots 
are the 25 DoE samples used for fitting the MBKG surrogate model. This example is 
going to focus on fitting an MBKG surrogate model to Eq. (4.7), keeping in mind the 
overall objective of carrying out confidence-based RBDO. Recall from Section 2.2 that 
the RBDO problem is formulated such that the failure is defined as response values 
greater than zero. The threshold value of zero of the function is referred to as the limit 
state. Figure 4.21 shows a contour plot of the true limit state, the red curve, for Eq. (4.5) 
in the domain of interest. The 25 DoE samples are shown as the red asterisks in the 
figure. 
The prior parameter values used for fitting the MBKG surrogate model to Eq. 
(4.5) are shown in Table 4.5. Note that, because this is a two-dimensional problem, there 
are two θ  parameters to be estimated. The prior parameter values for this example are 
actually the same as those used for the previous example. This is because these priors are 
relatively non-informative and thus are applicable for this example as well. 
 

 
 
74 
 
 
Figure 4.20 Surface Plot of Eq. (4.5) and 25 DoE Samples 
 
 
Figure 4.21 Contour Plot of True Limit State Function for Eq. (4.5) and 25 DoE Samples 
 

 
 
75 
 
Table 4.5 Prior Parameter Values for Fitting Eq. (4.5) 
Unknown 
Parameter 
Prior 
Distribution 
Parameter 1 
Parameter 2 
95% Interval 
c
µ  
Normal 
0
p
µ =
 
2
0.5
p
σ
=
 
1.39
−
 
1.39 
2
σ  
Inverse-Gamma 
8.25
σ
α =
 
72.5
σ
β =
 
4.91 
20.04  
λ  
Inverse-Gamma 
1.9548
λ
α =
 
0.2747
λ
β =
 
0.05 
1.20 
1θ   
Uniform 
1
0.15
aθ =
 
1
5
bθ =
 
0.27  
4.88  
2
θ  
Uniform 
2
0.15
aθ =
 
2
5
bθ =
 
0.27  
4.88  
 
 
The initial values for the Markov chains used for this example are given in Table 
4.6. Note that because the prior distributions are the same as those used in the example in 
the previous section, the initial values for the five unknown parameters in this example 
are the same as those in the previous section. This is because the initial values are based 
on the prior distributions. 
Table 4.6 Initial Values for Markov Chains for Fitting Eq. (4.5) 
Unknown 
Parameter 
Chain 1 
Initial Values 
Chain 2 
Initial Values 
Chain 3 
Initial Values 
c
µ  
0.00 
–1.00 
1.00 
2
σ  
4.00 
9.00 
17.0 
λ  
0.06 
0.50 
0.80 
1θ  
0.20 
2.50 
5.00 
2
θ  
0.20 
2.50 
5.00 
 
 
Using the prior parameters in Table 4.5 and the initial values in Table 4.6, MCMC 
was carried out to fit a MBKG surrogate model to the noisy responses given by Eq. (4.7). 
The contour plot of the limit state function, calculated using the mean of the predicted 
 

 
 
76 
 
response values, is shown in Figure 4.22. The true limit state function is shown as the red 
curve, and the predicted limit state function is shown as the blue curve in the figure. The 
25 DoE samples are shown as the red asterisks. 
 
Figure 4.22 MBKG Predicted Contour Plot of Limit State Function for Eq. (4.5) 
As seen in Figure 4.22, the predicted limit state by MBKG does not match 
extremely well to the true limit state function. This indicates that more DoE samples are 
required in order to improve the MBKG surrogate model and thus will improve the 
predicted limit state function given by the MBKG surrogate model. It is also easily seen 
in the figure that, out of the 25 DoE sample points, only a few of them are near the limit 
state function itself. This is another reason that the predicted limit state function is not 
very accurate. However, one thing that can be concluded from this is that adding 
additional samples near the limit state may be an efficient and effective way to improve 
the accuracy of the predicted limit state function given by the MBKG surrogate model. 
This will be discussed in more detail in the next chapter. 
 

 
 
77 
 
 
4.3.2 Second 2-D Mathematical Example 
The second two-dimensional function considered in this section is referred to as 
( )
2
G
x  and is defined as 
 
 
2
2
1
2
3
1
2
4
1
2
1
2
(
)
1
(0.9063
0.4226
6)
(0.9063
0.4226
6)
0.6(0.9063
0.4226
6)
( 0.4226
0.9063
)
G
X
X
X
X
X
X
X
X
X
= −+
+
−
+
+
−
−
+
−
−−
+
  
(4.8) 
where 
1
X  and 
2
X  are the two variables. For this example the two variables are correlated 
to each other by Clayton copula with a correlation coefficient of 0.5. Because of this 
correlation, the domain of interest for this example forms the raindrop shape seen in later 
figures. For this mathematical example, the noise is once again generated using a normal 
distribution with zero mean and a standard deviation of 0.1. Thus, the noise, ε , is taken 
as realizations from the distribution in Eq. (4.6). 
The response values with noise are then calculated as a summation of the true 
response value without noise give by Eq. (4.8) and a realization of ε . This is expressed as 
 
 
2(
)
i
i
i
y
G
ε
=
+
x
  
(4.9) 
where 
iy  is the 
thi  response value with noise, 
ix  is the 
thi  DoE sample point, and 
iε  is 
the 
thi  realization of noise 
The red surface in Figure 4.23 shows a 3-D plot of Eq. (4.8) for the domain of 
interest for 
1
X  and 
2
X  for this example; note that this is the same domain as for 
( )
1
G x  
in Eq. (4.5). Also shown in the figure as the light blue dots are the 25 DoE samples used 
for fitting the MBKG surrogate model. Note these are the same 25 DoE samples that 
were used for fitting Eq.(4.7). Figure 4.24 shows the contour plot of the true limit state 
 

 
 
78 
 
function, the red curve, for Eq. (4.8) in the domain of interest, and the red asterisks are the 
25 DoE. 
 
Figure 4.23 Surface Plot of Eq. (4.8) and 25 DoE Samples 
 
Figure 4.24 Contour Plot of True Limit State Function for Eq. (4.8) and 25 DoE Samples 
 

 
 
79 
 
The same prior distributions given in Table 4.5 are also used for fitting the 
MBKG surrogate model to Eq. (4.9). Because the same priors are being used, the same 
initial values for the Markov chains given in Table 4.6 are also used. Using these priors 
and initial values, MCMC was carried out to fit an MBKG surrogate model to Eq. (4.9). 
The contour plot of the limit state function, calculated using the mean of the predicted 
response values, is shown in Figure 4.25. 
 
Figure 4.25 MBKG Predicted Contour Plot of Limit State Function for Eq. (4.8) 
As seen in Figure 4.25, the predicted limit state by MBKG captures the general 
trend of the true limit state function; however, it does not accurately predict the true limit 
state function. This is an indication that more DoE samples need to be added, and the 
MBKG surrogate model refitted to give a more accurate prediction of the limit of the 
state function. Also, from the figure, it can be seen that only four of the 25 DoE samples 
are near the true limit state function. Thus, the same conclusion as for the 
( )
1
G x  function 
has been reached: adding additional samples near the limit state may be an efficient and 
 

 
 
80 
 
effective way to improve the accuracy of the predicted limit state given by the MBKG 
surrogate model. The next chapter will discuss adding more DoE samples. 
 
4.3.3 A Note about the Computational Time 
For both examples presented in this section, three parallel Markov chains each 
containing 100,000 iterations were generated, giving a total of 300,000 samples being 
drawn. The concern then becomes what is the computational time for generating 300,000 
iterations. For the two examples in this section, the computational time to fit the MBKG 
surrogate model for each example was about 10 minutes, i.e., it took about 10 minutes to 
draw the 300,000 samples. The hardware and software details used for fitting this model 
are as follows. A computer with Windows 7 Enterprise with Service Pack 1, 64-bit 
operating system was used. The processor in the computer used is an Intel® Core™ i7-
2600 CPU @ 3.40GHz. The computer has 16.0 GB of RAM. Hyperthreading is enabled 
on the machine, giving a total of eight cores that can be used. All the code for the MBKG 
surrogate modeling method has been developed in MATLAB. For this example, 
MATLAB Version 8.1.0.604 (R2013a) was used. 
Because the three Markov chains are independent, they were run in parallel using 
three cores. Thus, all three parallel Markov chains for both examples were run in about 
10 minutes using three cores. The computational time for these examples seems to be 
reasonable and is expected to be similar for other examples. 
 
 
 

 
 
81 
 
CHAPTER 5 
SEQUENTIAL SAMPLING VIA CREDIBLE SETS 
5.1 Introduction 
The two mathematical examples presented in Section 4.3 will be used and built 
upon in the next two sections. A sequential sampling method is developed for adding 
more design of experiment (DoE) samples to further improve the modified Bayesian 
Kriging (MBKG) surrogate model by utilizing the information in the posterior credible 
sets of the MBKG surrogate model. 
Recall that the overall objective of this study is to carry out confidence-based 
RBDO using the MBKG surrogate model for problems whose responses contain noise. 
Also recall from Section 2.2 that the RBDO problem is formulated such that failure is 
defined as response values greater than zero. Thus, in order to use an MBKG surrogate 
model, the accuracy of the limit state function is of high importance because it will 
dictate the accuracy of the reliability analysis. Keep in mind that the MBKG surrogate 
model is not a deterministic surrogate model, but rather a surrogate model that produces 
posterior distributions for the MBKG parameters 
c
µ , 
2
σ , λ , θ , and φ . Therefore, a 
predicted response value for a given point does not have one deterministic value but 
rather has a probability distribution that gives the probability of the predicted response 
value being in any interval. The Markov chain Monte Carlo (MCMC) samples drawn 
from the predictive distribution of the response variable can be used to estimate any 
desired characteristic of the distribution, e.g., the mean, standard deviation, and credible 
sets. The larger the standard deviation and the wider the credible set, the more uncertainty 
there is in the predicted value, i.e., more uncertainty in what the true value is. The 
probability distributions characterize this uncertainty and provide information that can be 
used to further improve the MBKG surrogate model. This section presents a sequential 
 

 
 
82 
 
sampling method for adding more DoE samples to the MBKG surrogate model using the 
posterior credible sets to improve the prediction of the limit state. 
 
5.2 Sequential Sampling for the First 2-D Mathematical 
Example 
Section 4.3.1 showed an example of fitting an MBKG surrogate model to a two-
dimensional problem with noise. Figure 4.22 shows that the predicted limit state (blue 
curve in Figure 4.22) did not match well to the true limit state (red curve in Figure 4.22). 
Figure 5.1 shows the contour plot of the true limit state function as the red curve, the 
MBKG predicted limit state as the blue curve, and the 400 test points as the light blue 
circles. The 400 test points are uniformly and randomly distributed in the domain. The 
response values for the 400 test points are predicted using the MBKG surrogate model 
created in Section 4.3.1 using the 25 DoE samples in Figure 4.22. Each of the 400 test 
points has a distribution of the predicted response for that test point, i.e., there are 400 
distributions of predicted responses, one for each of the 400 test points. 
As described previously, the limit state function is the key for doing a reliability 
analysis. Thus, out of the 400 test points, the ones of interest are the test points whose 
distribution of the predicted response contains the limit state, i.e., the distribution of the 
predicted response contains the value zero. Out of the 400 test points, Figure 5.2 shows 
the 134 test points whose 95% credible sets capture the value zero. These 134 test points 
are shown as the yellow-filled circles; the light blue circles are the remaining test points. 
The red asterisks are the 25 DoE samples used to fit the MBKG surrogate model. 
 

 
 
83 
 
 
Figure 5.1 Contour Plot of Limit State and 400 Test Points 
 
Figure 5.2 Test Points with 95% Credible Sets that Capture the Limit State Using 25 DoE 
The yellow-filled circles in Figure 5.2 showing the 95% credible sets that contain 
zero form a rather wide band around both the true limit state and the predicted limit state. 
 

 
 
84 
 
This wide band is a visualization of the amount of uncertainty we have in the predicted 
limit state. When studying Figure 5.2, it is not surprising to see that the yellow band 
contains only six out of the 25 DoE samples used to fit the MBKG surrogate model. This 
explains why the yellow band is so wide, i.e., why the uncertainty in the predicted limit 
state is so large. 
Naturally, this indicates that adding DoE samples chosen from the 134 test points 
that form the yellow band would reduce the amount of uncertainty in the limit state and 
effectively narrow the band. It is also desirable to select the additional DoE samples from 
the 134 test points such that they are as far from the existing 25 DoE samples as possible, 
as well as being far from the additional samples added. In addition to the new DoE 
samples being far from the existing samples, it is also desirable that they are at locations 
where the amount of uncertainty in the limit state is the largest. Placing the new DoE 
samples in this strategic way will provide the maximum amount of information to the 
MBKG surrogate model, requiring fewer DoE samples to be used to fit the MBKG 
surrogate model. 
To determine the locations that meet the desired requirements just described, a 
weighting system is proposed to select the new DoE samples from the 134 candidate test 
points. The first step is to calculate the distance between the test points and the existing 
25 DoE samples previously used, giving a matrix of distance values. This can be 
expressed mathematically as 
 
 
for
1,...,
and
1,...,
j
i
ij
test
DoE
D
i
n
j
n
=
−
    
 =    
 
  =    
s
t
 
(5.1) 
where 
ij
D  is the distance between the 
thi  test point and 
th
j  existing DoE sample, 
js  is 
the 
th
j  existing DoE sample, 
it  is the 
thi  test point, 
test
n
 is the number of test points 
whose 95% credible set capture zero, and 
DoE
n
 is the number of existing DoE samples. 
For this example, 
134
test
n
=
 and 
25
DoE
n
=
. Once these distances are obtained, the next 
 

 
 
85 
 
step is to find the minimum distance from all the existing DoE samples for each test 
point. This then gives a vector of the distance between the test point and the closest 
existing DoE sample, which is expressed mathematically as 
 
 
{
}
min
for
1,...,
i
ij
DoE
d
D
j
n
=
     
  =    
 
(5.2) 
where 
id  is the distance between the 
thi  test point and the closest existing DoE sample, 
ij
D  is the distance between the 
thi  test point and the 
th
j  existing DoE sample as 
calculated in Eq. (5.1), and 
DoE
n
 is the number of existing DoE samples.  
The last value needed to calculate the weight is the width of the 95% credible sets 
that capture zero. This width is simply calculated as 
 
 
for
1,...,
w
L
U
i
i
i
test
c
c
c
i
n
= 
 +
   
  =    
 
(5.3) 
where 
w
ic  is the width of the 95% credible set for the 
thi  test point, 
L
ic  is the lower bound 
of the 95% credible set for the 
thi  test point, 
U
ic  is the upper bound of the 95% credible 
set for the 
thi  test point, and 
test
n
 is the number of test points. Note that only the 95% 
credible sets that contain zero are used. Therefore the lower bound, 
L
ic , is always less 
than zero and upper bound, 
U
ic , is always greater than zero. The weight for each test 
point is then calculated as 
 
 
for
1,...,
w
i
i
i
test
w
d
c
i
n
= 
∗ 
  
  =    
 
(5.4) 
where 
iw  is the weight for the 
thi  test point, 
id  is the distance between the 
thi  test point 
and the closest existing DoE sample as calculated in Eq. (5.2), 
w
ic  is the width of the 95% 
credible set for the 
thi  test point as calculated in Eq. (5.3), and 
test
n
 is the number of test 
points. The test point that has the largest weight given by Eq. (5.4) is selected as the new 
DoE sample, expressed as 
 

 
 
86 
 
 
{ }
max
for
1,...,
new
i
test
w
i
n
=
   
  =    
s
 
(5.5) 
where 
new
s
 is the new DoE sample selected from the 
test
n
 test points, 
iw  is the weight for 
the 
thi  test point, and 
test
n
 is the number of test points. Then 
new
s
 is added to the list of 
existing DoE samples, and this selection process can be iterated until the desired number 
of new DoE samples are added. 
For this example, 20 additional DoE samples were added to the 25 initial DoE 
samples, giving a total of 45 DoE samples used to fit the MBKG surrogate model. Figure 
5.3 shows the 20 additional DoE samples added as the black squares in the figure. It can 
be seen in the figure that the samples selected are relatively uniformly spaced from each 
other as well as spaced from the existing 25 DoE samples shown as the red asterisks. 
 
Figure 5.3 Additional 20 DoE Samples 
The MBKG surrogate model was refitted using the 45 DoE samples and the same 
priors and initial values for the MCMC analysis that were used in Section 4.3.1. The 
contour plot of the limit state using the 45 DoE samples is shown in Figure 5.4 as the 
 

 
 
87 
 
green curve. The blue curve in the figure is the limit state given by the previous MBKG 
surrogate model that used 25 DoE samples. The red curve in the figure is the true limit 
state for Eq. (4.4). As seen in the figure, the left part of the green curve, i.e., the limit 
state using the 45 DoE samples, is a little closer to the true limit state (red curve). 
However, the right side of the green curve, i.e., the limit state using the 45 DoE samples, 
is a little farther away from the true limit state (red curve). 
 
Figure 5.4 Contour Plot of Limit State of Eq. (4.4) Using 45 DoE Samples 
Further investigation of the 45 DoE samples used to fit the MBKG surrogate 
model revealed that the noise added in Eq. (4.4) tended to be a little biased to the positive 
side. This is believed to be the reason that the predicted limit state for both the 25 DoE 
samples and 45 DoE samples seems to be consistently above the true limit state in Figure 
5.4. 
To see the overall improvement of the MBKG surrogate model using the 
additional 20 DoE samples, the response values for the same 400 test points shown in 
 

 
 
88 
 
Figure 5.1 were predicted using the MBKG surrogate model created using the 45 DoE 
samples. Then the 400 response values whose 95% credible sets that captured zero were 
selected again and are shown in Figure 5.5 as the small black diamonds. It is easily seen 
how the 95% credible set band formed by the small black diamonds is much narrower 
than the previous 95% credible set band formed by the yellow-filled circles. This shows 
how adding 20 DoE samples reduced the amount of uncertainty in the predicted limit 
state. 
 
Figure 5.5 Test Points with 95% Credible Sets that Capture the Limit State Using 45 DoE 
Despite the existence of bias in the DoE samples as explained earlier, the true 
limit state (red curve in Figure 5.5) is still within the 95% credible set of the limit state 
given by the MBKG surrogate model constructed using the 45 DoE samples. Further 
sequential sampling to add additional DoE samples using the same process will continue 
to improve the prediction of the limit state and reduce the width of the 95% credible set. 
 
 

 
 
89 
 
5.3 Sequential Sampling for the Second 2-D Mathematical 
Example 
5.3.1 Prior Distributions and MCMC Initial Values 
Section 4.3.2 showed an example of fitting an MBKG surrogate model to a two-
dimensional problem with noise. Figure 4.25 showed that the predicted limit state (blue 
curve in Figure 4.25) did not match well to the true limit state (red curve in Figure 4.25). 
When fitting the MBKG surrogate model in Section 4.3.2 a constant mean structure was 
used and a wide prior for the 
2
σ  parameter was used. As seen in Figure 4.25 the true 
limit state is highly nonlinear. Therefore, in this section, a second-order mean structure 
for the MBKG surrogate model will be used. Recall from Section 3.3.1 that using a non-
constant mean structure introduces the regression coefficients β  that need to be fitted 
when fitting the MBKG surrogate model. Thus, different prior parameter values for the 
prior distributions are needed than were used in Section 4.3.2 for this example. Table 5.1 
lists the prior parameters used for fitting the MBKG surrogate model in this section. The 
second-order mean structure is expected to absorb the trend of limit state function. 
Therefore, a slightly tighter prior for 
2
σ  is used than was used previously. The prior for 
the λ  parameter remains the same as it was previously. The prior for the θ  parameters 
also remains the same as previously used. Because the data used to fit the MBKG 
surrogate model is normalized as explained in Section 3.3.2, the β  coefficients are 
expected to take on small values near zero. Therefore, a noninformative prior for the β  
coefficients that has a zero mean vector and a variance of 650 for each coefficient is used. 
This gives the 95% interval for the coefficients to be 49.97
±
, which is expected to 
capture the β  coefficients. The initial values needed to carry out the MCMC simulation 
to fit the MBKG surrogate model are given in Table 5.2.  
 
 

 
 
90 
 
Table 5.1 Prior Parameter Values 
Unknown 
Parameter 
Prior 
Distribution 
Parameter 1 
Parameter 2 
95% Interval 
2
σ  
Inverse-Gamma 
2
σ
α =
 
2.5
σ
β =
 
0.49  
10.32 
λ  
Inverse-Gamma 
1.9548
λ
α =
 
0.2747
λ
β =
 
0.05 
1.20 
iθ  
Uniform 
0.15
aθ =
 
5
bθ =
 
0.27  
4.88  
β   
MVN 
0  
650I   
49.97
−
 
49.97  
Table 5.2 Initial Values for Markov Chains 
Unknown 
Parameter 
Chain 1 
Initial Values 
Chain 2 
Initial Values 
Chain 3 
Initial Values 
2
σ  
1.00 
3.00 
6.0 
λ  
0.06 
0.50 
0.80 
iθ  
0.20 
2.50 
5.00 
iβ   
0.00 
-50.0 
50.0 
 
 
5.3.2 Sequential Sampling 
The sequential sampling process using the 95% credible set that was presented in 
Section 5.2 will be used for this example as well. Out of the 400 test points predicted 
using the MBKG surrogate model, Figure 5.6 shows the 298 test points whose 95% 
credible sets capture the value zero. These 298 test points are shown as the yellow-filled 
circles; the light blue circles are the remaining 102 test points whose 95% credible sets 
did not capture zero. The red asterisks are the 25 DoE samples used to fit the MBKG 
surrogate model. The red curve is the true limit state for Eq. (4.6). The blue curve is the 
predicted limit state given by the MBKG surrogate model. It is clearly seen that the mean 
of the predicted limit state does not match well to the true limit state in the top portion. It 
does, however, match well for the left part of the limit state. 
 

 
 
91 
 
The large number of test points that have a 95% credible set that captures zero 
indicates that there is a large amount of uncertainty in the MBKG surrogate model about 
where the true limit state is. Recalling from Figure 4.23, which showed the 3-D surface 
plot of Eq. (4.6), the function is relatively flat in the region near the limit state and then 
suddenly drops off. This explains why there is so much uncertainty in the MBKG 
surrogate model about where the limit is compared to the example in the previous 
section. Using the same sequential sampling procedure described in the previous section, 
20 additional DoE samples are selected from the 298 test points to be used to improve the 
MBKG surrogate model. The 20 additional DoE samples are shown as the black squares 
in Figure 5.7. 
The MBKG surrogate model was refitted using the 45 DoE samples, and the same 
priors and initial values listed in Section 5.3.1 were used for the MCMC simulation. The 
contour plot of the limit state using the 45 DoE samples is shown in Figure 5.8 as the 
green curve. The blue curve in the figure is the limit state given by the previous MBKG 
surrogate model that used 25 DoE samples. The red curve in the figure is the true limit 
state for Eq. (4.6). Figure 5.8 shows that the left part of the limit state (green curve) still 
matches as closely to the true limit state (red curve) as it did previously. It is also seen 
that the top part of the limit state (green curve) has improved and is closer to the true 
limit state than it was previously. The figure also shows that, even with the 45 DoE 
samples, only a few are actually near the true limit state. 
To see the overall improvement of the MBKG surrogate model using the 
additional 20 DoE samples, the response values for the same 400 test points were 
predicted using the MBKG surrogate model created using the 45 DoE samples. Then the 
400 response values whose 95% credible sets that captured zero were selected again and 
are shown in Figure 5.9 as the small black diamonds. It is easily seen how this 95% 
credible set band formed by the small black diamonds did shrink in on both sides of the 
true limit state compared to the previous 95% credible set band formed by the yellow-
 

 
 
92 
 
filled circles. This shows how adding 20 DoE samples reduced the amount of uncertainty 
in the predicted limit state. From the figure it can be seen that the uncertainty in the lower 
left part of the limit state is rather small; this is reflected by the narrow 95% credible set 
in that area. However, the uncertainty in the upper top part of the limit state is still rather 
large, as reflected by the wider 95% credible set in that area. This indicates that 
additional DoE samples are needed to improve the MBKG surrogate model. Further 
sequential sampling using the same process is continued to improve the prediction of the 
limit state and reduce the width of the 95% credible set. 
 
Figure 5.6 Test Points with 95% Credible Sets that Capture the Limit State Using 25 DoE 
 

 
 
93 
 
 
Figure 5.7 Additional 20 DoE Samples Shown as Black Squares 
 
Figure 5.8 Contour Plot of Limit State Using 45 DoE Samples 
 

 
 
94 
 
 
Figure 5.9 Test Points with 95% Credible Sets that Capture the Limit State Using 45 DoE 
In Figure 5.9 there are 154 test points, shown as the black diamonds that have 
95% credible sets that capture zero. Using these 154 test points, an additional 20 DoE 
samples are selected using the same sequential sampling method as before. The 
additional 20 DoE samples selected are shown in Figure 5.10 as the pink circles. The 
MBKG surrogate model was refitted using the 65 DoE samples. The contour plot of the 
predicted limit state using the 65 DoE samples is shown in Figure 5.11 as the blue curve. 
It is seen that the predicted limit (blue curve) is a close match to the true limit except for 
the top portion, which is a better fit than it was using the 45 DoE samples as shown by 
the green curve in Figure 5.8. The response values of the 400 test points were predicted 
using the MBKG surrogate model created using the 65 DoE samples. The 400 response 
values whose 95% credible sets captured zero were selected and are shown in Figure 5.12 
as the light blue plus signs. It is seen that the 95% credible set did shrink, but not 
significantly. 
 

 
 
95 
 
 
Figure 5.10 Additional 20 DoE Samples Shown as Pink Circles 
 
Figure 5.11 Contour of Limit State Using 65 DoE Samples 
 

 
 
96 
 
 
Figure 5.12 Test Points with 95% Credible Sets that Capture the Limit State Using 65 
DoE Samples Shown as the Light Blue Plus Signs 
Using the same sequential sampling procedure, another 20 DoE were selected 
from the 112 test points whose 95% credible sets captured zero. Figure 5.13 shows the 
additional 20 DoE as the blue diamonds. Figure 5.13 also shows the predicted limit state 
given by the MBKG surrogate created using the 85 DoE samples as the green curve. It is 
seen that limit state did improve a little but did not change significantly. Figure 5.14 
shows the test points that have 95% credible sets capturing zero for the MBKG surrogate 
created using the 85 DoE samples; they are shown as the black crosses in the figure. It 
can be seen that the credible set has shrunk when compared to the credible set given by 
the 65 DoE samples shown in Figure 5.12. A final set of 20 DoE samples was added to 
give a total of 105 DoE samples using the same sequential sampling method. The 
additional 20 DoE are shown in Figure 5.15 as the green asterisks. It is seen that the 20 
additional DoE closely surround the true limit state and also that 19 of them are in the top 
part where the credible set is wider. The predicted limit state given by the MBKG 
surrogate created using all 105 DoE is also shown in Figure 5.15 as the blue curve. It can 
 

 
 
97 
 
be seen that the curve appears to visually match the true limit state exactly except for at 
the top right tip. The test points with 95% credible sets capturing zero are shown in 
Figure 5.16 as the red circles. As seen in the figure, the credible set has shrunk more, thus 
reflecting the accurate prediction of the true limit state. 
This section demonstrated how the developed sequential sampling method 
systemically reduces the uncertainty in the predicted limit and therefore reduces the 95% 
credible set of the limit state. The next section will show how the sequential sampling 
method reduces the uncertainty in the predicted probability of failure. 
 
Figure 5.13 Additional 20 DoE Samples as Blue Diamonds and Contour of Limit State 
Using 85 DoE Samples as the Green Curve 
 

 
 
98 
 
 
Figure 5.14 Test Points with 95% Credible Sets that Capture the Limit State Using 85 
DoE Samples Shown as Black Crosses 
 
Figure 5.15 Additional 20 DoE Samples as Green Asterisk and Contour of Limit State 
Using 105 DoE Samples as the Blue Curve 
 

 
 
99 
 
 
Figure 5.16 Test Points with 95% Credible Sets that Capture the Limit Statue Using 105 
DoE Samples Shown as the Red Circles 
5.3.3 Distribution of Probability of Failure for Small Noise 
The previous section demonstrated how the predicted limit state converged to the 
true limit state using the developed sequential sampling method. It was also shown how 
the uncertainty of the predicted limit state was decreased by using the developed 
sequential sampling method. This was demonstrated by showing how the 95% credible 
set of the predicted limit state continued to shrink and become narrower as sequential 
sampling was performed. Recall that the limit state is used to predict the probability of 
failure. Because of the uncertainty in the predicted limit state, this naturally leads to 
uncertainty in the predicted probability of failure when using the predicted limit state. 
When using Bayesian methods, all of the uncertainty about a predicted value is 
reflected in the posterior distribution of the predicted value. Thus, when predicting the 
probability of failure using the MBKG surrogate model, the probability of failure is not a 
deterministic value, but rather a distribution. The posterior distribution of the probability 
of failure captures all of the uncertainty in the probability of failure prediction. This 
 

 
 
100 
 
includes all of the uncertainty in the MBKG surrogate model, i.e., the uncertainty of the 
estimated parameters in the MBKG surrogate model and the uncertainty in the predicted 
limit state. This section will demonstrate how the distribution of the probability of failure 
converges as the sequential sampling method developed in Section 5.2 is used to improve 
the predicted limit state. 
The MBKG surrogate models created using the sequential sampling method will 
be used in this section to predict the probability of failure. For the study of the 
distribution of the probability of failure, two different concepts will be shown. This 
section will show how the distribution of the probability of failure converges when using 
the developed sequential sampling method. The next section will show how the 
distribution converges for three different noise levels in the response function. 
The response function used in this section is the same 
2
G  function given in Eq. 
(4.8) repeated below as Eq. (5.6). 
 
 
2
2
1
2
3
1
2
4
1
2
1
2
(
)
1
(0.9063
0.4226
6)
(0.9063
0.4226
6)
0.6(0.9063
0.4226
6)
( 0.4226
0.9063
)
G
X
X
X
X
X
X
X
X
X
= −+
+
−
+
+
−
−
+
−
−−
+
 
(5.6) 
Noise is added to the response function as shown in Eq. (5.7) 
 
 
2(
)
i
i
i
y
G
ε
=
+
x
 
(5.7) 
where 
iε  is generated as a realization from a normal distribution with zero mean and 
standard deviation of 
ε
σ  as shown in Eq. (5.8). 
 
 
(
)
2
~
0,
N
ε
ε
σ
 
(5.8) 
 

 
 
101 
 
Three different noise levels will be considered and will be referred to as small, 
medium, and large noise. The noise levels, the standard deviation for each noise level, 
and the 95% interval for each noise distribution is given in Table 5.3. 
Table 5.3 The Three Noise Levels Used 
Noise Level 
e
σ   
95% Interval 
Small 
0.01 
0.0196
−
 
0.0196  
Medium 
0.05 
0.098
−
 
0.098 
Large 
0.10 
0.196
−
 
0.196  
 
 
For this example, the two random variables 
1
X  and 
2
X  are correlated to each 
other with a Clayton Copula with a correlation coefficient of 0.5. The marginal 
distributions of both random variables are given in Eq. (5.9). To calculate the probability 
of failure, Monte Carlo points are drawn from the joint distribution of 
1
X  and 
2
X . 
 
 
(
)
(
)
2
1
2
2
X ~
5.19,0.3
X ~
0.74,0.3
N
N
 
(5.9) 
This section shows how using a different number of DoE samples affects the 
distribution of the probability of failure for the small noise case. Figures 5.17 – 5.21 show 
the posterior distributions of the probability of failure using 25, 45, 65, 85, and 105 DoE 
samples, respectively. To predict the probability of failure, 10,000 Monte Carlo points 
were used to predict the probability of failure at each MCMC iteration; a total of 300,000 
MCMC iterations were used for this example. Recall from Figure 5.6 how the 95% 
credible set was really wide for the limit state, indicating that the uncertainty in the limit 
is large. This large uncertainty is carried through and is included in the uncertainty of the 
 

 
 
102 
 
probability of failure, as shown in Figure 5.17. The distribution of the probability of 
failure is extremely wide, ranging between roughly 0.0% to just under 90%; thus, it is 
clear that the amount of uncertainty in the probability of failure is rather large. After 
adding another 20 DoE samples and using a total of 45 DoE samples to fit the MBKG 
surrogate model, the amount of uncertainty in the limit state—and therefore the amount 
of uncertainty in the probability of failure—shrinks. This is shown by the distribution of 
the probability of failure in Figure 5.18. The distribution is not as wide and the left tail of 
the distribution is not nearly as fat as it was when using only 25 DoE samples. 
Continuing to add 20 DoE samples using the sequential sampling method continues to 
shrink the width of the distribution of the probability of failure. Figure 5.21 shows the 
distribution of the probability of failure using 105 DoE samples, and the distribution is 
significantly narrower than when only 25 DoE samples are used.  
Table 5.4 shows a summary of the statistics for the probability of failure using the 
different numbers of DoE samples. The table lists the mean value, the standard deviation, 
and the 95% credible set for the probability of failure. It can be seen that the 95% 
credible set using 25 DoE samples is wide, with a lower bound of 3.94% and an upper 
bound of 71.82%. This means that there is 95% probability that the true probability of 
failure is in the interval [3.94%, 71.82%]. It is seen in the table how the 95% credible set 
continues to shrink as more DoE samples are added. Using the 105 DoE samples, the 
95% credible set of the probability of failure now has a lower bound of 43.75% and an 
upper bound of 51.73%. This means that there is 95% probability that the true probability 
of failure is in the interval [43.75%, 51.73%]. The true probability of failure of Eq. (5.6) 
without noise is 48.4719%, as shown in Table 5.4. The 95% credible sets in the table 
capture the true probability of failure. Also, the mean value of the probability of failure 
approaches the true value. 
The next section will look at how the level of noise in the response function 
affects the posterior distribution of the probability of failure. 
 

 
 
103 
 
 
Figure 5.17 Posterior Distribution of Probability of Failure Using 25 DoE Samples 
 
Figure 5.18 Posterior Distribution of Probability of Failure Using 45 DoE Samples 
 

 
 
104 
 
 
Figure 5.19 Posterior Distribution of Probability of Failure Using 65 DoE Samples 
 
Figure 5.20 Posterior Distribution of Probability of Failure Using 85 DoE Samples 
 

 
 
105 
 
 
Figure 5.21 Posterior Distribution of Probability of Failure Using 105 DoE Samples 
Table 5.4 Probability of Failure Statistics Using Different 
Numbers of DoE Samples 
# DoE 
Samples 
Mean 
Std. 
95% Credible Set 
25 
40.6553 % 
19.3437 % 
3.94 % 
71.82 % 
45 
43.7836 % 
10.7319 % 
18.41 % 
60.24 % 
65 
47.2684 % 
5.1559 % 
36.26 % 
56.63 % 
85 
47.6843 % 
2.7809 % 
41.97 % 
52.92 % 
105 
47.8461 % 
2.0249 % 
43.75 % 
51.73 % 
True 
48.4719 % 
N/A 
N/A 
N/A 
 
 
5.3.4 Distribution of the Probability of Failure for Small, 
Medium, and Large Noise 
This section will show how the posterior distribution of the probability of failure 
changes for different levels of noise when using the developed sequential sampling 
 

 
 
106 
 
method. The plots of the posterior distribution of the probability of failure for medium 
and large noise look similar to those for the small noise. Thus, they are not shown here. 
Instead, tables showing the statistics that summarize the posterior distributions are 
presented. Tables 5.5 – 5.7 show the statistics of the posterior distribution of the 
probability of failure for the small, medium, and large noise levels, respectively. Note 
that Table 5.5 is the same as Table 5.4 and is just repeated for convenience. When 
studying the tables, it is evident that the credible sets shrink as more DoE samples are 
added for all three noise levels. It should be noted that the 25 initial DoE samples used 
for the small, medium, and large noise cases are different. This is because the initial 
uniform samples are randomly generated. When comparing the 95% credible sets for the 
medium and large noise cases, it is evident that they are similar. The credible sets for 
large noise are slightly larger than they are for medium noise, which is expected. 
The 95% credible sets for the small noise are slightly larger than they are for the 
medium and large noise. It was found that the 25 initial DoE samples were not as uniform 
in the domain for the small noise as they were for the medium and large noise. Because 
of this, there is less information available when fitting the MBKG surrogate model, i.e., 
there is more uncertainty in the MBKG surrogate model. Therefore, the 95% credible sets 
are expected to be a little wider for the small noise for the same number of DoE samples 
than they are for the medium and large noise. 
The Bayesian definition of the 95% credible sets for the probability of failure is 
that, given the information we have, i.e., the DoE samples and prior information, there is 
95% probability that the true probability of failure is within the bounds of the 95% 
credible set. For this example, it is seen that the true probability of failure is in fact within 
the 95% credible sets for all noise levels considered. 
The next chapter develops a method that uses the distribution of the probability of 
failure to carry out confidence-based reliability-based design optimization to obtain a 
reliable design with confidence. 
 

 
 
107 
 
Table 5.5 Probability of Failure Statistics for Small Noise 
# DoE 
Samples 
Mean 
Std. 
95% Credible Set 
25 
40.6553 % 
19.3437 % 
3.94 % 
71.82 % 
45 
43.7836 % 
10.7319 % 
18.41 % 
60.24 % 
65 
47.2684 % 
5.1559 % 
36.26 % 
56.63 % 
85 
47.6843 % 
2.7809 % 
41.97 % 
52.92 % 
105 
47.8461 % 
2.0249 % 
43.75 % 
51.73 % 
True 
48.4719 % 
N/A 
N/A 
N/A 
Table 5.6 Probability of Failure Statistics for Medium Noise 
# DoE 
Samples 
Mean 
Std. 
95% Credible Set 
25 
42.1873 % 
16.4255 % 
9.26 % 
69.69 % 
45 
45.3714 % 
8.5311 % 
25.91 % 
59.11 % 
65 
47.0987 % 
4.0376 % 
38.37 % 
54.19 % 
85 
48.1298 % 
2.2939 % 
43.31 % 
52.35 % 
105 
48.0290 % 
1.8029 % 
44.3 % 
51.37 % 
True 
48.4719 % 
N/A 
N/A 
N/A 
Table 5.7 Probability of Failure Statistics for Large Noise 
# DoE 
Samples 
Mean 
Std. 
95% Credible Set 
25 
43.4495 % 
16.1065 % 
10.4 % 
70.19 % 
45 
45.1517 % 
8.5012 % 
25.87 % 
58.97 % 
65 
46.5778 % 
4.1338 % 
37.6 % 
53.84 % 
85 
47.0866 % 
2.6179 % 
41.47 % 
51.8 % 
105 
47.4154 % 
2.3101 % 
42.44 % 
51.51 % 
True 
48.4719 % 
N/A 
N/A 
N/A 
 
 
 

108 
 
CHAPTER 6 
CONFIDENCE-BASED RELIABILITY-BASED DESIGN 
OPTIMIZATION VIA POSTERIOR DISTRIBUTIONS 
6.1 Introduction 
The previous chapter demonstrated how the posterior distribution of the 
probability of failure converged using the developed sequential sampling method. This 
chapter is going to develop a confidence-based reliability-based design optimization 
(RBDO) method that uses the posterior distribution of the probability of failure to carry 
out optimization. As explained in the previous chapter, the posterior distribution of the 
probability of failure contains all of the uncertainty about the probability of failure. The 
goal of developing a confidence-based RBDO method is to use the uncertainty in the 
optimization process in order to obtain a reliable design with a user-specified confidence 
level. 
The next section will present the formulation of the confidence-based RBDO 
method. The following sections will then present examples using the confidence-based 
RBDO method. 
 
6.2 Confidence-Based Reliability-Based Design 
Optimization Formulation 
Recall from Section 2.4 that the constraint formulation for RBDO is that the 
probability of failure should be less than or equal to the target probability of failure as 
shown in Eq. (6.1), where 
F
P  is the probability of failure for the current design and 
TAR
F
P
 
is the target probability of failure. When using the MBKG surrogate model, we do not 
know the true probability of failure. However, we do have the posterior distribution of the 
probability of failure. The mean value of the probability of failure could be used as a 
point estimate of the true probability of failure. That point estimate could then potentially 
 

109 
 
be used to carry out RBDO. This, however, could lead to an optimum design that does 
not meet the target probability of failure because there is uncertainty about what the true 
probability of failure is. 
 
 
TAR
F
F
P
P
≤
 
(6.1) 
It is desirable to take the uncertainty of the probability of failure into 
consideration for optimization. The posterior distribution of the probability of failure can 
be used to do exactly this. Figure 6.1 shows a simple representation of the posterior 
distribution of the probability of failure. Note that this is just for illustration proposes and 
the actual posterior distribution of the probability of failure may not be symmetric as was 
seen in Section 5.3. In Figure 6.1, the green line represents where the target probability of 
failure, 
TAR
F
P
, is located on the 
axis
x −
. The gray area in the figure is the probability that 
the probability of failure is less than or equal to the target probability of failure as 
expressed in Eq. (6.2) 
 
Figure 6.1 Posterior Distribution of the Probability of Failure 
 

110 
 
 
(
)
TAR
F
F
Area
P P
P
=
≤
 
(6.2) 
where Area  is the gray shaded area in Figure 6.1, 
( )
P •  is a probability measure, 
F
P  is 
the probability of failure, and 
TAR
F
P
 is the target probability of failure. For confidence-
based RBDO, a target confidence level, denoted as 
. .
C L , is defined. For example, if 
. .
90%
C L =
, the goal is to find a design such that there is 90% probability that the 
probability of failure is less than or equal to the target probability of failure. If this 
condition is satisfied, then the grey shaded area in Figure 6.1 will be at least equal to 
. .
C L , expressed mathematically as 
 
 
(
)
. .
TAR
F
F
Area
P P
P
C L
=
≤
≥
 
(6.3) 
where 
. .
C L  is the target confidence level, e.g., 90%. Equation (6.3) states that the 
probability of the probability of failure being less than or equal to the target probability of 
failure is greater than or equal to the target confidence level 
. .
C L  
Now take for example the posterior distribution of the probability of failure for 
the current design shown in Figure 6.2. If the target confidence level is 
. .
80%
C L =
, then 
the gray shaded area in Figure 6.2 is equal to 20%. The quantile value for 
. .
80%
C L =
 is 
determined from the posterior distribution to be 
(
. .)
7.3%
C L
F
P
=
 as shown in Figure 6.2. 
This means that there is 80% probability that the true probability of failure is less than or 
equal to 7.3%. Now take for the same current design and posterior distribution of the 
probability of failure. If the target confidence level is 90% as shown in Figure 6.3, then 
the gray shaded area is equal to 10%. Thus, the quantile value is 
( . .)
12.4%
C L
F
P
=
 as shown 
in Figure 6.3. This means that for the same current design there is 90% probability that 
the true probability of failure is less than or equal to 12.4%. The goal of the confidence-
based RBDO method is to find a design point such that quantile value 
( . .)
C L
F
P
 is less than 
or equal to the target probability of failure 
TAR
F
P
. This would mean that there is 
. .
C L  
probability that the true probability of failure is less than or equal to 
( . .)
C L
F
P
. 
 

111 
 
 
Figure 6.2 Posterior Distribution of the Probability of Failure with 
. .
80%
C L =
 
 
Figure 6.3 Posterior Distribution of the Probability of Failure with 
. .
90%
C L =
 
Using the posterior distribution of the probability of failure, the constraint used 
for confidence-based RBDO is expressed in Eq. (6.4) 
 

112 
 
 
( . .)
C L
TAR
F
F
P
P
≤
 
(6.4) 
where 
( . .)
C L
F
P
 is the probability of failure quantile value at the target confidence level, 
( . .)
C L  denotes the target confidence level, and 
TAR
F
P
 is the target probability of failure. 
The constraint states that the probability of failure quantile value for the target confidence 
level 
. .
C L  should be less than or equal to the target probability of failure 
TAR
F
P
. Figure 6.4 
shows a diagram of the confidence-based RBDO constraint in Eq. (6.4). In the figure the 
blue curve is a representation of the posterior distribution of the probability of failure. 
The green line in the figure is the target probability of failure, denoted as 
TAR
F
P
. The gray 
shaded area in the figure has an area equal to 1
. .
C L
−
; e.g., if the target confidence level 
is 90%, then the gray area would be 10% of the area under the blue curve. The red line in 
the figure is the value of the probability of failure at which the gray shaded area is equal 
to 1
. .
C L
−
, i.e., the quantile value for 
. .
C L  denoted as 
( . .)
C L
F
P
 in the figure. As seen in the 
figure, the constraint given in Eq. (6.4) is violated because 
( . .)
C L
TAR
F
F
P
P
>
. During the 
confidence-based RBDO optimization process, the optimization algorithm would update 
the design so that the posterior distribution of the probability of failure would move to the 
left and the red line in Figure 6.4 meets the green line or is to the left of the green line, as 
shown by the arrow in the figure, in order to satisfy the constraint in Eq. (6.4). 
 

113 
 
 
Figure 6.4 Constraint Diagram for Eq. (6.4) 
The mathematical formulation for confidence-based RBDO is expressed as 
 
 
( . .)
minimize
Cost( )
,
1,
,
subject to
,
and
i
i
d
r
C L
TAR
F
F
c
n
n
L
U
P
P
i
n
                                        

≤
  =

≤
≤
 ∈
 
 
∈

d
d
d
d
d
X



 
(6.5) 
where 
{ }
[
]
{
}
T
T
i
i
d
E X
=
=
d
, 
1to
d
i
n
=   
 is the design vector, 
[ ]
E •  is the expectation 
operator, 
{
}
T
i
X
=
X
 is the vector of random variables, and 
cn , 
dn , and 
rn  are the number 
of constraints, design variables, and random variables, respectively. The constraint in Eq. 
(6.4) can be rewritten in normalized form to be used as an optimization constraint and is 
expressed as 
 
 
( . .)
1
0
C L
F
TAR
F
P
h
P
=
−≤
 
(6.6) 
 

114 
 
To use a sensitivity-based optimization algorithm to find the solution to Eq. (6.5), 
the sensitivity of the constraint in Eq. (6.6) is needed. The sensitivity of this constraint can 
be written as 
 
 
( . .)
1
C L
F
TAR
j
F
j
h
P
d
P
d
∂
∂
=
∂
∂
 
(6.7) 
where 
j
d  is the 
thj  design variable, 
( . .)
C L
F
P
  is the probability of failure quantile value at 
the target confidence level 
. .
C L  , and 
TAR
F
P
 is the target probability of failure. It can be 
seen that the partial derivative on the right-hand side of Eq. (6.7) is the partial derivative 
of the probability of failure with respect to the design variable when the probability of 
failure is equal to 
( . .)
C L
F
P
. Recall that the definition of the design variables in Eq. (6.5) are 
the mean values of the random variables X . Therefore, the partial derivative on the right-
hand side of Eq. (6.7) can be rewritten as 
 
 
(
. .)
C L
F
F
F
j P
P
P
µ
=
∂
∂
 
(6.8) 
where 
[
]
j
j
E X
µ =
 is the mean value of the random variable 
j
X , 
F
P  is the probability of 
failure, and 
( . .)
C L
F
P
 is the probability of failure quantile value at the target confidence level 
. .
C L  Upon studying the partial derivative in Eq. (6.8), it can be seen that it takes on the 
same form as Eq. (2.21) and thus can be calculated using the same score function method 
presented in Section 2.5.2 [Rubinstein and Shapior 1993; Rahman 2009; Lee et. al. 2011; 
Zhao 2011]. By gathering all of these pieces together, the optimization problem in Eq. 
(6.5) can now be solved. The next section will present an example carrying out 
confidence-based RBDO. 
 
 

115 
 
6.3 A 2-D Mathematical Example 
6.3.1 Problem Definition 
A two-dimensional mathematical confidence-based RBDO problem is formulated 
as 
 
( . .)
2
2
2
2
1
2
1
2
2
1
2
1
2
1
minimize
Cost( )
,
1, 2, 3
subject to
,
and
where
(
10)
(
10)
( )
30
120
( )
1
20
( )
1
(0.9063
0.42
i
i
C L
TAR
F
F
L
U
P
P
i
d
d
d
d
Cost
X X
G
G
X
                                        

≤
  =   

≤
≤
 ∈
 
 
∈

+
−
−
+
= −
−
= −
= −+
+
d
d
d
d
d
X
d
X
X


2
2
3
1
2
4
1
2
1
2
3
2
1
2
2
26
6)
(0.9063
0.4226
6)
0.6(0.9063
0.4226
6)
( 0.4226
0.9063
)
80
( )
1
(
8
5)
( )
( )
,
1, 2, 3
[0, 0] ,
[10, 0] ,
[5.19,
]
(
,0.3 ) ,
1, 2
~
i
i
L
T
U
T
T
initial
j
j
X
X
X
X
X
X
X
G
X
X
h
G
i
X
N d
j
ε
ε
−
+
+
−
−
+
−
−−
+
= −
+
+
=
+    =   
=
 
 
=
 1
 
=
 0.74
   =  
X
X
X
d
d
d

(
)
(
)
2
0,
P
( )
0 ,
1, 2, 3
2.275% ,
1, 2, 3and
. .
1, 2, 3
i
i
F
i
Tar
F
i
N
P h
i
P
i
C L
i
ε
σ
=
>
   =   
=
   =    
 
 = 90%, =   
X
 
(6.9) 
As seen in Eq. (6.9), the target confidence level defined for this example is 90%, 
and the target probability of failure is defined to be 2.275%. The general problem 
definition for this example has random noise added to the true response value. Random 
noise is defined in the same way as it was in Section 5.3.3 and also uses the three 
different noise levels listed in Table (5.3). Table (5.3) is repeated below as Table 6.1. 
 

116 
 
Table 6.1 The Three Noise Levels Used 
Noise Level 
e
σ  
95% Interval 
Small 
0.01 
0.0196
−
 
0.0196  
Medium 
0.05 
0.098
−
 
0.098 
Large 
0.10 
0.196
−
 
0.196  
 
 
6.3.2 Optimization Results for Small Noise Level 
This section presents the optimization results for the optimization problem 
defined in Eq. (6.9) for using small noise, i.e., 
2
0.01
ε
σ
=
. The MBKG surrogate models 
used in solving this problem performed sequential sampling to refine the limit state and 
reduce the uncertainty in the probability of failure. The MBKG surrogate models started 
out using 25 initial DoE samples and did four sequential sampling iterations so that a 
total of 105 DoE samples were used to create the MBKG surrogate models. The MBKG 
surrogate models were then used to generate the posterior distribution of the probability 
of failure, which was then used as described in Section 6.2 to calculate the constraint 
information needed for optimization. 
The optimization history for the first constraint is shown in Table 6.2, and the 
optimization history for the second constraint is shown in Table 6.3. Note that for this 
example the third constraint is not active; thus, it was not fitted when creating the MBKG 
surrogate models. From Table 6.2 it is seen that optimization converged after six 
iterations. When studying the optimization history in both tables, it is seen that initially 
( .L.)
49.8%
C
F
P
=
 for constraint 1 and 
( .L.)
50.3%
C
F
P
=
 for constraint 2, both of which are 
much larger than 
2.275%
TAR
F
P
=
. By iteration six, however, 
( .L.)
2.295%
C
F
P
=
 for 
constraint 1 and 
( .L.)
2.294%
C
F
P
=
 for constraint 2, both of which are close to the target 
probability of failure and were within the optimization tolerances, thus, optimization 
converged. From the formulation of the optimization problem, the optimum design gives 
 

117 
 
90% probability, given all available information (i.e., the DoE samples and prior 
information), that the true probability of failure is less than 2.295% and 2.294% for 
constraint 1 and constraint 2, respectively. Thus, there is 10% probability that the true 
probability of failure is larger than 2.295% and 2.294% for constraint 1 and constraint 2, 
respectively. This is due to limited information being available in this case a limited 
number of DoE samples being used, i.e., the true probability of failure cannot be 
calculated using the MBKG surrogate model. As more DoE samples are used, i.e., as 
more information is used, to create the MBKG surrogate model, the optimum result 
should converge to the true solution. As seen from both Tables 6.2 and 6.3, the true 
probability of failure is less than 2.295% and 2.294% for constraint 1 and constraint 2, 
respectively. 
Also note that the mean value of the probability of failure should be close to the 
true probability of failure (i.e., probability of failure of constraints without noise) if the 
MBKG surrogate model is accurate. From Table 6.2, it is seen that the mean value is 
close to the true probability of failure for constraint 1. For constraint 2, from Table 6.3 it 
is seen that the mean probability of failure value is close to the true probability of failure 
but is not as close as constraint 1. 
Table 6.2 Optimization History for Small Noise Constraint 1 
Iteration 
1d  
2
d  
Cost 
1
True
F
P
 
 
1
Mean
F
P
 
 
1
(
. .)
C L
F
P
 
1 
5.1900 
0.7400 
–2.2922 
48.891% 
48.881% 
49.814% 
2 
4.7314 
1.3088 
–2.0241 
15.223% 
15.419% 
16.285% 
3 
4.8180 
1.5269 
–1.9174 
5.466% 
5.507% 
5.933% 
4 
4.9635 
1.5982 
–1.8827 
2.782% 
2.853% 
3.162% 
5 
5.0247 
1.6258 
–1.8701 
2.082% 
2.090% 
2.318% 
6 
5.0338 
1.6249 
–1.8705 
2.029% 
2.041% 
2.295% 
 
 
 

118 
 
Table 6.3 Optimization History for Small Noise Constraint 2 
Iteration 
1d  
2
d  
Cost 
2
True
F
P
 
 
2
Mean
F
P
 
 
2
(
. .)
C L
F
P
 
1 
5.1900 
0.7400 
–2.2922 
48.473% 
47.977% 
50.329% 
2 
4.7314 
1.3088 
–2.0241 
1.908% 
2.187% 
3.119% 
3 
4.8180 
1.5269 
–1.9174 
0.833% 
1.009% 
1.538% 
4 
4.9635 
1.5982 
–1.8827 
1.187% 
1.380% 
1.972% 
5 
5.0247 
1.6258 
–1.8701 
1.352% 
1.499% 
2.134% 
6 
5.0338 
1.6249 
–1.8705 
1.451% 
1.636% 
2.294% 
 
 
To demonstrate how the optimum solution converges to the true solution without 
noise when more DoE samples are used, i.e., more information is used, optimization was 
carried out using a different number of DoE samples. A total of five different 
optimization runs were done using 25, 45, 65, 85, and 105 DoE samples, respectively. 
Tables 6.4 and 6.5 show the optimization history using the different numbers of DoE 
samples to fit the MBKG surrogate model. Note that all MBKG surrogate models started 
with 25 initial DoE samples, and the remaining DoE samples were inserted 20 at a time 
using the developed sequential sampling method. When looking at the design history in 
the tables, it is seen that the optimum design appears to be converging to the true 
optimum design. For this example, the true optimum design means the optimum design 
for using the constraints with no noise. It is also seen for both constraints that the true 
probability of failure is less than 
( . .)
C L
F
P
 for all optimization runs. It is seen that when only 
25 DoE samples are used the design is very conservative compared to when 105 DoE 
samples are used. 
The next section will present optimization results for the medium and large noise 
levels as well as compare the results for the small, medium, and large noise levels. 
 
 
 

119 
 
Table 6.4 Optimization History for Different Numbers of DoE 
Constraint 1 
# DoE 
1d  
2
d  
Cost 
1
True
F
P
 
 
1
Mean
F
P
 
 
1
(
. .)
C L
F
P
 
25 
3.8538 
2.6257 
–1.4637 
1.278% 
1.376% 
2.245% 
45 
4.7637 
1.7715 
–1.8068 
1.909% 
1.998% 
2.297% 
65 
4.9260 
1.6783 
–1.8469 
2.008% 
1.988% 
2.222% 
85 
4.9982 
1.6416 
–1.8630 
2.020% 
2.039% 
2.295% 
105 
5.0338 
1.6249 
–1.8705 
2.029% 
2.041% 
2.295% 
True 
5.05 
1.59 
–1.8860 
2.291% 
N/A 
N/A 
Table 6.5 Optimization History for Different Numbers of DoE 
Constraint 2 
# DoE 
1d  
2
d  
Cost 
2
True
F
P
 
 
2
Mean
F
P
 
 
2
(
. .)
C L
F
P
 
25 
3.8538 
2.6257 
–1.4637 
0.000% 
0.881% 
2.266% 
45 
4.7637 
1.7715 
–1.8068 
0.036% 
0.838% 
2.177% 
65 
4.9260 
1.6783 
–1.8469 
0.391% 
0.838% 
1.746% 
85 
4.9982 
1.6416 
–1.8630 
0.950% 
1.285% 
2.102% 
105 
5.0338 
1.6249 
–1.8705 
1.451% 
1.636% 
2.294% 
True 
5.05 
1.59 
–1.8860 
2.279% 
N/A 
N/A 
 
 
6.3.3 Comparing Small, Medium, and Large Noise 
Optimization Results 
The same optimization problem was solved using the medium and large noise 
values in Table 6.1. Tables 6.6 and 6.7 show the optimization history for the medium 
noise level for constraints 1 and 2, respectively. From the tables it is seen that 
optimization finished after eight iterations. Optimization stopped because the relative 
changes in the design variables were less than the tolerance. However, the relative 
maximum constraint violation at the eighth iteration was 1.46
3
e −, which is slightly 
larger than the tolerance value of 1
3
e −. For constraint 1 it is seen that 
( . .)
2.228%
C L
F
P
=
 
 

120 
 
is actually less than 
2.275%
TAR
F
P
=
. Constraint 2, on the other hand, is the one with the 
constraint violation, 
( . .)
2.347%
C L
F
P
=
, which is slightly larger than 
2.275%
TAR
F
P
=
. Even 
with the small constraint violation, it is seen that the true probability of failure is less than 
( . .)
C L
F
P
 for both constraints. 
It can also be seen from the tables that, while the posterior mean value of the 
probability of failure does not match well to the true probability of failure, it is close to 
the true value for both constraints. This is actually why there is a need to use the 
developed confidence-based RBDO method: because there is uncertainty in what the 
probability of failure value is. Sometimes the mean value of the probability of failure 
underestimates the true value, and sometimes it overestimates the true value. However, 
using the confidence-based RBDO method gives a probability of 90% (90% for this 
example because 
. .
90%
C L =
) that the true probability of failure is less than 
( . .)
C L
F
P
 at the 
optimum design. 
Table 6.6 Optimization History for Medium Noise Constraint 1 
Iteration 
1d  
2
d  
Cost 
1
True
F
P
 
 
1
Mean
F
P
 
 
1
(
. .)
C L
F
P
 
1 
5.1900 
0.7400 
–2.2922 
48.902% 
50.058% 
51.648% 
2 
4.7479 
1.3157 
–2.0200 
14.490% 
13.686% 
14.827% 
3 
4.8263 
1.5291 
–1.9163 
5.312% 
5.660% 
6.345% 
4 
4.9713 
1.6067 
–1.8788 
2.600% 
2.612% 
3.034% 
5 
5.0009 
1.6272 
–1.8694 
2.181% 
2.315% 
2.668% 
6 
5.0158 
1.6293 
–1.8685 
2.083% 
2.051% 
2.351% 
7 
5.0442 
1.6208 
–1.8723 
2.062% 
1.963% 
2.273% 
8 
5.0347 
1.6253 
–1.8703 
2.017% 
1.961% 
2.228% 
 
 
 
 
 
 

121 
 
Table 6.7 Optimization History for Medium Noise Constraint 2 
Iteration 
1d  
2
d  
Cost 
2
True
F
P
 
 
2
Mean
F
P
 
 
2
(
. .)
C L
F
P
 
1 
5.1900 
0.7400 
–2.2922 
48.431% 
48.949% 
51.465% 
2 
4.7479 
1.3157 
–2.0200 
2.102% 
2.306% 
3.375% 
3 
4.8263 
1.5291 
–1.9163 
0.876% 
1.009% 
1.607% 
4 
4.9713 
1.6067 
–1.8788 
1.146% 
1.529% 
2.224% 
5 
5.0009 
1.6272 
–1.8694 
1.144% 
1.267% 
1.861% 
6 
5.0158 
1.6293 
–1.8685 
1.233% 
1.261% 
1.801% 
7 
5.0442 
1.6208 
–1.8723 
1.618% 
1.863% 
2.643% 
8 
5.0347 
1.6253 
–1.8703 
1.466% 
1.493% 
2.347% 
 
 
Tables 6.8 and 6.9 show the optimization history for the large noise level for 
constraint 1 and constraint 2, respectively. From the tables it is seen that optimization 
finished after six iterations. However, similar to the medium noise level case, 
optimization stopped because the relative changes in the design variables were less than 
the tolerance. The relative constraint violation at iteration six was 2.64
3
e −
, which 
exceeds the tolerance value of 1
3
e −. This can be seen in the tables for both constraints, 
as 
( . .)
2.321%
C L
F
P
=
 and 
( . .)
2.408%
C L
F
P
=
 for constraint 1 and constraint 2 respectively; 
both exceeded 
2.275%
TAR
F
P
=
. Even with the constraint violation, the results in the tables  
make it seem as though optimization was converging towards a design that would satisfy 
the constraint tolerance if the tolerance for the relative change in design variables was 
tightened some. However, optimization would have to actually be done with this tighter 
tolerance to confirm that it does converge to a solution.  
It is also interesting to note, that for constraint 1 at the sixth iteration, the true 
probability of failure, 
2.362%
TRUE
F
P
=
, is actually larger than 
( . .)
2.321%
C L
F
P
=
. Recall 
that, by definition, given all information used, there is 90% probability that the true 
probability of failure is less than 
( . .)
C L
F
P
. This means that there is 10% probability that the 
true probability of failure is larger than 
( . .)
C L
F
P
. For constraint 1 this is an example of that 
 

122 
 
10%. This is an example where not having “perfect information” leads to a design that 
would not be acceptable. There may be two possibilities for why this occurred. The first 
is that maybe more DoE samples are needed to farther refine the MBKG surrogate model 
to improve the posterior distributions. From Table 6.8 is seen that the posterior mean of 
the probability of failure for constraint 1 does not compare well to the true probability of 
failure. This may indicate that using more DoE samples may help to further improve the 
MBKG surrogate model and posterior distributions. 
The second possibility is that the large noise may be so large that it is “washing 
out” the true underlying response value completely. If this is true, then the problem 
cannot be solved using the MBKG surrogate model as formulated in this work. That is 
one limitation of the developed MBKG surrogate modeling method: if the noise is larger 
than the overall response variance, the developed method is not applicable. Further 
investigation using more DoE samples to carry out optimization would need to be done to 
try to determine if this situation is due to the lack of information, i.e., lack of DoE 
samples, or if indeed the large noise is too great for this method to be able to solve the 
problem. 
Lastly, it is interesting to compare the optimization results for the three different 
noise levels, even though optimization did not fully converge for the medium and large 
noise levels. Tables 6.10 and 6.11 show a comparison of the optimization results for 
constraints 1 and 2 respectively. From the tables it can be seen that small and medium 
noise results are fairly similar to each other; they appear to have converged to a similar 
optimum design. The large noise, is not as similar to the others, but it is relatively close. 
This chapter demonstrated that for the small and medium noise levels the 
developed confidence-based RBDO method was able to solve the problems to come up 
with a reliable optimum satisfying the desired confidence levels. Further investigation of 
the large noise is needed to see if using more DoE samples would give a satisfactory 
solution. 
 

123 
 
Table 6.8 Optimization History for Large Noise Constraint 1 
Iteration 
1d  
2
d  
Cost 
1
True
F
P
 
 
1
Mean
F
P
 
 
1
(
. .)
C L
F
P
 
1 
5.1900 
0.7400 
–2.2922 
48.884% 
48.961% 
51.132% 
2 
4.7189 
1.3898 
–1.9853 
11.644% 
11.338% 
12.808% 
3 
4.8339 
1.5610 
–1.9013 
4.547% 
4.723% 
5.520% 
4 
5.0297 
1.5520 
–1.9032 
3.057% 
2.626% 
3.131% 
5 
5.0283 
1.6120 
–1.8762 
2.202% 
1.758% 
2.214% 
6 
4.9888 
1.6192 
–1.8731 
2.362% 
1.950% 
2.321% 
Table 6.9 Optimization History for Large Noise Constraint 2 
Iteration 
1d  
2
d  
Cost 
2
True
F
P
 
 
2
Mean
F
P
 
 
2
(
. .)
C L
F
P
 
1 
5.1900 
0.7400 
–2.2922 
48.412% 
49.642% 
52.655% 
2 
4.7189 
1.3898 
–1.9853 
1.105% 
1.320% 
2.252% 
3 
4.8339 
1.5610 
–1.9013 
0.677% 
0.553% 
0.924% 
4 
5.0297 
1.5520 
–1.9032 
2.803% 
2.596% 
3.891% 
5 
5.0283 
1.6120 
–1.8762 
1.579% 
1.992% 
3.213% 
6 
4.9888 
1.6192 
–1.8731 
1.141% 
1.473% 
2.408% 
Table 6.10 Comparing Optimization Results for Different Noise Levels Constraint 1 
Noise 
# Iterations 
1d  
2
d  
Cost 
1
True
F
P
 
 
1
Mean
F
P
 
 
1
(
. .)
C L
F
P
 
Small 
6 
5.0338 
1.6249 
–1.8705 
2.029% 
2.041% 
2.295% 
Medium 
8 
5.0347 
1.6253 
–1.8703 
2.017% 
1.961% 
2.228% 
Large 
6 
4.9888 
1.6192 
–1.8731 
2.362% 
1.950% 
2.321% 
No Noise 
True 
5.05 
1.59 
–1.8860 
2.291% 
N/A 
N/A 
Table 6.11 Comparing Optimization Results for Different Noise Levels Constraint 2 
Noise 
# Iterations 
1d  
2
d  
Cost 
2
True
F
P
 
 
2
Mean
F
P
 
 
2
(
. .)
C L
F
P
 
Small 
6 
5.0338 
1.6249 
–1.8705 
1.451% 
1.636% 
2.294% 
Medium 
8 
5.0347 
1.6253 
–1.8703 
1.466% 
1.493% 
2.347% 
Large 
6 
4.9888 
1.6192 
–1.8731 
1.141% 
1.473% 
2.408% 
No Noise 
True 
5.05 
1.59 
–1.8860 
2.279% 
N/A 
N/A 
 

124 
 
6.4 A 3-D Multibody Dynamics Block-Car Example 
This section will present a 3-D multibody dynamics (MBD) block-car example 
that uses the developed methods to carry out confidence-based RBDO. The problem is a 
simple example to demonstrate using the method for an actual engineering problem that 
contains noise in the response. Figure 6.5 shows the multibody dynamics block-car used 
for this example. The car is modeled as a simple block with four wheels as shown in the 
figure. 
Reliability-based design optimization of this example was attempted previously 
using standard Kriging methods as the surrogate modeling method. However, the 
standard Kriging methods failed when trying to create the surrogate model for the contact 
force due to the noise in the contact force response. The standard Kriging method failed 
in that the predicted response surface was not a smooth surface. The predicted response 
surface looked like white noise, i.e., it was not smooth and was jagged. This example was 
the motivation that led to the research carried out in this work and the development of the 
methods for handling noisy response problems. 
 
Figure 6.5 Multibody Dynamics Block-Car Example 
The objective is to maximize the distance that the car travels up the incline before 
it loses traction. The three design variables are the mass of the car and the locations of the 
 

125 
 
center of mass in the x  and y  directions. The two constraints for this example are the 
contact force between the front wheels and the ground. The contact force is constrained 
so that it should not be less than 125 pounds. These constraints are imposed so that the 
car does not flip over backwards when going up the incline. Contact forces calculated by 
MBD simulation software are known to be inherently noisy responses. For this example 
the commercially available MBD software package used was RecurDyn. Deterministic 
design optimization (DDO) for this problem was carried out first. This was done in order 
to have a good starting point for confidence-based RBDO. In order to carry out DDO, the 
MBKG surrogate modeling method developed in this work was used. The MBKG 
surrogate model was fitted using 25 DoE for each constraint and the objective. After 
fitting the MBKG surrogate model, the 25 mean values for the responses were then 
available. The 25 DoE and 25 mean response values were then used with the standard 
Kriging methods to calculate the sensitivity of the response for the current design point. 
This was done for the two constraints and the objective function. The mean response 
values and sensitivity of the responses for both constraints and the objective were then 
provided to the optimization algorithm. 
Table 6.12 shows the design bounds for the design variables and the initial 
starting design point, where M  is the mass of the car, 
x
CM  is the x  coordinate location 
for the center of mass, and 
y
CM  is the y  coordinate location for the center of mass, 
denoted as 
1d , 
2
d , and 
3
d , respectively. The objective function for optimization is to 
maximize the distance the car travels up the incline before it loses traction. The two 
constraints are that the contact force in the front wheels are greater than 125 pounds. 
 
 
 
 
 

126 
 
Table 6.12 Design Bounds and Initial Design Point for DDO 
Design Variable 
Initial 
Lower Bound 
Upper Bound 
1d
M
=
 
55 
45  
65 
2
x
d
CM
=
 
0  
60
−
 
60  
3
y
d
CM
=
 
40
−
 
44
−
 
36
−
 
 
 
The DDO problem is formulated as shown in Eq. (6.10). The initial design point 
for DDO is the center of the design domain. Table 6.13 shows the DDO optimization 
history results. As seen in the table, the distance the car traveled, i.e., the objective 
function, was maximized during the optimization process, and the constraints became 
active as the optimization algorithm progressed through the iterations. It is seen in the 
table that the optimization algorithm converged to a solution on the 11th iteration. The 
DDO optimum point was used as the initial design point for carrying out confidence-
based RBDO. 
 
 
3
1
2
maximize
Distance( )
( )
125,
1, 2
subject to
,
where
Distance( )
Distance traveled before slip
( )
Contact Force Right Front Wheel
i
L
U
h
i
h
h
                                
≥
 =  


≤
≤
 ∈
                   

  =
=
d
d
d
d
d
d
d
d

( )
Contact Force Left Front Wheel
[45,
, 44] ,
[65,60, 36]
[55,0, 40]
L
T
U
T
T
initial
=
=
 −60 −
 
=
−
 
=
−
d
d
d
d
 
(6.10) 
 
 
 
 

127 
 
Table 6.13 DDO Optimization History for Block Car 
Iteration 
1d  
2
d  
3
d  
Objective 
1h  
2h  
1 
55.0000 
0.0000 
–40.0000 
2381.773 
139.414 
139.069 
2 
54.9302 
–3.9493 
–38.9974 
2399.437 
132.918 
132.733 
3 
56.9627 
–20.8840 
–36.0000 
2507.059 
120.636 
120.289 
4 
62.0902 
–23.8850 
–36.6840 
2521.744 
127.108 
128.648 
5 
65.0000 
–28.6530 
–39.1904 
2550.072 
130.828 
129.992 
6 
65.0000 
–33.6832 
–43.5372 
2587.930 
125.868 
125.280 
7 
65.0000 
–34.2504 
–44.0000 
2593.061 
125.775 
124.736 
8 
65.0000 
–34.2465 
–43.6772 
2594.082 
125.207 
124.516 
9 
65.0000 
–33.8138 
–43.8212 
2588.483 
125.608 
125.984 
10 
65.0000 
–33.8853 
–43.7777 
2589.537 
125.797 
125.087 
11 
65.0000 
–33.9238 
–43.7830 
2589.827 
125.313 
125.218 
 
 
Table 6.14 shows the design bounds for the design variables and the initial 
starting design point, where M  is the mass of the car, 
x
CM  is the x  coordinate location 
for the center of mass, and 
y
CM  is the y  coordinate location for the center of mass. 
These design variables are denoted as 
1d , 
2
d , and 
3
d , respectively, for confidence-based 
RBDO. The objective function for optimization is to maximize the distance the car 
travels up the incline before it loses traction. The two constraints are that the contact 
force in the front wheels are greater than 125 pounds. 
Table 6.14 Design Bounds and Initial Design Point 
for Confidence-Based RBDO 
Design Variable 
Initial 
Lower Bound 
Upper Bound 
1d
M
=
 
65 
45  
65 
2
x
d
CM
=
 
33.92
−
 
60
−
 
60  
3
y
d
CM
=
 
43.78
−
 
44
−
 
36
−
 
 
 

128 
 
The confidence-based RBDO problem is formulated as shown in Eq. (6.11). The 
initial design point for confidence-based RBDO is the DDO solution. When fitting the 
MBKG surrogate models 25 initial DoE samples were used. After fitting the MBKG 
surrogate models two sequential sampling iterations were performed using the developed 
sequential sampling method. For each sequential sampling iteration 25 DoE samples were 
added. Thus, giving a total of 75 DoE samples that were used to fit the MBKG surrogate 
models. The MBKG surrogate models were then used to generate the posterior 
distribution of the probability of failure that was used to carry out confidence-based 
RBDO. 
Tables 6.15 and 6.16 show the optimization history for constraints 1 and 2. As 
seen in the tables, at the initial design 
( . .)
C L
F
P
 is 44.270% and 62.343% for constraint 1 
and 2, respectively; both are much larger than the target 2.275%. It is seen that, in the 
first three iterations of optimization, 
( . .)
C L
F
P
 decreases quickly, and then for the remaining 
iterations it decreases at a slower rate for both constraints. The optimization algorithm 
converged to a solution after nine iterations, the optimization tolerances were satisfied, 
and there is no constraint violation at the optimum solution. As seen in Table 6.15, the 
objective, i.e., the maximum distance the car travels before losing traction, is decreased 
as the constraints are satisfied. At the optimum design there is 10% probability that the 
true probability of failure is larger than 0.439% and 1.751% for constraints 1 and 2 as 
seen in Tables 6.15 and 6.16, respectively. These values clearly satisfy the target value of 
2.275%. It is interesting to note how the optimization algorithm did not change the mass 
of the car from 65, which was the mass value from the DDO solution. This is believed to 
be because the higher the mass the more traction the car will have to travel farther up the 
incline. Thus, the confidence-based RBDO method was successful in finding a solution 
that met both the target probability of failure and target confidence level. 
 
 

129 
 
 
( . .)
3
3
1
2
maximize
Distance( )
,
1, 2
subject to
,
and
where
Distance( )
Distance traveled before slip
( )
Contact Force Right Front Wheel
i
i
C L
TAR
F
F
L
U
P
P
i
h
h
                                

≤
  =  

≤
≤
 ∈
 
 
∈

  =
=
d
d
d
d
d
X
d
X


(
)
(
)
(
)
(
)
2
1
1
2
2
2
2
3
3
( )
Contact Force Left Front Wheel
[45,
, 44] ,
[65,60, 36]
[65, 33.92, 43.78]
,0.5
,0.4
,0.2
P
( )
125 ,
1, 2
2.275% ,
1, 2 and
. .
1
i
i
L
T
U
T
T
initial
F
i
Tar
F
i
X
N d
X
N d
X
N d
P h
i
P
i
C L
i
=
=
 −60 −
 
=
−
 
=
−
−
 
=
<
   =  
=
   =   
 
 = 90%, =
X
d
d
d
X



, 2  
(6.11) 
Table 6.15 Confidence-Based RBDO Optimization History for Block-Car 
Constraint 1 
Iteration 
1d  
2
d  
3
d  
Objective 
1
Mean
F
P
 
 
1
(
. .)
C L
F
P
 
1 
65 
–33.9238 
–43.7830 
2589.806 
38.657% 
44.270% 
2 
65 
–32.2557 
–42.3679 
2575.278 
4.508% 
5.878% 
3 
65 
–31.7703 
–41.3944 
2572.964 
2.061% 
2.547% 
4 
65 
–30.7138 
–37.8149 
2573.467 
1.852% 
2.573% 
5 
65 
–30.8124 
–38.8367 
2571.488 
1.566% 
2.240% 
6 
65 
–30.1286 
–37.7002 
2567.938 
0.327% 
0.603% 
7 
65 
–30.3040 
–37.5318 
2569.594 
0.412% 
0.622% 
8 
65 
–29.9892 
–36.7301 
2568.900 
0.521% 
0.746% 
9 
65 
–29.8552 
–36.3724 
2568.612 
0.307% 
0.439% 
 
 
 

130 
 
Table 6.16 Confidence-Based RBDO Optimization History for Block-Car 
Constraint 2 
Iteration 
1d  
2
d  
3
d  
Objective 
2
Mean
F
P
 
 
2
(
. .)
C L
F
P
 
1 
65 
–33.9238 
–43.7830 
2589.806 
56.693% 
62.343% 
2 
65 
–32.2557 
–42.3679 
2575.278 
14.801% 
18.107% 
3 
65 
–31.7703 
–41.3944 
2572.964 
7.821% 
9.917% 
4 
65 
–30.7138 
–37.8149 
2573.467 
6.281% 
8.765% 
5 
65 
–30.8124 
–38.8367 
2571.488 
4.933% 
6.709% 
6 
65 
–30.1286 
–37.7002 
2567.938 
1.119% 
1.682% 
7 
65 
–30.3040 
–37.5318 
2569.594 
2.189% 
3.220% 
8 
65 
–29.9892 
–36.7301 
2568.900 
1.744% 
2.625% 
9 
65 
–29.8552 
–36.3724 
2568.612 
0.990% 
1.751% 
 
 

 
 
131 
 
CHAPTER 7 
CONCLUSION, CURRENT RESEARCH, AND FUTURE RESEARCH 
7.1 Conclusion 
Numerous sensitivity-based reliability-based design optimization (RBDO) 
methods have been developed and applied to various engineering problems. However, it 
is common that the sensitivity is difficult or even impossible to calculate for highly 
nonlinear and coupled fluid structure interaction problems, e.g., crash and blast problems. 
Sampling-based RBDO methods that use surrogate models to approximate the simulation 
models and then carry out Monte Carlo simulation using the surrogate model to perform 
the reliability analysis have been developed. Current sampling-based RBDO applications 
have been used on problems in which the simulation models do not contain noise or it is 
assumed that there is no noise. However, it has been found that some simulation models 
contain noise in the responses, which makes carrying out RBDO difficult. This has 
brought about the need for a surrogate modeling method that can handle responses that 
contain noise. There is also a need for a surrogate modeling method that can provide a 
way to not only carry out RBDO but to carry out confidence-based RBDO to ensure a 
conservative reliable optimum design. 
A modified Bayesian Kriging (MBKG) surrogate modeling method was 
developed for handling problems whose responses contain noise. The prior distributions 
to be used for fitting the MBKG surrogate model have been determined, and the full 
conditional distributions derived. The full conditional distributions were coded into a 
Gibbs sampling algorithm in order to use Markov chain Monte Carlo (MCMC) to fit the 
MBKG surrogate model. All the coding was done in MATLAB so that the methods 
developed in this study can be easily integrated with previously developed RBDO 
methods. It was shown that MBKG surrogate model does work for handling problems 
whose responses contain noise for which the standard Kriging approaches fail to work. 
 

 
 
132 
 
A sequential sampling method that uses the posterior credible sets of the MBKG 
surrogate model has been developed. The new method was tested with an example 
problem, and it was demonstrated how the uncertainty of the predicted limit state is 
reduced and the predicted limit state converges to the true limit state as more design of 
experiment (DoE) samples are used. It was also demonstrated that the posterior 
distribution of the probability of failure converges as more DoE samples are used. 
A confidence-based RBDO method using the posterior distribution of the 
probability of failure was developed. The method was demonstrated using a mathematical 
example with different amounts of noise. It was shown that the method converged to a 
solution and was able to find an optimum reliable design that meets the desired 
confidence levels. It was shown that, for small and medium noise, the method found two 
solutions that are very similar, i.e., almost the same design point. For the large noise, the 
optimization did converge for the relative change in the design variable tolerance; 
however, the constraint tolerance was still violated. The optimization history did show 
that it was converging to a solution that would be similar to that of the small and medium 
noise. 
Overall, an MBKG surrogate modeling method, a sequential sampling method for 
reducing the uncertainty in the posterior distributions, and a confidence-based RBDO 
method were successfully developed and demonstrated using a mathematical example. 
The developed methods were also successfully demonstrated using a 3-D multibody 
dynamics engineering example. 
 
7.2 Future Research 
Running the MCMC chains to fit the models tends to be the most computationally 
intensive part. Further investigation is needed so that the Metropolis-Hastings algorithm 
can be finely tuned; this would greatly improve the computational time for fitting the 
 

 
 
133 
 
MCMC chains. There are also some known bottle-necks in some of the functions used in 
fitting the MCMC chains, they can be rewritten to improve the computational time. A 
study on the use of different priors for different noise levels would also help with fine-
tuning the model in regards to improving the computational efficiency. 
Additional future research would be investigating how a dynamic Bayesian 
Kriging method can be developed in order to dynamically select the best mean structure 
and the correlation function that best fits the data for the problem being solved. The 
deviance information criterion (DIC) is one possible method that can be used to compare 
Bayesian regression models and select the one that best fits the data. There are also 
methods that use penalized loss functions to compare regression models. Research to 
study these methods and how they could be applied the developed MBKG surrogate 
modeling method is needed to determine if they can be used to develop a dynamic 
Bayesian Kriging method. 
It has also been recognized that the use of the posterior distribution of the 
probability of failure from the Bayesian analysis provides for a convenient and natural 
way to carry out confidence-based RBDO. The same concept could be applied to a 
different Bayesian Kriging model for noise-free problems. Using the noise-free Bayesian 
Kriging model, confidence-based RBDO could be carried out using the same method as 
presented in Chapter 6. 
 
 

 
 
134 
 
REFERENCES 
Agatonovic-Kustrin, S., & Beresford, R. (2000). Basic concepts of artificial neural 
network (ANN) modeling and its application in pharmaceutical research. Journal of 
Pharmaceutical and Biomedical Analysis, 22(5), 717-727. doi:10.1016/S0731-
7085(99)00272-1 
Almeida, J. S. (2002). Predictive non-linear modeling of complex data by artificial neural 
networks. Current Opinion in Biotechnology, 13(1), 72-76. doi:10.1016/S0958-
1669(02)00288-4 
An, D., & Choi, J. H. (2012). Efficient reliability analysis based on bayesian framework 
under input variable and metamodel uncertainties. Structural and Multidisciplinary 
Optimization, 46(4), 533-547. doi:10.1007/s00158-012-0776-6 
Bayes, T., & Price, R. (1763). An essay towards solving a problem in the doctrine of 
chance. by the late rev. mr. bayes, communicated by mr. price, in a letter to john 
canton, A. M. F. R. S. Philosophical Transactions of the Royal Society of 
London, 53(0), 370-418. 
Beers, W. C. M. v., & Kleijnen, J. P. C. (2003). Kriging for interpolation in random 
simulation. The Journal of the Operational Research Society, 54(3), 255-262. 
Box, G. E. P., & Draper, N. R. (1987). Empirical model-building and response surface. 
New York : Wiley. 
Breitkopf, P., Naceur, H., Rassineux, A., & Villon, P. (2005). Moving least squares 
response surface approximation: Formulation and metal forming 
applications. Computers & Structures, 83(17–18), 1411-1428. 
doi:http://dx.doi.org/10.1016/j.compstruc.2004.07.011 
Breitung, K. (1984). Asymptotic approximations for multinormal integrals. Journal of 
Engineering Mechanics, 110(3), 357-366. doi:10.1061/(ASCE)0733-
9399(1984)110:3(357) 
Brooks, S. P., & Gelman, A. (1998). General methods for monitoring convergence of 
iterative simulations. Journal of Computational and Graphical Statistics,7(4), 434-
455. 
Buhmann, M. D. (2003). Radial basis functions : Theory and implementations / M.D. 
buhmann. Cambridge, U.K. ; New York, NY : Cambridge University Press. 
Burges, C. C. (1998). A tutorial on support vector machines for pattern recognition. Data 
Mining and Knowledge Discovery, 2(2), 121-167. doi:10.1023/A:1009715923555 
Choi, K. K., Tu, J., & Park, Y. H. (2001). Extensions of design potential concept for 
reliability-based design optimization to nonsmooth and extreme cases.Structural and 
Multidisciplinary Optimization, 22(5), 335-350. doi:10.1007/s00158-001-0154-2 
Cowles, M. K. (2013). Applied bayesian statistics [electronic resource] : With R and 
OpenBUGS examples. New York, NY : Springer New York : Imprint: Springer. 
 

 
 
135 
 
Cowles, M. K., & Carlin, B. P. (1996). Markov chain monte carlo convergence 
diagnostics: A comparative review. Journal of the American Statistical 
Association,91(434), 883-904. 
Cressie, N. A. C. (1991). Statistics for spatial data. New York : J. Wiley. 
Currin, C., Mitchell, T., Morris, M., & Ylvisaker, D. (1991). Bayesian prediction of 
deterministic functions, with applications to the design and analysis of computer 
experiments. Journal of the American Statistical Association, 86(416), 953-963. 
Dyn, N., Levin, D., & Rippa, S. (1986). Numerical procedures for surface fitting of 
scattered data by radial functions. SIAM Journal on Scientific and Statistical 
Computing, 7(2), 639-659. doi:10.1137/0907043 
Fang, H., Rais-Rohani, M., Liu, Z., & Horstemeyer, M. F. (2005). A comparative study 
of metamodeling methods for multiobjective crashworthiness 
optimization. Computers & Structures, 83(25–26), 2121-2136. 
doi:10.1016/j.compstruc.2005.02.025 
Feller, W. (1968). An introduction to probability theory and its applications (3rd ed.). 
New York: Wiley. 
Fonseca, D. J., Navaresse, D. O., & Moynihan, G. P. (2003). Simulation metamodeling 
through artificial neural networks. Engineering Applications of Artificial 
Intelligence, 16(3), 177-183. doi:10.1016/S0952-1976(03)00043-5 
Forrester, A. I. J., Sóbester, A., & Keane, A. J. (2008). Engineering design via surrogate 
modelling : A practical guide. Chichester, U.K. : J. Wiley. 
Forrester, A. I. J., & Keane, A. J. (2009). Recent advances in surrogate-based 
optimization. Progress in Aerospace Sciences, 45(1–3), 50-79. 
doi:10.1016/j.paerosci.2008.11.001 
Friedman, J. H. (1991). Multivariate adaptive regression splines. The Annals of 
Statistics, 19(1), 1-67. 
Friedman, J. H., & Roosen, C. B. (1995). An introduction to multivariate adaptive 
regression splines. Statistical Methods in Medical Research, 4(3), 197-217. 
doi:10.1177/096228029500400303 
Gallant, S. I. (1993). Neural network learning and expert systems / stephen I. gallant. 
Cambridge, Mass. : MIT Press. 
Galushkin, A. I. (2007). Neural networks theory / alexander I. galushkin. Berlin ; New 
York : Springer. 
Gelfand, A. E., & Smith, A. F. M. (1990). Sampling-based approaches to calculating 
marginal densities. Journal of the American Statistical Association, 85(410), 398-409. 
doi:10.1080/01621459.1990.10476213 
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004). Bayesian data analysis. 
Boca Raton, Fla. : Chapman & Hall/CRC. 
 

 
 
136 
 
Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple 
sequences. Statistical Science, 7(4), 457-472. 
Gilks, W. R., Richardson, S., & Spiegelhalter, D. J. (1998). Markov chain monte carlo in 
practice. Boca Raton: Boca Raton : Chapman & Hall/CRC. 
Haldar, A., & Mahadevan, S. (2000). Probability, reliability, and statistical methods in 
engineering design. New York : J. Wiley. 
Hamada, M. S., Wilson, A. G., Reese, C. S., & Martz, H. F. (2008). Bayesian reliability 
[electronic resource]. New York, NY : Springer. 
Hasofer, A. M., & Lind, N. C. (1974). Exact and invariant second-moment code 
format. Journal of Engineering Mechanics Division, 100(1), 111-121. 
Hastings, W. K. (1970). Monte carlo sampling methods using markov chains and their 
applications. Biometrika, 57(1), 97-109. 
Hearst, M. A., Dumais, S. T., Osman, E., Platt, J., & Scholkopf, B. (1998). Support 
vector machines. Intelligent Systems and their Applications, IEEE, 13(4), 18-28. 
doi:10.1109/5254.708428 
Hogg, R. V., McKean, J. W., & Craig, A. T. (2005). Introduction to mathematical 
statistics. Upper Saddle River, N.J. : Pearson Education. 
Hohenbichler, M., Gollwitzer, S., Kruse, W., & Rackwitz, R. (1987). New light on first- 
and second-order reliability methods. Structural Safety, 4(4), 267-284. 
doi:http://dx.doi.org/10.1016/0167-4730(87)90002-6 
Hohenbichler, M., & Rackwitz, R. (1988). Improvement of Second‐Order reliability 
estimates by importance sampling. Journal of Engineering Mechanics,114(12), 2195-
2199. doi:10.1061/(ASCE)0733-9399(1988)114:12(2195) 
Hu, C., & Youn, B. D. (2011). Adaptive-sparse polynomial chaos expansion for 
reliability analysis and design of complex engineering systems. Structural and 
Multidisciplinary Optimization, 43(3), 419-442. doi:10.1007/s00158-010-0568-9 
Isukapalli, S. S. (1999). Uncertainty analysis of transport-transformation models. (Ph.D., 
Rutgers The State University of New Jersey - New Brunswick).ProQuest 
Dissertations and Theses, . (304542746). 
Jin, R., Chen, W., & Simpson, T. W. (2001). Comparative studies of metamodelling 
techniques under multiple modelling criteria. Structural and Multidisciplinary 
Optimization, 23(1), 1-13. doi:10.1007/s00158-001-0160-4 
Kass, R. E., Carlin, B. P., Gelman, A., & Neal, R. M. (1998). Markov chain Monte Carlo 
in practice: A roundtable discussion. The American Statistician, 52(2), 93-100. 
Kewlani, G., & Iagnemma, K. (2008). A stochastic response surface approach to 
statistical prediction of mobile robot mobility. Intelligent Robots and Systems, 2008. 
IROS 2008. IEEE/RSJ International Conference on, 2234-2239. 
doi:10.1109/IROS.2008.4651187 
 

 
 
137 
 
Lancaster, P., & Salkauskas, K. (1981). Surfaces generated by moving least squares 
methods. Mathematics of Computation, 37(155), 141-158. 
Lee, I. (2008). Reliability-based design optimization and robust design optimization 
using univariate dimension reduction method. (Ph.D., The University of 
Iowa).ProQuest Dissertations and Theses, . (304609574). 
Lee, I., Choi, K. K., Du, L., & Gorsich, D. (2008). Inverse analysis method using MPP-
based dimension reduction for reliability-based design optimization of nonlinear and 
multi-dimensional systems. Computer Methods in Applied Mechanics and 
Engineering, 198(1), 14-27. doi:http://dx.doi.org/10.1016/j.cma.2008.03.004 
Lee, I., Choi, K. K., Du, L., & Gorsich, D. (2008). Dimension reduction method for 
reliability-based robust design optimization. Computers & Structures, 86(13–14), 
1550-1562. doi:http://dx.doi.org/10.1016/j.compstruc.2007.05.020 
Lee, I., Choi, K. K., & Gorsich, D. (2010). System reliability-based design optimization 
using the MPP-based dimension reduction method. Structural and Multidisciplinary 
Optimization, 41(6), 823-839. doi:10.1007/s00158-009-0459-0 
Lee, I., Gorsich, D., Choi, K. K., Noh, Y., & Zhao, L. (2011). Sampling-based stochastic 
sensitivity analysis using score functions for RBDO problems with correlated random 
variables. Journal of Mechanical Design, 133(2), 021003-021003. 
doi:10.1115/1.4003186 
Levin, D. (1998). The approximation power of moving least-squares. Mathematics of 
Computation, 67(224), 1517-1531. 
Lewis, P. A. W., & Stevens, J. G. (1991). Nonlinear modeling of time series using 
multivariate adaptive regression splines (MARS). Journal of the American Statistical 
Association, 86(416), 864-877. doi:10.1080/01621459.1991.10475126 
Liu, Y., & Fang, Y. (2009). Design of the nonlinear system predictor driven by the 
bayesian-gaussian neural network of sliding window data. Computer and Information 
Science, 2(2), 26. 
Lunn, D., Spiegelhalter, D., Thomas, A., & Best, N. (2009). The BUGS project: 
Evolution, critique and future directions. Statistics in Medicine, 28(25), 3049-3067. 
doi:10.1002/sim.3680 
Lunn, D., Thomas, A., Best, N., & Spiegelhalter, D. (2000). WinBUGS - A bayesian 
modelling framework: Concepts, structure, and extensibility. Statistics and 
Computing, 10(4), 325-337. doi:10.1023/A:1008929526011 
Madsen, H. O., Krenk, S., & Lind, N. C. (1985). Methods of structural safety. Englewood 
Cliffs, NJ : Prentice-Hall. 
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). 
Equation of state calculations by fast computing machines. The Journal of Chemical 
Physics, 21(6), 1087-1092. doi:http://dx.doi.org/10.1063/1.1699114 
Mullur, A., & Messac, A. (2006). Metamodeling using extended radial basis functions: A 
comparative approach. Engineering with Computers, 21(3), 203-217. 
doi:10.1007/s00366-005-0005-7 
 

 
 
138 
 
Noh, Y., Choi, K. K., & Du, L. (2009). Reliability-based design optimization of problems 
with correlated input variables using a gaussian copula. Structural and 
Multidisciplinary Optimization, 38(1), 1-16. doi:10.1007/s00158-008-0277-9 
Noh, Y., Choi, K. K., & Lee, I. (2010). Identification of marginal and joint CDFs using 
bayesian method for RBDO. Structural and Multidisciplinary Optimization,40(1-6), 
35-51. doi:10.1007/s00158-009-0385-1 
Omre, H., & Halvorsen, K. (1989). The bayesian bridge between simple and universal 
kriging. Mathematical Geology, 21(7), 767-786. doi:10.1007/BF00893321 
Park, J., & Sandberg, I. W. (1991). Universal approximation using radial-basis-function 
networks. Neural Computation, 3(2), 246-257. doi:10.1162/neco.1991.3.2.246 
Rahman, S., & Wei, D. (2008). Design sensitivity and reliability-based structural 
optimization by univariate decomposition. Structural and Multidisciplinary 
Optimization, 35(3), 245-261. doi:10.1007/s00158-007-0133-3 
Rahman, S., & Wei, D. (2006). A univariate approximation at most probable point for 
higher-order reliability analysis. International Journal of Solids and Structures, 43(9), 
2820-2839. doi:http://dx.doi.org/10.1016/j.ijsolstr.2005.05.053 
Rahman, S. (2009). Stochastic sensitivity analysis by dimensional decomposition and 
score functions. Probabilistic Engineering Mechanics, 24(3), 278-287. 
doi:http://dx.doi.org/10.1016/j.probengmech.2008.07.004 
Rajashekhar, M. R., & Ellingwood, B. R. (1993). A new look at the response surface 
approach for reliability analysis. Structural Safety, 12(3), 205-220. 
doi:http://dx.doi.org/10.1016/0167-4730(93)90003-J 
Romero, D. A. (2008). A multi-stage, multi-response bayesian methodology for surrogate 
modeling in engineering design. (Ph.D., Carnegie Mellon University).ProQuest 
Dissertations and Theses, . (304667685). 
Romero, D. A., Amon, C., & Finger, S. (2003). Computers and information in 
engineering. Modeling Time-Dependent Systems using Multi-Stage Bayesian 
Surrogates, Washington, DC, USA. 47-47-57. doi:10.1115/IMECE2003-55049 
Romero, D. A., Amon, C. H., & Finger, S. (2012). Multiresponse metamodeling in 
simulation-based design applications. Journal of Mechanical Design, 134(9), 091001-
091001. doi:10.1115/1.4006996 
Rosenblatt, M. (1952). Remarks on a multivariate transformation. The Annals of 
Mathematical Statistics, 23(3), 470-472. 
Rubinstein, R. Y., & Shapiro, A. (1993). In Shapiro A. (Ed.), Discrete event systems : 
Sensitivity analysis and stochastic optimization by the score function method. 
Chichester England] ; New York: Chichester England ; New York : Wiley. 
Sacks, J., Welch, W. J., Toby J. Mitchell, & Wynn, H. P. (1989). Design and analysis of 
computer experiments. Statistical Science, 4(4), 409-423. 
 

 
 
139 
 
Sakata, S., Ashida, F., & Zako, M. (2007). On applying kriging-based approximate 
optimization to inaccurate data. Computer Methods in Applied Mechanics and 
Engineering, 196(13–16), 2055-2069. 
doi:http://dx.doi.org/10.1016/j.cma.2006.11.004 
Sakata, S., Ashida, F., & Zako, M. (2010). Comparative study on gradient and hessian 
estimation using the kriging method and neural network 
approximation. Mathematical and Computer Modelling, 51(3–4), 309-319. 
doi:http://dx.doi.org/10.1016/j.mcm.2009.08.016 
Sakata, S., Ashida, F., & Zako, M. (2008). Approximate structural optimization using 
kriging method and digital modeling technique considering noise in sampling 
data. Computers & Structures, 86(13–14), 1477-1485. 
doi:http://dx.doi.org/10.1016/j.compstruc.2007.05.007 
Shi, L., Yang, R. J., & Zhu, P. (2012). A method for selecting surrogate models in 
crashworthiness optimization. Structural and Multidisciplinary Optimization,46(2), 
159-170. doi:10.1007/s00158-012-0760-1 
Simpson, T. W., Poplinski, J. D., Koch, P. N., & Allen, J. K. (2001). Metamodels for 
computer-based engineering design: Survey and recommendations.Engineering with 
Computers, 17(2), 129-150. doi:10.1007/PL00007198 
Song, H., Choi, K. K., Lee, I., Zhao, L., & Lamb, D. (2011). Adaptive virtual support 
vector machine for the reliability analysis of high-dimensional problems. 
Washington, DC, USA. , 5 
Tierney, L. (1998). A note on metropolis-hastings kernels for general state spaces. The 
Annals of Applied Probability, 8(1), 1-9. 
Tu, J., Choi, K. K., & Park, Y. H. (1999). A new study on reliability-based design 
optimization. Journal of Mechanical Design, 121(4), 557-564. 
doi:10.1115/1.2829499 
Tu, J., Choi, K. K., & Park, Y. H. (2001). Design potential method for robust system 
parameter design. AIAA Journal, 39(4), 667-677. doi:10.2514/2.1360 
Ukrainec, A., Haykin, S., & McGregor, J. (1989). A neural network nonlinear 
predictor. Neural Networks, 1989. IJCNN., International Joint Conference on, 622 
vol.2. 
van der Merwe, R., Leen, T. K., Lu, Z., Frolov, S., & Baptista, A. M. (2007). Fast neural 
network surrogates for very high dimensional physics-based models in computational 
oceanography. Neural Networks, 20(4), 462-478. doi:10.1016/j.neunet.2007.04.023 
Wang, G. G., & Shan, S. (2007). Review of metamodeling techniques in support of 
engineering design optimization. Journal of Mechanical Design, 129(4), 370-380. 
Wei, D. L., Cui, Z. S., & Chen, J. (2008). Uncertainty quantification using polynomial 
chaos expansion with points of monomial cubature rules. Computers & 
Structures, 86(23–24), 2102-2108. 
doi:http://dx.doi.org/10.1016/j.compstruc.2008.07.001 
 

 
 
140 
 
Wei, D. (2006). A univariate decomposition method for higher-order reliability analysis 
and design optimization. (Ph.D., The University of Iowa). ProQuest Dissertations 
and Theses, . (305309521). 
Wiener, N. (1938). The homogeneous chaos. American Journal of Mathematics, 60(4), 
897-936. 
Wu, Y. -. (1994). Computational methods for efficient structural reliability and reliability 
sensitivity analysis. AIAA Journal, 32(8), 1717-1723. doi:10.2514/3.12164 
Wu, Y. -., MILLWATER, H. R., & CRUSE, T. A. (1990). Advanced probabilistic 
structural analysis method for implicit performance functions. AIAA Journal, 28(9), 
1663-1669. doi:10.2514/3.25266 
Xu, H., & Rahman, S. (2003). A moment-based stochastic method for response moment 
and reliability analysis. Proceedings of 2nd MIT Conference on Computational Fluid 
and Solid Mechanics, Cambridge, MA. 
Xu, H., & Rahman, S. (2004). A generalized dimension-reduction method for 
multidimensional integration in stochastic mechanics. International Journal for 
Numerical Methods in Engineering, 61(12), 1992-2019. doi:10.1002/nme.1135 
Youn, B. D., & Choi, K. K. (2003). An investigation of nonlinearity of reliability-based 
design optimization approaches. Journal of Mechanical Design, 126(3), 403-411. 
doi:10.1115/1.1701880 
Youn, B. D., Choi, K. K., & Du, L. (2005). Enriched performance measure approach for 
reliability-based design optimization. AIAA Journal, 43(4), 874-884. 
doi:10.2514/1.6648 
Youn, B. D., Park, Y. H., & Choi, K. K. (2003). Hybrid analysis method for reliability-
based design optimization. Journal of Mechanical Design, 125(2), 221-232. 
doi:10.1115/1.1561042 
Zhao, L. (2011). Reliability-based design optimization using surrogate model with 
assessment of confidence level. (Ph.D., The University of Iowa). ProQuest 
Dissertations and Theses, . (894769536). 
Zhao, L., Choi, K. K., & Lee, I. (2011). Metamodeling method using dynamic kriging for 
design optimization. AIAA Journal, 49(9), 2034-2046. doi:10.2514/1.J051017 
Zobel, C. W., & Keeling, K. B. (2008). Neural network-based simulation metamodels for 
predicting probability distributions. Computers & Industrial Engineering,54(4), 879-
888. doi:10.1016/j.cie.2007.08.012 
 
 

