Dialogue Discourse-Aware Graph Model and Data Augmentation
for Meeting Summarization
Xiachong Feng , Xiaocheng Feng , Bing Qin , Xinwei Geng
Harbin Institute of Technology, China
{xiachongfeng, xcfeng, bqin, xwgeng}@ir.hit.edu.cn
Abstract
Meeting summarization is a challenging task due
to its dynamic interaction nature among multiple
speakers and lack of sufﬁcient training data. Ex-
isting methods view the meeting as a linear se-
quence of utterances while ignoring the diverse re-
lations between each utterance. Besides, the lim-
ited labeled data further hinders the ability of data-
hungry neural models. In this paper, we try to miti-
gate the above challenges by introducing dialogue-
discourse relations. First, we present a Dialogue
Discourse-Aware Meeting Summarizer (DDAMS)
to explicitly model the interaction between utter-
ances in a meeting by modeling different discourse
relations. The core module is a relational graph
encoder, where the utterances and discourse rela-
tions are modeled in a graph interaction manner.
Moreover, we devise a Dialogue Discourse-Aware
Data Augmentation (DDADA) strategy to construct
a pseudo-summarization corpus from existing input
meetings, which is 20 times larger than the original
dataset and can be used to pretrain DDAMS. Exper-
imental results on AMI and ICSI meeting datasets
show that our full system can achieve SOTA per-
formance Our codes and outputs are available at:
https://github.com/xcfcode/DDAMS/.
1
Introduction
Meeting summarization aims to distill the most important in-
formation from a recorded meeting into a short textual pas-
sage, which could be of great value to people by provid-
ing quick access to the essential content of past meetings
[Gurevych and Strube, 2004]. An example is shown in Figure
1, which includes 3 speakers (A, B and C) and their corre-
sponding utterances, as well as a human-written summary.
Recently,
neural-based sequence-to-sequence methods
have become the mainstream mode for generating meeting
summaries via accurately modeling the reﬁned semantic rep-
resentation of the original meeting text. Goo and Chen [2018]
incorporate utterance-level dialogue acts to enhance the rep-
resentation for each utterance. Li et al. [2019] consider topics
as a structure information to enrich the meeting representa-
tion. However, we claim that the current research suffers from
A : 
B : 
C :  
C :
Summary
A asked whether to include a battery charger. B 
answered his question. However, C disagrees with A  
since it would increase the final cost.
Parts of the Meeting
What if we have a battery charger? 
You can have neat design for it.
It would increase the cost.
We have to change the end cost.
Contrast
Continuation 
QA
Figure 1: An example of a meeting with its corresponding sum-
mary. QA, Contrast and Continuation are dialogue discourse rela-
tions, which explicitly show the interaction between utterances.
two problems. One is sequential text modeling. A meeting is
a dynamic information exchange ﬂow, which is informal, ver-
bose, and less structured than traditional documents [Sacks et
al., 1978]. But all previous works adopt a sequential mod-
eling strategy for encoding the meeting, hindering the explo-
ration of inherently rich interactive relations between utter-
ances, which makes the meeting modeling inadequate. The
other one is the lack of sufﬁcient training data. As we know,
the scale of corpus is signiﬁcant to the performance of neural
models, the data size of meeting summarization is only one-
thousandth of the traditional news summarization (e.g. 137
meetings in AMI v.s. 312K articles in CNN/DM).
To address the above concerns, we draw support from dia-
logue discourse, a dialogue-speciﬁc structure, which can pro-
vide pre-deﬁned relations between utterances [Stone et al.,
2013]. As shown in Figure 1, QA, Contrast and Continua-
tion are three dialogue discourse relations, which explicitly
show the information ﬂow and interactions between utter-
ances. After knowing these relations, we can generate better
summaries. Furthermore, we ﬁnd that a question often sparks
a discussion in a meeting. As shown in Figure 1, the dis-
cussion about “design” and “cost” surrounds the salient term
“battery charger” in the question. Therefore, we assume that
a question tends to contain core topics or key concepts that
can be used as a pseudo summary for subsequent discussion.
In this paper, we propose a Dialogue Discourse-Aware
Meeting Summarizer (DDAMS, see §3) for modeling the di-
alogue discourse relations among utterances. Speciﬁcally, we
ﬁrst convert meeting utterances with discourse relations into
a meeting graph where utterance vertices interact with rela-
arXiv:2012.03502v2  [cs.CL]  19 May 2021

U1
Contrast
Continuation
QA
U2
U3
U4
U1
U2
U4
U3
Dialogue Discourse Parser
(b) Levi Graph Transformation
U1
U2
U3
U4
QA
Contrast
Continuation
Default
Reverse
(a) Dialogue Discourse Parsing
(c) Levi Graph with 
global node and self edges
U1
U2
U3
U4
QA
Contrast
Continuation
QA
Contrast
Continuation
(d) Meeting Graph
G
U1
U2
U3
U4
G
Self
Global
Default-In-
Discourse
Default-Out-
Discourse
Reverse-In-
Discourse
Reverse-Out-
Discourse
Default
Reverse
Self
Global
Figure 2: Illustration of meeting graph construction process.
tion vertices. Then we design a graph-to-sequence framework
to generate meeting summaries. Besides, to tackle the in-
sufﬁcient training problem, we devise a Dialogue Discourse-
Aware Data Augmentation (DDADA, see §4) strategy, which
constructs a pseudo-summarization corpus from existing in-
put meetings. In detail, we use the QA discourse relation
to identify the question as pseudo-summary, then we view
subsequent utterances with associated discourse relations as
a pseudo-meeting.
Finally, we can construct a pseudo-
summarization corpus, which is 20 times larger than the orig-
inal dataset and can be used to pretrain DDAMS.
We conduct various experiments on the widely used AMI
[Carletta et al., 2005] and ICSI [Janin et al., 2003] datasets.
The results show the effectiveness of DDAMS method and
DDADA strategy. In summary, (1) We make the ﬁrst attempt
to successfully explore dialogue discourse to model the utter-
ances interactions for meeting summarization; (2) We devise
a dialogue discourse-aware data augmentation strategy to al-
leviate the data insufﬁciency problem; (3) Extensive experi-
ments show that our model achieves SOTA performance.
2
Preliminaries
In this section, we describe the task deﬁnition and give a brief
introduction to dialogue discourse.
2.1
Task Deﬁnition
Meeting summarization aims at producing a summary Y for
the input meeting U, where U consists of |U| utterances
[u1, u2, ...u|U|] and Y consists of |Y| words [y1, y2, ...y|Y|].
The i-th utterance of the meeting can be represented as a se-
quence of words ui = [ui,1, ui,2, ...ui,|ui|], where ui,j de-
notes the j-th word of i-th utterance. Each utterance ui asso-
ciates with a speaker pi ∈P , P being a set of participants.
2.2
Dialogue Discourse
Dialogue discourse indicates relations between discourse
units in a conversation (i.e. utterances in a meeting). This
dependency-based structure allows relations between non-
adjacent utterances that is applicable for multi-party con-
versations [Li et al., 2021].
There are 16 discourse rela-
tions [Asher et al., 2016] in total: comment, clariﬁcation-
question, elaboration, acknowledgment, continuation, expla-
nation, conditional, QA, alternation, question-elaboration,
result, background, narration, correction, parallel, contrast.
3
Dialogue Discourse-Aware Meeting
Summarizer
In this section, we present our Dialogue Discourse-Aware
Meeting Summarizer (DDAMS) that consists of four com-
ponents: a meeting graph, a vertex representation module, a
graph encoder, and a pointer decoder, as shown in Figure 3.
3.1
Meeting Graph
Given meeting utterances, we ﬁrst use a SOTA dialogue dis-
course parser [Shi and Huang, 2019] to get discourse rela-
tions, one relation connects from one utterance to another one
with a relation type, as shown in Figure 2(a).
Then, we perform the Levi graph transformation, which
turns labeled edges into additional vertices [Gross et al.,
2013]. Through this transformation, we can model discourse
relations explicitly and update the utterance and discourse
vertices simultaneously. There are two types of edges in the
Levi graph: default and reverse. For example, an edge (U3,
Contrast, U1) in the original graph becomes (U3, default,
Contrast) and (Contrast, default, U1) in the Levi graph, as
shown in Figure 2(b).
Furthermore, to aggregate non-local information, a global
vertex is added that connects all vertices by global edges and
will be used to initialize the decoder. We also add self edges
to aggregate self-information, as shown in Figure 2(c).
Note that different types of vertices may have different fea-
tures that fall in different space [Beck et al., 2018]. Specif-
ically, previous works ignore the type of source and target
vertices and use the same type of edge to pass information,
which may reduce the effectiveness of discourse information.
Thus, we propose our meeting graph which transforms the de-
fault edge into default-in-discourse and default-out-discourse
edges and the reverse edges into reverse-in-discourse and
reverse-out-discourse edges, as shown in Figure 2(d).
Let GM = (VM, EM, RM) denote a meeting graph, with
vertices vi ∈VM and labeled edges (vi, r, vj) ∈EM, where
r ∈RM is the relation type of the edge.
For our meet-
ing graph, there are six types of relations, where RM be-
comes default-in-discourse, default-out-discourse, reverse-
in-discourse, reverse-out-discourse, global, self.
3.2
Vertex Representation
Vertex representation module aims to obtain initial repre-
sentations for three types of vertices: global vertex, rela-
tion vertex and utterance vertex.
For global vertex and
relation vertices, we get the initial representation h0
i by

u4,1
u4,2
u4,3
u4,4
Attention and Copy
Vertex Representation
Graph Encoder
Pointer Decoder
Final 
Distribution
p4
p4
p4
p4
h0
4
h0
4,4
Word Attention
h0
1,1
h 0
2,1
h 0
3,1
…
…
…
h0
4,3
h0
4,2
h0
4,1
Meeting Graph
QA
u1
u2
u3
u4
G
Contr
Conti
Utterance Attention
Embedding Table
Figure 3: Illustration of our DDAMS model. (1) First, we construct our meeting graph consisting of three types of vertices: global vertex,
utterance vertex and discourse relation vertex. (2) Then, the vertex representation module gives each type of vertex an initial representation.
(3) Further, the graph encoder performs convolutional computation over the meeting graph based on the relational graph convolutional
network. (4) Finally, the pointer decoder attends to the updated utterance representations and the word representations to generate the
summary words either from the ﬁxed-length vocabulary or copy from the input.
looking up from an embedding table.
For utterance ver-
tices, we employ a BiLSTM1 as the utterance encoder that
encodes the utterance forwardly and backwardly [Qin et
al., 2020].
As −−→
hi,j = LSTMf
−−−→
hi,j−1, ei,j

, ←−−
hi,j =
LSTMb
←−−−
hi,j+1, ei,j

, hi,j = [−−→
hi,j; ←−−
hi,j], where hi,j and
ei,j denote the hidden state and embedding of word ui,j. ;
denotes concatenation. To involve speaker information, we
encode speaker pi using a one-hot vector and get ei,j by con-
catenating word embedding and one-hot speaker embedding
[Wang et al., 2020]. The representation h0
i =[−−−→
hi,|ui|; ←−−
hi,1] is
used as input to the graph encoder.
3.3
Graph Encoder
After getting the initial feature h0
i of each vertex vi ∈VM,
we feed them into the graph encoder to digest the structural
information. We use relational graph convolutional networks
[Schlichtkrull et al., 2018] to capture high-level hidden fea-
tures considering different types of edges. The convolutional
computation for vi at the (l + 1)-th layer takes the represen-
tation h(l) at the l-th layer as input can be deﬁned as:
h(l+1)
i
= RELU

X
r∈RM
X
vj∈Nr
i
1
|Nr
i |W (l)
r h(l)
j


(1)
where Nr
i denotes the set of neighbors of vertex vi under re-
lation r and W (l)
r
denotes relation-speciﬁc learnable param-
eters at the l-th layer.
However, uniformly accepting information from different
discourse relations is not suitable for identifying important
discourse. Thus, we use the gate mechanism [Marcheggiani
and Titov, 2017] to control the information passing:
g(l)
j
= sigmoid

W (l)
g,rh(l)
j

(2)
where W (l)
g,r denotes a learnable parameter under relation
type r at the l-th layer. Equipped with the gate mechanism,
1We have also tried Transformer [Vaswani et al., 2017] as our
backbone model. However, it has too many parameters which can
easily over-ﬁt on the small meeting dataset.
the convolutional computation can be deﬁned as:
h(l+1)
i
= RELU

X
r∈RM
X
vj∈Nr
i
g(l)
j
1
|Nr
i |W (l)
r h(l)
j


(3)
3.4
Pointer Decoder
We use a standard LSTM decoder with attention and copy
mechanism to generate the summary [Bahdanau et al., 2015;
See et al., 2017]. The global representation is used to ini-
tialize the decoder. At each step t, the decoder receives the
word embedding of previous word and has decoder state st.
The attention distribution is calculated as in [Luong et al.,
2015]. We consider both word-level and utterance-level at-
tention. The word-level context vector hwl
t
is computed as:
et
i,j = s⊤
t W ah0
i,j
at = softmax(et)
hwl
t
=
X
i
X
j
at
i,jh0
i,j
(4)
where W a is a learnable parameter and h0
i,j is obtained from
utterance encoder for ui,j. The utterance-level context vector
hul
t
is calculated similarly to the word-level context vector,
except that we use the ﬁnal outputs of the graph encoder h(l)
i
which represent utterances to calculate the attention distribu-
tion. The ﬁnal context vector is the concatenation of word-
level and utterance-level context vector h∗
t = [hwl
t ; hul
t ],
which is then used to calculate generation probability and the
ﬁnal probability distribution [See et al., 2017].
3.5
Training Objective
We use maximum likelihood estimation to train our model.
Given the ground truth summary Y∗= [y∗
1, y∗
2, ..., y∗
|Y∗|] for
an input meeting U. We minimize the negative log-likelihood
of the target words sequence:
L = −
|Y∗|
X
t=1
log p
 y∗
t |y∗
1 . . . y∗
t−1, U

(5)

Parts of the Meeting
contrast
continuation
continuation
elaboration
we have different colour  
we should have standard colour  
how about black, black is always 
the standard . 
yellow also regular remote colour
yellow seems not suitable
blue is more popular
sounds great
B:
A:
B:
B:
A:
B:
A:
what's the standard colour?  
A:
Pseudo Summary
Pseudo Meeting
acknowledgment
QA
AMI or ICSI 
Train Set Meetings
Figure 4:
Illustration of how to construct a pseudo meeting-
summary pair. Given a meeting from the original meeting train set,
we use QA discourse relation to identify the question in the meeting.
Then, the subsequent discussion with discourse relations becomes a
pseudo meeting and the question becomes a pseudo summary.
4
Dialogue Discourse-Aware Data
Augmentation
In this section, we introduce Dialogue Discourse-Aware
Data Augmentation strategy (DDADA), which constructs a
pseudo-summarization corpus from the original input meet-
ings based on the QA discourse relation.
4.1
Pseudo-summarization Corpus Construction
Given a meeting and its corresponding discourse relations, we
ﬁnd a question often sparks a discussion and contains salient
terms or concepts expressed in the discussion. As shown in
Figure 4, speaker A asked “What’s the standard colour?”,
other participants start to discuss this small topic. Thus, we
can view the discussion as a pseudo meeting and the question
as a pseudo summary of this pseudo meeting2.
According to the above observation, we collect pseudo-
summarization data from the original meeting summarization
dataset where the question identiﬁed by QA discourse serves
as the pseudo summary and N utterances after the question
with their associated discourse relations serve as the pseudo
meeting3. Note that there are some uninformative and normal
questions such as “what is this here”, which are not suitable
for pseudo-summarization corpus construction. Thus, we ﬁl-
ter out questions that contain no noun and adjective to make
the pseudo data more realistic. Table 1 shows the statistics
for the pseudo-summarization corpus.
4.2
Pretraining
Given the pseudo corpus, we ﬁrst pretrain DDAMS through
a question generation pretraining objective, which pretrains
DDAMS to generate questions conditioning on subsequent
discussions. Afterward, we ﬁne-tune DDAMS on the meeting
summarization dataset. Narayan et al. [2020] also reveal that
treating question generation as the pretraining objective can
help downstream tasks in identifying important contents in
the open domain. Our motivations are two-fold: (1) we can
potentially augment the training data; (2) since the pseudo
data is constructed from the meeting summarization dataset
(in-domain), the pre-training can give the model a warm start.
2In our preliminary experiments, we randomly sample 100
pseudo-summarization instances and observe that 68% of questions
can cover important information, such as salient terms and concepts.
3We conduct experiments with N ∈[6 : 12]. Finally, we choose
N=10 which performs the best.
AMI
Pseudo Corpus
ICSI
Pseudo Corpus
# of Original Data
97
53
# of Pseudo Data
1539
1877
Avg.Tokens
124.44
107.44
Avg.Sum
13.18
11.97
Table 1: Pseudo-summarization corpus statistics. “# of Original
Data” means the number of original meetings in the train set, “#
of Pseudo Data” means the number of pseudo meeting-summary
pairs, “Avg.Tokens” means the average length of pseudo meetings
and “Avg.Sum” means the average length of pseudo summaries.
5
Experiments
Datasets We experiment on AMI [Carletta et al., 2005] and
ICSI [Janin et al., 2003] datasets. We preprocess the dataset
into train, valid and test sets for AMI (97/20/20) and ICSI
(53/25/6) separately following Shang et al. [2018]. We get
discourse relations for one meeting based on the SOTA di-
alogue discourse parser, namely Deep Sequential [Shi and
Huang, 2019], which is trained on the STAC corpus [Asher
et al., 2016].4
Evaluation Metrics We adopt ROUGE [Lin, 2004] for eval-
uation and obtain the F1 scores for ROUGE-1, ROUGE-2,
and ROUGE-L between the ground-truth and the generated
summary respectively.
Baseline Models TextRank [Mihalcea and Tarau, 2004] is
a graph-based extractive method. SummaRunner [Nallap-
ati et al., 2017] is an extractive method based on hierarchi-
cal RNN network. UNS [Shang et al., 2018] is an unsuper-
vised abstractive method that combines several graph-based
approaches. Pointer-Generator [See et al., 2017] is an ab-
stractive method equips with copy mechanism. HRED [Ser-
ban et al., 2016] is a hierarchical Seq2Seq model. Sentence-
Gated [Goo and Chen, 2018] is an abstractive method that
incorporates dialogue acts by the sentence-gated mechanism.
TopicSeg [Li et al., 2019] is an abstractive method using a
hierarchical attention mechanism at three levels (topic, ut-
terance, word).5 HMNet [Zhu et al., 2020] is an abstrac-
tive method that employs a hierarchical model and incorpo-
rates part-of-speech and entity information, which is also pre-
trained on large-scale news summarization dataset6.
5.1
Automatic Evaluation
As shown in Table 2, our model DDAMS outperforms vari-
ous baselines, which shows the effectiveness of dialogue dis-
course. By pre-training on pseudo-summarization data, our
model DDAMS +DDADA can further boost the performance
by a large margin, which shows the need for DDADA. Spe-
cially, DDAMS +DDADA (w/o ﬁne-tune) still achieves a ba-
4More information about statistics for datasets, implementation
details and relation distributions are shown in the supplementary ﬁle.
5[Li et al., 2019] also proposed TopicSeg+VFOA by incorpo-
rating vision features in a multi-modal setting. In this paper, we
compare our model with baselines using only textual features.
6Zhu et al. [2020] directly use news summarization dataset to
pre-train their model, which can not be applied to our model, since
our DDAMS needs dialogue discourse information.

AMI
ICSI
Model
R-1
R-2
R-L
R-1
R-2
R-L
Extractive
TextRank [Mihalcea and Tarau, 2004]
35.19
6.13
15.70
30.72
4.69
12.97
SummaRunner [Nallapati et al., 2017]
30.98
5.54
13.91
27.60
3.70
12.52
Abstractive
UNS [Shang et al., 2018]
37.86
7.84
13.72
31.73
5.14
14.50
Pointer-Generator [See et al., 2017]
42.60
14.01
22.62
35.89
6.92
15.67
HRED [Serban et al., 2016]
49.75
18.36
23.90
39.15
7.86
16.25
Sentence-Gated [Goo and Chen, 2018]
49.29
19.31
24.82
39.37
9.57
17.17
TopicSeg [Li et al., 2019]
51.53
12.23
25.47
-
-
-
HMNet [Zhu et al., 2020]
52.36
18.63
24.00
45.97
10.14
18.54
Ours
DDAMS
51.42
20.99
24.89
39.66
10.09
17.53
DDAMS + DDADA
53.15
22.32
25.67
40.41
11.02
19.18
DDAMS + DDADA (w/o ﬁne-tune)
28.35
4.67
14.92
25.94
4.18
13.92
Table 2: Test set results on AMI and ICSI Datasets, where “R-1” is short for “ROUGE-1”, “R-2” for “ROUGE-2”, “R-L” for “ROUGE-L”. The
DDAMS represents the model that is trained only on the meeting dataset. The DDAMS +DDADA means the model that is pre-trained using
pseudo-summarization data and then ﬁne-tuned on meeting dataset. DDAMS +DDADA (w/o ﬁne-tune) means the model that is pre-trained
using pseudo-summarization data and without ﬁne-tuning on the meeting dataset.7
sic effect, which appears DDAMS +DDADA (w/o ﬁne-tune)
can simulate as a summarization model in terms of selecting
important information.
5.2
Human Evaluation
To further assess the quality of the generated summaries, we
conduct a human evaluation study. We choose two metrics:
relevance (consistent with the original input) and informa-
tiveness (preserves the meaning expressed in the ground-
truth). We hired ﬁve graduates to perform the human eval-
uation. They were asked to rate each summary on a scale of
1 (worst) to 5 (best) for each metric. The results are shown in
Table 3.
Model
Relevance
Informativeness
AMI
Ground-truth
4.60
4.56
Sentence-Gated
3.16
3.60
HMNet
3.60
3.72
DDAMS
3.80
3.76
DDAMS +DDADA
3.84
3.88
ICSI
Ground-truth
4.76
4.48
Sentence-Gated
3.32
3.48
HMNet
3.80
3.52
DDAMS
3.76
3.28
DDAMS +DDADA
3.84
3.60
Table 3: Human evaluation results.
We can see that our DDAMS + DDADA achieves higher
scores in both relevance and informativeness than other base-
lines. Ground truth obtains the highest scores compare with
generated summaries indicating the challenge of this task.
7ROUGE-2 of TopicSeg is signiﬁcantly lower than other base-
lines, which contains some anomalies. we reproduce the results of
UNS and HMNet by the ofﬁcial codes and evaluate them by Py-
Rouge package, the results are consistent with the original paper,
which shows the increase of our ROUGE-2 is reasonable.
5.3
Analysis
Effect of the number of dialogue discourse.
We conduct
experiments by randomly providing parts of discourse rela-
tions to our model DDAMS in the test process. The results
are shown in Figure 5. We can see that the more discourse
relations, the higher the ROUGE score, which indicates dis-
course can do good to summary generation. When given no
discourse information, our model gets similar scores com-
pared with HRED, which models utterances sequentially.
22.00
22.10
22.20
22.30
22.40
31.20
31.50
31.80
32.10
32.40
0%
20%
40%
60%
80%
100%
AMI
ICSI
Percentage of dialogue discourse
Figure 5: Average ROUGE scores with respect to the number of
dialogue discourse relations given at test process.
Effect of the quality of dialogue discourse.
We train our
DDAMS by using discourse relations that are provided by
parsers of different qualities8. We can see that the higher
the quality of discourse parser, the higher the ROUGE score,
which potentially indicates high-quality discourse relations
can further improve our model.
21.45
21.68
21.91
22.14
22.37
31.30
31.55
31.80
32.05
32.30
15%
30%
45%
60%
73%
AMI
ICSI
Quality of discourse parser
(𝐹! score (%) for link prediction on STAC corpus)
Figure 6: Average ROUGE scores with respect to the quality of dis-
course parser.
8We choose parsers of different qualities based on the link pre-
diction F1 score on the STAC corpus test set.

22.10
22.28
22.45
22.63
22.80
31.30
31.55
31.80
32.05
32.30
AMI
ICSI
Contrast
Acknowledgement
Conditional
Background
Comment
Explanation
Question-elaboration
Continuation
Narration
QA
Correction
Elaboration
Clarification-question
Parallel
Result
Alternation
ICSI
AMI
Figure 7: Average ROUGE scores with respect to different types of
discourse relations given at test process.
Effect of the type of dialogue discourse.
To verify the im-
portance of each discourse relation type, we then test our
model by giving different discourse relations. The results are
shown in Figure 7. On AMI dataset, the participants take a
remote control design project. We can see that Conditional
and Background provide more accurate and adequate infor-
mation, facilitating summary generation. On ICSI dataset,
meetings focus on academic discussion. We can see that the
relation Result is more important than other relations, we at-
tribute this to the fact that this relation can strongly indicate
the outcome of a discussion that covers salient information.
Effect of meeting graph.
To verify the effectiveness of de-
signing different types of edges for different types of vertices,
we replace our meeting graph with Levi graph (shown in Fig-
ure 2(c)), namely DDAMS (w/ Levi graph). The results are
shown in Table 4. We can see that taking the type of vertices
into consideration, our model DDAMS can get better results.
Model
R-1
R-2
R-L
AMI
DDAMS
51.42
20.99
24.89
DDAMS (w/ Levi graph)
51.46
20.75
24.31
ICSI
DDAMS
39.66
10.09
17.53
DDAMS (w/ Levi graph)
39.20
9.54
17.48
Table 4: Test set results of using meeting graph and Levi graph.
Effect of pseudo-summarization data.
To further study
the effectiveness of pseudo-summarization data, instead of
using questions identiﬁed by dialogue discourse as pseudo
summaries, we extract questions following two rules: (1) ut-
terances that begin with WH-words (e.g. “what”), (2) utter-
ances that end with a question mark. We call this Rule-based
Data Augmentation (RBDA). We ﬁrst pre-train our DDAMS
on the two types of pseudo data, which are constructed based
on DDADA and RBDA separately. And then ﬁne-tune it on
the meeting dataset. The results are shown in Table 5. We
can see that DDADA is better than RBDA, demonstrating the
effectiveness of dialogue discourse. Besides, pretraining on
pseudo-summarization data constructed based on RBDA still
achieves a better result, which indicates the rationality of our
pretraining strategy.
Others.
We show more analyses in the supplementary ﬁle,
including: (1) utterance-level and word-level attention mech-
anisms; (2) ﬁltering out useless relations according to Figure
7; (3) the case study.
Model
R-1
R-2
R-L
AMI
DDAMS
51.42
20.99
24.89
+ RBDA
52.94
21.96
25.05
+ DDADA
53.15
22.32
25.67
ICSI
DDAMS
39.66
10.09
17.53
+ RBDA
39.42
10.60
18.19
+ DDADA
40.41
11.02
19.18
Table 5: The results of pre-training DDAMS on different types of
pseudo-summarization data.
6
Related Work
Meeting Summarization With the abundance of automatic
meeting transcripts, meeting summarization attracts more and
more attention [Murray et al., 2010; Shang et al., 2018;
Zhao et al., 2019]. There are two main paradigms in meet-
ing summarization. In the ﬁrst paradigm, some works try to
incorporate auxiliary information for better modeling meet-
ings. Goo and Chen [2018] incorporated dialogue acts. Li
et al. [2019] incorporated topics and vision features. Ganesh
and Dingliwal [2019] reconstructed the dialogue into a se-
quence of words based on discourse labels. Koay et al. [2020]
investigated the impact of domain terminologies. In the sec-
ond paradigm, some works used news summarization data to
alleviate the insufﬁcient training problem. Zhu et al. [2020]
used news data to pretrain their model. Ganesh and Dingli-
wal [2019] used a news summarizer to generate meeting sum-
maries in a zero-shot manner. In this paper, we propose to
use dialogue discourse relations as the auxiliary information
to model the interaction between utterances and construct
a pseudo-summarization corpus, addressing the data insufﬁ-
ciency problem.
Discourse-Aware News Summarization Some works ap-
plied a constituency-based discourse structure,
namely
Rhetorical Structure Theory [Mann and Thompson, 1988] to
news summarization [Liu and Chen, 2019; Xu et al., 2020].
These works aimed to analyze the core part of a given sen-
tence and extracted sub-sentential discourse units to form the
summary. In this paper, we introduce the dependency-based
structure, namely dialogue discourse, for exploring the di-
verse interactive relations between utterances.
7
Conclusion
In this paper, we apply the dialogue discourse to model the
diverse interactions between utterances and present a Dia-
logue Discourse-Aware Meeting Summarizer (DDAMS). We
design a meeting graph to facilitate the information ﬂow
and demonstrate that incorporating dialogue discourse is ef-
fective for this task.
Moreover, we propose a Dialogue
Discourse-Aware Data Augmentation (DDADA) strategy to
alleviate the insufﬁcient training problem. We build a pseudo-
summarization corpus by utilizing the QA discourse relation.
Experiments on AMI and ICSI datasets show that our model
achieves new SOTA performances.

Acknowledgements
This work is supported by the National Key R&D Program
of China via grant 2018YFB1005103 and National Natural
Science Foundation of China (NSFC) via grant 61906053 and
61976073. We thank all the anonymous reviewers for their
insightful comments. We also thank Libo Qin, Yibo Sun and
Jiaqi Li for helpful discussion.
A
Experimental Details
Implementation Details
For our DDAMS, the dimension
of hidden states is set to 200. The embedding size is 300. We
use Adam with learning rate of 0.001. Dropout rate is set to
0.5. In test process, beam size is set to 10. For pre-training,
we stop training until the model converges on pseudo data.
For discourse parser training, we use default parameters and
set vocabulary size to 2,5009.
Baseline Codes
Ofﬁcial codes for each baseline model:
• SummaRunner: https://github.com/kedz/nnsum
• UNS: https://bitbucket.org/dascim/acl2018 abssumm
• Pointer-Generator: https://github.com/OpenNMT
• Sentence-Gated: https://github.com/MiuLab/DialSum
• HMNet: https://github.com/microsoft/HMNet
B
Details of AMI and ICSI Corpus
Statistics for AMI [Carletta et al., 2005] and ICSI [Janin et
al., 2003] are shown in Table 6. “#” means the number of
meetings in the dataset, “Avg.Turns” means the average turns
of all meetings, “Avg.Tokens” means the average length of
meetings and “Avg.Sum” means the average length of sum-
maries.
AMI
ICSI
#
137
59
Avg.Turns
289
464
Avg.Tokens
4,757
10,189
Avg.Sum
322
534
Table 6: Statistics for AMI and ICSI datasets.
C
Distribution of Discourse Relations
We get dialogue discourse relations for one meeting based on
Deep Sequential [Shi and Huang, 2019], a SOTA dialogue
discourse parser which is trained on the STAC corpus [Asher
et al., 2016]. The ﬁnal relations distributions for AMI and
ICSI are shown in Figure 8.
D
Effect of attention mechanisms
We conduct ablation studies to show the effectiveness of dif-
ferent types of attention mechanism. The results are shown
in Table 7. We can see that word attention is more important.
9https://github.com/shizhouxing/DialogueDiscourseParsing
Model
R-1
R-2
R-L
AMI
DDAMS
51.42
20.99
24.89
w/o utter-attn
51.22
20.57
25.02
w/o word-attn
50.27
19.81
23.91
ICSI
DDAMS
39.66
10.09
17.53
w/o utter-attn
39.59
9.90
17.24
w/o word-attn
38.96
9.61
17.40
Table 7: Ablation study for attention mechanism. utter-attn indi-
cates utterance-level attention and word-attn indicates word-level at-
tention.
Model
R-1
R-2
R-L
AMI
DDAMS
51.42
20.99
24.89
ﬁlter-useless-3
51.28
19.68
23.84
ﬁlter-useless-5
51.44
20.26
24.11
ICSI
DDAMS
39.66
10.09
17.53
ﬁlter-useless-3
39.71
9.64
17.46
ﬁlter-useless-5
39.21
9.52
17.33
Table 8: The results of ﬁltering out N useless relations.
However, our model achieves the best performance by equip-
ping both word and utterance attentions. The results reveal
the importance of combining both attention mechanisms for
meeting summarization task.
E
Effect of the type of dialogue discourse
Previous experiments reveal that different relations have dif-
ferent degrees of contribution. Thus, we ﬁlter out N useless
relations to see the ﬁnal results (shown in Table 8). We can
ﬁnd that compared with using all relations, ﬁltering some use-
less relations will result in performance degradation to some
extent. We attribute this to the fact that although these dis-
course relations are of small effect, directly removing them
from the meeting graph will do harm to the semantic coher-
ence of the entire graph, which will further hurt the model
performance.
F
Case study
Table 9 shows summaries generated by different models and
the visualization of utterance attention weights. The darker
the color, the higher the weight. Sentence-Gated [Goo and
Chen, 2018] focuses more on the second utterance than the
third utterance (shown in Table 9(a)). The second one mainly
talks about “fruit shape” which leads to the omission of the
keyword “vegetable”. Differently, by introducing dialogue
discourse, Utterance 1 and 3 are both related to two utter-
ances, which make them the core nodes of our graph (shown
in Table 9(b)), they are discussions around the keywords
“fruit” and “vegetable”, so our model can generate a better
summary that contains both keywords.

AMI
ICSI
Train
Train
Valid
Valid
Test
Test
Figure 8: Relation distribution statistics.
The fashion trends are that people want sort of clothes 
and shoes and things with fruit and vegetables theme .
If you start making the buttons fruit shaped, it might 
make it more complicated to use .
Fruit and vegetables may be popular at the moment but 
as we know how fickle the fashion markets are.
It just seems realistic that the remote control market isn't 
the thing which takes in those kinds of fashion trends .
Marketing
Expert
User
Interface
Project
Manager
Project
Manager
𝑈!
𝑈"
𝑈#
𝑈$
:
:
:
:
Inform
Access
Inform
Inform
𝑈!
𝑈"
𝑈#
𝑈$
Contrast
Continuation
Elaboration
DDAMS
Sentence
-Gated
(b) 
(a) 
Ground-truth
The Marketing Expert presented trends in the remote control market and the fruit and vegetable
and spongy material trends in fashion.
Pointer-Generator
They discussed the possibility of a fruit or fruit and fruit.
Sentence-Gated
The need to incorporate a fruit theme into the design of the remote.
DDAMS
The buttons will be included in a fruit and vegetable theme into the shape of the remote control.
Table 9: Example summaries generated by different models and utterance attention weights visualization of (a) Sentence-Gated (with dialogue
acts) and (b) DDAMS (with dialogue discourse relations).

References
[Asher et al., 2016] Nicholas Asher, Julie Hunter, Mathieu
Morey, Benamara Farah, and Stergos Afantenos.
Dis-
course structure and dialogue acts in multiparty dialogue:
the stac corpus. In LREC, 2016.
[Bahdanau et al., 2015] Dzmitry
Bahdanau,
Kyunghyun
Cho, and Yoshua Bengio.
Neural machine transla-
tion by jointly learning to align and translate.
CoRR,
abs/1409.0473, 2015.
[Beck et al., 2018] Daniel Beck, Gholamreza Haffari, and
Trevor Cohn.
Graph-to-sequence learning using gated
graph neural networks. In ACL, July 2018.
[Carletta et al., 2005] Jean Carletta, Simone Ashby, Se-
bastien Bourban, Mike Flynn, Mael Guillemot, Thomas
Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij,
Melissa Kronenthal, et al. The ami meeting corpus: A
pre-announcement. In International workshop on machine
learning for multimodal interaction. Springer, 2005.
[Ganesh and Dingliwal, 2019] Prakhar Ganesh and Saket
Dingliwal. Restructuring Conversations using Discourse
Relations for Zero-shot Abstractive Dialogue Summariza-
tion. arXiv.org, 2019.
[Goo and Chen, 2018] Chih-Wen Goo and Yun-Nung Chen.
Abstractive dialogue summarization with sentence-gated
modeling optimized by dialogue acts. In SLT. IEEE, 2018.
[Gross et al., 2013] Jonathan L Gross, Jay Yellen, and Ping
Zhang.
Handbook of graph theory.
Chapman and
Hall/CRC, 2013.
[Gurevych and Strube, 2004] Iryna Gurevych and Michael
Strube.
Semantic similarity applied to spoken dialogue
summarization. In COLING, 2004.
[Janin et al., 2003] Adam Janin, Don Baron, Jane Edwards,
Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke, et al. The
icsi meeting corpus. In ICASSP. IEEE, 2003.
[Koay et al., 2020] Jia Jin Koay, Alexander Roustai, Xiaojin
Dai, Dillon Burns, Alec Kerrigan, and Fei Liu. How do-
main terminology affects meeting summarization perfor-
mance. In Proceedings of the 28th International Confer-
ence on Computational Linguistics, December 2020.
[Li et al., 2019] Manling Li, Lingyu Zhang, Heng Ji, and
Richard J. Radke. Keep meeting summaries on topic: Ab-
stractive multi-modal meeting summarization.
In ACL,
July 2019.
[Li et al., 2021] Jiaqi Li, Ming Liu, Zihao Zheng, Heng
Zhang, Bing Qin, Min-Yen Kan, and Ting Liu. Dadgraph:
A discourse-aware dialogue graph neural network for mul-
tiparty dialogue machine reading comprehension. ArXiv,
2021.
[Lin, 2004] Chin-Yew Lin.
ROUGE: A package for auto-
matic evaluation of summaries.
In Text Summarization
Branches Out, Barcelona, Spain, July 2004. ACL.
[Liu and Chen, 2019] Zhengyuan Liu and Nancy Chen. Ex-
ploiting discourse-level segmentation for extractive sum-
marization. In Proceedings of the 2nd Workshop on New
Frontiers in Summarization, 2019.
[Luong et al., 2015] Thang Luong, Hieu Pham, and Christo-
pher D. Manning. Effective approaches to attention-based
neural machine translation. In EMNLP, 2015.
[Mann and Thompson, 1988] William C Mann and Sandra A
Thompson. Rhetorical structure theory: Toward a func-
tional theory of text organization. Text-interdisciplinary
Journal for the Study of Discourse, 8(3), 1988.
[Marcheggiani and Titov, 2017] Diego
Marcheggiani
and
Ivan Titov. Encoding sentences with graph convolutional
networks for semantic role labeling. In EMNLP, 2017.
[Mihalcea and Tarau, 2004] Rada Mihalcea and Paul Tarau.
TextRank: Bringing order into text. In EMNLP, Barcelona,
Spain, July 2004. Association for Computational Linguis-
tics.
[Murray et al., 2010] Gabriel Murray, Giuseppe Carenini,
and Raymond Ng. Generating and validating abstracts of
meeting conversations: a user study. In INLG, 2010.
[Nallapati et al., 2017] Ramesh Nallapati, Feifei Zhai, and
Bowen Zhou.
Summarunner: A recurrent neural net-
work based sequence model for extractive summarization
of documents. In AAAI, 2017.
[Narayan et al., 2020] Shashi Narayan,
Gonc¸alo Simoes,
Ji Ma, Hannah Craighead, and Ryan Mcdonald. Qurious:
Question generation pretraining for text generation. arXiv
preprint, 2020.
[Qin et al., 2020] Libo Qin, Xiao Xu, Wanxiang Che, and
Ting Liu.
AGIF: An adaptive graph-interactive frame-
work for joint multiple intent detection and slot ﬁlling. In
Findings of the Association for Computational Linguistics:
EMNLP 2020, pages 1807–1816. Association for Compu-
tational Linguistics, 2020.
[Sacks et al., 1978] Harvey Sacks, Emanuel A Schegloff,
and Gail Jefferson. A simplest systematics for the orga-
nization of turn taking for conversation. In Studies in the
organization of conversational interaction. Elsevier, 1978.
[Schlichtkrull et al., 2018] Michael Schlichtkrull, Thomas N
Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and
Max Welling. Modeling relational data with graph convo-
lutional networks. In European Semantic Web Conference.
Springer, 2018.
[See et al., 2017] Abigail See, Peter J. Liu, and Christo-
pher D. Manning. Get to the point: Summarization with
pointer-generator networks. In ACL, 2017.
[Serban et al., 2016] Iulian V Serban, Alessandro Sordoni,
Yoshua Bengio, Aaron Courville, and Joelle Pineau.
Building end-to-end dialogue systems using generative hi-
erarchical neural network models. In AAAI, 2016.
[Shang et al., 2018] Guokan Shang, Wensi Ding, Zekun
Zhang, Antoine Tixier, Polykarpos Meladianos, Michalis

Vazirgiannis, and Jean-Pierre Lorr´e.
Unsupervised ab-
stractive meeting summarization with multi-sentence com-
pression and budgeted submodular maximization. In ACL,
2018.
[Shi and Huang, 2019] Zhouxing Shi and Minlie Huang. A
deep sequential model for discourse parsing on multi-party
dialogues. In AAAI, volume 33, 2019.
[Stone et al., 2013] Matthew Stone, Una Stojnic, and Ernest
Lepore. Situated utterances and discourse relations. In
IWCS – Short Papers, March 2013.
[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. In Advances in neural information processing sys-
tems, 2017.
[Wang et al., 2020] Tianyi Wang, Yating Zhang, Xiaozhong
Liu, Changlong Sun, and Qiong Zhang. Masking orches-
tration: Multi-task pretraining for multi-role dialogue rep-
resentation learning. 2020.
[Xu et al., 2020] Jiacheng Xu, Zhe Gan, Yu Cheng, and
Jingjing Liu. Discourse-aware neural extractive text sum-
marization. In ACL, 2020.
[Zhao et al., 2019] Zhou Zhao, Haojie Pan, Changjie Fan,
Yan Liu, Linlin Li, Min Yang, and Deng Cai. Abstractive
meeting summarization via hierarchical adaptive segmen-
tal network learning. In The World Wide Web Conference,
2019.
[Zhu et al., 2020] Chenguang Zhu, Ruochen Xu, Michael
Zeng, and Xuedong Huang. A hierarchical network for
abstractive meeting summarization with cross-domain pre-
training. In Findings of EMNLP, 2020.

