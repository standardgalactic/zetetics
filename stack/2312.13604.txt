Ponymation: Learning 3D Animal Motions from Unlabeled Online Videos
Keqiang Sun1*, Dor Litvak2,3∗‡, Yunzhi Zhang2, Hongsheng Li1, Jiajun Wu2†, Shangzhe Wu2†
1CUHK MMLab
2Stanford University
3 UT Austin
keqiangsun.github.io/projects/ponymation
Unlabeled Online Videos
Test Image
Training Data
Articulated Shape
Generated Motion sequences
Single Image 3D Reconstruction
3D Motion Generation
Motion Latent Space
…
…
…
Figure 1. Learning Articulated 3D Animal Motions from Unlabeled Online Videos. Given a collection of monocular videos of an
animal category obtained from the Internet as training data, our method learns a generative model of the articulated 3D motions together
with a category-specific 3D reconstruction model, without relying on any shape templates or pose annotations for training. At inference
time, the model takes in a single test image and generates diverse 3D animations by sampling from the learned motion latent space.
Abstract
We introduce Ponymation, a new method for learning a
generative model of articulated 3D animal motions from
raw, unlabeled online videos. Unlike existing approaches
for motion synthesis, our model does not require any pose
annotations or parametric shape models for training, and is
learned purely from a collection of raw video clips obtained
from the Internet. We build upon a recent work, MagicPony,
which learns articulated 3D animal shapes purely from sin-
gle image collections, and extend it on two fronts. First,
instead of training on static images, we augment the frame-
work with a video training pipeline that incorporates tem-
poral regularizations, achieving more accurate and tempo-
rally consistent reconstructions. Second, we learn a gen-
erative model of the underlying articulated 3D motion se-
quences via a spatio-temporal transformer VAE, simply us-
ing 2D reconstruction losses without relying on any explicit
pose annotations. At inference time, given a single 2D im-
age of a new animal instance, our model reconstructs an ar-
ticulated, textured 3D mesh, and generates plausible 3D an-
imations by sampling from the learned motion latent space.
*Equal contribution. †Equal advising.
‡Work was primarily done while Dor Litvak was visiting Stanford.
1. Introduction
We share the planet with various kinds of lively animals.
Similarly to human beings, they navigate and interact with
the physical world, demonstrating various sophisticated
motion patterns. In fact, the first film in history, “The Horse
in Motion”, was a sequence of photographs capturing a gal-
loping horse, created by Eadweard Muybridge in 1887 [42].
Films only capture sequences of 2D projections of 3D ani-
mal movements. Further modeling dynamic animals in 3D
is not only useful for numerous mixed reality and content
creation applications, but can also provide computational
tools for biologists to study animal behaviors.
In this work, we are interested in learning a prior dis-
tribution of 3D animal motion patterns with a generative
model, simply from a collection of raw video sequences.
Although a lot of efforts have been invested in captur-
ing and modeling 3D human motions using computer vi-
sion techniques, significantly less attention has been paid to
animals. Existing learning-based approaches require an ex-
tensive amount of 3D scans [40, 51, 52], parametric shape
models [6, 26, 28, 50, 77], multi-view videos [16, 21, 38],
or geometric annotations, such as keypoints [18, 19, 22, 48–
50, 57], as supervision for training. Collecting large-scale
3D training data involves specialized capture devices and
intensive labor, which can only be justified for specific ob-
1
arXiv:2312.13604v1  [cs.CV]  21 Dec 2023

jects, like humans, that are of utmost value in applications.
Recent years have seen the rapid development of data-
efficient algorithms that can learn 3D objects simply from
collections of raw unstructured 2D images [10, 44, 45, 53,
60, 61, 67, 70, 76], without relying on explicit 3D train-
ing data or human annotations. At the core of these ap-
proaches is a differentiable inverse rendering framework
that is trained to explain the training images using a learned
canonical 3D model, following an analysis-by-synthesis
paradigm.
This allows the model to learn various types
of natural 3D objects beyond human bodies, including an-
imals in the wild [27, 70, 74, 76]. Despite impressive 3D
reconstruction results, these methods reconstruct 3D shapes
from individual static images, ignoring the temporal motion
dynamics of the underlying objects moving in the physical
world. Although a few recent works have used video data
for training [68, 72–74], they still either optimize over only
a few animal instances [72–74] or obtain a single image re-
construction model [68], without learning a prior distribu-
tion of diverse 3D animal motion patterns.
In this work, we would like to learn a generative model
that captures the prior distribution of 3D motions of an an-
imal category. Crucially, we study this problem under two
challenging conditions.
First, unlike existing video syn-
thesis methods [55, 66] that operate on 2D images, we
would like to obtain an explicit 3D motion representation,
in the form of a 3D mesh and a sequence of articulated
3D poses. This explicit, disentangled 3D representation al-
lows for downstream motion analysis and controllable 3D
animation. Second, unlike existing motion synthesis ap-
proaches [18, 22, 48, 49], our learning process does not
rely on explicit manual supervision, such as keypoints or
template shapes, which are often expensive to collect for
various animals at scale.
To do this, we capitalize on a recent work, Mag-
icPony [70], which learns articulated 3D models of animal
categories from image collections. Instead of relying on
explicit annotations for learning 3D shapes, it exploits se-
mantic correspondences distilled from self-supervised im-
age features [8], and learns a category-specific articulated
3D model by matching these noisy correspondences and re-
constructing the images. In our work, instead of learning to
explain individual static images in 3D, we task the model
to explain sequences of images with an explicit “4D” repre-
sentation, i.e., sequences of articulated 3D shapes, through
a 3D motion latent space.
Specifically, we design a spatio-temporal transformer
VAE that encodes a sequence of images into a latent vector
in the motion latent space, and decodes from it a sequence
of articulated 3D poses, expressed in the form of bone rota-
tions on an estimated skeleton. In conjunction with a base
shape and appearance reconstruction module, the learned
3D motion sequences can be rendered into RGB images, al-
lowing the model to be trained by simply minimizing 2D
reconstruction losses on raw RGB video frames, without
relying on any explicit pose annotations.
At inference time, given a single image of a new animal
instance, our model reconstructs its articulated 3D shape
and appearance, and generates diverse plausible 3D ani-
mations by sampling from the learned motion latent space.
Moreover, this end-to-end video training pipeline also al-
lows us to take advantage of additional temporal signals to
learn the 3D shape, leading to more accurate and temporally
consistent reconstructions.
To summarize, this paper makes several contributions:
• We propose a new method for learning a generative model
of articulated 3D animal motions from unlabeled online
videos, without any shape templates or pose annotations;
• We design a spatio-temporal transformer VAE architec-
ture that effectively extracts spatio-temporal motion in-
formation from an input video sequence;
• At inference time, the model takes in a single image
and generates various realistic 3D animations from the
learned motion latent space;
• This video training framework also leads to improved 3D
reconstruction accuracy, compared to baseline methods
trained on static images.
2. Related Work
2.1. Learning 3D Animals from Image Collections
While modeling dynamic 3D objects traditionally requires
motion capture markers or simultaneous multi-view cap-
tures [12, 13, 20], recent learning-based approaches have
demonstrated the possibility of learning 3D deformable
models simply from raw single-view image collections [17,
27, 34, 36, 67, 70, 76].
Most of these methods require
additional geometric supervision besides object masks for
training, such as keypoint [27, 35] and viewpoint annota-
tions [14, 46, 56], template shapes [17, 31, 34], semantic
correspondences [25, 36, 70, 76], and strong geometric as-
sumptions like symmetries [67, 69]. Among these, Mag-
icPony [70] demonstrates remarkable results in learning ar-
ticulated 3D animals, such as horses, using only single-view
images with object masks and self-supervised image fea-
tures as training supervision. However, it reconstructs static
images individually, ignoring the dynamic motions of the
underlying 3D animals behind those images. In this work,
we build upon MagicPony, and develop a method for learn-
ing a generative model of articulated 3D animal motion se-
quences simply from a raw monocular video collection.
2.2. Deformable Shapes from Monocular Videos
Reconstructing deformable shapes from monocular videos
is a long-standing problem in computer vision. Early ap-
proaches with Non-Rigid Structure from Motion (NRSfM)
2

feature field
3D shape
texture
Single Image 3D Reconstruction
Reconstruction Losses
≈
ℒ!"
≈
ℒ"
≈
ℒ#
Training Videos
Motion VAE
KL Divergence ℒ$%
⇒
Renderer
Enc
Dec
pose
sequence
canonical
model
Input Sequence "&:(
Rendered #"&:(
!"!:#
#
Figure 2. Training Pipeline. Our method learns a generative model of articulated 3D motion sequences from a collection of unlabeled
monocular videos. During training, the motion VAE encodes an input video sequence I1:T into a latent code z, and decodes from it a
sequence of articulated 3D poses ˆξ1:T . Working in tandem with a single-image 3D reconstruction module, the full pipeline is trained
simply using image reconstruction losses with unsupervised image features and segmentation masks obtained from off-the-shelf models.
reconstruct deformable shapes from 2D correspondences,
by incorporating heavy constraints on the motion pat-
terns [2, 7, 9, 11, 71].
DynamicFusion [43] further in-
tegrates additional depth information from depth sensors.
NRSfM pipelines have recently been revived with neural
representations. In particular, LASR [72] and its follow-
ups [73, 74] optimize deformable 3D shapes over a small set
of monocular videos, leveraging 2D optical flows in a heav-
ily engineered optimization procedure.
DOVE [68] pro-
poses a learning framework that learns a category-specific
3D model from a large collection of monocular online
videos, which allows for single image reconstruction of ar-
ticulated shapes at test time. However, despite using video
data for training, none of these approaches explicitly model
the distribution of temporal motion patterns of the objects,
which is the focus of this work.
2.3. Motion Analysis and Synthesis
Modeling motion patterns of dynamic objects has important
applications for both behavior analysis and content genera-
tion, and is instrumental to our visual perception system [4].
Computational techniques have been used for decades to
study and synthesize human motions [5, 47, 64]. In particu-
lar, recent works have explored learning generative models
for 3D human motions [1, 18, 22, 39, 41, 48, 49, 57], lever-
aging parametric human shape models, like SMPL [40], and
large-scale human pose annotations [3, 24]. In comparison,
much less efforts have been invested in learning animal mo-
tions. [23] proposes a hierarchical motion learning frame-
work for animals, but requires costly motion capture data
and hardly generalizes to animals in the wild. To sidestep
the collection of 3D data, [58] introduces a self-supervised
method for discovering and tracking keypoints from videos,
but is limited to a 2D representation. Such 2D keypoints
could be lifted to 3D, as shown in BKinD-3D [59], but this
requires multi-view videos for training. Unlike these prior
works, our motion learning framework does not require any
pose annotations or multi-view videos for training, and is
trained simply using raw monocular online videos.
3. Method
Given a collection of raw video clips of an animal category,
such as horses, our goal is to learn a generative model of its
articulated 3D motions, which allows us to sample diverse
and realistic motion sequences from the learned motion la-
tent space, and animate a new animal instance in 3D given
only a single 2D image at test time.
We train this model simply on raw online videos without
relying on any explicit pose annotations, building upon the
recent work of MagicPony [70]. However, instead of recon-
structing static images as in [70], we design a transformer-
based motion Variational Auto-Encoder (VAE) that learns
a generative model of 3D animal motions by training on
videos. Figure 2 gives an overview of the training pipeline.
3.1. Overview of MagicPony
Given a collection of static images and automatically ob-
tained instance masks as training data, MagicPony [70]
learns a monocular reconstruction model that takes a 2D
image as input and predicts the 3D shape, articulated pose,
and appearance of the underlying object.
To do this, for each animal category, the model learns a
category-specific prior 3D shape using a hybrid SDF-mesh
representation [54]. To reconstruct a specific instance in
an input image, the model first extracts a feature map and
a global feature vector (Φ, ϕ) = fϕ(I) using a pre-trained
image encoder [8]. With the features, it then estimates a de-
formation field f∆V that deforms the vertices of the prior
mesh accounting for non-structured shape variations (e.g.,
hair). To account for more controlled deformations caused
by the underlying skeletal structure, the model further in-
stantiates a set of bones on the instance mesh, and pre-
3

dicts an articulated pose ˆξ = fξ(ϕ), including a rigid pose
ˆξ1 ∈SE(3) w.r.t. an identity camera pose and the rotations
ˆξb ∈SO(3) of each bone b ∈{2, ..., B}. The final articu-
lated shape is obtained using a linear blend skinning equa-
tion g(V, ξ) [40]. The model also predicts the appearance ˆA
of the object, and the entire pipeline is trained by minimiz-
ing reconstruction losses on the rendered images and masks
(ˆI, ˆ
M) = R( ˆV , ˆξ, ˆA), where the pseudo ground-truth is ob-
tained from an off-the-shelf segmentation model [30].
As learning articulated 3D shapes from single-view im-
ages is a highly ill-posed inverse task, previous works
often rely on additional expensive geometric annotations
for training, such as keypoints or template shapes.
In-
stead, MagicPony exploits semantic correspondences dis-
tilled from self-supervised image features of DINO-ViT [8].
Specifically, it learns a category-specific feature field, and
compares the rendered features with image features ex-
tracted from a pre-trained DINO-ViT, which helps register
various posed images onto the same canonical model. For
more details, please refer to MagicPony [70].
3.2. Video Training
Despite reconstructing detailed 3D shapes, MagicPony
looks at one single image at a time and ignores the tem-
poral motion dynamics of the underlying animals.
This
results in sub-optimal reconstructions with discontinuous
poses, when applied to video sequences, as illustrated in
Figure 3. To enhance temporal consistency for better mo-
tion modeling, we introduce a video training pipeline that
learns to reconstruct an entire sequence of frames {It}T
t=1
from a training video clip in one go.
We further incorporate temporal smoothness constraints
on the predicted poses ˆξt between consecutive frames, by
minimizing the L2 distance between the consecutive pose
predictions: Rtemp = PT
t=2 ∥ˆξt −ˆξt−1∥2
2, including both
rigid poses ˆξt,1 and bone rotations ˆξt,2:B.
The training objective hence consists of the reconstruc-
tion losses on rendered RGB frames Lim,t, masks Lm,t, and
DINO features Lf,t: Lrecon,t = Lim,t + λmLm,t + λfLf,t.
Together with the shape regularizers and viewpoint hypoth-
esis loss inherited from [70] which we summarize by Rs,
and the proposed temporal smoothness regularizer Rtemp,
the objective of this video training baseline is:
Lvid =
T
X
t=1
Lrecon,t + λsRs + λtempRtemp.
(1)
Not only does this video training pipeline improve
temporal consistency of 3D reconstructions on video se-
quences, as shown in Figure 3, but also leads to better re-
construction results on static images, as indicated in Table 2,
demonstrating the advantages of training on videos. More
importantly, it lays the foundation for modeling the distri-
bution of articulated 3D animal motions, as described next.
3.3. Transformer VAE for Motion Synthesis
Based on this disentangled 3D representation, we would
like to learn a prior distribution over the animals’ articu-
lated 3D motions. To this end, inspired by prior work on
human motion synthesis [48], we adopt a Variational Auto-
Encoder (VAE) framework to learn a latent space of artic-
ulated motion sequences ξ1:T . To better model the tempo-
ral and spatial structure of the motion, we design a spatio-
temporal transformer architecture [65].
Specifically, instead of predicting the pose ˆξt of each in-
put frame It individually, the model encodes the entire se-
quence of frames I1:T into the latent space of a VAE, and
samples from this space a single latent code z that predicts
the entire sequence of poses ˆξ1:T . Crucially, unlike prior
work that requires 3D pose annotations as input, our model
is trained to reconstruct the raw RGB sequences, without
any pose annotations for training.
Sequence Encoding. To encode a sequence of RGB frames
into the VAE latent distribution parameters (ˆµ, ˆΣ) capturing
the motion, we design a pair of spatial and temporal trans-
former encoders Es and Et.
Specifically, given each frame It of the input sequence,
we first obtain a pose feature by fusing both global im-
age features ϕt and local features sampled at the pixel
of the bones. Following MagicPony [70], for each bone,
we construct a bone-specific feature descriptor b: νt,b =
(ϕt, Φt(ut,b), b, Jb, ut,b), where Jb denotes the 3D location
of the bone at rest-pose which projects to the pixel location
ut,b in the image, and Φt(ut,b) is the feature vector sampled
from the image feature map Φt at the pixel location ut,b.
The spatial transformer encoder Es then fuses these fea-
ture descriptors into a single feature vector νt,∗summariz-
ing the articulated pose of the animal in each frame:
νt,∗= Es(νt,2, · · · , νt,B).
(2)
In practice, we prepend a learnable token to the feature de-
scriptors, and take the first output token of the transformer
as the pose feature νt,∗. We call this Es a spatial transformer
as it extracts the spatial structural features from each input
frame that capture the articulated pose.
Next, we design a second temporal transformer encoder
Et that operates along the temporal dimension and maps
the entire sequence of pose features {νt,∗}T
t=1 into the VAE
latent space. Similar to the Es, Et takes in the pose feature
sequence and predicts VAE distribution parameters:
(ˆµ, ˆΣ) = Et(ν1,∗, · · · , νT,∗).
(3)
Using the reparametrization trick [29], we can then sam-
ple a single latent code from the Gaussian distribution z ∼
N(ˆµ, ˆΣ) describing the motion of the entire sequence.
4

Category
Sequence Num
Duration
Frame Number
Horse
640
28’09”
50,682
Zebra
47
05’27”
9,822
Giraffe
60
04’52”
8,768
Cow
69
07’25”
13,359
Total
816
45’54”
82,631
Table 1.
Statistics of the AnimalMotion Dataset. To train our
motion synthesis model, we collect a new animal video dataset
containing a total of 82.6k frames for 4 different categories.
Motion Decoding. Symmetric to the encoder, the motion
decoder also consists of a temporal decoder Dt that decodes
z into a sequence of pose features z1:T , and a spatial de-
coder Ds that further decodes each pose feature zt to a set
of bone rotations ˆξt.
Specifically, we query the temporal transformer decoder
Dt with a sequence of timestamps T , and use z as both key
and value tokens to obtain a sequence of pose features:
(z1, · · · , zT ) = Dt(T , z),
T = (1, · · · , T).
(4)
Similarly, given each pose feature zt, we then query the spa-
tial transformer decoder Ds with a sequence of bone indices
to generate the bone rotations:
(ˆξt,2, · · · , ˆξt,B) = Ds(B, zt),
B = (2, · · · , B).
(5)
Note that like in MagicPony [70], the rigid pose ˆξt,1 is pre-
dicted by a separate network and not included in this motion
VAE, since it is entangled with arbitrary camera poses.
To train the motion VAE, in addition to the reconstruc-
tion losses in Equation (1), we also minimize the Kull-
back–Leibler (KL) divergence between the learned latent
distribution and a standard Gaussian distribution:
LKL =
X
i
−1
2
 log σi −σi −µ2
i + 1

,
(6)
where µi and σ2
i are elements of the predicted distribution
parameters ˆµ and ˆΣ.
Training Schedule. As directly learning 3D articulated
motion from raw video frames is a challenging task, we
train our pipeline in two phases. In the first phase, instead of
activating the motion VAE, we use the original single-frame
pose prediction network from MagicPony and train with the
proposed video-training objective in Equation (1). As 3D
shape and pose predictions stablize after 140 epochs, we
swap out the pose network with the motion VAE model in
the second phase. To facilitate the training of the VAE, we
recycle pose predictions ˜ξt from the first phase and use them
as pseudo ground-truth to supervise the outputs of the VAE
decoder ˆξt, with a teacher loss Lteacher = PT
t=1 ∥ˆξt −˜ξt∥2
2.
The final training objective for the second phase is thus:
L = Lvid + λKLLKL + λteacherLteacher.
(7)
Method
Supervision
PCK ↑
Mask IoU ↑
Rigid-CSM [32]
kp. + mask
42.1%
-
A-CSM [33]
kp. + mask
44.6%
-
Rigid-CSM [32]
mask
31.2%
-
Dense-Equi [63]
mask
23.3%
-
UMR [37]
mask
24.4%
-
A-CSM [33]
mask
32.9%
-
MagicPony [70]
DINO + mask
42.8%
64.12%
Ponymation (ours)
DINO + mask
48.0%
71.83%
Table 2. Reconstruction Evaluation on PASCAL [15] Dataset. Our
method achieves superior reconstruction accuracy compared to a
state-of-the-art method MagicPony, and is competitive to methods
that require extra supervision from keypoints (denoted as ‘kp.’).
Inference. During inference time, we can sample a latent
code from the learned motion latent space and generate an
arbitrary articulated 3D motion sequence using the motion
decoder. Working in conjunction with the single-image 3D
reconstruction module, given a single test image of a new
animal instance, our final model can reconstruct the 3D
shape of the instance and automatically generate 3D ani-
mations of it, as illustrated in Figure 4.
4. Experiments
4.1. Experimental Setup
Datasets. To train our model, we collected an AnimalMo-
tion dataset consisting of video clips of several quadruped
animal categories extracted from the Internet. The statis-
tics of the dataset are summarized in Tab. 1.
As pre-
processing, we first detect and segment the animal instances
in the videos using the off-the-shelf segmentation model of
PointRend [30]. To remove occlusion between different in-
stances, we calculated the extent of mask overlap in each
frame and exclude crops where two or more masks over-
lap with each other.
We further apply a smoothing ker-
nel to the sequence of bounding boxes to avoid jittering.
The non-occluded instances are then cropped and resized to
256 × 256. The original videos are all at 30fps. To ensure
sufficient motion in each sequence, we remove frames with
minimal motion (roughly 20%), measured by the magni-
tude of optical flow estimated from RAFT [62]. To conduct
quantitative evaluations and compare with previous meth-
ods, we also use PASCAL VOC [15] and APT-36K [75],
both of which provide 2D keypoint annotations for each an-
imal in the image.
Implementation Details. The encoders and decoders of
the motion VAE model (Es, Et, Ds, Dt) from Sec. 3.3
are implemented as stacked transformers [65] with 4 trans-
former blocks and a latent dimension of 256, similar to [48].
We apply positional encoding, we followed [48] and used
a sinusoidal function.
For the rest of the networks de-
5

Input
Sequence
MagicPony
Ponymation
(Ours)
Input
Sequence
MagicPony
Ponymation
(Ours)
Wrong Viewpoint
Ours
Wrong Articulation
Figure 3. Comparison of 3D Reconstruction Results with Mag-
icPony [70].
With the video training framework, our method
produces temporally coherent and more accurate pose predictions.
In comparison, the baseline model of MagicPony often predicts in-
correct rigid poses ˆξt,1 (red boxes), and incorrect bone articulation
ˆξt,2:B (blue boxes), resulting in inaccurate 3D reconstruction.
scribed in Sec. 3.1, we follow the implementation of Mag-
icPony [70]. We train for 120 epochs in the first phase as
described in Sec. 3.3, with λm = 10, λf = 10, λs = 0.1,
λtemp = 1. In the second phase, we train the full pipeline
end-to-end for another 180 epochs with additionally λKL =
0.001 and λteacher = 1. The final model is trained on 8
A6000 GPUs for roughly 10 hours for the first training
phase and additional 48 hours for the second phase. In our
generation model, we empirically set the sequence length
T to 10. For the visualization results, we follow [70] and
finetune (only) the appearance network for 100 iterations
on each test image which takes less than 10 seconds, as the
model struggles to predict detailed textures in a single feed-
forward pass. More details are described in the supplemen-
tary material.
4.2. Single-image 3D Reconstruction
While the main task of this work is learning articulated 3D
motions from videos, the underlying 3D representation also
enables single-image 3D reconstruction at test time. We
first evaluate the quality of the 3D reconstruction, compar-
ing with several existing methods, and analyze motion gen-
eration in the next section.
APT-36K
PASCAL
Experiment
PCK ↑/ Velocity Err ↓
PCK@0.1 ↑/ MIoU ↑
MP [70]
53.93% / 57.31%
42.8% / 64.12%
MP [70] + AM
57.84% / 49.82%
46.8% / 71.58%
Ponymation (ours)
59.91% / 49.07%
48.0% / 71.83%
Table 3. Reconstruction Evaluation on APT-36K [75] and PAS-
CAL [15]. MP: Magicpony, AM: our AnimalMotion dataset. Both
the new AnimalMotion dataset and video training pipeline im-
prove the reconstruction accuracy and temporal consistency.
4.2.1
Qualitative Results
Figure 3 compares 3D reconstruction results with the base-
line model of MagicPony [70] on video sequences.
Al-
though MagicPony predicts a plausible 3D shape in most
cases, it tends to produce temporally inconsistent poses, in-
cluding both viewpoint ˆξt,1 and bone rotations ˆξt,2:B. In
comparison, our method leverages the temporal signals in
training videos, and produces temporally coherent recon-
struction results.
4.2.2
Quantitative Evaluations
Metrics. Since 3D ground-truth is often unavailable for in-
the-wild objects for a direct 3D evaluation, Percentage of
Correct Keypoints (PCK) has become a commonly used
metric on datasets with 2D keypoint annotations [33, 36, 70]
for 3D reconstruction evaluation. Given a set of 2D key-
points on a source image, we first map them to the closest
vertices on the surface of the reconstructed 3D model, and
further project these 3D vertices to the target image in 2D.
The Euclidean distance between these projected keypoints
and corresponding ground-truth annotations measures the
error of the 3D reconstruction results. We calculate the per-
centage of points with an error smaller than 0.1×max(h, w)
and denote it as PCK@0.1, where h and w are image height
and width, respectively. Another commonly used metric
is Mask Intersection over Union (MIoU) between the ren-
dered and ground-truth masks, which measures the recon-
struction quality in terms of projected 2D silhouettes. In
addition, we also measure the temporal consistency of the
reconstructions of video frames using the Velocity Error,
computed as
1
T
P
t ∥ˆδt −δt∥/δt, where ˆδt and δt are the
keypoint displacements between consecutive frame for pre-
dicted and GT pose sequences respectively.
Evaluation on PASCAL. We quantitatively evaluate our
method on PASCAL [15], a standard benchmarking dataset
for 3D reconstruction, and compare it with prior 3D re-
construction methods, as shown in Table 2.
The results
from MagicPony are obtained using the code provided by
the authors; results for other baselines are taken from A-
CSM [33]. Rigid-CSM [32] and A-CSM achieve good per-
formance when trained with extra keypoint supervision, but
the results significantly deteriorate when such supervision is
not available. Our method builds on top of MagicPony and
6

Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Figure 4. Test-Time Single-Image 3D Animation. During test time, our method can animate a novel animal instance in 3D from a single
input image. The method can faithfully reconstruct the shape and texture conditioned on the input image, and then render the obtained 3D
representation with plausible motion sampled from the motion latent space. The first row within each gray box shows textured animation,
and the second row shows corresponding 3D shapes overlapped with predicted bone articulation.
additionally benefits from the temporal signals by training
on videos, with an improved PCK@0.1 score from 42.8% to
48.0%, and a mask IoU from 64.12% to 71.83%, compared
to the MagicPony baseline. Overall, our method achieves
performance competitive with keypoint-supervised meth-
ods, and outperforms all existing unsupervised methods by
a significant margin.
We also conduct ablation studies to analyze the effects
of each component and report the results in Table 3. In par-
ticular, training on our new Animal Motion dataset leads to
significant performance boost, and combined with the tem-
poral smooth regularization, the final model demonstrates
further improvements.
Evaluation on APT-36K. To better understand the perfor-
mance on video reconstructions, we present a quantitative
evaluation on APT-36K [75], an animal video dataset with
2D keypoint annotations.
We use the 81 video clips of
horses with a total of 1207 frame (each frame may con-
7

Input Image
Generated
Input Image
Generated
Input Image
Generated
Figure 5. Motion Synthesis Results on More Categories. Our
method can be trained on video data from different categories such
as corws, zebras, and giraffes, and generate plausible motion se-
quences. The learned motions appear to be specific to the animal
category, such as the third example, where the generated neck mo-
tion is more prominent for giraffes than others.
tain multiple instances). Each horse is annotated with 17
keypoints. We use the same pre-processing procedure as
in the training set to crop all the instances, and only keep
instances that have full 17 keypoints annotations. This re-
sults in a total 92 sequences containing 965 frames over-
all, which we use to evaluate the sequence reconstruction
performance. Since the skeleton automatically discovered
by our model is different from the 17 keypoints annotated
in APT-36K, we map our predicted joints to the GT key-
points by simply optimizing a linear transformation, follow-
ing [27], and report PCK@0.1 in Tab. 3. Overall, our video
training pipeline leads to considerable improvements over
the MagicPony baselines.
4.3. 3D Motion Generation
4.3.1
Qualitative Results
After training, samples from the motion latent space can
generate diverse, realistic 3D motion sequences, which can
be rendered with textured meshes reconstructed from a sin-
gle 2D image, as shown in Figure 4. It also generalizes
to horse-like artifacts, such as carousel horses, which the
model has never seen during training. Beside horses, the
model can be trained on a wide range of categories, includ-
ing giraffes, zebras and cows, as shown in Figure 5. Be-
cause the datasets for these categories are limited in size and
diversity, similarly to [70], in the first phase of the training,
we fine-tune from the model trained on horses. With the
proposed training framework, our model is able to capture
a category-specific prior distribution of articulated 3D mo-
Experiment
MCD ↓
MP + VAE
38.77
MP + AM + VAE
38.12
Final Model (MP + AM + TS + VAE)
38.03
Table 4. Motion Chamfer Distance (MCD) on APT-36K [75] for
motion generation evaluation. MP: Magicpony, AM: AnimalMo-
tion Dataset, TS: Temporal Smoothness.
tions, and render realistic animation results on a variety of
animal categories. Additional 3D animation results are pro-
vided in the supplementary video.
4.3.2
Quantitative Evaluation
To assess the quality of our generated motion sequences, for
the lack of ground-truth animal motion capture data, we in-
troduce a proxy metric, bi-directional Motion Chamfer Dis-
tance (MCD), computed between a set of generated motion
sequences projected to 2D and the annotated keypoint se-
quences in APT-36K [75]. Specifically, we randomly sam-
ple from the learned motion latent space and generate 1, 400
motion samples, each consisting of 10 frames of articulated
poses projected to 2D. For the pose in each frame, we apply
the same mapping function described previously to align
with the the GT keypoints. To compute MCD, each GT
keypoint sequence in the test set, we find the closest gen-
erated motion sequence measured by keypoint MSE aver-
aged across all frames, and vice versa for each generated
sequence. In essence, MCD measures the fidelity of mo-
tion generation by comparing the distribution of generated
motions to that of the real motion sequences in videos. We
compare the results generated by different variants of the
model in Table 4. In summary, when combined with the mo-
tion VAE pipeline, training on the new video dataset with
the temporal constraints improve the fidelity of the gener-
ated motion sequences.
5. Conclusions
We have presented a generative model for learning the dis-
tribution of articulated 3D animal motions from raw Internet
videos, without relying on any pose annotations or shape
templates. We proposed a transformer-based motion VAE
model that exploits the temporal and spatial structure of an
explicit underlying representation of articulated 3D motions
in a video training framework. Experimental results show
that the proposed method captures a plausible distribution
of 3D animal motions for several animal categories. This
allows us to reconstruct a 3D model and automatically gen-
erate 3D animations of a new animal instance, from just a
single 2D image at test time.
Acknowledgments. We are grateful to Zizhang Li, Feng
Qiu and Ruining Li for insightful discussions.
8

References
[1] Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and
Songhwai Oh. Text2action: Generative adversarial synthesis
from language to action. In ICRA, pages 1–5, 2018. 3
[2] Ijaz Akhter, Yaser Sheikh, Sohaib Khan, and Takeo Kanade.
Nonrigid structure from motion in trajectory space.
In
NeurIPS, 2008. 3
[3] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and
Bernt Schiele. 2d human pose estimation: New benchmark
and state of the art analysis. In CVPR, 2014. 3
[4] Norman Badler. Temporal Scene Analysis: Conceptual De-
scriptions of Object Movements.
PhD thesis, Queensland
University of Technology, 1975. 3
[5] Norman I Badler, Cary B Phillips, and Bonnie Lynn Webber.
Simulating Humans: Computer Graphics, Animation, and
Control. Oxford University Press, 1993. 3
[6] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter
Gehler, Javier Romero, and Michael J. Black. Keep it SMPL:
Automatic estimation of 3D human pose and shape from a
single image. In ECCV, 2016. 1
[7] Christoph Bregler, Aaron Hertzmann, and Henning Bier-
mann. Recovering non-rigid 3d shape from image streams.
In CVPR, 2000. 3
[8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers.
In
ICCV, 2021. 2, 3, 4
[9] Thomas J. Cashman and Andrew W. Fitzgibbon. What shape
are dolphins? building 3d morphable models from 2d im-
ages. IEEE TPAMI, 2012. 3
[10] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and
Gordon Wetzstein. pi-GAN: Periodic implicit generative ad-
versarial networks for 3d-aware image synthesis. In CVPR,
2021. 2
[11] Yuchao Dai, Hongdong Li, and Mingyi He. A simple prior-
free method for non-rigid structure-from-motion factoriza-
tion. In CVPR, 2012. 3
[12] Edilson de Aguiar, Carsten Stoll, Christian Theobalt, Naveed
Ahmed, Hans-Peter Seidel, and Sebastian Thrun.
Perfor-
mance capture from sparse multi-view video. ACM TOG,
2008. 2
[13] Paul Debevec. The light stages and their applications to pho-
toreal digital actors. In SIGGRAPH Asia, 2012. 2
[14] Shivam Duggal and Deepak Pathak. Topologically-aware de-
formation fields for single-view 3d reconstruction. CVPR,
2022. 2
[15] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. IJCV,
111:98–136, 2015. 5, 6
[16] Xiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng,
Zicheng Liu, and Xin Tong. Mps-nerf: Generalizable 3d hu-
man rendering from multiview images. IEEE TPAMI, 2022.
1
[17] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.
Shape and viewpoints without keypoints. In ECCV, 2020.
2
[18] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng.
Ac-
tion2motion: Conditioned generation of 3d human motions.
In ACM MM, 2020. 1, 2, 3
[19] Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, Joe
Yearsley, and Taku Komura. A recurrent variational autoen-
coder for human motion synthesis. In BMVC, 2017. 1
[20] Richard Hartley and Andrew Zisserman. Multiple View Ge-
ometry in Computer Vision.
Cambridge University Press,
ISBN: 0521540518, second edition, 2004. 2
[21] Yannan He, Anqi Pang, Xin Chen, Han Liang, Minye Wu,
Yuexin Ma, and Lan Xu. Challencap: Monocular 3d cap-
ture of challenging human performances using multi-modal
references. In CVPR, pages 11400–11411, 2021. 1
[22] Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow.
Moglow: Probabilistic and controllable motion synthesis us-
ing normalising flows. ACM TOG, 39(6):1–14, 2020. 1, 2,
3
[23] Kang Huang, Yaning Han, Ke Chen, Hongli Pan, Gaoyang
Zhao, Wenling Yi, Xiaoxi Li, Siyuan Liu, Pengfei Wei, and
Liping Wang. A hierarchical 3d-motion learning framework
for animal spontaneous behavior mapping. Nature commu-
nications, 12(1):2784, 2021. 3
[24] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3.6m: Large scale datasets and predic-
tive methods for 3d human sensing in natural environments.
IEEE TPAMI, 36(7):1325–1339, 2014. 3
[25] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rup-
precht, and Andrea Vedaldi. Farm3D: Learning articulated
3D animals by distilling 2D diffusion. In 3DV, 2024. 2
[26] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In CVPR, 2018. 1
[27] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and
Jitendra Malik. Learning category-specific mesh reconstruc-
tion from image collections. In ECCV, 2018. 2, 8
[28] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jiten-
dra Malik. Learning 3d human dynamics from video. In
CVPR, 2019. 1
[29] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114, 2013. 4
[30] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Gir-
shick.
PointRend: Image segmentation as rendering.
In
CVPR, 2020. 4, 5
[31] Filippos Kokkinos and Iasonas Kokkinos.
To the point:
Correspondence-driven monocular 3d category reconstruc-
tion. In NeurIPS, 2021. 2
[32] Nilesh Kulkarni, Abhinav Gupta, and Shubham Tulsiani.
Canonical surface mapping via geometric cycle consistency.
In ICCV, 2019. 5, 6
[33] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shub-
ham Tulsiani.
Articulation-aware canonical surface map-
ping. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 452–461, 2020.
5, 6
[34] Nilesh Kulkarni, Abhinav Gupta, David F. Fouhey, and
Shubham Tulsiani.
Articulation-aware canonical surface
mapping. In CVPR, 2020. 2
9

[35] Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xi-
aolong Wang, Ming-Hsuan Yang, and Jan Kautz.
Online
adaptation for consistent mesh reconstruction in the wild. In
NeurIPS, 2020. 2
[36] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun
Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised
single-view 3d reconstruction via semantic consistency. In
ECCV, 2020. 2, 6
[37] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun
Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised
single-view 3d reconstruction via semantic consistency. In
ECCV, 2020. 5
[38] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker,
Noah Snavely, Ce Liu, and William T Freeman. Learning
the depths of moving people by watching frozen people. In
CVPR, pages 4521–4530, 2019. 1
[39] Xiaoyu Lin and Mohamed R. Amer. Human motion model-
ing using dvgans. arXiv preprint arXiv:1804.10652, 2018.
3
[40] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. SMPL: A skinned multi-
person linear model. ACM TOG, 2015. 1, 3, 4
[41] Matthias Minderer, Chen Sun, Ruben Villegas, Forrester
Cole, Kevin P Murphy, and Honglak Lee.
Unsupervised
learning of object structure and dynamics from videos.
NeurIPS, 32, 2019. 3
[42] Eadweard Muybridge. The horse in motion, 1887. 1
[43] Richard A. Newcombe, Dieter Fox, and Steven M. Seitz.
DynamicFusion: Reconstruction and tracking of non-rigid
scenes in real-time. In CVPR, 2015. 3
[44] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised
learning of 3d representations from natural images. In ICCV,
2019. 2
[45] Michael Niemeyer and Andreas Geiger.
GIRAFFE: Rep-
resenting scenes as compositional generative neural feature
fields. In CVPR, 2021. 2
[46] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Differentiable volumetric rendering: Learn-
ing implicit 3d representations without 3d supervision. In
CVPR, 2020. 2
[47] Dirk Ormoneit, Michael Black, Trevor Hastie, and Hedvig
Kjellstr¨om. Representing cyclic human motion using func-
tional analysis. Image and Vision Computing, 23:1264–1276,
2005. 3
[48] Mathis Petrovich, Michael J. Black, and G¨ul Varol. Action-
conditioned 3D human motion synthesis with transformer
VAE. In ICCV, 2021. 1, 2, 3, 4, 5
[49] Mathis Petrovich, Michael J Black, and G¨ul Varol. Temos:
Generating diverse human motions from textual descriptions.
In ECCV, 2022. 2, 3
[50] Jingtan Piao, Keqiang Sun, Quan Wang, Kwan-Yee Lin, and
Hongsheng Li. Inverting generative adversarial renderer for
face reconstruction. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
15619–15628, 2021. 1
[51] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned
implicit function for high-resolution clothed human digitiza-
tion. In ICCV, 2019. 1
[52] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul
Joo. Pifuhd: Multi-level pixel-aligned implicit function for
high-resolution 3d human digitization. In CVPR, 2020. 1
[53] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. GRAF: Generative radiance fields for 3d-aware im-
age synthesis. In NeurIPS, 2020. 2
[54] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and
Sanja Fidler. Deep marching tetrahedra: a hybrid represen-
tation for high-resolution 3d shape synthesis. In NeurIPS,
2021. 3
[55] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.
Make-a-video: Text-to-video generation without text-video
data. In ICLR, 2023. 2
[56] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wet-
zstein.
Scene representation networks:
Continuous 3d-
structure-aware neural scene representations.
In NeurIPS,
2019. 2
[57] Sebastian Starke, Ian Mason, and Taku Komura. Deepphase:
Periodic autoencoders for learning motion phase manifolds.
ACM Trans. Graph., 41(4), 2022. 1, 3
[58] Jennifer J Sun, Serim Ryou, Roni Goldshmid, Bran-
don Weissbourd, John Dabiri, David J Anderson, Ann
Kennedy, Yisong Yue, and Pietro Perona. Self-supervised
keypoint discovery in behavioral videos.
arXiv preprint
arXiv:2112.05121, 2021. 3
[59] Jennifer J Sun, Pierre Karashchuk, Amil Dravid, Serim
Ryou, Sonia Fereidooni, John Tuthill, Aggelos Katsagge-
los, Bingni W Brunton, Georgia Gkioxari, Ann Kennedy,
et al. Bkind-3d: Self-supervised 3d keypoint discovery from
multi-view videos. arXiv preprint arXiv:2212.07401, 2022.
3
[60] Keqiang Sun, Shangzhe Wu, Zhaoyang Huang, Ning Zhang,
Quan Wang, and Hongsheng Li. Controllable 3d face syn-
thesis with conditional generative occupancy fields. In Ad-
vances in Neural Information Processing Systems, 2022. 2
[61] Keqiang Sun, Shangzhe Wu, Ning Zhang, Zhaoyang Huang,
Quan Wang, and Hongsheng Li. Cgof++: Controllable 3d
face synthesis with conditional generative occupancy fields.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 2023. 2
[62] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow.
In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–
28, 2020, Proceedings, Part II 16, pages 402–419. Springer,
2020. 5
[63] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsuper-
vised learning of object frames by dense equivariant image
labelling. NeurIPS, 30, 2017. 5
[64] Raquel Urtasun, David J. Fleet, and Neil D. Lawrence. Mod-
eling human locomotion with topologically constrained la-
tent variable models. In Human Motion – Understanding,
10

Modeling, Capture and Animation, pages 104–118, Berlin,
Heidelberg, 2007. Springer Berlin Heidelberg. 3
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, 2017. 4,
5
[66] Yunbo Wang, Mingsheng Long, Jianmin Wang, Zhifeng
Gao, and Philip S Yu. Predrnn: Recurrent neural networks
for predictive learning using spatiotemporal lstms. NeurIPS,
30, 2017. 2
[67] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi.
Unsupervised learning of probably symmetric deformable
3D objects from images in the wild. In CVPR, 2020. 2
[68] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and An-
drea Vedaldi. DOVE: Learning deformable 3d objects by
watching videos. arXiv preprint arXiv:2107.10844, 2021. 2,
3
[69] Shangzhe Wu, Ameesh Makadia, Jiajun Wu, Noah Snavely,
Richard Tucker, and Angjoo Kanazawa. De-rendering the
world’s revolutionary artefacts. In CVPR, 2021. 2
[70] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rup-
precht, and Andrea Vedaldi. MagicPony: Learning articu-
lated 3d animals in the wild. In CVPR, 2023. 2, 3, 4, 5, 6,
8
[71] Jing Xiao, Jin xiang Chai, and Takeo Kanade. A closed-form
solution to non-rigid shape and motion recovery. In ECCV,
2004. 3
[72] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,
Forrester Cole, Huiwen Chang, Deva Ramanan, William T.
Freeman, and Ce Liu.
LASR: Learning articulated shape
reconstruction from a monocular video. In CVPR, 2021. 2,
3
[73] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vla-
sic, Forrester Cole, Ce Liu, and Deva Ramanan.
ViSER:
Video-specific surface embeddings for articulated 3d shape
reconstruction. In NeurIPS, 2021. 3
[74] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ra-
manan, Vedaldi Andrea, and Joo Hanbyul. BANMo: Build-
ing animatable 3d neural models from many casual videos.
In CVPR, 2022. 2, 3
[75] Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long
Lan, and Dacheng Tao. APT-36K: A large-scale benchmark
for animal pose estimation and tracking. In NeurIPS Dataset
and Benchmark Track, 2022. 5, 6, 7, 8, 12
[76] Chun-Han Yao,
Wei-Chih Hung,
Michael Rubinstein,
Yuanzhen Lee, Varun Jampani, and Ming-Hsuan Yang.
Lassie: Learning articulated shape from sparse image en-
semble via 3d part discovery. In NeurIPS, 2022. 2
[77] Jason Y Zhang, Panna Felsen, Angjoo Kanazawa, and Jiten-
dra Malik. Predicting 3d human dynamics from video. In
ICCV, 2019. 1
11

Appendices
A. Additional Qualitative Results
Results on Horses. More motion synthesis results for
horses are shown in Figure 6.
Please refer to the
supplementary.mp4 for more animation results. As
shown in the video, by sampling the learned motion la-
tent VAE, we can generate diverse motion patterns such
as eating with the head bending towards the ground,
walking with legs moving alternatively, and jumping
where front legs are lifted.
We trained our VAE model with a sequence length of
10 frames. To produce longer motion sequences as demon-
strated in the video, we first sample 2 latent codes to gen-
erate 2 motion sequences, each comprising 10 frames. We
then optimize 1 additional transition motion latents by en-
couraging the poses of the first frame and the last frame to
be consistent with the last frame and the first frame of two
consecutive sequences previously generated.
Results on Other Categories. We include more qualita-
tive results for giraffes, zebras and cows in Figure 7. For
different animal categories, our model automatically learns
different motion priors. For example, in the synthesized re-
sults, it is more common for horses to raise their hooves,
while neck movements are more typical for giraffes.
B. Implementation Details
Architecture. As explained in the paper, we adopt a spatio-
temporal transformer architecture for sequence feature en-
coding and motion decoding. As illustrated in Tab. 5, we
use the 4-layer transformer to implement the spatial and
temporal transformer encoder Es, Et and decoder Ds, Dt.
Given the DINO feature of the input image, we first con-
catenate the bone position as Positional Encoding to obtain
the feature with shape (JointNum, FrameNum, FeatureDim)
= (20 × 10 × 640). Then we adjust the feature dimension to
256 with a simple Linear layer and we concatenate a Bone-
FeatureQuery. We employ the 4-layer Es to query the global
feature of the 20 joints and Et to obtain the global feature
for all the 10 frames, represented by a mean value µ and
variance σ. With the output µ and σ, we reparameterize a
latent code z, which is first decoded in temporal dimension
by Dt and in spatial dimension by Ds. The recovered spatio-
temporal feature is eventually converted to the articulation
value of each joint at each time step.
C. Ablation Study
Sequence Length. We conducted experiments to under-
stand the effect of different sequence lengths during train-
ing (20 and 50 frames). To evaluate the longer motion se-
quences generated by these variants in a fair comparison, we
Operation
Output Size
Positional Encoding
20 × 10 × 640
Linear(640, 256)
20 × 10 × 256
Concat BoneFeatQuery
21 × 10 × 256
TransformerLayer × 4
1 × 10 × 256
Reshape
10 × 1 × 256
Concat muQuery and SigmaQuery
12 × 1 × 256
Positional Encoding
12 × 1 × 256
TransformerLayer × 4
2 × 1 × 256
Reparameterizion
1 × 1 × 256
TransformerLayer × 4
10 × 1 × 256
Reshape
1 × 10 × 256
TransformerLayer × 4
20 × 10 × 256
Reshape
10 × 20 × 256
Linear(256, 3)
10 × 20 × 3
Table 5. Architecture of the proposed spatio-temporal transformer
VAE.
Frame Num
10
20
50
MCD ↓
38.03
38.25
39.25
Table 6. Motion Chamfer Distance (MCD) on APT-36K [75] for
motion generation evaluation.
divide them into consecutive sub-sequences of 10 frames.
We use the same metric as introduced in the main paper, the
Motion Chamfer Distance (MCD) calculated between gen-
erated sequences and the annoated sequences in the APT-
36K dataset [75]. For the longer sequences, we average out
the MCD scores for all sub-sequences. The results are pre-
sented in Table 6.
Upon analyzing the results, we observed that the gener-
ated sequences still look plausible as the sequence length
increases from 10 to 20. However, a notable degradation in
performance can be seen as the sequence length increases to
50. This could potentially be attributed to limited capacity
of the motion VAE model as well as the limited size of the
training dataset. For our final model, we set the sequence
length to 10 which tends to yield the most satisfactory re-
sults with a reasonable training efficiency.
Spatio-Temporal Transformer Architecture. We con-
duct an ablation study to verify the effectiveness of the pro-
posed spatio-temporal transformer architecture. In particu-
lar, we remove the spatial transformer encoder and decoder,
Es and Ds and report the results in row 2 of Tab. 7. Specifi-
cally, instead of using the spatial transformer encoder Es to
fuse bone-specific local image features before passing them
to the temporal transformer encoder Et, we directly feed
the global image features {ϕ1, · · · , ϕT } into the temporal
encoder. Similarly, we also remove the spatial decoder Ds,
and directly decode a fixed set of bone rotations from the
12

Method Name
PCK@0.1
Mask IoU
1
Final (with ST-Transformer)
37.63%
62.03%
2
with S-Transformer
33.44%
58.89%
3
without motion VAE
44.30%
66.72%
Table 7. Ablation study on the architecture of the motion VAE
model.
Method Name
PCK@0.1
Mask IoU
1
λKL = 0.01
33.58%
59.85%
2
λKL = 0.001
37.63%
62.03%
3
λKL = 0.0001
35.75%
61.11%
Table 8. Ablation study on the weight of the KL divergence loss
λLKL.
temporal transformer decoder Dt.
Compared to the final model with spatio-temporal trans-
former architectures in row 1 of Tab. 7, the variant without
spatial transformer results in less accurate reconstructions,
and hence lower scores on the metrics. This confirms the ef-
fectiveness of the proposed spatial transformer in extracting
motion-specific spatial information from the images.
KL Loss Weight. To train the motion VAE, in addition to
the reconstruction losses, we also use the Kullback–Leibler
(KL) divergence loss in Equation (6) in the main paper. We
conducted an ablation study on its weight λKL to assess its
impact on the overall video reconstruction performance. As
shown in Table 8, λKL = 0.001 achieves the best recon-
struction performance and is used in all experiments from
the main paper.
D. Limitations
Despite promising results, this model can still be improved
on several fronts. Most critically, the articulated motion is
learned on top of a fixed bone topology, which is defined
with strong heuristics, e.g. the number of legs, and may
not generalize across distant animal species. Automatically
discovering the articulation structure jointly with the video
training will be an interesting future direction.
13

Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Figure 6. Additional Motion Synthesis Results on Horses. Conditioned on an input image, which can be either a real photo or a painting
of a horse, our model can generate realistic 4D animations of the instance. See the supplementary video for better visualizations.
14

Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Input Image
Generated 
3D Motion
Figure 7. Additional Motion Synthesis Results for Other Categories. Our model can also be trained on other categories besides horses,
and generates realistic motion sequences.
15

