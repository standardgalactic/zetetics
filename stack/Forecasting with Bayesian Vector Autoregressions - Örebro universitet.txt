 
http://www.oru.se/Institutioner/Handelshogskolan-vid-Orebro-universitet/Forskning/Publikationer/Working-papers/
 
 
 
 
 
 
WORKING PAPER 
 
 
 
12/2012 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Forecasting with Bayesian Vector Autoregressions 
 
 
 
Sune Karlsson 
Statistics 
 
 
 
ISSN 1403-0586 
 
 
Örebro University School of Business  
701 82 Örebro 
SWEDEN 
 
 

Forecasting with Bayesian Vector Autoregressions
Sune Karlsson∗
Department of Statistics, ¨Orebro University Business School
August 4, 2012
Abstract
Prepared for the Handbook of Economic Forecasting, vol 2
This chapter reviews Bayesian methods for inference and forecasting with VAR
models. Bayesian inference and, by extension, forecasting depends on numerical
methods for simulating from the posterior distribution of the parameters and spe-
cial attention is given to the implementation of the simulation algorithm.
JEL-codes: C11, C32, C53
Keywords: Markov chain Monte Carlo; Structural VAR; Cointegration; Condi-
tional forecasts; Time-varying parameters; Stochastic volatility; Model selection;
Large VAR
∗Sune.Karlsson@oru.se

Contents
1
Introduction
1
2
Bayesian Forecasting and Computation
2
2.1
Bayesian Forecasting and Inference . . . . . . . . . . . . . . . . . . . . . .
2
2.1.1
Vector Autoregressions . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Bayesian Computation and Simulation . . . . . . . . . . . . . . . . . . . .
7
2.2.1
Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . .
9
3
Reduced Form VARs
10
3.1
The Minnesota prior beliefs
. . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.1.1
Variations on the Minnesota prior . . . . . . . . . . . . . . . . . . .
12
3.2
Flexible prior distributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.2.1
The normal-Wishart prior . . . . . . . . . . . . . . . . . . . . . . .
14
3.2.2
The normal-diﬀuse and independent normal-Wishart priors . . . . .
16
3.2.3
A hierarchical prior for the hyperparameters . . . . . . . . . . . . .
17
3.3
The steady state VAR
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.4
Model speciﬁcation and choice of prior
. . . . . . . . . . . . . . . . . . . .
24
4
Structural VARs
26
4.1
”Unrestricted” triangular structural form . . . . . . . . . . . . . . . . . . .
27
4.2
Homogenous restrictions on the structural form parameters . . . . . . . . .
29
4.3
Identiﬁcation under general restrictions . . . . . . . . . . . . . . . . . . . .
33
5
Cointegration
37
5.1
Priors on the cointegrating vectors
. . . . . . . . . . . . . . . . . . . . . .
38
5.2
Priors on the cointegrating space
. . . . . . . . . . . . . . . . . . . . . . .
41
5.3
Determining the cointegrating rank . . . . . . . . . . . . . . . . . . . . . .
46
6
Conditional forecasts
48
7
Time-varying parameters and stochastic volatility
51
7.1
Time-varying parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
7.2
Stochastic volatility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
8
Model and variable selection
61
8.1
Restricting the parameter matrices - SSVS . . . . . . . . . . . . . . . . . .
61
8.2
Selecting variables to model . . . . . . . . . . . . . . . . . . . . . . . . . .
67
8.2.1
Marginalized predictive likelihoods
. . . . . . . . . . . . . . . . . .
67
8.2.2
Marginal likelihoods via Bayes factors . . . . . . . . . . . . . . . . .
69
9
High Dimensional VARs
70
9.1
Factor augmented VAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
9.2
Large BVARs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
9.2.1
Reducing parameter uncertainty by shrinkage
. . . . . . . . . . . .
74
9.2.2
Selecting variables - conjugate SSVS
. . . . . . . . . . . . . . . . .
76
9.3
Reduced rank VAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78

9.4
Predicting many variables
. . . . . . . . . . . . . . . . . . . . . . . . . . .
80
A Markov chain Monte Carlo Methods
82
A.1 Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
A.2 Metropolis-Hastings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
A.3 Autocorrelation in the Markov chain
. . . . . . . . . . . . . . . . . . . . .
85
A.4 Assessing convergence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
B State space models
88
B.1 Kalman ﬁlter
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
B.2 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
B.3 Simulation smoother . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
C Distributions
89

1
Introduction
Vector autoregressions (VARs) have become the workhorse model for macroeconomic fore-
casting. The initial use in economics was to a large degree motived by Sims (1980) critique
of the ”incredible restrictions” used by the large macroeconometric models developed in
the 1970s and much eﬀort was put into tools for policy analysis based on VAR models.
This role of the VAR model has to some degree been taken over by the current crop of
DSGE models, a new generation of theory based models which are – at times – ill at ease
with the data. The role of the VAR model as the baseline, serious, model for economic
forecasting is, however, unchallenged. The popularity stems in part from it’s relative
simplicity, ﬂexibility and ability to ﬁt the data but, of course, also from it’s success as a
forecasting device.
The ﬂexibility and ability to ﬁt the data comes from the rich parameterization of
VAR models brings with it a risk of overﬁtting the data, of imprecise inference and
large uncertainty about the future paths projected by the model.
This is essentially
the frequentist argument for Bayesian VAR models and one reason why Bayesian VAR
models forecast better than VARs estimated with frequentist techniques.
The widely
used Minnesota prior introduced by Litterman (1979) is a set of data centric prior beliefs
that shrinks the parameters towards a stylized representation of macroeconomic data
thereby reducing parameter uncertainty and improving forecast accuracy. The Bayesian
argument is diﬀerent. The Minnesota prior captures widely held beliefs about the long
run properties of the data, properties that are not readily apparent in the short samples
typically used for estimation. Bayes theorem then provides the optimal way of combining
these two sources of information leading to sharper inference and more precise forecasts.
The development of eﬃcient numerical techniques for evaluating posterior distributions is
also a contributing factor to the attractiveness of Bayesian methods. It is now possible to
tackle more complex problems under realistic assumptions when we no longer are limited
to problem formulations that lead to analytical solutions.
This chapter surveys Bayesian approaches to inference in VAR models with a focus on
forecasting. One important feature of the chapter is that it gathers many algorithms for
simulating from the posterior distribution of the parameters, some of which have not been
clearly stated previously. This provides the necessary tools for analyzing the posterior
and predictive distributions and forecast with the models and priors that are studied in
the chapter. Koop and Korobilis (2009) and DelNegro and Schorfheide (2011) provides
complementary reviews of Bayesian VAR models, Koop and Korobilis (2009) with a focus
on models that allows for time-varying parameters and stochastic volatility while DelNegro
and Schorfheide (2011) has the broader remit of Bayesian macroeconometrics.
Section 2 lays the foundations by placing the task of forecasting in a Bayesian context
and reviews modern simulation techniques for exploring posterior and predictive distri-
butions. Section 3 provides the basic building blocks for forecasting with Bayesian VAR
models by introducing the Minnesota prior beliefs in the context of reduced form VAR
models and reviews families of prior distributions that have been found useful for ex-
pressing the prior beliefs. The more general issue of model speciﬁcation is also discussed
and one important message that emerges is that, in line with a general conclusion in the
forecasting literature, simple methods works quite well.
The remaining sections can largely be read independently. Section 4 reviews Bayesian
1

analysis of a VAR in structural form (SVAR) and section 5 studies the vector error
correction (VECM) form of a VAR model. Both SVAR and VECM models have the
potential to improve forecast performance if the hard restrictions they impose on the model
are at least aproximately correct but they have seen relatively little use in forecasting
applications, in particular in their Bayesian ﬂavor.
This is partly because it is only
recently that satisfactory procedures for posterior inference in these models have become
available.
Section 6 consider forecasts conditional on future events. This can be a useful tool for
incorporating judgement and other late breaking information that is (perhaps due to the
slow release of data) not in the information set used by the model. In a policy setting
conditional forecasts are useful for what-if analysis and for producing forecasts that are
consistent with the current policy.
Section 7 relaxes the constant parameter assumption and shows how to allow for time-
varying parameters and stochastic volatility in Bayesian VAR models. There are encour-
aging studies that indicate that both time-varying parameters and stochastic volatility
can improve the forecast performance but both can also lead to a dramatic increase in
the number of parameters in a model. There is consequently a greater risk of overﬁt-
ting the data. The methods for model and variable selection discussed in Section 8 can
then be useful in addition to the Bayesian shrinkage that is routinely applied through
the prior. Section 8 provides tools both for selecting the variables to include as left hand
side variables in a VAR model and for reducing the number of parameters by eﬀectively
excluding some variables and lags from the right hand side. This touches on the issue
of model averaging and forecast combination which is not discussed here in spite of this
being a natural extension of the Bayesian framework for treating parameter uncertainty.
The reader is instead referred to Geweke and Whiteman (2006) and Timmermann (2006).
The ﬁnal section 9 considers the task of forecasting in a data rich environment where
several hundred potential predictors may be available. Recent work shows that Bayesian
VARs can be competitive in this setting as well and important recent developments are
reviewed.
2
Bayesian Forecasting and Computation
This section provides a brief overview of the underlying principles of Bayesian inference
and forecasting. See Geweke and Whiteman (2006) for a more complete discussion and,
for example, Gelman, Carlin, Stern and Rubin (2003), Geweke (2005) or Koop (2003) for
a text book treatment of Bayesian inference.
2.1
Bayesian Forecasting and Inference
The fundamental object in Bayesian forecasting is the (posterior) predictive distribution,
the distribution p (yT+1:T+H|YT) of future datapoints, yT+1:T+H =
 y′
T+1, . . . , y′
T+H
′
conditional on the currently observed data, YT = {yt}T
t=1 . By itself the predictive dis-
tribution captures all relevant information about the unknown future events. It is then
up to the forecaster or user of the forecast which features of the predictive distribution
are relevant for the situation at hand and should be reported as the forecast. This could,
2

for example, be the mean, mode or median of the predictive distribution together with a
probability interval indicating the range of likely outcomes.
Formally this is a decision problem which requires the speciﬁcation of a problem de-
pendent loss function, L
 a, yT+1:T+H

, where a is the action taken, the vector of real
numbers to report as the forecast, and yT+1:T+H represents the unknown future state
of nature. The Bayesian decision is to choose the action (forecast) that minimizes the
expected loss conditional on the available information YT,
E

L
 a, yT+1:T+H

|YT

=
Z
L
 a, yT+1:T+H

p (yT+1:T+H|YT) dyT+1:T+H.
For a given loss function and predictive distribution, p (yT+1:T+H|YT) the solution to the
minimization problem is a function of the data, a (YT) . For speciﬁc loss functions a (YT)
takes on simple forms. With quadratic loss function,
 a −yT+1:T+H
′  a −yT+1:T+H

, the
solution is the conditional expectation, a (YT) = E (yT+1:T+H|YT) , and with an absolute
value loss function, P |ai −ωi| , the conditional mode.
It remains to specify the form of the predictive distribution. This requires the speciﬁ-
cation of three diﬀerent distributions that completes the description of the problem, the
distribution of the future observations conditional on unknown parameter values, θ, and
the observed data, p (yT+1:T+H|YT, θ) , the distribution of the observed data – that is, the
model or likelihood – conditional on the parameters, L (YT|θ) , and the prior distribution,
π (θ) , representing our prior notions about likely or ”reasonable” values of the unknown
parameters, θ. In a time series and forecasting context, the likelihood L (YT|θ) usually
takes the form
L (YT|θ) =
TY
t=1
f (yt|Yt−1, θ)
with the history in Y1, Y2, . . . suitably extended to include initial observations that the
likelihood is conditional on.1 The distribution of future observations is of the same form,
f (yT+1:T+H|YT, θ) =
T+H
Y
t=T+1
f (yt|Yt−1, θ) .
With these in hand straightforward application of Bayes Rule yields the predictive distri-
bution as
p (yT+1:T+H|YT) = p (yT+1:T+H, YT)
m (YT)
=
R
f (yT+1:T+H|YT, θ) L (YT|θ) π (θ) dθ
R
L (YT|θ) π (θ) dθ
.
(1)
In practice an intermediate step through the posterior distribution of the parameters,
p (θ|YT) =
L (YT|θ) π (θ)
R
L (YT|θ) π (θ) dθ ∝L (YT|θ) π (θ) ,
(2)
is used with the predictive distribution given by
p (yT+1:T+H|YT) =
Z
f (yT+1:T+H|YT, θ) p (θ|YT) dθ.
(3)
1It is, of course, in many cases also possible to complete the likelihood with the marginal distribution
for the ﬁrst, say p, observations.
3

Note that the latter form of the predictive distribution makes it clear how Bayesian fore-
casts accounts for both the inherent uncertainty about the future embodied by f (yT+1:T+H|YT, θ)
and the uncertainty about the true parameter values described by the posterior distribu-
tion p (θ|YT) .
While the posterior distribution of the parameters may be available in closed form
in special cases when conjugate prior distributions are used closed form expressions for
the predictive distribution are generally unavailable when lead times greater than 1 are
considered. This makes the form (3) of the predictive distribution especially attractive.
Marginalizing out the parameters of the joint distribution of yT+1:T+H and θ analyti-
cally may be diﬃcult or impossible, on the other hand (3) suggests a straightforward
simulation scheme for the marginalization. Supposing that we can generate random num-
bers from the posterior p (θ|YT), for each draw of θ generate a sequence of draws of
yT+1, . . . , yT+H by repeatedly drawing from f (yt|Yt−1, θ) and adding the draw of yt to
the conditioning set for the distribution of yt+1. This gives a draw from the joint distribu-
tion of (θ, yT+1, . . . , yT+H) conditional on YT and marginalization is achieved by simply
discarding the draw of θ. Repeating this R times gives a sample from the predictive dis-
tribution that can be used to estimate E (yT+1:T+H|YT) or any other function or feature
a (YT) of the predictive distribution of interest.
The denominator in (1) and (2),
m (YT) =
Z
L (YT|θ) π (θ) dθ,
(4)
is known as the marginal likelihood or prior predictive distribution and plays a crucial
role in Bayesian hypothesis testing and model selection. Consider two alternative models,
M1 and M2, with corresponding likelihoods L (YT|θ1, M1) , L (YT|θ2, M2) and priors
π (θ1|M1) , π (θ2|M2) . Supposing that one of M1 and M2 is the true model but that we
are not certain which of the competing hypothesis or theories embodied in the models is
the correct one we can assign prior probabilities, π (M1) and π (M2) = 1 −π (M1) , that
each of the models is the correct one. With these in hand Bayes Rule yields the posterior
probabilities that the models are correct as
p (Mi) =
m (YT|Mi) π (Mi)
m (YT|M1) π (M1) + m (YT|M2) π (M2)
(5)
with m (YT|Mi) =
R
L (YT|θi, Mi) π (θi|Mi) dθi. The posterior odds for model 1 against
model 2 is given by
p (M1)
p (M2) = m (YT|M1) π (M1)
m (YT|M2) π (M2) = m (YT|M1)
m (YT|M2) × π (M1)
π (M2)
the Bayes factor BF1,2 = m (YT|M1) /m (YT|M2) comparing M1 to M2 times the prior
odds. Model choice can be based on the posterior odds but it is also common to use
the Bayes factors directly, implying equal prior probabilities. The Bayes factor captures
the data evidence and can be interpreted as measuring how much our opinion about the
models have changed after observing the data. The choice of model should of course
take account of the losses associated with making the wrong choice. Alternatively, we can
avoid conditioning on one single model being the correct one by averaging over the models
4

with the posterior model probabilities as weight. That is, instead of basing our forecasts
on the predictive distribution p (yT+1:T+H|YT, M1) and conditioning on M1 being the
correct model we conduct Bayesian Model Averaging (BMA) to obtain the marginalized
(with respect to the models) predictive distribution
p (yT+1:T+H|YT) = p (yT+1:T+H|YT, M1) π (M1) + p (yT+1:T+H|YT, M2) π (M2)
which accounts for both model and parameter uncertainty.
The calculations involved in (5) are non-trivial and the integral
R
L (YT|θi, Mi) π (θi|Mi) dθi
is only well deﬁned if the prior is proper. That is, if
R
π (θi|Mi) dθi = 1. For improper
priors, such as a uniform prior on the whole real line, the integral is not convergent and
the scale is arbitrary. For the uniform prior we can write π (θi|Mi) = ki and it follows
that m (YT) ∝ki and the Bayes factors and posterior probabilities are arbitrary.2 There
is, however, one circumstance where improper prior can be used. This is when there
are parameters that are common to all models, for example an error variance. We can
then partition θi =

eθi, σ2
and use proper priors for eθi and an improper prior, such as
π (σ2) ∝1/σ2, for the variance since the common scale factor cancels in the calculation
of posterior model probabilities and Bayes factors.
2.1.1
Vector Autoregressions
To illustrate the concepts we consider the VAR model with m variables
y′
t =
p
X
i=1
y′
t−iAi + x′
tC + u′
t
(6)
= z′
tΓ + u′
t
with xt a vector of d deterministic variables, z′
t =
 y′
t−1, . . . , y′
t−p, x′
t

a k = mp + d
dimensional vector and Γ =
 A′
1, . . . A′
p, C′′ a k × m matrix and normally distributed
errors, ut ∼N (0, Ψ) . That is, f (yt|Yt−1, θ) = N (yt; z′
tΓ, Ψ) . For simplicity we take the
prior to be uninformative (diﬀuse), a uniform distribution for Γ and a Jeﬀreys’ prior for
Ψ,3
π (Γ, Ψ) ∝|Ψ|−(m+1)/2 .
Using (2) we see that the joint posterior distribution of Γ and Ψ is proportional to the
likelihood function times the prior. Stacking the data in the usual way we can write the
model as
Y = ZΓ + U
2Note
that
this
does
not
aﬀect
the
posterior
distribution
as
long
as
the
integral
R
L (YT |θi, Mi) π (θi|Mi) dθi is convergent since the arbitrary scale factor cancels in (2).
3This is an improper prior and the use of improper prior distributions is not always advisable as this
can lead to improper posterior distributions. In the normal regression model with this prior the posterior
will be proper if the matrix of explanatory variables has full column rank, i.e. when the OLS estimate is
unique.
5

and the likelihood as
L (Y|Γ, Ψ) = (2π)−mT/2 |Ψ|−T/2 exp

−1
2
X
(y′
t −z′
tΓ) Ψ−1 (y′
t −z′
tΓ)′

(7)
= (2π)−mT/2 |Ψ|−T/2 exp

−1
2 tr

(Y −ZΓ) Ψ−1 (Y −ZΓ)′
= (2π)−mT/2 |Ψ|−T/2 exp

−1
2 tr

Ψ−1 (Y −ZΓ)′ (Y −ZΓ)

where Y and U are T × m matrices and Z is T × k. Adding and subtracting ZbΓ for
bΓ = (Z′Z)−1 Z′Y, the OLS estimate, and multiplying with the prior we have the joint
posterior as
p (Γ, Ψ|YT) ∝|Ψ|−T/2 exp

−1
2 tr

Ψ−1 
Y −ZbΓ
′ 
Y −ZbΓ

× exp

−1
2 tr

Ψ−1 
Γ −bΓ
′
Z′Z

Γ −bΓ

|Ψ|−(m+1)/2 .
Focusing on the part involving Γ and noting that
tr

Ψ−1 
Γ −bΓ
′
Z′Z

Γ −bΓ

= (γ −bγ)′  Ψ−1 ⊗Z′Z

(γ −bγ)
(8)
for γ = vec (Γ) and bγ = vec

bΓ

=

Im ⊗(Z′Z)−1 Z′
y4 we recognize this as a the
kernel of a multivariate normal distribution conditional on Ψ with mean bγ and variance-
covariance matrix Ψ⊗(Z′Z)−1 ,
γ|YT, Ψ ∼N

bγ, Ψ⊗(Z′Z)−1
.
With the special Kronecker structure of the variance-covariance matrix this is a matric-
variate normal5 distribution for Γ and we can also write the conditional posterior as
Γ|YT, Ψ ∼MNkm

bΓ, Ψ, (Z′Z)−1
. Integrating out γ from the joint posterior is triv-
ial using the properties of the normal distribution and we have the marginal posterior
distribution for Ψ as
p (Ψ|YT) ∝|Ψ|−(T+m+1−k)/2 exp

−1
2 tr

Ψ−1S

with S =

Y −ZbΓ
′ 
Y −ZbΓ

. This can be recognized as the kernel of an inverse
Wishart distribution with T −k degrees of freedom,
Ψ|YT ∼iWm (S,T −k) .
4bγ is the GLS estimate in the univariate regression model for y = vec (Y) = (Im ⊗Z) γ + u
with V (u) = Ψ ⊗IT . That is bγ
=
h
(Im ⊗Z)′ (Ψ ⊗IT )−1 (Im ⊗Z)
i−1
(Im ⊗Z)′ (Ψ ⊗IT )−1 y =
 Ψ−1 ⊗Z′Z
−1  Ψ−1 ⊗Z′
y =
h
Ψ ⊗(Z′Z)−1i  Ψ−1 ⊗Z′
y =
h
Im ⊗(Z′Z)−1 Z′i
y.
5See Appendix C for a review of some multivariate distributions.
6

We refer to the joint posterior of Γ and Ψ as a normal-Wishart distribution.
Alternatively, we can integrate out Ψ of the joint posterior. With the Kronecker vari-
ance matrix of the conditional normal distribution this yields a matricvariate t-distribution
with T −k degrees of freedom as the marginal posterior for Γ,
Γ|YT ∼Mtkm (bγ, Z′Z, S, T −k) .
(9)
This is the natural generalization of the scalar variance case where x|σ ∼N (µ, σ2V) with
σ−2 Gamma distributed with shape parameter v/2 and scale parameter 1/2 (or χ2 with
v degrees of freedom) yields a marginal t-distribution for x with v degrees of freedom.
For later reference note that the product of the prior and likelihood (7) has the form
of an inverse Wishart distribution for Ψ conditional on Γ,
p (Ψ|YT, Γ) ∝|Ψ|−(T+m+1)/2 exp

−1
2 tr

Ψ−1 (Y −ZΓ)′ (Y −ZΓ)

Ψ|YT, Γ ∼iW
 (Y −ZΓ)′ (Y −ZΓ) , T

.
Turning to the forecasts, recursive substitution in (6) with p = 2 yields
y′
T+1 = y′
TA1 + y′
T−1A2 + x′
T+1C + u′
T+1
y′
T+2 = y′
T
 A2
1 + A2

+ y′
T−1A2A1 + x′
T+2C + x′
T+1CA1+u′
T+2 + u′
T+1A1
etc. The one-step ahead predictive distribution for y′
T+1 can be shown to be matricvari-
ate t, Mt1m

z′
T+1bΓ,
 1 + z′
T+1 (Z′Z)−1 zT+1
−1 , S, T −k

. For higher lead times we have
increasingly non-linear functions of the parameters and no closed form expressions for
the predictive distribution are available. Instead the simulation scheme for generating a
sample from the predictive distribution described above can be used. Simulating from
the posterior and predictive distributions is particularly straightforward in this case and
the procedure for simulating from the predictive distribution is given as Algorithm 1.
2.2
Bayesian Computation and Simulation
Having a simulated sample, ey(j)
T+1, . . . ey(j)
T+H, of size R from the predictive distribution in
hand it is straightforward to estimate features, such as probability intervals, expectations,
etc., of the predictive distribution that we wish to report. An estimate of the minimum
mean square error (MSE) h period ahead forecast, yT (h) = E (yT+h|YT) , is given by the
simple average of the simulated forecasts,
byT (h) = 1
R
R
X
j=1
ey(j)
T+h.
(11)
With direct sampling and hence iid draws, as in the previous section, this is guaranteed
to be a consistent and asymptotically normal estimator if V (yT+h|YT) exists,
√
R (byT (h) −yT (h))
d→N (0, V (yT+h|YT)) .
Asymptotically motivated error bounds are thus readily available as 1 −α conﬁdence
intervals. Analogous results apply to any function of yT+h with ﬁnite second moment.
7

Algorithm 1 Simulating the predictive distribution with a normal-Wishart posterior
For j = 1, . . . R
1. Generate Ψ(j) from the marginal posterior Ψ|YT ∼iWm (S,T −k) distribution
using, e.g. the algorithm of Geweke (1988).
2. Generate Γ(j) from the conditional posterior Γ|YT, Ψ(j)∼N

bΓ, Ψ(j), (Z′Z)−1
3. Generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
(10)
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
Note that it only a factor P of Ψ(j) = PP′ is needed for step 3 and steps 1 and 2
can be replaced by Algorithm 22 which avoids the explicit computation of Ψ(j).
Estimates of prediction intervals are readily obtained by selecting the appropriate
quantiles from the simulated predictive distribution. Let eyT+h,(i) denote the ith order
statistic, a 1 −α prediction interval is then give by
 eyT+h,(l), eyT+h,(u)

for l = ⌊Rα/2⌋
and u = ⌊R (1 −α/2)⌋where ⌊·⌋denotes the integer part. eyT+h,(l) is an estimate of the
α/2 quantile ξα/2 of the predictive distribution, assessing the precision of this estimate
is somewhat more involved than for simple averages. For continuous distributions f (x),
the sample order statistic X(m) for m = ⌊nq⌋is a consistent and asymptotically normal
estimator of the population quantile ξq but the asymptotic variance depends on the under-
lying distribution, √n
 X(m) −ξq

d→N
 0, q (1 −q) /f (ξq)2
, and requires an estimate of
the density at ξq in order to be operational.
An alternative procedure based on order statistics can be used to produce distribution
free conﬁdence intervals for the population quantile. We seek order statistics X(r) and
X(s) that satisﬁes P
 X(r) < ξq < X(s)

≈1 −α. Noting that the probability statement
P
 X(r) < ξq < X(s)

is equivalent to the statement
P (at least r but no more than s −1 observations satisfy Xi < ξq)
this can be evaluated as a Binomial probability,
P
 X(r) < ξq < X(s)

=
s−1
X
k=r
n
k

qk (1 −q)n−k ,
and for small n it is straightforward to determine values of r and s that gives (approx-
imately) the desired conﬁdence level. For large n the Binomial distribution can be ap-
8

proximated by a normal distribution and r and s obtained as
r =
j
nq −z1−α/2
p
nq (1 −1)
k
s =
l
nq + z1−α/2
p
nq (1 −1)
m
.
2.2.1
Markov chain Monte Carlo
In general a simulation strategy similar to the one discussed in section 2.1 can be devised to
generate a sample from the predictive distribution. The main diﬃculty is how to generate
draws from the posterior distribution of the parameters when, unlike Algorithm 1, it is
not possible to sample directly from the posterior. The two most common procedures
for solving this problem is importance sampling (Kloek and van Dijk (1978) and Geweke
(1989)) and Markov chain Monte Carlo (MCMC). Here we will focus on MCMC methods
as these are, in general, quite straightforward to implement with VAR models. Geweke
and Whiteman (2006), Chib and Greenberg (1995) and Geweke (1999) gives a more in-
depth discussion and book length treatments include Gamerman (1997) and Robert and
Casella (1999).
The idea behind MCMC techniques is to construct a Markov chain for the parameters
θ which has the posterior distribution as it’s (unique) stationary distribution and fulﬁlls
the additional requirement that we can generate random number from the conditional
distribution, f
 θ(j+1)|θ(j)
that deﬁnes the transition kernel. If the initial draw, θ(0),
could somehow be drawn from the posterior distribution, all the following draws will also
be from the posterior distribution by virtue of this being the stationary distribution of
the chain. But this is, of course, not possible (or we would not need to resort to MCMC
methods) and the issue of convergence becomes important. Will the distribution of the
draws from the Markov chain converge to the posterior distribution if we start the chain
at an arbitrary point in the parameter space? And if so, how many draws are required
before the distribution of the draws is ”close enough” to the posterior distribution?
A precise answer to the ﬁrst question involves highly technical conditions (see Tierny
(1994)). It is, however, possible to state stronger conditions that are suﬃcient for conver-
gence and relatively easy to check (e.g. Geweke (2005, section 4.5)). One such condition
is that, loosely speaking, P
 θ(j+1) ∈A|θ(j)
> 0 for all θ(j) and any set A with positive
probability under the posterior distribution. The Markov chain is then ergodic and allows
consistent estimation of posterior quantities. The second question does not have a precise
answer and would be unimportant if we could generate an inﬁnite number of draws from
the chain. In practice we will only have a ﬁnite number of draws available and including
draws from the beginning of the chain, before it has converged to the posterior distri-
bution, can give very bad estimates of posterior quantities. As a practical matter it is
thus important to discard a suﬃciently large number, B, of initial draws (the burn-in).
Lacking a precise answer, the choice of the size of the burn-in is subjective and it is better
to err on the side of caution. Diagnostics that are useful in determining B are discussed
below.
The performance of the Markov chain and the precision of estimates is related to the
issue of convergence. Even if the Markov chain is convergent it might move very slowly
through the parameter space (mix slowly) with high autocorrelation between the draws
and a very large number of draws might be needed in order to explore the parameter space.
9

Even a well performing Markov chain will by construction have some, typically positive,
autocorrelation in the draws which tends to impact the precision of estimates negatively
with a larger variance of estimates of, say the posterior mean, than if direct sampling
had been possible. For an assessment of the precision of estimates and probabilistic error
bounds a central limit theorem is needed. This in turn requires a rate condition on the
speed of convergence to the posterior distribution. Let g (θ) be an arbitrary function of the
parameters and g the average over R draws from the chain. If the chain is geometrically
ergodic then
√
R (g −E (g|YT))
d→N
 0, σ2
MC

if E

g2+δ|YT

< ∞for δ > 0 and
σ2
MC = V (g|YT) + 2
∞
X
k=1
Cov

g
 θ(j)
, g
 θ(j+k)
|YT

.
(12)
If, in addition, the chain is uniformly ergodic then the result holds for δ = 0.
Technical details for the implementation of Gibbs and Metropolis-Hastings samplers
are given in Appendix A, including how to assess convergence and how to estimate the
variance, σ2
MC, of the Monte Carlo estimate. It should be clear to the reader that these
methods come with a health warning: Naive use without careful assessment of the behavior
and convergence property of the Markov chain may lead to completely misleading results.
3
Reduced Form VARs
For forecasting purposes reduced form Bayesian VARs, that is models that essentially
leaves the parameter matrices and the variance-covariance matrix of ut in the VAR y′
t =
Pp
i=1 y′
t−iAi + x′
tC + u′
t unrestricted have proven to be quite successful. While having a
long tradition in time series analysis their use in economic forecasting was limited until
Sims (1980) inﬂuential critique of the ”incredible” identifying restrictions used in the large
scale macroeconometric models of the day. Instead Sims argued in favour of VAR-models
built essentially on considerations of the time series properties of the data. While being
powerful forecast devices that can ﬁt the data well, VAR-models may require relatively
large lag lengths p in order to match the time series properties of the data which, with
the many parameters to estimate can cause poor forecasting performance. One possible
solution to the problems caused by the rich parameterization is to consider the larger
class of VARMA-models (see L¨utkepohl (2006)) which may be able to represent the data
in a more parsimonious fashion.
3.1
The Minnesota prior beliefs
Taking a diﬀerent route, Litterman(1979, 1980) argued from a largely frequentist view
point, using the analogy with Ridge regression and shrinkage estimation, that the precision
of estimates and forecasting performance can be improved by incorporating ”restrictions”
in the form of a prior distribution on the parameters. Litterman’s prior formulation is
essentially based on stylized facts about his data, macroeconomic variables for the US that
could be well characterized by unit root processes and he proposed shrinking towards a
univariate random walk for each variable in the VAR.
10

Recall the multivariate regression formulation for the VAR with m variables, y′
t =
z′
tΓ + u′
t for z′
t =
 y′
t−1, . . . , y′
t−p, x′
t

and elements γij of Γ. The shrinkage towards uni-
variate random walks corresponds the setting the prior mean of Γ to
γij = E (γij) =
 1, ﬁrst own lag, i = j
0, i ̸= j
.
(13)
Litterman suggested applying a harder shrinkage towards zero for longer lags, reﬂecting
the prior notion that more distant observations are less inﬂuential. In addition, a diﬀer-
ent amount of shrinkage is applied to lags of the dependent variable than to lags of other
variables in the same equation. Typically more shrinkage is applied to lags of other vari-
ables to reinforce the univariate random walk nature of the prior. Speciﬁcally, Litterman
suggested setting the prior standard deviations to
τij = sd (γij) =



π1/lπ3, lag l of the dependent variable, i = (l −1) m + j
(π1π2sj) / (lπ3sr) , lag l of variable r ̸= j, i = (l −1) m + r
∞, deterministic variables, i = mp + 1, . . . , k
.
(14)
Here sj/sr is a scale factor accounting for the diﬀerent variances of the dependent and
explanatory variables, π1 is referred to as the ”overall tightness”, π2 the ”relative tightness
of other variables” and π3 the ”lag decay rate”. The inﬁnite standard deviations for the
coeﬃcients on the deterministic variables xt corresponds to an improper uniform prior on
the whole real line and could, without aﬀecting the results, be replaced with an arbitrary
large value to obtain a proper prior. The prior is completed by using independent normals
for each regression coeﬃcient on the lags, γij ∼N

γij, τ 2
ij

.
To reduce the computational burden Litterman proceeded to estimate the VAR equa-
tion by equation rather than as a system of equations. The likelihood is normal and
the error variances are assumed to be known, V (utj) = s2
j, where s2
j is the OLS resid-
ual variance for equation j in the VAR or a univariate autoregression for variable j.
Equation by equation estimation is, in fact, appropriate if the variance of ut is diagonal,
Ψ = diag (s2
1, . . . , s2
m) but, as noted by Litterman, suboptimal if the error terms are cor-
related. Taking the error variances to be known (although data based) is, of course, also
a simpliﬁcation motivated by computational expediency.
For computational purposes, as well as a way to think about the prior in terms of
implications for the data, it is useful to note that the prior γij ∼N

γij, τ 2
ij

can be
restated as
sj
τij
γij = sj
τij
γij + euij
where euij ∼N
 0, s2
j

. The prior information for equation j can thus be written as pseudo
data,
rj = Rjγj + euj
with element i of rj set to γijsj/τj and element r, s of Rj zero for r ̸= s and sj/τi,j for
r = s = 1, . . . , mp. One can then apply the mixed estimation technique of Theil and
Goldberger (1960), that is apply OLS to the augmented regression equation
 yj
rj

=
 Z
Rj

γj +
 uj
euj

(15)
11

with known variance error variance s2
j. This yields the estimate
γj =
 Z′Z + R′
jRj
−1  Z′y + R′
jr

with variance
Vj = s2
j
 Z′Z + R′
jRj
−1
which corresponds to the posterior mean and variance, γj|YT ∼N
 γj, Vj

under the
assumption that uj ∼N
 0, s2
jI

with an (improper) normal prior for γj with mean
γj and precision (inverse variance)
1
s2
j R′
jRj. To see this note that rj = Rjγj and that
applying the Bayesian calculations directly leads to Vj =

1
s2
j Z′Z+ 1
s2
j R′
jRj
−1
and γj =
V

1
s2
j Z′Zbγ + 1
s2
j R′
jRjγ

= V

1
s2
j Z′yj + 1
s2
j R′
jrj

for bγ = (Z′Z)−1 Z′yj.
It remains to specify the prior hyperparameters π1, π2 and π3. Litterman(1979, 1980)
conducted several exercises to evaluate the eﬀect of the hyperparameters on the out of
sample forecast performance. Suitable choices for his data appear to be π1 ≈0.2, π2 ≈0.2
and π3 = 1. These are also close to the hyperparameters used in true out of sample forecast
results reported in Litterman (1986).
The actual forecasts produced by Litterman were not based on the predictive distribu-
tion (3), in an additional bow to the limited computation resources of the time Litterman
approximated the mean of the predictive distribution by calculating the forecasts using
the posterior means γj of the parameters and the chain rule of forecasting.
In the remainder of this chapter we will refer to priors with moments similar to (13)
and (14) as Minnesota type priors or as priors based on the Minnesota prior beliefs.
The term Litterman prior is reserved for the combination of these prior beliefs with the
assumption of a diagonal and known error variance matrix.
Forecasting performance
Using the Minnesota prior and the forecasting procedure
outlined above Litterman started issuing monthly forecasts from a six variable VAR with
real GNP, the GNP price deﬂator, real business ﬁxed investments, the 3-month treasury
bill, the unemployment rate and the money supply in 1980. Five years later, and with the
model essentially unchanged Litterman (1986) and McNees (1986) report on the forecast
accuracy of these true out of sample forecasts compared to commercial forecasts based on
large scale macroeconometric models. There is no clear winner in this comparison, the
BVAR forecasts dominated for the real variables (real GNP, investments and unemploy-
ment) but were among the worst for inﬂation and the T-bill rate.
3.1.1
Variations on the Minnesota prior
Many variations on the Minnesota prior have been suggested, common ones include
• Stationary variables: For variables believed to be stationary the prior mean on the
ﬁrst lag can be set to a value less than 1, for example γjj = 0.9 if the variable is
believed to be relatively persistent.
• Deterministic variables: Set the prior standard deviations to τij = π1π4sj, this has
the advantage of leading to a proper prior for the coeﬃcients on deterministic vari-
ables while still being uninformative about γij by setting π4 (moderately) large.
12

• ”Exogenous” variables: Set the prior standard deviation to τij = (π1π5sj) / (lπ3sr) ,
for lag l of the ”endogenous” variable r in the equation for the ”exogenous” depen-
dent variable j, i = (l −1) m+r. This is, for example, useful when modelling a small
open economy with ”rest of the world” variables included in the model. Forecasting
is simpliﬁed if these variables are included in yt as no external forecasts are needed.
Setting π5 small shrinks γij aggressively towards zero and allows us to express that
the rest of the world variables are essentially exogenous to the domestic economy.
• Sum of coeﬃcients prior: This prior (introduced by Doan, Litterman and Sims
(1984)) expresses the prior notion that the sum of coeﬃcients on own lags is 1 and
the sum of coeﬃcients on the lags of each of the other variables is 0 as well as the
idea that the recent average of the variable should be a reasonable forecast. To
implement this add m rows to Rj which are zero except for the p positions in the ith
row corresponding to variable i = 1, . . . , m, i.e. row i is given by
 w′
i ⊗j′
p, 0

where
the zeros correspond to the deterministic variables, element i of wi is y0,isi/ (π1π6sj)
for y0i = 1
p
P0
t=1−p yt,i the average of the initial conditions for variable i and the re-
maining m−1 elements zero, jp is a p×1 vector of ones. In addition add m elements
to rj with the jth element equal to y0,j/ (π1π6) . The prior induces correlation be-
tween the coeﬃcients on the same variable (the prior precision
1
s2
j R′
jRj is no longer
a diagonal matrix) and forces the model towards a random walk with possible drift
for variable j as π6 →0.
• Dummy initial observations prior: Add a row
 y′
0 ⊗j′
p, x′
0

/ (π1π7sj) to Rj and
y0j/ (π1π7sj) to rj. This prior also implies that the initial observations is a good
forecast without enforcing speciﬁc parameter values and induces prior correlation
among all parameters in the equation. Sims (1993) argues that the dummy initial
observations prior is preferable to the sum of coeﬃcients prior. As π7 →0 the prior
implies that either all variables are stationary with mean y0 or that there are unit
root components without drift (if there are no trends in xt).
3.2
Flexible prior distributions
The basic setup of Litterman has been generalized in several directions, attempting to
relax some of the more restrictive assumptions that were motivated by the computational
limitations of the time or that allows diﬀerent ways of expressing the prior beliefs. Com-
mon to these works is that they maintain the basic ﬂavour of the Minnesota prior as a
data centric speciﬁcation that embodies stylized facts about the time series properties of
the data.
Kadiyala and Karlsson (1993,1997) relaxes the assumption of a known diagonal error
variance-covariance matrix, Ψ, and studies the eﬀect of varying the family of distribution
used to parameterize the prior beliefs. They considered the diﬀuse prior (which we have
already encountered in section 2.1.1), the conjugate normal-Wishart prior, the normal-
diﬀuse prior and an adaption of the extended natural conjugate (ENC) prior originally
proposed by Dr`eze and Morales (1976) in the context of simultaneous equation models.
Kadiyala and Karlsson (1993) focuses on the forecasting performance and conducts three
small forecasting horse races comparing the forecasting performance of the ”new” priors
with the Minnesota prior and forecasts based on OLS estimates. With the exception of
13

the diﬀuse prior the priors are speciﬁed to embody prior beliefs about Γ that are similar
to the Minnesota prior. With the Minnesota prior and OLS the forecasts are calculated
using the chain rule based whereas Monte Carlo methods are used to evaluate the expected
value of the predictive distribution with the other priors. There is no clear cut winner,
priors that allow for correlation between equations tend to do better.
Kadiyala and Karlsson (1997) studies the same four priors but this time the focus
is on the implementation and eﬃciency of Monte Carlo methods for evaluating the ex-
pected value of the predictive distribution. Importance samplers and Gibbs samplers are
developed for the posterior distributions arising from the normal-diﬀuse and ENC priors.
Kadiyala and Karlsson concludes that Gibbs sampling is more eﬃcient than importance
sampling, in particular for larger models. The evaluation is done in the context of two
forecasting exercises, one using a small bivariate model for the Swedish industrial produc-
tion index and unemployment rate and one using the seven variable model of Litterman
(1986). In terms of forecast performance there is no clear winner, the diﬀuse, normal-
Wishart priors and forecasts based on the OLS estimates does best with the Swedish data
and the Minnesota, normal-Wishart and normal-diﬀuse does best with the Litterman
model. In the following we will focus on the normal-Wishart and normal-diﬀuse priors as
the ENC prior is quite complicated to work with and did not perform signiﬁcantly better
than the other priors in terms of forecasting performance.
Departing from the normal-Wishart prior Giannone, Lenza and Primiceri (2012) sug-
gests a hierarchical prior structure that allows the choice of prior hyperparameters to be
inﬂuenced by the data and, in a sense, makes the procedure more ”objective”.
3.2.1
The normal-Wishart prior
The normal-Wishart prior is the natural conjugate prior for normal multivariate regres-
sions. It generalizes the original Litterman prior by treating the error variance-covariance
matrix, Ψ, as an unknown positive deﬁnite symmetric matrix rather than a ﬁxed diagonal
matrix. By allowing for correlation between the equations this also leads to computation-
ally convenient system estimation instead of the equation by equation approach used by
Litterman. This does, however, come with the disadvantage of imposing a Kronecker
structure on the variance-covariance matrix of γ.
Using the trick of adding and subtracting ZbΓ in the likelihood (7) and letting S =

Y −ZbΓ
′ 
Y −ZbΓ

be the error sum of squares we see that the likelihood
L (Y|Γ, Ψ) ∝|Ψ|−T/2 exp

−1
2 tr

Ψ−1S

× exp

−1
2 tr

Ψ−1 
Γ −bΓ
′
Z′Z

Γ −bΓ

has the form of a normal-Wishart distribution when considered as a function of Γ and Ψ.
Specifying the prior similarly,
Γ|Ψ ∼MNkm (Γ, Ψ, Ω,)
(16)
Ψ ∼iW (S, v) ,
14

we have the conjugate normal-Wishart prior with the corresponding posterior,
p (Γ, Ψ|YT) ∝|Ψ|−T/2 exp

−1
2 tr

Ψ−1 
Γ −bΓ
′
Z′Z

Γ −bΓ

exp

−1
2 tr

Ψ−1S

(17)
× |Ψ|−(v+m+k+1)/2 exp

−1
2 tr

Ψ−1 (Γ −Γ)′ Ω−1 (Γ −Γ)

exp

−1
2 tr

Ψ−1S

= |Ψ|−(T+v+m+k+1)/2 exp

−1
2 tr
h
Ψ−1  Γ −Γ
′ Ω
−1  Γ−Γ
i
exp

−1
2 tr

Ψ−1S

,
where the last line is obtained by completing the square for Γ. That is
Γ|YT, Ψ ∼MNkm
 Γ, Ψ, Ω

(18)
Ω
−1 = Ω−1 + Z′Z,
Γ = Ω

Ω−1Γ + Z′ZbΓ

= Ω
 Ω−1Γ + Z′Y

and
Ψ|YT ∼iW
 S, v

, v = T + v
(19)
S= S + S+

Γ −bΓ
′ 
Ω+ (Z′Z)−1−1 
Γ −bΓ

with bΓ = (Z′Z)−1 Z′Y and S =

Y −ZbΓ
′ 
Y −ZbΓ

.
For the conjugate normal-Wishart prior the marginal likelihood is available in closed
form. It can easily be derived by integrating out Γ and Ψ in (17) while keeping track
of all the constants that have been left out in the product of the likelihood and the
prior. Alternatively we rely on the properties of the matricvariate normal and inverse
Wishart distributions given in Appendix C. From the likelihood we have the conditional
distribution of Y as Y|Γ, Ψ ∼MNTm (ZΓ, Ψ, IT) , from the prior we deduce that ZΓ|Ψ ∼
MNTm (ZΓ, Ψ, ZΩZ′) and Y|Ψ ∼MNTm (ZΓ, Ψ, IT + ZΩZ′) . Finally, since the prior
for Ψ is inverse Wishart this leads to a matricvariate-t marginal distribution for Y,
Y ∼MtTm

ZΓ, (IT + ZΩZ′)−1 , S,v

.
(20)
Specifying the prior beliefs
Specifying the prior means in the fashion of the Min-
nesota prior is straightforward while the prior variances involve some diﬃculties. First,
recall that the marginal prior distribution of Γ is matricvariate t with variance-covariance
matrix V (γ) =
1
v−m−1S ⊗Ωand that γ has moments up to order v −m. The Kronecker
structure of the variance-covariance matrix makes it apparent that it is not possible to
specify the prior standard deviations or variances as in (14). The variance-covariance ma-
trix of one equation must be proportional to the variance-covariance matrix of the other
equations. With V (γj) =
sjj
v−m−1Ωwe can set the diagonal elements of Ωto
ωii =
 π2
1/ (lπ3sr)2 , lag l of variable r, i = (l −1) m + r
(π1π4)2 , i = mp + 1, . . . , k
(21)
15

and let sjj = (v −m −1) s2
j to achieve something which approximates the variances of
the Minnesota prior. That is, the prior parameter matrix for the inverse Wishart is
S = (v −m −1) diag
 s2
1, . . . s2
m

(22)
with prior expectation E (Ψ) = diag (s2
1, . . . s2
m) . We are implicitly setting π2 = 1 in (14)
and it is reasonable to use a smaller value of π1 here to balance between the Minnesota
type tight prior on lags of other variables and a looser prior on own lags.
It is, in general, advisable to set the prior variances for coeﬃcients on deterministic
variables to a large positive number as in (21) rather than the improper uniform prior in
the original Minnesota prior. Noting that Ωenters the prior as the inverse and that S can
be rewritten as a function of Ω−1 it is, however, possible to work with Ω−1 and specify
an improper prior by setting the corresponding diagonal elements of Ω−1 to zero.
The prior degrees of freedom of the inverse Wishart for Ψ might also require some
care, we must have v ≥m + 2 for the prior variance to exists and v ≥m + 2h −T for the
variance of the predictive distribution at lead time h to exist.
Simulating from the posterior distribution
With a normal-Wishart posterior we
can proceed as in Algorithm 1 using the posterior distributions (18) and (19).
3.2.2
The normal-diﬀuse and independent normal-Wishart priors
The normal-diﬀuse prior takes a simple form with prior independence between Γ and Ψ.
A normal prior for γ, γ ∼N
 γ, Σγ

and a Jeﬀreys’ prior for Ψ,
p (Ψ) ∝|Ψ|−(m+1)/2 .
(23)
This prior lacks the computationally convenient Kronecker structure of the variance-
covariance matrix of the normal-Wishart prior but it has the great advantage of not
placing any restrictions on the prior variance-covariance Σ. The joint posterior distribu-
tion has the form
p (Γ, Ψ|YT) ∝|Ψ|−(T+m+1)/2 exp

−1
2 (γ −bγ)′  Ψ−1 ⊗Z′Z

(γ −bγ)

× exp

−1
2
 γ −γ
′ Σ−1
γ
 γ −γ

.
This prior was ﬁrst considered by Zellner (1971) in the context of seemingly unrelated
regression models.
He showed that the marginal posterior for γ can be expressed as
the product of the normal prior and the marginal matricvariate t-distribution (9). The
marginal posterior is bimodal if there is a suﬃciently large diﬀerence between the center
of the prior information and the center of the data information. This can be troublesome
for MCMC schemes which might get stuck at one of the modes.
The full conditional posteriors are easy to derive. Completing the square for γ we
have
γ|YT, Ψ ∼N
 γ, Σγ

(24)
Σγ =
 Σ−1
γ
+ Ψ−1 ⊗Z′Z
−1 ,
γ = Σγ

Σ−1
γ γ +
 Ψ−1 ⊗Z′Z
 bγ

= Σγ

Σ−1
γ γ + vec
 Z′YΨ−1
16

where we have used that bγ = [Ψ−1 ⊗Z′Z]−1 (Ψ−1 ⊗Z′) y and (Ψ−1 ⊗Z′) y = vec
 Z′YΨ−1
for the last line. Note that this involves the inversion of the mk×mk matrix Σ−1
γ +Ψ−1⊗
Z′Z which can be computationally demanding and numerically unstable for large models.6
The conditional posterior for Ψ follows directly from the likelihood (7),
Ψ|YT, Γ ∼iW
 S, v

, v = T
(25)
S= (Y −ZΓ)′ (Y −ZΓ) .
The normal-diﬀuse prior is not a proper prior, which might be an issue in some cases,
even if we are assured that the posterior is proper as long as T > k. A simple modiﬁcation
is to replace the improper Jeﬀreys’ prior for Ψ with an inverse Wishart, Ψ ∼iW (S, v) .
The use of the independent normal-Wishart prior leaves the conditional posterior for
Γ unaﬀected and the conditional posterior for Ψ is still inverse Wishart but now with
parameters
S = S+ (Y −ZΓ)′ (Y −ZΓ) , v = T + v.
(26)
Specifying the prior beliefs
With the N
 γ, Σγ

form of the prior for γ it is straight-
forward to implement a basic Minnesota prior that is informative about all regression
parameters. Improper priors for the coeﬃcients on deterministic variables can be im-
plemented by working with the prior precision and setting the corresponding diagonal
elements of Σ−1
γ
to zero. Similarly, in order to implement the sum of coeﬃcients prior
or the initial observations prior it is most convenient to form the dummy observations
Rjγj = rj and add
1
s2
j R′
jRj to the corresponding diagonal block of Σ−1
γ
and
1
s2
j R′r to γj.
Simulating from the posterior distribution
With the full conditional posteriors in
hand a straightforward Gibbs sampling scheme is available for sampling from the posterior
and predictive distributions, see Algorithm 2. The experience of Kadiyala and Karlsson
(1997) is that the Gibbs sampler convergences quickly to the posterior distribution and a
few hundred draws may be suﬃcient as burn-in when the posterior is unimodal.
3.2.3
A hierarchical prior for the hyperparameters
The prior hyperparameters are in general chosen in three diﬀerent ways, as default values
similar to the ones used by Litterman, to minimize the forecast errors over a training
sample or in an empirical Bayes fashion by maximizing the marginal likelihood with
respect to the hyperparameters. As an alternative Giannone et al. (2012) suggests a more
ﬂexible approach where one more layer is added to the prior structure by placing a prior
on the hyperparameters in a hierarchical fashion. Collecting the hyperparameters in the
vector δ and working with the normal-Wishart family of prior distributions the prior
structure becomes
π (Γ|Ψ,δ) π (Ψ|δ) π (δ) .
6This is exactly the computational advantage of the Normal-Wishart prior. By retaining the Kronecker
structure of the variance-covariance matrix, Ψ−1 ⊗S, in the conditional posterior for γ only inversion
of m × m and k × k matrices is needed and it is only Ψ−1 (or it’s Cholesky factor) that needs to be
recomputed for each draw from the posterior.
17

Algorithm 2 Gibbs sampler for normal-diﬀuse and independent normal-Wishart priors
Select a starting value, γ(0) for γ. For j = 1, . . . , B + R
1. Generate Ψ(j) from the full conditional posterior (25) with S evaluated at γ(j−1)
where the posterior parameters are given by (25) for the normal-diﬀuse prior and
(26) for the independent normal-Wishart prior.
2. Generate γ(j) from the full conditional posterior (24) with Σγ evaluated at Ψ(j).
3. For j > B, generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
The ﬁrst B draws are discarded as burn-in.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample of independent draws from the joint predictive
distribution.
Conditioning on δ the analysis is as before and the results in section 3.2.1 holds
when interpreted conditional on δ. In addition to putting a prior on the hyperparameters
Giannone et al. (2012) relaxes some of the simplifying choices that are commonly made
when setting up the prior. Instead of setting the diagonal elements of S in the prior for
Ψ based on the residual standard variance from OLS estimated VAR or univariate AR
models Giannone et al. proposes treating them as parameters. That is, they set S =
diag (κ1, . . . , κm) and endow κi with independent inverse Gamma priors, κi ∼iG (aκ, bκ).
The conditional prior for Ψ is thus Ψ|δ ∼iW (S, v) . The prior variance speciﬁcation for
Γ can then be completed by setting
ωii =

(π1 (v −m −1) / (lκr))2 , lag l of variable r, i = (l −1) m + r
π2
4 (v −m −1) , i = mp + 1, . . . , k
with Ω= diag (ω) yielding the prior variances
V (γij) =
 (π2
1κj) / (l2κr) , lag l of variable r, i = (l −1) m + r
π2
4κj, i = mp + 1, . . . , k
.
The prior means of Γ is, Γ, set to one for the ﬁrst own lag and zero otherwise, the prior
for Γ is thus Γ|Ψ,δ ∼MNkm (Γ, Ψ,Ω)
In addition to this Giannone et al. adds dummy observations for a sum of coeﬃcients
prior and a dummy initial observation prior. Let eY and eZ be the dummy observations
speciﬁed similar to section 3.1.1. Giannone et al. sets
eY =

1
π6 diag (y0)
1
π7y′
0

, eZ =

j′
m ⊗

1
π6 diag (y0)

0m×(k−mp)
j′
m ⊗

1
π7y′
0

1
π7j′
(k−mp)

.
18

The dummy observations are then appended to the data matrices Y and Z and the
posterior parameters calculated as usual.7
Giannone et al. uses independent Gamma
priors for the scale factors and we set πi ∼G (ai, bi) , i = 1, 4, 6, 7. The collection of
hyperparameters is thus δ = (κ1, . . . , κm, π1, π4, π6, π7)′ .
Specifying the prior beliefs
The priors for the hyperparameters π1, π4, π6, π7 can be
centered on ”standard” settings for these parameters with variance depending on how
conﬁdent we are about the ”standard” values. Giannone et al. (2012) sets the modes for
π1, π6 and π7 to 0.2, 1 and 1 with standard deviations 0.4, 1 and 1. For π4 a large mode,
say 50, with a large standard deviation seems reasonable. For the diagonal elements of S,
κi, Giannone et al. implements the prior in terms of κi/ (v −m −1) , i.e. the prior mean
of S, and use a highly non-informative prior with aκ = bκ = 0.022.
Simulating from the posterior distribution
The joint posterior of Γ, Ψ and δ is not
available in closed form but Giannone et al. (2012) devices a Metropolis-Hastings sampler
for the joint distribution, see Algorithm 3. The algorithm generates δ from the marginal
posterior with a Metropolis-Hastings update, after convergence of the δ sampler Ψ and
Γ can be drawn from their distributions conditional on δ. While Giannone et al. takes
advantage of the availability of the marginal likelihood conditional on δ to simplify the
acceptance probability in the Metropolis-Hastings step and achieve a marginal sampler
for δ this is not a requirement. The acceptance probability can also be written in terms
of the likelihood and the priors and a Metropolis within Gibbs sampler can be devised
when the conditional marginal likelihood is not available in closed form.
Forecasting performance
Giannone et al. (2012) conducts a forecasting experiment
where they forecast the US GDP, GDP deﬂator and federal funds rate. This done using
three diﬀerent BVARs implemented using the hierarchical prior with 3, 7 and 22 variables
with all variables in log-levels. In addition to the BVARs forecasts are also produced
with VARs estimated with OLS, a random walk with drift and a dynamic factor model
based on principal components from a data set with 149 macro variables. In terms of
mean square error the BVARs improve with the size of the model (in contrast to the OLS
estimated VARs) and the largest BVAR produces better one step ahead forecasts than
the factor model for the GDP deﬂator and the federal funds rate and better four step
ahead forecasts for the GDP deﬂator.
7The additional information in the dummy observations can of course also be incorporated through
the priors.
The implied prior parameters are Ω∗=

Ω−1 + eZ′eZ
−1
, Γ∗= Ω∗
Ω−1Γ + eZ′ eY

and
S∗= S+

Y∗−Z∗bΓ∗′ 
Y∗−Z∗bΓ∗
−

Y −ZbΓ
′ 
Y −ZbΓ

where Y∗and Z∗is the augmented data,
bΓ∗the OLS estimate on the augmented data and bΓ the OLS estimate on the original data. The eﬀect on
Ωand Γ is clear and intuitive whereas S is inﬂated in a data dependent and non-obvious way. The mixed
estimation technique underlying the device of adding prior information through dummy observations
works well when the error variance is assumed known but is less transparent when it is unknown.
19

Algorithm 3 MCMC sampler for a VAR with hierarchical prior
For the VAR model with the hierarchical prior outlined in section 3.2.3 select starting
values for the hyperparameters δ(0), Giannone et al. (2012) suggests using the posterior
mode of δ as starting values and setting the tuning constant c to achieve approximately
20% acceptance rate. Step 1 of the sampler samples from the marginal posterior for δ,
steps 2 and 3 draws from the posterior for Ψ and Γ conditional on δ.
For j = 1, . . . , B + R
1. Draw a proposal, δ∗, for the hyperparameters from the random walk proposal dis-
tribution, δ∗∼N
 δ(j−1), cH−1
where H is the Hessian of the negative of the
logposterior for δ. Set δ(j) = δ∗with probability α, otherwise set δ(j) = δ(j−1)
where
α = min

1,
m (Y|δ∗) π (δ∗)
m (Y|δ(j−1)) π (δ(j−1))

and m (Y|δ) is given by (20).
Redo 1 if j < B otherwise continue.
2. Draw Ψ(j) from the full conditional posterior Ψ|YT, δ(j) in (19)
3. Draw Γ(j) from the full conditional posterior Γ|YT, Ψ(j), δ(j) in (18).
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample of independent
draws from the joint predictive distribution.
3.3
The steady state VAR
Villani (2009) observed that it is often easier to formulate a prior opinion about the
steady state (unconditional mean) of a stationary VAR than about the dynamics. At
the same time this is one feature of the data that a Minnesota type prior is silent about
with the uninformative prior on the coeﬃcients on deterministic variables.8
This is,
however, not surprising as the unconditional expectation is a highly nonlinear function
of the parameters when the VAR is written as a linear regression model which makes it
diﬃcult to express any prior beliefs about the steady state. Let A (L) = I −A′
1L −. . . −
A′
pLp we can then write the stationary VAR (6) as
A (L) yt = C′xt + ut.
8The initial observations prior could be used to incorporate information about the steady state in the
prior formulation by replacing y0 with the expected steady state.
20

The unconditional expectation is the E (yt) = µt = A−1 (L) C′xt = Λxt.9 Given infor-
mation about likely values for µt it is straightforward to formulate an informative prior
for Λ but the implied prior for C is highly complicated. Instead Villani (2009) suggested
writing the model in mean deviation form,
A (L) (yt −Λxt) = ut.
(27)
This makes the model non-linear in parameters which complicates estimation but makes
it easy to formulate a prior for all the parameters.
Let Γ′
d =
 A′
1, . . . A′
p

represent the dynamics. Villani (2009) argued that there is no
obvious connection between the steady state and the parameters governing the dynamics
and suggested the prior
π (Γd, Λ, Ψ) = π (Γd) π (Λ) π (Ψ)
with π (Γd) and π (Λ) normal,
γd ∼N

γd, Σd

,
(28)
λ = vec (Λ) ∼N (λ, Σλ)
and a Jeﬀreys’ prior (23) for Ψ. Alternatively a proper inverse Wishart, Ψ ∼iW (S, v) ,for
Ψ can be used. π (Γd) can be based on the prior beliefs in the Minnesota prior, variances
as in (14) with prior means for the ﬁrst own lag, γjj less than 1 indicating stationarity
and existence of the steady state.
The joint posterior is, due to the nonlinearities, not a known distribution but Villani
derived the full conditional posteriors for Λ, Γd and Ψ which can serve as the basis for
a Gibbs sampler and MCMC based inference. To this end rewrite (27) as a nonlinear
regression
y′
t = x′
tΛ′+ [w′
t −q′
t (Ip ⊗Λ′)] Γd + u′
t
Y= XΛ′+ [W −Q (Ip ⊗Λ′)] Γd + U
with w′
t =
 y′
t−1, . . . , y′
t−p

, q′
t =
 x′
t−1, . . . , x′
t−p

. The full conditional posterior for
Ψ is easy to derive and analogous to the normal-diﬀuse prior, form U = Y −XΛ′ −
[W −Q (Ip ⊗Λ′)] Γd and S = U′U, the error sum of squares matrix conditional on Λ
and Γd. The conditional posterior for Ψ is then inverse Wishart
Ψ|YT, Γd, Λ ∼iW
 S, v

(29)
with S = S and v = T for Jeﬀreys’ prior and S = S + S and v = T + v for the inverse
Wishart prior.
For the full conditional posterior for Γd we can treat Λ as known and thus calculate
YΛ = Y −XΛ′ and WΛ = [W −Q (Ip ⊗Λ′)] . With these in hand we can write the
model as YΛ = WΛΓd + U, a standard multivariate regression conditional on Λ and
9For simplicity we assume that xt only consists of simple deterministic variables such as a constant,
time trend and seasonal dummies.
21

Ψ. This is analogous to the normal-diﬀuse prior (section 3.2.2) and the full conditional
posterior for γd is normal
γd|YT, Λ, Ψ ∼N
 γd, Σd

(30)
Σd =
 Σ−1
d
+ Ψ−1 ⊗W′
ΛWΛ
−1
γd = Σd

Σ−1
d γd + vec
 W′
ΛYΛΨ−1
.
The full conditional posterior for Λ is more complicated to derive and requires some
matrix manipulations. Let YΓ = Y −WΓd, B = (X, −Q) and Θ′= [Λ, Γ′
d (Ip ⊗Λ)] =

Λ, A′
1Λ, . . . , A′
pΛ

the regression can then be written as
YΓ = BΘ + U
vec (Y′
Γ) = vec (Θ′B′) + vec (U′)
= (B ⊗I) vec (Θ′) + vec (U′)
= (B ⊗I) F vec (Λ) + vec (U′)
a standard univariate regression with regression parameters λ for F′= [I, I ⊗A1, I ⊗Ap]
and vec(U′) ∼N (0, IT ⊗Ψ) . The usual Bayesian calculations yields a normal posterior
for λ conditional on Γd and Ψ,
λ|YT, Γd, Ψ ∼N
 λ, Σλ

(31)
Σλ =
 Σ−1
λ + F′  B′B ⊗Ψ−1
F
−1
λ = Σλ
h
Σ−1
λ λ + F′  B′B ⊗Ψ−1
Fbλ
i
= Σλ

Σ−1
λ λ + F′ vec
 Ψ−1Y′
ΓB

for bλ = [F′ (B′B ⊗Ψ−1) F]−1 F′ (B′ ⊗Ψ−1) vec (Y′
Γ) the GLS estimate.
Forecasting performance
Villani (2009) conducts a small forecasting exercise where
he compares the forecast performance of the steady-state prior to a standard BVAR
with the Litterman prior and a standard VAR estimated with maximum likelihood. The
focus is on modelling the Swedish economy and with Swedish GDP growth, inﬂation and
interest rate, the corresponding foreign (world) variables and the exchange rate in trade
weighted form included in the VAR models. The estimation period includes the Swedish
ﬁnancial crisis at the beginning of the 90-ties and the subsequent shift in monetary policy
to inﬂation targeting. To accommodate this xt includes a constant term and a dummy for
the pre-crisis period. The prior on the constant terms in the steady-state VAR are thus
centered on the perceived post-crisis steady state and the prior on the dummy variable
coeﬃcients reﬂects the higher pre-crisis inﬂation and interest rates and the belief that
the crisis had no eﬀect on long run GDP growth. For the dynamics, the prior on Γd,
Villani follows the Litterman prior with the addition of treating the foreign variables as
exogenous, i.e. applying more aggressive shrinkage towards zero in the prior, and sets the
prior mean of the ﬁrst own lag to 0.9. The forecast performance is evaluated over the
period 1999 to 2005. The steady-state VAR performs considerably better for the Swedish
variables, conﬁrming the intuition that it is useful to be informative about (changes in)
the steady state.
22

Adolfson, Andersson, Linde, Villani and Vredin (2007) evaluates the forecast perfor-
mance of two forecasting models in use at Sveriges Riksbank (the central bank of Sweden),
a steady-state BVAR with the same variables as Villani (2009) similar prior set up, and
the open economy DSGE model of Adolfson, Lasen, Lind and Villani (2008). The BVAR
provides better forecasts of Swedish inﬂation up to 5 quarters ahead while the DSGE
model has lower RMSE when forecasting 7 and 8 quarters ahead and both models im-
prove on the oﬃcial Riksbank forecast. The BVAR outperforms the DSGE model at
all lead times when forecasting the interest rate and the forecast performance for GDP
growth is almost identical but worse than the oﬃcial Riksbank forecast except for lead
times 6 through 8.
¨Osterholm (2008a) forecasts the Swedish inﬂation and interest rate using a bivariate
steady state BVAR and a univariate variant of the steady state BVAR, i.e. φ (L) (yt −α −θdt) ,
allowing for a shift at the time of the change in monetary policy regime. The forecasts are
compared to forecasts from standard BVAR and Bayesian AR models with the dummy
variable but without prior information about steady state. For inﬂation there is very little
diﬀerence between the models whereas the steady state models do signiﬁcantly better for
the interest rate.
Beechey and ¨Osterholm (2010) forecasts the inﬂation rate for ﬁve inﬂation targeting
countries, Australia, Canada, New Zealand, Sweden, the UK and the US using a univariate
variant of the steady state VAR as in ¨Osterholm (2008a). The prior for θ is informative
and centered on the target inﬂation rate with a diﬀuse prior for α and a Minnesota type
lag decay on the autoregressive parameters in φ (L) . As a comparison a standard AR
model with the dummy dt is also estimated using Bayesian and frequentist techniques,
thus allowing for a shift in average inﬂation level but without adding information about
the inﬂation target through a prior. The steady state AR improves on the forecasts of the
other two models by a large amount for Australia, New Zealand and Sweden, less so for
Canada and oﬀer no improvement for the UK. The US is a special case with no oﬃcially
announced inﬂation target, if a shift in the (unoﬃcial) target is assumed in 1993 there is
no improvement from the steady state model whereas there are substantial gains if the
target is assumed constant.
Wright (2010) propose to anchor the steady state at the long run expectation of the
variables as measured by survey responses.
Speciﬁcally at each time point the prior
mean of the steady state is set to the latest estimate from the Blue Chip survey. This
is a convenient way of bringing in expectational data and Wright refers to this as a
”democratic prior”. Using VARs with monthly data on 10 variables Wright forecasts the
US real GDP growth, GDP deﬂator, CPI inﬂation, industrial production growth three
month yields and the unemployment rate at horizons 0 - 13. The VAR variants include
one estimated by OLS, a normal-diﬀuse prior with Minnesota type prior beliefs and the
democratic steady state prior with three diﬀerent ways of specifying the prior mean on
the ﬁrst own lag, 0 for all variables, 0 for real variables and 0.85 for nominal variables and
estimated from the survey data. The BVARs improve on the OLS estimated VAR and
the democratic priors do better than the Minnesota prior with little diﬀerence between
the alternative speciﬁcation of the prior means. Wright also comparing the VAR forecasts
with additional forecast devices for a subset of the variables. When the comparison is
with survey estimates of short term expectations the diﬀerences are small with a few
cases where a BVAR improves signiﬁcantly on the survey estimates. Comparing the VAR
23

Algorithm 4 Gibbs sampler for the steady state prior
With the steady state prior 28 a Gibbs sampling algorithm follows immediately from the
full conditional posteriors. Select starting values, γ(0)
d
and λ(0). For j = 1, . . . , B + R
1. Generate Ψ(j) from the full conditional posterior in (29) with S evaluated at γ(j−1)
d
and λ(j−1). Note that S and v depends on the choice of prior for Ψ.
2. Generate γ(j)
d
from the full conditional posterior in (30) with γd and Σd evaluated
at Ψ(j) and λ(j−1).
3. Generate λ(j) from the full conditional posterior in (31) with λ and Σλ evaluated
at Ψ(j) and γ(j).
4. If j > B, generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h = x′
T+hΛ′(j) +
h−1
X
+i=1

ey(j)′
T+h−i −x′
T+h−1Λ′(j)
A(j)
i
+
p
X
i=h
 y′
T+h−i −x′
T+h−1Λ′(j)
A(j)
i +u(j)′
T+h.
The ﬁrst B draws are discarded as burn-in.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample of independent draws from the joint predictive
distribution.
forecasts with two time varying parameter models, an unobserved components stochastic
volatility (UCSV) model and the TVP-VAR with stochastic volatility of Primiceri (2005)
discussed in section 7, the VARs do better than the UCSV and the performance is similar
to the TVP-VAR with a slight edge for the TVP-VAR.
Simulating from the posterior distribution
With the full conditional posteriors in
hand a straightforward Gibbs sampling scheme is available for sampling from the posterior
and predictive distributions, see Algorithm 4. Villani reports that the Gibbs sampler
convergences quickly to the posterior distribution but also notes that there is a possible
issue of local nonidentiﬁcation of Λ when there are unit roots or explosive roots in the
autoregressive polynomial. This is only an issue for the convergence of the Gibbs sampler
if the prior for Λ is uninformative and the posterior for Γd has non-negligible probability
mass in the nonstationary region.
3.4
Model speciﬁcation and choice of prior
Carriero, Clark and Marcellino (2011) conducts an impressive study of the many speci-
ﬁcation choices needed when formulating a BVAR for forecasting purposes. Their main
application use monthly real time data on 18 US macroeconomic and ﬁnancial variables.
The baseline model is a BVAR with all 18 variables and 1 lag using the normal-Wishart
24

prior with ”standard” choices for the hyperparameters. Ωand S are speciﬁed as in (21)
and (22) with v = m + 2, π1 =
√
0.1, π3 = 1 and a diﬀuse prior on the constant term.
The prior mean of the ﬁrst own lag is set to 1 except when a variable is diﬀerenced in
which case it is set to zero. The forecasts are constructed using the recursion
ey′
T+h =
h−1
X
i=1
ey′
T+h−iAi +
p
X
i=h
y′
T+h−iAi + x′
T+hC
(32)
with the parameters set at the posterior means.
Choice of hyperparameters and lag length: Alternatives considered are setting
the hyperparameters by maximizing the marginal likelihood and using lag lengths 1-12.
Increasing the lag length improves forecast performance for most of the variables but not
for all. Choosing the lag length by maximizing the marginal likelihood leads to modest
improvements for a majority of the variables with small losses for the other variables
compared to the baseline. Choosing both hyperparameters and lag length by maximizing
the marginal likelihood oﬀers greater improvements than just maximizing with respect
to one of them. The gains are on the whole relatively small and Carriero, Clark and
Marcellino conclude that a lag length of 12 with π1 =
√
0.1 is a simple and eﬀective
choice.
Multi-step forecasting: The forecast function (32) is non-linear in the parameters
and using the posterior means of the parameters does not produce the means of the
predictive distribution when h > 1. Alternatives considered are 1) simulating from the
posterior distribution of the parameters and averaging over the forecasts and 2) using
directs forecasts based on estimating models that are speciﬁc to each horizon
y′
t+h =
p
X
i=1
y′
t−iAi + x′
tC + e′
t+h.
The gains from simulating the parameters is found to be negligible. Overall the diﬀerences
between the iterated and direct forecasts are small but there are large gains from the direct
forecast for some of the variables. This is presumably because the direct forecast is more
robust to misspeciﬁcation.
Cross-variable shrinkage and treatment of the error variance: The normal-
Wishart prior forces a symmetric treatment of the variables whereas the original Litterman
prior shrinks the parameters on ”other” variables harder towards zero. On the other hand
the normal-Wishart prior relaxes the assumption of a ﬁxed and diagonal error variance
matrix. Forecasting using the prior of Litterman as implemented in section 3.1, equation
by equation estimation and two choices of π2,
√
0.2 and
√
0.5 makes little diﬀerence except
for the federal funds rate where the improvement is dramatic for for the shorter forecast
horizons. The independent normal Wishart prior oﬀers both the possibility to impose
cross-variable shrinkage and an unrestricted error variance matrix.
When comparing
the forecast performance for the independent normal Wishart and Litterman priors the
diﬀerences are very small with a slight edge for the Litterman prior.
Size of model: When comparing the forecast performance of the 18 variable VAR to
a reduced model with 7 variables the larger model is found to forecast better. The gain
from using the larger model is smaller with direct forecasts than iterated forecasts, again
presumably due to the greater robustness against misspeciﬁcation.
25

Levels or diﬀerences: A speciﬁcation in levels can make use of any cointegration
between the variables which should improve forecasts, on the other hand a speciﬁcation in
diﬀerences oﬀers some robustness in the presence of structural breaks. The speciﬁcation
in diﬀerences improves on the levels speciﬁcation, the root mean square error is on average
11% larger with the levels speciﬁcation and the speciﬁcation in diﬀerences has the lowest
RMSE in 74% of the considered cases.
Carriero, Clark and Marcellino (2011) also conducts a robustness check using data from
Canada, France and the UK using a reduced set of variables. Overall the conclusions from
the US data is conﬁrmed when using data from these three countries.
Summarizing their ﬁndings Carriero, Clark and Marcellino notes that ”simple works”
and recommends transforming variables to stationarity, using a relatively long lag length
(12 with monthly data), the normal-Wishart prior and forecasts based on the posterior
means of the parameters.
4
Structural VARs
The reduced form VAR is designed to capture the time series properties of the data
and can, when coupled with suitable prior information, be an excellent forecasting device.
The reduced form nature makes it diﬃcult to incorporate economic insights into the prior.
Take, for example, the ”exogenous” variables prior in section 3.1.1. While it is tempting
to think about this as implying exogeneity it is actually a statement about Granger
causality. That, in a small open economy model, we do not expect the domestic variables
to be useful for forecasting the variables representing the rest of the world. Restrictions
on the variance-covariance matrix Ψ are needed in order to make claims about exogeneity.
This brings us to structural or identiﬁed VAR-models that, by allowing limited structural
interpretations of the parameters in the model, makes it possible to incorporate more
economic insights in the model formulation. If done well this has the potential to improve
the forecast performance of the model.
The basic structural VAR has the form
y′
tΛ =
p
X
i=1
y′
t−iBi + x′
tD + e′
t
(33)
y′
tΛ = z′
tΘ + e′
t
where Λ is full rank, Θ′ =
 B′
1, . . . , B′
p, D′
and et has a diagonal variance-covariance ma-
trix. The relation with the reduced form (6) is straightforward, Γ = ΘΛ−1, Ai = BiΛ−1,
C = DΛ−1, ut = Λ−Tet and Ψ = Λ−TV (et) Λ−1.10 The structural VAR (33) imposes
restrictions on the form of the reduced form variance-covariance matrix Ψ but leaves the
reduced form regression parameters Γ unrestricted since Λ is a full rank matrix unless
there are additional restrictions on Θ. For simplicity we take V (et) = I, with m (m + 1) /2
free parameters in the symmetric matrix Ψ this implies a simple order condition that
m (m −1) /2 restrictions on Λ are needed for identiﬁcation.11 The simplest such scheme
10The SVAR can also be written as y′
t = Pp
i=1 y′
t−iAi + x′
tC + e′
tL with L = Λ−T where the structure
of L indicates which of the ”identiﬁed” innovations eti has an immediate impact on ytj.
11This is only a neccessary and not a suﬃcient condition for identiﬁcation. Identiﬁcation is discussed
in more detail in section 4.3.
26

is to let L =Λ−T be the (lower) triangular Cholesky decomposition of Ψ = LL′. Subject
to a normalization that the diagonal elements of L and Λ are positive this is a one-to-one
mapping between Λ and Ψ and yields exact identiﬁcation without, in fact, imposing any
restrictions on the reduced form. In the following we will frequently work with λ = vec (Λ)
and λj, column j of Λ, it is then important to keep in mind that these are subject to
restrictions and that not all elements can vary freely.
The normalization is needed because the reduced form coeﬃcients are left unchanged
by reversing the sign of column j of Λ and Θ. The choice of normalization is, in general,
not innocuous. Waggoner and Zha (2003b) demonstrate how an unfortunate choice of
normalization can lead to misleading inference about Λ and impulse responses and give a
rule for ﬁnding a good normalization. As our focus is on forecasting where the predictive
distribution depends on the reduced form parameters we will largely ignore these issues.
4.1
”Unrestricted” triangular structural form
The structural form likelihood has the form
L (Y|Θ, Λ) ∝|det Λ|T exp

−1
2 tr

(YΛ −ZΘ) (YΛ −ZΘ)′
= |det Λ|T exp

−1
2 [vec (YΛ) −(Im ⊗Z) θ]′ [vec (YΛ) −(Im ⊗Z) θ]

= |det Λ|T exp

−1
2
h
vec (YΛ) −(Im ⊗Z) bθ
i′ h
vec (YΛ) −(Im ⊗Z) bθ
i
× exp

−1
2

θ −bθ
′
(Im ⊗Z′Z)

θ −bθ

of a normal distribution for θ = vec (Θ) conditional on Λ with bθ = vec

bΘ

= vec

(Z′Z)−1 Z′YΛ

.
Sims and Zha (1998) suggested matching this by specifying a normal prior for θ condi-
tional on Λ, θ|Λ ∼N (vec (MΛ) ,Σθ) with M′ = (Im, 0) together with a marginal prior,
π (Λ) for Λ. The choice of M implies a prior mean for the reduced form parameters Γ
that coincides with the univariate random walk of the Minnesota prior. The conditional
posterior for θ is then normal,
θ|YT, Λ ∼N
 θ, Σθ

Σθ =
 Σ−1
θ
+ Im ⊗Z′Z
−1
θ = Σθ
 Σ−1
θ vec (MΛ) + vec (Z′YΛ)

.
Similar to the normal-diﬀuse prior this involves the inversion of the mk × mk matrix Σθ
which can be computationally demanding. As noted by Sims and Zha (1998) this can be
simpliﬁed considerably if Σθ is block diagonal with diagonal blocks Σθ,j corresponding
to the equations.
That is, there is independence between the priors for the diﬀerent
equations conditional on Λ,
θj|Λ ∼N
 Mλj, Σθ,j

.
(34)
27

The inversion of a mk × mk matrix is then replaced by m inversions of k × k matrices as
we solve for the posterior parameters equation by equation,
θj|YT, Λ ∼N
 θj, Σθ,j

Σθ,j =
 Σθ,j + Z′Z
−1
θj = Σθ,j
 Σ−1
θ,jMλj + Z′Yλj

= Mjλj,
and this brings us close to the computational convenience of the normal-Wishart prior.
A further simpliﬁcation is available if the prior variance is the same for all equations,
Σθ,j = Σ
e
θ and Σθ = Im ⊗Σ
e
θ. The conditional posteriors for θj then only diﬀers in the
conditional mean with
θj|YT, Λ ∼N

f
Mλj, eΣθ

(35)
eΣθ =

Σ
e
−1
θ
+ Z′Z
−1
f
M = eΣθ

Σ
e
−1
θ M + Z′Y

which puts the computational requirements on par with the normal-Wishart prior for a
reduced form VAR.
The posterior for Λ is more complicated. Integrating out Θ from the joint posterior
and keeping track of the extra terms from completing the square for θ yields the marginal
posterior
π (Λ|YT) ∝π (Λ) |det Λ|T
×exp

−1
2λ′

Im ⊗Y′Y + (Im ⊗M)′ Σ−1
θ (Im ⊗M)
−

Σ−1
θ (Im ⊗M) + Im ⊗Z′Y
′ Σ
−1
θ

Σ−1
θ (Im ⊗M) + Im ⊗Z′Y


λ

.
This is not a known distribution except in special cases. One such case arises under the
prior θj|Λ ∼N
 Mλj, Σ
e
θ

on θ discussed above. The Kronecker structure of the prior
variance-covariance matrix Σθ is inherited by the posterior variance-covariance and there
is also a Kronecker structure in the posterior mean. The exponent in the posterior for Λ
simpliﬁes
π (Λ|YT) ∝π (Λ) |det Λ|T exp

−1
2λ′ 
Im ⊗
h
Y′Y + M′Σ
e
θM −f
M′ eΣ−1
θ f
M
i
λ

= π (Λ) |det Λ|T exp

−1
2 tr
h
Y′Y + M′Σ
e
θM −f
M′ eΣ−1
θ f
M
i
ΛΛ′
.
Ignoring the prior, this is similar to a Wishart distribution for Ψ−1 =ΛΛ′. It is,
however, only a Wishart if the structure of Λ imposes no restrictions on Ψ, e.g. if Λ
is upper triangular with no other restrictions except the normalization. In this, special
case, it is reasonable to specify an uninformative prior for Λ, π (Λ) ∝1 and the implied
posterior for Ψ−1 is Wishart,
Ψ−1|YT ∼Wm
 S−1,T + m + 1

(36)
S = Y′Y + M′Σ
e
θM −f
M′ eΣ−1
θ f
M.
28

A draw from the posterior of Λ can then be obtained by generating Ψ−1 from the Wishart
distribution and solving for Λ. In fact, if Λ is triangular it can be generated directly
as the Bartlett decomposition of a Wishart distributed matrix. Sims and Zha (1998)
and Zha (1999) suggests π (Λ) ∝|det Λ|k as an uninformative improper prior. This is,
however, in a slightly diﬀerent context and working with the prior and posterior for (Λ, Γ)
rather than (Λ, Θ) and the factor |det Λ|k corresponds to the Jacobian when transforming
from the (Λ, Θ) parameterization to the (Λ, Γ) parameterization. Inference in the two
parameterizations is thus equivalent with these two priors on Λ provided that the priors
on Γ and Θ are equivalent.
Specifying the prior
The triangular SVAR is just a reparameterization of the reduced
form VAR of section 3 and it is tempting to base the prior speciﬁcation on the Minnesota
prior. It should, however, be clear that it is not possible to mimic the Minnesota prior
completely without losing the computational convenience as the transformation Θ = ΓΛ
implies prior dependence between the columns of Θ. Sims and Zha (1998) proposed setting
the prior standard deviations to
sd (θij) =
 (π1π2) / (lπ3sr) , lag l of variable r, i = (l −1) m + r
π1π4, deterministic variables, i = mp + 1, . . . , k
.
(37)
This is close to the Minnesota prior but diﬀers in two aspects, there is no distinction
between own lags and ”other” variables since the choice of dependent variable in a si-
multaneous equation system is arbitrary and the scale factor sj drops out since the error
variances are normalized to 1. This leads to a common prior variance, Σ
e
θ, in (34) and
the simpliﬁed posterior (35) in the spirit of the Minnesota prior.
The symmetric treatment of the structural form equations does, however, not imply
a symmetric treatment of the reduced form equations. With Λ upper triangular we have
γj = Pj
i=1 θiλij for λij element i, j of Λ−1 and the ordering of the equations clearly matters
for the implied prior on the reduced form. The unconditional prior expectation of Γ is
M and the random walk type prior with mii = 1 can easily be modiﬁed to accommodate
variables that are believed to be stationary by setting mii less than 1.
With Λ triangular a truly structural interpretation of the parameters is diﬃcult and
an uninformative prior, π (Λ) ∝1 seems appropriate.
Sampling from the posterior distribution
With Λ triangular, the prior π (Λ) ∝1,
θj|Λ ∼N
 Mλj, Σ
e
θ

simulating from the posterior and predictive distributions using
algorithm 5 is straightforward.
4.2
Homogenous restrictions on the structural form parameters
When the structure of Λ implies restrictions on Ψ the posterior becomes quite complicated
irrespective of the choice of prior for Λ. Sims and Zha (1998) proposes to use importance
sampling, generating Λ from an approximation to the marginal posterior and Θ or Γ
conditional on Λ and Zha (1999) devices a scheme where blocks of equations can be
treated independently and importance sampling can be used for each block. The block
scheme should be more eﬃcient as one high dimensional problem is replaced by several
29

Algorithm 5 Simulating the posterior and predictive distributions for a triangular SVAR
For the SVAR with Λ triangular and the prior (34) with Σθ,j = Σ
e
θ for Θ and an uninfor-
mative prior for Λ, π (Λ) ∝1 draws from the posterior and predictive distributions can
be obtained as follows
For j = 1, . . . R
1. Generate Λ(j) directly as the Bartlett decomposition of a draw form the marginal
posterior (36).
2. For i = 1, . . . , m generate θ(j)
i
from the conditional posterior (35).
3. Calculate the reduced form parameters Γ(j) = Θ(j)Λ(j).
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
with Ψ(j) =
 Λ(j)Λ(j)′−1 and
calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
problems of smaller dimension. Nevertheless importance sampling has proven to be quite
ineﬃcient as it is diﬃcult to ﬁnd a good approximation to the marginal posterior of Λ.
Waggoner and Zha (2003a) develops a Gibbs sampler for the marginal posterior of Λ
in a setting allowing for exact restrictions and informative priors on both Λ and Θ. They
consider homogenous restrictions on the parameters of one equation (column in Λ and
Θ) of the form
Qjλj = 0
(38)
Rjθj = 0
for Qj a (m −qj)×m matrix of rank m−qj and Rj a (k −rj)×k matrix of rank k−rj, i.e.
there are m −qj restrictions on λj resulting in qj free parameters and m −rj restrictions
on θj resulting in rj free parameters, together with the normal prior suggested by Sims
and Zha (1998) for the unrestricted parameters,
λj ∼N
 0, Σλ,j

(39)
θj|λj ∼N
 Mjλj, Σθ,j

.
To form a prior incorporating the restrictions Waggoner and Zha conditioned on the
restrictions (38) in the prior (39). To this end let Uj and Vj be m × qj and k × rj
orthonormal matrices satisfying QjUj= 0 and RjVj= 012 if the restrictions hold there
12Uj and Vj form basis for the null spaces of Qj and Rj and can be obtained from the QR de-
compositions of Q′
j and R′
j as follows. For A m × n (m > n) of rank n we have A = QR with Q a
30

must then be vectors dj and tj that satisfy λj = Ujdj and θj = Vjtj. dj and tj represents
the free parameters and it is more convenient to work directly with them. The implied
prior for dj and tj is obtained by Waggoner and Zha as
dj ∼N
 0, Σd,j

, tj|dj ∼N
 M
f
jdj, Σt,j

(40)
with
Σt,j =
 V′
jΣ−1
θ,jVj
−1
M
f
j = Σt,jV′
jΣ−1
θ,jMjUj
Σd,j =

U′
jΣ−1
λ,jUj + U′
jM′
jΣ−1
θ,jMjUj −M
f
′
jΣ−1
t,j M
f
j
−1
.
In the case that there are no restrictions on θj we can take Vj = Ik and the expressions
simplify to Σt,j = Σθ,j, M
f
j = MjUj and Σd,j =
 U′
jΣ−1
λ,jUj
−1 .
Let H = (U1d1, . . . , Umdm), the likelihood for dj and tj, j = 1, . . . , m is then
L (Y|d, t) ∝|det H|T exp
(
−1
2
m
X
j=1
(YUjdj−ZVjtj)′ (YUjdj−ZVjtj)
)
= |det H|T exp

−1
2
X
d′
j

YUj−ZVjc
Mj
′ 
YUj−ZVjc
Mj

dj

× exp

−1
2
X 
tj −c
Mjdj
′
V′
jZ′ZVj

tj −c
Mjdj

for c
Mj =
 V′
jZ′ZVj
−1 V′
jZ′YUj. Multiplying with the prior (40), completing the square
for tj and collecting terms yields the joint posterior
p (d, t|YT) ∝|det H|T exp

−1
2
X
d′
j
h
U′
jY′YUj+M
f
′
jΣ−1
t,j M
f
j −M
′
jΣ
−1
t,j Mj + Σ−1
d,j
i
dj

(41)
× exp

−1
2
X  tj −Mjdj
′ Σ
−1
t,j
 tj −Mjdj

with Σt,j =
 Σ−1
t,j + V′
jZ′ZVj
−1 and f
Mj = Σt,j
 Σ−1
t,j M
f
j + V′
jZ′YUj

. The conditional
posterior for tj is thus normal,
tj|YT, dj ∼N

f
Mjdj, Σt,j

,
and the conditional posteriors for tj are independent conditional on d1, . . . , dm. The
marginal posterior for d1, . . . , dm is given by the ﬁrst line of (41) where we must take
account of H being a function of d1, . . . , dm. Clearly this is not a known distribution even
though it in part looks like a normal distribution with mean zero for dj.
m × m orthonormal matrix and R =
 R′
1, 0n×(m−n)
′ m × n with R1 upper triangular. We then have
A′Q = R′Q′Q = R′. Partioning Q = (Q1, Q2) with Q2 m × (m −n) we see that A′Q2 = 0 and Q2 is
a basis for the null space of A′. We can thus take Uj as the last qj columns of the Q matrix of the QR
decomposition of Q′
j and Vj as the last rj columns of the Q matrix of the QR decomposition of Rj.
31

Waggoner and Zha (2003a) develops a Gibbs sampling algorithm for the marginal
posterior of d1, . . . , dm that operates on the posterior distributions for dj conditional on
di, i ̸= j, the set of full conditional posteriors. To this end let
S−1
j
= 1
T
h
U′
jY′YUj+M
f
′
jΣ−1
t,j M
f
j −M
′
jΣ
−1
t,j Mj + Σ−1
d,j
i
,
TjT′
j = Sj and write dj = Tj
Pqj
i=1 βiwi = TjWβ where W is a qj × qj orthonormal
matrix with columns wi. The Jacobian for the change of variables from dj to β1, . . . , βqj
is unity and the trick is to choose W in a clever way where W does not depend on
dj. Let w be a m × 1 vector that is orthogonal to each of Uidi, i ̸= j,13 set w1 =
T′
jU′
jw/
q
w′UTT′
jU′
jw and w′
i = (w11wi1, . . . , wi−1,1wi1, −ci−1, 0, . . . , 0) /√ci−1ci for i =
2, . . . , qj where wi1 is element i of w1 and ci = Pi
k=1 w2
k1. By construction UjTjw1 is
linearly independent of Uidi, i ̸= j, while UjTjwi, i > 1, are contained in the column
space. Consequently
det H = det
 
U1d1, . . . , Uj−1dj−1, UjTj
qj
X
i=1
βiwi, Uj+1dj+1, . . . , Umdm
!
=
qj
X
i=1
βi det (U1d1, . . . , Uj−1dj−1, UjTjwi, Uj+1dj+1, . . . , Umdm)
= β1 det (U1d1, . . . , Uj−1dj−1, UjTjw1, Uj+1dj+1, . . . , Umdm) ∝β1,
Pm
i=1 d′
iS−1
i di = Pqj
k=1 β2
k + P
i̸=j d′
iS−1
i di and the conditional posterior simpliﬁes to
p (β1, . . . , βq|YT, di̸=j) ∝|β1|T exp
"
−T
2
qj
X
k=1
β2
k
#
= |β1|T exp

−Tβ2
1
2

exp
"
−T
2
qj
X
k=2
β2
k
#
the product of a Gamma distribution for r = β2
1 and qj −1 independent N (0, 1/T)
distributions.14
Specifying the prior
Waggoner and Zha (2003a) starts by specifying a prior (39) for
the unrestricted structural form parameters, λj and θj, and conditions on the restrictions
(38) in order to derive the prior (40) for the free parameters dj and tj in each equation.
As a default, the conditional prior for θj can be speciﬁed as in the unrestricted SVAR,
e.g. prior variances in accordance with (37) and a choice of M indicating if variables
are believed to be non-stationary or not. Unlike the unrestricted SVAR there are no
13w can be obtained by solving the equation system w′Uidi = 0, i ̸= j. A practical method is to
form the m × (m −1) matrix A = (U1d1, . . . , Uj−1dj−1, Uj+1dj+1, . . . , Umdm) and calculate the QR
decomposition A = QR and set w = qm, the last column of Q. Since the last row of R is zero and Q is
orthonormal we have w′A = w′QR = 0.
14The distribution of r
=
β2
1 is f (r)
∝
rT/2 exp
 −T r
2
  ∂β1
∂r

=
r(T +1)/2−1 exp
 −T r
2

/2 a
Gamma ((T + 1) /2, T/2) distribution.
32

computational gains from treating the equations symmetrically and the hard restrictions
in (38) can easily be combined with ”soft” restrictions on speciﬁc parameters.
It might be diﬃcult to formulate economically meaningful priors on λj with the prior
means ﬁxed at zero as in (39) but one can, at least, be informative about the relative
magnitude of coeﬃcients by working with the prior variances. Imposing the restrictions
(38) can have unexpected consequences on the prior if there is prior correlation between
coeﬃcients and the implied prior for λ and θ should be checked in this case.
Sampling form the posterior
The sampler developed by Waggoner and Zha (2003a)
is straightforward to implement and outlined in algorithm 6.
4.3
Identiﬁcation under general restrictions
Following Rothenberg (1971) we say that a parameter point (Λ, Θ) is identiﬁed if there
is no other parameter point that is observationally equivalent, i.e. that they imply the
same likelihood and hence the same reduced form parameters. Since the reduced form
parameters are given by Γ = ΘΛ−1, Ψ = (ΛΛ′)−1 it is clear that (Λ, Θ) and

eΛ, eΘ

are
observationally equivalent if and only if there exists an orthonormal matrix P such that
eΛ =ΛP and eΘ = ΘP. A SVAR is thus (globally) identiﬁed at (Λ, Θ) , subject to a set of
restrictions, if the only orthonormal matrix for which both (Λ, Θ) and (ΛP, ΘP) satisﬁes
the restrictions is the identity matrix.
Rubio-Ramirez, Waggoner and Zha (2010) considers general restrictions on the struc-
tural form parameters and obtains necessary and suﬃcient conditions for identiﬁcation of
SVARs. Let f (Λ, Θ) be a n×m matrix valued function of the structural form parameters
and Rj a rj × n matrix of linear restrictions on column j of f (Λ, Θ) , i.e.
Rjf (Λ, Θ) ej = 0
(42)
for ej column j of the identity matrix Im where the structural form parameters are subject
to a normalization rule as in Waggoner and Zha (2003b).
The order of the columns
(equations) in f () is arbitrary, as a convention the columns are ordered so that r1 ≥r2 ≥
. . . ≥rm. Some regularity conditions on f () are needed in order to state the identiﬁcation
results:
• Admissible: the restrictions are said to be admissible if f (ΛP, ΘP) = f (Λ, Θ) P
for P any orthonormal matrix.
• Regular: the restrictions are said to be regular if the domain U of f () is an open
set and f is continuously diﬀerentiable with f ′ of rank nm for all (Λ, Θ) ∈U.
• Strongly regular: the restrictions are said to be strongly regular if f is regular and
f (U) is dense in the set of n × m matrices.
Examples of admissible and strongly regular functions include the identity function
f (Λ, Θ) = (Λ′, Θ′)′ for linear restrictions on the parameters, the short run impulse
responses f (Λ, Θ) = Λ−1, long run impulse responses f (Λ, Θ) = (Λ′ −Pp
i=1 B′
i)−1 as
well as intermediate impulse responses and combinations of these.
33

Algorithm 6 Gibbs sampler for restricted Structural form VARs
The sampler is based on the Gibbs sampler for d1, . . . , dm. Once convergence is achieved
draws of t1, . . . , tm can be obtained from the conditional posterior and the structural form
parameters Λ and Θ calculated. Start by precomputing the matrices, Uk, Vk, Sk, and
Tk for k = 1, . . . , m and select starting values d(0)
2 , . . . , d(0)
m .
For j = 1, . . . , B + R
1. For k = 1, . . . , m
(a) Construct
a
vector
w
that
is
orthogonal
to
U1d(j)
1 , . . . , Uk−1d(j)
k−1, Uk+1d(j−1)
k+1 , . . . , Umd(j−1)
m
and
calculate
the
vectors
w1, . . . , wqk.
(b) Generate r from a G ((T + 1) /2, T/2) and u from a uniform (0, 1) distribution.
Let β1 = −√r if u ≤0.5 and set β1 = √r otherwise.
(c) Generate β2, . . . , βqk as independent N (0, 1/T) random numbers
(d) Calculate d(j)
k
= Tk
Pqk
i=1 βiwi
2. If j > B,
(a) For k = 1, . . . , m generate t(j)
k
from the conditional posterior tk|YT, d(j)
k
∼
N

f
Mkd(j)
k , Σt,k

(b) Calculate the structural form parameters λ(j)
k
= Ukd(j)
k
and θ(j)
k
= Vkt(j)
k
for
k = 1, . . . , m and form the matrices Λ(j) and Θ(j).
A normalization as in Waggoner and Zha (2003b) should be applied if the
purpose is inference on the structural form parameters or impulse responses.
(c) Calculate the reduced form parameters Γ(j) = Θ(j)Λ(j).
(d) Generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
with Ψ(j) =
 Λ(j)Λ(j)′−1 and
calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B as a sample of inde-
pendent draws from the joint predictive distribution.
34

Theorems 1 and 3 of Rubio-Ramirez et al. (2010) establishes that an SVAR with
admissible and regular restrictions (42) is globally identiﬁed almost everywhere15 if and
only if the matrices Mj (f (Λ, Θ)) , j = 1, . . . , m, has rank m for some (Λ, Θ) that satisﬁes
the restrictions. The (rj + j) × m matrix Mj (f (Λ, Θ)) is given by
Mj (f (Λ, Θ)) =
 Rjf (Λ, Θ)
Ij
0j×(m−j)

with Mj (f (Λ, Θ)) = (Ij, 0) if there are no restrictions on column j of f () .
Rubio-Ramirez et al. (2010, theorem 7) also develops a simple necessary and suﬃcient
condition for exact identiﬁcation.16 A SVAR with admissible and strongly regular restric-
tions (42) is exactly identiﬁed if and only if rj = m −j, j = 1, . . . , m. The restrictions
must thus follow a pattern, a simple special case is when Λ is a triangular matrix with
no other restrictions on the structural form parameters as in section 4.1.
To illustrate consider the following structure for the contemporaneous parameters in
Λ (see Rubio-Ramirez et al. (2010, section 5.2) for motivation and additional details, note
that our deﬁnition of Rj and consequently Mj diﬀers in that we leave out redundant rows
of these matrices)
Λ =






λ11
λ12
0
λ14
λ15
0
λ22
0
λ24
λ25
0
0
λ33
λ34
λ35
0
0
λ43
λ44
λ45
0
0
0
0
λ55






.
With no other restrictions, f () is just f (Λ, Θ) = Λ and the corresponding restriction
matrices are
R1 = (04×1, I4) , R2 = (03×2, I3) , R3 =


e′
1
e′
2
e′
5

, R4 = e′
5.
We can immediately see that the SVAR would be exactly identiﬁed if there was one less
zero restriction on the third column of Λ, or – after reordering the equations – one less
restriction on the second equation. As is, we need to verify that there exists a parameter
point that satisﬁes the restrictions and for which all the Mj matrices has full rank in
order to establish global identiﬁcation. Multiplying f (Λ, Θ) with Rj and ﬁlling out the
15”Globally identiﬁed almost everywhere” implies that we can check the rank condition at an arbitrary
parameter point satisfying the restrictions.
16The SVAR is exactly identiﬁed if for all, except for a set of measure zero, reduced form parameters
(Γ, Ψ) there is a unique structural parameter point (Λ, Θ) satisfying Γ = ΘΛ−1, Ψ =
 ΛΛ′−1 .
35

bottom rows we have
M1 =






0
λ22
0
λ24
λ25
0
0
λ33
λ34
λ35
0
0
λ43
λ44
λ45
0
0
0
0
λ55
1
0
0
0
0






, M2 =






0
0
λ33
λ34
λ35
0
0
λ43
λ44
λ45
0
0
0
0
λ55
1
0
0
0
0
0
1
0
0
0






,
M3 =








λ11
λ12
0
λ14
λ15
0
λ22
0
λ24
λ25
0
0
0
0
λ55
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0








, M4 =






0
0
0
0
λ55
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0






, M5 = I5
M5 is trivially full rank and M1, M2, M4 have, for example, full rank if λ22, λ33, λ44
and λ55 are non-zero with λ24 = λ25 = λ34 = λ35 = λ43 = λ45 = 0. M3 will have full rank
if, in addition, λ14 is non-zero with λ11 = λ12 = λ15 = 0. The SVAR is thus identiﬁed.
Rubio-Ramirez et al. (2010, theorem 5) also gives an alternative condition for exact
identiﬁcation that is useful for posterior simulation.
A SVAR is exactly identiﬁed if
for almost every structural parameter point (Λ, Θ) ∈U there is a unique orthonormal
matrix P such that (ΛP, ΘP) satisﬁes the restrictions. That is, we can simulate from
the unrestricted SVAR with Λ triangular and transform the draws into parameter points
that satisfy a set of exactly identifying restrictions provided that we can ﬁnd the matrix
P. Normally a transformation with an orthonormal matrix will not aﬀect the posterior
distribution since the Jacobian is unity.
In this case P is a function of (Λ, Θ) and
some care is needed to ensure that (ΛP, ΘP) and (Λ, Θ) has the same prior distribution.
Rubio-Ramirez et al. (2010, theorem 5) veriﬁes that the prior (39) with common variances
for the equations, i.e., λ ∼N (0, Im ⊗Σλ) and θ|λ ∼N (vec (MΛ) , Im ⊗Σθ) , is – due
to the Kronecker structure of the variances and the zero mean for λ – unaﬀected by a
transformation with P. It is also easy to see that this holds if the proper prior on λ is
replaced by the improper prior p (λ) ∝|det Λ|v since det (ΛP) = ± det Λ. In addition,
since an orthonormal transformation is observationally equivalent, it is possible to work
with any prior on the reduced form parameters, sample from the posterior distribution of
(Γ, Ψ) , Cholesky decompose Ψ = LL′ and transform to a triangular SVAR with Λ= L−T
and Θ = ΓL′.
An important implication of the alternative condition for exact identiﬁcation is that,
modulo the eﬀects of the prior speciﬁcation, the predictive distribution from a simple
triangular SVAR or reduced form VAR is identical to the predictive distribution from
any exactly identiﬁed SVAR. That this is the case is readily seen by noting that the
orthonormal transformation (ΛP, ΘP) has no eﬀect on reduced form parameters. For
forecasting purposes it is thus, depending on the choice of prior speciﬁcation, suﬃcient to
work with the reduced form model or a triangular SVAR as long as the set of restrictions
considered identify the SVAR exactly.
Note that the triangular SVAR can be as in
section 4.1 with p (λ) ∝|det Λ|v or as in section 4.2 with λ ∼N (0, Im ⊗Σλ) and
restrictions Qiλj = 0 yielding a triangular Λ. For completeness the algorithm for ﬁnding
the orthonormal transformation matrix P devised by Rubio-Ramirez et al. (2010) and
generating random numbers from the posterior distribution of an exactly identiﬁed SVAR
36

Algorithm 7 Sampler for exactly identiﬁed SVARs
Depending on the choice of prior speciﬁcation, generate reduced form parameters
 Γ(j), Λ(j)
using one of the algorithms in section 3 or the structural form parameters
 Λ(j), Θ(j)
for a triangular SVAR using algorithm 5 with p (λ) ∝|det Λ|v or algorithm
6 with λ ∼N (0, Im ⊗Σλ). In the former case calculate the Cholesky decomposition
Ψ(j)= LL′ and set Λ(j)= L−T, Θ(j)= Γ(j)L′. Discard any burn-in as needed.
For each draw from the original sampler
1. For k = 1, . . . , m
(a) Set eRk =





Rkf

Λ(j), Θ(j)
p′
1
. . .
p′
j−1




(eR1 = R1f

Λ(j), Θ(j)
)
(b) Solve for eRjpj = 0, for example by calculating the QR decomposition of eR′
j =
QR and setting pj = qm, the last column of Q.
2. Form P = (p1, . . . , pm) and calculate structural form parameters eΛ(j) = Λ(j)P and
eΘ(j)= Θ(j)P that satisﬁes the restrictions.
is given as algorithm 7.
Forecasting performance
¨Osterholm (2008b) use a structural BVAR to construct fan
charts for Sweden and provides a limited forecast evaluation. The model contains nine
variables, the foreign trade weighted GDP growth, inﬂation and interest rate, the Swedish
unemployment rate, GDP growth, growth rate in wages, inﬂation, interest rate and the
trade weighted exchange rate. The SVAR puts restriction on the Λ matrix which has
a basic lower triangular structure with the additional restrictions λ2,1 = λ3,1 = λ4,1 =
λ4,2 = λ4,3 = λ5,4 = λ6,3 = λ7,4 = λ8,1 = λ8,5 = 0 and allows λ4,5, λ5,7, λ5,8, λ6,7 and
λ8,9 to be non-zero. In the forecast evaluation a steady-state version of the SVAR and a
naive random walk is also included. The steady state SVAR produces the best forecasts
for Swedish inﬂation and forecast performance of the SVAR is somewhat better than
the random walk. For GDP growth the steady state SVAR is again best followed by the
random walk and the SVAR. The random walk provides the best forecasts for the inﬂation
rate followed by the steady state SVAR and the SVAR.
5
Cointegration
Cointegration, that two or more non-stationary (integrated) variables can form a station-
ary linear combination and thus are tied together in the long run, is a powerful concept
that is appealing both from an economic and forecasting standpoint. Economically this
can be interpreted as a statement about long run equilibria and the information that the
variables tend to move together in the long run should be useful for forecasting.
37

In order to explicitly model the cointegrating properties of the data we write the VAR
(6) in error correction form
∆yt = Πyt−1 +
p−1
X
i=1
Bi∆yt−i + C′xt + ut
(43)
where Π = −(Im −Pp
i=1 A′
i) and Bi = −Pp
j=i+1 A′
j. If the m time series in yt are
stationary Π is a full rank matrix and if they all are non-stationary, integrated of order
1 or I (1) , but there is no cointegration Π will be a zero matrix. Here the focus is on the
intermediate case where Π is of reduced rank r < m and can be decomposed into two
m×r matrices Π = αβ′ with β forming r cointegrating relations, β′yt, or stationary linear
combinations of the I (1) variables in yt. The analysis of the cointegrated VECM (43) is
complicated by the non-linear parameterization and, more fundamentally, by two issues of
identiﬁcation. Firstly, α and β are globally non-identiﬁed since any transformation with
a full rank matrix eα = αP, eβ = βP−T leaves Π unchanged. This is commonly solved by
imposing a normalization β′ = (Ir, β′
∗) but this can, as we shall see later, be problematic.
Secondly, as noted by Kleibergen and van Dijk (1994), β is locally non-identiﬁed when α
has reduced rank, e.g. when α = 0. See Koop, Strachan, van Dijk and Villani (2006) for
a more comprehensive review of Bayesian approaches to cointegration.
5.1
Priors on the cointegrating vectors
It is relatively straightforward to form prior opinions about the cointegrating vectors,
for example in the form of speciﬁc relations between the variables that are suggested
by economic theory. It is thus quite natural to formulate a prior on the cointegrating
vectors, β, and proceed with the analysis based on this prior. This leads to a relatively
straightforward procedure for posterior inference but it is not without problems as it
overlooks some of the fundamental issues in the analysis of the cointegrated VAR-model.
For a given number of cointegrating relations, r, the VECM can be rewritten in matrix
form as
Y∆= Y−1βα′ + XΘ + U
(44)
= ZβΓ + U
where Y∆has rows ∆yt, Y−1 rows yt−1, X rows
 ∆y′
t−1, . . . , ∆y′
t−p+1, x′
t

, Zβ= (Y−1β, X) ,
and Θ′= (B1, . . . , Bp−1, C′) and Γ′= (α, Θ′) k × m and (k + r) × m parameter matrices.
With ut ∼N (0, Ψ), and conditioning on β, (44) is just a standard multivariate regression
model and can be analyzed using one of the prior families for (Γ, Ψ) discussed in section
3.2 if there is prior independence between β and (Γ, Ψ). In particular, Geweke (1996a)
speciﬁed an independent normal-Wishart type prior (section 3.2.2) for the parameters in
(44) with Ψ ∼iW (S, v) and independent normal priors for vec (α) , vec (β) and vec (Θ)
with mean zero and variance-covariance matrix τ −2I. Based on this he derived the full
conditional posteriors and proposed a Gibbs sampling algorithm for exploring the joint
posterior. Here we will consider a slightly more general prior speciﬁcation,
vec (α′) ∼N (vec (α′) , Σα) , θ = vec (Θ) ∼N (θ, Σθ) , Ψ ∼iW (S, v)
(45)
38

and an independent normal prior for the free elements of β to be speciﬁed later. Note
that the prior for α is speciﬁed in terms of the transpose of α. The full conditionals for
Ψ, α and θ are obtained using standard results. We have
Ψ|YT, β, Γ ∼iW
 S, v

, S = S + (Y −ZβΓ)′ (Y −ZβΓ) , v = v + T.
(46)
Combine the priors for α and Θ into a joint prior for Γ = (α, Θ′)′ , γ = vec (Γ) ∼
N
 γ, Σγ

, we then have the full conditional posterior as
γ|YT, β, Ψ ∼N
 γ, Σγ

(47)
where γ and Σγ are given by (24) with Z and Y replaced by Zβ and Y∆.
The full conditional posterior for β is more complicated due to the nonlinear nature
of the model and the need for at least r2 identifying restrictions. A common identifying
scheme is to set β′ = (Ir, β′
∗) , more generally we can consider restrictions of the form
Riβi = ri on the individual cointegrating vectors (columns of β). These restrictions are
conveniently expressed as βi = hi + Hiξi where ξi corresponds to the free parameters in
βi.17 To derive the full conditional posterior for ξ, we follow Villani (2001) and vectorize
the model Yθ = Y∆−XΘ = Y−1βα′ + U to obtain
yθ = (α ⊗Y−1) vec (β) + u = Y−1,α (h + Hξ) + u
yθ,α = yθ −Y−1,αh = Y−1,αHξ + u
where h = (h′
1, . . . h′
r)′ , H = diag (Hi) and ξ = (ξ′
1, . . . , ξ′
r) . With a normal prior on ξ,
ξ ∼N
 ξ, Σξ

,
(48)
i.e. vec (β) ∼N
 h + Hξ, HΣξH′
which is a degenerate distribution due to the restric-
tions on β, standard results yields the full conditional posterior as
ξ|YT, Γ, Ψ ∼N
 ξ, Σξ

(49)
Σξ =
 Σ−1
ξ
+ H′  α′Ψ−1α ⊗Y′
−1Y−1

H
−1
ξ = Σξ

Σ−1
ξ ξ + H′  α′Ψ−1 ⊗Y′
−1

yθ,α

.
A Gibbs sampler can thus easily be constructed by sampling from the full conditional
posteriors for ξ (and forming β), Γ and Ψ.
It is, as noted by among others Kleibergen and van Dijk (1994) and Geweke (1996a),
crucial that a proper prior are used for β and α. Without this the local nonidentiﬁcation,
as well as the possibility that the true cointegrating is less than r, will lead to an improper
posterior.
It is also possible to work with a normal-Wishart type prior as in section 3.2.1, this is
close to a conjugate prior and leads to some simpliﬁcations. Bauwens and Lubrano (1996)
achieve similar simpliﬁcations with an uninformative Jeﬀreys type prior π (α, Θ, Ψ) ∝
|Ψ|−(m+1)/2 together with an independent prior on β. Similar to Sugita (2002) we specify
a normal-Wishart type prior,
α′|Ψ ∼MNrm (α′, Ψ, Ωα) , Θ|Ψ ∼MNkm (Θ, Ψ, Ωθ) and Ψ ∼iW (S, v)
(50)
17Set hi = ei, column i in the identity matrix Im, and Hi =
 0(m−r)×r, Im−r
′ to obtain the ”default”
normalisation β′ = (Ir, β′
∗) .
39

together with the independent normal prior (48) for the free elements ξ in β.18 It is
convenient to combine the priors on α and Θ in a prior on Γ,
Γ|Ψ ∼MN(r+k),m
 Γ, Ψ, Ωγ

, Γ′ = (α, Θ′) , Ωγ = diag (Ωα, Ωθ) .
(51)
With the prior for β independent of Γ and Ψ it is clear that the posterior for Γ and Ψ
conditional on β is of the normal-Wishart form,
Γ|YT, β, Ψ ∼MN(r+k),m
 Γ, Ψ, Ωγ

,
(52)
Ω
−1
γ
= Ω−1
γ
+ Z′
βZβ, Γ = Ωγ
 Ω−1
γ Γ + Z′
βY∆

,
Ψ|YT, β ∼iW
 S, v

,
(53)
S= S + S+

Γ −bΓ
′ 
Ωγ +
 Z′
βZβ
−1−1 
Γ −bΓ

, v = T + v.
Peters et al. (2010) propose using a Metropolis within Gibbs MCMC scheme for sam-
pling from the joint posterior distribution of β, Γ and Ψ with a Metropolis-Hastings step
for β. Peters et al. (2010) considered two random walk type proposals, a mixture pro-
posal with one component designed to produce local moves and one component producing
global moves and an adaptive proposal where variance-covariance matrix is continuously
updated based on the previous output of the Markov chain.
The MCMC scheme of Peters et al. (2010) has the advantage that it does not rely on a
speciﬁc form for the prior on β. On the other hand, if we specify a normal prior for β (or
ξ) the derivations leading to the full conditional posterior (49) with the normal-Wishart
type prior on Γ and Ψ and a standard Gibbs sampler is available in this case.
Specifying the prior beliefs
The Minnesota prior is a useful starting point when
thinking about the prior for Θ. Considering that Θ contains B1, . . . , Bp−1 which are
autoregressive coeﬃcient matrices on the stationary ﬁrst diﬀerences a reasonable choice
is to set the prior means to zero and prior variances as in 14 with the modiﬁcations
discussed in section 3.2.1 for the normal-Wishart type prior. Alternatively one can start
with a Minnesota prior for the autoregressive parameters Ai in the reduced form VAR
(6) and derive the prior mean and variance for Bj from the relation Bj = −Pp
i=j+1 A′
i.
The priors for α and β (or ξ) is a more delicate matter. Economic theory can in many
cases suggest plausible cointegrating vectors and restrictions Riβi = ri. Care is however
needed in the speciﬁcation of the restrictions, at least one element of ri must be nonzero
otherwise the ith cointegrating vector will only be identiﬁed up to an arbitrary scale factor.
This in turn has implication for which variables are, in fact, cointegrated and ﬁxing the
coeﬃcient of a variable that is not cointegrated to a non-zero value will clearly result in
misleading inference. It is harder to form prior beliefs about the adjustment coeﬃcients
α and a relatively uninformative prior with zero mean might be suitable. Note, however,
that under prior independence we have E (Π) = E (α) E (β′) and a prior mean of zero
for α or β implies that E (Π) = 0 which is at odds with the assumption that Π has rank
r > 0.
18Sugita (2002) and later Peters, Kannan, Lassock and Mellen (2010) used a matric-variate normal for
β∗in the normalization β′ = (Ir, β′
∗) but there seem to be no particular advantage to the more restrictive
Kronecker structure of the prior variance-covariance.
40

Algorithm 8 Gibbs sampler for VECM with a prior on β
With the normal-Wishart type prior (50), (48) select starting values β(0)
For j = 1, . . . , B + R
1. Generate Ψ(j) from the conditional posterior Ψ|YT, β(j−1) ∼iW
 S, v

in (53)
2. Generate
Γ(j)
from
the
full
conditional
posterior
Γ|YT, β(j−1), Ψ(j)
∼
MN(r+k),m

Γ, Ψ(j), Ωγ

in (52)
3. Generate ξ(j) from the full conditional posterior ξ|YT, Γ(j), Ψ(j) ∼N
 ξ, Σξ

in (49)
and form β(j)
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h
using A1 = I + B′
1 + βα′, Ai = B′
i −B′
i−1, i = 2, . . . , p −1 and Ap = −B′
p−1.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
Simulating from the posterior distribution
The adaptive MCMC scheme is well
described in Peters et al. (2010) and the Gibbs sampler is outlined in algorithm 8.
5.2
Priors on the cointegrating space
As alluded to above, the approach of working directly with the cointegrating vectors in β
can be problematic. The issues are most easily discussed in relation to the linear normal-
ization β = (Ir, β′
∗)′ frequently used to identify the model. Partitioning β = (β′
1, β′
2) with
β1 a r × r matrix, the normalization sets β∗= β2β−1
1 . This has two implications, ﬁrstly
the variables must be ordered in such a way that β1 is a full rank matrix, secondly β∗will
have a fat tailed distribution, possibly with no posterior moments19, unless a suﬃciently
informative prior on β∗is used.
The fundamental issue underlying this is the lack of identiﬁcation of β and that only
the space spanned by β (the cointegrating space, p = sp (β)) is identiﬁed by the model.
As argued by Villani (2000) we should then consider the prior for β in terms of this
space. The columns of the rank r, m×r matrix β deﬁnes an r-dimensional hyperplane in
Rm, the space spanned by the columns of β. Formally, the set of all such hyperplanes is
known as the Grassman manifold, Gr,m−r, and Villani (2005) shows that a uniform prior
on Gr,m−r implies a matricvariate t distribution with r degrees of freedom on β∗, β∗∼
19For the case of one cointegrating vector, r = 1, and an impoper prior on β Bauwens and Lubrano
(1996) shows that the marginal posterior for β has ﬁnite moments up to the order of the number of
overidentifying restrictions.
That is, for r = 1 two restrictions in addition to normalizing the ﬁrst
element of β to 1 is needed for the posterior variance to exist.
41

Mtm−r,r (0, I, I,r) or p (β∗) ∝|Ir + β′
∗β∗|−m/2 , when the linear normalization is used.
This is quite diﬀerent from using a uniform prior on β∗. While the linear normalization
implies a strong prior belief that the ﬁrst r variables are included in cointegrating relations
and that β1 is a full rank matrix, a uniform prior on β∗will, as pointed out by Strachan
and Inder (2004), in fact put most of the prior mass on regions where β1 is (close to)
non-invertible.
Departing from the uniform prior on the cointegrating spaces we consider a prior that
is similar to the reference prior of Villani (2005),
β∗∼Mtm−r,r (0, I, I,r)
(54)
α′|β, Ψ ∼MNr,m

0, Ψ,c−1 (β′β)−1
Θ|Ψ ∼MNkm (Θ, Ψ, Ωθ)
Ψ ∼iW (S, v) .
The main diﬀerence compared to Villani is that we use a normal prior for Θ instead of the
improper prior p (Θ) ∝1. The results for the ﬂat prior on Θ can be obtained by setting
Ω−1
θ
to zero below.
The prior distribution for α can be motivated by considering the prior for α when
β is orthonormal, i.e. β′β = Ir.20 Postmultiplying β with (β′β)−1/2 results in a set of
orthogonalized cointegrating vectors eβ = β (β′β)−1/2 and to keep Π = αβ′ unchanged we
need to adjust α accordingly, eα = α (β′β)1/2 . It follows that the conditional distribution
of eα is eα′| eβ, Ψ ∼MNr,m (0, Ψ,c−1I) or eαi|Ψ ∼N (0, c−1Ψ) , i = 1, . . . , r. Note that
within the class of matricvariate normal priors eα′| eβ, Ψ ∼MNr,m (µ, Ω1, Ω2) the only
ones which are invariant to orthogonal rotations of eβ are those with µ = 0 and Ω2 =
c−1I. The marginal prior for eα′ is matricvariate t and the prior variance-covariance is
V (vec eα) =
1
c(v−m−1)Ir ⊗S which clariﬁes the role of the scale factor c−1 in tuning the
prior variance.
Writing the VECM as Yαβ = Y∆−Y−1βα′ = XΘ + U we can derive the posterior
distributions for Θ and Ψ conditional on α and β. This requires that we keep track of
the contribution from the joint prior for α and β conditional on Ψ,
p (α, β|Ψ) ∝|β′β|m/2 |Ψ|−r/2 exp

−1
2 tr
 Ψ−1α (cβ′β) α′
× |β′β|−m/2
(55)
= |Ψ|−r/2 exp

−1
2 tr
 Ψ−1α (cβ′β) α′
where we have used that β′β = Ir + β′
∗β∗in the prior for β.
Using standard results we obtain the conditional posteriors as
Θ|YT, α, β, Ψ ∼MNk,m
 Θ, Ψ,Ωθ

(56)
Ωθ =
 Ω−1
θ
+ X′X
−1 , Θ = Ωθ
 Ω−1
θ Θ + X′Yαβ

,
20The restriction β′β = Ir is not suﬃcient to identify β since it can always be rotated to a new
orthogonal matrix by postmultiplying with an r × r orthogonal matrix.
42

and
Ψ|YT, α, β ∼iW
 S, v

, v = T + v + r
(57)
S = S + S + cαβ′βα′ + Θ′Ω−1
θ Θ + bΘ′X′X bΘ −Θ
′Ω
−1
θ Θ
for S =

Yαβ −X bΘ
′ 
Yαβ −X bΘ

and bΘ = (X′X)−1 X′Yαβ.
To derive the full conditional posterior for α write the VECM as Yθ = Y∆−XΘ =
Y−1βα′ +U and apply the results for the normal-Wishart prior in section 3.2.1 to obtain
the conditional posterior
α′|β, Θ, Ψ ∼MNr,m
 α′, Ψ, Ωα

(58)
Ωα =

β′  cIm + Y′
−1Y−1

β
−1
α′ = Ωαβ′Y′
−1Yθ.
For the full conditional posterior for β we note that the contribution from the prior can
be rewritten as tr Ψ−1α (cβ′β) α′ = tr β (cα′Ψ−1α) β′ = tr cα′Ψ−1α+tr β∗(cα′Ψ−1α) β′
∗
with tr β∗(cα′Ψ−1α) β′
∗= vec (β∗)′ (α′Ψ−1α ⊗cIm−r) vec (β∗) . That is, the prior for β∗
conditional on α and Ψ is matricvariate normal, MNm−r,m

0, (α′Ψ−1α)−1 , c−1Im−r

.
Next rewrite Y−1βα′ = (Y−1,1 + Y−1,2β∗) α′ and vectorize the regression Yθα = Y∆−
XΘ −Y−1,1α′= Y−1,2β∗α′ + U,
yθα = (α ⊗Y−1,2) vec (β∗) + u.
The full conditional posterior for β∗is then obtained as
β∗|YT, α, Θ, Ψ ∼MNm−r,m

β∗,
 α′Ψ−1α
−1 , Ωβ

(59)
Ωβ =
 cIm−r + Y′
−1,2Y−1,2
−1
β∗= ΩβY′
−1,2YθαΨ−1α
 α′Ψ−1α
−1 .
Using the improper prior p (Θ) ∝1 instead of a normal prior as here Villani (2005)
derived the conditional posteriors p (α|YT, β) and p (β|YT, α) as well as the marginal
posterior for β. Villani also shows that the posterior distribution of β∗has no ﬁnite
moments as can be expected with the linear normalization β = (Ir, β′
∗)′ .
The choice of normalization or identifying restrictions on β is thus crucial. Strachan
(2003) proposes a data based normalization which restricts the length of the cointegrating
vectors and ensures that the posterior for β is proper with ﬁnite moments but also implies
that the prior for β or sp (β) is data based. Strachan and Inder (2004) instead propose
working with the normalization β′β = Ir. While this is not suﬃcient to identify β it does
restrict β to the set of semi-orthonormal m × r matrices, the Stiefel manifold Vr,m.
There is often prior information about likely cointegrating vectors and Strachan and
Inder (2004) proposes a convenient method for specifying an informative prior on the
cointegrating space. First specify an m × r matrix with likely cointegrating vectors, e.g.
Hg =


1
0
−1
1
0
−1


43

for m = 3 and r = 2. Since sp (Hg) = sp (HgP) for any full rank r ×r matrix we can map
Hg into V2,3 by the transformation H = Hg
 H′
gHg
−1/2 and calculate the orthogonal
complement H⊥, i.e. H⊥⊆Vm−r,m and H′H⊥= 0.21 That is,
H =


p
1/12 + 1
2
1
2 −
p
1/12
−
p
1/3
p
1/3
p
1/12 −1
2
−
p
1/12 −1
2

, H⊥=


p
1/3
p
1/3
p
1/3

.
Next consider the space spanned by the matrix Pτ = HH′ + τH⊥H′
⊥, for τ = 0 this is
sp (H) and for τ = 1 we have Pτ = Im and sp (Pτ) = Rm. Specifying the prior for β as a
matrix angular central Gaussian distribution with parameter Pτ, MACG (Pτ) ,
p (β) ∝|Pτ|−r/2 β′P−1
τ β
−m/2 ,
(60)
centers the distribution of p = sp (β) on sp (H) with the dispersion controlled by τ. For
τ = 0 we have a dogmatic prior that p = sp (H) and for τ = 1 a uniform prior on
the Stiefel manifold which is equivalent to the uniform prior used by Villani (2005). By
varying τ ∈[0, 1] we can thus make the prior more or less informative.
Strachan and Inder (2004) propose using a Metropolis-Hastings sampler to evaluate
the posterior distribution of the parameters under the prior (60) on β and a prior sim-
ilar to (54) on the remaining parameters.
Koop, Le´on-Gonz´alez and Strachan (2010)
propose a convenient Gibbs sampling scheme that depends on reparameterizing and in
turn sample from a parameterization where α is semiorthogonal and β unrestricted and
a parameterization where β is semiorthogonal and α is unrestricted.
This solves the
main computational diﬃculty with the semiorthogonal normalization where it is diﬃcult
generate β subject to the restriction β′β = Ir.
Koop et al. (2010) develops the Gibbs sampling algorithm in a VECM without lags
of ∆yt or deterministic variables. We will consider the more general model (44) and will
thus need a prior for Θ in addition to the prior on β, α and Ψ speciﬁed by Koop et al.
(2010), in addition to (60) we have
α′|β, Ψ ∼MNr,m

0, Ψ,c−1  β′P1/τβ
−1
(61)
Θ|Ψ ∼MNkm (Θ, Ψ, Ωθ)
Ψ ∼iW (S, v) ,
where P1/τ = HH′ + τ −1H⊥H′
⊥= P−1
τ
a choice which facilitates the development of the
Gibbs sampler. Koop et al. (2010) also considers the improper prior p (Ψ) ∝|Ψ|−m/2, the
results for this prior can be obtained by setting S = 0 and v = 0 below.
The key to the Gibbs sampler of Koop et al. (2010) is the reparameterization
αβ′ =
 ακ−1
(βκ)′ =
h
α (α′α)−1/2i h
(α′α)1/2 β
i′
= AB′
21The square root matrix of a positive deﬁnite and symmetric matrix, such as C = H′
gHg, is unique
and can be obtained from the spectral decomposition C = XΛX′ where X is the matrix of orthonormal
eigenvectors and Λ has the eigenvalues, λi, on the diagonal. Consequently C1/2 = XΛ1/2X′ with λ1/2
i
as the diagonal elements of Λ1/2 and H′H =
 H′
gHg
−1/2 H′
gHg
 H′
gHg
−1/2 = I.
44

where A is semiorthogonal and B is unrestricted. For further reference note that the
transformations from α to (A, κ) and from B to (β, κ) where κ = (α′α)1/2 = (B′B)1/2
is symmetric and positive deﬁnite are one-to-one, in addition β = B (B′B)−1/2 and α =
A (B′B)1/2 . The implied priors for A and B can be obtained as (see Koop et al. (2010)
for details)
B|A, Ψ ∼MNm,r

0,
 A′Ψ−1A
−1 , c−1Pτ

(62)
A|Ψ ∼MACG (Ψ) .
The derivation of the full conditional posteriors proceed as above. The full conditional
posterior for Θ is matricvariate normal and given by (56) and the full conditional posterior
for Ψ is inverse Wishart,
Ψ|YT, α, β ∼iW
 S, v

, v = T + v + r
(63)
S = S + S + cαβ′P1/τβα′ + Θ′Ω−1
θ Θ + bΘ′X′X bΘ −Θ
′Ω
−1
θ Θ
with S and bΘ as in the conditional posterior (57). The full conditional posterior for α is
a straightforward modiﬁcation of the conditional posterior (58),
α′|β, Θ, Ψ ∼MNr,m
 α′, Ψ, Ωα

(64)
Ωα =

β′  cP1/τ + Y′
−1Y−1

β
−1
α′ = Ωαβ′Y′
−1Yθ.
The full conditional posterior for β is complicated by the semiorthogonal normaliza-
tion, instead the Gibbs sampling scheme of Koop et al. (2010) make use the full conditional
posterior for the unrestricted parameter B,
B|YT, A, Θ, Ψ ∼MNm,r

B,
 A′Ψ−1A
−1 , ΩB

(65)
ΩB =
 cP−1
τ
+ Y′
−1Y−1
−1
B = ΩBY′
−1 (Y∆−XΘ) Ψ−1A
 A′Ψ−1A
−1 .
The idea behind the Gibbs sampler of Koop et al. (2010) is based on the fact that a
draw α(∗) from (64) is also a draw
 A(∗),κ(∗)
from p (A, κ|YT, β, Θ, Ψ) , second drawing
B(j) from (65) yields a draw of
 β(j), κ(j)
from p
 β, κ|YT, A(∗), Θ, Ψ

and we can map
A(∗) and B(j) into α(j) = A(∗)  B(j)′B(j)1/2 , β(j) = B(j)  B(j)′B(j)−1/2 and the draws
κ(∗) and κ(j) are simply discarded.
In addition to the just identiﬁed case discussed here, Koop et al. (2010), also studies
the case with overidentifying restrictions of the form βi = Hiξi considered in section 5.1
and provides a Gibbs sampling algorithm.
Specifying the prior beliefs
The same considerations for the prior on Θ holds here as
in section 5.1. The informative prior (60) for β requires that we specify the tuning constant
τ. Keeping in mind that τ = 0 corresponds to a dogmatic prior and τ = 1 corresponds to
45

a uniform prior on sp (β), setting τ < 1/2 seems appropriate. It is, however, diﬃcult to
develop intuition for τ and some sensitivity analysis is advisable. The choice of central
location H (or Hg) should obviously be based on economic intuition and theory or other
subject speciﬁc information.
The prior distribution for α requires a choice of the scale factor c, the prior is centered
on zero with variance V (vec α) =
1
c(v−m−1)
 β′P1/τβ
−1 ⊗S conditional on β. Evaluating
this at the central location β = H of the informative prior (60) or P1/τ = I for the
uniform prior yields V (vec α) =
1
c(v−m−1)Ir ⊗S which can serve as a guide when choosing
c. Alternatively, as suggested by Koop et al. (2010), a hierarchical prior structure can be
used with inverse Gamma priors on c (and v) if the researcher prefers to treat them as
unknown parameters.
Sampling from the posterior distribution
The essential diﬀerence between the pos-
terior distributions discussed here is the type of normalization imposed on β. For the linear
normalization β = (Ir, β′
∗)′ and a ﬂat prior on sp (β) an adaption of the Gibbs sampler of
Villani (2005) is given as Algorithm 9. For the orthogonal normalization β′β = Ir and a
possibly informative prior on the cointegrating space an adaption of the Gibbs sampler of
Koop et al. (2010) is given in algorithm 10. Note that the orthogonal normalization does
not identify β and additional normalizations may be needed to obtain easily interpretable
cointegrating vectors.
5.3
Determining the cointegrating rank
A simple approach to inference on the cointegrating rank, r, used in early Bayesian work,
e.g. DeJong (1992) and Dorfman (1995), is to work with the reduced form VAR using one
of the priors in section 3. In this context the posterior distribution of the cointegrating
rank can be obtained from the posterior distribution of the roots of the autoregressive
polynomial or the rank of the impact matrix Π = −(Im −Pp
i=1 A′
i) . Sampling from the
posterior distribution of the parameters it is straightforward to estimate the posterior dis-
tribution of the cointegrating rank by counting the number of roots of the AR-polynomial
that are greater than, say, 0.99 or using the QR or SVD decompositions to ﬁnd the rank
of Π for each draw from the posterior.
While the unrestricted reduced form approach is straightforward it does not take
account of the reduced rank restrictions on Π for r < m. Proper Bayesian model selection
and model averaging account for this by basing the analysis on marginal likelihoods for
models with diﬀerent cointegrating rank r and calculating posterior probabilities p (r|YT)
as in (5). This does, however, require some care to ensure that the marginal likelihood
is well deﬁned. As a minimum proper priors for β and α are needed as these change
dimension with r and as a general rule at least mildly informative priors should be used
for all parameters with the possible exception of Ψ.
Using a prior on the cointegrating vectors as in section 5.1 with partially prespeci-
ﬁed cointegrating vectors Villani (2001) approximates the log marginal likelihood with
the Bayesian Information Criteria of Schwarz (1978). Sugita (2002) shows how to use
the generalized Savage-Dickey density ratio of Verdinelli and Wasserman (1995) to com-
pute the Bayes factors BFi,0 comparing the model with r = i against the model with
r = 0 and the posterior probabilities p (r|YT) with the prior setup (50) together with a
46

Algorithm 9 Gibbs sampler for VECM with a prior on sp (β) and linear normalization
With the prior (54), an uninformative prior on sp (β) coupled with the linear normalization
β = (Ir, β′
∗)′ , select starting values α(0) and β(0).
For j = 1, . . . , B + R
1. Generate Ψ(j) from the full conditional posterior Ψ|YT, α(j−1), β(j−1) ∼iW
 S, v

in (57)
2. Generate Θ(j) from the full conditional posterior Θ|YT, α(j−1), β(j−1), Ψ(j)
∼
MNk,m
 Θ, Ψ,Ωθ

in (56)
3. Generate
α(j)
from
the
full
conditional
posterior
α′|β(j−1), Θ(j), Ψ(j)
∼
MNr,m
 α′, Ψ(j), Ωα

in (58)
4. Generate β(j)
from the full conditional posterior β∗|YT, α(j), Θ(j), Ψ(j)
∼
MNm−r,m

β∗,

α(j)′  Ψ(j)−1 α(j)−1
, Ωβ

in (59)
5. If j > B generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h
using A1 = I + B′
1 + βα′, Ai = B′
i −B′
i−1, i = 2, . . . , p −1 and Ap = −B′
p−1.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
matricvariate normal prior for β∗. Villani (2005) derives closed form expressions for the
marginal likelihoods m (YT|r = 0) and m (YT|r = m) under the uniform prior on Gr,m
for sp (β) and linear normalization, i.e. the prior (54) but with π (Θ) ∝1, and uses the
Chib (1995) method to estimate the marginal likelihood for intermediate cases from the
Gibbs sampler output, e.g. Algorithm 9. With the orthogonal normalization and an un-
informative prior on Θ and Ψ, π (Θ, Ψ) ∝|Ψ|−(m+1)/2 Strachan and Inder (2004) derives
the posterior p (β, r|YT) and use a Laplace approximation to integrate out β to obtain
the posterior distribution of the cointegrating rank. Sugita (2009) studies rank selection
in a Monte Carlo experiment where the marginal likelihood is approximated with BIC or
estimated using the Chib method and ﬁnds that BIC performs well when T ≥100 and
Chib’s method requires considerably larger sample sizes, T > 500, to perform well.
Forecasting performance
Villani (2001) forecasts the Swedish inﬂation rate with sev-
eral versions of a 7 variable VECM with the Swedish GDP, CPI, interest rate, trade
weighted exchange and ”foreign” GDP, price level and interest rate. Villani considers
several theory based cointegrating relations which are all rejected by the data in favour of
a model with cointegrating rank 3 and unrestricted cointegrating relations. Nonetheless,
47

Algorithm 10 Gibbs sampler for VECM with a prior on sp (β) and orthogonal normal-
ization
With the orthogonal normalization β′β = Ir and the informative prior (60) and (61) the
Gibbs sampler of Koop et al. (2010) is applicable. Select starting values α(0) and β(0).
For j = 1, . . . , B + R
1. Generate Ψ(j) from the conditional posterior Ψ|YT, α(j−1), β(j−1) ∼iW
 S, v

in
(63)
2. Generate Θ(j) from the full conditional posterior Θ|YT, α(j−1), β(j−1), Ψ(j)
∼
MNk,m
 Θ, Ψ,Ωθ

in (56)
3. Generate
α(∗)
from
the
full
conditional
posterior
α′|β(j−1), Θ(j), Ψ(j)
∼
MNr,m
 α′, Ψ(j), Ωα

in (64) and calculate A(∗) = α(∗)  α(∗)′α(∗)−1/2
4. Generate
B(j)
from
the
conditional
posterior
B|YT, A(∗), Θ(j), Ψ(j)
∼
MNm,r

B,

A(∗)′  Ψ(j)−1 A(∗)−1
, ΩB

in
(65)
and
calculate
α(j)
=
A(∗)  B(j)′B(j)1/2 and β(j) = B(j)  B(j)′B(j)−1/2
5. If j > B generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h
using A1 = I + B′
1 + βα′, Ai = B′
i −B′
i−1, i = 2, . . . , p −1 and Ap = −B′
p−1.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
Villani goes ahead and forecasts with both theory based (PPP, stationary domestic and
foreign interest rates) and unrestricted cointegrating vectors with stationary Minnesota
type prior beliefs on the short run dynamics. Of the considered models, Bayesian VECMs
and ML-estimated VECMs and an ARIMA model, the Bayesian VECMs do best and are
very close to each other.
6
Conditional forecasts
It is often of interest to condition the forecasts on diﬀerent scenarios, for example diﬀerent
trajectories for the world economy, diﬀerent developments of the oil price or diﬀerent paths
for the interest rate considered by a central bank. Another use of conditional forecasts is
to incorporate information from higher frequency data or judgement into the model. An
early example of conditional forecasts is Doan et al. (1984) who note that conditioning on
a speciﬁc path for a variable is (given the parameters of the model) equivalent to imposing
a set of linear constraints on the future disturbances, uT+1, uT+2, . . . Conditional forecasts
48

can then be constructed by using the conditional means, buT+i, in the forecasting recursions
by′
T+h =
h−1
X
i=1
by′
T+h−iAi +
p
X
i=h
y′
T+h−iAi + x′
T+hC+bu′
T+h.
(66)
This approach, while straightforward, has two potential drawbacks.
It conditions on
speciﬁc parameter values (e.g.
the posterior means) and does not produce minimum
mean square error forecasts conditional on the restrictions. This can be overcome by
simulation from the posterior distribution of the parameters and solving for the restricted
distribution of the disturbances for each set of parameter values (the whole predictive
distribution can be simulated by also drawing uT+i from the restricted distribution). The
second issue is that the posterior distribution of the parameters will, in general, not be
consistent with the future path we are conditioning on.
Waggoner and Zha (1999) addresses both these issues. Let yT+1:T+H =
 y′
T+1, . . . , y′
T+H
′
denote the future values to be forecasted, we can then write the condition that some of
the variables follow a speciﬁc path or takes a speciﬁc value at a give time point as
RyT+1:T+H = r.
To see how this implies a restriction on the future disturbances we use recursive substi-
tution to rewrite the future yT+i in terms of past yt and future uT+j, j = 1, . . . , i.
yT+i = E (yT+i|YT, Γ, Ψ) +
i−1
X
j=0
B′
juT+i−j
(67)
where Bi are the parameter matrices in the MA-representation,
B0 = I
Bi =
q
X
m=1
AmBi−m, i > 0
and yT+i = E (yT+i|YT, Γ, Ψ) can be obtained trough the recursion (66) with buT+i = 0.
Stacking the equations (67) we obtain yT+1:T+H = yT+1:T+H + B′uT+1:T+H for
B =





B0
B1
· · ·
BH−1
0
B0
· · ·
BH−2
...
...
0
· · ·
0
B0




.
The restriction can then be written as
RyT+1:T+H = R
 yT+1:T+H + B′uT+1:T+H

= r
DuT+1:T+H = RB′uT+1:T+H = r −RyT+1:T+H = d.
Since uT+1:T+H ∼N (0, VH) with VH = IH ⊗Ψ, normal theory implies that the condi-
tional distribution of uT+1:T+H is
uT+1:T+H|DuT+1:T+H = d ∼N

VHD′ (DVHD′)−1 d, VH−VHD′ (DVHD′)−1 DVH

(68)
49

which can be used for the simulation of the predictive distribution discussed above. Note,
however, that the variance matrix is singular and some care is needed when generating
uT+1:T+H, see Jaroci´nski (2010) for an eﬃcient method to generate uT+1:T+H.22
This does not address the issue of the consistency of the posterior distribution of the
parameters and the restriction RyT+1:T+H = r. The restriction is information that in
principle should be incorporated in the prior. This is, however, not possible in practice
due to the highly non-linear relationship between the parameters and yT+1:T+H. Instead
Waggoner and Zha (1999) suggests treating yT+1:T+H as latent variables and simulate the
joint posterior distribution of the parameters and yT+1:T+H subject to the restriction and
gives a straightforward MCMC sampler for this. The sampler is reproduced as Algorithm
11.
In addition to the hard restrictions RyT+1:T+H = r Waggoner and Zha (1999) also
considers ”soft” restrictions on the form RyT+1:T+H ∈S where S is some subset of RmH in-
dicating an interval or region the forecasts that the forecasts are restricted to. Andersson,
Palmqvist and Waggoner (2010) generalizes the approach of Waggoner and Zha (1999)
to restrictions on the distribution of the future values, e.g.
RyT+1:T+H ∼N (r, Vr) .
Robertson, Tallman and Whiteman (2005) takes a diﬀerent approach and use exponential
tilting to modify the unrestricted predictive distribution to match moment conditions of
the form E [g (yT+1:T+H)] = g. An example of the use of the exponential tilting method
is Cogley, Morozov and Sargent (2005) who used it to adapt the predictive distribution
to the Bank of England target inﬂation rate and other information that is external to the
estimated VAR.
Forecasting performance
Bloor and Matheson (2011) forecasts New Zealand real
GDP, tradable CPI, non-tradable CPI, 90 interest rate and the trade weighted exchange
rates using a real time data set. The models considered includes univariate AR-models,
a small 5 variable VAR and BVAR, a medium sized 13 variable structural BVAR and a
large 35 variable structural BVAR. The prior speciﬁcation for the VARs is based on the
approach of Banbura, Giannone and Reichlin (2010) (see section 9.2). Overall the VAR
models do better than the univariate forecasting models with the large VAR improving
on the smaller models. Incorporating external information in the form of Reserve Bank
of New Zealand forecasts for variables where current data has not been released or future
trajectories of variables is found to improve the forecast performance of the models.
22Note that the formulation of Jaroci´nski (like the one of Waggoner and Zha) is in terms of a structural
VAR and generates structural form innovations rather than the reduced form used here. To see how the
method of Jaroci´nski maps to the results here let Λ−1 be a factor of Ψ, i.e. Λ−1Λ−T = Ψ where Λ might
come from a structural VAR or is the Cholesky factor of Ψ−1 (which is generally available as part of
generating Ψ from the full conditional posterior in a reduced form VAR). The reduced form disturbances
is related to the structural form innovations by uT +1:T +H =
 IH ⊗Λ−1
eT +1:T +H and the restriction on
the structural innovations is eReT +1:T +H = d for eR = D
 IH ⊗Λ−1
. Since the uconditional distribution
of eT +1:T +H is N (0, ImH) we get eT +1:T +H| eReT +1:T +H = d ∼N

eR′ 
eR′ eR

d, ImH −eR′ 
eR′ eR
−1 eR

and the method of Jaroci´nski can be used to generate ﬁrst eT +1:T +H and then uT +1:T +H.
50

Algorithm 11 MCMC sampler for VAR subject to ”hard” restrictions
For a VAR subject to the restrictions RyT+1:T+H = r select starting values Γ(0) and Ψ(0).
The starting values can be taken from a separate simulation run on the historical data.
For i = 1, . . . , B + R
1. Generate u(j)
T+1:T+H from the conditional distribution uT+1:T+H|DuT+1:T+H
=
d, Γ(j−1), Ψ(j−1) in (68) and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j−1)
i
+
p
X
i=h
y′
T+h−iA(j−1)
i
+ x′
T+hC(j−1)+u(j)′
T+h
2. Augment the data with ey(j)
T+1:T+H and generate the parameters Ψ(j) and Γ(j) from the
full conditional posteriors Ψ|YT, ey(j)
T+1:T+H, Γ(j−1) and Γ|YT, ey(j)
T+1:T+H, Ψ(j) using
the relevant steps from one of the samplers discussed in this chapter depending on
the choice of model structure and prior.
Discarding the parameters and keeping yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from
the joint predictive distribution subject to the restrictions.
7
Time-varying parameters and stochastic volatility
7.1
Time-varying parameters
The constant parameter assumption implicit in the formulation of the VAR model (6) is
often, but not always, reasonable. Parameter constancy might fail if the data covers a
long time period, if there are changes in economic policy (e.g. monetary policy) as well
as for many other reasons. It can thus be useful to allow for the parameters to change
over time and we write the VAR as
yt = Wtγt + ut
(69)
for Wt = Im ⊗z′
t.
Doan et al. (1984), Highﬁeld (1987) and Sims (1993) were among the earliest to in-
troduce parameter variation in VAR models. Sims (1993), Doan et al. (1984) retains
the equation by equation estimation strategy of Litterman while allowing the regression
parameters to follow an AR(1) process
γt+1 = π8γt + (1 −π8) γ + εt
(70)
with 0 ≤π8 ≤1. Doan et al. (1984) shows how the estimation can be conducted using
the Kalman ﬁlter to update the state vector γt and conducts a search over a subset of
the hyperparameters to ﬁnd the combinations that provides the best forecast accuracy
in a 10 variable VAR. Highﬁeld (1987) relaxes the assumption of a known diagonal error
variance-covariance matrix and uses the normal-Whishart conjugate prior in a state space
formulation of the model. These are all examples of the type of time-varying parameter
VAR models (TVP-VAR) formulated as state space models that we will focus on. There
51

are, of course, other ways to formulate a model where the parameters are allowed to
change over time. This includes models accommodating structural breaks by including
dummy variables that interact with some or all of the right hand side variables and Markov
switching models with a ﬁxed number of regimes (Chib (1998)) or an evolving number of
regimes (Pesaran, Petenuzzo and Timmermann (2006), Koop and Potter (2007)).
The popularity of the Bayesian approach to TVP-VARs owes much to Cogley and
Sargent (2002, 2005) and Primiceri (2005) who, although not primarily concerned with
forecasting, provides the foundations for Bayesian inference in these models. Koop and
Korobilis (2009) provides a good introduction to TVP-VARs.
The basic TVP-VAR complements the observation equation (69) with the state equa-
tion23
γt+1 = γt + εt.
(71)
That is, the parameters are assumed to follow a random walk and evolve smoothly over
time. εt is assumed to be normally distributed, εt ∼N (0, Q) and independent of the
error term in the observation equation which is also normal, ut ∼N (0, Ψ) . Note that
the state equation implies that γt+1|γt, Q ∼N (γt, Q) and that this in a sense serves as
prior distribution for γt+1 and the prior for all the states (parameters) is simply a product
of normal distributions that needs to be complemented with a prior for the ﬁrst state,
π (γ1) , which is then usefully also taken to be normal,
γ1 ∼N
 s1|0, P1|0

.
(72)
The prior speciﬁcation is completed with independent inverse Wishart priors for Ψ and
Q,
Ψ ∼iW (Ψ, v)
(73)
Q ∼iW
 Q, vQ

.
The time-varying parameter speciﬁcation introduces an additional layer of complica-
tion when forecasting since the parameters can not be assumed to be constant in the
forecast period. This contributes to additional variability in the predictive distribution
and we must simulate γT+h from the state equation 71 in order to simulate the predictive
distribution. See Cogley et al. (2005) for a discussion of these issues.
It is straightforward to set up a Gibbs sampler for the joint posterior distribution
of
 γT, Ψ, Q

(as a notational convention we will use superscripts to refer to sequences
of variables and parameters, i.e. xt to refer to the sequence x1, . . . , xt). Conditional on
the unobserved time varying parameters (states), γt, posterior inference for Ψ and Q is
standard and we have the full conditional posteriors as
Ψ|yT, γT ∼iW
 Ψ, v

, Ψ = Ψ +
T
X
i=1
(yt −Wtγt) (yt −Wtγt)′ , v = v + T
(74)
and
Q|γT ∼iW
 Q, v

, Q = Q +
T
X
i=1
(γt+1 −γt) (γt+1 −γt)′ , vQ = vQ + T.
(75)
23See Appendix B for a (very) brief introduction to state space models.
52

Algorithm 12 Gibbs sampler for the TVP-VAR
For the TVP-VAR (69), (71) and the priors (72), 73 select starting values Ψ(0) and Q(0).
For j = 1, . . . , B + R
1. Draw γ(j)
T
from the full conditional posterior γT|yT, Ψ(j−1), Q(j−1) ∼N
 sT|T, PT|T

obtained from the Kalman ﬁlter (equation (126) in Appendix B). For t = T −1, . . . , 1
draw γ(j)
t
from the full conditional γt|yT, Ψ(j−1), Q(j−1),γ(j)
t+1 in (76) by running the
simulation smoother (Algorithm B.3 in Appendix B).
2. Draw Q(j) from the full conditional Q|γT(j) in (75).
3. Draw Ψ(j) from the full conditional Ψ|yT, γT(j) in (74).
4. If j > B
Generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
, for h = 1, . . . , H, generate γ(j)
T+h
from the state equation (71) and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i,T+h +
p
X
i=h
y′
T+h−iA(j)
i,T+h + x′
T+hC(j)
T+h+u(j)′
T+h.
(77)
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
Generating γT from the full conditional posterior is somewhat more involved. The
basic idea is to make use of the linear state space structure.
Given Ψ and Q a run
of the (forward) Kalman ﬁlter (equation (126) in Appendix B24) produces the sequence
of conditional distributions γt|yt, Ψ, Q ∼N
 st|t, Pt|t

, t = 1, . . . , T. This gives the full
conditional posterior for the last state, γT|yT, Ψ, Q ∼N
 sT|T, PT|T

from which a draw
of γT can be made. To obtain the full conditional posterior for all the states we decompose
the joint distribution as p
 γT|yT, Ψ, Q

= p
 γT|yT, Ψ, Q
 T−1
Y
t=1
p
 γt|yT, Ψ, Q,γt+1

where
γt|yT, Ψ, Q,γt+1 ∼N
 st|T, Pt|T

(76)
and the moments are obtained from the backwards recursions in the simulation smoother
(Algorithm B.3 in Appendix B). Algorithm 12 summarizes the Gibbs sampler for the
TVP-VAR.
Specifying the prior
The random walk nature of the state equation (71) puts little
structure on the behavior of γt and the implied prior for the sequence of parameters,
γT, gets increasingly loose as the unconditional variance increases at the rate tQ. To
enforce some smoothness in γt it is useful to focus the prior for Q on small values of the
variances. In addition, the random walk nature can lead to explosive behavior at some
24Set Zt = Wt, Ht = Ψ, dt = 0, Tt = I and Qt = Q in the Kalman ﬁlter equations.
53

time points which can be undesirable if the data is believed to be stationary. To prevent
this Cogley and Sargent (2002, 2005) truncates the prior for γT to the region where γt,
∀t, is stationary. Truncated prior distributions like this are easily incorporated in a Gibbs
sampler, simply check the stationarity condition for all t at the end of step 1 of Algorithm
12 and redo step 1 if it fails for at least one time point.
It is common to use a training sample prior for the ﬁrst state, γ1. The ﬁrst, say k +20,
observations are set aside as a training sample and the prior mean and variance are based
on the OLS estimates using the training sample. The prior variance should, in general,
be relatively large so as not to make the prior too informative, see, for example Primiceri
(2005).
An alternative is to base the prior for γ1 on the Minnesota prior.
For this to be
eﬀective a modiﬁed state equation along the line of the Doan et al. (1984) speciﬁcation
(70) is useful. Generalizing this we can write the state equation as
γt+1 = s1|0 + Φ
 γt −s1|0

+ εt
(78)
with Φ a diagonal matrix. The state equation is stationary and mean reverting if |φii| < 1.
The diagonal elements can be taken as ﬁxed and speciﬁed along with the other prior
parameters or estimated. Inference on φii is straightforward in the latter case. Conditional
on γT(78) is just a multivariate regression and it is easy to add a block to the Gibbs
sampler drawing from the full conditional posterior for Φ.
Forecasting performance
Sims (1993) reports on the enhancements made to the orig-
inal Litterman forecasting model where he allows for conditional heteroskedasticity and
non-normal errors in addition to the time varying regression parameters. The result of
these modiﬁcations and the addition of three more variables, the trade weighted value of
the dollar, the S&P 500 stock index and the commodity price index, led to an improved
forecasting performance for the price variable, comparable or slightly better forecasts for
the real variables and slightly worse forecasts for interest rates compared to the original
Litterman model.
Canova (2007) forecasts the inﬂation rate of the G7 countries using a range of mod-
els. First there is a set of country speciﬁc models, univariate ARMA, several bivariate
VARs with the additional variable suggested by theory, trivariate VARs where the two
additional variables are selected to minimize the in sample mean square error for inﬂa-
tion. The trivariate VAR is estimated by OLS, as a BVAR with Minnesota style prior
beliefs and also as a Bayesian TVP-VAR and a Bayesian TVP-AR with mean revert-
ing state equations (70). Canova also use several ”international” models, three variables
controlling for international demand are added as predetermined variables to the coun-
try speciﬁc BVARs, a TVP-BVAR for the 7 inﬂation rates with the same international
variables as predetermined, a Bayesian panel VAR and a dynamic factor model for the
inﬂation rates where the factors are principal components of the variables in the panel
VAR. All the models are formulated in terms of direct rather than iterated forecasts, i.e.
yt+h = φyt + ut+h for an AR(1). The models diﬀer in two important dimensions, the rich-
ness of the information set and the ﬂexibility of the speciﬁcation. Overall the model with
the largest information set and the most general speciﬁcation, the Bayesian panel VAR
does best. Comparing models with similar information sets, a BVAR improves on a VAR
54

estimated with OLS and time-varying parameters improve the forecasts for univariate
models but not for the BVARs.
Clark and McCracken (2010) use a real time data set and forecasts the US inﬂation,
interest rate and output using a wide range of trivariate VAR models based on diﬀerent
approaches to allowing for structural change. This includes models in levels, in diﬀerences,
estimated on rolling windows of the data, estimated by OLS or as BVARs with Minnesota
type priors and TVP-BVARs with random walk state equations. While the focus is on
diﬀerent methods for combining forecasts and how well they cope with structural changes
Clark and McCracken (2010) do report some results for individual models. Of these a
BVAR with detrended inﬂation does best and, while not directly comparable, considerably
better than a TVP-BVAR where inﬂation has not been detrended.
7.2
Stochastic volatility
The constant error variance assumption can also be questioned, especially in light of the
so called ”great moderation” with considerable lower variability in key macroeconomic
variables since the mid 1980s. It can also be diﬃcult to empirically distinguish between
a model with constant parameters and time-varying variances and a model with time-
varying parameters and constant error variance.
It can thus be prudent to allow for
both. Cogley and Sargent (2005) and Primiceri (2005) construct TVPSV-VAR models
by adding stochastic volatility to the TVP-VAR. The setup in Primiceri (2005) is more
general and the overview here is based on Primiceri. See Koop and Korobilis (2009) for
a more in-depth discussion of stochastic volatility in VAR models.
To introduce time-varying volatilities and correlations we decompose the error variance
matrix into
Ψt = L−1
t DtL−T
t
where Dt is a diagonal matrix, Dt = diag (d1t, . . . , dmt) with a stochastic volatility speci-
ﬁcation,
dit = exp (hit/2) ,
(79)
hi,t+1 = µi + φi (hit −µi) + ηit
where ηt = (η1t, . . . , ηmt) is iid N (0, Vη). The purpose of Lt is to allow for an arbitrary
time-varying correlation structure. It is a lower triangular matrix with 1 on the diagonal,
e.g. for m = 3
Lt =


1
0
0
l21,t
1
0
l31,t
l32,t
1

,
(80)
where the time-varying elements under the diagonal follow a random walk,
lt+1 = lt + ζt
(81)
for lt a m (m −1) /2 vector that collects lij,t, i > j in row major order, ζt iid N
 0, Vζ

and Vζ block diagonal with blocks corresponding to the rows of Lt.
The triangular
speciﬁcation (80) is convenient and can also be interpreted as a structural VAR with
time-varying parameters.
55

Prior speciﬁcation
Prior distributions for the parameters µi, φi, Vη and Vζ are needed
in order to complete the model. µi is the unconditional expectation of the log volatilities
and in absence of speciﬁc information about the scale of the parameters a noninforma-
tive normal prior can be used, µ ∼N
 0, Σµ

with Σµ diagonal. For the autoregressive
parameter it is common to restrict this to the stationary region and specify a trun-
cated normal, φ ∼N
 φ, Σφ

I (|φi| < 1) with Σφ diagonal. Alternatively one can, as
in Primiceri (2005), work with a random walk speciﬁcation for hit with φi = 1 and
where µi drops out of the model. For the state equation variance Vη an inverse Wishart
prior, Vη ∼iW
 Sη, vη

, is conditionally conjugate and convenient. For the log volatil-
ities an initial condition (prior) is needed. With φi restricted to the stationary region,
hi1|µi, φi, σ2
ηi ∼N
 µi, σ2
ηi/ (1 −φ2
i )

for σ2
ηi the ith diagonal element of Vη, is a natural
choice, and with φi = 1 a noninformative normal distribution can be used for the initial
condition.
Vζ is assumed to be block diagonal in order to simplify the posterior sampler (Primiceri
(2005) shows how to relax this). With a block diagonal structure the prior for the blocks
Vζ,i, i = 2, . . . , m, can be speciﬁed with independent inverse Wishart distributions, Vζ,i ∼
iW
 Sζ,i, vζ,i

. In addition, an initial condition is needed for the elements of L1 collected
in l1. For simplicity, this can be taken as a noninformative normal distribution. For some
additional simpliﬁcation, Vη and Vζ can be speciﬁed as diagonal matrices with inverse
Gamma priors for the diagonal elements.
The exact choices for the parameters of the prior distribution can be based on a
training sample as for the TVP-VAR model, see Primiceri (2005) for an example.
Sampling from the posterior
When discussing inference on the variance parameters
Lt, Vζ, Dt, µi, φi and Vη we condition on the other parameters in the model and simply
take eyt = yt −Wtγt as our data. This implies that the (conditional) inference procedure
for the variance parameters does not depend on if the other parameters are time-varying
or constant. It will consist of a few blocks of a Gibbs sampler that can be combined with
a MCMC sampler for a VAR with constant or time-varying parameters. The inference
procedure for the remaining parameters is, on the other, aﬀected by the introduction
of time-varying variances. For the TVP-VAR this amounts to noting that Ψt (Ht in
the Kalman ﬁlter equations (126) in Appendix B) is now time-varying. The constant
parameter case can also be handled with the help of the Kalman ﬁlter by setting Qt= 0,
dt = 0 and Tt = I in addition to allowing for time-varying variances in the Kalman
ﬁlter. By setting Qt to zero the parameter variation is shut down and γt = γ1, ∀t, is
enforced. The prior (72) for γ1 is then a prior for the constant parameter. After running
the Kalman ﬁlter the (conditional) posterior mean and variance of γ is returned as sT|T
and PT|T and no smoothing is necessary. The conditional posterior for a constant γ can
of course also be derived analytically. Write the constant parameter VAR as
yt = Wtγ + ut
56

with ut ∼N (0, Ψt) and Wt = Im ⊗z′
t. With an independent normal prior γ ∼N
 γ, Σγ

as in section 3.2.2 the conditional posterior is
γ|YT, ΨT ∼N
 γ, Σγ

(82)
Σγ =
 
Σ−1
γ
+
T
X
i=1
W′
tΨ−1
T Wt
!−1
,
γ = Σγ
"
Σ−1
γ γ +
 T
X
i=1
W′
tΨ−1
T Wt
!
bγ
#
where bγ is the GLS estimate bγ =
PT
i=1 W′
tΨ−1
T Wt
−1 PT
i=1 W′
tΨ−1
T yt.
Turning to the conditional posteriors for the variance parameters, the one for the
correlation parameters in Lt is relatively straightforward and replicates the treatment
of time-varying parameters in the TVP-VAR. Multiplying each observation with Lt we
obtain Lteyt = Ltut = et with V (et) = Dt. This yields m −1 uncorrelated equations in a
triangular equation system,
eyit = −
i−1
X
j=1
eyjtlij,t + eit, i = 2, . . . , m.
(83)
This, together with the assumption that Vζ is block diagonal, means that the full condi-
tional posterior for Lt, t = 1, . . . , T can be recovered by running the corresponding Kalman
ﬁlter and simulation smoother for each equation in turn, setting Zt = (ey1t, . . . , eyi−1,t) ,
Ht = exp (hit) , dt = 0, Tt = I and Qt to the relevant block of Vζ in the Kalman ﬁlter
equations.
The posterior for Vζ is straightforward conditional on Lt. With the block diagonal
structure the blocks are inverse Wishart
Vζ,i|lT ∼iW
 Sζ,i, vζ,i

, i = 2, . . . , m
(84)
Sζ,i = Sζ,i +
T
X
t=1
(li,t −li,t−1) (li,t −li,t−1)′ , vζ,i = vζ,i + T
for li,t = (li1,t, . . . , li,i−1,t)′ .
The posterior analysis of the time-varying volatilities is complicated by the fact that
the observation equation is non-linear in the states hi,t.Let y∗
t = Lteyt, we then have
y∗
it = exp (hit/2) vit
where vit is iid N (0, 1) . Squaring and then taking logarithms yields
y∗∗
it = hit + v∗
it
where y∗∗
it
= ln

(y∗
it)2 + c

for c a small positive constant and with v∗
it = ln v2
it dis-
tributed as the logarithm of a χ2
1 random variable.
Kim, Shephard and Chib (1998)
show that the distribution of v∗
it is well approximated by a mixture of normals, p (v∗
it) ≈
P7
j=1 qjN
 v∗
it; mj −1.2704, τ 2
j

. The mixture coeﬃcients obtained by Kim et al. (1998)
57

are reproduced in Table 1.
By introducing a latent indicator variable δit for which
component in the mixture v∗
it has been drawn from, the mixture can be rewritten as
v∗
it|δit = j ∼N
 mj −1.2704, τ 2
j

with P (δit = j) = qj the problem can thus be trans-
formed into a linear and normal ﬁltering problem conditional on δit. Kim et al. (1998)
develops an MCMC algorithm for sampling from the posterior distribution of the states
hit.
Conditional on the sequence of states, δ1, . . . , δT, we can, using the notation of Ap-
pendix B, write the stochastic volatility part of the model as a linear state space system,
y∗∗
t = Zt
 1
ht

+ e∗
t
(85)
ht+1 = d + Tht + η,
for Zt = (mt, Im) , di = µi (1 −φi) , T = diag (φi) . The elements of mt are the conditional
means E (v∗
it|δit = j) = mj −1.2704 and e∗
t ∼N (0, Ht) for Ht diagonal with diagonal
elements given by the conditional variances, V (v∗
it|δit = j) = τ 2
j . Running the Kalman
ﬁlter and then the simulation smoother on (85) yields a draw of hit, i = 1, . . . , m, t =
1, . . . , T from the full conditional posterior.
The full conditional posterior for Vη is straightforward as an inverse Wishart,
Vη|hT, µ, φ ∼iW
 Sη, vη

, Sη = Sη+
T
X
t=1
(ht −d −Tht−1) (ht −d −Tht−1)′ , vη = vη+T.
(86)
The states, δit, can be sampled from the full conditional posterior
p (δit = j|y∗∗
it , hit) ∝qj
1
τj
exp
  y∗∗
i,t −mj −1.2704 −hit
2
2τ 2
j
!
.
(87)
For φi and µi, ﬁnally, write
h∗
t = ht −µ = Xtφ + η
for Xt = diag (hi,t−1 −µi) . Stacking the observations and performing the usual calcula-
tions yields the full conditional posterior for φ as
φ|hT, µ, Vη ∼N
 φ, Σφ

I (|φi| < 1)
(88)
Σφ =
 
Σ−1
φ +
T
X
t=1
X′
tV−1
η Xt
!−1
φ = Σφ
 
Σ−1
φ φ +
T
X
t=1
X′
tV−1
η h∗
t
!
.
The full conditional posterior for µ is obtained in a similar fashion as
µ|hT, φ, Vη ∼N
 µ, Σµ

(89)
Σµ =
 Σ−1
µ + TX′V−1
η X
−1
µ = Σµ
 
Σ−1
µ φ + X′V−1
η
T
X
t=1
h∗∗
t
!
.
58

Table 1 Normal mixture coeﬃcient for ln χ2
1
Component, δ
qj
mj
τ 2
j
1
0.00730
−10.12999
5.79596
2
0.10556
−3.97281
2.61369
3
0.00002
−8.56686
5.17950
4
0.04395
2.77786
0.16735
5
0.34001
0.61942
0.64009
6
0.24566
1.79518
0.34023
7
0.25750
−1.08819
1.26261
Source: Kim et al. (1998)
by writing
h∗∗
t = ht −diag (φ) ht−1 = Xµ + η
for X = diag (1 −φi) .
The steps of the resulting Gibbs sampler are summarized in algorithm 13.
Forecast performance
Clark (2011) studies point and density forecasts using real time
data on the US output growth, unemployment rate, inﬂation and federal funds rate. With
a focus on the eﬀects of changing data variability motivated by the possible end of the
great moderation Clark introduces a new model, the steady state BVAR of Villani (2009)
combined with stochastic volatility following the approach of Cogley and Sargent (2005).
The forecast performance of the new model is compared to univariate AR-models with
and without stochastic volatility, a standard Minnesota type BVAR with a normal-diﬀuse
prior and a Minnesota type steady state BVAR. The BVAR includes the four variables
to forecast whereas the steady state BVARs includes the detrended unemployment rate,
inﬂation and the interest rate less the long run inﬂation expectation and the long run in-
ﬂations expectation as an additional variable. The three BVAR variants are estimated on
both a recursively updated and rolling data window. When the point forecasts are evalu-
ated by their RMSEs, the BVARs generally do worse than the benchmark univariate AR
without stochastic volatilities at shorter lead times (1 and 2 quarters) but improve on the
benchmark for lead times of 1 and 2 years with the exception for the forecasts of inﬂation.
The steady state BVAR tend to do better than the standard BVAR and adding stochastic
volatility brings further improvements. In addition, the rolling updating scheme with an
80 observation window tends to produce better forecasts than recursive updating. The
density forecasts are evaluated using several criteria. In terms of the empirical coverage of
the prediction intervals it is found that the models without stochastic volatility produces
to wide intervals while the models with stochastic volatility produces better calibrated
prediction intervals. When evaluated using the probability integral transform (PIT, see
Corradi and Swanson (2006)) the hypothesis of a correctly speciﬁed predictive distribu-
tion is rejected in almost all cases for models without stochastic volatility whereas the
stochastic volatility models pass the test with only a few exceptions. Finally, evaluating
the density forecasts using the log predictive density score, the stochastic volatility models
do considerably better at the shorter lead times while the diﬀerences are quite small for
the longer lead times. Similarly the steady state BVAR outperforms the standard BVAR
at short lead times and the steady state BVAR with stochastic volatility outperforms the
59

Algorithm 13 Gibbs sampler for VAR with stochastic volatility
For the VAR with stochastic volatility select starting values ΨT(0) = Ψ(0)
1 , . . . , Ψ(0)
T , V(0)
ζ,i ,
i = 2, . . . , m, µ(0), φ(0), V(0)
η
and δT(0).
For j = 1, . . . , B + R
1. Draw the regression γ(j) from the full conditional posterior γ|YT, ΨT(j−1) in (82)
for the constant parameter case or using the Kalman ﬁlter and simulation smoother
for the time-varying parameter case as in Algorithm 12.
2. For i = 2, . . . , m run the Kalman ﬁlter for the observation equation (83) and
state equation 81) and generate l(j)
i,T from the normal full conditional posterior
li,T|YT, V(j−1)
ζ,i
with parameters given by sT|T and PT|T from the Kalman ﬁl-
ter.
For k = T −1, . . . , 1 draw l(j)
i,t from the normal full conditional posterior
l(j)
i,t |yt, V(j−1)
ζ,i
, l(j)
i,t+1 obtained from the simulation smoother.
3. For i = 2, . . . , m, draw V(j)
ζ,i from the full conditional posterior Vζ,i|lT(j) in (84).
4. Draw
the
log
volatilities
h(j)
T
from
the
normal
full
conditional
posterior
hT|YT, µ(j−1), φ(j−1), V(j−1)
η
, δT(j−1) with parameters sT|T and PT|T obtained by run-
ning the Kalman ﬁlter for the state space system (85). For t = T −1, . . . , 1 draw h(j)
t
from the normal full conditional posterior hT|yt, µ(j−1), φ(j−1), V(j−1)
η
, δT(j−1), h(j)
t+1
with parameters obtained from the simulation smoother.
5. For i = 1, . . . , m, t = 1, . . . , T draw the states δ(j)
it from the full conditional posterior
δij|y∗∗
it , hit in (87).
6. Draw V(j)
η
from the full conditional posterior Vη|hT(j), µ(j−1), φ(j−1) in (86).
7. Draw φ(j) from the full conditional posterior φ|hT(j), V(j)
η , µ(j−1) in (88).
8. Draw µ(j) from the full conditional posterior µ|hT(j), V(j)
η , φ(j) in (89).
9. If j > B
For h = 1, . . . , H, generate γ(j)
T+h from the state equation if γ is time varying,
generate l(j)
i,T+h from (81) and h(j)
T+h from (79), form Ψ(j)
T+h and generate u(j)
T+h from
uT+h ∼N

0, Ψ(j)
T+h

and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i,T+h +
p
X
i=h
y′
T+h−iA(j)
i,T+h + x′
T+hC(j)
T+h+u(j)′
T+h.
(90)
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
60

steady state BVAR at shorter lead times.
Using a real time data set D’Agostino, Gambetti and Giannone (forthcoming) fore-
casts the US unemployment rate, inﬂation and a short term interest rate. The aim of the
forecasting exercise is to investigate how important it is to allow for time-varying param-
eters and stochastic volatility. Forecasts are thus made with a number of models which
incorporate these futures to a varying degree: A univariate TVPSV-AR model, SV-AR
and SV-VAR models, standard AR and VAR models estimated using recursive and rolling
data windows and a TVPSV-VAR using the speciﬁcation of Primiceri (2005). The infer-
ence is Bayesian for all the models and the prior beliefs are based on the Minnesota prior.
Overall the TVPSV-VAR does best both in terms of point forecasts and density fore-
casts. The SV-AR and SV-VAR models improve on their constant variance counterparts
and D’Agostino et al. (forthcoming) concludes that there is a role for both time-varying
parameters and time-varying error variances when forecasting these variables.
8
Model and variable selection
Model speciﬁcation in VAR models essentially consists of two questions: Which variables
should be modelled (included in yt) and, given the content of yt, how many lags of yt
should be included? The answer to the ﬁrst question obviously depends on the objective
of the exercise which mandates the inclusion of some variables (e.g. the variables to be
forecasted) but there is usually also a secondary set of variables where the choice is not so
clear cut. These are variables that are not of primary interest but could be included in the
model if it leads to a better speciﬁed model that improves the forecasts of the variables
of interest or clearer inference by avoiding omitted variable bias.
The Litterman prior tries to answer the second question in its general form, how many
lags, by making the prior tighter around zero for coeﬃcients on larger lags and thus allow-
ing for a comfortably large lag length while reducing the risk of overﬁtting. The question
can, however, be made more speciﬁc: Which lags of which variables should be included
in each equation? This opens up for a huge number of diﬀerent model speciﬁcations and
requires new tools.
8.1
Restricting the parameter matrices - SSVS
George, Sun and Ni (2008) considers soft and hard restrictions on the parameters of
the VAR as a means of reducing the eﬀects of overparameterization and sharpening the
inference. This has some similarity with the structural VAR models discussed in section
4 but can also bee seen as a matter of selecting which lags of which dependent variable to
include in the model. In contrast with the SVAR approach the restrictions are determined
by the data rather than economic theory and are applied to a mixture of reduced form
and structural form parameters. Taking the reduced form VAR
y′
t =
p
X
i=1
y′
t−iAi + x′
tC + u′
t = z′
tΓ + u′
t
as the starting point George et al. (2008) considers restrictions on Γ and the Cholesky
factor Λ of the inverse variance matrix of ut, Ψ−1 = ΛΛ′. The Λ-matrix plays the
61

same role as in the SVAR and restrictions on Λ can, in contrast to restrictions on Γ, be
given a structural interpretation. Allowing for zero or ”near-zero” restrictions on arbitrary
parameters there is vast number of combinations of restrictions or models, 2mk+m(m−1)/2, to
consider. It is clearly impossible to evaluate all of them and George et al. (2008) proposes
a stochastic search variable selection (SSVS) procedure that will focus on the restrictions
with empirical support. The SSVS (George and McCulloch (1993)) is a MCMC algorithm
for simulating from the joint posterior distribution of the set of restrictions (or models) and
parameters based on a speciﬁc form of hierarchical prior distribution. For the regression
coeﬃcients γij let δij be an indicator variable, the prior conditional on δij is then γij ∼
N

γijδij, h2
ij

for
hij =
 τ0,ij if δij = 0
τ1,ij if δij = 1
(91)
with τ0,ij ≪τ1,ij. The idea being that the prior shrinks aggressively towards zero if δij = 0
(or imposes γij = 0 if τ 2
0,ij = 0) and allows for a non-zero γij if δij = 1 by setting τ1,ij
relatively large. The hierarchical prior is completed by specifying independent Bernoulli
priors for δij, P (δij = 1) = pij, where pij reﬂects the strength of the prior belief that γij
diﬀers from zero in a meaningful way. Note that a parameter/variable can be forced into
the model by setting pij to 1. For convenience (and to allow for prior correlation) we
write the prior as a multivariate normal distribution for γ = vec (Γ) ,
γ|δ ∼N
 Dγ, HRH

,
(92)
where D = diag (δ11, δ21, . . . , δkm) , H = diag (h11, h21, . . . , hkm) and R is a known corre-
lation matrix. With prior independence between Γ, δ and Ψ (or Λ) the full conditional
posterior for γ is standard and has the same form as with the independent normal-Wishart
prior,
γ|YT, Λ,δ ∼N
 γ, Σγ

(93)
Σγ =

(HRH)−1 + ΛΛ′ ⊗Z′Z
−1
γ = Σγ

(HRH)−1 Dγ + vec (Z′YΛΛ′)

.
George et al. (2008) gives the full conditional posterior for δij as a Bernoulli distribution,
P (δij = 1|YT, Γ, Λ,δ−ij) =
u1,ij
u1,ij + u0,ij
(94)
u1,ij = π (Γ|δ−ij, δij = 1) pij
u0,ij = π (Γ|δ−ij, δij = 0) (1 −pij)
where π (Γ|·) is the prior distribution (92).
The simple form follows since, with the
hierarchical prior structure, δij is independent of the data once we condition on Γ (see
George and McCulloch (1993)). If, as is frequently the case, the prior for γ is speciﬁed
with no correlation between the elements, R = I, the expressions simplify further and we
62

have
u1,ij =
1
τ1,ij
exp


−

γij −γij
2
2τ 2
1,ij


pij
u0,ij =
1
τ0,ij
exp

−γ2
ij
2τ 2
0,ij

(1 −pij) .
To facilitate similar selection among the oﬀ-diagonal elements of Λ, collect these in
vectors ηj = (λ1j, . . . , λj−1,j)′ , j = 2, . . . , m, (George et al. (2008) works with Λ upper tri-
angular) and let ωj = (ω1j, . . . , ωj−1,j)′ be the corresponding indicators. The conditional
prior for ηj is then speciﬁed in the same fashion as the prior for γ,
ηj|ωj ∼N (0, GjRjGj)
(95)
with Gj = diag (g1j, . . . , gj−1,j) , for
gij =
 κ0,ij if ωij = 0
κ1,ij if ωij = 1 ,
with κ0,ij ≪κ1,ij, and Rj a known correlation matrix. As for δij, the prior for ωij is
speciﬁed as independent Bernoulli distributions with P (ωij = 1) = qij. For the diagonal
elements of Λ, λ = (λ11, . . . , λmm)′ , George et al. (2008) speciﬁes independent Gamma
distributions for the square of the diagonal as the prior,
λ2
ii ∼G (ai, bi) .
(96)
Note that an inverse-Wishart prior for Ψ can be obtained as a special case when there is
no selection of zero elements in Λ,i.e. qij = 1 ∀i, j, see Algorithm 20 for details.
In order to derive the full conditional posteriors George et al. (2008) rewrites the
reduced form likelihood (7) as
L (Y|Γ, Λ) ∝|det Λ|T exp

−1
2 tr [Λ′SΛ]

=
m
Y
i=1
λT
ii exp
(
−1
2
" m
X
i=1
λ2
iivi +
m
X
j=2
 ηj + λjjS−1
j−1sj
′ Sj−1
 ηj + λjjS−1
j−1sj

#)
where S = (Y −ZΓ)′ (Y −ZΓ) , Sj the upper left j × j submatrix of S, sj =
(s1j, . . . , sj−1,j)′ , v1 = s11 and vj = |Sj| / |Sj−1| = sjj −s′
jS−1
j−1sj for j = 2, . . . , m. It
is then easy to show that the conditional posteriors for ηj are independent and normal,
ηj|YT, Γ, ω, λ ∼N
 ηj, Σj

(97)
with
Σj =

(GjRjGj)−1 + Sj−1
−1
ηj = −Σjλjjsj,
63

and that the conditional posteriors for λ2
jj are independent Gamma distributions,
λ2
jj|YT, Γ, ω ∼G
 aj, bj

(98)
aj = aj + T/2
bj =
(
b1 + s11/2, j = 1
bj +

sjj −s′
jΣ
−1
j sj

/2, j = 2, . . . , m .
The full conditional posteriors for ωij, ﬁnally, are Bernoulli distributions,
P (ωij = 1|YT, Γ, Λ,ω−ij) =
v1,ij
v1,ij + v0,ij
(99)
v1,ij = π (ηj|ω−ij, ωij = 1) qij
v0,ij = π (ηj|ω−ij, ωij = 0) (1 −qij)
where π (ηj|·) is the prior distribution (95). If the elements of ηj are uncorrelated a prior
(Rj = I) the expressions simplify
v1,ij =
1
κ1,ij
exp

−η2
ij
2κ2
1,ij

qij
v0,ij =
1
κ0,ij
exp

−η2
ij
2κ2
0,ij

(1 −qij) .
Specifying the prior beliefs
The prior ”inclusion probabilities” determines the prior
expected model size (number of non-zero parameters) and inﬂuences how aggressively
the restrictions are applied. Setting pij = qij = 1/2 is a reasonable starting point but a
smaller value can be useful with large and richly parameterized models. There are usually
some parameters, such as the constant term, that should always be in the model. This
is achieved by setting the corresponding prior inclusion probability to 1. We might also
have substantive information about how likely it is that a parameter will contribute to
model ﬁt and forecast performance. In VAR models it could, for example, be useful to let
the inclusion probability pij decrease with the lag length in the spirit of the Minnesota
prior. The choice of inclusion probabilities for the variance parameters could be guided
by the same type of considerations that leads to restrictions in structural VAR models.
The prior variances τ0,ij and κ0,ij should be suﬃciently small to eﬀectively shrink
the parameter to zero when δij or ωij are zero.
The choice of τ1,ij and κ1,ij is more
diﬃcult. George et al. (2008) suggests a semiautomatic choice with τ0,ij = bσγij/10 and
τ1,ij = 10bσγijwhere bσγij is the standard error of the OLS estimate of γij in the unrestricted
model. Alternatively τ1,ij can be based on the Minnesota prior and set as in (14).
The correlation matrices R and Rj, j = 2, . . . , m, are usefully set to the identity
matrix unless there is substantial prior information about the correlation structure. It
is also standard practice in SSVS applications to set the prior means γij to zero when
δij = 1 in addition to when δij = 0. With VAR models it can be useful to deviate from
this and set the prior mean for the ﬁrst own lag to a non-zero value in the spirit of the
Minnesota prior.
If no restriction search is wanted for the regression parameters the prior for Γ reduces
to the independent normal prior in section 3.2.2. The prior for η and λ can be overly
64

complicated if no restriction search is to be conducted on η and these priors can usefully
be replaced by a Jeﬀreys’ prior or an inverse Wishart prior on Ψ as in section 3.2.2.
Simulating from the posterior
With the conditional posterior distributions in hand
it is straightforward to implement a Gibbs sampler (see Algorithm 14) for the joint poste-
rior distribution and the predictive distributions needed in forecasting applications. This
will eﬀectively conduct model averaging over the diﬀerent models implied by the restric-
tions and produces the model averaged posterior distribution. If the variable/restriction
selection is of interest the indicator variables δij and ωij will provide evidence on this.
The posterior probability that a parameter is non-zero can be estimated by averaging δij
and ωij (or, for a more precise estimate, average the posterior probabilities P (δij = 1)
and P (ωij = 1) in (94) and (99)) over the output of the sampler.
Note that the sampler in Algorithm 14 will not converge to the joint posterior if hard
restrictions (τ0,ij = 0 or κ0,ij = 0) are used and will converge very slowly if the ratios
τ1,ij/τ0,ij or κ1,ij/κ0,ij are very large. The MCMC algorithm suggested by Geweke (1996b)
is a better choice in these cases. Korobilis (forthcomingb) suggests a convenient algorithm
for the case with hard restrictions on the regression parameters and no restriction search
on the variance parameters.
Forecast performance
Korobilis (2008) applies the SSVS in a forecasting exercise
where the base model is a VAR with eight US macroeconomic variables which is augmented
with an additional 124 exogenous variables that are entered into the model in the form of
their principal components. He ﬁnds that the SSVS model averaged predictions, as well
as the predictions from the ”median model” (i.e. the model containing the variables with
posterior inclusion probabilities greater than 0.5, Barbieri and Berger (2004)), improves
on the forecasts from OLS estimated VARs without the additional variables and model
selection using BIC, the Bayesian information criteria of Schwarz (1978).
Jochmann, Koop and Strachan (2010) extends the SSVS restriction search to VAR
models with Markov switching to allow for structural breaks and conducts a forecasting
exercise comparing models allowing for diﬀerent combinations of restriction searches and
breaks in the regression and variance parameters. Using a 4 lag VAR with US unemploy-
ment, interest rate and inﬂation they ﬁnd that the restriction search which eﬀectively sets
a large number of parameters to zero results in improved forecasts compared to BVARs
with a ”loose” prior (obtained by forcing δij = 1 and ωij = 1 for all parameters in the
SSVS prior) and a Minnesota prior. Allowing for structural breaks also improves on per-
formance in combination with SSVS if only a subset (either Γ or Λ) of the parameters
are allowed to change.
Korobilis (forthcomingb) considers SSVS in a richer class of multivariate time series
models than just linear VAR models but limits the restriction search to the conditional
mean parameters Γ and consider hard rather than soft restrictions, corresponding to
τ0,ij = 0 in (91). In a forecasting exercise where the aim is to forecast UK unemployment,
interest rate and inﬂation Korobilis uses a range of models, allowing for structural breaks
or time varying parameters, and prior speciﬁcations. The general conclusion is that the
restriction search does improve forecast performance when the prior is informative, the
model is richly parameterized or quite large.
65

Algorithm 14 Gibbs sampler for stochastic restriction search (SSVS)
For the priors (92), (95), (96) and independent Bernoulli priors on δij and ωij the fol-
lowing Gibbs sampler (George et al. (2008)) can be used to simulate the joint posterior
distribution of Γ, Λ, δ and ω. Select starting values γ(0), δ(0), η(0) and ω(0).
For j = 1, . . . , B + R
1. Generate
λ(j)
by
drawing
λ2
ii,
i
=
1, . . . , m
from
the
full
conditional
λ2
ii|YT, γ(j−1), ω(j−1) ∼G
 aj, bj

in (98).
2. Generate η(j)
i , i = 2, . . . , m from the full conditional ηi|YT, γ(j−1), ω(j−1), λ(j) ∼
N
 ηj, Σj

in (97)
3. Generate ω(j)
ik , i
=
1, . . . , k −1, k
=
2, . . . , m from the full conditional
ωik|η(j)
k , ω(j)
1k , . . . ω(j)
i−1,k, ω(j−1)
i+1,k, . . . , ω(j−1)
k−1,k ∼Ber (v1,ik/ (v1,ik + v0,ik)) in (99).
4. Generate γ(j) from the full conditional γ|YT, η(j), λ(j),δ(j−1) ∼N
 γ, Σγ

in (93).
5. Generate δ(j)
il , i = 1, . . . , k, l = 1, . . . , m from the full conditional posterior
δil|γ(j), δ(j)
11 , . . . δ(j)
i−1,l, δ(j−1)
i+1,l , . . . , δ(j−1)
mk
∼Ber (u1,il/ (u1,il + u0,il)) in (94)
6. If j > B form Ψ(j) from η(j) and λ(j), generate u(j)
T+1, . . . , u(j)
T+H from ut ∼
N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
If there is no restriction search on the elements of η, the priors (95) and (96) can be
replaced by a Jeﬀreys’ prior (23) for Ψ or an inverse Wishart, Ψ ∼iW (S, v) . Steps 1-3 can
then be replaced by a draw from the full conditional posterior Ψ|YT, Γ(j−1) ∼iW
 S, v

with parameters given in (25) or (26).
If there is no restriction search on the elements of Γ, the prior (92) reduces to an inde-
pendent normal prior that does not depend on δ and step 5 can be omitted.
66

8.2
Selecting variables to model
The standard Bayesian approach to model selection is based on the marginal likelihood
(4) and runs into problems when the issue is which variables to include as dependent
variables in a multivariate model. The likelihoods are simply not comparable when vari-
ables are added to or dropped from yt. In forecasting applications there is an additional
consideration. We are, in general, not interested in how well the model as a whole ﬁts the
data only in how well it forecasts a core set of variables of interest. Other variables are
then only included if they are expected to improve the forecast performance.
8.2.1
Marginalized predictive likelihoods
Andersson and Karlsson (2009) suggests replacing the marginal likelihood with the pre-
dictive likelihood for the variables of interest, that is after marginalizing out the other
variables, in the calculation of posterior ”probabilities” or model weights. This creates
a focused measure that can be used for model selection or forecast combination and is
attractive in a forecasting context since it directly addresses the forecasting performance
of the diﬀerent models
The predictive likelihood approach is based on a split of the data into two parts, the
training sample, Y∗
n = (y′
1, y′
2, . . . , y′
n)′ of size n, and an evaluation or hold out sample,
eYn =
 y′
n+1, y′
n+2, . . . , y′
T
′ of size T −n. The training sample is used to convert the
prior into a posterior and the predictive likelihood for the hold out sample is obtained by
marginalizing out the parameters from the joint distribution of data and parameters,
p

eYn
 Y∗
n,Mi

=
Z
L

eYn
 θi, Y∗
n, Mi

p (θi| Y∗
n, Mi) dθi.
Partitioning the hold out sample data into the variables of interest and the remaining
variables, eYn =

eY1,n, eY2,n

, the marginalized predictive likelihood for the variables of
interest is obtained by marginalizing out eY2,n,
MPL

eY1,n
 Y∗
n,Mi

=
Z
p

eYn
 Y∗
n,Mi

d eY2,n.
Predictive weights that can be used for model averaging or model selection are then
calculated as
w

Mi| eY1,n, Y∗
n

=
MPL

eY1,n
 Y∗
n,Mi

p (Mi)
PM
j=1 MPL

eY1,n
 Y∗
n,Mj

p (Mj)
(100)
where MPL

eY1,n
 Y∗
n,Mi

is evaluated at the observed values of the variables of interest
in the hold out sample.
While the predictive weights (100) strictly speaking can not be interpreted as posterior
probabilities they have the advantage that proper prior distributions are not required for
the parameters. The predictive likelihood is, in contrast to the marginal likelihood, well
deﬁned as long as the posterior distribution of the parameters conditioned on the training
sample is proper.
67

The use of the predictive likelihood is complicated by the dynamic nature of VAR
models. As noted by Andersson and Karlsson (2009) the predictive likelihood is the joint
predictive distribution over lead times h = 1 to T −n. This will become increasingly
uninformative for larger lead times and unrepresentative of lead times such as h = 4 or 8
usually considered in macroeconomic forecasting. At the same time the hold out sample
needs to be relatively large in order to provide a sound basis for assessing the forecast
performance of the models. To overcome this Andersson and Karlsson suggested focusing
the measure to speciﬁc lead times h1, . . . , hk and using a series of predictive likelihoods,
g

eY1,n|Mi

=
T−hk
Y
t=n
MPL (y1,t+h1, . . . , y1,t+hk| Y∗
t ,Mi) ,
(101)
in the calculation of the predictive weights.
A ﬁnal complication is that the predictive likelihood is not available in closed form
for lead times h > 1 and must be estimated using simulation methods. With a normal
likelihood the predictive likelihood for a VAR model will be normal conditional on the
parameters and easy to evaluate. Andersson and Karlsson suggested estimating the mul-
tiple horizon marginalized predictive likelihood using a Rao-Blackwellization technique
as
\
MPL (y1,t+h1, . . . , y1,t+hk| Y∗
t ,Mi) = 1
R
R
X
i=1
p

y1,t+h1, . . . , y1,t+hk| Y∗
t ,Mi, θ(j)
i

by averaging the conditional predictive likelihood p (y1,t+h1, . . . , y1,t+hk| Y∗
t ,Mi, θi) over
draws, θ(j)
i , of the parameters from the posterior distribution based on Y∗
t . This leads to
estimated predictive weights
bw

Mi| eY1,n, Y∗
n

=
bg (Y,n|Mi) p (Mi)
PM
j=1 bg

eY1,n|Mj

p (Mj)
(102)
with
bg

eY1,n|Mi

=
T−hk
Y
t=n
\
MPL (y1,t+h1, . . . , y1,t+hk| Y∗
t ,Mi) .
The marginalized predictive likelihood procedure is thus in principle applicable to any
forecasting model, with any type of prior, as long as the likelihood is normal and it is
possible to simulate the posterior distribution of the parameters.
Forecasting performance
Andersson and Karlsson (2009) conducted a forecasting
exercise with the aim of forecasting US GDP growth and considered VAR models with
up to four variables selected from a set of 19 variables (including GDP). Compared to an
AR(2) benchmark forecast combination using the predictive weights (102) does better for
shorter lead times (up to 4 quarters) but is outperformed for lead times 5 to 8. Selecting
a single model based on the predictive weights does slightly worse than the forecast
combination for the shorter lead times but performs better and on par with the AR(2)
for the longer lead times.
68

8.2.2
Marginal likelihoods via Bayes factors
Jaroci´nski and Ma´ckowiak (2011) favours the marginal likelihood as a basis for model
comparison and notes that the question of wether a set of variables is useful for forecasting
the variables of interest can be addressed in a model containing all entertained variables.
To see this write the VAR model in terms of two sets of variables y1,t and y2,t where y1,t
contains the variables of interest and, possibly, some additional variables and y2,t contains
the remaining variables,
 y′
1,t, y′
2,t

=
p
X
i=1
 y′
1,t−i, y′
2,t−i

Ai + x′
tC + u′
t
with
Ai =
 Ai,11
Ai,12
Ai,21
Ai,22

partitioned conformably. The notion that y2,t is not useful for predicting y1,t (does not
Granger-cause y1,t) then corresponds to the block-exogeneity restriction that Ai,21 = 0,
∀i. If the restriction holds y1,t can be modelled as a function of its own lags and y2.t is not
needed. Each partition of the variables into y1,t and y2,t gives rise to a diﬀerent block-
exogeneity restriction and the idea of Jaroci´nski and Ma´ckowiak (2011) is to compute the
marginal likelihood for all the variables under the diﬀerent restrictions and base model
selection or model averaging on these in a standard fashion.
This approach overcomes the main problem with the marginal likelihood of comparing
apples with oranges when diﬀerent sets of left hand variables are considered. Unfortu-
nately, the marginal likelihood under the restrictions is rarely available in closed form.
The marginal likelihoods and posterior model probabilities can, however, be computed
indirectly by way of the Savage-Dickey density ratio (Dickey (1971)),
BFR,U =
m

Y|Ai,21 = 0

m (Y)
= pAi,21 (Ai,21 = 0|Y)
πAi,21 (Ai,21 = 0) ,
(103)
which relates the Bayes factor comparing the restricted model and the unrestricted model
to the ratio of the marginal posterior to the marginal prior for the restricted parameters
evaluated under the restriction. The second equality in (103) holds under the speciﬁc
condition,
πR (ΓR, Ψ) = πU

ΓR, Ψ|Ai,21 = 0

,
(104)
that the prior for the parameters in the restricted model equals the prior for the unre-
stricted model when conditioning on the restriction.
Jaroci´nski and Ma´ckowiak (2011) suggests a normal-Wishart prior (section 3.2.1) as
a prior for the unrestricted model and constructs the prior for the restricted models by
conditioning on the restriction to ensure that condition (104) holds.25 With the conjugate
normal-Wishart prior both the marginal prior and posterior in the unrestricted model will
be matricvariate-t distributions. Partition yt into the n1 variables y1,t and the n2 variables
25The resulting prior for the restricted model is not normal-Wishart and presumably diﬀerent from
what would be used when estimating a model for just y1,t.
69

y2,t and let P and Q be the matrices of dimension pn2 × k and m × n1 that selects the
rows and columns of Γ corresponding to Ai,21,



A1,21
...
Ap,21


= PΓQ.
For the prior we have (see Appendix C) PΓQ|Ψ ∼MNpn2,n1 (PΓQ, Q′ΨQ, PΩP′) and
PΓQ ∼Mtpn2,n1
 PΓQ, (PΩP′)−1 , Q′SQ,v −n2

using that Q′ΨQ ∼iW (Q′SQ, v −n2)
since Q is a selection matrix. An equivalent result holds for the posterior and the Bayes
factor can be obtained as
BFR,U =
n1
Y
i=1
Γ ((v −n2 + 1 −i) /2) Γ ((v −n2 + pn2 + 1 −i) /2)
Γ ((v −n2 + pn2 + 1 −i) /2) Γ ((v −n2 + 1 −i) /2)
(105)
×
|PΩP′|n1/2 |Q′SQ|−(v−n2)/2 Q′SQ+
 PΓP′′  PΩP
−1  PΓP′
−(v−n2+pn2)/2
PΩP′n1/2 Q′SQ
−(v−n2)/2 Q′SQ+ (PΓP′)′ (PΩP)−1 (PΓP′)
−(v−n2+pn2)/2 .
The posterior model probabilities can be obtained directly from the Bayes factors or via
the marginal likelihoods for the restricted and unrestricted models by noting that the
marginal likelihood for the unrestricted model is the matricvariate-t distribution given in
(20).
Jaroci´nski and Ma´ckowiak (2011) also considered models deﬁned by multiple block-
exogeneity restrictions where the Bayes factor is not available in closed form but can be
evaluated using Monte Carlo methods. While the Bayes factor (105) is easy to evaluate
the computations can be prohibitive if the number of considered variables is large –
especially if multiple block-exogeneity restrictions are considered – and Jaroci´nski and
Ma´ckowiak (2011) proposes a Markov chain Monte Carlo model composition, (MC)3,
scheme (Madigan and York (1995)) to identify the most promising models.
9
High Dimensional VARs
Most applications involve relatively small VAR models with up to 5 or 6 variables and
occasionally 10 or more variables. There are obvious reasons for this – the number of
parameters to estimate grows rapidly with m and p and can exhaust the information
in the data while the models get unwieldy and sometimes diﬃcult to interpret. There
are, however, occasions that more or less demands that a large number of variables are
modelled jointly. The earliest such example is perhaps panel studies, e.g. to forecast
regional economic development one could specify a small VAR-model for each region or
specify a joint VAR for the panel of regions that allows for interaction between the regions.
In this case it is not only the increased size of the model that contributes to the complexity,
there is also the need to take account of heterogeneity across regions and, perhaps, time,
see Canova and Ciccarelli(2004, 2009).
A second situation is the task of forecasting in a data-rich environment with ”wide”
data sets that can contain 100 or more variables with potential predictive content for the
variables of interest. This has typically been tackled with dynamic factor models (Stock
70

and Watson (2002), Forni, Hallin, Lippi and Reichlin (2003)) where the information in
the data is summarized by a few factors or by combining forecasts from small models
with diﬀerent combinations of predictor variables (see Stock and Watson (2006) for a
review). Recent studies do, however, indicate that large Bayesian VAR models can be
quite competitive.
VAR models for wide data sets face numerical challenges due to the sheer size of the
model, with m = 100 variables and p = 4 lags there are 40 000 parameters in Γ. OLS
estimation is still feasible provided that T > 400 since this ”only” involves inversion of the
400 × 400 matrix Z′Z although estimates will be very imprecise due to the large number
of parameters. Similarly, Bayesian analysis with a normal-Wishart prior beneﬁts from
the Kronecker structure and is computationally feasible while more general priors such
as the normal-diﬀuse or independent normal Wishart are faced with the inversion of a 40
000 × 40 000 matrix. The sheer size of the problems makes MCMC exercises impractical
and too time consuming with current desktop resources even if one ignores the issue of
numerical stability when solving high dimensional equation systems.26
In line with the dynamic factor model literature De Mol, Giannone and Reichlin (2008)
considers ”direct” univariate forecasting models of the form
yt+h = x′
tβh + ut+h
where xt contains (a large number of) variables believed to be useful when forecasting yt.
Compared to a truly dynamic speciﬁcation (e.g. a VAR) this has the advantage that there
is no need to forecast xt for lead times h > 1. The disadvantage is that the distribution
of the error term is more complicated, in general ut+h follows a MA(h −1) process and
that separate equations must be estimated for each lead time. In a small forecasting ex-
ercise with n = 131 potential predictors in xt they demonstrate that principal component
regression (that is a dynamic factor model) and Bayesian forecasts based on a normal
or double exponential prior for βh are viable methods for dealing with very large data
sets. When n is large there is a considerable risk of overﬁtting and, pragmatically, the
success of the Bayesian approach depends on applying an appropriate amount of shrink-
age in the prior. De Mol et al. (2008) analyses the behavior of the forecasts as both n
and T →∞under the assumption that the data can be described by a factor structure,
yt+h = f ′
tγ + et+h, xt = Λf t + ξt where ft contains the r common factors. They show
that the Bayes forecast yt (h) = x′
tβ for β the posterior mean of β with a normal prior
β ∼N (0,Σ) converges to the ”population forecast” f ′
tγ if the variance of ξt is small
relative to the contribution of the factors to the variance of xt and the prior variance for
β is chosen such that |Σ| = O
 1
nT 1/2+δ

, 0 < δ < 1/2. That is, the degree of shrinkage
should increase with both n and T in order to protect against overﬁtting.
Korobilis (forthcominga) use the same type of univariate direct forecasting model as
De Mol et al. (2008) to forecast 129 US marcroeconomic variables using the other 128
variables as explanatory variables. The forecasts are made using 5 diﬀerent hierarchical
26It should be made clear that no actual matrix inverse of this size is needed. The conditional posterior
for γ has the form γ|YT , Ψ ∼N
 γ, Σγ

with γ = A−1b and Σγ = A−1. γ can be calculated by
Cholesky decomposing A = C′C and then use forward and back substitution to solve the triangular
equations systems C′x = b and Cγ= x in turn. A draw from γ|YT , Ψ is obtained by generating a vector
of standard normals, z, and computing γ + C−1z =γ + ez where ez is obtained by solving Cez = z by back
substitution. This is much more numerically stable than straightforward inversion of the matrices and
also faster.
71

shrinkage priors where the hierarchical structure is used to allow the degree shrinkage to be
inﬂuenced by the data and speciﬁc to each explanatory variable. As a comparison forecasts
are also made with a dynamic factor model using the ﬁrst ﬁve principal components as
factors. Priors designed to mimic the LASSO and the Elastic Net are found to perform
best when the forecasts are compared using the mean absolute error while the dynamic
factor model performs best if the mean squared error criterion is used.
9.1
Factor augmented VAR
Bernanke, Boivin and Eliasz (2005) proposed the factor augmented VAR (FAVAR) as
a means of incorporating the information from a large number of variables in a VAR
in a parsimonious way. There are two basic assumption, that the data admits a factor
structure, xt = Λf t + ξt where the information in the n auxiliary variables in xt can be
represented by the r factors in ft with r ≪n and that the variables of interest, yt, and
the factors can be jointly modelled as a VAR
ey′
t =
 ft
yt
′
=
p
X
i=1
ey′
t−iAi+u′
t.
(106)
yt and xt and hence also ft are assumed to be stationary and we will work with demeaned
data so there is no constant term in the VAR. Bernanke et al. (2005) augments the
factor structure by allowing the variables of interest to be directly related to the auxiliary
variables
xt = Λfft + Λyyt + ξt
(107)
instead of only indirectly through the factors ft. Like any factor model (107) suﬀers from
a fundamental lack of identiﬁcation since any full rank rotation of the factors will leave
the model unaﬀected, e.g. Λfft =
 ΛfP−1
(Pf t) = Λf∗f ∗
t for P full rank. Bernanke et al.
(2005) show that the restrictions
Λf =
 Ir
Λf
∗

, Λy =
 0r×m
Λy
∗

together with the exact factor model assumption that Ξ = V (ξt) is diagonal is suﬃcient
for identiﬁcation. The restriction on Λf is just a normalization whereas the restriction
on Λy is substantial and implies that the ﬁrst r variables in xt does not respond contem-
poraneously to yt.
The key to inference in the FAVAR model is to recognize that it is a state space model
(see Appendix B) with (107) as the observation equation and (106) as the state equation.
The Kalman ﬁlter requires that the state equation has the Markov property, i.e. that it
is autoregressive of order 1, and we rewrite the state equation with an expanded state
vector
st+1 =





eyt+1
eyt
...
eyt−p+2




=







A′
1
A′
2
· · ·
A′
p−1
A′
p
I
0
0
0
0
I
I
0
...
...
...
0
0
I
0







st +





ut
0
...
0





(108)
= Tst + ηt
72

and include the observable yt in the observation equation
wt =
 xt
yt

=


 Ir
Λf
∗

 0r×m
Λy
∗

0
· · ·
0
0
Im
0
· · ·
0

st +
 ξt
0

(109)
= Zst + εt.
We make the usual assumption that the innovations ut and ξt are iid normal and inde-
pendent of each other, ut ∼N (0, Ψ) and ξt ∼N (0, Ξ) with Ξ diagonal.
Conditional on the parameters T, Z, Ψ and Ξ we can use the Kalman ﬁlter and the
simulations smoother to draw the latent factors from the full conditional posterior (see
Appendix B). Note that system matrices T, Z, H = V (εt) and Q = V (ηt) contains
a large number of zeros and the computations can be speeded up by taking account of
the structure of the matrices. Note that including yt as left hand side variables in the
observation equation carries yt through to the state vector. That is, st|t contains yt since
it is known at time t and st+1|t contains the minimum mean squared error prediction,
Et (yt+1) , of the unknown yt+1. The Kalman ﬁlter recursions need to be started up with
a prior for the ﬁrst state, s1 ∼N
 s1|0, P1|0

.
Having run the Kalman ﬁlter, the last state can be sampled from the full conditional
sT|yT, xT, Γ, Λ, Ξ, Ψ ∼N
 sT|T, PT|T

(110)
where Γ collects the autoregressive parameters, Γ′=
 A′
1, . . . , A′
p

and Λ =
 Λf, Λy
.
Note that it suﬃces to draw the factor fT since yt, t = 1, . . . , T is known. Reﬂecting this,
the variance matrix PT|T is also singular which would cause numerical problems trying to
draw yT. The remaining factors, ft, t = T −1, . . . , 1, can be drawn from the conditionals
ft|yT, xT, Γ, Ξ, Ψ, f t+1 ∼N
 st|T, Pt|T

using the simulation smoother. Algorithm B.3 in
Appendix B can, however, not be used directly due to the presence of yt and lags in the
state vector. To implement the simulation smoother we need the conditional distributions
st|yt, xt, Γ, Λ, Ξ, Ψ, ft+1 ∼N
 st|t,ft+1, Pt|t,ft+1

.
(111)
Using that eyt+1|t =

f ′
t+1|t, y′
t+1|t
′
∼N
 Γ′st|t, Γ′Pt|tΓ + Ψ

it is easy to see that the
recursions for the parameters of the conditional distributions becomes (see Kim and Nelson
(1999, p. 194-196))
st|t,ft+1 = st|t + Pt|tΓ′  Γ′Pt|tΓ + Ψ
−1  eyt+1 −Γ′st|t

(112)
Pt|t,ft+1 = Pt|t −Pt|tΓ
 Γ′Pt|tΓ + Ψ
−1 Γ′Pt|t
for t = T −1, . . . , 1. It is again suﬃcient to only draw the factor ft at each iteration.
Inference on the remaining parameters is standard conditional on the factors. The
state equation (106) is a standard VAR and draws from the full conditional posterior for
Ψ and Γ can be obtained using the results in section 3.2.1 with a normal-Wishart prior
and section 3.2.2 with an independent normal Wishart or normal-diﬀuse prior.
The observation equation (107) can be analyzed as n univariate regressions when Ξ is
diagonal. The identifying restrictions imply that we have
xit = fit + ξit, i = 1, . . . , r
xit = ey′
tλi + ξit, i = r + 1, . . . , n
73

where λ′
i is row i of Λ =
 Λf, Λy
. Let σ2
i be the variance ξit, the conjugate prior is of
the normal-Gamma form,
σ2
i ∼iG (ai, bi)
λi|σ2
i ∼N
 λi, σ2
i Vi

and the full conditional posteriors are given by
λi|yT, xT, f T, σ2
i ∼N
 λi, σ2
i Vi

, i = r + 1, . . . , n
(113)
Vi =

V−1
i
+ eY′ eY
−1
λi = Vi

V−1
i λi + eY′ eYbλi
−1
and
σ2
i |yT, xT, f T ∼iG
 ai, bi

(114)
ai =

ai + T/2, i = 1, . . . , r
ai + (T −r −m) /2, i = r + 1, . . . , n
bi =
(
bi + 1
2
PT
i=1 (xit −fit)2 , i = 1, . . . , r
bi + 1
2

x′
ixi + λ′
iV−1
i λi −λ
′
iViλi

, i = r + 1, . . . , n
where eY is the matrix of explanatory variables ey′
t and bλi is the OLS estimate bλi =

eY′ eY
−1 eY′xi.
Specifying the prior
It is diﬃcult to have information a priori about the latent factors
and the prior for the ﬁrst state, s1 ∼N
 s1|0, P1|0

, is best taken to be non-informative,
for example s1|0 = 0 and P1|0 = 5I. It is also diﬃcult to form prior opinions about the
factor loadings in Λf and Λy and non-informative priors are advisable, Bernanke et al.
(2005) sets λi = 0, Vi = I, ai = 0.001 and bi = 3. The prior for Γ and Ψ can be based
on the same considerations as a standard VAR while taking account of the stationarity
assumption.
Sampling from the posterior
A Gibbs sampler for the joint posterior distribution of
the factors and the parameters can be constructed running the simulation smoother for
the factors and sample the parameters from the full conditional posteriors, see Algorithm
15.
Forecasting performance
See discussion of Gupta and Kabundi (2010) in section 9.2
and discussion of Korobilis (2008) in section 8.1
9.2
Large BVARs
9.2.1
Reducing parameter uncertainty by shrinkage
Banbura et al. (2010) studies the forecast performance of large BVARs. In an application
to forecasting US non-farm employment, CPI and the Federal Funds Rate the performance
74

Algorithm 15 Gibbs sampler for the FAVAR model
For the FAVAR (106, 107) select starting values Γ(0), Ψ(0), Λ(0) and Ξ(0).
For j = 1, . . . , B + R
1. Draw
the
factor
f (j)
T
from
the
full
conditional
posterior
sT|yT, xT, Γ(j−1), Λ(j−1), Ξ(j−1), Ψ(j−1) in (110) obtained by running the Kalman
ﬁlter (126) in Appendix B. For t = T −1, . . . , 1 draw f (j)
t
from the full condition
posterior st|yt, xt, Γ(j−1), Λ(j−1), Ξ(j−1), Ψ(j−1), f (j)
t+1 in (111) obtained by running
the simulation smoother (112).
2. Draw Ψ(j) and Γ(j) from the conditional posteriors Ψ|yT, xT, Λ(j−1), Ξ(j−1), f (j) in
(19) and Γ|yT, xT, Λ(j−1), Ξ(j−1), Ψ(j), f (j) in (18) with a normal-Wishart prior or
Ψ|yT, xT, Γ(j−1), Λ(j−1), Ξ(j−1), f (j) in (25) and Γ|yT, xT, Λ(j−1), Ξ(j−1), Ψ(j), f (j) in
(24) with an independent normal Wishart prior.
3. For i = 1, . . . , n draw σ2(j)
i
from the full conditional posterior σ2
i |yT, xT, f (j) in (114)
and (for i > r) λ(j)
i
from the full conditional posterior λi|yT, xT, f (j), σ2(j)
i
in (113).
4. If j > B generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey∗(j)′
T+h =
h−1
X
i=1
ey∗(j)′
T+h−iA(j)
i
+
p
X
i=h
ey′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
Discarding the parameters yields
n
ey∗(j)
T+1, . . . ey∗(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution of the factors and the variables of interest y.
of 4 diﬀerent VAR models with 3, 7, 20 and 131 variables, all with 13 lags, is evaluated.
Based on the theoretical results of De Mol et al. (2008), Banbura et al. (2010) suggest
that the degree of shrinkage applied through the prior should increase with the size of
the model. Working with a normal-Wishart prior distribution with Minnesota type prior
beliefs, the overall scaling factor π1 in (21) determines the amount of shrinkage. Banbura
et al. sets this to match the in sample ﬁt of the smallest VAR estimated with OLS (i.e.
π1 = ∞) for the three variables of interest where the ﬁt of model M with prior scaling
π1 is measured by
Fit (π1, M) = 1
3
X
i∈I
mse(π1,M)
i
mse(0)
i
,
(115)
the average of the in-sample mean square error normalized by the MSE of a pure random
walk model. In this particular application this leads to scaling factors ∞, 0.52, 0.33 and
0.19.27 That is, the small 3-variable VAR is estimated with OLS and the scale factor for
the 7 variable VAR is 0.52. The main ﬁnding is that, with the increased shrinkage, forecast
performance improves with model size but also that most of the gains was achieved with
27These numbers diﬀer from the ones reported by Banbura et al. (2010) since they parameterize the
prior in terms of the square of π1.
75

the 20 variable VAR. A moderately large VAR might thus be suﬃcient provided that the
right variables are selected.
Bloor and Matheson (2010) working in an open economy context with a need to make
foreign variables exogenous to domestic variables generalized the approach of Banbura
et al. (2010) by considering diﬀerent amounts of shrinkage for diﬀerent blocks of equa-
tions. This implies that the convenient normal-Wishart prior can not be used and Bloor
and Matheson (2010) base their inference on the blocked importance sampler of Zha (1999)
embodying the same kind of Minnesota type prior beliefs with the addition of a dummy
initial observations prior. To impose diﬀerent amount of shrinkage the prior hyperparam-
eters are made block speciﬁc and chosen using the same strategy as Bloor and Matheson
(2010). Bloor and Matheson (2010) forecasts the New Zealand real GDP, tradable CPI,
non-tradable CPI, an interest rate and the exchange rate using a range of BVARs with
5, 8, 14 and 94 variables. In addition, forecast results are reported for a univariate AR,
a random walk model and frequentist variants of the smallest BVAR. Overall the largest
BVAR provides the best forecasts except for the long horizon (4 quarters) forecast of
tradable CPI where two 5 variable VARs, the BVAR and a frequentist with lags selected
using BIC, gives signiﬁcantly MSEs.
Gupta and Kabundi (2010) conducts a forecasting exercise where the aim is to forecast
the South African per capita growth rate, inﬂation, the money market rate and the growth
rate of the nominal eﬀective exchange rate. The models used are a small (four variable)
DSGE model, a dynamic factor model, a FAVAR using the four variables of interest
estimated by OLS and a Bayesian variant using a Minnesota type prior, a four variable
unrestricted VAR, two BVARs with 4 and 266 variables. The factor models use principal
components as factors. For the VAR models the lag length is set to ﬁve with quarterly
data and the overall scale factor π1 in the Minnesota prior for the BVARs is set following
the approach of Banbura et al. (2010) in addition to common default settings. In addition
Gupta and Kabundi (2010) also experiment with the lag decay rate π3 using settings of
0.5, 1 and 2. For the small VAR the additional shrinkage on lags of other variables is set
to π2 = 0.5. In the large VAR a tighter speciﬁcation is used with π2 = 0.6 for foreign
(world) variables and for domestic variables 0.1 is used in domestic equations and 0.01
in world equations. Overall the large BVAR with the tightest shrinkage, π1 = 0.01 and
π3 = 1, does well and delivers the best forecast for three out of the four variables. The
exception being the exchange rate where the DSGE model does best.
9.2.2
Selecting variables - conjugate SSVS
Koop (2010) considers a range of prior speciﬁcations for forecasting with large BVARs.
This includes the normal-Wishart with Minnesota type prior beliefs used by De Mol
et al. (2008), the original Litterman prior with ﬁxed and diagonal error variance matrix
(section 3.1) which oﬀers the advantage that diﬀerent shrinkage can be applied to lags
of the dependent variable and lags on other variables, the same Minnesota prior but also
allowing for correlation between the variables of interest, the SSVS prior (section 8.1)
with two diﬀerent settings for the prior variances and a new ”conjugate” SSVS prior that
is less computationally demanding and better suited for large BVARs.
The new SSVS prior takes Γ to be distributed as a matricvariate normal conditionally
76

on Ψ and the vector of selection indicators δ,
Γ|Ψ, δ ∼MNkm (Γ, Ψ,Ωδ)
(116)
with Ωδ = diag (h1, . . . , hk) for
hi =
 τ 2
0,i if δi = 0
τ 2
1,i if δi = 1 .
The prior for δi is independent Bernoulli distributions with P (δi = 1) = pi and for Ψ an
inverse Wishart, Ψ ∼iW (S, v) is used. The prior structure is thus conjugate conditional
on δ.
In contrast with the standard SSVS procedure, the conjugate SSVS includes or ex-
cludes a variable in all equations at the same time instead of being speciﬁc to one variable
and equation. While this reduces the ﬂexibility, the Kronecker structure of the prior and
posterior variance matrices for Γ makes for much more eﬃcient computations.
The conditional posterior distributions in (18) and (19) still holds but should be inter-
preted as conditional on δ. The marginal posterior for δ is obtained up to a proportionality
constant by integrating out Γ and Ψ from the product of the likelihood and the prior
yielding the matricvariate-t distribution (20) times the prior for δ, π (δ) . After some sim-
pliﬁcations of the expression for the matricvariate-t density we have
p (δ|YT) ∝g (δ, YT) =
 |Ωδ| /
Ωδ
−m/2 S
−(v+T)/2 π (δ) .
(117)
For k small it is possible to enumerate p (δ|YT) but since there are 2k possible conﬁgu-
rations for δ this quickly becomes infeasible and Koop (2010) suggests a Gibbs sampling
approach for sampling from the marginal posterior of δ originally proposed by Brown,
Vanucci and Fearn (1998). This is reproduced as part A of Algorithm 16.
Forecasting performance
Koop evaluates the forecasting performance using a data
set with 168 US macroeconomic variables and four diﬀerent VAR models with 3, 20, 40
and 168 variables with four lags and formulated to generate direct forecasts. The variables
of interest are real GDP, the CPI and the Federal Funds Rate. For the SSVS priors two
diﬀerent ways of setting the prior variances are used. The ”semiautomatic” approach of
George et al. (2008) sets τ0,ij = bσγij/10 and τ1,ij = 10bσγij where bσγij is the standard error
of the OLS estimate of γij for the standard SSVS. For the conjugate SSVS the maximum
of bσγij for a given i is used. The other approach is based on the Minnesota prior and
sets τ1,ij according to (14) and τ0,ij = τ1,ij/10 for the standard SSVS and mimics the
normal-Wishart prior variance for the conjugate SSVS. For priors with Minnesota type
prior beliefs the scale factors π1 and π2 are set in the same way as in Banbura et al. (2010)
using (115).
As a complement forecasts are also calculated using for a number of FAVARs con-
structed by adding lags of the principal components to the three-variable VAR.
The results of the forecasting exercise is mixed and there is no clear winner but some
patterns do emerge. The factor models does not do very well and never performs best.
There is a gain from moving to larger models but as in Banbura et al. (2010) the additional
gains are small once one moves beyond 20 variables in the VAR. The Minnesota and SSVS
type priors have almost the same number of wins and there is no indication that one is
better than the other.
77

Algorithm 16 Gibbs sample for ”conjugate” SSVS
With the conditional prior (116) for Γ, inverse Wishart prior for Ψ and independent
Bernoulli priors on δi part A below samples from the marginal posterior for δ. After
convergence this can be complemented with part B to produce draws from the joint
posterior for δ, Γ and Ψ.
A. Select starting values δ(0)
For j = 1, . . . , B + R
1. Draw
δ(j)
i ,
i
=
1, . . . , k,
from
the
full
conditional
δ(j)
i |YT, δ(j)
1 , . . . , δ(j)
i−1, δ(j−1)
i+1 , . . . , δ(j−1)
k
∼
Ber (u1i/ (u0i + u1i))
where
u0i = g (δ−i, δi = 0, YT) , u1i = g (δ−i, δi = 1, YT) and g (·) is given by
(117).
B. If j > B and a sample from the joint posterior for δ, Γ and Ψ or the predictive
distribution is desired
2. Draw Ψ(j) from the full conditional posterior Ψ|YT, δ(j) ∼iW
 S, v

in (19).
3. Draw
Γ(j)
from
the
full
conditional
posterior
Γ|YT, Ψ(j), δ(j)
∼
MNkm
 Γ, Ψ, Ωδ

in (18).
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the
joint predictive distribution.
9.3
Reduced rank VAR
Carriero, Kapetanios and Marcellino (2011) use standard VAR models (6) with iterated
forecasts for lead times h > 1 and propose diﬀerent ways of overcoming the ”curse of
dimensionality”. In addition to the standard VAR-model with a tight normal-Wishart
prior they propose the use of models with reduced rank parameter matrices. Working
with data transformed to stationarity and then standardized a VAR without determin-
istic terms, y′
t = Pp
i=1 y′
t−iAi+u′
t, is used and the reduced rank assumption is that the
parameter matrices can be written as Ai = βiα′ where βi and α are m × r matrices. In
matrix form we can write the VAR as
Y = ZΓ + u = Zβα′ + u
(118)
for Γ′ =
 A′
1, . . . , A′
p

and β =
 β′
1, . . . , β′
p
′ a mp × r matrix with β and α of rank
r < m. The setup is similar to the cointegrated VECM (44) and the same basic issue
of the lack of identiﬁcation of β and α arises here. Following Geweke (1996a) Carriero,
78

Kapetanios and Marcellino use a linear normalization, α′ = (Ir, α′
∗) and specify a prior
of the form
vec (α∗) ∼N (vec (α∗) , Σα) , β ∼N
 β, Σβ

, Ψ ∼iW (S, v) .
(119)
Again, following Geweke, α∗and β are set to zero and Σα and Σβ are diagonal matrices
with diagonal elements 1/τ 2 in the application.
The derivation of the full conditional posteriors parallels the one for the VECM in
section 5.1 with obvious changes due to the diﬀerent normalization. The full conditional
posterior for Ψ is inverse Wishart,
Ψ|YT, β, α ∼iW
 S, v

, S = S + (Y −ZΓ)′ (Y −ZΓ) , v = v + T.
(120)
For the full conditional posterior for α∗rewrite the model as one set of equations for the
r ﬁrst variables which does not depend on α∗
Y1 = Zβ + u1
and a set of equations for the remaining m −r variables depending on α∗
Y2 = Zβα′
∗+ u2.
Using that u2|u1 ∼MNT,m−r

u1Ψ−1
11 Ψ12, (Ψ22)−1 , IT

for
Ψ =
 Ψ11
Ψ12
Ψ21
Ψ22

and Ψ22 =
 Ψ22 −Ψ21Ψ−1
11 Ψ12
−1 the lower right (m −r) × (m −r) block of Ψ−1 we
have
Y2|Y1, β, α∗∼MNT,m−r

[Y1 −Zβ] Ψ−1
11 Ψ12 + Zβα′
∗,
 Ψ22−1 , IT

or a conditional regression28
Yc = Y2 −[Y1 −Zβ] Ψ−1
11 Ψ12 = Zβα′
∗+ u2.
It follows that the full conditional posterior for α∗is normal29
vec (α∗) |YT, β, Ψ ∼N
 vec (α∗) , Σα

(121)
Σα =
 Σ−1
α + Ψ22 ⊗β′Z′Zβ
−1
vec (α∗) = Σα

Σ−1
α vec (α∗) + vec
 β′Z′YcΨ22	
.
The full conditional posterior for the unrestricted β matrix follows immediately after
vectorizing the model,
y = (α ⊗Z) vec (β) + u,
28That is, we factor the likelihood into a marginal distribution for Y1 which is functionally independent
of α∗and a conditional distribution for Y2 that depends on α∗. It is then suﬃcient to consider the
conditional likelihood.
29Expressions (14) - (16) in Carriero, Kapetanios and Marcellino (2011), which in turn are based on
results in Geweke (1996a), are incorrect. See Karlsson (2012) for details.
79

as a normal distribution,
vec (β) |YT, α, Ψ ∼N
 vec
 β

, Σβ

(122)
Σβ =
 Σ−1
β + α′Ψ−1α ⊗Z′Z
−1
vec
 β

= Σβ
 Σ−1
β vec
 β

+ vec
 Z′YΨ−1α

.
It is thus straightforward to implement a Gibbs sampler for the reduced rank VAR
model. Due to the relatively large variance matrix Σβ the Gibbs sampler can be time
consuming and Carriero, Kapetanios and Marcellino (2011) suggests a computationally
convenient alternative which they label reduced rank posterior. This is based on a re-
duced rank approximation to the posterior mean of Γ, Γ, with a tight normal-Wishart
prior. Let Γ = UDV′ be the singular value decomposition of Γ and collect the r largest
singular values and corresponding vectors in the matrices D∗= diag (d1, d2, . . . , dr) ,
U∗= (u1, u2, . . . , ur) and V∗= (v1, v2, . . . , vr) . A rank r < m approximation to Γ is
then given by Γ
∗= U∗D∗V∗′.
Forecast performance
In a forecasting exercise with 52 macroeconomic variables for
the US Carriero, Kapetanios and Marcellino (2011) compare the performance of the
Bayesian procedures, VAR with normal-Wishart prior, the reduced rank VAR and the
reduced rank posterior, with several alternatives, a reduced rank VAR estimated with
maximum likelihood, multivariate boosting, factor models and univariate autoregressions.
The reduced rank posterior and the Bayesian reduced rank VAR procedures are found to
give the best forecasts, both in terms of forecasting all the 52 variables and when spe-
ciﬁc variables of interest (industrial production, inﬂation and the federal funds rate) are
singled out.
9.4
Predicting many variables
Carriero, Kapetanios and Marcellino (2009, 2012) takes a slightly diﬀerent viewpoint and
considers the situation where a large number of variables are to be predicted rather than a
small set of variables of interest. In an application to forecasting exchange rates Carriero,
Kapetanios and Marcellino (2009) use a direct forecast version of a one-lag VAR
y′
t = y′
t−hΦh + φh + e′
t,h
(123)
with yt a m = 32 dimensional vector of log exchange rates. Taking et,h to be normal,
et,h ∼N (0, Ψh) they specify a normal-Wishart prior (section 3.2.1) for Γ′
h = (Φ′
h, φ′
h)
and Ψh centered on driftless univariate random walks. To avoid overﬁtting the prior is
very tight with π1 on the order of 0.01, about 1/10 of the conventional setting for medium
sized VARs, and allowed to vary over time and chosen to minimize the sum of the mean
square forecast errors for the previous period. In a comparison of the forecast performance
with naive random walk forecasts, univariate autoregressions, forecasts from a standard
VAR estimated with OLS and factor models with 4 factors, the BVAR is found to perform
best with the random walk second.
Carriero, Kapetanios and Marcellino (2012) propose to use the same direct VAR (123)
to forecast the term structure of interest rates. In contrast to Carriero et al. (2009) the
scaling factor π1 is chosen in an empirical Bayes fashion by maximizing the marginal
80

Algorithm 17 Gibbs sampler for the reduced rank VAR model
For the reduced rank VAR (118) and the prior (119) select starting values α(0)
∗
and Ψ(0).
For j = 1, . . . , B + R
1. Generate β(j) from the full conditional posterior vec (β) |YT, α(j−1), Ψ(j−1) ∼
N
 vec
 β

, Σβ

in (122).
2. Generate α(j)
∗
from the full conditional posterior vec (α∗) |YT, β(j), Ψ(j−1)
∼
N
 vec (α∗) , Σα

in (121).
3. Generate Ψ(j) from the full conditional posterior Ψ|YT, β(j), α(j) ∼iW
 S, v

in
(120).
4. If j > B form Γ(j) = β(j)α(j)′, generate u(j)
T+1, . . . , u(j)
T+H from ut ∼N
 0, Ψ(j)
and
calculate recursively
ey(j)′
T+h =
h−1
X
i=1
ey(j)′
T+h−iA(j)
i
+
p
X
i=h
y′
T+h−iA(j)
i
+ x′
T+hC(j)+u(j)′
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
likelihood (20) with respect to π1. In the application to forecasting bond yields for 18
diﬀerent maturities ranging from 1 to 120 months the marginal likelihood is maximized
with π1 close to 0.003. The forecasting exercise includes, in addition to the direct BVAR,
several atheoretic time series models as well as theory based forecasting models. Overall
the BVAR performs best when forecast performance is measured by the root mean square
error. The picture is less clear when the forecasts are evaluated using economic measures.
Carriero et al. (2012) considers two diﬀerent trading strategies. For the ﬁrst strategy
there is no clear ranking of the models when the diﬀerent maturities are considered,
for the second strategy the BVAR delivers the best result for maturities longer than 21
months.
81

A
Markov chain Monte Carlo Methods
A.1
Gibbs sampler
The Gibbs sampler is particularly well suited to Bayesian computation since it is based on
the conditional distributions of subsets of the parameter vector. It is frequently the case
that it is easy to generate random numbers from the conditional posteriors even if the
joint posterior for all the parameters is non-standard. A case in point is regression models
like the VAR-model in section 2.1.1 where the posterior distribution of the regression
parameters γ conditional on the error variance-covariance is normal and the posterior
distribution of the variance-covariance matrix Ψ conditional on γ is inverse Wishart.
MCMC is not needed in that particular case but we will see that this results carries over
to situations where it is not possible to generate random numbers directly from the joint
posterior.
The recipe for constructing a Gibbs sampler is as follows.
1. Find a suitable partition of the parameter vector into k subsets θ = (θ′
1, . . . , θ′
k)′
2. Obtain the set of full conditional posterior distributions for the subvectors
p (θi|YT, θ−i) , i = 1, . . . , k
3. Draw j + 1 from the Gibbs sampler is given by generating the subvectors from the
full conditional posteriors while updating the conditioning
θ(j+1)
1
∼p

θ1|YT, θ(j)
2 , . . . θ(j)
k

θ(j+1)
2
∼p

θ2|YT, θ(j+1)
1
, θ(j)
3 , . . . θ(j)
k

...
θ(j+1)
k
∼p

θk|YT, θ(j+1)
1
, . . . θ(j+1)
k−1

.
It is easy to verify that the joint posterior distribution is a stationary distribution of
the Gibbs sampler. For the simple case with two subvectors θ = (θ′
1, θ′
2)′ the transition
kernel is f
 θ(j+1)|θ(j)
= p

θ(j+1)
2
|YT, θ(j+1)
1

p

θ(j+1)
1
|YT, θ(j)
. If θ(j) is a draw from
the posterior p (θ1, θ2|YT) = p (θ1|YT, θ2) p (θ2|YT) marginalizing out θ(j) from the joint
distribution of θ(j+1) and θ(j) yields the posterior distribution
Z Z
p

θ(j+1)
2
|YT, θ(j+1)
1

p

θ(j+1)
1
|YT, θ(j)
2

p

θ(j)
1 |YT, θ(j)
2

p

θ(j)
2 |YT

dθ(j)
1 dθ(j)
2
=
Z
p

θ(j+1)
2
|YT, θ(j+1)
1

p

θ(j+1)
1
|YT, θ(j)
2

p

θ(j)
2 |YT

dθ(j)
2
= p

θ(j+1)
2
|YT, θ(j+1)
1

p

θ(j+1)
1
|YT

= p

θ(j+1)
1
, θ(j+1)
2
|YT

.
The Gibbs sampler is thus quite straightforward to implement and the form of the
sampler follows directly from the model when the full conditionals are well known distri-
butions. This makes it very appealing. There is, however, no guarantee that a naively
82

implemented Gibbs sampler will perform well or even that it is convergent. Reparame-
terizing the model or modifying the blocks (the partition into subvectors) to put highly
correlated parameters into the same block can often improve the performance and speed
of convergence dramatically.
A.2
Metropolis-Hastings
The Metropolis-Hastings algorithm is a more general method that does not rely on the
availability of tractable full conditionals. The basic idea is similar to acceptance-rejectance
sampling and importance sampling in that an approximation to the desired distribution is
used to generate a proposal for the next draw from the chain. The proposal is accepted or
rejected based on how well it agrees with the desired distribution and by a judicious choice
of the acceptance probability on can obtain a Markov chain with the desired distribution
as it’s stationary distribution.
In Metropolis-Hastings the proposal distribution itself is allowed to be a Markov chain
and the proposed value for θ(j+1) can depend on the current value θ(j) through the con-
ditional distribution q
 x|θ(j)
. The algorithm is as follows
1. Draw a proposal x from the conditional distribution q
 x|θ(j)
.
2. Set θ(j+1) = x with probability
α
 θ(j), x

= min
 
1, p (x|YT) /q
 x|θ(j)
p (θ(j)|YT) /q (θ(j)|x)
!
(124)
and keep the old value otherwise, θ(j+1) = θ(j). That is, generate u from a uniform
distribution on (0, 1) and set θ(j+1) = x if u ≤α and θ(j+1) = θ(j) otherwise.
The transition kernel of the resulting Markov chain is given by the conditional distri-
bution
f
 θ(j+1)|θ(j)
=

q
 θ(j+1)|θ(j)
α
 θ(j), θ(j+1)
, θ(j+1) ̸= θ(j)
q
 θ(j)|θ(j)
+
R
x̸=θ(j) q
 x|θ(j)  1 −α
 θ(j), x

dx, θ(j+1) = θ(j) .
That the Markov chain has the posterior as a stationary distribution can be checked by
verifying that the detailed balance condition
f
 θ(j)
p
 θ(j+1)|θ(j)
= f
 θ(j+1)
p
 θ(j)|θ(j+1)
holds for f (·) the posterior distribution. Note that the detailed balance condition implies
a form of symmetry, the chain moves from x to y at the same rate as it moves from y to
x.
Note that any constants cancel from the acceptance probability α and it can be written
in terms of the product of the likelihood and the prior instead of the, typically, unknown
joint posterior. That is
α
 θ(j), x

= min
 
1,
L (YT|x) π (x) /q
 x|θ(j)
L (YT|θ(j)) π (θ(j)) /q (θ(j)|x)
!
83

The choice of the proposal distribution q is crucial for the performance of the Markov
chain and it is important that it is well tailored to the posterior distribution. Examples
of common types of proposal chains are
• Independence chain: The proposal steps are drawn from a ﬁx density, q
 x|θ(j)
=
f (x) . It is important for the performance of the Markov chain that the proposal
distribution is well tailored to the posterior over the whole parameter space which
can be diﬃcult with high dimensional parameter vectors. There are, on the other
hand, theoretical advantages, the resulting Metropolis chain is uniformly ergodic if
the weights p (x|YT) /q
 x|θ(j)
are bounded on the parameter space.
• Random walk chain: The proposal steps follow a random walk, x = θ(j) + e,
q
 x|θ(j)
= f
 x −θ(j)
where f is the density of e. The random walk chain makes
it possible to construct a proposal density that matches the posterior well locally,
but the proposal should not be to local or the chain will move very slowly through
the parameter space.
It is possible to divide the parameter vector into blocks, just as with the Gibbs sampler,
and update one block at a time with diﬀerent proposal distributions for each block of
parameters. This makes it easier to adapt the proposal to the posterior and can make
for a better performing Markov chain. With a partition θ = (θ′
1, . . . , θ′
k) and updating in
order, the update for block m is analogous to the update for the full parameter vector.
1. Propose xm from the proposal density qm

xm, θ(j+1)
1
, . . . , θ(j+1)
m−1 , θ(j)
m , . . . , θ(j)
k

.
2. Accept the proposal and set θ(j+1)
m
= xk with probability α given by (124) otherwise
set θ(j+1)
m
= θ(j)
m .
Note that the acceptance probability simpliﬁes and can be written in terms of the full
conditional posterior for θm if this is available,
α
 θ(j), xm

= min

1,
p

xm|YT, θ(j+1)
1
, . . . , θ(j+1)
m−1 , θ(j)
m+1, . . . , θ(j)
k

qm

xm|θ(j+1)
1
, . . . , θ(j+1)
m−1 , θ(j)
m , . . . , θ(j)
k

,p

θ(j)|YT, θ(j+1)
1
, . . . , θ(j+1)
m−1 , θ(j)
m+1, . . . , θ(j)
k

qm

θ(j)
m |θ(j+1)
1
, . . . , θ(j+1)
m−1 , x, θ(j)
m+1, . . . , θ(j)
k


.
In this case the full conditional posterior is an excellent proposal density and with this
choice of qm the acceptance ratio simpliﬁes to one.
The Gibbs sampler is thus a special case of the Metropolis-Hastings algorithm and
we can use a mix of Metropolis-Hastings updates and Gibbs updates in the Markov
chain. Gibbs updates for the components with convenient full conditional posteriors and
Metropolis-Hastings for the other components. Although somewhat of a misnomer, this is
commonly known as a Metropolis-Hastings within Gibbs chain. In this context it is useful
to note that it is suﬃcient for uniform ergodicity that one of the Metropolis-Hasting steps
uses an independence proposal with bounded weights
p

xm|YT, θ(j+1)
1
, . . . , θ(j+1)
m−1 , θ(j)
m+1, . . . , θ(j)
k

/qm (xm) .
84

A.3
Autocorrelation in the Markov chain
The output from a Markov chain is by construction autocorrelated and this aﬀects the
precision of estimates of posterior quantities sometimes to the point where they are close
to being unusable.
Ideally one would go back to the drawing board and construct a
Markov chain with lower autocorrelation that mixes well. This is, however, not always
possible and one must then be particularly careful in the choice of burn-in and make sure
that the Markov chain runs long enough to explore the full parameter space.
A common strategy in these situations is to thin the chain, i.e. to retain only every
mth draw from the chain where m is chosen to make the autocorrelation between θ(j) and
θ(j+m) negligible. Based on a sample of size R/m after discarding the burn-in we can then
estimate the posterior mean of a function g (·) of the parameters as
gR/m = m
R
R/m
X
i=1
g
 θ([i−1]m+1)
and an estimate of the numerical standard error is given by the square root of
bV
 gR/m

=
bV (g (θ) |YT)
R/m
.
This is a statistically ineﬃcient procedure and it can be shown that V
 gR/m

≥V (gR) .
On the other hand it might reduce the storage and memory requirements considerably.
If the chain is not thinned or when the thinning leaves some autocorrelation this must
be accounted for when estimating the numerical standard errors. Two common methods
is the batched mean method and the time series based spectral estimate. The batched
mean method divides the data into R/m batches, each containing m consecutive draws
from the Markov chain, and calculate the batch means
gm,j = 1
m
jm
X
i=(j−1)m+1
g
 θ(i)
.
For suﬃciently large m the batch means will be essentially uncorrelated and we can
estimate the variance of the batch means as
bV (gm) =
1
R/m −1
R/m
X
j=1
 gm,j −gR
2 .
An estimate of the variance of gR is then given by
bV (gR) = m
R
bV (gm) =
m
R (R/m −1)
R/m
X
j=1
 gm,j −gR
2 .
The so called spectral estimate is simply the Newey and West (1987) autocorrelation
consistent estimator of the asymptotic variance (12). A common implementation is the
estimator
bV (gR) = 1
R
m
X
j=−m

1 −
|j|
m + 1

bγj
85

with truncation at lag m and the autocovariances,
bγj = 1
R
R−j
X
i=1

g
 θ(i)
−gR
 
g
 θ(i+j)
−gR

,
at larger lags are downweighted using a Bartlett kernel. For consistency the truncation
should go to inﬁnity with R, m = o
 R1/4
.
The autocorrelation will in general lead to a loss of eﬃciency compared to the case
when we can generate iid draws from the posterior. It is common to measure the loss
with the relative numerical eﬃciency (RNE)
RNE =
bV (g (θ)) /R
bV (g)
.
An alternative measure is the eﬀective sample size, the number of iid draws that would
give the same numerical standard error as the R draws we have from the sampler. This
is simply R times the RNE.
A.4
Assessing convergence
It should be clear from the discussion above that it can not be taken for granted that
a Gibbs or Metropolis-Hastings sampler converges to the desired posterior distribution.
Nor that, if the sampler is convergent, it does converge in a reasonable number of steps
and that the output can be used to compute reliable estimates of posterior quantities.
Trying to assess if the sampler fails to converge or not and the approximate number of
steps required to be ”close enough” to convergence is thus important. Even if convergence
can be proved for a particular sampler there is very little information about how quickly
it converges and an empirical assessment of the amount of burn-in needed must be made.
Unfortunately, the output from the chain only constitutes a sample and can not be used
to prove convergence – all we can do is to look for signs of lack of convergence or slow
convergence.
Some of the most powerful diagnostics or indicators of problems are quite simple in
nature. High and persistent autocorrelation in the chain indicates slow mixing and slow
convergence to the posterior distribution. Is the posterior multimodal? If so, the chain
might get stuck at one of the modes if the probability mass connecting the modes is
small. Simple plots of the output, trace plots of the parameters, θ(j)
i , or some function
of the parameters, g
 θ(j)
, plots of running means, gt = 1
t
Pt
j=1 g
 θ(j)
, or CUSUMs,
St = Pt
j=1

g
 θ(j)
−gR

, can also be informative.
If the trace plot or the running
means settle down after a number of steps this can indicate a suitable amount of burn-
in. Similarly for the CUSUM plots. In addition Yu and Mykland (1998) argue that the
CUSUM plot can be informative about how well the sampler mixes, ”a good sampler
should have an oscillatory path plot and small excursions; or a bad sampler should have
a smooth path plot and large excursions”.
Brooks (1998) proposed a formal test for deviations from the ideal case of iid output
from the sampler based on the CUSUM. First determine a suitable amount of burn-in,
B, based on preliminary plots of the output and calculate bµ = (R −B)−1 PR
j=B+1 g
 θ(j)
86

and St = Pt
j=B+1

g
 θ(j)
−bµ

for t = B + 1, . . . , R and g (·) some function of the
parameters. Next deﬁne
dj =
 1 if Sj−1 > Sj and Sj < Sj+1 or Sj−1 < Sj and Sj > Sj+1
0 otherwise
where dj = 1 indicates non-smoothness or ”hairiness” of the plot and dj = 0 indicates
smoothness. The running means Dt = (t −B −1)−1 Pt−1
j=B+1 dj for t = B + 2, . . . , R
lies between 0 and 1 and captures the overall behavior of the Markov chain. If we, in
addition to the iid assumption, assume that g
 θ(j)
is symmetric around the mean we
have P (dj = 1) = 1/2 and Dt is Binomially distributed. We can then plot Dt against
the bounds ±Zα/2
q
1
4(t−B−1) and diagnose non-convergence if DT fails to lie within the
bounds 100 (1 −α) % of the time.
Geweke (1992) proposed monitoring convergence by the statistic
zG =
ga −gb
p
V (ga) + V (gb)
where g (·) is some function of the output of the chain and
ga = 1
na
m+na
X
j=m+1
g
 θ(j)
, gb = 1
nb
R
X
j=R−nb+1
g
 θ(j)
for a chain that is run R steps with R > na + nb + m and the distance between the
estimates such that they can be taken to be uncorrelated. The variances are estimated
taking account of the autocorrelation structure in the chain, for example by the spectral
estimate above. If the chain has converged after m steps the distribution of the draws
m + 1, . . . , m + na is the same as the distribution of the draws at the end of the chain
and zG approximately standard normal. Calculating zG for a range of values of m and
comparing to critical values from a standard normal will thus give an indication of the
burn in needed for the chain.
Gelman and Rubin (1992), proposed running several shorter chains started at points
that are overdispersed compared to the posterior. Let θ(j)
i
denote the output from chain i
for m chains run n = R −B steps from burn-in and deﬁne the between and within chain
variances as
B =
n
m −1
m
X
i=1
(gi −g)2 ,
W =
1
m (n −1)
m
X
i=1
n
X
j=1

g

θ(j)
i

−gi
2
.
gi = 1
n
n
X
j=1
g

θ(j)
i

,
g = 1
m
m
X
i=1
gi
Convergence failure or convergence on diﬀerent stationary distributions after the selected
burn-in is indicated by the between chain variation, B, being larger than the within chain
variation, W. If the chains have converged after B draws we have two unbiased estimates
of the variance, V = (1 −1/n) W + B/n and W. The ﬁrst tends to overestimate the
variance if convergence has not been achieved (the between chain variation is large) and
87

the latter tends to underestimate the variance (the chains have not had time to explore the
full parameter space). The convergence diagnostic is r =
p
V/W or a version including
a ”degree of freedom” correction. Gelman (1996) suggested the rule of thumb to accept
convergence if r < 1.2 for all monitored quantities.
The Brooks and Geweke diagnostics and Gelman-Rubin diagnostics are quite diﬀerent
in nature. The Brooks and Geweke diagnostics are based on a single long chain and will
fail to detect convergence failures caused by the chain being stuck at one of the modes of
a multimodal posterior. The Gelman-Rubin statistic, on the other hand, is more likely
to detect this type of problem but is much less informative about the amount of burn-in
needed.
B
State space models
Consider the linear state space model for the m observed variables in yt, t = 1, . . . , T,
yt = Ztst + εt, εt ∼N (0, Ht)
(125)
st+1 = dt + Ttst + ηt, ηt ∼N (0, Qt)
with the initial condition or prior on the ﬁrst state, s1 ∼N
 s1|0, P1|0

. The n dimensional
state vectors st are unobserved and the matrices Zt, Ht, Tt and Qt are assumed known
for the purpose of the discussion here (they are in general functions of the data, unknown
parameters or simply known constants). The subscript t|s indicates a time t property
conditional on information up to time s, a superscript t indicates a sequence running
from 1 to t, e.g. si|j = E (si|yj) = E (si|y1, . . . , yj) .
General references on state space models include Harvey (1989) and Durbin and Koop-
man (2001), West and Harrison (1997) and Kim and Nelson (1999) provides a Bayesian
treatment and Giordini, Pitt and Kohn (2011) reviews Bayesian inference in general state
space models. The Kalman ﬁlter and smoothing algorithms given below are standard. The
version of the simulation smoother is due to Carter and Kohn (1994). There are many
variations on these algorithms, the ones given here are straightforward and intuitive but
not the most computationally eﬃcient versions.
B.1
Kalman ﬁlter
The Kalman ﬁlter runs forward through the data and returns the means and variances of
the conditional distributions st|yt ∼N
 st|t, Pt|t

and st+1|yt ∼N
 st+1|t, Pt+1|t

,
vt = yt −Ztst|t−1
Ft = ZtPt|t−1Z′
t + Ht
Kt = Pt|t−1Z′
tFt
st|t = st|t−1 + Ktvt
Pt|t = Pt|t−1 −KtZtPt|t−1
st+1|t = dt + Ttst|t
Pt+1|t = TtPt|tT′
t + Qt,
(126)
for t = 1, . . . , T.
B.2
Smoothing
At the end of the ﬁltering run we have the distribution of the last state, sT|yT, conditional
on all the data but for the earlier states we only have the distribution conditional on a
88

Algorithm 18 Simulation Smoother
1. Generate sT from the conditional distribution, sT|yT ∼N
 sT|T, PT|T

2. For t = T −1, . . . , 1
(a) Calculate
st|t,st+1 = st|t + Pt|tT′
tP−1
t+1|t
 st+1 −st+1|t

Pt|t,st+1 = Pt|t −Pt|tT′
tP−1
t+1|tTtPt|t
(b) Generate st from the conditional distribution st|yt, st+1 = st|yT, st+1 ∼
N
 st|t,st+1, Pt|t,st+1

.
subset of the data and all information has not been used. The ﬁxed-interval smoother
runs backwards through the data and returns the means and variances of the conditional
distributions st|yT ∼N
 st|T, Pt|T

,
st|T = st|t + Pt|tT′
tP−1
t+1|t
 st+1|T −st+1|t

(127)
Pt|T = Pt|t −Pt|tT′
tP−1
t+1|t
 Pt+1|T−Pt+1|t

P−1
t+1|tTtPt|t
for t = T −1, . . . , 1.
B.3
Simulation smoother
The simulation smoother is a device for generating random numbers from the joint dis-
tribution of the states conditional on the data, sT|yT. The output from the ﬁxed-interval
smoother can not be used for this since it carries no information about the dependence
between the states at diﬀerent time points. The simulation smoother is based on the
partition
p
 s1, . . . , sT|yT
= p
 sT|yT t−1
Y
t=1
p
 st|yT, st+1

and generates a draw from the joint distribution by working backwards through the data
and generating st from the conditional distributions.
C
Distributions
Deﬁnition 1 (Gamma) x is Gamma distributed with shape parameter α and inverse
scale parameter β, x ∼G (α, β) if the density is
f (x) =
βα
Γ (α)xα−1 exp (−βx) .
We have E (x) = α/β and V (x) = α/β2.
89

Deﬁnition 2 (Inverse Gamma) y = x−1 is inverse Gamma distributed, y ∼iG (α, β)
if x is Gamma distributed x ∼G (α, β) . The density of y is
f (y) =
βα
Γ (α)y−(α+1) exp

−β
y

with moments E (y) = β/ (α −1) and V (y) = β/

(α −1)2 (α −2)

.
Deﬁnition 3 (Matricvariate normal) The p × q matrix X is said to have a matric-
variate normal distribution
X ∼MNpq (M, Q, P)
where M is p × q and P and Q are positive deﬁnite symmetric matrices of dimensions
p × p and q × q if vec (X) is multivariate normal
vec (X) ∼N (vec (M) , Q ⊗P) .
The density of X is
MNpq (X; M, Q, P)
= (2π)−pq/2 |Q ⊗P|−1/2
× exp

−1
2 (vec (X) −vec (M))′  Q−1 ⊗P−1
(vec (X) −vec (M))

= (2π)−pq/2 |Q|−p/2 |P|−q/2 exp

−1
2 tr

Q−1 (X −M)′ P−1 (X −M)

.
Remark 1 The deﬁning feature of the matricvariate normal is the Kronecker structure
for the variance-covariance matrix. Q is proportional to the variance matrix of the rows
of. X and P is proportional to the variance matrix of the columns of X. The elements in
row i are correlated with the elements in row j if pij ̸= 0 and the elements in column i
are correlated with the elements in column j if qij ̸= 0.
Remark 2 Suppose that X ∼MNpq (M, Q, P)
1. X′ ∼MNqp (M′, P, Q) .
2. AXB ∼MNkl (AMB, B′QB, APA′) for A k × p and B q × l.
Algorithm 19 Matricvariate normal random number generator
To generate X
∼
MNpq (M, Q, P) calculate the Cholesky factors of Q and P,
Q = LL′, P = CC′, generate Y as a p × q matrix of standard normals and calculate
X = M + CYL′ ∼MNpq (M, Q, P) .
90

Deﬁnition 4 (Wishart) A q × q positive semi deﬁnite symmetric matrix A is said to
have a Wishart distribution, A ∼Wq (B,v) if it’s density is given by
Wq (A; B, v) = k−1 |B|−v/2 |A|(v−q−1)/2 exp

−1
2 tr AB−1

for B a positive deﬁnite symmetric matrix and v ≥q degrees of freedom and
k = 2vq/2πq(q−1)/4
qY
i=1
Γ ((v + 1 −i) /2) .
E (A) = vB
V (aij) = v
 b2
ij + biibjj

for aij one of the q (q + 1) /2 distinct elements of A.
Remark 3 The Wishart distribution is a matricvariate generalization of the χ2 distribu-
tion and arises frequently in multivariate analysis. If xi are iid. N (µ, Σ) q-dimensional
random vectors then Pn
i=1 (xi −µ) (xi −µ)′ ∼Wq (Σ, n) and Pn
i=1 (xi −x) (xi −x)′ ∼
W (Σ, n −1) . If A ∼Wq (B, v) then PAP′ ∼Wp (PBP′, v) for P a p × q matrix of rank
p (p ≤q) .
Algorithm 20 Wishart random number generator
Wishart distributed matrices, A ∼Wq (B, v) can be generated by brute force by ﬁrst
generating v vectors xi ∼N (0, B) and forming A = Pv
i=1 xix′
i. A more eﬃcient algorithm
is based on the Bartlett decomposition of a Wishart matrix (Anderson (1984)) has been
proposed by Smith and Hocking (1972) and Geweke (1988).
Let P be a q × q lower
triangular matrix where p2
ii ∼χ2
v−i+1 (i.e. pii is the square root of the χ2) and pij ∼
N (0, 1) , i < j, then PP′ ∼Wq (I, v) . In addition let L be the lower triangular Cholesky
factor of B = LL′, then A = (LP) (LP)′ ∼Wq (B, v) . Note that in many cases it is
more convenient to work directly with the lower triangular matrix C = LP than A, e.g.
when the ultimate objective is to generate random numbers z ∼N (µ, A) . First generate
xi ∼N (0, 1) , i = 1, . . . , q and form z = µ + Cx.
In some cases it is more convenient with an upper triangular decomposition QQ′ ∼
Wq (I, v) . For this let q2
ii ∼χ2
v−q+i and qij ∼N (0, 1) , i > j.
Deﬁnition 5 (Inverse Wishart) The q×q matrix A is said to have an inverse Wishart
distribution,
A ∼iWq (B, v)
if A−1 ∼W (B−1, v) . The density of A is given by
iWq (A; B, v) = k−1 |B|v/2 |A|−(v+q+1)/2 exp

−1
2 tr A−1B

91

Algorithm 21 Inverse Wishart random number generator
To generate A ∼iWq (B,v) , ﬁrst generate the upper triangular Bartlett decomposition
matrix Q of a Wishart distributed, Wq (I,v) , matrix. Second calculate the lower triangular
Cholesky decomposition, LL′ = B, we then have L−TL−1 = B−1 and L−TQQ′L−1 ∼
Wq (B−1, v) . Let C = LQ−T and we have A = CC′ ∼iWq (B, v) for C lower triangular.
Sometimes A ∼iWq (D−1, v) is needed. The inversion of D can be avoided by letting
L be the Cholesky decomposition of D, LL′ = D, generate the lower triangular Bartlett
decomposition matrix P and let C be the upper triangular matrix C = (LP)−T for
A = CC′ ∼iWq (D−1, v)
with k as for the Wishart distribution.
E (A) =
1
v −q −1B, v > q + 1
V (aij) = (v −q −1) biibjj + (v −q + 1) b2
ij
(v −q −1)2 (v −q) (v −q −3)
, v > q + 3
Deﬁnition 6 (normal-Wishart) If X|Σ ∼MNpq (M, Σ, P) and Σ ∼iWq (Q, v) then
the joint distribution of X and Σ is said to be normal-Wishart with kernel
p (X, Σ) ∝|Σ|−(v+p+q+1)/2 exp

−1
2 tr

Σ−1 (X −M)′ P−1 (X −M)

exp

−1
2 tr Σ−1B

Algorithm 22 Normal-Wishart random number generator
To generate X|Σ ∼MNpq (M, Σ, A) and Σ ∼iWq (B, v) ﬁrst generate the triangular
factor C of an inverse Wishart and calculate Σ = CC′ (if needed). Second generate Y as
a p × q matrix of standard normals and form X = M + LYC′ ∼MNpq (M, Σ, A) for L
the Cholesky factor of A = LL′.
Deﬁnition 7 (Matricvariate t) A random p×q matrix X is said to have a matricvari-
ate t distribution if the density is given by
Mtpq (X; M, P, Q,v) = k−1 Q+ (X −M)′ P (X −M)
−(v+p)/2
for M a p × q mean matrix, Q and P, q × q and p × p positive deﬁnite symmetric scale
matrices and v ≥q degrees of freedom. The integrating constant is given by
k = πpq/2 |P|−q/2 |Q|−v/2
qY
i=1
Γ ((v + 1 −i) /2)
Γ ((v + p + 1 −i) /2).
We have
E (X) = M,v > q
V (vec X) =
1
v −q −1Q ⊗P−1, v > q + 1.
92

Remark 4 If X|Σ ∼MNpq (M, Σ, P) and Σ ∼iWq (Q, v) (normal-Wishart) then the
marginal distribution of X is matricvariate t.
X ∼Mtpq
 M, P−1, Q,v

.
It follows that Algorithm 22 also is a matricvariate t random number generator where the
draws of C or Σ are simply discarded.
In addition, the distribution of Σ conditional on X is inverse Wishart
Σ|X ∼iWq
 Q+ (X −M)′ P−1 (X −M) , v + p

.
93

References
Adolfson, M., Andersson, M. K., Linde, J., Villani, M. and Vredin, A. (2007), ‘Modern
forecasting models in action: Improving macroeconomic analyses at central banks’,
International Journal of Central Banking 3, 111–144.
Adolfson, M., Lasen, S., Lind, J. and Villani, M. (2008), ‘Evaluating an estimated new
keynesian small open economy model’, Journal of Economic Dynamics and Control
32(8), 2690–2721.
Anderson, T. W. (1984), An Introduction to Multivariate Statistical Analysis, 2nd edn,
John Wiley & Sons, New York.
Andersson, M. K., Palmqvist, S. and Waggoner, D. F. (2010), Density conditional fore-
casts in dynamic multivariate models, Working Paper Series 247, Sveriges Riksbank.
Andersson, M. and Karlsson, S. (2009), Bayesian forecast combination for VAR models,
in Chib, Koop and Griﬃths (2008), pp. 501–524.
Banbura, M., Giannone, D. and Reichlin, L. (2010), ‘Large bayesian vector auto regres-
sions’, Journal of Applied Econometrics 25, 71–92.
Barbieri, M. M. and Berger, J. O. (2004), ‘Optimal predictive model selection’, The Annals
of Statistics 32(3), 870–897.
Bauwens, L. and Lubrano, M. (1996), Identiﬁcation restrictions and posterior densities in
cointegrated gaussian var systems, in T. B. Fomby and R. C. Hill, eds, ‘Advances in
Econometrics’, Vol. 11B, JAI Press.
Beechey, M. and ¨Osterholm, P. (2010), ‘Forecasting inﬂation in an inﬂation-targeting
regime: A role for informative steady-state priors’, International Journal of Fore-
casting 26(2), 248–264.
Bernanke, B., Boivin, J. and Eliasz, P. (2005), ‘Measuring the eﬀect of monetary policy:
a factor augmented vector autoregressive (FAVAR) approach’, Quarterly Journal of
Economics 120, 387–422.
Bloor, C. and Matheson, T. (2010), ‘Analysing shock transmission in a data-rich environ-
ment: a large bvar for new zealand’, Empirical Economics 39, 537–558.
Bloor, C. and Matheson, T. D. (2011), ‘Real-time conditional forecasts with bayesian
vars: An application to new zealand’, The North American Journal of Economics
and Finance 22(1), 26–42.
Brooks, S. P. (1998), ‘Quantitative convergence assessment for markov chain monte carlo
via cusums’, Statistics and Computing 8, 267–274.
Brown, P. J., Vanucci, M. and Fearn, T. (1998), ‘Multivariate bayesian variable selection
and prediction’, Journal of the Royal Statistical Society, Ser. B 60, 627–641.
Canova, F. (2007), ‘G-7 inﬂation forecasts: Random walk, phillips curve or what else?’,
Macroeconomic Dynamics 11(01), 1–30.
94

Canova, F. and Ciccarelli, M. (2004), ‘Forecasting and turning point predictions in a
bayesian panel var model’, Journal of Econometrics 120, 327–359.
Canova, F. and Ciccarelli, M. (2009), ‘Estimating multicountry var models’, International
Economic Review 50(3), 929–959.
Carriero, A., Clark, T. and Marcellino, M. (2011), Bayesian vars: speciﬁcation choices
and forecast accuracy, Working Paper 1112, Federal Reserve Bank of Cleveland.
Carriero, A., Kapetanios, G. and Marcellino, M. (2009), ‘Forecasting exchange rates with
a large bayesian var’, International Journal of Forecasting 25(2), 400–417.
Carriero, A., Kapetanios, G. and Marcellino, M. (2011), ‘Forecasting large datasets
with bayesian reduced rank multivariate models’, Journal of Applied Econometrics
26, 735–761.
Carriero, A., Kapetanios, G. and Marcellino, M. (2012), ‘Forecasting government bond
yields with large bayesian vector autoregressions’, Journal of Banking &amp; Finance
36(7), 2026 – 2047.
Carter, C. K. and Kohn, R. (1994), ‘On gibbs sampling for state space models’, Biometrika
81(3), pp. 541–553.
Chib, S. (1995), ‘Marginal likelihood from the Gibbs output’, Journal of the American
Statistical Association 90, 1313–1321.
Chib, S. (1998), ‘Estimation and comparison of multiple change point models’, Journal
of Econometrics 86, 221–241.
Chib, S. and Greenberg, E. (1995), ‘Understanding the Metropolis-Hastings algorithm’,
American Statistician 40, 327–335.
Chib, S., Koop, G. and Griﬃths, B., eds (2008), Bayesian Econometrics, Vol. 23 of
Advances in Econometrics, Emerald.
Clark, T. E. (2011), ‘Real-time density forecasts from bayesian vector autoregressions with
stochastic volatility’, Journal of Business & Economic Statistics 29(3), 327–341.
Clark, T. E. and McCracken, M. W. (2010), ‘Averaging forecasts from VARs with uncer-
tain instabilities’, Journal of Applied Econometrics 25, 5–29.
Clements, M. P. and Hendry, D. F., eds (2011), The Oxford Handbook of Economic Fore-
casting, Oxford University Press.
Cogley, T., Morozov, S. and Sargent, T. J. (2005), ‘Bayesian fan charts for u.k. inﬂation:
Forecasting and sources of uncertainty in an evolving monetary system’, Journal of
Economic Dynamics and Control 29(11), 1893 – 1925.
Cogley, T. and Sargent, T. J. (2002), Evolving post-world war ii u.s. inﬂation dynamics, in
B. S. Bernanke and K. S. Rogoﬀ, eds, ‘NBER Macroeconomics Annual 2001, Volume
16’, National Bureau of Economic Research, Inc, pp. 331–388.
95

Cogley, T. and Sargent, T. J. (2005), ‘Drifts and volatilities: monetary policies and
outcomes in the post wwii us’, Review of Economic Dynamics 8, 262–302.
Corradi, V. and Swanson, N. R. (2006), Predictive density evaluation, in Elliott, Granger
and Timmermann (2006), pp. 197 – 284.
D’Agostino, A., Gambetti, L. and Giannone, D. (forthcoming), ‘Macroeconomic forecast-
ing and structural change’, Journal of Applied Econometrics .
De Mol, C., Giannone, D. and Reichlin, L. (2008), ‘Forecasting using a large number
of predictors: is bayesian regression a valid alternative to principal components?’,
Journal of Econometrics 146, 318–328.
DeJong, D. N. (1992), ‘Co-integration and trend-stationarity in macroeconomic time se-
ries: Evidence from the likelihood function’, Journal of Econometrics 52(3), 347–370.
DelNegro, M. and Schorfheide, F. (2011), Bayesian methods in microeconometrics, in
Clements and Hendry (2011), chapter 7, pp. 293–389.
Dickey, J. M. (1971), ‘The weighted likelihood ratio, linear hypothesis on normal location
parameters’, The Annals of Mathematical Statistics 42, 204–223.
Doan, T., Litterman, R. B. and Sims, C. (1984), ‘Forecasting and conditional projection
using realistic prior distributions’, Econometric Reviews 3, 1–144.
Dorfman, J. H. (1995), ‘A numerical bayesian test for cointegration of ar processes’,
Journal of Econometrics 66, 289–324.
Dr`eze, J. H. and Morales, J. A. (1976), ‘Bayesian full information analysis of simultaneous
equations’, Journal of the American Statistical Association 71, 919–923.
Durbin, J. and Koopman, S. J. (2001), Time Series Analysis by State Space Methods,
Oxford University Press.
Elliott, G., Granger, C. W. J. and Timmermann, A., eds (2006), Handbook of Economic
Forecasting, Vol. 1, Elsevier.
Forni, M., Hallin, M., Lippi, M. and Reichlin, L. (2003), ‘Do ﬁnancial variables help fore-
casting inﬂation and real activity in the euro area?’, Journal of Monetary Economics
50, 1243–1255.
Gamerman, D. (1997), Markov Chain Monte Carlo: Stochastic Simulation for Bayesian
Inference, Chapman & Hall.
Gelman, A. (1996), Inference and monitoring convergence, in W. R. Gilks, S. Richardson
and D. J. Spiegelhalter, eds, ‘Markov Chain Monte Carlo in Practice’, Chapman and
Hall, chapter 8, pp. 131–143.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (2003), Bayesian Data Analysis,
2 edn, Chapman and Hall/CRC.
96

Gelman, A. and Rubin, D. B. (1992), ‘Inference from iterative simulation using multiple
sequences’, Statistical Science 7, 457–511. with discussion.
George, E. I. and McCulloch, R. E. (1993), ‘Variable selection via gibbs sampling’, Journal
of the American Statistical Association 88, 881–889.
George, E. I., Sun, D. and Ni, S. (2008), ‘Bayesian stochastic search for var model restric-
tions’, Journal of Econometrics 142, 553–580.
Geweke, J. (1988), ‘Antithetic acceleration of monte carlo integration in bayesian infer-
ence’, Journal of Econometrics 38, 73–89.
Geweke, J. (1989), ‘Bayesian inference in econometric models using monte carlo integra-
tion’, Econometrica 57, 1317–1339.
Geweke, J. (1992), Evaluating the accuracy of sampling-based approaches to the calcula-
tion of posterior moments, in J. M. Bernardo, J. O. Berger, A. P. David and A. F. M.
Smith, eds, ‘Bayesian Statistics 4’, Clarendon Press, pp. 169–193.
Geweke, J. (1996a), ‘Bayesian reduced rank regression in econometrics’, Journal of Econo-
metrics 75, 121–146.
Geweke, J. (1996b), Variable selection and model comparison in regression, in J. M.
Bernardo, J. O. Berger, A. P. David and A. F. M. Smith, eds, ‘Bayesian Statistics’,
Vol. 5, Oxford University Press, pp. 609–620.
Geweke, J. (1999), ‘Using simulation methods for bayesian econometric models: Inference,
development and communication’, Econometric Reviews 18, 1–126. with discussion.
Geweke,
J. (2005),
Contemporary Bayesian Econometrics and Statistics,
Wiley-
Interscience.
Geweke, J. and Whiteman, C. H. (2006), Bayesian forecasting, in Elliott et al. (2006),
chapter 1, pp. 3–80.
Giannone, D., Lenza, M. and Primiceri, G. E. (2012), Prior selection for vector autore-
gressions, Working Papers ECARES ECARES 2012-002, ULB – Universite Libre de
Bruxelles.
Giordini, P., Pitt, M. and Kohn, R. (2011), Bayesian inference for time series state space
models, in Clements and Hendry (2011), chapter 3, pp. 61–124.
Gupta, R. and Kabundi, A. (2010), ‘Forecasting macroeconomic variables in a small
open economy: A comparison between small- and large-scale model’, Journal of
Forecasting 29, 168–186.
Harvey, A. C. (1989), Forecasting, Structural Time Series Models and the Kalman Filter,
Cambridge University Press.
Highﬁeld, R. A. (1987), Forecasting with Bayesian State Space Models, PhD thesis, Grad-
uate School of Business, University of Chicago.
97

Jaroci´nski, M. (2010), ‘Conditional forecasts and uncertainty about forecast revisions in
vector autoregressions’, Economics Letters 108(3), 257 – 259.
Jaroci´nski, M. and Ma´ckowiak, B. (2011), Choice of variables in vector autoregressions.
Manuscript.
Jochmann, M., Koop, G. and Strachan, R. (2010), ‘Bayesian forecasting using stochas-
tic search variable selection in a var subject to breaks’, International Journal of
Forecasting 26(2), 326–347.
Kadiyala, K. R. and Karlsson, S. (1993), ‘Forecasting with generalized bayesian vector
autoregressions’, Journal of Forecasting 12, 365–378.
Kadiyala, K. R. and Karlsson, S. (1997), ‘Numerical methods for estimation and inference
in bayesian var-models’, Journal of Applied Econometrics 12, 99–132.
Karlsson, S. (2012), Conditional posteriors for the reduced rank regression model, Working
Papers 2012:11, ¨Orebro University Business School.
Kim, C. and Nelson, C. R. (1999), State Space Models with Regime Switching, MIT Press.
Kim, S., Shephard, N. and Chib, S. (1998), ‘Stochastic volatility: Likelihood inference
and comparison with arch models’, The Review of Economic Studies 65(3), 361–393.
Kleibergen, F. and van Dijk, H. K. (1994), ‘On the shape of the likelihood/posterior in
cointegration models’, Econometric Theory 1, 514–551.
Kloek, T. and van Dijk, H. K. (1978), ‘Bayesian estimates of equation system parameters:
An application of integration by monte carlo’, Econometrica 46, 1–19.
Koop, G. (2003), Bayesian Econometrics, John Wiley & Sons, Chichester.
Koop, G. (2010), Forecasting with medium and large bayesian vars, Technical Report WP
10-43, The Rimini Centre for Economic Analysis.
Koop, G. and Korobilis, D. (2009), ‘Bayesian multivariate time series methods for empir-
ical macroeconomics’, Foundations and Trends in Econometrics 3, 267–358.
Koop, G., Le´on-Gonz´alez, R. and Strachan, R. W. (2010), ‘Eﬃcient posterior simulation
for cointegrated models with priors on the cointegration space’, Econometric Reviews
29, 224–242.
Koop, G. and Potter, S. (2007), ‘Estimation and forecasting in models with multiple
breaks’, Review of Economic Studies 74, 763–789.
Koop, G., Strachan, R. W., van Dijk, H. K. and Villani, M. (2006), Bayesian approaches to
cointegration, in T. C. Mills and P. K., eds, ‘The Palgrave Handbook of Theoretical
Econometrics’, Vol. 1, Palgrave McMillan, chapter 25.
Korobilis, D. (2008), Forecasting in vector autoregressions with many predictors, in Chib
et al. (2008), pp. 403–431.
98

Korobilis, D. (forthcominga), ‘Hierarchical shrinkage priors for dynamic regressions with
many predictors’, International Journal of Forecasting .
Korobilis, D. (forthcomingb), ‘Var forecasting using bayesian variable selection’, Journal
of Applied Econometrics .
Litterman, R. B. (1979), Techniques of forecasting using vector autoregressions, Working
Paper 115, Federal Reserve Bank of Minneapolis.
Litterman, R. B. (1980), A bayesian procedure for forecasting with vector autoregressions,
mimeo, Massachusetts Institute of Technology.
Litterman, R. B. (1986), ‘Forecasting with bayesian vector autoregressions - ﬁve years of
experience’, Journal of Business & Economic Statistics 4, 25–38.
L¨utkepohl, H. (2006), Forecasting with VARMA models, in Elliott et al. (2006), chapter 6,
pp. 287–325.
Madigan, D. and York, J. (1995), ‘Bayesian graphical models for discrete data’, Interna-
tional Statistical Review 63, 215–232.
McNees, S. K. (1986), ‘Forecasting accuracy of alternative techniques: A comparison of
u.s. macroeconomic forecasts’, Journal of Business & Economic Statistics 4, 5–15.
Newey, W. K. and West, K. D. (1987), ‘A simple, positive semi-deﬁnite, heteroskedasticity
and autocorrelation consistent covariance matrix’, Econometrica 55, 703–708.
¨Osterholm, P. (2008a), ‘Can forecasting performance be improved by considering the
steady state? an application to swedish inﬂation and interest rate’, Journal of Fore-
casting 27(1), 41–51.
¨Osterholm, P. (2008b), ‘A structural bayesian var for model-based fan charts’, Applied
Economics 40(12), 1557–1569.
Pesaran, M. H., Petenuzzo, D. and Timmermann, A. (2006), ‘Forecasting time series
subject to multiple structural breaks’, Review of Economic Studies 73, 1057–1084.
Peters, G. W., Kannan, B., Lassock, B. and Mellen, C. (2010), ‘Model selection and
adaptive markov chain monte carlo for bayesian cointegrated var-models’, Bayesian
Analysis 5, 465–492.
Primiceri, G. E. (2005), ‘Time varying structural vector autoregressions and monetary
policy’, The Review of Economic Studies 72(3), 821–852.
Robert, C. P. and Casella, G. (1999), Monte Carlo Statistical Methods, Springer Verlag.
Robertson, J. C., Tallman, E. W. and Whiteman, C. H. (2005), ‘Forecasting using relative
entropy’, Journal of Money, Credit and Banking 37(3), 383–401.
Rothenberg, T. J. (1971), ‘Identiﬁcation in parametric models’, Econometrica 39, 577–
599.
99

Rubio-Ramirez, J. F., Waggoner, D. F. and Zha, T. (2010), ‘Structural vector autoregres-
sions: Theory of identiﬁcation and algorithms for inference’, The Review of Economic
Studies 77, 665–696.
Schwarz, G. (1978), ‘Estimating the dimension of a model’, The Annals of Statistics
6, 461–464.
Sims, C. A. (1980), ‘Macroeconomics and reality’, Econometrica 48, 1–48.
Sims, C. A. (1993), A nine-variable probabalistic macroeconomic forecasting model, in
J. H. Stock and M. W. Watson, eds, ‘Business Cycles, Indicators and Forecasting’,
University of Chicago Press, pp. 179–204.
Sims, C. A. and Zha, T. (1998), ‘Bayesian methods for dynamic multivariate models’,
International Econom Review 39, 949–968.
Smith, W. B. and Hocking, R. R. (1972), ‘Algorithm as 53: Wishart variate generator’,
Journal of the Royal Statistical Society. Series C (Applied Statistics) 21, 341–345.
Stock, J. H. and Watson, M. W. (2002), ‘Macroeconomic forecasting using diﬀusion in-
dexes’, Journal of Business & Economic Statistics 20, 147–162.
Stock, J. H. and Watson, M. W. (2006), Forecasting with many predictors, in Elliott et al.
(2006), chapter 10.
Strachan, R. (2003), ‘Valid bayesian estimation of the cointegrating error correction
model’, Journal of Business & Economic Statistics 21(1), 185–95.
Strachan, R. and Inder, B. (2004), ‘Bayesian analysis of the error correction model’,
Journal of Econometrics 123(2), 307–325.
Sugita, K. (2002), Testing for cointegration rank using bayes factors, Warwick Economic
Research Papers 654, University of Warwick.
Sugita, K. (2009), ‘A monte carlo comparison of bayesian testing for cointegration rank’,
Economics Bulletin 29(3), 2145–2151.
Theil, H. and Goldberger, A. S. (1960), ‘On pure and mixed statistical estimation in
economics’, International Economic Review 2, 65–78.
Tierny, L. (1994), ‘Markov chains for exploring posterior distributions’, The Annals of
Statistics 22, 1701–1762. with discussion.
Timmermann, A. (2006), Forecast combinations, in Elliott et al. (2006), chapter 4.
Verdinelli, I. and Wasserman, L. (1995), ‘Computing bayes factors using a generalization
of the savage-dickey density ratio’, Journal of the American Statistical Association
90(430), pp. 614–618.
Villani, M. (2000), Aspects of Bayesian Cointegration, PhD thesis, Stockholm University.
100

Villani, M. (2001), ‘Bayesian prediction with cointegrated vector autoregressions’, Inter-
national Journal of Forecasting 17, 585–605.
Villani, M. (2005), ‘Bayesian reference analysis of cointegration’, Economtric Theory
21, 326–357.
Villani, M. (2009), ‘Steady state priors for vector autoregressions’, Journal of Applied
Econometrics 24, 630–650.
Waggoner, D. F. and Zha, T. (1999), ‘Conditional forecasts in dynamic multivariate
models’, The Review of Economics and Statistics 81, 639–651.
Waggoner, D. F. and Zha, T. (2003a), ‘A Gibbs sampler for structural vector autoregres-
sions’, Journal of Economic Dynamics & Control 28, 349–366.
Waggoner, D. F. and Zha, T. (2003b), ‘Likelihood preserving normalization in multiple
equation models’, Journal of Econometrics 114(2), 329–347.
West, M. and Harrison, P. (1997), Bayesian Forecasting and Dynamic Models, 2nd ed.
edn, Springer.
Wright, J. H. (2010), Evaluating real-time var forecasts with an informative democratic
prior, Working Papers 10-19, Federal Reserve Bank of Philadelphia.
Yu, B. and Mykland, P. (1998), ‘Looking at Markov samplers through cusum path plots.
a simple diagnostic idea’, Statistics and Computing 8, 275–286.
Zellner, A. (1971), An Introduction to Bayesian Inference in Econometrics, John Wiley
& Sons.
Zha, T. (1999), ‘Block recursion and structural vector autoregressions’, Journal of Econo-
metrics 90(2), 291–316.
101

