Â 
http://www.oru.se/Institutioner/Handelshogskolan-vid-Orebro-universitet/Forskning/Publikationer/Working-papers/
 
 
Â 
 
 
 
WORKING PAPER 
Â 
 
 
12/2012 
 
Â 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Â 
Â 
Forecasting with Bayesian Vector Autoregressions 
 
 
 
Sune Karlsson 
Statistics 
 
 
 
ISSN 1403-0586 
 
 
Ã–rebro University School of Business  
701 82 Ã–rebro 
SWEDEN 
 
 

Forecasting with Bayesian Vector Autoregressions
Sune Karlssonâˆ—
Department of Statistics, Â¨Orebro University Business School
August 4, 2012
Abstract
Prepared for the Handbook of Economic Forecasting, vol 2
This chapter reviews Bayesian methods for inference and forecasting with VAR
models. Bayesian inference and, by extension, forecasting depends on numerical
methods for simulating from the posterior distribution of the parameters and spe-
cial attention is given to the implementation of the simulation algorithm.
JEL-codes: C11, C32, C53
Keywords: Markov chain Monte Carlo; Structural VAR; Cointegration; Condi-
tional forecasts; Time-varying parameters; Stochastic volatility; Model selection;
Large VAR
âˆ—Sune.Karlsson@oru.se

Contents
1
Introduction
1
2
Bayesian Forecasting and Computation
2
2.1
Bayesian Forecasting and Inference . . . . . . . . . . . . . . . . . . . . . .
2
2.1.1
Vector Autoregressions . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Bayesian Computation and Simulation . . . . . . . . . . . . . . . . . . . .
7
2.2.1
Markov chain Monte Carlo . . . . . . . . . . . . . . . . . . . . . . .
9
3
Reduced Form VARs
10
3.1
The Minnesota prior beliefs
. . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.1.1
Variations on the Minnesota prior . . . . . . . . . . . . . . . . . . .
12
3.2
Flexible prior distributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.2.1
The normal-Wishart prior . . . . . . . . . . . . . . . . . . . . . . .
14
3.2.2
The normal-diï¬€use and independent normal-Wishart priors . . . . .
16
3.2.3
A hierarchical prior for the hyperparameters . . . . . . . . . . . . .
17
3.3
The steady state VAR
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.4
Model speciï¬cation and choice of prior
. . . . . . . . . . . . . . . . . . . .
24
4
Structural VARs
26
4.1
â€Unrestrictedâ€ triangular structural form . . . . . . . . . . . . . . . . . . .
27
4.2
Homogenous restrictions on the structural form parameters . . . . . . . . .
29
4.3
Identiï¬cation under general restrictions . . . . . . . . . . . . . . . . . . . .
33
5
Cointegration
37
5.1
Priors on the cointegrating vectors
. . . . . . . . . . . . . . . . . . . . . .
38
5.2
Priors on the cointegrating space
. . . . . . . . . . . . . . . . . . . . . . .
41
5.3
Determining the cointegrating rank . . . . . . . . . . . . . . . . . . . . . .
46
6
Conditional forecasts
48
7
Time-varying parameters and stochastic volatility
51
7.1
Time-varying parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
7.2
Stochastic volatility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
8
Model and variable selection
61
8.1
Restricting the parameter matrices - SSVS . . . . . . . . . . . . . . . . . .
61
8.2
Selecting variables to model . . . . . . . . . . . . . . . . . . . . . . . . . .
67
8.2.1
Marginalized predictive likelihoods
. . . . . . . . . . . . . . . . . .
67
8.2.2
Marginal likelihoods via Bayes factors . . . . . . . . . . . . . . . . .
69
9
High Dimensional VARs
70
9.1
Factor augmented VAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
9.2
Large BVARs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
9.2.1
Reducing parameter uncertainty by shrinkage
. . . . . . . . . . . .
74
9.2.2
Selecting variables - conjugate SSVS
. . . . . . . . . . . . . . . . .
76
9.3
Reduced rank VAR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78

9.4
Predicting many variables
. . . . . . . . . . . . . . . . . . . . . . . . . . .
80
A Markov chain Monte Carlo Methods
82
A.1 Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
A.2 Metropolis-Hastings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
A.3 Autocorrelation in the Markov chain
. . . . . . . . . . . . . . . . . . . . .
85
A.4 Assessing convergence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
B State space models
88
B.1 Kalman ï¬lter
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
B.2 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
B.3 Simulation smoother . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
C Distributions
89

1
Introduction
Vector autoregressions (VARs) have become the workhorse model for macroeconomic fore-
casting. The initial use in economics was to a large degree motived by Sims (1980) critique
of the â€incredible restrictionsâ€ used by the large macroeconometric models developed in
the 1970s and much eï¬€ort was put into tools for policy analysis based on VAR models.
This role of the VAR model has to some degree been taken over by the current crop of
DSGE models, a new generation of theory based models which are â€“ at times â€“ ill at ease
with the data. The role of the VAR model as the baseline, serious, model for economic
forecasting is, however, unchallenged. The popularity stems in part from itâ€™s relative
simplicity, ï¬‚exibility and ability to ï¬t the data but, of course, also from itâ€™s success as a
forecasting device.
The ï¬‚exibility and ability to ï¬t the data comes from the rich parameterization of
VAR models brings with it a risk of overï¬tting the data, of imprecise inference and
large uncertainty about the future paths projected by the model.
This is essentially
the frequentist argument for Bayesian VAR models and one reason why Bayesian VAR
models forecast better than VARs estimated with frequentist techniques.
The widely
used Minnesota prior introduced by Litterman (1979) is a set of data centric prior beliefs
that shrinks the parameters towards a stylized representation of macroeconomic data
thereby reducing parameter uncertainty and improving forecast accuracy. The Bayesian
argument is diï¬€erent. The Minnesota prior captures widely held beliefs about the long
run properties of the data, properties that are not readily apparent in the short samples
typically used for estimation. Bayes theorem then provides the optimal way of combining
these two sources of information leading to sharper inference and more precise forecasts.
The development of eï¬ƒcient numerical techniques for evaluating posterior distributions is
also a contributing factor to the attractiveness of Bayesian methods. It is now possible to
tackle more complex problems under realistic assumptions when we no longer are limited
to problem formulations that lead to analytical solutions.
This chapter surveys Bayesian approaches to inference in VAR models with a focus on
forecasting. One important feature of the chapter is that it gathers many algorithms for
simulating from the posterior distribution of the parameters, some of which have not been
clearly stated previously. This provides the necessary tools for analyzing the posterior
and predictive distributions and forecast with the models and priors that are studied in
the chapter. Koop and Korobilis (2009) and DelNegro and Schorfheide (2011) provides
complementary reviews of Bayesian VAR models, Koop and Korobilis (2009) with a focus
on models that allows for time-varying parameters and stochastic volatility while DelNegro
and Schorfheide (2011) has the broader remit of Bayesian macroeconometrics.
Section 2 lays the foundations by placing the task of forecasting in a Bayesian context
and reviews modern simulation techniques for exploring posterior and predictive distri-
butions. Section 3 provides the basic building blocks for forecasting with Bayesian VAR
models by introducing the Minnesota prior beliefs in the context of reduced form VAR
models and reviews families of prior distributions that have been found useful for ex-
pressing the prior beliefs. The more general issue of model speciï¬cation is also discussed
and one important message that emerges is that, in line with a general conclusion in the
forecasting literature, simple methods works quite well.
The remaining sections can largely be read independently. Section 4 reviews Bayesian
1

analysis of a VAR in structural form (SVAR) and section 5 studies the vector error
correction (VECM) form of a VAR model. Both SVAR and VECM models have the
potential to improve forecast performance if the hard restrictions they impose on the model
are at least aproximately correct but they have seen relatively little use in forecasting
applications, in particular in their Bayesian ï¬‚avor.
This is partly because it is only
recently that satisfactory procedures for posterior inference in these models have become
available.
Section 6 consider forecasts conditional on future events. This can be a useful tool for
incorporating judgement and other late breaking information that is (perhaps due to the
slow release of data) not in the information set used by the model. In a policy setting
conditional forecasts are useful for what-if analysis and for producing forecasts that are
consistent with the current policy.
Section 7 relaxes the constant parameter assumption and shows how to allow for time-
varying parameters and stochastic volatility in Bayesian VAR models. There are encour-
aging studies that indicate that both time-varying parameters and stochastic volatility
can improve the forecast performance but both can also lead to a dramatic increase in
the number of parameters in a model. There is consequently a greater risk of overï¬t-
ting the data. The methods for model and variable selection discussed in Section 8 can
then be useful in addition to the Bayesian shrinkage that is routinely applied through
the prior. Section 8 provides tools both for selecting the variables to include as left hand
side variables in a VAR model and for reducing the number of parameters by eï¬€ectively
excluding some variables and lags from the right hand side. This touches on the issue
of model averaging and forecast combination which is not discussed here in spite of this
being a natural extension of the Bayesian framework for treating parameter uncertainty.
The reader is instead referred to Geweke and Whiteman (2006) and Timmermann (2006).
The ï¬nal section 9 considers the task of forecasting in a data rich environment where
several hundred potential predictors may be available. Recent work shows that Bayesian
VARs can be competitive in this setting as well and important recent developments are
reviewed.
2
Bayesian Forecasting and Computation
This section provides a brief overview of the underlying principles of Bayesian inference
and forecasting. See Geweke and Whiteman (2006) for a more complete discussion and,
for example, Gelman, Carlin, Stern and Rubin (2003), Geweke (2005) or Koop (2003) for
a text book treatment of Bayesian inference.
2.1
Bayesian Forecasting and Inference
The fundamental object in Bayesian forecasting is the (posterior) predictive distribution,
the distribution p (yT+1:T+H|YT) of future datapoints, yT+1:T+H =
 yâ€²
T+1, . . . , yâ€²
T+H
â€²
conditional on the currently observed data, YT = {yt}T
t=1 . By itself the predictive dis-
tribution captures all relevant information about the unknown future events. It is then
up to the forecaster or user of the forecast which features of the predictive distribution
are relevant for the situation at hand and should be reported as the forecast. This could,
2

for example, be the mean, mode or median of the predictive distribution together with a
probability interval indicating the range of likely outcomes.
Formally this is a decision problem which requires the speciï¬cation of a problem de-
pendent loss function, L
 a, yT+1:T+H

, where a is the action taken, the vector of real
numbers to report as the forecast, and yT+1:T+H represents the unknown future state
of nature. The Bayesian decision is to choose the action (forecast) that minimizes the
expected loss conditional on the available information YT,
E

L
 a, yT+1:T+H

|YT

=
Z
L
 a, yT+1:T+H

p (yT+1:T+H|YT) dyT+1:T+H.
For a given loss function and predictive distribution, p (yT+1:T+H|YT) the solution to the
minimization problem is a function of the data, a (YT) . For speciï¬c loss functions a (YT)
takes on simple forms. With quadratic loss function,
 a âˆ’yT+1:T+H
â€²  a âˆ’yT+1:T+H

, the
solution is the conditional expectation, a (YT) = E (yT+1:T+H|YT) , and with an absolute
value loss function, P |ai âˆ’Ï‰i| , the conditional mode.
It remains to specify the form of the predictive distribution. This requires the speciï¬-
cation of three diï¬€erent distributions that completes the description of the problem, the
distribution of the future observations conditional on unknown parameter values, Î¸, and
the observed data, p (yT+1:T+H|YT, Î¸) , the distribution of the observed data â€“ that is, the
model or likelihood â€“ conditional on the parameters, L (YT|Î¸) , and the prior distribution,
Ï€ (Î¸) , representing our prior notions about likely or â€reasonableâ€ values of the unknown
parameters, Î¸. In a time series and forecasting context, the likelihood L (YT|Î¸) usually
takes the form
L (YT|Î¸) =
TY
t=1
f (yt|Ytâˆ’1, Î¸)
with the history in Y1, Y2, . . . suitably extended to include initial observations that the
likelihood is conditional on.1 The distribution of future observations is of the same form,
f (yT+1:T+H|YT, Î¸) =
T+H
Y
t=T+1
f (yt|Ytâˆ’1, Î¸) .
With these in hand straightforward application of Bayes Rule yields the predictive distri-
bution as
p (yT+1:T+H|YT) = p (yT+1:T+H, YT)
m (YT)
=
R
f (yT+1:T+H|YT, Î¸) L (YT|Î¸) Ï€ (Î¸) dÎ¸
R
L (YT|Î¸) Ï€ (Î¸) dÎ¸
.
(1)
In practice an intermediate step through the posterior distribution of the parameters,
p (Î¸|YT) =
L (YT|Î¸) Ï€ (Î¸)
R
L (YT|Î¸) Ï€ (Î¸) dÎ¸ âˆL (YT|Î¸) Ï€ (Î¸) ,
(2)
is used with the predictive distribution given by
p (yT+1:T+H|YT) =
Z
f (yT+1:T+H|YT, Î¸) p (Î¸|YT) dÎ¸.
(3)
1It is, of course, in many cases also possible to complete the likelihood with the marginal distribution
for the ï¬rst, say p, observations.
3

Note that the latter form of the predictive distribution makes it clear how Bayesian fore-
casts accounts for both the inherent uncertainty about the future embodied by f (yT+1:T+H|YT, Î¸)
and the uncertainty about the true parameter values described by the posterior distribu-
tion p (Î¸|YT) .
While the posterior distribution of the parameters may be available in closed form
in special cases when conjugate prior distributions are used closed form expressions for
the predictive distribution are generally unavailable when lead times greater than 1 are
considered. This makes the form (3) of the predictive distribution especially attractive.
Marginalizing out the parameters of the joint distribution of yT+1:T+H and Î¸ analyti-
cally may be diï¬ƒcult or impossible, on the other hand (3) suggests a straightforward
simulation scheme for the marginalization. Supposing that we can generate random num-
bers from the posterior p (Î¸|YT), for each draw of Î¸ generate a sequence of draws of
yT+1, . . . , yT+H by repeatedly drawing from f (yt|Ytâˆ’1, Î¸) and adding the draw of yt to
the conditioning set for the distribution of yt+1. This gives a draw from the joint distribu-
tion of (Î¸, yT+1, . . . , yT+H) conditional on YT and marginalization is achieved by simply
discarding the draw of Î¸. Repeating this R times gives a sample from the predictive dis-
tribution that can be used to estimate E (yT+1:T+H|YT) or any other function or feature
a (YT) of the predictive distribution of interest.
The denominator in (1) and (2),
m (YT) =
Z
L (YT|Î¸) Ï€ (Î¸) dÎ¸,
(4)
is known as the marginal likelihood or prior predictive distribution and plays a crucial
role in Bayesian hypothesis testing and model selection. Consider two alternative models,
M1 and M2, with corresponding likelihoods L (YT|Î¸1, M1) , L (YT|Î¸2, M2) and priors
Ï€ (Î¸1|M1) , Ï€ (Î¸2|M2) . Supposing that one of M1 and M2 is the true model but that we
are not certain which of the competing hypothesis or theories embodied in the models is
the correct one we can assign prior probabilities, Ï€ (M1) and Ï€ (M2) = 1 âˆ’Ï€ (M1) , that
each of the models is the correct one. With these in hand Bayes Rule yields the posterior
probabilities that the models are correct as
p (Mi) =
m (YT|Mi) Ï€ (Mi)
m (YT|M1) Ï€ (M1) + m (YT|M2) Ï€ (M2)
(5)
with m (YT|Mi) =
R
L (YT|Î¸i, Mi) Ï€ (Î¸i|Mi) dÎ¸i. The posterior odds for model 1 against
model 2 is given by
p (M1)
p (M2) = m (YT|M1) Ï€ (M1)
m (YT|M2) Ï€ (M2) = m (YT|M1)
m (YT|M2) Ã— Ï€ (M1)
Ï€ (M2)
the Bayes factor BF1,2 = m (YT|M1) /m (YT|M2) comparing M1 to M2 times the prior
odds. Model choice can be based on the posterior odds but it is also common to use
the Bayes factors directly, implying equal prior probabilities. The Bayes factor captures
the data evidence and can be interpreted as measuring how much our opinion about the
models have changed after observing the data. The choice of model should of course
take account of the losses associated with making the wrong choice. Alternatively, we can
avoid conditioning on one single model being the correct one by averaging over the models
4

with the posterior model probabilities as weight. That is, instead of basing our forecasts
on the predictive distribution p (yT+1:T+H|YT, M1) and conditioning on M1 being the
correct model we conduct Bayesian Model Averaging (BMA) to obtain the marginalized
(with respect to the models) predictive distribution
p (yT+1:T+H|YT) = p (yT+1:T+H|YT, M1) Ï€ (M1) + p (yT+1:T+H|YT, M2) Ï€ (M2)
which accounts for both model and parameter uncertainty.
The calculations involved in (5) are non-trivial and the integral
R
L (YT|Î¸i, Mi) Ï€ (Î¸i|Mi) dÎ¸i
is only well deï¬ned if the prior is proper. That is, if
R
Ï€ (Î¸i|Mi) dÎ¸i = 1. For improper
priors, such as a uniform prior on the whole real line, the integral is not convergent and
the scale is arbitrary. For the uniform prior we can write Ï€ (Î¸i|Mi) = ki and it follows
that m (YT) âˆki and the Bayes factors and posterior probabilities are arbitrary.2 There
is, however, one circumstance where improper prior can be used. This is when there
are parameters that are common to all models, for example an error variance. We can
then partition Î¸i =

eÎ¸i, Ïƒ2
and use proper priors for eÎ¸i and an improper prior, such as
Ï€ (Ïƒ2) âˆ1/Ïƒ2, for the variance since the common scale factor cancels in the calculation
of posterior model probabilities and Bayes factors.
2.1.1
Vector Autoregressions
To illustrate the concepts we consider the VAR model with m variables
yâ€²
t =
p
X
i=1
yâ€²
tâˆ’iAi + xâ€²
tC + uâ€²
t
(6)
= zâ€²
tÎ“ + uâ€²
t
with xt a vector of d deterministic variables, zâ€²
t =
 yâ€²
tâˆ’1, . . . , yâ€²
tâˆ’p, xâ€²
t

a k = mp + d
dimensional vector and Î“ =
 Aâ€²
1, . . . Aâ€²
p, Câ€²â€² a k Ã— m matrix and normally distributed
errors, ut âˆ¼N (0, Î¨) . That is, f (yt|Ytâˆ’1, Î¸) = N (yt; zâ€²
tÎ“, Î¨) . For simplicity we take the
prior to be uninformative (diï¬€use), a uniform distribution for Î“ and a Jeï¬€reysâ€™ prior for
Î¨,3
Ï€ (Î“, Î¨) âˆ|Î¨|âˆ’(m+1)/2 .
Using (2) we see that the joint posterior distribution of Î“ and Î¨ is proportional to the
likelihood function times the prior. Stacking the data in the usual way we can write the
model as
Y = ZÎ“ + U
2Note
that
this
does
not
aï¬€ect
the
posterior
distribution
as
long
as
the
integral
R
L (YT |Î¸i, Mi) Ï€ (Î¸i|Mi) dÎ¸i is convergent since the arbitrary scale factor cancels in (2).
3This is an improper prior and the use of improper prior distributions is not always advisable as this
can lead to improper posterior distributions. In the normal regression model with this prior the posterior
will be proper if the matrix of explanatory variables has full column rank, i.e. when the OLS estimate is
unique.
5

and the likelihood as
L (Y|Î“, Î¨) = (2Ï€)âˆ’mT/2 |Î¨|âˆ’T/2 exp

âˆ’1
2
X
(yâ€²
t âˆ’zâ€²
tÎ“) Î¨âˆ’1 (yâ€²
t âˆ’zâ€²
tÎ“)â€²

(7)
= (2Ï€)âˆ’mT/2 |Î¨|âˆ’T/2 exp

âˆ’1
2 tr

(Y âˆ’ZÎ“) Î¨âˆ’1 (Y âˆ’ZÎ“)â€²
= (2Ï€)âˆ’mT/2 |Î¨|âˆ’T/2 exp

âˆ’1
2 tr

Î¨âˆ’1 (Y âˆ’ZÎ“)â€² (Y âˆ’ZÎ“)

where Y and U are T Ã— m matrices and Z is T Ã— k. Adding and subtracting ZbÎ“ for
bÎ“ = (Zâ€²Z)âˆ’1 Zâ€²Y, the OLS estimate, and multiplying with the prior we have the joint
posterior as
p (Î“, Î¨|YT) âˆ|Î¨|âˆ’T/2 exp

âˆ’1
2 tr

Î¨âˆ’1 
Y âˆ’ZbÎ“
â€² 
Y âˆ’ZbÎ“

Ã— exp

âˆ’1
2 tr

Î¨âˆ’1 
Î“ âˆ’bÎ“
â€²
Zâ€²Z

Î“ âˆ’bÎ“

|Î¨|âˆ’(m+1)/2 .
Focusing on the part involving Î“ and noting that
tr

Î¨âˆ’1 
Î“ âˆ’bÎ“
â€²
Zâ€²Z

Î“ âˆ’bÎ“

= (Î³ âˆ’bÎ³)â€²  Î¨âˆ’1 âŠ—Zâ€²Z

(Î³ âˆ’bÎ³)
(8)
for Î³ = vec (Î“) and bÎ³ = vec

bÎ“

=

Im âŠ—(Zâ€²Z)âˆ’1 Zâ€²
y4 we recognize this as a the
kernel of a multivariate normal distribution conditional on Î¨ with mean bÎ³ and variance-
covariance matrix Î¨âŠ—(Zâ€²Z)âˆ’1 ,
Î³|YT, Î¨ âˆ¼N

bÎ³, Î¨âŠ—(Zâ€²Z)âˆ’1
.
With the special Kronecker structure of the variance-covariance matrix this is a matric-
variate normal5 distribution for Î“ and we can also write the conditional posterior as
Î“|YT, Î¨ âˆ¼MNkm

bÎ“, Î¨, (Zâ€²Z)âˆ’1
. Integrating out Î³ from the joint posterior is triv-
ial using the properties of the normal distribution and we have the marginal posterior
distribution for Î¨ as
p (Î¨|YT) âˆ|Î¨|âˆ’(T+m+1âˆ’k)/2 exp

âˆ’1
2 tr

Î¨âˆ’1S

with S =

Y âˆ’ZbÎ“
â€² 
Y âˆ’ZbÎ“

. This can be recognized as the kernel of an inverse
Wishart distribution with T âˆ’k degrees of freedom,
Î¨|YT âˆ¼iWm (S,T âˆ’k) .
4bÎ³ is the GLS estimate in the univariate regression model for y = vec (Y) = (Im âŠ—Z) Î³ + u
with V (u) = Î¨ âŠ—IT . That is bÎ³
=
h
(Im âŠ—Z)â€² (Î¨ âŠ—IT )âˆ’1 (Im âŠ—Z)
iâˆ’1
(Im âŠ—Z)â€² (Î¨ âŠ—IT )âˆ’1 y =
 Î¨âˆ’1 âŠ—Zâ€²Z
âˆ’1  Î¨âˆ’1 âŠ—Zâ€²
y =
h
Î¨ âŠ—(Zâ€²Z)âˆ’1i  Î¨âˆ’1 âŠ—Zâ€²
y =
h
Im âŠ—(Zâ€²Z)âˆ’1 Zâ€²i
y.
5See Appendix C for a review of some multivariate distributions.
6

We refer to the joint posterior of Î“ and Î¨ as a normal-Wishart distribution.
Alternatively, we can integrate out Î¨ of the joint posterior. With the Kronecker vari-
ance matrix of the conditional normal distribution this yields a matricvariate t-distribution
with T âˆ’k degrees of freedom as the marginal posterior for Î“,
Î“|YT âˆ¼Mtkm (bÎ³, Zâ€²Z, S, T âˆ’k) .
(9)
This is the natural generalization of the scalar variance case where x|Ïƒ âˆ¼N (Âµ, Ïƒ2V) with
Ïƒâˆ’2 Gamma distributed with shape parameter v/2 and scale parameter 1/2 (or Ï‡2 with
v degrees of freedom) yields a marginal t-distribution for x with v degrees of freedom.
For later reference note that the product of the prior and likelihood (7) has the form
of an inverse Wishart distribution for Î¨ conditional on Î“,
p (Î¨|YT, Î“) âˆ|Î¨|âˆ’(T+m+1)/2 exp

âˆ’1
2 tr

Î¨âˆ’1 (Y âˆ’ZÎ“)â€² (Y âˆ’ZÎ“)

Î¨|YT, Î“ âˆ¼iW
 (Y âˆ’ZÎ“)â€² (Y âˆ’ZÎ“) , T

.
Turning to the forecasts, recursive substitution in (6) with p = 2 yields
yâ€²
T+1 = yâ€²
TA1 + yâ€²
Tâˆ’1A2 + xâ€²
T+1C + uâ€²
T+1
yâ€²
T+2 = yâ€²
T
 A2
1 + A2

+ yâ€²
Tâˆ’1A2A1 + xâ€²
T+2C + xâ€²
T+1CA1+uâ€²
T+2 + uâ€²
T+1A1
etc. The one-step ahead predictive distribution for yâ€²
T+1 can be shown to be matricvari-
ate t, Mt1m

zâ€²
T+1bÎ“,
 1 + zâ€²
T+1 (Zâ€²Z)âˆ’1 zT+1
âˆ’1 , S, T âˆ’k

. For higher lead times we have
increasingly non-linear functions of the parameters and no closed form expressions for
the predictive distribution are available. Instead the simulation scheme for generating a
sample from the predictive distribution described above can be used. Simulating from
the posterior and predictive distributions is particularly straightforward in this case and
the procedure for simulating from the predictive distribution is given as Algorithm 1.
2.2
Bayesian Computation and Simulation
Having a simulated sample, ey(j)
T+1, . . . ey(j)
T+H, of size R from the predictive distribution in
hand it is straightforward to estimate features, such as probability intervals, expectations,
etc., of the predictive distribution that we wish to report. An estimate of the minimum
mean square error (MSE) h period ahead forecast, yT (h) = E (yT+h|YT) , is given by the
simple average of the simulated forecasts,
byT (h) = 1
R
R
X
j=1
ey(j)
T+h.
(11)
With direct sampling and hence iid draws, as in the previous section, this is guaranteed
to be a consistent and asymptotically normal estimator if V (yT+h|YT) exists,
âˆš
R (byT (h) âˆ’yT (h))
dâ†’N (0, V (yT+h|YT)) .
Asymptotically motivated error bounds are thus readily available as 1 âˆ’Î± conï¬dence
intervals. Analogous results apply to any function of yT+h with ï¬nite second moment.
7

Algorithm 1 Simulating the predictive distribution with a normal-Wishart posterior
For j = 1, . . . R
1. Generate Î¨(j) from the marginal posterior Î¨|YT âˆ¼iWm (S,T âˆ’k) distribution
using, e.g. the algorithm of Geweke (1988).
2. Generate Î“(j) from the conditional posterior Î“|YT, Î¨(j)âˆ¼N

bÎ“, Î¨(j), (Zâ€²Z)âˆ’1
3. Generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
(10)
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
Note that it only a factor P of Î¨(j) = PPâ€² is needed for step 3 and steps 1 and 2
can be replaced by Algorithm 22 which avoids the explicit computation of Î¨(j).
Estimates of prediction intervals are readily obtained by selecting the appropriate
quantiles from the simulated predictive distribution. Let eyT+h,(i) denote the ith order
statistic, a 1 âˆ’Î± prediction interval is then give by
 eyT+h,(l), eyT+h,(u)

for l = âŒŠRÎ±/2âŒ‹
and u = âŒŠR (1 âˆ’Î±/2)âŒ‹where âŒŠÂ·âŒ‹denotes the integer part. eyT+h,(l) is an estimate of the
Î±/2 quantile Î¾Î±/2 of the predictive distribution, assessing the precision of this estimate
is somewhat more involved than for simple averages. For continuous distributions f (x),
the sample order statistic X(m) for m = âŒŠnqâŒ‹is a consistent and asymptotically normal
estimator of the population quantile Î¾q but the asymptotic variance depends on the under-
lying distribution, âˆšn
 X(m) âˆ’Î¾q

dâ†’N
 0, q (1 âˆ’q) /f (Î¾q)2
, and requires an estimate of
the density at Î¾q in order to be operational.
An alternative procedure based on order statistics can be used to produce distribution
free conï¬dence intervals for the population quantile. We seek order statistics X(r) and
X(s) that satisï¬es P
 X(r) < Î¾q < X(s)

â‰ˆ1 âˆ’Î±. Noting that the probability statement
P
 X(r) < Î¾q < X(s)

is equivalent to the statement
P (at least r but no more than s âˆ’1 observations satisfy Xi < Î¾q)
this can be evaluated as a Binomial probability,
P
 X(r) < Î¾q < X(s)

=
sâˆ’1
X
k=r
n
k

qk (1 âˆ’q)nâˆ’k ,
and for small n it is straightforward to determine values of r and s that gives (approx-
imately) the desired conï¬dence level. For large n the Binomial distribution can be ap-
8

proximated by a normal distribution and r and s obtained as
r =
j
nq âˆ’z1âˆ’Î±/2
p
nq (1 âˆ’1)
k
s =
l
nq + z1âˆ’Î±/2
p
nq (1 âˆ’1)
m
.
2.2.1
Markov chain Monte Carlo
In general a simulation strategy similar to the one discussed in section 2.1 can be devised to
generate a sample from the predictive distribution. The main diï¬ƒculty is how to generate
draws from the posterior distribution of the parameters when, unlike Algorithm 1, it is
not possible to sample directly from the posterior. The two most common procedures
for solving this problem is importance sampling (Kloek and van Dijk (1978) and Geweke
(1989)) and Markov chain Monte Carlo (MCMC). Here we will focus on MCMC methods
as these are, in general, quite straightforward to implement with VAR models. Geweke
and Whiteman (2006), Chib and Greenberg (1995) and Geweke (1999) gives a more in-
depth discussion and book length treatments include Gamerman (1997) and Robert and
Casella (1999).
The idea behind MCMC techniques is to construct a Markov chain for the parameters
Î¸ which has the posterior distribution as itâ€™s (unique) stationary distribution and fulï¬lls
the additional requirement that we can generate random number from the conditional
distribution, f
 Î¸(j+1)|Î¸(j)
that deï¬nes the transition kernel. If the initial draw, Î¸(0),
could somehow be drawn from the posterior distribution, all the following draws will also
be from the posterior distribution by virtue of this being the stationary distribution of
the chain. But this is, of course, not possible (or we would not need to resort to MCMC
methods) and the issue of convergence becomes important. Will the distribution of the
draws from the Markov chain converge to the posterior distribution if we start the chain
at an arbitrary point in the parameter space? And if so, how many draws are required
before the distribution of the draws is â€close enoughâ€ to the posterior distribution?
A precise answer to the ï¬rst question involves highly technical conditions (see Tierny
(1994)). It is, however, possible to state stronger conditions that are suï¬ƒcient for conver-
gence and relatively easy to check (e.g. Geweke (2005, section 4.5)). One such condition
is that, loosely speaking, P
 Î¸(j+1) âˆˆA|Î¸(j)
> 0 for all Î¸(j) and any set A with positive
probability under the posterior distribution. The Markov chain is then ergodic and allows
consistent estimation of posterior quantities. The second question does not have a precise
answer and would be unimportant if we could generate an inï¬nite number of draws from
the chain. In practice we will only have a ï¬nite number of draws available and including
draws from the beginning of the chain, before it has converged to the posterior distri-
bution, can give very bad estimates of posterior quantities. As a practical matter it is
thus important to discard a suï¬ƒciently large number, B, of initial draws (the burn-in).
Lacking a precise answer, the choice of the size of the burn-in is subjective and it is better
to err on the side of caution. Diagnostics that are useful in determining B are discussed
below.
The performance of the Markov chain and the precision of estimates is related to the
issue of convergence. Even if the Markov chain is convergent it might move very slowly
through the parameter space (mix slowly) with high autocorrelation between the draws
and a very large number of draws might be needed in order to explore the parameter space.
9

Even a well performing Markov chain will by construction have some, typically positive,
autocorrelation in the draws which tends to impact the precision of estimates negatively
with a larger variance of estimates of, say the posterior mean, than if direct sampling
had been possible. For an assessment of the precision of estimates and probabilistic error
bounds a central limit theorem is needed. This in turn requires a rate condition on the
speed of convergence to the posterior distribution. Let g (Î¸) be an arbitrary function of the
parameters and g the average over R draws from the chain. If the chain is geometrically
ergodic then
âˆš
R (g âˆ’E (g|YT))
dâ†’N
 0, Ïƒ2
MC

if E

g2+Î´|YT

< âˆfor Î´ > 0 and
Ïƒ2
MC = V (g|YT) + 2
âˆ
X
k=1
Cov

g
 Î¸(j)
, g
 Î¸(j+k)
|YT

.
(12)
If, in addition, the chain is uniformly ergodic then the result holds for Î´ = 0.
Technical details for the implementation of Gibbs and Metropolis-Hastings samplers
are given in Appendix A, including how to assess convergence and how to estimate the
variance, Ïƒ2
MC, of the Monte Carlo estimate. It should be clear to the reader that these
methods come with a health warning: Naive use without careful assessment of the behavior
and convergence property of the Markov chain may lead to completely misleading results.
3
Reduced Form VARs
For forecasting purposes reduced form Bayesian VARs, that is models that essentially
leaves the parameter matrices and the variance-covariance matrix of ut in the VAR yâ€²
t =
Pp
i=1 yâ€²
tâˆ’iAi + xâ€²
tC + uâ€²
t unrestricted have proven to be quite successful. While having a
long tradition in time series analysis their use in economic forecasting was limited until
Sims (1980) inï¬‚uential critique of the â€incredibleâ€ identifying restrictions used in the large
scale macroeconometric models of the day. Instead Sims argued in favour of VAR-models
built essentially on considerations of the time series properties of the data. While being
powerful forecast devices that can ï¬t the data well, VAR-models may require relatively
large lag lengths p in order to match the time series properties of the data which, with
the many parameters to estimate can cause poor forecasting performance. One possible
solution to the problems caused by the rich parameterization is to consider the larger
class of VARMA-models (see LÂ¨utkepohl (2006)) which may be able to represent the data
in a more parsimonious fashion.
3.1
The Minnesota prior beliefs
Taking a diï¬€erent route, Litterman(1979, 1980) argued from a largely frequentist view
point, using the analogy with Ridge regression and shrinkage estimation, that the precision
of estimates and forecasting performance can be improved by incorporating â€restrictionsâ€
in the form of a prior distribution on the parameters. Littermanâ€™s prior formulation is
essentially based on stylized facts about his data, macroeconomic variables for the US that
could be well characterized by unit root processes and he proposed shrinking towards a
univariate random walk for each variable in the VAR.
10

Recall the multivariate regression formulation for the VAR with m variables, yâ€²
t =
zâ€²
tÎ“ + uâ€²
t for zâ€²
t =
 yâ€²
tâˆ’1, . . . , yâ€²
tâˆ’p, xâ€²
t

and elements Î³ij of Î“. The shrinkage towards uni-
variate random walks corresponds the setting the prior mean of Î“ to
Î³ij = E (Î³ij) =
 1, ï¬rst own lag, i = j
0, i Ì¸= j
.
(13)
Litterman suggested applying a harder shrinkage towards zero for longer lags, reï¬‚ecting
the prior notion that more distant observations are less inï¬‚uential. In addition, a diï¬€er-
ent amount of shrinkage is applied to lags of the dependent variable than to lags of other
variables in the same equation. Typically more shrinkage is applied to lags of other vari-
ables to reinforce the univariate random walk nature of the prior. Speciï¬cally, Litterman
suggested setting the prior standard deviations to
Ï„ij = sd (Î³ij) =
ï£±
ï£²
ï£³
Ï€1/lÏ€3, lag l of the dependent variable, i = (l âˆ’1) m + j
(Ï€1Ï€2sj) / (lÏ€3sr) , lag l of variable r Ì¸= j, i = (l âˆ’1) m + r
âˆ, deterministic variables, i = mp + 1, . . . , k
.
(14)
Here sj/sr is a scale factor accounting for the diï¬€erent variances of the dependent and
explanatory variables, Ï€1 is referred to as the â€overall tightnessâ€, Ï€2 the â€relative tightness
of other variablesâ€ and Ï€3 the â€lag decay rateâ€. The inï¬nite standard deviations for the
coeï¬ƒcients on the deterministic variables xt corresponds to an improper uniform prior on
the whole real line and could, without aï¬€ecting the results, be replaced with an arbitrary
large value to obtain a proper prior. The prior is completed by using independent normals
for each regression coeï¬ƒcient on the lags, Î³ij âˆ¼N

Î³ij, Ï„ 2
ij

.
To reduce the computational burden Litterman proceeded to estimate the VAR equa-
tion by equation rather than as a system of equations. The likelihood is normal and
the error variances are assumed to be known, V (utj) = s2
j, where s2
j is the OLS resid-
ual variance for equation j in the VAR or a univariate autoregression for variable j.
Equation by equation estimation is, in fact, appropriate if the variance of ut is diagonal,
Î¨ = diag (s2
1, . . . , s2
m) but, as noted by Litterman, suboptimal if the error terms are cor-
related. Taking the error variances to be known (although data based) is, of course, also
a simpliï¬cation motivated by computational expediency.
For computational purposes, as well as a way to think about the prior in terms of
implications for the data, it is useful to note that the prior Î³ij âˆ¼N

Î³ij, Ï„ 2
ij

can be
restated as
sj
Ï„ij
Î³ij = sj
Ï„ij
Î³ij + euij
where euij âˆ¼N
 0, s2
j

. The prior information for equation j can thus be written as pseudo
data,
rj = RjÎ³j + euj
with element i of rj set to Î³ijsj/Ï„j and element r, s of Rj zero for r Ì¸= s and sj/Ï„i,j for
r = s = 1, . . . , mp. One can then apply the mixed estimation technique of Theil and
Goldberger (1960), that is apply OLS to the augmented regression equation
 yj
rj

=
 Z
Rj

Î³j +
 uj
euj

(15)
11

with known variance error variance s2
j. This yields the estimate
Î³j =
 Zâ€²Z + Râ€²
jRj
âˆ’1  Zâ€²y + Râ€²
jr

with variance
Vj = s2
j
 Zâ€²Z + Râ€²
jRj
âˆ’1
which corresponds to the posterior mean and variance, Î³j|YT âˆ¼N
 Î³j, Vj

under the
assumption that uj âˆ¼N
 0, s2
jI

with an (improper) normal prior for Î³j with mean
Î³j and precision (inverse variance)
1
s2
j Râ€²
jRj. To see this note that rj = RjÎ³j and that
applying the Bayesian calculations directly leads to Vj =

1
s2
j Zâ€²Z+ 1
s2
j Râ€²
jRj
âˆ’1
and Î³j =
V

1
s2
j Zâ€²ZbÎ³ + 1
s2
j Râ€²
jRjÎ³

= V

1
s2
j Zâ€²yj + 1
s2
j Râ€²
jrj

for bÎ³ = (Zâ€²Z)âˆ’1 Zâ€²yj.
It remains to specify the prior hyperparameters Ï€1, Ï€2 and Ï€3. Litterman(1979, 1980)
conducted several exercises to evaluate the eï¬€ect of the hyperparameters on the out of
sample forecast performance. Suitable choices for his data appear to be Ï€1 â‰ˆ0.2, Ï€2 â‰ˆ0.2
and Ï€3 = 1. These are also close to the hyperparameters used in true out of sample forecast
results reported in Litterman (1986).
The actual forecasts produced by Litterman were not based on the predictive distribu-
tion (3), in an additional bow to the limited computation resources of the time Litterman
approximated the mean of the predictive distribution by calculating the forecasts using
the posterior means Î³j of the parameters and the chain rule of forecasting.
In the remainder of this chapter we will refer to priors with moments similar to (13)
and (14) as Minnesota type priors or as priors based on the Minnesota prior beliefs.
The term Litterman prior is reserved for the combination of these prior beliefs with the
assumption of a diagonal and known error variance matrix.
Forecasting performance
Using the Minnesota prior and the forecasting procedure
outlined above Litterman started issuing monthly forecasts from a six variable VAR with
real GNP, the GNP price deï¬‚ator, real business ï¬xed investments, the 3-month treasury
bill, the unemployment rate and the money supply in 1980. Five years later, and with the
model essentially unchanged Litterman (1986) and McNees (1986) report on the forecast
accuracy of these true out of sample forecasts compared to commercial forecasts based on
large scale macroeconometric models. There is no clear winner in this comparison, the
BVAR forecasts dominated for the real variables (real GNP, investments and unemploy-
ment) but were among the worst for inï¬‚ation and the T-bill rate.
3.1.1
Variations on the Minnesota prior
Many variations on the Minnesota prior have been suggested, common ones include
â€¢ Stationary variables: For variables believed to be stationary the prior mean on the
ï¬rst lag can be set to a value less than 1, for example Î³jj = 0.9 if the variable is
believed to be relatively persistent.
â€¢ Deterministic variables: Set the prior standard deviations to Ï„ij = Ï€1Ï€4sj, this has
the advantage of leading to a proper prior for the coeï¬ƒcients on deterministic vari-
ables while still being uninformative about Î³ij by setting Ï€4 (moderately) large.
12

â€¢ â€Exogenousâ€ variables: Set the prior standard deviation to Ï„ij = (Ï€1Ï€5sj) / (lÏ€3sr) ,
for lag l of the â€endogenousâ€ variable r in the equation for the â€exogenousâ€ depen-
dent variable j, i = (l âˆ’1) m+r. This is, for example, useful when modelling a small
open economy with â€rest of the worldâ€ variables included in the model. Forecasting
is simpliï¬ed if these variables are included in yt as no external forecasts are needed.
Setting Ï€5 small shrinks Î³ij aggressively towards zero and allows us to express that
the rest of the world variables are essentially exogenous to the domestic economy.
â€¢ Sum of coeï¬ƒcients prior: This prior (introduced by Doan, Litterman and Sims
(1984)) expresses the prior notion that the sum of coeï¬ƒcients on own lags is 1 and
the sum of coeï¬ƒcients on the lags of each of the other variables is 0 as well as the
idea that the recent average of the variable should be a reasonable forecast. To
implement this add m rows to Rj which are zero except for the p positions in the ith
row corresponding to variable i = 1, . . . , m, i.e. row i is given by
 wâ€²
i âŠ—jâ€²
p, 0

where
the zeros correspond to the deterministic variables, element i of wi is y0,isi/ (Ï€1Ï€6sj)
for y0i = 1
p
P0
t=1âˆ’p yt,i the average of the initial conditions for variable i and the re-
maining mâˆ’1 elements zero, jp is a pÃ—1 vector of ones. In addition add m elements
to rj with the jth element equal to y0,j/ (Ï€1Ï€6) . The prior induces correlation be-
tween the coeï¬ƒcients on the same variable (the prior precision
1
s2
j Râ€²
jRj is no longer
a diagonal matrix) and forces the model towards a random walk with possible drift
for variable j as Ï€6 â†’0.
â€¢ Dummy initial observations prior: Add a row
 yâ€²
0 âŠ—jâ€²
p, xâ€²
0

/ (Ï€1Ï€7sj) to Rj and
y0j/ (Ï€1Ï€7sj) to rj. This prior also implies that the initial observations is a good
forecast without enforcing speciï¬c parameter values and induces prior correlation
among all parameters in the equation. Sims (1993) argues that the dummy initial
observations prior is preferable to the sum of coeï¬ƒcients prior. As Ï€7 â†’0 the prior
implies that either all variables are stationary with mean y0 or that there are unit
root components without drift (if there are no trends in xt).
3.2
Flexible prior distributions
The basic setup of Litterman has been generalized in several directions, attempting to
relax some of the more restrictive assumptions that were motivated by the computational
limitations of the time or that allows diï¬€erent ways of expressing the prior beliefs. Com-
mon to these works is that they maintain the basic ï¬‚avour of the Minnesota prior as a
data centric speciï¬cation that embodies stylized facts about the time series properties of
the data.
Kadiyala and Karlsson (1993,1997) relaxes the assumption of a known diagonal error
variance-covariance matrix, Î¨, and studies the eï¬€ect of varying the family of distribution
used to parameterize the prior beliefs. They considered the diï¬€use prior (which we have
already encountered in section 2.1.1), the conjugate normal-Wishart prior, the normal-
diï¬€use prior and an adaption of the extended natural conjugate (ENC) prior originally
proposed by Dr`eze and Morales (1976) in the context of simultaneous equation models.
Kadiyala and Karlsson (1993) focuses on the forecasting performance and conducts three
small forecasting horse races comparing the forecasting performance of the â€newâ€ priors
with the Minnesota prior and forecasts based on OLS estimates. With the exception of
13

the diï¬€use prior the priors are speciï¬ed to embody prior beliefs about Î“ that are similar
to the Minnesota prior. With the Minnesota prior and OLS the forecasts are calculated
using the chain rule based whereas Monte Carlo methods are used to evaluate the expected
value of the predictive distribution with the other priors. There is no clear cut winner,
priors that allow for correlation between equations tend to do better.
Kadiyala and Karlsson (1997) studies the same four priors but this time the focus
is on the implementation and eï¬ƒciency of Monte Carlo methods for evaluating the ex-
pected value of the predictive distribution. Importance samplers and Gibbs samplers are
developed for the posterior distributions arising from the normal-diï¬€use and ENC priors.
Kadiyala and Karlsson concludes that Gibbs sampling is more eï¬ƒcient than importance
sampling, in particular for larger models. The evaluation is done in the context of two
forecasting exercises, one using a small bivariate model for the Swedish industrial produc-
tion index and unemployment rate and one using the seven variable model of Litterman
(1986). In terms of forecast performance there is no clear winner, the diï¬€use, normal-
Wishart priors and forecasts based on the OLS estimates does best with the Swedish data
and the Minnesota, normal-Wishart and normal-diï¬€use does best with the Litterman
model. In the following we will focus on the normal-Wishart and normal-diï¬€use priors as
the ENC prior is quite complicated to work with and did not perform signiï¬cantly better
than the other priors in terms of forecasting performance.
Departing from the normal-Wishart prior Giannone, Lenza and Primiceri (2012) sug-
gests a hierarchical prior structure that allows the choice of prior hyperparameters to be
inï¬‚uenced by the data and, in a sense, makes the procedure more â€objectiveâ€.
3.2.1
The normal-Wishart prior
The normal-Wishart prior is the natural conjugate prior for normal multivariate regres-
sions. It generalizes the original Litterman prior by treating the error variance-covariance
matrix, Î¨, as an unknown positive deï¬nite symmetric matrix rather than a ï¬xed diagonal
matrix. By allowing for correlation between the equations this also leads to computation-
ally convenient system estimation instead of the equation by equation approach used by
Litterman. This does, however, come with the disadvantage of imposing a Kronecker
structure on the variance-covariance matrix of Î³.
Using the trick of adding and subtracting ZbÎ“ in the likelihood (7) and letting S =

Y âˆ’ZbÎ“
â€² 
Y âˆ’ZbÎ“

be the error sum of squares we see that the likelihood
L (Y|Î“, Î¨) âˆ|Î¨|âˆ’T/2 exp

âˆ’1
2 tr

Î¨âˆ’1S

Ã— exp

âˆ’1
2 tr

Î¨âˆ’1 
Î“ âˆ’bÎ“
â€²
Zâ€²Z

Î“ âˆ’bÎ“

has the form of a normal-Wishart distribution when considered as a function of Î“ and Î¨.
Specifying the prior similarly,
Î“|Î¨ âˆ¼MNkm (Î“, Î¨, â„¦,)
(16)
Î¨ âˆ¼iW (S, v) ,
14

we have the conjugate normal-Wishart prior with the corresponding posterior,
p (Î“, Î¨|YT) âˆ|Î¨|âˆ’T/2 exp

âˆ’1
2 tr

Î¨âˆ’1 
Î“ âˆ’bÎ“
â€²
Zâ€²Z

Î“ âˆ’bÎ“

exp

âˆ’1
2 tr

Î¨âˆ’1S

(17)
Ã— |Î¨|âˆ’(v+m+k+1)/2 exp

âˆ’1
2 tr

Î¨âˆ’1 (Î“ âˆ’Î“)â€² â„¦âˆ’1 (Î“ âˆ’Î“)

exp

âˆ’1
2 tr

Î¨âˆ’1S

= |Î¨|âˆ’(T+v+m+k+1)/2 exp

âˆ’1
2 tr
h
Î¨âˆ’1  Î“ âˆ’Î“
â€² â„¦
âˆ’1  Î“âˆ’Î“
i
exp

âˆ’1
2 tr

Î¨âˆ’1S

,
where the last line is obtained by completing the square for Î“. That is
Î“|YT, Î¨ âˆ¼MNkm
 Î“, Î¨, â„¦

(18)
â„¦
âˆ’1 = â„¦âˆ’1 + Zâ€²Z,
Î“ = â„¦

â„¦âˆ’1Î“ + Zâ€²ZbÎ“

= â„¦
 â„¦âˆ’1Î“ + Zâ€²Y

and
Î¨|YT âˆ¼iW
 S, v

, v = T + v
(19)
S= S + S+

Î“ âˆ’bÎ“
â€² 
â„¦+ (Zâ€²Z)âˆ’1âˆ’1 
Î“ âˆ’bÎ“

with bÎ“ = (Zâ€²Z)âˆ’1 Zâ€²Y and S =

Y âˆ’ZbÎ“
â€² 
Y âˆ’ZbÎ“

.
For the conjugate normal-Wishart prior the marginal likelihood is available in closed
form. It can easily be derived by integrating out Î“ and Î¨ in (17) while keeping track
of all the constants that have been left out in the product of the likelihood and the
prior. Alternatively we rely on the properties of the matricvariate normal and inverse
Wishart distributions given in Appendix C. From the likelihood we have the conditional
distribution of Y as Y|Î“, Î¨ âˆ¼MNTm (ZÎ“, Î¨, IT) , from the prior we deduce that ZÎ“|Î¨ âˆ¼
MNTm (ZÎ“, Î¨, Zâ„¦Zâ€²) and Y|Î¨ âˆ¼MNTm (ZÎ“, Î¨, IT + Zâ„¦Zâ€²) . Finally, since the prior
for Î¨ is inverse Wishart this leads to a matricvariate-t marginal distribution for Y,
Y âˆ¼MtTm

ZÎ“, (IT + Zâ„¦Zâ€²)âˆ’1 , S,v

.
(20)
Specifying the prior beliefs
Specifying the prior means in the fashion of the Min-
nesota prior is straightforward while the prior variances involve some diï¬ƒculties. First,
recall that the marginal prior distribution of Î“ is matricvariate t with variance-covariance
matrix V (Î³) =
1
vâˆ’mâˆ’1S âŠ—â„¦and that Î³ has moments up to order v âˆ’m. The Kronecker
structure of the variance-covariance matrix makes it apparent that it is not possible to
specify the prior standard deviations or variances as in (14). The variance-covariance ma-
trix of one equation must be proportional to the variance-covariance matrix of the other
equations. With V (Î³j) =
sjj
vâˆ’mâˆ’1â„¦we can set the diagonal elements of â„¦to
Ï‰ii =
 Ï€2
1/ (lÏ€3sr)2 , lag l of variable r, i = (l âˆ’1) m + r
(Ï€1Ï€4)2 , i = mp + 1, . . . , k
(21)
15

and let sjj = (v âˆ’m âˆ’1) s2
j to achieve something which approximates the variances of
the Minnesota prior. That is, the prior parameter matrix for the inverse Wishart is
S = (v âˆ’m âˆ’1) diag
 s2
1, . . . s2
m

(22)
with prior expectation E (Î¨) = diag (s2
1, . . . s2
m) . We are implicitly setting Ï€2 = 1 in (14)
and it is reasonable to use a smaller value of Ï€1 here to balance between the Minnesota
type tight prior on lags of other variables and a looser prior on own lags.
It is, in general, advisable to set the prior variances for coeï¬ƒcients on deterministic
variables to a large positive number as in (21) rather than the improper uniform prior in
the original Minnesota prior. Noting that â„¦enters the prior as the inverse and that S can
be rewritten as a function of â„¦âˆ’1 it is, however, possible to work with â„¦âˆ’1 and specify
an improper prior by setting the corresponding diagonal elements of â„¦âˆ’1 to zero.
The prior degrees of freedom of the inverse Wishart for Î¨ might also require some
care, we must have v â‰¥m + 2 for the prior variance to exists and v â‰¥m + 2h âˆ’T for the
variance of the predictive distribution at lead time h to exist.
Simulating from the posterior distribution
With a normal-Wishart posterior we
can proceed as in Algorithm 1 using the posterior distributions (18) and (19).
3.2.2
The normal-diï¬€use and independent normal-Wishart priors
The normal-diï¬€use prior takes a simple form with prior independence between Î“ and Î¨.
A normal prior for Î³, Î³ âˆ¼N
 Î³, Î£Î³

and a Jeï¬€reysâ€™ prior for Î¨,
p (Î¨) âˆ|Î¨|âˆ’(m+1)/2 .
(23)
This prior lacks the computationally convenient Kronecker structure of the variance-
covariance matrix of the normal-Wishart prior but it has the great advantage of not
placing any restrictions on the prior variance-covariance Î£. The joint posterior distribu-
tion has the form
p (Î“, Î¨|YT) âˆ|Î¨|âˆ’(T+m+1)/2 exp

âˆ’1
2 (Î³ âˆ’bÎ³)â€²  Î¨âˆ’1 âŠ—Zâ€²Z

(Î³ âˆ’bÎ³)

Ã— exp

âˆ’1
2
 Î³ âˆ’Î³
â€² Î£âˆ’1
Î³
 Î³ âˆ’Î³

.
This prior was ï¬rst considered by Zellner (1971) in the context of seemingly unrelated
regression models.
He showed that the marginal posterior for Î³ can be expressed as
the product of the normal prior and the marginal matricvariate t-distribution (9). The
marginal posterior is bimodal if there is a suï¬ƒciently large diï¬€erence between the center
of the prior information and the center of the data information. This can be troublesome
for MCMC schemes which might get stuck at one of the modes.
The full conditional posteriors are easy to derive. Completing the square for Î³ we
have
Î³|YT, Î¨ âˆ¼N
 Î³, Î£Î³

(24)
Î£Î³ =
 Î£âˆ’1
Î³
+ Î¨âˆ’1 âŠ—Zâ€²Z
âˆ’1 ,
Î³ = Î£Î³

Î£âˆ’1
Î³ Î³ +
 Î¨âˆ’1 âŠ—Zâ€²Z
 bÎ³

= Î£Î³

Î£âˆ’1
Î³ Î³ + vec
 Zâ€²YÎ¨âˆ’1
16

where we have used that bÎ³ = [Î¨âˆ’1 âŠ—Zâ€²Z]âˆ’1 (Î¨âˆ’1 âŠ—Zâ€²) y and (Î¨âˆ’1 âŠ—Zâ€²) y = vec
 Zâ€²YÎ¨âˆ’1
for the last line. Note that this involves the inversion of the mkÃ—mk matrix Î£âˆ’1
Î³ +Î¨âˆ’1âŠ—
Zâ€²Z which can be computationally demanding and numerically unstable for large models.6
The conditional posterior for Î¨ follows directly from the likelihood (7),
Î¨|YT, Î“ âˆ¼iW
 S, v

, v = T
(25)
S= (Y âˆ’ZÎ“)â€² (Y âˆ’ZÎ“) .
The normal-diï¬€use prior is not a proper prior, which might be an issue in some cases,
even if we are assured that the posterior is proper as long as T > k. A simple modiï¬cation
is to replace the improper Jeï¬€reysâ€™ prior for Î¨ with an inverse Wishart, Î¨ âˆ¼iW (S, v) .
The use of the independent normal-Wishart prior leaves the conditional posterior for
Î“ unaï¬€ected and the conditional posterior for Î¨ is still inverse Wishart but now with
parameters
S = S+ (Y âˆ’ZÎ“)â€² (Y âˆ’ZÎ“) , v = T + v.
(26)
Specifying the prior beliefs
With the N
 Î³, Î£Î³

form of the prior for Î³ it is straight-
forward to implement a basic Minnesota prior that is informative about all regression
parameters. Improper priors for the coeï¬ƒcients on deterministic variables can be im-
plemented by working with the prior precision and setting the corresponding diagonal
elements of Î£âˆ’1
Î³
to zero. Similarly, in order to implement the sum of coeï¬ƒcients prior
or the initial observations prior it is most convenient to form the dummy observations
RjÎ³j = rj and add
1
s2
j Râ€²
jRj to the corresponding diagonal block of Î£âˆ’1
Î³
and
1
s2
j Râ€²r to Î³j.
Simulating from the posterior distribution
With the full conditional posteriors in
hand a straightforward Gibbs sampling scheme is available for sampling from the posterior
and predictive distributions, see Algorithm 2. The experience of Kadiyala and Karlsson
(1997) is that the Gibbs sampler convergences quickly to the posterior distribution and a
few hundred draws may be suï¬ƒcient as burn-in when the posterior is unimodal.
3.2.3
A hierarchical prior for the hyperparameters
The prior hyperparameters are in general chosen in three diï¬€erent ways, as default values
similar to the ones used by Litterman, to minimize the forecast errors over a training
sample or in an empirical Bayes fashion by maximizing the marginal likelihood with
respect to the hyperparameters. As an alternative Giannone et al. (2012) suggests a more
ï¬‚exible approach where one more layer is added to the prior structure by placing a prior
on the hyperparameters in a hierarchical fashion. Collecting the hyperparameters in the
vector Î´ and working with the normal-Wishart family of prior distributions the prior
structure becomes
Ï€ (Î“|Î¨,Î´) Ï€ (Î¨|Î´) Ï€ (Î´) .
6This is exactly the computational advantage of the Normal-Wishart prior. By retaining the Kronecker
structure of the variance-covariance matrix, Î¨âˆ’1 âŠ—S, in the conditional posterior for Î³ only inversion
of m Ã— m and k Ã— k matrices is needed and it is only Î¨âˆ’1 (or itâ€™s Cholesky factor) that needs to be
recomputed for each draw from the posterior.
17

Algorithm 2 Gibbs sampler for normal-diï¬€use and independent normal-Wishart priors
Select a starting value, Î³(0) for Î³. For j = 1, . . . , B + R
1. Generate Î¨(j) from the full conditional posterior (25) with S evaluated at Î³(jâˆ’1)
where the posterior parameters are given by (25) for the normal-diï¬€use prior and
(26) for the independent normal-Wishart prior.
2. Generate Î³(j) from the full conditional posterior (24) with Î£Î³ evaluated at Î¨(j).
3. For j > B, generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
The ï¬rst B draws are discarded as burn-in.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample of independent draws from the joint predictive
distribution.
Conditioning on Î´ the analysis is as before and the results in section 3.2.1 holds
when interpreted conditional on Î´. In addition to putting a prior on the hyperparameters
Giannone et al. (2012) relaxes some of the simplifying choices that are commonly made
when setting up the prior. Instead of setting the diagonal elements of S in the prior for
Î¨ based on the residual standard variance from OLS estimated VAR or univariate AR
models Giannone et al. proposes treating them as parameters. That is, they set S =
diag (Îº1, . . . , Îºm) and endow Îºi with independent inverse Gamma priors, Îºi âˆ¼iG (aÎº, bÎº).
The conditional prior for Î¨ is thus Î¨|Î´ âˆ¼iW (S, v) . The prior variance speciï¬cation for
Î“ can then be completed by setting
Ï‰ii =

(Ï€1 (v âˆ’m âˆ’1) / (lÎºr))2 , lag l of variable r, i = (l âˆ’1) m + r
Ï€2
4 (v âˆ’m âˆ’1) , i = mp + 1, . . . , k
with â„¦= diag (Ï‰) yielding the prior variances
V (Î³ij) =
 (Ï€2
1Îºj) / (l2Îºr) , lag l of variable r, i = (l âˆ’1) m + r
Ï€2
4Îºj, i = mp + 1, . . . , k
.
The prior means of Î“ is, Î“, set to one for the ï¬rst own lag and zero otherwise, the prior
for Î“ is thus Î“|Î¨,Î´ âˆ¼MNkm (Î“, Î¨,â„¦)
In addition to this Giannone et al. adds dummy observations for a sum of coeï¬ƒcients
prior and a dummy initial observation prior. Let eY and eZ be the dummy observations
speciï¬ed similar to section 3.1.1. Giannone et al. sets
eY =

1
Ï€6 diag (y0)
1
Ï€7yâ€²
0

, eZ =
ï£«
ï£­jâ€²
m âŠ—

1
Ï€6 diag (y0)

0mÃ—(kâˆ’mp)
jâ€²
m âŠ—

1
Ï€7yâ€²
0

1
Ï€7jâ€²
(kâˆ’mp)
ï£¶
ï£¸.
18

The dummy observations are then appended to the data matrices Y and Z and the
posterior parameters calculated as usual.7
Giannone et al. uses independent Gamma
priors for the scale factors and we set Ï€i âˆ¼G (ai, bi) , i = 1, 4, 6, 7. The collection of
hyperparameters is thus Î´ = (Îº1, . . . , Îºm, Ï€1, Ï€4, Ï€6, Ï€7)â€² .
Specifying the prior beliefs
The priors for the hyperparameters Ï€1, Ï€4, Ï€6, Ï€7 can be
centered on â€standardâ€ settings for these parameters with variance depending on how
conï¬dent we are about the â€standardâ€ values. Giannone et al. (2012) sets the modes for
Ï€1, Ï€6 and Ï€7 to 0.2, 1 and 1 with standard deviations 0.4, 1 and 1. For Ï€4 a large mode,
say 50, with a large standard deviation seems reasonable. For the diagonal elements of S,
Îºi, Giannone et al. implements the prior in terms of Îºi/ (v âˆ’m âˆ’1) , i.e. the prior mean
of S, and use a highly non-informative prior with aÎº = bÎº = 0.022.
Simulating from the posterior distribution
The joint posterior of Î“, Î¨ and Î´ is not
available in closed form but Giannone et al. (2012) devices a Metropolis-Hastings sampler
for the joint distribution, see Algorithm 3. The algorithm generates Î´ from the marginal
posterior with a Metropolis-Hastings update, after convergence of the Î´ sampler Î¨ and
Î“ can be drawn from their distributions conditional on Î´. While Giannone et al. takes
advantage of the availability of the marginal likelihood conditional on Î´ to simplify the
acceptance probability in the Metropolis-Hastings step and achieve a marginal sampler
for Î´ this is not a requirement. The acceptance probability can also be written in terms
of the likelihood and the priors and a Metropolis within Gibbs sampler can be devised
when the conditional marginal likelihood is not available in closed form.
Forecasting performance
Giannone et al. (2012) conducts a forecasting experiment
where they forecast the US GDP, GDP deï¬‚ator and federal funds rate. This done using
three diï¬€erent BVARs implemented using the hierarchical prior with 3, 7 and 22 variables
with all variables in log-levels. In addition to the BVARs forecasts are also produced
with VARs estimated with OLS, a random walk with drift and a dynamic factor model
based on principal components from a data set with 149 macro variables. In terms of
mean square error the BVARs improve with the size of the model (in contrast to the OLS
estimated VARs) and the largest BVAR produces better one step ahead forecasts than
the factor model for the GDP deï¬‚ator and the federal funds rate and better four step
ahead forecasts for the GDP deï¬‚ator.
7The additional information in the dummy observations can of course also be incorporated through
the priors.
The implied prior parameters are â„¦âˆ—=

â„¦âˆ’1 + eZâ€²eZ
âˆ’1
, Î“âˆ—= â„¦âˆ—
â„¦âˆ’1Î“ + eZâ€² eY

and
Sâˆ—= S+

Yâˆ—âˆ’Zâˆ—bÎ“âˆ—â€² 
Yâˆ—âˆ’Zâˆ—bÎ“âˆ—
âˆ’

Y âˆ’ZbÎ“
â€² 
Y âˆ’ZbÎ“

where Yâˆ—and Zâˆ—is the augmented data,
bÎ“âˆ—the OLS estimate on the augmented data and bÎ“ the OLS estimate on the original data. The eï¬€ect on
â„¦and Î“ is clear and intuitive whereas S is inï¬‚ated in a data dependent and non-obvious way. The mixed
estimation technique underlying the device of adding prior information through dummy observations
works well when the error variance is assumed known but is less transparent when it is unknown.
19

Algorithm 3 MCMC sampler for a VAR with hierarchical prior
For the VAR model with the hierarchical prior outlined in section 3.2.3 select starting
values for the hyperparameters Î´(0), Giannone et al. (2012) suggests using the posterior
mode of Î´ as starting values and setting the tuning constant c to achieve approximately
20% acceptance rate. Step 1 of the sampler samples from the marginal posterior for Î´,
steps 2 and 3 draws from the posterior for Î¨ and Î“ conditional on Î´.
For j = 1, . . . , B + R
1. Draw a proposal, Î´âˆ—, for the hyperparameters from the random walk proposal dis-
tribution, Î´âˆ—âˆ¼N
 Î´(jâˆ’1), cHâˆ’1
where H is the Hessian of the negative of the
logposterior for Î´. Set Î´(j) = Î´âˆ—with probability Î±, otherwise set Î´(j) = Î´(jâˆ’1)
where
Î± = min

1,
m (Y|Î´âˆ—) Ï€ (Î´âˆ—)
m (Y|Î´(jâˆ’1)) Ï€ (Î´(jâˆ’1))

and m (Y|Î´) is given by (20).
Redo 1 if j < B otherwise continue.
2. Draw Î¨(j) from the full conditional posterior Î¨|YT, Î´(j) in (19)
3. Draw Î“(j) from the full conditional posterior Î“|YT, Î¨(j), Î´(j) in (18).
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample of independent
draws from the joint predictive distribution.
3.3
The steady state VAR
Villani (2009) observed that it is often easier to formulate a prior opinion about the
steady state (unconditional mean) of a stationary VAR than about the dynamics. At
the same time this is one feature of the data that a Minnesota type prior is silent about
with the uninformative prior on the coeï¬ƒcients on deterministic variables.8
This is,
however, not surprising as the unconditional expectation is a highly nonlinear function
of the parameters when the VAR is written as a linear regression model which makes it
diï¬ƒcult to express any prior beliefs about the steady state. Let A (L) = I âˆ’Aâ€²
1L âˆ’. . . âˆ’
Aâ€²
pLp we can then write the stationary VAR (6) as
A (L) yt = Câ€²xt + ut.
8The initial observations prior could be used to incorporate information about the steady state in the
prior formulation by replacing y0 with the expected steady state.
20

The unconditional expectation is the E (yt) = Âµt = Aâˆ’1 (L) Câ€²xt = Î›xt.9 Given infor-
mation about likely values for Âµt it is straightforward to formulate an informative prior
for Î› but the implied prior for C is highly complicated. Instead Villani (2009) suggested
writing the model in mean deviation form,
A (L) (yt âˆ’Î›xt) = ut.
(27)
This makes the model non-linear in parameters which complicates estimation but makes
it easy to formulate a prior for all the parameters.
Let Î“â€²
d =
 Aâ€²
1, . . . Aâ€²
p

represent the dynamics. Villani (2009) argued that there is no
obvious connection between the steady state and the parameters governing the dynamics
and suggested the prior
Ï€ (Î“d, Î›, Î¨) = Ï€ (Î“d) Ï€ (Î›) Ï€ (Î¨)
with Ï€ (Î“d) and Ï€ (Î›) normal,
Î³d âˆ¼N

Î³d, Î£d

,
(28)
Î» = vec (Î›) âˆ¼N (Î», Î£Î»)
and a Jeï¬€reysâ€™ prior (23) for Î¨. Alternatively a proper inverse Wishart, Î¨ âˆ¼iW (S, v) ,for
Î¨ can be used. Ï€ (Î“d) can be based on the prior beliefs in the Minnesota prior, variances
as in (14) with prior means for the ï¬rst own lag, Î³jj less than 1 indicating stationarity
and existence of the steady state.
The joint posterior is, due to the nonlinearities, not a known distribution but Villani
derived the full conditional posteriors for Î›, Î“d and Î¨ which can serve as the basis for
a Gibbs sampler and MCMC based inference. To this end rewrite (27) as a nonlinear
regression
yâ€²
t = xâ€²
tÎ›â€²+ [wâ€²
t âˆ’qâ€²
t (Ip âŠ—Î›â€²)] Î“d + uâ€²
t
Y= XÎ›â€²+ [W âˆ’Q (Ip âŠ—Î›â€²)] Î“d + U
with wâ€²
t =
 yâ€²
tâˆ’1, . . . , yâ€²
tâˆ’p

, qâ€²
t =
 xâ€²
tâˆ’1, . . . , xâ€²
tâˆ’p

. The full conditional posterior for
Î¨ is easy to derive and analogous to the normal-diï¬€use prior, form U = Y âˆ’XÎ›â€² âˆ’
[W âˆ’Q (Ip âŠ—Î›â€²)] Î“d and S = Uâ€²U, the error sum of squares matrix conditional on Î›
and Î“d. The conditional posterior for Î¨ is then inverse Wishart
Î¨|YT, Î“d, Î› âˆ¼iW
 S, v

(29)
with S = S and v = T for Jeï¬€reysâ€™ prior and S = S + S and v = T + v for the inverse
Wishart prior.
For the full conditional posterior for Î“d we can treat Î› as known and thus calculate
YÎ› = Y âˆ’XÎ›â€² and WÎ› = [W âˆ’Q (Ip âŠ—Î›â€²)] . With these in hand we can write the
model as YÎ› = WÎ›Î“d + U, a standard multivariate regression conditional on Î› and
9For simplicity we assume that xt only consists of simple deterministic variables such as a constant,
time trend and seasonal dummies.
21

Î¨. This is analogous to the normal-diï¬€use prior (section 3.2.2) and the full conditional
posterior for Î³d is normal
Î³d|YT, Î›, Î¨ âˆ¼N
 Î³d, Î£d

(30)
Î£d =
 Î£âˆ’1
d
+ Î¨âˆ’1 âŠ—Wâ€²
Î›WÎ›
âˆ’1
Î³d = Î£d

Î£âˆ’1
d Î³d + vec
 Wâ€²
Î›YÎ›Î¨âˆ’1
.
The full conditional posterior for Î› is more complicated to derive and requires some
matrix manipulations. Let YÎ“ = Y âˆ’WÎ“d, B = (X, âˆ’Q) and Î˜â€²= [Î›, Î“â€²
d (Ip âŠ—Î›)] =

Î›, Aâ€²
1Î›, . . . , Aâ€²
pÎ›

the regression can then be written as
YÎ“ = BÎ˜ + U
vec (Yâ€²
Î“) = vec (Î˜â€²Bâ€²) + vec (Uâ€²)
= (B âŠ—I) vec (Î˜â€²) + vec (Uâ€²)
= (B âŠ—I) F vec (Î›) + vec (Uâ€²)
a standard univariate regression with regression parameters Î» for Fâ€²= [I, I âŠ—A1, I âŠ—Ap]
and vec(Uâ€²) âˆ¼N (0, IT âŠ—Î¨) . The usual Bayesian calculations yields a normal posterior
for Î» conditional on Î“d and Î¨,
Î»|YT, Î“d, Î¨ âˆ¼N
 Î», Î£Î»

(31)
Î£Î» =
 Î£âˆ’1
Î» + Fâ€²  Bâ€²B âŠ—Î¨âˆ’1
F
âˆ’1
Î» = Î£Î»
h
Î£âˆ’1
Î» Î» + Fâ€²  Bâ€²B âŠ—Î¨âˆ’1
FbÎ»
i
= Î£Î»

Î£âˆ’1
Î» Î» + Fâ€² vec
 Î¨âˆ’1Yâ€²
Î“B

for bÎ» = [Fâ€² (Bâ€²B âŠ—Î¨âˆ’1) F]âˆ’1 Fâ€² (Bâ€² âŠ—Î¨âˆ’1) vec (Yâ€²
Î“) the GLS estimate.
Forecasting performance
Villani (2009) conducts a small forecasting exercise where
he compares the forecast performance of the steady-state prior to a standard BVAR
with the Litterman prior and a standard VAR estimated with maximum likelihood. The
focus is on modelling the Swedish economy and with Swedish GDP growth, inï¬‚ation and
interest rate, the corresponding foreign (world) variables and the exchange rate in trade
weighted form included in the VAR models. The estimation period includes the Swedish
ï¬nancial crisis at the beginning of the 90-ties and the subsequent shift in monetary policy
to inï¬‚ation targeting. To accommodate this xt includes a constant term and a dummy for
the pre-crisis period. The prior on the constant terms in the steady-state VAR are thus
centered on the perceived post-crisis steady state and the prior on the dummy variable
coeï¬ƒcients reï¬‚ects the higher pre-crisis inï¬‚ation and interest rates and the belief that
the crisis had no eï¬€ect on long run GDP growth. For the dynamics, the prior on Î“d,
Villani follows the Litterman prior with the addition of treating the foreign variables as
exogenous, i.e. applying more aggressive shrinkage towards zero in the prior, and sets the
prior mean of the ï¬rst own lag to 0.9. The forecast performance is evaluated over the
period 1999 to 2005. The steady-state VAR performs considerably better for the Swedish
variables, conï¬rming the intuition that it is useful to be informative about (changes in)
the steady state.
22

Adolfson, Andersson, Linde, Villani and Vredin (2007) evaluates the forecast perfor-
mance of two forecasting models in use at Sveriges Riksbank (the central bank of Sweden),
a steady-state BVAR with the same variables as Villani (2009) similar prior set up, and
the open economy DSGE model of Adolfson, Lasen, Lind and Villani (2008). The BVAR
provides better forecasts of Swedish inï¬‚ation up to 5 quarters ahead while the DSGE
model has lower RMSE when forecasting 7 and 8 quarters ahead and both models im-
prove on the oï¬ƒcial Riksbank forecast. The BVAR outperforms the DSGE model at
all lead times when forecasting the interest rate and the forecast performance for GDP
growth is almost identical but worse than the oï¬ƒcial Riksbank forecast except for lead
times 6 through 8.
Â¨Osterholm (2008a) forecasts the Swedish inï¬‚ation and interest rate using a bivariate
steady state BVAR and a univariate variant of the steady state BVAR, i.e. Ï† (L) (yt âˆ’Î± âˆ’Î¸dt) ,
allowing for a shift at the time of the change in monetary policy regime. The forecasts are
compared to forecasts from standard BVAR and Bayesian AR models with the dummy
variable but without prior information about steady state. For inï¬‚ation there is very little
diï¬€erence between the models whereas the steady state models do signiï¬cantly better for
the interest rate.
Beechey and Â¨Osterholm (2010) forecasts the inï¬‚ation rate for ï¬ve inï¬‚ation targeting
countries, Australia, Canada, New Zealand, Sweden, the UK and the US using a univariate
variant of the steady state VAR as in Â¨Osterholm (2008a). The prior for Î¸ is informative
and centered on the target inï¬‚ation rate with a diï¬€use prior for Î± and a Minnesota type
lag decay on the autoregressive parameters in Ï† (L) . As a comparison a standard AR
model with the dummy dt is also estimated using Bayesian and frequentist techniques,
thus allowing for a shift in average inï¬‚ation level but without adding information about
the inï¬‚ation target through a prior. The steady state AR improves on the forecasts of the
other two models by a large amount for Australia, New Zealand and Sweden, less so for
Canada and oï¬€er no improvement for the UK. The US is a special case with no oï¬ƒcially
announced inï¬‚ation target, if a shift in the (unoï¬ƒcial) target is assumed in 1993 there is
no improvement from the steady state model whereas there are substantial gains if the
target is assumed constant.
Wright (2010) propose to anchor the steady state at the long run expectation of the
variables as measured by survey responses.
Speciï¬cally at each time point the prior
mean of the steady state is set to the latest estimate from the Blue Chip survey. This
is a convenient way of bringing in expectational data and Wright refers to this as a
â€democratic priorâ€. Using VARs with monthly data on 10 variables Wright forecasts the
US real GDP growth, GDP deï¬‚ator, CPI inï¬‚ation, industrial production growth three
month yields and the unemployment rate at horizons 0 - 13. The VAR variants include
one estimated by OLS, a normal-diï¬€use prior with Minnesota type prior beliefs and the
democratic steady state prior with three diï¬€erent ways of specifying the prior mean on
the ï¬rst own lag, 0 for all variables, 0 for real variables and 0.85 for nominal variables and
estimated from the survey data. The BVARs improve on the OLS estimated VAR and
the democratic priors do better than the Minnesota prior with little diï¬€erence between
the alternative speciï¬cation of the prior means. Wright also comparing the VAR forecasts
with additional forecast devices for a subset of the variables. When the comparison is
with survey estimates of short term expectations the diï¬€erences are small with a few
cases where a BVAR improves signiï¬cantly on the survey estimates. Comparing the VAR
23

Algorithm 4 Gibbs sampler for the steady state prior
With the steady state prior 28 a Gibbs sampling algorithm follows immediately from the
full conditional posteriors. Select starting values, Î³(0)
d
and Î»(0). For j = 1, . . . , B + R
1. Generate Î¨(j) from the full conditional posterior in (29) with S evaluated at Î³(jâˆ’1)
d
and Î»(jâˆ’1). Note that S and v depends on the choice of prior for Î¨.
2. Generate Î³(j)
d
from the full conditional posterior in (30) with Î³d and Î£d evaluated
at Î¨(j) and Î»(jâˆ’1).
3. Generate Î»(j) from the full conditional posterior in (31) with Î» and Î£Î» evaluated
at Î¨(j) and Î³(j).
4. If j > B, generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h = xâ€²
T+hÎ›â€²(j) +
hâˆ’1
X
+i=1

ey(j)â€²
T+hâˆ’i âˆ’xâ€²
T+hâˆ’1Î›â€²(j)
A(j)
i
+
p
X
i=h
 yâ€²
T+hâˆ’i âˆ’xâ€²
T+hâˆ’1Î›â€²(j)
A(j)
i +u(j)â€²
T+h.
The ï¬rst B draws are discarded as burn-in.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample of independent draws from the joint predictive
distribution.
forecasts with two time varying parameter models, an unobserved components stochastic
volatility (UCSV) model and the TVP-VAR with stochastic volatility of Primiceri (2005)
discussed in section 7, the VARs do better than the UCSV and the performance is similar
to the TVP-VAR with a slight edge for the TVP-VAR.
Simulating from the posterior distribution
With the full conditional posteriors in
hand a straightforward Gibbs sampling scheme is available for sampling from the posterior
and predictive distributions, see Algorithm 4. Villani reports that the Gibbs sampler
convergences quickly to the posterior distribution but also notes that there is a possible
issue of local nonidentiï¬cation of Î› when there are unit roots or explosive roots in the
autoregressive polynomial. This is only an issue for the convergence of the Gibbs sampler
if the prior for Î› is uninformative and the posterior for Î“d has non-negligible probability
mass in the nonstationary region.
3.4
Model speciï¬cation and choice of prior
Carriero, Clark and Marcellino (2011) conducts an impressive study of the many speci-
ï¬cation choices needed when formulating a BVAR for forecasting purposes. Their main
application use monthly real time data on 18 US macroeconomic and ï¬nancial variables.
The baseline model is a BVAR with all 18 variables and 1 lag using the normal-Wishart
24

prior with â€standardâ€ choices for the hyperparameters. â„¦and S are speciï¬ed as in (21)
and (22) with v = m + 2, Ï€1 =
âˆš
0.1, Ï€3 = 1 and a diï¬€use prior on the constant term.
The prior mean of the ï¬rst own lag is set to 1 except when a variable is diï¬€erenced in
which case it is set to zero. The forecasts are constructed using the recursion
eyâ€²
T+h =
hâˆ’1
X
i=1
eyâ€²
T+hâˆ’iAi +
p
X
i=h
yâ€²
T+hâˆ’iAi + xâ€²
T+hC
(32)
with the parameters set at the posterior means.
Choice of hyperparameters and lag length: Alternatives considered are setting
the hyperparameters by maximizing the marginal likelihood and using lag lengths 1-12.
Increasing the lag length improves forecast performance for most of the variables but not
for all. Choosing the lag length by maximizing the marginal likelihood leads to modest
improvements for a majority of the variables with small losses for the other variables
compared to the baseline. Choosing both hyperparameters and lag length by maximizing
the marginal likelihood oï¬€ers greater improvements than just maximizing with respect
to one of them. The gains are on the whole relatively small and Carriero, Clark and
Marcellino conclude that a lag length of 12 with Ï€1 =
âˆš
0.1 is a simple and eï¬€ective
choice.
Multi-step forecasting: The forecast function (32) is non-linear in the parameters
and using the posterior means of the parameters does not produce the means of the
predictive distribution when h > 1. Alternatives considered are 1) simulating from the
posterior distribution of the parameters and averaging over the forecasts and 2) using
directs forecasts based on estimating models that are speciï¬c to each horizon
yâ€²
t+h =
p
X
i=1
yâ€²
tâˆ’iAi + xâ€²
tC + eâ€²
t+h.
The gains from simulating the parameters is found to be negligible. Overall the diï¬€erences
between the iterated and direct forecasts are small but there are large gains from the direct
forecast for some of the variables. This is presumably because the direct forecast is more
robust to misspeciï¬cation.
Cross-variable shrinkage and treatment of the error variance: The normal-
Wishart prior forces a symmetric treatment of the variables whereas the original Litterman
prior shrinks the parameters on â€otherâ€ variables harder towards zero. On the other hand
the normal-Wishart prior relaxes the assumption of a ï¬xed and diagonal error variance
matrix. Forecasting using the prior of Litterman as implemented in section 3.1, equation
by equation estimation and two choices of Ï€2,
âˆš
0.2 and
âˆš
0.5 makes little diï¬€erence except
for the federal funds rate where the improvement is dramatic for for the shorter forecast
horizons. The independent normal Wishart prior oï¬€ers both the possibility to impose
cross-variable shrinkage and an unrestricted error variance matrix.
When comparing
the forecast performance for the independent normal Wishart and Litterman priors the
diï¬€erences are very small with a slight edge for the Litterman prior.
Size of model: When comparing the forecast performance of the 18 variable VAR to
a reduced model with 7 variables the larger model is found to forecast better. The gain
from using the larger model is smaller with direct forecasts than iterated forecasts, again
presumably due to the greater robustness against misspeciï¬cation.
25

Levels or diï¬€erences: A speciï¬cation in levels can make use of any cointegration
between the variables which should improve forecasts, on the other hand a speciï¬cation in
diï¬€erences oï¬€ers some robustness in the presence of structural breaks. The speciï¬cation
in diï¬€erences improves on the levels speciï¬cation, the root mean square error is on average
11% larger with the levels speciï¬cation and the speciï¬cation in diï¬€erences has the lowest
RMSE in 74% of the considered cases.
Carriero, Clark and Marcellino (2011) also conducts a robustness check using data from
Canada, France and the UK using a reduced set of variables. Overall the conclusions from
the US data is conï¬rmed when using data from these three countries.
Summarizing their ï¬ndings Carriero, Clark and Marcellino notes that â€simple worksâ€
and recommends transforming variables to stationarity, using a relatively long lag length
(12 with monthly data), the normal-Wishart prior and forecasts based on the posterior
means of the parameters.
4
Structural VARs
The reduced form VAR is designed to capture the time series properties of the data
and can, when coupled with suitable prior information, be an excellent forecasting device.
The reduced form nature makes it diï¬ƒcult to incorporate economic insights into the prior.
Take, for example, the â€exogenousâ€ variables prior in section 3.1.1. While it is tempting
to think about this as implying exogeneity it is actually a statement about Granger
causality. That, in a small open economy model, we do not expect the domestic variables
to be useful for forecasting the variables representing the rest of the world. Restrictions
on the variance-covariance matrix Î¨ are needed in order to make claims about exogeneity.
This brings us to structural or identiï¬ed VAR-models that, by allowing limited structural
interpretations of the parameters in the model, makes it possible to incorporate more
economic insights in the model formulation. If done well this has the potential to improve
the forecast performance of the model.
The basic structural VAR has the form
yâ€²
tÎ› =
p
X
i=1
yâ€²
tâˆ’iBi + xâ€²
tD + eâ€²
t
(33)
yâ€²
tÎ› = zâ€²
tÎ˜ + eâ€²
t
where Î› is full rank, Î˜â€² =
 Bâ€²
1, . . . , Bâ€²
p, Dâ€²
and et has a diagonal variance-covariance ma-
trix. The relation with the reduced form (6) is straightforward, Î“ = Î˜Î›âˆ’1, Ai = BiÎ›âˆ’1,
C = DÎ›âˆ’1, ut = Î›âˆ’Tet and Î¨ = Î›âˆ’TV (et) Î›âˆ’1.10 The structural VAR (33) imposes
restrictions on the form of the reduced form variance-covariance matrix Î¨ but leaves the
reduced form regression parameters Î“ unrestricted since Î› is a full rank matrix unless
there are additional restrictions on Î˜. For simplicity we take V (et) = I, with m (m + 1) /2
free parameters in the symmetric matrix Î¨ this implies a simple order condition that
m (m âˆ’1) /2 restrictions on Î› are needed for identiï¬cation.11 The simplest such scheme
10The SVAR can also be written as yâ€²
t = Pp
i=1 yâ€²
tâˆ’iAi + xâ€²
tC + eâ€²
tL with L = Î›âˆ’T where the structure
of L indicates which of the â€identiï¬edâ€ innovations eti has an immediate impact on ytj.
11This is only a neccessary and not a suï¬ƒcient condition for identiï¬cation. Identiï¬cation is discussed
in more detail in section 4.3.
26

is to let L =Î›âˆ’T be the (lower) triangular Cholesky decomposition of Î¨ = LLâ€². Subject
to a normalization that the diagonal elements of L and Î› are positive this is a one-to-one
mapping between Î› and Î¨ and yields exact identiï¬cation without, in fact, imposing any
restrictions on the reduced form. In the following we will frequently work with Î» = vec (Î›)
and Î»j, column j of Î›, it is then important to keep in mind that these are subject to
restrictions and that not all elements can vary freely.
The normalization is needed because the reduced form coeï¬ƒcients are left unchanged
by reversing the sign of column j of Î› and Î˜. The choice of normalization is, in general,
not innocuous. Waggoner and Zha (2003b) demonstrate how an unfortunate choice of
normalization can lead to misleading inference about Î› and impulse responses and give a
rule for ï¬nding a good normalization. As our focus is on forecasting where the predictive
distribution depends on the reduced form parameters we will largely ignore these issues.
4.1
â€Unrestrictedâ€ triangular structural form
The structural form likelihood has the form
L (Y|Î˜, Î›) âˆ|det Î›|T exp

âˆ’1
2 tr

(YÎ› âˆ’ZÎ˜) (YÎ› âˆ’ZÎ˜)â€²
= |det Î›|T exp

âˆ’1
2 [vec (YÎ›) âˆ’(Im âŠ—Z) Î¸]â€² [vec (YÎ›) âˆ’(Im âŠ—Z) Î¸]

= |det Î›|T exp

âˆ’1
2
h
vec (YÎ›) âˆ’(Im âŠ—Z) bÎ¸
iâ€² h
vec (YÎ›) âˆ’(Im âŠ—Z) bÎ¸
i
Ã— exp

âˆ’1
2

Î¸ âˆ’bÎ¸
â€²
(Im âŠ—Zâ€²Z)

Î¸ âˆ’bÎ¸

of a normal distribution for Î¸ = vec (Î˜) conditional on Î› with bÎ¸ = vec

bÎ˜

= vec

(Zâ€²Z)âˆ’1 Zâ€²YÎ›

.
Sims and Zha (1998) suggested matching this by specifying a normal prior for Î¸ condi-
tional on Î›, Î¸|Î› âˆ¼N (vec (MÎ›) ,Î£Î¸) with Mâ€² = (Im, 0) together with a marginal prior,
Ï€ (Î›) for Î›. The choice of M implies a prior mean for the reduced form parameters Î“
that coincides with the univariate random walk of the Minnesota prior. The conditional
posterior for Î¸ is then normal,
Î¸|YT, Î› âˆ¼N
 Î¸, Î£Î¸

Î£Î¸ =
 Î£âˆ’1
Î¸
+ Im âŠ—Zâ€²Z
âˆ’1
Î¸ = Î£Î¸
 Î£âˆ’1
Î¸ vec (MÎ›) + vec (Zâ€²YÎ›)

.
Similar to the normal-diï¬€use prior this involves the inversion of the mk Ã— mk matrix Î£Î¸
which can be computationally demanding. As noted by Sims and Zha (1998) this can be
simpliï¬ed considerably if Î£Î¸ is block diagonal with diagonal blocks Î£Î¸,j corresponding
to the equations.
That is, there is independence between the priors for the diï¬€erent
equations conditional on Î›,
Î¸j|Î› âˆ¼N
 MÎ»j, Î£Î¸,j

.
(34)
27

The inversion of a mk Ã— mk matrix is then replaced by m inversions of k Ã— k matrices as
we solve for the posterior parameters equation by equation,
Î¸j|YT, Î› âˆ¼N
 Î¸j, Î£Î¸,j

Î£Î¸,j =
 Î£Î¸,j + Zâ€²Z
âˆ’1
Î¸j = Î£Î¸,j
 Î£âˆ’1
Î¸,jMÎ»j + Zâ€²YÎ»j

= MjÎ»j,
and this brings us close to the computational convenience of the normal-Wishart prior.
A further simpliï¬cation is available if the prior variance is the same for all equations,
Î£Î¸,j = Î£
e
Î¸ and Î£Î¸ = Im âŠ—Î£
e
Î¸. The conditional posteriors for Î¸j then only diï¬€ers in the
conditional mean with
Î¸j|YT, Î› âˆ¼N

f
MÎ»j, eÎ£Î¸

(35)
eÎ£Î¸ =

Î£
e
âˆ’1
Î¸
+ Zâ€²Z
âˆ’1
f
M = eÎ£Î¸

Î£
e
âˆ’1
Î¸ M + Zâ€²Y

which puts the computational requirements on par with the normal-Wishart prior for a
reduced form VAR.
The posterior for Î› is more complicated. Integrating out Î˜ from the joint posterior
and keeping track of the extra terms from completing the square for Î¸ yields the marginal
posterior
Ï€ (Î›|YT) âˆÏ€ (Î›) |det Î›|T
Ã—exp

âˆ’1
2Î»â€²

Im âŠ—Yâ€²Y + (Im âŠ—M)â€² Î£âˆ’1
Î¸ (Im âŠ—M)
âˆ’

Î£âˆ’1
Î¸ (Im âŠ—M) + Im âŠ—Zâ€²Y
â€² Î£
âˆ’1
Î¸

Î£âˆ’1
Î¸ (Im âŠ—M) + Im âŠ—Zâ€²Y


Î»

.
This is not a known distribution except in special cases. One such case arises under the
prior Î¸j|Î› âˆ¼N
 MÎ»j, Î£
e
Î¸

on Î¸ discussed above. The Kronecker structure of the prior
variance-covariance matrix Î£Î¸ is inherited by the posterior variance-covariance and there
is also a Kronecker structure in the posterior mean. The exponent in the posterior for Î›
simpliï¬es
Ï€ (Î›|YT) âˆÏ€ (Î›) |det Î›|T exp

âˆ’1
2Î»â€² 
Im âŠ—
h
Yâ€²Y + Mâ€²Î£
e
Î¸M âˆ’f
Mâ€² eÎ£âˆ’1
Î¸ f
M
i
Î»

= Ï€ (Î›) |det Î›|T exp

âˆ’1
2 tr
h
Yâ€²Y + Mâ€²Î£
e
Î¸M âˆ’f
Mâ€² eÎ£âˆ’1
Î¸ f
M
i
Î›Î›â€²
.
Ignoring the prior, this is similar to a Wishart distribution for Î¨âˆ’1 =Î›Î›â€². It is,
however, only a Wishart if the structure of Î› imposes no restrictions on Î¨, e.g. if Î›
is upper triangular with no other restrictions except the normalization. In this, special
case, it is reasonable to specify an uninformative prior for Î›, Ï€ (Î›) âˆ1 and the implied
posterior for Î¨âˆ’1 is Wishart,
Î¨âˆ’1|YT âˆ¼Wm
 Sâˆ’1,T + m + 1

(36)
S = Yâ€²Y + Mâ€²Î£
e
Î¸M âˆ’f
Mâ€² eÎ£âˆ’1
Î¸ f
M.
28

A draw from the posterior of Î› can then be obtained by generating Î¨âˆ’1 from the Wishart
distribution and solving for Î›. In fact, if Î› is triangular it can be generated directly
as the Bartlett decomposition of a Wishart distributed matrix. Sims and Zha (1998)
and Zha (1999) suggests Ï€ (Î›) âˆ|det Î›|k as an uninformative improper prior. This is,
however, in a slightly diï¬€erent context and working with the prior and posterior for (Î›, Î“)
rather than (Î›, Î˜) and the factor |det Î›|k corresponds to the Jacobian when transforming
from the (Î›, Î˜) parameterization to the (Î›, Î“) parameterization. Inference in the two
parameterizations is thus equivalent with these two priors on Î› provided that the priors
on Î“ and Î˜ are equivalent.
Specifying the prior
The triangular SVAR is just a reparameterization of the reduced
form VAR of section 3 and it is tempting to base the prior speciï¬cation on the Minnesota
prior. It should, however, be clear that it is not possible to mimic the Minnesota prior
completely without losing the computational convenience as the transformation Î˜ = Î“Î›
implies prior dependence between the columns of Î˜. Sims and Zha (1998) proposed setting
the prior standard deviations to
sd (Î¸ij) =
 (Ï€1Ï€2) / (lÏ€3sr) , lag l of variable r, i = (l âˆ’1) m + r
Ï€1Ï€4, deterministic variables, i = mp + 1, . . . , k
.
(37)
This is close to the Minnesota prior but diï¬€ers in two aspects, there is no distinction
between own lags and â€otherâ€ variables since the choice of dependent variable in a si-
multaneous equation system is arbitrary and the scale factor sj drops out since the error
variances are normalized to 1. This leads to a common prior variance, Î£
e
Î¸, in (34) and
the simpliï¬ed posterior (35) in the spirit of the Minnesota prior.
The symmetric treatment of the structural form equations does, however, not imply
a symmetric treatment of the reduced form equations. With Î› upper triangular we have
Î³j = Pj
i=1 Î¸iÎ»ij for Î»ij element i, j of Î›âˆ’1 and the ordering of the equations clearly matters
for the implied prior on the reduced form. The unconditional prior expectation of Î“ is
M and the random walk type prior with mii = 1 can easily be modiï¬ed to accommodate
variables that are believed to be stationary by setting mii less than 1.
With Î› triangular a truly structural interpretation of the parameters is diï¬ƒcult and
an uninformative prior, Ï€ (Î›) âˆ1 seems appropriate.
Sampling from the posterior distribution
With Î› triangular, the prior Ï€ (Î›) âˆ1,
Î¸j|Î› âˆ¼N
 MÎ»j, Î£
e
Î¸

simulating from the posterior and predictive distributions using
algorithm 5 is straightforward.
4.2
Homogenous restrictions on the structural form parameters
When the structure of Î› implies restrictions on Î¨ the posterior becomes quite complicated
irrespective of the choice of prior for Î›. Sims and Zha (1998) proposes to use importance
sampling, generating Î› from an approximation to the marginal posterior and Î˜ or Î“
conditional on Î› and Zha (1999) devices a scheme where blocks of equations can be
treated independently and importance sampling can be used for each block. The block
scheme should be more eï¬ƒcient as one high dimensional problem is replaced by several
29

Algorithm 5 Simulating the posterior and predictive distributions for a triangular SVAR
For the SVAR with Î› triangular and the prior (34) with Î£Î¸,j = Î£
e
Î¸ for Î˜ and an uninfor-
mative prior for Î›, Ï€ (Î›) âˆ1 draws from the posterior and predictive distributions can
be obtained as follows
For j = 1, . . . R
1. Generate Î›(j) directly as the Bartlett decomposition of a draw form the marginal
posterior (36).
2. For i = 1, . . . , m generate Î¸(j)
i
from the conditional posterior (35).
3. Calculate the reduced form parameters Î“(j) = Î˜(j)Î›(j).
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
with Î¨(j) =
 Î›(j)Î›(j)â€²âˆ’1 and
calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
problems of smaller dimension. Nevertheless importance sampling has proven to be quite
ineï¬ƒcient as it is diï¬ƒcult to ï¬nd a good approximation to the marginal posterior of Î›.
Waggoner and Zha (2003a) develops a Gibbs sampler for the marginal posterior of Î›
in a setting allowing for exact restrictions and informative priors on both Î› and Î˜. They
consider homogenous restrictions on the parameters of one equation (column in Î› and
Î˜) of the form
QjÎ»j = 0
(38)
RjÎ¸j = 0
for Qj a (m âˆ’qj)Ã—m matrix of rank mâˆ’qj and Rj a (k âˆ’rj)Ã—k matrix of rank kâˆ’rj, i.e.
there are m âˆ’qj restrictions on Î»j resulting in qj free parameters and m âˆ’rj restrictions
on Î¸j resulting in rj free parameters, together with the normal prior suggested by Sims
and Zha (1998) for the unrestricted parameters,
Î»j âˆ¼N
 0, Î£Î»,j

(39)
Î¸j|Î»j âˆ¼N
 MjÎ»j, Î£Î¸,j

.
To form a prior incorporating the restrictions Waggoner and Zha conditioned on the
restrictions (38) in the prior (39). To this end let Uj and Vj be m Ã— qj and k Ã— rj
orthonormal matrices satisfying QjUj= 0 and RjVj= 012 if the restrictions hold there
12Uj and Vj form basis for the null spaces of Qj and Rj and can be obtained from the QR de-
compositions of Qâ€²
j and Râ€²
j as follows. For A m Ã— n (m > n) of rank n we have A = QR with Q a
30

must then be vectors dj and tj that satisfy Î»j = Ujdj and Î¸j = Vjtj. dj and tj represents
the free parameters and it is more convenient to work directly with them. The implied
prior for dj and tj is obtained by Waggoner and Zha as
dj âˆ¼N
 0, Î£d,j

, tj|dj âˆ¼N
 M
f
jdj, Î£t,j

(40)
with
Î£t,j =
 Vâ€²
jÎ£âˆ’1
Î¸,jVj
âˆ’1
M
f
j = Î£t,jVâ€²
jÎ£âˆ’1
Î¸,jMjUj
Î£d,j =

Uâ€²
jÎ£âˆ’1
Î»,jUj + Uâ€²
jMâ€²
jÎ£âˆ’1
Î¸,jMjUj âˆ’M
f
â€²
jÎ£âˆ’1
t,j M
f
j
âˆ’1
.
In the case that there are no restrictions on Î¸j we can take Vj = Ik and the expressions
simplify to Î£t,j = Î£Î¸,j, M
f
j = MjUj and Î£d,j =
 Uâ€²
jÎ£âˆ’1
Î»,jUj
âˆ’1 .
Let H = (U1d1, . . . , Umdm), the likelihood for dj and tj, j = 1, . . . , m is then
L (Y|d, t) âˆ|det H|T exp
(
âˆ’1
2
m
X
j=1
(YUjdjâˆ’ZVjtj)â€² (YUjdjâˆ’ZVjtj)
)
= |det H|T exp

âˆ’1
2
X
dâ€²
j

YUjâˆ’ZVjc
Mj
â€² 
YUjâˆ’ZVjc
Mj

dj

Ã— exp

âˆ’1
2
X 
tj âˆ’c
Mjdj
â€²
Vâ€²
jZâ€²ZVj

tj âˆ’c
Mjdj

for c
Mj =
 Vâ€²
jZâ€²ZVj
âˆ’1 Vâ€²
jZâ€²YUj. Multiplying with the prior (40), completing the square
for tj and collecting terms yields the joint posterior
p (d, t|YT) âˆ|det H|T exp

âˆ’1
2
X
dâ€²
j
h
Uâ€²
jYâ€²YUj+M
f
â€²
jÎ£âˆ’1
t,j M
f
j âˆ’M
â€²
jÎ£
âˆ’1
t,j Mj + Î£âˆ’1
d,j
i
dj

(41)
Ã— exp

âˆ’1
2
X  tj âˆ’Mjdj
â€² Î£
âˆ’1
t,j
 tj âˆ’Mjdj

with Î£t,j =
 Î£âˆ’1
t,j + Vâ€²
jZâ€²ZVj
âˆ’1 and f
Mj = Î£t,j
 Î£âˆ’1
t,j M
f
j + Vâ€²
jZâ€²YUj

. The conditional
posterior for tj is thus normal,
tj|YT, dj âˆ¼N

f
Mjdj, Î£t,j

,
and the conditional posteriors for tj are independent conditional on d1, . . . , dm. The
marginal posterior for d1, . . . , dm is given by the ï¬rst line of (41) where we must take
account of H being a function of d1, . . . , dm. Clearly this is not a known distribution even
though it in part looks like a normal distribution with mean zero for dj.
m Ã— m orthonormal matrix and R =
 Râ€²
1, 0nÃ—(mâˆ’n)
â€² m Ã— n with R1 upper triangular. We then have
Aâ€²Q = Râ€²Qâ€²Q = Râ€². Partioning Q = (Q1, Q2) with Q2 m Ã— (m âˆ’n) we see that Aâ€²Q2 = 0 and Q2 is
a basis for the null space of Aâ€². We can thus take Uj as the last qj columns of the Q matrix of the QR
decomposition of Qâ€²
j and Vj as the last rj columns of the Q matrix of the QR decomposition of Rj.
31

Waggoner and Zha (2003a) develops a Gibbs sampling algorithm for the marginal
posterior of d1, . . . , dm that operates on the posterior distributions for dj conditional on
di, i Ì¸= j, the set of full conditional posteriors. To this end let
Sâˆ’1
j
= 1
T
h
Uâ€²
jYâ€²YUj+M
f
â€²
jÎ£âˆ’1
t,j M
f
j âˆ’M
â€²
jÎ£
âˆ’1
t,j Mj + Î£âˆ’1
d,j
i
,
TjTâ€²
j = Sj and write dj = Tj
Pqj
i=1 Î²iwi = TjWÎ² where W is a qj Ã— qj orthonormal
matrix with columns wi. The Jacobian for the change of variables from dj to Î²1, . . . , Î²qj
is unity and the trick is to choose W in a clever way where W does not depend on
dj. Let w be a m Ã— 1 vector that is orthogonal to each of Uidi, i Ì¸= j,13 set w1 =
Tâ€²
jUâ€²
jw/
q
wâ€²UTTâ€²
jUâ€²
jw and wâ€²
i = (w11wi1, . . . , wiâˆ’1,1wi1, âˆ’ciâˆ’1, 0, . . . , 0) /âˆšciâˆ’1ci for i =
2, . . . , qj where wi1 is element i of w1 and ci = Pi
k=1 w2
k1. By construction UjTjw1 is
linearly independent of Uidi, i Ì¸= j, while UjTjwi, i > 1, are contained in the column
space. Consequently
det H = det
 
U1d1, . . . , Ujâˆ’1djâˆ’1, UjTj
qj
X
i=1
Î²iwi, Uj+1dj+1, . . . , Umdm
!
=
qj
X
i=1
Î²i det (U1d1, . . . , Ujâˆ’1djâˆ’1, UjTjwi, Uj+1dj+1, . . . , Umdm)
= Î²1 det (U1d1, . . . , Ujâˆ’1djâˆ’1, UjTjw1, Uj+1dj+1, . . . , Umdm) âˆÎ²1,
Pm
i=1 dâ€²
iSâˆ’1
i di = Pqj
k=1 Î²2
k + P
iÌ¸=j dâ€²
iSâˆ’1
i di and the conditional posterior simpliï¬es to
p (Î²1, . . . , Î²q|YT, diÌ¸=j) âˆ|Î²1|T exp
"
âˆ’T
2
qj
X
k=1
Î²2
k
#
= |Î²1|T exp

âˆ’TÎ²2
1
2

exp
"
âˆ’T
2
qj
X
k=2
Î²2
k
#
the product of a Gamma distribution for r = Î²2
1 and qj âˆ’1 independent N (0, 1/T)
distributions.14
Specifying the prior
Waggoner and Zha (2003a) starts by specifying a prior (39) for
the unrestricted structural form parameters, Î»j and Î¸j, and conditions on the restrictions
(38) in order to derive the prior (40) for the free parameters dj and tj in each equation.
As a default, the conditional prior for Î¸j can be speciï¬ed as in the unrestricted SVAR,
e.g. prior variances in accordance with (37) and a choice of M indicating if variables
are believed to be non-stationary or not. Unlike the unrestricted SVAR there are no
13w can be obtained by solving the equation system wâ€²Uidi = 0, i Ì¸= j. A practical method is to
form the m Ã— (m âˆ’1) matrix A = (U1d1, . . . , Ujâˆ’1djâˆ’1, Uj+1dj+1, . . . , Umdm) and calculate the QR
decomposition A = QR and set w = qm, the last column of Q. Since the last row of R is zero and Q is
orthonormal we have wâ€²A = wâ€²QR = 0.
14The distribution of r
=
Î²2
1 is f (r)
âˆ
rT/2 exp
 âˆ’T r
2
  âˆ‚Î²1
âˆ‚r

=
r(T +1)/2âˆ’1 exp
 âˆ’T r
2

/2 a
Gamma ((T + 1) /2, T/2) distribution.
32

computational gains from treating the equations symmetrically and the hard restrictions
in (38) can easily be combined with â€softâ€ restrictions on speciï¬c parameters.
It might be diï¬ƒcult to formulate economically meaningful priors on Î»j with the prior
means ï¬xed at zero as in (39) but one can, at least, be informative about the relative
magnitude of coeï¬ƒcients by working with the prior variances. Imposing the restrictions
(38) can have unexpected consequences on the prior if there is prior correlation between
coeï¬ƒcients and the implied prior for Î» and Î¸ should be checked in this case.
Sampling form the posterior
The sampler developed by Waggoner and Zha (2003a)
is straightforward to implement and outlined in algorithm 6.
4.3
Identiï¬cation under general restrictions
Following Rothenberg (1971) we say that a parameter point (Î›, Î˜) is identiï¬ed if there
is no other parameter point that is observationally equivalent, i.e. that they imply the
same likelihood and hence the same reduced form parameters. Since the reduced form
parameters are given by Î“ = Î˜Î›âˆ’1, Î¨ = (Î›Î›â€²)âˆ’1 it is clear that (Î›, Î˜) and

eÎ›, eÎ˜

are
observationally equivalent if and only if there exists an orthonormal matrix P such that
eÎ› =Î›P and eÎ˜ = Î˜P. A SVAR is thus (globally) identiï¬ed at (Î›, Î˜) , subject to a set of
restrictions, if the only orthonormal matrix for which both (Î›, Î˜) and (Î›P, Î˜P) satisï¬es
the restrictions is the identity matrix.
Rubio-Ramirez, Waggoner and Zha (2010) considers general restrictions on the struc-
tural form parameters and obtains necessary and suï¬ƒcient conditions for identiï¬cation of
SVARs. Let f (Î›, Î˜) be a nÃ—m matrix valued function of the structural form parameters
and Rj a rj Ã— n matrix of linear restrictions on column j of f (Î›, Î˜) , i.e.
Rjf (Î›, Î˜) ej = 0
(42)
for ej column j of the identity matrix Im where the structural form parameters are subject
to a normalization rule as in Waggoner and Zha (2003b).
The order of the columns
(equations) in f () is arbitrary, as a convention the columns are ordered so that r1 â‰¥r2 â‰¥
. . . â‰¥rm. Some regularity conditions on f () are needed in order to state the identiï¬cation
results:
â€¢ Admissible: the restrictions are said to be admissible if f (Î›P, Î˜P) = f (Î›, Î˜) P
for P any orthonormal matrix.
â€¢ Regular: the restrictions are said to be regular if the domain U of f () is an open
set and f is continuously diï¬€erentiable with f â€² of rank nm for all (Î›, Î˜) âˆˆU.
â€¢ Strongly regular: the restrictions are said to be strongly regular if f is regular and
f (U) is dense in the set of n Ã— m matrices.
Examples of admissible and strongly regular functions include the identity function
f (Î›, Î˜) = (Î›â€², Î˜â€²)â€² for linear restrictions on the parameters, the short run impulse
responses f (Î›, Î˜) = Î›âˆ’1, long run impulse responses f (Î›, Î˜) = (Î›â€² âˆ’Pp
i=1 Bâ€²
i)âˆ’1 as
well as intermediate impulse responses and combinations of these.
33

Algorithm 6 Gibbs sampler for restricted Structural form VARs
The sampler is based on the Gibbs sampler for d1, . . . , dm. Once convergence is achieved
draws of t1, . . . , tm can be obtained from the conditional posterior and the structural form
parameters Î› and Î˜ calculated. Start by precomputing the matrices, Uk, Vk, Sk, and
Tk for k = 1, . . . , m and select starting values d(0)
2 , . . . , d(0)
m .
For j = 1, . . . , B + R
1. For k = 1, . . . , m
(a) Construct
a
vector
w
that
is
orthogonal
to
U1d(j)
1 , . . . , Ukâˆ’1d(j)
kâˆ’1, Uk+1d(jâˆ’1)
k+1 , . . . , Umd(jâˆ’1)
m
and
calculate
the
vectors
w1, . . . , wqk.
(b) Generate r from a G ((T + 1) /2, T/2) and u from a uniform (0, 1) distribution.
Let Î²1 = âˆ’âˆšr if u â‰¤0.5 and set Î²1 = âˆšr otherwise.
(c) Generate Î²2, . . . , Î²qk as independent N (0, 1/T) random numbers
(d) Calculate d(j)
k
= Tk
Pqk
i=1 Î²iwi
2. If j > B,
(a) For k = 1, . . . , m generate t(j)
k
from the conditional posterior tk|YT, d(j)
k
âˆ¼
N

f
Mkd(j)
k , Î£t,k

(b) Calculate the structural form parameters Î»(j)
k
= Ukd(j)
k
and Î¸(j)
k
= Vkt(j)
k
for
k = 1, . . . , m and form the matrices Î›(j) and Î˜(j).
A normalization as in Waggoner and Zha (2003b) should be applied if the
purpose is inference on the structural form parameters or impulse responses.
(c) Calculate the reduced form parameters Î“(j) = Î˜(j)Î›(j).
(d) Generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
with Î¨(j) =
 Î›(j)Î›(j)â€²âˆ’1 and
calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B as a sample of inde-
pendent draws from the joint predictive distribution.
34

Theorems 1 and 3 of Rubio-Ramirez et al. (2010) establishes that an SVAR with
admissible and regular restrictions (42) is globally identiï¬ed almost everywhere15 if and
only if the matrices Mj (f (Î›, Î˜)) , j = 1, . . . , m, has rank m for some (Î›, Î˜) that satisï¬es
the restrictions. The (rj + j) Ã— m matrix Mj (f (Î›, Î˜)) is given by
Mj (f (Î›, Î˜)) =
 Rjf (Î›, Î˜)
Ij
0jÃ—(mâˆ’j)

with Mj (f (Î›, Î˜)) = (Ij, 0) if there are no restrictions on column j of f () .
Rubio-Ramirez et al. (2010, theorem 7) also develops a simple necessary and suï¬ƒcient
condition for exact identiï¬cation.16 A SVAR with admissible and strongly regular restric-
tions (42) is exactly identiï¬ed if and only if rj = m âˆ’j, j = 1, . . . , m. The restrictions
must thus follow a pattern, a simple special case is when Î› is a triangular matrix with
no other restrictions on the structural form parameters as in section 4.1.
To illustrate consider the following structure for the contemporaneous parameters in
Î› (see Rubio-Ramirez et al. (2010, section 5.2) for motivation and additional details, note
that our deï¬nition of Rj and consequently Mj diï¬€ers in that we leave out redundant rows
of these matrices)
Î› =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î»11
Î»12
0
Î»14
Î»15
0
Î»22
0
Î»24
Î»25
0
0
Î»33
Î»34
Î»35
0
0
Î»43
Î»44
Î»45
0
0
0
0
Î»55
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
With no other restrictions, f () is just f (Î›, Î˜) = Î› and the corresponding restriction
matrices are
R1 = (04Ã—1, I4) , R2 = (03Ã—2, I3) , R3 =
ï£«
ï£­
eâ€²
1
eâ€²
2
eâ€²
5
ï£¶
ï£¸, R4 = eâ€²
5.
We can immediately see that the SVAR would be exactly identiï¬ed if there was one less
zero restriction on the third column of Î›, or â€“ after reordering the equations â€“ one less
restriction on the second equation. As is, we need to verify that there exists a parameter
point that satisï¬es the restrictions and for which all the Mj matrices has full rank in
order to establish global identiï¬cation. Multiplying f (Î›, Î˜) with Rj and ï¬lling out the
15â€Globally identiï¬ed almost everywhereâ€ implies that we can check the rank condition at an arbitrary
parameter point satisfying the restrictions.
16The SVAR is exactly identiï¬ed if for all, except for a set of measure zero, reduced form parameters
(Î“, Î¨) there is a unique structural parameter point (Î›, Î˜) satisfying Î“ = Î˜Î›âˆ’1, Î¨ =
 Î›Î›â€²âˆ’1 .
35

bottom rows we have
M1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
Î»22
0
Î»24
Î»25
0
0
Î»33
Î»34
Î»35
0
0
Î»43
Î»44
Î»45
0
0
0
0
Î»55
1
0
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
, M2 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
Î»33
Î»34
Î»35
0
0
Î»43
Î»44
Î»45
0
0
0
0
Î»55
1
0
0
0
0
0
1
0
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
M3 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î»11
Î»12
0
Î»14
Î»15
0
Î»22
0
Î»24
Î»25
0
0
0
0
Î»55
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
, M4 =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
0
0
0
Î»55
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
, M5 = I5
M5 is trivially full rank and M1, M2, M4 have, for example, full rank if Î»22, Î»33, Î»44
and Î»55 are non-zero with Î»24 = Î»25 = Î»34 = Î»35 = Î»43 = Î»45 = 0. M3 will have full rank
if, in addition, Î»14 is non-zero with Î»11 = Î»12 = Î»15 = 0. The SVAR is thus identiï¬ed.
Rubio-Ramirez et al. (2010, theorem 5) also gives an alternative condition for exact
identiï¬cation that is useful for posterior simulation.
A SVAR is exactly identiï¬ed if
for almost every structural parameter point (Î›, Î˜) âˆˆU there is a unique orthonormal
matrix P such that (Î›P, Î˜P) satisï¬es the restrictions. That is, we can simulate from
the unrestricted SVAR with Î› triangular and transform the draws into parameter points
that satisfy a set of exactly identifying restrictions provided that we can ï¬nd the matrix
P. Normally a transformation with an orthonormal matrix will not aï¬€ect the posterior
distribution since the Jacobian is unity.
In this case P is a function of (Î›, Î˜) and
some care is needed to ensure that (Î›P, Î˜P) and (Î›, Î˜) has the same prior distribution.
Rubio-Ramirez et al. (2010, theorem 5) veriï¬es that the prior (39) with common variances
for the equations, i.e., Î» âˆ¼N (0, Im âŠ—Î£Î») and Î¸|Î» âˆ¼N (vec (MÎ›) , Im âŠ—Î£Î¸) , is â€“ due
to the Kronecker structure of the variances and the zero mean for Î» â€“ unaï¬€ected by a
transformation with P. It is also easy to see that this holds if the proper prior on Î» is
replaced by the improper prior p (Î») âˆ|det Î›|v since det (Î›P) = Â± det Î›. In addition,
since an orthonormal transformation is observationally equivalent, it is possible to work
with any prior on the reduced form parameters, sample from the posterior distribution of
(Î“, Î¨) , Cholesky decompose Î¨ = LLâ€² and transform to a triangular SVAR with Î›= Lâˆ’T
and Î˜ = Î“Lâ€².
An important implication of the alternative condition for exact identiï¬cation is that,
modulo the eï¬€ects of the prior speciï¬cation, the predictive distribution from a simple
triangular SVAR or reduced form VAR is identical to the predictive distribution from
any exactly identiï¬ed SVAR. That this is the case is readily seen by noting that the
orthonormal transformation (Î›P, Î˜P) has no eï¬€ect on reduced form parameters. For
forecasting purposes it is thus, depending on the choice of prior speciï¬cation, suï¬ƒcient to
work with the reduced form model or a triangular SVAR as long as the set of restrictions
considered identify the SVAR exactly.
Note that the triangular SVAR can be as in
section 4.1 with p (Î») âˆ|det Î›|v or as in section 4.2 with Î» âˆ¼N (0, Im âŠ—Î£Î») and
restrictions QiÎ»j = 0 yielding a triangular Î›. For completeness the algorithm for ï¬nding
the orthonormal transformation matrix P devised by Rubio-Ramirez et al. (2010) and
generating random numbers from the posterior distribution of an exactly identiï¬ed SVAR
36

Algorithm 7 Sampler for exactly identiï¬ed SVARs
Depending on the choice of prior speciï¬cation, generate reduced form parameters
 Î“(j), Î›(j)
using one of the algorithms in section 3 or the structural form parameters
 Î›(j), Î˜(j)
for a triangular SVAR using algorithm 5 with p (Î») âˆ|det Î›|v or algorithm
6 with Î» âˆ¼N (0, Im âŠ—Î£Î»). In the former case calculate the Cholesky decomposition
Î¨(j)= LLâ€² and set Î›(j)= Lâˆ’T, Î˜(j)= Î“(j)Lâ€². Discard any burn-in as needed.
For each draw from the original sampler
1. For k = 1, . . . , m
(a) Set eRk =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Rkf

Î›(j), Î˜(j)
pâ€²
1
. . .
pâ€²
jâˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£¸(eR1 = R1f

Î›(j), Î˜(j)
)
(b) Solve for eRjpj = 0, for example by calculating the QR decomposition of eRâ€²
j =
QR and setting pj = qm, the last column of Q.
2. Form P = (p1, . . . , pm) and calculate structural form parameters eÎ›(j) = Î›(j)P and
eÎ˜(j)= Î˜(j)P that satisï¬es the restrictions.
is given as algorithm 7.
Forecasting performance
Â¨Osterholm (2008b) use a structural BVAR to construct fan
charts for Sweden and provides a limited forecast evaluation. The model contains nine
variables, the foreign trade weighted GDP growth, inï¬‚ation and interest rate, the Swedish
unemployment rate, GDP growth, growth rate in wages, inï¬‚ation, interest rate and the
trade weighted exchange rate. The SVAR puts restriction on the Î› matrix which has
a basic lower triangular structure with the additional restrictions Î»2,1 = Î»3,1 = Î»4,1 =
Î»4,2 = Î»4,3 = Î»5,4 = Î»6,3 = Î»7,4 = Î»8,1 = Î»8,5 = 0 and allows Î»4,5, Î»5,7, Î»5,8, Î»6,7 and
Î»8,9 to be non-zero. In the forecast evaluation a steady-state version of the SVAR and a
naive random walk is also included. The steady state SVAR produces the best forecasts
for Swedish inï¬‚ation and forecast performance of the SVAR is somewhat better than
the random walk. For GDP growth the steady state SVAR is again best followed by the
random walk and the SVAR. The random walk provides the best forecasts for the inï¬‚ation
rate followed by the steady state SVAR and the SVAR.
5
Cointegration
Cointegration, that two or more non-stationary (integrated) variables can form a station-
ary linear combination and thus are tied together in the long run, is a powerful concept
that is appealing both from an economic and forecasting standpoint. Economically this
can be interpreted as a statement about long run equilibria and the information that the
variables tend to move together in the long run should be useful for forecasting.
37

In order to explicitly model the cointegrating properties of the data we write the VAR
(6) in error correction form
âˆ†yt = Î ytâˆ’1 +
pâˆ’1
X
i=1
Biâˆ†ytâˆ’i + Câ€²xt + ut
(43)
where Î  = âˆ’(Im âˆ’Pp
i=1 Aâ€²
i) and Bi = âˆ’Pp
j=i+1 Aâ€²
j. If the m time series in yt are
stationary Î  is a full rank matrix and if they all are non-stationary, integrated of order
1 or I (1) , but there is no cointegration Î  will be a zero matrix. Here the focus is on the
intermediate case where Î  is of reduced rank r < m and can be decomposed into two
mÃ—r matrices Î  = Î±Î²â€² with Î² forming r cointegrating relations, Î²â€²yt, or stationary linear
combinations of the I (1) variables in yt. The analysis of the cointegrated VECM (43) is
complicated by the non-linear parameterization and, more fundamentally, by two issues of
identiï¬cation. Firstly, Î± and Î² are globally non-identiï¬ed since any transformation with
a full rank matrix eÎ± = Î±P, eÎ² = Î²Pâˆ’T leaves Î  unchanged. This is commonly solved by
imposing a normalization Î²â€² = (Ir, Î²â€²
âˆ—) but this can, as we shall see later, be problematic.
Secondly, as noted by Kleibergen and van Dijk (1994), Î² is locally non-identiï¬ed when Î±
has reduced rank, e.g. when Î± = 0. See Koop, Strachan, van Dijk and Villani (2006) for
a more comprehensive review of Bayesian approaches to cointegration.
5.1
Priors on the cointegrating vectors
It is relatively straightforward to form prior opinions about the cointegrating vectors,
for example in the form of speciï¬c relations between the variables that are suggested
by economic theory. It is thus quite natural to formulate a prior on the cointegrating
vectors, Î², and proceed with the analysis based on this prior. This leads to a relatively
straightforward procedure for posterior inference but it is not without problems as it
overlooks some of the fundamental issues in the analysis of the cointegrated VAR-model.
For a given number of cointegrating relations, r, the VECM can be rewritten in matrix
form as
Yâˆ†= Yâˆ’1Î²Î±â€² + XÎ˜ + U
(44)
= ZÎ²Î“ + U
where Yâˆ†has rows âˆ†yt, Yâˆ’1 rows ytâˆ’1, X rows
 âˆ†yâ€²
tâˆ’1, . . . , âˆ†yâ€²
tâˆ’p+1, xâ€²
t

, ZÎ²= (Yâˆ’1Î², X) ,
and Î˜â€²= (B1, . . . , Bpâˆ’1, Câ€²) and Î“â€²= (Î±, Î˜â€²) k Ã— m and (k + r) Ã— m parameter matrices.
With ut âˆ¼N (0, Î¨), and conditioning on Î², (44) is just a standard multivariate regression
model and can be analyzed using one of the prior families for (Î“, Î¨) discussed in section
3.2 if there is prior independence between Î² and (Î“, Î¨). In particular, Geweke (1996a)
speciï¬ed an independent normal-Wishart type prior (section 3.2.2) for the parameters in
(44) with Î¨ âˆ¼iW (S, v) and independent normal priors for vec (Î±) , vec (Î²) and vec (Î˜)
with mean zero and variance-covariance matrix Ï„ âˆ’2I. Based on this he derived the full
conditional posteriors and proposed a Gibbs sampling algorithm for exploring the joint
posterior. Here we will consider a slightly more general prior speciï¬cation,
vec (Î±â€²) âˆ¼N (vec (Î±â€²) , Î£Î±) , Î¸ = vec (Î˜) âˆ¼N (Î¸, Î£Î¸) , Î¨ âˆ¼iW (S, v)
(45)
38

and an independent normal prior for the free elements of Î² to be speciï¬ed later. Note
that the prior for Î± is speciï¬ed in terms of the transpose of Î±. The full conditionals for
Î¨, Î± and Î¸ are obtained using standard results. We have
Î¨|YT, Î², Î“ âˆ¼iW
 S, v

, S = S + (Y âˆ’ZÎ²Î“)â€² (Y âˆ’ZÎ²Î“) , v = v + T.
(46)
Combine the priors for Î± and Î˜ into a joint prior for Î“ = (Î±, Î˜â€²)â€² , Î³ = vec (Î“) âˆ¼
N
 Î³, Î£Î³

, we then have the full conditional posterior as
Î³|YT, Î², Î¨ âˆ¼N
 Î³, Î£Î³

(47)
where Î³ and Î£Î³ are given by (24) with Z and Y replaced by ZÎ² and Yâˆ†.
The full conditional posterior for Î² is more complicated due to the nonlinear nature
of the model and the need for at least r2 identifying restrictions. A common identifying
scheme is to set Î²â€² = (Ir, Î²â€²
âˆ—) , more generally we can consider restrictions of the form
RiÎ²i = ri on the individual cointegrating vectors (columns of Î²). These restrictions are
conveniently expressed as Î²i = hi + HiÎ¾i where Î¾i corresponds to the free parameters in
Î²i.17 To derive the full conditional posterior for Î¾, we follow Villani (2001) and vectorize
the model YÎ¸ = Yâˆ†âˆ’XÎ˜ = Yâˆ’1Î²Î±â€² + U to obtain
yÎ¸ = (Î± âŠ—Yâˆ’1) vec (Î²) + u = Yâˆ’1,Î± (h + HÎ¾) + u
yÎ¸,Î± = yÎ¸ âˆ’Yâˆ’1,Î±h = Yâˆ’1,Î±HÎ¾ + u
where h = (hâ€²
1, . . . hâ€²
r)â€² , H = diag (Hi) and Î¾ = (Î¾â€²
1, . . . , Î¾â€²
r) . With a normal prior on Î¾,
Î¾ âˆ¼N
 Î¾, Î£Î¾

,
(48)
i.e. vec (Î²) âˆ¼N
 h + HÎ¾, HÎ£Î¾Hâ€²
which is a degenerate distribution due to the restric-
tions on Î², standard results yields the full conditional posterior as
Î¾|YT, Î“, Î¨ âˆ¼N
 Î¾, Î£Î¾

(49)
Î£Î¾ =
 Î£âˆ’1
Î¾
+ Hâ€²  Î±â€²Î¨âˆ’1Î± âŠ—Yâ€²
âˆ’1Yâˆ’1

H
âˆ’1
Î¾ = Î£Î¾

Î£âˆ’1
Î¾ Î¾ + Hâ€²  Î±â€²Î¨âˆ’1 âŠ—Yâ€²
âˆ’1

yÎ¸,Î±

.
A Gibbs sampler can thus easily be constructed by sampling from the full conditional
posteriors for Î¾ (and forming Î²), Î“ and Î¨.
It is, as noted by among others Kleibergen and van Dijk (1994) and Geweke (1996a),
crucial that a proper prior are used for Î² and Î±. Without this the local nonidentiï¬cation,
as well as the possibility that the true cointegrating is less than r, will lead to an improper
posterior.
It is also possible to work with a normal-Wishart type prior as in section 3.2.1, this is
close to a conjugate prior and leads to some simpliï¬cations. Bauwens and Lubrano (1996)
achieve similar simpliï¬cations with an uninformative Jeï¬€reys type prior Ï€ (Î±, Î˜, Î¨) âˆ
|Î¨|âˆ’(m+1)/2 together with an independent prior on Î². Similar to Sugita (2002) we specify
a normal-Wishart type prior,
Î±â€²|Î¨ âˆ¼MNrm (Î±â€², Î¨, â„¦Î±) , Î˜|Î¨ âˆ¼MNkm (Î˜, Î¨, â„¦Î¸) and Î¨ âˆ¼iW (S, v)
(50)
17Set hi = ei, column i in the identity matrix Im, and Hi =
 0(mâˆ’r)Ã—r, Imâˆ’r
â€² to obtain the â€defaultâ€
normalisation Î²â€² = (Ir, Î²â€²
âˆ—) .
39

together with the independent normal prior (48) for the free elements Î¾ in Î².18 It is
convenient to combine the priors on Î± and Î˜ in a prior on Î“,
Î“|Î¨ âˆ¼MN(r+k),m
 Î“, Î¨, â„¦Î³

, Î“â€² = (Î±, Î˜â€²) , â„¦Î³ = diag (â„¦Î±, â„¦Î¸) .
(51)
With the prior for Î² independent of Î“ and Î¨ it is clear that the posterior for Î“ and Î¨
conditional on Î² is of the normal-Wishart form,
Î“|YT, Î², Î¨ âˆ¼MN(r+k),m
 Î“, Î¨, â„¦Î³

,
(52)
â„¦
âˆ’1
Î³
= â„¦âˆ’1
Î³
+ Zâ€²
Î²ZÎ², Î“ = â„¦Î³
 â„¦âˆ’1
Î³ Î“ + Zâ€²
Î²Yâˆ†

,
Î¨|YT, Î² âˆ¼iW
 S, v

,
(53)
S= S + S+

Î“ âˆ’bÎ“
â€² 
â„¦Î³ +
 Zâ€²
Î²ZÎ²
âˆ’1âˆ’1 
Î“ âˆ’bÎ“

, v = T + v.
Peters et al. (2010) propose using a Metropolis within Gibbs MCMC scheme for sam-
pling from the joint posterior distribution of Î², Î“ and Î¨ with a Metropolis-Hastings step
for Î². Peters et al. (2010) considered two random walk type proposals, a mixture pro-
posal with one component designed to produce local moves and one component producing
global moves and an adaptive proposal where variance-covariance matrix is continuously
updated based on the previous output of the Markov chain.
The MCMC scheme of Peters et al. (2010) has the advantage that it does not rely on a
speciï¬c form for the prior on Î². On the other hand, if we specify a normal prior for Î² (or
Î¾) the derivations leading to the full conditional posterior (49) with the normal-Wishart
type prior on Î“ and Î¨ and a standard Gibbs sampler is available in this case.
Specifying the prior beliefs
The Minnesota prior is a useful starting point when
thinking about the prior for Î˜. Considering that Î˜ contains B1, . . . , Bpâˆ’1 which are
autoregressive coeï¬ƒcient matrices on the stationary ï¬rst diï¬€erences a reasonable choice
is to set the prior means to zero and prior variances as in 14 with the modiï¬cations
discussed in section 3.2.1 for the normal-Wishart type prior. Alternatively one can start
with a Minnesota prior for the autoregressive parameters Ai in the reduced form VAR
(6) and derive the prior mean and variance for Bj from the relation Bj = âˆ’Pp
i=j+1 Aâ€²
i.
The priors for Î± and Î² (or Î¾) is a more delicate matter. Economic theory can in many
cases suggest plausible cointegrating vectors and restrictions RiÎ²i = ri. Care is however
needed in the speciï¬cation of the restrictions, at least one element of ri must be nonzero
otherwise the ith cointegrating vector will only be identiï¬ed up to an arbitrary scale factor.
This in turn has implication for which variables are, in fact, cointegrated and ï¬xing the
coeï¬ƒcient of a variable that is not cointegrated to a non-zero value will clearly result in
misleading inference. It is harder to form prior beliefs about the adjustment coeï¬ƒcients
Î± and a relatively uninformative prior with zero mean might be suitable. Note, however,
that under prior independence we have E (Î ) = E (Î±) E (Î²â€²) and a prior mean of zero
for Î± or Î² implies that E (Î ) = 0 which is at odds with the assumption that Î  has rank
r > 0.
18Sugita (2002) and later Peters, Kannan, Lassock and Mellen (2010) used a matric-variate normal for
Î²âˆ—in the normalization Î²â€² = (Ir, Î²â€²
âˆ—) but there seem to be no particular advantage to the more restrictive
Kronecker structure of the prior variance-covariance.
40

Algorithm 8 Gibbs sampler for VECM with a prior on Î²
With the normal-Wishart type prior (50), (48) select starting values Î²(0)
For j = 1, . . . , B + R
1. Generate Î¨(j) from the conditional posterior Î¨|YT, Î²(jâˆ’1) âˆ¼iW
 S, v

in (53)
2. Generate
Î“(j)
from
the
full
conditional
posterior
Î“|YT, Î²(jâˆ’1), Î¨(j)
âˆ¼
MN(r+k),m

Î“, Î¨(j), â„¦Î³

in (52)
3. Generate Î¾(j) from the full conditional posterior Î¾|YT, Î“(j), Î¨(j) âˆ¼N
 Î¾, Î£Î¾

in (49)
and form Î²(j)
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h
using A1 = I + Bâ€²
1 + Î²Î±â€², Ai = Bâ€²
i âˆ’Bâ€²
iâˆ’1, i = 2, . . . , p âˆ’1 and Ap = âˆ’Bâ€²
pâˆ’1.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
Simulating from the posterior distribution
The adaptive MCMC scheme is well
described in Peters et al. (2010) and the Gibbs sampler is outlined in algorithm 8.
5.2
Priors on the cointegrating space
As alluded to above, the approach of working directly with the cointegrating vectors in Î²
can be problematic. The issues are most easily discussed in relation to the linear normal-
ization Î² = (Ir, Î²â€²
âˆ—)â€² frequently used to identify the model. Partitioning Î² = (Î²â€²
1, Î²â€²
2) with
Î²1 a r Ã— r matrix, the normalization sets Î²âˆ—= Î²2Î²âˆ’1
1 . This has two implications, ï¬rstly
the variables must be ordered in such a way that Î²1 is a full rank matrix, secondly Î²âˆ—will
have a fat tailed distribution, possibly with no posterior moments19, unless a suï¬ƒciently
informative prior on Î²âˆ—is used.
The fundamental issue underlying this is the lack of identiï¬cation of Î² and that only
the space spanned by Î² (the cointegrating space, p = sp (Î²)) is identiï¬ed by the model.
As argued by Villani (2000) we should then consider the prior for Î² in terms of this
space. The columns of the rank r, mÃ—r matrix Î² deï¬nes an r-dimensional hyperplane in
Rm, the space spanned by the columns of Î². Formally, the set of all such hyperplanes is
known as the Grassman manifold, Gr,mâˆ’r, and Villani (2005) shows that a uniform prior
on Gr,mâˆ’r implies a matricvariate t distribution with r degrees of freedom on Î²âˆ—, Î²âˆ—âˆ¼
19For the case of one cointegrating vector, r = 1, and an impoper prior on Î² Bauwens and Lubrano
(1996) shows that the marginal posterior for Î² has ï¬nite moments up to the order of the number of
overidentifying restrictions.
That is, for r = 1 two restrictions in addition to normalizing the ï¬rst
element of Î² to 1 is needed for the posterior variance to exist.
41

Mtmâˆ’r,r (0, I, I,r) or p (Î²âˆ—) âˆ|Ir + Î²â€²
âˆ—Î²âˆ—|âˆ’m/2 , when the linear normalization is used.
This is quite diï¬€erent from using a uniform prior on Î²âˆ—. While the linear normalization
implies a strong prior belief that the ï¬rst r variables are included in cointegrating relations
and that Î²1 is a full rank matrix, a uniform prior on Î²âˆ—will, as pointed out by Strachan
and Inder (2004), in fact put most of the prior mass on regions where Î²1 is (close to)
non-invertible.
Departing from the uniform prior on the cointegrating spaces we consider a prior that
is similar to the reference prior of Villani (2005),
Î²âˆ—âˆ¼Mtmâˆ’r,r (0, I, I,r)
(54)
Î±â€²|Î², Î¨ âˆ¼MNr,m

0, Î¨,câˆ’1 (Î²â€²Î²)âˆ’1
Î˜|Î¨ âˆ¼MNkm (Î˜, Î¨, â„¦Î¸)
Î¨ âˆ¼iW (S, v) .
The main diï¬€erence compared to Villani is that we use a normal prior for Î˜ instead of the
improper prior p (Î˜) âˆ1. The results for the ï¬‚at prior on Î˜ can be obtained by setting
â„¦âˆ’1
Î¸
to zero below.
The prior distribution for Î± can be motivated by considering the prior for Î± when
Î² is orthonormal, i.e. Î²â€²Î² = Ir.20 Postmultiplying Î² with (Î²â€²Î²)âˆ’1/2 results in a set of
orthogonalized cointegrating vectors eÎ² = Î² (Î²â€²Î²)âˆ’1/2 and to keep Î  = Î±Î²â€² unchanged we
need to adjust Î± accordingly, eÎ± = Î± (Î²â€²Î²)1/2 . It follows that the conditional distribution
of eÎ± is eÎ±â€²| eÎ², Î¨ âˆ¼MNr,m (0, Î¨,câˆ’1I) or eÎ±i|Î¨ âˆ¼N (0, câˆ’1Î¨) , i = 1, . . . , r. Note that
within the class of matricvariate normal priors eÎ±â€²| eÎ², Î¨ âˆ¼MNr,m (Âµ, â„¦1, â„¦2) the only
ones which are invariant to orthogonal rotations of eÎ² are those with Âµ = 0 and â„¦2 =
câˆ’1I. The marginal prior for eÎ±â€² is matricvariate t and the prior variance-covariance is
V (vec eÎ±) =
1
c(vâˆ’mâˆ’1)Ir âŠ—S which clariï¬es the role of the scale factor câˆ’1 in tuning the
prior variance.
Writing the VECM as YÎ±Î² = Yâˆ†âˆ’Yâˆ’1Î²Î±â€² = XÎ˜ + U we can derive the posterior
distributions for Î˜ and Î¨ conditional on Î± and Î². This requires that we keep track of
the contribution from the joint prior for Î± and Î² conditional on Î¨,
p (Î±, Î²|Î¨) âˆ|Î²â€²Î²|m/2 |Î¨|âˆ’r/2 exp

âˆ’1
2 tr
 Î¨âˆ’1Î± (cÎ²â€²Î²) Î±â€²
Ã— |Î²â€²Î²|âˆ’m/2
(55)
= |Î¨|âˆ’r/2 exp

âˆ’1
2 tr
 Î¨âˆ’1Î± (cÎ²â€²Î²) Î±â€²
where we have used that Î²â€²Î² = Ir + Î²â€²
âˆ—Î²âˆ—in the prior for Î².
Using standard results we obtain the conditional posteriors as
Î˜|YT, Î±, Î², Î¨ âˆ¼MNk,m
 Î˜, Î¨,â„¦Î¸

(56)
â„¦Î¸ =
 â„¦âˆ’1
Î¸
+ Xâ€²X
âˆ’1 , Î˜ = â„¦Î¸
 â„¦âˆ’1
Î¸ Î˜ + Xâ€²YÎ±Î²

,
20The restriction Î²â€²Î² = Ir is not suï¬ƒcient to identify Î² since it can always be rotated to a new
orthogonal matrix by postmultiplying with an r Ã— r orthogonal matrix.
42

and
Î¨|YT, Î±, Î² âˆ¼iW
 S, v

, v = T + v + r
(57)
S = S + S + cÎ±Î²â€²Î²Î±â€² + Î˜â€²â„¦âˆ’1
Î¸ Î˜ + bÎ˜â€²Xâ€²X bÎ˜ âˆ’Î˜
â€²â„¦
âˆ’1
Î¸ Î˜
for S =

YÎ±Î² âˆ’X bÎ˜
â€² 
YÎ±Î² âˆ’X bÎ˜

and bÎ˜ = (Xâ€²X)âˆ’1 Xâ€²YÎ±Î².
To derive the full conditional posterior for Î± write the VECM as YÎ¸ = Yâˆ†âˆ’XÎ˜ =
Yâˆ’1Î²Î±â€² +U and apply the results for the normal-Wishart prior in section 3.2.1 to obtain
the conditional posterior
Î±â€²|Î², Î˜, Î¨ âˆ¼MNr,m
 Î±â€², Î¨, â„¦Î±

(58)
â„¦Î± =

Î²â€²  cIm + Yâ€²
âˆ’1Yâˆ’1

Î²
âˆ’1
Î±â€² = â„¦Î±Î²â€²Yâ€²
âˆ’1YÎ¸.
For the full conditional posterior for Î² we note that the contribution from the prior can
be rewritten as tr Î¨âˆ’1Î± (cÎ²â€²Î²) Î±â€² = tr Î² (cÎ±â€²Î¨âˆ’1Î±) Î²â€² = tr cÎ±â€²Î¨âˆ’1Î±+tr Î²âˆ—(cÎ±â€²Î¨âˆ’1Î±) Î²â€²
âˆ—
with tr Î²âˆ—(cÎ±â€²Î¨âˆ’1Î±) Î²â€²
âˆ—= vec (Î²âˆ—)â€² (Î±â€²Î¨âˆ’1Î± âŠ—cImâˆ’r) vec (Î²âˆ—) . That is, the prior for Î²âˆ—
conditional on Î± and Î¨ is matricvariate normal, MNmâˆ’r,m

0, (Î±â€²Î¨âˆ’1Î±)âˆ’1 , câˆ’1Imâˆ’r

.
Next rewrite Yâˆ’1Î²Î±â€² = (Yâˆ’1,1 + Yâˆ’1,2Î²âˆ—) Î±â€² and vectorize the regression YÎ¸Î± = Yâˆ†âˆ’
XÎ˜ âˆ’Yâˆ’1,1Î±â€²= Yâˆ’1,2Î²âˆ—Î±â€² + U,
yÎ¸Î± = (Î± âŠ—Yâˆ’1,2) vec (Î²âˆ—) + u.
The full conditional posterior for Î²âˆ—is then obtained as
Î²âˆ—|YT, Î±, Î˜, Î¨ âˆ¼MNmâˆ’r,m

Î²âˆ—,
 Î±â€²Î¨âˆ’1Î±
âˆ’1 , â„¦Î²

(59)
â„¦Î² =
 cImâˆ’r + Yâ€²
âˆ’1,2Yâˆ’1,2
âˆ’1
Î²âˆ—= â„¦Î²Yâ€²
âˆ’1,2YÎ¸Î±Î¨âˆ’1Î±
 Î±â€²Î¨âˆ’1Î±
âˆ’1 .
Using the improper prior p (Î˜) âˆ1 instead of a normal prior as here Villani (2005)
derived the conditional posteriors p (Î±|YT, Î²) and p (Î²|YT, Î±) as well as the marginal
posterior for Î². Villani also shows that the posterior distribution of Î²âˆ—has no ï¬nite
moments as can be expected with the linear normalization Î² = (Ir, Î²â€²
âˆ—)â€² .
The choice of normalization or identifying restrictions on Î² is thus crucial. Strachan
(2003) proposes a data based normalization which restricts the length of the cointegrating
vectors and ensures that the posterior for Î² is proper with ï¬nite moments but also implies
that the prior for Î² or sp (Î²) is data based. Strachan and Inder (2004) instead propose
working with the normalization Î²â€²Î² = Ir. While this is not suï¬ƒcient to identify Î² it does
restrict Î² to the set of semi-orthonormal m Ã— r matrices, the Stiefel manifold Vr,m.
There is often prior information about likely cointegrating vectors and Strachan and
Inder (2004) proposes a convenient method for specifying an informative prior on the
cointegrating space. First specify an m Ã— r matrix with likely cointegrating vectors, e.g.
Hg =
ï£«
ï£­
1
0
âˆ’1
1
0
âˆ’1
ï£¶
ï£¸
43

for m = 3 and r = 2. Since sp (Hg) = sp (HgP) for any full rank r Ã—r matrix we can map
Hg into V2,3 by the transformation H = Hg
 Hâ€²
gHg
âˆ’1/2 and calculate the orthogonal
complement HâŠ¥, i.e. HâŠ¥âŠ†Vmâˆ’r,m and Hâ€²HâŠ¥= 0.21 That is,
H =
ï£«
ï£­
p
1/12 + 1
2
1
2 âˆ’
p
1/12
âˆ’
p
1/3
p
1/3
p
1/12 âˆ’1
2
âˆ’
p
1/12 âˆ’1
2
ï£¶
ï£¸, HâŠ¥=
ï£«
ï£­
p
1/3
p
1/3
p
1/3
ï£¶
ï£¸.
Next consider the space spanned by the matrix PÏ„ = HHâ€² + Ï„HâŠ¥Hâ€²
âŠ¥, for Ï„ = 0 this is
sp (H) and for Ï„ = 1 we have PÏ„ = Im and sp (PÏ„) = Rm. Specifying the prior for Î² as a
matrix angular central Gaussian distribution with parameter PÏ„, MACG (PÏ„) ,
p (Î²) âˆ|PÏ„|âˆ’r/2 Î²â€²Pâˆ’1
Ï„ Î²
âˆ’m/2 ,
(60)
centers the distribution of p = sp (Î²) on sp (H) with the dispersion controlled by Ï„. For
Ï„ = 0 we have a dogmatic prior that p = sp (H) and for Ï„ = 1 a uniform prior on
the Stiefel manifold which is equivalent to the uniform prior used by Villani (2005). By
varying Ï„ âˆˆ[0, 1] we can thus make the prior more or less informative.
Strachan and Inder (2004) propose using a Metropolis-Hastings sampler to evaluate
the posterior distribution of the parameters under the prior (60) on Î² and a prior sim-
ilar to (54) on the remaining parameters.
Koop, LeÂ´on-GonzÂ´alez and Strachan (2010)
propose a convenient Gibbs sampling scheme that depends on reparameterizing and in
turn sample from a parameterization where Î± is semiorthogonal and Î² unrestricted and
a parameterization where Î² is semiorthogonal and Î± is unrestricted.
This solves the
main computational diï¬ƒculty with the semiorthogonal normalization where it is diï¬ƒcult
generate Î² subject to the restriction Î²â€²Î² = Ir.
Koop et al. (2010) develops the Gibbs sampling algorithm in a VECM without lags
of âˆ†yt or deterministic variables. We will consider the more general model (44) and will
thus need a prior for Î˜ in addition to the prior on Î², Î± and Î¨ speciï¬ed by Koop et al.
(2010), in addition to (60) we have
Î±â€²|Î², Î¨ âˆ¼MNr,m

0, Î¨,câˆ’1  Î²â€²P1/Ï„Î²
âˆ’1
(61)
Î˜|Î¨ âˆ¼MNkm (Î˜, Î¨, â„¦Î¸)
Î¨ âˆ¼iW (S, v) ,
where P1/Ï„ = HHâ€² + Ï„ âˆ’1HâŠ¥Hâ€²
âŠ¥= Pâˆ’1
Ï„
a choice which facilitates the development of the
Gibbs sampler. Koop et al. (2010) also considers the improper prior p (Î¨) âˆ|Î¨|âˆ’m/2, the
results for this prior can be obtained by setting S = 0 and v = 0 below.
The key to the Gibbs sampler of Koop et al. (2010) is the reparameterization
Î±Î²â€² =
 Î±Îºâˆ’1
(Î²Îº)â€² =
h
Î± (Î±â€²Î±)âˆ’1/2i h
(Î±â€²Î±)1/2 Î²
iâ€²
= ABâ€²
21The square root matrix of a positive deï¬nite and symmetric matrix, such as C = Hâ€²
gHg, is unique
and can be obtained from the spectral decomposition C = XÎ›Xâ€² where X is the matrix of orthonormal
eigenvectors and Î› has the eigenvalues, Î»i, on the diagonal. Consequently C1/2 = XÎ›1/2Xâ€² with Î»1/2
i
as the diagonal elements of Î›1/2 and Hâ€²H =
 Hâ€²
gHg
âˆ’1/2 Hâ€²
gHg
 Hâ€²
gHg
âˆ’1/2 = I.
44

where A is semiorthogonal and B is unrestricted. For further reference note that the
transformations from Î± to (A, Îº) and from B to (Î², Îº) where Îº = (Î±â€²Î±)1/2 = (Bâ€²B)1/2
is symmetric and positive deï¬nite are one-to-one, in addition Î² = B (Bâ€²B)âˆ’1/2 and Î± =
A (Bâ€²B)1/2 . The implied priors for A and B can be obtained as (see Koop et al. (2010)
for details)
B|A, Î¨ âˆ¼MNm,r

0,
 Aâ€²Î¨âˆ’1A
âˆ’1 , câˆ’1PÏ„

(62)
A|Î¨ âˆ¼MACG (Î¨) .
The derivation of the full conditional posteriors proceed as above. The full conditional
posterior for Î˜ is matricvariate normal and given by (56) and the full conditional posterior
for Î¨ is inverse Wishart,
Î¨|YT, Î±, Î² âˆ¼iW
 S, v

, v = T + v + r
(63)
S = S + S + cÎ±Î²â€²P1/Ï„Î²Î±â€² + Î˜â€²â„¦âˆ’1
Î¸ Î˜ + bÎ˜â€²Xâ€²X bÎ˜ âˆ’Î˜
â€²â„¦
âˆ’1
Î¸ Î˜
with S and bÎ˜ as in the conditional posterior (57). The full conditional posterior for Î± is
a straightforward modiï¬cation of the conditional posterior (58),
Î±â€²|Î², Î˜, Î¨ âˆ¼MNr,m
 Î±â€², Î¨, â„¦Î±

(64)
â„¦Î± =

Î²â€²  cP1/Ï„ + Yâ€²
âˆ’1Yâˆ’1

Î²
âˆ’1
Î±â€² = â„¦Î±Î²â€²Yâ€²
âˆ’1YÎ¸.
The full conditional posterior for Î² is complicated by the semiorthogonal normaliza-
tion, instead the Gibbs sampling scheme of Koop et al. (2010) make use the full conditional
posterior for the unrestricted parameter B,
B|YT, A, Î˜, Î¨ âˆ¼MNm,r

B,
 Aâ€²Î¨âˆ’1A
âˆ’1 , â„¦B

(65)
â„¦B =
 cPâˆ’1
Ï„
+ Yâ€²
âˆ’1Yâˆ’1
âˆ’1
B = â„¦BYâ€²
âˆ’1 (Yâˆ†âˆ’XÎ˜) Î¨âˆ’1A
 Aâ€²Î¨âˆ’1A
âˆ’1 .
The idea behind the Gibbs sampler of Koop et al. (2010) is based on the fact that a
draw Î±(âˆ—) from (64) is also a draw
 A(âˆ—),Îº(âˆ—)
from p (A, Îº|YT, Î², Î˜, Î¨) , second drawing
B(j) from (65) yields a draw of
 Î²(j), Îº(j)
from p
 Î², Îº|YT, A(âˆ—), Î˜, Î¨

and we can map
A(âˆ—) and B(j) into Î±(j) = A(âˆ—)  B(j)â€²B(j)1/2 , Î²(j) = B(j)  B(j)â€²B(j)âˆ’1/2 and the draws
Îº(âˆ—) and Îº(j) are simply discarded.
In addition to the just identiï¬ed case discussed here, Koop et al. (2010), also studies
the case with overidentifying restrictions of the form Î²i = HiÎ¾i considered in section 5.1
and provides a Gibbs sampling algorithm.
Specifying the prior beliefs
The same considerations for the prior on Î˜ holds here as
in section 5.1. The informative prior (60) for Î² requires that we specify the tuning constant
Ï„. Keeping in mind that Ï„ = 0 corresponds to a dogmatic prior and Ï„ = 1 corresponds to
45

a uniform prior on sp (Î²), setting Ï„ < 1/2 seems appropriate. It is, however, diï¬ƒcult to
develop intuition for Ï„ and some sensitivity analysis is advisable. The choice of central
location H (or Hg) should obviously be based on economic intuition and theory or other
subject speciï¬c information.
The prior distribution for Î± requires a choice of the scale factor c, the prior is centered
on zero with variance V (vec Î±) =
1
c(vâˆ’mâˆ’1)
 Î²â€²P1/Ï„Î²
âˆ’1 âŠ—S conditional on Î². Evaluating
this at the central location Î² = H of the informative prior (60) or P1/Ï„ = I for the
uniform prior yields V (vec Î±) =
1
c(vâˆ’mâˆ’1)Ir âŠ—S which can serve as a guide when choosing
c. Alternatively, as suggested by Koop et al. (2010), a hierarchical prior structure can be
used with inverse Gamma priors on c (and v) if the researcher prefers to treat them as
unknown parameters.
Sampling from the posterior distribution
The essential diï¬€erence between the pos-
terior distributions discussed here is the type of normalization imposed on Î². For the linear
normalization Î² = (Ir, Î²â€²
âˆ—)â€² and a ï¬‚at prior on sp (Î²) an adaption of the Gibbs sampler of
Villani (2005) is given as Algorithm 9. For the orthogonal normalization Î²â€²Î² = Ir and a
possibly informative prior on the cointegrating space an adaption of the Gibbs sampler of
Koop et al. (2010) is given in algorithm 10. Note that the orthogonal normalization does
not identify Î² and additional normalizations may be needed to obtain easily interpretable
cointegrating vectors.
5.3
Determining the cointegrating rank
A simple approach to inference on the cointegrating rank, r, used in early Bayesian work,
e.g. DeJong (1992) and Dorfman (1995), is to work with the reduced form VAR using one
of the priors in section 3. In this context the posterior distribution of the cointegrating
rank can be obtained from the posterior distribution of the roots of the autoregressive
polynomial or the rank of the impact matrix Î  = âˆ’(Im âˆ’Pp
i=1 Aâ€²
i) . Sampling from the
posterior distribution of the parameters it is straightforward to estimate the posterior dis-
tribution of the cointegrating rank by counting the number of roots of the AR-polynomial
that are greater than, say, 0.99 or using the QR or SVD decompositions to ï¬nd the rank
of Î  for each draw from the posterior.
While the unrestricted reduced form approach is straightforward it does not take
account of the reduced rank restrictions on Î  for r < m. Proper Bayesian model selection
and model averaging account for this by basing the analysis on marginal likelihoods for
models with diï¬€erent cointegrating rank r and calculating posterior probabilities p (r|YT)
as in (5). This does, however, require some care to ensure that the marginal likelihood
is well deï¬ned. As a minimum proper priors for Î² and Î± are needed as these change
dimension with r and as a general rule at least mildly informative priors should be used
for all parameters with the possible exception of Î¨.
Using a prior on the cointegrating vectors as in section 5.1 with partially prespeci-
ï¬ed cointegrating vectors Villani (2001) approximates the log marginal likelihood with
the Bayesian Information Criteria of Schwarz (1978). Sugita (2002) shows how to use
the generalized Savage-Dickey density ratio of Verdinelli and Wasserman (1995) to com-
pute the Bayes factors BFi,0 comparing the model with r = i against the model with
r = 0 and the posterior probabilities p (r|YT) with the prior setup (50) together with a
46

Algorithm 9 Gibbs sampler for VECM with a prior on sp (Î²) and linear normalization
With the prior (54), an uninformative prior on sp (Î²) coupled with the linear normalization
Î² = (Ir, Î²â€²
âˆ—)â€² , select starting values Î±(0) and Î²(0).
For j = 1, . . . , B + R
1. Generate Î¨(j) from the full conditional posterior Î¨|YT, Î±(jâˆ’1), Î²(jâˆ’1) âˆ¼iW
 S, v

in (57)
2. Generate Î˜(j) from the full conditional posterior Î˜|YT, Î±(jâˆ’1), Î²(jâˆ’1), Î¨(j)
âˆ¼
MNk,m
 Î˜, Î¨,â„¦Î¸

in (56)
3. Generate
Î±(j)
from
the
full
conditional
posterior
Î±â€²|Î²(jâˆ’1), Î˜(j), Î¨(j)
âˆ¼
MNr,m
 Î±â€², Î¨(j), â„¦Î±

in (58)
4. Generate Î²(j)
from the full conditional posterior Î²âˆ—|YT, Î±(j), Î˜(j), Î¨(j)
âˆ¼
MNmâˆ’r,m

Î²âˆ—,

Î±(j)â€²  Î¨(j)âˆ’1 Î±(j)âˆ’1
, â„¦Î²

in (59)
5. If j > B generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h
using A1 = I + Bâ€²
1 + Î²Î±â€², Ai = Bâ€²
i âˆ’Bâ€²
iâˆ’1, i = 2, . . . , p âˆ’1 and Ap = âˆ’Bâ€²
pâˆ’1.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
matricvariate normal prior for Î²âˆ—. Villani (2005) derives closed form expressions for the
marginal likelihoods m (YT|r = 0) and m (YT|r = m) under the uniform prior on Gr,m
for sp (Î²) and linear normalization, i.e. the prior (54) but with Ï€ (Î˜) âˆ1, and uses the
Chib (1995) method to estimate the marginal likelihood for intermediate cases from the
Gibbs sampler output, e.g. Algorithm 9. With the orthogonal normalization and an un-
informative prior on Î˜ and Î¨, Ï€ (Î˜, Î¨) âˆ|Î¨|âˆ’(m+1)/2 Strachan and Inder (2004) derives
the posterior p (Î², r|YT) and use a Laplace approximation to integrate out Î² to obtain
the posterior distribution of the cointegrating rank. Sugita (2009) studies rank selection
in a Monte Carlo experiment where the marginal likelihood is approximated with BIC or
estimated using the Chib method and ï¬nds that BIC performs well when T â‰¥100 and
Chibâ€™s method requires considerably larger sample sizes, T > 500, to perform well.
Forecasting performance
Villani (2001) forecasts the Swedish inï¬‚ation rate with sev-
eral versions of a 7 variable VECM with the Swedish GDP, CPI, interest rate, trade
weighted exchange and â€foreignâ€ GDP, price level and interest rate. Villani considers
several theory based cointegrating relations which are all rejected by the data in favour of
a model with cointegrating rank 3 and unrestricted cointegrating relations. Nonetheless,
47

Algorithm 10 Gibbs sampler for VECM with a prior on sp (Î²) and orthogonal normal-
ization
With the orthogonal normalization Î²â€²Î² = Ir and the informative prior (60) and (61) the
Gibbs sampler of Koop et al. (2010) is applicable. Select starting values Î±(0) and Î²(0).
For j = 1, . . . , B + R
1. Generate Î¨(j) from the conditional posterior Î¨|YT, Î±(jâˆ’1), Î²(jâˆ’1) âˆ¼iW
 S, v

in
(63)
2. Generate Î˜(j) from the full conditional posterior Î˜|YT, Î±(jâˆ’1), Î²(jâˆ’1), Î¨(j)
âˆ¼
MNk,m
 Î˜, Î¨,â„¦Î¸

in (56)
3. Generate
Î±(âˆ—)
from
the
full
conditional
posterior
Î±â€²|Î²(jâˆ’1), Î˜(j), Î¨(j)
âˆ¼
MNr,m
 Î±â€², Î¨(j), â„¦Î±

in (64) and calculate A(âˆ—) = Î±(âˆ—)  Î±(âˆ—)â€²Î±(âˆ—)âˆ’1/2
4. Generate
B(j)
from
the
conditional
posterior
B|YT, A(âˆ—), Î˜(j), Î¨(j)
âˆ¼
MNm,r

B,

A(âˆ—)â€²  Î¨(j)âˆ’1 A(âˆ—)âˆ’1
, â„¦B

in
(65)
and
calculate
Î±(j)
=
A(âˆ—)  B(j)â€²B(j)1/2 and Î²(j) = B(j)  B(j)â€²B(j)âˆ’1/2
5. If j > B generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h
using A1 = I + Bâ€²
1 + Î²Î±â€², Ai = Bâ€²
i âˆ’Bâ€²
iâˆ’1, i = 2, . . . , p âˆ’1 and Ap = âˆ’Bâ€²
pâˆ’1.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
Villani goes ahead and forecasts with both theory based (PPP, stationary domestic and
foreign interest rates) and unrestricted cointegrating vectors with stationary Minnesota
type prior beliefs on the short run dynamics. Of the considered models, Bayesian VECMs
and ML-estimated VECMs and an ARIMA model, the Bayesian VECMs do best and are
very close to each other.
6
Conditional forecasts
It is often of interest to condition the forecasts on diï¬€erent scenarios, for example diï¬€erent
trajectories for the world economy, diï¬€erent developments of the oil price or diï¬€erent paths
for the interest rate considered by a central bank. Another use of conditional forecasts is
to incorporate information from higher frequency data or judgement into the model. An
early example of conditional forecasts is Doan et al. (1984) who note that conditioning on
a speciï¬c path for a variable is (given the parameters of the model) equivalent to imposing
a set of linear constraints on the future disturbances, uT+1, uT+2, . . . Conditional forecasts
48

can then be constructed by using the conditional means, buT+i, in the forecasting recursions
byâ€²
T+h =
hâˆ’1
X
i=1
byâ€²
T+hâˆ’iAi +
p
X
i=h
yâ€²
T+hâˆ’iAi + xâ€²
T+hC+buâ€²
T+h.
(66)
This approach, while straightforward, has two potential drawbacks.
It conditions on
speciï¬c parameter values (e.g.
the posterior means) and does not produce minimum
mean square error forecasts conditional on the restrictions. This can be overcome by
simulation from the posterior distribution of the parameters and solving for the restricted
distribution of the disturbances for each set of parameter values (the whole predictive
distribution can be simulated by also drawing uT+i from the restricted distribution). The
second issue is that the posterior distribution of the parameters will, in general, not be
consistent with the future path we are conditioning on.
Waggoner and Zha (1999) addresses both these issues. Let yT+1:T+H =
 yâ€²
T+1, . . . , yâ€²
T+H
â€²
denote the future values to be forecasted, we can then write the condition that some of
the variables follow a speciï¬c path or takes a speciï¬c value at a give time point as
RyT+1:T+H = r.
To see how this implies a restriction on the future disturbances we use recursive substi-
tution to rewrite the future yT+i in terms of past yt and future uT+j, j = 1, . . . , i.
yT+i = E (yT+i|YT, Î“, Î¨) +
iâˆ’1
X
j=0
Bâ€²
juT+iâˆ’j
(67)
where Bi are the parameter matrices in the MA-representation,
B0 = I
Bi =
q
X
m=1
AmBiâˆ’m, i > 0
and yT+i = E (yT+i|YT, Î“, Î¨) can be obtained trough the recursion (66) with buT+i = 0.
Stacking the equations (67) we obtain yT+1:T+H = yT+1:T+H + Bâ€²uT+1:T+H for
B =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
B0
B1
Â· Â· Â·
BHâˆ’1
0
B0
Â· Â· Â·
BHâˆ’2
...
...
0
Â· Â· Â·
0
B0
ï£¶
ï£·
ï£·
ï£·
ï£¸.
The restriction can then be written as
RyT+1:T+H = R
 yT+1:T+H + Bâ€²uT+1:T+H

= r
DuT+1:T+H = RBâ€²uT+1:T+H = r âˆ’RyT+1:T+H = d.
Since uT+1:T+H âˆ¼N (0, VH) with VH = IH âŠ—Î¨, normal theory implies that the condi-
tional distribution of uT+1:T+H is
uT+1:T+H|DuT+1:T+H = d âˆ¼N

VHDâ€² (DVHDâ€²)âˆ’1 d, VHâˆ’VHDâ€² (DVHDâ€²)âˆ’1 DVH

(68)
49

which can be used for the simulation of the predictive distribution discussed above. Note,
however, that the variance matrix is singular and some care is needed when generating
uT+1:T+H, see JarociÂ´nski (2010) for an eï¬ƒcient method to generate uT+1:T+H.22
This does not address the issue of the consistency of the posterior distribution of the
parameters and the restriction RyT+1:T+H = r. The restriction is information that in
principle should be incorporated in the prior. This is, however, not possible in practice
due to the highly non-linear relationship between the parameters and yT+1:T+H. Instead
Waggoner and Zha (1999) suggests treating yT+1:T+H as latent variables and simulate the
joint posterior distribution of the parameters and yT+1:T+H subject to the restriction and
gives a straightforward MCMC sampler for this. The sampler is reproduced as Algorithm
11.
In addition to the hard restrictions RyT+1:T+H = r Waggoner and Zha (1999) also
considers â€softâ€ restrictions on the form RyT+1:T+H âˆˆS where S is some subset of RmH in-
dicating an interval or region the forecasts that the forecasts are restricted to. Andersson,
Palmqvist and Waggoner (2010) generalizes the approach of Waggoner and Zha (1999)
to restrictions on the distribution of the future values, e.g.
RyT+1:T+H âˆ¼N (r, Vr) .
Robertson, Tallman and Whiteman (2005) takes a diï¬€erent approach and use exponential
tilting to modify the unrestricted predictive distribution to match moment conditions of
the form E [g (yT+1:T+H)] = g. An example of the use of the exponential tilting method
is Cogley, Morozov and Sargent (2005) who used it to adapt the predictive distribution
to the Bank of England target inï¬‚ation rate and other information that is external to the
estimated VAR.
Forecasting performance
Bloor and Matheson (2011) forecasts New Zealand real
GDP, tradable CPI, non-tradable CPI, 90 interest rate and the trade weighted exchange
rates using a real time data set. The models considered includes univariate AR-models,
a small 5 variable VAR and BVAR, a medium sized 13 variable structural BVAR and a
large 35 variable structural BVAR. The prior speciï¬cation for the VARs is based on the
approach of Banbura, Giannone and Reichlin (2010) (see section 9.2). Overall the VAR
models do better than the univariate forecasting models with the large VAR improving
on the smaller models. Incorporating external information in the form of Reserve Bank
of New Zealand forecasts for variables where current data has not been released or future
trajectories of variables is found to improve the forecast performance of the models.
22Note that the formulation of JarociÂ´nski (like the one of Waggoner and Zha) is in terms of a structural
VAR and generates structural form innovations rather than the reduced form used here. To see how the
method of JarociÂ´nski maps to the results here let Î›âˆ’1 be a factor of Î¨, i.e. Î›âˆ’1Î›âˆ’T = Î¨ where Î› might
come from a structural VAR or is the Cholesky factor of Î¨âˆ’1 (which is generally available as part of
generating Î¨ from the full conditional posterior in a reduced form VAR). The reduced form disturbances
is related to the structural form innovations by uT +1:T +H =
 IH âŠ—Î›âˆ’1
eT +1:T +H and the restriction on
the structural innovations is eReT +1:T +H = d for eR = D
 IH âŠ—Î›âˆ’1
. Since the uconditional distribution
of eT +1:T +H is N (0, ImH) we get eT +1:T +H| eReT +1:T +H = d âˆ¼N

eRâ€² 
eRâ€² eR

d, ImH âˆ’eRâ€² 
eRâ€² eR
âˆ’1 eR

and the method of JarociÂ´nski can be used to generate ï¬rst eT +1:T +H and then uT +1:T +H.
50

Algorithm 11 MCMC sampler for VAR subject to â€hardâ€ restrictions
For a VAR subject to the restrictions RyT+1:T+H = r select starting values Î“(0) and Î¨(0).
The starting values can be taken from a separate simulation run on the historical data.
For i = 1, . . . , B + R
1. Generate u(j)
T+1:T+H from the conditional distribution uT+1:T+H|DuT+1:T+H
=
d, Î“(jâˆ’1), Î¨(jâˆ’1) in (68) and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(jâˆ’1)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(jâˆ’1)
i
+ xâ€²
T+hC(jâˆ’1)+u(j)â€²
T+h
2. Augment the data with ey(j)
T+1:T+H and generate the parameters Î¨(j) and Î“(j) from the
full conditional posteriors Î¨|YT, ey(j)
T+1:T+H, Î“(jâˆ’1) and Î“|YT, ey(j)
T+1:T+H, Î¨(j) using
the relevant steps from one of the samplers discussed in this chapter depending on
the choice of model structure and prior.
Discarding the parameters and keeping yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from
the joint predictive distribution subject to the restrictions.
7
Time-varying parameters and stochastic volatility
7.1
Time-varying parameters
The constant parameter assumption implicit in the formulation of the VAR model (6) is
often, but not always, reasonable. Parameter constancy might fail if the data covers a
long time period, if there are changes in economic policy (e.g. monetary policy) as well
as for many other reasons. It can thus be useful to allow for the parameters to change
over time and we write the VAR as
yt = WtÎ³t + ut
(69)
for Wt = Im âŠ—zâ€²
t.
Doan et al. (1984), Highï¬eld (1987) and Sims (1993) were among the earliest to in-
troduce parameter variation in VAR models. Sims (1993), Doan et al. (1984) retains
the equation by equation estimation strategy of Litterman while allowing the regression
parameters to follow an AR(1) process
Î³t+1 = Ï€8Î³t + (1 âˆ’Ï€8) Î³ + Îµt
(70)
with 0 â‰¤Ï€8 â‰¤1. Doan et al. (1984) shows how the estimation can be conducted using
the Kalman ï¬lter to update the state vector Î³t and conducts a search over a subset of
the hyperparameters to ï¬nd the combinations that provides the best forecast accuracy
in a 10 variable VAR. Highï¬eld (1987) relaxes the assumption of a known diagonal error
variance-covariance matrix and uses the normal-Whishart conjugate prior in a state space
formulation of the model. These are all examples of the type of time-varying parameter
VAR models (TVP-VAR) formulated as state space models that we will focus on. There
51

are, of course, other ways to formulate a model where the parameters are allowed to
change over time. This includes models accommodating structural breaks by including
dummy variables that interact with some or all of the right hand side variables and Markov
switching models with a ï¬xed number of regimes (Chib (1998)) or an evolving number of
regimes (Pesaran, Petenuzzo and Timmermann (2006), Koop and Potter (2007)).
The popularity of the Bayesian approach to TVP-VARs owes much to Cogley and
Sargent (2002, 2005) and Primiceri (2005) who, although not primarily concerned with
forecasting, provides the foundations for Bayesian inference in these models. Koop and
Korobilis (2009) provides a good introduction to TVP-VARs.
The basic TVP-VAR complements the observation equation (69) with the state equa-
tion23
Î³t+1 = Î³t + Îµt.
(71)
That is, the parameters are assumed to follow a random walk and evolve smoothly over
time. Îµt is assumed to be normally distributed, Îµt âˆ¼N (0, Q) and independent of the
error term in the observation equation which is also normal, ut âˆ¼N (0, Î¨) . Note that
the state equation implies that Î³t+1|Î³t, Q âˆ¼N (Î³t, Q) and that this in a sense serves as
prior distribution for Î³t+1 and the prior for all the states (parameters) is simply a product
of normal distributions that needs to be complemented with a prior for the ï¬rst state,
Ï€ (Î³1) , which is then usefully also taken to be normal,
Î³1 âˆ¼N
 s1|0, P1|0

.
(72)
The prior speciï¬cation is completed with independent inverse Wishart priors for Î¨ and
Q,
Î¨ âˆ¼iW (Î¨, v)
(73)
Q âˆ¼iW
 Q, vQ

.
The time-varying parameter speciï¬cation introduces an additional layer of complica-
tion when forecasting since the parameters can not be assumed to be constant in the
forecast period. This contributes to additional variability in the predictive distribution
and we must simulate Î³T+h from the state equation 71 in order to simulate the predictive
distribution. See Cogley et al. (2005) for a discussion of these issues.
It is straightforward to set up a Gibbs sampler for the joint posterior distribution
of
 Î³T, Î¨, Q

(as a notational convention we will use superscripts to refer to sequences
of variables and parameters, i.e. xt to refer to the sequence x1, . . . , xt). Conditional on
the unobserved time varying parameters (states), Î³t, posterior inference for Î¨ and Q is
standard and we have the full conditional posteriors as
Î¨|yT, Î³T âˆ¼iW
 Î¨, v

, Î¨ = Î¨ +
T
X
i=1
(yt âˆ’WtÎ³t) (yt âˆ’WtÎ³t)â€² , v = v + T
(74)
and
Q|Î³T âˆ¼iW
 Q, v

, Q = Q +
T
X
i=1
(Î³t+1 âˆ’Î³t) (Î³t+1 âˆ’Î³t)â€² , vQ = vQ + T.
(75)
23See Appendix B for a (very) brief introduction to state space models.
52

Algorithm 12 Gibbs sampler for the TVP-VAR
For the TVP-VAR (69), (71) and the priors (72), 73 select starting values Î¨(0) and Q(0).
For j = 1, . . . , B + R
1. Draw Î³(j)
T
from the full conditional posterior Î³T|yT, Î¨(jâˆ’1), Q(jâˆ’1) âˆ¼N
 sT|T, PT|T

obtained from the Kalman ï¬lter (equation (126) in Appendix B). For t = T âˆ’1, . . . , 1
draw Î³(j)
t
from the full conditional Î³t|yT, Î¨(jâˆ’1), Q(jâˆ’1),Î³(j)
t+1 in (76) by running the
simulation smoother (Algorithm B.3 in Appendix B).
2. Draw Q(j) from the full conditional Q|Î³T(j) in (75).
3. Draw Î¨(j) from the full conditional Î¨|yT, Î³T(j) in (74).
4. If j > B
Generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
, for h = 1, . . . , H, generate Î³(j)
T+h
from the state equation (71) and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i,T+h +
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i,T+h + xâ€²
T+hC(j)
T+h+u(j)â€²
T+h.
(77)
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
Generating Î³T from the full conditional posterior is somewhat more involved. The
basic idea is to make use of the linear state space structure.
Given Î¨ and Q a run
of the (forward) Kalman ï¬lter (equation (126) in Appendix B24) produces the sequence
of conditional distributions Î³t|yt, Î¨, Q âˆ¼N
 st|t, Pt|t

, t = 1, . . . , T. This gives the full
conditional posterior for the last state, Î³T|yT, Î¨, Q âˆ¼N
 sT|T, PT|T

from which a draw
of Î³T can be made. To obtain the full conditional posterior for all the states we decompose
the joint distribution as p
 Î³T|yT, Î¨, Q

= p
 Î³T|yT, Î¨, Q
 Tâˆ’1
Y
t=1
p
 Î³t|yT, Î¨, Q,Î³t+1

where
Î³t|yT, Î¨, Q,Î³t+1 âˆ¼N
 st|T, Pt|T

(76)
and the moments are obtained from the backwards recursions in the simulation smoother
(Algorithm B.3 in Appendix B). Algorithm 12 summarizes the Gibbs sampler for the
TVP-VAR.
Specifying the prior
The random walk nature of the state equation (71) puts little
structure on the behavior of Î³t and the implied prior for the sequence of parameters,
Î³T, gets increasingly loose as the unconditional variance increases at the rate tQ. To
enforce some smoothness in Î³t it is useful to focus the prior for Q on small values of the
variances. In addition, the random walk nature can lead to explosive behavior at some
24Set Zt = Wt, Ht = Î¨, dt = 0, Tt = I and Qt = Q in the Kalman ï¬lter equations.
53

time points which can be undesirable if the data is believed to be stationary. To prevent
this Cogley and Sargent (2002, 2005) truncates the prior for Î³T to the region where Î³t,
âˆ€t, is stationary. Truncated prior distributions like this are easily incorporated in a Gibbs
sampler, simply check the stationarity condition for all t at the end of step 1 of Algorithm
12 and redo step 1 if it fails for at least one time point.
It is common to use a training sample prior for the ï¬rst state, Î³1. The ï¬rst, say k +20,
observations are set aside as a training sample and the prior mean and variance are based
on the OLS estimates using the training sample. The prior variance should, in general,
be relatively large so as not to make the prior too informative, see, for example Primiceri
(2005).
An alternative is to base the prior for Î³1 on the Minnesota prior.
For this to be
eï¬€ective a modiï¬ed state equation along the line of the Doan et al. (1984) speciï¬cation
(70) is useful. Generalizing this we can write the state equation as
Î³t+1 = s1|0 + Î¦
 Î³t âˆ’s1|0

+ Îµt
(78)
with Î¦ a diagonal matrix. The state equation is stationary and mean reverting if |Ï†ii| < 1.
The diagonal elements can be taken as ï¬xed and speciï¬ed along with the other prior
parameters or estimated. Inference on Ï†ii is straightforward in the latter case. Conditional
on Î³T(78) is just a multivariate regression and it is easy to add a block to the Gibbs
sampler drawing from the full conditional posterior for Î¦.
Forecasting performance
Sims (1993) reports on the enhancements made to the orig-
inal Litterman forecasting model where he allows for conditional heteroskedasticity and
non-normal errors in addition to the time varying regression parameters. The result of
these modiï¬cations and the addition of three more variables, the trade weighted value of
the dollar, the S&P 500 stock index and the commodity price index, led to an improved
forecasting performance for the price variable, comparable or slightly better forecasts for
the real variables and slightly worse forecasts for interest rates compared to the original
Litterman model.
Canova (2007) forecasts the inï¬‚ation rate of the G7 countries using a range of mod-
els. First there is a set of country speciï¬c models, univariate ARMA, several bivariate
VARs with the additional variable suggested by theory, trivariate VARs where the two
additional variables are selected to minimize the in sample mean square error for inï¬‚a-
tion. The trivariate VAR is estimated by OLS, as a BVAR with Minnesota style prior
beliefs and also as a Bayesian TVP-VAR and a Bayesian TVP-AR with mean revert-
ing state equations (70). Canova also use several â€internationalâ€ models, three variables
controlling for international demand are added as predetermined variables to the coun-
try speciï¬c BVARs, a TVP-BVAR for the 7 inï¬‚ation rates with the same international
variables as predetermined, a Bayesian panel VAR and a dynamic factor model for the
inï¬‚ation rates where the factors are principal components of the variables in the panel
VAR. All the models are formulated in terms of direct rather than iterated forecasts, i.e.
yt+h = Ï†yt + ut+h for an AR(1). The models diï¬€er in two important dimensions, the rich-
ness of the information set and the ï¬‚exibility of the speciï¬cation. Overall the model with
the largest information set and the most general speciï¬cation, the Bayesian panel VAR
does best. Comparing models with similar information sets, a BVAR improves on a VAR
54

estimated with OLS and time-varying parameters improve the forecasts for univariate
models but not for the BVARs.
Clark and McCracken (2010) use a real time data set and forecasts the US inï¬‚ation,
interest rate and output using a wide range of trivariate VAR models based on diï¬€erent
approaches to allowing for structural change. This includes models in levels, in diï¬€erences,
estimated on rolling windows of the data, estimated by OLS or as BVARs with Minnesota
type priors and TVP-BVARs with random walk state equations. While the focus is on
diï¬€erent methods for combining forecasts and how well they cope with structural changes
Clark and McCracken (2010) do report some results for individual models. Of these a
BVAR with detrended inï¬‚ation does best and, while not directly comparable, considerably
better than a TVP-BVAR where inï¬‚ation has not been detrended.
7.2
Stochastic volatility
The constant error variance assumption can also be questioned, especially in light of the
so called â€great moderationâ€ with considerable lower variability in key macroeconomic
variables since the mid 1980s. It can also be diï¬ƒcult to empirically distinguish between
a model with constant parameters and time-varying variances and a model with time-
varying parameters and constant error variance.
It can thus be prudent to allow for
both. Cogley and Sargent (2005) and Primiceri (2005) construct TVPSV-VAR models
by adding stochastic volatility to the TVP-VAR. The setup in Primiceri (2005) is more
general and the overview here is based on Primiceri. See Koop and Korobilis (2009) for
a more in-depth discussion of stochastic volatility in VAR models.
To introduce time-varying volatilities and correlations we decompose the error variance
matrix into
Î¨t = Lâˆ’1
t DtLâˆ’T
t
where Dt is a diagonal matrix, Dt = diag (d1t, . . . , dmt) with a stochastic volatility speci-
ï¬cation,
dit = exp (hit/2) ,
(79)
hi,t+1 = Âµi + Ï†i (hit âˆ’Âµi) + Î·it
where Î·t = (Î·1t, . . . , Î·mt) is iid N (0, VÎ·). The purpose of Lt is to allow for an arbitrary
time-varying correlation structure. It is a lower triangular matrix with 1 on the diagonal,
e.g. for m = 3
Lt =
ï£«
ï£­
1
0
0
l21,t
1
0
l31,t
l32,t
1
ï£¶
ï£¸,
(80)
where the time-varying elements under the diagonal follow a random walk,
lt+1 = lt + Î¶t
(81)
for lt a m (m âˆ’1) /2 vector that collects lij,t, i > j in row major order, Î¶t iid N
 0, VÎ¶

and VÎ¶ block diagonal with blocks corresponding to the rows of Lt.
The triangular
speciï¬cation (80) is convenient and can also be interpreted as a structural VAR with
time-varying parameters.
55

Prior speciï¬cation
Prior distributions for the parameters Âµi, Ï†i, VÎ· and VÎ¶ are needed
in order to complete the model. Âµi is the unconditional expectation of the log volatilities
and in absence of speciï¬c information about the scale of the parameters a noninforma-
tive normal prior can be used, Âµ âˆ¼N
 0, Î£Âµ

with Î£Âµ diagonal. For the autoregressive
parameter it is common to restrict this to the stationary region and specify a trun-
cated normal, Ï† âˆ¼N
 Ï†, Î£Ï†

I (|Ï†i| < 1) with Î£Ï† diagonal. Alternatively one can, as
in Primiceri (2005), work with a random walk speciï¬cation for hit with Ï†i = 1 and
where Âµi drops out of the model. For the state equation variance VÎ· an inverse Wishart
prior, VÎ· âˆ¼iW
 SÎ·, vÎ·

, is conditionally conjugate and convenient. For the log volatil-
ities an initial condition (prior) is needed. With Ï†i restricted to the stationary region,
hi1|Âµi, Ï†i, Ïƒ2
Î·i âˆ¼N
 Âµi, Ïƒ2
Î·i/ (1 âˆ’Ï†2
i )

for Ïƒ2
Î·i the ith diagonal element of VÎ·, is a natural
choice, and with Ï†i = 1 a noninformative normal distribution can be used for the initial
condition.
VÎ¶ is assumed to be block diagonal in order to simplify the posterior sampler (Primiceri
(2005) shows how to relax this). With a block diagonal structure the prior for the blocks
VÎ¶,i, i = 2, . . . , m, can be speciï¬ed with independent inverse Wishart distributions, VÎ¶,i âˆ¼
iW
 SÎ¶,i, vÎ¶,i

. In addition, an initial condition is needed for the elements of L1 collected
in l1. For simplicity, this can be taken as a noninformative normal distribution. For some
additional simpliï¬cation, VÎ· and VÎ¶ can be speciï¬ed as diagonal matrices with inverse
Gamma priors for the diagonal elements.
The exact choices for the parameters of the prior distribution can be based on a
training sample as for the TVP-VAR model, see Primiceri (2005) for an example.
Sampling from the posterior
When discussing inference on the variance parameters
Lt, VÎ¶, Dt, Âµi, Ï†i and VÎ· we condition on the other parameters in the model and simply
take eyt = yt âˆ’WtÎ³t as our data. This implies that the (conditional) inference procedure
for the variance parameters does not depend on if the other parameters are time-varying
or constant. It will consist of a few blocks of a Gibbs sampler that can be combined with
a MCMC sampler for a VAR with constant or time-varying parameters. The inference
procedure for the remaining parameters is, on the other, aï¬€ected by the introduction
of time-varying variances. For the TVP-VAR this amounts to noting that Î¨t (Ht in
the Kalman ï¬lter equations (126) in Appendix B) is now time-varying. The constant
parameter case can also be handled with the help of the Kalman ï¬lter by setting Qt= 0,
dt = 0 and Tt = I in addition to allowing for time-varying variances in the Kalman
ï¬lter. By setting Qt to zero the parameter variation is shut down and Î³t = Î³1, âˆ€t, is
enforced. The prior (72) for Î³1 is then a prior for the constant parameter. After running
the Kalman ï¬lter the (conditional) posterior mean and variance of Î³ is returned as sT|T
and PT|T and no smoothing is necessary. The conditional posterior for a constant Î³ can
of course also be derived analytically. Write the constant parameter VAR as
yt = WtÎ³ + ut
56

with ut âˆ¼N (0, Î¨t) and Wt = Im âŠ—zâ€²
t. With an independent normal prior Î³ âˆ¼N
 Î³, Î£Î³

as in section 3.2.2 the conditional posterior is
Î³|YT, Î¨T âˆ¼N
 Î³, Î£Î³

(82)
Î£Î³ =
 
Î£âˆ’1
Î³
+
T
X
i=1
Wâ€²
tÎ¨âˆ’1
T Wt
!âˆ’1
,
Î³ = Î£Î³
"
Î£âˆ’1
Î³ Î³ +
 T
X
i=1
Wâ€²
tÎ¨âˆ’1
T Wt
!
bÎ³
#
where bÎ³ is the GLS estimate bÎ³ =
PT
i=1 Wâ€²
tÎ¨âˆ’1
T Wt
âˆ’1 PT
i=1 Wâ€²
tÎ¨âˆ’1
T yt.
Turning to the conditional posteriors for the variance parameters, the one for the
correlation parameters in Lt is relatively straightforward and replicates the treatment
of time-varying parameters in the TVP-VAR. Multiplying each observation with Lt we
obtain Lteyt = Ltut = et with V (et) = Dt. This yields m âˆ’1 uncorrelated equations in a
triangular equation system,
eyit = âˆ’
iâˆ’1
X
j=1
eyjtlij,t + eit, i = 2, . . . , m.
(83)
This, together with the assumption that VÎ¶ is block diagonal, means that the full condi-
tional posterior for Lt, t = 1, . . . , T can be recovered by running the corresponding Kalman
ï¬lter and simulation smoother for each equation in turn, setting Zt = (ey1t, . . . , eyiâˆ’1,t) ,
Ht = exp (hit) , dt = 0, Tt = I and Qt to the relevant block of VÎ¶ in the Kalman ï¬lter
equations.
The posterior for VÎ¶ is straightforward conditional on Lt. With the block diagonal
structure the blocks are inverse Wishart
VÎ¶,i|lT âˆ¼iW
 SÎ¶,i, vÎ¶,i

, i = 2, . . . , m
(84)
SÎ¶,i = SÎ¶,i +
T
X
t=1
(li,t âˆ’li,tâˆ’1) (li,t âˆ’li,tâˆ’1)â€² , vÎ¶,i = vÎ¶,i + T
for li,t = (li1,t, . . . , li,iâˆ’1,t)â€² .
The posterior analysis of the time-varying volatilities is complicated by the fact that
the observation equation is non-linear in the states hi,t.Let yâˆ—
t = Lteyt, we then have
yâˆ—
it = exp (hit/2) vit
where vit is iid N (0, 1) . Squaring and then taking logarithms yields
yâˆ—âˆ—
it = hit + vâˆ—
it
where yâˆ—âˆ—
it
= ln

(yâˆ—
it)2 + c

for c a small positive constant and with vâˆ—
it = ln v2
it dis-
tributed as the logarithm of a Ï‡2
1 random variable.
Kim, Shephard and Chib (1998)
show that the distribution of vâˆ—
it is well approximated by a mixture of normals, p (vâˆ—
it) â‰ˆ
P7
j=1 qjN
 vâˆ—
it; mj âˆ’1.2704, Ï„ 2
j

. The mixture coeï¬ƒcients obtained by Kim et al. (1998)
57

are reproduced in Table 1.
By introducing a latent indicator variable Î´it for which
component in the mixture vâˆ—
it has been drawn from, the mixture can be rewritten as
vâˆ—
it|Î´it = j âˆ¼N
 mj âˆ’1.2704, Ï„ 2
j

with P (Î´it = j) = qj the problem can thus be trans-
formed into a linear and normal ï¬ltering problem conditional on Î´it. Kim et al. (1998)
develops an MCMC algorithm for sampling from the posterior distribution of the states
hit.
Conditional on the sequence of states, Î´1, . . . , Î´T, we can, using the notation of Ap-
pendix B, write the stochastic volatility part of the model as a linear state space system,
yâˆ—âˆ—
t = Zt
 1
ht

+ eâˆ—
t
(85)
ht+1 = d + Tht + Î·,
for Zt = (mt, Im) , di = Âµi (1 âˆ’Ï†i) , T = diag (Ï†i) . The elements of mt are the conditional
means E (vâˆ—
it|Î´it = j) = mj âˆ’1.2704 and eâˆ—
t âˆ¼N (0, Ht) for Ht diagonal with diagonal
elements given by the conditional variances, V (vâˆ—
it|Î´it = j) = Ï„ 2
j . Running the Kalman
ï¬lter and then the simulation smoother on (85) yields a draw of hit, i = 1, . . . , m, t =
1, . . . , T from the full conditional posterior.
The full conditional posterior for VÎ· is straightforward as an inverse Wishart,
VÎ·|hT, Âµ, Ï† âˆ¼iW
 SÎ·, vÎ·

, SÎ· = SÎ·+
T
X
t=1
(ht âˆ’d âˆ’Thtâˆ’1) (ht âˆ’d âˆ’Thtâˆ’1)â€² , vÎ· = vÎ·+T.
(86)
The states, Î´it, can be sampled from the full conditional posterior
p (Î´it = j|yâˆ—âˆ—
it , hit) âˆqj
1
Ï„j
exp
  yâˆ—âˆ—
i,t âˆ’mj âˆ’1.2704 âˆ’hit
2
2Ï„ 2
j
!
.
(87)
For Ï†i and Âµi, ï¬nally, write
hâˆ—
t = ht âˆ’Âµ = XtÏ† + Î·
for Xt = diag (hi,tâˆ’1 âˆ’Âµi) . Stacking the observations and performing the usual calcula-
tions yields the full conditional posterior for Ï† as
Ï†|hT, Âµ, VÎ· âˆ¼N
 Ï†, Î£Ï†

I (|Ï†i| < 1)
(88)
Î£Ï† =
 
Î£âˆ’1
Ï† +
T
X
t=1
Xâ€²
tVâˆ’1
Î· Xt
!âˆ’1
Ï† = Î£Ï†
 
Î£âˆ’1
Ï† Ï† +
T
X
t=1
Xâ€²
tVâˆ’1
Î· hâˆ—
t
!
.
The full conditional posterior for Âµ is obtained in a similar fashion as
Âµ|hT, Ï†, VÎ· âˆ¼N
 Âµ, Î£Âµ

(89)
Î£Âµ =
 Î£âˆ’1
Âµ + TXâ€²Vâˆ’1
Î· X
âˆ’1
Âµ = Î£Âµ
 
Î£âˆ’1
Âµ Ï† + Xâ€²Vâˆ’1
Î·
T
X
t=1
hâˆ—âˆ—
t
!
.
58

Table 1 Normal mixture coeï¬ƒcient for ln Ï‡2
1
Component, Î´
qj
mj
Ï„ 2
j
1
0.00730
âˆ’10.12999
5.79596
2
0.10556
âˆ’3.97281
2.61369
3
0.00002
âˆ’8.56686
5.17950
4
0.04395
2.77786
0.16735
5
0.34001
0.61942
0.64009
6
0.24566
1.79518
0.34023
7
0.25750
âˆ’1.08819
1.26261
Source: Kim et al. (1998)
by writing
hâˆ—âˆ—
t = ht âˆ’diag (Ï†) htâˆ’1 = XÂµ + Î·
for X = diag (1 âˆ’Ï†i) .
The steps of the resulting Gibbs sampler are summarized in algorithm 13.
Forecast performance
Clark (2011) studies point and density forecasts using real time
data on the US output growth, unemployment rate, inï¬‚ation and federal funds rate. With
a focus on the eï¬€ects of changing data variability motivated by the possible end of the
great moderation Clark introduces a new model, the steady state BVAR of Villani (2009)
combined with stochastic volatility following the approach of Cogley and Sargent (2005).
The forecast performance of the new model is compared to univariate AR-models with
and without stochastic volatility, a standard Minnesota type BVAR with a normal-diï¬€use
prior and a Minnesota type steady state BVAR. The BVAR includes the four variables
to forecast whereas the steady state BVARs includes the detrended unemployment rate,
inï¬‚ation and the interest rate less the long run inï¬‚ation expectation and the long run in-
ï¬‚ations expectation as an additional variable. The three BVAR variants are estimated on
both a recursively updated and rolling data window. When the point forecasts are evalu-
ated by their RMSEs, the BVARs generally do worse than the benchmark univariate AR
without stochastic volatilities at shorter lead times (1 and 2 quarters) but improve on the
benchmark for lead times of 1 and 2 years with the exception for the forecasts of inï¬‚ation.
The steady state BVAR tend to do better than the standard BVAR and adding stochastic
volatility brings further improvements. In addition, the rolling updating scheme with an
80 observation window tends to produce better forecasts than recursive updating. The
density forecasts are evaluated using several criteria. In terms of the empirical coverage of
the prediction intervals it is found that the models without stochastic volatility produces
to wide intervals while the models with stochastic volatility produces better calibrated
prediction intervals. When evaluated using the probability integral transform (PIT, see
Corradi and Swanson (2006)) the hypothesis of a correctly speciï¬ed predictive distribu-
tion is rejected in almost all cases for models without stochastic volatility whereas the
stochastic volatility models pass the test with only a few exceptions. Finally, evaluating
the density forecasts using the log predictive density score, the stochastic volatility models
do considerably better at the shorter lead times while the diï¬€erences are quite small for
the longer lead times. Similarly the steady state BVAR outperforms the standard BVAR
at short lead times and the steady state BVAR with stochastic volatility outperforms the
59

Algorithm 13 Gibbs sampler for VAR with stochastic volatility
For the VAR with stochastic volatility select starting values Î¨T(0) = Î¨(0)
1 , . . . , Î¨(0)
T , V(0)
Î¶,i ,
i = 2, . . . , m, Âµ(0), Ï†(0), V(0)
Î·
and Î´T(0).
For j = 1, . . . , B + R
1. Draw the regression Î³(j) from the full conditional posterior Î³|YT, Î¨T(jâˆ’1) in (82)
for the constant parameter case or using the Kalman ï¬lter and simulation smoother
for the time-varying parameter case as in Algorithm 12.
2. For i = 2, . . . , m run the Kalman ï¬lter for the observation equation (83) and
state equation 81) and generate l(j)
i,T from the normal full conditional posterior
li,T|YT, V(jâˆ’1)
Î¶,i
with parameters given by sT|T and PT|T from the Kalman ï¬l-
ter.
For k = T âˆ’1, . . . , 1 draw l(j)
i,t from the normal full conditional posterior
l(j)
i,t |yt, V(jâˆ’1)
Î¶,i
, l(j)
i,t+1 obtained from the simulation smoother.
3. For i = 2, . . . , m, draw V(j)
Î¶,i from the full conditional posterior VÎ¶,i|lT(j) in (84).
4. Draw
the
log
volatilities
h(j)
T
from
the
normal
full
conditional
posterior
hT|YT, Âµ(jâˆ’1), Ï†(jâˆ’1), V(jâˆ’1)
Î·
, Î´T(jâˆ’1) with parameters sT|T and PT|T obtained by run-
ning the Kalman ï¬lter for the state space system (85). For t = T âˆ’1, . . . , 1 draw h(j)
t
from the normal full conditional posterior hT|yt, Âµ(jâˆ’1), Ï†(jâˆ’1), V(jâˆ’1)
Î·
, Î´T(jâˆ’1), h(j)
t+1
with parameters obtained from the simulation smoother.
5. For i = 1, . . . , m, t = 1, . . . , T draw the states Î´(j)
it from the full conditional posterior
Î´ij|yâˆ—âˆ—
it , hit in (87).
6. Draw V(j)
Î·
from the full conditional posterior VÎ·|hT(j), Âµ(jâˆ’1), Ï†(jâˆ’1) in (86).
7. Draw Ï†(j) from the full conditional posterior Ï†|hT(j), V(j)
Î· , Âµ(jâˆ’1) in (88).
8. Draw Âµ(j) from the full conditional posterior Âµ|hT(j), V(j)
Î· , Ï†(j) in (89).
9. If j > B
For h = 1, . . . , H, generate Î³(j)
T+h from the state equation if Î³ is time varying,
generate l(j)
i,T+h from (81) and h(j)
T+h from (79), form Î¨(j)
T+h and generate u(j)
T+h from
uT+h âˆ¼N

0, Î¨(j)
T+h

and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i,T+h +
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i,T+h + xâ€²
T+hC(j)
T+h+u(j)â€²
T+h.
(90)
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oR
j=1 as a sample of independent
draws from the joint predictive distribution.
60

steady state BVAR at shorter lead times.
Using a real time data set Dâ€™Agostino, Gambetti and Giannone (forthcoming) fore-
casts the US unemployment rate, inï¬‚ation and a short term interest rate. The aim of the
forecasting exercise is to investigate how important it is to allow for time-varying param-
eters and stochastic volatility. Forecasts are thus made with a number of models which
incorporate these futures to a varying degree: A univariate TVPSV-AR model, SV-AR
and SV-VAR models, standard AR and VAR models estimated using recursive and rolling
data windows and a TVPSV-VAR using the speciï¬cation of Primiceri (2005). The infer-
ence is Bayesian for all the models and the prior beliefs are based on the Minnesota prior.
Overall the TVPSV-VAR does best both in terms of point forecasts and density fore-
casts. The SV-AR and SV-VAR models improve on their constant variance counterparts
and Dâ€™Agostino et al. (forthcoming) concludes that there is a role for both time-varying
parameters and time-varying error variances when forecasting these variables.
8
Model and variable selection
Model speciï¬cation in VAR models essentially consists of two questions: Which variables
should be modelled (included in yt) and, given the content of yt, how many lags of yt
should be included? The answer to the ï¬rst question obviously depends on the objective
of the exercise which mandates the inclusion of some variables (e.g. the variables to be
forecasted) but there is usually also a secondary set of variables where the choice is not so
clear cut. These are variables that are not of primary interest but could be included in the
model if it leads to a better speciï¬ed model that improves the forecasts of the variables
of interest or clearer inference by avoiding omitted variable bias.
The Litterman prior tries to answer the second question in its general form, how many
lags, by making the prior tighter around zero for coeï¬ƒcients on larger lags and thus allow-
ing for a comfortably large lag length while reducing the risk of overï¬tting. The question
can, however, be made more speciï¬c: Which lags of which variables should be included
in each equation? This opens up for a huge number of diï¬€erent model speciï¬cations and
requires new tools.
8.1
Restricting the parameter matrices - SSVS
George, Sun and Ni (2008) considers soft and hard restrictions on the parameters of
the VAR as a means of reducing the eï¬€ects of overparameterization and sharpening the
inference. This has some similarity with the structural VAR models discussed in section
4 but can also bee seen as a matter of selecting which lags of which dependent variable to
include in the model. In contrast with the SVAR approach the restrictions are determined
by the data rather than economic theory and are applied to a mixture of reduced form
and structural form parameters. Taking the reduced form VAR
yâ€²
t =
p
X
i=1
yâ€²
tâˆ’iAi + xâ€²
tC + uâ€²
t = zâ€²
tÎ“ + uâ€²
t
as the starting point George et al. (2008) considers restrictions on Î“ and the Cholesky
factor Î› of the inverse variance matrix of ut, Î¨âˆ’1 = Î›Î›â€². The Î›-matrix plays the
61

same role as in the SVAR and restrictions on Î› can, in contrast to restrictions on Î“, be
given a structural interpretation. Allowing for zero or â€near-zeroâ€ restrictions on arbitrary
parameters there is vast number of combinations of restrictions or models, 2mk+m(mâˆ’1)/2, to
consider. It is clearly impossible to evaluate all of them and George et al. (2008) proposes
a stochastic search variable selection (SSVS) procedure that will focus on the restrictions
with empirical support. The SSVS (George and McCulloch (1993)) is a MCMC algorithm
for simulating from the joint posterior distribution of the set of restrictions (or models) and
parameters based on a speciï¬c form of hierarchical prior distribution. For the regression
coeï¬ƒcients Î³ij let Î´ij be an indicator variable, the prior conditional on Î´ij is then Î³ij âˆ¼
N

Î³ijÎ´ij, h2
ij

for
hij =
 Ï„0,ij if Î´ij = 0
Ï„1,ij if Î´ij = 1
(91)
with Ï„0,ij â‰ªÏ„1,ij. The idea being that the prior shrinks aggressively towards zero if Î´ij = 0
(or imposes Î³ij = 0 if Ï„ 2
0,ij = 0) and allows for a non-zero Î³ij if Î´ij = 1 by setting Ï„1,ij
relatively large. The hierarchical prior is completed by specifying independent Bernoulli
priors for Î´ij, P (Î´ij = 1) = pij, where pij reï¬‚ects the strength of the prior belief that Î³ij
diï¬€ers from zero in a meaningful way. Note that a parameter/variable can be forced into
the model by setting pij to 1. For convenience (and to allow for prior correlation) we
write the prior as a multivariate normal distribution for Î³ = vec (Î“) ,
Î³|Î´ âˆ¼N
 DÎ³, HRH

,
(92)
where D = diag (Î´11, Î´21, . . . , Î´km) , H = diag (h11, h21, . . . , hkm) and R is a known corre-
lation matrix. With prior independence between Î“, Î´ and Î¨ (or Î›) the full conditional
posterior for Î³ is standard and has the same form as with the independent normal-Wishart
prior,
Î³|YT, Î›,Î´ âˆ¼N
 Î³, Î£Î³

(93)
Î£Î³ =

(HRH)âˆ’1 + Î›Î›â€² âŠ—Zâ€²Z
âˆ’1
Î³ = Î£Î³

(HRH)âˆ’1 DÎ³ + vec (Zâ€²YÎ›Î›â€²)

.
George et al. (2008) gives the full conditional posterior for Î´ij as a Bernoulli distribution,
P (Î´ij = 1|YT, Î“, Î›,Î´âˆ’ij) =
u1,ij
u1,ij + u0,ij
(94)
u1,ij = Ï€ (Î“|Î´âˆ’ij, Î´ij = 1) pij
u0,ij = Ï€ (Î“|Î´âˆ’ij, Î´ij = 0) (1 âˆ’pij)
where Ï€ (Î“|Â·) is the prior distribution (92).
The simple form follows since, with the
hierarchical prior structure, Î´ij is independent of the data once we condition on Î“ (see
George and McCulloch (1993)). If, as is frequently the case, the prior for Î³ is speciï¬ed
with no correlation between the elements, R = I, the expressions simplify further and we
62

have
u1,ij =
1
Ï„1,ij
exp
ï£«
ï£¬
ï£­âˆ’

Î³ij âˆ’Î³ij
2
2Ï„ 2
1,ij
ï£¶
ï£·
ï£¸pij
u0,ij =
1
Ï„0,ij
exp

âˆ’Î³2
ij
2Ï„ 2
0,ij

(1 âˆ’pij) .
To facilitate similar selection among the oï¬€-diagonal elements of Î›, collect these in
vectors Î·j = (Î»1j, . . . , Î»jâˆ’1,j)â€² , j = 2, . . . , m, (George et al. (2008) works with Î› upper tri-
angular) and let Ï‰j = (Ï‰1j, . . . , Ï‰jâˆ’1,j)â€² be the corresponding indicators. The conditional
prior for Î·j is then speciï¬ed in the same fashion as the prior for Î³,
Î·j|Ï‰j âˆ¼N (0, GjRjGj)
(95)
with Gj = diag (g1j, . . . , gjâˆ’1,j) , for
gij =
 Îº0,ij if Ï‰ij = 0
Îº1,ij if Ï‰ij = 1 ,
with Îº0,ij â‰ªÎº1,ij, and Rj a known correlation matrix. As for Î´ij, the prior for Ï‰ij is
speciï¬ed as independent Bernoulli distributions with P (Ï‰ij = 1) = qij. For the diagonal
elements of Î›, Î» = (Î»11, . . . , Î»mm)â€² , George et al. (2008) speciï¬es independent Gamma
distributions for the square of the diagonal as the prior,
Î»2
ii âˆ¼G (ai, bi) .
(96)
Note that an inverse-Wishart prior for Î¨ can be obtained as a special case when there is
no selection of zero elements in Î›,i.e. qij = 1 âˆ€i, j, see Algorithm 20 for details.
In order to derive the full conditional posteriors George et al. (2008) rewrites the
reduced form likelihood (7) as
L (Y|Î“, Î›) âˆ|det Î›|T exp

âˆ’1
2 tr [Î›â€²SÎ›]

=
m
Y
i=1
Î»T
ii exp
(
âˆ’1
2
" m
X
i=1
Î»2
iivi +
m
X
j=2
 Î·j + Î»jjSâˆ’1
jâˆ’1sj
â€² Sjâˆ’1
 Î·j + Î»jjSâˆ’1
jâˆ’1sj

#)
where S = (Y âˆ’ZÎ“)â€² (Y âˆ’ZÎ“) , Sj the upper left j Ã— j submatrix of S, sj =
(s1j, . . . , sjâˆ’1,j)â€² , v1 = s11 and vj = |Sj| / |Sjâˆ’1| = sjj âˆ’sâ€²
jSâˆ’1
jâˆ’1sj for j = 2, . . . , m. It
is then easy to show that the conditional posteriors for Î·j are independent and normal,
Î·j|YT, Î“, Ï‰, Î» âˆ¼N
 Î·j, Î£j

(97)
with
Î£j =

(GjRjGj)âˆ’1 + Sjâˆ’1
âˆ’1
Î·j = âˆ’Î£jÎ»jjsj,
63

and that the conditional posteriors for Î»2
jj are independent Gamma distributions,
Î»2
jj|YT, Î“, Ï‰ âˆ¼G
 aj, bj

(98)
aj = aj + T/2
bj =
(
b1 + s11/2, j = 1
bj +

sjj âˆ’sâ€²
jÎ£
âˆ’1
j sj

/2, j = 2, . . . , m .
The full conditional posteriors for Ï‰ij, ï¬nally, are Bernoulli distributions,
P (Ï‰ij = 1|YT, Î“, Î›,Ï‰âˆ’ij) =
v1,ij
v1,ij + v0,ij
(99)
v1,ij = Ï€ (Î·j|Ï‰âˆ’ij, Ï‰ij = 1) qij
v0,ij = Ï€ (Î·j|Ï‰âˆ’ij, Ï‰ij = 0) (1 âˆ’qij)
where Ï€ (Î·j|Â·) is the prior distribution (95). If the elements of Î·j are uncorrelated a prior
(Rj = I) the expressions simplify
v1,ij =
1
Îº1,ij
exp

âˆ’Î·2
ij
2Îº2
1,ij

qij
v0,ij =
1
Îº0,ij
exp

âˆ’Î·2
ij
2Îº2
0,ij

(1 âˆ’qij) .
Specifying the prior beliefs
The prior â€inclusion probabilitiesâ€ determines the prior
expected model size (number of non-zero parameters) and inï¬‚uences how aggressively
the restrictions are applied. Setting pij = qij = 1/2 is a reasonable starting point but a
smaller value can be useful with large and richly parameterized models. There are usually
some parameters, such as the constant term, that should always be in the model. This
is achieved by setting the corresponding prior inclusion probability to 1. We might also
have substantive information about how likely it is that a parameter will contribute to
model ï¬t and forecast performance. In VAR models it could, for example, be useful to let
the inclusion probability pij decrease with the lag length in the spirit of the Minnesota
prior. The choice of inclusion probabilities for the variance parameters could be guided
by the same type of considerations that leads to restrictions in structural VAR models.
The prior variances Ï„0,ij and Îº0,ij should be suï¬ƒciently small to eï¬€ectively shrink
the parameter to zero when Î´ij or Ï‰ij are zero.
The choice of Ï„1,ij and Îº1,ij is more
diï¬ƒcult. George et al. (2008) suggests a semiautomatic choice with Ï„0,ij = bÏƒÎ³ij/10 and
Ï„1,ij = 10bÏƒÎ³ijwhere bÏƒÎ³ij is the standard error of the OLS estimate of Î³ij in the unrestricted
model. Alternatively Ï„1,ij can be based on the Minnesota prior and set as in (14).
The correlation matrices R and Rj, j = 2, . . . , m, are usefully set to the identity
matrix unless there is substantial prior information about the correlation structure. It
is also standard practice in SSVS applications to set the prior means Î³ij to zero when
Î´ij = 1 in addition to when Î´ij = 0. With VAR models it can be useful to deviate from
this and set the prior mean for the ï¬rst own lag to a non-zero value in the spirit of the
Minnesota prior.
If no restriction search is wanted for the regression parameters the prior for Î“ reduces
to the independent normal prior in section 3.2.2. The prior for Î· and Î» can be overly
64

complicated if no restriction search is to be conducted on Î· and these priors can usefully
be replaced by a Jeï¬€reysâ€™ prior or an inverse Wishart prior on Î¨ as in section 3.2.2.
Simulating from the posterior
With the conditional posterior distributions in hand
it is straightforward to implement a Gibbs sampler (see Algorithm 14) for the joint poste-
rior distribution and the predictive distributions needed in forecasting applications. This
will eï¬€ectively conduct model averaging over the diï¬€erent models implied by the restric-
tions and produces the model averaged posterior distribution. If the variable/restriction
selection is of interest the indicator variables Î´ij and Ï‰ij will provide evidence on this.
The posterior probability that a parameter is non-zero can be estimated by averaging Î´ij
and Ï‰ij (or, for a more precise estimate, average the posterior probabilities P (Î´ij = 1)
and P (Ï‰ij = 1) in (94) and (99)) over the output of the sampler.
Note that the sampler in Algorithm 14 will not converge to the joint posterior if hard
restrictions (Ï„0,ij = 0 or Îº0,ij = 0) are used and will converge very slowly if the ratios
Ï„1,ij/Ï„0,ij or Îº1,ij/Îº0,ij are very large. The MCMC algorithm suggested by Geweke (1996b)
is a better choice in these cases. Korobilis (forthcomingb) suggests a convenient algorithm
for the case with hard restrictions on the regression parameters and no restriction search
on the variance parameters.
Forecast performance
Korobilis (2008) applies the SSVS in a forecasting exercise
where the base model is a VAR with eight US macroeconomic variables which is augmented
with an additional 124 exogenous variables that are entered into the model in the form of
their principal components. He ï¬nds that the SSVS model averaged predictions, as well
as the predictions from the â€median modelâ€ (i.e. the model containing the variables with
posterior inclusion probabilities greater than 0.5, Barbieri and Berger (2004)), improves
on the forecasts from OLS estimated VARs without the additional variables and model
selection using BIC, the Bayesian information criteria of Schwarz (1978).
Jochmann, Koop and Strachan (2010) extends the SSVS restriction search to VAR
models with Markov switching to allow for structural breaks and conducts a forecasting
exercise comparing models allowing for diï¬€erent combinations of restriction searches and
breaks in the regression and variance parameters. Using a 4 lag VAR with US unemploy-
ment, interest rate and inï¬‚ation they ï¬nd that the restriction search which eï¬€ectively sets
a large number of parameters to zero results in improved forecasts compared to BVARs
with a â€looseâ€ prior (obtained by forcing Î´ij = 1 and Ï‰ij = 1 for all parameters in the
SSVS prior) and a Minnesota prior. Allowing for structural breaks also improves on per-
formance in combination with SSVS if only a subset (either Î“ or Î›) of the parameters
are allowed to change.
Korobilis (forthcomingb) considers SSVS in a richer class of multivariate time series
models than just linear VAR models but limits the restriction search to the conditional
mean parameters Î“ and consider hard rather than soft restrictions, corresponding to
Ï„0,ij = 0 in (91). In a forecasting exercise where the aim is to forecast UK unemployment,
interest rate and inï¬‚ation Korobilis uses a range of models, allowing for structural breaks
or time varying parameters, and prior speciï¬cations. The general conclusion is that the
restriction search does improve forecast performance when the prior is informative, the
model is richly parameterized or quite large.
65

Algorithm 14 Gibbs sampler for stochastic restriction search (SSVS)
For the priors (92), (95), (96) and independent Bernoulli priors on Î´ij and Ï‰ij the fol-
lowing Gibbs sampler (George et al. (2008)) can be used to simulate the joint posterior
distribution of Î“, Î›, Î´ and Ï‰. Select starting values Î³(0), Î´(0), Î·(0) and Ï‰(0).
For j = 1, . . . , B + R
1. Generate
Î»(j)
by
drawing
Î»2
ii,
i
=
1, . . . , m
from
the
full
conditional
Î»2
ii|YT, Î³(jâˆ’1), Ï‰(jâˆ’1) âˆ¼G
 aj, bj

in (98).
2. Generate Î·(j)
i , i = 2, . . . , m from the full conditional Î·i|YT, Î³(jâˆ’1), Ï‰(jâˆ’1), Î»(j) âˆ¼
N
 Î·j, Î£j

in (97)
3. Generate Ï‰(j)
ik , i
=
1, . . . , k âˆ’1, k
=
2, . . . , m from the full conditional
Ï‰ik|Î·(j)
k , Ï‰(j)
1k , . . . Ï‰(j)
iâˆ’1,k, Ï‰(jâˆ’1)
i+1,k, . . . , Ï‰(jâˆ’1)
kâˆ’1,k âˆ¼Ber (v1,ik/ (v1,ik + v0,ik)) in (99).
4. Generate Î³(j) from the full conditional Î³|YT, Î·(j), Î»(j),Î´(jâˆ’1) âˆ¼N
 Î³, Î£Î³

in (93).
5. Generate Î´(j)
il , i = 1, . . . , k, l = 1, . . . , m from the full conditional posterior
Î´il|Î³(j), Î´(j)
11 , . . . Î´(j)
iâˆ’1,l, Î´(jâˆ’1)
i+1,l , . . . , Î´(jâˆ’1)
mk
âˆ¼Ber (u1,il/ (u1,il + u0,il)) in (94)
6. If j > B form Î¨(j) from Î·(j) and Î»(j), generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼
N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
If there is no restriction search on the elements of Î·, the priors (95) and (96) can be
replaced by a Jeï¬€reysâ€™ prior (23) for Î¨ or an inverse Wishart, Î¨ âˆ¼iW (S, v) . Steps 1-3 can
then be replaced by a draw from the full conditional posterior Î¨|YT, Î“(jâˆ’1) âˆ¼iW
 S, v

with parameters given in (25) or (26).
If there is no restriction search on the elements of Î“, the prior (92) reduces to an inde-
pendent normal prior that does not depend on Î´ and step 5 can be omitted.
66

8.2
Selecting variables to model
The standard Bayesian approach to model selection is based on the marginal likelihood
(4) and runs into problems when the issue is which variables to include as dependent
variables in a multivariate model. The likelihoods are simply not comparable when vari-
ables are added to or dropped from yt. In forecasting applications there is an additional
consideration. We are, in general, not interested in how well the model as a whole ï¬ts the
data only in how well it forecasts a core set of variables of interest. Other variables are
then only included if they are expected to improve the forecast performance.
8.2.1
Marginalized predictive likelihoods
Andersson and Karlsson (2009) suggests replacing the marginal likelihood with the pre-
dictive likelihood for the variables of interest, that is after marginalizing out the other
variables, in the calculation of posterior â€probabilitiesâ€ or model weights. This creates
a focused measure that can be used for model selection or forecast combination and is
attractive in a forecasting context since it directly addresses the forecasting performance
of the diï¬€erent models
The predictive likelihood approach is based on a split of the data into two parts, the
training sample, Yâˆ—
n = (yâ€²
1, yâ€²
2, . . . , yâ€²
n)â€² of size n, and an evaluation or hold out sample,
eYn =
 yâ€²
n+1, yâ€²
n+2, . . . , yâ€²
T
â€² of size T âˆ’n. The training sample is used to convert the
prior into a posterior and the predictive likelihood for the hold out sample is obtained by
marginalizing out the parameters from the joint distribution of data and parameters,
p

eYn
 Yâˆ—
n,Mi

=
Z
L

eYn
 Î¸i, Yâˆ—
n, Mi

p (Î¸i| Yâˆ—
n, Mi) dÎ¸i.
Partitioning the hold out sample data into the variables of interest and the remaining
variables, eYn =

eY1,n, eY2,n

, the marginalized predictive likelihood for the variables of
interest is obtained by marginalizing out eY2,n,
MPL

eY1,n
 Yâˆ—
n,Mi

=
Z
p

eYn
 Yâˆ—
n,Mi

d eY2,n.
Predictive weights that can be used for model averaging or model selection are then
calculated as
w

Mi| eY1,n, Yâˆ—
n

=
MPL

eY1,n
 Yâˆ—
n,Mi

p (Mi)
PM
j=1 MPL

eY1,n
 Yâˆ—
n,Mj

p (Mj)
(100)
where MPL

eY1,n
 Yâˆ—
n,Mi

is evaluated at the observed values of the variables of interest
in the hold out sample.
While the predictive weights (100) strictly speaking can not be interpreted as posterior
probabilities they have the advantage that proper prior distributions are not required for
the parameters. The predictive likelihood is, in contrast to the marginal likelihood, well
deï¬ned as long as the posterior distribution of the parameters conditioned on the training
sample is proper.
67

The use of the predictive likelihood is complicated by the dynamic nature of VAR
models. As noted by Andersson and Karlsson (2009) the predictive likelihood is the joint
predictive distribution over lead times h = 1 to T âˆ’n. This will become increasingly
uninformative for larger lead times and unrepresentative of lead times such as h = 4 or 8
usually considered in macroeconomic forecasting. At the same time the hold out sample
needs to be relatively large in order to provide a sound basis for assessing the forecast
performance of the models. To overcome this Andersson and Karlsson suggested focusing
the measure to speciï¬c lead times h1, . . . , hk and using a series of predictive likelihoods,
g

eY1,n|Mi

=
Tâˆ’hk
Y
t=n
MPL (y1,t+h1, . . . , y1,t+hk| Yâˆ—
t ,Mi) ,
(101)
in the calculation of the predictive weights.
A ï¬nal complication is that the predictive likelihood is not available in closed form
for lead times h > 1 and must be estimated using simulation methods. With a normal
likelihood the predictive likelihood for a VAR model will be normal conditional on the
parameters and easy to evaluate. Andersson and Karlsson suggested estimating the mul-
tiple horizon marginalized predictive likelihood using a Rao-Blackwellization technique
as
\
MPL (y1,t+h1, . . . , y1,t+hk| Yâˆ—
t ,Mi) = 1
R
R
X
i=1
p

y1,t+h1, . . . , y1,t+hk| Yâˆ—
t ,Mi, Î¸(j)
i

by averaging the conditional predictive likelihood p (y1,t+h1, . . . , y1,t+hk| Yâˆ—
t ,Mi, Î¸i) over
draws, Î¸(j)
i , of the parameters from the posterior distribution based on Yâˆ—
t . This leads to
estimated predictive weights
bw

Mi| eY1,n, Yâˆ—
n

=
bg (Y,n|Mi) p (Mi)
PM
j=1 bg

eY1,n|Mj

p (Mj)
(102)
with
bg

eY1,n|Mi

=
Tâˆ’hk
Y
t=n
\
MPL (y1,t+h1, . . . , y1,t+hk| Yâˆ—
t ,Mi) .
The marginalized predictive likelihood procedure is thus in principle applicable to any
forecasting model, with any type of prior, as long as the likelihood is normal and it is
possible to simulate the posterior distribution of the parameters.
Forecasting performance
Andersson and Karlsson (2009) conducted a forecasting
exercise with the aim of forecasting US GDP growth and considered VAR models with
up to four variables selected from a set of 19 variables (including GDP). Compared to an
AR(2) benchmark forecast combination using the predictive weights (102) does better for
shorter lead times (up to 4 quarters) but is outperformed for lead times 5 to 8. Selecting
a single model based on the predictive weights does slightly worse than the forecast
combination for the shorter lead times but performs better and on par with the AR(2)
for the longer lead times.
68

8.2.2
Marginal likelihoods via Bayes factors
JarociÂ´nski and MaÂ´ckowiak (2011) favours the marginal likelihood as a basis for model
comparison and notes that the question of wether a set of variables is useful for forecasting
the variables of interest can be addressed in a model containing all entertained variables.
To see this write the VAR model in terms of two sets of variables y1,t and y2,t where y1,t
contains the variables of interest and, possibly, some additional variables and y2,t contains
the remaining variables,
 yâ€²
1,t, yâ€²
2,t

=
p
X
i=1
 yâ€²
1,tâˆ’i, yâ€²
2,tâˆ’i

Ai + xâ€²
tC + uâ€²
t
with
Ai =
 Ai,11
Ai,12
Ai,21
Ai,22

partitioned conformably. The notion that y2,t is not useful for predicting y1,t (does not
Granger-cause y1,t) then corresponds to the block-exogeneity restriction that Ai,21 = 0,
âˆ€i. If the restriction holds y1,t can be modelled as a function of its own lags and y2.t is not
needed. Each partition of the variables into y1,t and y2,t gives rise to a diï¬€erent block-
exogeneity restriction and the idea of JarociÂ´nski and MaÂ´ckowiak (2011) is to compute the
marginal likelihood for all the variables under the diï¬€erent restrictions and base model
selection or model averaging on these in a standard fashion.
This approach overcomes the main problem with the marginal likelihood of comparing
apples with oranges when diï¬€erent sets of left hand variables are considered. Unfortu-
nately, the marginal likelihood under the restrictions is rarely available in closed form.
The marginal likelihoods and posterior model probabilities can, however, be computed
indirectly by way of the Savage-Dickey density ratio (Dickey (1971)),
BFR,U =
m

Y|Ai,21 = 0

m (Y)
= pAi,21 (Ai,21 = 0|Y)
Ï€Ai,21 (Ai,21 = 0) ,
(103)
which relates the Bayes factor comparing the restricted model and the unrestricted model
to the ratio of the marginal posterior to the marginal prior for the restricted parameters
evaluated under the restriction. The second equality in (103) holds under the speciï¬c
condition,
Ï€R (Î“R, Î¨) = Ï€U

Î“R, Î¨|Ai,21 = 0

,
(104)
that the prior for the parameters in the restricted model equals the prior for the unre-
stricted model when conditioning on the restriction.
JarociÂ´nski and MaÂ´ckowiak (2011) suggests a normal-Wishart prior (section 3.2.1) as
a prior for the unrestricted model and constructs the prior for the restricted models by
conditioning on the restriction to ensure that condition (104) holds.25 With the conjugate
normal-Wishart prior both the marginal prior and posterior in the unrestricted model will
be matricvariate-t distributions. Partition yt into the n1 variables y1,t and the n2 variables
25The resulting prior for the restricted model is not normal-Wishart and presumably diï¬€erent from
what would be used when estimating a model for just y1,t.
69

y2,t and let P and Q be the matrices of dimension pn2 Ã— k and m Ã— n1 that selects the
rows and columns of Î“ corresponding to Ai,21,
ï£«
ï£¬
ï£­
A1,21
...
Ap,21
ï£¶
ï£·
ï£¸= PÎ“Q.
For the prior we have (see Appendix C) PÎ“Q|Î¨ âˆ¼MNpn2,n1 (PÎ“Q, Qâ€²Î¨Q, Pâ„¦Pâ€²) and
PÎ“Q âˆ¼Mtpn2,n1
 PÎ“Q, (Pâ„¦Pâ€²)âˆ’1 , Qâ€²SQ,v âˆ’n2

using that Qâ€²Î¨Q âˆ¼iW (Qâ€²SQ, v âˆ’n2)
since Q is a selection matrix. An equivalent result holds for the posterior and the Bayes
factor can be obtained as
BFR,U =
n1
Y
i=1
Î“ ((v âˆ’n2 + 1 âˆ’i) /2) Î“ ((v âˆ’n2 + pn2 + 1 âˆ’i) /2)
Î“ ((v âˆ’n2 + pn2 + 1 âˆ’i) /2) Î“ ((v âˆ’n2 + 1 âˆ’i) /2)
(105)
Ã—
|Pâ„¦Pâ€²|n1/2 |Qâ€²SQ|âˆ’(vâˆ’n2)/2 Qâ€²SQ+
 PÎ“Pâ€²â€²  Pâ„¦P
âˆ’1  PÎ“Pâ€²
âˆ’(vâˆ’n2+pn2)/2
Pâ„¦Pâ€²n1/2 Qâ€²SQ
âˆ’(vâˆ’n2)/2 Qâ€²SQ+ (PÎ“Pâ€²)â€² (Pâ„¦P)âˆ’1 (PÎ“Pâ€²)
âˆ’(vâˆ’n2+pn2)/2 .
The posterior model probabilities can be obtained directly from the Bayes factors or via
the marginal likelihoods for the restricted and unrestricted models by noting that the
marginal likelihood for the unrestricted model is the matricvariate-t distribution given in
(20).
JarociÂ´nski and MaÂ´ckowiak (2011) also considered models deï¬ned by multiple block-
exogeneity restrictions where the Bayes factor is not available in closed form but can be
evaluated using Monte Carlo methods. While the Bayes factor (105) is easy to evaluate
the computations can be prohibitive if the number of considered variables is large â€“
especially if multiple block-exogeneity restrictions are considered â€“ and JarociÂ´nski and
MaÂ´ckowiak (2011) proposes a Markov chain Monte Carlo model composition, (MC)3,
scheme (Madigan and York (1995)) to identify the most promising models.
9
High Dimensional VARs
Most applications involve relatively small VAR models with up to 5 or 6 variables and
occasionally 10 or more variables. There are obvious reasons for this â€“ the number of
parameters to estimate grows rapidly with m and p and can exhaust the information
in the data while the models get unwieldy and sometimes diï¬ƒcult to interpret. There
are, however, occasions that more or less demands that a large number of variables are
modelled jointly. The earliest such example is perhaps panel studies, e.g. to forecast
regional economic development one could specify a small VAR-model for each region or
specify a joint VAR for the panel of regions that allows for interaction between the regions.
In this case it is not only the increased size of the model that contributes to the complexity,
there is also the need to take account of heterogeneity across regions and, perhaps, time,
see Canova and Ciccarelli(2004, 2009).
A second situation is the task of forecasting in a data-rich environment with â€wideâ€
data sets that can contain 100 or more variables with potential predictive content for the
variables of interest. This has typically been tackled with dynamic factor models (Stock
70

and Watson (2002), Forni, Hallin, Lippi and Reichlin (2003)) where the information in
the data is summarized by a few factors or by combining forecasts from small models
with diï¬€erent combinations of predictor variables (see Stock and Watson (2006) for a
review). Recent studies do, however, indicate that large Bayesian VAR models can be
quite competitive.
VAR models for wide data sets face numerical challenges due to the sheer size of the
model, with m = 100 variables and p = 4 lags there are 40 000 parameters in Î“. OLS
estimation is still feasible provided that T > 400 since this â€onlyâ€ involves inversion of the
400 Ã— 400 matrix Zâ€²Z although estimates will be very imprecise due to the large number
of parameters. Similarly, Bayesian analysis with a normal-Wishart prior beneï¬ts from
the Kronecker structure and is computationally feasible while more general priors such
as the normal-diï¬€use or independent normal Wishart are faced with the inversion of a 40
000 Ã— 40 000 matrix. The sheer size of the problems makes MCMC exercises impractical
and too time consuming with current desktop resources even if one ignores the issue of
numerical stability when solving high dimensional equation systems.26
In line with the dynamic factor model literature De Mol, Giannone and Reichlin (2008)
considers â€directâ€ univariate forecasting models of the form
yt+h = xâ€²
tÎ²h + ut+h
where xt contains (a large number of) variables believed to be useful when forecasting yt.
Compared to a truly dynamic speciï¬cation (e.g. a VAR) this has the advantage that there
is no need to forecast xt for lead times h > 1. The disadvantage is that the distribution
of the error term is more complicated, in general ut+h follows a MA(h âˆ’1) process and
that separate equations must be estimated for each lead time. In a small forecasting ex-
ercise with n = 131 potential predictors in xt they demonstrate that principal component
regression (that is a dynamic factor model) and Bayesian forecasts based on a normal
or double exponential prior for Î²h are viable methods for dealing with very large data
sets. When n is large there is a considerable risk of overï¬tting and, pragmatically, the
success of the Bayesian approach depends on applying an appropriate amount of shrink-
age in the prior. De Mol et al. (2008) analyses the behavior of the forecasts as both n
and T â†’âˆunder the assumption that the data can be described by a factor structure,
yt+h = f â€²
tÎ³ + et+h, xt = Î›f t + Î¾t where ft contains the r common factors. They show
that the Bayes forecast yt (h) = xâ€²
tÎ² for Î² the posterior mean of Î² with a normal prior
Î² âˆ¼N (0,Î£) converges to the â€population forecastâ€ f â€²
tÎ³ if the variance of Î¾t is small
relative to the contribution of the factors to the variance of xt and the prior variance for
Î² is chosen such that |Î£| = O
 1
nT 1/2+Î´

, 0 < Î´ < 1/2. That is, the degree of shrinkage
should increase with both n and T in order to protect against overï¬tting.
Korobilis (forthcominga) use the same type of univariate direct forecasting model as
De Mol et al. (2008) to forecast 129 US marcroeconomic variables using the other 128
variables as explanatory variables. The forecasts are made using 5 diï¬€erent hierarchical
26It should be made clear that no actual matrix inverse of this size is needed. The conditional posterior
for Î³ has the form Î³|YT , Î¨ âˆ¼N
 Î³, Î£Î³

with Î³ = Aâˆ’1b and Î£Î³ = Aâˆ’1. Î³ can be calculated by
Cholesky decomposing A = Câ€²C and then use forward and back substitution to solve the triangular
equations systems Câ€²x = b and CÎ³= x in turn. A draw from Î³|YT , Î¨ is obtained by generating a vector
of standard normals, z, and computing Î³ + Câˆ’1z =Î³ + ez where ez is obtained by solving Cez = z by back
substitution. This is much more numerically stable than straightforward inversion of the matrices and
also faster.
71

shrinkage priors where the hierarchical structure is used to allow the degree shrinkage to be
inï¬‚uenced by the data and speciï¬c to each explanatory variable. As a comparison forecasts
are also made with a dynamic factor model using the ï¬rst ï¬ve principal components as
factors. Priors designed to mimic the LASSO and the Elastic Net are found to perform
best when the forecasts are compared using the mean absolute error while the dynamic
factor model performs best if the mean squared error criterion is used.
9.1
Factor augmented VAR
Bernanke, Boivin and Eliasz (2005) proposed the factor augmented VAR (FAVAR) as
a means of incorporating the information from a large number of variables in a VAR
in a parsimonious way. There are two basic assumption, that the data admits a factor
structure, xt = Î›f t + Î¾t where the information in the n auxiliary variables in xt can be
represented by the r factors in ft with r â‰ªn and that the variables of interest, yt, and
the factors can be jointly modelled as a VAR
eyâ€²
t =
 ft
yt
â€²
=
p
X
i=1
eyâ€²
tâˆ’iAi+uâ€²
t.
(106)
yt and xt and hence also ft are assumed to be stationary and we will work with demeaned
data so there is no constant term in the VAR. Bernanke et al. (2005) augments the
factor structure by allowing the variables of interest to be directly related to the auxiliary
variables
xt = Î›fft + Î›yyt + Î¾t
(107)
instead of only indirectly through the factors ft. Like any factor model (107) suï¬€ers from
a fundamental lack of identiï¬cation since any full rank rotation of the factors will leave
the model unaï¬€ected, e.g. Î›fft =
 Î›fPâˆ’1
(Pf t) = Î›fâˆ—f âˆ—
t for P full rank. Bernanke et al.
(2005) show that the restrictions
Î›f =
 Ir
Î›f
âˆ—

, Î›y =
 0rÃ—m
Î›y
âˆ—

together with the exact factor model assumption that Î = V (Î¾t) is diagonal is suï¬ƒcient
for identiï¬cation. The restriction on Î›f is just a normalization whereas the restriction
on Î›y is substantial and implies that the ï¬rst r variables in xt does not respond contem-
poraneously to yt.
The key to inference in the FAVAR model is to recognize that it is a state space model
(see Appendix B) with (107) as the observation equation and (106) as the state equation.
The Kalman ï¬lter requires that the state equation has the Markov property, i.e. that it
is autoregressive of order 1, and we rewrite the state equation with an expanded state
vector
st+1 =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
eyt+1
eyt
...
eytâˆ’p+2
ï£¶
ï£·
ï£·
ï£·
ï£¸=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Aâ€²
1
Aâ€²
2
Â· Â· Â·
Aâ€²
pâˆ’1
Aâ€²
p
I
0
0
0
0
I
I
0
...
...
...
0
0
I
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
st +
ï£«
ï£¬
ï£¬
ï£¬
ï£­
ut
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£¸
(108)
= Tst + Î·t
72

and include the observable yt in the observation equation
wt =
 xt
yt

=
ï£«
ï£­
 Ir
Î›f
âˆ—

 0rÃ—m
Î›y
âˆ—

0
Â· Â· Â·
0
0
Im
0
Â· Â· Â·
0
ï£¶
ï£¸st +
 Î¾t
0

(109)
= Zst + Îµt.
We make the usual assumption that the innovations ut and Î¾t are iid normal and inde-
pendent of each other, ut âˆ¼N (0, Î¨) and Î¾t âˆ¼N (0, Î) with Î diagonal.
Conditional on the parameters T, Z, Î¨ and Î we can use the Kalman ï¬lter and the
simulations smoother to draw the latent factors from the full conditional posterior (see
Appendix B). Note that system matrices T, Z, H = V (Îµt) and Q = V (Î·t) contains
a large number of zeros and the computations can be speeded up by taking account of
the structure of the matrices. Note that including yt as left hand side variables in the
observation equation carries yt through to the state vector. That is, st|t contains yt since
it is known at time t and st+1|t contains the minimum mean squared error prediction,
Et (yt+1) , of the unknown yt+1. The Kalman ï¬lter recursions need to be started up with
a prior for the ï¬rst state, s1 âˆ¼N
 s1|0, P1|0

.
Having run the Kalman ï¬lter, the last state can be sampled from the full conditional
sT|yT, xT, Î“, Î›, Î, Î¨ âˆ¼N
 sT|T, PT|T

(110)
where Î“ collects the autoregressive parameters, Î“â€²=
 Aâ€²
1, . . . , Aâ€²
p

and Î› =
 Î›f, Î›y
.
Note that it suï¬ƒces to draw the factor fT since yt, t = 1, . . . , T is known. Reï¬‚ecting this,
the variance matrix PT|T is also singular which would cause numerical problems trying to
draw yT. The remaining factors, ft, t = T âˆ’1, . . . , 1, can be drawn from the conditionals
ft|yT, xT, Î“, Î, Î¨, f t+1 âˆ¼N
 st|T, Pt|T

using the simulation smoother. Algorithm B.3 in
Appendix B can, however, not be used directly due to the presence of yt and lags in the
state vector. To implement the simulation smoother we need the conditional distributions
st|yt, xt, Î“, Î›, Î, Î¨, ft+1 âˆ¼N
 st|t,ft+1, Pt|t,ft+1

.
(111)
Using that eyt+1|t =

f â€²
t+1|t, yâ€²
t+1|t
â€²
âˆ¼N
 Î“â€²st|t, Î“â€²Pt|tÎ“ + Î¨

it is easy to see that the
recursions for the parameters of the conditional distributions becomes (see Kim and Nelson
(1999, p. 194-196))
st|t,ft+1 = st|t + Pt|tÎ“â€²  Î“â€²Pt|tÎ“ + Î¨
âˆ’1  eyt+1 âˆ’Î“â€²st|t

(112)
Pt|t,ft+1 = Pt|t âˆ’Pt|tÎ“
 Î“â€²Pt|tÎ“ + Î¨
âˆ’1 Î“â€²Pt|t
for t = T âˆ’1, . . . , 1. It is again suï¬ƒcient to only draw the factor ft at each iteration.
Inference on the remaining parameters is standard conditional on the factors. The
state equation (106) is a standard VAR and draws from the full conditional posterior for
Î¨ and Î“ can be obtained using the results in section 3.2.1 with a normal-Wishart prior
and section 3.2.2 with an independent normal Wishart or normal-diï¬€use prior.
The observation equation (107) can be analyzed as n univariate regressions when Î is
diagonal. The identifying restrictions imply that we have
xit = fit + Î¾it, i = 1, . . . , r
xit = eyâ€²
tÎ»i + Î¾it, i = r + 1, . . . , n
73

where Î»â€²
i is row i of Î› =
 Î›f, Î›y
. Let Ïƒ2
i be the variance Î¾it, the conjugate prior is of
the normal-Gamma form,
Ïƒ2
i âˆ¼iG (ai, bi)
Î»i|Ïƒ2
i âˆ¼N
 Î»i, Ïƒ2
i Vi

and the full conditional posteriors are given by
Î»i|yT, xT, f T, Ïƒ2
i âˆ¼N
 Î»i, Ïƒ2
i Vi

, i = r + 1, . . . , n
(113)
Vi =

Vâˆ’1
i
+ eYâ€² eY
âˆ’1
Î»i = Vi

Vâˆ’1
i Î»i + eYâ€² eYbÎ»i
âˆ’1
and
Ïƒ2
i |yT, xT, f T âˆ¼iG
 ai, bi

(114)
ai =

ai + T/2, i = 1, . . . , r
ai + (T âˆ’r âˆ’m) /2, i = r + 1, . . . , n
bi =
(
bi + 1
2
PT
i=1 (xit âˆ’fit)2 , i = 1, . . . , r
bi + 1
2

xâ€²
ixi + Î»â€²
iVâˆ’1
i Î»i âˆ’Î»
â€²
iViÎ»i

, i = r + 1, . . . , n
where eY is the matrix of explanatory variables eyâ€²
t and bÎ»i is the OLS estimate bÎ»i =

eYâ€² eY
âˆ’1 eYâ€²xi.
Specifying the prior
It is diï¬ƒcult to have information a priori about the latent factors
and the prior for the ï¬rst state, s1 âˆ¼N
 s1|0, P1|0

, is best taken to be non-informative,
for example s1|0 = 0 and P1|0 = 5I. It is also diï¬ƒcult to form prior opinions about the
factor loadings in Î›f and Î›y and non-informative priors are advisable, Bernanke et al.
(2005) sets Î»i = 0, Vi = I, ai = 0.001 and bi = 3. The prior for Î“ and Î¨ can be based
on the same considerations as a standard VAR while taking account of the stationarity
assumption.
Sampling from the posterior
A Gibbs sampler for the joint posterior distribution of
the factors and the parameters can be constructed running the simulation smoother for
the factors and sample the parameters from the full conditional posteriors, see Algorithm
15.
Forecasting performance
See discussion of Gupta and Kabundi (2010) in section 9.2
and discussion of Korobilis (2008) in section 8.1
9.2
Large BVARs
9.2.1
Reducing parameter uncertainty by shrinkage
Banbura et al. (2010) studies the forecast performance of large BVARs. In an application
to forecasting US non-farm employment, CPI and the Federal Funds Rate the performance
74

Algorithm 15 Gibbs sampler for the FAVAR model
For the FAVAR (106, 107) select starting values Î“(0), Î¨(0), Î›(0) and Î(0).
For j = 1, . . . , B + R
1. Draw
the
factor
f (j)
T
from
the
full
conditional
posterior
sT|yT, xT, Î“(jâˆ’1), Î›(jâˆ’1), Î(jâˆ’1), Î¨(jâˆ’1) in (110) obtained by running the Kalman
ï¬lter (126) in Appendix B. For t = T âˆ’1, . . . , 1 draw f (j)
t
from the full condition
posterior st|yt, xt, Î“(jâˆ’1), Î›(jâˆ’1), Î(jâˆ’1), Î¨(jâˆ’1), f (j)
t+1 in (111) obtained by running
the simulation smoother (112).
2. Draw Î¨(j) and Î“(j) from the conditional posteriors Î¨|yT, xT, Î›(jâˆ’1), Î(jâˆ’1), f (j) in
(19) and Î“|yT, xT, Î›(jâˆ’1), Î(jâˆ’1), Î¨(j), f (j) in (18) with a normal-Wishart prior or
Î¨|yT, xT, Î“(jâˆ’1), Î›(jâˆ’1), Î(jâˆ’1), f (j) in (25) and Î“|yT, xT, Î›(jâˆ’1), Î(jâˆ’1), Î¨(j), f (j) in
(24) with an independent normal Wishart prior.
3. For i = 1, . . . , n draw Ïƒ2(j)
i
from the full conditional posterior Ïƒ2
i |yT, xT, f (j) in (114)
and (for i > r) Î»(j)
i
from the full conditional posterior Î»i|yT, xT, f (j), Ïƒ2(j)
i
in (113).
4. If j > B generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
eyâˆ—(j)â€²
T+h =
hâˆ’1
X
i=1
eyâˆ—(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
eyâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
Discarding the parameters yields
n
eyâˆ—(j)
T+1, . . . eyâˆ—(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution of the factors and the variables of interest y.
of 4 diï¬€erent VAR models with 3, 7, 20 and 131 variables, all with 13 lags, is evaluated.
Based on the theoretical results of De Mol et al. (2008), Banbura et al. (2010) suggest
that the degree of shrinkage applied through the prior should increase with the size of
the model. Working with a normal-Wishart prior distribution with Minnesota type prior
beliefs, the overall scaling factor Ï€1 in (21) determines the amount of shrinkage. Banbura
et al. sets this to match the in sample ï¬t of the smallest VAR estimated with OLS (i.e.
Ï€1 = âˆ) for the three variables of interest where the ï¬t of model M with prior scaling
Ï€1 is measured by
Fit (Ï€1, M) = 1
3
X
iâˆˆI
mse(Ï€1,M)
i
mse(0)
i
,
(115)
the average of the in-sample mean square error normalized by the MSE of a pure random
walk model. In this particular application this leads to scaling factors âˆ, 0.52, 0.33 and
0.19.27 That is, the small 3-variable VAR is estimated with OLS and the scale factor for
the 7 variable VAR is 0.52. The main ï¬nding is that, with the increased shrinkage, forecast
performance improves with model size but also that most of the gains was achieved with
27These numbers diï¬€er from the ones reported by Banbura et al. (2010) since they parameterize the
prior in terms of the square of Ï€1.
75

the 20 variable VAR. A moderately large VAR might thus be suï¬ƒcient provided that the
right variables are selected.
Bloor and Matheson (2010) working in an open economy context with a need to make
foreign variables exogenous to domestic variables generalized the approach of Banbura
et al. (2010) by considering diï¬€erent amounts of shrinkage for diï¬€erent blocks of equa-
tions. This implies that the convenient normal-Wishart prior can not be used and Bloor
and Matheson (2010) base their inference on the blocked importance sampler of Zha (1999)
embodying the same kind of Minnesota type prior beliefs with the addition of a dummy
initial observations prior. To impose diï¬€erent amount of shrinkage the prior hyperparam-
eters are made block speciï¬c and chosen using the same strategy as Bloor and Matheson
(2010). Bloor and Matheson (2010) forecasts the New Zealand real GDP, tradable CPI,
non-tradable CPI, an interest rate and the exchange rate using a range of BVARs with
5, 8, 14 and 94 variables. In addition, forecast results are reported for a univariate AR,
a random walk model and frequentist variants of the smallest BVAR. Overall the largest
BVAR provides the best forecasts except for the long horizon (4 quarters) forecast of
tradable CPI where two 5 variable VARs, the BVAR and a frequentist with lags selected
using BIC, gives signiï¬cantly MSEs.
Gupta and Kabundi (2010) conducts a forecasting exercise where the aim is to forecast
the South African per capita growth rate, inï¬‚ation, the money market rate and the growth
rate of the nominal eï¬€ective exchange rate. The models used are a small (four variable)
DSGE model, a dynamic factor model, a FAVAR using the four variables of interest
estimated by OLS and a Bayesian variant using a Minnesota type prior, a four variable
unrestricted VAR, two BVARs with 4 and 266 variables. The factor models use principal
components as factors. For the VAR models the lag length is set to ï¬ve with quarterly
data and the overall scale factor Ï€1 in the Minnesota prior for the BVARs is set following
the approach of Banbura et al. (2010) in addition to common default settings. In addition
Gupta and Kabundi (2010) also experiment with the lag decay rate Ï€3 using settings of
0.5, 1 and 2. For the small VAR the additional shrinkage on lags of other variables is set
to Ï€2 = 0.5. In the large VAR a tighter speciï¬cation is used with Ï€2 = 0.6 for foreign
(world) variables and for domestic variables 0.1 is used in domestic equations and 0.01
in world equations. Overall the large BVAR with the tightest shrinkage, Ï€1 = 0.01 and
Ï€3 = 1, does well and delivers the best forecast for three out of the four variables. The
exception being the exchange rate where the DSGE model does best.
9.2.2
Selecting variables - conjugate SSVS
Koop (2010) considers a range of prior speciï¬cations for forecasting with large BVARs.
This includes the normal-Wishart with Minnesota type prior beliefs used by De Mol
et al. (2008), the original Litterman prior with ï¬xed and diagonal error variance matrix
(section 3.1) which oï¬€ers the advantage that diï¬€erent shrinkage can be applied to lags
of the dependent variable and lags on other variables, the same Minnesota prior but also
allowing for correlation between the variables of interest, the SSVS prior (section 8.1)
with two diï¬€erent settings for the prior variances and a new â€conjugateâ€ SSVS prior that
is less computationally demanding and better suited for large BVARs.
The new SSVS prior takes Î“ to be distributed as a matricvariate normal conditionally
76

on Î¨ and the vector of selection indicators Î´,
Î“|Î¨, Î´ âˆ¼MNkm (Î“, Î¨,â„¦Î´)
(116)
with â„¦Î´ = diag (h1, . . . , hk) for
hi =
 Ï„ 2
0,i if Î´i = 0
Ï„ 2
1,i if Î´i = 1 .
The prior for Î´i is independent Bernoulli distributions with P (Î´i = 1) = pi and for Î¨ an
inverse Wishart, Î¨ âˆ¼iW (S, v) is used. The prior structure is thus conjugate conditional
on Î´.
In contrast with the standard SSVS procedure, the conjugate SSVS includes or ex-
cludes a variable in all equations at the same time instead of being speciï¬c to one variable
and equation. While this reduces the ï¬‚exibility, the Kronecker structure of the prior and
posterior variance matrices for Î“ makes for much more eï¬ƒcient computations.
The conditional posterior distributions in (18) and (19) still holds but should be inter-
preted as conditional on Î´. The marginal posterior for Î´ is obtained up to a proportionality
constant by integrating out Î“ and Î¨ from the product of the likelihood and the prior
yielding the matricvariate-t distribution (20) times the prior for Î´, Ï€ (Î´) . After some sim-
pliï¬cations of the expression for the matricvariate-t density we have
p (Î´|YT) âˆg (Î´, YT) =
 |â„¦Î´| /
â„¦Î´
âˆ’m/2 S
âˆ’(v+T)/2 Ï€ (Î´) .
(117)
For k small it is possible to enumerate p (Î´|YT) but since there are 2k possible conï¬gu-
rations for Î´ this quickly becomes infeasible and Koop (2010) suggests a Gibbs sampling
approach for sampling from the marginal posterior of Î´ originally proposed by Brown,
Vanucci and Fearn (1998). This is reproduced as part A of Algorithm 16.
Forecasting performance
Koop evaluates the forecasting performance using a data
set with 168 US macroeconomic variables and four diï¬€erent VAR models with 3, 20, 40
and 168 variables with four lags and formulated to generate direct forecasts. The variables
of interest are real GDP, the CPI and the Federal Funds Rate. For the SSVS priors two
diï¬€erent ways of setting the prior variances are used. The â€semiautomaticâ€ approach of
George et al. (2008) sets Ï„0,ij = bÏƒÎ³ij/10 and Ï„1,ij = 10bÏƒÎ³ij where bÏƒÎ³ij is the standard error
of the OLS estimate of Î³ij for the standard SSVS. For the conjugate SSVS the maximum
of bÏƒÎ³ij for a given i is used. The other approach is based on the Minnesota prior and
sets Ï„1,ij according to (14) and Ï„0,ij = Ï„1,ij/10 for the standard SSVS and mimics the
normal-Wishart prior variance for the conjugate SSVS. For priors with Minnesota type
prior beliefs the scale factors Ï€1 and Ï€2 are set in the same way as in Banbura et al. (2010)
using (115).
As a complement forecasts are also calculated using for a number of FAVARs con-
structed by adding lags of the principal components to the three-variable VAR.
The results of the forecasting exercise is mixed and there is no clear winner but some
patterns do emerge. The factor models does not do very well and never performs best.
There is a gain from moving to larger models but as in Banbura et al. (2010) the additional
gains are small once one moves beyond 20 variables in the VAR. The Minnesota and SSVS
type priors have almost the same number of wins and there is no indication that one is
better than the other.
77

Algorithm 16 Gibbs sample for â€conjugateâ€ SSVS
With the conditional prior (116) for Î“, inverse Wishart prior for Î¨ and independent
Bernoulli priors on Î´i part A below samples from the marginal posterior for Î´. After
convergence this can be complemented with part B to produce draws from the joint
posterior for Î´, Î“ and Î¨.
A. Select starting values Î´(0)
For j = 1, . . . , B + R
1. Draw
Î´(j)
i ,
i
=
1, . . . , k,
from
the
full
conditional
Î´(j)
i |YT, Î´(j)
1 , . . . , Î´(j)
iâˆ’1, Î´(jâˆ’1)
i+1 , . . . , Î´(jâˆ’1)
k
âˆ¼
Ber (u1i/ (u0i + u1i))
where
u0i = g (Î´âˆ’i, Î´i = 0, YT) , u1i = g (Î´âˆ’i, Î´i = 1, YT) and g (Â·) is given by
(117).
B. If j > B and a sample from the joint posterior for Î´, Î“ and Î¨ or the predictive
distribution is desired
2. Draw Î¨(j) from the full conditional posterior Î¨|YT, Î´(j) âˆ¼iW
 S, v

in (19).
3. Draw
Î“(j)
from
the
full
conditional
posterior
Î“|YT, Î¨(j), Î´(j)
âˆ¼
MNkm
 Î“, Î¨, â„¦Î´

in (18).
4. Generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the
joint predictive distribution.
9.3
Reduced rank VAR
Carriero, Kapetanios and Marcellino (2011) use standard VAR models (6) with iterated
forecasts for lead times h > 1 and propose diï¬€erent ways of overcoming the â€curse of
dimensionalityâ€. In addition to the standard VAR-model with a tight normal-Wishart
prior they propose the use of models with reduced rank parameter matrices. Working
with data transformed to stationarity and then standardized a VAR without determin-
istic terms, yâ€²
t = Pp
i=1 yâ€²
tâˆ’iAi+uâ€²
t, is used and the reduced rank assumption is that the
parameter matrices can be written as Ai = Î²iÎ±â€² where Î²i and Î± are m Ã— r matrices. In
matrix form we can write the VAR as
Y = ZÎ“ + u = ZÎ²Î±â€² + u
(118)
for Î“â€² =
 Aâ€²
1, . . . , Aâ€²
p

and Î² =
 Î²â€²
1, . . . , Î²â€²
p
â€² a mp Ã— r matrix with Î² and Î± of rank
r < m. The setup is similar to the cointegrated VECM (44) and the same basic issue
of the lack of identiï¬cation of Î² and Î± arises here. Following Geweke (1996a) Carriero,
78

Kapetanios and Marcellino use a linear normalization, Î±â€² = (Ir, Î±â€²
âˆ—) and specify a prior
of the form
vec (Î±âˆ—) âˆ¼N (vec (Î±âˆ—) , Î£Î±) , Î² âˆ¼N
 Î², Î£Î²

, Î¨ âˆ¼iW (S, v) .
(119)
Again, following Geweke, Î±âˆ—and Î² are set to zero and Î£Î± and Î£Î² are diagonal matrices
with diagonal elements 1/Ï„ 2 in the application.
The derivation of the full conditional posteriors parallels the one for the VECM in
section 5.1 with obvious changes due to the diï¬€erent normalization. The full conditional
posterior for Î¨ is inverse Wishart,
Î¨|YT, Î², Î± âˆ¼iW
 S, v

, S = S + (Y âˆ’ZÎ“)â€² (Y âˆ’ZÎ“) , v = v + T.
(120)
For the full conditional posterior for Î±âˆ—rewrite the model as one set of equations for the
r ï¬rst variables which does not depend on Î±âˆ—
Y1 = ZÎ² + u1
and a set of equations for the remaining m âˆ’r variables depending on Î±âˆ—
Y2 = ZÎ²Î±â€²
âˆ—+ u2.
Using that u2|u1 âˆ¼MNT,mâˆ’r

u1Î¨âˆ’1
11 Î¨12, (Î¨22)âˆ’1 , IT

for
Î¨ =
 Î¨11
Î¨12
Î¨21
Î¨22

and Î¨22 =
 Î¨22 âˆ’Î¨21Î¨âˆ’1
11 Î¨12
âˆ’1 the lower right (m âˆ’r) Ã— (m âˆ’r) block of Î¨âˆ’1 we
have
Y2|Y1, Î², Î±âˆ—âˆ¼MNT,mâˆ’r

[Y1 âˆ’ZÎ²] Î¨âˆ’1
11 Î¨12 + ZÎ²Î±â€²
âˆ—,
 Î¨22âˆ’1 , IT

or a conditional regression28
Yc = Y2 âˆ’[Y1 âˆ’ZÎ²] Î¨âˆ’1
11 Î¨12 = ZÎ²Î±â€²
âˆ—+ u2.
It follows that the full conditional posterior for Î±âˆ—is normal29
vec (Î±âˆ—) |YT, Î², Î¨ âˆ¼N
 vec (Î±âˆ—) , Î£Î±

(121)
Î£Î± =
 Î£âˆ’1
Î± + Î¨22 âŠ—Î²â€²Zâ€²ZÎ²
âˆ’1
vec (Î±âˆ—) = Î£Î±

Î£âˆ’1
Î± vec (Î±âˆ—) + vec
 Î²â€²Zâ€²YcÎ¨22	
.
The full conditional posterior for the unrestricted Î² matrix follows immediately after
vectorizing the model,
y = (Î± âŠ—Z) vec (Î²) + u,
28That is, we factor the likelihood into a marginal distribution for Y1 which is functionally independent
of Î±âˆ—and a conditional distribution for Y2 that depends on Î±âˆ—. It is then suï¬ƒcient to consider the
conditional likelihood.
29Expressions (14) - (16) in Carriero, Kapetanios and Marcellino (2011), which in turn are based on
results in Geweke (1996a), are incorrect. See Karlsson (2012) for details.
79

as a normal distribution,
vec (Î²) |YT, Î±, Î¨ âˆ¼N
 vec
 Î²

, Î£Î²

(122)
Î£Î² =
 Î£âˆ’1
Î² + Î±â€²Î¨âˆ’1Î± âŠ—Zâ€²Z
âˆ’1
vec
 Î²

= Î£Î²
 Î£âˆ’1
Î² vec
 Î²

+ vec
 Zâ€²YÎ¨âˆ’1Î±

.
It is thus straightforward to implement a Gibbs sampler for the reduced rank VAR
model. Due to the relatively large variance matrix Î£Î² the Gibbs sampler can be time
consuming and Carriero, Kapetanios and Marcellino (2011) suggests a computationally
convenient alternative which they label reduced rank posterior. This is based on a re-
duced rank approximation to the posterior mean of Î“, Î“, with a tight normal-Wishart
prior. Let Î“ = UDVâ€² be the singular value decomposition of Î“ and collect the r largest
singular values and corresponding vectors in the matrices Dâˆ—= diag (d1, d2, . . . , dr) ,
Uâˆ—= (u1, u2, . . . , ur) and Vâˆ—= (v1, v2, . . . , vr) . A rank r < m approximation to Î“ is
then given by Î“
âˆ—= Uâˆ—Dâˆ—Vâˆ—â€².
Forecast performance
In a forecasting exercise with 52 macroeconomic variables for
the US Carriero, Kapetanios and Marcellino (2011) compare the performance of the
Bayesian procedures, VAR with normal-Wishart prior, the reduced rank VAR and the
reduced rank posterior, with several alternatives, a reduced rank VAR estimated with
maximum likelihood, multivariate boosting, factor models and univariate autoregressions.
The reduced rank posterior and the Bayesian reduced rank VAR procedures are found to
give the best forecasts, both in terms of forecasting all the 52 variables and when spe-
ciï¬c variables of interest (industrial production, inï¬‚ation and the federal funds rate) are
singled out.
9.4
Predicting many variables
Carriero, Kapetanios and Marcellino (2009, 2012) takes a slightly diï¬€erent viewpoint and
considers the situation where a large number of variables are to be predicted rather than a
small set of variables of interest. In an application to forecasting exchange rates Carriero,
Kapetanios and Marcellino (2009) use a direct forecast version of a one-lag VAR
yâ€²
t = yâ€²
tâˆ’hÎ¦h + Ï†h + eâ€²
t,h
(123)
with yt a m = 32 dimensional vector of log exchange rates. Taking et,h to be normal,
et,h âˆ¼N (0, Î¨h) they specify a normal-Wishart prior (section 3.2.1) for Î“â€²
h = (Î¦â€²
h, Ï†â€²
h)
and Î¨h centered on driftless univariate random walks. To avoid overï¬tting the prior is
very tight with Ï€1 on the order of 0.01, about 1/10 of the conventional setting for medium
sized VARs, and allowed to vary over time and chosen to minimize the sum of the mean
square forecast errors for the previous period. In a comparison of the forecast performance
with naive random walk forecasts, univariate autoregressions, forecasts from a standard
VAR estimated with OLS and factor models with 4 factors, the BVAR is found to perform
best with the random walk second.
Carriero, Kapetanios and Marcellino (2012) propose to use the same direct VAR (123)
to forecast the term structure of interest rates. In contrast to Carriero et al. (2009) the
scaling factor Ï€1 is chosen in an empirical Bayes fashion by maximizing the marginal
80

Algorithm 17 Gibbs sampler for the reduced rank VAR model
For the reduced rank VAR (118) and the prior (119) select starting values Î±(0)
âˆ—
and Î¨(0).
For j = 1, . . . , B + R
1. Generate Î²(j) from the full conditional posterior vec (Î²) |YT, Î±(jâˆ’1), Î¨(jâˆ’1) âˆ¼
N
 vec
 Î²

, Î£Î²

in (122).
2. Generate Î±(j)
âˆ—
from the full conditional posterior vec (Î±âˆ—) |YT, Î²(j), Î¨(jâˆ’1)
âˆ¼
N
 vec (Î±âˆ—) , Î£Î±

in (121).
3. Generate Î¨(j) from the full conditional posterior Î¨|YT, Î²(j), Î±(j) âˆ¼iW
 S, v

in
(120).
4. If j > B form Î“(j) = Î²(j)Î±(j)â€², generate u(j)
T+1, . . . , u(j)
T+H from ut âˆ¼N
 0, Î¨(j)
and
calculate recursively
ey(j)â€²
T+h =
hâˆ’1
X
i=1
ey(j)â€²
T+hâˆ’iA(j)
i
+
p
X
i=h
yâ€²
T+hâˆ’iA(j)
i
+ xâ€²
T+hC(j)+u(j)â€²
T+h.
Discarding the parameters yields
n
ey(j)
T+1, . . . ey(j)
T+H
oB+R
j=B+1 as a sample from the joint
predictive distribution.
likelihood (20) with respect to Ï€1. In the application to forecasting bond yields for 18
diï¬€erent maturities ranging from 1 to 120 months the marginal likelihood is maximized
with Ï€1 close to 0.003. The forecasting exercise includes, in addition to the direct BVAR,
several atheoretic time series models as well as theory based forecasting models. Overall
the BVAR performs best when forecast performance is measured by the root mean square
error. The picture is less clear when the forecasts are evaluated using economic measures.
Carriero et al. (2012) considers two diï¬€erent trading strategies. For the ï¬rst strategy
there is no clear ranking of the models when the diï¬€erent maturities are considered,
for the second strategy the BVAR delivers the best result for maturities longer than 21
months.
81

A
Markov chain Monte Carlo Methods
A.1
Gibbs sampler
The Gibbs sampler is particularly well suited to Bayesian computation since it is based on
the conditional distributions of subsets of the parameter vector. It is frequently the case
that it is easy to generate random numbers from the conditional posteriors even if the
joint posterior for all the parameters is non-standard. A case in point is regression models
like the VAR-model in section 2.1.1 where the posterior distribution of the regression
parameters Î³ conditional on the error variance-covariance is normal and the posterior
distribution of the variance-covariance matrix Î¨ conditional on Î³ is inverse Wishart.
MCMC is not needed in that particular case but we will see that this results carries over
to situations where it is not possible to generate random numbers directly from the joint
posterior.
The recipe for constructing a Gibbs sampler is as follows.
1. Find a suitable partition of the parameter vector into k subsets Î¸ = (Î¸â€²
1, . . . , Î¸â€²
k)â€²
2. Obtain the set of full conditional posterior distributions for the subvectors
p (Î¸i|YT, Î¸âˆ’i) , i = 1, . . . , k
3. Draw j + 1 from the Gibbs sampler is given by generating the subvectors from the
full conditional posteriors while updating the conditioning
Î¸(j+1)
1
âˆ¼p

Î¸1|YT, Î¸(j)
2 , . . . Î¸(j)
k

Î¸(j+1)
2
âˆ¼p

Î¸2|YT, Î¸(j+1)
1
, Î¸(j)
3 , . . . Î¸(j)
k

...
Î¸(j+1)
k
âˆ¼p

Î¸k|YT, Î¸(j+1)
1
, . . . Î¸(j+1)
kâˆ’1

.
It is easy to verify that the joint posterior distribution is a stationary distribution of
the Gibbs sampler. For the simple case with two subvectors Î¸ = (Î¸â€²
1, Î¸â€²
2)â€² the transition
kernel is f
 Î¸(j+1)|Î¸(j)
= p

Î¸(j+1)
2
|YT, Î¸(j+1)
1

p

Î¸(j+1)
1
|YT, Î¸(j)
. If Î¸(j) is a draw from
the posterior p (Î¸1, Î¸2|YT) = p (Î¸1|YT, Î¸2) p (Î¸2|YT) marginalizing out Î¸(j) from the joint
distribution of Î¸(j+1) and Î¸(j) yields the posterior distribution
Z Z
p

Î¸(j+1)
2
|YT, Î¸(j+1)
1

p

Î¸(j+1)
1
|YT, Î¸(j)
2

p

Î¸(j)
1 |YT, Î¸(j)
2

p

Î¸(j)
2 |YT

dÎ¸(j)
1 dÎ¸(j)
2
=
Z
p

Î¸(j+1)
2
|YT, Î¸(j+1)
1

p

Î¸(j+1)
1
|YT, Î¸(j)
2

p

Î¸(j)
2 |YT

dÎ¸(j)
2
= p

Î¸(j+1)
2
|YT, Î¸(j+1)
1

p

Î¸(j+1)
1
|YT

= p

Î¸(j+1)
1
, Î¸(j+1)
2
|YT

.
The Gibbs sampler is thus quite straightforward to implement and the form of the
sampler follows directly from the model when the full conditionals are well known distri-
butions. This makes it very appealing. There is, however, no guarantee that a naively
82

implemented Gibbs sampler will perform well or even that it is convergent. Reparame-
terizing the model or modifying the blocks (the partition into subvectors) to put highly
correlated parameters into the same block can often improve the performance and speed
of convergence dramatically.
A.2
Metropolis-Hastings
The Metropolis-Hastings algorithm is a more general method that does not rely on the
availability of tractable full conditionals. The basic idea is similar to acceptance-rejectance
sampling and importance sampling in that an approximation to the desired distribution is
used to generate a proposal for the next draw from the chain. The proposal is accepted or
rejected based on how well it agrees with the desired distribution and by a judicious choice
of the acceptance probability on can obtain a Markov chain with the desired distribution
as itâ€™s stationary distribution.
In Metropolis-Hastings the proposal distribution itself is allowed to be a Markov chain
and the proposed value for Î¸(j+1) can depend on the current value Î¸(j) through the con-
ditional distribution q
 x|Î¸(j)
. The algorithm is as follows
1. Draw a proposal x from the conditional distribution q
 x|Î¸(j)
.
2. Set Î¸(j+1) = x with probability
Î±
 Î¸(j), x

= min
 
1, p (x|YT) /q
 x|Î¸(j)
p (Î¸(j)|YT) /q (Î¸(j)|x)
!
(124)
and keep the old value otherwise, Î¸(j+1) = Î¸(j). That is, generate u from a uniform
distribution on (0, 1) and set Î¸(j+1) = x if u â‰¤Î± and Î¸(j+1) = Î¸(j) otherwise.
The transition kernel of the resulting Markov chain is given by the conditional distri-
bution
f
 Î¸(j+1)|Î¸(j)
=

q
 Î¸(j+1)|Î¸(j)
Î±
 Î¸(j), Î¸(j+1)
, Î¸(j+1) Ì¸= Î¸(j)
q
 Î¸(j)|Î¸(j)
+
R
xÌ¸=Î¸(j) q
 x|Î¸(j)  1 âˆ’Î±
 Î¸(j), x

dx, Î¸(j+1) = Î¸(j) .
That the Markov chain has the posterior as a stationary distribution can be checked by
verifying that the detailed balance condition
f
 Î¸(j)
p
 Î¸(j+1)|Î¸(j)
= f
 Î¸(j+1)
p
 Î¸(j)|Î¸(j+1)
holds for f (Â·) the posterior distribution. Note that the detailed balance condition implies
a form of symmetry, the chain moves from x to y at the same rate as it moves from y to
x.
Note that any constants cancel from the acceptance probability Î± and it can be written
in terms of the product of the likelihood and the prior instead of the, typically, unknown
joint posterior. That is
Î±
 Î¸(j), x

= min
 
1,
L (YT|x) Ï€ (x) /q
 x|Î¸(j)
L (YT|Î¸(j)) Ï€ (Î¸(j)) /q (Î¸(j)|x)
!
83

The choice of the proposal distribution q is crucial for the performance of the Markov
chain and it is important that it is well tailored to the posterior distribution. Examples
of common types of proposal chains are
â€¢ Independence chain: The proposal steps are drawn from a ï¬x density, q
 x|Î¸(j)
=
f (x) . It is important for the performance of the Markov chain that the proposal
distribution is well tailored to the posterior over the whole parameter space which
can be diï¬ƒcult with high dimensional parameter vectors. There are, on the other
hand, theoretical advantages, the resulting Metropolis chain is uniformly ergodic if
the weights p (x|YT) /q
 x|Î¸(j)
are bounded on the parameter space.
â€¢ Random walk chain: The proposal steps follow a random walk, x = Î¸(j) + e,
q
 x|Î¸(j)
= f
 x âˆ’Î¸(j)
where f is the density of e. The random walk chain makes
it possible to construct a proposal density that matches the posterior well locally,
but the proposal should not be to local or the chain will move very slowly through
the parameter space.
It is possible to divide the parameter vector into blocks, just as with the Gibbs sampler,
and update one block at a time with diï¬€erent proposal distributions for each block of
parameters. This makes it easier to adapt the proposal to the posterior and can make
for a better performing Markov chain. With a partition Î¸ = (Î¸â€²
1, . . . , Î¸â€²
k) and updating in
order, the update for block m is analogous to the update for the full parameter vector.
1. Propose xm from the proposal density qm

xm, Î¸(j+1)
1
, . . . , Î¸(j+1)
mâˆ’1 , Î¸(j)
m , . . . , Î¸(j)
k

.
2. Accept the proposal and set Î¸(j+1)
m
= xk with probability Î± given by (124) otherwise
set Î¸(j+1)
m
= Î¸(j)
m .
Note that the acceptance probability simpliï¬es and can be written in terms of the full
conditional posterior for Î¸m if this is available,
Î±
 Î¸(j), xm

= min
ï£«
ï£­1,
p

xm|YT, Î¸(j+1)
1
, . . . , Î¸(j+1)
mâˆ’1 , Î¸(j)
m+1, . . . , Î¸(j)
k

qm

xm|Î¸(j+1)
1
, . . . , Î¸(j+1)
mâˆ’1 , Î¸(j)
m , . . . , Î¸(j)
k

,p

Î¸(j)|YT, Î¸(j+1)
1
, . . . , Î¸(j+1)
mâˆ’1 , Î¸(j)
m+1, . . . , Î¸(j)
k

qm

Î¸(j)
m |Î¸(j+1)
1
, . . . , Î¸(j+1)
mâˆ’1 , x, Î¸(j)
m+1, . . . , Î¸(j)
k

ï£¶
ï£¸.
In this case the full conditional posterior is an excellent proposal density and with this
choice of qm the acceptance ratio simpliï¬es to one.
The Gibbs sampler is thus a special case of the Metropolis-Hastings algorithm and
we can use a mix of Metropolis-Hastings updates and Gibbs updates in the Markov
chain. Gibbs updates for the components with convenient full conditional posteriors and
Metropolis-Hastings for the other components. Although somewhat of a misnomer, this is
commonly known as a Metropolis-Hastings within Gibbs chain. In this context it is useful
to note that it is suï¬ƒcient for uniform ergodicity that one of the Metropolis-Hasting steps
uses an independence proposal with bounded weights
p

xm|YT, Î¸(j+1)
1
, . . . , Î¸(j+1)
mâˆ’1 , Î¸(j)
m+1, . . . , Î¸(j)
k

/qm (xm) .
84

A.3
Autocorrelation in the Markov chain
The output from a Markov chain is by construction autocorrelated and this aï¬€ects the
precision of estimates of posterior quantities sometimes to the point where they are close
to being unusable.
Ideally one would go back to the drawing board and construct a
Markov chain with lower autocorrelation that mixes well. This is, however, not always
possible and one must then be particularly careful in the choice of burn-in and make sure
that the Markov chain runs long enough to explore the full parameter space.
A common strategy in these situations is to thin the chain, i.e. to retain only every
mth draw from the chain where m is chosen to make the autocorrelation between Î¸(j) and
Î¸(j+m) negligible. Based on a sample of size R/m after discarding the burn-in we can then
estimate the posterior mean of a function g (Â·) of the parameters as
gR/m = m
R
R/m
X
i=1
g
 Î¸([iâˆ’1]m+1)
and an estimate of the numerical standard error is given by the square root of
bV
 gR/m

=
bV (g (Î¸) |YT)
R/m
.
This is a statistically ineï¬ƒcient procedure and it can be shown that V
 gR/m

â‰¥V (gR) .
On the other hand it might reduce the storage and memory requirements considerably.
If the chain is not thinned or when the thinning leaves some autocorrelation this must
be accounted for when estimating the numerical standard errors. Two common methods
is the batched mean method and the time series based spectral estimate. The batched
mean method divides the data into R/m batches, each containing m consecutive draws
from the Markov chain, and calculate the batch means
gm,j = 1
m
jm
X
i=(jâˆ’1)m+1
g
 Î¸(i)
.
For suï¬ƒciently large m the batch means will be essentially uncorrelated and we can
estimate the variance of the batch means as
bV (gm) =
1
R/m âˆ’1
R/m
X
j=1
 gm,j âˆ’gR
2 .
An estimate of the variance of gR is then given by
bV (gR) = m
R
bV (gm) =
m
R (R/m âˆ’1)
R/m
X
j=1
 gm,j âˆ’gR
2 .
The so called spectral estimate is simply the Newey and West (1987) autocorrelation
consistent estimator of the asymptotic variance (12). A common implementation is the
estimator
bV (gR) = 1
R
m
X
j=âˆ’m

1 âˆ’
|j|
m + 1

bÎ³j
85

with truncation at lag m and the autocovariances,
bÎ³j = 1
R
Râˆ’j
X
i=1

g
 Î¸(i)
âˆ’gR
 
g
 Î¸(i+j)
âˆ’gR

,
at larger lags are downweighted using a Bartlett kernel. For consistency the truncation
should go to inï¬nity with R, m = o
 R1/4
.
The autocorrelation will in general lead to a loss of eï¬ƒciency compared to the case
when we can generate iid draws from the posterior. It is common to measure the loss
with the relative numerical eï¬ƒciency (RNE)
RNE =
bV (g (Î¸)) /R
bV (g)
.
An alternative measure is the eï¬€ective sample size, the number of iid draws that would
give the same numerical standard error as the R draws we have from the sampler. This
is simply R times the RNE.
A.4
Assessing convergence
It should be clear from the discussion above that it can not be taken for granted that
a Gibbs or Metropolis-Hastings sampler converges to the desired posterior distribution.
Nor that, if the sampler is convergent, it does converge in a reasonable number of steps
and that the output can be used to compute reliable estimates of posterior quantities.
Trying to assess if the sampler fails to converge or not and the approximate number of
steps required to be â€close enoughâ€ to convergence is thus important. Even if convergence
can be proved for a particular sampler there is very little information about how quickly
it converges and an empirical assessment of the amount of burn-in needed must be made.
Unfortunately, the output from the chain only constitutes a sample and can not be used
to prove convergence â€“ all we can do is to look for signs of lack of convergence or slow
convergence.
Some of the most powerful diagnostics or indicators of problems are quite simple in
nature. High and persistent autocorrelation in the chain indicates slow mixing and slow
convergence to the posterior distribution. Is the posterior multimodal? If so, the chain
might get stuck at one of the modes if the probability mass connecting the modes is
small. Simple plots of the output, trace plots of the parameters, Î¸(j)
i , or some function
of the parameters, g
 Î¸(j)
, plots of running means, gt = 1
t
Pt
j=1 g
 Î¸(j)
, or CUSUMs,
St = Pt
j=1

g
 Î¸(j)
âˆ’gR

, can also be informative.
If the trace plot or the running
means settle down after a number of steps this can indicate a suitable amount of burn-
in. Similarly for the CUSUM plots. In addition Yu and Mykland (1998) argue that the
CUSUM plot can be informative about how well the sampler mixes, â€a good sampler
should have an oscillatory path plot and small excursions; or a bad sampler should have
a smooth path plot and large excursionsâ€.
Brooks (1998) proposed a formal test for deviations from the ideal case of iid output
from the sampler based on the CUSUM. First determine a suitable amount of burn-in,
B, based on preliminary plots of the output and calculate bÂµ = (R âˆ’B)âˆ’1 PR
j=B+1 g
 Î¸(j)
86

and St = Pt
j=B+1

g
 Î¸(j)
âˆ’bÂµ

for t = B + 1, . . . , R and g (Â·) some function of the
parameters. Next deï¬ne
dj =
 1 if Sjâˆ’1 > Sj and Sj < Sj+1 or Sjâˆ’1 < Sj and Sj > Sj+1
0 otherwise
where dj = 1 indicates non-smoothness or â€hairinessâ€ of the plot and dj = 0 indicates
smoothness. The running means Dt = (t âˆ’B âˆ’1)âˆ’1 Ptâˆ’1
j=B+1 dj for t = B + 2, . . . , R
lies between 0 and 1 and captures the overall behavior of the Markov chain. If we, in
addition to the iid assumption, assume that g
 Î¸(j)
is symmetric around the mean we
have P (dj = 1) = 1/2 and Dt is Binomially distributed. We can then plot Dt against
the bounds Â±ZÎ±/2
q
1
4(tâˆ’Bâˆ’1) and diagnose non-convergence if DT fails to lie within the
bounds 100 (1 âˆ’Î±) % of the time.
Geweke (1992) proposed monitoring convergence by the statistic
zG =
ga âˆ’gb
p
V (ga) + V (gb)
where g (Â·) is some function of the output of the chain and
ga = 1
na
m+na
X
j=m+1
g
 Î¸(j)
, gb = 1
nb
R
X
j=Râˆ’nb+1
g
 Î¸(j)
for a chain that is run R steps with R > na + nb + m and the distance between the
estimates such that they can be taken to be uncorrelated. The variances are estimated
taking account of the autocorrelation structure in the chain, for example by the spectral
estimate above. If the chain has converged after m steps the distribution of the draws
m + 1, . . . , m + na is the same as the distribution of the draws at the end of the chain
and zG approximately standard normal. Calculating zG for a range of values of m and
comparing to critical values from a standard normal will thus give an indication of the
burn in needed for the chain.
Gelman and Rubin (1992), proposed running several shorter chains started at points
that are overdispersed compared to the posterior. Let Î¸(j)
i
denote the output from chain i
for m chains run n = R âˆ’B steps from burn-in and deï¬ne the between and within chain
variances as
B =
n
m âˆ’1
m
X
i=1
(gi âˆ’g)2 ,
W =
1
m (n âˆ’1)
m
X
i=1
n
X
j=1

g

Î¸(j)
i

âˆ’gi
2
.
gi = 1
n
n
X
j=1
g

Î¸(j)
i

,
g = 1
m
m
X
i=1
gi
Convergence failure or convergence on diï¬€erent stationary distributions after the selected
burn-in is indicated by the between chain variation, B, being larger than the within chain
variation, W. If the chains have converged after B draws we have two unbiased estimates
of the variance, V = (1 âˆ’1/n) W + B/n and W. The ï¬rst tends to overestimate the
variance if convergence has not been achieved (the between chain variation is large) and
87

the latter tends to underestimate the variance (the chains have not had time to explore the
full parameter space). The convergence diagnostic is r =
p
V/W or a version including
a â€degree of freedomâ€ correction. Gelman (1996) suggested the rule of thumb to accept
convergence if r < 1.2 for all monitored quantities.
The Brooks and Geweke diagnostics and Gelman-Rubin diagnostics are quite diï¬€erent
in nature. The Brooks and Geweke diagnostics are based on a single long chain and will
fail to detect convergence failures caused by the chain being stuck at one of the modes of
a multimodal posterior. The Gelman-Rubin statistic, on the other hand, is more likely
to detect this type of problem but is much less informative about the amount of burn-in
needed.
B
State space models
Consider the linear state space model for the m observed variables in yt, t = 1, . . . , T,
yt = Ztst + Îµt, Îµt âˆ¼N (0, Ht)
(125)
st+1 = dt + Ttst + Î·t, Î·t âˆ¼N (0, Qt)
with the initial condition or prior on the ï¬rst state, s1 âˆ¼N
 s1|0, P1|0

. The n dimensional
state vectors st are unobserved and the matrices Zt, Ht, Tt and Qt are assumed known
for the purpose of the discussion here (they are in general functions of the data, unknown
parameters or simply known constants). The subscript t|s indicates a time t property
conditional on information up to time s, a superscript t indicates a sequence running
from 1 to t, e.g. si|j = E (si|yj) = E (si|y1, . . . , yj) .
General references on state space models include Harvey (1989) and Durbin and Koop-
man (2001), West and Harrison (1997) and Kim and Nelson (1999) provides a Bayesian
treatment and Giordini, Pitt and Kohn (2011) reviews Bayesian inference in general state
space models. The Kalman ï¬lter and smoothing algorithms given below are standard. The
version of the simulation smoother is due to Carter and Kohn (1994). There are many
variations on these algorithms, the ones given here are straightforward and intuitive but
not the most computationally eï¬ƒcient versions.
B.1
Kalman ï¬lter
The Kalman ï¬lter runs forward through the data and returns the means and variances of
the conditional distributions st|yt âˆ¼N
 st|t, Pt|t

and st+1|yt âˆ¼N
 st+1|t, Pt+1|t

,
vt = yt âˆ’Ztst|tâˆ’1
Ft = ZtPt|tâˆ’1Zâ€²
t + Ht
Kt = Pt|tâˆ’1Zâ€²
tFt
st|t = st|tâˆ’1 + Ktvt
Pt|t = Pt|tâˆ’1 âˆ’KtZtPt|tâˆ’1
st+1|t = dt + Ttst|t
Pt+1|t = TtPt|tTâ€²
t + Qt,
(126)
for t = 1, . . . , T.
B.2
Smoothing
At the end of the ï¬ltering run we have the distribution of the last state, sT|yT, conditional
on all the data but for the earlier states we only have the distribution conditional on a
88

Algorithm 18 Simulation Smoother
1. Generate sT from the conditional distribution, sT|yT âˆ¼N
 sT|T, PT|T

2. For t = T âˆ’1, . . . , 1
(a) Calculate
st|t,st+1 = st|t + Pt|tTâ€²
tPâˆ’1
t+1|t
 st+1 âˆ’st+1|t

Pt|t,st+1 = Pt|t âˆ’Pt|tTâ€²
tPâˆ’1
t+1|tTtPt|t
(b) Generate st from the conditional distribution st|yt, st+1 = st|yT, st+1 âˆ¼
N
 st|t,st+1, Pt|t,st+1

.
subset of the data and all information has not been used. The ï¬xed-interval smoother
runs backwards through the data and returns the means and variances of the conditional
distributions st|yT âˆ¼N
 st|T, Pt|T

,
st|T = st|t + Pt|tTâ€²
tPâˆ’1
t+1|t
 st+1|T âˆ’st+1|t

(127)
Pt|T = Pt|t âˆ’Pt|tTâ€²
tPâˆ’1
t+1|t
 Pt+1|Tâˆ’Pt+1|t

Pâˆ’1
t+1|tTtPt|t
for t = T âˆ’1, . . . , 1.
B.3
Simulation smoother
The simulation smoother is a device for generating random numbers from the joint dis-
tribution of the states conditional on the data, sT|yT. The output from the ï¬xed-interval
smoother can not be used for this since it carries no information about the dependence
between the states at diï¬€erent time points. The simulation smoother is based on the
partition
p
 s1, . . . , sT|yT
= p
 sT|yT tâˆ’1
Y
t=1
p
 st|yT, st+1

and generates a draw from the joint distribution by working backwards through the data
and generating st from the conditional distributions.
C
Distributions
Deï¬nition 1 (Gamma) x is Gamma distributed with shape parameter Î± and inverse
scale parameter Î², x âˆ¼G (Î±, Î²) if the density is
f (x) =
Î²Î±
Î“ (Î±)xÎ±âˆ’1 exp (âˆ’Î²x) .
We have E (x) = Î±/Î² and V (x) = Î±/Î²2.
89

Deï¬nition 2 (Inverse Gamma) y = xâˆ’1 is inverse Gamma distributed, y âˆ¼iG (Î±, Î²)
if x is Gamma distributed x âˆ¼G (Î±, Î²) . The density of y is
f (y) =
Î²Î±
Î“ (Î±)yâˆ’(Î±+1) exp

âˆ’Î²
y

with moments E (y) = Î²/ (Î± âˆ’1) and V (y) = Î²/

(Î± âˆ’1)2 (Î± âˆ’2)

.
Deï¬nition 3 (Matricvariate normal) The p Ã— q matrix X is said to have a matric-
variate normal distribution
X âˆ¼MNpq (M, Q, P)
where M is p Ã— q and P and Q are positive deï¬nite symmetric matrices of dimensions
p Ã— p and q Ã— q if vec (X) is multivariate normal
vec (X) âˆ¼N (vec (M) , Q âŠ—P) .
The density of X is
MNpq (X; M, Q, P)
= (2Ï€)âˆ’pq/2 |Q âŠ—P|âˆ’1/2
Ã— exp

âˆ’1
2 (vec (X) âˆ’vec (M))â€²  Qâˆ’1 âŠ—Pâˆ’1
(vec (X) âˆ’vec (M))

= (2Ï€)âˆ’pq/2 |Q|âˆ’p/2 |P|âˆ’q/2 exp

âˆ’1
2 tr

Qâˆ’1 (X âˆ’M)â€² Pâˆ’1 (X âˆ’M)

.
Remark 1 The deï¬ning feature of the matricvariate normal is the Kronecker structure
for the variance-covariance matrix. Q is proportional to the variance matrix of the rows
of. X and P is proportional to the variance matrix of the columns of X. The elements in
row i are correlated with the elements in row j if pij Ì¸= 0 and the elements in column i
are correlated with the elements in column j if qij Ì¸= 0.
Remark 2 Suppose that X âˆ¼MNpq (M, Q, P)
1. Xâ€² âˆ¼MNqp (Mâ€², P, Q) .
2. AXB âˆ¼MNkl (AMB, Bâ€²QB, APAâ€²) for A k Ã— p and B q Ã— l.
Algorithm 19 Matricvariate normal random number generator
To generate X
âˆ¼
MNpq (M, Q, P) calculate the Cholesky factors of Q and P,
Q = LLâ€², P = CCâ€², generate Y as a p Ã— q matrix of standard normals and calculate
X = M + CYLâ€² âˆ¼MNpq (M, Q, P) .
90

Deï¬nition 4 (Wishart) A q Ã— q positive semi deï¬nite symmetric matrix A is said to
have a Wishart distribution, A âˆ¼Wq (B,v) if itâ€™s density is given by
Wq (A; B, v) = kâˆ’1 |B|âˆ’v/2 |A|(vâˆ’qâˆ’1)/2 exp

âˆ’1
2 tr ABâˆ’1

for B a positive deï¬nite symmetric matrix and v â‰¥q degrees of freedom and
k = 2vq/2Ï€q(qâˆ’1)/4
qY
i=1
Î“ ((v + 1 âˆ’i) /2) .
E (A) = vB
V (aij) = v
 b2
ij + biibjj

for aij one of the q (q + 1) /2 distinct elements of A.
Remark 3 The Wishart distribution is a matricvariate generalization of the Ï‡2 distribu-
tion and arises frequently in multivariate analysis. If xi are iid. N (Âµ, Î£) q-dimensional
random vectors then Pn
i=1 (xi âˆ’Âµ) (xi âˆ’Âµ)â€² âˆ¼Wq (Î£, n) and Pn
i=1 (xi âˆ’x) (xi âˆ’x)â€² âˆ¼
W (Î£, n âˆ’1) . If A âˆ¼Wq (B, v) then PAPâ€² âˆ¼Wp (PBPâ€², v) for P a p Ã— q matrix of rank
p (p â‰¤q) .
Algorithm 20 Wishart random number generator
Wishart distributed matrices, A âˆ¼Wq (B, v) can be generated by brute force by ï¬rst
generating v vectors xi âˆ¼N (0, B) and forming A = Pv
i=1 xixâ€²
i. A more eï¬ƒcient algorithm
is based on the Bartlett decomposition of a Wishart matrix (Anderson (1984)) has been
proposed by Smith and Hocking (1972) and Geweke (1988).
Let P be a q Ã— q lower
triangular matrix where p2
ii âˆ¼Ï‡2
vâˆ’i+1 (i.e. pii is the square root of the Ï‡2) and pij âˆ¼
N (0, 1) , i < j, then PPâ€² âˆ¼Wq (I, v) . In addition let L be the lower triangular Cholesky
factor of B = LLâ€², then A = (LP) (LP)â€² âˆ¼Wq (B, v) . Note that in many cases it is
more convenient to work directly with the lower triangular matrix C = LP than A, e.g.
when the ultimate objective is to generate random numbers z âˆ¼N (Âµ, A) . First generate
xi âˆ¼N (0, 1) , i = 1, . . . , q and form z = Âµ + Cx.
In some cases it is more convenient with an upper triangular decomposition QQâ€² âˆ¼
Wq (I, v) . For this let q2
ii âˆ¼Ï‡2
vâˆ’q+i and qij âˆ¼N (0, 1) , i > j.
Deï¬nition 5 (Inverse Wishart) The qÃ—q matrix A is said to have an inverse Wishart
distribution,
A âˆ¼iWq (B, v)
if Aâˆ’1 âˆ¼W (Bâˆ’1, v) . The density of A is given by
iWq (A; B, v) = kâˆ’1 |B|v/2 |A|âˆ’(v+q+1)/2 exp

âˆ’1
2 tr Aâˆ’1B

91

Algorithm 21 Inverse Wishart random number generator
To generate A âˆ¼iWq (B,v) , ï¬rst generate the upper triangular Bartlett decomposition
matrix Q of a Wishart distributed, Wq (I,v) , matrix. Second calculate the lower triangular
Cholesky decomposition, LLâ€² = B, we then have Lâˆ’TLâˆ’1 = Bâˆ’1 and Lâˆ’TQQâ€²Lâˆ’1 âˆ¼
Wq (Bâˆ’1, v) . Let C = LQâˆ’T and we have A = CCâ€² âˆ¼iWq (B, v) for C lower triangular.
Sometimes A âˆ¼iWq (Dâˆ’1, v) is needed. The inversion of D can be avoided by letting
L be the Cholesky decomposition of D, LLâ€² = D, generate the lower triangular Bartlett
decomposition matrix P and let C be the upper triangular matrix C = (LP)âˆ’T for
A = CCâ€² âˆ¼iWq (Dâˆ’1, v)
with k as for the Wishart distribution.
E (A) =
1
v âˆ’q âˆ’1B, v > q + 1
V (aij) = (v âˆ’q âˆ’1) biibjj + (v âˆ’q + 1) b2
ij
(v âˆ’q âˆ’1)2 (v âˆ’q) (v âˆ’q âˆ’3)
, v > q + 3
Deï¬nition 6 (normal-Wishart) If X|Î£ âˆ¼MNpq (M, Î£, P) and Î£ âˆ¼iWq (Q, v) then
the joint distribution of X and Î£ is said to be normal-Wishart with kernel
p (X, Î£) âˆ|Î£|âˆ’(v+p+q+1)/2 exp

âˆ’1
2 tr

Î£âˆ’1 (X âˆ’M)â€² Pâˆ’1 (X âˆ’M)

exp

âˆ’1
2 tr Î£âˆ’1B

Algorithm 22 Normal-Wishart random number generator
To generate X|Î£ âˆ¼MNpq (M, Î£, A) and Î£ âˆ¼iWq (B, v) ï¬rst generate the triangular
factor C of an inverse Wishart and calculate Î£ = CCâ€² (if needed). Second generate Y as
a p Ã— q matrix of standard normals and form X = M + LYCâ€² âˆ¼MNpq (M, Î£, A) for L
the Cholesky factor of A = LLâ€².
Deï¬nition 7 (Matricvariate t) A random pÃ—q matrix X is said to have a matricvari-
ate t distribution if the density is given by
Mtpq (X; M, P, Q,v) = kâˆ’1 Q+ (X âˆ’M)â€² P (X âˆ’M)
âˆ’(v+p)/2
for M a p Ã— q mean matrix, Q and P, q Ã— q and p Ã— p positive deï¬nite symmetric scale
matrices and v â‰¥q degrees of freedom. The integrating constant is given by
k = Ï€pq/2 |P|âˆ’q/2 |Q|âˆ’v/2
qY
i=1
Î“ ((v + 1 âˆ’i) /2)
Î“ ((v + p + 1 âˆ’i) /2).
We have
E (X) = M,v > q
V (vec X) =
1
v âˆ’q âˆ’1Q âŠ—Pâˆ’1, v > q + 1.
92

Remark 4 If X|Î£ âˆ¼MNpq (M, Î£, P) and Î£ âˆ¼iWq (Q, v) (normal-Wishart) then the
marginal distribution of X is matricvariate t.
X âˆ¼Mtpq
 M, Pâˆ’1, Q,v

.
It follows that Algorithm 22 also is a matricvariate t random number generator where the
draws of C or Î£ are simply discarded.
In addition, the distribution of Î£ conditional on X is inverse Wishart
Î£|X âˆ¼iWq
 Q+ (X âˆ’M)â€² Pâˆ’1 (X âˆ’M) , v + p

.
93

References
Adolfson, M., Andersson, M. K., Linde, J., Villani, M. and Vredin, A. (2007), â€˜Modern
forecasting models in action: Improving macroeconomic analyses at central banksâ€™,
International Journal of Central Banking 3, 111â€“144.
Adolfson, M., Lasen, S., Lind, J. and Villani, M. (2008), â€˜Evaluating an estimated new
keynesian small open economy modelâ€™, Journal of Economic Dynamics and Control
32(8), 2690â€“2721.
Anderson, T. W. (1984), An Introduction to Multivariate Statistical Analysis, 2nd edn,
John Wiley & Sons, New York.
Andersson, M. K., Palmqvist, S. and Waggoner, D. F. (2010), Density conditional fore-
casts in dynamic multivariate models, Working Paper Series 247, Sveriges Riksbank.
Andersson, M. and Karlsson, S. (2009), Bayesian forecast combination for VAR models,
in Chib, Koop and Griï¬ƒths (2008), pp. 501â€“524.
Banbura, M., Giannone, D. and Reichlin, L. (2010), â€˜Large bayesian vector auto regres-
sionsâ€™, Journal of Applied Econometrics 25, 71â€“92.
Barbieri, M. M. and Berger, J. O. (2004), â€˜Optimal predictive model selectionâ€™, The Annals
of Statistics 32(3), 870â€“897.
Bauwens, L. and Lubrano, M. (1996), Identiï¬cation restrictions and posterior densities in
cointegrated gaussian var systems, in T. B. Fomby and R. C. Hill, eds, â€˜Advances in
Econometricsâ€™, Vol. 11B, JAI Press.
Beechey, M. and Â¨Osterholm, P. (2010), â€˜Forecasting inï¬‚ation in an inï¬‚ation-targeting
regime: A role for informative steady-state priorsâ€™, International Journal of Fore-
casting 26(2), 248â€“264.
Bernanke, B., Boivin, J. and Eliasz, P. (2005), â€˜Measuring the eï¬€ect of monetary policy:
a factor augmented vector autoregressive (FAVAR) approachâ€™, Quarterly Journal of
Economics 120, 387â€“422.
Bloor, C. and Matheson, T. (2010), â€˜Analysing shock transmission in a data-rich environ-
ment: a large bvar for new zealandâ€™, Empirical Economics 39, 537â€“558.
Bloor, C. and Matheson, T. D. (2011), â€˜Real-time conditional forecasts with bayesian
vars: An application to new zealandâ€™, The North American Journal of Economics
and Finance 22(1), 26â€“42.
Brooks, S. P. (1998), â€˜Quantitative convergence assessment for markov chain monte carlo
via cusumsâ€™, Statistics and Computing 8, 267â€“274.
Brown, P. J., Vanucci, M. and Fearn, T. (1998), â€˜Multivariate bayesian variable selection
and predictionâ€™, Journal of the Royal Statistical Society, Ser. B 60, 627â€“641.
Canova, F. (2007), â€˜G-7 inï¬‚ation forecasts: Random walk, phillips curve or what else?â€™,
Macroeconomic Dynamics 11(01), 1â€“30.
94

Canova, F. and Ciccarelli, M. (2004), â€˜Forecasting and turning point predictions in a
bayesian panel var modelâ€™, Journal of Econometrics 120, 327â€“359.
Canova, F. and Ciccarelli, M. (2009), â€˜Estimating multicountry var modelsâ€™, International
Economic Review 50(3), 929â€“959.
Carriero, A., Clark, T. and Marcellino, M. (2011), Bayesian vars: speciï¬cation choices
and forecast accuracy, Working Paper 1112, Federal Reserve Bank of Cleveland.
Carriero, A., Kapetanios, G. and Marcellino, M. (2009), â€˜Forecasting exchange rates with
a large bayesian varâ€™, International Journal of Forecasting 25(2), 400â€“417.
Carriero, A., Kapetanios, G. and Marcellino, M. (2011), â€˜Forecasting large datasets
with bayesian reduced rank multivariate modelsâ€™, Journal of Applied Econometrics
26, 735â€“761.
Carriero, A., Kapetanios, G. and Marcellino, M. (2012), â€˜Forecasting government bond
yields with large bayesian vector autoregressionsâ€™, Journal of Banking &amp; Finance
36(7), 2026 â€“ 2047.
Carter, C. K. and Kohn, R. (1994), â€˜On gibbs sampling for state space modelsâ€™, Biometrika
81(3), pp. 541â€“553.
Chib, S. (1995), â€˜Marginal likelihood from the Gibbs outputâ€™, Journal of the American
Statistical Association 90, 1313â€“1321.
Chib, S. (1998), â€˜Estimation and comparison of multiple change point modelsâ€™, Journal
of Econometrics 86, 221â€“241.
Chib, S. and Greenberg, E. (1995), â€˜Understanding the Metropolis-Hastings algorithmâ€™,
American Statistician 40, 327â€“335.
Chib, S., Koop, G. and Griï¬ƒths, B., eds (2008), Bayesian Econometrics, Vol. 23 of
Advances in Econometrics, Emerald.
Clark, T. E. (2011), â€˜Real-time density forecasts from bayesian vector autoregressions with
stochastic volatilityâ€™, Journal of Business & Economic Statistics 29(3), 327â€“341.
Clark, T. E. and McCracken, M. W. (2010), â€˜Averaging forecasts from VARs with uncer-
tain instabilitiesâ€™, Journal of Applied Econometrics 25, 5â€“29.
Clements, M. P. and Hendry, D. F., eds (2011), The Oxford Handbook of Economic Fore-
casting, Oxford University Press.
Cogley, T., Morozov, S. and Sargent, T. J. (2005), â€˜Bayesian fan charts for u.k. inï¬‚ation:
Forecasting and sources of uncertainty in an evolving monetary systemâ€™, Journal of
Economic Dynamics and Control 29(11), 1893 â€“ 1925.
Cogley, T. and Sargent, T. J. (2002), Evolving post-world war ii u.s. inï¬‚ation dynamics, in
B. S. Bernanke and K. S. Rogoï¬€, eds, â€˜NBER Macroeconomics Annual 2001, Volume
16â€™, National Bureau of Economic Research, Inc, pp. 331â€“388.
95

Cogley, T. and Sargent, T. J. (2005), â€˜Drifts and volatilities: monetary policies and
outcomes in the post wwii usâ€™, Review of Economic Dynamics 8, 262â€“302.
Corradi, V. and Swanson, N. R. (2006), Predictive density evaluation, in Elliott, Granger
and Timmermann (2006), pp. 197 â€“ 284.
Dâ€™Agostino, A., Gambetti, L. and Giannone, D. (forthcoming), â€˜Macroeconomic forecast-
ing and structural changeâ€™, Journal of Applied Econometrics .
De Mol, C., Giannone, D. and Reichlin, L. (2008), â€˜Forecasting using a large number
of predictors: is bayesian regression a valid alternative to principal components?â€™,
Journal of Econometrics 146, 318â€“328.
DeJong, D. N. (1992), â€˜Co-integration and trend-stationarity in macroeconomic time se-
ries: Evidence from the likelihood functionâ€™, Journal of Econometrics 52(3), 347â€“370.
DelNegro, M. and Schorfheide, F. (2011), Bayesian methods in microeconometrics, in
Clements and Hendry (2011), chapter 7, pp. 293â€“389.
Dickey, J. M. (1971), â€˜The weighted likelihood ratio, linear hypothesis on normal location
parametersâ€™, The Annals of Mathematical Statistics 42, 204â€“223.
Doan, T., Litterman, R. B. and Sims, C. (1984), â€˜Forecasting and conditional projection
using realistic prior distributionsâ€™, Econometric Reviews 3, 1â€“144.
Dorfman, J. H. (1995), â€˜A numerical bayesian test for cointegration of ar processesâ€™,
Journal of Econometrics 66, 289â€“324.
Dr`eze, J. H. and Morales, J. A. (1976), â€˜Bayesian full information analysis of simultaneous
equationsâ€™, Journal of the American Statistical Association 71, 919â€“923.
Durbin, J. and Koopman, S. J. (2001), Time Series Analysis by State Space Methods,
Oxford University Press.
Elliott, G., Granger, C. W. J. and Timmermann, A., eds (2006), Handbook of Economic
Forecasting, Vol. 1, Elsevier.
Forni, M., Hallin, M., Lippi, M. and Reichlin, L. (2003), â€˜Do ï¬nancial variables help fore-
casting inï¬‚ation and real activity in the euro area?â€™, Journal of Monetary Economics
50, 1243â€“1255.
Gamerman, D. (1997), Markov Chain Monte Carlo: Stochastic Simulation for Bayesian
Inference, Chapman & Hall.
Gelman, A. (1996), Inference and monitoring convergence, in W. R. Gilks, S. Richardson
and D. J. Spiegelhalter, eds, â€˜Markov Chain Monte Carlo in Practiceâ€™, Chapman and
Hall, chapter 8, pp. 131â€“143.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (2003), Bayesian Data Analysis,
2 edn, Chapman and Hall/CRC.
96

Gelman, A. and Rubin, D. B. (1992), â€˜Inference from iterative simulation using multiple
sequencesâ€™, Statistical Science 7, 457â€“511. with discussion.
George, E. I. and McCulloch, R. E. (1993), â€˜Variable selection via gibbs samplingâ€™, Journal
of the American Statistical Association 88, 881â€“889.
George, E. I., Sun, D. and Ni, S. (2008), â€˜Bayesian stochastic search for var model restric-
tionsâ€™, Journal of Econometrics 142, 553â€“580.
Geweke, J. (1988), â€˜Antithetic acceleration of monte carlo integration in bayesian infer-
enceâ€™, Journal of Econometrics 38, 73â€“89.
Geweke, J. (1989), â€˜Bayesian inference in econometric models using monte carlo integra-
tionâ€™, Econometrica 57, 1317â€“1339.
Geweke, J. (1992), Evaluating the accuracy of sampling-based approaches to the calcula-
tion of posterior moments, in J. M. Bernardo, J. O. Berger, A. P. David and A. F. M.
Smith, eds, â€˜Bayesian Statistics 4â€™, Clarendon Press, pp. 169â€“193.
Geweke, J. (1996a), â€˜Bayesian reduced rank regression in econometricsâ€™, Journal of Econo-
metrics 75, 121â€“146.
Geweke, J. (1996b), Variable selection and model comparison in regression, in J. M.
Bernardo, J. O. Berger, A. P. David and A. F. M. Smith, eds, â€˜Bayesian Statisticsâ€™,
Vol. 5, Oxford University Press, pp. 609â€“620.
Geweke, J. (1999), â€˜Using simulation methods for bayesian econometric models: Inference,
development and communicationâ€™, Econometric Reviews 18, 1â€“126. with discussion.
Geweke,
J. (2005),
Contemporary Bayesian Econometrics and Statistics,
Wiley-
Interscience.
Geweke, J. and Whiteman, C. H. (2006), Bayesian forecasting, in Elliott et al. (2006),
chapter 1, pp. 3â€“80.
Giannone, D., Lenza, M. and Primiceri, G. E. (2012), Prior selection for vector autore-
gressions, Working Papers ECARES ECARES 2012-002, ULB â€“ Universite Libre de
Bruxelles.
Giordini, P., Pitt, M. and Kohn, R. (2011), Bayesian inference for time series state space
models, in Clements and Hendry (2011), chapter 3, pp. 61â€“124.
Gupta, R. and Kabundi, A. (2010), â€˜Forecasting macroeconomic variables in a small
open economy: A comparison between small- and large-scale modelâ€™, Journal of
Forecasting 29, 168â€“186.
Harvey, A. C. (1989), Forecasting, Structural Time Series Models and the Kalman Filter,
Cambridge University Press.
Highï¬eld, R. A. (1987), Forecasting with Bayesian State Space Models, PhD thesis, Grad-
uate School of Business, University of Chicago.
97

JarociÂ´nski, M. (2010), â€˜Conditional forecasts and uncertainty about forecast revisions in
vector autoregressionsâ€™, Economics Letters 108(3), 257 â€“ 259.
JarociÂ´nski, M. and MaÂ´ckowiak, B. (2011), Choice of variables in vector autoregressions.
Manuscript.
Jochmann, M., Koop, G. and Strachan, R. (2010), â€˜Bayesian forecasting using stochas-
tic search variable selection in a var subject to breaksâ€™, International Journal of
Forecasting 26(2), 326â€“347.
Kadiyala, K. R. and Karlsson, S. (1993), â€˜Forecasting with generalized bayesian vector
autoregressionsâ€™, Journal of Forecasting 12, 365â€“378.
Kadiyala, K. R. and Karlsson, S. (1997), â€˜Numerical methods for estimation and inference
in bayesian var-modelsâ€™, Journal of Applied Econometrics 12, 99â€“132.
Karlsson, S. (2012), Conditional posteriors for the reduced rank regression model, Working
Papers 2012:11, Â¨Orebro University Business School.
Kim, C. and Nelson, C. R. (1999), State Space Models with Regime Switching, MIT Press.
Kim, S., Shephard, N. and Chib, S. (1998), â€˜Stochastic volatility: Likelihood inference
and comparison with arch modelsâ€™, The Review of Economic Studies 65(3), 361â€“393.
Kleibergen, F. and van Dijk, H. K. (1994), â€˜On the shape of the likelihood/posterior in
cointegration modelsâ€™, Econometric Theory 1, 514â€“551.
Kloek, T. and van Dijk, H. K. (1978), â€˜Bayesian estimates of equation system parameters:
An application of integration by monte carloâ€™, Econometrica 46, 1â€“19.
Koop, G. (2003), Bayesian Econometrics, John Wiley & Sons, Chichester.
Koop, G. (2010), Forecasting with medium and large bayesian vars, Technical Report WP
10-43, The Rimini Centre for Economic Analysis.
Koop, G. and Korobilis, D. (2009), â€˜Bayesian multivariate time series methods for empir-
ical macroeconomicsâ€™, Foundations and Trends in Econometrics 3, 267â€“358.
Koop, G., LeÂ´on-GonzÂ´alez, R. and Strachan, R. W. (2010), â€˜Eï¬ƒcient posterior simulation
for cointegrated models with priors on the cointegration spaceâ€™, Econometric Reviews
29, 224â€“242.
Koop, G. and Potter, S. (2007), â€˜Estimation and forecasting in models with multiple
breaksâ€™, Review of Economic Studies 74, 763â€“789.
Koop, G., Strachan, R. W., van Dijk, H. K. and Villani, M. (2006), Bayesian approaches to
cointegration, in T. C. Mills and P. K., eds, â€˜The Palgrave Handbook of Theoretical
Econometricsâ€™, Vol. 1, Palgrave McMillan, chapter 25.
Korobilis, D. (2008), Forecasting in vector autoregressions with many predictors, in Chib
et al. (2008), pp. 403â€“431.
98

Korobilis, D. (forthcominga), â€˜Hierarchical shrinkage priors for dynamic regressions with
many predictorsâ€™, International Journal of Forecasting .
Korobilis, D. (forthcomingb), â€˜Var forecasting using bayesian variable selectionâ€™, Journal
of Applied Econometrics .
Litterman, R. B. (1979), Techniques of forecasting using vector autoregressions, Working
Paper 115, Federal Reserve Bank of Minneapolis.
Litterman, R. B. (1980), A bayesian procedure for forecasting with vector autoregressions,
mimeo, Massachusetts Institute of Technology.
Litterman, R. B. (1986), â€˜Forecasting with bayesian vector autoregressions - ï¬ve years of
experienceâ€™, Journal of Business & Economic Statistics 4, 25â€“38.
LÂ¨utkepohl, H. (2006), Forecasting with VARMA models, in Elliott et al. (2006), chapter 6,
pp. 287â€“325.
Madigan, D. and York, J. (1995), â€˜Bayesian graphical models for discrete dataâ€™, Interna-
tional Statistical Review 63, 215â€“232.
McNees, S. K. (1986), â€˜Forecasting accuracy of alternative techniques: A comparison of
u.s. macroeconomic forecastsâ€™, Journal of Business & Economic Statistics 4, 5â€“15.
Newey, W. K. and West, K. D. (1987), â€˜A simple, positive semi-deï¬nite, heteroskedasticity
and autocorrelation consistent covariance matrixâ€™, Econometrica 55, 703â€“708.
Â¨Osterholm, P. (2008a), â€˜Can forecasting performance be improved by considering the
steady state? an application to swedish inï¬‚ation and interest rateâ€™, Journal of Fore-
casting 27(1), 41â€“51.
Â¨Osterholm, P. (2008b), â€˜A structural bayesian var for model-based fan chartsâ€™, Applied
Economics 40(12), 1557â€“1569.
Pesaran, M. H., Petenuzzo, D. and Timmermann, A. (2006), â€˜Forecasting time series
subject to multiple structural breaksâ€™, Review of Economic Studies 73, 1057â€“1084.
Peters, G. W., Kannan, B., Lassock, B. and Mellen, C. (2010), â€˜Model selection and
adaptive markov chain monte carlo for bayesian cointegrated var-modelsâ€™, Bayesian
Analysis 5, 465â€“492.
Primiceri, G. E. (2005), â€˜Time varying structural vector autoregressions and monetary
policyâ€™, The Review of Economic Studies 72(3), 821â€“852.
Robert, C. P. and Casella, G. (1999), Monte Carlo Statistical Methods, Springer Verlag.
Robertson, J. C., Tallman, E. W. and Whiteman, C. H. (2005), â€˜Forecasting using relative
entropyâ€™, Journal of Money, Credit and Banking 37(3), 383â€“401.
Rothenberg, T. J. (1971), â€˜Identiï¬cation in parametric modelsâ€™, Econometrica 39, 577â€“
599.
99

Rubio-Ramirez, J. F., Waggoner, D. F. and Zha, T. (2010), â€˜Structural vector autoregres-
sions: Theory of identiï¬cation and algorithms for inferenceâ€™, The Review of Economic
Studies 77, 665â€“696.
Schwarz, G. (1978), â€˜Estimating the dimension of a modelâ€™, The Annals of Statistics
6, 461â€“464.
Sims, C. A. (1980), â€˜Macroeconomics and realityâ€™, Econometrica 48, 1â€“48.
Sims, C. A. (1993), A nine-variable probabalistic macroeconomic forecasting model, in
J. H. Stock and M. W. Watson, eds, â€˜Business Cycles, Indicators and Forecastingâ€™,
University of Chicago Press, pp. 179â€“204.
Sims, C. A. and Zha, T. (1998), â€˜Bayesian methods for dynamic multivariate modelsâ€™,
International Econom Review 39, 949â€“968.
Smith, W. B. and Hocking, R. R. (1972), â€˜Algorithm as 53: Wishart variate generatorâ€™,
Journal of the Royal Statistical Society. Series C (Applied Statistics) 21, 341â€“345.
Stock, J. H. and Watson, M. W. (2002), â€˜Macroeconomic forecasting using diï¬€usion in-
dexesâ€™, Journal of Business & Economic Statistics 20, 147â€“162.
Stock, J. H. and Watson, M. W. (2006), Forecasting with many predictors, in Elliott et al.
(2006), chapter 10.
Strachan, R. (2003), â€˜Valid bayesian estimation of the cointegrating error correction
modelâ€™, Journal of Business & Economic Statistics 21(1), 185â€“95.
Strachan, R. and Inder, B. (2004), â€˜Bayesian analysis of the error correction modelâ€™,
Journal of Econometrics 123(2), 307â€“325.
Sugita, K. (2002), Testing for cointegration rank using bayes factors, Warwick Economic
Research Papers 654, University of Warwick.
Sugita, K. (2009), â€˜A monte carlo comparison of bayesian testing for cointegration rankâ€™,
Economics Bulletin 29(3), 2145â€“2151.
Theil, H. and Goldberger, A. S. (1960), â€˜On pure and mixed statistical estimation in
economicsâ€™, International Economic Review 2, 65â€“78.
Tierny, L. (1994), â€˜Markov chains for exploring posterior distributionsâ€™, The Annals of
Statistics 22, 1701â€“1762. with discussion.
Timmermann, A. (2006), Forecast combinations, in Elliott et al. (2006), chapter 4.
Verdinelli, I. and Wasserman, L. (1995), â€˜Computing bayes factors using a generalization
of the savage-dickey density ratioâ€™, Journal of the American Statistical Association
90(430), pp. 614â€“618.
Villani, M. (2000), Aspects of Bayesian Cointegration, PhD thesis, Stockholm University.
100

Villani, M. (2001), â€˜Bayesian prediction with cointegrated vector autoregressionsâ€™, Inter-
national Journal of Forecasting 17, 585â€“605.
Villani, M. (2005), â€˜Bayesian reference analysis of cointegrationâ€™, Economtric Theory
21, 326â€“357.
Villani, M. (2009), â€˜Steady state priors for vector autoregressionsâ€™, Journal of Applied
Econometrics 24, 630â€“650.
Waggoner, D. F. and Zha, T. (1999), â€˜Conditional forecasts in dynamic multivariate
modelsâ€™, The Review of Economics and Statistics 81, 639â€“651.
Waggoner, D. F. and Zha, T. (2003a), â€˜A Gibbs sampler for structural vector autoregres-
sionsâ€™, Journal of Economic Dynamics & Control 28, 349â€“366.
Waggoner, D. F. and Zha, T. (2003b), â€˜Likelihood preserving normalization in multiple
equation modelsâ€™, Journal of Econometrics 114(2), 329â€“347.
West, M. and Harrison, P. (1997), Bayesian Forecasting and Dynamic Models, 2nd ed.
edn, Springer.
Wright, J. H. (2010), Evaluating real-time var forecasts with an informative democratic
prior, Working Papers 10-19, Federal Reserve Bank of Philadelphia.
Yu, B. and Mykland, P. (1998), â€˜Looking at Markov samplers through cusum path plots.
a simple diagnostic ideaâ€™, Statistics and Computing 8, 275â€“286.
Zellner, A. (1971), An Introduction to Bayesian Inference in Econometrics, John Wiley
& Sons.
Zha, T. (1999), â€˜Block recursion and structural vector autoregressionsâ€™, Journal of Econo-
metrics 90(2), 291â€“316.
101

