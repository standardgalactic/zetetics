4
Learning, Regret minimization, and Equilibria
A. Blum and Y. Mansour
Abstract
Many situations involve repeatedly making decisions in an uncertain envi-
ronment: for instance, deciding what route to drive to work each day, or
repeated play of a game against an opponent with an unknown strategy. In
this chapter we describe learning algorithms with strong guarantees for set-
tings of this type, along with connections to game-theoretic equilibria when
all players in a system are simultaneously adapting in such a manner.
We begin by presenting algorithms for repeated play of a matrix game with
the guarantee that against any opponent, they will perform nearly as well as
the best Ô¨Åxed action in hindsight (also called the problem of combining expert
advice or minimizing external regret). In a zero-sum game, such algorithms
are guaranteed to approach or exceed the minimax value of the game, and
even provide a simple proof of the minimax theorem.
We then turn to
algorithms that minimize an even stronger form of regret, known as internal
or swap regret. We present a general reduction showing how to convert any
algorithm for minimizing external regret to one that minimizes this stronger
form of regret as well. Internal regret is important because when all players
in a game minimize this stronger type of regret, the empirical distribution
of play is known to converge to correlated equilibrium.
The third part of this chapter explains a diÔ¨Äerent reduction: how to con-
vert from the full information setting in which the action chosen by the
opponent is revealed after each time step, to the partial information (ban-
dit) setting, where at each time step only the payoÔ¨Äof the selected action
is observed (such as in routing), and still maintain a small external regret.
Finally, we end by discussing routing games in the Wardrop model, where
one can show that if all participants minimize their own external regret, then
4

Learning, Regret minimization, and Equilibria
5
overall traÔ¨Éc is guaranteed to converge to an approximate Nash Equilibrium.
This further motivates price-of-anarchy results.
4.1 Introduction
In this chapter we consider the problem of repeatedly making decisions in an
uncertain environment. The basic setting is we have a space of N actions,
such as what route to use to drive to work, or the rows of a matrix game like
{rock, paper, scissors}. At each time step, the algorithm probabilistically
chooses an action (say, selecting what route to take), the environment makes
its ‚Äúmove‚Äù (setting the road congestions on that day), and the algorithm
then incurs the loss for its action chosen (how long its route took). The
process then repeats the next day. What we would like are adaptive algo-
rithms that can perform well in such settings, as well as to understand the
dynamics of the system when there are multiple players, all adjusting their
behavior in such a way.
A key technique for analyzing problems of this sort is known as regret
analysis. The motivation behind regret analysis can be viewed as the fol-
lowing: we design a sophisticated online algorithm that deals with various
issues of uncertainty and decision making, and sell it to a client. Our algo-
rithm runs for some time and incurs a certain loss. We would like to avoid
the embarrassment that our client will come back to us and claim that in
retrospect we could have incurred a much lower loss if we used his simple
alternative policy œÄ. The regret of our online algorithm is the diÔ¨Äerence
between the loss of our algorithm and the loss using œÄ.
DiÔ¨Äerent notions of regret quantify diÔ¨Äerently what is considered to be
a ‚Äúsimple‚Äù alternative policy. External regret, also called the problem of
combining expert advice, compares performance to the best single action in
retrospect. This implies that the simple alternative policy performs the same
action in all time steps, which indeed is quite simple. Nonetheless, exter-
nal regret provides a general methodology for developing online algorithms
whose performance matches that of an optimal static oÔ¨Ñine algorithm by
modeling the possible static solutions as diÔ¨Äerent actions. In the context of
machine learning, algorithms with good external regret bounds can be pow-
erful tools for achieving performance comparable to the optimal prediction
rule from some large class of hypotheses.
In Section 4.3 we describe several algorithms with particularly strong
external regret bounds.
We start with the very weak greedy algorithm,
and build up to an algorithm whose loss is at most O(‚àöT log N) greater
than that of the best action, where T is the number of time steps. That is,

6
A. Blum and Y. Mansour
the regret per time step drops as O(
p
(log N)/T ). In Section 4.4 we show
that in a zero-sum game, such algorithms are guaranteed to approach or
exceed the value of the game, and even yield a simple proof of the minimax
theorem.
A second category of alternative policies are those that consider the online
sequence of actions and suggest a simple modiÔ¨Åcation to it, such as ‚Äúevery
time you bought IBM, you should have bought Microsoft instead‚Äù. While
one can study very general classes of modiÔ¨Åcation rules, the most common
form, known as internal or swap regret, allows one to modify the online
action sequence by changing every occurrence of a given action i by an
alternative action j.
(The distinction between internal and swap regret
is that internal regret allows only one action to be replaced by another,
whereas swap regret allows any mapping from {1, . . . , N} to {1, . . . , N} and
can be up to a factor N larger). In Section 4.5 we present a simple way to
eÔ¨Éciently convert any external regret minimizing algorithm into one that
minimizes swap regret with only a factor N increase in the regret term.
Using the results for external regret this achieves a swap regret bound of
O(‚àöTN log N). (Algorithms for swap regret have also been developed from
Ô¨Årst principles ‚Äî see the Notes section of this chapter for references ‚Äî but
this procedure gives the best bounds known for eÔ¨Écient algorithms).
The importance of swap regret is due to its tight connection to correlated
equilibria, deÔ¨Åned in Chapter 1. In fact, one way to think of a correlated
equilibrium is that it is a distribution Q over the joint action space such
that every player would have zero internal (or swap) regret when playing it.
As we point out in Section 4.4, if each player can achieve swap regret œµT,
then the empirical distribution of the joint actions of the players will be an
œµ-correlated equilibrium.
We also describe how external regret results can be extended to the partial
information model, also called the multi-armed bandit (MAB) problem. In
this model, the online algorithm only gets to observe the loss of the action
actually selected, and does not see the losses of the actions not chosen. For
example, in the case of driving to work, you may only observe the travel time
on the route you actually drive, and do not get to Ô¨Ånd out how long it would
have taken had you chosen some alternative route. In Section 4.6 we present
a general reduction, showing how to convert an algorithm with low external
regret in the full information model to one for the partial information model
(though the bounds produced not the best known bounds for this problem).
Notice that the route-choosing problem can be viewed as a general-sum
game: your travel time depends on the choices of the other drivers as well.
In Section 4.7 we discuss results showing that in the Wardrop model of

Learning, Regret minimization, and Equilibria
7
inÔ¨Ånitesimal agents (considered in Chapter 18), if each driver acts to mini-
mize external regret, then traÔ¨Éc Ô¨Çow over time can be shown to approach
an approximate Nash equilibrium. This serves to further motivate price-of-
anarchy results in this context, since it means they apply to the case that
participants are using well-motivated self-interested adaptive behavior.
We remark that the results we present in this chapter are not always the
strongest known, and the interested reader is referred to the recent book
[CBL06] which gives a thorough coverage of many of the the topics in this
chapter. See also the Notes section for further references.
4.2 Model and Preliminaries
We assume an adversarial online model where there are N available actions
X = {1, . . . , N}.
At each time step t, an online algorithm H selects a
distribution pt over the N actions. After that, the adversary selects a loss
vector ‚Ñìt ‚àà[0, 1]N, where ‚Ñìt
i ‚àà[0, 1] is the loss of the i-th action at time t. In
the full information model, the online algorithm H receives the loss vector ‚Ñìt
and experiences a loss ‚Ñìt
H = PN
i=1 pt
i‚Ñìt
i. (This can be viewed as an expected
loss when the online algorithm selects action i ‚ààX with probability pt
i.) In
the partial information model, the online algorithm receives (‚Ñìt
kt, kt), where
kt is distributed according to pt, and ‚Ñìt
H = ‚Ñìt
kt is its loss. The loss of the
i-th action during the Ô¨Årst T time steps is LT
i = PT
t=1 ‚Ñìt
i, and the loss of H
is LT
H = PT
t=1 ‚Ñìt
H.
The aim for the external regret setting is to design an online algorithm that
will be able to approach the performance of the best algorithm from a given
class of algorithms G; namely, to have a loss close to LT
G,min = ming‚ààG LT
g .
Formally we would like to minimize the external regret RG = LT
H ‚àíLT
G,min,
and G is called the comparison class. The most studied comparison class G
is the one that consists of all the single actions, i.e., G = X. In this chapter
we concentrate on this important comparison class, namely, we want the
online algorithm‚Äôs loss to be close to LT
min = mini LT
i , and let the external
regret be R = LT
H ‚àíLT
min.
External regret uses a Ô¨Åxed comparison class G, but one can also envision
a comparison class that depends on the online algorithm‚Äôs actions. We can
consider modiÔ¨Åcation rules that modify the actions selected by the online
algorithm, producing an alternative strategy which we will want to compete
against.
A modiÔ¨Åcation rule F has as input the history and the current
action selected by the online procedure and outputs a (possibly diÔ¨Äerent)
action. (We denote by F t the function F at time t, including any dependency
on the history.) Given a sequence of probability distributions pt used by an

8
A. Blum and Y. Mansour
online algorithm H, and a modiÔ¨Åcation rule F, we deÔ¨Åne a new sequence
of probability distributions f t = F t(pt), where f t
i = P
j:F t(j)=i pt
j. The loss
of the modiÔ¨Åed sequence is LH,F = P
t
P
i f t
i ‚Ñìt
i. Note that at time t the
modiÔ¨Åcation rule F shifts the probability that H assigned to action j to
action F t(j). This implies that the modiÔ¨Åcation rule F generates a diÔ¨Äerent
distribution, as a function of the online algorithm‚Äôs distribution pt.
We will focus on the case of a Ô¨Ånite set F of memoryless modiÔ¨Åcation
rules (they do not depend on history). Given a sequence of loss vectors, the
regret of an online algorithm H with respect to the modiÔ¨Åcation rules F is
RF = max
F ‚ààF{LT
H ‚àíLT
H,F}.
Note that the external regret setting is equivalent to having a set Fex of N
modiÔ¨Åcation rules Fi, where Fi always outputs action i. For internal regret,
the set Fin consists of N(N ‚àí1) modiÔ¨Åcation rules Fi,j, where Fi,j(i) = j
and Fi,j(i‚Ä≤) = i‚Ä≤ for i‚Ä≤ Ã∏= i. That is, the internal regret of H is
max
F ‚ààFin{LT
H ‚àíLT
H,F } = max
i,j‚ààX
( T
X
t=1
pt
i(‚Ñìt
i ‚àí‚Ñìt
j)
)
.
A more general class of memoryless modiÔ¨Åcation rules is swap regret de-
Ô¨Åned by the class Fsw, which includes all N N functions F : {1, . . . , N} ‚Üí
{1, . . . , N}, where the function F swaps the current online action i with F(i)
(which can be the same or a diÔ¨Äerent action). That is, the swap regret of
H is
max
F ‚ààFsw{LT
H ‚àíLT
H,F } =
N
X
i=1
max
j‚ààX
( T
X
t=1
pt
i(‚Ñìt
i ‚àí‚Ñìt
j)
)
.
Note that since Fex ‚äÜFsw and Fin ‚äÜFsw, both external and internal
regret are upper-bounded by swap regret. (See also Exercises 1 and 2.)
4.3 External Regret Minimization
Before describing the external regret results, we begin by pointing out that
it is not possible to guarantee low regret with respect to the overall optimal
sequence of decisions in hindsight, as is done in competitive analysis [ST85,
BEY98]. This will motivate why we will be concentrating on more restricted
comparison classes. In particular, let Gall be the set of all functions mapping
times {1, . . . , T} to actions X = {1, . . . , N}.
Theorem 4.1 For any online algorithm H there exists a sequence of T loss
vectors such that regret RGall is at least T(1 ‚àí1/N).

Learning, Regret minimization, and Equilibria
9
Proof
The sequence is simply as follows: at each time t, the action it of
lowest probability pt
i gets a loss of 0, and all the other actions get a loss of 1.
Since mini{pt
i} ‚â§1/N, this means the loss of H in T time steps is at least
T(1‚àí1/N). On the other hand, there exists g ‚ààGall, namely g(t) = it, with
a total loss of 0.
The above proof shows that if we consider all possible functions, we have a
very large regret. For the rest of the section we will use the comparison class
Ga = {gi : i ‚ààX}, where gi always selects action i. Namely, we compare the
online algorithm to the best single action.
Warmup: Greedy and Randomized-Greedy Algorithms
In this section, for simplicity we will assume all losses are either 0 or 1 (rather
than a real number in [0, 1]), which will simplify notation and proofs, though
everything presented can be easily extended to the general case.
Our Ô¨Årst attempt to develop a good regret minimization algorithm will
be to consider the greedy algorithm. Recall that Lt
i = Pt
œÑ=1 ‚ÑìœÑ
i , namely the
cumulative loss up to time t of action i. The Greedy algorithm at each time
t selects action xt = arg mini‚ààX Lt‚àí1
i
(if there are multiple actions with the
same cumulative loss, it prefers the action with the lowest index). Formally:
Greedy Algorithm
Initially:
x1 = 1.
At time t:
Let Lt‚àí1
min = mini‚ààX Lt‚àí1
i
, and St‚àí1 = {i : Lt‚àí1
i
= Lt‚àí1
min}.
Let xt = min St‚àí1.
Theorem 4.2 The Greedy algorithm, for any sequence of losses has
LT
Greedy ‚â§N ¬∑ LT
min + (N ‚àí1).
Proof
At each time t such that Greedy incurs a loss of 1 and Lt
min does
not increase, at least one action is removed from St.
This can occur at
most N times before Lt
min increases by 1. Therefore, Greedy incurs loss at
most N between successive increments in Lt
min. More formally, this shows
inductively that Lt
Greedy ‚â§N ‚àí|St| + N ¬∑ Lt
min .
The above guarantee on Greedy is quite weak, stating only that its loss is
at most a factor of N larger than the loss of the best action. The following
theorem shows that this weakness is shared by any deterministic online
algorithm. (A deterministic algorithm concentrates its entire weight on a
single action at each time step.)

10
A. Blum and Y. Mansour
Theorem 4.3 For any deterministic algorithm D there exists a loss se-
quence for which LT
D = T and LT
min = ‚åäT/N‚åã.
Note that the above theorem implies that LT
D ‚â•N ¬∑ LT
min + (T mod N),
which almost matches the upper bound for Greedy (Theorem 4.2).
Proof
Fix a deterministic online algorithm D and let xt be the action it
selects at time t. We will generate the loss sequence in the following way.
At time t, let the loss of xt be 1 and the loss of any other action be 0. This
ensures that D incurs loss 1 at each time step, so LT
D = T.
Since there are N diÔ¨Äerent actions, there is some action that algorithm
D has selected at most ‚åäT/N‚åãtimes.
By construction, only the actions
selected by D ever have a loss, so this implies that LT
min ‚â§‚åäT/N‚åã.
Theorem 4.3 motivates considering randomized algorithms. In particular,
one weakness of the greedy algorithm was that it had a deterministic tie
breaker. One can hope that if the online algorithm splits its weight between
all the currently best actions, better performance could be achieved. SpeciÔ¨Å-
cally, let Randomized Greedy (RG) be the procedure that assigns a uniform
distribution over all those actions with minimum total loss so far. We now
will show that this algorithm achieves a signiÔ¨Åcant performance improve-
ment: its loss is at most an O(log N) factor from the best action, rather
than O(N).
(This is similar to the analysis of the randomized marking
algorithm in competitive analysis).
Randomized Greedy (RG) Algorithm
Initially:
p1
i = 1/N for i ‚ààX.
At time t:
Let Lt‚àí1
min = mini‚ààX Lt‚àí1
i
, and St‚àí1 = {i : Lt‚àí1
i
= Lt‚àí1
min}.
Let pt
i = 1/|St‚àí1| for i ‚ààSt‚àí1 and pt
i = 0 otherwise.
Theorem 4.4 The Randomized Greedy (RG) algorithm, for any loss se-
quence, has
LT
RG ‚â§(ln N) + (1 + ln N)LT
min .
Proof
The proof follows from showing that the loss incurred by Randomized
Greedy between successive increases in Lt
min is at most 1+ln N. SpeciÔ¨Åcally,
let tj denote the time step at which Lt
min Ô¨Årst reaches a loss of j, so we are
interested in the loss of Randomized Greedy between time steps tj and tj+1.
At time any t we have 1 ‚â§|St| ‚â§N. Furthermore, if at time t ‚àà(tj, tj+1] the
size of St shrinks by k from some size n‚Ä≤ down to n‚Ä≤ ‚àík, then the loss of the
online algorithm RG is k/n‚Ä≤, since each such action has weight 1/n‚Ä≤. Finally,

Learning, Regret minimization, and Equilibria
11
notice that we can upper bound k/n‚Ä≤ by 1/n‚Ä≤+1/(n‚Ä≤‚àí1)+. . .+1/(n‚Ä≤‚àík+1).
Therefore, over the entire time-interval (tj, tj+1], the loss of Randomized
Greedy is at most:
1/N + 1/(N ‚àí1) + 1/(N ‚àí2) + . . . + 1/1 ‚â§1 + ln N.
More formally, this shows inductively that Lt
RG ‚â§(1/N + 1/(N ‚àí1) + . . . +
1/(|St| + 1)) + (1 + ln N) ¬∑ Lt
min.
Randomized Weighted Majority algorithm
Although Randomized Greedy achieved a signiÔ¨Åcant performance gain com-
pared to the Greedy algorithm, we still have a logarithmic ratio to the best
action. Looking more closely at the proof, one can see that the losses are
greatest when the sets St are small, since the online loss can be viewed as
proportional to 1/|St|. One way to overcome this weakness is to give some
weight to actions which are currently ‚Äúnear best‚Äù. That is, we would like
the probability mass on some action to decay gracefully with its distance to
optimality. This is the idea of the Randomized Weighted Majority algorithm
of Littlestone and Warmuth.
SpeciÔ¨Åcally, in the Randomized Weighted Majority algorithm, we give an
action i whose total loss so far is Li a weight wi = (1‚àíŒ∑)Li, and then choose
probabilities proportional to the weights: pi = wi/ PN
j=1 wj. The parameter
Œ∑ will be set to optimize certain tradeoÔ¨Äs but conceptually think of it as a
small constant, say 0.01. In this section we will again assume losses in {0, 1}
rather than [0, 1] because it allows for an especially intuitive interpretation
of the proof (Theorem 4.5).
We then relax this assumption in the next
section (Theorem 4.6).
Randomized Weighted Majority (RWM) Algorithm
Initially:
w1
i = 1 and p1
i = 1/N, for i ‚ààX.
At time t: If ‚Ñìt‚àí1
i
= 1, let wt
i = wt‚àí1
i
(1 ‚àíŒ∑); else (‚Ñìt‚àí1
i
= 0) let wt
i = wt‚àí1
i
.
Let pt
i = wt
i/W t, where W t = P
i‚ààX wt
i.
Algorithm RWM and Theorem 4.5 can be generalized to losses in [0, 1] by
replacing the update rule with wt
i = wt‚àí1
i
(1 ‚àíŒ∑)‚Ñìt‚àí1
i
(see Exercise 3).
Theorem 4.5 For Œ∑ ‚â§1/2, the loss of Randomized Weighted Majority
(RWM) on any sequence of binary {0, 1} losses satisÔ¨Åes:
LT
RWM ‚â§(1 + Œ∑)LT
min + ln N
Œ∑
.
Setting Œ∑ = min{
p
(ln N)/T , 1/2} yields LT
RWM ‚â§LT
min + 2
‚àö
T ln N.

12
A. Blum and Y. Mansour
(Note: the second part of the theorem assumes T is known in advance. If T is
unknown, then a ‚Äúguess and double‚Äù approach can be used to set Œ∑ with just
a constant-factor loss in regret. In fact, one can achieve the potentially better
bound LT
RWM ‚â§LT
min+2‚àöLmin ln N by setting Œ∑ = min{
p
(ln N)/Lmin, 1/2}.)
Proof The key to the proof is to consider the total weight W t. What we will
show is that any time the online algorithm has signiÔ¨Åcant expected loss, the
total weight must drop substantially. We will then combine this with the
fact that W T+1 ‚â•maxi wT+1
i
= (1 ‚àíŒ∑)LT
min to achieve the desired bound.
SpeciÔ¨Åcally, let F t = (P
i:‚Ñìt
i=1 wt
i)/W t denote the fraction of the weight
W t that is on actions that experience a loss of 1 at time t; so, F t equals
the expected loss of algorithm RWM at time t.
Now, each of the actions
experiencing a loss of 1 has its weight multiplied by (1 ‚àíŒ∑) while the rest
are unchanged. Therefore, W t+1 = W t ‚àíŒ∑F tW t = W t(1 ‚àíŒ∑F t). In other
words, the proportion of the weight removed from the system at each time
t is exactly proportional to the expected loss of the online algorithm. Now,
using the fact that W 1 = N and using our lower bound on W T+1 we have:
(1 ‚àíŒ∑)LT
min ‚â§W T+1 = W 1
T
Y
t=1
(1 ‚àíŒ∑F t) = N
T
Y
t=1
(1 ‚àíŒ∑F t).
Taking logarithms,
LT
min ln(1 ‚àíŒ∑)
‚â§
(ln N) +
T
X
t=1
ln(1 ‚àíŒ∑F t)
‚â§
(ln N) ‚àí
T
X
t=1
Œ∑F t
(Using the inequality ln(1 ‚àíz) ‚â§‚àíz)
=
(ln N) ‚àíŒ∑LT
RWM
(by deÔ¨Ånition of F t)
Therefore,
LT
RWM
‚â§
‚àíLT
min ln(1 ‚àíŒ∑)
Œ∑
+ ln(N)
Œ∑
‚â§
(1 + Œ∑)LT
min + ln(N)
Œ∑
,
(Using the inequality ‚àíln(1 ‚àíz) ‚â§z + z2 for 0 ‚â§z ‚â§1
2)
which completes the proof.

Learning, Regret minimization, and Equilibria
13
Polynomial Weights algorithm
The Polynomial Weights (PW) algorithm is a natural extension of the RWM
algorithm to losses in [0, 1] (or even to the case of both losses and gains, see
Exercise 4) that maintains the same proof structure as that used for RWM
and in addition performs especially well in the case of small losses.
Polynomial Weights (PW) Algorithm
Initially:
w1
i = 1 and p1
i = 1/N, for i ‚ààX.
At time t: Let wt
i = wt‚àí1
i
(1 ‚àíŒ∑‚Ñìt‚àí1
i
).
Let pt
i = wt
i/W t, where W t = P
i‚ààX wt
i.
Notice that the only diÔ¨Äerence between PW and RWM is in the update step.
In particular, it is no longer necessarily the case that an action of total loss
L has weight (1 ‚àíŒ∑)L. However, what is maintained is the property that
if the algorithm‚Äôs loss at time t is F t, then exactly an Œ∑F t fraction of the
total weight is removed from the system. SpeciÔ¨Åcally, from the update rule
we have W t+1 = W t ‚àíP
i Œ∑wt
i‚Ñìt
i = W t(1 ‚àíŒ∑F t) where F t = (P
i wt
i‚Ñìt
i)/W t
is the loss of PW at time t. We can use this fact to prove the following:
Theorem 4.6 The Polynomial Weights (PW) algorithm, using Œ∑ ‚â§1/2,
for any [0, 1]-valued loss sequence and for any k has,
LT
PW ‚â§LT
k + Œ∑QT
k + ln(N)
Œ∑
,
where QT
k = PT
t=1(‚Ñìt
k)2. Setting Œ∑ = min{
p
(ln N)/T , 1/2} and noting that
QT
k ‚â§T, we have LT
PW ‚â§LT
min + 2
‚àö
T ln N.‚Ä†
Proof As noted above, we have W t+1 = W t(1 ‚àíŒ∑F t) where F t is PW‚Äôs loss
at time t. So, as with the analysis of RWM, we have W T+1 = N QT
t=1(1‚àíŒ∑F t)
and therefore:
ln W T+1 = ln N +
T
X
t=1
ln(1 ‚àíŒ∑F t) ‚â§ln N ‚àíŒ∑
T
X
t=1
F t = ln N ‚àíŒ∑LT
PW.
Now for the lower bound, we have:
ln W T+1
‚â•
ln wT+1
k
=
T
X
t=1
ln(1 ‚àíŒ∑‚Ñìt
k)
(using the recursive deÔ¨Ånition of weights)
‚Ä† Again, for simplicity we assume that the number of time steps T is given as a parameter to
the algorithm; otherwise one can use a ‚Äúguess and double‚Äù method to set Œ∑.

14
A. Blum and Y. Mansour
‚â•
‚àí
T
X
t=1
Œ∑‚Ñìt
k ‚àí
T
X
t=1
(Œ∑‚Ñìt
k)2
(using the inequality ln(1 ‚àíz) ‚â•‚àíz ‚àíz2 for 0 ‚â§z ‚â§1
2 )
=
‚àíŒ∑LT
k ‚àíŒ∑2QT
k .
Combining the upper and lower bounds on ln W T+1 we have:
‚àíŒ∑LT
k ‚àíŒ∑2QT
k ‚â§ln N ‚àíŒ∑LT
PW,
which yields the theorem.
Lower Bounds
An obvious question is whether one can signiÔ¨Åcantly improve the bound in
Theorem 4.6. We will show two simple results that imply that the regret
bound is near optimal (see Exercise 5 for a better lower bound). The Ô¨Årst
result shows that one cannot hope to get sublinear regret when T is small
compared to log N, and the second shows that one cannot hope to achieve
regret o(
‚àö
T) even when N = 2.
Theorem 4.7 Consider T < log2 N. There exists a stochastic generation
of losses such that, for any online algorithm R1, we have E[LT
R1] = T/2 and
yet LT
min = 0.
Proof Consider the following sequence of losses. At time t = 1, a random
subset of N/2 actions get a loss of 0 and the rest get a loss of 1. At time
t = 2, a random subset of N/4 of the actions that had loss 0 at time t = 1
get a loss of 0, and the rest (including actions that had a loss of 1 at time
1) get a loss of 1. This process repeats: at each time step, a random subset
of half of the actions that have received loss 0 so far get a loss of 0, while
all the rest get a loss of 1. Any online algorithm incurs an expected loss of
1/2 at each time step, because at each time step t the expected fraction of
probability mass pt
i on actions that receive a loss of 0 is at most 1/2. Yet,
for T < log2 N there will always be some action with total loss of 0.
Theorem 4.8 Consider N = 2.
There exists a stochastic generation of
losses such that, for any online algorithm R2, we have E[LT
R2 ‚àíLT
min] =
‚Ñ¶(
‚àö
T).
Proof At time t, we Ô¨Çip a fair coin and set ‚Ñìt = z1 = (0, 1) with probability
1/2 and ‚Ñìt = z2 = (1, 0) with probability 1/2. For any distribution pt the

Learning, Regret minimization, and Equilibria
15
expected loss at time t is exactly 1/2. Therefore any online algorithm R2
has expected loss of T/2.
Given a sequence of T such losses, with T/2+y losses z1 and T/2‚àíy losses
z2, we have T/2‚àíLT
min = |y|. It remains to lower bound E[|y|]. Note that the
probability of y is
 T
T/2+y
/2T , which is upper bounded by O(1/
‚àö
T) (using
a Sterling approximation). This implies that with a constant probability we
have |y| = ‚Ñ¶(
‚àö
T), which completes the proof.
4.4 Regret minimization and game theory
In this section we outline the connection between regret minimization and
central concepts in game theory. We start by showing that in a two player
constant sum game, a player with external regret sublinear in T will have
an average payoÔ¨Äthat is at least the value of the game, minus a vanish-
ing error term. For a general game, we will see that if all the players use
procedures with sublinear swap-regret, then they will converge to an approx-
imate correlated equilibrium. We also show that for a player who minimizes
swap-regret, the frequency of playing dominated actions is vanishing.
Game theoretic model
We start with the standard deÔ¨Ånitions of a game (see also Chapter 1). A
game G = ‚ü®M, (Xi), (si)‚ü©has a Ô¨Ånite set M of m players. Player i has a set
Xi of N actions and a loss function si : Xi √ó (√ójÃ∏=iXj) ‚Üí[0, 1] that maps
the action of player i and the actions of the other players to a real number.
(We have scaled losses to [0, 1].) The joint action space is X = √óXi.
We consider a player i that plays a game G for T time steps using an online
procedure ON. At time step t, player i plays a distribution (mixed action)
P t
i , while the other players play the joint distribution P t
‚àíi. We denote by
‚Ñìt
ON the loss of player i at time t, i.e., Ex‚àºP t[si(xt)], and its cumulative loss
is LT
ON = PT
t=1 ‚Ñìt
ON.‚Ä† It is natural to deÔ¨Åne, for player i at time t, the loss
vector as ‚Ñìt = (‚Ñìt
1, . . . , ‚Ñìt
N), where ‚Ñìt
j = Ext
‚àíi‚àºP t
‚àíi[si(xt
j, xt
‚àíi)]. Namely, ‚Ñìt
j
is the loss player i would have observed if at time t it had played action
xj. The cumulative loss of action xj ‚ààXi of player i is LT
j = PT
t=1 ‚Ñìt
j, and
LT
min = minj LT
j .
‚Ä† Alternatively, we could consider xt
i as a random variable distributed according to P t
i , and
similarly discuss the expected loss. We prefer the above presentation for consistency with the
rest of the chapter.

16
A. Blum and Y. Mansour
Constant sum games and external regret minimization
A two player constant sum game G = ‚ü®{1, 2}, (Xi), (si)‚ü©has the property
that for some constant c, for every x1 ‚ààX1 and x2 ‚ààX2 we have s1(x1, x2)+
s2(x1, x2) = c. It is well known that any constant sum game has a well
deÔ¨Åned value (v1, v2) for the game, and player i ‚àà{1, 2} has a mixed strategy
which guarantees that its expected loss is at most vi, regardless of the other
player‚Äôs strategy. (See [Owe82] for more details.) In such games, external
regret-minimization procedures provide the following guarantee:
Theorem 4.9 Let G be a constant sum game with game value (v1, v2). If
player i ‚àà{1, 2} plays for T steps using a procedure ON with external regret
R, then its average loss 1
T LT
ON is at most vi + R/T.
Proof Let q be the mixed strategy corresponding to the observed frequencies
of the actions player 2 has played; that is, qj = PT
t=1 P t
2,j/T, where P t
2,j is
the weight player 2 gives to action j at time t. By the theory of constant
sum games, for any mixed strategy q of player 2, player 1 has some action
xk ‚ààX1 such that Ex2‚àºq[s1(xk, x2)] ‚â§v1 (see [Owe82]). This implies, in
our setting, that if player 1 has always played action xk, then its loss would
be at most v1T. Therefore LT
min ‚â§LT
k ‚â§v1T. Now, using the fact that
player 1 is playing a procedure ON with external regret R, we have that
LT
ON ‚â§LT
min + R ‚â§v1T + R .
Thus, using a procedure with regret R = O(‚àöT log N) as in Theorem 4.6
will guarantee average loss at most vi + O(
p
(log N)/T ).
In fact, we can use the existence of external regret minimization algo-
rithms to prove the minimax theorem of two-player zero-sum games. For
player 1, let v1
min = minx1‚ààX1 maxz‚àà‚àÜ(X2) Ex2‚àºz[s1(x1, x2)] and v1
max =
maxx2‚ààX2 minz‚àà‚àÜ(X1) Ex1‚àºz[s1(x1, x2)]. That is, v1
min is the best loss that
player 1 can guarantee for itself if it is told the mixed action of player 2
in advance. Similarly, v1
max is the best loss that player 1 can guarantee to
itself if it has to go Ô¨Årst in selecting a mixed action, and player 2‚Äôs action
may then depend on it. The minimax theorem states that v1
min = v1
max.
Since s1(x1, x2) = ‚àís2(x1, x2) we can similarly deÔ¨Åne v2
min = ‚àív1
max and
v2
max = ‚àív1
min.
In the following we give a proof of the minimax theorem based on the ex-
istence of external regret algorithms. Assume for contradiction that v1
max =
v1
min + Œ≥ for some Œ≥ > 0 (it is easy to see that v1
max ‚â•v1
min). Consider both
players playing a regret minimization algorithm for T steps having external
regret of at most R, such that R/T < Œ≥/2. Let LON be the loss of player 1

Learning, Regret minimization, and Equilibria
17
and note that ‚àíLON is the loss of player 2. Let Li
min be the cumulative loss
of the best action of player i ‚àà{1, 2}. As before, let qi be the mixed strat-
egy corresponding to the observed frequencies of actions of player i ‚àà{1, 2}.
Then, L1
min/T ‚â§v1
min, since for L1
min we select the best action with respect
to a speciÔ¨Åc mixed action, namely q2. Similarly, L2
min/T ‚â§v2
min. The regret
minimization algorithms guarantee for player 1 that LON ‚â§L1
min + R, and
for player 2 that ‚àíLON ‚â§L2
min + R. Combining the inequalities we have:
Tv1
max ‚àíR = ‚àíTv2
max ‚àíR ‚â§‚àíL2
min ‚àíR ‚â§LON ‚â§L1
min + R ‚â§Tv1
min + R.
This implies that v1
max ‚àív1
min ‚â§2R/T < Œ≥, which is a contradiction. There-
fore, v1
max = v1
min, which establishes the minimax theorem.
Correlated Equilibrium and swap regret minimization
We Ô¨Årst deÔ¨Åne the relevant modiÔ¨Åcation rules and establish the connection
between them and equilibrium notions. For x1, b1, b2 ‚ààXi, let switchi(x1, b1, b2)
be the following modiÔ¨Åcation function of the action x1 of player i:
switchi(x1, b1, b2) =
(
b2
if x1 = b1
x1
otherwise
Given a modiÔ¨Åcation function f for player i, we can measure the regret of
player i with respect to f as the decrease in its loss, i.e.,
regreti(x, f) = si(x) ‚àísi(f(xi), x‚àíi).
For example, when we consider f(x1) = switchi(x1, b1, b2), for a Ô¨Åxed b1, b2 ‚àà
Xi, then regreti(x, f) is measuring the regret player i has for playing action
b1 rather than b2, when the other players play x‚àíi.
A correlated equilibrium is a distribution P over the joint action space
with the following property. Imagine a correlating device draws a vector of
actions x ‚ààX using distribution P over X, and gives player i the action
xi from x. (Player i is not given any other information regarding x.) The
probability distribution P is a correlated equilibrium if, for each player, it is
a best response to play the suggested action, provided that the other players
also do not deviate. (For a more detailed discussion of correlated equilibrium
see Chapter 1.)
DeÔ¨Ånition 4.10 A joint probability distribution P over X is a correlated
equilibrium if for every player i, and any actions b1, b2 ‚ààXi, we have that
Ex‚àºP [regreti(x, switchi(¬∑, b1, b2))] ‚â§0.

18
A. Blum and Y. Mansour
An equivalent deÔ¨Ånition that extends more naturally to the case of ap-
proximate equilibria is to say that rather than only switching between a pair
of actions, we allow simultaneously replacing every action in Xi with another
action in Xi (possibly the same action). A distribution P is a correlated equi-
librium iÔ¨Äfor any function F : Xi ‚ÜíXi we have Ex‚àºP [regreti(x, F)] ‚â§0.
We now deÔ¨Åne an œµ-correlated equilibrium. An œµ-correlated equilibrium
is a distribution P such that each player has in expectation at most an œµ
incentive to deviate. Formally,
DeÔ¨Ånition 4.11 A joint probability distribution P over X is an œµ-correlated
equilibria if for every player i and for any function Fi : Xi ‚ÜíXi, we have
Ex‚àºP[regreti(x, Fi)] ‚â§œµ.
The following theorem relates the empirical distribution of the actions
performed by each player, their swap regret, and the distance to correlated
equilibrium.
Theorem 4.12 Let G = ‚ü®M, (Xi), (si)‚ü©be a game and assume that for T
time steps every player follows a strategy that has swap regret of at most R.
Then, the empirical distribution Q of the joint actions played by the players
is an (R/T)-correlated equilibrium.
Proof The empirical distribution Q assigns to every P t a probability of 1/T.
Fix a function F : Xi ‚ÜíXi for player i. Since player i has swap regret at
most R, we have LT
ON ‚â§LT
ON,F + R, where LT
ON is the loss of player i. By
deÔ¨Ånition of the regret function, we therefore have:
LT
ON ‚àíLT
ON,F
=
T
X
t=1
Ext‚àºP t[si(xt)] ‚àí
T
X
t=1
Ext‚àºP t[si(F(xt
i), xt
‚àíi)]
=
T
X
t=1
Ext‚àºP t[regreti(xt, F)] = T ¬∑ Ex‚àºQ[regreti(x, F)].
Therefore, for any function Fi : Xi ‚ÜíXi we have Ex‚àºQ[regreti(x, Fi)] ‚â§
R/T.
The above theorem states that the payoÔ¨Äof each player is its payoÔ¨Äin
some approximate correlated equilibrium. In addition, it relates the swap
regret to the distance from equilibrium. Note that if the average swap regret
vanishes then the procedure converges, in the limit, to the set of correlated
equilibria.

Learning, Regret minimization, and Equilibria
19
Dominated strategies
We say that an action xj ‚ààXi is œµ-dominated by action xk ‚ààXi if for any
x‚àíi ‚ààX‚àíi we have si(xj, x‚àíi) ‚â•œµ + si(xk, x‚àíi). Similarly, action xj ‚ààXi
is œµ-dominated by a mixed action y ‚àà‚àÜ(Xi) if for any x‚àíi ‚ààX‚àíi we have
si(xj, x‚àíi) ‚â•œµ + Exd‚àºy[si(xd, x‚àíi)].
Intuitively, a good learning algorithm ought to be able to learn not to
play actions that are œµ-dominated by others, and in this section we show
that indeed if player i plays a procedure with sublinear swap regret, then
it will very rarely play dominated actions. More precisely, let action xj be
œµ-dominated by action xk ‚ààXi. Using our notation, this implies that for
any x‚àíi we have that regreti(x, switchi(¬∑, xj, xk)) ‚â•œµ. Let Dœµ be the set of
œµ-dominated actions of player i, and let w be the weight that player i puts
on actions in Dœµ, averaged over time, i.e., w = 1
T
PT
t=1
P
j‚ààDœµ P t
i,j. Player
i‚Äôs swap regret is at least œµwT (since we could replace each action in Dœµ
with the action that dominates it). So, if the player‚Äôs swap regret is R, then
œµwT ‚â§R. Therefore, the time-average weight that player i puts on the set
of œµ-dominated actions is at most R/(œµT) which tends to 0 if R is sublinear
in T. That is:
Theorem 4.13 Consider a game G and a player i that uses a procedure of
swap regret R for T time steps. Then the average weight that player i puts
on the set of œµ-dominated actions is at most R/(œµT).
We remark that in general the property of having low external regret is
not suÔ¨Écient by itself to give such a guarantee, though the algorithms RWM
and PW do indeed have such a guarantee (see Exercise 8).
4.5 Generic reduction from external to swap regret
In this section we give a black-box reduction showing how any procedure A
achieving good external regret can be used as a subroutine to achieve good
swap regret as well. The high-level idea is as follows (see also Fig. 4.1).
We will instantiate N copies A1, . . . , AN of the external-regret procedure.
At each time step, these procedures will each give us a probability vector,
which we will combine in a particular way to produce our own probability
vector p. When we receive a loss vector ‚Ñì, we will partition it among the
N procedures, giving procedure Ai a fraction pi (pi is our probability mass
on action i), so that Ai‚Äôs belief about the loss of action j is P
t pt
i‚Ñìt
j, and
matches the cost we would incur putting i‚Äôs probability mass on j. In the
proof, procedure Ai will in some sense be responsible for ensuring low regret

20
A. Blum and Y. Mansour
-


-

-
AN
A1
H
qt
1
p1‚Ñìt
pt
n‚Ñìt
pt
‚Ñìt
qt
N
e
e
e
Fig. 4.1. The structure of the swap regret reduction.
of the i ‚Üíj variety. The key to making this work is that we will be able
to deÔ¨Åne the p‚Äôs so that the sum of the losses of the procedures Ai on their
own loss vectors matches our overall true loss. Recall the deÔ¨Ånition of an R
external regret procedure.
DeÔ¨Ånition 4.14 An R external regret procedure A guarantees that for any
sequence of T losses ‚Ñìt and for any action j ‚àà{1, . . . , N}, we have
LT
A =
T
X
t=1
‚Ñìt
A ‚â§
T
X
t=1
‚Ñìt
j + R = LT
j + R.
We assume we have N copies A1, . . . , AN of an R external regret proce-
dure. We combine the N procedures to one master procedure H as follows.
At each time step t, each procedure Ai outputs a distribution qt
i, where qt
i,j
is the fraction it assigns action j. We compute a single distribution pt such
that pt
j = P
i pt
iqt
i,j. That is, pt = ptQt, where pt is our distribution and
Qt is the matrix of qt
i,j. (We can view pt as a stationary distribution of the
Markov Process deÔ¨Åned by Qt, and it is well known such a pt exists and is
eÔ¨Éciently computable.) For intuition into this choice of pt, notice that it
implies we can consider action selection in two equivalent ways. The Ô¨Årst is
simply using the distribution pt to select action j with probability pt
j. The

Learning, Regret minimization, and Equilibria
21
second is to select procedure Ai with probability pt
i and then to use Ai to
select the action (which produces distribution ptQt).
When the adversary returns the loss vector ‚Ñìt, we return to each Ai the
loss vector pi‚Ñìt. So, procedure Ai experiences loss (pt
i‚Ñìt) ¬∑ qt
i = pt
i(qt
i ¬∑ ‚Ñìt).
Since Ai is an R external regret procedure, for any action j, we have,
T
X
t=1
pt
i(qt
i ¬∑ ‚Ñìt)
‚â§
T
X
t=1
pt
i‚Ñìt
j + R
(4.1)
If we sum the losses of the N procedures at a given time t, we get P
i pt
i(qt
i ¬∑
‚Ñìt) = ptQt‚Ñìt, where pt is the row-vector of our distribution, Qt is the matrix
of qt
i,j, and ‚Ñìt is viewed as a column-vector. By design of pt, we have ptQt =
pt. So, the sum of the perceived losses of the N procedures is equal to our
actual loss pt‚Ñìt.
Therefore, summing equation (4.1) over all N procedures, the left-hand-
side sums to LT
H, where H is our master online procedure. Since the right-
hand-side of equation (4.1) holds for any j, we have that for any function
F : {1, . . . , N} ‚Üí{1, . . . , N},
LT
H ‚â§
N
X
i=1
T
X
t=1
pt
i‚Ñìt
F (i) + NR = LT
H,F + NR
Therefore we have proven the following theorem.
Theorem 4.15 Given an R external regret procedure, the master online
procedure H has the following guarantee. For every function F : {1, . . . , N} ‚Üí
{1, . . . , N},
LH ‚â§LH,F + NR ,
i.e., the swap regret of H is at most NR.
Using Theorem 4.6 we can immediately derive the following corollary.
Corollary 4.16 There exists an online algorithm H such that for every
function F : {1, . . . , N} ‚Üí{1, . . . , N}, we have that
LH ‚â§LH,F + O(N
p
T log N) ,
i.e., the swap regret of H is at most O(N‚àöT log N).
Remark: See Exercise 6 for an improvement to O(‚àöNT log N).

22
A. Blum and Y. Mansour
4.6 The Partial Information Model
In this section we show, for external regret, a simple reduction from the
partial information to the full information model.‚Ä†
The main diÔ¨Äerence
between the two models is that in the full information model, the online
procedure has access to the loss of every action. In the partial information
model the online procedure receives as feedback only the loss of a single
action, the action it performed. This very naturally leads to an exploration
versus exploitation tradeoÔ¨Äin the partial information model, and essentially
any online procedure will have to somehow explore the various actions and
estimate their loss.
The high level idea of the reduction is as follows. Assume that the number
of time steps T is given as a parameter. We will partition the T time steps
into K blocks. The procedure will use the same distribution over actions in
all the time steps of any given block, except it will also randomly sample each
action once (the exploration part). The partial information procedure MAB
will pass to the full information procedure FIB the vector of losses received
from its exploration steps. The full information procedure FIB will then
return a new distribution over actions. The main part of the proof will be
to relate the loss of the full information procedure FIB on the loss sequence
it observes to the loss of the partial information procedure MAB on the real
loss sequence.
We start by considering a full information procedure FIB that partitions
the T time steps into K blocks, B1, . . . , BK, where Bi = {(i ‚àí1)(T/K) +
1, . . . , i(T/K)}, and uses the same distribution in all the time steps of a
block.
(For simplicity we assume that K divides T.)
Consider an RK
external regret minimization procedure FIB (over K time steps), which at
the end of block i updates the distribution using the average loss vector, i.e.,
cœÑ = P
t‚ààBœÑ ‚Ñìt/|BœÑ|. Let CK
i
= PK
œÑ=1 cœÑ
i and CK
min = mini CK
i . Since FIB has
external regret at most RK, this implies that the loss of FIB, over the loss
sequence cœÑ, is at most CK
min + RK. Since in every block BœÑ the procedure
FIB uses a single distribution pœÑ, its loss on the entire loss sequence is:
LTFIB =
K
X
œÑ=1
X
t‚ààBœÑ
pœÑ ¬∑ ‚Ñìt = T
K
K
X
œÑ=1
pœÑ ¬∑ cœÑ ‚â§T
K [CK
min + RK].
At this point it is worth noting that if RK = O(‚àöK log N) the overall
regret is O((T/
‚àö
K)‚àölog N), which is minimized at K = T, namely by
having each block be a single time step. However, we will have an additional
‚Ä† This reduction does not produce the best known bounds for the partial information model (see,
e.g., [ACBFS02] for better bounds) but is particularly simple and generic.

Learning, Regret minimization, and Equilibria
23
loss associated with each block (due to the sampling) which will cause the
optimization to require that K ‚â™T.
The next step in developing the partial information procedure MAB is to
use loss vectors which are not the ‚Äútrue average‚Äù but whose expectation is
the same. More formally, the feedback to the full information procedure
FIB will be a random variable vector ÀÜcœÑ such that for any action i we have
E[ÀÜcœÑ
i ] = cœÑ
i . Similarly, let ÀÜCK
i
= PK
œÑ=1 ÀÜcœÑ
i and ÀÜCK
min = mini ÀÜCK
i . (Intuitively,
we will generate the vector ÀÜcœÑ using sampling within a block.) This implies
that for any block BœÑ and any distribution pœÑ we have
1
|BœÑ|
X
t‚ààBœÑ
pœÑ ¬∑ ‚Ñìt = pœÑ ¬∑ cœÑ =
N
X
i=1
pœÑ
i cœÑ
i =
N
X
i=1
pœÑ
i E[ÀÜcœÑ
i ]
(4.2)
That is, the loss of pœÑ in BœÑ is equal to its expected loss with respect to ÀÜcœÑ.
The full information procedure FIB observes the losses ÀÜcœÑ, for œÑ ‚àà{1, . . . ,
K}. However, since ÀÜcœÑ are random variables, the distribution pœÑ is also a
random variable that depends on the previous losses, i.e., ÀÜc1, . . . ÀÜcœÑ‚àí1. Still,
with respect to any sequence of losses ÀÜcœÑ, we have that
ÀÜCKFIB =
K
X
œÑ=1
pœÑ ¬∑ ÀÜcœÑ ‚â§ÀÜCK
min + RK
Since E[ ÀÜCK
i ] = CK
i , this implies that
E[ ÀÜCKFIB] ‚â§E[ ÀÜCK
min] + RK ‚â§CK
min + RK,
where we used the fact that E[mini ÀÜCK
i ] ‚â§mini E[ ÀÜCK
i ] and the expectation
is over the choices of ÀÜcœÑ.
Note that for any sequence of losses ÀÜc1, . . . , ÀÜcK, both FIB and MAB will use
the same sequence of distributions p1, . . . , pK. From (4.2) we have that in
any block BœÑ the expected loss of FIB and the loss of MAB are the same,
assuming they both use the same distribution pœÑ. This implies that
E[CKMAB] = E[ ÀÜCKFIB] .
We now need to show how to derive random variables ÀÜcœÑ with the desired
property. This will be done by choosing randomly, for each action i and block
BœÑ, an exploration time ti ‚ààBœÑ. (These do not need to be independent over
the diÔ¨Äerent actions, so can easily be done without collisions.) At time ti
the procedure MAB will play action i (i.e., the probability vector with all
probability mass on i). This implies that the feedback that it receives will
be ‚Ñìti
i , and we will then set ÀÜcœÑ
i to be ‚Ñìti
i . This guarantees that E[ÀÜcœÑ
i ] = cœÑ
i .

24
A. Blum and Y. Mansour
So far we have ignored the loss in the exploration steps. Since the max-
imum loss is 1, and there are N exploration steps in each of the K blocks,
the total loss in all the exploration steps is at most NK. Therefore we have:
E[LTMAB]
‚â§
NK + (T/K)E[CKMAB]
‚â§
NK + (T/K)[CK
min + RK]
=
LT
min + NK + (T/K)RK.
By Theorem 4.6, there are external regret procedures that have regret RK =
O(‚àöK log N). By setting K = (T/N)2/3, for T ‚â•N, we have the following
theorem.
Theorem 4.17 Given an O(‚àöK log N) external regret procedure FIB (for
K time steps), there is a partial information procedure MAB that guarantees
LTMAB ‚â§LT
min + O(T 2/3N 1/3 log N) ,
where T ‚â•N.
4.7 On convergence of regret-minimizing strategies to Nash
equilibrium in routing games
As mentioned earlier, one natural setting for regret-minimizing algorithms
is online routing. For example, a person could use such algorithms to select
which of N available routes to use to drive to work each morning in such
a way that his performance will be nearly as good as the best Ô¨Åxed route
in hindsight, even if traÔ¨Éc changes arbitrarily from day to day.
In fact,
even though in a graph G, the number of paths N between two nodes may
be exponential in the size of G, there are a number of external-regret min-
imizing algorithms whose running time and regret bounds are polynomial
in the graph size. Moreover, a number of extensions have shown how these
algorithms can be applied even to the partial-information setting where only
the cost of the path traversed is revealed to the algorithm.
In this section we consider the game-theoretic properties of such algo-
rithms in the Wardrop model of traÔ¨Éc Ô¨Çow.
In this model, we have a
directed network G = (V, E), and one unit Ô¨Çow of traÔ¨Éc (a large population
of inÔ¨Ånitesimal users that we view as having one unit of volume) wanting
to travel between two distinguished nodes vstart and vend. (For simplicity,
we are considering just the single-commodity version of the model.) We
assume each edge e has a cost given by a latency function ‚Ñìe that is some
non-decreasing function of the amount of traÔ¨Éc Ô¨Çowing on edge e. In other

Learning, Regret minimization, and Equilibria
25
words, the time to traverse each edge e is a function of the amount of con-
gestion on that edge. In particular, given some Ô¨Çow f, where we use fe to
denote the amount of Ô¨Çow on a given edge e, the cost of some path P is
P
e‚ààP ‚Ñìe(fe) and the average travel time of all users in the population can be
written as P
e‚ààE ‚Ñìe(fe)fe. A Ô¨Çow f is at Nash equilibrium if all Ô¨Çow-carrying
paths P from vstart to vend are minimum-latency paths given the Ô¨Çow f.
Chapter 18 considers this model in much more detail, analyzing the rela-
tionship between latencies in Nash equilibrium Ô¨Çows and those in globally-
optimum Ô¨Çows (Ô¨Çows that minimize the total travel time averaged over all
users). In this section we describe results showing that if the users in such a
setting are adapting their paths from day to day using external-regret min-
imizing algorithms (or even if they just happen to experience low-regret,
regardless of the speciÔ¨Åc algorithms used) then Ô¨Çow will approach Nash
equilibrium. Note that a Nash equilibrium is precisely a set of static strate-
gies that are all no-regret with respect to each other, so such a result seems
natural; however there are many simple games for which regret-minimizing
algorithms do not approach Nash equilibrium and can even perform much
worse than any Nash equilibrium.
SpeciÔ¨Åcally, one can show that if each user has regret o(T), or even if just
the average regret (averaged over the users) is o(T), then Ô¨Çow approaches
Nash equilibrium in the sense that a 1‚àíœµ fraction of days t have the property
that a 1 ‚àíœµ fraction of the users that day experience travel time at most œµ
larger than the best path for that day, where œµ approaches 0 at a rate that
depends polynomially on the size of the graph, the regret-bounds of the
algorithms, and the maximum slope of any latency function. Note that this
is a somewhat nonstandard notion of convergence to equilibrium: usually
for an ‚Äúœµ-approximate equilibrium‚Äù one requires that all participants have
at most œµ incentive to deviate.
However, since low-regret algorithms are
allowed to occasionally take long paths, and in fact algorithms in the MAB
model must occasionally explore paths they have not tried in a long time
(to avoid regret if the paths have become much better in the meantime), the
multiple levels of hedging are actually necessary for a result of this kind.
In this section we present just a special case of this result. Let P denote
the set of all simple paths from vstart to vend and let f t denote the Ô¨Çow on
day t. Let C(f) = P
e‚ààE ‚Ñìe(fe)fe denote the cost of a Ô¨Çow f. Note that
C(f) is a weighted average of costs of paths in P and in fact is equal to
the average cost of all users in the Ô¨Çow f. DeÔ¨Åne a Ô¨Çow f to be œµ-Nash if
C(f) ‚â§œµ + minP ‚ààP
P
e‚ààP ‚Ñìe(fe); that is, the average incentive to deviate
over all users is at most œµ. Let R(T) denote the average regret (averaged

26
A. Blum and Y. Mansour
over users) up through day T, so
R(T) ‚â°
T
X
t=1
X
e‚ààE
‚Ñìe(f t
e)f t
e ‚àímin
P ‚ààP
T
X
t=1
X
e‚ààP
‚Ñìe(f t
e).
Finally, let Tœµ denote the number of time steps T needed so that R(T) ‚â§œµT
for all T ‚â•Tœµ. For example the RWM and PW algorithms discussed in Section
4.3 achieve Tœµ = O( 1
œµ2 log N) if we set Œ∑ = œµ/2. Then we will show:
Theorem 4.18 Suppose the latency functions ‚Ñìe are linear. Then for T ‚â•
Tœµ, the average Ô¨Çow ÀÜf = 1
T (f 1 + . . . + f T) is œµ-Nash.
Proof From the linearity of the latency functions, we have for all e, ‚Ñìe( ÀÜfe) =
1
T
PT
t=1 ‚Ñìe(f t
e). Since ‚Ñìe(f t
e)f t
e is a convex function of the Ô¨Çow, this implies
‚Ñìe( ÀÜfe) ÀÜfe ‚â§1
T
T
X
t=1
‚Ñìe(f t
e)f t
e.
Summing over all e, we have
C( ÀÜf)
‚â§
1
T
PT
t=1 C(f t)
‚â§
œµ + minP 1
T
PT
t=1
P
e‚ààP ‚Ñìe(f t
e)
(by deÔ¨Ånition of Tœµ)
=
œµ + minP
P
e‚ààP ‚Ñìe( ÀÜfe).
(by linearity)
This result shows the time-average Ô¨Çow is an approximate Nash equilib-
rium. This can then be used to prove that most of the f t must in fact be
approximate Nash. The key idea here is that if the cost of any edge were to
Ô¨Çuctuate wildly over time, then that would imply that most of the users of
that edge experienced latency substantially greater than the edge‚Äôs average
cost (because more users are using the edge when it is congested than when
it is not congested), which in turn implies they experience substantial regret.
These arguments can then be carried over to the case of general (non-linear)
latency functions.
Current Research Directions
In this section we sketch some current research directions with respect to
regret minimization.

Learning, Regret minimization, and Equilibria
27
ReÔ¨Åned Regret Bounds: The regret bounds that we presented depend
on the number of time steps T, and are independent of the performance
of the best action. Such bounds are also called zero order bounds. More
reÔ¨Åned Ô¨Årst order bounds depend on the loss of the best action, and second
order bounds depend on the sum of squares of the losses (such as QT
k in The-
orem 4.6). An interesting open problem is to get an external regret which
is proportional to the empirical variance of the best action. Another chal-
lenge is to reduce the prior information needed by the regret minimization
algorithm. Ideally, it should be able to learn and adapt to parameters such
as the maximum and minimum loss. See [CBMS05] for a detailed discussion
of those issues.
Large actions spaces: In this chapter we assumed the number of actions
N is small enough to be able to list them all, and our algorithms work in
time proportional to N. However, in many settings N is exponential in the
natural parameters of the problem. For example, the N actions might be all
simple paths between two nodes s and t in an n-node graph, or all binary
search trees on {1, . . . , n}. Since the full information external regret bounds
are only logarithmic in N, from the point of view of information, we can
derive polynomial regret bounds. The challenge is whether in such settings
we can produce computationally eÔ¨Écient algorithms.
There have recently been several results able to handle broad classes of
problems of this type. Kalai and Vempala [KV03] give an eÔ¨Écient algorithm
for any problem in which (a) the set X of actions can be viewed as a subset
of Rn, (b) the loss vectors ‚Ñìare linear functions over Rn (so the loss of
action x is ‚Ñì¬∑ x), and (c) we can eÔ¨Éciently solve the oÔ¨Ñine optimization
problem argminx‚ààS[x ¬∑ ‚Ñì] for any given loss vector ‚Ñì.
For instance, this
setting can model the path and search-tree examples above.‚Ä†
Zinkevich
[Zin03] extends this to convex loss functions with a projection oracle, and
there is substantial interest in trying to broaden the class of settings that
eÔ¨Écient regret-minimization algorithms can be applied to.
Dynamics: It is also very interesting to analyze the dynamics of regret min-
imization algorithms. The classical example is that of swap regret: when all
the players play swap regret minimization algorithms, the empirical distribu-
tion converges to the set of correlated equilibria (Section 4.4). We also saw
convergence in two-player zero sum games to the minimax value of the game
‚Ä† The case of search trees has the additional issue that there is a rotation cost associated with
using a diÔ¨Äerent action (tree) at time t + 1 than that used at time t. This is addressed in
[KV03] as well.

28
A. Blum and Y. Mansour
(Section 4.4), and convergence to Nash equilibrium in a Wardrop-model
routing game (Section 4.7).
Further results on convergence to equilibria
in other settings would be of substantial interest. At a high level, under-
standing the dynamics of regret minimization algorithms would allow us to
better understand the strengths and weaknesses of using such procedures.
For more information on learning in games, see the book [FL98].
Exercises
4.1
Show that swap regret is at most N times larger than internal regret.
4.2
Show an example (even with N = 3) where the ratio between the
external and swap regret is unbounded.
4.3
Show that the RWM algorithm with update rule wt
i = wt‚àí1
i
(1 ‚àíŒ∑)‚Ñìt‚àí1
i
achieves the same external regret bound as given in Theorem 4.6 for
the PW algorithm, for losses in [0, 1].
4.4
Consider a setting where the payoÔ¨Äs are in the range [‚àí1, +1], and
the goal of the algorithm is to maximize its payoÔ¨Ä. Derive a modi-
Ô¨Åed PW algorithm whose external regret is O(
q
QTmax log N + log N),
where QT
max ‚â•QT
k for k ‚ààXi.
4.5
Show a ‚Ñ¶(‚àöT log N) lower bound on external regret, for the case
that T ‚â•N.
4.6
Improve the swap regret bound to O(‚àöNT log N). Hint: use the
observation that the sum of the losses of all the Ai is bounded by T.
4.7
(Open Problem) Does there exist an ‚Ñ¶(‚àöTN log N) lower bound
for swap regret?
4.8
Show that if a player plays algorithm RWM (or PW) then it give œµ-
dominated actions small weight.
Also, show that there are cases
where the external regret of a player can be small, yet it gives œµ-
dominated actions high weight.
Notes
Hannan [Han57] was the Ô¨Årst to develop algorithms with external regret
sublinear in T. Later, motivated by machine learning settings in which N
can be quite large, algorithms that furthermore have only a logarithmic
dependence on N were developed in [LW94, FS97, FS99, CBFH+97]. In
particular, the Randomized Weighted Majority algorithm and Theorem 4.5
are from [LW94] and the Polynomial Weights algorithm and Theorem 4.6
is from [CBMS05]. Computationally eÔ¨Écient algorithms for generic frame-
works that model many settings in which N may be exponential in the

Exercises
29
natural problem description (such as considering all s-t paths in a graph or
all binary search trees on n elements) were developed in [KV03, Zin03].
The notion of internal regret and its connection to correlated equilib-
rium appear in [FV98, HMC00], and more general modiÔ¨Åcation rules were
considered in [Leh03]. A number of speciÔ¨Åc low internal regret algorithms
were developed by [FV97, FV98, FV99, HMC00, CBL03, BM05, SL05]. The
reduction in Section 4.5 from external to swap regret is from [BM05].
Algorithms with strong external regret bounds for the partial information
model are given in [ACBFS02], and algorithms with low internal regret
appear in [BM05, CBLS06]. The reduction from full information to partial
information in Section 4.6 is in the spirit of algorithms of [AM03, AK04].
Extensions of the algorithm of [KV03] to the partial information setting
appear in [AK04, MB04, DH06]. The results in Section 4.7 on approaching
Nash equilibria in routing games are from [BEL06].
Bibliography
[ACBFS02] Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire.
The nonstochastic multiarmed bandit problem. SIAM Journal on Computing,
32(1):48‚Äì77, 2002.
[AK04] Baruch Awerbuch and Robert D. Kleinberg. Adaptive routing with end-to-
end feedback: distributed learning and geometric approaches. In STOC, pages
45‚Äì53, 2004.
[AM03] Baruch Awerbuch and Yishay Mansour.
Adapting to a reliable network
path. In PODC, pages 360‚Äì367, 2003.
[BEL06] Avrim Blum, Eyal Even-Dar, and Katrina Ligett. Routing without regret:
On convergence to nash equilibria of regret-minimizing algorithms in routing
games. In PODC, 2006.
[BEY98] Allan Borodin and Ran El-Yaniv. Online Computation and Competitive
Analysis. Cambridge University Press, 1998.
[BM05] Avrim Blum and Yishay Mansour. From external to internal regret. In
COLT, 2005.
[CBFH+97] Nicol`o Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haussler,
Robert E. Schapire, and Manfred K. Warmuth. How to use expert advice.
Journal of the ACM, 44(3):427‚Äì485, 1997.
[CBL03] Nicol`o Cesa-Bianchi and G¬¥abor Lugosi. Potential-based algorithms in on-
line prediction and game theory. Machine Learning, 51(3):239‚Äì261, 2003.
[CBL06] Nicol`o Cesa-Bianchi and G¬¥abor Lugosi. Prediction, Learning and Games.
Cambridge University Press, 2006.
[CBLS06] Nicol`o Cesa-Bianchi, G¬¥abor Lugosi, and Gilles Stoltz. Regret minimiza-
tion under partial monitoring. Math of O.R. (to appear), 2006.
[CBMS05] Nicol`o Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz.
Improved
second-order bounds for prediction with expert advice. In COLT, 2005.
[DH06] Varsha Dani and Thomas P. Hayes. Robbing the bandit: Less regret in
online geometric optimization against an adaptive adversary. In SODA, pages
937‚Äì943, 2006.

30
A. Blum and Y. Mansour
[FL98] Drew Fudenberg and David K. Levine. The theory of learning in games. MIT
press, 1998.
[FS97] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of
on-line learning and an application to boosting. JCSS, 55(1):119‚Äì139, 1997.
[FS99] Yoav Freund and Robert E. Schapire. Adaptive game playing using multi-
plicative weights. Games and Economic Behavior, 29:79‚Äì103, 1999.
[FV97] D. Foster and R. Vohra. Calibrated learning and correlated equilibrium.
Games and Economic Behavior, 21:40‚Äì55, 1997.
[FV98] D. Foster and R. Vohra. Asymptotic calibration. Biometrika, 85:379‚Äì390,
1998.
[FV99] D. Foster and R. Vohra. Regret in the on-line decision problem. Games and
Economic Behavior, 29:7‚Äì36, 1999.
[Han57] J. Hannan. Approximation to bayes risk in repeated plays. In M. Dresher,
A. Tucker, and P. Wolfe, editors, Contributions to the Theory of Games, vol-
ume 3, pages 97‚Äì139. Princeton University Press, 1957.
[HMC00] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to corre-
lated equilibrium. Econometrica, 68:1127‚Äì1150, 2000.
[KV03] Adam Kalai and Santosh Vempala. EÔ¨Écient algorithms for online decision
problems. In COLT, pages 26‚Äì40, 2003.
[Leh03] E. Lehrer. A wide range no-regret theorem. Games and Economic Behavior,
42:101‚Äì115, 2003.
[LW94] Nick Littlestone and Manfred K. Warmuth. The weighted majority algo-
rithm. Information and Computation, 108:212‚Äì261, 1994.
[MB04] H. Brendan McMahan and Avrim Blum.
Online geometric optimization
in the bandit setting against an adaptive adversary. In Proc. 17th Annual
Conference on Learning Theory (COLT), pages 109‚Äì123, 2004.
[Owe82] Guillermo Owen. Game theory. Academic press, 1982.
[SL05] Gilles Stoltz and G¬¥abor Lugosi. Internal regret in on-line portfolio selection.
Machine Learning Journal, 59:125‚Äì159, 2005.
[ST85] D. Sleator and R. E. Tarjan. Amortized eÔ¨Éciency of list update and paging
rules. Communications of the ACM, 28:202‚Äì208, 1985.
[Zin03] Martin Zinkevich. Online convex programming and generalized inÔ¨Ånitesimal
gradient ascent. In Proc. ICML, pages 928‚Äì936, 2003.

