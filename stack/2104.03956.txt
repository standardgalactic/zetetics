Just Label What You Need: Fine-Grained Active Selection for
Perception and Prediction through Partially Labeled Scenes
Sean Segal12*, Nishanth Kumar3*†, Sergio Casas12, Wenyuan Zeng12
Mengye Ren12, Jingkang Wang12, Raquel Urtasun12
University of Toronto1, Uber ATG2, Brown University3
{seansegal, sergio, wenyuan, mren, wangjk, urtasun}@cs.toronto.edu, nishanth kumar@brown.edu
Abstract
Self-driving vehicles must perceive and predict the fu-
ture positions of nearby actors in order to avoid collisions
and drive safely. A learned deep learning module is of-
ten responsible for this task, requiring large-scale, high-
quality training datasets. As data collection is often sig-
niﬁcantly cheaper than labeling in this domain, the deci-
sion of which subset of examples to label can have a pro-
found impact on model performance. Active learning tech-
niques, which leverage the state of the current model to
iteratively select examples for labeling, offer a promising
solution to this problem. However, despite the appeal of
this approach, there has been little scientiﬁc analysis of ac-
tive learning approaches for the perception and prediction
(P&P) problem. In this work, we study active learning tech-
niques for P&P and ﬁnd that the traditional active learning
formulation is ill-suited for the P&P setting. We thus intro-
duce generalizations that ensure that our approach is both
cost-aware and allows for ﬁne-grained selection of exam-
ples through partially labeled scenes. Our experiments on
a real-world, large-scale self-driving dataset suggest that
ﬁne-grained selection can improve the performance across
perception, prediction, and downstream planning tasks.
1. Introduction
In order for self-driving vehicles to safely plan a route,
they must perceive nearby actors and predict their future lo-
cations. In a self-driving stack, a learned perception and
prediction (P&P) model is responsible for this task, tak-
ing raw sensor data as input and producing object detec-
tions and future predictions. These models typically require
large-scale, high-quality training datasets for best perfor-
mance due to the high dimensional sensor inputs and long
tail of possible outcomes that must be learned. While self-
driving companies collect massive amounts of data from
*Equal Contribution
†Work done while at Uber ATG
Score Unlabeled Examples
Label Best Regions
Train from Partial Supervision
Figure 1. Fine-Grained Active Selection: Train on labeled re-
gions (top), score remaining unlabeled examples (bottom right),
query labels for the highest scoring regions (bottom left).
real-world driving, annotating these scenes remains a major
bottleneck. Furthermore, some of the collected data may be
less interesting for model training – e.g., a prediction dataset
with many parked vehicles is less informative than one with
many highly interactive, moving actors. As a consequence,
the choice of which examples to label is crucial to maximize
performance for a given budget.
Given a particular model, it is natural to ask whether
it can be used to determine examples most likely to im-
prove performance when labeled. This problem is well-
studied in the ﬁeld of active learning and recent work has
shown impressive performance gains over random selec-
tion in many tasks, including image classiﬁcation, semantic
segmentation and 2D object detection [37, 31, 1]. Active
learning presents a promising framework to employ in real-
world self-driving development, where models can contin-
ually improve as new batches of examples are selected iter-
atively for labeling (see Figure 1).
Despite the appeal of active learning, few approaches
have been developed for the self-driving domain. Scientiﬁc
arXiv:2104.03956v1  [cs.CV]  8 Apr 2021

analysis is limited to object detection [13, 9], with no ap-
proaches designed for P&P. When applying active selection
to this task, we ﬁnd the traditional active learning formula-
tion to be ill-suited. First, while approaches typically as-
sume ﬁxed labeling costs per example, annotation costs can
vary drastically as the cost is highly dependent on the num-
ber of actors present. Furthermore, the spatial label struc-
ture can be exploited to support partial labeling, allowing
for ﬁne-grained active selection. Speciﬁcally, with small
modiﬁcations, P&P models can be trained from partial su-
pervision. This enables the active learner to select speciﬁc
actors in a scene without requiring the remaining actors to
be selected, as they may be uninteresting for model im-
provement. With these differences in mind, we introduce a
ﬁne-grained, cost-aware selection along with speciﬁc scor-
ing criteria for active selection in the P&P setting.
We leverage a real-world, large-scale dataset to ana-
lyze the effects of partial labeling and active selection for
P&P. First, we study models trained on partial supervision,
without active selection, and observe that sparse labeling
leads to better performance, while trading off model train-
ing time, as models require more training iterations when
supervision is less dense. Next, we analyze the additional
beneﬁts of ﬁne-grained active selection for prediction and
perception. Our results demonstrate that ﬁne-grained se-
lection with a simple prediction entropy selection criterion
outperforms a variety of popular active learning approaches.
We further analyze prediction performance broken down
by action, demonstrating the gains are most signiﬁcant on
rare events. Ultimately, we evaluate the effects on down-
stream motion planning and show signiﬁcant improvements
for ﬁne-grained active selection. We also see similar im-
provements when optimizing perception performance using
detection entropy for selection. All together, our analysis
suggests that the dominant paradigm of labeling entire self-
driving scenes is not the most efﬁcient use of a ﬁxed label-
ing budget and that more ﬁne-grained active selection may
be required to most effectively select examples for labeling.
2. Related Work
Perception and Prediction:
While perception and pre-
diction have traditionally been handled separately, [21, 6]
introduce models to jointly perform both tasks, improving
performance and efﬁciency. Among the exciting progress
made in both tasks over recent years, most relevant to our
work are improvements in prediction representations, al-
lowing models to better characterize uncertainty. Examples
of representations include trajectories [6, 25], probabilistic
occupancy maps [16, 24], Gaussian mixtures [8], implicit
latent variable models [5], and auto-regressive models [34].
Active Learning:
We focus on pool-based active learn-
ing, in which new training examples are queried from a
large, unlabeled pool [30]. One class of approaches seeks
to characterize model uncertainty, measured via model dis-
agreement [18, 3], entropy [10, 14], a learned loss predic-
tion [37], or a discriminator score [32], and select examples
with high uncertainty for labeling. While often effective,
uncertainty based approaches can also be prone to select-
ing a subset of similar examples when computational con-
straints require large batches of examples to be selected be-
fore retraining. This motivates diversity-based approaches
[23, 29, 12, 11] which seek to ﬁnd a diverse subset of the
unlabeled pool that best characterizes the entire pool. [1, 2]
introduce approaches which balance both uncertainty and
diversity in the selection process. Most related to our do-
main are [13, 9, 33, 26] which study active learning ap-
proaches speciﬁcally for object detection. While most ap-
proaches assume ﬁxed labeling costs per example, [35, 36]
have explored explicitly modeling individual labeling costs
as part of the selection process. Leveraging partially labeled
data for ﬁne-grained active selection has been explored in
semantic segmentation [17, 22] and more generally, in the
context of structured prediction problems [20].
Dataset Selection:
In practice, many self-driving datasets
select examples manually [4], randomly, or via hardcoded
rules.
Argoverse describes rules-based criteria to mine
interesting trajectories for prediction [7]. More recently,
[27] proposed a set of complexity measures for dataset se-
lection. Finally, [28] proposed tagging attributes of self-
driving scenes, enabling log retrieval for dataset curation.
3. Active Learning for P&P
Given the high costs associated with labeling perception
and prediction datasets, there is a signiﬁcant opportunity to
spend budgets more efﬁciently by selecting only the best ex-
amples for labeling. Active learning offers a promising so-
lution, selecting examples predicted most likely to improve
model performance. In this section, we ﬁrst review the tra-
ditional pool-based active learning formulation, where ex-
amples are iteratively selected from an unlabeled pool to in-
crementally build a high-quality labeled dataset. Then, we
address this formulation’s shortcomings in the P&P setting
by introducing a new paradigm which is both cost-aware
and enables ﬁne-grained selection though partially labeled
scenes, providing ﬂexibility to ensure labeling budgets are
spent most effectively. Finally, we provide concrete selec-
tion criteria which can be used within our framework to op-
timize the model’s perception and prediction performance.
3.1. Traditional Active Learning
Self-driving companies typically collect large amounts
of unlabeled real-world data when operating their vehicles.
Our goal is to select the best subset for labeling to improve

Algorithm 1 Active Learning Selection
Input:
Unlabeled pool: XU , Initial labels: X(0)
L
Budget: K, Iterations: N, Model: M
Output:
Final dataset: X(N)
L
, Optimized Model: M
1: for i ∈1 . . . N do
2:
S(i) ←score(M, XU \ X(i−1)
L
)
3:
Q(i) ←select top k(S(i), K)
4:
X(i+1)
L
←X(i)
L ∪Q(i)
5:
M ←train(M, X(i+1)
L
)
6: end for
model performance. We assume access to a large, unlabeled
pool of examples, XU, and an initial subset of labeled ex-
amples, X(0)
L . Each example x ∈XU represents an input to
our model f(x) and if selected, a labeling oracle returns the
ground truth supervision, y = L(x). In the P&P setting,
inputs x represent raw sensor observations and HD Maps,
and labels y represent actor bounding boxes at the current
timestep and for the prediction horizon of T seconds. Each
active learning iteration, we select a subset from the remain-
ing unlabeled examples, Q(i) ⊂XU \ X(i−1)
L
, query the
labeling oracle, and add the examples to our labeled set,
X(i)
L = X(i−1)
L
∪Q(i) .
(1)
At the end of each iteration, the model can be retrained or
ﬁne-tuned with the latest dataset,
D(i) = {(x, y) : x ∈X(i)
L } .
(2)
Traditionally, the active learner will select a ﬁxed num-
ber of examples at each iteration, |Q(i)| = K. This implic-
itly assumes that each example x ∈XU can be labeled for
the same cost, an assumption clearly violated in the P&P
setting, which we will relax in the next section. While a va-
riety of approaches have been studied for active selection,
we focus on methods which produce a scalar score for each
example, S(x) ∈R. Scores represent some notion of infor-
mativeness where highly scored examples are believed to be
most likely to improve model performance. For example,
measures of model uncertainty, such as entropy, are com-
monly used (see Section 3.3 for concrete scoring functions
for P&P). As different models may beneﬁt from different
types of examples, most scoring approaches depend on the
model’s current state. After scores have been computed for
the remaining unlabeled examples, the top K examples can
be selected for labeling. This process repeats for N active
learning iterations and is summarized in Algorithm 1.
3.2. Fine-Grained Cost-Aware Active Learning
In this section, we generalize two aspects critical to the
P&P setting, allowing for variable labeling costs and ﬁne-
grained selection through partial supervision.
Cost-Aware Active Learning:
As a self-driving vehicle
operates, the surrounding environment will change, leading
to scenes with drastically different labeling costs. Crowded
scenes can contain up to hundreds of actors, which are each
traditionally labeled with a precise bounding box. Sparser
scenes, on the other hand, can be labeled with little manual
effort. To account for these differences, we explicitly model
the cost to label each example, C(x). At each iteration,
rather than select a ﬁxed K examples, the learner is instead
given a ﬁxed budget B, which cannot be exceeded,
X
x∈Q(i)
C(x) ≤B .
(3)
This formulation is a generalization of the previous set-
ting, which can be recovered by setting C(x) = 1 for all
examples and B = K. In practice, labeling cost for P&P
examples can be accurately modeled as a linear function of
the number of actors in the scene as most annotation time is
spent drawing detailed bounding boxes for each actor. This
new formulation requires modiﬁcations to our selection al-
gorithm, since high scoring examples S(x) may also have
high costs C(x). Therefore, rather than sorting by score, we
can select examples with the highest value, V (x) = S(x)
C(x).
In practice, the cost of labeling an example C(x) is un-
known until it has been labeled. Therefore, for selection,
we approximate the cost using the number of detections af-
ter NMS as a proxy. Once an example is selected for label-
ing, its true cost is known, and active selection can continue
iteratively until the budget is reached.
Since examples in the P&P setting represent large
scenes, scores S(x) and costs C(x) can vary signiﬁcantly
as scenes can have few to many actors, each contributing to
the total score and cost. As a consequence, coarse-grained
scoring will be suboptimal as scenes may contain regions
with high score and low cost (e.g., a single car performing a
rare U-Turn) and other regions with low score and high cost
(e.g., a parking lot ﬁlled with many static vehicles). This
motivates the need for more ﬁne-grained scoring and selec-
tion. Next, we describe modiﬁcations to support partially
labeled scenes, which will enable ﬁne-grained selection for
better performance in the cost-aware active learning setting.
Partially Labeled Scenes:
We generalize the labeling
process to allow for partial labeling. Along with the ﬂex-
ibility it will provide for active selection, this setting is also
realistic in practice. Even as entire scenes are labeled today,
annotation platforms often decompose work into smaller

40K
60K
80K 100K 120K 140K
Number of Labels
0.72
0.74
0.76
0.78
0.80
0.82
Vehicle mAP@0.7IoU
40K
60K
80K 100K 120K 140K
Number of Labels
1.4
1.5
1.6
1.7
1.8
1.9
meanADE (meters)
r = 1
r = 1/2
r = 1/4
r = 1/6
Figure 2. P&P from Partial Supervision: P&P performance
when trained on partial labels at varying densities, r.
subtasks, which can be more easily distributed and validated
across a labeling team. As a simple extension, platforms
could support querying labels for only particular regions in
the scene. To support partial labels, we redeﬁne an example
xR as the scene augmented with a labeling region R,
xR = (x, R) .
(4)
Given the set of labels for the entire scene y = L(x) and a
region R, each actor’s bounding box label yi will either be
fully contained in R, completely outside of R, or partially
inside of R. For simplicity, we assume that if any part of
the bounding box yi of an actor is inside R then it will be
provided as a label. In practice, this translates to labelers
annotating all actors, even those that are only partially vis-
ible in the queried region of interest. More formally, the
labeling oracle returns labels for an example xR,
L(xR) = {yi : yi ∈R and yi ∈L(x)} .
(5)
Training from Partial Supervision:
We adapt training to
support partial supervision by applying the loss only on the
labeled region, R. Importantly, we do not alter the network
input x, since we do not want to bias the network by chang-
ing the input statistics. Therefore, the forward pass remains
unchanged, ˆy = f(x). Then, when computing the loss, we
only consider the labels that we have received in R,
L(y, ˆy, R) = ℓB(R) +
X
yi∈R
ℓP (yi, ˆyi) .
(6)
Here, ℓP (·, ·) represents traditional multi-task perception
and prediction losses applied over the positive examples in
R and ℓB(R) represents a “background” loss which encour-
ages the network not to output detections for negative re-
gions in R. For example, in our experiments, ℓP includes a
probabilistic prediction loss, a bounding box regression loss
and cross-entropy on positive examples, whereas ℓB(R)
represents the hard negative mining loss, sampling only
negative anchors from R. Please see the supplementary ma-
terials for further details.
40K
60K
80K
100K
120K
140K
Number of Labels
1.4
1.5
1.6
1.7
1.8
1.9
2.0
meanADE (meters)
Random Scenes
Random Regions
Core-Set
LearnLoss
Coarse-Grained
Fine-Grained
Figure 3. Active Learning for Prediction: Performance of vari-
ous selection approaches over N = 5 active learning iterations.
Fine-Grained Selection:
Without restrictions on R, there
are inﬁnite regions to consider for a given scene. Therefore,
in order to efﬁciently score and select regions for labeling,
we consider the set obtained by discretizing the entire scene
into a rectangular grid. Speciﬁcally, we divide each exam-
ple x into HW non-overlapping regions,
xR = (x, Rh,w), h = 1 . . . H, w = 1 . . . W .
(7)
By setting H = W = 1, we obtain a single region for
each scene and recover the original formulation. As H and
W increase, candidate regions become smaller, providing
the learner more ﬁne-grained precision for selection. Most
steps of the selection process can remain unchanged. Scor-
ing functions now operate over examples augmented with
regions, S(xR) returning a score that only considers net-
work predictions in R. Similarly, only the cost of labeling
the queried region C(xR) is incurred when selecting xR.
As regions sizes shrink, we observe that the active
learner is more likely to select a large number of scenes,
each labeled with very sparse supervision.
While this
dataset would contain many interesting actors, we also ﬁnd
this unconstrained selection results in signiﬁcantly longer
training times as examples are less densely labeled. Ad-
ditionally, we observe training instabilities due to the im-
balances between the amount of supervision available for
each example.
To alleviate these issues, we introduce a
sparsity regularizer, which requires that the active learner
select a minimum number of positive examples M for any
selected scene. Therefore, letting P(xR) represent the num-
ber of positive examples in an example, our new formula-
tion can be summarized by the following optimization prob-
lem solved by the learner at each iteration,
max
Q(i)
X
xR∈Q(i)
S(xR)
s.t.
X
xR∈Q(i)
C(xR) ≤B
P(xR) ≥M ∀xR ∈Q(i) .
(8)
We solve this optimization greedily by ﬁrst selecting the
highest scoring scene remaining, then selecting the highest

Prediction (meanADE) ↓
Selection
Straight
Left
Right
Stationary
Random Scenes
2.89
5.31
5.68
0.22
Random Regions
2.46
4.82
4.96
0.20
Core-Set
2.45
4.71
5.01
0.21
LearnLoss
2.46
4.74
4.99
0.21
Coarse-Grained
2.44
4.79
5.03
0.22
Fine-Grained
2.29
4.52
4.91
0.21
Table 1. Prediction Performance By High Level Action
regions in the scene until at least M actors are labeled. We
continue selecting new scenes until the budget is reached.
3.3. Selection Criteria
Our ﬁne-grained, cost-aware active learning formulation
introduced above generally supports any approach which
provides an informativeness score S(xR) per example. In
this work, we focus on uncertainty-based approaches, an
extremely common active learning paradigm based on the
assumption that training on uncertain examples are most
likely to improve future performance. As this approach de-
pends on a model’s characterization of uncertainty, we ﬁrst
describe our probabilistic P&P model, followed by possible
uncertainty-measures that can be used for scoring.
Model:
Following [21], we jointly train a model for both
perception and prediction from LiDAR and HD map inputs.
A model which naturally characterizes uncertainty over pre-
dictions is desirable, as these uncertainty estimates provide
a useful measure of informativeness for scoring. Therefore,
we leverage the output representation of [8], using a mixture
of K Gaussians to represent the distribution of each actor’s
future positions. For simplicity, independence is assumed
between timesteps of the prediction horizon, allowing the
distribution to be factorized over time. Thus the likelihood
of a particular actor trajectory, yi, can be written as,
p(yi) =
K
X
k=1
πk
T
Y
t=1
N
 yi; µt
k, Σt
k

,
(9)
where N is the pdf of a 2D multivariate Gaussian with
parameters µt
k, Σt
k, and πk represent Gaussian mixture
weights. These parameters, for each detected actor, are pre-
dicted by a deep neural network trained with negative log
likelihood (see Section 4 for more details). With knowledge
of the model, we now introduce measures of uncertainty to
use as selection criteria. Due to the multi-task nature of the
task, we present separate selection criteria for the detection
and prediction task. In practice, a mix of both can be used
to ensure performance improves across both tasks.
Detection Entropy:
We focus on characterizing the un-
certainty over the model’s classiﬁcation predictions for each
Selection
Collision ↓L2 Human ↓Lat. acc. ↓Jerk ↓Progress ↑
(% up to 5s)
(m @5s)
(m/s2)
(m/s3) (m @ 5s)
Random Scenes
5.02
5.89
2.80
2.67
33.46
’ Random Regions
5.07
5.71
2.70
2.47
33.65
Core-Set
5.14
5.72
2.65
2.45
33.63
LearnLoss
5.15
5.74
2.68
2.47
33.61
Coarse-Grained
5.17
5.71
2.67
2.44
33.81
Fine-Grained
4.63
5.56
2.62
2.38
33.68
Table 2. Downstream Planning Performance
anchor. For classiﬁcation tasks, the uncertainty is typically
estimated by calculating the entropy of the model’s pre-
dicted probabilities. Given anchors a ∈A with associated
probabilities pa, the entropy of the predictions are given by,
HD(A) = −
X
a∈A
pa log pa + (1 −pa) log(1 −pa) . (10)
To compute a region’s score, we assume independence be-
tween anchors and sum the entropies of anchors in R.
Prediction Entropy:
Computing prediction entropy nat-
urally depends on the output representation of the model.
Our model outputs a Gaussian mixture for each predicted
actor. Unfortunately, there is no known closed form solu-
tion to computing this distribution’s entropy [15]. There-
fore, we are required to estimate the entropy via approxi-
mations. We explored various approximations, including a
sample-based monte-carlo estimate, but all performed sim-
ilarly to or worse than an approximation via the entropy of
the discrete categorical distribution induced by the mixture
weights πk,
HP (yi) = −
X
πk
πk log πk .
(11)
Intuitively, this approximation is well-suited to capture
cases where the model is uncertain between multiple possi-
ble modes, which is likely representative of the true entropy
of distribution. We use this approximation due to its sim-
plicity and computational efﬁciency while providing sim-
ilar performance. Finally, assuming independence across
predicted actors, we sum the entropies of all predictions in
a region to obtain the ﬁnal score for the region.
4. Experiments
In this section, we analyze the effects of partial labeling
and ﬁne-grained active selection. First, we explore partial
labeling independent of active selection and observe signif-
icant beneﬁts from sparsely labeled datasets under a ﬁxed
labeling budget. Next, we explore the improvements pro-
vided by active selection for prediction.
We ﬁnd that a
simple prediction entropy combined with ﬁne-grained ac-
tive selection outperforms various traditional scene-based

Figure 4. Selection Statistics: Statistics of the labels selected by each active selection approach at the ﬁnal active learning iteration.
approaches. More detailed analysis shows that ﬁne-grained
selection enables the learner to better oversample labels ex-
hibiting complex driving behaviors, resulting in better per-
formance on these challenging behaviors in the test-set.
In practice, we are most interested in the effects of these
improvements on the downstream motion planning task,
where we ﬁnd signiﬁcant improvements across most met-
rics. Finally, we observe similar improvements for percep-
tion when using detection entropy as the selection criterion.
Overall, our experiments demonstrate that partial labeling
and ﬁne-grained active selection signiﬁcantly improve de-
tection, prediction and downstream planning performance.
Dataset:
For our experiments, we leverage a real-world
large-scale self-driving dataset collected across multiple
cities in North America. To simulate the active learning set-
ting, we follow standard practice in active learning research
and treat the large labeled dataset as if it were an unlabeled
pool. Then, active learning approaches select from this pool
containing 100K scenes, with roughly 2 million actors. For
each scene, we have access to LiDAR sweeps recorded at
10Hz with a localized HD map given as input to the model.
Metrics:
We evaluate perception and prediction perfor-
mance at each active learning iteration. The model is re-
trained on its current set of labels and evaluated on a seper-
ate held-out test set. The test-set is held constant across
all approaches and contains traditional fully labeled scenes.
While there exist many metrics for P&P, we focus on
mAP@0.7 for detection and meanADE for prediction as
they are most commonly employed. Additionally, we in-
clude metrics divided by high level action as well as the
performance of the downstream planning task.
Implementation Details:
For model implementation, we
follow the exact details of the Gaussian mixture baseline
from [5], an implementation of MTP [8] for the joint per-
ception and prediction setting. To support partial labeling,
we ﬁnd it is necessary to use sum instead of mean to reduce
losses in a batch, ensuring actors in less densely labeled
scenes are not up-weighted relative to those in more densely
labeled scenes. All models are trained for 50 epochs, us-
ing budgeted training [19] for the learning rate schedule.
For region scoring and selection, we use H = W = 20
to discretize the entire scene into 400 rectangular regions.
Finally, when using sparsity regularization, we set M = 5.
4.1. P&P from Partial Supervision
Experimental Setup:
To test the effects of partial label-
ing, we randomly select labels at varying levels of ground-
truth density per scene. To ensure a fair comparison, the
total labeling budget is ﬁxed across densities. Speciﬁcally,
let r be the labeling density. When r = 1, we recover the
traditional fully labeled setting. When r = 1
2, we ﬁrst sam-
ple scenes randomly and then sample regions from half of
each scene, providing labels only for selected regions. No-
tice that since we ﬁx the labeling budget, selecting at lower
densities r, will result in more scenes for the same budget.
Results:
The effect of partial labeling on both detection
and prediction performance is shown in Figure 2. We ﬁnd
that across all dataset sizes, there is a signiﬁcant increase in
performance with lower density datasets. Beneﬁts appear
to saturate around r =
1
4, as the sparser labeling density
r = 1
6 does not provide further improvements. Performance
gains can be explained by the fact that more sparsely labeled
datasets naturally include supervision from more scenes,
improving the model’s ability to generalize.
Our results
suggest to optimize P&P performance under a ﬁxed labeling
budget, the best strategy is to sparsely label scenes rather
than label them in their entirety. However, we note that in
practice there is a tradeoff between labeling at lower den-
sities and model training times, as datasets with less dense
supervision must be trained for more iterations. We explore
this further in our sparsity regularization experiments.
4.2. Fine-Grained Active Selection for Prediction
Experimental Setup:
In this experiment, we evaluate
active learning approaches to improve prediction perfor-
mance.
For each method, we sample an initial labeled
set X(0)
L
of 40K vehicles from XU. To ensure that ﬁnal
datasets contain scenes with similar density of supervision,
the initial data for ﬁne-grained methods are partially la-
beled scenes, whereas coarse-grained approaches sample
full scenes. For fair comparison, we ﬁx the labeling bud-
get at each active learning iteration. Speciﬁcally, for each
of the N = 5 active learning iterations, the learner is given
a budget of B = 20K vehicles.

Figure 5. Qualitative Examples: Regions selected by ﬁne-grained active selection. We visualize all labels and color those in regions
selected by the active learner in red. Selected regions tend to have moving vehicles performing interesting actions (e.g., U-Turns).
Additional Baselines:
We compare ﬁne-grained active
selection to full scenes selected randomly (Random
Scenes), partially labeled scenes at density r = 1
4 selected
randomly (Random Regions), full scenes selected by
prediction entropy (Coarse-Grained), and two addi-
tional active learning baselines adapted to the P&P set-
ting. We compare against a recent uncertainty-based ap-
proach which learns to predict the loss of unlabeled exam-
ples, which we refer to as LearnLoss [37]. We train the
loss prediction module to predict only the prediction loss
(not perception related losses) from the model’s interme-
diate features and re-tune hyperparameters, resulting in a
margin ξ = 1.0 and loss prediction weight λ = 0.001. We
additionally compare against the common diversity-based
approach of Core-Set selection [29]. Rather than score
examples independently, the method seeks to select a rep-
resentative sample based on distances between examples in
a learned space. Following common practice, we leverage
the learned feature representations of the network to com-
pute distances between examples. For efﬁciency, we lever-
age the k-Greedy center variant of the algorithm from the
paper, which we ﬁnd is most commonly used in practice.
Prediction Performance:
Results are shown in Figure 3.
All active selection techniques offer signiﬁcant improve-
ments over random selection. Interestingly, despite large
differences in the selection criteria (e.g., uncertainty-based
vs. diversity-based), scene-based approaches achieve simi-
lar performance, indicating that gains may be saturated due
to the inﬂexibility of selecting entire scenes. Surprisingly,
simply labeling random regions appears to perform better
than or similar to many of the coarse-grained active learning
approaches. Finally, ﬁne-grained selection offers the best
performance. While the improvements may appear to be
relatively small, we recall that aggregate prediction metrics
are averaged over more than 1M actors in the test-set and
may hide large differences between the speciﬁc behaviors
of the prediction models, calling for more detailed analysis.
Performance By High Level Action:
The advantages of
ﬁne-grained selection becomes more apparent through more
detailed metrics. In Table 1, we break down the predic-
tion performance by action: driving straight, turning left,
turning right, and stationary. We notice that differences be-
tween selection algorithms become more apparent across
actions associated with more difﬁcult predictions (i.e., all
non-stationary actions). These results are explained by the
fact that ﬁne-grained entropy selects more unpredictable
moving actors compared to other selection approaches.
Selection Statistics:
Figure 4 contains histograms of the
statistics of the labels selected by each method com-
puted at the ﬁnal iteration of active learning. Core-Set
and LearnLoss are omitted due to similarities with
Coarse-Grained. For each approach, we compute the
histograms based on label metadata, including their high
level action (driving straight, left, right, stationary), the ve-
hicle speed, distance to the SDV, and the number of LiDAR
points contained inside the bounding box. As expected, we
notice that active-selection methods tend to sample more
non-stationary vehicles and vehicles further from the SDV.
This effect is more apparent for ﬁne-grained selection meth-
ods due to the additional ﬂexibility provided by the partially
labeled setup. One potential downside of ﬁne-grained se-
lection is that it will be biased towards regions with actors
detected by the current model. While this leads to sampling
more visible labels (i.e., labels with more LiDAR points),
we do not see this affect model performance.

40K
60K
80K
100K
120K
140K
Number of Labels
1.4
1.5
1.6
meanADE (meters)
40K
60K
80K
100K
120K
140K
Number of Labels
25%
50%
75%
100%
Scenes w/ Label
Unconstrained
Sparisty Regularized
Figure 6. Sparsity Regularization: The effect of sparsity regular-
ization on performance (top) and training time (bottom).
Planning Performance:
Following [5],
we evaluate
downstream performance on motion planning, computing
the collision rate, L2 error, lateral acceleration, jerk, and
progress of a planner which utilizes the predictions from
each trained model, shown in Table 2. We notice that ﬁne-
grained selection outperforms all baselines across all plan-
ning metrics, except progress, for which the differences are
not signiﬁcant. This demonstrates that, given a ﬁxed budget,
ﬁne-grained active selection can improve downstream plan-
ning, which is ultimately most important for self-driving.
Qualitative Examples:
Figure 5 shows qualitative exam-
ples of regions selected from ﬁne-grained selection. Se-
lected regions tend to include vehicle labels (shown in red)
with moving actors, actors at intersections, or actors per-
forming odd maneuvers (e.g., U-Turn in the top-right ex-
ample). Parked and non-moving actors are rarely selected.
Sparsity Regularization Ablation:
In Figure 6, we ab-
late the effect of sparsity regularization on performance and
number of scenes selected. At early iterations, we ﬁnd un-
constrained selection outperforms the sparsity regularized
approach. This is expected, as the unconstrained approach
has the freedom to select a larger set of scenes, each with
less supervision. However, at later iterations, we observe
the unconstrained selection performance degrades relative
to sparsity constrained selection. We believe this is caused
by the imbalances between the supervision available for
each scene in the unconstrained selection dataset, as we
have found empirically that these imbalances can lead to
degraded performance. Beyond performance, there is an
additional, perhaps more important, beneﬁt of sparsity reg-
ularization. Since the active learner must select at least M
actors per scene, the number of scenes in the dataset grows
linearly with each iteration. Alternatively, in the uncon-
strained approach, the dataset size explodes at early itera-
tions until there is at least one label for every scene in XU.
40K
55K
70K
85K
100K
115K
130K
145K
Number of Labels
0.56
0.58
0.60
0.62
0.64
0.66
mAP (Multiclass)
Random Scenes
Random Regions
Coarse-Grained
Fine-Grained
Figure 7. Active Learning for Perception: Multi-class detection
performance for N = 7 iterations of Active Learning.
4.3. Fine-Grained Active Selection for Perception
Experimental Setup:
We additionally experiment with
leveraging detection entropy to select examples most likely
to improve perception performance. We follow a similar
experimental setup, replacing prediction entropy with de-
tection entropy for selection. As all methods perform simi-
larly when evaluated only on vehicle detection, we evaluate
on the more challenging multi-class setting where cyclists
and pedestrians must be detected.
Results:
Similar to the prediction setting, results in Fig-
ure 7 show ﬁne-grained selection is most effective. Interest-
ingly, the performance of coarse-grained selection is similar
to random scenes, likely explained by an averaging effect of
summing the entropy of predictions over the entire scene.
5. Conclusion
In this paper, we studied active learning techniques to
intelligently select examples to label from large collections
of unlabeled self-driving data logs for perception and pre-
diction models. We found the traditional active learning
setting ill-suited and introduced generalizations to account
for variable labeling costs and enable ﬁne-grained selec-
tion through partially labeled scenes. In our experiments,
we found signiﬁcant improvements from partial labeling
without any active selection, and further gains across per-
ception, prediction and downstream planning by leveraging
ﬁne-grained active selection. Our results demonstrate that
the dominant paradigm of labeling entire self-driving scenes
may not be most efﬁcient under a ﬁxed budget and that ﬁne-
grained selection is likely required for maximal efﬁciency.
In practice, the best labeling policy should not only opti-
mize performance under a ﬁxed labeling budget, but also
account for model training times, generalization to new ar-
chitectures, and robustness to long-tailed events. We hope
our analysis inspires future work on more complex selection
criteria designed for these additional considerations.

References
[1] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan
Arora. Contextual diversity for active learning. In ECCV,
pages 137–153. Springer, 2020. 1, 2
[2] Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy,
John Langford, and Alekh Agarwal. Deep batch active learn-
ing by diverse, uncertain gradient lower bounds. ICLR, 2020.
2
[3] William H Beluch, Tim Genewein, Andreas N¨urnberger, and
Jan M K¨ohler. The power of ensembles for active learning in
image classiﬁcation. CVPR, pages 9368–9377, 2018. 2
[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,
Giancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-
timodal dataset for autonomous driving.
In CVPR, pages
11621–11631, 2020. 2
[5] Sergio Casas, Cole Gulino, Simon Suo, Katie Luo, Renjie
Liao, and Raquel Urtasun. Implicit latent variable model for
scene-consistent motion forecasting. ECCV, 2020. 2, 6, 8
[6] Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet:
Learning to predict intention from raw sensor data. In CoRL,
pages 947–956. PMLR, 2018. 2
[7] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter
Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d
tracking and forecasting with rich maps. In CVPR, pages
8748–8757, 2019. 2
[8] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,
Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-
der, and Nemanja Djuric. Multimodal trajectory predictions
for autonomous driving using deep convolutional networks.
In ICRA, pages 2090–2096. IEEE, 2019. 2, 5, 6
[9] Di Feng, Xiao Wei, Lars Rosenbaum, Atsuto Maki, and
Klaus Dietmayer. Deep active learning for efﬁcient training
of a lidar 3d object detector. In IV, pages 667–674. IEEE,
2019. 2
[10] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep
bayesian active learning with image data. In ICML, pages
1183–1192. PMLR, 2017. 2
[11] Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi,
and Sotaro Tsukizawa.
Deep active learning for biased
datasets via ﬁsher kernel self-supervision. In CVPR, pages
9041–9049, 2020. 2
[12] Yuhong Guo. Active instance sampling via matrix partition.
In NIPS, pages 802–810, 2010. 2
[13] Elmar Haussmann, Michele Fenzi, Kashyap Chitta, J´an Iva-
neck´y, Hanson Xu, D. Roy, A. Mittel, Nicolas Koumchatzky,
C. Farabet, and Jose M. Alvarez. Scalable active learning for
object detection. ArXiv, abs/2004.04699, 2020. 2
[14] Alex Holub, Pietro Perona, and Michael C Burl. Entropy-
based active learning for object recognition. In CVPR Work-
shops, pages 1–8. IEEE, 2008. 2
[15] Marco F Huber, Tim Bailey, Hugh Durrant-Whyte, and
Uwe D Hanebeck. On entropy approximation for gaussian
mixture random vectors.
In MFI, pages 181–188. IEEE,
2008. 5
[16] Ajay Jain, Sergio Casas, Renjie Liao, Yuwen Xiong, Song
Feng, Sean Segal, and Raquel Urtasun.
Discrete residual
ﬂow for probabilistic pedestrian behavior prediction.
In
CoRL, pages 407–419. PMLR, 2020. 2
[17] Tejaswi Kasarla, Gattigorla Nagendar, Guruprasad M Hegde,
Vineeth Balasubramanian, and CV Jawahar. Region-based
active learning for efﬁcient labeling in semantic segmenta-
tion. In WACV, pages 1109–1117. IEEE, 2019. 2
[18] Balaji Lakshminarayanan, Alexander Pritzel, and Charles
Blundell.
Simple and scalable predictive uncertainty esti-
mation using deep ensembles. NIPS, 2017. 2
[19] Mengtian Li, Ersin Yumer, and Deva Ramanan. Budgeted
training: Rethinking deep neural network training under re-
source constraints. ICLR, 2020. 6
[20] Wenjie Luo, Alex Schwing, and Raquel Urtasun.
Latent
structured active learning. NIPS, 26:728–736, 2013. 2
[21] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-
ous: Real time end-to-end 3d detection, tracking and motion
forecasting with a single convolutional net. In CVPR, pages
3569–3577, 2018. 2, 5
[22] Radek Mackowiak, Philip Lenz, Omair Ghori, F. Diego,
Oliver Lange, and C. Rother. Cereals - cost-effective region-
based active learning for semantic segmentation. In BMVC,
2018. 2
[23] Hieu T Nguyen and Arnold Smeulders. Active learning using
pre-clustering. In ICML, page 79, 2004. 2
[24] Geunseob Oh and Jean-Sebastien Valois.
Hcnaf: Hyper-
conditioned neural autoregressive ﬂow and its application for
probabilistic occupancy map forecasting. In CVPR, pages
14550–14559, 2020. 2
[25] Tung Phan-Minh, E. Grigore, F. Boulton, Oscar Beijbom,
and Eric M. Wolff. Covernet: Multimodal behavior predic-
tion using trajectory sets. CVPR, pages 14062–14071, 2020.
2
[26] Soumya Roy, Asim Unmesh, and Vinay P Namboodiri. Deep
active learning for object detection. In BMVC, page 91, 2018.
2
[27] Abbas Sadat, Sean Segal, Sergio Casas, James Tu, Bin Yang,
Raquel Urtasun, and Ersin Yumer. Diverse complexity mea-
sures for dataset curation in self-driving.
arXiv preprint
arXiv:2101.06554, 2021. 2
[28] Sean Segal, Eric Kee, Wenjie Luo, Abbas Sadat, Ersin
Yumer, and Raquel Urtasun.
Universal embeddings for
spatio-temporal tagging of self-driving logs. CoRL, 2020.
2
[29] Ozan Sener and Silvio Savarese. Active learning for convo-
lutional neural networks: A core-set approach. ICLR, 2018.
2, 7
[30] Burr Settles. Active learning literature survey. 2009. 2
[31] Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Vari-
ational adversarial active learning. In ICCV, pages 5972–
5981, 2019. 1
[32] Samarth Sinha, S. Ebrahimi, and T. Darrell. Variational ad-
versarial active learning. ICCV, pages 5971–5980, 2019. 2
[33] Sayanan Sivaraman and Mohan M Trivedi. Active learning
for on-road vehicle detection: A comparative study. Machine
vision and applications, 25(3):599–611, 2014. 2

[34] Yichuan Charlie Tang and Ruslan Salakhutdinov. Multiple
futures prediction. In NeurIPS, 2019. 2
[35] Gaoang Wang, Jenq-Neng Hwang, Craig Rose, and Farron
Wallace. Uncertainty-based active learning via sparse mod-
eling for image classiﬁcation. TIP, 28(1):316–329, 2018. 2
[36] Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and
Liang Lin.
Cost-Effective Active Learning for Deep Im-
age Classiﬁcation. TCSVT, 27(12):2591–2600, Dec. 2017.
arXiv: 1701.03551. 2
[37] Donggeun Yoo and In So Kweon. Learning loss for active
learning. In CVPR, pages 93–102, 2019. 1, 2, 7

