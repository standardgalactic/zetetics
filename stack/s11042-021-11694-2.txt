Vol.:(0123456789)
https://doi.org/10.1007/s11042-021-11694-2
1 3
Deep semantic space guided multiâ€‘scale neural style transfer
JiachenÂ Yu1Â Â· LiÂ Jin1Â Â· JiayiÂ Chen2Â Â· YouziÂ Xiao1Â Â· ZhiqiangÂ Tian1Â Â· XuguangÂ Lan2
Received: 29 July 2019 / Revised: 21 December 2020 / Accepted: 25 October 2021 
Â© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2021
Abstract
This paper mainly studies the Neural Style Transfer (NST) problem based on convolutional 
neural networks (CNN). Existing deep style migration algorithms do not mimic the styles 
to a reasonable position. To solve the problem, this paper proposes a multi-scale style 
transfer algorithm based on deep semantic matching. For purpose of guiding the correct 
migration of the style, we use the priori spatial segmentation and illumination information 
of the input image to integrate the deep semantic information. First, we find that spatial 
division and illumination analysis are two important visual understanding approaches for 
artists to make each painting decision. In order to simulate these two visual understand-
ing approaches, this paper defines the DSS (deep semantic space), which contains spatial 
segmentation and contextual illumination information. The semantic exists in the form 
of CNN (convolution neural network) characteristic graph. Second, we propose a deep 
semantic loss function based on DSS matching and nearest neighbor search to optimize 
the effect of deep style migration. Third, we propose a multi-scale optimization strategy for 
improving the speed of our method. The experiments show that our method can reasonably 
synthesize images in spatial structures. The placement of each style is more reasonable and 
has a good visual aesthetic.
Keywordsâ€‚ Neural style transferÂ Â· Image segmentationÂ Â· Illumination estimationÂ Â· Patch 
matching
1â€‚ Introduction
Style transfer [22] allows the computer to automatically complete the reproduction of artis-
tic style [7, 14], transferring the style of a classic art work to daily photos, so that the pho-
tos retain the original content and present a unique artistic style [8â€“10, 12] (Fig.Â 1). In real 
life, style transfer has many application scenarios. Style transfer can promote the develop-
ment of animation and illustration design, and help non-professional users to freely create 
works with artistic style.
 *	 Li Jin 
	
19138682@qq.com
1	
School ofÂ Software Engineering, Xiâ€™an Jiaotong University, Xiâ€™an, China
2	
Institute ofÂ Artificial Intelligence andÂ Robotics, Xiâ€™an Jiaotong University, Xiâ€™an, China
Published online: 25 November 2021
Multimedia Tools and Applications (2022) 81:3915â€“3938
/

1 3
The traditional style transfer method can be regarded as an extension of the non-pho-
torealistic rendering (NPR) [7, 16, 34], or the texture transfer [10] process of the image. 
In recent years, the development of deep learning [33] has made a breakthrough in style 
transfer [14, 23, 26, 29, 33, 35]. The neural network enables the extraction of artistic 
styles, which makes it easier to convert daily photos into artistic photos. For example, 
Gatys etÂ al. [14] use the convolutional neural network (CNN) to reconstruct the content 
and style, and optimize the stylized image iteratively based on a loss function. From 
then on, CNN-based neural style transfer become a hot topic.
The research in image style transfer mainly focuses on the following issues: artis-
tic composition and computing speed. However,the effect of style transfer is affected 
by many factors. Lack of consideration of the structure in the input image will lead to 
wrong placement of style texture, and the loss of the illumination of the image will lead 
to unsatisfactory synthesis results.
This paper discuss two important visual understanding mechanisms: spatial and illu-
mination, and incorporate them into the process of style transfer. We add these prior 
information into DSS (deep semantic space) and propose a new loss term-Ll
DSS to 
improve the spatial rationality of the output image. On the basis, we propose a DMS-MS 
method to improve image synthesis quality, and experiments prove the guiding effect 
of spatial and illumination on style transfer to make the placement of art texture more 
reasonable. In addition, we propose a multi-scale iteration strategy, which significantly 
speeds up the algorithm speed of Online-style transfer. â€œOnline-style transferâ€ does not 
need to pre-train a model to represent a particular style, which directly conducts multi-
ple algorithm iterations to generate target stylized image.
The main contributions of this paper is presented as follows: Firstly, the guiding 
role of spatial segmentation and illumination analysis in the process of real painting 
is analyzed. Secondly, the DSS (deep semantic space) is defined to simulate two kinds 
of visual understanding mechanism. Then, Our DSM-MS algorithm is proposed, which 
enables the art style to be effectively guided by space segmentation and illumination 
information in the transfer. Finally yet importantly, the multi-scale iterative strategy 
for the target image is proposed, which greatly reduces the total computing time of the 
Online-NST algorithm.
Style image
Source image
Other priori
information
Image artistic
style transfer
Input
Output
Composite image
Fig.â€¯1â€‚ â€‰The frame diagram for the transfer of image art style
3916
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
2â€‚ Related work
Early graphic art stylization is a non - realistic rendering problem [7, 16, 34]. These meth-
ods need to use the pixel constraint of the source image, and arrange the strokes logically 
on the blank canvas to gradually form artworks [3, 6, 36, 37]. Another research direction 
is to define the stroke attributes, Hertzmann proposes a method to form strokes based on 
the color and gradient direction of the source image [19]. Zeng etÂ al. use the semantic 
information of the image to determine the position and direction of the stroke [40]. In this 
case, semantic information includes the estimation of special area, object identification, 
direction field, and illumination direction. However, these methods lack efficiency and flex-
ibility, because their modeling system is very complex.
Classic style transfer or texture transfer methods are parameterless and policy-based [1, 
9, 25]. Policy-based method is to propose a stylization strategy to protect the content and 
structure of the source image in a variety of ways. This method focuses on specific stylized 
strategy to limit the selection and placement of texture image blocks, so as to realize the 
protection of the content and spatial structure of the source image. Efros and Freemand use 
various prior maps extracted from source images (statistical pixels) to restrict the selection 
and placement of texture image blocks [9]. Ashikhmin etÂ al. take artistic style as high-fre-
quency information of images, and keep low-frequency information of source images while 
transferring styles [1]. The output image of this kind of methods maintains good content 
and structure of the source image, but the overall visual sense of art is not strong. This is 
because pixel-based or block-based migration can only simulate simple brush textures, not 
more advanced art styles.
Gatys etÂ al. firstly utilize the strong feature extraction ability of convolutional neural 
network to realize the extraction and use of complex artistic style [14] and turn on the NST 
(neural style transfer) genre. Using deep neural network to extract and use advanced style 
elements can improve the artistic sense of the output image. The loss functions of the task 
is divided two parts, the content loss and the style loss. The content loss minimizes the 
distance of the content image and stylized image in feature space. The style loss is defined 
based on the difference between two Gram matrices from style image and stylized image. 
Li etÂ al. proves that the essence of the style loss function is the maximum mean discrep-
ancy (MMD) between the data of two characteristic layers [27]. Although NST algorithm 
solves the limitations of the traditional method, it lacks the ability to preserve low-level 
features, making the composite image not ideal. In order to improve the synthetic effect, 
the researchers explored from increasing loss items and modifying strategies respectively. 
Risser etÂ al. consider that high art style could be measured by the mean and variance of fea-
ture layer [38]. Liu etÂ al. hope to restrict the placement of certain artistic styles by making 
the source image and the target image have similar Depth maps [30]. Gatys etÂ al. attempt to 
mix multiple works of a painter to construct the style loss function [15].
Nowadays, the Generative Adversarial Networks (GAN) are widely used in image-to-
image translation, which is similar to the neural style transfer. And the Online-NST method 
cannot meet the requirement of application in real-time scenes. Johnson etÂ al. add an image 
generation network before the loss model proposed by Gayts. Before the target is synthe-
sized, a generation network is trained for each style image [23]. Some websites and mobile 
phone applications with deep style transfer (for example, Prisma) use this method [4, 31]. 
Ulyanov etÂ al. replace the image generation network with a multi-scale generation network 
in the original frame to achieve optimization [35]. However, it takes about 3.5Â h for such 
methods to train a style network, and they cannot choose their own artistic style.
3917
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
In addition to the method of using Gram matrix statistics as complex artistic styles, 
scholars have also made other attempts. Li etÂ  al. use the depth feature blocks of style 
images (Markov Random Field) to replace the Gram matrix as a high-level art style [26]. 
Liao etÂ al. propose a depth image analogy framework [29]. These algorithms significantly 
reduce the visual clutter, but they can cause errors if the structure of style image is not as 
same as the source image. Moreover, the algorithms may mistakenly transfer the small-
scale content of the style image to the target as the style texture, resulting in the wrong 
content in the composite image (e.g. character facial deformation).
Through the above discussion, traditional methods based on pixel statistics and image 
block matching are unable to extract and use complex artistic styles, but they can preserve 
spatial structures with reasonable styles. On the contrary, the neural style transfer method 
can extract and use advanced artistic styles, but the details of content image cannot be pre-
served due to the lack of low-level features.By considering the advantages of traditional 
methods and neural style transfer method, we add a new DSS loss term to optimize the 
effect of style transfer. At the same time, a multi-scale iterative strategy is proposed accel-
eration our algorithm. Itâ€™s worth mentioning that we still use the Gram matrix statistics to 
represent advanced artistic styles in this paper. Compared with other algorithms, our model 
can also guarantee the spatial rationality of the synthesis effect when facing some special 
input images, which ensures the application value of the model in the industry.
The rest of this paper will be organized as follows. We introduce our DSM-MS algo-
rithm in SectionÂ 3. In SectionÂ 4, we compare the output of our algorithm with other algo-
rithms. Finally, we conclude our work in the SectionÂ 5.
3â€‚ Methods
3.1â€‚ Overview ofÂ theÂ proposed method
The proposed method consists of three parts that include the calculation of DSS, the DSS-
guided deep semantic loss function, and the multi-scale iteration strategy (Fig.Â 2).
In this algorithm, we define a Deep Semantic Space (DSS) containing spatial segmenta-
tion and contextual illumination information. In real life, the artist needs to observe and 
distinguish different objects in the scene before painting, and then divide the scene ele-
ments and analyze the lighting situation. On this basis, the artist controls the brush to draw 
scene elements and express the due texture and shape of each object, thus making the ele-
ments in the scene come alive. Deep semantic space is to integrate these two important 
understanding mechanisms in painting, and logically guide the placement of style texture 
to improve the spatial rationality of the composite image.
Based on the spatial segmentation and illumination information contained in the 
above deep semantic space, the deep semantic loss function is defined, and it realize the 
reasonable migration of the style of DSS guided brushwork. Among them, in the deep 
semantic mapping function, the nearest neighbor space is obtained by using similar-
ity matching between content semantics and style semantics. Furthermore, the deforma-
tion of style features is introduced, which greatly improve the computational efficiency. 
Content loss [14] proposed by Gatys etÂ al. and style loss based on Gram statistics are 
integrated, and DSS loss term is added in the loss function. The result of work synthesis 
is better than the method proposed by Gatys etÂ al. After solving the spatial rationality 
3918
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
of the synthesis effect, the proposed multi-scale iteration strategy greatly improves the 
algorithm speed when synthesizing high-resolution images.
VGG-19
VGG-19
VGG-19
Feature map
relu3_1deform
Feature map
relu4_1deform
Content loss
DSS loss
DSS matching
VGG-19
DSS matching
DSS matching
VGG-19
Feature map
relu2_1deform
Deep semantic matching
Style loss
Fig.â€¯2â€‚ â€‰The framework of our DSM algorithm
3919
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
3.2â€‚ Deep semantic space
3.2.1â€‚ Probing intoÂ theÂ mechanism ofÂ painting understanding
Before the introduction of DSS, we need to understand the painting mechanism, so that we 
can better determine what needs to be extracted in the style picture. Ideally, styles would 
need to be properly integrated with the structure of the source image space in their migra-
tion to give the results an artificial sense of art. The rationality and internal logic of the 
painting process is guided by human visual perception. Therefore, the key to improve the 
synthetic image space rationality is to integrate the visual understanding mechanisms of 
artistic creation process into the migration process, and logically lead the placement of 
style texture. In this article, we discuss only two important understanding mechanisms.
1)	 Spatial Segmentation: Distinguish between objects, obtain shape and context of object, 
etc. The above information guides the painter to control the motion of the brush to 
represent the due texture and shape of each object, or to guide the painter to control the 
use of color to paint different objects.
2)	 Illumination Analysis: Lighting conditions include the projection area of natural light on 
an object, the location of highlights and shadows, etc. When an artist paints an object, 
this information guides the artist to control the motion of the brush and the choice of 
color diversity.
3.2.2â€‚ The calculation ofÂ DSS
Deep Semantic Space (DSS) is defined, which contains spatial segmentation and contex-
tual illumination information. The semantic exists in the form of convolution neural net-
work characteristic graph. And in this section, we give the complete DSS calculation pro-
cess of an oil painting image (Fig.Â 3) to make the method more intuitive.
As we mentioned in the previous section, the artist will properly change the use of 
brushstroke textures and colors when painting different objects. Therefore, it is a natural 
Fig.â€¯3â€‚ â€‰The complete DSS calculation process of an oil painting image. Only relu2_1, relu3_1 and relu4_1 
of VGG-19[8] are used to calculate the deep semantic space. Fíœ is the guided filter, and íœ† is an artificial lin-
ear fusion parameter used to integrate spatial segmentation and illumination. Ml
s is the vector space, which 
contains deep semantic vector(DSV)
3920
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
idea to simulate this visual understanding of distinguishing objects before style migration. 
Our approach is to treat this spatial segmentation as a precondition (Fig.Â 4). At present, the 
space segmentation methods include automatic segmentation and manual labeling. How-
ever, automatic segmentation has the following problems in style migration:1) Automatic 
segmentation cannot mark untrained semantics; 2) Existing automatic semantic segmenta-
tion algorithms cannot mark objects in some artworks; 3) Compared with automatic seg-
mentation, the method of image structure segmentation by manual annotation is not limited 
to object category. Therefore, manual space segmentation is applied in this article. For the 
input images of each group, the first step is to obtain the spatial segmentation image Dc of 
the source image Pc, and the spatial segmentation image Ds of the style image Ps.
The creator reasonably uses different strokes of color, saturation, and brightness to 
â€œplaceâ€ them in a reasonable place. This reasonable placement is actually determined by 
the illumination on the object being painted. However, the existing methods of NST rarely 
take into account the importance of illumination for artistic stylization. Our algorithm inte-
grates light information into space segmentation image to calculate. The way to obtain a 
comprehensive semantic map containing spatial segmentation and illumination informa-
tion is as follows.
As can be seen from Fig.Â 5(c), the light estimation we obtained with most of the textures 
and strokes are successfully filtered out. Also, a guide filter protects the edges, the structure 
of the five senses (larger scale) is better protected than the texture (smaller scale). Each 
segment in Fig.Â 5(e) contains significant illumination information. Compared with figure 
Fig.â€¯4â€‚ â€‰Examples of the selected images and their corresponding spatial segmentation results
(a) Ps
(b) Ds
(c) Is
(d) Pssâ€™
(e) Pss
Fig.â€¯5â€‚ â€‰The example of integrating illumination in segmentation of object image. Â­Ps is a portrait (here is 
style image). Â­Ds is the spatial segmentation of Â­Ps. Â­Is is the light estimation of Â­Ps. Â­Pss is the composition result 
with textured filtering, while Â­Pssâ€™ has no textured filtering.Â (a) Ps (b) Ds (c) Is (d) Pssâ€™ (e) Pss
3921
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
Fig.Â 5(c), in Fig.Â 5(e), the guide filter is used to filter brush texture, therefore it looks more 
in line with the lighting situation of the real characters. In Fig.Â 5(d), the result shows the 
Pssâ€² result of direct processing of the original image without textured filtering.
In order to obtain estimated illuminationÂ Is andÂ Icâ€Š, RGB image is converted into YCrCb 
color space, because the Y channel can better extract the brightness in the image while 
ignoring the color difference. For some artworks (e.g. oil painting, sketch, etc.), the Y 
channel contains brush textures. However, it cannot directly express the true illumination 
of the original painted object (as shown in Fig.Â 5(a)). Therefore, Y channel is not used as 
illumination directly, Guided Filter [17] is applied to do blurring filtering on the brightness 
channel of the style image, that is,Â Is = Fíœ*Ysâ€Š. And Fíœ is the guided filter (5â€‰% size of the 
style image). As shown in Fig.Â 5(c), the brightness channel after blurring is the light infor-
mation we need.
Compared to style image, a real photo does not contain a brush texture that distorts its 
light. Moreover, the target we wanted can present the overall spatial rationality and the 
local details of the source image. When processing the source image, we need structure and 
texture information about the image to improve the expression of the content in the source 
image. In order to retain more of the image content, the light estimation of Pc not require 
blurring processing, that isÂ Ic=Yc.
Then the linear fusion method is used to integrate light estimation into the spatial seg-
mentation image. In algorithm, we set Î» = 1âˆ•2â€Š. Dc and Ds are the spatial segmentation of 
source image and style image, Ic and Is are the light estimation of source image and style 
image. Psc and Pss are the integration results of spatial segmentation and light information.
At last, the calculation of deep semantic space is shown below. After the last step, Psc 
and Pss have a degree of matching on the pixel block. However, the target is wanted to take 
on a more advanced and complex texture at a particular light point for an object, but the 
pixel blocks donâ€™t match up to that goal. Therefore, spatial position matching can base on 
the visual understanding of context orientation. In addition, each object we hoped to have a 
more three-dimensional artistic effect. Therefore, the position of the particular light need to 
be gained in the overall light of the region.
In Fig.Â 6(a), the light of two faces is contrasting. It will be difficult to establish a rea-
sonable contact in the pixel level matching Pss and Pscâ€Š: as shown in Fig.Â 6(b). In the girlâ€™s 
facial region, the matching yellow box corresponding to the brightest part in the male face 
almost occupy all the regions, while the darkest part labeled by red box match only a little. 
However, the male face need to be more stereoscopic after style transfer, meaningPss and 
Psc achieved the lighting matching effect shown in Fig.Â 6(c).
(1)
{
Psc = íœ†Dc + (1 âˆ’íœ†)Ic
Pss = íœ†Ds + (1 âˆ’íœ†)Is
(a) Ic and Is
(b) pixel level matching           
(c) ideally matching 
Fig.â€¯6â€‚ â€‰The results of Ic and Is matching in pixel level and deep semantic space. Ic and Is are the light estima-
tion of source image and style image.Â (a) Ic and Is (b) pixel level matching (c) ideally matching
3922
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
The response ofÂ Pss and Psc are used in CNNâ€™s deep feature to calculate the deep seman-
tic space. Since each channel in the feature space Fl
sc and Fl
ss is the response after convolu-
tion with a different filter, the contextual semantics of illumination need to be calculated by 
combining each channel. Deep semantic space of the source image and style image Ml
c and 
Ml
s as the vector space containing Vcl and Vsl deep semantic vectors (DSV):
Ml
câ€Š-- deep semantic space of the source image at layer l, Ml
sâ€Š-- deep semantic space of 
the style image at layer l, Vclâ€Š-- deep semantic vectors of source image at layer l, Vslâ€Š-- deep 
semantic vectors of style image at layer l.
In (3), the value of each DSV (â€Šml
câ€Š) consists of the response value ofFl
sc (orFl
ssâ€Š) at the 
same coordinate in all channels:
(Fl)
p,nâ€Š-- feature response at layer l, feature channel n and pixel p. Each channel in the 
feature space Fl
sc and Fl
ss is the response after convolution with a different filter.
The response of synthetically semantic map with spatial segmentation and illuminationÂ Pss 
and Psc in the feature layer of CNN has the following characteristics: The shallow layer of the 
VGG-19 network can make different characteristic responses to different colors. Therefore, 
space prior information of Pss and Psc will not be affected in the shallow layer of the net-
work. The shallow and middle layers of the VGG-19 network have the ability to understand 
low-level and intermediate image features. The networkâ€™s understanding of gradient and post-
gradient can abstract the context semantics of illumination and achieve more even illumination 
matching as shown in Fig.Â 6(c).
3.3â€‚ DSS â€‘ guided deep semantic loss function
3.3.1â€‚ Deep semantic mapping function
In the class of algorithms for NST, obtaining the target image X is equivalent to finding a 
determined optimization target for its CNN feature space Fl
x in advance, and then making it 
approach the target continuously through training X[14].
In order to find a certain stylized target for each object and light in the source image. For 
each feature response 
(Fl
x
)
p,n of the target image, if the source image has semantic ml
c(p) here, 
then we hope that the value of (Fl
x
)
p,ncan be as close as possible to the feature response value 
(Fl
s
)
í›¿(p),nwith the most similar semantic ml
s(í›¿(p))â€Š. í›¿(p) is the semantic mapping function, 
which means each ml
c(p) of the source image, finds the ml
s(í›¿(p)) that matches it best in the 
style image:
similarity(o1, o2) -- similarity function, using the Euclidean distance between two feature 
blocks of size 3 Ã— 3 Ã— Nlcentered on feature vectors o1, o2.
(2)
{ Ml
c =
[
ml
c(1), ml
c(2) â€¦ ml
c
(
Vcl
)]
Ml
s =
[
ml
s(1), ml
s(2) â€¦ ml
s
(
Vsl
)]
(3)
{
ml
c(p) = [(Fl
sc
)
p,1, (Fl
sc
)
p,2, â€¦ (Fl
sc
)
p,Nl]
T, âˆ€p â‰¤Vcl
ml
s(p) = [(Fl
ss
)
p,1, (Fl
ss
)
p,2, â€¦ (Fl
ss
)
p,Nl]
T, âˆ€p â‰¤Vsl
(4)
Î´(p) = q â†
q argmin{similarity(ml
c(p), ml
s(í›¿(p)))}
3923
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
ml
c(p) and ml
s(í›¿(p))â€Šin (4) are known information, thus í›¿(p) can complete the calcula-
tion before optimizing the image. In practice, PatchMatch algorithm is used [2] to cal-
culate semantic mapping function, which is defined as follow:
where NNFMl
c,Ml
s--the nearest neighbor space obtained by similarity matching between two 
deep semantic Spaces {Ml
c, Ml
s}.
3.3.2â€‚ Deformation ofÂ style features
When the algorithm iterates, it will take unnecessary computation time to find the target 
value 
(Fl
s
)
í›¿(p),n for each (Fl
x
)
p,nâ€Š. In order to eliminate this step, after calculating í›¿(p)â€Š, the 
style feature space[13] Fl
s is deformed to obtain the new style feature space Al
sâ€Š, 
namely:Al
s = align(Fl
s
)
From the optimization process in Fig.Â 7(b), it can be seen that the deformation treat-
ment of style features can achieve the expected effect. In Fig.Â 7(a), the activation value 
of n characteristic channels at each coordinate p is â€œpackagedâ€ and move together. This 
means that the deformation of style features only changes the original spatial position 
and light region of each object, without changing the stroke style of each semantics. 
The figure above of Fig.Â 7(b) shows the process of changing the style feature graph by 
usingÂ í›¿(p).
(5)
í›¿(p) = NNFMl
c,Ml
s(p)
(6)
(Al
s
)
p,n = (Fl
s
)
í›¿(p),n, âˆ€n â‰¤Nl
Fig.â€¯7â€‚ â€‰The process of obtaining semantic mapping function í›¿(p)Â after matching DSS.Â (a) DSS matching (b) 
calculation of DSS-loss
3924
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
3.3.3â€‚ Loss function ofÂ theÂ algorithm
The deformed style feature space Al
s has three properties. First of all, the style feature space Al
s 
after deformation retains the brushstroke style of these three levels. Secondly, feature graphs 
of Al
sâ€Š, Fl
c and Fl
x are of the same size. Finally, Al
s and Fl
c [39] have the same object labeling and 
contextual illumination at the same spatial location.
The total loss function is
where í›¼â€Š, weight of content loss; í›½â€Š, weight of style loss; í›¾â€Š, weight for DSS. In experiments, 
the fixed values of the parameters in the loss function are shown in TableÂ 1. Among them, 
Ll
content is the content loss function in algorithm [14] of Gayts etÂ al., Ll
style is the style loss 
function based on Gram statistics, and the last item is our proposed deep semantic loss 
function Ll
DSS:
where Fl
xâ€Š-- CNN feature space of the target image X.
In Eq.Â (7), the three loss functions have mutual compensation. As Al
s has both the contex-
tual space segmentation and illumination stereoscopic sense of the source image, as well as 
the superficial brushwork style of the style image. Ideally, if we can compute the optimal í›¿(p)â€Š, 
then Al
s is also the optimal, and the total loss function only needs Ll
DSSterms. Even if Al
sâ€Šis spa-
tially reasonable, some continuous features will still match to the same position, resulting in a 
slight texture distortion in art style of Al
s compared to that of Fl
s.
At this point, the second style loss of Ll
style will play a compensation role, because the tex-
ture deformation will cause the difference between target and style to increase, and then Ll
style 
will increase. Therefore, Ll
stylecan prevent the excessive distortion of texture. Similarly, if Al
s
contains a large number of wrong matching results, the first term Ll
content will be used as com-
pensation, because the wrong matching may lead to increased content loss in this part.
When solving Eq.Â (8), the optimization of Ll
DSSâ€Šis similar to that ofÂ Ll
contentâ€Š, which is calcu-
lated by the chain rule of BP algorithm. Since Al
s is known quantity before the iteration, the 
derivative of Ll
DSS with respect to each characteristic response is
To sum up, it can be seen as a solution of reasonable migration of DSS guided style that 
include using DSS to calculate the depth semantic mapping function í›¿(p)â€Š, using í›¿(p) to 
deform Fl
s to Al
sâ€Š, and using Al
s to optimize the target.
(7)
Ltotal = í›¼
âˆ‘
l
Ll
content + í›½
âˆ‘
l
Ll
style + í›¾
âˆ‘
l
Ll
DSS
(8)
Ll
DSS = âˆ¥Fl
x âˆ’Al
s âˆ¥
2
(9)
Ll
DSS
ğ›¿(Fl
x
)
i,j
=
{
(Fl
x âˆ’Al
s)j,iif(Fl
x
)
i,j > 0
0if(Fl
x
)
i,j â‰¤0.
Tableâ€¯1â€‚ â€‰Loss function parameter experience value of DSM-MS, where í›¼â€Š, weight of content loss; í›½â€Š, weight 
of style loss; í›¾â€Š, weight for DSS
í›¼âˆ•í›½
í›¼âˆ•í›¾
Ll
content
Ll
style
Ll
DSS
5 Ã— 10âˆ’4
2 Ã— 10âˆ’2
{relu4_1, relu5_1}
{relu1_1, relu2_1,
relu3_1, relu4_1,
relu5_1}
{relu2_1,
relu3_1,
relu4_1}
3925
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
3.4â€‚ Multiâ€‘scale iteration strategy
In the existing NST framework, if we want to synthesize the target image X with higher 
resolution, the number of feature response, parameter and loss will increase accordingly. 
The process of optimizing x requires more computing space and time. This section pre-
sents a multi-scale iterative strategy for NST. And we give an example of the multiscale 
iterative process in Fig.Â 8.
FigureÂ 9 illustrates the gradual refinement of styles to achieve the desired result. It 
is not necessary to use the target parameter X of W Ã— H size during the initial itera-
tion, because there is still a lot of noise/source image texture. Optimization of small 
scale X can quickly converge the image to a rough state. At the beginning of optimizing 
the maximum scale of X, since its initial state has achieved a good effect, the required 
number of iterations can be greatly reduced. In conclusion, our multi-scale strategy can 
achieve the expected effect more quickly, compared with initializing X directly to an 
image of size W Ã— H.
The multi-scale iteration strategy can be summarized as:
a.	  Downsample the input image to get {Xk
c}k=1,2,3â€Š, {Xk
s}k=1,2,3â€Š, and {Xk
sc}k=1,2,3â€Š, and the 
downsampling coefficient is Î± = 1âˆ•2.
b.	  When k =1, the target image is initialized to X1(0) = random
(
W
4 , H
4
)
 or X1(0) = X1
câ€Š; 
When k >1, the target image is initialized to Xk(0) = upsampling(Xkâˆ’1(T))â€Š, and the 
upsampling coefficient is 1âˆ•Î± = 2.
	 iii.	  The objective function of each scale is set as Eq.Â (7), and feature space is replaced 
by the input image of the corresponding scale:
(10)
Xk â†argminLtotal
(Xk|Xk
c, Xk
s, Xk
cs, Xk
ss
), k = 1,2, 3.
Fig.â€¯8â€‚ â€‰The multiscale iterative process of the final experimental effect in Fig.Â  15(d). Here are iterative 
results of three scales. We choose 4 of 7 small scale, 3 of 7 medium scale, and final image to show the 
details of change
3926
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
	 iv.	  The iterative update process of the target is: Xk(0) â†’Xk(1) â†’Xk(2)â‹¯â†’Xk(T).Gener-
ally speaking, each scale can get good results after T=10 iterations. At this point, the 
loss function starts to slow down with Ll
DSS converged.
	 v.	  The composite image is X = X3(T).
3.5â€‚ Application inÂ 3D scene art rendering
In this section, we present the application of the proposed method for painterly rendering 
of 3D scenes. FiÅ¡er etÂ al. propose the StyLit algorithm, which can render any given style 
onto a 3D scene [11]. However, this method is to render at the pixel level of the image, 
most of the brushstroke texture and direction of the artistic style disappear. Our method has 
the advantages of synthesizing complex brushstroke styles and protecting space and light-
ing rationality.
Because there is no source image available in 3D scenes, the content loss needs to be 
modified. In addition, it needs to use the 3D scene information to calculate the input image 
expected by the NST algorithm. We use the light path representation rendering method 
of NVIDIA Iray SDK to transform the 3D scene into an image with spatial and lighting 
semantics [18]. The LPEs method can render the three types of lighting effects on the 
k = 1
k = 2
k = 3
initialized
upsampling
17 s / iteration
80 s / iteration
220 s / iteration
256*256
128*128
512*512
256*256
upsampling
Fig.â€¯9â€‚ â€‰Multi-scale iterative strategy for DSM-MS
3927
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
object separately. We linearly superimpose the three rendering effects to obtain the seman-
tic map Psc required to calculate the DSS. The calculation process of using LPEs to convert 
3D scenes into Psc is shown in the second row of Fig.Â 10.
After conversion, the image Psc of NST algorithm is obtained, and the following render-
ing process could be executed.
4â€‚ Experimental result
4.1â€‚ Implementation details
This section briefly describes implementation details and compares experimental results 
produced with different algorithm. While we do our best to show the style transfer effect 
between different kinds of images, such as portrait, photo, natural scene and landscape 
painting. A total of twenty style images and twenty content images are selected.
The experimental environment in this paper is shown in TableÂ  2. The programming 
language environment of DSM-MS algorithm is Python, and the computing language 
environment of light estimation and comprehensive semantic graph is Matlab. In all the 
experiments below, the number of iterations of Gatys etÂ al. [14] is fixed to 1000 times. The 
weight of style loss function is 1000 times of the weight of content loss. Results of other 
algorithms are directly extracted from their papers. As shown in TableÂ 1, the parameters of 
loss function are fixed. Objective x is divided into 3 scales, and each scale is iterated for 7 
times.
(a) Meshc
(b) Ps
(c) Is
(d) Ds
(e) Pss
(f) LD1Ec
(g) LD2Ec
(h) LSEc
(i) Psc
Fig.â€¯10â€‚ â€‰The calculation process of using LPEs to convert 3D scene intoPscâ€Š. Â­Ps is a portrait (here is style 
image). Â­Ds is the spatial segmentation of Â­Ps. Â­Is is the light estimation of Â­Ps. Â­Pss is the composition result with 
textured filtering. LD1Ec is the effect of the light source directly shining on the target object. LD2Ec is the 
effect of ambient light diffusely reflected to the target object. LSEc is specular reflection of ambient light on 
smooth objects. Psc is the semantic map of 3D Meshc.Â (a) Meshc (b) Ps (c) Is (d) Ds (e) PssÂ (f) LD1Ec (g) 
LD2Ec (h) LSEc (i) Psc
Tableâ€¯2â€‚ â€‰Experimental 
environment
Language
DeepLearning-Lib
GPU
OS
Python/Matlab
Theano
NVIDIA Tesla K4
CentOS 7
3928
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
Our experimental data can be obtained at https://â€‹github.â€‹com/â€‹sunlum/â€‹Deep-â€‹Semanâ€‹tic-â€‹
Space-â€‹NST, which also includes the images and source code.
4.2â€‚ Data andÂ preâ€‘ treatment
The data sets of experimental input images come from Internet and other algorithms. In 
this paper, these input images are divided into two categories: face image and natural land-
scape image. Each image is pre-calibrated to simulate space segment.
The Wand tool of Photoshop is used to label the images, which takes an average of 
4Â min to complete the task. In the following experiments, the images of natural landscapes 
are uniformly divided into foreground (green) and background (red), and we divide face 
image into background, skin, clothes, hair and other areas (as shown in Figs.Â 4 and 5(b)).
4.3â€‚ Result andÂ analysis
4.3.1â€‚ Ablation experiment ofÂ lighting information onÂ style migration
The output of the algorithm of Gatys etÂ  al. presents wrong illumination information 
compared with the source image. FigureÂ  11(f)Â  demonstrates that space segmentation 
Dc and Ds successfully guide the general placement of style textures. FigureÂ 11(h)Â is 
more consistent with the source image than Fig.Â 11(f). The only difference is the way 
the initialization is done. However, it still cannot fully reproduce face illumination 
in Fig.Â 11(a). For example, a personâ€™s ear using Fig.Â 11(b)Â lighting conditions should 
not be highlighted. The light gradients at cheekbones did not show facial features in 
the source image. The error in the latter is similar to Fig.Â 11(d). The results shown in 
Fig.Â 11(g) and (i)Â can be seen that the lighting condition of the face of the two images is 
more consistent with the source image. Even if the target initialization of are different, 
the output is almost identical.
FigureÂ  11 proves that DSS can guide the style to reasonably migrate to the posi-
tion close to the same annotation and illumination as the source image. The output has 
the style texture while the spatial stereoscopic impression is more consistent with the 
source image.
4.3.2â€‚ Discussion onÂ theÂ effect ofÂ style migration betweenÂ objects ofÂ different 
categories
Our algorithm can achieve style migration between different categories of objects. DSS can 
guide the style migration between two objects with the same color marking but different 
categories. The reason is that manual space segmentation limits the search scope of light 
structure, so the deep semantic matching in DSM-MS algorithm does not require the object 
itself to have the same category characteristics.
Due to the large color difference in the face of the source image, the content loss in the 
face area decreases slowly. The optimization of the Gram matrix for the overall style loss 
focuses on the background and hair area. In the second set of experiments, the flaws of 
the algorithm are more obvious. The white background of the style image is taken as the 
brightest style and migrated to the brighter face area of the source image. Compared with 
the results of Gayts etÂ al., it can be seen that the facial color of the image synthesized by 
our algorithm is more normal. Although the color of the face is normal, there are still two 
3929
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
problems in the output results of Liao etÂ al. It causes slight distortion in the details of the 
charactersâ€™ features. Compared with the results of Liao etÂ al., our synthetic images protect 
the original lighting conditions, which are more in line with the spatial structure of the 
source image object (Fig.Â 12).
In the following experiments, validation of our algorithm is divided into four cases. 
Style migration from portrait to face image, style migration from non-portrait to face 
image, style migration from portrait to natural landscape image and style migration from 
non-portrait to natural landscape image. Because other NST algorithms with improved spa-
tial structures cannot handle style migration between different categories of objects [26, 
29], we only compare our results with the algorithm of Gatys etÂ al.
1)	 Facial stylization of portrait
Fig.â€¯11â€‚ â€‰Comparison of the output of the traditional style migration algorithm and our algorithm in this 
paper. (a) and (b) are the source image and style image, respectively. (c) is the result of matching the light 
information Â­Psc and Â­Pss at the pixel level [20]. (d) shows the output of the algorithm of Gatys etÂ al. [14], 
which presents wrong illumination information compared with the source image (a). And (e) shows the 
output of the algorithm of Neural Doodle [5]. (f) and (g) are two groups of contrast experiments initialized 
to white noise. (g) contains illumination information but (f) not. (h) and (i) is the contrast test of (f) and (g), 
but the difference is that the image is initialized to Â­Pc instead of white noise
3930
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
Especially for sketch and oil painting styles, our algorithm can correctly extract illumi-
nation and make DSS matching. Compared with each group of results and DSM-MS algo-
rithm results, the texture of the latter image is placed gradually along the correct illumina-
tion, thus, the original illumination situation is completely protected and more consistent 
with the spatial structure of the source image. These groups of experiments prove that our 
algorithm is also applicable to other art styles (Fig.Â 13).
We can see that in the first and third sets of experiments, our method better protects 
the content information in the source image, and at the same time uses the color and artis-
tic texture in the style image to stylize the source image. In the second and fifth sets of 
experiments, our results are closer to the artistic effects in the style images, and at the same 
time the processing of the details and boundaries of the charactersâ€™ faces is more superior. 
In contrast, the output result of the baseline algorithm lost a lot of content information, 
especially in the fifth set of experiments, even texture noise is generated on the face of the 
character. It is worth mentioning that in the expression of the style texture, the size of the 
style texture can be adjusted through algorithm parameters to achieve the desired output 
stylized image. In the experimental part, we set the parameters to obtain the corresponding 
algorithm output in order to better show the superiority of our algorithm in texture place-
ment and detail processing.
2)	 Facial stylization of landscape painting
In this group of experiments, our evaluation goal is to hope that the face can be painted 
using the style of the marked area of the style image. The original illumination and three-
dimensional structure of the face of the figure are retained by our algorithm. This stereo-
scopic feeling is realized by using the excessive light and shade within the same marked 
area in the style image (Fig.Â 14).
3)	 Natural scenes stylization of portraits
Fig.â€¯12â€‚ â€‰The mutual style transfer experiment of two groups of faces, our DSM-MS algorithm is compared 
with the previous algorithms. (a) contains the source image and the style image. In each experiment, we 
compare the effect of the DSM-MS algorithm with those of Gayts etÂ al. [14] and Liao etÂ al. [29]. (b) and (c) 
are the results of Gayts etÂ al. and Liao etÂ al. respectively, (d) is ours
3931
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
In the first image of Fig.Â 15(d), all the white snow is covered with a textured brush of 
light powder, light blue and light yellow, exactly the same color and texture as the style 
image for the brighter areas of the face. In the second image of Fig.Â 15(d), we can feel that 
the white part has the texture of â€œwhite paperâ€, while all the other grey/black parts have 
a certain artistic expression. In the two groups of experiment, Gaytsâ€™ algorithm identify 
the brightest area as a kind of content. Therefore, when the style loss function converges, 
it chooses to place a texture around the area instead of filling the area with the texture 
it should have. However, the DSM-MS algorithm can match this region with the corre-
sponding brightness region of the style image and place the texture for it when Â­LDSS con-
verges. In the cloud portion of the sky, Fig.Â 15(d) also shows the correct shading and shape. 
In the foreground part of Fig.Â 15(c), certain differences in light and shade can be seen in 
Fig.â€¯13â€‚ â€‰Five other groups of stylistic migration experiments from portraiture to face images. (a) contains 
the source image and the style image, (b) is the result of Gayts etÂ al., and (c) is ours
3932
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
mountain areas. The texture synthesized by Gaytsâ€™ algorithm can distinguish white snow, 
black trees and the brightest distant mountains. However, these light and shade relation-
ships do not use the same brush texture as the style image.
4)	 Natural scenes stylization of landscape paintings
It can be clearly seen from Fig.Â 16(d) that the foreground and background areas present 
different colors and textures. This segmentation makes the image more realistic than the 
output of algorithms such as Gayts. In Fig.Â 16(d), the sky of sixth image clearly reproduces 
the structure of clouds. The seventh image in Fig.Â 16(d) migrates the texture of the â€œtreeâ€ 
to the surface of the mountain, giving the image a seasonal shift from winter to summer. 
Compared with the third image in Fig.Â 16(c), the third and fourth images in Fig.Â 16(d) 
Fig.â€¯14â€‚ â€‰Two groups of experiments on face image style migration using landscape painting. (a) is style 
image, (b) is source image, (c) is the result of Gayts etÂ al., and (d) is ours
Fig.â€¯15â€‚ â€‰Two groups of experiments using portraits to stylize images of natural landscapes. (a) is source 
image, (b) is style image, (c) is the result of Gayts etÂ al., and (d) is ours
3933
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
achieve more delicate texture migration. In the sky section of Fig.Â 16(d), the color transi-
tion is more natural and consistent with the style image. In Fig.Â 16(c), the sky part of the 
fourth image presents a state that is not filled by the style texture. This is because the loss 
function of Gayts etÂ al. [14] cannot place texture that increases the loss of the content in 
order to protect the contents of the sky. On the contrary, DSS algorithm can detect the faint 
light-gradient and force it to match the texture with the corresponding brightness.
In the first scale, it can be seen that the algorithm first places some combination of 
strokes as a whole in the target. After about 7 iterations, the pixel style in the source image 
Fig.â€¯16â€‚ â€‰Seven groups of experiments on natural scenes style migration using landscape painting. (a) is 
source image, (b) is style image, (c) is the result of Gayts etÂ al., (d) is ours
3934
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
has disappeared. In the later iteration of the scale, the brush strokes gradually began to 
separate, showing a more detailed representation of the light.
5)	 Output of our method in 3D scene art rendering
The comparison results of the proposed and Gaytsâ€™ method in 3D scene art render-
ing are shown in Fig.Â 17. The comparison between our result Fig.Â 17(c) and Gaytsâ€™ result 
Fig.Â 17(d) shows that the foreground and background textures of the two output images are 
opposite. This is because the texture placement of Gayts etÂ al. is related to the color of the 
source image. Therefore, Fig.Â 17(d) mistakenly migrates the red texture in the background 
of the style image to the rhinoceros in the foreground of Meshc. In our algorithm, the depth 
semantic information DSS of the target scene and style image controls the placement of 
the texture. The red texture marked with a blue background can only be placed in the back-
ground of the target scene, and the blue texture marked with red can only be placed in the 
object (rhinoceros) of the target scene. At the same time, the overall rendering effect is 
satisfactory.
To sum up, the proposed DSS in this paper can correctly guide the placement area of 
different art styles on target images. From these results, the DSM-MS algorithm can suc-
cessfully improve artistic feeling of the output image. The spatial structure of the com-
posite image is presented by the corresponding strokes and colors in the style image. The 
composite effect is more natural, and the spatial structure of the source image is better 
preserved from the overall vision.
4.3.3â€‚ Algorithm efficiency analysis
Our algorithm does not use pre-training network. Considering the configuration envi-
ronment, we use i7-6700 CPU and Nvidia GTX1070 graphics server for the efficiency 
experiment.
In the experiment, it takes 700-1000 iterations for Gaytsâ€™ original framework to achieve 
a better synthesis effect. In our DSM-MS algorithm, a multi-scale iterative strategy is 
adopted, and the deformed style feature space enables the target to converge faster. Only 
about 20 iterations can achieve a better synthesis effect. Li and Wandâ€™s method needs 300-
500 iterations to get good synthesis result (TableÂ 3).
Fig.â€¯17â€‚ â€‰Application of DSM-MS algorithm in 3D scene art rendering. The 3D Â­Meshc in (a) includes two 
objects, rhinoceros and a rectangular background. The target semantic graph Â­Psc shown in the upper left 
corner of (a), can be obtained by LPEs (Lighting Path Expressions). We use Â­Psc as the source (content) 
image. (b) is the style image in this group of experiment, the Â­Pss is shown in the upper left corner of (b). (d) 
is the output of Gatys etÂ al. [12], and it incorrectly migrate the red stripe texture of the style image to the 
red rhinoceros. In our output (c), the DSS of target scene and style image control the placement of texture in 
the way we expect
3935
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
In general, time of our DSM-MS algorithm increase for demarcating spatial segmenta-
tion map, calculating illumination, forward computing for DSS, and DSS matching the net-
work feature layer. However, because the application of multi-scale iteration strategy, the 
target optimization of small scale and the total number of iterations is greatly reduced. The 
DSM-MS algorithm is faster than the algorithm of Gayts. Our algorithm is about 3.5 times 
faster when optimizing images of size 1024*1024.
Deception rate [32] is an automatic evaluation indicator of style transfer proposed in a 
2018 ECCV paper. By using the VGG network to classify the works of artists of different 
styles, learn their artistic style characteristics.
The network automatically judges the experimentally synthesized pictures, and calcu-
lates and generates the image can be determined that the probability of some artistic style, 
the style to determine the quality of the results of the migration. The criterion is that the 
higher the Deception rate score, the better the quality of the generated style transfer pic-
tures (TableÂ 4).
5â€‚ Conclusions
This paper proposes a multi-scale depth style transfer algorithm based on deep semantic 
matching(DSM-MS). According to two key visual understanding approaches in the process 
of painting, we integrate this visual guidance in the process of style transfer. Our algorithm 
can use the spatial segmentation and illumination information of the input image, which 
guide the artistic styles of different textures and colors to a reasonable position.
Tableâ€¯3â€‚ â€‰Efficiency analysis
Methods
Pre-network 
Training times(h)
Iteration
Time(s)
Styles/Model
256*256 512*512 1024*1024
Gayts etÂ al. [14]
0
1000
50.64 179.45 683.57
âˆ
Li and Wand [26]
0
500
89.39 468.62 -
âˆ
Shaohua Li etÂ al. [28]
0
1000
39.40 153.46 604.49
âˆ
Ours
0
21
65.42 120.67 190.84
âˆ
Johnson etÂ al. [24]
3.5
1
0.69 3.07 14.39
1
Huang etÂ al. [21]
-
1
1.31 3.38 20.58
âˆ
Tableâ€¯4â€‚ â€‰Deception Rate Analysis
Methods
Deception 
rate(0.001)
Absolute optimi-
zation difference
Styles/Model
Gayts etÂ al. [14]
0.427
+0.169
âˆ
Li and Wand [26]
0.315
+0.281
âˆ
Shaohua Li etÂ al. [28]
0.367
+0.229
âˆ
Ours
0.596
0
âˆ
Johnson etÂ al. [24]
0.398
+0.198
1
Huang etÂ al. [21]
0.502
+0.094
âˆ
3936
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
We define DSS deep semantic space to simulate the spatial and illumination visual guid-
ance information, and give the algorithm of using DSS to guide the style transfer process. 
The multi-scale iteration method is used to optimize the target, which improve the total 
computing efficiency and save computing resources, especially on the HD image synthesis 
rate.
In addition, we compare the DSM-MS algorithm with several existing NST algorithms. 
Our method enables the texture within each segmented region to be rendered in accordance 
with the original spatial structure, which protect the original spatial stereoscopic feeling. 
At the same time, the properly placed style texture and color make the picture more real in 
the overall vision. In terms of algorithm applicability, the DSM-MS algorithm is applicable 
to style transfer between different categories of objects.
Our work is a semi-automated approach that requires user intervention to initialize 
the source image and the style image. The automatic segmentation method can simplify 
the process, but the existing problems have been discussed. Therefore, it is important to 
develop a completely automatic segmentation method which can get a high degree of 
acceptability at the same time. Our future work will focus on developing robust, accurate, 
and fully automatic methods.
Acknowledgementsâ€‚ This work was also supported in part by the key project of Trico-Robot plan of NSFC 
under grant No. 91748208, key project of Shaanxi province No.2018ZDCXL-GY-0607, the Fundamental 
Research Funds for the Central Universities No. XJJ2018254, and China Postdoctoral Science Foundation 
No. 2018M631164.
References
	 1.	 Ashikhmin N (2003) Fast texture transfer. IEEE Comput Graph Appl 23(4):38â€“43
	 2.	 Barnes C, Shechtman E, Finkelstein A etÂ al (2009) PatchMatch: a randomized correspondence algo-
rithm for structural image editing. ACM Trans Graph 28(3):24
	 3.	 Baxter W, Govindaraju N (2010) Simple data-driven modeling of brushes. In: Proceedings of the 2010 
ACM SIGGRAPH symposium on Interactive 3D Graphics and Games. ACM, 135-142
	 4.	 Champandard AJ (2015) Deep forger: Paint photos in the style of famous artists. https://â€‹deepfâ€‹orger.â€‹
com/
	 5.	 Champandard AJ (2016) Semantic style transfer and turning two-bit doodles into fine artworks.arXiv 
preprintarXiv:1603.01768
	 6.	 Chen ZL, Kim B, Ito D etÂ al (2015) Wetbrush: GPU-based 3D painting simulation at the bristle level. 
ACM Trans Graph 34(6):200:1-200:11
	 7.	 Collomosse J (2012) Image and video-based artistic stylisation, vol 42. Springer Science & Business 
Media
	 8.	 Drori I, Cohen-Or D, Yeshurun H (2003) Example based style synthesis. In: Proceedings of the IEEE 
Conference on Computer Vision and Pattern Recognition, vol 2. IEEE, New York, pp IIâ€“143
	 9.	 Efros AA, Freeman WT (2001) Image quilting for texture synthesis and transfer. In: Proceedings of the 
28th annual conference on Computer graphics and interactive techniques. ACM, 341-346
	10.	 Elad M, Milanfar P (2017) Style transfer via texture synthesis. IEEE Trans Image Process 
26(5):2338â€“2351
	11.	 FiÅ¡er J, JamriÅ¡ka O, LukÃ¡Ä M etÂ al (2016) StyLit: illumination-guided example-based stylization of 3D 
renderings[J]. ACM Trans Graph 35(4):92
	12.	 Frigo O, Sabater N, Delon J, Hellier P (2016) Split and match: Example-based adaptive patch sam-
pling for unsupervised style transfer. In: Proceedings of the IEEE Conference on Computer Vision and 
Pattern Recognition, pp 553â€“561
	13.	 Gatys L, Ecker AS, Bethge M (2015) Texture synthesis using convolutional neural networks. Advances 
in Neural Information Processing Systems, 262â€“270
3937
Multimedia Tools and Applications (2022) 81:3915â€“3938

1 3
	14.	 Gatys LA, Ecker AS, Bethge M (2016) Image style transfer using convolutional neural networks. Com-
puter Vision and Pattern Recognition (CVPR), 2016 Conference on. IEEE, New York, pp 2414-2423
	15.	 Gatys LA, Ecker AS, Bethge M etÂ al (2017) Controlling perceptual factors in neural style transfer. 
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 
3985-3993
	16.	 Gooch B, Gooch A (2001) Non-photorealistic rendering. A. K. Peters, Ltd.,Â Natick
	17.	 He K, Sun J, Tang X (2010) Guided image filtering. European Conference on Computer Vision. 
Springer, Berlin, 1-14
	18.	 Heckbert PS (1990) Adaptive radiosity textures for bidirectional ray tracing[J]. ACM SIGGRAPH 
Comput Graph 24(4):145â€“154
	19.	 Hertzmann A (1998) Painterly rendering with curved brush strokes of multiple sizes. In: Proceedings 
of the 25th Annual Conference on Computer Graphics and Interactive Techniques, 453-460
	20.	 Hertzmann A, Jacobs CE, Oliver N etÂ al (2001) Image analogies//Proceedings of the 28th annual con-
ference on Computer graphics and interactive techniques. ACM, New York, pp 327-340
	21.	 Huang X, Belongie S (2017) Arbitrary Style Transfer in Real-time with Adaptive Instance Normaliza-
tion. In: Proceedings of the IEEE International Conference on Computer Vision(ICCV), v 2017-Octo-
ber, pÂ 1510-1519
	22.	 Jing Y, Yang Y, Feng Z etÂ al (2019) Neural style transfer: A Review//Computer Vision and Pattern 
Recognition (CVPR), 2019 Conference on. IEEE, arXiv:1705.04058
	23.	 Johnson J, Alahi A, Fei-Fei L (2016) Perceptual losses for real-time style transfer and super-resolution. 
In: European Conference on Computer Vision. Springer, Cham, 694-711
	24.	 Johnson J, Alahi A, Fei-Fei L (2016) Perceptual losses for real-time style transfer and super-resolution. 
In: Proc. European Conference on Computer Vision (ECCV), pp 694â€“711
	25.	 Lee H, Seo S, Ryoo S etÂ al (2010) Directional texture transfer. In: Proceedings of the 8th International 
Symposium on Non-Photorealistic Animation and Rendering. ACM, 43-48
	26.	 Li C, Wand M (2016) Combining markov random fields and convolutional neural networks for image 
synthesis. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
(CVPR), 2479-2486
	27.	 Li Y, Wang N, Liu J etÂ al (2017) Demystifying neural style transfer. In: Proceedings of the 26th Inter-
national Joint Conference on Artificial Intelligence. AAAI Press,Â Palo Alto, pp 2230-2236
	28.	 Li S, Xu X, Nie L, Chua T-S (2017) Laplacian-steered neural style transfer. In: ACM Multimedia Con-
ference (MM)
	29.	 Liao J, Yao Y, Yuan L etÂ al (2017) Visual attribute transfer through deep image analogy. ACM Trans 
Graph 36(4):120
	30.	 Liu XC, Cheng MM, Lai YK etÂ al (2017) Depth-aware neural style transfer. In: Proceedings of the 
Symposium on Non-Photorealistic Animation and Rendering. ACM, New York, 4
	31.	 Prisma L, Prisma (2016) Turn memories into art using artificial intelligence. https://â€‹prisma-â€‹ai.â€‹com
	32.	 Sanakoyeu A, Kotovenko D, Lang S etÂ al (2018) A style-aware content loss for real-time HD style 
transfer[J].Â arXiv:180710201v2, ECCV
	33.	 Simonyan K, Zisserman A (2014) Very deep convolutional networks for large-scale image recognition. 
arXiv preprint arXiv:1409.1556
	34.	 Strothotte T, Schlechtweg S (2002) Non-photorealistic computer graphics: modeling, rendering, and 
animation. Morgan Kaufmann
	35.	 Ulyanov D, Lebedev V, Lempitsky V (2016) Texture networks: feed-forward synthesis of textures and 
stylized images. In: International Conference on Machine Learning. 1349-1357
	36.	 Wang CM, Wang RJ (2007) Image-based color ink diffusion rendering. IEEE Trans Vis Comput Graph 
13(2):235â€“246
	37.	 Wang M, Wang B, Fei Y etÂ al (2014) Towards photo watercolorization with artistic verisimilitude. 
IEEE Trans Vis Comput Graph 10(20):1451â€“1460
	38.	 Wilmot P, Risser E, Barnes C (2017) Stable and controllable neural texture synthesis and style transfer 
using histogram losses.arXiv preprintarXiv:1701.08893
	39.	 Yosinski J, Clune J, Fuchs T etÂ al (2015) Understanding neural networks through deep visualization. 
In: ICML Workshop on Deep Learning.
	40.	 Zeng K, Zhao M, Xiong C etÂ al (2009) From image parsing to painterly rendering. ACM Trans Graph 
29(1):2
Publisherâ€™s Noteâ€‚ Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
institutional affiliations.
3938
Multimedia Tools and Applications (2022) 81:3915â€“3938

